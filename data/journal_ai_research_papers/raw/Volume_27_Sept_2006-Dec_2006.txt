Journal of Artificial Intelligence Research 27 (2006) 465-503

Submitted 04/06; published 12/06

Preference-based Search using Example-Critiquing with Suggestions
Paolo Viappiani
Boi Faltings

PAOLO . VIAPPIANI @ EPFL . CH
BOI . FALTINGS @ EPFL . CH

Artificial Intelligence Laboratory (LIA)
Ecole Polytechnique Fédérale de Lausanne (EPFL)
Station 14, 1015 Lausanne, Switzerland

Pearl Pu

PEARL . PU @ EPFL . CH

Human Computer Interaction Group (HCI)
Ecole Polytechnique Fédérale de Lausanne (EPFL)
Station 14, 1015 Lausanne, Switzerland

Abstract
We consider interactive tools that help users search for their most preferred item in a large
collection of options. In particular, we examine example-critiquing, a technique for enabling users
to incrementally construct preference models by critiquing example options that are presented to
them. We present novel techniques for improving the example-critiquing technology by adding
suggestions to its displayed options. Such suggestions are calculated based on an analysis of users’
current preference model and their potential hidden preferences. We evaluate the performance of
our model-based suggestion techniques with both synthetic and real users. Results show that such
suggestions are highly attractive to users and can stimulate them to express more preferences to
improve the chance of identifying their most preferred item by up to 78%.

1. Introduction
The internet makes an unprecedented variety of opportunities available to people. Whether looking
for a place to go for vacation, an apartment to rent, or a PC to buy, the potential customer is faced
with countless possibilities. Most people have difficulty finding exactly what they are looking for,
and the current tools available for searching for desired items are widely considered inadequate.
Artificial intelligence provides powerful techniques that can help people address this essential problem. Search engines can be very effective in locating items if users provide the correct queries.
However, most users do not know how to map their preferences to a query that will find the item
that most closely matches their requirements.
Recommender systems (Resnick et al., 1994; Adomavicius & Tuzhilin, 2005; Burke, 2002b)
address this problem by mapping explicit or implicit user preferences to items that are likely to fit
these preferences. They range from systems that require very little input from the users to more
user-involved systems. Many collaborative filtering techniques (Konstan et al., 1997), infer user
preferences from their past actions, such as previously purchased or rated items. On the other hand,
popular comparison websites1 often require that users state at least some preferences on desired
attribute values before producing a list of recommended digital cameras, portable computers, etc.
In this article, we consider tools that provide recommendations based on explicitly stated preferences, a task that we call preference-based search. In particular, the problem is defined as:
1. E.g., www.shopping.com
c
°2006
AI Access Foundation. All rights reserved.

V IAPPIANI , FALTINGS , & P U

Given a collection O = {o1 , .., on } of n options, preference-based search (PBS) is an
interactive process that helps users identify the most preferred option, called the target
option ot , based on a set of preferences that they have stated on the attributes of the
target.
Tools for preference-based search face a tradeoff between two conflicting design goals:
• decision accuracy, measured as the percentage of time that the user finds the target option
when using the tool, and
• user effort, measured as the number of interaction cycles or task time that the user takes to
find the option that she believes to be the target using the tool.
By target option, we refer to the option that a user prefers most among the available options.
To determine the accuracy of a product search tool, we measure whether the target option a user
finds with the tool corresponds to the option that she finds after reviewing all available options in
an offline setting. This procedure, also known as the switching task, is used in consumer decision
making literature (Haubl & Trifts, 2000). Notice that such procedure is only used to measure the
accuracy of a system. We do not suggest that such procedure models human decision behavior.
In one approach, researchers focus purely on accuracy in order to help users find the most preferred choice. For example, Keeney and Raiffa (1976) suggested a method to obtain a precise model
of the user’s preferences. This method, known as the value function assessment procedure, asks the
user to respond to a long list of questions. Consider the case of search for an ideal apartment. Suppose the decision outcome involves trading off some preferred values of the size of an apartment
against the distance between the apartment and the city center. A typical assessment question is in
the form of “All else being equal, which is better: 30 sqm at 60 minutes distance or 20 sqm at 5 minutes distance?” Even though the results obtained in this way provide a precise model to determine
the most preferred outcome for the user, this process is often cognitively arduous. It requires the
decision maker to have a full knowledge of the value function in order to articulate answers to the
value function assessment questions. Without training and expertise, even professionals are known
to produce incomplete, erroneous, and inconsistent answers (Tversky, 1974). Therefore, such techniques are most useful for well-informed decision makers, but less so for users who need the help
of a recommender system.
Recently, researches have made significant improvement to this method. Chajewska, Koller, and
Parr (2000) consider a prior probability distribution of a user’s utility function and only ask questions
having the highest value of information on attributes that will give the highest expected utility. Even
though it was developed for decision problems under uncertainty, this adaptive elicitation principle
can be used for preference elicitation for product search which is often modeled as decision with
multiple objectives (see in the related work section the approach of Price & Messinger, 2005).
Boutilier (2002) and Boutilier, Patrascu, Poupart, and Schuurmans (2005) further improved this
method by taking into account the value assigned to future preference elicitation questions in order
to further reduce user effort by modeling the maximum possible regret as a stopping criterion.
In another extreme, researchers have emphasized providing recommendations with as little effort as possible from the users. Collaborative filtering techniques (Konstan et al., 1997), for example, infer an implicit model of a user’s preferences from items that they have rated. An example of
such a technique is Amazon’s “people who bought this item also bought...” recommendation. However, users may still have to make a significant effort in assigning ratings in order to obtain accurate
466

P REFERENCE - BASED S EARCH USING E XAMPLE -C RITIQUING WITH S UGGESTIONS

recommendations, especially as a new user to such systems (known as the new user problem). Other
techniques produce recommendations based on a user’s demographic data (Rich, 1979; Krulwich,
1997).
1.1 Mixed Initiative Based Product Search and Recommender Systems
In between these two extremes, mixed-initiative dialogue systems have emerged as promising solutions because they can flexibly scale user’s effort in specifying their preferences according to the
benefits they perceive in revealing and refining preferences already stated. They have been also
referred to as utility and knowledge-based recommender systems according to Burke (2002b), and
utility-based decision support interface systems (DSIS) according to Spiekermann and Paraschiv
(2002). In a mixed-initiative system, the user takes the initiative to state preferences, typically in
reaction to example options displayed by the tool. Thus, the user can provide explicit preferences
as in decision-theoretic methods, but is free to flexibly choose what information to provide, as in
recommender systems.
The success of these systems depends not only on the AI techniques in supporting the search
and recommending task, but also on an effective user-system interaction model that motivates users
to state complete and accurate preferences. It must strike the right compromise between the recommendation accuracy it offers and the effort it requires from the users. A key criterion to evaluate
these systems is therefore the accuracy vs. effort framework which favors systems that offer maximum accuracy while requiring the same or less user effort. This framework was first proposed
by Payne, Bettman, and Johnson (1993) while studying user behaviors in high-stake decision making settings and later adapted to online user behaviors in medium-stake decision making environments by Pu and Chen (2005) and Zhang and Pu (2006).
In current practice, a mixed-initiative product search and recommender system computes its
display set (i.e., the items presented to the user) based on the closeness of these items to a user’s
preference model. However, this set of items is not likely to provide for diversity and hence may
compromise on the decision accuracy. Consider for example a user who is looking for a portable
PC and gives a low price and a long battery life as initial preferences. The best matching products
are all likely to be standard models with a 14-inch display and a weight around 3 kilograms. The
user may thus never get the impression that a good variety is available in weight and size, and may
never express any preferences on these criteria. Including a lighter product in the display set may
greatly help a user identify her true choice and hence increase her decision accuracy.
Recently, the need for recommending not only the best matches, called the candidates, but also
a diverse set of other items, called suggestions, has been recognized. One of the first to recognize
the importance of suggestive examples was ATA (Linden, Hanks, & Lesh, 1997), which explicitly
generated examples that showed the extreme values of certain attributes, called extreme examples.
In case-based recommender systems, the strategy of generating both similar and diverse cases was
used (McSherry, 2002; Smyth & McGinty, 2003). Hebrard, Hnich, O’Sullivan, and Walsh (2005) investigated algorithms for generating similar and diverse solutions in constraint programming, which
can be used to recommend configurable products. The complexity of such algorithms was further
analyzed.
So far, the suggestive examples only aim at providing a diverse set of items without analyzing more deeply whether variety actually helps users make better decisions. One exception is
the compromise-driven diversity generation strategy by McSherry (2003) who proposes to suggest
467

V IAPPIANI , FALTINGS , & P U

items which are representative of all possible compromises the user might be prepared to consider.
As Pu and Li (2005) pointed out, tradeoff reasoning (making compromises) can increase decision accuracy, which indicates that the compromise-driven diversity might have a high potential to
achieve better decision quality for users. However, no empirical studies have been carried out to
prove this.
1.2 Contribution of Our Work
We consider a mixed-initiative framework with an explicit preference model, consisting of an iterative process of showing examples, eliciting critiques and refining the preference model. Users
are never forced to answer questions about preferences they do not yet possess. On the other hand,
their preferences are volunteered and constructed, not directly asked. This is the key difference
between navigation-by-proposing used in the mixed-initiative user interaction model as opposed to
value assessment-by-asking used in traditional decision support systems.
With a set of simulated and real-user involved experiments, we argue that including diverse
suggestions among the examples shown by a mixed initiative based product recommender is a
significant improvement in the state-of-the-art in this field. More specifically, we show that the
model-based suggestion techniques that we have developed indeed motivate users to express more
preferences and help them achieve a much higher level of decision accuracy without additional
effort.
The rest of this article is organized as follows. We first describe a set of model-based techniques
for generating suggestions in preference-based search. The novelty of our method includes: 1) it
expands a user’s current preference model, 2) it generates a set of suggestions based on an analysis
of the likelihood of the missing attributes, and 3) it displays suggested options whose attractiveness
stimulates users’ preference expression. To validate our theory, we then examine how suggestion
techniques help users identify their target choice in both simulation environments and with real
users. We base the evaluation of these experiments on two main criteria. Firstly, we consider the
completeness of a user’s preference model as measured by preference enumeration, i.e., the number
of features for which a user has stated preferences. The higher the enumeration, the more likely a
user has considered all aspects of a decision goal, and therefore the decision is more likely to be
rational. Secondly, we consider decision accuracy as measured by the contrary of the switching rate,
which is the number of users who did not find their target option using the tool and choose another
product after reviewing all options in detail. The smaller the switching rate, the more likely a user
is content with what she has chosen using the tool, and thus the higher decision accuracy.
The success of the suggestion techniques is confirmed by experimental evaluations. An online
evaluation was performed with real users exploring a student housing database. A supervised user
study was additionally carried out with 40 users, performed in a within-subject experiment setup
that evaluated the quantitative benefits of model-based suggestion. The results demonstrate that
model-based suggestion increased decision accuracy by up to 78%, while the user’s effort is about
the same as using the example-critiquing search tool without suggestions. Such user studies which
consider the particular criteria of accuracy vs. effort have never been carried out by other researchers
for validating suggestion strategies or optimal elicitation procedures.
Finally, we end by reviewing related works followed by a conclusion.
468

P REFERENCE - BASED S EARCH USING E XAMPLE -C RITIQUING WITH S UGGESTIONS

Initial
preferences

User revises the
preference model by
critiquing examples

System shows K
examples

User picks the final
choice

Figure 1: Example-critiquing interaction. The dark box is the computer’s action, the other boxes show
actions of the user.

2. Example-critiquing
In many cases, users searching for products or information are not very familiar with the available
items and their characteristics. Thus, their preferences are not well established, but constructed
while learning about the possibilities (Payne et al., 1993). To allow such construction to take place,
a search tool should ask questions with a complete and realistic context, not in an abstract way.
A good way to follow this principle is to implement an example critiquing interaction (see
Figure 1). It shows examples of available options and invites users to state their critique of these
examples. This allows users to better understand their preferences.
Example-critiquing has been proposed by numerous researchers in two main forms: systems
without and with explicit preference models:
• in systems without preference models, the user proceeds by tweaking the current best example (“I like this but cheaper”,“I like this but French cuisine”) to make it fit with his or her
preferences better. The preference model is represented implicitly by the currently chosen
example and the interaction is that of navigation-by-proposing. Examples of such systems
are the FindMe systems (Burke, Hammond, & Young, 1997; Burke, 2002a), the ExpertClerk
system (Shimazu, 2001), and the dynamic critiquing systems (Reilly, McCarthy, McGinty, &
Smyth, 2004).
• in systems with preference models, each critique is added to an explicit preference model
that is used to refine the query. Examples of systems with explicit preference models include
the ATA system (Linden et al., 1997), SmartClient (Pu & Faltings, 2000), and more recently
incremental critiquing (McCarthy, McGinty, Smyth, & Reilly, 2005).
In this article, we focus on example-critiquing with an explicit preference model for the advantage of effectively resolving users’ preference conflicts. Moreover, this approach not only helps
users make a particular choice, but also obtains an accurate preference model for future purchases
or cross-domain recommendations.
469

V IAPPIANI , FALTINGS , & P U

2.1 Example
As a simple example consider a student looking for housing. Options are characterized by the
following 4 attributes:
1. rent in Swiss Francs;
2. type of accommodation: room in a shared apartment, studio, or apartment
3. distance to the university in minutes;
4. furnished/unfurnished.
Assume that the choice is among the following options:
o1
o2
o3
o4
o5
o6
o7

rent
400
500
600
600
650
700
800

type-of-accommodation
room
room
apartment
studio
apartment
studio
apartment

distance-to-university
17
32
14
5
32
2
7

furnished
yes
yes
no
no
no
yes
no

Assume that the user initially only articulates a preference for the lowest price. She also has hidden
preferences for an unfurnished accomodation, and a distance of less than 10 minutes to the university. None of the options can satisfy all of these preferences, so the most suitable option requires the
user to make a tradeoff among her preferences. Let us assume that the tradeoffs are such that option
o4 would be the user’s most preferred option. We call this the target option.
The user may start the search with only the first preference (lowest price), and the tool would
show the k best options according to the order shown in the table. Here, let k = 1 so that only
option o1 is shown.
In an example-critiquing tool without a preference model, the user indicates a critique of the
currently shown example, and the system then searches for another example that is as similar as
possible to the current one while also satisfying the critique. In this case, the user might critique
o1 for being furnished, and the tool might then show o3 which is most similar to the unfurnished
preference. The user might add the critique that the option should be at most 10 minutes from the
university, and the system would then return o7 as the most similar option that satisfies this critique.
The user might again critique this option as being too expensive, in which case the system would
return to o3 as most similar to the preference on the ”cheaper” option. As there is no memory of
earlier critiques, the process is stuck in a cycle, and the user can never discover the target o4 .
In a tool with a preference model, the user is able to state her preference for an unfurnished
option, making o3 the best option. Next, she might add the additional preference for a distance of
less than 10 minutes to the university, ending up with o4 which is her target choice. This illustrates
how an explicit preference model ensures the convergence of the process. In fact, decision theory
shows that when all preferences have been expressed, a user will always be able to identify the
target choice. Note however that more complex scenarios might require explicit tradeoffs among
preferences to locate the right target choice (Pu & Kumar, 2004).
470

P REFERENCE - BASED S EARCH USING E XAMPLE -C RITIQUING WITH S UGGESTIONS

A popular approach to obtain a preference model is to elicit it by asking questions to the user.
However, this can lead to means objectives (Keeney, 1992) that distract from the true target choice.
As an example, the tool might first ask the user whether she prefers a room, a studio or an apartment.
If the user truly has no preference, she might try to translate her preference for an unfurnished option
into a preference for an apartment, since this is most likely to be unfurnished. However, this is not
her true preference and will shift the best tradeoff from o4 to o3 or even o7 . This illustrates the
importance of a mixed-initiative approach where the user can state preferences in any order on her
own initiative.
The example-critiquing framework raises issues of how to model preferences, how to generate
the solutions shown to the user, and how to efficiently implement the process. We now briefly
summarize the results of our previous work addressing these issues.
2.2 Preference Modeling
When a tool forces users to formulate preferences using particular attributes or a particular order,
they can fall prey to means objectives (Keeney, 1992) because they do not have the catalog knowledge to relate this to their true intentions. Means objectives are objectives that a person believes to
correlate positively to the true objectives. For example, a manufacturer with a reputation for good
quality may become an objective when it is impossible to state an objective on the quality itself.
To avoid such means objectives, we require a preference model that allows users to state preferences incrementally using any attribute, in any order they wish. Furthermore, the preference model
must be easy to revise at each critiquing cycle by adding or removing preferences.
This rules out commonly used techniques such as question-answer dialogues or selection of a
fixed set of preferences that are commonly used on the web today.
An effective formalism that satisfies these criteria is to formulate preferences using soft constraints. A soft constraint is a function from an attribute or a combination of attributes to a number
that indicates the degree to which the constraint is violated. More generally, the values of a soft
constraint can be elements of a semiring (Bistarelli, Montanari, & Rossi, 1997). When there are
several soft constraints, they are combined into a single preference measure. Examples of combination operators are summing or taking the maximum. The overall preference order of outcomes is
then given by this combined measure.
For example, for an attribute that can take values a, b and c, a soft constraint indicating a
preference for value b could map a and c to 1, and b to 0, thus indicating that only b does not
violate the preference. A preference for the surface area to be at least 30 square meters, where a
small violation of up to 5 square meters could be acceptable, can be expressed by a piecewise linear
function:
1
0.2(30 − x)
0

if x < 25
if 25 ≤ x ≤ 30
if x > 30

In example-critiquing, each critique can be expressed as a soft constraint, and the preference
model is incrementally constructed by simply collecting the critiques. Note that it is also possible
for a user to express several preferences involving the same attributes, for example to express in one
soft constraint that the surface area should be at least 30 square meters (as above), and in another
471

V IAPPIANI , FALTINGS , & P U

soft constraint that it should be no more than 50 square meters. If the soft constraints are combined
by summing their effects, this result leads to a piecewise linear function:
1

if x < 25

0.2(30 − x)

if 25 ≤ x ≤ 30

0

if 30 < x < 50

0.2(x − 50)

if 50 ≤ x ≤ 55

1

if x > 55

Thus, soft constraints allow users to express relatively complex preferences in an intuitive manner.
This makes soft constraints a useful model for example-critiquing preference models. Furthermore,
there exist numerous algorithms that combine branch-and-bound with constraint consistency techniques to efficiently find the most preferred options in the combined order. More details on how to
use soft constraints for preference models are provided by Pu & Faltings (2004).
However soft constraints are a technique that allows a user to partially and incrementally specify
her preferences. The advantage over utility functions is that it is not necessary to elicit a user’s
preference for every attribute. Only attributes whose values concern the current decision context
are elicited. For example, if a user is not interested in a certain brand of notebooks, then she does
not have to concern herself with stating preferences on those products. This parsimonious approach
is similar to the adaptive elicitation method proposed by Chajewska et al. (2000). However, in
example-critiquing for preference-based search, user’s preferences are volunteered as reactions to
the displayed examples, not elicited; users are never forced to answer questions about preferences
without the benefit of a concrete decision context.
2.3 Generating Candidate Choices
In general, users are not able to state each of their preferences with numerical precision. Instead, a
practical tool needs to use an approximate preference model where users can specify their preferences in a qualitative way.
A good way to implement such a preference model is to use standardized soft constraints where
numerical parameters are chosen to fit most users. Such models will necessarily be inaccurate for
certain users. However, this inaccuracy can be compensated by showing not just one, but a set
of k best candidate solutions. The user then chooses the most preferred one from this set, thus
compensating for the preference model’s inaccuracy. This technique is commonly used in most
search engines.
We have analyzed this technique for several types of preference models: weighted soft constraints, fuzzy-lexicographic soft constraints, and simple dominance relations (Faltings, Torrens, &
Pu, 2004).
A remarkable result is that for both weighted and fuzzy-lexicographic constraint models, assuming a bound on the possible error (deviation between true value and the one used by the application)
of the soft constraints modeling the preferences, the probability that the true most preferred solution
is within k depends only on the number of the preferences and the error bound of the soft constraints
but not on the overall size of the solution set. Thus, it is particularly suitable when searching a very
large space of items.
We also found that if the preference model contains many different soft constraints, the probability of finding the most preferred option among the k best quickly decreases. Thus, compensating
472

P REFERENCE - BASED S EARCH USING E XAMPLE -C RITIQUING WITH S UGGESTIONS

model inaccuracy by showing many solutions is only useful when preference models are relatively
simple. Fortunately, this is often the case in preference-based search, where people usually lack the
patience to input complex models.
As a result, the most desirable process in practice might be a two-stage process where examplecritiquing with a preference model is used in the first stage to narrow down the set of options from
a large (thousands) space of possibilities to a small (20) most promising subset. The second phase
would use a tweaking interaction where no preference model is maintained to find the best choice.
Pu and Chen (2005) have shown tradeoff strategies in a tweaking interaction that provide excellent
decision accuracy even when user preferences are very complex.
2.4 Practical Implementation
Another challenge for implementing example-critiquing in large scale practical settings is that it
requires solutions to be computed specifically for the preference model of a particular user. This
may be a challenge for web sites with many users.
However, it has been shown (Torrens, Weigel, & Faltings, 1998; Torrens, Faltings, & Pu, 2002)
that the computation and data necessary for computing solutions can be coded in very compact form
and run as an applet on the user’s computer. This allows a completely scaleable architecture where
the load for the central servers is no higher than for a conventional web site. Torrens, Faltings &
Pu (2002) describe an implementation of example-critiquing using this architecture in a tool for
planning travel arrangements. It has been commercialized as part of a tool for business travelers (Pu
& Faltings, 2000).

3. Suggestions
In the basic example-critiquing cycle, we can expect users to state any additional preference as long
as they perceive it to bring a better solution. The process ends when users can no longer see potential
improvements by stating additional preferences and have thus reached an optimum. However, since
the process is one of hill-climbing, this optimum may only be a local optimum. Consider again
the example of a user looking for a notebook computer with a low price range. Since all of the
presented products have about the same weight, say around 3 kg, she might never bother to look for
lighter products. In marketing science literature, this is called the anchoring effect (Tversky, 1974).
Buyers are likely to make comparisons of products against a reference product, in this case the
set of displayed heavy products. Therefore, a buyer might not consider the possibility of a lighter
notebook that might fit her requirements better, and accept a sub-optimal result.
Just as in hillclimbing, such local minima can be avoided by randomizing the search process.
Consequently, several authors have proposed including additional examples selected in order to
educate the user about other opportunities present in the choice of options (Linden et al., 1997;
Shimazu, 2001; McSherry, 2002; Smyth & McClave, 2001). Thus, the displayed examples would
include:
• candidate examples that are optimal for the current preference query, and
• suggested examples that are chosen to stimulate the expression of preferences.
473

V IAPPIANI , FALTINGS , & P U

Different strategies for suggestions have been proposed in literature. Linden (1997) used extreme examples, where some attribute takes an extreme value. Others use diverse examples as
suggestions (Smyth & McClave, 2001; Smyth & McGinty, 2003; Shimazu, 2001).
Consider again the example of searching for housing mentioned in the previous section. Recall
that the choice is among the following options:
o1
o2
o3
o4
o5
o6
o7

rent
400
500
600
600
650
700
800

type-of-accommodation
room
room
apartment
studio
apartment
studio
apartment

distance-to-university
17
32
14
5
32
2
7

furnished
yes
yes
no
no
no
yes
no

In the initial dialogue with the system, the user has stated the preference of lowest price. Consequently, the options are ordered o1 Â o2 Â o3 = o4 Â o5 Â o6 Â o7 .
Assume that the system shows only one candidate, which is the most promising option according
to the known preferences: o1 . What other options should be shown as suggestions to motivate the
user to express her remaining preferences?
Linden et al. (1997) proposed using extreme examples, defined as examples where some attribute
takes an extreme value. For example, consider the distance: o6 is the example with the smallest
distance. However, it has a much higher price, and being furnished does not satisfy the user’s other
hidden preference. Thus, it does not give the user the impression that a closer distance is achievable
without compromising her other preferences. Only when the user wants a distance of less than 5
minutes can option o6 be a good suggestion, otherwise o4 is likely to be better. Another problem
with extreme examples is that we need two such examples for each attribute, which is usually more
than the user can absorb.
Another strategy (Smyth & McClave, 2001; McSherry, 2002, 2003; Smyth & McGinty, 2003;
Shimazu, 2001) is to select suggestions to achieve a certain diversity, while also observing a certain
goodness according to currently known preferences. As the tool already shows o1 as the optimal example, the most different example is o5 , which differs in all attributes but does not have an excessive
price. So is o5 a good suggestion? It shows the user the following opportunities:
• apartment instead of room: however, o3 would be a cheaper way to achieve this.
• distance of 32 instead of 17 minutes: however, o2 would be a cheaper way to achieve this.
• unfurnished instead of furnished: however, o3 would be a cheaper way to achieve this.
Thus, while o5 is very diverse, it does not give the user an accurate picture of what the true opportunities are. The problem is that diversity does not consider the already known preferences, in
this case price, and the dominance relations they imply on the available options. While this can be
mitigated somewhat by combining diversity with similarity measures, for example by using a linear
combination of both (Smyth & McClave, 2001; McSherry, 2003), this does not solve the problem
as the effects of diversity should be limited to attributes without known preferences while similarity
should only be applied to attributes with known preferences.
474

P REFERENCE - BASED S EARCH USING E XAMPLE -C RITIQUING WITH S UGGESTIONS

We now consider strategies for generating suggestions based on the current preference model.
We call such strategies model-based suggestion strategies.
We assume that the user is minimizing his or her own effort and will add preferences to the
model only when he or she expects them to have an impact on the solutions. This is the case when:
• the user can see several options that differ in a possible preference, and
• these options are relevant, i.e. they could be acceptable choices, and
• they are not already optimal for the already stated preferences.
In all other cases, stating an additional preference is irrelevant: when all options would evaluate the
same way, or when the preference only has an effect on options that would not be eligible anyway
or that are already the best choices, stating it would be wasted effort. On the contrary, upon display
of a suggested outcome whose optimality becomes clear only if a particular preference is stated, the
user can recognize the importance of stating that preference. This seems to be confirmed by our
user studies.
This has led us to the following principle, which we call the look-ahead principle, as a basis for
model-based suggestion strategies:
Suggestions should not be optimal under the current preference model, but should provide a high likelihood of optimality when an additional preference is added.
We stress that this is a heuristic principle based on assumptions about human behavior that we
cannot formally prove. However, it is justified by the fact that suggestion strategies based on the
look-ahead principle work very well in real user studies, as we report later in this article.
In the example, o4 and o3 have the highest probability of satisfying the lookahead principle:
both are currently dominated by o1 . o4 becomes Pareto-optimal when the user wants a studio, an
unfurnished option, or a distance of less than 14 minutes. o3 becomes Pareto-optimal when the user
wants an apartment, an unfurnished option, or a distance of less than 17 minutes. Thus, they give a
good illustration of what is possible within the set of examples.
We now develop our method for computing suggestions and show that how it can generate these
suggestions.
3.1 Assumptions about the Preference Model
To further show how to implement model-based suggestion strategies, we have to define preference
models and some minimal assumptions about the shape that user preferences might take. We stress
that these assumptions are only made for generating suggestions. The preference model used in the
search tool could be more diverse or more specific as required by the application.
We consider a collection of options O = {o1 , .., on } and a fixed set of k attributes A =
{A1 , .., Ak }, associated with domains D1 , .., Dn . Each option o is characterized by the values
a1 (o), ..., ak (o); where ai (o) represents the value that o takes for attribute Ai .
A qualitative domain (the color, the name of neighborhood) consists in an enumerated set of
possibilities; a numeric domain has numerical values (as price, distance to center), either discrete
or continuous. For numeric domains, we consider a function range(Att) that gives the range on
which the attribute domain is defined. For simplicity we call qualitative (respectively numeric)
attributes those with qualitative (numeric) domains.
The user’s preferences are assumed to be independent and defined on individual attributes:
475

V IAPPIANI , FALTINGS , & P U

Definition 1 A preference r is an order relation ¹r of the values of an attribute a; ∼r expresses
that two values are equally preferred. A preference model R is a set of preferences {r1 , .., rm }.
Note that ¹r might be a partial or total order.
If there can be preferences over a combination of attributes, such as the total travel time in a
journey, we assume that the model includes additional attributes that model these combinations so
that we can make the assumption of independent preferences on each attribute. The drawback is
that the designer has to know the preferential dependence in advance. However, this is required for
designing the user interface anyway.
As a preference ri always applies to the same attribute ai , we simplify the notation and apply
¹ri and ∼ri to the options directly: o1 ≺ri o2 iff ai (o1 ) ≺ri ai (o2 ). We use ≺ri to indicate that ¹ri
holds but not ∼ri .
Depending on the formalism used for modeling preferences, there are different ways of combining the order relations given by the individual preferences ri in the user’s preference model R
into a combined order of the options. For example, each preference may be expressed by a number,
and the combination may be formed by summing the numbers corresponding to each preference or
by taking their minimum or maximum.
Any rational decision maker will prefer an option to another if the first is at least as good in all
criteria and better for at least one. This concept is expressed by the Pareto-dominance (also just
called dominance), that is a partial order relation of the options.
Definition 2 An option o is Pareto-dominated by an option o0 with respect to R if and only if for all
ri ∈ R, o ¹ri o0 and for at least one rj ∈ R, o ≺rj o0 . We write o ≺R o0 (equivalently we can say
that o0 Pareto-dominates o and write o0 ÂR o).
We also say that o is dominated (without specifying o0 ).
Note that we use the same symbol ≺ for both individual preferences and sets of preferences.
We will do the same with ∼, meaning that o ∼R o0 if ∀r ∈ R, o ∼r o0 .
In the following, the only assumption we make about this combination is that it is dominancepreserving according to this definition of Pareto-dominance. Pareto dominance is the most general
order relation that can be defined based on the individual preferences. Other forms of domination
can be defined as extensions of Pareto dominance. In the following, whenever we use “dominance”
without further specification, we refer to Pareto-dominance.
Definition 3 A preference combination function is dominance-preserving if and only if whenever an
option o’ dominates another option o in all individual orders, then o’ dominates o in the combined
order.
Most of the combination functions used in practice are dominance-preserving. An example of
a combination that is not dominance-preserving is the case where the preferences are represented
as soft constraints and combined using Min(), as in fuzzy CSP (Ruttkay, 1994). In this case, two
options with the constraint valuations
o1 (0.3, 0.5, 0.7)
o2 (0.3, 0.4, 0.4)
will be considered equally preferred by the combination function as M in(0.3, 0.5, 0.7) = 0.3 =
M in(0.3, 0.4, 0.4), even though o1 is dominated by o2 .
476

P REFERENCE - BASED S EARCH USING E XAMPLE -C RITIQUING WITH S UGGESTIONS

3.2 Qualitative Notions of Optimality
The model-based suggestion strategies we are going to introduce are based on the principle of selecting options that have the highest chance of becoming optimal. This is determined by considering
possible new preferences and characterizing the likelihood that they make the option optimal. Since
we do not know the weight that a new preference will take in the user’s perception, we must evaluate
this using a qualitative notion of optimality. We present two qualitative notions, one based only on
Pareto-optimality and another based on the combination function used for generating the candidate
solutions.
We can obtain suggestion strategies that are valid with any preference modeling formalism,
using qualitative optimality criteria based on the concept of Pareto-dominance introduced before.
Definition 4 An option o is Pareto-optimal (PO) if and only if it is not dominated by any other
option.
Since dominance is a partial order, Pareto optimal options can be seen as the maximal elements
of O. Pareto-optimality is useful because it applies to any preference model as long as the combination function is dominance-preserving.
For any dominance-preserving combination function, an option o∗ that is most preferred in the
combined preference order is Pareto-optimal, since any option o0 that dominates it would be more
preferred. Therefore, only Pareto-optimal solutions can be optimal in the combined preference
order, no matter what the combination function is. This makes Pareto-optimality a useful heuristic
for generating suggestions independently of the true preference combination in the user’s mind.
In example-critiquing, users initially state only a subset R of their eventual preference model
R. When a preference is added, dominated options with respect to R can become Pareto-optimal.
On the other hand, no option can loose its Pareto-optimality when preferences are added except
that an option that was equally preferred with respect to all the preferences considered can become
dominated.
Note that one can also consider this as using weak Pareto-optimality as defined by Chomicki
(2003), as we consider that all options are equal with respect to attributes where no preference has
been stated.
We now introduce the notions of dominating set and equal set:
Definition 5 The dominating set of an option o with respect to a set of preferences R is the set of
>
all options that dominate o: OR
(o) = {o0 ∈ O : o0 ÂR o}. We write O> (o), without specifying R,
the set of preferences, if R is clear from the context.
The equal set of an option o with respect to R is the set of options that are equally preferred to
= (o) = {o0 ∈ O : o0 ∼ o}. We also use O ≥ for O > ∪ O = .
o: OR
R
The following observation is the basis for evaluating the likelihood that a dominated option will
become Pareto-optimal when a new preference ri is stated.
Proposition 1 A dominated option o with respect to R becomes Pareto-optimal with respect to
R ∪ ri if and only if o is
• strictly better with respect to ri than all options that dominate it with respect to R and
• not worse with respect to ri than all options that are equally preferred with respect to R.
477

V IAPPIANI , FALTINGS , & P U

Proof 1 Suppose there was an option o0 that dominates o with respect to R and that o is not strictly
better than o0 in the new preference ri ; then o0 would still dominate o, so o could not be Paretooptimal. Similarly, suppose that o is equally preferred to o00 and o00 is strictly better than o with
respect to ri ; then o00 would dominate o, so o could not be Pareto-optimal.
Thus, the dominating set O> and the equal set O= of a given option are the potential dominators
when a new preference is considered.
Utility-dominance We can consider other forms of dominance as long as they imply Paretodominance. In particular, we might use the total order established by the combination function defined in the preference modeling formalism, such as a weighted sum. We call this utility-domination,
and the utility-optimal option is the most preferred one.
We may ask when an option can become utility-optimal. A weaker form of Proposition 1 holds
for utility domination:
Proposition 2 For dominance-preserving combination functions, a utility-dominated option o0 with
respect to R may become utility-optimal with respect to R ∪ ri only if o0 is strictly better with
respect to ri than all options that currently utility-dominate it and not worse than all options that
are currently equally preferred.
Proof 2 Suppose there was an option that became utility-optimal without being more preferred
according to the new preference; then there would be a violation of the assumption that the combination function was dominance-preserving.
Even though it is not a sufficient condition, Proposition 2 can be used as a heuristic to characterize
an option’s chance to become utility-optimal.
3.3 Model-based Suggestion Strategies
We propose model-based suggestion strategies that can be implemented both with the concept of
Pareto- and utility-dominance. They are based on the look-ahead principle discussed earlier:
suggestions should not be optimal under the current preference model, but have a high
likelihood of becoming optimal when an additional preference is added.
We assume that the system knows a subset R of the user’s preference model R. An ideal suggestion
is an option that is optimal with respect to the full preference model R but is dominated in R,
the current (partial) preference model. To be optimal in the full model, from Propositions 1 and 2
we know that such suggestions have to break the dominance relations with their dominating set.
Model-based strategies order possible suggestions by the likelihood of breaking those dominance
relations.
3.3.1 C OUNTING S TRATEGY
The first suggestion strategy, the counting strategy, is based on the assumption that dominating
options are independently distributed. From Proposition 1 we can compute the probability that a
dominated option o becomes Pareto-optimal through a currently hidden preference as:
478

P REFERENCE - BASED S EARCH USING E XAMPLE -C RITIQUING WITH S UGGESTIONS

Y

popt (o) =

0

pd (o, o )

0

Y

pnw (o, o0 )

o0 ∈O= (o)

o ∈O> (o)

where pd is the probability that a new preference makes o escape the domination relation with a
dominating option o0 , i.e. if o is preferred over o0 according to the new preference; similarly pnw is
the probability that o is not worse than a equally preferred option o0 .
Evaluating this probability requires the exact probability distribution of the possible preferences,
which is in general difficult to obtain.
The strategy assumes that pd = pnw is constant for all dominance relations.
Y
popt (o) =
pd
o0 ∈O≥ (o)
|O≥ (o)|

= pd

Since pd ≤ 1, this probability is largest for the smallest set O≥ (o). Consequently, the best
suggestions are those with the lowest value of the following counting metric:
FC (o) = |O≥ (o)|

(1)

The counting strategy selects the option with the lowest value of this metric as the best suggestion.
3.3.2 P ROBABILISTIC S TRATEGY
The probabilistic strategy uses a more precise estimate of the chance that a particular solution will
become Pareto-optimal.
General assumptions We assume that each preference ri is expressed by a cost function ci . In
order to have a well-defined interface, these cost functions will usually be restricted to a family of
functions parameterized by one or more parameters. Here we assume a single parameter θ, but the
method can be generalized to handle cases of multiple parameters:
ci = ci (θ, ai (o)) = ci (θ, o)
We assume that the possible preferences are characterized by the following probability distributions:
• pai , the probability that the user has a preference over an attribute ai ,
• p(θ), the probability distribution of the parameter associated with the cost function of the
considered attribute
In the user experiments in the last section, we use a uniform distribution for both. The probability that a preference on attribute i makes o1 be preferred to o2 can be computed integrating over the
values of θ for which the cost of o1 is less than o2 . This can be expressed using the Heavyside step
function H(x) ≡ if (x > 0) then 1 else 0:
479

V IAPPIANI , FALTINGS , & P U

Z
δi (o1 , o2 ) =

θ

H(ci (θ, o2 ) − ci (θ, o1 ))p(θ)dθ

For a qualitative domain, we iterate over θ and sum up the probability contribution of the cases in
which the value of θ makes o1 preferred over o2 :
δi (o1 , o2 ) =

X

H(ci (θ, o2 ) − ci (θ, o1 ))p(θ)

θ∈Di

To determine the probability of simultaneously breaking the dominance relation with all dominating or equal options in O≥ , aQ
first possibility is to assume independence between the options,
and thus calculate δi (o, O≥ ) = o0 ∈O≥ δi (o, o0 ), where δi is the chance of breaking one single
domination when the preference is on attribute i.
A better estimate can be defined that does not require the independence assumption, and directly
considers the distribution of all the dominating options. For breaking the dominance relation with
all the options in the dominating set through ai , all dominating options must have a less preferred
value for ai than that of the considered option.
For numeric domains, we have to integrate over all possible values of θ, check whether the given
option o has lower cost than all its dominators in O> and weigh the probability of that particular
value of θ.
Z Y
δi (o, O> ) = [
H(ci (θ, o0 ) − ci (θ, o))]p(θ)dθ
o0 ∈O>

For qualitative domains, we replace the integral with a summation over θ.
We also need to consider the second condition of Proposition 1, namely that no new dominance
relations with options in the equal set should be created. This can be done by adding a second term
into the integral:
Z
≥

δi (o, O ) =

[

Y

H(ci (θ, o0 ) − ci (θ, o))

Y

H ∗ (ci (θ, o00 ) − ci (θ, o))]p(θ)dθ

(2)

o00 ∈O=

o0 ∈O>

where H ∗ is a modified Heavyside function that assigns value 1 whenever the difference of the
two costs is 0 or greater. (H ∗ (x) ≡ if (x ≥ 0) then 1 else 0).
We consider the overall probability of becoming Pareto optimal when a preference is added as
the combination of the event that the new preference is on a particular attribute, and the chance that
a preference on this attribute will make the option be preferred over all values of the dominating
options:
Y
FP (o) = 1 −
(1 − Pai δi (o, O≥ ))
(3)
ai ∈Au

If we assume that the user has only one hidden preference, we can use the following simplification:
X
FP (o) =
Pai δi (o, O≥ )
(4)
ai ∈Au

480

P REFERENCE - BASED S EARCH USING E XAMPLE -C RITIQUING WITH S UGGESTIONS

which is also a good approximation when the probabilities for additional preferences are small. In
both cases, we select the options with the highest values as suggestions.
The computation depends on the particular choice of preference representation and in many
cases it can be greatly simplified by exploiting properties of the cost functions. In general, the
designer of the application has to consider what preferences the user can express through the user
interface and how to translate them into quantitative cost functions. A similar approach is taken by
Kiessling (2002) in the design of PREFERENCE SQL, a database system for processing queries
with preferences.
We now consider several examples of common preference functions and show how the the
suggestions can be computed for these cases.
Preference for a single value in a qualitative domain Let θ be the value preferred by the user;
the function ci (θ, x) gives a penalty to every value for attribute ai except θ . This would allow to
express statements like “I prefer German cars”, meaning that cars manufactured in Germany are
preferred to cars manufactured in another country.
ci (θ, x) ≡ if ai (x) = θ then 0 else 1.
The probability of breaking a dominance relation between option o1 and o2 simplifies to the
probability that the value of option o1 for attribute i is the preferred value, when it differs from the
value of o2 .
½
p[θ = ai (o1 )] if ai (o1 ) 6= ai (o2 )
δi (o1 , o2 ) =
0
otherwise
1
for any θ (meaning that any value of the
|Di |
domain is equally likely to be the preferred value), the probability becomes 1/|Di | when ai (o1 ) 6=
ai (o2 ), and 0 otherwise.
The probability of breaking all dominance relations with a set of dominators without creating
new dominance relations is the same as that for a single dominator, as long as all these options have
a different value for ai :
½
1/|Di | if (∀o0 ∈ O> ) ai (o) 6= ai (o0 )
δi (o, O≥ )
(5)
0
otherwise
Assuming a uniform distribution, p(θ) =

Note that, given the structure of the preference, δi (o, O≥ ) = δi (o, O> ), because an option o
can only break the dominance relations if ai (o) takes the preferred value and in that case, no other
option can be strictly better with respect to that preference.
Directional preferences A particular case of preferences in numeric domains is when the preference order can be assumed to have a known direction, such as for price (cheaper is always preferred,
everything else being equal). In this case, δ(o1 , o2 ) can be computed by simply comparing the values that the options take on that attribute (Figure 2).
½
δi (o1 , o2 )

if ai (o1 ) < ai (o2 ) then 1 else 0
if ai (o1 ) > ai (o2 ) then 1 else 0
481

ai numeric, natural preference <
ai numeric, natural preference >

(6)

V IAPPIANI , FALTINGS , & P U

Figure 2: In a directional preference, the cost function is a monotone function of the attribute value. In the
case shown here, smaller values are preferred.

Figure 3: When the preference LessThan(θ) is represented by a step function, an option is preferred over
a set of options with minimum value li if the reference value θ falls in between the values of the
given option and li .

For a set of options O≥ whose values on ai lie between li and hi we have
½
≥

δi (o, O )

1 if ai (o) < li
0 otherwise

(7)

when smaller values are always preferred, and
½
≥

δi (o, O )

1 if ai (o) > hi
0 otherwise

(8)

when larger values are always preferred. Note that in both cases, the expressions are independent
of the shape of the cost function as long as it is monotonic.
Threshold preferences in numeric domains Another commonly used preference expression in
numeric domains is to define a smallest or largest acceptable threshold, i.e. to express a preference
LessThan(θ) (the value should be lower than θ) or GreaterThan(θ) (the value should be
greater than θ). Such a preference is most straightforwardly expressed by a cost function that follows
482

P REFERENCE - BASED S EARCH USING E XAMPLE -C RITIQUING WITH S UGGESTIONS

Figure 4: When the preference LessThan(θ) is represented by a graded step function, an option is preferred over a set of options with minimum value li if the reference value θ falls in the interval
between ai (o) − t and li , where t = 1/α.

a step curve (Figure 3). To express the fact that there is usually some tolerance for small violations,
more generally a graded step function, where the cost gradually increases, might be used (Figure 4).
A possible cost function for LessThan might be the following:
½
Min(1, α ∗ (x − θ)) if x > θ
cless−than (θ, x) =
(9)
0
otherwise
assigning a penalty when the option takes a value greater than the reference value θ; such cost
is the difference between the value and the reference, up to a maximum of 1. α is a parameter that
expresses the degree to which the violations can be allowed; for the following computations it is
convenient to use the length of the ramp from 0 to 1 t = 1/α.
In this case the computation of δ(o1 , o2 ) will be, if ai (o1 ) < ai (o2 ):
Z ai (o2 )
δi (o1 , o2 ) =
1p(θ)dθ = p[(ai (o1 ) − t) < θ < ai (o2 )];
ai (o1 )−t

and 0 otherwise (since lower values are preferred in Equation 9).
When the transition phase from 0 to 1 is small (the cost function approximates a step function as
in Figure 3), δi (o1 , o2 ) ' p[ai (o1 )−t < θ < ai (o2 )], approximating the probability of the reference
point falling between the two options. Assuming uniform distribution, the probability evaluates to
(ai (o2 )−ai (o1 )+t)/range(ai ), where range(ai ) is that difference between the largest and smallest
values of ai . The reasoning is illustrated by Figure 4.
The probability computed is conditioned on the knowledge of the polarity of the user’s preference (LessThan in this case), and needs to be weighted by the probability of that polarity. Below,
we assume that both polarities are equally likely, and use a weight of 1/2.
All the dominance relations can be broken simultaneously only if the considered option has a
value for that attribute that is smaller or bigger than that of all the options in the dominating set.
To estimate the probability that the reference value for the new preference falls in such a way that
all the dominance relations are broken, it is sufficient to consider the extrema of the values that the
dominating options take on the considered attribute:
• hi = maxo0 ∈O> ai (o0 )
483

V IAPPIANI , FALTINGS , & P U

Figure 5: An example of peaked preferences. gi is the greatest value below ai (o) of ai for any option in

O≥ (o), si is the smallest value above ai (o). m1 = (ai (o) + gi )/2, m2 = (ai (o) + si )/2 are the
two midpoints between ai (o) and gi , si . To make o be preferred over all options in O≥ (o), θ has
to fall between max(m1 , ai (o) − t) and min(m2 , ai (o) + t). As it can be seen graphically, in this
case the interval is ]m1 , ai (o) + t[.

• li = mino0 ∈O> ai (o0 )
If the values for the current option lies outside the interval [li , hi ], we can consider the probability of breaking all the relations as in the single dominance case. It will be proportional to the
difference between the current option value and the minimum/maximum, scaled by the range of
values for ai :

(ai (o1 ) − hi + t)/2 ∗ range(ai ) if ai (o1 ) > hi





(li − ai (o1 ) + t)/2 ∗ range(ai ) if ai (o1 ) < li
δi (o, O≥ )





0
otherwise

(10)

Peaked preferences for numeric domains Another common case is to have preferences for a particular numerical value θ, for example “I prefer to arrive around 12am”. To allow some tolerance
for deviation, a cost function might have a slope in both directions:
cpeak (x, θ) = α ∗ |ai (o) − θ|.
In this case, an option is preferred to another one if it is closer to θ. For example, letting m be
the midpoint between ai (o1 ) and ai (o2 ) and supposing ai (o1 ) < ai (o2 ), we have
δ(o1 , o2 ) = p[θ < m]
For calculating the probability of simultaneously breaking all the dominance relations without
generating new ones, we define gi as the maximum of all dominating or equal options with a value
for ai less than ai (o) and si as the minimum value of all dominating or equal options greater than
ai (o). As option o is more preferred whenever ai (o) is closer to θ, and the interval for θ where this
is the case is one half the interval between si and gi , we have:
δ(o, O≥ ) =

si − gi
range(ai )

484

P REFERENCE - BASED S EARCH USING E XAMPLE -C RITIQUING WITH S UGGESTIONS

A more realistic cost function would include a “saturation point” from which the cost always
evaluates to 1, as shown in Figure 5:
cpeak−with−saturation (x, θ) = Min(1, α ∗ |ai (o) − θ|).

(11)

Let t = 1/α be the tolerance of the preference to either side, gi be the greatest value below ai (o)
of ai for any option in O≥ (o), and si be the smallest value above ai (o). We define two midpoints
m1 = (ai (o) + gi )/2 and m2 = (ai (o) + si )/2, and we then have:
δ(o, O≥ ) = p[max(m1 , ai (o) − t) < θ < min(m2 , ai (o) + t)]
If the reference point is uniformly distributed, this evaluates to:
δ(o, O≥ ) =

min(m2 , ai (o) + t) − max(m1 , ai (o) − t)
range(ai )

(12)

3.4 Example
The following table shows the relevant values for the example shown earlier. Recall that we had
earlier identified o4 and o3 as the most attractive suggestions.

o1
o2
o3
o4
o5
o6
o7

O+

rent
(a1 )

type
(a2 )

δ2

distance
(a3 )

δ3

furnished
(a4 )

δ4

popt

o1
o1 , o2
o1 , o2
o1 − o4
o1 − o5
o1 − o6

400
500
600
600
650
700
800

room
room
apartment
studio
apartment
studio
apartment

0
0.5
0.5
0
0
0

17
32
14
5
32
2
7

0.25
0.05
0.20
0
0.05
0

yes
yes
no
no
no
yes
no

0
0.5
0.5
0
0
0

0.125
0.451
0.494
0
0.025
0

In the counting strategy, options are ranked according to the size of the set O+ . Thus, we have o2
as the highest ranked suggestion, followed by o3 and o4 .
In the probabilistic strategy, attribute values of an option are compared with the range of values
present in its dominators. For each attribute, this leads to the δ values as indicated in the table. If
we assume that the user is equally likely to have a preference on each attribute, with a probability
of Pai = 0.5, the probabilistic strategy scores the options as shown in the last column of the table.
Clearly, o4 is the best suggestion, followed by o3 . o2 and also o6 follow further behind.
Thus, at least in this example, the model-based strategies are successful at identifying good
suggestions.
3.5 Optimizing a Set of Several Suggestions
The strategies discussed so far only concern generating single suggestions. However, in practice it
is often possible to show a set of l suggestions simultaneously. Suggestions are interdependent, and
it is likely that we can obtain better results by choosing suggestions in a diverse way. This need for
diversity has also been observed by others (Shimazu, 2001; Smyth & McClave, 2001).
More precisely, we should choose a group G of suggested options by maximizing the probability
popt (G) that at least one of the suggestions in the set G will become optimal through a new user
preference:
Y
Y
popt (G) = 1 −
(1 − Pai (1 −
(1 − δi (o0 , O≥ (o0 )))))
(13)
o0 ∈G

ai ∈Au

485

V IAPPIANI , FALTINGS , & P U

Explicitly optimizing this measure would lead to combinatorial complexity. Thus, we use an
algorithm that adds suggestions one by one in the order of their contribution to this measure given
the already chosen suggestions. This is similar to the algorithm used by Smyth and McClave (2001)
and by Hebrard et al. (2005) to generate diverse solutions.
The algorithm first chooses the best single suggestion as the first element of the set G. It then
evaluates each option o as to how much it would change the combined measure popt (G) if it were
added to the current G, and adds the option with the largest increment. This process repeats until
the desired size of set G is reached.
3.6 Complexity
Let n be the number of options, k the number of attributes and m the number of preferences, d the
number of dominators, Au the attributes on which the user did not state any preference.
All three model-based strategies are based on the dominating set of an option. We use a straightforward algorithm that computes this as the intersection of the set of options that are better with
respect to individual preferences. There are m such sets, each with at most n elements, so the complexity of this algorithm is O(n2 m). In general, the dominating set of each option is of size O(n)
so that the output of this procedure is of size O(n2 ), so it is unlikely that we can find a much better
algorithm.
Once the dominating sets are known, the counting strategy has complexity O(nd), while the
attribute and probabilistic strategies have complexity O(ndku ), where ku = |Au | and ku < k. In
general, d depends on the data-set. In the worst case it can be proportional to n, so the resulting
complexity is O(n2 ).
When utility is used as a domination criterion, the dominating set is composed by the options
that are higher in the ranking. Therefore the process of computing the dominating set is highly
simplified and can be performed while computing the candidates. However the algorithm still has
overall worst case complexity O(n2 ): the the last option in the ranking has n − 1 dominators, and
so d = O(n).
When several examples are selected according to their diversity, the complexity increases since
the metrics must be recomputed after selecting each suggestion.
In comparison, consider the extreme strategy, proposed initially by Linden et al. in ATA (1997).
It selects options that have either the smallest or the largest value for an attribute on which the
user did not initially state any preference. This strategy needs to scan through all available options
once. Its complexity is O(n), where n is the number of options (the size of the catalog). Thus, it
is significantly more efficient, but does not appear to provide the same benefits as a model-based
strategy, as we shall see in the experiments.
Another strategy considered for comparison, that of generating a maximally diverse set of options (Hebrard et al., 2005; Smyth & McClave, 2001), has an exponential complexity for the number
of available options. However, greedy approximations (Hebrard et al., 2005) have a complexity of
only O(n2 ) , similar to our model-based strategies.
The greedy algorithm we use for optimizing a set of several suggestions does not add to the
complexity; once the distances δi have been computed for each attribute, the greedy algorithm for
computing the set of suggestions has a complexity proportional to the product of the number of
options, the number of attributes, and the square of the number of suggestions to be computed. We
486

P REFERENCE - BASED S EARCH USING E XAMPLE -C RITIQUING WITH S UGGESTIONS

100

random
diversity
extremes
counting
probabilistic assuming independence
probabilistic assuming no independence

fraction of users

80

60

40

20

0
1

2

3
4
number of preferences

5

6

Figure 6: Simulation results on a database of actual apartment offers. For 100 simulated users, each with a
randomly chosen preference model of 6 hidden preferences, we plot the number of times that the
simulation discovered at least the number of preferences shown on the abscissa. The higher the
curve, the more preferences were discovered on average.

suspect that an exact optimization would be NP-hard in the number of suggestions, but we do not
have a proof of this.

4. Experimental Results: Simulations
The suggestion strategies we presented are heuristic, and it is not clear which of them performs best
under the assumptions underlying their design. Since evaluations with real users can only be carried
out for a specific design, we first select the best suggestion strategy by simulating the interaction
of a computer generated user with randomly generated preferences. This allows us to compare the
different techniques in much greater detail than would be possible in an actual user study, and thus
select the most promising techniques for further development. This is followed by real user studies
that are discussed in the next section.
In the simulations, users have a randomly generated set of m preferences on the different attributes of items stored in a database. As a measure of accuracy, we are interested in whether the
interaction allows the system to obtain a complete model of the user’s preferences. This tests the
design objective of the suggestion strategies (to motivate the user to express as many preferences as
possible) given that the assumptions about user behavior hold. We verify that these assumptions are
reasonable in the study with real users reported in the next section.
The simulation starts by assigning the user a set of randomly generated preferences and selecting
one of them as an initial preference. At each stage of the interaction, the simulated user is presented
with 5 suggestions.
We implemented 6 different strategies for suggestions, including the three model-based strategies described above as well as the following three strategies for comparison:
• the random strategy suggests randomly chosen options;
487

V IAPPIANI , FALTINGS , & P U

100

random
diversity
extremes
counting
probabilistic assuming independence
probabilistic assuming no independence

fraction of users

80

60

40

20

0
1

2

3

4
5
number of preferences

6

7

8

Figure 7: Simulation results for randomly generated catalogs. For 100 simulated users, each with a randomly chosen preference model of 8 hidden preferences, we plot the number of times that the
simulation discovered at least the number of preferences shown on the abscissa. The higher the
curve, the more preferences were discovered on average.

• the extremes strategy suggests options where attributes take extreme values, as proposed by
Linden (1997);
• the diversity strategy computes the 20 best solutions according to the current model and then
generates a maximally diverse set of 5 of them, following the proposal of McSherry (2002).
The simulated user behaves according to an opportunistic model by stating one of its hidden
preferences whenever the suggestions contain an option that would become optimal if that preference was added to the model with the proper weight. The interaction continues until either the
preference model is complete, or the simulated user states no further preference. Note that when the
complete preference model is discovered, the user finds the target option.
We first ran a simulation on a catalog of student accommodations with 160 options described
using 10 attributes. The simulated user was shown 5 suggestions, and had a randomly generated
model of 7 preferences, of which one is given by the user initially. The results are shown in Figure 6.
For each value of x, it shows the percentage of runs (out of 100) that discover at least x out of the 6
hidden preferences in the complete model. Using random suggestions as the baseline, we see that the
extremes strategy performs only slightly better, while diversity provides a significant improvement.
The model-based strategies give the best results, with the counting strategy being about equally
good as diversity, and the probabilistic strategies providing markedly better results.
In another test, we ran the same simulation for a catalog of 50 randomly generated options with
9 attributes, and a random preference model of 9 preferences, of which one is known initially. The
results are shown in Figure 7. We can see that there is now a much more pronounced difference
between model-based and non model-based strategies. We attribute this to the fact that attributes are
less correlated, and thus the extreme and diversity filters tend to produce solutions that are too scat488

P REFERENCE - BASED S EARCH USING E XAMPLE -C RITIQUING WITH S UGGESTIONS

#P / #A
6/6
6/9
6/12

random
0.12
0.12
0.11

extreme
0.09
0.12
0.13

diversity
0.23
0.27
0.24

counting
0.57
0.65
0.62

prob1
0.59
0.63
0.64

prob2
0.64
0.67
0.63

Table 1: The fraction of preferences that are correctly discovered as a function of the number of attributes;
keeping constant the number of preferences (6) to be discovered. All attributes have integer domains.

#P / #A
3/9
6/9
9/9

random
0.25
0.11
0.041

extreme
0.36
0.12
0.17

diversity
0.28
0.11
0.05

counting
0.70
0.67
0.66

prob1
0.71
0.68
0.70

prob2
0.71
0.68
0.73

Table 2: The fraction of preferences that are correctly discovered (on average) as a function of the number
of preferences to be discovered. All attributes have integer domains.

tered in the space of possibilities. Also the probabilistic strategy with both possible implementations
(assuming the attributes values independent or not) give very close results.
We investigated the impact of the number of preferences, the number and type of attributes, and
the size of the data set on random data sets. In the following, prob1 refers to the probabilistic strategy
with the independence assumption, prob2 to the probabilistic strategy without that assumption.
Surprisingly we discovered that varying the number of attributes only slightly changes the results. Keeping the number of preferences constant at 6 (one being the initial preference), we ran
simulations with the number of attributes equal to 6, 9 and 12. The average fraction of discovered preferences varied for each strategy and simulation scenario by no more than 5%, as shown in
Table 1.
The impact of the variation of the number of preferences to discover is shown in Table 2. All
of our model-based strategies perform significatively better than random choice, suggestions of
extrema, and maximization of diversity. This shows the importance of considering the already
known preferences when selecting suggestions.
domain
type
mixed
integer

random
choice
0.048
0.04

extreme

diversity

counting

prob1

prob2

0.30
0.17

0.18
0.05

0.81
0.66

0.87
0.70

0.86
0.72

Table 3: The fraction of preferences that are correctly discovered as a function of the different kinds of
attribute domains: integer domains against a mix of 5 integer, 2 discrete domains and 2 domains
with a natural order. We ran 100 simulations with 9 attributes and 9 preferences.

489

V IAPPIANI , FALTINGS , & P U

data
size
50
75
100
200

random
choice
0.25
0.16
0.11
0.05

extreme

diversity

counting

prob1

prob2

0.50
0.42
0.29
0.22

0.56
0.54
0.57
0.54

0.89
0.88
0.90
0.86

0.94
0.97
0.96
0.91

0.93
0.95
0.97
0.93

Table 4: The fraction of preferences that are correctly discovered as a function of the database size. We ran
100 simulations with 9 attributes and 9 preferences (mixed domains).

The performances are higher with mixed domains than with all numeric domains (Table 3). This
is easily explained by the larger outcome space in the second case.
Interestingly, as the size of the item set grows, the performance of random and extreme strategies
significantly degrades while the model-based strategies maintain about the same performance (Table 4).
In all simulations, it appears that the probabilistic suggestion strategy is the best of all, sometimes by a significant margin. We thus chose to evaluate this strategy in a real user study.

5. Experimental Results: User Study
The strategies we have developed so far depend on many assumptions about user behavior and can
only be truly tested by evaluating them on real users. However, because of the many factors that
influence user behavior, only testing very general hypotheses is possible. Here, we are interested in
verifying that:
1. using model-based suggestions leads to more complete preference models.
2. using model-based suggestions leads to more accurate decisions.
3. more complete preference models tend to give more accurate decisions, so that the reasoning
underlying the model-based suggestions is correct.
We measure decision accuracy as the percentage of users that find their most preferred choice using
the tool. The most preferred choice was determined by having the subjects go through the entire
database of offers in detail after they finished using the tool. This measure of decision accuracy,
also called the switching rate, is the commonly accepted measure in marketing science (e.g., Haubl
& Trifts, 2000).
We performed user studies using FlatFinder, a web application for finding student housing that
uses actual offers from a university database that is updated daily. This database was ideal because
it contains a high enough number - about 200 - of offers to present a real search problem, while at
the same time being small enough that it is feasible to go through the entire list and determine the
best choice in less than 1 hour. We recruited student subjects who had an interest in finding housing
and thus were quite motivated to perform the task accurately.
We studied two settings:
490

P REFERENCE - BASED S EARCH USING E XAMPLE -C RITIQUING WITH S UGGESTIONS

• in an unsupervised setting, we monitored user behavior on a publicly accessible examplecritiquing search tool for the listing. This allowed us to obtain data from over a hundred
different users; however, it was not possible to judge decision accuracy since we were not
able to interview the users themselves.
• in a supervised setting, we had 40 volunteer students use the tool under supervision. Here,
we could determine decision accuracy by asking the subjects to carefully examine the entire
database of offers to determine their target option at the end of the procedure. Thus, we could
determine the switching rate and measure decision accuracy.
There are 10 attributes: type of accommodation (room in a family house, room in a shared apartment, studio apartment, apartment), rent, number of rooms, furnished (or not), type of bathroom
(private or shared), type of kitchen (shared, private), transportation available (none, bus, subway,
commuter train), distance to the university and distance to the town center.
For numerical attributes, a preference consists of a relational operator (less than, equal, greater
than), a threshold value and an importance weight between 1-5; for example, ”price less than 600
Francs” with importance 4. For qualitative attributes, a preference specifies that a certain value is
preferred with a certain importance value. Preferences are combined by summing their weights
whenever the preference is satisfied, and options are ordered so that the highest value is the most
preferred.
Users start by stating a set PI of initial preferences, and then they obtain options by pressing
a search button. Subsequently, they go through a sequence of interaction cycles where they refine
their preferences by critiquing the displayed examples. The system maintains their current set of
preferences, and the user can state additional preferences, change the reference value of existing
preferences, or even remove one or more of the preferences. Finally, the process finishes with a
final set of preferences PF , and the user chooses one of the displayed examples.
The increment of preferences | PF −PI | is the number of extra preferences stated and represents
the degree to which the process stimulates preference expression.
The search tool was made available in two versions:
• C, only showing a set of 6 candidate apartments without suggestions, and
• C+S, showing a set of 3 candidate apartments and 3 suggestions selected according to the
probabilistic strategy with a utility-dominance criterion.
We now describe the results of the two experiments.
5.1 Online User Study
FlatFinder has been hosted on the laboratory web-server and made accessible to students looking for
accommodation during the winter of 2004-2005. For each user, it anonymously recorded a log of
the interactions for later analysis. The server presented users with alternate versions of the system,
i.e. with (C+S) and without (C) suggestions. We collected logs from 63 active users who went
through several cycles of preference revision.
In analyzing the results of these experiments, whenever we present a hypothesis comparing
users of the same group, we show its statistical significance using a paired test. For all hypotheses
491

V IAPPIANI , FALTINGS , & P U

number of critiquing cycles
initial preferences
final preferences
increment

tool without suggestions
2.89
2.39
3.04
0.64

tool with suggestions
3.00
2.23
3.69
1.46

Table 5: Average behavior of users of the on-line experiment. We collected logs of real users looking for a
student accommodation with our tool, hosted on the laboratory website.

comparing users of different groups, we use the impaired student test to indicate statistical significance. In both cases, we indicate significance by p, the probability of obtaining the observed data
under the condition that the null hypothesis is true. Values of p < 0.05 are considered significant, p
< 0.01 highly significant and p < 0.001 very highly significant.
We first considered the increment from initial preference enumeration | PI | to final preference
enumeration | PF |, as shown in Table 5. This increment was on average 1.46 for the tool with
suggestions C+S and only 0.64 for the tool C (128% increase), showing the higher involvement of
users when they see suggestions. This hypothesis was confirmed with p = 0.002.
It is interesting to see that in both groups the users interacted for a similar number of cycles
(average of 2.89 and 3.00; p = 0.42, the null hypothesis cannot be rejected), and that the number of
initial preferences is also close (average of 2.39 and 2.23, null hypothesis cannot be rejected with p
= 0.37), meaning that the groups are relatively unbiased.
The result of the test (Table 5) shows clearly that users are more likely to state preferences when
suggestions are present, thus verifying Hypothesis 1. However, as this is an online experiment, we
are not able to measure decision accuracy. In order to obtain these measures, we also conducted a
supervised user study.
5.2 Supervised User study
The supervised user study used the same tool as the online user study but users were followed during
their interaction.
To measure improvement of accuracy, we instructed all of the users to identify their most preferred item by searching the database using interface 1. This choice was recorded and was called
c1 . Then the users were instructed to interact with the database using interface 2 and indicate a
new choice (c2 ) if the latter was an improvement on c1 in their opinion. To evaluate whether the
second choice was better than the initial one, we instructed the users to review all apartments (100
apartments in this case) and tell us whether c1 , c2 , or a completely different one truly seemed best.
Thus, the experiment allowed us to measure decision accuracy, since we obtained the true target
choice for each user. If users stood by their first choice, it indicated that they had found their target
choice without further help from the second interface. If users stood by their second choice, it
indicated that they had found their target choice with the help of the second interface. If users chose
yet another item, it indicated that they had not found their target choice even though they performed
search with both interfaces.
40 subjects, mostly undergraduate students, with 9 different nationalities took part in the study.
Most of them (27 out of 40) had searched for an apartment in the area before and had used online
492

P REFERENCE - BASED S EARCH USING E XAMPLE -C RITIQUING WITH S UGGESTIONS

Characteristics
Gender
Male
Female
Age
10s
20s
30s
Education
Undergraduate
Phd
Familiar with online apartment search
Yes
No
Familiar with apartments in the area
Yes
No

Participants
31
9
2
36
2
36
4
26
14
27
13

Table 6: Demographic characteristics of participants for the supervised user study.

group 1
(C first)

Tool version
Decision Accuracy (mean)
Preference Enumeration (mean)
Interaction cycles (mean)
Interaction time (min.,mean)

Interaction with
first interface
C
0.45
5.30
5.60
8:09

group 2
(C+S first)

Tool version
Decision Accuracy (mean)
Preference Enumeration (mean)
Interaction cycles (mean)
Interaction time (mean)

C+S
0.72
5.44
4.05
7.39

Interaction with
second interface
C+S
0.80
6.15
4.55
4.33
C
0.67
4.50
6.25
3.33

Table 7: Results for the supervised experiment. Decision accuracy and preference enumeration (the number
of preferences stated) are higher when suggestions are provided (interface C+S, showing 3 candidates and 3 suggestions) rather than when suggestions are not provided (interface C, 6 candidates).

493

V IAPPIANI , FALTINGS , & P U

sites (26 out of 40) to look for accommodations. Table 6 shows some of their demographic characteristics. The subjects were motivated by the interest of finding a better apartment for themselves,
which meant that they treated the study seriously.
To overcome bias due to learning and fatigue, we divided the users in two groups, who were
asked to interact with the versions in two different orders:
• group 1 used tool C (step 1) and then C+S (step 2)
• group 2 used tool C+S (step 1) and then C (step 2)
Both groups then examined the entire list to find the true most preferred option. For each version
of the tool and each group, we recorded the fraction of subjects where the final choice made using
that interface was equal to the target option as decision accuracy. For both groups, we refer to
accuracy of interface 1 as acc1 , and accuracy of interface 2 as acc2 .
We expected that the order of presenting the versions would be important. Once the users
realized their own preferences and found a satisfactory option, they are likely to be consistent with
that. Therefore, we expected acc2 > acc1 in both cases. However, we expected that average
accuracy would significantly increase with suggestions, and so the results would show acc2 >>
acc1 in the first group and acc2 only slightly higher than acc1 in group 2.
Table 7 shows the results. In the next section we want to verify Hypothesis 2 (decision accuracy
improves with suggestions) and 3 (preference enumeration improves accuracy). Finally we will
check whether a mediation phenomenon is present (meaning that the improvement of accuracy is
entirely explained by the fact that suggestions lead to an increase of preferences).
Decision Accuracy improves with suggestions Figure 8 shows the variation of decision accuracy
and the number of interaction cycles for the two groups.
For group 1, after interaction with tool C, the average accuracy is only 45%, but after interaction
with C+S, the version with suggestions, it goes up to 80%. This confirms the hypothesis that
suggestions improve accuracy with p = 0.00076. 10 of the 20 subjects in this group switched to
another choice between the two versions, and 8 of them reported that the new choice was better.
Clearly, the use of suggestions significantly improved decision accuracy for this group.
Users of group 2 used C+S straight away and achieved an average accuracy of 72% at the outset.
We expected that a consequent use of tool C would have a small positive effect on the accuracy,
but in reality the accuracy decreased to 67%. 10 subjects changed their final choice using the tool
without suggestions, and 6 of them said that the newly chosen was only equally good as the one
they originally chose. The fact that accuracy does not drop significantly in this case is not surprising
because users remember their preferences from using the tool with suggestions and will thus state
them more accurately independently of the tool. We can conclude from this group that improved
accuracy is not simply the result of performing the search a second time, but due to the provision
of suggestions in the tool. Also, the closeness of the accuracy levels reached by both groups when
using suggestions can be interpreted as confirmation of its significance.
We also note that users of interface C+S needed fewer cycles (and thus less effort) to make
decisions (average of 4.15) than interface C (5.92).
Interestingly, the price of the chosen apartment increased for the first group (average of 586.75
for C to 612.50 for C+S; p = 0.04, statistically significant), whereas it decreased for the second group
(average of 527.20 for C+S to 477.25 for C; p = 0.18, the decrease is not statically significant). We
494

P REFERENCE - BASED S EARCH USING E XAMPLE -C RITIQUING WITH S UGGESTIONS

User study - Group 2
1

0.8

0.8

0.6

0.6

Accuracy

Accuracy

User study - Group 1
1

0.4

0.2

0.4

0.2

0

0
1) tool without suggestions

2) tool with suggestions

1) tool with suggestions

2) tool without suggestions

(a) For group 1, accuracy dramatically increased when (b) For group 2, accuracy is already very high when

they used the version with suggestions (C+S).

they use the version with suggestions (C+S). Further
interaction cycles with the tool C showing 6 candidates
does not increase accuracy any further.
User study - Group 2
7

6

6

5

5

Interaction cycles

Interaction cycles

User study - Group 1
7

4
3

4
3

2

2

1

1

0

0
1) tool without suggestions

2) tool with suggestions

1) tool with suggestions

2) tool without suggestions

(c) For group 1, users needed less interaction cycles to (d) For group 2, the number of interaction cycles sig-

make a choice when using the interface with sugges- nificantly increased when they used the version withtions (C+S).
out suggestions (C).

Figure 8: Decision accuracy and interaction cycles for both groups of users of the supervised experiment.

495

V IAPPIANI , FALTINGS , & P U

found
still not found

0.45
0.55
∆|P | <= 0

0.83
0.17
∆|P | > 0

Table 8: For users who did not find their target in the first use of the tool, the table shows the fraction that did
and did not find their target in the next try, depending on whether the size of their preference model
did or did not increase. (∆|P | is the variation of the number of stated preferences |P | between the
two uses of the tool).

believe that subjects in the first group did not find a good choice, and thus paid a relatively high
price to get an apartment with which they would feel comfortable. Conditioned on this high price,
they were then willing to spend even more as they discovered more interesting features through
suggestions. On the other hand, subjects in group 2 already found a good choice in the first use
of the tool, and were unwilling to accept a high price when they did not find a better choice in the
second search without suggestions.
Thus, we conclude that Hypothesis 2 is confirmed: suggestions indeed increase decision accuracy.
Preference enumeration improves accuracy In this study, we notice that when suggestions are
present, users state a higher number of preferences (average of 5.8 preferences vs. only 4.8 without
suggestions, p = 0.021), so that Hypothesis 1 is again confirmed.
To validate hypothesis 3, that a higher preference enumeration also leads to more accurate decisions, we can compare the average size of the preference model for those users who found their
target solution with the first use of the tool and those who did not. In both groups, users who did find
their target in the first try stated on average 5.56 preferences (5.56 in group 1 and 5.57 in group 2)
while users who did not find their target stated only an average of 4.88 preferences (5.09 in group 1
and 4.67 in group 2). This shows that increased preference enumeration indeed improves accuracy
but unfortunately we did not find this statistically significant (p = 0.17). In fact, there is a chance that
this correlation is due to some users being more informed and thus making more accurate decisions
and stating more preferences.
As an evaluation independent of user’s a priori knowledge, we considered those users who did
not find their target in the first try only. As a measure of correlation of preference enumeration
and accuracy, we considered how often an increase in preference enumeration in the second try
led to finding the most preferred option on the second try. Table 8 shows that among users whose
preference model did not grow in size, only 45% found their target, whereas of those that increased
their preference model, 83% found their target. Again, we see a significant confirmation that higher
preference enumeration leads to a more accurate decision with real users (p = 0.038251).
Finally, a third confirmation can be obtained by considering the influence that variations in the
size of the preference model have on decision accuracy, shown in Table 9. Each column corresponds
to users where the size of the preference model decreased, stayed the same, or increased. It also
shows the fraction for which the accuracy increased, stayed the same or decreased (note that when
accuracy is 1 at the first step, it cannot further increase). We can see that a significant increase in
accuracy occurs only when the size of the preference model increases. In all other cases there are
496

P REFERENCE - BASED S EARCH USING E XAMPLE -C RITIQUING WITH S UGGESTIONS

∆acc > 0
∆acc = 0
∆acc < 0

0.23
0.62
0.15
∆|P | < 0

0.14
0.71
0.14
∆|P | = 0

0.38
0.62
0.00
∆|P | > 0

Table 9: Variation of accuracy against variation of the number of stated preferences |P | between the two
uses of the tool.

some random variations but no major increases. The statistical test shows that the hypothesis that
an increase in preference enumeration causes an increase in accuracy is confirmed with p = 0.0322.
Thus, we conclude that hypothesis 3 is also validated by the user study; a more complete preference model indeed leads to more accurate decisions.
Mediation analysis Since our three hypotheses are verified, the presence of suggestions lead to
an increase of the preferences stated and consequently to an increase in accuracy. With a 3-step
mediation analysis we want to check whether there is a mediation phenomenon, meaning that the
increase of accuracy is entirely explained by the increase of the preferences.
However, a Sobel test did not show statistical significance (p=0.14), so we cannot conclude that
the increase of the preference enumeration is a “mediator”. Our interpretation is that suggestions
influence decision accuracy by also making the users state better preferences.
5.3 Other Observations
A more objective measure of confidence is the price that people are willing to pay for the chosen
option as a measure of their satisfaction, since they would only pay more if the choice satisfies them
more based on the other attributes. For the 40 subjects, the average rent of the chosen housing with
suggestion was CHF 569.85, an increase of about 7% from the average without suggestions, which
was CHF532.00. In fact, we can observe a general correlation between price and accuracy, as 9 out
of the 10 subjects that did not find their target in the first interaction finally chose an apartment with
higher rent.
All subjects notably liked the interaction (average 4.1 out of 5) with no significant difference
between the versions. We asked the subjects which version they considered more productive. The
majority of them, 22 out of 40, preferred the version with suggestions, while 13 preferred the version
with more candidates and 5 had no opinion.
Another indication that suggestions are helpful is the average time to complete the decision task:
while it took subjects an average of 8:09 minutes to find their target without suggestions, the version
with suggestions took only 7:39 minutes on average. Thus, using suggestions users take less time
but obtain a more accurate decision.

6. Related Work
Example-based search tools Burke and others (1997) have been among the first to recognize the
challenge of developing intelligent tools for preference-based search. Their approach, called as497

V IAPPIANI , FALTINGS , & P U

sisted browsing combines searching and browsing with knowledge based assistance and recognizes
that users are an integral part of the search process.
They developed the FindMe approach, consisting of a family of prototypes that implement the
same intuition in a variety of domains (restaurants, apartments, cars, video, etc.). The main features
are the possibility of similarity based retrieval (look for a restaurant similar to this, but in San
Francisco), the support for tweaking (look for bigger, nicer, closer to centre, ..), abstraction of high
level features (users might look for a restaurant with casual look, where look is not defined in the
database directly, but decoupled into a few basic features), and multiple similarity metrics. The
display follows a hierarchical sort where the preferences (described by goals: minimize price, find
a seafood cuisine) have a fixed priority. The restaurant advisor was tested on-line for several years.
Another early and similar work is the ATA system of Linden et al. (1997). ATA is a tool for
planning travel itineraries based on user’s constraints. It followed the so-called candidate-critiquing
cycle where users could post constraints on their travel and would be shown the 3 best matching
flights from the database. ATA was tested on-line for several months.
In more recent work, Shearin and Lieberman (2001), have described AptDecision, an examplecritiquing interface where the user is able to guide the search by giving feedback on any feature (in
the form of either positive or negative weights) at any time. All of these critiques are stored in a
profile that is displayed at the bottom part of the interface and can be modified or stored for later
use. Instead of providing feedback manually, the user might prefer to let AptDecision learn his or
her profile weights by comparing two sample examples. However, they did not investigate strategies
for suggestions.
Improving example selection Techniques to induce users to state their preferences more accurately have been proposed in various recommender systems. Suggestion mechanisms include extreme values, diversity, and compound critiques.
The ATA system of Linden et al. (1997) included a suggestion strategy of showing extreme
examples applied to the airplane travel domain, for example the first and last flight of the day. In
our simulations, we compared our model-based techniques to this strategy.
Several researchers (Bridge & Ferguson, 2002; Smyth & McClave, 2001; McSherry, 2002;
McGinty & Smyth, 2003; Smyth & McGinty, 2003; McSherry, 2003) have studied the issue of
achieving a good compromise between generating similar and diverse results in case-based retrieval.
They consider the problem of finding cases that are most similar to a given query case, but at the
same time maximize the diversity of the options proposed to the user. Smyth et. al (2003) improves
the common query show me more like this: their adaptive search algorithm alternates between a
strategy that privileges similarity and one that privileges diversity (refocus). McSherry (2002) took
this idea further and provided selection algorithms that maximize diversity and similarity at the
same time. McSherry (2003) proposes a technique where retrieved cases are associated with a set
of like cases that share identical differences with the query case. The like cases are not displayed
among the examples, but accessible to users on demand. Thus, the retrieval set can be more diverse.
Reilly et al. (2004) also uses a mixture of similarity and diversity, with the goal of providing
possible standardized critiques to allow trade-offs analysis in an e-commerce environment. A critique is, in this scope, a modification of a user’s current preferences for narrowing down the search
or it is an indication of a trade-off. Users can select either unit critiques which revise preferences
on individual attributes, or compound critiques which revise preferences on multiple attributes. The
compound critiques are organized into categories and displayed in natural language form, for ex498

P REFERENCE - BASED S EARCH USING E XAMPLE -C RITIQUING WITH S UGGESTIONS

ample more memory and larger and heavier. One of the innovations in their work is the automatic
generation of sensible critiques involving several features based on available items using the Apriori
algorithm. Both simulated and real user studies have shown that compound critiques significantly
reduce the number of interaction cycles.
All of these approaches, however, differ from ours in the sense that they do not have an explicit
preference model. The recent work of Hebrard et al. (2005) has investigated the computational
problem of generating diverse solutions to constraint satisfaction problems.
Dialogue-based approaches Many other related works try to simulate human conversation in
order to guide the customer through the decision making process. Shimazu (2001) describes ExpertClerk, an agent system that imitates a human salesclerk. In the first phase, the agent tries to
narrow down the possibilities by asking questions. An optimal discrimination tree is built using
information gain (as in ID3 algorithm) where each node represents a specific question to the user,
and the user’s answer leads into a specific portion of the subtree. In fact, each node is equivalent to
a crisp constraint, and the problem of getting to a node with no compatible examples may occur. In
the second phase, the agent proposes three possible items, chosen to be one in the central and two
in the opposite extreme region of the available product space. It is shown that an intelligent use of
both strategies (asking and proposing) is more efficient that one of the two strategies alone.
Thompson, Göker, and Langley (2004) also propose a conversational, dialogue-based approach
in ADAPTIVE PLACE ADVISOR, a conversational recommendation system for restaurants in the
Palo Alto area. Their approach mimics a conversation that proceeds with questions like What type
of food would you like?; the user might either answer with a particular answer like Chinese, say
that he or she does not care about this aspect, or ask the advisor about the possible choices. User
preferences obtained during the current conversation are treated as crisp constraints and only items
that satisfy them are considered. When there are no items that satisfy all preferences, the system
may ask the user whether he or she is willing to relax some constraints.
The tool also develops a long-term user model that keeps track of preferences expressed in
previous interactions. It is used to sort the results that are shown to the user.
Using prior knowledge It is also possible to optimize the set of examples given an expectation of
the user’s preferences, without actually asking the users to state their own preferences. This is the
approach described by Price and Messinger (2005). This work differs from ours in that they do not
consider preferences of an individual user, but average preferences for a group of users.
Preference elicitation can be optimized using prior distributions of possible preferences. This
approach was proposed by Chajewska et al. (2000) to produce a more efficient preference elicitation
procedure. The elicitation is a question-answering interaction where the questions are selected to
maximize the expected value of information. Boutilier (2002) has extended this work by taking into
account values of future questions to further optimize decision quality while minimizing user effort.
He views the elicitation procedure itself as a decision process and uses observable Markov process
(POMDP) to obtain an elicitation strategy.
Such approaches require that users are familiar enough with the available options to answer any
question about value functions without the benefit of example outcomes to assess them. In contrast,
in a mixed-initiative system as described here the user is free to furnish only the information she is
confident about. It is also questionable whether one can assume a prior distribution on preferences
in personalized recommendation systems where users may be very diverse.
499

V IAPPIANI , FALTINGS , & P U

7. Conclusion
We considered AI techniques used for product search and recommender systems based on a set of
preferences explicitly stated by users. One of the challenges recognized in this field is the elicitation
of an accurate preference model from each user. In particular, we face the dilemma of accuracy at
the cost of user effort.
Some systems may introduce severe errors into the model because users cannot expend the
amount of effort required to state preferences, while others may require little effort but provide
very general recommendations because the preference model was never completely established.
The ideal solution is one that provides users with accurate recommendations while minimizing
their effort in stating preferences. Therefore, this article also examined user interaction issues and
emphasized models that motivate users to state more complete and accurate preferences, while
requiring the least amount of effort from the user.
We conjectured that the benefit of discovering attractive recommendations presents a strong motivation for users to state additional preferences. Thus, we developed a model-based approach that
analyzes the user’s current preference model and potential hidden preferences in order to generate a
set of suggestions that would be attractive to a rational user. This suggestion set is calculated based
on the look-ahead principle: a good suggestion is an outcome that becomes optimal when additional
hidden preferences have been considered. Through simulations, we demonstrated the superior performance of these model-based strategies in comparison to the other proposed strategies.
We further validated our hypothesis that such strategies are highly likely to stimulate users to
express more preferences through a significant within-subject user study involving 40 real users.
We measured decision accuracy, defined as the percentage of users who actually found their most
preferred option with the tool, for an example-critiquing tool with and without suggestions.
The study showed that users are able to achieve a significantly higher level of decision accuracy
with an example-critiquing tool with suggestions than without suggestions, increasing from 45 to
80%, while the effort spent on both tools is comparable. This shows that there is significant potential
for improving the tools that are currently in use.
It is important to note that this performance is obtained with users who are not bound to a
particular dialogue, but are free to interact with the system on their own initiative.
This process particularly supports preference expression for users who are unfamiliar with the
domain, and typically for decisions which require low to medium financial commitments. For highly
important decisions where users understand their preferences well, other preference elicitation techniques (Keeney & Raiffa, 1976; Boutilier et al., 2005) are likely to provide superior results.
As the strategies are based on the very general notion of Pareto-optimality, they can be applied
to a broad range of preference modeling formalisms, including utility functions, soft constraints
(Bistarelli et al., 1997), and CP-networks (Boutilier, Brafman, Domshlak, Hoos, & Poole, 2004).
This will greatly strengthen the performance of example-critiquing systems in applications ranging
from decision support to e-commerce.

8. Acknowledgements
The authors would like to thank Vincent Schickel-Zuber for his significant contribution in the development of the web based interface for FlatFinder, and Jennifer Graetzel for her insightful sug500

P REFERENCE - BASED S EARCH USING E XAMPLE -C RITIQUING WITH S UGGESTIONS

gestions on the various draft versions of this manuscript to improve its readability. This work was
supported by the Swiss National Science Foundation under contract No. 200020-103421.

References
Adomavicius, G., & Tuzhilin, A. (2005). Toward the next generation of recommender systems: A
survey of the state-of-the-art and possible extensions. IEEE Transactions on Knowledge and
Data Engineering, 17(6), 734–749.
Bistarelli, S., Montanari, U., & Rossi, F. (1997). Semiring-based constraint satisfaction and optimization. Journal of ACM, 44(2), 201–236.
Boutilier, C. (2002). A pomdp formulation of preference elicitation problems. In Proceedings of
the Eighteenth National Conference on Artificial Intelligence (AAAI’02), pp. 239–246.
Boutilier, C., Brafman, R. I., Domshlak, C., Hoos, H. H., & Poole, D. (2004). CP-nets: A tool for
representing and reasoning with conditional ceteris paribus preference statements. Journal of
Artificial Intelligence Research, 21, 135–191.
Boutilier, C., Patrascu, R., Poupart, P., & Schuurmans, D. (2005). Regret-based utility elicitation
in constraint-based decision problems.. In Proceedings of the Nineteenth International Joint
Conference on Artificial Intelligence (IJCAI’05), pp. 929–934.
Bridge, D. G., & Ferguson, A. (2002). Diverse product recommendations using an expressive language for case retrieval. In Proceedings of the 6th European Conference on Advances in
Case-Based Reasoning (ECCBR’02), pp. 43–57.
Burke, R. (2002a). Interactive critiquing for catalog navigation in e-commerce. Artificial Intelligence Review, 18(3-4), 245–267.
Burke, R. D. (2002b). Hybrid recommender systems: Survey and experiments. User Model. UserAdapt. Interact., 12(4), 331–370.
Burke, R. D., Hammond, K. J., & Young, B. C. (1997). The FindMe approach to assisted browsing.
IEEE Expert, 12(4), 32–40.
Chajewska, U., Koller, D., & Parr, R. (2000). Making rational decisions using adaptive utility
elicitation. In Proceedings of the Seventeenth National Conference on Artificial Intelligence
and Twelfth Conference on Innovative Applications of Artificial Intelligence (AAAI’00), pp.
363–369. AAAI Press / The MIT Press.
Chomicki, J. (2003). Preference formulas in relational queries. ACM Trans. Database Syst., 28(4),
427–466.
Faltings, B., Torrens, M., & Pu, P. (2004). Solution generation with qualitative models of preferences. In Computational Intelligence, pp. 246–263(18). ACM.
Haubl, G., & Trifts, V. (2000). Consumer decision making in online shopping environments: The
effects of interactive decision aids. Marketing Science, 19(1), 4–21.
Hebrard, E., Hnich, B., O’Sullivan, B., & Walsh, T. (2005). Finding diverse and similar solutions in
constraint programming. In Proceedings of the Twentieth National Conference on Artificial
Intelligence (AAAI’05), pp. 372–377.
501

V IAPPIANI , FALTINGS , & P U

Keeney, R. L. (1992). Value-Focused Thinking. A Path to Creative Decision Making. Cambridge:
Harvard University Press.
Keeney, R. L., & Raiffa, H. (1976). Decisions with Multiple Objectives: Preferences and Value
Tradeoffs. John Wiley and Sons, New York.
Kiesling, W. (2002). Foundations of preferences in database systems. In Proceedings of the 28th
International Conference on Very Large Data Bases (VLDB’02), pp. 311–322.
Konstan, J. A., Miller, B. N., Maltz, D., Herlocker, J. L., Gordon, L. R., & Riedl, J. (1997). Grouplens: Applying collaborative filtering to usenet news. Commun. ACM, 40(3), 77–87.
Krulwich, B. (1997). Lifestyle finder: Intelligent user profiling using large-scale demographic data.
AI Magazine, 18(2), 37–45.
Linden, G., Hanks, S., & Lesh, N. (1997). Interactive assessment of user preference models: The
automated travel assistant. In Proceedings of the Fifth Internation Conference on User Modeling (UM’97).
McCarthy, K., McGinty, L., Smyth, B., & Reilly, J. (2005). A live-user evaluation of incremental dynamic critiquing. In Proceedings of the 6th International Conference on Case-Based
Reasoning (ICCBR’05), Vol. 3620, pp. 339–352. Springer LNAI.
McGinty, L., & Smyth, B. (2003). On the role of diversity in conversational recommender system.
In Proceedings of the Fifth International Conference on Case-Based Reasoning (ICCBR’03),
pp. 276–290. LNAI 2689.
McSherry, D. (2002). Diversity-conscious retrieval. In Proceedings of 6th European Conference on
Advances in Case-Based Reasoning (ECCBR’02), pp. 219–233.
McSherry, D. (2003). Similarity and compromise. In Proceedings of the 5th International Conference on Case-Based Reasoning (ICCBR’03), pp. 291–305.
Payne, J., Bettman, J., & Johnson, E. (1993). The Adaptive Decision Maker. Cambridge University
Press.
Price, R., & Messinger, P. R. (2005). Optimal recommendation sets: Covering uncertainty over user
preferences. In Proceedings of the Twentieth National Conference on Artificial Intelligence
(AAAI’05), pp. 541–548.
Pu, P., & Chen, L. (2005). Integrating tradeoff support in product search tools for e-commerce sites.
In Proceedings of ACM Conference on Electronic Commerce (EC’05), pp. 269–278.
Pu, P., & Faltings, B. (2000). Enriching buyers’ experiences: the smartclient approach. In Proceedings of the SIGCHI conference on Human factors in computing systems (CHI’00), pp.
289–296. ACM Press New York, NY, USA.
Pu, P., & Faltings, B. (2004). Decision tradeoff using example-critiquing and constraint programming. Constraints: An International Journal, 9(4).
Pu, P., & Kumar, P. (2004). Evaluating example-based search tools. In Proceedings of the ACM
Conference on Electronic Commerce (EC’04).
Reilly, J., McCarthy, K., McGinty, L., & Smyth, B. (2004). Dynamic critiquing. In Proceedings
of the 7th European Conference on Advances in Case-Based Reasoning (ECCBR’04), pp.
763–777.
502

P REFERENCE - BASED S EARCH USING E XAMPLE -C RITIQUING WITH S UGGESTIONS

Resnick, P., Iacovou, N., Suchak, M., Bergstorm, P., & Riedl, J. (1994). Grouplens: An open architecture for collaborative filtering of netnews. In Proceedings of ACM 1994 Conference on
Computer Supported Cooperative Work, pp. 175–186, Chapel Hill, North Carolina. ACM.
Rich, E. (1979). User modeling via stereotypes. Cognitive Science, 3, 329–354.
Ruttkay, Z. (1994). Fuzzy constraint satisfaction. In Proceedings of the 3rd IEEE Conference on
Fuzzy Systems, pp. 1263–1268, Orlando.
Shearin, S., & Lieberman, H. (2001). Intelligent profiling by example. In Proceedings of Intelligent
User Interfaces (IUI 2001), pp. 145–151.
Shimazu, H. (2001). Expertclerk: Navigating shoppers buying process with the combination of
asking and proposing. In Proceedings of the Seventeenth International Joint Conference on
Artificial Intelligence (IJCAI’01), Vol. 2, pp. 1443–1448.
Smyth, B., & McClave, P. (2001). Similarity vs. diversity. In Proceedings of the 4th International
Conference on Case-Based Reasoning (ICCBR’01), pp. 347–361.
Smyth, B., & McGinty, L. (2003). The power of suggestion. In Proceedings of the Eighteenth
International Joint Conference on Artificial Intelligence (IJCAI 2003), Acapulco, Mexico, pp.
127–132.
Spiekermann, S., & Paraschiv, C. (2002). Motivating human–agent interaction: Transferring insights
from behavioral marketing to interface design. Electronic Commerce Research, 2(3), 255–
285.
Thompson, C. A., Göker, M. H., & Langley, P. (2004). A personalized system for conversational
recommendations. Journal of Artificial Intelligence Research, 21, 393–428.
Torrens, M., Faltings, B., & Pu, P. (2002). Smart clients: Constraint satisfaction as a paradigm for
scaleable intelligent information systems. Special issue on Constraints and Agents. CONSTRAINTS: an Internation Journal. Kluwer Academic Publishers, pp. 49–69.
Torrens, M., Weigel, R., & Faltings, B. (1998). Distributing problem solving on the web using
constraint technology. In Proceedings of the International Conference on Tools with Artificial
Intelligence (ICTAI’98), pp. 42–49, Taipei, Taiwan. IEEE Computer Society Press.
Tversky, A. (1974). Judgement under uncertainity: Heuristics and biases. Science, 185, 1124–1131.
Zhang, J., & Pu, P. (2006). Performance evaluation of consumer decision support systems. International Journal of E-Business Research, 2, 28–45.

503

Journal of Artificial Intelligence Research 27 (2006) 577–615

Submitted 2/2006; published 12/2006

Understanding Algorithm Performance on an
Oversubscribed Scheduling Application
Laura Barbulescu

laurabar@cs.cmu.edu

The Robotics Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA

Adele E. Howe
L. Darrell Whitley
Mark Roberts

howe@cs.colostate.edu
whitley@cs.colostate.edu
mroberts@cs.colostate.edu

Computer Science Department
Colorado State University
Fort Collins, CO 80523 USA

Abstract
The best performing algorithms for a particular oversubscribed scheduling application,
Air Force Satellite Control Network (AFSCN) scheduling, appear to have little in common. Yet, through careful experimentation and modeling of performance in real problem
instances, we can relate characteristics of the best algorithms to characteristics of the
application. In particular, we find that plateaus dominate the search spaces (thus favoring algorithms that make larger changes to solutions) and that some randomization in
exploration is critical to good performance (due to the lack of gradient information on
the plateaus). Based on our explanations of algorithm performance, we develop a new
algorithm that combines characteristics of the best performers; the new algorithm’s performance is better than the previous best. We show how hypothesis driven experimentation
and search modeling can both explain algorithm performance and motivate the design of
a new algorithm.

1. Introduction
Effective solution of the Air Force Satellite Control Network (AFSCN) oversubscribed
scheduling problem runs counter to what works well for similar scheduling problems. Other
similar oversubscribed problems, e.g., United States Air Force (USAF) Air Mobility Command (AMC) airlift (Kramer & Smith, 2003) and scheduling telescope observations (Bresina,
1996), are well solved by heuristically guided constructive or repair based search. The best
performing solutions to AFSCN are a genetic algorithm (Genitor), Squeaky Wheel Optimization (SWO) and randomized next-descent local search. We have not yet found a
constructive or repair based solution that is competitive.
The three best performing solutions to AFSCN appear to have little in common, making
it difficult to explain their superior performance. Genitor combines two candidate solutions
preserving elements of each. SWO creates an initial greedy solution and then attempts to
improve the scheduling of all tasks known to contribute detrimentally to the current evaluation. Randomized local search makes incremental changes based on observed immediate
gradients in schedule evaluation. In this paper, we examine the performance of these differc
2006
AI Access Foundation. All rights reserved.

Barbulescu, Howe, Whitley, & Roberts

ent algorithms, identify factors that do or do not help explain the performance and leverage
the explanations to design a new search algorithm that is well suited to the characteristics
of this application.
Our target application is an oversubscribed scheduling application with alternative resources. AFSCN (Air Force Satellite Control Network) access scheduling requires assigning
access requests (communication relays to U.S.A. government satellites) to specific time slots
on an antenna at a ground station. It is oversubscribed in that not all tasks can be accommodated given the available resources. To be considered to be oversubscribed, at least
some problem instances need to overtax the available resources; for our application though,
it appears that most problem instances specify more tasks than can be feasibly scheduled.
The application is challenging and shares characteristics with other applications such as
Earth Observing Satellites (EOS). It is important in that a team of human schedulers have
laboriously performed the task every day for at least 15 years with minimal automated
assistance.
All of the algorithms are designed to traverse essentially the same search space: solutions
are represented as permutations of tasks, which a greedy schedule builder converts into a
schedule by assigning start time and resources to the tasks in the order in which they
appear in the permutation. We find that this search space is dominated by large flat
regions (plateaus). Additionally, the size of the plateaus increases dramatically as the best
solution is approached. The presence of the plateaus indicates that each algorithm needs
to effectively manage them in order to find improving solutions.
We have explored a number of different hypotheses to explain the performance of each
algorithm. Some of these hypotheses include the following:
Genitor, a genetic algorithm, identifies patterns of relative task orderings, similar to backbones from SAT (Singer, Gent, & Smaill, 2000), which are preserved in the members of
the population. This is in effect a type of classic building block hypothesis (Goldberg,
1989).
SWO starts extremely close to the best solution and so need not enact much change. This
hypothesis also implies that it is relatively easy to modify good greedy solutions to
find the best known solutions.
Randomized Local Search performs essentially a random walk on the plateaus to find
exits leading to better solutions; given the distribution of solutions and lack of gradient
information, this may be as good a strategy as any.
We tested each of these hypotheses. There is limited evidence for the existence of building
blocks or backbone structure. And while Squeaky Wheel Optimization does quickly find
good solutions, it cannot reliably find best known solutions. Therefore, while the first
two hypotheses were somewhat supported by the data, the hypotheses were not enough to
explain the observed performance.
The third hypothesis appears to be the best explanation of why the particular local
search strategy we have used works so well. In light of this, we formulated another hypothesis:
SWO and Genitor make long leaps in the search space, which allow them to relatively
quickly traverse the plateaus.
578

Understanding Algorithm Performance

This last hypothesis appears to well explain the performance of the two methods. For
the genetic algorithm the leaps are naturally longer during the early phases of search when
parent solutions are less similar.
Based on these studies, we constructed a new search algorithm that exploits what we
have learned about the search space and the behavior of successful algorithms. Attenuated
Leap Local Search makes multiple changes to the solution before evaluating a candidate
solution. In addition, the number of changes decreases proportionately with expected proximity to the solution. The number of multiple changes, or the length of the leap, is larger
early in the search, and reduces (shortens) as better solutions are found. We find that this
algorithm performs quite well: it quickly finds best known solutions to all of the AFSCN
problems.

2. AFSCN Scheduling
The U.S.A. Air Force Satellite Control Network is currently responsible for coordinating
communications between civilian and military organizations and more than 100 USAF managed satellites. Space-ground communications are performed using 16 antennas located at
nine tracking stations around the globe 1 . Figure 1 shows a map of the current configuration
of AFSCN; this map shows one fewer tracking station and antennae than are in our data,
due to those resources apparently having been taken off-line recently. Customer organizations submit task requests to reserve an antenna at a tracking station for a specified time
period based on the visibility windows between target satellites and tracking stations. Two
types of task requests can be distinguished: low altitude and high altitude orbits. The low
altitude tasks specify requests for access to low altitude satellites; such requests tend to
be short (e.g., 15 minutes) and have a tight visibility window. High altitude tasks specify
requests for high altitude satellites; the durations for these requests are more varied and
usually longer, with large visibility windows.
Approximately 500 requests are typically received for a single day. Separate schedules
are produced by a staff of human schedulers at Schriever Air Force Base for each day. Of the
500 requests, often about 120 conflicts remain after the first pass of scheduling. “Conflicts”
are defined as requests that cannot be scheduled, since they conflict with other scheduled
requests (this means that 120 requests remain unscheduled after an initial schedule is
produced).
From real problem data, we extract a description of the problem specification in terms of
task requests to be scheduled with their corresponding type (low or high altitude), duration,
time windows and alternative resources. The AFSCN data also include information about
satellite revolution numbers, optional site equipment, tracking station maintenance times
(downtimes), possible loss of data due to antenna problems, various comments, etc.; we do
not incorporate such information in our problem specification. The information about the
type of the task (low or high altitude) as well as the identifier for the satellite involved are
included in the task specification. However, we do not know how the satellite identifier
1. The U.S.A. government is planning to make the AFSCN the core of an Integrated Satellite Control
Network for managing satellite assets for other U.S.A. government agencies as well, e.g., NASA, NOAA,
other DoD affiliates. By 2011, when the system first becomes operational, the Remote Tracking Stations
will be increased and enhanced to accommodate the additional load.

579

Barbulescu, Howe, Whitley, & Roberts

Figure 1: Map of the current AFSCN network including tracking stations, control and relay.
The figure was produced for U.S.A. Space and Missile Systems Center (SMC).

corresponds to an actual satellite and so rely on precomputed visibility information which
is present in the requests.
A problem instance consists of n task requests. Each task request T i , 1 ≤ i ≤ n, specifies
a required processing duration TiDur . Each task request also specifies a number of j ≥ 0
pairs of the form (Rj , TijWin ), each identifying a particular alternative resource (antenna
Rj ) and time window TijWin for the task. The duration of the task is the same for all
possible alternative resources. The start and end of the visibility time window is specific
to each alternative resource; therefore while the duration is the same, the time windows
can be different for the alternative resources. Once a resource is assigned to the request,
the duration needs to be allocated within the corresponding time window. We denote the
lower and upper bounds of each time window j corresponding to request i by T ijWin (LB)
and TijWin (UB), respectively. For each task, only one of the alternative antennas needs to be
chosen; also, the tasks cannot be preempted once processing is initiated.
While requests are made for a specific antenna, often a different antenna at the same
tracking station may serve as an alternate because it has the same capabilities. We assume
that all antennas at a tracking station can serve as alternate resources. While this is not
always the case in practice, the same assumption was made by previous research from the Air
580

Understanding Algorithm Performance

Force Institute of Technology (AFIT) 2 . A low altitude request specifies as possible resources
the antennas present at a single tracking station (for visibility reasons, only one tracking
station can accommodate such a request). Usually there are two or three antennas present
at a tracking station, and therefore, only two or three possible resources are associated with
each of these requests. High altitude requests specify all the antennas present at all the
tracking stations that satisfy the visibility constraints; as many as 14 possible alternatives
are specified in our data.
Previous research and development on AFSCN scheduling focused on minimizing the
number of request conflicts for AFSCN scheduling, or alternatively, maximizing the number
of requests that can be scheduled without conflict. Those requests that cannot be scheduled
without conflict are bumped out of the schedule. This is not what happens when humans
carry out AFSCN scheduling3 . Satellites are valuable resources, and the AFSCN operators
work to fit in every request. What this means in practice is that after negotiation with the
customers, some requests are given less time than requested, or shifted to less desirable,
but still usable time slots. In effect, the requests are altered until all requests are at least
partially satisfied or deferred to another day. By using an evaluation function that minimizes
the number of request conflicts, an assumption is being made that we should fit in as many
requests as possible before requiring human schedulers to figure out how to place those
requests that have been bumped.
However, given that all requests need to be eventually scheduled, we designed a new
evaluation criterion that schedules all the requests by allowing them to overlap and minimizing the sum of overlaps between conflicting tasks. This appears to yield schedules that are
much closer to those that human schedulers construct. When conflicting tasks are bumped
out of the schedule, large and difficult to schedule tasks are most likely to be bumped;
placing these requests back into a negotiated schedule means deconstructing the minimal
conflict schedule and rebuilding a new schedule. Thus, a schedule that minimizes conflicts
may not help all that much when constructing the negotiated schedule, whereas a schedule
that minimizes overlaps can suggest ways of fitting tasks into the schedule, for example
by reducing a task’s duration by two or three minutes, or shifting a start outside of the
requested window by a short amount of time.
We obtained 12 days of data for the AFSCN application 4 . The first seven days are from
a week in 1992 and were given to us by Colonel James Moore at the Air Force Institute of
Technology. These data were used in the first research projects on AFSCN. We obtained an
additional five days of data from schedulers at Schriever Air Force Base. Table 2 summarizes
the characteristics of the data. The best known solutions were obtained by performing long
runs over hundreds of experiments. Using various algorithms and allowing for hundreds

2. In fact, large antennas are needed for high altitude requests, while smaller antennas can handle the low
altitude requests. Depending on the type of antennas present at a tracking station, not all antennas can
always serve as alternate resources for a request.
3. We met with the several of the schedulers at Schriever to discuss their procedure and have them crosscheck our solution. We appreciate the assistance of Brian Bayless and William Szary in setting up the
meeting and giving us data.
4. We have approval to make public some, but not all of the data.
See http://www.cs.colostate.edu/sched/data.html for details on obtaining the problems.

581

Barbulescu, Howe, Whitley, & Roberts

ID
A1
A2
A3
A4
A5
A6
A7
R1
R2
R3
R4
R5

Date
10/12/92
10/13/92
10/14/92
10/15/92
10/16/92
10/17/92
10/18/92
03/07/02
03/20/02
03/26/03
04/02/03
05/02/03

# Requests
322
302
311
318
305
299
297
483
457
426
431
419

# High
169
165
165
176
163
155
155
258
263
243
246
241

# Low
153
137
146
142
142
144
142
225
194
183
185
178

Best Conflicts
8
4
3
2
4
6
6
42
29
17
28
12

Best Overlaps
104
13
28
9
30
45
46
773
486
250
725
146

Table 1: Problem characteristics for the 12 days of AFSCN data used in our experiments.
ID is used in other tables. Best conflicts and best overlaps are the best known
values for each problem for these two objective functions.

of thousands of evaluations, we have not found better solutions than these 5 . We will refer
to the problems from 1992 as the A problems, and to the more recent problems, as the R
problems.

3. Related Scheduling Research
The AFSCN application is a multiple resource, oversubscribed problem. Examples of other
such applications are USAF Air Mobility Command (AMC) airlift scheduling (Kramer &
Smith, 2003), NASA’s shuttle ground processing (Deale et al., 1994), scheduling telescope
observations (Bresina, 1996) and satellite observation scheduling (Frank, Jonsson, Morris,
& Smith, 2001; Globus, Crawford, Lohn, & Pryor, 2003).
AMC scheduling assigns delivery missions to air wings (Kramer & Smith, 2003). Their
system adopts an iterative repair approach by greedily creating an initial schedule ordering
the tasks by priority and then attempting to insert unscheduled tasks by retracting and
re-arranging conflicting tasks.
The Gerry scheduler was designed to manage the large set of tasks needed to prepare
a space shuttle for its next mission (Zweben, Daun, & Deale, 1994). Tasks are described
in terms of resource requirements, temporal constraints and required time windows. The
original version used constructive search with dependency-directed backtracking, which was
not adequate to the task; a subsequent version employed constraint-directed iterative repair.
In satellite scheduling, customer requests for data collection need to be matched with
satellite and tracking station resources. The requests specify the instruments required,
the window of time when the request needs to be executed, and the location of the sensing/communication event. These task constraints need to be coordinated with resource
5. All the best known values can be obtained by running Genitor with the population size increased to 400
and allowing 50,000 evaluations per run.

582

Understanding Algorithm Performance

constraints; these include the windows of visibility for the satellites, maintenance periods
and downtimes for the tracking stations, etc. Typically, more requests need to be scheduled
than can be accommodated by the available resources. A general description of the satellite
scheduling domain is provided by Jeremy Frank et al. (2001).
Pemberton (2000) solves a simple one-resource satellite scheduling problem in which
the requests have priorities, fixed start times and fixed durations. The objective function
maximizes the sum of the priorities of the scheduled requests. A priority segmentation algorithm is proposed, which is a hybrid algorithm combining a greedy approach with branchand-bound. Wolfe and Sorensen (2000) define a more complex one-resource problem, the
window-constrained packing problem (WCP), which specifies for each request the earliest
start time, latest final time and the minimum and maximum duration. The objective function is complex, combining request priority with the position of the scheduled request in its
required window and the number of requests scheduled. Two greedy heuristic approaches
and a genetic algorithm are implemented; the genetic algorithm is found to perform best.
Globus et al. (2003) compare a genetic algorithm, simulated annealing, Squeaky Wheel
Optimization (Joslin & Clements, 1999) and hill climbing on a simplified, synthetic form
of the satellite scheduling problem (two satellites with a single instrument) and find that
simulated annealing excels and that the genetic algorithm performs relatively poorly. For
a general version of satellite scheduling (EOS observation scheduling), Frank et al. (2001)
propose a constraint-based planner with a stochastic greedy search algorithm based on
Bresina’s Heuristic-Biased Stochastic Sampling (HBSS) algorithm (Bresina, 1996). HBSS
was originally applied to scheduling astronomy observations for telescopes.
Lemaı̂tre et al. (2000) research the problem of scheduling the set of photographs for Agile
EOS (ROADEF Challenge, 2003). Task constraints include the minimal time between two
successive acquisitions, pairings of requests such that images are acquired twice in different
time windows, and hard requirements that certain images must always be acquired. They
find that a local search approach performs better than a hybrid algorithm combining branchand-bound with various domain-specific heuristics.
The AFSCN application was previously studied by researchers from the Air Force Institute of Technology (AFIT). Gooley (1993) and Schalck (1993) described algorithms based
on mixed-integer programming (MIP) and insertion heuristics, which achieved good overall performance: 91% – 95% of all requests scheduled. Parish (1994) used the Genitor
(Whitley, 1989) genetic algorithm, which scheduled roughly 96% of all task requests, outperforming the MIP approaches. All three of these researchers used the AFIT benchmark
suite consisting of seven problem instances, representing actual AFSCN task request data
and visibilities for seven consecutive days from October 12 to 18, 1992. Later, Jang (1996)
introduced a problem generator employing a bootstrap mechanism to produce additional
test problems that are qualitatively similar to the AFIT benchmark problems. Jang then
used this generator to analyze the maximum capacity of the AFSCN, as measured by the
aggregate number of task requests that can be satisfied in a single-day.
While the general decision problem of AFSCN Scheduling with minimal conflicts is N Pcomplete, special subclasses of AFSCN Scheduling are polynomial. Burrowbridge (1999)
considers a simplified version of AFSCN scheduling, where each task specifies only one resource (antenna) and only low-altitude satellites are present. The objective is to maximize
the number of scheduled tasks. Due to the orbital dynamics of low-altitude satellites, the
583

Barbulescu, Howe, Whitley, & Roberts

task requests in this problem have negligible slack ; i.e., the window size is equal to the
request duration. Assuming that only one task can be scheduled per time window, the wellknown greedy activity-selector algorithm (Cormen, Leiserson, & Rivest, 1990) is used to
schedule the requests since it yields a solution with the maximal number of scheduled tasks.
To schedule low altitude requests on one of the multiple antennas present at a particular
ground station, we extended the greedy activity-selector algorithm for multiple resource
problems. We proved that this extension of the greedy activity-selector optimally schedules the low altitude requests for the general problem of AFSCN Scheduling (Barbulescu,
Watson, Whitley, & Howe, 2004b).

4. Algorithms
We implemented a variety of algorithms for AFSCN scheduling: iterative repair, heuristic
constructive search, local search, a genetic algorithm (GA), and Squeaky Wheel Optimization (SWO). As will be shown in Section 5, we found that randomized next descent local
search, the GA and SWO work best for AFSCN scheduling.
We also considered constructive search algorithms based on texture (Beck, Davenport,
Davis, & Fox, 1998) and slack (Smith & Cheng, 1993) constraint-based scheduling heuristics.
We implemented straightforward extensions of such algorithms for our application. The
results were poor; the number of request tasks combined with the presence of multiple
alternative resources for each task make the application of such methods impractical. We
do not report the performance values for the constructive search methods because these
methods depend critically on the heuristics; we are uncomfortable concluding that the
methods are poor because we may not have found good enough heuristics for them. We
also tried using a commercial off-the-shelf satellite scheduling package and had similarly
poor results. We do not report performance values for the commercial system because it
had not been designed specifically for this application and we did not have access to the
source to determine the reason for the poor performance.
4.1 Solution Representation
Permutation based representations are frequently used when solving scheduling problems
(e.g., Whitley, Starkweather, Fuquay, 1989; Syswerda, 1991; Wolfe, Sorensen, 2000; Aickelin, Dowsland, 2003; Globus et al., 2003). All of our algorithms, except iterative-repair,
encode solutions using a permutation π of the n task request IDs (i.e., [1..n]). A schedule
builder is used to generate solutions from a permutation of request IDs. The schedule builder
considers task requests in the order that they appear in π. Each task request is assigned
to the first resource available from the sequence of resource and window pairs provided in
the task description (this is the first feasible resource in the sequence); the earliest possible
starting time is then chosen for this resource. When minimizing the number of conflicts,
if the request cannot be scheduled on any of the alternative resources, it is dropped from
the schedule (i.e., bumped). When minimizing the sum of overlaps, if a request cannot be
scheduled without conflict on any of the alternative resources, we assign it to the resource
584

Understanding Algorithm Performance

on which the overlap with requests scheduled so far is minimized. 6 Note that our schedule
builder does favor the order in which the alternative resources are specified in the request,
even though no preference is specified for any of the alternatives.
4.2 Iterative Repair
Iterative repair methods have been successfully used to solve various oversubscribed scheduling problems, e.g., Hubble Space Telescope observations (Johnston & Miller, 1994) and space
shuttle payloads (Zweben et al., 1994; Rabideau, Chien, Willis, & Mann, 1999). NASA’s
ASPEN (A Scheduling and Planning Environment) framework (Chien et al., 2000), employs both constructive and repair-based methods and has been used to model and solve
real-world space applications such as scheduling EOS. More recently, Kramer and Smith
(2003) used repair-based methods to solve the airlift scheduling problem for the USAF Air
Mobility Command.
In each case, a key component to the implementation was a domain appropriate ordering heuristic to guide the repairs. For AFSCN scheduling, Gooley’s algorithm (1993)
uses domain-specific knowledge to implement a repair-based approach. We implement an
improvement to Gooley’s algorithm that is guaranteed to yield results at least as good as
those produced by the original version.
Gooley’s algorithm has two phases. In the first phase, the low altitude requests are
scheduled, mainly using Mixed Integer Programming (MIP). Because there is a large number
of low altitude requests, the requests are divided into two blocks. MIP procedures are
first used to schedule the requests in the first block. Then MIP is used to schedule the
requests in the second block, which are inserted in the schedule around the requests from
the first block. Finally, an interchange procedure attempts to optimize the total number
of low altitude requests scheduled. This is needed because the low altitude requests are
scheduled in disjoint blocks. Once the low altitude requests are scheduled, their start time
and assigned resources remain fixed. In our implementation, we replaced this first phase
with a greedy algorithm (Barbulescu et al., 2004b) proven to schedule the optimal number
of low altitude requests7 . Our greedy algorithm modifies the well-known activity-selector
algorithm (Cormen et al., 1990) for multiple resource problems: the algorithm still schedules
the requests in increasing order of their due date, however it specifies that each request is
scheduled on the resource for which the idle time before its start time is the minimum.
Our version accomplishes the same function as Gooley’s first phase, but does so with a
guarantee that the optimal number of low-altitude requests are scheduled. Thus, the result
is guaranteed to be equal to or better than Gooley’s original algorithm.
In the second phase, the high altitude requests are inserted in the schedule (without
rescheduling any of the low altitude requests). An order of insertion for the high altitude
requests is computed. The requests are sorted in decreasing order of the ratio of the duration
of the request to the average length of its time windows (this is similar to the flexibility
measure defined by Kramer and Smith, 2003 for AMC); ties are broken based on the number
of alternative resources specified (fewer alternatives scheduled first). After all the high
6. If two or more non-scheduled tasks overlap with each other, this mutual overlap is not part of the sum
of overlaps. Only the overlap with scheduled requests is considered.
7. Our algorithm optimally solves the problem of scheduling only the low altitude requests, in polynomial
time.

585

Barbulescu, Howe, Whitley, & Roberts

altitude requests have been considered for insertion, an interchange procedure attempts to
accommodate the unscheduled requests, by rescheduling some of the high altitude requests.
For each unscheduled high altitude request, a list of candidate requests for rescheduling
is computed (such that after a successful rescheduling operation, the unscheduled request
can be placed in the spot initially occupied by such a candidate). A heuristic measure is
used to determine which requests from the candidate list should be rescheduled. For the
chosen candidates, if no scheduling alternatives are available, the same procedure is applied
to identify requests that can be rescheduled. This interchange procedure is defined with
two levels of recursion and is called “three satellite interchange”.
4.3 Randomized Local Search (RLS)
We implemented a hill-climber we call “randomized local search”, which starts from a randomly generated solution and iteratively moves toward a better or equally good neighboring solution. Because it has been successfully applied to a number of well-known
scheduling problems, we selected a domain-independent move operator, the shift operator. From a current solution π, a neighborhood is defined by considering all (N − 1) 2
pairs (x, y) of positions in π, subject to the restriction that y 6= x − 1. The neighbor
0
π corresponding to the position pair (x, y) is produced by shifting the job at position
x into the position y, while leaving all other relative job orders unchanged. If x < y,
then π 0 = (π(1), ..., π(x − 1), π(x + 1), ..., π(y), π(x), π(y + 1), ..., π(n)). If x > y, then
π 0 = (π(1), ..., π(y − 1), π(x), π(y), ..., π(x − 1), π(x + 1), ..., π(n)).
Given the large neighborhood size, we use the shift operator in conjunction with nextdescent hill-climbing. Our implementation completely randomizes which neighbor to examine next, and does so with replacement: at each step, both x and y are chosen randomly.
This general approach has been termed “stochastic hill-climbing” by Ackley (1987). If the
value of the randomly chosen neighbor is equal or better than the value of the current
solution, it becomes the new current solution.
It should be emphasized that Randomized Local Search, or stochastic hill-climbing, can
sometimes be much more effective than steepest-descent local search or next-descent local
search where the neighbors are checked in a predefined order (as opposed to random order).
Forrest and Mitchell (1993) showed that a random mutation hill climber (much like our RLS
or Ackley’s stochastic hill climber) found solutions much faster than steepest-descent local
search on a problem they called “The Royal Road” function. The random mutation hill
climber also found solutions much faster than a hill climber that generated and examined
the neighbors systematically (in a predefined order). Random mutation hill climber was
also much more effective than a genetic algorithm for this problem – despite the existence
of what would appear to be natural “building blocks” in the function. It is notable that
“The Royal Road” function is a staircase like function, where each step in the staircase is
a plateau.
4.4 Genetic Algorithm
Genetic algorithms were found to perform well on the AFSCN scheduling problem in some
early studies (Parish, 1994). Genetic algorithms have also been found to be effective in other
oversubscribed scheduling applications, such as scheduling F-14 flight simulators (Syswerda,
586

Understanding Algorithm Performance

1991) or an abstraction of NASA’s EOS problem (Wolfe & Sorensen, 2000). For our studies,
we used the version of Genitor originally developed for a warehouse scheduling application
(Starkweather et al., 1991); this is also the version used by Parish for AFSCN scheduling.
Like all genetic algorithms, Genitor maintains a population of solutions; in our implementation, we fixed the population size to be 200. In each step of the algorithm, a pair of parent
solutions is selected, and a crossover operator is used to generate a single child solution,
which then replaces the worst solution in the population. Selection of parent solutions is
based on the rank of their fitness, relative to other solutions in the population. Following
Parish (1994) and Starkweather et al. (1991), we used Syswerda’s (1991) position-based
crossover operator.
Syswerda’s position-based crossover operator starts by selecting a number of random
positions in the second parent. The corresponding selected elements will appear in exactly
the same positions in the offspring. The remaining positions in the offspring are filled with
elements from the first parent in the order in which they appear in this parent:
Parent 1: A B C D E F G H I J
Parent 2: C F A J H D I G B E
Selected Elements:
* *
*
*
Offspring: C F A E G D H I B J
For our implementation, we randomly choose the number of positions to be selected,
such that it is larger than one third of the total number of positions and smaller than two
thirds of the total number of positions.
4.5 Squeaky Wheel Optimization
Squeaky Wheel Optimization (SWO) (Joslin & Clements, 1999) repeatedly iterates through
a cycle composed of three phases. First, a greedy solution is built, based on priorities associated with the elements in the problem. Then, the solution is analyzed, and the elements
causing “trouble” are identified based on their contribution to the objective function. Third,
the priorities of such “trouble makers” are modified, such that they will be considered earlier during the next iteration. The cycle is then repeated, until a termination condition is
met.
We constructed the initial greedy permutation for SWO by sorting the requests in increasing order of their flexibility. Our flexibility measure is similar to that defined for the
AMC application (Kramer & Smith, 2003): the duration of the request divided by the
average time window on the possible alternative resources. We break ties based on the
number of alternative resources available. For requests with equal flexibilities and numbers
of alternative resources, the earlier request is scheduled first. For multiple runs of SWO,
we restarted it from a modified permutation created by performing 20 random swaps in the
initial greedy permutation.
When minimizing the sum of overlaps, we identified the overlapping requests as the
“trouble spots” in the schedule. Note that for any overlap, we considered one request to
be scheduled; the other request (or requests, if more than two requests are involved) is
“the overlapping request”. We sorted the overlapping requests in increasing order of their
contribution to the sum of overlaps. We associated with each such request a distance to
587

Barbulescu, Howe, Whitley, & Roberts

move forward, based on its rank in the sorted order. We fixed the minimum distance of
moving forward to one and the maximum distance to five (this seems to work better than
other possible values we tried). The distance values are equally distributed among the ranks.
We moved the requests forward in the permutation in increasing order of their contribution
to the sum of overlaps: requests with smaller overlaps are moved first. We tried versions
of SWO where the distance to move forward is proportional with the contribution to the
sum of overlaps or is fixed. However, these versions performed worse than the rank based
distance implementation described above. When minimizing conflicts in the schedule all
conflicts have an equal contribution to the objective function; therefore we decided to move
them forward for a fixed distance of five (we tried values between two and seven but five
was best).
4.6 Heuristic Biased Stochastic Sampling (HBSS)
HBSS (Bresina, 1996) is an incremental construction algorithm in which multiple rootto-leaf paths are stochastically generated. At each step, the HBSS algorithm needs to
heuristically choose the next request to schedule from the unscheduled requests. We used
the flexibility measure as described for SWO to rank the unscheduled requests. We compute
the flexibility for each request and order them in decreasing order of the flexibility; each
request is then given a rank according to this ordering (first request has rank 1, second
request rank 2, etc.). A bias function is applied to the ranks; as noted by Bresina (1996,
p.271), the choice of bias function “reflects the confidence one has in the heuristic’s accuracy
- the higher the confidence, the stronger the bias.” The flexibility heuristic is an effective
greedy heuristic for constructing solutions in AFSCN scheduling. Therefore we used a
relatively strong bias function, an exponential bias. For each rank r, the bias is computed:
bias(r) = e−r . The probability to select the unscheduled request with rank r is then
computed as:
bias(r)
P (r) = P
i∈Unscheduled bias(rank(i))
where Unscheduled represents the set of unscheduled requests.
Our implementation of HBSS does not re-compute the flexibility of the unscheduled tasks
every time we choose the next request to be scheduled. In other words, HBSS is building a
permutation of requests and then the schedule builder produces the corresponding schedule.
In terms of CPU time, this means that the time required by HBSS to build a solution is
similar to those of the other algorithms (dominated by the number of evaluations). A version
re-computing the flexibility of the unscheduled tasks as tasks are scheduled would be a lot
more expensive. In fact, for EOS which is a similar oversubscribed scheduling problem,
Globus et al. (2004) found that updating the heuristic values in HBSS while scheduling
was “hundreds of times slower than the permutation-based techniques, required far more
memory, and produced very poor schedules”.

588

Understanding Algorithm Performance

5. What Works Well?
A first step to understanding how best to solve a problem is to assess what methods perform
best. The results of running each of the algorithms are summarized in Tables 2 and 3
respectively. For Genitor, randomized local search (RLS) and Squeaky Wheel Optimization
(SWO), we report the best and mean value and the standard deviation observed over 30
runs, with 8000 evaluations per run. For HBSS, the statistics are taken over 240,000 samples.
Both Genitor and RLS were initialized from random permutations.
The best known values for the sum of overlaps (see Table 2) were obtained by running
Genitor with the population size increased to 400 and up to 50,000 evaluations; over hundreds of experiments using numerous algorithms, we have not found better solutions than
these. When we report that an algorithm is better than Genitor it means that it was better
than Genitor when both algorithms were limited to 8000 evaluations.
With the exception of Gooley’s algorithm, the CPU times are dominated by the number
of evaluations and therefore are similar. On a Dell Precision 650 with 3.06 GHz Xeon
running Linux, 30 runs with 8000 evaluations per run take between 80 and 190 seconds (for
more precise values, see Barbulescu et al., 2004).
The increase in the number of requests received for a day in the more recent R problems
causes an increase in the number and percentage of unscheduled requests. For the A problems, at most eight task requests (or 2.5% of the tasks) are not scheduled; between 97.5%
and 99% of the task requests are scheduled. For the R problems, at most 42 (or 8.7% of
the tasks) are not scheduled; between 91.3% and 97.2% of the tasks requests are scheduled.
To compare algorithm performance, our statistical analyses include Genitor, SWO, and
RLS. We also include in our analyses the algorithms SWO1Move (a variant of SWO we
explore in Section 6.5.2), and ALLS (a variant of Local Search we present in Section 7). We
judge significant differences of the final evaluations using an ANOVA for the five algorithms
on each of the recent days of data. All ANOVAs came back significant, so we are justified
in performing pair-wise tests. We examined a single-tailed, two sample t-test as well as the
non-parametric Wilcoxon Rank Sum test. The Wilcoxon test significance results were the
same as the t-test except in two pairs, so we only present p-values from the t-test that are
close to our rejection threshold of p ≤ .005 per pair-wise test 8 .
When minimizing conflicts, many of the algorithms find solutions with the best known
values. Pair-wise t-tests show that Genitor and RLS are not significantly different for R1,
R3, and R4. Genitor significantly outperforms RLS on R2 (p = .0023) and R5 (p = .0017).
SWO does not perform significantly different from RLS for all five days and significantly
outperforms Genitor on R5. Genitor significantly outperforms SWO on R2 and R4; however,
some adjusting of the parameters used to run SWO may fix this problem. It is in fact
surprising how well SWO performs when minimizing the conflicts, given that we chose a
very simple implementation, where all the tasks in conflict are moved forward with a fixed
distance. HBSS performs well for the A problems; however, it fails to find the best known
values for R1, R2 and R3. The original solution to the problem, Gooley’s, only computes a
single solution; its results can be improved by a sampling variant (see Section 6.2.1).
8. Five algorithms imply, at worst, 10 pair-wise comparisons per day of data. To control the experimentwise error, we use a (very conservative, but simple) Bonferroni adjustment; this adjustment is known to
increase the probability of a Type II error (favoring false acceptance that the distributions are similar).
At α = .05, we judge two algorithms as significantly different if p ≤ .005.

589

Barbulescu, Howe, Whitley, & Roberts

Day
A1
A2
A3
A4
A5
A6
A7
R1
R2
R3
R4
R5

Min
8
4
3
2
4
6
6
42
29
17
28
12

Genitor
Mean
8.6
4
3.03
2.06
4.1
6.03
6
43.7
29.3
17.63
28.03
12.03

SD
0.49
0
0.18
0.25
0.3
0.18
0
0.98
0.46
0.49
0.18
0.18

Min
8
4
3
2
4
6
6
42
29
17
28
12

RLS
Mean
8.7
4.0
3.1
2.2
4.7
6.16
6.06
44.0
29.8
18.0
28.36
12.4

SD
0.46
0
0.3
0.48
0.46
0.37
0.25
1.25
0.71
0.69
0.66
0.56

Min
8
4
3
2
4
6
6
43
29
18
28
12

SWO
Mean
8
4
3
2.06
4
6
6
43.3
29.96
18
28.3
12

SD
0.0
0.0
0.0
0.25
0.0
0.0
0.0
0.46
0.18
0.0
0.46
0

Min
8
4
3
2
4
6
6
45
32
19
28
12

HBSS
Mean
9.76
4.64
3.37
3.09
4.27
6.39
7.35
48.44
35.16
21.08
31.22
12.36

Gooley
SD
0.46
0.66
0.54
0.43
0.45
0.49
0.54
1.15
1.27
0.89
1.10
0.55

11
7
5
4
5
7
6
45
36
20
29
13

Table 2: Performance of Genitor, RLS, SWO, HBSS and Gooley’s algorithm in terms of
the best and mean number of conflicts. Statistics for Genitor, local search and
SWO are collected over 30 independent runs, with 8000 evaluations per run. For
HBSS, 240,000 samples are considered. Min numbers in boldface indicate best
known values.

When minimizing overlaps, RLS finds the best known solutions for all but two of the
problems. It significantly outperforms Genitor on R1 and R2, significantly under-performs
on R3, and does not significantly differ in performance on R4 and R5. RLS and SWO do not
perform significantly different except for R3 where RLS under-performs. SWO significantly
outperforms Genitor on all five days. However, if run beyond 8000 evaluations, Genitor
continues to improve the solution quality but SWO fails to find better solutions. HBSS
finds best known solutions to only a few problems. For comparison, we computed the
overlaps corresponding to the schedules built using Gooley’s algorithm and present them
in the last column of Table 3; however, Gooley’s algorithm was not designed to minimize
overlaps.
5.1 Progress Toward the Solution
SWO and Genitor apply different criteria to determine solution modifications. RLS randomly chooses the first shift resulting in an equally good or improving solution. To assess
the effect of the differences, we tracked the best value obtained so far when running the
algorithms. For each problem, we collected the best value found by SWO, Genitor and RLS
in increments of 100 evaluations, for 8000 evaluations. We averaged these values over 30
runs of SWO, RLS, and Genitor, respectively.
A typical example for each objective function is presented in Figures 2 and 3. For
both objective functions, the curves are similar, as is relative performance. SWO quickly
finds a good solution, then its performance levels off. RLS progresses quickly during the
first half of the search, while Genitor exacts smaller improvements. In the second half of
the search though, RLS takes longer to find better solutions, while Genitor continues to
steadily progress toward the best solution. The best so far for Genitor does not improve
590

Understanding Algorithm Performance

Day
A1
A2
A3
A4
A5
A6
A7
R1
R2
R3
R4
R5

Min
104
13
28
9
30
45
46
913
519
275
738
146

Genitor
Mean
106.9
13
28.4
9.2
30.4
45.1
46.1
987.8
540.7
292.3
755.4
146.5

SD
0.6
0.0
1.2
0.7
0.5
0.4
0.6
40.8
13.3
10.9
10.3
1.9

Min
104
13
28
9
30
45
46
798
494
250
725
146

RLS
Mean
106.76
13.66
30.7
10.16
30.83
45.13
49.96
848.66
521.9
327.53
755.46
147.1

SD
1.81
2.59
4.31
2.39
1.36
0.5
5.95
38.42
20.28
55.34
25.42
2.85

Min
104
13
28
9
30
45
46
798
491
265
731
146

SWO
Mean
104
13.4
28.1
13.3
30
45.1
46
841.4
503.8
270.1
736.2
146.0

SD
0.0
2.0
0.6
7.8
0.0
0.3
0.0
14.0
6.5
2.8
3.0
0.0

Min
128
43
28
9
50
45
83
1105
598
416
827
146

HBSS
Mean
158.7
70.1
52.5
45.7
82.6
65.5
126.4
1242.6
681.8
571.0
978.4
164.4

Gooley
SD
28.7
31.1
16.9
13.0
13.2
16.8
12.5
42.1
27.0
46.0
28.7
10.8

687
535
217
216
231
152
260
1713
1047
899
1288
198

Table 3: Performance of Genitor, local search, SWO, HBSS and Gooley’s algorithm in
terms of the best and mean sum of overlaps. All statistics are collected over 30
independent runs, with 8000 evaluations per run. For HBSS, 240,000 samples are
considered. Min numbers in boldface indicate best known values.

as quickly as the best so far for RLS. This is not unexpected: the best solution in the
Genitor population isn’t likely to improve frequently in the beginning of the run. In a
sense, tracking the evolution of the median in the population when running Genitor would
be more indicative of its progress; we use the best so far to allow for a uniform comparison
of the three algorithms.
We observe two differences in the objective functions. First, when minimizing the number of conflicts, both Genitor and RLS eventually equal or outperform SWO. For minimizing
overlaps, Genitor and RLS take longer to find good solutions; after 8000 evaluations, SWO
found the best solutions. Second, when minimizing the number of conflicts, toward the
end of the run, Genitor outperforms RLS. When minimizing overlaps, RLS performs better than Genitor. Best known solutions for the R problems when minimizing overlaps can
be obtained by running RLS for 50,000 evaluations in 30 runs. Running SWO for 50,000
evaluations in 30 runs results in small improvements, and on just two of the problems.

6. Hypotheses for Explaining Algorithm Performance
Genitor, SWO and RLS are the most successful algorithms we have tested on the AFSCN
problem. Although each operate in the same search space (permutations), they traverse the
space rather differently. The puzzle is how can all three be apparently well suited to this
problem. To solve the puzzle, first, we describe why plateaus are the dominant feature of the
search space. We show that the greedy schedule builder is the main reason for the presence
of the plateaus. Then, we test hypotheses that appear to follow from the dominance of the
plateaus and the characteristics of each algorithm.
In our study, the greedy schedule builder as well as the objective function are part of the
problem specification. Therefore, when formulating and testing our hypotheses, we consider
the search space features (such as the plateaus or the number of identical solutions) fixed.
591

36

Genitor
RLS
SWO

70

Average Best So Far Number of Bumps

Average Best So Far Number of Bumps

Barbulescu, Howe, Whitley, & Roberts

60

50

40

30
0

500

1000

1500
2000
2500
Evaluations

3000

3500

34
33
32
31
30
29
4000

4000

Genitor
RLS
SWO

35

4500

5000

5500
6000
6500
Evaluations

7000

7500

8000

Figure 2: Evolutions of the average best value for conflicts obtained by SWO, RLS and
Genitor during 8000 evaluations, over 30 runs. The left figure depicts the improvement in the average best value over the first 4000 evaluations. The last
4000 evaluations are depicted in the right figure; note that the scale is different
on the y-axis. The curves were obtained for R2.

850

Genitor
RLS
SWO

1600

Average Best So Far Sum of Overlaps

Average Best So Far Sum of Overlaps

1800

1400
1200
1000
800
600
400

0

500

1000

1500
Evaluations

2000

2500

3000

Genitor
RLS
SWO

800
750
700
650
600
550
500
3000

4000

5000
6000
Evaluations

7000

8000

Figure 3: Evolutions of the average best value for sum of overlaps obtained by SWO, RLS
and Genitor during 8000 evaluations, over 30 runs. The left figure depicts the
improvement in the average best value over the first 3000 evaluations. The last
5000 evaluations are depicted in the right figure; note that the scale is different
on the y-axis. The curves were obtained for R2.

6.1 Redundancy of the Search Space
More than a third of the neighbors in RLS result in exactly the same schedule for both the
overlaps and minimal conflicts evaluation functions (Barbulescu et al., 2004a; Barbulescu,
Whitley, & Howe, 2004c); more than 62% of neighbors in RLS result in the same evaluation
(see Section 6.4). The AFSCN search space is dominated by plateaus for three reasons.
592

Understanding Algorithm Performance

The main reason for the presence of plateaus is the greedy schedule builder: each request
is scheduled on the first available resource from its list of possible alternatives. For example,
consider a permutation of n−1 from the total of n requests. If the last request X is inserted
in the first position in the permutation and the schedule builder is applied, a schedule S is
obtained. We now scan the permutation of n − 1 requests from left to right, successively
inserting X in the second position, then the third and so on, building the corresponding
schedule. As long as none of the requests appearing before X in the permutation require
the particular spot occupied by X in S as their first feasible alternative to be scheduled, the
same schedule S will be obtained. This happens for two reasons: 1) the requests are inserted
in the schedule in the order in which they appear in the permutation and 2) the greedy
schedule builder considers the possible alternatives in the order in which they are specified
and accepts the first alternative for which the request can be scheduled. Let k + 1 be the
first position to insert X that will alter S; this means that the first feasible alternative to
schedule the request in position k overlaps with the spot occupied by X in S. When X
is inserted in position k + 1, a new schedule S1 is obtained; the same schedule S1 will be
built by inserting X in subsequent positions, until encountering a request for which its first
feasible alternative overlaps with the spot occupied by X in S1, etc. This example also
shows that shifting in a permutation might not change the corresponding schedule.
To address the presence of the plateaus in the search space as a result of the greedy
schedule builder, we could have used some randomization scheme to diversify the scheduler.
However, randomization when implementing a schedule builder can result in problems because of the unpredictability of the value assigned to a permutation. For example, Shaw and
Fleming (1997) argue that the use of randomization in a schedule builder can be detrimental to the performance of a genetic algorithm when an indirect representation is used (for
which the chromosomes are not schedules, as is the case of Genitor for AFSCN scheduling).
They support this idea by noting that in general, genetic algorithms rely on the preservation
of the good fitness values. Also, for SWO, randomization in the schedule builder changes
the significance of reprioritization from one iteration to the next one. If the scheduler is
randomized, the new order of requests is very likely to result in a schedule that is not
the “repaired version” of the previous one. If the same permutation of requests can be
transformed into multiple different schedules because of the nondeterministic nature of the
scheduler, the SWO mechanism will not operate as intended.
A second reason for the plateaus in the search space is the presence of time windows.
If a request X needs to be scheduled sometime at the end of the day, even if it appears
in the beginning of the permutation, it will still occupy a spot in the schedule towards the
end (assuming it can be scheduled) and therefore, after most of the other requests (which
appeared after X in the permutation).
A third reason is the discretization of the objective function. Clearly, the range of
conflicts is a small number of discrete values (with a weak upper bound of the number
of tasks). The range for overlaps is still discrete but is larger than for conflicts. Using
overlaps as the evaluation function, approximately 20 times more unique objective function
values are observed during search compared to searches where the objective is to minimize
conflicts. The effect of the discretization can be seen in the differing results using the two
objective functions. Thus, one reason for including both in our studies is to show some of
the effects of the discretization.
593

Barbulescu, Howe, Whitley, & Roberts

6.2 Does Genitor Learn Patterns of Request Ordering?
We hypothesize that Genitor performs well because it discovers which interactions between
the requests matter. We examine sets of permutations that correspond to schedules with the
best known values and identify chains of common request orderings in these permutations,
similar in spirit to the notion of backbone in SAT (e.g., Singer et al., 2000). The presence
of such chains would support the hypothesis that Genitor is discovering patterns of request
orderings. This is a classic building block hypothesis: some pattern that is present in
parent solutions contributes to their evaluation in some critical way; these patterns are
then recombined and inherited during genetic recombination (Goldberg, 1989).
6.2.1 Common Request Orderings
One of the particular characteristics of the AFSCN scheduling problem is the presence of
two categories of requests. The low altitude requests have fixed start times and specify only
one to three alternative resources. The high altitude requests implicitly specify multiple
possible start times (because their corresponding time windows are usually longer than the
duration that needs to be scheduled) and up to 14 possible alternative resources. Clearly
the low altitude requests are more constrained. This suggests a possible solution pattern,
where low altitude requests would be scheduled first.
To explore the viability of such a pattern, we implemented a heuristic that schedules
the low altitude requests before the high altitude ones; we call this heuristic the “split
heuristic”. We incorporated the split heuristic in the schedule builder: given a permutation
of requests, the new schedule builder first schedules only the low altitude requests, in the
order in which they appear in the permutation. Without modifying the position of the low
altitude requests in the schedule, the high altitude requests are then inserted in the schedule,
again in the order in which they appear in the permutation. The idea of scheduling low
altitude requests before high altitude requests was the basis of Gooley’s heuristic (1993).
Also, the split heuristic is similar to the contention measures defined by Frank et al. (2001).
Some of the results we obtained using the split heuristic are surprising: when minimizing
conflicts, best known valued schedules can be obtained quickly for the A problems by simply
sampling a small number of random permutations. The results obtained by sampling 100
random permutations are shown in Table 4.
While such performance of the split heuristic does not transfer to the R problems or when
minimizing the number of overlaps, the results in Table 4 offer some indication of a possible
request ordering pattern in good solutions. Is Genitor in fact performing well because it
discovers that scheduling low before high altitude requests produces good solutions?
As a more general explanation for Genitor’s performance, we hypothesize that Genitor
is discovering patterns of request ordering: certain requests that must come before other
requests. To test this, we identify common request orderings present in solutions obtained
from multiple runs of Genitor. We ran 1000 trials of Genitor and selected the solutions
corresponding to best known values. First, we checked for request orderings of the form
“requestA before requestB” which appear in all the permutations corresponding to best
known solutions for the A problems and corresponding to good solutions for the R problems.
The results are summarized in Table 5. The Sol. Value columns show the value of the
solutions chosen for the analysis (out of 1000 solutions). The number of solutions (out of
594

Understanding Algorithm Performance

Day
A1
A2
A3
A4
A5
A6
A7

Best
Known
8
4
3
2
4
6
6

Random Sampling-S
Min Mean Stdev
8
8.2
0.41
4
4
0
3
3.3
0.46
2
2.43
0.51
4
4.66
0.48
6
6.5
0.51
6
6
0

Table 4: Results of running random sampling with the split heuristic (Random SamplingS) in 30 experiments, by generating 100 random permutations per experiment for
minimizing conflicts.

1000) corresponding to the chosen value is shown in the # of Solutions columns. When
analyzing the common pairs of request orderings for minimizing the number of conflicts, we
observed that most pairs specified a low altitude request appearing before a high altitude
one. Therefore, we separate the pairs into two categories: pairs specifying a low altitude
request before a high altitude requests (column: (Low,High) Pair Count) and the rest
(column: Other Pairs). For the A problems, the results clearly show that most common
pairs of ordering requests specify a low altitude request before a high altitude request. For
the R problems, more “Other pairs” can be observed. In part, this might be due to the
small number of solutions corresponding to the same value (only 25 out of 1000 for R1 when
minimizing conflicts). The small number of solutions corresponding to the same value is also
the reason for the big pair counts reported when minimizing overlaps for the R problems.
We know that for the A problems the split heuristic results in best-known solutions when
minimizing conflicts; therefore, the results in Table 5 are somewhat surprising. We expected
to see more low-before-high common pairs of requests for the A problems when minimizing
the number of conflicts; instead, the pair counts are similar for the two objective functions.
Genitor seems to discover patterns of request interaction, and most of them specify a low
altitude request before a high altitude request.
The results in Table 5 are heavily biased by the number of solutions considered 9 . Indeed,
let s denote the number of solutions of identical value (the number in column # of Solutions).
Also, let n denote the total number of requests. Suppose there are no preferences of orderings
between the tasks in good solutions. For a request ordering A before B there is a probability
of 1/2 that it will be present in one of the solutions, and therefore, a probability of 1/2 s that
it will be present in all s solutions. Given that there exist n ∗ (n − 1) possible precedences,
the expected number of common orderings if no preferences of orderings between tasks
exist is n(n − 1)/2s . For the A problems and for R5, s >= 420. The expected number
of common orderings assuming no preferences of orderings between tasks exist is smaller
than n(n − 1)/2420 , which is negligible. Therefore, the number of actually detected common
9. We wish to thank the anonymous reviewer of an earlier version of this work for this insightful observation;
the rest of the paragraph is based on his/her comments.

595

Barbulescu, Howe, Whitley, & Roberts

Day
A1
A2
A3
A4
A5
A6
A7
R1
R2
R3
R4
R5

Sol.
Value
8
4
3
2
4
6
6
43
29
17
28
12

Minimizing Conflicts
# of
(Low,High)
Solutions Pair Count
420
77
1000
29
936
86
937
132
862
45
967
101
1000
43
25
2166
573
64
470
78
974
54
892
57

Other
Pairs
1
1
1
3
9
10
3
149
5
21
16
10

Sol.
Value
107
13
28
9
30
45
46
947
530
285
744
146

Minimizing Overlaps
# of
(Low,High)
Solutions Pair Count
922
78
959
50
833
72
912
117
646
48
817
124
891
57
15
2815
30
1597
37
1185
31
1240
722
109

Other
Pairs
7
3
10
5
17
10
11
1222
308
400
347
11

Table 5: Common pairs of request orderings found in permutations corresponding to best
known/good Genitor solutions for both objective functions.

precedences (approximately 30 to 125 for low before high pairs and anywhere from 1 to
17 for the others) seem to be actual request patterns. This is also the case for the other
R problems. Indeed, for example, for R1, when s = 15, the expected number of common
orderings if no preferences of orderings between tasks exist is 7.1, while the number of
actually detected precedences is 2815 for low before high and 1222 for the other pairs.
The experiment above found evidence to support the hypothesis that Genitor solutions
exhibit patterns of low before high altitude requests. Given this result, we next investigate
if the “split” heuristic (always scheduling low before high altitude requests) can enhance the
performance of Genitor. To answer this question, we run a second experiment using Genitor,
where the split heuristic schedule builder is used to evaluate every schedule generated during
the search.
Table 6 shows the results of using the split heuristic with Genitor on the R problems.
Genitor with the split heuristic fails to find the best-known solution for R2 and R3. This
is not surprising: in fact, we can show that scheduling all the low altitude requests before
high altitude requests may prevent finding the optimal solutions.
The results for minimizing sum of overlaps are shown in Table 7. With the exception
of A3, A4 and A6, Genitor using the split heuristic fails to find best known solutions for
the A problems. For the R problems, using the split heuristic actually improves the results
obtained by Genitor for R1 and R2; it should be noted that the R1 and R2 solutions are
not as good as those found by RLS using 8000 evaluation however. Thus a search that
hybridizes the genetic algorithm with a schedule builder using the split heuristic sometimes
helps and sometimes hurts in terms of finding good solutions.
We attempted to identify longer chains of common request ordering. We were not successful: while Genitor does seem to discover patterns of request ordering, multiple different
patterns of request orderings can result in the same conflicts (or even the same schedule).
596

Understanding Algorithm Performance

Day
R1
R2
R3
R4
R5

Best
Known
42
29
17
28
12

Genitor with New
Schedule Builder
Min Mean Stdev
42
42
0
30
30
0
18
18
0
28
28
0
12
12
0

Table 6: Minimizing conflicts: results of running Genitor with the split heuristic in 30 trials,
with 8000 evaluations per trial.

Day
A1
A2
A3
A4
A5
A6
A7
R1
R2
R3
R4
R5

Best
Known
104
13
28
9
30
45
46
774
486
250
725
146

Genitor with New
Schedule Builder
Min Mean Stdev
119
119
0.0
43
43
0.0
28
28
0.0
9
9
0.0
50
50
0.0
45
45
0.0
69
69
0.0
907 924.33 6.01
513 516.63 5.03
276 276.03 0.18
752 752.03 0.0
146
146
0.0

Table 7: Minimizing sum of overlaps: results of running Genitor with the split heuristic
using the split heuristic schedule builder to evaluate each schedule. The results
are based on 30 experiments, with 8000 evaluations per experiment.

We could think of these patterns as building blocks. Genitor identifies good building blocks
(orderings of requests resulting in good partial solutions) and propagates them into the final
population (and the final solution). Such patterns are essential in building a good solution.
However, the patterns are not ubiquitous (not all of them are necessary) and, therefore,
attempts to identify them across different solutions produced by Genitor have failed.
597

Barbulescu, Howe, Whitley, & Roberts

Day
A1
A2
A3
A4
A5
A6
A7
R1
R2
R3
R4
R5

Minimizing Conflicts
Best Known Min Mean Stdev
8
8
8.0
0.0
4
4
4.0
0.0
3
3
3.16
0.46
2
2
2.13
0.34
4
4
4.03
0.18
6
6
6.23
0.63
6
6
6.0
0.0
42
42* 43.43
0.56
29
30
30.1
0.3
17
17* 17.73
0.44
28
28
28.53
0.57
12
12
13.1
0.4

Minimizing Overlaps
Best Known Min Mean Stdev
104
104 104.46
0.68
13
13
13.83
1.89
28
28
30.13
1.96
9
9
11.66
1.39
30
30
30.33
0.54
45
45
48.3
6.63
46
46
46.26
0.45
774
851 889.96 31.34
486
503
522.2
9.8
250
268
276.4
4.19
725
738 758.26 12.27
146
147 151.03
2.19

Table 8: Statistics for the results obtained in 30 runs of SWO initialized with random
permutations (i.e., RandomStartSWO), with 8000 evaluations per run. The mean
and best value from 30 runs as well as the standard deviations are shown. The
entries with a ∗ indicate values that are better than the corresponding SWO values.
For each problem, the best known solution for each objective function is also
included.

6.3 Is SWO’s Performance Due to Initialization?
The graphs of search progress for SWO (Figures 2 and 3) show that it starts with much
better solutions than do the other algorithms. The initial greedy solution for SWO translated into best known values for five problems (A2, A3, A5, A6 and R5) when minimizing
the number of conflicts and for two problems (A6 and R5) when minimizing overlaps.
How important is the initial greedy permutation for SWO? To answer this question, we
replaced the initial greedy permutation (and its variations in subsequent iterations of SWO)
with random permutations and then used the SWO mechanism to iteratively move forward
the requests in conflict. We call this version of SWO RandomStartSWO. We compared the
results produced by RandomStartSWO with results from SWO to assess the effects of the
initial greedy solution. The results produced by RandomStartSWO are presented in Table 8.
The entries with a ∗ indicate that RandomStartSWO produced a better result than SWO.
With the exception of R2, when minimizing the number of conflicts, best known values are
obtained by RandomStartSWO for all the problems. In fact, for R1 and R3, the best results
obtained are slightly better than the best found by SWO. When minimizing the sum of
overlaps, best known values are obtained for the A problems; only for the R problems, the
performance of SWO worsens when it is initialized with a random permutation. However,
RandomStartSWO still performs better or as well as Genitor (with the exception of R2
when minimizing the number of conflicts and R5 for overlaps) for both objective functions.
These results suggest that the initial greedy permutation is not the main performance factor
for SWO: the performance of RandomStartSWO is competitive with that of Genitor.
598

Understanding Algorithm Performance

Day
A1
A2
A3
A4
A5
A6
A7
R1
R2
R3
R4
R5

Total
Neighbors
103041
90601
96100
100489
92416
88804
87616
232324
207936
180625
184900
174724

Minimizing
Random Perms
Mean
Avg %
87581.1
84.9
79189.3
87.4
82937
86.8
84759
84.3
77952
84.3
74671.5
84.0
76489.6
87.3
189566
81.5
173434
83.4
153207
84.8
157459
85.1
154347
88.3

Conflicts
Optimal Perms
Mean
Avg %
91609.1
88.9
83717.9
92.4
84915.4
88.9
87568.2
87.1
82057.4
88.7
78730.3
88.6
79756.5
91.0
190736
82.0
177264
85.2
156413
86.5
162996
88.1
159581
91.3

Minimizing
Random Perms
Mean
Avg %
75877.4
73.6
70440.9
77.7
73073.3
76.5
72767.7
72.4
67649.3
73.2
63667.4
71.6
67839
77.4
145514
62.6
137568
66.1
126511
70.0
130684
70.6
133672
76.5

Overlaps
Optimal Perms
Mean
Avg %
88621.2
86.0
81141.9
89.5
82407.7
86.3
85290
84.8
79735.9
86.2
75737.9
85.2
77584.3
88.5
160489
69.0
160350
77.1
139012
76.9
145953
78.9
152629
87.3

Table 9: Statistics for the number of neighbors resulting in schedules of the same value
as the original, over 30 random and optimal permutations, for both objective
functions

6.4 Is RLS Performing a Random Walk?
RLS spends most of the time traversing plateaus in the search space (by accepting nonimproving moves). In this section, we study the average length of random walks on the
plateaus encountered by local search. We show that as search progresses the random walks
become longer before finding an improvement, mirroring the progress of RLS. We note that
a similar phenomenon has been observed for SAT (Frank, Cheeseman, & Stutz, 1997).
More than a third of all shifting pairs of requests result in schedules identical with
the current solution (Barbulescu et al., 2004a, 2004c). However, an even larger number of
neighbors result in different schedules with the same value as the current solution. This
means that most of the accepted moves during search are non-improving moves; search ends
up randomly walking on a plateau until an exit is found. We collected results about the
number of schedules with the same value as the original schedule, when perturbing the solutions in all possible pairwise changes. Note that these schedules include the ones identical
with the current solution. The results are summarized in Table 9. We report the average
percentage of neighbors identical in value with the original permutation. The results show
that: 1) More than 84% of the shifts result in schedules with the same value as the original one, when minimizing conflicts. When minimizing overlaps, more than 62% (usually
around 70%) of the shifts result in same value schedules. 2) Best known solutions have
slightly more same-value neighbors than do random permutations; the difference is statistically significant when minimizing overlaps. This suggests that the plateaus corresponding
to good values in the search space might be larger in size than the plateaus corresponding
to random permutations.
To assess the size of the plateaus and their impact on RLS, we performed random walks
at fixed intervals during RLS. At every 500 evaluations of RLS, we identified the current
599

Barbulescu, Howe, Whitley, & Roberts

solution Crt. For each such Crt, we performed 100 iterations of local search starting from
Crt and stopping as soon as a better solution or a maximum number of equally good
solutions were encountered. For the A problems, best known solutions are often found early
in the search; most of the 100 iterations of local search started from such a Crt would reach
the maximum number of equally good solutions. Therefore, we chose a limit of 1000 steps
on the plateau for the A problems and 8000 steps for the R problems. We averaged the
number of equally good solutions encountered during the 100 trials of search performed for
each Crt; this represents the average number of steps needed to find an exit from a plateau.
Figure 4 displays the results obtained for R4; similar behavior was observed for the rest
of the problems. Note that we used a log scale on the y axis for the graph corresponding to
minimizing overlaps: most of the 100 walks performed from the current solution of value 729
end up taking the maximum number of steps allowed (8000) without finding an exit from the
plateau. Also, the random walk steps counts only equal moves; the number of evaluations
needed by RLS (x-axis) is considerably higher due to needing to check detrimental moves
before accepting equal ones. The results show that large plateaus are present in the search
space; improving moves lead to longer walks on lower plateaus, which when detrimental
moves are factored in, appears to mirror the performance of RLS.
1800

10000

LS

729

729

LS
729 729

1600
29

29

1200
30

29

1000
800

30

30

600
32

31

29

1000
Average number steps plateau

Average number steps plateau

1400

29

30

30

30

782

814
100

794

786

786

777

751

740

864
1068

944

10

400

1450
200
0

33
63
0

45
1000

2000

3000

4000
Evals

5000

6000

7000

8000

1

0

1000

2000

3000

4000
Evals

5000

6000

7000

8000

Figure 4: Average length of the random walk on plateaus when minimizing conflicts (left)
or overlaps (right) for a single local search run on R4. The labels on the graphs
represent the value of the current solution. Note the log scale on the y axis for
the graph corresponding to minimizing overlaps. The best known value for this
problem is 28 when minimizing conflicts and 725 when minimizing overlaps.

For the AFSCN scheduling problems, most of the states on a plateau have at least one
neighbor that has a better value (this neighbor represents an exit). However, the number of
such exits is a very small percentage of the total number of neighbors, and therefore, local
search has a very small probability of finding an exit. Using the terminology introduced
by Frank et al. (1997), most of the plateaus encountered by search in the AFSCN domain
would be classified as benches, meaning that exits to states at lower levels are present. If
there are no exits from a plateau, the plateau is a local minimum. Determining which of the
plateaus are local minima (by enumerating all the states on the plateau and their neighbors)
600

Understanding Algorithm Performance

is prohibitive because of the large size of the neighborhoods and the large number of equally
good neighbors present for each state in the search space. Instead, we focus on the average
length of the random walk on a plateau as a factor in local search performance. The length
of the random walk on the plateau depends on two features: the size of the plateau and
the number of exits from the plateau. Preliminary investigations show that the number of
improving neighbors for a solution decreases as the solution becomes better - therefore we
conjecture that there are more exits from higher level plateaus than from the lower level
ones. This would account for the trend of needing more steps to find an exit when moving
to lower plateaus (corresponding to better solutions). It is also possible that the plateaus
corresponding to better solutions are larger in size; however, enumerating all the states on
a plateau for the AFSCN domain is impractical (following a technique developed by Frank
et al., 1997, just the first iteration of breadth first search would result in approximately
0.8 ∗ (n − 1)2 states on the same plateau).
6.5 Are Long Leaps Instrumental?
As in other problems with large plateaus (e.g., research published by Gent and Walsh,
1995 on SAT), we hypothesize that long leaps in the search space are instrumental for an
algorithm to perform well on AFSCN scheduling. SWO is moving forward multiple requests
that are known to be problematic. The position crossover mechanism in Genitor can be
viewed as applying multiple consecutive shifts to the first parent, such that the requests in
the selected positions of the second parent are moved into those selected positions of the
first. In a sense, each time the crossover operator is applied, a multiple move is proposed for
the first parent. We hypothesize that this multiple move mechanism present in both SWO
and Genitor allows them to make long leaps in the space and thus reach solutions fast.
Note that if we knew exactly which requests to move, moving forward only a small
number of requests (or even only one) might be all that is needed to reach the solutions
quickly. Finding which requests to move is difficult; in fact we studied the performance of
a more informed move operator that only moves requests into positions which guarantee
schedule changes (Roberts et al., 2005). We found surprising results: the more informed
move operator performs worse than the random unrestricted shift employed by RLS. We
argue that the multiple moves are a desired algorithm feature as they make it more likely
that one of the moves will be the right one.
To investigate our hypothesis about the role of multiple moves when traversing the
search space, we perform experiments with a variable number of moves at each step for
both Genitor and SWO. For Genitor, we vary the number of crossover positions allowed.
For SWO, we vary the number of requests in conflict moved forward.
6.5.1 The Effect of Multiple Moves on Genitor
To test the effect of multiple moves on Genitor, we change Syswerda’s position crossover
by imposing a fixed number of selected positions in the second parent (see Section 4.4 for a
description of Syswerda’s position crossover). We call this implementation Genitor-k where
k is the number of selected positions. Recall that our implementation of Syswerda’s position
crossover randomly selects a number of positions that is larger than one third and smaller
than two thirds of the total number of positions. If multiple moves are indeed a factor in
601

Barbulescu, Howe, Whitley, & Roberts

Average Best So Far Sum of Overlaps

2000

Genitor
Genitor-10
Genitor-50
Genitor-100
Genitor-150
Genitor-300
Genitor-350

1900
1800
1700
1600
1500
1400
1300
1200

0

500

1000

1500 2000 2500
Evaluations

Average Best So Far Sum of Overlaps

1600

3000

3500

4000

Genitor
Genitor-10
Genitor-50
Genitor-100
Genitor-150
Genitor-300
Genitor-350

1500
1400
1300
1200
1100
1000
900
4000

4500

5000

5500 6000 6500
Evaluations

7000

7500

8000

Figure 5: Evolutions of the average best value obtained by Genitor and its versions with a
fixed number of selected positions for crossover. during 8000 evaluations, over 30
runs. The graphs were obtained for R1; best solution value is 773.

performance then increasing the number of selected positions up to a point should result in
finding improvements faster. If only a few positions are selected, the offspring will be very
similar to the first parent. If the number of selected positions is large, close to the number
of total requests, the offspring will be very similar to the second parent. If the offspring is
very similar to one of its two parents, we expect a slower rate in finding improvements to the
current best solution. Therefore, both for small and for large k values, we expect Genitor-k
602

Understanding Algorithm Performance

Day
A1
A2
A3
A4
A5
A6
A7
R1
R2
R3
R4
R5

Min
11
5
6
5
5
9
8
57
42
27
36
13

Genitor-10
Mean Stdev
14.93
1.94
7.13
1.77
10.4
2.12
10.66
2.7
9.6
2.29
12.63
1.8
10.6
1.75
66.5
4.38
47.16
3.59
31.1
2.41
41.9
2.74
20.73
2.53

Min
8
4
3
2
4
6
6
47
32
19
28
12

Genitor-50
Mean Stdev
9.26
0.63
4.03
0.18
3.36
0.55
3.13
0.81
4.73
0.69
6.83
0.94
6.1
0.30
52.0
2.82
34.53
1.47
21.6
1.67
30.96
2.04
13.23
0.81

Min
8
4
3
2
4
6
6
42
29
17
28
12

Genitor-100
Mean Stdev
8.66
0.47
4.0
0.0
3.0
0.0
2.23
0.50
4.26
0.44
6.03
0.18
6.0
0.0
45.83
1.68
30.0
0.78
18.03
0.61
28.33
0.47
12.46
0.62

Genitor-150
Min Mean Stdev
8
8.53
0.5
4
4.0
0.0
3
3.0
0.0
2
2.06
0.25
4
4.2
0.4
6
6.06
0.25
6
6.0
0.0
42
44.36
1.24
29
29.6
0.56
17
17.63
0.61
28
28.1
0.4
12
12.2
0.4

Table 10: Performance of Genitor-k, where k represents the fixed number of selected positions for Syswerda’s position crossover, in terms of the best and mean number
of conflicts. Statistics are taken over 30 independent runs, with 8000 evaluations
per run. Min numbers in boldface indicate best known values.

to find improvements at a much slower rate than Genitor or Genitor-k with average k values
(values closer to half of the number of requests).
For our study, we run Genitor-k with k=10, 50, 100, 150, 200, 250, 300 and 350. We
allowed 8000 evaluations per run and performed 30 runs for each problem. The results are
summarized in Tables 10 and 11 for minimizing the number of conflicts and in Tables 12
and 13 for minimizing the sum of overlaps. Note that for A6 and A7 there are 299 and 297
requests to schedule respectively. Therefore Genitor-k with k = 300 and k = 350 cannot be
run for these two problems. Also note that for example, k = 200 does not mean that there
are 200 differences in the selected positions between the two parents. The offspring is likely
to be very similar to its parents, regardless of the value of k, when the parents are similar.
When minimizing the number of conflicts, the worst results are produced by k = 10. For
k = 50, the results improve, best knowns are found for all of the A problems; however, for R1,
R2, and R3, the best knowns are not found. Starting with k = 100 up to k = 250 Genitor-k
finds best known solutions for all the problems. The means and standard deviations are also
very similar for all these k values; the smallest means and standard deviations correspond
to k = 200 for the A problems and k = 250 for the R problems (with the exception of R3
for which k = 200 produces better results). For k = 300, the best knowns are not found
anymore for the A problems; 300 is very close to the size of the five A problems for which
it is feasible to run Genitor-300. The decay in performance is not as significant for the R
problems: there is an increase in the means and standard deviations for k = 300 and k = 350;
however, best knowns are still found for four out of the five problems. Note that k = 400
would have been a lot closer to the total number of requests for the R problems; we believe
the performance would have degraded more for the R problems for larger k values. When
minimizing overlaps, we observe trends that are very similar to the ones for minimizing the
number of conflicts. k = 10 produces poor results, followed by k = 50. Similar results are
603

Barbulescu, Howe, Whitley, & Roberts

Day
A1
A2
A3
A4
A5
A6
A7
R1
R2
R3
R4
R5

Min
8
4
3
2
4
6
6
42
29
17
28
12

Genitor-200
Mean Stdev
8.56
0.56
4.0
0.0
3.0
0.0
2.0
0.0
4.3
0.46
6.06
0.25
6.0
0.0
44.03
1.15
29.36
0.49
17.33
0.4
28.03
0.18
12.1
0.3

Min
8
4
3
2
4
6
6
42
29
17
28
12

Genitor-250
Mean Stdev
8.9
0.3
4.03
0.18
3.06
0.25
3.13
0.81
4.73
0.58
6.5
0.57
6.06
0.25
44.03
0.85
29.4
0.49
17.7
0.65
28
0.0
12.06
0.25

Min
9
9
4
4
10
43
29
17
28
12

Genitor-300
Mean Stdev
11.8
1.66
13.66
1.76
9.2
2.1
8.56
1.94
13.86
2.14
44.26
1.01
29.7
0.59
17.73
0.58
28.03
0.18
12.16
0.37

Genitor-350
Min Mean Stdev
43
45.46
1.22
29
30.13
0.86
17
18.63
0.8
28
28.63
0.71
12
12.6
0.81

Table 11: Performance of Genitor-k, where k represents the fixed number of selected positions for Syswerda’s position crossover, in terms of the best and mean number
of conflicts. Statistics are collected over 30 independent runs, with 8000 evaluations per run. Min numbers in boldface indicate best known values. The
dashes indicate that the permutation solutions for A6 and A7 are shorter than
300 (299 and 297, respectively), and therefore cannot select 300 positions in these
permutations.

Day
A1
A2
A3
A4
A5
A6
A7
R1
R2
R3
R4
R5

Min
149
30
51
59
43
94
67
1321
743
480
866
208

Genitor-10
Mean Stdev
221.53
38.85
69.66
29.22
122.86
36.12
124.5
42.25
90.7
32.01
145.06
33.12
115.66
27.96
1531.13 107.35
961.13
81.62
652.5
90.37
1069.23
74.65
309.03
46.3

Min
107
13
28
9
30
45
46
987
557
319
768
146

Genitor-50
Mean Stdev
115.76 11.53
15.73
3.86
36.26
8.19
19.36
9.3
33.06
3.62
49.6
5.54
51.7
7.89
1139.5 76.57
643.86
50.0
391.56 47.31
840.23 38.79
172.13 18.18

Min
107
13
28
9
30
45
46
914
515
268
735
146

Genitor-100
Mean Stdev
107.2
0.76
13.43
1.54
28.9
1.72
9.23
0.72
30.36
0.96
45.36
0.8
46.5
2.23
991.13 38.19
549.1
18.8
305.3 20.63
757.43 15.95
151.53
7.63

Min
107
13
28
9
30
45
46
915
516
269
731
146

Genitor-150
Mean Stdev
107.1
0.54
13.03
0.18
28.16
0.64
9.06
0.36
30.43
0.5
45.16
0.46
47.63
3.9
963.96 26.89
540.86 15.82
291.3 13.36
752.7 14.07
148.23
5.26

Table 12: Performance of Genitor-k, where k represents the fixed number of selected positions for Syswerda’s position crossover, in terms of the best and mean sum of
overlaps. Statistics are collected over 30 independent runs, with 8000 evaluations
per run. Min numbers in boldface indicate best known values.

produced by k = 100, 150, 200, 250. k = 150 results in the smallest means and standard
deviations for the A problems, while k = 200 and k = 250 produce best results for the R
problems. For k = 300 and k = 350, similarly to minimizing the number of conflicts, the
604

Understanding Algorithm Performance

Day
A1
A2
A3
A4
A5
A6
A7
R1
R2
R3
R4
R5

Min
107
13
28
9
30
45
46
878
512
268
730
146

Genitor-200
Mean Stdev
107.1
0.74
13.2
0.92
28.9
1.6
9.1
0.4
30.6
1.3
46.33
2.7
47.63
4.2
970.1 38.38
538.43 13.94
287.96 11.05
752.1 12.25
147.633
2.95

Min
107
13
28
9
30
45
46
914
511
270
734
146

Genitor-250
Mean Stdev
108.03
2.22
17.0
5.8
31.63
4.47
10.36
3.41
31.56
2.22
50.96
8.82
49.93
5.39
968.63 31.59
538.93 12.88
292.23 12.85
754.53 11.89
147.96
3.7

Min
113
116
63
37
76
935
526
272
745
146

Genitor-300
Mean Stdev
157.66 26.21
185.56 33.27
106.23 26.21
78.06 25.78
160.0 36.59
986.7
37.9
551.63 12.27
299.43
16.3
764 13.36
148.6
3.84

Min
927
532
299
743
146

Genitor-350
Mean Stdev
1008.8 42.17
559.46
19.7
332.46 20.14
785.36 26.63
157
10.6

Table 13: Performance of Genitor-k, where k represents the fixed number of selected positions for Syswerda’s position crossover, in terms of the best and mean sum of
overlaps. Statistics are taken over 30 independent runs, with 8000 evaluations
per run. Min numbers in boldface indicate best known values.

means and standard deviations increase and so do the best solutions found; best knowns
are only found for R5.
In terms of the evolution to the solution, we observe very similar trends for the two
objective functions. A typical examples is presented in Figure 5 (minimizing overlaps for
R1). Genitor-k with k = 10 is slower in finding improvements than k = 50 which is slower
than k = 100. k = 150 up to k = 250 are performing similarly and also similar to the
original Genitor implementation. k = 300 is still moving through the space at a rate that’s
similar to Genitor’s. Only for k = 350 does the performance start to decay.
The original implementation of the crossover operator (with a variable number of selected position) was shown to work well not only for our domain but also for other scheduling
applications (Syswerda, 1991; Watson, Rana, Whitley, & Howe, 1999; Syswerda & Palmucci, 1991). For our test problems, the results in this subsection show that the number of
crossover positions influences the performance of Genitor, both in terms of best solutions
found and in terms of the rate of finding improvements. For a small number of crossover
positions (10 or 50), the solutions found are not competitive, and the improvements are
found at a slower rate than in the original Genitor implementation. Similarity to Genitor’s
original performance is obtained for k values between 100 and 250. Higher k values result
in a decay in performance. These results also offer an empirical motivation for the choice
of the number of crossover positions in the original Genitor implementation. Indeed, in
the original implementation, the crossover uses a number of positions randomly selected
between one third and two thirds of the total number of requests. This translates for the
sizes of problems in our sets to a number of positions that is approximately between 100
and 300.
605

Barbulescu, Howe, Whitley, & Roberts

Day
A1
A2
A3
A4
A5
A6
A7
R1
R2
R3
R4
R5

Minimizing Conflicts
Min Mean Stdev
8
8
0
4
4
0
3
3
0
2
2
0
4
4
0
6
6
0
6
6
0
42* 43.4
0.7
29 29.9
0.3
18 18
0
28 28.1
0.3
12 12
0

Minimizing Overlaps
Min Mean Stdev
104 104
0
13 13
0
28 28
0
9
9
0
30 30
0
45 45
0
46 46
0
872 926.7 22.1
506 522.9 8.9
271 283.0 6.1
745 765.2 10.7
146 146
0

Table 14: Performance of a modified version of SWO where only one request is moved
forward for a constant distance 5. For both minimizing conflicts and minimizing
the sum of overlaps, the request is randomly chosen. All statistics are collected
over 30 independent runs, with 8000 evaluations per run. A ∗ indicates that the
best value is better than the corresponding SWO result. Min numbers in boldface
indicate best known values.

6.5.2 The Effect of Multiple Moves on SWO
We hypothesize that the multiple moves present in SWO are necessary for its performance.
To test this hypothesis, we start by investigating the effect of moving forward only one
request. This is somewhat similar to the shifting operator present in RLS: a request is shifted
forward in the permutation. However, to implement the SWO reprioritization mechanism,
we restrict both the chosen request to be moved and the position where it gets moved. For
minimizing the conflicts, one of the bumped requests is randomly chosen; for minimizing
overlaps, one of the requests contributing to the sum of overlaps is randomly chosen. For
both minimizing the conflicts and minimizing the sum of overlaps the chosen request is
moved forward for a constant distance of five 10 . We call this new algorithm SWO1Move.
The results obtained by running SWO1Move for 30 runs with 8000 evaluations per run are
presented in Table 14. The entries with a ∗ indicate that the value produced by SWO1Move
is better than the corresponding SWO result. The initial solutions are identical to the
solutions produced using the flexibility heuristic for initializing SWO.
When minimizing conflicts, SWO1Move performs as well as SWO (in fact, it finds the
best known solution for R1 as well). When minimizing the sum of overlaps, the performance
of SWO for the R problems worsens significantly when only one task is moved forward. Previously, we implemented SWO1Move for minimizing overlaps by moving forward the request
that contributes most to the total overlap (Barbulescu et al., 2004c). Randomly choosing
10. We tried other values; on average, a value of five seems to work best.

606

Understanding Algorithm Performance

Day
R1
R2
R3
R4

Min
840
512
284
764

k=10
Mean
862.5
530.2
291.36
778.57

Stdev
11.28
9.18
4.65
8.45

Min
815
498
266
749

k=20
Mean
829.77
506.53
268.9
757.3

Stdev
8.45
5.25
2.21
6.16

Min
798
493
266
740

k=30
Mean
820.63
508.97
271.07
744.47

Stdev
8.12
5.26
3.52
2.6

Min
825
508
266
737

k=40
Mean
841.13
526.26
273.2
747.2

Stdev
8.02
6.25
3.54
5.06

Table 15: Performance of a modified version of SWO where k of the requests contributing
to the sum of overlaps are moved forward for a constant distance 5. All statistics
are collected over 30 independent runs, with 8000 evaluations per run.

the request to be moved forward improved the performance of SWO1Move. Randomization
is useful because SWO can become trapped in cycles (Joslin & Clements, 1999); however,
the improvement is not enough to equal the performance of SWO when minimizing overlaps
for the new days of data. In fact, longer runs of SWO1Move with a random choice of the
request to be moved (30 runs with 50,000 evaluations) produce solutions that are still worse
than those obtained by SWO. These results support the conjecture that the performance
of SWO is due to the simultaneous moves of requests.
We attribute the discrepancy in the SWO1Move performance for the two objective
functions to the difference in the discretization of the two search spaces. When minimizing
conflicts, SWO1Move only needs to identify the requests that cannot be scheduled. More
fine tuning is needed when minimizing the sum of overlaps; besides identifying the requests
that cannot be scheduled, SWO1Move also needs to find the positions for these requests in
the permutation such that the sum of overlaps is minimized. We conjecture that this fine
tuning is achieved by simultaneously moving forward multiple requests.
Next, we investigate the changes in performance when an increasing number of requests
(from the requests contributing to the objective function) are moved forward. We design
an experiment where a constant number of the requests involved in conflicts are moved
forward. To do this, we need to decide how many requests to move and which ones. Moving
two or three requests forward results in small improvements over the results in Table 14.
Therefore, we run multiple versions of SWO moving k requests forward, with k = 10, 20,
30, 40. We determined empirically that when moving multiple requests (more than five)
forward, choosing them at random as opposed to based on their contribution to the sum
of overlaps hurts algorithm performance. To determine which requests are moved forward,
at each step we sort the requests contributing to the sum of overlaps in decreasing order of
their contribution and only move forward the first k (or all of them, if k is greater than the
number of requests contributing to the sum of overlaps).
The results obtained for R1, R2, R3 and R4 are summarized in Table 15. For the A
problems, all these new SWO versions find best known solutions. We did not include R5
in this study because the SWO greedy initial permutation computed for R5 corresponds to
a best known value schedule. The results show a general performance improvement as k
grows from 10 to 20. k = 20 and k = 30 produce similar performance for R1, R2 and R3.
For R4, k = 30 results in better performance than k = 20. k = 40 results in worsening
performance for R1 and R2. Note that the algorithm performance for R3 does not change
607

Barbulescu, Howe, Whitley, & Roberts

when k >= 20. This is not surprising; since good solutions (in terms of overlaps) for this
problem correspond to schedules with a small number of overlapping tasks, moving forward
20 requests or more means moving most of the requests in conflict once good solutions are
found. The results indicate that for the problems in our set, when minimizing overlaps, if
SWO is allowed to only move forward a constant number k of requests, k = 30 seems to be
a good choice.
The results in this section support the hypothesis that moving multiple requests forward
is necessary to obtain good SWO performance. First, we showed that moving only one
request forward (or a small number of requests, smaller than 30 for the R problems) results
in inferior SWO performance. Second, as the number of requests moved forward is increased
(from 10 up), the performance of SWO improves.

7. New Algorithm: Attenuated Leap Local Search
The empirical data and analyses suggest that the key to competitive performance on this
application is moving as quickly as possible across the plateaus. Two of the competitive
algorithms, Genitor and SWO, perform multiple moves. A simpler algorithm, RLS, actually
finds more best known solutions in 8000 evaluations, even though it does not perform multiple moves. RLS does however, perform a significant number of “neutral moves” between
solutions with the same evaluation. Given this, we conjecture that a version of local search
that performs multiple moves before evaluating the result may be even better suited to this
application. The intuition behind this conjecture is that the search should sample at greater
distances (i.e., longer than a single move) to more quickly find exits from plateaus.
We modified the RLS move operator as follows: choose a number of pairs of positions
and apply shifting to these pairs, one after another, without building the schedule after
each shift; build the schedule only after shifting has been applied for the designated number
of pairs. In our first version, we tried a static number of shifts (10 turned out to be the
best value); however, it performed no better and sometimes worse than the original move
operator. We next conjectured that as search progresses to better solutions, the number
of shifts should also decrease because the probability of finding detrimental moves (rather
than improving) increases significantly as well. The better the solution, the fewer exits are
expected and the harder they are to find.
We implemented a multiple move hill-climber with a variable move count operator: given
a decay rate, we start by shifting ten requests, then nine, eight etc. We chose to decrement
the number of shifts for every 800 evaluations; we call this version of hill-climbing Attenuated
Leap Local Search (ALLS). This is similar to the idea behind the “temperature dependent”
hill-climbing move operator implemented by Globus et al. (2004), for which the number of
requests to move is chosen by random but biased such that a large number of requests are
moved early in the search while later only a few requests are moved 11 . Hill-climbing with
the temperature dependent operator produced better results for EOS than simply choosing
a random number of requests to move.
ALLS performs remarkably well. As shown in Table 16, it finds best known values
for all problems using conflicts and all but two of the problems using overlaps (as does
11. The operator is similar to the temperature dependent behavior in simulated annealing; this explains the
name of the operator.

608

Understanding Algorithm Performance

Average Best So Far Number of Bumps

70

Genitor
RLS
SWO
ALLS

65
60
55
50
45
40
35
30
25

0

500

1000

1500
2000
2500
Evaluations

Average Best So Far Number of Bumps

32

3000

3500

4000

Genitor
RLS
SWO
ALLS

31.5
31
30.5
30
29.5
29
28.5
28
4000

4500

5000

5500 6000 6500
Evaluations

7000

7500

8000

Figure 6: Evolutions of the average best value obtained by Genitor, RLS, SWO and ALLS
during 8000 evaluations, over 30 runs. The improvement over the first 4000
evaluations is shown in the top figure. The last 4000 evaluations are depicted in
the bottom figure; note that the scale is different on the y-axis. The graphs were
obtained for R4; best solution value is 28.
RLS). Additionally, it finds better best values than all the algorithms in our set for the
two problems with non-best solutions. In fact, a single tailed, two sample t-test comparing
ALLS to RLS shows that ALLS finds statistically significantly better solutions (p < 0.023)
on both conflicts and overlaps for the five more recent days.
609

Barbulescu, Howe, Whitley, & Roberts

Average Best So Far Number of Overlaps

1700

Genitor
RLS
SWO
ALLS

1600
1500
1400
1300
1200
1100
1000
900
800
700

0

500

1000

1500 2000 2500
Evaluations

Average Best So Far Number of Overlaps

900

3000

3500

4000

Genitor
RLS
SWO
ALLS

880
860
840
820
800
780
760
740
720
4000

4500

5000

5500 6000 6500
Evaluations

7000

7500

8000

Figure 7: Evolutions of the average best value obtained by Genitor, RLS, SWO and ALLS
during 8000 evaluations, over 30 runs. The improvement over the first 4000
evaluations is shown in the top figure. The last 4000 evaluations are depicted in
the bottom figure; note that the scale is different on the y-axis.The graphs were
obtained for R4; best solution value is 725.
In Section 5, we discussed a comparison across all algorithms (again at p < 0.005).
Under this much more restrictive performance comparison, ALLS still outperforms RLS,
SWO and Genitor for most of the pair-wise tests. Both when minimizing conflicts and when
minimizing overlaps, ALLS significantly outperforms all other algorithms on R1. When
610

Understanding Algorithm Performance

Day
A1
A2
A3
A4
A5
A6
A7
R1
R2
R3
R4
R5

Minimizing Conflicts
Min Mean Stdev
8
8.2
0.4
4
4.0
0.0
3
3.0
0.0
2
2.03
0.18
4
4.1
0.3
6
6.0
0.0
6
6.0
0.0
42 42.63
0.72
29
29.1
0.3
17
17.5
0.57
28 28.07
0.25
12
12.0
0.0

Minimizing Overlaps
Min Mean Stdev
104 107.1
1.24
13
13.0
0.0
28
28.33
1.3
9
9.13
0.73
30
30.23
0.43
45
45.0
0.0
46
46.0
0.0
785 817.83 27.07
490 510.37 19.14
250 273.33 43.68
725 740.07 19.56
146 146.03 0.19

Table 16: Statistics for the results obtained in 30 runs of ALLS, with 8,000 evaluations per
run. The best and mean values as well as the standard deviations are shown.
Bold indicates best known values.

minimizing conflicts, ALLS outperforms for all but five of the twelve pair-wise tests on the
other four days (for which the difference was not significant). The exceptions are: R2, R3,
R4, and R5 for Genitor and R4 for RLS. When minimizing overlaps, ALLS significantly
outperforms Genitor for R2, RLS for R3, Genitor for R4 and SWO for R5; the rest of the
pair-wise comparisons were not statistically significant at p < 0.005. It is clear that ALLS
is at least as good as the best algorithms and outperforms them on most days of data.
ALLS also finds improving solutions faster than both Genitor and RLS (see Figures 6
and 7 for R4 on both conflicts and overlaps). ALLS achieves such good performance by
combining the power of finding good solutions fast using multiple moves in the beginning
of the search with the accuracy of locating the best solutions using one-move shifting at the
end of the search.
In 6.4 we showed that as the solutions improve the random walks on plateaus become
longer. Two hypotheses support this observation: 1) The plateaus are bigger 2) The plateaus
are harder to escape because there are fewer exits. These two hypotheses are consistent if
the missing exits are replaced by moves of equal value. They are not consistent if the exits
are replaced by worse moves. Our ALLS design implicitly assumes the latter. If the exits
were replaced by equal moves then as the search progresses more moves would be needed
per each large step12 . In fact, we ran some tests where we increased the number of moves
as search progresses and we found that this can significantly worsen the performance. For
example, for R1 when minimizing overlaps, shifting initially ten requests and then increasing
the number of shifted requests by 1 every 800 iterations (instead of decreasing it as in ALLS)
12. We wish to thank the anonymous reviewer for this insightful observation.

611

Barbulescu, Howe, Whitley, & Roberts

results in a minimum overlap of 885, with a mean of 957.97 and a standard deviation of
51.36, which is significantly worse than the corresponding ALLS result.

8. Conclusion
A key algorithm characteristic for AFSCN appears to be multiple moves. In fact, this
observation might hold for other oversubscribed scheduling problems as well. Globus et
al. (Globus et al., 2004) found that when solving the oversubscribed problem of scheduling
fleets of EOS using hill-climbing, moving only one request at a time was inefficient. Their
temperature dependent hill-climbing operator proved to work better than simply choosing
a random number of requests to move. As in our domain, a permutation representation and
a greedy deterministic schedule builder are used. We conjecture that their schedule builder
also results in multiple permutations being mapped to the same schedule, and therefore
that plateaus are present in the EOS search space as well. The fact that moving more
than one request improved the results suggests that our conjecture could also hold for EOS
scheduling: multiple moves might speed up plateau traversal for this domain as well.
We developed and tested four hypotheses explaining the performance of three competitive algorithms for a real scheduling application. We found that all of the hypotheses held
to varying degrees. Based on the evidence, we designed a new algorithm that combined
what appeared to be critical elements of the best performing algorithms and produced an
algorithm that performed better than the original ones. Our results suggest that multiple moves are a useful algorithm feature to obtain good performance results for AFSCN
scheduling. Alternatively, it is possible that in fact only one move during each iteration
would be enough to obtain good performance, but it is difficult to identify which request to
move. Future research in this direction will examine heuristics such as combining HBSS and
SWO to decide which request to move forward, as well as heuristics to find where to move
the request to guarantee a change in the schedule. Also as future research, we will be testing
other oversubscribed scheduling applications to determine to what extent our analyses and
results generalize: do they exhibit the same characteristics and are they amenable to the
same kind of solution?

Acknowledgments

This research was supported in part by a grant from the Air Force Office of Scientific Research, Air Force Materiel Command, USAF under grant number F49620-03-1-0233. Adele
Howe was also supported by the National Science Foundation under Grant No. IIS-0138690.
Any opinions, findings, and conclusions or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect the views of the National Science
Foundation. The U.S. Government is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright notation thereon.
612

Understanding Algorithm Performance

References
Ackley, D. (1987). A Connectionist Machine for Genetic Hillclimbing. Kluwer Academic
Publishers.
Aickelin, U., & Dowsland, K. (2003). An indirect genetic algorithm for a nurse scheduling
problem. Computers & Operations Research, 31 (5), 761–778.
Barbulescu, L., Howe, A., Whitley, L., & Roberts, M. (2004a). Trading places: How to
schedule more in a multi-resource oversubscribed scheduling problem. In Proceedings
of the International Conference on Planning and Scheduling, Whistler, CA.
Barbulescu, L., Watson, J., Whitley, D., & Howe, A. (2004b). Scheduling Space-Ground
Communications for the Air Force Satellite Control Network. Journal of Scheduling,
7, 7–34.
Barbulescu, L., Whitley, L., & Howe, A. (2004c). Leap before you look: An effective strategy
in an oversubscribed problem. In Proceedings of the Nineteenth National Artificial
Intelligence Conference, San Jose, CA.
Beck, J., Davenport, A., Davis, E., & Fox, M. (1998). The ODO Project: Toward a Unified
Basis for Constraint-directed Scheduling. Journal of Scheduling, 1, 89–125.
Bresina, J. (1996). Heuristic-Biased Stochastic Sampling. In Proceedings of the Thirteenth
National Conference on Artificial Intelligence, pp. 271–278, Portland, OR.
Burrowbridge, S. E. (1999). Optimal Allocation of Satellite Network Resources. In Masters
Thesis. Virginia Polytechnic Institute and State University.
Chien, S., Rabideau, G., Knight, R., Sherwood, R., Engelhardt, B., Mutz, D., Estlin, T.,
Smith, B., Fisher, F., Barrett, T., Stebbins, G., & Tran, D. (2000). ASPEN - Automating space mission operations using automated planning and scheduling. In 6th
International SpaceOps Symposium (Space Operations), Toulouse (France).
Cormen, T., Leiserson, C., & Rivest, R. (1990). Introduction to Algorithms. MIT press,
Cambridge, MA.
Deale, M., Yvanovich, M., Schnitzuius, D., Kautz, D., Carpenter, M., Zweben, M., Davis,
G., & Daun, B. (1994). The Space Shuttle ground processing scheduling system. In
Zweben, M., & Fox, M. (Eds.), Intelligent Scheduling, pp. 423–449. Morgan Kaufmann.
Forrest, S., & Mitchell, M. (1993). Relative Building-Block Fitness and the Building Block
Hypothesis. In Whitley, L. D. (Ed.), Foundations of Genetic Algorithms 2, pp. 109–
126. Morgan Kaufmann.
Frank, J., Cheeseman, P., & Stutz, J. (1997). When gravity fails: Local search topology.
Journal of Artificial Intelligence Research, 7, 249–281.
Frank, J., Jonsson, A., Morris, R., & Smith, D. (2001). Planning and scheduling for fleets
of earth observing satellites. In Proceedings of the Sixth International Symposium on
Artificial Intelligence, Robotics, Automation and Space.
Gent, I., & Walsh, T. (1995). Unsatisfied variables in local search. In Hybrid Problems,
Hybrid Solutions, pp. 73–85. IOS Press Amsterdam.
613

Barbulescu, Howe, Whitley, & Roberts

Globus, A., Crawford, J., Lohn, J., & Pryor, A. (2003). Scheduling earth observing satellites
with evolutionary agorithms. In International Conference on Space Mission Challenges for Information Technology, Pasadena, CA.
Globus, A., Crawford, J., Lohn, J., & Pryor, A. (2004). A comparison of techniques for
scheduling earth observing satellites. In Proceedings of the Sixteenth Innovative Applications of Artificial Intelligence Conference, San Jose, CA.
Goldberg, D. (1989). Genetic Algorithms in Search, Optimization and Machine Learning.
Addison-Wesley, Reading, MA.
Gooley, T. (1993). Automating the Satellite Range Scheduling Process. In Masters Thesis.
Air Force Institute of Technology.
Jang, K. (1996). The Capacity of the Air Force Satellite Control Network. In Masters
Thesis. Air Force Institute of Technology.
Johnston, M., & Miller, G. (1994). Spike: Intelligent scheduling of Hubble space telescope
observations. In Morgan, M. B. (Ed.), Intelligent Scheduling, pp. 391–422. Morgan
Kaufmann Publishers.
Joslin, D. E., & Clements, D. P. (1999). “Squeaky Wheel” Optimization. In Journal of
Artificial Intelligence Research, Vol. 10, pp. 353–373.
Kramer, L., & Smith, S. (2003). Maximizing flexibility: A retraction heuristic for oversubscribed scheduling problems. In Proceedings of 18th International Joint Conference
on Artificial Intelligence, Acapulco, Mexico.
Lemaı̂tre, M., Verfaillie, G., & Jouhaud, F. (2000). How to manage the new generation of
Agile Earth Observation Satellites. In 6th International SpaceOps Symposium (Space
Operations), Toulouse, France.
Parish, D. (1994). A Genetic Algorithm Approach to Automating Satellite Range Scheduling. In Masters Thesis. Air Force Institute of Technology.
Pemberton, J. (2000). Toward Scheduling Over-Constrained Remote-Sensing Satellites. In
Proceedings of the Second NASA International Workshop on Planning and Scheduling
for Space, San Francisco, CA.
Rabideau, G., Chien, S., Willis, J., & Mann, T. (1999). Using iterative repair to automate
planning and scheduling of shuttle payload operations. In Innovative Applications of
Artificial Intelligence (IAAI 99), Orlando,FL.
ROADEF Challenge (2003).
French Society of Operations Research and Decision Analisys ROADEF Challenge 2003.
http://www.prism.uvsq.fr/˜
vdc/ROADEF/CHALLENGES/2003/.
Roberts, M., Whitley, L., Howe, A., & Barbulescu, L. (2005). Random walks and neighborhood bias in oversubscribed scheduling. In Multidisciplinary International Conference
on Scheduling (MISTA-05), New York, NY.
Schalck, S. (1993). Automating Satellite Range Scheduling. In Masters Thesis. Air Force
Institute of Technology.
614

Understanding Algorithm Performance

Shaw, K., & Fleming, P. (1997). Use of rules and preferences for schedule builders in
genetic algorithms for production scheduling. Proceedings of the AISB’97 Workshop
on Evolutionary Computation. Lecture Notes in Computer Science, 1305, 237–250.
Singer, J., Gent, I., & Smaill, A. (2000). Backbone Fragility and the Local Search Cost
Peak. In Journal of Artificial Intelligence Research, Vol. 12, pp. 235–270.
Smith, S., & Cheng, C. (1993). Slack-based Heuristics for Constraint Satisfaction Problems.
In Proceedings of the Eleventh National Conference on Artificial Intelligence (AAAI93), pp. 139–144, Washington, DC. AAAI Press.
Starkweather, T., McDaniel, S., Mathias, K., Whitley, D., & Whitley, C. (1991). A Comparison of Genetic Sequencing Operators. In Booker, L., & Belew, R. (Eds.), Proc. of
the 4th Int’l. Conf. on GAs, pp. 69–76. Morgan Kaufmann.
Syswerda, G. (1991). Schedule Optimization Using Genetic Algorithms. In Davis, L. (Ed.),
Handbook of Genetic Algorithms, chap. 21. Van Nostrand Reinhold, NY.
Syswerda, G., & Palmucci, J. (1991). The Application of Genetic Algorithms to Resource
Scheduling. In Booker, L., & Belew, R. (Eds.), Proc. of the 4th Int’l. Conf. on GAs.
Morgan Kaufmann.
Watson, J. P., Rana, S., Whitley, D., & Howe, A. (1999). The Impact of Approximate Evaluation on the Performance of Search Algorithms for Warehouse Scheduling. Journal
of Scheduling, 2(2), 79–98.
Whitley, D., Starkweather, T., & Fuquay, D. (1989). Scheduling Problems and Traveling
Salesmen: The Genetic Edge Recombination Operator. In Schaffer, J. D. (Ed.), Proc.
of the 3rd Int’l. Conf. on GAs. Morgan Kaufmann.
Whitley, L. D. (1989). The GENITOR Algorithm and Selective Pressure: Why Rank Based
Allocation of Reproductive Trials is Best. In Schaffer, J. D. (Ed.), Proc. of the 3rd
Int’l. Conf. on GAs, pp. 116–121. Morgan Kaufmann.
Wolfe, W. J., & Sorensen, S. E. (2000). Three Scheduling Algorithms Applied to the Earth
Observing Systems Domain. In Management Science, Vol. 46(1), pp. 148–168.
Zweben, M., Daun, B., & Deale, M. (1994). Scheduling and rescheduling with iterative
repair. In Zweben, M., & Fox, M. (Eds.), Intelligent Scheduling. Morgan Kaufmann.

615

Journal of Artificial Intelligence Research 27 (2006) 119–151

Submitted 1/06; published 10/06

A Comparison of Different Machine Transliteration Models
Jong-Hoon Oh

rovellia@nict.go.jp

Computational Linguistics Group
National Institute of Information and Communications Technology (NICT)
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289 Japan

Key-Sun Choi

kschoi@cs.kaist.ac.kr

Computer Science Division, Department of EECS
Korea Advanced Institute of Science and Technology (KAIST)
373-1 Guseong-dong, Yuseong-gu, Daejeon 305-701 Republic of Korea

Hitoshi Isahara

isahara@nict.go.jp

Computational Linguistics Group
National Institute of Information and Communications Technology (NICT)
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289 Japan

Abstract
Machine transliteration is a method for automatically converting words in one language into phonetically equivalent ones in another language. Machine transliteration plays
an important role in natural language applications such as information retrieval and machine translation, especially for handling proper nouns and technical terms. Four machine
transliteration models – grapheme-based transliteration model, phoneme-based transliteration model, hybrid transliteration model, and correspondence-based transliteration model –
have been proposed by several researchers. To date, however, there has been little research
on a framework in which multiple transliteration models can operate simultaneously. Furthermore, there has been no comparison of the four models within the same framework and
using the same data. We addressed these problems by 1) modeling the four models within
the same framework, 2) comparing them under the same conditions, and 3) developing a
way to improve machine transliteration through this comparison. Our comparison showed
that the hybrid and correspondence-based models were the most effective and that the
four models can be used in a complementary manner to improve machine transliteration
performance.

1. Introduction
With the advent of new technology and the flood of information through the Web, it has
become increasingly common to adopt foreign words into one’s language. This usually entails adjusting the adopted word’s original pronunciation to follow the phonological rules
of the target language, along with modification of its orthographical form. This phonetic
“translation” of foreign words is called transliteration. For example, the English word
data is transliterated into Korean as ‘de-i-teo’1 and into Japanese as ‘de-e-ta’. Transliteration is particularly used to translate proper names and technical terms from languages
1. In this paper, target language transliterations are represented in their Romanized form with single
quotation marks and hyphens between syllables.
c
°2006
AI Access Foundation. All rights reserved.

Oh, Choi, & Isahara

using Roman alphabets into ones using non-Roman alphabets such as from English to
Korean, Japanese, or Chinese. Because transliteration is one of the main causes of the
out-of-vocabulary (OOV) problem, transliteration by means of dictionary lookup is impractical (Fujii & Tetsuya, 2001; Lin & Chen, 2002). One way to solve the OOV problem is
to use machine transliteration. Machine transliteration is usually used to support machine
translation (MT) (Knight & Graehl, 1997; Al-Onaizan & Knight, 2002) and cross-language
information retrieval (CLIR) (Fujii & Tetsuya, 2001; Lin & Chen, 2002). For CLIR, machine
transliteration bridges the gap between the transliterated localized form and its original form
by generating all possible transliterations from the original form (or generating all possible
original forms from the transliteration)2 . For example, machine transliteration can assist
query translation in CLIR, where proper names and technical terms frequently appear in
source language queries. In the area of MT, machine transliteration helps preventing translation errors when translations of proper names and technical terms are not registered in
the translation dictionary. Machine transliteration can therefore improve the performance
of MT and CLIR.
Four machine transliteration models have been proposed by several researchers: grapheme3 -based transliteration model (ψG ) (Lee & Choi, 1998; Jeong, Myaeng, Lee, &
Choi, 1999; Kim, Lee, & Choi, 1999; Lee, 1999; Kang & Choi, 2000; Kang & Kim, 2000;
Kang, 2001; Goto, Kato, Uratani, & Ehara, 2003; Li, Zhang, & Su, 2004), phoneme4 based transliteration model (ψP ) (Knight & Graehl, 1997; Lee, 1999; Jung, Hong, &
Paek, 2000; Meng, Lo, Chen, & Tang, 2001), hybrid transliteration model (ψH ) (Lee,
1999; Al-Onaizan & Knight, 2002; Bilac & Tanaka, 2004), and correspondence-based
transliteration model (ψC ) (Oh & Choi, 2002). These models are classified in terms of
the units to be transliterated. The ψG is sometimes referred to as the direct method because
it directly transforms source language graphemes into target language graphemes without
any phonetic knowledge of the source language words. The ψP is sometimes referred to as
the pivot method because it uses source language phonemes as a pivot when it produces
target language graphemes from source language graphemes. The ψP therefore usually
needs two steps: 1) produce source language phonemes from source language graphemes;
2) produce target language graphemes from source phonemes5 . The ψH and ψC make use
of both source language graphemes and source language phonemes when producing target
language transliterations. Hereafter, we refer to a source language grapheme as a source
2. The former process is generally called “transliteration”, and the latter is generally called “backtransliteration” (Knight & Graehl, 1997)
3. Graphemes refer to the basic units (or the smallest contrastive units) of a written language: for example,
English has 26 graphemes or letters, Korean has 24, and German has 30.
4. Phonemes are the simplest significant unit of sound (or the smallest contrastive units of a spoken language); for example, /M/, /AE/, and /TH/ in /M AE TH/, the pronunciation of math. We use the
ARPAbet symbols to represent source phonemes. ARPAbet is one of the methods used for coding source
phonemes into ASCII characters (http://www.cs.cmu.edu/~laura/pages/arpabet.ps). Here we denote
source phonemes and pronunciation with two slashes, as in /AH/, and use pronunciation based on The
CMU Pronunciation Dictionary and The American Heritage(r) Dictionary of the English Language.
5. These two steps are explicit if the transliteration system produces target language transliterations after
producing the pronunciations of the source language words; they are implicit if the system uses phonemes
implicitly in the transliteration stage and explicitly in the learning stage, as described elsewhere (Bilac
& Tanaka, 2004)

120

A Comparison of Machine Transliteration Models

grapheme, a source language phoneme as a source phoneme, and a target language grapheme
as a target grapheme.
The transliterations produced by the four models usually differ because the models use
different information. Generally, transliteration is a phonetic process, as in ψP , rather
than an orthographic one, as in ψG (Knight & Graehl, 1997). However, standard transliterations are not restricted to phoneme-based transliterations. For example, the standard
Korean transliterations of data, amylase, and neomycin are, respectively, the phonemebased transliteration ‘de-i-teo’, the grapheme-based transliteration ‘a-mil-la-a-je’, and ‘neo-ma-i-sin’, which is a combination of the grapheme-based transliteration ‘ne-o’ and the
phoneme-based transliteration ‘ma-i-sin’. Furthermore, if the unit to be transliterated is
restricted to either a source grapheme or a source phoneme, it is hard to produce the correct
transliteration in many cases. For example, ψP cannot easily produce the grapheme-based
transliteration ‘a-mil-la-a-je’, the standard Korean transliteration of amylase, because ψP
tends to produce ‘a-mil-le-i-seu’ based on the sequence of source phonemes /AE M AH
L EY S/. Multiple transliteration models should therefore be applied to better cover the
various transliteration processes. To date, however, there has been little published research
regarding a framework in which multiple transliteration models can operate simultaneously.
Furthermore, there has been no reported comparison of the transliteration models within
the same framework and using the same data although many English-to-Korean transliteration methods based on ψG have been compared to each other with the same data (Kang
& Choi, 2000; Kang & Kim, 2000; Oh & Choi, 2002).
To address these problems, we 1) modeled a framework in which the four transliteration models can operate simultaneously, 2) compared the transliteration
models under the same conditions, and 3) using the results of the comparison,
developed a way to improve the performance of machine transliteration.
The rest of this paper is organized as follows. Section 2 describes previous work relevant
to our study. Section 3 describes our implementation of the four transliteration models.
Section 4 describes our testing and results. Section 5 describes a way to improve machine
transliteration based on the results of our comparison. Section 6 describes a transliteration ranking method that can be used to improve transliteration performance. Section 7
concludes the paper with a summary and a look at future work.

2. Related Work
Machine transliteration has received significant research attention in recent years. In most
cases, the source language and target language have been English and an Asian language, respectively – for example, English to Japanese (Goto et al., 2003), English to Chinese (Meng
et al., 2001; Li et al., 2004), and English to Korean (Lee & Choi, 1998; Kim et al., 1999;
Jeong et al., 1999; Lee, 1999; Jung et al., 2000; Kang & Choi, 2000; Kang & Kim, 2000;
Kang, 2001; Oh & Choi, 2002). In this section, we review previous work related to the four
transliteration models.
2.1 Grapheme-based Transliteration Model
Conceptually, the ψG is direct orthographical mapping from source graphemes to target
graphemes. Several transliteration methods based on this model have been proposed, such
121

Oh, Choi, & Isahara

as those based on a source-channel model (Lee & Choi, 1998; Lee, 1999; Jeong et al.,
1999; Kim et al., 1999), a decision tree (Kang & Choi, 2000; Kang, 2001), a transliteration
network (Kang & Kim, 2000; Goto et al., 2003), and a joint source-channel model (Li et al.,
2004).
The methods based on the source-channel model deal with English-Korean transliteration. They use a chunk of graphemes that can correspond to a source phoneme. First,
English words are segmented into a chunk of English graphemes. Next, all possible chunks of
Korean graphemes corresponding to the chunk of English graphemes are produced. Finally,
the most relevant sequence of Korean graphemes is identified by using the source-channel
model. The advantage of this approach is that it considers a chunk of graphemes representing a phonetic property of the source language word. However, errors in the first step
(segmenting the English words) propagate to the subsequent steps, making it difficult to
produce correct transliterations in those steps. Moreover, there is high time complexity
because all possible chunks of graphemes are generated in both languages.
In the method based on a decision tree, decision trees that transform each source
grapheme into target graphemes are learned and then directly applied to machine transliteration. The advantage of this approach is that it considers a wide range of contextual
information, say, the left three and right three contexts. However, it does not consider any
phonetic aspects of transliteration.
Kang and Kim (2000) and Goto et al. (2003) proposed methods based on a transliteration network for, respectively, English-to-Korean and English-to-Japanese transliteration.
Their frameworks for constructing a transliteration network are similar – both are composed
of nodes and arcs. A node represents a chunk of source graphemes and its corresponding
target graphemes. An arc represents a possible link between nodes and has a weight showing
its strength. Like the methods based on the source-channel model, their methods consider
the phonetic aspect in the form of chunks of graphemes. Furthermore, they segment a chunk
of graphemes and identify the most relevant sequence of target graphemes in one step. This
means that errors are not propagated from one step to the next, as in the methods based
on the source-channel model.
The method based on the joint source-channel model simultaneously considers the source
language and target language contexts (bigram and trigram) for machine transliteration.
Its main advantage is the use of bilingual contexts.
2.2 Phoneme-based Transliteration Model
In the ψP , the transliteration key is pronunciation or the source phoneme rather than
spelling or the source grapheme. This model is basically source grapheme-to-source phoneme
transformation and source phoneme-to-target grapheme transformation.
Knight and Graehl (1997) modeled Japanese-to-English transliteration with weighted
finite state transducers (WFSTs) by combining several parameters including romaji-tophoneme, phoneme-to-English, English word probabilities, and so on. A similar model was
developed for Arabic-to-English transliteration (Stalls & Knight, 1998). Meng et al. (2001)
proposed an English-to-Chinese transliteration method based on English grapheme-to-phoneme
conversion, cross-lingual phonological rules, mapping rules between English phonemes and
Chinese phonemes, and Chinese syllable-based and character-based language models. Jung
122

A Comparison of Machine Transliteration Models

et al. (2000) modeled English-to-Korean transliteration with an extended Markov window.
The method transforms an English word into English pronunciation by using a pronunciation dictionary. Then it segments the English phonemes into chunks of English phonemes;
each chunk corresponds to a Korean grapheme as defined by handcrafted rules. Finally, it
automatically transforms each chunk of English phonemes into Korean graphemes by using
an extended Markov window.
Lee (1999) modeled English-to-Korean transliteration in two steps. The English graphemeto-English phoneme transformation is modeled in a manner similar to his method based
on the source-channel model described in Section 2.1. The English phonemes are then
transformed into Korean graphemes by using English-to-Korean standard conversion rules
(EKSCR) (Korea Ministry of Culture & Tourism, 1995). These rules are in the form of
context-sensitive rewrite rules, “PA PX PB → y”, meaning that English phoneme PX is
rewritten as Korean grapheme y in the context PA and PB , where PX , PA , and PB represent English phonemes. For example, “PA = ∗, PX = /SH/, PB = end → ‘si’” means
“English phoneme /SH/ is rewritten into Korean grapheme ‘si’ if it occurs at the end of
the word (end) after any phoneme (∗)”. This approach suffers from both the propagation
of errors and the limitations of EKSCR. The first step, grapheme-to-phoneme transformation, usually results in errors, and the errors propagate to the next step. Propagated errors
make it difficult for a transliteration system to work correctly. In addition, EKSCR does
not contain enough rules to generate correct Korean transliterations since its main focus is
mapping from an English phoneme to Korean graphemes without taking into account the
contexts of the English grapheme.
2.3 Hybrid and Correspondence-based Transliteration Models
Attempts to use both source graphemes and source phonemes in machine transliteration
led to the correspondence-based transliteration model (ψC ) (Oh & Choi, 2002) and the
hybrid transliteration model (ψH ) (Lee, 1999; Al-Onaizan & Knight, 2002; Bilac & Tanaka,
2004). The former makes use of the correspondence between a source grapheme and a source
phoneme when it produces target language graphemes; the latter simply combines ψG and
ψP through linear interpolation. Note that the ψH combines the grapheme-based transliteration probability (P r(ψG )) and the phoneme-based transliteration probability (P r(ψP ))
using linear interpolation.
Oh and Choi (2002) considered the contexts of a source grapheme and its corresponding source phoneme for English-to-Korean transliteration. They used EKSCR as the basic rules in their method. Additional contextual rules are semi-automatically constructed
by examining the cases in which EKSCR produced incorrect transliterations because of
a lack of contexts. These contextual rules are in the form of context-sensitive rewrite
rules, “CA CX CB → y”, meaning “CX is rewritten as target grapheme y in the context
CA and CB ”. Note that CX , CA , and CB represent the correspondence between the English grapheme and phoneme. For example, we can read “CA = (∗ : /V owel/), CX =
(r : /R/), CB = (∗ : /Consonant/) → NULL” as “English grapheme r corresponding to
phoneme /R/ is rewritten into null Korean graphemes when it occurs after vowel phonemes,
(∗ : /V owel/), before consonant phonemes, (∗ : /Consonant/)”. The main advantage of
this approach is the application of a sophisticated rule that reflects the context of the source
123

Oh, Choi, & Isahara

grapheme and source phoneme by considering their correspondence. However, there is lack
of portability to other languages because the rules are restricted to Korean.
Several researchers (Lee, 1999; Al-Onaizan & Knight, 2002; Bilac & Tanaka, 2004) have
proposed hybrid model-based transliteration methods. They model ψG and ψP with WFSTs or a source-channel model and combine ψG and ψP through linear interpolation. In
their ψP , several parameters are considered, such as the source grapheme-to-source phoneme
probability, source phoneme-to-target grapheme probability, and target language word probability. In their ψG , the source grapheme-to-target grapheme probability is mainly considered. The main disadvantage of the hybrid model is that the dependence between the source
grapheme and source phoneme is not taken into consideration in the combining process; in
contrast, Oh and Choi’s approach (Oh & Choi, 2002) considers this dependence by using
the correspondence between the source grapheme and phoneme.

3. Modeling Machine Transliteration Models
In this section, we describe our implementation of the four machine transliteration models
(ψG , ψP , ψH , and ψC ) using three machine learning algorithms: memory-based learning,
decision-tree learning, and the maximum entropy model.
3.1 Framework for Four Machine Transliteration Models
Figure 1 summarizes the differences among the transliteration models and their component
functions. The ψG directly transforms source graphemes (S) into target graphemes (T).
The ψP and ψC transform source graphemes into source phonemes and then generate target
graphemes6 . While ψP uses only the source phonemes, ψC uses the correspondence between
the source grapheme and the source phoneme when it generates target graphemes. We
describe their differences with two functions, φP T and φ(SP )T . The ψH is represented as the
linear interpolation of P r(ψG ) and P r(ψP ) by means of α (0 ≤ α ≤ 1). Here, P r(ψP ) is the
probability that ψP will produce target graphemes, while P r(ψG ) is the probability that ψG
will produce target graphemes. We can thus regard ψH as being composed of component
functions of ψG and ψP (φSP , φP T , and φST ). Here we use the maximum entropy model
as the machine learning algorithm for ψH because ψH requires P r(ψP ) and P r(ψG ), and
only the maximum entropy model among memory-based learning, decision-tree learning,
and the maximum entropy model can produce the probabilities.
To train each component function, we need to define the features that represent training
instances and data. Table 1 shows five feature types, fS , fP , fStype , fP type , and fT . The
feature types used depend on the component functions. The modeling of each component
function with the feature types is explained in Sections 3.2 and 3.3.
3.2 Component Functions of Each Transliteration Model
Table 2 shows the definitions of the four component functions that we used. Each is defined
in terms of its input and output: the first and last characters in the notation of each
correspond respectively to its input and output. The role of each component function in
6. According to (g◦f )(x) = g(f (x)), we can write (φ(SP )T ◦φSP )(x) = φ(SP )T (φSP (x)) and (φP T ◦φSP )(x) =
φP T (φSP (x)).

124

A Comparison of Machine Transliteration Models

φST

S

T
φ(SP)T

φSP

P
SS:: Source graphemes
P
P:: Source Phonemes
TT:: Target graphemes

φPT
ψ G : φST
ψ P : φPT o φSP
ψ C : φ( SP )T o φSP
ψ H : α × Pr(ψ P )
+ (1 − α ) × Pr(ψ G )

Figure 1: Graphical representation of each component function and four transliteration
models: S is a set of source graphemes (e.g., letters of the English alphabet), P is
a set of source phonemes defined in ARPAbet, and T is a set of target graphemes.

Feature type
fS
fS,Stype
fStype
fP
fP,P type
fP type
fT

Description and possible values
Source graphemes in S:
26 letters in English alphabet
Source grapheme types:
Consonant (C) and Vowel (V)
Source phonemes in P
(/AA/, /AE/, and so on)
Source phoneme types: Consonant (C), Vowel (V),
Semi-vowel (SV), and silence (/∼/)
Target graphemes in T

Table 1: Feature types used for transliteration models: fS,Stype indicates both fS and fStype ,
while fP,P type indicates both fP and fP type .

each transliteration model is to produce the most relevant output from its input. The
performance of a transliteration model therefore depends strongly on that of its component
functions. In other words, the better the modeling of each component function, the better
the performance of the machine transliteration system.
The modeling strongly depends on the feature type. Different feature types are used
by the φ(SP )T , φP T , and φST functions, as shown in Table 2. These three component
functions thus have different strengths and weaknesses for machine transliteration. The
φST function is good at producing grapheme-based transliterations and poor at producing
125

Oh, Choi, & Isahara

Notation
φSP
φ(SP )T
φP T
φST

Feature types used
fS,Stype , fP
fS,Stype , fP,P type , fT
fP,P type , fT
fS,Stype , fT

Input
si , c(si )
si , pi , c(si ), c(pi )
pi , c(pi )
si , c(si )

Output
pi
ti
ti
ti

Table 2: Definition of each component function: si , c(si ), pi , c(pi ), and ti respectively represent the ith source grapheme, the context of si (si−n , · · · , si−1 and si+1 , · · · , si+n ),
the ith source phoneme, the context of pi (pi−n , · · · , pi−1 and pi+1 , · · · , pi+n ), and
the ith target grapheme.

phoneme-based ones. In contrast, the φP T function is good at producing phoneme-based
transliterations and poor at producing grapheme-based ones. For amylase and its standard
Korean transliteration, ‘a-mil-la-a-je’, which is a grapheme-based transliteration, φST tends
to produce the correct transliteration; φP T tends to produce wrong ones like ‘ae-meol-le-iseu’, which is derived from /AE M AH L EY S/, the pronunciation of amylase. In contrast,
φP T can produce ‘de-i-teo’, which is the standard Korean transliteration of data and a
phoneme-based transliteration, while φST tends to give a wrong one, like ‘da-ta’.
The φ(SP )T function combines the advantages of φST and φP T by utilizing the correspondence between the source grapheme and source phoneme. This correspondence enables φ(SP )T to produce both grapheme-based and phoneme-based transliterations. Furthermore, the correspondence provides important clues for use in resolving transliteration
ambiguities7 . For example, the source phoneme /AH/ produces much ambiguity in machine transliteration because it can be mapped to almost every vowel in the source and
target languages (the underlined graphemes in the following example corresponds to /AH/:
holocaust in English, ‘hol-lo-ko-seu-teu’ in its Korean counterpart, and ‘ho-ro-ko-o-su-to’ in
its Japanese counterpart). If we know the correspondence between the source grapheme and
source phoneme, we can more easily infer the correct transliteration of /AH/ because the
correct target grapheme corresponding to /AH/ usually depends on the source grapheme
corresponding to /AH/. Moreover, there are various Korean transliterations of the source
grapheme a: ‘a’, ‘ae’, ‘ei’, ‘i’, and ‘o’. In this case, the English phonemes corresponding
to the English grapheme can help a component function resolve transliteration ambiguities, as shown in Table 3. In Table 3, the a underlined in the example words shown in
the last column is pronounced as the English phoneme in the second column. By looking
at English grapheme and its corresponding English phoneme, we can find correct Korean
transliterations more easily.
Though φ(SP )T is more effective than both φST and φP T in many cases, φ(SP )T sometimes works poorly when the standard transliteration is strongly biased to either graphemebased or phoneme-based transliteration. In such cases, either the source grapheme or source
phoneme does not contribute to the correct transliteration, making it difficult for φ(SP )T
to produce the correct transliteration. Because φST , φP T , and φ(SP )T are the core parts
7. Though contextual information can also be used to reduce ambiguities, we limit our discussion here to
the feature type.

126

A Comparison of Machine Transliteration Models

Korean Grapheme
‘a’
‘ae’
‘ei’
‘i’
‘o’

English Phoneme
/AA/
/AE/
/EY/
/IH/
/AO/

Example usage
adagio, safari, vivace
advantage, alabaster, travertine
chamber, champagne, chaos
advantage, average, silage
allspice, ball, chalk

Table 3: Examples of Korean graphemes derived from English grapheme a and its corresponding English phonemes: the underlines in the example words indicate the
English grapheme corresponding to English phonemes in the second column.

of ψG , ψP , and ψC , respectively, the advantages and disadvantages of the three component
functions correspond to those of the transliteration models in which each is used.
Transliteration usually depends on context. For example, the English grapheme a can
be transliterated into Korean graphemes on the basis of its context, like ‘ei’ in the context
of -ation and ‘a’ in the context of art. When context information is used, determining
the context window size is important. A context window that is too narrow can degrade
transliteration performance because of a lack of context information. For example, when
English grapheme t in -tion is transliterated into Korean, the one right English grapheme is
insufficient as context because the three right contexts, -ion, are necessary to get the correct
Korean grapheme, ‘s’. A context window that is too wide can also degrade transliteration
performance because it reduces the power to resolve transliteration ambiguities. Many
previous studies have determined that an appropriate context window size is 3. In this
paper, we use a window size of 3, as in previous work (Kang & Choi, 2000; Goto et al.,
2003). The effect of the context window size on transliteration performance will be discussed
in Section 4.
Table 4 shows how to identify the most relevant output in each component function using
context information. The L3-L1, C0, and R1-R3 represent the left context, current context
(i.e., that to be transliterated), and right context, respectively. The φSP function produces
the most relevant source phoneme for each source grapheme. If SW = s1 · s2 · . . . · sn is
an English word, SW ’s pronunciation can be represented as a sequence of source phonemes
produced by φSP ; that is, PSW = p1 · p2 · . . . · pn , where pi = φSP (si , c(si )). φSP transforms
source graphemes into phonemes in two ways. The first one is to search in a pronunciation
dictionary containing English words and their pronunciation (CMU, 1997). The second one
is to estimate the pronunciation (or automatic grapheme-to-phoneme conversion) (Andersen, Kuhn, Lazarides, Dalsgaard, Haas, & Noth, 1996; Daelemans & van den Bosch, 1996;
Pagel, Lenzo, & Black, 1998; Damper, Marchand, Adamson, & Gustafson, 1999; Chen,
2003). If an English word is not registered in the pronunciation dictionary, we must estimate its pronunciation. The produced pronunciation is used for φP T in ψP and φ(SP )T in
ψC . For training the automatic grapheme-to-phoneme conversion in φSP , we use The CMU
Pronouncing Dictionary (CMU, 1997).
The φST , φP T , and φ(SP )T functions produce target graphemes using their input. Like
φSP , these three functions use their previous outputs, which are represented by fT . As
127

Oh, Choi, & Isahara

φSP

φST

φP T

φ(SP )T

T ype
fS
fStype
fP
fS
fStype
fT
fP
fP type
fT
fS
fP
fStype
fP type
fT

L3
$
$
$
$
$
$
$
$
$
$
$
$
$
$

L2
$
$
$
$
$
$
$
$
$
$
$
$
$
$

L1
$
$
$
$
$
$
$
$
$
$
$
$
$
$

C0
b
C

R1
o
V

R2
a
V

R3
r
C

Output
→

/B/

a
V

r
C

→

‘b’

/∼/
/∼/

/R/
C

→

‘b’

a
/∼/
V
/∼/

r
/R/
C
C

→

‘b’

²
b
C
/B/
C
b
/B/
C
C

o
V
²
/AO/
V
²
o
/AO/
V
V
²

Table 4: Framework for each component function: $ represents start of words and ² means
unused contexts for each component function.

shown in Table 4, φST , φP T , and φ(SP )T produce target grapheme ‘b’ for source grapheme
b and source phoneme /B/ in board and /B AO R D/. Because the b and /B/ are the
first source grapheme of board and the first source phoneme of /B AO R D/, respectively,
their left context is $, which represents the start of words. Source graphemes (o, a, and r )
and their type (V: vowel, V: vowel, and C: consonant) can be the right context in φST and
φ(SP )T . Source phonemes (/AO/, /∼/, and /R/) and their type (V: vowel, /∼/: silence,
V: vowel) can be the right context in φP T and φ(SP )T . Depending on the feature type
used in each component function and described in Table 2, φST , φP T , and φ(SP )T produce
a sequence of target graphemes, TSW = t1 · t2 · . . . · tn , for SW = s1 · s2 · . . . · sn and
PSW = p1 · p2 · . . . · pn . For board, SW , PSW , and TSW can be represented as follows. The
/∼/ represents silence (null source phonemes), and the ‘∼’ represents null target graphemes.
• SW = s1 · s2 · s3 · s4 · s5 = b · o · a · r · d
• PSW = p1 · p2 · p3 · p4 · p5 = /B/ · /AO/ · / ∼ / · /R/ · /D/
• TSW = t1 · t2 · t3 · t4 · t5 = ‘b’· ‘o’ · ‘∼’ · ‘∼’ · ‘deu’
3.3 Machine Learning Algorithms for Each Component Function
In this section we describe a way to model component functions using three machine learning algorithms (the maximum entropy model, decision-tree learning, and memory-based
learning)8 . Because the four component functions share a similar framework, we limit our
focus to φ(SP )T in this section.
8. These three algorithms are typically applied to automatic grapheme-to-phoneme conversion (Andersen
et al., 1996; Daelemans & van den Bosch, 1996; Pagel et al., 1998; Damper et al., 1999; Chen, 2003).

128

A Comparison of Machine Transliteration Models

3.3.1 Maximum entropy model
The maximum entropy model (MEM) is a widely used probability model that can incorporate heterogeneous information effectively (Berger, Pietra, & Pietra, 1996). In the
MEM, an event (ev) is usually composed of a target event (te) and a history event (he);
say ev =< te, he >. Event ev is represented by a bundle of feature functions, f ei (ev),
which represent the existence of certain characteristics in event ev. A feature function is
a binary-valued function. It is activated (f ei (ev) = 1) when it meets its activating condition; otherwise it is deactivated (f ei (ev) = 0) (Berger et al., 1996). Let source language
word SW be composed of n graphemes. SW, PSW , and TSW can then be represented as
SW = s1 , · · · , sn , PSW = p1 , · · · , pn , and TSW = t1 , · · · , tn , respectively. PSW and TSW
represent the pronunciation and target language word corresponding to SW, and pi and ti
represent the source phoneme and target grapheme corresponding to si . Function φ(SP )T
based on the maximum entropy model can be represented as
P r(TSW |SW, PSW ) = P r(t1 , · · · , tn |s1 , · · · , sn , p1 , · · · , pn )

(1)

With the assumption that φ(SP )T depends on the context information in window size k, we
simplify Formula (1) to
P r(TSW |SW, PSW ) ≈

Y

P r(ti |ti−k , · · · , ti−1 , pi−k , · · · , pi+k , si−k , · · · , si+k )

(2)

i

Because t1 , · · · , tn , s1 , · · · , sn , and p1 , · · · , pn can be represented by fT , fS,Stype , and fP,P type ,
respectively, we can rewrite Formula (2) as
P r(TSW |SW, PSW ) ≈

Y

P r(ti |fT(i−k,i−1) , fP,P type(i−k,i+k) , fS,Stype(i−k,i+k) )

(3)

i

where i is the index of the current source grapheme and source phoneme to be transliterated
and fX(l,m) represents the features of feature type fX located from position l to position m.
An important factor in designing a model based on the maximum entropy model is
to identify feature functions that effectively support certain decisions of the model. Our
basic philosophy of feature function design for each component function is that the context
information collocated with the unit of interest is important. We thus designed the feature
function with collocated features in each feature type and between different feature types.
Features used for φ(SP )T are listed below. These features are used as activating conditions
or history events of feature functions.
• Feature type and features used for designing feature functions in φ(SP )T (k = 3)
– All possible features in fS,Stypei−k,i+k , fP,P typei−k,i+k , and fTi−k,i−1 (e.g., fSi−1 ,
fPi−1 , and fTi−1 )
– All possible feature combinations between features of the same feature type (e.g.,
{fSi−2 , fSi−1 , fSi+1 }, {fPi−2 , fPi , fPi+2 }, and {fTi−2 , fTi−1 })
– All possible feature combinations between features of different feature types (e.g.,
{fSi−1 , fPi−1 }, {fSi−1 , fTi−2 } , and {fP typei−2 , fPi−3 , fTi−2 })
∗ between fS,Stypei−k,i+k and fP,P typei−k,i+k
129

Oh, Choi, & Isahara

f ej

te
ti

f e1
f e2
f e3
f e4
f e5

‘b’
‘b’
‘b’
‘b’
‘b’

he
fT(i−k,i−1)

fS,Stype(i−k,i+k)

–
–
fTi−1 = $
–
fTi−2 = $

fSi+1

fP,P type(i−k,i+k)

fSi = b
fSi−1 = $
= o and fStypei+2 = V
–
fSi+3 = r

fPi = /B/
–
fPi = /B/
fPi+1 = /AO/
fP typei = C

Table 5: Feature functions for φ(SP )T derived from Table 4.

∗ between fS,Stypei−k,i+k and fTi−k,i−1
∗ between fP,P typei−k,i+k and fTi−k,i−1
Generally, a conditional maximum entropy model that gives the conditional probability
P r(y|x) is represented as Formula (4) (Berger et al., 1996).
P r(y|x) =
Z(x) =

X
1
exp( λi f ei (x, y))
Z(x)
i

X

exp(

y

X

(4)

λi f ei (x, y))

i

In φ(SP )T , the target event (te) is target graphemes to be assigned, and the history event
(he) can be represented as a tuple < fT(i−k,i−1) , fS,Stype(i−k,i+k) , fP,P type(i−k,i+k) >. Therefore,
we can rewrite Formula (3) as
P r(ti |fT(i−k,i−1) , fS,Stype(i−k,i+k) , fP,P type(i−k,i+k) )
X
1
exp( λi f ei (he, te))
= P r(te|he) =
Z(he)
i

(5)

Table 5 shows example feature functions for φ(SP )T ; Table 4 was used to derive the
functions. For example, f e1 represents an event where he (history event) is “fSi is b and
fPi is /B/” and te (target event) is “fTi is ‘b’”. To model each component function based
on the MEM, Zhang’s maximum entropy modeling tool is used (Zhang, 2004).
3.3.2 Decision-tree learning
Decision-tree learning (DTL) is one of the most widely used and well-known methods for
inductive inference (Quinlan, 1986; Mitchell, 1997). ID3, which is a greedy algorithm
that constructs decision trees in a top-down manner, uses the information gain, which is a
measure of how well a given feature (or attribute) separates training examples on the basis of
their target class (Quinlan, 1993; Manning & Schutze, 1999). We use C4.5 (Quinlan, 1993),
which is a well-known tool for DTL and an implementation of Quinlan’s ID3 algorithm.
The training data for each component function is represented by features located in L3L1, C0, and R1-R3, as shown in Table 4. C4.5 tries to construct a decision tree by looking
for regularities in the training data (Mitchell, 1997). Figure 2 shows part of the decision
130

A Comparison of Machine Transliteration Models

tree constructed for φ(SP )T in English-to-Korean transliteration. A set of the target classes
in the decision tree for φ(SP )T is a set of the target graphemes. The rectangles indicate the
leaf nodes, which represent the target classes, and the circles indicate the decision nodes.
To simplify our examples, we use only fS and fP . Note that all feature types for each
component function, as described in Table 4, are actually used to construct decision trees.
Intuitively, the most effective feature from among L3-L1, C0, and R1-R3 for φ(SP )T may be
located in C0 because the correct outputs of φ(SP )T strongly depend on the source grapheme
or source phoneme in the C0 position. As we expected, the most effective feature in the
decision tree is located in the C0 position, that is, C0(fP ). (Note that the first feature
to be tested in decision trees is the most effective feature.) In Figure 2, the decision tree
produces the target grapheme (Korean grapheme) ‘o’ for the instance x(SP T ) by retrieving
the decision nodes from C0(fP ) = /AO/ to R1(fP ) = / ∼ / represented by ‘∗’.

C0(f
C0(fPP): /AO/ (*)
C0(fSS): e

C0(fSS): a

‘o’

‘a’

C0(f
C0(fSS): o(*)

x(SPT)

C0(fSS): others
……
‘eu’

R1(fPP): /R/

L2(fSS): $

C0(fSS): i

L2(fSS): a

R1(f
R1(fPP): /~/(*)

R1(fPP): others

‘‘o’
o’ (*)
(*)

‘o’

…… L2(fSS): r

Feature type L3 L2
fS

$

$

fP

$

$

L1

C0

R1

R2

R3

b

o

a

r

d

φ(SP)T
→

‘o’

/B/ /AO/ /~/ /R/ /D/

Figure 2: Decision tree for φ(SP )T .

3.3.3 Memory-based learning
Memory-based learning (MBL), also called “instance-based learning” and “case-based learning”, is an example-based learning method. It is based on a k-nearest neighborhood algorithm (Aha, Kibler, & Albert, 1991; Aha, 1997; Cover & Hart, 1967; Devijver & Kittler.,
1982). MBL represents training data as a vector and, in the training phase, it places all
training data as examples in memory and clusters some examples on the basis of the knearest neighborhood principle. Training data for MBL is represented in the same form
as training data for a decision tree. Note that the target classes for φ(SP )T , which MBL
outputs, are target graphemes. Feature weighting to deal with features of differing importance is also done in the training phase9 . It then produces an output using similarity-based
9. TiMBL (Daelemans, Zavrel, Sloot, & Bosch, 2004) supports gain ratio weighting, information gain
weighting, chi-squared (χ2 ) weighting, and shared variance weighting of the features.

131

Oh, Choi, & Isahara

reasoning between test data and the examples in memory. If the test data is x and the
set of examples in memory is Y , the similarity between x and Y can be estimated using
distance function ∆(x, Y )10 . MBL selects an example yi or the cluster of examples that are
most similar to x and then assigns the example’s target class to x’s target class. We use
an MBL tool called TiMBL (Tilburg memory-based learner) version 5.0 (Daelemans et al.,
2004).

4. Experiments
We tested the four machine transliteration models on English-to-Korean and English-toJapanese transliteration. The test set for the former (EKSet) (Nam, 1997) consisted of
7,172 English-Korean pairs – the number of training items was about 6,000 and that of the
blind test items was about 1,000. EKSet contained no transliteration variations, meaning
that there was one transliteration for each English word. The test set for the latter (EJSet)
contained English-katakana pairs from EDICT (Breen, 2003) and consisted of 10,417 pairs
– the number of training items was about 9,000 and that of the blind test items was about
1,000. EJSet contained transliteration variations, like <micro, ‘ma-i-ku-ro’>, and <micro,
‘mi-ku-ro’>; the average number of Japanese transliterations for an English word was 1.15.
EKSet and EJSet covered proper names, technical terms, and general terms. We used
The CMU Pronouncing Dictionary (CMU, 1997) for training pronunciation estimation (or
automatic grapheme-to-phoneme conversion) in φSP . The training for automatic graphemeto-phoneme conversion was done ignoring the lexical stress of vowels in the dictionary (CMU,
1997). The evaluation was done in terms of word accuracy (W A), the evaluation measure
used in previous work (Kang & Choi, 2000; Kang & Kim, 2000; Goto et al., 2003; Bilac &
Tanaka, 2004). Here, W A can be represented as Formula (6). A generated transliteration
for an English word was judged to be correct if it exactly matched a transliteration for that
word in the test data.
WA =

number of correct transliterations output by system
number of transliterations in blind test data

(6)

In the evaluation, we used k-fold cross-validation (k=7 for EKSet and k=10 for EJSet). The
test set was divided into k subsets. Each was used in turn for testing while the remainder was
used for training. The average W A computed across all k trials was used as the evaluation
results presented in this section.
We conducted six tests.
• Hybrid Model Test: Evaluation of hybrid transliteration model by changing value of α
(the parameter of the hybrid transliteration model)
• Comparison Test I: Comparison among four machine transliteration models
• Comparison Test II: Comparison of four machine transliteration models to previously
proposed transliteration methods
10. Modified value difference metric, overlap metric, Jeffrey divergence metric, dot product metric, etc. are
used as the distance function (Daelemans et al., 2004).

132

A Comparison of Machine Transliteration Models

• Dictionary Test: Evaluation of transliteration models on words registered and not
registered in pronunciation dictionary to determine effect of pronunciation dictionary
on each model
• Context Window-Size Test: Evaluation of transliteration models for various sizes of
context window
• Training Data-Size Test: Evaluation of transliteration models for various sizes of training data sets
4.1 Hybrid Model Test
The objective of this test was to estimate the dependence of the performance of ψH on
parameter α. We evaluated the performance by changing α from 0 to 1 at intervals of
0.1 (i.e., α=0, 0.1, 0.2, · · ·, 0.9, 1.0). Note that the hybrid model can be represented as
“α × P r(ψP ) + (1 − α) × P r(ψG )”. Therefore, ψH is ψG when α = 0 and ψP when α = 1.
As shown in Table 6, the performance of ψH depended on that of ψG and ψP . For example,
the performance of ψG exceeded that of ψP for EKSet. Therefore, ψH tended to perform
better when α ≤ 0.5 than when α > 0.5 for EKSet. The best performance was attained
when α = 0.4 for EKSet and when α = 0.5 for EJSet. Hereinafter, we use α = 0.4 for
EKSet and α = 0.5 for EJSet as the linear interpolation parameter for ψH .
α
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0

EKSet
58.8%
61.2%
62.0%
63.0%
64.1%
63.4%
61.1%
59.6%
58.2%
57.0%
55.2%

EJSet
58.8%
60.9%
62.6%
64.1%
65.4%
65.8%
65.0%
63.4%
62.1%
61.2%
59.2%

Table 6: Results of Hybrid Model Test.

4.2 Comparison Test I
The objectives of the first comparison test were to compare performance among the four
transliteration models (ψG , ψP , ψH , and ψC ) and to compare the performance of each model
with the combined performance of three of the models (ψG+P +C ). Table 7 summarizes the
performance of each model for English-to-Korean and English-to-Japanese transliteration,
133

Oh, Choi, & Isahara

where DTL, MBL11 and MEM represent decision-tree learning, memory-based learning,
and maximum entropy model.
The unit to be transliterated was restricted to either a source grapheme or a source
phoneme in ψG and ψP ; it was dynamically selected on the basis of the contexts in ψH
and ψC . This means that ψG and ψP could produce an incorrect result if either a source
phoneme or a source grapheme, which, respectively, they do not consider, holds the key to
producing the correct transliteration result. For this reason, ψH and ψC performed better
than both ψG and ψP .
Transliteration Model
ψG
ψP
ψH
ψC
ψG+P +C

DTL
53.1%
50.8%
N/A
59.5%
72.0%

EKSet
MBL
54.6%
50.6%
N/A
60.3%
71.4%

MEM
58.8%
55.2%
64.1%
65.5%
75.2%

DTL
55.6%
55.8%
N/A
64.0%
73.4%

EJSet
MBL
58.9%
56.1%
N/A
65.8%
74.2%

MEM
58.8%
59.2%
65.8%
69.1%
76.6%

Table 7: Results of Comparison Test I.
In the table, ψG+P +C means the combined results for the three transliteration models,
ψG , ψP , and ψC . We exclude ψH from the combining because it is implemented only
with the MEM (the performance of combining the four transliteration models are discussed
in Section 5). In evaluating ψG+P +C , we judged the transliteration results to be correct
if there was at least one correct transliteration among the results produced by the three
models. Though ψC showed the best results among the three transliteration models due to
its ability to use the correspondence between the source grapheme and source phoneme, the
source grapheme or the source phoneme can create noise when the correct transliteration
is produced by the other one. In other words, when the correct transliteration is strongly
biased to either grapheme-based or phoneme-based transliteration, ψG and ψP may be more
suitable for producing the correct transliteration.
Table 8 shows example transliterations produced by each transliteration model. The
ψG produced correct transliterations for cyclase and bacteroid, while ψP did the same for
geoid and silo. ψC produced correct transliterations for saxhorn and bacteroid, and ψH
produced correct transliterations for geoid and bacteroid. As shown by these results, there
are transliterations that only one transliteration model can produce correctly. For example,
only ψG , ψP , and ψC produced the correct transliterations of cyclase, silo, and saxhorn,
respectively. Therefore, these three transliteration models can be used in a complementary
manner to improve transliteration performance because at least one can usually produce the
correct transliteration. This combination increased the performance by compared to ψG ,
ψP , and ψC (on average, 30.1% in EKSet and 24.6% in EJSet). In short, ψG , ψP , and ψC are
complementary transliteration models that together produce more correct transliterations,
11. We tested all possible combinations between ∆(x, Y ) and a weighting scheme supported by
TiMBL (Daelemans et al., 2004) and did not detect any significant differences in performance for the
various combinations. Therefore, we used the default setting of TiMBL (Overlap metric for ∆(x, Y ) and
gain ratio weighting for feature weighting).

134

A Comparison of Machine Transliteration Models

so combining different transliteration models can improve transliteration performance. The
transliteration results produced by ψG+P +C are analyzed in detail in Section 5.

cyclase
bacteroid
geoid
silo
saxhorn
cyclase
bacteroid
geoid
silo
saxhorn

ψG
‘si-keul-la-a-je’
‘bak-te-lo-i-deu’
∗‘je-o-i-deu’
∗‘sil-lo’
∗‘saek-seon’
ψH
∗‘sa-i-keul-la-a-je’
‘bak-te-lo-i-deu’
‘ji-o-i-deu’
∗‘sil-lo’
∗‘saek-seon’

ψP
∗‘sa-i-keul-la-a-je’
∗‘bak-teo-o-i-deu’
‘ji-o-i-deu’
‘sa-il-lo’
∗‘saek-seu-ho-leun’
ψC
∗‘sa-i-keul-la-a-je’
‘bak-te-lo-i-deu’
∗‘ge-o-i-deu’
∗‘sil-lo’
‘saek-seu-hon’

Table 8: Example transliterations produced by each transliteration model (∗ indicates an
incorrect transliteration).
In our subsequent testing, we used the maximum entropy model as the machine learning
algorithm for two reasons. First, it produced the best results of the three algorithms we
tested12 . Second, it can support ψH .
4.3 Comparison Test II
In this test, we compared four previously proposed machine transliteration methods (Kang
& Choi, 2000; Kang & Kim, 2000; Goto et al., 2003; Bilac & Tanaka, 2004) to the four
transliteration models (ψG , ψP , ψH , and ψC ), which were based on the MEM. Table 9 shows
the results. We trained and tested the previous methods with the same data sets used for
the four transliteration models. Table 10 shows the key features of the methods and models
from the viewpoint of information type and usage. Information type indicates the type of
information considered: source grapheme, source phoneme, and correspondence between
the two. For example, the first three methods use only the source grapheme. Information
usage indicates the context used and whether the previous output is used.
It is obvious from the table that the more information types a transliteration model
considers, the better its performance. Either the source phoneme or the correspondence –
which are not considered in the methods of Kang and Choi (2000), Kang and Kim (2000),
and Goto et al. (2003) – is the key to the higher performance of the method of Bilac and
Tanaka (2004) and the ψH and ψC .
From the viewpoint of information usage, the models and methods that consider the
previous output tended to achieve better performance. For example, the method of Goto et
al. (2003) had better results than that of Kang and Choi (2000). Because machine translit12. A one-tail paired t-test showed that the results with the MEM were always significantly better (except
for φG in EJSet) than those of DTL and MBL (level of significance = 0.001).

135

Oh, Choi, & Isahara

Method/Model
Kang and Choi (2000)
Kang and Kim (2000)
Previous methods
Goto et al. (2003)
Bilac and Tanaka (2004)
ψG
ψP
MEM-based models
ψH
ψC

EKSet
51.4%
55.1%
55.9%
58.3%
58.8%
55.2%
64.1%
65.5%

EJSet
50.3%
53.2%
56.2%
62.5%
58.8%
59.2%
65.8%
69.1%

Table 9: Results of Comparison Test II.

Method/Model
Kang and Choi (2000)
Kang and Kim (2000)
Goto et al. (2003)
Bilac and Tanaka (2004)
ψG
ψP
ψH
ψC

Info. type
S P C
+ – –
+ – –
+ – –
+ + –
+ – –
– + –
+ + –
+ + +

Info. usage
Context
< −3 ∼ +3 >
Unbounded
< −3 ∼ +3 >
Unbounded
< −3 ∼ +3 >
< −3 ∼ +3 >
< −3 ∼ +3 >
< −3 ∼ +3 >

PO
–
+
+
–
+
+
+
+

Table 10: Information type and usage for previous methods and four transliteration models, where S, P, C, and PO respectively represent the source grapheme, source
phoneme, correspondence between S and P, and previous output.

eration is sensitive to context, a reasonable context size usually enhances transliteration
ability. Note that the size of the context window for the previous methods was limited to 3
because a context window wider than 3 degrades performance (Kang & Choi, 2000) or does
not significantly improve it (Kang & Kim, 2000). Experimental results related to context
window size are given in Section 4.5.
Overall, ψH and ψC had better performance than the previous methods (on average,
17.04% better for EKSet and 21.78% better for EJSet), ψG (on average, 9.6% better for
EKSet and 14.4% better for EJSet), and ψP (on average, 16.7% better for EKSet and
19.0% better for EJSet). In short, a good machine transliteration model should 1) consider
either the correspondence between the source grapheme and the source phoneme or both
the source grapheme and the source phoneme, 2) have a reasonable context size, and 3)
consider previous output. The ψH and ψC satisfy all three conditions.
136

A Comparison of Machine Transliteration Models

4.4 Dictionary Test
Table 11 shows the performance of each transliteration model for the dictionary test. In this
test, we evaluated four transliteration models according to a way of pronunciation generation
(or grapheme-to-phoneme conversion). Registered represents the performance for words
registered in the pronunciation dictionary, and Unregistered represents that for unregistered
words. On average, the number of Registered words in EKSet was about 600, and that in
EJSet was about 700 in k-fold cross-validation test data. In other words, Registered words
accounted for about 60% of the test data in EKSet and about 70% of the test data in
EJSet. The correct pronunciation can always be acquired from the pronunciation dictionary
for Registered words, while the pronunciation must be estimated for Unregistered words
through automatic grapheme-to-phoneme conversion. However, the automatic graphemeto-phoneme conversion does not always produce correct pronunciations – the estimated rate
of correct pronunciations was about 70% accuracy.

ψG
ψP
ψH
ψC
ALL

EKSet
Registered Unregistered
60.91%
55.74%
66.70%
38.45%
70.34%
53.31%
73.32%
54.12%
80.78%
68.41%

EJSet
Registered Unregistered
61.18%
50.24%
64.35%
40.78%
70.20%
50.02%
74.04%
51.39%
81.17%
62.31%

Table 11: Results of Dictionary Test: ALL means ψG+P +H+C .

Analysis of the results showed that the four transliteration models fall into three categories. Since the ψG is free from the need for correct pronunciation, that is, it does not use
the source phoneme, its performance is not affected by pronunciation correctness. Therefore,
ψG can be regarded as the baseline performance for Registered and Unregistered. Because
ψP (φP T ◦ φSP ), ψH (α× P r(ψP )+(1 − α)× P r(ψG )), and ψC (φ(SP )T ◦ φSP ) depend on
the source phoneme, their performance tends to be affected by the performance of φSP .
Therefore, ψP , ψH , and ψC show notable differences in performance between Registered
and Unregistered. However, the performance gap differs with the strength of the dependence. ψP falls into the second category: its performance strongly depends on the correct
pronunciation. ψP tends to perform well for Registered and poorly for Unregistered. ψH
and ψC weakly depend on the correct pronunciation. Unlike ψP , they make use of both
the source grapheme and source phoneme. Therefore, they can perform reasonably well
without the correct pronunciation because using the source grapheme weakens the negative
effect of incorrect pronunciation in machine transliteration.
Comparing ψC and ψP , we find two interesting things. First, ψP was more sensitive to
errors in φSP for Unregistered. Second, ψC showed better results for both Registered and
Unregistered. Because ψP and ψC share the same function, φSP , the key factor accounting
for the performance gap between them is the component functions, φP T and φ(SP )T . From
the results shown in Table 11, we can infer that φ(SP )T (in ψC ) performed better than
φP T (in ψP ) for both Registered and Unregistered. In φ(SP )T , the source grapheme corre137

Oh, Choi, & Isahara

sponding to the source phonemes, which φP T does not consider, made two contributions
to the higher performance of φ(SP )T . First, the source grapheme in the correspondence
made it possible to produce more accurate transliterations. Because φ(SP )T considers the
correspondence, φ(SP )T has a more powerful transliteration ability than φP T , which uses
just the source phonemes, when the correspondence is needed to produce correct transliterations. This is the main reason φ(SP )T performed better than φP T for Registered. Second,
source graphemes in the correspondence compensated for errors produced by φSP in producing target graphemes. This is the main reason φ(SP )T performed better than φP T for
Unregistered. In the comparison between ψC and ψG , the performances were similar for Unregistered. This indicates that the transliteration power of ψC is similar to that of ψG , even
though the pronunciation of the source language word may not be correct. Furthermore, the
performance of ψC was significantly higher than that of ψG for Registered. This indicates
that the transliteration power of ψC is greater than that of ψG if the correct pronunciation
is given.
The behavior of ψH was similar to that of ψC . For Unregistered, P r(ψG ) in ψH made
it possible for ψH to avoid errors caused by P r(ψP ). Therefore, it worked better than ψP .
For Registered, P r(ψP ) enabled ψH to perform better than ψG .
The results of this test showed that ψH and ψC perform better than ψG and ψP while
complementing ψG and ψP (and thus overcoming their disadvantage) by considering either
the correspondence between the source grapheme and the source phoneme or both the
source grapheme and the source phoneme.
4.5 Context Window-Size Test
In our testing of the effect of the context window size, we varied the size from 1 to 5.
Regardless of the size, ψH and ψC always performed better than both ψG and ψP . When
the size was 4 or 5, each model had difficulty identifying regularities in the training data.
Thus, there were consistent drops in performance for all models when the size was increased
from 3 to 4 or 5. Although the best performance was obtained when the size was 3, as shown
in Table 12, the differences in performance were not significant in the range of 2-4. However,
there was a significant difference between a size of 1 and a size of 2. This indicates that
a lack of contextual information can easily lead to incorrect transliteration. For example,
to produce the correct target language grapheme of t in -tion, we need the right three
graphemes (or at least the right two) of t, -ion (or -io). The results of this testing indicate
that the context size should be more than 1 to avoid degraded performance.
4.6 Training Data-Size Test
Table 13 shows the results of the Training Data-Size Test using MEM-based machine
transliteration models. We evaluated the performance of the four models and ALL while
varying the size of the training data from 20% to 100%. Obviously, the more training data
used, the higher the system performance. However, the objective of this test was to determine whether the transliteration models perform reasonably well even for a small amount
of training data. We found that ψG was the most sensitive of the four models to the amount
of training data; it had the largest difference in performance between 20% and 100%. In
contrast, ALL showed the smallest performance gap. The results of this test shows that
138

A Comparison of Machine Transliteration Models

Context Size
1
2
3
4
5

ψG
44.9%
57.3%
58.8%
56.1%
53.7%

Context Size
1
2
3
4
5

ψG
46.4%
58.2%
58.8%
56.4%
53.9%

EKSet
ψP
44.9%
52.8%
55.2%
54.6%
52.6%
EJSet
ψP
52.1%
59.5%
59.2%
58.5%
56.4%

ψH
51.8%
61.7%
64.1%
61.8%
60.4%

ψC
52.4%
64.4%
65.5%
64.3%
62.5%

ALL
65.8%
74.4%
75.8%
74.4%
73.9%

ψH
58.0%
65.6%
65.8%
64.4%
62.9%

ψC
62.0%
68.7%
69.1%
68.2%
66.3%

ALL
70.4%
76.3%
77.0%
76.0%
75.5%

Table 12: Results of Context Window-Size Test: ALL means ψG+P +H+C .

combining different transliteration models is helpful in producing correct transliterations
even if there is little training data.

Training Data Size
20%
40%
60%
80%
100%
Training Data Size
20%
40%
60%
80%
100%

EKSet
ψG
ψP
46.6% 47.3%
52.6% 51.5%
55.2% 53.0%
58.9% 54.0%
58.8% 55.2%
EJSet
ψG
ψP
47.6% 51.2%
52.4% 55.1%
55.2% 57.3%
57.9% 58.8%
58.8% 59.2%

ψH
53.4%
58.7%
61.5%
62.6%
64.1%

ψC
57.0%
62.1%
63.3%
64.6%
65.5%

ALL
67.5%
71.6%
73.0%
74.7%
75.8%

ψH
56.4%
60.7%
62.9%
65.4%
65.8%

ψC
60.4%
64.8%
66.6%
68.0%
69.1%

ALL
69.6%
72.6%
74.7%
76.7%
77.0%

Table 13: Results of Training Data-Size Test: ALL means ψG+P +H+C .

5. Discussion
Figures 3 and 4 show the distribution of the correct transliterations produced by each
transliteration model and by the combination of models, all based on the MEM. The ψG ,
139

Oh, Choi, & Isahara

ψP , ψH , and ψC in the figures represent the set of correct transliterations produced by each
model through k-fold validation. For example, |ψG | = 4,220 for EKSet and |ψG | = 6,121
for EJSet mean that ψG produced 4,220 correct transliterations for 7,172 English words
in EKSet (|KT G| in Figure 3) and 6,121 correct ones for 10,417 English words in EJSet
(|JT G| in Figure 4). An important factor in modeling a transliteration model is to reflect the
dynamic transliteration behaviors, which means that a transliteration process dynamically
uses the source grapheme and source phoneme to produce transliterations. Due to these
dynamic behaviors, a transliteration can be grapheme-based transliteration, phoneme-based
transliteration, or some combination of the two. The forms of transliterations are classified
on the basis of the information upon which the transliteration process mainly relies (either
a source grapheme or a source phoneme or some combination of the two). Therefore, an
effective transliteration system should be able to produce various types of transliterations
at the same time. One way to accommodate the different dynamic transliteration behaviors
is to combine different transliteration models, each of which can handle a different behavior.
Synergy can be achieved by combining models so that one model can produce the correct
transliteration when the others cannot. Naturally, if the models tend to produce the same
transliteration, less synergy can be realized from combining them. Figures 3 and 4 show the
synergy gained from combining transliteration models in terms of the size of the intersection
and the union of the transliteration models.
ψG
407

ψP
82

680 3,051
344

207
624

ψC

|KTG-(ψG ∪ ψP ∪ ψC )|
=1,777

(a) ψG +ψP +ψC

ψG
188

7

899 3,126
119

ψH

ψP

305

374

ψP
267 129

713 3,423
457
311

ψH

|KTG-(ψG ∪ ψP ∪ ψH )|
=2,002

(b) ψG +ψP +ψH

252

ψC

|KTG-(ψH ∪ ψP ∪ ψC )|
=1,879

(c) ψP +ψH +ψC

ψG
393

ψH
340 369

46 3,685
763

451

ψC

|KTG-(ψG ∪ψH ∪ψC )|
=1,859

(d) ψG +ψH +ψC

Figure 3: Distributions of correct transliterations produced by models for English-toKorean transliteration. KTG represents “Korean Transliterations in the Gold
standard”. Note that |ψG ∪ ψP ∪ ψH ∪ ψC | = 5,439, |ψG ∩ ψP ∩ ψH ∩ ψC | =
3,047, and |KT G| = 7,172.

The figures show that, as the area of intersection between different transliteration models
becomes smaller, the size of their union tends to become bigger. The main characteristics
obtained from these figures are summarized in Table 14. The first thing to note is that
|ψG ∩ ψP | is clearly smaller than any other intersection. The main reason for this is that
ψG and ψP use no common information (ψG uses source graphemes while ψP uses source
phonemes). However, the others use at least one of source grapheme and source phoneme
(source graphemes are information common to ψG , ψH , and ψC while source phonemes
are information common to ψP , ψH , and ψC ). Therefore, we can infer that the synergy
derived from combining ψG and ψP is greater than that derived from the other combinations.
140

A Comparison of Machine Transliteration Models

ψG
379

ψP
141

805 4,796
628

261
963

ψC

ψG
378

ψH

ψP
12

806 4,925
202

222

308

ψP
267 135

786 5,574
916
647

ψH

185

ψC

ψG
207

ψH
313 176

183 5,418
649

942

ψC

|JTG-(ψG ∪ ψP ∪ ψC )|
=2,444

|JTG-(ψG ∪ ψP ∪ ψH )|
=2,870

|JTG-(ψH ∪ ψP ∪ ψC )|
=2,601

|JTG-(ψG ∪ψH ∪ψC )|
=2,529

(a) ψG +ψP +ψC

(b) ψG +ψP +ψH

(c) ψP +ψH +ψC

(d) ψG +ψH +ψC

Figure 4: Distributions of correct transliterations produced by models for English-toJapanese transliteration. JTG represents “Japanese Transliterations in the Gold
standard”. Note that |ψG ∪ ψP ∪ ψH ∪ ψC |=8,021, |ψG ∩ ψP ∩ ψH ∩ ψC |=4,786,
and |JT G| = 10,417.

|ψG |
|ψP |
|ψH |
|ψC |
|ψG ∩ ψP |
|ψG ∩ ψC |
|ψG ∩ ψH |
|ψC ∩ ψH |
|ψP ∩ ψC |
|ψP ∩ ψH |
|ψG ∪ ψP |
|ψG ∪ ψC |
|ψG ∪ ψH |
|ψC ∪ ψH |
|ψP ∪ ψC |
|ψP ∪ ψH |

EKSet
4,202
3,947
4,583
4,680
3,133
3,731
4,025
4,136
3,675
3,583
5,051
5,188
4,796
5,164
4,988
4,982

EJSet
6,118
6,158
6,846
7,189
4,937
5,601
5,731
6,360
5,759
5,841
7,345
7,712
7,239
7,681
7,594
7,169

Table 14: Main characteristics obtained from Figures 3 and 4.

However, the size of the union between the various pairs of transliteration models in Table 14
shows that |ψC ∪ ψH | and |ψG ∪ ψC | are bigger than |ψG ∪ ψP |. The main reason for this
might be the higher transliteration power of ψC and ψH compared to that of ψG and ψP
– ψC and ψH cover more of the KTG and JTG than both ψG and ψP . The second thing
to note is that the contribution of each transliteration model to |ψG ∪ ψP ∪ ψH ∪ ψC | can
be estimated from the difference between |ψG ∪ ψP ∪ ψH ∪ ψC | and the union of the three
other transliteration models. For example, we can measure the contribution of ψH from the
141

Oh, Choi, & Isahara

difference between |ψG ∪ ψP ∪ ψH ∪ ψC | and |ψG ∪ ψP ∪ ψC |. As shown in Figures 3(a)
and 4(a)), ψH makes the smallest contribution while ψC (Figures 3(b) and 4(b)) makes the
largest contribution. The main reason for ψH making the smallest contribution is that it
tends to produce the same transliteration as the others, so the intersection between ψH and
the others tends to be large.
It is also important to rank the transliterations produced by a transliteration system for
a source language word on the basis of their relevance. While a transliteration system can
produce a list of transliterations, each reflecting a dynamic transliteration behavior, it will
fail to perform well unless it can distinguish between correct and wrong transliterations.
Therefore, a transliteration system should be able to produce various kinds of transliterations depending on the dynamic transliteration behaviors and be able to rank them on the
basis of their relevance. In addition, the application of transliteration results to natural
language applications such as machine translation and information retrieval requires that
the transliterations be ranked and assigned a relevance score.
In summary, 1) producing a list of transliterations reflecting dynamic transliteration behaviors (one way is to combine the results of different transliteration models,
each reflecting one of the dynamic transliteration behaviors) and 2) ranking the transliterations in terms of their relevance are both necessary to improve the performance of
machine transliteration. In the next section, we describe a way to calculate the relevance
of transliterations produced by a combination of the four transliteration models.

6. Transliteration Ranking
The basic assumption of transliteration ranking is that correct transliterations are more
frequently used in real-world texts than incorrect ones. Web data reflecting the real-world
usage of transliterations can thus be used as a language resource to rank transliterations.
Transliterations that appear more frequently in web documents are given either a higher
rank or a higher score. The goal of transliteration ranking, therefore, is to rank correct
transliterations higher and rank incorrect ones lower. The transliterations produced for a
given English word by the four transliteration models (ψG , ψP , ψH , and ψC ), based on the
MEM, were ranked using web data.
Our transliteration ranking relies on web frequency (number of web documents). To
obtain reliable web frequencies, it is important to consider a transliteration and its corresponding source language word together rather than the transliteration alone. This is
because our aim is to find correct transliterations corresponding to a source language word
rather than to find transliterations that are frequently used in the target language. Therefore, the best approach to transliteration ranking using web data is to find web documents
in which transliterations are used as translations of the source language word.
A bilingual phrasal search (BPS) retrieves the Web with a Web search engine query,
which is a phrase composed of a transliteration and its source language word (e.g., {‘a-milla-a-je’ amylase}). The BPS enables the Web search engine to find web documents that
contain correct transliterations corresponding to the source language word. Note that a
phrasal query is represented in brackets, where the first part is a transliteration and the
second part is the corresponding source language word. Figure 5 shows Korean and Japanese
web documents retrieved using a BPS for amylase and its Korean/Japanese transliterations,
142

A Comparison of Machine Transliteration Models

Retrieved
RetrievedKorean
Koreanweb
webpages
pagesfor
forquery
query
{‘a-mil-la-a-je’
amylase}
{‘a-mil-la-a-je’ amylase}

Retrieved
RetrievedJapanese
Japaneseweb
webpages
pagesfor
forquery
query
{‘a-mi-ra-a-je’
amylase}
{‘a-mi-ra-a-je’ amylase}

Query

아밀라아제
아밀라아제amylase
amylase
(
amylase)
)
아밀라아제
amylase
아밀라아제 (amylase)
아밀라아제
[
amylase]
amylase]
아밀라아제 [amylase]
‘a-mil-la-a-je’
‘a-mil-la-a-je’amylase
amylase
amylase)
‘a-mil-la-a-je’
amylase)
‘a-mil-la-a-je’((amylase)
amylase]
‘a-mil-la-a-je’
amylase]
‘a-mil-la-a-je’[[amylase]

アミラーゼ
アミラーゼamylase
amylase
(
amylase)
)
アミラーゼ
amylase
アミラーゼ (amylase)
アミラーゼ
[
amylase]
amylase]
アミラーゼ [amylase]
‘a-mi-ra-a-je’
‘a-mi-ra-a-je’amylase
amylase
amylase)
‘a-mi-ra-a-je’
amylase)
‘a-mi-ra-a-je’((amylase)
amylase]
‘a-mi-ra-a-je’
amylase]
‘a-mi-ra-a-je’[[amylase]

Figure 5: Desirable retrieved web pages for transliteration ranking.

‘a-mil-la-a-je’ and ‘a-mi-ra-a-je’. The web documents retrieved by a BPS usually contain a
transliteration and its corresponding source language word as a translation pair, with one
of them often placed in parentheses, as shown in Figure 5.
A dilemma arises, though, regarding the quality and coverage of retrieved web documents. Though a BPS generally provides high-quality web documents that contain correct
transliterations corresponding to the source language word, the coverage is relatively low,
meaning that it may not find any web documents for some transliterations. For example, a BPS for the Japanese phrasal query {‘a-ru-ka-ro-si-su’ alkalosis} and the Korean
phrasal query {‘eo-min’ ermine} found no web documents. Therefore, alternative search
methods are necessary when the BPS fails to find any relevant web documents. A bilingual
keyword search (BKS) (Qu & Grefenstette, 2004; Huang, Zhang, & Vogel, 2005; Zhang,
Huang, & Vogel, 2005) can be used when the BPS fails, and a monolingual keyword search
(MKS) (Grefenstette, Qu, & Evans, 2004) can be used when both the BPS and BKS fail.
Like a BPS, a BKS makes use of two keywords, a transliteration and its source language
word, as a search engine query. Whereas a BPS retrieves web documents containing the
two keywords as a phrase, a BKS retrieves web documents containing them anywhere in
the document. This means that the web frequencies of noisy transliterations are sometimes
higher than those of correct transliterations in a BKS, especially when the noisy transliterations are one-syllable transliterations. For example, ‘mok’, which is a Korean transliteration
produced for mook and a one-syllable noisy transliteration, has a higher web frequency than
‘mu-keu’, which is the correct transliteration for mook, because ‘mok’ is a common Korean
143

Oh, Choi, & Isahara

noun that frequently appears in Korean texts with the meaning of neck. However, a BKS
can improve coverage without a great loss of quality in the retrieved web documents if the
transliterations are composed of two or more syllables.
Though a BKS has higher coverage than a BPS, it can fail to retrieve web documents
in some cases. In such cases, an MKS (Grefenstette et al., 2004) is used. In an MKS,
a transliteration alone is used as the search engine query. A BPS and a BKS act like a
translation model, while an MKS acts like a language model. Though an MKS cannot give
information as to whether the transliteration is correct, it does provide information as to
whether the transliteration is likely to be a target language word. The three search methods
are used sequentially (BPS, BKS, MKS). If one method fails to retrieve any relevant web
documents, the next one is used. Table 15 summarizes the conditions for applying each
search method.
Along with these three search strategies, three different search engines are used to obtain
more web documents. The search engines used for this purpose should satisfy two conditions: 1) support Korean/Japanese web document retrieval and 2) support both phrasal
and keyword searches. Google13 , Yahoo14 , and MSN15 satisfy these conditions, and we used
them as our search engines.
Search method
BPS
BKS
MKS

Condition

P P
W FBP Sj (e, ck )) 6= 0
Pj Pck ∈C
W FBP Sj (e, ck )) = 0
P j P ck ∈C
W
FBKSj (e, ck )) 6= 0
Pj Pck ∈C
W FBP Sj (e, ck ) = 0
P j P ck ∈C
W
FBKSj (e, ck ) = 0
P j P ck ∈C
j

ck ∈C

W FM KSj (e, ck ) 6= 0

Table 15: Conditions under which each search method is applied.

RF (e, ci ) =

X

N W Fj (e, ci ) =

j

X
j

P

W Fj (e, ci )
ck ∈C W Fj (e, ck )

(7)

Web frequencies acquired from these three search methods and these three search engines were used to rank transliterations on the basis of Formula (7), where ci is the ith
transliteration produced by the four transliteration models, e is the source language word
of ci , RF is a function for ranking transliterations, W F is a function for calculating web
frequency, N W F is a function for normalizing web frequency, C is a set of produced transliterations, and j is an index for the j th search engine. We used the normalized web frequency
as a ranking factor. The normalized web frequency is the web frequency divided by the
total web frequency of all produced transliterations corresponding to one source language
word. The score for a transliteration is then calculated by summing up the normalized
13. http://www.google.com
14. http://www.yahoo.com
15. http://www.msn.com

144

A Comparison of Machine Transliteration Models

web frequencies of the transliteration given by the three search engines. Table 16 shows an
example ranking for the English word data and its possible Korean transliterations, ‘de-iteo’, ‘de-i-ta’, and ‘de-ta’, which web frequencies are obtained using a BPS. The normalized
W FBP S (N W FBP S ) for search engine A was calculated as follows.
• N W FBP S (data, ‘de-i-teo’) = 94,100 / (94,100 + 67,800 + 54) = 0.5811
• N W FBP S (data, ‘de-i-ta’) = 67,800 / (94,100 + 67,800 + 54) = 0.4186
• N W FBP S (data, ‘de-ta’) = 54 / (94,100 + 67,800 + 54) = 0.0003
The ranking score for ‘de-i-teo’ was then calculated by summing up N W FBP S (data, ‘de-iteo’) for each search engine:
• RFBP S (data, ‘de-i-teo’) = 0.5810 + 0.7957 + 0.3080 = 1.6848

Search Engine
A
B
C
RF

c1 = ‘de-i-teo’
WF
NWF
94,100 0.5811
101,834 0.7957
1,358
0.3080
1.6848

e=data
c2 = ‘de-i-ta’
WF
NWF
67,800 0.4186
26,132 0.2042
3,028 0.6868
1.3096

c3 = ‘de-ta’
WF NWF
54
0.0003
11
0.0001
23
0.0052
0.0056

Table 16: Example transliteration ranking for data and its transliterations; W F , N W F ,
and RF represent W FBP S , N W FBP S , and RFBP S , respectively.

6.1 Evaluation
We tested the performance of the transliteration ranking under two conditions: 1) with all
test data (ALL) and 2) with test data for which at least one transliteration model produced
the correct transliteration (CTC). Testing with ALL showed the overall performance of the
machine transliteration while testing with CTC showed the performance of the transliteration ranking alone. We used the performance of the individual transliteration models
(ψG , ψP , ψH , and ψC ) as the baseline. The results are shown in Table 17. “Top-n” means
that the correct transliteration was within the Top-n ranked transliterations. The average
number of produced Korean transliterations was 3.87 and that of Japanese ones was 4.50;
note that ψP and ψC produced more than one transliteration because of pronunciation
variations. The results for both English-to-Korean and English-to-Japanese transliteration
indicate that our ranking method effectively filters out noisy transliterations and positions
the correct transliterations in the top rank; most of the correct transliterations were in
Top-1. We see that transliteration ranking (in Top-1) significantly improved performance
of the individual models for both EKSet and EJSet16 . The overall performance of the
16. A one-tail paired t-test showed that the performance improvement was significant (level of significance
= 0.001.

145

Oh, Choi, & Isahara

transliteration (for ALL) as well that of the ranking (for CTC) were relatively good. Notably, the CTC performance showed that web data is a useful language resource for ranking
transliterations.
Test data
ALL

ALL

CTC

ψG
ψP
ψH
ψC
Top-1
Top-2
Top-3
Top-1
Top-2
Top-3

EKSet
58.8%
55.2%
64.1%
65.5%
71.5%
75.3%
75.8%
94.3%
99.2%
100%

EJSet
58.8%
59.2%
65.8%
69.1%
74.8%
76.9%
77.0%
97.2%
99.9%
100%

Table 17: Results of Transliteration ranking.

6.2 Analysis of Results
We defined two error types: production errors and ranking errors. A production error
is when there is no correct transliteration among the produced transliterations. A ranking
error is when the correct transliteration does not appear in the Top-1 ranked transliterations.
We examined the relationship between the search method and the transliteration ranking. Table 18 shows the ranking performance by each search method. The RTC represents
correct transliterations ranked by each search method. The NTC represents test data
ranked, that is, the coverage of each search method. The ratio of RTC to NTC represents
the upper bound of performance and the difference between RTC and NTC is the number
of errors.
The best performance was with a BPS. A BPS handled 5,270 out of 7,172 cases for
EKSet and 8,829 out of 10,417 cases for EJSet. That is, it did the best job of retrieving
web documents containing transliteration pairs. Analysis of the ranking errors revealed
that the main cause of such errors in a BPS was transliteration variations. These variations
contribute to ranking errors in two ways. First, when the web frequencies of transliteration
variations are higher than those of the standard ones, the variations are ranked higher than
the standard ones, as shown by the examples in Table 19. Second, when the transliterations
include only transliteration variations (i.e., there are no correct transliterations), the correct
ranking cannot be. In this case, ranking errors are caused by production errors. With a
BPS, there were 603 cases of this for EKSet and 895 cases for EJSet.
NTC was smaller with a BKS and an MKS because a BPS retrieves web documents
whenever possible. Table 18 shows that production errors are the main reason a BPS fails
to retrieve web documents. (When a BKS or MKS was used, production errors occurred in
146

A Comparison of Machine Transliteration Models

Top-1
Top-2
Top-3
RTC
NTC

BPS
83.8%
86.6%
86.6%
4,568
5,270

EKSet
BKS
MKS
55.1% 16.7%
58.4% 27.0%
58.2% 31.3%
596
275
1,024
878

BPS
86.2%
88.3%
88.35%
7,800
8,829

EJSet
BKS
19.0%
22.8%
22.9%
188
820

MKS
2.7%
4.2%
4.3%
33
768

Table 18: Ranking performance of each search method.

compact → Korean
pathos → Korean
cohen → Japanese
criteria → Japanese

Transliteration
‘kom-paek-teu’
‘keom-paek-teu’∗
‘pa-to-seu’
‘pae-to-seu’∗
‘ko-o-he-n’
‘ko-o-e-n’∗
‘ku-ra-i-te-ri-a’
‘ku-ri-te-ri-a’∗

Web Frequency
1,075
1,793
1,615
14,062
23
112
104
1,050

Table 19: Example ranking errors when a BPS was used (∗ indicates a variation).

all but 87117 cases for EKSet and 22118 cases for EJSet). The results also show that a BKS
was more effective than an MKS.
The trade-off between the quality and coverage of retrieved web documents is an important factor in transliteration ranking. A BPS provides better quality rather than wider
coverage, but is effective since it provides reasonable coverage.

7. Conclusion
We tested and compared four transliteration models, grapheme-based transliteration
model (ψG ), phoneme-based transliteration model (ψP ), hybrid transliteration
model (ψH ), and correspondence-based transliteration model (ψC ), for English-toKorean and English-to-Japanese transliteration. We modeled a framework for the four
transliteration models and compared them within the framework. Using the results, we
examined a way to improve the performance of machine transliteration.
We found that the ψH and ψC are more effective than the ψG and ψP . The main reason
for the better performance of ψC is that it uses the correspondence between the source
grapheme and the source phoneme. The use of this correspondence positively affected
transliteration performance in various tests.
17. 596 (RTC of BKS in EKSet) + 275 (RTC of MKS in EKSet) = 871
18. 188 (RTC of BKS for EJSet) + 33 (RTC of MKS for EJSet) = 221

147

Oh, Choi, & Isahara

We demonstrated that ψG , ψP , ψH , and ψC can be used as complementary transliteration models to improve the chances of producing correct transliterations. A combination of
the four models produced more correct transliterations both in English-to-Korean transliteration and English-to-Japanese transliteration compared to each model alone. Given these
results, we described a way to improve machine transliteration that combines different
transliteration models: 1) produce a list of transliterations by combining transliterations produced by multiple transliteration models; 2) rank the transliterations
on the basis of their relevance.
Testing showed that transliteration ranking based on web frequency is an effective way
to calculate the relevance of transliterations. This is because web data reflects real-world
usage, so it can be used to filter out noisy transliterations, which are not used as target
language words or are incorrect transliterations for a source language word.
There are several directions for future work. Although we considered some transliteration variations, our test sets mainly covered standard transliterations. In corpora or
web pages, however, we routinely find other types of transliterations – misspelled transliterations, transliterations of common phrases, etc. – along with the standard transliterations and transliteration variations. Therefore, further testing using such transliterations
is needed to enable the transliteration models to be compared more precisely. To achieve
a machine transliteration system capable of higher performance, we need a more sophisticated transliteration method and a more sophisticated ranking algorithm. Though many
correct transliterations can be acquired through the combination of the four transliteration
models, there are still some transliterations that none of the models can produce. We need
to devise a method that can produce them. Our transliteration ranking method works well,
but, because it depends on web data, it faces limitations if the correct transliteration does
not appear in web data. We need a complementary ranking method to handle such cases.
Moreover, to demonstrate the effectiveness of these four transliteration models, we need to
apply them to various natural language processing applications.

Acknowledgments
We are grateful to Claire Cardie and the anonymous reviewers for providing constructive
and insightful comments to earlier drafts of this paper.

References
Aha, D. W. (1997). Lazy learning: Special issue editorial. Artificial Intelligence Review,
11:710.
Aha, D. W., Kibler, D., & Albert, M. (1991). Instance-based learning algorithms. Machine
Learning, 6 (3766).
Al-Onaizan, Y., & Knight, K. (2002). Translating named entities using monolingual and
bilingual resources. In Proceedings of ACL 2002, pp. 400–408.
Andersen, O., Kuhn, R., Lazarides, A., Dalsgaard, P., Haas, J., & Noth, E. (1996). Comparison of two tree-structured approaches for grapheme-to-phoneme conversion. In
Proceedings of ICSLP 1996, pp. 1808–1811.
148

A Comparison of Machine Transliteration Models

Berger, A. L., Pietra, S. D., & Pietra, V. J. D. (1996). A maximum entropy approach to
natural language processing. Computational Linguistics, 22 (1), 39–71.
Bilac, S., & Tanaka, H. (2004). Improving back-transliteration by combining information
sources. In Proceedings of IJCNLP2004, pp. 542–547.
Breen, J. (2003). EDICT Japanese/English dictionary .le. The Electronic Dictionary Research and Development Group, Monash University. http://www.csse.monash.edu.
au/~jwb/edict.html.
Chen, S. F. (2003). Conditional and joint models for grapheme-to-phoneme conversion. In
Proceedings of Eurospeech, pp. 2033–2036.
CMU (1997). The CMU pronouncing dictionary version 0.6. http://www.speech.cs.cmu.
edu/cgi-bin/cmudict.
Cover, T. M., & Hart, P. E. (1967). Nearest neighbor pattern classification. Institute of
Electrical and Electronics Engineers Transactions on Information Theory, 13 (2127).
Daelemans, W., Zavrel, J., Sloot, K. V. D., & Bosch, A. V. D. (2004). TiMBL: Tilburg
Memory-Based Learner - version 5.1 reference guide. Tech. rep. 04-02, ILK Technical
Report Series.
Daelemans, W., & van den Bosch, A. (1996). Language-independent data-oriented
grapheme-to-phoneme conversion. In J. Van Santen, R. Sproat, J. O., & Hirschberg,
J. (Eds.), Progress in Speech Synthesis, pp. 77–90. Springer Verlag, New York.
Damper, R. I., Marchand, Y., Adamson, M. J., & Gustafson, K. (1999). Evaluating the
pronunciation component of text-to-speech systems for English: A performance comparison of different approaches. Computer Speech and Language, 13 (2), 155–176.
Devijver, P. A., & Kittler., J. (1982). Pattern recognition: A statistical approach. PrenticeHall.
Fujii, A., & Tetsuya, I. (2001). Japanese/English cross-language information retrieval: Exploration of query translation and transliteration. Computers and the Humanities,
35 (4), 389–420.
Goto, I., Kato, N., Uratani, N., & Ehara, T. (2003). Transliteration considering context
information based on the maximum entropy method. In Proceedings of MT-Summit
IX, pp. 125–132.
Grefenstette, G., Qu, Y., & Evans, D. A. (2004). Mining the web to create a language
model for mapping between English names and phrases and Japanese. In Proceedings
of Web Intelligence, pp. 110–116.
Huang, F., Zhang, Y., & Vogel, S. (2005). Mining key phrase translations from web corpora. In Proceedings of Human Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing, pp. 483–490.
Jeong, K. S., Myaeng, S. H., Lee, J. S., & Choi, K. S. (1999). Automatic identification and
back-transliteration of foreign words for information retrieval. Information Processing
and Management, 35 (1), 523–540.
149

Oh, Choi, & Isahara

Jung, S. Y., Hong, S., & Paek, E. (2000). An English to Korean transliteration model of
extended markov window. In Proceedings of the 18th conference on Computational
linguistics, pp. 383 – 389.
Kang, B. J. (2001). A resolution of word mismatch problem caused by foreign word transliterations and English words in Korean information retrieval. Ph.D. thesis, Computer
Science Dept., KAIST.
Kang, B. J., & Choi, K. S. (2000). Automatic transliteration and back-transliteration by
decision tree learning. In Proceedings of the 2nd International Conference on Language
Resources and Evaluation, pp. 1135–1411.
Kang, I. H., & Kim, G. C. (2000). English-to-Korean transliteration using multiple unbounded overlapping phoneme chunks. In Proceedings of the 18th International Conference on Computational Linguistics, pp. 418–424.
Kim, J. J., Lee, J. S., & Choi, K. S. (1999). Pronunciation unit based automatic EnglishKorean transliteration model using neural network. In Proceedings of Korea Cognitive
Science Association, pp. 247–252.
Knight, K., & Graehl, J. (1997). Machine transliteration. In Proceedings of the 35th Annual
Meetings of the Association for Computational Linguistics, pp. 128–135.
Korea Ministry of Culture & Tourism (1995). English to Korean standard conversion rule.
http://www.hangeul.or.kr/nmf/23f.pdf.
Lee, J. S. (1999). An English-Korean transliteration and retransliteration model for Crosslingual information retrieval. Ph.D. thesis, Computer Science Dept., KAIST.
Lee, J. S., & Choi, K. S. (1998). English to Korean statistical transliteration for information
retrieval. Computer Processing of Oriental Languages, 12 (1), 17–37.
Li, H., Zhang, M., & Su, J. (2004). A joint source-channel model for machine transliteration.
In Proceedings of ACL 2004, pp. 160–167.
Lin, W. H., & Chen, H. H. (2002). Backward machine transliteration by learning phonetic similarity. In Proceedings of the Sixth Conference on Natural Language Learning
(CoNLL), pp. 139–145.
Manning, C., & Schutze, H. (1999). Foundations of Statistical natural language Processing.
MIT Press.
Meng, H., Lo, W.-K., Chen, B., & Tang, K. (2001). Generating phonetic cognates to
handle named entities in English-Chinese cross-language spoken document retrieval.
In Proceedings of Automatic Speech Recognition and Understanding, 2001. ASRU ’01,
pp. 311–314.
Mitchell, T. M. (1997). Machine learning. New-York: McGraw-Hill.
Nam, Y. S. (1997). Foreign dictionary. Sung An Dang.
Oh, J. H., & Choi, K. S. (2002). An English-Korean transliteration model using pronunciation and contextual rules. In Proceedings of COLING2002, pp. 758–764.
Pagel, V., Lenzo, K., & Black, A. W. (1998). Letter to sound rules for accented lexicon compression. In Proceedings of International Conference on Spoken Language Processing,
pp. 2015–2018.
150

A Comparison of Machine Transliteration Models

Qu, Y., & Grefenstette, G. (2004). Finding ideographic representations of Japanese names
written in Latin script via language identification and corpus validation.. In Proc. of
ACL, pp. 183–190.
Quinlan, J. R. (1986). Induction of decision trees. Machine Learning, 1, 81–106.
Quinlan, J. R. (1993). C4.5: Programs for Machine Learning. Morgan Kauffman.
Stalls, B. G., & Knight, K. (1998). Translating names and technical terms in arabic text.
In Proceedings of COLING/ACL Workshop on Computational Approaches to Semitic
Languages, pp. 34–41.
Zhang, L. (2004). Maximum entropy modeling toolkit for python and C++. http://
homepages.inf.ed.ac.uk/s0450736/software/maxent/manual.pdf.
Zhang, Y., Huang, F., & Vogel, S. (2005). Mining translations of OOV terms from the web
through cross-lingual query expansion. In Proceedings of the 28th annual international
ACM SIGIR conference on Research and development in information retrieval, pp.
669–670.

151

Journal of Artificial Intelligence Research 27 (2006) 381-417

Submitted 3/06; published 11/06

Multi-Issue Negotiation with Deadlines
Shaheen S. Fatima
Michael Wooldridge

S.S.FATIMA @ CSC . LIV. AC . UK
M.J.W OOLDRIDGE @ CSC . LIV. AC . UK

Department of Computer Science,
University of Liverpool, Liverpool L69 3BX, U.K.

Nicholas R. Jennings

NRJ @ ECS . SOTON . AC . UK

School of Electronics and Computer Science,
University of Southampton, Southampton SO17 1BJ, U.K.

Abstract
This paper studies bilateral multi-issue negotiation between self-interested autonomous agents.
Now, there are a number of different procedures that can be used for this process; the three main
ones being the package deal procedure in which all the issues are bundled and discussed together,
the simultaneous procedure in which the issues are discussed simultaneously but independently of
each other, and the sequential procedure in which the issues are discussed one after another. Since
each of them yields a different outcome, a key problem is to decide which one to use in which
circumstances. Specifically, we consider this question for a model in which the agents have time
constraints (in the form of both deadlines and discount factors) and information uncertainty (in that
the agents do not know the opponent’s utility function). For this model, we consider issues that are
both independent and those that are interdependent and determine equilibria for each case for each
procedure. In so doing, we show that the package deal is in fact the optimal procedure for each
party. We then go on to show that, although the package deal may be computationally more complex than the other two procedures, it generates Pareto optimal outcomes (unlike the other two), it
has similar earliest and latest possible times of agreement to the simultaneous procedure (which is
better than the sequential procedure), and that it (like the other two procedures) generates a unique
outcome only under certain conditions (which we define).

1. Introduction
Negotiation is a key form of interaction in multiagent systems (Maes, Guttman, & Moukas, 1999;
Sandholm, 2000). It is a process in which disputing agents decide how to divide the gains from
cooperation. Since this decision is made jointly by the agents themselves (Rosenschein & Zlotkin,
1994; Raiffa, 1982; Pruitt, 1981; Fisher & Ury, 1981; Young, 1975; Kraus, 2001), each agent
can only obtain what the other is prepared to allow them. Now, the simplest form of negotiation
involves two agents and a single-issue. For example, consider a scenario in which a buyer and a
seller negotiate on the price of a good. To begin, the two agents are likely to differ on the price at
which they believe the trade should take place, but through a process of joint decision-making they
either arrive at a price that is mutually acceptable or they fail to reach an agreement. Since agents
are likely to begin with different prices, one or both of them must move toward the other, through
a series of offers and counter offers, in order to obtain a mutually acceptable outcome. However,
before the agents can actually perform such negotiations, they must decide the rules for making
offers and counter offers. That is, they must set the negotiation protocol (Lax & Sebenius, 1986;
Osborne & Rubinstein, 1990; Rosenschein & Zlotkin, 1994; Kraus, Wilkenfeld, & Zlotkin, 1995;
Lomuscio, Wooldridge, & Jennings, 2003).
c
2006
AI Access Foundation. All rights reserved.

FATIMA , W OOLDRIDGE , & J ENNINGS

On the basis of this protocol, each agent chooses its strategy (i.e., what offers it should make
during the course of negotiation). For competitive scenarios with self-interested agents, each participant defines its strategy so as to maximise its individual utility. Furthermore, for such scenarios, an
agent’s optimal strategy depends very strongly on the information it has about its opponent (Fatima,
Wooldridge, & Jennings, 2002, 2004). For example, the strategy that a buyer would use if it knew
the seller’s reserve price differs from the one it would use if it did not. From all of this, it can be
seen that the outcome of single-issue negotiation depends on four key factors (Harsanyi, 1977): the
negotiation protocol, the players’ strategies, the players’ preferences over the possible outcomes,
and the information that the players have about each other. However, in most bilateral negotiations,
the parties involved need to settle more than one issue. For example, agents may need to come to
agreements about objects/services that are characterised by attributes such as price, delivery time,
quality, reliability, and so on. For such multi-issue negotiations, the outcome also depends on one
additional factor: the negotiation procedure (Schelling, 1956, 1960; Fershtman, 1990), which specifies how the issues will be settled. Broadly speaking, there are three ways of negotiating multiple
issues (Keeney & Raiffa, 1976; Raiffa, 1982):
• Package deal: This approach links all the issues and discusses them together as bundle.
• Simultaneous negotiation: This involves settling the issues simultaneously, but independently,
of each other.
• Sequential negotiation: This involves negotiating the issues sequentially, one after another.
Now, these three different procedures have different properties and yield different outcomes to the
negotiators (Fershtman, 2000). So the key question to answer is: which of them is best? Here,
since we are concerned with self-interested agents, our notion of the optimal procedure is the one
that maximises an agent’s individual return. However, such optimality is only part of the story;
given our motivations we are also concerned with the Pareto optimality of the solutions for these
procedures (because Pareto optimality ensures that utility does not go wasted), the computational
complexity of the procedures (because for scenarios with information uncertainty, the agents need
to compute their equilibrium offers during the process of negotiation, as opposed to the complete information scenario where the strategies can be precompiled), the actual time of agreement (because
for scenarios with information uncertainty, this time depends on an agent’s beliefs about its opponent and an agreement may not occur in the first time period), and the uniqueness of the solutions
they generate (because this allows the agents to know their actual shares).
One immediate observation in this vein is that the package deal gives rise to the possibility of
making tradeoffs across issues. Such tradeoffs are possible when different agents value different
issues differently. For example, if there are two issues and one agent values the first more than the
second, while the other agent values the second issue more than the first, then it is possible to make
tradeoffs and thereby improve the utility of both agents relative to the situation without tradeoffs. In
contrast, for the simultaneous and sequential approaches, the issues are settled independently and so
there is no scope for such tradeoffs between them. Moreover, we seek to answer the above question
about optimality for the types of situation that are commonly faced by agents in real-world contexts.
Thus, we consider negotiations in which there are:
1. Time constraints. Agents have time constraints in the form of both deadlines and discount
factors. Here we view deadlines as an essential element since negotiation cannot go on indefinitely, rather it must end within a reasonable time limit (Livne, 1979). Likewise, discount
382

M ULTI -I SSUE N EGOTIATION

WITH

D EADLINES

factors are essential since the desirability of the good being traded often declines with time.
This happens either because the good is perishable or due to inflation. Moreover, the strategic
behaviour of agents with deadlines and discount factors differs from those without (see Rubinstein, 1982, for single issue bargaining without deadlines and Sandholm & Vulkan, 1999;
Ma & Manove, 1993; Fershtman & Seidmann, 1993; Kraus, 2001, for bargaining with deadlines and discount factors). For instance, the presence of a deadline induces each negotiator
to play a strategy that ensures the best possible agreement before the deadline is reached.
Likewise, the presence of a discount factor means that reaching an agreement today is not the
same as reaching it tomorrow. Hence, the agents try to reach an agreement sooner rather than
later.
2. Uncertainty about the opponent’s negotiation parameters. The information that agents have
about their negotiation opponent is likely to be uncertain (see Fudenberg & Tirole, 1983;
Fudenberg, Levine, & Tirole, 1985; Rubinstein, 1985, for single issue bargaining with uncertainty). Moreover, in some bargaining situations, one of the players may know something of
relevance that the other does not. For example, when bargaining over the price of a second
hand car, the seller knows its quality, but the buyer does not. Such situations are said to have
asymmetry in information between the players (Muthoo, 1999). On the other hand, in symmetric information situations both players have the same information. Again, agents have to
operate in both situations and so we analyse both cases.
3. Interdependence between issues. The issues under negotiation may be independent or interdependent. In the former case, an agent’s utility from an issue depends only on the agreement
that is reached on it, not on how the other issues are settled. In the latter case, an agent’s
utility from an issue depends not only on the agreement that is reached on it but also on how
the other issues are settled (Bar-Yam, 1997; Klein, Faratin, Sayama, & Bar-Yam, 2003). Both
situations are common in multiagent systems and so again we analyse both cases.
Thus we study five different settings: i) complete information setting (CI), ii) a setting with
independent issues and symmetric uncertainty about the agents’ utilities (SUI ), iii) a setting with
independent issues and asymmetric uncertainty about the agents’ utilities (AUI ), iv) a setting with
interdependent issues and symmetric uncertainty about the agents’ utilities (SUD ), and v) a setting
with interdependent issues and asymmetric uncertainty about the agents’ utilities (AUD ).
Our methodology is to first derive equilibria for each of the procedures in each of the above
settings, From this, we can determine which of them is optimal. As we will see, this analysis shows
that, for all the settings, the package deal is the best. We then go on to analyse the procedures in
terms of other performance metrics. Specifically, we show that, in all the settings, only the package
deal generates a Pareto optimal outcome. We also show that although the package deal may be computationally more complex than the other two procedures, it has similar earliest and latest possible
times of agreement to the simultaneous procedure (which is better than the sequential procedure),
and it (like the other two procedures) generates a unique outcome only in certain situations (which
we define). The key results of our study are summarised in Table 1.
There has previously been some formal comparison of different procedures to find the optimal
one (see Section 7 for details). However, all this work has at least one of the following major
limitations. First, it has focused on comparing procedures for negotiation without deadlines. Note
that existing work has obtained equilibrium for negotiation with deadlines, but only for the single
383

FATIMA , W OOLDRIDGE , & J ENNINGS

Information
setting
CI
Time of
agreement
tc

Time to
compute
equilibrium
Pareto
optimal?
Unique
equilibrium?

SUI , SUD
AUI , and AUD
CI
SUI and SUD
AUI and AUD
CI,
SUI ,SUD ,
AUI , and AUD
CI
SUI ,SUD ,
AUI , and AUD

Package deal

Simultaneous

Sequential

For the cth issue
tc = 1
for 1 ≤ c ≤ m
For the cth issue
tec = 1
tlc = min(2r − 1, n)
for 1 ≤ c ≤ m

For the cth issue
tc = 1
for 1 ≤ c ≤ m
For the cth issue
tec = 1
tlc = min(2r − 1, n)
for 1 ≤ c ≤ m

For the cth partition
tc = c
for 1 ≤ c ≤ µ
For the cth partition
tec = tsc
tlc = tsc + min(2r − 1, n)
for 1 ≤ c ≤ µ

O(mn)
O(mπ̂r 3 T (n − T2 ))

O(M n)
O(|Sz |π̂z r 3 T (n − T2 ))

O(mπ̂r3 (n −

T T
2)2)

O(|Sz |π̂z r3 (n −

T T
2)2

)

O(M n)
O(|Sz |π̂z r 3 T (n −
O(|Sz |π̂z r3 (n −

Yes

No

No

If ¬C1
If ¬C3 ∨ C4

If C2
If C5

If C2
If C5

Table 1: A summary of key results. tsc denotes the start time for the cth partition, tec the earliest
possible time of agreement, and tlc the latest possible time of agreement).

issue case (Sandholm & Vulkan, 1999; Stahl, 1972), and a special type of the sequential procedure
for multiple issues (Fatima et al., 2004). See Section 7 for details. Second, it has focussed only on
independent issues and asymmetric information settings. Third, it has only focused on finding the
optimal procedure, but has not considered the additional solution properties of different procedures.
Given this, our paper makes a threefold contribution. First, we obtain the equilibrium for each
procedure when there are deadlines. Second, we analyse multiple issues that are both independent
and interdependent. Moreover, we analyse both symmetric and asymmetric information settings.
Finally, on the basis of the equilibrium for different procedures, we provide the first comprehensive
comparison of their solution properties (viz. time complexity, Pareto optimality, uniqueness, and
time of agreement). When taken together, the results clearly indicate the choices and tradeoffs
involved in choosing a negotiation procedure in a wide range of circumstances. This knowledge
can be used by a system designer who is responsible for designing the mechanism that should be
used to moderate the negotiation encounters and by the agents themselves if they can choose how to
arrange their interactions. Furthermore, this knowledge also tells the agents what their equilibrium
offers are during negotiation.
The remainder of the paper is organised as follows. We begin by giving a brief overview of
single-issue negotiation in Section 2. In Section 3, we study the three multi-issue procedures for the
setting with complete information and where the issues are independent. This study is undertaken
to provide a foundation for Sections 4, 5, and 6, which treat the information about the agents’
utilities as uncertain. More specifically, in Section 4, we analyse a scenario with symmetric uncer384

T
2

T
2 ))

) T2 )

M ULTI -I SSUE N EGOTIATION

WITH

D EADLINES

tainty about the opponent’s utility. In Section 5, we analyse a scenario with asymmetric uncertainty
about the opponent’s utility. Sections 4 and 5 both deal with independent issues. In Section 6, we
extend the analysis to interdependent issues. Section 7 discusses the related literature and Section 8
concludes. Appendix A provides a summary of notation employed throughout the paper.

2. Single-Issue Negotiation
Assume there are two agents: a and b. Each agent has time constraints in the form of deadlines and
discount factors. Since we focus on competitive scenarios with self-interested agents, we model
negotiation using the ‘split the pie game’ analysed by Osborne and Rubinstein (1994), Binmore,
Osborne, and Rubinstein (1992). We begin by introducing this complete information game.
Let the two agents be negotiating over a single issue (i). This issue is a ‘pie’ of size 1 and the
agents want to determine how to divide it between themselves. There is a deadline (i.e., a number
of rounds by which negotiation must end). Let n ∈ N+ denote this deadline. The agents use
Rubinstein’s alternating offers protocol (Osborne & Rubinstein, 1994), which proceeds through a
series of time periods. One of the agents, say a, starts negotiation in the first time period (i.e., t = 1)
by making an offer (xi ), that lies in the interval [0, 1], to b. Agent b can either accept or reject
the offer. If it accepts, negotiation ends in an agreement with a getting a share of xi and b getting
yi = 1 − xi . Otherwise, negotiation proceeds to the next time period, in which agent b makes a
counter-offer. This process of making offers continues until one of the agents either accepts an offer
or quits negotiation (resulting in a conflict). Thus, there are three possible actions an agent can take
during any time period: accept the last offer, make a new counter-offer, or quit the negotiation.
An essential feature of negotiations involving alternating offers is that the pie is assumed to
shrink with time (Rubinstein, 1982). Specifically, it shrinks at each step of offer and counteroffer.
This shrinkage models a decrease in the value of the pie (representing the fact that the pie perishes
with time or there is inflation). This shrinkage is represented with a discount factor denoted 0 <
δi ≤ 1 for both1 agents. At t = 1, the size of the pie is 1, but in all subsequent time periods t > 1,
the pie shrinks to δit−1 .
We denote the set of real numbers by R and the set of real numbers in the interval [0, 1] by R1 .
Then let [xti , yit ] denote the offer made at time period t where xti and yit denote the share for agent a
and b respectively. Then, for a given pie, the set of possible offers is:
{[xti , yit ] : xti ≥ 0, yit ≥ 0, and xti + yit = δit−1 }
where xti ∈ R1 and yit ∈ R1 . Each player’s utility function is defined over the set R. Let uai :
R1 × N+ → R and ubi : R1 × N+ → R denote the utility functions of the two agents. At time t, if
a and b receive a share of xti and yit respectively (where xti + yit = δit−1 ), then their utilities are:
 t
xi if t ≤ n
a t
ui (xi , t) =
0 otherwise
ubi (yit , t)

=



yit if t ≤ n
0 otherwise

1. Having a different discount factor for different agents only makes the presentation more involved without leading to
any changes in the analysis of the strategic behaviour of the agents or the time complexity of finding the equilibrium
offers. Hence we have a single discount factor for both agents.

385

FATIMA , W OOLDRIDGE , & J ENNINGS

The conflict utility (i.e., the utility received in the event that no deal is struck) is zero for both
agents. Note that δ is not shown explicitly in an agent’s utility function but is implicit. This is
because, during any time period t, xti and yit denote a’s and b’s actual shares respectively (not the
ratios of their shares) where xti + yit = δit−1 . In other words δ is included in an agent’s share. This
will become clearer when we show the agents’ shares in Expression 1.
For the above setting, the agents reason as follows in order to determine what to offer. Let agent
a denote the first mover (i.e., at t = 1, a proposes to b how to split the pie). To begin, consider the
case where the deadline for both agents is n = 1. If b accepts, the division occurs as agreed; if not,
neither agent gets anything (since n = 1 is the deadline). Here, a is in a powerful position and is
able to propose to keep 100 percent of the pie and give nothing to b 2 . Since the deadline is n = 1,
b accepts this offer and agreement takes place in the first time period.
Now, consider the case where the deadline is n = 2. In the first round, the size of the pie is 1
but it shrinks to δi in the second round. In order to decide what to offer in the first round, a looks
ahead to t = 2 and reasons backwards3 . Agent a reasons that if negotiation proceeds to the second
round, b will take 100 percent of the shrunken pie by offering [0, δi ] and leave nothing for a. Thus,
in the first time period, if a offers b anything less than δi , b will reject the offer. Hence, during the
first time period, agent a offers [1 − δi , δi ]. Agent b accepts this and an agreement occurs in the first
time period.
In general, if the deadline is n, negotiation proceeds as follows. As before, agent a decides what
to offer in the first round by looking ahead as far as t = n and then reasoning backwards. This
decision making leads a to make the following offer in the first time period:
n−1
n−1
[Σj=0
[(−1)j δij ], 1 − Σj=0
[(−1)j δij ]]

(1)

Agent b accepts this offer and negotiation ends in the first time period. Note that the equilibrium
outcome depends on who makes the first move. Since we have two agents and either of them could
move first, we get two possible equilibrium outcomes.
On the basis of the above equilibrium for single-issue negotiation with complete information, we
first obtain the equilibrium for multiple issues and then determine the optimal negotiation procedure
for the various settings that we have previously described.

3. Multi-Issue Negotiation with Complete Information
As mentioned in Section 1, the existing literature does not provide an analysis of all the multi-issue
procedures for negotiation with deadlines. Hence, we begin by analysing the complete information
setting. From this base, we can then extend to the case where there is information uncertainty.
Here a and b negotiate over m > 1 independent issues (Section 6 deals with interdependent
issues). These issues are m distinct pies and the agents want to determine how to split each of them.
Let S = {1, 2, . . . , m} denote the set of m pies. As before, each pie is of size 1. Let the discount
factor for issue c, where 1 ≤ c ≤ m, be 0 < δc ≤ 1. For each issue, let n denote each agent’s
2. It is possible that b may reject such a proposal. In practice, a will have to propose an offer that is just enough to
induce b to accept. However, to keep the exposition simple, we assume that a can get the whole pie by making the
100 percent proposal.
3. This backward reasoning method is adopted from (Stahl, 1972). Our model is a generalisation of (Stahl, 1972);
during time period t, an agent in our model can propose any offer between zero and δ t−1 (because the size of the pie
is δ t−1 ), but a player in (Stahl, 1972) is given a fixed number of alternatives to choose from.

386

M ULTI -I SSUE N EGOTIATION

WITH

D EADLINES

deadline. In the offer for time period t (where 1 ≤ t ≤ n), agent a’s (b’s) share for each of the m
t
m
issues is now represented as an m element vector xt ∈ Rm
1 (y ∈ R1 ). Thus, if agent a’s share for
issue c at time t is xtc , then agent b’s share is yct = (δct−1 − xtc ). The shares for a and b are together
represented as the package [xt , y t ].
We define an agent’s cumulative utility using the additive form. There are two reasons for this.
First, it is the most common form for cumulative utilities in traditional multi-issue utility theory
(Keeney & Raiffa, 1976). Second, additive cumulative utilities are linear and so the problem of
+
m
making tradeoffs becomes computationally tractable4 . The functions U a : Rm
1 × R1 × N → R
b
m
m
+
and U : R1 × R1 × N → R give the cumulative utilities for a and b respectively at time t. These
are defined as follows:
(
a a t
Σm
c=1 kc uc (xc , t) if t ≤ n
(2)
U a ([xt , y t ], t) =
0
otherwise
U b ([xt , y t ], t) =

(

b b t
Σm
c=1 kc uc (yc , t)
0

if t ≤ n
otherwise

(3)

b
m
where ka ∈ Rm
+ denotes an m element vector of constants for agent a and k ∈ R+ that for b. Here
R+ denotes the set of positive real numbers. These vectors indicate how the agents value different
a , then agent a values issue c more than issue c + 1. Likewise
issues. For example, if kca > kc+1
for agent b. In other words, the m issues are perfect substitutes (i.e., all that matters to an agent is
its total utility for all the m issues and not that for any subset of those Varian, 2003; Mas-Colell,
Whinston, & Green, 1995). In all the settings we study, the issues will be perfect substitutes.
Each agent has complete information about all negotiation parameters (i.e., n, m, kca , kcb , and δc
for 1 ≤ c ≤ m). For this complete information setting, we now determine the equilibrium for the
package deal, the simultaneous procedure, and the sequential procedure.

3.1 The Package Deal Procedure
In this procedure, the agents use the same protocol as for single-issue negotiation (described in Section 2). However, an offer for the package deal includes a proposal for each issue under negotiation.
Thus, for m issues, an offer includes m divisions, one for each issue. Agents are allowed to either
accept a complete offer (i.e., all m issues) or reject a complete offer. An agreement can therefore
take place either on all m issues or on none of them.
As per single-issue negotiation, an agent decides what to offer by looking ahead and reasoning
backwards. However, since an offer for the package deal includes a share for all the m issues,
agents can now make tradeoffs across the issues in order to maximise their cumulative utilities. For
1 ≤ c ≤ m, the equilibrium offer for issue c at time t is denoted as [atc , btc ] where atc and btc denote
the shares for agent a and b respectively. We denote the equilibrium package at time t as [at , bt ]
4. Using a form other than the additive one will make the function nonlinear. Consequently an agent’s tradeoff problem
becomes a global optimization problem with a nonlinear objective function. Due to their computational complexity,
such nonlinear optimization problems can only be solved using approximation methods (Horst & Tuy, 1996; BarYam, 1997; Klein et al., 2003). Moreover, these methods are not general in that they depend on how the cumulative
utilities are actually defined. In order to overcome this difficulty, we used the additive form for defining cumulative
utilities. Consequently, our tradeoff problem is a linear optimization problem, the exact solution to which can be
found in polynomial time (as shown in Theorems 1 and 2). Although our results apply to the above defined additive
cumulative utilities, in Section 6.4 we discuss how they would hold for nonlinear utilities.

387

FATIMA , W OOLDRIDGE , & J ENNINGS

t
m
where at ∈ Rm
1 (b ∈ R1 ) is an m element vector that denotes a’s (b’s) share for each of the m
issues. Also, for 1 ≤ t ≤ n, δt−1 ∈ Rm
1 is an m element vector that represents the sizes of the
m pies at time t. The symbol 0 denotes an m element vector of zeroes. Note that for 1 ≤ t ≤ n,
at + bt = δt−1 (i.e., the sum of the agents’ shares (at time t) for each pie is equal to the size of
the pie at t). Finally, for time period t (for 1 ≤ t ≤ n) we let a(t) (respectively b(t)) denote the
equilibrium strategy for agent a (respectively b).
As mentioned in Section 1, the package deal allows agents to make tradeoffs. We let TRADEOFFA
(TRADEOFFB ) denote agent a’s (b’s) function for making tradeoffs. Given this, the following theorem characterises the equilibrium for the package deal procedure.

Theorem 1 For the package deal procedure, the following strategies form a Nash equilibrium. The
equilibrium strategy for t = n is:

OFFER [δn−1 , 0] IF a’s TURN
a(n) =
ACCEPT
IF b’s TURN
b(n) =



OFFER [0, δn−1 ] IF b’s TURN
ACCEPT
IF a’s TURN

For all preceding time periods t < n, if [xt , y t ] denotes the offer made at time t, then the equilibrium
strategies are defined as follows:

OFFER tradeoffa(ka , kb , δ, ub(t), m, t)
IF a’s TURN
a(t) =
a
t
t
If (U ([x , y ], t) ≥ ua(t)) ACCEPT else REJECT IF b’s TURN
b(t) =



OFFER tradeoffb(ka , kb , δ, ua(t), m, t)
IF b’s TURN
If (U b ([xt , y t ], t) ≥ ub(t)) ACCEPT else REJECT IF a’s TURN

where ua(t) = U a ([at+1 , bt+1 ], t + 1) and ub(t) = U b ([at+1 , bt+1 ], t + 1). An agreement takes
place at t = 1.
Proof: We look ahead to the last time period (i.e., t = n) and then reason backwards. To begin,
if negotiation reaches the deadline (n), then the agent whose turn it is takes everything and leaves
nothing for its opponent. Hence, we get the strategies a(n) and b(n) as given in the statement of
the theorem.
In all the preceding time periods (t < n), the offering agent proposes a package that gives its
opponent a cumulative utility equal to what the opponent would get from its own equilibrium offer
for the next time period. During time period t, either a or b could be the offering agent. Consider
the case where a makes an offer at t. The package that a offers at t gives b a cumulative utility of
U b ([at+1 , bt+1 ], t + 1). However, since there is more than one issue, there is more than one package
that gives b a cumulative utility of U b ([at+1 , bt+1 ], t + 1). From among these packages, a offers the
one that maximises its own cumulative utility (because it is a utility maximiser). Thus, the problem
for a is to find the package [at , bt ] so as to:
maximise
such that

a t
Σm
c=1 kc ac
t−1
Σm
− atc )kcb = ub(t)
c=1 (δc

0 ≤ atc ≤ 1
388

for 1 ≤ c ≤ m

(4)

M ULTI -I SSUE N EGOTIATION

WITH

D EADLINES

This tradeoff problem is similar to the fractional knapsack problem (Martello & Toth, 1990; Cormen, Leiserson, Rivest, & Stein, 2003), the optimal solution for which can be generated using a
greedy approach5 (i.e., by filling the knapsack with items in the decreasing order of value per unit
weight). The items in the knapsack problem are analogous to the issues in our case. The only difference is that the fractional knapsack problem starts with an empty knapsack and aims to fill it with
items so as to maximise the cumulative value, while an agent’s tradeoff problem can be viewed as
starting with the agent having 100 per cent of all the issues and then aiming to give away portions
of issues to its opponent so that the latter gets a given cumulative utility, while the resulting loss
in its own utility is minimised. Thus, in order to find how to split the m issues, agent a considers
kca /kcb for 1 ≤ c ≤ m because kca /kcb is the utility that a needs to give up in order increase b’s utility
by one. Since a wants to maximise its own utility and give b a utility of U b ([at+1 , bt+1 ], t + 1), it
divides the m pies such that it gets the maximum possible share for those issues for which kca /kcb is
high and gives to agent b the maximum possible share for those issues for which kca /kcb is low. Thus,
a begins by giving b the maximum possible share for the issue with the lowest kca /kcb . It then does
the same for the issue with the next lowest kca /kcb and repeats this process until b gets a cumulative
utility of U b ([at+1 , bt+1 ], t + 1). In order to facilitate this process of making tradeoffs, the individa /k b . The function tradeoffa takes six
ual elements of kb are arranged such that kca /kcb > kc+1
c+1
parameters: ka , kb , δ, ub(t), m, and t and uses the above described greedy method to solve the
maximisation problem given in Equation 4 and return the corresponding package. If there is more
than one package that solves Equation 4, then tradeoffa returns any one of them (because agent
a gets equal utility from all such packages and so does agent b). The function tradeoffb for
agent b is analogous to that for a.
On the other hand, the equilibrium strategy for the agent that receives an offer is as follows. For
time period t, let b denote the receiving agent. Then, b accepts [xt , y t ] if ub(t) ≤ U b ([xt , y t ], t), otherwise it rejects the offer because it can get a higher utility in the next time period. The equilibrium
strategy for a as receiving agent is defined analogously. Hence we get the equilibrium strategies
(a(t) and b(t)) given in the statement of the theorem.
In this way, we reason backwards and obtain the offers for t = 1. The first mover makes this
offer and the other agent accepts it. An agreement therefore occurs in the first time period. 
Theorem 2 For the package deal procedure, the time taken to determine an equilibrium offer for
t = 1 is O(mn) where m is the number of issues and n is the deadline.
Proof: We know from Theorem 1 that the time to compute the equilibrium offer for t = n is linear in
the number of issues (see strategies a(n) and b(n)). Consider a time period t < n. During this time
period, the function tradeoffa is used to make tradeoffs. The time complexity of tradeoffa
(which uses the greedy approach described in the proof of Theorem 1) is O(m) (Martello & Toth,
1990; Cormen et al., 2003). This function needs to be repeated for every time period from the
(n − 1)th to the first. Hence the time complexity of finding an offer for the first time period is
O(mn). 
5. The time complexity of this approach is O(m) (Martello & Toth, 1990), where m denotes the number of items. Note
that the greedy method for the fractional knapsack problem takes O(m) time regardless of whether the coefficients
kca and kcb (for 1 ≤ c ≤ m) in Equation 4 are positive or negative (Martello & Toth, 1990). In the present setting
(as we mentioned at the beginning of Section 3) these coefficients are all positive. However, we will come across
negative coefficients when we deal with interdependent issues in Section 6.

389

FATIMA , W OOLDRIDGE , & J ENNINGS

Theorem 3 The package deal procedure generates a Pareto optimal outcome.
Proof: Recall that we consider competitive negotiations. Hence, for an individual issue c (where
1 ≤ c ≤ m), an increase in one agent’s utility results in a decrease in that of the other. However,
for the package deal procedure, an agent considers its cumulative utility from all m issues. Consequently, during the process of backward reasoning, at time t < n, the agent that makes tradeoffs
maximises its own cumulative utility without lowering that of its opponent (with respect to what the
opponent would offer in the next time period). Hence the equilibrium outcome for the package deal
is Pareto optimal. 
Theorem 4 For a given first mover, the package deal procedure has a unique equilibrium outcome
if the following condition is false:
C1 . There exists an i and a j (where 1 ≤ i ≤ m and 1 ≤ j ≤ m) such that (i 6= j) and
(kia /kib = kja /kjb ).
Proof: Consider a time period t < n and let a denote the offering agent. Recall from Theorem 1 that
a splits the m issues in the increasing order of kia /kib . Thus, for a given i and j, if kia /kib = kja /kjb ,
then agent a is indifferent between which of the two issues (i or j) it splits up first. For example, if
m = 2, n = 2, δ = 0.5, k1a = 1, k2a = 2, k1b = 2, and k2b = 4, then k1a /k1b = k2a /k2b = 0.5. If a
is the offering agent at t = 1, it can offer (1, 0) for issue 1 and (1/4, 3/4) for issue 2. This gives a
cumulative utility of 1.5 to a and 3 to b. Alternatively a can offer (0, 1) for issue 1 and (3/4, 1/4)
for issue 2 since this also results in the same cumulative utilities to a and b.
On the other hand, if kia /kib 6= kja /kjb , then a splits issue i first if kia /kib < kja /kjb and issue j first
if kia /kib > kja /kjb . In other words, there is only one possible equilibrium offer that a can make at
any time t < n. Likewise there is one possible equilibrium offer that b can make at any time t < n.
Since there is a unique offer for each time period, the equilibrium outcome is unique. 
Note that the uniqueness we refer to in Theorem 4 is with respect to a given first mover. If the
first mover changes, then the equilibrium outcome may change, as the following example illustrates.
Let m = 2, n = 2, δ = 0.5, k1a = 1, k2a = 2, k1b = 2, and k2b = 1. If a is the offering agent at
t = 1, its equilibrium offer is (1/4, 3/4) for the first issue and (1, 0) for the second. This results in
a cumulative of 2.25 to a and 1.5 to b. In contrast, if b is the offering agent at t = 1, its equilibrium
offer is (0, 1) for the first issue and (3/4, 1/4) for the second. This results in a cumulative utility
of 1.5 to a and 2.25 to b. In the following discussion, we use the term unique to mean unique with
respect to a given first mover.
3.2 The Simultaneous Procedure
For this procedure, the m issues are partitioned into µ > 1 disjoint subsets. For 1 ≤ c ≤ µ, let
Sc denote the cth partition where ∪µc=1 Sc = {1, . . . , m}. The issues within each subset are settled
using the package deal. Negotiation for each of the µ partitions starts at t = 1. Thus, for µ = m, all
m issues are settled simultaneously and independently of each other. At the other extreme, we have
only one partition (i.e., µ = 1) which is the package deal procedure described in Section 3.1. Since
the issues in each subset (i.e., each Sc ) are settled using the package deal, the equilibrium for each
of these µ partitions is obtained from Theorem 1. Consequently, we get the following results.
First, an agreement for each issue occurs in the first round. This is because negotiation for each
partition starts at t = 1. Also, from Theorem 1, we know that an agreement for the package deal
390

M ULTI -I SSUE N EGOTIATION

WITH

D EADLINES

occurs at t = 1. Hence, for the simultaneous procedure, an agreement for each partition (and hence
each issue) occurs in the first time period.
Second, for the simultaneous procedure, the time taken to determine an equilibrium offer for
t = 1 is Σµc=1 O(|Sc |n) where |Sc | is the number of issues in the cth partition and n is the deadline.
This is explained as follows. Since the time taken to find the equilibrium offer for t = 1 for the
package deal (i.e., for µ = 1) is O(mn) (see Theorem 2), the time taken to compute the equilibrium
offer for t = 1 for the cth partition is O(|Sc |n). Hence, for all µ partitions, the time complexity
is Σµc=1 O(|Sc |n) which is equal to O(M n), where M denotes the number of issues in the largest
partition.
Third, it follows from Theorem 4 that the simultaneous procedure has a unique equilibrium
outcome if the following condition C2 is true:
C2 . There is no partition c (where 1 ≤ c ≤ µ) for which the condition C1 is true.
Finally, as Theorem 5 shows, the simultaneous procedure may not generate a Pareto optimal
outcome.
Theorem 5 The simultaneous procedure may not generate a Pareto optimal outcome.
Proof: The package deal allows tradeoffs to be made across all the m issues, while the simultaneous
procedure allows tradeoffs to be made across issues within each partition but not across partitions.
Hence the simultaneous procedure may not generate a Pareto optimal outcome. We show this with a
counter example. Consider the case where n = 2, δ = 0.5, m = 3, µ = 2, S1 = {1, 2}, S2 = {3},
k1a = 1, k2a = 2, k3a = 3, k1b = 1, k2b = 0.5, and k3b = 0.25. Let a denote the first mover. From
Theorem 1, we know that in the equilibrium for partition S1 , agent a gets a share of 0.25 for issue
1 and 1 for issue 2, and b gets a share of 0.75 for issue 1 and nothing for issue 2. For partition S2 ,
each agent gets a share of 1/2. Thus, a’s cumulative utility from all three issues is 3.75 and that of
b is 0.875.
Now consider the case where all three issues are discussed using the package deal. Here, µ = 1
and all other parameters remain the same. In the equilibrium outcome for this procedure, a gets a
cumulative utility of 5.125 and b gets 0.875. This means that the procedure with µ = 2 does not
generate a Pareto optimal outcome. 
3.3 The Sequential Procedure
For this procedure, the m issues are partitioned into µ > 1 disjoint subsets. For 1 ≤ c ≤ µ, let Sc
denote the cth partition where ∪µc=1 Sc = {1, . . . , m}. The µ partitions are negotiated sequentially,
one after another. The issues within a subset are settled using the package deal. Negotiation for the
first partition starts at time t = 1. If negotiation for the cth (for 1 ≤ c ≤ µ) partition ends at tc ,
then negotiation for the (c + 1)th partition starts at time tc + 1. Each player gets its share for all
the issues in a partition as soon as the partition is settled. Thus, for µ = m, all m issues are settled
in sequence. At the other extreme, we have only one partition (i.e., µ = 1) which is the package
deal procedure described in Section 3.1. Since the issues in each subset (i.e., each Sc ) are settled
using the package deal, the equilibrium for each of these µ subsets is obtained from Theorem 1 by
substituting the appropriate negotiation start times for each partition.

391

FATIMA , W OOLDRIDGE , & J ENNINGS

Theorem 6 For the sequential procedure, the equilibrium time of agreement for the cth partition
(for 1 ≤ c ≤ µ) is Tc = c.
Proof: From Theorem 1, we know that an agreement for the package deal occurs in the first time
period. Hence, negotiation for each partition ends in the same time period in which it starts (i.e.,
negotiation for the cth partition starts at t = c and results in an agreement in the same time period).
The time taken to settle all the m issues is therefore µ. 
Note that the time complexity of the sequential procedure (i.e., the time to compute equilibrium
offers) is the same as that for the simultaneous procedure. Also, like the simultaneous procedure, the
equilibrium outcome for the sequential procedure may not be Pareto optimal. Finally, the condition
for the equilibrium outcome for the sequential procedure to be unique is the same as that for the
simultaneous procedure.
3.4 The Optimal Procedure
Having obtained the equilibrium outcomes for the three multi-issue procedures, we now compare
them in terms of the utilities they generate for each player. Then the procedure that gives a player
the maximum utility is its optimal one.
Note that, for the sequential procedure, the equilibrium outcome strongly depends on the order
in which the partitions are settled. This ordering is called the negotiation agenda. There are two
ways of defining the agenda (Fershtman, 1990): exogenously or endogenously. If the agenda is
determined before the actual negotiation over the issues begins, then it is said to be exogenous. On
the other hand, for the endogenous agenda, the agents decide what issue they will settle next during
the process of negotiation. The agenda that gives an agent the maximum utility between all possible
agendas is its optimal one (Fatima et al., 2004). Our objective here is not to determine the optimal
agenda, but to consider a given agenda and compare the equilibrium outcome for the sequential
procedure for that agenda with the outcomes for the simultaneous and the package deal procedures,
in order to find the optimal procedure. The following theorem characterises this procedure.
Theorem 7 Irrespective of how the m issues are split into µ > 1 partitions, the package deal is
optimal for both parties.
Proof: In order to compare an agent’s utility from different procedures, it is important to take into
account who initiates negotiation. For the package deal, the first mover makes an offer on all the
issues. Hence we compare an agent’s utilities for the three procedures, given the agent that will be
the first mover for all the three procedures for all the issues.
We first show that the outcome for the package deal is no worse than that for the simultaneous
procedure. Consider the simultaneous procedure for any µ > 1. For this procedure, for t ≤ n, the
offering agent makes tradeoffs across the issues in each partition independently of the other partitions. Now consider the package deal procedure (i.e., with µ = 1 partitions). For this procedure,
the offering agent makes tradeoffs across all m issues. Since the difference between the procedure
with µ = 1 and the one with µ > 1 is that the former makes tradeoffs across all m issues while the
latter does not, each agent’s utility from the former procedure is no worse than its utility from the
latter.
We now show that for a given µ (where µ > 1), for each agent, the outcome for the simultaneous
procedure is better than that for the sequential one (irrespective of the agenda for the sequential
procedure). We do this by considering each of the µ partitions.
392

M ULTI -I SSUE N EGOTIATION

Time of
agreement (tc )
Time to compute
equilibrium
Pareto optimal?
Unique equilibrium?

WITH

D EADLINES

Package deal
For the cth issue
tc = 1
for 1 ≤ c ≤ m
O(mn)

Simultaneous
For the cth issue
tc = 1
for 1 ≤ c ≤ m
O(M n)

Sequential
For the cth partition
tc = c
for 1 ≤ c ≤ µ
O(M n)

Yes
If ¬C1

No
If C2

No
If C2

Table 2: A comparison of the outcomes for the three multi-issue procedures for the complete information setting (CI).

• Partition c = 1. Since negotiation for the first partition starts at t = 1 for both the simultaneous and the sequential procedures, the outcome for this partition is the same for µ = 1 and
µ > 1. Hence, for the first partition, an agent gets equal utility from the two procedures.
• Partition c > 1. Let agent a denote the first mover for partition c (for 2 ≤ c ≤ µ) for
a and U a denote a’s cumulative
both simultaneous and sequential procedures. Also, let Usim
seq
utility for this partition from the equilibrium outcome for the simultaneous and the sequential
b
b denote b’s cumulative utility for this
procedures respectively. Likewise, let Usim
and Useq
partition from the equilibrium outcome for the simultaneous and the sequential procedures
respectively.
Now for the simultaneous procedure, negotiation for each partition starts in the first time
period. An agreement for each partition also occurs in the first time period. On the other hand,
for the sequential procedure, negotiation for the cth partition starts in the cth time period and
results in an agreement in the same time period (see Theorem 6). Since each pie shrinks with
a
a , and agent b’s cumulative utility
time, agent a’s cumulative utility Usim
is greater than Useq
b
b .
Usim
is greater than Useq
Thus, the simultaneous procedure is better than the sequential one for both agents. Furthermore
(as shown above), the outcome for the package deal is no worse than that for the simultaneous
procedure for both agents. Therefore, for each agent, the package deal is the optimal procedure. 
These results are summarised in Table 2. For the above analysis, the negotiation parameters n, δc ,
kca , and kcb (for 1 ≤ c ≤ m) were common knowledge to the agents. However, this is unlikely to
be the case for most encounters. Therefore we now extend this analysis to incomplete information
scenarios with uncertainty about utility functions6 . In Section 4, we focus on the symmetric information setting where each agent is uncertain about the other’s utility function. Then, in Section 5,
we examine the asymmetric information setting where one of the two agents is uncertain about the
other’s utility function, but the other agent knows the utility function of both agents.
6. There are two other sources of uncertainty: uncertainty about the negotiation deadline and uncertainty about discount
factors. Future work will deal with uncertainty about discount factors. However, for independent issues, we analysed
the case with symmetric uncertainty about deadlines in (Fatima, Wooldridge, & Jennings, 2006). The extension of
this work to the case of interdependent issues is another direction for future work.

393

FATIMA , W OOLDRIDGE , & J ENNINGS

4. Multi-Issue Negotiation with Symmetric Uncertainty about the Opponent’s Utility
In this symmetric information setting, each agent is uncertain about its opponent’s utility function:
for 1 ≤ c ≤ m, agent a (b) is uncertain about kcb (kca ). Specifically, let K denote a vector of r vectors
where each vector Ki ∈ Rm
+ (for 1 ≤ i ≤ r) consists of m constant positive real numbers. These
b
m
7
r vectors are the possible values for ka ∈ Rm
+ and k ∈ R+ . In other words, there are r types
for agent a and r types for agent b. Let P a : N+ → R1 denote the discrete probability distribution
function for ka and P b : N+ → R1 that for kb . The domain for these two functions is [1..r]. In other
words, for 1 ≤ i ≤ r, P a (i) (P b (i)) is the probability that agent a (b) is of type i. For 1 ≤ c ≤ m,
let Kic denote the cth element of vector Ki .
In this setting, the vector K and the functions P a and P b are common knowledge to the negotiators. Also, each agent knows its own type, but not that of its opponent. In addition, each agent
knows r, δ, n, and m.
Since there are r types for agent a and r types for agent b, we define r different cumulative
utility functions for each of the two agents. If agent a (b) is of type i (for 1 ≤ i ≤ r) then its utility
+
m
+
b
m
m
Uia : Rm
1 × R1 × N → R (Ui : R1 × R1 × N → R) from the division specified by the package
[xt , y t ] at time t is:
(
a t
Σm
c=1 Kic uc (xc , t) if t ≤ n
a
t t
(5)
Ui ([x , y ], t) =
0
otherwise
(
b t
Σm
c=1 Kic uc (yc , t) if t ≤ n
b
t t
(6)
Ui ([x , y ], t) =
0
otherwise
Note that, as before, the issues are perfect substitutes. For this setting, we determine the equilibrium outcomes for each of the three multi-issue procedures and then compare them.
4.1 The Package Deal Procedure
We know from Theorem 1 that the equilibrium outcome for the complete information setting depends on kca and kcb (for 1 ≤ c ≤ m). However, in this setting, there is uncertainty about kca and
kcb . Hence we use the standard expected utility theory (Neumann & Morgenstern, 1947; Fishburn,
1988; Harsanyi & Selten, 1972) to find an agent’s optimal strategy. Before doing so, however, we
first introduce some notation.
For 1 ≤ i ≤ r, we let a(i, t) denote the equilibrium strategy for an agent a of type i for the
time period t. Analogously, b(i, t) denotes the equilibrium strategy for an agent b of type i for the
time period t. Note that for 1 ≤ i ≤ r, if [at , bt ] is the package offered at time t in equilibrium,
then at + bt = δt−1 (i.e., for each pie, the sum of the shares of the two agents is equal to the size
of the pie at time t). Also, for 1 ≤ i ≤ r, we let a(i, j, t) denote the equilibrium strategy for an
agent a of type i for the time period t, assuming that b is of type j. Analogously, b(i, j, t) denotes
the equilibrium strategy for an agent b of type i for the time period t, assuming that a is of type j.
Also, let eua(i, t) denote the cumulative utility that an agent a of type i expects to get from b’s
equilibrium offer at time t (i.e., a is the receiving agent and b the offering agent at t). Likewise,
eub(i, t) denotes the cumulative utility that an agent b of type i expects to get from a’s equilibrium
offer at time t (i.e., b is the receiving agent and a the offering agent at t). We let eua(i, j, t) denote
agent a’s expected cumulative utility from its own equilibrium offer at time t if a is of type i,
7. An agent’s type indicates which of the r vectors it corresponds to.

394

M ULTI -I SSUE N EGOTIATION

WITH

D EADLINES

assuming that b is of type j. Note that this is a’s utility when it is the offering agent at t. And let
eub(i, j, t) denote agent b’s expected cumulative utility from its own equilibrium offer at time t if b
is of type i and assuming that a is of type j. Note that this is b’s utility when it is the offering agent
at t.
Recall that in this setting, each agent only knows its own type, but not that of its opponent.
Since there are r possible types, there are r possible offers an agent can make at any time period
(one offer corresponding to each of the opponent’s types). Among these r offers, the one that gives
an agent the maximum expected cumulative utility is its optimal offer. If the cth offer (1 ≤ c ≤ r)
gives an agent the maximum expected cumulative utility, then we say that the optimal choice for the
agent is c. For time period t, we let opta(i, t) (optb(i, t)) denote the optimal choice for agent a
(b) of type i.
At t = n, the offering agent gets everything and the opponent gets zero utility. Thus, for t = n,
we have the following:
eua(i, n) = 0

for 1 ≤ i ≤ r

(7)

eub(i, n) = 0
m
X
Kic δct−1
eua(i, j, n) =

for 1 ≤ i ≤ r

(8)

for 1 ≤ i ≤ r and 1 ≤ j ≤ r

(9)

for 1 ≤ i ≤ r and 1 ≤ j ≤ r

(10)

eub(i, j, n) =

c=1
m
X

Kic δct−1

c=1

Note that for t = n, eua(i, j, n) and eub(i, j, n) do not depend on j because in the last time period,
the offering agent gets 100 percent of all the m pies. For all preceding time periods t < n, we have
the following:
eua(i, t) = eua(i, θ, t + 1)

for 1 ≤ i ≤ r where θ = opta(i, t + 1)

(11)

eub(i, t) = eub(i, λ, t + 1)
for 1 ≤ i ≤ r where λ = optb(i, t + 1)
r
X
F a (i, j, e, t) × P b (e)
for 1 ≤ i ≤ r and 1 ≤ j ≤ r
eua(i, j, t) =

(12)
(13)

e=1

eub(i, j, t) =

r
X

F b (i, j, e, t) × P a (e)

for 1 ≤ i ≤ r and 1 ≤ j ≤ r

(14)

e=1

Fa

The function
takes four parameters: i, j, e, and t, and returns the utility that an agent a of type
i gets from offering the equilibrium package for time t, assuming that agent b is of type j where in
fact it is of type e. Obviously, agent b accepts a’s offer at t if Ueb (a(i, j, t), t) ≥ eub(e, γ, t + 1)
where γ = optb(e, t + 1). Otherwise, agent b rejects a’s offer and negotiation proceeds to the next
round in which case a’s expected utility is EUA (i, t + 1). Hence, F a is defined as follows:
 a
Ui (a(i, j, t), t) if Ueb (a(i, j, t), t) ≥ eub(e, γ, t + 1) where γ = optb(e, t + 1)
a
F (i, j, e, t) =
eua(i, t + 1)
otherwise
where the strategy a(i, j, t) for t = n is defined as follows:

OFFER [δn−1 , 0] if a’s turn
A (i, j, n) =
ACCEPT
otherwise
395

FATIMA , W OOLDRIDGE , & J ENNINGS

and for all preceding time periods t < n it is defined as:

OFFER tradeoffa1(K, δ, eub(j, t), i, j, m, t, P a , P b ) if a’s turn
A (i, j, t) =
if Uia ([xt , y t ], t) ≥ EUA (i, t) ACCEPT else REJECT
otherwise
where [xt , y t ] denotes the offer made at t and the function8 TRADEOFFA 1 is defined as follows.
Like TRADEOFFA , the function TRADEOFFA 1 solves the following maximisation problem:
maximise
such that

t
Σm
c=1 Kic ac
t−1
− atc )Kjc = eub(j, t)
Σm
c=1 (δc

0 ≤ atc ≤ 1

for 1 ≤ c ≤ m

(15)

where i denotes a’s type and j that of b. However, the difference between TRADEOFFA 1 and
TRADEOFFA arises when there is more than one package that maximises a’s cumulative utility (i.e.,
t
Σm
c=1 Kic ac ) while giving b a cumulative utility of eub(j, t). If there is more than one such package,
then in Theorem 1, it does not matter which of these packages a offers to b (because both agents
have complete information). Hence, TRADEOFFA can return any one such package. However, in
the present setting, there is uncertainty. Therefore, if there is more than one package that maximises
a’s cumulative utility while giving b a cumulative utility of eub(j, t), then TRADEOFFA 1 returns
the package that maximises a’s expected cumulative utility. For instance, let [at , bt ] be one such
package that maximises a’s cumulative utility. Then a’s expected cumulative utility from [at , bt ]
(i.e., eua(i, j, t)) is as given in Equation 13 where:
 a t t
Ui ([a , b ], t) if Ueb ([at , bt ], t) ≥ eub(e, γ, t + 1) where γ = optb(e, t + 1)
F a (i, j, e, t) =
eua(i, t + 1) otherwise
Obviously, if there is more than one package that maximises a’s expected cumulative utility and
gives b a utility of eub(j, t) then TRADEOFFA 1 returns any one such package.
We now turn to agent b. For this agent, F b , B(i, j, t), and tradeoffb1 are defined analogously
as follows:
 b
Ui (b(i, j, t), t) if Uea (b(i, j, t), t) ≥ eua(e, α, t + 1) where α = opta(e, t + 1)
b
F (i, j, e, t) =
eub(i, t + 1)
otherwise
where the strategy b(i, j, t) for t = n is defined as follows:

OFFER [0, δn−1 ] if b’s turn
B (i, j, n) =
ACCEPT
otherwise
and for all preceding time periods t < n it is defined as:

OFFER tradeoffb1(K, δ, eua(j, t), i, j, m, t, P a , P b ) if b’s turn
B (i, j, t) =
if Uib ([xt , y t ], t) ≥ EUB (i, t) ACCEPT else REJECT
otherwise
8. A method for making tradeoffs has been proposed by Faratin, Sierra, and Jennings (2002) for an incomplete information setting, but this method differs from ours. Also, Faratin et al. only present a method for making tradeoffs, but
they do not show that the resulting offer is in equilibrium. In contrast, our method shows that the resulting offer is in
equilibrium.

396

M ULTI -I SSUE N EGOTIATION

WITH

D EADLINES

Thus, the optimal choice for agent a (i.e., opta(i, t)) and that for agent b (i.e., optb(i, t)) are
defined as follows:
opta(i, t) = arg maxrj=1 eua(i, j, t)

for 1 ≤ i ≤ r

(16)

maxrj=1 eub(i, j, t)

for 1 ≤ i ≤ r

(17)

optb(i, t) = arg

Note that the offering agent’s optimal choice for t = n does not depend on its opponent’s type since
the offering agent gets all the pies.
We compute the optimal choice for the first time period by reasoning backwards from t = n.
At t = 1, if an agent a of type i is the offering agent, then it offers the package that corresponds to
agent b being of type opta(i, 1). Likewise, if an agent b of type i is the offering agent, then it offers
the package that corresponds to agent a being of type optb(i, 1).
However, since opta(i, 1) and optb(i, 1) are obtained in the absence of complete information,
an agreement may or may not take place in the first time period. If an agreement does not occur
at t = 1, then the agents need to update their beliefs as follows. Let Tta ⊆ {1, 2, . . . , r} denote
the set of possible types for agent a at time t. For t = 1, we have T1a = {1, 2, . . . , r} and T1b =
{1, 2, . . . , r}. Assume that an agent a of type i makes an offer at t = 1. If the offer that a makes
gets rejected, then it means that b is not of type opta(i, 1) and so a updates its beliefs about b using
Bayes’ rule. Now, on the basis of a’s offer at t = 1 (say [x1 , y 1 ]), agent b can infer the possible
types for agent a. Thus, agent b too updates its beliefs using Bayes’ rule. The belief update rules
for time t are as defined below.
UPDATE BELIEFS: Agent a puts all the weight of the posterior distribution of b’s type
over Ttb − {optb(i, t)} using Bayes’ rule. Agent b puts all the weight of the posterior
distribution of a’s type over K using Bayes’ rule where K ⊆ {1, 2, . . . , r} is the set of
possible types for a that can offer [xt , y t ] in equilibrium.
The belief update rule for the case where b offers at t = 1 is analogous to the above case where a
offers at t = 1.
Thus if the offer at t = 1 gets rejected, then negotiation goes to the next round. At t = 2, the
offering agent (say an agent a of type i) finds opta(i, 2) with its updated beliefs. This process of
updating beliefs and making offers continues until an agreement is reached.
In Section 3, we used the concept of Nash equilibrium because the agents had complete information. However, in the current setting, each agent is uncertain about its opponent’s type and so
an agent’s optimal strategy depends on its beliefs about its opponent. Hence we use the concept
of sequential equilibrium (Kreps & Wilson, 1982; van Damme, 1983) for this setting. Sequential
equilibrium is defined in terms of two elements: a strategy profile and a system of beliefs. The
strategy profile comprises of a pair of strategies, one for each agent. The belief system has the following properties. Each agent has a belief about its opponent’s type. In each time period, an agent’s
strategy is optimal given its current beliefs (during the time period) and the opponent’s possible
strategies. For each time period, each agent’s beliefs (about its opponent) are consistent with the
offers it received. Using this concept of sequential equilibrium, the following theorem characterises
the equilibrium for the package deal procedure.
Theorem 8 For the package deal procedure, the following strategies form a sequential equilibrium.
The equilibrium strategies for t = n are:

OFFER [δn−1 , 0] IF a’s TURN
a(i, n) =
ACCEPT
IF b’s TURN
397

FATIMA , W OOLDRIDGE , & J ENNINGS

b(i, n) =



OFFER [0, δn−1 ] IF b’s TURN
ACCEPT
IF a’s TURN

for 1 ≤ i ≤ r. For all preceding time periods t < n, if [xt , y t ] denotes the offer made at time t, then
the equilibrium strategies are defined as follows:

OFFER tradeoffa1(K, δ, eub(ψ, t), i, ψ, m, t, P a , P b ) IF a’s TURN



If offer gets rejected UPDATE BELIEFS
a(i, t) =
RECEIVE OFFER and UPDATE BELIEFS
IF b’s TURN



If (Uia ([xt , y t ], t) ≥ eua(i, t)) ACCEPT else REJECT

OFFER tradeoffb1(K, δ, eua(φ, t), i, φ, m, t, P a , P b ) IF b’s TURN



If offer gets rejected UPDATE BELIEFS
b(i, t) =
RECEIVE OFFER and UPDATE BELIEFS
IF a’s TURN



b
t
t
If (Ui (x , y ], t) ≥ eub(i, t)) ACCEPT else REJECT

for 1 ≤ i ≤ r. Here, ψ = opta(i, t) and φ = optb(i, t). The earliest possible time of agreement
is t = 1 and the latest possible time of agreement is t = min(2r − 1, n).
Proof: At time t = n, the offering agent takes all the pies and leaves nothing for its opponent.
The opponent accepts this and we get a(i, n) and b(i, n). Now consider a time period t < n.
Recall that during negotiation for the complete information setting (see Section 3.1), at time t < n,
the offering agent proposes a package that gives its opponent a cumulative utility equal to what
the opponent would get from its own equilibrium offer for the next time period. However, for the
current incomplete information setting, an agent knows its own type but not that of its opponent.
Hence, for this scenario, at time t < n, the offering agent (say a) proposes a package that gives b
an expected cumulative utility equal to what b would get from its own equilibrium offer for the next
time period (i.e., eub(ψ, t)). This package is determined by the tradeoffa1 function. Likewise,
if b is the offering agent at time t, then it makes tradeoffs using tradeoffb1 and offers a an
expected cumulative utility eua(φ, t).
We obtain the equilibrium offer for t = n − 1 and then reason backwards until we obtain the
equilibrium offer for t = 1. However, since these offers are computed in the absence of complete
information (i.e., on the basis of expected utilities), an agreement may or may not take place at
t = 1. If an agreement does not take place at t = 1, then negotiation proceeds as follows. Consider
a time period t such that 1 ≤ t < n. Let [xt , y t ] denote the offer made at time t. The agent
that receives the offer (say agent a) updates its beliefs using Bayes’ rule: put all the weight of the
posterior distribution of b’s type over K where K ⊆ {1, 2, . . . , r} is the set of possible types for b
that can offer [xt , y t ] in equilibrium. If the proposed offer ([xt , y t ]) gets rejected, then the offering
agent (say agent b of type i) updates its beliefs using Bayes’ rule: put all the weight of the posterior
distribution of a’s type over Tta − {optb(i, t)}. The belief update rule for the case where agent a
offers at time t are analogous to the above rule. These belief update rules when incorporated in the
agents’ strategies give a(i, t) and b(i, t) as shown in the statement of the theorem.
We now show that the beliefs specified above are consistent. During any time period t < n, let
the strategy profile (a(i, t), b(i, t)) assign probability 1 − ǫ to the above specified posterior beliefs
and probability ǫ to the rest of the support for the opponent’s type. As ǫ → 0, the above strategy pair
converges to (a, b). Also, the beliefs generated by the strategy pair converge to the beliefs described
above. Given these beliefs, the strategies a and b are sequentially rational.
398

M ULTI -I SSUE N EGOTIATION

WITH

D EADLINES

The earliest possible time of agreement is t = 1. We show this with the following example. Let
n = 2, m = 2, r = 2, δ = 1/2, and K = [1, 2; 5, 1]. Let agent a be the offering agent at time t = 1.
Assume that a is of type 1 (i.e., ka = [1, 2]). Let P b (1) = 0.1 and P b (2) = 0.9. Since r = 2, agent
a can play two possible strategies at time t = 1: one that corresponds to the case where b is of type 1
and the other that corresponds to the case where b is of type 2. For the former case, a’s equilibrium
offer at t = 1 is [0, 1] for the first issue and [ 43 , 14 ] for the second one. Hence eua(1, 1, 1) = 1.5.
For the latter case, a’s equilibrium offer at t = 1 is [ 52 , 35 ] for the first issue and [1, 0] for the second
issue. Hence eua(1, 2, 1) = 2.16. Since eua(1, 2, 1) > eua(1, 1, 1), opta(1, 1) = 2 and a plays
the latter strategy. Now if b is in fact of type 2, then it accepts a’s offer at t = 1. But if b is in fact of
type 1, it rejects a’s offer at t = 1 since it can get a higher utility at t = 2. An agreement therefore
occurs at t = 2. Thus, the earliest possible time of agreement is t = 1.
Now consider the case where an a of type i offers at t = 1 but an agreement does not occur at
this time. When a’s offer gets rejected, it knows that b is not of type opta(i, 1). Thus the number
of possible types for b is now reduced to r − 1. This happens every time a makes an offer (i.e., every
alternate time period) but it gets rejected. When negotiation reaches time period t = 2r − 1, there
is only one possible type for b. Likewise, there is only one possible type for agent a. An agreement
therefore takes place at t = 2r − 1. However, if n < 2r − 1 then an agreement occurs at t = n (see
a(i, n) and b(i, n)). In other words, if an agreement does not occur at t = 1, then it occurs at the
latest by t = min(2r − 1, n). 
As we mentioned earlier, if there is more than one package that solves Equation 15, then tradeoffa1
returns the one that maximises a’s expected cumulative utility. Let paij
t (where i denotes a’s type
and j that of b) denote the set of all possible packages that tradeoffa1 can return at time t. The
set pbij
t for agent b is defined analogously.
Theorem 9 For a given first mover, the package deal procedure has a unique equilibrium outcome
if the condition C3 is false or C4 is true.
C3 . There exists an i, j, c, and d, such that (c 6= d) and (i 6= j) and (Kic /Kjc = Kid /Kjd ) where
1 ≤ i ≤ r, 1 ≤ j ≤ r, 1 ≤ c ≤ m, and 1 ≤ d ≤ m.
ij
C4 . |paij
t | = 1 and |pbt | = 1 where 1 ≤ i ≤ r, 1 ≤ j ≤ r, i 6= j, and 1 ≤ t ≤ n.

Proof: Let i denote agent a’s type and j denote b’s type where i 6= j, 1 ≤ i ≤ r, and 1 ≤ k ≤ r.
Note that if a and b are of the same type, they have similar preferences for different issues. So i 6= j
because the agents gain from making tradeoffs when they are of different types. The rest of the
proof for the condition C3 follows from Theorem 4. Consider C4 . If C3 is true, then we know that,
at time t, tradeoffa1 returns that package that solves Equation 15 and maximises a’s expected
cumulative utility. Hence if paij
t contains a single element, then there is only one possible package
that tradeoffa1 can return. Likewise, if pbij
t contains a single element, then there is only one
possible package that tradeoffb1 can return. If there is only one possible offer for each time
period 1 ≤ t ≤ n, then the equilibrium outcome is unique. 
In order to determine the time complexity of the package deal, we first find the complexity
of the tradeoffa1 function. As we mentioned before, tradeoffa1 differs from tradeoffa
when there is more than one package that solves the maximisation problem of Equation 15. We
know from Theorem 9 that there is more than one such package if the condition C3 is true. We also
399

FATIMA , W OOLDRIDGE , & J ENNINGS

know from Theorem 1 that using the greedy approach, tradeoffa considers the m issues in the
increasing order of Kic /Kjc where i denotes a’s type and j denotes b’s type. Let Spij ⊆ S denote a
set of issues (where 0 ≤ D ij < m, 1 ≤ p ≤ D ij , i denotes a’s type, and j denotes b’s type) such
that:
|Spij | > 1 for 1 ≤ p ≤ D ij
and:
∀c,d∈Spij

Kic
Kjc

=

Kid
Kjd

In other words, Spij is a set of issues such that if c and d belong to Spij then Kic /Kjc = Kid /Kjd , and
D ij is the number of sets that satisfy this condition. So if D ij = 0 then it means that there is only
one package that solves Equation 15. But if D ij > 0 then there is more than one package that solves
Equation 15 and from among these tradeoffa1 must find the one that maximises a’s expected
cumulative utility. For example if the set of issues is S = {1, 2, 3, 4}, r = 2, K1 = {5, 6, 7, 8},
and K2 = {9, 6, 7, 8}, then D12 = 1, S112 = {2, 3, 4}, and |S112 | = 3. So while making tradeoffs,
a can consider the issues in S112 in any order because for all the three issues it needs to give up the
same amount of utility in order to increase b’s utility by 1. The three issues in S112 can be ordered
in 3! different ways resulting in 3! different packages. From among these 3! different packages,
tradeoffa1 must find the one that maximises a’s expected cumulative utility. In general, for
D ij > 1, let π ij denote the number9 of possible packages tradeoffa1 needs to consider where
π ij is:
ij
D
Y
ij
|Spij |!
π =
p=1

In other words, if a’s type is i and b’s type is j, then there are π ij packages that solve Equation 15
and from among these tradeoffa1 must find the one that maximises a’s expected cumulative
utility. So if Dij = 0, then π ij = 1. Let π̂ be defined as:
π̂ =

max

1≤i≤r,1≤j≤r,i6=j

π ij

(18)

In other words, π̂ is the maximum number of packages that tradeoffa1 will have to search to
find the one that maximises a’s expected cumulative utility (considering all possible types of a
and all possible types of b). Note that, as before, a and b are of different types (i.e., i 6= j in
Equation 18) because the agents gain from making tradeoffs when they are of different types. The
time complexity of tradeoffa1 depends on π̂.
Theorem 10 The time complexity of tradeoffa1 is O(mπ̂).
Proof: We know from Theorem 2 that the time complexity of finding any one package that solves
Equation 15 is O(m). However, if there is more than one package that solves Equation 15 then
tradeoffa1 returns the one that maximises a’s expected cumulative utility. The time to compute
a’s expected cumulative utility from any one such package is O(m). The maximum number of such
packages for which a needs to find its expected cumulative utility is π̂. Thus the time complexity of
tradeoffa1 is O(mπ̂). 
9. Note that π ij is defined in terms of the factorial of |Spij |, but |Spij | is independent of m and it is assumed that
|Spij | ≪ m.

400

M ULTI -I SSUE N EGOTIATION

WITH

D EADLINES

Corollary 1 If D ij = 0 for 1 ≤ i ≤ r, 1 ≤ j ≤ r, and i 6= j, then the time complexity of
tradeoffa1 is the same as the complexity of tradeoffa.
Proof: If D ij = 0 for 1 ≤ i ≤ r, 1 ≤ j ≤ r, and i 6= j, then π ij = 1 and so π̂ = 1. So the time
complexity of tradeoffa1 is O(m). 
Theorem 11 The time complexity of computing the equilibrium offers for the package deal procedure is O(mπ̂r 3 T (n − T2 )) where T = min(2r − 1, n).
Proof: Let a denote the agent that offers at t = 1 and assume that n is even (the proof for odd
n is analogous). We begin with the last time period and then reason backwards. Since n is even
and a starts at t = 1, it is b’s turn to offer in the last time period. For t = n, the time taken to find
eub(i, j, t) (for a given i and j) is O(m) (see Equation 10). Hence, the time taken to find eub(i, j, t)
for all possible types of b (i.e., 1 ≤ j ≤ r) is O(mr). Note that, at this stage, eub(i, t − 1) is known
for 1 ≤ i ≤ r (see Equation 12).
Now consider the time period t = n − 1. Since n is even, it is a’s turn to offer at t = n − 1.
In order to find a(i, t), we first need to find ψ where ψ = opta(i, t). From Equation 16 we know
that, for a given i, the time to find opta(i, t) depends on the time taken to find eua(i, j, t) which
in turn depends on the time to find fa (i, j, e, t) (see Equation 13). The time taken for fa (i, j, e, t)
depends on the time taken for a(i, j, t). For a given i and a given j, the time taken to find a(i, j, t)
is the time taken by the function tradeoffa. Since eub(j, t) is already known at time t, the time
taken by tradeoffa1 is O(mπ̂) (see Theorem 10). The time taken to find fa (i, j, e, t) is therefore
O(mπ̂). Given this, the time to find eua(i, j, t) (for a given i, j, and t) is O(mπ̂r). Hence, for a
given i, the time to find ψ = opta(i, t) is O(mπ̂r 2 ). At this stage, EUB (ψ, t) is known (see the last
sentence in the first paragraph of this proof). Consequently, for a given i, the time to find a(i, t) is
O(mπ̂r 2 ). Recall that each agent knows only its own type and not that of its opponent. Hence we
need to determine a(i, t) for all possible types of a (i.e., for 1 ≤ i ≤ r). This takes O(mπ̂r 3 ) time.
Note that at this stage eua(i, j, t) is known for all possible values of i and all possible values of j
(where 1 ≤ i ≤ r and 1 ≤ j ≤ r).
Now consider the time period t = n − 2 when it is b’s turn to offer. For t = n − 2 and a given i,
the time to find optb(i, t) is O(mπ̂r 2 ) and so the time to find optb(i, t) for all possible types of
b (i.e., for 1 ≤ i ≤ r) is O(mπ̂r 3 ).
In the same way, the time required to do all the necessary computation for each time period
t < n is O(mπ̂r 3 ). Hence, the total time to find the equilibrium offer for the first time period is
O((n − 1)mπ̂r 3 ). However, as noted previously, an agreement may or may not occur in the first
time period. If an agreement does not take place at t = 1, then the agents update their beliefs
and compute the equilibrium offer for t = 2 with the updated beliefs. The time to compute the
equilibrium offer for t = 2 is O((n − 2)mπ̂r 3 ). This process of updating beliefs and finding the
equilibrium offer is repeated at most T = min(2r − 1, n) times. Hence the time complexity of the
package deal is ΣTi=1 O((n − i)mπ̂r 3 ) = O(mπ̂r 3 T (n − T2 )) (see Cormen et al., 2003, – page 47
– for details on how to simplify an expression of the form ΣTi=1 O((n − i)mπ̂r 3 )). 
Theorem 12 The package deal procedure generates a Pareto optimal outcome.
Proof: This follows from Theorem 3. The difference between the complete information setting of
Theorem 3 and the current incomplete information setting is that for the former setting the agents
maximise their cumulative utilities, whereas in the current setting they maximise their expected
cumulative utilities. Specifically, for every time period, the offering agent maximises its expected
401

FATIMA , W OOLDRIDGE , & J ENNINGS

cumulative utility from all the m issues such that its opponent’s expected cumulative utility is equal
to what the opponent would get from its own equilibrium offer for the next time period. Hence, for
the current setting, the equilibrium offer for every time period is Pareto optimal. 
4.2 The Simultaneous Procedure
Recall that for this procedure, the µ > 1 partitions are discussed in parallel but independently of
each other. The offers made during the negotiation for any one partition do not affect the offers
for the others. Specifically, negotiation for each partition starts at t = 1 and each partition is
settled using the package deal procedure. Since each partition is dealt with separately, the results of
Theorem 8 apply directly to each of the µ partitions.
Let π̂c denote π̂ for the cth partition. Then, from Theorem 11, we know that the time taken for the
cth (for 1 ≤ c ≤ µ) partition is O(|Sc |π̂c r 3 T (n − T2 )). Let the partition for which |Sc |πˆc is highest
be denoted Sz . Then the time complexity of the simultaneous procedure is O(|Sz |π̂z r 3 T (n −
T
2 )). Also, from Theorem 5, it follows that the simultaneous procedure may not generate a Pareto
optimal outcome. Finally, from Theorem 9 we know that the simultaneous procedure has a unique
equilibrium outcome if the following condition is satisfied:
C5 . If there is no partition c (where 1 ≤ c ≤ µ) for which the condition (¬C3 ∨ C4 ) is false.
4.3 The Sequential Procedure
For this procedure, the µ > 1 partitions are discussed independently and one after another. Also,
for 1 ≤ c ≤ µ, negotiation on the cth partition starts in the time period that follows an agreement
on the (c − 1)th partition. Since the package deal is used for each partition, the following results are
obtained on the basis of Theorem 8.
First, Theorem 8 applies to each of the µ > 1 partitions. Thus, for the sequential procedure,
if negotiation for the cth (for 1 ≤ c ≤ µ) partition starts at time tc , then it ends at the earliest at
time tc and at the latest by tc + min(2r − 1, n). Second, it follows from Theorem 11 that the time
taken for the sequential procedure is O(|Sz |π̂z r 3 T (n − T2 )). Third, the sequential procedure may
not generate a Pareto optimal outcome (see Theorem 5). Finally, the conditions for uniqueness are
the same as those for the simultaneous procedure.
4.4 The Optimal Procedure
Having obtained the equilibrium outcomes for the three procedures for the above defined incomplete
information scenario, we now compare them in terms of the expected utilities they generate to each
player. Again, the procedure that gives a player the maximum expected utility is the optimal one.
Theorem 13 The package deal is optimal for each agent.
Proof: The proof for this is the same as Theorem 7. The only difference between the complete
information setting of Theorem 7 and the current incomplete information setting is that for the
package deal procedure for the former setting (during time period t < n), the offering agent proposes a package that maximises its own cumulative utility, while giving its opponent a cumulative
utility equal to what the opponent would get from its own equilibrium offer in the next time period.
On the other hand, for the current incomplete information setting, the offering agent proposes a
package that maximises its own expected cumulative utility while giving its opponent an expected
402

M ULTI -I SSUE N EGOTIATION

Time of
agreement

Time to compute
equilibrium
Pareto optimal?
Unique equilibrium?

Package deal
Earliest: 1
Latest: min(2r − 1, n)
for all m issues
O(mπ̂r 3 T (n − T2 ))

WITH

D EADLINES

Simultaneous
Earliest: 1
Latest: min(2r − 1, n)
for all m issues
O(|Sz |π̂z r 3 T (n −

Yes
If ¬C3 ∨ C4

No
If C5

T
2 ))

Sequential
For the cth partition
tec = tsc
l
s
tc = tc + min(2r − 1, n)
for 1 ≤ c ≤ µ
O(|Sz |π̂z r 3 T (n − T2 ))
No
If C5

Table 3: A comparison of the expected outcomes for the three multi-issue procedures for the symmetric information setting (for the sequential procedure, tsc denotes the start time for the
cth partition, tec the earliest possible time of agreement, and tlc the latest possible time of
agreement).

cumulative utility equal to what the opponent would get from its own equilibrium offer in the next
time period. Also, for each agent, the package deal maximises the expected cumulative utility from
all the m issues (since tradeoffs are made across all the m issues). But the simultaneous procedure
maximises each agent’s expected cumulative utility for each partition (i.e., the simultaneous procedure does not make tradeoffs across partitions). Hence each agent’s expected cumulative utility for
all the m issues is higher for the package deal relative to the simultaneous procedure. Furthermore,
irrespective of how the m issues are partitioned into µ partitions, we know that the simultaneous
procedure is better than the sequential one for each agent (see Theorem 7). Hence, the package deal
is optimal for each agent. 
These results are summarised in Table 3.

5. Multi-Issue Negotiation with Asymmetric Uncertainty about the Opponent’s
Utility
In some bargaining situations, one of the players may know something of relevance that the other
may not know. For example, when bargaining over the price of a second hand car, the seller knows
its quality but the buyer does not. Such situations are said to have asymmetry in information between
the players (Muthoo, 1999). Our asymmetric information setting differs from the symmetric one
explored in the previous section in that one of the two agents (say a) has complete information, but
the other (say b) is uncertain about a’s utility function: for 1 ≤ c ≤ m, agent b is uncertain about
kca . Here, K, P a , P b , n, r, and m are as defined in Section 4. The negotiation parameters K, P a ,
P b , r, δ, n, and m are common knowledge to the negotiators. Furthermore, a knows its own type
and that of b, while b knows its own type but not that of a. Finally, the definitions for the cumulative
utility functions remain the same as in Section 4. For this setting, we now determine the equilibrium
for each of the three multi-issue procedures.
403

FATIMA , W OOLDRIDGE , & J ENNINGS

5.1 The Package Deal Procedure
We extend the analysis of Section 4 to the current setting as follows. It is clear that for the last time
period (t = n), the utilities eua(i, t) and eub(i, t) are as per Section 4. Let j̄ denote b’s actual
type. Recall that agent a now knows j̄. Hence on the basis of Equation 13 for the SUI setting, we
get eua(i, j, t) for the current asymmetric information setting as follows:
eua(i, j, t) = F a (i, j, j̄, t)

for 1 ≤ i ≤ r and 1 ≤ j ≤ r

(19)

On the other hand, since agent b is uncertain about a’s type, the definitions for eub(i, t) and
eub(i, j, t) are as given in Section 4. Also, the definitions for F a , F b , a(i, j, t), b(i, j, t), opta(i, t),
and optb(i, t) for all time periods remain the same as in Section 4.
Finally, in this setting, belief updating does not apply to agent a because it has complete information. Only agent b updates its beliefs about a. This is done in the same way described in
Section 4. Because of b’s uncertainty, we use the concept of sequential equilibrium in this setting as
well. The following theorem characterises the equilibrium for the package deal procedure.
Theorem 14 For the package deal procedure the following strategies form a sequential equilibrium. The equilibrium strategies for t = n are:

OFFER [δn−1 , 0] IF a’s TURN
a(i, n) =
ACCEPT
IF b’s TURN
b(i, n) =



OFFER [0, δn−1 ] IF b’s TURN
ACCEPT
IF a’s TURN

for 1 ≤ i ≤ r. For all preceding time periods t < n, if [xt , y t ] denotes the offer made at time t, then
the equilibrium strategies are defined as follows:

 OFFER tradeoffa1(K, δ, eub(j̄, t), i, j̄, m, t, P a , P b ) IF a’s TURN
a(i, t) =
RECEIVE OFFER
IF b’s TURN

If (Uia ([xt , y t ], t) ≥ eua(i, t)) ACCEPT else REJECT

OFFER tradeoffb1(K, δ, eua(φ, t), i, φ, m, t, P a , P b ) IF b’s TURN



If offer gets rejected UPDATE BELIEFS
b(i, t) =
IF a’s TURN
 RECEIVE OFFER and UPDATE BELIEFS


If (Uib (xt , y t ], t) ≥ eub(i, t)) ACCEPT else REJECT

for 1 ≤ i ≤ r. Here, j̄ denotes agent b’s type and φ = optb(i, t). The earliest possible time of
agreement is t = 1 and the latest possible time is t = min(2r − 1, n).
Proof: As Theorem 8. The only difference is that a now knows b’s type (j̄). Hence this information
is used as a parameter for tradeoffa1.
The earliest possible time of agreement is t = 1. We show this with the following example.
Let n = 2, m = 2, r = 2, δ = 1/2, and K = [1, 2; 5, 1]. Let b (i.e., the agent with uncertain
information) be the offering agent at time t = 1. Assume that b is of type 2 (i.e., kb = [5, 1]). Let
P a (1) = 0.9 and P a (2) = 0.1. Since r = 2, b can play two possible strategies at time t = 1:
one that corresponds to the case where a is of type 1 and the other that corresponds to the case
where a is of type 2. For the former case, b’s equilibrium offer at t = 1 is [0, 1] for the first issue
404

M ULTI -I SSUE N EGOTIATION

WITH

D EADLINES

and [ 34 , 14 ] for the second. Hence eub(1, 1, 1) = 4.725. For the latter case, b’s equilibrium offer
at t = 1 is [ 52 , 35 ] for the first issue and [1, 0] for the second one. Hence eub(1, 2, 1) = 3. Since
eub(1, 1, 1) > eub(1, 2, 1), optb(1, 1) = 1 and b plays the former strategy. Now if a is in fact
of type 1, then it accepts b’s offer at t = 1. But if a is in fact of type 2, it rejects b’s offer at t = 1
since it can get a higher utility at t = 2. An agreement therefore occurs at t = 2. Thus, the earliest
possible time of agreement is t = 1.
Now consider the case where an agent b of type i offers at t = 1 but an agreement does not
occur at this time. When b’s offer gets rejected, it knows that a is not of type optb(i, 1). Thus
the number of possible types for a is now reduced to r − 1. This happens every time b makes an
offer (i.e., every alternate time period) but it gets rejected. When negotiation reaches time period
t = 2r − 1, there is only one possible type for a. Since a knows b’s type, an agreement therefore
takes place at t = 2r − 1. However, if n < 2r − 1 then an agreement occurs at t = n (see a(i, n)
and b(i, n)). In other words, if an agreement does not occur at t = 1, then it occurs at the latest by
t = min(2r − 1, n). 
Note that the latest possible time of agreement for the asymmetric information setting is the same as
that for the symmetric information setting of Theorem 8. This is because, in the asymmetric setting,
although a knows b’s type, b is uncertain about a’s type. Also, it takes 2r − 1 time periods for b to
come to know a’s actual type. Hence, the earliest and latest time of agreement is the same for both
settings.
Theorem 15 The time complexity of computing the equilibrium offers for the package deal procedure is O(mπ̂r 3 T2 (n − T2 )) where T = min(2r − 1, n).
Proof: Let a denote the agent that offers at t = 1 and assume that n is even (the proof for odd
n is analogous). We begin with the last time period and then reason backwards. Since n is even
and agent a starts at t = 1, it is b’s turn to offer in the last time period. For t = n, the time
taken to find eub(i, j, t) (for a given i and j) is O(m) (see Equation 10). Hence, the time taken
to find eub(i, j, t) for all possible types of b (i.e., 1 ≤ j ≤ r) is O(mr). Note that, at this stage,
eub(i, t − 1) is known for 1 ≤ i ≤ r (see Equation 12).
Now consider the time period t = n − 1. Since n is even, it is a’s turn to offer at t = n − 1.
In order to find a(i, t), we first need to find ψ where ψ = opta(i, t). From Equation 16 we know
that, for a given i, the time to find opta(i, t) depends on the time taken to find eua(i, j, t) which,
in turn, depends on the time to find fa (i, j, e, t) (see Equation 19). The time taken for fa (i, j, e, t)
depends on the time taken for a(i, j, t). For a given i and a given j, the time taken to find a(i, j, t) is
the time taken by tradeoffa1. Since eub(j, t) is already known at time t, the time taken by the
function tradeoffa1 is O(mπ̂) (as Theorem 2). The time taken to find fa (i, j, e, t) is therefore
O(mπ̂). Given this, the time to find eua(i, j, t) (for a given i, j, and t) is O(mπ̂) since b’s type
is known to both agents – see Equation 19. Hence, for a given i, the time to find ψ = opta(i, t)
is O(mπ̂r). At this stage, EUB (ψ, t) is known (see the last sentence in the first paragraph of this
proof). Consequently, for a given i, the time to find a(i, t) is O(mπ̂r). Recall that b does not know
a’s type. Hence we need to determine a(i, t) for all possible types of a (i.e., for 1 ≤ i ≤ r). This
takes O(mπ̂r 2 ) time. Note that at this stage eua(i, j, t) is known for all possible values of i and all
possible values of j (where 1 ≤ i ≤ r and 1 ≤ j ≤ r).
Now consider the time period t = n − 2 when it is b’s turn to offer. The only difference between
the computation for t = n − 1 and t = n − 2 is that for the former case, the time to find eua(i, j, t)
(for a given i, j, and t) is O(mπ̂) since b’s type is known to both agents. However for the latter
405

FATIMA , W OOLDRIDGE , & J ENNINGS

case, the time to find eub(i, j, t) (for a given i, j, and t) is O(mπ̂r) since a’s type is not known to
b (see Equation 14). Consequently, for a given i, the time to find b(i, t) is O(mπ̂r 2 ). So the time to
determine b(i, t) for all possible types of b (i.e., for 1 ≤ i ≤ r) is O(mπ̂r 3 ) time. Note that at this
stage eub(i, j, t) is known for all possible values of i and all possible values of j (where 1 ≤ i ≤ r
and 1 ≤ j ≤ r).
In the same way, the time required to do all the necessary computation for each odd time period
t < n is O(mπ̂r 2 ), while that for each even time period is O(mπ̂r 3 ). Hence, the total time to find
the equilibrium offer for the first time period is O(mπ̂r 3 ( n−1
2 )). However, as noted previously, an
agreement may or may not occur in the first time period. If an agreement does not take place at t =
1, then the agents update their beliefs and compute the equilibrium offer for t = 2 with the updated
beliefs. The time to compute the equilibrium offer for t = 2 is O(mπ̂r 3 ( n−2
2 )). This process of
updating beliefs and finding the equilibrium offer is repeated at most T = min(2r − 1, n) times.
T T
3
Hence the time complexity of the package deal is ΣTi=1 O(mπ̂r 3 ( n−i
2 )) = O(mπ̂r (n − 2 ) 2 ). 
Theorem 16 The package deal procedure generates a Pareto optimal outcome.
Proof: As per Theorem 12. 
Theorem 17 For a given first mover, the package deal procedure has a unique equilibrium outcome
if ¬C3 ∨ C4 is true.
Proof: As per Theorem 9. 
5.2 The Simultaneous Procedure
Theorem 14 applies to each of the µ > 1 partitions. Hence, from Theorem 15, we know that the
T
time taken for the cth (for 1 ≤ c ≤ µ) partition is O(|Sc |π̂c r 3 ( n−T
2 ) 2 ). Hence, the time complexity
T T
3
of the simultaneous procedure is O(|Sz |π̂z r (n − 2 ) 2 ). Also, from Theorem 5, it follows that the
simultaneous procedure may not generate a Pareto optimal outcome. Finally, from Theorem 17 we
know that the simultaneous procedure has a unique equilibrium outcome if the condition C5 is true.
5.3 The Sequential Procedure
First, Theorem 14 applies to each of the µ > 1 partitions. Thus, for the sequential procedure, if
negotiation for the cth (for 1 ≤ c ≤ µ) partition starts at time tc , then it ends at the earliest at time
tc and at the latest by tc + min(2r − 1, n). Second, it follows from Theorem 15 that the time taken
for the sequential procedure is O(|Sz |π̂z r 3 (n − T2 ) T2 ). Third, the sequential procedure may not
generate a Pareto optimal outcome (see Theorem 5). Finally, the conditions for uniqueness are the
same as those for the simultaneous procedure.
5.4 The Optimal Procedure
It follows from Theorem 13 that, for each agent, the optimal procedure is the package deal. These
results are summarised in Table 4.

6. Multi-Issue Negotiation for Interdependent Issues
For the independent issues case of Section 4, an agent’s utility for issue c (for 1 ≤ c ≤ m) depends
only on its share for that issue and is independent of other issues. However, in many cases, an
406

M ULTI -I SSUE N EGOTIATION

Package deal
Earliest: 1
Latest: min(2r − 1, n)
for all m issues

Time of
agreement

Time to compute
equilibrium
Pareto optimal?
Unique equilibrium?

O(mπ̂r 3 T2 (n −

T
2 ))

WITH

D EADLINES

Simultaneous
Earliest: 1
Latest: min(2r − 1, n)
for all m issues
O(|Sz |π̂z r 3 (n −

Yes
If ¬C3 ∨ C4

T T
2)2)

Sequential
For the cth partition
tec = tsc
l
s
tc = tc + min(2r − 1, n)
for 1 ≤ c ≤ µ
O(|Sz |π̂z r 3 (n − T2 ) T2 )

No
If C5

No
If C5

Table 4: A comparison of the expected outcomes for the three multi-issue procedures for the asymmetric information setting (for the sequential procedure, tsc denotes the start time for the
cth partition, tec the earliest possible time of agreement, and tlc the latest possible time of
agreement).

agent’s utility from an issue depends not only on its share for the issue, but also on its share for
others (Klein et al., 2003). Given this, in this section we focus on such interdependent issues.
Specifically, we model interdependence between the issues as follows. Consider a package [xt , y t ].
For this package, for an agent a of type i, the utility from issue c at time t is now of the form:
(
Kic xc + Σm
j=1 χij (xc − xj ) if t ≤ n
a
t t
(20)
uic ([x , y ], t) =
0
otherwise
and that for an agent b of type i, it is:
ubic ([xt , y t ], t)

(
Kic yc + Σm
j=1 χij (yc − yj ) if t ≤ n
=
0
otherwise

(21)

where Kic denotes a constant positive real number and χij a constant real number that may be
either positive or negative. As before, an agent’s cumulative utility is the sum of its utilities from
the individual issues:
(
t
Σm
c=1 K̄ic xc if t ≤ n
(22)
Uia ([xt , y t ], t) =
0
otherwise
(
t
Σm
c=1 K̄ic yc if t ≤ n
(23)
Uib ([xt , y t ], t) =
0
otherwise
Here K̄ denotes a vector analogous to the vector K except that the individual elements of the
latter are all constant positive real numbers, while those of the former may be positive or negative.
Note that in Equations 5 and 6, all the coefficients are positive (i.e., Kic > 0 for 1 ≤ i ≤ r and
1 ≤ c ≤ m). But in Equations 22 and 23, the coefficient (K̄ic ) may be a positive or a negative real
number.
407

FATIMA , W OOLDRIDGE , & J ENNINGS

The above cumulative utility functions are linear (see Pollak, 1976; Charness & Rabin, 2002;
Sobel, 2005, for other forms of utility functions for interdependent preferences10 ). As mentioned
before, we chose the linear form for reasons of computational tractability.
In this setting the vector K̄ and the functions P a and P b are common knowledge to the negotiators. Also, each agent knows its own type, but not that of its opponent. In addition, each agent
knows r, δ, n, and m. In other words, there is symmetric uncertainty about the opponent’s utility
(as we will see in Section 6.4, the results for the asymmetric case can easily be obtained from the
following analysis for the symmetric case).
6.1 The Package Deal Procedure
For the cumulative utilities defined in Equations 22 and 23, Theorem 18 characterises the equilibrium for the package deal.
Theorem 18 For the package deal procedure, the following strategies form a sequential equilibrium. The equilibrium strategies for t = n are:

OFFER [δn−1 , 0] IF a’s TURN
a(i, n) =
ACCEPT
IF b’s TURN

OFFER [0, δn−1 ] IF b’s TURN
b(i, n) =
ACCEPT
IF a’s TURN
for 1 ≤ i ≤ r. For all preceding time periods t < n, if [xt , y t ] denotes the offer made at time t, then
the equilibrium strategies are defined as follows:

OFFER tradeoffa1(K̄, δ, eub(ψ, t), i, ψ, m, t, P a , P b ) IF a’s TURN



If offer gets rejected UPDATE BELIEFS
a(i, t) =
RECEIVE OFFER and UPDATE BELIEFS
IF b’s TURN



a
t
t
If (Ui ([x , y ], t) ≥ eua(i, t)) ACCEPT else REJECT

OFFER tradeoffb1(K̄, δ, eua(φ, t), i, φ, m, t, P a , P b ) IF b’s TURN



If offer gets rejected UPDATE BELIEFS
b(i, t) =
RECEIVE OFFER and UPDATE BELIEFS
IF a’s TURN



If (Uib (xt , y t ], t) ≥ eub(i, t)) ACCEPT else REJECT

for 1 ≤ i ≤ r. Here, ψ = opta(i, t) and φ = optb(i, t). The earliest possible time of agreement
is t = 1 and the latest possible time is t = min(2r − 1, n).
Proof: As Theorem 8. The only difference between the independent issues setting of Theorem 8
and the present interdependent issues one is in terms of the definition for cumulative utilities: in
Equations 5 and 6, all the coefficients are positive (i.e., Kic > 0 for 1 ≤ i ≤ r and 1 ≤ c ≤ m).
But in Equations 22 and 23, the coefficient (K̄ic ) may be a positive or a negative real number.
However, the greedy method (given in Theorem 1) for solving the fractional knapsack problem of
Equation 15 works for both positive and negative coefficients (Martello & Toth, 1990; Cormen et al.,
2003). Hence, the proof of Theorem 8 applies to this setting as well. 
10. Although in (Pollak, 1976; Charness & Rabin, 2002; Sobel, 2005) these forms are discussed in the context of how
an agent’s utility depends on the utility of other agent’s, they may equally well be interpreted for the case where an
agent’s utility for an issue depends on its share for other issues.

408

M ULTI -I SSUE N EGOTIATION

WITH

D EADLINES

Theorem 19 The time complexity of computing the equilibrium offers for the package deal procedure is O(mπ̂r 3 T (n − T2 )) where T = min(2r − 1, n).
Proof: As Theorem 11. Since the method for making tradeoffs is the same as that for the setting
with symmetric uncertainty and independent issues (i.e., SUI ), the time complexity is the same as
in Theorem 11. 
It is obvious that Theorems 9 and 12 extend to this setting as well.
6.2 The Simultaneous Procedure
It follows from above that all the results of Section 4.2 apply to this setting as well.
6.3 The Sequential Procedure
It also follows from above that the results of Section 4.3 apply to this setting as well.
6.4 The Optimal Procedure
It follows from Theorem 13 that the package deal remains the optimal procedure even if the issues
are interdependent. The results for this setting are the same as those in Section 4 and are summarised
in Table 3.
Finally, consider the asymmetric information setting of Section 5 but in the current context
of interdependent issues. From the above analysis for symmetric uncertainty with interdependent
issues, it is clear that the method for making tradeoffs remains the same irrespective of whether
the information is symmetric or asymmetric. Consequently, for the case of asymmetric information
with interdependent issues, we get the same results as those in Section 5.
Recall that this analysis was done for linear cumulative utilities. We now discuss how our
results would hold for more complex utility functions that are non-linear11 . For cumulative utilities
that are nonlinear, the tradeoff problem becomes a global optimization problem with a nonlinear
objective function. Due to their computational complexity, such nonlinear optimization problems
can only be solved using approximation methods (Horst & Tuy, 1996; Bar-Yam, 1997; Klein et al.,
2003). In contrast, our tradeoff problem is a linear optimization problem, the exact solution to
which can be found in polynomial time (as shown in Theorems 1 and 2). Although our results
apply to linear cumulative utilities, it is not difficult to see how they would hold for the nonlinear
case. First, the time of agreement for our case would hold for other (nonlinear) functions. This
is because this time depends not on the actual definition of the agents’ cumulative utilities but
on the information setting (i.e., whether or not the information is complete). Second, let O(ω)
denote the time complexity of TRADEOFFA 1 for nonlinear utilities for the package deal with µ = 1,
and O(ωc ) that for the cth partition. Also, let Sz denote the partition for which O(ωz ) is the
highest between all partitions. Then, we know from Theorem 11 that the time complexity of the
package deal for the setting with symmetric uncertainty is O(ωr 3 T (n− T2 )). Consequently, the time
complexity of both the simultaneous and the sequential procedures is O(ωz r 3 T (n − T2 )). Third,
while the package deal outcome for our additive cumulative utilities is Pareto optimal, the package
deal outcome for nonlinear utilities may not be Pareto optimal. This is because (as stated above)
11. Note that bilateral bargaining for which the players’ utility functions are nonlinear has been studied by Hoel (1986)
in the context of a single issue as opposed to the multi-issue case which is the focus of our study.

409

FATIMA , W OOLDRIDGE , & J ENNINGS

nonlinear optimization problems can only be solved using approximation methods while the linear
optimization problem can be solved using an exact method (as in proof of Theorem 1). Finally,
since the conditions for a unique solution depend on the actual definition of cumulative utilities, the
conditions given in Tables 1 2, 3, and 4 may not hold for other forms of utility functions.

7. Related Work
Since Schelling (1956) first noted the fact that the outcome of negotiation depends on the choice of
negotiation procedure, much research effort has been devoted to the study of different procedures
for negotiating multiple issues. For instance, Fershtman (1990) extended the model developed by
Rubinstein (1982), for splitting a single pie, to sequential negotiation for two pies. However, this
model assumes complete information, imposes an agenda exogenously, and then studies the relation
between the agenda and the outcome of the sequential bargaining game. In more detail, for two pies
of different sizes, he analyses the effect of going first on the large and the small pie.
A number of researchers have also studied negotiations with an endogenous agenda (Inderst,
2000; In & Serrano, 2003; Bac & Raff, 1996). In Inderst (2000) players have discount factors,
but no deadlines. For independent issues, this work assumes complete information and studies
three different negotiation procedures: package deal, simultaneous, and sequential negotiation with
endogenous agenda. Their main result is that the package deal is the optimal procedure and that for
each procedure there exist multiple equilibria. In and Serrano (2003) extend this work by finding
conditions under which the equilibrium becomes unique. Note that our work differs from both of
these in that we analyse negotiations with both discount factors and deadlines, which we consider
to be much more common with automated negotiations. Moreover, we do this for both independent
and interdependent issues without making the complete information assumption.
Bac and Raff (1996) also developed a model that has an endogenous agenda. They extended
the model developed by Rubinstein (1985) for single pie bargaining with incomplete information
by adding a second pie. In this model, the players have discount factors, but no deadlines. The
size of the pie is known to both agents and the discounting factor is assumed to be equal for all
the issues for both agents. Also, there is asymmetric information: one of the players knows its
own discounting factor and that of its opponent, while the other player knows its own discounting
factor, but is uncertain of its opponent’s. In more detail, this factor can take one of two values, δH
with probability x, and δL with probability 1 − x. These probabilities are common knowledge. For
this model, the authors determine the equilibrium for the package deal and the sequential procedure.
They show that, under certain conditions, the sequential procedure can be the optimal one. However,
there are three key differences between this model and ours. First, we analyse both symmetric and
asymmetric information settings, while Bac and Raff analyse only the latter. Second, the negotiators
in our model have a deadline, while in Bac and Raff they do not. Again, we believe our analysis
covers situations that often occur in automated negotiation settings. Finally, Bac and Raff focus on
independent issues, but we analyse both independent and interdependent issues.
A slightly different approach (from the above ones) was taken by Busch and Horstmann (1997).
Again, they extended the model developed by Rubinstein (1985), but by adding a preliminary period
in which the agents bargain over the agenda. The outcome of this stage is then used as the agenda for
negotiating over the issues. In this complete information model, there are two pies for bargaining.
Furthermore, these two issues become available for negotiation at different time points. The players
have discount factors or fixed time costs, but no deadlines. Since there are two issues, there are two
410

M ULTI -I SSUE N EGOTIATION

WITH

D EADLINES

possible agendas. The outcome for these two agendas is compared with that for the package deal.
Their main result is that the players may have conflicting preferences over the optimal agenda. Note
that a key difference between this model and ours is that all the issues in our model are available
from the beginning, while in their model the two issues become available at different time points.
Furthermore, Busch and Horstman assume complete information, while we do not.
From all the models mentioned above, perhaps the one that is closest to ours is the one developed
by Inderst (2000). Unlike our work, Inderst assumes complete information and independent issues.
Also, it does not model player deadlines, while we do. However, Inderst does model players’ time
preferences as discount factors. Also, just like our model, all the issues for negotiation are available
at the beginning of negotiation. In terms of results, Inderst shows that the package deal is the
optimal procedure. Our study also shows that the package deal is the optimal procedure for both
agents. Finally, our work provides a detailed analysis of the attributes of the different procedures
(such as the time of agreement, the time complexity, the Pareto optimality, and the conditions for
uniqueness), while Inderst does not.
In summary, all the aforementioned models for multi-issue negotiation differ from ours in at
least one of three major ways. The players in our model have both discount factors and deadlines,
but a general characteristic of the above models is that the players only have discount factors but no
deadlines12 . Negotiation with deadlines has been studied by Sandholm and Vulkan (1999) (in the
context of a single issue) and by Fatima et al. (2004) for the sequential procedure with µ = m. Given
this, our contribution lies firstly in finding the equilibrium for all the three procedures. Second, we
analyse both asymmetric and symmetric information settings, while previous work analyses only
the former. Third, we analyse both independent and interdependent issues while previous work
focuses primarily on independent issues. Furthermore, the existing literature does not compare the
different multi-issue procedures in terms of their attributes (viz. time complexity, Pareto optimality,
uniqueness, and time of agreement). By considering these, our study allows a more informed choice
to be made about a wider range of tradeoffs that are involved in determining which is the most
appropriate procedure.
Finally, we would like to point that in Fatima et al. (2006), we considered independent issues
and carried out the same study as we do in this work, but in a symmetric information setting with
uncertainty about the negotiation deadline (as opposed to uncertainty over the agents’ utility functions that is the focus of this work). The key result of (Fatima et al., 2006) is similar to the result of
our current work, namely that the optimal procedure in (Fatima et al., 2006) is the package deal.

8. Conclusions and Future Work
This paper studied bilateral multi-issue negotiation between self-interested agents in a wide range of
settings. Each player has time constraints in the form of deadlines and discount factors. Specifically,
we considered both independent and interdependent issues and studied the three main multi-issue
procedures for conducting such negotiations: the package deal, the simultaneous procedure, and the
sequential procedure. We determined equilibria for each procedure for two different information
settings. In the first, there is symmetric uncertainty about the opponent’s utility. In the second,
there is asymmetric uncertainty about the opponent’s utility. We analysed both settings for the
case of independent and interdependent issues. For each setting, we compared the outcomes of the
12. (Fatima et al., 2004) studies a multi-issue model with deadlines, but it focuses on determining the equilibrium for one
specific sequential procedure: the one in which each partition has a single issue.

411

FATIMA , W OOLDRIDGE , & J ENNINGS

different procedures and showed that the package deal is optimal for each agent. We then compared
the three procedures in terms of four attributes: the time complexity of the procedure, the Pareto
optimality of the equilibrium solution, the uniqueness of the equilibrium solution, and the time of
agreement (see Table 1).
In more detail, our study shows that the package deal is in fact the optimal procedure for each
party. We also showed that although the package deal may be computationally more complex than
the other two procedures, it generates Pareto optimal outcomes (unlike the other two procedures), it
has similar earliest and latest possible times of agreement as the simultaneous procedure (which is
better than the sequential procedure), and that it (like the other two procedures) generates a unique
outcome only under certain conditions (which we defined).
There are several interesting directions for extending the current analysis. First, in this work, we
modelled the players’ time preferences in the form of discount factors which is the most common
basis for such analysis. However, existing literature (Busch & Horstman, 1997) shows that the
outcome for negotiation with discount factors can differ from the outcome for negotiation with
fixed time costs. It will, therefore, be interesting to extend our results to negotiations with fixed
time costs. Second, our present work analysed the setting with uncertainty about utility functions.
Generalisation of our results to scenarios with other sources of uncertainties such as the agents’
discount factors is another direction for future work.
Acknowledgements
We are grateful to Sarit Kraus for her detailed comments on earlier versions of this paper. We also
thank the anonymous referees; their comments helped us to substantially improve the readability
and accuracy of the paper.

412

M ULTI -I SSUE N EGOTIATION

WITH

D EADLINES

Appendix A. Summary of Notation
a, b The two negotiating agents.
n Negotiation deadline for both agents.
m Total number of issues.
S The set of m issues.
Sc A subset of S (Sc ⊆ S).
M Number of issues in the largest partition.
µ Number of partitions for the simultaneous and sequential procedures.
δc Discount factor for issue c (for 1 ≤ c ≤ m).
δ An m element vector that represents the discount factor for the m issues.
xt An m element vector that denotes a’s share for each of the m issues at time t.
y t An m element vector that denotes b’s share for each of the m issues at time t.
[xt , y t ] The package offered at time t.
atc Agent a’s share for issue c in the equilibrium offer for time period t.
btc Agent b’s share for issue c in the equilibrium offer for time period t.
at An m element vector that denotes a’s share for each of the m issues in equilibrium at time t.
bt An m element vector that denotes b’s share for each of the m issues in equilibrium at time t.
[at , bt ] The equilibrium package offered at time t.
Uia Cumulative utility function for agent a of type i.
Uib Cumulative utility function for agent b of type i.
ua(t) Agent a’s cumulative utility from the equilibrium offer for time t.
ub(t) Agent b’s cumulative utility from the equilibrium offer for time t.
a(i, j, t) Agent a’s equilibrium offer for time t if a is of type i assuming b is type j.
b(i, j, t) Agent b’s equilibrium offer for time t if b is of type i assuming a is type j.
a(i, t) Equilibrium strategy for an agent a of type i at time t.
b(i, t) Equilibrium strategy for an agent b of type i at time t.
eua(i, t) Cumulative utility that an agent a of type i expects to get from b’s equilibrium offer at
time t (i.e., a is the receiving agent and b the offering agent at t).
413

FATIMA , W OOLDRIDGE , & J ENNINGS

eub(i, t) Cumulative utility that an agent b of type i expects to get from a’s equilibrium offer at
time t (i.e., b is the receiving agent and a the offering agent at t).
eua(i, j, t) Agent a’s expected cumulative utility from its equilibrium offer for time t if a is type i
and assuming that b is type j.
eub(i, j, t) Agent b’s expected cumulative utility from its equilibrium offer for time t if b is type i
and assuming a is type j.
r Number of types for agent a (and also the number of types for agent b).
Tta Set of possible types for agent a at time t.
Ttb Set of possible types for agent b at time t.
P a The probability distribution function for ka .
P b The probability distribution function for kb .
K A vector of r vectors each element of which is in turn a vector of m positive reals.
Spij A subset of S (Spij ⊆ S where i denotes a’s type and j that of b) such that |Spij | > 1 and
Kid
Kic
.
=K
∀c,d∈Spij K
jc
jd
tradeoffa Agent a’s function for making tradeoffs in the complete information setting.
tradeoffb Agent b’s function for making tradeoffs in the complete information setting.
tradeoffa1 Agent a’s function for making tradeoffs in the four incomplete information settings:
SUI , SUD , AUI , AUD .
tradeoffb1 Agent b’s function for making tradeoffs in the four incomplete information settings:
SUI , SUD , AUI , AUD .
π̂ Maximum number of packages that tradeoffa1 (or tradeoffb1) will have to search to find
the one that maximises a’s (or b’s) expected cumulative utility (considering all possible types
of a and b).
paij
t The set of all possible packages that tradeoffa1 can return at time t (i denotes a’s type and
j that of b).
pbij
t The set of all possible packages that tradeoffb1 can return at time t (i denotes a’s type and
j that of b).

References
Bac, M., & Raff, H. (1996). Issue-by-issue negotiations: the role of information and time preference.
Games and Economic Behavior, 13, 125–134.
Bar-Yam, Y. (1997). Dynamics of Complex Systems. Addison Wesley.
414

M ULTI -I SSUE N EGOTIATION

WITH

D EADLINES

Binmore, K., Osborne, M. J., & Rubinstein, A. (1992). Noncooperative models of bargaining. In
Aumann, R. J., & Hart, S. (Eds.), Handbook of Game theory with Economic Applications,
Vol. 1, pp. 179–225. North-Holland.
Busch, L. A., & Horstman, I. J. (1997). Bargaining frictions, bargaining procedures and implied
costs in multiple-issue bargaining. Economica, 64, 669–680.
Charness, G., & Rabin, M. (2002). Understanding social preferences with simple tests. The Quarterly Journal of Economics, 117(3), 817–869.
Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2003). An introduction to algorithms.
The MIT Press, Cambridge, Massachusetts.
Faratin, P., Sierra, C., & Jennings, N. R. (2002). Using similarity criteria to make trade-offs in
automated negotiations. Artificial Intelligence Journal, 142(2), 205–237.
Fatima, S. S., Wooldridge, M., & Jennings, N. R. (2002). The influence of information on negotiation equilibrium. In Agent Mediated Electronic Commerce IV, Designing Mechanisms and
Systems, No. 2531 in LNCS, pp. 180 – 193. Springer Verlag.
Fatima, S. S., Wooldridge, M., & Jennings, N. R. (2004). An agenda based framework for multiissue negotiation. Artificial Intelligence Journal, 152(1), 1–45.
Fatima, S. S., Wooldridge, M., & Jennings, N. R. (2006). On efficient procedures for multi-issue negotiation. In Proceedings of the Eighth International Workshop on Agent Mediated Electronic
Commerce (AMEC), pp. 71–85, Hakodate, Japan.
Fershtman, C. (1990). The importance of the agenda in bargaining. Games and Economic Behavior,
2, 224–238.
Fershtman, C. (2000). A note on multi-issue two-sided bargaining: bilateral procedures. Games and
Economic Behavior, 30, 216–227.
Fershtman, C., & Seidmann, D. J. (1993). Deadline effects and inefficient delay in bargaining with
endogenous commitment. Journal of Economic Theory, 60(2), 306–321.
Fishburn, P. C. (1988). Normative thoeries of decision making under risk and uncertainty. In
Bell, D. E., Raiffa, H., & Tversky, A. (Eds.), Decision making: Descriptive, normative, and
prescriptive interactions. Cambridge University Press.
Fisher, R., & Ury, W. (1981). Getting to yes: Negotiating agreement without giving in. Houghton
Mifflin, Boston.
Fudenberg, D., Levine, D., & Tirole, J. (1985). Infinite horizon models of bargaining with one sided
incomplete information. In Roth, A. (Ed.), Game Theoretic Models of Bargaining. University
of Cambridge Press, Cambridge.
Fudenberg, D., & Tirole, J. (1983). Sequential bargaining with incomplete information. Review of
Economic Studies, 50, 221–247.
Harsanyi, J. C. (1977). Rational behavior and bargaining equilibrium in games and social situations. Cambridge University Press.
Harsanyi, J. C., & Selten, R. (1972). A generalized Nash solution for two-person bargaining games
with incomplete information. Management Science, 18(5), 80–106.
415

FATIMA , W OOLDRIDGE , & J ENNINGS

Hoel, M. (1986). Perfect equilibria in sequential bargaining games with nonlinear utility functions.
Scandinavian Journal of Economics, 88(2), 383–400.
Horst, R., & Tuy, H. (1996). Global optimazation: Deterministic approaches. Springer.
In, Y., & Serrano, R. (2003). Agenda restrictions in multi-issue bargaining (ii): unrestricted agendas.
Economics Letters, 79, 325–331.
Inderst, R. (2000). Multi-issue bargaining with endogenous agenda. Games and Economic Behavior, 30, 64–82.
Keeney, R., & Raiffa, H. (1976). Decisions with Multiple Objectives: Preferences and Value Tradeoffs. New York: John Wiley.
Klein, M., Faratin, P., Sayama, H., & Bar-Yam, Y. (2003). Negotiating complex contracts. IEEE
Intelligent Systems, 8(6), 32–38.
Kraus, S. (2001). Strategic negotiation in multi-agent environments. The MIT Press, Cambridge,
Massachusetts.
Kraus, S., Wilkenfeld, J., & Zlotkin, G. (1995). Negotiation under time constraints. Artificial
Intelligence Journal, 75(2), 297–345.
Kreps, D. M., & Wilson, R. (1982). Sequential equilibrium. Econometrica, 50, 863–894.
Lax, D. A., & Sebenius, J. K. (1986). The manager as negotiator: Bargaining for cooperation and
competitive gain. The Free Press, New York.
Livne, Z. A. (1979). The role of time in negotiation. Ph.D. thesis, Massachusetts Institute of
Technology.
Lomuscio, A., Wooldridge, M., & Jennings, N. R. (2003). A classification scheme for negotiation
in electronic commerce. International Journal of Group Decision and Negotiation, 12(1),
31–56.
Ma, C. A., & Manove, M. (1993). Bargaining with deadlines and imperfect player control. Econometrica, 61, 1313–1339.
Maes, P., Guttman, R., & Moukas, A. (1999). Agents that buy and sell. Communications of the
ACM, 42(3), 81–91.
Martello, S., & Toth, P. (1990). Knapsack problems: Algorithms and computer implementations.
John Wiley and Sons. Chapter 2.
Mas-Colell, A., Whinston, M. D., & Green, J. R. (1995). Microeconomic Theory. Oxford University
Press.
Muthoo, A. (1999). Bargaining Theory with Applications. Cambridge University Press.
Neumann, J. V., & Morgenstern, O. (1947). Theory of Games and Economic Behavior. Princeton:
Princeton University Press.
Osborne, M. J., & Rubinstein, A. (1990). Bargaining and Markets. Academic Press, San Diego,
California.
Osborne, M. J., & Rubinstein, A. (1994). A Course in Game Theory. The MIT Press.
Pollak, R. A. (1976). Interdependent preferences. American Economic Review, 66(3), 309–320.
416

M ULTI -I SSUE N EGOTIATION

WITH

D EADLINES

Pruitt, D. G. (1981). Negotiation Behavior. Academic Press.
Raiffa, H. (1982). The Art and Science of Negotiation. Harvard University Press, Cambridge, USA.
Rosenschein, J. S., & Zlotkin, G. (1994). Rules of Encounter. MIT Press.
Rubinstein, A. (1982). Perfect equilibrium in a bargaining model. Econometrica, 50(1), 97–109.
Rubinstein, A. (1985). A bargaining model with incomplete information about time preferences.
Econometrica, 53, 1151–1172.
Sandholm, T. (2000). Agents in electronic commerce: component technologies for automated negotiation and coalition formation.. Autonomous Agents and Multi-Agent Systems, 3(1), 73–96.
Sandholm, T., & Vulkan, N. (1999). Bargaining with deadlines. In AAAI-99, pp. 44–51, Orlando,
FL.
Schelling, T. C. (1956). An essay on bargaining. American Economic Review, 46, 281–306.
Schelling, T. C. (1960). The strategy of conflict. Oxford University Press.
Sobel, J. (2005). Interdependent preferences and reciprocity. Journal of Economic Literature, XLIII,
392–436.
Stahl, I. (1972). Bargaining Theory. Economics Research Institute, Stockholm School of Economics, Stockholm.
van Damme, E. (1983). Refinements of the Nash equilibrium concept. Berlin:Springer-Verlag.
Varian, H. R. (2003). Intermediate Microeconomics. W. W. Norton and Company.
Young, O. R. (1975). Bargaining: Formal theories of negotiation. Urbana: University of Illinois
Press.

417

Journal of Artificial Intelligence Research 27 (2006) 505–549

Submitted 06/06; published 12/06

Resource Allocation Among Agents with MDP-Induced
Preferences
Dmitri A. Dolgov

ddolgov@ai.stanford.edu

Technical Research Department (AI & Robotics Group)
Toyota Technical Center
2350 Green Road
Ann Arbor, MI 48105, USA

Edmund H. Durfee

durfee@umich.edu

Electrical Engineering and Computer Science
University of Michigan
2260 Hayward St.
Ann Arbor, MI 48109, USA

Abstract
Allocating scarce resources among agents to maximize global utility is, in general, computationally challenging. We focus on problems where resources enable agents to execute
actions in stochastic environments, modeled as Markov decision processes (MDPs), such
that the value of a resource bundle is defined as the expected value of the optimal MDP
policy realizable given these resources. We present an algorithm that simultaneously solves
the resource-allocation and the policy-optimization problems. This allows us to avoid explicitly representing utilities over exponentially many resource bundles, leading to drastic
(often exponential) reductions in computational complexity. We then use this algorithm
in the context of self-interested agents to design a combinatorial auction for allocating resources. We empirically demonstrate the effectiveness of our approach by showing that
it can, in minutes, optimally solve problems for which a straightforward combinatorial
resource-allocation technique would require the agents to enumerate up to 2100 resource
bundles and the auctioneer to solve an NP-complete problem with an input of that size.

1. Introduction
The problem of resource allocation is ubiquitous in many diverse research fields such as
economics, operations research, and computer science, with applications ranging from decentralized scheduling (e.g., Wellman, Walsh, Wurman, & MacKie-Mason, 2001) and network routing (e.g., Feldmann, Gairing, Lucking, Monien, & Rode, 2003) to transportation
logistics (e.g., Sheffi, 2004; Song & Regan, 2002) and bandwidth allocation (e.g., McMillan,
1994; McAfee & McMillan, 1996), just to name a few. The core question in resource allocation is how to distribute a set of scarce resources among a set of agents (either cooperative or
self-interested) in a way that maximizes some measure of global utility, with social welfare
(sum of agents’ utilities) being one of the most popular criteria.
In many domains, an agent’s utility for obtaining a set of resources is defined by what
the agent can accomplish using these resources. For example, the value of a vehicle to a
delivery agent is defined by the additional revenue that the agent can obtain by using the
vehicle. However, to figure out how to best utilize a resource (or set of resources), an agent
c
2006
AI Access Foundation. All rights reserved.

Dolgov & Durfee

Available
Resources

Available
Actions

Resource-Allocation
Problem

Planning Problem
(MDP)

Utility Function on
Resources

Best Plan &
Its Payoff

Figure 1: Dependency Cycle: To formulate their planning problems, the agents need to
know what resources they will get, but their utility functions, which define the
input to the resource-allocation problem, depend on the solutions to the planning
problems.

often must solve a non-trivial planning problem because actions might have long-term, nondeterministic effects. Therefore, an agent’s value for a set of resources is defined by a solution
to a planning problem, but to formulate its planning problem the agent needs to know what
resources it will obtain. This leads to cyclic dependencies (depicted in Figure 1), wherein the
input to the resource allocation problem depends on the solution to the planning problem,
and vice versa. Unfortunately, for anything but the simplest domains, neither the resourceallocation nor the planning problem can be solved in closed form, making it impossible to
obtain parameterized solutions.
Our focus in this paper is on solving the interdependent problems of resource allocation
and stochastic planning. The main question we consider is how to allocate the resources in
a way that maximizes the social welfare of the agents when the utility function of each agent
is defined by a Markov decision process (Puterman, 1994) whose action set is parameterized
by the resources. In this paper, we specifically focus on non-consumable resources (such
as vehicles) that enable actions, but are not themselves consumed during action execution.
We briefly mention the case of consumable resources in Section 6, and refer to the work by
Dolgov (2006) for a more detailed treatment.
We assume that the agents’ MDPs are weakly-coupled, meaning the agents only interact
through the resources, and once the resources are allocated, the transition and reward
functions of their MDPs are independent. Our model of weakly-coupled MDPs connected
via shared resources is similar to that of Meuleau, Hauskrecht, Kim, Peshkin, Kaelbling,
Dean, and Boutilier (1998) and Benazera, Brafman, Meuleau, and Hansen (2005), but
differs in that we further assume that resources are only allocated once, prior to any actions
being taken. While this “one-shot” allocation assumption limits our approach somewhat,
it also allows our approach to apply more broadly to non-cooperative settings (without
this assumption, the game-theoretic analysis of agents’ interactions is significantly more
complex). More importantly, it allows us to avoid the state space explosion (due to including
resource information in the MDP states), which limits that other work to finding only
approximately optimal solutions for non-trivial problems.
The main result presented in this paper is thus a new algorithm that, under the above
conditions, optimally solves the resource-allocation and the policy-optimization problems
simultaneously. By considering the two problems together, it sidesteps the dependency cycle
506

Resource Allocation Among Agents with MDP-Induced Preferences

mentioned above, which allows us to avoid an explicit representation of utility functions
on resource bundles, leading to an exponential reduction in complexity over combinatorial
resource allocation with flat utility functions. We empirically demonstrate that the resulting
algorithm scales well to finding optimal solutions for problems involving numerous agents
and resources.
Our algorithm can be viewed as contributing a new approach for dealing with the computational complexity of resource allocation in domains with complex utility functions that
are not linearly decomposable in the resources (due to the effects of substitutability and
complementarity). In such combinatorial allocation problems, finding an optimal allocation is NP-complete in the (often exponentially large) space of resource bundles (Rothkopf,
Pekec, & Harstad, 1998). Previous approaches for addressing the complexity have included
determining classes of utility functions that lead to tractable problems (as surveyed by
de Vries & Vohra, 2003), iterative algorithms for resource allocation and preference elicitation (as surveyed by Sandholm & Boutilier, 2006), and concise languages for expressing
agents’ preferences (Sandholm, 1999; Nisan, 2000; Boutilier & Hoos, 2001; Boutilier, 2002).
The novelty of our approach with respect to these is that it explicitly embraces the underlying processes that define the agents’ utility functions, for cases where these processes can
be modeled as resource-parameterized MDPs. By doing so, not only does our approach use
such MDP-based models as a concise language for agents’ utility functions, but more importantly, it directly exploits the structure in these models to drastically reduce computational
complexity by simultaneously solving the planning and resource-allocation problems.
In the context of cooperative agents, our approach can be viewed as a way of solving
weakly-coupled multiagent MDPs, where agents’ transition and reward functions are independent, but the space of joint actions is constrained, as, for example, in the models used
by Singh and Cohn (1998) or Meuleau et al. (1998). From that perspective, the concept of
resources can be viewed as a compact way of representing the interactions between agents,
similarly to the model used by Bererton, Gordon, and Thrun (2003); however, our work
differs in a number of assumptions. Moreover, our algorithms can be easily modified to work
with models where the constraints on the joint actions are modeled directly (for example,
via SAT formulas).
For non-cooperative agents, we apply our resource-allocation algorithm to the mechanismdesign problem (e.g., Mas-Colell, Whinston, & Green, 1995), where the goal is to allocate
resources among the agents in a way that maximizes the social welfare, given that each
participating agent is selfishly maximizing its own utility. For domains with self-interested
agents with complex preferences that exhibit combinatorial effects between the resources,
combinatorial auctions (e.g., de Vries & Vohra, 2003) are often used for resource-allocation.
The Generalized Vickrey Auction (GVA) (MacKie-Mason & Varian, 1994), which is an extension of Vickrey-Clarke-Groves (VCG) mechanisms (Vickrey, 1961; Clarke, 1971; Groves,
1973) to combinatorial auctions, is particularly attractive because of its nice analytical
properties (as described in Section 4.1). We develop a variant of a VCG auction, where
agents submit their resource-parameterized MDPs as bids, and the auctioneer simultaneously solves the resource-allocation and the policy-optimization problems, thus retaining the
compact representation of agents’ preferences throughout the process. We describe extensions to the mechanism for distributing the computation and for encoding MDP information
to reduce the revelation of private information.
507

Dolgov & Durfee

The remainder of this paper proceeds as follows. After a brief review of MDPs in
Section 2, we present (in Section 3) our model of a decision-making agent: the resourceparameterized MDP with capacity constraints. We analyze the problem of optimal policy
formulation in such a resource-parameterized capacity-constrained MDP, study its properties, and present a solution algorithm, based on the formulation of this (NP-complete)
problem as a mixed integer program.
With these building blocks, we move to the multiagent setting and present our main
result, the algorithm for simultaneously allocating resources and planning across agents
(Section 4). Based on that algorithm, we then design a combinatorial auction for allocating
resources among self-interested agents. We describe a distributed implementation of the
mechanism, and discuss techniques for preserving information privacy. In Section 5, we
analyze the computational efficiency of our approach, empirically demonstrating exponential reductions in computational complexity, compared to a straightforward combinatorial
resource-allocation algorithm with flat utility functions. Finally, in Section 6, we conclude
with a discussion of possible generalizations and extensions to our approach. For conciseness
and better readability, all proofs and some generalizations are deferred to appendices.

2. Markov Decision Processes
We base our model of agents’ decision problems on infinite-horizon fully-observable MDPs
with the total expected discounted reward optimization criterion (although our results are
also applicable to other classes of MDPs, such as MDPs with the average per-step rewards).
This section introduces our notation and assumptions, and serves as a brief overview of the
basic MDP results (see, for example, the text by Puterman (1994) for a detailed discussion
of the material in this section).
A classical single-agent, unconstrained, stationary, fully-observable MDP can be defined
as a 4-tuple hS, A, p, ri, where:
• S is a finite set of states the agent can be in.
• A is a finite set of actions the agent can execute.
• p : S × A × S 7→ [0, 1] defines the transition function. The probability that the agent
goes to state σ ∈ S upon execution of action a ∈ A in state s ∈ S is p(σ|s, a).
We
P assume that, for any action, the corresponding transition matrix is stochastic:
σ p(σ|s, a) = 1 ∀s ∈ S, a ∈ A.
• r : S × A 7→ R defines the reward function. The agent obtains a reward of r(s, a) if it
executes action a ∈ A in state s ∈ S. We assume the rewards are bounded.
In a discrete-time fully-observable MDP, at each time step, the agent observes the current
state of the system and chooses an action according to its policy. A policy is said to be
Markovian (or history-independent) if the choice of action does not depend on the history
of states and actions encountered in the past, but rather only on the current state and
time. If, in addition to that, the policy does not depend on time, it is called stationary. By
definition, a stationary policy is always Markovian. A deterministic policy always prescribes
the execution of the same action in a state, while a randomized policy chooses actions
according to a probability distribution.
508

Resource Allocation Among Agents with MDP-Induced Preferences

Following the standard notation (Puterman, 1994), we refer to different classes of policies
as Πxy , where x = {H, M, S} specifies whether a policy is History-dependent, Markovian,
or Stationary, and y = {R, D} specifies whether the policy is Randomized or Deterministic
(e.g., the class of stationary deterministic policies is labeled ΠSD ). Obviously, ΠHy ⊃ ΠMy ⊃
ΠSy and ΠxR ⊃ ΠxD , with history-dependent randomized policies ΠHR and stationary
deterministic policies ΠSD being the most and the least general, respectively.
A stationary randomized policy is thus a mapping of states to probability distributions
over actions: π : S × A 7→ [0, 1], where π(s, a) defines the probability that action a is
executed in state s. A stationary deterministic policy can be viewed as a degenerate case
of a randomized policy for which there is only one action for each state that has a nonzero
probability of being executed.
In an unconstrained discounted MDP, the goal is to find a policy that maximizes the
total expected discounted reward over an infinite time horizon:1
∞
hX
i
Uγ (π, α) = E
(γ)t rt (π, α) ,
(1)
t=0

where γ ∈ [0, 1) is the discount factor (a unit reward at time t + 1 is worth the same to the
agent as a reward of γ at time t), rt is the (random) reward the agent receives at time t,
whose distribution depends on the policy π and the initial distribution over the state space
α : S 7→ [0, 1].
One of the most important results in the theory of MDPs states that, for an unconstrained discounted MDP with the total expected reward optimization criterion, there always exists an optimal policy that is stationary, deterministic, and uniformly optimal, where
the latter term means that the policy is optimal for all distributions over the starting state.2
There are several commonly-used ways of finding the optimal policy, and central to all of
them is the concept of a value function of a policy, vπ : S 7→ R, where vπ (s) is the expected
cumulative value of the reward the agent would receive if it started in state s and behaved
according to policy π. For a given policy π, the value of every state is the unique solution
to the following system of |S| linear equations:
X
X
vπ (s) =
r(s, a)π(s, a) + γ
p(σ|s, a)vπ (σ),
∀s ∈ S.
(2)
a

σ

To find the optimal policy, it is handy to consider the optimal value function v ∗ : S 7→ R,
where v ∗ (s) represents the value of state s, given that the agent behaves optimally. The
optimal value function satisfies the following system of |S| nonlinear equations:
h
i
X
v ∗ (s) = max r(s, a) + γ
p(σ|s, a)v ∗ (σ) ,
∀s ∈ S.
(3)
a

σ

v∗,

Given the optimal value function
an optimal policy is to simply act greedily with respect
∗
to v (with any method of tie-breaking in case of multiple optimal actions):
h
i
(
P
1 if a ∈ arg maxa r(s, a) + γ σ p(σ|s, a)v ∗ (σ) ,
π(s, a) =
(4)
0 otherwise.
1. Notation: here and below (x)y is an exponent, while xy is a superscript.
2. Uniform optimality of policies is the reason why α is not included as a component of a textbook MDP.

509

Dolgov & Durfee

One of the possible ways of solving for the optimal value function is to formulate the
nonlinear system (3) as a linear program (LP) with |S| optimization variables v(s) and
|S||A| constraints:
X
min
α(s)v(s)
s

subject to:

(5)

v(s) ≥ r(s, a) + γ

X

p(σ|s, a)v(σ),

∀s ∈ S, a ∈ A,

σ

where α is an arbitrary constant vector with |S| positive components (α(s) > 0 ∀s ∈ S).3
In many problems (including the ones that are the focus of this paper), it is very useful to consider the equivalent dual LP with |S||A| optimization variables x(s, a) and |S|
constraints:4
XX
r(s, a)x(s, a)
max
x

s

a

subject to:
X
XX
x(σ, a) − γ
x(s, a)p(σ|s, a) = α(σ),
a

s

(6)
∀σ ∈ S;

a

x(s, a) ≥ 0

∀s ∈ S, a ∈ A.

The optimization variables x(s, a) are often called the visitation frequencies or the occupation measure of a policy. If we think of α as the initial probability distribution, then x(s, a)
can be interpreted
P as the total expected number of times action a is executed in state s.
Then, x(s) = a x(s, a) gives the total expected flow through state s, and the constraints
in the above LP can be interpreted as the conservation of flow through each of the states.
An optimal policy can be computed from a solution to the dual LP as:
x(s, a)
,
(7)
π(s, a) = P
a x(s, a)
P
where non-negativity of α guarantees that a x(s, a) > 0 ∀s ∈ S. In general, this appears to
lead to randomized policies. However, a bounded LP with n constraints always has a basic
feasible solution (e.g., Bertsimas & Tsitsiklis, 1997), which by definition has no more than
n non-zero components. If α is strictly positive, a basic feasible solution to the LP (7) will
have precisely |S| nonzero components (one for each state), which guarantees the existence
of an optimal deterministic policy. Such a policy can be easily obtained by most LP solvers
(e.g., simplex will always produce solutions that map to deterministic policies).
Furthermore, as mentioned above, for unconstrained discounted MDPs, there always
exist policies that are uniformly optimal (optimal for all initial distributions). The above
dual LP (6) yields uniformly optimal policies if a strictly positive α is used. However, the
3. The overloading of α as the objective function coefficients here and the initial probability distribution
of an MDP earlier is intentional and is explained shortly.
4. Note that some authors (e.g., Altman, 1996) prefer the opposite convention, where (6) is called the dual,
and (5), the primal.

510

Resource Allocation Among Agents with MDP-Induced Preferences

solution (x) to the dual LP retains its interpretation as the expected number of times state s
is visited and action a is executed only for the initial probability distribution α that was
used in the LP.
The main benefit of the dual LP (6) is manifested in constrained MDPs (Altman, 1999;
Kallenberg, 1983; Heyman & Sobel, 1984), where each action, in addition to producing a
reward r(s, a), also incurs a vector of costs ηk (s, a) : S × A 7→ R ∀k ∈ [1..K]. The problem then is to maximize the expected reward, subject to constraints on the expected costs.
Constrained models of this type arise in many domains, such as telecommunication applications (e.g., Ross & Chen, 1988; Ross & Varadarajan, 1989), where it is often desirable to
maximize expected throughput, subject to conditions on the average delay. Such problems,
where constraints are imposed on the expected costs, can be solved in polynomial time
using linear programming by simply augmenting the dual LP (6) with the following linear
constraints:
XX
ηk (s, a)x(s, a) ≤ ηbk ,
∀k ∈ [1..K],
(8)
s

a

where ηbk is the upper bound on the expected cost of type k. The resulting constrained MDP
differs from the standard unconstrained MDP: in particular, deterministic policies are no
longer optimal, and uniformly optimal policies do not, in general, exist for such problems
(Kallenberg, 1983).
For the same reason of being easily augmentable with constraints, the dual LP (6) also
forms the basis for our approach. However, the constraints that arise in resource-allocation
problems that are our focus in this paper are very different from the linear constraints in (8),
leading to different optimization problems with different properties and requiring different
solution techniques (as described in more detail in Section 3).
We conclude the background section by introducing a simple unconstrained MDP that
will serve as the basis for a running example, to which we will refer throughout the rest of
this paper.
Example 1 Consider a simple delivery domain, depicted in Figure 2, where the agent can
obtain rewards for delivering furniture (action a1 ) or delivering appliances (action a2 ).
Delivering appliances produces higher rewards (as shown on the diagram), but it does more
damage to the delivery vehicle. If the agent only delivers furniture, the damage to the vehicle
is negligible, whereas if the agent delivers appliances, the vehicle is guaranteed to function
reliably for the first year (state s1 ), but after that (state s2 ) has a 10% probability of failure,
per year. The vehicle can be serviced (action a3 ), resetting its condition, but at the expense
of lowering profits. If the truck does break (state s3 ), it can be repaired (action a4 ), but with
a more significant negative impact on profits. We will assume a discount factor γ = 0.9.
The optimal value function is v ∗ (s1 ) ≈ 95.3, v ∗ (s2 ) ≈ 94.7, v ∗ (s3 ) ≈ 86.7, and the
corresponding optimal occupation measure (assuming a uniform α) is the following (listing
only the non-zero elements): x(s1 , a2 ) ≈ 4.9, x(s2 , a3 ) ≈ 4.8, x(s3 , a4 ) ≈ 0.3. This maps to
the optimal policy that dictates that the agent is to start by delivering appliances (action a2
in state s1 ), then service the vehicle after the first year (action a3 in state s2 ), and fix the
vehicle if it ever gets broken (a4 in s3 ) (the latter has zero probability of happening under
this policy if the agent starts in state s1 or s2 ).

511

Dolgov & Durfee

a1: deliver furniture
p=1
r=5

s1
(initial)

a4: fix truck
p=1
r=1

a1: deliver furniture
p=1
r=5

a2: deliver appliances
p=1
r=10
a3: service truck
p=1
r=9

s2
(2nd year)
a2: deliver appliances
p=0.9
r=10
p=0.1
r=10

s3
(truck broken)

Figure 2: Unconstrained MDP example for a delivery domain. Transition probabilities (p)
and rewards (r) are shown on the diagram. Actions not shown result in transition
to same state with no reward. There is also a noop action a0 that corresponds to
doing nothing; it does not change state and produces zero reward.

3. Agent Model: Resource-Parameterized MDP
In this section, we introduce our model of the decision-making agent, and describe the
single-agent stochastic policy-optimization problem that defines the agent’s preferences over
resources. We show that, for this single-agent problem, formulated as an MDP whose
action set is parameterized on the resources available to the agent, stationary deterministic
policies are optimal, but uniformly optimal policies do not, in general, exist. We also show
that the problem of finding optimal policies is NP-complete. Finally, we present a policyoptimization algorithm, based on a formulation of the problem as a mixed integer linear
program (MILP).
We model the agent’s resource-parameterized MDP as follows. The agent has a set of
actions that are potentially executable, and each action requires a certain combination of
resources. To capture local constraints on the sets of resources an agent can use, we use
the concept of capacities: each resource has capacity costs associated with it, and each
agent has capacity constraints. For example, a delivery company needs vehicles and loading
equipment (resources to be allocated) to make its deliveries (execute actions). However,
all equipment costs money and requires manpower to operate it (the agent’s local capacity
costs). Therefore, the amount of equipment the agent can acquire and successfully utilize
is constrained by factors such as its budget and limited manpower (agent’s local capacity
bounds). This two-layer model with capacities and resources represented separately might
seem unnecessarily complex (why not fold them together or impose constraints directly
on resources?), but the separation becomes evident and useful in the multiagent model
discussed in Section 4. We emphasize the difference here: resources are the items being
allocated among the agents, while capacities define the inherent limitations of an individual
agent on what combinations of resources it can usefully possess.
The agent’s optimization problem is to choose a subset of the available resources that
does not violate its capacity constraints, such that the best policy feasible under that bundle
of resources yields the highest utility. In other words, the single-agent problem analyzed in
512

Resource Allocation Among Agents with MDP-Induced Preferences

this section has no constraints on the total resource amounts (they are introduced in the
multiagent problem in the next section), and the constraints are only due to the agent’s
capacity limits. Adding limited resource amounts to the single-agent model would be a very
simple matter, since such constraints can be handled with a simple pruning of the agent’s
action space. Further, note that without capacity constraints, the single-agent problem
would be trivial, as it would always be optimal for the agent to simply take all resources
that are of potential use. However, in the presence of capacity constraints, we face a problem
that is similar to the cyclic dependency in Figure 1: the resource-selection problem requires
knowing the values of all resource bundles, which are defined by the planning problem, and
the planning problem is ill-defined until a resource bundle is chosen. In this section, we
focus on the single-agent problem of selecting an optimal subset of resources that satisfies
the agent’s capacity constraints and assume the agent has no value for acquiring additional
resources that exceed its capacity bounds.
The resources in the model outlined above are non-consumable, i.e., actions require
resources but do not consume them during execution. As mentioned in the Introduction,
in this work we focus only on non-consumable resources, and only briefly outline the case
of consumable resources in Section 6.
We can model the agent’s optimization problem as an n-tuple hS, A, p, r, O, ρ, C, κ, κ
b, αi:
• hS, A, p, ri are the standard components of an MDP, as defined earlier in Section 2.
• O is the set of resources (e.g., O = {production equipment, vehicle, . . .}). We will use
o ∈ O to refer to a resource type.
• ρ : A × O 7→ R is a function that specifies the resource requirements of all actions;
ρ(a, o) defines how much of resource o ∈ O action a ∈ A needs to be executable (e.g.,
ρ(a, vehicle) = 1 means that action a requires one vehicle).
• C is the set of capacities of our agent (e.g., C = {space, money, manpower, . . .}). We
will use c ∈ C to refer to a capacity type.
• κ : O ×C 7→ R is a function that specifies the capacity costs of resources; κ(o, c) defines
how much of capacity c ∈ C a unit of resource o ∈ O requires (e.g., κ(vehicle, money) =
$50000 defines the monetary cost of a vehicle, while κ(vehicle, manpower) = 2 means
that two people are required to operate the vehicle).
• κ
b : C 7→ R specifies the upper bound on the capacities; κ
b(c) gives the upper bound
on capacity c ∈ C (e.g., κ
b(money) = $1,000,000 defines the budget constraint, and
κ
b(manpower) = 7 specifies the size of the workforce).
• α : S 7→ R is the initial probability distribution; α(s) is the probability that the agent
starts in state s.
Our goal is to find a policy π that yields the highest expected reward, under the conditions that the resource requirements of that policy do not exceed the capacity bounds of
513

Dolgov & Durfee

the agent. In other words, we have to solve the following mathematical program:5
max Uγ (π, α)
π

subject to:
n
X
X
o
κ(o, c) max ρ(a, o)H
π(s, a) ≤ κ
b(c),
o

a

(9)
∀c ∈ C,

s

where H is the Heaviside “step” function of a nonnegative argument, defined as:
(
0 z = 0,
H(z) =
1 z > 0.
The constraint in (9) can be interpreted as follows. The argument of H is nonzero if
theP
policy π assigns a nonzero probability to using action a in at least one state. Thus,
function
H( s π(s, a)) serves as an indicator

	 us whether the agent plans to use
P that tells
action a in its policy, and max ρ(a, o)H( s π(s, a)) tells us how much of resource o the
agent needs for its policy. We take a max with respect to a, because the same resource o can
be used by different actions. Therefore, when summed over all resources o, the left-hand
side gives us the total requirements of policy π in terms of capacity c, which has to be no
greater than the bound κ
b(c).
The following example illustrates the single-agent model.
Example 2 Let us augment Example 1 as follows. Suppose the agents needs to obtain a
truck to perform its delivery actions (a1 and a2 ). The truck is also required by the service
and repair actions (a3 and a4 ). Further, to deliver appliances, the agent needs to acquire
a forklift, and it needs to hire a mechanic to be able to repair the vehicle (a4 ). The noop
action a0 requires no resources. This maps to a model with three resources (truck, forklift,
and mechanic): O = {ot , of , om }, and the following action resource costs (listing only the
non-zero ones):
ρ(a1 , ot ) = 1, ρ(a2 , ot ) = 1, ρ(a2 , of ) = 1, ρ(a3 , ot ) = 1, ρ(a4 , ot ) = 1, ρ(a4 , om ) = 1.
Moreover, suppose that the resources (truck ot , forklift of , or mechanic om ) have the
following capacity costs (there is only one capacity type, money: C = {c1 })
κ(ot , c1 ) = 2, κ(of , c1 ) = 3, κ(om , c1 ) = 4,
and the agent has a limited budget of κ
b = 8. It can, therefore acquire no more than two of
the three resources, which means that the optimal solution to the unconstrained problem as
in Example 1 is no longer feasible.

Let us observe that the MDP-based model of agents’ preferences presented above is
fully general for discrete indivisible resources, i.e., any non-decreasing utility function over
resource bundles can be represented via the resource-constrained MDP model described
above.
5. This formulation assumes a stationary policy, which is supported by the argument in Section 3.1.

514

Resource Allocation Among Agents with MDP-Induced Preferences

Theorem 1 Consider a finite set of n indivisible resources O = {oi } (i ∈ [1, n]), with
m ∈ N available units of each resource. Then, for any non-decreasing utility function
defined over resource bundles f : [0, m]n 7→ R, there exists a resource-constrained MDP
hS, A, p, r, O, ρ, C, κ, κ
b, αi (with the same resource set O) whose induced utility function over
the resource bundles is the same as f . In other words, for every resource bundle z ∈ [0, m]n ,
the value of the optimal policy among those whose resource requirements do not exceed z
(call this set Π(z)) is the same as f (z):
∀f : [0, m]n 7→ R, ∃ hS, A, p, r, O, ρ, C, κ, κ
b, αi :
n 
h
o
X
i

∀ z ∈ [0, m]n , Π(z) = π  max ρ(a, oi )H
π(s, a) ≤ zi =⇒ max Uγ (π, α) = f (z).
a

s

π∈Π(z)

Proof: See Appendix A.1.

Let us comment that while Theorem 1 establishes the generality of the MDP-based preference model introduced in this section, the construction used in the proof is of little practical interest, as it requires an MDP with an exponentially large state or action space. Indeed,
we do not advocate mapping arbitrary unstructured utility functions to exponentially-large
MDPs as a general solution technique. Rather, our contention is that our techniques apply to domains where utility functions are induced by a stochastic decision-making process
(modeled as an MDP), thus resulting in well-structured preferences over resources that
can be exploited to drastically lower the computational complexity of resource-allocation
algorithms.
3.1 Properties of the Single-Agent Constrained MDP
In this section, we analyze the constrained policy-optimization problem (9). Namely, we
show that stationary deterministic policies are optimal for this problem, meaning that it
is not necessary to consider randomized, or history-dependent policies. However, solutions
to problem (9) are not, in general, uniformly optimal (optimal for any initial distribution).
Furthermore, we show that (9) is NP-hard, unlike the unconstrained MDPs, which can be
solved in polynomial time (Littman, Dean, & Kaelbling, 1995).
We begin by showing optimality of stationary deterministic policies for (9). In the
following, we use ΠHR to refer to the class of history-dependent randomized policies (the
most general policies), and ΠSD ⊂ ΠHR to refer to the class of stationary deterministic
policies.
Theorem 2 Given an MDP M = hS, A, p, r, O, ρ, C, κ, κ
b, αi with resource and capacity
HR
constraints, if there exists a policy π ∈ Π
that is a feasible solution for M , there exists a
stationary deterministic policy π SD ∈ ΠSD that is also feasible, and the expected total reward
of π SD is no less than that of π:
∀ π ∈ ΠHR , ∃ π SD ∈ ΠSD : Uγ (π SD , α) ≥ Uγ (π, α)
Proof: See Appendix A.2.

The result of Theorem 2 is not at all surprising: intuitively, stationary deterministic
policies are optimal, because history dependence does not increase the utility of the policy,
515

Dolgov & Durfee

and using randomization can only increase resource costs. The latter is true because including an action in a policy incurs the same costs in terms of resources regardless of the
probability of executing that action (or the expected number of times the action will be
executed). This is true because we are dealing with non-consumable resources; this property does not hold for MDPs with consumable resources (as we discuss in more detail in
Section 6).
We now show that uniformly optimal policies do not always exist for our constrained
problem. This result is well known for another class of constrained MDPs, where constraints
are imposed on the total expected costs that are proportional to the expected number of
times the corresponding actions are executed (discussed earlier in Section 2). MDPs with
such constraints arise, for example, when bounds are imposed on the expected usage of
consumable resources, and as mentioned in Section 2, these problems can be solved using
linear programming by augmenting the dual LP (6) with linear constraints on expected
costs (8). Below, we establish the same result for problems with non-consumable resources
and capacity constraints.
Observation 1 There do not always exist uniformly optimal solutions to (9). In other
words, there exist two constrained MDPs that differ only in their initial conditions: M =
hS, A, p, r, O, ρ, C, κ, κ
b, αi and M 0 = hS, A, p, r, O, ρ, C, κ, κ
b, α0 i, such that there is no policy
that is optimal for both problems simultaneously, i.e., for any two policies π and π 0 that are
optimal solutions to M and M 0 , respectively, the following holds:
Uγ (π, α) > Uγ (π 0 , α),

Uγ (π, α0 ) < Uγ (π 0 , α0 )

(10)

We demonstrate this observation by example.
Example 3 Consider the resource-constrained problem as in Example 2. It is easy to see
that if the initial conditions are α = [1, 0, 0] (the agent starts in state s1 with certainty),
the optimal policy for states s1 and s2 is the same as in Example 1 (s1 → a2 and s2 → a3 ),
which, given the initial conditions, results in zero probability of reaching state s3 (to which
the noop a0 is assigned). This policy requires the truck and the forklift. However, if the
agent starts in state s3 (α = [0, 0, 1]), the optimal policy is to fix the truck (execute a4 in
s3 ), and to resort to furniture delivery (do a1 in s1 and assign the noop ao to s2 , which is
then never visited). This policy requires the mechanic and the truck. These two policies are
uniquely optimal for the corresponding initial conditions, and are suboptimal for other initial
conditions, which demonstrates that no uniformly optimal policy exists for this example. 
The intuition behind the fact that uniformly optimal policies do not, in general, exist
for constrained MDPs is that since the resource information is not a part of the MDP state
space, and there are constraints imposed on resource usage, the principle of Bellman optimality does not hold (optimal actions for different states cannot be chosen independently).
Given a constrained MDP, it is possible to construct an equivalent unconstrained MDP with
the standard properties of optimal solutions (by folding the resource information into the
state space, and modeling resource constraints via the transition function), but the resulting
state space will be exponential in the number of resources.
We now analyze the computational complexity of the optimization problem (9).
516

Resource Allocation Among Agents with MDP-Induced Preferences

Theorem 3 The following decision problem is NP-complete. Given an instance of an MDP
hS, A, p, r, O, ρ, C, κ, κ
b, αi with resources and capacity constraints, and a rational number Y ,
does there exist a feasible policy π, whose expected total reward, given α, is no less than Y ?
Proof: See Appendix A.3.

Note that the above complexity result stems from the limited capacities of the agents
and the fact that we define the resource requirements of a policy as the set of all resources
needed to carry out all actions that have a nonzero probability of being executed. If,
however, we defined constraints on the expected resource requirements, then actions with
low probability of being executed would have lower resource requirements, optimal policies
would be randomized, and the problem would be equivalent to a knapsack with continuously
divisible items, which is solvable in polynomial time via the LP formulation of MDPs with
linear constraints (6,8).
3.2 MILP Solution
Now that we have analyzed the properties of the optimization problem (9), we present a
formulation of (9) as a mixed integer linear program (MILP). Given that we have established
NP-completeness of (9) in the previous section, MILP (also NP-complete) is a reasonable
formulation that allows us to reap the benefits of a vast selection of efficient algorithms and
tools (see, for example, the text by Wolsey, 1998 and references therein).
In this section and in the rest of the paper we will assume that the resource requirements
of actions are binary, i.e., ρ(a, o) = {0, 1}. We make this assumption to simplify the
discussion, and it does not limit the generality of our results. We briefly describe the case
of non-binary resource costs in Appendix B for completeness, but refer to the work by
Dolgov (2006) for a more detailed discussion and examples.
Let us rewrite (9) in the occupation measure coordinates x by adding the constraints
from (9) to the standard LP in occupancy coordinates (6). Noticing that (for states with
nonzero probability of being visited) π(s, a) and x(s, a) are either zero or nonzero simultaneously:
X
X


x(s, a) ,
∀a ∈ A,
π(s, a) = H
H
s

s

and that, when ρ(a, o) = {0, 1}, the total resource requirements of a policy can be simplified
as follows:
X

n
X
X
o
max ρ(a, o)H
x(s, a) = H
ρ(a, o)
x(s, a) ,
∀o ∈ O,
(11)
a

a

s

s

we get the following program in x:
XX
max
x(s, a)r(s, a)
x

s

a

subject to:
X
XX
x(σ, a) − γ
x(s, a)p(σ|s, a) = α(σ),
a

X

s

κ(o, c)H

o

X
a

∀σ ∈ S;

a

ρ(a, o)

X


x(s, a) ≤ κ
b(c),

∀c ∈ C;

s

x(s, a) ≥ 0,

∀s ∈ S, a ∈ A.
517

(12)

Dolgov & Durfee

The challenge in solving this mathematical program is that the constraints are nonlinear
due to the Heaviside function H.
To linearize the Heaviside function, we augment the original
variables
 Poptimization
 x
P
with a set of |O| binary variables δ(o) ∈ {0, 1}, where δ(o) = H
a ρ(a, o)
s x(s, a) . In
other words, δ(o) is an indicator variable that shows whether the policy requires resource o.
Using δ(o), we can rewrite the resource constraints in (12) as:
X
κ(o, c)δ(o) ≤ κ
b(c),
∀c ∈ C,
(13)
o

which are linear in δ. We can then synchronize δ and x via the following linear inequalities:
X
X
1/X
ρ(a, o)
x(s, a) ≤ δ(o),
∀o ∈ O,
(14)
a

s

P
P
where X ≥ maxo a ρ(a, o) s x(s, a) is the normalization constant, for which any upper bound on the argument of H() can be used. The bound X is guaranteed
to exist
P
−1 max
ρ(a,
o),
since
for
discounted
problems.
For
example,
we
can
use
X
=
(1
−
γ)
o
a
P
−1
for any x that is a valid occupation measure for an MDP with
s,a x(s, a) = (1 − γ)
6
discount factor γ.
Putting it all together, the problem (9) of finding optimal policies under resource constraints can be formulated as the following MILP:
XX
max
x(s, a)r(s, a)
x,δ

s

a

subject to:
X
XX
x(σ, a) − γ
x(s, a)p(σ|s, a) = α(σ),
a

X

s

∀σ ∈ S;

a

κ(o, c)δ(o) ≤ κ
b(c),

∀c ∈ C;

X

∀o ∈ O;

(15)

o

1/X

ρ(a, o)

a

X

x(s, a) ≤ δ(o),

s

x(s, a) ≥ 0,

∀s ∈ S, a ∈ A;

δ(o) ∈ {0, 1},

∀o ∈ O.

We illustrate the MILP construction with an example.
Example 4 Let us formulate the MILP for the constrained problem from Example 3. Recall that in that problem there are three resources O = {ot , of , om } (truck, forklift, and
mechanic), one capacity type C = {c1 } (money), and actions have the following resource
requirements (again, listing only the nonzero ones):
ρ(a1 , ot ) = 1, ρ(a2 , ot ) = 1, ρ(a2 , of ) = 1, ρ(a3 , ot ) = 1, ρ(a4 , ot ) = 1, ρ(a4 , om ) = 1
P
P
6. Instead of using a single X for all resources, a different X(o) ≥ a ρ(a, o) s x(s, a) can be used for
every resource, leading to more uniform normalization and potentially better numerical stability of the
MILP solver.

518

Resource Allocation Among Agents with MDP-Induced Preferences

The resources have the following capacity costs:
κ(ot , c1 ) = 2,

κ(of , c1 ) = 3,

κ(om , c1 ) = 4,

and the agent has a limited budget, i.e., a capacity bound, κ
b(c1 ) = 8.
To compute the optimal policy for an arbitrary α, we can formulate the problem as
an MILP as described above. Using binary variables δ(o) = {δ(ot ), δ(of ), δ(om )}, we can
express the constraint on capacity cost as the following inequality:
2δ(ot ) + 3δ(of ) + 4δ(om ) ≤ 8,
For the constraints that synchronizeP
the occupation measure x and the binary indicators δ(o),
we can set X = (1 − γ)−1 maxo a ρ(a, o) = 4(1 − γ)−1 . Combining this with other
constraints from (15), we get an MILP with 12 continuous and 4 binary variables, and
|S| + |C| + |O| = 3 + 3 + 1 = 7 constraints (not counting the last two sets of range constraints).

As mentioned earlier, even though solving such programs is, in general, an NP-complete
task, there is a wide variety of very efficient algorithms and tools for doing so. Therefore,
one of the benefits of formulating the optimization problem (9) as an MILP is that it allows
us to make use of the highly efficient existing tools.

4. Multiagent Resource Allocation
We now consider the multiagent problem of resource allocation between several agents,
where the resource preferences of the agents are defined by the constrained MDP model
described in the previous section. We reiterate our main assumptions about the problem:
1. Weak coupling. We assume that agents are weakly-coupled (Meuleau et al., 1998),
i.e., they only interact through the shared resources, and once the resources are allocated, the agents’ transitions and rewards are independent. This assumption is critical
to our results.7
2. One-shot resource allocation. The resources are distributed once before the agents
start executing their MDPs. There is no reallocation of resources during the MDP
phase. This assumption is critical to our results; allowing reallocation of resources
would violate the weak-coupling assumption.
3. Initial central control over the resources. We assume that at the beginning of
the resource-allocation phase, the resources are controlled by a single authority. This
is the standard sell-auction setting. For problems where the resources are distributed
7. If agents are cooperative, the assumption about weak coupling can be relaxed (at the expense of an
increase in complexity), and the same MILP-based algorithm for simultaneously performing policy optimization and resource allocation can be applied if we consider the joint state spaces of the interacting
agents. For self-interested agents, a violation of the weakly-coupled assumption would mean that the
agents would be playing a Markov game (Shapley, 1953) once the resources are allocated, which would
significantly complicate the strategic analysis of the agents’ bidding strategies during the initial resource
allocation.

519

Dolgov & Durfee

among the agents to begin with, we face the problem of designing a computationallyefficient combinatorial exchange (Parkes, Kalagnanam, & Eso, 2001), which is a more
complicated problem that is outside the scope of this work. However, many of the
ideas presented in this paper could potentially be applicable to that domain as well.
4. Binary resource costs. As before, we assume that agents’ resource costs are binary.
This assumption is not limiting. The case of non-binary resources is discussed in
Appendix B.
Formally, the input to the resource-allocation problem consists of the following:
• M is the set of agents; we will use m ∈ M to refer to an agent.
• {hS, A, pm , rm , αm , ρm , κ
bm i} is the collection of weakly-coupled single-agent MDPs,
as defined in the single-agent model in Section 3. For simplicity, but without loss of
generality, we assume that all agents have the same state and action spaces S and A,
but each has its own transition and reward functions pm and rm , initial conditions αm ,
as well as its own resource requirements ρm : A × O 7→ {0, 1} and capacity bounds
κ
bm : C 7→ R. We also assume that all agents have the same discount factor γ, but this
assumption can be trivially relaxed.
• O and C are the sets of resources and capacities, defined exactly as in the single-agent
model in Section 3.
• κ : O × C 7→ R specifies the capacity costs of the resources, defined exactly as in the
single-agent model in Section 3.
• ρb : O 7→ R specifies the upper bound on the amounts of the shared resources (this
defines the additional bound for the multiagent problem).
Given the above, our goal is to design a mechanism for allocating the resources to the
agents in an economically efficient way, i.e., in a way that maximizes the social welfare of
the agents (one of the most often-used criteria in mechanism design). We would also like
the mechanism to be efficient from the computational standpoint.
Example 5 Suppose that there are two delivery agents. The MDP and capacity constraints
of the first agent are exactly as they were defined previously in Examples 1 and 2. The MDP
of the second agent is almost the same as that of the first agent, with the only difference
that it gets a slightly higher reward for delivering appliances: r2 (s1 , a2 ) = 12 (whereas
r1 (s1 , a2 ) = 10 for the first agent). Suppose there are two trucks, one forklift, and one
mechanic that are shared by the two agents. These bounds are specified as follows:
ρb(ot ) = 2,

ρb(of ) = 1,

ρb(om ) = 1.

Then the problem is to decide which agent should get the forklift, and which should get the
mechanic (trucks are plentiful in this example).

520

Resource Allocation Among Agents with MDP-Induced Preferences

4.1 Combinatorial Auctions
As previously mentioned, the problem of finding an optimal resource allocation among
self-interested agents that have complex valuations over combinations of resources arises
in many different domains (e.g., Ferguson, Nikolaou, Sairamesh, & Yemini, 1996; Wellman
et al., 2001) and is often called a combinatorial allocation problem. A natural and widely
used mechanism for solving such problems is a combinatorial auction (CA) (e.g., de Vries
& Vohra, 2003). In a CA, each agent submits a set of bids for resource bundles to the
auctioneer, who then decides what resources each agent will get and at what price.
Consider the problem of allocating among a set of agents M a set of indivisible resources O, where the total quantity of resource o ∈ O is bounded by ρb(o). Our earlier
simplifying assumption that actions’ resource requirements are binary implies that agents
will only be interested in bundles that contain no more than one unit of a particular resource.
In a combinatorial auction, each agent m ∈ M submits a bid bm
w (specifying how much
the agent is willing to pay) for every bundle w ∈ W m that has some value um
w > 0. In some
cases, it is possible to express such bids without enumerating all bundles (for example, using
an XOR bidding language (Sandholm, 1999) where it is necessary to only consider bundles
with strictly positive value, such that no subset of a bundle has the same value). Such
techniques often reduce the complexity of the resource-allocation problem, but do not, in
general, avoid the exponential blow up in the number of bids. Therefore, below we describe
the simplest combinatorial auction with flat bids, but it should be noted that many concise
bidding languages exist that in special cases can reduce the number of explicit bids.
After collecting all the bids, the auctioneer solves the winner-determination problem
(WDP), a solution to which prescribes how the resources should be distributed among the
m , its utility is um − q m
agents and at what prices. If agent m wins bundle w at price qw
w
w
(we are assuming risk-neutral agents with quasi-linear utility functions). Thus, the optimal
bidding strategy of an agent depends on how the auctioneer allocates the resources and sets
prices.
Vickrey-Clarke-Groves (VCG) mechanisms (Vickrey, 1961; Clarke, 1971; Groves, 1973)
are a widely used family of mechanisms that have certain very attractive properties (discussed in more detail below). An instantiation of a VCG mechanism in the context of
combinatorial auctions is the Generalized Vickrey Auction (GVA) (MacKie-Mason & Varian, 1994), which allocates resources and sets prices as follows. Given the bids bm
w of all
agents, the auctioneer chooses an allocation that maximizes the sum of agents’ bids. This
problem is NP-complete (Rothkopf et al., 1998) and can be expressed as the following inm = {0, 1} are indicator variables that
teger program, where the optimization variables zw
show whether bundle w is assigned to agent m, and nwo = {0, 1} specifies whether bundle w
contains o:8

8. There are other related algorithms for solving the WDP (e.g., Sandholm, 2002), but we will use the
integer program (16) as a representative formulation for the class of algorithms that perform a search in
the space of binary decisions on resource bundles.

521

Dolgov & Durfee

X

max
z

X

m m
zw
bw

m∈M w∈W m

subject to:
X
m
zw
≤ 1,

∀m ∈ M;

(16)

w∈W m

X

X

m∈M

w∈W m

m
zw
nwo ≤ ρb(o),

∀o ∈ O.

The first constraint in (16) says that no agent can receive more than one bundle, while the
second constraint ensures that the total amount of resource o assigned to the agents does
not exceed the total amount available. Notice that MILP (16) performs the summation over
exponentially large sets of bundles w ∈ W m . As outlined above, in an auction with XOR
bidding, these sets would typically be smaller, but, in general, still exponentially large.
A GVA assigns resources according to the optimal solution ze to (16) and sets the payment
for agent m to:
X
m
∗
m0 m0
qw
= V−m
−
zew
bw ,
(17)
m0 6=m
∗
where V−m
is the value of (16) if m were to not participate in the auction (the optimal
value if m does not submit any bids), and the second term is the sum of other agents’ bids
in the solution ze to the WDP with m participating.
A GVA has a number of nice properties. It is strategy-proof, meaning that the dominant
m
strategy of every agent is to bid its true value for every bundle: bm
w = uw . The auction
is economically efficient, meaning that it allocates the resources to maximize the social
welfare of the agents (because, when agents bid their true values, the objective function
of (16) becomes the social welfare). Finally, a GVA satisfies the participation constraint,
meaning that no agent decreases its utility by participating in the auction.
A straightforward way to implement a GVA for our MDP-based problem is the following.
Let each agent m ∈ M enumerate all resource bundles W m that satisfy its local capacity
constraints defined by κ
bm (c) (this is sufficient because our MDP model implies free disposal
of resources for the agents, and we make the same assumption about the auctioneer). For
each bundle w ∈ W m , agent m would determine the feasible action set A(w) and formulate
an MDP Λm (w) = hS, A(w), pm (w), rm (w), αm i, where pm (w) and rm (w) are the transition
and reward functions defined on the pruned action space A(w). Every agent would then
solve each Λm (w) corresponding to a feasible bundle to find the optimal policy π
em (w), whose
m
m
m
expected discounted reward would define the value of bundle w: uw = Uγ (e
π (w), αm ).
This mechanism suffers from two major complexity problems. First, the agents have
to enumerate an exponential number of resource bundles and compute the value of each
by solving the corresponding (possibly large) MDP. Second, the auctioneer has to solve
an NP-complete winner-determination problem on exponentially large input. The following
sections are devoted to tackling these complexity problems.

Example 6 Consider the two-agent problem described in Example 5, where two trucks, one
forklift, and the services of one mechanic are being auctioned off. Using the straightforward
version of the combinatorial auction outlined above, each agent would have to consider
522

Resource Allocation Among Agents with MDP-Induced Preferences

2|O| = 23 = 8 possible resource bundles (since resource requirements of both agents are
binary, neither agent is going to bid on a bundle that contains two trucks). For every
resource bundle, each agent will have to formulate and solve the corresponding MDP to
compute the utility of the bundle.
For example, if we assume that both agents start in state s1 (different initial conditions
would result in different expected rewards, and thus different utility functions), the value of
the null resource bundle to both agents would be 0 (since the only action they would be able
to execute is the noop a0 ). On the other hand, the value of bundle [ot , of , om ] = [1, 1, 1] that
contains all the resources would be 95.3 to the first agent and 112.4 to the second one. The
value of bundle [1, 1, 0] to each agent would be the same as the value of [1, 1, 1] (since their
optimal policies for the initial conditions that put them in s1 do not require the mechanic).
Once the agents submit their bids to the auctioneer, it will have to solve the WDP via
the integer program (16) with |M|2|O| = 2(2)3 = 16 binary variables. Given the above, the
optimal way to allocate the resources would be to assign a truck to each of the agents, the
forklift to the second agent, and the mechanic to either (or neither) of the two. Thus, the
agents would receive bundles [1, 0, 0] and [1, 1, 0], respectively, resulting in social welfare of
50 + 112.4 = 162.4. However, if at least one of the agents had a non-zero probability of
starting in state s3 , the value of the resource bundles involving the mechanic would change
drastically, as would the optimal resource allocation and its social value.

4.2 Avoiding Bundle Enumeration
To avoid enumerating all resource bundles that have non-zero value to an agent, two things
are required: i) the mechanism has to support a concise bidding language that allows
the agent to express its preferences to the auctioneer in a compact manner, and ii) the
agents have to be able to find a good representation of their preferences in that language. A
simple way to achieve both in our model is to create an auction where the agents submit the
specifications of their resource-parameterized MDPs to the auctioneer as bids: the language
is compact and, given our assumption that each agent can formulate its planning problem
as an MDP, this does not require additional computation for the agents. However, this
only changes the communication protocol between the agents and the auctioneer, similarly
to other concise bidding languages (Sandholm, 1999; Nisan, 2000; Boutilier & Hoos, 2001;
Boutilier, 2002). As such, it simply moves the burden of solving the valuation problem from
the agents to the auctioneer, which by itself does not lead to any gains in computational
efficiency. Such a mechanism also has implications on information privacy issues, because
the agents have to reveal their local MDPs to the auctioneer (which they might not want
to do). Nevertheless, we can build on this idea to increase the efficiency of solving both the
valuation and winner-determination problems and do so while keeping most of the agents’
MDP information private. We address ways of maintaining information privacy in the next
section, and for the moment focus on improving the computational complexity of the agents’
valuation and the auctioneer’s winner-determination problems.
The question we pose in this section is as follows. Given that the bid of each agent consists of its MDP, its resource information and its capacity bounds hS, A, pm , rm , αm , ρm , κ
bm i,
can the auctioneer formulate and solve the winner-determination problem more efficiently
523

Dolgov & Durfee

than by simply enumerating each agent’s resource bundles and solving the standard integer
program (16) with an exponential number of binary variables?
Therefore, the goal of the auctioneer is to find a joint policy (a collection of single-agent
policies under our weak-coupling assumption) that maximizes the sum of the expected total
discounted rewards for all agents, under the conditions that: i) no agent m is assigned a set
of resources that violates its capacity bound κ
bm (i.e., no agent is assigned more resources
than it can carry), and ii) the total amounts of resources assigned to all agents do not exceed
the global resource bounds ρb(o) (i.e., we cannot allocate to the agents more resources than
are available). This problem can be expressed as the following mathematical program:
X
max
Uγm (π m , αm )
π

m

subject to:
X
X

κ(o, c)H ρm (a, o)
π m (s, a) ≤ κ
bm (c),
o

X

∀c ∈ C, m ∈ M;

(18)

s
m

ρ (a, o)H

X

m


π (s, a) ≤ ρb(o),

∀o ∈ O.

s

m

Obviously, the decision version of this problem is NP-complete, as it subsumes the singleagent MDP with capacity constraints, NP-completeness of which was shown in Section 3.1.
Moreover, the problem remains NP-complete even in the absence of single-agent capacity
constraints. Indeed, the global constraint on the amounts of the shared resources is sufficient
to make the problem NP-complete, which can be shown with a straightforward reduction
from KNAPSACK, similar to the one used in the single-agent case in Section 3.1.
We can linearize (18) similarly to the single-agent problem from Section 3.2, yielding
the following MILP, which is simply a multiagent version of (15) (recall the assumption of
this section that resource requirements are binary):
XXX
max
xm (s, a)rm (s, a)
x,δ

m

s

a

subject to:
X
XX
xm (σ, a) − γ
xm (s, a)pm (σ|s, a) = αm (σ),
a

X

s

∀σ ∈ S, m ∈ M;

a

κ(o, c)δ m (o) ≤ κ
bm (c),

∀c ∈ C, m ∈ M;
(19)

o

X

m

δ (o) ≤ ρb(o),

∀o ∈ O;

m

1/X

X

ρm (a, o)

a

X

xm (s, a) ≤ δ m (o),

∀o ∈ O, m ∈ M;

s

xm (s, a) ≥ 0,

∀s ∈ S, a ∈ A, m ∈ M;

m

δ (o) ∈ {0, 1},

∀o ∈ O, m ∈ M,

where X ≥ maxo,m a ρ(a, o) s xm (s, a) is an upper bound on the argument of H(·),
used for normalization. As in the single-agent case, this bound is guaranteed to exist for
discounted MDPs and is easy to obtain.
P

P

524

Resource Allocation Among Agents with MDP-Induced Preferences

The MILP (19) allows the auctioneer to solve the WDP without having to enumerate
the possible resource bundles. As compared to the standard WDP formulation (16), which
has on the order of |M|2|O| binary variables, (19) has only |M||O| binary variables. This
exponential reduction is attained by exploiting the knowledge of the agents’ MDP-based
valuations and simultaneously solving the policy-optimization and resource-allocation problems. Given that the worst-case solution time for MILPs is exponential in the number of
integer variables, this reduction has a significant impact on the worst-case performance of
the mechanism. The average-case running time is also reduced drastically, as demonstrated
by our experiments, presented in Section 5.
Example 7 If we apply the mechanism discussed above to our running example as an alternative to the straightforward combinatorial auction presented in Example 6, the winnerdetermination MILP (19) will look as follows. It will have |M||S||A| = (2)(3)(5) = 30 continuous occupation-measure variables xm , and |M||O| = (2)(3) = 6 binary variables δ m (o).
It will have |M||S| = (2)(3) = 6 conservation-of-flow constraints that involve continuous
variables only, as well as |M||C| + |O| + |M||O| = (2)(1) + 3 + (2)(3) = 9 constraints that
involve binary variables.
The capacity constraints for the agents will be exactly as in the single-agent case described
in Example 4, and the global resource constraints will be:
δ 1 (ot ) + δ 2 (ot ) ≤ 2,

δ 1 (of ) + δ 2 (of ) ≤ 1,

δ 1 (om ) + δ 2 (om ) ≤ 1.

Notice that in this example there is one binary decision variable per resource per agent
(yielding 6 such variables for this simple problem). This is exponentially fewer than the
number of binary variables in the straightforward CA formulation of Example 6, which
requires one binary variable per resource bundle per agent (yielding 16 such variables for
this problem). Given that MILPs are NP-complete in the number of integer variables, this
reduction from 16 to 6 variables is noticeable even in a small problem like this one and can
lead to drastic speedup for larger domains.

The mechanism described above is an instantiation of the GVA, so by the well-known
properties of VCG mechanisms, this auction is strategy-proof (the agents have no incentive
to lie to the auctioneer about their MDPs), it attains the socially optimal resource allocation,
and no agent decreases its utility by participating in the auction.
To sum up the results of this section: by having the agents submit their MDP information to the auctioneer instead of their valuations over resource bundles, we have essentially
removed all computational burden from the agents and at the same time significantly simplified the auctioneer’s winner-determination problem (the number of integer variables in
the WDP is reduced exponentially).
4.3 Distributing the Winner-Determination Problem
Unlike the straightforward combinatorial auction implementation discussed earlier in Section 4.1, where the agents shared some computational burden with the auctioneer, in the
mechanism from Section 4.2, the agents submit their information to the auctioneer and
then just idle while waiting for a solution. This suggests further potential improvements in
computational efficiency. Indeed, given the complexity of MILPs, it would be beneficial to
525

Dolgov & Durfee

exploit the computational power of the agents to offload some of the computation from the
auctioneer back to the agents (we assume that agents have no cost for “helping out” and
would prefer for the outcome to be computed faster).9 Thus, we would like to distribute
the computation of the winner-determination problem (19), the common objective in distributed algorithmic mechanism design (Feigenbaum & Shenker, 2002; Parkes & Shneidman,
2004).10
For concreteness, we will base the algorithm of this section on the branch and bound
method for solving MILPs (Wolsey, 1998), but exactly the same techniques will also work
for other MILP algorithms (e.g., cutting planes) that perform a search in the space of LP
relaxations of the MILP. In branch and bound for MILPs with binary variables, LP relaxations are created by choosing a binary variable and setting it to either 0 or 1, and relaxing
the integrality constraints of other binary variables. If a solution to an LP relaxation happens to be integer-valued, it provides a lower bound on the value of the global solution. A
non-integer solution provides an upper bound for the current subproblem, which (combined
with other lower bounds) is used to prune the search space.
Thus, a simple way for the auctioneer to distribute the branch and bound algorithm is
to simply farm out LP relaxations to other agents and ask them to solve the LPs. However,
it is easy to see that this mechanism is not strategy-proof. Indeed, an agent that is tasked
with performing some computation for determining the optimal resource allocation or the
associated payments could benefit from lying about the outcome of its computation to
the auctioneer. This is a common phenomenon in distributed mechanism implementations:
whenever some WDP calculations are offloaded to an agent participating in the auction, the
agent might be able to benefit from sabotaging the computation. There are several methods
to ensuring the strategy-proofness of a distributed implementation. The approach best
suited for our problem is based on the idea of redundant computation (Parkes & Shneidman,
2004),11 where multiple agents are asked to do the same task and any disagreement is
carefully punished to discourage lying. In the rest of this section, we demonstrate that this
is very easy to implement in our case.
The basic idea is very simple: let the auctioneer distribute LP relaxations to the agents,
but check their solutions and re-solve the problems if the agents return incorrect solutions
(this would make truthful computation a weakly-dominant strategy for the agents, and
a nonzero punishment can be used to achieve strong dominance). This strategy of the
auctioneer removes the incentive for the agents to lie and yields exactly the same solution
as the centralized algorithm. However, in order for this to be beneficial, the complexity of
checking a solution must be significantly lower than the complexity of solving the problem.
Fortunately, this is true for LPs.
Suppose the auctioneer has to solve the following LP, which can be written in two
equivalent ways (let us refer to the one on the left as the primal, and to the one on the right
9. As observed by Parkes and Shneidman (2004), this assumption is a bit controversial, since a desire for
efficient computation implies nonzero cost for computation, while the agents’ cost for “helping out” is
not modeled. It is, nonetheless, a common assumption in distributed mechanism implementations.
10. We describe one simple way of distributing the mechanism, others are also possible.
11. Redundant computation is discussed by Parkes and Shneidman (2004) in the context of ex post Nash
equilibria, whereas we are interested in dominant strategies, but the high-level idea is very similar.

526

Resource Allocation Among Agents with MDP-Induced Preferences

as the dual):
min αT v

max rT x

subject to:

(20)

subject to:

T

Ax = α;

A v ≥ r.

x ≥ 0.

By the strong duality property, if the primal LP has a solution v∗ , then the dual also has a
solution x∗ , and αT v∗ = rT x∗ . Furthermore, given a solution to the primal LP, it is easy to
compute a solution to the dual: by complementary slackness, v∗T = rT B −1 and x∗ = B −1 α,
where B is a square invertible matrix composed of columns of A that correspond to basic
variables of the solution.
These well-known properties can be used by the auctioneer to quickly check optimality
of solutions returned by the agents. Suppose that an agent returns v as a solution to the
primal LP. The auctioneer can calculate the dual solution vT = rT B −1 and check whether
rT x = αT v. Thus, the most expensive operation that the auctioneer has to perform is
the inversion of B, which can be done in sub-cubic time. As a matter of fact, from the
implementation perspective, it would be more efficient to ask the agents to return both the
primal and the dual solutions, since many popular algorithms compute both in the process
of solving LPs.
Thus, we have provided a simple method that allows us to effectively distribute the
winner-determination problem, while maintaining strategy-proofness of the mechanism and
with a negligible computation overhead for the auctioneer.
4.4 Preserving Information Privacy
The mechanism that we have discussed so far has the drawback that it requires agents
to reveal complete information about their MDPs to the auctioneer. The problem is also
exacerbated in the distributed WDP algorithm from the previous section, since not only
does each agent reveal its MDP information to the auctioneer, but that information is then
also spread to other agents via the LP relaxations of the global MILP. We now show how
to alleviate this problem.
Let us note that, in saying that agents prefer not to reveal their local information, we
are implicitly assuming that there is an external factor that affects agents’ utilities that is
not captured in the agents’ MDPs. A sensible way to measure the value of information is by
how it changes one’s decision-making process and its outcomes. Since this effect is not part
of our model (in fact, it contradicts our weak-coupling assumption), we cannot in a domainindependent manner define what constitutes “useful” information, and how bad it is for an
agent to reveal too much about its MDP. Modeling such effects and carefully analyzing
them is an interesting research task, but it is outside the scope of this paper. Thus, for the
purposes of this section, we will be content with a mechanism that hides enough information
to make it impossible for the auctioneer or an agent to uniquely determine the transition
or reward function of any other agent (in fact, information revealed to any agent will map
to infinitely many MDPs of other agents).12 Many such transformations are possible; we
present just one to illustrate the concept.
12. A more stringent condition would require that agents’ preferences over resource bundles are not revealed
(Parkes, 2001), but we set a lower bar here.

527

Dolgov & Durfee

The main idea of our approach is to modify the previous mechanism so that the agents
submit their private information to the auctioneer in an “encrypted” form that allows the
auctioneer to solve the winner-determination problem, but does not allow it to infer the
agents’ original MDPs.
First, note that, instead of passing an MDP to the auctioneer, each agent can submit
an equivalent LP (6). So, the question becomes: can the agent transform its LP in such a
way that the auctioneer will be able to solve it, but will not be able to infer the transition
and reward functions of the originating MDP? In other words, the problem reduces to the
following. Given an LP L1 (created from an MDP Λ = hS, A, p, r, αi via (6)), we need to
find a transformation L1 → L2 such that a solution to the transformed LP L2 will uniquely
map to a solution to the original LP L1 , but L2 will not reveal the transition or the reward
functions of the original MDP (p or r). We show that a simple change of variables suffices.
Suppose agent m1 has an MDP-originated LP and is going to ask agent m2 to solve it. In
order to maintain the linearity of the problem (to keep it simple for m2 to solve), m1 should
limit itself to linear transformations. Consider a linear, invertible transformation of the
primal coordinates u = F v, and a linear, invertible transformation of the dual coordinates
y = Dx. Then, the LP from (20) will be transformed (by applying F , switching to the
dual, and then applying D) to an equivalent LP in the new coordinates y:
max rT D−1 y
subject to:
(F −1 )T AD−1 y = (F −1 )T α;

(21)

D−1 y ≥ 0.
The value of the optimal solution to (21) will be the same as the value of the optimal solution
to (20), and given an optimal solution y∗ to (21), it is easy to compute the solution to the
original: x∗ = D−1 y∗ . Indeed, from the perspective of the dual, the primal transformation F
is equivalent to a linear transformation of the dual equality constraints Ax = α, which (given
that F is non-singular) has no effect on the solution or the objective function. Furthermore,
the dual transformation D is equivalent to a change of variables that modifies the solution
but not the value of the objective function.
However, a problem with the above transformations is that it gives away D−1 . Indeed,
agent m2 will be able to simply read (up to a set of multiplicative constants) the transformation off the constraints D−1 y ≥ 0. Therefore, only diagonal matrices with positive
coefficients (which are equivalent to stretching the coordinate system) are not trivially deduced by m2 , since they also map to y ≥ 0. Choosing a negative multiplier for some xi
(inverting the axis) is pointless, because that flips the non-negativity constraints to yi ≤ 0,
immediately revealing the sign to m2 .
Let us demonstrate that, given any MDP Λ and the corresponding LP L1 , we can
choose D and F such that it will be impossible for m2 to determine the coefficients of L1
(or equivalently the original transition and reward functions p and r). When agent m2
receives L2 (as in (21)), all it knows is that L2 was created from an MDP, so the columns
of the constraint matrix of the original LP L1 must sum to a constant:
X
X
Aji = 1 − γ
p(σ|s, a) = 1 − γ.
(22)
j

σ

528

Resource Allocation Among Agents with MDP-Induced Preferences

a1:
p=1
r=3

a1:
p=1
r=1

s2

s1

a2:
p=1
r=4

a2:
p=0.99
r=9.8

a2:
p=0.5
r=0.02

a1:
p=0.5
r=0.02
a2:
p=0.01
r=0.98

a1:
p=0.5
r=0.02

a1:
p=1
r=1

s2

s1

a2:
p=1
r=2

a2:
p=0.5
r=0.02

(a)

a1:
p=1
r=3

a1:
p=1
r=1

s2

s1

a2:
p=1
r=4

a2:
p=0.99
r=9.8

(b)

Figure 3: Preserving privacy example. Two different MDPs that can lead to the same LP
constraint matrix.

This gives m2 a system of |S| nonlinear equations for the diagonal D and arbitrary F ,
which have a total of |S||A| + |S|2 free parameters. For everything but the most degenerate
cases (which can be easily handled by an appropriate choice of D and F ), these equations
are hugely under-constrained and will have infinitely many solutions. As a matter of fact,
by sacrificing |S| of the free parameters, m1 can choose D and F in such a way that the
columns of constraints in L2 will also sum to a constant γ 0 ∈ (0, 1), which would have the
effect of transforming L1 to an L2 that corresponds to another valid MDP Λ2 . Therefore,
given an L2 , there are infinitely many original MDPs Λ and transformations D and F that
map to the same LP L2 .
We also have to consider the connection of resource and capacity costs to agents’ occupation measures in the global WDP (19). There are two things that the auctioneer has
to be able to do: i) determine the value of each agent’s policy (to be able to maximize
the social welfare), and ii) determine the resource requirements of the policies (to check
the resource constraints). So, the question is, how does our transformation affect these?
As noted earlier, the transformation does not change the objective function, so the first
requirement holds. On the other hand, D does change the occupation measure xm (s, a) by
arbitrary multipliers. However, a multiplicative factor of xm (s, a) has no effect on the usage
of non-consumable resources, as it only matters whether the corresponding xm (s, a) is zero
or not (step function H nullifies the scaling effect). Thus, the second condition also holds.
Example 8 Consider the two-state MDP depicted in Figure 3a that represents the decisionmaking problem of a sales company, with the two states corresponding to possible market
conditions, and the two actions — to two possible sales strategies. Market conditions in state
s1 are much more favorable than in state s2 (the rewards for both actions are higher). The
transitions between the two states correspond to probabilities of market conditions changing
and the rewards reflect the expected profitability in these two states. Obtaining such numbers
in a realistic scenario would require performing costly and time-consuming research, and the
company might not want to make this information public.
Therefore, if the company were to participate in the resource-allocation mechanism described above, it would want to encrypt its MDP before submitting it to the auctioneer.
529

a2:
p=0.5
r=0.02

Dolgov & Durfee

The MDP has the following reward function
r = (1, 19.622, 0.063, 0.084)T ,
and the following transition function:


1
0
p(a1 ) =
,
0.5 0.5


p(a2 ) =


0.986 0.014
.
0.5
0.5

(23)

(24)

Using γ = 0.8, this corresponds to the following conservation of flow constraint matrix:


0.2 0.212 −0.4 −0.4
A=
.
(25)
0 −0.012 0.6
0.6
Before submitting its LP to the auctioneer, the agent applies the following transformations:


2
0
,
(26)
D = diag(1, 0.102, 47.619, 47.619), F =
−0.084 0.126
yielding the following new constraint matrix:


0.1
1
0
0
0
−1 T
−1
A = (F ) AD =
.
0 −0.9 0.1 0.1

(27)

However, the above constraint matrix A0 corresponds to a non-transformed conservation
of flow constraint for a different MDP (shown in Figure 3b) with γ = 0.9, the following
reward function:
r = (1, 2, 3, 4)T ,
(28)
and the following transition function:


1 0
p(a1 ) =
,
0 1


p(a2 ) =


0 1
.
0 1

(29)

Therefore, when the auctioneer receives the constraint matrix A0 , it has no way of knowing whether the agent has an MDP with transition function (29) that was transformed
using (26) or the MDP with transition function (24) that was not transformed. Notice that
the dynamics of the two MDPs vary significantly: both in transition probabilities and state
connectivity. The second MDP does not reveal any information about the originating MDP
and the corresponding market dynamics.

To sum up, we can, to a large extent, maintain information privacy in our mechanism by
allowing agents to apply linear transformations to their original LPs. The information that
is revealed by our mechanism consists of agents’ resource costs ρm (a, o), capacity bounds
κ
bm (c), and the sizes of their state and action spaces (the latter can be hidden by adding
dummy states and actions to the MDP).
The revealed information can be used to infer agents’ preferences and resource requirements. Further, numeric policies are revealed, but the lack of information about transition
and reward functions renders this information worthless (as just illustrated in Example 8,
there could be multiple originating MDPs with very different properties).
530

Resource Allocation Among Agents with MDP-Induced Preferences

5. Experimental Results
In this section we present an empirical analysis of the computational complexity of the
resource-allocation mechanism described in Section 4. We report results on the computational complexity of the mechanism from Section 4.2, where the agents submit their
MDPs to the auctioneer, who then simultaneously solves the resource-allocation and policyoptimization problems. As far as the additional speedup achieved by distributing the WDP,
as described in Section 4.3, we do not report empirical results, since it is well-established
in the parallel programming literature that parallel versions of branch-and-bound MILP
solvers consistently achieve linear speedup (Eckstein, Phillips, & Hart, 2000). This is due
to the fact that branch-and-bound algorithms require very little inter-process communication.
For our experiments, we implemented a multiagent delivery problem, based on the
multiagent rover domain (Dolgov & Durfee, 2004). In this problem, agents operate in a
stochastic grid world with delivery locations randomly placed throughout the grid. Each
delivery task requires a set of resources, and there are limited quantities of the resources.
There are random delivery locations on the grid, and each location has a set of deliveries that
it accepts. Each resource has some size requirements (capacity cost), and each delivery agent
has bounded space to hold the resources (limited capacity). The agents participate in an
auction where they bid on delivery resources. In this setting, the value of a resource depends
on what other resources the agent acquires and what other deliveries it can make. Given a
bundle of resources, an agent’s policy optimization problem is to find the optimal delivery
plan. The exact parameters used in our experiments are not critical for the trends seen in
the results presented below, but for the sake of reproducibility the domain is described in
detail in Appendix C.13 All of the resource costs in the experiments presented below are
binary.
Computational complexity of constrained optimization problems can vary greatly as
constraints are tightened or relaxed. Therefore, as our first step in the analysis of empirical
computational complexity of our mechanism, we investigate how its running time depends
on the capacity constraints of each agent and on the bounds on the total amounts of
resources shared by the agents. As is common with other types of constrained optimization
or constraint-satisfaction problems, it is natural to expect that the WDP MILP will be
easy to solve when the problem is over- or under-constrained in either the capacity or the
resource bounds. To empirically verify this, we varied local capacity constraint levels from 0
(meaning agents cannot use any resources) to 1 (meaning each agent has the capacity to
use enough resources to execute its optimal unconstrained policy), as well as the global
constraint levels for which 0 meant that no resources were available to the agents, and 1
meant that there were enough resources to assign to each agent its most desired resource
bundle. In all of our experiments, the part of the MILP solver was played by CPLEX 8.1
on a Pentium-4 machine with 2GB of RAM (RAM was not a bottleneck due to the use of
sparse matrix representations). A typical running-time profile is shown in Figure 4. The
problem is very easy when it is over-constrained, becomes more difficult as the constraints
are relaxed and then abruptly becomes easy again when capacity and resource levels start
to approach utopia.
13. We also investigated other, randomly generated domains, and the results were qualitatively the same.

531

Dolgov & Durfee

1
0.9
0.8
Local Constraint Level

6

t, sec

4

2

0
1

1

0.5
Local Constraints

0.5
0

0

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0

Global Constraints

0.2

0.4
0.6
Global Constraint Level

0.8

1

Figure 4: Running time for MDP-based winner-determination MILP (19) for different levels of global (b
ρ) and local (b
κm ) constraints. The constraint levels are fractions
of utopian levels that are needed to implement optimal unconstrained policies.
Problems involved 10 agents, each operating on a 5-by-5 grid, with 10 shared
resource types. Each data point shown is an average of ten runs over randomlygenerated problems.

In all of the following experiments we aim to avoid the easy regions of constraint levels.
Therefore, given the complexity profiles, we set the constraint levels to 0.5 for both local
capacity and global resource bounds. We also set the discount factor to γ = 0.95. This
value was chosen arbitrarily, because our investigations into the effect of the value of γ on
the running time of the MILP revealed no significant trends.
We begin by comparing the performance of our MDP-based auction (Section 4.2) to the
performance of the straightforward CA with flat preferences (as described in Section 4.1).
The results are summarized in Figure 5, which compares the time it takes to solve the
standard winner-determination problem on the space of all resource bundles (16) to the
time needed to solve the combined MDP-WDP problem (19) used in our mechanism, as
the number of resources is increased (with 5 agents, on a 5-by-5 grid). Despite the fact
that both algorithms have exponential worst-case running time, the number of integer
variables in (16) is exponentially larger than in our MILP (19), the effect of which is clearly
demonstrated in Figure 5. Furthermore, this comparison gives an extremely optimistic
view of the performance of the standard CA, as it does not take into account the additional
complexity of the valuation problem, which requires formulating and solving a very large
number of MDPs (one per resource bundle). On the other hand, the latter is embedded into
the WDP of our mechanism (19), thus including the time for solving the valuation problem
in the comparison would only magnify the effect. In fact, in our experiments, the time
required to solve the MDPs for the valuation problem was significantly greater than the
time for solving the resulting WDP MILP. However, we do not present quantitative results
to that effect here, because of the difference in implementation (iterating over resource
bundles and solving MDPs was done via a straightforward implementation in Matlab, while
532

Resource Allocation Among Agents with MDP-Induced Preferences

Figure 5: Gains in computation efficiency: MDP-based WDP versus a WDP in a straightforward CA implementation. The latter does not include the time for solving
the MDPs to compute resource-bundle values. Error bars show the standard
deviation over ten runs.
n=5, |O| = 10
4

10

3

10

2

t, sec

10

1

10

0

10

−1

10

5

10
15
Number of Agents |M|

20

25

Figure 6: Scaling the MDP-based winner-determination MILP (19) to more agents. Agents
operated on 5-by-5 grids and shared 10 types of resources.

MILPs were solved using highly-optimized CPLEX code). No parallelization of the WDP
was performed for these experiments for either algorithm.
Below we analyze the performance of our algorithm on larger problems infeasible for
the straightforward CA. Figure 6 illustrates the scaling effect as the number of agents
participating in the auction is increased. Here and below, each point on the plot corresponds
to a single run of the experiment (with no less than ten runs performed for every value of
parameters), and the solid line is the mean. Recall that the size of the WDP scales linearly
533

Dolgov & Durfee

n = 10, |M| = 5

n = 5, |M| = 10

2

10

2

10

1

1

10
t, sec

t, sec

10

0

10

0

10

−1

10

−1

10

0

20

40
60
80
Number of Resource Types |O|

−2

10

100

(a)

0

20

40
60
80
Number of Resource Types |O|

100

(b)

n = 7, |M| = 10
3

10

2

t, sec

10

1

10

0

10

−1

10

−2

10

0

20

40
60
80
Number of Resource Types |O|

100

(c)

(d)

Figure 7: (a)–(c): scaling of the MDP-based winner-determination MILP (19) with the
number of resources on three sets of problems with different grid sizes (n) and
different numbers of agents (|M|); (d): a linear-scale plot of the tail of the data
in (c).

with the number of agents. The graph therefore reflects a rather standard scaling effect
for an NP-complete problem. As can be seen from the plot, problems with 25 agents
and 10 resource types are well within the reach of the method, on average taking around
30 minutes.
Next, we analyze how the method scales with the number of resource types. Figure 7
shows the solution time as a function of the number of resource types for three different
sets of problems. In these problems, the number of actions scaled linearly with the number
of resource types, but each action required a constant number of resources, i.e., the number
534

Resource Allocation Among Agents with MDP-Induced Preferences

n = 5, |M| = 3

80
70
60

t, sec

50
40
30
20
10
0
0

5

10
15
Resources Per Action

20

Figure 8: Complexity of MDP-based winner-determination MILP (19) as a function of the
number of actions’ resource requirements.

of nonzero ρ(a, o) per action was constant (two) regardless of the total number of resource
types. These problems exhibit an interesting trait wherein the running time peaks for
relatively low numbers of resource types, then falls quickly, and then increases much more
slowly as the number of resource types increases (as illustrated in Figure 7d, which uses
a linear scale). This is due to the fact that when the total number of resource types is
much higher than the number of resources required by any action, there is less contention
for a particular resource among the agents and between one agent’s actions. Therefore, the
problems become relatively under-constrained and the solution time increases slowly.
To better illustrate this effect, we ran a set of experiments inverse to the ones shown in
Figure 7: we kept the total number of resource types constant and increased the number of
resource types required by each action. The results are shown in Figure 8. The running-time
profile is similar to what we observed earlier when we varied the local and global constraints:
when the total number of resources per action is low or high, the problem is under- or overconstrained and is relatively easy to solve, but its complexity increases significantly when
the number of resources required by each resource is in the range of 50-80% of the total
number of resource types.
Based on the above, we would expect that if the actions’ resource requirements increased
with the total number of resource types, the problem would not scale as gracefully as in
Figure 7. For example, Figure 9 illustrates the running time for problems where the number
of resources required by each action scales linearly with the total number of resources. There,
the complexity does increase significantly faster. However, it is not unreasonable to assume
that in many domains the number of actions does not, in fact, increase with the total
number of resource types involved. Indeed, it is natural to assume that the total number
of resource types increases as the problem becomes more complicated and the number of
tasks the agent can perform increases. However, why should the resource requirements of an
action increase as well? If the delivery agent from our running example acquires the ability
535

Dolgov & Durfee

n= 5, |M| = 5
4

10

3

10

2

t, sec

10

1

10

0

10

−1

10

−2

10

0

10

20
30
40
Number of Resource Types |O|

50

Figure 9: Complexity when actions’ resource requirements grow proportionally to the total
number of resource types. The number of resource types needed by each action
is 10% of the total number of resource types |O|.

to deliver pizza, it might need new resources to perform actions related to this new activity,
but one would not expect the resource requirements for delivering furniture or appliances
to change. Therefore, we believe that in many real applications, our method will scale up
gracefully with the total number of resource types.
The above experiments illustrate the point that for domains where agents have preferences that are defined by underlying Markov decision processes, the resource-allocation
mechanism developed in this paper can lead to significant computational advantages. As
shown in Figure 7, the method can be successfully applied to very large problems that, we
argue, are well beyond the reach of combinatorial resource-allocation mechanisms with flat
preferences. As our experiments show (Figure 5), even for small problems, combinatorial
resource allocation mechanisms with flat preferences can be time-consuming, and our attempts to empirically evaluate those simpler mechanisms on larger problems proved futile.
For instance, our method takes under one minute to solve a problem that, in the standard
CA, requires the agents to enumerate up to 2100 bundles and the auctioneer to solve an
NP-complete problem with an input of that size.

6. Generalizations, Extensions, and Conclusions
There are many possible extensions and generalizations of the work presented here, and we
briefly outline several below.
The treatment in this paper focused on the problem of resource allocation among
self-interested agents, but the algorithms also apply to cooperative MDPs with weaklyinteracting agents. In the cooperative setting, the concept of resources can be viewed as
a compact way to model inter-agent constraints and their inability to include some combinations of joint actions in their policies. Such weakly-coupled MDPs, where agents have
536

Resource Allocation Among Agents with MDP-Induced Preferences

independent transition and reward functions, but certain combinations of joint actions are
not feasible is a widely used model of agents’ interactions (e.g., Singh & Cohn, 1998). Our
model was resource-centric, but more direct models are also certainly possible. For example, agents can use SAT formulas to describe valid combinations of joint actions. This case
can be easily handled via simple modifications to the single and multiagent MILPs (15)
and (19). Indeed, any SAT formula can be expressed as a set of linear inequalities on
binary variables ∆(a) (or ∆m (a) in the multiagent case), which can be directly added to
the corresponding MILP (see the case of non-binary resources in Appendix B for an MILP
defined on indicators ∆(a), instead of the δ(o) used in the binary case).
As mentioned previously, our work can be extended to handle consumable resources that
are used up whenever agents execute actions. In fact, under some conditions, the problem
can be considerably simplified for domains with only these kinds of resources. The most
important change is that we have to redefine the value of a particular resource bundle to
an agent. The difficulty is that, given a policy, the total use of consumable resources is
uncertain, and the definition of the value of a resource bundle becomes ambiguous. One
possibility is to define the value of a bundle as the payoff of the best policy whose expected
resource usage does not exceed the amounts of resources in the bundle. The interpretation
of ρm (a, o) would also change to mean the amount of resource o consumed by action a
every time it is executed. This would make the constraints in (19) linear in the occupation
measure, which would tremendously simplify the WDP (making it polynomial). This is
analogous to the models used in constrained MDPs (Altman & Shwartz, 1991), as briefly
described earlier in Section 2. Information privacy can be handled similarly to the case of
non-consumable resources. However, given the transformation y = Dx, the resource cost
function ρm will also have to be scaled by D−1 (since the total consumption of consumable
resources is proportional to the occupation measure). This has the additional benefit of
hiding the resource cost functions (unlike the case of non-consumable resources where they
were revealed). A more detailed treatment of the model with consumable resources is
presented in the work by Dolgov (2006), including a discussion of risk-sensitive cases, where
the value of a resource bundle is defined as the payoff of the best policy whose probability
of exceeding the resource amounts is bounded.
In this work we exploited structure in agents’ preferences that stems from the underlying
policy-optimization problems. However, the latter were modeled using “flat” MDPs that
enumerate all possible states and actions. Such flat MDPs do not scale well due to the
curse of dimensionality (Bellman, 1961). To address this, the WDP MILP can be modified
to work with factored MDPs (Boutilier, Dearden, & Goldszmidt, 1995) by using a factored
resource-allocation algorithm (Dolgov & Durfee, 2006), which is based on the dual ALP
method for solving factored MDPs as developed by Guestrin (2003). This method allows us
to exploit both types of structure in the resource-allocation algorithms: structure in agents’
preferences induced by the underlying MDPs, as well as structure in MDPs themselves.
The resource-allocation mechanism discussed in this paper assumed a one-shot allocation
of resources and a static population of agents. An interesting extension of our work would
be to consider a system where agents and resources arrive and depart dynamically, as in the
online mechanism design work (Parkes & Singh, 2003; Parkes, Singh, & Yanovsky, 2004).
Combining the MDP-based model of utility functions with the dynamics of online problems
could be a valuable result and thus appears to be a worthwhile direction of future work. If
537

Dolgov & Durfee

the agent population is static, but a periodic re-allocation of resources is allowed, techniques
like phasing can be used to solve the resulting problem (Wu & Durfee, 2005).
To summarize the results of this paper, we presented a variant of a combinatorial auction for resource allocation among self-interested agents whose valuations of resource bundles
are defined by their weakly-coupled constrained MDPs. For such problems, our mechanism,
which exploits knowledge of the structure of agents’ MDP-based preferences, achieves an
exponential reduction in the number of integer decision variables, which in turn leads to
tremendous speedup over a straightforward implementation, as confirmed by our experimental results. Our mechanism can be implemented to achieve its reduction in computational
complexity without sacrificing any of the nice properties of a VCG mechanism (optimal outcomes, strategy-proofness, and voluntary participation). We also discussed a distributed
implementation of the mechanism that retains strategy-proofness (using the fact that an
LP solution can be easily verified), and does not reveal agents’ private MDP information
(using a transformation of agents’ MDPs).
We believe that the models and solution algorithms described in this paper significantly
further the applicability of combinatorial resource-allocation mechanisms to practical problems, where the utility functions for resource bundles are defined by sequential stochastic
decision-making problems.

7. Acknowledgments
We thank the anonymous reviewers for their helpful comments, as well as our colleagues
Satinder Singh, Kang Shin, Michael Wellman, Demothenis Teneketsis, Jianhui Wu, and
Jeffrey Cox for the valuable discussions related to this work.
This material is based in part upon work supported by Honeywell International, and
by the DARPA IPTO COORDINATORs program and the Air Force Research Laboratory
under Contract No. FA8750–05–C–0030. The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official
policies, either expressed or implied, of the Defense Advanced Research Projects Agency or
the U.S. Government.

Appendix A. Proofs
A.1 Proof of Theorem 1
Theorem 1 Consider a finite set of n indivisible resources O = {oi } (i ∈ [1, n]), with
m ∈ N available units of each resource. Then, for any non-decreasing utility function
defined over resource bundles f : [0, m]n 7→ R, there exists a resource-constrained MDP
hS, A, p, r, O, ρ, C, κ, κ
b, αi (with the same resource set O) whose induced utility function over
the resource bundles is the same as f . In other words, for every resource bundle z ∈ [0, m]n ,
the value of the optimal policy among those whose resource requirements do not exceed z
(call this set Π(z)) is the same as f (z):
∀f q : [0, m]n 7→ R, ∃ hS, A, p, r, O, ρ, C, κ, κ
b, αi :
h
i
o
n 
X


∀ z ∈ [0, m]n , Π(z) = π  max ρ(a, oi )H
π(s, a) ≤ zi =⇒ max Uγ (π, α) = f (z).
a

s

538

π∈Π(z)

Resource Allocation Among Agents with MDP-Induced Preferences

s000
a11
s100 a
0
a21

a11

a21

a0
r=f’(0,0,0)

a31

s010 a
0
a31

s110 a0
a31

a11

s001
a31

s101 a0
a21

a21

a0
r=f’(0,0,1)
a0
r=f’(0,1,1)

s0

a0
r=0

s011

a11

a0
r=f’(1,1,1)

s111

Figure 10: Creating an MDP with resources for an arbitrary non-decreasing utility function.
The case shown has three binary resources. All transitions are deterministic.

Proof: This statement can be shown via a straightforward construction of an MDP that
has an exponential number (one per resource bundle) of states or actions. Below we present
a reduction with a linear number of actions and an exponential number of states. Our choice
is due to the fact that, although the reverse mapping requiring two states and exponentially
many actions is even more straightforward, such an MDP feels somewhat unnatural.
Given an arbitrary non-decreasing utility function f , a corresponding MDP can be
constructed as follows (illustrated in Figure 10 for n = 3 and m = 1). The state space S of
the MDP consists of (m+1)n +1 states – one state (sz ) for every resource bundle z ∈ [0, m]n ,
plus a sink state (s0 ).
S
The action space of the MDP A = a0 {aij }, i ∈ [1, n], j ∈ [1, m] consists of mn + 1
actions: m actions per each resource oi , i ∈ [1, n], plus an additional action a0 .
The transition function p is deterministic and is defined as follows. Action a0 is applicable in every state and leads to the sink state s0 . Every other action aij is applicable in
states sz , where zi = (j − 1) and leads with certainty to the states where zi = j:

0

1 a = aij , s = sz , σ = sz0 , zi = (j − 1), zi = j,
p(σ|s, a) = 1 a = a0 , σ = s0 ,


0 otherwise.
In other words, aij only applies in states that have j − 1 units of resource i and leads to the
state where the amount of ith resource increases to j.
The reward function r is defined as follows. There are no rewards in state s0 , and
action a0 is the only action that produces rewards in other states:
(
f 0 (z) a = ao , s = sz , ∀z ∈ [0, m]n
r(s, a) =
0
otherwise,
where f 0 is a simple transformation of f that compensates for the effects of discounting:
f 0 (z) = f (z)(γ)−
539

P

i zi

.

Dolgov & Durfee

P
In other words, it takes i zi transitions to get to state sz , so the contribution of the above
into the total discounted reward will be exactly f (z).
The resource requirements ρ of actions are as follows: action a0 does not require any
resources, while every other action aij requires j units of resource oi .
Finally, the initial conditions are α(sz=0 ) = 1, meaning that the agent always starts
in the state that corresponds to the empty resource bundle (state s000 in Figure 10). The
capacity costs κ and limits κ
b are not used, so we set C = ∅.
It is easy to see that in the MDP constructed above, given a resource bundle z, any
policy from the feasible set Π(z) has zero probability of reaching any state sz0 for which
z0 > z (for any component i). Furthermore, an optimal policy from the set Π(z) will be to
transition to state sz (since f (z) is non-decreasing) and then use action a0 , thus obtaining
a total discounted reward of f (z).

A.2 Proof of Theorem 2
Theorem 2 Given an MDP M = hS, A, p, r, O, ρ, C, κ, κ
b, αi with resource and capacity
constraints, if there exists a policy π ∈ ΠHR that is a feasible solution for M , there exists a
stationary deterministic policy π SD ∈ ΠSD that is also feasible, and the expected total reward
of π SD is no less than that of π:
∀ π ∈ ΠHR , ∃ π SD ∈ ΠSD : Uγ (π SD , α) ≥ Uγ (π, α)
Proof: Let us label A0 ⊆ A the set of all actions that have a non-zero probability of being
executed according to π, i.e.,
A0 = {a|∃s : π(s, a) > 0}
Let us also construct an unconstrained MDP: M 0 = hS, A0 , p0 , r0 i, where p0 and r0 are the
restricted versions of p and r with the action domain limited to A0 :
p0 : S × A0 × S 7→ [0, 1]
r0 : S × A0 7→ R
p0 (σ|s, a) = p(σ|s, a), r0 (s, a) = r(s, a) ∀s ∈ S, σ ∈ S, a ∈ A0
Due to a well-known property of unconstrained infinite-horizon MDPs with the total
expected discounted reward optimization criterion, M 0 is guaranteed to have an optimal
stationary deterministic solution (e.g., Theorem 6.2.10, Puterman, 1994), which we label
π SD .
Consider π SD as a potential solution to M . Clearly, π SD is a feasible solution, because
its actions come from the set A0 that includes actions that π uses with non-zero probability,
which means that the resource requirements (as in (9)) of π SD can be no greater than those
of π. Indeed:
n
n
X
X
o
o
max0 ρ(a, o)H
π(s, a) ,
(30)
π SD (s, a) ≤ max0 ρ(a, o) = max ρ(a, o)H
a∈A

s

a∈A

a∈A

s

where the first inequality is due to the fact that H(z) ≤ 1 ∀z, and the second equality
follows from the definition of A0 .
540

Resource Allocation Among Agents with MDP-Induced Preferences

s1

a1:
r=v(u1),
o a1,o1)=1
o1)=c(z1)

a2:
v(u2),
a
,o
)=1
o 2 2
o2)=c(z2)

a :

r=

s2

a0:
r=0,
a ,.)=0
o o

r=
o

...

s3

a0:
r=0,
a ,.)=0
o o

sm

m
1-mv(u ),
m

am,om)=1
om)=c(zm)
a0:
r=0,
a ,.)=0
o o

sm+1

a0:
r= ,
=0
o

Figure 11: Reduction of KNAPSACK to M-OPER-CMDP. All transitions are deterministic.

Furthermore, observe that π SD yields the same total reward under M 0 and M . Additionally, since π SD is a uniformly optimal solution to M 0 , it is, in particular, optimal for
the initial conditions α of the constrained MDP M . Therefore, π SD constitutes a feasible
solution to M whose expected reward is greater than or equal to the expected reward of
any feasible policy π.

A.3 Proof of Theorem 3
Theorem 3 The following decision problem is NP-complete. Given an instance of an MDP
hS, A, p, r, O, ρ, C, κ, κ
b, αi with resources and capacity constraints, and a rational number Y ,
does there exist a feasible policy π, whose expected total reward, given α, is no less than Y ?
Proof: As shown in Theorem 2, there always exists an optimal policy for (9) that is
stationary and deterministic. Therefore, the presence in NP is obvious, since we can, in
polynomial time, guess a stationary deterministic policy, verify that it satisfies the resource
constraints, and calculate its expected total reward (the latter can be done by solving the
standard system of linear Markov equations (2) on the values of all states).
To show NP-completeness of the problem, we use a reduction from KNAPSACK (Garey
& Johnson, 1979). Recall that KNAPSACK in an NP-complete problem, which asks
whether, for a given set of items z ∈ Z, each of which has a cost c(z) and a value v(z),
there exists a subset Z 0 ⊆ Z such that the total value of all items in Z 0 is no less than
some
c, i.e.,
P constant vb, and
Pthe total cost of the items is no greater than another constant b
c(z)
≤
b
c
and
v(z)
≥
v
b
.
Our
reduction
is
illustrated
in
Figure
11
and
proceeds
0
0
z∈Z
z∈Z
as follows.
Given an instance of KNAPSACK with |Z| = m, let us number all items as zi , i ∈
[1, m] as a notational convenience. For such an instance of KNAPSACK, we create an
MDP with m + 1 states {s1 , s2 , . . . sm+1 }, m + 1 actions {a0 , . . . am }, m types of resources
O = {o1 , . . . om }, and a single capacity C = {c1 }.
The transition function on these states is defined as follows. Every state si , i ∈ [1, m]
has two transitions from it, corresponding to actions ai and a0 . Both actions lead to state
si+1 with probability 1. State sm+1 is absorbing and all transitions from it lead back to
itself.
The reward and the cost functions are defined as follows. We want action ai , i ∈ [1, m]
(which corresponds to item zi in KNAPSACK) to contribute v(zi ) to the total discounted
541

Dolgov & Durfee

reward. Hence, we set the immediate reward for every action ai to v(zi )(γ)1−i , which, given
that our transition function implies that state si is reached exactly at step i − 1, ensures
that if action ai is ever executed, its contribution to the total discounted reward will be
v(zi )(γ)1−i (γ)i−1 = v(zi ). Action a0 produces a reward of zero in all states.
The resource requirements of actions are defined as follows. Action ai , i ∈ [1, m] only
needs resource oi , i.e., ρ(ai , oj ) = 1 ⇐⇒ i = j. We set the cost of resource oi to be the
cost c(zi ) of item i in the KNAPSACK problem. The “null” action a0 requires no resources.
In order to complete the construction, we set the initial distribution α = [1, 0, . . .] so that
the agent starts in state s1 with probability 1. We also define the decision parameter Y = vb
and the upper bound on the single capacity κ
b=b
c.
Essentially, this construction allows the agent to choose action ai or a0 at every state si .
Choosing action ai is equivalent to putting item zi into the knapsack, while action a0
corresponds to the choice of not including zi in the knapsack. Therefore, there exists a
policy that has the expected payoff no less than Y = vb and uses no more than κ
b = b
c
of the shared resource if and only if there exists a solution to the original instance of
KNAPSACK.


Appendix B. Non-binary Resource Requirements
Below we describe an MILP formulation of the capacity-constrained single-agent optimization problem (9) for arbitrary resource costs ρ : A × O 7→ R, as opposed to binary costs
that were assumed in the main parts of the paper. The corresponding multiagent winnerdetermination problem (the non-binary equivalent of (19)) follows immediately from the
single-agent MILP.
For arbitrary resource costs, we obtain the following non-binary equivalent of the optimization problem (12) in the occupation measure coordinates:
max
x

XX
s

x(s, a)r(s, a)

a

subject to:
X
XX
x(σ, a) − γ
x(s, a)p(σ|s, a) = α(σ),
a

X

s

∀σ ∈ S;

a

n
X
o
κ(o, c) max ρ(a, o)H
x(s, a) ≤ κ
b(c),
a

o

∀c ∈ C;

s

x(s, a) ≥ 0,

∀s ∈ S, a ∈ A.

To linearize the sum of max operators in (31), let us observe that the inequality
n
X
i

g(ui ) max f (z, ui ) = g(u1 ) max f (z, u1 ) + . . . + g(un ) max f (z, un ) ≤ a
z∈Z

z∈Z

z∈Z

is equivalent to the following system of |Z|n linear inequalities:
g(u1 )f (z1 , u1 ) + g(u2 )f (z2 , u2 ) + . . . + g(un )f (zn , un ) ≤ a,
542

∀z1 , z2 , . . . zn ∈ Z.

(31)

Resource Allocation Among Agents with MDP-Induced Preferences

Applying this to the constraints from (31), we can express the original system of |C| nonlinear
constraints (each of which has a max):
X

n
X
o
κ(o, c) max ρ(a, o)H
x(s, a) ≤ κ
b(c),
a

o

∀c ∈ C

s

as the following system of |C||A||O| constraints where the max is removed:
X

κ(o, c)ρ(ao , o)H

X

o


x(s, a) ≤ κ
b(c),

∀c ∈ C, ao1 , ao2 , . . . ∈ A.

(32)

s

Notice that this way of eliminating the maximization exponentially increases the number
of constraints, because the above expansion enumerates all possible actions for each resource
(i.e., it enumerates policies where each resource o is used by action a1 , where it is used by
action a2 , action a3 , etc.) However, in many problems not all resources are used by all
actions. In such cases, most of the above constraints
Q become redundant, and the number of
constraints can be reduced from |C||A||O| to |C| o |Ao |, where Ao is the number of actions
that use resource o.
We can linearize the Heaviside function analogously to the case of binary resource costs
in Section 3.2: we create a binary indicator variable that corresponds to the argument of
H() and tie it to the occupation measure x via linear inequalities. The only difference is that
for non-binary resource costs, instead of using
P indicators on resources, we use indicators on
actions: ∆(a) ∈ {0, 1}, where ∆(a) = H( s x(s, a)) is an indicator that shows whether
action a is used in the policy. Using ∆ and expanding the max as above, we can represent
the optimization problem (9) as the following MILP:
max
x,∆

XX
s

x(s, a)r(s, a)

a

subject to:
X
XX
x(σ, a) − γ
x(s, a)p(σ|s, a) = α(σ),
a

X

s

∀σ ∈ S;

a

κ(o, c)ρ(ao , o)∆(ao ) ≤ κ
b(c),

∀c ∈ C, ao1 , ao2 , . . . ∈ A;

(33)

o

X

x(s, a)/X ≤ ∆(a),

∀a ∈ A;

s

x(s, a) ≥ 0,

∀s ∈ S, a ∈ A;

∆(a) ∈ {0, 1},

∀a ∈ A,

P
where X ≥ max s x(s, a) is some constant finite upper bound on the expected number
of times action a is used,
P which exists for any discounted MDP. We can, for example, let
X = (1 − γ)−1 , since s,a x(s, a) = (1 − γ)−1 for any x that is a valid occupation measure
for an MDP with discount factor γ.
Example 9 Let us formulate the MILP for the constrained problem from Example 3. Recall
that there are three resources O = {ot , of , om } (truck, forklift, and mechanic), one capacity
543

Dolgov & Durfee

type C = {c1 } (money), and actions have the following resource requirements (listing only
the nonzero ones):
ρ(a1 , ot ) = 1, ρ(a2 , ot ) = 1, ρ(a2 , of ) = 1, ρ(a3 , ot ) = 1, ρ(a4 , ot ) = 1, ρ(a4 , om ) = 1
The resources have the following capacity costs:
κ(ot , c1 ) = 2, κ(of , c1 ) = 3, κ(om , c1 ) = 4,
and the agent has a limited budget, i.e., a capacity bound κ
b(c1 ) = 8.
To compute the optimal policy for an arbitrary α, we can formulate the problem as an
MILP using the techniques described above. Using binary variables {∆(ai )} = {∆i } =
{∆1 , ∆2 , ∆3 , ∆4 },14 we can express the constraint on capacity cost as the following system
of |C||A||O| = 1(4)3 = 64 linear constraints:
(2)(1)∆1 + (3)(0)∆1 + (4)(0)∆1 ≤ 8,
(2)(1)∆1 + (3)(0)∆1 + (4)(0)∆2 ≤ 8,
(2)(1)∆1 + (3)(0)∆1 + (4)(0)∆3 ≤ 8,
(2)(1)∆1 + (3)(0)∆1 + (4)(1)∆4 ≤ 8,
(2)(1)∆1 + (3)(1)∆2 + (4)(0)∆1 ≤ 8,
...
(2)(0)∆4 + (3)(0)∆4 + (4)(1)∆4 ≤ 8.
It is easy to see that most of these constraints are redundant, and the fact that each action
only requires a small subset of the resources allows us to prune many of the constraints. In
fact, the only resource that is used byQmultiple actions is ot . Therefore, in accordance with
our earlier discussion, we only need o |Ao | = 1 × 4 × 1 = 4 constraints:
(2)(1)∆1 + (3)(1)∆2 + (4)(1)∆4 ≤ 8,
(2)(1)∆2 + (3)(1)∆2 + (4)(1)∆4 ≤ 8,
(2)(1)∆3 + (3)(1)∆2 + (4)(1)∆4 ≤ 8,
(2)(1)∆4 + (3)(1)∆2 + (4)(1)∆4 ≤ 8,
where each of the four constraints corresponds to a case where the first resource (ot ) is used
by a different action.
As mentioned earlier, we can set X = (1 − γ)−1 for the constraints that synchronize the
occupation measure x and the binary indicators ∆. Combining this with other constraints
Q
from (33), we get an MILP with 12 continuous and 4 binary variables, and |S|+|C| o |Ao |+
|A| = 3 + 4 + 3 = 10 constraints (not counting the last two sets of range constraints).

Finally, let us observe that by expanding the resource and action sets, any problem
can be represented using binary resources only. If the domain contains mostly binary
requirements, it may be more effective to expand the non-binary resource requirements ρ
by augmenting the resource set O, and then use the binary formulation of Section 3.2 rather
than directly applying the more-general formulation described above.
14. We do not create a ∆0 for the noop action a0 , as its resource costs are zero, and it drops out of all
expressions.

544

Resource Allocation Among Agents with MDP-Induced Preferences

Appendix C. Experimental Setup
This appendix details how our experimental domains were constructed. For a delivery
domain with |M| agents operating on an n-by-n grid and sharing |O| resource types, we
used the following parameters.
The resources enable agents to carry out delivery tasks. For a problem with |O| resource
types, there are |O| delivery actions, and performing action i ∈ [1, |O|] requires a random
subset of resources from O (where the number of resources required by an action is an
important parameter, whose effect on complexity is discussed in Section 5). The probability
that task i ∈ [1, |O|] can be carried at a location is 0.1+0.4(|O|−i)/(|O|−1), i.e., uniformly
distributed between 0.1 and 0.5, as a function of the action ID (actions with lower IDs are
more rewarding, per the definition of the reward function below, but can be executed at
fewer locations).
There are n2 /5 possible delivery locations randomly placed on the grid. Each delivery
location is assigned a set of delivery tasks that can be executed there (a single location can
be used for multiple delivery tasks, and a single task can be carried out at any of several
locations). The assignment of tasks to locations is done randomly.
Each agent has 4 + |O| actions: drive in any of the four perpendicular directions and
execute one of the delivery tasks. The drive actions result in movement in the intended
direction with probability of 0.8 and with probability of 0.2 produce no change of location.
All movement actions incur a negative reward, the amount of which depends on the size of
the agent. For a problem with |M| agents, the movement penalty incurred by agent m ∈
[1, |M|] is −1 − 9(m − 1)/(|M| − 1), i.e., distributed uniformly on [−1, −10] as a function
of the agent’s ID.
Execution of an action corresponding to a delivery task i ∈ [1, |O|] in a location to which
the task is assigned produces a reward of 100i/|O| and moves the agent to a new random
location on the grid. The new location is chosen randomly at problem generation (thus
known to agent), but the transition is deterministic, which induces a topology with nearby
and remote locations. Attempting execution of a delivery task in an incorrect location does
not change state and produces zero reward.
The agents bid for delivery resources of |O| types. There are cglob |M| units of each
resource, where cglob is the global constraint level (set to 0.5 for most of our experiments,
as described in more detail in Section 5). There is one capacity type: size. The size
requirements for making deliveries of type i ∈ [1, |O|] is i. The capacity limit of agent m
is cloc /2|O|(|O| + 1), where cloc is the local constraint level (set to 0.5 for most of our
experiments, as was described in more detail in Section 5).
The initial location of each agent is randomly selected from a uniform distribution. The
discount factor is γ = 0.95.

References
Altman, E. (1996). Constrained Markov decision processes with total cost criteria: Occupation measures and primal LP. Methods and Models in Operations Research, 43 (1),
45–72.
545

Dolgov & Durfee

Altman, E., & Shwartz, A. (1991). Adaptive control of constrained Markov chains: Criteria and policies. Annals of Operations Research, special issue on Markov Decision
Processes, 28, 101–134.
Altman, E. (1999). Constrained Markov Decision Processes. Chapman and HALL/CRC.
Bellman, R. (1961). Adaptive Control Processes: A Guided Tour. Princeton University
Press.
Benazera, M. E., Brafman, R. I., Meuleau, N., & Hansen, E. (2005). Planning with continuous resources in stochastic domains. In Proceedings of the Nineteenth International
Joint Conference on Artificial Intelligence (IJCAI-05), pp. 1244–1251.
Bererton, C., Gordon, G., & Thrun, S. (2003). Auction mechanism design for multi-robot
coordination. In Thrun, S., Saul, L., & Schölkopf, B. (Eds.), Proceedings of Conference
on Neural Information Processing Systems (NIPS). MIT Press.
Bertsimas, D., & Tsitsiklis, J. N. (1997). Introduction to Linear Optimization. Athena
Scientific.
Boutilier, C. (2002). Solving concisely expressed combinatorial auction problems. In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-02),
pp. 359–366.
Boutilier, C., Dearden, R., & Goldszmidt, M. (1995). Exploiting structure in policy construction. In Proceedings of the Fourteenth International Joint Conference on Artificial
Intelligence (IJCAI-95), pp. 1104–1111.
Boutilier, C., & Hoos, H. H. (2001). Bidding languages for combinatorial auctions. In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence
(IJCAI-01), pp. 1211–1217.
Clarke, E. H. (1971). Multipart pricing of public goods. Public Choice, 18, 19–33.
de Vries, S., & Vohra, R. V. (2003). Combinatorial auctions: A survey. INFORMS Journal
on Computing, 15 (3), 284–309.
Dolgov, D. (2006). Integrated Resource Allocation and Planning in Stochastic Multiagent
Environments. Ph.D. thesis, Computer Science Department, University of Michigan.
Dolgov, D. A., & Durfee, E. H. (2004). Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes. In Proceedings of the Fourteenth
International Conference on Automated Planning and Scheduling (ICAPS-04), pp.
315–324.
Dolgov, D. A., & Durfee, E. H. (2006). Resource allocation among agents with preferences
induced by factored MDPs. In Proceedings of the Fifth International Joint Conference
on Autonomous Agents and Multiagent Systems (AAMAS-06), Hakodate, Japan.
Eckstein, J., Phillips, C., & Hart, W. (2000). Pico: An object-oriented framework for parallel
branch and bound. In Proceedings of the Workshop on Inherently Parallel Algorithms
in Optimization and Feasibility and their Applications.
Feigenbaum, J., & Shenker, S. (2002). Distributed algorithmic mechanism design: Recent
results and future directions. In Proceedings of the Sixths International Workshop
546

Resource Allocation Among Agents with MDP-Induced Preferences

on Discrete Algorithms and Methods for Mobile Computing and Communications, pp.
1–13. ACM Press, New York.
Feldmann, R., Gairing, M., Lucking, T., Monien, B., & Rode, M. (2003). Selfish routing in
non-cooperative networks: A survey. In Proceedings of the Twenty-Eights International
Symposium on Mathematical Foundations of Computer Science (MFCS-03), pp. 21–
45. Springer-Verlag.
Ferguson, D., Nikolaou, C., Sairamesh, J., & Yemini, Y. (1996). Economic models for allocating resources in computer systems. In Clearwater, S. (Ed.), Market-Based Control:
A Paradigm for Distributed Resource Allocation, pp. 156–183, Hong Kong. World
Scientific.
Garey, M. R., & Johnson, D. S. (1979). Computers and Intractability: A Guide to the Theory
of NP-Completeness. W. H. Freeman & Co.
Groves, T. (1973). Incentives in teams. Econometrica, 41 (4), 617–631.
Guestrin, C. (2003). Planning Under Uncertainty in Complex Structured Environments.
Ph.D. thesis, Computer Science Department, Stanford University.
Heyman, D. P., & Sobel, M. J. (1984). Volume II: Stochastic Models in Operations Research.
McGraw-Hill, New York.
Kallenberg, L. (1983). Linear Programming and Finite Markovian Control Problems. Math.
Centrum, Amsterdam.
Littman, M. L., Dean, T. L., & Kaelbling, L. P. (1995). On the complexity of solving Markov
decision problems. In Proceedings of the Eleventh Annual Conference on Uncertainty
in Artificial Intelligence (UAI–95), pp. 394–402, Montreal.
MacKie-Mason, J. K., & Varian, H. (1994). Generalized Vickrey auctions. Tech. rep.,
University of Michigan.
Mas-Colell, A., Whinston, M. D., & Green, J. R. (1995). Microeconomic Theory. Oxford
University Press, New York.
McAfee, R. P., & McMillan, J. (1996). Analyzing the airwaves auction. Journal of Economic
Perspectives, 10 (1), 159–75.
McMillan, J. (1994). Selling spectrum rights. Journal of Economic Perspectives, 8 (3),
145–62.
Meuleau, N., Hauskrecht, M., Kim, K.-E., Peshkin, L., Kaelbling, L., Dean, T., & Boutilier,
C. (1998). Solving very large weakly coupled Markov decision processes. In Proceedings
of the Fifteenth National Conference on Artificial Intelligence (AAAI-98), pp. 165–
172.
Nisan, N. (2000). Bidding and allocation in combinatorial auctions. In Electronic Commerce.
Parkes, D. (2001). Iterative Combinatorial Auctions: Achieving Economic and Computational Efficiency. Ph.D. thesis, Department of Computer and Information Science,
University of Pennsylvania.
Parkes, D. C., Kalagnanam, J. R., & Eso, M. (2001). Achieving budget-balance with
Vickrey-based payment schemes in exchanges. In Proc. 17th International Joint Conference on Artificial Intelligence (IJCAI-01), pp. 1161–1168.
547

Dolgov & Durfee

Parkes, D. C., & Shneidman, J. (2004). Distributed implementations of Vickrey-ClarkeGroves mechanisms. In Proceedings of the Third International Joint Conference on
Autonomous Agents and Multi Agent Systems (AAMAS-04), pp. 261–268.
Parkes, D. C., & Singh, S. (2003). An MDP-based approach to Online Mechanism Design. In
Proceedings of the Seventeenths Annual Conference on Neural Information Processing
Systems (NIPS-03).
Parkes, D. C., Singh, S., & Yanovsky, D. (2004). Approximately efficient online mechanism
design. In Proceedings of the Eighteenths Annual Conference on Neural Information
Processing Systems (NIPS-04).
Puterman, M. L. (1994). Markov Decision Processes. John Wiley & Sons, New York.
Ross, K., & Chen, B. (1988). Optimal scheduling of interactive and non-interactive traffic in
telecommunication systems. IEEE Transactions on Automatic Control, 33, 261–267.
Ross, K., & Varadarajan, R. (1989). Markov decision processes with sample path constraints: the communicating case. Operations Research, 37, 780–790.
Rothkopf, M. H., Pekec, A., & Harstad, R. M. (1998). Computationally manageable combinational auctions. Management Science, 44 (8), 1131–1147.
Sandholm, T., & Boutilier, C. (2006). Preference elicitation in combinatorial auctions. In
Cramton, Shoham, & Steinberg (Eds.), Combinatorial Auctions, chap. 10. MIT Press.
Sandholm, T. (1999). An algorithm for optimal winner determination in combinatorial
auctions. In Proceedings of the Sixteenth International Joint Conference on Artificial
Intelligence (IJCAI-99), pp. 542–547, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
Sandholm, T. (2002). Algorithm for optimal winner determination in combinatorial auctions. Artificial Intelligence, 135 (1-2), 1–54.
Shapley, L. S. (1953). Stochastic games. Proceedings of National Academy of Science, USA,
39, 1095–1100.
Sheffi, Y. (2004). Combinatorial auctions in the procurement of transportation services.
Interfaces, 34 (4), 245–252.
Singh, S., & Cohn, D. (1998). How to dynamically merge Markov decision processes. In
Jordan, M. I., Kearns, M. J., & Solla, S. A. (Eds.), Advances in Neural Information
Processing Systems, Vol. 10, pp. 1057–1063. The MIT Press.
Song, J., & Regan, A. (2002). Combinatorial auctions for transportation service procurement: The carrier perspective. Transportation Research Record, 1833, 40–46.
Vickrey, W. (1961). Counterspeculation, auctions and competitive sealed tenders. Journal
of Finance, 16, 8–37.
Wellman, M. P., Walsh, W. E., Wurman, P. R., & MacKie-Mason, J. K. (2001). Auction
protocols for decentralized scheduling. Games and Economic Behavior, 35, 271–303.
Wolsey, L. (1998). Integer Programming. John Wiley & Sons.
548

Resource Allocation Among Agents with MDP-Induced Preferences

Wu, J., & Durfee, E. H. (2005). Automated resource-driven mission phasing techniques for
constrained agents. In Proceedings of the Fourth International Joint Conference on
Autonomous Agents and Multiagent Systems (AAMAS-05), pp. 331–338.

549

Journal of Artificial Intelligence Research 27 (2006) 335-380

Submitted 05/06; published 11/06

Anytime Point-Based Approximations for Large POMDPs
Joelle Pineau

JPINEAU @ CS . MCGILL . CA

School of Computer Science
McGill University
Montréal QC, H3A 2A7 CANADA

Geoffrey Gordon

GGORDON @ CS . CMU . EDU

Machine Learning Department
Carnegie Mellon University
Pittsburgh PA, 15232 USA

Sebastian Thrun

THRUN @ STANFORD . EDU

Computer Science Department
Stanford University
Stanford CA, 94305 USA

Abstract
The Partially Observable Markov Decision Process has long been recognized as a rich framework for real-world planning and control problems, especially in robotics. However exact solutions in this framework are typically computationally intractable for all but the smallest problems.
A well-known technique for speeding up POMDP solving involves performing value backups at
specific belief points, rather than over the entire belief simplex. The efficiency of this approach,
however, depends greatly on the selection of points. This paper presents a set of novel techniques
for selecting informative belief points which work well in practice. The point selection procedure
is combined with point-based value backups to form an effective anytime POMDP algorithm called
Point-Based Value Iteration (PBVI). The first aim of this paper is to introduce this algorithm and
present a theoretical analysis justifying the choice of belief selection technique. The second aim of
this paper is to provide a thorough empirical comparison between PBVI and other state-of-the-art
POMDP methods, in particular the Perseus algorithm, in an effort to highlight their similarities and
differences. Evaluation is performed using both standard POMDP domains and realistic robotic
tasks.

1. Introduction
The concept of planning has a long tradition in the AI literature (Fikes & Nilsson, 1971; Chapman,
1987; McAllester & Roseblitt, 1991; Penberthy & Weld, 1992; Blum & Furst, 1997). Classical
planning is generally concerned with agents which operate in environments that are fully observable,
deterministic, finite, static, and discrete. While these techniques are able to solve increasingly
large state-space problems, the basic assumptions of classical planning—full observability, static
environment, deterministic actions—make these unsuitable for most robotic applications.
Planning under uncertainty aims to improve robustness by explicitly reasoning about the type of
uncertainty that can arise. The Partially Observable Markov Decision Process (POMDP) (Ästrom,
1965; Sondik, 1971; Monahan, 1982; White, 1991; Lovejoy, 1991b; Kaelbling, Littman, & Cassandra, 1998; Boutilier, Dean, & Hanks, 1999) has emerged as possibly the most general representation
for (single-agent) planning under uncertainty. The POMDP supersedes other frameworks in terms
c
2006
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

P INEAU , G ORDON & T HRUN

of representational power simply because it combines the most essential features for planning under
uncertainty.
First, POMDPs handle uncertainty in both action effects and state observability, whereas many
other frameworks handle neither of these, and some handle only stochastic action effects. To handle partial state observability, plans are expressed over information states, instead of world states,
since the latter ones are not directly observable. The space of information states is the space of all
beliefs a system might have regarding the world state. Information states are easily calculated from
the measurements of noisy and imperfect sensors. In POMDPs, information states are typically
represented by probability distributions over world states.
Second, many POMDP algorithms form plans by optimizing a value function. This is a powerful approach to plan optimization, since it allows one to numerically trade off between alternative
ways to satisfy a goal, compare actions with different costs/rewards, as well as plan for multiple
interacting goals. While value function optimization is used in other planning approaches—for example Markov Decision Processes (MDPs) (Bellman, 1957)—POMDPs are unique in expressing
the value function over information states, rather than world states.
Finally, whereas classical and conditional planners produce a sequence of actions, POMDPs
produce a full policy for action selection, which prescribes the choice of action for any possible
information state. By producing a universal plan, POMDPs alleviate the need for re-planning, and
allow fast execution. Naturally, the main drawback of optimizing a universal plan is the computational complexity of doing so. This is precisely what we seek to alleviate with the work described
in this paper
Most known algorithms for exact planning in POMDPs operate by optimizing the value function
over all possible information states (also known as beliefs). These algorithms can run into the wellknown curse of dimensionality, where the dimensionality of planning problem is directly related to
the number of states (Kaelbling et al., 1998). But they can also suffer from the lesser known curse
of history, where the number of belief-contingent plans increases exponentially with the planning
horizon. In fact, exact POMDP planning is known to be PSPACE-complete, whereas propositional
planning is only NP-complete (Littman, 1996). As a result, many POMDP domains with only a few
states, actions and sensor observations are computationally intractable.
A commonly used technique for speeding up POMDP solving involves selecting a finite set
of belief points and performing value backups on this set (Sondik, 1971; Cheng, 1988; Lovejoy,
1991a; Hauskrecht, 2000; Zhang & Zhang, 2001). While the usefulness of belief point updates
is well acknowledged, how and when these backups should be applied has not been thoroughly
explored.
This paper describes a class of Point-Based Value Iteration (PBVI) POMDP approximations
where the value function is estimated based strictly on point-based updates. In this context, the
choice of points is an integral part of the algorithm, and our approach interleaves value backups
with steps of belief point selection. One of the key contributions of this paper is the presentation
and analysis of a set of heuristics for selecting informative belief points. These range from a naive
version that combines point-based value updates with random belief point selection, to a sophisticated algorithm that combines the standard point-based value update with an estimate of the error
bound between the approximate and exact solutions to select belief points. Empirical and theoretical evaluation of these techniques reveals the importance of taking distance between points into
consideration when selecting belief points. The result is an approach which exhibits good perfor336

A NYTIME P OINT-BASED A PPROXIMATIONS FOR L ARGE POMDP S

mance with very few belief points (sometimes less than the number of states), thereby overcoming
the curse of history.
The PBVI class of algorithms has a number of important properties, which are discussed at
greater length in the paper:
• Theoretical guarantees. We present a bound on the error of the value function obtained by
point-based approximation, with respect to the exact solution. This bound applies to a number
of point-based approaches, including our own PBVI, Perseus (Spaan & Vlassis, 2005), and
others.
• Scalability. We are able to handle problems on the order of 103 states, which is an order of magnitude larger than problems solved by more traditional POMDP techniques. The
empirical performance is evaluated extensively in realistic robot tasks, including a search-formissing-person scenario.
• Wide applicability. The approach makes few assumptions about the nature or structure of the
domain. The PBVI framework does assume known discrete state/ action/observation spaces
and a known model (i.e., state-to-state transitions, observation probabilities, costs/rewards),
but no additional specific structure (e.g., constrained policy class, factored model).
• Anytime performance. An anytime solution can be achieved by gradually alternating phases
of belief point selection and phases of point-based value updates. This allows for an effective
trade-off between planning time and solution quality.
While PBVI has many important properties, there are a number of other recent POMDP approaches which exhibit competitive performance (Braziunas & Boutilier, 2004; Poupart & Boutilier,
2004; Smith & Simmons, 2004; Spaan & Vlassis, 2005). We provide an overview of these techniques in the later part of the paper. We also provide a comparative evaluation of these algorithms
and PBVI using standard POMDP domains, in an effort to guide practitioners in their choice of
algorithm. One of the algorithms, Perseus (Spaan & Vlassis, 2005), is most closely related to PBVI
both in design and in performance. We therefore provide a direct comparison of the two approaches
using a realistic robot task, in an effort to shed further light on the comparative strengths and weaknesses of these two approaches.
The paper is organized as follows. Section 2 begins by exploring the basic concepts in POMDP
solving, including representation, inference, and exact planning. Section 3 presents the general
anytime PBVI algorithm and its theoretical properties. Section 4 discusses novel strategies to select good belief points. Section 6 presents an empirical comparison of POMDP algorithms using
standard simulation problems. Section 7 pursues the empirical evaluation by tackling complex robot
domains and directly comparing PBVI with Perseus. Finally, Section 5 surveys a number of existing
POMDP approaches that are closely related to PBVI.

2. Review of POMDPs
Partially Observable Markov Decision Processes provide a general planning and decision-making
framework for acting optimally in partially observable domains. They are well-suited to a great
number of real-world problems where decision-making is required despite prevalent uncertainty.
They generally assume a complete and correct world model, with stochastic state transitions, imperfect state tracking, and a reward structure. Given this information, the goal is to find an action
337

P INEAU , G ORDON & T HRUN

strategy which maximizes expected reward gains. This section first establishes the basic terminology and essential concepts pertaining to POMDPs, and then reviews optimal techniques for POMDP
planning.
2.1 Basic POMDP Terminology
Formally, a POMDP is defined by six distinct quantities, denoted {S, A, Z, T, O, R}. The first three
of these are:
• States. The state of the world is denoted s, with the finite set of all states denoted by S =
{s0 , s1 , . . .}. The state at time t is denoted st , where t is a discrete time index. The state is
not directly observable in POMDPs, where an agent can only compute a belief over the state
space S.
• Observations. To infer a belief regarding the world’s state s, the agent can take sensor measurements. The set of all measurements, or observations, is denoted Z = {z0 , z1 , . . .}. The
observation at time t is denoted zt . Observation zt is usually an incomplete projection of the
world state st , contaminated by sensor noise.
• Actions. To act in the world, the agent is given a finite set of actions, denoted A =
{a0 , a1 , . . .}. Actions stochastically affect the state of the world. Choosing the right action as
a function of history is the core problem in POMDPs.
Throughout this paper, we assume that states, actions and observations are discrete and finite.
For mathematical convenience, we also assume that actions and observations are alternated over
time.
To fully define a POMDP, we have to specify the probabilistic laws that describe state transitions
and observations. These laws are given by the following distributions:
• The state transition probability distribution,
T (s, a, s0 ) := P r(st = s0 | st−1 = s, at−1 = a) ∀t,

(1)

is the probability of transitioning to state s0 , given that the agent is in state s and selects action a, for any (s, a, s0 ). Since T is a conditional probability distribution, we have
P
0
s0 ∈S T (s, a, s ) = 1, ∀(s, a). As our notation suggests, T is time-invariant.
• The observation probability distribution,
O(s, a, z) := P r(zt = z | st−1 = s, at−1 = a) ∀t,

(2)

is the probability that the agent will perceive observation z upon executing action a in state s.
P
This conditional probability is defined for all (s, a, z) triplets, for which z∈Z O(s, a, z) =
1, ∀(s, a). The probability function O is also time-invariant.
Finally, the objective of POMDP planning is to optimize action selection, so the agent is given
a reward function describing its performance:
338

A NYTIME P OINT-BASED A PPROXIMATIONS FOR L ARGE POMDP S

• The reward function. R(s, a) : S × A −→ <, assigns a numerical value quantifying the
utility of performing action a when in state s. We assume the reward is bounded, Rmin <
R < Rmax . The goal of the agent is to collect as much reward as possible over time. More
precisely, it wants to maximize the sum:
E[

T
X

γ t−t0 rt ],

(3)

t=t0

where rt is the reward at time t, E[ ] is the mathematical expectation, and γ where 0 ≤ γ < 1
is a discount factor, which ensures that the sum in Equation 3 is finite.
These items together, the states S, actions A, observations Z, reward R, and the probability
distributions, T and O, define the probabilistic world model that underlies each POMDP.
2.2 Belief Computation
POMDPs are instances of Markov processes, which implies that the current world state, st , is sufficient to predict the future, independent of the past {s0 , s1 , ..., st−1 }. The key characteristic that
sets POMDPs apart from many other probabilistic models (such as MDPs) is the fact that the state
st is not directly observable. Instead, the agent can only perceive observations {z1 , . . . , zt }, which
convey incomplete information about the world’s state.
Given that the state is not directly observable, the agent can instead maintain a complete trace
of all observations and all actions it ever executed, and use this to select its actions. The action/observation trace is known as a history. We formally define
ht := {a0 , z1 , . . . , zt−1 , at−1 , zt }

(4)

to be the history at time t.
This history trace can get very long as time goes on. A well-known fact is that this history
does not need to be represented explicitly, but can instead be summarized via a belief distribution (Ästrom, 1965), which is the following posterior probability distribution:
bt (s) := P r(st = s | zt , at−1 , zt−1 , . . . , a0 , b0 ).

(5)

This of course requires knowing the initial state probability distribution:
b0 (s) := P r(s0 = s),

(6)

which defines the probability that the domain is in state s at time t = 0. It is common either to
specify this initial belief as part of the model, or to give it only to the runtime system which tracks
beliefs and selects actions. For our work, we will assume that this initial belief (or a set of possible
initial beliefs) are available to the planner.
Because the belief distribution bt is a sufficient statistic for the history, it suffices to condition
the selection of actions on bt , instead of on the ever-growing sequence of past observations and
actions. Furthermore, the belief bt at time t is calculated recursively, using only the belief one time
step earlier, bt−1 , along with the most recent action at−1 and observation zt .
339

P INEAU , G ORDON & T HRUN

We define the belief update equation, τ (), as:
τ (bt−1 , at−1 , zt ) = bt (s0 )
X

=

O(s0 , at−1 , zt ) T (s, at−1 , s0 ) bt−1 (s)

s0

P r(zt |bt−1 , at−1 )

(7)

where the denominator is a normalizing constant.
This equation is equivalent to the decades-old Bayes filter (Jazwinski, 1970), and is commonly
applied in the context of hidden Markov models (Rabiner, 1989), where it is known as the forward
algorithm. Its continuous generalization forms the basis of Kalman filters (Kalman, 1960).
It is interesting to consider the nature of belief distributions. Even for finite state spaces, the
belief is a continuous quantity. It is defined over a simplex describing the space of all distributions
over the state space S. For very large state spaces, calculating the belief update (Eqn 7) can be computationally challenging. Recent research has led to efficient techniques for belief state computation
that exploit structure of the domain (Dean & Kanazawa, 1988; Boyen & Koller, 1998; Poupart &
Boutilier, 2000; Thrun, Fox, Burgard, & Dellaert, 2000). However, by far the most complex aspect of POMDP planning is the generation of a policy for action selection, which is described next.
For example in robotics, calculating beliefs over state spaces with 106 states is easily done in realtime (Burgard et al., 1999). In contrast, calculating optimal action selection policies exactly appears
to be infeasible for environments with more than a few dozen states (Kaelbling et al., 1998), not
directly because of the size of the state space, but because of the complexity of the optimal policies.
Hence we assume throughout this paper that the belief can be computed accurately, and instead
focus on the problem of finding good approximations to the optimal policy.
2.3 Optimal Policy Computation
The central objective of the POMDP perspective is to compute a policy for selecting actions. A
policy is of the form:
π(b) −→ a,

(8)

where b is a belief distribution and a is the action chosen by the policy π.
Of particular interest is the notion of optimal policy, which is a policy that maximizes the expected future discounted cumulative reward:


π ∗ (bt0 ) = argmax Eπ 
π

T
X

t=t0

 


γ t−t0 rt bt0  .


(9)

There are two distinct but interdependent reasons why computing an optimal policy is challenging. The more widely-known reason is the so-called curse of dimensionality: in a problem with
n physical states, π is defined over all belief states in an (n − 1)-dimensional continuous space.
The less-well-known reason is the curse of history: POMDP solving is in many ways like a search
through the space of possible POMDP histories. It starts by searching over short histories (through
which it can select the best short policies), and gradually considers increasingly long histories. Unfortunately the number of distinct possible action-observation histories grows exponentially with
the planning horizon.
340

A NYTIME P OINT-BASED A PPROXIMATIONS FOR L ARGE POMDP S

The two curses—dimensionality and history—often act independently: planning complexity
can grow exponentially with horizon even in problems with only a few states, and problems with a
large number of physical states may still only have a small number of relevant histories. Which curse
is predominant depends both on the problem at hand, and the solution technique. For example, the
belief point methods that are the focus of this paper specifically target the curse of history, leaving
themselves vulnerable to the curse of dimensionality. Exact algorithms on the other hand typically
suffer far more from the curse of history. The goal is therefore to find techniques that offer the best
balance between both.
We now describe a straightforward approach to finding optimal policies by Sondik (1971). The
overall idea is to apply multiple iterations of dynamic programming, to compute increasingly more
accurate values for each belief state b. Let V be a value function that maps belief states to values in
<. Beginning with the initial value function:
V0 (b) = max
a

X

R(s, a)b(s),

(10)

s∈S

then the t-th value function is constructed from the (t − 1)-th by the following recursive equation:
"

Vt (b) = max
a

#
X

X

R(s, a)b(s) + γ

s∈S

P r(z | a, b)Vt−1 (τ (b, a, z)) ,

(11)

z∈Z

where τ (b, a, z) is the belief updating function defined in Equation 7. This value function update
maximizes the expected sum of all (possibly discounted) future pay-offs the agent receives in the
next t time steps, for any belief state b. Thus, it produces a policy that is optimal under the planning
horizon t. The optimal policy can also be directly extracted from the previous-step value function:
#

"

πt∗ (b)

= argmax
a

X

R(s, a)b(s) + γ

X

P r(z | a, b)Vt−1 (τ (b, a, z)) .

(12)

z∈Z

s∈S

Sondik (1971) showed that the value function at any finite horizon t can be expressed by a set
of vectors: Γt = {α0 , α1 , . . . , αm }. Each α-vector represents an |S|-dimensional hyper-plane, and
defines the value function over a bounded region of the belief:
X

Vt (b) = max
α∈Γt

α(s)b(s).

(13)

s∈S

In addition, each α-vector is associated with an action, defining the best immediate policy
assuming optimal behavior for the following (t − 1) steps (as defined respectively by the sets
{Vt−1 , ..., V0 }).
The t-horizon solution set, Γt , can be computed as follows. First, we rewrite Equation 11 as:


Vt (b) = max 
a∈A


X

s∈S

R(s, a)b(s) + γ

X
z∈Z

max

α∈Γt−1

XX
s∈S

T (s, a, s0 )O(s0 , a, z)α(s0 )b(s) . (14)

s0 ∈S

Notice that in this representation of Vt (b), the nonlinearity in the term P (z|a, b) from Equation 11
cancels out the nonlinearity in the term τ (b, a, z), leaving a linear function of b(s) inside the max
operator.
341

P INEAU , G ORDON & T HRUN

The value Vt (b) cannot be computed directly for each belief b ∈ B (since there are infinitely
many beliefs), but the corresponding set Γt can be generated through a sequence of operations on
the set Γt−1 .
a,z
The first operation is to generate intermediate sets Γa,∗
t and Γt , ∀a ∈ A, ∀z ∈ Z (Step 1):
← αa,∗ (s) = R(s, a)
Γa,∗
t
Γa,z
t

←

αia,z (s)

=γ

X

(15)
0

0

0

T (s, a, s )O(s , a, z)αi (s ), ∀αi ∈ Γt−1

s0 ∈S

where each αa,∗ and αia,z is once again an |S|-dimensional hyper-plane.
Next we create Γat (∀a ∈ A), the cross-sum over observations1 , which includes one αa,z from
each Γa,z
t (Step 2):
a,z1
2
⊕ ...
⊕ Γa,z
Γat = Γa,∗
t
t + Γt

(16)

Finally we take the union of Γat sets (Step 3):
Γt = ∪a∈A Γat .

(17)

This forms the pieces of the backup solution at horizon t. The actual value function Vt is
extracted from the set Γt as described in Equation 13.
Using this approach, bounded-time POMDP problems with finite state, action, and observation
spaces can be solved exactly given a choice of the horizon T . If the environment is such that the
agent might not be able to bound the planning horizon in advance, the policy πt∗ (b) is an approximation to the optimal one whose quality improves in expectation with the planning horizon t (assuming
0 ≤ γ < 1).
As mentioned above, the value function Vt can be extracted directly from the set Γt . An important aspect of this algorithm (and of all optimal finite-horizon POMDP solutions) is that the
value function is guaranteed to be a piecewise linear, convex, and continuous function of the belief (Sondik, 1971). The piecewise-linearity and continuous properties are a direct result of the fact
that Vt is composed of finitely many linear α-vectors. The convexity property is a result of the
a,∗
a
maximization operator (Eqn 13). It is worth pointing out that the intermediate sets Γa,z
t , Γt and Γt
also represent functions of the belief which are composed entirely of linear segments. This property
holds for the intermediate representations because they incorporate the expectation over observation
probabilities (Eqn 15).
In the worst case, the exact value update procedure described could require time doubly exponential in the planning horizon T (Kaelbling et al., 1998). To better understand the complexity
of the exact update, let |S| be the number of states, |A| the number of actions, |Z| the number of
observations, and |Γt−1 | the number of α-vectors in the previous solution set. Then Step 1 creates
|A| |Z| |Γt−1 | projections and Step 2 generates |A| |Γt−1 ||Z| cross-sums. So, in the worst case, the
new solution requires:
|Γt | = O(|A||Γt−1 ||Z| )

(18)

1. The symbol ⊕ denotes the cross-sum operator. A cross-sum operation is defined over two sets, A =
{a1 , a2 , . . . , am } and B = {b1 , b2 , . . . , bn }, and produces a third set, C = {a1 + b1 , a1 + b2 , . . . , a1 + bn , a2 +
b1 , a2 + b2 , . . . , . . . , am + bn }.

342

A NYTIME P OINT-BASED A PPROXIMATIONS FOR L ARGE POMDP S

α-vectors to represent the value function at horizon t; these can be computed in time
O(|S|2 |A| |Γt−1 ||Z| ).
It is often the case that a vector in Γt will be completely dominated by another vector over the
entire belief simplex:
αi · b < αj · b, ∀b.
(19)
Similarly, a vector may be fully dominated by a set of other vectors (e.g., α2 in Fig. 1 is dominated by the combination of α1 and α3 ). This vector can then be pruned away without affecting
the solution. Finding dominated vectors can be expensive. Checking whether a single vector is
dominated requires solving a linear program with |S| variables and |Γt | constraints. Nonetheless it
can be time-effective to apply pruning after each iteration to prevent an explosion of the solution
size. In practice, |Γt | often appears to grow singly exponentially in t, given clever mechanisms for
pruning unnecessary linear functions. This enormous computational complexity has long been a
key impediment toward applying POMDPs to practical problems.

V={ α 0 ,α 1 ,α 2 ,α 3 }

Figure 1: POMDP value function representation

2.4 Point-Based Value Backup
Exact POMDP solving, as outlined above, optimizes the value function over all beliefs. Many
approximate POMDP solutions, including the PBVI approach proposed in this paper, gain computational advantage by applying value updates at specific (and few) belief points, rather than over all
beliefs (Cheng, 1988; Zhang & Zhang, 2001; Poon, 2001). These approaches differ significantly
(and to great consequence) in how they select the belief points, but once a set of points is selected,
the procedure for updating their value is standard. We now describe the procedure for updating the
value function at a set of known belief points.
As in Section 2.3, the value function update is implemented as a sequence of operations on a
set of α-vectors. If we assume that we are only interested in updating the value function at a fixed
set of belief points, B = {b0 , b1 , ..., bq }, then it follows that the value function will contain at most
one α-vector for each belief point. The point-based value function is therefore represented by the
corresponding set {α0 , α1 , . . . , αq }.
Given a solution set Γt−1 , we simply modify the exact backup operator (Eqn 14) such that only
one α-vector per belief point is maintained. The point-based backup now gives an α-vector which
is valid over a region around b. It assumes that the other belief points in that region have the same
action choice and lead to the same facets of Vt−1 as the point b. This is the key idea behind all
algorithms presented in this paper, and the reason for the large computational savings associated
with this class of algorithms.
343

P INEAU , G ORDON & T HRUN

To obtain solution set Γt from the previous set Γt−1 , we begin once again by generating intera,z
mediate sets Γa,∗
t and Γt , ∀a ∈ A, ∀z ∈ Z (exactly as in Eqn 15) (Step 1):
Γa,∗
← αa,∗ (s) = R(s, a)
t
Γa,z
t

←

αia,z (s)

=γ

(20)
0

X

0

0

T (s, a, s )O(s , a, z)αi (s ), ∀αi ∈ Γt−1 .

s0 ∈S

Next, whereas performing an exact value update requires a cross-sum operation (Eqn 16), by
operating over a finite set of points, we can instead use a simple summation. We construct Γat , ∀a ∈
A (Step 2):
Γat ← αba = Γa,∗
t +

X

argmax(

a,z
z∈Z α∈Γt

X

α(s)b(s)), ∀b ∈ B.

(21)

s∈S

Finally, we find the best action for each belief point (Step 3):
αb = argmax(

X

Γa
t ,∀a∈A s∈S

Γat (s)b(s)), ∀b ∈ B.

Γt = ∪b∈B αb

(22)
(23)

While these operations preserve only the best α-vector at each belief point b ∈ B, an estimate
of the value function at any belief in the simplex (including b ∈
/ B) can be extracted from the set Γt
just as before:
X

Vt (b) = max
α∈Γt

α(s)b(s).

(24)

s∈S

To better understand the complexity of updating the value of a set of points B, let |S| be the
number of states, |A| the number of actions, |Z| the number of observations, and |Γt−1 | the number
of α-vectors in the previous solution set. As with an exact update, Step 1 creates |A| |Z| |Γt−1 |
projections (in time |S|2 |A| |Z| |Γt−1 |). Steps 2 and 3 then reduce this set to at most |B| components
(in time |S| |A| |Γt−1 | |Z| |B|). Thus, a full point-based value update takes only polynomial time,
and even more crucially, the size of the solution set Γt remains constant at every iteration. The
point-based value backup algorithm is summarized in Table 1.
Note that the algorithm as outlined in Table 1 includes a trivial pruning step (lines 13-14),
whereby we refrain from adding to Γt any vector already included in it. As a result, it is often the
case that |Γt | ≤ |B|. This situation arises whenever multiple nearby belief points support the same
vector. This pruning step can be computed rapidly (without solving linear programs) and is clearly
advantageous in terms of reducing the set Γt .
The point-based value backup is found in many POMDP solvers, and in general serves to improve estimates of the value function. It is also an integral part of the PBVI framework.

3. Anytime Point-Based Value Iteration
We now describe the algorithmic framework for our new class of fast approximate POMDP algorithms called Point-Based Value Iteration (PBVI). PBVI-class algorithms offer an anytime solution
to large-scale discrete POMDP domains. The key to achieving an anytime solution is to interleave
344

A NYTIME P OINT-BASED A PPROXIMATIONS FOR L ARGE POMDP S

Γt =BACKUP(B, Γt−1 )
For each action a ∈ A
For each observation z ∈ Z
For each solution vector αi ∈ Γt−1
P
αia,z (s) = γ s0 ∈S T (s, a, s0 )O(s0 , a, z)αi (s0 ), ∀s ∈ S
End
a,z
Γa,z
t = ∪i αi
End
End
Γt = ∅
For each belief point hb ∈ B
i
P
P
P
αb = argmaxa∈A
[
α(s)b(s)]
s∈S R(s, a)b(s) +
z∈Z maxα∈Γa,z
s∈S
t
If(αb ∈
/ Γt )
Γt = Γt ∪ αb
End
Return Γt

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

Table 1: Point-based value backup

two main components: the point-based update described in Table 1 and steps of belief set selection. The approximate value function we find is guaranteed to have bounded error (compared to the
optimal) for any discrete POMDP domain.
The current section focuses on the overall anytime algorithm and its theoretical properties, independent of the belief point selection process. Section 4 then discusses in detail various novel
techniques for belief point selection.
The overall PBVI framework is simple. We start with a (small) initial set of belief points to
which are applied a first series of backup operations. The set of belief points is then grown, a
new series of backup operations are applied to all belief points (old and new), and so on, until a
satisfactory solution is obtained. By interleaving value backup iterations with expansions of the
belief set, PBVI offers a range of solutions, gradually trading off computation time and solution
quality.
The full algorithm is presented in Table 2. The algorithm accepts as input an initial belief point
set (BInit ), an initial value (Γ0 ), the number of desired expansions (N ), and the planning horizon
(T ). A common choice for BInit is the initial belief b0 ; alternately, a larger set could be used,
especially in cases where sample trajectories are available. The initial value, Γ0 , is typically set to
min
be purposefully low (e.g., α0 (s) = R1−γ
, ∀s ∈ S). When we do this, we can show that the pointbased solution is always be a lower-bound on the exact solution (Lovejoy, 1991a). This follows
from the simple observation that failing to compute an α-vector can only lower the value function.
For problems with a finite horizon, we run T value backups between each expansion of the
belief set. In infinite-horizon problems, we select the horizon T so that
γ T [Rmax − Rmin ] < ,
where Rmax = maxs,a R(s, a) and Rmin = mins,a R(s, a).
345

(25)

P INEAU , G ORDON & T HRUN

The complete algorithm terminates once a fixed number of expansions (N ) have been completed. Alternately, the algorithm could terminate once the value function approximation reaches a
given performance criterion. This is discussed further below.
The algorithm uses the BACKUP routine described in Table 1. We can assume for the moment
that the EXPAND subroutine (line 8) selects belief points at random. This performs reasonably
well for small problems where it is easy to achieve good coverage of the entire belief simplex.
However it scales poorly to larger domains where exponentially many points are needed to guarantee
good coverage of the belief simplex. More sophisticated approaches to selecting belief points are
presented in Section 4. Overall, the PBVI framework described here offers a simple yet flexible
approach to solving large-scale POMDPs.
Γ=PBVI-MAIN(BInit , Γ0 , N , T )
B=BInit
Γ = Γ0
For N expansions
For T iterations
Γ =BACKUP(B,Γ)
End
Bnew =EXPAND(B,Γ)
B = B ∪ Bnew
End
Return Γ

1
2
3
4
5
6
7
8
9
10
11

Table 2: Algorithm for Point-Based Value Iteration (PBVI)
For any belief set B and horizon t, the algorithm in Table 2 will produce an estimate of the value
function, denoted VtB . We now show that the error between VtB and the optimal value function V ∗
is bounded. The bound depends on how densely B samples the belief simplex ∆; with denser
sampling, VtB converges to Vt∗ , the t-horizon optimal solution, which in turn has bounded error
with respect to V ∗ , the optimal solution. So cutting off the PBVI iterations at any sufficiently large
horizon, we can show that the difference between VtB and the optimal infinite-horizon V ∗ is not too
large. The overall error in PBVI is bounded, according to the triangle inequality, by:
kVtB − V ∗ k∞ ≤ kVtB − Vt∗ k∞ + kVt∗ − V ∗ k∞ .

(26)

The second term is bounded by γ t kV0∗ − V ∗ k (Bertsekas & Tsitsiklis, 1996). The remainder of this
section states and proves a bound on the first term, which we denote t .
Begin by assuming that H denotes an exact value backup, and H̃ denotes the PBVI backup.
Now define (b) to be the error introduced at a specific belief b ∈ ∆ by performing one iteration of
point-based backup:
(b) = |H̃V B (b) − HV B (b)|∞ .
Next define  to be the maximum total error introduced by doing one iteration of point-based backup:
 = |H̃V B − HV B |∞
= max (b).
b∈∆

346

A NYTIME P OINT-BASED A PPROXIMATIONS FOR L ARGE POMDP S

Finally define the density δB of a set of belief points B to be the maximum distance from any belief
in the simplex ∆ to a belief in set B. More precisely:
δB = max
min kb − b0 k1 .
0
b ∈∆ b∈B

(27)

Now we can prove the following lemma:
Lemma 1. The error introduced in PBVI when performing one iteration of value backup over B,
instead of over ∆, is bounded by
(Rmax − Rmin )δB
≤
1−γ
Proof: Let b0 ∈ ∆ be the point where PBVI makes its worst error in value update, and b ∈ B
be the closest (1-norm) sampled belief to b0 . Let α be the vector that is maximal at b, and α0 be the
vector that would be maximal at b0 . By failing to include α0 in its solution set, PBVI makes an error
of at most α0 · b0 − α · b0 . On the other hand, since α is maximal at b, then α0 · b ≤ α · b. So,
 ≤ α 0 · b 0 − α · b0
=
≤
=
≤
≤

α0 · b0 − α · b0 + (α0 · b − α0 · b)
α 0 · b0 − α · b 0 + α · b − α 0 · b
(α0 − α) · (b0 − b)
kα0 − αk∞ kb0 − bk1
kα0 − αk∞ δB

≤

(Rmax −Rmin )δB
1−γ

Add zero
Assume α is optimal at b
Re-arrange the terms
By Hölder inequality
By definition of δB

The last inequality holds because each α-vector represents the reward achievable starting from
some state and following some sequence of actions and observations. Therefore the sum of rewards
min
max
must fall between R1−γ
and R1−γ
.
Lemma 1 states a bound on the approximation error introduced by one iteration of point-based
value updates within the PBVI framework. We now look at the bound over multiple value updates.
Theorem 3.1. For any belief set B and any horizon t, the error of the PBVI algorithm t = kVtB −
Vt∗ k∞ is bounded by
(Rmax − Rmin )δB
t ≤
(1 − γ)2
Proof:
t = ||VtB − Vt∗ ||∞
B − HV ∗ ||
= ||H̃Vt−1
t−1 ∞

≤
≤
≤
=
≤

By definition of H̃

B − HV B || + ||HV B − HV ∗ ||
||H̃Vt−1
t−1 ∞
t−1
t−1 ∞
(Rmax −Rmin )δB
B
∗
+ ||HVt−1 − HVt−1 ||∞
1−γ
(Rmax −Rmin )δB
B − V ∗ ||
+ γ||Vt−1
t−1 ∞
1−γ

By triangle inequality

(Rmax −Rmin )δB
1−γ
(Rmax −Rmin )δB
(1−γ)2

By definition of t−1

+ γt−1

By lemma 1
By contraction of exact value backup

By sum of a geometric series
347

P INEAU , G ORDON & T HRUN

The bound described in this section depends on how densely B samples the belief simplex ∆.
In the case where not all beliefs are reachable, PBVI does not need to sample all of ∆ densely, but
¯ (Fig. 2). The error bounds and convergence results
can replace ∆ by the set of reachable beliefs ∆
0
¯
¯
hold on ∆. We simply need to re-define b ∈ ∆ in lemma 1.
As a side note, it is worth pointing out that because PBVI makes no assumption regarding
the initial value function V0B , the point-based solution V B is not guaranteed to improve with the
addition of belief points. Nonetheless, the theorem presented in this section shows that the bound
on the error between VtB (the point-based solution) and V ∗ (the optimal solution) is guaranteed
to decrease (or stay the same) with the addition of belief points. In cases where VtB is initialized
min
pessimistically (e.g., V0B (s) = R1−γ
, ∀s ∈ S, as suggested above), then VtB will improve (or stay
the same) with each value backup and addition of belief points.
This section has thus far skirted the issue of belief point selection, however the bound presented
in this section clearly argues in favor of dense sampling over the belief simplex. While randomly
selecting points according to a uniform distribution may eventually accomplish this, it is generally
inefficient, in particular for high dimensional cases. Furthermore, it does not take advantage of
the fact that the error bound holds for dense sampling over reachable beliefs. Thus we seek more
efficient ways to generate belief points than at random over the entire simplex. This is the issue
explored in the next section.

4. Belief Point Selection
In section 3, we outlined the prototypical PBVI algorithm, while conveniently avoiding the question
of how and when belief points should be selected. There is a clear trade-off between including fewer
beliefs (which would favor fast planning over good performance), versus including many beliefs
(which would slow down planning, but ensure a better bound on performance). This brings up the
question of how many belief points should be included. However the number of points is not the only
consideration. It is likely that some collections of belief points (e.g., those frequently encountered)
are more likely to produce a good value function than others. This brings up the question of which
beliefs should be included.
A number of approaches have been proposed in the literature. For example, some exact value
function approaches use linear programs to identify points where the value function needs to be
further improved (Cheng, 1988; Littman, 1996; Zhang & Zhang, 2001), however this is typically
very expensive. The value function can also be approximated by learning the value at regular points,
using a fixed-resolution (Lovejoy, 1991a), or variable-resolution (Zhou & Hansen, 2001) grid. This
is less expensive than solving LPs, but can scales poorly as the number of states increases. Alternately, one can use heuristics to generate grid-points (Hauskrecht, 2000; Poon, 2001). This tends
to be more scalable, though significant experimentation is required to establish which heuristics are
most useful.
This section presents five heuristic strategies for selecting belief points, from fast and naive
random sampling, to increasingly more sophisticated stochastic simulation techniques. The most
effective strategy we propose is one that carefully selects points that are likely to have the largest
impact in reducing the error bound (Theorem 3.1).
Most of the strategies we consider focus on selecting reachable beliefs, rather than getting
uniform coverage over the entire belief simplex. Therefore it is useful to begin this discussion by
looking at how reachability is assessed.
348

A NYTIME P OINT-BASED A PPROXIMATIONS FOR L ARGE POMDP S

While some exact POMDP value iteration solutions are optimal for any initial belief, PBVI (and
other related techniques) assume a known initial belief b0 . As shown in Figure 2, we can use the
initial belief to build a tree of reachable beliefs. In this representation, each path through the tree
corresponds to a sequence in belief space, and increasing depth corresponds to an increasing plan
horizon. When selecting a set of belief points for PBVI, including all reachable beliefs would guarantee optimal performance (conditioned on the initial belief), but at the expense of computational
¯ can grow exponentially with the planning horitractability, since the set of reachable beliefs, ∆,
¯ which is sufficiently small for computational
zon. Therefore, it is best to select a subset B ⊂ ∆
tractability, but sufficiently large for good value function approximation.2

b0

...
ba z ba z

ba z

0 q

0 0 a0 z0

1 1

ba z

1 q

...

ba z

1 0

...
...

...

...

ba z ba z

...
ba z ba z
p 0

p 1

ba z

p q

...

0 1

...

...
...

0 0

...

...
ba z

0 0 ap zq

ba z

0 1 a0 z0

ba z

0 1 ap zq

...

...

...

...

Figure 2: The set of reachable beliefs
In domains where the initial belief is not known (or not unique), it is still possible to use reachability analysis by sampling a few initial beliefs (or using a set of known initial beliefs) to seed
multiple reachability trees.
We now discuss five strategies for selecting belief points, each of which can be used within the
PBVI framework to perform expansion of the belief set.
4.1 Random Belief Selection (RA)
The first strategy is also the simplest. It consists of sampling belief points from a uniform distribution over the entire belief simplex. To sample over the simplex, we cannot simply sample each
P
b(s) independently over [0, 1] (this would violate the constraint that s b(s) = 1). Instead, we use
the algorithm described in Table 3 (see Devroye, 1986, for more details including proof of uniform
coverage).
This random point selection strategy, unlike the other strategies presented below, does not focus
on reachable beliefs. For this reason, we do not necessarily advocate this approach. However we
include it because it is an obvious choice, it is by far the simplest to implement, and it has been used
in related work by Hauskrecht (2000) and Poon (2001). In smaller domains (e.g., <20 states), it
2. All strategies discussed below assume that the belief point set, B, approximately doubles in size on each belief
expansion. This ensures that the number of rounds of value iteration is logarithmic (in the final number of belief
points needed). Alternately, each strategy could be used (with very little modification) to add a fixed number of new
belief points, but this may require many more rounds of value iteration. Since value iteration is much more expensive
than belief computation, it seems appropriate to double the size of B at each expansion.

349

P INEAU , G ORDON & T HRUN

Bnew =EXPANDRA (B, Γ)
Bnew = B
Foreach b ∈ B
S := number of states
For i = 0 : S
btmp [i]=randuniform (0,1)
End
Sort btmp in ascending order
For i = 1 : S − 1
bnew [i]=btmp [i + 1] − btmp [i]
End
Bnew = Bnew ∪ bnew
End
Return Bnew

1
2
3
4
5
6
7
8
9
10
11
12
13
14

Table 3: Algorithm for belief expansion with random action selection

performs reasonably well, since the belief simplex is relatively low-dimensional. In large domains
(e.g., 100+ states), it cannot provide good coverage of the belief simplex with a reasonable number
of points, and therefore exhibits poor performance. This is demonstrated in the experimental results
presented in Section 6.
All of the remaining belief selection strategies make use of the belief tree (Figure 2) to focus on
reachable beliefs, rather than trying to cover the entire belief simplex.
4.2 Stochastic Simulation with Random Action (SSRA)
To generate points along the belief tree, we use a technique called stochastic simulation. It involves
running single-step forward trajectories from belief points already in B. Simulating a single-step
forward trajectory for a given b ∈ B requires selecting an action and observation pair (a, z), and
then computing the new belief τ (b, a, z) using the Bayesian update rule (Eqn 7). In the case of
Stochastic Simulation with Random Action (SSRA), the action selected for forward simulation is
picked (uniformly) at random from the full action set. Table 4 summarizes the belief expansion
procedure for SSRA. First, a state s is drawn from the belief distribution b. Second, an action a
is drawn at random from the full action set. Next, a posterior state s0 is drawn from the transition
model T (s, a, s0 ). Finally, an observation z is drawn from the observation model O(s0 , a, z). Using
the triple (b, a, z), we can calculate the new belief bnew = τ (b, a, z) (according to Equation 7), and
add to the set of belief points Bnew .
This strategy is better than picking points at random (as described above), because it restricts
Bnew to the belief tree (Fig. 2). However this belief tree is still very large, especially when the
branching factor is high, due to large numbers of actions/observations. By being more selective
about which paths in the belief tree are explored, one can hope to effectively restrict the belief set
further.
350

A NYTIME P OINT-BASED A PPROXIMATIONS FOR L ARGE POMDP S

Bnew =EXPANDSSRA (B, Γ)
Bnew = B
Foreach b ∈ B
s=randmultinomial (b)
a=randuniform (A)
s0 =randmultinomial (T (s, a, ·))
z=randmultinomial (O(s0 , a, ·))
bnew = τ (b, a, z) (see Eqn 7)
Bnew = Bnew ∪ bnew
End
Return Bnew

1
2
3
4
5
6
7
8
9
10
11

Table 4: Algorithm for belief expansion with random action selection

A similar technique for stochastic simulation was discussed by Poon (2001), however the belief set was initialized differently (not using b0 ), and therefore the stochastic simulations were not
restricted to the set of reachable beliefs.
4.3 Stochastic Simulation with Greedy Action (SSGA)
The procedure for generating points using Stochastic Simulation with Greedy Action (SSGA) is
based on the well-known -greedy exploration strategy used in reinforcement learning (Sutton &
Barto, 1998). This strategy is similar to the SSRA procedure, except that rather than choosing an
action randomly, SSEA will choose the greedy action (i.e., the current best action at the given belief
b) with probability 1 − , and will chose a random action with probability  (we use  = 0.1). Once
the action is selected, we perform a single-step forward simulation as in SSRA to yield a new belief
point. Table 5 summarizes the belief expansion procedure for SSGA.
A similar technique, featuring stochastic simulation using greedy actions, was outlined
by Hauskrecht (2000). However in that case, the belief set included all extreme points of the belief
simplex, and stochastic simulation was done from those extreme points, rather than from the initial
belief.
4.4 Stochastic Simulation with Exploratory Action (SSEA)
The error bound in Section 3 suggests that PBVI performs best when its belief set is uniformly dense
in the set of reachable beliefs. The belief point strategies proposed thus far ignore this information.
The next approach we propose gradually expands B by greedily choosing new reachable beliefs
that improve the worst-case density.
Unlike SSRA and SSGA which select a single action to simulate the forward trajectory for
a given b ∈ B, Stochastic Sampling with Exploratory Action (SSEA) does a one step forward
simulation with each action, thus producing new beliefs {ba0 , ba1 , ...}. However it does not accept
all new beliefs {ba0 , ba1 , ...}, but rather calculates the L1 distance between each ba and its closest
neighbor in B. We then keep only that point ba that is farthest away from any point already in B.
351

P INEAU , G ORDON & T HRUN

Bnew =EXPANDSSGA (B, Γ)
Bnew = B
Foreach b ∈ B
s=randmultinomial (b)
If randuniform [0, 1] < 
a=randuniform (A)
Else
P
a=argmaxα∈Γ s∈S α(s)b(s)
End
s0 =randmultinomial (T (s, a, ·))
z=randmultinomial (O(s0 , a, ·))
bnew = τ (b, a, z) (see Eqn 7)
Bnew = Bnew ∪ bnew
End
Return Bnew

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

Table 5: Algorithm for belief expansion with greedy action selection

We use the L1 norm to calculate distance between belief points to be consistent with the error bound
in Theorem 3.1. Table 6 summarizes the SSEA expansion procedure.
Bnew =EXPANDSSEA (B, Γ)
Bnew = B
Foreach b ∈ B
Foreach a ∈ A
s=randmultinomial (b)
s0 =randmultinomial (T (s, a, ·))
z=randmultinomial (O(s0 , a, ·))
ba =τ (b, a, z) (see Eqn 7)
End
P
bnew = maxa∈A minb0 ∈Bnew s∈S |ba (s) − b0 (s)|
Bnew = Bnew ∪ bnew (see Eqn 7)
End
Return Bnew

1
2
3
4
5
6
7
8
9
10
11
12
13

Table 6: Algorithm for belief expansion with exploratory action selection

4.5 Greedy Error Reduction (GER)
While the SSEA strategy above is able to improve the worst-case density of reachable beliefs, it
does not directly minimize the expected error. And while we would like to directly minimize the
352

A NYTIME P OINT-BASED A PPROXIMATIONS FOR L ARGE POMDP S

error, all we can measure is a bound on the error (Lemma 1). We therefore propose a final strategy
which greedily adds the candidate beliefs that will most effectively reduce this error bound. Our
empirical results, as presented below, show that this strategy is the most successful one discovered
thus far.
To understand how we expand the belief set in the GER strategy, it is useful to re-consider the
belief tree, which we reproduce in Figure 3. Each node in the tree corresponds to a specific belief.
We can divide these nodes into three sets. Set 1 includes those belief points already in B, in this
case b0 and ba0 z0 . Set 2 contains those belief points that are immediate descendants of the points
in B (i.e., the nodes in the grey zone). These are the candidates from which we will select the new
points to be added to B. We call this set the envelope (denoted B̄). Set 3 contains all other reachable
beliefs.

b0

...
ba z ba z

0 q

1 q

...

...

1 1

...
...

0

0 a0 z0

...

...

ba z

1 0

...
ba z

...

ba z ba z
p 0

p 1

ba z

p q

...

0 1

ba z ba z

...
...

0 0

...

ba z

ba z

0 0 ap zq

...

...

Figure 3: The set of reachable beliefs
We need to decide which belief b should be removed from the envelope B̄ and added to the set
of active belief points B. Every point that is added to B will improve our estimate of the value
function. The new point will reduce the error bounds (as defined in Section 3 for points that were
already in B; however, the error bound for the new point itself might be quite large. That means that
the largest error bound for points in B will not monotonically decrease; however, for a particular
point in B (such as the initial belief b0 ) the error bound will be decreasing.
To find the point which will most reduce our error bound, we can look at the analysis of
Lemma 1. Lemma 1 bounds the amount of additional error that a single point-based backup introduces. Write b0 for the new belief which we are considering adding, and write b for some belief
which is already in B. Write α for the value hyper-plane at b, and write α0 for b0 . As the lemma
points out, we have
(b0 ) ≤ (α0 − α) · (b0 − b)
When evaluating this error, we need to minimize over all b ∈ B. Also, since we do not know what
α0 will be until we have done some backups at b0 , we make a conservative assumption and choose
the worst-case value of α0 ∈ [Rmin /(1 − γ), Rmax /(1 − γ)]|S| . Thus, we can evaluate:
(

(b0 ) ≤ min
b∈B

X
s∈S

max
− α(s))(b0 (s) − b(s)) b0 (s) ≥ b(s)
( R1−γ
Rmin
( 1−γ − α(s))(b0 (s) − b(s)) b0 (s) < b(s)

353

(28)

P INEAU , G ORDON & T HRUN

While one could simply pick the candidate b0 ∈ B̄ which currently has the largest error bound,3
this would ignore reachability considerations. Rather, we evaluate the error at each b ∈ B, by
weighing the error of the fringe nodes by their reachability probability:
(b0 ),

X

(b) = max
a∈A

O(b, a, z) (τ (b, a, z))


X

= max
a∈A

(29)

z∈Z


XX


z∈Z

T (s, a, s0 )O(s0 , a, z)b(s) (τ (b, a, z)),

s∈S s0 ∈S

noting that τ (b, a, z) ∈ B̄, and (τ (b, a, z)) can be evaluated according to Equation 28.
Using Equation 29, we find the existing point b ∈ B with the largest error bound. We can now
directly reduce its error by adding to our set one of its descendants. We select the next-step belief
τ (b, a, z) which maximizes error bound reduction:
B

=

B ∪ τ (b̃, ã, z̃),

where b̃, ã := argmax

(30)
X

O(b, a, z) (τ (b, a, z))

(31)

b∈B,a∈A z∈Z

z̃ := argmax O(b̃, ã, z) (τ (b̃, ã, z))

(32)

z∈Z

Table 7 summarizes the GER approach to belief point selection.
Bnew =EXPANDGER (B, Γ)
Bnew = B
N =|B|
For i = 1 : N
P
b̃, ã := argmaxb∈B,a∈A z∈Z O(b, a, z) (τ (b, a, z))
z̃ := argmaxz∈Z O(b̃, ã, z) (τ (b̃, ã, z))
bnew = τ (b̃, ã, z̃)
Bnew = Bnew ∪ bnew
End
Return Bnew

1
2
3
4
5
6
7
8
9
10

Table 7: Algorithm for belief expansion

The complexity of adding one new points with GER is O(SAZB) (where S=#states,
A=#actions, Z=#observations, B=#beliefs already selected). In comparison, a value backup (for
one point) is O(S 2 AZB), and each point typically needs to be updated several times. As we point
out in empirical results below, belief selection (even with GER) takes minimal time compared to
value backup.
This concludes our presentation of belief selection techniques for the PBVI framework. In
summary, there are three factors to consider when picking a belief point: (1) how likely is it to
3. We tried this, however it did not perform as well empirically as what we suggest in Equation 29, because it did not
consider the probability of reaching that belief.

354

A NYTIME P OINT-BASED A PPROXIMATIONS FOR L ARGE POMDP S

occur? (2) how far is it from other belief points already selected? (3) what is the current approximate
value for that point? The simplest heuristic (RA) accounts for none of these, whereas some of the
others (SSRA, SSGA, SSEA) account for one, and GER incorporates all three factors.
4.6 Belief Expansion Example
We consider a simple example, shown in Figure 4, to illustrate the difference between the various
belief expansion techniques outlined above. This 1D POMDP (Littman, 1996) has four states, one
of which is the goal (indicated by the star). The two actions, left and right, have the expected
(deterministic) effect. The goal state is fully observable (observation=goal), while the other three
states are aliased (observation=none). A reward of +1 is received for being in the goal state,
otherwise the reward is zero. We assume a discount factor of γ = 0.75. The initial distribution is
uniform over non-goal states, and the system resets to that distribution whenever the goal is reached.

Figure 4: 1D POMDP
The belief set B is always initialized to contain the initial belief b0 . Figure 5 shows part of
the belief tree, including the original belief set (top node), and its envelope (leaf nodes). We now
consider what each belief expansion method might do.
b0=[ 1/ 3 1/ 3 0 1/ 3 ]
a=left

a=right

[ 2/ 3 0 1/ 3 0 ]

Pr(z=none)=2/3

b1=[ 1 0 0 0 ]

[ 0 1/ 3 1/ 3 1/ 3 ]

Pr(z=goal) = 1/3

Pr(z=none)=2/3

b2=[ 0 0 1 0 ]

b3=[ 0 0.5 0 0.5 ]

Pr(z=goal) = 1/3

b4=[ 0 0 1 0 ]

Figure 5: 1D POMDP belief tree
The Random heuristic can pick any belief point (with equal probability) from the entire belief
simplex. It does not directly expand any branches of the belief tree, but it will eventually put samples
nearby.
The Stochastic Simulation with Random Action has a 50% chance of picking each action.
Then, regardless of which action was picked, there’s a 2/3 chance of seeing observation none, and a
1/3 chance of seeing observation goal. As a result, the SSRA will select: P r(bnew = b1) = 0.5 ∗ 32 ,
P r(bnew = b2) = 0.5 ∗ 13 , P r(bnew = b3) = 0.5 ∗ 23 , P r(bnew = b4) = 0.5 ∗ 31 .
355

P INEAU , G ORDON & T HRUN

The Stochastic Simulation with Greedy Action first needs to know the policy at b0 . A few
iterations of point-based updates (Section 2.4) applied to this initial (single point) belief set reveal
that π(b0 ) = lef t.4 As a result, expansion of the belief will greedily select action lef t with proba
bility 1 −  + |A|
= 0.95 (assuming  = 0.1 and |A| = 2). Action right will be selected for belief

expansion with probability |A|
= 0.05. Combining this along with the observation probabilities, we
can tell that SSGA will expand as follows: P r(bnew = b1) = 0.95 ∗ 32 , P r(bnew = b2) = 0.95 ∗ 13 ,
P r(bnew = b3) = 0.05 ∗ 23 , P r(bnew = b4) = 0.05 ∗ 13 .
Predicting the choice of Stochastic Simulation with Exploratory Action is slightly more complicated. Four cases can occur, depending on the outcomes of random forward simulation from b0 :
1. If action left goes to b1 (P r = 2/3) and action right goes to b3 (P r = 2/3), then b1 will be
selected because ||b0 − b1 ||1 = 4/3 whereas ||b0 − b3 ||1 = 2/3. This case will occur with
P r = 4/9.
2. If action left goes to b1 (P r = 2/3) and action right goes to b4 (P r = 1/3), then b4 will be
selected because ||b0 − b4 ||1 = 2. This case will occur with P r = 2/9.
3. If action left goes to b2 (P r = 1/3) and action right goes to b3 (P r = 2/3), then b2 will be
selected because ||b0 − b2 ||1 = 2. This case will occur with P r = 2/9.
4. If action left goes to b2 (P r = 1/3) and action right goes to b4 (P r = 1/3), then either can
be selected (since they are equidistant to b0 ). In this case each b2 and b4 has P r = 1/18 of
being selected.
All told, P r(bnew = b1) = 4/9, P r(bnew = b2) = 5/18, P r(bnew = b3) = 0, P r(bnew = b4) =
5/18.
Now looking at belief expansion using Greedy Error Reduction, we need to compute the
error (τ (b0 , a, z)), ∀a, z. We consider Equation 28: since B has only one point, b0 , then necessarily b = b0 . To estimate α, we apply multiple steps of value backup at b0 and obtain
α = [0.94 0.94 0.92 1.74]. Using b and α as such, we can now estimate the error at each candidate belief: (b1 ) = 2.93, (b2 ) = 4.28, (b3 ) = 1.20, (b4 ) = 4.28. Note that because B has
only one point, the dominating factor is their distance to b0 . Next, we factor in the observation
probabilities, as in Eqns 31-32, which allows us to determine that ã = lef t and z̃ = none, and
therefore we should select bnew = b1 .
In summary, we note that SSGA, SSEA and GER all favor selecting b1 , whereas SSRA picks
each option with equal probability (considering that b2 and b4 are actually the same). In general,
for a problem of this size, it is reasonable to expand the entire belief tree. Any of the techniques
discussed here will be do this quickly, except RA which will not pick the exact nodes in the belief
tree, but will select equally good nearby beliefs. This example is provided simply to illustrate the
different choices made by each strategy.

5. A Review of Point-Based Approaches for POMDP Solving
The previous section describes a new class of point-based algorithms for POMDP solving. The idea
of using point-based updates in POMDPs has been explored previously in the literature, and in this
4. This may not be obvious to the reader, but it follows directly from the repeated application of equations 20–23.

356

A NYTIME P OINT-BASED A PPROXIMATIONS FOR L ARGE POMDP S

section we summarize the main results. For most of the approaches discussed below, the procedure
for updating the value function at a given point remains unchanged (as outlined in Section 2.4).
Rather, the approaches are mainly differentiated by how the belief points are selected, and by how
the updates are ordered.
5.1 Exact Point-Based Algorithms
Some of the earlier exact POMDP techniques use point-based backups to optimize the value function over limited regions of the belief simplex (Sondik, 1971; Cheng, 1988). These techniques
typically require solving multiple linear programs to find candidate belief points where the value
function is sub-optimal, which can be an expensive operation. Furthermore, to guarantee that an exact solution is found, relevant beliefs must be generated systematically, meaning that all reachable
beliefs must be considered. As a result, these methods typically cannot scale beyond a handful of
states/actions/observations.
In work by Zhang and Zhang (2001), point-based updates are interleaved with standard dynamic programming updates to further accelerate planning. In this case the points are not generated
systematically, but rather backups are applied to both a set of witness points and LP points. The
witness points are identified as a result of the standard dynamic programming updates, whereas
the LP points are identified by solving linear programs to identify beliefs where the value has not
yet been improved. Both of these procedures are significantly more expensive than the belief selection heuristics presented in this paper and results are limited to domains with at most a dozen
states/actions/observations. Nonetheless this approach is guaranteed to converge to the optimal solution.
5.2 Grid-Based Approximations
There exists many approaches that approximate the value function using a finite set of belief points
along with their values. These points are often distributed according to a grid pattern over the belief
space, thus the name grid-based approximation. An interpolation-extrapolation rule specifies the
value at non-grid points as a function of the value of neighboring grid-points. These approaches
ignore the convexity of the POMDP value function.
Performing value backups over grid-points is relatively straightforward: dynamic programming
updates as specified in Equation 11 can be adapted to grid-points for a simple polynomial-time
algorithm. Given a set of grid points G, the value at each bG ∈ G is defined by:
"
G

V (b ) = max
a

#
X

X

G

b (s)R(s, a) + γ

s∈S

P r(z | a, b)V (τ (b, a, z)) .

(33)

z∈Z

If τ (b, a, z) is part of the grid, then V (τ (b, a, z)) is defined by the value backups. Otherwise,
V (τ (b, a, z)) is approximated using an interpolation rule such as:
V (τ (b, a, z) =

|G|
X

λ(i)V (bG
i ),

(34)

i=1

P|G|

where λ(i) ≥ 0 and i=1 λ(i) = 1. This produces a convex combination over grid-points. The
two more interesting questions with respect to grid-based approximations are (1) how to calculate
the interpolation function; and (2) how to select grid points.
357

P INEAU , G ORDON & T HRUN

In general, to find the interpolation that leads to the best value function approximation at a point
b requires solving the following linear program:
Minimize

|G|
X

λ(i)V (bG
i )

(35)

i=1

Subject to

b=

|G|
X

λ(i)bG
i

(36)

i=1
|G|
X

λ(i) = 1

(37)

i=1

0 ≤ λ(i) ≤ 1, 1 ≤ i ≤ |G|.

(38)

Different approaches have been proposed to select grid points. Lovejoy (1991a) constructs a
fixed-resolution regular grid over the entire belief space. A benefit is that value interpolations can be
calculated quickly by considering only neighboring grid-points. The disadvantage is that the number
of grid points grows exponentially with the dimensionality of the belief (i.e., with the number of
states). A simpler approach would be to select random points over the belief space (Hauskrecht,
1997). But this requires slower interpolation for estimating the value of the new points. Both
of these methods are less than ideal when the beliefs encountered are not uniformly distributed.
In particular, many problems are characterized by dense beliefs at the edges of the simplex (i.e.,
probability mass focused on a few states, and most other states have zero probability), and low
belief density in the middle of the simplex. A distribution of grid-points that better reflects the
actual distribution over belief points is therefore preferable.
Alternately, Hauskrecht (1997) also proposes using the corner points of the belief simplex (e.g.,
[1 0 0 . . . ], [0 1 0 . . . ], . . . , [0 0 0 . . . 1]), and generating additional successor belief points through
one-step stochastic simulations (Eqn 7) from the corner points. He also proposes an approximate
interpolation algorithm that uses the values at |S|−1 critical points plus one non-critical point in the
grid. An alternative approach is that by Brafman (1997), which builds a grid by also starting with the
critical points of the belief simplex, but then uses a heuristic to estimate the usefulness of gradually
adding intermediate points (e.g., bk = 0.5bi + 0.5bj , for any pair of points). Both Hauskrecht’s
and Brafman’s methods—generally referred to as non-regular grid approximations—require fewer
points than Lovejoy’s regular grid approach. However the interpolation rule used to calculate the
value at non-grid points is typically more expensive to compute, since it involves searching over all
grid points, rather than just the neighboring sub-simplex.
Zhou and Hansen (2001) propose a grid-based approximation that combines advantages from
both regular and non-regular grids. The idea is to sub-sample the regular fixed-resolution grid
proposed by Lovejoy. This gives a variable resolution grid since some parts of the beliefs can be
more densely sampled than others and by restricting grid points to lie on the fixed-resolution grid
the approach can guarantee fast value interpolation for non-grid points. Nonetheless, the algorithm
often requires a large number of grid points to achieve good performance.
Finally, Bonet (2002) proposes the first grid-based algorithm for POMDPs with -optimality
(for any  > 0). This approach requires thorough coverage of the belief space such that every point
is within δ of a grid-point. The value update for each grid point is fast to implement, since the
interpolation rule depends only on the nearest neighbor of the one-step successor belief for each
grid point (which can be pre-computed). The main limitation is the fact that -coverage of the belief
358

A NYTIME P OINT-BASED A PPROXIMATIONS FOR L ARGE POMDP S

space can only be attained by using exponentially many grid points. Furthermore, this method
requires good coverage of the entire belief space, as opposed to the algorithms of Section 4, which
focus on coverage of the reachable beliefs.
5.3 Approximate Point-Based Algorithms
More similar to the PBVI-class of algorithms are those approaches that update both the value and
gradient at each grid point (Lovejoy, 1991a; Hauskrecht, 2000; Poon, 2001). These methods are able
to preserve the piecewise linearity and convexity of the value function, and define a value function
over the entire belief simplex. Most of these methods use random beliefs, and/or require the inclusion of a large number of fixed beliefs such as the corners of the probability simplex. In contrast, the
PBVI-class algorithms we propose (with the exception of PBVI+RA) select only reachable beliefs,
and in particular those belief points that improve the error bounds as quickly as possible. The idea
of using reachability analysis (also known as stochastic simulation) to generate new points was explored by some of the earlier approaches (Hauskrecht, 2000; Poon, 2001). However their analysis
indicated that stochastic simulation was not superior to random point placements. We re-visit this
question (and conclude otherwise) in the empirical evaluation presented below.
More recently, a technique closely related to PBVI called Perseus has been proposed (Vlassis
& Spaan, 2004; Spaan & Vlassis, 2005). Perseus uses point-based backups similar to the ones
used in PBVI, but the two approaches differ in two ways. First, Perseus uses randomly generated
trajectories through the belief space to select a set of belief points. This is in contrast to the beliefpoint selection heuristics outlined above for PBVI. Second, whereas PBVI systematically updates
the value at all belief points at every epoch of value iteration, Perseus selects a subset of points
to update at every epoch. The method used to select points is the following: points are randomly
sampled one at a time and their value is updated. This continues until the value of all points has
been improved. The insight resides in observing that updating the α-vector at one point often also
improves the value estimate of other nearby points (which are then removed from the sampling set).
This approach is conceptually simple and empirically effective.
The HSVI algorithm (Smith & Simmons, 2004) is another point-based algorithm, which differs
from PBVI both in how it picks belief points, and in how it orders value updates. It maintains a lower
and an upper bound on the value function approximation, and uses it to select belief points. The
updating of the upper bound requires solving linear programs and is generally the most expensive
step. The ordering of value update is as follows: whenever a belief point is expanded from the
belief tree, HSVI updates only the value of its direct ancestors (parents, grand-parents, etc., all the
way back to the initial belief in the head node). This is in contrast to PBVI which performs a batch
of belief point expansions, followed by a batch of value updates over all points. In other respects,
HSVI and PBVI share many similarities: both offer anytime performance, theoretical guarantees,
and scalability; finally the HSVI also takes reachability into account. We will evaluate empirical
differences between HSVI and PBVI in the next section.
Finally, the RTBSS algorithm (Paquet, 2005) offers an online version of point-based algorithms.
The idea is to construct a belief reachability tree similar to Figure 2, but using the current belief
as the top node, and terminating the tree at some fixed depth d. The value at each node can be
computed recursively over the finite planning horizon d. The algorithm can eliminate some subtrees
by calculating a bound on their value, and comparing it to the value of other computed subtrees.
RTBSS can in fact be combined with an offline algorithms such as PBVI, where the offline algorithm
359

P INEAU , G ORDON & T HRUN

is used to pre-compute a lower bound on the exact value function; this can be used to increase subtree
pruning, thereby increasing the depth of the online tree construction and thus also the quality of the
solution. This online algorithm can yield fast results in very large POMDP domains. However the
overall solution quality does not achieve the same error guarantees as the offline approaches.

6. Experimental Evaluation
This section looks at a variety of simulated POMDP domains to evaluate the empirical performance
of PBVI. The first three domains—Tiger-grid, Hallway, Hallway2—are extracted from the established POMDP literature (Cassandra, 1999). The fourth—Tag—was introduced in some of our
earlier work as a new challenge for POMDP algorithms.
The first goal of these experiments is to establish the scalability of the PBVI framework; this is
accomplished by showing that PBVI-type algorithms can successfully solve problems in excess of
800 states. We also demonstrate that PBVI algorithms compare favorably to alternative approximate
value iteration methods. Finally, following on the example of Section 4.6, we study at a larger scale
the impact of the belief selection strategy, which confirms the superior performance of the GER
strategy.
6.1 Maze Problems
There exists a set of benchmark problems commonly used to evaluate POMDP planning algorithms (Cassandra, 1999). This section presents results demonstrating the performance of PBVIclass algorithms on some of those problems. While these benchmark problems are relatively small
(at most 92 states, 5 actions, and 17 observations) compared to most robotics planning domains,
they are useful from an analysis point of view and for comparison to previous work.
The initial performance analysis focuses on three well-known problems from the POMDP literature: Tiger-grid (also known as Maze33), Hallway, and Hallway2. All three are maze navigation
problems of various sizes. The problems are fully described by Littman, Cassandra, and Kaelbling
(1995a); parameterization is available from Cassandra (1999).
Figure 6a presents results for the Tiger-grid domain. Replicating earlier experiments by Brafman (1997), test runs terminate after 500 steps (there’s an automatic reset every time the goal is
reached) and results are averaged over 151 runs.
Figures 6b and 6c present results for the Hallway and Hallway2 domains, respectively. In this
case, test runs are terminated when the goal is reached or after 251 steps (whichever occurs first),
and the results are averaged over 251 runs. This is consistent with earlier experiments by Littman,
Cassandra, and Kaelbling (1995b).
All three figures compare the performance of three different algorithms:
1. PBVI with Greedy Error Reduction (GER) belief point selection (Section 4.5).
2. QMDP (Littman et al., 1995b),
3. Incremental Pruning (Cassandra, Littman, & Zhang, 1997),
The QMDP heuristic (Littman et al., 1995b) takes into account partial observability at the current step, but assumes full observability on subsequent steps:
πQM DP (b) = argmax
a∈A

360

X
s∈S

b(s)QM DP (s, a).

(39)

A NYTIME P OINT-BASED A PPROXIMATIONS FOR L ARGE POMDP S

The resulting policy has some ability to resolve uncertainty, but cannot benefit from long-term
information gathering, or compare actions with different information potential. QMDP can be seen
as providing a good performance baseline. For the three problems considered, it finds a policy
extremely quickly, but the policy is clearly sub-optimal.
At the other end of the spectrum, the Incremental Pruning algorithm (Zhang & Liu, 1996; Cassandra et al., 1997) is a direct extension of the enumeration algorithm described above. The principal insight is that the pruning of dominated α-vectors (Eqn 19) can be interleaved directly with the
cross-sum operator (Eqn 16). The resulting value function is the same, but the algorithm is more
efficient because it discards unnecessary vectors earlier on. While Incremental Pruning algorithm
can theoretically find an optimal policy, for the three problems considered here it would take far too
long. In fact, only a few iterations of exact backups were completed in reasonable time. In all three
problems, the resulting short-horizon policy was worse than the corresponding PBVI policy.
As shown in Figure 6, PBVI+GER provides a much better time/performance trade-off. It finds
policies that are better than those obtained with QMDP, and does so in a matter of seconds, thereby
demonstrating that it does not suffer from the same paralyzing complexity as Incremental Pruning.
While those who take a closer look at these results may be surprised to see that the performance
of PBVI actually decreases at some points (e.g., the “dip” in Fig. 6c), this is not unexpected. It
is important to remember that the theoretical properties of PBVI only guarantee a bound on the
estimate of the value function, but as shown here, this does not necessarily imply that the policy
needs to improve monotonically. Nonetheless, as the value function converges, so will the policy
(albeit at a slower rate).
6.2 Tag Problem
While the previous section establishes the good performance of PBVI on some well-known simulation problems, these are quite small and do not fully demonstrate the scalability of the algorithm.
To provide a better understanding of PBVI’s effectiveness for large problems, this section presents
results obtained when applying PBVI to the Tag problem, a robot version of the popular game of
lasertag. In this problem, the agent must navigate its environment with the goal of searching for,
and tagging, a moving target (Rosencrantz, Gordon, & Thrun, 2003). Real-world versions of this
problem can take many forms, and in Section 7 we present a similar problem domain where an
interactive service robot must find an elderly patient roaming the corridors of a nursing home.
The synthetic scenario considered here is an order of magnitude larger (870 states) than most
other POMDP benchmarks in the literature (Cassandra, 1999). When formulated as a POMDP problem, the goal is for the robot to optimize a policy allowing it to quickly find the person, assuming
that the person moves (stochastically) according to a fixed policy. The spatial configuration of the
environment used throughout this experiment is illustrated in Figure 7.
The state space is described by the cross-product of two position features, Robot =
{s0 , . . . , s29 } and Person = {s0 , . . . , s29 , sf ound }. Both start in independently-selected random
positions, and the scenario finishes when Person = sf ound . The robot can select from five actions:
{North, South, East, West, Tag}. A reward of −1 is imposed for each motion action; the Tag action
results in a +10 reward if the robot and person are in the same cell, or −10 otherwise. Throughout the scenario, the Robot’s position is fully observable, and a Move action has the predictable
deterministic effect, e. g.:
P r(Robot = s10 | Robot = s0 , N orth) = 1,
361

P INEAU , G ORDON & T HRUN

2.5

0.7
PBVI+GER
QMDP
IncPrune

PBVI+GER
QMDP
IncPrune

0.6

2

1.5

REWARD

REWARD

0.5

1

0.4
0.3
0.2

0.5
0.1
0 −2
10

−1

0

10

1

2

10
10
TIME (secs)

0 −2
10

3

10

10

−1

0

10

1

10
10
TIME (secs)

(a) Tiger-grid

2

10

3

10

(b) Hallway

0.45
0.4

PBVI+GER
QMDP
IncPrune

0.35

REWARD

0.3
0.25
0.2
0.15
0.1
0.05
0 −2
10

−1

0

10

1

10
TIME (secs)

2

10

10

(c) Hallway2

Figure 6: PBVI performance on well-known POMDP problems. Each figure shows the sum of
discounted reward as a function of the computation time for a different problem domain.

26

27

28

23

24

25

20

21

22

10

11

12

13

14

15

16

17

18

19

0

1

2

3

4

5

6

7

8

9

Figure 7: Spatial configuration of the domain

362

A NYTIME P OINT-BASED A PPROXIMATIONS FOR L ARGE POMDP S

and so on for each adjacent cell and direction. The position of the person, on the other hand,
is completely unobservable unless both agents are in the same cell. Meanwhile at each step, the
person (with omniscient knowledge) moves away from the robot with P r = 0.8 and stays in place
with P r = 0.2, e. g.:
P r(P erson = s16 | P erson = s15 &Robot = s0 ) = 0.4
P r(P erson = s20 | P erson = s15 &Robot = s0 ) = 0.4
P r(P erson = s15 | P erson = s15 &Robot = s0 ) = 0.2.
Figure 8 shows the performance of PBVI with Greedy Error Reduction on the Tag domain. Results are averaged over 1000 runs, using different (randomly chosen) start positions for each run.
The QMDP approximation is also tested to provide a baseline comparison. The results show a gradual improvement in PBVI’s performance as samples are added (each shown data point represents a
new expansion of the belief set with value backups). It also confirms that computation time is directly related to the number of belief points. PBVI requires fewer than 100 belief points to overcome
QMDP, and the performance keeps on improving as more points are added. Performance appears to
be converging with approximately 250 belief points. These results show that a PBVI-class algorithm
can effectively tackle a problem with 870 states.
−6
PBVI+GER
QMDP
−8

REWARD

−10
−12
−14
−16
−18
−20 0
10

1

10

2

3

10
10
TIME (secs)

4

10

5

10

Figure 8: PBVI performance on Tag problem. We show the sum of discounted reward as a function
of the computation time.

This problem is far beyond the reach of the Incremental Pruning algorithm. A single iteration of
optimal value iteration on a problem of this size could produce over 1020 α-vectors before pruning.
Therefore, it was not applied.
This section describes one version of the Tag problem, which was used for simulation purposes
in our work and that of others (Braziunas & Boutilier, 2004; Poupart & Boutilier, 2004; Smith &
Simmons, 2004; Vlassis & Spaan, 2004). In fact, the problem can be re-formulated in a variety
of ways to accommodate different environments, person motion models, and observation models.
Section 7 discusses variations on this problem using more realistic robot and person models, and
presents results validated onboard an independently developed robot simulator.
363

P INEAU , G ORDON & T HRUN

6.3 Empirical Comparison of PBVI-Class Algorithms
Having establish the good performance of PBVI+GER on a number of problems, we now consider
empirical results for the different PBVI-class algorithms. This allows us to compare the effects
of the various belief expansion heuristics. We repeat the experiments on the Tiger-grid, Hallway,
Hallway2 and Tag domains, as outlined above, but in this case we compare the performance of five
different PBVI-class algorithms:
1. PBVI+RA: PBVI with belief points selected randomly from belief simplex (Section 4.1).
2. PBVI+SSRA: PBVI with belief points selected using stochastic simulation with random action (Section 4.2).
3. PBVI+SSGA: PBVI with belief points selected using stochastic simulation with greedy action
(Section 4.3).
4. PBVI+SSEA: PBVI with belief points selected using stochastic simulation with exploratory
action (Section 4.4).
5. PBVI+GER: PBVI with belief points selected using greedy error reduction (Section 4.5).
All PBVI-class algorithms can converge to the optimal value function given a sufficiently large
set of belief points. But the rate at which they converge depends on their ability to generally pick
useful points, and leave out the points containing less information. Since the computation time
is directly proportional to the number of belief points, the algorithm with the best performance is
generally the one which can find a good solution with the fewest belief points.
Figure 9 shows a comparison between the performance of each of the five PBVI-class algorithms
enumerated above on each of the four problem domains. In these pictures, we present performance
results as a function of computation time.5
As seen from these results, in the smallest domain—Tiger-grid—PBVI+GER is similar in performance to the random approach PBVI+RA. In the Hallway domain, PBVI+GER reaches nearoptimal performance earlier than the other algorithms. In Hallway2, it is unclear which of the five
algorithms is best, though GER seems to converge earlier.
In the larger Tag domain, the situation is more interesting. The PBVI+GER combination is
clearly superior to the others. There is reason to believe that PBVI+SSEA could match its performance, but would require on the order of twice as many points to do so. Nonetheless, PBVI+SSEA
performs better than either PBVI+SSRA or PBVI+SSGA. With the random heuristic (PBVI+RA),
the reward did not improve regardless of how many belief points were added (4000+), and therefore we do not include it in the results. The results presented in Figure 9 suggest that the choice
of belief points is crucial when dealing with large problems. In general, we believe that GER (and
SSEA to a lesser degree) is superior to the other heuristics for solving domains with large numbers
of action/observation pairs, because it has the ability to selectively chooses which branches of the
reachability tree to explore.
As a side note, we were surprised by SSGA’s poor performance (in comparison with SSRA) on
the Tiger-grid and Tag domains. This could be due to a poorly tuned greedy bias , which we did
5. Nearly identical graphs can be produced showing performance results as a function of the number of belief points.
This confirms complexity analysis showing that the computation time is directly related to the number of belief
points.

364

A NYTIME P OINT-BASED A PPROXIMATIONS FOR L ARGE POMDP S

2.5

0.7
RA
SSRA
SSGA
SSEA
GER

0.5

1.5

REWARD

REWARD

2

0.6

1

RA
SSRA
SSGA
SSEA
GER

0.4
0.3
0.2

0.5
0.1
0 −2
10

−1

0

10

1

10
10
TIME (secs)

2

10

0 −2
10

3

10

−1

10

(a) Tiger-grid

0

1

10
TIME (secs)

2

10

10

(b) Hallway
−6

0.45

REWARD

0.3

−8

SSRA
SSGA
SSEA
GER

−10
REWARD

0.4
0.35

RA
SSRA
SSGA
SSEA
GER

0.25
0.2
0.15

−12
−14
−16

0.1
−18

0.05
0 −2
10

−1

10

0

10
TIME (secs)

1

10

−20 0
10

2

10

(c) Hallway2

1

10

2

3

10
10
TIME (secs)

4

10

5

10

(d) Tag

Figure 9: Belief expansion results showing execution performance as a function of the computation
time.

not investigate at length. Future investigations using problems with a larger number of actions may
shed better light on this issue.
In terms of computational requirement, GER is the most expensive to compute, followed by
SSEA. However in all cases, the time to perform the belief expansion step is generally negligible
(< 1%) compared to the cost of the value update steps. Therefore it seems best to use the more
effective (though more expensive) heuristic.
The PBVI framework can accommodate a wide variety of strategies, past what is described in
this paper. For example, one could extract belief points directly from sampled experimental traces.
This will be the subject of future investigations.
6.4 Comparative Analysis
While the results outlined above show that PBVI-type algorithms are able to handle a wide spectrum
of large-scale POMDP domains, it is not sufficient to compare the performance of PBVI only to
365

P INEAU , G ORDON & T HRUN

QMDP and Incremental Pruning—the two ends of the spectrum—as done in Section 6.1. In fact
there has been significant activity in recent years in the development of fast approximate POMDP
algorithms, and so it is worthwhile to spend some time comparing the PBVI framework to these
alternative approaches. This is made easy by the fact that many of these have been validated using
the same set of problems as described above.
Table 8 summarizes the performance of a large number of recent POMDP approximation algorithms, including PBVI, on the four target domains: Tiger-grid, Hallway, Hallway2, and Tag. The
algorithms listed were selected based on the availability of comparable published results or available
code, or in some cases because the algorithm could be re-implemented easily.
We compare their empirical performance, in terms of execution performance versus planning,
on a set of simulation domains. However as is often the case, these results show that there is not a
single algorithm that is best for solving all problems. We therefore also compile a summary of the
attributes and characteristics of each algorithm, in an attempt to tell which algorithm may be best
for what types of problems. Table 8 includes (whenever possible) the goal completion rates, sum
of rewards, policy computation time, number of required belief points, and policy size (number of
α-vectors, or number of nodes in finite state controllers). The number of belief points and policy
size are often identical, however the latter can be smaller if a single α-vector is best for multiple
belief points.
The results marked [*] were computed by us on a 3GHz Pentium 4; other results were likely
computed on different platforms, and therefore time comparisons may be approximate at best.
Nonetheless the number of samples and the size of the final policy are both useful indicators of
computation time. The results reported for PBVI correspond to the earliest data point from Figures 6 and 8 where PBVI+GER achieves top performance.
Algorithms are listed in order of performance, starting with the algorithm(s) achieving the highest reward. All results assume a standard (not lookahead) controller (see Hauskrecht, 2000, for
definition).
Overall, the results indicate that some of the algorithms achieve sub-par performance in terms of
expected reward. In the case of QMDP, this is because of fundamental limitations in the algorithm.
While Incremental Pruning and the exact value-directed compression can theoretically reach optimal
performance, they would require longer computation time to do so. The grid method (see Tiger-grid
results), BPI (see Tiger-grid, Hallway and Tag results) and PBUA (see Tag results) suffer from a
similar problem, but offer much more graceful performance degradation. It is worth noting that none
of these approaches assumes a known initial belief, so in effect they are solving harder problems.
The results for BBSLS are not sufficiently extensive to comment at length, but it appears to be able
to find reasonable policies with small controllers (see Tag results).
The remaining algorithms—HSVI, Perseus, and our own PBVI+GER—all offer comparable
performance on these relatively large POMDP domains. HSVI seems to offer good control performance on the full range of tasks, but requires bigger controllers, and is therefore probably slower,
especially on domains with high stochasticity (e.g., Tiger-grid, Hallway, Hallway2). The trade-offs
between Perseus and PBVI+GER are less clear: the planning time, controller size and performance
quality are quite comparable, and in fact the two approaches are very similar. Similarities and
differences between the two approaches are explored further in Section 7.

366

A NYTIME P OINT-BASED A PPROXIMATIONS FOR L ARGE POMDP S

Method
Tiger-Grid (Maze33)
HSVI (Smith & Simmons, 2004)
Perseus (Vlassis & Spaan, 2004)
PBUA (Poon, 2001)
PBVI+GER[*]
BPI (Poupart & Boutilier, 2004)
Grid (Brafman, 1997)
QMDP (Littman et al., 1995b)[*]
IncPrune (Cassandra et al., 1997)[*]
Exact VDC (Poupart & Boutilier, 2003)[*]
Hallway
PBUA (Poon, 2001)
HSVI (Smith & Simmons, 2004)
PBVI+GER[*]
Perseus (Vlassis & Spaan, 2004)
BPI (Poupart & Boutilier, 2004)
QMDP (Littman et al., 1995b)[*]
Exact VDC (Poupart & Boutilier, 2003)[*]
IncPrune (Cassandra et al., 1997)[*]
Hallway2
PBVI+GER[*]
Perseus (Vlassis & Spaan, 2004)
HSVI (Smith & Simmons, 2004)
PBUA (Poon, 2001)
BPI (Poupart & Boutilier, 2004)
Grid (Brafman, 1997)
QMDP (Littman et al., 1995b)[*]
Exact VDC (Poupart & Boutilier, 2003)[*]
IncPrune (Cassandra et al., 1997)[*]
Tag
HSVI (Smith & Simmons, 2004)
PBVI+GER[*]
Perseus (Vlassis & Spaan, 2004)
BBSLS (Braziunas & Boutilier, 2004)
BPI (Poupart & Boutilier, 2004)
QMDP (Littman et al., 1995b)[*]
PBUA (Poon, 2001)[*]
IncPrune (Cassandra et al., 1997)[*]

Goal%

Reward ± Conf.Int.

Time(s)

|B|

|π|

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

2.35
2.34
2.30
2.27 ± 0.13
1.81
0.94
0.276
0.0
0.0

10341
104
12116
397
163420
n.v.
0.02
24hrs+
24hrs+

n.v.
10000
660
512
n.a.
174
n.a.
n.a.
n.a.

4860
134
n.v.
508
1500
n.a.
5
n.v.
n.v.

100
100
100
n.v.
n.v.
51
39
39

0.53
0.52
0.51 ± 0.03
0.51
0.51
0.265
0.161
0.161

450
10836
19
35
249730
0.03
24hrs+
24hrs+

300
n.v.
64
10000
n.a.
n.a.
n.a.
n.a.

n.v.
1341
64
55
1500
5
n.v.
n.v.

100
n.v.
100
100
n.v.
98
22
48
48

0.37 ± 0.04
0.35
0.35
0.35
0.28
n.v.
0.109
0.137
0.137

6
10
10010
27898
274280
n.v.
1.44
24hrs+
24hrs+

32
10000
n.v.
1840
n.a.
337
n.a.
n.a.
n.a.

31
56
1571
n.v.
1500
n.a.
5
n.v.
n.v.

100
100
n.v.
n.v.
n.v.
19
0
0

-6.37
-6.75 ± 0.39
-6.85
-8.31
-9.18
-16.62
-19.9
-19.9

10113
8946
3076
100054
59772
1.33
24hrs+
24hrs+

n.v.
256
10000
n.a.
n.a.
n.a.
4096
n.a.

1657
203
205
30
940
5
n.v.
n.v.

n.a.=not applicable

n.v.=not available

[*]=results computed by us

Table 8: Results of PBVI for standard POMDP domains

367

P INEAU , G ORDON & T HRUN

6.5 Error Estimates
The results presented thus far suggest that the PBVI framework performs best when using the
Greedy Error Reduction (GER) technique for selecting belief points. Under this scheme, to decide which belief points will be included, we estimate an error bound at a set of candidate points
and then pick the one with the largest error estimate. The error bound is estimated as described in
Equation 28. We now consider the question of how this estimate evolves as more and more points
are added. The natural intuition is that with the first few points, error estimates will be very large,
but as the density of the belief set increases, the error estimates will become much smaller.
Figure 10 reconsiders the four target domains: Tiger-grid, Hallway, Hallway2 and Tag. In each
case, we present both the reward performance as a function of the number of belief points (top
row graphs), and the error estimate of each point selected according the order in which points were
picked (bottom row graphs). In addition, the bottom graphs also show (in dashed line) a trivial
−Rmin
bound on the error ||Vt − Vt∗ || ≤ Rmax1−γ
, valid for any t-step value function of an arbitrary
policy. As expected, our bound is typically tighter than the trivial bound. In Tag, this only occurs
once the number of belief points exceeds the number of states, which is not surprising, given that our
bound depends on distance between reachable beliefs, and that all states are reachable beliefs in this
domain. Overall, it seems that there is reasonably good correspondence between an improvement
in performance and a decrease in our error estimates. We can conclude from this figure that even
though the PBVI error is quite loose, it can in fact be informative in guiding exploration of the belief
simplex.
We note that there is significant variance in our error estimates from one belief point to the next,
as illustrated by the non-monotonic behavior of the curves in the bottom graphs of Figure 10. This
behavior can be attributed to a few possibilities. First, there is the fact that the error estimate at
a given belief is only approximate. And the value function used to calculate the error estimate is
itself approximate. In addition, there is the fact that new belief points are always selected from the
envelope of reachable beliefs, not from the set of all reachable beliefs. This suggests that GER could
be improved by maintaining a deeper envelope of candidate belief points. Currently the envelope
contains those points that are 1-step forward simulations from the points already selected. It may
be useful to consider points 2–3 steps ahead. We predict this would reduce the jaggedness seen in
Figure 10, and more importantly, also reduce the number of points necessary for good performance.
Of course, the tradeoff between the time spent selecting points and the time spent planning would
have to be re-evaluated under this light.

7. Robotic Applications
The overall motivation behind the work described in this paper is the desire to provide high-quality
robust planning for real-world autonomous systems, and in particular for robots. On the practical side, our search for a robust robot controller has been in large part guided by the Nursebot
project (Pineau, Montermerlo, Pollack, Roy, & Thrun, 2003). The overall goal of the project is to
develop personalized robotic technology that can play an active role in providing improved care
and services to non-institutionalized elderly people. Pearl, shown in Figure 11, is the main robotic
platform used for this project.
From the many services a nursing-assistant robot could provide (Engelberger, 1999; Lacey
& Dawson-Howe, 1998), much of the work to date has focused on providing timely cognitive reminders (e.g., medications to take, appointments to attend, etc.) to elderly subjects (Pollack, 2002).
368

A NYTIME P OINT-BASED A PPROXIMATIONS FOR L ARGE POMDP S

Tiger−grid

Hallway

2.5

Reward

2

Hallway2

0.8

0.4

0.6

0.3

0.4

0.2

0.2

0.1

Tag
−5

−10

1.5
1

−15

0.5
0
0
10

1

10

2

10

3

10

0
0
10

# belief points
60

1

10

2

10

0
0
10

3

10

# belief points

1

10

2

10

20

20

Error

2

10

3

10

# belief points

400

15
40

15

300

10

20

200

10
5

100

10
0
0
10

1

10

500

50

30

−20
3
0
10 10

# belief points

1

10

2

10

# belief points

0
3
0
10 10

1

10

2

10

3

10

# belief points

5
0
10

1

10

2

10

# belief points

3

10

0
0
10

1

10

2

10

3

10

# belief points

Figure 10: Sum of discounted reward (top graphs) and estimate of the bound on the error (bottom
graphs) as a function of the number of selected belief points.

Figure 11: Pearl the Nursebot, interacting with elderly people at a nursing facility
An important component of this task is finding the patient whenever it is time to issue a reminder.
This task shares many similarities with the Tag problem presented in Section 6.2. In this case,
however, a robot-generated map of a real physical environment is used as the basis for the spatial
configuration of the domain. This map is shown in Figure 12. The white areas correspond to free
space, the black lines indicate walls (or other obstacles) and the dark gray areas are not visible or
accessible to the robot. One can easily imagine the patient’s room and physiotherapy unit lying at
either end of the corridor, with a common area shown in the upper-middle section.
The overall goal is for the robot to traverse the domain in order to find the missing patient and
then deliver a message. The robot must systematically explore the environment, reasoning about
both spatial coverage and human motion patterns, in order to find the person.
369

P INEAU , G ORDON & T HRUN

Figure 12: Map of the environment
7.1 POMDP Modeling
The problem domain is represented jointly by two state features: RobotPosition, PersonPosition.
Each feature is expressed through a discretization of the environment. Most of the experiments
below assume a discretization of 2 meters, which means 26 discrete cells for each feature, for a total
of 676 states.
It is assumed that the person and robot can move freely throughout this space. The robot’s
motion is deterministically controlled by the choice of action (North, South, East, West). The robot
has a fifth action (DeliverMessage), which concludes the scenario when used appropriately (i.e.,
when the robot and person are in the same location).
The person’s motion is stochastic and falls in one of two modes. Part of the time, the person
moves according to Brownian motion (e.g., moves in each cardinal direction with P r = 0.1, otherwise stays put). At other times, the person moves directly away from the robot. The Tag domain of
Section 6.2 assumes that the person always moves always moves away the robot. This is not realistic when the person cannot see the robot. The current experiment instead assumes that the person
moves according to Brownian motion when the robot is far away, and moves away from the robot
when it is closer (e.g., < 4m). The person policy was designed this way to encourage the robot to
find a robust policy.
In terms of state observability, there are two components: what the robot can sense about its
own position, and what it can sense about the person’s position. In the first case, the assumption
is that the robot knows its own position at all times. While this may seem like a generous (or
optimistic) assumption, substantial experience with domains of this size and maps of this quality
have demonstrated robust localization abilities (Thrun et al., 2000). This is especially true when
planning operates at relatively coarse resolution (2 meters) compared to the localization precision
(10 cm). While exact position information is assumed for planning in this domain, the execution
phase (during which we actually measure performance) does update the belief using full localization
information, which includes positional uncertainty whenever appropriate.
Regarding the detection of the person, the assumption is that the robot has no knowledge of
the person’s position unless s/he is within a range of 2 meters. This is plausible given the robot’s
sensors. However, even in short-range, there is a small probability (P r = 0.01) that the robot will
miss the person and therefore return a false negative.
In general, one could make sensible assumptions about the person’s likely position (e.g., based
on a knowledge of their daily activities), however we currently have no such information and therefore assume a uniform distribution over all initial positions. The person’s subsequent movements
370

A NYTIME P OINT-BASED A PPROXIMATIONS FOR L ARGE POMDP S

are expressed through the motion model described above (i.e., a mix of Brownian motion and purposeful avoidance).
The reward function is straightforward: R = −1 for any motion action, R = 10 when the
robot decides to DeliverMessage and it is in the same cell as the person, and R = −100 when
the robot decides to DeliverMessage in the person’s absence. The task terminates when the robot
successfully delivers the message (i.e., a = DeliverM essage and srobot = sperson ). We assume a
discount factor of 0.95.
We assume a known initial belief, b0 , consisting of a uniform distribution over all states. This
is used both for selecting belief points during planning, and subsequently for executing and testing
the final policy.
The initial map (Fig. 12) of the domain was collected by a mobile robot, and slightly cleaned
up by hand to remove artifacts (e.g., people walking by). We then assumed the model parameters
described here, and applied PBVI planning to the problem as such. Value updates and belief point
expansions were applied in alternation until (in simulation) the policy was able to find the person
on 99% of trials (trials were terminated when the person is found or after 100 execution steps). The
final policy was implemented and tested onboard the publicly available CARMEN robot simulator (Montemerlo, Roy, & Thrun, 2003).
7.2 Comparative Evaluation of PBVI and Perseus
The subtask described here, with its 626 states, is beyond the capabilities of exact POMDP solvers.
Furthermore, as will be demonstrated below, MDP-type approximations are not equipped to handle
uncertainty of the type exhibited in this task. The main purpose of our analysis is to evaluate the
effectiveness of the point-based approach described in this paper to address this problem. While the
results on the Tag domain (Section 6.2) hint at the fact that PBVI and other algorithms may be able
to handle this task, the more realistic map and modified motion model provide new challenges.
We begin our investigation by directly comparing the performance of PBVI (with GER belief points selection) with that of the Perseus algorithm on this complex robot domain. Perseus
was described in Section 5; results presented below were produced using code provided by its authors (Perseus, 2004). Results for both algorithms assume that a fixed POMDP model was generated
by the robot simulator. This model is then stored and solved offline by each algorithm.
PBVI and Perseus each have a few parameters to set. PBVI requires: number of new belief
points to add at each expansion (Badd ) and planning horizon for each expansion (H). Perseus
requires: number of belief points to generate during random walk (B) and the maximum planning
time (T ). Results presented below assume the following parameter settings: Badd = 30, H = 25,
B=10,000, T =1500. Both algorithms were fairly robust to changes in these parameters.6
Figure 13 summarizes the results of this experiment. These suggest a number of observations.
• As shown in Figure 13(a), both algorithms find the best solution in a similar time, but
PBVI+GER has better anytime performance than Perseus (e.g., a much better policy is found
given only 100 sec).
• As shown in Figure 13(b), both algorithms require a similar number of α-vectors.
• As shown in Figure 13(c), PBVI+GER requires many fewer beliefs.
6. A ±25% change in parameter value yielded sensibly similar results in terms of reward and number of α vectors,
though of course the time, memory, and number of beliefs varied.

371

P INEAU , G ORDON & T HRUN

−5
−10

120
PBVI+GER
GER
Perseus
QMDP

PBVI+GER
Perseus
100

−15
# alpha vectors

REWARD

−20
−25
−30
−35

80
60
40

−40
20
−45
−50 −1
10

0

1

10

2

3

10
10
TIME (secs)

10

0 0
10

4

10

1

10

5

10

PBVI+GER
Perseus
4

10

# beliefs

3

10

2

10

1

10

0

10 0
10

1

10

2

10
TIME (secs)

3

10

4

10

(b)
Memory requirement = (#alphas + #beliefs)*#states

(a)

2

10
TIME (secs)

3

10

4

10

(c)

8

10

PBVI+GER
Perseus
7

10

6

10

5

10

4

10

0

10

1

10

2

10
TIME (secs)

3

10

4

10

(d)

Figure 13: Comparison of PBVI and Perseus on robot simulation domain
• Because it requires fewer beliefs, PBVI+GER has much lower memory requirements; this is
quantified in Figure 13(d).
These new results suggest that PBVI and Perseus have similar performance if the objective is to
find a near-optimal solution, and time and memory are not constrained. In cases where one is willing
to trade off accuracy for time, then PBVI may provide superior anytime performance. And in cases
where memory is limited, PBVI’s conservative approach with respect to belief point selection is
advantageous. Both these properties suggest that PBVI may scale better to very large domains.
7.3 Experimental Results with Robot Simulator
The results presented in above assume that the same POMDP model is used for planning and testing (i.e., to compute the reward in Figure 13(a)). This is useful to carry out a large number of
experiments. The model however cannot entirely capture the dynamics of a realistic robot system,
therefore there is some concern that the policy learned by point-based methods will not perform as
well on a realistic robot. To verify the robustness of our approach, the final PBVI control policy
was implemented and tested onboard the publicly available CARMEN robot simulator (Montemerlo
et al., 2003).
372

A NYTIME P OINT-BASED A PPROXIMATIONS FOR L ARGE POMDP S

The resulting policy is illustrated in Figure 14. This figure shows five snapshots obtained from
a single run. In this particular scenario, the person starts at the far end of the left corridor. The
person’s location is not shown in any of the figures since it is not observable by the robot. The
figure instead shows the belief over person positions, represented by a distribution of point samples
(grey dots in Fig. 14). Each point represents a plausible hypothesis about the person’s position. The
figure shows the robot starting at the far right end of the corridor (Fig. 14a). The robot moves toward
the left until the room’s entrance (Fig. 14b). It then proceeds to check the entire room (Fig. 14c).
Once relatively certain that the person is nowhere to be found, it exits the room (Fig. 14d), and
moves down the left branch of the corridor, where it finally finds the person at the very end of the
corridor (Fig. 14e).
This policy is optimized for any start position (for both the person and the robot). The scenario
shown in Figure 14 is one of the longer execution traces since the robot ends up searching the entire
environment before finding the person. It is interesting to compare the choice of action between
snapshots (b) and (d). The robot position in both is practically identical. Yet in (b) the robot chooses
to go up into the room, whereas in (d) the robot chooses to move toward the left. This is a direct
result of planning over beliefs, rather than over states. The belief distribution over person positions
is clearly different between those two cases, and therefore the policy specifies a very different course
of action.
Figure 15 looks at the policy obtained when solving this same problem using the QMDP heuristic. Four snapshots are offered from different stages of a specific scenario, assuming the person
started on the far left side and the robot on the far right side (Fig. 15a). After proceeding to the
room entrance (Fig. 15b), the robot continues down the corridor until it almost reaches the end
(Fig. 15c). It then turns around and comes back toward the room entrance, where it stations itself
(Fig. 15d) until the scenario is forcibly terminated. As a result, the robot cannot find the person
when s/he is at the left edge of the corridor or in the room. What’s more, because of the runningaway behavior adopted by the subject, even when the person starts elsewhere in the corridor, as the
robot approaches the person will gradually retreat to the left and similarly escape from the robot.
Even though QMDP does not explicitly plan over beliefs, it can generate different policy actions
for cases where the state is identical but the belief is different. This is seen when comparing Figure 15 (b) and (d). In both of these, the robot is identically located, however the belief over person
positions is different. In (b), most of the probability mass is to the left of the robot, therefore it travels in that direction. In (d), the probability mass is distributed evenly between the three branches
(left corridor, room, right corridor). The robot is equally pulled in all directions and therefore stops
there. This scenario illustrates some of the strength of QMDP. Namely, there are many cases where
it is not necessary to explicitly reduce uncertainty. However, it also shows that more sophisticated
approaches are needed to handle some cases.
These results show that PBVI can perform outside the bounds of simple maze domains, and
is able to handle realistic problem domains. In particular, throughout this evaluation, the robot
simulator was in no way constrained to behave as described in our POMDP model (Sec. 7.1). This
means that the robot’s actions often had stochastic effects, the robot’s position was not always
fully observable, and that belief tracking had to be performed asynchronously (i.e., not always a
straightforward ordering of actions and observations). Despite this misalignment between the model
assumed for planning, and the execution environment, the control policy optimized by PBVI could
successfully be used to complete the task.

373

P INEAU , G ORDON & T HRUN

(a) t=1

(b) t=7

(c) t=12

(d) t=17

(e) t=29
Figure 14: Example of a PBVI policy successfully finding the person

374

A NYTIME P OINT-BASED A PPROXIMATIONS FOR L ARGE POMDP S

(a) t=1

(b) t=7

(c) t=17

(d) t=27
Figure 15: Example of a QMDP policy failing to find the person

375

P INEAU , G ORDON & T HRUN

8. Discussion
This paper describes a class of anytime point-based POMDP algorithms called PBVI, which combines point-based value updates with strategic selection of belief points, to solve large POMDPs.
Further extensions to the PBVI framework, whereby value updates are applied to groups of belief
points according to their spatial distribution, are described by (Pineau, Gordon, & Thrun, 2004).
The main contributions pertaining to the PBVI framework are now summarized.
Scalability. The PBVI framework is an important step towards truly scalable POMDP solutions.
This is achieved by bounding the policy size through the selection of a small set of belief points.
Anytime planning. PBVI-class algorithms alternates between steps of value updating and steps
of belief point selection. As new points are added, the solution improves, at the expense of increased
computational time. The trade-off can be controlled by adjusting the number of points. The algorithm can be terminated either when a satisfactory solution is found, or when planning time is
elapsed.
Bounded error. We provide a theoretical bound on the error of the approximation introduced
in the PBVI framework. This result holds for a range of belief point selection methods, and lead
directly to the development of a new PBVI-type algorithm: PBVI+GER, where estimates of the
error bound are used directly to select belief points. Furthermore we find that the bounds can be
useful in assessing when to stop adding belief points.
Exploration. We proposed a set of new point selection heuristics, which explore the tree of
reachable beliefs to select useful belief points. The most successful technique described, Greedy
Error Reduction (GER), uses an estimate of the error bound on candidate belief points to select the
most useful points.
Improved empirical performance. PBVI has demonstrated the ability to reduce planning time
for a number of well-known POMDP problems, including Tiger-grid, Hallway, and Hallway2. By
operating on a set of discrete points, PBVI algorithms can perform polynomial-time value updates,
thereby overcoming the curse of history that paralyzes exact algorithms. The GER technique used to
select points allows us to solve large problems with fewer belief points than alternative approaches.
New problem domain. PBVI was applied to a new POMDP planning domain (Tag), for which
it generated an approximate solution that outperformed baseline algorithms QMDP and Incremental
Pruning. This new domain has since been adopted as a test case for other algorithms (Vlassis &
Spaan, 2004; Smith & Simmons, 2004; Braziunas & Boutilier, 2004; Poupart & Boutilier, 2004).
This fosters an increased ease of comparison between new techniques. Further comparative analysis
was provided in Section 7.2 highlighting similarities and differences between PBVI and Perseus.
Demonstrated performance. PBVI was applied in the context of a robotic search-and-rescue
type scenario, where a mobile robot is required to search its environment and find a non-stationary
individual. PBVI’s performance was evaluated using a realistic, independently-developed, robot
simulator.
A significant portion of this paper is dedicated to a thorough comparative analysis of point-based
methods. This includes evaluating a range of point-based selection methods, as well as evaluating
mechanisms for ordering value updates. The comparison of point-based selection techniques suggest that the GER method presented in Section 4.5 is superior to more naive techniques. In terms of
ordering of value updates, the randomized strategy which is used in the Perseus algorithm appears
effective to accelerate planning. A natural next step would be to combine the GER belief selection
heuristic with Perseus’s random value updates. We performed experiments along these lines, but
376

A NYTIME P OINT-BASED A PPROXIMATIONS FOR L ARGE POMDP S

did not achieve any significant speed-up over the current performance of PBVI or Perseus (e.g., as
reported in Figure 13(a)). It is likely that when belief points are chosen carefully (as in GER), each
of these points needs to be updated systematically and therefore there is no additional benefit to
using randomized value updates.
Looking towards the future, it is important to remember that while we have demonstrated the
ability to solve problems which are large by POMDP standards, many real-world domains far exceed
the complex of domains considered in this paper. In particular, it is not unusual for a problem to be
expressed through a number of multi-valued state features, in which case the number of states grows
exponentially with the number of features. This is of concern because each belief point and each
α-vector has dimensionality |S| (where |S| is the number of states) and all dimensions are updated
simultaneously. This is an important issue to address to improve the scalability of point-based value
approaches in general.
There are various existing attempts at overcoming the curse of dimensionality in POMDPs.
Some of these—e. g. the belief compression techniques by Roy and Gordon (2003)—cannot be
incorporated within the PBVI framework without compromising its theoretical properties (as discussed in Section 3). Others, in particular the exact compression algorithm by Poupart and Boutilier
(2003), can be combined with PBVI. However, preliminary experiments in this direction have
yielded little performance improvement. There is reason to believe that approximate value compression would yield better results, but again at the expense of forgoing PBVI’s theoretical properties. The challenge therefore is to devise function-approximation techniques that both reduce the
dimensionality effectively, while maintaining the convexity properties of the solution.
A secondary (but no less important) issue concerning the scalability of PBVI pertains to the
number of belief points necessary to obtain a good solution. While problems addressed thus far can
usually be solved with O(|S|) belief points, this need not be true. In the worse case, the number
of belief points necessary may be exponential in the plan length. The PBVI framework can accommodate a wide variety of strategies for generating belief points, and the Greedy Error Reduction
technique seems particularly effective. However this is unlikely to be the definitive answer to belief
point selection. In more general terms, this relates closely to the well-known issue of exploration
versus exploitation, which arises across a wide array of problem-solving techniques.
These promising opportunities for future research aside, the PBVI framework has already
pushed the envelope of POMDP problems that can be solved with existing computational resources.
As the field of POMDPs matures, finding ways of computing policies efficiently will likely continue
to be a major bottleneck. We hope that point based algorithms such as the PBVI will play a leading
role in the search for more efficient algorithms.

Acknowledgments
The authors wish to thank Craig Boutilier, Michael Littman, Andrew Moore and Matthew Mason for
many thoughtful comments and discussions regarding this work. We also thank Darius Braziunas,
Pascal Poupart, Trey Smith and Nikos Vlassis, for conversations regarding their algorithms and
results. The contributions of Michael Montemerlo and Nicholas Roy in conducting the empirical
robot evaluations are gratefully acknowledged. Finally, we thank three anonymous reviewers and
one dedicated editor (Sridhar Mahadevan) whose feedback significantly improved the paper. This
work was funded through the DARPA MARS program and NSF’s ITR program (Project: ”Robotic
Assistants for the Elderly”, PI: J. Dunbar-Jacob).
377

P INEAU , G ORDON & T HRUN

References
Ästrom, K. J. (1965). Optimal control of markov decision processes with incomplete state estimation. Journal of Mathematical Analysis and Applications, 10, 174–205.
Bellman, R. (1957). Dynamic Programming. Princeton University Press.
Bertsekas, D. P., & Tsitsiklis, J. (1996). Neuro-Dynamic Programming. Athena Scientific.
Blum, A. L., & Furst, M. L. (1997). Fast planning through planning graph analysis. Artificial
Intelligence, 90(1-2), 281–300.
Bonet, B. (2002). An epsilon-optimal grid-based algorithm for partially obserable Markov decision
processes. In Machine Learning: Proceedings of the 2002 International Conference (ICML),
pp. 51–58.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions
and computational leverage. Journal of Artificial Intelligence Research, 11, 1–94.
Boyen, X., & Koller, D. (1998). Tractable inference for complex stochastic processes. In Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI), pp. 33–42.
Brafman, R. I. (1997). A heuristic variable grid solution method for POMDPs. In Proceedings of
the Fourteenth National Conference on Artificial Intelligence (AAAI), pp. 727–733.
Braziunas, D., & Boutilier, C. (2004). Stochastic local search for POMDP controllers. In Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI), pp. 690–696.
Burgard, W., Cremers, A. B., Fox, D., Hahnel, D., Lakemeyer, G., Schulz, D., Steiner, W., & Thrun,
S. (1999). Experiences with an interactive museum tour-guide robot. Artificial Intelligence,
114, 3–55.
Cassandra, A. (1999).
Tony’s
research/ai/pomdp/code/index.html.

POMDP

page.

http://www.cs.brown.edu/

Cassandra, A., Littman, M. L., & Zhang, N. L. (1997). Incremental pruning: A simple, fast, exact
method for partially observable Markov decision processes. In Proceedings of the Thirteenth
Conference on Uncertainty in Artificial Intelligence (UAI), pp. 54–61.
Chapman, D. (1987). Planning for conjunctive goals. Artificial Intelligence, 32(3), 333–377.
Cheng, H.-T. (1988). Algorithms for Partially Observable Markov Decision Processes. Ph.D. thesis,
University of British Columbia.
Dean, T., & Kanazawa, K. (1988). Probabilistic temporal reasoning. In Proceedings of the Seventh
National Conference on Artificial Intelligence (AAAI), pp. 524–528.
Devroye, L. (1986). Non-Uniform Random Variate Generation. Springer-Verlag, New York.
Engelberger, G. (1999). Handbook of Industrial Robotics. John Wiley and Sons.
Fikes, R. E., & Nilsson, N. J. (1971). STRIPS: A new approach to the application of theorem
proving to problem solving. Artificial Intelligence, 2, 189–208.
Hauskrecht, M. (1997). Incremental methods for computing bounds in partially observable Markov
decision processes. In Proceedings of the Fourteenth National Conference on Artificial Intelligence (AAAI), pp. 734–739.
378

A NYTIME P OINT-BASED A PPROXIMATIONS FOR L ARGE POMDP S

Hauskrecht, M. (2000). Value-function approximations for partially observable Markov decision
processes. Journal of Artificial Intelligence Research, 13, 33–94.
Jazwinski, A. M. (1970). Stochastic Processes and Filtering Theory. Academic, New York.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning and acting in partially
observable stochastic domains. Artificial Intelligence, 101, 99–134.
Kalman, R. E. (1960). A new approach to linear filtering and prediction problems. Transactions of
the ASME, Journal of Basic Engineering, 82, 35–45.
Lacey, G., & Dawson-Howe, K. M. (1998). The application of robotics to a mobility aid for the
elderly blind. Robotics and Autonomous Systems, 23, 245–252.
Littman, M. L. (1996). Algorithms for Sequential Decision Making. Ph.D. thesis, Brown University.
Littman, M. L., Cassandra, A. R., & Kaelbling, L. P. (1995a). Learning policies for partially obsevable environments: Scaling up. Tech. rep. CS-95-11, Brown University, Department of
Computer Science.
Littman, M. L., Cassandra, A. R., & Kaelbling, L. P. (1995b). Learning policies for partially obsevable environments: Scaling up. In Proceedings of Twelfth International Conference on
Machine Learning, pp. 362–370.
Lovejoy, W. S. (1991a). Computationally feasible bounds for partially observed Markov decision
processes. Operations Research, 39(1), 162–175.
Lovejoy, W. S. (1991b). A survey of algorithmic methods for partially observable Markov decision
processes. Annals of Operations Research, 28, 47–66.
McAllester, D., & Roseblitt, D. (1991). Systematic nonlinear planning. In Proceedings of the Ninth
National Conference on Artificial Intelligence (AAAI), pp. 634–639.
Monahan, G. E. (1982). A survey of partially observable Markov decision processes: Theory, models, and algorithms. Management Science, 28(1), 1–16.
Montemerlo, M., Roy, N., & Thrun, S. (2003). Perspectives on standardization in mobile robot
programming: The Carnegie Mellon navigation (CARMEN) toolkit. In Proceedings of the
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Vol. 3, pp. pp
2436–2441.
Paquet, S. (2005). Distributed Decision-Making and Task Coordination in Dynamic, Uncertain and
Real-Time Multiagent Environments. Ph.D. thesis, Universite Laval.
Penberthy, J. S., & Weld, D. (1992). UCPOP: A sound, complete, partial order planning for ADL.
In Proceedings of the Third International Conference on Knowledge Representation and Reasoning, pp. 103–114.
Perseus (2004) http://staff.science.uva.nl/m̃tjspaan/software/approx.
Pineau, J., Gordon, G., & Thrun, S. (2004). Applying metric-trees to belief-point POMDPs. In
Neural Information Processing Systems (NIPS), Vol. 16.
Pineau, J., Montermerlo, M., Pollack, M., Roy, N., & Thrun, S. (2003). Towards robotic assistants in
nursing homes: challenges and results. Robotics and Autonomous Systems, 42(3-4), 271–281.
Pollack, M. (2002). Planning technology for intelligent cognifitve orthotics. In Proceedings of the
6th International Conference on AI Planning & Scheduling (AIPS).
379

P INEAU , G ORDON & T HRUN

Poon, K.-M. (2001). A fast heuristic algorithm for decision-theoretic planning. Master’s thesis, The
Hong-Kong University of Science and Technology.
Poupart, P., & Boutilier, C. (2000). Value-directed belief state approximation for POMDPs. In
Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI), pp.
409–416.
Poupart, P., & Boutilier, C. (2003). Value-directed compression of POMDPs. In Advances in Neural
Information Processing Systems (NIPS), Vol. 15.
Poupart, P., & Boutilier, C. (2004). Bounded finite state controllers. In Advances in Neural Information Processing Systems (NIPS), Vol. 16.
Rabiner, L. R. (1989). A tutorial on hidden Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2), 257–285.
Rosencrantz, M., Gordon, G., & Thrun, S. (2003). Locating moving entities in dynamic indoor
environments with teams of mobile robots. In Second International Joint Conference on
Autonomous Agents and MultiAgent Systems (AAMAS), pp. 233–240.
Roy, N., & Gordon, G. (2003). Exponential family PCA for belief compression in POMDPs. In
Advances in Neural Information Processing Systems (NIPS), Vol. 15, pp. 1043–1049.
Smith, T., & Simmons, R. (2004). Heuristic search value iteration for POMDPs. In Proceedings of
the Twentieth Conference on Uncertainty in Artificial Intelligence (UAI).
Sondik, E. J. (1971). The Optimal Control of Partially Observable Markov Processes. Ph.D. thesis,
Stanford University.
Spaan, M., & Vlassis, N. (2005). Perseus: Randomized point-based value iteration for POMDPs.
Journal of Artificial Intelligence Research (JAIR), 195–220.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press.
Thrun, S., Fox, D., Burgard, W., & Dellaert, F. (2000). Robust Monte Carlo localization for mobile
robots. Artificial Intelligence, 128(1-2), 99–141.
Vlassis, N., & Spaan, M. T. J. (2004). A fast point-based algorithm for POMDPs. In Proceedings
of the Belgian-Dutch Conference on Machine Learning.
White, C. C. (1991). A survey of solution techniques for the partially observed Markov decision
process. Annals of Operations Research, 32, 215–230.
Zhang, N. L., & Liu, W. (1996). Planning in stochastic domains: Problem characteristics and approximation. Tech. rep. HKUST-CS96-31, Dept. of Computer Science, Hong Kong University of Science and Technology.
Zhang, N. L., & Zhang, W. (2001). Speeding up the convergence of value iteration in partially
observable Markov decision processes. Journal of Artificial Intelligence Research, 14, 29–
51.
Zhou, R., & Hansen, E. A. (2001). An improved grid-based approximation algorithm for POMDPs.
In Proceedings of the 17th International Joint Conference on Artificial Intelligence (IJCAI),
pp. 707–716.

380

Journal of Artificial Intelligence Research 27 (2006) 203-233

Submitted 01/06; published 10/06

Active Learning with Multiple Views
Ion Muslea

imuslea@languageweaver.com

Language Weaver, Inc.
4640 Admiralty Way, Suite 1210
Marina del Rey, CA 90292

Steven Minton

minton@fetch.com

Fetch Technologies, Inc.
2041 Rosecrans Ave., Suite 245
El Segundo, CA 90245

Craig A. Knoblock

knoblock@isi.edu

University of Southern California
4676 Admiralty Way
Marina del Rey, CA 90292

Abstract
Active learners alleviate the burden of labeling large amounts of data by detecting and
asking the user to label only the most informative examples in the domain. We focus here
on active learning for multi-view domains, in which there are several disjoint subsets of
features (views), each of which is sufficient to learn the target concept. In this paper we
make several contributions. First, we introduce Co-Testing, which is the first approach
to multi-view active learning. Second, we extend the multi-view learning framework by
also exploiting weak views, which are adequate only for learning a concept that is more
general/specific than the target concept. Finally, we empirically show that Co-Testing
outperforms existing active learners on a variety of real world domains such as wrapper
induction, Web page classification, advertisement removal, and discourse tree parsing.

1. Introduction
Labeling the training data for a machine learning algorithm is a tedious, time consuming,
error prone process; furthermore, in some application domains, the labeling of each example
may also be extremely expensive (e.g., it may require running costly laboratory tests).
Active learning algorithms (Cohn, Atlas, & Ladner, 1994; Roy & McCallum, 2001; Tong
& Koller, 2001) cope with this problem by detecting and asking the user to label only the
most informative examples in the domain, thus reducing the user’s involvement in the data
labeling process.
In this paper, we introduce Co-Testing, an active learning technique for multi-view
learning tasks; i.e., tasks that have several disjoint subsets of features (views), each of
which is sufficient to learn the concepts of interest. For instance, Web page classification
is a multi-view task because Web pages can be classified based on the words that appear
either in the documents or in the hyperlinks pointing to them (Blum & Mitchell, 1998);
similarly, one can classify segments of televised broadcast based either on the video or on
the audio information, or one can perform speech recognition based on either sound or lip
motion features (de Sa & Ballard, 1998).
c
2006
AI Access Foundation. All rights reserved.

Muslea, Minton, & Knoblock

Co-Testing is a two-step iterative algorithm that requires as input a few labeled and
many unlabeled examples. First, Co-Testing uses the few labeled examples to learn a
hypothesis in each view. Then it applies the learned hypotheses to all unlabeled examples
and detects the set of contention points (i.e., unlabeled examples on which the views predict
a different label); finally, it queries (i.e., asks the user to label) one of the contention
points, adds the newly labeled example to the training set, and repeats the whole process.
Intuitively, Co-Testing relies on the following observation: if, for an unlabeled example,
the hypotheses learned in each view predict a different label, at least one of them makes a
mistake on that particular prediction. By asking the user to label such a contention point,
Co-Testing is guaranteed to provide useful information for the view that made the mistake.
In this paper we make several contributions. First, we introduce Co-Testing, a family of
active learners for multi-view learning tasks. Second, we extend the traditional multi-view
learning framework by also allowing the use of weak views, in which one can adequately learn
only a concept that is strictly more general or more specific than the target concept (all
previous multi-view work makes the strong view assumption that each view is adequate for
learning the target concept). Last but not least, we show that, in practice, Co-Testing clearly
outperforms existing active learners on a variety of real world domains such as wrapper
induction, Web page classification, advertisement removal, and discourse tree parsing.
Compared with previous work, Co-Testing is unique in several ways:
1. existing multi-view approaches (Blum & Mitchell, 1998; Collins & Singer, 1999; Pierce
& Cardie, 2001), which also use a small set of labeled and a large set of unlabeled
examples, are based on the idea of bootstrapping the views from each other. In contrast, Co-Testing is the first algorithm that exploits multiple views for active learning
purposes. Furthermore, Co-Testing allows the simultaneous use of strong and weak
views without additional data engineering costs.
2. existing active learners, which pool all domain features together, are typically designed
to exploit some properties specific to a particular (type of) base learner (i.e., the algorithm used to learn the target concept); for example, uncertainty reduction methods
assume that the base learner provides a reliable estimate of its confidence in each
prediction. In contrast, Co-Testing uses the multiple views to detect the contention
points, among which it chooses the next query. This approach has several advantages:
- it converges quickly to the target concept because it is based on the idea of learning
from mistakes (remember that each contention point is guaranteed to represent
a mistake in at least one of the views). In contrast, existing active learners often
times query examples that are classified correctly, but with a low confidence.
- in its simplest form (i.e., Naive Co-Testing, which is described in section 4), it
makes no assumptions about the properties of the base learner. More precisely,
by simply querying an arbitrary contention point, Co-Testing is guaranteed to
provide “the mistaken view” with a highly informative example.
- by considering only the contention points as query candidates, it allows the use of
query selection heuristics that - computationally - are too expensive to be applied
to the entire set of unlabeled examples.
204

Active Learning with Multiple Views

The remainder of the paper is organized as follows. First, we introduce the concepts
and notation, followed by a comprehensive survey of the literature on active and multi-view
learning. Then we formally introduce the Co-Testing family of algorithms and we present
our empirical evaluation on a variety of real-world domains.

2. Preliminaries: Terminology and Notation
For any given learning task, the set of all possible domain examples is called the instance
space and is denoted by X. Any x ∈ X represents a particular example or instance. In this
paper we are concerned mostly with examples that are represented as feature vectors that
store the values of the various attributes or features that describe the example.
The concept to be learned is called the target concept, and it can be seen as a function
c : X → {l1 , l2 , . . . , lN } that classifies any instance x as a member of one of the N classes of
interest l1 , l2 , . . . , lN . In order to learn the target concept, the user provides a set of training
examples, each of which consists of an instance x ∈ X and its label, c(x). The notation
hx, c(x)i denotes such a training example. The symbol L is used to denote the set of labeled
training examples (also known as the training set).
Given a training set L for the target concept c, an inductive learning algorithm L
searches for a function h : X → {l1 , l2 , . . . , lN } such that ∀x ∈ X, h(x) = c(x). The learner
L searches for h within the set H of all possible hypotheses, which is (typically) determined
by the person who designs the learning algorithm. A hypothesis h is consistent with the
training set L if and only if ∀hx, c(x)i ∈ L, h(x) = c(x). Finally, the version space V S H,L
represents the subset of hypotheses in H that are consistent with the training set L.
By definition, a passive learning algorithm takes as input a randomly chosen training
set L. In contrast, active learning algorithms have the ability to choose the examples in L.
That is, they detect the most informative examples in the instance space X and ask the
user to label only them; the examples that are chosen for labeling are called queries. In
this paper we focus on selective sampling algorithms, which are active learners that choose
the queries from a given working set of unlabeled examples U (we use the notation hx, ?i
to denote an unlabeled examples). In this paper the terms active learning and selective
sampling are used interchangeably.
In the traditional, single-view machine learning scenario, a learner has access to the entire
set of domain features. By contrast, in the multi-view setting one can partition the domain’s
features in subsets (views) that are sufficient for learning the target concept. Existing multiview learners are semi-supervised algorithms: they exploit unlabeled examples to boost the
accuracy of the classifiers learned in each view by bootstrapping the views from each other.
In multi-view learning, an example x is described by a different set of features in each
view. For example, in a domain with k views V1 , V2 , . . . Vk , a labeled example can be seen
as a tuple hx1 , x2 , . . . , xk , li, where l is its label, and x1 , x2 , . . . , xk are its descriptions in
the k views. Similarly, a k-view unlabeled example is denoted by hx1 , x2 , . . . , xk , ?i. For
any example x, Vi (x) denotes the descriptions xi of x in Vi . Similarly, Vi (L) consists of the
descriptions in Vi of all the examples in L.
205

Muslea, Minton, & Knoblock

3. Background on Active and Multi-view Learning
Active learning can be seen as a natural development from the earlier work on optimum
experimental design (Fedorov, 1972). In the early 1980s, the machine learning community
started recognizing the advantages of inductive systems that are capable of querying their
instructors. For example, in order to detect errors in Prolog programs, the Algorithmic
Debugging System (Shapiro, 1981, 1982) was allowed to ask the user several types of queries.
Similarly, concept learning systems such as Marvin (Sammut & Banerji, 1986) and cat
(Gross, 1991) used queries as an integral part of their respective learning strategies.
Our literature review below is structured as follows. First, we discuss the early, mostly
theoretical results on query construction. Then we focus on selective sampling algorithms,
which select as the next query one of the unlabeled examples from the working set. Finally,
we conclude by reviewing the existing multi-view learning algorithms.
3.1 Active Learning by Query Construction
The earliest approaches to formalizing active learning appeared in the seminal papers of
Angluin (1982, 1988) and Valiant (1984), who focused on exact concept induction and
learning in the pac framework, respectively. This theoretic work focused on learning classes
of concepts such as regular sets, monotone dnf expressions, and µ−expressions. Besides
membership queries such as “is this an example of the target concept?,” Angluin also used more
sophisticated types of queries such as equivalence queries (“is this concept equivalent with the
target concept?”) or superset queries (“is this concept a superset of the target concept?”).
These early active learners took a constructive approach to query generation in the
sense that each query is (artificially) constructed by setting the values of the attributes
so that the query is as informative as possible. In practice, this may raise some serious
problems; for example, consider a hand-writing recognizer that must discriminate between
the 10 digits (Lang & Baum, 1992). In this scenario, an informative query may consist of an
image that represents a “fusion” of two similarly-looking digits, such as “3” and “5.” When
presented with such an image, a user cannot label it properly because it does not represent
a recognizable digit. Consequently, a query is “wasted” on a totally irrelevant image.
Similar situations appear in many real world tasks such as text classification, information
extraction, or speech recognition: whenever the active learner artificially builds a query for
such a domain, it is highly unlikely that the newly created object has any meaning for the
human user.
Despite this practical applicability issue, the constructive approach to active learning
leads to interesting theoretical insights about the merits of various types of queries. For
example, researchers considered learning with:
- incomplete queries, for which the query’s answer may be “I don’t know.” (Angluin &
Slonim, 1991; Goldman & Mathias, 1992; Sloan & Turan, 1994; Blum, Chalasani,
Goldman, & Slonim, 1998);
- malicious queries, for which the answer to the queries may be erroneous (Angluin, Krikis,
Sloan, & Turan, 1997; Angluin & Krikis, 1994; Angluin, 1994).
New learning problems were also considered, from unrestricted dnf expressions (Jackson, 1994; Blum, Furst, Jackson, Kearns, Mansour, & Rudich, 1994) and unions of boxes
206

Active Learning with Multiple Views

(Goldberg, Goldman, & Mathias, 1994) to tree patterns (Amoth, Cull, & Tadepalli, 1998,
1999) and Horn clauses (Reddy & Tadepalli, 1997). Researchers also reported results on
applying active learning to neural networks (Hwang, Choi, Oh, & Marks, 1991; Baum, 1991;
Watkin & Rau, 1992; Hasenjager & Ritter, 1998) and for combining declarative bias (prior
knowledge) and active learning (Tadepalli, 1993; Tadepalli & Russell, 1998).
3.2 Selective Sampling
Selective sampling represents an alternative active learning approach. It typically applies to
classification tasks in which the learner has access to a large number of unlabeled examples.
In this scenario, rather than constructing an informative query, the active learner asks the
user to label one of the existing unlabeled examples. Depending on the source of unlabeled
examples, there are two main types of sampling algorithms: stream- and pool- based.
The former assumes that the active learner has access to an (infinite) stream of unlabeled
examples (Freund, Seung, Shamir, & Tishby, 1997; Argamon-Engelson & Dagan, 1999;
Dagan & Engelson, 1995); as successive examples are presented to it, the active learner
must decide which of them should be labeled by the user. In contrast, in the pool-based
scenario (Lewis & Gale, 1994; Lewis & Catlett, 1994; McCallum & Nigam, 1998; Muslea,
Minton, & Knoblock, 2000, 2002a), the learner is presented with a working set of unlabeled
examples; in order to make a query, the active learner goes through the entire pool and
selects the example to be labeled next.
Based on the criterion used to select the next query, selective sampling algorithms fall
under three main categories:
- uncertainty reduction: the system queries the example on which the current hypothesis
makes the least confident prediction;
- expected-error minimization: the system queries the example that maximizes the expected
reduction in classification error;
- version space reduction: the system queries the example that, once labeled, removes as
much as possible of the version space.
The uncertainty reduction approach to selective sampling works as follows: first, one uses
the labeled examples to learn a classifier; then the system queries the unlabeled example
on which this classifier makes the least confident prediction. This straightforward idea can
be applied to any base learner for which one can reliably estimate the confidence of its
predictions. Confidence-estimation heuristics were proposed for a variety of base learners
such as logistic regression (Lewis & Gale, 1994; Lewis & Catlett, 1994), partially hidden
Markov Models (Scheffer & Wrobel, 2001), support vector machines (Schohn & Cohn, 2000;
Campbell, Cristianini, & Smola, 2000), and inductive logic programming (Thompson, Califf,
& Mooney, 1999).
The second, more sophisticated approach to selective sampling, expected-error minimization, is based on the statistically optimal solution to the active learning problem. In
this scenario, the intuition is to query the unlabeled example that minimizes the error rate
of the (future) classifier on the test set. Even though for some (extremely simple) base
learners one can find such optimal queries (Cohn, Ghahramani, & Jordan, 1996), this is not
207

Muslea, Minton, & Knoblock

true for most inductive learners. Consequently, researchers proposed methods to estimate
the error reduction for various types of base learners. For example, Roy and McCallum
(2001) use a sample estimation method for the Naive Bayes classifier; similar approaches
were also described for parameter learning in Bayesian nets (Tong & Koller, 2000) and for
nearest neighbor classifiers (Lindenbaum, Markovitch, & Rusakov, 2004).
The heuristic approach to expected-error minimization can be summarized as follows.
First, one chooses a loss function (Roy & McCallum, 2001) that is used to estimate the
future error rate. Then each unlabeled example x in the working set is considered as the
possible next query, and the system estimates the expected reduction of the error rate for
each possible label that x may take. Finally, the system queries the unlabeled example that
leads to the largest estimated reduction in the error rate.
Finally, a typical version space reduction active learner works as follows: it generates
a committee of several hypotheses, and it queries the unlabeled examples on which the
disagreement within the committee is the greatest. In a two-class learning problem, this
strategy translates into making queries that remove approximately half of the version space.
Depending on the method used to generate the committee, one can distinguish several types
of active learners:
- Query-by-Committee selects a committee by randomly sampling hypotheses from the
version space. Query-by-Committee was applied to a variety of base learners such as
perceptrons (Freund et al., 1997), Naive Bayes (McCallum & Nigam, 1998), and Winnow (Liere & Tadepalli, 1997). Furthermore, Argamon-Engelson and Dagan (1999,
1995) introduce an extension to Query-by-Committee for Bayesian learning. In the
Bayesian framework, one can create the committee by sampling classifiers according
to their posterior distributions; that is, the better a hypothesis explains the training
data, the more likely it is to be sampled. The main limitation of Query-by-Committee
is that it can be applied only to base learners for which it is feasible to randomly sample hypotheses from the version space.
- sg-net (Cohn et al., 1994) creates a 2-hypothesis committee that consists of a “mostgeneral” and a “most-specific” classifier. These two hypotheses are generated by modifying the base learner so that it learns a classifier that labels as many as possible of
the unlabeled examples in the working set as positive or negative, respectively. This
approach has an obvious drawback: it requires the user to modify the base learner so
that it can generate “most-general” and “most-specific” classifiers.
- Active-Decorate (Melville & Mooney, 2004) can be seen as both a generalization and an
improvement of sg-net. It generates a large and diverse committee by successively
augmenting the original training set with additional sets of artificially-generated examples. More precisely, it generates artificial examples in keeping with the distribution
of the instance space; then it applies the current committee to each such example, and
it labels the artificial example with the label that contradicts most of the committee’s
predictions. A new classifier is learned from this augmented dataset, and then the
entire process is repeated until the desired committee size is reached. Active-Decorate
was successfully used for domains with nominal and numeric features, but it is unclear
how it could be applied to domains such as text classification or extraction, where
generating the artificial examples may be problematic.
208

Active Learning with Multiple Views

- Query-by-Bagging and Query-by-Boosting (Abe & Mamitsuka, 1998) create the committee by using the well-known bagging (Breiman, 1996) and boosting (Schapire, 1990)
algorithms, respectively. These algorithms were introduced for the c4.5 base learner,
for which both bagging and boosting are known to work extremely well.
In general, committee-based sampling tends to be associated with the version space
reduction approach. However, for base learners such as support vector machines, one can
use a single hypothesis to make queries that remove (approximately) half of the version
space (Tong & Koller, 2001). Conversely, committee-based sampling can also be seen as
relying on the uncertainty reduction principle: after all, the unlabeled example on which
the disagreement within the committee is the greatest can be also seen as the example that
has the least certain classification.
3.3 Multi-view, Semi-supervised Learning
As already mentioned, Blum and Mitchell (1998) provided the first formalization of learning
in the multi-view framework. Previously, this topic was largely ignored, though the idea
clearly shows up in applications such as word sense disambiguation (Yarowsky, 1995) and
speech recognition (de Sa & Ballard, 1998). Blum and Mitchell proved that two independent, compatible views can be used to pac-learn (Valiant, 1984) a concept based on few
labeled and many unlabeled examples. They also introduced Co-Training, which is the first
general-purpose, multi-view algorithm.
Collins and Singer (1999) proposed a version of Co-Training that is biased towards
learning hypotheses that predict the same label on most of the unlabeled examples. They
introduce an explicit objective function that measures the compatibility of the learned
hypotheses and use a boosting algorithm to optimize this objective function. In a related
paper (Dasgupta, Littman, & McAllester, 2001), the authors provide pac-like guarantees
for this novel Co-Training algorithm (the assumption is, again, that the views are both
independent and compatible). Intuitively, Dasgupta et al. (2001) show that the ratio of
contention points to unlabeled examples is an upper-bound on the error rate of the classifiers
learned in the two views.
Abney (2002) extends the work of Dasgupta et al. by relaxing the view independence
assumption. More precisely, the author shows that even with views that are weakly dependent, the ratio of contention points to unlabeled examples still represents an upper-bound
on the two view’s error rate. Unfortunately, this paper introduces just a theoretical definition for the weak dependence of the views, without providing an intuitive explanation of its
practical consequences.
Researchers proposed two main types of extensions to the original Co-Training algorithm: modifications of the actual algorithm and changes aiming to extend its practical
applicability. The former cover a wide variety of scenarios:
- Co-EM (Nigam & Ghani, 2000; Brefeld & Scheffer, 2004) uses Expectation Maximization
(Dempster, Laird, & Rubin, 1977) for multi-view learning. Co-EM can be seen as the
closest implementation of the theoretical framework proposed by Blum and Mitchell
(1998).
209

Muslea, Minton, & Knoblock

- Ghani (2002) uses Error-Correcting Output Codes to allow Co-Training and Co-EM to
scale up to problems with a large number of classes.
- Corrected Co-Training (Pierce & Cardie, 2001) asks the user to manually correct the
labels of the bootstrapped examples. This approach is motivated by the observation
that the quality of the bootstrapped data is crucial for Co-Training’s convergence.
- Co-Boost (Collins & Singer, 1999) and Greedy Agreement (Abney, 2002) are Co-Training
algorithms that explicitly aim to minimize the number of contention points.
The second group of extensions to Co-Training is motivated by the fact that, in practice,
one also encounters many problems for which there is no straightforward way to split the
features in two views. In order to cope with this problem, Goldman and Zhou (2000)
advocate the use of multiple biases instead of multiple views. The authors introduce an
algorithm similar to Co-Training, which bootstraps from each other hypotheses learned by
two different base learners; this approach relies on the assumption that the base learners
generate hypotheses that partition the instance space into equivalence classes. In a recent
paper, Zhou and Goldman (2004) use the idea of a multi-biased committee for active
learning; i.e., they use various types base learners to obtain a diverse committee, and then
query the examples on which this committee disagree the most.
Within the multi-view framework, Nigam and Ghani (2000) show that, for “bag-ofwords” text classification, one can create two views by arbitrarily splitting the original set
of features into two sub-sets. Such an approach fits well the text classification domain, in
which the features are abundant, but it is unlikely to work on other types of problems.
An alternative solution is proposed by Raskutti, Ferra, and Kowalczyk (2002), where the
authors create a second view that consists of a variety of features that measure the examples’
similarity with the N largest clusters in the domain. Finally, Muslea, Minton, and Knoblock
(2002b) propose a meta-learning approach that uses past experiences to predict whether
the given views are appropriate for a new, unseen learning task.

4. The Co-Testing Family of Algorithms
In this section, we discuss in detail the Co-Testing family of algorithms. As we already
mentioned, Co-Testing can be seen as a two-step iterative process: first, it uses a few
labeled examples to learn a hypothesis in each view; then it queries an unlabeled example
for which the views predict different labels. After adding the queried example to the training
set, the entire process is repeated for a number of iterations.
The remainder of this section is organized as follows: first, we formally present the CoTesting family of algorithms and we discuss several of its members. Then we introduce the
concepts of strong and weak views, and we analyze how Co-Testing can exploit both types
of views (previous multi-view learners could only use strong views). Finally, we compare
and contrast Co-Testing to the related approaches.
4.1 The Family of Algorithms
Table 1 provides a formal description on the Co-Testing family of algorithms. The input
consists of k views V1 , V2 , . . . , Vk , a base learner L, and the sets L and U of labeled and
210

Active Learning with Multiple Views

Table 1: The Co-Testing family of algorithms: repeatedly learn a classifier in each view and
query an example on which they predict different labels.
Given:
- a base learner L
- a learning domain with features V = {a1 , a2 , . . . , aN }
- k views V1 , V2 , . . . , Vk such that V =

k
[

i=1

Vi and ∀i, j ∈ {1, 2, . . . , k}, i 6= j, Vi ∩ Vj = Ø

- the sets L and U of labeled and unlabeled examples, respectively
- number N of queries to be made
- LOOP for N iterations
- use L to learn the classifiers h1 , h2 , . . . , hk in the views V1 , V2 , . . . , Vk , respectively
- let ContentionP oints = { hx1 , x2 , . . . , xk , ?i ∈ U | ∃i, j hi (xi ) 6= hj (xj ) }
- let hx1 , x2 , . . . , xk , ?i = SelectQuery(ContentionP oints)
- remove hx1 , x2 , . . . , xk , ?i from U and ask for its label l
- add hx1 , x2 , . . . , xk , li to L
- hOU T = CreateOutputHypothesis( h1 , h2 , . . . , hk )

unlabeled examples, respectively. Co-Testing algorithms work as follows: first, they learn
the classifiers h1 , h2 , . . . , hk by applying the algorithm L to the projection of the examples in
L onto each view. Then they apply h1 , h2 , . . . , hk to all unlabeled examples in U and create
the set of contention points, which consists of all unlabeled examples for which at least two
of these hypotheses predict a different label. Finally, they query one of the contention points
and then repeat the whole process for a number of iterations. After making the allowed
number of queries, Co-Testing creates an output hypothesis that is used to make the actual
predictions.
The various members of the Co-Testing family differ from each other with two respects:
the strategy used to select the next query, and the manner in which the output hypothesis
is constructed. In other words, each Co-Testing algorithm is uniquely defined by the choice
of the functions SelectQuery() and CreateOutputHypothesis().
In this paper we consider three types of query selection strategies:
- naive: choose at random one of the contention points. This straightforward strategy
is appropriate for base learners that lack the capability of reliably estimating the
confidence of their predictions. As this naive query selection strategy is independent
of both the domain and the base learner properties, it follows that it can be used for
solving any multi-view learning task.
- aggressive: choose as query the contention point Q on which the least confident of the
hypotheses h1 , h2 , . . . , hk makes the most confident prediction; more formally,
211

Muslea, Minton, & Knoblock

Q=

arg max

min

x∈ContentionP oints i∈{1,2,...,k}

Conf idence(hi (x))

Aggressive Co-Testing is designed for high accuracy domains, in which there is little
or no noise. On such domains, discovering unlabeled examples that are misclassified
“with high confidence” translates into queries that remove significantly more than half
of the version space.
- conservative: choose the contention point on which the confidence of the predictions
made by h1 , h2 , . . . , hk is as close as possible (ideally, they would be equally confident
in predicting different labels); that is,
Q=

arg min
x∈ContentionP oints

max

f ∈{h1 ,...,hk }

Conf idence(f (x)) −

min

g∈{h1 ,...,hk }

Conf idence(g(x))

!

Conservative Co-Testing is appropriate for noisy domains, where the aggressive strategy may end up querying mostly noisy examples.
Creating the output hypothesis also allows the user to choose from a variety of alternatives, such as:
- weighted vote: combines the vote of each hypothesis, weighted by the confidence of their
respective predictions.
X

hOU T (x) = arg max
l∈Labels

Conf idence(g(x))

g ∈ {h1 , . . . , hk }
g(x) = l

- majority vote: Co-Testing chooses the label that was predicted by most of the hypotheses
learned in the k views.
hOU T (x) = arg max
l∈Labels

X

1

g ∈ {h1 , . . . , hk }
g(x) = l

This strategy is appropriate when there are at least three views, and the base learner
cannot reliably estimate the confidence of its predictions.
- winner-takes-all: the output hypothesis is the one learned in the view that makes the
smallest number of mistakes over the N queries. This is the most obvious solution for
2-view learning tasks in which the base learner cannot (reliably) estimate the confidence of its predictions. If we denote by M istakes(h1 ), M istakes(h2 ), . . . , M istakes(hk )
the number of mistakes made by the hypotheses learned in the k views on the N
queries, then
hOU T (x) = arg min M istakes(g)
g∈{h1 ,...,hk }

212

Active Learning with Multiple Views

4.2 Learning with Strong and Weak Views
In the original multi-view setting (Blum & Mitchell, 1998; Muslea et al., 2000), one makes
the strong views assumption that each view is sufficient to learn the target concept. However,
in practice, one also encounters views in which one can accurately learn only a concept that
is strictly more general or more specific than the concept of interest (Muslea, Minton, &
Knoblock, 2003). This is often the case in domains that involve hierarchical classification,
such as information extraction or email classification. For example, it may be extremely
easy to discriminate (with a high accuracy) between work and personal emails based solely
on the email’s sender; however, this same information may be insufficient for predicting the
work or personal sub-folder in which the email should be stored.
We introduce now the notion of a weak view, in which one can accurately learn only a
concept that is strictly more general or more specific than the target concept. Note that
learning in a weak view is qualitatively different from learning “an approximation” of the
target concept: the latter represents learning with imperfect features, while the former
typically refers to a (easily) learnable concept that is a strict generalization/specialization
of the target concept (note that, in the real world, imperfect features and noisy labels affect
learning in both strong and weak views).
In the context of learning with strong and weak views, we redefine contention points
as the unlabeled examples on which the strong views predict a different label. This is a
necessary step because of two reasons: first, as the weak view is inadequate for learning the
target concept, it typically disagrees with the strong views on a large number of unlabeled
examples; in turn, this would increase the number of contention points and skew their
distribution. Second, we are not interested in fixing the mistakes made by a weak view, but
rather in using this view as an additional information source that allows faster learning in
the strong views (i.e., from fewer examples).
Even though the weak views are inadequate for learning the target concept, they can
be exploited by Co-Testing both in the SelectQuery() and CreateOutputHypothesis()
functions. In particular, weak views are extremely useful for domains that have only two
strong views:
- the weak view can be used in CreateOutputHypothesis() as a tie-breaker when the
two strong views predict a different label.
- SelectQuery() can be designed so that, ideally, each query would represent a mistake in
both strong views. This can be done by first detecting the contention points - if any
- on which the weak view disagrees with both strong views; among these, the next
query is the one on which the weak view makes the most confident prediction. Such
queries are likely to represent a mistake in both strong views, rather than in just one
of them; in turn, this implies simultaneous large cuts in both strong version spaces,
thus leading to faster convergence.
In section 5.2.2 we describe a Co-Testing algorithm that exploits strong and weak views
for wrapper induction domains (Muslea et al., 2003; Muslea, Minton, & Knoblock, 2001).
Note that learning from strong and weak views clearly extends beyond wrapper induction
tasks: for example, the idea of exploiting complementary information sources (i.e., different
213

Muslea, Minton, & Knoblock

types of features) appears in the two multi-strategy learners (Kushmerick, Johnston, &
McGuinness, 2001; Nahm & Mooney, 2000) that are discussed in section 4.3.2.
4.3 Co-Testing vs. Related Approaches
As we already mentioned in section 3.3, existing multi-view approaches are typically semisupervised learners that bootstrap the views from each other. The two exceptions (Muslea
et al., 2002a; Jones, Ghani, Mitchell, & Riloff, 2003) interleave Co-Testing and Co-EM
(Nigam & Ghani, 2000), thus combining the best of both worlds: semi-supervised learning
provides the active learner with more accurate hypotheses, which lead to more informative queries; active learning provides the semi-supervised learner with a more informative
training set, thus leading to faster convergence.
4.3.1 Co-Testing vs. Existing Active Learners
Intuitively, Co-Testing can be seen as a committee-based active learner that generates a
committee that consists of one hypothesis in each view. Also note that Co-Testing can be
combined with virtually any of the existing single-view active learners: among the contention
points, Co-Testing can select the next query based on any of the heuristics discussed in
section 3.2.
There are two main differences between Co-Testing and other active learners:
- except for Co-Testing and its variants (Muslea et al., 2002a; Jones et al., 2003), all
other active learners work in the single-view framework (i.e., they pool together all
the domain features).
- single-view active learners are typically designed for a particular (class of) base learner(s).
For example, Query-by-Committee (Seung, Opper, & Sompolinski, 1992) assumes
that one can randomly sample hypotheses from the version space, while Uncertainty
Sampling (Lewis & Gale, 1994; Lewis & Catlett, 1994) relies on the base learner’s
ability to reliably evaluate the confidence of its predictions. In contrast, the basic
idea of Co-Testing (i.e., querying contention points) applies to any multi-view problem,
independently of the base learner to be used.
The Co-Testing approach to active learning has both advantages and disadvantages.
On one hand, Co-Testing cannot be applied to problems that do not have at least two
views. On the other hand, for any multi-view problem, Co-Testing can be used with the
best base learner for that particular task. In contrast, in the single-view framework, one
often must either create a new active learning method for a particular base learner or, even
worse, modify an existing base learner so that it can be used in conjunction with an existing
sampling algorithm.
To illustrate this last point, let us briefly consider learning for information extraction,
where the goal is to use machine learning for extracting relevant strings from a collection
of documents (e.g., extract the perpetrators, weapons, and victims from a corpus of news
stories on terrorist attacks). As information extraction is different in nature from a typical
classification task, existing active learners cannot be applied in a straightforward manner:
- for information extraction from free text (ie), the existing active learners (Thompson
et al., 1999; Soderland, 1999; Scheffer & Wrobel, 2001) are crafted based on heuristics
214

Active Learning with Multiple Views

specific to their respective base learners, rapier, whisk, and Partially Hidden Markov
Models. An alternative is discussed by Finn and Kushmerick (2003), who explore
a variety of ie-specific heuristics that can be used for active learning purposes and
analyze the trade-offs related to using these heuristics.
- for wrapper induction, where the goal is to extract data from Web pages that share
the same underlying structure, there are no reported results for applying (singleview) active learning. This is because typical wrapper induction algorithms (Muslea
et al., 2001; Kushmerick, 2000; Hsu & Dung, 1998) are base learners that lack the
properties exploited by the single-view active learners reviewed in section 3.2.: they are
determinist learners that are noise sensitive, provide no confidence in their predictions,
and make no mistakes on the training set.
In contrast, Co-Testing applies naturally to both wrapper induction (Muslea et al., 2000,
2003) and information extraction from free text (Jones et al., 2003). This is due to the
fact that Co-Testing does not rely on the base learner’s properties to identify its highly
informative set of candidate queries; instead, it focuses on the contention points, which, by
definition, are guaranteed to represent mistakes in some of the views.
4.3.2 Exploiting Weak Views
We briefly discuss now two learning tasks that can be seen as learning from strong and
weak views, even though they were not formalized as such, and the views were not used
for active learning. An additional application domain with strong and weak views, wrapper
induction, is discussed at length in section 5.2.
The discotex (Nahm & Mooney, 2000) system was designed to extract job titles,
salaries, locations, etc from computer science job postings to the newsgroup austin.jobs.
discotex proceeds in four steps: first, it uses rapier (Califf & Mooney, 1999) to learn
extraction rules for each item of interest. Second, it applies the learned rules to a large,
unlabeled corpus of job postings and creates a database that is populated with the extracted
data. Third, by text mining this database, discotex learns to predict the value of each item
based on the values of the other fields; e.g., it may discover that “IF the job requires c++
and corba THEN the development platforms include Windows.” Finally, when the system
is deployed and the rapier rules fail to extract an item, the mined rules are used to predict
the item’s content.
In this scenario, the rapier rules represent the strong view because they are sufficient
for extracting the data of interest. In contrast, the mined rules represent the weak view
because they cannot be learned or used by themselves. Furthermore, as discotex discards
all but the most accurate of the mined rules, which are highly-specific, it follows that this
weak view is used to learn concepts that are more specific than the target concept. Nahm
and Mooney (2000) show that the mined rules improve the extraction accuracy by capturing
information that complements the rapier extraction rules.
Another domain with strong and weak views is presented by Kushmerick et al. (2001).
The learning task here is to classify the lines of text on a business card as a person’s name,
affiliation, address, phone number, etc. In this domain, the strong view consists of the
words that appear on each line, based on which a Naive Bayes text classifier is learned.
In the weak view, one can exploit the relative order of the lines on the card by learning a
215

Muslea, Minton, & Knoblock

Hidden Markov Model that predicts the probability of a particular ordering of the lines on
the business card (e.g., name followed by address, followed by phone number).
This weak view defines a class of concepts that is more general than the target concept:
all line orderings are possible, even though they are not equally probable. Even though
the order of the text lines cannot be used by itself to accurately classify the lines, when
combined with the strong view, the ordering information leads to a classifier that clearly
outperforms the stand-alone strong view (Kushmerick et al., 2001).
Note that both approaches above use the strong and weak views for passive, rather
than active learning. That is, given a fixed set of labeled and no unlabeled examples, these
algorithms learn one weak and one strong hypothesis that are then used to craft a domainspecific predictor that outperforms each individual hypothesis. In contrast, Co-Testing is
an active learner that seamlessly integrates weak and strong hypotheses without requiring
additional, domain-specific data engineering.

5. Empirical Validation
In this section we empirically compare Co-Testing with other state of the art learners. Our
goal is to test the following hypothesis: given a multi-view learning problem, Co-Testing
converges faster than its single-view counterparts.
We begin by presenting the results on three real-world classification domains: Webpage classification, discourse tree parsings, and advertisement removal. Then we focus on
an important industrial application, wrapper induction (Muslea et al., 2001; Kushmerick,
2000; Hsu & Dung, 1998), in which the goal is to learn rules that extract the relevant data
from a collection of documents (e.g., extract book titles and prices from a Web site).
The results for classification and wrapper induction are analyzed separately because:
- for each of the three classification tasks, there are only two strong views that are available;
in contrast, for wrapper induction we have two strong and one weak views, which
allows us to explore a wider range of options.
- for each classification domain, there is exactly one available dataset. In contrast, for
wrapper induction we use a testbed of 33 distinct tasks. This imbalance in the number
of available datasets requires different presentation styles for the results.
- in contrast to typical classification, a major requirement for wrapper induction is to learn
(close to) 100%-accurate extraction rules from just a handful of examples (Muslea,
2002, pages 3-6). This requirement leads to significant differences in both the experimental setup and the interpretation of the results (e.g., results that are excellent for
most classification tasks may be unacceptable for wrapper induction).

5.1 Co-Testing for Classification
We begin our empirical study by using three classification tasks to compare Co-Testing with
existing active learners. We first introduce these three domains and their respective views;
then we discuss the learners used in the evaluation and analyze the experimental results.
216

Active Learning with Multiple Views

Domain

L

ad
tf

ib
mc4
Naive
Bayes

courses

Co-Testing
Query
Output
Selection
Hypothesis
naive
winner
naive
winner
naive
weighted
conservative
vote

Single-view Algorithms
QBC
−
−

qBag
√
√

qBst
√
√

US
√

√

√

√

√

−

Rnd
√
√
√

Table 2: The algorithms used for classification. The last five columns denote Query-byCommittee/-Bagging/-Boosting, Uncertainty Sampling and Random Sampling.

5.1.1 The Views used by Co-Testing
We applied Co-Testing to three real-world classification domains for which there is a natural,
intuitive way to create two views:
- ad (Kushmerick, 1999) is a classification problem with two classes, 1500 attributes, and
3279 examples. In ad, images that appear in Web pages are classified into ads and
non-ads. The view V1 consists of all textual features that describe the image; e.g.,
1-grams and 2-grams from the caption, from the url of the page that contains the
image, from the url of the page the image points to, etc. In turn, V2 describes the
properties of the image itself: length, width, aspect ratio, and “origin” (i.e., are the
image and the page that contains it coming from the same Web server?).
- courses (Blum & Mitchell, 1998) is a domain with two classes, 2206 features, and 1042
examples. The learning task consists of classifying Web pages as course homepages
and other pages. In courses the two views consist of words that appear in the page
itself and words that appear in hyperlinks pointing to them, respectively.
- tf (Marcu, Carlson, & Watanabe, 2000) is a classification problem with seven classes, 99
features and 11,193 examples. In the context of a machine translation system, it uses
the shift-reduce parsing paradigm to learn how to rewrite Japanese discourse trees as
English-like discourse trees. In this case, V1 uses features specific to the shift-reduce
parser: the elements in the input list and the partial trees in the stack. V2 consists of
features specific to the Japanese tree given as input.
5.1.2 The Algorithms used in the Evaluation
Table 2 shows the learners used in this empirical comparison. We have implemented all
active learners as extensions of the MLC++ library (Kohavi, Sommerfield, & Dougherty,
1997). For each domain, we choose the base learner as follows: after applying all primitive
learners in MLC ++ on the dataset (10-fold cross-validation), we select the one that obtains
the best performance. More precisely, we are using the following base learners: ib (Aha,
1992) for ad, Naive Bayes (Blum & Mitchell, 1998) for courses, and mc4, which is an
implementation of c4.5, for tf.
217

Muslea, Minton, & Knoblock

The five single-view algorithms from Table 2 use all available features (i.e., V 1 ∪ V2 )
to learn the target concept.1 On all three domains, Random Sampling (Rnd) is used as
strawman; Query-by-Bagging and -Boosting, denoted by qBag and qBst, are also run
on all three domains. In contrast, Uncertainty Sampling (US) is applied only on ad and
courses because mc4, which is the base learner for tf, does not provide an estimate of the
confidence of its prediction.
As there is no known method for randomly sampling from the ib or mc4 version spaces,
Query-by-Committee (QBC) is not applied to ad and tf. However, we apply QBC to
courses by borrowing an idea from McCallum and Nigam (1998): we create the committee by sampling hypotheses according to the (Gamma) distribution of the Naive Bayes
parameters estimated from the training set L.
For Query-by-Committee we use a typical 2-hypothesis committee. For Query-byBagging and -Boosting, we use a relatively small 5-hypothesis committees because of the
cpu constraints: the running time increases linearly with the number of learned hypotheses,
and, in some domains, it takes more than 50 cpu hours to complete the experiments even
with the 5-hypothesis committees.
Because of the limitations of their respective base learners (i.e., the above-mentioned
issue of estimating the confidence in each prediction), for ad and tf we use Naive Co-Testing
with a winner-takes-all output hypothesis; that is, each query is randomly selected among
the contention points, and the output hypothesis is the one learned in the view that makes
the fewest mistakes on the queries. In contrast, for courses we follow the methodology
from the original Co-Training paper (Blum & Mitchell, 1998), where the output hypothesis
consists of the weighted vote of the classifiers learned in each view.
On courses we investigate two of the Co-Testing query selection strategies: naive and
conservative. The third, aggressive query selection strategy is not appropriate for courses
because the “hyperlink view” is significantly less accurate than the other one (after all, one
rarely encounters more than a handful of words in a hyperlink). Consequently, most of
the high-confidence contention points are “unfixable mistakes” in the hyperlink view, which
means that even after seeing the correct label, they cannot be classified correctly in that
view.
5.1.3 The Experimental Results
The learners’ performance is evaluated based on 10-fold, stratified cross validation. On ad,
each algorithm starts with 150 randomly chosen examples and makes 10 queries after each
of the 40 learning episodes, for a total of 550 labeled examples. On courses, the algorithms
start with 6 randomly chosen examples and make one query after each of the 175 learning
episodes. Finally, on tf the algorithms start with 110 randomly chosen examples and make
20 queries after each of the 100 learning episodes.
Figures 1 and 2 display the learning curves of the various algorithms on ad, tf, and
course. On all three domains, Co-Testing reaches the highest accuracy (i.e., smallest error
rate). Table 3 summarizes the statistical significance results (t-test confidence of at least
1. In a preliminary experiment, we have also ran the algorithms of the individual views. The results on
V1 and V2 were either worse then those on V1 ∪ V2 or the differences were statistically insignificant.
Consequently, for sake of simplicity, we decided to show here only the single-view results for V 1 ∪ V2 .

218

Active Learning with Multiple Views

error rate (%)

AD
9
Naive Co-Testing
8.5
Uncertainty Sampling
8
Rnd
7.5
7
6.5
6
5.5
5
4.5
4
3.5
150 200 250 300 350 400 450 500
labeled examples
AD
10
Naive Co-Testing
qBag
qBst
Rnd

error rate (%)

9
8
7
6
5
4

3
150 200 250 300 350 400 450 500
labeled examples
TF
32
Naive Co-Testing
qBag
qBst
Rnd

error rate (%)

30
28
26
24
22
20
18
110

510

910

1310

1710

2110

labeled examples

Figure 1: Empirical results on the ad and tf problems
219

Muslea, Minton, & Knoblock

courses
12
Conservative Co-Testing
Uncertainty Sampling
QBC
Rnd

error rate (%)

10
8
6
4
2
20

60

100

140

180

labeled examples
courses
12
Conservative Co-Testing
qBag
qBst
Rnd

error rate (%)

10
8
6
4
2
20

60

100

140

180

labeled examples
courses
12
Conservative Co-Testing
Naive Co-Testing

error rate (%)

10
8
6
4
2
20

60

100

140

180

labeled examples

Figure 2: Empirical results on the courses problem
220

Active Learning with Multiple Views

Naive Co-Testing

Algorithm
Random Sampling
Uncertainty Sampling
Query-by-Committee
Query-by-Bagging
Query-by-Boosting
Naive Co-Testing

ad

Conservative Co-Testing

tf

courses

Loss

Tie

Win

Loss

Tie

Win

Loss

Tie

Win

0
0
0
0
-

0
2
18
15
-

19
17
1
4
-

0
0
0
0
0
-

21
2
60
6
0
-

70
89
31
85
91
-

0
0
0
0

0
28
0
21

49
21
49
28

Table 3: Statistical significance results in the empirical (pair-wise) comparison of the various
algorithms on the three domains.

95%) obtained in a pair-wise comparison of the various algorithms. These comparisons are
performed on the right-most half of each learning curve (i.e., towards convergence). The
best way to explain the results in Table 3 is via examples: the results of comparing Naive
Co-Testing and Random Sampling on ad appear in the first three columns of the first
row. The three numbers (i.e., 0, 0, and 19) mean that on (all) 19 comparison points Naive
Co-Testing outperforms Random Sampling in a statistically significant manner. Similarly,
comparing Naive and Conservative Co-Testing on courses (the last three columns on the
last row) leads to the following results: on 28 of the comparison points Conservative CoTesting outperforms Naive Co-Testing in a statistically significant manner; on 21 other
points the differences are statistically insignificant; finally, on no comparison point Naive
Co-Testing outperforms its Conservative counterpart.
The results in Table 3 can be summarized as follows. First of all, no single-view algorithm
outperforms Co-Testing in a statistically significant manner on any of the comparison points.
Furthermore, except for the comparison with Query-by-Bagging and -Boosting on ad, where
the difference in accuracy is statistically insignificant on almost all comparison points, CoTesting clearly outperform all algorithms on all domains.
Finally, let us briefly comment on applying multi-view, semi-supervised learners to the
three tasks above. As mentioned in section 3.3, such algorithms bootstrap the views from
each other by training each view on the examples labeled with high-confidence by the
other view. For ad and tf, we could not use multi-view, semi-supervised learning because
the base learners ib and mc4 do not provide a (reliable) estimate of the confidence in
their predictions. More precisely, mc4 provides no estimate at all, while ib’s estimates
are extremely poor when the training data is scarce (e.g., see the poor performance of
Uncertainty Sampling on ad, where it barely outperforms Random Sampling).
On courses, we have applied both Co-Training and Co-EM in conjunction with the
Naive Bayes base learner. Both these multi-view learners reach their maximum accuracy
(close to 95%) based on solely 12 labeled and 933 unlabeled examples, after which their
221

Muslea, Minton, & Knoblock

R2

R1

Name:<i>Gino’s</i><p>Phone:<i> (800)111-1717 </i><p>Cuisine: …
Figure 3: Both the forward and backward rules detect the beginning of the phone number.
performance does not improve in a statistically significant manner.2 As shown in Figure 2,
when the training data is scarce (i.e., under 40 labeled examples), Co-Testing’s accuracy is
less than 95%; however, after making additional queries, Co-Testing reaches 98% accuracy,
while Co-Training and Co-EM remain at 95% even when trained on 180 labeled and 765
unlabeled examples. These results are consistent with the different goals of active and
semi-supervised learning: the former focuses on learning the perfect target concept from
a minimal amount of labeled data, while the latter uses unlabeled examples to boost the
accuracy of a hypothesis learned from just a handful of labeled examples.
5.2 Co-Testing for Wrapper Induction
We focus now on a different type of learning application, wrapper induction (Muslea et al.,
2001; Kushmerick, 2000), in which the goal is to learn rules that extract relevant sub-strings
from a collection of documents. Wrapper induction is a key component of commercial
systems that integrate data from a variety of Web-based information sources.
5.2.1 The Views used by Co-Testing
Consider the illustrative task of extracting phone numbers from documents similar to the
fragment in Figure 3. To find where the phone number begins,3 one can use the rule
R1 = SkipT o( Phone:<i> )
This rule is applied forward, from the beginning of the page, and it ignores everything until
it finds the string Phone:<i>. Note that such forward-going rules do not represent the only
way to detect where the phone number begins: an alternative approach is to use the rule
R2 = BackT o( Cuisine ) BackT o( ( Number ) )
which is applied backward, from the end of the document. R2 ignores everything until it
finds “Cuisine” and then, again, skips to the first number between parentheses.
Forward and backward rules such as R1 and R2 can be learned from user-provided
examples by the state of the art wrapper induction system stalker (Muslea et al., 2001),
which we use as base learner for Co-Testing. Intuitively, stalker creates a forward or a
2. A recent paper (Brefeld & Scheffer, 2004) shows that - for text classification - svm is more appropriate
than Naive Bayes as base learner for Co-EM, though not necessarily for Co-Training. As the MLC ++
library does not provide svm as base learner, we could not compare our results with those by Brefeld
and Scheffer (2004), where Co-EM + svm reaches 99% accuracy based on 12 labeled and 933 unlabeled
examples. However, in all fairness, it is unlikely that Co-Testing could lead to an even faster convergence.
3. As shown by Muslea et al. (2001), the end of the phone number can be found in a similar manner.

222

Active Learning with Multiple Views

backward rule that consumes all the tokens that precede or follow the extraction point,
respectively. It follows that rules such as R1 and R2 represent descriptions of the same
concept (i.e., beginning of phone number) that are learned in two different views: the
sequences of tokens that precede and follow the beginning of the item, respectively. These
views are strong views because each of them is sufficient to accurately extract the items of
interest (Muslea et al., 2001, 2000).
In addition to these two views, which rely mostly on the context of the item to be
extracted (i.e., the text surrounding the item), one can use a third view that describes
the content of the item to be extracted. For example, phone numbers can be described
by the simple grammar: “( Number ) Number - Number”; similarly, most urls start with
“http://www.”, end with “.html”, and contain no html tags.
Such a content-based view is a weak view because it often represents a concept more
general than the target one. For example, the phone number grammar above cannot discriminate between the home, office, cell, and fax numbers that appear within the same Web
page; similarly, the url grammar cannot distinguish between the urls of interest (e.g., a
product’s review) and all the other ones (e.g., advertisements).
In this weak view, we use as base learner a version of DataPro (Lerman, Minton, &
Knoblock, 2003) that is described elsewhere (Muslea et al., 2003). DataPro learns - from
positives examples only - the “prototypes” of the items to be extracted; i.e., it finds statistically significant sequences of tokens that (1) are highly unlikely to have been generated
by chance and (2) describe the content of many of the positive examples. The features used
by this base learner consist of the length range (in tokens) of the seen examples, the token
types that appear in the training set (e.g., Number, AllCaps, etc), and the start- and endpattern (e.g., “http://www.” and “AlphaNum .html”, respectively).
5.2.2 The Algorithms used in the Evaluation
As the extraction rules learned in the two strong views do not provide any estimate of
the confidence of the extractions, the only Co-Testing algorithm that can be implemented
based solely on the forward and backward views is Naive Co-Testing with a winner-takes-all
output hypothesis:
- each query is randomly chosen (Naive Co-Testing) among the contention points, which
are the documents from which the learned rules extract different strings.
- the output hypothesis is the rule learned in the view that makes the fewest mistakes over
the allowed number of queries (i.e., winner-takes-all).
Given the additional, content-based view, we can also implement an aggressive version
of Co-Testing for wrapper induction:
- the contention points are, again, the unlabeled examples on which the rules learned in
the strong views do not extract the same string.
- the aggressive query selection strategy works by selecting the contention point for which
the hypothesis learned in the weak view is maximally confident that both stalker
rules are extracting incorrect strings. More formally, for each contention point, let s 1
and s2 be the strings extracted by the strong views; let us also denote by n1 and n2
223

Muslea, Minton, & Knoblock

the number of constraints learned in the weak views that are violated by s1 and s2 .
Using this notation, the next query is the contention point for which min(n1 , n2 ) has
the largest value.
- the output hypothesis is obtained by the following majority voting scheme: if the strings
extracted by the strong views are identical, then they represent the extracted item;
otherwise the result is the one of these two strings that violates fewer of the constraints
learned in the weak view.
In the empirical evaluation below, we compare these two Co-Testing algorithms with
Random Sampling and Query-by-Bagging. The former is used as strawman, while the
latter is the only general-purpose active learner that can be applied in a straightforward
manner to wrapper induction (for details, see the discussion in section 4.3.1). Finally,
existing multi-view, semi-supervised learners cannot be used for wrapper induction because
the base learners do not provide an estimate in the confidence of each extraction; and even
if such an estimate could be obtained, wrapper induction algorithms are extremely sensitive
to mislabeled examples, which would make the bootstrapping process unacceptably brittle.
In this paper, the implementation of Random Sampling is identical with that of Naive
Co-Testing with winner takes all, except that it randomly queries one of the unlabeled
examples from the working set. For Query-by-Bagging, the committee of hypotheses is
created by repeatedly re-sampling (with substitution) the examples in the original training
set L. We use a relatively small committee (i.e., 10 extraction rules) because when learning
from a handful of examples, re-sampling with replacement leads to just a few distinct
training sets. In order to make a fair comparison with Co-Testing, we run Query-by-Bagging
once in each strong view and report the best of the obtained results.
5.2.3 The Experimental Results
In our empirical comparison, we use the 33 most difficult wrapper induction tasks from the
testbed introduced by Kushmerick (1998, 2000). These tasks, which were previously used in
the literature (Muslea et al., 2003; Muslea, 2002), are briefly described in Table 4. We use
20-fold cross-validation to compare the performance of Naive and Aggressive Co-Testing,
Random Sampling, and Query-by-Bagging on the 33 tasks. Each algorithm starts with two
randomly chosen examples and then makes 18 successive queries.
The results below can be summarized as follows: for 12 tasks, only the two Co-Testing
algorithms learn 100% accurate rules; for another 18 tasks, Co-Testing and at least another
algorithm reach 100% accuracy, but Co-Testing requires the smallest number of queries.
Finally, on the remaining three tasks, no algorithm learns a 100% accurate rule.
Figure 4 shows the aggregate performance of the four algorithms over the 33 tasks. In
each of the six graphs, the X axis shows the number of queries made by the algorithm, while
the Y axis shows the number of tasks for which a 100% accurate rule was learned based on
exactly X queries. As mentioned earlier, all algorithms start with 2 random examples and
make 18 additional queries, for a total of 20 labeled examples. By convention, the rightmost point on the X axis, which is labeled “19 queries”, represents the number of tasks that
require more than the allowed 18 queries to learn a 100% accurate rule. This additional
“19 queries” data-point allows us to summarize the results without dramatically extending
the X axis beyond 18 queries: as for some of the extraction tasks Random Sampling and
224

Active Learning with Multiple Views

Task
ID
S1-0
S1-1
S1-2
S2-0
S2-1
S2-2
S2-3
S3-0
S3-1
S3-3
S3-4
S3-5
S6-1
S9-10
S9-11
S11-1
S11-2

Source
name
Computer
ESP
CNN/Time
AllPolitics

Film.com
Search
PharmaWeb
Internet
Travel Net.
Internet
Address

Item
name
Price
URL
Item
URL
Source
Title
Date
URL
Name
Size
Date
Time
University
Arrival Time
Availability
Email
Update

Task
ID
S11-3
S15-1
S19-1
S19-3
S20-3
S20-5
S24-0
S24-1
S24-3
S25-0
S26-3
S26-4
S26-5
S28-0
S28-1
S30-1

Nmb
exs
404
404
404
501
501
499
492
175
175
175
175
175
27
44
39
91
91

Source
name
Finder
NewJour
Shops.Net
Democratic
Party Online
Foreign
Languages for
Travelers
us Tax Code
CD Club
Web Server
Cyberider
Cycling www
Congress
Quarterly

Item
name
Organization
Name
Score
Item Name
Score
File Type
Language
URL
Translation
URL
Price
Artist
Album
URL
Relevance
Person Name

Nmb
exs
72
355
201
201
91
328
690
424
690
328
377
377
377
751
751
30

Table 4: The 33 wrapper induction tasks used in the empirical evaluation.

Query-by-Bagging need hundreds of queries to learn the correct rules, the histograms would
become difficult to read if the entire X axis were shown.
As shown in Figure 4, the two Co-Testing algorithms clearly outperform their single-view
counterparts, with Aggressive Co-Testing doing significantly better than Naive Co-Testing
(the results are statistically significant with a confidence of at least 99%). Aggressive CoTesting learns 100%-accurate rules on 30 of the 33 tasks; for all these tasks, the extraction
rules are learned from at most seven queries. Naive Co-Testing learns 100% accurate rules
on 28 of the 33 tasks. On 26 of these 28 tasks, the extraction rules are learned based
on at most six queries. In contrast, Random Sampling and Query-by-Bagging learn 100%
accurate rules for only seven and twelve of the tasks, respectively. In other words, both
Co-Testing algorithms learn the correct target concept for more than twice as many tasks
than Query-by-Bagging or Random Sampling.
We must emphasize the power of Aggressive Co-Testing on high-accuracy tasks such as
wrapper induction: for 11 of the 33 tasks, a single, “aggressively-chosen” query is sufficient
to learn the correct extraction rule. In contrast, Naive Co-Testing converges in a single
query on just four of the 33 tasks, while the other two learners never converge in a single
query.
For the three tasks on which Aggressive Co-Testing does not learn 100% accurate rules,
the failure is due to the fact that one of the views is significantly less accurate than the other
one. This leads to a majority of contention points that are mislabeled by the “bad view,”
which - in turn - skews the distribution of the queries towards mistakes of the “bad view.”
Consequently, Co-Testing’s performance suffers because such queries are uninformative for
225

20
15
10
5
0
1

extraction task that converged

extraction task that converged

Aggressive Co-Testing

25

4

7

10
13
queries

16

20
15
10
5
0
1

4

7

10
13
queries

16

20
15
10
5
0
1

Query-by-Bagging (FB)

25

Naive Co-Testing

25

19

extraction task that converged

extraction task that converged

Muslea, Minton, & Knoblock

19

4

7

10
13
queries

16

19

16

19

Random Sampling

25
20
15
10
5
0
1

4

7

10
13
queries

Figure 4: Convergence results for the 33 wrapper induction tasks.

both views: the “good view” makes the correct prediction on them, while the “bad view” is
inadequate to learn the target concept. In order to cope with this problem, we introduced
a view validation algorithm (Muslea et al., 2002b) that predicts whether the views are
appropriate for a particular task.
Finally, let us briefly compare the results above with the ones obtained by wien (Kushmerick, 2000), which is the only wrapper induction system that was evaluated on all the
extraction tasks used here. As the two experimental setups are not identical (i.e., crossvalidation vs. random splits) this is just an informal comparison; however, it puts our
results into perspective by contrasting Co-Testing with another state of the art approach
to wrapper induction.
The results can be summarized as follows: wien fails on 18 of the 33 task; these 18 tasks
include the three for which Aggressive and Naive Co-Testing failed to learn perfect rules.
On the remaining 15 tasks, wien requires between 25 and 90 examples 4 to learn the correct
rule. For the same 15 tasks, both Aggressive and Naive Co-Testing learn 100% accurate
rules based on at most eight examples (two random plus at most six queries).
4. In the wien framework, an example consists of a document in which all items of interest are labeled. For
example, a page that contains a list of 100 names, all labeled, represents a single labeled example. In
contrast, for stalker the same labeled document represents 100 distinct labeled examples. In order to
compare the wien and stalker results, we convert the wien data to stalker-like data by multiplying
the number of labeled wien pages by the average number of item occurrences in each page.

226

Active Learning with Multiple Views

6. Conclusion
In this paper we introduce Co-Testing, which is an active learning technique for multi-view
learning tasks. This novel approach to active learning is based on the idea of learning from
mistakes; i.e., Co-Testing queries unlabeled examples on which the views predict a different
label (such contention points are guaranteed to represent mistakes made in one of the views).
We have analyzed several members of the Co-Testing family (e.g., Naive, Conservative and
Aggressive Co-Testing). We have also introduced and evaluated a Co-Testing algorithm
that simultaneously exploits both strong and weak views.
Our empirical results show that Co-Testing is a powerful approach to active learning. Our experiments use four extremely different base learners (i.e., stalker, ib, Naive
Bayes, and mc4) on four different types of domains: wrapper induction, text classification (courses), ad removal (ad), and discourse tree parsing (tf). In all these scenarios,
Co-Testing clearly outperforms the single-view, state of the art active learning algorithms.
Furthermore, except for Query-by-Bagging, Co-Testing is the only algorithm that can be
applied to all the problems considered in the empirical evaluation. In contrast to Queryby-Bagging, which has a poor performance on courses and wrapper induction, Co-Testing
obtains the highest accuracy among the considered algorithms.
Co-Testing’s success is due to its ability to discover the mistakes made in each view. As
each contention point represents a mistake (i.e., an erroneous prediction) in at least one of
the views, it follows that each query is extremely informative for the view that misclassified
that example; that is, mistakes are more informative than correctly labeled examples. This
is particularly true for base learners such as stalker, which do not improve the current
hypothesis unless they are provided with examples of misclassified instances.
As a limitation, Co-Testing can be applied only to multi-view tasks; that is, unless the
user can provide two views, Co-Testing cannot be used at all. However, researchers have
shown that besides the four problems above, multiple views exist in a variety of real world
problems, such as named entity classification (Collins & Singer, 1999), statistical parsing
(Sarkar, 2001), speech recognition (de Sa & Ballard, 1998), word sense disambiguation
(Yarowsky, 1995), or base noun phrase bracketing (Pierce & Cardie, 2001).
The other concern about Co-Testing is related to the potential violations of the two
multi-view assumptions, which require that the views are both uncorrelated and compatible.
For example, in the case of correlated views, the hypotheses learned in each view may be so
similar that there are no contention points among which to select the next query. In terms
of view incompatibility, remember that, for three of the 33 wrapper induction tasks, one of
the views was so inaccurate that the Co-Testing could not outperform Random Sampling.
In two companion papers (Muslea et al., 2002a, 2002b) we have proposed practical solutions
for both these problems.

Acknowledgments
This research is based upon work supported in part by the National Science Foundation
under Award No. IIS-0324955 and grant number 0090978, in part by the Defense Advanced
Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No. NBCHD030010, and in part by the Air Force
227

Muslea, Minton, & Knoblock

Office of Scientific Research under grant number FA9550-04-1-0105. The U.S.Government is
authorized to reproduce and distribute reports for Governmental purposes notwithstanding
any copy right annotation thereon. The views and conclusions contained herein are those of
the authors and should not be interpreted as necessarily representing the official policies or
endorsements, either expressed or implied, of any of the above organizations or any person
connected with them.

References
Abe, N., & Mamitsuka, H. (1998). Query learning using boosting and bagging. In Proceedings of the 15th International Conference on Machine Learning (ICML-98), pp.
1–10.
Abney, S. (2002). Bootstrapping. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp. 360–367.
Aha, D. (1992). Tolerating noisy, irrelevant and novel attributes in instance-based learning
algorithms. International Journal of Man-Machine Studies, 36 (1), 267–287.
Amoth, T., Cull, P., & Tadepalli, P. (1998). Exact learning of tree patterns from queries
and counterexamples. In Proceedings of the Conference on Computational Learing
Theory, pp. 175–186.
Amoth, T., Cull, P., & Tadepalli, P. (1999). Exact learning of unordered tree patterns
from queries. In Proceedings of the Conference on Computational Learing Theory, pp.
323–332.
Angluin, D. (1982). A note on the number of queries needed to identify regular languages.
Information and Control, 51, 76–87.
Angluin, D. (1988). Queries and concept learning. Machine Learning, 2, 319–342.
Angluin, D. (1994). Exact learning of µ−DNF formulas with malicious membership queries.
Tech. rep. YALEU/DCS/TR-1020, Yale University.
Angluin, D., & Krikis, M. (1994). Malicious membership queries and exceptions. Tech. rep.
YALEU/DCS/TR-1019, Yale University.
Angluin, D., Krikis, M., Sloan, R., & Turan, G. (1997). Malicious omissions and errors in
answers to membership queries. Machine Learning, 28, 211–255.
Angluin, D., & Slonim, D. (1991). Randomly fallible teachers: learning monotone DNF with
an incomplete membership oracle. Machine Learning, 14 (1), 7–26.
Argamon-Engelson, S., & Dagan, I. (1999). Committee-based sample selection for probabilistic classifiers. Journal of Artificial Intelligence Research, 11, 335–360.
Baum, E. (1991). Neural net algorithms that learn in polynomial time from examples and
queries. IEEE Transactions on Neural Networks, 2, 5–19.
Blum, A., Chalasani, P., Goldman, S., & Slonim, D. (1998). Learning with unreliable
boundary queries. Journal of Computer and System Sciences, 56 (2), 209–222.
228

Active Learning with Multiple Views

Blum, A., Furst, M., Jackson, J., Kearns, M., Mansour, Y., & Rudich, S. (1994). Weakly
learning DNF and characterizing statistical query learning using Fourier analysis. In
Proceedings of the 26th ACM Symposium on the Theory of Computing, pp. 253–262.
Blum, A., & Mitchell, T. (1998). Combining labeled and unlabeled data with co-training. In
Proceedings of the 1988 Conference on Computational Learning Theory, pp. 92–100.
Brefeld, U., & Scheffer, T. (2004). Co-EM support vector learning. In Proceedings of the
21st International Conference on Machine Learning (ICML-2004), pp. 121–128.
Breiman, L. (1996). Bagging predictors. Machine Learning, 24(2), 123–140.
Califf, M. E., & Mooney, R. (1999). Relational learning of pattern-match rules for information extraction. In Proceedings of the Sixteenth National Conference on Artificial
Intelligence (AAAI-99), pp. 328–334.
Campbell, C., Cristianini, N., & Smola, A. (2000). Query learning with large margin classifiers. In Proceedings of the 17th International Conference on Machine Learning
(ICML-2000), pp. 111–118.
Cohn, D., Atlas, L., & Ladner, R. (1994). Improving generalization with active learning.
Machine Learning, 15, 201–221.
Cohn, D., Ghahramani, Z., & Jordan, M. (1996). Active learning with statistical models.
In Advances in Neural Information Processing Systems, Vol. 9, pp. 705–712.
Collins, M., & Singer, Y. (1999). Unsupervised models for named entity classification. In
Proceedings of the Empirical NLP and Very Large Corpora Conference, pp. 100–110.
Dagan, I., & Engelson, S. (1995). Committee-based sampling for training probabilistic
classifiers. In Proceedings of the 12th International Conference on Machine Learning,
pp. 150–157.
Dasgupta, S., Littman, M., & McAllester, D. (2001). PAC generalization bounds for cotraining. In Neural Information Processing Systems, pp. 375–382.
de Sa, V., & Ballard, D. (1998). Category learning from multi-modality. Neural Computation, 10 (5), 1097–1117.
Dempster, A., Laird, N., & Rubin, D. (1977). Maximum likelihood from incomplete data
vie the em algorithm. Journal of Royal Statistical Society, 39, 1–38.
Fedorov, V. V. (1972). Theory of optimal experiment. Academic Press.
Finn, A., & Kushmerick, N. (2003). Active learning selection strategies for information
extraction. In Proceedings of the ECML-2004 Workshop on Adaptive Text Extraction
and Mining (ATEM-2003).
Freund, Y., Seung, H. S., Shamir, E., & Tishby, N. (1997). Selective sampling using the
query by committee algorithm. Machine Learning, 28, 133–168.
Ghani, R. (2002). Combining labeled and unlabeled data for multiclass text classification. In
Proceedings of the 19th International Conference on Machine Learning (ICML-2002),
pp. 187–194.
229

Muslea, Minton, & Knoblock

Goldberg, P., Goldman, S., & Mathias, D. (1994). Learning unions of boxes with membership and equivalence queries. In Proceedings of the Conference on Computational
Learing Theory, pp. 198–207.
Goldman, S., & Mathias, D. (1992). Learning k -term DNF formulas with an incomplete
membership oracle. In Proceedings of the Conference on Computational Learing Theory, pp. 77–84.
Goldman, S., & Zhou, Y. (2000). Enhancing supervised learning with unlabeled data. In
Proceedings of the 17th International Conference on Machine Learning (ICML-2000),
pp. 327–334.
Gross, K. (1991). Concept acquisition through attribute evolution and experiment selection.
Ph.D. thesis, School of Computer Science, Carnegie Mellon University.
Hasenjager, M., & Ritter, H. (1998). Active learning with local models. Neural Processing
Letters, 7, 107–117.
Hsu, C.-N., & Dung, M.-T. (1998). Generating finite-state transducers for semi-structured
data extraction from the web. Journal of Information Systems, 23(8), 521–538.
Hwang, J.-N., Choi, J., Oh, S., & Marks, R. (1991). Query-based learning applied to
partially trained multilayer perceptrons. IEEE Transactions on Neural Networks, 2,
131 –136.
Jackson, J. (1994). An efficient membership-query algorithm for learning DNF with respect
to the uniform distribution. In Proceedings of the IEEE Symposium on Foundations
of Computer Science, pp. 42–53.
Jones, R., Ghani, R., Mitchell, T., & Riloff, E. (2003). Active learning for information extraction with multiple view feature sets. In Proceedings of the ECML-2004 Workshop
on Adaptive Text Extraction and Mining (ATEM-2003).
Kohavi, R., Sommerfield, D., & Dougherty, J. (1997). Data mining using MLC++, a machine learning library in C++. International Journal of AI Tools, 6(4), 537–566.
Kushmerick, N. (1999). Learning to remove internet advertisements. In Proceedings of the
Third International Conference on Autonomous Agents (Agents-99), pp. 175–181.
Kushmerick, N. (2000). Wrapper induction: efficiency and expressiveness. Artificial Intelligence Journal, 118 (1-2), 15–68.
Kushmerick, N., Johnston, E., & McGuinness, S. (2001). Information extraction by text
classification. In The IJCAI-2001 Workshop on Adaptive Text Extraction and Mining.
Lang, K., & Baum, E. (1992). Query learning can work poorly when a human oracle is
used. In Proceedings of the IEEE International Joint Conference on Neural Networks.
Lerman, K., Minton, S., & Knoblock, C. (2003). Wrapper maintenance: A machine learning
approach. Journal of Artificial Intelligence Research, 18, 149–181.
Lewis, D., & Catlett, J. (1994). Heterogeneous uncertainty sampling for supervised learning.
In Proceedings of the 11th International Conference on Machine Learning (ICML-94),
pp. 148–156.
230

Active Learning with Multiple Views

Lewis, D., & Gale, W. (1994). A sequential algorithm for training text classifiers. In
Proceedings of Research and Development in Information Retrieval, pp. 3–12.
Liere, R., & Tadepalli, P. (1997). Active learning with committees for text categorization.
In The 14th National Conference on Artificial Intelligence (AAAI-97), pp. 591–596.
Lindenbaum, M., Markovitch, S., & Rusakov, D. (2004). Selective sampling for nearest
neighbor classifiers. Machine Learning, 54 (2), 125–152.
Marcu, D., Carlson, L., & Watanabe, M. (2000). The automatic translation of discourse
structures. In Proceedings of the 1st Annual Meeting of the North American Chapter
of the Association for Computational Linguistics (NAACL-2000), pp. 9–17.
McCallum, A., & Nigam, K. (1998). Employing EM in pool-based active learning for
text classification. In Proceedings of the 15th International Conference on Machine
Learning, pp. 359–367.
Melville, P., & Mooney, R. J. (2004). Diverse ensembles for active learning. In Proceedings
of the International Conference on Machine Learning, pp. 584–591.
Muslea, I. (2002). Active Learning with Multiple Views. Ph.D. thesis, Department of Computer Science, University of Southern California.
Muslea, I., Minton, S., & Knoblock, C. (2000). Selective sampling with redundant views.
In Proceedings of National Conference on Artificial Intelligence (AAAI-2000), pp.
621–626.
Muslea, I., Minton, S., & Knoblock, C. (2001). Hierarchical wrapper induction for semistructured sources. Journal of Autonomous Agents and Multi-Agent Systems, 4, 93–114.
Muslea, I., Minton, S., & Knoblock, C. (2002a). Active + Semi-supervised Learning = Robust Multi-view Learning. In The 19th International Conference on Machine Learning
(ICML-2002), pp. 435–442.
Muslea, I., Minton, S., & Knoblock, C. (2002b). Adaptive view validation: A first step
towards automatic view detection. In The 19th International Conference on Machine
Learning (ICML-2002), pp. 443–450.
Muslea, I., Minton, S., & Knoblock, C. (2003). Active learning with strong and weak views:
a case study on wrapper induction. In Proceedings of International Joint Conference
on Atificial Intelligence (IJCAI-2003), pp. 415–420.
Nahm, U.-Y., & Mooney, R. (2000). A mutually beneficial integration of data mining and
information extraction. In The 17th National Conference on Artificial Intelligence
(AAAI-2000), pp. 627–632.
Nigam, K., & Ghani, R. (2000). Analyzing the effectiveness and applicability of co-training.
In Proceedings of Information and Knowledge Management, pp. 86–93.
Pierce, D., & Cardie, C. (2001). Limitations of co-training for natural language learning from
large datasets. In Proceedings of Empirical Methods in Natural Language Processing
(EMNLP-2001), pp. 1–10.
Raskutti, B., Ferra, H., & Kowalczyk, A. (2002). Combining clustering and co-training
to enhance text classification using unlabeled data. In Proceedings of the SIGKDD
International Conference on Knowledge Discovery and Data Mining, pp. 620–625.
231

Muslea, Minton, & Knoblock

Reddy, C., & Tadepalli, P. (1997). Learning horn definitions with equivalence and membership queries. In Proceedings of the 7th International Workshop on Inductive Logic
Programming, pp. 243–255.
Roy, N., & McCallum, A. (2001). Toward optimal active learning through sampling estimation of error reduction. In Proceedings of the 18th International Conference on
Machine Learning (ICML-2001), pp. 441–448.
Sammut, C., & Banerji, R. B. (1986). Learning concepts by asking questions. In Carbonell,
R. S. M., Carbonell, J., & 2), T. M. M. V. (Eds.), Machine Learning: An Artificial
Intelligence Approach, pp. 167–192. Morgan Kaufmann.
Sarkar, A. (2001). Applying co-training methods to statistical parsing. In Proceedings
of the 2nd Annual Meeting of the North American Chapter of the Association for
Computational Linguistics (NAACL-2001), pp. 175–182.
Schapire, R. (1990). The strength of weak learnability. Machine Learning, 5(2), 197–227.
Scheffer, T., & Wrobel, S. (2001). Active learning of partially hidden Markov models.
In Proceedings of the ECML/PKDD-2001 Workshop on Active Learning, Database
Sampling, Experimental Design: Views on Instance Selection.
Schohn, G., & Cohn, D. (2000). Less is more: Active learning with support vector machines.
In Proceedings of the 17th International Conference on Machine Learning (ICML2000), pp. 839–846.
Seung, H. S., Opper, M., & Sompolinski, H. (1992). Query by committee. In Proceedings
of the 1988 Conference on Computational Learning Theory (COLT-72), pp. 287–294.
Shapiro, E. (1981). A general incremental algorithm that infers theories from facts. In
Proceedings of the 7th International Joint Conference on Artificial Intelligence, pp.
446–451.
Shapiro, E. (1982). Algorithmic program diagnosis. In Proceedings of the 9th ACM Symposium on Principles of Programming Languages, pp. 299–308.
Sloan, R., & Turan, G. (1994). Learning with queries but incomplete information (extended
abstract). In Proceedings of the Conference on Computational Learing Theory, pp.
237–245.
Soderland, S. (1999). Learning extraction rules for semi-structured and free text. Machine
Learning, 34, 233–272.
Tadepalli, P. (1993). Learning from queries and examples with tree-structured bias. In
Proceedings of the 10th International Conference on Machine Learning (ICML-93),
pp. 322–329.
Tadepalli, P., & Russell, S. (1998). Learning from queries and examples with structured
determinations. Machine Learning, 245–295.
Thompson, C., Califf, M. E., & Mooney, R. (1999). Active learning for natural language
parsing and information extraction. In Proceedings of the 16th International Conference on Machine Learning (ICML-99), pp. 406–414.
Tong, S., & Koller, D. (2000). Active learning for parameter estimation in Bayesian networks. In Advances in Neural Information Processing Systems, Vol. 13, pp. 647–653.
232

Active Learning with Multiple Views

Tong, S., & Koller, D. (2001). Support vector machine active learning with applications to
text classification. Journal of Machine Learning Research, 2, 45–66.
Valiant, L. (1984). A theory of the learnable. Communications of the ACM, 27 (11), 1134–
1142.
Watkin, T., & Rau, A. (1992). Selecting examples for perceptrons. Journal of Physics A:
Mathematical and General, 25 (1), 113–121.
Yarowsky, D. (1995). Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd annual meeting of the Association of Computational
Linguistics, pp. 189–196.
Zhou, Y., & Goldman, S. (2004). Democratic co-learning. In Proceedings of the International
Conference on Tools with Artificial Intelligence, pp. 594–602.

233

Journal of Artificial Intelligence Research 27 (2006) 441-464

Submitted 03/06; published 12/06

Set Intersection and Consistency in Constraint Networks
Yuanlin Zhang

yzhang@cs.ttu.edu

Department of Computer Science, Texas Tech University
Lubbock, TX 79414 USA

Roland H. C. Yap

ryap@comp.nus.edu.sg

Department of Computer Science, National University of Singapore
3 Science Drive 2, Singapore 117543

Abstract
In this paper, we show that there is a close relation between consistency in a constraint
network and set intersection. A proof schema is provided as a generic way to obtain consistency properties from properties on set intersection. This approach not only simplifies the
understanding of and unifies many existing consistency results, but also directs the study
of consistency to that of set intersection properties in many situations, as demonstrated
by the results on the convexity and tightness of constraints in this paper. Specifically,
we identify a new class of tree convex constraints where local consistency ensures global
consistency. This generalizes row convex constraints. Various consistency results are also
obtained on constraint networks where only some, in contrast to all in the existing work,
constraints are tight.

1. Introduction
A constraint network consists of a set of variables over finite domains and a system of
constraints over those variables. An important task is to find an assignment for all the
variables such that all the constraints in the network are satisfied. If such an assignment
exists, the network is satisfiable or globally consistent, and the assignment is called a solution.
The problem of determining the global consistency of a general constraint network is NPcomplete. Usually a search procedure is employed to find a solution. In practice, due
to efficiency considerations, the search is usually equipped with a filtering algorithm that
prunes values of a variable or the combinations of values of a certain number of variables
that cannot be part of any solution. The filtering algorithm can make a constraint network
locally consistent in the sense that a consistent assignment of some variables can always be
extensible to a new variable. An important and interesting question on local consistency is:
Is the local consistency obtained sufficient to determine the global consistency of
the network without further search? As the filtering algorithm is of polynomial
complexity, a positive answer would mean that the network can be solved in
polynomial time.
Much work has been done to explore the relationship between local and global consistency in particular and the properties of local consistency in general. One direction is to
make use of the topological structure of a constraint network. A classical result is that
when the graph of a constraint network is a tree, arc consistency of the network is sufficient
to ensure its global consistency (Freuder, 1982).
c
2006
AI Access Foundation. All rights reserved.

Zhang & Yap

The second direction1 makes use of semantic properties of the constraints. For monotone
constraints, path consistency implies global consistency (Montanari, 1974). Van Beek and
Dechter (1995) generalize monotone constraints to a larger class of row convex constraints.
Dechter (1992) shows that a certain level of consistency in a constraint network whose
domains are of limited size ensures global consistency. Later, Van Beek and Dechter (1997)
study the consistency of constraint networks with tight and loose constraints.
The existing work along the two approaches has used specific and different techniques to
study local and global consistency. In particular, there is little commonality in the details
of the existing work. In much of the existing work, the techniques and consequently the
proofs given are developed specifically for the results concerned.
In this paper, we show how much of this work can be connected together through a
new approach to studying consistency in a constraint network. We unite two seemingly
disparate areas: the study of set intersection on special sets and the study of k-consistency
in constraint networks. In fact, k-consistency can be expressed in terms of set intersection,
which allows one to obtain relationships between local and global consistency in a constraint
network through the properties of set intersection on special sets. The main result of this
approach is a proof schema that can be used to lift results from set intersection, which are
rather general, to particular consistency results on constraint networks. One benefit of the
proof schema lies in that it provides a modular way to greatly simplify the understanding
and proofs of consistency results. This benefit is considerable as often the proofs of many
existing results are complex and “hard-wired”. Using this new approach, we show that
it is precisely the various properties of set intersection that are the key to those results.
Furthermore, the proofs become mechanical.
The following sketch illustrates briefly the use of our approach. One property of set
intersection is that if the intersection of every pair (2) of tree convex sets (see Section 3) is
not empty, the intersection of the whole collection of these sets is not empty too. From this
property, we can see that the local information on the intersection of every pair of sets gives
rise to the global information on the intersection of all sets. Intuitively, this relationship
between the local and global information corresponds to obtaining global consistency from
local consistency. The proof schema is used to lift the result on tree convex sets to the
following consistency result. For a binary constraint network of tree convex constraints,
(2+1)-consistency (path consistency) implies global consistency of the network.
The usefulness of our new set-based approach is twofold. Firstly, it gives a clear picture
of many of the existing results. For example, many well known results in the second direction
based on semantic properties of the constraints (including van Beek & Dechter, 1995, 1997),
as well as results from the first direction, can be shown with easy proofs that make use of
set intersection properties. Secondly, by directing the study of consistency to that of set
intersection properties, it helps improve some of the existing results and derive new results
as demonstrated in sections 5–7.
This paper is organized as follows. In Section 2, we present necessary notations and
concepts. In Section 3, we focus on properties of the intersection of tree convex sets and sets
1. There is a difference between the work concerned here and that studying the tractability of constraint
languages (e.g., Schaefer, 1978; Jeavons, Cohen, & Gyssens, 1997). The latter considers the problems
whose constraints are from a fixed set of relations while the former studies constraint networks with
special properties.

442

Set Intersection and Consistency in Constraint Networks

with cardinality restrictions. In Section 4, we develop a characterization of k-consistency
utilizing set intersection and the proof schema that offers a generic way to obtain consistency
results from set intersection properties. The power of the new approach is demonstrated
by new consistency results on the convexity and tightness of constraints. Tree convex
constraints are studied in Section 5. On a constraint network of tree convex constraints,
local consistency ensures global consistency, as a result of the intersection property of tree
convex sets. The tightness of constraints is studied in Section 6. Thanks to the intersection
properties of sets with cardinality restriction, a relation between local and global consistency
is identified on weakly tight constraint networks in Section 6.1. These networks require only
some, rather than all, constraints to be m-tight, improving the tightness result by van Beek
and Dechter (1997). With the help of relational consistency, we show that global consistency
can be achieved through local consistency on weakly tight constraint networks in Section 6.2.
This type of result on tightness was not known before. In Section 6.3, we explore when
a constraint network is weakly m-tight and present several results about the number of
tight constraints sufficient or necessary for a network to be weakly tight. To make full use
of the tightness of the constraints in a network, we propose dually adaptive consistency
in Section 6.4. Dually adaptive consistency of a constraint network is determined by its
topology and the tightest relevant constraint on each variable. For completeness, we include
in Section 7 results on tightness and tree convexity that are based on relational consistency.
We conclude in Section 8.

2. Preliminaries
A constraint network R is defined as a set of variables N = {x1 , x2 , . . . , xn }; a set of finite
domains D = {D1 , D2 , . . . , Dn } where domain Di , for all i ∈ 1..n, is a set of values that
variable xi can take; and a set of constraints C = {cS1 , cS2 , . . . , cSe } where Si , for all i ∈ 1..e,
is a subset of {x1 , x2 , . . . , xn } and each constraint cSi is a relation defined on the domains
of all variables in Si . Without loss of generality, we assume that, for any two constraints
cSi , cSj ∈ C (i 6= j), Si 6= Sj . The arity of constraint cSi is the number of variables in Si .
For a variable x, Dx denotes its domain. In the rest of this paper, we will often use network
to mean constraint network.
An instantiation of variables Y = {x1 , . . . , xj } is denoted by ā = (a1 , . . . , aj ) where
ai ∈ Di for i ∈ 1..j. An extension of ā to a variable x(∈
/ Y ) is denoted by (ā, u) where
u ∈ Dx . An instantiation of a set of variables Y is consistent if it satisfies all constraints in
R that do not involve any variables outside Y .
A constraint network R is k-consistent if and only if for any consistent instantiation ā
of any distinct k − 1 variables, and for any new variable x, there exists u ∈ Dx such that
(ā, u) is a consistent instantiation of the k variables. R is strongly k-consistent if and only if
it is j-consistent for all j ≤ k. A strongly n-consistent network is called globally consistent.
For more information on constraint networks and consistency, the reader is referred to
the work by Mackworth (1977), Freuder (1978) and Dechter (2003).

443

Zhang & Yap

3. Properties on Set Intersection
In this section, we develop a number of set intersection results that will be used later to
derive results on consistency. The set intersection property that we are concerned with is:
Given a collection of l finite sets, under what conditions is the intersection of
all l sets not empty?
Here, we are particularly interested in the intersection property on sets with two interesting and useful restrictions: convexity and cardinality.
3.1 Tree Convex Sets
Given a collection of sets, some structures can be associated with the elements of the sets
such that we can obtain interesting and useful set intersection results. Here we study the
sets whose elements form a tree. We first introduce the concept of a tree convex set.
Definition 1 Given a discrete set U and a tree T with vertices U , a set A ⊆ U is tree
convex under T if there exists a subtree of T whose vertices are A.
A subtree of a tree T is a subgraph of T that is a tree. Next we define when we can say
a collection of sets are tree convex.
Definition 2 Given a collection of discrete sets S, let the union of the sets of S be U . The
sets of S are tree convex under a tree T on U if every set of S is tree convex under T .
A collection of sets are said tree convex if there exists a tree such that the sets in the
collection are tree convex under the tree.
b

a

........ ......
....... ... ............
.....
......
......
......
.
.
.
.
.
......
......
......
.....
......
.
.
.
.
.
......
...
.
.
.
.
..
.
..

..
...... ...........
......
......
......
......
......
......
......
......
.
.
.
.
.
......
...
.
.
.
.
......
.
...
.....
......

b

c

....
.....
....
....
.
.
.
.
.
.....
....
....

a

c........

d

d

.
..................
........
....
..
... .....
...
..
...
...
...
.
.
...
...
...
..
.
.
.
.
.
...
...
.
... ....
... .....
... ...
... ...
......
......

e

e

(a)

f

(b)

Figure 1: (a) A tree with nodes {a, b, c, d, e} (b) A partial order with nodes {a, b, c, d, e, f }

Example 1 Consider a set U = {a, b, c, d, e} and a tree given in Figure 1. The subset
{a, b, c, d} is tree convex under the given tree. So is the set {b, a, c, e} since the elements of
the set are a subtree. However, {b, c, e} is not tree convex as its elements do not form a
subtree of the given tree.
Example 2 Consider S = {{1, 9}, {3, 9}, {5, 9}}. If we construct a tree on {1, 3, 5, 9} with
9 being the root and 1, 3, 5 being its children, each set of S covers the nodes of exactly one
branch of the tree. Hence, the sets of S are tree convex.
444

Set Intersection and Consistency in Constraint Networks

Tree convex sets have the following intersection property.
Lemma 1 (Tree Convex Sets Intersection)
Given a finite collection T
of finite sets S,
T
assume the sets of S are tree convex.
E 6= ∅ iff for all E1 , E2 ∈ S, E1 E2 6= ∅.
E∈S
Proof. Let l be the number of sets in S, and T a tree such that, for each Ei ∈ S, Ei is
the vertices of a subtree Ti of T . Assuming T is a rooted tree, every Ti (i ∈ 1..l) is a rooted
tree whose root is exactly the node nearest to the root of T . Let ri denote the root of Ti
for i ∈ 1..l. T
To prove
Ei 6= ∅, we want to show the intersection of the trees {Ti | i ∈ 1..l} is not
i∈1..l

empty. The following propositions on subtrees are necessary in our main proof.
Proposition 1 Let T1 , T2 be two subtrees of a tree T , and T = T1 ∩ T2 . T is a tree.
If T = ∅, it is a trivial tree. Now let T 6= ∅. Since T is a portion of T1 , there is no circuit in
it. It is only necessary to prove T is connected. That is to show, for any two nodes u, v ∈ T ,
there is a path between them. u, v ∈ T1 and u, v ∈ T2 respectively imply that there exist
paths P1 : u, . . . , v in T1 and P2 : u, . . . , v in T2 respectively. Recall that there is a unique
path from u to v in T and that T1 and T2 are subtrees of T . Therefore, P1 and P2 cover
the same nodes and edges, and thus they are in T , the intersection of T1 and T2 . P1 is the
path we want.
Proposition 2 Let T1 , T2 be two subtrees of a tree T , and T = T1 ∩ T2 . T is not empty if
and only if at least one of the roots of T1 and T2 is in T .
Let r1 and r2 be the roots of T1 and T2 respectively. If r1 ∈ T , the proposition is correct.
Otherwise, we show r2 ∈ T . Assume the contrary r2 ∈
/ T . Clearly, r1 6= r2 . Let r be the
first common ancestor of r1 and r2 and v the root of T (T is a tree by Proposition 1). We
have paths P1 : r1 , . . . , v in T1 ; P2 : r2 , . . . , v in T2 ; and P3 : r, . . . , r1 , and P4 : r, . . . , r2 in
T . Since v is a descendant of both r1 and r2 , P1 and P2 share only the vertex v. Since r is
the first common ancestor of r1 and r2 , P3 and P4 share only the vertex r. It can also be
verified that P3 and P1 share only r1 , P2 and P4 share only r2 , and no vertex is shared by
either P1 and P4 or P2 and P3 . Hence, the closed walk P3 P1 P20 P40 , where P20 and P40 are the
reverse of P2 and P4 respectively, is a simple circuit. It contradicts that there is no circuit
in T .
Further, we have the following observation.
Proposition 3 Let T be a tree with root r, and T1 and T2 two subtrees of T with roots r1
and r2 respectively. Let r1 be not closer to r than r2 , and T the intersection of T1 and T2 .
r1 is the root of T if T is not empty.
The proposition is true if r1 = r2 . Now let r1 be farther to r than r2 . Clearly r2 ∈
/ T1 and
thus r2 ∈
/T
.
By
Proposition
2,
r
is
the
root
of
T
.
1
T
Let T =
Ti . We are ready now to prove our main result T 6= ∅. Select a tree Tmax from
i∈1..l

T1 , T2 , . . . , Tl such that its root rmax is the farthest away from r of T among the roots of
445

Zhang & Yap

the concerned trees. In accordance with Proposition 3, that Tmax intersect with every other
tree implies that rmax is a node of every Ti (i ∈ 1..l). Therefore, rmax ∈ T . 2
Remark. A partial order can be represented by an acyclic directed graph. It is tempting to further generalize tree convexity to partial convexity in the following way.
Given a set U and a partial order on it, a set A ⊂ U is partially convex if and only if A
is the set of nodes of a connected subgraph of the partial order. Given a collection of sets S,
let the union of the sets of S be U . The sets of S are partially convex if there is a partial
order on U such that every set of S is partially convex under the partial order.
However, with this generalization, we can not get a result similar to Lemma 1, which
is illustrated by the following example. Consider three sets {c, b, d}, {d, f, a} and {a, e, c}
that are the nodes of the diagram given in Figure 1(b). These sets are partially convex and
intersect pairwise. However, the intersection of all three sets is empty.
3.2 Sets with Cardinality Restrictions
Another useful restriction that we will place on sets is to restrict their cardinalities. As a
special case, consider a set with only one element a. If its intersection with every other set
is not empty, we are able to conclude that every set contains a, and thus the intersection of
all the sets is not empty. Generally, if a set has at most m elements, we have the following
result.
Lemma 2 Consider a finite collection of l sets S={E1 , E2 , . . . , El } and a number m < l.
Assume one set E1 ∈ S has at most m elements.
\
E 6= ∅
E∈S
iff the intersection of E1 and any other m sets of S is not empty.
Proof. The necessary condition is immediate.
To prove the sufficient condition, we show that the intersection of E1 and any other
k (m ≤ k ≤ l − 1) sets of S is not empty by induction on k. When k = m, the lemma
is true according to its assumption. Assuming that the intersection of E1 and any other
k − 1 (≥ m) sets of S is not empty, we show that the intersection of E1 and any other k
sets of S is not empty. Without loss of generality, the subscripts of the k sets are numbered
from 2 to k + 1. For 2 ≤ i ≤ k + 1, let Ai be the intersection of E1 and the k sets except Ei :
Ai = E1 ∩ . . . ∩ Ei−1 ∩ Ei+1 ∩ . . . ∩ Ek+1 .
First, we show by contradiction that there exist some i, j ∈ 2..k + 1, i 6= j such that
Ai ∩ Aj 6= ∅. Assume Ai ∩ Aj = ∅ for all distinct i and j. According to the construction of
Ai ’s,
[
E1 ⊇
Ai ,
i∈2..k+1

446

Set Intersection and Consistency in Constraint Networks

and |Ai | ≥ 1 by the induction assumption. Hence,
X
|E1 | ≥
|Ai | ≥ k > m,
i∈2..k+1

which contradicts |E1 | ≤ m.
Since Ai ∩ Aj 6= ∅ for some i, j ∈ 2..k + 1, i 6= j,
\
Ei 6= ∅.
Ai ∩ Aj =
i∈1..k+1

2
This lemma leads to the following corollary where the intersection of every m + 1 sets
is not empty.
Corollary 1 (Small Set Intersection) Consider a finite collection of l sets S and a
number m < l. Assume one set of S has at most m elements.
\
E 6= ∅
E∈S
iff the intersection of any m + 1 sets of S is not empty.
There are other specialized versions (Zhang & Yap, 2003) of Lemma 2 on which some
existing works by van Beek and Dechter (1997) and David (1993) are based.
When the sets of concern have a cardinality larger than a certain number, the intersection
of these sets is not empty under some conditions. The reader may refer to the Large Sets
Intersection lemma (Zhang & Yap, 2003) for more details.

4. Set Intersection and Consistency
In this section, we first relate consistency in constraint networks to set intersection. Using
this result, we present a proof schema that allows us to study the relationship between local
and global consistency from the properties of set intersection.
Underlying the concept of k-consistency is whether an instantiation of some variables
can be extended to a new variable such that all relevant constraints on the new variable are
satisfied. A relevant constraint on a variable xi with respect to Y is a constraint that contains
only xi and some variables of Y . Given an instantiation of Y , each relevant constraint allows
a set (possibly empty) of values for the new variable. We call this set an extension set. The
satisfiability of all relevant constraints depends on whether the intersection of their extension
sets is non-empty (see Lemma 3).
Definition 3 Given a constraint cSi , a variable x ∈ Si , and any instantiation ā of Si −{x},
the extension set of ā to x with respect to cSi is defined as
Ei,x (ā) = {b ∈ Dx | (ā, b) satisf ies cSi }.
An extension set is trivial if it is empty; otherwise it is non-trivial.
447

Zhang & Yap

Recall that Dx refers to the domain of variable x. Throughout the paper, it is often
the case that an instantiation ā of S − {x} is already given, where S − {x} is a superset
of Si − {x}. Let b̄ be the instantiation obtained by restricting ā to the variables only in
Si − {x}. For ease of presentation, we continue to use Ei,x (ā), rather than Ei,x (b̄), to denote
the extension of b̄ to x under constraint cSi . To make the presentation easy to follow, some
of the three parameters i, ā, and x may be omitted from an expression hereafter whenever
they are clear from the context. For example, given an instantiation ā and a new variable
x, to emphasize different extension sets with respect to different constraints cSi , we write
Ei instead of Ei,x (ā) to simply denote an extension set.
Example 3 Consider a network with variables {x1 , x2 , x3 , x4 , x5 }:
cS1 =
cS2 =
cS3 =
cS4 =
D1 = D4

{(a, b, d), (a, b, a)}, S1 = {x1 , x2 , x3 };
{(b, a, d), (b, a, b)}, S2 = {x2 , x4 , x3 };
{(b, d), (b, c)},
S3 = {x2 , x3 };
{(b, a, d), (b, a, a)}, S4 = {x2 , x5 , x3 };
= D5 = {a}, D2 = {b}, D3 = {a, b, c, d}.

Let ā = (a, b, a) be an instantiation of variables Y = {x1 , x2 , x4 }. The relevant constraints
to x3 are cS1 , cS2 , and cS3 . cS4 is not relevant since it contains x5 outside Y . The extension
sets of ā to x3 with respect to the relevant constraints are:
E1 (ā) = {d, a}, E2 (ā) = {d, b}, E3 (ā) = {d, c}.
The intersection of the extension sets above is not empty, implying that ā can be extended
to satisfy all relevant constraints cS1 , cS2 and cS3 .
Let ā = (b, c) be an instantiation of {x2 , x3 }. E1,x1 (ā) = ∅ and thus it is trivial. In
other words, with a trivial extension set, an instantiation can not be extended to satisfy the
constraint of concern.
The relationship between k-consistency and set intersection is characterized by the following lemma.
Lemma 3 (Set Intersection and Consistency; Lifting) A constraint network R is kconsistent if and only if for any consistent instantiation ā of any (k − 1) distinct variables
Y = {x1 , x2 , . . . , xk−1 }, and any new variable xk ,
\
Eij 6= ∅
j∈1..l

where Eij is the extension set of ā to xk with respect to cSij , and cSi1 , . . . , cSil are all relevant
constraints.
Proof. It follows directly from the definition of k-consistency in Section 2 and the
definition of extension set. 2
The insight behind this lemma is to examine consistency from the perspective of set
intersection.
448

Set Intersection and Consistency in Constraint Networks

Example 4 Consider again Example 3. We would like to check whether the network is 4consistent. Consider the instantiation ā of Y again. This is a trivial consistent instantiation
since the network doesn’t have a constraint among the variables in Y . To extend it to x,
we need to check the first three constraints cS1 to cS3 . The extension is feasible because
the intersection of E1 , E2 , and E3 is not empty. We show the network is 4-consistent, by
exhausting all consistent instantiations of any three variables. Conversely, if we know the
network is 4-consistent, we can immediately say that the intersection of the three extension
sets of ā to x is not empty.
The usefulness of this lemma is that it allows consistency information to be obtained from
the intersection of extension sets, and vice versa. With this point of view of consistency as
set intersection, some results on set intersection properties, including all those in Section 3,
can be lifted to get various consistency results for a constraint network through the following
proof schema.
Proof Schema
1. (Consistency to Set) From a certain level of consistency in the constraint network,
we derive information on the intersection of the extension sets by Lemma 3.
2. (Set to Set) From the local intersection information of sets, information may be
obtained on intersection of more sets.
3. (Set to Consistency) From the new information on intersection of extension sets,
higher level of consistency is obtained by Lemma 3.
4. (Formulate conclusion on the consistency of the constraint network). 2
In the proof schema, step 1 (consistency to set), step 3 (set to consistency), and step 4 are
straightforward in many cases. So, Lemma 3 is also called the lifting lemma because once we
have a set intersection result (step 3), we can easily have consistency results on a network
(step 4). The proof schema establishes a direct relationship between set intersection and
consistency properties in a constraint network.
In the following sections, we demonstrate how the set intersection properties and the
proof schema are used to obtain new results on the consistency of a constraint network.

5. Global Consistency of Tree Convex Constraints
The notion of extension set plays the role of a bridge between the restrictions to set(s) and
properties of special constraints. In this section, we consider the constraints arising from
tree convex sets (Lemma 1). A constraint is tree convex if all the extension sets with respect
to the constraint are tree convex.
Definition 4 A constraint cS is tree convex with respect to xi and a tree Ti on Di if and
only if the sets in
A = {ES,xi | ES,xi is a non-trivial extension of some instantiation of S − {xi }}
are tree convex under Ti . A constraint cS is tree convex under a tree T on the union of the
domains of the variables in S, if it is tree convex with respect to every x ∈ S under T .
Example 5 Tree convex constraints can occur where there is a relationship among the
values of a variable. Consider the constraint on the accessibility of a set of facilities by a
449

Zhang & Yap

set of persons. The personnel include a network engineer, web server engineer, application
engineer, and a team leader. The relationship among the staff is that the team leader
manages the rest, which forms a tree structure shown in Figure 2(b). There are different
accessibilities to a system which includes basic access, access to the network routers, access
to the web server, and access to the file server. In order to access the routers and servers,
one has to have the basic access right, implying a tree structure (Figure 2(c)) on the access
rights. The constraint is that the team leader is able to access all the facilities while each
engineer can access only the corresponding facility (e.g., the web server engineer can access
the web server). This tree convex constraint is shown in Figure 2(a) where the rows are
named by the initials of the engineers and the columns by the initials of the access rights. The
tree on the union of personnel and the accessibilities can be obtained from their respective
trees (in Figure 2(b) and (c)) by adding an edge, say between web server and leader. Note
that the constraint in Figure 2(a) is not row convex.

n
w
a
l

r
*

w

f

*
*

*

(a)

*
*

b
*
*
*
*

leader

network
engineer

web application
engineer engineer routers

(b)

basic access

web
server

file
server

(c)

Figure 2: A tree convex constraint between accessibilities and staffs
Example 6 Tree convex constraints can also be used to model some scene labeling problems
naturally as shown by Zhang and Freuder (2004).
Definition 5 A constraint network is tree convex if there exists a tree T on the union of
all its variable domains such that all constraints are tree convex under T .
Tree convex constraints generalize row convex constraints introduced by van Beek and
Dechter (1995).
Definition 6 A constraint cS is row convex with respect to x if and only if the sets in
A = {ES,x | ES,x is a non-trivial extension of some instantiation of S − {x}}
are tree convex under a tree where any node has at most one child. Such a tree is called a
total ordering. A constraint cS is row convex if, under a total ordering on the union of the
involved domains, it is row convex with respect to every x ∈ S.
Example 7 For the constraint c in Example 5 to be row convex, b (basic access) has to be
the neighbor of r (routers), w (web server), and f (file server). However, in a total ordering,
a value can be the neighbor of at most two other values. Hence, c is not row convex but is
tree convex.
By the property of set intersection on tree convex sets and the proof schema, we have
the following consistency results on tree convex constraints.
450

Set Intersection and Consistency in Constraint Networks

Theorem 1 (Tree Convexity) Let R be a network of constraints with arity at most r
and strongly 2(r − 1) + 1 consistent. If R is tree convex then it is globally consistent.
Proof. The network is strongly 2(r − 1) + 1 consistent by assumption. We prove that
the network is k consistent for any k ∈ {2r, . . . , n}.
Consider any instantiation ā of any k − 1 variables and any new variable x. Let the
number of relevant constraints be l. For each relevant constraint, there is one extension set
of ā to x. So, we have l extension sets. If the intersection of all l sets is not empty, we have
a value for x such that the extended instantiation satisfies all relevant constraints.
(Consistency to Set) Consider any two of the l extension sets: E1 and E2 . The two
corresponding constraints involve at most 2(r−1)+1 variables since the arity of a constraint
is at most r and each of the two constraints has x as a variable. By the consistency lemma,
that R is (2(r − 1) + 1)-consistent implies that the intersection of E1 and E2 is not empty.
(Set to Set) Since all relevant constraints are tree convex under the given tree, the
extension sets of ā to x are tree convex. Henceforth, the fact that every two of the extension
sets intersect shows that the intersection of all l extension sets is not empty, by the tree
convex sets intersection lemma.
(Set to Consistency) From the consistency lemma, we have that R is k-consistent. 2
Since a row convex constraint is tree convex, this result generalizes the consistency result
on row convex constraints reported by van Beek and Dechter (1995). It is interesting to
observe that the latter can be lifted from a set intersection results on convex sets (Zhang
& Yap, 2003).
A question raised by Theorem 1 is how efficient it is to check whether a constraint
network is tree convex. Yosiphon (2003) has proposed an algorithm to recognize a tree
convex constraint network in polynomial time.

6. Consistency and the Tightness of Constraints
In this section, we will present various consistency results on the networks with m-tight
constraints.
6.1 Global Consistency on Weakly Tight Networks
The tightness of constraints has been related to the consistency of a constraint network
by van Beek and Dechter (1997). The m-tightness of a constraint is characterized by the
cardinality of the extension sets in the following way.
Definition 7 (van Beek & Dechter, 1997) A constraint cSi is m-tight with respect to x ∈ Si
iff for any instantiation ā of Si − {x},
|Ei,x | ≤ m or |Ei,x | = |Dx |.
A constraint cSi is m-tight iff it is m-tight with respect to every x ∈ Si .
Given an instantiation, if its extension set with respect to x is the same as the domain
of variable x, i.e., |Ei,x | = |Dx |, the instantiation is supported by all values of x and thus
easy to be satisfiable. Hence, in the definition above, these instantiations do not affect the
m-tightness of a constraint.
451

Zhang & Yap

y

x
a

a

b

b

c

c

.......
.......
....... ............
....... ............
.....
.....
...
.
...
...
...............................................................................................................
..
.............
.
.
...
.
.
.
.
.
.......
... ...............
...
...
............ ..................... ....
...
...
.
.
.
.
.
....
.
...
...
....... ................... ...
.
.
.
.
.
.
.
.
...
.
.............
... ...........
...
.
.
.
.
.
.
.
.
...
.
.
.
.....
..
.......
...
....
.
........... ...
...
... ............
..
.
.
..
....
.
.
...
.
.
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
........ ...
...
.
.
.
.
.
..
.
.
.
.
.
.
.
.
.
.
.
...
...
.......
...
...
.........................
...
..
...
...
.................
...
...
...
.....
...
...
.....
.
.
.
.
.
.
.
.
.
.
.
..................
..................

Figure 3: The constraint cxy is 2-tight or 3-tight
Example 8 Consider the constraint cxy in Figure 3 where Dx = Dy = {a, b, c}. An edge
in the graph denotes that its ends are allowed by cxy . It can be verified that for the values
of x, their extension sets have a cardinality of 2, and for values of y, their extension sets
have a cardinality from 1 to 3. Hence, cxy can be said 2-tight or 3-tight but not 1-tight.
We are specially interested in the following tightness.
Definition 8 A constraint cSi is properly m-tight with respect to x ∈ Si iff for any instantiation ā of Si − {x},
|Ei,x | ≤ m.
A constraint cSi is properly m-tight iff it is properly m-tight with respect to every x ∈ Si .
A constraint is m-tight if it is properly m-tight. The converse might not be true. For
example, the constraint x ≤ y, where x ∈ {1, 2, . . . , 10} and y ∈ {1, 2, . . . , 10}, is 9-tight
but not properly 9-tight. It is properly 10-tight since |Ex (10)| = 10 when y = 10.
Next, we define a special constraint network which allows us to make a more accurate
connection between the tightness of constraints and the consistency of the network.
Definition 9 A constraint network is weakly m-tight at level k iff for every set of variables
{x1 , x2 , . . . , xl }(k ≤ l < n) and a new variable x, there exists a properly m-tight constraint
among the relevant constraints on x with respect to {x1 , x2 , . . . , xl }.

x1

x2

y1

x3

x4

y3

y2

y4
(b)

(a)

Figure 4: Two constraint networks. A thin edge represents a properly m-tight constraint
while a thick one represents a non properly m-tight constraint

452

Set Intersection and Consistency in Constraint Networks

Example 9 The network in Figure 4(a) is weakly tight at level 3 because for any three
variables and a fourth variable, one of the relevant constraints is properly m-tight. The
network in Figure 4(b) is not weakly tight at level 3 since for {y1 , y3 , y4 } and y2 , none of
the relevant constraints cy1 y2 and cy4 y2 is properly m-tight.
By the small set intersection corollary (Corollary 1), we have the following consistency
result on a weakly m-tight network.
Theorem 2 (Weak Tightness) If a constraint network R with constraints of arity at
most r is strongly ((m+1)(r−1)+1)-consistent and weakly m-tight at level ((m+1)(r−1)+1),
it is globally consistent.
Proof. Let j = (m+1)(r−1)+1. The constraint network R will be shown to be k-consistent
for all k (j < k ≤ n).
Let Y = {x1 , . . . , xk−1 } be a set of any k − 1 variables, and ā be an instantiation of
all variables in Y . Consider any additional variable xk . Without loss of generality, let the
relevant constraints be cS1 , . . . , cSl , and Ei be the extension set of ā to xk with respect to
cSi for i ≤ l.
(Consistency to Set) Consider any m + 1 of the l extension sets. All the corresponding
m + 1 constraints contain at most (m + 1)(r − 1) + 1 variables including xk . Since R is
((m+1)(r−1)+1)-consistent, by the set intersection and consistency lemma, the intersection
of the m + 1 extension sets is not empty.
(Set to Set) The network is weakly m-tight at level ((m + 1)(r − 1) + 1). So, there must
be a properly m-tight constraint among the relevant constraints cS1 , . . . , cSl . Let it be cSi .
We know its extension set |Ei | ≤ m. Since the intersection of every m + 1 of the extension
sets is not empty, all l extension sets share a common element by the small set intersection
corollary.
(Set to Consistency) By the lifting lemma, R is k-consistent. 2
In a similar fashion, the main tightness result by van Beek and Dechter (1997), where
all the constraints are required to be m-tight, can be lifted from the small sets intersection
corollary by Zhang and Yap (2003). This uniform treatment of lifting set intersection results
to consistency results is absent from the existing works (e.g., Dechter, 1992; van Beek &
Dechter, 1995, 1997; David, 1993).
The tightness result by van Beek and Dechter (1997) requires every constraint to be
m-tight. The weak tightness theorem, on the other hand, does not require all constraints
to be properly m-tight. The following example illustrates this difference.
Example 10 For a weakly m-tight network, we are interested in its topological structure.
Thus we have omitted the domains of variables here. Consider a network with five variables
labeled {1, 2, 3, 4, 5}. In this network, for any pair of variables and for any three variables,
there is a constraint. Assume the network is already strongly 4-consistent.
Since the network is already strongly 4-consistent, we can simply ignore the instantiations
with less than 4 variables. This is why we introduce the level at which the network is weakly
m-tight. The interesting level here is 4. Table 1 shows the relevant constraints for each
possible extension of four instantiated variables to the other one. In the first row, 1234 → 5
453

Zhang & Yap

Extension
1234 → 5,
2345 → 1,
3451 → 2,
4512 → 3,
5123 → 4,

125*,
231 ,
132 ,
123 ,
124 ,

135 ,
241 ,
142 ,
143*,
134*,

145 ,
251*,
152*,
153 ,
154 ,

Relevant constraints
235, 245, 345, 15+,
341, 351, 451, 21 ,
342, 352, 452, 12 ,
243, 253, 453, 13 ,
234, 254, 354, 14 ,

25 ,
31 ,
32+,
23+,
24 ,

35 ,
41 ,
42 ,
43 ,
34+,

45
51+
52
53
54

Table 1: Relevant constraints in extending an instantiation of four variables to a new variable

stands for extending the instantiation of variables {1, 2, 3, 4} to variable 5. Entries in its
second column denote a constraint. For example, 125 denotes c125 . If the constraints on
{1, 2, 5} and {1, 3, 4} (suffixed by * in the table) are properly m-tight, the network is weakly
m-tight at level 4. Alternatively, if the constraints {1, 5}, {2, 3} and {3, 4} (suffixed by +)
are properly m-tight, the network will also be weakly m-tight. The tightness result by van
Beek and Dechter (1997) requires all binary and ternary constraints to be m-tight.
6.2 Making Weakly Tight Networks Globally Consistent
Consider the weak tightness theorem in the previous section. Generally, a weakly m-tight
network might not have the level of local consistency required by the theorem. It is tempting
to enforce such a level of consistency on the network to make it globally consistent. However,
this procedure may result in constraints with higher arity.
Example 11 Consider a network with variables {x, x1 , x2 , x3 }. Let the domains of x1 , x2 , x3
be {1, 2, 3}, the domain of x be {1, 2, 3, 4}, and the constraints be that all the variables should
take different values: x 6= x1 , x 6= x2 , x 6= x3 , x1 6= x2 , x1 6= x3 , x2 6= x3 . This network is
strongly path consistent. In checking the 4-consistency of the network, we know that the
instantiation (1, 2, 3) of {x1 , x2 , x} is consistent but can not be extended to x3 . To enforce 4-consistency, it is necessary to introduce a ternary constraint on {x1 , x2 , x} to make
(1, 2, 3) no longer a valid instantiation.
To make the new network globally consistent, the newly introduced constraints with
higher arity may in turn require higher local consistency in accordance with Theorem 2.
Therefore, it is difficult to predict an exact level of consistency (variable based) to enforce
on the network to make it globally consistent.
In this section, relational consistency will be used to make a constraint network globally
consistent.
Definition 10 (van Beek & Dechter, 1997) A constraint network is relationally m-consistent
iff given (1) any m distinct constraints cS1 , . . . , cSm , and (2) any x ∈ ∩m
i=1 Si , and (3) any
m
consistent instantiation ā of the variables in (∪i=1 Si − {x}), there exists an extension of
ā to x such that the extension is consistent with the m relations. A network is strongly
relationally m-consistent if it is relationally j-consistent for every j ≤ m.
454

Set Intersection and Consistency in Constraint Networks

Variables are no longer of concern in relational consistency. Instead, constraints are the
basic unit of consideration. Intuitively, relational m-consistency concerns whether all m
constraints agree at every one of their shared variables. It makes sense because different
constraints interact with each other exactly through the shared variables.
Relational 1-, and 2-consistency are also called relational arc, and path consistency,
respectively.
Using relational consistency, we are able to obtain global consistency by enforcing local
consistency on the network.
Proposition 4 The weak m-tightness at level k of a constraint network is preserved by the
process of enforcing relational consistency on the network.
Proof. Let R be the constraint network before relational consistency enforcing and R1
the network after consistency enforcing. Clearly, R and R1 have the same set of variables.
Consider any set of variables {x1 , x2 , . . . , xl } (k ≤ l < n) and a new variable x. Since
R is weakly m-tight at level k, there exists a properly m-tight constraint c among the
relevant constraints on x with respect to {x1 , x2 , . . . , xl }. Enforcing relational consistency
on a constraint network will only tighten a constraint. So, the proper m-tightness of c is
preserved. Hence, R1 is weakly m-tight at level k. 2
Now we have the main result of this subsection.
Theorem 3 A constraint network weakly m-tight at level (m + 1)(r − 1) + 1, where r is
the maximal arity of the constraints of the network, is globally consistent after it is made
strongly relationally (m + 1)-consistent.
Proof. By Proposition 4, the network is still weakly m-tight at (m + 1)(r − 1) + 1 after
enforcing strong relational (m + 1)-consistency on it. Let r1 be the maximal arity of the
constraints of the new network after consistency enforcing. Clearly, r1 ≥ r. So, the network
is m-tight at (m + 1)(r1 − 1) + 1 by Proposition 6. The theorem follows immediately from
Theorem 8 in Section 7. 2
The implication of this theorem is that as long as we have certain properly m-tight constraints on certain combinations of variables, the network can be made globally consistent
by enforcing relational (m + 1)-consistency.
We have the following observation on the weak m-tightness of a network.
Proposition 5 A constraint network is weakly m-tight at any level if the constraint between
every two variables in the network is properly m-tight.
Proof. Consider any level k, any set of variables Y = {x1 , x2 , . . . , xl }(k ≤ l ≤ n), and
any new variable x ∈
/ Y . Since the constraint between any two variables is properly m-tight,
the constraint c{x1 ,x} on x1 and x is properly m-tight. Therefore, there is a properly m-tight
constraint c{x1 ,x} among the relevant constraints after an instantiation of Y . 2
This observation shows that the proper m-tightness of the constraints on every two
variables is sufficient to determine the level of local consistency needed to ensure global
consistency of a constraint network.
Remark. Proposition 5 assumes there is a constraint between every two variables. If
there is no constraint between some two variables, a universal constraint is introduced. In
455

Zhang & Yap

this case, we can enforce path consistency on the constraint network to make the binary
constraints tighter so that lower level of relational consistency is needed to make the network
globally consistent.
6.3 Properties of Weakly Tight Constraint Networks
Since for a weakly m-tight constraint network global consistency can be achieved through
local consistency, it is interesting and important to investigate the conditions for a network
to be weakly m-tight. Although Proposition 5 shows a sufficient condition, it requires every
binary constraint be tight. As we can see from Example 9(a), the required number of
tight constraints for a constraint network to be weakly tight can be further reduced. This
subsection is focused on the understanding of the relationship between the number of tight
constraints and the weak tightness of a constraint network.
There is a strong relationship among different levels of weak tightness in a network.
Proposition 6 If a constraint network is weakly m-tight at level k for some m, it is weakly
m-tight at any level j > k.
Proof. For any j > k, we prove that the network is weakly tight at level j. That is, for
any set of variables Y = {x1 , . . . , xj }(k ≤ j < n) and a new variable x, we show that there
exists an m-tight relevant constraint on x with respect to Y . Since the network is weakly
tight for k < j, there exists an m-tight relevant constraint on x with respect to a subset of
Y . This constraint is still relevant on x with respect to Y , and thus the one we look for. 2
In the following, we present two results on sufficient conditions for a constraint network
to be weakly m-tight.
Theorem 4 Given a constraint network (V, D, C) and a number m, if for every x ∈ V ,
there are at least n − 2 properly m-tight binary constraints on it, then the network is weakly
m-tight at level 2.
Proof. For any two variables {x, y} and a third variable z, the relevant constraints
on z with respect to {x, y} are cxz and cyz . We know that the number of relevant binary
constraints on z with respect to V is n − 1. That n − 2 of them are properly m-tight means
either cxz or cyz must be properly m-tight. 2
In fact, for the weak tightness at a higher level, we need fewer constraints to be m-tight
as shown by the following result.
Theorem 5 A constraint network (V, D, C) is weakly m-tight at level k if for every x ∈ V ,
there are at least n − k properly m-tight binary constraints on it.
Proof. For any set Y of k variables and a new variable z, we show that there is a
properly m-tight relevant constraint on z with respect to Y . Otherwise, none of the k
binary constraints on z is properly m-tight. Since the total number of the relevant binary
constraints on z is n − 1, the number of properly m-tight binary constraints on z is at most
(n−1)−k, which contradicts that z is involved in n−k properly m-tight binary constraints.
2
456

Set Intersection and Consistency in Constraint Networks

This result reveals that for a constraint network to be weakly tight at level k, it could
need as few as n(n − k + 1)/2 properly m-tight binary constraints, in contrast to the result
in Theorem 3 where all binary constraints are required to be properly m-tight.
An immediate question is: What is the minimum number of m-tight constraints required
for a network to be weakly tight? It can be answered by the following result on weak
tightness at level 2.
Theorem 6 Given a number m, for a constraint network to be weakly m-tight at level 2,
it needs at least
n(n − 1)/2 − 2bn/3c if n = 0, 1 (mod 3)
or otherwise
(n − 2)(3n − 1)/6
m-tight binary or ternary constraints.
Proof.
Given a network, its weak m-tightness at level 2 depends on the tightness
of only binary and ternary constraints. Among all weakly m-tight (at level 3) constraint
networks with n variables, let R1 be the network that has a minimal set of properly m-tight
binary and ternary constraints.
In the following exposition, a constraint is denoted by its scope. For example, we
use {u, v, w} and {u, v} to denote ternary constraint c{u,v,w} and binary constraint cuv
respectively. A constraint is non-properly-m-tight if it is not properly m-tight.
The proof consists of three steps.
Step 1. While preserving the weak m-tightness of R1 and the number of properly mtight constraints in R1 , we modify, if necessary, the proper m-tightness of some constraints
in R1 such that, for any properly weak m-tight constraint {u, v, w}, none of the binary
constraints {u, v}, {v, w}, and {u, w} is properly m-tight.
To modify the proper m-tightness of a constraint c in R1 is to remove c from the network
and introduce a new constraint on the same set of variables of c with the desirable proper
m-tightness.
We claim that, for any properly m-tight constraint {u, v, w}, at most one of {u, v},
{v, w}, and {u, w} is properly m-tight. Otherwise, at least two of them are properly mtight, which means {u, v, w} can be modified to be not properly m-tight, contradicting the
minimality of the number of properly m-tight constraints in R1 .
Assume {u, v} is properly m-tight. Since {u, v, w} is properly m-tight, there should be
a reason for {u, v} to be properly m-tight. The only reason is that there exists another
variable z such that one of {u, z} and {v, z} is not properly m-tight, and {u, v, z} is not
properly m-tight, too. See Figure 5. Without loss of generality, let {u, z} be properly mtight, implying that the constraint {v, z} is not properly m-tight. The constraint {z, v, w}
is properly m-tight because {v, z} and {v, w} are not properly m-tight.
Now we modify the constraints {u, v, w} and {z, v, w} to be not properly m-tight and
modify the constraints {z, v} and {v, w} to be properly m-tight. This modification preserves
the number of properly m-tight constraints in R1 and the weak m-tightness of R1 .
Step 2. While preserving the weak m-tightness of R1 and the number of properly m-tight
constraints in R1 , we next modify, if necessary, the proper m-tightness of the constraints
in R1 such that no two properly m-tight ternary constraints share any variables.
457

Zhang & Yap

....................
.........
......
......
....... ................................
....
....
......
.....
..
..................................
.
.........
....... .... ..... .. ....
........
....
... ........................
........... .........
.
....
.
......
... ...............
..
...
.
. . ..
.
...
........
..
...
...
....
......
...
.....
....
.
.......
.
.
.
.
.......................

z

u

w

v

Figure 5: The circle represents the properly m-tight ternary constraint {u, v, w}. An edge
between two variables indicates a binary constraint. A tick besides an edge means
it is properly m-tight while a cross means it is not.

Case 1: Two properly m-tight constraints {u, v, w} and {u, v, z} share two variables
{u, v}. See Figure 6(a). Since {w, u} and {u, z} are not properly m-tight (in terms of step
1), {w, u, z} has to be properly m-tight. Since {w, v} and {v, z} are not m-tight, {w, v, z}
has to be m-tight.
We modify the four ternary constraints to be not properly m-tight and modify the four
binary constraints {w, u}, {u, z}, {z, v} and {v, w} to be properly m-tight. This preserves
the weak m-tightness of R1 and the number of properly m-tight constraints in R1 .

w
v

u

x

z

z

v
w

w

v

u

u

v

u

w

x

y

y

(b)

(a)

Figure 6: A dotted ellipse together with the three variables inside it represents a ternary
constraint. (a) Left: Two ternary constraints share two variables {u, v}. Right:
The ternary constraints have to be properly m-tight. (b) Left: Two ternary
constraints share one variable w. Right: The ternary constraints have to be
properly m-tight.
Case 2: Two properly m-tight constraints {u, v, w}, and {w, x, y} share one variable
w. Since {u, w} and {w, x} are not properly m-tight, {u, w, x} has to be properly m-tight.
Since {v, w} and {w, y} are not properly m-tight, {v, w, y} has to be properly m-tight.
Similarly, {u, w, y} and {v, w, x} have to be properly m-tight. Now, if we modify the four
binary constraints {u, w}, {w, x}, {v, w}, and {w, y} to be properly m-tight and the six
ternary constraints to be non-properly-m-tight, the new network is still weakly m-tight
with fewer m-tight constraints. This contradicts the minimality of the number of properly
m-tight constraints in R1 . Hence, case 2 is not possible.
Step 3. As a result of the first two steps, in the network R1 , the scopes of the properly
m-tight ternary constraints are disjoint, and the binary constraint between any two variables
of a properly m-tight ternary constraint is not properly m-tight.
458

Set Intersection and Consistency in Constraint Networks

Let B (and T respectively) be the set of the properly m-tight binary (and ternary
respectively) constraints of R1 .
Assume |T | = k. Since it is difficult to count B, we count the maximum number of
non-properly-m-tight binary constraints in R1 . We have 3k non-properly-m-tight binary
constraints due to T . We should not have any non-properly-m-tight binary constraints
between a variable in T and a variable outside T . Let V 0 be the variables outside T . We
have |V 0 | = n − 3k. The other non-properly-m-tight constraints fall only between variables
in V 0 . Since R1 is weakly tight at level 2, there is no two non-properly-m-tight constraints
on any variable in V 0 . Hence, there are at most (n − 3k)/2 non-properly-m-tight constraints
if n − 3k is even, and otherwise at most (n − 3k − 1)/2 ones. So the number, denoted by δ,
of the properly m-tight constraints in R1 would be the sum of the cardinality of T and B:
δ = k + (n(n − 1)/2 − 3k − b(n − 3k)/2c) = n(n − 1)/2 − 2k − b(n − 3k)/2c.
The fact that δ is minimal implies that k should be maximized. If n is a multiple of 3, the
number of properly m-tight constraints is n(n − 1)/2 − 2n/3; if n is 1 more than a multiple
of 3, the number is n(n − 1)/2 − 2(n − 1)/3; otherwise the number is (n − 1)(3n − 1)/6. 2
This result shows that under the concept of k-consistency we still need a significant
number of constraints to be properly m-tight to predict the global consistency of a network
in terms of constraint tightness.
6.4 Dually Adaptive Consistency
A main purpose of our characterization of weak m-tightness of a network is to help identify a
consistency condition under which a solution of a network can be found without backtracking, i.e., efficiently. We have studied constraint tightness under the concept of k-consistency
in the previous subsections. In this subsection, we introduce dually adaptive consistency to
achieve backtrack free search by taking into account both the tightness of constraints and
the topological structure of a network.
The idea of adaptive consistency (Dechter & Pearl, 1987) is to enforce only the necessary
level of consistency on each part of a network to ensure global consistency. It assumes an
ordering on the variables. For any variable x, it only requires that a consistent instantiation
of the relevant variables before x can be consistently extensible to x. Other variables do not
play any direct role on x and thus are ignored when dealing with x.
We first introduce some notations used in adaptive consistency.
The width of a variable with respect to a variable ordering is the number of constraints
involving x and only variables before x. See Figure 7 for an example.
Given a network, a variable ordering, and a variable x, the directionally relevant constraints on x are those involving x and only variables before x. In the following, DR(x) is
used to denote the directionally relevant constraints on x, and S used to denote all variables
occurring in the constraints of DR(x).
The constraints of DR(x) are consistent on x if and only if, for any consistent instantiation ā of S − {x}, there exists u ∈ Dx such that (ā, u) satisfies all the constraints of
DR(x).
We next define the adaptive consistency of a network.

459

Zhang & Yap

x

1
...... ..........
.....
....
...
.....
...
...
.
.
.
...
..
...
...
.. ........ 2
.
...
.... ....
.
... ....
...
..
.... ...
.
3 .....
... ...
.
... ...
....
....
...
...... ..........
...
.
...
...
4
...
.....
.....
......

x
x
x

x5

Figure 7: The variables {x1 , x2 , . . . , x5 } are ordered according to their subscripts. For example, x1 is before x2 . The width of x2 is 1.

Definition 11 Given a constraint network and an ordering on its variables, the network is
adaptively consistent if and only if for any variable x, its directionally relevant constraints
are consistent on x.
The adaptive consistency is presented as an algorithm by Dechter (2003) although, for
the purpose of this paper, we prefer a declarative characterization.
For an adaptively consistent network, a solution can be found without backtracking.
Proposition 7 Given a constraint network and an ordering on its variables, a backtrack
free search is ensured if the network is adaptively consistent.
Proof. Assume we have found a consistent instantiation of the first k variables (in terms
of the given ordering). They can be consistently extended to xk+1 because all directionally
relevant constraints on xk+1 are consistent on xk+1 . 2
When a network is not adaptively consistent, the algorithm by Dechter (2003, p. 105)
can be used to enforce adaptive consistency on it.
Adaptive consistency is not only more accurate in estimating the local consistency that
ensures global consistency, but also makes intuitive the algorithms to enforce consistency
and to find a solution.
With the knowledge of constraint tightness presented in the previous subsections, we
know that for a network to be adaptively consistent, it is sufficient to make sure that only
some, not all, directionally relevant constraints on a variable are consistent. We are now in
a position to define dually adaptive consistency of a constraint network.
Definition 12 Consider a constraint network and an ordering of its variables. For any
variable x in the network, let cx be one of the tightest directionally relevant constraints on
x and cx be properly mx -tight. The network is dually adaptively consistent if and only if
1) for any variable x whose width is not greater than mx , its directionally relevant
constraints are consistent on it, and
2) for any variable x whose width is greater than mx , cx is consistent with every other
mx directionally relevant constraints on x.
Thanks to the set intersection result of Lemma 2, we have the main result on dually
adaptive consistency.
460

Set Intersection and Consistency in Constraint Networks

Theorem 7 Given a constraint network and an ordering of its variables, a backtrack free
search is ensured if it is dually adaptively consistent.
Proof. We only need to prove that the network is adaptively consistent: For any
variable x, its directionally relevant constraints DR(x) are consistent on x. Let S be the
variables involved in DR(x). Consider any consistent instantiation ā of S − {x}. We show
that there exists u ∈ Dx such that (ā, u) satisfies constraints in DR(x). Let l be the number
of constraints in DR(x), and let cx be one of the tightest constraint in DR(x) with proper
tightness mx . For any constraint ci ∈ DR(x), let ā’s extension set to x under ci be Ei . It
is sufficient to show
∩c ∈DR(x) Ei 6= ∅.
i

We know cx is consistent with every other mx constraints. Hence, Ex , ā’s extension set
under cx , intersects with every other mx extension sets of ā. Lemma 2 implies that
∩c ∈DR(x) Ei 6= ∅.
i

2
By this theorem, we need only the tightest of the directionally relevant constraint on
each variable, totally n − 1 such constraints, to predict the global consistency of a network.
This could be considered a significant improvement over the results in the previous two
subsections.
Compared with the result by Dechter and Pearl (1987), this theorem also provides a
lower level (the smaller of tightness or width) of consistency ensuring global consistency.
When a constraint network is not dually adaptively consistent with respect to a variable
ordering, it can be made so by enforcing the required consistency on each variable, in the
reverse order of the given variable ordering. To make the procedure more efficient, we
should chose a better variable ordering, depending on both the topological structure of the
network and the tightness of the constraints.

7. Tightness and Convexity Revisited
The consistency results derived from small set intersection and tree convex set intersection
in Section 5 and Section 6.1 can be rephrased in a relational consistency setting. For
example, a new version of weak tightness based on relational consistency is given as follows.
Theorem 8 (Weak Tightness) If a constraint network R of constraints with arity of at
most r is strongly relationally (m + 1)-consistent and weakly m-tight at level of (m + 1)(r −
1) + 1, it is globally consistent.
Proof. Let j = (m + 1)(r − 1) + 1. The constraint network R will be shown to be
k-consistent for all k (j < k ≤ n).
Let Y = {x1 , . . . , xk−1 } be a set of any k −1 variables, and ā a consistent instantiation of
all variables in Y . Consider any new variable xk . Without loss of generality, let cS1 , . . . , cSl
be the relevant constraints on xk , and Ei the extension set of ā to xk with respect to cSi
for i ≤ l.

461

Zhang & Yap

(Consistency to Set) Consider any m + 1 of the l extension sets. Since R is relationally
(m + 1)-consistent, the intersection of m + 1 extension sets is not empty.
(Set to Set) The network is weakly m-tight. So, there must be a properly m-tight
constraint in the relevant constraints cS1 , . . . , cSl . Let it be cSi . Its extension set |Ei | ≤ m.
Since every m + 1 of the extension sets have a non-empty intersection, all l extension sets
share a common element by the small set intersection result (Corollary 1).
(Set to Consistency) From the lifting lemma, we have that R is k-consistent. 2
Compared with the weak tightness theorem in Section 6.1, the exposition of the result
is neater and the proof is simpler.
For completeness, we also include here a new version of the tree convex theorem using
relational consistency. The proof is omitted since it is a simplified version of the one in
Section 5 as hinted by the proof above.
Theorem 9 (Tree Convexity) Let R be a tree convex constraint network. R is globally
consistent if it is strongly relationally path consistent.

8. Conclusion
Through the lifting lemma and proof schema, we have shown that set intersection results can
be easily lifted to consistency results in a constraint network. There are a few advantages
for this approach of studying consistency.
Firstly, although this approach does not offer a “completely new” way to prove consistency results, it does provide a uniform way to understand many seemingly different results
on the impact of convexity and tightness on global consistency. In addition to the results
shown here, some other results can also be obtained easily by the lifting lemma and proof
schema. For example, the work by David (1993) can be obtained by lifting the corollary of
Lemma 2 (Zhang & Yap, 2003). The work by Sam-Haroud and Faltings (1996) on convex
constraint networks with continuous domains can be lifted from Helly’s theorem (Eckhoff,
1993) on the intersection of convex sets in Euclidean spaces.
Secondly, the establishment of the relationship between set intersection and consistency
in a constraint network makes it easier to communicate the consistency results to the researchers outside the constraint network community. It is also made possible for them to
contribute to consistency results by exploiting their knowledge on set intersection properties.
More importantly, this approach singles out the fact that set intersection properties play
a fundamental role in determining the consistency of a constraint network. This perspective
helps us focus on properties of set intersection and discover or generalize the intersection
properties of tree convex sets and sets with cardinality restrictions. The corresponding
consistency results have extended our understanding of the convexity and tightness of constraints since Dechter and van Beek’s work (1995, 1997). We identify a new class of tree
convex constraints for which global consistency is ensured by a certain level of local consistency. This generalizes row convex constraints by van Beek and Dechter (1995). We also
show that a weakly m-tight constraint network can be made globally consistent by enforcing
local consistency. This type of result on tightness is new. Detailed study has been carried
out on when a constraint network is weakly m-tight. To make full use of the tightness of
the constraints, we propose dually adaptive consistency that exploits both the topology and
462

Set Intersection and Consistency in Constraint Networks

the semantics of a constraint network, which again results from the relation between set
intersection and consistency. Under dually adaptive consistency, the topology of a network
and the tightest relevant constraint on each variable determine the local consistency that
ensures backtrack-free search.

Acknowledgments
We are indebted to Dr. Peter van Beek and Dr. Fengming Dong for very helpful discussions.
The constructive comments from the anonymous referees of various versions of this paper
have improved its quality. This material is based on works partially supported by a grant
under the Academic Research Fund of National University of Singapore and by Science
Foundation Ireland under Grant 00/PI.1/C075. Some materials of this paper appeared in
the Proceedings of the International Joint Conference on Artificial Intelligence 2003 (Zhang
& Yap, 2003) and the Proceedings of Principles and Practice of Constraint Programming
2004 (Zhang, 2004).

References
David, P. (1993). When functional and bijective constraints make a CSP polynomial. In
Proceedings of Thirteenth International Joint Conference on Artificial Intelligence,
Vol. 1, pp. 224–229 Chambery, France. IJCAI, Inc.
Dechter, R. (1992). From local to global consistency. Artificial Intelligence, 55, 87–107.
Dechter, R. (2003). Constraint Processing. Morgan Kaufmann, San Francisco, CA.
Dechter, R., & Pearl, J. (1987). Network-based heuristics for constraint satisfaction problems. Artificial Intelligence, 34, 1–38.
Eckhoff, J. (1993). Helly, Radon, and Carathéodory type theorems. In Gruber, P. M.,
& Wills, J. M. (Eds.), Handbook of Convex Geometry, pp. 389–448. North Holland,
Amsterdam.
Freuder, E. (1978). Synthesizing constraint expressions. Communications of ACM, 21 (11),
958–966.
Freuder, E. (1982). A sufficient condition for backtrack-free search. Journal of The ACM,
29 (1), 24–32.
Jeavons, P. G., Cohen, D. A., & Gyssens, M. (1997). Closure properties of constraints.
Journal of The ACM, 44 (4), 527–548.
Mackworth, A. K. (1977). Consistency in networks of relations. Artificial Intelligence, 8 (1),
118–126.
Montanari, U. (1974). Networks of constraints: fundamental properties and applications.
Information Science, 7 (2), 95–132.
Sam-Haroud, D., & Faltings, B. V. (1996). Solving non-binary convex CSPs in continous
domains. In Proceedings of International Conference on Principles and Practice of
Constraint Programming 1996, pp. 410–424 Cambridge, Massachusetts. Springer.

463

Zhang & Yap

Schaefer, T. J. (1978). The complexity of satisfiability problems. In Proceedings of 10th
ACM Symposium on the Theory of Computing, pp. 216–226.
van Beek, P., & Dechter, R. (1995). On the minimality and global consistency of row-convex
constraint networks. Journal of The ACM, 42 (3), 543–561.
van Beek, P., & Dechter, R. (1997). Constraint tightness and looseness versus local and
global consistency. Journal of The ACM, 44 (4), 549–566.
Yosiphon, G. (2003). Efficient algorithm for identifying tree convex constraints. Manuscript.
Zhang, Y. (2004). On the tightness of constraints. In Proceedings of Principles and Practice
of Constraint Programming 2004, pp. 777–781 Toronto, Canada. Springer.
Zhang, Y., & Freuder, E. C. (2004). Tractable tree convex constraints. In Proceedings of
National Conference on Artificial Intelligence 2004, pp. 197–202 San Jose, CA, USA.
AAAI press.
Zhang, Y., & Yap, R. H. C. (2003). Consistency and set intersection. In Proceedings of
International Joint Conference on Artificial Intelligence 2003, pp. 263–268 Acapulco,
Mexico. IJCAI Inc.

464

Journal of Artificial Intelligence Research 27 (2006) 1-23

Submitted 02/06; published 09/06

A Variational Inference Procedure Allowing Internal
Structure for Overlapping Clusters and Deterministic
Constraints
Dan Geiger

dang@cs.technion.ac.il

Computer Science Dept., Technion,
Haifa, 32000, Israel

Christopher Meek

meek@microsoft.com

Microsoft Research, Microsoft Corporation,
Redmond, WA 98052, USA

Ydo Wexler

ywex@cs.technion.ac.il

Computer Science Dept., Technion,
Haifa, 32000, Israel

Abstract
We develop a novel algorithm, called VIP*, for structured variational approximate
inference. This algorithm extends known algorithms to allow efficient multiple potential
updates for overlapping clusters, and overcomes the difficulties imposed by deterministic
constraints. The algorithm’s convergence is proven and its applicability demonstrated for
genetic linkage analysis.

1. Introduction
Probabilistic graphical models are an elegant framework to represent joint probability distributions in a compact manner. The independence relationships between random variables
which are nodes in the graph are represented through the absence of arcs in the model.
This intuitively appealing presentation also naturally enables the design of efficient generalpurpose algorithms for computing marginal probabilities, called inference algorithms.
The general inference problem is NP-hard (Cooper, 1990; Dagum & Luby, 1993), and
although there are many cases where the model is small (or, more precisely, has a small
treewidth) and exact inference algorithms are feasible, there are others in which the time
and space complexity makes the use of such algorithms infeasible. In these cases fast yet
accurate approximations are desired.
We focus on variational algorithms: a powerful tool for efficient approximate inference
that offers guarantees in the form of a lower bound on the marginal probabilities. This
family of approaches aims to minimize the KL divergence between a distribution Q and the
target distribution P by finding the best distribution Q from some family of distributions
for which inference is feasible. In particular, we have a joint distribution P (X) over a set of
discrete variables X and our goal is to compute the marginal probability P (Y = y) where
Y ⊆ X. Further assume that this exact computation is not feasible. The idea is to replace
P with a distribution Q which can be used to compute a lower bound for P (Y = y). We
c
2006
AI Access Foundation. All rights reserved.

Geiger, Meek & Wexler

let H = X \ Y . Then, by using Jensen’s inequality we get the following bound:
log P (y) = log

X
h

Q(h)

P (y, h) X
P (y, h)
≥
Q(h) log
= −D(Q(H) || P (Y = y, H))
Q(h)
Q(h)
h

where D(· || ·) denotes the KL divergence between two probability distributions. The quantity −D(Q || P ) is often called the free-energy where P and Q are possibly un-normalized
distributions. Variational techniques aim to choose a distribution Q such that the lower
bound is as high as possible, or equivalently, such that the KL divergence between Q(h)
and P (h|Y = y) is minimized.
Variational approaches such as the mean field, generalized mean field, and structured
mean field differ only with respect to the family of approximating distributions that can
be used, with the structural mean field approach subsuming the remaining approaches as
special cases. The research of several authors guided our work: Saul & Jordan (1996),
Ghahramani & Jordan (1997), Wiegerinck (2000) and Bishop & Winn (2003).
The contributions of this paper are threefold. First we develop an extension to the
algorithm by Wiegerinck (2000), which we call vip? , that allows for a set potentials of the
approximating distribution Q to be updated simultaneously even if the clusters of Q overlap. Algorithm vip? is N -fold faster than Wiegerinck’s algorithm for N ×N grid-like models
and yields two orders of magnitude improvement for large graphs such as genetic linkage
analysis model of large pedigrees. Note that simultaneous updates were first presented for
phylogenic trees by Jojic et al. (2004). Second, we prove the convergence of vip? and of previous variational methods via a novel proof method, using properties of the KL divergence.
Third, we extend vip? to allow deterministic constraints in the model and demonstrate the
applicability of this extension to genetic linkage analysis.

2. Background
This background section is based primarily on the paper by Weigerinck (2000), which in
turn builds on pioneering works such as the papers of Saul & Jordan (1996) and Ghahramani
& Jordan (1997). Our review provides a new exposition of this material.
We denote distributions by P (x) and Q(x) and related un-normalized distributions by
P̃ (x) ∝ P (x) and Q̃(x) ∝ Q(x). Let
X be a finite set of variables and x be an instantiation
1 Q
of these variables. Let P (x) = ZP i Ψi (di ) where di is the projection of the instantiation
x to the variables in Di ⊆ X and where Ψi is a non-negative function, commonly called a
potential. The constant ZP normalizes the product of potentials and the subsets {Di }Ii=1
are allowed to overlap. We often suppress the arguments of a potential and of a distribution,
using Ψi instead of Ψi (di ) and P instead of P (X).
Our goal is to find a distribution Q that minimizes Q
the KL divergence between Q and P .
We further constrain Q to be of the form Q(x) = Z1Q j Φj (cj ) where ZQ is a normalizing
constant and where C1 , . . . , CJ are possibly overlapping subsets of X, which we call clusters. Finding an optimum Q, however, can be difficult. A more modest and common goal
is devising iterative converging algorithms such that in each iteration the KL divergence
between an approximating distribution Q and P decreases unless Q is a stationary point.
Throughout, we define Q(w|u) = |W1\U | for instantiations U = u for which Q(u) = 0.
Consequently, all terms in the equality Q(w, u) = Q(u)Q(w|u) are well defined even if
2

A Variational Inference Procedure

P
Q(u) = 0. Moreover, this convention maintains properties such as
W \U Q(w|u) = 1
1
1
1
and Q(w, z|u) = Q(w|z, u)Q(z|u) = |W \{U ∪Z}| · |Z\U | = |{W ∪Z}\U | . We also note that
Q(x) log Q(x) = 0 whenever Q(x) = 0 and thus the KL divergence
D(Q || P ) =

X

Q(x) log

x

Q(x)
P (x)

is not finite if and only if P (x) = 0 and Q(x) > 0 for some instance x.
Our starting point is the algorithm developed by Wiegerinck (2000). The algorithm finds
such a distribution Q as follows: it iterates over the clusters Cj and their instantiations cj
to update the potentials Φj (cj ) = eγj (cj ) via the following update equation:

γj (cj ) ← −

X

X

Q(ck |cj ) log Φk (ck ) +

{k:gkj =1} Ck \Cj

X

X

Q(di |cj ) log Ψi (di )

(1)

{i:fij =1} Di \Cj

where gkj and fij are two indicator functions defined via gkj = 0 if Q(Ck |cj ) = Q(Ck ) for
every instance cj of Cj and 1 otherwise, and fij = 0 if Q(Di |cj ) = Q(Di ) for every instance
cj of Cj and 1 otherwise. Wiegerinck (2000) proved convergence of this algorithm to a stationary point using Lagrangians. Throughout we call this iterative procedure, Wiegerinck’s
algorithm.
Wiegerinck’s algorithm relies at each step on an algorithm to compute the conditional
probabilities Q(ck |cj ) and Q(di |cj ) from an un-normalized distribution Q̃ represented by
a set of potentials Φj (cj ). This can be accomplished by any inference algorithm such as
bucket elimination algorithm or the sum-product algorithm described by Dechter (1999)
and
Q
Kschischang, Frey & Loeliger (2001) . It is important to note that for Q̃(x) = j Φj (cj )
the computation of these conditionals is not affected by multiplying any Φj by a constant
α.
Wiegerinck’s algorithm generalizes the mean field (MF) algorithm and the generalized
mean field (GMF) algorithm (Xing, Jordan & Russell, 2003, 2004). The mean field algorithm is the special case of Wiegerinck’s algorithm in which each Cj contains a single
variable. Similarly, the generalized mean field algorithm is the special case in which the Cj
are disjoint subsets of variables. When Cj are disjoint clusters, the formula for γj in Eq. 1
simplifies to the GMF equations as follows (first term drops out):
γj (cj ) ←

X

X

Q(di |cj ) log Ψi (di ).

(2)

{i:fij =1} Di \Cj

The term Q(di |cj ) can be made more explicit when Cj are disjoint clusters (Bishop & Winn
2003). In particular, the set Di \ Cj partitions into Dik = (Di \ Cj ) ∩ Ck Q
for k = 1, . . . , J
where k 6= j. Note that Dik = Di ∩ Ck . Using this notation, Q(di |cj ) = k Q(dki ) where
Q(dki ) = 1 whenever Dik = ∅. This factorization further simplifies the formula for γj as
follows:
X
X X
γj (cj ) ←
Q(d1i ) . . .
Q(dJi ) log Ψi (di ).
(3)
{i:fij =1} Di1

DiJ

3

Geiger, Meek & Wexler

This simplification is achieved automatically when using bucket elimination for computing
γj . The iterated sums in Eq. 3 are in fact the buckets formed by bucket elimination when
Cj are disjoint.
Eq. 1 requires repeated computation of the quantities Q(ck |cj ) and Q(di |cj ). This repetition can be significant because there could be many indices k such that Q(Ck |cj ) 6= Q(Ck ),
and many indices i such that Q(Di |cj ) 6= Q(Di ). As these computations share many subcomputations it is therefore reasonable to add a data structure to facilitate a more efficient
implementation for these function calls. In particular, it is possible to save computations if
the sets C1 , . . . , CJ form a junction tree.
A set of clusters C1 , . . . , CJ forms a junction tree iff there exists a set of trees JT having one node, called Cj , for each cluster of variables Cj , and for every two nodes Ci and
Cj of JT, which are connected with a path in JT, and for each node Ck on this path,
Ci ∩ Cj ⊆ Ck holds. By a set of trees we mean an undirected graph, not necessarily
connected, with no cycles. Note that this definition allows a junction tree to be a disconnected graph.
When
Q
Q C1 , . . . , CJ form a junction tree, Q(x) has the decomposable form
Q(x) = j Φj (cj )/ e Φe (se ), where Φj are marginals on the subsets Cj of X, and where
Φe are the marginals on intersections Se = Ci ∩ Cj , one for each two neighboring clusters
in the junction tree (Jensen 1996).
Wiegerinck (2000) enhanced his basic algorithm so that it maintains
a consistent
junction
P
P
tree JT for the distribution Q(x). Consistency means that Cj \Ck Φj = Ck \Cj Φk for
every two clusters. In a consistent junction tree, each potential Φj (Cj ) is proportional to
Q(Cj ). An update of a potential during the algorithm may yield an inconsistent junction
tree, however, consistency is maintained by applying DistributeEvidence(Φj ) (Jensen
1996) after each update of a potential. The procedure DistributeEvidence(Φ0j ) accepts
as input a consistent junction tree and a new cluster marginal Φ0j for Cj , and updates the
potential of every neighboring cluster Ck of Cj via
Cj \Ck

Φ0j (cj )

Cj \Ck

Φj (cj )

P
Φ0k (ck )

← Φk (ck ) P

(4)

and each neighboring cluster recursively propagates the update by applying Eq. 4 to all its
neighbors except the one from which the update came. The output of this procedure is a
consistent junction tree, having the same clusters, where Φ0j is the (possibly un-normalized)
marginal probability of Q on Cj , and where the conditional probability Q(X|Cj ) remains
unchanged (Jensen 1996, pp. 74).
Wiegerinck’s enhanced algorithm, which uses a junction tree, iteratively updates the
potential of each cluster (node in the junction tree), using the potentials of other clusters
and separators. However, since the junction tree may not be consistent after the update,
the algorithm applies the procedure DistributeEvidence(Φj ) to the junction tree, after
each update. Note that our description omits a normalization step in Wiegerinck (2000)
that is not needed for convergence.
The most time consuming computation in variational algorithms is computing conditional probabilities of the form Q(ck |cj ) and Q(di |cj ). We distinguish among these conditional probabilities as follows.

4

A Variational Inference Procedure

Definition: A conditional probability Q(A|cj ) is subsumed by Q if the set of target variables A is a subset of some cluster Ck of Q (i.e., (A \ Cj ) ⊆ Ck ).
Wiegerinck’s enhanced algorithm has substantial computational benefits when the conditional probabilities are subsumed. In such cases the needed quantities in Eq. 1, Q(di |cj )
and Q(ck |cj ), are obtained by a mere lookup in the junction tree, and only one call to
DistributeEvidence is made for each update.
Weigerinck’s basic and enhanced algorithms do not assume any structure for Φj , namely,
the algorithms hold tables Φj with an explicit entry for every instantiation of Cj . Since
the computations Q(ck |cj ) and Q(di |cj ) grow exponentially in the size of Di and Ck , the
algorithms become infeasible for large cliques or clusters. For simplification, additional
structure to Φj was suggested by Wiegerinck (2000, Section 4) of the form,

Φj (cj ) =

nj
Y

Φjl (cjl ),

(5)

l=1

where the sets Cjl , l = 1, . . . , nj , are possibly overlapping subsets of Cj , and cjl is the
projection of the instantiation cj on the variables in Cjl . Using such structure it is sufficient
to hold tables for the subsets Cjl which are considerably smaller. Note that when Φj has
an entry to each instantiation cj , then nj = 1 and Φj (cj ) = Φj1 (cj1 ). Weigerinck uses this
structure for the potentials Φj under the following assumptions:
Definition [SelfQ
compatibility]: A distribution Q with clusters Cj and subsets Cjl of the
1
form Q(x) = ZQ j Φj (cj ), with clusters that factor according to Eq. 5, is self compatible
if for every Cj and Ck the set of indices Njk = {l : Q(Ck |cj ) = Q(Ck |cjl )} is non-empty
regardless of the values of the potentials Φj , where cj is an arbitrary instantiation of Cj
and cjl is the projection of cj on Cjl .
Definition [Compatibility
wrt P ]: A distribution Q with clusters Cj and subsets Cjl of
Q
the form Q(x) = Z1Q j Φj (cj ), with clusters that factor according to Eq. 5, is compatible
Q
wrt a distribution P with sets Di of the form P (x) = Z1P i Ψi (di ) if for every Di and Cj
the set of indices Mij = {l : Q(Di |cj ) = Q(Di |cjl )} is non-empty, where cj is an arbitrary
instantiation of Cj and cjl is the projection of cj on Cjl .
Note that self-compatibility and compatibility wrt P depend on the form of Q and not on
a particular realization of the potentials Φj .
Under these assumptions Weigerinck states that considerable simplifications can be deduced, and provides some examples for this statement.
We note that the algorithms of Bishop & Winn (2003) and Jojic et al. (2004) use a
stronger assumption that the clusters Cj of the approximating distribution Q are disjoint
and that Q(Ck |cj ) = Q(Ck ). This assumption, which implies that Q(Ck |cj ) = Q(Ck |cjl )
and Q(Di |cj ) = Q(Di |cjl ) for every index l, is relaxed by requiring each of these equalities
to hold for a single index l (but possibly for multiple indices).
5

Geiger, Meek & Wexler

3. Multiple Potential Update using Overlapping Clusters
In this section we develop a new algorithm, called vip? , that uses the additional structure
of potentials offered by Eq. 5 to speed up computations. In particular, rather than updatnj
ing each potential Φjl separately, we offer a way to update the set of potentials {Φjl }l=1
simultaneously, saving considerable computations. Furthermore, this simultaneous update
is enhanced by using a junction tree, despite the fact that the sets {Cjl } need not form a
junction tree, and only {Cj } form a junction tree.
Our algorithm uses the definitions of self compatibility and compatibility wrt P , defined
earlier, and the following definition of indices.
Definition: Let the indicator function gjk (l) equal 1 for a single fixed index l ∈ Njk and
0 for all other indices in Njk when Q(Ck |cj ) 6= Q(Ck ), and equal 0 otherwise. Let the
indicator function fij (l) equal 1 for a single fixed index l ∈ Mij and 0 for all other indices
in Mij when Q(Di |cj ) 6= Q(Di ), and equal 0 otherwise.
Algorithm vip? is given in Figure 1. Its convergence is proved in Section 4. The proof
requires Q to be self-compatible, compatible wrt P , and in addition, to satisfy (P (x) = 0) ⇒
(Q(x) = 0). Note that D(Q || P ) = ∞ for distributions Q which do not satisfy the last
assumption.
The main improvement of the algorithm is an efficient update of potentials. For potentials Φj that factorize into smaller potentials Φjl according to Eq. 5, algorithm vip? only
updates Φjl instead of updating the whole potential Φj , as done in Weigerinck’s algorithms.
The update of the potentials Φjl as done by vip? is equivalent to updating Φj according to
Eq. 1, up to an irrelevant constant, but does not require to compute the update equation for
each instance of the cluster Cj . The proposed change considerably speeds up the previous
algorithms.
The algorithm gets as input a target distribution P with sets Di of the form P (x) =
1 Q
i Ψi (di ) and an approximating distribution Q with clusters Cj which is self-compatible,
ZP
compatible wrt P and satisfies
the condition (P (x) = 0) ⇒ (Q(x) = 0). Distribution Q is
Q
of the form Q(x) = Z1Q j Φj (cj ) where the potential of every cluster Cj factors according
Qnj
Φjl (cjl ) and the clusters form a consistent junction tree. The algorithm
to Φj (cj ) = l=1
iterates over the clusters, updating the potential of every instantiation of each of the subsets
Cjl according to Eq. 6. To apply the update equation, the quantities Q(di |cjl ) are computed
via variable propagation (Jensen, pp 69-80) on the junction tree. When these quantities are
subsumed, they are obtained by a mere lookup in the junction tree. Then, after updating
the potentials of all subsets Cjl for a cluster Cj , procedure DistributeEvidence is applied
once to make the junction tree consistent with respect to Φj . Since the clusters Cj form a
junction tree only via their subsets Cjl , Eq. 4 is replaced with Eq. 7. After convergence,
algorithm vip? outputs the approximating distribution Q with its revised potentials.
Example 1 The target distribution P is an N ×N grid of pairwise potentials (see Figure 2a)
and the approximating family is defined by a single row and the set of columns in the grid,
each augmented with edges to the middle vertex (see Figure 2b) where C7 is a row of the
grid and Ci (i = 1, . . . , N = 6) are the columns. Using the notation Xi,j to denote the
vertex at row i and column j in the grid, cluster C7 is associated with N − 1 subsets
6

A Variational Inference Procedure

Algorithm VIP? (Q,P)
Q
Q
Input: Two probability distributions P (x) = Z1P i Ψi (di ) and Q(x) = Z1Q j Φj (cj ) where
Qnj
the initial potentials Φj (cj ) = l=1
Φjl (cjl ) form a consistent junction tree, and where Q is
self-compatible, compatible wrt P , and satisfies (P (x) = 0) ⇒ (Q(x) = 0).
Output:
A revised set of potentials Φjl (cj ) defining a probability distribution Q via Q(x) ∝
Q
Φ
(c
j,l jl jl ) such that Q is a stationary point of D(Q || P ).
Iterate over all clusters Cj until convergence
Step 1.
For l = 1, . . . , nj :
For every instantiation cjl of Cjl apply the following update equation:
γjl (cjl ) ← −

X

X

X

Q(ck |cjl ) log Φk (ck ) +

{k:gjk (l)=1} Ck \Cjl

X

Q(di |cjl ) log Ψi (di ) (6)

{i:fij (l)=1} Di \Cjl

Φjl (cjl ) ← eγjl (cjl )
Note: Q(di |cjl ) is computed via variable propagation (Jensen, pp 69-80) on the junction
tree JT. However, when these quantities are subsumed, they are obtained by a mere lookup
in JT.
Step 2. Make JT consistent with respect to Φj :

DistributeEvidence(JT, Φj )

DistributeEvidence(JT, Φ0j )
Input: A junction tree JT with nodes Ck and potentials Φk (ck ) =
node Cj with a revised potential Φ0j .
Output: A consistent junction tree.

Qnk

l=1 Φkl (ckl ).

A starting

initialization source(j) ← 0; updated← {j}
While (updated6= ∅)
β ← first element in updated; updated← updated\{β}
For all neighboring nodes Ck of Cβ in JT such that k 6= source(β)
P
Qnβ 0
C
\C
l=1 Φβl (cβl )
Φ0km (ckm ) ← Φkm (ckm ) P β k Qnβ
Cβ \Ck
l=1 Φβl (cβl )
for a single subset m of Ck for which (Cβ ∩ Ck ) ⊆ Ckm
source(k) ← β
updated← updated∪{k}
Figure 1: Algorithm vip?
7

(7)

Geiger, Meek & Wexler

(a)

(b)

(c)

Figure 2: (a) Grid-like P distribution (b) & (c) Approximating distributions Q.
C7l = {X3,l , X3,l+1 }. Each column cluster Cj is associated with 2N-4=8 subsets Cjl from
which Cjl = {Xl,j , Xl+1,j } for N-1 subsets (l = 1, . . . , 5), Cjl = {X1,j , X3,j } for l = N , and
Cjl = {Xl−N +4,j , X3,j } for each of the additional N-4 subsets (l = 7, 8).
This choice induces a self-compatible approximating distribution Q; every column cluster
Cj is independent of another cluster given a subset that contains X3,j (such as Cj2 ). In
addition, the row cluster C7 is independent of every column cluster Cj given C7j . The
induced distribution is also compatible wrt P ; for each vertical edge Dv = {Xi,j , Xi+1,j }
of P , distribution Q satisfies Q(Dv |ck ) = Q(Dv |ck2 ) for a column cluster Ck such that
k 6= j, and Q(Dv |c7 ) = Q(Dv |c7j ). In addition, for each horizontal edge Dh = {Xi,j , Xi,j+1 }
in P , distribution Q satisfies Q(Dh |c7 ) = Q(Dh |c7j ), and Q(Dh |ck ) = Q(Dh |ck2 ) for k 6=
j, j + 1. Finally, for the edge Dh and k = j, j + 1, the approximating distribution satisfies
Q(Dh |ck ) = Q(Dh |ckl ) where Ckl = {Xi,k , X3,k }, due to the additional N − 3 edges added
to each column cluster.
Like Wiegerinck’s enhanced algorithm, algorithm vip? has substantial computational
benefits when the conditional probabilities Q(di |cjl ) are subsumed. In such cases the needed
quantities, Q(di |cjl ) and Q(ck |cjl ), are obtained by a mere lookup in the junction tree in
step 1 of the algorithm, and only one call to DistributeEvidence is made in step 2,
as demonstrated in the next paragraph. The computational efficiency of vip? is achieved
even if the quantities Q(di |cjl ) are not subsumed but only factor to subsumed probabilities.
Disjoint clusters is one such special case, in which the quantities Q(di |cjl ) can factor into
subsumed probabilities Q(dki |cjl ), where Dik = Di ∩ Ck , which are obtainable by lookup in
a junction tree.
Consider Example 1 to compare the computational cost of vip? versus Wiegerinck’s
basic and enhanced algorithms. Assume Wiegerinck’s basic algorithm (Eq. 1), uses the
distribution Q given in Figure 2c, with 35 clusters Cj0 and 60 sets Di . Therefore, when a
junction tree is not used, 4 · (60 + 34) = 376 conditionals are computed for each cluster Cj0
(edge) not on the boundary of the grid, 94 for each of the four possible values for the edge
cluster Cj0 . Clearly, if additional clusters are introduced, as shown for example by Figure 2b,
then the computational cost grows. By using a junction tree, as done by Wiegerinck’s enhanced algorithm, the subsumed conditional probabilities, which are computed separately
by Wiegerinck’s basic algorithm, are computed with a single call to DistributeEvidence.
These computation covers all subsets in Figure 2c. The only conditionals that are not sub8

A Variational Inference Procedure

(a)

(b)

(c)

Figure 3: The target distribution P is a grid of pairwise potentials as in (a). Two different
partitions of the grid into clusters are shown in Figures (b) and (c), both contain
the same subsets.

sumed are Q(di |cj ) of horizontal edges Di that are not contained in a single cluster, namely,
edges in 2a but not in 2c. These factor to two subsumed probabilities, one computed by the
single call described earlier and the other requires a second call to DistributeEvidence.
For example, let Di = {X1 , X2 } be a horizontal edge in P which does not overlap with Cj0 ,
then Q(x1 , x2 |c0j ) = Q(x1 |c0j , x2 )Q(x2 |c0j ). The two conditionals are subsumed, but a second
call to DistributeEvidence is needed to obtain Q(x1 |c0j , x2 ). This yields 25 calls to DistributeEvidence. However, in Example 1, one call to DistributeEvidence is sufficient
to compute the conditionals for two adjacent horizontal edges, yielding the need only for
15 calls. Therefore, since there are 4 · 15 = 60 calls to DistributeEvidence, and since
the cost of the junction tree algorithm is typically twice the cost of computing conditional
probabilities without using a junction tree, this yields a 3-fold speedup for Wiegerinck’s
enhanced algorithm versus Wiegerinck’s basic algorithm. For edges on a boundary, the
speedup factor is less than 3. As the size of the grid grows, a smaller fraction of the edges
are on the boundary, and, thus, the speedup approaches a 3-fold speedup.
A significant speedup is obtained by using algorithm vip? with clusters Cj and subsets
as described in Figure 2b. Note that the additional subsets are needed to meet the compatibility assumption of vip? . Algorithm vip? makes one call to DistributeEvidence
per cluster Cj for each non-subsumed conditional, rather than for every edge cluster Cj0 .
Since vip? uses N + 1 clusters Cj , the speedup when compared to Wiegerinck’s enhanced
algorithm approaches N as the the N × N grid grows. This O(N ) speedup is confirmed in
the experiments section (Figure 9).
Another potential benefit of vip? is the possibility of alternating between different
choices of clusters which contain identical subsets Cjl . A simple example is the grid in
Figure 3a, where two such choices are illustrated in Figures 3b and 3c. The two sets of clusters update of the potentials Φj differently and therefore can yield better approximations as
the distance D(Q || P ) is reduced with every alternation. In general, we can iterate through
a set of choices for clusters and execute vip? for one choice using as initial potentials the
potentials Φjl found for an earlier choice of clusters. The practical benefit of this option of
added flexibility remains to be tested in an application.
9

Geiger, Meek & Wexler

4. Proof of Convergence
We develop several lemmas that culminate with a proof of convergence of algorithm vip? .
Lemma 1 states that for two un-normalized probability distributions P (x) and Q(x) the
KL divergence is minimized when Q(x) is proportional to P (x). Lemma 2 rewrites the
KL divergence D(Q || P ) in terms of the potentials P
of Q and P using the quantity υj (cj )
which, according to Lemma 3, differs from γj (cj ) = l γjl (cjl ) only by a constant. Finally,
Theorem 1 asserts that the KL divergence between Q and P decreases with each iteration
of Algorithm vip? unless Q is a stationary point. The proof exploits the new form of
D(Q || P ) provided by Lemma 2, and replaces the term υj (cj ) with the terms γjl (cjl ) used
in the update equation of vip? . A final observation, which uses Lemma 1, closes the proof
showing that when the potentials are updated as in algorithm vip? , the KL divergence is
minimized wrt Φj .
The first lemma provides a variant of a well known property of KL. Recall that for
every two probability distributions Q(x) and P (x), the KL divergence D(Q(x) || P (x)) ≥ 0
and equality holds if and only if Q(x) = P (x) (Cover & Thomas 1991; Theorem 2.6.3). A
similar result holds also for un-normalized probability distributions.
Lemma 1 Let Q̃(x) and P̃ (x) be non-negative functions such that
let
Q̂(x) =
D(Q̃(x) || P̃ (x))
P min
{Q̃|

x

P

x P̃ (x)

= ZP > 0, and

Q̃(x)=ZQ }

where ZQ is a positive constant. Then Q̂(x) =

ZQ
ZP P̃ (x).

Proof. We observe that
P̃ (x)
D(Q̃(x) || P̃ (x)) = ZQ · D( Q̃(x)
ZQ || ZP ) + ZQ log

ZQ
ZP

which implies, using the cited result about normalized distributions, that the minimum is
P̃ (x)
obtained when Q̃(x)
ZQ = ZP , yielding the desired claim. 
The next lemma rewrites the KL divergence so that an optimizing update equation for
cluster Cj becomes readily available.
Lemma 2 Let P (x) =
tions. Then,

1
ZP

Q

i Ψi (di )

D(Q || P ) =

X
Cj

and Q(x) =

Q(cj ) log

1
ZQ

Q

j

Φj (cj ) be two probability distribu-

Φj (cj )
+ log(ZP ) − log(ZQ )
Υj (cj )

(8)

where Υj (cj ) = eυj (cj ) , and where
υj (cj ) = −

X X

Q(ck |cj ) log Φk (ck ) +

X X
i

k Ck \Cj

10

Di \Cj

Q(di |cj ) log Ψi (di )

(9)

A Variational Inference Procedure

Proof: Recall that
D(Q || P ) =

X

Q(x) log

X

Q(x)
= − [H(Q) + EQ [log P (x)]]
P (x)

(10)

where H(Q) denotes the entropy of Q(x) and EQ denotes expectation with respect to Q.
The entropy term can be written as
X X
H(Q) = −
Q(cj )Q(x|cj ) [log Q(cj ) + log Q(x|cj )]
Cj X\Cj

=−

P

Cj

Q(cj ) log Q(cj ) −

P

Cj

Q(cj )

P

X\Cj

Q(x|cj ) log Q(x|cj ).

This is a variation of a well known form of H(Q) which is derived by splitting
summation
P
over X into summation over Cj and X \ Cj , and using the fact that Q(cj ) X\Cj Q(x|cj ) =
Q(cj ) which holds for every distribution. To split the sum over X \ Cj for Q(cj ) > 0 we use
1 Q
X
k Φk (ck )
ZQ
=
log Φk (ck ) − log Q(cj ) − log(ZQ )
log Q(x|cj ) = log
Q(cj )
k

Thus,
X

Q(x|cj ) log Q(x|cj ) =

X\Cj

P P

P
Q(ck |cj )Q(x|ck , cj ) log Φk (ck ) − X\Cj Q(x|cj ) [log Q(cj ) + log(ZQ )]
P
and by using Q(ck , cj ) X\{Ck ∪Cj } Q(x|ck , cj ) = Q(ck , cj ) this term is further rewritten as
X
H(Q) = −
Q(cj ) log Q(cj )
k

X\Cj

Cj

−

P

Cj Q(cj ) ·

hP

k6=j

P

Ck \Cj Q(ck |cj ) log Φk (ck ) + log

Φj (cj )
Q(cj )

i

+ log(ZQ )

Note that when Q(cj ) = 0 the bracketed term is multiplied by zero, and due to the equality
0 log 0 = 0, the product is also zero.

The second term of Eq. 10 is similarly written as
XX
X
EQ [log P (x)] =
Q(cj )
Q(x|cj ) log Ψi (di ) − log(ZP )
i

=

X

Q(cj )

Cj

X X
i

Cj

(11)

X\Cj

Q(di |cj ) log Ψi (di ) − log(ZP )

Di \Cj

Hence Eq. 10 is rewritten as
X
D(Q || P ) =
Q(cj ) log Q(cj )
Cj


−

X
Cj

Q(cj ) −


X X

Q(ck |cj ) log Φk (ck ) +

X X
i

k Ck \Cj

11

Di \Cj

Q(di |cj ) log Ψi (di )

Geiger, Meek & Wexler

−

X
Cj

Q(cj ) log

Q(cj )
+ log(ZP ) − log(ZQ )
Φj (cj )

Denoting the bracketed term by υj (cj ), and letting Υj (cj ) = eυj (cj ) , we get
D(Q || P ) =

X
Cj

Q(cj ) log

Φj (cj )
+ log(ZP ) − log(ZQ ).
Υj (cj )



P
The next lemma shows that υj (cj ), defined in Eq. 9, and γj (cj ) = l γjl (cjl ), used to
update potentials of Q in vip? , differ only by an additive constant which does not depend
on cj . As argued in Theorem 1, the fact that this difference is a constant enables vip? to
use the latter form, which is a more efficient representation.
Q
Q
Q
Lemma 3 Let P (x) = Z1P i Ψi (di ) and Q(x) = Z1Q j Φj (cj ) where Φj (cj ) = l Φjl (cjl ),
be two probability distributions such that Q is self-compatible and compatible wrt P . Let
X
X
X
X
γjl (cjl ) = −
Q(ck |cjl ) log Φk (ck ) +
Q(di |cjl ) log Ψi (di ) (12)
{k:gjk (l)=1} Ck \Cjl

{i:fij (l)=1} Di \Cjl

Then, the difference between υj (cj ) defined by Eq. 9 and γj (cj ) =
that does not depend on cj .

P

l

γjl (cjl ) is a constant

P
Proof: We first argue that each term of the form Ck \Cj Q(ck |cj ) log Φk (ck ) and each term
P
of the form Di \Cj Q(di |cj ) log Ψi (di ) in Eq. 9 that depends on cj appears exactly once for
a single subset Cjl in Eq. 12. We then argue that every term in Eq. 12 appears once in
Eq. 9.
Since Q is self-compatible, it follows that for every cluster Ck that depends on Cj the
function gkj (l) equalsPone for a single subset Cjl , namely Q(ck |cj ) = Q(ck |cjl ), in which
case the expression
Ck \Cj Q(ck |cjl ) log Φk (ck ) appears in Eq. 12. Similarly, since Q is
compatible wrt P it follows that for every set Di that depends on Cj the function fij (l)
equals
one for a single subset Cjl , namely Q(di |cj ) = Q(di |cjl ), in which case the expression
P
Di \Cj Q(di |cjl ) log Ψi (di ) appears in the second term of Eq. 12.
It remains to show that every term in Eq. 12 appears once in Eq. 9. Since Q is selfcompatible, it is implied that Ck ∩ Cj ⊆ Cjl and thus summing over Ck \ Cj is equivalent
to summing over Ck \ Cjl . Therefore, for every k such that gjk (l) = 1, the first term of
Eq. 12 appears once in Eq. 9. Similarly, since Q is compatible wrt P , it is implied that
Di ∩ Cj ⊆ Cjl and thus summing over Di \ Cj is equivalent to summing over Di \ Cjl and
therefore for every i such that fij (l) = 1 the second term of Eq. 12 appears once in Eq. 9.


Theorem 1 (Convergence of vip? ) Let the initial approximating distribution Q be selfcompatible and compatible wrt a given distribution P , and assume that (P (x) = 0) ⇒
(Q(x) = 0). Then, the revised distribution Q retains these properties, and in each iteration
of Algorithm vip? the KL divergence between Q and P decreases unless Q is a stationary
point.
12

A Variational Inference Procedure

Q
Proof. Let Q(x) = Z1Q jl Φjl (cjl ) where Φjl (cjl ) = eγjl (cjl ) . We need to show that
0
?
at the start
Q of0 each iteration of vip the function Q defined by the revised potentials
0
Φj (cj ) = l Φjl (cjl ) is a probability distribution with the listed properties and that it is
closer to P in KL divergence than Q at the start of the previous iteration.
First, we show that Q0 maintains the properties listed in the theorem throughout the
updates done in vip? . The properties of self-compatibility and compatibility wrt P are
derived from the form of Q and thus are not affected by the updates done in vip? . For
the property
(P (x) = 0) ⇒ (Q(x) = 0), consider an instance x for which P (x) = 0. Since
1 Q
P (x) = ZP i Ψi (di ) there exists a potential Ψi of P for which Ψi (di ) = 0, where di is the
Q
projection of x on the set Di . Since Q(x) = 0 and Q(x) = Z1Q jl Φjl (cjl ) there exists a
subset Cjl for which Q(cjl ) = 0, where cjl is the projection of x on Cjl . Algorithm vip?
1
by convention,
updates γjl (cjl ) to −∞ because log Ψi (di ) = −∞ and Q(di |cjl ) = |Di \C
jl |
0
yielding Q (x) = 0, as claimed.
Now, we show that no additional zeroes are introduced into Q whenever (P (x) = 0) ⇒
(Q(x) = 0). Hence, the normalizing constant ZQ > 0 and therefore the revised Q0 is a
probability distribution. For instances cjl for which Q(cjl ) > 0 the terms Q(ck |cjl ) log Φ(ck )
and Q(di |cjl ) log Ψ(di ) are finite as long as (P (x) = 0) ⇒ (Q(x) = 0). This implies Φjl (cjl )
is updated to a positive value and thus, no additional zeroes are introduced into Q.
Using the given form of Q, we have


1 X Y
Q(cj ) =
Φk (ck ) Φj (cj ).
(13)
ZQ
X\Cj k6=j

We denote the bracketed coefficient of Φj (cj ) by Bj (cj ) and note that it is constant in the
sense that it does not depend on the quantity Φj being optimized.
We now use Eq. 13 to rewrite the KL divergence as justified by Eq. 8 in Lemma 2:


Φj (cj )Bj (cj ) 
1 X
D(Q || P ) =
Φj (cj )Bj (cj ) log
+ log(ZP ) − log(ZQ ).
(14)
ZQ
Υj (cj )Bj (cj )
Cj

Due to Lemma 3, the distance D(Q || P
Υj (cj )
P) only changes by a constant when replacing
γ
(c
)
?
j
j
with Γj (cj ) = e
, where γj (cj ) = l γjl (cjl ) as computed via Eq. 6 of vip . Note that
Γj (cj ) does not depend on Φ(cj ) and is a function of Q(x) only through the conditional
distribution of X \ Cj given Cj (via Q(ck |cj )). Hence, Lemma 1 states that the minimum
of D(Q || P ) wrt Φj is achieved when Φj (cj ) is proportional to Γj (cj ). As the potential Φj
is only held implicitly through the partial potentials Φjl , step 1 of vip? updates Φj (cj ) via
Eq. 6 to be proportional to Γj (cj ) by setting each potential Φjl (cjl ) to be proportional to
γjl (cjl ). The proportionality constant does not matter because if Φj is multiplied by α, and
the arbitrary constraining constant ZQ is also multiplied by α, these influences cancel in
Eq. 14. For simplicity, the algorithm uses α = 1 and therefore Φj (cj ) ← eγj (cj ) . Algorithm
vip? implicitly computes Φj (cj ) according to this formula and hence decreases D(Q || P ) at
each iteration by improving Φj (cj ) while holding all other cluster potentials fixed. Since
the KL divergence is lower bounded by zero, vip? converges. 

13

Geiger, Meek & Wexler

The properties of Q required by Theorem 1 of self-compatibility and compatibility wrt
P are derived from the form of Q and are satisfied by setting the clusters Cj appropriately.
In addition, the condition (P (x) = 0) ⇒ (Q(x) = 0) is trivially satisfied for strictly positive
distributions P .
Note that the difference between υj (cj ) defined by Eq. 9 and γj (cj ) defined by Eq. 1 is
a constant that does not depend on cj . Consequently, our convergence proof also applies to
Wiegerinck’s algorithm, because this algorithm is a special case of vip? where every cluster
Cj has a single subset Cj1 = Cj .

5. Handling Deterministic Potentials
When the distribution P is not strictly positive the property (P (x) = 0) ⇒ (Q(x) = 0)
must hold for the convergence proof of vip? to apply. In this section we provide a sufficient
condition for Q to retain this property.
Definition: An instantiation W = w is feasible (wrt to a distribution P ) if P (W = w) > 0.
Otherwise, the instantiation is infeasible.
Q
Definition: A constraining set wrt a distribution P (x) = Z1P i Ψi (di ) with sets Di is a
minimal set of variables Λ ⊆ Di which has an infeasible instantiation λ.
Q
Definition: A distribution Q(x) = Z1Q j Φj (cj ) with clusters Cj is containable wrt a
Q
distribution P (x) = Z1P i Ψi (di ), if for every constraining set Λ of P there exists at least
one cluster Cj such that Λ ⊆ Cj .
Q
Q
Theorem 2 Let P (x) = Z1P i Ψi (di ) and Q(x) = Z1Q j Φj (cj ) be two distributions where
Q
Φj (cj ) = l Φjl (cjl ) and where Q is containable and compatible wrt P and strictly positive. Then, after vip? iterates once over all clusters Cj , the revised distribution Q satisfies
(P (x) = 0) ⇒ (Q(x) = 0).
Proof: From the definition of a constraining set, for every infeasible instantiation x, there
exists an infeasible instantiation λ of a constraining set Λ such that λ is a projection of x
on Λ. We show that vip? updates Q(x) = 0 for such instantiations. Since Q is containable
wrt P there exists a cluster Cj which contains Λ. Furthermore, since Λ ⊆ Di where Di is
a set of P and since Q is compatible wrt P , there exists a subset Cjl that contains Λ. For
every instantiation cjl which is a projection of λ on Cjl the expression γjl (cjl ) is updated
to −∞ according Q
to Eq. 6 of vip? . This is true because Q(di |cjl ) > 0 and log Ψ(di ) = −∞.
1
Since Q(x) = ZQ j,l Φjl (cjl ) this update implies Q(x) = 0.
Whenever the first two compatibility conditions of Theorem 1 hold, it follows that vip?
converges for containable distributions. Note that since every iteration of vip? decreases the
KL divergence, following iterations can not change Q to be greater than zero for instantiation
which are infeasible wrt P , as this leads to an infinite distance. However, containability
implies a stronger property stated in the next theorem.

14

A Variational Inference Procedure

Q
Q
Theorem 3 Let P (x) = Z1P i Ψi (di ) and Q(x) = Z1Q j Φj (cj ) be two distributions where
Q
Φj (cj ) = l Φjl (cjl ) and where Q is containable wrt P and (P (x) = 0) ⇒ (Q(x) = 0). Then,
after vip? iterates once over all clusters Cj , the revised distribution Q satisfies (Q(x) = 0) ⇒
(P (x) = 0).
Proof: Consider an instantiation x for which P (x)
> 0. We show that Eq. 6 of vip? upQ
dates Q(x) to a positive value. Since Q(x) = Z1Q j,l Φjl (cjl ), it is sufficient to show that
the revised potential Φjl (cjl ) is positive for each subset Cjl and an instance cjl which is
the projection of x on the Cjl . For such instances cjl the value γjl (cjl ) is set by Eq. 6 to
a finite value since Ψi (di ) > 0 for every instance di which is a projection of x on a set Di ,
and Φk (ck ) = 0 implies Q(ck |cjl ) = 0. Therefore, Φjl (cjl ) = eγjl (cjl ) > 0 and Q(x) > 0.
The consequence of Theorem 3 is that Q(x) = 0 iff P (x) = 0. Conditions weaker than
containability may be sufficient to ensure the requirement needed for convergence, however,
containability is easily satisfiable in applications of variational techniques as explicated in
the next section.

6. Genetic Linkage Analysis via Variational Algorithms
Genetic linkage analysis takes as input a family pedigree in which some individuals are
affected with a genetic disease, affection status of members of the pedigree, marker readings
across the genome, and mode of inheritance. The output is the likelihood of data as a function of the location of a disease gene and the given pedigree. Locations yielding maximum
or close to maximum likelihood are singled out as suspect regions for further scrutiny. The
exact computation of this likelihood is often too complex and approximations are needed.
Algorithm vip? has been developed to facilitate such likelihood computations. In particular, vip? allows there to be overlapping clusters which minimizes the loss of valuable
information and, more importantly, can handle the deterministic constraints that are common in these models. In this section, we describe the standard probabilistic model for
genetic linkage, several approximate distributions that we use when applying vip? to the
genetic linkage model, and demonstrate vip? on a real-world data set of a large pedigree
with 115 individuals.
The standard probabilistic model for genetic linkage is based on a pedigree which contains several variables for each person at each location and conditional probability tables
for each variable Xm given a set of variables called parents of Xm and denoted by π(Xm ).
The distribution P (x) that represents the joint distribution of the variables in the pedigree
is written using multiple indices; one set of indices for persons (i), one for loci (j), and
another for the type of variable (t) as follows:
P (x) =

YY
j

i,t
P (xi,t
j |π(xj )) =

Y

i t∈{ps,ms,pg,mg,f }

1 YY
ZP
j

Y

i t∈{ps,ms,pg,mg,f }

15

i,t
i,t
Ψi,t
j (xj |π(xj ))

(15)

Geiger, Meek & Wexler

where the five possible types of variables are: paternal selector (ps), maternal selector (ms),
paternal genotype (pg), maternal genotype (mg) and phenotype (f). Thus, the set Dji,t
equals {Xji,t , π(Xji,t )}.
We denote the variables of the different types, ps, ms, pg, mg and f, of individual i at
i,m
locus j by Sji,p , Sji,m , Gi,p
and Fji respectively. In this notation the possible potentials
j , Gj
a,p
a,m
i,p
i,m
b,p
b,m
i,m
i,p
i,p
i,m
i,m
of P are Ψ(Gi,p
j , Gj , Gj , Sj ), Ψ(Gj , Gj , Gj , Sj ), Ψ(Sj , Sj−1 ), Ψ(Sj , Sj−1 ) and
i,m
Ψ(Fji , Gi,p
j , Gj ) where a and b are i’s father and mother in the pedigree, respectively. Some
i,p
a,p
a,m
i,ms
i,m
exemplifying sets are Dji,pg = {Gi,p
= {Sji,m , Sj−1
}, and Dji,f =
j , Sj , Gj , Gj }, Dj
i,m
{Fji , Gi,p
j , Gj }, where a is the father of individual i in the pedigree. We note that the first
two types of potentials and possibly the last one are deterministic potentials which equal
zero for some instantiations.
A directed
Q acyclic graph along with a probability distribution R that factors according
to R(Z) = i R(zi |π(zi )) is called a Bayesian network. A Bayesian network defined by
Eq. 15, that describes parents-offspring interaction in a simple genetic analysis problem
with two siblings and their parents across 3 loci, is given in Figure 4. The dashed boxes
contain all variables that describe the variables in a single location. In this example we
assume that each phenotype variable depends on the genotype at a single locus. This is
reflected by the fact that only edges from a single locus point into each phenotype variable.
The full semantics of all variables and more details regarding the conditional probability
tables can be found in the paper by Fishelson & Geiger (2002); these details are not needed
here.
We use several choices to cluster the Bayesian network for P (x) such that Q is selfcompatible and compatible wrt P . In addition, since some potentials in P are constrained
(e.g. Ψi,pg
j ), we choose clusters such that Q is containable wrt P . According to Theorem 2
this choice ensures that Q satisfies all conditions necessary for convergence of vip? , and in
particular (P (x) = 0) ⇒ (Q(x) = 0).
Consider a partition of the network into slots, each containing a set of consecutive loci.
In the most simple case every slot is a single locus and each of the subsets Cji contains
variables related to one individual i and the genotypes of his parents in that slot. We set
S
ρ(i)
Cji = {Gj , Sji } and Cj = i {Cji } where ρ(i) denotes the union of i and his parents.
An illustration of such a setting in a pedigree of two siblings and their parents over three
loci is given in Figure 5. In this setting, self-compatibility is trivially satisfied because the
clusters Cj of Q are disjoint, and Q is containable wrt P since only sets Dji,ps and Dji,ms , the
potentials of which are not constrained, span across more than a single locus. It remains to
show that compatibility of Q wrt P is satisfied. For sets Dji,t contained in a single subset

Cji this is trivial as Q(Dji,t |cj ) = Q(Dji,t |cji ) = 1. Otherwise, t equals ps or ms and without
i,p
i,p
i,p
i,p
loss of generality, Q(Sj−1
, Sji,p |cj ) = Q(Sj−1
|cj ) = Q(Sj−1
|cji ) = Q(Sj−1
, Sji,p |cji ).
In a more complex setup, which is similar to Example 1, we add a cluster CJ+1 which
cuts across the loci for selector variables of an individual r, as shown in Figure 6. The
S
ρ(i)
subset Cji are set to Cji = {GjS , Sji , Sjr } and the clusters are Cj = i {Cji }, for j = 1 . . . J.
r
In addition, we set CJ+1 = l {CJ+1,l } and the subsets CJ+1,l = {Sl,l+1
}, for a single
chosen individual r. We verify that Q still satisfies the conditions of Theorem 1. Selfcompatibility is maintained since Q(Cj |cJ+1 ) = Q(Cj |cJ+1,j ), Q(CJ+1 |cj ) = Q(CJ+1 |cjr ),

16

A Variational Inference Procedure

Figure 4: A Bayesian network representation of a pedigree of two siblings and their parents
in a 3-loci model. The circles, squares and diamond shapes represent genotype,
phenotype and selector variables respectively.

and Q(Ck |cj ) = Q(Ck |cjr ) for every two clusters Cj , Ck such that j, k ≤ J. Sets Dji,t
such that t 6= ms, ps are contained in cluster Cj and thus maintain independence given a
subset. For t = ms, ps the sets Dji,t that connect two adjacent loci are independent of CJ+1
given CJ+1,j , and are independent of other clusters Cj given the subset Cji , maintaining
compatibility of Q wrt P . Finally, Q is containable wrt P since all clusters from the previous
option remain.
Immediate extensions of the above clustering schemes allow every slot to contain several
consecutive loci and a set of possibly more than one individual R to cut across the loci. To
maintain the compatibility of Q wrt P in the latter extension, the subsets CjR are set to
ρ(R)
CjR = {Gj , SjR }, where ρ(R) denotes the union of individuals in R and their parents.
We now describe the experiments performed using a large pedigree with 115 individuals
spanning across 21 locations, which was studied by Narkis et al. (2004) to locate an area
that contains a gene that causes a fatal neurological disorder (LCCS type 2). First, the
proximity of the disease gene to each of the markers was tested through two-point analysis
- a model-selection method in which only two loci are considered simultaneously, with one
of them being the disease locus. In this method, loci which yield likelihood maxima suggest
probable locations of the disease locus. Two-point analysis of the abovementioned pedigree
took several days using the exact inference software superlink v1.5 designed for genetic
linkage analysis.
17

Geiger, Meek & Wexler

Figure 5: A schematic division of a pedigree into clusters, where each locus is a cluster.
In cluster C3 the variables of every individual are in a separate ellipse and the
number of that individual is written. In the other two clusters the marked areas
are C14 and C23 .

The log-likelihood probabilities obtained by vip? using the abovementioned extended
clustering scheme with 5 clusters across all loci and with no cluster cutting across the
loci, are shown in Figure 7. The figure shows clearly that the exact and approximate loglikelihood curves have a similar shape and almost identical extremum points, but have a
major difference in absolute value.
Next, we tested vip? on three-point analysis problems on the same pedigree, where
two markers are considered simultaneously along with one disease locus. We note that
exact inference for this task on the specified pedigree is hard on a single PC, taking several
weeks. Since the exact location of the disease gene is known with high probability, we
wished to test whether or not the lower-bounds found by vip? indicate this location. We
considered two nearby markers (number 4 and 6) and seven models which differ by the
location of the speculated disease gene: in the first two, the disease locus was positioned
left to marker 4 at distances 0.01 and 0.02 centi-Morgan (cM), and in the remaining five it
was positioned to the right of marker 4 at distances 0.01 to 0.05 cM with 0.01 cM difference
between locations. The location of the disease gene is 0.01cM to the right of marker 4. The
algorithm was run three times on each model with random initial potentials, taking into
account only the maximum value obtained. The results of this test are plotted in Figure 8
versus the approximation found by the sampling software simwalk2 introduced by Sobel,
Papp & Lange (2002) which is designed for approximate pedigree analysis. As shown, the
probabilities found by vip? are higher as we approach the location of the disease gene. The
same in not true for the probabilities found by simwalk2. However, we note that vip? is
18

A Variational Inference Procedure

ln-likelihood

Figure 6: A pedigree partitioned into clusters, where each locus is a cluster and an additional cluster C4 , in the striped area, contains the variables of one of the individuals.

0
-20
-40
-60
-80
-100
-120
-140
-160
-180
-200
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21

Exact

locus

VIP* - 5 clusters across all loci
VIP* - disjoint clusters

Figure 7: Approximations to the log-likelihood of two-point analysis using vip? .

much slower on this problem than simwalk2, taking several hours for each run. In addition,
19

Geiger, Meek & Wexler

we note that the ln-likelihood probabilities in Figures 8(a) and (b) are drawn on different
scales due to the markedly different output of the two methods.

-182

-77
-78

-186

ln-likelihood

ln-likelihood

-184

-188
-190
-192
-194

-79
-80
-81
-82

-0.02

-0.01

0.01

0.02

0.03

0.04

0.05

-0.02

location (relative to marker 4)

-0.01

0.01

0.02

0.03

0.04

0.05

location (relative to marker 4)

(a) using vip?

(b) using simwalk2

Figure 8: Approximations to the log-likelihood of three-point analysis.
We compared the convergence times of vip? and Wiegerinck’s algorithm on various size
problems of genetic linkage analysis. The original network on which we performed the test
includes 332 variables and represents a pedigree with 28 individuals over four loci. To create
various sized problems, subsets of the original pedigree with increasing number of individuals
were considered. In addition, for a fair comparison, the clusters of Wiegerinck’s algorithm
were chosen to be a subset of the subsets used by vip? . Since the number of iterations until
convergence did not vary significantly between the two algorithms, we report the ratio of
iteration times which also tests the theoretical speedup predicted in Section 3 of vip? over
Wiegerinck’s algorithm. Figure 9 illustrates the ratio of time for an update iteration of the
two algorithms, where it is evident that the ratio increases linearly with the problem size.
The ratios indicated are averaged over 5 runs each with 10 iterations for every problem size.
Finally we examine the convergence of vip? in Figure 10 using six representatives of
the original 21 two-point analysis runs described earlier, each on a different network. The
algorithm is halted when the change in the lower-bound is smaller than 10−5 for more than
3 iterations. Although not all runs converge at the same rate, it seems that they obey a
certain pattern of convergence where the first few iterations show significant improvements
in the lower-bound, followed by slow convergence to a local maximum, and then another
moderate improvement to a better maximum point.

7. Discussion
In this paper we present an efficient algorithm called vip? for structured variational approximate inference. This algorithm, which extends known algorithms, can handle overlapping
clusters and overcome the difficulties imposed by deterministic constraints. We show that
for N × N grid-like models, algorithm vip? is N fold faster than Wiegerinck’s algorithm,
20

A Variational Inference Procedure

20
18

ratio of iteration times

16
14
12
10
8
6
4
2
0
10

12

14

16

18

20

22

24

26

28

number of individuals

Figure 9: Speedup of vip? over Wiegerinck’s algorithm.

-70
-90

ln-likelihood

-110
-130
-150
-170
-190
-210
-230
1

3

5

7

9

11

13

15

17

19

21

23

iteration

Figure 10: Convergence of vip? for 6 runs of two-point analysis.
when a junction tree is used. In addition, we prove the convergence of vip? and of previous
variational methods via a novel proof method, using properties of the KL divergence.
Finally, algorithm vip? is tested on Bayesian networks that model genetic linkage analysis problems. These graphs resemble grid-like models and are notoriously difficult to
approximate due to the numerous deterministic constraints. The results show a linear
improvement in speed of vip? versus Wiegerinck’s algorithm, and that the approximation
21

Geiger, Meek & Wexler

follows the shape of the real likelihood probabilities. Nevertheless, Figure 7 shows that variational methods such as Wiegerinck’s algorithm or vip? are still not appropriate to produce
accurate approximation of the likelihood for genetic linkage analysis.

Acknowledgments
This paper is an extension of a paper that originally appeared at the 10th workshop on
Artificial Intelligence and Statistics (Geiger & Meek 2005). We thank D. Heckerman, N.
Jojic and V. Jojic for helpful discussions. We also thank the two anonymous reviewers
for correcting several errors that appeared in the early version as well as improving the
presentation. Part of the work was done while the first author was a visitor at Microsoft
Research. This work is supported by the Israeli Science Foundation and the Israeli Science
Ministry.

References
Bishop, C. & Winn, J. (2003). Structured variational distributions in VIBES. In Artificial
Intelligence and Statistics. Society for Artificial Intelligence and Statistics.
Cooper, G. (1990). Probabilistic inference using belief networks is NP-hard. Artificial
Intelligence, 42, 393–405.
Cover, T. M. & Thomas, J. A. (1991). Elements of Information Theory. Wiley.
Dagum, P. & Luby, M. (1993). Approximating probabilistic inference in Bayesian belief
networks is NP-hard. Artificial Intelligence, 60 (1), 141–153.
Dechter, R. (1999). Bucket elimination: A unifying framework for reasoning. Artificial
Intelligence, 113 (1-2), 41–85.
Fishelson, M. & Geiger, D. (2002). Exact genetic linkage computations for general pedigrees.
Bioinformatics, 18, S189–S198.
Geiger, D. & Meek, C. (2005). Structured variational inference procedures and their realizations. In Proceedings of Tenth International Workshop on Artificial Intelligence
and Statistics, The Barbados. The Society for Artificial Intelligence and Statistics.
Ghahramani, Z. & Jordan, M. I. (1997). Factorial hidden Markov models. Machine Learning, 29, 245–273.
Jensen, F. V. (1996). An Introduction to Bayesian Networks. Springer.
Jojic, V., Jojic, N., Meek, C., Geiger, D., Siepel, A., Haussler, D., & Heckerman, D.
(2004). Efficient approximations for learning phylogenetic HMM models from data.
Bioinformatics, 20, 161–168.
Kschischang, F. R., Frey, B. J., & Loeliger, H. A. (2001). Factor graphs and the sum-product
algorithm. IEEE Transactions on information theory, 47 (2), 498–519.
22

A Variational Inference Procedure

Narkis, G., Landau, D., Manor, E., Elbedour, K., Tzemach, A., Fishelson, M., Geiger, D.,
Ofir, R., Carmi, R., & Birk, O. (2004). Homozygosity mapping of lethal congenital
contractural syndrome type 2 (LCCS2) to a 6 cM interval on chromosome 12q13.
American Journal of Medical Genetics, 130 (3), 272–276.
Saul, L. & Jordan, M. I. (1996). Exploiting tractable substructures in intractable networks.
In Advances in Neural Information Processing Systems (NIPS). MIT Press.
Sobel, E., Papp, J., & Lange, K. (2002). Detection and integration of genotyping errors in
statistical genetics. American Journal of Human Genetics, 70, 496–508.
Wiegerinck, W. (2000). Variational approximations between mean field theory and the junction tree algorithm. In Uncertainty in Artificial Intelligence, (pp. 626–633). Morgan
Kaufmann.
Xing, E. P., Jordan, M. I., & Russell, S. (2003). A generalized mean field algorithm for
variational inference in exponential families. In Uncertainty in Artificial Intelligence,
(pp. 583–591). Morgan Kaufmann.
Xing, E. P., Jordan, M. I., & Russell, S. (2004). Graph partition strategies for generalized
mean field inference. In Uncertainty in Artificial Intelligence, (pp. 602 – 610). Morgan
Kaufmann.

23

Journal of Artificial Intelligence Research 27 (2006) 551-575

Submitted 03/06; published 12/06

Causes of Ineradicable Spurious Predictions in Qualitative Simulation
Özgür Yılmaz
A. C. Cem Say
Department of Computer Engineering
Boğaziçi University
Bebek 34342 Đstanbul, Turkey

YILMOZGU@BOUN.EDU.TR
SAY@BOUN.EDU.TR

Abstract
It was recently proved that a sound and complete qualitative simulator does not exist, that is,
as long as the input-output vocabulary of the state-of-the-art QSIM algorithm is used, there will
always be input models which cause any simulator with a coverage guarantee to make spurious
predictions in its output. In this paper, we examine whether a meaningfully expressive restriction
of this vocabulary is possible so that one can build a simulator with both the soundness and
completeness properties. We prove several negative results: All sound qualitative simulators,
employing subsets of the QSIM representation which retain the operating region transition feature,
and support at least the addition and constancy constraints, are shown to be inherently incomplete.
Even when the simulations are restricted to run in a single operating region, a constraint
vocabulary containing just the addition, constancy, derivative, and multiplication relations makes
the construction of sound and complete qualitative simulators impossible.

1. Introduction
It was recently proved (Say & Akın, 2003) that a sound and complete qualitative simulator does
not exist, that is, as long as the input-output vocabulary of the state-of-the-art QSIM algorithm
(Kuipers, 1994) is used, there will always be input models which cause any simulator with a
coverage guarantee to make spurious predictions in its output. In this paper, we examine whether
a meaningfully expressive restriction of this vocabulary is possible so that one can build a
simulator which will always output all and only the consistent solutions of its input model. We
prove several negative results: All sound qualitative simulators, employing subsets of the QSIM
representation which retain the operating region transition feature, and support at least the
addition and constancy constraints, are shown to be inherently incomplete. The problem persists
when all variables are forced to change continuously during region transitions if a slightly larger
set of constraint types is allowed. Even when the simulations are restricted to run in a single
operating region, a constraint vocabulary containing just the addition, constancy, derivative, and
multiplication relations makes the construction of sound and complete qualitative simulators
impossible. Our findings may be helpful for researchers interested in constructing qualitative
simulators with improved theoretical coverage guarantees using weaker representations.

2. Background
We start with a brief overview of qualitative simulation, concentrating on the representations used
in the input-output vocabularies of qualitative simulators. Subsection 2.2 summarizes previous
work on the two theoretical properties of qualitative simulators that interest us. Subsection 2.3 is
a short “requirements specification” for a hypothetical sound and complete qualitative simulator.

©2006 AI Access Foundation. All rights reserved.

YILMAZ & SAY

2.1 Qualitative Simulation
In many domains, scientists and engineers have only an incomplete amount of information about
the model governing the dynamic system under consideration, which renders formulating an
exact ordinary differential equation (ODE) impossible. Incompletely specified differential
equations may also appear in contexts where the aim is to find collective proofs for behavioral
properties of an infinite set of systems sharing most, but not all, of the structure of the ODEs
describing them. To proceed with the reasoning task in such cases, mathematical tools embodying
methods making the most use of the available information to obtain a (hopefully small) set of
possible solutions matching the model are needed. Qualitative reasoning (QR) researchers
develop AI programs which use “weak” representations (like intervals rather than point values for
quantities, and general shape descriptions rather than exact formulae for functional relationships)
in their vocabularies to perform various reasoning tasks about systems with incomplete
specifications. In the following, we use the notation and terminology of QSIM (Kuipers, 1994),
which is a state-of-the art qualitative simulation methodology, although it should be noted that the
incompleteness results that we will be proving are valid for all reasoners whose input-output
vocabularies are rich enough to support the representational techniques that will be used in our
proofs.
A qualitative simulator takes as input a qualitative differential equation model of a system in
terms of constraints representing relations between the system’s variables. In addition to this
model, the qualitative values of the variables at the time point from which the simulation should
start are also given. The algorithm produces a list of the possible future behaviors that may be
exhibited by systems whose ordinary differential equations match the input model.
The variables of a system modeled in QSIM are continuously differentiable functions of time.
The limits of each variable and their first derivatives exist as they approach the endpoints of their
domains. Each variable has a quantity space; a totally ordered collection of symbols (landmarks)
representing important values that it can take. Zero is a standard landmark common to all
variables. Quantity spaces are allowed to have the landmarks -∞ and ∞ at their ends, so functional
relationships with asymptotic shapes can be explicitly represented. When appropriate, a quantity
space can be declared to span only a proper subset of the extended reals; for instance, it makes
sense to bound the quantity space for a variable which is certainly nonnegative (like pressure)
with 0 at the left. If necessary, the user can specify one or both bounds of a quantity space to be
unreachable; for example, ∞ will be an unreachable value for all variables in all the models to be
discussed in this paper. The (reachable) points and intervals in its quantity space make up the set
of possible qualitative magnitudes of a variable. The qualitative direction of a variable is defined
to be the sign of its derivative; therefore its possible values are: inc (+), dec (−) and std (0). A
variable’s qualitative value is the pair consisting of its qualitative magnitude and qualitative
direction. The collection of the qualitative values of its variables makes up the state of a system.
The “laws” according to which the system operates are represented by constraints describing
time-independent relations between the variables. At each step of the simulation, QSIM uses a set
of transition rules to implicitly generate all possible “next” values of the variables. The
combinations of these values are filtered so that only those which constitute complete states, in
which every constraint is still satisfied by the new values of its variables, remain.
There are seven “basic” types of constraints in QSIM. (See Table 1.) Each type of constraint
imposes a different kind of relation on its arguments. For example, if we have the constraint
A(t) = −B(t), any combination of variable values in which variables A and B have the same
(nonzero) sign in their magnitudes or directions will be filtered out. Sometimes, additional
knowledge about the constraints allows further filtering. In the above example, if we know that A
and B had the landmark values a1 and b1 at the same moment at some time in the past, we can
552

CAUSES OF INERADICABLE SPURIOUS PREDICTIONS IN QUALITATIVE SIMULATION

eliminate all value combinations in which A and B have magnitudes both less (or both greater)
than these landmarks. a1 and b1 are called corresponding values of that constraint, and the
equation a1 = −b1 is a correspondence. Each constraint in a model (except those of the derivative
type) can have such correspondence equations. A “sign algebra” (Kuipers, 1994) is employed to
implement the arithmetic relations using qualitative magnitudes. Note that, since each M + ( M − )
relationship corresponds to an infinite number of possible “quantitative” functions having the
monotonicity property, a single QSIM model containing such constraints can correspond to
infinitely many ODEs.
CONSTRAINT NAME
add

NOTATION
X(t) + Y(t) = Z(t)

constant

X(t) = a landmark

derivative

d/dt(X,Y)

M+

X(t) = f(Y(t)), f ∈ M +

M−

X(t) = f(Y(t)), f ∈ M −

minus
mult

X(t) = −Y(t)
X(t) × Y(t) = Z(t)

EXPLANATION
d
X(t) = 0
dt
d
X(t) = Y(t)
dt
∃f such that X(t) = f(Y(t)), where f ′ > 0
over f ’s domain
∃f such that X(t) = f(Y(t)), where f ′ < 0
over f ’s domain

Table 1: The Qualitative Constraint Types
The QSIM input vocabulary enables the user to describe more complicated models in terms of
several different constraint sets representing different operating regions of the system under
consideration. The user specifies the boundaries of the applicability ranges of the operating
regions in terms of conditions which indicate that the simulator should effect a transition to
another operating region when they are obtained.
For each operating region from which such a transition can occur, one has to specify the
following for each possible transition:
• Boolean expressions composed of primitives of the form “VariableName=QualitativeValue”,
which will trigger this transition when they are satisfied,
• The name of the target operating region,
• The names of variables which will inherit their qualitative magnitudes and/or directions in the
first state after the transition from the last state before the transition,
• Value assignments for any variables which will have explicitly specified values in the first state
after the transition.
When provided with a qualitative system model, the name of the initial operating region, and a
description of the qualitative values of all variables in the initial state, QSIM starts simulation,
and generates a tree of system states to represent the solutions of the qualitative differential
equation composed of the constraints in its input. The root of this tree is the input initial state with
the time-point label t0, representing the numerical value of the initial instant. Every path from the
553

YILMAZ & SAY

root to a leaf is a predicted behavior of the system. Being in the qualitative format, each such
behavior usually corresponds to an infinite set of trajectories sharing the same qualitative
structure in phase space. Time-point and interval states appear alternately in behaviors as long as
the same operating region is valid. Operating region transitions are reflected in behaviors as two
time-point states following each other.
2.2 Related Work on Soundness and Incompleteness
A very important property of qualitative simulators is their “coverage guarantee”: A qualitative
simulation algorithm is sound if it is guaranteed that, for any ODE and initial state that matches
the simulator’s input, there will be a behavior in its output which matches the ODE’s solution.
Kuipers (1986) proved that there exists a qualitative simulator (namely, QSIM) that has the
soundness property. This guarantee makes qualitative simulation a valuable design and diagnosis
method (Kuipers, 1994): In design, if the set of simulation predictions of our model does not
contain a catastrophic failure, this is a proof that our modeled system will not exhibit that failure
(Shults & Kuipers, 1997). In diagnosis, if none of the behaviors in the simulation output of a
model is being exhibited by a particular system, we can be 100% sure that the actual system is not
governed by that model.
Another property that one would wish one’s qualitative simulator to possess is completeness;
that is, a guarantee that every behavior in its output corresponds to the solution of at least one
ODE matching its input. In the early days of QR research, it was conjectured (de Kleer & Brown,
1984) that qualitative simulators employing local constraint satisfaction methods (Weld & de
Kleer, 1990) were complete. However, in the same paper which contained the guaranteed
coverage theorem, Kuipers (1986) also showed that the version of QSIM described there, and,
indeed, all qualitative simulators of the day, were incomplete, by demonstrating that the
simulation of a frictionless mass-spring oscillator predicts unrealizable (spurious) behaviors,
where the amplitude decreases in some periods and increases in others. The lack of a guarantee
that all the predicted behaviors are real has a negative impact on potential applications: In design,
if the set of simulation predictions of our model does contain a catastrophic failure, this does not
necessarily point to an error in our mechanism; maybe the prediction in question was just a
spurious behavior. A similar problem occurs in diagnosis applications.
Several other types of spurious qualitative simulation predictions were discovered in the
following years: Struss (1990) pointed out that, whenever a variable appeared more than once in
an arithmetic constraint, spurious states could pass the filter. For instance, the filters of the add
constraint are unable to delete states involving nonzero values for the variable Z in the equation
A(t) + Z(t) = A(t) when A is nonzero. Clearly, any sound and complete qualitative simulator
would have to possess the algebraic manipulation capabilities that enable us to conclude that Z = 0
in this case. Say and Kuru (1993) discovered a class of spurious predictions caused by a rigidity
in the internal representation of correspondences, and an unnecessarily weak implementation of
subtraction. Say (1998) showed that some other spurious behaviors are due to a lack of explicit
enforcement of l’Hôpital’s rule in the original algorithm. Yet another family of inconsistent
predictions was found out to be caused by weaknesses in the methods used to distinguish finite
and infinite time intervals in the behaviors (Say, 2001, also see Missier, 1991). Könik and Say
(2003) proved that some model and behavior descriptions could “encode” information about the
relative (finite) lengths of the intervals that they contain, and failure to check the overall
consistency of these pieces of information yields another class of spurious predictions. Finally,
Say (2003) showed that a similar encoding could occur about the exact numerical values of some
landmarks, so that a sound and complete qualitative simulator would have to support a capability

554

CAUSES OF INERADICABLE SPURIOUS PREDICTIONS IN QUALITATIVE SIMULATION

of comparing the magnitudes of any two elements of a very rich subset of the real numbers to
avoid a particular set of spurious predictions.
Interestingly, all these discoveries were actually good news for the users of qualitative
simulators: In order to be able to say that a particular predicted behavior is spurious, and therefore
suitable for elimination from the simulator output without forsaking the soundness property, one
first proves that that behavior is mathematically inconsistent with the simulated model and
starting state. For instance, the aforementioned spurious oscillations of the frictionless massspring system can be shown to violate a conservation constraint that follows directly from the
structure of the input equations. But this proof can itself be seen as the specification of a new
filter routine which would eliminate exactly the set of behaviors that violate the “law” that it
establishes. The kinetic energy constraint (Fouché & Kuipers, 1992) is a filter which has been
developed in this fashion to eliminate the class of spurious predictions exemplified by the ones
about the mass-spring system (Kuipers 1994). So all the spurious prediction classes mentioned in
the previous paragraph had, in fact, been discovered simultaneously with their “cures.”
The question of whether there exists a sound and complete qualitative simulator was finally
settled by Say and Akın (2003). They proved that, for any sound qualitative simulator using the
input-output representation and task specification of the QSIM methodology, there exist input
models and initial states whose simulation output will contain spurious predictions. (Note that this
does not just say that the present QSIM algorithm can not be augmented by more filters to make it
both sound and complete; it refutes the existence of any program whatsoever that can perform
this job.)
The proof by Say and Akın (2003) shows that a sound and complete qualitative simulator
employing the vocabulary mentioned above, if it existed, could be used to solve any given
instance of Hilbert’s Tenth Problem, which is famously undecidable (Matiyasevich, 1993). The
procedure involves building a QSIM model representing the given problem, simulating it several
times starting from carefully constructed initial states representing candidate solutions, and
examining the output to read out the solution. The model is set up to contain an inconsistency if
and only if the answer to the considered problem is “no,” so the very existence of one or more
behaviors in the output means “yes.” Since it is impossible to make this decision correctly in the
general case, it follows that there would be input models giving rise to behavior predictions
whose consistency status can not be determined by the simulator, whose best course of action
would be to include them in the output, to keep the soundness guarantee intact. In the cases where
the correct answer is “no,” this would result in the prediction of spurious behaviors. Note that
these are ineradicable spurious predictions, unlike the ones discussed earlier.
It is important to note that this proof does not necessarily mean that all hope of constructing a
sound and complete qualitative simulator is lost. One may try to “weaken” the input-output
representation so that it no longer possesses the problematic power which enables one to
unambiguously encode instances of Hilbert’s Tenth Problem into a QSIM model. (Of course, this
weakening must be kept at the minimum possible level for the resulting program to be a useful
reasoner; for instance, removing the program’s ability to distinguish between negative and
nonnegative numbers would possibly yield a sound and complete simulator, but the output of that
program would just state that “everything is possible” and this is not what we want from these
methods.) This is why one should examine the incompleteness proof (Say & Akın, 2003) to see
exactly which features of the QSIM representation are used in the construction of the reduction;
any future qualitative simulator supporting the same vocabulary subset would be incorporating
the same problem from the start.
Here is a listing of the QSIM representational items used in that proof: Only the M+,
derivative, mult, and constant constraint types are utilized. (Note the absence of the add
constraint, which can be “implemented” using the others, in this list.) Qualitative interval
555

YILMAZ & SAY

magnitudes like (0, ∞), with what one might call “infinite uncertainty” about the actual value of
the represented number, are used for initializing several variables, and form an essential part of
the argument. QSIM’s ability to explicitly represent infinite limits is utilized for equating a
landmark to the number π, by stating that it is twice the limit of the function arctan x as x nears
infinity. Finally, the operating region transition feature is used heavily, since it is thanks to this
characteristic that the sine function between two dependent variables can be represented in the
qualitative vocabulary.
In Section 3, we examine several different ways of weakening the QSIM vocabulary to try to
understand which combinations of these features are responsible for the problem of ineradicable
spurious predictions.
2.3 Desiderata for a Sound and Complete Qualitative Simulator
It is important at this point to clarify exactly what one would expect from a hypothetical sound
and complete qualitative simulator. If the input model yields a finite behavior tree of genuine
solutions, it is obvious that the program is supposed to print the descriptions of the behaviors
forming the branches of this tree, and nothing else, in finite time. If the input model and initial
state are inconsistent, i.e., the “correct” output is the empty tree, the program should report this
inconsistency in finite time.
Finally, if the input yields a behavior tree with infinitely many branches, the program is
supposed to run forever, adding a new state to its output every once in a while. More formally, for
every positive i, there has to be an integer s such that the program will have printed out the “first”
i states of the behavior tree (according to some ordering where the root, i.e. the initial state, is
state number 1, and no descendants of any particular state are printed before that state itself) at
the end of the sth step in its execution. Note that these requirements mean that a sound and
complete simulator would have to be able to decide whether the initial system state description
given to it is consistent with the input model or not within finite time. This necessity is used in the
proofs of incompleteness in Section 3.

3. New Incompleteness Results for Qualitative Simulators
In this section, we examine two different ways of restricting the qualitative vocabulary in the
hope of obtaining a representation which allows the construction of sound and complete
simulators. Subsection 3.1 considers the usage of several reduced sets of qualitative constraint
types, while retaining the operating region transition feature. We also examine a possible
restriction on the way variable values are handled during operating region transitions. Subsection
3.2 is an investigation of the capabilities of qualitative simulators which are restricted to input
models with a single operating region. Both variants are shown to exhibit the problem of
ineradicable spurious predictions when the soundness guarantee is present.
3.1 Reduced Constraint Sets
The results in this subsection are based on the undecidability properties of abstract computational
devices called unlimited register machines (URMs). We first present a brief introduction to
URMs, and then proceed with our proofs.

556

CAUSES OF INERADICABLE SPURIOUS PREDICTIONS IN QUALITATIVE SIMULATION

3.1.1 UNLIMITED REGISTER MACHINES
The easiest way of thinking about a URM is to see it as a computer with infinite memory which
supports a particularly simple programming language. A URM (Cutland, 1980) program P
consists of a finite sequence of instructions I1, I2, ..., I|P|. The instructions refer to the machine’s
registers Ri, each of which can store an arbitrarily big natural number. We use the notation r1, r2,
r3, ... for the register contents.
For our purposes, it will be sufficient to consider three types of URM instructions:
succ(n): Increment the content of register n by one.
Rn ← rn + 1
zero(n): Set the content of register n to zero.
Rn ← 0
jump(m, n, q): Compare registers m and n. If they are equal, continue with instruction q.
If rm = rn then jump to Iq
A URM program starts execution with the first instruction. If the current instruction is not a
jump whose equality condition is satisfied, it is followed by the next instruction in the list. The
program ends if it attempts to continue beyond the last instruction, or if a jump to a nonexistent
address is attempted. We assume without loss of generality that all such jumps are to address I|P|+1.
If P = I1, ..., I|P| is a URM program, it computes a function P(k) : Nk → N. P(k)(a1, ..., ak) is
computed as follows:
- Initialization: Store a1, ..., ak in registers R1, ..., Rk, respectively, and set all other registers
referenced in the program to 0.
- Iteration: Starting with I1, execute the instructions in the order described above.
- Output: If the program ends, then the computed value of the function is the number r1
contained in register R1. If the program never stops, then P(k)(a1, ..., ak) is undefined.
Table 2 contains an example URM program which computes the function f(x, y) = x + y. Note
that the function is from N2 to N, where the input values x and y are stored in registers R1 and R2,
and the output of the function is expected to be stored in R1 at the end of the program.
I1:

zero(3)

I2:

jump(2, 3, 6)

I3:

succ(1)

I4:

succ(3)

I5:

jump(1, 1, 2)

Table 2: URM Program Computing f(x, y) = x + y
The program first sets R3 to zero. It then checks to see if R2 = R3 (in the case that y = 0).
Otherwise, it increments both R1 and R3. This continues until x has been incremented y times, and
the value in R1 is returned.
The URM model of computation is equivalent to the numerous alternative models such as the
Turing machine model, the Gödel-Kleene partial recursive functions model and Church’s lambda
557

YILMAZ & SAY

calculus, (Cutland, 1980; Shepherdson & Sturgis, 1963) in the sense that the set of functions
computable by URMs is identical to the set of the functions that can be computed by any of the
other models. This means that a device which can simulate any given URM is as powerful as a
Turing machine, since it can simulate any given Turing machine. In our proofs of the
incompleteness of the QSIM vocabulary with reduced constraint sets, we will make use of the
fact that the halting problem for URMs is undecidable (Cutland, 1980).
3.1.2 SOUND AND COMPLETE QSIM WITH REDUCED CONSTRAINT SETS SOLVES THE HALTING
PROBLEM
All the incompleteness results about new subsets of the QSIM vocabulary that are presented in
this subsection are based on the following theorem, which shows that QSIM can simulate any
URM, and thereby has Turing-equivalent computational power.
Theorem 1: The execution of any URM program P with |P| instructions on any given input can
be represented by the simulation of a QSIM model with |P|+2 operating regions.
Proof: The proof will be by construction. Suppose we are given a URM program P with
instructions I1, ..., I|P|. Let R1, ..., RN be the registers mentioned in the instructions of P.
For each of the Ri, we define a QSIM variable NRi which will represent it, and, in case Ri has a
nonzero initial value ai, a set of auxiliary QSIM variables for representing ai. Table 3 describes
the idea behind this representation. We have four more variables named U, W, Z, and B. U
represents a “clock” which rises from 0 to 1 in every computational step. W is the derivative of U.
Z is constant at zero in every operating region. B is an additional auxiliary variable.
CONSTRAINTS

INITIAL VALUES

CONCLUSIONS

ONE(t) × ONE(t) = ONE(t)
ONE(t) + ONE(t) = TWO(t)
ONE(t) + TWO(t) = THREE(t)

ONE(t0) = one ∈ (0, ∞)

ONE is constant at 1
TWO is constant at 2
THREE is constant at 3

Table 3: QSIM Model Fragment Demonstrating the Representability of Exact Integer Values
Our QSIM model will have |P|+2 operating regions: Each instruction Ii of P will have a
corresponding operating region named OpRegi. The two remaining regions are OpReg0,
corresponding to the “initialization” stage of P, and OpReg|P|+1, corresponding to its end.
The specification of each operating region must contain the constraints that are valid in that
region, the Boolean conditions (if any) composed of primitives of the form
Variable = <qualitative magnitude, qualitative direction> which would trigger transitions to
other operating regions when they are obtained, and lists that detail which variables inherit their
previous magnitudes and/or directions after such a transition, and which of them are initialized to
new values during that switch. Tables 4-9 describe how to prepare these items for the operating
regions in our target model, based on the program P. There are five different operating region
templates (or “types”) used in the construction; one for each URM instruction type, one for
OpReg0, and one for OpReg|P|+1.
The model of OpReg0 is depicted in Table 4. This is where our simulation of P will start. All
the NRi variables are equated to their proper initial values specified by the “user” of P: The ones
initialized to zero are handled by “constant at 0” constraints. The ones with positive initial values
558

CAUSES OF INERADICABLE SPURIOUS PREDICTIONS IN QUALITATIVE SIMULATION

are specified to be constant at those values using add constraints to link them to the “number”
variables exemplified in Table 3; for instance, if NR2 is to be initialized to three, we have the
constraint THREE(t) + Z(t) = NR2(t). The add constraint between B, U, and ONE serves to express
the fact that the landmark named one in U’s quantity space is also equal to 1. (Note that all the
add constraints mentioned in this paragraph exist only in OpReg0, since they would disrupt the
intended behavior in the other operating regions.)
As seen in Tables 4-8, exactly which variables keep their values during a transition depends on
the type of the target operating region. Regions corresponding to instructions of the type zero(n)
should not inherit the value of Rn from their predecessors, since they involve the replacement of
that value by zero anyway. All other types of regions, including the succ(n) type, inherit all the
register contents from their predecessors. (Although the value of Rn does change in a succ
instruction, the new value depends on the old one, unlike the case of zero(n). The corresponding
QSIM variable NRn increases continuously during the simulation of a region of type succ(n), and
a new region transition occurs exactly at the moment when it has increased by one unit.)
Operating Region: OpReg0
{Type: Initialization}
Constraint Set: d/dt(U, W)
B(t) + U(t) = ONE(t) with correspondence “0 + one = one”
All the required “number representation” constraints (see Table 3)
add constraints linking the NRi to the relevant “number” variables (see text)
All variables except U and B are constant.
Possible Transition:
Trigger: ( U = <one, inc> )
New operating region: OpReg1
Variables inheriting magnitudes or directions: See Table 5, indexed by the type of OpReg1
Variables with new asserted values: U ← <0, inc>

Table 4: Model of the Operating Region OpReg0, Corresponding to the Initialization of the URM

TYPE OF TARGET
REGION
jump(m, n, q)

VARIABLES INHERITING
QUALITATIVE MAGNITUDES
All variables except U and B

VARIABLES INHERITING
QUALITATIVE DIRECTIONS
All variables except U, B, and all the NRi

succ(n)

All variables except U and B

All variables except U, B, and all the NRi

zero(n)

All variables except U and NRn

All variables except U, B, and all the NRi

End

All variables except U

All variables except U and all the NRi

Table 5: Variables which should Inherit Magnitudes and/or Directions According to the Type of
the Target Operating Region

559

YILMAZ & SAY

The simulation of the given URM program proceeds as follows: As described in the previous
subsection, the URM starts with an initial configuration, where the registers R1, ..., Rk store the
nonnegative integers a1, ..., ak, which form the input of the program, respectively. The other N−k
registers are set to 0. Correspondingly, each of the NRi variables in our QSIM model has the
quantity space [0, ∞). The NRi variables with nonzero initial values start simulation with
qualitative values <(0, ∞), std>, whereas the other ones start with <0, std>. The quantity space of
the variable U is [0, one], where the landmark one is equated to 1, as mentioned above. U starts
initially at qualitative value <0, inc>. The derivative of U, W, has as quantity space [0, speed, ∞),
where speed is also equated to 1. It starts at qualitative value <speed, std> and is constant in the
whole simulation. The variable B has the quantity space (-∞, 0, ∞) and starts at <(0, ∞), dec>.
When started in OpReg0, QSIM will compute a single qualitative behavior segment, which ends
with a transition to OpReg1 when U reaches <one, inc> at time-point t1.
Operating Region: OpRegi
{Type: zero(n)}
Constraint Set: d/dt(U, W)
NRn(t) = 0
All variables except U are constant.
Possible Transition:
Trigger: ( U = <one, inc> )
New operating region: OpRegi+1
Variables inheriting magnitudes or directions: See Table 5, indexed by the type of OpRegi+1
Variables with new asserted values: U ← <0, inc>

Table 6: Model Template for Operating Regions Corresponding to zero(n) Instructions

Operating Region: OpRegi
{Type: succ(n)}
Constraint Set: d/dt(U, W)
B(t) + U(t) = NRn(t)
All variables except U and NRn are constant.
Possible Transition:
Trigger: ( U = <one, inc> )
New operating region: OpRegi+1
Variables inheriting magnitudes or directions: See Table 5, indexed by the type of OpRegi+1
Variables with new asserted values: U ← <0, inc>

Table 7: Model Template for Operating Regions Corresponding to succ(n) Instructions

560

CAUSES OF INERADICABLE SPURIOUS PREDICTIONS IN QUALITATIVE SIMULATION

Note that there is zero uncertainty about the values of all variables, even the ones with initial
magnitude (0, ∞), at the start of the simulation.
Our model is so constrained that a sound and complete qualitative simulator is guaranteed to
produce exactly one behavior prediction for any initial state corresponding to a valid URM input.
To see this, it is sufficient to observe that, at any step of the simulation, there is sufficient
information available to the simulator to compute the exact numerical value of every variable in
the model. (This just corresponds to “tracing” the URM program and keeping note of the register
contents up to that step.) If the modeled URM halts on the particular input given in the initial
state, the QSIM behavior is supposed to be a finite one, ending when the variable U attempts to
exceed one in OpReg|P|+1. If the URM computation does not halt, then the QSIM behavior is
supposed to be a single infinite sequence of states, which never visits OpReg|P|+1. □
Operating Region: OpRegi
{Type: jump(m, n, q)}
Constraint Set: d/dt(U, W)
NRm(t) + B(t) = NRn(t)
All variables except U are constant.
Possible Transition:
Trigger: ( U = <one, inc> ) AND ( B ≠ <0, std> )
New operating region: OpRegi+1
Variables inheriting magnitudes or directions: See Table 5, indexed by the type of OpRegi+1
Variables with new asserted values: U ← <0, inc>
Possible Transition:
Trigger: ( U = <one, inc> ) AND ( B = <0, std> )
New operating region: OpRegq
Variables inheriting magnitudes or directions: See Table 5, indexed by the type of OpRegq
Variables with new asserted values: U ← <0, inc>

Table 8: Model Template for Operating Regions Corresponding to jump(m, n, q) Instructions

Operating Region: OpReg|P|+1
{Type: End}
Constraint Set: d/dt(U, W)
All variables except U are constant.

Table 9: Model of the Operating Region OpReg|P|+1, Corresponding to the End of the URM
Program
We are now ready to state a new version of the incompleteness theorem.
561

YILMAZ & SAY

Theorem 2: Even if the qualitative representation is narrowed so that only the derivative, add,
mult, and constant constraints can be used in QDE’s, and each variable is forced to start at a finite
value with zero uncertainty in the initial state, it is still impossible to build a sound and complete
qualitative simulator based on this input-output vocabulary.
Proof: Assume that such a sound and complete simulator exists. We now show how to solve the
halting problem for URMs using that algorithm as a subroutine.
Construct the corresponding QSIM model as described in Theorem 1 for the URM program P
whose halting status on a particular input is supposed to be decided. Now define a new variable S
with quantity space [0, one, ∞), where the landmark one is equated to the number 1. S starts at the
value <one, std> in the initial state. Add constraints indicating that S is constant to all the
operating regions, and specify that the value of S is inherited in all possible transitions. Insert the
new constraint S(t) = 0 in OpReg|P|+1. Consider what the simulator is supposed to do when
checking the initial state for consistency. Note that we would have an inconsistency if the
simulation ever enters OpReg|P|+1, since the new constraint that we inserted to that region says that
S is zero, which would contradict with the inherited value of one. So a simulator which is
supposed not to make any spurious predictions is expected to reject the initial state at time t0 as
inconsistent if the simulation is going to enter OpReg|P|+1, in other words, if the URM program
under consideration is going to halt. If this sound and complete simulator does not reject the
initial state due to inconsistency, but goes on with the simulation, then we can conclude that the
program P will not halt. This forms a decision procedure for the halting problem. Since the
halting problem is undecidable, we have obtained a contradiction, and conclude that a sound and
complete simulator using this representation can not exist. □
It is in fact possible to remove the derivative constraint (which is only used in our proof to
ensure that the behavior tree has at most one branch) from the representation as well, and the
incompleteness result shown above would still stand:
Theorem 3: Even if the qualitative representation is narrowed so that only the add, mult, and
constant constraints can be used in QDE’s, and each variable is forced to start at a finite value
with zero uncertainty in the initial state, it is still impossible to build a sound and complete
qualitative simulator based on this input-output vocabulary.
Proof: We will make a minor modification to the proof of Theorem 2. We observe that in the
construction of Theorem 1, U always starts every operating region at <0, inc> and the fact that its
derivative is a positive constant forces it to reach the value <one, inc> in the next time point.
Then the transition to next operating region occurs, and U again receives the value <0, inc>. What
happens if we remove the variable W and all derivative constraints from the model? In this case,
since U’s derivative is not fixed, there are three possible states for U in the second time point
during the simulation of any operating region: <one, inc>, <one, std>, and <(0, one), std>. We fix
this problem by inserting another possible region transition specification to all of our regions,
except OpReg|P|+1. This transition will be triggered when U has one of the values <one, std>, and
<(0, one), std>, and its target will be OpReg|P|+1. The variable S from the proof of Theorem 2, as
well as all other variables, are inherited completely during this transition. So all the “unwanted”
behaviors which would be created due to the elimination of U’s derivative end up in OpReg|P|+1,
and should therefore be eliminated as spurious in accordance with the argument of the previous
proof. Hence, once again, the simulator is supposed to accept the initial state as consistent if and
only if P does not halt, meaning that a sound and complete simulation is impossible with this
representation as well. □

562

CAUSES OF INERADICABLE SPURIOUS PREDICTIONS IN QUALITATIVE SIMULATION

Interestingly, one can even restrict the representation so that only nonnegative numbers are
supported, and the incompleteness result we proved above still stands:
Theorem 4: Even if the qualitative representation is narrowed so that only the add, mult, and
constant constraints can be used in QDE’s, each variable is forced to start at a finite value with
zero uncertainty in the initial state, and no variable is allowed to have a negative value at any time
during the simulation, it is still impossible to build a sound and complete qualitative simulator
based on this input-output vocabulary.
Proof: In our previous proof, only variable B ever has the possibility of having a negative value,
and that can occur only in a jump region. We replace the definition of the jump region template
with Table 10, and introduce the new variables C and Y. We insert constraints that say that these
variables are constant to all operating regions. C and Y start at zero, and are inherited by all
transitions, except when the target region is of type jump. As can be seen in Table 10, B gets the
value 1 if and only if the two compared register values are equal. If they are unequal, B has a
positive value different than 1. In this setup, B’s quantity space is defined as [0, one, ∞), where
one is equated to 1, and no variable ever has a negative value during the simulation. B now starts
simulation with the value <one, dec> to satisfy the add constraint seen in Table 4. The rest of the
argument is identical to that of Theorem 3. □
Operating Region: OpRegi
{Type: jump(m, n, q)}
Constraint Set: NRm(t) + ONE(t) = C(t)
NRn(t) + ONE(t) = Y(t)
B(t) × C(t) = Y(t)
All variables except U are constant.
Possible Transition:
Trigger: ( U = <one, inc> ) AND ( B ≠ <one, std> )
New operating region: OpRegi+1
Variables inheriting magnitudes or directions: Depends on the type of OpRegi+1, as before
Variables with new asserted values: U ← <0, inc>
Possible Transition:
Trigger: ( U = <one, inc> ) AND ( B = <one, std> )
New operating region: OpRegq
Variables inheriting magnitudes or directions: Depends on the type of OpRegq, as before
Variables with new asserted values: U ← <0, inc>

Table 10: Alternative Model Template for Operating Regions Corresponding to jump(m, n, q)
Instructions which Avoids Negative Numbers
Alternatively, we can keep negative numbers and remove the mult constraint from the
representation, if we drop the requirement that each variable starts simulation at a value with zero
uncertainty.

563

YILMAZ & SAY

Theorem 5: Even if the qualitative representation is narrowed so that only the add and constant
constraints can be used in QDE’s, it is still impossible to build a sound and complete qualitative
simulator based on this input-output vocabulary.
Proof: We used the mult constraint in the proofs of Theorems 1-3 only for equating variable and
landmark values to unambiguous integers. Assume that we delete the mult constraints from our
model of Theorem 3. The “number” variables of Table 3 are replaced with the setup shown in
Table 11. If Ri is supposed to be initialized to the positive integer ai in P, we equate NRi to the
“ai×unit” variable in OpReg0 using the method explained in the proof of Theorem 1. Note that we
only use constant and add constraints (and a lot of auxiliary variables) for this purpose.
CONSTRAINTS
ONEUNIT(t) = unit
ONEUNIT(t) + ONEUNIT(t) = TWOUNITS(t)
ONEUNIT(t) + TWOUNITS(t) = THREEUNITS(t)

CONCLUSIONS
TWOUNITS is constant at 2×unit
THREEUNITS is constant at 3×unit

Table 11: Sample Model Fragment for Equating Variables to Integer Multiples of the Positive
Landmark unit
The landmarks previously named one in other variables’ quantity spaces are now equated to
unit. In this new model, execution of a succ(n) instruction increments NRn’s value by one unit.
The jump instruction compares landmarks whose values equal u×unit and v×unit instead of
comparing two landmarks whose values equal the natural numbers u and v. The zero instruction
sets the target register to 0, as in the previous construction. So the modeled machine does just
what the original URM does, since the multiplication of all values by the coefficient unit does not
change the flow of the program, and, in particular, whether it halts on its input or not. The rest of
the argument is identical to that of the proof of Theorem 3. □
We observe that some of the variables change their qualitative magnitudes and directions
discontinuously during operating region transitions in the proofs of the previous theorems. The
next theorem proves that maintaining soundness and completeness simultaneously is impossible
even if we do not allow any qualitative variable to perform such a change, and force each
variable’s magnitude and direction to be inherited to the next operating region.
Theorem 6: Even if the qualitative representation is narrowed so that only the derivative, add and
constant constraints can be used in QDE’s, and no variable’s magnitude and direction are allowed
to perform discontinuous changes during operating region transitions, it is still impossible to build
a sound and complete qualitative simulator based on this input-output vocabulary.
Proof: Once again, we make some changes to the QSIM models used for simulating the given
URM in the previous theorems. As always, we have a QSIM variable NRi for each of the N
registers Ri appearing in the URM program. In addition to that, we define the variables Dij for all
i, j ∈ {1, ...,N} such that i ≠ j. Each of these satisfies the equation Dij = NRi – NRj throughout the
simulation; that is, we keep track of the differences of all pairs of register values. This can clearly
be achieved by inserting several add constraints to all the operating regions in our model. These
difference variables will enable us to compare two register values in operating regions of type
jump.
564

CAUSES OF INERADICABLE SPURIOUS PREDICTIONS IN QUALITATIVE SIMULATION

Furthermore, we define auxiliary variables TRi for all i ∈ {1, ..., N}. All the TRi are initialized
to the same values as the corresponding NRi, using the same technique as for the NRi.
For each instruction type in the given URM program, we define two operating regions. Our
clock variable U will increase in the first of these operating regions to the value <unit, std>, and
decrease in the next one from <unit, std> to <0, std>, performing no discontinuous jump in the
program. In order to obtain a variable with such behavior, we make use of the simple harmonic
oscillator model given in Table 12, where the variable X (denoting the displacement from the
“rest” position of the oscillating “object”) oscillates between the values unit/2 and −unit/2, and
the variable U is equated to X + unit/2, oscillating between 0 and unit. The model template given
in Table 12 is added to every operating region. (That table contains some variable names used in
the constructions of the previous proofs. All such variables are treated in the previously described
manner, unless this proof specifies otherwise.) The following lemma establishes the correctness
of this construction.
CONSTRAINTS

CORRESPONDENCES

MEANING

HALFUNIT(t) + HALFUNIT(t) = ONEUNIT(t)

c1 + c1 = unit

c1 = unit / 2

HALFUNIT(t) + V(t) = E(t)

c1 + v1 = 0

v1 = −unit / 2

d/dt(X, V)

dX
=V
dt

d/dt(V, A)

d2X
=A
dt

X(t) + A(t) = Z(t)

d2X
+ X =0
dt

X(t) + HALFUNIT(t) = U(t)

U = X+unit/2

Table 12: Model Template to Obtain the Desired Behavior for the Variable U as a Clock
Oscillating between Qualitative Values <0, std> and <unit, std> (This Template is to be Inserted
to All Constructed Operating Regions.)
Lemma 7: For any number r which can be represented by a QSIM landmark, a QSIM variable X
can be equated to the function r sin(t − t0) using only derivative, add and constant constraints.
Proof of Lemma 7:
As seen in Table 12, the equation
d2
X (t ) + X (t ) = 0
dt

can be expressed using only derivative, add and constant constraints. This equation has a general
solution of the form
X (t ) = c1 sin t + c2 cos t ,
565

(1)

YILMAZ & SAY

and hence its time derivative V has the form
V (t ) = c1 cos t − c2 sin t .

Assume that X and V are initialized as follows:
X (t0 ) = 0
V (t0 ) = r .

By substituting these values in the equations above, one obtains the equation system
0 = c1 sin t0 + c2 cos t0
r = c1 cos t0 − c2 sin t0 ,

whose solution (Yılmaz, 2005) yields c1 = r cos t0 and c2 = −r sin t0. Substituting c1 and c2 into
Equation (1), we get X (t ) = r × (cos t 0 sin t − sin t 0 cos t ) = r sin (t − t 0 ) , thereby proving the
lemma. □
Proof of Theorem 6 (continued):
Therefore, if we equate the landmark v1 of V to −unit/2 as shown in Table 12, and initialize X and
V to 0 and v1, respectively, we will ensure that
X (t ) = −

unit
sin(t − t0 ) ,
2

i.e., that the variable X oscillates between the values unit/2 and −unit/2, as desired.
To be consistent with Lemma 7, the oscillating variables of Table 12 start simulation with the
qualitative values listed in Table 13. All other variables, except B, which starts with the value
<(0, ∞), inc>, are initialized as previously described.
VARIABLE

QUANTITY SPACE

INITIAL VALUE

U

[0, unit]

<(0, unit), dec>

E

(-∞, 0, ∞)

<0, std>

X

(-∞, 0, ∞)

<0, dec>

V

(-∞, v1, 0, ∞)

<v1, std>

A

(-∞, 0, ∞)

<0, inc>

Table 13: The Quantity Spaces and Initial Values of the Oscillating Variables
We are going to denote the two operating regions corresponding to the ith instruction of the
URM program with OpRegi,1 and OpRegi,2. All variables’ qualitative values are inherited in all
possible transitions, such that no variable ever undergoes a discontinuous change. Looking
carefully at Tables 14-21, which correspond to the initialization, instruction types, and ending of
the URM, we see that the simulation flows in a unique branch with the exception of zero type
566

CAUSES OF INERADICABLE SPURIOUS PREDICTIONS IN QUALITATIVE SIMULATION

operating regions, where there is the possibility that the simulation branches into more than one
behavior, and the behaviors which do not correspond to the expected trajectory of the actual
URM are directed to OpReg|P|+1,1. (Note that transitions to infinite landmarks do not need to be
considered, since we assume that all infinite landmarks are specified as unreachable values for all
variables in our models.) The registers stay constant when OpReg|P|+1,1, which is a single operating
region corresponding to the end of the URM program, is reached. The rest of the proof is the
same as in Theorem 2. Our contradiction variable S ensures that only the behavior of a nonhalting URM leads to a consistent initial state, hence determining the consistency of the initial
state is equivalent to deciding the halting problem, leading to a contradiction. □
Operating Region: OpReg0
{Type: Initialization}
Constraint Set: All the required “input value representation” constraints (see Table 11)
B(t) + U(t) = ONEUNIT(t) with correspondence “0 + unit = unit”
add constraints linking the NRi and the TRi to the relevant “n×unit” variables
add constraints defining the Dij variables
the “clock” constraints (Table 12)
All variables except B, U, X, V, E, and A are constant.
Possible Transition:
Trigger: ( U = <0, std> )
New operating region: OpReg1,1
Variables inheriting qualitative values: All variables

Table 14: Template for the Single Operating Region Corresponding to the Initialization Stage
3.2 Simulation within a Single Operating Region
The incompleteness proofs in subsection 3.1 (as well as that of Say & Akın, 2003) depend on the
capability of “turning the constraints on or off” when necessary, which is provided by the
operating region transition feature. Would the problem persist if we forsook that feature, and
focused on the simulation of qualitative models with a single operating region? We now show
that the answer to this question is affirmative.
3.2.1 HILBERT’S TENTH PROBLEM
As the name suggests, Hilbert’s Tenth Problem is the tenth of 23 problems which were
announced in 1900 by the famous mathematician David Hilbert as a challenge to the
mathematicians of the 20th century. It asks for an algorithm for deciding whether a given
multivariate polynomial with integer coefficients has integer solutions. It has been proven that no
such algorithm exists (Matiyasevich, 1993). This fact was used by Say and Akın (2003) in their
original proof of the existence of ineradicable spurious predictions in the outputs of all qualitative
simulators employing the operating region transition feature and a larger set of constraint types
than those we deal with in this paper.

567

YILMAZ & SAY

In the proof to be presented shortly, we use the undecidability of a slightly modified variant of
the setup described by Hilbert: We assume a guarantee that none of the variables in the given
polynomial are zero in the solution whose existence is in question. It is clear that this modified
problem is unsolvable as well, by the following argument: Assume that we do have an algorithm
A which takes a multivariate polynomial with integer coefficients as input, and announces
whether a solution where all the variables have nonzero integer values exists or not in finite time.
We can use A as a subroutine in the construction of the algorithm sought in Hilbert’s original
problem as follows: We systematically produce 2n polynomials from the input polynomial with n
variables, such that each of these new polynomials corresponds to a different subset of the
variables of the original polynomial replaced with zero. We then run A on each of these new
polynomials. It is easy to see that A will find that one or more of these polynomials have nonzero
integer solutions if and only if the original polynomial has integer solutions.
Operating Region: OpRegi,1
{Type: zero(n)}
Constraint Set: add constraints defining the Dij variables
the “clock” constraints (Table 12)
All variables except U, X, V, E, A, NRn, and Dij with n ∈ {i,j} are constant.
Possible Transition:
Trigger: ( U = <unit, std> ) AND ( NRn = <0, std> )
New operating region: OpRegi,2
Variables inheriting qualitative values: All variables
Possible Transition:
Trigger: (( U = <unit, std> ) AND ( NRn ≠ <0, std> )) OR ( NRn = <(0, ∞), std> ) OR ( NRn = <(-∞, 0), std> )
New operating region: OpReg|P|+1, 1
Variables inheriting qualitative values: All variables

Table 15: Template for the First Operating Region Corresponding to zero(n) Instructions
3.2.2 SOUND AND COMPLETE QSIM WITHIN A SINGLE OPERATING REGION SOLVES HILBERT’S
TENTH PROBLEM
Theorem 8: Even if the qualitative representation is narrowed so that only the derivative, add,
mult and constant constraints can be used in QDE’s, and the simulation proceeds only in a single
operating region, it is still impossible to build a sound and complete qualitative simulator based
on this input-output vocabulary.
We are going to start our proof with some preliminary lemmata, the first of which is
reminiscent of Lemma 7 from the previous subsection:
Lemma 9: For any real constant equated to the QSIM variable Xi, a QSIM variable Yi can be
equated to the function sin ( X i × (t − t0 )) using only derivative, add, mult, and constant constraints.
Proof: The case for Xi = 0 is trivial, and can be handled with a single constant constraint. For the
remaining case, we will consider the following equation set:
568

CAUSES OF INERADICABLE SPURIOUS PREDICTIONS IN QUALITATIVE SIMULATION

Operating Region: OpRegi,2
{Type: zero(n)}
Constraint Set: add constraints defining the Dij variables
the “clock” constraints (Table 12)
All variables except U, X, V, E, A, and TRn are constant.
Possible Transition:
Trigger: ( U = <0, std> ) AND ( TRn = <0, std> )
New operating region: OpRegi+1,1
Variables inheriting qualitative values: All variables
Possible Transition:
Trigger: (( U = <0, std> ) AND ( TRn ≠ <0, std> ) ) OR ( TRn = <(0, ∞), std> ) OR ( TRn = <(-∞, 0), std> )
New operating region: OpReg|P|+1, 1
Variables inheriting qualitative values: All variables

Table 16: Template for the Second Operating Region Corresponding to zero(n) Instructions

Operating Region: OpRegi,1
{Type: succ(n)}
Constraint Set: TRn(t)+ U(t)= NRn(t)
add constraints defining the Dij variables
the “clock” constraints (Table 12)
All variables except U, X, V, E, A, NRn, and Dij with n ∈ {i,j} are constant.
Possible Transition:
Trigger: ( U = <unit, std> )
New operating region: OpRegi,2
Variables inheriting qualitative values: All variables

Table 17: Template for the First Operating Region Corresponding to succ(n) Instructions
d2
Yi (t ) + Wi × Yi (t ) = 0
dt

Xi > 0
X i ,
Wi = X i2 , so that Wi = 
 − X i , X i < 0.

with the initial values
Yi (t0 ) = 0

569

(2)

(3)

YILMAZ & SAY

Operating Region: OpRegi,2
{Type: succ(n)}
Constraint Set: TRn(t)+ U(t)= NRn(t)
add constraints defining the Dij variables
the “clock” constraints (Table 12)
All variables except U, X, V, E, A, and TRn are constant.
Possible Transition:
Trigger: ( U = <0, std> )
New operating region: OpRegi+1,1
Variables inheriting qualitative values: All variables

Table 18: Template for the Second Operating Region Corresponding to succ(n) Instructions

Operating Region: OpRegi,1
{Type: jump(m, n, q)}
Constraint Set: add constraints defining the Dij variables
the “clock” constraints (Table 12)
All variables except U, X, V, E, and A are constant.
Possible Transition:
Trigger: ( U = <unit, std> )
New operating region: OpRegi,2
Variables inheriting qualitative values: All variables

Table 19: Template for the First Operating Region Corresponding to jump(m, n, q) Instructions
Vi (t0 ) = X i ,

where Vi(t) is the time derivative of Yi(t).
The general solution of Equation (2) is:

(

)

(

)

Yi (t ) = c1 sin Wi × t + c2 cos Wi × t .

Substituting the Wi from Equation (3) and the initial values in the equations for Yi and Vi, and
solving these equation systems results in the following (Yılmaz, 2005):
For Xi > 0, c1 = cos(Xi×t0) and c2 = −sin(Xi×t0).
For Xi < 0, c1 = −cos(Xi×t0) and c2 = −sin(Xi×t0).
When we substitute these in the formula for Yi(t), we obtain:
Yi (t ) = cos( X i × t0 )sin ( X i × t ) − sin ( X i × t0 ) cos( X i × t ) = sin ( X i × (t − t0 )) ,
570

Xi > 0,

CAUSES OF INERADICABLE SPURIOUS PREDICTIONS IN QUALITATIVE SIMULATION

Operating Region: OpRegi,2
{Type: jump(m, n, q)}
Constraint Set: add constraints defining the Dij variables
the “clock” constraints (Table 12)
All variables except U, X, V, E, and A are constant.
Possible Transition:
Trigger: ( Dmn = <0, std> ) AND ( U = <0, std> )
New operating region: OpRegq,1
Variables inheriting qualitative values: All variables
Possible Transition:
Trigger: ( Dmn ≠ <0, std> ) AND ( U = <0, std> )
New operating region: OpRegi+1,1
Variables inheriting qualitative values: All variables

Table 20: Template for the Second Operating Region Corresponding to jump(m, n, q) Instructions

Operating Region: OpReg|P|+1, 1
{Type: End}
Constraint Set: S(t) = 0
All variables except U, X, V, E, and A are constant.

Table 21: Model of the Operating Region Corresponding to the End of the URM Program
Yi (t ) = − cos( X i × t0 )sin ((− X i ) × t ) − sin ( X i × t0 ) cos((− X i ) × t ) = sin ( X i × (t − t0 )) ,

Xi < 0.

Hence, we have Yi(t) = sin ( X i × (t − t0 )) for all Xi ≠ 0.
Table 22 shows that Equations (2) and (3) and the initial values given above are representable
using the derivative, add, mult, and constant constraints. Note that Xi has to be kept constant and
initialized to either (0, ∞) or (−∞, 0), depending on the intended sign for that number, and both Yi
and Ci must start at zero, to be consistent with the construction above. □
Lemma 10: Starting at t0, the function Y = sin(t−t0) reaches the value 0 for the first time at time
point tE = t0 + π. Moreover the function Yi = sin(Xi×(t−t0)) reaches 0 at the same time point tE if
and only if Xi is an integer.
Proof: The equation sin(t−t0) = 0 implies t−t0 = nπ, n ∈ Ζ, and since we are interested in the first
time point after t0 where it becomes 0, we get tE = t0 + π. For the “only if” part of the second
statement, assume that the function Yi = sin(Xi×(t−t0)) reaches 0 at tE = t0 + π. Then
sin ( X i × (t 0 + π − t 0 )) = sin ( X i × π ) = 0 implies that Xi is an integer. For the “if” part, we use the
knowledge that Xi is an integer to conclude that Yi(tE) = sin(Xi×(t0 + π − t0)) = sin(Xi×π) = 0.
571

YILMAZ & SAY

CONSTRAINTS

MEANING

Z=0
Xi(t) + Ci(t) = Vi(t)

Vi(t0) = Xi

d/dt(Yi, Vi)

dYi
= Vi (t )
dt

d/dt(Vi, Ai)

d 2Yi
= Ai (t )
dt

Xi(t) × Xi(t) = Wi(t)

Wi = X i2

Wi(t) × Yi(t) = Li(t)

Li (t ) = Wi × Yi (t )

Ai(t) + Li(t) = Z(t)

d 2Yi
+ Wi × Yi (t ) = 0
dt

Table 22: Model Fragment Used to Obtain the Relationship Yi = sin ( X i × (t − t0 ))
Proof of Theorem 8: As already mentioned, the proof relies on a contradiction, namely that a
sound and complete simulator, if it existed, could be used to construct an algorithm for solving
Hilbert’s Tenth Problem, as follows:
Assume that we are given a polynomial P(x1, x2, x3, ..., xn) with integer coefficients. We start
by constructing a QSIM model fragment that “says” that P(x1, x2, x3, ..., xn) = 0: We have already
seen in Section 3.1 how to equate any desired integer to a QSIM variable. Represent all integers
appearing as coefficients in the polynomial in that manner. Introduce a QSIM variable Xi for each
of the xi, declare all these Xi to be constant, and use add and mult constraints to equate the sum of
products that is P(X1, X2, X3, ..., Xn) to a QSIM variable P, which will be initialized to 0. Note that
this is tantamount to saying that the present values of the Xi form a solution for the polynomial.
All this can clearly be done in a single operating region, with constraints of the types mult, add
and constant.
We then extend this model with the necessary constraints and auxiliary variables to equate a
new variable Y to the function sin(t−t0). (Either Lemma 7 or Lemma 9 can be used for this
purpose.) We specify Y’s quantity space as [0, ∞), so that the simulation is guaranteed to finish at
t = tE = t0 + π. For each Xi, we define associated auxiliary variables Ci, Li, Wi, Vi, Ai and Yi, and
add the template in Table 22 to our model to express the relationship Yi = sin(Xi×(t−t0)). We also

equate a variable YS with the sum of the squares of the Yi, i.e. YS =

n

∑Y

i

2

. Note that if YS = 0,

i =1

then all the Yi are 0.
Finally, we need make sure that the only consistent behaviors are the ones in which the Xi are
integers (that is, relying on Lemma 10, the behaviors in which the variable YS becomes 0 at tE).
To serve this aim, we add the constraint F(t) × Y(t) = YS(t) to our model.
We will simulate this model 2n times, each run corresponding to a different way of initializing
the Xi to magnitudes selected from the set {(0, ∞), (−∞, 0)}. A sound and complete simulator
572

CAUSES OF INERADICABLE SPURIOUS PREDICTIONS IN QUALITATIVE SIMULATION

would accept all and only the initial states with those Xi whose values do not cause any
inconsistency with our model. But those Xi correspond exactly to the integer solutions of the
given polynomial, by the following reasoning about the variable F:
Note that F is defined to be YS/Y by the F(t) × Y(t) = YS(t) constraint. We know that Y, Yi, and
hence YS are all initially 0, meaning that one has to use l’Hôpital’s rule to find out the initial
value of F. This is important, since if F’s initial magnitude or derivative are infinite, QSIM is not
even supposed to consider successors for the initial state. (We declare the infinite landmarks as
unreachable values for all variables, as mentioned earlier. Even if F’s magnitude is finite and just
its derivative is infinite, simulation is not supposed to continue, because the continuity
requirement would be violated.) Fortunately,
n

∑ sin ( X × (t − t ))
2

F (t ) =

0

i

i =1

sin (t − t0 )

does have finite magnitude and derivative at t0: At t = t0, we use l’Hôpital’s rule to find
n

0
F (t0 ) = =
0

∑ 2X

i

sin ( X i × (t 0 −t0 )) cos( X i × (t0 − t0 ))

i =1

= 0.

cos(t0 − t0 )

As for F’s qualitative direction;
dF
(t ) =
dt

 2 X i sin ( X i × (t − t0 )) cos( X i × (t − t0 )) sin 2 ( X i × (t − t0 )) cos(t − t0 ) 

,
−


sin (t − t0 )
sin 2 (t − t0 )
i =1 

n

∑

and it turns out, after several applications of l’Hôpital’s rule, that
dF
(t0 ) =
dt

n

∑X

2
i

,

i =1

which is clearly a finite positive number, and F’s initial qualitative value is therefore <0, inc>.
Obviously, F = YS/Y is guaranteed to be finite until tE, when Y reaches 0. If the variable YS is
nonzero (implying that at least one of the Yi is nonzero, and by Lemma 10 that the corresponding
Xi is not an integer) at tE, F(tE) has to equal ∞, which is impossible since infinity was declared to
be unreachable, so such states would be eliminated as spurious. If, on the other hand, YS(tE) = 0,
then, we see by l’Hôpital’s rule, and the knowledge that all the Xi are integers, that
n

0
F (t E ) = =
0

∑
i =1

n

2 X i sin ( X i × (t E − t0 ))cos ( X i × (t E − t0 ))
=

cos (t E − t0 )

∑ 2X
i =1

i

sin ( X i × π ) cos( X i × π )
cos(π )

= 0,

and behaviors ending with such states are supposed to be included in the simulation output. So if
our supposedly complete and sound simulator rejects the initial states of our model due to
inconsistency in all the 2n runs, we reason that all the behavior predictions considered by the
simulator ended with F(tE) = ∞, and this inconsistency propagated back to the initial state and led
to its rejection in all cases. We conclude that the polynomial has no integer solutions. On the
other hand, if even one of the simulations prints out the initial state and goes on with its
successors, we conclude that a solution exists. This forms the decision procedure required in
573

YILMAZ & SAY

Hilbert’s Tenth Problem, leading to a contradiction. Therefore, sound and complete simulation is
impossible even if one restricts oneself to a single operating region and the limited constraint
vocabulary mentioned in the statement of the theorem. □

4. Conclusion
In this paper, we considered several alternative subsets of the qualitative representation, and
showed that the ineradicable spurious prediction problem persists even when only the add and
constant constraints are allowed. If one allows the mult constraint as well, then any resulting
qualitative simulator is inherently incomplete even when the representation of negative numbers
is forbidden and every variable is forced to be specified with zero uncertainty (i.e. as a single
unambiguous real number) in the initial state. Our final proof shows that even the ability of
handling models with multiple operating regions can be removed from the representation, and the
incompleteness problem would still persist, provided the add, constant, derivative, and mult
constraints are allowed in the vocabulary. Note that none of these vocabularies include the
monotonic function constraint, which is the only relation type “native” to the qualitative
representation.
Although the results in this paper are demonstrated using the QSIM representation for input
and output, they are valid for all qualitative simulators whose input and output vocabularies are as
expressive as the specified subsets of those of QSIM. (Also note that our proofs apply
automatically to semi-quantitative simulators, whose representations are an extension of that of
pure QSIM.) We believe that these results are important in the sense that they provide deeper
insight to the causes of spurious predictions, and they can be helpful for researchers aiming to
construct provably sound and complete simulators using weaker representations.
Finally, we wish to stress that our findings here do not amount to as bad a piece of news about
the usefulness of qualitative simulators in the practical domains that they are usually utilized as it
may seem to the uninitiated eye. When one’s model is specified at the level of precision that is
involved in the models in this paper, one does not employ a qualitative reasoner anyway. What
really annoys the users of qualitative simulators is the occasional prediction of eradicable
spurious behaviors, and the strengthening of the algorithms with additional filters of increasing
mathematical sophistication to get rid of more of these continues to be an important line of
research.

References
Cutland, N. J. (1980). Computability: An Introduction to Recursive Function Theory. Cambridge,
UK: Cambridge University Press.
de Kleer, J., & Brown, J. S. (1984). A qualitative physics based on confluences. Artificial
Intelligence, 24, 7-83.
Fouché, P., & Kuipers, B. J. (1992). Reasoning about energy in qualitative simulation. IEEE
Transactions on Systems, Man, and Cybernetics, 22, 47-63.
Könik, T., & Say, A. C. C. (2003). Duration consistency filtering for qualitative simulation.
Annals of Mathematics and Artificial Intelligence, 38, 269-309.
Kuipers, B. J., (1986). Qualitative simulation. Artificial Intelligence, 29, 289-338.

574

CAUSES OF INERADICABLE SPURIOUS PREDICTIONS IN QUALITATIVE SIMULATION

Kuipers, B. J. (1994). Qualitative Reasoning: Modeling and Simulation with Incomplete
Knowledge. Cambridge, MA: MIT Press.
Matiyasevich, Y. (1993). Hilbert’s Tenth Problem. Cambridge, MA: MIT Press.
Missier, A. (1991). Mathematical structures for qualitative calculus, a contribution to qualitative
simulation. (In French) Ph.D. thesis, Institut National des Sciences Appliquées de Toulouse.
Say, A. C. C. (1998). L’Hôpital’s filter for QSIM. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 20, 1-8.
Say, A. C. C. (2001). Improved reasoning about infinity using qualitative simulation. Computing
and Informatics, 20, 487-507.
Say, A. C. C. (2003). Sound and complete qualitative simulation requires “quantitative” filtering.
Annals of Mathematics and Artificial Intelligence, 38, 257-267.
Say, A. C. C., & Akın, H. L. (2003). Sound and complete qualitative simulation is impossible.
Artificial Intelligence, 149, 251-266.
Say, A. C. C., & Kuru, S. (1993). Improved filtering for the QSIM algorithm. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 15, 967-971.
Shepherdson, J. C., & Sturgis, H. E. (1963). Computability of recursive functions. Journal of the
ACM, 10, 217-255.
Shults, B., & Kuipers, B. (1997). Proving properties of continuous systems: Qualitative
simulation and temporal logic. Artificial Intelligence, 92, 91-129.
Struss, P. (1990). Problems of interval-based qualitative reasoning. In Weld, D. S., & de Kleer, J.
(Eds.) Readings in Qualitative Reasoning about Physical Systems. San Mateo, CA: Morgan
Kaufmann, 288-305.
Weld, D. S., & de Kleer, J. (Eds.) (1990). Readings in Qualitative Reasoning about Physical
Systems. San Mateo, CA: Morgan Kaufmann.
Yılmaz, Ö. (2005). Computability-theoretic limitations of qualitative simulation. M. S. Thesis,
Boğaziçi University, Đstanbul, Turkey.
(http://www.cmpe.boun.edu.tr/graduate/allthesis/m_3.pdf)

575

Journal of Artificial Intelligence Research 27 (2006) 617-674

Submitted 07/06; published 12/06

Uncertainty in Soft Temporal Constraint Problems: A General
Framework and Controllability Algorithms for The Fuzzy Case
Francesca Rossi
Kristen Brent Venable

FROSSI @ MATH . UNIPD . IT
KVENABLE @ MATH . UNIPD . IT

University of Padova, Department of Pure and Applied Mathematics,
Via Trieste, 63 35121 PADOVA ITALY

Neil Yorke-Smith

NYSMITH @ AI . SRI . COM

SRI International,
333 Ravenswood Ave, Menlo Park, CA 94025 USA

Abstract
In real-life temporal scenarios, uncertainty and preferences are often essential and coexisting
aspects. We present a formalism where quantitative temporal constraints with both preferences and
uncertainty can be defined. We show how three classical notions of controllability (that is, strong,
weak, and dynamic), which have been developed for uncertain temporal problems, can be generalized to handle preferences as well. After defining this general framework, we focus on problems
where preferences follow the fuzzy approach, and with properties that assure tractability. For such
problems, we propose algorithms to check the presence of the controllability properties. In particular, we show that in such a setting dealing simultaneously with preferences and uncertainty does not
increase the complexity of controllability testing. We also develop a dynamic execution algorithm,
of polynomial complexity, that produces temporal plans under uncertainty that are optimal with
respect to fuzzy preferences.

1. Introduction
Current research on temporal constraint reasoning, once exposed to the difficulties of real-life problems, can be found lacking both expressiveness and flexibility. In rich application domains it is
often necessary to simultaneously handle not only temporal constraints, but also preferences and
uncertainty.
This need can be seen in many scheduling domains. The motivation for the line of research
described in this paper is the domain of planning and scheduling for NASA space missions. NASA
has tackled many scheduling problems in which temporal constraints have been used with reasonable success, while showing their limitations in their lack of capability to deal with uncertainty and
preferences. For example, the Remote Agent (Rajan, Bernard, Dorais, Gamble, Kanefsky, Kurien,
Millar, Muscettola, Nayak, Rouquette, Smith, Taylor, & Tung, 2000; Muscettola, Morris, Pell, &
Smith, 1998) experiments, which consisted of placing an AI system on-board to plan and execute
spacecraft activities, represents one of the most interesting examples of this. Remote Agent worked
with high level goals which specified, for example, the duration and frequency of time windows
within which the spacecraft had to take asteroid images to be used for orbit determination for the
on-board navigator. Remote Agent dealt with both flexible time intervals and uncontrollable events;
however, it did not deal with preferences: all the temporal constraints are hard. The benefit of
adding preferences to this framework would be to allow the planner to handle uncontrollable events
while at the same time maximizing the mission manager’s preferences.
c
2006
AI Access Foundation. All rights reserved.

ROSSI , V ENABLE ,& YORKE -S MITH

A more recent NASA application is in the rovers domain (Dearden, Meuleau, Ramakrishnan,
Smith, & Washington, 2002; Bresina, Jonsson, Morris, & Rajan, 2005). NASA is interested in
the generation of optimal plans for rovers designed to explore a planetary surface (e.g. Spirit and
Opportunity for Mars) (Bresina et al., 2005). Dearden et al. (2002) describe the problem of generating plans for planetary rovers that handle uncertainty over time and resources. The approach
involves first constructing a “seed” plan, and then incrementally adding contingent branches to this
plan in order to improve its utility. Again, preferences could be used to embed utilities directly in
the temporal model.
A third space application, which will be used several times in this paper as a running example,
concerns planning for fleets of Earth Observing Satellites (EOS) (Frank, Jonsson, Morris, & Smith,
2001). This planning problem involves multiple satellites, hundreds of requests, constraints on when
and how to serve each request, and resources such as instruments, recording devices, transmitters
and ground stations. In response to requests placed by scientists, image data is acquired by an EOS.
The data can be either downlinked in real time or recorded on board for playback at a later time.
Ground stations or other satellites are available to receive downlinked images. Different satellites
may be able to communicate only with a subset of these resources, and transmission rates will differ
from satellite to satellite and from station to station. Further, there may be different financial costs
associated with using different communication resources. In (Frank et al., 2001) the EOS scheduling problem is dealt with by using a constraint-based interval representation. Candidate plans are
represented by variables and constraints, which reflect the temporal relationship between actions
and the constraints on the parameters of states or actions. Also, temporal constraints are necessary
to model duration and ordering constraints associated with the data collection, recording, and downlinking tasks. Solutions are preferred based on objectives (such as maximizing the number of high
priority requests served, maximizing the expected quality of the observations, and minimizing the
cost of downlink operations). Uncertainty is present due to weather: specifically due to duration and
persistence of cloud cover, since image quality is obviously affected by the amount of clouds over
the target. In addition, some of the events that need to be observed may happen at unpredictable
times and have uncertain durations (e.g. fires or volcanic eruptions).
Some existing frameworks, such as Simple Temporal Problems with Preferences (STPPs) (Khatib,
Morris, Morris, & Rossi, 2001), address the lack of expressiveness of hard temporal constraints by
adding preferences to the temporal framework, but do not take into account uncertainty. Other models, such as Simple Temporal Problems with Uncertainty (STPUs) (Vidal & Fargier, 1999), account
for contingent events, but have no notion of preferences. In this paper we introduce a framework
which allows us to handle both preferences and uncertainty in Simple Temporal Problems. The
proposed model, called Simple Temporal Problems with Preferences and Uncertainty (STPPUs),
merges the two pre-existing models of STPPs and STPUs.
An STPPU instance represents a quantitative temporal problem with preferences and uncertainty
via a set of variables, representing the starting or ending times of events (which can be controllable
by the agent or not), and a set of soft temporal constraints over such variables, each of which includes an interval containing the allowed durations of the event or the allowed times between events.
A preference function associating each element of the interval with a value specifies how much that
value is preferred. Such soft constraints can be defined on both controllable and uncontrollable
events. In order to further clarify what is modeled by an STPPU, let us emphasize that graduality
is only allowed in terms of preferences and not of uncertainty. In this sense, the uncertainty represented by contingent STPPU constraints is the same as that of contingent STPU constraints: all
618

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

durations are assumed to be equally possible. In addition to expressing uncertainty, in STPPUs, contingent constraints can be soft and different preference levels can be associated to different durations
of contingent events.
On these problems, we consider notions of controllability similar to those defined for STPUs,
to be used instead of consistency because of the presence of uncertainty, and we adapt them to
handle preferences. These notions, usually called strong, weak, and dynamic controllability, refer
to the possibility of “controlling” the problem, that is, of the executing agent assigning values to
the controllable variables, in a way that is optimal with respect to what Nature has decided, or will
decide, for the uncontrollable variables. The word optimal here is crucial, since in STPUs, where
there are no preferences, we just need to care about controllability, and not optimality. In fact, the
notions we define in this paper that directly correspond to those for STPUs are called strong, weak,
and dynamic optimal controllability.
After defining these controllability notions and proving their properties, we then consider the
same restrictions which have been shown to make temporal problems with preferences tractable
(Khatib et al., 2001; Rossi, Sperduti, Venable, Khatib, Morris, & Morris, 2002), i.e, semi-convex
preference functions and totally ordered preferences combined with an idempotent operator. In
this context, for each of the above controllability notions, we give algorithms that check whether
they hold, and we show that adding preferences does not make the complexity of testing such
properties worse than in the case without preferences. Moreover, dealing with different levels of
preferences, we also define testing algorithms which refer to the possibility of controlling a problem
while maintaining a preference of at least a certain level (called α-controllability). Finally, in the
context of dynamic controllability, we also consider the execution of dynamic optimal plans.
Parts of the content of this paper have appeared in (Venable & Yorke-Smith, 2003; Rossi, Venable, & Yorke-Smith, 2003; Yorke-Smith, Venable, & Rossi, 2003; Rossi, Venable, & Yorke-Smith,
2004). This paper extends the previous work in at least two directions. First, while in those papers optimal and α controllability (strong or dynamic) were checked separately, now we can check
optimal (strong or dynamic) controllability and, if it does not hold, the algorithm will return the
highest α such that the given problem is α-strong or α-dynamic controllable. Moreover, results are
presented in a uniform technical environment, by providing a thorough theoretical study of the properties of the algorithms and their computational aspects, which makes use of several unpublished
proofs.
This paper is structured as follows. In Section 2 we give the background on temporal constraints
with preference and with uncertainty. In Section 3 we define our formalism for Simple Temporal
Problems with both preferences and uncertainty and, in Section 4, we describe our new notions of
controllability. Algorithms to test such notions are described respectively in Section 5 for Optimal
Strong Controllability, in Section 6 for Optimal Weak Controllability, and in Section 7 for Optimal
Dynamic Controllability. In Section 8 we then give a general strategy for using such notions. Finally, in Section 9, we discuss related work, and in Section 10 we summarize the main results and
we point out some directions for future developments. To make the paper more readable, the proofs
of all theorems are contained in the Appendix.

2. Background
In this section we give the main notions of temporal constraints (Dechter, Meiri, & Pearl, 1991) and
the framework of Temporal Constraint Satisfaction Problems with Preferences (TCSPPs) (Khatib

619

ROSSI , V ENABLE ,& YORKE -S MITH

et al., 2001; Rossi et al., 2002), which extend quantitative temporal constraints (Dechter et al., 1991)
with semiring-based preferences (Bistarelli, Montanari, & Rossi, 1997). We also describe Simple
Temporal Problems with Uncertainty (STPUs) (Vidal & Fargier, 1999; Morris, Muscettola, & Vidal,
2001), which extend a tractable subclass of temporal constraints to model agent-uncontrollable
contingent events, and we define the corresponding notions of controllability, introduced in (Vidal
& Fargier, 1999).
2.1 Temporal Constraint Satisfaction Problems
One of the requirements of a temporal reasoning system for planning and scheduling problems is
an ability to deal with metric information; in other words, to handle quantitative information on
duration of events (such as “It will take from ten to twenty minutes to get home”). Quantitative
temporal networks provide a convenient formalism to deal with such information. They consider
instantaneous events as the variables of the problem, whose domains are the entire timeline. A
variable may represent either the beginning or an ending point of an event, or a neutral point of
time. An effective representation of quantitative temporal networks, based on constraints, is within
the framework of Temporal Constraint Satisfaction Problems (TCSPs) (Dechter et al., 1991).
In this paper we are interested in a particular subclass of TCSPs, known as Simple Temporal
Problems (STPs) (Dechter et al., 1991). In such a problem, a constraint between time-points X i and
Xj is represented in the constraint graph as an edge X i → Xj , labeled by a single interval [aij , bij ]
that represents the constraint aij ≤ Xj − Xi ≤ bij . Solving an STP means finding an assignment
of values to variables such that all temporal constraints are satisfied.
Whereas the complexity of a general TCSP comes from having more than one interval in a
constraint, STPs can be solved in polynomial time. Despite the restriction to a single interval per
constraint, STPs have been shown to be valuable in many practical applications. This is why STPs
have attracted attention in the literature.
An STP can be associated with a directed weighted graph G d = (V, Ed ), called the distance
graph. It has the same set of nodes as the constraint graph but twice the number of edges: for
each binary constraint over variables X i and Xj , the distance graph has an edge Xi → Xj which is
labeled by weight bij , representing the linear inequality X j −Xi ≤ bij , as well as an edge Xj → Xi
which is labeled by weight −aij , representing the linear inequality X i − Xj ≤ −aij .
Each path from Xi to Xj in the distance graph Gd , say through variables Xi0 = Xi , Xi1 , Xi2 , . . .
P
, Xik = Xj induces the following path constraint: X j −Xi ≤ kh=1 bih−1 ih . The intersection of all
induced path constraints yields the inequality X j − Xi ≤ dij , where dij is the length of the shortest
path from Xi to Xj , if such a length is defined, i.e. if there are no negative cycles in the distance
graph. An STP is consistent if and only if its distance graph has no negative cycles (Shostak, 1981;
Leiserson & Saxe, 1988). This means that enforcing path consistency, by an algorithm such as
PC-2, is sufficient for solving STPs (Dechter et al., 1991). It follows that a given STP can be effectively specified by another complete directed graph, called a d-graph, where each edge Xi → Xj is
labeled by the shortest path length d ij in the distance graph Gd .
In (Dechter et al., 1991) it is shown that any consistent STP is backtrack-free (that is, decomposable) relative to the constraints in its d-graph. Moreover, the set of temporal constraints of the
form [−dji , dij ] is the minimal STP corresponding to the original STP and it is possible to find one
of its solutions using a backtrack-free search that simply assigns to each variable any value that
satisfies the minimal network constraints compatibly with previous assignments. Two specific solu-

620

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

tions (usually called the latest and the earliest assignments) are given by S L = {d01 , . . . , d0n } and
SE = {d10 , . . . , dn0 }, which assign to each variable respectively its latest and earliest possible time
(Dechter et al., 1991).
The d-graph (and thus the minimal network) of an STP can be found by applying FloydWarshall’s All Pairs Shortest Path algorithm (Floyd, 1962) to the distance graph with a complexity
of O(n3 ) where n is the number of variables. If the graph is sparse, the Bellman-Ford Single Source
Shortest Path algorithm can be used instead, with a complexity equal to O(nE), where E is the
number of edges. We refer to (Dechter et al., 1991; Xu & Choueiry, 2003) for more details on
efficient STP solving.
2.2 Temporal CSPs with Preferences
Although expressive, TCSPs model only hard temporal constraints. This means that all constraints
have to be satisfied, and that the solutions of a constraint are all equally satisfying. However, in
many real-life situations some solutions are preferred over others and, thus, the global problem is to
find a way to satisfy the constraints optimally, according to the preferences specified.
To address this need, the TCSP framework has been generalized in (Khatib et al., 2001) to
associate each temporal constraint with a preference function which specifies the preference for
each distance allowed by the constraint. This framework merges TCSPs and semiring-based soft
constraints (Bistarelli et al., 1997).
Definition 1 (soft temporal constraint) A soft temporal constraint is a 4-tuple h{X, Y }, I, A, f i
consisting of
• a set of two variables {X, Y } over the integers, called the scope of the constraint;
• a set of disjoint intervals I = {[a1 , b1 ], . . . , [an , bn ]}, where ai , bi ∈ Z, and ai ≤ bi for all
i = 1, . . . , n;
• a set of preferences A;
• a preference function f : I → A, which is a mapping of the elements of I into preference
values, taken from the set A.
Given an assignment of the variables X and Y , X = v x and Y = vy , we say that this assignment
satisfies the constraint h{X, Y }, I, A, f i iff there exists [a i , bi ] ∈ I such that ai ≤ vy − vx ≤ bi . In
such a case, the preference associated with the assignment by the constraint is f (v y − vx ) = p.2
When the variables and the preference set of an STPP are apparent, we will omit them and write
a soft temporal constraint just as a pair hI, f i.
Following the soft constraint approach (Bistarelli et al., 1997), the preference set is the carrier
of an algebraic structure known as a c-semiring. Informally a c-semiring S = hA, +, ×, 0, 1i is
a set equipped with two operators satisfying some proscribed properties (for details, see Bistarelli
et al., 1997)). The additive operator + is used to induce the ordering on the preference set A; given
two elements a, b ∈ A, a ≥ b iff a + b = a. The multiplicative operator × is used to combine
preferences.

621

ROSSI , V ENABLE ,& YORKE -S MITH

Definition 2 (TCSPP) Given a semiring S = hA, +, ×, 0, 1i, a Temporal Constraint Satisfaction
Problems with Preferences (TCSPP) over S is a pair hV, Ci, where V is a set of variables and C is
a set of soft temporal constraint over pairs of variables in V and with preferences in A.2
Definition 3 (solution) Given a TCSPP hV, Ci over a semiring S, a solution is a complete assignment of the variables in V . A solution t is said to satisfy a constraint c in C with preference p if
the projection of t over the pair of variables of c’s scope satisfies c with preference p. We will write
pref (t, c) = p.2
Each solution has a global preference value, obtained by combining, via the × operator, the
preference levels at which the solution satisfies the constraints in C.
Definition 4 (preference of a solution) Given a TCSPP hV, Ci over a semiring S, the preference
of a solution t = hv1 , . . . , vn i, denoted val(t), is computed by Πc∈C pref (s, c).2
The optimal solutions of a TCSPP are those solutions which have the best global preference
value, where “best” is determined by the ordering ≤ of the values in the semiring.
Definition 5 (optimal solutions) Given a TCSPP P = hV, Ci over the semiring S, a solution t of
P is optimal if for every other solution t 0 of P , t0 6≥S t.2
Choosing a specific semiring means selecting a class of preferences. For example, the semiring
SF CSP = h[0, 1], max, min, 0, 1i
allows one to model the so-called fuzzy preferences (Ruttkay, 1994; Schiex, 1992), which associate
to each element allowed by a constraint a preference between 0 and 1 (with 0 being the worst and
1 being the best preferences), and gives to each complete assignment the minimal among all preferences selected in the constraints. The optimal solutions are then those solutions with the maximal
preference. Another example is the semiring S CSP = h{f alse, true}, ∨, ∧, f alse, truei, which
allows one to model classical TCSPs, without preferences, in the more general TCSPP framework.
In this paper we will refer to fuzzy temporal constraints. However, the absence of preferences
in some temporal constraints can always be modelled using just the two elements 0 and 1 in such
constraints. Thus preferences can always coexists with hard constraints.
A special case occurs when each constraint of a TCSPP contains a single interval. In analogy to
what is done in the case without preferences, such problems are called Simple Temporal Problems
with Preferences (STPPs). This class of temporal problems is interesting because, as noted above,
STPs are polynomially solvable while general TCSPs are NP-hard, and the computational effect of
adding preferences to STPs is not immediately obvious.
Example 1 Consider the EOS example given in Section 1. In Figure 1 we show an STPP that
models the scenario in which there are three events to be scheduled on a satellite: the start time (Ss)
and ending time (Se) of a slewing procedure and the starting time (Is) of an image retrieval. The
slewing activity in this example can take from 3 to 10 units of time, ideally between 3 to 5 units of
time, and the shortest time possible otherwise. The image taking can start any time between 3 and
20 units of time after the slewing has been initiated. The third constraint, on variables Is and Se,
models the fact that it is better for the image taking to start as soon as the slewing has stopped.2
622

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

11 1
0.9

0.8
0.7
0.6
0.5

3 4 5 6 7

8 9 10

Ss

Se

1

1 1 1
0.9

0.9
0.8

0.8

0.7

0.7

0.6

0.6
3

20
Is

−4 −3 −2 −1

0 1 2 3

4

5

6

Figure 1: The STPP for Example 1.
In the following example, instead, we consider an STPP which uses the set-based semiring:
Sset = h℘(A), ∪, ∩, ∅, Ai. Notice that, as in the fuzzy semiring, the multiplicative operator, i.e.,
intersection, is idempotent, while the order induced by the additive operator, i.e., union, is partial.
Example 2 Consider a scenario where three friends, Alice, Bob, and Carol, want to meet for a drink
and then for dinner and must decide at what time to meet and where to reserve dinner depending
on how long it takes to get to the restaurant. The variables involved in the problem are: the global
start time X0 , with only the value 0 in its domain, the start time of the drink (Ds), the time to
leave for dinner (De), and the time of arrival at the restaurant (Rs). They can meet, for the drink,
between 8 and 9:00pm and they will leave for dinner after half an hour. Moreover, depending on
the restaurant they choose, it will take from 20 to 40 minutes to get to dinner. Alice prefers to
meet early and have dinner early, like Carol. Bob prefers to meet at 8:30 and to go to the best
restaurant which is the farthest. Thus, we have the following two soft temporal constraints. The first
constraint is defined on the variable pair (X0 , Ds), the interval is [8:00,9:00] and the preference
function, fs , is such that, fs (8 : 00) = {Alice, Carol}, fs (8 : 30) = {Bob} and fs (9 : 00) = ∅.
The second constraint is a binary constraint on pair (De,Rs), with interval [20, 40] and preference
function fse , such that, fse (20) = {Alice, Carol} and fse (20) = ∅ and fse (20) = {Bob}. There
is an additional “hard” constraint on the variable pair (Ds, De), which can be modeled by the
interval [30, 30] and a single preference equal to {Alice, Carol, Bob}. The optimal solution is
(X0 = 0, Ds = 8 : 00, De = 8 : 30, Rs = 8 : 50), with preference {Alice, Carol}. 2
Although both TCSPPs and STPPs are NP-hard, in (Khatib et al., 2001) a tractable subclass of
STPPs is described. The tractability assumptions are: the semi-convexity of preference functions,
the idempotence of the combination operator of the semiring, and a totally ordered preference set.
A preference function f of a soft temporal constraint hI, f i is semi-convex iff for all y ∈ < + , the set
{x ∈ I, f (x) ≥ y} forms an interval. Notice that semi-convex functions include linear, convex, and
also some step functions. The only aggregation operator on a totally ordered set that is idempotent
is min (Dubois & Prade, 1985), i.e. the combination operator of the S F CSP semiring.
If such tractability assumptions are met, STPPs can be solved in polynomial time. In (Rossi
et al., 2002) two polynomial solvers for this tractable subclass of STPPs are proposed. One solver is

623

ROSSI , V ENABLE ,& YORKE -S MITH

based on the extension of path consistency to TCSPPs. The second solver decomposes the problem
into solving a set of hard STPs.
2.3 Simple Temporal Problems with Uncertainty
When reasoning concerns activities that an agent performs interacting with an external world, uncertainty is often unavoidable. TCSPs assume that all activities have durations under the control of
the agent. Simple Temporal Problems with Uncertainty (STPUs) (Vidal & Fargier, 1999) extend
STPs by distinguishing contingent events, whose occurrence is controlled by exogenous factors
often referred to as “Nature”.
As in STPs, activity durations in STPUs are modelled by intervals. The start times of all activities are assumed to be controlled by the agent (this brings no loss of generality). The end times,
however, fall into two classes: requirement (“free” in Vidal & Fargier, 1999) and contingent. The
former, as in STPs, are decided by the agent, but the agent has no control over the latter: it only can
observe their occurrence after the event; observation is supposed to be known immediately after the
event. The only information known prior to observation of a time-point is that nature will respect
the interval on the duration. Durations of contingent links are assumed to be independent.
In an STPU, the variables are thus divided into two sets depending on the type of time-points
they represent.
Definition 6 (variables) The variables of an STPU are divided into:
• executable time-points: are those points, b i , whose time is assigned by the executing agent;
• contingent time-points: are those points, e i , whose time is assigned by the external world.2
The distinction on variables leads to constraints which are also divided into two sets, requirement and contingent, depending on the type of variables they constrain. Note that as in STPs all the
constraints are binary. Formally:
Definition 7 The constraints of an STPU are divided into:
• a requirement constraint (or link) r ij , on generic time-points ti and tj 1 , is an interval Iij =
[lij , uij ] such that lij ≤ γ(tj ) − γ(ti ) ≤ uij where γ(ti ) is a value assigned to variable ti
• a contingent link ghk , on executable point bh and contingent point ek , is an interval Ihk =
[lij , uij ] which contains all the possible durations of the contingent event represented by b h
and ek .2
The formal definition of an STPU is the following:
Definition 8 (STPU) A Simple Temporal Problem with Uncertainty (STPU) is a 4-tuple N =
{Xe , Xc , Rr , Rc } such that:
• Xe = {b1 , . . . , bne }: is the set of executable time-points;
• Xc = {e1 , . . . , enc }: is the set of contingent time-points;
1. In general ti and tj can be either contingent or executable time-points.

624

U NCERTAINTY

Start
Cooking

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

[20,40]
End
Cooking

[0,10]

requirement constr.

[30,60]

contingent constr.

Start
Dinner

contingent timepoint

End
Dinner

executable timepoint

Figure 2: The STPU for Example 3.
• Rr = {ci1 j1 , . . . , ciC jC }: is the set C of requirement constraints;
• Rc = {gi1 j1 , . . . , giG jG }: is the set G of contingent constraints.2
Example 3 This is an example taken from (Vidal & Fargier, 1999), which describes a scenario
which can be modeled using an STPU. Consider two activities Cooking and Having dinner. Assume
you don’t want to eat your dinner cold. Also, assume you can control when you start cooking and
when the dinner starts but not when you finish cooking or when the dinner will be over. The
STPU modeling this example is depicted in Figure 2. There are two executable time-points {Startcooking, Start-dinner} and two contingent time-points {End-cooking, End-dinner}. Moreover, the
contingent constraint on variables {Start-cooking, End-cooking} models the uncontrollable duration
of fixing dinner which can take anywhere from 20 to 40 minutes; the contingent constraint on
variables {Start-dinner, End-dinner} models the uncontrollable duration of the dinner that can last
from 30 to 60 minutes. Finally, there is a requirement constraint on variables {End-cooking, Startdinner} that simply bounds to 10 minutes the time between when the food is ready and when the
dinner starts.2
Assignments to executable variables and assignments to contingent variables are distinguished:
Definition 9 (control sequence) A control sequence δ is an assignment to executable time-points.
It is said to be partial if it assigns values to a proper subset of the executables, otherwise complete.2
Definition 10 (situation) A situation ω is a set of durations on contingent constraints. If not all the
contingent constraints are assigned a duration it is said to be partial, otherwise complete.2
Definition 11 (schedule) A schedule is a complete assignment to all the time-points in X e and Xc .
A schedule T identifies a control sequence, δT , consisting of all the assignments to the executable
time-points, and a situation, ωT , which is the set of all the durations identified by the assignments
in T on the contingent constraints. Sol(P ) denotes the set of all schedules of an STPU.2
It is easy to see that to each situation corresponds an STP. In fact, once the durations of the
contingent constraints are fixed, there is no more uncertainty in the problem, which becomes an
STP, called the underlying STP. This is formalized by the notion of projection.
Definition 12 (projection) A projection P ω , corresponding to a situation ω, is the STP obtained
leaving all requirement constraints unchanged and replacing each contingent constraint g hk with

625

ROSSI , V ENABLE ,& YORKE -S MITH

the constraint h[ωhk , ωhk ]i, where ωhk is the duration of event represented by g hk in ω. P roj(P ) is
the set of all projections of an STPU P .2
2.4 Controllability
It is clear that in order to solve a problem with uncertainty all possible situations must be considered.
The notion of consistency defined for STPs does not apply since it requires the existence of a single
schedule, which is not sufficient in this case since all situations are equally possible.2 For this
reason, in (Vidal & Fargier, 1999), the notion of controllability has been introduced. Controllability
of an STPU is, in some sense, the analogue of consistency of an STP. Controllable means the agent
has a means to execute the time-points under its control, subject to all constraints. The notion
of controllability is expressed, in terms of the ability of the agent to find, given a situation, an
appropriate control sequence. This ability is identified with having a strategy:
Definition 13 (strategy) A strategy S is a map S : P roj(P ) → Sol(P ), such that for every projection Pω , S(Pω ) is a schedule which induces the durations in ω on the contingent constraints.
Further, a strategy is viable if, for every projection P ω , S(Pω ) is a solution of Pω .2
We will write [S(Pω )]x to indicate the value assigned to executable time-point x in schedule
S(Pω ), and [S(Pω )]<x the history of x in S(Pω ), that is, the set of durations of contingent constraints which occurred in S(Pω ) before the execution of x, i.e. the partial solution so far.
In (Vidal & Fargier, 1999), three notions of controllability are introduced for STPUs.
2.4.1 S TRONG C ONTROLLABILITY
The first notion is, as the name suggests, the most restrictive in terms of the requirements that the
control sequence must satisfy.
Definition 14 (Strong Controllability) An STPU P is Strongly Controllable (SC) iff there is an
execution strategy S s.t. ∀Pω ∈ P roj(P ), S(Pω ) is a solution of Pω , and [S(P1 )]x = [S(P2 )]x ,
∀P1 , P2 projections and for every executable time-point x.2
In words, an STPU is strongly controllable if there is a fixed execution strategy that works in all
situations. This means that there is a fixed control sequence that will be consistent with any possible
scenario of the world. Thus, the notion of strong controllability is related to that of conformant
planning. It is clearly a very strong requirement. As Vidal and Fargier (1999) suggest, SC may
be relevant in some applications where the situation is not observable at all or where the complete
control sequence must be known beforehand (for example in cases in which other activities depend
on the control sequence, as in the production planning area).
In (Vidal & Fargier, 1999) a polynomial time algorithm for checking if an STPU is strongly
controllable is proposed. The main idea is to rewrite the STPU given in input as an equivalent STP
only on the executable variables. What is important to notice, for the contents of this paper, is
that algorithm StronglyControllable takes in input an STPU P = {X e , Xc , Rr , Rc } and returns in
output an STP defined on variables Xe . The STPU in input is strongly controllable iff the derived
STP is consistent. Moreover, every solution of the STP is a control sequence which guarantees
2. Tsamardinos (2002) has augmented STPUs to include probability distributions over the possible situations; in this
paper we implicitly assume a uniform, independent distribution on each link.

626

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

strong controllability for the STPU. When the STP is consistent, the output of StronglyControllable
is its minimal form.
In (Vidal & Fargier, 1999) it is shown that the complexity of StronglyControllable is O(n 3 ),
where n is the number of variables.
2.4.2 W EAK C ONTROLLABILITY
On the other hand, the notion of controllability with the fewest restrictions on the control sequences
is Weak Controllability.
Definition 15 (Weak Controllability) An STPU P is said to be Weakly Controllable (WC) iff
∀Pω ∈ P roj(P ) there is a strategy Sω s.t. Sω (Pω ) is a solution of Pω .2
In words, an STPU is weakly controllable if there is a viable global execution strategy: there
exists at least one schedule for every situation. This can be seen as a minimum requirement since,
if this property does not hold, then there are some situations such that there is no way to execute
the controllable events in a consistent way. It also looks attractive since, once an STPU is shown
to WC, as soon as one knows the situation, one can pick out and apply the control sequence that
matches that situation. Unfortunately in (Vidal & Fargier, 1999) it is shown that this property is not
so useful in classical planning. Nonetheless, WC may be relevant in specific applications (as largescale warehouse scheduling) where the actual situation will be totally observable before (possibly
just before) the execution starts, but one wants to know in advance that, whatever the situation, there
will always be at least one feasible control sequence.
In (Vidal & Fargier, 1999) it is conjectured and in (Morris & Muscettola, 1999) it is proven
that the complexity of checking weak controllability is co-NP-hard. The algorithm proposed for
testing WC in (Vidal & Fargier, 1999) is based on a classical enumerative process and a lookahead
technique.
Strong Controllability implies Weak Controllability (Vidal & Fargier, 1999). Moreover, an
STPU can be seen as an STP if the uncertainty is ignored. If enforcing path consistency removes
some elements from the contingent intervals, then these elements belong to no solution. If so, it is
possible to conclude that the STPU is not weakly controllable.
Definition 16 (pseudo-controllability) An STPU is pseudo-controllable if applying path consistency leaves the intervals on the contingent constraints unchanged.2
Unfortunately, if path consistency leaves the contingent intervals untouched, we cannot conclude that the STPU is weakly controllable. That is, WC implies pseudo-controllability but the
converse is false. In fact, weak controllability requires that given any possible combination of durations of all contingent constraints the STP corresponding to that projection must be consistent.
Pseudo-controllability, instead, only guarantees that for each possible duration on a contingent constraint there is at least one projection that contains such a duration and it is a consistent STP.
2.4.3 DYNAMIC C ONTROLLABILITY
In dynamic applications domains, such as planning, the situation is observed over a time. Thus
decisions have to be made even if the situation remains partially unknown. Indeed the distinction
between Strong and Dynamic Controllability is equivalent to that between conformant and conditional planning. The final notion of controllability defined in (Vidal & Fargier, 1999) address this
627

ROSSI , V ENABLE ,& YORKE -S MITH

Pseudocode of DynamicallyControllable
1. input STPU W;
2. If W is not pseudo-controllable then write “not DC” and stop;
3. Select all triangles ABC, C uncontrollable, A before C,
such that the upper bound of the BC interval, v, is non-negative.
4. Introduce any tightenings required by the Precede case
and any waits required by the Unordered case.
5. Do all possible regressions of waits,
while converting unconditional waits to lower bounds.
Also introduce lower bounds as provided by the general reduction.
6. If steps 3 and 4 do not produce any new (or tighter)
constraints, then return true, otherwise go to 2.
Figure 3: Algorithm DynamicallyControllable proposed in (Morris et al., 2001) for checking DC
of an STPU.
[x,y]

A

C

requirement constr.
contingent constr.
contingent timepoint

[p,q]

[u,v]

executable timepoint

B

Figure 4: A triangular STPU.
case. Here we give the definition provided in (Morris et al., 2001) which is equivalent but more
compact.
Definition 17 (Dynamic Controllability) An STPU P is Dynamically Controllable (DC) iff there
is a strategy S such that ∀P1 , P2 in P roj(P ) and for any executable time-point x:
1. if [S(P1 )]<x = [S(P2 )]<x then [S(P1 )]x = [S(P2 )]x ;
2. S(P1 ) is a solution of P1 and S(P2 ) is a solution of P2 .2
In words, an STPU is dynamically controllable if there exists a viable strategy that can be built,
step-by-step, depending only the observed events at each step. SC =⇒ DC and that DC =⇒
WC. Dynamic Controllability, seen as the most useful controllability notion in practice, is also the
one that requires the most complicated algorithm. Surprisingly, Morris et al. (2001) and Morris and
Muscettola (2005) proved DC is polynomial in the size of the STPU representation. In Figure 3 the
pseudocode of algorithm DynamicallyControllable is shown.
In this paper we will extend the notion of dynamic controllability in order to deal with preferences. The algorithm we will propose to test this extended property will require a good (even if not
complete) understanding of the DynamicallyControllable algorithm. Thus, we will now give the
necessary details on this algorithm.
628

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

As it can be seen, the algorithm is based on some considerations on triangles of constraints. The
triangle shown in Figure 4 is a triangular STPU with one contingent constraint, AC, two executable
time-points, A and B, and a contingent time-point C. Based on the sign of u and v, three different
cases can occur:
• Follow case (v < 0): B will always follow C. If the STPU is path consistent then it is also DC
since, given the time at which C occurs after A, by definition of path consistency, it is always
possible to find a consistent value for B.
• Precede case (u ≥ 0): B will always precede or happen simultaneously with C. Then the
STPU is dynamically controllable if y − v ≤ x − u, and the interval [p, q] on AB should be
replaced by interval [y − v, x − u], that is by the sub-interval containing all the elements of
[p, q] that are consistent with each element of [x, y].
• Unordered case (u < 0 and v ≥ 0): B can either follow or precede C. To ensure dynamic
controllability, B must wait either for C to occur first, or for t = y − v units of time to go by
after A. In other words, either C occurs and B can be executed at the first value consistent with
C’s time, or B can safely be executed t units of time after A’s execution. This can be described
by an additional constraint which is expressed as a wait on AB and is written < C, t >, where
t = y − v. Of course if x ≥ y − v then we can raise the lower bound of AB, p, to y − v
(Unconditional Unordered Reduction), and in any case we can raise it to x if x > p (General
Unordered reduction) .
It can be shown that waits can be propagated (in Morris et al., 2001, the term “regressed”is used
) from one constraint to another: a wait on AB induces a wait on another constraint involving A,
e.g. AD, depending on the type of constraint DB. In particular, there are two possible ways in which
the waits can be regressed.
• Regression 1: assume that the AB constraint has a wait hC, ti. Then, if there is any DB
constraint (including AB itself) with an upper bound, w, it is possible to deduce a wait hC, t −
wi on AD. Figure 5(a) shows this type of regression.
• Regression 2: assume that the AB constraint has a wait hC, ti, where t ≥ 0. Then, if there
is a contingent constraint DB with a lower bound, z, and such that B 6= C, it is possible to
deduce a wait hC, t − zi on AD. Figure 5(b) shows this type of regression.
Assume for simplicity and without loss of generality that A is executed at time 0. Then, B
can be executed before the wait only if C is executed first. After the wait expires, B can safely be
executed at any time left in the interval. As Figure 6 shows, it is possible to consider the Follow and
Precede cases as special cases of the Unordered. In the Follow case we can put a “dummy” wait
after the end of the interval, meaning that B must wait for C to be executed in any case (Figure 6
(a)). In the Precede case, we can set a wait that expires at the first element of the interval meaning
that B will be executed before C and any element in the interval will be consistent with C (Figure 6
(b)). The Unordered case can thus be seen as a combination of the two previous states. The part of
the interval before the wait can be seen as a Follow case (in fact, B must wait for C until the wait
expires), while the second part including and following the wait can be seen as a Precede case (after
the wait has expired, B can be executed and any assignment to B that corresponds to an element of
this part of interval AB will be consistent with any possible future value assigned to C).
629

ROSSI , V ENABLE ,& YORKE -S MITH

[x,y]
A

[x,y]
C

Contingent

<C,t>

<C,t−w>

C

A

[u,v]

Contingent

<C,t>

<C,t−z>

[p,q]

D

[u,v]

[p,q]

B

B

D

[z,w]

[z,w]

requirement constr.
contingent constr.
controllable timepoint

requirement constr.
contingent constr.
controllable timepoint

contingent timepoint

contingent timepoint

(a) Regression 1

(b) Regression 2

Figure 5: Regressions in algorithm DynamicallyControllable.

Follow Case

wait for C to be executed
wait

execute regardless C

Precede Case

wait

Unordered Case

wait fo C to be executed

execute regardless C

wait

Figure 6: The resulting AB interval constraint in the three cases considered by the
DynamicallyControllable algorithm.

The DynamicallyControllable algorithm applies these rules to all triangles in the STPU and
regresses all possible waits. If no inconsistency is found, that is no requirement interval becomes
empty and no contingent interval is squeezed, the STPU is DC and the algorithm returns an STPU
where some constraints may have waits to satisfy, and the intervals contain elements that appear
in at least one possible dynamic strategy. This STPU can then be given to an execution algorithm
which dynamically assigns values to the executables according to the current situation.
The pseudocode of the execution algorithm, DC-Execute, is shown in Figure 7. The execution
algorithm observes, as the time goes by, the occurrence of the contingent events and accordingly
executes the controllables. For any controllable B, its execution is triggered if it is (1) live, that
630

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

Pseudocode for DC-Execute
1. input STPU P ;
2. Perform initial propagation from the start time-point;
3. repeat
4. immediately execute any executable time-points that
have reached their upper bounds;
5. arbitrarily pick an executable time-point x that
is live and enabled and not yet executed, and whose waits,
if any, have all been satisfied;
6. execute x;
7. propagate the effect of the execution;
8. if network execution is complete then return;
9. else advance current time,
propagating the effect of any contingent time-points that occur;
10. until false;
Figure 7: Algorithm that executes a dynamic strategy for an STPU.
is, if current time is within its bounds, it is (2) enabled, that is, all the executables constrained to
happen before have occurred, and (3) all the waits imposed by the contingent time-points on B have
expired.
DC-Execute produces dynamically a consistent schedule on every STPU on which algorithm
DynamicallyControllable reports success (Morris et al., 2001). The complexity of the algorithm is
O(n3 r), where n is the number of variables and r is the number of elements in an interval. Since the
polynomial complexity relies on the assumption of a bounded maximum interval size, Morris et al.
(2001) conclude that DynamicallyControllable is pseudo-polynomial. A DC algorithm of “strong”
polynomial complexity is presented in (Morris & Muscettola, 2005). The new algorithm differs
from the previous one mainly because it manipulates the distance graph rather than the constraint
graph of the STPU. It’s complexity is O(n 5 ). What is important to notice for our purposes is that,
from the distance graph produced in output by the new algorithm, it is possible to directly recover the
intervals and waits of the STPU produced in output by the original algorithm described in (Morris
et al., 2001).

3. Simple Temporal Problems with Preferences and Uncertainty (STPPUs)
Consider a temporal problem that we would model naturally with preferences in addition to hard
constraints, but one also features uncertainty. Neither an STPP nor an STPU is adequate to model
such a problem. Therefore we propose what we will call Simple Temporal Problems with Preferences and Uncertainty, or STPPUs for short.
Intuitively, an STPPU is an STPP for which time-points are partitioned into two classes, requirement and contingent, just as in an STPU. Since some time-points are not controllable by the
agent, the notion of consistency of an STP(P) is replaced by that of controllability, just as in an
STPU. Every solution to the STPPU has a global preference value, just as in an STPP, and we seek
a solution which maximizes this value, while satisfying controllability requirements.

631

ROSSI , V ENABLE ,& YORKE -S MITH

More precisely, we can extend some definitions given for STPPs and STPUs to fit STPPUs in
the following way.
Definition 18 In a context with preferences:
• an executable time-point is a variable, x i , whose time is assigned by the agent;
• a contingent time-point is a variable, e i , whose time is assigned by the external world;
• a soft requirement link rij , on generic time-points ti and tj 3 , is a pair hIij , fij i, where Iij =
[lij , uij ] such that lij ≤ γ(tj ) − γ(ti ) ≤ uij where γ(ti ) is a value assigned to variable ti , and
fij : Iij → A is a preference function mapping each element of the interval into an element
of the preference set, A, of the semiring S = hA, +, ×, 0, 1i;
• a soft contingent link ghk , on executable point bh and contingent point ek , is a pair hIhk , fhk i
where interval Ihk = [lhk , uhk ] contains all the possible durations of the contingent event
represented by bh and ek and fhk : Ihk → A is a preference function that maps each element
of the interval into an element of the preference set A.2
In both types of constraints, the preference function represents the preference of the agent on
the duration of an event or on the distance between two events. However, while for soft requirement
constraints the agent has control and can be guided by the preferences in choosing values for the
time-points, for soft contingent constraints the preference represents merely a desire of the agent
on the possible outcomes of Nature: there is no control on the outcomes. It should be noticed that
in STPPUs uncertainty is modeled, just like in STPUs, assuming “complete ignorance” on when
events are more likely to happen. Thus, all durations of contingent events are assumed to be equally
possible (or plausible) and different levels of plausibility are not allowed.
We can now state formally the definition of STPPUs, which combines preferences from the
definition of an STPP with contingency from the definition of an STPU.
Definition 19 (STPPU) A Simple Temporal Problem with Preferences and Uncertainty (STPPU)
is a tuple P = (Ne , Nc , Lr , Lc , S) where:
• Ne is the set of executable time-points;
• Nc is the set of contingent time-points;
• S = hA, +, ×, 0, 1i is a c-semiring;
• Lr is the set of soft requirement constraints over S;
• Lc is the set of soft contingent constraints over S.2
Note that, as STPPs, also STPPUs can model hard constraints by soft constraints in which each
element of the interval is mapped into the maximal element of the preference set. Further, without
loss of generality, and following the assumptions made for STPUs (Morris et al., 2001), we assume
that no two contingent constraints end at the same time-point.
3. Again, in general ti and tj can be either contingent or executable time-points.

632

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

Once we have a complete assignment to all time-points we can compute its global preference,
as in STPPs. This is done according to the semiring-based soft constraint schema: first we project
the assignment on each soft constraint, obtaining an element of the interval and the preference
associated to that element; then we combine the preferences obtained on all constraints with the
multiplicative operator of the semiring. Given two assignments with their preference, the best is
chosen using the additive operator. An assignment is optimal if there is no other assignment with a
preference which is better in the semiring’s ordering.
In the following we summarize some of the definitions given for STPUs, extending them directly
to STPPUs.
Definition 20 Given an STPPU P :
• A schedule is a complete assignment to all the time-points in N e and Nc ;
• Sched(P) is the set of all schedules of P ; while Sol(P) the set of all schedules of P that are
consistent with all the constraints of P (see Definition 1, Section 2.2);
• Given a schedule s for P , a situation (usually written ω s ) is the set of durations of all contingent constraints in s;
• Given a schedule s for P , a control sequence (usually written δ s is the set of assignments to
executable time-points in s;
• Tδ,ω is a schedule such that [Tδ,ω ]x = [δ]x 4 , ∀x ∈ Ne , and for every contingent constraint,
ghk ∈ Lc , defined on executable bh and contingent time-point ek , [Tδ,ω ]ek -[Tδ,ω ]bh = ωhk ,
where ωhk is the duration of ghk in ω;
• A projection Pω corresponding to a situation ω is the STPP obtained from P by leaving all
requirement constraints unchanged and replacing each contingent constraint g hk with the soft
constraint h[ωhk , ωhk ], f (ωhk )i, where ωhk is the duration of the event represented by g hk in
ω, and f (ωhk ) is the preference associated to such duration;
• Given a projection Pω we indicate with Sol(Pω ) the set of solutions of Pω and we define
OptSol(Pω ) = {s ∈ Sol(Pω )| 6 ∃s0 ∈ Sol(Pω ), pref (s0 ) > pref (s)}; if the set of preferences is totally ordered we indicate with opt(P ω ) the preference of any optimal solution of
Pω ;
• Proj(P) is the set of all projections of an STPPU P;
• A strategy s is a map s : P roj(P ) → Sched(P ) such that for every projection P ω , s(Pω ) is
a schedule which includes ω;
• A strategy is viable if ∀ω, S(Pω ) is a solution of Pω , that is, if it satisfies all its soft temporal
constraints. Thus a viable strategy is a mapping S : P roj(P ) −→ Sol(P ). In this case
we indicate with pref (S(Pω )) the global preference associated to schedule S(P ω ) in STPP
Pω .2
4. Regarding notation, as in the case with hard constraints, given an executable time-point x, we will write [S(Pω )]x
to indicate the value assigned to x in S(Pω ), and [S(Pω )]<x to indicate the durations of the contingent events that
finish prior to x in S(Pω ).

633

ROSSI , V ENABLE ,& YORKE -S MITH

1

0.9
0.5

x=1

8=y

EC

SC
1

1

0.9

p=1

0.9

0.6

0.6

u=−6

5=q

4=v

SA
requirement constr.

1

contingent constr.

0.6

contingent timepoint

0.8

EA

executable timepoint

s=2

5=t

Figure 8: Example STPPU from the Earth Observing Satellites domain.
Example 4 Consider as an example the following scenario from the Earth Observing Satellites
domain (Frank et al., 2001) described in Section 1. Suppose a request for observing a region of
interest has been received and accepted. To collect the data, the instrument must be aimed at the
target before images can be taken. It might be, however, that for a certain period during the time
window allocated for this observation, the region of interest is covered by clouds. The earlier the
cloud coverage ends the better, since it will maximise both the quality and the quantity of retrieved
data; but coverage is not controllable.
Suppose the time window reserved for an observation is from 1 to 8 units of time and that we
start counting time when the cloud occlusion on the region of interest is observable. Also, suppose,
in order for the observation to succeed, the aiming procedure must start before 5 units after the
starting time, ideally before 3 units, and it actually can only begin after at least 1 time unit after
the weather becomes observable. Ideally the aiming procedure should start slightly after the cloud
coverage will end. If it starts too early, then, since the instrument is activated immediately after it is
aimed, clouds might still occlude the region and the image quality will be poor. On the other hand,
if it waits too long after the clouds have disappeared then precious time during which there is no
occlusion will be wasted aiming the instrument instead of taking images. The aiming procedure can
be controlled by the mission manager and it can take anywhere between 2 and 5 units of time. An
ideal duration is 3 or 4 units, since a short time of 2 units would put the instrument under pressure,
while a long duration, like 5 units, would waste energy.
This scenario, rather tedious to describe in words, can be compactly represented by the STPPU
shown in Figure 8 with the following features:
• a set of executable time-points SC (Start Clouds), SA (Start Aiming), EA (End Aiming);
• a contingent time-point EC (End Clouds);
• a set of soft requirement constraints on {SC → SA, SA → EC, SA → EA};
• a soft contingent constraint {SC → EC};

634

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

• the fuzzy semiring SFCSP = h[0, 1], max, min, 0, 1i.
A solution of the STPPU in Figure 8 is the schedule s = {SC = 0, SA = 2, EC = 5, EA = 7}.
The situation associated with s is the projection on the only contingent constraint, SC → EC,
i.e. ωs = 5, while the control sequence is the assignment to the executable time-points, i.e. δ s =
{SC = 0, SA = 2, EA = 7}. The global preference is obtained by considering the preferences
associated with the projections on all constraints, that is pref(2) = 1 on SC → SA, pref(3) = 0.6
on SA → EC, pref(5) = 0.9 on SA → EA, and pref(5) = 0.8 on SC → EC. The preferences
must then be combined using the multiplicative operator of the semiring, which is min, so the
global preference of s is 0.6. Another solution s 0 = {SC = 0, SA = 4, EC = 5, EA = 9} has
global preference 0.8. Thus s0 is a better solution than s according to the semiring ordering since
max(0.6, 0.8) = 0.8.2

4. Controllability with Preferences
We now consider how it is possible to extend the notion of controllability to accommodate preferences. In general we are interested in the ability of the agent to execute the time-points under its
control, not only subject to all constraints but also in the best possible way with respect to preferences.
It transpires that the meaning of ‘best possible way’ depends on the types of controllability
required. In particular, the concept of optimality must be reinterpreted due to the presence of uncontrollable events. In fact, the distinction on the nature of the events induces a difference on the
meaning of the preferences expressed on them, as mentioned in the previous section. Once a scenario is given it will have a certain level of desirability, expressing how much the agent likes such a
situation. Then, the agent often has several choices for the events he controls that are consistent with
that scenario. Some of these choices might be preferable with respect to others. This is expressed
by the preferences on the requirement constraints and such information should guide the agent in
choosing the best possible actions to take. Thus, the concept of optimality is now ‘relative’ to the
specific scenario. The final preference of a complete assignment is an overall value which combines
how much the corresponding scenario is desirable for the agent and how well the agent has reacted
in that scenario.
The concepts of controllability we will propose here are, thus, based on the possibility of the
agent to execute the events under her control in the best possible way given the actual situation. Acting in an optimal way can be seen as not lowering further the preference given by the uncontrollable
events.
4.1 Strong Controllability with Preferences
We start by considering the strongest notion of controllability. We extend this notion, taking into
account preferences, in two ways, obtaining Optimal Strong Controllability and α-Strong Controllability, where α ∈ A is a preference level. As we will see, the first notion corresponds to a stronger
requirement, since it assumes the existence of a fixed unique assignment for all the executable timepoints that is optimal in every projection. The second notion requires such a fixed assignment to be
optimal only in those projections that have a maximum preference value not greater than α, and to
yield a preference 6< α in all other cases.

635

ROSSI , V ENABLE ,& YORKE -S MITH

Definition 21 (Optimal Strong Controllability) An STPPU P is Optimally Strongly Controllable
(OSC) iff there is a viable execution strategy S s.t.
1. [S(P1 )]x = [S(P2 )]x , ∀P1 , P2 ∈ P roj(P ) and for every executable time-point x;
2. S(Pω ) ∈ OptSol(Pω ), ∀Pω ∈ P roj(P ). 2
In other words, an STPPU is OSC if there is a fixed control sequence that works in all possible
situations and is optimal in each of them. In the definition, ‘optimal’ means that there is no other
assignment the agent can choose for the executable time-points that could yield a higher preference
in any situation. Since this is a powerful restriction, as mentioned before, we can instead look at
just reaching a certain quality threshold:
Definition 22 (α-Strong Controllability) An STPPU P is α-Strongly Controllable (α-SC), with
α ∈ A a preference, iff there is a viable strategy S s.t.
1. [S(P1 )]x = [S(P2 )]x , ∀P1 , P2 ∈ P roj(P ) and for every executable time-point x;
2. S(Pω ) ∈ OptSol(Pω ),∀Pω ∈ P roj(P ) such that 6 ∃s0 ∈ OptSol(Pω ) with pref (s0 ) > α;
3. pref (S(Pω )) 6< α otherwise.2
In other words, an STPPU is α-SC if there is a fixed control sequence that works in all situations and results in optimal schedules for those situations where the optimal preference level of the
projection is not > α in a schedule with preference not smaller than α in all other cases.
4.2 Weak Controllability with Preferences
Secondly, we extend similarly the least restrictive notion of controllability. Weak Controllability requires the existence of a solution in any possible situation, possibly a different one in each situation.
We extend this definition by requiring the existence of an optimal solution in every situation.
Definition 23 (Optimal Weak Controllability) An STPPU P is Optimally Weakly Controllable
(OWC) iff ∀Pω ∈ P roj(P ) there is a strategy Sω s.t. Sω (Pω ) is an optimal solution of Pω .2
In other words, an STPPU is OWC if, for every situation, there is a control sequence that results
in an optimal schedule for that situation.
Optimal Weak Controllability of an STPPU is equivalent to Weak Controllability of the corresponding STPU obtained by ignoring preferences, as we will formally prove in Section 6. The
reason is that if a projection Pω has at least one solution then it must have an optimal solution.
Moreover, any STPPU is such that its underlying STPU is either WC or not. Hence it does not
make sense to define a notion of α-Weak Controllability.
4.3 Dynamic Controllability with Preferences
Dynamic Controllability (DC) addresses the ability of the agent to execute a schedule by choosing
incrementally the values to be assigned to executable time-points, looking only at the past. When
preferences are available, it is desirable that the agent acts not only in a way that is guaranteed to
be consistent with any possible future outcome but also in a way that ensures the absence of regrets
w.r.t. preferences.
636

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

Definition 24 (Optimal Dynamic Controllability) An STPPU P is Optimally Dynamically Controllable (ODC) iff there is a viable strategy S such that ∀P 1 , P2 ∈ P roj(P ) and for any executable
time-point x:
1. if [S(P1 )]<x = [S(P2 )]<x then [S(P1 )]x = [S(P2 )]x ;
2. S(P1 ) ∈ OptSol(P1 ) and S(P2 ) = OptSol(P2 ).2
In other words, an STPPU is ODC if there exists a means of extending any current partial control
sequence to a complete control sequence in the future in such a way that the resulting schedule will
be optimal. As before, we also soften the optimality requirement to having a preference reaching a
certain threshold.
Definition 25 (α-Dynamic Controllability) An STPPU P is α-Dynamically Controllable (α-DC)
iff there is a viable strategy S such that ∀P 1 , P2 ∈ P roj(P ) and for every executable time-point x:
1. if [S(P1 )]<x = [S(P2 )]<x then [S(P1 )]x = [S(P2 )]x ;
2. S(P1 ) ∈ OptSol(P1 ) and S(P2 ) ∈ OptSol(P2 ) if 6 ∃s1 ∈ OptSol(P1 ) with pref (s1 ) > α
and 6 ∃s2 ∈ OptSol(P2 ) with pref (s2 ) > α;
3. pref(S(P1 )) 6< α and pref(S(P2 )) 6< α otherwise.2
In other words, an STPPU is α-DC if there is a means of extending any current partial control
sequence to a complete sequence; but optimality is guaranteed only for situations with preference
6> α. For all other projections the resulting dynamic schedule will have preference at not smaller
than α.
4.4 Comparing the Controllability Notions
We will now consider the relation among the different notions of controllability for STPPUs.
Recall that for STPUs, SC =⇒ DC =⇒ W C (see Section 2). We start by giving a similar
result that holds for the definitions of optimal controllability with preferences. Intuitively, if there
is a single control sequence that will be optimal in all situations, then clearly it can be executed
dynamically, just assigning the values in the control sequence when the current time reaches them.
Moreover if, whatever the final situation will be, we know we can consistently assign values to
executables, just looking at the past assignments, and never having to backtrack on preferences,
then it is clear that every situation has at least an optimal solution.
Theorem 1 If an STPPU P is OSC, then it is ODC; if it is ODC, then it is OWC.
Proofs of theorems are given in the appendix. The opposite implications of Theorem 1 do
not hold in general. It is in fact sufficient to recall that hard constraints are a special case of soft
constraints and to use the known result for STPUs (Morris et al., 2001).
As examples consider the following two, both defined on the fuzzy semiring. Figure 9 shows
an STPPU which is OWC but is not ODC. It is, in fact, easy to see that any assignment to A and C,
which is a projection of the STPPU can be consistently extended to an assignment of B. However,
we will show in Section 7 that the STPPU depicted is not ODC.
637

ROSSI , V ENABLE ,& YORKE -S MITH

11 1
0.9

0.8
0.7
0.6
0.5

x=3 4

5 6

A

7

8

9 10=y

C

contingent

1
0.9

1 1 1
0.9

0.8

0.9
0.8

0.8

0.7

0.7

0.7

0.6

0.6

0.6
p=3 4 5

6 7=q

requirement constr.

u=−4 −3 −2 −1

B

contingent constr.

0 1

2

3

contingent timepoint
executable timepoint

Figure 9: An STPPU which is OWC but not ODC.

1

1

A

2

C

1

2

1

3

B

−1

requirement constr.
contingent constr.
contingent timepoint
executable timepoint

Figure 10: An STPPU which is ODC but not OSC.

638

4

5

6=v

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

Figure 10, instead, shows an ODC STPPU which is not OSC. A and B are two executable timepoints and C is a contingent time-point. There are only two projections, say P 1 and P2 , corresponding respectively to point 1 and point 2 in the AC interval. The optimal preference level for both is 1.
In fact, hA = 0, C = 1, B = 2i is a solution of P 1 with preference 1 and hA = 0, C = 2, B = 3i is
a solution of P2 with preference 1. The STPPU is ODC. In fact, there is a dynamic strategy S that
assigns to B value 2, if C occurs at 1, and value 3, if C occurs at 2 (assuming A is always assigned
0). However there is no single value for B that will be optimal in both scenarios.
Similar results apply in the case of α-controllability, as the following formal treatment shows.
Theorem 2 For any given preference level α, if an STPPU P is α-SC then it is α-DC.
Again, the converse does not hold in general. As an example consider again the STPPU shown
in Figure 10 and α = 1. Assuming α = 1, such an STPPU is 1-DC but, as we have shown above, it
is not 1-SC.
Another useful result is that if a controllability property holds at a given preference level, say β,
then it holds also ∀α < β, as stated in the following theorem.
Theorem 3 Given an STPPU P and a preference level β, if P is β-SC (resp. β-DC), then it is α-SC
(resp. α-DC), ∀α < β.
Let us now consider case in which the preference set is totally ordered. If we eliminate the
uncertainty in an STPPU, by regarding all contingent time-points as executables, we obtain an
STPP. Such an STPP can be solved obtaining its optimal preference value opt. This preference level,
opt, will be useful to relate optimal controllability to α-controllability. As stated in the following
theorem, an STPPU is optimally strongly or dynamically controllable if and only if it satisfies the
corresponding notion of α-controllability at α = opt.
Theorem 4 Given an STPPU P defined on a c-semiring with totally ordered preferences, let opt =
maxT ∈Sol(P ) pref (T ). Then, P is OSC (resp. ODC) iff it is opt-SC (resp. opt-DC).
For OWC, we will formally prove in Section 6 that an STPPU is OWC iff the STPU obtained
by ignoring the preference functions is WC. As for the relation between α min -controllability and
controllability without preferences, we recall that considering the elements of the intervals mapped
in a preference ≥ αmin coincides by definition to considering the underlying STPU obtained by
ignoring the preference functions of the STPPU. Thus, α min -X holds iff X holds, where X is either
SC or DC.
In Figure 11 we summarize the relationships holding among the various controllability notions
when preferences are totally ordered. When instead they are partially ordered, the relationships
opt − X and αmin − X, where X is a controllability notion, do not make sense. In fact, in the
partially ordered case, there can be several optimal elements and several minimal elements, not just
one.

5. Determining Optimal Strong Controllability and α-Strong Controllability
In the next sections we give methods to determine which levels of controllability hold for an STPPU.
Strong Controllability fits when off-line scheduling is allowed, in the sense that the fixed optimal
control sequence is computed before execution begins. This approach is reasonable if the planning
639

ROSSI , V ENABLE ,& YORKE -S MITH

OSC o


ODC o

/ opt-SC

/ opt-DC

/ α-SC


/ α-DC

/ αmin -SC o

/ αmin -DC o



OWC o

/ SC


/ DC


/ WC

Figure 11: Comparison of controllability notions for total orders. α min is the smallest preference
over any constraint: opt ≥ α ≥ αmin .
algorithm has no knowledge on the possible outcomes, other than the agent’s preferences. Such a
situation requires us to find a fixed way to execute controllable events that will be consistent with
any possible outcome of the uncontrollables and that will give the best possible final preference.
5.1 Algorithm Best-SC
The algorithm described in this section checks whether an STPPU is OSC. If it is not OSC, the
algorithm will detect this and will also return the highest preference level α such that the problem
is α-SC.
All the algorithms we will present in this paper rely on the following tractability assumptions,
inherited from STPPs: (1) the underlying semiring is the fuzzy semiring S F CSP defined in Section 2.2, (2) the preference functions are semi-convex, and (3) the set of preferences [0, 1] is discretized in a finite number of elements according to a given granularity.
The algorithm Best-SC is based on a simple idea: for each preference level β, it finds all the
control sequences that guarantee strong controllability for all projections such that their optimal
preference is ≥ β, and optimality for those with optimal preference β. Then, it keeps only those
control sequences that do the same for all preference levels > β.
The pseudocode is shown in Figure 12. The algorithm takes in input an STPPU P (line 1). As a
first step, the lowest preference αmin is computed. Notice that, to do this efficiently, the analytical
structure of the preference functions (semi-convexity) can be exploited.
In line 3 the STPU obtained from P by cutting it at preference level α min is considered. Such
STPU is obtained by applying function α min -Cut(STPPU G) with G=P 5 . In general, the result of
β-Cut(P ) is the STPU Qβ (i.e., a temporal problem with uncertainty but not preferences) defined
as follows:
• Qβ has the same variables with the same domains as in P;
• for every soft temporal constraint (requirement or contingent) in P on variables X i , and Xj ,
say c = hI, f i, there is, in Qβ , a simple temporal constraint on the same variables defined as
{x ∈ I|f (x) ≥ β}.
Notice that the semi-convexity of the preference functions guarantees that the set {x ∈ I|f (x) ≥ β}
forms an interval. The intervals in Q β contain all the durations of requirement and contingent events
that have a local preference of at least β.
5. Notice that function β-Cut can be applied to both STPPs and STPPUs: in the first case the output is an STP, while in
the latter case an STPU. Notice also that, α-Cut is a known concept in fuzzy literature.

640

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

Pseudocode for Best-SC
1. input STPPU P ;
2. compute αmin ;
3. STPU Qαmin ← αmin -Cut(P );
4. if (StronglyControllable (Qαmin ) inconsistent) write “not αmin -SC” and stop;
5. else {
6. STP P αmin ← StronglyControllable (Qαmin );
7. preference β ← αmin + 1;
8. bool OSC←false, bool α-SC←false;
9. do {
10.
STPU Qβ ← β-Cut(P );
11.
if (PC(Qβ ) inconsistent) OSC←true;
12.
else {
13.
if (StronglyControllable(PC(Q β )) inconsistent) α-SC← true;
14.
else {
N
15.
STP P β ← P β−1
StronglyControllable(PC(Qβ )) ;
β
16.
if (P inconsistent) { α-SC ← true };
17.
else { β ← β + 1 };
18.
}
19.
}
20.
}while (OSC=false and α-SC=false);
21. if (OSC=true) write “P is OSC”;
22. if (α-SC=true) write “P is” (β − 1) ”-SC”;
23. se =Earliest-Solution(P β−1 ), sl =Latest-Solution(P β−1 );
24. return P β−1 , se , sl ;
25. };
Figure 12: Algorithm Best-SC: it tests if an STPPU is OSC and finds the highest α such that
STPPU P is α-SC.

641

ROSSI , V ENABLE ,& YORKE -S MITH

Once STPU Qαmin is obtained, the algorithm checks if it is strongly controllable. If the STP
obtained applying algorithm StronglyControllable (Vidal & Fargier, 1999) to STPU Q αmin is not
consistent, then, according to Theorem 3, there is no hope for any higher preference, and the algorithm can stop (line 4), reporting that the STPPU is not α-SC ∀α ≥ 0 and thus is not OSC as well.
If, instead, no inconsistency is found, Best-SC stores the resulting STP (lines 5-6) and proceeds
moving to the next preference level α min + 1 6 (line 7).
In the remaining part of the algorithm (lines 9-21), three steps are performed at each preference
level considered:
• Cut STPPU P and obtain STPU Qβ (line 10);
• Apply path consistency to Qβ considering it as an STP: PC(Qβ ) (line 11);
• Apply strong controllability to STPU PC(Q β ) (line 13).
Let us now consider the last two steps in detail.
Applying path consistency to STPU Q β means considering it as an STP, that is, treating contingent constraints as requirement constraints. We denote as algorithm PC any algorithm enforcing
path-consistency on the temporal network (see Section 2.1 and Dechter et al., 1991). It returns the
minimal network leaving in the intervals only values that are contained in at least one solution. This
allows us to identify all the situations, ω, that correspond to contingent durations that locally have
preference ≥ β and that are consistent with at least one control sequence of elements in Q β . In
other words, applying path consistency to Q β leaves in the contingent intervals only durations that
belong to situations such that the corresponding projections have optimal value at least β. If such
a test gives an inconsistency, it means that the given STPU, seen as an STP, has no solution, and
hence that all the projections corresponding to scenarios of STPPU P have optimal preference < β
(line 11).
The third and last step applies StronglyControllable to path-consistent STPU PC(Q β ), reintroducing the information on uncertainty on the contingent constraints. Recall that the algorithm
rewrites all the contingent constraints in terms of constraints on only executable time-points. If the
STPU is strongly controllable, StronglyControllable will leave in the requirement intervals only
elements that identify control sequences that are consistent with any possible situation. In our case,
applying StronglyControllable to PC(Q β ) will find, if any, all the control sequences of PC(Qβ )
that are consistent with any possible situation in PC(Q β ).
However, if STPU PC(Qβ ) is strongly controllable, some of the control sequences found might
not be optimal for scenarios with optimal preference lower than β. In order to keep only those
control sequences that guarantee optimal strong controllability for all preference levels up to β, the
STP obtained by StronglyControllable(PC(Q β )) is intersected with the corresponding STP found
in the previous step (at preference level β − 1), that is P β−1 (line 15). We recall that given two
two STPs, P1 and P2 , defined on the same set of variables, the STP P3 = P1 ⊗ P2 has the same
variables as P1 and P2 and each temporal constraint, c3ij = c1ij ⊗ c2ij , is the intersection of the
corresponding intervals of P1 and P2 . If the intersection becomes empty on some constraint or the
STP obtained is inconsistent, we can conclude that there is no control sequence that will guarantee
strong controllability and optimality for preference level β and, at the same time, for all preferences
6. By writing αmin + 1 we mean the next preference level higher than αmin defined in terms of the granularity of the
preferences in the [0,1] interval.

642

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

Table 1: In this table each row corresponds to a preference level β and represents the intervals of
STPU Qβ obtained by cutting the STPPU in Figure 8 at level β.
STPU

(SC → EC)

(SC → SA)

(SA → EC)

Q0.5
Q0.6
Q0.7
Q0.8
Q0.9
Q1

[1, 8]
[1, 7]
[1, 6]
[1, 5]
[1, 4]
[1, 2]

[1, 5]
[1, 5]
[1, 5]
[1, 5]
[1, 5]
[1, 3]

[−6, 4]
[−6, 4]
[−5, 2]
[−4, 1]
[−3, 0]
[−2, −1]

Table 2: In this table each row corresponds to a preference level β and represents the intervals of
STPU PC(Qβ ) obtained applying path consistency to the STPUs in Table 1.
STPU
0.5

PC(Q )
PC(Q0.6 )
PC(Q0.7 )
PC(Q0.8 )
PC(Q0.9 )
PC(Q1 )

(SC → EC)

(SC → SA)

(SA → EC)

[1, 8]
[1, 7]
[1, 6]
[1, 5]
[1, 4]
[1, 2]

[1, 5]
[1, 5]
[1, 5]
[1, 5]
[1, 5]
[2, 3]

[−4, 4]
[−4, 4]
[−4, 2]
[−4, 1]
[−3, 0]
[−2, −1]

< β (line 16). If, instead, the STP obtained is consistent, algorithm Best-SC considers the next
preference level, β + 1, and performs the three steps again.
The output of the algorithm is the STP, P β−1 , obtained in the iteration previous to the one
causing the execution to stop (lines 23-24) and two of its solutions, s e and sl . This STP, as we will
show shortly, contains all the control sequences that guarantee α-SC up to α = β − 1. Only if
β − 1 is the highest preference level at which cutting gives a consistent problem, then the STPPU is
OSC. The solutions provided by the algorithm are respectively the the earliest, s e , and the latest, sl ,
solutions of P β−1 . In fact, as proved in (Dechter et al., 1991) and mentioned in Section 2.1, since
P β−1 is minimal, the earliest (resp. latest) solution corresponds to assigning to each variable the
lower (resp. upper) bound of the interval on the constraint defined on X0 and the variable. This is
indicated in the algorithm by procedures Earliest-Solution and Latest-Solution. Let us also recall
that every other solution can be found from P β−1 without backtracking.
Before formally proving the correctness of algorithm Best-SC, we give an example.
Example 5 Consider the STPPU described in Example 4, and depicted in Figure 8. For simplicity
we focus on the triangular sub-problem on variables SC, SA, and EC. In their example, α min = 0.5.
Table 1 shows the STPUs Qβ obtained cutting the problem at each preference level β = 0.5, . . . , 1.
Table 2 shows the result of applying path consistency (line 11) to each of the STPUs shown in
Table 1. As can be seen, all of the STPUs are consistent. Finally, Table 3 shows the STPs defined
only on executable variables, SC and SA, that are obtained applying StronglyControllable to the
STPUs in Table 2.

643

ROSSI , V ENABLE ,& YORKE -S MITH

Table 3: In this table each row corresponds to a preference level β and represents the intervals of
STP StronglyControllable PC(Qβ ) obtained applying the strong controllability check to
the STPUs in Table 2.
(SC → SA)

STP
0.5

StronglyControllable(PC(Q ))
StronglyControllable(PC(Q0.6 ))
StronglyControllable(PC(Q0.7 ))
StronglyControllable(PC(Q0.8 ))
StronglyControllable(PC(Q0.9 ))
StronglyControllable(PC(Q1 ))

[4, 5]
[3, 5]
[4, 5]
[4, 5]
[4, 4]
[3, 3]

By looking at Tables 2 and 3 it is easy to deduce that the Best-SC will stop at preference level
1. In fact, by looking more carefully at Table 3, we can see that STP P 0.9 consists of interval [4, 4]
on the constraint SC → SA, while StronglyControllable(PC(Q 1 )) consist of interval [3, 3] on the
same constraint. Obviously intersecting the two gives an inconsistency, causing the condition in
line 16 of Figure 12 to be satisfied.
The conclusion of executing Best-SC on the example depicted in Figure8 is that it is 0.9-SC
but not OSC. Let us now see why this is correct. Without loss of generality we can assume that SC
is assigned value 0. From the last line of Table 3 observe that the only value that can be assigned to
SA that is optimal with both scenarios that have optimal preference 1 (that is when EC is assigned
1 or 2) is 3. However, assigning 3 to SA is not optimal if EC happens at 6, since this scenario has
optimal preference value 0.7 (e.g. if SA is assigned 5) while in this case it would have a global
preference 0.6 (given in constraint SA → EC) 7 .2
5.2 Properties of Best-SC
We will now prove that algorithm Best-SC correctly identifies whether an STPPU P is OSC, and,
if not, finds the highest preference level at which P is α-SC. Let us first consider the events in which
Best-SC stops.
• Event 1. StronglyControllable(Q αmin ) is inconsistent (line 4);
• Event 2. PC(Qγ ) returns an inconsistency (line 11);
• Event 3. PC(Qγ ) is consistent but it is not strongly controllable (line 13);
• Event 4. PC(Qγ ) is strongly controllable, however the intersection of the STP obtained
by StronglyControllable(PC(Qγ )) with the STP obtained at the previous preference level,
P γ−1 , is inconsistent (line 16).
First notice that the algorithm terminates.
Theorem 5 Given any STPPU P with a finite number of preference levels, the execution of algorithm Best-SC over P terminates.
7. Recall that in the fuzzy semiring context the global preference of any assignment is computed taking the minimum
preference assigned to any of its projections.

644

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

Intuitively, either one of the termination events occur or all the preference levels will be exhausted.
Next, let us show that Best-DC is a sound and complete algorithm for checking if an STPPU is
OSC and for finding the highest preference level at which it is α-SC.
As we have said before, cutting an STPPU P at a preference level γ gives an STPU Q γ .
Moreover, every situation ω = {ω1 , . . . , ωl } of Qγ can be seen as a situation of P such that
fj (ωj ) ≥ γ, ∀j. This implies that every projection P ω ∈ P roj(Qγ ), which is an STP, corresponds to a projection Pω ∈ P roj(P ) which is an STPP. For all situations ω of Q γ , in what follows
we will write always Pω which should be interpreted as an STP when seen as a projection of Q γ
and as an STPP when seen as a projection of P . In the following lemmas we state properties which
relate the solutions of such projections in the two contexts: without and with preferences.
Theorem 6 Consider an STPPU P = hNe , Nc , Lr , Lc , SF CSP i and preference level γ, and consider the STPU Qγ = hNe , Nc , L0r , L0c i obtained by cutting P at γ, and STPU PC(Q γ )=hNe , Nc ,
L00r , L00c i. Then:
1. ∀ω situation of P , Pω ∈ P roj(PC(Qγ )) iff optP (Pω ) ≥ γ;
2. for every control sequence δ, δ is a solution of T γ = StronglyControllable(PC(Qγ ), iff ∀Pω ∈
Proj(PC (Qγ )), Tδ,ω ∈ Sol(Pω ) and pref (Tδ,ω ) ≥ γ.
The first part of the theorem states that, by applying path consistency to STPU Qγ , we remove
those situations that cannot be extended to complete solutions in Q γ , and thus correspond to projections having optimal preference strictly less than γ. The second part of the lemma considers the
STP T γ obtained applying StronglyControllable after path consistency. In particular it is stated
that all the solutions of T γ result, for all the projections of PC (Q γ ), in solutions with preference
at least γ. Notice that this implies that they result in optimal solutions for those projections of P
having optimal preference exactly γ. They might not be optimal, however, for some projections
with optimal preference strictly greater than γ.
From the above theorem, we get the following corollary, which clarifies the relation between
the STPU obtained cutting an STPPU at preference level γ, and the γ-SC of the STPPU.
Corollary 1 Consider an STPPU P and a preference level γ and assume that ∃ ω, situation of P ,
such that opt(Pω ) ≥ γ, where Pω is the corresponding projection. Then, if STPU PC(Q γ ), obtained
by cutting P at γ, and then applying path consistency, is not SC the P is not γ-SC.
Now if we consider all preference levels between α min and γ, and compute the corresponding
STPs, say T αmin , . . . , T γ , each such STP will identify the assignments to executable variables guaranteeing strong controllability and optimality at each level. By intersecting all these STPs we keep
only the common solutions and thus those which guarantee strong controllability and optimality for
all the situations of P with optimal preference smaller than or equal to γ.
Theorem 7 Consider an STPPU P , and all preference levels from α min to γ, and assume that the
corresponding STPs, T αmin , . . . , T γ obtained by cutting P at preference levels αN
min , . . . , γ, and
enforcing strong controllability are consistent. Then, δ ∈ Sol(P γ ), where P γ = i=αmin ,...,γ T i ,
iff ∀Pω ∈ P roj(P ): Tδ,ω ∈ Sol(Pω ), if opt(Pω ) ≤ γ, then pref (Tδ,ω ) = opt(Pω ), otherwise
pref (Tδ,ω ) ≥ γ.
645

ROSSI , V ENABLE ,& YORKE -S MITH

We now consider each of the events in which Best-SC can stop and for each of them we prove
which of the strong controllability properties hold.
Theorem 8 If the execution of algorithm Best-SC on STPPU P stops due to the occurrence of
Event 1 (line 4), then P is not α-SC ∀α ≥ 0.
This is the case when the underlying STPU obtained from the STPPU by ignoring the preference
functions is not strongly controllable. Since cutting at higher preferences will give even smaller
intervals there is no hope for controllability at any level and the execution can halt.
Theorem 9 If the execution of algorithm Best-SC on STPPU P stops due to the occurrence of
Event 2 (line 11) at preference level γ, then
• γ − 1 = opt = maxT ∈Sol(P ) pref (T );
• P is OSC and a control sequence δ is a solution of STP P opt (returned by the algorithm) iff
it is optimal in any scenario of P .
This event occurs when the algorithm cuts the STPPU at a given preference level and the STPU
obtained, seen as an STP, is inconsistent. In particular, this means that no projection of P roj(P )
has an optimal preference equal to or greater than this preference level. However, if such a level has
been reached, then up to the previous level, assignments guaranteeing SC and optimality had been
found. Moreover, this previous level must have been also the highest preference of any solution of
P , opt(P ). This means that opt(P )-SC has been established, which by Theorem 4 is equivalent to
OSC.
Theorem 10 If the execution of algorithm Best-SC on STPPU P stops due to the occurrence of
Event 3 (line 13) or Event 4 (line 16) at preference level γ, then P is not OSC but it is (γ − 1)SC and any solution δ of STP P γ−1 (returned by the algorithm) is such that, ∀P ω ∈ P roj(P ):
Tδ,ω ∈ Sol(Pω ), if opt(Pω ) ≤ γ − 1, then pref (Tδ,ω ) = opt(Pω ), otherwise pref (Tδ,ω ) ≥ γ − 1.
Intuitively, if the algorithm reaches γ and stops in line 13, then there are projections of P with
optimal preference ≥ γ but the corresponding set of situations is not SC. Notice that this is exactly
the situation considered in Corollary 1. If instead it stops in line 16, then this set of situations is SC,
but none of the assignments guaranteeing SC for these situations does the same and is optimal for
situations at all preference levels up to γ. In both cases the problem is not γ-SC. However, assuming
that γ is the first level at which the execution is stopped the problem is γ − 1-SC.
We conclude this section considering the complexity of Best-SC.
Theorem 11 Determining the OSC or the highest preference level of α-SC of an STPPU with n
variables and ` preference levels can be achieved in time O(n 3 `).
Notice that we cannot use a binary search over preference levels (in contrast to algorithms for
STPPs), since the correctness of the procedure is based on the intersection of the result obtained at
a given preference level, γ, with those obtained at all preference levels < γ.
The above theorem allows us to conclude that the cost of adding preferences, and thus a considerable expressive power, is low. In fact, the complexity is still polynomial and it has grown only of
a factor equal to the number of preference levels.
646

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

6. Determining Optimal Weak Controllability
Optimal Weak Controllability is the least useful property in practice and also the property in which
adding preferences has the smallest impact in terms of expressiveness. What OWC requires is
the existence of an optimal solution in every possible scenario. This is equivalent to requiring the
existence of a solution for every situation, as stated in the following theorem.
Theorem 12 STPPU P is OWC iff the STPU Q, obtained by simply ignoring the preference functions on all the constraints WC.
By ignoring the preference functions we mean mapping each soft constraint hI, f i into a hard
constraint hIi defined on the same variables. This theorem allows us to conclude that, to check
OWC, it is enough to apply algorithm WeaklyControllable as proposed in (Vidal & Ghallab, 1996)
and described in Section 2. If, instead, we are given a scenario ω, then we can find an optimal
solution of its projection, STPP P roj(ω), by using one of the solvers described in (Rossi et al.,
2002).
Let us now consider Example 4 again. Section 5 showed that the STPU obtained by cutting the
STPPU of Figure 8 at preference level α min is strongly controllable. Since SC implies WC, we can
conclude that the STPU is weakly controllable and, thus, that the STPPU in Figure 8 is Optimally
Weakly Controllable.

7. Determining Optimal Dynamic Controllability and α-Dynamic Controllability
Optimal Dynamic Controllability (ODC) is the most interesting and useful property in practice.
As described in Section 1, many industrial applications can only be solved in a dynamic fashion,
making decisions in response to occurrences of events during the execution of the plan. This is
true in the space application domains, where planning for a mission is handled by decomposing the
problem into a set of scheduling subproblems, most of which depend on the occurrence of semipredictable, contingent events (Frank et al., 2001).
In this section we describe an algorithm that tests whether an STPPU P is ODC and, if not ODC,
it finds the highest α at which P is α-DC. The algorithm presented bears some similarities with
Best-SC, in the sense it decomposes the problem into STPUs corresponding to different preference
levels and performs a bottom up search for dynamically controllable problems in this space.
Notice that the algorithm is attractive also in practice, since its output is the minimal form of the
problem where only assignments belonging to at least one optimal solution are left in the domains
of the executable time-points. This minimal form is to be given as input to an execution algorithm,
which we also describe, that assigns feasible values to executable time-points dynamically while
observing the current situation (i.e., the values of the contingent time-points that have occurred).
7.1 A Necessary and Sufficient Condition for Testing ODC
We now define a necessary and sufficient condition for ODC, which is defined on the intervals of
the STPPU. We then propose an algorithm which tests such a condition, and we show that it is a
sound and complete algorithm for testing ODC.
The first claim is that, given an STPPU, the dynamic controllability of the STPUs obtained
by cutting the STPPU and applying PC at every preference level is a necessary but not sufficient
condition for the optimal dynamic controllability of the given STPPU.
647

ROSSI , V ENABLE ,& YORKE -S MITH

Theorem 13 Given an STPPU P , consider any preference level α such that STPU Q α , obtained
cutting P at α, is consistent. If STPU PC(Q α ) is not DC then P is not ODC and it is not β-DC,
∀β ≥ α.
Unfortunately this condition is not sufficient, since an STPPU can still be not ODC even if at
every preference level the STPU obtained after PC is DC. An example was shown in Figure 9 and
is described below.
Example 6 Another potential application of STPPUs is scheduling for aircraft analysis of airborne
particles (Coggiola, Shi, & Young, 2000). As an example consider an aircraft which is equipped
with instruments as the Small Ice Detector and a Nevzorov probe, both of which are used to discriminate between liquid and ice in given types of clouds. Such analysis is important for the prediction
of the evolution of precipitatory systems and of the occurrence and severity of aircraft icing (Field,
Hogan, Brown, Illingworth, Choularton, Kaye, Hirst, & Greenaway, 2004). Both instruments need
an uncertain amount of time to determine which is the predominant state, between liquid and ice,
when activated inside a cloud.
In the example shown in Figure 9 we consider the sensing event represented by variables A
and C and the start time of a maneuver of the aircraft represented by variable B. Due to how the
instruments function, an aircraft maneuver can impact the analysis. In the example constraint AC
represents the duration of the sensing event and the preference function models the fact that the
earlier the predominant state is determined the better. Constraint AB models instead the fact that
the maneuver should start as soon as possible, for example, due to time constraints imposed by the
aircraft’s fuel availability. Constraint BC models the fact that the maneuver should ideally start just
before or just after the sensing event has ended.
Let us call P the STPPU depicted in Figure 9. In order to determine the highest preference level
of any schedule of P we can, for example use algorithm Chop-solver (Rossi et al., 2002). The
highest preference level at which cutting the functions gives a consistent STP is 1 (interval [3, 3] on
AB, [3, 5] on AC and interval [0, 2] on BC is a consistent STP). The optimal solutions of P , regarded
as an STPP, will have global preference 1.
Consider all the STPUs obtained by cutting at every preference level from the highest, 1, to the
lowest 0.5. The minimum preference on any constraint in P is α min = 0.5 and, it is easy to see,
that all the STPUs obtained by cutting P and applying PC at all preference levels from 0.5 to 1 are
DC. However, P is not ODC. In fact, the only dynamic assignment to B that belongs to an optimal
solution of projections corresponding to elements 3, 4 and 5 in [x, y] is 3. But executing B at 3 will
cause an inconsistency if C happens at 10, since 10 − 3 = 7 doesn’t belong to [u, v].2
We now elaborate on this example to find a sufficient condition for ODC. Consider the intervals
on AB, [pα , q α ], and the waits < C, tα > obtained applying the DC checking algorithm at preference
level α. These are shown in Table 4.
If we look at the first and last intervals, resp., at α = 1 and α = 0.5, there is no way to assign a
value to B that at the same time induces a preference 1 on constraints AB and BC, if C occurs at 3, 4
or 5, and that also satisfies the wait < C, 4 >, ensuring consistency if C occurs at 10. This depends
on the fact that the intersection of [p 1 , q 1 ], i.e., [3], and the sub interval of [p 0.5 , q 0.5 ] that satisfies
< C, 4 >, that is, [4, 7], is empty.
We claim that the non-emptiness of such an intersection, together with the DC of the STPUs
obtained by cutting the problem at all preference levels is a necessary and sufficient condition for
648

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

Table 4: In this table each row corresponds to a preference level α and represents the corresponding
interval and wait on the AB constraint of the STPPU shown in Figure 9.
α

[pα , q α ]

wait

1
0.9
0.8
0.7
0.6
0.5

[3, 3]
[3, 4]
[3, 5]
[3, 6]
[3, 7]
[3, 7]

< C, 3 >
< C, 3 >
< C, 3 >
< C, 3 >
< C, 4 >

ODC. In the following section we will describe an algorithm which tests such a condition. Then,
in Section 7.3, we will prove that such an algorithm is sound and complete w.r.t. testing ODC and
finding the highest level of α-DC.
7.2 Algorithm Best-DC
The algorithm Best-DC echoes Section 5’s algorithm for checking Optimal Strong Controllability.
As done by Best-SC, it considers the STPUs obtained by cutting the STPPU at various preference
levels. For each preference level, first it tests whether the STPU obtained considering it as an STP
is path consistent. Then, it checks if the path consistent STPU obtained is dynamically controllable,
using the algorithm proposed in (Morris et al., 2001). Thus, the control sequences that guarantee
DC for scenarios having different optimal preferences are found. The next step is to select only
those sequences that satisfy the DC requirement and are optimal at all preference levels.
The pseudocode is given in Figure 13. Algorithm Best-DC takes as input an STPPU P (line 1)
and then computes the minimum preference, α min , assigned on any constraint (line 2).
Once αmin is known, the STPU obtained by cutting P at α min is computed (line 3). This
STPU can be seen as the STPPU P with the same variables and intervals on the constraints as P
but with no preferences. Such an STPU, which is denoted as Q αmin , is given as input to algorithm
DynamicallyControllable. If Qαmin is not dynamically controllable, then P is not ODC nor γDC (for any γ ≥ αmin , hence for all γ), as shown in Theorem 13. The algorithm detects the
inconsistency and halts (line 4). If, instead, Q αmin is dynamically controllable, then the STPU that
is returned in output by DynamicallyControllable is saved and denoted with P αmin (line 6). Notice
that this STPU is minimal, in the sense that in the intervals there are only elements belonging to
at least one dynamic schedule (Morris et al., 2001). In addition, since we have preferences, the
elements of the requirement intervals, as well as belonging to at least one dynamic schedule, are
part of optimal schedules for scenarios which have a projection with optimal preference equal to
αmin 8 .
In line 7 the preference level is updated to the next value in the ordering to be considered (according to the given preference granularity). In line 8 two Boolean flags, ODC and α-DC are
defined. Setting flag ODC to true will signal that the algorithm has established that the problem is ODC, while setting flag α-DC to true will signal that the algorithm has found the highest
preference level at which the STPPU is α-DC.
8. In fact, they all have preference at least αmin by definition.

649

ROSSI , V ENABLE ,& YORKE -S MITH

Pseudocode for Best-DC
1. input STPPU P ;
2. compute αmin ;
3. STPU Qαmin ← αmin -Cut(P );
4. if (DynamicallyControllable(Q αmin ) inconsistent) write “not αmin -DC” and stop;
5. else {
6. STP P αmin ← DynamicallyControllable(Qαmin );
7. preference β ← αmin + 1;
8. bool ODC ← false, bool α-DC ← false;
9. do {
10.
STPU Qβ ← β-Cut(P );
11.
if (PC(Qβ ) inconsistent) ODC ← true;
12.
else {
13.
if (DynamicallyControllable(PC(Q β )) inconsistent) α-DC ← true;
14.
else {
15.
STPU T β ← DynamicallyControllable(PC(Qβ ));
16.
if(Merge(P β−1 , T β ) FAILS) { α-DC ← true }
17.
else {
18.
STPU P β ←Merge(P β−1 , T β );
19.
β ← β + 1;
20.
};
21.
};
22.
};
23. }while (ODC=false and α-DC=false);
24. if (ODC=true) write “P is ODC”;
25. if (α-DC=true) write “P is” (β − 1) ”-DC”;
26. return STPPU F β−1 ← resulting STPPU(P ,P β−1 );
27. };
Figure 13: Algorithm that tests if an STPPU is ODC and, if not, finds the highest γ such that STPPU
P is γ-DC.

650

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

Pseudocode for Merge
1. input (STPU T β , STPU T β+1 );
2. STPU P β+1 ← T β ;
3. for each constraint AB, A and B executables, in P β+1
define interval [p0 , q 0 ] and wait t0 ,
given { interval [pβ , q β ], wait tβ in T β }
and { interval [pβ+1 , q β+1 ], wait tβ+1 in T β+1 }, as follows:;
4. if (tβ = pβ and tβ+1 = pβ+1 ) (Precede - Precede)
5. p0 ← max(pβ , pβ+1 ), q 0 ← min(q β , q β+1 ), t0 ← max(tβ , tβ+1 );
6. if (q 0 < p0 ) return FAILED;
7. if (pβ < tβ < q β and pβ+1 ≤ tβ+1 < q β+1 ) (Unordered - Unordered or Precede)
8. t0 ← max(tβ , tβ+1 ), q 0 ← min(q β , q β+1 );
9. if (q 0 < t0 ) return FAILED;
10. output P β+1 .
Figure 14: Algorithm Merge.
Lines 9-25 contain the main loop of the algorithm. In short, each time the loop is executed, it
cuts P at the current preference level and looks if the cutting has produced a path consistent STPU
(seen as an STP). If so, it checks if the path consistent version of the STPU is also dynamically
controllable and, if also this test is passed, then a new STPU is created by ‘merging’ the current
results with those of previous levels.
We now describe each step in detail. Line 10 cuts P at the current preference level β. In line 11
the consistency of the STPU Qβ is tested applying algorithm PC. If PC returns an inconsistency,
then we can conclude that P has no schedule with preference β (or greater).
The next step is to check if STPU PC(Q β ) is DC. Notice that this is required for all preference
levels up to the optimal level in order for P to be ODC, and up to γ in order for P to be γ-DC
(Theorem 13). If applying algorithm DynamicallyControllable detects that PC(Q β ) is not dynamically controllable, then the algorithm sets flag α-DC to true. If, instead, PC(Q β ) is dynamically
controllable the resulting minimal STPU is saved and denoted T β (line 15).
In line 16, the output of procedure Merge is tested. This procedure is used to combine the
results up to preference β − 1 with those at preference β, by applying it to the STPU obtained
at the end of the previous while iteration, P β−1 , and STPU T β . The pseudocode for Merge is
shown in Figure 14, and we will describe it in detail shortly. If no inconsistency is found, the new
STPU obtained by the merging procedure is denoted with P β (line 18) and a new preference level
is considered (line 19).
Lines 24-27 take care of the output. Lines 24 and 25 will write in output if P is ODC or, if not,
the highest γ at which it is γ-DC. In line 27 the final STPPU, F , to be given in output, is obtained
from STPU P β−1 , that is, the STPU obtained by the last iteration of the while cycle which was
completed with success (i.e., it had reached line 20). Function Resulting STPPU restores all the
preferences on all the intervals of P β−1 by setting them as they are in P . We will show that the
requirement constraints of F will contain only elements corresponding to dynamic schedules that
are always optimal, if the result is that P is ODC, or are optimal for scenarios corresponding to
projections with optimal preference ≤ γ and guarantee a global preference level of at least γ in all
others, if the result is that P is γ-DC.
651

ROSSI , V ENABLE ,& YORKE -S MITH

The pseudocode of procedure Merge is given in Figure 14. The input consists of two STPUs
defined on the same set of variables. In describing how Merge works, we will assume it is given in
input two STPUs, T β and T β+1 , obtained by cutting two STPPUs at preference levels β and β + 1
and applying, by hypothesis with success, PC and DynamicallyControllable (line 1 Figure 14).
In line 2, Merge initializes the STPU which will be given in output to T β . As will be formally proven in Theorem 14, due to the semi-convexity of the preference functions we have that
P roj(T β+1 ) ⊆ P roj(T β ). Notice that Merge leaves all contingent constraints unaltered. Thus,
all the projection with optimal preference β or β + 1 are contained in the set of projections of P β+1 .
Merge considers every requirement constraint defined on any two executables, say A and B,
respectively in T β and T β+1 . Since we are assuming that algorithm DynamicallyControllable has
been applied to both STPUs, there can be some waits on the intervals. Figure 6 illustrates the three
cases in which the interval on AB can be. If the wait expires after the upper bound of the interval
(Figure 6 (a)), then the execution of B must follow the execution of every contingent time-point
(Follow case). If the wait coincides with the lower bound of the interval (Figure 6 (b)), then the
execution of B must precede that of any contingent time-point (Precede case). Finally, as shown
in Figure 6 (c), if the wait is within the interval, then B is in the Unordered case with at least a
contingent time-point, say C.
Merge considers in which case the corresponding intervals are in T β and in T β+1 (line 3).
Such intervals are respectively indicated as [p β , q β ], with wait tβ , and [pβ+1 , q β+1 ], with wait tβ+1 .
Merge obtains a new interval [p0 , q 0 ] and new wait t0 , which will replace the old wait in T β+1 .
Interval [p0 q 0 ] will contain all and only the values which are projections on the AB constraint of
some optimal solution of some STPP corresponding to a situation in T β or T β+1 . Wait t0 is the wait
that should be respected during a dynamic execution in order to guarantee that the solution obtained
is optimal, if the projection corresponding to the final scenario has preference β or β + 1.
Due to the semi-convexity of the preference functions it cannot be the case that:
• AB is a Follow or a Precede case in T β and an Unordered case in T β+1 ;
• AB is a Follow case in T β and a Precede case in T β+1 ;
• AB is a Precede case in T β and a Follow case in T β+1 ;
This means that the cases which should be considered are:
• AB is a Follow case in both T β and T β+1 ;
• AB is a Precede case in T β and in T β+1 ;
• AB is a Unordered case in T β and a Precede or an Unordered case in T β+1 ;
In the first two cases the AB interval is left as it is in Tβ+1 . A formal motivation of this is
contained in the proof of Theorem 14. However, informally, we can say that the AB interval in
T β+1 already satisfies the desired property.
In lines 4 and 5 the case in which AB is in a Precede case in both STPUs is examined. Here, B
will always occur before any contingent time-point. The values in the [p β , q β ] (resp. [pβ+1 , q β+1 ])
are assignments for B that will be consistent with any future occurrence of C mapped into a preference ≥ β (resp. ≥ β + 1). Clearly the intersection should be taken in order not to lower the

652

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

preference if C occurs with preference ≥ β + 1. Line 6 considers the event in which such intersection is empty. This means that there is no common assignment to B, given that of A, that will be
optimal both in scenarios with optimal preference β and in scenarios with optimal preference β + 1.
In lines 7 and 8 two scenarios are considered: when AB is in the Unordered case in T β and
in the Precede case in T β+1 and when AB is in the Unordered case in both STPUs. Figure 15
shows the second case. Merge takes the union of the parts of the intervals preceding the wait and
the intersection of the parts following the wait. The intuition underlying this is that any execution
of B identifying an element of either [p β , tβ [ or [pβ+1 , tβ+1 [ will be preceded by the execution of
all the contingent time-points for which it has to wait. This means that when B is executed, for
any such contingent time-point C, both the time at which C has been executed, say t C , and the
associated preference, say fAC (tC ), on constraint AC in STPPU P will be known. The propagation
of this information will allow us to identify those elements of [p β , tβ [ (resp. [pβ+1 , tβ+1 [) that have
a preference ≥ fAC (tC ) and thus an optimal assignment for B. This means that all the elements in
both interval [pβ , tβ [ and interval [pβ+1 , tβ+1 [ are eligible to be chosen. For example, if f AC (tC ) =
β there might be values for B with preference equal to β that are optimal in this case but would not
if C occurred at a time such that fAC (tC ) > β. But since in any case we know when and with what
preference C has occurred, it will be the propagation step that will prune non-optimal choices for B.
In short, leaving all elements allows more flexibility in the propagation step. Moreover, as will be
proven in Theorem 14, pβ ≤ pβ+1 .
If instead we consider elements of interval [t β , q β ], we know that they identify assignments
for B that can be executed regardless of when C will happen (however we know it will happen
with a preference greater ≥ β). This means that we must take the intersection of this part with
the corresponding one, [tβ+1 , q β+1 ], in order to guarantee consistency and optimality also when
C occurs at any time with preference = β + 1. An easy way to see this is that interval [t β , q β ]
may contain elements that in P are mapped into preference β. These elements can be optimal in
scenarios in which C happens at a time associated with a preference = β in the AC constraint;
however, they cannot be optimal in scenarios with C occurring at a time with preference β + 1.
Line 9 handles the case in which the two parts of the intervals, following the waits, have an
empty intersection. In this case, optimality cannot be guaranteed neither at level β nor β + 1, in
particular if the contingent events occur after the waits expire.
7.3 Properties of Best-DC
We will now show that Best-DC is a sound and complete algorithm for testing ODC and for finding
the highest preference level at which the STPPU given in input is α-DC. We recall, once more,
that all the results that follow rely on the tractability assumptions requiring semi-convex preference
functions and the fuzzy semiring h[0, 1], max, min, 0, 1i as underlying structure.
Let us consider STPPU P and STPUs T β and T β+1 , as defined in the previous section. Then,
STPU P β+1 =Merge (T β , T β+1 ) will have the same contingent constraints as T β 9 and requirement constraints as defined by the merging procedure. We start by proving that Merge is a sound
and complete algorithm for testing the existence of a viable dynamic strategy, common to both such
STPUs, which is optimal for projections having optimal preference equal to either β or β + 1.
9. We recall that the projections of T β coincide with the projections of STPPU P with optimal preference ≥ β (see
Theorem 6), and that, due to the semi-convexity of the preference functions, P roj(T β+1 ) ⊆ P roj(T β ).

653

ROSSI , V ENABLE ,& YORKE -S MITH

Interval AB
in STPU T β+1

Interval AB
in STPU T β

(a)
β+1

β+1

β+1

p ΑΒ

t ΑΒ

q ΑΒ

β

β

β

p ΑΒ

q ΑΒ

t ΑΒ

Merged interval AB
β

β

p ΑΒ

t ΑΒ

(b)

β+1

(c)

q ΑΒ

Figure 15: Merging two intervals in the Unordered case.
Theorem 14 Consider STPPU P and STPUs, T β and T β+1 , obtained by cutting P respectively at
level β and β + 1 and applying PC, without finding inconsistencies, and DynamicallyControllable
with success. Consider STPU P β+1 = Merge(T β , T β+1 ).
Then, Merge(T β , T β+1 ) does not fail if and only if
• P β+1 is dynamically controllable and
• there is a viable dynamic strategy S such that for every projection P i ∈ P roj(P β+1 ),
– if opt(Pi ) = β or opt(Pi ) = β + 1 in P , pref (S(Pi )) = opt(Pi );
– otherwise pref (S(Pi )) ≥ β + 1.
The following theorem extends the result for the merging procedure to more than two preference
levels, in particular to all preference levels smaller or equal to a given threshold β.
Theorem 15 Consider STPPU P and for every preference level, α, define T α as the STPU obtained
by cutting P at α, then applying PC and then DynamicallyControllable. Assume that ∀α ≤ β, T α
is DC. Consider STPU P β :
P β = Merge(Merge(. . . Merge(Merge(T αmin , T αmin +1 ), T αmin +2 ), . . . ), T β )
with αmin the minimum preference on any constraint in P. Assume that, when applied, Merge
always returned a consistent STPU. Then, there is a viable dynamic strategy S, such that ∀P i ∈
P roj(P ), if opt(Pi ) ≤ β then S(Pi ) is an optimal solution of Pi , otherwise pref (S(Pi )) ≥ β + 1.
Theorem 15 allows us to prove the main result. Informally, Best-DC applies Merge from the
lowest preference to the highest threshold γ, above which the returned problem becomes inconsistent. If there is no projection of the STPPU with an optimal solution higher than γ, then, by using
Theorem 15, we can conclude that the STPPU is ODC; otherwise it is γ-DC.
Let us start by enumerating the conditions at which Best-DC terminates:
654

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

• Event 1. Best-DC stops because the STPU obtained at level α min is not DC (line 4);
• Event 2. Best-DC exits because it has reached a preference level β at which the STPU (seen
as an STP) is not path consistent (line 11);
• Event 3. Best-DC stops because it has reached a preference level β at which the path consistent STPU is not dynamically controllable (line 13);
• Event 4. Best-DC stops because procedure Merge has found an inconsistency (line 16).
The following theorem shows that the execution of Best-DC always terminates.
Theorem 16 Given an STPPU P, the execution of algorithm Best-DC on P terminates.
Best-DC considers each preference level, starting from the lowest and moving up each time of
one level according to the granularity of the preference set. It stops either when an inconsistency is
found or when all levels, which are assumed to be finite, have been precessed.
We are now ready to prove the soundness and completeness of Best-DC. We split the proof
into three theorems, each considering a different terminating condition. The first theorem considers
the case in which the underlying STPU obtained from P , by ignoring the preferences, is not DC. In
such a case the output is that the STPPU is not α-DC at any level and thus is not ODC.
Theorem 17 Given an STPPU P as input, Best-DC terminates in line 4 iff 6 ∃α ≥ 0 such that P is
α-DC.
The next theorem considers the case in which the highest preference level reached with success
by the merging procedure is also the highest optimal preference of any projection of P . In such a
case, the problem is ODC.
Theorem 18 Given an STPPU P as input, Best-DC terminates in line 11 iff P is ODC.
The last result considers the case in which there is at least a projection with an optimal preference strictly higher than the highest reached with success by the merging procedure. In such case
the problem is not ODC and Best-DC has found the highest level at which the STPPU α-DC.
Theorem 19 Given STPPU P in input, Best-DC stops at lines 13 or 16 at preference level β iff P
is (β − 1)-DC and not ODC.
As mentioned in Section 2.3, in (Morris & Muscettola, 2005), it is proven that checking DC of
an STPU can be done in O(n5 ), where n is the number of variables. The revised algorithm processes the distance graph of the STPU, rather than its constraint graph. It also maintains additional
information, in the form of additional labeled edges which correspond to waits. The main feature
of the new algorithm, as noted earlier, it is a strongly polynomial algorithm for determining the
dynamic controllability of an STPU. What is important in our context is to stress the fact that the
output of the two algorithms, presented in (Morris et al., 2001) and (Morris & Muscettola, 2005),
is essentially the same. In fact it is easy to obtain, in polynomial time O(n 2 ), the constraint graph
with waits produced by DynamicallyControllable starting from the distance graph produced by the
new algorithm, and vice versa.

655

ROSSI , V ENABLE ,& YORKE -S MITH

Theorem 20 The complexity of determining ODC or the highest preference level α of α-DC of an
STPPU with n variables, a bounded number of preference levels ` is time O(n 5 `).
The complexity result given in Theorem 20 is unexpectedly good. In fact, it shows that the cost
of adding a considerable expressive power through preferences to STPUs is a factor equal to the
number of different preference levels. This implies that solving the optimization problem and, at
the same time, the controllability problem, remains in P, if the number of different preference levels
is bounded.
7.4 The Execution Algorithm
The execution algorithm we propose is very similar to that for STPUs presented in (Morris et al.,
2001), which we described in Section 2 and shown in Figure 7. Of course the execution algorithm
for STPPUs will take in input an STPPU to which Best-DC has been successfully applied. In
line 2 of Figure 7, the algorithm performs the initial propagation from the starting point. The main
difference between our STPPU execution algorithm and the STPU algorithm in (Morris et al., 2001)
is that the definition of ‘propagation’ also involves preferences.
Definition 26 (soft temporal propagation) Consider an STPPU P and a variable Y ∈ P and a
value vY ∈ D(Y ). Then propagating the assignment Y = v Y in P , means:
• for all constraints, cXY involving Y such that X is already assigned value v X ∈ D(X):
replace the interval on cXY with interval h[vY − vX , vY − vX ]i;
• cut P at preference level minX {fcXY (vY − vX )}.2
We will call ODC-Execute the algorithm DC-Execute where propagation is defined as in
Definition 26. Assume we apply ODC-Execute to an ODC or α-DC STPPU P to which Best-DC
has been applied. If, up to a given time T , the preference of the partial schedule was β, then we
know that if P was ODC or α-DC with α ≥ β, by Theorem 14 and Theorem 15, the execution
algorithm has been assigning values in T β+1 . Assume now that a contingent event occurs and
lowers the preference to β − 2. This will be propagated and the STPPU will be cut at preference
level β − 2. From now on, the execution algorithm will assign values in T β−2 and, by Theorem 14
and Theorem 15, the new waits imposed will be such that the assignments for the executables will
be optimal in any situation where the optimal preference is ≤ β − 2. In all other situations such
assignments guarantee a preference of at least β − 2.

8. Using the Algorithms
Section 4.4 described the relations between our notions of controllability. As a general strategy,
given an STPPU, the first property to consider is OSC. If it holds, the solution obtained is feasible
and optimal in all possible scenarios. However, OSC is a strong property and holds infrequently.
If the STPPU is not OSC, but we still need to have a control sequence before execution begins,
Best-SC will find the best solution that is consistent with all possible future situations.
Most commonly, dynamic controllability will be more useful. If the control sequence needs
not be known before execution begins, ODC is ideal. Notice that, from the results in Section 4.4,
an STPPU may be not OSC and still be ODC. If, however, the STPPU is not even ODC, then

656

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

Best-DC will give a dynamic solution with the highest preference. Recall, as we have shown in
Section 4.4, that for any given preference level α, α-SC implies α-DC but not vice versa. Thus, it
may be that a given STPPU is β-SC and γ-DC with γ > β. Being β-SC means that there is a fixed
way to assign values to the executables such that it will be optimal only in situations with optimal
preference ≤ β and will give a preference at least β in all other cases. On the other hand, γ-DC
implies that a solution obtained dynamically, by the ODC-Execute algorithm, will be optimal for
all those situations where the best solution has preference ≤ γ and will yield a preference ≥ γ in all
other cases. Thus, if γ > β, using the dynamic strategy will guarantee optimality in more situations
and a higher preference in all others.
The last possibility is to check OWC. This will at least allow the executing agent to know in
advance if there is some situation that has no solution. Moreover, if the situation is revealed just
before the execution begins, using any of the solvers for STPPs described in (Rossi et al., 2002) will
allow us to find an optimal assignment for that scenario.

9. Related Work
In this section we survey work which we regard as closely related to ours. Temporal uncertainty
has been studied before, but it has been defined in different ways according to the different contexts
where it has been used.
We start considering the work proposed by Vila and Godo (1994). They propose Fuzzy Temporal Constraint Networks, which are STPs where the interval in each constraint is mapped into a
possibility distribution. In fact, they handle temporal uncertainty using possibility theory (Zadeh,
1975), using the term ‘uncertainty’ to describe vagueness in the temporal information available.
Their aim is to model statements as “He called me more or less an hour ago”, where the uncertainty
is the lack of precise information on a temporal event. Their goal thus is completely different from
ours. In fact, we are in a scenario where an agent must execute some activities at certain times, and
such activities are constrained by temporal relations with uncertain events. Our goal is to find a way
to execute what is in the agents control in a way that will be consistent whatever nature decides in
the future.
In (Vila & Godo, 1994), instead, they assume to have imprecise temporal information on events
happened in the past. Their aim is to check if such information is consistent, that is, if there are
no contradictions implied and to study what is entailed by the set of constraints. In order to model
such imprecise knowledge, possibilities are again used. Every element of an interval is mapped into
a value that indicates how possible that event is or how certain it is. Thus, another major difference
with their approach is that they do not consider preferences, only possibilities. On the other hand, in
the work presented here we do not allow to express information on how possible or probable a value
is for a contingent time-point. This is one of the lines of research we want to pursue in the future.
Moreover, in (Vila & Godo, 1994), they are concerned with the classical notion of consistency
(consistency level) rather than with controllability.
Another work related to the way we handle uncertainty is that of Badaloni and Giacomin (2000).
They introduce Flexible Temporal Constraints where soft constraints are used to express preferences
among feasible solutions and prioritized constraints are used to express the degree of necessity of the
constraints’ satisfaction. In particular, they consider qualitative Allen-style temporal relations and
they associate each such relation to a preference. The uncertainty they deal with is not on the time
of occurrence of an event but is on whether a constraint belongs or not to the constraint problem.

657

ROSSI , V ENABLE ,& YORKE -S MITH

In their model, information coming from plausibility and information coming from preferences is
mixed and is not distinguishable by the solver. In other words, it is not possible to say whether a
solution is bad due to its poor preference on some relation or due to it violating a constraint with a
high priority. In our approach, instead, uncertainty and preferences are separated. The compatibility
with an uncertain event does not change the preference of an assignment to an executable. The
robustness to temporal uncertainty is handled intrinsically by the different degrees of controllability.
In (Dubois, HadjAli, & Prade, 2003b) the authors consider fuzziness and uncertainty in temporal reasoning by introducing Fuzzy Allen Relations. More precisely, they present an extension
of Allen relational calculus, based on fuzzy comparators expressing linguistic tolerance. Dubois
et al. (2003b) want to handle situations in which the information about dates and relative positions
of intervals is complete but, for some reason, there is no interest in describing it in a precise manner. For example, when one wants to speak only in terms of “approximate equality”, or proximity
rather that in terms of precise equality. Secondly, they want to be able to deal with available information pervaded with imprecision, vagueness or uncertainty. In the framework we have presented
we restrict the uncertainty to when an event will occur within a range. On the other hand, we put
ourselves into a “complete ignorance” position, that would be equivalent, in the context of (Dubois
et al., 2003b), to setting to 1 all possibilities of all contingent events. Moreover, in (Dubois et al.,
2003b) they do not allow preferences nor address controllability. Instead, they consider, similarly
to (Vila & Godo, 1994), the notions of consistency and entailment. The first notion is checked by
computing the transitive closure of the fuzzy temporal relations using inference rules appropriately
defined. The second notion is checked by defining several patterns of inference.
Another work which addresses also temporal uncertainty is presented in (Dubois, Fargier, &
Prade, 1995) and in (Dubois, Fargier, & Prade, 2003a). In this work both preferences and activities
with ill-known durations in the classical job-shop scheduling problem are handled using the fuzzy
framework. There are three types of constraints: precedence constraints, capacity constraints and
due dates, and release time constraints. In order to model such unpredictable events they use possibility theory. As the authors mention in (Dubois et al., 1995), possibility distributions can be viewed
as modeling uncertainty as well as preference (see Dubois, Fargier, & Prade, 1993). Everything depends on whether the variable X on which the possibility distribution is defined is controllable or
not. Thus Dubois et al. (1995) distinguish between controllable and uncontrollable variables. However they do not allow to specify preferences on uncontrollable events. Our preference functions
over contingent constraints would be interpreted as possibility distributions in their framework. In
some sense, our work is complementary to theirs. We assume a constraint possibility distribution
on contingent events always equal to 1 and we allow no representation of any further information
on more or less possible values; on the other hand, we allow to specify preferences also on uncontrollable events. They, on the contrary, allow to put possibility distributions on contingent events,
but not preferences.
Finally, Dubois et al. (1995) show that a scheduling problem with uncertain durations can be
formally expressed by the same kind of constraints as a problem involving what they call flexible
durations (i.e. durations with fuzzy preferences). However the interpretation is quite different: in
the case of flexible durations, the fuzzy information comes from the specifications of preferences
and represents the possible values that can be assigned to a variable representing a duration. In the
case of imprecisely known durations, the fuzzy information comes from uncertainty about the real
value of some durations. The formal correspondence between the two constraints is so close that the
authors do not distinguish among them when describing the solving procedure. Further, the problem
658

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

they solve is to find the starting times of activities such that these activities take place within the
global feasibility window whatever the actual values of the unpredictable durations will be. Clearly
this is equivalent to Optimal Strong Controllability. They do not address the problem of dynamic or
weak controllability with preferences.

10. Summary and Future Work
We have defined a formalism to model problems with quantitative temporal constraints with both
preferences and uncertainty, and we have generalized to this formalism three classical notions of
controllability (that is, strong, weak and dynamic). We have then focused on a tractable class of
such problems, and we have developed algorithms that check the presence of these properties.
This work advances the state of the art in temporal reasoning and uncertainty since it provides a
way to handle preferences in this context, and to select the best solution (rather than a feasible one)
in the presence of uncontrollable events. Moreover, it shows that the computational properties of
the controllability checking algorithms do not change by adding preferences. In particular, dynamic
controllability can still be checked in polynomial time for the considered class of problems, producing dynamically temporal plans under uncertainty that are optimal with respect to preferences.
Among the future directions we want to pursue within this line of research, the first is a deeper
study of methods and algorithms for adding preferences different from fuzzy ones. Notice that
the framework that we have proposed here is able to represent any kind on preference within the
soft constraint framework. However, our algorithms apply only to fuzzy preferences and semiconvex functions. In particular, we would like to consider the impact on the design and complexity
of algorithms when there are uncontrollable events and the underlying preference structures is the
weighted or the probabilistic semiring. Both of these semirings are characterized by non-idempotent
multiplicative operators. This can be a problem when applying constraint propagation (Bistarelli
et al., 1997), such as path-consistency, in such constraints. Thus search and propagation techniques
will have to be adapted to an environment featuring uncertainty as well. It should be noticed that in
(Peintner & Pollack, 2005) some algorithms for finding optimal solutions of STPs with preferences
in the weighted semiring have been proposed. Another interesting class of preferences are utilitarian
ones. In such a context each preference represents a utility and the goal is to maximize the sum of
the utilities. Such preferences have been used in a temporal context without uncertainty for example
in (Morris, Morris, Khatib, Ramakrishnan, & Bachmann, 2004).
Recently, another approach for handling temporal uncertainty has been introduced in (Tsamardinos, 2002; Tsamardinos, Pollack, & Ramakrishnan, 2003a): Probabilistic Simple Temporal Problems (PSTPs); similar ideas are presented in (Lau, Ou, & Sim, 2005). In the PSTP framework,
rather than bounding the occurrence of an uncontrollable event within an interval, as in STPUs, a
probability distribution describing when the event is more likely to occur is defined on the entire set
of reals. As in STPUs, the way the problem is solved depends on the assumptions made regarding
the knowledge about the uncontrollable variables. In particular they define the Static Scheduling
Optimization Problem, which is the equivalent to finding an execution satisfying SC in STPUs, and
the Dynamic Scheduling Optimization Problem, equivalent to finding a dynamic execution strategy
in the context of STPUs. In the above framework, optimal means “with the highest probability
of satisfying all the constraints”. Preferences are not considered in this framework. We believe it
would be interesting to add preferences also to this approach. A first step could consists of keeping,
for each strategy, separately its global preference and its probability of success. In this way we

659

ROSSI , V ENABLE ,& YORKE -S MITH

could use the existing frameworks for handling the two aspects. Then, we can order the strategies
by giving priority to preferences, thus taking in some sense a risky attitude, or, on the contrary, by
giving priority to probabilities, adopting a more cautious attitude. A step in this direction has been
recently proposed in (Morris, Morris, Khatib, & Yorke-Smith, 2005), where, however, the authors,
rather than actually extending the notions of consistency of PSTPs to handle preferences, consider
inducing preferences from probabilities. In contrast, our approach is preliminary advanced in (Pini,
Rossi, & Venable, 2005).
Up to now we have focused our attention on non-disjunctive temporal problems, that is, with
only one interval per constraint. We would like to consider adding uncertainty to Disjunctive Temporal Problems (Stergiou & Koubarakis, 2000), and to consider scenarios where there are both
preferences and uncertainty. Such problems are not polynomial even without preferences or uncertainty but it has been shown that the cost of adding preferences is small (Peintner & Pollack,
2004), so we hope that the same will hold in environments with uncertainty as well. Surprisingly,
uncertainty in Disjoint Temporal Problems has not been considered yet, although it is easy to see
how allowing multiple intervals on a constraint is itself a form of uncontrollability. We, thus, plan to
start defining DTPUs (preliminary results are in Venable and Yorke-Smith, 2005) and then to merge
this approach with the existing one for DTPPs.
Extending Conditional Temporal Problems, a framework proposed in (Tsamardinos, Vidal, &
Pollack, 2003b), is also a topic of interest for us. In such model a Boolean formula is attached to
each temporal variable. These formulae represent the conditions which must be satisfied in order
for the execution of events to be enabled. In this framework the uncertainty is on which temporal
variables will be executed. We believe that it would be interesting to extend this approach in order
to allow for conditional preferences: allowing preference functions on constraints to have different
shapes according to the truth values of some formulas, or the occurrence of some event at some
time. This would provide an additional gain in expressiveness, allowing one to express the dynamic
aspect of preferences that change over time.

Appendix A
Theorem 1 If an STPPU P is OSC, then it is ODC; if it is ODC, then it is OWC.
Proof: Let us assume that P is OSC. Then there is a viable execution strategy S such that, ∀P 1 , P2 ∈
P roj(P ) and for every executable time-point x, [S(P 1 )]x = [S(P2 )]x and S(P1 ) ∈ OptSol(P1 )
and S(P2 ) ∈ OptSol(P2 ). Thus, in particular, [S(P1 )]x = [S(P2 )]x for every pair f projections
such that [S(P1 )]<x = [S(P2 )]<x . This allows us to conclude that if P is OSC then it is also ODC
and any strategy which is a witness of OSC is also a witness of ODC.
Let us now assume that P is ODC. Then, in particular, there is a viable dynamic strategy S such
that ∀P1 ∈ P roj(P ), S(P1 ) is an optimal solution of P1 . This clearly means that every projection
has at least an optimal solution. Thus P is OWC. 2
Theorem 2 For any given preference level α, if an STPPU P is α-SC then it is α-DC.
Proof: Assume that P is α-SC. Then there is a viable strategy S such that: [S(P 1 )]x = [S(P2 )]x ,
∀P1 , P2 ∈ P roj(P ) and for every executable time-point x, and S(P ω ) is an optimal solution of
projection Pω , if there is no optimal solution of P ω with preference > α and pref (S(Pω )) 6< α,
otherwise.
660

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

Thus, [S(P1 )]x = [S(P2 )]x also for all pairs of projections, P 1 and P2 such that [S(P1 )]<x =
[S(P2 )]<x . This implies that P is α-DC. 2
Theorem 3 Given an STPPU P and a preference level β, if P is β-SC (resp. β-DC), then it is α-SC
(resp. α-DC), ∀α < β.
Proof: If P is β-SC then there is a viable strategy S such that: [S(P 1 )]x = [S(P2 )]x , ∀P1 , P2 ∈
P roj(P ) and for every executable time-point x, and S(P ω ) is an optimal solution of Pω if there is
no optimal solution of Pω with preference > β and pref (S(Pω )) 6< β, otherwise. But, of course,
∀α < β the set of projections with no optimal solution with preference > α is included in that of
projections with no optimal solution with preference > β. Moreover, for all the other projections,
Pz , pref (S(Pz )) 6< β implies that pref (S(Pz )) 6< α since β > α. Similarly for β-DC.2
Theorem 4 Given an STPPU P , let opt = max T ∈Sol(P ) pref (T ). Then, P is OSC (resp. ODC) iff
it is opt-SC (resp. opt-DC).
Proof: The result comes directly from the fact that ∀P i ∈ P roj(P ), opt(Pi ) ≤ opt, and there is
always at least a projection, Pj , such that opt(Pj ) = opt.2
Theorem 5 Given any STPPU P with a finite number of preference levels, the execution of algorithm Best-SC over P terminates.
Proof: Consider STPPU P and its optimal preference value opt = max T ∈Sol(P ) pref (T ), that is,
the highest preference assigned to any of its solutions. By definition, Qopt+1 is not consistent. This
means that if the algorithm reaches level opt + 1 (that is, the next preference level higher than opt
in the granularity of the preferences) then the condition in line 11 will be satisfied and the execution
will halt. By looking at lines 9-20 we can see that either one of the events that cause the execution
to terminate occurs or the preference level is incremented in line 16. Since there is a finite number
of preference levels, this allows us to conclude that the algorithm will terminate in a finite number
of steps. 2
Theorem 6 Consider an STPPU P = hNe , Nc , Lr , Lc , SF CSP i and preference level γ, and consider the STPU Qγ = hNe , Nc , L0r , L0c i obtained by cutting P at γ, and STPU PC(Q γ )=hNe , Nc ,
L00r , L00c i. Then:
1. ∀ω situation of P , Pω ∈ P roj(PC(Qγ )) iff optP (Pω ) ≥ γ;
2. for every control sequence δ, δ is a solution of T γ = StronglyControllable(PC(Qγ ) iff, ∀Pω ∈
Proj(PC (Qγ )), Tδ,ω ∈ Sol(Pω ) and pref (Tδ,ω ) ≥ γ.
Proof: We will prove each item of the theorem.
1. (⇒): Consider any situation ω such that P ω ∈ P roj(PC(Qγ )). Since PC(Qγ ) is path consistent, any consistent partial assignment (e.g. that defined by ω) can be extended to a complete
consistent assignment, say Tδ,ω of PC(Qγ ). Moreover, Tδ,ω ∈ Sol(Pω ), and pref (Tδ,ω ) ≥ γ,
since the preference functions are semi-convex and every interval of PC(Q γ ) is a subinterval
of the corresponding one in Qγ . Thus, opt(Pω ) ≥ γ in P . (⇐): Consider a situation ω such
that opt(Pω ) ≥ γ. This implies that ∃Tδ,ω ∈ Sol(Pω ) such that pref (Tδ,ω ) ≥ γ. Since we
661

ROSSI , V ENABLE ,& YORKE -S MITH

are in the fuzzy semiring, this happens iff min cij ∈Lr ∪Lc fij (Tδ,ω ) ↓cij ) ≥ γ. Thus it must
be that fij (Tδ,ω ↓cij ) ≥ γ, ∀cij ∈ Lr ∪ Lc and thus (Tδ,ω ) ↓cij ∈ c0ij , where c0ij ∈ L0r ∪ L0c .
This implies that Pω ∈ P roj(Qγ ). Moreover, since Tδ,ω is a consistent solution of Pω in Qγ ,
Pω ∈ P roj(PC(Qγ )).
2. By construction of T γ , δ ∈ Sol(T γ ) iff, ∀Pω ∈ P roj(PC(Qγ )), Tδ,ω ∈ Sol(Pω ) ∩
Sol(PC(Qγ )). Notice that the fact that Tδ,ω ∈ Sol(PC(Qγ )) implies that pref (Tδ,ω ) ≥ γ. 2
Corollary 1 Consider an STPPU P and a preference level γ and assume that ∃ ω, situation of P ,
such that opt(Pω ) ≥ γ, where Pω is the corresponding projection. Then, if STPU PC(Q γ ), obtained
by cutting P at γ, and then applying path consistency, is not SC the P is not γ-SC.
Proof: From item 1 of Theorem 6 we get that P ω is a projection of P such that opt(Pω ) ≥ γ
iff Pω ∈ P roj(PC(Qγ )). Thus, there are complete assignments to controllable and contingent
variables of P with global preference ≥ γ iff PC(Q γ ) is consistent, i.e., iff Qγ is path consistent. Let
us now assume that PC(Qγ ) is not SC. Then by item 2 of Theorem 6, there is no fixed assignment to
controllable variables such that it is a solution of every projection in P roj(PC(Q γ )) and, for every
such projection, it gives a global preference ≥ γ.
This means that either such set of projections has no common solution in P or every common
solution gives a preference strictly lower that γ. Thus, P is not γ-SC since this requires the existence
of a fixed assignment to controllable variables which must be an optimal solution for projections
with preference at most γ (Definition 22, Item 1 and 2) and give a preference ≥ γ in all other
projections (Definition 22, Item 3).
Theorem 7 Consider an STPPU P , and all preference levels from α min to γ, and assume that the
corresponding STPs, T αmin , . . . , T γ obtained by cutting P at preference levels αN
min , . . . , γ, and
γ
γ
enforcing strong controllability are consistent. Then, δ ∈ Sol(P ), where P = i=αmin ,...,γ T i ,
iff ∀Pω ∈ P roj(P ): Tδ,ω ∈ Sol(Pω ), if opt(Pω ) ≤ γ, then pref (Tδ,ω ) = opt(Pω ), otherwise
pref (Tδ,ω ) ≥ γ.
Proof: (⇒): Let us first recall that given two STPs, P1 and P2 , defined on the same set of variables,
the STP P3 = P1 ⊗ P2 has the same variables as P1 and P2 and each temporal constraint c3ij =
c1ij ⊗ c2ij , that is, the intervals of P3 are the intersection of the corresponding intervals of P 1 and
P2 . Given this, and the fact that the set of projections of P is the same as the set of projections
of the STPU obtained cutting P at αmin , we can immediately derive from Theorem 6 that any
solution of P γ satisfies the condition. (⇐): Let us now consider a control sequence δ of P such that
δ 6∈ Sol(P γ ). Then, ∃j ∈ {αmin . . . γ} such that δ 6∈ Sol(T j ). From Theorem 6 we can conclude
that ∃Pω such that opt(Pω ) = j ≤ γ such that Tδ,ω is not an optimal solution of Pω . 2
Theorem 8 If the execution of algorithm Best-SC on STPPU P stops due to the occurrence of
Event 1 (line 4), then P is not α-SC ∀α ≥ 0.
Proof: For every preference level γ ≤ α min , Qγ =γ-Cut(P ), =αmin -Cut(P )=Qαmin . The occurrence of Event 1 implies that Qαmin is not strongly controllable. So it must be the same for all
Qγ , γ ≤ αmin . And thus P is not α-SC ∀α ≤ αmin . Theorem 3 allows us to conclude the same
∀γ > αmin . 2

662

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

Theorem 9 If the execution of algorithm Best-SC on STPPU P stops due to the occurrence of
Event 2 (line 11) at preference level γ, then
• γ − 1 = opt = maxT ∈Sol(P ) pref (T );
• P is OSC and a control sequence δ is a solution of STP P opt (returned by the algorithm) iff
it is optimal in any scenario of P .
Proof: If the condition of line 11 is satisfied by STPU Qγ , it means that there are no schedules of
P that have preference γ. However, the same condition was not satisfied at the previous preference
level, γ − 1, which means that there are schedules with preference γ − 1. This allows us to conclude
that γ − 1 is the optimal preference for STPPU P seen as an STPP, that is, γ − 1 = opt =
maxT ∈Sol(P ) pref (T ). Since we are assuming that line 11 is executed by Best-SC at level opt + 1,
the conditions in lines 13 and 16 must have not been satisfied at preference opt. This means that
at level opt the STP P opt (line 15) is consistent. By looking at line 15, we can see that STP P opt
satisfies the hypothesis of Theorem 7 from preference αmin to preference opt. This allows us to
conclude that any solution of P opt is optimal in any scenario of P and vice versa. Thus, P is opt-SC
and, by Theorem 4, it is OSC. 2
Theorem 10 If the execution of algorithm Best-SC on STPPU P stops due to the occurrence of
Event 3 (line 13) or Event 4 (line 16) at preference level γ then P is not OSC but it is (γ − 1)SC and any solution δ of STP P γ−1 (returned by the algorithm) is such that, ∀P ω ∈ P roj(P ):
Tδ,ω ∈ Sol(Pω ), if opt(Pω ) ≤ γ − 1, then pref (Tδ,ω ) = opt(Pω ), otherwise pref (Tδ,ω ) ≥ γ − 1.
Proof: If Event 3 or Event 4 occurs the condition in line 11 must have not been satisfied at preference level γ. This means that STPU PC(Q γ ) is consistent and thus there are schedules of P with
preference γ. If Event 3 occurs, then the condition in line 13 must be satisfied. The STPU obtained
by cutting P at preference level γ and applying path consistency is not strongly controllable. We
can thus conclude, using Corollary 1, that P is not OSC. However since the algorithm had executed
line 11 at preference level γ, at γ − 1 it must have reached line 18. By looking at line 15 we can
see that STP P γ−1 satisfies the hypothesis of Theorem 7 from preference αmin to preference level
γ − 1. This allows us to conclude that P is γ − 1-SC.
If instead Event 4 occurs then it is P γ to be inconsistent which (by Theorem 7) means that there
is no common assignment to executables that is optimal for all scenarios with preference < γ and
at the same time for those with preference equal to γ. However since the execution has reached line
16 at preference level γ, again we can assume it had successfully completed the loop at preference
γ − 1 and conclude as above that P is γ − 1-SC.2
Theorem 11 Determining the optimal strong controllability or the highest preference level of α-SC
of an STPPU with n variables and ` preference levels can be achieved in O(n 3 `).
Proof: Notice first that the complexity of procedure α-Cut (lines 3 and 10) and of intersecting two
STPs (line 15) is linear in the number of constraints and thus O(n 2 ). Assuming we have at most `
different preference levels, we can conclude that the complexity of Best-SC is bounded by that of
applying ` times StronglyControllable, that is O(n 3 `) (see Section 2).2
Theorem 12 STPPU P is OWC iff the STPU Q, obtained by simply ignoring the preference functions on all the constraints WC.
663

ROSSI , V ENABLE ,& YORKE -S MITH

Proof: If P is OWC, then for every situation ω of P there exists a control sequence δ such that
schedule Tδ,ω is consistent and optimal for projection P ω . For every projection Pω of P there is
a corresponding projection of Q, say Q ω , which is the STP obtained from the P ω by ignoring the
preference functions. It is easy to see that Definition 1 in Section 2.2 implies that any assignment
which is an optimal solution of Pω is a solution of Qω . If STPU Q is WC then for every projection
Qω there exists a control sequence δ such that schedule T δ,ω is a solution of Qω . Again by Definition 1 in Section 2.2 we can conclude that the corresponding STPP P ω at least a solution and thus
it must have at least an optimal solution, that is a solution such that no other solution has a higher
preference. 2
Theorem 13 Given an STPPU P , consider any preference level α such that STPU Q α , obtained
cutting P at α, is consistent. If STPU PC(Q α ) is not DC then P is not ODC and it is not β-DC,
∀β ≥ α.
Proof: Assume that there is a preference level α such that PC(Q α ) is not DC. This means that
there is no viable execution strategy S α : P roj(PC(Qα ))−→ Sol(PC(Qα )) such that ∀P1 , P2 in
P roj(Qα ) and for any executable x, if [S(P1 )]<x = [S(P2 )]<x then [S(P1 )]x = [S(P2 )]x .
Let us recall that, due to the semi-convexity of the preference functions, cutting the STPPU
at any given preference level can only return smaller intervals on the constraints. Thus, every
projection in P roj(Qα ) (which is an STP) corresponds to a projection in P roj(P ) which is the
STPP obtained from the STP by restoring the preference functions as in P.
Let us now assume, on the contrary, that P is ODC and, thus, that there exists a viable strategy
0
S : P roj(P ) −→ Sol(P ) such that ∀P1 , P2 ∈ P roj(P ), if [S 0 (P1 )]<x = [S 0 (P2 )]<x then
[S 0 (P1 )]x = [S 0 (P2 )]x , and pref (S 0 (Pi )) = opt(Pi ), i = 1, 2. Consider, now the restriction of
S 0 to the projections in P roj(PC(Qα )). Since pref (S 0 (Pω ) = opt(Pω ) for every Pω , it must
be that ∀Pω ∈ P roj((PC(Qα )), S 0 (Pω ) ∈ Sol((PC(Qα )). Thus the restriction of S 0 satisfies
the requirements of the strategy in the definition of DC. This is in contradiction with the fact that
PC(Qα ) is not DC. Thus P cannot be ODC.
By Theorem 6, ∀Pω ∈ P roj(P ), Pω ∈ P roj(PC(Qα )) iff opt(Pω ) ≥ α. This allows us to
conclude that P is not α-DC. Finally, Theorem 3 allows to conclude that P is not β-DC, ∀β ≥ α.
2
Lemma 1 (useful for the proof of Theorem 14) Consider an STPU Q on which DynamicallyCo
ntrollable has reported success on Q. Consider any constraint AB, where A and B are executables
and the execution of A always precedes that of B, defined by interval [p, q] and wait t max 10 . Then,
there exists a viable dynamic strategy S such that ∀Q i ∈ P roj(Q), [S(Qi )]B − [S(Qi )]A ≤ tmax .
Proof: Such a dynamic strategy is produced by algorithm DC-Execute shown in Figure 7, Section 2. In fact, in line 5 it is stated that an executable B can be executed as soon as, at the current
time, the three following conditions are all satisfied: (1) B is live, i.e. the current time must lie between its lower and upper bounds, (2) B is enabled, i.e. all the variables which must precede B have
been executed, and (3) all waits on B have been satisfied. Let us denote the current time as T , and
assume B is live and enabled at T . Thus, T −([S(Q i )]A ) ∈ [p, q]. The third requirement is satisfied
at T only in one of the two following scenarios: either the last contingent time-point for which B
had to wait has just occurred and thus B can be executed immediately, or the waits for the contingent
10. Notice that tmax is the longest wait B must satisfy imposed by any contingent time-point C on constraint AB.

664

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

time-points, among those for which B had to wait, which have not yet occurred have expired at T .
In both cases it must be that T ≤ tmax + [S(Qi )A ]. Thus, ([S(Qi )]B = T ) − [S(Qi )]A ≤ tmax . 2
Theorem 14 Consider STPPU P and STPUs, T β and T β+1 , obtained by cutting P respectively at
level β and β + 1 and applying PC, without finding inconsistencies, and DynamicallyControllable
with success. Consider STPU P β+1 = Merge(T β , T β+1 ).
Then, Merge(T β , T β+1 ) does not fail if and only if
• P β+1 is dynamically controllable and
• there is a viable dynamic strategy S such that for every projection P i ∈ P roj(P β+1 ),
– if opt(Pi ) = β or opt(Pi ) = β + 1 in P , pref (S(Pi )) = opt(Pi );
– otherwise pref (S(Pi )) ≥ β + 1.
Proof: ⇒ The following is a constructive proof in which, assuming Merge has not failed, a strategy
S, satisfying the requirements of the theorem, is defined.
First notice that P roj(P β+1 ) = P roj(T β ). In fact, in line 2 of Merge, P β+1 is initialized to
β
T . and Merge changes only requirement intervals leaving all contingent intervals unaltered.
Furthermore, P roj(T β+1 ) ⊆ P roj(T β ). This can be seen using the first claim of Theorem 6
in Section 5.
Let S 0 and S 00 be the viable dynamic execution strategies obtained running DC-Execute respectively on T β and T β+1 . Now, since P roj(T β+1 ) ⊆ P roj(T β ), the projections of T β will be
mapped into two, possibly different, schedules: one by S 0 and one by S 00 . For every projection
Pi ∈ P roj(P β+1 ) and for every executable B, notice that if S 00 [Pi ]<B exists then it is equal to
S 0 [Pi ]<B . We can thus define the history of B (which we recall is the set of durations of all contingent events which have finished prior to B) in the new strategy S as S[Pi ]<B = S 0 [Pi ]<B for every
projection Pi ∈ P roj(P β+1 ) . Notice that S 00 [Pi ]<B is not defined if the history of B in Pi contains
a duration which is mapped into a preference exactly equal to β and thus P i cannot be a projection
of T β+1 .
We will now consider how to define S depending on which case the AB constraint is in Tβ and
in T β+1 .
• Constraint AB is a Follow or Unordered in T β and Follow in T β+1 . In both cases, Merge
does not change interval AB, leaving it as it is in T β .
Let us first analyze the scenario in which AB is in the Follow case in both STPUs. In such
a case, the execution of B will always follow that of any contingent time point C in both
problems. Thus, for every projection P ω ∈ P roj(P β+1 ), we have S[Pω ]<B = ω. Since
both problems are dynamically controllable [p β , q β ] 6= ∅ and [pβ+1 , q β+1 ] 6= ∅. Furthermore,
since path consistency has been enforced in both problems, the constraints are in minimal
form (see Section 2), that is, for every value δ AB in [pβ , q β ] (resp. [pβ+1 , q β+1 ]) there is a
situation ω of T β (resp. T β+1 ) such that Tδ,ω ∈ Sol(Pω ) and δ↓AB = δAB . Finally, since
P roj(T β+1 ) ⊆ P roj(T β ), it must be that [pβ+1 , q β+1 ] ⊆ [pβ , q β ].
Next we consider the scenario in which AB is in the Unordered case in T β+1 . Let us start
by proving that, in such a case, it must be that [p β+1 , q β+1 ] ⊆ [pβ , tβ ]. First, we show that
665

ROSSI , V ENABLE ,& YORKE -S MITH

pβ+1 ≥ pβ . By definition, pβ+1 is such that there is a situation ω such that P ω ∈ P roj(T β+1 )
and there is a schedule Tδ,ω ∈ Sol(Pω ) such that δ↓AB = pβ+1 . Since P roj(T β+1 ) ⊆
P roj(T β ), then pβ+1 ∈ [pβ , q β ]. Next let us prove that it must be tβ > q β+1 . Notice that
the wait tβ induces a partition of the situations of T β into two sets: those such that, for every
contingent point C, ω↓AC < tβ , and those which for some contingent point C 0 , ω↓AC 0 ≥ tβ .
In the first case, all the contingent events will have occurred before the expiration of the wait
and B will be executed before tA + tβ (where tA is the execution time of A). In the second
case it will be safe to execute B at tA + tβ . Given that P roj(T β+1 ) ⊆ P roj(T β ), and that B
is constrained to follow the execution of every contingent time-point in T β+1 , it must be that
all the projections of T β+1 belong to the first set of the partition and thus qβ+1 < tβ .
In both cases it is, hence, sufficient to define the new strategy S as follows: on all projections,
Pi , Pj ∈ P roj(P β+1 ) such that [S(Pi )]<B = [S(Pj )]<B then [S(Pi )]B = [S(Pj )]B =
[S 00 (Pi )]B if [S 00 (Pi )]B exists, otherwise [S(Pi )]B = [S(Pj )]B = [S 0 (Pi )]B . This assignment
guarantees to identify projections on constraints mapped into preferences ≥ β+1 if [S 00 (Pi )]B
exists and thus Pi ∈ P roj(T β+1 ), otherwise ≥ β for those projections in P roj(T β ) but not
in P roj(T β+1 ).
• Constraint AB is a Precede case in T β and in T β+1 . B must precede any contingent timepoint C. This means that any assignment to A and B corresponding to a value in [p β , q β ] (resp.
[pβ+1 , q β+1 ]) can be extended to a complete solution of any projection in P roj(T β ) (resp.
P roj(T β+1 )). Interval [p0 , q 0 ] is, in fact, obtained by Merge, by intersecting the two intervals.
Since we are assuming that Merge has not failed, such intersection cannot be empty (line 6
of Figure 14). We can, thus, for example, define S as follows: on any pair of projections
Pi , Pj ∈ P roj(P β+1 ) if [S(Pi )]<B = [S(Pj )]<B then [S(Pi )]B (= [S(Pj )]B ) = p0 .
• Constraint AB is Unordered in T β and Unordered or Precede in T β+1 . First let us recall
that the result of applying Merge is interval [p 0 , q 0 ], where p0 = pβ , q 0 = min(q β , q β+1 ) and
wait t0 = max(tβ , tβ+1 ). Since, by hypothesis, Merge has not failed, it must be that t 0 ≤ q 0
(line 9, Figure 14.
Notice that, due to the semi-convexity of the preference functions, p β ≤ pβ+1 . In fact, B will
be executed at tA + pβ (where tA is the time at which A has been executed) only if all the
contingent time-points for which B has too wait for have occurred. Let us indicate with x βmlb
β
β+1 ), where
(resp. xβ+1
mlb ) the maximum lower bound on any AC constraint in T (resp. in T
β
β+1
B has to wait for C. Then it must be that p β ≥ xmlb (resp. pβ+1 ≥ xmlb ). However due to
the semi-convexity of the preference functions x βmlb ≤ xβ+1
mlb .
In this case we will define strategy S as follows. For any pair of projections Pi , Pj ∈
P roj(P β+1 ), if [S(Pi )]<B =[S(Pj )]<B then [S(Pi )]B = [S(Pj )]B = max([S 00 (Pi )]B ,
[S 0 (Pi )]B ) whenever [S 00 (Pi )]B is defined. Otherwise [S(Pi )]B =[S(Pj )]B = [S 0 (Pi )]B .
From Lemma 1 we have that max([S 00 (Pi )]B , [S 0 (Pi )]B ) ≤ t0 , hence [S(Pi )]B = ([S(Pj )]B ) ∈
[p0 , q 0 ].
Let us now consider the preferences induced on the constraints by this assignment. First
let us consider the case when max([S 00 (Pi )]B , [S 0 (Pi )]B ) = [S 00 (Pi )]B . Since S 00 is the
dynamic strategy in T β+1 all its assignment identify projections with preference ≥ β + 1. If
instead max([S 00 (Pi )]B , [S 0 (Pi )]B ) = [S 0 (Pi )]B , then it must be that [S 0 (Pi )]B > [S 00 (Pi )]B .
666

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

However we know, from Lemma 1 that [S 00 (Pi )]B ≤ tβ+1 ≤ t0 and that [S 0 (Pi )]B ≤ t0 .
This implies that [S 0 (Pi )]B ∈ [pβ+1 , t0 ] and thus it is an assignment with preference ≥ β +
1. Finally, if [S 00 (Pi )]B is not defined, as noted above, then Pi 6∈ P roj(T β+1 ) and thus
opt(Pi ) = β (since by Theorem 6 in Section 5 we have that P i ∈ P roj(T β ) ⇔ opt(Pi ) ≥
β). Thus, [S(Pi )]B =[S(Pj )]B = [S 0 (Pi )]B , which, being an assignment in T β , identifies
preferences ≥ β = opt(Pi ).
⇐ We have just shown that, if Merge does not fail, then there is a dynamic strategy (with the
required additional properties) which certifies that Pβ+1 is dynamically controllable.
Assume, instead, that Merge fails on some constraint. There are two cases in which this can
happen. The first one is when AB is a Precede case in both Tβ and T β+1 and [pβ , q β ] ∩[pβ+1 , q β+1 ]
= ∅. As proven in (Morris et al., 2001), the projection on AB of any viable dynamic strategy for T β
is in [pβ , q β ] and the projection on AB of any viable dynamic strategy for T β+1 is in [pβ+1 , q β+1 ].
The dynamic viable strategies of T β give optimal solutions for projections with optimal preference
equal to β. The dynamic viable strategies of the T β+1 give optimal solutions for projections with
optimal preference equal to β + 1. Since the projections of T β+1 are a subset of those in T β , if
[pβ , q β ] ∩ [pβ+1 , q β+1 ] = ∅ then a strategy either is optimal for projection in T β but not for those in
T β+1 or vice-versa.
The second case occurs when Merge fails on some constraint AB which is either an Unordered
case in both T β and T β+1 or is an Unordered case in T β and a precede case in T β+1 . In such
cases the failure is due to the fact that [t β , q β ] ∩ [tβ+1 , q β+1 ] = ∅. It must be that either q β+1 < tβ
or q β < tβ+1 . If the upper bound of the interval on AB is q β+1 the there must be at at least a
contingent time-point C such that executing B more than q β+1 after A is either inconsistent with
some assignment of C or it gives a preference lower than β + 1. On the other side, if the wait on
constraint AB in T β is tβ there must be at least a contingent time-point C 0 such that executing B
before tβ is either inconsistent or not optimal with some future occurrences of C’. Again there is no
way to define a viable dynamic strategy that is simultaneously optimal for projections with optimal
value equal to β and for those with optimal value β + 1. 2
Lemma 2 (Useful for the proof of Theorem 15) Consider strategies S 0 , S 00 and S as defined in
Theorem 14. Then
1. for any projection of P β+1 , Pi , pref (S(Pi )) ≥ pref (S 0 (Pi )) and for every projection, Pz ,
of T β+1 , pref (S(Pz )) ≥ β + 1;
2. for any constraint AB, [S(Pi )]B ≥ t0 .
Proof:
1. Obvious, since in all cases either [S(P i )]B = [S 0 (Pi )]B or [S(Pi )]B = [S 00 (Pi ) ]B and
pref (S 00 (Pi )) ≥ pref (S 0 (Pi )) since for every executable B [S 00 (Pi )]B ∈ T β+1 . Moreover,
for every projection Pz of T β+1 , for every executable B, [S(Pz )]B = [S 00 (Pz )]B .
2. Derives directly from the fact that either [S(P i )]B = [S 0 (Pi )]B or [S(Pi )]B = [S 00 (Pi )]B and
Lemma 1 2.

667

ROSSI , V ENABLE ,& YORKE -S MITH

Theorem 15 Consider STPPU P and for every preference level, α, define T α as the STPU obtained
by cutting P at α, then applying PC and then DynamicallyControllable. Assume that ∀α ≤ β, T α
is DC. Consider STPU P β :
P β = Merge(Merge(. . . Merge(Merge(T αmin , T αmin +1 ), T αmin +2 ), . . . ), T β )
with αmin the minimum preference on any constraint in P. Assume that, when applied, Merge
always returned a consistent STPU. Then, there is a viable dynamic strategy S, such that ∀P i ∈
P roj(P ), if opt(Pi ) ≤ β then S(Pi ) is an optimal solution of Pi , otherwise pref (S(Pi )) ≥ β + 1.
Proof: We will prove the theorem by induction. First, notice that, by construction P roj (T αmin ) =
P roj(P ). This allows us to conclude that P roj(P β ) = P roj(P ), since, every time Merge is
applied, the new STPU has the same contingent constraints as the STPU given as first argument.
Now, since T αmin is dynamically controllable any of its viable dynamic strategies, say S αmin
will be such that S αmin (Pi ) is optimal if opt(Pi ) = αmin and, otherwise, pref (S(Pi )) ≥ αmin .
Consider now P αmin +1 =Merge (T αmin ,T αmin +1 ). Then by Theorem 14, we know that there is a
strategy, S αmin +1 , such that S αmin +1 (Pi ) is an optimal solution of Pi if opt(Pi ) ≤ αmin + 1 and
pref (S(PI )) ≥ αmin + 1 otherwise.
Let us assume that STPU P αmin +k , as defined in the hypothesis, satisfies the thesis and that
P αmin +k+1 , as defined in the hypothesis, where αmin + k + 1 ≤ β, does not. Notice that this
implies that there is a strategy, S αmin +k , such that S αmin +k (Pi ) is an optimal solution of Pi if
opt(Pi ) ≤ αmin + k and pref (S(Pi )) ≥ αmin + k for all other projections. Since α min +
k + 1 ≤ β, then, by hypothesis we also have that T αmin +k+1 is DC. Moreover, by construction,
P αmin +k+1 =Merge (P αmin +k ,T αmin +k+1 ), since Merge doesn’t fail. Thus, using Theorem 14
and using strategy S αmin +k for P αmin +k in the construction of Theorem 14, by Lemma 2, we will
obtain a dynamic strategy, S αmin +k+1 , such that for every projection Pi , pref (S αmin +k+1 (Pi )) ≥
pref (S αmin +k (Pi )) and such that S αmin +k+1 (Pj ) is an optimal solution for all projections P j such
that opt(Pj ) = αmin + k + 1 and pref (S(Pj )) ≥ αmin + k + 1 on all other projections. This
allows us to conclude that S αmin +k+1 (Ph ) is an optimal solution for all projections P h such that
opt(Ph ) ≤ αmin + k + 1. This is contradiction with the assumption that P αmin +k+1 doesn’t satisfy
the thesis of the theorem. 2
Theorem 16 Given an STPPU P, the execution of algorithm Best-DC on P terminates.
Proof: We assume that the preference set is discretized and that there are a finite number of different
preferences. Best-DC starts from the lowest preference and cuts at each level P. If, at a given level,
the STPU obtained is not consistent or not dynamically controllable or the merging procedure fails,
then Best-DC stops at that level. Assume, instead, that, as it moves up in the preference ordering,
none of the events above occur. However at a certain point the cutting level will be higher than the
maximum on some preference function (or it will be outside of the preference set) in which case
cutting the problem will give an inconsistent STP.2
Theorem 17 Given an STPPU P as input, Best-DC terminates in line 4 iff 6 ∃α ≥ 0 such that P is
α-DC.
Proof: ⇒. Assume Best-DC terminates in line 4. Then, the STPU obtained by cutting P at
the minimum preference, αmin , on any constraint is not DC. However cutting at the minimum
668

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

preference on any constraint or at preference level 0 gives the same STPU. By Theorem 13 we can
conclude that P is not α-DC ∀α ≥ 0 and, thus, not ODC.
⇐. Assume P is not α-DC for all preferences α ≥ 0. Then cutting P at the minimum preference
αmin cannot give a dynamically controllable problem, otherwise, P would be α min -DC. Hence,
Best-DC will exit in line 4. 2
Theorem 18 Given an STPPU P as input, Best-DC terminates in line 11 iff P is ODC.
Proof: ⇒. Assume Best-DC terminates in line 11 when considering preference level β. Then,
STPU Qβ obtained by cutting STPPU P at level β is not path consistent. From this we can immediately conclude that there is no projection P i ∈ P roj(Pi ) such that opt(Pi ) ≥ β.
Since Best-DC did not terminate before, we must assume that up to preference β − 1, all the
tests (path consistency, dynamic controllability, and Merge) were successful.
Now consider the STPU P β−1 obtained at the end of the iteration corresponding to preference
level β − 1. It is easy to see that P β−1 satisfies the hypothesis of Theorem 15. This allows us
to conclude that there is a viable dynamic strategy S such that for every projection P i , such that
opt(Pi ) ≤ β − 1, S(Pi ) is an optimal solution of Pi . However since we know that all projections
of P are such that opt(Pi ) < β, this allows us to conclude that P is ODC.
⇐. If P is ODC then there is a viable strategy S such that for every pair of projections, P i , Pj ∈
P roj(P ), and for very executable B, if [S(P i )]<B = [S(Pj )]<B then [S(Pi )]B = [S(Pj )]B and
S(Pi ) is an optimal solution of Pi and S(Pj ) is an optimal solution of Pj .
By Theorem 17 we know that Best-DC cannot stop in line 4.
Let us now consider line 13 and show that if Best-DC sets α-DC to true in that line then P
cannot be ODC. In fact the condition of setting α-DC to true in line 13 is that the STPU obtained by
cutting P at preference level β is path consistent but not dynamically controllable. This means that
there are projections, e.g. Pj , of P such that opt(Pj ) = β. However, there is no dynamic strategy
for the set of those projections. Thus, P cannot be ODC.
Let us now consider line 16, and show that, if P is ODC Best-DC cannot set α-DC to true. If
Best-DC sets α-DC to true then Merge failed. Using Theorem 14, we can conclude that there is no
dynamic viable strategy S such that for every projection of P , P i , (remember that P roj(P β−1 ) =
P roj(P )) S(Pi ) is an optimal solution if opt(Pi ) ≤ β. However, we know there are projections of
P with optimal preference equal to β (since we are assuming Best-DC is stopping at line 16 and not
11). Thus, P cannot be ODC.2
Theorem 19 Given STPPU P in input, Best-DC stops at lines 13 or 16 at preference level β iff P
is (β − 1)-DC and not ODC.
Proof: ⇒. Assume that Best-DC sets α-DC to true in line 13, when considering preference level
β. Thus, the STPU obtained by cutting P at level β is path consistent but not DC. However since β
must be the first preference level at which this happens, otherwise the Best-DC would have stopped
sooner, we can conclude that the iteration at preference level β − 1 was successful. Considering
P β−1 and using Theorem 15 we can conclude that there is a viable dynamic strategy S such that,
for every projection of P , Pi , if opt(Pi ) ≤ β − 1 then S(Pi ) is an optimal solution of Pi and
pref (S(Pi )) ≥ β − 1 otherwise. But this is the definition of β − 1-dynamic controllability.
If Best-DC terminates in line 16, by Theorem 15 and and Theorem 14 we can conclude that,
while there is a viable dynamic strategy S such that for every projection of P , P i , if opt(Pi ) ≤ β −1
669

ROSSI , V ENABLE ,& YORKE -S MITH

then S(Pi ) is an optimal solution of Pi and pref (S(Pi )) ≥ β−1 otherwise, there is no such strategy
guaranteeing optimality also for projections with optimal preference β. Again, P is β − 1-DC.
⇐. If P is α-DC, for some α ≥ 0 then by Theorem 17, Best-DC does not stop in line 4. If P
is α-DC, but not ODC, for some α ≥ 0 then by Theorem 18, Best-DC does not stop in line 11. By
Theorem 16, Best-DC always terminates, so it must stop at line 13 or 16.2
Theorem 20 The complexity of determining ODC or the highest preference level α of α-DC of an
STPPU with n variables, a bounded number of preference levels l is O(n 5 `).
Proof: Consider the pseudocode of algorithm Best-DC in Figure 13.
The complexity of αmin -Cut(P ) in line 3 is O(n2 ), since every constraint must be considered,
an there are up to O(n2 ) constraints, and for each constraint the time for finding the interval of
elements mapped into preference ≥ α min is constant. The complexity of checking if the STPU obtained is DC is O(n5 ). Thus, lines 3 and 4, which are always performed, have an overall complexity
of O(n5 ). Lines 7 and 8, clearly, take constant time.
Let us now consider a fixed preference level β and compute the cost of a complete while iteration on β.
• (line 10) the complexity of β-Cut(P ) is O(n 2 );
• (line 11) the complexity of applying PC for testing path consistency is O(n 3 ) (see Section 2.1,
(Dechter et al., 1991));
• (line 13) the complexity of testing DC using DynamicallyControllable is O(n 5 ), (see Section 2 and Morris and Muscettola, 2005);
• (line 15) constant time;
• (line 16-18) the complexity of Merge is O(n 2 ), since at most O(n2 ) constraints must be
considered and for each constraint merging the two intervals has constant cost;
• (line 19) constant time.
We can conclude that the complexity of a complete iteration at any given preference level is O(n 5 ).
In the worst case, the while cycle is performed ` times. We can, thus, conclude that the total
complexity of Best-DC is O(n5 `) since the complexity of the operations performed in lines 24-27
is constant. 2

References
Badaloni, S., & Giacomin, M. (2000). Flexible temporal constraints. In 8th Conference on Information Processing and Management of Uncertainty in knowledge-Based System (IPMU 2000),
pp. 1262–1269.
Bistarelli, S., Montanari, U., & Rossi, F. (1997). Semiring-based constraint solving and optimization. Journal of the ACM, 44(2), 201–236.
Bresina, J., Jonsson, A., Morris, P., & Rajan, K. (2005). Activity planning for the mars exploration
rovers. In 15th International Conference on Automated Planning and Scheduling (ICAPS
2005), pp. 40–49.
670

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

Coggiola, M., Shi, Z., & Young, S. (2000). Airborne deployment of an instrument for the real-time
analysis of single aerosol particles. Aerosol Science and Technology, 33, 20–29.
Dearden, R., Meuleau, N., Ramakrishnan, S., Smith, D., & Washington, R. (2002). Contingency
planning for planetary rovers. In 3rd Intl. Workshop on Planning and Scheduling for Space.
Dechter, R., Meiri, I., & Pearl, J. (1991). Temporal constraint networks. Artificial Intelligence,
49(1-3), 61–95.
Dubois, D., Fargier, H., & Prade, H. (1993). Flexible constraint satisfaction problems with application to scheduling problems. Tech. rep. Report IRIT/’93-30-R, I.R.I.T., Universite’ P.
Sabatier.
Dubois, D., Fargier, H., & Prade, H. (1995). Fuzzy constraints in job shop-scheduling. Journal of
Intelligent Manufacturing, 6, 215–234.
Dubois, D., Fargier, H., & Prade, H. (2003a). Fuzzy scheduling: Modelling flexible constraints
vs. coping with incomplete knowledge. European Journal of Operational Research, 147,
231–252.
Dubois, D., HadjAli, A., & Prade, H. (2003b). Fuzziness and uncertainty in temporal reasoning.
Journal of Universal Computer Science, 9(9), 1168–.
Dubois, D., & Prade, H. (1985). A review of fuzzy set aggregation connectives. Journal of Information Science, 36(1-2), 85–121.
Field, P., Hogan, R., Brown, P., Illingworth, A., Choularton, T., Kaye, P., Hirst, E., & Greenaway,
R. (2004). Simultaneous radar and aircraft observations of mixed-phase cloud at the 100m
scale. Quarterly Journal of the Royal Meteorological Society, 130, 1877–1904.
Floyd, R. W. (1962). Algorithm 97: Shortest path. Communication of the ACM, 36(6), 345.
Frank, J., Jonsson, A., Morris, R., & Smith, D. (2001). Planning and scheduling for fleets of earth
observing satellites. In 6th Intl. Symposium on AI, Robotics, and Automation in Space (iSAIRAS’01).
Khatib, L., Morris, P., Morris, R. A., & Rossi, F. (2001). Temporal constraint reasoning with
preferences. In Nebel, B. (Ed.), 17th International Joint Conference on Artificial Intelligence,
(IJCAI 2001), pp. 322–327. Morgan Kaufmann.
Lau, H. C., Ou, T., & Sim, M. (2005). Robust temporal constraint networks. In Proc. of 17th IEEE
Conf. on Tools with Artificial Intelligence (ICTAI’05), pp. 82–88 Hong Kong.
Leiserson, C. E., & Saxe, J. B. (1988). A mixed-integer linear programming problem which is
efficiently solvable. Journal of Algorithms, 9(1), 114–128.
Morris, P., Morris, R., Khatib, L., Ramakrishnan, S., & Bachmann, A. (2004). Strategies for global
optimization of temporal preferences. In Wallace, M. (Ed.), Proceeding of the 10th International Conference on Principles and Practice of Constraint Programming (CP-04), Vol. 3258
of Lecture Notes in Computer Science, pp. 588–603. Springer.
671

ROSSI , V ENABLE ,& YORKE -S MITH

Morris, P. H., Muscettola, N., & Vidal, T. (2001). Dynamic control of plans with temporal uncertainty. In Nebel, B. (Ed.), 17th International Joint Conference on Artificial Intelligence
(IJCAI 2001), pp. 494–502. Morgan Kaufmann.
Morris, P. H., & Muscettola, N. (1999). Managing temporal uncertainty through waypoint controllability. In Dean, T. (Ed.), 16th International Joint Conference on Artificial Intelligence
(IJCAI’99), pp. 1253–1258. Morgan Kaufmann.
Morris, P. H., & Muscettola, N. (2005). Temporal dynamic controllability revisited. In 20th National
Conference on Artificial Intelligence (AAAI 2005), pp. 1193–1198. AAAI Press / The MIT
Press.
Morris, R. A., Morris, P. H., Khatib, L., & Yorke-Smith, N. (2005). Temporal planning with preferences and probabilities. In ICAPS’05 Workshop on Constraint Programming for Planning
and Scheduling.
Muscettola, N., Morris, P. H., Pell, B., & Smith, B. D. (1998). Issues in temporal reasoning for
autonomous control systems. In Agents, pp. 362–368.
Peintner, B., & Pollack, M. E. (2004). Low-cost addition of preferences to DTPs and TCSPs. In
McGuinness, D. L., & Ferguson, G. (Eds.), 19th National Conference on Artificial Intelligence, pp. 723–728. AAAI Press / The MIT Press.
Peintner, B., & Pollack, M. E. (2005). Anytime, complete algorithm for finding utilitarian optimal
solutions to STPPs. In 20th National Conference on Artificial Intelligence (AAAI 2005), pp.
443–448. AAAI Press / The MIT Press.
Pini, M. S., Rossi, F., & Venable, K. B. (2005). Possibility theory for reasoning about uncertain
soft constraints. In Godo, L. (Ed.), 8th European Conference on Symbolic and Quantitative
Approaches to Reasoning with Uncertainty (ECSQARU 2005), Vol. 3571 of LNCS, pp. 800–
811. Springer.
Rajan, K., Bernard, D. E., Dorais, G., Gamble, E. B., Kanefsky, B., Kurien, J., Millar, W., Muscettola, N., Nayak, P. P., Rouquette, N. F., Smith, B. D., Taylor, W., & Tung, Y. W. (2000).
Remote Agent: An autonomous control system for the new millennium. In Horn, W. (Ed.),
14th European Conference on Artificial Intelligence, ECAI 2000, pp. 726–730. IOS Press.
Rossi, F., Sperduti, A., Venable, K., Khatib, L., Morris, P., & Morris, R. (2002). Learning and solving soft temporal constraints: An experimental study. In Van Hentenryck, P. (Ed.), Principles
and Practice of Constraint Programming, 8th International Conference (CP 2002), Vol. 2470
of LNCS, pp. 249–263. Springer.
Rossi, F., Venable, K. B., & Yorke-Smith, N. (2004). Controllability of soft temporal constraint
problems. In 10th International Conference on Principles and Practice of Constraint Programming (CP-04), Vol. 3258 of LNCS, pp. 588–603.
Rossi, F., Venable, K., & Yorke-Smith, N. (2003). Preferences and uncertainty in simple temporal problems. In Proc. CP03 Workshop: Online-2003 (International Workshop on Online
Constraints Solving - Handling Change and Uncertainty).
672

U NCERTAINTY

IN SOFT TEMPORAL CONSTRAINT PROBLEMS

Ruttkay, Z. (1994). Fuzzy constraint satisfaction. In Proceedings 1st IEEE Conference on Evolutionary Computing, pp. 542–547 Orlando.
Schiex, T. (1992). Possibilistic Constraint Satisfaction problems or ”How to handle soft constraints?”. In Dubois, D., & Wellman, M. P. (Eds.), 8th Annual Conference on Uncertainty in
Artificial Intelligence (UAI’92), pp. 268–275. Morgan Kaufmann.
Shostak, R. E. (1981). Deciding linear inequalities by computing loop residues. Journal of the
ACM, 28(4), 769–779.
Stergiou, K., & Koubarakis, M. (2000). Backtracking algorithms for disjunctions of temporal constraints. Artificial Intelligence, 120(1), 81–117.
Tsamardinos, I. (2002). A probabilistic approach to robust execution of temporal plans with uncertainty. In Vlahavas, I. P., & Spyropoulos, C. D. (Eds.), Methods and Applications of Artificial
Intelligence, Second Hellenic Conference on AI (SETN 2002), Vol. 2308 of LNCS, pp. 97–
108. Springer.
Tsamardinos, I., Pollack, M. E., & Ramakrishnan, S. (2003a). Assessing the probability of legal execution of plans with temporal uncertainty. In Workshop on Planning Under Uncertainty and
Incomplete Information of the Thirteenth International Conference on Automated Planning
and Scheduling (ICAPS 2003).
Tsamardinos, I., Vidal, T., & Pollack, M. E. (2003b). CTP: A new constraint-based formalism for
conditional, temporal planning. Constraints, 8(4), 365–388.
Venable, K., & Yorke-Smith, N. (2003). Simple Temporal Problems with Preferences and Uncertainty. In Doctoral Consortium of the 13th International Conference on Automated Planning
and Scheduling (ICAPS 2003). AAAI Press.
Venable, K., & Yorke-Smith, N. (2005). Disjunctive temporal planning with uncertainty. In 19th
International Joint Conference on Artificial Intelligence (IJCAI 2005), pp. 1721–22. Morgan
Kaufmann.
Vidal, T., & Fargier, H. (1999). Handling contingency in temporal constraint networks: from consistency to controllabilities. Journal of Experimental and Theoretical Artificial Intelligence,
11(1), 23–45.
Vidal, T., & Ghallab, M. (1996). Dealing with uncertain durations in temporal constraint networks
dedicated to planning. In Wahlster, W. (Ed.), 12th European Conference on Artificial Intelligence (ECAI’96), pp. 48–54. John Wiley and Sons.
Vila, L., & Godo, L. (1994). On fuzzy temporal constraint networks. Mathware and Soft Computing,
3, 315–334.
Xu, L., & Choueiry, B. Y. (2003). A new efficient algorithm for solving the simple temporal problem. In 10th Intl. Symposium on Temporal Representation and Reasoning and Fourth Intl.
Conf. on Temporal Logic (TIME-ICTP’03), pp. 212–222.

673

ROSSI , V ENABLE ,& YORKE -S MITH

Yorke-Smith, N., Venable, K. B., & Rossi, F. (2003). Temporal reasoning with preferences and uncertainty. In Gottlob, G., & Walsh, T. (Eds.), 18th International Joint Conference on Artificial
Intelligence (IJCAI’03), pp. 1385–1386. Morgan Kaufmann.
Zadeh, L. A. (1975). Calculus of fuzzy restrictions. Fuzzy Sets and their Applications to Cognitive
and Decision Processes, 1–40.

674

Journal of Artificial Intelligence Research 27 (2006) 299-334

Submitted 01/06; published 11/06

Properties and Applications of Programs with Monotone and Convex
Constraints
Lengning Liu
Mirosław Truszczyński

LLIU 1@ CS . UKY. EDU
MIREK @ CS . UKY. EDU

Department of Computer Science, University of Kentucky,
Lexington, KY 40506-0046, USA

Abstract
We study properties of programs with monotone and convex constraints. We extend to these
formalisms concepts and results from normal logic programming. They include the notions of
strong and uniform equivalence with their characterizations, tight programs and Fages Lemma,
program completion and loop formulas. Our results provide an abstract account of properties of
some recent extensions of logic programming with aggregates, especially the formalism of lparse
programs. They imply a method to compute stable models of lparse programs by means of off-theshelf solvers of pseudo-boolean constraints, which is often much faster than the smodels system.

1. Introduction
We study programs with monotone constraints (Marek & Truszczy ński, 2004; Marek, Niemelä,
& Truszczyński, 2004, 2006) and introduce a related class of programs with convex constraints.
These formalisms allow constraints to appear in the heads of program rules, which sets them apart
from other recent proposals for integrating constraints into logic programs (Pelov, 2004; Pelov,
Denecker, & Bruynooghe, 2004, 2006; Dell’Armi, Faber, Ielpa, Leone, & Pfeifer, 2003; Faber,
Leone, & Pfeifer, 2004), and makes them suitable as an abstract basis for formalisms such as lparse
programs (Simons, Niemelä, & Soininen, 2002).
We show that several results from normal logic programming generalize to programs with monotone constraints. We also discuss how these techniques and results can be extended further to the
setting of programs with convex constraints. We then apply some of our general results to design
and implement a method to compute stable models of lparse programs and show that it is often
much more effective than smodels (Simons et al., 2002).
Normal logic programming with the semantics of stable models is an effective knowledge representation formalism, mostly due to its ability to express default assumptions (Baral, 2003; Gelfond
& Leone, 2002). However, modeling numeric constraints on sets in normal logic programming is
cumbersome, requires auxiliary atoms and leads to large programs hard to process efficiently. Since
such constraints, often called aggregates, are ubiquitous, researchers proposed extensions of normal
logic programming with explicit means to express aggregates, and generalized the stable-model semantics to the extended settings.
Aggregates imposing bounds on weights of sets of atoms and literals, called weight constraints,
are especially common in practical applications and are included in all recent extensions of logic
programs with aggregates. Typically, these extensions do not allow aggregates to appear in the
c
2006
AI Access Foundation. All rights reserved.

´
L IU & T RUSZCZY NSKI

heads of rules. A notable exception is the formalism of programs with weight constraints (Niemel ä,
Simons, & Soininen, 1999; Simons et al., 2002), which we refer to as lparse programs 1 .
Lparse programs are logic programs whose rules have weight constraints in their heads and
whose bodies are conjunctions of weight constraints. Normal logic programs can be viewed as a
subclass of lparse programs and the semantics of lparse programs generalizes the stable-model
semantics of normal logic programs (Gelfond & Lifschitz, 1988). Lparse programs are one of the
most commonly used extensions of logic programming with weight constraints.
Since rules in lparse programs may have weight constraints as their heads, the concept of onestep provability is nondeterministic, which hides direct parallels between lparse and normal logic
programs. An explicit connection emerged when Marek and Truszczy ński (2004) and Marek et al.
(2004, 2006) introduced logic programs with monotone constraints. These programs allow aggregates in the heads of rules and support nondeterministic computations. Marek and Truszczy ński
(2004) and Marek et al. (2004, 2006) proposed a generalization of the van Emden-Kowalski onestep provability operator to account for that nondeterminism, defined supported and stable models
for programs with monotone constraints that mirror their normal logic programming counterparts,
and showed encodings of smodels programs as programs with monotone constraints.
In this paper, we continue investigations of programs with monotone constraints. We show that
the notions of uniform and strong equivalence of programs (Lifschitz, Pearce, & Valverde, 2001;
Lin, 2002; Turner, 2003; Eiter & Fink, 2003) extend to programs with monotone constraints, and
that their characterizations (Turner, 2003; Eiter & Fink, 2003) generalize, too.
We adapt to programs with monotone constraints the notion of a tight program (Erdem & Lifschitz, 2003) and generalize Fages Lemma (Fages, 1994).
We introduce extensions of propositional logic with monotone constraints. We define the completion of a monotone-constraint program with respect to this logic, and generalize the notion of a
loop formula. We then prove the loop-formula characterization of stable models of programs with
monotone constraints, extending to the setting of monotone-constraint programs results obtained
for normal logic programs by Clark (1978) and Lin and Zhao (2002).
Programs with monotone constraints make explicit references to the default negation operator.
We show that by allowing a more general class of constraints, called convex, default negation can be
eliminated from the language. We argue that all results in our paper extend to programs with convex
constraints.
Our paper shows that programs with monotone and convex constraints have a rich theory that
closely follows that of normal logic programming. It implies that programs with monotone and convex constraints form an abstract generalization of extensions of normal logic programs. In particular, all results we obtain in the abstract setting of programs with monotone and convex constraints
specialize to lparse programs and, in most cases, yield results that are new.
These results have practical implications. The properties of the program completion and loop
formulas, when specialized to the class of lparse programs, yield a method to compute stable models
of lparse programs by means of solvers of pseudo-boolean constraints, developed by the propositional satisfiability and integer programming communities (Eén & Sörensson, 2003; Aloul, Ramani,
Markov, & Sakallah, 2002; Walser, 1997; Manquinho & Roussel, 2005; Liu & Truszczy ński, 2003).
We describe this method in detail and present experimental results on its performance. The results
show that our method on problems we used for testing typically outperforms smodels.
1. Aggregates in the heads of rules have also been studied recently by Son and Pontelli (2006) and Son, Pontelli, and
Tu (2006).

300

P ROPERTIES AND A PPLICATIONS OF P ROGRAMS WITH M ONOTONE AND C ONVEX C ONSTRAINTS

2. Preliminaries
We consider the propositional case only and assume a fixed set At of propositional atoms. It does
not lead to loss of generality, as it is common to interpret programs with variables in terms of their
propositional groundings.
The definitions and results we present in this section come from papers by Marek and Truszczyński (2004) and Marek et al. (2006). Some of them are more general as we allow constraints
with infinite domains and programs with inconsistent constraints in the heads.
Constraints. A constraint is an expression A = (X, C), where X ⊆ At and C ⊆ P(X) (P(X)
denotes the powerset of X). We call the set X the domain of the constraint A = (X, C) and denote it
by Dom(A). Informally speaking, a constraint (X, C) describes a property of subsets of its domain,
with C consisting precisely of these subsets of X that satisfy the constraint (have property) C.
In the paper, we identify truth assignments (interpretations) with the sets of atoms they assign
the truth value true. That is, given an interpretation M ⊆ At, we have M |= a if and only if a ∈ M .
We say that an interpretation M ⊆ At satisfies a constraint A = (X, C) (M |= A), if M ∩ X ∈ C.
Otherwise, M does not satisfy A, (M 6|= A).
A constraint A = (X, C) is consistent if there is M such that M |= A. Clearly, a constraint
A = (X, C) is consistent if and only if C 6= ∅.
We note that propositional atoms can be regarded as constraints. Let a ∈ At and M ⊆ At. We
define C(a) = ({a}, {{a}}). It is evident that M |= C(a) if and only if M |= a. Therefore, in the
paper we often write a as a shorthand for the constraint C(a).
Constraint programs. Constraints are building blocks of rules and programs. Marek and Truszczyński (2004) defined constraint programs as sets of constraint rules
A ← A1 , . . . , Ak , not(Ak+1 ), . . . , not(Am )

(1)

where A, A1 , . . . , An are constraints and not is the default negation operator.
In the context of constraint programs, we refer to constraints and negated constraints as literals.
Given a rule r of the form (1), the constraint (literal) A is the head of r and the set {A 1 , . . . ,
Ak , . . . , not(Ak+1 ), . . . , not(Am )} of literals is the body of r 2 . We denote the head and the body
of r by hd (r) and bd (r), respectively. We define the the headset of r, written hset(r), as the domain
of the head of r. That is, hset(r) = Dom(hd (r)).
For a constraint program P , we denote by At(P ) the set of atoms that appear in the domains of
constraints in P . We define the headset of P , written hset(P ), as the union of the headsets of all
rules in P .
Models. The concept of satisfiability extends in a standard way to literals not(A) (M |= not(A)
if M 6|= A), to sets (conjunctions) of literals and, finally, to constraint programs.
M-applicable rules. Let M ⊆ At be an interpretation. A rule (1) is M -applicable if M satisfies
every literal in bd (r). We denote by P (M ) the set of all M -applicable rules in P .
Supported models. Supportedness is a property of models. Intuitively, every atom a in a supported
model must have “reasons” for being “in”. Such reasons are M -applicable rules whose heads contain a in their domains. Formally, let P be a constraint program and M a subset of At(P ). A model
M of P is supported if M ⊆ hset(P (M )).
Examples. We illustrate the concept with examples. Let P be the constraint program that consists
of the following two rules:
2. Sometimes we view the body of a rule as the conjunction of its literals.

301

´
L IU & T RUSZCZY NSKI

({c, d, e}, {{c}, {d}, {e}, {c, d, e}}) ←
({a, b}, {{a}, {b}}) ← ({c, d}, {{c}, {c, d}}), not(({e}, {{e}}))
A set M = {a, c} is a model of P as M satisfies the heads of the two rules. Both rules in P are
M -applicable. The first of them provides the support for c, the second one — for a. Thus, M is a
supported model.
A set M 0 = {a, c, d, e} is also a model of P . However, a has no support in P . Indeed, a only
appears in the headset of the second rule. This rule is not M 0 -applicable and so, it does not support
a. Therefore, M 0 is not a supported model of P .
4
Nondeterministic one-step provability. Let P be a constraint program and M a set of atoms. A set
M 0 is nondeterministically one-step provable from M by means of P , if M 0 ⊆ hset(P (M )) and
M 0 |= hd (r), for every rule r in P (M ).
The nondeterministic one-step provability operator TPnd for a program P is an operator on
P(At) such that for every M ⊆ At, TPnd (M ) consists of all sets that are nondeterministically
one-step provable from M by means of P .
The operator TPnd is nondeterministic as it assigns to each M ⊆ At a family of subsets of At,
each being a possible outcome of applying P to M . In general, T Pnd is partial, since there may be
sets M such that TPnd (M ) = ∅ (no set can be derived from M by means of P ). For instance, if
P (M ) contains a rule r such that hd (r) is inconsistent, then TPnd (M ) = ∅.
Monotone constraints. A constraint (X, C) is monotone if C is closed under superset, that is, for
every W, Y ⊆ X, if W ∈ C and W ⊆ Y then Y ∈ C.
Cardinality and weight constraints provide examples of monotone constraints. Let X be a finite
set and let Ck (X) = {Y : Y ⊆ X, k ≤ |Y |}, where k is a non-negative integer. Then (X, C k (X))
is a constraint expressing the property that a subset of X has at least k elements. We call it a lowerbound cardinality constraint on X and denote it by kX.
A more general class of constraints are weight constraints. Let X be a finite set, say X =
{x1 , . . . , xn }, and let w, w1 , . . . , wn be non-negative reals. We interpret each wi as the weight assigned to xi . A lower-bound weight constraint is a constraint of the form (X, C w ), where Cw consists of those subsets of X whose total weight (the sum of weights of elements in the subset) is at
least w. We write it as
w[x1 = w1 , . . . , xn = wn ].
If all weights are equal to 1 and w is an integer, weight constraints become cardinality constraints. We also note that the constraint C(a) is a cardinality constraint 1{a} and also a weight
constraint 1[a = 1]. Finally, we observe that lower-bound cardinality and weight constraints are
monotone.
Cardinality and weight constraints (in a somewhat more general form) appear in the language of
lparse programs (Simons et al., 2002), which we discuss later in the paper. The notation we adopted
for these constraints in this paper follows the one proposed by Simons et al. (2002).
We use cardinality and weight constraints in some of our examples. They are also the focus of
the last part of the paper, where we use our abstract results to design a new algorithm to compute
models of lparse programs.
Monotone-constraint programs. We call constraint programs built of monotone constraints —
monotone-constraint programs or programs with monotone constraints. That is, monotone-constraint
programs consist of rules of rules of the form (1), where A, A 1 , . . . , Am are monotone constraints.
302

P ROPERTIES AND A PPLICATIONS OF P ROGRAMS WITH M ONOTONE AND C ONVEX C ONSTRAINTS

From now on, unless explicitly stated otherwise, programs we consider are monotone-constraint
programs.
2.1 Horn Programs and Bottom-up Computations
Since we allow constraints with infinite domains and inconsistent constraints in heads of rules, the
results given in this subsection are more general than their counterparts by Marek and Truszczy ński
(2004) and Marek et al. (2004, 2006). Thus, for the sake of completeness, we present them with
proofs.
A rule (1) is Horn if k = m (no occurrences of the negation operator in the body or, equivalently,
only monotone constraints). A constraint program is Horn if every rule in the program is Horn.
With a Horn constraint program we associate bottom-up computations, generalizing the corresponding notion of a bottom-up computation for a normal Horn program.
Definition 1. Let P be a Horn program. A P -computation is a (transfinite) sequence hX α i such
that
1. X0 = ∅,
2. for every ordinal number α, Xα ⊆ Xα+1 and Xα+1 ∈ TPnd (Xα ),
S
3. for every limit ordinal α, Xα = β<α Xβ .
Let t = hXα i be a P -computation. Since for every β < β 0 , Xβ ⊆ Xβ 0 ⊆ At, there is a least
ordinal number αt such that Xαt +1 = Xαt , in other words, a least ordinal when the P -computation
stabilizes. We refer to αt as the length of the P -computation t.
Examples. Here is a simple example showing that some programs have computations of length
exceeding ω and so, the transfinite induction in the definition cannot be avoided. Let P be the
program consisting of the following rules:
({a0 }, {{a0 }}) ← .
({ai }, {{ai }}) ← (Xi−1 , {Xi−1 }), for i = 1, 2, . . .
({a}, {{a}}) ← (X∞ , {X∞ }),
where Xi = {a0 , . . . ai }, 0 ≤ i, and X∞ = {a0 , a1 , . . .}. Since the body of the last rule contains a constraint with an infinite domain X∞ , it does not become applicable in any finite step of
computation. However, it does become applicable in the step ω and so, a ∈ X ω+1 . Consequently,
Xω+1 6= Xω .
4
S
For a P -computation t = hXα i, we call α Xα the result of the computation and denote it by
Rt . Directly from the definitions, it follows that Rt = Xαt .
Proposition 1. Let P be a Horn constraint program and t a P -computation. Then R t is a supported
model of P .
Proof. Let M = Rt be the result of a P -computation t = hXα i. We need to show that: (1) M is a
model of P ; and (2) M ⊆ hset(P (M )).
(1) Let us consider a rule r ∈ P such that M |= bd (r). Since M = Rt = Xαt (where αt is the
length of t), Xαt |= bd (r). Thus, Xαt +1 |= hd (r). Since M = Xαt +1 , M is a model of r and,
consequently, of P , as well.
303

´
L IU & T RUSZCZY NSKI

(2) We will prove by induction that, for every set Xα in the computation t, Xα ⊆ hset(P (M )). The
base case holds since X0 = ∅ ⊆ hset(P (M )).
If α = β + 1, then Xα ∈ TPnd (Xβ ). It follows that Xα ⊆ hset(P (Xβ )). Since P is a Horn
program and Xβ ⊆ M , hset(P (Xβ )) ⊆Shset(P (M )). Therefore, Xα ⊆ hset(P (M )).
If α is a limit ordinal, then Xα = β<α Xβ . By the induction hypothesis, for every β < α,
Xβ ⊆ hset(P (M )). Thus, Xα ⊆ hset(P (M )). By induction, M ⊆ hset(P (M )).
Derivable models. We use computations to define derivable models of Horn constraint programs.
A set M of atoms is a derivable model of a Horn constraint program P if for some P -computation
t, we have M = Rt . By Proposition 1, derivable models of P are supported models of P and so,
also models of P .
Derivable models are similar to the least model of a normal Horn program in that both can be
derived from a program by means of a bottom-up computation. However, due to the nondeterminism
of bottom-up computations of Horn constraint programs, derivable models are not in general unique
nor minimal.
Examples. For example, let P be the following Horn constraint program:
P = {1{a, b} ←}
Then {a}, {b} and {a, b} are its derivable models. The derivable models {a} and {b} are minimal
models of P . The third derivable model, {a, b}, is not a minimal model of P .
4
Since inconsistent monotone constraints may appear in the heads of Horn rules, there are Horn
programs P and sets X ⊆ At, such that TPnd (X) = ∅. Thus, some Horn constraint programs have
no computations and no derivable models. However, if a Horn constraint program has models, the
existence of computations and derivable models is guaranteed.
To see this, let M be a model of a Horn constraint program P . We define a canonical computation tP,M = hXαP,M i by specifying the choice of the next set in the computation in part (2) of
Definition 1. Namely, for every ordinal β, we set
P,M
Xβ+1
= hset(P (XβP,M )) ∩ M.

That is, we include in XαP,M all those atoms occurring in the heads of XβP,M -applicable rules that
belong to M . We denote the result of tP,M by Can(P, M ). Canonical computations are indeed
P -computations.
Proposition 2. Let P be a Horn constraint program. If M ⊆ At is a model of P , the sequence t P,M
is a P -computation.
Proof. As P and M are fixed, to simplify the notation in the proof we will write X α instead of
XαP,M .
To prove the assertion, it suffices to show that (1) hset(P (Xα )) ∩ M ∈ TPnd (Xα ), and (2)
Xα ⊆ hset(P (Xα )) ∩ M , for every ordinal α.
(1) Let X ⊆ M and r ∈ P (X). Since all constraints in bd (r) are monotone, and X |= bd (r), M |=
bd (r), as well. From the fact that M is a model of P it follows now that M |= hd (r). Consequently,
M ∩ hset(P (X)) |= hd (r) for every r ∈ P (X). Since M ∩ hset(P (X)) ⊆ hset(P (X)),
M ∩ hset(P (X)) ∈ TPnd (X).
304

P ROPERTIES AND A PPLICATIONS OF P ROGRAMS WITH M ONOTONE AND C ONVEX C ONSTRAINTS

Directly from the definition of the canonical computation for P and M we obtain that for every
ordinal α, Xα ⊆ M . Thus, (1), follows.
(2) We proceed by induction. The basis is evident as X0 = ∅. Let us consider an ordinal α > 0
and let us assume that (2) holds for every ordinal β < α. If α = β + 1, then X α = Xβ+1 =
hset(P (Xβ )) ∩ M . Thus, by the induction hypothesis, Xβ ⊆ Xα . Since P is a Horn constraint
program, it follows that P (Xβ ) ⊆ P (Xα ). Thus
Xα = Xβ+1 = hset(P (Xβ )) ∩ M ⊆ hset(P (Xα )) ∩ M.
If α is a limit ordinal then for every β < α, Xβ ⊆ Xα and, as before, also P (Xβ ) ⊆ P (Xα ). Thus,
by the induction hypothesis for every β < α,
Xβ ⊆ hset(P (Xβ )) ∩ M ⊆ hset(P (Xα )) ∩ M,
which implies that
Xα =

[

Xβ ⊆ hset(P (Xα )) ∩ M.

β<α

Canonical computations have the following fixpoint property.
Proposition 3. Let P be a Horn constraint program. For every model M of P , we have
hset(P (Can(P, M ))) ∩ M = Can(P, M ).
P,M
= XαP,M = Can(P, M ).
Proof. Let α be the length of the canonical computation tP,M . Then, Xα+1
Since Xα+1 = hset(Xα ) ∩ M , the assertion follows.

We now gather properties of derivable models that extend properties of the least model of normal
Horn logic programs.
Proposition 4. Let P be a Horn constraint program. Then:
1. For every model M of P , Can(P, M ) is a greatest derivable model of P contained in M
2. A model M of P is a derivable model if and only if M = Can(P, M )
3. If M is a minimal model of P then M is a derivable model of P .
Proof. (1) Let M 0 be a derivable model of P such that M 0 ⊆ M . Let T = hXα i be a P -derivation
such that M 0 = Rt . We will prove that for every ordinal α, Xα ⊆ XαP,M . We proceed by transfinite
induction. Since X0 = X0P,M = ∅, the basis for the induction is evident. Let us consider an ordinal
α > 0 and assume that for every ordinal β < α, Xβ ⊆ XβP,M .
If α = β + 1, then Xα ∈ TPnd (Xβ ) and so, Xα ⊆ hset(P (Xβ )). By the induction hypothesis
and by the monotonicity of the constraints in the bodies of rules in P , X α ⊆ hset(P (XβP,M )). Thus,
since Xα ⊆ Rt = M 0 ⊆ M ,
P,M
Xα ⊆ hset(P (XβP,M )) ∩ M = Xβ+1
= XαP,M .

305

´
L IU & T RUSZCZY NSKI

S
S
The case when α is a limit ordinal is straightforward as Xα = β<α Xβ and XαP,M = β<α XβP,M .
(2) (⇐) If M = Can(P, M ), then M is the result of the canonical P -derivation for P and M . In
particular, M is a derivable model of P .
(⇒) if M is a derivable model of P , then M is also a model of P . From (1) it follows that
Can(P, M ) is the greatest derivable model of P contained in M . Since M itself is derivable,
M = Can(P, M ).
(3) From (1) it follows that Can(P, M ) is a derivable model of P and that Can(P, M ) ⊆ M . Since
M is a minimal model, Can(P, M ) = M and, by (2), M is a derivable model of P .
2.2 Stable Models
In this section, we will recall and adapt to our setting the definition of stable models proposed and
studied by Marek and Truszczyński (2004) and Marek et al. (2004, 2006) Let P be a monotoneconstraint program and M a subset of At(P ). The reduct of P , denoted by P M , is a program
obtained from P by:
1. removing from P all rules whose body contains a literal not(B) such that M |= B;
2. removing literals not(B) for the bodies of the remaining rules.
The reduct of a monotone-constraint program is Horn since it contains no occurrences of default
negation. Therefore, the following definition is sound.
Definition 2. Let P be a monotone-constraint program. A set of atoms M is a stable model of P if
M is a derivable model of P M . We denote the set of stable models of P by St(P ).
The definitions of the reduct and stable models follow and generalize those proposed for normal
logic programs, since in the setting of Horn constraint programs, derivable models play the role of
a least model.
As in normal logic programming and its standard extensions, stable models of monotoneconstraint programs are supported models and, consequently, models.
Proposition 5. Let P be a monotone-constraint program. If M ⊆ At(P ) is a stable model of P ,
then M is a supported model of P .
Proof. Let M be a stable model of P . Then, M is a derivable model of P M and, by Proposition 1,
M is a supported model of P M . It follows that M is a model of P M . Directly from the definition
of the reduct it follows that M is a model of P .
It also follows that M ⊆ hset(P M (M )). For every rule r in P M (M ), there is a rule r 0 in P (M ),
which has the same head and the same non-negated literals in the body as r. Thus, hset(P M (M )) ⊆
hset(P (M )) and, consequently, M ⊆ hset(P (M )). It follows that M is a supported model of
P.
Examples. Here is an example of stable models of a monotone-constraint program. Let P be a
monotone-constraint program that contains the following rules:
2{a, b, c} ← 1{a, d}, not(1{c})
1{b, c, d} ← 1{a}, not(3{a, b, d}))
1{a} ←
306

P ROPERTIES AND A PPLICATIONS OF P ROGRAMS WITH M ONOTONE AND C ONVEX C ONSTRAINTS

Let M = {a, b}. Therefore, M 6|= 1{c} and M 6|= 3{a, b, d}. Hence the reduct P M contains the
following three Horn rules:
2{a, b, c} ← 1{a, d}
1{b, c, d} ← 1{a}
1{a} ←
Since M = {a, b} is a derivable model of P M , M is a stable model of P .
0
Let M 0 = {a, b, c}. Then M 0 |= 1{c} and M 6|= 3{a, b, d}. Therefore, the reduct P M contains
two Horn rules:
1{b, c, d} ← 1{a}
1{a} ←
0

Since M 0 = {a, b, c} is a derivable models of P M , M 0 is also a stable model of P . We note that
stable models of a monotone-constraint program, in general, do not form an anti-chain.
4
If a normal logic program is Horn then its least model is its (only) stable model. Here we have
an analogous situation.
Proposition 6. Let P be a Horn monotone-constraint program. Then M ⊆ At(P ) is a derivable
model of P if and only if M is a stable model of P .
Proof. For every set M of atoms P = P M . Thus, M is a derivable model of P if and only if it is a
derivable model of P M or, equivalently, a stable model of P .
In the next four sections of the paper we show that several fundamental results concerning
normal logic programs extend to the class of monotone-constraint programs.

3. Strong and Uniform Equivalence of Monotone-constraint Programs
Strong equivalence and uniform equivalence concern the problem of replacing some rules in a logic
program with others without changing the overall semantics of the program. More specifically,
the strong equivalence concerns replacement of rules within arbitrary programs, and the uniform
equivalence concerns replacements of all non-fact rules. In each case, the stipulation is that the
resulting program must have the same stable models as the original one. Strong (and uniform)
equivalence is an important concept due to its potential uses in program rewriting and optimization.
Strong and uniform equivalence have been studied in the literature mostly for normal logic
programs (Lifschitz et al., 2001; Lin, 2002; Turner, 2003; Eiter & Fink, 2003).
Turner (2003) presented an elegant characterization of strong equivalence of smodels programs,
and Eiter and Fink (2003) described a similar characterization of uniform equivalence of normal
and disjunctive logic programs. We show that both characterizations can be adapted to the case of
monotone-constraint programs. In fact, one can show that under the representations of normal logic
programs as monotone-constraint programs (Marek et al., 2004, 2006) our definitions and characterizations of strong and uniform equivalence reduce to those introduced and developed originally
for normal logic programs.
307

´
L IU & T RUSZCZY NSKI

3.1 M-maximal Models
A key role in our approach is played by models of Horn constraint programs satisfying a certain
maximality condition.
Definition 3. Let P be a Horn constraint program and let M be a model of P . A set N ⊆ M such
that N is a model of P and M ∩hset(P (N )) ⊆ N is an M -maximal model of P , written N |= M P .
Intuitively, N is an M -maximal model of P if N satisfies each rule r ∈ P (N ) “maximally”
with respect to M . That is, for every r ∈ P (N ), N contains all atoms in M that belong to hset(r)
— the domain of the head of r.
To illustrate this notion, let us consider a Horn constraint program P consisting of a single rule:
1{p, q, r} ← 1{s, t}.
Let M = {p, q, s, t} and N = {p, q, s}. One can verify that both M and N are models of P .
Moreover, since the only rule in P is N -applicable, and M ∩ {p, q, r} ⊆ N , N is an M -maximal
model of P . On the other hand, N 0 = {p, s} is not M -maximal even though N 0 is a model of P and
it is contained in M .
There are several similarities between properties of models of normal Horn programs and M maximal models of Horn constraint programs. We state and prove here one of them that turns out to
be especially relevant to our study of strong and uniform equivalence.
Proposition 7. Let P be a Horn constraint program and let M be a model of P . Then M is an
M -maximal model of P and Can(P, M ) is the least M -maximal model of P .
Proof. The first claim follows directly from the definition. To prove the second one, we simplify the
notation: we will write N for Can(P, M ) and Xα for XαP,M .
We first show that N is an M -maximal model of P . Clearly, N ⊆ M . Moreover, by Proposition
3, hset(P (N )) ∩ M = N . Thus, N is indeed an M -maximal model of P .
We now show N is the least M -maximal model of P .
Let N 0 be any M -maximal model of P . We will show by transfinite induction that N ⊆ N 0 .
Since X0 = ∅, the basis for the induction holds. Let us consider an ordinal α > 0 and let us assume
that Xβ ⊆ N 0 , for every β < α. To show N ⊆ N 0 , it is sufficient to show that Xα ⊆ N 0 .
Let us assume that α = β + 1 for some β < α. Then, since Xβ ⊆ N 0 and P is a Horn constraint
program, we have P (Xβ ) ⊆ P (N 0 ). Consequently,
Xα = Xβ+1 = hset(P (Xβ )) ∩ M ⊆ hset(P (N 0 )) ∩ M ⊆ N 0 ,
the last inclusion follows from the factS
thatN 0 is an M -maximal model of P .
If α is a limit ordinal, then Xα = β<α Xβ and the inclusion Xα ⊆ N 0 follows directly from
the induction hypothesis.
3.2 Strong Equivalence and SE-models
Monotone-constraint programs P and Q are strongly equivalent, denoted by P ≡ s Q, if for every
monotone-constraint program R, P ∪ R and Q ∪ R have the same set of stable models.
To study the strong equivalence of monotone-constraint programs, we generalize the concept of
an SE-model due to Turner (2003).
308

P ROPERTIES AND A PPLICATIONS OF P ROGRAMS WITH M ONOTONE AND C ONVEX C ONSTRAINTS

There are close connections between strong equivalence of normal logic programs and the logic
here-and-there. The semantics of the logic here-and-there is given in terms of Kripke models with
two words which, when rephrased in terms of pairs of interpretations (pairs of sets of propositional
atoms), give rise to SE-models.
Definition 4. Let P be a monotone-constraint program and let X, Y be sets of atoms. We say that
(X, Y ) is an SE-model of P if the following conditions hold: (1) X ⊆ Y ; (2) Y |= P ; and (3)
X |=Y P Y . We denote by SE(P ) the set of all SE-models of P .
Examples. To illustrate the notion of an SE-model of a monotone-constraint program, let P consist
of the following two rules:
2{p, q, r} ← 1{q, r}, not(3{p, q, r})}
1{p, s} ← 1{p, r}, not(2{p, r})
We observe that M = {p, q} is a model of P . Let N = ∅. Then N ⊆ M and P M (N ) is empty. It
follows that M ∩ hset(P M (N )) = ∅ ⊆ N and so, N |=M P M . Hence, (N, M ) is an SE-models
of P .
Next, let N 0 = {p}. It is clear that N 0 ⊆ M . Moreover, P M (N 0 ) = {1{p, s} ← 1{p, r}}.
Hence M ∩ hset(P M (N 0 )) = {p} ⊆ N 0 and so, N 0 |=M P M . That is, (N 0 , M ) is another SEmodel of P .
4
SE-models yield a simple characterization of strong equivalence of monotone-constraint programs. To state and prove it, we need several auxiliary results.
Lemma 1. Let P be a monotone-constraint program and let M be a model of P . Then (M, M ) and
(Can(P M , M ), M ) are both SE-models of P .
Proof. The requirements (1) and (2) of an SE-model hold for (M, M ). Furthermore, since M is a
model of P , M |= P M . Finally, we also have hset(P (M )) ∩ M ⊆ M . Thus, M |=M P M .
Similarly, the definition of a canonical computation and Proposition 1, imply the first two requirements of the definition of SE-models for (Can(P M , M ), M ). The third requirement follows
from Proposition 7.
Lemma 2. Let P and Q be two monotone-constraint programs such that SE(P ) = SE(Q). Then
St(P ) = St(Q).
Proof. If M ∈ St(P ), then M is a model of P and, by Lemma 1, (M, M ) ∈ SE(P ). Hence,
(M, M ) ∈ SE(Q) and, in particular, M |= Q. By Lemma 1 again,
(Can(QM , M ), M ) ∈ SE(Q).
By the assumption,
(Can(QM , M ), M ) ∈ SE(P )
and so, Can(QM , M ) |=M P M or, in other terms, Can(QM , M ) is an M -maximal model of P M .
Since M ∈ St(P ), M = Can(P M , M ). By Proposition 7, M is the least M -maximal model
of P M . Thus, M ⊆ Can(QM , M ). On the other hand, we have Can(QM , M ) ⊆ M and so,
M = Can(QM , M ). It follows that M is a stable model of Q. The other inclusion can be proved
in the same way.
309

´
L IU & T RUSZCZY NSKI

Lemma 3. Let P and R be two monotone-constraint programs. Then SE(P ∪ R) = SE(P ) ∩
SE(R).
Proof. The assertion follows from the following two simple observations. First, for every set Y of
atoms, Y |= (P ∪ R) if and only if Y |= P and Y |= R. Second, for every two sets X and Y of
atoms, X |=Y (P ∪ R)Y if and only if X |=Y P Y and X |=Y RY .
Lemma 4. Let P , Q be two monotone-constraint programs. If P ≡ s Q, then P and Q have the
same models.
Proof. Let M be a model of P . By r we denote a constraint rule (M, {M }) ← . Then, M ∈
St(P ∪ {r}). Since P and Q are strongly equivalent, M ∈ St(Q ∪ {r}). It follows that M is a
model of Q ∪ {r} and so, also a model of Q. The converse inclusion can be proved in the same
way.
Theorem 1. Let P and Q be monotone-constraint programs. Then P ≡ s Q if and only if SE(P ) =
SE(Q).
Proof. (⇐) Let R be an arbitrary monotone-constraint program. Lemma 3 implies that SE(P ∪
R) = SE(P ) ∩ SE(R) and SE(Q ∪ R) = SE(Q) ∩ SE(R). Since SE(P ) = SE(Q), we have
that SE(P ∪ R) = SE(Q ∪ R). By Lemma 2, P ∪ R and Q ∪ R have the same stable models.
Hence, P ≡s Q holds.
(⇒) Let us assume SE(P ) \ SE(Q) 6= ∅ and let us consider (X, Y ) ∈ SE(P ) \ SE(Q). It follows
that X ⊆ Y and Y |= P . By Lemma 4, Y |= Q. Since (X, Y ) ∈
/ SE(Q), X 6|= Y QY . It follows
that X 6|= QY or hset(QY (X)) ∩ Y 6⊆ X. In the first case, there is a rule r ∈ QY (X) such that
X 6|= hd (r). Since X ⊆ Y and QY is a Horn constraint program, r ∈ QY (Y ). Let us recall that
Y |= Q and so, we also have Y |= QY . It follows that Y |= hd (r). Since hset(r) ⊆ hset(QY (X)),
Y ∩ hset(QY (X)) |= hd (r). Thus, hset(QY (X)) ∩ Y 6⊆ X (otherwise, by the monotonicity of
hd (r), we would have X |= hd (r)).
The same property holds in the second case. Thus, it follows that
(hset(QY (X)) ∩ Y ) \ X 6= ∅.
We define
X 0 = (hset(QY (X)) ∩ Y ) \ X.
Let R be a constraint program consisting of the following two rules:
(X, {X}) ←
(Y, {Y }) ← (X 0 , {X 0 }).
Let us consider a program Q0 = Q ∪ R. Since Y |= Q and X ⊆ Y , Y |= Q0 . Thus, Y |= QY0 and,
in particular, Can(QY0 , Y ) is well defined. Since R ⊆ QY0 , X ⊆ Can(QY0 , Y ). Thus, we have
hset(QY0 (X)) ∩ Y ⊆ hset(QY0 (Can(QY0 , Y ))) ∩ Y = Can(QY0 , Y )
(the last equality follows from Proposition 3). We also have Q ⊆ Q 0 and so,
X 0 ⊆ hset(QY (X)) ∩ Y ⊆ hset(QY0 (X)) ∩ Y.
310

P ROPERTIES AND A PPLICATIONS OF P ROGRAMS WITH M ONOTONE AND C ONVEX C ONSTRAINTS

Thus, X 0 ⊆ Can(QY0 , Y ). Consequently, by Proposition 3, Y ⊆ Can(QY0 , Y ). Since Can(QY0 , Y )
⊆ Y , Y = Can(QY0 , Y ) and so, Y ∈ St(Q0 ).
Since P and Q are strongly equivalent, Y ∈ St(P0 ), where P0 = P ∪ R. Let us recall that
(X, Y ) ∈ SE(P ). By Proposition 7, Can(P Y , Y ) is a least Y -maximal model of P Y . Since X
is a Y -maximal model of P (as X |=Y P Y ), it follows that Can(P Y , Y ) ⊆ X. Since X 0 6⊆ X,
Can(P0Y , Y ) ⊆ X. Finally, since X 0 ⊆ Y , Y 6⊆ X. Thus, Y 6= Can(P0Y , Y ), a contradiction.
It follows that SE(P ) \ SE(Q) = ∅. By symmetry, SE(Q) \ SE(P ) = ∅, too. Thus, SE(P ) =
SE(Q).
3.3 Uniform Equivalence and UE-models
Let D be a set of atoms. By rD we denote a monotone-constraint rule
rD = (D, {D}) ← .
Adding a rule rD to a program forces all atoms in D to be true (independently of the program).
Monotone-constraint programs P and Q are uniformly equivalent, denoted by P ≡ u Q, if for
every set of atoms D, P ∪ {rD } and Q ∪ {rD } have the same stable models.
An SE-model (X, Y ) of a monotone-constraint program P is a UE-model of P if for every
SE-model (X 0 , Y ) of P with X ⊆ X 0 , either X = X 0 or X 0 = Y holds. We write U E(P ) to
denote the set of all UE-models of P . Our notion of a UE-model is a generalization of the notion of
a UE-model due to Eiter and Fink (2003) to the setting of monotone-constraint programs.
Examples. Let us look again at the program we used to illustrate the concept of an SE-model. We
showed there that (∅, {p, q}) and ({p}, {p, q}) are SE-models of P . Directly from the definition of
UE-models it follows that ({p}, {p, q}) is a UE-model of P .
4
We will now present a characterization of uniform equivalence of monotone-constraint programs
under the assumption that their sets of atoms are finite. One can prove a characterization of uniform
equivalence of arbitrary monotone-constraint programs, generalizing one of the results by Eiter and
Fink (2003). However, both the characterization and its proof are more complex and, for brevity, we
restrict our attention to the finite case only.
We start with an auxiliary result, which allows us to focus only on atoms in At(P ) when deciding whether a pair (X, Y ) of sets of atoms is an SE-model of a monotone-constraint program
P.
Lemma 5. Let P be a monotone-constraint program, X ⊆ Y two sets of atoms. Then (X, Y ) ∈
SE(P ) if and only if (X ∩ At(P ), Y ∩ At(P )) ∈ SE(P ).
Proof. Since X ⊆ Y is given, and X ⊆ Y implies X ∩ At(P ) ⊆ Y ∩ At(P ), the first condition of
the definition of an SE-model holds on both sides of the equivalence.
Next, we note that for every constraint C, Y |= C if and only if Y ∩ Dom(C) |= C. Therefore,
Y |= P if and only if Y ∩At(P ) |= P . That is, the second condition of the definition of an SE-model
holds for (X, Y ) if and only if it holds for (X ∩ At(P ), Y ∩ At(P )).
Finally, we observe that P Y = P Y ∩At(P ) and P (X) = P (X ∩ At(P )). Therefore,
Y ∩ hset(P Y (X)) = Y ∩ hset(P Y ∩At(P ) (X ∩ At(P ))).
Since hset(P Y ∩At(P ) (X ∩ At(P ))) ⊆ At(P ), it follows that
Y ∩ hset(P Y (X)) ⊆ X
311

´
L IU & T RUSZCZY NSKI

if and only if
Y ∩ At(P ) ∩ hset(P Y ∩At(P ) (X ∩ At(P ))) ⊆ X ∩ At(P ).
Thus, X |=Y P Y if and only if X ∩ At(P ) |=Y ∩At(P ) P Y ∩At(P ) . That is, the third condition of the
definition of an SE-model holds for (X, Y ) if and only if it holds for (X ∩ At(P ), Y ∩ At(P )).
Lemma 6. Let P be a monotone-constraint program such that At(P ) is finite. Then for every
(X, Y ) ∈ SE(P ) such that X 6= Y , the set
{X 0 : X ⊆ X 0 ⊆ Y, X 0 6= Y, (X 0 , Y ) ∈ SE(P )}

(2)

has a maximal element.
Proof. If At(P )∩X = At(P )∩Y , then for every element y ∈ Y \X, Y \{y} is a maximal element
of the set (2). Indeed, since (X, Y ) ∈ SE(P ), by Lemma 5, (X ∩ At(P ), Y ∩ At(P )) ∈ SE(P ).
Since X ∩ At(P ) = Y ∩ At(P ) and y 6∈ At(P ), X ∩ At(P ) = (Y \ {y}) ∩ At(P ). Therefore,
((Y \{y})∩At(P ), Y ∩At(P )) ∈ SE(P ). Then from Lemma 5 and the fact Y \{y} ⊆ Y , we have
(Y \ {y}, Y ) ∈ SE(P ). Therefore, Y \ {y} belongs to the set (2) and so, it is a maximal element
of this set.
Thus, let us assume that At(P ) ∩ X 6= At(P ) ∩ Y . Let us define X 0 = X ∪ (Y \ At(P )).
Then X ⊆ X 0 ⊆ Y and X 0 6= Y . Moreover, no element in X 0 \ X belongs to At(P ). That is,
X 0 ∩ At(P ) = X ∩ At(P ). Thus, by Lemma 5, (X 0 , Y ) ∈ SE(P ) and so, X 0 belongs to the set
(2). Since Y \ X 0 ⊆ At(P ), by the finiteness of At(P ) it follows that the set (2) contains a maximal
element containing X 0 . In particular, it contains a maximal element.
Theorem 2. Let P and Q be two monotone-constraint programs such that At(P ) ∪ At(Q) is finite.
Then P ≡u Q if and only if U E(P ) = U E(Q).
Proof. (⇐) Let D be an arbitrary set of atoms and Y be a stable model of P ∪ {r D }. Then Y is
a model of P ∪ {rD }. In particular, Y is a model of P and so, (Y, Y ) ∈ U E(P ). It follows that
(Y, Y ) ∈ U E(Q), too. Thus, Y is a model of Q. Since Y is a model of r D , D ⊆ Y . Consequently,
Y is a model of Q ∪ {rD } and thus, also of (Q ∪ {rD })Y .
Let X = Can((Q ∪ {rD })Y , Y ). Then D ⊆ X ⊆ Y and, by Proposition 7, X is a Y -maximal
model of (Q ∪ {rD })Y . Consequently, X is a Y -maximal model of QY . Since X ⊆ Y and Y |= Q,
(X, Y ) ∈ SE(Q).
Let us assume that X 6= Y . Then, by Lemma 6, there is a maximal set X 0 such that X ⊆ X 0 ⊆
Y , X 0 6= Y and (X 0 , Y ) ∈ SE(Q). It follows that (X 0 , Y ) ∈ U E(Q). Thus, (X 0 , Y ) ∈ U E(P )
and so, X 0 |=Y P Y . Since D ⊆ X 0 , X 0 |=Y (P ∪ {rD })Y . We recall that Y is a stable model of
P ∪ {rD }. Thus, Y = Can((P ∪ {rD })Y , Y ). By Proposition 7, Y ⊆ X 0 and so we get X 0 = Y ,
a contradiction. It follows that X = Y and, consequently, Y is a stable model of Q ∪ {r D }.
By symmetry, every stable model of Q ∪ {rD } is also a stable model of P ∪ {rD }.
(⇒) First, we note that (Y, Y ) ∈ U E(P ) if and only if Y is a model of P . Next, we note that P and
Q have the same models. Indeed, the argument used in the proof of Lemma 4 works also under the
assumption that P ≡u Q. Thus, (Y, Y ) ∈ U E(P ) if and only if (Y, Y ) ∈ U E(Q).
Now let us assume that U E(P ) 6= U E(Q). Let (X, Y ) be an element of (U E(P ) \ U E(Q)) ∪
(U E(Q) \ U E(P )). Without loss of generality, we can assume that (X, Y ) ∈ U E(P ) \ U E(Q).
Since (X, Y ) ∈ U E(P ), it follows that
312

P ROPERTIES AND A PPLICATIONS OF P ROGRAMS WITH M ONOTONE AND C ONVEX C ONSTRAINTS

1. X ⊆ Y
2. Y |= P and, consequently, Y |= Q
3. X 6= Y (otherwise, by our earlier observations, (X, Y ) would belong to U E(Q)).
Let R = (Q ∪ {rX })Y . Clearly, R is a Horn constraint program. Moreover, since Y |= Q and
X ⊆ Y , Y |= R. Thus, Can(R, Y ) is defined. We have X ⊆ Can(R, Y ) ⊆ Y . We claim that
Can(R, Y ) 6= Y . Let us assume to the contrary that Can(R, Y ) = Y . Then Y ∈ St(Q ∪ {r X }).
Hence, Y ∈ St(P ∪ {rX }), that is, Y = Can((P ∪ {rX })Y , Y ). By Proposition 7, Y is the
least Y -maximal model of (P ∪ {rX })Y and X is a Y -maximal model of (P ∪ {rX })Y (since
(X, Y ) ∈ SE(P ), X |=Y P Y and so, X |=Y (P ∪ {rX })Y , too). Consequently, Y ⊆ X and, as
X ⊆ Y , X = Y , a contradiction.
Thus, Can(R, Y ) 6= Y . By Proposition 7, Can(R, Y ) is a Y -maximal model of R. Since
QY ⊆ R, it follows that Can(R, Y ) is a Y -maximal model of QY and so, (Can(R, Y ), Y ) ∈
SE(Q). Since Can(R, Y ) 6= Y , from Lemma 6 it follows that there is a maximal set X 0 such that
Can(R, Y ) ⊆ X 0 ⊆ Y , X 0 6= Y and (X 0 , Y ) ∈ SE(Q). By the definition, (X 0 , Y ) ∈ U E(Q).
Since (X, Y ) ∈
/ U E(Q). X 6= X 0 . Consequently, since X ⊆ X 0 , X 0 6= Y and (X, Y ) ∈ U E(P ),
(X 0 , Y ) ∈
/ U E(P ).
Thus, (X 0 , Y ) ∈ U E(Q) \ U E(P ). By applying now the same argument as above to (X 0 , Y )
we show the existence of X 00 such that X 0 ⊆ X 00 ⊆ Y , X 0 6= X 00 , X 00 6= Y and (X 00 , Y ) ∈ SE(P ).
Consequently, we have X ⊆ X 00 , X 6= X 00 and Y 6= X 00 , which contradicts the fact that (X, Y ) ∈
U E(P ). It follows then that U E(P ) = U E(Q).
Examples. Let P = {1{p, q} ← not(2{p, q})}, and Q = {p ← not(q), q ← not(p)}. Then
P and Q are strongly equivalent. We note that both programs have {p}, {q}, and {p, q} as models.
Furthermore, ({p}, {p}), ({q}, {q}), ({p}, {p, q}), ({q}, {p, q}), ({p, q}, {p, q}) and (∅, {p, q}) are
“all” SE-models of the two programs 3 .
Thus, by Theorem 1, P and Q are strongly equivalent.
We also observe that the first five SE-models are precisely UE-models of P and Q. Therefore,
by Theorem 2, P and Q are also uniformly equivalent.
It is possible for two monotone-constraint programs to be uniformly but not strongly equivalent.
If we add rule p ← to P , and rule p ← q to Q, then the two resulting programs, say P 0 and Q0 , are
uniformly equivalent. However, they are not strongly equivalent. The programs P 0 ∪ {q ← p} and
Q0 ∪ {q ← p} have different stable models. Another way to show it is by observing that (∅, {p, q})
is an SE-model of Q0 but not an SE-model of P 0 .
4

4. Fages Lemma
In general, supported models and stable models of a logic program (both in the normal case and the
monotone-constraint case) do not coincide. Fages Lemma (Fages, 1994), later extended by Erdem
and Lifschitz (2003), establishes a sufficient condition under which a supported model of a normal logic program is stable. In this section, we show that Fages Lemma extends to programs with
monotone constraints.
3. From Lemma 5 and Theorem 1, it follows that only those SE-models that contain atoms only from At(P ) ∪ At(Q)
are the essential ones.

313

´
L IU & T RUSZCZY NSKI

Definition 5. A monotone-constraint program P is called tight on a set M ⊆ At(P ) of atoms, if
there exists a mapping λ from M to ordinals such that for every rule A ← A 1 , . . . , Ak , not(Ak+1 ),
. . . , not(Am ) in P (M ), if X is the domain
S of A and Xi the domain of Ai , 1 ≤ i ≤ k, then for
every x ∈ M ∩ X and for every a ∈ M ∩ ki=1 Xi , λ(a) < λ(x).
We will now show that tightness provides a sufficient condition for a supported model to be
stable. In order to prove a general result, we first establish it in the Horn case.
Lemma 7. Let P be a Horn monotone-constraint program and let M be a supported model of P .
If P is tight on M , then M is a stable model of P .
Proof. Let M be an arbitrary supported model of P such that P is tight on M . Let λ be a mapping
showing the tightness of P on M . We will show that for every ordinal α and for every atom x ∈ M
such that λ(x) ≤ α, x ∈ Can(P, M ). We will proceed by induction.
For the basis of the induction, let us consider an atom x ∈ M such that λ(x) = 0. Since M is a
supported model for P and x ∈ M , there exists a rule r ∈ P (M ) such that x ∈ hset(r). Moreover,
since P is tight on M , for every A ∈ bd (r) and for every y ∈ Dom(A) ∩ M , λ(y) < λ(x) = 0.
Thus, for every A ∈ bd (r), Dom(A) ∩ M = ∅. Since M |= bd (r) and since P is a Horn monotoneconstraint program, it follows that ∅ |= bd (r). Consequently, hset(r) ∩ M ⊆ Can(P, M ) and so,
x ∈ Can(P, M ).
Let us assume that the assertion holds for every ordinal β < α and let us consider x ∈ M such
that λ(x) = α. As before, since M is a supported model of P , there exists a rule r ∈ P (M ) such
that x ∈ hset(r). By the assumption, P is tight on M and, consequently, for every A ∈ bd (r) and
for every y ∈ Dom(A) ∩ M , λ(y) < λ(x) = α. By the induction hypothesis, for every A ∈ bd (r),
Dom(A) ∩ M ⊆ Can(P, M ). Since P is a Horn monotone-constraint program, Can(P, M ) |=
bd (r). By Proposition 3, hset(r) ∩ M ⊆ Can(P, M ) and so, x ∈ Can(P, M ).
It follows that M ⊆ Can(P, M ). By the definition of a canonical computation, we have
Can(P, M ) ⊆ M . Thus, M = Can(P, M ). By Proposition 6, M is a stable model of P .
Given this lemma, the general result follows easily.
Theorem 3. Let P be a monotone-constraint program and let M be a supported model of P . If P
is tight on M , then M is a stable model of P .
Proof. One can check that if M is a supported model of P , then it is a supported model of the reduct
P M . Since P is tight on M , the reduct P M is tight on M , too. Thus, M is a stable model of P M
(by Lemma 7) and, consequently, a derivable model of P M (by Proposition 6). It follows that M is
a stable model of P .

5. Logic PLmc and the Completion of a Monotone-constraint Program
The completion of a normal logic program (Clark, 1978) is a propositional theory whose models are
precisely supported models of the program. Thus, supported models of normal logic programs can
be computed by means of SAT solvers. Under some conditions, for instance, when the assumptions
of Fages Lemma hold, supported models are stable. Thus, computing models of the completion
can yield stable models, an idea implemented in the first version of cmodels software (Babovich &
Lifschitz, 2002).
314

P ROPERTIES AND A PPLICATIONS OF P ROGRAMS WITH M ONOTONE AND C ONVEX C ONSTRAINTS

Our goal is to extend the concept of the completion to programs with monotone constraints. The
completion, as we define it, retains much of the structure of monotone-constraint rules and allow
us, in the restricted setting of lparse programs, to use pseudo-boolean constraint solvers to compute
supported models of such programs. In this section we define the completion and prove a result
relating supported models of programs to models of the completion. We discuss extensions of this
result in the next section and their practical computational applications in Section 8.
To define the completion, we first introduce an extension of propositional logic with monotone
constraints, a formalism we denote by PLmc . A formula in the logic PLmc is an expression built
from monotone constraints by means of boolean connectives ∧, ∨ (and their infinitary counterparts),
→ and ¬. The notion of a model of a constraint, which we discussed earlier, extends in a standard
way to the class of formulas in the logic PLmc .
For a set L = {A1 , . . . , Ak , not(Ak+1 ), . . . , not(Am )} of literals, we define
L∧ = A1 ∧ . . . ∧ Ak ∧ ¬Ak+1 ∧ . . . ∧ ¬Am .
Let P be a monotone-constraint program. We form the completion of P , denoted Comp(P ), as
follows:
1. For every rule r ∈ P we include in Comp(P ) a PLmc formula
[bd (r)]∧ → hd (r)
2. For every atom x ∈ At(P ), we include in Comp(P ) a PLmc formula
_
x → {[bd (r)]∧ : r ∈ P, x ∈ hset(r)}
(we note that when the set of rules in P is infinite, the disjunction may be infinitary).
The following theorem generalizes a fundamental result on the program completion from normal
logic programming (Clark, 1978) to the case of programs with monotone constraints.
Theorem 4. Let P be a monotone-constraint program. A set M ⊆ At(P ) is a supported model of
P if and only if M is a model of Comp(P ).
Proof. (⇒) Let us suppose that M is a supported model of P . Then M is a model of P , that is, for
each rule r ∈ P , if M |= bd (r) then M |= hd (r). Since M |= bd (r) if and only if M |= [bd (r)] ∧ ,
it follows that all formulas in Comp(P ) of the first type are satisfied by M .
Moreover, since M is a supported model of P , M ⊆ hset(P (M )). That is, for every atom
x ∈ M , there exists at least one rule r in P such that x ∈ hset(r) and M |= bd (r). Therefore, all
formulas in Comp(P ) of the second type are satisfied by M , too.
(⇐) Let us now suppose that M is a model of Comp(P ). Since M |= bd (r) if and only if M |=
[bd (r)]∧ , and since M satisfies formulas of the first type
W in Comp(P ), M is a model of P .
Let x ∈ M . W
Since M satisfies the formula x → {[bd (r)]∧ : r ∈ P, x ∈ hset(r)}, it follows
that M satisfies {[bd (r)]∧ : r ∈ P, x ∈ hset(r)}. That is, there is r ∈ P such that M satisfies
[bd (r)]∧ (and so, bd (r), too) and x ∈ hset(r). Thus, x ∈ hset(P (M )). Hence, M is a supported
model of P .
Theorems 3 and 4 have the following corollary.
315

´
L IU & T RUSZCZY NSKI

Corollary 5. Let P be a monotone-constraint program. A set M ⊆ At(P ) is a stable model of P if
P is tight on M and M is a model of Comp(P ).
We observe that for the material in this section it is not necessary to require that constraints
appearing in the bodies of program rules be monotone. However, since we are only interested in this
case, we adopted the monotonicity assumption here, as well.

6. Loops and Loop Formulas in Monotone-constraint Programs
The completion alone is not quite satisfactory as it relates supported not stable models of monotoneconstraint programs with models of PLmc theories. Loop formulas, proposed by Lin and Zhao
(2002), provide a way to eliminate those supported models of normal logic programs, which are not
stable. Thus, they allow us to use SAT solvers to compute stable models of arbitrary normal logic
programs and not only those, for which supported and stable models coincide.
We will now extend this idea to monotone-constraint programs. In this section, we will restrict
our considerations to programs P that are finitary, that is, At(P ) is finite. This restriction implies
that monotone constraints that appear in finitary programs have finite domains.
Let P be a finitary monotone-constraint program. The positive dependency graph of P is the
directed graph GP = (V, E), where V = At(P ) and hu, vi is an edge in E if there exists a rule
r ∈ P such that u ∈ hset(r) and v ∈ Dom(A) for some monotone constraint A ∈ bd (r) (that is,
A appears non-negated in bd (r)). We note that positive dependency graphs of finitary programs are
finite.
Let G = (V, E) be a directed graph. A set L ⊆ V is a loop in G if the subgraph of G induced
by L is strongly connected. A loop is maximal if it is not a proper subset of any other loop in G.
Thus, maximal loops are vertex sets of strongly connected components of G. A maximal loop is
terminating if there is no edge in G from L to any other maximal loop.
These concepts can be extended to the case of programs. By a loop (maximal loop, terminating
loop) of a monotone-constraint program P , we mean the loop (maximal loop, terminating loop)
of the positive dependency graph GP of P . We observe that every finitary monotone-constraint
program P has a terminating loop, since GP is finite.
Let X ⊆ At(P ). By GP [X] we denote the subgraph of GP induced by X. We observe that if
X 6= ∅ then every loop of GP [X] is a loop of GP .
Let P be a monotone-constraint program P . For every model M of P (in particular, for every
model M of Comp(P )), we define M − = M \ Can(P M , M ). Since M is a model of P , M is a
model of P M . Thus, Can(P M , M ) is well defined and so is M − .
For every loop in the graph GP we will now define the corresponding loop formula. First, for a
constraint A = (X, C) and a set L ⊆ At, we set A|L = (X, {Y ∈ C : Y ∩ L = ∅}) and call A|L
the restriction of A to L. Next, let r be a monotone-constraint rule, say
r = A ← A1 , . . . , Ak , not(Ak+1 ), . . . , not(Am ).
If L ⊆ At, then define a PLmc formula βL (r) by setting
βL (r) = A1 |L ∧ . . . ∧ Ak |L ∧ ¬Ak+1 ∧ . . . ∧ ¬Am .
Let L be a loop of a monotone-constraint program P . Then, the loop formula for L, denoted by
LP (L), is the PLmc formula
_
_
LP (L) =
L → {βL (r) : r ∈ P and L ∩ hset(r) 6= ∅}
316

P ROPERTIES AND A PPLICATIONS OF P ROGRAMS WITH M ONOTONE AND C ONVEX C ONSTRAINTS

(we recall that we use the convention to write a for the constraint C(a) = ({a}, {{a}}). A loop
completion of a finitary monotone-constraint program P is the PL mc theory
LComp(P ) = Comp(P ) ∪ {LP (L) : L is a loop in GP }.
The following theorem exploits the concept of a loop formula to provide a necessary and sufficient condition for a model being a stable model. transfinite one.
Theorem 6. Let P be a finitary monotone-constraint program. A set M ⊆ At(P ) is a stable model
of P if and only if M is a model of LComp(P ).
Proof. (⇒) Let M be a stable model of P . Then M is a supported model of P and, by Theorem 4,
M |= Comp(P ).
Let L be a loop in P . If M ∩ L = ∅ then M |= LP (L). Thus, let us assume that M ∩ L 6= ∅.
Since M is a stable model of P , M is a derivable model of P M , that is, M = Can(P M , M ).
Let (Xn )n=0,1,... be the canonical P M -derivation with respect to M (since we assume that P is
finite and each constraint in P has a finite domain, P -derivations reach their results in finitely many
steps). Since Can(P M , M ) ∩ L = M ∩ L 6= ∅, there is a smallest index n such that Xn ∩ L 6= ∅.
In particular, it follows that n > 0 (as X0 = ∅) and L ∩ Xn−1 = ∅.
Since Xn = hset(P M (Xn−1 ) ∩ M and Xn ∩ L 6= ∅, there is a rule r ∈ P M (Xn−1 ) such that
hset(r) ∩ L 6= ∅, that is, such that L ∩ hset(r)) 6= ∅. Let r 0 be a rule in P , which contributes r to
P M . Then, for every literal not(A) ∈ bd (r 0 ), M |= not(A). Let A ∈ bd (r 0 ). Then A ∈ bd (r) and
so, Xn−1 |= A. Since Xn−1 ∩ L = ∅, Xn−1 |= A|L , too, By the monotonicity of A|L , M |= A|L .
Thus, M |= βL (r0 ). Since hset(r 0 ) ∩ L 6= ∅, L ∩ hset(r)) 6= ∅ and so, M |= LP (L). Thus,
M |= LComp(P ).
(⇐) Let us consider a set M ⊆ At(P ) such that M is not a stable model of P . If M is not a
supported model of P that M 6|= Comp(P ) and so M is not a model of LComp(P ). Thus, let us
assume that M is a supported model of P . It follows that M − 6= ∅. Let L ⊆ M − be a terminating
loop for GP [M − ].
Let r0 be an arbitrary rule in P such that L∩hset(r 0 )) 6= ∅, and let r be the rule obtained from r 0
by removing negated constraints from its body. Now, let us assume that M |= β r0 (L). It follows that
for every literal not(A) ∈ bd (r 0 ), M |= not(A). Thus, r ∈ P M . Moreover, since L is a terminating
loop for GP [M − ], for every constraint A ∈ bd (r 0 ), Dom(A)∩M − ⊆ L. Since M |= A|L , it follows
that Can(P M , M ) |= A. Consequently, hset(r 0 ) ∩ LW⊆ hset(r 0 ) ∩ M ⊆ Can(P M , M ) and so,
0
0
0
L ∩ Can(P M
W, M ) 6= ∅, a contradiction. Thus, M 6|= {βr (L) : r ∈ P and L ∩ hset(r )) 6= ∅}.
Since M |= L, it follows that M 6|= LP (L) and so, M 6|= LComp(P ).
The following result follows directly from the proof of Theorem 6 and provides us with a way
to filter out specific non-stable supported models from Comp(P ).
Theorem 7. Let P be a finitary monotone-constraint program and M a model of Comp(P ). If M −
is not empty, then M violates the loop formula of every terminating loop of G P [M − ].
Finally, we point out that, Theorem 6 does not hold when a program P contains infinitely many
rules. Here is a counterexample:
Examples. Let P be the set of following rules:
317

´
L IU & T RUSZCZY NSKI

1{a0 } ← 1{a1 }
1{a1 } ← 1{a2 }
···
1{an } ← 1{an+1 }
···
Let M = {a0 , . . . , an , . . .}. Then M is a supported model of P . The only stable model of P is
∅. However, M − = M \ ∅ does not contain any terminating loop. The problem arises because there
is an infinite simple path in GP [M − ]. Therefore, GP [M − ] does not have a sink, yet it does not have
a terminating loop either.
4
The results of this section, concerning the program completion and loop formulas — most importantly, the loop-completion theorem — form the basis of a new software system to compute
stable models of lparse programs. We discuss this matter in Section 8.

7. Programs with Convex Constraints
We will now discuss programs with convex constraints, which are closely related to programs with
monotone constraints. Programs with convex constraints are of interest as they do not involve explicit occurrences of the default negation operator not, yet are as expressive as programs with
monotone-constraints. Moreover, they directly subsume an essential fragment of the class of lparse
programs (Simons et al., 2002).
A constraint (X, C) is convex, if for every W, Y, Z ⊆ X such that W ⊆ Y ⊆ Z and W, Z ∈ C,
we have Y ∈ C. A constraint rule of the form (1) is a convex-constraint rule if A, A 1 , . . . , An are
convex constraints and m = k. Similarly, a constraint program built of convex-constraint rules is a
convex-constraint program.
The concept of a model discussed in Section 2 applies to convex-constraint programs. To define
supported and stable models of convex-constraint programs, we view them as special programs with
monotone-constraints.
To this end, we define the upward and downward closures of a constraint A = (X, C) to be
constraints A+ = (X, C + ) and A− = (X, C − ), respectively, where
C + = {Y ⊆ X : for some W ∈ C, W ⊆ Y }, and
C − = {Y ⊆ X : for some W ∈ C, Y ⊆ W }.
We note that the constraint A+ is monotone. We call a constraint (X, C) antimonotone if C is closed
under subset, that is, for every W, Y ⊆ X, if Y ∈ C and W ⊆ Y then W ∈ C. It is clear that the
constraint A− is antimonotone.
The upward and downward closures allow us to represent any convex constraint as the “conjunction” of a monotone constraint and an antimonotone constraint.Namely, we have the following
property of convex constraints.
Proposition 8. A constraint (X, C) is convex if and only if C = C + ∩ C − .
Proof. (⇐) Let us assume that C = C + ∩C − and let us consider a set M such that M 0 ⊆ M ⊆ M 00 ,
where M 0 , M 00 ∈ C. it follows that M 0 ∈ C + and M 00 ∈ C − . Thus, M ∈ C + and M ∈ C − .
Consequently, M ∈ C, which implies that (X, C) is convex.
318

P ROPERTIES AND A PPLICATIONS OF P ROGRAMS WITH M ONOTONE AND C ONVEX C ONSTRAINTS

(⇒) The definitions directly imply that C ⊆ C + and C ⊆ C − . Thus, C ⊆ C + ∩ C − . Let us
consider M ∈ C + ∩ C − . Then there are sets M 0 , M 00 ∈ C such that M 0 ⊆ M and M ⊆ M 00 . Since
C is convex, M ∈ C. Thus, C + ∩ C − ⊆ C and so, C = C + ∩ C − .
Proposition 8 suggests an encoding of convex-constraint programs as monotone-constraint programs. To present it, we need more notation. For a constraint A = (X, C), we call the constraint
(X, C), where C = P(X) \ C, the dual constraint for A. We denote it by A. It is a direct consequence of the definitions that a constraint A is monotone if and only if its dual A is antimonotone.
Let C be a convex constraint. We set mc(C) = {C} if C is monotone. We set mc(C) =
{not(C)}, if C is antimonotone. We define mc(C) = {C + , not(C − )}, if C is neither monotone
nor antimonotone. Clearly, C and mc(C) have the same models.
Let P be a convex-constraint program. By mc(P ) we denote the program with monotone constraints obtained by replacing every rule r in P with a rule r 0 such that
[
hd (r0 ) = hd (r)+ and bd (r 0 ) = {mc(A) : A ∈ bd (r)}
and, if hd (r) is not monotone, also with an additional rule r 00 such that
hd (r00 ) = (∅, ∅) and bd (r 00 ) = {hd (r)− } ∪ bd (r 0 ).
By our observation above, all constraints appearing in rules of mc(P ) are indeed monotone, that is,
mc(P ) is a program with monotone constraints.
It follows from Proposition 8 that M is a model of P if and only if M is a model of mc(P ). We
extend this correspondence to supported and stable models of a convex constraint program P and
the monotone-constraint program mc(P ).
Definition 6. Let P be a convex constraint program. Then a set of atoms M is a supported (or
stable) model of P if M is a supported (or stable) model of mc(P ).
With these definitions, monotone-constraint programs can be viewed (almost) directly as convexconstraint programs. Namely, we note that monotone and antimonotone constraints are convex.
Next, we observe that if A is a monotone constraint, the expression not(A) has the same meaning
as the antimonotone constraint A in the sense that for every interpretation M , M |= not(A) if and
only if M |= A.
Let P be a monotone-constraint program. By cc(P ) we denote the program obtained from P by
replacing every rule r of the form (1) in P with r 0 such that
[
[
hd (r0 ) = hd (r) and bd (r 0 ) = {Ai : i = 1, . . . , k} ∪ {Aj : j = k + 1, . . . , m}
One can show that programs P and cc(P ) have the same models, supported models and stable
models. In fact, for every monotone-constraint program P we have P = mc(cc(P )).
Remark. Another consequence of our discussion is that the default negation operator can be eliminated from the syntax at the price of allowing antimonotone constraints and using antimonotone
constraints as negated literals.
2
Due to the correspondences we have established above, one can extend to convex-constraint
programs all concepts and results we discussed earlier in the context of monotone-constraint programs. In many cases, they can also be stated directly in the language of convex-constraints. The
319

´
L IU & T RUSZCZY NSKI

most important for us are the notions of the completion and loop formulas, as they lead to new
algorithms for computing stable models of lparse programs. Therefore, we will now discuss them
in some detail.
As we just mentioned, we could use Comp(mc(P )) as a definition of the completion Comp(P )
for a convex-constraint logic program P . Under this definition Theorems 9 extends to the case
of convex-constraint programs. However, Comp(mc(P )) involves monotone constraints and their
negations and not convex constraints that appear in P . Therefore, we will now propose another
approach, which preserves convex constraints of P .
To this end, we first extend the logic PLmc with convex constraints. In this extension, which
we denote by PLcc and refer to as the propositional logic with convex-constraints, formulas are
boolean combinations of convex constraints. The semantics of such formulas is given by the notion
of a model obtained by extending over boolean connectives the concept of a model of a convex
constraint.
Thus, the only difference between the logic PLmc , which we used to define the completion and
loop completion for monotone-convex programs and the logic PL cc is that the former uses monotone constraints as building blocks of formulas, whereas the latter is based on convex constraints. In
fact, since monotone constraints are special convex constraints, the logic PL mc is a fragment of the
logic PLcc .
Let P be a convex-constraint program. The completion of P , denoted by
Comp(P ), is the following set of PLcc formulas:
1. For every rule r ∈ P we include in Comp(P ) a PLcc formula
[bd (r)]∧ → hd (r)
(as before, for a set of convex constraints L, L∧ denotes the conjunction of the constraints in
L)
2. For every atom x ∈ At(P ), we include in Comp(P ) a PLcc formula
_
x → {[bd (r)]∧ : r ∈ P, x ∈ hset(r)}
(again, we note that when the set of rules in P is infinite, the disjunction may be infinitary).
One can now show the following theorem.
Theorem 8. Let P be a convex-constraint program and let M ⊆ At(P ). Then M is a supported
model of P if and only if M is a model of Comp(P ).
Proof. (Sketch) By the definition, M is a supported model of P if and only if M is a supported
model of mc(P ). It is a matter of routine checking that Comp(mc(P )) and Comp(P ) have the
same models. Thus the assertion follows from Theorem 4.
Next, we restrict attention to finitary convex-constraint programs, that is, programs with finite
set of atoms, and extend to this class of programs the notions of the positive dependency graph and
loops. To this end, we exploit its representation as a monotone-constraint program mc(P ). That is,
we define the positive dependency graph, loops and loop formulas for P as the positive dependency
graph, loops and loop formulas of mc(P ), respectively. In particular, L is a loop of P if and only if
320

P ROPERTIES AND A PPLICATIONS OF P ROGRAMS WITH M ONOTONE AND C ONVEX C ONSTRAINTS

L is a loop of mc(P ) and the loop formula for L, with respect to a convex-constraint program P ,
is defined as the loop formula LP (L) with respect to the program mc(P ) 4 . We note that since loop
formulas for monotone-constraint programs only modify non-negated literals in the bodies of rules
and leave negated literals intact, there seems to be no simple way to extend the notion of a loop
formula to the case of a convex-constraint program P without making references to mc(P ).
We now define a loop completion of a finitary convex-constraint program P as the PL cc theory
LComp(P ) = Comp(P ) ∪ {LP (L) : L is a loop of P }.
We have the following theorem that provides a necessary and sufficient condition for a set of
atoms to be a stable model of a convex-constraint program.
Theorem 9. Let P be a finitary convex-constraint program. A set M ⊆ At(P ) is a stable model of
P if and only if M is a model of LComp(P ).
Proof. (Sketch) Since M is a stable model of P if and only of M is a stable model of mc(P ), Theorem 6 implies that M is a stable model of P if and only if M is a stable model of LComp(mc(P )).
It is a matter of routine checking that LComp(mc(P )) and LComp(P ) have the same models.
Thus, the result follows.
In a similar way, Theorem 7 implies the following result for convex-constraint programs.
Theorem 10. Let P be a finitary convex-constraint program and M a model of Comp(P ). If M −
is not empty, then M violates the loop formula of every terminating loop of G P [M − ].
We emphasize that one could simply use LComp(mc(P )) as a definition of the loop completion
for a convex-constraint logic program. However, our definition of the completion component of the
loop completion retains the structure of constraints in a program P , which is important when using
loop completion for computation of stable models, the topic we address in the next section of the
paper.

8. Applications
In this section, we will use theoretical results on the program completion, loop formulas and loop
completion of programs with convex constraints to design and implement a new method for computing stable models of lparse programs (Simons et al., 2002).
8.1 Lparse Programs
Simons et al. (2002) introduced and studied an extension of normal logic programming with weight
atoms. Formally, a weight atom is an expression
A = l[a1 = w1 , . . . , ak = wk ]u,
where ai , 1 ≤ i ≤ k are propositional atoms, and l, u and wi , 1 ≤ i ≤ k are non-negative integers.
If all weights wi are equal to 1, A is a cardinality atom, written as l{a1 , . . . , ak }u.
4. There is one minor simplification one might employ. For a monotone constraint A, ¬A and A are equivalent and
A is antimonotone and so, convex. Thus, we can eliminate the operator ¬ from loop formulas of convex-constraint
programs by writing A instead of ¬A.

321

´
L IU & T RUSZCZY NSKI

An lparse rule is an expression of the form
A ← A 1 , . . . , An
where A, A1 , . . . , An are weight atoms. We refer to sets of lparse rules as lparse programs. Simons
et al. (2002) defined for lparse programs the semantics of stable models.
A set M of atoms is a model of (or satisfies) a weight atom l[a1 = w1 , . . . , ak = wk ]u if
l≤

k
X
{wi : ai ∈ M } ≤ u.
i=1

With this semantics a weight atom l[a1 = w1 , . . . , ak = wk ]u can be identified with a constraint
(X, C), where X = {a1 , . . . , ak } and
k
X
{wi : ai ∈ Y } ≤ u}.
C = {Y ⊆ X : l ≤
i=1

We notice that all weights in a weight atom W are non-negative. Therefore, if M ⊆ M 0 ⊆ M 00
and both M and M 00 are models of W , then M 0 is also a model of W . It follows that the constraint
(X, C) we define above is convex.
Since (X, C) is convex, weight atoms represent a class of convex constraints and lparse programs syntactically are a class of programs with convex constraints. This relationship extends to
the stable-model semantics. Namely, Marek and Truszczy ński (2004) and Marek et al. (2004, 2006)
showed that lparse programs can be encoded as programs with monotone constraints so that the
concept of a stable model is preserved. The transformation used there coincides with the encoding
mc described in the previous section, when we restrict the latter to lparse programs. Thus, we have
the following theorem.
Theorem 11. Let P be an lparse program. A set M ⊆ At is a stable model of P according to the
definition by Simons et al. (2002) if and only if M is a stable model of P according to the definition
given in the previous section (when P is viewed as a convex-constraint program).
It follows that to compute stable models of lparse programs we can use the results obtained
earlier in the paper, specifically the results on program completion and loop formulas for convexconstraint programs.
Remark. To be precise, the syntax of lparse programs is more general. It allows both atoms and
negated atoms to appear within weight atoms. It also allows weights to be negative. However,
negative weights in lparse programs are treated just as a notational convenience. Specifically, an
expression of the form a = w within a weight atom (where w < 0) represents the expression
not(a) = −w (eliminating negative weights in this way from a weight atom requires modifications of the bounds associated with this weight atom). Moreover, by introducing new propositional
variables one can remove occurrences of negative literals from programs. These transformations preserve stable models (modulo new atoms). Marek and Truszczy ński (2004) and Marek et al. (2004,
2006) provide a detailed discussion of this transformation.
In addition to weight atoms, the bodies of lparse rules may contain propositional literals (atoms
and negated atoms) as conjuncts. We can replace these propositional literals with weight atoms
322

P ROPERTIES AND A PPLICATIONS OF P ROGRAMS WITH M ONOTONE AND C ONVEX C ONSTRAINTS

as follows: an atom a can be replaced with the cardinality atom 1{a}, and a literal not(a) —
with the cardinality atom {a}0. This transformation preserves stable models, too. Moreover, the
size of the resulting program does not increase more than by a constant factor. Thus, through the
transformations discussed here, monotone- and convex-constraint programs capture arbitrary lparse
programs.
2
8.2 Computing Stable Models of Lparse Programs
In this section we present an algorithm for computing stable models of lparse programs. Our method
uses the results we obtained in Section 7 to reduce the problem to that of computing models of the
loop completion of an lparse program. The loop completion is a formula in the logic PL cc , in
which the class of convex constraints is restricted to weight constraints, as defined in the previous
subsection. We will denote the fragment of the logic PLcc consisting of such formulas by PLwa .
To make the method practical, we need programs to compute models of theories in the logic
PLwa . We will now show a general way to adapt to this task off-the-shelf pseudo-boolean constraint
solvers (Eén & Sörensson, 2003; Aloul et al., 2002; Walser, 1997; Manquinho & Roussel, 2005; Liu
& Truszczyński, 2003).
Pseudo-boolean constraints (PB for short) are integer programming constraints in which variables have 0-1 domains. We will write them as inequalities
w1 × x1 + . . . + wk × xk comp w,

(3)

where comp stands for one of the relations ≤, ≥, < and >, w i ’s and w are integer coefficients
(not necessarily non-negative), and xi ’s are integers taking value 0 or 1. A set of pseudo-boolean
constraints is a pseudo-boolean theory.
Pseudo-boolean constraints can be viewed as constraints. The basic idea is to treat each 0-1 variable x as a propositional atom (which we will denote by the same letter). Under this correspondence,
a pseudo-boolean constraint (3) is equivalent to the constraint (X, C), where X = {x 1 , . . . , xk } and
C = {Y ⊆ X :

k
X

{wi : xi ∈ Y } comp w}

i=1

in the sense that solutions to (3) correspond to models of (X, C) (x i = 1 in a solution if and only if
xi is true in the corresponding model). In particular, if all coefficients w i and the bound w in (3) are
non-negative, and if comp = ‘≥’, then the constraint (3) is equivalent to a monotone lower-bound
weight atom w[x1 = w1 , . . . , xn = wn ].
It follows that an arbitrary weight atom can be represented by one or two pseudo-boolean constraints. More generally, an arbitrary PLwa formula F can be encoded as a set of PB constraints.
We will describe the translation as a two-step process.
The first step consists of converting F to a clausal form τcl (F )5 . To control the size of the
translation, we introduce auxiliary propositional atoms. Below, we describe the translation F 7→
τcl (F ) under the assumption that F is a formula of the loop completion of an lparse program P .
Our main motivation is to compute stable models of logic programs and to this end algorithms for
computing models of loop completions are sufficient.
5. A PLwa clause is any formula B1 ∧ . . . ∧ Bm → H1 ∨ . . . ∨ Hn , where Bi and Hj are weight atoms.

323

´
L IU & T RUSZCZY NSKI

Let F be a formula in the loop completion of an lparse-program P . We define τ cl (F ) as follows
(in the transformation, we use a propositional atom x as a shorthand for the cardinality atom C(x) =
1{x}).
1. If F is of the form A1 ∧ . . . ∧ An → A, then τcl (F ) = F
2. If F is of the form x → ([bd (r1 )]∧ ) ∨ . . . ∨ ([bd (rl )]∧ ), then we introduce new propositional
atoms br,1 , . . . , br,l and set τcl (F ) to consist of the following PLwa clauses:
x → br,1 ∨ . . . ∨ br,l
[bd (ri )]∧ → br,i , for every bd (ri )
br,i → Aj , for every bd (ri ) and Aj ∈ bd (ri )
W
3. If F is of the form L → r {βL (r)}, where L is a set of atoms, and every βL (r) is a conjunction of weight
W atoms, then we introduce new propositional atoms bdf L,r for every βL (r) in F and
represent L as the weight atom WL = 1[li = 1 : li ∈ L]. We then define τcl (F ) to consist of the
following clauses:
_
WL →
bdfL,r
W

βL (r) → bdfL,r , for every βL (r) ∈ F
bdfL,r → Aj , for every βL (r) ∈ F and Aj ∈ βL (r).
It is clear that the size τcl (F ) is linear in the size of F .
The second step of the translation, converts a PLwa formula C in a clausal form into a set of
PB constraints, τpb (C). To define the translation C → τpb (C), let us consider a PLwa clause C of
the form
B1 ∧ . . . ∧ B m → H 1 ∨ . . . ∨ H n ,
(4)
where Bi ’s and Hi ’s are weight atoms.
We introduce new propositional atoms b1 , . . . , bm and h1 , . . . , hn to represent each weight atom
in the clause. As noted earlier in the paper, we simply write x for a weight atoms of the form 1[x =
1]. With the new atoms, the clause (4) becomes a propositional clause b 1 ∧ . . . ∧ bm → h1 ∨ . . . ∨ hn .
We represent it by the following PB constraint:
−b1 − . . . − bm + h1 + . . . + hn ≥ 1 − m.

(5)

Here and later in the paper, we use the same symbols to denote propositional variables and the corresponding 0-1 integer variables. The context will always imply the correct meaning of the symbols.
Under this convention, it is easy to see that a propositional clause b 1 ∧ . . . ∧ bm → h1 ∨ . . . ∨ hn
and its PB constraint (5) have the same models.
We introduce next PB constraints that enforce the equivalence of the newly introduced atoms
bi (or hi ) and the corresponding weight atoms Bi (or Hi ).
Let B = l[a1 = w1 , . . . , ak = wk ]u be a weight atom and b a propositional atom. We split B
to B + and B − and introduce two more atoms b+ and b− . To model B ≡ b, we model with pseudoboolean constraints the following three equivalences: b ≡ b + ∧ b− , b+ ≡ B + , and b− ≡ B − .
1. The first equivalence can be captured with three propositional clauses. Hence the following three
PB constraints model that equivalence:
−b + b+ ≥ 0
324

(6)

P ROPERTIES AND A PPLICATIONS OF P ROGRAMS WITH M ONOTONE AND C ONVEX C ONSTRAINTS

−b + b− ≥ 0

(7)

−b+ − b− + b ≥ −1

(8)

2. The second equivalence, b+ ≡ B + , can be modeled by the following two PB constraints
(−l) × b+ +

k
X

(ai × wi ) ≥ 0

(9)

i=1

−(

k
X

wi − l + 1) × b+ +

k
X

3. Similarly, the third equivalence,
k
X

(10)

i=1

i=1

(

(ai × wi ) ≤ l − 1

b−

≡

B−,

can be modeled by the following two PB constraints

wi − u) × b− +

k
X
i=1

i=1

(u + 1) × b− +

k
X

(ai × wi ) ≤

k
X

wi

(11)

i=1

(ai × wi ) ≥ u + 1

(12)

i=1

PLwa

We define now τpb (C), for a
clause C, as the set of all pseudo-boolean constraints (5) and
(6), (7), (8), (11), (12), (9), (10) constructed for every weight atom occurring in C. One can verify
that the size of τpb (C) is linear in the size of C. Therefore, τpb (τcl (F )) has size linear in the size of
F.
In the special case where all Bi ’s and Hj ’s are weight atoms of the form 1[bi = 1] and 1[hj = 1],
we do not need to introduce any new atoms and PB constraints (6), (7), (8), (11), (12), (9), (10).
Then τpb (C) consists of a single PB constraint (5).
We have the following theorem establishing the correctness of the transformation τ . The proof
of the theorem is straightforward.
Theorem 12. Let F be a loop completion formula in logic PLwa , and M a set of atoms, M ⊆
At(F ). Then M is a model of F in PLwa logic if and only if M has a unique extension M 0 by
some of the new atoms in At(τpb (τcl (F ))) such that M 0 is a model of the pseudo-boolean theory
τpb (τcl (F )).
We note that when we use solvers designed for PLwa theories, then translation τpb is no longer
needed. The benefit of using such solvers is that we do not need to split weight atoms in the PL wa
theories and do not need the auxiliary atoms introduced in τ pb .
8.2.1 T HE A LGORITHM
We follow the approach proposed by Lin and Zhao (2002). As in that paper, we first compute the
completion of a lparse program. Then, we iteratively compute models of the completion using a
PB solver. Whenever a model is found, we test it for stability. If the model is not a stable model of
the program, we extend the completion by loop formulas identified in Theorem 10. Often, adding a
single loop formula filters out several models of Comp(P ) that are not stable models of P .
The results given in the previous section ensure that our algorithm is correct. We present it in
Figure 1. We note that it may happen that in the worst case exponentially many loop formulas are
325

´
L IU & T RUSZCZY NSKI

Input: P — a lparse program;
A — a pseudo-boolean solver
BEGIN
compute the completion Comp(P ) of P ;
T := τpb (τcl (Comp(P )));
do
if (solver A finds no models of T )
output “no stable models found” and terminate;
M := a model of T found by A;
if (M is stable) output M and terminate;
compute the reduct P M of P with respect to M ;
compute the greatest stable model M 0 , contained in M , of P M ;
M − := M \ M 0 ;
find all terminating loops in M − ;
compute loop formulas and convert them into PB constraints using
τpb and τcl ;
add all PB constraints computed in the previous step to T ;
while (true);
END

Figure 1: Algorithm of pbmodels
needed before the first stable model is found or we determine that no stable models exist (Lin &
Zhao, 2002). However, that problem arises only rarely in practical situations 6 .
The implementation of pbmodels supports several PB solvers such as satzoo (Eén & Sörensson,
2003), pbs (Aloul et al., 2002), wsatoip (Walser, 1997). It also supports a program wsatcc (Liu &
Truszczyński, 2003) for computing models of PLwa theories. When this last program is used, the
transformation, from “clausal” PLwa theories to pseudo-boolean theories is not needed. The first
two of these four programs are complete PB solvers. The latter two are local-search solvers based
on wsat (Selman, Kautz, & Cohen, 1994).
We output the message “no stable model found” in the first line of the loop and not simply “no
stable models exist” since in the case when A is a local-search algorithm, failure to find a model
of the completion (extended with loop formulas in iteration two and the subsequent ones) does not
imply that no models exist.
8.3 Performance
In this section, we present experimental results concerning the performance of pbmodels. The experiments compared pbmodels, combined with several PB solvers, to smodels (Simons et al., 2002)
and cmodels (Babovich & Lifschitz, 2002). We focused our experiments on problems whose state6. In fact, in many cases programs turn out to be tight with respect to their supported models. Therefore, supported
models are stable and no loop formulas are necessary at all.

326

P ROPERTIES AND A PPLICATIONS OF P ROGRAMS WITH M ONOTONE AND C ONVEX C ONSTRAINTS

ments explicitly involve pseudo-boolean constraints, as we designed pbmodels with such problems
in mind.
For most benchmark problems we tried cmodels did not perform well. Only in one case (vertexcover benchmark) the performance of cmodels was competitive, although even in this case it was
not the best performer. Therefore, we do not report here results we compiled for cmodels. For a
complete set of results we obtained in the experiments we refer to http://www.cs.uky.edu/
ai/pbmodels.
In the experiments we used instances of the following problems: traveling salesperson, weighted
n-queens, weighted Latin square, magic square, vertex cover, and Towers of Hanoi. The lparse
programs we used for the first four problems involve general pseudo-boolean constraints. Programs
modeling the last two problems contain cardinality constraints only.
Traveling salesperson problem (TSP). An instance consists of a weighted complete graph with n
vertices, and a bound w. All edge weights and w are non-negative integers. A solution to an instance
is a Hamiltonian cycle whose total weight (the sum of the weights of all its edges) is less than or
equal to w.
We randomly generated 50 weighted complete graphs with 20 vertices, To this end, in each case
we assign to every edge of a complete undirected graph an integer weight selected uniformly at
random from the range [1..19]. By setting w to 100 we obtained a set of “easy” instances, denoted
by TSP-e (the bound is high enough for every instance in the set to have a solution). From the same
collection of graphs, we also created a set of “hard” instances, denoted by TSP-h, by setting w to 62.
Since the requirement on the total weight is stronger, the instances in this set in general take more
time.
Weighted n-queens problem (WNQ). An instance to the problem consists of a weighted n × n
chess board and a bound w. All weights and the bound are non-negative integers. A solution to an
instance is a placement of n queens on the chess board so that no two queens attack each other and
the weight of the placement (the sum of the weights of the squares with queens) is not greater than
w.
We randomly generated 50 weighted chess boards of the size 20 × 20, where each chess board
is represented by a set of n × n integer weights wi,j , 1 ≤ i, j ≤ n, all selected uniformly at random
from the range [1..19]. We then created two sets of instances, easy (denoted by wnq-e) and hard
(denoted by wnq-h), by setting the bound w to 70 and 50, respectively.
Weighted Latin square problem (WLSQ). An instance consists of an n × n array of weights w i,j ,
and a bound w. All weights wi,j and w are non-negative integers. A solution to an instance is an
n × n array L with all entries from {1, . . . , n} and such
P thatPeach element in {1, . . . , n} occurs
exactly once in each row and in each column of L, and ni=1 nj=1 L[i, j] × wi,j ≤ w.
We set n = 10 and we randomly generated 50 sets of integer weights, selecting them uniformly
at random from the range [1..9]. Again we created two families of instances, easy (wlsq-e) and hard
(wlsq-h), by setting w to 280 and 225, respectively.
Magic square problem. An instance consists of a positive integer n. The goal is to construct an
n × n array using each integer 1, . . . n2 as an entry in the array exactly once in such a way that
entries in each row, each column and in both main diagonals sum up to n(n 2 + 1)/2. For the
experiments we used the magic square problem for n = 4, 5 and 6.
Vertex cover problem. An instance consists of graph with n vertices and m edges, and a nonnegative integer k — a bound. A solution to the instance is a subset of vertices of the graph with no
more than k vertices and such that at least one end vertex of every edge in the graph is in the subset.
327

´
L IU & T RUSZCZY NSKI

We randomly generated 50 graphs, each with 80 vertices and 400 edges. For each graph, we set
k to be a smallest integer such that a vertex cover with that many elements still exists.
Towers of Hanoi problem. This is a slight generalization of the original problem. We considered
the case with six disks and three pegs. An instance consists of an initial configuration of disks that
satisfies the constraint of the problem (larger disk must not be on top of a smaller one) but does not
necessarily requires that all disks are on one peg. These initial configurations were selected so that
they were 31, 36, 41 and 63 steps away from the goal configuration (all disks from the largest to the
smallest on the third peg), respectively. We also considered a standard version of the problem with
seven disks, in which the initial configuration is 127 steps away from the goal.
We encoded each of these problems as a program in the general syntax of lparse, which allows
the use of relation symbols and variables (Syrjänen, 1999). The programs are available at http:
//www.cs.uky.edu/ai/pbmodels. We then used these programs in combination with appropriate instances as inputs to lparse (Syrjänen, 1999). In this way, for each problem and each
set of instances we generated a family of ground (propositional) lparse programs so that stable
models of each of these programs represent solutions to the corresponding instances of the problem (if there are no stable models, there are no solutions). We used these families of lparse programs as inputs to solvers we were testing. All these ground programs are also available at http:
//www.cs.uky.edu/ai/pbmodels.
In the tests, we used pbmodels with the following four PB solvers: satzoo (Eén & Sörensson,
2003), pbs (Aloul et al., 2002), wsatcc (Liu & Truszczyński, 2003), and wsatoip (Walser, 1997). In
particular, wsatcc deals with PLwa theories directly.
All experiments were run on machines with 3.2GHz Pentium 4 CPU, 1GB memory, running
Linux with kernel version 2.6.11, gcc version 3.3.4. In all cases, we used 1000 seconds as the
timeout limit.
We first show the results for the magic square and towers of Hanoi problems. In Table 1, for
each solver and each instance, we report the corresponding running time in seconds. Local-search
solvers were unable to solve any of the instances in the two problems and so are not included in the
table.
Benchmark
magic square (4 × 4)
magic square (5 × 5)
magic square (6 × 6)
towers of Hanoi (d = 6, t = 31)
towers of Hanoi (d = 6, t = 36)
towers of Hanoi (d = 6, t = 41)
towers of Hanoi (d = 6, t = 63)
towers of Hanoi (d = 7, t = 127)

smodels
1.36
> 1000
> 1000
16.19
32.21
296.32
> 1000
> 1000

pbmodels-satzoo
1.70
28.13
75.58
18.47
31.72
49.90
> 1000
> 1000

pbmodels-pbs
2.41
0.31
> 1000
1.44
1.54
3.12
3.67
22.83

Table 1: Magic square and towers of Hanoi problems
Both pbmodels-satzoo and pbmodels-pbs perform better than smodels on programs obtained
from the instances of both problems. We observe that pbmodels-pbs performs exceptionally well
in the tower of Hanoi problem. It is the only solver that can compute a plan for 7 disks, which
requires 127 steps. Magic square and Towers of Hanoi problems are highly regular. Such problems
328

P ROPERTIES AND A PPLICATIONS OF P ROGRAMS WITH M ONOTONE AND C ONVEX C ONSTRAINTS

TSP-e
TSP-h
wnq-e
wnq-h
wlsq-e
wlsq-h
vtxcov

# of SAT instances
50
31
49
29
45
8
50

# of UNSAT instances
0
1
0
0
4
41
0

# of UNKNOWN instances
0
18
1
21
1
1
0

Table 2: Summary of Instances

TSP-e
TSP-h
wnq-e
wnq-h
wlsq-e
wlsq-h
vtxcov
sum over all

smodels
45/17
7/3
11/5
2/2
21/1
0/0
50/40
136/68

pbmodels-satzoo
50/30
16/14
26/23
0/0
49/29
47/26
50/1
238/123

pbmodels-pbs
18/3
0/0
0/0
0/0
46/19
47/23
47/3
158/48

Table 3: Summary on all instances
are often a challenge for local-search problems, which may explain a poor performance we observed
for pbmodels-wsatcc and pbmodels-wsatoip on these two benchmarks.
For the remaining four problems, we used 50-element families of instances, which we generated randomly in the way discussed above. We studied the performance of complete solvers
(smodels, pbmodels-satzoo and pbmodels-pbs) on all instances. We then included local-search
solvers (pbmodels-wsatcc and pbmodelswsatoip) in the comparisons but restricted attention only
to instances that were determined to be satisfiable (as local-search solvers are, by their design, unable to decide unsatisfiability). In Table 2, for each family we list how many of its instances are
satisfiable, unsatisfiable, and for how many of the instances none of the solvers we tried was able to
decide satisfiability.
In Table 3, for each of the seven families of instances and for each complete solver, we report
two values s/w, where s is the number of instances solved by the solver and w is the number of
times it was the fastest among the three.
The results in Table 3 show that overall pbmodels-satzoo solved more instances than pbmodelspbs, followed by smodels. When we look at the number of times a solver was the fastest one,
pbmodels-satzoo was a clear winner overall, followed by smodels and then by pbmodels-pbs.
Looking at the seven families of tests individually, we see that pbmodels-satzoo performed better
than the other two solvers on five of the families. On the other two smodels was the best performer
(although, it is a clear winner only on the vertex-cover benchmark; all solvers were essentially
ineffective on the wnq-h).
We also studied the performance of pbmodels combined with local-search solvers wsatcc (Liu
& Truszczyński, 2003) and wsatoip (Walser, 1997). For this study, we considered only those instances in the seven families that we knew were satisfiable. Table 4 presents results for all solvers
329

´
L IU & T RUSZCZY NSKI

TSP-e
TSP-h
wnq-e
wnq-h
wlsq-e
wlsq-h
vtxcov
sum over all

smodels
45/3
7/0
11/0
2/0
21/0
0/0
50/0
136/3

pbmd-satzoo
50/5
16/2
26/0
0/0
45/0
7/0
50/0
194/7

pbmd-pbs
18/2
0/0
0/0
0/0
44/0
8/0
47/0
117/2

pbmd-wsatcc
32/7
19/6
49/45
29/15
45/33
7/1
50/36
231/143

pbmd-wsatoip
47/34
28/22
49/4
29/14
45/14
8/7
50/15
256/110

Table 4: Summary on SAT instances
we studied (including the complete ones). As before, each entry provides a pair of numbers s/w,
where s is the number of solved instances and w is the number of times the solver performed better
than its competitors.
The results show superior performance of pbmodels combined with local-search solvers. They
solve more instances than complete solvers (including smodels). In addition, they are significantly
faster, winning much more frequently than complete solvers do (complete solvers were faster only
on 12 instances, while local-search solvers were faster on 253 instances).
Our results demonstrate that pbmodels with solvers of pseudo-boolean constraints outperforms
smodels on several types of search problems involving pseudo-boolean (weight) constraints).
We note that we also analyzed the run-time distributions for each of these families of instances.
A run-time distribution is regarded as a more accurate and detailed measure of the performance of
algorithms on randomly generated instances7 . The results are consistent with the summary results
presented above and confirm our conclusions. As the discussion of run-time distributions requires
much space, we do not include this analysis here. They are available at the website http://www.
cs.uky.edu/ai/pbmodels.

9. Related work
Extensions of logic programming with means to model properties of sets (typically consisting of
ground terms) have been extensively studied. Usually, these extensions are referred to by the common term of logic programming with aggregates. The term comes from the fact that most properties
of sets of practical interest are defined through “aggregate” operations such as sum, count, maximum, minimum and average. We chose the term constraint to stress that we speak about abstract
properties that define constraints on truth assignments (which we view as sets of atoms).
Mumick, Pirahesh, and Ramakrishnan (1990), and Kemp and Stuckey (1991) were among the
first to study logic programs with aggregates. Recently, Niemelä et al. (1999) and Simons et al.
(2002) introduced the class of lparse programs. We discussed this formalism in detail earlier in this
paper.
Pelov (2004) and Pelov et al. (2006) studied a more general class of aggregates and developed a systematic theory of aggregates in logic programming based on the approximation theory
(Denecker, Marek, & Truszczyński, 2000). The resulting theory covers not only the stable models
semantics but also the supported-model semantics and extensions of 3-valued Kripke-Kleene and
7. Hoos and Stützle (2005) provide a detailed discussion of this matter in the context of local-search methods.

330

P ROPERTIES AND A PPLICATIONS OF P ROGRAMS WITH M ONOTONE AND C ONVEX C ONSTRAINTS

well-founded semantics. The formalism introduced and studied by Pelov (2004) and Pelov et al.
(2006) allows for arbitrary aggregates (not only monotone ones) to appear in the bodies of rules.
However, it does not allow for aggregates to appear in the heads of program clauses. Due to differences in the syntax and the scope of semantics studied there is no simple way to relate Pelov’s
(2004) and Pelov et al.’s (2006) formalism to programs with monotone (convex) constraints. We
note though that programs with abstract monotone constraints with the heads of rules of the form
C(a) can be viewed almost literally as programs in the formalism by Pelov (2004) and Pelov et al.
(2006) and that they have the same stable models according to the definitions we used in this paper
and those by Pelov (2004) and Pelov et al. (2006).
Faber et al. (2004) developed the theory of disjunctive logic programs with aggregates. Similarly
as Pelov (2004) and Pelov et al. (2006), Faber et al. (2004) do not allow for aggregates to appear
in the heads of program clauses. This is one of the differences between that approach and programs
with monotone (convex) constraints we studied here. The other major difference is related to the
postulate of the minimality of stable models (called answer sets in the context of the formalism
considered by Faber et al., 2004). In keeping with the spirit of the original answer-set semantics
(Gelfond & Lifschitz, 1991), answer sets of disjunctive programs with aggregates, as defined by
Faber et al. (2004), are minimal models. Stable models of programs with abstract constraints do
not have this property. However, for the class of programs with abstract monotone constraints with
the heads of rules of the form C(a) the semantics of answer sets defined by Faber et al. (2004)
coincides with the semantics of stable models by Marek and Truszczy ński (2004) and Marek et al.
(2004, 2006).
Yet another approach to aggregates in logic programming was presented by Son and Pontelli
(2006). That approach considered programs of the syntax similar to programs with monotone abstract constraints. It allowed arbitrary constraints (not only monotone ones) but not under the scope
of not operator. A general principle behind the definition of the stable-model semantics by Son
and Pontelli (2006) is to view a program with constraints as a concise representation of a set of its
“instances”, each being a normal logic program. Stable models of the program with constraints are
defined as stable models of its instances and is quite different from the operator-based definition
by Marek and Truszczyński (2004) and Marek et al. (2004, 2006). However, for programs with
monotone constraint atoms which fall in the scope of the formalism of Son and Pontelli (2006) both
approaches coincide.
We also note that recently Son et al. (2006) presented a conservative extension of the syntax
proposed by Marek and Truszczyński (2004) Marek et al. (2006), in which clauses are built of
arbitrary constraint atoms.
Finally, we point out the work by Ferraris and Lifschitz (2004) and Ferraris (2005) which treats
aggregates as nested expressions. In particular, Ferraris (2005) introduces a propositional logic with
a certain nonclassical semantics, and shows that it extends several approaches to programs with
aggregates, including those by Simons et al. (2002) (restricted to core lparse programs) and Faber
et al. (2004). The nature of the relationship of the formalism by Ferraris (2005) and programs with
abstract constraints remains an open problem.

10. Conclusions
Our work shows that concepts, techniques and results from normal logic programming, concerning
strong and uniform equivalence, tightness and Fages lemma, program completion and loop formu331

´
L IU & T RUSZCZY NSKI

las, generalize to the abstract setting of programs with monotone and convex constraints. These
general properties specialize to new results about lparse programs (with the exception of the characterization strong equivalence of lparse programs, which was first obtained by Turner, 2003).
Given these results we implemented a new software pbmodels for computing stable models of
lparse programs. The approach reduces the problem to that of computing models of theories consisting of pseudo-boolean constraints, for which several fast solvers exist (Manquinho & Roussel,
2005). Our experimental results show that pbmodels with PB solvers, especially local search PB
solvers, performs better than smodels on several types of search problems we tested. Moreover, as
new and more efficient solvers of pseudo-boolean constraints become available (the problem is receiving much attention in the satisfiability and integer programming communities), the performance
of pbmodels will improve accordingly.

Acknowledgments
We acknowledge the support of NSF grants IIS-0097278 and IIS-0325063. We are grateful to the
reviewers for their useful comments and suggestions.
This paper combines and extends results included in conference papers (Liu & Truszczy ński,
2005b, 2005a).

References
Aloul, F., Ramani, A., Markov, I., & Sakallah, K. (2002). PBS: a backtrack-search pseudo-boolean solver
and optimizer. In Proceedings of the 5th International Symposium on Theory and Applications of
Satisfiability, pp. 346 – 353.
Babovich, Y., & Lifschitz, V. (2002). Cmodels package.. http://www.cs.utexas.edu/users/
tag/cmodels.html.
Baral, C. (2003). Knowledge representation, reasoning and declarative problem solving. Cambridge University Press.
Clark, K. (1978). Negation as failure. In Gallaire, H., & Minker, J. (Eds.), Logic and data bases, pp. 293–322.
Plenum Press, New York-London.
Dell’Armi, T., Faber, W., Ielpa, G., Leone, N., & Pfeifer, G. (2003). Aggregate functions in disjunctive logic
programming: semantics, complexity, and implementation in DLV. In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI-2003), pp. 847–852. Morgan Kaufmann.
Denecker, M., Marek, V., & Truszczy´
nski, M. (2000). Approximations, stable operators, well-founded fixpoints and applications in nonmonotonic reasoning. In Minker, J. (Ed.), Logic-Based Artificial Intelligence, pp. 127–144. Kluwer Academic Publishers.
E´
en, N., & Sörensson, N. (2003). An extensible SAT solver. In Theory and Applications of Satisfiability
Testing, 6th International Conference, SAT-2003, Vol. 2919 of LNCS, pp. 502–518. Springer.
Eiter, T., & Fink, M. (2003). Uniform equivalence of logic programs under the stable model semantics. In
Proceedings of the 2003 International Conference on Logic Programming, Vol. 2916 of Lecture Notes
in Computer Science, pp. 224–238, Berlin. Springer.
Erdem, E., & Lifschitz, V. (2003). Tight logic programs. Theory and Practice of Logic Programming, 3(4-5),
499–518.
Faber, W., Leone, N., & Pfeifer, G. (2004). Recursive aggregates in disjunctive logic programs: Semantics
and complexity.. In Proceedings of the 9th European Conference on Artificial Intelligence (JELIA
2004), Vol. 3229 of LNAI, pp. 200 – 212. Springer.
332

P ROPERTIES AND A PPLICATIONS OF P ROGRAMS WITH M ONOTONE AND C ONVEX C ONSTRAINTS

Fages, F. (1994). Consistency of Clark’s completion and existence of stable models. Journal of Methods of
Logic in Computer Science, 1, 51–60.
Ferraris, P. (2005). Answer sets for propositional theories. In Logic Programming and Nonmonotonic Reasoning, 8th International Conference, LPNMR 2005, Vol. 3662 of LNAI, pp. 119–131. Springer.
Ferraris, P., & Lifschitz, V. (2004). Weight constraints ans nested expressions. Theory and Practice of Logic
Programming, 5, 45–74.
Gelfond, M., & Leone, N. (2002). Logic programming and knowledge representation – the A-prolog perspective. Artificial Intelligence, 138, 3–38.
Gelfond, M., & Lifschitz, V. (1988). The stable semantics for logic programs. In Proceedings of the 5th
International Conference on Logic Programming, pp. 1070–1080. MIT Press.
Gelfond, M., & Lifschitz, V. (1991). Classical negation in logic programs and disjunctive databases. New
Generation Computing, 9, 365–385.
Hoos, H., & Stützle, T. (2005). Stochastic Local Search Algorithms — Foundations and Applications.
Morgan-Kaufmann.
Kemp, D., & Stuckey, P. (1991). Semantics of logic programs with aggregates. In Logic Programming,
Proceedings of the 1991 International Symposium, pp. 387–401. MIT Press.
Lifschitz, V., Pearce, D., & Valverde, A. (2001). Strongly equivalent logic programs. ACM Transactions on
Computational Logic, 2(4), 526–541.
Lin, F. (2002). Reducing strong equivalence of logic programs to entailment in classical propositional logic.
In Principles of Knowledge Representation and Reasoning, Proceedings of the 8th International Conference (KR2002). Morgan Kaufmann Publishers.
Lin, F., & Zhao, Y. (2002). ASSAT: Computing answer sets of a logic program by SAT solvers. In Proceedings of the 18th National Conference on Artificial Intelligence (AAAI-2002), pp. 112–117. AAAI
Press.
Liu, L., & Truszczy´
nski, M. (2003). Local-search techniques in propositional logic extended with cardinality
atoms. In Rossi, F. (Ed.), Proceedings of the 9th International Conference on Principles and Practice
of Constraint Programming, CP-2003, Vol. 2833 of LNCS, pp. 495–509. Springer.
Liu, L., & Truszczy´
nski, M. (2005a). Pbmodels - software to compute stable models by pseudoboolean
solvers. In Logic Programming and Nonmonotonic Reasoning, Proceedings of the 8th International
Conference (LPNMR-05), LNAI 3662, pp. 410–415. Springer.
Liu, L., & Truszczy´
nski, M. (2005b). Properties of programs with monotone and convex constraints. In
Proceedings of the 20th National Conference on Artificial Intelligence (AAAI-05), pp. 701–706. AAAI
Press.
Manquinho, V., & Roussel, O. (2005).
univ-artois.fr/PB05/.

Pseudo boolean evaluation 2005..

http://www.cril.

Marek, V., Niemel¨
a, I., & Truszczy´
nski, M. (2004). Characterizing stable models of logic programs with
cardinality constraints. In Proceedings of the 7th International Conference on Logic Programming
and Nonmonotonic Reasoning, Vol. 2923 of Lecture Notes in Artificial Intelligence, pp. 154–166.
Springer.
Marek, V., Niemel¨
a, I., & Truszczy´
nski, M. (2006). Logic programs with monotone abstract constraint atoms.
Theory and Practice of Logic Programming. Submitted.
Marek, V., & Truszczy´
nski, M. (2004). Logic programs with abstract constraint atoms. In Proceedings of the
19th National Conference on Artificial Intelligence (AAAI-04), pp. 86–91. AAAI Press.
Mumick, I., Pirahesh, H., & Ramakrishnan, R. (1990). The magic of duplicates and aggregates. In Proceedings of the 16th International Conference on Very Large Data Bases, VLDB 1990, pp. 264–277.
Morgan Kaufmann.
333

´
L IU & T RUSZCZY NSKI

Niemel¨
a, I., Simons, P., & Soininen, T. (1999). Stable model semantics of weight constraint rules. In Proceedings of LPNMR-1999, Vol. 1730 of LNAI, pp. 317–331. Springer.
Pelov, N. (2004). Semantics of logic programs with aggregates. PhD Thesis. Department of Computer
Science, K.U.Leuven, Leuven, Belgium.
Pelov, N., Denecker, M., & Bruynooghe, M. (2004). Partial stable models for logic programs with aggregates.
In Lifschitz, V., & Niemel¨
a, I. (Eds.), Logic programming and Nonmonotonic Reasoning, Proceedings
of the 7th International Conference, Vol. 2923, pp. 207–219. Springer.
Pelov, N., Denecker, M., & Bruynooghe, M. (2006). Well-founded and stable semantics of logic programs
with aggregates. Theory and Practice of Logic Programming. Accepted (available at http://www.
cs.kuleuven.ac.be/˜dtai/projects/ALP/TPLP/).
Selman, B., Kautz, H., & Cohen, B. (1994). Noise strategies for improving local search. In Proceedings
of the 12th National Conference on Artificial Intelligence (AAAI-1994), pp. 337–343, Seattle, USA.
AAAI Press.
Simons, P., Niemel¨
a, I., & Soininen, T. (2002). Extending and implementing the stable model semantics.
Artificial Intelligence, 138, 181–234.
Son, T., & Pontelli, E. (2006). A constructive semantic characterization of aggregates in anser set programming. Theory and Practice of Logic Programming. Accepted (available at http://arxiv.org/
abs/cs.AI/0601051).
Son, T., Pontelli, E., & Tu, P. (2006). Answer sets for logic programs with arbitrary abstract constraint atoms.
In Proceedings of the 21st National Conference on Artificial Intelligence (AAAI-06). AAAI Press.
Syrj¨
anen, T. (1999). lparse, a procedure for grounding domain restricted logic programs. http://www.
tcs.hut.fi/Software/smodels/lparse/.
Turner, H. (2003). Strong equivalence made easy: Nested expressions and weight constraints. Theory and
Practice of Logic Programming, 3, (4&5), 609–622.
Walser, J. (1997). Solving linear pseudo-boolean constraints with local search. In Proceedings of the 14th
National Conference on Artificial Intelligence (AAAI-97), pp. 269–274. AAAI Press.

334

Journal of Artificial Intelligence Research 27 (2006) 55-83

Submitted 10/05; published 09/06

Cognitive Principles in Robust Multimodal Interpretation
Joyce Y. Chai
Zahar Prasov
Shaolin Qu

jchai@cse.msu.edu
prasovza@cse.msu.edu
qushaoli@cse.msu.edu

Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824 USA

Abstract
Multimodal conversational interfaces provide a natural means for users to communicate with computer systems through multiple modalities such as speech and gesture. To
build eﬀective multimodal interfaces, automated interpretation of user multimodal inputs
is important. Inspired by the previous investigation on cognitive status in multimodal
human machine interaction, we have developed a greedy algorithm for interpreting user
referring expressions (i.e., multimodal reference resolution). This algorithm incorporates
the cognitive principles of Conversational Implicature and Givenness Hierarchy and applies constraints from various sources (e.g., temporal, semantic, and contextual) to resolve
references. Our empirical results have shown the advantage of this algorithm in eﬃciently
resolving a variety of user references. Because of its simplicity and generality, this approach
has the potential to improve the robustness of multimodal input interpretation.

1. Introduction
Multimodal systems provide a natural and eﬀective way for users to interact with computers
through multiple modalities such as speech, gesture, and gaze. Since the ﬁrst appearance
of the “Put-That-There” system (Bolt, 1980), a number of multimodal systems have been
built, among which there are systems that combine speech, pointing (Neal & Shapiro, 1991;
Stock, 1993), and gaze (Koons, Sparrell, & Thorisson, 1993), systems that integrate speech
with pen inputs (e.g., drawn graphics) (Cohen, Johnston, McGee, Oviatt, Pittman, Smith,
Chen, & Clow, 1996; Wahlster, 1998), systems that combine multimodal inputs and outputs
(Cassell, Bickmore, Billinghurst, Campbell, Chang, Vilhjalmsson, & Yan, 1999), systems
in mobile environments (Oviatt, 1999a), and systems that engage users in an intelligent
conversation (Gustafson, Bell, Beskow, Boye, Carlson, Edlund, Granstrom, House, & Wiren,
2000; Stent, Dowding, Gawron, Bratt, & Moore, 1999). Earlier studies have shown that
multimodal interfaces enable users to interact with computers naturally and eﬀectively
(Oviatt, 1996, 1999b).
One important aspect of building multimodal systems is multimodal interpretation,
which is a process that identiﬁes the meanings of user inputs. In particular, a key element
in multimodal interpretation is known as reference resolution, which is a process that ﬁnds
the most proper referents to referring expressions. Here a referring expression is a phrase
that is given by a user in her inputs (most likely in speech inputs) to refer to a speciﬁc
entity or entities. A referent is an entity (e.g., a speciﬁc object) to which the user refers.
Suppose that a user points to House 6 on the screen and says how much is this one. In this
c
2006
AI Access Foundation. All rights reserved.

Chai, Prasov, & Qu

case, reference resolution must infer that the referent House 6 should be assigned to the
referring expression this one. This paper particularly addresses this problem of reference
resolution in multimodal interpretation.
In a multimodal conversation, the way users communicate with a system depends on
the available interaction channels and the situated context (e.g., conversation focus, visual
feedback). These dependencies form a rich set of constraints from various aspects (e.g.,
semantic, temporal, and contextual). A correct interpretation can only be attained by
simultaneously considering these constraints.
Previous studies have shown that user referring behavior during multimodal conversation
does not occur randomly, but rather follows certain linguistic and cognitive principles.
In human machine interaction, earlier work has shown strong correlations between the
cognitive status in Givenness Hierarchy and the form of referring expressions (Kehler, 2000).
Inspired by this early work, we have developed a greedy algorithm for multimodal reference
resolution. This algorithm incorporates the principles of Conversational Implicature and
Givenness Hierarchy and applies constraints from various sources (e.g., gesture, conversation
context, and visual display). Our empirical results have shown the promise of this algorithm
in eﬃciently resolving a variety of user references. One major advantage of this greedy
algorithm is that the prior linguistic and cognitive knowledge can be used to guide the
search and prune the search space during constraint satisfaction. Because of its simplicity
and generality, this approach has the potential to improve the robustness of interpretation
and provide a practical solution to multimodal reference resolution (Chai, Prasov, Blaim,
& Jin, 2005).
In the following sections, we will ﬁrst demonstrate diﬀerent types of referring behavior
observed in our studies. We then brieﬂy introduce the underlying cognitive principles for
human-human communication and describe how these principles can be used in a computational model to eﬃciently resolve multimodal references. Finally, we will present the
experimental results.

2. Multimodal Reference Resolution
In our previous work (Chai, Hong, & Zhou, 2004b; Chai, Hong, Zhou, & Prasov, 2004), a
multimodal conversational system was developed for users to acquire real estate information1 .
Figure 1 is the snapshot of a graphical user interface. Users can interact with this interface
through both speech and gesture. Table 1 shows a fragment of the conversation.
In this fragment, the user exhibits diﬀerent types of referring behavior. For example,
the input from U1 is considered as a simple input. This type of simple input only has one
referring expression in the spoken utterance and one accompanying gesture. Multimodal
fusion that combines information from speech and gesture will likely resolve what this
refers to. In the second user input (U2 ), there is no accompanying gesture and no referring
expression is explicitly used in the speech utterance. At this time, the system needs to
use the conversation context to infer that the object of interest is the house mentioned in
the previous turn of the conversation. In the third user input, there are multiple referring
expressions and multiple gestures. These types of inputs are considered complex inputs.
1. The first prototype of this system was developed at IBM T. J. Watson Research Center with P. Hong,
M. Zhou, and colleagues at the Intelligent Multimedia Interaction group.

56

Minimizing Conflicts: A Heuristic Repair Method

Figure 1: A snapshot of the multimodal conversational system.

U1
S1
U2
S2
U3
S3

Speech: How much does this cost?
Gesture: point to a position on the screen
Speech: The price is 400K
Graphics: highlight the house in discussion
Speech: How large?
Speech: 2500 square feet
Speech: Compare it with this house and this one
Gesture: ....circle....cirle (put two consecutive circles on the screen)
Speech: Here are your comparison results
Graphics: show a table of comparison

Table 1: A fragment demonstrating interaction with diﬀerent types of referring behavior
Complex inputs are more diﬃcult to resolve. We need to consider the temporal relations
between the referring expressions and the gestures, the semantic constraints speciﬁed by
the referring expressions, and the contextual constraints from the prior conversation. For
example, in the case of U3 , the system needs to understand that it refers to the house that
was the focus of the previous turn; and this house and this one should be aligned with the
two consecutive gestures. Any subtle variations in any of the constraints, including the
temporal ordering, the semantic compatibility, and the gesture recognition results will lead
to diﬀerent interpretations.
From this example, we can see that in a multimodal conversation, the way a user interacts with a system is dependent not only on the available input channels (e.g., speech and
gesture), but also upon his/her conversation goals, the state of the conversation, and the
multimedia feedback from the system. In other words, there is a rich context that involves
57

Chai, Prasov, & Qu

dependencies from many diﬀerent aspects established during the interaction. Interpreting
user inputs can only be situated in this rich context. For example, the temporal relations
between speech and gesture are important criteria that determine how the information from
these two modalities can be combined. The focus of attention from the prior conversation
shapes how users refer to those objects, and thus, inﬂuences the interpretation of referring
expressions. Therefore, we need to simultaneously consider the temporal relations between
the referring expressions and the gestures, the semantic constraints speciﬁed by the referring expressions, and the contextual constraints from the prior conversation. In this paper,
we present an eﬃcient approach that is driven by cognitive principles to combine temporal,
semantic, and contextual constraints for multimodal reference resolution.

3. Related Work
Considerable eﬀort has been devoted to studying user multimodal behavior (Cohen, 1984;
Oviatt, 1999a) and mechanisms to interpret user multimodal inputs (Chai et al., 2004b;
Gustafson et al., 2000; Huls, Bos, & Classen, 1995; Johnston, Cohen, McGee, Oviatt,
Pittman, & Smith, 1997; Johnston, 1998; Johnston & Bangalore, 2000; Kehler, 2000; Koons
et al., 1993; Neal & Shapiro, 1991; Oviatt, DeAngeli, & Kuhn, 1997; Stent et al., 1999; Stock,
1993; Wahlster, 1998; Wu & Oviatt, 1999; Zancanaro, Stock, & Strapparava, 1997).
For multimodal reference resolution, some early work keeps track of a focus space from
the dialog (Grosz & Sidner, 1986) and a display model to capture all objects visible on the
graphical display (Neal, Thielman, Dobes, M., & Shapiro, 1998). It then checks semantic
constraints such as the type of the candidate objects being referenced and their properties
for reference resolution. A modiﬁed centering model for multimodal reference resolution
is also introduced in previous work (Zancanaro et al., 1997). The idea is that based on
the centering movement between turns, segments of discourse can be constructed. The
discourse entities appearing in the segment that is accessible to the current turn can be
used to constrain the referents to referring expressions. Another approach is introduced
to use contextual factors for multimodal reference resolution (Huls et al., 1995). In this
approach, a salience value is assigned to each instance based on the contextual factors.
To determine the referents of multimodal referring expressions, this approach retrieves the
most salient referent that satisﬁes the semantic restrictions of the referring expressions. All
these earlier approaches have some greedy nature, which is largely dependent on semantic
constraints and/or constraints from conversation context.
To resolve multimodal references, there are two important issues. First it is the mechanism to combine information from various sources and modalities. The second is the capability to obtain the best interpretation (among all the possible alternatives) given a set of
temporal, semantic, and contextual constraints. In this section, we give a brief introduction
to three recent approaches that address these issues.
3.1 Multimodal Fusion
Approaches to multimodal fusion (Johnston, 1998; Johnston & Bangalore, 2000), although
they focus on a diﬀerent problem of overall input interpretation, provide eﬀective solutions
to reference resolution. There are two major approaches to multimodal fusion: uniﬁcation58

Minimizing Conflicts: A Heuristic Repair Method

based approaches (Johnston, 1998) and ﬁnite state approaches (Johnston & Bangalore,
2000).
The uniﬁcation-based approach identiﬁes referents to referring expressions by unifying
feature structures generated from speech utterances and gestures using a multimodal grammar (Johnston et al., 1997; Johnston, 1998). The multimodal grammar combines both
temporal and spatial constraints. Temporal constraints encode the absolute temporal relations between speech and gesture (Johnston, 1998),. The grammar rules are predeﬁned
based on empirical studies of multimodal interaction (Oviatt et al., 1997). For example, one
rule indicates that speech and gesture can be combined only when the speech either overlaps
with gesture or follows the gesture within a certain time frame. The uniﬁcation approach
can also process certain complex cases (as long as they satisfy the predeﬁned multimodal
grammar) in which a speech utterance is accompanied by more than one gesture of diﬀerent
types (Johnston, 1998). Using this approach to accommodate various situations such as
those described in Figure 1 will require adding diﬀerent rules to cope with each situation.
If a speciﬁc user referring behavior did not exactly match any existing integration rules
(e.g., temporal relations), the uniﬁcation would fail and therefore references would not be
resolved.
The ﬁnite state approach applies ﬁnite-state transducers for multimodal parsing and
understanding (Johnston & Bangalore, 2000). Unlike the uniﬁcation-based approach with
chart parsing that is subject to signiﬁcant computational complexity concerns (Johnston
& Bangalore, 2000), the ﬁnite state approach provides more eﬃcient, tight-coupling of
multimodal understanding with speech recognition. In this approach, a multimodal contextfree grammar is deﬁned to transform the syntax of multimodal inputs to the semantic
meanings. The domain-speciﬁc semantics are directly encoded in the grammar. Based
on these grammars, multi-tape ﬁnite state automata can be constructed. These automata
are used for identifying semantics of combined inputs. Rather than absolute temporal
constraints as in the uniﬁcation-based approach, this approach relies on temporal order
between diﬀerent modalities. During the parsing stage, the gesture input from the gesture
tape (e.g., pointing to a particular person) that can be combined with the speech expression
in the speech tape (e.g., this person) is considered as the referent to the expression. A
problem with this approach is that the multi-tape structure only takes input from speech
and gesture and does not incorporate the conversation history into consideration.
3.2 Decision List
To identify potential referents, previous work has investigated Givenness Hierarchy (to
be introduced later) in multimodal interaction (Kehler, 2000). Based on data collected
from Wizard of Oz experiments, this investigation suggests that users tend to tailor their
expressions to what they perceive to be the system’s beliefs concerning the cognitive status
of referents from their prominence (e.g., highlight) on the display. The tailored referring
expressions can then be resolved with a high accuracy based on the following decision list:
1. If an object is gestured to, choose that object.
2. Otherwise, if the currently selected object meets all semantic type constraints imposed
by the referring expression, choose that object.
59

Chai, Prasov, & Qu

3. Otherwise, if there is a visible object that is semantically compatible, then choose
that object.
4. Otherwise, a full NP (such as a proper name) is used to uniquely identify the referent.
From our studies (Chai, Prasov, & Hong, 2004a), we found this decision list has the
following limitations:
• Depending on the interface design, ambiguities (from a system’s perspective) could
occur. For example, given an interface where one object (e.g., house) can sometimes
be created on top of another object (e.g., town), a pointing gesture could result in
multiple potential objects. Furthermore, given an interface with crowded objects, a
ﬁnger point could also result in multiple objects with diﬀerent probabilities. The
decision list is not able to handle these ambiguous cases.
• User inputs are not always simple (consisting of no more than one referring expression
and one gesture as indicated in the decision list). In fact, in our study (Chai et al.,
2004a), we found that user inputs can also be complex, consisting of multiple referring
expressions and/or multiple gestures. The referents to these referring expressions
could come from diﬀerent sources, such as gesture inputs and conversation context.
The temporal alignment between speech and gesture is also important in determining
the correct referent for a given expression. The decision list is not able to handle these
types of complex inputs.
Nevertheless, the previous ﬁndings (Kehler, 2000) have inspired this work and provided a
basis for the algorithm described in this paper.
3.3 Optimization
Recently, a probabilistic approach was developed for optimizing reference resolution based
on graph matching (Chai et al., 2004b). In the graph-matching approach, information
gathered from multiple input modalities and the conversation context is represented as
attributed relational graphs (ARGs) (Tsai & Fu, 1979). Speciﬁcally, two graphs are used.
One graph represents referring expressions from speech utterances (i.e., called referring
graph). A referring graph contains referring expressions used in a speech utterance and
the relations between these expressions. Each node corresponds to one referring expression
and consists of the semantic and temporal information extracted from that expression.
Each edge represents the semantic and temporal relation between two referring expressions.
The resulting graph is a fully connected, undirected, graph. For example, as shown in
Figure 2(a), from the speech input compare this house, the green house, and the brown one,
three nodes are generated in the referring graph representing three referring expressions.
Each node contains semantic and temporal features related to its corresponding referring
expression. These include the expression’s semantic type (house, town, etc.), number of
potential referents, type dependent features (size, price, etc.), syntactic category of the
expression, and the timestamp of when the expression was produced. Each edge contains
features describing semantic and temporal relations between a pair of nodes. The semantic
features simply indicate whether or not two nodes share the same semantic type if this
60

Minimizing Conflicts: A Heuristic Repair Method

Figure 2: Reference resolution through probabilistic graph-matching

can be inferred from the utterance. Otherwise, the semantic type relation is deemed to be
unknown. The temporal features indicate which of the two expressions was uttered ﬁrst.
Similarly, another graph represents all potential referents gathered from gestures, history, and the visual display (i.e., called referent graph). Each node in a referent graph
captures the semantic and temporal information about a potential referent, together with
its selection probability. The selection probability is particularly applied to objects indicated by a gesture. Because a gesture such as a pointing or a circle can potentially introduce
ambiguity in terms of the intended referents, a selection probability is used to indicate how
likely it is that an object is selected by a particular gesture. This selection probability is
derived by a function of the distance between the location of the entity and the focus point
of the recognized gesture on the display. As in a referring graph, each edge in a referent
graph captures the semantic and temporal relations between two potential referents such
as whether the two referents share the same semantic type and the temporal order between
two referents as they are introduced into the discourse. For example, since the gesture input
consists of two pointings, the referent graph (Figure 2b) consists of all potential referents
from these two pointings. The objects in the ﬁrst dashed rectangle are potential referents
selected by the ﬁrst pointing, and those in the second dashed rectangle correspond to the
second pointing. Furthermore, the salient objects from the prior conversation are also included in the referent graph since they could be the potential referents as well (e.g., the
rightmost dashed rectangle in Figure 2b).
Given these graph representations, the reference resolution problem becomes a probabilistic graph-matching problem (Gold & Rangarajan, 1996). The goal is to ﬁnd a match
between the referring graph Gs and the referent graph Gc 2 that achieves the maximum
compatibility (i.e., maximizes Q(Gc , Gs )) as described in the following equation:
2. The subscription s in Gs refers to speech referring expressions and c in Gc refers to candidate referents.

61

Chai, Prasov, & Qu

Q(Gc , Gs ) =

 
(αx , αm )N odeSim(αx , αm )
x
m P




+

x

y

m

n P (αx , αm )P (αy , αn )EdgeSim(γxy , γmn )

(1)

P (αx , αm ) is the matching probability between a referent node αx and a referring node
αm . The overall compatibility Q(Gc , Gs ) depends on the node compatibility N odeSim
and the edge compatibility EdgeSim, which were further deﬁned by temporal and semantic
constraints (Chai et al., 2004). When the algorithm converges, P (αx , αm ) gives the matching
probabilities between a referent node αx and a referring node αm that maximizes the overall
compatibility function. Using these matching probabilities, the system is able to identify the
most probable referent αx to each referring node αm . Speciﬁcally, the referring expression
that matches a potential referent is assigned to the referent if the probability of this match
exceeds an empirically computed threshold. If this threshold is not met, the referring
expression remains unresolved.
Theoretically, this approach provides a solution that maximizes the overall satisfaction
of semantic, temporal, and contextual constraints. However, like many other optimization
approaches, this algorithm is non-polynomial. It relies on an expensive matching process,
which attempts every possible assignment, in order to converge on an optimal interpretation
based on those constraints. However, previous linguistic and cognitive studies indicate that
user language behavior does not occur randomly, but rather follows certain cognitive principles. Therefore, a question arises whether any knowledge from these cognitive principles
can be used to guide this matching process and reduce the complexity.

4. Cognitive Principles
Motivated by previous work (Kehler, 2000), we speciﬁcally focus on two principles: Conversational Implicature and Givenness Hierarchy.
4.1 Conversational Implicature
Grice’s Conversational Implicature Theory indicates that the interpretation and inference of
an utterance during communication is guided by a set of four maxims (Grice, 1975). Among
these four maxims, the Maxim of Quantity and the Maxim of Manner are particularly useful
for our purpose.
The Maxim of Quantity has two components: (1) make your contribution as informative as is required (for the current purposes of the exchange), and (2) do not make your
contribution more informative than is required. In the context of multimodal conversation,
this maxim indicates that users generally will not make any unnecessary gestures or speech
utterances. This is especially true for pen-based gestures since they usually require a special
eﬀort from a user. Therefore, when a pen-based gesture is intentionally delivered by a user,
the information conveyed is often a crucial component used in interpretation.
Grice’s Maxim of Manner has four components: (1) avoid obscurity of expression, (2)
avoid ambiguity, (3) be brief, and (4) be orderly. This maxim indicates that users will
not intentionally make ambiguous references. They will use expressions (either speech or
gesture) they believe can uniquely describe the object of interest so that listeners (in this
case a computer system) can understand. The expressions they choose depend on the
62

Minimizing Conflicts: A Heuristic Repair Method

Status
Expression Form
In f ocus
it
↓
Activated
that, this, this N
↓
F amiliar
that N
↓
U nique identif iable
the N
↓
Ref erential
indef inite this N
↓
Identif iable
aN
Figure 3: Givenness Hierarchy

information in their mental models about the current state of the conversation. However,
the information in a user’s mental model might be diﬀerent from the information the system
possesses. When such an information gap happens, diﬀerent ambiguities could occur from
the system point of view. In fact, most ambiguities are not intentionally caused by the
human speakers, but rather by the system’s incapability of choosing among alternatives
given incomplete knowledge representation, limited capability of contextual inference, and
other factors (e.g., interface design issues). Therefore, the system should not anticipate
deliberate ambiguities from users (e.g., a user only utters a house to refer to a particular
house on the screen), but rather should focus on dealing with the types of ambiguities
caused by the system’s limitations (e.g., gesture ambiguity due to the interface design or
speech ambiguity due to incorrect recognition).
These two maxims help positioning the role of gestures in reference resolution. In
particular, these maxims have put the potential referents indicated by a gesture at a very
important position, which is described in Section 5.
4.2 Givenness Hierarchy
The Givenness Hierarchy proposed by Gundel et al. explains how diﬀerent determiners
and pronominal forms signal diﬀerent information about memory and attention state (i.e.,
cognitive status) (Gundel, Hedberg, & Zacharski, 1993). As in Figure 3, there are six
cognitive statuses in the hierarchy. For example, In focus indicates the highest attentional
state that is likely to continue to be the topic. Activated indicates entities in short term
memory. Each of these statuses is associated with some forms of referring expressions. In
this hierarchy, each cognitive status implies the statuses down the list. For example, In focus
implies Activated, Familiar, etc. The use of a particular expression form not only signals
that the associated cognitive status is met, but also signals that all lower statuses have been
met. In other words, a given form that is used to describe a lower status can also be used to
refer to a higher status, but not vice versa. Cognitive statuses are necessary conditions for
63

Chai, Prasov, & Qu

appropriate use of diﬀerent forms of referring expressions. Gundel et al. found that diﬀerent
referring expressions almost exclusively correlate with the six statuses in this hierarchy.
The Givenness Hierarchy has been investigated earlier in algorithms for resolving pronouns and demonstratives in spoken dialog systems (Eckert & Strube, 2000; Byron, 2002)
and in multimodal interaction (Kehler, 2000). In particular, we would like to extend the previous work (Kehler, 2000) and investigate whether Conversational Implicature and Givenness Hierarchy can be used to resolve a variety of references from simple to complex, and
from precise to ambiguous. Furthermore, the decision list used in Kehler (2000) is proposed based on data analysis and has not been implemented or evaluated in a real-time
system. Therefore, our second goal is to design and implement an eﬃcient algorithm by
incorporating these cognitive principles and empirically compare its performance with the
optimization approach (Chai et al., 2004), the ﬁnite state approach (Johnston & Bangalore,
2000), and the decision list approach (Kehler, 2000).

5. A Greedy Algorithm
A greedy algorithm always makes the choice that looks best at the moment of processing.
That is, it makes a locally optimal choice in the hope that this choice will lead to a globally optimal solution. Simple and eﬃcient greedy algorithms can be used to approximate
many optimization problems. Here we explore the use of Conversational Implicature and
Givenness Hierarchy in designing an eﬃcient greedy algorithm. In particular, we extend the
decision list from Kehler (2000) and utilize the concepts from the two cognitive principles
in the following way:
• Corresponding to the Givenness Hierarchy, the following hierarchy holds for potential
referents: F ocus > V isible. This hierarchy indicates that objects in focus have higher
status in terms of attention states than objects in the visual display. Here Focus
corresponds to the cognitive statuses In focus and Activated in the Givenness Hierarchy,
and Visible corresponds to the statuses Familiar and Uniquely identifiable. Note that
Givenness Hierarchy is ﬁne grained in terms of diﬀerent statuses. Our application
may not be able to distinguish the diﬀerence between these statuses (e.g., In focus
and Activated) and eﬀectively use them. Therefore, Focus and Visible are introduced
here to group some similar statuses (with respect to our application) together. Since
there is a need to diﬀerentiate the objects that have been mentioned recently (e.g.,
in focus and activated) and objects that are accessible either on the graph display
or from the domain model (e.g., familiar and unique identiﬁable), we assign them to
diﬀerent modiﬁed statuses (e.g., Focus and Visible).
• Based on the Conversational Implicature, since a pen-based gesture takes a special effort to deliver, it must convey certain useful information. In fact, objects indicated by
a gesture should have the highest attentional state since they are deliberately singled
out by a user. Therefore, by combining (1) and (2), we derive a modiﬁed hierarchy
Gesture > F ocus > V isible > Others. Here Others corresponds to indeﬁnite cases
in Givenness Hierarchy. This modiﬁed hierarchy coincides with the processing order
of the Kehler’s decision list (2000). This modiﬁed hierarchy will guide the greedy
64

Minimizing Conflicts: A Heuristic Repair Method

algorithm in its search for solutions. Next, we describe in detail the algorithm and
related representations and functions.
5.1 Representation
At each turn3 (i.e., after receiving a user input) of the conversation, we use three vectors to
represent the ﬁrst three statuses in our modiﬁed hierarchy: objects selected by a gesture,
objects in the focus, and objects visible on the display as follows:
• Gesture vector (g ) captures objects selected by a series of gestures. Each element gi
is an object potentially selected by a gesture. For elements gi and gj where i < j, the
gesture that selects objects gi should: 1) temporally precede the gesture that selects
gj or 2) be the same as the gesture that selects gj since one gesture could result in
multiple objects.
• Focus vector (f) captures objects that are in the focus but are not selected by any
gesture. Each element represents an object considered to be the focus of attention
from the previous turn of the conversation. There is no temporal precedence relation
between these elements. We consider all the corresponding objects are simultaneously
accessible to the current turn of the conversation.
 captures objects that are visible on the display but are neither
• Display vector (d)
selected by any gesture (i.e., g) nor in the focus (f). There is also no temporal precedence relation between these elements. All elements are simultaneously accessible.
Based on these representations, each object in the domain of interest belongs to either
one of these above vectors or Others. Each object in the above vectors consists of the
following attributes:
• Semantic type of the object. For example, the semantic type could be a House or a
Town.
• The attributes of the object. This is a domain dependent feature. A set of attributes
is associated with each semantic type. For example, a house object has Price, Size,
Year Built, etc. as its attributes. Furthermore, each object has visual properties that
reﬂect the appearance of the object on the display such as Color of an object icon.
• The identiﬁer of the object. Each object has a unique name.
• The selection probability. It refers to the probability that a given object is selected.
Depending on the interface design, a gesture could result in a list of potential referents.
We use this selection probability to indicate the likelihood of an object selected by
a gesture. The calculation of the selection probability is described later. For objects
from the focus vector and the display vector, the selection probabilities are set to 1/N
where N is the total number of objects in the respective vector.
3. Currently, user inactivity (i.e., 2 seconds with no input from either speech or gesture) is used as the
boundary to decide an interaction turn.

65

Chai, Prasov, & Qu

• Temporal information. The relative temporal ordering information for the corresponding gesture. Instead of applying time stamps as in our previous work (Chai et al.,
2004b), here we only use the index of gestures according to the order of their occurrences. If an object is selected by the ﬁrst gesture, then its temporal information
would be 1.
In addition to vectors that capture potential referents, for each user input, a vector
that represents referring expressions from a speech utterance (r) is also maintained. Each
element (i.e., a referring expression) has the following information:
• The identiﬁer of the potential referent indicated by the referring expression. For
example, the identiﬁer of the potential referent to the expression house number eight
is a house object with an identiﬁer Eight.
• The semantic type of the potential referents indicated by the expression. For example,
the semantic type of the referring expression this house is House.
• The number of potential referents as indicated by the referring expression or the
utterance context. For example, a singular noun phrase refers to one object. A
phrase like three houses provides the exact number of referents (i.e., 3).
• Type dependent features. Any features associated with potential referents, such as
Color and Price, are extracted from the referring expression.
• The temporal ordering information indicating the order of referring expressions as
they are uttered. Again, instead of the speciﬁc time stamp, here we only use the
temporal ordering information. If an utterance consists of N consecutive referring
expressions, then the temporal ordering information for each of them would be 1, 2,
and up to N .
• The syntactic categories of the referring expressions. Currently, for each referring
expression, we assign it to one of six syntactic categories (e.g., demonstrative and
pronoun). Details are explained later.
These four vectors are updated after each user turn in the conversation based on the current
user input and the system state (e.g., what is shown on the screen and what was identiﬁed
as focus from the previous turn of the conversation).
5.2 Algorithm
The ﬂow chart with the pseudo code of the algorithm is shown in Figure 4. For each
multimodal input at a particular turn in the conversation, this algorithm takes the inputs
of a vector (r) of referring expressions with size k, a gesture vector (g ) of size m, a focus
 of size l. It ﬁrst creates three matrices
vector of (f ) of size n, and a display vector (d)
G[i][j], F [i][j], and D[i][j] to capture the scores of matching each referring expression from
r to each object in the three vectors. Calculation of the matching score is described later.
Note that, if any of the g ,f, and d is empty, then the corresponding matrix (i.e., G, F , or
D) is empty.
66

Minimizing Conflicts: A Heuristic Repair Method

InitializeMatchMatrix (,,,){
for (i = 1..m; j = 1..k) G[i][j] = Match(gi, rj)
for (i = 1..n; j = 1..k) F[i][j] = Match(fi, rj)
for (i = 1..l; j = 1..k) D[i][j] = Match(di, rj)
}

Yes

Is G empty

No
GreedySortingGesture {
index_max = 1; //index to the column
for (i = 1..m) {
find j t index_max, where G[i][j] is the largest among the elements in row i.
add a mark “*” to the G[i][j];
index_max = j; } //complete finding the best match from a view of each object
AssignReferentsFromMatrix (G);
}

All references resolved?

No

Yes

Yes
Is F empty

No

Return results

GreedySortingFocus{
for (j = 1..k)
if (rj is resolved)
then Cross out column j in F //only keep ones not resolved
for ( i = 1..n){
find j where F[i][j] is the largest among the elements in row i.
mark “*” to the F[i][j]; }
AssignReferentsFromMatrix (F);
}

All references resolved?

No

GreedySortingDisplay{
for (j = 1..k)
if (rj is resolved)
then Cross out column j in D;
for ( i = 1..l){
find j where D[i][j] is the largest among the elements in row i.
mark “*” to D[i][j]; }
AssignReferentsFromMatrix (D);
}

Return results

AssignReferentsFromMatrix (Matrix X){
for (i = 1..k) // i.e., for each expression ri in column i
if (ri indicates a specific number N and more than N elements
in ith column of X with “*”)
then assign N largest elements with “*” to ri as referents.
else assign all elements with “*” to ri as referents;
}

Figure 4: A greedy algorithm for multimodal reference resolution

67

Yes

Return results

Chai, Prasov, & Qu

Based on the matching scores in the three matrices, the algorithm applies a greedy
search that is guided by our modiﬁed hierarchy as described earlier. Since Gesture has
the highest status, the algorithm ﬁrst searches the Gesture Matrix (G) that keeps track of
matching scores between all referring expressions and all objects from gestures. It identiﬁes
the highest (or multiple highest) matching scores and assigns all possible objects from
gestures to the expressions (GreedySortingGesture).
If more referring expressions are left to be resolved after gestures are processed, the
algorithm looks at objects from the Focus Matrix (F ) since Focus is the next highest cognitive status (GreedySortingFocus). If there are still more expressions to be resolved, then the
algorithm looks at objects from the Display Matrix (D) (GreedySortingDisplay). Currently,
our algorithm focuses on these three statuses. Certainly, if there are still more expressions
to be resolved after all these steps, the algorithm can consult with proper name resolution.
Once all the referring expressions are resolved, the system will output the results. For the
next multimodal input, the system will generate four new vectors and then apply the greedy
algorithm again.
Note that in GreedySortingGesture, we use index-max to keep track of the column index
that corresponds to the largest matching value. As the algorithm incrementally processes
each row in the matrix, this index-max should incrementally increase. This is because the referring expressions and the gesture should be aligned according to their order of occurrences.
Since objects in the Focus Matrix and the Display Matrix do not have temporal precedence
relations, GreedySortingFocus and GreedySortingDisplay do not use this constraint.
The reason we call this algorithm greedy is that it always ﬁnds the best assignment for a
referring expression given a cognitive status in the hierarchy. In other words, this algorithm
always makes the best choice for each referring expression one at a time according to the
order of their occurrence in the utterance. One can imagine that a mistaken assignment
made to an expression can aﬀect the assignment of the following expressions. Therefore,
the greedy algorithm may not lead to a globally optimal solution. Nevertheless, the general
user behavior following the guiding principles makes this greedy algorithm useful.
One major advantage of this greedy algorithm is that the use of the modiﬁed hierarchy can signiﬁcantly prune the search space compared to the graph-matching approach.
Given m referring expressions and n potential referents from various sources (e.g., gesture,
conversation context, and visual display), this algorithm can ﬁnd a solution in O(mn).
Furthermore, this algorithm goes beyond simple and precise inputs as illustrated by the
decision list in Kehler (2000). The scoring mechanism (described later) and the greedy
sorting process accommodate both complex and ambiguous user inputs.
5.3 Matching Functions
An important component of the algorithm is the matching score between an object (o) and
a referring expression (e). We use the following equation to calculate the matching score:
M atch(o, e) = [



P (o|S) ∗ P (S|e)] ∗ Compatibility(o, e)

(2)

S∈{G,F,D}

In this formula, S represents the possible associated status of an object o. It could
have three potential values: G (representing Gesture), F (Focus), and D (Display). This
function is determined by three components:
68

Minimizing Conflicts: A Heuristic Repair Method

• The ﬁrst, P (o|S), is the object selectivity component that measures the probability
of an object to be the referent given a status (S) of that object (i.e., gesture, focus,
or visual display).
• The second, P (S|e), is the likelihood of status component that measures the likelihood
of the status of the potential referent given a particular type of referring expression.
• The third, Compatibility(o, e), is the compatibility component that measures the
semantic and temporal compatibility between the object o and the referring expression
e.
Next we explain these three components in detail.
5.3.1 Object Selectivity
To calculate P (o|S = Gesture), we use a function that takes into consideration of the
distance between an object and the focus point of a gesture on the display (Chai et al.,
2004b).
Given an object from Focus (i.e., not selected by any gesture), P (o|S = F ocus) = 1/N ,
where N is the total number of objects that are in the Focus vector. If an object is neither
selected by a gesture, nor in the focus, but visible on the screen, then P (o|S = Display) =
1/M , where M is the total number of objects that are in the Display vector. Currently,
we only applied the simplest uniform distribution for objects in focus and on the graphical
display. In the future, we intend to incorporate the recency in conversation discourse to
model P (o|S = F ocus) and use visual prominence (e.g., based on visual characteristics)
to model P (o|S = Display). Note that, as discussed earlier in Section 5.1, each object is
associated with only one of the three statuses. In other words, for a given object o, only
one of P (o|S = Gesture), P (o|S = F ocus), and P (o|S = Display) is non-zero.
5.3.2 Likelihood of Status
Motivated by the Givenness Hierarchy and earlier work (Kehler, 2000) that the form of
referring expressions can reﬂect the cognitive status of referred entities in a user’s mental
model, we use the likelihood of status to measure the probability of a reﬂected status given
a particular type of referring expression. In particular, we use the data reported in Kehler
(2000) to derive the likelihood of the status of potential referents given a particular type
of referring expression P (S|e). We categorize referring expressions into the following six
categories:
• Empty: no referring expression is used in the utterance.
• Pronouns: such as it, they, and them
• Locative adverbs: such as here and there
• Demonstratives: such as this, that, these, and those
• Deﬁnite Noun Phrases: noun phrases with the deﬁnite article the
• Full noun phrases: other types such as proper nouns.
69

Chai, Prasov, & Qu

P (S|E)
Visible
Focus
Gesture
Sum

Empty
0
0.56
0.44
1

Pronoun
0
0.85
0.15
1

Locative
0
0.57
0.43
1

Demonstratives
0
0.33
0.67
1

Definite
0
0.07
0.67
1

Full
0
0.47
0.16
1

Table 2: Likelihood of status of referents given a particular type of expression
Table 2 shows the estimated P (S|e). Note that, in the original data provided by Kehler
(2000), there is zero count for a certain combination of a referring type and a referent status.
These zero counts result in zero probability in the table. We did not use any smoothing
techniques to re-distribute the probability mass. Furthermore, there is no probability mass
assigned to the status Others.
5.3.3 Compatibility Measurement
The term Compatibility(o, e) measures the compatibility between an object o and a referring
expression e. Similar to the compatibility measurement in our earlier work (Chai et al.,
2004), it is deﬁned by a multiplication of many factors in the following equation:
Compatibility(o, e) = Id(o, e) ∗ Sem(o, e) ∗



Attrk (o, e) ∗ T emp(o, e)

(3)

k

In this equation:
Id(o, e) It captures the compatibility between the identiﬁer (or name) for o and the identiﬁer
(or name) speciﬁed in e. It indicates that the identiﬁer of the potential referent, as
expressed in a referring expression, should match the identiﬁer of the true referent.
This is particularly useful for resolving proper nouns. For example, if the referring
expression is house number eight, then the correct referent should have the identiﬁer
number eight. Id(o, e) = 0 if the identities of o and e are diﬀerent. Id(o, e) = 1 if the
identities of o and e are either the same or one/both of them unknown.
Sem(o, e) It captures the semantic type compatibility between o and e. It indicates that the
semantic type of a potential referent as expressed in the referring expression should
match the semantic type of the correct referent. Sem(o, e) = 0 if the semantic types
of o and e are diﬀerent. Sem(o, e) = 1 if they are the same or unknown.
Attrk (o, e) It captures the type-speciﬁc constraint concerning a particular semantic feature
(indicated by the subscript k). This constraint indicates that the expected features of
a potential referent as expressed in a referring expression should be compatible with
features associated with the true referent. For example, in the referring expression
the Victorian house, the style feature is Victorian. Therefore, an object can only be a
possible referent if the style of that object is Victorian. Thus, we deﬁne the following:
Attrk (o, e) = 0 if both o and e have the feature k and the values of the feature k are
not equal. Otherwise, Attrk (o, e) = 1.
70

Minimizing Conflicts: A Heuristic Repair Method

(House 3
(House 9 (House 1
Town 1)
Town 2) Town 2)
Gesture input: ……...……… ..i….….i…...…i……
Speech input: Compare it with these houses.
Time

Figure 5: An example of a complex input

T emp(o, e) It captures the temporal compatibility between o and e. Here we only consider the temporal ordering between speech and gesture. Speciﬁcally, the temporal
compatibility is deﬁned as the following:
T emp(o, e) = exp(−|OrderIndex(o) − OrderIndex(e)|)

(4)

The order when the speech and the accompanying gestures occur is important in
deciding which gestures should be aligned with which referring expressions. The
order in which the accompanying gestures are introduced into the discourse should
be consistent with the order in which the corresponding referring expressions are
uttered. For example, suppose a user input consists of three gestures g1 , g2 , g3 and
two referring expressions, s1 , s2 . It will not be possible for g3 to align with s1 and
g2 to align with s2 . Note that, if the status of an object is either Focus or Visible,
then T emp(o, e) = 1. This deﬁnition of temporal compatibility is diﬀerent from the
function used in our previous work (Chai et al., 2004) that takes real time stamps
into consideration. Section 6.2 shows diﬀerent performance results based on diﬀerent
temporal compatibility functions.
5.4 An Example
Figure 5 shows an example of a complex input that involves multiple referring expressions
and multiple gestures. Because the interface displays house icons on top of town icons, a
point (or circle) could result in both a house and a town object. In this example, the ﬁrst
gesture results in both House 3 and Town 1. The second gesture results in House 9 and
Town 2, and the third results in House 1 and Town 2. Suppose before this input takes
place, House 8 is highlighted on the screen from the previous turn of conversation (i.e.,
House 8 is in the focus). Furthermore, there are eight other objects visible on the screen.
To resolve referents to the expressions it and these houses, the greedy algorithm takes the
following steps:
 and r are created with lengths 6, 1, 8, 2, respectively to
1. The four input vectors, g ,f, d,
represent six objects in the gesture vector, one object in the focus, eight more objects
on the graphical display, and two referring expressions used in the utterance.
2. Gesture Matrix G62 , Focus Matrix F12 , and Display Matrix D82 are created.
3. These three matrixes are then initialized by Equation 2. Figure 6 shows the resulting
Gesture Matrix. The probability values of P (S|e) come from Table 2. The diﬀerence
71

Chai, Prasov, & Qu

Status
(G)

Referring Expression Match

Potential
Referent

j = 1:

it

j = 2: these houses

1  0.15  1 = 0.15

i = 1: House 3

1  0.67  0.37 = 0.25*

Gesture 1
i = 2: Town 2

1  0.15  0 = 0

1  0.67  0 = 0

i = 3: House 9

1  0.15  0.37 = 0.055

1  0.67  1 = 0.67*

i = 4: Town 2

1  0.15  0 = 0

1  0.67  0 = 0

Gesture 2
i = 5: House 1

1  0.15  0.14 = 0.02

1  0.67  0.37 = 0.25*

i = 6: Town 2

1  0.15  0 = 0

1  0.67  0 = 0

Gesture 3

(a) Gesture Matrix
Status
(F)

Potential
Referent

Referring Expression Match

Focus

i = 1: House 8

j = 1:

it

j = 2: these houses

1  0.85  1= 0.85*

(b) Focus Matrix

Figure 6: The Gesture Matrix (a) and Focus Matrix (b) for processing the example in Figure 5.
Each cell in the Referring Expression Match columns corresponds to an instantiation of
the matching function.

in the compatibility values for the house objects in the Gesture Matrix is mainly due
to the temporal ordering compatibilities.
4. Next the GreedySortingGesture procedure is executed. For each row in Gesture Matrix, the algorithm ﬁnds the largest legitimate value and marks the corresponding cell
with *. The legitimate means that the corresponding cell for the row i + 1 has to
be either on the same column or the column to the right of the corresponding cell
in row i. These values are shown in bold in Figure 6(a). Next, starting from each
column, the algorithm checks for each referring expression whether any ∗ exists in
its corresponding column. If so, those objects with ∗ are assigned to the referring
expressions based on the number constraints. In this case, since no speciﬁc number is
given in the referring expression these houses, all three marked objects are assigned
to these houses.
5. After these houses, there is still it left to be resolved. Now the algorithm continues to
execute GreedySortingFocus. The Focus Matrix prior to executing GreedySortingFocus
is shown in Figure 6(b). Note that since these houses is no longer considered, its
corresponding column is deleted from the Focus Matrix. Similar to the previous step,
the largest non-zero match value is marked (shown in bold in Figure 6(b)) and assigned
to the remaining referring expression it.
6. The resulting Display Matrix is not shown because at this point, all referring expressions are resolved.
72

Minimizing Conflicts: A Heuristic Repair Method

s1 : the(adj)∗ (N |N s)
s2 : (this|that)(adj ∗ )N
s3 : (these|those)(num+ )(adj ∗ )N s
s4 : it|this|that|(this|that|the)adj ∗ one
s5 : (these|those)num+ adj ∗ ones|them
s6 : here|there
s7 : empty expression
s8 : proper nouns
s9 : multiple expressions
Total Num:

g1
no
gest.
2
4
0
3
0
1
1
1
1
13

g2
one
pt
8
43
0
8
0
1
1
5
0
66

g3
mult.
pts
0
3
0
0
0
0
0
3
4
10

g4
one
cir
2
33
31
10
2
5
1
3
11
98

g5
mult.
cirs
0
1
0
0
0
0
0
0
13
14

g6
pts &
cirs
1
7
5
0
0
0
0
3
2
18

Total
Num
13
91
36
21
2
7
3
15
31
219

Table 3: Detailed description of user referring behavior

6. Evaluation
We use the data collected from our previous work (Chai et al., 2004) to evaluate this greedy
algorithm. The questions addressed in our evaluation are the following:
• What is the impact of temporal alignment between speech and gesture on the performance of the greedy algorithm?
• What is the role of modeling the cognitive status in the greedy algorithm?
• How eﬀective is the greedy algorithm compared to the graph matching algorithm
(Section 3.3)?
• What error sources contribute to the failure in real-time reference resolution?
• How is the greedy algorithm compared to the ﬁnite state approach (Section 3.1) and
the decision list approach (Section 3.2)?
6.1 Experiment Setup
The evaluation data were collected from eleven subjects who participated in our study.
Each of the subjects was asked to interact with the system using both speech and gestures
(e.g., pointing and circle) to accomplish ﬁve tasks related to real estate information seeking.
The ﬁrst task was to ﬁnd the least expensive house in the most populated town. In order
to accomplish this task, the user would have to ﬁrst ﬁnd the town that has the highest
population and then ﬁnd the least expensive house in this town. The next task involved
obtaining a description of the house located in the previous task. The next task was to
compare the house that was located in the ﬁrst task with all of the houses in a particular
town in terms of price. Additionally, the least expensive house in this second town should
be determined. Another task was to ﬁnd the most expensive house in a particular town.
73

Chai, Prasov, & Qu

S0 : No referring expression
S1 : One referring expression
S2 : Multiple referring expressions
Total Num:

G0 : No
Gesture
1 (a)
11 (a)
1 (c)
13

G1 : One
Gesture
2 (a)
151 (b)
11 (c)
164

G2 : MultiGesture
0 (c)
23 (c)
19 (c)
42

Total
Num
3
185
31
219

Table 4: Summary of user referring behavior
The last task involved comparing the resulting houses of the previous four tasks. For this
last task, the previous four tasks may have to be completely or partially repeated. These
tasks were designed so that users were required to explore the interface to acquire various
types of information.
The acoustic model for each subject was trained individually to minimize speech recognition errors. The study session was videotaped to capture both audio and video on the
screen movement (including gestures and system responses). The IBM Viavoice speech
recognizer was used to process each speech input.
Table 3 provides a detailed description of the referring behavior observed in the study.
The columns indicate whether no gesture, one gesture (pointing or circle), or multiple gestures are involved in a multimodal input. The rows indicate the type of referring expressions
in a speech utterance. Each table entry shows the number of a particular combination of
speech and gesture inputs.
Table 4 summarizes Table 3 in terms of whether no gesture, one gesture, or multiple
gestures (shown as columns) and whether no referring expression, one referring expression,
or multiple referring expressions (shown as rows) are involved in the input. Note that in
this table an intended input is counted as one input even if this input may be split into a
few turns by our system during the run time.
Based on Table 4, we further categorize user inputs into the following three categories:
• Simple Inputs with One-Zero Alignment: inputs that contain no speech referring
expression with no gesture (i.e.,< S0 , G0 >), one referring expression with zero gesture
(i.e.,< S1 , G0 >), and no referring expression with one gesture (i.e., < S0 , G1 >).
These types of inputs require the conversation context or visual context to resolve
references. One example of this type is the U2 in Table 1. From our data, a total of
14 inputs belong to this category (marked (a) in Table 4).
• Simple Inputs with One-One Alignment: inputs that contain exactly one referring
expression and one gesture (i.e., < S1 , G1 >). These types of inputs can be resolved
mostly by combining gesture and speech using multimodal fusion. A total of 151
inputs belong to this category (marked (b) in Table 4).
• Complex Inputs: inputs that contain more than one referring expression and/or gesture. This corresponds to the entry < S1 , G2 >, < S2 , G0 >,< S2 , G1 >, and
< S2 , G2 > in Table 4. One example of this type is U3 in Table 1. A total of 54
74

Minimizing Conflicts: A Heuristic Repair Method

No. Correctly Resolved
Simple One-Zero Alignment
Simple One-One Alignment
Complex
Total
Accuracy

Ordering
5
104
24
133
60.7%

Absolute
5
104
19
128
58.4%

Combined
5
104
23
132
60.3%

Table 5: Performance comparison based on diﬀerent temporal compatibility functions
inputs belong to this category (marked (c) in Table 4). These types of inputs are
particularly challenging to resolve.
In this section, we will focus on diﬀerent performance evaluations based on these three
types of referring behaviors.
6.2 Temporal Alignment Between Speech and Gesture
In multimodal interpretation, how to align speech and gesture based on their temporal
information is an important question. This is especially the case for complex inputs where
a multimodal input consists of multiple referring expressions and multiple gestures. We
evaluated diﬀerent temporal compatibility functions for the greedy approach. In particular,
we compared the following three functions:
• The ordering temporal constraint as in Equation 4.
• The absolute temporal constraint as deﬁned by the following formula:
T emp(o, e) = exp(−|BeginT ime(o) − BeginT ime(e)|)

(5)

Here, the absolute timestamps of the potential referents (e.g., indicated by a gesture)
and the referring expressions are used instead of the relative orders of relevant entities
in a user input.
• The combined temporal constraint that combines the two aforementioned constraints,
giving each equal weight in determining the compatibility score between an object
and a referring expression.
The results are shown in Table 5. Diﬀerent temporal constraints only aﬀect the processing of complex inputs. The ordering temporal constraint worked slightly better than the
absolute temporal constraint. In fact, temporal alignment between speech and gesture is often one of the problems that may aﬀect interpretation results. Previous studies have found
the gestures tend to occur before the corresponding speech unit takes place (Oviatt et al.,
1997). The ﬁndings suggest that users tend to tap on the screen ﬁrst and then start the
speech utterance. This behavior was observed in a simple command based system (Oviatt
et al., 1997) where each speech unit corresponds with a single gesture (i.e., the simple inputs
in our work).
75

Chai, Prasov, & Qu

Non-overlap
Overlap
Total :

Speech First
7%
8%
15%

Gesture First
45%
40%
85%

Total
52%
48%
100%

Table 6: Overall temporal relations between speech and gesture

From our study, we found that temporal alignment between gesture and corresponding
speech units is still an issue that needs to be further investigated in order to improve
the robustness in multimodal interpretation. Table 6 shows the percentage of diﬀerent
temporal relations observed in our study. The rows indicate whether there is an overlap
between speech referring expressions and their accompanied gestures. The columns indicate
whether the speech (more precisely, the referring expressions) or the gesture occurred ﬁrst.
Consistent with the previous ﬁndings (Oviatt et al., 1997), in most cases (85% of time),
gestures occurred before the referring expressions were uttered. However, in 15% of the cases
the speech referring expressions were uttered before the corresponding gesture occurred.
Among those cases, 8% had an overlap between the referring expressions and the gesture
and 7% had no overlap.
Furthermore, although multimodal behaviors such as sequential (i.e., non-overlap) or
simultaneous (e.g., overlap) integration are quite consistent during the course of interaction (Oviatt, Coulston, Tomko, Xiao, Bunsford, Wesson, & Carmichael, 2003), there are
some exceptions. Figure 7 shows the temporal alignments from individual users in our study.
User 2 , User 6, and User 8 maintained a consistent behavior in that User 2’s gesture always
happened before and overlapped with the corresponding speech referring expressions; User
6’s gesture always occurred ahead of the speech expressions without overlapping; and User
8’s speech referring expressions always occurred before the corresponding gestures (without
any overlap). The other users exhibited varied temporal alignment between speech and
gesture during the interaction. It will be diﬃcult for a system using pre-deﬁned temporal
constraints to anticipate and accommodate all these diﬀerent behaviors. Therefore, it is
desirable to have a mechanism that can automatically learn the user behavior of alignment
and automatically adjust to that behavior.
One potential approach is to introduce a calibration process before real human computer
interaction. In this calibration process, two tasks will be performed by a user. In the ﬁrst
task, the user will be asked to describe objects on the graph display with both speech
and deictic gestures. In the second task, the user will be asked to respond to the system
questions by using both speech and deictic gestures. The reason to have users perform
these two tasks is to identify whether there is any diﬀerence between user initiated inputs
and system initiated user responses. Based on these tasks, the temporal relations between
the speech units and corresponding gestures can be captured and used in the real-time
interaction.
76

Minimizing Conflicts: A Heuristic Repair Method

Percentage of Occurance

Non-overlap Speech First
Overlap Speech First

Non-overlap Gesture First
Overlap Gesture First

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1

2

3

4

5

6

7

8

9

10

11

User Index

Figure 7: Temporal alignment behavior from our user study

No. Correctly Resolved
Simple One-Zero Alignment
Simple One-One Alignment
Complex
Total

with Cognitive Principles
5
104
24
133

without Cognitive Principles
5
92
18
115

Table 7: The role of cognitive principles in the greedy algorithm

6.3 The Role of Cognitive Principles
To further examine the role of modeling cognitive status in multimodal reference, we compared the two conﬁgurations of the greedy algorithm. The ﬁrst conﬁguration is based on the
matching score deﬁned in Equation 2, which incorporates the cognitive principles described
earlier. The second conﬁguration only uses the matching score that is completely dependent on the compatibility between a referring expression and a gesture (i.e., Section 5.3.3)
without using the cognitive principles (i.e., P (o|S) and P (S|e) are not included in Equation
2).
Table 7 shows the comparison results in terms of these two conﬁgurations. The algorithm
using the cognitive principles outperforms the algorithm that does not use the cognitive
principles by more than 15%. The performance diﬀerence applies to both simple inputs
with one-one alignment and complex inputs. The results indicate that modeling cognitive
status can potentially improve reference resolution performance.
77

Chai, Prasov, & Qu

Total Num
Total
Simple One-Zero Alignment
Simple One-One Alignment
Complex

219
14
151
54

Graph-matching
Num %
130
59.4%
7
50.0%
104
68.9%
19
35.2%

Greedy
Num %
133
60.7%
5
35.7%
104
68.9%
24
44.4%

Table 8: Performance comparison between the graph-matching algorithm and the greedy
algorithm

6.4 Greedy Algorithm versus Graph-matching Algorithm
We further compared the greedy algorithm and the graph-matching algorithm in terms of
performance and runtime. Table 8 shows the performance comparison. Overall, the greedy
algorithm performs comparably with the graph-matching algorithm.
To compare the runtime, we ran each algorithm on each user 10 times where each input
was run 100 times. In other words, each user input was run 1000 times by each algorithm
to get the average runtime measurement. This experiment was done on a UltraSPARC-III
server with 750MHz and 64bit.
Both the greedy algorithm and the graph-matching algorithm have the same function
calls to process speech inputs (e.g., parsing) and gesture inputs (e.g., identify potentially
intended objects). The diﬀerence between these algorithms are the speciﬁc implementations
regarding graph creation and matching as in the graph-matching algorithm and the greedy
search as in the greedy algorithm. As a result, the average time for the greedy algorithm
to process simple inputs and complex inputs are 17.3 milliseconds and 21.2 milliseconds
respectively. The average time for the graph matching algorithm to process simple and
complex inputs are 22.3 milliseconds and 24.8 milliseconds respectively. These results show
that on average the greedy algorithm runs slightly faster than the graph-matching algorithm
given our current implementation, although in the worst case, the graph-matching algorithm
is asymptotically more complex.
6.5 Real-time Error Analysis
To understand the bottleneck in real-time multimodal reference resolution, we examined
the error cases where the algorithm failed to provide correct referents.
Like in most spoken dialog systems, speech recognition is a major bottleneck. Although
we have trained each user’s acoustic model individually, the speech recognition rate is still
very low. Only 127 of inputs had correctly recognized referring expressions. Among these
inputs, 103 of them were resolved with correct referents. Fusing inputs from multiple
modalities together can sometimes compensate for the recognition errors (Oviatt, 1996).
Among 92 inputs in which referring expressions were incorrectly recognized, 29 of them
were correctly assigned referents due to the mutual disambiguation. A mechanism to reduce
78

Minimizing Conflicts: A Heuristic Repair Method

the recognition errors, especially by utilizing information from other modalities, will be
important to provide a robust solution for real time multimodal reference resolution.
The second source of errors comes from another common problem in most spoken dialog
systems, namely out-of-vocabulary words. For example, area was not in our vocabulary.
So the additional semantic constraint expressed by area was not captured. Therefore, the
system could not identify whether a house or a town was referred to when the user uttered
this area. It is important for the system to have a capability to acquire knowledge (e.g.,
vocabulary) dynamically by utilizing information from other modalities and the interaction
context. Furthermore, the errors also came from a lack of understanding of spatial relations
(as in the house just close to the red one) and superlatives (as in the most expensive house).
Algorithms for aligning visual features to resolve spatial references are desirable (Gorniak
& Roy, 2004).
In addition to these two main sources, some errors are caused by unsynchronized inputs.
Currently, we use an idle status (i.e., 2 seconds with no input from either speech or gesture)
as the boundary to delimit an interaction turn. Two types of out of synchronization were
observed. The ﬁrst type is unsynchronized inputs from the user (such as a big pause between
speech and gesture) and the other comes from the underlying system implementation. The
system captures speech inputs and gesture inputs from two diﬀerent servers through a
TCP/IP protocol. A communication delay sometimes split one synchronized input into
two separate turns of inputs (e.g., one turn was speech input alone and the other turn was
gesture input alone). A better engineering mechanism for synchronizing inputs is desired.
The disﬂuencies from the users also accounted for a small number of errors. The current algorithm is incapable of distinguishing disﬂuent cases from normal cases. Fortunately,
the disﬂuent situations did not occur frequently in our study (only 6 inputs with disﬂuency). This is consistent with the previous ﬁndings that speech disﬂuency rate is lower in
human machine conversation than in spontaneous speech (Brennan, 2000). During humancomputer conversation, users tend to speak carefully and utterances tend to be short. Recent
ﬁndings indicated that gesture patterns could be used as an additional source to identify
diﬀerent types of speech disﬂuencies during human-human conversation (Chen, Harper, &
Quek, 2002). Based on our limited cases, we found that gesture patterns could be indicators
of speech disﬂuencies when they did occur. For example, if a user says show me the red
house (point to house A), the green house (still point to the house A), then the behavior of
pointing to the same house with diﬀerent speech description usually indicates a repair. Furthermore, gestures also involve disﬂuencies; for example, repeatedly pointing to an object is
a gesture repetition. Failure in identifying these disﬂuencies caused problems with reference
resolution. It will be ideal to have a mechanism that can identify these disﬂuencies using
multimodal information.
6.6 Comparative Evaluation with Two Other Approaches
To further examine how the greedy algorithm is compared to the ﬁnite state approach
(Section 3.1) and the decision list approach (Section 3.2), we conducted a comparative evaluation. In the original ﬁnite state approach, the N-best speech hypotheses are maintained
in the speech tape. In our data here, we only had the best speech hypothesis for each speech
input. Therefore, we manually updated some incorrectly recognized words so that the ﬁnite
79

Chai, Prasov, & Qu

No. Correctly Resolved
Simple Inputs with one-one alighment
Simple Inputs with zero-one alighment
Complex Inputs
Total

Greedy
116
8
24
148

Finite State
115
0
13
128

Decision List
88
12
0
100

Table 9: Performance comparison with two other approaches
state approach would not be penalized because of the lack of N-best speech hypotheses 4 .
The modiﬁed data were used in all three approaches. Table 9 shows the comparison results.
As shown in this table, the greedy algorithm correctly resolved more inputs than the
ﬁnite state approach and the decision list approach. The major problem with the ﬁnite state
approach is that it does not incorporate conversation context in the ﬁnite state transducer.
This problem contributes to the failure in resolving simple inputs with zero-one alignment
and some of the complex inputs. The major problem with the decision list approach, as
described earlier, is the lack of capabilities to process ambiguous gestures and complex
inputs.
Note that the greedy algorithm is not an algorithm to obtain the full semantic interpretation of a multimodal input. But rather it is an algorithm speciﬁcally for reference
resolution, which uses information from context and gesture to resolve speech referring expressions. In this regard, the greedy algorithm is diﬀerent from the ﬁnite state approach
whose goal is to get a full interpretation of user inputs and reference resolution is only a
part of this process.

7. Conclusion
Motivated by earlier investigation on the cognitive status in human machine interaction, this
paper describes a greedy algorithm that incorporates the cognitive principles underlying human referring behavior to resolve a variety of references during human machine multimodal
interaction. In particular, this algorithm relies on the theories of Conversation Implicature
and Givenness Hierarchy to eﬀectively guide the system in searching for potential referents. Our empirical studies have shown that modeling the form of referring experssions and
its implication on the cognitive status can achieve better results than the algorithm that
only considers the compatibility between referring expressions and potential referents. This
greedy algorithm can eﬃciently achieve comparable performance as a previous optimization
approach based on graph-matching. Furthermore, because this greedy algorithm handles
a variety of user inputs ranging from precise to ambiguous and from simple to complex,
it outperforms the ﬁnite state approach and the decision list approach in our experiments.
Because of its simplicity and generality, this approach has a potential to improve the robustness of multimodal interpretation. We have learned from this investigation that prior
4. Note that we only corrected those inputs where there was a direct correspondence between the recognized
words and transcribed words to maintain the consistency of timestamps.

80

Minimizing Conflicts: A Heuristic Repair Method

knowledge from linguistic and cognitive studies can be very beneﬁcial in designing eﬃcient
and practical algorithms for enabling multimodal human machine communication.

Acknowledgments
This work was supported by a NSF CAREER award IIS-0347548. The authors would like
to thank anonymous reviewers for their valuable comments and suggestions.

References
Bolt, R. (1980). Put that there: Voice and gesture at the graphics interface. Computer
Graphics, 14 (3), 262–270.
Brennan, S. (2000). Processes that shape conversation and their implications for computational linguistics. In Proceedings of 38th Annual Meeting of ACL, pp. 1–8.
Byron, D. (2002). Resolving pronominal reference to abstract entities. In Proceedings of
40th Annual Meeting of ACL, pp. 80–87.
Cassell, J., Bickmore, T., Billinghurst, M., Campbell, L., Chang, K., Vilhjalmsson, H., &
Yan, H. (1999). Embodiment in conversational interfaces: Rea. In Proceedings of the
CHI’99, pp. 520–527.
Chai, J., Hong, P., Zhou, M., & Prasov, Z. (2004). Optimization in multimodal interpretation. In Proceedings of 42nd Annual Meeting of Association for Computational
Linguistics (ACL), pp. 1–8.
Chai, J., Prasov, Z., Blaim, J., & Jin, R. (2005). Linguistic theories in eﬃcient multimodal
reference resolution: An empirical study. In Proceedings of The 10th International
Conference on Intelligent User Interfaces(IUI), pp. 43–50.
Chai, J., Prasov, Z., & Hong, P. (2004a). Performance evaluation and error analysis for
multimodal reference resolution in a conversational system. In Proceedings of HLTNAACL 2004 (Companion Volumn), pp. 41–44.
Chai, J. Y., Hong, P., & Zhou, M. X. (2004b). A probabilistic approach to reference resolution in multimodal user interfaces. In Proceedings of 9th International Conference
on Intelligent User Interfaces (IUI), pp. 70–77.
Chen, L., Harper, M., & Quek, F. (2002). Gesture patterns during speech repairs. In
Proceedings of International Conference on Multimodal Interfaces (ICMI), pp. 155–
160.
Cohen, P. (1984). The pragmatics of referring and modality of communication. Computational Linguistics, 10, 97–146.
Cohen, P., Johnston, M., McGee, D., Oviatt, S., Pittman, J., Smith, I., Chen, L., & Clow, J.
(1996). Quickset: Multimodal interaction for distributed applications. In Proceedings
of ACM Multimedia, pp. 31–40.
Eckert, M., & Strube, M. (2000). Dialogue acts, synchronising units and anaphora resolution. In Journal of Semantics, Vol. 17(1), pp. 51–89.
81

Chai, Prasov, & Qu

Gold, S., & Rangarajan, A. (1996). A graduated assignment algorithm for graph-matching.
IEEE Trans. Pattern Analysis and Machine Intelligence, 18 (4), 377–388.
Gorniak, P., & Roy, D. (2004). Grounded semantic composition for visual scenes. Journal
of Artificial Intelligence Research, 21, 429–470.
Grice, H. P. (1975). Logic and conversation. In Cole, P., & Morgan, J. (Eds.), Speech Acts,
pp. 41–58. New York: Academic Press.
Grosz, B. J., & Sidner, C. (1986). Attention, intention, and the structure of discourse.
Computational Linguistics, 12 (3), 175–204.
Gundel, J. K., Hedberg, N., & Zacharski, R. (1993). Cognitive status and the form of
referring expressions in discourse. Language, 69 (2), 274–307.
Gustafson, J., Bell, L., Beskow, J., Boye, J., Carlson, R., Edlund, J., Granstrom, B., House,
D., & Wiren, M. (2000). Adapt - a multimodal conversational dialogue system in
an apartment domain. In Proceedings of 6th International Conference on Spoken
Language Processing (ICSLP), Vol. 2, pp. 134–137.
Huls, C., Bos, E., & Classen, W. (1995). Automatic referent resolution of deictic and
anaphoric expressions. Computational Linguistics, 21 (1), 59–79.
Johnston, M. (1998). Uniﬁcation-based multimodal parsing. In Proceedings of COLINGACL’98, pp. 624–630.
Johnston, M., & Bangalore, S. (2000). Finite-state multimodal parsing and understanding.
In Proceedings of COLING’00, pp. 369–375.
Johnston, M., Cohen, P., McGee, D., Oviatt, S., Pittman, J., & Smith, I. (1997). Uniﬁcationbased multimodal integration. In Proceedings of ACL’97, pp. 281–288.
Kehler, A. (2000). Cognitive status and form of reference in multimodal human-computer
interaction. In Proceedings of AAAI’00, pp. 685–689.
Koons, D. B., Sparrell, C. J., & Thorisson, K. R. (1993). Integrating simultaneous input
from speech, gaze, and hand gestures. In Maybury, M. (Ed.), Intelligent Multimedia
Interfaces, pp. 257–276. MIT Press.
Neal, J. G., & Shapiro, S. C. (1991). Intelligent multimedia interface technology. In Sullivan,
J., & Tyler, S. (Eds.), Intelligent User Interfaces, pp. 45–68. ACM: New York.
Neal, J. G., Thielman, C. Y., Dobes, Z. H., M., S., & Shapiro, S. C. (1998). Natural language
with integrated deictic and graphic gestures. In Maybury, M., & Wahlster, W. (Eds.),
Intelligent User Interfaces, pp. 38–51. CA: Morgan Kaufmann Press.
Oviatt, S., Coulston, R., Tomko, S., Xiao, B., Bunsford, R., Wesson, M., & Carmichael, L.
(2003). Toward a theory of organized multimodal integration patterns during humancomputer interaction. In Proceedings of Fifth International Conference on Multimodal
Interfaces, pp. 44–51.
Oviatt, S., DeAngeli, A., & Kuhn, K. (1997). Integration and synchronization of input
modes during multimodal human-computer interaction. In Proceedings of Conference
on Human Factors in Computing Systems: CHI’97, pp. 415–422.
82

Minimizing Conflicts: A Heuristic Repair Method

Oviatt, S. L. (1996). Multimodal interfaces for dynamic interactive maps. In Proceedings
of Conference on Human Factors in Computing Systems: CHI’96, pp. 95–102.
Oviatt, S. L. (1999a). Multimodal system processing in mobile environments. In Proceedings
of the Thirteenth Annual ACM Symposium on User Interface Software Technology
(UIST’2000), pp. 21–30.
Oviatt, S. L. (1999b). Mutual disambiguation of recognition errors in a multimodal architecture. In Proceedings of Conference on Human Factors in Computing Systems:
CHI’99, pp. 576–583.
Stent, A., Dowding, J., Gawron, J. M., Bratt, E. O., & Moore, R. (1999). The commandtalk
spoken dialog system. In Proceedings of ACL’99, pp. 183–190.
Stock, O. (1993). Alfresco: Enjoying the combination of natural language processing and
hypermedia for information exploration. In Maybury, M. (Ed.), Intelligent Multimedia
Interfaces, pp. 197–224. MIT Press.
Tsai, W. H., & Fu, K. S. (1979). Error-correcting isomorphism of attributed relational
graphs for pattern analysis. IEEE Trans. Sys., Man and Cyb., 9, 757–768.
Wahlster, W. (1998). User and discourse models for multimodal communication. In Maybury, M., & Wahlster, W. (Eds.), Intelligent User Interfaces, pp. 359–370. ACM Press.
Wu, L., & Oviatt, S. (1999). Multimodal integration - a statistical view. IEEE Transactions
on Multimedia, 1 (4), 334–341.
Zancanaro, M., Stock, O., & Strapparava, C. (1997). Multimodal interaction for information
access: Exploiting cohesion. Computational Intelligence, 13 (7), 439–464.

83

Journal of Artificial Intelligence Research 27 (2006) 153–201

Submitted 05/06; published 10/06

Solving Factored MDPs with Hybrid State and Action
Variables
Branislav Kveton

bkveton@cs.pitt.edu

Intelligent Systems Program
5406 Sennott Square
University of Pittsburgh
Pittsburgh, PA 15260

Milos Hauskrecht

milos@cs.pitt.edu

Department of Computer Science
5329 Sennott Square
University of Pittsburgh
Pittsburgh, PA 15260

Carlos Guestrin

guestrin@cs.cmu.edu

Machine Learning Department and
Computer Science Department
5313 Wean Hall
Carnegie Mellon University
Pittsburgh, PA 15213

Abstract
Efficient representations and solutions for large decision problems with continuous and
discrete variables are among the most important challenges faced by the designers of automated decision support systems. In this paper, we describe a novel hybrid factored Markov
decision process (MDP) model that allows for a compact representation of these problems,
and a new hybrid approximate linear programming (HALP) framework that permits their
efficient solutions. The central idea of HALP is to approximate the optimal value function
by a linear combination of basis functions and optimize its weights by linear programming.
We analyze both theoretical and computational aspects of this approach, and demonstrate
its scale-up potential on several hybrid optimization problems.

1. Introduction
A dynamic decision problem with components of uncertainty can be very often formulated as
a Markov decision process (MDP). An MDP represents a controlled stochastic process whose
dynamics is described by state transitions. Objectives of the control are modeled by rewards
(or costs), which are assigned to state-action configurations. In the simplest form, the states
and actions of an MDP are discrete and unstructured. These models can be solved efficiently
by standard dynamic programming methods (Bellman, 1957; Puterman, 1994; Bertsekas &
Tsitsiklis, 1996).
Unfortunately, textbook models rarely meet the practice and its needs. First, real-world
decision problems are naturally described in a factored form and may involve a combination
of discrete and continuous variables. Second, there are no guarantees that compact forms of
the optimal value function or policy for these problems exist. Therefore, hybrid optimization
problems are usually discretized and solved approximately by the methods for discrete-state
c
°2006
AI Access Foundation. All rights reserved.

Kveton, Hauskrecht, & Guestrin

MDPs. The contribution of this work is a principled, sound, and efficient approach to solving
large-scale factored MDPs that avoids this discretization step.
Our framework is based on approximate linear programming (ALP) (Schweitzer & Seidmann, 1985), which has been already applied to solve decision problems with discrete state
and action variables efficiently (Schuurmans & Patrascu, 2002; de Farias & Van Roy, 2003;
Guestrin et al., 2003). These applications include context-specific planning (Guestrin et al.,
2002), multiagent planning (Guestrin et al., 2002), relational MDPs (Guestrin et al., 2003),
and first-order MDPs (Sanner & Boutilier, 2005). In this work, we show how to adapt ALP
to solving large-scale factored MDPs in hybrid state and action spaces.
The presented approach combines factored MDP representations (Sections 3 and 4) and
optimization techniques for solving large-scale structured linear programs (Section 6). This
leads to various benefits. First, the quality and complexity of value function approximations
is controlled by using basis functions (Section 3.2). Therefore, we can prevent an exponential
blowup in the complexity of computations when other techniques cannot. Second, we always
guarantee that HALP returns a solution. Its quality naturally depends on the choice of basis
functions. As analyzed in Section 5.1, if these are selected appropriately, we achieve a close
approximation to the optimal value function V ∗ . Third, a well-chosen class of basis functions
yields closed-form solutions to the backprojections of our value functions (Section 5.2). This
step is important for solving hybrid optimization problems more efficiently. Finally, solving
hybrid factored MDPs reduces to building and satisfying relaxed formulations of the original
problem (Section 6). The formulations can be solved efficiently by the cutting plane method,
which has been studied extensively in applied mathematics and operations research.
For better readability of the paper, our proofs are deferred to Appendix A. The following
notation is adopted throughout the work. Sets and their members are represented by capital
and small italic letters as S and s, respectively. Sets of variables, their subsets, and members
of these sets are denoted by capital letters as X, Xi , and Xi . In general, corresponding small
letters represent value assignments to these objects. The subscripted indices D and C denote
the discrete and continuous variables in a variable set and its value assignment. The function
Dom(·) computes the domain of a variable or the domain of a function. The function Par(·)
returns the parent set of a variable in a graphical model (Howard & Matheson, 1984; Dean
& Kanazawa, 1989).

2. Markov Decision Processes
Markov decision processes (Bellman, 1957) provide an elegant mathematical framework for
modeling and solving sequential decision problems in the presence of uncertainty. Formally,
a finite-state Markov decision process (MDP) is given by a 4-tuple M = (S, A, P, R), where
S = {s1 , . . . , sn } is a set of states, A = {a1 , . . . , am } is a set of actions, P : S ×A×S → [0, 1]
is a stochastic transition function of state dynamics conditioned on the preceding state and
action, and R : S × A → R is a reward function assigning immediate payoffs to state-action
configurations. Without loss of generality, the reward function is assumed to be nonnegative
and bounded from above by a constant Rmax (Puterman, 1994). Moreover, we assume that
the transition and reward models are stationary and known a priori.
Once a decision problem is formulated as an MDP, the goal is to find a policy π : S → A
that maximizes some objective function. In this paper, the quality of a policy π is measured
154

Solving Factored MDPs with Hybrid State and Action Variables

by the infinite horizon discounted reward :
¯
#
" ∞
¯
X
t
(t)
(t) ¯ (0)
γ R(s , π(s ))¯ s ∼ ϕ ,
Eπ
¯

(1)

t=0

where γ ∈ [0, 1) is a discount factor, s(t) is the state at the time step t, and the expectation is
taken with respect to all state-action trajectories that start in the states s(0) and follow the
policy π thereafter. The states s(0) are chosen according to a distribution ϕ. This optimality
criterion assures that there exists an optimal policy π ∗ which is stationary and deterministic
(Puterman, 1994). The policy is greedy with respect to the optimal value function V ∗ , which
is a fixed point of the Bellman equation (Bellman, 1957):
"
#
X
∗
′
∗ ′
V (s) = max R(s, a) + γ
P (s | s, a)V (s ) .
(2)
a

s′

The Bellman equation plays a fundamental role in all dynamic programming (DP) methods
for solving MDPs (Puterman, 1994; Bertsekas & Tsitsiklis, 1996), including value iteration,
policy iteration, and linear programming. The focus of this paper is on linear programming
methods and their refinements. Briefly, it is well known that the optimal value function V ∗
is a solution to the linear programming (LP) formulation (Manne, 1960):
X
minimize
ψ(s)V (s)
(3)
s

subject to: V (s) ≥ R(s, a) + γ

X

P (s′ | s, a)V (s′ ) ∀ s ∈ S, a ∈ A;

s′

where V (s) represents the variables in the LP, one for each state s, and ψ(s) > 0 is a strictly
positive weighting on the state space S. The number of constraints equals to the cardinality
of the cross product of the state and action spaces |S × A|.
Linear programming and its efficient solutions have been studied extensively in applied
mathematics and operations research (Bertsimas & Tsitsiklis, 1997). The simplex algorithm
is a common way of solving LPs. Its worst-case time complexity is exponential in the number
of variables. The ellipsoid method (Khachiyan, 1979) offers polynomial time guarantees but
it is impractical for solving LPs of even moderate size.
The LP formulation (3) can be solved compactly by the cutting plane method (Bertsimas
& Tsitsiklis, 1997) if its objective function and constraint space are structured. Briefly, this
method searches for violated constraints in relaxed formulations of the original LP. In every
step, we start with a relaxed solution V (t) , find a violated constraint given V (t) , add it to the
LP, and resolve for a new vector V (t+1) . The method is iterated until no violated constraint
is found, so that V (t) is an optimal solution to the LP. The approach has a potential to solve
large structured linear programs if we can identify violated constraints efficiently (Bertsimas
& Tsitsiklis, 1997). The violated constraint and the method that found it are often referred
to as a separating hyperplane and a separation oracle, respectively.
Delayed column generation is based on a similar idea as the cutting plane method, which
is applied to the column space of variables instead of the row space of constraints. Bender’s
and Dantzig-Wolfe decompositions reflect the structure in the constraint space and are often
used for solving large structured linear programs.
155

Kveton, Hauskrecht, & Guestrin

3. Discrete-State Factored MDPs
Many real-world decision problems are naturally described in a factored form. Discrete-state
factored MDPs (Boutilier et al., 1995) allow for a compact representation of this structure.
3.1 Factored Transition and Reward Models
A discrete-state factored MDP (Boutilier et al., 1995) is a 4-tuple M = (X, A, P, R), where
X = {X1 , . . . , Xn } is a state space described by a set of state variables, A = {a1 , . . . , am } is
a set of actions1 , P (X′ | X, A) is a stochastic transition model of state dynamics conditioned
on the preceding state and action, and R is a reward function assigning immediate payoffs to
state-action configurations. The state of the system is completely observed and represented
by a vector of value assignments x = (x1 , . . . , xn ). We assume that the values of every state
variable Xi are restricted to a finite domain Dom(Xi ).
Transition model: The transition model is given by the conditional probability distribution P (X′ | X, A), where X and X′ denote the state variables at two successive time steps.
Since the complete tabular representation of P (X′ | X, A) is infeasible, we assume that the
transition model factors along X′ as:
′

P (X | X, a) =

n
Y

P (Xi′ | Par(Xi′ ), a)

(4)

i=1

and can be described compactly by a dynamic Bayesian network (DBN) (Dean & Kanazawa,
1989). This DBN representation captures independencies among the state variables X and
X′ given an action a. One-step dynamics of every state variable is modeled by its conditional
probability distribution P (Xi′ | Par(Xi′ ), a), where Par(Xi′ ) ⊆ X denotes the parent set of Xi′ .
Typically, the parent set is a subset of state variables which simplifies the parameterization
of the model. In principle, the parent set can be extended to the state variables X′ . Such an
extension poses only few new challenges when solving the new problems efficiently (Guestrin,
2003). Therefore, we omit the discussion on the modeling of intra-layer dependencies in this
paper.
Reward model: The reward model
factors similarly to the transition model. In particular,
P
the reward function R(x, a) = j Rj (xj , a) is an additive function of local reward functions
defined on the subsets Xj and A. In graphical models, the local functions can be described
compactly by reward nodes Rj , which are conditioned on their parent sets Par(Rj ) = Xj ∪A.
To allow this representation, we formally extend our DBN to an influence diagram (Howard
& Matheson, 1984).
Example 1 (Guestrin et al., 2001) To illustrate the concept of a factored MDP, we consider a network administration problem, in which the computers are unreliable and fail. The
failures of these computers propagate through network connections to the whole network. For
instance, if the server X1 (Figure 1a) is down, the chance that the neighboring computer X2
1. For simplicity of exposition, we discuss a simpler model, which assumes a single action variable A instead
of the factored action space A = {A1 , . . . , Am }. Our conclusions in Sections 3.1 and 3.3 extend to MDPs
with factored action spaces (Guestrin et al., 2002).

156

Solving Factored MDPs with Hybrid State and Action Variables

(a)

(b)

(c)

Figure 1: a. Four computers in a ring topology. Direction of propagating failures is denoted
by arrows. b. A graphical representation of factored transition and reward models
after taking an action a1 in the 4-ring topology. The future state of the server X1′
is independent of the rest of the network because the server is rebooted. Reward
nodes R1 and Rj (j ≥ 2) denote the components 2x1 and xj (j ≥ 2) of the reward
model. c. A graphical
representation of the linear value function approximation
P
V w (x) = w0 + 4i=1 wi xi in the 4-ring topology. Reward nodes H0 and Hi (i ≥ 1)
denote the value function components w0 and wi xi (i ≥ 1).
crashes increases. The administrator can prevent the propagation of the failures by rebooting
computers that have already crashed.
This network administration problem can be formulated as a factored MDP. The state of
the network is completely observable and represented by n binary variables X = {X1 , . . . , Xn },
where the variable Xi denotes the state of the i-th computer: 0 (being down) or 1 (running).
At each time step, the administrator selects an action from the set A = {a1 , . . . , an+1 }. The
action ai (i ≤ n) corresponds to rebooting the i-th computer. The last action an+1 is dummy.
The transition function reflects the propagation of failures in the network and can be encoded
locally by conditioning on the parent set of every computer. A natural metric for evaluating
the performance of an administrator is the total number of running computers. This metric
factors along the computer states xi and can be represented compactly by an additive reward
function:
R(x, a) = 2x1 +

n
X

xj .

j=2

The weighting of states establishes our preferences for maintaining the server X1 and workstations X2 , . . . , Xn . An example of transition and reward models after taking an action a1
in the 4-ring topology (Figure 1a) is given in Figure 1b.
3.2 Solving Discrete-State Factored MDPs
Markov decision processes can be solved by exact DP methods in polynomial time in the size
of the state space X (Puterman, 1994). Unfortunately, factored state spaces are exponential
in the number of state variables. Therefore, the DP methods are unsuitable for solving large
157

Kveton, Hauskrecht, & Guestrin

factored MDPs. Since a factored representation of an MDP (Section 3.1) may not guarantee
a structure in the optimal value function or policy (Koller & Parr, 1999), we resort to value
function approximations to alleviate this concern.
Value function approximations have been successfully applied to a variety of real-world
domains, including backgammon (Tesauro, 1992, 1994, 1995), elevator dispatching (Crites
& Barto, 1996), and job-shop scheduling (Zhang & Dietterich, 1995, 1996). These partial
successes suggest that the approximate dynamic programming is a powerful tool for solving
large optimization problems.
In this work, we focus on linear value function approximation (Bellman et al., 1963; Van
Roy, 1998):
V w (x) =

X

wi fi (x).

(5)

i

The approximation restricts the form of the value function V w to the linear combination of
|w| basis functions fi (x), where w is a vector of optimized weights. Every basis function can
be defined over the complete state space X, but usually is limited to a small subset of state
variables Xi (Bellman et al., 1963; Koller & Parr, 1999). The role of basis functions is similar
to features in machine learning. They are often provided by domain experts, although there
is a growing amount of work on learning basis functions automatically (Patrascu et al., 2002;
Mahadevan, 2005; Kveton & Hauskrecht, 2006a; Mahadevan & Maggioni, 2006; Mahadevan
et al., 2006).
Example 2 To demonstrate the concept of the linear value function model, we consider the
network administration problem (Example 1) and assume a low chance of a single computer
failing. Then the value function in Figure 1c is sufficient to derive a close-to-optimal policy
on the 4-ring topology (Figure 1a) because the indicator functions fi (x) = xi capture changes
in the states of individual computers. For instance, if the computer Xi fails, the linear policy:
"
#
X
′
w ′
u(x) = arg max R(x, a) + γ
P (x | x, a)V (x )
a

x′

immediately leads to rebooting it. If the failure has already propagated to the computer Xi+1 ,
the policy recovers it in the next step. This procedure is repeated until the spread of the initial
failure is stopped.
3.3 Approximate Linear Programming
Various methods for fitting of the linear value function approximation have been proposed
and analyzed (Bertsekas & Tsitsiklis, 1996). We focus on approximate linear programming
(ALP) (Schweitzer & Seidmann, 1985), which recasts this problem as a linear program:
minimizew

X
x

subject to:

X
i

ψ(x)

X

wi fi (x)

i

wi fi (x) ≥ R(x, a) + γ

(6)
X

P (x′ | x, a)

x′

158

X
i

wi fi (x′ )

∀ x ∈ X, a ∈ A;

Solving Factored MDPs with Hybrid State and Action Variables

where w represents the variables in theP
LP, ψ(x) ≥ 0 are
Pstate relevance weights weighting
the quality of the approximation, and γ x′ P (x′ | x, a) i wi fi (x′ ) is a discounted backprojection of the value function V w (Equation 5). The ALP formulation can be easily derived
from the standard LP formulation (3) by substituting V w (x) for V (x). The formulation is
feasible if the set of basis functions contains a constant function f0 (x) ≡ 1. We assume that
such a basis function is always present. Note that the state relevance weights are no longer
enforced to be strictly positive (Section 1). Comparing to the standard LP formulation (3),
which is solved by the optimal value function V ∗ for arbitrary weights ψ(s) > 0, a solution
e to the ALP formulation depends on the weights ψ(x). Intuitively, the higher the weights,
w
e
the higher the quality of the approximation V w
in a corresponding state.
Since our basis functions are usually restricted to subsets of state variables (Section 3.2),
summation terms in the ALP formulation can be computed efficiently (Guestrin et al., 2001;
Schuurmans & Patrascu, 2002).
Pexample, the order of summation in the backprojection
P For
term can be rearranged as γ i wi x′ P (x′i | x, a)fi (x′i ), which allows its aggregation in the
i
space of Xi instead of X. Similarly, a factored form of ψ(x) yields an efficiently computable
objective function (Guestrin, 2003).
The number of constraints in the ALP formulation is exponential in the number of state
variables. Fortunately, the constraints are structured. This results from combining factored
transition and reward models (Section 3.1) with the linear approximation (Equation 5). As
a consequence, the constraints can be satisfied without enumerating them exhaustively.
Example 3 The notion of a factored constraint space is important for compact satisfaction
of exponentially many constraints. To illustrate this concept, let us consider the linear value
function (Example 2) on the 4-ring network administration problem (Example 1). Intuitively,
by combining the graphical representations of P (x′ | x, a1 ), R(x, a1 ) (Figure 1b), and V w (x)
(Figure 1c), we obtain a factored model of constraint violations:
X
P (x′ | x, a1 )V w (x′ ) − R(x, a1 )
τ w (x, a1 ) = V w (x) − γ
=

X

x′

wi fi (x) − γ

i

= w0 +

X

wi

i

4
X

X

P (x′i | x, a1 )fi (x′i ) − R(x, a1 )

x′i

wi xi − γw0 − γw1 P (x′1 = 1 | a1 )−

i=1

γ

4
X

wi P (x′i = 1 | xi , xi−1 , a1 ) − 2x1 −

4
X

xj .

j=2

i=2

for an arbitrary solution w (Figure 2a). Note that this cost function:
τ w (x, a1 ) = φw +

4
X

φw (xi ) +

4
X

φw (xi , xi−1 )

i=2

i=1

is a linear combination of a constant φw in x, and univariate and bivariate functions φw (xi )
and φw (xi , xi−1 ). It can be represented compactly by a cost network (Guestrin et al., 2001),
which is an undirected graph over a set of variables X. Two nodes in the graph are connected
159

Kveton, Hauskrecht, & Guestrin

(a)

(b)

Figure 2: a. A graphical representation of combining factored transition and reward models
(Figure 1b) with the linear approximation (Figure 1c). Reward nodes G0 and Gi
(i ≥ 1) represent the discounted backprojection terms −γw0 and −γwi x′i (i ≥ 1).
Gray regions are the cost components of the constraint space. b. A cost network
corresponding to our factored constraint space (Figure 2a). The network captures
pairwise dependencies X1 −X2 , X2 −X3 , and X3 −X4 . The treewidth of the cost
network is 1.

if any of the cost terms depends on both variables. Therefore, the cost network corresponding
to the function τ w (x, a1 ) must contain edges X1 −X2 , X2 −X3 , and X3 −X4 (Figure 2b).
Savings achieved by the compact representation of constraints are related to the efficiency
of computing arg minx τ w (x, a1 ) (Guestrin, 2003). This computation can be done by variable
elimination and its complexity increases exponentially in the width of the tree decomposition
of the cost network. The smallest width of all tree decompositions is referred to as treewidth.
Inspired by the factorization, Guestrin et al. (2001) proposed a variable-elimination method
(Dechter, 1996) that rewrites the constraint space in ALP compactly. Schuurmans and Patrascu (2002) solved the same problem by the cutting plane method. The method iteratively
searches for the most violated constraint:


¸
X
X (t) ·
P (x′i | x, a)fi (x′i ) − R(x, a)
(7)
arg min 
wi fi (xi ) − γ
x,a

i

x′i

with respect to the solution w(t) of a relaxed ALP. The constraint is added to the LP, which
is resolved for a new solution w(t+1) . This procedure is iterated until no violated constraint
is found, so that w(t) is an optimal solution to the ALP.
The quality of the ALP formulation has been studied by de Farias and Van Roy (2003).
e
Based on their work, we conclude that ALP yields a close approximation V w
to the optimal
∗
∗
w
value function V if the weighted max-norm error kV − V k∞,1/L can be minimized. We
return to this theoretical result in Section 5.1.

160

Solving Factored MDPs with Hybrid State and Action Variables

e be a solution to the ALP formulation
Theorem 1 (de Farias & Van Roy, 2003) Let w
e
(6). Then the expected error of the value function V w
can be bounded as:
°
°
2ψ T L
° ∗
e°
min kV ∗ − V w k∞,1/L ,
° ≤
°V − V w
1−κ w
1,ψ
P
where k·k1,ψ is an L1 -norm weighted by the state relevance weights ψ, L(x) = i wiL fi (x) is a
Lyapunov function such that the inequality κL(x) ≥ γ supa EP (x′ |x,a) [L(x′ )] holds, κ ∈ [0, 1)
denotes its contraction factor, and k·k∞,1/L is a max-norm reweighted by the reciprocal 1/L.
¯
¯ ∗
°
°
e¯
e°
¯V − V w
over
Note that the L1 -norm distance °V ∗ − V w
equals
to
the
expectation
E
ψ
1,ψ
the state space X with respect to the state relevance weights ψ. Similarly to Theorem 1, we
utilize the L1 and L∞ norms in the rest of the work to measure the expected and worst-case
errors of value functions. These norms are defined as follows.
Definition
1 The L1 (Manhattan) and L∞ (infinity) norms are typically defined as kf k1 =
P
x |f (x)| and kf k∞ = maxx |f (x)|. If the state space X is represented by both discrete and
continuous variables XD and XC , the definition of the norms changes accordingly:
XZ
kf k1 =
|f (x)| dxC and kf k∞ = sup |f (x)| .
(8)
xD

x

xC

The following definitions:
XZ
kf k1,ψ =
ψ(x) |f (x)| dxC
xD

and

xC

kf k∞,ψ = sup ψ(x) |f (x)|

(9)

x

correspond to the L1 and L∞ norms reweighted by a function ψ(x).

4. Hybrid Factored MDPs
Discrete-state factored MDPs (Section 3) permit a compact representation of decision problems with discrete states. However, real-world domains often involve continuous quantities,
such as temperature and pressure. A sufficient discretization of these quantities may require
hundreds of points in a single dimension, which renders the representation of our transition
model (Equation 4) infeasible. In addition, rough and uninformative discretization impacts
the quality of policies. Therefore, we want to avoid discretization or defer it until necessary.
As a step in this direction, we discuss a formalism for representing hybrid decision problems
in the domains of discrete and continuous variables.
4.1 Factored Transition and Reward Models
A hybrid factored MDP (HMDP) is a 4-tuple M = (X, A, P, R), where X = {X1 , . . . , Xn } is
a state space described by state variables, A = {A1 , . . . , Am } is an action space described by
action variables, P (X′ | X, A) is a stochastic transition model of state dynamics conditioned
on the preceding state and action, and R is a reward function assigning immediate payoffs to
state-action configurations.2
2. General state and action space MDP is an alternative term for a hybrid MDP. The term hybrid does not
refer to the dynamics of the model, which is discrete-time.

161

Kveton, Hauskrecht, & Guestrin

P(X′2 | X2 = 0)

P(X′1 )

P(X′2 | X2 = 1, X1 = 0)

P(X′2 | X2 = 1, X1 = 1)

Probability density

8
6
4
2
0

0

0.5
X′1

1

4

4

4

3

3

3

2

2

2

1

1

1

0

0

0.5
X′2

0

1

0

0.5
X′2

1

0

0

0.5
X′2

1

Figure 3: Transition functions for continuous variables X1′ and X2′ after taking an action a1
in the 4-ring topology (Example 4). The densities are shown for extreme values
of their parent variables X1 and X2 .

State variables: State variables are either discrete or continuous. Every discrete variable
Xi takes on values from a finite domain Dom(Xi ). Following Hauskrecht and Kveton (2004),
we assume that every continuous variable is bounded to the [0, 1] subspace. In general, this
assumption is very mild and permits modeling of any closed interval on R. The state of the
system is completely observed and described by a vector of value assignments x = (xD , xC )
which partitions along its discrete and continuous components xD and xC .
Action variables: The action space is distributed and represented by action variables A.
The composite action is defined by a vector of individual action choices a = (aD , aC ) which
partitions along its discrete and continuous components aD and aC .
Transition model: The transition model is given by the conditional probability distribution P (X′ | X, A), where X and X′ denote the state variables at two Q
successive time steps.
We assume that this distribution factors along X′ as P (X′ | X, A) = ni=1 P (Xi′ | Par(Xi′ ))
and can be described compactly by a DBN (Dean & Kanazawa, 1989). Typically, the parent
set Par(Xi′ ) ⊆ X ∪ A is a small subset of state and action variables which allows for a local
parameterization of the transition model.
Parameterization of our transition model: One-step dynamics of every state variable
is described by its conditional probability distribution P (Xi′ | Par(Xi′ )). If Xi′ is a continuous
variable, its transition function is represented by a mixture of beta distributions (Hauskrecht
& Kveton, 2004):
X
P (Xi′ = x | Par(Xi′ )) =
πij Pbeta (x | αj , βj )
(10)
j

Γ(α + β) α−1
x
(1 − x)β−1 ,
Pbeta (x | α, β) =
Γ(α)Γ(β)

where πij is the weight assigned to the j-th component of the mixture, and αj = φαij (Par(Xi′ ))
and βj = φβij (Par(Xi′ )) are arbitrary positive functions of the parent set. The mixture of beta
distributions provides a very general class of transition functions and yet allows closed-form
162

Solving Factored MDPs with Hybrid State and Action Variables

solutions3 to the expectation terms in HALP (Section 5). If every βj = 1, Equation 10 turns
into a polynomial in Xi′ . Due to the Weierstrass approximation theorem (Jeffreys & Jeffreys,
1988), such a polynomial is sufficient to approximate any continuous transition density over
Xi′ with any precision. If Xi′ is a discrete variable, its transition model is parameterized by
|Dom(Xi′ )| nonnegative discriminant functions θj = φθij (Par(Xi′ )) (Guestrin et al., 2004):
θj
P (Xi′ = j | Par(Xi′ )) = P
|Dom(Xi′ )|
j=1

.

(11)

θj

Note that the parameters αj , βj , and θj (Equations 10 and 11) are functions instantiated by
value assignments to the variables Par(Xi′ ) ⊆ X ∪ A. We keep separate parameters for every
state variable Xi′ although our indexing does not reflect this explicitly. The only restriction
on the functions is that they return valid parameters for all state-action pairs (x, a). Hence,
P|Dom(X ′ )|
we assume that αj (x, a) ≥ 0, βj (x, a) ≥ 0, θj (x, a) ≥ 0, and j=1 i θj (x, a) > 0.
Reward model: The reward P
model factors similarly to the transition model. In particular,
the reward function R(x, a) = j Rj (xj , aj ) is an additive function of local reward functions
defined on the subsets Xj and Aj . In graphical models, the local functions can be described
compactly by reward nodes Rj , which are conditioned on their parent sets Par(Rj ) = Xj ∪Aj .
To allow this representation, we formally extend our DBN to an influence diagram (Howard
& Matheson, 1984). Note that the form of the reward functions Rj (xj , aj ) is not restricted.
Optimal value function and policy: The optimal policy π ∗ can be defined greedily with
respect to the optimal value function V ∗ , which is a fixed point of the Bellman equation:
£
¤¤
£
(12)
V ∗ (x) = sup R(x, a) + γEP (x′ |x,a) V ∗ (x′ )
a


XZ

P (x′ | x, a)V ∗ (x′ ) dx′C  .
= sup R(x, a) + γ
a

x′C

x′D

Accordingly, the hybrid Bellman operator T ∗ is given by:
£
¤¤
£
T ∗ V (x) = sup R(x, a) + γEP (x′ |x,a) V (x′ ) .

(13)

a

In the rest of the paper, we denote expectation terms over discrete and continuous variables
in a unified form:
XZ
EP (x) [f (x)] =
P (x)f (x) dxC .
(14)
xD

xC

Example 4 (Hauskrecht & Kveton, 2004) Continuous-state network administration is
a variation on Example 1, where the computer states are represented by continuous variables
on the interval between 0 (being down) and 1 (running). At each time step, the administrator
3. The term closed-form refers to a generally accepted set of closed-form operations and functions extended
by the gamma and incomplete beta functions.

163

Kveton, Hauskrecht, & Guestrin

selects a single action from the set A = {a1 , . . . , an+1 }. The action ai (i ≤ n) corresponds to
rebooting the i-th computer. The last action an+1 is dummy. The transition model captures
the propagation of failures in the network and is encoded locally by beta distributions:
α = 20
a=i
β=2
6 i
α = 2 + 13xi − 5xi E[Par(Xi′ )] a =
′
β = 10 − 2xi − 6xi E[Par(Xi )]

P (Xi′ = x | Par(Xi′ )) = Pbeta (x | α, β)

where the variables xi and E[Par(Xi′ )] denote the state of the i-th computer and the expected
state of its parents. Note that this transition function is similar to Example 1. For instance,
in the 4-ring topology, the modes of transition densities for continuous variables X1′ and X2′
after taking an action a1 (Figure 3):
Pb(X2′

Pb(X1′ | a = a1 ) = 0.95 Pb(X2′ | X2 = 1, X1 = 0, a = a1 ) ≈ 0.67
| X2 = 0, a = a1 ) = 0.10 Pb(X2′ | X2 = 1, X1 = 1, a = a1 ) = 0.90

equal to the expected values of their discrete counterparts (Figure 1b). The reward function
is additive:
R(x, a) =

2x21

+

n
X

x2j

j=2

and establishes our preferences for maintaining the server X1 and workstations X2 , . . . , Xn .
4.2 Solving Hybrid Factored MDPs
Value iteration, policy iteration, and linear programming are the most fundamental dynamic
programming methods for solving MDPs (Puterman, 1994; Bertsekas & Tsitsiklis, 1996).
Unfortunately, none of these techniques is suitable for solving hybrid factored MDPs. First,
their complexity is exponential in the number of state variables if the variables are discrete.
Second, the methods assume a finite support for the optimal value function or policy, which
may not exist if continuous variables are present. Therefore, any feasible approach to solving
arbitrary HMDPs is likely to be approximate. In the rest of the section, we review two major
classes of methods for approximating value functions in hybrid domains.
Grid-based approximation: Grid-based methods (Chow & ©Tsitsiklis, 1991;
ª Rust, 1997)
transform the initial state space X into a set of grid points G = x(1) , . . . , x(N ) . The points
are used to estimate the optimal value function VG∗ on the grid, which in turn approximates
V ∗ . The Bellman operator on the grid is defined as (Rust, 1997):


N
X
PG (x(j) | x(i) , a)V (x(j) ) ,
(15)
TG∗ V (x(i) ) = max R(x(i) , a) + γ
a

j=1

(i)
(j) | x(i) , a) is a transition function, which is normalwhere PG (x(j) | x(i) , a) = Ψ−1
aP(x )P (x
(j) | x(i) , a). The operator T ∗ allows the computation
ized by the term Ψa (x(i) ) = N
j=1 P (x
G
∗
of the value function VG by standard techniques for solving discrete-state MDPs.

164

Solving Factored MDPs with Hybrid State and Action Variables

Inputs:
a hybrid factored MDP M = (X, A, P, R)
basis functions f0 (x), f1 (x), f2 (x), . . .
initial basis function© weights w(0) ª
a set of states G = x(1) , . . . , x(N )

Algorithm:
t=0
while a stopping criterion is not met
for every state x(j)
for every basis function fi (x)
Xji = fhi (x(j) )

h (t)
ii
yj = maxa R(x(j) , a) + γEP (x′ |x(j) ,a) V w (x′ )

w(t+1) = (XT X)−1 XT y
t=t+1

Outputs:
basis function weights w(t)

Figure 4: Pseudo-code implementation of the least-squares value iteration (L2 VI) with the
linear value function approximation (Equation 5). The
is often
°
° stopping criterion
(t)
°
° w(t)
− T ∗ V w ° measured
based on the number of steps or the L2 -norm error °V
2
on the set G. Our discussion in Sections 5.2 and 6 provides a recipe for an efficient
(t)
implementation of the backup operation T ∗ V w (x(j) ).

Rust (1997) analyzed the convergence of these methods for random and pseudo-random
samples. Clearly, a uniform discretization of increasing precision guarantees the convergence
of VG∗ to V ∗ but causes an exponential blowup in the state space (Chow & Tsitsiklis, 1991).
To overcome this concern, Munos and Moore (2002) proposed an adaptive algorithm for nonuniform discretization based on the Kuhn triangulation. Ferns et al. (2005) analyzed metrics
for aggregating states in continuous-state MDPs based on the notion of bisimulation. Trick
and Zin (1993) used linear programming to solve low-dimensional problems with continuous
variables. These continuous variables were discretized manually.
Parametric value function approximation: An alternative approach to solving factored
MDPs with continuous-state components is the approximation of the optimal value function
V ∗ by some parameterized model V λ (Bertsekas & Tsitsiklis, 1996; Van Roy, 1998; Gordon,
1999). The parameters λ are typically optimized iteratively
by applying
the backup operator
°
°
T ∗ to a finite set of states. The least-squares error °V λ − T ∗ V λ °2 is a commonly minimized
error metric (Figure 4). Online updating by gradient methods (Bertsekas & Tsitsiklis, 1996;
Sutton & Barto, 1998) is another way of optimizing value functions. The limitation of these
techniques is that their solutions are often unstable and may diverge (Bertsekas, 1995). On
the other hand, they generate high-quality approximations.

165

Kveton, Hauskrecht, & Guestrin

Parametric approximations often assume fixed value function models. However, in some
cases, it is possible to derive flexible forms of V λ that combine well with the backup operator
T ∗ . For instance, Sondik (1971) showed that convex piecewise linear functions are sufficient
to represent value functions and their DP backups in partially-observable MDPs (POMDPs)
(Astrom, 1965; Hauskrecht, 2000). Based on this idea, Feng et al. (2004) proposed a method
for solving MDPs with continuous variables. To obtain full DP backups, the value function
approximation is restricted to rectangular piecewise linear and convex (RPWLC) functions.
Further restrictions are placed on the transition and reward models of MDPs. The advantage
of the approach is its adaptivity. The major disadvantages are restrictions on solved MDPs
and the complexity of RPWLC value functions, which may grow exponentially in the number
of backups. As a result, without further modifications, this approach is less likely to succeed
in solving high-dimensional and distributed decision problems.

5. Hybrid Approximate Linear Programming
To overcome the limitations of existing methods for solving HMDPs (Section 4.2), we extend
the discrete-state ALP (Section 3.3) to hybrid state and action spaces. We refer to this novel
framework as hybrid approximate linear programming (HALP).
Similarly to the discrete-state ALP, HALP optimizes the linear value function approximation (Equation 5). Therefore, it transforms an initially intractable problem of computing
V ∗ in the hybrid state space X into a lower dimensional space of w. The HALP formulation
is given by a linear program4 :
minimizew

X

wi αi

(16)

i

subject to:

X

wi Fi (x, a) − R(x, a) ≥ 0 ∀ x ∈ X, a ∈ A;

i

where w represents the variables in the LP, αi denotes basis function relevance weight:
αi = Eψ(x) [fi (x)]
XZ
=
ψ(x)fi (x) dxC ,
xD

(17)

xC

ψ(x) ≥ 0 is a state relevance density function that weights the quality of the approximation,
and Fi (x, a) = fi (x) − γgi (x, a) denotes the difference between the basis function fi (x) and
its discounted backprojection:
£
¤
gi (x, a) = EP (x′ |x,a) fi (x′ )
XZ
P (x′ | x, a)fi (x′ ) dx′C .
=
x′D

(18)

x′C

4. More precisely, the HALP formulation (16) is a linear semi-infinite optimization problem with an infinite
number of constraints. The number of basis functions is finite. For brevity, we refer to this optimization
problem as linear programming.

166

Solving Factored MDPs with Hybrid State and Action Variables

Vectors xD (x′D ) and xC (x′C ) are the discrete and continuous components of value assignments x (x′ ) to all state variables X (X′ ). The linear program can be rewritten compactly:
minimizew

Eψ [V w ]

subject to: V

w

(19)
∗

−T V

w

≥0

by using the Bellman operator T ∗ .
The HALP formulation reduces to the discrete-state ALP (Section 3.3) if the state and
action variables are discrete, and to the continuous-state ALP (Hauskrecht & Kveton, 2004)
if the state variables are continuous. The formulation is feasible if the set of basis functions
contains a constant function f0 (x) ≡ 1. We assume that such a basis function is present.
In the rest of the paper, we address several concerns related to the HALP formulation.
First, we analyze the quality of this approximation and relate it to the minimization of the
max-norm error kV ∗ − V w k∞ , which is a commonly-used metric (Section 5.1). Second, we
present rich classes of basis functions that lead to closed-form solutions to the expectation
terms in the objective function and constraints (Equations 17 and 18). These terms involve
sums and integrals over the complete state space X (Section 5.2), and therefore are hard to
evaluate. Finally, we discuss approximations to the constraint space in HALP and introduce
a framework for solving HALP formulations in a unified way (Section 6). Note that complete
satisfaction of this constraint space may not be possible since every state-action pair (x, a)
induces a constraint.
5.1 Error Bounds
The quality of the ALP approximation (Section 3.3) has been studied by de Farias and Van
Roy (2003). We follow up on their work and extend it to structured state and action spaces
with continuous variables. Before we proceed, we demonstrate that a solution to the HALP
formulation (16) constitutes an upper bound on the optimal value function V ∗ .
e
e be a solution to the HALP formulation (16). Then V w
Proposition 1 Let w
≥ V ∗.

This result allows us to restate the objective Eψ [V w ] in HALP.

e is a solution to the HALP formulation (16):
Proposition 2 Vector w
minimizew

Eψ [V w ]

subject to:

V w − T ∗V w ≥ 0

minimizew

kV ∗ − V w k1,ψ

subject to:

V w − T ∗ V w ≥ 0;

if and only if it solves:

where k·k1,ψ is an L1 -norm weighted by the state relevance density function ψ and T ∗ is the
hybrid Bellman operator.
167

Kveton, Hauskrecht, & Guestrin

Based on Proposition 2, we conclude that HALP optimizes the linear value function approximation with respect to the reweighted L1 -norm error kV ∗ − V w k1,ψ . The following theorem
draws a parallel between minimizing this objective and max-norm error kV ∗ − V w k∞ . More
e
precisely, the theorem says that HALP yields a close approximation V w
to the optimal value
∗
∗
function V if V is close to the span of basis functions fi (x).
e be an optimal solution to the HALP formulation (16). Then the expected
Theorem 2 Let w
e
error of the value function V w
can be bounded as:
°
°
2
° ∗
e°
w
V
−
V
min kV ∗ − V w k∞ ,
° ≤
°
1−γ w
1,ψ

where k·k1,ψ is an L1 -norm weighted by the state relevance density function ψ and k·k∞ is a
max-norm.
°
°
e°
Unfortunately, Theorem 2 rarely yields a tight bound on °V ∗ − V w
. First, it is hard to
1,ψ
guarantee a uniformly low max-norm error kV ∗ − V w k∞ if the dimensionality of a problem
grows but the basis functions fi (x) are local. Second, the bound ignores the state relevance
density function ψ(x) although this one impacts the quality of HALP solutions. To address
these concerns, we introduce non-uniform weighting of the max-norm error in Theorem 3.
e be an optimal solution to the HALP formulation (16). Then the expected
Theorem 3 Let w
e
error of the value function V w
can be bounded as:
°
°
° ∗
e°
°
°V − V w

1,ψ

≤

2Eψ [L]
min kV ∗ − V w k∞,1/L ,
1−κ w

P
where k·k1,ψ is an L1 -norm weighted by the state relevance density ψ, L(x) = i wiL fi (x) is a
Lyapunov function such that the inequality κL(x) ≥ γ supa EP (x′ |x,a) [L(x′ )] holds, κ ∈ [0, 1)
denotes its contraction factor, and k·k∞,1/L is a max-norm reweighted by the reciprocal 1/L.
Note that Theorem 2 is a special form of Theorem 3 when L(x) ≡ 1 and κ = γ. Therefore,
the Lyapunov function L(x) permits at least as good bounds as Theorem 2. To make these
bounds tight, the function L(x) should return large values in the regions of the state space,
which are unimportant for modeling. In turn, the reciprocal 1/L(x) is close to zero in these
undesirable regions, which makes their impact on the max-norm error kV ∗ − V w k∞,1/L less
likely. Since the state relevance density function ψ(x) reflects the importance of states, the
term Eψ [L] should remain small. These two factors contribute to tighter bounds than those
by Theorem 2.
P
Since the Lyapunov function L(x) = i wiL fi (x) lies in the span of basis functions fi (x),
Theorem 3 provides a recipe for achieving high-quality approximations. Intuitively, a good
set of basis functions always involves two types of functions. The first type guarantees small
errors |V ∗ (x) − V w (x)| in the important regions of the state space, where the state relevance
density ψ(x) is high. The second type returns high values where the state relevance density
ψ(x) is low, and vice versa. The latter functions allow the satisfaction of the constraint space
V w ≥ T ∗ V w in the unimportant regions of the state space without impacting the optimized
objective function kV ∗ − V w k1,ψ . Note that a trivial value function V w (x) = (1−γ)−1 Rmax
168

Solving Factored MDPs with Hybrid State and Action Variables

satisfies all constraints in any HALP but unlikely leads to good policies. For a comprehensive
discussion on selecting appropriate ψ(x) and L(x), refer to the case studies of de Farias and
Van Roy (2003).
Our discussion is concluded by clarifying the notion of the state relevance density ψ(x).
As demonstrated by Theorem 4, its choice is closely related to the quality of a greedy policy
e
for the value function V w
(de Farias & Van Roy, 2003).
e be an optimal solution to the HALP formulation (16). Then the expected
Theorem 4 Let w
error of a greedy policy:
h
h
ii
e
u(x) = arg sup R(x, a) + γEP (x′ |x,a) V w
(x′ )
a

can be bounded as:
kV ∗ − V u k1,ν ≤

°
1 °
° ∗
e°
,
°V − V w
°
1−γ
1,µu,ν

where k·k1,ν and k·k1,µu,ν are weighted L1 -norms, V u is a value function for the greedy policy
u, and µu,ν is the expected frequency of state visits generated by following the policy u given
the initial state distribution ν.
Based on Theorem 4, we may conclude that the expected error of greedy policies for HALP
approximations is bounded when ψ = µu,ν . Note that the distribution µu,ν is unknown when
e
optimizing V w
because it is a function of the optimized quantity itself. To break this cycle,
de Farias and Van Roy (2003) suggested an iterative procedure that solves several LPs and
adapts µu,ν accordingly. In addition, real-world control problems exhibit a lot of structure,
which permits the guessing of µu,ν .
Finally, it is important to realize that although our bounds (Theorems 3 and 4) build a
foundation for better HALP approximations, they can be rarely used in practice because the
optimal value function V ∗ is generally unknown. After all, if it was known, there is no need
to approximate it. Moreover, note that the optimization of kV ∗ − V w k∞,1/L (Theorem 3) is
a hard problem and there are no methods that would minimize this error directly (Patrascu
et al., 2002). Despite these facts, both bounds provide a loose guidance for empirical choices
of basis functions. In Section 7, we use this intuition and propose basis functions that should
closely approximate unknown optimal value functions V ∗ .
5.2 Expectation Terms
Since our basis functions are often restricted to small subsets of state variables, expectation
terms (Equations 17 and 18) in the HALP formulation (16) should be efficiently computable.
To unify the analysis of these expectation terms, Eψ(x) [fi (x)] and EP (x′ |x,a) [fi (x′ )], we show
that their evaluation constitutes the same computational problem EP (x) [fi (x)], where P (x)
denotes some factored distribution.
Before we discuss expectation terms in the constraints, note that the transition function
P (x′ | x, a) is factored and its parameterization is determined by the state-action pair (x, a).
We keep the pair (x, a) fixed in the rest of the section, which corresponds to choosing a single
constraint (x, a). Based on this selection, we rewrite the expectation terms EP (x′ |x,a) [fi (x′ )]
169

Kveton, Hauskrecht, & Guestrin

in a simpler notation EP (x′ ) [fi (x′ )], where P (x′ ) = P (x′ | x, a) denotes a factored distribution
with fixed parameters.
We also assume that the state relevance density function ψ(x) factors along X as:
ψ(x) =

n
Y

ψi (xi ),

(20)

i=1

where ψi (xi ) is a distribution over the random state variable Xi . Based on this assumption,
we can rewrite the expectation terms Eψ(x) [fi (x)] in the objective function in a new notation
EP (x) [fi (x)], where P (x) = ψ(x) denotes a factored distribution. In line with our discussion
in the last two paragraphs, efficient solutions to the expectation terms in HALP are obtained
by solving the generalized term EP (x) [fi (x)] efficiently. We address this problem in the rest
of the section.
Before computing the expectation term EP (x) [fi (x)] over the complete state space X, we
recall that the basis function fi (x) is defined on a subset of state variables Xi . Therefore, we
may conclude that EP (x) [fi (x)] = EP (xi ) [fi (xi )], where P (xi ) denotes a factored distribution
on a lower dimensional space Xi . If no further assumptions are made, the local expectation
term EP (xi ) [fi (xi )] may be still hard to compute. Although it can be estimated by a variety
of numerical methods, for instance Monte Carlo (Andrieu et al., 2003), these techniques are
imprecise if the sample size is small, and quite computationally expensive if a high precision
is needed. Consequently, we try to avoid such an approximation step. Instead, we introduce
an appropriate form of basis functions that leads to closed-form solutions to the expectation
term EP (xi ) [fi (xi )].
In particular, let us assume that every basis function fi (xi ) factors as:
fi (xi ) = fiD (xiD )fiC (xiC )

(21)

along its discrete and continuous components fiD (xiD ) and fiC (xiC ), where the continuous
component further decouples as a product:
Y
fiC (xiC ) =
fij (xj )
(22)
Xj ∈XiC

of univariate basis function factors fij (xj ). Note that the basis functions remain multivariate
despite the two independence assumptions. We make these presumptions for computational
purposes and they are relaxed later in the section.
Based on Equation 21, we conclude that the expectation term:
EP (xi ) [fi (xi )] = EP (xi ) [fiD (xiD )fiC (xiC )]
= EP (xi

D

) [fiD (xiD )] EP (xiC ) [fiC (xiC )]

(23)

decomposes along the discrete and continuous variables XiD and XiC , where xi = (xiD , xiC )
and P (xi ) = P (xiD )P (xiC ). The evaluation of the discrete part EP (xi ) [fiD (xiD )] requires
D
aggregation in the subspace XiD :
X
(24)
P (xiD )fiD (xiD ),
EP (xi ) [fiD (xiD )] =
D

xiD

170

Solving Factored MDPs with Hybrid State and Action Variables

Probability density

fpoly (x′2 )

fpwl (x′2 )

fbeta (x′2 )

4

4

4

3

3

3

2

2

2

1

1

1

0

0

0.5
X′2

1

0

0

0.5
X′2

1

0

0

0.5
X′2

1

Figure 5: Expectation of three basis functions f (x′2 ) (Example 5) with respect to the transition function P (X2′ | X2 = 1, X1 = 0, a = a1 ) from Figure 3. Every basis function
f (x′2 ) is depicted by a thick black line. The transition function is shown in a light
gray color. Darker gray lines represent the values of the product P (x′2 | x, a1 )f (x′2 ).
The area below corresponds to the expectation terms EP (x′2 |x,a1 ) [f (x′2 )].
Q
which can be carried out efficiently in O( Xj ∈Xi |Dom(Xj )|) time (Section 3.3). Following
D
Equation 22, the continuous term EP (xi ) [fiC (xiC )] decouples as a product:
C



EP (xi ) [fiC (xiC )] = EP (xi ) 
C

C

=

Y

Y

Xj ∈XiC



fij (xj )

EP (xj ) [fij (xj )] ,

(25)

Xj ∈XiC

where EP (xj ) [fij (xj )] represents the expectation terms over individual random variables Xj .
Consequently, an efficient solution to the local expectation term EP (xi ) [fi (xi )] is guaranteed
by efficient solutions to its univariate components EP (xj ) [fij (xj )].
In this paper, we consider three univariate basis function factors fij (xj ): piecewise linear
functions, polynomials, and beta distributions. These factors support a very general class of
basis functions and yet allow closed-form solutions to the expectation terms EP (xj ) [fij (xj )].
These solutions are provided in the following propositions and demonstrated in Example 5.
Proposition 3 (Polynomial basis functions) Let:
P (x) = Pbeta (x | α, β)
be a beta distribution over X and:
f (x) = xn (1 − x)m
be a polynomial in x and (1 − x). Then EP (x) [f (x)] has a closed-form solution:
EP (x) [f (x)] =

Γ(α + β) Γ(α + n)Γ(β + m)
.
Γ(α)Γ(β) Γ(α + β + n + m)
171

Kveton, Hauskrecht, & Guestrin

Corollary 1 (Beta basis functions) Let:
P (x) = Pbeta (x | α, β)
f (x) = Pbeta (x | αf , βf )
be beta distributions over X. Then EP (x) [f (x)] has a closed-form solution:
EP (x) [f (x)] =

Γ(α + β) Γ(αf + βf ) Γ(α + αf − 1)Γ(β + βf − 1)
.
Γ(α)Γ(β) Γ(αf )Γ(βf ) Γ(α + αf + β + βf − 2)

Proof: A direct consequence of Proposition 3. Since integration is a distributive operation,
our claim straightforwardly generalizes to the mixture of beta distributions P (x). ¤
Proposition 4 (Piecewise linear basis functions) Let:
P (x) = Pbeta (x | α, β)
be a beta distribution over X and:
f (x) =

X

1[li ,ri ] (x)(ai x + bi )

i

be a piecewise linear (PWL) function in x, where 1[li ,ri ] (x) represents the indicator function
of the interval [li , ri ]. Then EP (x) [f (x)] has a closed-form solution:
¸
X·
α
+
+
(F (ri ) − F (li )) + bi (F (ri ) − F (li )) ,
ai
EP (x) [f (x)] =
α+β
i

where F (u) = Fbeta (u | α, β) and F + (u) = Fbeta (u | α + 1, β) denote the cumulative density
functions of beta distributions.
Example 5 Efficient closed-form solutions to the expectation terms in HALP are illustrated
on the 4-ring network administration problem (Example 4) with three hypothetical univariate
basis functions:
fpoly (x′2 ) = x′4
2
fbeta (x′2 ) = Pbeta (x′2 | 2, 6)
fpwl (x′2 ) = 1[0.3,0.5] (x′2 )(5x′2 − 1.5) + 1[0.5,0.7] (x′2 )(−5x′2 + 3.5)
Suppose that our goal is to evaluate expectation terms in a single constraint that corresponds
to the network state x = (0, 1, 0, 0) and the administrator rebooting the server. Based on these
assumptions, the expectation terms in the constraint (x, a1 ) simplify as:
¤
£
£
¤
EP (x′ |x,a1 ) f (x′2 ) = EP (x′2 |x,a1 ) f (x′2 ) ,
where the transition function P (x′2 | x, a1 ) is given by:

P (x′2 | x, a1 ) = P (X2′ = x′2 | X2 = 1, X1 = 0, a = a1 )
= Pbeta (x′2 | 15, 8).
172

Solving Factored MDPs with Hybrid State and Action Variables

Closed-form solutions to the simplified expectation terms EP (x′2 |x,a1 ) [f (x′2 )] are computed as:
Z
£
¤
′
′
Pbeta (x′2 | 15, 8)x′4
EP (x′2 |x,a1 ) fpoly (x2 ) =
2 dx2
x′2

Γ(15 + 8) Γ(15 + 4)Γ(8)
Γ(15)Γ(8) Γ(15 + 8 + 4)
≈ 0.20
Z
£
¤
Pbeta (x′2 | 15, 8)Pbeta (x′2 | 2, 6) dx′2
EP (x′2 |x,a1 ) fbeta (x′2 ) =
(Proposition 3)

=

x′2

Γ(15 + 8) Γ(2 + 6) Γ(15 + 2 − 1)Γ(8 + 6 − 1)
Γ(15)Γ(8) Γ(2)Γ(6) Γ(15 + 2 + 8 + 6 − 2)
≈ 0.22
Z
£
¤
′
Pbeta (x′2 | 15, 8)1[0.3,0.5] (x′2 )(5x′2 − 1.5) dx′2 +
EP (x′2 |x,a1 ) fpwl (x2 ) =
(Corollary 1)

=

x′2

Z

x′2

(Proposition 4)

Pbeta (x′2 | 15, 8)1[0.5,0.7] (x′2 )(−5x′2 + 3.5) dx′2

15
(F + (0.5) − F + (0.3)) − 1.5(F (0.5) − F (0.3))−
15 + 8
15
(F + (0.7) − F + (0.5)) + 3.5(F (0.7) − F (0.5))
5
15 + 8
≈ 0.30

= 5

where F (u) = Fbeta (u | 15, 8) and F + (u) = Fbeta (u | 15+1, 8) denote the cumulative density
functions of beta distributions. A graphical interpretation of these computations is presented
in Figure 5. Brief inspection verifies that the term EP (x′2 |x,a1 ) [fpwl (x′2 )] is indeed the largest
one.
Up to this point, we obtained efficient closed-form solutions for factored basis functions and
state relevance densities. Unfortunately, the factorization assumptions in Equations 20, 21,
and 22 are rarely justified in practice. In the rest of the section, we show how to relax them.
In Section 6, we apply our current results and propose several methods that approximately
satisfy the constraint space in HALP.
5.2.1 Factored State Relevance Density Functions
Note that the state relevance density function ψ(x) is very unlikely to be completely factored
(Section 5.1). Therefore, the independence assumption in Equation 20 is extremely
P limiting.
ω
To relax this assumption, we approximate ψ(x)
Q by a linear combination ψ (x) = ℓ ωℓ ψℓ (x)
of factored state relevance densities ψℓ (x) = ni=1 ψℓi (xi ). As a result, the expectation terms
in the objective function decompose as:
Eψω (x) [fi (x)] = EP ℓ ωℓ ψℓ (x) [fi (x)]
X
ωℓ Eψℓ (x) [fi (x)] ,
=
ℓ

173

(26)

Kveton, Hauskrecht, & Guestrin

where the factored terms Eψℓ (x) [fi (x)] can be evaluated efficiently (Equation 23). Moreover,
if we assume the factored densities ψℓ (x) are polynomials, their linear combination ψ ω (x) is a
polynomial. Due to the Weierstrass approximation theorem (Jeffreys & Jeffreys, 1988), this
polynomial is sufficient to approximate any state relevance density ψ(x) with any precision.
It follows that the linear combinations permit state relevance densities that reflect arbitrary
dependencies among the state variables X.
5.2.2 Factored Basis Functions
P
In line with the previous discussion, note that the linear value function V w (x) = i wi fi (x)
with factored basis functions (Equations 21 and 22) is sufficient to approximate the optimal
value function V ∗ within any max-norm error kV ∗ − V w k∞ . Based on Theorem
know
°
° ∗ 2, wew
e°
°
that the same set of basis functions guarantees a bound on the L1 -norm error V − V 1,ψ .
Therefore, despite our independence assumptions (Equations 21 and 22), we have a potential
e
to obtain an arbitrarily close HALP approximation V w
to V ∗ .

6. Constraint Space Approximations
e to the HALP formulation (16) is determined by a finite set of active
An optimal solution w
constraints at a vertex of the feasible region. Unfortunately, identification of this active set
is a hard computational problem. In particular, it requires searching through an exponential
number of constraints, if the state and action variables are discrete, and an infinite number
of constraints, if any of the variables are continuous. As a result, it is in general infeasible to
e to the HALP formulation. Hence, we resort to approximations
find the optimal solution w
b is close to w.
e This notion of an
to the constraint space in HALP whose optimal solution w
approximation is formalized as follows.
Definition 2 The HALP formulation is relaxed:
X
wi αi
minimizew

(27)

i

subject to:

X

wi Fi (x, a) − R(x, a) ≥ 0

(x, a) ∈ C;

i

if only a subset C of its constraints is satisfied.
The HALP formulation (16) can be solved approximately by solving its relaxed formulations
(27). Several methods for building and solving these approximate LPs have been proposed:
Monte Carlo sampling of constraints, (Hauskrecht & Kveton, 2004), ε-grid discretization of
the constraint space (Guestrin et al., 2004), and an adaptive search for a violated constraint
(Kveton & Hauskrecht, 2005). In the remainder of this section, we introduce these methods.
From now on, we denote optimal solutions to the complete and relaxed HALP formulations
e and w,
b respectively.
by the symbols w
e
Before we proceed, note that while V w
is an upper bound on the optimal value function
b
∗
V (Figure 6a), the relaxed value function V w
does not have to be (Figure 6b). The reason
b
b
is that the relaxed HALP formulation does not guarantee that the constraint V w
≥ T ∗V w
is
b
w
satisfied for all states x. As a result, we cannot simply use Proposition 1 to prove V ≥ V ∗ .
174

Solving Factored MDPs with Hybrid State and Action Variables

e
Vw

V∗

0
1

1
X1
0 0

1

b
Vw

V∗

0
1

1
X1

X2

e and w
b
w
Objective value

1

b
V∗ and Vw

Value function

Value function

e
V∗ and Vw

(a)

0 0

X2

1

e
w

b
w
0
1

1
w1
0 0

(b)

w2

(c)

Figure 6: a. Graphical relation between the value function V ∗ and its HALP approximation
e
e
Vw
. The function V w
is guaranteed to be an upper bound on V ∗ . b. The relaxed
b
HALP approximation V w
may not lead to an upper bound. c. Graphical relation
e and w.
b The feasible regions of the
between the optimal and relaxed solutions w
complete and relaxed HALP formulations are shown in dark and light gray colors.
e
b
The value function approximations V w
and V w
are typically nonlinear in the state
space X but always linear in the space of parameters w.
£ b¤
£ e¤
Furthermore, note that the inequality Eψ V w
≤ Eψ V w
always holds because the optimal
e is feasible in the relaxed HALP (Figure 6c). These observations become helpful
solution w
for understanding the rest of the section.
6.1 MC-HALP
In the simplest case, the constraint space in HALP can be approximated by its Monte Carlo
(MC) sample. In such a relaxation, the set of constraints C is selected with respect to some
proposal distribution ϕ over state-action pairs (x, a). Since the set C is finite, it establishes
a relaxed formulation (27), which can be solved by any LP solver. An algorithm that builds
and satisfies relaxed MC-HALP formulations is outlined in Figure 7.
Constraint sampling is easily applied in continuous domains and its space complexity is
proportional to the number of state and action components. Hauskrecht and Kveton (2004)
used it to solve continuous-state factored MDPs and further refined it by heuristics (Kveton
& Hauskrecht, 2004). In discrete-state domains, the quality of the sampled approximations
was analyzed by de Farias and Van Roy (2004). Their result is summarized by Theorem 5.
e be a solution to the ALP formulation
Theorem 5 (de Farias & Van Roy, 2004) Let w
b be a solution to its relaxed formulation whose constraints are sampled with respect
(6) and w
to a proposal distribution ϕ over state-action pairs (x, a). Then there exist a distribution ϕ
and sample size:
N ≥O

µ

Aθ
(1 − γ)ǫ

µ
K ln
175

1
Aθ
+ ln
(1 − γ)ǫ
δ

¶¶

Kveton, Hauskrecht, & Guestrin

Inputs:
a hybrid factored MDP M = (X, A, P, R)
basis functions f0 (x), f1 (x), f2 (x), . . .
a proposal distribution ϕ
Algorithm:
initialize a relaxed HALP formulation with an empty set of constraints
t=0
while a stopping criterion is not met
sample (x, a) ∼ ϕ
add the constraint (x, a) to the relaxed HALP
t=t+1
solve the relaxed MC-HALP formulation
Outputs:
basis function weights w

Figure 7: Pseudo-code implementation of the MC-HALP solver.
such that with probability at least 1 − δ:
°
°
°
°
°
° ∗
e°
b°
°
° ≤ °V ∗ − V w
°V − V w
1,ψ

1,ψ

+ ǫ kV ∗ k1,ψ ,

where k·k1,ψ is an L1 -norm weighted by the state relevance weights ψ, θ is a problem-specific
constant, A and K denote the numbers of actions and basis functions, and ǫ and δ are scalars
from the interval (0, 1).
Unfortunately, proposing a sampling distribution ϕ that guarantees this polynomial bound
on the sample size is as hard as knowing the optimal policy π ∗ (de Farias & Van Roy, 2004).
This conclusion is parallel to those in importance sampling. Note that uniform Monte Carlo
sampling can guarantee a low probability of constraints being violated but it is not sufficient
to bound the magnitude of their violation (de Farias & Van Roy, 2004).
6.2 ε-HALP
Another way of approximating the constraint space in HALP is by discretizing its continuous
variables XC and AC on a uniform ε-grid. The new discretized constraint space preserves its
original factored structure but spans discrete variables only. Therefore, it can be compactly
satisfied by the methods for discrete-state ALP (Section 3.3). An algorithm that builds and
satisfies relaxed ε-HALP formulations is outlined in Figure 8. Note that the new constraint
space involves exponentially many constraints O(⌈1/ε + 1⌉|XC |+|AC | ) in the number of state
and action variables XC and AC .
6.2.1 Error Bounds
Recall that the ε-HALP formulation approximates the constraint space in HALP by a finite
set of equally-spaced grid points. In this section, we study the quality of this approximation
176

Solving Factored MDPs with Hybrid State and Action Variables

Inputs:
a hybrid factored MDP M = (X, A, P, R)
basis functions f0 (x), f1 (x), f2 (x), . . .
grid resolution ε
Algorithm:
discretize continuous variables XC and AC into ⌈1/ε + 1⌉ equally-spaced values
identify subsets Xi and Ai (Xj and Aj ) corresponding to the domains of Fi (x, a) (Rj (x, a))
evaluate Fi (xi , ai ) (Rj (xj , aj )) for all configurations xi and ai (xj and aj ) on the ε-grid
calculate basis function relevance weights αi
solve the relaxed ε-HALP formulation (Section 3.3)
Outputs:
basis function weights w

Figure 8: Pseudo-code implementation of the ε-HALP solver.
and bound it in terms violating constraints in the complete HALP. More precisely, we prove
b violates the constraints in the complete HALP by a small
that if a relaxed HALP solution w
b
e
amount, the quality of the approximation V w
is close to V w
. In the next section, we extend
b
w
this result and relate V to the grid resolution ε. Before we proceed, we quantify our notion
of constraint violation.
b be an optimal solution to a relaxed HALP formulation (27). The vector
Definition 3 Let w
b is δ-infeasible if:
w
b
b
Vw
− T ∗V w
≥ −δ,

(28)

where T ∗ is the hybrid Bellman operator.

b the closer the quality
Intuitively, the lower the δ-infeasibility of a relaxed HALP solution w,
b
e
of the approximation V w
to V w
. Proposition 5 states this intuition formally. In particular,
b
it says that the relaxed HALP formulation leads to a close approximation V w
to the optimal
∗
b violates its constraints by
value function V if the complete HALP does and the solution w
a small amount.
e be an optimal solution to the HALP formulation (16) and w
b be an
Proposition 5 Let w
optimal δ-infeasible solution to its relaxed formulation (27). Then the expected error of the
b
value function V w
can be bounded as:
°
°
°
°
2δ
° ∗
°
b°
e°
,
°V − V w
° ≤ °V ∗ − V w
° +
1−γ
1,ψ
1,ψ
where k·k1,ψ is an L1 -norm weighted by the state relevance density function ψ.

Based on Proposition 5, we can generalize our conclusions from Section 5.1 to relaxed HALP
formulations.
For instance, we may draw a parallel between optimizing the relaxed objective
£ b¤
Eψ V w
and the max-norm error kV ∗ − V w k∞,1/L .
177

Kveton, Hauskrecht, & Guestrin

b be an optimal δ-infeasible solution to a relaxed HALP formulation (27).
Theorem 6 Let w
b
Then the expected error of the value function V w
can be bounded as:
°
°
° ∗
b°
°
°V − V w

1,ψ

≤

2Eψ [L]
2δ
min kV ∗ − V w k∞,1/L +
,
w
1−κ
1−γ

P
where k·k1,ψ is an L1 -norm weighted by the state relevance density ψ, L(x) = i wiL fi (x) is a
Lyapunov function such that the inequality κL(x) ≥ γ supa EP (x′ |x,a) [L(x′ )] holds, κ ∈ [0, 1)
denotes its contraction factor, and k·k∞,1/L is a max-norm reweighted by the reciprocal 1/L.
Proof: Direct combination of Theorem 3 and Proposition 5. ¤
6.2.2 Grid Resolution
In Section 6.2.1, we bounded the error of a relaxed HALP formulation by its δ-infeasibility
(Theorem 6), a measure of constraint violation in the complete HALP. However, it is unclear
how the grid resolution ε relates to δ-infeasibility. In this section, we analyze the relationship
between ε and δ. Moreover, we show how to exploit the factored structure in the constraint
b efficiently.
space to achieve the δ-infeasibility of a relaxed HALP solution w
b is an optimal δ-infeasible solution to an ε-HALP formulation
First, let us assume that w
and Z = X∪A is the joint set of state and action variables. To derive a bound
P relating both
b
w
bi Fi (z) − R(z)
ε and δ, we assume that the magnitudes of constraint violations τ (z) = i w
are Lipschitz continuous.
Definition 4 The function f (x) is Lipschitz continuous if:
¯
¯
°
°
¯f (x) − f (x′ )¯ ≤ K °x − x′ °
∀ x, x′ ∈ X;
∞

(29)

where K is referred to as a Lipschitz constant.

Based on the ε-grid discretization of the constraint space, we know that the distance of any
point z to its closest grid point zG = arg minz′ kz − z′ k∞ is bounded as:
ε
kz − zG k∞ < .
2

(30)

b
From the Lipschitz continuity of τ w
(z), we conclude:

¯
¯
Kε
¯ w
¯
b
.
(z)¯ ≤ K kzG − zk∞ ≤
¯τ b (zG ) − τ w
2

(31)

b
Since every constraint in the relaxed ε-HALP formulation is satisfied, τ w
(zG ) is nonnegative
b
for all grid points zG . As a result, Equation 31 yields τ w
(z) > −Kε/2 for every state-action
b is δ-infeasible for δ ≥ Kε/2.
pair z = (x, a). Therefore, based on Definition 3, the solution w
b is guaranteed by choosing ε ≤ 2δ/K.
Conversely, the δ-infeasibility of w
Unfortunately, K may increase rapidly with the dimensionality of a function. To address
this issue, we use the structure in the constraint space and demonstrate that this is not our
case. First, we observe that the global Lipschitz constant Kglob is additive in local Lipschitz
constants that correspond to the terms w
bi Fi (z) and Rj (z). Moreover, Kglob ≤ N Kloc , where

178

Solving Factored MDPs with Hybrid State and Action Variables

Inputs:
a hybrid factored MDP M = (X, A, P, R)
basis functions f0 (x), f1 (x), f2 (x), . . .
initial basis function weights w(0)
a separation oracle O
Algorithm:
initialize a relaxed HALP formulation with an empty set of constraints
t=0
while a stopping criterion is not met
query the oracle O for a violated constraint (xO , aO ) with respect to w(t)
if the constraint (xO , aO ) is violated
add the constraint to the relaxed HALP
resolve the LP for a new vector w(t+1)
t=t+1
Outputs:
basis function weights w(t)

Figure 9: Pseudo-code implementation of a HALP solver with the cutting plane method.
N denotes the total number of the terms and Kloc is the maximum over the local constants.
b is achieved
Finally, parallel to Equation 31, the δ-infeasibility of a relaxed HALP solution w
by the discretization:
ε≤

2δ
2δ
≤
.
N Kloc
Kglob

(32)

Since the factors w
bi Fi (z) and Rj (z) are often restricted to small subsets of state and action
variables, Kloc should change a little when the size of a problem increases but its structure is
fixed. To prove that Kloc is bounded, we have to bound the weights w
bi . If all basis functions
are of unit magnitude, the weights w
bi are intuitively bounded as |w
bi | ≤ (1−γ)−1 Rmax , where
Rmax denotes the maximum one-step reward in the HMDP.
Based on Equation 32, we conclude that the number of discretization points in a single
dimension ⌈1/ε + 1⌉ is bounded by a polynomial in N , Kloc , and 1/δ. Hence, the constraint
space in the relaxed ε-HALP formulation involves O([N Kloc (1/δ)]|X|+|A| ) constraints, where
|X| and |A| denote the number of state and action variables. The idea of variable elimination
can be used to write the constraints compactly by O([N Kloc (1/δ)]T +1 (|X|+|A|)) constraints
(Example 3), where T is the treewidth of a corresponding cost network. Therefore, satisfying
this constraint space is polynomial in N , Kloc , 1/δ, |X|, and |A|, but still exponential in T .
6.3 Cutting Plane Method
Both MC and ε-HALP formulations (Sections 6.1 and 6.2) approximate the constraint space
in HALP by a finite set of constraints C. Therefore, they can be solved directly by any linear
programming solver. However, if the number of constraints is large, formulating and solving
179

Kveton, Hauskrecht, & Guestrin

Inputs:
a hybrid factored MDP M = (X, A, P, R)
basis functions f0 (x), f1 (x), f2 (x), . . .
basis function weights w
grid resolution ε
Algorithm:
discretize continuous variables XC and AC into (⌈1/ε + 1⌉) equally-spaced values
identify subsets Xi and Ai (Xj and Aj ) corresponding to the domains of Fi (x, a) (Rj (x, a))
evaluate Fi (xi , ai ) (Rj (xj , aj )) for all configurations xi and ai (xj and aj ) on the ε-grid
build a cost network
P for the factored cost function:
τ w (x, a) = i wi Fi (x, a) − R(x, a)
find the most violated constraint in the cost network:
(xO , aO ) = arg minx,a τ w (x, a)
Outputs:
state-action pair (xO , aO )

Figure 10: Pseudo-code implementation of the ε-HALP separation oracle Oε .
LPs with the complete set of constraints is infeasible. In this section, we show how to build
relaxed HALP approximations efficiently by the cutting plane method.
The cutting plane method for solving HALP formulations is outlined in Figure 9. Briefly,
this approach builds the set of LP constraints incrementally by adding a violated constraint
to this set in every step. In the remainder of the paper, we refer to any method that returns
b as a separation oracle. Formally, every HALP
a violated constraint for an arbitrary vector w
oracle approaches the optimization problem:
h
i
i
h
b
b
arg min V w
(x) − γEP (x′ |x,a) V w
(x′ ) − R(x, a) .
(33)
x,a

Consequently, the problem of solving hybrid factored MDPs efficiently reduces to the design
of efficient separation oracles. Note that the cutting plane method (Figure 9) can be applied
to suboptimal solutions to Equation 33 if these correspond to violated constraints.
The presented approach can be directly used to satisfy the constraints in relaxed ε-HALP
formulations (Schuurmans & Patrascu, 2002). Briefly, the solver from Figure 9 iterates until
no violated constraint is found and the ε-HALP separation oracle Oε (Figure 10) returns the
most violated constraint in the discretized cost network given an intermediate solution w(t) .
Note that although the search for the most violated constraint is polynomial in |X| and |A|
(Section 6.2.2), the running time of our solver does not have to be (Guestrin, 2003). In fact,
the number of generated cuts is exponential in |X| and |A| in the worst case. However, the
same oracle embedded into the ellipsoid method (Khachiyan, 1979) yields a polynomial-time
algorithm (Bertsimas & Tsitsiklis, 1997). Although this technique is impractical for solving
large LPs, we may conclude that our approach is indeed polynomial-time if implemented in
this particular way.
Finally, note that searching for the most violated constraint (Equation 33) has application beyond satisfying the constraint space in HALP. For instance, computation of a greedy
180

Solving Factored MDPs with Hybrid State and Action Variables

b
policy for the value function V w
:

h
ii
h
b
u(x) = arg max R(x, a) + γEP (x′ |x,a) V w
(x′ )
a
h
ii
h
b
= arg min −R(x, a) − γEP (x′ |x,a) V w
(x′ )
a

(34)

is almost an identical optimization problem, where the state variables X are fixed. Moreover,
the magnitude of the most violated constraint is equal to the lowest δ for which the relaxed
b is δ-infeasible (Equation 28):
HALP solution w
h
h
h
iii
b
b
δ = min V w
(x) − max R(x, a) + γEP (x′ |x,a) V w
(x′ )
x
a
h
ii
h
b
b
= min V w
(x) − R(x, a) − γEP (x′ |x,a) V w
(x′ ) .
(35)
x,a

6.4 MCMC-HALP
In practice, both MC and ε-HALP formulations (Sections 6.1 and 6.2) are built on a blindly
selected set of constraints C. More specifically, the constraints in the MC-HALP formulation
are chosen randomly (with respect to a prior distribution ϕ) while the ε-HALP formulation
is based on a uniform ε-grid. This discretized constraint space preserves its original factored
structure, which allows for its compact satisfaction. However, the complexity of solving the
ε-HALP formulation is exponential in the treewidth of its discretized constraint space. Note
that if the discretized constraint space is represented by binary variables only, the treewidth
increases by a multiplicative factor of log2 ⌈1/ε + 1⌉, where ⌈1/ε + 1⌉ denotes the number of
discretization points in a single dimension. Consequently, even if the treewidth of a problem
is relatively small, solving its ε-HALP formulation becomes intractable for small values of ε.
To address the issues of the discussed approximations (Sections 6.1 and 6.2), we propose
a novel Markov chain Monte Carlo (MCMC) method for finding the most violated constraint
of a relaxed HALP. The procedure directly operates in the domains of continuous variables,
takes into account the structure of factored MDPs, and its space complexity is proportional
to the number of variables. This separation oracle can be easily embedded into the ellipsoid
or cutting plane method for solving linear programs (Section 6.3), and therefore constitutes
a key step towards solving HALP efficiently. Before we proceed, we represent the constraint
space in HALP compactly and state an optimization problem for finding violated constraints
in this factored representation.
6.4.1 Compact Representation of Constraints
In Section 3.3, we showed how the factored representation of the constraint space allows for
its compact satisfaction. Following this idea, we define violation magnitude τ w (x, a):
£
¤
¤
£
(36)
τ w (x, a) = − V w (x) − γEP (x′ |x,a) V w (x′ ) − R(x, a)
X
= −
wi [fi (x) − γgi (x, a)] + R(x, a),
i

which measures the amount by which the solution w violates the constraints in the complete
HALP. We represent the magnitude of violation τ w (x, a) compactly by an influence diagram
181

Kveton, Hauskrecht, & Guestrin

(ID), where X and A are decision nodes, and X′ are random variables. This representation
is built on the transition model P (X′ | X, A), which is factored and captures independencies
among the variables X, X′ , and A. We extend the diagram by three types of reward nodes,
one for each term in Equation 36: Hi = −wi fi (x) for every basis function, Gi = γwi fi (x′ ) for
every backprojection, and Rj = Rj (xj , aj ) for every local reward function. The construction
is completed by adding arcs that graphically represent the dependencies of the reward nodes
on the variables. Finally, we can verify that:


X
X
τ w (x, a) = EP (x′ |x,a)  (Hi + Gi ) +
Rj  .
(37)
i

j

Consequently, the decision that maximizes the expected utility in the ID corresponds to the
most violated constraint. A graphical representation of the violation magnitude τ w (x, a) on
the 4-ring network administration problem (Example 4) is given in Figure 2a. The structure
of the constraint space is identical to Example 3 if the basis functions are univariate.
We conclude that any algorithm for solving IDs can be applied to find the most violated
constraint. However, most of these methods (Cooper, 1988; Jensen et al., 1994; Ortiz, 2002)
are restricted to discrete variables. Fortunately, special properties of the ID representation
allow its further simplification. If the basis functions are chosen conjugate to the transition
model (Section 5.2), we obtain a closed-form solution to the expectation term EP (x′ |x,a) [Gi ]
(Equation 18), and the random variables X′ are marginalized out of the diagram. The new
representation contains no random variables and is known as a cost network (Section 3.3).
Note that the problem of finding the most violated constraint in the ID representation is
also identical to finding the maximum a posteriori (MAP) configuration of random variables
in Bayesian networks (Dechter, 1996; Park & Darwiche, 2001, 2003; Yuan et al., 2004). The
latter problem is difficult because of the alternating summation and maximization operators.
Since we marginalized out the random variables X′ , we can solve the maximization problem
by standard large-scale optimization techniques.
6.4.2 Separation Oracle OMCMC
To find the most violated constraint in the cost network, we apply the Metropolis-Hastings
(MH) algorithm (Metropolis et al., 1953; Hastings, 1970) and propose a Markov chain whose
invariant distribution converges to the vicinity of arg maxz τ w (z), where z = (x, a) is a value
assignment to the joint set of state and action variables Z = X ∪ A.
In short, the Metropolis-Hastings algorithm defines a Markov chain that transits between
an existing state z and a proposed state z∗ with the acceptance probability:
½
¾
p(z∗ )q(z | z∗ )
∗
A(z, z ) = min 1,
,
(38)
p(z)q(z∗ | z)
where q(z∗ | z) and p(z) are a proposal distribution and a target density, respectively. Under
mild restrictions on p(z) and q(z∗ | z), the frequency of state visits generated by the Markov
chain always converges to the target function p(z) (Andrieu et al., 2003). In the remainder of
this section, we discuss the choices of p(z) and q(z∗ | z) to solve our optimization problem.5
5. For an introduction to Markov chain Monte Carlo (MCMC) methods, refer to the work of Andrieu et al.
(2003).

182

Solving Factored MDPs with Hybrid State and Action Variables

Target density: The violation magnitude τ w (z) is turned into a density by the transformation p(z) = exp[τ w (z)]. Due to its monotonic character, p(z) retains the same set of global
maxima as τ w (z). Therefore, the search for arg maxz τ w (z)
PcanRbe done on the new function
p(z). To prove that p(z) is a density, we demonstrate that zD zC p(z) dzC is a normalizing
constant, where zD and zC are the discrete and continuous parts of the value assignment z.
|ZC | . As a result, the integral
RFirst, note that the integrand zC is restricted to the space [0, 1]
zC p(z) dzC is proper if p(z) is bounded, and hence it is Riemann integrable and finite. To
prove that p(z) = exp[τ w (z)] is bounded, we bound the magnitude of violation τ w (z). If all
basis functions are of unit magnitude, the weights w
bi can be bounded as |w
bi | ≤ (1−γ)−1 Rmax
w
−1
(Section 6.2.2), which in turn yields the bound |τ (z)| ≤ (|w| (1−γ) +1)Rmax . Therefore,
p(z) is bounded and can be treated as a density function.
To find the mode of p(z), we employ simulating annealing (Kirkpatrick et al., 1983) and
generate a non-homogeneous Markov chain whose invariant distribution is equal to p1/Tt (z),
where Tt is a cooling schedule such that limt→∞ Tt = 0. Under weak regularity assumptions
on p(z), p∞ (z) is a probability density that concentrates on the set of the global maxima of
p(z) (Andrieu et al., 2003). If our cooling schedule Tt decreases such that Tt ≥ c/ ln(t + 1),
where c is a problem-specific constant, the chain from Equation 38 converges to the vicinity
of arg maxz τ w (z) with the probability converging to 1 (Geman & Geman, 1984). However,
this logarithmic cooling schedule is slow in practice, especially for a high initial temperature
c. To overcome this problem, we select a smaller value of c (Geman & Geman, 1984) than is
required by the convergence criterion. Therefore, the convergence of our chain to the global
optimum arg maxz τ w (z) is no longer guaranteed.
Proposal distribution: We take advantage of the factored character of Z and adopt the
following proposal distribution (Geman & Geman, 1984):
∗

q(z | z) =

½

p(zi∗ | z−i ) if z∗−i = z−i
,
0
otherwise

(39)

where z−i and z∗−i are value assignments to all variables but Zi in the original and proposed
states. If Zi is a discrete variable, its conditional:
p(z1 , . . . , zi−1 , zi∗ , zi+1 , . . . , zn+m )
p(zi∗ | z−i ) = P
zi p(z1 , . . . , zi−1 , zi , zi+1 , . . . , zn+m )

(40)

can be derived in a closed form. If Zi is a continuous variable, a closed form of its cumulative
density function is unlikely to exist. To sample from the conditional, we embed another MH
step within the original chain. In the experimental section, we use the Metropolis algorithm
with the acceptance probability:
¾
½
p(zi∗ | z−i )
,
A(zi , zi∗ ) = min 1,
p(zi | z−i )

(41)

where zi and zi∗ are the original and proposed values of the variable Zi . Note that sampling
from both conditionals can be performed in the space of τ w (z) and locally.
183

Kveton, Hauskrecht, & Guestrin

Inputs:
a hybrid factored MDP M = (X, A, P, R)
basis functions f0 (x), f1 (x), f2 (x), . . .
basis function weights w
Algorithm:
initialize a state-action pair z(t)
t=0
while a stopping criterion is not met
for every variable Zi
sample u ∼ U[0,1]
(t)
sample zi∗ ∼
i | z−i )
½ p(Z1/T
¾
t −1 (z ∗ |z(t) )
p
i
−i
if u < min 1, 1/Tt −1 (t)
(t)
(zi |z−i )

p

(t+1)

zi

= zi∗

else
(t+1)

(t)

zi
= zi
update Tt+1 according to the cooling schedule
t=t+1
(xO , aO ) = z(t)
Outputs:
state-action pair (xO , aO )

Figure 11: Pseudo-code implementation of the MCMC-HALP oracle OMCMC . The symbol
U[0,1] denotes the uniform distribution on the interval [0, 1]. Since the testing for
violated constraints (Figure 9) is inexpensive, our implementation of the MCMCHALP solver in Section 7 tests all constraints z(t) generated by the Markov chain
and not only the last one. Therefore, the separation oracle OMCMC returns more
than one constraint per chain.

Finally, by assuming that z∗−i = z−i (Equation 39), we derive a non-homogenous Markov
chain with the acceptance probability:
∗

A(z, z ) =
=
=
=

(

)
p1/Tt (z∗ )q(z | z∗ )
min 1, 1/T
p t (z)q(z∗ | z)
(
p1/Tt (z ∗ | z∗−i )p1/Tt (z∗−i )p(zi
min 1, 1/T i
p t (zi | z−i )p1/Tt (z−i )p(zi∗
(
p1/Tt (z ∗ | z−i )p1/Tt (z−i )p(zi
min 1, 1/T i
p t (zi | z−i )p1/Tt (z−i )p(zi∗
)
(
p1/Tt −1 (zi∗ | z−i )
,
min 1, 1/T −1
p t (zi | z−i )
184

| z∗−i )
| z−i )
| z−i )
| z−i )

)
)
(42)

Solving Factored MDPs with Hybrid State and Action Variables

which converges to the vicinity of the most violated constraint. Yuan et al. (2004) proposed
a similar chain for finding the MAP configuration of random variables in Bayesian networks.
6.4.3 Constraint Satisfaction
If the MCMC-HALP separation oracle OMCMC (Figure 11) converges to a violated constraint
(not necessarily the most violated) in polynomial time, the ellipsoid method is guaranteed to
solve HALP formulations in polynomial time (Bertsimas & Tsitsiklis, 1997). Unfortunately,
convergence of our chain within arbitrary precision requires an exponential number of steps
(Geman & Geman, 1984). Although the bound is loose to be of practical interest, it suggests
that the time complexity of proposing violated constraints dominates the time complexity of
solving relaxed HALP formulations. Therefore, the oracle OMCMC should search for violated
constraints efficiently. Convergence speedups that directly apply to our work include hybrid
Monte Carlo (HMC) (Duane et al., 1987), Rao-Blackwellization (Casella & Robert, 1996),
and slice sampling (Higdon, 1998).

7. Experiments
Experimental section is divided in three parts. First, we show that HALP can solve a simple
HMDP problem at least as efficiently as alternative approaches. Second, we demonstrate the
scale-up potential of our framework and compare several approaches to satisfy the constraint
space in HALP (Section 6). Finally, we argue for solving our constraint satisfaction problem
in the domains of continuous variables without discretizing them.
All experiments are performed on a Dell Precision 380 workstation with 3.2GHz Pentium
4 CPU and 2GB RAM. Linear programs are solved by the simplex method in the LP SOLVE
package. The expected return of policies is estimated by the Monte Carlo simulation of 100
trajectories. The results of randomized methods are additionally averaged over 10 randomly
initialized runs. Whenever necessary, we present errors on the expected values. These errors
correspond to the standard deviations of measured quantities. The discount factor γ is 0.95.
7.1 A Simple Example
To illustrate the ability of HALP to solve factored MDPs, we compare it to L2 (Figure 4) and
grid-based value iteration (Section 4.2) on the 4-ring topology of the network administration
problem (Example 4). Our experiments are conducted on uniform and non-uniform grids of
varying sizes. Grid points are kept fixed for all compared methods, which allows for their fair
comparison. Both value iteration methods are iterated for 100 steps and terminated earlier
if their Bellman error drops below 10−6 . Both the L2 and HALP methods approximate the
optimal value function V ∗ by a linear combination of basis functions, one for each computer
Xi (fi (x) = xi ), and one for every connection Xi → Xj in the ring topology (fi→j (x) = xi xj ).
We assume that our basis functions are sufficient to derive a one-step lookahead policy that
reboots the least efficient computer. We believe that such a policy is close-to-optimal in the
ring topology. The constraint space in the complete HALP formulation is approximated by
its MC-HALP and ε-HALP formulations (Sections 6.1 and 6.2). The state relevance density
function ψ(x) is uniform. Our experimental results are reported in Figure 12.
185

Kveton, Hauskrecht, & Guestrin

N
8
91
625
561

ε-HALP
Reward Time
52.1 ± 2.2
<1
52.1 ± 2.2
<1
52.1 ± 2.2
<1
52.1 ± 2.2
2

Uniform ε-grid
L2 VI
Reward Time
52.1 ± 2.2
2
52.1 ± 2.2
7
52.1 ± 2.2
55
52.1 ± 2.2
577

47.6 ± 2.2
<1
51.5 ± 2.2
20
52.0 ± 2.3 2 216

N
10
50
250
1 250

MC-HALP
Reward Time
45.2 ± 5.1
<1
50.2 ± 2.4
<1
51.5 ± 2.4
<1
51.8 ± 2.3
<1

Non-uniform grid
L2 VI
Reward Time
45.9 ± 5.8
1
51.8 ± 2.2
4
51.9 ± 2.2
22
51.9 ± 2.2
110

Grid-based VI
Reward Time
47.5 ± 2.8
<1
48.7 ± 2.5
<1
50.4 ± 2.3
2
51.6 ± 2.2
60

ε
1
1/2
1/4
1/8 6

Heuristics
Policy
Reward
Dummy 25.0 ± 2.8
Random 42.1 ± 3.3
Server
47.6 ± 2.2
Utopian 83.0

Grid-based VI
Reward Time

Figure 12: Comparison of three approaches to solving hybrid MDPs on the 4-ring topology
of the network administration problem (Example 4). The methods are compared
on uniform and non-uniform grids of varying size (N ) by the expected discounted
reward of policies and their computation time (in seconds).

To verify that our solutions are non-trivial, we compare them to three heuristic policies:
dummy, random, and server. The dummy policy πdummy (x) = a5 always takes the dummy
action a5 . Therefore, it establishes a lower bound on the performance of any administrator.
The random policy behaves randomly. The server policy πserver (x) = a1 protects the server
X1 . The performance of our heuristics is shown in Figure 12. Assuming that we can reboot
all computers at each time step, a utopian upper bound on the performance of any policy π
can be derived as:
"∞
#
·
¸
X
1
t
′ ′
Eπ
γ R(xt , π(xt )) ≤
max R(x , a )
max E ′
1 − γ x,a P (x |x,a) a′
t=0
Z
4
X
1
′
=
P (x′j | x, a)x′2
2P (x′1 | x, a)x′2
+
max
j dx
1
1 − γ x,a x′
j=2
Z
5
Pbeta (x′ | 20, 2)x′2 dx′
≤
1 − γ x′
≈ 83.0.
(43)
We do not analyze the quality of HALP solutions with respect to the optimal value function
V ∗ (Section 5.1) because this one is unknown.
Based on our results, we draw the following conclusions. First, grid-based value iteration
is not practical for solving hybrid optimization problems of even small size. The main reason
is the space complexity of the method, which is quadratic in the number of grid points N .
If the state space is discretized uniformly, N is exponential in the number of state variables.
Second, the quality of the HALP policies is close to the L2 VI policies. This result is positive
since L2 value iteration is commonly applied in approximate dynamic programming. Third,
186

Solving Factored MDPs with Hybrid State and Action Variables

both the L2 and HALP approaches yield better policies than grid-based value iteration. This
result is due to the quality of our value function estimator. Its extremely good performance
for ε = 1 can be explained from the monotonicity of the reward and basis functions. Finally,
the computation time of the L2 VI policies is significantly longer than the computation time
of the HALP policies. Since a step of L2 value iteration (Figure 4) is as hard as formulating
a corresponding relaxed HALP, this result comes at no surprise.
7.2 Scale-up Potential
To illustrate the scale-up potential of HALP, we apply three relaxed HALP approximations
(Section 6) to solve two irrigation network problems of varying complexity. These problems
are challenging for state-of-the-art MDP solvers due to the factored state and action spaces.
Example 6 (Irrigation network operator) An irrigation network is a system of irrigation channels connected by regulation devices (Figure 13). The goal of an irrigation network
operator is to route water between the channels to optimize water levels in the whole system.
The optimal levels are determined by the type of a planted crop. For simplicity of exposition,
we assume that all irrigation channels are oriented and of the same size.
This optimization problem can be formulated as a factored MDP. The state of the network
is completely observable and represented by n continuous variables X = {X1 , . . . , Xn }, where
the variable Xi denotes the water level in the i-th channel. At each time step, the irrigation
network operator regulates m devices Ai that pump water between every pair of their inbound
and outbound channels. The operation modes of these devices are described by discrete action
variables A = {A1 , . . . , Am }. Inflow and outflow devices (no inbound or outbound channels)
are not controlled and just pump water in and out of the network.
The transition model reflects water flows in the irrigation network and is encoded locally
by conditioning on the operation modes A:
′
′
P (Xi→j
= x | Par(Xi→j
)) ∝ Pbeta (x | α, β)

µ′i→j = µi→j +

X

α = 46µ′i→j + 2
β = 46(1 − µ′i→j ) + 2

1ah→i→j (Ai ) min(1 − µi→j , min(xh→i , τi ))

h

µi→j = xi→j −

X

1ai→j→k (Aj ) min(xi→j , τj )

k

where Xi→j represents the water level between the regulation devices Ai and Aj , 1ah→i→j (Ai )
and 1ai→j→k (Aj ) denote the indicator functions of water routing actions ah→i→j and ai→j→k
at the devices Ai and Aj , and τi and τj are the highest tolerated flows through these devices.
In short, this transition model conserves water mass in the network and adds some variance
′
to the resulting state Xi→j
. The introduced indexing of state and action variables is explained
on the 6-ring irrigation network in Figure 14a. In the rest of the paper, we assume an inflow
of 0.1 to any inflow device Ai (τi = 0.1), an outflow of 1 from any outflow device Aj (τj = 1),
and the highest tolerated flow of 1/3
P at the remaining devices Ak (τk = 1/3).
The reward function R(x, a) = j Rj (xj ) is factored along individual irrigation channels
and described by the univariate function:
Rj (xj ) = 2xj
187

Kveton, Hauskrecht, & Guestrin

(a)

(b)

(c)

Figure 13: Illustrations of three irrigation network topologies: a. 6-ring, b. 6-ring-of-rings,
and c. 3×3 grid. Irrigation channels and their regulation devices are represented
by arrows and rectangles. Inflow and outflow nodes are colored in light and dark
gray. The ring and ring-of-rings networks are parameterized by the total number
of regulation devices except for the last four (n).

for each outflow channel (one of its regulation devices must be outflow), and by the function:
Rj (xj ) =

N (xj | 0.4, 0.025) N (xj | 0.55, 0.05)
+
25.6
32

for the remaining channels (Figure 14b). Therefore, we reward both for maintaining optimal
water levels and pumping water out of the irrigation network. Several examples of irrigation
network topologies are shown in Figure 13.
Similarly to Equation 43, we derive a utopian upper bound on the performance of any policy
π in an arbitrary irrigation network as:
#
"∞
"
X
1
t
γ R(xt , π(xt )) ≤
Eπ
0.2nin + (n − nout )
1−γ
t=0
#
Z
Pbeta (x′ | 46x + 2, 46(1 − x) + 2)R(x′ ) dx′ ,
(44)
max
x

x′

where n is the total number of irrigation channels, nin and nout denote the number of inflow
and outflow channels, respectively, and R(x′ ) = N (x′ | 0.4, 0.025)/25.6+N (x′ | 0.55, 0.05)/32.
We do not analyze the quality of HALP solutions with respect to the optimal value function
V ∗ (Section 5.1) because this one is unknown.
In the rest of the section, we illustrate the performance of three HALP approximations,
MC-HALP, ε-HALP, and MCMC-HALP (Section 6), on the ring and ring-of-rings topologies
(Figure 13) of the irrigation network problem. The constraints in the MC-HALP formulation
are sampled uniformly at random. This establishes a baseline for all HALP approximations.
The ε-HALP and MCMC-HALP formulations are generated iteratively by the cutting plane
188

Solving Factored MDPs with Hybrid State and Action Variables

fi (xi )

0.6

Density

Reward

Non−outflow channel

0.4
0.2
0

0

0.5

1

1

0.5

0.5

0

1

0

0

(a)

Density

Reward

1

0

0.5
Xj

1

(b)

0.5

0

1

0

fi+2n (xi )

Outflow channel
2

fi+n (xi )

1

1

0.5

0.5

0

0

0.5
Xi

0.5

1

fi+3n (xi )

0

1

0

0.5
Xi

1

(c)

Figure 14: a. Indexing used in the description of the transition function in Example 6. The
parameters h, i, j, and k are equal to 6, 7, 10, and 1, respectively. b. Univariate
reward functions over water levels Xj (Example 6). c. Univariate basis functions
over water levels Xi .

method. The MCMC oracle OMCMC is simulated for 500 steps from the initial temperature
c = 0.2, which leads to a decreasing cooling schedule from T0 = 0.2 to T500 ≈ 0.02. These
parameters are selected empirically to demonstrate the characteristics of the oracle OMCMC
rather than to maximize its performance. The value function V ∗ is approximated by a linear
combination of four univariate piecewise linear basis functions for each channel (Figure 14c).
We assume that our basis functions are sufficient to derive a one-step lookahead policy that
routes water between the channels if their water levels are too high or too low (Figure 14b).
We believe that such a policy is close-to-optimal in irrigation networks. The state relevance
density function ψ(x) is uniform. Our experimental results are reported in Figures 15–17.
Based on the results, we draw the following conclusions. First, all HALP approximations
scale up in the dimensionality of solved problems. As shown in Figure 16, the return of the
policies grows linearly in n. Moreover, the time complexity of computing them is polynomial
in n. Therefore, if a problem and its approximate solution are structured, we take advantage
of this structure to avoid an exponential blowup in the computation time. At the same time,
the quality of the policies is not deteriorating with increasing problem size n.
Second, the MCMC solver (N = 250) achieves the highest objective values on all solved
problems. Higher objective values are interpreted as closer approximations to the constraint
space in HALP since the solvers operate on relaxed formulations of HALP. Third, the quality
of the MCMC-HALP policies (N = 250) surpasses the MC-HALP policies (N = 106 ) while
both solvers consume approximately the same computation time. This result is due to the
informative search for violated constraints in the MCMC-HALP solver. Fourth, the quality
of the MCMC-HALP policies (N = 250) is close to the ε-HALP policies (ε = 1/16) although
there is a significant difference between their objective values. Further analysis shows that
the shape of the value functions is similar (Figure 17) and they differ the most in the weight
189

Kveton, Hauskrecht, & Guestrin

Ring
topology
ε-HALP 1/4
ε=
1/8
1/16
MCMC
10
N=
50
250
MC
102
N=
104
106
Utopian

OV
24.3
55.4
59.1
60.9
70.1
70.7
16.2
40.8
51.2

n=6
n = 12
Reward Time
OV
Reward
34.6 ± 2.0
11 36.2 53.9 ± 2.7
39.6 ± 2.5
41 88.1 61.5 ± 3.5
40.3 ± 2.6
281 93.2 62.6 ± 3.4
30.3 ± 4.9
38 86.3 47.6 ± 6.3
40.2 ± 2.6
194 110.3 62.4 ± 3.5
40.2 ± 2.6
940 112.0 63.0 ± 3.4
25.0 ± 5.1
< 1 16.9 41.9 ± 5.6
37.9 ± 2.8
10 52.8 58.8 ± 3.5
39.4 ± 2.7
855 67.1 60.3 ± 3.8
49.1
79.2

Ring-of-rings
topology
ε-HALP 1/4
ε=
1/8
1/16
MCMC
10
N=
50
250
MC
102
N=
104
106
Utopian

OV
28.4
65.4
68.9
66.9
80.9
81.7
13.7
44.3
55.8

n=6
Reward
40.4 ± 2.5
47.5 ± 3.0
47.0 ± 2.9
35.3 ± 6.1
47.1 ± 2.9
47.2 ± 2.9
31.0 ± 4.9
43.3 ± 3.2
45.1 ± 3.1
59.1

Time
85
495
4 417
60
309
1 522
<1
12
1 026

OV
44.1
107.9
113.1
94.6
131.9
134.1
15.4
59.0
75.1

Time
44
107
665
62
328
1 609
<1
18
1 415

OV
48.0
118.8
126.1
109.5
148.8
151.7
17.2
63.8
81.1

n = 18
Reward
74.3 ± 2.9
84.3 ± 3.8
86.3 ± 3.8
56.8 ± 7.4
85.0 ± 3.6
85.4 ± 3.8
51.8 ± 8.8
75.9 ± 6.6
82.9 ± 3.8
109.2

Time
87
178
1 119
87
483
2 280
<1
31
1 938

n = 12
n = 18
Reward Time
OV
Reward Time
66.5 ± 3.2
382 59.8 93.0 ± 3.8
931
76.1 ± 4.1 2 379 148.8 105.3 ± 4.2 5 877
77.3 ± 4.2 19 794 156.9 107.8 ± 4.1 53 655
54.4 ± 9.4
107 110.6 47.8 ± 13.2
157
76.6 ± 3.6
571 181.4 104.6 ± 4.4
859
77.3 ± 3.5 2 800 186.0 106.6 ± 3.9 4 291
46.1 ± 6.4
< 1 16.8 66.6 ± 9.4
1
68.9 ± 5.4
26 71.5 92.2 ± 6.8
49
74.3 ± 3.8 1 738 92.0 103.1 ± 4.2 2 539
99.2
139.3

Figure 15: Comparison of three HALP solvers on two irrigation network topologies of varying sizes (n). The solvers are compared by the objective value of a relaxed HALP
(OV), the expected discounted reward of a corresponding policy, and computation time (in seconds). The ε-HALP, MCMC-HALP, and MC-HALP solvers are
parameterized by the resolution of ε-grid (ε), the number of MCMC chains (N ),
and the number of samples (N ). Note that the quality of policies improves with
higher grid resolution (1/ε) and larger sample size (N ). Upper bounds on their
expected returns are shown in the last rows of the tables.

of the constant basis function f0 (x) ≡ 1. Note that increasing w0 does not affect the quality
of a greedy policy for V w . However, this trick allows the satisfaction of the constraint space
in HALP (Section 5.1).
Finally, the computation time of the ε-HALP solver is seriously affected by the topologies
of the irrigation networks, which can be explained as follows. For a small ε and large n, the
time complexity of formulating cost networks for the ring and ring-of-rings topologies grows
by the rates of ⌈1/ε + 1⌉2 and ⌈1/ε + 1⌉3 , respectively. Since the ε-HALP method consumes
a significant amount of time by constructing cost networks, its quadratic (in ⌈1/ε + 1⌉) time
complexity on the ring topology worsens to cubic (in ⌈1/ε + 1⌉) on the ring-of-rings topology.
On the other hand, a similar cross-topology comparison of the MCMC-HALP solver shows
that its computation times differ only by a multiplicative factor of 2. This difference is due
190

Solving Factored MDPs with Hybrid State and Action Variables

Ring topology
ε−HALP
Reward

125
100

MCMC
125

f(n) = 3.823n + c
R2 = 1.000

100

125

f(n) = 3.766n + c
R2 = 1.000

100

75

75

75

50

50

50

6

9

12

15

18

1
0.75
Time

MC

0.5

9

12

15

18

1
f(n) = 0.019n + c
R2 = 0.998

0.75
0.5

0.25
0

6

9

12
n

15

18

0

6

9

12

15

18

15

18

15

18

15

18

1
f(n) = 0.031n + c
R2 = 0.999

0.75
0.5

0.25
6

f(n) = 3.594n + c
R2 = 0.999

f(n) = 0.025n + c
R2 = 0.999

0.25
6

9

12
n

15

18

0

6

9

12
n

Ring-of-rings topology
ε−HALP
Reward

125
100

MCMC
125

f(n) = 5.092n + c
R2 = 1.000

100

75

9

12

15

18

24
Time

100

50
6

9

12

15

18

2
f(n) = 0.002n3 + c
R2 = 0.997

1.5
1

6

0.5

0

0

6

9

12
n

15

18

f(n) = 4.838n + c
R2 = 1.000

75

50
6

12

f(n) = 4.964n + c
R2 = 1.000

75

50

18

MC
125

6

9

12

2
f(n) = 0.064n + c
R2 = 0.998

1.5
1

f(n) = 0.035n + c
R2 = 0.998

0.5
6

9

12
n

15

18

0

6

9

12
n

Figure 16: Scale-up potential of the ε-HALP, MCMC-HALP, and MC-HALP solvers on two
irrigation network topologies of varying sizes (n). The graphs show the expected
discounted reward of policies and their computation time (in hours) as functions
of n. The HALP solvers are parameterized by the resolution of ε-grid (ε = 1/16),
the number of MCMC chains (N = 250), and the number of samples (N = 106 ).
Note that all trends can be approximated by a polynomial f (n) (gray line) with
a high degree of confidence (the coefficient of determination R2 ), where c denotes
a constant independent of n.

to the increased complexity of sampling p(zi∗ | z−i ), which results from more complex local
dependencies in the ring-of-rings topology and not its treewidth.
Before we proceed, note that our relaxed formulations (Figure 15) have significantly less
constraints than their complete sets (Section 6.3). For instance, the MC-HALP formulation
(N = 106 ) on the 6-ring irrigation network problem is originally established by 106 randomly
sampled constraints. Based on our empirical results, the constraints can be satisfied greedily
191

Kveton, Hauskrecht, & Guestrin

MC−HALP MCMC−HALP

ε−HALP

b (x)|
w
b
w
b
w
b
w
b
w
b
w
b
w
b
w
b
w
b
Vw
X1 V (x)| X2 V (x)| X3 V (x)| X4 V (x)| X5 V (x)| X6 V (x)| X7 V (x)| X8 V (x)| X9 V (x)| X10
2
1
0
2
1
0
2
1
0

0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1
X1
X2
X3
X4
X5
X6
X7
X8
X9
X10

b
Figure 17: Univariate projections V w
(x)|Xj =
b
w

P

i:Xj =Xi

w
bi fi (xi ) of approximate value func-

tions V on the 6-ring irrigation network problem (Figure 13a). These functions
are learned from 40 basis functions (Figure 14c) by the ε-HALP, MCMC-HALP,
and MC-HALP solvers. The solvers are parameterized by the resolution of ε-grid
(ε = 1/16), the number of MCMC chains (N = 250), and the number of samples
b
(N = 106 ). Note that the univariate projections V w
(x)|Xj are very similar. The
proximity of their greedy policies can be explained based on this observation.

ε-HALP
ε OV
Reward Time
1 30.4 48.3 ± 3.0
9
1/2 42.9 58.7 ± 3.1
342
1/4 49.1 61.9 ± 3.1 9 443

MCMC
N
OV
Reward Time
10 45.3 43.6 ± 6.5
83
50 116.2 72.2 ± 3.6
458
250 118.5 73.2 ± 3.7 2 012

MC
N OV
Reward Time
102 12.8 56.6 ± 4.5
<1
104 49.9 53.4 ± 5.9
19
106 71.7 70.3 ± 3.9 1 400

Figure 18: Comparison of three HALP solvers on the 3 × 3 grid irrigation network problem
(Figure 13). The solvers are compared by the objective value of a relaxed HALP
(OV), the expected discounted reward of a corresponding policy, and computation time (in seconds). The ε-HALP, MCMC-HALP, and MC-HALP solvers are
parameterized by the resolution of ε-grid (ε), the number of MCMC chains (N ),
and the number of samples (N ). Note that the quality of policies improves with
higher grid resolution (1/ε) and larger sample size (N ). An upper bound on the
expected returns is 87.2.

by a subset of 400 constraints on average (Kveton & Hauskrecht, 2004). Similarly, the oracle
OMCMC in the MCMC-HALP formulation (N = 250) iterates through 250×500×(10+10) =
2, 500, 000 state-action configurations (Figure 11). However, corresponding LP formulations
involve only 700 constraints on average.
7.3 The Curse of Treewidth
In the ring and ring-of-rings topologies, the treewidth of the constraint space (in continuous
variables) is 2 and 3, respectively. As a result, the oracle Oε can perform variable elimination
192

Solving Factored MDPs with Hybrid State and Action Variables

for a small ε, and the ε-HALP solver returns close-to-optimal policies. Unfortunately, small
treewidth is atypical in real-world domains. For instance, the treewidth of a more complex
3 × 3 grid irrigation network (Figure 13c) is 6. To perform variable elimination for ε = 1/16,
the separation oracle Oε requires the space of ⌈1/ε + 1⌉7 ≈ 228 , which is at the memory limit
of existing PCs. To analyze the behavior of our separation oracles (Section 6) in this setting,
we repeat our experiments from Section 7.2 on the 3 × 3 grid irrigation network.
Based on the results in Figure 18, we conclude that the time complexity of the ε-HALP
solver grows by the rate of ⌈1/ε + 1⌉7 . Therefore, approximate constraint space satisfaction
(MC-HALP and MCMC-HALP) generates better results than a combinatorial optimization
on an insufficiently discretized ε-grid (ε-HALP). This conclusion is parallel to those in large
structured optimization problems with continuous variables. We believe that a combination
of exact and approximate steps delivers the best tradeoff between the quality and complexity
of our solutions (Section 6.4).

8. Conclusions
Development of scalable algorithms for solving real-world decision problems is a challenging
task. In this paper, we presented a theoretically sound framework that allows for a compact
representation and efficient solutions to hybrid factored MDPs. We believe that our results
can be applied to a variety of optimization problems in robotics, manufacturing, or financial
mathematics. This work can be extended in several interesting directions.
First, note that the concept of closed-form solutions to the expectations terms in HALP
is not limited to the choices in Section 5.2. For instance, we can show that if P (x) and f (x)
are normal densities, EP (x) [f (x)] has a closed-form solution (Kveton & Hauskrecht, 2006b).
Therefore, we can directly reason with normal transition functions instead of approximating
them by a mixture of beta distributions. Similar conclusions are true for piecewise constant,
piecewise linear, and gamma transition and basis functions. Note that our efficient solutions
apply to any approach to solving hybrid factored MDPs that approximates the optimal value
function by a linear combination of basis functions (Equation 5).
Second, the constraint space in HALP (16) V w − T ∗ V w ≥ 0 exhibits the same structure
as the constraint space in approximate policy iteration (API) (Guestrin et al., 2001; Patrascu
et al., 2002) kV w − T ∗ V w k∞ ≤ ε, where ε is a variable subject to minimization. As a result,
our work provides a recipe for solving API formulations in hybrid state and action domains.
In discrete-state spaces, Patrascu et al. (2002) and Guestrin (2003) showed that API returns
better policies than ALP for the same set of basis functions. Note that API is more complex
than ALP because every step of API involves satisfying the constraint kV w − T ∗ V w k∞ ≤ ε
for some fixed ε.
Third, automatic learning of basis functions seems critical for the application of HALP
to real-world domains. Patrascu et al. (2002) analyzed this problem in discrete-state spaces
and proposed a greedy approach to learning basis functions. Kveton and Hauskrecht (2006a)
generalized these ideas and showed how to learn parametric basis functions in hybrid spaces.
We believe that a combination of the greedy search with a state space analysis (Mahadevan,
2005; Mahadevan & Maggioni, 2006) can yield even better basis functions.
Finally, we proposed several bounds (Section 5.1 and 6.2.1) that may explain the quality
of the complete and relaxed HALP formulations. In future, we plan to empirically evaluate
193

Kveton, Hauskrecht, & Guestrin

their tightness on a variety of low-dimensional hybrid optimization problems (Bresina et al.,
2002; Munos & Moore, 2002) with known optimal value functions.

Acknowledgment
This work was supported in part by National Science Foundation grants CMS-0416754 and
ANI-0325353. The first author was supported by Andrew Mellon Predoctoral Fellowships for
the academic years 2004-06. The first author also recognizes support from Intel Corporation
in the summer 2005 and 2006.

Appendix A. Proofs
Proof of Proposition 1: The Bellman operator T ∗ is known to be a contraction mapping.
Based on its monotonicity, for any value function V , V ≥ T ∗ V implies V ≥ T ∗ V ≥ · · · ≥ V ∗ .
e
e
e
Since constraints in the HALP formulation (16) enforce V w
≥ T ∗V w
, we conclude V w
≥ V ∗.
¤
Proof of Proposition 2: Based on Proposition 1, we note that the constraint V w ≥ T ∗ V w
guarantees that V w ≥ V ∗ . Subsequently, our claim is proved by realizing:
arg min Eψ [V w ] = arg min Eψ [V w − V ∗ ]
w

w

and
Eψ [V w − V ∗ ] = Eψ |V w − V ∗ |
= Eψ |V ∗ − V w |
= kV ∗ − V w k1,ψ .
The proof generalizes from the discrete-state case (de Farias & Van Roy, 2003) without any
alternations. ¤
Proof of Theorem 2: Similarly to Theorem 2 (de Farias & Van Roy, 2003), this claim is
proved in three steps. First, we find a point w in the feasible region of the HALP such that
∗
V w is within O(ǫ) distance from V w , where:
w∗ = arg min kV ∗ − V w k∞
w
°
°
∗°
° ∗
ǫ = °V − V w ° .
∞

Such a point w is given by:

w = w∗ +

(1 + γ)ǫ
e,
1−γ

where e = (1, 0, . . . , 0) is an indicator of the constant basis function f0 (x) ≡ 1. This point
satisfies all requirements and its feasibility can be handily verified by solving:
µ
¶
(1 + γ)ǫ
γ(1 + γ)ǫ
w
∗ w
w∗
∗ w∗
V −T V
= V
+
− T V
+
1−γ
1−γ
∗

∗

= V w − T ∗ V w + (1 + γ)ǫ

≥ 0,
194

Solving Factored MDPs with Hybrid State and Action Variables

where the last step follows from the inequality:
°
°
° ∗
°
°
° ∗
° w
° ∗
° w
∗°
∗ w∗ °
∗ w∗ °
− V ° + °V − T V °
− T V ° ≤ °V
°V
∞°
∞
°∞ °
°
° ∗ ∗
° ∗
∗ w∗ °
w∗ °
= °V − V ° + °T V − T V °
∞

∞

≤ (1 + γ)ǫ.

Subsequently, we bound the max-norm error of V w by using the triangle inequality:
°
°
° ∗
°
° ∗
°
° w
w°
∗
w∗ °
°V − V w ° ≤ °
+
V
−
V
V
−
V
°
°
°
°
∞
∞
¶∞
µ
1+γ
ǫ
= 1+
1−γ
2ǫ
,
=
1−γ
e
which yields a bound on the weighted L1 -norm error of V w
:
°
°
°
°
° ∗
e°
≤ °V ∗ − V w °1,ψ
°V − V w
°
1,ψ
°
°
≤ °V ∗ − V w °∞
2ǫ
≤
.
1−γ

The proof generalizes from the discrete-state case (de Farias & Van Roy, 2003) without any
alternations. ¤
Proof of Theorem 3: Similarly to Theorem 2, this claim is proved in three steps: finding
a point w in the feasible region of the HALP, bounding the max-norm error of V w , which
e
in turn yields a bound on the L1 -norm error of V w
. A comprehensive proof for the discretestate case was done by de Farias and Van Roy (2003). This proof generalizes to structured
state and action spaces with continuous variables. ¤
Proof of Proposition 3: The proposition is proved in a sequence of steps:
Z
Pbeta (x | α, β)xn (1 − x)m dx
EP (x) [f (x)] =
Zx
Γ(α + β) α−1
x
(1 − x)β−1 xn (1 − x)m dx
=
Γ(α)Γ(β)
x
Z
Γ(α + β)
=
xα+n−1 (1 − x)β+m−1 dx
Γ(α)Γ(β) x
Z
Γ(α + β) Γ(α + n)Γ(β + m)
Γ(α + β + n + m) α+n−1
=
x
(1 − x)β+m−1 dx
Γ(α)Γ(β) Γ(α + β + n + m) x Γ(α + n)Γ(β + m)
Z
Γ(α + β) Γ(α + n)Γ(β + m)
=
Pbeta (x | α + n, β + m) dx .
Γ(α)Γ(β) Γ(α + β + n + m) x
{z
}
|
1

195

Kveton, Hauskrecht, & Guestrin

Since integration is a distributive operation, our claim straightforwardly generalizes to the
mixture of beta distributions P (x). ¤
Proof of Proposition 4: The proposition is proved in a sequence of steps:
Z
X
Pbeta (x | α, β)
1[li ,ri ] (x)(ai x + bi ) dx
EP (x) [f (x)] =
x

=

XZ
i

i

ri

Pbeta (x | α, β)(ai x + bi ) dx
li

X· Z
=
ai
i

X·
ai
=
i

X·
ai
=
i

ri

Pbeta (x | α, β)x dx + bi

li

α
α+β

Z

Z

ri

li

ri

¸
Pbeta (x | α, β) dx

Pbeta (x | α + 1, β) dx + bi
li

Z

ri

li

¸
Pbeta (x | α, β) dx

¸
α
+
+
(F (ri ) − F (li )) + bi (F (ri ) − F (li )) .
α+β

Since integration is a distributive operation, our claim straightforwardly generalizes to the
mixture of beta distributions P (x). ¤
Proof of Proposition 5: This claim is proved in three steps. First, we construct a point
b
w in the feasible region of the HALP such that V w is within O(δ) distance from V w
. Such
a point w is given by:
b+
w=w

δ
e,
1−γ

where e = (1, 0, . . . , 0) is an indicator of the constant basis function f0 (x) ≡ 1. This point
satisfies all requirements and its feasibility can be handily verified by solving:
µ
¶
δ
γδ
b
b
w
w
∗ w
∗ w
V −T V
= V +
− T V +
1−γ
1−γ
b
b
= Vw
− T ∗V w
+δ

≥ 0,

b
b
where the inequality V w
−T ∗ V w
≥ −δ holds from the δ-infeasibility
of w.
the optimal
£ b¤
£b eSince
¤
e is feasible in the relaxed HALP, we conclude Eψ V w
solution w
≤ Eψ V w
. Subsequently,
this inequality yields a bound on the weighted L1 -norm error of V w :
¯
¯
°
¯ w
¯
° ∗
δ
b
w
∗
°V − V °
= Eψ ¯¯V +
− V ¯¯
1,ψ
1−γ
h
i
δ
b
= Eψ V w
+
− Eψ [V ∗ ]
1−γ
h
i
δ
e
− Eψ [V ∗ ]
≤ Eψ V w
+
1−γ
°
°
δ
°
e°
.
= °V ∗ − V w
° +
1−γ
1,ψ

196

Solving Factored MDPs with Hybrid State and Action Variables

Finally, we combine this result with the triangle inequality:
°
°
°
°
° ∗
°
° ∗
° w
b°
b°
w
w
w°
°
≤ V − V 1,ψ + °V − V °
°V − V °
1,ψ
1,ψ
°
°
2δ
°
e°
≤ °V ∗ − V w
,
° +
1−γ
1,ψ
b
which leads to a bound on the weighted L1 -norm error of V w
. ¤

References
Andrieu, C., de Freitas, N., Doucet, A., & Jordan, M. (2003). An introduction to MCMC
for machine learning. Machine Learning, 50, 5–43.
Astrom, K. (1965). Optimal control of Markov processes with incomplete state information.
Journal of Mathematical Analysis and Applications, 10 (1), 174–205.
Bellman, R. (1957). Dynamic Programming. Princeton University Press, Princeton, NJ.
Bellman, R., Kalaba, R., & Kotkin, B. (1963). Polynomial approximation – a new computational technique in dynamic programming: Allocation processes. Mathematics of
Computation, 17 (82), 155–161.
Bertsekas, D. (1995). A counterexample for temporal differences learning. Neural Computation, 7 (2), 270–279.
Bertsekas, D., & Tsitsiklis, J. (1996). Neuro-Dynamic Programming. Athena Scientific,
Belmont, MA.
Bertsimas, D., & Tsitsiklis, J. (1997). Introduction to Linear Optimization. Athena Scientific, Belmont, MA.
Boutilier, C., Dearden, R., & Goldszmidt, M. (1995). Exploiting structure in policy construction. In Proceedings of the 14th International Joint Conference on Artificial
Intelligence, pp. 1104–1111.
Bresina, J., Dearden, R., Meuleau, N., Ramakrishnan, S., Smith, D., & Washington, R.
(2002). Planning under continuous time and resource uncertainty: A challenge for AI.
In Proceedings of the 18th Conference on Uncertainty in Artificial Intelligence, pp.
77–84.
Casella, G., & Robert, C. (1996). Rao-Blackwellisation of sampling schemes. Biometrika,
83 (1), 81–94.
Chow, C.-S., & Tsitsiklis, J. (1991). An optimal one-way multigrid algorithm for discretetime stochastic control. IEEE Transactions on Automatic Control, 36 (8), 898–914.
Cooper, G. (1988). A method for using belief networks as influence diagrams. In Proceedings
of the Workshop on Uncertainty in Artificial Intelligence, pp. 55–63.
Crites, R., & Barto, A. (1996). Improving elevator performance using reinforcement learning. In Advances in Neural Information Processing Systems 8, pp. 1017–1023.
de Farias, D. P., & Van Roy, B. (2003). The linear programming approach to approximate
dynamic programming. Operations Research, 51 (6), 850–856.
197

Kveton, Hauskrecht, & Guestrin

de Farias, D. P., & Van Roy, B. (2004). On constraint sampling for the linear programming approach to approximate dynamic programming. Mathematics of Operations
Research, 29 (3), 462–478.
Dean, T., & Kanazawa, K. (1989). A model for reasoning about persistence and causation.
Computational Intelligence, 5, 142–150.
Dechter, R. (1996). Bucket elimination: A unifying framework for probabilistic inference.
In Proceedings of the 12th Conference on Uncertainty in Artificial Intelligence, pp.
211–219.
Duane, S., Kennedy, A. D., Pendleton, B., & Roweth, D. (1987). Hybrid Monte Carlo.
Physics Letters B, 195 (2), 216–222.
Feng, Z., Dearden, R., Meuleau, N., & Washington, R. (2004). Dynamic programming for
structured continuous Markov decision problems. In Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence, pp. 154–161.
Ferns, N., Panangaden, P., & Precup, D. (2005). Metrics for Markov decision processes
with infinite state spaces. In Proceedings of the 21st Conference on Uncertainty in
Artificial Intelligence.
Geman, S., & Geman, D. (1984). Stochastic relaxation, Gibbs distribution, and the Bayesian
restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6 (6), 721–741.
Gordon, G. (1999). Approximate Solutions to Markov Decision Processes. Ph.D. thesis,
Carnegie Mellon University.
Guestrin, C. (2003). Planning Under Uncertainty in Complex Structured Environments.
Ph.D. thesis, Stanford University.
Guestrin, C., Hauskrecht, M., & Kveton, B. (2004). Solving factored MDPs with continuous and discrete variables. In Proceedings of the 20th Conference on Uncertainty in
Artificial Intelligence, pp. 235–242.
Guestrin, C., Koller, D., Gearhart, C., & Kanodia, N. (2003). Generalizing plans to new
environments in relational MDPs. In Proceedings of the 18th International Joint
Conference on Artificial Intelligence, pp. 1003–1010.
Guestrin, C., Koller, D., & Parr, R. (2001). Max-norm projections for factored MDPs. In
Proceedings of the 17th International Joint Conference on Artificial Intelligence, pp.
673–682.
Guestrin, C., Koller, D., & Parr, R. (2002). Multiagent planning with factored MDPs. In
Advances in Neural Information Processing Systems 14, pp. 1523–1530.
Guestrin, C., Koller, D., Parr, R., & Venkataraman, S. (2003). Efficient solution algorithms
for factored MDPs. Journal of Artificial Intelligence Research, 19, 399–468.
Guestrin, C., Venkataraman, S., & Koller, D. (2002). Context specific multiagent coordination and planning with factored MDPs. In Proceedings of the 18th National Conference
on Artificial Intelligence, pp. 253–259.
Hastings, W. K. (1970). Monte Carlo sampling methods using Markov chains and their
application. Biometrika, 57, 97–109.
198

Solving Factored MDPs with Hybrid State and Action Variables

Hauskrecht, M. (2000). Value-function approximations for partially observable Markov
decision processes. Journal of Artificial Intelligence Research, 13, 33–94.
Hauskrecht, M., & Kveton, B. (2004). Linear program approximations for factored
continuous-state Markov decision processes. In Advances in Neural Information Processing Systems 16, pp. 895–902.
Higdon, D. (1998). Auxiliary variable methods for Markov chain Monte Carlo with applications. Journal of the American Statistical Association, 93 (442), 585–595.
Howard, R., & Matheson, J. (1984). Influence diagrams. In Readings on the Principles and
Applications of Decision Analysis, Vol. 2, pp. 719–762. Strategic Decisions Group,
Menlo Park, CA.
Jeffreys, H., & Jeffreys, B. (1988). Methods of Mathematical Physics. Cambridge University
Press, Cambridge, United Kingdom.
Jensen, F., Jensen, F., & Dittmer, S. (1994). From influence diagrams to junction trees.
In Proceedings of the 10th Conference on Uncertainty in Artificial Intelligence, pp.
367–373.
Khachiyan, L. (1979). A polynomial algorithm in linear programming. Doklady Akademii
Nauk SSSR, 244, 1093–1096.
Kirkpatrick, S., Gelatt, C. D., & Vecchi, M. P. (1983). Optimization by simulated annealing.
Science, 220 (4598), 671–680.
Koller, D., & Parr, R. (1999). Computing factored value functions for policies in structured MDPs. In Proceedings of the 16th International Joint Conference on Artificial
Intelligence, pp. 1332–1339.
Kveton, B., & Hauskrecht, M. (2004). Heuristic refinements of approximate linear programming for factored continuous-state Markov decision processes. In Proceedings of the
14th International Conference on Automated Planning and Scheduling, pp. 306–314.
Kveton, B., & Hauskrecht, M. (2005). An MCMC approach to solving hybrid factored
MDPs. In Proceedings of the 19th International Joint Conference on Artificial Intelligence, pp. 1346–1351.
Kveton, B., & Hauskrecht, M. (2006a). Learning basis functions in hybrid domains. In
Proceedings of the 21st National Conference on Artificial Intelligence, pp. 1161–1166.
Kveton, B., & Hauskrecht, M. (2006b). Solving factored MDPs with exponential-family
transition models. In Proceedings of the 16th International Conference on Automated
Planning and Scheduling, pp. 114–120.
Mahadevan, S. (2005). Samuel meets Amarel: Automating value function approximation
using global state space analysis. In Proceedings of the 20th National Conference on
Artificial Intelligence, pp. 1000–1005.
Mahadevan, S., & Maggioni, M. (2006). Value function approximation with diffusion
wavelets and Laplacian eigenfunctions. In Advances in Neural Information Processing
Systems 18, pp. 843–850.
199

Kveton, Hauskrecht, & Guestrin

Mahadevan, S., Maggioni, M., Ferguson, K., & Osentoski, S. (2006). Learning representation and control in continuous Markov decision processes. In Proceedings of the 21st
National Conference on Artificial Intelligence.
Manne, A. (1960). Linear programming and sequential decisions. Management Science,
6 (3), 259–267.
Metropolis, N., Rosenbluth, A., Rosenbluth, M., Teller, A., & Teller, E. (1953). Equation
of state calculations by fast computing machines. Journal of Chemical Physics, 21,
1087–1092.
Munos, R., & Moore, A. (2002). Variable resolution discretization in optimal control. Machine Learning, 49, 291–323.
Ortiz, L. (2002). Selecting Approximately-Optimal Actions in Complex Structured Domains.
Ph.D. thesis, Brown University.
Park, J., & Darwiche, A. (2001). Approximating MAP using local search. In Proceedings
of the 17th Conference on Uncertainty in Artificial Intelligence, pp. 403–410.
Park, J., & Darwiche, A. (2003). Solving MAP exactly using systematic search. In Proceedings of the 19th Conference on Uncertainty in Artificial Intelligence, pp. 459–468.
Patrascu, R., Poupart, P., Schuurmans, D., Boutilier, C., & Guestrin, C. (2002). Greedy
linear value-approximation for factored Markov decision processes. In Proceedings of
the 18th National Conference on Artificial Intelligence, pp. 285–291.
Puterman, M. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, New York, NY.
Rust, J. (1997). Using randomization to break the curse of dimensionality. Econometrica,
65 (3), 487–516.
Sanner, S., & Boutilier, C. (2005). Approximate linear programming for first-order MDPs.
In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence.
Schuurmans, D., & Patrascu, R. (2002). Direct value-approximation for factored MDPs. In
Advances in Neural Information Processing Systems 14, pp. 1579–1586.
Schweitzer, P., & Seidmann, A. (1985). Generalized polynomial approximations in Markovian decision processes. Journal of Mathematical Analysis and Applications, 110,
568–582.
Sondik, E. (1971). The Optimal Control of Partially Observable Markov Decision Processes.
Ph.D. thesis, Stanford University.
Sutton, R., & Barto, A. (1998). Reinforcement Learning: An Introduction. MIT Press,
Cambridge, MA.
Tesauro, G. (1992). Practical issues in temporal difference learning. Machine Learning,
8 (3-4), 257–277.
Tesauro, G. (1994). TD-Gammon, a self-teaching backgammon program, achieves masterlevel play. Neural Computation, 6 (2), 215–219.
Tesauro, G. (1995). Temporal difference learning and TD-Gammon. Communications of
the ACM, 38 (3), 58–68.
200

Solving Factored MDPs with Hybrid State and Action Variables

Trick, M., & Zin, S. (1993). A linear programming approach to solving stochastic dynamic
programs. Tech. rep., Carnegie Mellon University.
Van Roy, B. (1998). Planning Under Uncertainty in Complex Structured Environments.
Ph.D. thesis, Massachusetts Institute of Technology.
Yuan, C., Lu, T.-C., & Druzdzel, M. (2004). Annealed MAP. In Proceedings of the 20th
Conference on Uncertainty in Artificial Intelligence, pp. 628–635.
Zhang, W., & Dietterich, T. (1995). A reinforcement learning approach to job-shop scheduling. In Proceedings of the 14th International Joint Conference on Artificial Intelligence, pp. 1114–1120.
Zhang, W., & Dietterich, T. (1996). High-performance job-shop scheduling with a timedelay TD(λ) network. In Advances in Neural Information Processing Systems 8, pp.
1024–1030.

201

Journal of Artificial Intelligence Research 27 (2006) 419-439

Submitted 12/05; published 12/06

Engineering Note
FluCaP: A Heuristic Search Planner for First-Order MDPs
Steffen Hölldobler
Eldar Karabaev
Olga Skvortsova

sh@iccl.tu-dresden.de
eldar@iccl.tu-dresden.de
skvortsova@iccl.tu-dresden.de

International Center for Computational Logic
Technische Universität Dresden, Dresden, Germany

Abstract
We present a heuristic search algorithm for solving first-order Markov Decision Processes (FOMDPs). Our approach combines first-order state abstraction that avoids evaluating states individually, and heuristic search that avoids evaluating all states. Firstly,
in contrast to existing systems, which start with propositionalizing the FOMDP and then
perform state abstraction on its propositionalized version we apply state abstraction directly on the FOMDP avoiding propositionalization. This kind of abstraction is referred to
as first-order state abstraction. Secondly, guided by an admissible heuristic, the search is
restricted to those states that are reachable from the initial state. We demonstrate the usefulness of the above techniques for solving FOMDPs with a system, referred to as FluCaP
(formerly, FCPlanner), that entered the probabilistic track of the 2004 International Planning Competition (IPC’2004) and demonstrated an advantage over other planners on the
problems represented in first-order terms.

1. Introduction
Markov decision processes (MDPs) have been adopted as a representational and computational model for decision-theoretic planning problems in much recent work, e.g., by Barto,
Bradtke, and Singh (1995). The basic solution techniques for MDPs rely on the dynamic
programming (DP) principle (Boutilier, Dean, & Hanks, 1999). Unfortunately, classical dynamic programming algorithms require explicit enumeration of the state space that grows
exponentially with the number of variables relevant to the planning domain. Therefore,
these algorithms do not scale up to complex AI planning problems.
However, several methods that avoid explicit state enumeration have been developed
recently. One technique, referred to as state abstraction, exploits the structure of the factored MDP representation to solve problems efficiently, circumventing explicit state space
enumeration (Boutilier et al., 1999). Another technique, referred to as heuristic search,
restricts the computation to states that are reachable from the initial state, e.g., RTDP
by Barto et al. (1995), envelope DP by Dean, Kaelbling, Kirman, and Nicholson (1995) and
LAO∗ by Feng and Hansen (2002). One existing approach that combines both these techniques is the symbolic LAO∗ algorithm by Feng and Hansen (2002) which performs heuristic
search symbolically for factored MDPs. It exploits state abstraction, i.e., manipulates sets of
states instead of individual states. More precisely, following the SPUDD approach by Hoey,
St-Aubin, Hu, and Boutilier (1999), all MDP components, value functions, policies, and
admissible heuristic functions are compactly represented using algebraic decision diagrams
c
2006
AI Access Foundation. All rights reserved.

Hölldobler, Karabaev & Skvortsova

(ADDs). This allows computations of the LAO∗ algorithm to be performed efficiently using
ADDs.
Following ideas of symbolic LAO∗ , given an initial state, we use an admissible heuristic
to restrict search only to those states that are reachable from the initial state. Moreover,
we exploit state abstraction in order to avoid evaluating states individually. Thus, our
work is very much in the spirit of symbolic LAO∗ but extends it in an important way.
Whereas the symbolic LAO∗ algorithm starts with propositionalization of the FOMDP,
and only after that performs state abstraction on its propositionalized version by means of
propositional ADDs, we apply state abstraction directly on the structure of the FOMDP,
avoiding propositionalization. This kind of abstraction is referred to as first-order state
abstraction.
Recently, following work by Boutilier, Reiter, and Price (2001), Hölldobler and Skvortsova
(2004) have developed an algorithm, referred to as first-order value iteration (FOVI) that
exploits first-order state abstraction. The dynamics of an MDP is specified in the Probabilistic Fluent Calculus established by Hölldobler and Schneeberger (1990), which is a
first-order language for reasoning about states and actions. More precisely, FOVI produces
a logical representation of value functions and policies by constructing first-order formulae
that partition the state space into clusters, referred to as abstract states. In effect, the
algorithm performs value iteration on top of these clusters, obviating the need for explicit
state enumeration. This allows problems that are represented in first-order terms to be
solved without requiring explicit state enumeration or propositionalization.
Indeed, propositionalizing FOMDPs can be very impractical: the number of propositions grows considerably with the number of domain objects and relations. This has a
dramatic impact on the complexity of the algorithms that depends directly on the number of propositions. Finally, systems for solving FOMDPs that rely on propositionalizing
states also propositionalize actions which is problematic in first-order domains, because the
number of ground actions also grows dramatically with domain size.
In this paper, we address these limitations by proposing an approach for solving FOMDPs
that combines first-order state abstraction and heuristic search in a novel way, exploiting
the power of logical representations. Our algorithm can be viewed as a first-order generalization of LAO∗ , in which our contribution is to show how to perform heuristic search
for first-order MDPs, circumventing their propositionalization. In fact, we show how to
improve the performance of symbolic LAO∗ by providing a compact first-order MDP representation using Probabilistic Fluent Calculus instead of propositional ADDs. Alternatively,
our approach can be considered as a way to improve the efficiency of the FOVI algorithm
by using heuristic search together with symbolic dynamic programming.

2. First-order Representation of MDPs
Recently, several representations for propositionally-factored MDPs have been proposed,
including dynamic Bayesian networks by Boutilier et al. (1999) and ADDs by Hoey et al.
(1999). For instance, the SPUDD algorithm by Hoey et al. (1999) has been used to solve
MDPs with hundreds of millions of states optimally, producing logical descriptions of value
functions that involve only hundreds of distinct values. This work demonstrates that large
420

FluCaP: A Heuristic Search Planner for First-Order MDPs

MDPs, described in a logical fashion, can often be solved optimally by exploiting the logical
structure of the problem.
Meanwhile, many realistic planning domains are best represented in first-order terms.
However, most existing implemented solutions for first-order MDPs rely on propositionalization, i.e., eliminate all variables at the outset of a solution attempt by instantiating terms
with all possible combinations of domain objects. This technique can be very impractical
because the number of propositions grows dramatically with the number of domain objects
and relations.
For example, consider the following goal statement taken from the colored Blocksworld
scenario, where the blocks, in addition to unique identifiers, are associated with colors.
G = ∃X0 . . . X7 . red(X0 ) ∧ green(X1 ) ∧ blue(X2 ) ∧ red(X3 ) ∧ red(X4 )∧
red(X5 ) ∧ green(X6 ) ∧ green(X7 ) ∧ T ower(X0 , . . . , X7 ) ,
where T ower(X0 , . . . , X7 ) represents the fact that all eight blocks comprise one tower. We
assume that the number of blocks in the domain and their color distribution agrees with
that in the goal statement, namely there are eight blocks a, b, . . . , h in the domain, where
four of them are red, three are green and one is blue. Then, the full propositionalization
of the goal statement G results in 4!3!1! = 144 different ground towers, because there are
exactly that many ways of arranging four red, three green and one blue block in a tower of
eight blocks with the required color characteristics.
The number of ground combinations, and hence, the complexity of reasoning in a propositional planner, depends dramatically on the number of blocks and, most importantly, on
the number of colors in the domain. The fewer colors a domain contains, the harder it is to
solve by a propositional planner. For example, a goal statement G0 , that is the same as G
above, but all eight blocks are of the same color, results in 8! = 40320 ground towers, when
grounded.
To address these limitations, we propose a concise representation of FOMDPs within the
Probabilistic Fluent Calculus which is a logical approach to modelling dynamically changing
systems based on first-order logic. But first, we briefly describe the basics of the theory of
MDPs.
2.1 MDPs
A Markov decision process (MDP), is a tuple (Z, A, P, R, C), where Z is a finite set of
states, A is a finite set of actions, and P : Z × Z × A → [0, 1], written P(z 0 |z, a), specifies
transition probabilities. In particular, P(z 0 |z, a) denotes the probability of ending up at
state z 0 given that the agent was in state z and action a was executed. R : Z → R is a realvalued reward function associating with each state z its immediate utility R(z). C : A → R
is a real-valued cost function associating a cost C(a) with each action a. A sequential
decision problem consists of an MDP and is the problem of finding a policy π : Z → A that
maximizes the total expected discounted reward received when executing the policy π over
an infinite (or indefinite) horizon.
The value of state z, when starting in z and following the policy π afterwards, can be
computed by the following system of linear equations:
X
Vπ (z) = R(z) + C(π(z)) + γ
P(z 0 |z, π(z))Vπ (z 0 ),
z 0 ∈Z

421

Hölldobler, Karabaev & Skvortsova

where 0 ≤ γ ≤ 1 is a discount factor. We take γ equal to 1 for indefinite-horizon problems
only, i.e., when a goal is reached the system enters an absorbing state in which no further
rewards or costs are accrued. The optimal value function V ∗ satisfies:
X
V ∗ (z) = R(z) + max{C(a) + γ
P(z 0 |z, a)V ∗ (z 0 )} ,
a∈A

z 0 ∈Z

for each z ∈ Z.
For the competition, the expected total reward model was used as the optimality criterion. Without discounting, some care is required in the design of planning problems to
ensure that the expected total reward is bounded for the optimal policy. The following
restrictions were made for problems used in the planning competition:
1. Each problem had a goal statement, identifying a set of absorbing goal states.
2. A positive reward was associated with transitioning into a goal state.
3. A cost was associated with each action.
4. A “done” action was available in all states, which could be used to end further accumulation of reward.
These conditions ensure that an MDP model of a planning problem is a positive bounded
model described by Puterman (1994). The only positive reward is for transitioning into a
goal state. Since goal states are absorbing, that is, they have no outgoing transitions, the
maximum value of any state is bounded by the goal reward. Furthermore, the “done” action
ensures that there is an action available in each state that guarantees a non-negative future
reward.
2.2 Probabilistic Fluent Calculus
Fluent Calculus (FC) by Hölldobler and Schneeberger (1990) was originally set up as a
first-order logic program with equality using SLDE-resolution as the sole inference rule.
The Probabilistic Fluent Calculus (PFC) is an extension of the original FC for expressing
planning domains with actions which have probabilistic effects.
States
Formally, let Σ denote a set of function symbols. We distinguish two function symbols in
Σ, namely the binary function symbol ◦, which is associative, commutative, and admits
the unit element, and a constant 1. Let Σ− = Σ \ {◦, 1}. Non-variable Σ− -terms are
called fluents. The function names of fluents are referred to as fluent names. For example,
on(X, table) is a fluent meaning informally that some block X is on the table, where on
is a fluent name. Fluent terms are defined inductively as follows: 1 is a fluent term; each
fluent is a fluent term; F ◦ G is a fluent term, if F and G are fluent terms. For example,
on(b, table) ◦ holding(X) is a fluent term denoting informally that the block b is on the table
and some block X is in the robot’s gripper. In other words, freely occurring variables are
assumed to be existentially quantified.
422

FluCaP: A Heuristic Search Planner for First-Order MDPs

We assume that each fluent may occur at most once in a state. Moreover, function
symbols, except for the binary ◦ operator, constant 1, fluent names and constants, are
disallowed. In addition, the binary function symbol ◦ is allowed to appear only as an
outermost connective in a fluent term. We denote a set of fluents as F and a set of fluent
terms as LF , respectively. An abstract state is defined by a pair (P, N ), where P ∈ LF and
N ⊆ LF . We denote individual states by z, z1 , z2 etc., abstract states by Z, Z1 , Z2 etc.
and a set of abstract states LP N .
The interpretation over F, denoted as I, is the pair (∆, ·I ), where the domain ∆ is a set
of all finite sets of ground fluents from F; and an interpretation function ·I which assigns
to each fluent term F a set F I ⊆ ∆ and to each abstract state Z = (P, N ) a set Z I ⊆ ∆
as follows:
F I = {d ∈ ∆ | ∃θ.F θ ⊆ d}
Z I = {d ∈ ∆ | ∃θ.P θ ⊆ d ∧ ∀N ∈ N . d ∈
/ (N θ)I },
where θ is a substitution. For example, Figure 1 depicts the interpretation of an abstract
state Z
Z = (on(X, a) ◦ on(a, table), {on(Y, X), holding(X 0 )})
that can be informally read: There exists a block X that is on the block a which is on
the table, there is no such block Y that is on X and there exists no such block X 0 that
the robot holds. Since Z I contains all such finite sets of ground fluents that satisfy the
P -part and do not satisfy any of the elements of the N -part, we subtract all sets of ground
fluents that belong to each of Ni ∈ N from the set of ground fluents that correspond
to the P -part. Thus, the bold area in Figure 1 contains exactly those sets of ground
fluents (or, individual states) that do satisfy the P -part of Z and none of the elements of
its N -part. For example, an individual state z1 = {on(b, a), on(a, table)} belongs to Z I ,
whereas z2 = {on(b, a), on(a, table), holding(c)} does not. In other words, abstract states
are characterized by means of conditions that must hold in each ground instance thereof
and, thus, they represent clusters of individual states. In this way, abstract states embody
a form of state space abstraction. This kind of abstraction is referred to as first-order state
abstraction.
Actions
Actions are first-order terms starting with an action function symbol. For example, the
action of picking up some block X from another block Y might be denoted as pickup (X, Y ).
Formally, let Na denote a set of action names disjoint with Σ. An action space is a tuple
A = (A, Pre , Eff ), where A is a set of terms of the form a(p1 , . . . , pn ), referred to as
actions, with a ∈ Na and each pi being either a variable, or a constant; Pre : A → LP N is
a precondition of a; and Eff : A → LP N is an effect of a.
So far, we have described deterministic actions only. But actions in PFC may have
probabilistic effects as well. Similar to the work by Boutilier et al. (2001), we decompose a
stochastic action into deterministic primitives under nature’s control, referred to as nature’s
choices. We use a relation symbol choice/2 to model nature’s choice. Consider the action
pickup (X, Y ):
choice (pickup (X, Y ), A) ↔
(A = pickupS (X, Y ) ∨ A = pickupF (X, Y )) ,
423

Hölldobler, Karabaev & Skvortsova

{on(b,a), on(a,table)}
{on(c,a), on(a,table), on(b,d)}
{on(b,a), on(a,table), holding(c)}
{on(c,a), on(a,table), on(b,c)}
{on(b,a), on(a,table)}
{on(c,a), on(a,table), on(b,d)}
{on(b,a), on(a,table), holding(c)}
{on(c,a), on(a,table), on(b,c)}

(b)

(a)
{on(b,a), on(a,table)}

{on(c,a), on(a,table), on(b,d)}
{on(b,a), on(a,table), holding(c)}
{on(c,a), on(a,table), on(b,c)}

(c)
Figure 1: (a) Interpretation of the fluent term F = on(X, a) ◦ on(a, table); (b) Bold area is
the interpretation of the abstract state Z 0 = (on(X, a) ◦ on(a, table), {on(Y, X)});
(c) Bold area is the interpretation of the abstract state Z = (on(X, a) ◦
on(a, table), {on(Y, X), holding(X 0 )}).

424

FluCaP: A Heuristic Search Planner for First-Order MDPs

where pickupS (X, Y ) and pickupF (X, Y ) define two nature’s choices for action pickup (X, Y ),
viz., that it succeeds or fails. For example, the nature’s choice pickupS can be defined as
follows:
Pre (pickupS (X, Y )) := (on(X, Y ) ◦ e, {on(W, X)})
Eff (pickupS (X, Y )) := (holding(X), {on(X, Y )}) ,
where the fluent e denotes the empty robot’s gripper. For simplicity, we denote the set of
nature’s choices of an action a as Ch (a) := {aj |choice (a, aj )}. Please note that nowhere
do these action descriptions restrict the domain of discourse to some pre-specified set of
blocks.
For each of nature’s choices aj associated with an action a we define the probability
prob (aj , a, Z) denoting the probability with which one of nature’s choices aj is chosen in a
state Z. For example,
prob (pickupS (X, Y ), pickup (X, Y ), Z) = .75
states that the probability for the successful execution of the pickup action in state Z is
.75.
In the next step, we define the reward function for each state. For example, we might
want to give a reward of 500 to all states in which some block X is on block a and 0,
otherwise:
reward (Z) = 500 ↔ Z v (on(X, a), ∅)
reward (Z) = 0 ↔ Z 6v (on(X, a), ∅) ,
where v denotes the subsumption relation, which will be described in detail in Section 3.2.1.
One should observe that we have specified the reward function without explicit state enumeration. Instead, the state space is divided into two abstract states depending on whether
or not, a block X is on block a. Likewise, value functions can be specified with respect to
the abstract states only. This is in contrast to classical DP algorithms, in which the states
are explicitly enumerated. Action costs can be analogously defined as follows:
cost(pickup (X, Y )) = 3
penalizing the execution of the pickup -action with the value of 3.
Inference Mechanism
Herein, we show how to perform inferences, i.e., compute successors of a given abstract state,
with action schemata directly, avoiding unnecessary grounding. We note that computation
of predecessors can be performed in a similar way.
Let Z = (P, N ) be an abstract state, a(p1 , . . . , pn ) be an action with parameters
p1 , . . . , pn , preconditions Pre (a) = (Pp , Np ) and effects Eff (a) = (Pe , Ne ). Let θ and σ
be substitutions. An action a(p1 , . . . , pn ) is forward applicable, or simply applicable, to Z
with θ and σ, denoted as forward (Z, a, θ, σ), if the following conditions hold:
(f1) (Pp ◦ U1 )θ =AC1 P
(f2) ∀Np ∈ Np .∃N ∈ N .(P ◦ N ◦ U2 )σ =AC1 (P ◦ Np )θ ,
425

Hölldobler, Karabaev & Skvortsova

where U1 and U2 are new AC1-variables and AC1 is the equational theory for ◦ that is
represented by the following system of “associativity”, “commutativity”, and “unit element”
equations:
EAC1 = { (∀X, Y, Z) X ◦ (Y ◦ Z) = (X ◦ Y ) ◦ Z
(∀X, Y ) X ◦ Y = Y ◦ X
(∀X) X ◦ 1 = X
}.
In other words, the conditions (f1) and (f2) guarantee that Z contains both positive and
negative preconditions of the action a. If an action a is forward applicable to Z with θ and
σ then Zsucc = (P 0 , N 0 ), where
P 0 := (Pe ◦ U1 )θ
N 0 := N σ \ Np θ ∪ Ne θ

(1)

is referred to as the a-successor of Z with θ and σ and denoted as succ(Z, a, θ, σ).
For example, consider the action pickupS (X, Y ) as defined above, take Z = (P, N ) =
(on(b, table) ◦ on(X1 , b) ◦ e, {on(X2 , X1 )}). The action pickupS (X, Y ) is forward applicable
to Z with θ = {X 7→ X1 , Y 7→ b, U1 7→ on(b, table)} and σ = {X2 7→ W, U2 7→ 1}. Thus,
Zsucc = succ(Z, pickupS (X, Y ), θ, σ) = (P 0 , N 0 ) with
P 0 = holding(X1 ) ◦ on(b, table) N 0 = {on(X1 , b)} .

3. First-Order LAO*
We present a generalization of the symbolic LAO∗ algorithm by Feng and Hansen (2002),
referred to as first-order LAO∗ (FOLAO∗ ), for solving FOMDPs. Symbolic LAO∗ is a
heuristic search algorithm that exploits state abstraction for solving factored MDPs. Given
an initial state, symbolic LAO∗ uses an admissible heuristic to focus computation on the
parts of the state space that are reachable from the initial state. Moreover, it specifies MDP
components, value functions, policies, and admissible heuristics using propositional ADDs.
This allows symbolic LAO∗ to manipulate sets of states instead of individual states.
Despite the fact that symbolic LAO∗ shows an advantageous behaviour in comparison
to classical non-symbolic LAO∗ by Hansen and Zilberstein (2001) that evaluates states
individually, it suffers from an important drawback. While solving FOMDPs, symbolic
LAO∗ propositionalizes the problem. This approach is impractical for large FOMDPs. Our
intention is to show how to improve the performance of symbolic LAO∗ by providing a
compact first-order representation of MDPs so that the heuristic search can be performed
without propositionalization. More precisely, we propose to switch the representational
formalism for FOMDPs in symbolic LAO∗ from propositional ADDs to Probabilistic Fluent
Calculus. The FOLAO∗ algorithm is presented in Figure 2.
As symbolic LAO∗ , FOLAO∗ has two phases that alternate until a complete solution
is found, which is guaranteed to be optimal. First, it expands the best partial policy and
evaluates the states on its fringe using an admissible heuristic function. Then it performs
dynamic programming on the states visited by the best partial policy, to update their values
and possibly revise the current best partial policy. We note that we focus on partial policies
that map a subcollection of states into actions.
426

FluCaP: A Heuristic Search Planner for First-Order MDPs

policyExpansion(π, S 0 , G)
E := F := ∅
f rom := S 0
repeat S
S
{succ(Z, aj , θ, σ)},
to :=
Z∈f rom aj ∈Ch(a)

where (a, θ, σ) := π(Z)
F := F ∪ (to − G)
E := E ∪ f rom
f rom := to ∩ G − E
until (f rom = ∅)
E := E ∪ F
G := G ∪ F
return (E, F, G)
FOVI(E, A, prob, reward, cost, γ, V )
repeat
V 0 := V
loop for each Z ∈ E
loop for each a ∈ A
loop for each θ, σ such that forward (Z, a, θ, σ)
Q(Z, a, θ,
Pσ) := reward(Z) + cost(a)+
γ
prob(aj , a, Z) · V 0 (succ(Z, aj , θ, σ))
aj ∈Ch(a)

end loop
end loop
V (Z) := max Q(Z, a, θ, σ)
(a,θ,σ)

end loop
V := normalize(V )
r := kV − V 0 k
until stopping criterion
π := extractP olicy(V )
return (V, π, r)
FOLAO∗ (A, prob, reward, cost, γ, S 0 , h, ε)
V := h
G := ∅
For each Z ∈ S 0 , initialize π with an arbitrary action
repeat
(E, F, G) := policyExpansion(π, S 0 , G)
(V, π, r) := FOVI(E, A, prob, reward, cost, γ, V )
until (F = ∅) and r ≤ ε
return (π, V )

Figure 2: First-order LAO∗ algorithm.
In the policy expansion step, we perform reachability analysis to find the set F of states
that have not yet been expanded, but are reachable from the set S 0 of initial states by
following the partial policy π. The set of states G contains states that have been expanded
so far. By expanding a partial policy we mean that it will be defined for a larger set of
states in the dynamic programming step. In symbolic LAO∗ , reachability analysis on ADDs
is performed by means of the image operator from symbolic model checking, that computes
427

Hölldobler, Karabaev & Skvortsova

the set to of successor states following the best current policy. Instead, in FOLAO∗ , we apply
the succ-operator, defined in Equation 1. One should observe that since the reachability
analysis in FOLAO∗ is performed on abstract states that are defined as first-order entities,
the reasoning about successor states is kept on the first-order level. In contrast, symbolic
LAO∗ would first instantiate S 0 with all possible combinations of objects, in order to be
able to perform computations using propositional ADDs later on.
In contrast to symbolic LAO∗ , where the dynamic programming step is performed using
a modified version of SPUDD, we employ a modified first-order value iteration algorithm
(FOVI). The original FOVI by Hölldobler and Skvortsova (2004) performs value iteration
over the entire state space. We modify it so that it computes on states that are reachable
from the initial states, more precisely, on the set E of states that are visited by the best current partial policy. In this way, we improve the efficiency of the original FOVI algorithm by
using reachability analysis together with symbolic dynamic programming. FOVI produces
a PFC representation of value functions and policies by constructing first-order formulae
that partition the state space into abstract states. In effect, it performs value iteration on
top of abstract states, obviating the need for explicit state enumeration.
Given a FOMDP and a value function represented in PFC, FOVI returns the best partial
value function V , the best partial policy π and the residual r. In order to update the values
of the states Z in E, we assign the values from the current value function to the successors
of Z. We compute successors with respect to all nature’s choices aj . The residual r is
computed as the absolute value of the largest difference between the current and the newly
computed value functions V 0 and V , respectively. We note that the newly computed value
function V is taken in its normalized form, i.e., as a result of the normalize procedure that
will be described in Section 3.2.1. Extraction of a best partial policy π is straightforward:
One simply needs to extract the maximizing actions from the best partial value function V .
As with symbolic LAO∗ , FOLAO∗ converges to an ε-optimal policy when three conditions are met: (1) its current policy does not have any unexpanded states, (2) the residual
r is less than the predefined threshold ε, and (3) the value function is initialized with an admissible heuristic. The original convergence proofs for LAO∗ and symbolic LAO∗ by Hansen
and Zilberstein (2001) carry over in a straightforward way to FOLAO∗ .
When calling FOLAO∗ , we initialize the value function with an admissible heuristic
function h that focuses the search on a subset of reachable states. A simple way to create
an admissible heuristic is to use dynamic programming to compute an approximate value
function. Therefore, in order to obtain an admissible heuristic h in FOLAO∗ , we perform
several iterations of the original FOVI. We start the algorithm on an initial value function
that is admissible. Since each step of FOVI preserves admissibility, the resulting value
function is admissible as well. The initial value function assigns the goal reward to each
state thereby overestimating the optimal value, since the goal reward is the maximal possible
reward.
Since all computations of FOLAO∗ are performed on abstract states instead of individual
states, FOMDPs are solved avoiding explicit state and action enumeration and propositionalization. The first-order reasoning leads to better performance of FOLAO∗ in comparison
to symbolic LAO∗ , as shown in Section 4.
428

FluCaP: A Heuristic Search Planner for First-Order MDPs

from = { Z 0 }
Z0

a11

a12

Z1
F,G
Z2

to = { Z 1 , Z 2 }
F = {Z1,Z 2 }
E = { Z 0 , Z 1 , Z 2}
G = {Z1,Z 2 }

from = { Z 0 }

Z1

Z0

G

FOVIA ({ Z 0 , Z 1 , Z 2})

a)

a21

Z2

a22

F
Z3

from = { Z 2 }
Z0

to = { Z 2 , Z 3 }
F = {Z3}
E = {Z0}

b)

to = { Z 4 , Z 5 }
Z 1 F = { Z , Z ,Z }
4
5
3
,
,
{
Z
Z
Z
=
E
2
3 ,Z 4 ,Z 5 }
0
,
,
{
Z
=
Z
Z
G
G
3 ,Z 4 ,Z 5 }
2
1
a11

Z2
Z3

Z4
a12

F

c)

Z5

FOVIA ( { Z 0 , Z 2 , Z 3 , Z 4 , Z 5 } )

Figure 3: Policy Expansion.

3.1 Policy Expansion
The policy expansion step in FOLAO∗ is very similar to the one in the symbolic LAO∗
algorithm. Therefore, we illustrate the expansion procedure by means of an example. Assume that we start from the initial state Z0 and two nondeterministic actions a1 and a2 are
applicable in Z0 , each having two outcomes a11 , a12 and a21 , a22 , respectively. Without loss
of generality, we assume that the current best policy π chooses a1 as an optimal action at
state Z0 . We construct the successors Z1 and Z2 of Z0 with respect to both outcomes a11
and a12 of the action a1 .
The fringe set F as well as the set G of states expanded so far contain the states Z1
and Z2 only, whereas, the set E of states visited by the best current partial policy gets the
state Z0 in addition. See Figure 3a. In the next step, FOVI is performed on the set E. We
assume that the values have been updated in such a way that a2 becomes an optimal action
in Z0 . Thus, the successors of Z0 have to be recomputed with respect to the optimal action
a2 . See Figure 3b.
One should observe that one of the a2 -successors of Z0 , namely Z2 , is an element of the
set G and thus, it has been contained already in the fringe F during the previous expansion
step. Hence, the state Z2 should be expanded and its value recomputed. This is shown
in Figure 3c, where states Z4 and Z5 are a1 -successors of Z2 , under assumption that a1
is an optimal action in Z2 . As a result, the fringe set F contains the newly discovered
states Z3 , Z4 and Z5 and we perform FOVI on E = {Z0 , Z2 , Z3 , Z4 , Z5 }. The state Z1 is
not contained in E, because it does not belong to the best current partial policy, and the
429

Hölldobler, Karabaev & Skvortsova

dynamic programming step is performed only on the states that were visited by the best
current partial policy.
3.2 First-Order Value Iteration
In FOLAO∗ , the first-order value iteration algorithm (FOVI) serves two purposes: First, we
perform several iterations of FOVI in order to create an admissible heuristic h in FOLAO∗ .
Second, in the dynamic programming step of FOLAO∗ , we apply FOVI on the states visited
by the best partial policy in order to update their values and possibly revise the current
best partial policy.
The original FOVI by Hölldobler and Skvortsova (2004) takes a finite state space of
abstract states, a finite set of stochastic actions, real-valued reward and cost functions, and
an initial value function as input. It produces a first-order representation of the optimal
value function and policy by exploiting the logical structure of a FOMDP. Thus, FOVI can
be seen as a first-order counterpart of the classical value iteration algorithm by Bellman
(1957).
3.2.1 Normalization
Following the ideas of Boutilier et al. (2001), FOVI relies on the normalization of the state
space that represents the value function. By normalization of a state space, we mean an
equivalence-preserving procedure that reduces the size of a state space. This would have an
effect only if a state space contains redundant entries, which is usually the case in symbolic
computations.
Although normalization is considered to be an important issue, it has been done by
hand so far. To the best of our knowledge, the preliminary implementation of the approach by Boutilier et al. (2001) performs only rudimentary logical simplifications and the
authors suggest using an automated first-order theorem prover for the normalization task.
Hölldobler and Skvortsova (2004) have developed an automated normalization procedure
for FOVI that, given a state space, delivers an equivalent one that contains no redundancy.
The technique employs the notion of a subsumption relation.
More formally, let Z1 = (P1 , N1 ) and Z2 = (P2 , N2 ) be abstract states. Then Z1 is said
to be subsumed by Z2 , written Z1 v Z2 , if and only if there exist substitutions θ and σ such
that the following conditions hold:
(s1) (P2 ◦ U1 )θ =AC1 P1
(s2) ∀N2 ∈ N2 .∃N1 ∈ N1 .(P1 ◦ N1 ◦ U2 )σ =AC1 (P1 ◦ N2 )θ ,
where U1 and U2 are new AC1-variables. The motivation for the notion of subsumption
on abstract states is inherited from the notion of θ-subsumption between first-order clauses
by Robinson (1965) with the difference that abstract states contain more complicated negative parts in contrast to the first-order clauses.
For example, consider two abstract states Z1 and Z2 that are defined as follows:
Z1 = (on(X1 , a) ◦ on(a, table), {red(Y1 )})
Z2 = (on(X2 , a), {red(X2 )}) ,
430

FluCaP: A Heuristic Search Planner for First-Order MDPs

N
0
1
2
3
4
5
6
7
8
9

Number of states
Supdate
Snorm
9
6
24
14
94
23
129
33
328
39
361
48
604
52
627
54
795
56
811
59

Time, msec
Update Norm
144
1
393
3
884
12
1377
16
2079
46
2519
51
3268
107
3534
110
3873
157
4131
154

Runtime, msec

Runtime w/o norm, msec

145
396
896
1393
2125
2570
3375
3644
4030
4285

144
593
2219
13293
77514
805753
n/a
n/a
n/a
n/a

Table 1: Representative timing results for first ten iterations of FOVI.
where Z1 informally asserts that some block X1 is on the block a which is on the table and
no blocks are red. Whereas Z2 informally states that some block X2 is on the block a and
X2 is not red. We show that Z1 v Z2 . The relation holds since both conditions (s1) and
(s2) are satisfied. Indeed,
(on(X2 , a) ◦ U1 )θ =AC1 on(X1 , a) ◦ on(a, table)
and
(on(X1 , a) ◦ on(a, table) ◦ red(Y1 ) ◦ U2 )σ = (on(X1 , a) ◦ on(a, table) ◦ red(X2 ))θ
with θ = {X2 7→ X1 , U1 7→ on(a, table)} and σ = {Y1 7→ X1 , U2 7→ 1}.
One should note that subsumption in the language of abstract states inherits the complexity bounds of θ-subsumption (Kapur & Narendran, 1986). Namely, deciding subsumption between two abstract states is NP-complete, in general. However, Karabaev et al.
(2006) have recently developed an efficient algorithm that delivers all solutions of the subsumption problem for the case where abstract states are fluent terms.
For the purpose of normalization, it is convenient to represent the value function as a
set of pairs of the form hZ, αi, where Z is an abstract state and α is a real value. In essence,
the normalization algorithm can be seen as an exhaustive application of the following simplification rule to the value function V .
hZ1 , αi hZ2 , αi
Z1 v Z2
hZ2 , αi
Table 1 illustrates the importance of the normalization algorithm by providing some representative timing results for the first ten iterations of FOVI. The experiments were carried
out on the problem taken from the colored Blocksworld scenario consisting of ten blocks.
Even on such a relatively simple problem FOVI with the normalization switched off does
not scale beyond the sixth iteration.
The results in Table 1 demonstrate that the normalization during some iteration of
FOVI dramatically shrinks the computational effort during the next iterations. The columns
labelled Supdate and Snorm show the size of the state space after performing the value updates
431

Hölldobler, Karabaev & Skvortsova

and the normalization, respectively. For example, the normalization factor, i.e., the ratio
between the number Supdate of states obtained after performing one update step and the
number Snorm of states obtained after performing the normalization step, at the seventh
iteration is 11.6. This means that more than ninety percent of the state space contained
redundant information. The fourth and fifth columns in Table 1 contain the time Update
and Norm spent on performing value updates and on the normalization, respectively. The
total runtime Runtime, when the normalization is switched on, is given in the sixth column.
The seventh column labelled Runtime w/o norm depicts the total runtime of FOVI when the
normalization is switched off. If we would sum up all values in the seventh column and the
values in the sixth column up to the sixth iteration inclusively, subtract the latter from the
former and divide the result by the total time Norm needed for performing normalization
during the first six iterations, then we would obtain the normalization gain of about three
orders of magnitude.

4. Experimental Evaluation
We demonstrate the advantages of combining the heuristic search together with first-order
state abstraction on a system, referred to as FluCaP, that has successfully entered the
probabilistic track of the 2004 International Planning Competition (IPC’2004). The experimental results were all obtained using RedHat Linux running on a 3.4GHz Pentium IV
machine with 3GB of RAM.
In Table 2, we present the performance comparison of FluCaP together with symbolic
LAO∗ on examples taken from the colored Blocksworld (BW) scenario that was introduced
during IPC’2004.
Our main objective was to investigate whether first-order state abstraction using logic
could improve the computational behaviour of a planning system for solving FOMDPs. The
colored BW problems were our main interest since they were the only ones represented in
first-order terms and hence the only ones that allowed us to make use of the first-order
state abstraction. Therefore, we have concentrated on the design of a domain-dependent
planning system that was tuned for the problems taken from the Blocksworld scenario.
The colored BW problems differ from the classical BW ones in that, along with the
unique identifier, each block is assigned a specific color. A goal formula, specified in firstorder terms, provides an arrangement of colors instead of an arrangement of blocks.
At the outset of solving a colored BW problem, symbolic LAO∗ starts by propositionalizing its components, namely, the goal statement and actions. Only after that, the abstraction
using propositional ADDs is applied. In contrast, FluCaP performs first-order abstraction on a colored BW problem directly, avoiding unnecessary grounding. In the following,
we show how an abstraction technique affects the computation of a heuristic function. To
create an admissible heuristic, FluCaP performs twenty iterations of FOVI and symbolic
LAO∗ performs twenty iterations of an approximate value iteration algorithm similar to
APRICODD by St-Aubin, Hoey, and Boutilier (2000). The columns labelled H.time and
NAS show the time needed for computing a heuristic function and the number of abstract
states it covers, respectively. In comparison to FluCaP, symbolic LAO∗ needs to evaluate
fewer abstract states in the heuristic function but takes considerably more time. One can
432

FluCaP: A Heuristic Search Planner for First-Order MDPs

FluCaP

FluCaP

31.1
8.7
25.1
9.5
16.5
12.7
285.4
76.7
128.5 85.0
63.3
135.0
n/a
757.0
2813
718.3
443.6 1241
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a

LAO*

LAO*

FluCaP–

FOVI

FluCaP

LAO*

FluCaP–

494 22.3 22.0 23.4
496 23.1 17.8 22.7
495 27.3 11.7 15.7
493 137.6 78.5 261.6
492 150.5 33.0 119.1
496 221.3 16.6 56.4
491 1644 198.1 2776
494 1265 161.6 1809
494 2210 27.3 317.7
n/a n/a 1212 n/a
n/a n/a 598.5 n/a
n/a n/a 215.3 1908
n/a n/a 1809 n/a
n/a n/a 3548 n/a

4.2
1.3
0.3
21.0
9.3
1.2
171.3
143.6
12.3
804.1
301.2
153.2
1733
1751

35
34
32
68
82
46
143
112
101
n/a
n/a
n/a
n/a
n/a

410
172
55
1061
539
130
2953
2133
425
8328
3956
2019
7276
15225

1077
687
278
3847
1738
902
12014
7591
2109
n/a
n/a
7251
n/a
n/a

0.86
0.86
0.86
7.05
7.05
7.05
65.9
65.9
65.9
n/a
n/a
n/a
n/a
n/a

0.82
0.68
0.66
4.24
6.50
6.24
23.6
51.2
61.2
66.6
379.7
1121
1.2 · 107
2.5 · 107

2.7
2.1
1.9
3.1
2.3
2.0
3.5
2.4
2.0
4.1
3.0
2.3
5.7
6.1

Table 2: Performance comparison of FluCaP (denoted as FluCaP) and symbolic LAO∗
(denoted as LAO*), where the cells n/a denote the fact that a planner did not
deliver a solution within the time limit of one hour. NAS and NGS are number of
abstract and ground states, respectively.

conclude that abstract states in symbolic LAO∗ enjoy more complex structure than those
in FluCaP.
We note that, in comparison to FOVI, FluCaP restricts the value iteration to a smaller
state space. Intuitively, the value function, which is delivered by FOVI, covers a larger
state space, because the time that is allocated for the heuristic search in FluCaP is now
used for performing additional iterations in FOVI. The results in the column labelled %
justify that the harder the problem is (that is, the more colors it contains), the higher the
percentage of runtime spent on normalization. Almost on all test problems, the effort spent
on normalization takes three percent of the total runtime on average.
In order to compare the heuristic accuracy, we present in the column labelled NGS the
number of ground states which the heuristic assigns non-zero values to. One can see that the
heuristics returned by FluCaP and symbolic LAO∗ have similar accuracy, but FluCaP
takes much less time to compute them. This reflects the advantage of the plain first-order
abstraction in comparison to the marriage of propositionalization with abstraction using
propositional ADDs. In some examples, we gain several orders of magnitude in H.time.
The column labelled Total time presents the time needed to solve a problem. During this
time, a planner must execute 30 runs from an initial state to a goal state. A one-hour block
is allocated for each problem. We note that, in comparison to FluCaP, the time required
by heuristic search in symbolic LAO∗ (i.e., difference between Total time and H.time) grows
considerably faster in the size of the problem. This reflects the potential of employing
433

%

FOVI

15
17

494
495
495
493
493
495
491
494
494
n/a
n/a
n/a
n/a
n/a

NGS, ×103

NAS
FluCaP

8

494
495
495
493
492
494
491
494
494
490
490
492
486
481

H.time, sec.
LAO*

7

494
496
496
493
493
495
492
494
494
n/a
n/a
n/a
n/a
n/a

Total time, sec.

FluCaP

6

FOVI

5

C
4
3
2
4
3
2
4
3
2
4
3
2
3
4

FluCaP

B

Total av. reward
LAO*

Problem

Hölldobler, Karabaev & Skvortsova

B
20
22
24
26
28
30
32
34
36

Total av. reward, ≤500
489.0
487.4
492.0
482.8
493.0
491.2
476.0
475.6
n/a

Total time, sec.
137.5
293.8
757.3
817.0
2511.3
3580.4
3953.8
3954.1
n/a

H.time, sec.
56.8
110.2
409.8
117.2
823.3
1174.0
781.8
939.4
n/a

NAS
711
976
1676
1141
2832
4290
2811
3248
n/a

NGS × 1021
1.7
1.1 × 103
1.0 × 106
4.6 × 108
8.6 × 1011
1.1 × 1015
7.4 × 1017
9.6× 1020
n/a

Table 3: Performance of FluCaP on larger instances of one-color Blocksworld problems,
where the cells n/a denote the fact that a planner did not deliver a solution within
the time limit.

first-order abstraction instead of abstraction based on propositional ADDs during heuristic
search.
The average reward obtained over 30 runs, shown in column Total av. reward, is the
planner’s evaluation score. The reward value close to 500 (which is the maximum possible
reward) simply indicates that a planner found a reasonably good policy. Each time the
number of blocks B increases by 1, the running time for symbolic LAO∗ increases roughly
10 times. Thus, it could not scale to problems having more than seven blocks. This is
in contrast to FluCaP which could solve problems of seventeen blocks. We note that
the number of colors C in a problem affects the efficiency of an abstraction technique. In
FluCaP, as C decreases, the abstraction rate increases which, in turn, is reflected by the
dramatic decrease in runtime. The opposite holds for symbolic LAO∗ .
In addition, we compare FluCaP with two variants. The first one, denoted as FOVI,
performs no heuristic search at all, but rather, employs FOVI to compute the ε-optimal
total value function from which a policy is extracted. The second one, denoted as FluCaP– ,
performs ‘trivial’ heuristic search starting with an initial value function as an admissible
heuristic. As expected, FluCaP that combines heuristic search and FOVI demonstrates
an advantage over plain FOVI and trivial heuristic search. These results illustrate the
significance of heuristic search in general (FluCaP vs. FOVI) and the importance of heuristic
accuracy, in particular (FluCaP vs. FluCaP– ). FOVI and FluCaP– do not scale to problems
with more than seven blocks.
Table 3 presents the performance results of FluCaP on larger instances of one-color
BW problems with the number of blocks varying from twenty to thirty four. We believe that
FluCaP does not scale to problems of larger size because the implementation is not yet
well optimized. In general, we believe that the FluCaP system should not be as sensitive
to the size of a problem as propositional planners are.
Our experiments were targeted at the one-color problems only because they are, on the
one hand, the simplest ones for us and, on the other hand, the bottleneck for propositional
planners. The structure of one-color problems allows us to apply first-order state abstraction in its full power. For example, for a 34-blocks problem FluCaP operates on about
3.3 thousand abstract states that explode to 9.6 × 1041 individual states after proposition434

FluCaP: A Heuristic Search Planner for First-Order MDPs

Total av. reward, ≤500
UMass

Michigan

Purdue1

Purdue2

Purdue3

Caracas

Toulouse

C
3
3
5
0
0
0
0
0
0

Dresden

B
5
8
11
5
8
11
15
18
21

Canberra

Problem

494.6
486.5
479.7
494.6
489.7
479.1
467.5
351.8
285.7

496.4
492.8
486.3
494.6
489.9
n/a
n/a
n/a
n/a

n/a
n/a
n/a
494.8
n/a
n/a
n/a
n/a
n/a

n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a

496.5
486.6
481.3
494.1
488.7
480.3
469.4
462.4
455.7

496.5
486.4
481.5
494.6
490.3
479.7
467.7
-54.9
455.1

495.8
487.2
481.9
494.4
490
481.1
486.3
n/a
459

n/a
n/a
n/a
494.9
488.8
465.7
397.2
n/a
n/a

n/a
n/a
n/a
494.1
n/a
n/a
n/a
n/a
n/a

Table 4: Official competition results for colored and non-colored Blocksworld scenarios.
May, 2004. The n/a-entries in the table indicate that either a planner was not
successful in solving a problem or did not attempt to solve it.

alization. A propositional planner must be highly optimized in order to cope with this
non-trivial state space.
We note that additional colors in larger instances (more than 20 blocks) of BW problems
cause dramatic increase in computational time, so we consider these problems as being
unsolved. One should also observe that the number of abstract states NAS increases with
the number of blocks non-monotonically because the problems are generated randomly. For
example, the 30-blocks problem happens to be harder than the 34-blocks one. Finally, we
note that all results that appear in Tables 2 and 3 were obtained by using the new version of
the evaluation software that does not rely on propositionalization in contrast to the initial
version that was used during the competition.
Table 4 presents the competition results from IPC’2004, where FluCaP was competitive
in comparison with other planners on colored BW problems. FluCaP did not perform
well on non-colored BW problems because these problems were propositional ones (that
is, goal statements and initial states are ground) and FluCaP does not yet incorporate
optimization techniques applied in modern propositional planners. The contestants are
indicated by their origin. For example, Dresden - FluCaP, UMass - symbolic LAO∗ etc.
Because only the pickup action has cost 1, the gain of five points in total reward means
that the plan contains ten fewer actions on average. The competition domains and log files
are available in an online appendix of Younes, Littman, Weissman, and Asmuth (2005).
Although the empirical results that are presented in this work were obtained on the
domain-dependent version of FluCaP, we have recently developed in (Karabaev et al.,
2006) an efficient domain-independent inference mechanism that is the core of a domainindependent version of FluCaP.
435

Hölldobler, Karabaev & Skvortsova

5. Related Work
We follow the symbolic DP (SDP) approach within Situation Calculus (SC) of Boutilier
et al. (2001) in using first-order state abstraction for FOMDPs. One difference is in the
representation language: We use PFC instead of SC. In the course of symbolic value iteration, a state space may contain redundant abstract states that dramatically affect the
algorithm’s efficiency. In order to achieve computational savings, normalization must be performed to remove this redundancy. However, in the original work by Boutilier et al. (2001)
this was done by hand. To the best of our knowledge, the preliminary implementation of
the SDP approach within SC uses human-provided rewrite rules for logical simplification.
In contrast, Hölldobler and Skvortsova (2004) have developed an automated normalization
procedure for FOVI that is incorporated in the competition version of FluCaP and brings
the computational gain of several orders of magnitude. Another crucial difference is that our
algorithm uses heuristic search to limit the number of states for which a policy is computed.
The ReBel algorithm by Kersting, van Otterlo, and De Raedt (2004) relates to FOLAO∗
in that it also uses a representation language that is simpler than Situation Calculus. This
feature makes the state space normalization computationally feasible.
In motivation, our approach is closely connected to Relational Envelope-based Planning
(REBP) by Gardiol and Kaelbling (2003) that represents MDP dynamics by a compact set
of relational rules and extends the envelope method by Dean et al. (1995). However, REBP
propositionalizes actions first, and only afterwards employs abstraction using equivalenceclass sampling. In contrast, FOLAO∗ directly applies state and action abstraction on the
first-order structure of an MDP. In this respect, REBP is closer to symbolic LAO∗ than to
FOLAO∗ . Moreover, in contrast to PFC, action descriptions in REBP do not allow negation
to appear in preconditions or in effects. In organization, FOLAO∗ , as symbolic LAO∗ , is
similar to real-time DP by Barto et al. (1995) that is an online search algorithm for MDPs.
In contrast, FOLAO∗ works offline.
All the above algorithms can be classified as deductive approaches to solving FOMDPs.
They can be characterized by the following features: (1) they are model-based, (2) they
aim at exact solutions, and (3) logical reasoning methods are used to compute abstractions.
We should note that FOVI aims at exact solution for a FOMDP, whereas FOLAO∗ , due
to the heuristic search that avoids evaluating all states, seeks for an approximate solution.
Therefore, it would be more appropriate to classify FOLAO∗ as an approximate deductive
approach to FOMDPs.
In another vein, there is some research on developing inductive approaches to solving
FOMDPs, e.g., by Fern, Yoon, and Givan (2003). The authors propose the approximate
policy iteration (API) algorithm, where they replace the use of cost-function approximations
as policy representations in API with direct, compact state-action mappings, and use a
standard relational learner to learn these mappings. In effect, Fern et al. provide policylanguage biases that enable solution of very large relational MDPs. All inductive approaches
can be characterized by the following features: (1) they are model-free, (2) they aim at
approximate solutions, and (3) an abstract model is used to generate biased samples from
the underlying FOMDP and the abstract model is altered based on them.
A recent approach by Gretton and Thiebaux (2004) proposes an inductive policy construction algorithm that strikes a middle-ground between deductive and inductive tech436

FluCaP: A Heuristic Search Planner for First-Order MDPs

niques. The idea is to use reasoning, in particular first-order regression, to automatically
generate a hypothesis language, which is then used as input by an inductive solver. The
approach by Gretton and Thiebaux is related to SDP and to our approach in the sense that
a first-order domain specification language as well as logical reasoning are employed.

6. Conclusions
We have proposed an approach that combines heuristic search and first-order state abstraction for solving FOMDPs more efficiently. Our approach can be seen as two-fold:
First, we use dynamic programming to compute an approximate value function that serves
as an admissible heuristic. Then heuristic search is performed to find an exact solution
for those states that are reachable from the initial state. In both phases, we exploit the
power of first-order state abstraction in order to avoid evaluating states individually. As
experimental results show, our approach breaks new ground in exploring the efficiency of
first-order representations in solving MDPs. In comparison to existing MDP planners that
must propositionalize the domain, e.g., symbolic LAO∗ , our solution scales better on larger
FOMDPs.
However, there is plenty remaining to be done. For example, we are interested in the
question of to what extent the optimization techniques applied in modern propositional
planners can be combined with first-order state abstraction. In future competitions, we
would like to face problems where the goal and/or initial states are only partially defined
and where the underlying domain contains infinitely many objects.
The current version of FOLAO∗ is targeted at the problems that allow for efficient
first-order state abstraction. More precisely, these are the problems that can be polynomially translated into PFC. For example in the colored BW domain, existentially-closed
goal descriptions were linearly translated into the equivalent PFC representation. Whereas
universally-closed goal descriptions would require full propositionalization. Thus, the current version of PFC is less first-order expressive than, e.g., Situation Calculus. In the future,
it would be interesting to study the extensions of the PFC language, in particular, to find
the trade-off between the PFC’s expressive power and the tractability of solution methods
for FOMDPs based on PFC.

Acknowledgements
We are very grateful to all anonymous reviewers for the thorough reading of the previous versions of this paper. We also thank Zhengzhu Feng for fruitful discussions and for
providing us with the executable of the symbolic LAO∗ planner. We greatly appreciate
David E. Smith for his patience and encouragement. His valuable comments have helped
us to improve this paper. Olga Skvortsova was supported by a grant within the Graduate Programme GRK 334 “Specification of discrete processes and systems of processes
by operational models and logics” under auspices of the Deutsche Forschungsgemeinschaft
(DFG).
437

Hölldobler, Karabaev & Skvortsova

References
Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning to act using real-time dynamic
programming. Artificial Intelligence, 72 (1-2), 81–138.
Bellman, R. E. (1957). Dynamic programming. Princeton University Press, Princeton, NJ,
USA.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions and computational leverage. Journal of Artificial Intelligence Research,
11, 1–94.
Boutilier, C., Reiter, R., & Price, B. (2001). Symbolic Dynamic Programming for FirstOrder MDPs. In Nebel, B. (Ed.), Proceedings of the Seventeenth International Conference on Artificial Intelligence (IJCAI’2001), pp. 690–700. Morgan Kaufmann.
Dean, T., Kaelbling, L., Kirman, J., & Nicholson, A. (1995). Planning under time constraints
in stochastic domains. Artificial Intelligence, 76, 35–74.
Feng, Z., & Hansen, E. (2002). Symbolic heuristic search for factored Markov Decision Processes. In Dechter, R., Kearns, M., & Sutton, R. (Eds.), Proceedings of the Eighteenth
National Conference on Artificial Intelligence (AAAI’2002), pp. 455–460, Edmonton,
Canada. AAAI Press.
Fern, A., Yoon, S., & Givan, R. (2003). Approximate policy iteration with a policy language
bias. In Thrun, S., Saul, L., & Schölkopf, B. (Eds.), Proceedings of the Seventeenth Annual Conference on Neural Information Processing Systems (NIPS’2003), Vancouver,
Canada. MIT Press.
Gardiol, N., & Kaelbling, L. (2003). Envelope-based planning in relational MDPs. In Thrun,
S., Saul, L., & Schölkopf, B. (Eds.), Proceedings of the Seventeenth Annual Conference
on Neural Information Processing Systems (NIPS’2003), Vancouver, Canada. MIT
Press.
Gretton, C., & Thiebaux, S. (2004). Exploiting first-order regression in inductive policy
selection. In Chickering, M., & Halpern, J. (Eds.), Proceedings of the Twentieth Conference on Uncertainty in Artificial Intelligence (UAI’2004), Banff, Canada. Morgan
Kaufmann.
Hansen, E., & Zilberstein, S. (2001). LAO*: A heuristic search algorithm that finds solutions
with loops. Artificial Intelligence, 129, 35–62.
Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (1999). SPUDD: Stochastic Planning using
Decision Diagrams. In Laskey, K. B., & Prade, H. (Eds.), Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI’1999), pp. 279–288,
Stockholm. Morgan Kaufmann.
Hölldobler, S., & Schneeberger, J. (1990). A new deductive approach to planning. New
Generation Computing, 8, 225–244.
Hölldobler, S., & Skvortsova, O. (2004). A Logic-Based Approach to Dynamic Programming.
In Proceedings of the Workshop on “Learning and Planning in Markov Processes–
Advances and Challenges” at the Nineteenth National Conference on Artificial Intelligence (AAAI’04), pp. 31–36, San Jose, CA. AAAI Press.
438

FluCaP: A Heuristic Search Planner for First-Order MDPs

Kapur, D., & Narendran, P. (1986). NP-completeness of the set unification and matching
problems. In Siekmann, J. H. (Ed.), Proceedings of the Eighth International Conference in Automated Deduction (CADE’1986), pp. 489–495, Oxford, England. Springer
Verlag.
Karabaev, E., Rammé, G., & Skvortsova, O. (2006). Efficient symbolic reasoning for firstorder MDPs. In Proceedings of the Workshop on “Planning, Learning and Monitoring
with Uncertainty and Dynamic Worlds” at the Seventeenth European Conference on
Artificial Intelligence (ECAI’2006), Riva del Garda, Italy. To appear.
Kersting, K., van Otterlo, M., & De Raedt, L. (2004). Bellman goes relational. In Brodley,
C. E. (Ed.), Proceedings of the Twenty-First International Conference in Machine
Learning (ICML’2004), pp. 465–472, Banff, Canada. ACM.
Puterman, M. L. (1994). Markov Decision Processes - Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., New York, NY.
Robinson, J. (1965). A machine-learning logic based on the resolution principle. Journal of
the Association for Computing Machinery, 12 (1), 23–41.
St-Aubin, R., Hoey, H., & Boutilier, C. (2000). APRICODD: Approximate policy construction using decision diagrams. In Leen, T. K., Dietterich, T. G., & Tresp, V. (Eds.),
Proceedings of the Fourteenth Annual Conference on Neural Information Processing
Systems (NIPS’2000), pp. 1089–1095, Denver. MIT Press.
Younes, H., Littman, M., Weissman, D., & Asmuth, J. (2005). The first probabilistic track
of the International Planning Competition. Journal of Artificial Intelligence Research,
24, 851–887.

439

Journal of Artificial Intelligence Research 27 (2006) 235–297

Submitted 03/06; published 10/06

Modelling Mixed Discrete-Continuous Domains for Planning
Maria Fox
Derek Long

maria.fox@cis.strath.ac.uk
derek.long@cis.strath.ac.uk

Department of Computer and Information Sciences
University of Strathclyde,
26 Richmond Street, Glasgow, G1 1XH, UK

Abstract
In this paper we present pddl+, a planning domain description language for modelling
mixed discrete-continuous planning domains. We describe the syntax and modelling style
of pddl+, showing that the language makes convenient the modelling of complex timedependent effects. We provide a formal semantics for pddl+ by mapping planning instances
into constructs of hybrid automata. Using the syntax of HAs as our semantic model
we construct a semantic mapping to labelled transition systems to complete the formal
interpretation of pddl+ planning instances.
An advantage of building a mapping from pddl+ to HA theory is that it forms a bridge
between the Planning and Real Time Systems research communities. One consequence
is that we can expect to make use of some of the theoretical properties of HAs. For
example, for a restricted class of HAs the Reachability problem (which is equivalent to
Plan Existence) is decidable.
pddl+ provides an alternative to the continuous durative action model of pddl2.1,
adding a more flexible and robust model of time-dependent behaviour.

1. Introduction
This paper describes pddl+, an extension of the pddl (McDermott & the AIPS’98 Planning Competition Committee, 1998; Fox & Long, 2003; Hoffmann & Edelkamp, 2005) family
of deterministic planning modelling languages. pddl+ is intended to support the representation of mixed discrete-continuous planning domains. pddl was developed by McDermott (McDermott & the AIPS’98 Planning Competition Committee, 1998) as a standard
modelling language for planning domains. It was later extended (Fox & Long, 2003) to
allow temporal structure to be modelled under certain restricting assumptions. The resulting language, pddl2.1, was further extended to include domain axioms and timed initial
literals, resulting in pddl2.2 (Hoffmann & Edelkamp, 2005). In pddl2.1, durative actions
with fixed-length duration and discrete effects can be modelled. A limited capability to
model continuous change within the durative action framework is also provided.
pddl+ provides a more flexible model of continuous change through the use of autonomous processes and events. The modelling of continuous processes has also been considered by McDermott (2005), Herrmann and Thielscher (1996), Reiter (1996), Shanahan (1990), Sandewall (1989) and others in the knowledge representation and reasoning
communities, as well as by Henzinger (1996), Rasmussen, Larsen and Subramani (2004),
Haroud and Faltings (1994) and others in the real time systems and constraint-reasoning
communities.
c
2006
AI Access Foundation. All rights reserved.

Fox & Long

The most frequently used subset of pddl2.1 is the fragment modelling discretised change.
This is the part used in the 3rd International Planning Competition and used as the basis
of pddl2.2. The continuous modelling constructs of pddl2.1 have not been adopted by
the community at large, partly because they are not considered an attractive or natural
way to represent certain kinds of continuous change (McDermott, 2003a; Boddy, 2003). By
wrapping up continuous change inside durative actions pddl2.1 forces episodes of change on
a variable to coincide with logical state changes. An important limitation of the continuous
durative actions of pddl2.1 is therefore that the planning agent must take full control over
all change in the world, so there can be no change without direct action on the part of the
agent.
The key extension that pddl+ provides is the ability to model the interaction between
the agent’s behaviour and changes that are initiated by the world. Processes run over time
and have a continuous effect on numeric values. They are initiated and terminated either
by the direct action of the agent or by events triggered in the world. We refer to this
three-part structure as the start-process-stop model. We make a distinction between logical
and numeric state, and say that transitions between logical states are instantaneous whilst
occupation of a given logical state can endure over time. This approach takes a transition
system view of the modelling of change and allows a direct mapping to the languages of
the real time systems community where the same modelling approach is used (Yi, Larsen,
& Pettersson, 1997; Henzinger, 1996).
In this paper we provide a detailed discussion of the features of pddl+, and the reasons
for their addition. We develop a formal semantics for our primitives in terms of a formal
mapping between pddl+ and Henzinger’s theory of hybrid automata (Henzinger, 1996).
Henzinger provides the formal semantics of HAs by means of the labelled transition system.
We therefore adopt the labelled transition semantics for planning instances by going through
this route. We explain what it means for a plan to be valid by showing how a plan can be
interpreted as an accepting run through the corresponding labelled transition system.
We note that, under certain constraints, the Plan Existence problem for pddl+ planning
instances (which corresponds to the Reachability problem for the corresponding hybrid automaton) remains decidable. We discuss these constraints and their utility in the modelling
of mixed discrete-continuous planning problems.

2. Motivation
Many realistic contexts in which planning can be applied feature a mixture of discrete and
continuous behaviours. For example, the management of a refinery (Boddy & Johnson,
2004), the start-up procedure of a chemical plant (Aylett, Soutter, Petley, Chung, & Edwards, 2001), the control of an autonomous vehicle (Léauté & Williams, 2005) and the
coordination of the activities of a planetary lander (Blake et al., 2004) are problems for
which reasoning about continuous change is fundamental to the planning process. These
problems also contain discrete change which can be modelled through traditional planning
formalisms. Such situations motivate the need to model mixed discrete-continuous domains
as planning problems.
236

Modelling Mixed Discrete-Continuous Domains for Planning

We present two motivating examples to demonstrate how discrete and continuous behaviours can interact to yield interesting planning problems. These are Boddy and Johnson’s
petroleum refinery domain and the battery power model of Beagle 2.
2.1 Petroleum refinery production planning
Boddy and Johnson (2004) describe a planning and scheduling problem arising in the management of petroleum refinement operations. The objects of this problem include materials,
in the form of hydrocarbon mixtures and fractions, tanks and processing units. During the
operation of the refinery the mixtures and fractions pass through a series of processing
units including distillation units, desulphurisation units and cracking units. Inside these
units they are converted and combined to produce desired materials and to remove waste
products. Processes include the filling and emptying of tanks, which in some cases can
happen simultaneously on the same tank, treatment of materials and their transfer between
tanks. The continuous components of the problem include process unit control settings,
flow volumes and rates, material properties and volumes and the time-dependent properties
of materials being combined in tanks as a consequence of refinement operations.
An example demonstrating the utility of a continuous model arises in the construction
of a gasoline blend. The success of a gasoline blend depends on the chemical balance of
its constituents. Blending results from materials being pumped into and out of tanks and
pipelines at rates which enable the exact quantities of the required chemical constituents
to be controlled. For example, when diluting crude oil with a less sulphrous material the
rate of in-flow of the diluting material, and its volume in the tank, have to be balanced by
out-flow of the diluted crude oil and perhaps by other refinement operations.
Boddy and Johnson treat the problem of planning and scheduling refinery operations as
an optimisation problem. Approximations based on discretisation lead to poor solutions,
leading to a financial motivation for Boddy and Johnson’s application. As they observe, a
moderately large refinery can produce in the order of half a million barrels per day. They
calculate that a 1% decrease in efficiency, resulting from approximation, could result in the
loss of a quarter of a million dollars per day. The more accurate the model of the continuous
dynamics the more efficient and cost-effective the refinery.
Boddy and Johnson’s planning and scheduling approach is based on dynamic constraint
satisfaction involving continuous, and non-linear, constraints. A domain-specific solver was
constructed, demonstrating that direct handling of continuous problem components can
be realistic. Boddy and Johnson describe applying their solver to a real problem involving 18,000 continuous constraints including 2,700 quadratic constraints, 14,000 continuous
variables and around 40 discrete decisions (Lamba, Dietz, Johnson, & Boddy, 2003; Boddy
& Johnson, 2002). It is interesting to observe that this scale of problem is solvable, to
optimality, with reasonable computational effort.
2.2 Planning Activities for a Planetary Lander
Beagle 2, the ill-fated probe intended for the surface of Mars, was designed to operate within
tight resource constraints. The constraint on payload mass, the desire to maximise science
return and the rigours of the hostile Martian environment combine to make it essential to
squeeze high performance from the limited energy and time available during its mission.
237

Fox & Long

One of the tightest constraints on operations is that of energy. On Beagle 2, energy was
stored in a battery, recharged from solar power and consumed by instruments, the on-board
processor, communications equipment and a heater required to protect sensitive components
from the extreme cold over Martian nights. These features of Beagle 2 are common to all
deep space planetary landers.
The performance of the battery and the solar panels are both subject to variations due
to ageing, atmospheric dust conditions and temperature. Nevertheless, with long periods
between communication windows, a lander can only achieve dense scientific data-gathering
if its activities are carefully planned and this planning must be performed against a nominal
model of the behaviour of battery, solar panels and instruments. The state of charge of the
battery of the lander falls within an envelope defined by the maximum level of the capacity
of the battery and the minimum level dictated by the safety requirements of the lander.
This safety requirement ensures there is enough power at nightfall to power the heater
through night operations and to achieve the next communications session.
All operations change the state of battery charge, causing it to follow a continuous
curve within this envelope. In order to achieve a dense performance, the operations of
the lander must be pushed into the envelope as tightly as possible. The equations that
govern the physical behaviour of the energy curve are complex, but an approximation of
them is possible that is both tractable and more accurate than a discretised model of the
curve would be. As in the refinery domain, any approximation has a cost: the coarser the
approximation of the model, the less accurately it is possible to determine the limits of the
performance of a plan.
In this paper we refer to a simplified model of this domain, which we call the Planetary
Lander Domain. The details of this model are presented in Appendix C, and discussed in
Section 4.3.

2.3 Remarks
In these two examples plans must interact with the background continuous behaviours that
are triggered by the world. In the refinery domain concurrent episodes of continuous change
(such as the filling and emptying of a tank) affect the same variable (such as the sulphur
content of the crude oil in the tank), and the flow into and out of the tank must be carefully
controlled to achieve a mixture with the right chemical composition. In the Beagle 2 domain
the power generation and consumption processes act concurrently on the power supply in
a way that must be controlled to avoid the supply dropping below the critical minimal
threshold. In both domains the continuous processes are subject to discontinuous first
derivative effects, resulting from events being triggered, actions being executed or processes
interacting. When events trigger the discontinuities might not coincide with the end-points
of actions. A planner needs an explicit model of how such events might be triggered in
order to be able to reason about their effects.
We argue that discretisation represents an inappropriate simplification of these domains,
and that adequate modelling of the continuous dynamics is necessary to capture their critical
features for planning.
238

Modelling Mixed Discrete-Continuous Domains for Planning

3. Layout of the Paper
In Section 4 we explain how pddl+ builds on the foundations of the pddl family of languages. We describe the syntactic elements that are new to pddl+ and we remind the
reader of the representation language used for expressing temporal plans in the family. We
develop a detailed example of a domain, the battery power model of a planetary lander,
in which continuous modelling is required to properly capture the behaviours with which a
plan must interact. We complete this section with a formal proof showing that pddl+ is
strictly more expressive than pddl2.1.
In Section 5 we explain why the theory of hybrid automata is relevant to our work,
and we provide the key automaton constructs that we will use in the development of the
semantics of pddl+. In Section 6 we present the mapping from planning instances to HAs.
In doing this we are using the syntactic constructs of the HA as our semantic model. In
Section 7 we discuss the subset of HAs for which the Reachability problem is decidable, and
why we might be interested in these models in the context of planning. We conclude the
paper with a discussion of related work.

4. Formalism
In this section we present the syntactic foundations of pddl+, clarifying how they extend the
foregoing line of development of the pddl family of languages. We rely on the definitions
of the syntactic structures of pddl2.1, which we call the Core Definitions. These were
published in 2003 (Fox & Long, 2003) but we repeat them in Appendix A for ease of
reference.
pddl+ includes the timed initial literal construct of pddl2.2 (which provides a syntactically convenient way of expressing the class of events that can be predicted from the
initial state). Although derived predicates are a powerful modelling concept, they have not
so far been included in pddl+. Further work is required to explore the relationship between
derived predicates and the start-process-stop model and we do not consider this further in
this paper.
4.1 Syntactic Foundations
pddl+ builds directly on the discrete fragment of pddl2.1: that is, the fragment containing fixed-length durative actions. This is supplemented with the timed initial literals of
pddl2.2 (Hoffmann & Edelkamp, 2005). It introduces two new constructs: events and processes. These are represented by similar syntactic frames to actions. The elements of the
formal syntax that are relevant are given below (these are to be read in conjunction with
the BNF description of pddl2.1 given in Fox & Long, 2003).
<structure-def>
<structure-def>

::=:events <event-def>
::=:events <process-def>

The following is an event from the Planetary Lander Domain. It models the transition
from night to day that occurs when the clock variable daytime reaches zero.

239

Fox & Long

(:event daybreak
:parameters ()
:precondition (and (not (day)) (>= (daytime) 0))
:effect (day)
)

The BNF for an event is identical to that of actions, while for processes it is modified
by allowing only a conjunction of process effects in the effects field. A process effect has
the same structure as a continuous effect in pddl2.1:
<process-effect>

::=(<assign-op-t> <f-head> <f-exp-t>)

The following is a process taken from the Planetary Lander Domain. It describes how
the battery state of charge, soc, is affected when power demand exceeds supply. The
interpretation of process effects is explained in Section 4.2.
(:process discharging
:parameters ()
:precondition (> (demand) (supply))
:effect (decrease soc (* #t (- (demand) (supply))))
)

We now provide the basic abstract syntactic structures that form the core of a pddl+
planning domain and problem and for which our semantic mappings will be constructed.
Core Definition 1 defines a simple planning instance in which actions are the only structures describing state change. Definition 1 extends Core Definition 1 to include events and
processes. We avoid repeating the parts of the core definition that are unchanged in this
extended version.
Definition 1 Planning Instance A planning instance is defined to be a pair
I = (Dom, P rob)
where Dom = (F s, Rs, As, Es, P s, arity) is a tuple consisting of finite sets of function
symbols, relation symbols, actions, and a function arity mapping all of these symbols to
their respective arities, as described in Core Definition 1. In addition it contains finite sets
of events Es and processes P s.
Ground events, E, are defined by the obvious generalisation of Core Definition 6 which
defines ground actions. The fact that events are required to have at least one numeric
precondition makes them a special case of actions. The details of ground processes, P, are
given in Definition 2. Processes have continuous effects on primitive numeric expressions
(PNEs). Core Definition 1 defines PNEs as ground instances of metric function expressions.
Definition 2 Ground Process Each p ∈ P is a ground process having the following
components:
• Name The process schema name together with its actual parameters.
240

Modelling Mixed Discrete-Continuous Domains for Planning

Time
0.01:
0.01:
0.71
0.9
15.02:
18.03:
19.51:
21.04:

Action
Action 1
Action 2
Action 3
Action 4
Action 5
Action 6
Action 7
Action 8

Duration
[13.000]

[1.000]
[1.000]
[1.000]

Figure 1: An example of a pddl+ plan showing the time stamp and duration associated
with each action, where applicable. Actions 2, 3, 4 and 7 are instantaneous, so
have no associated duration.

• Precondition This is a proposition, P rep , the atoms of which are either ground
atoms in the planning domain or else comparisons between terms constructed from
arithmetic operations applied to PNEs or real values.
• Numeric Postcondition The numeric postcondition is a conjunction of additive assignment propositions, NPp , the rvalues1 of which are expressions that can be assumed
to be of the form (* #t exp) where exp is #t-free.
Definition 3 Plan A plan, for a planning instance with the ground action set A, is a finite
set of pairs in Q>0 × A (where Q>0 denotes the set of all positive rationals).
The pddl family of languages imposes a restrictive formalism for the representation of
plans. In the temporal members of this family, pddl2.1 (Fox & Long, 2003), pddl2.2 (Hoffmann & Edelkamp, 2005) and pddl+, plans are expressed as collections of time-stamped
actions. Definition 3 makes this precise. Where actions are durative the plan also records
the durations over which they must execute. Figure 1 shows an abstract example of a
pddl+ plan in which some of the actions are fixed-length durative actions (their durations are shown in square brackets after each action name). Plans do not report events or
processes.
In these plans the time stamps are interpreted as the amount of time elapsed since
the start of the plan, in whatever units have been used for modelling durations and timedependent effects.
Definition 4 Happening A happening is a time point at which one or more discrete
changes occurs, including the activation or deactivation of one or more continuous processes.
The term is used to denote the set of discrete changes associated with a single time point.
1. Core Definition 3 defines rvalues to be the right-hand sides of assignment propositions.

241

Fox & Long

4.2 Expressing Continuous Change
In pddl2.1 the time-dependent effect of continuous change on a numeric variable is expressed by means of intervals of durative activity. Continuous effects are represented by
update expressions that refer to the special variable #t. This variable is a syntactic device that marks the update as time-dependent. For example, consider the following two
processes:
(:process heatwater
:parameters ()
:precondition (and (< (temperature) 100) (heating-on))
:effect (increase (temperature) (* #t (heating-rate)))
)
(:process superheat
:parameters ()
:precondition (and (< (temperature) 100) (secondaryburner-on))
:effect (increase (temperature) (* #t (additional-heating-rate)))
)

When these processes are both active (that is, when the water is heating and a secondary
burner is applied and the water is not yet boiling) they lead to a combined effect equivalent
to:
dtemperature
= (heating-rate) + (additional-heating-rate)
dt
Actions that have continuous update expressions in their effects represent an increased
level of modelling power over that provided by fixed length, discrete, durative actions.
In pddl+ continuous update expressions are restricted to occur only in process effects.
Actions and events, which are instantaneous, are restricted to the expression of discrete
change. This introduces the three-part modelling of periods of continuous change: an action
or event starts a period of continuous change on a numeric variable expressed by means of
a process. An action or event finally stops the execution of that process and terminates its
effect on the numeric variable. The goals of the plan might be achieved before an active
process is stopped.
Notwithstanding the limitations of durative actions, observed by Boddy (2003) and
McDermott (2003a), for modelling continuous change, the durative action model can be
convenient for capturing activities that endure over time but whose internal structure is
irrelevant to the plan. This includes actions whose fixed duration might depend on the
values of their parameters. For example, the continuous activities of riding a bicycle (whose
duration might depend on the start and destination of the ride), cleaning a window and
eating a meal might be conveniently modelled using fixed-length durative actions. pddl+
does not force the modeller to represent change at a lower level of abstraction than is
required for the adequate capture of the domain. When such activities need to be modelled
fixed duration actions might suffice.
The following durative action, again taken from the Planetary Lander Domain, illustrates how durative actions can be used alongside processes and events when it is unnecessary to expose the internal structure of the associated activity. In this case, the action
models a preparation activity that represents pre-programmed behaviour. The constants
242

Modelling Mixed Discrete-Continuous Domains for Planning

partTime1 and B-rate are defined in the initial state so the duration and schedule of effects
within the specified interval of the behaviour are known in advance of the application of
the prepareObs1 action.
(:durative-action prepareObs1
:parameters ()
:duration (= ?duration (partTime1))
:condition (and (at start (available unit))
(over all (> (soc) (safelevel))))
:effect (and
(at start (not (available unit)))
(at start (increase (demand) (B-rate)))
(at end (available unit))
(at end (decrease (demand) (B-rate)))
(at end (readyForObs1)))
)

4.3 Planetary Lander Example
We now present an example of a pddl+ domain description, illustrating how continuous
functions, driven by interacting processes, events and actions, can constrain the structure of
plans. The example is based on a simplified model of a solar-powered lander. The actions
of the system are durative actions that draw a fixed power throughout their operation.
There are two observation actions, observe1 and observe2, which observe the two different
phenomena. The system must prepare for these, either by using a single long action,
called fullPrepare, or by using two shorter actions, called prepareObs1 and prepareObs2,
each specific to one of the observation actions. The shorter actions both have higher power
requirements over their execution than the single preparation action. The lander is required
to execute both observation actions before a communication link is established (controlled
by a timed initial literal), which sets a deadline on the activities.
These activities are all carried out against a background of fluctuating power supply.
The lander is equipped with solar panels that generate electrical power. The generation
process is governed by the position of the sun, so that at night there is no power generated,
rising smoothly to a peak at midday and falling back to zero at dusk. The curve for power
generation is shown in Figure 2. Two key events affect the power generation: at nightfall the
generation process ends and the lander enters night operational mode. In this mode it draws
a constant power requirement for a heater used to protect its instruments, in addition to any
requirements for instruments. At dawn the night operations end and generation restarts.
Both of these events are triggered by a simple clock that is driven by the twin processes of
power generation and night operations and reset by the events.
The lander is equipped with a battery, allowing it to store electrical energy as charge.
When the solar panels are producing more power than is required by the instruments of
the lander, the excess is directed into recharging the battery (the charging process), while
when the demand from instruments exceeds the solar power then the shortfall must be
supplied from the battery (the discharging process). The charging process follows an
inverse exponential function, since the rate of charging is proportional to the power devoted
to charging and also proportional to the difference between the maximum and current levels
of charge. Discharge occurs linearly at a rate determined by the current demands of all the
lander activities. Since the solar generation process is itself a non-linear function of time
243

Fox & Long

20
18
16

Power (Watts)

14
12
10
8
6
4
2
0
0

2

4

6
8
Time (hours after dawn)

10

12

14

Figure 2: Graph of power generated by the solar panels.
Charging
(Supply exceeds demand)
Supply











Discharging
(Demand exceeds supply)


















































Demand





	




Start of plan

End of plan




	








	








	








	








	




Action A






























	





































	



































































	
































































	






Action B







Action C

	







	

Dawn

Nightfall

Figure 3: An abstracted example lander plan showing demand curve and supply curve over
the period of execution.

during the day, the state of charge of the battery follows a complex curve with discontinuities
in its rate of change caused by the instantaneous initiation or termination of the durative
instrument actions. Figure 3 shows an example of a plan and the demand curve it generates
compared with the supply over the same period.
Figures 5 and 6 show graphs of the battery state of charge for the two alternative plans
shown in Figure 4. The plans both start an hour before dawn and the deadline is set to 10
hours later. The parameters have been set to ensure that there are 15 hours of daylight, so
244

Modelling Mixed Discrete-Continuous Domains for Planning

2.6:
4.7:
6.8:
7.9:

0.1: (fullPrepare) [5]
5.2: (observe1) [2]
7.3: (observe2) [2]

(prepareObs1) [2]
(observe1) [2]
(prepareObs2) [1]
(observe2) [2]

Figure 4: Two alternative plans to complete the observations before the deadline.
the plan must complete within two hours after midday. The battery begins at 45% of fully
charged.
Value

6
99.5197

15
fullPrepare

0
0

daybreak

d>s

observe1
5.1

observe2

-Time
10

Figure 5: Graph of battery state of charge (as a percentage of full charge) for first plan.
The timepoint marked d > s is the first point at which demand exceeds supply, so
that the battery begins to recharge. The vertical lines mark the points at which
processes are affected. Where the state of charge is falling over an interval the
discharge process is active and where it is rising the charge process is active.

The lander is subject to a critical constraint throughout its activities: the battery state
of charge may never fall below a safety threshold. This is a typical requirement on remote
systems to protect them from system failures and unexpected problems and it is intended to
ensure that they will always have enough power to survive until human operators have had
the opportunity to intervene. This threshold is marked in Figure 5, where it can be seen
that the state of charge drops to approximately 20%. The lowest point in the graph is at a
time 2.95 hours after dawn, when the solar power generation just matches the instrument
demand. At this point the discharging process ends and the generation process starts. This
time point does not correspond to the start or end of any of the activities of the lander
and is not a point explicitly selected by the planner. It is, instead, a point defined by the
intersection of two continuous functions. In order to confirm satisfaction of the constraint,
that the state of charge may never fall below its safety threshold, the state of charge must
be monitored throughout the activity. It is not sufficient to consider its value at only its
end points, where the state of charge is well above the minimum required, since the curve
might dip well below these values in the middle.
We will use this example to illustrate further points later in this paper. The complete domain description and the initial state for this problem instance can be found in Appendix C,
245

Fox & Long

Value

6
98.1479

p’Obs1

0
0

obs’1

daybreak

p’Obs2

obs’2 -Time
10

Figure 6: Graph of battery state of charge (as a percentage of full charge) for second plan.
As in the previous case, discontinuities in the gradient of the state of charge
correspond to points at which the charge or discharge process is changed by an
action (start or end point) or event.

while the two reports generated by Val (Howey, Long, & Fox, 2004) are available in the
online appendices associated with this paper.
4.4 Expressive Power of pddl+
We now consider whether pddl+ represents a real extension to the expressive power of
pddl2.1. Of course, the fragment of pddl2.1 that was used in the competition and has
been widely used since (the fragment restricted to discrete durative actions) does not include the parts that express continuous change, and without those elements pddl2.1 is
certainly less expressive than pddl+. In this section we discuss the differences between
modelling continuous change using the continuous durative action constructs of pddl2.1,
and modelling it using the start-process-stop model.
pddl2.1, complete with continuous durative actions, comprises a powerful modelling
language. Allowing continuous effects within flexible duration actions offers an expressive
combination that appears close to the processes and events of pddl+. The essential difference between the languages arises from the separation, in pddl+, between the changes
to the world that are directly enacted by the executive and those indirect changes that are
due to physical processes and their consequences.
To model physical processes and their consequences in pddl2.1 requires the addition to
the domain model of artificial actions to simulate the way in which processes and events
interact with eachother and with the direct actions of the executive. For example, to force
intervals to abut, so that the triggering of an event is correctly modelled, requires artificial
actions that force the corresponding end points of the intervals to synchronise. These actions
must be applied by the planner, since there is no other dynamic to force indirect events
to coincide in the way that they would coincide in nature. In earlier work (Fox & Long,
2004) we show how clips can be constructed in pddl2.1 and used to achieve this effect.
Clips prevent time passing between the end points of actions modelling the background
246

Modelling Mixed Discrete-Continuous Domains for Planning

Supply














































Start of plan

End of plan


























Demand























































Action A







































































































Action B















Plan activities























































































































	







D

C


D


C



D

Initiation action

C

D

C

D
	

Charging
Clip actions

N

C

D

Termination action
Discharging

Generation
Night operations

N
Simulation activities

Figure 7: An illustration of the structure of the simulation activities required to model a
simple pddl+ plan in pddl2.1. A and B are actions, C and D are the charging
and discharging processes, respectively.

behaviour of the world. The example shown in Figure 7 illustrates how clips can be used to
model interacting continuous effects in a pddl2.1 representation of the Planetary Lander
Domain.
The example shows two activities executing against a backdrop of continuous charging
and discharging of a battery. The bell-shaped curve represents the solar power production, which starts at daybreak, reaches its peak at midday and then drops off to zero at
nightfall. Two concurrent power-consuming activities, A and B, are executing during the
daylight hours. The stepped curve shows their (cumulative) power requirements. A pddl+
representation of this plan would contain just the two actions A and B — the processes
and events governing power consumption and production would be triggered autonomously
and would not be explicit in the plan. By contrast, a pddl2.1 representation of the same
plan would contain durative actions for each of the episodes of charge, C, and discharge, D,
which need to be precisely positioned (using clips) with respect to the two activities A and
B. Clips are required because actions C and D do not have fixed durations so they have
to be joined together to force them to respect the underlying timeline. The two dotted
rectangles of the figure depict a pddl2.1 plan containing 28 action instances in addition to
A and B. Of these, there are four points simulating the intersections between the supply
and the demand curves that do not correspond to the end points of actions A and B. A
planner using a pddl2.1 model is forced to construct these points in its simulation of the
background behaviours. All of the necessary clip actions would be explicit in the plan.
As can be seen, the construction of an accurate simulation in pddl2.1 is far from trivial.
Indeed, although this is not an issue that we highlight in the example, there are cases where
247

Fox & Long

no simulation can be constructed to be consistent with the use of -separation for interfering
action effects. This occurs when events or process interactions occur with arbitrarily small
temporal separations. Even where simulations can be constructed, the lack of distinction
between direct and indirect causes of change means that a planner is forced to construct the
simulated process and event sequences as though they are part of the plan it is constructing.
This means that the planner is required to consider the simulation components as though
they are choice points in the plan construction, leading to a combinatorial blow up in the
cost of constructing plans.
The explicit distinction between actions and events yields a compact plan representation
in pddl+. A pddl2.1 plan, using a simulation of the events and processes, would contain
explicit representations of every happening in the execution trace of the pddl+ plan. The
fact that the pddl2.1 plan represents a form of constructive proof of the existence of an
execution trace of the pddl+ plan is one way to understand Theorem 1 below: the work in
validating a pddl+ plan is that required to construct the proof that a pddl2.1 plan would
have to supply explicitly.
By distinguishing between the direct action of the executive and the continuous behaviours of the physical world we facilitate a decomposition of the planning problem into
its discrete and continuous components. This decomposition admits the use of hybrid
reasoning techniques, including Mixed Integer Non-Linear Programming (MINLP) (Grossmann, 2002), Benders Decomposition (Benders, 1962), Branch-and-Bound approaches that
relax the discrete components of the domain into continuous representations (Androulakis,
2001), and other such techniques that have proved promising in mixed discrete-continuous
problem-solving (Wu & Chow, 1995). By contrast, trying to treat a hybrid problem using
purely discrete reasoning techniques seems likely to result in an unmanageable combinatorial explosion. Of course, the trade-offs cannot be fully understood until planners exist for
tackling mixed discrete-continuous domains featuring complex non-linear change.
We now prove that pddl+ has a formally greater expressive power than pddl2.1.
Theorem 1 pddl+ is strictly more expressive than pddl2.1.
Proof: We demonstrate this by showing that we can encode the computation of an arbitrary
register machine (RM) in the language of pddl+. The instructions of the RM are encoded as
pddl+ events and the correct execution of a plan can be made to depend on the termination
of the corresponding RM program. This means that the general plan validation problem
for pddl+ plans is undecidable, while for pddl2.1 plans it is decidable. This is because
pddl2.1 plans explicitly list all the points in a plan at which a state transition occurs (as
actions) and these can be checked for validity by simulated execution. In contrast, a pddl+
plan leaves events implicit, so a plan cannot be tested without identifying the events that
are triggered and confirming their outcomes.
To simulate an arbitrary RM program, we need an action that will initiate execution of
the program:
(:action start
:parameters ()
:precondition ()
:effect (started))
248

Modelling Mixed Discrete-Continuous Domains for Planning

Now we construct a family of events that simulate execution of the program. We use
an encoding of a register machine with three instructions: inc(j,k) which increments
register j and then jumps to instruction k, dec(j,k,z), which tests register j and jumps
to instruction z if it is zero and otherwise decrements it and jumps to instruction k, and
HALT which terminates the program. We assume that the instructions are labelled 0, . . . , n
and the registers used are labelled 0, . . . , m. We also assume that instruction 0 is the start
of the program.
(:event beginExection
:parameters ()
:precondition (started)
:effect
(and (not (started))
(in 0)))

For an instruction of the form: l:

inc(j,k) where l is the label, we construct:

(:event dol
:parameters ()
:precondition (in l)
:effect
(and (not (in l))
(in k)
(increase (reg j) 1)))

For an instruction of the form: l:

dec(j,k,z) we construct:

(:event dol
:parameters ()
:precondition (in l)
:effect
(and (not (in l))
(when (= (reg j) 0) (in z))
(when (> (reg j) 0) (and (decrease (reg j) 1)
(in k)))))

Finally, for an instruction l:

HALT we have:

(:event dol
:parameters ()
:precondition (in l)
:effect
(and (not (in l))
(halted)))

We now create an initial state in which registers reg 0. . .reg m are all initialised to 0 and
the goal halted. It is now apparent that the plan:
1:

(beginExecution)

is valid if and only if the computation of the embedded RM halts. Therefore, a general plan
validation system for pddl+ would have to be able to solve the halting problem.

Theorem 1 is a formal demonstration of the increase in expressive power offered by
pddl+. It depends on the fact that a pddl+ plan is defined to exclude explicit indication
of events and processes that are triggered during the execution of the plan. It might be
argued that this is an artificial problem, but there are two points to consider. Firstly, by
249

Fox & Long

avoiding the requirement that events and processes be captured explicitly in the plan we
remain agnostic about the nature of the reasoning that a planner might perform about
these phenomena. It might be that a planner can synthesise an approximation of a continuous process that simplifies the reasoning and is sufficiently accurate to allow it to place
its actions around the process behaviour, but that would be insufficient to determine the
precise moments at which the process triggers events. Secondly, a planner might be able to
determine that some collection of processes and events is irrelevant to the valid execution of
a plan it has constructed to solve a problem, even though it is apparent that some pattern
of processes and events will be triggered during the execution of the plan. In this case, the
requirement that the plan correctly and explicitly captures all of this background activity
is an unreasonable additional demand.
The undecidability of the pddl+ validation problem need not be confronted in practice.
If certain restrictions are imposed (no cascading events, functions restricted to polynomials
and some exponentials), which do not undermine the ability to capture realistic domains, the
processes and events underlying a pddl+ plan can be efficiently simulated using well-known
numerical methods. In (Fox, Howey, & Long, 2006) we show how numerical simulation is
achieved in the pddl+ plan validator, VAL. A validation procedure must simulate these
processes and events to ensure that critical values remain in acceptable ranges throughout
the plan (and satisfy the conditions of planned actions). The restrictions that it is sensible
to apply, in particular to sequences of events that may be triggered at the same time point,
but also to the forms of continuous functions that arise in a domain, do not prevent us from
achieving close approximations of realistic behaviours. Boddy and Johnson (2004) and
Hofmann and Williams (2006) use linear and quadratic approximations to model complex
non-linear functions. We use a quartic approximation and an inverse exponential function to
represent the power dynamics in our own model of the planetary lander (see Appendix C).
In practice, although there is a formal separation between the expressive power of pddl+
and pddl2.1, the conceptual separation between the activities of the executive and those
of the world is the most important feature of pddl+.
We now present a simple family of domains to illustrate that pddl2.2 encodings grow
larger than pddl+ domains encoding equivalent behaviours. The difference arises from
the fact that durative actions encapsulate not only the way in which a process starts, but
also the way it concludes. This means that in domains where there is a significant choice
of different ways to start and end a process, the pddl2.1 encoding expands faster than
the corresponding pddl+ encoding. Consider the pddl+ domain containing the following
action and process schemas:
(:action Ai
:parameters ()
:precondition (and (not (started)) (ai ))
:effect (and (started) (not (ai )) (assign (dur) dAi ))
)
(:action Bj
:parameters ()
:precondition (and (bj ) (= (C) (* (dur) dBj ))
250

Modelling Mixed Discrete-Continuous Domains for Planning

:effect (and (not (started)) (done))
)
(:process P
:parameters ()
:precondition (started)
:effect (increase (C) (* #t 1))
)
The action schemas are families Ai and Bj , indexed by i and j which take values i ∈
{1, ..., n} and j ∈ {1, ..., m} respectively. The values dAi and dBj are (different) actioninstance-dependent constants. Any plan starting in an initial state {(ax ), (by )}, with C = 0,
that achieves done, must contain the actions Ax , By , separated by exactly dAx .dBy . To
encode an equivalent durative action model requires an action schema:

(:durative-action ABi,j
:parameters ()
:duration (= ?duration (* dAi dBj ))
:condition (and (at start (ai )) (at end (bj )))
:effect (and (at start (not (ai ))) (at end (done)))
)
As can be seen, the size of the encoding of the family of pddl+ domains grows as
O(n + m), while the corresponding size of the pddl2.2 encodings grows as O(n.m). The
need to couple each possible initiation of the process with each possible conclusion to the
process leads to this multiplicative growth. Reification of the propositions in the durative
action encoding can be used to reduce the encoding to an O(n + m) encoding, but the
ground action set continues to grow as O(n.m) compared with the O(n + m) growth of the
ground actions and processes for the pddl+ model.
It is clear that it is easier to build the plan given the O(n.m) encoding, because this
provides ready-made solutions to the problem. However, the trade-off to be explored lies in
how large a representation can be tolerated to obtain this advantage in general. It is always
possible to compile parts of the solution to a problem into the problem representation, but
the price that is paid is in the size of the encoding and the effort required to construct it.
On this basis we argue that a compact representation is preferable. The example we have
presented is an artificial example demonstrating a theoretical difference in the expressive
powers of pddl2.2 and pddl+. It remains to be seen whether this phenomenon arises in
practice in realistic domains.

5. pddl+ and Hybrid Automata
In this section we discuss the role of Hybrid Automata in relation to pddl+. We motivate
our interest in Hybrid Automata and then proceed to describe them in more detail.
251

Fox & Long

5.1 The Relevance of HA Theory
Researchers concerned with the modelling of real-time systems have developed techniques
for modelling and reasoning about mixed discrete-continuous systems (Yi et al., 1997; Henzinger, Ho, & Wong-Toi, 1995; Rasmussen et al., 2004). These techniques have become
well-established. The theory of hybrid automata (Henzinger, 1996; Gupta, Henziner, &
Jagadeesan, 1997; Henzinger & Raskin, 2000), which has been a focus of interest in the
model-checking community for some years, provides an underlying theoretical basis for
such work. As discussed in Section 2, the central motivation for the extensions introduced
in pddl+ is to enable the representation of mixed discrete-continuous domains. Therefore, the theory of hybrid automata provides an ideal formal basis for the development of a
semantics for pddl+.
Henzinger (1996) describes a digital controller of an analogue plant as a paradigmatic
example of a mixed discrete-continuous system. The discrete states (control modes) and
dynamics (control switches) of the controller are modelled by the vertices and edges of a
graph. The continuous states and dynamics of the plant are modelled by vectors of real
numbers and differential equations. The behaviour of the plant depends on the state of
the controller, and vice versa: when the controller switches between modes it can update
the variables that describe the continuous behaviour of the plant and hence bring about
discrete changes to the state of the plant. A continuous change in the state of the plant
can affect invariant conditions on the control mode of the controller and result in a control
switch.
In a similar way, pddl+ distinguishes processes, responsible for continuous change, from
events and actions, responsible for discrete change. Further, the constraint in pddl+, that
numeric values only appear as the values of functions whose arguments are drawn from finite
domains, corresponds to the requirement made in hybrid automata that the dimension of
the automaton be finite.
An important contribution of our work is to demonstrate that pddl+ can support succinct encodings of deterministic hybrid automata for use in planning. We expect that both
the formal (semantics and formal properties) and practical (model-checking techniques) results in Hybrid Automata theory will be able to be exploited by the planning community in
addressing the problem of planning for discrete-continuous planning domains. Indeed, some
cross-fertilisation is already beginning (Dierks, 2005; Rasmussen et al., 2004; Edelkamp,
2003).
5.2 Hybrid Automata
We now present the relevant definition of a Hybrid Automaton from Henzinger’s theory (Henzinger, 1996) that will be used in the construction of our formal semantics for
pddl+ planning domains.
Definition 5 Hybrid Automaton A Hybrid Automaton H consists of the following components:
• Variables. A finite set X = {x1 , . . . , xn } of real-valued variables. The number n is
called the dimension of H. We write Ẋ for the set {ẋ1 , . . . , ẋn } of dotted variables,
252

Modelling Mixed Discrete-Continuous Domains for Planning

representing first derivatives during continuous change, and X 0 for the set {x01 , . . . , x0n }
of primed variables, representing values at the conclusion of discrete change.
• Control Graph. A finite directed graph hV, Ei. The vertices in V are control modes.
The edges in E are control switches.
• Initial, invariant and flow conditions. Three vertex labelling functions, init, inv,
and f low, that assign to each control mode v ∈ V three predicates. Each initial
condition init(v) is a predicate whose free variables are from X. Each invariant
condition inv(v) is a predicate whose free variables are from X. Each flow condition
f low(v) is a predicate whose free variables are from X ∪ Ẋ.
• Jump conditions. An edge labelling function jump that assigns to each control
switch e ∈ E a predicate. Each jump condition jump(e) is a predicate whose free
variables are from X ∪ X 0 .
• H-Events. A finite set Σ of h-events and a function, hevent : E → Σ, that assigns
to each control switch an h-event.
An h-event is referred to by Henzinger as an event, but we have changed the name to
avoid terminological confusion with events in pddl+.
Figure 8 shows a very simple dynamic system expressed as a hybrid automaton.
The initial, jump and flow conditions are not necessarily satisfied by unique valuations.
When multiple valuations satisfy these conditions it is possible for the behaviour of the
automaton to be non-deterministic.
It should be observed that Henzinger’s model needs to be extended, in a simple way, to
include the undefined value, ⊥, for real-valued variables. This is because pddl+ states can
contain unassigned real-valued variables, as shown in Core Definition 2 which defines the
metric valuation of a state. The role of this value is to allow for the situations in which a
metric fluent is created in a domain, but is not given an initial value. Such a fluent will be
given the undefined value and attempts to inspect it before it is assigned a value will be
considered to yield an error. This introduces no semantic difficulties and we have left the
details of this modification implicit.
Just as the input language of a finite automaton can be defined to be the set of all
sequences of symbols from its alphabet, it is possible to define the input language of a
Hybrid Automaton. In this case, the elements of the language are called traces:
Definition 6 Trace Given a Hybrid Automaton, H, with h-event set Σ, a trace for H is
an element of the language (R≥0 ∪ Σ)∗ .
Informally, a trace consists of a sequence of h-events interleaved with real values corresponding to time periods during which no h-events are applied. The time at which each
h-event is applied is readily determined by summing the values of the time periods in the sequence up to the point at which the h-event appears: h-events take no time to execute. Note
that this definition does not require that the trace is accepted by the Hybrid Automaton:
this is a property of traces we consider in Section 6.4.
A further minor point to note is that the definition of a trace allows traces where the
first transition occurs at time 0. Our convention in the semantics of pddl2.1 is to forbid
253

Fox & Long

pump−on
pump−rate measures flow rate into tank
(0 when pump off, P when pump on)
max−threshold = 10

.
water−level = pump−rate

turn pump on
Jump: water−level’ = water−level

.

Inv:
water−level < 10
Flow:

water−level = 0
pump−off

.

water−level = P
pump−on

Inv:
water−level < 10
Flow:

turn pump off
Jump: water−level’ = water−level
Inv:

flood
Jump:
water−level = 10
water−level’ = water−level

Flow:

.
water−level = 0
flooded

Figure 8: A simple tank-filling situation modelled as a hybrid automaton. This has three
control modes and three control switches. The control switch flood has a jump
condition which requires the level to exceed the bath capacity. Flow conditions
govern the change in water level.

254

Modelling Mixed Discrete-Continuous Domains for Planning

THA

implies

Traces

interpretation
interpretation
representation
Planning
instance

Plans
implies

Figure 9: The semantic mapping between plans and traces
actions to occur at time 0. The reason for this is discussed in (Fox & Long, 2003), but,
briefly, in order to be consistent with the model in which states hold over an interval that
is closed on the left and open on the right, the initial state (which holds at time 0) must
persist for a non-zero interval. As a consequence, we will not be interested in traces that
have action transitions at time 0.

6. Semantics
In this section we present the semantics of pddl+. We begin by explaining our approach
and then proceed to develop the semantics incrementally.
6.1 Semantics of pddl+
We present the semantics in two stages. In Section 6.2 we give the semantics of planning instances in terms of Hybrid Automata, by defining a formal mapping from planning
constructs to constructs of the corresponding automata. Figure 9 illustrates the syntactic
relationship between a pddl+ planning instance and the plans that it implies, and the
semantic relationships both between the pddl+ instance and the corresponding Hybrid
Automaton and between the plans implied by the model and the traces implied by the
automaton. The pddl+ instance and its plans are syntactic constructs for which the HA
and its traces provide a formal semantics. We show that, whilst plans can be interpreted as
traces, traces can be represented as plans by means of the abstraction of events appearing
in the traces. The figure represents the first stage in the development of our formalisation. We then summarise, in Section 6.3, Henzinger’s interpretation of Hybrid Automata
in terms of labelled transition systems and the accompanying transition semantics. We use
this semantic step as the basis of the second stage of our formalism, as is shown in Figure 13.
The important distinction that we make in our planning models between actions and
events requires us to introduce a time-slip monitoring process (explained below), which is
used to ensure events are executed immediately when their preconditions are satisfied.
In Core Definition 4 we define how PNEs are mapped to a vector of position-indexed
~ = hX1 , ..., Xn i. The purpose of this mapping is to allow us to define and manipvariables, X
ulate the entire collection of PNEs in a consistent way. The collection is given a valuation in
a state as shown in Core Definition 2 where the logical and metric components of a state are
identified. The updating function defined in Core Definition 8 specifies the relationship that
255

Fox & Long

must hold between the valuations of the PNEs before and after the application of an action.
Normalisation of expressions that use PNEs involves replacing each of the PNEs with its
corresponding position-indexed variable denoting its position in the valuation held within a
state. This is performed by the semantic function N . Update expressions are constructed
using the “primed” form of the position-indexed variable for the lvalue2 of an effect, to distinguish the pre- and post-condition values of each variable. In mapping planning instances
to Hybrid Automata we make use of this collection of position-indexed variables to form
the set of metric variables of the constructed automaton. In Definition 5, Henzinger uses
the names X, X 0 and Ẋ for the vectors of variables, the post-condition variables following
discrete updates and the derivatives of the variables during continuous change, respectively.
In order to reduce the potential for confusion in the following, we note that we use X, X 0
and Ẋ as Henzinger does, X1 , . . . , Xn as the names of the position-indexed variables for
the planning instance being interpreted and Xn+1 as an extra variable used to represent
time-slip.
6.2 The Semantics of a Planning Instance
In the following we present the semantics of a planning instance in stages in order to
facilitate understanding. We begin with the definition of a uniprocess planning instance
and an event-free uniprocess planning instance. We then introduce the general concept of a
planning instance. These definitions rely on the concept of relevance of actions, events and
processes, which we now present.
The following definition uses the interpretation of preconditions defined in Core Definition 9. This core definition explains how, given a proposition P and a logical state s,
the truth of the proposition is determined. N um(s, P ) is a predicate over the PNEs in the
domain. As explained in Core Definition 9, to determine the truth of a proposition in a
state, with respect to a vector of numeric values ~x, the formal numeric parameters of the
proposition are substituted with the values in ~x and the resulting proposition is evaluated in
the logical state. The purpose of Definition 7 is to identify actions, events or processes that
could be applicable in a given logical state, if the values of the metric fluents are appropriate
to satisfy their preconditions.
Definition 7 Relevance of Actions, Events and Processes A ground action a (event
e, or process p) of a planning instance I of dimension n, is relevant in the logical state s if
there is some value ~x ∈ Rn such that N um(s, P rea )(~x) (N um(s, P ree )(~x) or N um(s, P rep )(~x),
respectively).
Ps (Es ) is the set of all ground processes (events) that are relevant in state s.
In general, an action, event or process that is relevant in a particular (logical) state might
not actually become applicable, since the valuations of the numeric state that arise while the
system is in the logical state might not include any of those that satisfy the preconditions
of the corresponding transition.
In the construction of a HA, in the mappings described below, the vertices of its control
graph are subsets of ground atoms and are therefore equivalent to logical states. We use
2. The lvalue in an update expression is the variable on the left of the expression, to which the value of the
expression on the right is assigned.

256

Modelling Mixed Discrete-Continuous Domains for Planning

the variable v to denote a vertex and Pv (Ev ) to denote the processes (events) relevant in
the corresponding logical state.
In any given logical state a subset of the relevant processes will be active, according
to the precise valuation of the metric fluents in the current state. We first consider the
restricted case in which no more than one active process affects the value of each variable
at any time, which we call a uniprocess planning instance. The proposition unary-contextflow(i,π), defined below, is used to describe the effects on the ith variable of the process π,
when it is active. We later extend this definition to the concurrent case in which multiple
processes may contribute to the behaviour of a variable.
Definition 8 Uniprocess Planning Instance A planning instance is a uniprocess planning instance if, for each metric variable, Xi , the set of processes that can affect the value of
Xi relevant in state v, denoted Pv |Xi , contains processes whose preconditions are pairwise
mutually exclusive. That is, for π1 , π2 ∈ Pv |Xi (π1 6= π2 ) there is no numeric state such
that N (P reπ1 ) ∧ N (P reπ2 ).
Definition 9 Unary-context-flow If v is a logical state for a uniprocess planning instance
I of dimension n and π ∈ Pv |Xi then the unary-context-flow proposition is defined as follows.
Let the effect of π on Xi take the form (increase Xi (* #t Qi )) for some expression Qi .
unary-context-flowv (i, π) = (N (P reπ ) → Ẋi = Qi )
We begin by presenting the semantics of a event-free uniprocess planning instance — that
is, a uniprocess planning instance that contains no events. We then extend our definition
to include events, and present the semantics of a uniprocess planning instance. Finally
we introduce concurrent process effects and the definition of the semantics of a planning
instance.
Definition 10 Semantics of an Event-free Uniprocess Planning Instance An eventfree uniprocess planning instance, I = (Dom, P rob), is interpreted as a Hybrid Automaton,
HI , as follows:
• Variables. The variables of HI are X = {X1 , . . . , Xn }, where n is the dimension of
the planning problem.
• Control Graph. The set of vertices V is formed from all subsets of the ground atoms
of the planning instance. The set of edges in E contains an edge e between v and v 0
iff there is an action, a, relevant in v and:
v 0 = (v − Dela ) ∪ Adda
The action a is associated with the edge e.
• Initial, invariant and flow conditions. The vertex labelling function init is defined
as:

f alse
if v 6= Initlogical
V
init(v) =
N (Initnumeric ) ∧ {Xi =⊥ |Xi 6∈ N (Initnumeric )} otherwise
257

Fox & Long

The vertex labelling function inv is the proposition True.
The vertex labelling function flow is defined:
f low(v) = (

n
^

i=1

^

unary-context-flowv (i, π) ∧ (

π∈Pv |Xi

^

	
¬N (P reπ ) → Ẋi = 0) )

π∈Pv |Xi

• Jump conditions. The edge labelling function jump is defined as follows. Given an
edge e from vertex v, associated with an action a:
jump(e) = N (P rea ) ∧ U Fa (X) = X 0
where U Fa is the updating function for action a, as defined in Core Definition 8 which
specifies the relationship that must hold between the valuations of the PNEs before and
after the application of an action.
• H-events. Σ is the set of all names of all ground actions. The edge labelling function
hevent : E → Σ assigns to each edge the name of the action associated with the edge.
The flow condition states that, for each variable, if the precondition of one of the processes
that could affect it is true then that process defines its rate of change (through the unarycontext-flow proposition), or else, if none of the process preconditions is satisfied then the
rate of change of that variable is zero.
To illustrate this construction we now present a simple example. The planetary lander
domain requires events and so cannot be used as an example of the event-free model.
Consider the pddl+ domain containing just the following actions and process:
(:action startEngine
:precondition (stopped)
:effect (and (not (stopped))
(running))
)
(:action accelerate
:precondition (running)
:effect (increase (a) 1)
)
(:action decelerate
:precondition (running)
:effect (decrease (a) 1)
)
(:action stop
:precondition (and (= (v) 0) (running))
:effect (and (not (running))
(stopped)
(assign (a) 0))
)

258

Modelling Mixed Discrete-Continuous Domains for Planning

Inv: True
Flow:

startEngine
Jump: d’ = d
v’ = 0
a’ = 0

d=0
v=0
a=0

accelerate
Jump: d’ = d
v’ = v
a’ = a +1
Inv: True
Flow:

d=v
v=a
a=0
running

stopped

stop
Jump:

v=0
d’ = d
v’ = 0
a’ = 0
decelerate
Jump: d’ = d
v’ = v
a’ = a −1

Figure 10: A hybrid automaton constructed by the translation of an event-free uniprocess
planning instance. We ignore the init function, which simply asserts the appropriate initial state for a particular problem instance.

(:process moving
:precondition (running)
:effect (and (increase (d) (* #t (v)))
(increase (v) (* #t (a))))
)

The translation process described in Definition 10 leads to the hybrid automaton shown
in Figure 10. As can be seen, there are two vertices, corresponding to the logical states
{stopped} and {running}. The four actions translate into edges, each edge linking a vertex
in which an action is relevant to one in which its logical effects have been enacted. The
metric effects of actions are encoded in the jump conditions associated with an edge, using
the convention that the primed versions of the variables refer to the state following the
transition. Note that variables that are not explicitly affected by an action are constrained
to take the same value after the transition as they did before the transition: this is the
metric equivalent of the strips assumption. The stop action has a metric precondition and
this is expressed in the jump condition of the transition, requiring that the velocity variable,
v, be zero for this transition. In the stopped state no process can affect the variables, so
the flow conditions simply assert that the variables have a zero rate of change. In the
running state the moving process is relevant — indeed, since it has no other preconditions,
it is active whenever the system is in this state. The effect of this process is expressed in
the flow conditions for that state which show that the distance variable, d, changes as the
value of velocity, v, which changes in turn as the value of acceleration, a. These constraints
create a system of differential equations describing the simultaneous effects of velocity and
acceleration on the system.
259

Fox & Long

We now consider the case in which events are included but there can only be one process
active on any one fluent at any one time.
In planning domains it is important to distinguish between state changes that are deliberately planned, called actions, and those, called events, that are brought about spontaneously in the world. There is no such distinction in the HA, where all control switches
are called events. This distinction complicates the relationship between plans and traces,
because plans contain only the control switches that correspond to actions. Any events
triggered by the evolution of the domain under the influence of planned actions must be
inferred and added to the sequence of actions in order to arrive at the corresponding traces.
Henzinger et al. (1998) discuss the use of -moves, which are transitions that are not
labelled with a corresponding control-switch, but with the special label . The significance
of these transitions is that they do not appear in traces. A trace that corresponds to
an accepting run using -moves will contain only those transitions that are labelled with
elements from Σ. Where -moves appear between time transitions, the lengths of these
transitions can be accumulated into a single transition in the corresponding trace. The
purpose of these silent transitions is that they allow special book-keeping transitions to
be inserted into automata that can be used to simulate automata with syntactically richer
constraints, allowing various reducibility results to be demonstrated. The convenient aspect
of the -moves is that they do not affect traces when they are transferred from the original
automata to the simulations.
In pddl+, events are similar to -moves in that they do not appear explicitly in plans.
However, in contrast to -moves, applicable events are always forced to occur before any
actions may be applied in a state.
When extending event-free planning instances to include events we require a mechanism
for capturing the fact that events occur at the instant at which they are triggered by
the world, and not at the convenience of the planner. No time must be allowed to pass
between the satisfaction of event preconditions and the triggering of the event. In our
semantic models we use a variable to measure the amount of time that elapses between the
preconditions of an event becoming true and the event triggering. Obviously this quantity,
which we call time-slip, must be 0 in any valid planning instance. In the HA that we
construct we associate an invariant with each vertex in the control graph to enforce this
requirement. It might appear that a simpler way to handle events would be to simply assert
that an invariant condition for each state is that the preconditions of all events are false,
while each event is represented by an outgoing transition with a jump condition specifying
the precondition of the corresponding event. However, it is not possible for a jump condition
on a transition to be inconsistent with the invariant of the state it leaves since both must
hold simultaneously at the time at which the transition is made.
The variable used to monitor time-slip for a planning instance of dimension n is the
variable Xn+1 . This variable operates as a clock tracking the passage of time once an event
becomes applicable. We define the time-slippage proposition to switch the clock on whenever
the preconditions of any event become true in any state. When no event is applicable the
clock is switched off.
260

Modelling Mixed Discrete-Continuous Domains for Planning

Definition 11 Time-slippage For a planning instance of dimension n the variable Xn+1
is called the time-slip variable and time-slippage is defined as follows.
_
time-slippage(R) = (Ẋn+1 = 0 ∨ Ẋn+1 = 1) ∧ (
N (P ree ) → Ẋn+1 = 1)
e∈R

where R is a set of ground events.
The use of time-slip allows us to model pddl+ domains directly in Hybrid Automata in a
standard form. An alternative would be to introduce a modified definition of Hybrid Automata that makes explicit distinction between controllable and uncontrollable transitions
(actions and events respectively) and then require that uncontrollable transitions should
always occur immediately when their jump conditions are satisfied. This approach would
lead to an essentially equivalent formalism, but would complicate the opportunity to draw
on the existing body of research into Hybrid Automata, which is why we have followed the
time-slip approach.
The interpretation of a Uniprocess Planning Instance extends the interpretation of the
Event-free Uniprocess Planning Instance. The added components are underlined for ease
of comparison.
Definition 12 Semantics of a Uniprocess Planning Instance A unary process planning instance I = (Dom, P rob) is interpreted as a Hybrid Automaton, HI , as follows:
• Variables. The variables of HI are X = {X1 , . . . , Xn+1 }, where n is the dimension
of the planning problem.
The n + 1th variable is a special control variable used to measure time-slip.
• Control Graph. The set of vertices V is formed from all subsets of the ground atoms
of the planning instance. The set of edges in E contains an edge e between v and v 0
iff there is an action or event, a, relevant in v and:
v 0 = (v − Dela ) ∪ Adda
The action or event a is associated with the edge e.
• Initial, invariant and flow conditions. The vertex labelling function init is defined
as:

f alse
if v 6= Initlogical
V
init(v) =
N (Initnumeric ) ∧ {Xi =⊥ |Xi 6∈ N (Initnumeric )} otherwise
The vertex labelling function inv is the simple proposition that ensures time-slip is zero.
inv(v) = (Xn+1 = 0)
The vertex labelling function flow is defined:
V
V
f low(v) = ( ni=1
unary-context-flowv (i, π)∧
	
V π∈Pv |Xi
( π∈Pv |X ¬N (P reπ ) → Ẋi = 0) ∧time-slippage(Ev ))
i

261

Fox & Long

• Jump conditions. The edge labelling function jump is defined as follows. Given an
edge e from vertex v, associated with an action a:
^
jump(e) = N (P rea ) ∧ U Fa (X) = X 0 ∧
¬N (P reev )
ev∈Ev

Given an edge e from vertex v, associated with an event ev:
jump(e) = N (P reev ) ∧ U Fev (X) = X 0
where U Fa (U Fev ) is the updating function for action a (event ev) respectively.
• H-events. Σ is the set of all names of all ground actions and events. The edge
labelling function hevent : E → Σ assigns to each edge the name of the action or event
associated with the edge.
In this case, the flow condition says the same thing as for the event-free uniprocess planning
instance, but with the additional constraint that whenever the precondition of an event is
satisfied, the time-slip variable must increase at rate 1 (and may increase at rate zero
otherwise). Since the invariant condition for every state insists that the time-slip variable
is never greater than 0, a valid trace for this machine cannot rest in a state for any period
of time once the preconditions of an event become true. It can also be seen that the
jump condition for action transitions asserts that all event preconditions must be false.
This ensures that events are always applied before action transitions are permitted. We
now extend the preceding simple example domain to include an event, to illustrate the
construction described in Definition 12:
(:event engineExplode
:parameters ()
:precondition (and (running) (>= (a) 1) (>= (v) 100))
:effect (and (not (running)) (assign (a) 0) (engineBlown))
)
The corresponding machine is shown in Figure 11. The structure of this machine is
similar to the previous example, but includes an extra state, reachable by an event transition. The addition of an event also requires the addition of the time-slip variable, T .
The behaviour of this variable is controlled, in particular, by a new flow constraint in the
running state that ensures that if the event precondition becomes true then the time-slip
starts to increase as soon as any time passes. The addition of this variable and its control
also propagates into the jump and flow conditions of the other states.
Finally we consider the case in which concurrent process effects occur and must be
combined. This is the general case to which we refer in the rest of this paper.
In any given logical state a subset of the relevant processes will be active, according to the
precise valuation of the metric fluents in the current state. The proposition context-flow(i,Π)
asserts that the rate of change of the ith variable is defined by precisely those processes
in Π, provided that they (and only they) are all active, and is not affected by any other
process. The following definition explains how the contributions to the rate of change of a
262

Modelling Mixed Discrete-Continuous Domains for Planning

Inv: T = 0
Flow:

d=0
a=0 v=0
Τ=0 ∨ Τ=1
stopped

startEngine
Jump: d’ = d
v’ = 0
a’ = 0
T’ = T
Inv: T = 0

accelerate
Jump: d’ = d
v’ = v
a’ = a +1
a < 0 ∨ v < 100
T’ = T
a ≥ 1 ∧ v ≥ 100 → Τ = 1

Flow:
d=v v=a a=0
Τ=0 ∨ Τ=1
running

stop
Jump:

v=0
d’ = d
v’ = 0
a’ = 0
T’ = T

Inv: T = 0
Flow:

d=0
v=0
Τ=0 ∨ Τ=1
engineBlown
a=0

engineExplode
decelerate
Jump: d’ = d
v’ = v
a’ = a −1
a < 0 ∨ v < 100
T’ = T

Jump: a ≥ 1
v ≥ 100
a’ = 0
v’ = v
d’ = d
T’ = T

Figure 11: A hybrid automaton constructed by the translation of a uniprocess planning
instance.

variable by several different concurrent processes are combined (see also Section 4.2). This
simply involves summing the contributions that are active at an instant. Here we assume
without loss of generality that contributions are increasing effects. Decreasing effects are
handled by simply negating the contributions made by these effects.
Definition 13 Combined Concurrent Effects Given a finite set of process effects, E =
e1 . . . ek , where ei is of the form (increase Pi (* #t Qi )), the combined concurrent
effect of E on the PNE P , called C(P, E), is defined to be
X

{Qi | i = 1, . . . k, P = Pi }

Given a set of processes, Π, the combined concurrent effect of Π on the PNE P , denoted
C(P, Π), is C(P, E), where E is the set of all the effects of processes in Π.
It will be noted that if E contains no processes that affect a specific variable, P , then
C(P, E) = 0.
Definition 14 Context-flow If v is a logical state for a planning instance I of dimension
n and Π is a subset of Pv , then the context-flow proposition is defined as follows.
^
^
context-flowv (i, Π) = (
N (P rep ) ∧
¬N (P rep )) → Ẋi = C(Xi , Π)
p∈Π

p∈Pv \Π

where 1 ≤ i ≤ n.
263

Fox & Long

If Π is empty then the context flow proposition asserts that Ẋi = 0 for each i.
The interpretation of a Planning Instance extends the interpretation of a Uniprocess
Planning Instance. Again, the added components are underlined for convenience.
Definition 15 Semantics of a Planning Instance A planning instance I = (Dom, P rob)
is interpreted as a Hybrid Automaton, HI , as follows:
• Variables. The variables of HI are X = {X1 , . . . , Xn+1 }, where n is the dimension
of the planning problem. The n + 1th variable is a special control variable used to
measure time-slip .
• Control Graph. The set of vertices V is formed from all subsets of the ground atoms
of the planning instance. The set of edges in E contains an edge e between v and v 0
iff there is an action or event, a, relevant in v and:
v 0 = (v − Dela ) ∪ Adda
The action or event a is associated with the edge e.
• Initial, invariant and flow conditions. The vertex labelling function init is defined
as:

f alse
if v 6= Initlogical
V
init(v) =
N (Initnumeric ) ∧ {Xi =⊥ |Xi 6∈ N (Initnumeric )} otherwise
The vertex labelling function inv is the simple proposition that ensures time-slip is
zero.
inv(v) = (Xn+1 = 0)
The vertex labelling function flow is defined:
f low(v) = (

n
^

^

context-flowv (i, Π)) ∧ time-slippage(Ev )

i=1 Π∈P(Pv )

• Jump conditions. The edge labelling function jump is defined as follows. Given an
edge e from vertex v, associated with an action a:
^
jump(e) = N (P rea ) ∧ U Fa (X) = X 0 ∧
¬N (P reev )
ev∈Ev

Given an edge e from vertex v, associated with an event ev:
jump(e) = N (P reev ) ∧ U Fev (X) = X 0
• H-events. Σ is the set of all names of all ground actions and events. The edge
labelling function hevent : E → Σ assigns to each edge the name of the action or
event associated with the edge.
264

Modelling Mixed Discrete-Continuous Domains for Planning

The final conjunct in the jump definition for actions ensures that the state cannot be
left by an action if there is an event the preconditions of which are satisfied. It is possible
for more than one event to be simultaneously applicable in the same state. This is discussed
further in Section 6.4.
As an illustration of this final extension in the sequence of definitions, we now add one
further process to the preceding example:
(:process windResistance
:parameters ()
:precondition (and (running) (>= (v) 50))
:effect (decrease (v) (* #t (* 0.1 (* (- (v) 50) (- (v) 50)))))
)
This process causes the vehicle to be slowed by a wind resistance that becomes effective
from 50mph, and is proportional to the square of the speed excess over 50mph. This leads
to the flow constraint in the running state having two new clauses that replace the original
constraint on the rate of change of velocity. These new clauses are shown in the second
box out on the right of Figure 12. As can be observed, the velocity of the vehicle is now
governed by two different differential equations, according to whether v < 50 or v ≥ 50.
These equations are:
dv
if v < 50
dt = a,
dv
2 , if v ≥ 50
=
a
−
0.1(v
−
50)
dt
The solution to the first is: v = at + v0 where v0 is the velocity at the point when the
equation first applies (and t is measured from this point). The solution to the second is:
√
√
c0 ( 10a − 50)e−c1 t + 50 + 10a
v=
1 − c0 e−c1 t
√

and c0 is a constant determined by the initial value of the velocity when
where c1 = 10a
5
the process first applies (and, again, t is then measured from that point). As is shown by
the this example, the simple differential equations that can be expressed in pddl+ can lead
to complex expressions. Of course, there is a significant difference between the provision
of a semantics for this expressiveness and finding a planning algorithm that can manage
it — in this paper we are concerned only with the former. We anticipate that planning
will require sensible constraints on the extent to which the expressive power of pddl+ is
exploited.
The definition of the semantics of a planning instance is constructed around a basic
framework of the discrete state space model of the domain. This follows a familiar discrete
planning model semantics. The continuous dimensions of the model are constructed to
ensure that the Hybrid Automaton will always start in an initial state consistent with the
planning instance (modelled by the init labelling function). The state invariants ensure
that no time-slip occurs in the model and that, therefore, events always occur once their
preconditions are satisfied. It will be noted that since the negation of the event preconditions
is added to all jump conditions for other exiting transitions, it will be impossible for a state
to be exited in any other way than by the triggered event. Finally, flow models the effect
265

Fox & Long

Inv: T = 0
Flow:

d=0
a=0 v=0
Τ=0 ∨ Τ=1
stopped

startEngine
Jump: d’ = d
v’ = 0
a’ = 0
T’ = T
Inv: T = 0

accelerate
Jump: d’ = d
v’ = v
a’ = a +1
a < 0 ∨ v < 100
T’ = T
a ≥ 1 ∧ v ≥ 100 → Τ = 1

Flow:
d=v a=0
Τ=0 ∨ Τ=1
running

stop
Jump:

v=0
d’ = d
v’ = 0
a’ = 0
T’ = T

v ≥ 50 → v = a − 0.1 (v − 50)
v < 50 → v = a

2

Inv: T = 0
Flow:
engineExplode
decelerate
Jump: d’ = d
v’ = v
a’ = a −1
a < 0 ∨ v < 100
T’ = T

Jump: a ≥ 1
v ≥ 100
a’ = 0
v’ = v
d’ = d
T’ = T

d=0
v=0
Τ=0 ∨ Τ=1
engineBlown
a=0

Figure 12: A hybrid automaton constructed by the translation of a planning instance.

on the real values of all of the active processes in a state. The flow function assigns a
proposition to each state that determines a piece-wise continuous behaviour for each of the
real values in the planning domain. The function is piece-wise differentiable, with only a
finite number of segments within any finite interval. The reason for this is that it is possible
for the behaviour of a metric fluent undergoing continuous change to affect the precondition
of a process and cause the continuous change in itself or of other metric fluents to change.
Such a change cannot cause discontinuity in the value of the metric fluents themselves,
but it can cause discontinuity in the derivatives. The consequence of this is that the time
interval between two successive actions or events might include a finite sequence of distinct
periods of continuous change. As will be seen in the following section, this requires that
an acceptable trace describing such behaviour will explicitly subdivide the interval into a
sequence of subintervals in each of which the continuous change is governed by a stable set
of differential equations.
6.3 Semantics of HAs
Henzinger gives a semantics for HAs by constructing a mapping to Labelled Transition Systems (Keller, 1976). Figure 13 shows the complete semantic relationship between planning
instances and labelled transition systems and between plans and accepting runs. The top
half of the figure shows the relationship already constructed by Henzinger to give a semantics to Hybrid Automata. This completes the bridge between planning instances and
labelled transition systems. The details of the mapping from Hybrid Automata to labelled
transition semantics are provided in this section.
The following definitions are repeated from Henzinger’s paper (1996).
266

Modelling Mixed Discrete-Continuous Domains for Planning

Labelled
transition
systems

Accepting
runs

implies

interpretation

THA

interpretation
implies

Traces

interpretation
interpretation
representation
Planning
instance

Plans
implies

Figure 13: The semantic mapping between HAs and LTS
Definition 16 Labelled Transition System A labelled transition system, S, consists of
the following components:
• State Space. A (possibly infinite) set, Q, of states and a subset, Q0 ⊆ Q of initial
states.
• Transition Relation. A (possibly infinite) set, A, of labels. For each label a ∈ A a
a
a
binary relation → on the state space Q. Each triple q → q 0 is called a transition.
Definition 17 The Transition Semantics of Hybrid Automata The timed transition
t of the Hybrid Automaton H is the labelled transition system with components
system SH
a
0
Q, Q , A and →, for each a ∈ A, defined as follows:
• Define Q, Q0 ⊆ V × Rn , such that (v, ~x) ∈ Q iff the closed proposition inv(v)[X := ~x]
is true, and (v, ~x) ∈ Q0 iff both init(v)[X := ~x] and inv(v)[X := ~x] are both true. The
set Q is called the state space of H.
• A = Σ ∪ R≥0 .
σ

• For each event σ ∈ Σ, define (v, ~x) → (v 0 , ~x0 ) iff there is a control switch e ∈ E
such that: (1) the source of e is v and the target of e is v 0 , (2) the closed proposition
jump(e)[X, X 0 := ~x, ~x0 ] is true, and (3) hevent(e) = σ.
δ

• For each non-negative real δ ∈ R≥0 , define (v, ~x) → (v 0 , ~x0 ) iff v = v 0 and there is a
differentiable function f : [0, δ] → Rn , with the first derivative f˙ : (0, δ) → Rn such
that: (1) f (0) = ~x and f (δ) = ~x0 and (2) for all reals  ∈ (0, δ), both inv(v)[X := f ()]
and f low(v)[X, Ẋ := f (), f˙()] are both true. The function f is called a witness for
δ

the transition (v, ~x) → (v 0 , ~x0 ).
It is in this last definition that we see the requirement that each interval of continuous change in a timed transition system should be governed by a single set of differential
267

Fox & Long

equations, with a single solution as exhibited by the (continuous and differentiable) witness
function.
The labelled transition system allows transitions that are arbitrary non-negative intervals of time, during which processes execute as dictated by the witness function for the
corresponding period. In our definition of plans (Definition 3) we only allow rational times
to be associated with actions, with the consequence that only rational intervals can elapse
between actions. This means that plans are restricted to expressing only a subset of the
transitions possible in the labelled transition system. We will return to discussion of this
point in the following section, where we consider the relationship between plans and accepting runs explicitly.
6.4 Interpretation of Plans and Traces
We complete the two-layered semantics presented in Figure 13 by showing how a plan is interpreted using Henzinger’s notion of trace acceptance. This will conclude our presentation
of the formal semantics of pddl+.
Using Henzinger’s syntax as a semantic model, we first map plans to traces and then
rely on the interpretation of traces in terms of accepting runs. A plan is a set of timestamped actions (Definition 3): neither events nor processes appear in the specification of
a plan, even though the planned actions can initiate them. By contrast, since Henzinger
does not distinguish between actions and events, a trace through a HA contains control
switches which might be actions or events, together with explicit time intervals between
them. Plans are finite, so we are concerned only with finite traces, but plans are normally
subsets of traces, because of the missing events and the possible subdivision of intervals
between actions into distinct subintervals of continuous activity.
We define a new structure, the plantrace, which contains the sequence of control switches
corresponding to the actions in a plan being interpreted. We then map plantraces to sets
of traces and proceed as indicated above.
Definition 18 Plantrace Let H be a Hybrid Automaton, with h-event set Σ partitioned
into two subsets, A, the actions, and E, the events. A plantrace of H is an element of the
language (Q>0 A+ )∗ .
A plantrace consists of sequences of one or more action control switches (denoted by A+ ),
each following a single time interval which must be greater than 0 in length (0 length
intervals are not allowed because the actions on either side of them would actually be
occurring simultaneously). For example, the sequence h3 a0 a1 a2 2.7 a3 a4 i is a plantrace.
Note that we have only allowed rational valued intervals between actions. This is to be
consistent with the history of pddl in which irrational time points have not been considered.
In the semantics of Hybrid Automata actions that occur at the same time point are
considered to be sequenced according to the ordering in which they are recorded in the
trace. In reality, it is not possible to execute actions at the same time and yet ensure that
they are somehow ordered to respect the possible consequences of interaction. In order to
respect this constraint we introduce an additional element in the interpretation of plans: we
consider the impact of ordering actions with the same time stamp in a plan in all possible
permutations in order to confirm that there are no possible interactions between them. This
motivates the following definition:
268

Modelling Mixed Discrete-Continuous Domains for Planning

Definition 19 Permutation equivalent plantraces Two plantraces τ1 and τ2 , are permutation equivalent, written τ1 ≡ τ2 if τ1 can be transformed into τ2 by permuting any
subsequence that contains only actions.
Definition 20 Plan projection The projection of a plan, P , yields a plantrace proj(P )
as follows. Assume that the plan (a sequence of pairs of times and action instance names)
is given sorted by time:
proj(P )
= proj2(0, P )
proj2(t, hi)
= hi
proj2(t, h(t1 , a), resti) = hai + proj2(t1 , rest), if t = t1
= ht1 − t, ai + proj2(t1 , rest), otherwise
Plan projection is a functional description of the process by which plans are interpreted as
plantraces. This process involves constructing the sequence of intervals between collections
of actions that share the same time of execution, interleaved with the sequences of actions
that occur together at each execution time. The most significant point to make is that
where actions are given the same time for execution, the order in which they occur in the
plantrace is determined simply by the (arbitrary) order in which they are listed in the plan.
This does not affect the interpretation of the plan as we see in the following definition.
Definition 21 Interpretation of a plan The interpretation of a plan P for planning
instance I is the set P TP of all plantraces for HI that are permutation equivalent to proj(P ).
By taking all plantraces that are permutation equivalent to the projection of the plan we
remove any dependency of the interpretation of a plan on the ordering of the actions with
the same time stamp. Our objective is to link the validity of plans to the acceptance of
traces, so we now consider trace acceptance.
Henzinger defines trace acceptance for a trace of an HA. The definition is equivalent to
the following:
Definition 22 Trace Acceptance A trace, τ = hai ii=1,...,n where ai ∈ Σ ∪ R≥ is accepted
by H if there is a sequence r = hqi ii=0,...,n , where the qi are states in the timed transition
t of H, and:
system SH
• q0 ∈ Q0 .
a

t .
• For each i = 1, . . . , n, qi−1 →i qi is a transition in SH

r is called an accepting run for τ in H and, if qn = (v, ~x), we say that it ends in state v
with final values ~x.
A plan does not contain the transitions that represent events. For this reason, we make
the following definition:
Definition 23 Trace abstraction A trace, τ , for a Hybrid Automaton with h-events partitioned into two sets, the actions A and the events E, is abstracted to create a plantrace by
removing all events in τ and then replacing each maximal contiguous sequence of numbers
with a single number equal to the sum of the sequence. Finally, if the last value in this
modified trace is a number it is removed.
269

Fox & Long

We can now use these definitions in the interpretation of plantraces. To avoid unnecessary multiplication of terms, we reuse the term accepted and rely on context to disambiguate
which form of acceptance we intend in its use.
Definition 24 Plantrace Acceptance Given a Hybrid Automaton, H, with h-event set
Σ partitioned into action A and events E, a plantrace, τ , is accepted by H if there is a
trace τ 0 that is accepted by H such that τ is an abstraction of τ 0 .
It is important to observe that this definition implies that checking for acceptance of a
plantrace could be computationally significantly harder than checking for standard trace
acceptance. This is because the test requires the discovery of events that could complete
the gaps between actions in the plantrace. However, since the events are constrained so
that if they are applicable then they are forced to be applied, provided we restrict attention to commuting events, the problem of determining plantrace acceptance does not
involve searching through alternative event sequences. If reasonable constraints are placed
on the kinds of event cascades that may interleave between actions, the problem of checking
plantrace acceptance becomes straightforward.
Finally, we return to plans and consider which plans are actually valid.
Definition 25 Validity of a Plan A plan P , for planning instance I, is valid if all of the
plantraces in P TP are accepted by the Hybrid Automaton HI . The plan achieves the goal
G of I if every accepted trace with an abstraction in P TP ends in a state that satisfies G.
The constraint that simultaneously executed actions are non-mutex is sufficient to ensure
that it is only necessary to consider one representative from the set of all permutation
equivalent plantraces in order to confirm validity of a plan.
The definitions we have constructed demonstrate the relationship between plans, plantraces,
traces and accepting runs. It can be observed that the definitions leave open the possibility
that events will trigger in a non-deterministic way. This possibility arises when more than
one event is applicable in the same state and the events do not commute. In this case,
a non-deterministic choice will be made, in any accepting run, between all the applicable
events. It is not possible for an action to execute before any of the events in such a state
because of the time-slip process, but that process does not affect which of the events is
applied. The non-deterministic choice between applicable events would allow pddl+ to
capture actions with non-deterministic outcomes. For the purposes of this paper we restrict
our attention to event-deterministic planning instances.
Definition 26 Event-deterministic Planning Instances A pddl+ planning instance,
I, is event-deterministic if in every state in HI in which two events, e1 and e2 , are applicable,
the transition sequences e1 followed by e2 and e2 followed by e1 are both valid and reach the
same resulting state. In this case e1 and e2 are said to commute.
If every pair of events that are ever applicable in any state commute then the planning
instance is event-deterministic. In general, deciding whether a planning instance is eventdeterministic is an expensive operation because the entire state space must be enumerated.
However, it is much easier to construct event-deterministic planning instances because it is
only necessary to consider whether pairs of events will commute. In particular, non-mutex
events will always commute.
270

Modelling Mixed Discrete-Continuous Domains for Planning

We conclude by making some observations about the relationship between plans and
accepting runs. Firstly, every valid plan corresponds to a collection of accepting runs for
the labelled transition system that corresponds to the HA that is the interpretation of the
planning instance. The only difference between accepting runs corresponding to a given
plan is in the order in which events or actions are executed at any given single time point.
In contrast, there can be accepting runs for which there is no corresponding plan. This
situation arises when a domain admits accepting runs with actions occurring at irrational
time points. It would be possible to extend plans to allow irrational timestamps for actions.
The restriction to rationals is based on the fact that an explicit report of a plan generated
by a planner can only make use of timestamps with a finite representation, so there are only
countably many plans that can be expressed. The fact that there are uncountably many
possible transitions based on the use of arbitrary real time values is of no use to us if a
planner cannot express them all. A further point of relevance is the observation, discussed
in (Gupta et al., 1997), that in constructing plans for execution it is of no practical interest
to rely on measurement of time to arbitrary precision. Instead, it is more appropriate to
look for plans that form the core of a fuzzy tube of traces all of which are accepted. In this
case, the difference between rational and irrational timestamps becomes irrelevant, since any
irrational value lies arbitrarily close to a rational value, so any robust plan can be represented
using rational timestamps alone. We consider that this is an important direction for future
exploration, where planning problems require an additional specification of a metric and a
size for the fuzzy tube that the solution plan must define, with traces in the tube having a
high probability of acceptance (Fox, Howey, & Long, 2005).

7. Analysis
In the previous sections we have constructed a semantics for pddl+ by mapping to Hybrid
Automata and we have constructed a formal relationship between plans and traces. As a
consequence, we can demonstrate that in general pddl+ domains provide succinct encodings
of their corresponding Hybrid Automata, since a pddl+ model can have a state space that
is exponential in the size of the encoding.
Having established the relationship between pddl+ domains and Hybrid Automata,
we can now benefit from the large body of research into Hybrid Automata and their subclasses. One issue that has been widely addressed is the boundary between decidable and
undecidable classes of Hybrid Automata and this boundary can be reinterpreted for the
reachability question for interesting subsets of the pddl+ language.
In the following we only consider subsets of pddl+ that are interesting in the sense of
modelling different kinds of restricted continuous temporal behaviours.
7.1 Reachability within Hybrid Automata
The Reachability problem for Hybrid Automata is the problem of determining, given an
t , that visits
automaton H, whether there is a trajectory of the timed transition system, SH
a state of the form (v, x). In the context of Hybrid Automata, v is typically an error state,
so that Reachability questions are posed to determine whether an automaton is safe. In
the context of planning, the Reachability question is equivalent to Plan Existence. As is
discussed in Henzinger’s paper (1996), the general Reachability question for Hybrid Au271

Fox & Long

tomata is undecidable. This is unsurprising, since introducing metric fluents with arbitrary
behaviours into the language results in sufficient expressive power to model Turing Machine
computations. Indeed, Helmert has shown (2002) that even relatively simple operations
on discrete metric variables are sufficient to create undecidable planning problems. However, under various constraints, Reachability is decidable for several kinds of hybrid system,
including Initialised Rectangular Automata (Henzinger et al., 1998). In the following discussion we restrict our attention to deterministic Initialised Rectangular Automata, focussing
particularly on Timed Automata (Alur & Dill, 1994), Priced Timed Automata (Rasmussen
et al., 2004) and Initialised Singular Automata (Henzinger, 1996) and their relationships
with fragments of the pddl family of languages.
To simplify the definition of Rectangular Automata we introduce the following definition:
Definition 27 Interval Constraint Any constraint on a variable x of the form x ./ c for
a rational constant c and ./∈ {≤, <, =, >, ≥} is an interval constraint. A conjunction of
interval constraints is also an interval constraint.
Rectangular Automata are Hybrid Automata in which all initial, invariant and flow
conditions are interval constraints, whose flow conditions refer only to variables in Ẋ and
whose jump conditions are a conjunction of interval constraints and constraints of the form
x0i = xi . Rectangular Automata may be non-deterministic because the interval constraints
that determine the initial, flow and jump conditions might not determine unique values
for the variables they constrain. Initialised Rectangular Automata meet the additional
constraint that for each control switch, e, from v to w, if f low(v)i 6= f low(w)i then x0i = xi
does not appear in jump(e).
Initialised Rectangular Automata are important because they represent the boundary
of decidability for Hybrid Automata (Henzinger et al., 1998).
Since pddl is a deterministic language, we are most interested in the deterministic
version of Rectangular Automata. A Singular Automaton is a Rectangular Automaton with
deterministic jumps and variables of finite slope (that is, the flow conditions determine
a unique constant rate of change for each variable). It is initialised if each continuous
variable is reset every time its rate of change is altered by the flow function. The fact that
the variables have finite slope allows rates of change to be modified. Initialised Singular
Automata allow the modelling of linear continuous change and the rate of change to be
modified provided that the corresponding variable is reset when this occurs. This constraint
prevents the modelling of stopwatches (Alur & Dill, 1994), so ensures decidability of the
Reachability problem. Whilst quite expressive, such automata cannot capture the dynamics
that arise in many continuous planning domains. For example, it would not be possible to
model the effect, on the level of water in a tank, of adding a second water supply some time
into the filling process. According to the reset constraint the level value would have to be
reset to zero when the second water source is introduced.
A Timed Automaton is a Singular Automaton in which every variable is a clock (a
one-slope variable). These automata can be used to model timed behaviour and to express
constraints on the temporal separation of events (Alur & Dill, 1994). However, they cannot
be used to model the continuous change of other quantities because clocks cannot be used to
store intermediate values. They can be stopped and reset to zero, but they cannot operate as
memory cells. Alur and Dill (1994) prove that reachability is decidable in Timed Automata
272

Modelling Mixed Discrete-Continuous Domains for Planning

because the infinite part of the model (the behaviour of the clocks) can be decomposed
into a finite number of regions. This demonstrates that the infinite character of Timed
Automata can be characterised by an underlying finite behaviour. Helmert (2002) used a
similar regionalisation technique in proving that the plan existence question for planning
models including certain combinations of metric conditions and effects remains decidable.
A Priced Timed Automaton is a Timed Automaton in which costs are associated with
the edges and locations of the automaton. Costs are accumulated over traces and enable a
preference ordering over traces. PTAs have been used to solve simple Linear Programming
and scheduling problems (Rasmussen et al., 2004). Costs behave like clocks except that
they can be stopped, or their rates changed, without being reset. To retain decidability
despite the addition of cost variables their use is restricted: costs cannot be referred to in
jump conditions although they can be updated following both edge and delay transitions.
PTAs have been used to model planning problems in which actions are associated with
linearly changing costs. For example, the Airplane scheduling bench mark, in which planes
incur cost penalties for late and early landing, can be expressed in the syntax of pddl+
and solved using PTA solution techniques (Rasmussen et al., 2004). The models are very
restrictive: the dependence of the logical dynamics of the domain on the cost values cannot
be expressed so costs cannot, for example, be used to keep track of resource levels during
planning when resources are over-subscribed. However, they are capable of expressing a
class of problems which require the modelling of continuous change and can therefore be
seen as a fundamental step towards the modelling of mixed discrete-continuous domains.
Since PTAs allow the modelling of non-trivial planning domains with continuous change
we begin by constructing a fragment of pddl+ that yields state-space models exactly equivalent to PTAs. In the remainder of this section we then discuss the relationships between
richer pddl+ fragments and different automata.
7.2 pddl+ and Priced Timed Automata
Rasmussen et al. (2004) describe the components of a Priced Timed Automaton (PTA) as
follows. It contains a set of real-valued clocks, C, over which constraints can be expressed by
a set of clock constraints, B(C). There is a set of actions, Act. A PTA is given as a 5-tuple
(L, l0 , E, I, P ), where L is a finite set of locations (that is, states), l0 is the initial location,
E ∈ L × B(C) × Act × 2C × L, I : L → B(C) is a function assigning invariants to locations
and P : (L ∪ E) → N assigns prices to edges and locations. An edge, E = (l, b, a, r, l0 ) is a
transition between locations l and l0 using action a, which has the jump condition b (over
the clocks) and resets the clocks in r to zero. The price function represents a discrete cost
for transitions and a continuous cost associated with staying in each location.
For the modelling of arbitrary Priced Timed Automata we define a pddl+ fragment
which we refer to as pddl+P T A . The following definition of this fragment is not unique.
Other subsets of pddl+ exist with the same expressive power, including subsets directly
relying on processes and events. By excluding events in this fragment we are trivially
guaranteed to have a deterministic language.
The pddl+P T A fragment uses the standard pddl+ language features, but subject to
the following constraints:
273

Fox & Long

1. Processes may have only logical preconditions and exactly one process must be active
in each state except, possibly, one special state, error.
2. Each process must increase all the metric fluents at rate 1, except for one special
variable, c, which may be increased at any constant rate.
3. Each action may have preconditions that refer to any of the metric fluents except
c. These preconditions must appear in any of the forms (called clock constraints)
conforming to the following: (./ xi n) or (./ (- xi xj ) m) where n, m are natural
numbers and ./∈ {<, ≤, =, ≥, >}.
4. Each action may reset the value of any metric fluent, other than c, to zero. They may
increase the value of c by any constant value.
5. The domain may contain events whose precondition may include literals and clock
constraints. The effect of every event must be to leave the system in the special state
error, from which no further transitions are possible.
6. The plan metric is of the form: (:metric minimize (c)).
These constraints ensure that the state space yielded by a pddl+P T A description, and its
transition behaviour, is equivalent (within a constant multiple encoding size) to a PTA. We
cannot demonstrate direct equivalence between pddl+P T A and PTAs because pddl+P T A
models can be exponentially more compact than the explicit PTA to which it corresponds.
This compaction is similar to that which can be obtained by factorising PTA models (Dierks,
2005). We demonstrate the indirect equivalence of the two languages in Theorem 2.
Definition 28 Indirectly Equivalent Expressiveness Given two languages, L1 and L2 ,
L1 is indirectly equivalent in expressive power to L2 if each sentence, s1 , in L1 defines a
model, Ms1 , such that L2 can express Ms1 with only a polynomial increase in encoding size
(in the size of Ms1 ) and each sentence, s2 , of L2 can be expressed in L1 with at most a
polynomial increase in the size of the encoding (in the size s2 ).
We note that this definition is asymmetric: sentences of L1 define models that can
be expressed efficiently in L2 , but sentences of L2 can be efficiently expressed directly in
L1 . The sentences of L1 might be compact encodings of their corresponding models and
therefore we cannot claim a direct equivalence in expressiveness between L1 and L2 . This
is intentional and allows us to exploit the pddl property that it is a compact encoding
language for the state spaces and transition behaviours of the planning domains it defines.
Theorem 2 pddl+P T A has indirectly equivalent expressive power to Priced Timed Automata.
The proof of this theorem can be found in Appendix B.
Lemma 1 A sentence, s, of pddl+ defines a transition system that is at most doubly
exponential in the size of s.
274

Modelling Mixed Discrete-Continuous Domains for Planning

This is straightforward: the set of literals defined by a pddl+ problem instance is at
most exponential in the number of parameters in the predicate of the highest arity and the
state space this defines is at most exponential in the size of the set of literals.

Corollary 1 Reachability in pddl+P T A is decidable.
This follows from Theorem 2, Lemma 1 and the decidability of PTAs (Larsen, Behrmann,
Brinksma, Fehnker, Hune, Pettersson, & Romijn, 2001).

pddl+P T A can be extended, while remaining decidable, by allowing additional metric
fluents whose (non-continuous) behaviour is constrained according to one of the decidable
subsets identified by Helmert (2002), provided that these fluents are distinct from the clock
and cost variables.
We can similarly define a language fragment with expressive power equivalent to Initialised Singular Automata. Initialised Singular Automata represent the most expressive
form of deterministic automaton with a decidable Reachability problem. Their constraints
limit the extent to which we can reason about the dependence of behaviour on temporal
and metric quantities.
In an Initialised Singular Automaton continuous change is not restricted to variables of
slope 1, in contrast to the clocks of Timed Automata. All variables have finite slope (that
is, their rates of change can only take one of a finite set of different values), but must be
initialised to a given constant whenever their rates of change are altered. The values of
these variables can be referred to in the jump conditions of the automaton.
For the modelling of Initialised Singular Automata we define a pddl+ fragment which we
refer to as pddl+ISA . This fragment contains the same syntactic components as pddl+P T A
but is subject to slightly different constraints:
1. Processes may have only logical preconditions and exactly one process must be active
in each state except, possibly, one special state, error.
2. Each process must increase all the metric fluents at constant rates.
3. Each action may have preconditions that refer to any of the metric fluents. These
preconditions must appear in any of the forms (called clock constraints): (./ xi n) or
(./ (- xi xj ) m) where n, m are natural numbers and ./∈ {<, ≤, =, ≥, >}.
4. Each action may reset the value of any metric fluent to any constant natural number
value. Any action that causes a transition to a state where a new process is active
must also reset all metric fluents whose rates of change are different under the new
process.
5. The domain may contain events whose precondition may include literals and clock
constraints. The effect of every event must be to leave the system in the special state
error, from which no further transitions are possible.
275

Fox & Long

This fragment does not contain a special plan metric: problems in pddl+ISA are of interest
for the plan existence problem.
Theorem 3 pddl+ISA has indirectly equivalent expressive power to Initialised Singular
Automata.
The proof of this result is analogous to Theorem 2. The constraints ensure that the
behaviour of Initialised Singular Automata is captured in this language and that the language contains only expressions that can be effectively modelled within Initialised Singular
Automata.
It is of interest to review the domains we have considered in this paper, the planetary
lander domain and the accelerating vehicle domain, with respect to the modelling power
of pddl+P T A and pddl+ISA . In the first instance, since both domains involve non-linear
change, in the power generation and state of charge curves and in the distance travelled
by the vehicle when acceleration is non-zero, it is clear that both domains lie outside the
modelling power of these constrained languages. It is feasible to consider approximating
the non-linear behaviour in some cases. For example, the generation curve might be approximated by a small set of linear functions (with a corresponding loss of opportunities to
exploit the margins of the model). The generation curve can be tied to absolute points on
the timeline and the values of the points at which the linear functions would be required to
meet in order to approximate the original curve can be identified in advance. In order to
create a reasonable approximation, the functions would require different slopes (shallow at
the start, then steeper and then shallower again, to approximate the first half of the bell
curve), which cannot be achieved using pddl+P T A , since this only allows clock variables.
pddl+ISA is powerful enough to express differently sloped linearly changing variables of
this kind. Unfortunately, the state of charge curve, even with approximations, is beyond
the expressive power of either language. For pddl+ISA the problem is that there must
be a memory of the old state of charge when the slope of the charge curve changes. With
pddl+P T A the cost variable could be used to model the state of charge, since it has the
capacity to both be used as a memory and to have different rates of change. However, the
cost variable cannot be used in preconditions of any actions, which means that any attempt
to model the battery in this way would force a decoupling between the battery state of
charge and the actions that use power. A more realistic model of the battery management
lies outside the power of pddl+P T A and pddl+ISA .
However, other interesting problems involving continuous change are perfectly amenable,
as Dierks (2005) and Behrmann et al. (2005) have shown. The Aircraft Landing problem (Beasley, Krishnamoorthy, Sharaiha, & Abramson, 2000) can be modelled as a PTA
because it has one source of continuous change which can be modelled as a cost variable. In
this domain, some number of aircraft must land on a single airport, sometime between an
earliest and latest landing time and as close to a target time as possible. The earliest, latest
and target times are defined for each aircraft. A cost is associated with the problem and
there is a charge associated with landing a plane early or late, where the charge decreases
linearly from the earliest landing time towards the target time and increases linearly from
the target up to the latest landing time. The behaviour of the aircraft is not dependent
on the value of the cost variable, although the quality of a landing schedule is determined
276

Modelling Mixed Discrete-Continuous Domains for Planning

Deterministic Hybrid Automata

Hybrid Automata

Linear Automata

PDDL+

Undecidable
Deterministic Linear Automata

PDDL+

All metric fluents
besides clock and
cost variables
satisfy one of
Helmert’s
constraint sets.

#t lin

Rectangular Automata

PDDL+ ISA

Initialised Singular Automata

PDDL+ PTA

Priced Timed Automata

Deterministic Timed Automata
DPDDLTT+SE
DPDDLTT
DPDDLSE

Initialised Rectangular Automata

Decidable
Non−deterministic TA

Finite Automata

Deterministic

Non−deterministic

DPDDL
NPDDL

Figure 14: Mapping pddl fragments to corresponding deterministic automata. pddl is
a family of deterministic languages. However, extension to support temporal,
numeric or logical non-determinism would give access to the automata on the
right hand side of the figure.

by it. The Aircraft Landing problem is an optimisation problem where the quantity being
optimized changes continuously over time.
In Figure 14 we present some formal relationships between pddl+ fragments and classes
of hybrid automata. The bottom half of the figure concerns decidable fragments of pddl+
whilst the top half concerns undecidable fragments. In the figure, dpddl is the pddl+
fragment in which fixed duration durative actions are allowed (and no events or processes),
restricted to those with no at start effects and npddl (non-temporal pddl) is the fragment
in which they do not occur. Initial effects are significant because it is these that make it
possible to create domains in which there are problems that can only be solved by exploiting
concurrency. The reason for this is that if all effects are restricted to the end points of
actions, then it is always possible to find a sequential plan to solve a problem for which
there is a concurrent solution. In cases where concurrency is not required to solve a problem,
durative actions can be sequentialised and their durations simply summed as discrete end
effects. When actions can have initial effects then it is possible for there to be effects that
are added at the start of an action and deleted at the end, creating windows of opportunity
into which other actions must be fitted concurrently if they are to exploit the effects. Once
concurrency matters it is necessary to monitor the passage of time and this requires the
power of timed automata. The other language variants we show are as follows:
277

Fox & Long

• dpddlSE which allows start effects, raising the need for concurrency as explained
above.
• dpddlT T , which is dpddl together with the use of plan metrics using the term
total-time: this also requires concurrency since the time required by a plan depends
on the extent to which non-interfering actions can be selected to reduce the make-span
of the plan.
• dpddlT T +SE which is dpddl together with total-time and allowing the use of start
effects.
• pddl+#t lin is pddl+ restricted to linear rates of change on all metric fluents. This
restriction is insufficient to ensure decidability, but the use of linear rates of change
makes it possible to apply linear constraint solvers to the problems. As a consequence,
this subset of pddl+ captures the continuous planning problems to which planning
technology has already been applied (Shin & Davis, 2005; McDermott, 2003b; Wolfman & Weld, 1999; Penberthy & Weld, 1994) (in the first of these, in particular, the
models are actually expressed in pddl+).
In the bottom half of the figure both npddl and dpddl include metric conditions and effects constrained to occur in decidable combinations as defined by Helmert (2002). Helmert
demonstrated that adding (discrete) metric effects and conditions to the propositional pddl
fragment already adds dramatically to the expressive power of the language. In particular,
with a quite limited set of (discrete) metric effects and conditions decidability is already
lost. However, there are restrictions to the use of metric variables that leave a decidable
fragment. In order for the extensions we are discussing to retain decidability we must not,
of course, sacrifice it by adopting too rich a set of discrete metric effects or preconditions.
This is why in the lower half of the figure we work within these constraints. Our results
extend Helmert’s by introducing continuous metric change, while demonstrating boundaries
that retain decidability. In the top half of the figure these constraints are lifted. The lefthand half of the figure concerns deterministic models: pddl+ is a language for expressing
deterministic domains, so we have restricted our attention to this side. The right-hand half
concerns non-deterministic variants of the models considered. We include these only to
provide a general context for our results.

8. Related Work
In this section we discuss the relationship between pddl+ and several other related formalisms in the literature. In addition, we consider the extent to which pddl+ addresses
the inadequacies of pddl2.1 in terms of expressive power and convenience. Our paper on
pddl2.1 was published in the Journal of Artificial Intelligence Research Special Issue on
the 3rd IPC (Fox & Long, 2003). The editors of this special issue invited five influential
members of the planning community to contribute short commentaries on the language, indicating their support for, and objections to, the choices we made in the modelling of time
and time-dependent change. These were Fahiem Bacchus, Mark Boddy, Hector Geffner,
Drew McDermott and David Smith. We now discuss the parts of their commentaries that
are relevant to the modelling of durative behaviour and continuous change and explain how
278

Modelling Mixed Discrete-Continuous Domains for Planning

we believe pddl+ addresses the issues raised. It is interesting to note that many of the
objections raised by these commentaries are addressed by the start-process-stop model of
pddl+. We begin by considering these commentaries and then go on to discuss related
formalisms.
8.1 pddl+ versus pddl2.1
The commentators were invited to comment on the decisions made in pddl2.1, and the
limitations they impose on temporal domain modelling. Some of the issues raised by the
commentators have been addressed in pddl+. In this section we identify the issues that
were raised that are relevant to the development of pddl+ and explain how we think pddl+
resolves them.
Bacchus (2003) proposes an alternative to the continuous durative actions of pddl2.1
which has similarities with the start-process-stop model of pddl+. Both approaches recognise that durative activity is sometimes best modelled using an underlying, interruptible
process. Whilst Bacchus proposes that the initiation and running of the processes be
wrapped up into durative actions with conditional effects, pddl+ achieves the same effect
with cleanly separated continuous autonomous processes and events. pddl+ can express
behaviours that are dependent on continuous variables other than time, while Bacchus’ proposal is limited to purely time-dependent processes (he does not allow interacting processes
so does not consider other forms of continuous change).
McDermott and Boddy have consistently supported the use of autonomous processes
in the representation of continuous change. In his commentary McDermott (2003a) identifies the weaknesses of the durative action-based representation of change and argues that
the continuous durative actions of pddl2.1, which allow the modelling of duration inequalities and time-dependent effects, are “headed for extinction in favour of straightforward
autonomous processes”. The start-process-stop model of pddl+ replaces the continuous
durative actions of pddl2.1 with constructs that fully exploit autonomous processes to support richer and more natural models of continuous domains. As we have shown in this
paper, the modelling of events adds expressive power as Boddy anticipates (Boddy, 2003).
In his commentary Smith (2003) raises some philosophical objections to the durative
action model of pddl2.1 which he argues is too restrictive to support convenient models
of interesting durative behaviours. In pddl2.1 actions specify effects only at their start
and end points, although conditions can be required to remain invariant over the whole
durative interval. Smith proposes a durative action model that is richer than that proposed
in pddl2.1, in which effects can occur at arbitrary points within the durative interval and
conditions might also be required to hold at identified timepoints other than at the start
or end of an action. Although, in principle, it is possible to decompose pddl2.1 durative
actions into sequences of actions that achieve these effects, Smith correctly observes that
this would generally result in inconvenient and impractical models. He argues that action
representations should encapsulate the many consequences of their application in a way that
frees a planner from the burden of reasoning about them in their minutiae. He observes that
the computational effort involved, in stringing together the sub-actions that are required to
realise a complex activity, would normally be prohibitive.
279

Fox & Long

We agree that the pddl2.1 durative action model is restrictive in forcing effects to
occur only at the end points of the actions. Smith’s rich durative model can be seen as
encapsulating the effects of starting and ending one or more processes, together with the
effects of these processes, into an action-based representation. By committing the activity
to a certain amount of time these actions abstract out the time-dependent details and avoid
the need for the planner to reason about them or their interactions. This is a simplification
that is no doubt sufficient in many practical contexts (and indeed, is sufficient for the
satellite domain that he discusses in his commentary). It might indeed be of interest to
provide such representations as abstractions of the start-process-stop model.
This point goes to the heart of the contribution of this paper: we have provided a
set of primitives for building modelling constructs. In providing a formal semantics for
these primitives we have provided a way of interpreting abstract constructs built from
these primitives. We argue that, in combination with natural modelling concepts like fixedlength durative actions, the start-process-stop primitives provide a usable planning domain
description language. However, we are concerned here with the formal underpinnings of
the language rather than with the modelling convenience it provides. We agree with Smith
that abstract modelling constructs, built from the primitives, might enhance the modelling
experience in the same way that abstract programming constructs enhance the programming
experience over programming at the machine code level.
8.2 Related Formalisms
A number of representational formalisms have been proposed for expressing temporal and
metric activity in planning. The closest recent counterpart to pddl+ is the modelling
language, Opt, of the Optop planner (McDermott, 2004). This language was developed independently, by Drew McDermott, at the same time as pddl+ was first proposed, and some
of the similarities between Opt and pddl+ are due to discussions between the developers
of the two languages at the time. Opt is a pddl-like dialect which was strongly influenced
by the work of McDermott and other authors on the pddl family of languages. Opt supports autonomous processes which run when their preconditions are satisfied and are not
under the control of the planner. Unlike pddl+, Opt does not contain explicit events these are embedded inside processes which run for as long as their preconditions remain
true. Opt also retains durative actions as an alternative to the explicit modelling of continuous change and models timed initial literals and derived predicates. A planner, Optop,
was developed by McDermott (McDermott, 2005) for the subset of Opt that models linear
continuous change. Planners already exist for handling interesting subsets of pddl+, both
directly (Shin & Davis, 2005) and indirectly (Dierks, 2005). In the later case, the language
of the PTA solver UPPAAL-cora (Behrmann et al., 2005) is used, which has modelling
power equivalent to that of pddl+P T A .
The semantics of Opt processes is given in terms of infinitely many situations occurring within a finite time, each associated with different fluent values of the continuously
changing variables. Opt and pddl+ are fundamentally related to Reiter’s work on continuous dynamics in the situation calculus (Reiter, 2001). McDermott developed a situation
calculus semantics for Opt, whereas we have constructed an explicit relationship between
pddl+ and the theory of Hybrid Automata in order to make explicit the relationship be280

Modelling Mixed Discrete-Continuous Domains for Planning

tween pddl+ planning and control theory. A similar relationship is drawn in the CIRCA
architecture (Musliner, Durfee, & Shin, 1993), which integrates planning with real time
control using probabilistic timed automata.
The qualitative reasoning community have proposed hybrid state-based models of dynamic physical systems (Forbus, 1984; de Kleer & Brown, 1984; Kuipers, 1984). Kuipers (1984)
considers the qualitative simulation of physical systems described in terms of continuously
varying parameters. He proposes a qualitative representation of the differential equations
governing the behaviour of a system, expressed as systems of constraints over the key
parameters describing the state of the system at discrete points in time. This representation supports commonsense reasoning about the evolution of physical systems about which
quantitative reasoning would be computationally prohibitive.
Other formalisms have been developed within the fields of planning and reasoning about
action and change. Temporal and resource management are provided in HSTS and Europa (Frank & Jónsson, 2003; Jónsson, Morris, Muscettola, Rajan, & Smith, 2000), IxTeT (Laborie & Ghallab, 1995), CIRCA (Musliner et al., 1993), LPSAT (Wolfman & Weld,
1999), Zeno (Penberthy & Weld, 1994) and HAO* (Benazera, Brafman, Meuleau, Mausam,
& Hansen, 2005) to mention only a few such systems in the planning literature. For the most
part, these systems are plan-generation systems using representation languages that support
the restricted modelling of continuous change and metric time. By contrast, pddl+ proposes an unrestricted representation language, and its semantics, without describing specific
search algorithms for the construction of plans. Finding efficient algorithms for reasoning
with pddl+ domains is a separate topic, in which there has already been progress (Shin
& Davis, 2005; Dierks, 2005; McDermott, 2004) as mentioned above. Of course, the demands of practical planning restrict how ambitious one can be in using pddl+ to model
real planning applications, but this is not a reason to impose artificial restrictions on the
expressiveness of the language.
The key objective of HSTS (Heuristic Scheduling Testbed System) (Muscettola, 1993) is
to maintain as much flexibility as possible in the development of a plan, so that the plan can
be robustly executed in the face of unexpected events in the environment. HSTS embodies
the close integration of planning and scheduling, enabling the representation of complex
resource-intensive planning problems as dynamic constraint satisfaction problems (DCSP).
In DDL, the Domain Description Language of HSTS, A distinction is made between domain
attributes (ie: the components of the domain that can exhibit behaviours) and their states,
with the activity of each attribute represented on a separate time line. No distinction is
made between states and actions: actions are added to the time lines by inserting tokens
representing predicates holding over flexible intervals of time. Each token is associated with
a set of compatibilities which explain how they are constrained with respect to activities
on the same and other time lines. Compatibilities express relations very similar to the
TIMELOGIC constructs of Allen and Koomen (Allen & Koomen, 1983). Choices in the
development of the plan are explored through a heuristic search and inconsistencies in the
DCSPs representing the corresponding partial plans result in pruning.
Events of the kind provided by pddl+ are expressed as disjuncts in the compatibility
constraints associated with the actions that produce them. For example, the action of
opening a water source to fill a tank would be expressed as a token constrained to meet
either the event of flooding or an interval in which the water source is closed. Then, whether
281

Fox & Long

the tank floods or not depends on how long the interval of filling lasts, and the flooding
event can be avoided by expressing the constraint that it end before the water level exceeds
the tank capacity. The process by which the water level increases while the tank is filling
is expressed using sequence compatibilities, which allow variables to take arbitrarily many
contiguous values from a sequence during an interval. The actual water level at the end
of the interval can be identified, using linear programming techniques, as one of the values
in this sequence. The notion of procedural reasoning (Frank, Jönsson, & Morris, 2000) has
been introduced into the framework to support efficient reasoning with rates of change on
continuous variables and their interactions within a plan.
HSTS therefore supports the representation of the interaction between actions, processes
and events and their exploitation in the development of flexible plan/schedules. In this
respect, DDL is somewhat more expressive than pddl+ because of the flexibility in its
temporal database. Allowing intervals to last any amount of time between a specified lower
and upper bound introduces bounded temporal flexibility into the reasoning framework.
IxTeT (Laborie & Ghallab, 1995) is a partial order causal link planner that uses a
task representation very similar to that of discrete durative actions in pddl2.1. A key
difference is that pddl2.1 discrete durative actions are restricted to the representation of
step function change at the start and end points of the interval, whilst IxTeT tasks can have
effects at any specified point during the interval. This allows piecewise continuous change
to be represented. Continuous change cannot be modelled (except by means of very small
intervals in the piecewise representation of the appropriate function). Furthermore, the
durative actions of IxTeT are not of fixed duration — they can endure any amount of time
within a specified interval. Thus, IxTeT also models bounded temporal flexibility and is able
to construct flexible plans. IxTeT continues the traditions of POCL planning (McAllester
& Rosenblitt, 1991; Penberthy & Weld, 1992) in which a plan is built up as a partially
ordered graph of activities with complex flexible temporal constraints. A Simple Temporal
Network (Dechter, Meiri, & Pearl, 1991) is used to determine the consistency of a given
temporal constraint set. STNs, also used in HSTS, are a powerful technique for temporal
reasoning but are restricted to reasoning about discrete time points.
There is an important difference between modelling the continuous process of change
and computing the values of continuous-valued variables during planning. In some systems,
continuous change is explicitly modelled, so that trajectories can be constructed for the
variables throughout the timeline of a plan. In other systems, the continuous processes that
determine the behaviour of metric variables are implicit, but the values of these variables
are available, through some computation, at certain times along their trajectories.
Zeno (Penberthy & Weld, 1994) uses an explicit representation of processes as differential
equations and solves them to determine whether the temporal and metric constraints of
a problem are met in any partial plan (and to identify the values of continuous-valued
resources when action preconditions require them). LPSAT (Wolfman & Weld, 1999) also
uses an explicit model of the processes that govern continuous change (although its use of
linear constraint solving limits it to linear processes). A different approach, in which the
processes are not explicitly modelled, can be seen in HAO* (Benazera et al., 2005). Here,
although continuous-valued resources are modelled, the way in which they change over time
is not. The different possible metric outcomes of an action are discretised and associated
with probabilities. Plan construction can be seen in terms of policy construction within
282

Modelling Mixed Discrete-Continuous Domains for Planning

a hybrid MDP framework. Time can be managed in the same way as any other metric
resource (a certain action will take t time units with probability 1 − p) so there is no need to
model the passage of time directly. Whilst the time-dependent nature of the metric effects
of actions can be captured in this way, actions cannot interact with the passage of time in
order to exploit or control episodes of continuous change.

9. Conclusions
In this paper we have presented a planning domain description language, pddl+, that
supports the modelling of continuous dynamics. We have provided a formal semantics by
specifying a mapping from pddl+ constructs to those of deterministic hybrid automata.
We have also related fragments of pddl+ to automata with different levels of expressive
power. Our goal has been to develop a pddl extension that properly models the passage of
time, and the continuous change of quantities over time, to support the modelling of mixed
discrete-continuous planning domains.
Our primary goal has been to establish a baseline for mixed discrete-continuous modelling, and to provide a formal semantics for the resulting language. Additionally we wanted
to make a strong connection between planning and automata theory in order to facilitate
the cross-fertilisation of ideas between the planning and real-time systems and modelchecking communities. We have explored the relationship between fragments of pddl+
and automata-theoretic models on the boundary of decidability in order to better understand what is gained or lost in expressive power by the addition or removal of modelling
constructs.
We have not focussed on how to make pddl+ convenient for modelling because modelling convenience is not related to expressive power. We agree that it might be desirable
to build abstract modelling constructs on top of the baseline language in order to enhance
modelling convenience. Nevertheless, so that pddl+ can be used for experimental purposes, in the development of technology for mixed discrete-continuous planning, we have
presented it as a usable language that builds directly upon the current standard modelling
language for temporal domains. We have presented two examples of domain models that
exploit processes, events and durative actions in the succinct representation of continuous
change. Future work will consider what more powerful modelling constructs might be built
to support the convenient modelling of larger scale mixed discrete-continuous domains.

Acknowledgments
We would like to extend special thanks to David Smith for his detailed critical analysis of
earlier drafts of this paper and his many insightful comments and suggestions. We would
also like to thank Subbarao Kambhampati and the anonymous referees for helping us to
organise and clarify the presentation of this work, and Jeremy Frank, Stefan Edelkamp,
Nicola Muscettola, Drew McDermott, Brian Williams and Mark Boddy, all of whom helped
us to refine and sharpen our ideas and formulations.
283

Fox & Long

Appendix A. pddl2.1 Core Definitions
In this appendix we present the definitions from (Fox & Long, 2003) that are most relevant
to this paper, for ease of reference. For detailed discussions of these definitions and their
relationships see the original source.
Core Definition 1 Simple Planning Instance A simple planning instance is defined to
be a pair
I = (Dom, P rob)
where Dom = (F s, Rs, As, arity) is a 4-tuple consisting of (finite sets of ) function symbols,
relation symbols, actions (non-durative), and a function arity mapping all of these symbols
to their respective arities. P rob = (Os, Init, G) is a triple consisting of the objects in the
domain, the initial state specification and the goal state specification.
The primitive numeric expressions of a planning instance, P N Es, are the terms constructed from the function symbols of the domain applied to (an appropriate number of )
objects drawn from Os. The dimension of the planning instance, dim, is the number of
distinct primitive numeric expressions that can be constructed in the instance.
The atoms of the planning instance, Atms, are the (finitely many) expressions formed
by applying the relation symbols in Rs to the objects in Os (respecting arities).
Init consists of two parts: Initlogical is a set of literals formed from the atoms in Atms.
Initnumeric is a set of propositions asserting the initial values of a subset of the primitive
numeric expressions of the domain. These assertions each assign to a single primitive
numeric expression a constant real value. The goal condition is a proposition that can
include both atoms formed from the relation symbols and objects of the planning instance
and numeric propositions between primitive numeric expressions and numbers.
As is a collection of action schemas (non-durative actions) each expressed in the syntax
of pddl. The primitive numeric expression schemas and atom schemas used in these action
schemas are formed from the function symbols and relation symbols (used with appropriate
arities) defined in the domain applied to objects in Os and the schema variables.
Core Definition 2 Logical States and States Given the finite collection of atoms for a
planning instance I, AtmsI , a logical state is a subset of AtmsI . For a planning instance
with dimension dim, a state is a tuple in (R, P(AtmsI ), Rdim
⊥ ) where R⊥ = R ∪ {⊥} and
⊥ denotes the undefined value. The first value is the time of the state, the second is the
logical state and the third value is the vector of the dim values of the dim primitive numeric
expressions in the planning instance.
The initial state for a planning instance is (0, Initlogical , ~x) where ~x is the vector of values
in R⊥ corresponding to the initial assignments given by Initnumeric (treating unspecified
values as ⊥).
Core Definition 3 Assignment Proposition The syntactic form of a numeric effect
consists of an assignment operator (assign, increase, decrease, scale-up or scale-down),
one primitive numeric expression, referred to as the lvalue, and a numeric expression (which
is an arithmetic expression whose terms are numbers and primitive numeric expressions),
referred to as the rvalue.
284

Modelling Mixed Discrete-Continuous Domains for Planning

The assignment proposition corresponding to a numeric effect is formed by replacing
the assignment operator with its equivalent arithmetic operation (that is (increase p q)
becomes (= p (+ p q)) and so on) and then annotating the lvalue with a “prime”.
A numeric effect in which the assignment operator is either increase or decrease is
called an additive assignment effect, one in which the operator is either scale-up or scale-down
is called a scaling assignment effect and all others are called simple assignment effects.
Core Definition 4 Normalisation Let I be a planning instance of dimension dimI and
let
indexI : P N EsI → {1, . . . , dim}
be an (instance-dependent) correspondence between the primitive numeric expressions and
I
integer indices into the elements of a vector of dimI real values, Rdim
.
⊥
The normalised form of a ground proposition, p, in I is defined to be the result of substituting for each primitive numeric expression f in p, the literal XindexI (f ) . The normalised
form of p will be referred to as N (p). Numeric effects are normalised by first converting
them into assignment propositions. Primed primitive numeric expressions are replaced with
~ is used to represent the vector hX1 . . . Xn i.
their corresponding primed literals. X
Core Definition 5 Flattening Actions Given a planning instance, I, containing an action schema A ∈ AsI , the set of action schemas f latten(A), is defined to be the set S,
initially containing A and constructed as follows:
• While S contains an action schema, X, with a conditional effect, (when P Q), create
two new schemas which are copies of X, but without this conditional effect, and conjoin
the condition P to the precondition of one copy and Q to the effects of that copy, and
conjoin (not P) to the precondition of the other copy. Add the modified copies to S.
• While S contains an action schema, X, with a formula containing a quantifier, replace
X with a version in which the quantified formula ( Q ( var1 . . . vark ) P) in X is
replaced with the conjunction (if the quantifier, Q, is forall) or disjunction (if Q is
exists) of the propositions formed by substituting objects in I for each variable in
var1 . . . vark in P in all possible ways.
These steps are repeated until neither step is applicable.
Core Definition 6 Ground Action Given a planning instance, I, containing an action
schema A ∈ AsI , the set of ground actions for A, GAA , is defined to be the set of all the
structures, a, formed by substituting objects for each of the schema variables in each schema,
X, in f latten(A) where the components of a are:
• Name is the name from the action schema, X, together with the values substituted for
the parameters of X in forming a.
• Prea , the precondition of a, is the propositional precondition of a. The set of ground
atoms that appear in Prea is referred to as GPrea .
• Adda , the positive postcondition of a, is the set of ground atoms that are asserted as
positive literals in the effect of a.
285

Fox & Long

• Dela , the negative postcondition of a,is the set of ground atoms that are asserted as
negative literals in the effect of a.
• NPa , the numeric postcondition of a, is the set of all assignment propositions corresponding to the numeric effects of a.
The following sets of primitive numeric expressions are defined for each ground action,
a ∈ GAA :
• La = {f |f appears as an lvalue in a}
• Ra = {f |f is a PNE in an rvalue in a or appears in P rea }
• L∗a = {f |f appears as an lvalue in an additive assignment effect in a}
Core Definition 7 Valid Ground Action Let a be a ground action. a is valid if no
primitive numeric expression appears as an lvalue in more than one simple assignment
effect, or in more than one different type of assignment effect.
Core Definition 8 Updating Function Let a be a valid ground action. The updating
function for a is the composition of the set of functions:
dim
{NPFp : Rdim
⊥ → R⊥ | p ∈ N P a }

such that NPFp (~x) = ~x0 where for each primitive numeric expression x0i that does not appear
~ 0 := ~x0 , X
~ := ~x] is satisfied.
as an lvalue in N (p), x0i = xi and N (p)[X
~ 0 := ~x0 , X
~ := ~x] should be read as the result of normalising p and
The notation N (p)[X
~ 0 and actual values ~x
then substituting the vector of actual values ~x0 for the parameters X
~
for formal parameters X.
Core Definition 9 Satisfaction of Propositions Given a logical state, s, a ground
propositional formula of pddl2.1, p, defines a predicate on Rdim
⊥ , Num(s, p), as follows:
Num(s, p)(~x)

iff

~ := ~x]
s |= N (p)[X

where s |= q means that q is true under the interpretation in which each atom, a, that is not
a numeric comparison, is assigned true iff a ∈ s, each numeric comparison is interpreted
using standard equality and ordering for reals and logical connectives are given their usual
interpretations. p is satisfied in a state (t, s, ~x) if Num(s, p)(~x).
Comparisons involving ⊥, including direct equality between two ⊥ values are all undefined, so that enclosing propositions are also undefined and not satisfied in any state.
Core Definition 10 Applicability of an Action Let a be a ground action. a is applicable in a state s if the P rea is satisfied in s.
286

Modelling Mixed Discrete-Continuous Domains for Planning

Appendix B. Proof of theorem
Theorem 2 pddl+P T A has indirectly equivalent expressive power to Priced Timed Automata.
We begin by showing that an arbitrary PTA can be expressed as a pddl+P T A domain
without any blow-up in the size of the encoding. We then show that the converse is also
the case.
Given a PTA hL, l0 , E, I, P i we construct a pddl+P T A model as follows. For each edge
e = (li , g, a, r, lj ), where g is the clock constraint, a is the action and r is the subset of clock
variables reset by this edge, we construct the following instantaneous action schema, called
a transition action, which models the instantaneous transition from li into lj .
(:action transitione
:parameters ()
:precondition (and (in li ) g)
:effect (and {∀x ∈ r · (assign (x) 0)}
(increase (c) P(e))
(not (in li ))
(in lj )))

We also construct, for each location li , the following process schema:
(:process process-locationli
:parameters ()
:precondition (in li )
:effect
(and (increase (c) (* #t P(li ))
{∀x ∈ C · (increase (x) (* #t 1))}
))

Finally, we construct an event for each location:
(:event event-locationli
:parameters ()
:precondition (and (in li ) (not (I(li ))))
:effect
(not (in li )))

The initial state specifies that each clock variable and the cost variable starts with value
0. It asserts (in l0 ). The special error state is the empty state (if an event is triggered then
it will remove the current location proposition, leaving it impossible to progress).
Note that this domain is valid pddl+P T A . No two actions can be applied in parallel
because the system can only ever satisfy one condition of the form (in li ).
For this construction to correctly capture the PTA, it remains to be shown that any PTA
trajectory corresponds to a pddl+P T A plan for this domain and that any plan corresponds
to a trajectory.
Consider a (valid) PTA trajectory,
δ

δ

1
2
(l0 , u0 ) →
(l1 , u1 ) →
. . . (ln , un )

287

Fox & Long

where each ui is a clock valuation and each δi is a transition which may either be an edge or a
positive time delay. In the case of time delay transitions, the location remains the same but
the clock valuation is updated by the delay duration, while for edge transitions the location
is updated but the clock valuation remains the same. This trajectory is mapped to a plan
by creating an action instance for each edge transition (using the corresponding action in
the pddl+P T A description). The action is set to be applied at the time corresponding to
the sum of the time delay transitions that precede the transition in the trajectory. This is
a valid plan since the processes are active at exactly the same times in the pddl model as
the corresponding time transitions in the trajectory.
Similarly, given any valid pddl plan for the above domain, the plan defines a trajectory
by mapping the instantaneous actions into their corresponding edge transitions and the
gaps between them into time delay transitions. The trajectory will be a valid one because
of the construction of the actions and process models. Note that no valid plan can trigger
an event, since this will leave the system in a state that cannot be progressed, preventing
satisfaction of any goal. This means that invariant conditions for each location are always
maintained.
We now consider the opposite direction of the proof: pddl+P T A planning instances
yield state space and transition models that can be expressed as PTAs with a constant
factor transformation. We observe that the constraints on the pddl+P T A language ensures
that clock variables and a cost variable are distinguished and behave exactly as required for
a PTA. In the ground state space, all states correspond directly to locations of a PTA and
the legal actions that transition between states correspond to the edges of the corresponding
PTA. With the exception of event transitions leading into an exceptional error state, each
edge in a pddl+P T A transition system will, by the constraints defining the language, always
correspond to an instantaneous action and this action determines the clock constraints and
reset effects of the corresponding PTA edge. Exactly one process is active in each state, and
causes the variables to behave as clocks (except for the cost variable). The events govern
the invariants for the states, causing any violation of an invariant condition to trigger a
transition into an error state.
The correspondence between trajectories for the PTA defined in this way and plans
for the pddl+P T A transition system is immediate, subject to the same observations made
above.


Appendix C. The Planetary Lander Domain in pddl+
The following domain encoding shows the pddl+ model for the Planetary Lander problem
discussed in Section 2.2. According to implementation details of systems designed to handle
pddl+ models, it might be necessary to introduce further events to manage the transition
between charging and discharging, because imprecision of the system in measuring the
values of the demand and supply might lead to difficulties at the boundary where supply
and demand are equal.
(define (domain power)
(:requirements :typing :durative-actions :fluents :time
288

Modelling Mixed Discrete-Continuous Domains for Planning

:negative-preconditions :timed-initial-literals)
(:types equipment)
(:constants unit - equipment)
(:predicates (day) (commsOpen) (readyForObs1) (readyForObs2)
(gotObs1) (gotObs2)
(available ?e - equipment))
(:functions (demand) (supply) (soc) (charge-rate) (daytime)
(heater-rate) (dusk) (dawn)
(fullTime) (partTime1) (partTime2)
(obs1Time) (obs2Time) (obs1-rate) (obs2-rate)
(A-rate) (B-rate) (C-rate) (D-rate) (safeLevel)
(solar-const))

(:process charging
:parameters ()
:precondition (and (< (demand) (supply)) (day))
:effect (and (increase (soc) (* #t (* (* (- (supply) (demand))
(charge-rate))
(- 100 (soc)))
)))
)
(:process discharging
:parameters ()
:precondition (> (demand) (supply))
:effect (decrease soc (* #t (- (demand) (supply))))
)
(:process generating
:parameters ()
:precondition (day)
:effect (and (increase (supply)
(* #t (* (* (solar-const) (daytime))
(+ (* (daytime)
(- (* 4 (daytime)) 90)) 450))))
(increase (daytime) (* #t 1)))
)
(:process night-operations
:parameters ()
:precondition (not (day))
:effect (and (increase (daytime) (* #t 1))
(decrease (soc) (* #t (heater-rate))))
)
(:event nightfall
:parameters ()
:precondition (and (day) (>= (daytime) (dusk)))
:effect (and (assign (daytime) (- (dawn)))
(not (day)))
289

Fox & Long

)
(:event daybreak
:parameters ()
:precondition (and (not (day)) (>= (daytime) 0))
:effect (day)
)
(:durative-action fullPrepare
:parameters ()
:duration (= ?duration (fullTime))
:condition (and (at start (available unit))
(over all (> (soc) (safelevel))))
:effect (and (at start (not (available unit)))
(at start (increase (demand) (A-rate)))
(at end (available unit))
(at end (decrease (demand) (A-rate)))
(at end (readyForObs1))
(at end (readyForObs2)))
)
(:durative-action prepareObs1
:parameters ()
:duration (= ?duration (partTime1))
:condition (and (at start (available unit))
(over all (> (soc) (safelevel))))
:effect (and (at start (not (available unit)))
(at start (increase (demand) (B-rate)))
(at end (available unit))
(at end (decrease (demand) (B-rate)))
(at end (readyForObs1)))
)
(:durative-action prepareObs2
:parameters ()
:duration (= ?duration (partTime2))
:condition (and (at start (available unit))
(over all (> (soc) (safelevel))))
:effect (and (at start (not (available unit)))
(at start (increase (demand) (C-rate)))
(at end (available unit))
(at end (decrease (demand) (C-rate)))
(at end (readyForObs2)))
)
(:durative-action observe1
:parameters ()
:duration (= ?duration (obs1Time))
:condition (and (at start (available unit))
(at start (readyForObs1))
(over all (> (soc) (safelevel)))
290

Modelling Mixed Discrete-Continuous Domains for Planning

(over all (not (commsOpen))))
:effect (and (at start (not (available unit)))
(at start (increase (demand) (obs1-rate)))
(at end (available unit))
(at end (decrease (demand) (obs1-rate)))
(at end (not (readyForObs1)))
(at end (gotObs1)))
)
(:durative-action observe2
:parameters ()
:duration (= ?duration (obs2Time))
:condition (and (at start (available unit))
(at start (readyForObs2))
(over all (> (soc) (safelevel)))
(over all (not (commsOpen))))
:effect (and (at start (not (available unit)))
(at start (increase (demand) (obs2-rate)))
(at end (available unit))
(at end (decrease (demand) (obs2-rate)))
(at end (not (readyForObs2)))
(at end (gotObs2)))
)
)

Appendix D. Durative Actions in pddl+
The syntax for durative actions can be seen as a purely syntactic convenience. In order
to support this view, durative actions must be mapped directly into an equivalent startprocess-stop representation using the basic syntax of pddl+. The reason for performing
the mapping is in order to give durative actions a semantics in terms of the underlying
structures of pddl+, which are themselves given meaning in terms of hybrid automata as
discussed in Section 6. The mapping is as follows.
Consider the following generic structure for a pddl2.1 durative action, excluding the use
of duration inequalities and continuous effects. Conditional effects are ignored, since they
can be handled by flattening the actions as described in Core Definition 5. The complication
of conditional effects that combine initial and final conditions is discussed in (Fox & Long,
2003) and does not affect the basic principles we demonstrate in the translation we give
here. Similarly, for convenience, we do not consider duration constraints referring to the end
state — extension of the treatment below to manage these constraints is straightforward.
(:durative-action name
:parameters (~
p)
:duration (= ?duration Dur[~
p])
:condition (and (at start P reS ) (at end P reE ) (over all Inv))
:effect (and (at start P ostS )(at end P ostE [?duration]))
)
We now construct the following structures in pddl+:
291

Fox & Long

(:action name-start
:parameters (~
p)
:precondition (and P reS (not (name clock started p~)))
:effect (and P ostS
(name clock started p~)
(assign (name clock p~) 0)
(assign (name duration p~) Dur[~
p])
(increase (clock count) 1))
(:process name-process
:parameters (~
p)
:precondition (name clock started p~)
:effect (increase (name clock p~) (* #t 1))
(:event name-failure
:parameters (~
p)
:precondition (and (name clock started p~)
(not (= (name clock p~) (name duration p~)))
(not Inv))
:effect (assign (name clock p~) (+ (name duration p~) 1)))
(:action name-end
:parameters (~
p)
:precondition (and P reE (name clock started p~)
(= (name clock p~) (name duration p~)))
:effect (and P ostE [(name duration p~)]
(not (name clock started p~))
(decrease (clock count) 1)))
To complete the transformation, the initial state has the (= (clock count) 0) added
to it and the goal has the condition (= (clock count) 0) added to it.
Note that the clock is uniquely determined by the name and arguments of the durative
action, so the clock is not shared between different durative actions. Also note that there is
only one action that can start each such clock and only one that can terminate it. If a plan
makes use of the durative action then in the transformed domain the durative action can be
simulated by the actions that start and stop the clock. In order for these actions to execute
successfully the conditions must be identical to those stipulated in the durative action and
their effects are equivalent to the original durative action. Furthermore, the end action can
only be executed once the clock has reached the correct duration value (which is recorded
at the start). The clock is driven by a process that is active only once the start action has
been executed and only until the end action is executed. An event is used to monitor the
invariant conditions. If ever the invariant becomes false while the clock is running then
the event sets the clock to be after the duration of the durative action. This makes it
impossible to complete the action and terminate the clock, so that no valid execution trace
can be constructed to achieve the goal in which the event is triggered. Every time a clock
292

Modelling Mixed Discrete-Continuous Domains for Planning

starts a count is incremented and it is decremented when a clock stops. The count must be
zero at the end, meaning that every clock must have stopped and therefore every durative
action must have been ended before the plan is complete.
The duration of the action is managed by using a metric fluent to store the duration at
the outset in order to use the value at the conclusion of the action. If the value is a fixed
constant then this can be simplified by replacing the metric fluent with the appropriate
constant.
Packaging actions, events and processes into a durative action abstracts the details of
how the interval ends — whether by an action or by an event. For example, if a durative
action is used to represent the activity of filling a bath, its end point will represent the
action of turning off the taps, whilst in a durative action representing a ball being dropped,
the end of the action is the event of the ball landing. However, there is no syntactic
difference between the structures encoding these examples that allows a distinction to be
drawn between an end point that is an action and one that is an event. The mapping
of durative actions into actions, processes and events requires an arbitrary decision to be
made about how to handle the end points of durative actions. While we have chosen to use
actions, an alternative formulation is possible using events.
The mapping converts every durative action into the family of pddl+ constructs shown
above — two actions to start and end the interval, a process to execute during the interval,and a monitoring event for the invariant. Because an action is always used to end the
interval a planner must always choose to apply it in order to obtain a goal state. In the final
plan the terminating action will appear, chosen by the planner, even though its application
is in fact forced, and its point of application must be consistent with the duration constraint
of the original durative action. Thus, although the planner is free to choose whether and
when to apply the action, in order to construct a valid plan it is forced to apply the action at
a point that exactly meets the appropriate temporal constraints. Of course, the end points
of the simulated durative actions can be trivially post-processed to make a plan containing
durative actions instead of to their underlying components.

References
Allen, J., & Koomen, J. A. (1983). Planning in a Temporal World Model. In Proceedings
of the Eigth International Joint Conference on Artificial Intelligence, pp. 741–747.
Alur, R., & Dill, D. L. (1994). A Theory of Timed Automata. Theoretical Computer Science,
126, 183–235.
Androulakis, I. P. (2001). MINLP: Branch and Bound Methods. In Floudas, C. A., &
Pardalos, P. M. (Eds.), Encyclopaedia of Optimisation, Vol. 3, pp. 325–331. Kluwer
Academic.
Aylett, R., Soutter, J., Petley, G., Chung, P., & Edwards, D. (2001). Planning plant operating procedures for a chemical plant. Engineering Applications of Artificial Intelligence,
14(3).
Bacchus, F. (2003). The power of modeling—a response to PDDL2.1. Journal of AI Research, 20, 125–132.
293

Fox & Long

Beasley, J., Krishnamoorthy, M., Sharaiha, Y., & Abramson, D. (2000). Scheduling Aircraft
Landings: The Static Case. Transportation Science, 34 (2), 180–197.
Behrmann, G., Larsen, K., & Rasmussen, J. (2005). Optimal Scheduling Using Priced
Timed Automata. SIGMETRICS Perform. Eval. Rev., 32 (4), 34–40.
Benazera, E., Brafman, R., Meuleau, N., Mausam, & Hansen, E. A. (2005). An AO*
Algorithm for Planning with Continuous Resources. In Workshop on Planning under
Uncertainty for Autonomous Systems, associated with the International Conference
on AI Planning and Scheduling (ICAPS).
Benders, J. F. (1962). Partitioning Procedures for Solving Mixed-Variables Programming
Problems. Numerische Mathematik, 4, 238–252.
Blake, O., Bridges, J., Chester, E., Clemmet, J., Hall, S., Hannington, M., Hurst, S., Johnson, G., Lewis, S., Malin, M., Morison, I., Northey, D., Pullan, D., Rennie, G., Richter,
L., Rothery, D., Shaughnessy, B., Sims, M., Smith, A., Townend, M., & Waugh, L.
(2004). Beagle2 Mars: Mission Report. Lander Operations Control Centre, National
Space Centre, University of Leicester.
Boddy, M. (2003). Imperfect match: PDDL2. and real applications. Journal of AI Research,
20, 133–137.
Boddy, M., & Johnson, D. (2002). A new method for the solution of large systems of continuous constraints. In Proceedings of 1st International Workshop on Global Constrained
Optimization and Constraint Satisfaction (COCOS-02).
Boddy, M., & Johnson, D. (2004). Integrated planning and scheduling for petroleum refinery operations. In Proceedings of ICAPS Workshop on Integrating Planning into
Scheduling (WIPIS).
de Kleer, J., & Brown, J. S. (1984). A Qualitative Physics based on Confluences. Artificial
Intelligence, 24, 7–83.
Dechter, R., Meiri, I., & Pearl, J. (1991). Temporal constraint networks. Artificial Intelligence, 49.
Dierks, H. (2005). Finding Optimal Plans for Domains with Restricted Continuous Effects
with UPPAAL-cora. In First Workshop on Validation and Verification for Planning,
ICAPS-05.
Edelkamp, S. (2003). Promela Planning. In Proceedings of 10th International SPIN Workshop on Model Checking of Software, pp. 197–212.
Forbus, K. (1984). Qualitative Process Theory. Artificial Intelligence, 24, 85–168.
Fox, M., Howey, R., & Long, D. (2005). Exploration of the Robustness of Plans. In Workshop
on Verification and Validation of Model-based Planning and Scheduling Systems, associated with the International Conference on AI Planning and Scheduling (ICAPS)).
Fox, M., Howey, R., & Long, D. (2006). Exploration of the Robustness of Plans. In Proceedings of The 21st National Conference on Artificial Intelligence (AAAI-06).
Fox, M., & Long, D. (2003). PDDL2.1: An Extension to PDDL for Expressing Temporal
Planning Domains. Journal of AI Research, 20, 61–124.
294

Modelling Mixed Discrete-Continuous Domains for Planning

Fox, M., & Long, D. (2004). An Investigation into the Expressive Power of PDDL2.1. In
Proceedings of the Sixteenth European Conference on Artificial Intelligence.
Frank, J., & Jónsson, A. (2003). Constraint-based Attribute and Interval Planning. Journal
of Constraints, 8 (Special Issue on Constraints and Planning)(4), 339–364.
Frank, J., Jönsson, A., & Morris, P. (2000). On Reformulating Planning as Dynamic Constraint Satisfaction (Extended Abstract). In Symposium on Abstraction, Reformulation and Approximation (SARA).
Grossmann, I. E. (2002). Review of Nonlinear, Mixed-Integer and Disjunctive Programming
Techniques. Optimization and Engineering, 3, 227–252.
Gupta, V., Henziner, T., & Jagadeesan, R. (1997). Robust Timed Automata. In HART’97:
Hybrid and Real-time Systems, LNCS 1201, pp. 331–345. Springer-Verlag.
Haroud, D., & Faltings, B. (1994). Global Consistency for Continuous Constraints. In
Principles and Practice of Constraint Programming, pp. 40–50.
Helmert, M. (2002). Decidability and undecidability results for planning with numerical
state variables. In Proceedings of sixth conference on AI Planning Systems (AIPS).
Henzinger, T. (1996). The Theory of Hybrid Automata. In Proceedings of the 11th Annual Symposium on ogic in Computer Science. Invited tutorial., pp. 278–292. IEEE
Computer Society Press.
Henzinger, T., Ho, P.-H., & Wong-Toi, H. (1995). A user guide to HYTECH. In E.
Brinksma, W.R. Cleaveland, K.G. Larsen, T. Margaria, and B. Steffen, editors, Tool
and Algorithms for the Construction and Analysis of Systems: (TACAS 95), volume
1019 of Lecture Notes in Computer Science, pp. 41–71.
Henzinger, T., & Raskin, J.-F. (2000). Robust Undecidability of Timed and Hybrid Systems.
In Proceedings of the 3rd International Workshop on Hybrid Systems: Computation
and Control. LNCS 1790., pp. 145–159. Springer-Verlag.
Henzinger, T. A., Kopke, P. W., Puri, A., & Varaiya, P. (1998). What’s Decidable about
Hybrid Automata?. Journal of Computer and System Sciences, 57, 94–124.
Herrmann, C. S., & Thielscher, M. (1996). Reasoning about continuous processes. In
Clancey, B., & Weld, D. (Eds.), Proceedings of the Thirteenth National Conference on
Artificial Intelligence (AAAI), pp. 639–644, Portland, OR. MIT Press.
Hoffmann, J., & Edelkamp, S. (2005). The Classical Part of IPC-4: An Overview. Journal
of AI Research, To appear.
Hofmann, A., & Williams, B. (2006). Robust execution of temporally flexible plans for
bipedal walking devices. In Proceedings of 16th International Conference on Automated Planning and Scheduling (ICAPS), pp. 386–389.
Howey, R., Long, D., & Fox, M. (2004). Val: Automatic plan validation, continuous effects
and mixed initiative planning using pddl. In Proceedings of 16th IEEE International
Conference on Tools with Artificial Intelligence.
Jónsson, A., & Frank, J. (2000). A Framework for Dynamic Constraint Reasoning using
Procedural Constraints. In Proceedings of 14th European Conference on AI, pp. 93–97.
295

Fox & Long

Jónsson, A., Morris, P., Muscettola, N., Rajan, K., & Smith, B. (2000). Planning in Interplanetary Space: Theory and Practice. In Proceedings of 5th International Conference
on AI Planning Systems, pp. 177–186.
Keller, R. (1976). Formal Verification of Parallel Programs. Communications of the ACM,
19 (7), 371–384.
Kuipers, B. (1984). Commonsense Reasoning about Causality: Deriving Behaviour from
Structure. Artificial Intelligence, 24, 169–203.
Laborie, P., & Ghallab, M. (1995). Planning with sharable resource constraints. In Proc.
of 14th International Joint Conference on AI. Morgan Kaufmann.
Lamba, N., Dietz, M., Johnson, D., & Boddy, M. (2003). A method for global optimization of
large systems of quadratic constraints. In Proceedings of 2nd International Workshop
on Global Constrained Optimization and Constraint Satisfaction (COCOS-03).
Larsen, K. G., Behrmann, G., Brinksma, E., Fehnker, A., Hune, T., Pettersson, P., &
Romijn, J. (2001). The use of optimistic and pessimistic resource profiles to inform
search in an activity based planner. In Proceedings of 13th Conference on Computer
Aided Verification (CAV-01)). Springer Verlag, Lecture Notes in Computer Science
2102.
Léauté, T., & Williams, B. (2005). Coordinating Agile Systems through the Model-based
Execution of Temporal Plans. In Proceedings of 20th National Conference on AI
(AAAI), pp. 114–120.
McAllester, D., & Rosenblitt, D. (1991). Systematic Nonlinear Planning. In Proceedings
of the Ninth National Conference on Artificial Intelligence (AAAI-91), Vol. 2, pp.
634–639, Anaheim, California, USA. AAAI Press/MIT Press.
McDermott, D. (2003a). PDDL2.1 – The Art of the Possible? Commentary on Fox and
Long. Journal of AI Research, 20, 145–148.
McDermott, D. (2003b). Reasoning about autonomous processes in an estimated-regression
planner. In Proceedings of the International Conference on Automated Planning and
Scheduling (ICAPS’03).
McDermott, D. (2005). Reasoning about Autonomous Processes in an Estimated Regression
Planner. In Proceedings of 13th International Conference on Automated Planning and
Scheduling (ICAPS), pp. 143–152. AAAI-Press.
McDermott, D., & the AIPS’98 Planning Competition Committee (1998). PDDL–the planning domain definition language. Tech. rep., Available at: www.cs.yale.edu/homes/dvm.
McDermott, D. (2004). The Opt and Optop API. Tech. rep., Yale University.
Muscettola, N. (1993). HSTS: Integrating Planning and Scheduling. In Zweben, M., & Fox,
M. (Eds.), Intelligent Scheduling, pp. 169–212. Morgan Kaufmann, San Mateo, CA.
Musliner, D. J., Durfee, E. H., & Shin, K. G. (1993). CIRCA: A Cooperative Intelligent
Real-time Control Archtecture. IEEE Transactions on Systems, Man and Cybernetics,
23 (6), 1561–1574.
296

Modelling Mixed Discrete-Continuous Domains for Planning

Penberthy, J., & Weld, D. (1992). UCPOP: a sound, complete, partial-order planner for
ADL. In Proc. Int. Conf. On Principles of Knowledge Representation and Reasoning,
pp. 103–114, Los Altos, CA. Kaufmann.
Penberthy, S., & Weld, D. (1994). Temporal Planning with Continuous Change. In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI), pp.
1010–1015. AAAI/MIT Press.
Rasmussen, J. I., Larsen, K. G., & Subramani, K. (2004). Resource Optimal Scheduling
using Priced Timed Automata. In Proceedings of 10th International Conference on
Tools and Algorithms for the Construction and Analysis of Systems (TACAS), pp.
220–235. Springer-Verlag, Lecture Notes in Computer Science, 2988.
Reiter, R. (1996). Natural Actions, Concurrency and Continuous Time in the Situation
Calculus. In Aiello, L., Doyle, J., & Shapiro, S. (Eds.), KR-96: Principles of Knowledge
Representation and Reasoning, pp. 2–13. Morgan Kaufmann.
Reiter, R. (2001). Knowledge in Action: Logical Foundations for Secifying and Implementing
Dynamical Systems. MIT Press.
Sandewall, E. (1989). Combining Logic and Differential Equations for describing Real World
Systems. In Proceedings of Knowledge Representation (KR), pp. 412–420.
Shanahan, M. (1990). Representing Continuous Change in the Event Calculus. In Proceedings of 9th European Conference on AI, pp. 598–603.
Shin, J.-A., & Davis, E. (2005). Processes and Continuous Change in a SAT-based Planner.
Artificial Intelligence, 166, 194–253.
Smith, D. (2003). The case for durative actions: A commentary on PDDL2.1. Journal of
AI Research, 20, 149–154.
Wolfman, S., & Weld, D. (1999). The LPSAT System and its Application to Resource
Planning. In Proceedings of the Sixteenth International Joint Conference on Artificial
Intelligence.
Wu, S. J., & Chow, P. T. (1995). Genetic Algorithms for Nonlinear Mixed Discrete-Integer
Optimization Problems via Meta-Genetic Parameter Optimization. Engineering Optimization, 24 (2), 137–159.
Yi, W., Larsen, K., & Pettersson, P. (1997). UPPAAL in a Nutshell. International Journal
of Software Tools for Technology Transfer, 1 (1).

297

Journal of Artificial Intelligence Research 27 (2006) 85–117

Submitted 01/06; published 09/06

Learning Sentence-internal Temporal Relations
MLAP @ INF. ED . AC . UK

Mirella Lapata
Alex Lascarides

ALEX @ INF. ED . AC . UK

School of Informatics,
University of Edinburgh,
2 Buccleuch Place,
Edinburgh, EH8 9LW,
Scotland, UK

Abstract
In this paper we propose a data intensive approach for inferring sentence-internal temporal
relations. Temporal inference is relevant for practical NLP applications which either extract or synthesize temporal information (e.g., summarisation, question answering). Our method bypasses the
need for manual coding by exploiting the presence of markers like after, which overtly signal a
temporal relation. We first show that models trained on main and subordinate clauses connected
with a temporal marker achieve good performance on a pseudo-disambiguation task simulating
temporal inference (during testing the temporal marker is treated as unseen and the models must
select the right marker from a set of possible candidates). Secondly, we assess whether the proposed
approach holds promise for the semi-automatic creation of temporal annotations. Specifically, we
use a model trained on noisy and approximate data (i.e., main and subordinate clauses) to predict
intra-sentential relations present in TimeBank, a corpus annotated rich temporal information. Our
experiments compare and contrast several probabilistic models differing in their feature space, linguistic assumptions and data requirements. We evaluate performance against gold standard corpora
and also against human subjects.

1. Introduction
The computational treatment of temporal information has recently attracted much attention, in part
because of its increasing importance for potential applications. In multidocument summarization,
for example, information that is to be included in the summary must be extracted from various
documents and synthesized into a meaningful text. Knowledge about the temporal order of events
is important for determining what content should be communicated and for correctly merging and
presenting information in the summary. Indeed, ignoring temporal relations in either the information
extraction or the summary generation phase may result in a summary which is misleading with
respect to the temporal information in the original documents. In question answering, one often
seeks information about the temporal properties of events (e.g., When did X resign? ) or how events
relate to each other (e.g., Did X resign before Y? ).
An important first step towards the automatic handling of temporal phenomena is the analysis
and identification of time expressions. Such expressions include absolute date or time specifications (e.g., October 19th, 2000 ), descriptions of intervals (e.g., thirty years ), indexical expressions
(e.g., last week ), etc. It is therefore not surprising that much previous work has focused on the recog-

c 2006 AI Access Foundation. All rights reserved.

L APATA & L ASCARIDES

nition, interpretation, and normalization of time expressions 1 (Wilson, Mani, Sundheim, & Ferro,
2001; Schilder & Habel, 2001; Wiebe, O’Hara, Öhrström Sandgren, & McKeever, 1998). Reasoning
with time, however, goes beyond temporal expressions; it also involves drawing inferences about the
temporal relations among events and other temporal elements in discourse. An additional challenge
to this task stems from the nature of temporal information itself, which is often implicit (i.e., not
overtly verbalized) and must be inferred using both linguistic and non-linguistic knowledge.
Consider the examples in (1) taken from Katz and Arosio (2001). Native speakers can infer that
John first met and then kissed the girl; that he left the party after kissing the girl and then walked
home; and that the events of talking to her and asking her for her name temporally overlap (and
occurred before he left the party).
(1)

a.

John kissed the girl he met at a party.

b.

Leaving the party, John walked home.

c.

He remembered talking to her and asking her for her name.

The temporal relations just described are part of the interpretation of this text, even though
there are no overt markers, such as after or while, signaling them. They are inferable from a variety
of cues, including the order of the clauses, their compositional semantics (e.g., information about
tense and aspect), lexical semantics and world knowledge. In this paper we describe a data intensive
approach that automatically captures information pertaining to the temporal relations among events
like the ones illustrated in (1).
A standard approach to this task would be to acquire a model of temporal relations from a
corpus annotated with temporal information. Although efforts are underway to develop treebanks
marked with temporal relations (Katz & Arosio, 2001) and devise annotation schemes that are suitable for coding temporal relations (Saurı́, Littman, Gaizauskas, Setzer, & Pustejovsky, 2004; Ferro,
Mani, Sundheim, & Wilson, 2000; Setzer & Gaizauskas, 2001), the existing corpora are too small
in size to be amenable to supervised machine learning techniques which normally require thousands of training examples. The TimeBank 2 corpus, for example, contains a set of 186 news report
documents annotated with the TimeML mark-up language for temporal events and expressions (for
details, see Sections 2 and 7). The corpus consists of 68.5K words in total. Contrast this with the
Penn Treebank, a corpus which is often used in many NLP tasks and contains approximately 1M
words (i.e., it is 16 times larger than TimeBank). The annotation of temporal information is not only
time-consuming but also error prone. In particular, if there are n kinds of temporal relations, then
the number of possible relations to annotate is a polynomial of factor n on the number of events in
the text. Pustejovsky, Mani, Belanger, Boguraev, Knippen, Litman, Rumshisky, See, Symonen, van
Guilder, van Guilder, and Verhagen (2003) found evidence that this annotation task is sufficiently
complex that human annotators can realistically identify only a small number of the temporal relations in text, thus compromising recall.
In default of large volumes of data labeled with temporal information, we turn to unannotated
texts which nevertheless contain expressions that overtly convey the information we want our models to learn. Although temporal relations are often underspecified, sometimes there are temporal
markers, such as before, after, and while, which make relations among events explicit:
1. See also the Time Expression Recognition and Normalisation (TERN) evaluation exercise (http://timex2.mitre.
org/tern.html).
2. Available from http://www.cs.brandeis.edu/˜jamesp/arda/time/timebank.html

86

L EARNING S ENTENCE - INTERNAL T EMPORAL R ELATIONS

(2)

a.

Leonard Shane, 65 years old, held the post of president before William Shane, 37, was
elected to it last year.

b.

The results were announced after the market closed.

c.

Investors in most markets sat out while awaiting the U.S. trade figures.

It is precisely this type of data that we will exploit for making predictions about the temporal
relationships among events in text. We will assess the feasibility of such an approach by initially
focusing on sentence-internal temporal relations. From a large corpus, we will obtain sentences like
the ones shown in (2), where a main clause is connected to a subordinate clause with a temporal
marker, and we will develop a probabilistic framework where the temporal relations will be inferred
by gathering informative features from the two clauses. Our models will view the marker from each
sentence in the training corpus as the label to be learned. In the test corpus the marker will be
removed and the models’ task will be to pick the most likely label—or equivalently marker.
We will also examine whether models trained on data containing main and subordinate clauses
together with their temporal connectives can be used to infer relations among events when temporal
information is underspecified and overt temporal markers are absent (as in each of the three sentences in (1)). For this purpose, we will resort to the TimeBank corpus. The latter contains detailed
annotations of events and their temporal relations irrespectively of whether connectives are present
or not. Using the TimeBank annotations solely as test data, we will assess whether the approach
put forward generalizes to different structures and corpora. Our evaluation study will also highlight
whether a model learned from unannotated examples could alleviate the data acquisition bottleneck
involved in the creation of temporal annotations. For example, by automatically creating a high
volume of annotations which could be subsequently corrected manually.
In attempting to infer temporal relations probabilistically, we consider several classes of models with varying degrees of faithfulness to linguistic theory. Our models differ along two dimensions:
the employed feature space and the underlying independence assumptions. We compare and contrast models which utilize word-co-occurrences with models which exploit linguistically motivated
features (such as verb classes, argument relations, and so on). Linguistic features typically allow
our models to form generalizations over classes of words, thereby requiring less training data than
word co-occurrence models. We also compare and contrast two kinds of models: one assumes that
the properties of the two clauses are mutually independent; the other makes slightly more realistic assumptions about dependence. (Details of the models and features used are given in Sections 3
and 4). We furthermore explore the benefits of ensemble learning methods for the temporal interpretation task and show that improved performance can be achieved when different learners (modeling
sufficiently distinct knowledge sources) are combined. Our machine learning experiments are complemented by a study in which we investigate human performance on the interpretation task thereby
assessing its feasibility and providing a ceiling on model performance.
The next section gives an overview of previous work in the area of computing temporal information and discusses related work which utilizes overt markers as a means for avoiding manual
labeling of training data. Section 3 describes our probabilistic models and Section 4 discusses our
features and the motivation behind their selection. Our experiments are presented in Sections 5–7.
Section 8 offers some discussion and concluding remarks.

87

L APATA & L ASCARIDES

2. Related Work
Traditionally, methods for inferring temporal relations among events in discourse have utilized a
semantics and inference-based approach. This involves complex reasoning over a variety of rich information sources, including elaborate domain knowledge and detailed logical form representations
(e.g., Dowty, 1986; Hwang & Schubert, 1992; Hobbs et al., 1993; Lascarides & Asher, 1993; Kamp
& Reyle, 1993; Kehler, 2002). This approach, while theoretically elegant, is impractical except for
applications in very narrow domains. This is for (at least) two reasons. First, grammars that produce
detailed semantic representations inevitably lack linguistic coverage and are brittle in the face of
natural data; similarly, the representations of domain knowledge can lack coverage. Secondly, the
complex reasoning required with these rich information sources typically involves nonmonotonic
inferences (e.g., Hobbs et al., 1993; Lascarides & Asher, 1993), which become intractable except
for toy examples.
Allen (1995), Hitzeman, Moens, and Grover (1995), and Han and Lavie (2004) propose more
computationally tractable approaches to infer temporal information from text, by hand-crafting algorithms which integrate shallow versions of the knowledge sources that are exploited in the above
theoretical literature (e.g., Hobbs et al., 1993; Kamp & Reyle, 1993). While this type of symbolic
approach is promising, and overcomes some of the impracticalities of utilizing full logical forms and
complex reasoning over rich domain knowledge sources, it is not grounded in empirical evidence of
the way the various linguistic features contribute to the temporal semantics of a discourse; nor are
these algorithms evaluated against real data. Moreover, the approach is typically domain-dependent
and robustness is compromised when porting to new domains or applications.
Acquiring a model of temporal relations via machine learning over a training corpus promises
to provide systems which are precise, robust, and grounded in empirical evidence. A number of
markup languages have recently emerged that can greatly facilitate annotation efforts in creating suitable corpora. A notable example is TimeML (Pustejovsky, Ingria, Sauri, Castano, Littman,
Gaizauskas, & Setzer, 2004; see also the annotation scheme by Katz & Arosio, 2001), a metadata
standard for expressing information about the temporal properties of events and temporal relations
between them. The scheme can be used to annotate a variety of temporal expressions, including
tensed verbs, adjectives and nominals that correspond to times, events or states. The type of temporal information that can be expressed on these various linguistic expressions includes the class of
event, its tense, grammatical aspect, polarity (positive or negative), the time denoted (e.g., one can
annotate yesterday as denoting the day before the document date), and temporal relations between
pairs of eventualities and between events and times. TimeML’s expressive capabilities are illustrated
in the TimeBank corpus which contains temporal annotations of news report documents (for details,
see Section 7).
Mani, Schiffman, and Zhang (2003) and Mani and Schiffman (2005) demonstrate that
TimeML-compliant annotations are useful for learning a model of temporal relations in news text.
They focus on the problem of ordering pairs of successively described events. A decision tree classifier is trained on a corpus of temporal relations provided by human subjects. Using features such
as the position of the sentence within the paragraph (and the position of the paragraph in the text),
discourse connectives, temporal prepositions and other temporal modifiers, tense features, aspect
shifts and tense shifts, their best model achieves 75.4% accuracy in identifying the temporal order
of events. Boguraev and Ando (2005) use semi-supervised learning for recognizing events and inferring temporal relations (between an event and a time expression). Their method exploits TimeML
88

L EARNING S ENTENCE - INTERNAL T EMPORAL R ELATIONS

annotations from the TimeBank corpus and large amounts of unannotated data. They first build a
classifier from the TimeML annotations using a variety of features based on syntactic analysis and
the identification of temporal expressions. The original feature vectors are next augmented with
unlabeled data sharing structural similarities with the training data. Their algorithm yields performances well above the baseline for both tasks.
Conceivably, existing corpus data annotated with discourse structure, such as the RST treebank (Carlson et al., 2001), might be reused to train a temporal relations classifier. For instance,
for text spans connected with RESULT, it is implied by the semantics of this relation that the events
in the first span temporally precede the second; thus, a classifier of rhetorical relations could indirectly contribute to a classifier of temporal relations. Corpus-based methods for computing discourse structure are beginning to emerge (e.g., Marcu, 1999; Soricut & Marcu, 2003; Baldridge &
Lascarides, 2005). But there is currently no automatic mapping from discourse structures to their
temporal consequences; so although there is potential for eventually using linguistic resources labeled with discourse structure to acquire a model of temporal relations, that potential cannot be
presently realized.
Continuing on the topic of discourse relations, it is worth mentioning Marcu and Echihabi
(2002) whose approach bypasses altogether the need for manual coding in a supervised learning
setting. A key insight in their work is that rhetorical relations (e.g., EXPLANATION and CONTRAST)
are sometimes signaled by a discourse connective (e.g., because for EXPLANATION and but for
CONTRAST). They extract sentences containing such markers from a corpus, and then (automatically) identify the text spans connected by the marker, remove the marker and replace it with the
rhetorical relation it signals. A Naive Bayes classifier is trained on this automatically labeled data.
The model is designed to be maximally simple and employs solely word bigrams as features. Specifically, bigrams are constructed over the cartesian product of words occurring in the two text spans
and it is assumed that word pairs are conditionally independent. Marcu and Echihabi demonstrate
that such a knowledge-lean approach performs well, achieving an accuracy of 49.70% when distinguishing six relations (over a baseline of 16.67%). However, since the model relies exlusively on
word-co-occurrences, an extremely large training corpus (in the order of 40 M sentences) is required
to avoid sparse data (see Sporleder & Lascarides, 2005 for more detailed discussion on the tradeoff
between training size and feature space for discourse-based models).
In a sense, when considering the complexity of various models used to infer temporal and
discourse relations, Marcu and Echihabi’s (2002) model lies at the simple extreme of the spectrum,
whereas the semantics and inference-based approaches to discourse interpretation (e.g., Hobbs et al.,
1993; Asher & Lascarides, 2003) lie at the other extreme, for these latter theories assume no independence among the properties of the spans, and they exploit linguistic and non-linguistic features
to the full. In this paper, we aim to explore a number of probabilistic models which lie in between
these two extremes, thereby giving us the opportunity to study the tradeoff between the complexity
of the model on the one hand, and the amount of training data required on the other. We are particularly interested in assessing the performance of models on smaller training sets than those used by
Marcu and Echihabi (2002); such models will be useful for classifiers that are trained on data sets
where relatively rare temporal markers are exploited.
Our work differs from Mani et al. (2003) and Boguraev and Ando (2005) in that we do not
exploit manual annotations in any way. Our aim is however similar, since we also infer temporal
relations between pairs of events. We share with Marcu and Echihabi (2002) the use of data with
overt markers as a proxy for hand coded relations. Apart from the fact that our interpretation task is
89

L APATA & L ASCARIDES

different from theirs, our work departs from Marcu and Echihabi (2002) in three further important
ways. First, we propose alternative models and explore the contribution of linguistic information
to the inference task, investigating how this enables one to train on considerably smaller data sets.
Secondly, the proposed models are used to infer relations between events in a more realistic setting,
where temporal markers are naturally absent (i.e., the test data is not simulated by removing the
markers in question). And finally, we evaluate the models against human subjects performing the
same task, as well as against a gold standard corpus.

3. Problem Formulation

 

Given a main clause and a subordinate clause attached to it, our task is to infer the temporal marker
linking the two clauses. P SM t j SS represents the probability that a marker t j relates a main clause
SM and a subordinate clause SS . We aim to identify which marker t j in the set of possible markers T
maximizes the joint probability P S M t j SS :

 
 
 
  
     



  
     
     
   	 
  	 
   
	 
   	 
  	 
   	 
 
	 
   	 
   
argmax P SM t j SS

t

(3)

tj T

argmax P SM P SS SM P t j SM SS

t

tj T

We ignore the terms P SM and P SS SM in (3) as they are constant. We use Bayes’ Rule to calculate
P t j SM SS :
t

argmax P t j SM SS

(4)

tj T

t

argmax P t j P SM SS t j

t

argmax P t j P a M 1

tj T

a S n tj

tj T

SM and SS are vectors of features a M 1
a M n and a S 1
a S n characteristic of the propositions
occurring with the marker t j (our features are described in detail in Section 4.2). Estimating the
different P a M 1
a S n t j terms will not be feasible unless we have a very large set of training
data. We will therefore make the simplifying assumption that a temporal marker t j can be determined
by observing feature pairs representative of a main and a subordinate clause. We further assume that
these feature pairs are conditionally independent given the temporal marker and are not arbitrary:
rather than considering all pairs in the cartesian product of a M 1
a S n (see Marcu & Echihabi,
2002), we restrict ourselves to feature pairs that belong to the same class i. Thus, the probability of
observing the conjunction a M 1
a S n given t j is:

	 
  	 
 

	 
  	 
 

    ∏ 	 
 
   	 
   
t

argmax P t j
tj T

	 
 
  	 
   

n

i 1

(5)

P a M i a S i tj

For example, if we were assuming our feature space consisted solely of nouns and verbs, we would
estimate P a M i a S i t j by taking into account all noun-noun and verb-verb bigrams that are
attested in SS and SM and co-occur with t j .
The model in (4) can be further simplified by assuming that the likelihood of the subordinate
clause SS is conditionally independent of the main clause S M (i.e., P SS SM t j
P SS t j P SM t j ).

  

90

 

L EARNING S ENTENCE - INTERNAL T EMPORAL R ELATIONS





The assumption is clearly a simplification but makes the estimation of the probabilities P S M t j and
P SS t j more reliable in the face of sparse data:
t

 

  
	 
  	 
  	 
   	 
 

 ∏ 	 
    	 
   
	 
 


argmax P t j P SM t j P SS t j

(6)

tj T

SM and SS are again vectors of features a M 1
a M n and a S 1
a S n representing the clauses
co-occurring with the marker t j . Now individual features (instead of feature pairs) are assumed to
be conditionally independent given the temporal marker, and therefore:
t

 

argmax P t j
tj T

n

(7)

P a M i tj P a S i tj

i 1

	 
 

Returning to our example feature space of nouns and verbs, P a M i t j and P a S i t j will be now
estimated by considering how often verbs and nouns co-occur with t j . These co-occurrences will be
estimated separately for main and subordinate clauses.
Throughout this paper we will use the terms conjunctive for model (5) and disjunctive for
model (7). We effectively treat the temporal interpretation problem as a disambiguation task. From
a (confusion) set T of temporal markers, e.g., after, before, since , we select the one that maximizes (5) or (7) (see Section 4 for details on our confusion set and corpus). The conjunctive model
explicitly captures dependencies between the main and subordinate clauses, whereas the disjunctive
model is somewhat simplistic in that relationships between features across the two clauses are not
represented directly. However, if two values of these features for the main and subordinate clauses
co-occur frequently with a particular marker, then the conditional probability of these features on
that marker will approximate the right biases.
The conjunctive model is more closely related to the kinds of symbolic rules for inferring
temporal relations that are used in semantics and inference-based accounts (e.g., Hobbs et al., 1993).
Many rules typically draw on the relationships between the verbs in both clauses, or the nouns in
both clauses, and so on. Both the disjunctive and conjunctive models are different from Marcu
and Echihabi’s (2002) model in several respects. They utilize linguistic features rather than word
bigrams. The conjunctive model’s features are two-dimensional with each dimension belonging to
the same feature class. The disjunctive model has the added difference that it assumes independence
in the features attested in the two clauses.




4. Parameter Estimation

	 
 	 

		 
   



We can estimate the parameters for our models from a large corpus. In their simplest form, the
features a M i and a S i can be the words making up main and subordinate clauses. In order to extract
relevant features, we first identify clauses in a hypotactic relation, i.e., main clauses of which the
subordinate clause is a constituent. In the training phase, we estimate the probabilities P a M i t j
and P a S i t j for the disjunctive model by simply counting the occurrence of the features a M i
and a S i with marker t j (i.e., f a M i t j ) and ( f a S i t j ). In essence, we assume for this model
that the corpus is representative of the way various temporal markers are used in English. For the
conjunctive model we estimate the co-occurrence frequencies f a M i a S i t j . Features with zero
counts are smoothed in both models; we adopt the m-estimate with uniform priors, with m equal to
the size of the feature space (Cestnik, 1990).

	 
 

	 
 

91

	 
 	  



	 
 	 
 

L APATA & L ASCARIDES

(S1 (S (NP (DT The) (NN company))
(VP (VBD said)
(S (NP (NNS employees))
(VP (MD will)
(VP (VB lose)
(NP (PRP their) (NNS jobs))
(SBAR-TMP (IN after)
(S (NP (DT the) (NN sale))
(VP (AUX is) (VP (VBN completed)))
))))))))

Figure 1: Extraction of main and subordinate clause from parse tree
4.1 Data Extraction
In order to obtain training and testing data for the models described in the previous section, subordinate clauses (and their main clause counterparts) were extracted from the B LLIP corpus (30 M
words). The latter is a Treebank-style, machine-parsed version of the Wall Street Journal (WSJ,
years 1987–89) which was produced using Charniak’s (2000) parser. Our study focused on the following (confusion) set of temporal markers: after, before, while, when, as, once, until, since . We
initially compiled a list of all temporal markers discussed in Quirk, Greenbaum, Leech, and Svartvik
(1985) and eliminated markers with frequency less than 10 per million in our corpus.
We extract main and subordinate clauses connected by temporal discourse markers, by first
traversing the tree top-down until we identify the tree node bearing the subordinate clause label
we are interested in and then extract the subtree it dominates. Assuming we want to extract after
subordinate clauses, this would be the subtree dominated by SBAR-TMP in Figure 1 indicated by
the arrow pointing down (see after the sale is completed ). Having found the subordinate clause, we
proceed to extract the main clause by traversing the tree upwards and identifying the S node immediately dominating the subordinate clause node (see the arrow pointing up in Figure 1, employees
will lose their jobs ). In cases where the subordinate clause is sentence initial, we first identify the
SBAR-TMP node and extract the subtree dominated by it, and then traverse the tree downwards in
order to extract the S-tree immediately dominating it.
For the experiments described here we focus solely on subordinate clauses immediately dominated by S, thus ignoring cases where nouns are related to clauses via a temporal marker (e.g., John
left after lunch ). Note that there can be more than one main clause that qualify as attachment sites
for a subordinate clause. In Figure 1 the subordinate clause after the sale is completed can be attached either to said or will loose. There can be similar structural ambiguities for identifying the
subordinate clause; for example see (8), where the conjunction and should lie within the scope of the
subordinate before -clause (and indeed, the parser disambiguates the structural ambiguity correctly
for this case):




(8)

[ Mr. Grambling made off with $250,000 of the bank’s money [ before Colonial caught on and
denied him the remaining $100,000. ] ]

We are relying on the parser for providing relatively accurate resolutions of structural ambiguities, but unavoidably this will create some noise in the data. To estimate the extent of this noise,
we manually inspected 30 randomly selected examples for each of our temporal discourse markers
92

L EARNING S ENTENCE - INTERNAL T EMPORAL R ELATIONS

TMark
when
as
after
before
until
while
since
once
TOTAL

Frequency
35,895
15,904
13,228
6,572
5,307
3,524
2,742
638
83,810

Distribution (%)
42 83
19 00
15 79
7 84
6 33
4 20
3 27
0 76
100 00

Table 1: Subordinate clauses extracted from B LLIP corpus
i.e., 240 examples in total. All the examples that we inspected were true positives of temporal discourse markers save one, where the parser assumed that as took a sentential complement whereas
in reality it had an NP complement (i.e., an anti-poverty worker ):
(9)

[ He first moved to West Virginia [ as an anti-poverty worker, then decided to stay and start a
political career, eventually serving two terms as governor. ] ]

In most cases the noise is due to the fact that the parser either overestimates or underestimates
the extent of the text span for the two clauses. 98.3% of the main clauses and 99.6% of the subordinate clauses were accurately identified in our data set. Sentence (10) is an example where the parser
incorrectly identifies the main clause: it predicts that the after -clause is attached to to denationalise
the country’s water industry. Note, however, that the subordinate clause (as some managers resisted
the move and workers threatened lawsuits ) is correctly identified.
(10) [ Last July, the government postponed plans [ to denationalise the country’s water industry
[ after some managers resisted the move and workers threatened lawsuits. ] ] ]
The size of the corpus we obtain with these extraction methods is detailed in Table 1. There are
83,810 instances overall (i.e., just 0.20% of the size of the corpus used by Marcu and Echihabi,
2002). Also note that the distribution of temporal markers ranges from 0.76% (for once ) to 42.83%
(for when ).
Some discourse markers from our confusion set underspecify temporal semantic information.
For example, when can entail temporal overlap (see (11a), from Kamp & Reyle, 1993), or temporal
progression (see (11c), from Moens & Steedman, 1988). The same is true for once, since, and as :
(11) a.
b.
(12) a.
b.

Mary left when Bill was preparing dinner.

(temporal overlap)

When they built the bridge, they solved all their traffic problems. (temporal progression)
Once John moved to London, he got a job with the council.
Once John was living in London, he got a job with the council.

93

(temporal progression)
(temporal overlap)

L APATA & L ASCARIDES

(13) a.
b.
(14) a.
b.

John has worked for the council since he’s been living in London.
John moved to London since he got a job with the council there.
temporal precedence)

(temporal overlap)
(cause and hence

Grand melodies poured out of him as he contemplated Caesar’s conquest of Egypt. (temporal overlap)
I went to the bank as I ran out of cash.

(cause, and hence temporal precedence)

This means that if the model chooses when, once, or since as the most likely marker between
a main and subordinate clause, then the temporal relation between the events described is left underspecified. Of course the semantics of when or once limits the range of possible relations, but
our model does not identify which specific relation is conveyed by these markers for a given example. Similarly, while is ambiguous between a temporal use in which it signals that the eventualities
temporally overlap (see (15a)) and a contrastive use which does not convey any particular temporal
relation (although such relations may be conveyed by other features in the sentence, such as tense,
aspect and world knowledge; see (15b)).
(15) a.

While the stock market was rising steadily, even companies stuffed with cash rushed to
issue equity.

b.

While on the point of history he was directly opposed to Liberal Theology, his appeal
to a ‘spirit’ somehow detachable from the Jesus of history run very much along similar
lines to the Liberal approach.

We inspected 30 randomly-selected examples for markers with underspecified readings
(i.e., when, once, since, while and as ). The marker when entails a temporal overlap interpretation 70% of the time and as entails temporal overlap 75% of the time, whereas once and since are
more likely to entail temporal progression (74% and 80%, respectively). The markers while and as
receive predominantly temporal interpretations in our corpus. Specifically, while has non-temporal
uses in 13.3% of the instances in our sample and as in 25%. Once our interpretation model has
been applied, we could use these biases to disambiguate, albeit coarsely, markers with underspecified meanings. Indeed, we demonstrate with Experiment 3 (see Section 7) that our model is useful
for estimating unambiguous temporal relations, even when the original sentence had no temporal
marker, ambiguous or otherwise.
4.2 Model Features
A number of knowledge sources are involved in inferring temporal ordering including tense, aspect, temporal adverbials, lexical semantic information, and world knowledge (Asher & Lascarides,
2003). By selecting features that represent these knowledge sources, notwithstanding indirectly and
imperfectly, we aim to empirically assess their contribution to the temporal inference task. Below
we introduce our features and provide motivation behind their selection.
Temporal Signature (T) It is well known that verbal tense and aspect impose constraints on the
temporal order of events and also on the choice of temporal markers. These constraints are perhaps
best illustrated in the system of Dorr and Gaasterland (1995) who examine how inherent (i.e., states
and events) and non-inherent (i.e., progressive, perfective) aspectual features interact with the time
stamps of the eventualities in order to generate clauses and the markers that relate them.
94

L EARNING S ENTENCE - INTERNAL T EMPORAL R ELATIONS

FINITE

NON - FINITE
MODALITY
ASPECT
VOICE
NEGATION

=
=
=
=
=
=


past, present
0, infinitive, ing-form, en-form
/ future, ability, possibility, obligation
0,
imperfective, perfective, progressive
active, passive
affirmative, negative






















Table 2: Temporal signatures
Feature
FIN
PAST
ACT
MOD
NEG

onceM
0.69
0.28
0.87
0.22
0.97

onceS
0.72
0.34
0.51
0.02
0.98

sinceM
0.75
0.35
0.85
0.07
0.95

sinceS
0.79
0.71
0.81
0.05
0.97

Table 3: Relative frequency counts for temporal features in main (subscript M) and subordinate
(subscript S) clauses

Although we cannot infer inherent aspectual features from verb surface form (for this we would
need a dictionary of verbs and their aspectual classes together with a process that assigns aspectual
classes in a given context), we can extract non-inherent features from our parse trees. We first
identify verb complexes including modals and auxiliaries and then classify tensed and non-tensed
expressions along the following dimensions: finiteness, non-finiteness, modality, aspect, voice, and
polarity. The values of these features are shown in Table 2. The features finiteness and non-finiteness
are mutually exclusive.
Verbal complexes were identified from the parse trees heuristically by devising a set of 30 patterns that search for sequences of auxiliaries and verbs. From the parser output verbs were classified
as passive or active by building a set of 10 passive identifying patterns requiring both a passive
auxiliary (some form of be and get ) and a past participle.
To illustrate with an example, consider again the parse tree in Figure 1. We identify the verbal
groups will lose and is completed from the main and subordinate clause respectively. The former
is mapped to the features present, 0, future, imperfective, active, affirmative , whereas the latter is
/ imperfective, passive, affirmative , where 0 indicates the verb form is finite
mapped to present, 0, 0,
and 0/ indicates the absence of a modal. In Table 3 we show the relative frequencies in our corpus for
finiteness (FIN), past tense (PAST), active voice (ACT), and negation (NEG) for main and subordinate
clauses conjoined with the markers once and since. As can be seen there are differences in the
distribution of counts between main and subordinate clauses for the same and different markers. For
instance, the past tense is more frequent in since than once subordinate clauses and modal verbs
are more often attested in since main clauses when compared with once main clauses. Also, once
main clauses are more likely to be active, whereas once subordinate clauses can be either active or
passive.








95

L APATA & L ASCARIDES

TMark
after
as
before
once
since
until
when
while

VerbM
sell
come
say
become
rise
protect
make
wait

VerbS
leave
acquire
announce
complete
expect
pay
sell
complete

SupersenseM
communication
motion
stative
stative
stative
communication
stative
communication

SupersenseS
communication
motion
stative
stative
change
possession
motion
social

LevinM
say
say
say
say
say
say
characterize
say

LevinS
say
begin
begin
get
begin
get
get
amuse

Table 4: Most frequent verbs and verb classes in main (subscript M) and subordinate clauses (subscript M)

Verb Identity (V) Investigations into the interpretation of narrative discourse have shown that specific lexical information plays an important role in determining temporal interpretation (e.g., Asher
& Lascarides, 2003). For example, the fact that verbs like push can cause movement of their object
and verbs like fall describe the movement of their subject can be used to interpret the discourse
in (16) as the pushing causing the falling, thus making the linear order of the events mismatch their
temporal order.
(16) Max fell. John pushed him.
We operationalize lexical relationships among verbs in our data by counting their occurrence in
main and subordinate clauses from a lemmatized version of the B LLIP corpus. Verbs were extracted
from the parse trees containing main and subordinate clauses. Consider again the tree in Figure 1.
Here, we identify lose and complete, without preserving information about tense or passivisation
which is explicitly represented in our temporal signatures. Table 4 lists the most frequent verbs
attested in main (VerbM ) and subordinate (VerbS ) clauses conjoined with the temporal markers after,
as, before, once, since, until, when, and while (TMark).
Verb Class (VW , VL ) The verb identity feature does not capture meaning regularities concerning
the types of verbs entering in temporal relations. For example, in Table 4 sell and pay are possession
verbs, say and announce are communication verbs, and come and rise are motion verbs. Asher and
Lascarides (2003) argue that many of the rules for inferring temporal relations should be specified in
terms of the semantic class of the verbs, as opposed to the verb forms themselves, so as to maximize
the linguistic generalizations captured by a model of temporal relations. For our purposes, there is an
additional empirical motivation for utilizing verb classes as well as the verbs themselves: it reduces
the risk of sparse data. Accordingly, we use two well-known semantic classifications for obtaining
some degree of generalization over the extracted verb occurrences, namely WordNet (Fellbaum,
1998) and the verb classification proposed by Levin (1995).
Verbs in WordNet are classified in 15 broad semantic domains (e.g., verbs of change, verbs of
cognition, etc.) often referred to as supersenses (Ciaramita & Johnson, 2003). We therefore mapped
the verbs occurring in main and subordinate clauses to WordNet supersenses (feature V W ). Semantically ambiguous verbs will correspond to more than one semantic class. We resolve ambiguity
96

L EARNING S ENTENCE - INTERNAL T EMPORAL R ELATIONS

TMark
after
as
before
once
since
until
when
while

NounN
year
market
time
stock
company
president
act
group

NounS
company
dollar
year
place
month
year
act
act

SupersenseM
act
act
act
act
act
act
year
chairman

SupersenseS
act
act
group
act
act
act
year
plan

AdjM
last
recent
long
more
first
new
last
first

AdjS
new
previous
new
new
last
next
last
other

Table 5: Most frequent nouns, noun classes, and adjectives in main (subscript M) and subordinate
clauses (subscript M)

heuristically by always defaulting to the verb’s prime sense (as indicated in WordNet) and selecting its corresponding supersense. In cases where a verb is not listed in WordNet we default to its
lemmatized form.
Levin (1995) focuses on the relation between verbs and their arguments and hypothesizes that
verbs which behave similarly with respect to the expression and interpretation of their arguments
share certain meaning components and can therefore be organized into semantically coherent classes
(200 in total). Asher and Lascarides (2003) argue that these classes provide important information
for identifying semantic relationships between clauses. Verbs in our data were mapped into their
corresponding Levin classes (feature V L ); polysemous verbs were disambiguated by the method
proposed by Lapata and Brew (2004). 3 Again, for verbs not included in Levin, the lemmatized verb
form was used. Examples of the most frequent Levin classes in main and subordinate clauses as
well as WordNet supersenses are given in Table 4.
Noun Identity (N) It is not only verbs, but also nouns that can provide important information
about the semantic relation between two clauses; Asher and Lascarides (2003) discuss an example
in which having the noun meal in one sentence and salmon in the other serves to trigger inferences
that the events are in a part-whole relation (eating the salmon was part of the meal). An example
from our corpus concerns the nouns share and market. The former is typically found in main clauses
preceding the latter which is often in a subordinate clause. Table 5 shows the most frequently attested nouns (excluding proper names) in main (Noun M ) and subordinate (NounS ) clauses for each
temporal marker. Notice that time denoting nouns (e.g., year, month ) are relatively frequent in this
data set.
Nouns were extracted from a lemmatized version of the B LLIP corpus. In Figure 1 the nouns
employees, jobs and sales are relevant for the Noun feature. In cases of noun compounds, only
the compound head (i.e., rightmost noun) was taken into account. A small set of rules was used
to identify organizations (e.g., United Laboratories Inc.), person names (e.g., Jose Y. Campos ),
3. Lapata and Brew (2004) develop a simple probabilistic model which determines for a given polysemous verb and its
frame its most likely meaning overall (i.e., across a corpus), without relying on the availability of a disambiguated
corpus. Their model combines linguistic knowledge in the form of Levin (1995) classes and frame frequencies acquired from a parsed corpus.

97

L APATA & L ASCARIDES

and locations (e.g., New England ) which were subsequently substituted by the general categories
person, organization, and location.
Noun Class (NW ) As with verbs, Asher and Lascarides (2003) argue in favor of symbolic rules
for inferring temporal relations that utilize the semantic classes of nouns wherever possible, so as
to maximize the linguistic generalizations that are captured. For example, they argue that one can
infer a causal relation in (17) on the basis that the noun bruise has a cause via some act-on predicate
with some underspecified agent (other nouns in this class include injury, sinking, construction ):
(17) John hit Susan. Her bruise is enormous.
Similarly, inferring that salmon is part of a meal in (18) rests on the fact that the noun salmon, in
one sense at least, denotes an edible substance.
(18) John ate a wonderful meal. He devoured lots of salmon.
As in the case of verbs, nouns were also represented by supersenses from the WordNet taxonomy. Nouns in WordNet do not form a single hierarchy; instead they are partitioned according to a
set of semantic primitives into 25 supersenses (e.g., nouns of cognition, events, plants, substances,
etc.), which are treated as the unique beginners of separate hierarchies. The nouns extracted from
the parser were mapped to WordNet classes. Ambiguity was handled in the same way as for verbs.
Examples of the most frequent noun classes attested in main and subordinate clauses are illustrated
in Table 5.
Adjective (A) Our motivation for including adjectives in the feature set is twofold. First, we hypothesize that temporal adjectives (e.g., old, new, later ) will be frequent in subordinate clauses
introduced by temporal markers such as before, after, and until and therefore may provide clues for
relations signaled by these markers. Secondly, similarly to verbs and nouns, adjectives carry important lexical information that can be used for inferring the semantic relation that holds between two
clauses. For example, antonyms can often provide clues about the temporal sequence of two events
(see incoming and outgoing in (19)).
(19) The incoming president delivered his inaugural speech. The outgoing president resigned last
week.
As with verbs and nouns, adjectives were extracted from the parser’s output. The most frequent
adjectives in main (AdjM ) and subordinate (AdjS ) clauses are given in Table 4.
Syntactic Signature (S) The syntactic differences in main and subordinate clauses are captured
by the syntactic signature feature. The feature can be viewed as a measure of tree complexity,
as it encodes for each main and subordinate clause the number of NPs, VPs, PPs, ADJPs, and
ADVPs it contains. The feature can be easily read off from the parse tree. The syntactic signature
for the main clause in Figure 1 is [NP:2 VP:2 ADJP:0 ADVP:0 PP:0] and for the subordinate
clause [NP:1 VP:1 ADJP:0 ADVP:0 PP:0]. The most frequent syntactic signature for main clauses is
[NP:2 VP:1 PP:0 ADJP:0 ADVP:0]; subordinate clauses typically contain an adverbial phrase [NP:2
VP:1 ADJP:0 ADVP:1 PP:0]. One motivating case for using this syntactic feature involves verbs
describing propositional attitudes (e.g., said, believe, realize ). Our set of temporal discourse markers
will have varying distributions as to their relative semantic scope to these verbs. For example, one
98

L EARNING S ENTENCE - INTERNAL T EMPORAL R ELATIONS

would expect until to take narrow semantic scope (i.e., the until-clause would typically attach to the
verb in the sentential complement to the propositional attitude verb, rather than to the propositional
attitude verb itself), while the situation might be different for once.
Argument Signature (R) This feature captures the argument structure profile of main and subordinate clauses. It applies only to verbs and encodes whether a verb has a direct or indirect object, and
whether it is modified by a preposition or an adverbial. As the rules for inferring temporal relations
in Hobbs et al. (1993) and Asher and Lascarides (2003) attest, the predicate argument structure of
clauses is crucial to making the correct temporal inferences in many cases. To take a simple example, observe that inferring the causal relation in (16) crucially depends on the fact that the subject of
fall denotes the same person as the direct object of push ; without this, a relation other than a causal
one would be inferred.
As with syntactic signature, this feature was read from the main and subordinate clause parsetrees. The parsed version of the B LLIP corpus contains information about subjects. NPs whose
nearest ancestor was a VP were identified as objects. Modification relations were recovered from
the parse trees by finding all PPs and ADVPs immediately dominated by a VP. In Figure 1 the
argument signature of the main clause is [SUBJ OBJ] and for the subordinate it is [OBJ].
Position (P) This feature simply records the position of the two clauses in the parse tree,
i.e., whether the subordinate clause precedes or follows the main clause. The majority of the main
clauses in our data are sentence initial (80.8%). However, there are differences among individual
markers. For example, once clauses are equally frequent in both positions. 30% of the when clauses
are sentence initial whereas 90% of the after clauses are found in the second position. These statistics clearly show that the relative positions of the main vs. subordinate clauses are going to be
relatively informative for the the interpretation task.
In the following sections we describe our experiments with the models introduced in Section 3. We first investigate their performance on temporal interpretation in the context of a pseudodisambiguation task (Experiment 1). We also describe a study with humans (Experiment 2) which
enables us to examine in more depth the models’ behavior and the difficulty of the inference task.
Finally, we evaluate the proposed approach in a more realistic setting, using sentences that do not
contain explicit temporal markers (Experiment 3).

5. Experiment 1: Temporal Inference as Pseudo-disambiguation
Method Our models were trained on main and subordinate clauses extracted from the B LLIP
corpus as detailed in Section 4. In the testing phase, all occurrences of the relevant temporal markers
were removed and the models were used to select the marker which was originally attested in the
corpus. This experimental setup is admittedly artificial, but important in revealing the difficulty of
the task at hand. A model that performs deficiently at the pseudo-disambiguation task, has little
hope of inferring temporal relations in a more natural setting where events are neither connected via
temporal markers nor found in a main-subordinate relationship.
Recall that we obtained 83,810 main-subordinate pairs. These were randomly partitioned into
training (80%), development (10%) and test data (10%). Eighty randomly selected pairs from the
test data were reserved for the human study reported in Experiment 2. We performed parameter
tuning on the development set; all our results are reported on the unseen test set, unless otherwise
stated. We compare the performance of the conjunctive and disjunctive models, thereby assessing
99

L APATA & L ASCARIDES

Symbols
†
$
‡
&
#

Meaning
significantly different from Majority Baseline
significantly different from Word-based Baseline
significantly different from Conjunctive Model
significantly different from Disjunctive Model
significantly different from Conjunctive Ensemble
significantly different from Disjunctive Ensemble

Table 6: Meaning of diacritics indicating statistical significance (χ 2 tests, p


0 05)

the effect of feature (in)dependence on the temporal interpretation task. Furthermore, we compare
the performance of the two proposed models against a baseline disjunctive model that employs a
w M i t j ) and P a S i
w S i t j ). This model
word-based feature space (see (7) where P a M i
resembles Marcu and Echihabi’s (2002)’s model in that it does not make use of the linguistically
motivated features presented in the previous section; all that is needed for estimating its parameters
is a corpus of main-subordinate clause pairs. We also report the performance of a majority baseline
(i.e., always select when, the most frequent marker in our data set).
In order to assess the impact of our feature classes (see Section 4.2) on the interpretation task,
the feature space was exhaustively evaluated on the development set. We have nine classes, which
results in 9 9!k ! combinations where k is the arity of the combination (unary, binary, ternary, etc.).
We measured the accuracy of all class combinations (1,023 in total) on the development set. From
these, we selected the best performing ones for evaluating the models on the test set.

	 
  	 
 

	 
  	 
 







Results Our results are shown in Table 7. We report both accuracy and F-score. A set of diacritics
is used to indicate significance (on accuracy) throughout this paper (see Table 6). The best performing disjunctive model on the test set (accuracy 62.6%) was observed with the combination of verbs
(V) with syntactic signatures (S). The combination of verbs (V), verb classes (V L , VW ), syntactic signatures (S) and clause position (P) yielded the highest accuracy (60.3%) for the conjunctive
model. Both conjunctive and disjunctive models performed significantly better than the majority
baseline and word-based model which also significantly outperformed the majority baseline. The
disjunctive model (SV) significantly outperformed the conjunctive one (V W VL PSV).
We attribute the conjunctive model’s worse performance to data sparseness. There is clearly
a trade-off between reflecting the true complexity of the task of inferring temporal relations and
the amount of training data available. The size of our data set favors a simpler model over a more
complex one. The difference in performance between the models relying on linguistically-motivated
features and the word-based model also shows that linguistic abstractions are useful in overcoming
sparse data.
We further analyzed the data requirements for our models by varying the amount of instances
on which they are trained. Figure 2 shows learning curves for the best conjunctive and disjunctive
models (VW VL PSV and SV). For comparison, we also examine how training data size affects the
(disjunctive) word-based baseline model. As can be seen, the disjunctive model has an advantage
over the conjunctive one; the difference is more pronounced with smaller amounts of training data.
Very small performance gains are obtained with increased training data for the word baseline model.
A considerably larger training set is required for this model to be competitive against the more lin100

L EARNING S ENTENCE - INTERNAL T EMPORAL R ELATIONS

Model
Majority Baseline
Word-based Baseline
Conjunctive (VW VL PSV)
Disjunctive (SV)

Accuracy
42 6†$‡#&
48 2 $‡#&
60 3 †‡#&
62 6 †$#&

Ensemble (Conjunctive)
Ensemble (Disjunctive)

64 5
70 6






F-score
NA
44.7
53.3
62.3

†$‡&
†$‡#

59.9
69.1

Table 7: Summary of results for the temporal pseudo-disambiguation task; comparison of baseline
models against conjunctive and disjunctive models and their ensembles (V: verbs, V W :
WordNet verb supersenses, VL : Levin verb classes, P: clause position, S: syntactic signature)
70

Accuracy (%)

65

Word-based Baseline
Conjunctive Model
Disjunctive Model

60
55
50
45
40

K K K K K K K K K K K K K K K K K K K K
3.3 6.7 10 13.4 16.8 20.1 23.4 26.8 30.1 33.5 36.840.2 43.6 46.9 50.3 53.6 56.9 60.3 63.6 67.1

Number of instances in training data

Figure 2: Learning curve for conjunctive, disjunctive, and word-based models.
guistically aware models. This result is in agreement with Marcu and Echihabi (2002) who employ
a very large corpus (1 billion words, from which they extract 40 million training examples) for
training their word-based model.
Further analysis of our models’ output revealed that some feature combinations performed reasonably well on individual markers for both the disjunctive and conjunctive model, even though
their overall accuracy did not match the best feature combinations for either model class. Some
accuracies for these combinations are shown in Table 8. For example, NPRSTV was one of the best
combinations for generating after under the disjunctive model, whereas SV was better for before
(feature abbreviations are as introduced in Section 4.2). Given the complementarity of different
models, an obvious question is whether these can be combined. An important finding in machine
learning is that a set of classifiers whose individual decisions are combined in some way (an ensemble ) can be more accurate than any of its component classifiers if the errors of the individual
101

L APATA & L ASCARIDES

TMark

Disjunctive Model
Features
Accuracy

Conjunctive Model
Features
Accuracy

after
as
before
once
since
when
while
until

NPRSTV
ANNW PSV
SV
PRS
PRST
VL PS
PST
VL VW RT

VW PTV
VW VL SV
TV
VW P
VL V
VL NV
VL PV
VW VL PV

69 9
57 0
42 1
40 7
25 1
85 5
49 0
69 4

79 6
57 0
11 3
37
10
86 5
96
95

Table 8: Best feature combinations for individual markers (development set; V: verbs, V W : WordNet verb supersenses, VL : Levin verb classes, N: nouns, NW : WordNet noun supersenses,
P: clause position, R: argument signature, S: syntactic signature, T: tense signature)

classifiers are sufficiently uncorrelated (Dietterich, 1997). The next section reports on our ensemble
learning experiments.
Ensemble Learning An ensemble of classifiers is a set of classifiers whose individual decisions
are combined to classify new examples. This simple idea has been applied to a variety of classification problems ranging from optical character recognition to medical diagnosis and part-of-speech
tagging (for overviews, see Dietterich, 1997; van Halteren, Zavrel, & Daelemans, 2001). Ensemble
learners often yield superior results to individual learners provided that the component learners are
accurate and diverse (Hansen & Salamon, 1990).
An ensemble is typically built in two steps: first multiple component learners are trained and
next their predictions are combined. Multiple classifiers can be generated either by using subsamples
of the training data (Breiman, 1996a; Freund & Shapire, 1996) or by manipulating the set of input
features available to the component learners (Cherkauer, 1996). Weighted or unweighted voting is
the method of choice for combining individual classifiers in an ensemble. A more sophisticated
combination method is stacking where a learner is trained to predict the correct output class when
given as input the outputs of the ensemble classifiers (Wolpert, 1992; Breiman, 1996b; van Halteren
et al., 2001). In other words, a second-level learner is trained to select its output on the basis of the
patterns of co-occurrence of the output of several component learners.
We generated multiple classifiers (for combination in the ensemble) by varying the number
and type of features available to the conjunctive and disjunctive models discussed in the previous
section. The outputs of these models were next combined using c5.0 (Quinlan, 1993), a decision-tree
second level-learner. Decision trees are among the most widely used machine learning algorithms.
They perform a general to specific search of a feature space, adding the most informative features
to a tree structure as the search proceeds. The objective is to select a minimal set of features that
efficiently partitions the feature space into classes of observations and assemble them into a tree
(for details, see Quinlan, 1993). A classification for a test case is made by traversing the tree until
either a leaf node is found or all further branches do not match the test case, and returning the most
frequent class at the last node.
102

L EARNING S ENTENCE - INTERNAL T EMPORAL R ELATIONS

Conjunctive Ensemble
PSVVWNW VL NPVVW VL
PRTVVWVL
NSVVW
PSVVW PVVW NW PSVVL
NSV
PSV
PV
Disjunctive Ensemble
ANW NPSV APSV
ASV
PRSVW
PRS
PRST
PRSV
PSV
APTV
SVVW VL
NPV

PSVWVL PSVVWVL PVVWVL
PSVL
PVVL
NPSV
SV
TV
V
PSVN
SV

SVL

NPRSTV

Table 9: Component models for ensemble learning (A: adjectives, V: verbs, V W : WordNet verb
supersenses, VL : Levin verb classes, N: nouns, NW : WordNet noun supersenses, P: clause
position, R: argument signature, S: syntactic signature, T: tense signature)

Learning in this framework requires a primary training set for training the component learners;
a secondary training set for training the second-level learner and a test set for assessing the stacked
classifier. We trained the decision-tree learner on the development set using 10-fold cross-validation.
We experimented with 133 different conjunctive models and 65 disjunctive models; the best results
on the development set were obtained with the combination of 22 conjunctive models and 12 disjunctive models. The component models are presented in Table 9. The ensembles’ performance on
the test set is reported in Table 7.
As can be seen, both types of ensemble significantly outperform the word-based baseline, and
the best performing individual models. Furthermore, the disjunctive ensemble significantly outperforms the conjunctive one. Table 10 details the performance of the two ensembles for each individual
marker. Both ensembles have difficulty inferring the markers since, once and while ; the difficulty
is more pronounced in the conjunctive ensemble. We believe that the worse performance for predicting these relations is due to a combination of sparse data and ambiguity. First, observe that
these three classes have fewest examples in our data set (see Table 1). Secondly, once is temporally
ambiguous, conveying temporal progression and temporal overlap (see example (12)). The same
ambiguity is observed with since (see example (13)). Finally, although the temporal sense of while
always conveys temporal overlap, it has a non-temporal, contrastive sense too which potentially
creates some noise in the training data, as discussed in Section 4.1. Another contributing factor to
while ’s poor performance is the lack of sufficient training data. Note that the extracted instances
for this marker constitute only 4.2% of our data. In fact, the model often confuses the marker since
with the semantically similar while. This could be explained by the fact that the majority of training
examples for since had interpretations that imply temporal overlap, thereby matching the temporal
relation implied by while, which in turn was also the majority interpretation in our training corpus
(the non-temporal, contrastive sense accounting for only 13.3% of our training examples).
Let us now examine which classes of features have the most impact on the interpretation task
by observing the component learners selected for our ensembles. As shown in Table 8, verbs either
as lexical forms (V) or classes (VW , VL ), the syntactic structure of the main and subordinate clauses
(S) and their position (P) are the most important features for interpretation. Verb-based features are
present in all component learners making up the conjunctive ensemble and in 10 (out of 12) learners
for the disjunctive ensemble. The argument structure feature (R) seems to have some influence
(it is present in five of the 12 component (disjunctive) models), however we suspect that there is
some overlap with S. Nouns, adjectives and temporal signatures seem to have a small impact on
103

L APATA & L ASCARIDES

TMark
after
as
before
once
since
when
while
until
All

Disjunctive Ensemble
Accuracy
F-score
66 4
62 5
51 4
24 6
26 2
91 0
28 8
47 8
70 6

63 9
62 0
50 6
35 3
38 2
86 9
41 2
52 4
69 1

Conjunctive Ensemble
Accuracy
F-score
59 3
59 0
17 1
00
39
90 5
11 5
17 3
64 5

57 6
55 1
22 3
00
45
84 7
15 8
24 4
59 9

Table 10: Ensemble results on sentence interpretation for individual markers (test set)
the interpretation task, at least in the WSJ domain. Our results so far point to the importance of
the lexicon for inferring temporal relations but also indicate that the syntactic complexity of the
two clauses is an another key predictor. Asher and Lascarides’ (2003) symbolic theory of discourse
interpretation also emphasizes the importance of lexical information in inferring temporal relations,
while Soricut and Marcu (2003) find that syntax trees are useful for inferring discourse relations,
some of which have temporal consequences.

6. Experiment 2: Human Evaluation
Method We further assessed the temporal interpretation model by comparing its performance
against human judges. Participants were asked to perform a multiple choice task. They were given
a set of 40 main-subordinate pairs (five for each marker) randomly chosen from our test data. The
marker linking the two clauses was removed and participants were asked to select the missing word
from a set of eight temporal markers, thus mimicking the model’s task. Examples of the materials
our participants saw are given in Apendix A.
The study was conducted remotely over the Internet. Subjects first read a set of instructions that
explained the task, and had to fill in a short questionnaire including basic demographic information.
A random order of main-subordinate pairs and a random order of markers per pair was generated for
each subject. The study was completed by 198 volunteers, all native speakers of English. Subjects
were recruited via postings to local Email lists.
Results Our results are summarized in Table 11. We measured how well our subjects (Human)
agree with the gold standard (Gold)—i.e., the corpus from which the experimental items were
selected—and how well they agree with each other (Human-Human). We also show how well the
disjunctive ensemble (Ensemble) agrees with the subjects (Ensemble-Human) and the gold standard (Ensemble-Gold). We measured agreement using the Kappa coefficient (Siegel & Castellan,
1988) but also report percentage agreement to facilitate comparison with our model. In all cases we
compute pairwise agreements and report the mean.

104

L EARNING S ENTENCE - INTERNAL T EMPORAL R ELATIONS

Human-Human
Human-Gold
Ensemble-Human
Ensemble-Gold

K
.410
.421
.390
.413

%
45.0
46.9
44.3
47.5

Table 11: Agreement figures for subjects and disjunctive ensemble (Human-Human: inter-subject
agreement, Human-Gold: agreement between subjects and gold standard corpus,
Ensemble-Human: agreement between ensemble and subjects, Ensemble-Gold: agreement between ensemble and gold standard corpus)

after
as
before
once
since
until
when
while

after
.55
.14
.05
.17
.10
.06
.20
.16

as
.06
.33
.05
.06
.09
.03
.07
.05

before
.03
.02
.52
.10
.04
.05
.09
.08

once
.10
.02
.08
.35
.04
.10
.09
.03

since
.04
.03
.03
.07
.63
.03
.04
.04

until
.01
.03
.15
.03
.03
.65
.03
.02

when
.20
.20
.08
.17
.06
.05
.45
.10

while
.01
.23
.04
.05
.01
.03
.03
.52

Table 12: Confusion matrix based on percent agreement between subjects

As shown in Table 11 there is moderate agreement 4 among humans when selecting an appropriate temporal marker for a main and a subordinate clause. The ensemble’s agreement with the gold
standard approximates human performance on the interpretation task (K
413 for Ensemble-Gold
vs. K
421 for Human-Gold). The agreement of the ensemble with the subjects is also close to the
upper bound, i.e., inter-subject agreement (see Ensemble-Human and Human-Human in Table 11).
Further analysis revealed that the majority of disagreements among our subjects arose for as and
once clauses. Once was also problematic for the ensemble model (see Table 10). The inter-subject
agreement was 33% for as clauses and 35% for once clauses. For the other markers, the subject
agreement was around 55%. The highest agreement was observed for since and until (63% and
65% respectively). A confusion matrix summarizing the resulting inter-subject agreement for the
interpretation task is shown in Table 12.
The moderate agreement is not entirely unexpected given that some of the markers are semantically similar and in some cases more than one marker are compatible with the temporal implicatures
that arise from joining the two clauses. For example, when can be compatible with after, as, before,
once, and since. Besides when, as can be compatible with since, and while. Consider for example
the following sentence from our experimental materials: More and more older women are divorcing
when their husbands retire. Although when is the right connective according to the corpus, once





4. Landis and Koch (1977) give the following five qualifications for different values of Kappa: .00–.20 is slight, .21–.40
is fair, .41–.60 is moderate, .61–.80 is substantial, whereas .81–1.00 is almost perfect.

105

L APATA & L ASCARIDES

or after are also valid choices. Indeed after is often chosen instead of when by our subjects (see
Table 12). Also note that neither the model nor the subjects have access to the context surrounding
the sentence whose marker must be inferred. In the sentence A lot of them want to get out before
they get kicked out (again taken from our materials), knowing the referents of them and they is important in selecting the right relation. In some cases, substantial background knowledge is required
to make a valid temporal inference. In the sentence Are more certified deaths required before the
FDA acts? (see Appendix A), one must know what FDA stands for (i.e., Federal, Food, Drug, and
Cosmetic Act). In a less strict evaluation setting where more than one connective are considered
correct (on the basis of semantic compatibility), the inter-subject agreement is K
640 (67.7%).
Moreover, the ensemble’s agreement with the subjects is K
609 (67%).
We next evaluate the performance of the ensemble model on a more challenging task. Our test
data so far has been somewhat artificially created by removing the temporal marker connecting a
main and subordinate clause. Although this experimental setup allows to develop and evaluate temporal inference models relatively straightforwardly, it remains unsatisfactory. In most cases a temporal model would be required for interpreting events that are not only attested in main-subordinate
clauses but in a variety of constructions (e.g., in parataxis or indirect speech) which may not contain
temporal markers. We use the annotations in the TimeBank corpus for investigating whether our
model, which is trained on automatically annotated data, performs well on a more realistic test set.





7. Experiment 3: Predicting TimeML Relations
Method As mentioned earlier the TimeBank corpus has been manually annotated with the
TimeML coding scheme. In this scheme, verbs, adjectives, and nominals are annotated as EVENTs
and are marked up with attributes such as the class of the event (e.g., state, reporting), its tense
(e.g., present, past), aspect (e.g., perfective, progressive), and polarity (positive or negative). The
TLINK tag is used to represent temporal relationships between events, or between an event and a
time. These relationships can be inter- or intra-sentential. Table 13 illustrates the TLINK relationships with sentences taken from the TimeBank corpus. We focus solely on intra-sentential temporal
relations between events; Table 13 does not include the IDENTITY relationship which is commonly
attested inter-sententially.
Our intent here is to use the model presented in the previous sections to interpret the temporal
relationships between events like those shown in Table 13 in the absence of overtly verbalized temporal information (e.g., temporal markers). However, one stumbling block to performing this kind
of evaluation is that the corpus on which our model was trained uses different labels from those in
Table 13 (e.g., (ambiguous) temporal markers like when ). Fortunately, the temporal markers we considered and the TimeML relations are more or less semantically compatible, and so a mapping can
be devised. First notice that some of the relations in Table 13 are redundant. For instance BEFORE
is the inverse of AFTER, IS INCLUDED is the inverse of INCLUDES, and so on. Furthermore, some
semantic distinctions are too fine-grained for our model to identify accurately (e.g., BEFORE and
IBEFORE (immediately before), SIMULTANEOUS and DURING). We therefore reduced the relations
in Table 13 into a smaller set by collapsing BEFORE, IBEFORE, AFTER and IAFTER (immediately
after) into one relationship. Analogously, we collapsed SIMULTANEOUSLY and DURING, INCLUDES
and IS INCLUDED, BEGINS and BEGUN BY, and ENDS and ENDED BY. The reduced relation set is
also shown in Table 13 (within parentheses).

106

L EARNING S ENTENCE - INTERNAL T EMPORAL R ELATIONS

BEFORE

(BEFORE)

IBEFORE

(BEFORE)

AFTER

(BEFORE)

IAFTER

(BEFORE)

INCLUDES

(INCLUDES )

IS INCLUDED

(INCLUDES )

DURING

(INCLUDES)

ENDS

(ENDS)

ENDED BY

(ENDS)

BEGINS

(BEGINS)

BEGUN BY

(BEGINS)

SIMULTANEOUS

Table 13:

Pacific First Financial Corp. said shareholders approved its acquisition by Royal Trusstco Ltd. of Toronto for $27 a share, or
$212 million.
The first would be to launch the much-feared direct invasion of
Saudi Arabia, hoping to seize some Saudi oil fields and improve
his bargaining position.
In Washington today the Federal Aviation Administration released air traffic control tapes from the night the TWA flight eight
hundred went down.
In addition, Hewlett-Packard acquired a two-year option to buy
an extra 10%, of which half may be sold directly to HewlettPackard by Octel.
Under the offer, shareholders will receive one right for each 105
common shares owned.
The purchase price was disclosed in a preliminary prospectus issued in connection with MGM Grand’s planned offering of six
million common shares.
According to Jordanian officials, a smaller line into Jordan remained operating.
The government may move to seize the money that Mr. Antar is
using to pay legal fees.
The Financial Times 100-share index shed 47.3 points to close at
2082.1, down 4.5% from the previous Friday.
DPC, an investor group led by New York-based Crescott Investment Associates, had itself filed a suit in state court in Los Angeles seeking to nullify the agreement.
Saddam said he will begin withdrawing troops from Iranian territory on Friday and release Iranian prisoners of war.
Nearly 200 Israeli soldiers have been killed fighting Hezbollah
and other guerrillas guerrillas.

relationships in TimeBank; the events participating in the relationship are marked
with boldface; a more coarse-grained set of relationships is shown within parentheses.
TILINK

We next defined a mapping between our temporal connectives and the reduced set of TimeML
relations (see Table 14). Such a mapping cannot be one-to-one, since some of our connectives are
compatible with more than one temporal relationship (see Section 4.1). For instance when can
indicate an INCLUDES or BEFORE relationship. We also expect this mapping to be relatively noisy
given that some temporal markers entail non-temporal relationships (e.g., while ). Table 14 includes
an additional relation, namely “no-temp-rel”. We thus have the option of not assigning any temporal
relation, thereby avoiding the pitfall of making a wrong prediction in cases where non-temporal

107

L APATA & L ASCARIDES

TMark
after,before,once,when
as,when,while
as,when,while
since
until
no-temp-rel

TimeMLRel
BEFORE
INCLUDES
SIMULTANEOUS
BEGINS
ENDS

NO - TEMP - REL

TrainInst
31 643
21 859
22 165
2 810
5 333
22 523






TestInst
877
246
360
19
64
967

Table 14: Mapping between temporal markers and coarse-grained set of TimeML relations; number
of training and test instances per relation.

inferences are entailed by any two events. We next describe how training and test instances were
generated for our experiments.
The disjunctive ensemble model from Experiment 1 was trained on the B LLIP corpus using the
same features and component learners described in Sections 4.2 and 5. The training data consisted
of our original 83,810 main-subordinate clause pairs labeled with the temporal relations from Table 14 (second column). To these we added 22,523 instances representative of the NO - TEMP - REL
relation. Such instances were gathered by randomly concatenating main and subordinate clauses
belonging to different documents (for a similar method, see Marcu & Echihabi, 2002). We hypothesize that the two clauses do not trigger temporal relations, since they are neither syntactically nor
semantically related. Instances with connectives since and once were mapped to labels BEGINS and
ENDS, respectively. In addition to BEGINS, since can signal BEFORE, INCLUDES, and SIMULTANE OUS temporal relations. However, in our experiments instances with since were used to exclusively
learn the BEGINS relation. This is far from perfect, but we felt necessary since BEGINS is not represented by any other temporal marker. The training instances for as and while were equally split
between the relationships INCLUDES and SIMULTANEOUS. Similarly, the data for when was equally
split among BEFORE, INCLUDES, and SIMULTANEOUS. Instances with after, before, and once were
exclusively used for learning the BEFORE relation. The number of training instances per relation
(TrainInst) is given in Table 14.
As test data, we used sentences from the TimeBank corpus. We only tested the ensemble model
on intra-sentential event-event relations. Furthermore, we excluded sentences with overt temporal
connectives, as we did not want to positively influence the model’s performance. The TimeBank
corpus is not explicitly annotated with the NO - TEMP - REL relation. There are however sentences
in the corpus whose events do not participate in any temporal relationship. We therefore hypothesized that these sentences were representative of NO - TEMP - REL . The total number of test instances
(TestInst) used in this experiment is given in Table 14.
Results Our results are summarized in Table 15. We compare the performance of the disjunctive
ensemble from Section 5 against a naive word-based model. Both these models were trained on
main and subordinate clauses from the B LLIP corpus. We also report the accuracy of a majority
baseline which defaults to the most frequent class in the B LLIP training data (i.e., BEFORE). Finally,
we report the performance of a (disjunctive) ensemble model that has been trained and tested on
the TimeBank corpus (see the column TestInst in Table 14) using leave-one-out crossvalidation.
Comparison between the latter model and the B LLIP-trained ensemble will indicate whether unan108

L EARNING S ENTENCE - INTERNAL T EMPORAL R ELATIONS

Model
Majority Baseline
Word-based Baseline
Ensemble (Disjunctive)
Ensemble (Disjunctive)

TrainCorpus
B LLIP
B LLIP
B LLIP
TimeBank

Accuracy
34 7
39 1
53 0 †
42 7



F-score
NA
21.1
45.8
40.5

Table 15: Results on predicting TimeML event-event relationships; comparison between wordbased baseline and disjunctive ensemble models.

TimeMLRel
BEFORE
BEGINS
ENDS
INCLUDES
SIMULTANEOUS
NO - TEMP - REL

All

B LLIP
Accuracy F-score
46.4
47 6
10.5
78
14.1
37
50.0
51 5
46.7
47 8
62.8
66 1
53.0
45 8

TimeBank
Accuracy F-score
63 2
53 2
00
00
47
77
85
98
67
89
49 6
53 5
42 7
40 5

Table 16: Ensemble results on inferring individual temporal relations; comparison between ensemble model trained on B LLIP and TimeBank corpora.

notated data is indeed useful in reducing annotation effort and training requirements for temporal
interpretation models.
As can be seen, the disjunctive model trained on the B LLIP corpus significantly outperforms
the two baseline models. It also outperforms the ensemble model trained on TimeBank by a wide
margin.5 We find these results encouraging considering the approximations in our temporal interpretation model and the noise inherent in the B LLIP training data. Also note that, despite being
linguistically informed, our feature space encodes very basic semantic and temporal distinctions.
For example, aspectual information is not taken into account, and temporal expressions are not analyzed in detail. One would hope that more extensive feature engineering would result in improved
results.
We further examined how performance varies for each class. Table 16 provides a comparison
between the two ensemble models trained on B LLIP and the TimeBank corpus, respectively. Both
models have difficulty with BEGINS and ENDS classes. This is not entirely surprising, since these
classes are represented by a relatively small number of training instances (see Table 14). The two
models yield comparable results for BEFORE, whereas the B LLIP-trained ensemble delivers better
performance for INCLUDES, SIMULTANEOUS, and NO - TEMP - REL .
5. Unfortunately, we cannot use a χ2 test to assess whether the differences between the two ensembles are statistically
significant due to the leave-one-out crossvalidation methodology employed when training and testing on the TimeBank corpus. This was necessary given the small size of the event-event relation data extracted from TimeBank (2,533
instances in total, see Table 14).

109

L APATA & L ASCARIDES

We are not aware of any previous work that attempts to do a similar task. However, it is worth
mentioning Boguraev and Ando (2005) who consider the interpretation of event-time temporal relations inter- and intra-sententially. They report accuracies ranging from 53.1% and 58.8% depending
on the intervening distance between the events and the times in question (performance is better for
events and times occurring close to each other). Interestingly, their interpretation model exploits
unannotated corpora in conjunction with TimeML annotations to increase the amount of labeled
data for training. Their method identifies unannotated instances that are distributionally similar to
the manually annotated corpus. In contrast, we rely solely on unannotated data during training while
exploiting instances explicitly marked with temporal information. An interesting future direction is
the combination of such data with TimeML annotations as a basis for devising improved models
(for details, see Section 8).

8. General Discussion
In this paper we proposed a data intensive approach to temporal inference. We introduced models
that learn temporal relations from sentences where temporal information is made explicit via temporal markers and assessed their potential in inferring relations in cases where overt temporal markers
are absent. Previous work has focused on the automatic tagging of temporal expressions (Wilson
et al., 2001), on learning the ordering of events from manually annotated data (Mani et al., 2003),
and inferring the temporal relations between events and time expressions from both annotated and
unannotated data (Boguraev & Ando, 2005).
Our models bypass the need for manual annotation by training exclusively on instances of
temporal relations that are made explicit by the presence of temporal markers. We compared and
contrasted several models varying in their linguistic assumptions and employed feature space. We
also explored the tradeoff between model complexity and data requirements. Our results indicate
that less sophisticated models (e.g., the disjunctive model) tend to perform reasonably when utilizing
expressive features and training data sets that are relatively modest in size. We experimented with a
variety of linguistically motivated features ranging from verbs and their semantic classes to temporal
signatures and argument structure. Many of these features were inspired by symbolic theories of
temporal interpretation, which often exploit semantic representations (e.g., of the two clauses) as
well as complex inferences over world knowledge (e.g., Hobbs et al., 1993; Lascarides & Asher,
1993; Kehler, 2002).
Our best model achieved an F-score of 69.1% on inferring temporal relations when trained
and tested on the B LLIP corpus in the context of a pseudo-disambiguation task. This performance
is a significant improvement over the baseline and compares favorably with human performance
on the same task. Detailed exploration of the feature space further revealed that not only lexical
but also syntactic information is important for temporal inference. This result is in agreement with
Soricut and Marcu (2003) who find that syntax trees encode sufficient information to enable accurate
derivation of discourse relations.
We also evaluated our model’s performance on the more realistic task of predicting temporal
relations when these are not explicitly signaled in text. To this end, we evaluated a B LLIP-trained
model against TimeBank, a corpus that has been manually annotated with temporal relations according to the TimeML specifications. This experimental set-up was challenging from many perspectives. First, some of the temporal markers used in our study received multiple meanings. The ambiguity unavoidably introduced a certain amount of noise in estimating the parameters of our model
110

L EARNING S ENTENCE - INTERNAL T EMPORAL R ELATIONS

and defining a mapping between markers and TimeML relations. Second, there is no guarantee that
the relations signaled by temporal markers connecting main and subordinate clauses hold for events
attested in other syntactic configurations such as non-temporal subordination or coordination. Given
these approximations, our model performed reasonably, reaching an overall F-score of 45.8% on the
temporal inference task and showing best performance for relations BEFORE, INCLUDES, SIMUL TANEOUS and NO - TEMP - REL . These results show that it is possible to infer temporal information
from corpora even if they are not semantically annotated in any way and hold promise for relieving
the data acquisition bottleneck associated with creating temporal annotations.
An important future direction lies in modeling the temporal relations of events across sentences. In order to achieve full-scale temporal reasoning, the current model must be extended in a
number of ways. These involve the incorporation of extra-sentential information to the modeling
task as well as richer temporal information (e.g., tagged time expressions; see Mani et al., 2003).
The current models perform the inference task independently of their surrounding context. Experiment 2 revealed this is a rather difficult task; even humans cannot easily make decisions regarding
temporal relations out-of-context. In future work, we plan to take into account contextual (lexical and syntactic) as well as discourse-based features (e.g., coreference resolution). Many linguists
have also observed that identifying the discourse structure of a text, conceptualized as a hierarchical structure of rhetorically connected segments, and identifying the temporal relations among its
events are logically co-dependent tasks (e.g., Kamp & Reyle, 1993; Hobbs et al., 1993; Lascarides
& Asher, 1993). For example, the fact that we interpret (1a) as forming a narrative with (1c) and
(1c) as providing background information to (1b) yields the temporal relations among the events that
we described in Section 1: namely, the temporal progression between kissing the girl and walking
home, and the temporal overlap between remembering talking to her and walking home.
(1)

a.
b.
c.

John kissed the girl he met at a party.
Leaving the party, John walked home.
He remembered talking to her and asking her for her name.

This logical relationship between discourse structure and temporal structure suggests that the
output of a discourse parser (e.g., Marcu, 1999; Soricut & Marcu, 2003; Baldridge & Lascarides,
2005) could be used as an informative source of features for inferring temporal relations across
sentence boundaries. This would be analogous at the discourse level to the use we made here of a
sentential parser as a source of features in our experiments for inferring sentence-internal temporal
relations.
The approach presented in this paper can also be combined with the annotations present in
the TimeML corpus in a semi-supervised setting similar to Boguraev and Ando (2005) to yield
improved performance. Another interesting direction for future work would be to use the models
proposed here in a bootstrapping approach. Initially, a model is learned from unannotated data and
its output is manually edited following the “annotate automatically, correct manually” methodology
used to provide high volume annotation in the Penn Treebank project. At each iteration the model is
retrained on progressively more accurate and representative data. Another issue related to the nature
of our training data concerns the temporal information entailed by some of our markers which can
be ambiguous. This could be remedied either heuristically as discussed in Section 4.1 or by using
models trained on unambiguous markers (e.g., before, after ) to disambiguate instances with multiple
readings. Another possibility is to apply a separate disambiguation procedure on the training data
(i.e., prior to the learning of temporal inference models).
111

L APATA & L ASCARIDES

Finally, we would like to investigate the utility of these temporal inference models within
the context of specific natural language processing applications. We thus intend to explore their
potential in improving the performance of a multi-document summarisation system. For example, a
temporal reasoning component could be useful not only for extracting temporally congruent events,
but also for structuring the output summaries, i.e., by temporally ordering the extracted sentences.
Although the models presented here target primarily interpretation tasks, they could also be adapted
for generation tasks, e.g., for inferring if a temporal marker should be generated and where it should
be placed.

Acknowledgments
This work was supported by EPSRC (Lapata, grant GR/T04540/01; Lascarides,
grant GR/R40036/01). We are grateful to Regina Barzilay and Frank Keller for helpful
comments and suggestions. Thanks to the anonymous referees whose feedback helped to substantially improve the present paper. A preliminary version of this work was published in the
proceedings of NAACL 2004; we also thank the anonymous reviewers of that paper for their
comments.

112

L EARNING S ENTENCE - INTERNAL T EMPORAL R ELATIONS

Appendix A. Experimental Materials for Human Evaluation
The following is the list of materials used in the human evaluation study reported in Experiment 2
(Section 6). The sentences were extracted from the B LLIP corpus following the procedure described
in Section 4.1.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

In addition, agencies weren’t always efficient in getting the word to other agencies
the company
was barred.
when
Mr. Reagan learned of the news
National Security Adviser Frank Carlucci called to tell him he’d
seen it on television.
when
For instance, National Geographic caused an uproar
it used a computer to neatly move two
Egyptian pyramids closer together in a photo.
when
Rowes Wharf looks its best
seen from the new Airport Water Shuttle speeding across Boston
harbor.
when
More and more older women are divorcing
their husbands retire.
when
Together they prepared to head up a Fortune company
enjoying a tranquil country life. while
it has been estimated that 190,000 legal abortions to adolescents occurred, an unknown number
of illegal and unreported abortions took place as well.
while
Mr. Rough, who is in his late 40s, allegedly leaked the information
he served as a New York
Federal Reserve Bank director from January 1982 through December 1984.
while
The contest became an obsession for Fumio Hirai, a 30-year-old mechanical engineer, whose wife took
to ignoring him
he and two other men tinkered for months with his dancing house plants. while
He calls the whole experience “wonderful, enlightening, fulfilling” and is proud that MCI functioned
so well
he was gone.
while
A lot of them want to get out
they get kicked out.
before
prices started falling, the market was doing $1.5 billion a week in new issues, says the head of
investment banking at a major Wall Street firm.
before
But
you start feeling sorry for the fair sex, note that these are the Bundys, not the Bunkers.
before
The Organization of Petroleum Exporting Countries will travel a rocky road
its Persian Gulf
members again rule world oil markets.
before
Are more certified deaths required
the FDA acts?
before
Currently, a large store can be built only
smaller merchants in the area approve it, a difficult and
time consuming process.
after
The review began last week
Robert L. Starer was named president.
after
The lower rate came
the nation’s central bank, the Bank of Canada, cut its weekly bank rate to
7.2% from 7.54%.
after
Black residents of Washington’s low-income Anacostia section forced a three-month closing of a
Chinese-owned restaurant
the owner threatened an elderly black woman customer with a pistol.
after
Laurie Massa’s back hurt for months
a delivery truck slammed into her car in 1986.
after

Table 17: Materials for the temporal pseudo-disambiguation task; markers in bodlface indicate the
gold standard completion; subjects were asked to select the missing word from the set of
temporal markers after, before, while, when, as, once, until, since




113

L APATA & L ASCARIDES

21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40

Donald Lasater, 62, chairman and chief executive office, will assume the posts Mr. Farrell vacates
a successor is found.
until
The council said that the national assembly will be replaced with appointed legislators and that no
new elections will be held
the U.S. lifts economic sanctions.
until
those problems disappear, Mr. Melzer suggests working with the base, the raw material for all
forms of the money supply.
until
A green-coffee importer said there is sufficient supply in Brazil
the harvest gets into full swing
next month.
until
They will pump
the fire at hand is out.
until
the gene is inserted in the human TIL cells, another safety check would be made.
once
part of a bus system is subject to market discipline, the entire operation tends to respond. once
In China by contrast,
joint ventures were legal, hundreds were created.
once
The company said the problem goes away
the car warms up.
once
the Toronto merger is complete, the combined entity will have 352 lawyers.
once
The justices ruled that his admission could be used
he clearly had chosen speech over silence.
since
Milosevic’s popularity has risen
he became party chief in Serbia, Yugoslavia’s biggest republic,
in 1986.
since
The government says it has already eliminated 600 million hours of paperwork a year
Congress
passed the Paperwork Reduction Act in 1980.
since
It was the most serious rebellion in the Conservative ranks
Mr. Mulroney was elected four years
ago.
since
There have been at least eight settlement attempts
a Texas court handed down its multi-billion
dollar judgment two years ago.
since
Brud LeTourneau, a Seattle management consultant and Merit smoker, laughs at himself
he
keeps trying to flick non-existent ashes into an ashtray.
as
Britain’s airports were disrupted
a 24-hour strike by air traffic control assistants resulted in the
cancellation of more thank 500 flights and lengthy delays for travelers.
as
Stocks plunged
investors ignored cuts in European interest rates and dollar and bond rallies. as
At Boston’s Logan Airport, a Delta plane landed on the wrong runway
another jet was taking
off.
as
Polish strikers shut Gdansk’s port
Warsaw rushed riot police to the city.
as

Table 17: (continued)

114

L EARNING S ENTENCE - INTERNAL T EMPORAL R ELATIONS

References
Allen, J. (1995). Natural Language Understanding. Benjamin Cummins.
Asher, N., & Lascarides, A. (2003). Logics of Conversation. Cambridge University Press.
Baldridge, J., & Lascarides, A. (2005). Probabilistic head-driven parsing for discourse structure.
In Proceedings of the Ninth Conference on Computational Natural Language Learning, pp.
96–103, Ann Arbor, MI.
Boguraev, B., & Ando, R. K. (2005). TimeML-compliant text analysis for temporal reasoning. In
Proceedings of the 19th International Joint Conference on Artificial Intelligence, pp. 997–
1003, Edingburgh, UK.
Breiman, L. (1996a). Bagging predictors. Machine Learning, 2(24), 123–140.
Breiman, L. (1996b). Stacked regressions. Machine Learning, 3(24), 49–64.
Carlson, L., Marcu, D., & Okurowski, M. (2001). Building a discourse-tagged corpus in the framework of Rhetorical Structure Theory. In Proceedings of the 2nd SIGDIAL Workshop on Discourse and Dialogue, Eurospeech 2001, Aalborg, Denmark.
Cestnik, B. (1990). Estimating probabilities: a crucial task in machine learning. In Proceedings of
the 16th European Conference on Artificial Intelligence, pp. 147–149, Stockholm, Sweden.
Charniak, E. (2000). A maximum-entropy-inspired parser. In Proceedings of the 1st Conference of
the North American Chapter of the Assocation for Computational Linguistics, pp. 132–139,
Seattle, WA.
Cherkauer, K. J. (1996). Human expert-level performance on a scientific image analysis task by a
system using combined artificial neural networks. In Working Notes of the AAAI Workshop
on Integrating Multiple Learned Models, pp. 15–21, Portland, OR.
Ciaramita, M., & Johnson, M. (2003). Supersense tagging of unknown words in WordNet. In
Proceedings of the 8th Conference on Empirical Methods in Natural Language Processing,
pp. 168–175, Sapporo, Japan.
Dietterich, T. G. (1997). Machine learning research: Four current directions. AI Magazine, 18(4),
97–136.
Dorr, B., & Gaasterland, T. (1995). Selecting tense aspect and connective words in language generation. In Proceedings of the 14th International Joint Conference on Artificial Intelligence,
pp. 1299–1307, Montreal, Canada.
Dowty, D. (1986). The effects of aspectual class on the temporal sturcture of discourse: Semantics
or pragmatics?. Linguistics and Philosophy, 9(1), 37–61.
Fellbaum, C. (Ed.). (1998). WordNet: An Electronic Database. MIT Press, Cambridge, MA.
Ferro, L., Mani, I., Sundheim, B., & Wilson, G. (2000). TIDES temporal annotation guidelines.
Tech. rep., The MITRE Corporation.
Freund, Y., & Shapire, R. E. (1996). Experiments with a new boosting algorithm. In Proceedings
of the 13th International Conference on Machine Learning, pp. 148–156, Stanford, CA.
Han, B., & Lavie, A. (2004). A framework for resolution of time in natural language. ACM Transactions on Asian Language Information Processing (TALIP), 3(1), 11–32.
115

L APATA & L ASCARIDES

Hansen, L. K., & Salamon, P. (1990). Neural network ensembles. IEEE Transactions in Pattern
Analysis and Machine Intelligence, 12, 993–1001.
Hitzeman, J., Moens, M., & Grover, C. (1995). Algorithms for analyzing the temporal structure of
discourse. In Proceedings of the 7th Meeting of the European Chapter of the Association for
Computational Linguistics, pp. 253–260, Dublin, Ireland.
Hobbs, J. R., Stickel, M., Appelt, D., & Martin, P. (1993). Interpretation as abduction. Artificial
Intelligence, 63(1–2), 69–142.
Hwang, C., & Schubert, L. (1992). Tense trees as the finite structure of discourse. In Proceedings
of the 30th Annual Meeting of the Association for Computational Linguistics, pp. 232–240,
Newark, DE.
Kamp, H., & Reyle, U. (1993). From Discourse to the Lexicon: Introduction to Modeltheoretic
Semantics of Natural Language, Formal Logic and Discourse Representation Theory. Kluwer
Academic Publishers.
Katz, G., & Arosio, F. (2001). The annotation of temporal information in natural language sentences.
In Proceedings of ACL Workshop on Temporal and Spatial Information Processing, pp. 104–
111, Toulouse, France.
Kehler, A. (2002). Coherence, Reference and the Theory of Grammar. CSLI Publications, Cambridge University Press.
Landis, J. R., & Koch, G. G. (1977). The measurement of observer agreement for categorical data.
Biometrics, 33, 159–174.
Lapata, M., & Brew, C. (2004). Verb class disambiguation using informative priors. Computational
Linguistics, 30(1), 45–73.
Lascarides, A., & Asher, N. (1993). Temporal interpretation, discourse relations and commonsense
entailment. Linguistics and Philosophy, 16(5), 437–493.
Levin, B. (1995). English Verb Classes and Alternations. Chicago University Press.
Mani, I., & Schiffman, B. (2005). Temporally anchoring and ordering events in news. In Pustejovsky, J., & Gaizauskas, R. (Eds.), Time Event Recognition and Natural Language. John
Benjamins.
Mani, I., Schiffman, B., & Zhang, J. (2003). Inferring temporal ordering of events in news. In
Proceedings of the 1st Human Language Technology Conference and Annual Meeting of the
North American Chapter of the Association for Computational Linguistics, pp. 55–57, Edmonton, Canada.
Marcu, D. (1999). A decision-based approach to rhetorical parsing. In Proceedings of the 37th
Annual Meeting of the Association for Computational Linguistics, pp. 365–372, College Park,
MD.
Marcu, D., & Echihabi, A. (2002). An unsupervised approach to recognizing discourse relations.
In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,
pp. 368–375, Philadelphia, PA.
Moens, M., & Steedman, M. J. (1988). Temporal ontology and temporal reference. Computational
Linguistics, 14(2), 15–28.
116

L EARNING S ENTENCE - INTERNAL T EMPORAL R ELATIONS

Pustejovsky, J., Ingria, B., Sauri, R., Castano, J., Littman, J., Gaizauskas, R., & Setzer, A. (2004).
The specification of TimeML. In Mani, I., Pustejovsky, J., & Gaizauskas, R. (Eds.), The
Language of Time: A reader, pp. 545–558. Oxford University Press.
Pustejovsky, J., Mani, I., Belanger, L., Boguraev, B., Knippen, B., Litman, J., Rumshisky, A., See,
A., Symonen, S., van Guilder, J., van Guilder, L., & Verhagen, M. (2003). ARDA summer
workshop on graphical annotation toolkit for TimeML. Tech. rep..
Quinlan, R. J. (1993). C4.5: Programs for Machine Learning. Series in Machine Learning. Morgan
Kaufman, San Mateo, CA.
Quirk, R., Greenbaum, S., Leech, G., & Svartvik, J. (1985). A Comprehensive Grammar of the
English Language. Longman, London.
Saurı́, R., Littman, J., Gaizauskas, R., Setzer, A., & Pustejovsky, J. (2004). TimeML Annotation
Guidelines. TERQAS Workshop. Version 1.1.
Schilder, F., & Habel, C. (2001). From temporal expressions to temporal information: Semantic
tagging of news messages. In Proceedings of ACL Workshop on Temporal and Spatial Information Processing, pp. 65–72, Toulouse, France.
Setzer, A., & Gaizauskas, R. (2001). A pilot study on annotating temporal relations in text. In
Proceedings of ACL Workshop on Temporal and Spatial Information Processing, pp. 73–80,
Toulouse, France.
Siegel, S., & Castellan, N. J. (1988).
McGraw-Hill, New York.

Non Parametric Statistics for the Behavioral Sciences.

Soricut, R., & Marcu, D. (2003). Sentence level discourse parsing using syntactic and lexical information. In Proceedings of the 1st Human Language Technology Conference and Annual
Meeting of the North American Chapter of the Association for Computational Linguistics, pp.
228–235, Edmonton, Canada.
Sporleder, C., & Lascarides, A. (2005). Exploiting linguistic cues to classify rhetorical relations.
In Proceedings of Recent Advances in Natural Language Processing, pp. 532–539, Borovets,
Bulgaria.
van Halteren, H., Zavrel, J., & Daelemans, W. (2001). Improving accuracy in word class tagging
through combination of machine learning systems. Computational Linguistics, 27(2), 199–
230.
Wiebe, J. M., O’Hara, T. P., Öhrström Sandgren, T., & McKeever, K. J. (1998). An empirical
approach to temporal reference resolution. Journal of Artifical Intelligence Research, 9, 247–
293.
Wilson, G., Mani, I., Sundheim, B., & Ferro, L. (2001). A multilingual approach to annotating and
extracting temporal information. In Proceedings of ACL Workshop on Temporal and Spatial
Information Processing, pp. 81–87, Toulouse, France.
Wolpert, D. H. (1992). Stacked generalization. Neural Networks, 5, 241–259.

117

Journal of Artificial Intelligence Research 27 (2006) 25-53

Submitted 10/05; published 9/06

Generative Prior Knowledge for Discriminative Classification
Arkady Epshteyn
Gerald DeJong

aepshtey@uiuc.edu
dejong@uiuc.edu

Department of Computer Science
University of Illinois at Urbana-Champaign
201 N. Goodwin
Urbana, IL, 61801 USA

Abstract
We present a novel framework for integrating prior knowledge into discriminative classifiers. Our framework allows discriminative classifiers such as Support Vector Machines
(SVMs) to utilize prior knowledge specified in the generative setting. The dual objective of
fitting the data and respecting prior knowledge is formulated as a bilevel program, which
is solved (approximately) via iterative application of second-order cone programming. To
test our approach, we consider the problem of using WordNet (a semantic database of
English language) to improve low-sample classification accuracy of newsgroup categorization. WordNet is viewed as an approximate, but readily available source of background
knowledge, and our framework is capable of utilizing it in a flexible way.

1. Introduction
While SVM (Vapnik, 1995) classification accuracy on many classification tasks is often
competitive with that of human subjects, the number of training examples required to
achieve this accuracy is prohibitively large for some domains. Intelligent user interfaces,
for example, must adopt to the behavior of an individual user after a limited amount of
interaction in order to be useful. Medical systems diagnosing rare diseases have to generalize
well after seeing very few examples. Any natural language processing task that performs
processing at the level of n-grams or phrases (which is frequent in translation systems)
cannot expect to see the same sequence of words a sufficient number of times even in large
training corpora. Moreover, supervised classification methods rely on manually labeled
data, which can be expensive to obtain. Thus, it is important to improve classification
performance on very small datasets. Most classifiers are not competitive with humans
in their ability to generalize after seeing very few examples. Various techniques have been
proposed to address this problem, such as active learning (Tong & Koller, 2000b; Campbell,
Cristianini, & Smola, 2000), hybrid generative-discriminative classification (Raina, Shen,
Ng, & McCallum, 2003), learning-to-learn by extracting common information from related
learning tasks (Thrun, 1995; Baxter, 2000; Fink, 2004), and using prior knowledge.
In this work, we concentrate on improving small-sample classification accuracy with
prior knowledge. While prior knowledge has proven useful for classification (Scholkopf,
Simard, Vapnik, & Smola, 2002; Wu & Srihari, 2004; Fung, Mangasarian, & Shavlik, 2002;
Epshteyn & DeJong, 2005; Sun & DeJong, 2005), it is notoriously hard to apply in practice
because there is a mismatch between the form of prior knowledge that can be employed by
classification algorithms (either prior probabilities or explicit constraints on the hypothesis
c
2006
AI Access Foundation. All rights reserved.

Epshteyn & DeJong

space of the classifier) and the domain theories articulated by human experts. This is
unfortunate because various ontologies and domain theories are available in abundance, but
considerable amount of manual effort is required to incorporate existing prior knowledge
into the native learning bias of the chosen algorithm. What would it take to apply an
existing domain theory automatically to a classification task for which it was not specifically
designed? In this work, we take the first steps towards answering this question.
In our experiments, such a domain theory is exemplified by WordNet, a linguistic
database of semantic connections among English words (Miller, 1990). We apply WordNet to a standard benchmark task of newsgroup categorization. Conceptually, a generative
model describes how the world works, while a discriminative model is inextricably linked to
a specific classification task. Thus, there is reason to believe that a generative interpretation
of a domain theory would seem to be more natural and generalize better across different
classification tasks. In Section 2 we present empirical evidence that this is, indeed, the
case with WordNet in the context of newsgroup classification. For this reason, we interpret
the domain theory in the generative setting. However, many successful learning algorithms
(such as support vector machines) are discriminative. We present a framework which allows
the use of generative prior in the discriminative classification setting.
Our algorithm assumes that the generative distribution of the data is given in the
Bayesian framework: P rob(data|model) and the prior P rob0 (model) are known. However,
instead of performing Bayesian model averaging, we assume that a single model M ∗ has
been selected a-priori, and the observed data is a manifestation of that model (i.e., it is
drawn according to P rob(data|M ∗ )). The goal of the learning algorithm is to estimate
M ∗ . This estimation is performed as a two-player sequential game of full information.
The bottom (generative) player chooses the Bayes-optimal discriminator function f (M ) for
the probability distribution P rob(data|model = M ) (without taking the training data into
account) given the model M . The model M is chosen by the top (discriminative) player in
such a way that its prior probability of occurring, given by P rob0 (M ), is high, and it forces
the bottom player to minimize the training-set error of its Bayes-optimal discriminator
f (M ). This estimation procedure gives rise to a bilevel program. We show that, while the
problem is known to be NP-hard, its approximation can be solved efficiently by iterative
application of second-order cone programming.
The only remaining issue is how to construct the generative prior P rob0 (model) automatically from the domain theory. We describe how to solve this problem in Section 2,
where we also argue that the generative setting is appropriate for capturing expert knowledge, employing WordNet as an illustrative example. In Section 3, we give the necessary
preliminary information and important known facts and definitions. Our framework for incorporating generative prior into discriminative classification is described in detail in Section
4. We demonstrate the efficacy of our approach experimentally by presenting the results
of using WordNet for newsgroup classification in Section 5. A theoretical explanation of
the improved generalization ability of our discriminative classifier constrained by generative
prior knowledge appears in Section 6. Section 7 describes related work. Section 8 concludes
the paper and outlines directions for future research.
26

Generative Prior Knowledge for Discriminative Classification

2. Generative vs. Discriminative Interpretation of Domain Knowledge
WordNet can be viewed as a network, with nodes representing words and links representing
relationships between two words (such as synonyms, hypernyms (is-a), meronyms (partof), etc.). An important property of WordNet is that of semantic distance - the length
(in links) of the shortest path between any two words. Semantic distance approximately
captures the degree of semantic relatedness of two words. We set up an experiment to
evaluate the usefulness of WordNet for the task of newsgroup categorization. Each posting
was represented by a bag-of-words, with each binary feature representing the presence of
the corresponding word. The evaluation was done on pairwise classification tasks in the
following two settings:
1. The generative framework assumes that each posting x = [x1 , .., xn ] is generated by
a distinct probability distribution for each newsgroup. The simplest version of a
Linear Discriminan Analysis (LDA) classifier posits that x|(y = −1) ∼ N (µ 1 , I) and
x|(y = 1) ∼ N (µ2 , I) for posting x given label y ∈ {−1, 1}, where I ∈ R(n×n) is
the identity matrix. Classification is done by assigning the most probable label to
x: y(x) = 1 ⇔ P rob(x|1) > P rob(x| − 1). It is well-known (e.g. see Duda, Hart, &
Stork, 2001) that this decision rule is equivalent to the one given by the hyperplane
c1 , .., µ
cn ] are estimated via
(µ2 − µ1 )T x − 21 (µT2 µ2 − µT1 µ1 ) > 0. The means µbi = [µ
i
i
1
maximum likelihood from the training data [x1 , y1 ], .., [xm , ym ] .

2. The discriminative SVM classifier sets the separating hyperplane to directly minimize
the number of errors on the training data:
c1 , .., w
cn ], bb] = arg minw,b kwk s.t. yi (wT xi + b) ≥ 1, i = 1, .., m.
[w
b = [w

Our experiment was conducted in the learning-to-learn framework (Thrun, 1995; Baxter,
2000; Fink, 2004). In the first stage, each classifier was trained using training data from the
training task (e.g., for classifying postings into the newsgroups ’atheism’ and ’guns’). In the
second stage, the classifier was generalized using WordNet’s semantic information. In the
third stage, the generalized classifier was applied to a different, test task (e.g., for classifying
postings for the newsgroups ’atheism’ vs. ’mideast’) without seeing any data from this new
classification task. The only way for a classifier to generalize in this setting is to use the
original sample to acquire information about WordNet, and then exploit this information
to help it label examples from the test sample. In learning how to perform this task, the
system also learns how to utilize the classification knowledge implicit in WordNet.
We now describe the second and third stages for the two classifiers in more detail:

1. It is intuitive to interpret information embedded in WordNet as follows: if the title
of the newsgroup is ’guns’, then all the words with the same semantic distance to
’gun’ (e.g., ’artillery’, ’shooter’, and ’ordnance’ with the distance of two) provide a
similar degree of classification information. To quantify this intuition, let li,train =
j
1
n
[li,train
, .., li,train
, .., li,train
] be the vector of semantic distances in WordNet between
each feature word j and the label of each training task newsgroup i ∈ {1, 2}. Define
1. The standard LDA classifier assumes that x|(y = −1) ∼ N (µ1 , Σ) and x|(y = −1) ∼ N (µ2 , Σ) and
estimates the covariance matrix Σ as well as the means µ1 , µ2 from the training data. In our experiments,
we take Σ = I.

27

Epshteyn & DeJong

1)Train: atheism vs. guns
2)Train: atheism vs. guns
3)Train: guns vs. mideast
Test: atheism vs. mideast
Test: guns vs. mideast
Test: atheism vs. mideast
1

1

0.9

1

0.9

0.9

0.8

0.8

0.8

0.7

0.7

0.7

0.6

0.6

0.6

0.5

0.5

0.5
0

200 400 600 800 1000 1200 1400 1600 1800

0

200 400 600 800 1000 1200 1400 1600 1800

0

200 400 600 800 1000 1200 1400 1600 1800

Legend:

Generative
Discriminative

Figure 2.1: Test set accuracy as a percentage versus the number of training points for 3
different classification experiments. For each classification task, a random test
set is chosen from the full set of articles in 20 different ways. Error bars are
based on 95% confidence intervals.

P

χi (v) ,

c
µj
j
=v i
i,train
j
=v|
|j:li,train
j:l

, i = 1, 2, where | · | denotes cardinality of a set. χi compresses

information in µbi based on the assumption that words equidistant from the newsgroup
label are equally likely to appear in a posting from that newsgroup. To test the
performance of this compressed classifier on a new task with semantic distances given
j
). Notice
by li,test , the generative distributions are reconstructed via µji := χi (li,test
that if the classifier is trained and tested on the same task, applying the function χ i
is equivalent to averaging the components of the means of the generative distribution
corresponding to the equivalence classes of words equidistant from the label. If the
classifier is tested on a different classification task, the reconstruction process reassigns
the averages based on the semantic distances to the new labels.
2. It is less intuitive to interpret WordNet in a discriminative setting. One possible
interpretation is that coefficients w j of the separating hyperplane are governed by
semantic distances to labels, as captured by the compression function χ0 (v, u) ,
P

cj

w
j
j
=u
=v,l
2,train
1,train
j
j
|j:l1,train
=v,l2,train
=u|
j:l

j
j
and reconstructed via w j := χ0 (l1,test
, l2,test
).

Note that both the LDA generative classifier and the SVM discriminative classifier have
the same hypothesis space of separating hyperplanes. The resulting test set classification
accuracy for each classifier for a few classification tasks from the 20-newsgroup dataset
28

Generative Prior Knowledge for Discriminative Classification

(Blake & Merz, 1998) is presented in Figure 2.1. The x-axis of each graph represents the
size of the training task sample, and the y-axis - the classifier’s performance on the test
classification task. The generative classifier consistently outperforms the discriminative
classifier. It converges much faster, and on two out of three tasks the discriminative classifier
is not able to use prior knowledge nearly as effectively as the generative classifier even after
seeing 90% of all of the available training data. The generative classifier is also more
consistent in its performance - note that its error bars are much smaller than those of
the discriminative classifier. The results clearly show the potential of using background
knowledge as a vehicle for sharing information between tasks. But the effective sharing is
contingent on an appropriate task decomposition, here supplied by the tuned generative
model.
The evidence in Figure 2.1 seemingly contradicts the conventional wisdom that discriminative training outperforms generative for sufficiently large training samples. However, our
experiment evaluates the two frameworks in the context of using an ontology to transfer
information between learning tasks. This was never done before. The experiment demonstrates that the interpretation of semantic distance in WordNet is more intuitive in the
generative classification setting, probably because it better reflects the human intuitions
behind WordNet.
However, our goal is not just to construct a classifier that performs well without seeing
any examples of the test classification task. We also want a classifier that improves its
behavior as it sees new labeled data from the test classification task. This presents us
with a problem: one of the best-performing classifiers (and certainly the best on the text
classification task according to the study by Joachims, 1998) is SVM, a discriminative
classifier. Therefore, in the rest of this work, we focus on incorporating generative prior
knowledge into the discriminative classification framework of support vector machines.

3. Preliminaries
It has been observed that constraints on the probability measure of a half-space can be
captured by second-order cone constraints for Gaussian distributions (see, e.g., the tutorial
by Lobo, Vandenberghe, Boyd, & Lebret, 1998). This allows for efficient processing of such
constraints within the framework of second-order cone programming (SOCP). We intend
to model prior knowledge with elliptical distributions, a family of probability distributions
which generalizes Gaussians. In what follows, we give a brief overview of second-order
cone programming and its relationship to constraints imposed on the Gaussian probability
distribution. We also note that it is possible to extend the argument presented by Lobo et
al. (1998) to elliptical distributions.
Second-order cone program is a mathematical program of the form:
min v T x

(3.1)

x

s.t. kAi x + bi k ≤ cTi x + di , i = 1, ..., N

(3.2)

where x ∈ Rn is the optimization variable and v ∈ Rn , Ai ∈ R(ki xn) , bi ∈ Rki , ci ∈ Rn ,
di ∈ R are problem parameters (k·k represents the usual L2 -norm in this paper). SOCPs
can be solved efficiently with interior-point methods, as described by Lobo et al. (1998) in
a tutorial which contains an excellent overview of the theory and applications of SOCP.
29

Epshteyn & DeJong

We use the elliptical distribution to model distribution of the data a-priori. Elliptical
distributions are distributions with ellipsoidally-shaped equiprobable contours. The density
function of the n-variate elliptical distribution has the form fµ,Σ,g (x) = c(det Σ)−1 g((x −
µ)T Σ−1 (x − µ)), where x ∈ Rn is the random variable, µ ∈ Rn is the location parameter,
Σ ∈ R(nxn) is a positive definite (n × n)-matrix representing the scale parameter, function
g(·) is the density generator, and c is the normalizing constant. We will use the notation X ∼ E(µ, Σ, g) to denote that the random variable X has an elliptical distribution
with parameters µ, Σ, g. Choosing appropriate density generator functions g, the Gaussian
distribution, the Student-t distribution, the Cauchy distribution, the Laplace distribution,
and the logistic distribution can be seen as special cases of the elliptical distribution. Using an elliptical distribution relaxes the restrictive assumptions the user has to make when
imposing a Gaussian prior, while keeping many desirable properties of Gaussians, such as:
1. If X ∼ E(µ, Σ, g), A ∈ R(k×n) , and B ∈ Rk , then AX + B ∼ E(Aµ + B, AΣAT , g)
2. If X ∼ E(µ, Σ, g), then E(X) = µ.
3. If X ∼ E(µ, Σ, g), then V ar(X) = αg Σ, where αg is a constant that depends on the
density generator g.
The following proposition shows that for elliptical distributions, the constraint P (w T x+b ≥
0) ≤ η (i.e., the probability that X takes values in the half-space {w T x + b ≥ 0} is less than
η) is equivalent to a second-order cone constraint for η ≤ 21 :
Proposition
3.1. If X ∼ E(µ, Σ, g), P rob(w T x + b ≥ 0) ≤ η ≤ 12 is equivalent to −(w T µ +
 1/2
b)/βg,η ≥ Σ w, where βg,η is a constant which only depends on g and η.
Proof. The proof is identical to the one given by Lobo (1998) and Lanckriet et al. (2001)
for Gaussian distributions and is provided here for completeness:
Assume P rob(w T x + b ≥ 0) ≤ η.

(3.3)

Let u = w T x+b. Let u denote the mean of u, and σ denote its variance. Then the constraint
3.3 can be written as
u−u
u
(3.4)
P rob( √ ≥ − √ ) ≤ η.
σ
σ

√ 
√
By the properties of elliptical distributions, u = w T µ + b, σ = αg Σ1/2 w, and u−u
∼
σ
T

)≤
E(0, 1, g). Thus, statement 3.4 above can be expressed as P robX∼E(0,1,g) (X ≥ − √αw Σµ+b
1/2 w
k
gk
T
η, which is equivalent to − √αw Σµ+b
≥ Φ−1 (η), where Φ(z) = P robX∼E(0,1,g) (X ≥ z). The
1/2 w
k
gk
√
proposition follows with βg,η = αg Φ−1 (η).

Proposition 3.2. For any monotonically decreasing g, P robX∼E(µ,Σ,g) (x) ≥ δ is equivalent


to Σ−1/2 (x − µ) ≤ ϕg,c,Σ , where ϕg,c,Σ,δ = g −1 ( δ|Σ|
c ) is a constant which only depends on
g, c, Σ, δ.
Proof. Follows directly from the definition of P robX∼E(µ,Σ,g) (x).
30

Generative Prior Knowledge for Discriminative Classification

4. Generative Prior via Bilevel Programming
We deal with the binary classification task: the classifier is a function f (x) which maps
instances x ∈ Rn to labels y ∈ {−1, 1}. In the generative setting, the probability densities
P rob(x|y = −1; µ1 ) and P rob(x|y = 1; µ2 ) parameterized by µ = [µ1 , µ2 ] are provided (or
estimated from the data), along with the prior probabilities on class labels Π(y = −1) and
Π(y = 1), and the Bayes optimal decision rule is given by the classifier
f (x|µ) = sign(P rob(x|y = −1; µ1 )Π(y = −1) − P rob(x|y = 1; µ2 )Π(y = 1)),
where sign(x) := 1 if x ≥ 0 and −1 otherwise. In LDA, for instance, the parameters µ 1 and
µ2 are the means of the two Gaussian distributions generating the data given each label.
Informally, our approach to incorporating prior knowledge is straightforward: we assume
a two-level hierarchical generative probability distribution model. The low-level probability
distribution of the data given the label P rob(x|y; µ) is parameterized by µ, which, in turn,
has a known probability distribution P rob0 (µ). The goal of the classifier is to estimate the
values of the parameter vector µ from the training set of labeled points [x 1 , y1 ]...[xm , ym ].
This estimation is performed as a two-player sequential game of full information. The
bottom (generative) player, given µ, selects the Bayes optimal decision rule f (x|µ). The
top (discriminative) player selects the value of µ which has a high probability of occurring
(according to P rob0 (µ)) and which will force the bottom player to select the decision rule
which minimizes the discriminative error on the training set. We now give a more formal
specification of this training problem and formulate it as a bilevel program. Some of the
assumptions are subsequently relaxed to enforce both tractability and flexibility.
We use an elliptical distribution E(µ1 , Σ1 , g) to model X|y = −1, and another elliptical
distribution E(µ2 , Σ2 , g) to model X|y = 1. If the parameters µi , Σi , i = 1, 2 are known,
the Bayes optimal decision rule restricted to the class of linear classifiers 2 of the form
fw,b (x) = sign(w T x + b) is given by f (x) which minimizes the probability of error among all
linear discriminants: P rob(error) = P rob(w T x + b ≥ 0|y = 1)Π(y = 1) + P rob(w T x + b ≤
0|y = −1)Π(y = −1) = 12 (P robX∼E(µ1 ,Σ1 ,g) (wT x + b ≥ 0) + P robX∼E(µ2 ,Σ2 ,g) (wT x + b ≤ 0)),
assuming equal prior probabilities for both classes. We now model the uncertainty in the
means of the elliptical distributions µi , i = 1, 2 by imposing elliptical prior distributions on
the locations of the means: µi ∼ E(ti , Ωi , g), i = 1, 2. In addition, to ensure the optimization
problem is well-defined, we maximize the margin of the hyperplane subject to the imposed
generative probability constraints:
min kwk

(4.1)

µ1 ,µ2

s.t.yi (wT xi + b) ≥ 1, i = 1, .., m

P robµi ∼E(ti ,Ωi ,g) (µi ) ≥ δ, i = 1, 2

(4.2)
(4.3)
T

T

[w, b] solves min[P robX∼E(µ1 ,Σ1 ,g) (w x + b ≥ 0) + P robX∼E(µ2 ,Σ2 ,g) (w x + b ≤ 0)]
w,b

(4.4)
This is a bilevel mathematical program (i.e., an optimization problem in which the
constraint region is implicitly defined by another optimization problem), which is strongly
2. A decision rule restricted to some class of classifiers H is optimal if its probability of error is no larger
than that of any other classifier in H (Tong & Koller, 2000a).

31

Epshteyn & DeJong

NP-hard even when all the constraints and both objectives are linear (Hansen, Jaumard,
& Savard, 1992). However, we show that it is possible to solve a reasonable approximation of this problem efficiently with several iterations of second-order cone programming.
First, we relax the second-level minimization (4.4) by breaking it up into two constraints:
P robX∼E(µ1 ,Σ1 ,g) (wT x + b ≥ 0) ≤ η and P robX∼E(µ2 ,Σ2 ,g) (wT x + b ≤ 0) ≤ η. Thus, instead of looking for the Bayes optimal decision boundary, the algorithm looks for a decision
boundary with low probability of error, where low error is quantified by the choice of η.
Propositions 3.1 and 3.2 enable us to rewrite the optimization problem resulting from
this relaxation as follows :
min kwk

(4.5)

µ1 ,µ2 ,w,b

s.t.yi (wT xi + b) ≥ 1, i = 1, .., m




 −1/2
(µi − ti ) ≤ ϕ, i = 1, 2
P robµi ∼E(ti ,Ωi ,g) (µi ) ≥ δ, i = 1, 2 ⇔ Ωi

w T µ1 + b

P robX∼E(µ1 ,Σ1 ,g) (wT x + b ≥ 0) ≤ η ⇔ − 
 1/2  ≥ β
Σ1 w 

w T µ2 + b

P robX∼E(µ2 ,Σ2 ,g) (wT x + b ≤ 0) ≤ η ⇔ 
 1/2  ≥ β
Σ2 w 

(4.6)
(4.7)
(4.8)

(4.9)

Notice that the form of this program does not depend on the generator function g of the
elliptical distribution - only constants β and ϕ depend on it. ϕ defines how far the system
is willing to deviate from the prior in its choice of a generative model, and β bounds the
tail probabilities of error (Type I and Type II) which the system will tolerate assuming its
chosen generative model is correct. These constants depend both on the specific generator
g and the amount of error the user is willing to tolerate. In our experiments, we select
the values of these constants to optimize performance. Unless the user wants to control
the probability bounds through these constants, it is sufficient to assume a-priori only that
probability distributions (both prior and hyper-prior) are elliptical, without making any
further commitments.
Our algorithm solves the above problem by repeating the following two steps:
1. Fix the top-level optimization parameters µ1 and µ2 . This step combines the objectives of maximizing the margin of the classifier on the training data and ensuring that
the decision boundary is (approximately) Bayes optimal with respect to the given
generative probability densities specified by the µ1 , µ2 .
2. Fix the bottom-level optimization parameters w, b. Expand the feasible region of the
program in step 1 as a function of µ1 , µ2 . This step fixes the decision boundary and
pushes the means of the generative distribution as far away from the boundary as the
constraint (4.7) will allow.
The steps are repeated until convergence (in practice, convergence is detected when the
optimization parameters do not change appreciably from one iteration to the next). Each
step of the algorithm can be formulated as a second-order cone program:
32

Generative Prior Knowledge for Discriminative Classification

Step 1. Fix µ1 and µ2 . Removing unnecessary constraints from the mathematical
program above and pushing the objective into constraints, we get the following SOCP:
minρ

(4.10)

w,b

s.t.ρ ≥ kwk

(4.11)

yi (wT xi + b) ≥ 1, i = 1, .., m

(4.12)

wT µ

1+b

− 
 1/2  ≥ β
Σ1 w 

(4.13)

w T µ2 + b


 1/2  ≥ β
Σ2 w 

(4.14)

Step 2. Fix w, b and expand the span of the feasible region, as measured by
T
‚w µ1 +b‚ .
‚ 1/2 ‚
‚Σ1 w‚

T
‚w µ2 +b‚
‚ 1/2 ‚
‚Σ2 w‚

−

Removing unnecessary constraints, we get:
w T µ2 + b w T µ1 + b
 − 

max 
 1/2 
µ1 ,µ2  1/2 
Σ2 w 
Σ 1 w 



 −1/2
(µi − ti ) ≤ ϕ, i = 1, 2
s.t. Ωi

(4.15)
(4.16)

The behavior of the algorithm is illustrated in Figure 4.1.
The following theorems state that the algorithm converges.
Theorem
4.1. Suppose
n
o∞ that the algorithm produces a sequence of iterates


(t) (t)
(t)
(t)
, and the quality of each iterate is evaluated by its margin w(t) .
µ1 , µ2 , w , b
t=0
This evaluation function converges.
(t)

(t)

(t)

(t)

Proof. Let µ1 , µ2 be the values of the prior location parameters, and w1 , b1 be the
minimum error hyperplane the algorithm finds at the end of the t-th step. At the end of
(t+1) (t+1)
the (t + 1)-st step, w1
, b1
is still in the feasible region of the t-th step SOCP. This
(t) )T µ

(t)
2 +b‚
1/2 (t) ‚
w
‚
‚Σ2

is true because the function f ( (w‚‚

(t) )T µ

(t)
1 +b‚
1/2 (t) ‚
w
‚
‚Σ1

, − (w‚‚

)=

(w‚(t) )T µ2 +b‚(t)
‚ 1/2 (t) ‚
‚Σ2 w ‚

−

(w‚(t) )T µ1 +b‚(t)
‚ 1/2 (t) ‚
‚Σ1 w ‚

is monotonically increasing in each one of its arguments when the other argument is fixed,
(t+1) (t+1)
and fixing µ1 (or µ2 ) fixes exactly one argument. If the solution µ1
, µ2
at the end
(t+1)

of the (t + 1)-st step were such that
(t+1)

fixing µ1

(t)

T
(t)
(w(t)
+b
‚
‚) µ2
‚ 1/2 (t) ‚
‚Σ2 w ‚

< β, then f could be increased by

(t)

and using the value of µ2 from the beginning of the step which ensures that

(t) )T µ
(t)
(w‚
2 +b
‚
‚ 1/2 (t) ‚
‚Σ2 w ‚

≥ β, which contradicts the observation that f is maximized at the end of
(t+1)

the second step. The same contradiction is reached if −

T
(t)
(w(t)
+b
‚
‚) µ1
‚ 1/2 (t) ‚
‚Σ1 w ‚

< β. Since the

minimum error hyperplane from the previous
iteration is in the feasible region at the start

(t)


must decrease monotonically from one iteration
of the next iteration, the objective w
to the next. Since it is bounded below by zero, the algorithm converges.
33

Epshteyn & DeJong

1)

2)

5

5

4

4

3

3

2

2

1

1

0

0

−1
−1

0

1

2

3

4

5

−1
−1

0

1

2

3

3)
5

4

4

3

3

2

2

1

1

0

0

0

5

4)

5

−1
−1

4

1

2

3

4

5

−1
−1

0

1

2

3

4

5

Figure 4.1: Steps of the iterative (hard-margin) SOCP procedure:
(The region where the hyperprior probability is larger than δ is shaded for each prior
distribution. The covariance matrices are represented by equiprobable elliptical contours.
In this example, the covariance matrices of the hyperprior and the prior distributions are
multiples of each other. Data points from two different classes are represented by diamonds
and squares.)
1. Data, prior, and hyperprior before the algorithm is executed.
2. Hyperplane discriminator at the end of step 1, iteration 1
3. Priors at the end of step 2, iteration 1
4. Hyperplane discriminator at the end of step 2, iteration 2
The algorithm converges at the end of step 2 for this problem (step 3 does not move the
hyperplane).
In addition to the convergence of the objective function, the accumulation points of the
sequence of iterates can be characterized by the following theorem:
n
o
(t) (t)
Theorem 4.2. The accumulation points of the sequence µ1 , µ2 , w(t) , b(t) (i.e., limiting
points of its convergent subsequences) have no feasible descent directions for the original
optimization problem given by (4.5)-(4.9).
Proof. See Appendix A.
34

Generative Prior Knowledge for Discriminative Classification

If a point has no feasible descent directions, then any sufficiently small step along any
directional vector will either increase the objective function, leave it unchanged, or take the
algorithm outside of the feasible region. The set of points with no feasible descent directions
is a subset of the set of local minima. Hence, convergence to such a point is a somewhat
weaker result than convergence to a local minimum.
In practice, we observed rapid convergence usually within 2-4 iterations.
Finally, we may want to relax the strict assumptions of the correctness of the prior/linear
separability of the data by introducing slack variables into the optimization problem above.
This results in the following program:
min

µ1 ,µ2 ,w,b,ξi ,ζ1 ,ζ2 ,ν1 ,ν2

kwk + C1

m
X

ξi + C2 (ζ1 + ζ2 ) + C3 (ν1 + ν2 )

(4.17)

i=1

s.t.yi (wT xi + b) ≥ 1 − ξi , i = 1, .., m


 −1/2

(µi − ti ) ≤ ϕ + νi , i = 1, 2
Ωi

w T µ1 + b 
 1/2 
≥ Σ 1 w  − ζ 1
−
β

T
w µ2 + b 
 1/2 
≥ Σ2 w  − ζ 2
β
ξi ≥ 0, i = 1, .., m
νi ≥ 0, i = 1, 2

ζi ≥ 0, i = 1, 2

(4.18)
(4.19)
(4.20)
(4.21)
(4.22)
(4.23)
(4.24)

As before, this problem can be solved with the two-step iterative SOCP procedure.
Imposing the generative prior with soft constraints ensures that, as the amount of training
data increases, the data overwhelms the prior and the algorithm converges to the maximummargin separating hyperplane.

5. Experiments
The experiments were designed both to demonstrate the usefulness of the proposed approach
for incorporation of generative prior into discriminative classification, and to address a
broader question by showing that it is possible to use an existing domain theory to aid in
a classification task for which it was not specifically designed. In order to construct the
generative prior, the generative LDA classifier was trained on the data from the training
classification task to estimate the Gaussian location parameters µbi , i = 1, 2, as described
in Section 2. The compression function χi (v) is subsequently computed (also as described
j
in Section 2), and is used to set the hyperprior parameters via µji := χi (li,test
), i = 1, 2.
In order to apply a domain theory effectively to the task for which it was not specifically
designed, the algorithm must be able to estimate its confidence in the decomposition of the
domain theory with respect to this new learning task. In order to model the uncertainty in
applicability of WordNet to newsgroup categorization, our system estimated its confidence in
homogeneity of equivalence classes of semantic distances by computing the variance of each
35

Epshteyn & DeJong

0.85
Bilevel Gen/Discr

0.8
0.75
0.7
0.65
0.6
0.55
0.5
0.5

0.55

0.6

0.65 0.7
SVM

0.75

0.8

0.85

Figure 5.1: Performance of the bilevel discriminative classifier constrained by generative
prior knowledge versus performance of SVM. Each point represents a unique
pair of training/test tasks, with 0.5% of the test task data used for training.
The results are averaged over 100 experiments.

P

random variable χi (v) as follows: σi (v) ,

j:l

c
(µji −χi (v))2
j
i,train=v
j
|j:li,tran
=v|

. The hyperprior confidence

matrices Ωi , i = 1, 2 were then reconstructed
with respect to the test task semantic distances

j
σi (li,test
), k = j
. Identity matrices were used as
li,test , i = 1, 2 as follows: [Ωi ]j,k :=
0, k 6= j
covariance matrices of the lower-level prior: Σ1 = Σ2 := I. The rest of the parameters
were set as follows: β := 0.2, ϕ := 0.01, C1 = C2 := 1, C3 := ∞. These constants were
chosen manually to optimize performance on Experiment 1 (for the training task: atheism
vs. guns, test task: guns vs. mideast, see Figure 5.2) without observing any data from any
other classification tasks.
The resulting classifier was evaluated in different experimental setups (with different
pairs of newsgroups chosen for the training and the test tasks) to justify the following
claims:
1. The bilevel generative/discriminative classifier with WordNet-derived prior knowledge has good low-sample performance, showing both the feasibility of automatically
interpreting the knowledge embedded in WordNet and the efficacy of the proposed
algorithm.
2. The bilevel classifier’s performance improves with increasing training sample size.
3. Integrating generative prior into the discriminative classification framework results
in better performance than integrating the same prior directly into the generative
framework via Bayes’ rule.
36

Generative Prior Knowledge for Discriminative Classification

4. The bilevel classifier outperforms a state-of-the-art discriminative multitask classifier
proposed by Evgeniou and Pontil (2004) by taking advantage of the WordNet domain
theory.
In order to evaluate the low-sample performance of the proposed classifier, four newsgroups
from the 20-newsgroup dataset were selected for experiments: atheism, guns, middle east,
and auto. Using these categories, thirty experimental setups were created for all the possible
ways of assigning newsgroups to training and test tasks (with a pair of newsgroups assigned
to each task, under the constraint that the training and test pairs cannot be identical) 3 . In
each experiment, we compared the following two classifiers:
1. Our bilevel generative-discriminative classifier with the knowledge transfer functions
χi (v), σi (v), i = 1, 2 learned from the labeled training data provided for the training task (using 90% of all the available data for that task). The resulting prior was
subsequently introduced into the discriminative classification framework via our approximate bilevel programming approach
2. A vanilla SVM classifier which minimizes the regularized empirical risk:
min

w,b,ξi

m
X
i=1

ξi + C1 kwk2

s.t.yi (wT xi + b) ≥ 1 − ξi , i = 1, .., m

(5.1)
(5.2)

Both classifiers were trained on 0.5% of all the available data from the test classification
task4 , and evaluated on the remaining 99.5% of the test task data. The results, averaged
over one hundred randomly selected datasets, are presented in Figure 5.1, which shows the
plot of the accuracy of the bilevel generative/discriminative classifier versus the accuracy
of the SVM classifier, evaluated in each of the thirty experimental setups. All the points
lie above the 45o line, indicating improvement in performance due to incorporation of prior
knowledge via the bilevel programming framework. The amount of improvement ranges
from 10% to 30%, with all of the improvements being statistically significant at the 5%
level.
The next experiment was conducted to evaluate the effect of increasing training data
(from the test task) on the performance of the system. For this experiment, we selected
three newsgroups (atheism, guns, and middle east) and generated six experimental setups
based on all the possible ways of splitting these newsgroups into unique training/test pairs.
In addition to the classifiers 1 and 2 above, the following classifiers were evaluated:
3. A state-of-the art multi-task classifier designed by Evgeniou and Pontil (2004). The
classifier learns a set of related classification functions ft (x) = wtT x + bt for classification tasks t ∈ {training task, test task} given m(t) data points [x1t , y1t ], .., [xm(t)t , ym(t)t ]
3. Newsgroup articles were preprocessed by removing words which could not be interpreted as nouns by
WordNet. This preprocessing ensured that only one part of WordNet domain theory was exercised and
resulted in virtually no reduction in classification accuracy.
4. SeDuMi software (Sturm, 1999) was used to solve the iterative SOCP programs.

37

Epshteyn & DeJong

for each task t by minimizing the regularized empirical risk:
min

w0 ,wt ,bt ,ξit

X
X m(t)
t

i=1

ξit +

C1 X
kwt − w0 k2 + C1 kw0 k2
C2 t

s.t. yit (wtT xit + bt ) ≥ 1 − ξit , i = 1, .., m(t), ∀t
ξit ≥ 0, i = 1, .., m(t), ∀t

(5.3)
(5.4)
(5.5)

The regularization constraint captures a tradeoff between final models w t being close
to the average model w0 and having a large margin on the training data. 90% of the
training task data was made available to the classifier. Constant C1 := 1 was chosen,
and C2 := 1000 was selected from the set {.1, .5, 1, 2, 10, 1000, 105 , 1010 } to optimize
the classifier’s performance on Experiment 1 (for the training task: atheism vs. guns,
test task: guns vs. mideast, see Figure 5.2) after observing .05% of the test task data
(in addition to the training task data).
4. The LDA classifier described in Section 2 trained on 90% of the test task data. Since
this classifier is the same as the bottom-level generative classifier used in the bilevel
algorithm, its performance gives an upper bound on the performance of the bottomlevel classifier trained in a generative fashion.
Figure 5.2 shows performance of classifiers 1-3 as a function of the size of the training
data from the test task (evaluation was done on the remaining test-task data). The results
are averaged over one hundred randomly selected datasets. The performance of the bilevel
classifier improves with increasing training data both because the discriminative portion of
the classifier aims to minimize the training error and because the generative prior is imposed
with soft constraints. As expected, the performance curves of the classifiers converge as the
amount of available training data increases. Even though the constants used in the mathematical program were selected in a single experimental setup, the classifier’s performance
is reasonable for a wide range of data sets across different experimental setups, with the
possible exception of Experiment 4 (training task: guns vs. mideast, testing task: atheism
vs. mideast), where the means of the constructed elliptical priors are much closer to each
other than in the other experiments. Thus, the prior is imposed with greater confidence
than is warranted, adversely affecting the classifier’s performance.
The multi-task classifier 3 outperforms the vanilla SVM by generalizing from data points
across classification tasks. However, it does not take advantage of prior knowledge, while our
classifier does. The gain in performance of the bilevel generative/discriminative classifier
is due to the fact that the relationship between the classification tasks is captured much
better by WordNet than by simple linear averaging of weight vectors.
Because of the constants involved in both the bilevel classifier and the generative classifiers with Bayesian priors, it is hard to do a fair comparison between classifiers constrained
by generative priors in these two frameworks. Instead, the generatively trained classifier 4
gives an empirical upper bound on the performance achievable by the bottom-level classifier
trained generatively on the test task data. The accuracy of this classifier is shown as as
a horizontal in the plots in Figure 5.2. Since discriminative classification is known to be
superior to generative classification for this problem, the SVM classifier outperforms the
38

Generative Prior Knowledge for Discriminative Classification

1) Train:atheism vs. guns

2) Train:atheism vs. guns

3) Train:guns vs. mideast

Test:atheism vs. mideast

Test:guns vs. mideast

Test:atheism vs. guns

1

1

1

0.95

0.95

0.95

0.9

0.9

0.9

0.85

0.85

0.85

0.8

0.8

0.8

0.75

0.75

0.75

0.7

0.7

0.7

0.65

0.65

0.65

0.6

0.6

0.6

0.55

0.55
0

0

10 20 30 40 50 60 70 80 90 100

4) Train: guns vs. mideast

10

20

30

40

50

60

70

80

90

5) Train: atheism vs. mideast

0.55
0

1

1

1

0.95

0.95

0.9

0.9

0.9

0.85

0.85

0.85

0.8

0.8

0.8

0.75

0.75

0.75

0.7

0.7

0.7

0.65

0.65

0.65

0.6

0.6
0

10

20

30

40

50

60

70

80

90

30

40

50

60

70

80

90

Test:atheism vs. guns

0.95

0.55

20

6) Train: atheism vs. mideast

Test:guns vs. mideast

Test:atheism vs. mideast

10

0.6

0.55

0.55
0

10 20 30 40 50 60 70 80 90 100

0

10

20

30

40

50

60

70

80

90

Legend:

LDA,max performance
Bilevel Gen/Discr
SVM
Multitask SVM

Figure 5.2: Test set accuracy as a percentage versus number of test task training points for
two classifiers (SVM and Bilevel Gen/Discr) tested on six different classification
tasks. For each classification experiment, the data set was split randomly into
training and test sets in 100 different ways. The error bars based on 95%
confidence intervals.

generative classifier given enough data in four out of six experimental setups. What is more
interesting, is that, for a range of training sample sizes, the bilevel classifier constrained
by the generative prior outperforms both the SVM trained on the same sample and the
generative classifier trained on a much larger sample in these four setups. This means that,
unless prior knowledge outweighs the effect of learning, it cannot enable the LDA classifier
to compete with our bilevel classifier on those problems.
Finally, a set of experiments was performed to determine the effect of varying mathematical program parameters β and ϕ on the generalization error. Each parameter was
varied over a set of values, with the rest of the parameters held fixed (β was increased up
to its maximum feasible value). The evaluation was done in the setup of Experiment 1 (for
39

Epshteyn & DeJong

1) Accuracy as a function of β

2) Accuracy as a function of ϕ

0.18

0.18

0.16

0.16

0.14

0.14

0.12

0.12

0.1

0.1

0.08

0.08

0.06

0.06

0.04

0.04

0.02

0.02

0

0
0

0.05

0.1

0.15

0.2

0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1

Figure 5.3: Plots of test set accuracy as percentage versus mathematical program parameter
values. For each classification task, a random training set of size 9 was chosen
from the full set of test task articles in 100 different ways. Error bars are based
on 95% confidence intervals. All the experiments were performed on the training
task: atheism vs. guns, test task: guns vs. mideast.

the training task:atheism vs. guns, test task: guns vs. mideast), with the training set size
of 9 points. The results are presented in Figure 5.3. Increasing the value of β is equivalent
to requiring a hyperplane separator to have smaller error given the prior. Decreasing the
value of ϕ is equivalent to increasing the confidence in the hyperprior. Both of these actions
tighten the constraints (i.e., decrease the feasible region). With good prior knowledge, this
should have the effect of improving generalization performance for small training samples
since the prior is imposed with higher confidence. This is precisely what we observe in the
plots of Figure 5.3.

6. Generalization Performance
Why does the algorithm generalize well for low sample sizes? In this section, we derive a
theorem which demonstrates that the convergence rate of the generalization error of the
constrained generative-discriminative classifier depends on the parameters of the mathematical program and not just the margin, as would be expected in the case of large-margin
classification without the prior. In particular, we show that as the certainty of the generative prior knowledge increases, the upper bound on the generalization error of the classifier
constrained by the prior decreases. By increasing certainty of the prior, we mean that
either the hyper-prior becomes more peaked (i.e., the confidence in the locations of the
prior means increases) or the desired upper bounds on the Type I and Type II probabilities
of error of the classifier decrease (i.e., the requirement that the lower-level discriminative
player choose the restricted Bayes-optimal hyperplane is more strictly enforced).
The argument proceeds by bounding the fat-shattering dimension of the classifier constrained by prior knowledge. The fat-shattering dimension of a large margin classifier is
given by the following definition (Taylor & Bartlett, 1998):
Definition 6.1. A set of points S = {x1 ...xm } is γ-shattered by a set of functions F
mapping from a domain X to R if there are real numbers r 1 , ..., rm such that, for each
b ∈ {−1, 1}m , there is a function fb in F with b(fb (xi ) − ri ) ≥ γ, i = 1..m. We say
40

Generative Prior Knowledge for Discriminative Classification

that r 1 , ..., rm witness the shattering. Then the fat-shattering dimension of F is a function
fatF (γ) that maps γ to the cardinality of the largest γ-shattered set S.
Specifically, we consider the class of functions
F = {x → w T x : kxk ≤ R, kwk = 1,

(6.1)





wT (−µ1 )
w T µ2
 −1/2


−1/2
 ≥ β, 



≥
β,
(µ
−
t
)
(µ
−
t
)
Ω
Ω
≤
ϕ,



1
1
2
2  ≤ ϕ}.
1
2
 1/2 
 1/2 
Σ 1 w 
Σ 2 w 

The following theorem bounds the fat-shattering dimension of our classifier:

Theorem 6.2. Let F be the class of a-priori constrained functions defined by (6.1), and
let λmin (P ) and λmax (P ) denote the minimum and maximum eigenvalues of matrix P ,
2
2
2 ))
, where
respectively. If a set of points S is γ-shattered by F , then |S| ≤ 4R (α γ(1−α
2
2

2

(Σ1 )β
(Σ1 )β
2 k −(λmax (Ω2 )ϕ)
, kt2ktk(λ
,
α = max(α1 , α2 ) with α1 = min( λmin
) and α2 = min( λmin
2
kµ2 k
kµ1 k
max (Ω2 )ϕ) +kt2 k)
kt1 k2 −(λmax (Ω1 )ϕ)2
),
kt1 k((λmax (Ω1 )ϕ)2 +kt1 k)

assuming that β ≥ 0, kti k ≥ kti − µi k, and αi ≥

√1 , i
2

= 1, 2.

Proof. See Appendix B.
We have the following corollary which follows directly from Taylor and Bartlett’s (1998)
Theorem 1.5 and bounds the classifier’s generalization error based on its fat-shattering
dimension:
Corollary 6.3. Let G be a class of real-valued functions. Then, with probability at least
1 − δ over m independently generated examples z, if a classifier h = sgn(g) ∈ sgn(G) has
2
margin at least γ on all the examples in z, then the error of h is no more than m
(d ∗
γ
8m
)log(32m)
+
log(
))
where
d
=
f
at
(
).
If
G
=
F
is
the
class
of
functions
log( 8em
G
G
d
δ
16
2

2

2

defined by (6.1), then dF ≤ 265R (4(αγ 2 (1−α ))) . If G = F 0 is the usual class of large margin
classifiers (without the prior), then the result in (Taylor & Bartlett, 1998) shows that d F 0 ≤
265R2
.
γ2
2

Notice that both bounds depend on R
. However, the bound of the classifier constrained
γ2
by the generative prior also depends on β and ϕ through the term 4(α 2 (1 − α2 )). In particular, as β increases, tightening the constraints, the bound decreases, ensuring, as expected,
quicker convergence of the generalization error. Similarly, decreasing ϕ also tightens the
constraints and decreases the upper bound on the generalization error. For α > √12 , the
factor 4(α2 (1 − α2 )) is less than 1 and the upper bound on the fat-shattering dimension dF
is tighter than the usual bound in the no-prior case on dF 0 .
Since β controls the amount of deviation of the decision boundary from the Bayesoptimal hyperplane and ϕ depends on the variance of the hyper-prior distribution, tightening
of these constraints corresponds to increasing our confidence in the prior. Note that a high
value β represents high level of user confidence in the generative elliptical model. Also
note that there are two ways of increasing the tightness of the hyperprior constraint (4.7)
- one is through the user-defined parameter ϕ, the other is through the automatically
estimated covariance matrices Ωi , i = 1, 2. These matrices estimate the extent to which the
41

Epshteyn & DeJong

equivalence classes defined by WordNet create an appropriate decomposition of the domain
theory for the newsgroup categorization task. Thus, tight constraint (4.7) represents both
high level of user confidence in the means of the generative classification model (estimated
from WordNet) and a good correspondence between the partition of the words imposed
by the semantic distance of WordNet and the elliptical generative model of the data. As
ϕ approaches zero and β approaches its highest feasible value, the solution of the bilevel
mathematical program reduces to the restricted Bayes optimal decision boundary computed
solely from the generative prior distributions, without using the data.
Hence, we have shown that, as the prior is imposed with increasing level of confidence
(which means that the elliptical generative model is deemed good, or the estimates of
its means are good, which in turn implies that the domain theory is well-suited for the
classification task at hand), the convergence rate of the generalization error of the classifier
increases. Intuitively, this is precisely the desired effect of increased confidence in the prior
since the benefit derived from the training data is outweighed by the benefit derived from
prior knowledge. For low data samples, this should result in improved accuracy assuming
the domain theory is good, which is what the plots in Figure 5.3 show.

7. Related Work
There are a number of approaches to combining generative and discriminative models. Several of these focus on deriving discriminative classifiers from generative distributions (Tong
& Koller, 2000a; Tipping, 2001) or on learning the parameters of generative classifiers via
discriminative training methods (Greiner & Zhou, 2002; Roos, Wettig, Grunwald, Myllymaki, & Tirri, 2005). The closest in spirit to our approach is the Maximum Entropy
Discrimination framework (Jebara, 2004; Jaakkola, Meila, & Jebara, 1999), which performs
discriminative estimation of parameters of a generative model, taking into account the constraints of fitting the data and respecting the prior. One important difference with our
framework is that, in estimating these parameters, maximum entropy discrimination minimizes the distance between the generative model and the prior, subject to satisfying the
discriminative constraint that the training data be classified correctly with a given margin.
Our framework, on the other hand, maximizes the margin on the training data subject to
the constraint that the generative model is not too far from the prior. This emphasis on
maximizing the margin allows us to derive a-priori bounds on the generalization error of
our classifier based on the confidence in the prior which are not (yet) available for the maximum entropy framework. Another difference is that our approach performs classification
via a single generative model, while maximum entropy discrimination averages over a set of
generative models weighted by their probabilities. This is similar to the distinction between
maximum-a-posteriori and Bayesian estimation and has repercussions for tractability. Maximum entropy discrimination, however, is more general than our framework in a sense of
allowing a richer set of behaviors based on different priors.
Ng et al. (2003, 2001) explore the relative advantages of discriminative and generative
classification and propose a hybrid approach which improves classification accuracy for
both low-sample and high-sample scenarios. Collins (2002) proposes to use the Viterbi
algorithm for HMMs for inferencing (which is based on generative assumptions), combined
with a discriminative learning algorithm for HMM parameter estimation. These research
42

Generative Prior Knowledge for Discriminative Classification

directions are orthogonal to our work since they do not explicitly consider the question of
integration of prior knowledge into the learning problem.
In the context of support vector classification, various forms of prior knowledge have
been explored. Scholkopf et al. (2002) demonstrate how to integrate prior knowledge about
invariance under transformations and importance of local structure into the kernel function.
Fung et al. (2002) use domain knowledge in form of labeled polyhedral sets to augment
the training data. Wu and Srihari (2004) allow domain experts to specify their confidence
in the example’s label, varying the effect of each example on the separating hyperplane
proportionately to its confidence. Epshteyn and DeJong (2005) explore the effects of rotational constraints on the normal of the separating hyperplane. Sun and DeJong (2005)
propose an algorithm which uses domain knowledge (such as WordNet) to identify relevant
features of examples and incorporate resulting information in form of soft constraints on
the hypothesis space of SVM classifier. Mangasarian et al. (2004) suggest the use of prior
knowledge for support vector regression. In all of these approaches, prior knowledge takes
the form of explicit constraints on the hypothesis space of the large-margin classifier. In this
work, the emphasis is on generating such constraints automatically from domain knowledge
interpreted in the generative setting. As we demonstrate with our WordNet application,
generative interpretation of background knowledge is very intuitive for natural language
processing problems.
Second-order cone constraints have been applied extensively to model probability constraints in robust convex optimization (Lobo et al., 1998; Bhattacharyya, Pannagadatta, &
Smola, 2004) and constraints on the distribution of the data in minimax machines (Lanckriet
et al., 2001; Huang, King, Lyu, & Chan, 2004). Our work, as far as we know, is the first one
which models prior knowledge with such constraints. The resulting optimization problem
and its connection with Bayes optimal classification is very different from the approaches
mentioned above.
Our work is also related to empirical Bayes estimation (Carlin & Louis, 2000). In empirical Bayes estimation, the hyper-prior parameters of the generative model are estimated
using statistical estimation methods (usually maximum likelihood or method of moments)
through the marginal distribution of the data, while our approach learns those parameters
discriminatively using the training data.

8. Conclusions and Future Work.
Since many sources of domain knowledge (such as WordNet) are readily available, we believe
that significant benefit can be achieved by developing algorithms for automatically applying
their information to new classification problems. In this paper, we argued that the generative paradigm for interpreting background knowledge is preferable to the discriminative
interpretation, and presented a novel algorithm which enables discriminative classifiers to
utilize generative prior knowledge. Our algorithm was evaluated in the context of a complete system which, faced with the newsgroup classification task, was able to estimate the
parameters needed to construct the generative prior from the domain theory, and use this
construction to achieve improved performance on new newsgroup classification tasks.
In this work, we restricted our hypothesis class to that of linear classifiers. Extending
the form of the prior distribution to distributions other than elliptical and/or looking for
43

Epshteyn & DeJong

Bayes-optimal classifiers restricted to a more expressive class than that of linear separators
may result in improvement in classification accuracy for non linearly-separable domains.
However, it is not obvious how to approximate this more expressive form of prior knowledge
with convex constraints. The kernel trick may be helpful in handling nonlinear problems,
assuming that it is possible to represent the optimization problem exclusively in terms of
dot products of the data points and constraints. This is an important issue which requires
further study.
We have demonstrated that interpreting domain theory in the generative setting is
intuitive and produces good empirical results. However, there are usually multiple ways
of interpreting a domain theory. In WordNet, for instance, semantic distance between
words is only one measure of information contained in the domain theory. Other, more
complicated, interpretations might, for example, take into account types of links on the
path between the words (hypernyms, synonyms, meronyms, etc.) and exploit commonsense observations about WordNet such as words that are closer to the category label
are more likely to be informative than words farther away. Comparing multiple ways of
constructing the generative prior from the domain theory and, ultimately, selecting one of
these interpretations automatically is a fruitful direction for further research.

Acknowledgments
The authors thank the anonymous reviewers for valuable suggestions on improving the paper. This material is based upon work supported in part by the National Science Foundation
under Award NSF IIS 04-13161 and in part by the Information Processing Technology Office of the Defense Advanced Research Projects Agency under award HR0011-05-1-0040.
Any opinions, findings, and conclusions or recommendations expressed in this publication
are those of the authors and do not necessarily reflect the views of the National Science
Foundation or the Defense Advanced Research Projects Agency.

Appendix A. Convergence of the Generative/Discriminative Algorithm
Let the map	H : Z → Z determine an algorithm that, given a point µ(0) , generates a se∞
quence µ(t) t=0 of iterates through the iteration µ(t+1) = H(µ(t) ). The iterative algorithm
(t)

(t)

in Section 4 generates a sequence of iterates µ(t) = [µ1 , µ2 ] ∈ Z by applying the following
map H:
H = H 2 ◦ H1 :
(A.1)
In step 1, H1 ([µ1 , µ2 ]) = arg

min

[w,b]∈U ([µ1 ,µ2 ])

kwk ,

with the set U ([µ1 , µ2 ]) defined by constraints:
T

(A.2)
(A.3)

yi (w xi + b) − 1 ≥ 0, i = 1, .., m

(A.4)

c1 (w, b; µ2 , Σ2 ) − β ≥ 0


wT µ+b
.
with the conic constraints cs (w, b; µ, Σ) , s
kΣ1/2 wk

(A.6)

c−1 (w, b; µ1 , Σ1 ) − β ≥ 0

44

(A.5)

Generative Prior Knowledge for Discriminative Classification

In step 2, H2 (w, b) = arg

min

(µ1 ,µ2 )∈V

−(c−1 (w, b; µ1 , Σ1 ) + c1 (w, b; µ2 , Σ2 ))

(A.7)

with the set V given by the constraints
ϕ − o(µ1 ; Ω1 , t1 ) ≥ 0

(A.8)

ϕ − o(µ2 ; Ω2 , t2 ) ≥ 0

(A.9)



with o(µ; Ω, t) , Ω−1/2 (µ − t).
Notice that H1 and H2 are functions because the minima for optimization problems
(4.10)-(4.14) and (4.15)-(4.16) are unique. This is the case because Step 1 optimizes a
strictly convex function on a convex set, and Step 2 optimizes a linear non-constant function
on a strictly convex set.
Convergence of the objective function ψ(µ(t) ) , min[w,b]∈U ([µ(t) ,µ(t) ]) kwk of the algorithm
1
2
was shown in Theorem 4.1. Let Γ denote the set of points on which the map H does not
change the value of the objective function, i.e. µ∗ ∈ Γ ⇔ ψ(H(µ∗ )) = ψ(µ∗ ). We will
show that every accumulation point of {µ(t) } lies in Γ. We will also show that every point
[µ∗1 , µ∗2 ] ∈ Γ augmented with [w ∗ , b∗ ] = H1 ([µ∗1 , µ∗2 ]) is a point with no feasible descent
directions for the optimization problem (4.5)-(4.9), which can be equivalently expressed as:
min kwk s.t.[µ1 , µ2 ] ∈ V ; [w, b] ∈ U ([µ1 , µ2 ])

µ1 ,µ2 ,w,b

(A.10)

In order to formally state our result, we need a few concepts from the duality theory.
Let a constrained optimization problem be given by
min f (x) s.t. ci (x) ≥ 0, i = 1, .., k
x

(A.11)

The following conditions, known as Karush-Kuhn-Tucker(KKT) conditions are necessary
for x∗ to be a local minimum:
Proposition A.1. If x∗ is a local minimum of (A.11), then ∃λ1 , .., λk such that
P
1. ∇f (x∗ ) = ki=1 λi ∇ci (x∗ )
2. λi ≥ 0 for ∀i ∈ {1, .., k}

3. ci (x∗ ) ≥ 0 for ∀i ∈ {1, .., k}
4. λi ci (x∗ ) = 0 for ∀i ∈ {1, .., k}
λ1 , .., λk are known as Lagrange multipliers of constraints c1 , .., ck .
The following well-known result states that KKT conditions are sufficient for x ∗ to be
a point with no feasible descent directions:
Proposition A.2. If ∃λ1 , .., λk such that the following conditions are satisfied at x∗ :
P
1. ∇f (x∗ ) = ki=1 λi ∇ci (x∗ )
45

Epshteyn & DeJong

2. λi ≥ 0 for ∀i ∈ {1, .., k}
then x∗ has no feasible descent directions in the problem (A.11)
Proof. (sketch) We reproduce the proof given in a textbook by Fletcher (1987). The proposition is true because for any P
feasible direction vector s, sT ∇ci (x) ≥ 0 for ∀x and for ∀i ∈
T
∗
{1, .., k}. Hence, s ∇f (x ) = ki=1 λi sT ∇ci (x∗ ) ≥ 0, so s is not a descent direction.
The following lemma characterizes the points in the set Γ:

Lemma A.3. Let µ∗ ∈ Γ, and let [w ∗ , b∗ ] = H1 (µ∗ ) be the optimizer of ψ(µ∗ ), and let
λ∗ = [λ∗(A.4),1 , .., λ∗(A.4),m , λ∗(A.5) , λ∗(A.6) ] be a set of Lagrange multipliers corresponding to the
constraints for the solution [w ∗ , b∗ ]. Define µ0 = H(µ∗ ), and let [w 0 , b0 ] be the optimizer of
ψ(µ0 ). If µ02 6= µ∗2 , then λ∗(A.6) = 0 for some λ∗ . If µ01 6= µ∗1 , then λ∗(A.5) = 0 for some λ∗ .
If both µ01 6= µ∗1 and µ02 6= µ∗2 , then λ∗(A.6) = λ∗(A.5) = 0 for some λ∗ .
Proof. Consider the case when
µ02 6= µ∗2

(A.12)

µ01 = µ∗1

(A.13)

and
Since µ∗ ∈ Γ, kw0 k = kw ∗ k. Let λ0 be a set of Lagrange multipliers corresponding to the
constraints for the solution [w 0 , b0 ]. Since w∗ is still feasible for the optimization problem
given by ψ(µ0 ) (by the argument in Theorem 4.1) and the minimum of this problem is
unique, this can only happen if
[w0 , b0 ] = [w∗ , b∗ ].
(A.14)
Then [w∗ , b∗ ] and λ0 must satisfy KKT conditions for ψ(µ0 ). (A.12) implies that
c1 (w∗ ; µ02 , Σ2 ) > c1 (w∗ ; µ∗2 , Σ2 ) ≥ β by the same argument as in Theorem 4.1, which means
that, by KKT condition (4) for ψ(µ0 ),
λ0(A.6) = 0.

(A.15)

Therefore, by KKT condition (1) for ψ(µ0 ) and (A.15), at [w, b, µ1 , µ2 ] = [w∗ = w0 , b∗ =
b0 , µ∗1 = µ01 , µ∗2 ]
"
"
"
#
#
#


m
∂c−1 (w,b∗ ;µ∗1 ,Σ1 )
∂c1 (w,b∗ ;µ∗2 ,Σ2 )
∂kwk
X
y
x
i
i
∂w
+ λ0(A.5) ∂c−1 (w∂w
=
λ0(A.4),i
+ λ0(A.6) ∂c1 (w∗∂w
,
∗ ,b;µ∗ ,Σ )
∂kwk
,b;µ∗2 ,Σ2 )
1
1
yi
∂b

∂b

i=1

∂b

which means that KKT conditions (1),(2) for the optimization problem ψ(µ ∗ ) are satisfied
0
at the point [w ∗ , b∗ ] with λ∗ = λ . KKT condition (3) is satisfied by feasibility of [w ∗ , b∗ ]
and KKT condition (4) is satisfied by the same condition for ψ(µ0 ) and observations (A.13),
(A.14), and (A.15).
The proofs for the other two cases (µ02 = µ∗2 , µ01 6= µ∗1 and µ02 6= µ∗2 , µ01 6= µ∗1 ) are
analogous.
The following theorem states that the points in Γ are KKT points (i.e., points at which
KKT conditions are satisfied) for the optimization problem given by (A.10).
46

Generative Prior Knowledge for Discriminative Classification

Theorem A.4. If µ∗ ∈ Γ and let [w ∗ , b∗ ] = H1 (µ∗ ), then [w ∗ , b∗ , µ∗1 , µ∗2 ] is a KKT point for
the optimization problem given by (A.10).
Proof. Let µ0 = H(µ∗ ). Just like in Lemma A.3, we only consider the case
µ02 6= µ∗2 ,

(A.16)

µ01 = µ∗1 ⇒ λ∗(A.6) = 0 (by Lemma A.3).

(A.17)

(the proofs for the other two cases are similar).
By KKT conditions for H2 (w∗ , b∗ ), at µ1 = µ01
−

∂(−o(µ1 ; Ω1 , t))
∂c−1 (w∗ , b∗ ; µ1 , Σ1 )
= λ0A.8
for some λ0A.8 ≥ 0.
∂µ1
∂µ1

(A.18)

By KKT conditions for H1 (µ∗ ) and (A.17), at [w, b] = [w ∗ , b∗ ]
"

∂kwk
∂w
∂kwk
∂b

#

=

m
X
i=1

λ∗(A.4),i



yi x i
yi



+

λ∗(A.5)

"

∂c−1 (w,b∗ ;µ∗1 ,Σ1 )
∂w
∂c−1 (w∗ ,b;µ∗1 ,Σ1 )
∂b

#





for some 


λ∗(A.4),1
..
λ∗(A.4),m
λ∗(A.5)





  0.


(A.19)

By (A.16),(A.17),(A.18), and (A.19), at [w, b, µ1 , µ2 ] = [w∗ , b∗ , µ∗1 = µ01 , µ∗2 ]


 ∂c−1 (w,b∗ ;µ∗ ,Σ1 ) 
∂kwk


 ∂kwk 
1
y
x
∂w
i
i
∂w
 ∂kwk 
∂w
∗ ,b;µ∗ ,Σ ) 
m

∂c
(w
−1
1
X
1
 yi 
 ∂b   0 


∗
 + λ∗

=
 ∂kwk  = 
∂b∗
λ
+

∗
(A.5)
(A.4),i  0 
  0 

∂c−1 (w ,b ;µ1 ,Σ1 ) 


 ∂µ1 
i=1
∂µ1
∂kwk
0
0
0
∂µ2


∗
∗




∂c1 (w,b ;µ2 ,Σ2 )
0
0
∂w ∗
∗
 ∂c1 (w ,b;µ2 ,Σ2 ) 




0
0


 + λ∗
,
∂b
λ0A.8 λ∗(A.5) 
 + λ∗(A.6) 
∂(−o(µ
;Ω
,t))
1 1
(A.6) 




0


0
∂µ1
∂(−o(µ2 ;Ω2 ,t))
∂c1 (w∗ ,b∗ ;µ2 ,Σ2 )
0
∂µ2
∂µ
2

which means that KKT conditions (1),(2) for the optimization problem (A.10) are satisfied
00
at the point [w ∗ , b∗ , µ∗1 , µ∗2 ] with λ = [λ∗(A.4),1 , .., λ∗(A.4),m , λ∗(A.5) , λ∗(A.6) , λ0A.8 λ∗(A.5) , λ∗(A.6) ].
00

λ also satisfies KKT conditions (3),(4) by assumption (A.17) and the KKT conditions for
H1 and H2 .
In order to prove convergence properties of the iterates µ(t) , we use the following theorem
due to Zangwill (1969):
Theorem A.5. Let the map H : Z → Z determine an iterative algorithm via µ(t+1) =
H(µ(t) ), let ψ(µ) denote the objective function, and let Γ be the set of points on which the
map H does not change the value of the objective function, i.e. µ ∈ Γ ⇔ ψ(H(µ)) = ψ(µ).
Suppose
47

Epshteyn & DeJong

1. H is uniformly compact on Z, i.e. there is a compact subset Z0 ⊆ Z such that
H(µ) ∈ Z0 for ∀µ ∈ Z.
2. H is strictly monotonic on Z − Γ, i.e. ψ(H(µ)) < ψ(µ).
3. H is closed on Z − Γ, i.e. if wi → w and H(wi ) → ξ, then ξ = H(w).
Then the accumulation points of the sequence of µ(t) lie in Γ.
The following proposition shows that minimization of a continuous function on a feasible
set which is a continuous map of the function’s argument forms a closed function.
Proposition A.6. Given
1. a real-valued continuous function f on A × B,
2. a point-to-set map U : A → 2B continuous with respect to the Hausdorff metric:5
dist(X, Y ) , max(d(X, Y ), d(Y, X)), where d(X, Y ) , maxx∈X miny∈Y kx − yk,
define the function F : A → B by
F (a) = arg min f (a, b0 ) = {b : f (a, b) < f (a, b0 ) for ∀b0 ∈ U (a)},
b0 ∈U (a)

assuming the minimum exists and is unique. Then, the function F is closed at a.
Proof. This proof is a minor modification of the one given by Gunawardana and Byrne
(2005). Let {a(t) } be a sequence in A such that
a(t) → a, F (a(t) ) → b

(A.20)

The function F is closed at a if F (a) = b. Suppose this is not the case, i.e. b 6= F (a) =
arg minb0 ∈U (a) f (a, b0 ). Therefore,
∃b̂ = arg min f (b0 ) such that f (a, b) > f (a, b̂)
b0 ∈U (a)

(A.21)

By continuity of f (·, ·) and (A.20),
f (a(t) , F (a(t) )) → f (a, b)

(A.22)

By continuity of U (·) and (A.20),
dist(U (a(t) ), U (a)) → 0 ⇒ ∃b̂(t) → b̂ and b̂(t) ∈ U (at ), for ∀t.

(A.23)

(A.22), (A.23), and (A.21) imply that
∃K such that f (a(t) , F (a(t) )) > f (a(t) , b̂(t) ), for ∀t > K

(A.24)

which is a contradiction since by assumption, F (a(t) ) = arg minb0 ∈U (at ) f (b0 ) and by (A.24),
b̂(t) ∈ U (a(t) ).
5. A point-to-set map U (a) maps a point a to a set of points. U (a) is continuous with respect to a distance
metric dist iff a(t) → a implies dist(U (a(t) ), U (a)) → 0.

48

Generative Prior Knowledge for Discriminative Classification

Proposition A.7. The function H defined by (A.1)-(A.7) is closed.
Proof. Let {µ(t) } be a sequence such that µ(t) → µ∗ . Since all the iterates µ(t) lie in
the closed feasible region bounded by constraints (4.6)-(4.9) and the boundary of U (µ) is
piecewise linear in µ, the boundary of U (µ(t) ) converges uniformly to the boundary of U (µ∗ )
as µ(t) → µ∗ , which implies that the Hausdorff distance between the boundaries converges
to zero. Since the Hausdorff distance between convex sets is equal to the Hausdorff distance
between their boundaries, dist(U (µ(t) ), U (µ∗ )) also converges to zero. Hence, proposition
A.6 implies that H1 is closed. The same proposition implies that H2 is closed. A composition
of closed functions is closed, hence H is closed.
We now prove the main result of this Section:
Theorem 4.2. Let H be the function defined by (A.1)-(A.7) which determines the generative/discriminative algorithm via µ(t+1) = H(µ(t) ). Then accumulation points µ∗ of the
sequence µ(t) augmented with [w ∗ , b∗ ] = H1 (µ∗ ) have no feasible descent directions for the
original optimization problem given by (4.5)-(4.9).
Proof. The proof is by verifying that H satisfies the properties of Theorem A.5. Closedness
of H was shown in Proposition A.7. Strict monotonicity of ψ(µ(t) ) was shown in Theorem
4.1. Since all the iterates µ(t) are in the closed feasible region bounded by constraints (4.6)(4.9), H is uniformly compact on Z. Since all the accumulation points µ∗ lie in Γ, they are
KKT points of the original optimization problem by Theorem A.4, and, therefore, have no
feasible descent directions by Proposition A.2.

Appendix B. Generalization of the Generative/Discriminative Classifier
We need a few auxiliary results before proving Theorem 6.2. The first proposition bounds
the angle of rotation between two vectors w1 , w2 and the distance between them if the angle
of rotation between each of these vectors and some reference vector v is sufficiently small:
Proposition B.1. Let kw1 k = kw2 k = kvk = 1. If w1T v ≥ α ≥ 0 and w2T v ≥ α ≥ 0, then
1. w1T w2 ≥ 2α2 − 1
p
2. kw1 − w2 k ≤ 2 (1 − α2 )

Proof.

1. By the triangle inequality, arccos(w1T w2 ) ≤ arccos(w1T v) + arccos(w2T v) ≤ 2 arccos(α)
(since the angle between two vectors is a distance measure). Taking cosines of both
sides and using trigonometric equalities yields w1T w2 ≥ 2α2 − 1.
2. Expand kw1 − w2 k2 = kw1 k2 + kw2 k2 − 2w1T w2 = 2(1 − w1T w2 ). Since w1T w2 ≥ 2α2 − 1
from part 1, kw1 − w2 k2 ≤ 4(1 − α2 ).

The next proposition bounds the angle of rotation between two vectors t and µ if they
are not too far away from each other as measured by the L2 -norm distance:
49

Epshteyn & DeJong

Proposition B.2. Let ktk = ν, kµ − tk ≤ τ . Then

tT µ
ktkkµk

≥

ν 2 −τ 2
ν(ν+τ ) .

Proof. Expanding kµ − tk2 = ktk2 + kµk2 − 2tt µ and using kµ − tk2 ≤ τ 2 , we get
1 ktk
2 ( kµk

kµk
ktk

τ2

tT µ
ktkkµk

≥

+
− ktkkµk ). We now use the triangle inequality ν − τ ≤ ktk − kµ − tk ≤ kµk ≤
ktk + kµ − tk ≤ ν + τ and simplify.

The following proposition will be used to bound the angle of rotation between the normal
w of the separating hyperplane and the mean vector t of the hyper-prior distribution:
wT µ
kwkkµk
ktk2 −ϕ2
ktk(ϕ+ktk) ).

Proposition B.3. Let
where α = min(β,

≥ β ≥ 0 and kµ − tk ≤ ϕ ≤ ktk. Then

wT t
kwkktk

≥ (2α2 − 1),

Proof. Follows directly from Propositions B.1 (part 1) and B.2.
We now prove Theorem 6.2, which relies on parts of the well-known proof of the fatshattering dimension bound for large margin classifiers derived by Taylor and Bartlett
(1998).
Theorem 6.2. Let F be the class of a-priori constrained functions defined by 6.1, and
let λmin (P ) and λmax (P ) denote the minimum and maximum eigenvalues of matrix P ,
2
2
2 ))
respectively. If a set of points S is γ-shattered by F , then |S| ≤ 4R (α γ(1−α
, where
2
2

2

(Σ1 )β
(Σ1 )β
2 k −(λmax (Ω2 )ϕ)
, kt2ktk(λ
,
) and α2 = min( λmin
α = max(α1 , α2 ) with α1 = min( λmin
2
kµ2 k
kµ1 k
max (Ω2 )ϕ) +kt2 k)
kt1 k2 −(λmax (Ω1 )ϕ)2
),
kt1 k((λmax (Ω1 )ϕ)2 +kt1 k)

assuming that β ≥ 0, kti k ≥ kti − µi k, and αi ≥

√1 , i
2

= 1, 2.



Proof. First, we use the inequality λmin (P ) kwk ≤ P 1/2 w ≤ λmax (P ) kwk to relax the
constraints
w T µ2
w T µ2
 ≥β⇒

≥ λmin (Σ2 )β
(B.1)
 1/2 
kwk
Σ 2 w 


ϕ
 −1/2

= ϕλmax (Ω2 ).
(B.2)
Ω2 (µ2 − t2 ) ≤ ϕ ⇒ kµ2 − t2 k ≤
λmin (Ω−1
2 )



 −1/2
−wT µ1‚
‚
The constraints imposed by the second prior ‚ 1/2 ‚ ≥ β, Ω1 (µ1 − t1 ) ≤ ϕ are relaxed

in a similar fashion to produce:

‚Σ2 w‚

wT (−µ1 )
≥ λmin (Σ1 )β
kwk

(B.3)

kµ1 − t1 k ≤ ϕλmax (Ω1 )

(B.4)

Now, we show that if the assumptions made in the statement of the theorem hold, then
2
2
2
P
P
every subset So ⊆ S satisfies k So − (S − S0 )k ≤ 4R (αγ 2(1−α ) .
Assume that S is γ-shattered by F . The argument used by Taylor and Bartlett (1998)
in Lemma 1.2 shows that, by the definition of fat-shattering, there exists a vector w 1 such
that
X
X
w1 (
So −
(S − S0 )) ≥ |S| γ.
(B.5)
50

Generative Prior Knowledge for Discriminative Classification

Similarly (reversing the labeling of S0 and S1 − S0 ), there exists a vector w2 such that
X
X
w2 ( (S − S0 ) −
So ) ≥ |S| γ.
(B.6)
Hence, (w1 − w2 )(
implies that

P

So −

P

(S − S0 )) ≥ 2 |S| γ, which, by Cauchy-Schwartz inequality,

2 |S| γ
P
kw1 − w2 k ≥ P
k So − (S − S0 )k

(B.7)

The constraints on the classifier represented in B.1 and B.2 imply by Proposition B.3 that
w1T t2
w2T t2
2
2
kw1 kkt2 k ≥ (2α1 − 1) and kw2 kkt2 k ≥ (2α2 − 1) . Now, applying Proposition B.1 (part 2) and
simplifying, we get
q
kw1 − w2 k ≤ 4

α12 (1 − α12 ).

Applying the same analysis to the constraints B.3 and B.4, we get
q
kw1 − w2 k ≤ 4 α22 (1 − α22 ).
Combining B.7, B.8, and B.9, we get
X

X
|S| γ


So −
(S − S0 ) ≥ p

2 α2 (1 − α2 )

(B.8)

(B.9)

(B.10)

with α as defined in the statement of the theorem.
Taylor and Bartlett’s (1998) Lemma 1.3 proves, using the probabilistic method, that
some So ⊆ S satisfies
X
 p
X


(B.11)
So −
(S − S0 ) ≤ |S|R.

Combining B.10 and B.11 yields |S| ≤

4R2 (α2 (1−α2 ))
.
γ2

References
Baxter, J. (2000). A model of inductive bias learning. Journal of Artificial Intelligence
Research, 12, 149–198.
Bhattacharyya, C., Pannagadatta, K. S., & Smola, A. (2004). A second order cone programming formulation for classifying missing data. In NIPS.
Blake,
C.,
&
Merz,
C.
(1998).
20
newsgroups
http://people.csail.mit.edu/people/jrennie/20newsgroups/..

database,

Campbell, C., Cristianini, N., & Smola, A. (2000). Query learning with large margin classifiers. In Proceedings of The Seventeenth International Conference on Machine Learning.
Carlin, B., & Louis, T. (2000). Bayes and Empirical Bayes Methods for Data Analysis.
Chapman and Hall.
Collins, M. (2002). Discriminative training methods for hidden markov models: Theory
and experiments with perceptron algorithms. In Proceedings of 2002 Conference on
Empirical Methods in Natural Language Processing.
51

Epshteyn & DeJong

Duda, R., Hart, P., & Stork, D. (2001). Pattern Classification. John Wiley. 2nd edition.
Epshteyn, A., & DeJong, G. (2005). Rotational prior knowledge for svms. In Proceedings
of the Sixteenth European Conference on Machine Learning.
Evgeniou, T., & Pontil, M. (2004). Regularized multi-task learning. In Proceedings of the
Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining.
Fink, M. (2004). Object classification from a single example utilizing class relevance metrics.
In Advances in Neural Information Processing Systems.
Fletcher, R. (1987). Practical Methods of Optimization. John Wiley and Sons, West Sussex,
England.
Fung, G., Mangasarian, O., & Shavlik, J. (2002). Knowledge-based support vector machine
classifiers. In Advances in Neural Information Processing Systems.
Greiner, R., & Zhou, W. (2002). Structural extension to logistic regression: Discriminative
parameter learning of belief net classifiers. In Proceedings of the Eighteenth National
Conference on Artificial Intelligence.
Gunawardana, A., & Byrne, W. (2005). Convergence theorems for generalized alternating
minimization procedures. Journal of Machine Learning Research, 6, 2049–2073.
Hansen, P., Jaumard, B., & Savard, G. (1992). New branch-and-bound rules for linear bilevel
programming. SIAM Journal on Scientific and Statistical Computing, 13, 1194–1217.
Huang, K., King, I., Lyu, M. R., & Chan, L. (2004). The minimum error minimax probability
machine. Journal of Machine Learning Research, 5, 1253–1286.
Jaakkola, T., Meila, M., & Jebara, T. (1999). Maximum entropy discrimination. In Advances
in Neural Information Processing Systems.
Jebara, T. (2004). Machine Learning: Discriminative and Generative. Kluwer Academic
Publishers.
Joachims, T. (1998). Text categorization with support vector machines: learning with many
relevant features. In Proceedings of the Tenth European Conference on Machine Learning.
Lanckriet, G. R. G., Ghaoui, L. E., Bhattacharyya, C., & Jordan, M. I. (2001). Minimax
probability machine. In Advances in Neural Information Processing Systems.
Lobo, M. S., Vandenberghe, L., Boyd, S., & Lebret, H. (1998). Applications of second-order
cone programming. Linear Algebra and its Applications, 284 (1–3), 193–228.
Mangasarian, O., Shavlik, J., & Wild, E. (2004). Knowledge-based kernel approximation.
Journal of Machine Learning Research.
Miller, G. (1990). WordNet: an online lexical database. International Journal of Lexicography, 3 (4).
Ng, A. Y., & Jordan, M. I. (2001). On discriminative vs. generative classifiers: A comparison
of logistic regression and naive bayes. In Advances in Neural Information Processing
Systems.
52

Generative Prior Knowledge for Discriminative Classification

Raina, R., Shen, Y., Ng, A. Y., & McCallum, A. (2003). Classification with hybrid generative/discriminative models. In Advances in Neural Information Processing Systems.
Roos, T., Wettig, H., Grunwald, P., Myllymaki, P., & Tirri, H. (2005). On discriminative
bayesian network classifiers and logistic regression. Machine Learning, 59, 267–296.
Scholkopf, B., Simard, P., Vapnik, V., & Smola, A. (2002). Prior knowledge in support
vector kernels. Advances in kernel methods - support vector learning.
Sturm, J. F. (1999). Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones. Optimization Methods and Software, 11, 625–653.
Sun, Q., & DeJong, G. (2005). Explanation-augmented svm: an approach to incorporating
domain knowledge into svm learning. In Proceedings of The Twenty Second International Conference on Machine Learning.
Taylor, J. S., & Bartlett, P. (1998). Generalization performance of support vector machines
and other pattern classifiers. In Advances in kernel methods: support vector learning.
Thrun, S. (1995). Is learning the n-th thing any easier than learning the first?. In Advances
in Neural Information Processing Systems.
Tipping, M. E. (2001). Sparse bayesian learning and the relevance vector machine. Journal
of Machine Learning Research, 1, 211–244.
Tong, S., & Koller, D. (2000a). Restricted bayes optimal classifiers. In Proceedings of the
Seventeenth National Conference on Artificial Intelligence.
Tong, S., & Koller, D. (2000b). Support vector machine active learning with applications
to text classification. In Proceedings of The Seventeenth International Conference on
Machine Learning.
Vapnik, V. (1995). The Nature of Statistical Learning Theory. Springer-Verlag.
Wu, X., & Srihari, R. (2004). Incorporating prior knowledge with weighted margin support
vector machines. In Proceedings of the Tenth ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining.
Zangwill, W. (1969). Convergence conditions for nonlinear programming algorithms. Management Science, 16, 1–13.

53

