Journal of Artificial Intelligence Research 25 (2006) 503–527

Submitted 6/04; published 4/06

Fault Tolerant Boolean Satisfiability
Amitabha Roy

aroy@cs.bc.edu

Computer Science Department,
Boston College, Chestnut Hill, MA 02467.

Abstract
A δ-model is a satisfying assignment of a Boolean formula for which any small alteration,
such as a single bit flip, can be repaired by flips to some small number of other bits, yielding
a new satisfying assignment. These satisfying assignments represent robust solutions to
optimization problems (e.g., scheduling) where it is possible to recover from unforeseen
events (e.g., a resource becoming unavailable). The concept of δ-models was introduced by
Ginsberg, Parkes, and Roy (1998), where it was proved that finding δ-models for general
Boolean formulas is NP-complete. In this paper, we extend that result by studying the
complexity of finding δ-models for classes of Boolean formulas which are known to have
polynomial time satisfiability solvers. In particular, we examine 2-SAT, Horn-SAT, AffineSAT, dual-Horn-SAT, 0-valid and 1-valid SAT. We see a wide variation in the complexity of
finding δ-models, e.g., while 2-SAT and Affine-SAT have polynomial time tests for δ-models,
testing whether a Horn-SAT formula has one is NP-complete.

1. Introduction
An important problem in the artificial intelligence community concerns the allocation of
resources at or near the minimal cost. An optimal solution to such a problem might be
rendered infeasible due to some unforeseen event (for example, a resource becoming unavailable or a task exceeding its allocated deadline). Hence, the motivation is to search
for optimal solutions which are immune from such events. In this paper, we consider the
complexity of finding such “robust” solutions, where we only allow for a fixed small number
of bad events, with the added condition that such bad events can be rectified by making a
small change to the solution. These solutions, which we call δ-models, were introduced by
Ginsberg et al. (1998), and further explored in Bailleux and Marquis (1999). This approach
to fault tolerance has been extended to constraint-satisfaction problems (CSPs) (Hebrard,
Hnich, & Walsh, 2004b, 2004a) and to applications in combinatorial auctions (Holland &
O’Sullivan, 2004). Hoos and O’Neill (2000) consider this approach to robustness in the
framework of dynamic satisfiability (which they call DynSAT) where the goal is to be able
to revise optimal solutions under a constantly changing input problem.
We extend the initial complexity results in Ginsberg et al. (1998) by looking at the
theoretical complexity of tractable instances of satisfiability (SAT) identified by Schaefer’s
dichotomy theorem (Schaefer, 1978). The dichotomy theorem proves that the polynomial
time solvable instances of SAT are 2-SAT, Horn-SAT, dual-Horn-SAT, Affine-SAT, 0-valid
SAT and 1-valid SAT and any other form is NP-complete. Our goal is to study the complexity of finding δ-models for the tractable problems identified by the dichotomy theorem.
We show a wide variation in complexity by type (2-SAT vs Horn-SAT) and by parameter
(the number of repairs allowed for each break).
c
2006
AI Access Foundation. All rights reserved.

Roy

Formally, a δ-model of a Boolean formula, called supermodels by Ginsberg et al. (1998),
is a satisfying assignment (satisfying assignments are usually called models) such that if any
bit of the assignment is flipped (from 0 to 1 or vice versa), one of the following conditions
hold:
(i) either the new assignment is a model or
(ii) there is at least one other bit that can be flipped to obtain another model.
Flipping a bit of a δ-model is called a break, corresponding to a “bad” event. The bit that is
flipped to get another satisfying assignment is a repair (we allow that some breaks may not
need a repair). We also study a generalization of the concept: δ(r, s)-models are satisfying
assignments for which breaks to every set of up to r bits need up to s repairs (to avoid
trivialities, we require that the repair bits are different from the break bits).
We let δ-SAT refer to the decision question as to whether an input Boolean formula
has a δ-model. When we restrict the form of the input Boolean formula, we refer to the
corresponding decision questions as δ-2-SAT, δ-Horn-SAT etc. The higher degree variants
of these problems are δ(r, s)-SAT etc. where we consider r and s to be fixed integers. The
following problems are proved to be NP-complete:
- δ(r, s)-SAT (Ginsberg et al., 1998), δ(1, s)-2-SAT for s > 1,
- δ(1, s)-Horn-SAT, δ(1, s)-dual-Horn-SAT,
- δ(r, s)-0-valid-SAT and δ(r, s)-1-valid-SAT.
In contrast, we prove that the following problems are in P:
- δ(1, 1)-2-SAT, δ(r, s)-Affine-SAT.
The definition of δ-models does not require that the new model obtained by repairing
a break to a δ-model is itself a δ-model. We define δ ∗ -models to be δ-models such that
every break needs at most one repair to obtain another δ-model. Such models represent
the greatest degree of fault tolerance that can be achieved for the problem. We refer to
the corresponding decision problems as δ ∗ -SAT, δ ∗ -2-SAT etc. We prove that δ ∗ -SAT is
in NEXP (non-deterministic exponential time) and is NP-hard, δ ∗ -2-SAT is in P and that
δ ∗ -Affine-SAT is in P.
Remark. Since our goal in this paper is to study the problems in Schaefer’s tractable
class with respect to fault tolerance, our yardstick to measure complexity is membership
in P. Hence, we do not concern ourselves with finding the exact running times within P.
Optimizing runtimes may well prove important for practical applications (at least in the
rare instances when we find polynomial time algorithms).
Organization of the paper: In Section 2, we introduce and define the problem and establish
notation. In Section 3, we study the complexity of finding δ-models of general Boolean
formulas. In Section 4, we consider the complexity of finding δ-models for restricted classes
of formulas: we consider 2-SAT (Section 4.1), Horn-SAT (Section 4.2), 0-valid-SAT, 1-validSAT (Section 4.3) and Affine-SAT (Section 4.4). We conclude with a section on future work
(Section 5).
504

Fault Tolerant Boolean Satisfiability

2. Definitions and Notations
In this section, we establish some of the notation used in the rest of the paper and formally
define the problems we wish to study.
A Boolean variable can take on two values – true or false which we write as 1 and 0
respectively. A literal is either a variable v or its negation, denoted by ¬v (a variable is often
called a pure literal ). A clause is a disjunction ( ∨ ) of literals (for example, v1 ∨ ¬v2 ∨ v3
is a clause). A Boolean formula is a function from some set of Boolean variables V =
{v1 , v2 , . . . , vn } to {0, 1}. In computational problems, we assume that Boolean formulas are
input in a canonical fashion: usually as a conjunction ( ∧ ) of clauses (in which case, we say
that they are in conjunctive normal form (CNF)).
We consider various forms of CNF formulas. A 2-SAT formula is a Boolean formula in
CNF with at most 2 literals per clause (more generally, a k-CNF formula or k-SAT formula is
a CNF formula with k literals per clause). A Horn-SAT formula is a Boolean formula in CNF
where each clause has at most one positive literal (each such clause is called a Horn clause).
Equivalently, a Horn clause can be written as an implication ((v1 ∧ v2 . . . ∧ vr ) → u) where
u, v1 , v2 , . . . , vr are pure literals and r ≥ 0. A dual-Horn-SAT formula is a CNF formula
where each clause has at most one negative literal. An Affine-SAT formula is a CNF formula
in which each clause is an exclusive-or (⊕) of its literals or a negation of the exclusive-or of
its literals (such a clause is satisfied exactly when an odd number of the literals are set to
1). Equivalently, each clause of an Affine-SAT formula can be written as a linear equation
over the finite field {0, 1} of 2 elements.
An assignment is a function X : V → {0, 1} that assigns a truth value (true or false) to
each variable in V . Given such an assignment of truth values to V , any Boolean formula φ
defined over V also inherits a truth value (we denote this by φ(X)), by applying the rules
of Boolean logic. A model is an assignment X such that φ(X) is true. We will often treat
an assignment X as an n-bit vector where the i-th bit, denoted by X(i), 1 ≤ i ≤ n, is the
truth value of the variable vi . With a slight abuse of notation, we let X(l) denote the value
of the literal l under the assignment X.
A 0-valid-SAT (resp. 1-valid-SAT) formula is one which is satisfied by an assignment
with every variable set to 0 (resp. 1).
The propositional satisfiability problem is defined as follows:
Problem (SAT).
Instance: A Boolean formula φ.
Question: Does φ have a model ?
SAT is the canonical example of an NP -complete decision problem (for definitions of the
complexity class NP and completeness, see e.g., Garey & Johnson, 1979; Papadimitriou,
1994). Many computational difficult problems in artificial intelligence have SAT encodings
(for example, in planning (Kautz & Selman, 1992)) and so finding heuristic algorithms
for solving SAT is an important research area in artificial intelligence. Polynomial time
algorithms are known for SAT when the input instance is either Horn-SAT, dual-Horn-SAT,
2-SAT, Affine-SAT, 0-valid-SAT or 1-valid-SAT. Schaefer (1978) proved that these are the
only cases when SAT is solvable in polynomial time, every other case being NP-complete
505

Roy

(Schaefer’s theorem applies to a more general situation called “generalized satisfiability”
where the truth value of each clause is determined by a set of constraints specified as a
relation).
We now introduce the concept of fault-tolerant models. Given an n-bit assignment X,
the operation δi flips the i-th bit of X (from a 0 to a 1, or vice versa). The operation produces
a new assignment which we denote by δi (X). Similarly, if we flip two distinct bits (say bits
i and j), we write the new assignment as δij (X) and more generally, δS (X) represents X
with the bits in S flipped (where S is some subset of the coordinates {1, 2, . . . , n}).
Definition 2.1. A δ-model of a Boolean formula φ is a model X of φ such that for all i,
1 ≤ i ≤ n, either
(i) the assignment δi (X) is a model or
(ii) there is some other bit j, where 1 ≤ j ≤ n and i 6= j, such that δij (X) is a model.
In other words, a δ-model is a model such that if any bit is flipped (we call this a break ),
at most one other bit flip is required to produce a new model. The second bit flip is called
a repair.
Example 2.1. Let H(n, k) be a Boolean formula defined over n variables v1 , v2 , . . . , vn ,
whose models are n-bit assignments with exactly k bits set to 1. For example:





!


n
n 
n
_
^
^

H(n, 1) =
vi ∧
vi → 
¬vj 





i=1
i=1 
j=1

j6=i

The first clause specifies that at least one bit of a model is 1 and each successive clause
specifies that if the i-th bit is 1, then every other bit is set to 0 where 1 ≤ i ≤ n. Each
model of H(n, 1) is a δ-model: any break to a 0-bit has a unique repair (the bit set to 1)
and a break to the 1-bit has (n − 1) possible repairs (any one of the 0-bits).
The following decision problem can be interpreted as the fault-tolerant analogue of SAT:
Problem (δ-SAT).
Instance: A Boolean formula φ.
Question: Does φ have a δ-model ?
The problem δ-SAT and its variants (when we restrict the form of the input Boolean
formula) is the focus of this paper.
We now extend our notion of single repairability to repairability of a sequence of breaks
to a model.
Definition 2.2. A δ(r, s)-model of a Boolean formula φ is a model of φ such that for every
choice of at most r bit flips (the “break” set) of the model, there is a disjoint set of at most
s bits (the “repair” set) that can be flipped to obtain another model of φ.
506

Fault Tolerant Boolean Satisfiability

Remark. (i) We view r and s as fixed constants unless otherwise mentioned. To avoid
redundancies, we have required that the repair set is disjoint from the break set. Since
we require “at most s bits” for repair, we also allow for the case when no repair or
fewer than s repairs are needed.
(ii) Under this definition, δ(1, 1)-models are δ-models and we continue to refer to them as
δ-models for notational simplicity.
(iii) Similar to the definition of δ-SAT, we can define a decision problem δ(r, s)-SAT which
asks whether an input Boolean formula has a δ(r, s)-model.
Example 2.2. Each model of H(n, k) (see, e.g., Example 2.1) is also a δ(k, k) model when
k ≤ n/2.
Assumptions: In all our discussions, we will assume that every variable of an input Boolean
formula appears in both positive and negative literals and that an input Boolean formula
is in clausal form with no variable appearing more than once in a clause (i.e., there is no
clause of the form v1 ∨ ¬v1 ∨ v2 ). We also assume that in any instance of δ-SAT (or its
variants), there is no clause which consists of a single literal, since in that case the input
formula cannot have a δ-model.
Consider a δ-model X of a Boolean formula and suppose that Y is a model which repairs
some break to X. Our definition (Definition 2.1) of δ-models does not require that Y itself
is a δ-model. If we enforce that every break to X is repaired by some δ-model, then not
only is X tolerant to a single break, but so is the repair. We thus can define a degree of
fault tolerance. In this setting, models will be fault tolerant of degree 0. Then, δ-models
will be fault-tolerant of degree 1. More generally, degree-k fault-tolerant models (which we
call δ k -models) consist of δ k−1 fault-tolerant models such that every break is repaired by a
δ k−1 model. We give the formal definition below.
Definition 2.3. Let φ be a Boolean formula. We define δ k (r, s)-models inductively: δ 0 (r, s)models are models of φ. Then for k ≥ 1, δ k (r, s)-models of φ are δ k−1 (r, s)-models X of φ
such that for every break of at most r coordinates of X, there is a disjoint set of at most s
coordinates of X that can be flipped to get a δ k−1 (r, s)-model of φ.
We define the corresponding decision problem δ k (r, s)-SAT, which asks whether an input
Boolean formula has a δ k (r, s)-model. Observe also that by definition a δ k (r, s)-model is a
δ i (r, s)-model for all i, 0 ≤ i ≤ k − 1.
Example 2.3. Let n ≥ 6 be even and let φ be the Boolean formula:
(v1 = v2 ) ∧ (v3 = v4 ) ∧ · · · (vn−1 = vn ) ∧ (

4
_

H(n, k))

k=0

Then the models of φ are vectors with either 0, 2 or 4 variables set to 1. The variables
in {v2i−1 , v2i } have to have the same truth value (and this forces breaks to have unique
repairs).
507

Roy

We claim that X = (0, 0, . . . , 0) is a δ 2 (1, 1)-model of φ. Any break (without loss of
generality, assume it is to coordinate 1) is repaired by a flip to coordinate 2 (and vice versa).
The new vector (1, 1, 0, 0, . . . , 0) is itself a δ-model. A break to some other coordinate (say,
bit 3) has a unique repair (bit 4) to give a model (1, 1, 1, 1, 0, . . . , 0) with 4 1’s. This model is
no longer repairable, since any model has to have at most 4 1’s, so a break to any coordinate
with a 0 (e.g., to bit 5) has no repair.
Let n ≥ 2 be even. Consider the formula
(v1 = v2 ) ∧ (v3 = v4 ) · · · ∧ (vn−1 = vn )
which has 2n/2 models. Observe that each model is a δ-model. So these models are δ k (1, 1)models for every integer k ≥ 0. We call these models δ ∗ (1, 1)-models (as usual, when r = 1
and s = 1, we denote δ ∗ (r, s)-models as δ ∗ -models for simplicity).
Definition 2.4. Let φ be a Boolean formula defined over n Boolean variables. Then a
model of φ which is a δ k (r, s) model for each k ≥ 0 is called a δ ∗ (r, s)-model.
Observe that the set of all δ ∗ -models of φ form a set M of models which satisfies the
following properties:
(i) Each vector in M is a δ-model, i.e., a break to a bit needs at most 1 repair.
(ii) When any bit of a vector in M is broken, there is some repair (if such a repair is
needed) such that the new vector is also a member of M .
We call such sets of δ ∗ -models stable sets of φ. These stable sets have been studied in
a combinatorial setting by Luks and Roy (2005).
Remark. The existence of families of models which satisfy conditions (i) and (ii) above
may be used to give an alternate definition of δ ∗ -models which is perhaps more natural.
However, the notion of degrees of repairability and that δ ∗ -models appear as the limit of
these degrees, is not apparent from this definition, hence we use the formulation leading to
Definition 2.4.
The corresponding decision problem, named δ ∗ -SAT, asks whether an input Boolean
formula has a δ ∗ -model. Note that a “yes” answer to this question implies the existence of
not one but a family of such models, in particular, a set M as above.
Complexity Classes: We refer to Papadimitriou (1994) for definitions of basic complexity
classes like P and NP. A language L is said to be in NEXP if there is a non-deterministic
Turing machine (NDTM) that decides L in exponential time (exponential in the length of
the input). A language L is said to be NP-hard if there is a polynomial time reduction
from SAT to L. A language is NP-complete if it is in NP and is NP-hard. The complexity
class NL (non-deterministic log space), which is contained in P, consists of languages that
are accepted by non-deterministic Turing machines using space logarithmic in the size of
its input. The complexity classes ΣPk are defined as follows: ΣP1 is NP, ΣPk for k ≥ 2 is the
set of languages accepted by a NDTM that has access to an oracle TM for ΣPk−1 .
508

Fault Tolerant Boolean Satisfiability

3. Complexity of Finding δ-models
In this section, we study the computational complexity of finding δ-models for general
Boolean formulas.
Theorem 3.1. (Ginsberg et al., 1998) The decision problem δ(r, s)-SAT is NP-complete.
Remark. The proof technique used in Ginsberg et al. (1998) to prove Theorem 3.1 is used
to prove other NP-hardness results in this paper, e.g., in Theorem 3.2 and Theorem 4.19.
Theorem 3.2. The decision problem δ ∗ -SAT is in NEXP and is NP-hard.
Proof. Since an NDTM can guess a stable set of models (which could be of exponential size)
and check that it satisfies the required conditions for stability in exponential time, δ ∗ -SAT
is in NEXP.
We reduce SAT to δ ∗ -SAT using the same reduction used in the proof of Theorem 3.1
in Ginsberg et al. (1998): given an instance φ of SAT, a Boolean formula φ over n variables
v1 , v2 , . . . , vn , we construct an instance of δ ∗ -SAT: the formula φ0 = φ ∨ vn+1 with vn+1
being a new variable (to put φ0 in CNF form, we add the variable vn+1 to each clause in
the CNF formula φ).
Suppose φ has a model X. We show that φ0 has a δ ∗ -model by showing that it has a
stable set of models M . Extend X to a model Y of φ0 by setting vn+1 = 0. Let Xi = δi (X)
for 1 ≤ i ≤ n. Extend each assignment Xi to a model Yi of φ0 by setting vn+1 = 1. Then
let
M = {Y, Y1 , Y2 , . . . , Yn }.
We now show that M is a stable set. Suppose some bit j 6= i, where 1 ≤ j ≤ n of Yi
is broken, then repair by flipping the i-th bit (in which case, we get the repaired vector
Yj ∈ M ). If the i-th bit of Yi is broken, the repair is the (n + 1)-th bit (and vice versa), in
which case the repaired vector is Y . If instead the i-th bit of Y is broken, where 1 ≤ i ≤ n,
then the repair is the (n + 1)-th bit (we obtain Yi as the repaired vector in this case). If
the (n + 1)-th bit of Y is broken, we can repair by flipping any of the first n bits. Hence
M is a stable set of models and so φ0 has a δ ∗ -model (in fact, we have exhibited n + 1 such
models).
Now we show that if φ0 has a δ ∗ -model, then φ has a model. If φ0 has a δ ∗ -model, it
must have a δ ∗ -model with the (n + 1)-th coordinate set to 0. Then the restriction of this
assignment to v1 , v2 , . . . , vn has to be a model of φ. This completes the reduction from
SAT.
Remark. Note that while every δ ∗ -model is a δ k -model for each k ≥ 1, the NP-hardness
of δ ∗ -SAT (Theorem 3.2) does not imply the NP-hardness of δ k -SAT (Theorem 3.3 below).
The reduction used in Theorem 3.2 can however be adapted to prove Theorem 3.3.
Theorem 3.3. δ k -SAT is NP-complete, where k ≥ 0.
Proof. When k = 0, this is Cook’s Theorem (Garey & Johnson, 1979), so assume that k ≥ 1.
First observe that δ k -SAT is in NP. This is because an NDTM can guess an assignment X
and check that it is a δ k (1, 1)-model: to check whether X is a δ k (1, 1)-model, it suffices to
509

Roy

consider all possible nk break sets, and check that a repair exists for each break applied in
sequence from the break set. Since k is fixed, this can be done in polynomial time.
To prove that δ k -SAT is NP-hard, we use, once again, the proof technique used in Ginsberg et al. (1998) to prove Theorem 3.1. Given an instance φ of SAT, defined on n variables
v1 , v2 , . . . , vn , we construct φ0 = φ ∨ vn+1 (and modify φ0 to a CNF formula), where vn+1 is
a newly introduced variable. The argument used in Theorem 3.2 can now be used to prove
that φ is satisfiable iff φ0 has a δ k -model. In particular, we construct a stable set of models
M for φ0 from a single model of φ. Since a δ ∗ -model is a δ k -model, this proves that if φ is
satisfiable, then φ0 has a δ ∗ -model. The other direction also follows: if φ0 has a δ k -model
then it has a model with vn+1 set to 0. The restriction of that model to v1 , . . . , vn is a
model of φ.

4. Finding δ-models for Restricted Boolean Formulas
In this section, we consider the complexity of δ(r, s)-SAT for restricted classes of SAT
formulas which are known to have polynomial-time algorithms for satisfiability: 2-SAT,
Horn-SAT, dual-Horn-SAT, 0-valid SAT, 1-valid SAT and Affine-SAT. We observe that
these problems have different complexity of testing fault tolerance. For example, 2-SAT
and Affine-SAT have polynomial time tests for the existence of δ-models (see Section 4.1
and 4.4) whereas the same problem is NP-complete for Horn-SAT (Section 4.2).
4.1 Finding δ-models for 2-SAT
We now prove that finding δ-models for 2-SAT formulas is in polynomial time. We give
two independent proofs: the first proof (Section 4.1.1) exploits the structure of the formula
and the second proof (suggested by a referee) uses CSP (constraint satisfaction problem)
techniques (Section 4.1.2). In contrast, we show that finding δ(1, s)-models for 2-SAT
formulas is NP-complete for s ≥ 2 (Section 4.1.3). However, we also show that finding
δ ∗ -models for 2-SAT formulas is in polynomial time (Section 4.1.4).

4.1.1 Polynomial time algorithm for δ(1, 1)-2-SAT
Notation: Let φ be an instance of 2-SAT. Following the notation in Papadimitriou (1994),
we define the directed graph G(φ) = (V, E) as follows: the vertices of the graph are the
literals of φ and for each clause li → lj (where li , lj are literals), there are two directed edges
(li , lj ) and ( ¬ lj , ¬ li ) in E. A path in G(φ) is an ordered sequence of vertices (l1 , l2 , . . . , lr )
where (li , li+1 ) ∈ E for 1 ≤ i ≤ r − 1. We define a simple path in G(φ) to be a path
(l1 , l2 , . . . , lr ) where the literals li involve distinct variables, i.e., li 6= lj and li 6= ¬lj for all
i 6= j, where 1 ≤ i, j ≤ r. A simple cycle of G(φ) is a simple path where we allow the start
and end vertices to be identical. A source vertex (resp. a sink vertex ) in G(φ) is a vertex
with in-degree (resp. out-degree) 0. A vertex l in G(φ) is said to be a k-ancestor (resp.
k-descendant) if there exists a simple path (l, l1 , l2 , . . . , lk ) (resp. (l1 , l2 , . . . , lk , l)) of length
k in G(φ).
The following well-known lemma provides a necessary and sufficient condition for a
2-SAT formula to be satisfiable.
510

Fault Tolerant Boolean Satisfiability

Lemma 4.1. (Papadimitriou, 1994) A 2-SAT formula φ is unsatisfiable iff there is a variable x appearing in φ such that there is a path from x to ¬ x and a path from ¬ x to x in
G(φ).
If φ has a δ-model, then G(φ) has further restrictions.
Lemma 4.2. If a 2-SAT formula φ has a δ-model, then there is no path from l to ¬ l for
any vertex l in G(φ).
Proof. If there was a path from l to ¬ l in G(φ), then any satisfying assignment has to set
l to false. If we now flip the value of the literal l (by flipping the associated variable), we
cannot repair to get a model of φ.
Remark. Lemma 4.2 establishes a necessary condition for a satisfiable 2-SAT formula to
have a δ-model. Unlike Lemma 4.1, this condition is not sufficient: consider, for example,
the 2-SAT formula (which also illustrates many of the constraints that have to be satisfied
if a δ-model exists):
(v1 → v2 ) ∧ (v2 → v3 ) ∧ (v3 → v4 ) ∧ (v4 → v5 ).
Any δ-model of this formula has to set v1 to false (otherwise every variable has to be set to
true and a break to v5 requires more than one repair). Similarly v5 has to set to 1, v2 to
0 and v4 to 1. No choice of v3 will allow a single repair to a break to both v1 or v5 . This
formula thus does not have a δ-model, yet it satisfies the necessary condition of Lemma 4.2.
We now establish a necessary and sufficient condition for a model of 2-SAT formula φ
to be a δ-model.
Lemma 4.3. Let φ be a satisfiable 2-SAT formula. Suppose that there is no path from l to
¬l for any vertex l in G(φ). Let X be a model of φ. Then X is a δ-model if and only if it
satisfies the following conditions:
(C1) Let P = (l1 , l2 , l3 ) be a simple path in G(φ) of length 2. Then X(l1 ) = 0 and X(l3 ) = 1.
(C2) If (l1 , l2 ) and (l1 , l3 ) are edges in G(φ), then X(l1 ), X(l2 ), X(l3 ) cannot all be 0.
Proof. (⇒) Suppose X is a δ-model of φ. Let P = (l1 , l2 , l3 ) be a simple path of length 2.
If X(l1 ) = 1, then X(l2 ) = X(l3 ) = 1, otherwise X cannot be a model of φ. A break to l3
requires the values of both l1 and l2 to be flipped so X cannot be a δ-model, a contradiction.
So X(l1 ) = 0. Similar arguments show that X(l3 ) = 1. Condition (C2) holds similarly: if
X(l1 ), X(l2 ), X(l3 ) were all false, then a break to l1 would require two repairs (both l2 and
l3 ). Hence one of them has to be set to true.
(⇐) Let X be a model of φ which satisfies conditions (C1) and (C2). We show that
X is actually a δ-model. Suppose not; say a break to a variable v is not repairable by at
most one other bit flip. Assume without loss of generality, that X(v) = 0 and so after the
break, v is set to 1. There must be at least one clause of the form v → l where l is a literal,
with X(l) = 0, otherwise the break does not need a response. If there is more than 1 such
clause, say clauses v → l and v → l0 with X(l) = X(l0 ) = 0, then X violates condition (C2),
contradicting the hypothesis. So there is exactly one clause of the form v → l with X(l) = 0
511

Roy

and moreover, it must be the case that flipping l does not produce a model of φ (then one
repair would have sufficed). Now since a flip of the variable associated with l repairs the
clause v → l, there must be other clauses that break when l is repaired. Such a clause must
be of the form l → l0 for some literal l0 with X(l0 ) = 0. We know that l0 cannot be ¬v since
then we would have a path between v and ¬v in G(φ), which violates the hypothesis. Our
assumption that each clause has distinct literals implies that l0 6= ¬l. Hence (v, l, l0 ) is a
simple path such that X(v) = 0 and X(l0 ) = 0, contradicting condition (C1). Hence X is a
δ-model.
Remark. (i) If φ has a δ-model, then it is indeed the case that if (v, u) and (w, u) are
edges of G(φ), then u, v and w cannot all be set to true in such a δ-model (since a
break to u is not repairable by a single flip). We do not need to include this condition
explicitly in Lemma 4.3, because this condition happens if and only if ( ¬ u, ¬ v) and
( ¬ u, ¬ w) satisfy condition (C2) in Lemma 4.3.
(ii) If φ has a δ-model, then condition (C1) can be extended to specify the values of literals
(vertices) on any path of length 3 (the maximum possible length, see Corollary 4.4
below) as follows: if (u1 , u2 , u3 , u4 ) is a simple path, then apply condition (C1) twice
to get X(u1 ) = X(u2 ) = 0 and X(u3 ) = X(u4 ) = 1. Thus we do not include this
condition explicitly.
Lemma 4.3 has further consequences for G(φ):
Corollary 4.4. If a 2-SAT formula φ has a δ-model, then G(φ) satisfies the following
properties:
(i) The longest simple path in G(φ) has length at most 3.
(ii) The longest simple cycle in G(φ) has length at most 2.
(iii) A vertex v can take part in at most 1 simple cycle.
Proof. Suppose that there is a simple path (l1 , l2 , l3 , l4 , l5 ) of length 4 in G(φ). If X is a δmodel of φ, Lemma 4.3 implies that X(l3 ) = 1 when we apply (C2) to the segment (l1 , l2 , l3 )
and X(l3 ) = 0 when we apply (C2) to the segment (l3 , l4 , l5 ). Hence such a δ-model cannot
exist. The other conditions follow from similar arguments.
Pseudo-code for our algorithm is given in Algorithm (1). Observe that Algorithm (1)
is a polynomial time reduction from δ-2-SAT to the satisfiability question of a new 2-SAT
formula φB . Proof of correctness follows.
We first need to prove the following easy lemma.
Lemma 4.5. If a 2-SAT formula φ has a δ-model, then it has a δ-model with each source
vertex (respectively, sink vertex) in G(φ) set to false (resp. true).
Proof. Modify a δ-model X of φ by setting each sink vertex to 1 (and hence each source
vertex to 0). Let the new assignment be X 0 . Clearly, X 0 is still a model of φ (setting the
antecedent p, or the consequent q, to 0, or 1 respectively, satisfies every implication p → q).
512

Fault Tolerant Boolean Satisfiability

Algorithm 1 Algorithm for δ-2-SAT
1:
2:

Input: 2-SAT formula φ
Output: True if φ has a δ(1, 1)-model, false otherwise

if φ is not satisfiable then
return false.
5: end if
3:

4:

/* Check if necessary condition holds (Lemma 4.2) */
Construct G(φ)
8: if there is a path in G(φ) between l and ¬l for any literal l then
9:
return false.
10: end if
6:
7:

11:

φB ← φ

/* Enforce condition (C1) from Lemma 4.3 */
13: for all 2-ancestor vertex l in G(φ) do
14:
φB ← φB ∧ (¬l)
15: end for
12:

16:
17:
18:
19:
20:
21:
22:

/* Force each source (resp. sink) vertex to value 0 (resp. 1) */
for all source vertices l in G(φ) do
φB ← φB ∧ (¬l)
end for
for all sink vertices l in G(φ) do
φB ← φB ∧ (l)
end for

/* Enforce condition (C2) from Lemma 4.3 */
24: for all 1-ancestors l in G(φ) do
25:
for all pairs of distinct vertices l1 , l2 do
26:
if (l, l1 ), (l, l2 ) are edges in G(φ) then
27:
φB ← φB ∧ (l1 ∨ l2 )
28:
end if
29:
end for
30: end for
23:

31:
32:
33:
34:
35:

if φB is satisfiable then
return true
else
return false
end if

513

Roy

We show that this model satisfies condition (C1) and (C2) of Lemma 4.3, thus proving that
it is a δ-model. If condition (C1) is violated, then there is some simple path (l1 , l2 , l3 ) in
G(φ) where X 0 (l1 ) = 1 or X 0 (l3 ) = 0. If X 0 (l1 ) = 1, then X(l1 ) = 1 (suppose not and let
X(l1 ) = 0: since (l1 , l2 ) is an edge in G(φ), l1 is not a sink vertex, so its value would not
have been changed). Similarly, X 0 (l3 ) = 0 would imply that X(l3 ) = 0. Thus X would
violate condition (C1) with respect to the simple path (l1 , l2 , l3 ) and could not have been a
δ-model (a contradiction). Condition (C2) similarly holds.
Algorithm (1) adds literals to the input 2-SAT formula φ to enforce variable assignments
that must hold if φ has a δ-model (see Lines 12–15, 24–30 in the body of Algorithm (1)).
Since we are guaranteed by Lemma 4.3 that these conditions are a necessary and sufficient
condition for the existence of a δ-model, the satisfiability of the resulting Boolean formula
would imply that φ has a δ-model. To simplify the proof of correctness (which is now simply
Corollary 4.6 below), we enforce that source and label vertices get default values prescribed
by Lemma 4.5.
Corollary 4.6. The formula φB is satisfiable iff φ has a δ-model.
Proof. Immediate from Lemma 4.3 and Lemma 4.5.
Example 4.1. Let φ be the 2-SAT formula:
(v1 → v2 ) ∧ (v2 → v3 )
(v1 → v4 ) ∧ (v4 → v3 )
(v1 → v5 ) ∧ (v5 → v3 )
Then Algorithm (1) constructs φB where
φB = φ ∧
(¬v1 ) ∧ (v3 )

(added by lines 13–16 in Algorithm (1))

∧ (v2 ∨ v4 ) ∧ (v2 ∨ v5 ) ∧ (v4 ∨ v5 )

(added by lines 24–31 )

∧ (¬v2 ∨ ¬v4 ) ∧ (¬v2 ∨ ¬v5 ) ∧ (¬v4 ∨ ¬v5 )

(added by lines 24-31 )

Note that in the construction of G(φ), ¬v3 is a 2-ancestor. Since two of the variables
v2 , v4 , v5 have to be set to the same value, φB is unsatisfiable. Hence φ does not have a
δ-model.
Theorem 4.7. In polynomial time, one can determine if a 2-SAT formula has a δ-model
and find one if it exists.
Proof. Satisfiability of a 2-SAT formula is in P (Papadimitriou, 1994). Other steps in the
procedure consist of looping over simple paths of length 3, which can be done in time O(n3 )
where n is the number of variables.
Remark. It is possible to further characterize the space complexity of δ(1, 1)-2-SAT. In
fact, δ(1, 1)-2-SAT is complete for NL (non-deterministic log space). To see that δ(1, 1)-2SAT is in NL, observe that Algorithm (1) can be executed in space logarithmic in the input.
Completeness can be established via a log-space reduction from 2-SAT. Since this result is
not very relevant in the present context, we leave the details out.
514

Fault Tolerant Boolean Satisfiability

4.1.2 An alternative proof of Theorem 4.7
An alternative proof of Theorem 4.7 was suggested by one of the reviewers. It is possible
to cast any satisfiability problem as a constraint satisfaction problem (CSP) over binary
variables. This transformation, particularly when the input instance is a 2-SAT problem,
produces a CSP for which local consistency (consistency of subproblems involving fewer
variables) ensures the presence of a global solution. In this framework, asserting that a
Boolean formula has a δ-model becomes particularly convenient.
Notation: Let φ be a Boolean formula in CNF. For a subset S of variables, we let φ(S)
denote the subformula of φ consisting of clauses from φ which only involve variables in S.
Definition 4.1. A formula φ is said to be k-consistent if for every subset S of k−1 variables,
every model of φ(S) can be extended to a model of φ(S ∪ {v}) for every variable v (i.e., a
larger subformula of φ involving one more variable). A formula is strong k-consistent if it
is i-consistent for all i, 1 ≤ i ≤ k.
Remark. The concept of k-consistency has other equivalent formulations (Jeavons, Cohen,
& Cooper, 1998; Dechter, 1992). Since our goal in this paper is to study satisfiability
exclusively, we rephrase some of the definitions and theorems to apply to our present context.
Theorem 4.8. (Dechter, 1992) Let φ be a 2-SAT formula. Then the following hold:
(a) If φ is strong 3-consistent, then φ is satisfiable and for any 2 element set S, φ(S) is
satisfiable.
(b) In polynomial time (see e.g., (Jeavons et al., 1998)) one can check whether φ is
strong 3-consistent. If φ is satisfiable but not strong 3-consistent, then one can add
extra clauses (also in 2-CNF) to φ in polynomial time such that the resulting 2-SAT
formula is strong 3-consistent.
Remark. More generally, given an input Boolean formula φ, one can establish k-consistency
by adding extra constraints that do not change the set of models. This is done by iterating
over all possible k-element subsets of variables and solving the subproblem for these variables. Clauses are added which restrict the values of any subset of k − 1 variables to only
those values that can be extended to another variable. If there is a set of k −1 variables none
of whose assignments can be extended, then we can conclude φ is unsatisfiable. If not, then
these extra clauses are added to φ to make it k-consistent. Enforcing strong k-consistency
(for fixed k) can be accomplished in polynomial time (Jeavons et al., 1998; Dechter, 1992).
In the special case when φ is a 2-SAT formula these extra clauses are also binary and so
b with exactly
we end up with a strong 3-consistent 2-SAT formula (which we denote by φ)
the same models (and hence, the same set of δ-models).
Notation: For an ordered pair of variables (u, v), we let Mφ (u, v) denote the set of models
of φ({u, v}).
Theorem 4.8 (b) implies that we can assume without loss of generality that the input
is a strong 3-consistent 2-SAT formula φ. Theorem 4.8 also implies that an assignment X
515

Roy

is a model of φ iff (X(u), X(v)) ∈ Mφ (u, v) for all pairs (u, v). Clearly, we can construct
all the sets Mφ (u, v) in polynomial time (there are Θ(n2 ) such variable tuples, where n is
the number of variables, and each set Mφ (u, v) consists of models of a 2-SAT formula with
at most 2 variables). With a slight abuse of notation, we denote Mφ (−u, v) to be the set
{(¬α, β)| (α, β) ∈ Mφ (u, v)}.
Let u be any variable of φ. Let φu,0 = φ ∧ (¬u) and φu,1 = φ ∧ (u). If either φu,0 or
φu,1 is unsatisfiable, then it is clear that φ cannot have a δ-model. Assume then that both
d
φu,0 and φu,1 are satisfiable and let φd
u,0 and φu,1 be the corresponding strong 3-consistent
formulas. Let Nu be the set of variable pairs (v, w) such that Mφd
(v, w) ∩ Mφd
(v, w) = ∅.
u,0
u,1
Lemma 4.9. Suppose Nu 6= ∅ for some variable u. If φ has a δ-model, then there is some
variable v, where v 6= u, such that v belongs to every pair in Nu .
Proof. If we flip the value of u in a δ-model of φ, we can repair by flipping at most one
other variable and we are forced to flip one variable from each pair in Nu . This means that
this repair variable is in every pair of Nu .
Lemma 4.9 implies that we may assume that the pairs in Nu have a common member.
We can similarly show:
Lemma 4.10. Suppose that v is a variable that appears in every pair in Nu . Then the
following hold:
(i) If there exists a w such that
Mφd
(−v, w) ∩ Mφd
(v, w) = ∅
u,0
u,1
then any δ-model X of φ has to set X(u) = 0.
(ii) If there exists a w such that
Mφd
(v, w) ∩ Mφd
(−v, w) = ∅,
u,0
u,1
then any δ-model X of φ has to set X(u) = 1.
Thus either of the two conditions in Lemma 4.10 force the value of the variable u in any
δ-model of φ. Together Lemmas 4.9 and 4.10 enable us to set the values of the variables
that are forced (cf. Lemma 4.3). If after setting the values of these variables, we derive a
contradiction then φ cannot have a δ-model.
Algorithm (2) provides the detailed description of the algorithm.
Theorem 4.11. Algorithm (2) decides δ(1, 1)-2-SAT in polynomial time.
Proof. Enforcing 3-consistency is in polynomial time (Dechter, 1992). The outer loop in
Line 3 executes n times where n is the number of variables. Within the body of the loop,
calls are made to enforce satisfiability and 3-consistency, along with calls to construct Nu
for the variable u under consideration. Each step takes polynomial time, hence the claim
follows.
Remark. While Algorithm (2) solves the yes/no problem of testing whether an input 2SAT formula has a δ-model, it is a simple matter to modify the algorithm so that it outputs
a δ-model if such a model exists. The forced variable assignments along with any satisfying
assignment of the remaining 2-SAT formula is a δ-model of the input formula.
516

Fault Tolerant Boolean Satisfiability

Algorithm 2 Algorithm for δ(1, 1)-2-SAT
Input: A strong 3-consistent 2-SAT formula φ
2: Output: True if φ has a δ(1, 1)-model, false otherwise
1:

3:
4:
5:
6:
7:
8:

for every variable u do
if φu,0 or φu,1 is unsatisfiable then
Output false.
end if
Find sets Mφd
(v, w) and Mφd
(v, w) for variables v, w.
u,0
u,1
Compute N = set of pairs (v, w) such that
Mφd
(v, w) ∩ Mφd
(v, w) = ∅.
u,0
u,1

9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:

If the pairs in N do not have a common member, then output false.
if N 6= ∅ then
For the common member v,
if there is a variable w such that Mφd
(−v, w) ∩ Mφd
(v, w) = ∅ then
u,0
u,1
set φ = φ ∧ (¬u)
end if
if there is a variable w such that Mφd
(v, w) ∩ Mφd
(−v, w) = ∅ then
u,0
u,1
set φ = φ ∧ u
end if
end if
Check if φ is satisfiable, if not output false.
If φ is satisfiable, add extra clauses to φ to make it 3-consistent.
end for
Output true

517

Roy

4.1.3 Complexity of δ(1, s)-2-SAT for s ≥ 2
Theorem 4.12. The problem δ(1, s)-2-SAT is NP-complete for all s > 1.
Proof. Clearly this problem is in NP: an NDTM can guess such an assignment and check
that it is a model and that for every break, there are at most s other bits that can be flipped
to get a model (since s is fixed a priori, this leads to at most O(ns ) possible repair sets, a
polynomial number of choices).
We prove NP-completeness via a reduction from (s + 1)-SAT. Let
T = C1 ∧ C2 . . . ∧ Cm
be an instance of (s + 1)-SAT where each clause Ci is a disjunction of s + 1 literals:
vi,1 ∨ vi,2 . . . ∨ vi,s+1 .
We construct an instance T 0 of δ(1, s)-2-SAT as follows: for each clause Ci in T , we
construct an appropriate 2-SAT formula Ci0 . Our resulting instance of δ(1, s)-2-SAT is a
conjunction of these 2-SAT formulas. Thus,
^
T0 =
Ci0
1≤i≤m

where Ci0 is a 2-SAT formula defined for each clause Ci as follows:
^
Ci0 =
(zi ⇒ vi,j )
1≤j≤(s+1)

^

(vi,j ⇒ αi,j,1 )

(4.1)

1≤j≤(s+1)

^

^

(αi,j,k ⇒ αi,j,k+1 )

1≤j≤s+1 1≤k≤(s−1)

where we have introduced 1+s(s+1) new variables: zi and αi,j,k for 1 ≤ j ≤ s+1, 1 ≤ k ≤ s
to define the gadget Ci0 . The gadget Ci0 is best understood via Figure (1).
Let T have a model X. Extend that to a model of T 0 by setting zi = 0 for all 1 ≤ i ≤ m
and αi,j,k = 1 for all 1 ≤ i ≤ m, 1 ≤ j ≤ s + 1, 1 ≤ k ≤ s. We claim that this is a
δ(1, s)-model of T 0 . Suppose we flip the variable corresponding to literal l. Now we do a
case analysis of how many repairs are needed:
• [l = zi ] Since vi,1 ∨ vi,2 . . . ∨ vi,s+1 is set true by the model X, we need to flip at most
s false literals in {vi,1 , . . . , vi,s+1 }. Observe that no more repairs are necessary.
• [l = αi,j,k ] Need to flip αi,k,k0 where 1 ≤ k 0 < k and we might need to flip the variable
corresponding to vi,j if vi,j was set to true by X. This repair does not affect the truth
value of other clauses of T 0 . Hence we flip at most s variables.
• [l = variable occurring in T ] This will flip the value of all literals involving l. Because
we set every ai,j,k = 1 and zi = 0, no repairs are needed in T 0 , as each implication
(clause) of T 0 still remains true.
518

Fault Tolerant Boolean Satisfiability

zi
vi,1

vi,j

vi,s+1

αi,1,1

αi,j,1

αi,s+1,1

αi,1,k

αi,j,k

αi,s+1,k

αi,1,s

αi,j,s

αi,s+1,s

Figure 1: Gadget for 2-SAT
Now suppose T 0 has a δ(1, s)-model. We show that T has a model. Note that in such a
model zi = 0 for all i (otherwise if zi = 1, then vi,j = αi,j,k = 1 and we will need more than
s repairs when we flip the value of αi,1,s ). Now all literals {vi,1 , vi,2 , . . . , vi,s+1 } cannot be
set to 0, since a break to zi would again necessitate s + 1 repairs. Hence at least one of the
literals in {vi,1 , vi,2 , . . . , vi,s+1 } is set to 1. In other words, the clause Ci in T is satisfied.
Since zi = 0 for all i, T must have a model.

4.1.4 Complexity of δ ∗ -2-SAT
In this section, we show that δ ∗ -2-SAT is in polynomial time.
Let φ be the input 2-SAT formula over n variables. We construct the graph G(φ) as
described before in Section 4.1.1. Since a δ ∗ -model is by definition also a δ-model, we must
have the same path restrictions set forth by Lemma 4.3 and Lemma 4.2. If φ has a δ ∗ -model,
then G(φ) has further restrictions.
Lemma 4.13. Let φ be a 2-SAT formula with a δ ∗ -model. Then every non-trivial simple
path in G(φ) has length 1.
Proof. Suppose that (l1 , l2 , l3 ) is a simple path in G(φ) of length 2. Let X be a δ ∗ -model
of φ. Because of Lemma 4.3, we know that X(l1 ) = 0, X(l3 ) = 1 and this has to be the
case for all δ ∗ -models. This means that a break to X(l1 ) cannot be repaired to get another
δ ∗ -model. Hence, X cannot be a δ ∗ -model, a contradiction.

519

Roy

Remark. Note G(φ) may have cycles (l1 , l2 , l1 ), however in that situation, Lemma 4.13
implies that {l1 , l2 } must form one connected component. Any δ ∗ -model if it exists assigns
the same value to l1 and l2 such that the respective variables form a break-repair pair and are
independent of the remaining variables. We can thus remove the cycles from consideration.
So without loss of generality, we assume that G(φ) has no cycles.
Let R be the vertices in G(φ) with in-degree 0 and B be the vertices with out-degree
0. Since a vertex cannot have positive in-degree and positive out-degree, this creates a
bipartition R ∪ B of the vertices of G(φ), where R, B are disjoint vertex sets and all edges
in G(φ) are of the form (l, l0 ) with l ∈ R and l0 ∈ B.
Note that if (l, l0 ) is an edge in G(φ), then the out-degree of ¬ l is 0: otherwise, there
would be a path of length 2 or a cycle, both of which we have excluded. Hence l ∈ R iff
¬ l ∈ B. We also observe that there are no isolated vertices in G(φ) since every clause is
a disjunction of distinct literals. This gives a complete graph theoretic characterization of
the structure of G(φ) when φ has a δ ∗ -model.
Now let Y0 be an assignment that sets every literal in R false (0) and (hence sets) every
literal in B true (since we have assumed that every variable appears in both positive and
negative literals).
Lemma 4.14. The assignment Y0 is a δ ∗ -model.
Proof. We exhibit a stable set C of models of φ that contains Y0 . Let Y B (respectively,
Y R ) denote the restriction of an assignment Y onto the literals in B (respectively, R).
Let
C = {Y | Y B contains at most one literal set false }.
Note that if Y B contains at most one false literal, then Y R contains at most one true
literal. Clearly Y0 ∈ C.
We now show that C is a stable set. Let Y ∈ C, where Y 6= Y0 . Suppose that Y sets the
literal l ∈ R to true and ¬l to false in B. If the value of the literal l is flipped, then we get
Y0 (a model in C) and so no repairs are needed. If a different variable is flipped, then this
creates a new literal l0 in R set to true (and ¬l0 false in B) in the new assignment. Then
we repair by flipping the value of l from true to false, thereby allowing only one positive
literal in R. Thus any break to Y is repairable by another model in C. A break to Y0 ∈ C
does not need any repairs. Hence C is a stable set and Y0 is a δ ∗ -model.
Theorem 4.15. δ ∗ -2-SAT ∈ P .
Proof. The graph G(φ) can be constructed in polynomial time (in time linear in the size of
φ). All conditions needed for the existence of a δ-model can be checked in polynomial time:
using depth-first search, one can check if the longest simple path of G(φ) has length 1 and
check whether the subgraph of G without any 2-cycles is bipartite.
4.2 Finding δ-models for Horn-SAT and dual Horn-SAT
Recall that an instance of Horn-SAT is a Boolean formula in CNF where each clause contains
at most 1 positive literal. As in 2-SAT, there is a polynomial time algorithm to find a model
520

Fault Tolerant Boolean Satisfiability

of a Horn formula (see, e.g., Papadimitriou (1994)). However, unlike the situation in 2SAT, finding δ(1, s)-models for Horn formulas is NP complete for all s ≥ 1. The proof of
this fact can be easily modified to show that the same problem is NP-complete for dual
Horn-SAT.
We first prove a technical lemma which will be used in the NP-completeness proof.
Define the Boolean formula φ = φ(x, y, β1 , . . . , β2s ) over variables x, y, β1 , . . . , β2s as follows:

φ(x, y, β1 , . . . , β2s ) =

s−1
^

(βi ⇒ βi+1 )

i=1

∧ (βs ⇒ x) ∧ (βs ⇒ y)

(4.2)

∧ (x ⇒ βs+1 ) ∧ (y ⇒ βs+1 ) ∧
2s−1
^

(βi ⇒ βi+1 )

i=s+1

The formula φ is best visualized as in Figure (2). Observe that each variable x and y
appears both as the head and tail of a chain of implications of length s.
Figure 2: Gadget φ

x
β1

β2

βs

βs+1

βs+2

β2s

y
The crucial property of this gadget that we use is as follows:
Lemma 4.16. Let X be a model of φ. Then X is a δ(1, s)-model iff it satisfies x ⇔ ¬y.
Proof. (⇐) Let X be a model of φ. If x ⇔ ¬y holds for X (i.e., x and y get opposite truth
values in X), then X has to set all βi with i ≤ s to 0 (because either x and y is set to 0)
and all βj with j ≥ s + 1 to true (because either x or y is set to 1). Then a break to βi
with i ≤ s requires repairs to βj where i < j ≤ s and exactly one of x and y (the variable
set to 0). Similarly a break to βi with i ≥ s + 1 requires repairs to βj , s + 1 ≤ j ≤ i − 1 and
exactly one of x and y (the variable set to 1). A break to x or y does not need any repairs.
Since we never need more than s repairs for every break, X is a δ(1, s)-model.
(⇒) Any δ(1, s)-model X of φ has to set each βj to 0 where 1 ≤ j ≤ s and each βj
to 1 where s + 1 ≤ j ≤ 2s (otherwise more than s repairs are needed for breaks to these
variables). If both X(x) = X(y) = 0, then a break to β1 (from a 0 to 1) would require
repairs to β2 , β3 , . . . , βs as well as to both x and y, for a total of s + 1 repairs. Hence both
x and y cannot be false. Similarly, both x and y cannot be true because then a break to
β2s would require s + 1 repairs. Hence X satisfies x ⇔ ¬y.
Theorem 4.17. δ(1, s)-Horn-SAT is NP-complete for s ≥ 1.
521

Roy

Proof.VWe prove this via a reduction from 3-SAT. Let T be an instance of 3-SAT, where
T = m
i=1 Ci is defined over n variables x1 , x2 , . . . , xn and clause Ci is a disjunction of 3
distinct literals. Clearly we can assume that every variable appears in both positive and
negative literals in T (if not, we may set the pure literal to be true or false appropriately
and consider the resulting formula as T ).
We first apply an intermediate transformation to T . We replace any positive literal
(say xj ) in Ci by a negative literal, ¬ x0j , where x0j is a new variable not occurring in T .
The new clause, which now has no positive literal, is denoted by Ci0 . Remembering our
global assumption that every variable in input Boolean formulas appear in both positive
and negative literals, we see that this transformation will introduce variables x0j for every
variable xj in T . To maintain logical equivalence, we also need to enforce that ¬ x0j ⇔ xj in
the new formula: so we add the following clauses: ( ¬ x0j ∨ ¬ xj ) and (x0j ∨ xj ). Note that
these two clauses imply that in any model of this new Boolean formula, xj and x0j cannot
have the same truth value.
Thus we obtain
o
^
^ n
T0 =
Ci0
( ¬ x0i ∨ ¬ xi ) ∧ (x0i ∨ xi )
1≤i≤m

1≤i≤n

Note that T 0 is almost Horn (since every clause Ci0 is Horn), the only non-Horn clauses are
the clauses of the form (xi ∨ x0i ). We have introduced n new variables and 2n new clauses,
so that T 0 has m + 2n clauses and is defined over 2n variables. Clearly T 0 is satisfiable iff
T is satisfiable.
We now construct an instance T 00 of δ(1, s)-Horn-SAT from T 0 such that T 00 has a δ(1, s)model iff T 0 is satisfiable. We first introduce s + 1 new variables A1 , A2 , . . . , As+1 . For each
clause Ci0 = ¬vi,1 ∨ ¬vi,2 ∨ ¬vi,3 , we construct a formula Γi,1 consisting of a single clause
(note that at this step, each vi,j is a variable of the form xk or of the form x0k for some
k, 1 ≤ k ≤ n):

Γi,1 = ( ¬ zi ∨ ¬ wi,1 ∨ ¬ wi,2 ∨ ¬ wi,3 )

(4.3)

where zi , wi,1 , wi,2 , wi,3 are new variables introduced for each clause Ci0 . This step introduces 4 new variables per clause Ci0 for a total of 4m new variables. Our next step creates
formulas that places restrictions on these new variables and ties them in with the variables vi,j in the original clause. We introduce new variables αi,j,k for each clause Ci0 , where
1 ≤ j ≤ 3, 1 ≤ k ≤ s − 1, these variables forming the intermediate variables in a chain of
implications of length s from vi,j to wi,j as below:
Γi,2 =(vi,1 ⇒ αi,1,1 ) ∧ (αi,1,1 ⇒ αi,1,2 ) · · · ∧ (αi,1,s−1 ⇒ wi,1 )
∧ (vi,2 ⇒ αi,2,1 ) ∧ (αi,2,1 ⇒ αi,2,2 ) · · · ∧ (αi,2,s−1 ⇒ wi,2 )

(4.4)

∧ (vi,3 ⇒ αi,3,1 ) ∧ (αi,3,1 ⇒ αi,3,2 ) · · · ∧ (αi,3,s−1 ⇒ wi,3 )
The reader may wish to compare the the gadget Γi,2 with a similar gadget Ci0 in Equation (4.1) and shown in Figure (1) that was used in the proof of Theorem 4.12.
522

Fault Tolerant Boolean Satisfiability

We also make zi , one of the new variables introduced in Γi,1 , appear as the head of a
chain of implications of length s + 1 as shown below in formula Γi,3 :
Γi,3 = (zi ⇒ A1 ) ∧ (A1 ⇒ A2 ) . . . ∧ (As ⇒ As+1 )
We now define the formula Ci00 constructed for each clause Ci0 , 1 ≤ i ≤ m, of T 0 :
Ci00 = Γi,1 ∧ Γi,2 ∧ Γi,3
Note that each Ci00 is Horn and has introduced new variables αi,j,k , wi,j , zi for a total of
3(s − 1) + 3 + 1 = 3s + 1 new variables. The other new variables Ai are global, i.e, reused
in the formulas for Ci00 for various i.
For the clauses of the form (¬x0i ∨ ¬xi ) ∧ (x0i ∨ xi ) from T 0 , where 1 ≤ i ≤ n, we
introduce new variables βi,j for each i where 1 ≤ j ≤ 2s and construct the gadget φi =
φ(xi , x0i , βi,1 , βi,2 , . . . , βi,2s ) defined in Equation (4.2).
Our instance of δ(1, s)-Horn-SAT is then:
T 00 =

^

Ci00 ∧

1≤i≤m

^

φi

1≤i≤n

We first show that if T 0 is satisfiable, then T 00 has a δ-model. Suppose T 0 had a model
Extend that to an assignment X 00 of the variables of T 00 by setting the values of the
newly introduced variables as follows:
X 0.

Ai = 1 for 1 ≤ i ≤ s + 1,
zi = 0 for 1 ≤ i ≤ m,
wi,j = 1 for all i and j, where 1 ≤ i ≤ m and 1 ≤ j ≤ 3,
αi,j,k = 1 for all i, j where 1 ≤ i ≤ m, 1 ≤ j ≤ 3, 1 ≤ k ≤ s − 1,
βi,j = 0 for all j, 1 ≤ j ≤ s and all i, 1 ≤ i ≤ n,
βi,j = 1 for all j, s + 1 ≤ j ≤ 2s and all i, 1 ≤ i ≤ n.
Since X 00 satisfies each clause in T 00 , it is a model of T 00 . We now show that X 00 is
actually a δ(1, s)-model. Suppose that some variable v of T 00 is flipped. We do a case by
case analysis of the possible repairs to this break.
[v = xi or x0i ] No repairs are needed since each implication remains satisfied in T 0 .
[v = Ai for some i, 1 ≤ i ≤ s + 1 ] The repairs needed are A1 , A2 , . . . , Ai−1 (since zi = 0
for all i, it does not need to be flipped) for i − 1 (≤ s) repairs.
[v = βi,j for some 1 ≤ i ≤ n, 1 ≤ j ≤ s ] The repairs are all βi,k where j +1 ≤ k ≤ s. Since
X 0 is a model of T 0 , exactly one of xi and x0i is set to false and we need to flip just
that variable. This leads to at most s − j + 1 ≤ s repairs.
[v = βi,j for some 1 ≤ i ≤ n, s + 1 ≤ j ≤ 2s ] The repairs needed are βi,k for all s + 1 ≤
k < j and one of xi or x0i (since X 0 is a model of T 0 only one of xi , x0i is set to true in
X 0 ) for at most j − s ≤ s repairs.
523

Roy

[v = wi,j for some 1 ≤ i ≤ m, 1 ≤ j ≤ 3 ] The repairs needed are αi,j,k for all 1 ≤ k ≤ s−1
and vi,j (if X 0 (vi,j ) = 1), for at most s repairs.
[v = αi,j,k for some 1 ≤ i ≤ m, 1 ≤ j ≤ 3, 1 ≤ k ≤ s − 1 ] The repairs needed are αi,j,k0 for
1 ≤ k 0 ≤ k − 1 and vi,j (if X 0 (vi,j ) = 1) for at most k ≤ s − 1 repairs.
[v = zi ] It is this break alone whose repair crucially depends on the satisfiability of T 0 .
Note that this break changes zi from a 0 to a 1 and makes the clause ( ¬ zi ∨ ¬ wi,1 ∨
¬ wi,2 ∨ ¬ wi,3 ) false since each wi,j is true in X 00 . So repairs will have to include one
or more of the wi,j ’s, which consequently might trigger flips to αi,j,k and vi,j . The
choice of which wi,j to involve in the repair process is indicated by the vi,j set to 0
by X 0 . Since X 0 is a model, note also that at least one vi,j is set to 0. Without loss
of generality, assume that X 0 (vi,1 ) = 0 then repair a break to zi by flipping wi,1 , αi,1,j
for all 1 ≤ j ≤ s − 1 for exactly s repairs.
Now suppose T 00 has a δ-model X 00 . We show that T 0 is satisfiable. Specifically, we claim
that the restriction of X 00 to the variables of T 0 is a model of T 0 . From Lemma 4.16, we
know that βi,j = 0 for all 1 ≤ i ≤ n, 1 ≤ j ≤ s and βi,j = 1 for all 1 ≤ i ≤ n, s + 1 ≤ j ≤ 2s
and also ¬x0i ⇔ xi in T 0 is satisfied for each i, 1 ≤ i ≤ n. Note that in T 00 , wi,j is at the
end of a chain of implications:
βk,1 → βk,2 → · · · → βk,s → vi,j → αi,j,1 → · · · → αi,j,s−1 → wi,j

(4.5)

where vi,j is either xk or x0k for some k, 1 ≤ k ≤ n. Note that the variables in the above chain
are from different gadgets – from both φk and from Γi,2 . This implies that X 00 (wi,j ) = 1 since
otherwise X 00 would have to set all variables in this chain to 0 and then this would violate
Lemma 4.16. Since X 00 is a model of T 00 , it must be that X 00 (zi ) = 1 for all i, otherwise
¬zi ∨ ¬wi,1 ∨ ¬wi,2 ∨ ¬wi,3 will be false. When zi is flipped, we are guaranteed a repair of
at most s flips that will make the clause ¬zi ∨ ¬wi,1 ∨ ¬wi,2 ∨ ¬wi,3 true. This will involve
flipping at least one of wi,j , for j = 1, 2, 3. If vi,1 , vi,2 and vi,3 were all set to true by X 00
(which would in turn have implied that X 00 (αi,j,k ) = 1 for all 1 ≤ j ≤ 3, 1 ≤ k ≤ s − 1) then
any such flip would require s additional repairs, for a total of s+1 repairs to a break to zi . So
it must be that vi,j is false for some j, 1 ≤ j ≤ 3. In other words, Ci0 = ¬vi,1 ∨ ¬vi,2 ∨ ¬vi,3
is satisfied by X 00 . Hence the restriction of X 00 to T 0 satisfies all clauses of T 0 . Thus T 0 is
satisfiable.
So T 0 is satisfiable iff T 00 has a δ(1, s)-model. Since T is satisfiable iff T 0 is satisfiable and
T is a SAT instance, this accomplishes the reduction from SAT. This reduction is clearly a
polynomial time reduction. Since δ(1, s)-Horn-SAT is clearly in NP for fixed r and s, this
proves that it is NP-complete.
Recall that an dual-Horn formula is a Boolean formula in CNF where each clause has
at most one negative literal. Not surprisingly, dual-Horn-SAT formulas behave similarly to
Horn-SAT when it comes to finding δ-models.
Theorem 4.18. δ(1, s)-dual-Horn-SAT is NP-complete.
The proof of this theorem is very similar to that of Theorem 4.17: we replace Equation (4.3) by Γi,1 = (zi ∨ wi,1 ∨ wi,2 ∨ wi,3 ) and change the direction of implications in Γi,3
and Equation (4.4).
524

Fault Tolerant Boolean Satisfiability

4.3 Finding δ-models for 0-valid, 1-valid SAT formulas
Recall that a 0-valid (resp. 1-valid) Boolean formula is one which is satisfied by a model
with every variable set to 0 (resp. 1). We now consider the complexity of finding faulttolerant models of an input 0-valid (or 1-valid) formula and refer to the corresponding
decision questions as δ(r, s)-0-valid-SAT, δ(r, s)-1-valid-SAT, δ ∗ -0-valid-SAT etc.
The knowledge that an input Boolean formula is satisfied by some particular assignment
does not provide information about the presence of fault-tolerant models. Hence we would
expect (correctly) that finding such models to be NP-hard. We first prove:
Theorem 4.19. The decision problem δ(r, s)-0-valid-SAT is NP-complete.
Proof. For the proof, we refer to the proof of Theorem 3.1 which, with slight modification,
works for this problem as well. We reduce from SAT. Let T be a SAT instance, we construct
an instance of δ(r, s)-0-valid-SAT, T 0 = T ∨ ¬y where y is a new variable not appearing in
T . Observe that T 0 is 0-valid (its the value of y that matters). The proof that T 0 has a
δ-model iff T is satisfiable is identical to the proof of Theorem 3.1: if T is satisfiable and
has a model X, extend that to a model X 0 of T 0 by setting the value of y to 1. Then any
break consisting of r variables in X 0 does not require a repair if the r variables involve y. If
they do not involve y, then flipping the value of y from a 1 to a 0 makes T 0 true, hence one
repair suffices. Hence X 0 is a δ(r, s)-model. If T 0 has a δ(r, s)-model, it must have a model
with y set to 1. The restriction of that model to the variables of T makes T true, hence T
is satisfiable.
Similarly, it is easy to verify that the proofs of Theorem 3.1, Theorem 3.2 work when
the input formula is a 0-valid or 1-valid formula. Hence we have the following:
Theorem 4.20. The decision problem δ(r, s)-1-valid-SAT is NP-complete. The problem
δ ∗ (1, 1)-0-valid-SAT and δ ∗ (1, 1)-1-valid-SAT are in NEXP and are NP-hard.
4.4 Finding δ-models for Affine-SAT
Another class of Boolean formulas that have polynomial time satisfiability checkers is AffineSAT: these are formulas which are a conjunction of clauses, where each clause is an exclusiveor (denoted by ⊕) of distinct literals (a ⊕ b = 1 iff exactly one of the Boolean variables a, b
is set to 1).
Example 4.2. An example of an Affine-SAT formula is
(x1 ⊕ x2 ⊕ x3 ⊕ x4 = 1) ∧ (x3 ⊕ x4 = 0)
This formula has a δ-model X = (1, 0, 0, 0). In fact, X is easily seen to be a δ ∗ -model (which
is true of all δ-models of Affine-SAT formulas, as we shall shortly see).
One can find a satisfying assignment for a formula in affine form by a variant of Gaussian
elimination. We now prove that finding δ-models for affine formulas is also in polynomial
time.
Lemma 4.21. An Affine-SAT formula φ has a δ-model iff φ is satisfiable and for every
variable v ∈ V appearing in φ there exists a variable w = w(v) such that v and w appear in
exactly the same clauses.
525

Roy

Proof. Let X be a δ-model of φ. If a variable v is flipped, then the clauses that v appears
in become false, to repair them we need to flip some other variable that appears in exactly
those clauses (and no others). Thus such a variable pairing must exist. The reverse direction
is easily proved: if such a variable pairing exists, then the variables form a break-repair
pair.
Since the conditions of Lemma 4.21 are easy to check in polynomial time, we have the
following theorem:
Theorem 4.22. δ(1, 1)-Affine-SAT ∈ P.
We can, in fact, slightly strengthen our theorem. We first state an analogue of Lemma 4.21,
where the variable pairings can be easily generalized.
Definition: The parity of an integer n is n mod 2.
Lemma 4.23. An Affine-SAT formula φ has a δ(r, s)-model iff φ is satisfiable and for every
set R of at most r variables, there exists a set S, S ∩ R = ∅ of at most s variables, such
that for all clauses C of φ, the parity of the number of variables of R appearing in C is the
same as the parity of the number of variables of S appearing in C.
We now prove:
Theorem 4.24. δ(r, s)-Affine-SAT is in P .
Proof. Since r and s are fixed constants, the conditions in Lemma 4.23 can be checked in
polynomial time: for each choice of the set R such that R ≤ r, (there are O(nr ) such sets),
cycle through each possible set S where |S| ≤ s, S ∩ R = ∅ (there are O(ns ) such sets),
check to see if the conditions of Lemma 4.23 are satisfied (in particular, test whether the
parity of the variables of R appearing in any clause = parity of the variables of S appearing
in the clause, which also can be accomplished in polynomial time).
Hence δ(r, s)-Affine-SAT is in polynomial time.
Theorem 4.22 implies that any δ-model of φ is actually a δ ∗ -model, since if the pairings
(u, w(v)) exist, any model of φ will become a δ-model (with {v, w(v)} forming break-repair
pairs).
Hence an Affine-SAT formula has a δ-model iff it has a δ ∗ -model, hence finding a δ ∗ model for Affine-SAT formulas is also in polynomial time.
We thus have the following theorem:
Theorem 4.25. δ ∗ -Affine-SAT ∈ P.

5. Future Work
The complexity of δ(r, s)-SAT where r and s are part of the input as opposed to being fixed
constants is not known. This problem is in the complexity class Σp3 , but is it complete for
that class? The status of this problem for restricted Boolean formulas like 2-SAT, HornSAT etc., when r and s are specified in the input is similarly open. At present, we do not
also know if δ ∗ (r, s)-SAT can be decided in polynomial space when r, s are fixed constants.
Finally, a practical modification of the concept of δ-models would involve weakening the
condition to allow for only a high percentage of breaks to be repairable.
526

Fault Tolerant Boolean Satisfiability

Acknowledgements
The author is grateful to Eugene M. Luks for his encouragement and advice. We also thank
the anonymous referees for their detailed comments and suggestions.

References
Bailleux, O., & Marquis, P. (1999). Distance-sat: Complexity and algorithms. In Proceedings
of the Sixteenth National Conference on Artificial Intelligence (AAAI–99), pp. 642–
647.
Dechter, R. (1992). From local to global consistency. Artificial Intelligence, 55 (1), 87–108.
Garey, M. R., & Johnson, D. S. (1979). Computers and Intractability: A Guide to the Theory
of NP-completeness. W. H. Freeman and Company, New York.
Ginsberg, M., Parkes, A., & Roy, A. (1998). Supermodels and robustness. In Proceedings
of the Fifteenth National Conference of the American Association for Artificial Intelligence and the tenth conference on Innovative Applications of Artificial Intelligence,
1998, Madison, WI, pp. 334–339.
Hebrard, E., Hnich, B., & Walsh, T. (2004a). Robust solutions for constraint satisfaction
and optimization. In Proceedings of the sixteenth European Conference on Artificial
Intelligence, ECAI, pp. 186–190, Valencia, Spain.
Hebrard, E., Hnich, B., & Walsh, T. (2004b). Super solutions in constraint programming.
In Proceedings of the Internation Conference on Integration of AI and OR Techniques
in Constraint Programming for Combinatorial Optimization Problems (CPAIOR), pp.
157–172.
Holland, A., & O’Sullivan, B. (2004). Super solutions for combinatorial auctions. In Proceedings of CSCLP 2004: Joint Annual Workshop of ERCIM/CoLogNet on Constraint
Solving and Constraint Logic Programming.
Hoos, H., & O’Neill, K. (2000). Stochastic local search methods for dynamic sat – an initial
investigation. In AAAI-2000 Workshop Leveraging Probability and Uncertainty in
Computation, pp. 22–26.
Jeavons, P. G., Cohen, D. A., & Cooper, M. C. (1998). Constraints, consistency and closure.
Artificial Intelligence, 101 (1–2), 251–265.
Kautz, H. A., & Selman, B. (1992). Planning as satisfiability. In Proceedings of the Tenth
European Conference on Artificial Intelligence (ECAI’92), pp. 359–363.
Luks, E. M., & Roy, A. (2005). Combinatorics of singly-repairable families. Electronic
Journal of Combinatorics, 12 (1), Research Paper 59, 17 pp. (electronic).
Papadimitriou, C. H. (1994). Computational Complexity. Addison-Wesley.
Schaefer, T. J. (1978). The complexity of satisfiability problems. In STOC ’78: Proceedings
of the tenth annual ACM symposium on Theory of computing, pp. 216–226. ACM
Press.

527

Journal of Artificial Intelligence Research 25 (2006) 159-185

Submitted 5/05; published 2/06

Dynamic Local Search for the Maximum Clique Problem
Wayne Pullan

w.pullan@griffith.edu.au

School of Information and Communication Technology,
Griffith University,
Gold Coast, QLD, Australia

Holger H. Hoos

hoos@cs.ubc.ca

Department of Computer Science
University of British Columbia
2366 Main Mall, Vancouver, BC, V6T 1Z4 Canada

Abstract
In this paper, we introduce DLS-MC, a new stochastic local search algorithm for the maximum clique problem. DLS-MC alternates between phases of iterative improvement, during
which suitable vertices are added to the current clique, and plateau search, during which
vertices of the current clique are swapped with vertices not contained in the current clique.
The selection of vertices is solely based on vertex penalties that are dynamically adjusted
during the search, and a perturbation mechanism is used to overcome search stagnation.
The behaviour of DLS-MC is controlled by a single parameter, penalty delay, which controls the frequency at which vertex penalties are reduced. We show empirically that DLSMC achieves substantial performance improvements over state-of-the-art algorithms for the
maximum clique problem over a large range of the commonly used DIMACS benchmark
instances.

1. Introduction
The maximum clique problem (MAX-CLIQUE) calls for finding the maximum sized subgraph of pairwise adjacent vertices in a given graph. MAX-CLIQUE is a prominent combinatorial optimisation problem with many applications, for example, information retrieval,
experimental design, signal transmission and computer vision (Balus & Yu, 1986). More
recently, applications in bioinformatics have become important (Pevzner & Sze, 2000; Ji,
Xu, & Stormo, 2004). The search variant of MAX-CLIQUE can be stated as follows: Given
an undirected graph G = (V, E), where V is the set of all vertices and E the set of all edges,
find a maximum size clique in G, where a clique in G is a subset of vertices, C ⊆ V , such
that all pairs of vertices in C are connected by an edge, i.e., for all v, v ′ ∈ C, {v, v ′ } ∈ E,
and the size of a clique C is the number of vertices in C. MAX-CLIQUE is N P-hard
and the associated decision problem is N P-complete (Garey & Johnson, 1979); furthermore, it is inapproximable in the sense that no deterministic polynomial-time algorithm
can find cliques of size |V |1−ǫ for any ǫ > 0, unless N P = ZPP (Håstad, 1999).1 The
best polynomial-time approximation algorithm for MAX-CLIQUE achieves an approximation ratio of O(|V |/(log |V |)2 ) (Boppana & Halldórsson, 1992). Therefore, large and hard
instances of MAX-CLIQUE are typically solved using heuristic approaches, in particular,
1. ZPP is the class of problems that can be solved in expected polynomial time by a probabilistic algorithm
with zero error probability.
c
2006
AI Access Foundation. All rights reserved.

Pullan & Hoos

greedy construction algorithms and stochastic local search (SLS) algorithms such as simulated annealing, genetic algorithms and tabu search. (For an overview of these and other
methods for solving MAX-CLIQUE, see Bomze, Budinich, Pardalos, & Pelillo, 1999.) It
may be noted that the maximum clique problem is equivalent to the independent set problem as well as to the minimum vertex cover problem, and any algorithm for MAX-CLIQUE
can be directly applied to these equally fundamental and application relevant problems
(Bomze et al., 1999).
From the recent literature on MAX-CLIQUE algorithms, it seems that, somewhat unsurprisingly, there is no single best algorithm. Although most algorithms have been empirically
evaluated on benchmark instances from the Second DIMACS Challenge (Johnson & Trick,
1996), it is quite difficult to compare experimental results between studies, mostly because
of differences in the respective experimental protocols and run-time environments. Nevertheless, particularly considering the comparative results reported by Grosso et al. (Grosso,
Locatelli, & Croce, 2004), it seems that there are five heuristic MAX-CLIQUE algorithms
that achieve state-of-the-art performance.
Reactive Local Search (RLS) (Battiti & Protasi, 2001) has been derived from Reactive
Tabu Search (Battiti & Tecchiolli, 1994), an advanced and general tabu search method
that automatically adapts the tabu tenure parameter (which controls the amount of diversification) during the search process; RLS also uses a dynamic restart strategy to provide
additional long-term diversification.
QUALEX-MS (Busygin, 2002) is a deterministic iterated greedy construction algorithm that uses vertex weights derived from a nonlinear programming formulation of MAXCLIQUE.
The more recent Deep Adaptive Greedy Search (DAGS) algorithm (Grosso et al., 2004)
also uses an iterated greedy construction procedure with vertex weights; the weights in
DAGS, however, are initialised uniformly and updated after every iteration of the greedy
construction procedure. In DAGS, this weighted iterated greedy construction procedure is
executed after an iterative improvement phase that permits a limited amount of plateau
search. Empirical performance results indicate that DAGS is superior to QUALEX-MS for
most of the MAX-CLIQUE instances from the DIMACS benchmark sets, but for some hard
instances it does not reach the performance of RLS (Grosso et al., 2004).
The k-opt algorithm (Katayama, Hamamoto, & Narihisa, 2004) is based on a conceptually simple variable depth search procedure that uses elementary search steps in which a
vertex is added to or removed from the current clique; while there is some evidence that it
performs better than RLS on many instances from the DIMACS benchmark sets (Katayama
et al., 2004), its performance relative to DAGS is unclear.
Finally, Edge-AC+LS (Solnon & Fenet, 2004), a recent ant colony optimisation algorithm for MAX-CLIQUE that uses an elitist subsidiary local search procedure, appears to
reach (or exceed) the performance of DAGS and RLS on at least some of the DIMACS
instances.
In this work, we introduce a new SLS algorithm for MAX-CLIQUE algorithm dubbed
Dynamic Local Search – Max Clique, DLS-MC, which is based on a combination of constructive search and perturbative local search, and makes use of penalty values associated
with the vertices of the graph, which are dynamically determined during the search and
help the algorithm to avoid search stagnation.
160

Dynamic Local Search for Max-Clique Problem

Based on extensive computational experiments, we show that DLS-MC outperforms
other state-of-the-art MAX-CLIQUE search algorithms, in particular DAGS, on a broad
range of widely studied benchmark instances, and hence represents an improvement in
heuristic MAX-CLIQUE solving algorithms. We also present detailed results on the behaviour of DLS-MC and offer insights into the roles of its single parameter and the dynamic
vertex penalties. We note that the use of vertex penalties in DLS-MC is inspired by the
dynamic weights in DAGS and, more generally, by current state-of-the-art Dynamic Local
Search (DLS) algorithms for other well-known combinatorial problems, such as SAT and
MAX-SAT (Hutter, Tompkins, & Hoos, 2002; Tompkins & Hoos, 2003; Thornton, Pham,
Bain, & Ferreira, 2004; Pullan & Zhao, 2004); for a general introduction to DLS, see also
the work of (Hoos & Stützle, 2004). Our results therefore provide further evidence for the
effectiveness and broad applicability of this algorithmic approach.
The remainder of this article is structured as follows. We first describe the DLS-MC
algorithm and key aspects of its efficient implementation. Next, we present empirical performance results that establish DLS-MC as the new state-of-the-art in heuristic MAX-CLIQUE
solving. This is followed by a more detailed investigation of the behaviour of DLS-MC and
the factors determining its performance. Finally, we summarise the main contributions of
this work, insights gained from our study and outline some directions for future research.

2. The DLS-MC Algorithm
Like the DAGS algorithm by Grosso et al., our new DLS-MC algorithm is based on the fundamental idea of augmenting a combination of iterative improvement and plateau search
with vertex penalties that are modified during the search. The iterative improvement procedure used by both algorithms is based on a greedy construction mechanism that starts with
a trivial clique consisting of a single vertex and successively expands this clique C by adding
vertices that are adjacent to all vertices in C. When such an expansion is impossible, there
may still exist vertices that are connected to all but one of the vertices in C. By including
such a vertex v in C and removing the single vertex in C not connected to v, a new clique
with the same number of vertices can be obtained. This type of search is called plateau
search. It should be noted that after one or more plateau search steps, further expansion
of the current clique may become possible; therefore, DLS-MC alternates between phases
of expansion and plateau search.
The purpose of vertex penalties is to provide additional diversification to the search
process, which otherwise could easily stagnate in situations where the current clique has
few or no vertices in common with an optimal solution to a given MAX-CLIQUE instance.
Perhaps the most obvious approach for avoiding this kind of search stagnation is to simply
restart the constructive search process from a different initial vertex. However, even if there
is random (or systematic) variation in the choice of this initial vertex, there is still a risk that
the heuristic guidance built into the greedy construction mechanism causes a bias towards
a limited set of suboptimal cliques. Therefore, both DAGS and DLS-MC utilise numerical
weights associated with the vertices; these weights modulate the heuristic selection function
used in the greedy construction procedure in such a way that vertices that repeatedly occur
in the cliques obtained from the constructive search process are discouraged from being used
in future constructions. Following this intuition, and consistent with the general approach
161

Pullan & Hoos

of dynamic local search (DLS), which is based on the same idea, in this paper, we refer to
the numerical weights as vertex penalties.
Based on these general considerations, the DLS-MC algorithm works as follows (see also
the algorithm outline in Figure 1): After picking an initial vertex from the given graph G
uniformly at random and setting the current clique C to the set consisting of this single
vertex, all vertex penalties are initialised to zero. Then, the search alternates between
an iterative improvement phase, during which suitable vertices are repeatedly added to
the current clique C, and a plateau search phase, in which repeatedly one vertex of C is
swapped with a vertex currently not contained in C.
The two subsidiary search procedures implementing the iterative improvement and
plateau search phases, expand and plateauSearch, are shown in Figure 2. Note that both,
expand and plateauSearch select the vertex to be added to the current clique C using only
the penalties associated with all candidate vertices. In the case of expand, the selection is
made from the set NI (C) of all vertices that are connected to all vertices in C by some
edge in G; we call this set the improving neighbour set of C. In plateauSearch, on the other
hand, the vertex to be added to C is selected from the level neighbour set of C, NL (C),
which comprises the vertices that are connected to all vertices in C except for one vertex,
say v ′ , which is subsequently removed from C.
Note that both procedures always maintain a current clique C; expand terminates when
the improving neighbour set of C becomes empty, while plateauSearch terminates when
either NI (C) is no longer empty or when NL (C) becomes empty. Also, in order to reduce the
incidence of unproductive plateau search phases, DLS-MC implements the plateau search
termination condition of (Katayama et al., 2004) by recording the current clique (C ′ ) at the
start of the plateau search phase and terminating plateauSearch when there is no overlap
between the recorded clique C ′ and the current clique C.
At the end of the plateau search phase, the vertex penalties are updated by incrementing
the penalty values of all vertices in the current clique, C, by one. Additionally, every pd
penalty value update cycles (where pd is a parameter called penalty delay), all non-zero
vertex penalties are decremented by one. This latter mechanism prevents penalty values
from becoming too large and allows DLS-MC to ‘forget’ penalty values over time.
After updating the penalties, the current clique is perturbed in one of two ways. If the
penalty delay is greater than one, i.e., penalties are only decreased occasionally, the current
clique is reduced to the last vertex v that was added to it. Because the removed vertices
all have increased penalty values, they are unlikely to be added back into the current clique
in the subsequent iterative improvement phase. This is equivalent to restarting the search
from v. However, as a penalty delay of one corresponds to a behaviour in which penalties are
effectively not used at all (since an increase of any vertex penalty is immediately undone),
keeping even a single vertex of the current clique C carries a high likelihood of reconstructing
C in the subsequent iterative improvement phase. Therefore, to achieve a diversification
of the search, when the penalty delay is one, C is perturbed by adding a vertex v that is
chosen uniformly at random from the given graph G and removing all vertices from C that
are not connected to v.
As stated above, the penalty values are used in the selection of a vertex from a given
neighbour set S. More precisely, the selectMinPenalty(S) selects a vertex from S by choosing
uniformly at random from the set of vertices in S with minimal penalty values. After a vertex
162

Dynamic Local Search for Max-Clique Problem

procedure DLS-MC(G, tcs, pd, maxSteps)
input: graph G = (V, E); integers tcs (target clique size), pd (penalty delay), maxSteps
output: clique in G of size at least tcs or ‘failed’
begin
numSteps := 0;
C := {random(V )};
initPenalties;
while numSteps < maxSteps do
(C, v) := expand(G, C);
if |C| = tcs then return(C); end if
C ′ := C;
(C, v) := plateauSearch(G, C, C ′ );
while NI (C) 6= ∅ do
(C, v) := expand(G, C);
if |C| = tcs then return(C); end if
(C, v) := plateauSearch(G, C, C ′ );
end while
updatePenalties(pd );
if pd > 1 then
C := {v};
else
v := random(V );
C := C ∪ {v};
remove all vertices from C that are not connected to v in G;
end if
end while
return(‘failed’);
end

Figure 1: Outline of the DLS-MC algorithm; for details, see text.

has been selected from S, it becomes unavailable for subsequent selections until penalties
have been updated and perturbation has been performed. This prevents the plateau search
phase from repeatedly visiting the same clique. Also, as a safeguard to prevent penalty
values from becoming too large, vertices with a penalty value greater than 10 are never
selected.
In order to implement DLS-MC efficiently, all sets are maintained using two array data
structures. The first of these, the vertex list array, contains the vertices that are currently
in the set; the second one, the vertex index array, is indexed by vertex number and contains
the index of the vertex in the vertex list array (or −1, if the vertex is not in the set). All
additions to the set are performed by adding to the end of the vertex list array and updating
the vertex index array. Deletions from the set are performed by overwriting the vertex list
entry of the vertex to be deleted with the last entry in vertex list and then updating the
vertex index array. Furthermore, as vertices can only be swapped once into the current
clique during the plateau search phase, the intersection between the current clique and the
recorded clique can be simply maintained by recording the size of the current clique at the
start of the plateau search and decrementing this by one every time a vertex is swapped
163

Pullan & Hoos

procedure expand(G, C)
input: graph G = (V, E); vertex set C ⊆ V (clique)
output: vertex set C ⊆ V (expanded clique); vertex v (most recently added vertex)
begin
while NI (C) 6= ∅ do
v := selectMinPenalty(NI (C));
C := C ∪ {v};
numSteps := numSteps + 1;
end while;
return((C, v));
end

procedure plateauSearch(G, C, C ′ )
input: graph G = (V, E); vertex sets C ⊆ V (clique), C ′ ⊆ C (recorded clique)
output: vertex set C ⊆ V (modified clique); vertex v (most recently added vertex)
begin
while NI (C) = ∅ and NL (C) 6= ∅ and C ∩ C ′ 6= ∅ do
v := selectMinPenalty(NL (C));
C := C ∪ {v};
remove the vertex from C that is not connected to v in G;
numSteps := numSteps + 1;
end while;
return((C, v));
end

Figure 2: Subsidiary search procedures of DLS-MC; for details, see text.
into the current clique. Finally, all array elements are accessed using pointers rather than
via direct indexing of the array. 2
Finally, it may be noted that in order to keep the time-complexity of the individual
search steps minimal, the selection from the improving and level neighbour sets does not
attempt to maximise the size of the set after the respective search step, but rather chooses
a vertex with minimal penalty uniformly at random; this is in keeping with the common
intuition that, in the context of SLS algorithms, it is often preferable to perform many
relatively simple, but efficiently computable search steps rather than fewer complex search
steps.

3. Empirical Performance Results
In order to evaluate the performance and behaviour of DLS-MC, we performed extensive computational experiments on all MAX-CLIQUE instances from the Second DIMACS
Implementation Challenge (1992–1993)3 , which have also been used extensively for benchmarking purposes in the recent literature on MAX-CLIQUE algorithms. The 80 DIMACS
MAX-CLIQUE instances were generated from problems in coding theory, fault diagnosis
problems, Keller’s conjecture on tilings using hypercubes and the Steiner triple problem,
2. Several of these techniques are based on implementation details of Henry Kautz’s highly efficient WalkSAT code, see http://www.cs.washington.edu/homes/kautz/walksat.
3. http://dimacs.rutgers.edu/Challenges/

164

Dynamic Local Search for Max-Clique Problem

in addition to randomly generated graphs and graphs where the maximum clique has been
‘hidden’ by incorporating low-degree vertices. These problem instances range in size from
less than 50 vertices and 1 000 edges to greater than 3 300 vertices and 5 000 000 edges.
All experiments for this study were performed on a dedicated 2.2 GHz Pentium IV machine with 512KB L2 cache and 512MB RAM, running Redhat Linux 3.2.2-5 and using the
g++ C++ compiler with the ‘-O2’ option. To execute the DIMACS Machine Benchmark4 ,
this machine required 0.72 CPU seconds for r300.5, 4.47 CPU seconds for r400.5 and 17.44
CPU seconds for r500.5. In the following, unless explicitly stated otherwise, all CPU times
refer to our reference machine.
In the following sections, we first present results from a series of experiments that were
aimed at providing a detailed assessment of the performance of DLS-MC. Then, we report
additional experimental results that facilitate a more direct comparison between DLS-MC
and other state-of-the-art MAX-CLIQUE algorithms.
3.1 DLS-MC Performance
To evaluate the performance of DLS-MC on the DIMACS benchmark instances, we performed 100 independent runs of it for each instance, using target clique sizes (tcs) corresponding to the respective provably optimal clique sizes or, in cases where such provably
optimal solutions are unknown, largest known clique sizes. In order to assess the peak
performance of DLS-MC, we conducted each such experiment for multiple values of the
penalty delay parameter, pd, and report the best performance obtained. The behaviour of
DLS-MC for suboptimal pd values and the method used to identify the optimal pd value
are discussed in Section 4.2. The only remaining parameter of DLS-MC, maxSteps, was
set to 100 000 000, in order to maximise the probability of reaching the target clique size in
every run.
The results from these experiments are displayed in Table 1. For each benchmark
instance we show the DLS-MC performance results (averaged over 100 independent runs)
for the complete set of 80 DIMACS benchmark instances. Note that DLS-MC finds optimal
(or best known) solutions with a success rate of 100% over all 100 runs per instance for 77
of the 80 instances; the only cases where the target clique size was not reached consistently
within the alotted maximum number of search steps (maxSteps) are:
• C2000.9, where 93 of 100 runs were successful giving a maximum clique size (average
clique size, minimum clique size) of 78 (77.93, 77);
• MANN a81, where 96 of 100 runs obtained cliques of size 1098, while the remaining
runs produced cliques of size 1097; and
• MANN a45, where all runs achieved a maximum clique size of 344.
For these three cases, the reported CPU time statistics are over successful runs only and
are shown in parentheses in Table 1. Furthermore, the expected time required by DLS-MC
to reach the target clique size is less than 1 CPU second for 67 of the 80 instances, and an

4. dmclique, ftp://dimacs.rutgers.edu in directory /pub/dsj/clique

165

Pullan & Hoos

Instance
brock200 1
brock200 2
brock200 3
brock200 4
brock400 1
brock400 2
brock400 3
brock400 4
brock800 1
brock800 2
brock800 3
brock800 4
DSJC1000 5
DSJC500 5
hamming10-2
hamming10-4
hamming6-2
hamming6-4
hamming8-2
hamming8-4
johnson16-2-4
johnson32-2-4
johnson8-2-4
johnson8-4-4
MANN a27
MANN a45
MANN a81
MANN a9
san1000
san200 0.7 1
san200 0.7 2
san200 0.9 1
san200 0.9 2
san200 0.9 3
san400 0.5 1
san400 0.7 1
san400 0.7 2
san400 0.7 3
san400 0.9 1
sanr200 0.7

BR
21
12*
15
17*
27
29*
31
33*
23
24
25
26
15*
13*
512
40
32
4
128
16*
8
16
4
14
126*
345*
1099
16
15
30
18
70
60
44
13
40
30
22
100
18

pd
2
2
2
2
15
15
15
15
45
45
45
45
2
2
5
5
5
5
5
5
5
5
5
5
3
3
3
3
85
2
2
2
2
2
2
2
2
2
2
2

CPU(s)
0.0182
0.0242
0.0367
0.0468
2.2299
0.4774
0.1758
0.0673
56.4971
15.7335
21.9197
8.8807
0.799
0.0138
0.0008
0.0089
<ǫ
<ǫ
0.0003
<ǫ
<ǫ
<ǫ
<ǫ
<ǫ
0.0476
(51.9602)
(264.0094)
<ǫ
8.3636
0.0029
0.0684
0.0003
0.0002
0.0015
0.1641
0.1088
0.2111
0.4249
0.0029
0.002

Steps
14091
11875
21802
30508
955520
205440
74758
28936
10691276
3044775
4264921
1731725
91696
2913
1129
1903
43
3
244
31
7
15
3
21
41976
(16956750)
(27840958)
21
521086
1727
33661
415
347
1564
26235
29635
57358
113905
1820
1342

Sols.
2
1
1
1
1
1
1
1
1
1
1
1
25
42
2
100
2
83
100
92
100
100
66
29
100
(100)
(96)
99
1
1
2
1
1
1
1
1
1
1
1
13

Instance
sanr200 0.9
sanr400 0.5
sanr400 0.7
C1000.9
C125.9
C2000.5
C2000.9
C250.9
C4000.5
C500.9
c-fat200-1
c-fat200-2
c-fat200-5
c-fat500-1
c-fat500-10
c-fat500-2
c-fat500-5
gen200 p0.9 44
gen200 p0.9 55
gen400 p0.9 55
gen400 p0.9 65
gen400 p0.9 75
keller4
keller5
keller6
p hat1000-1
p hat1000-2
p hat1000-3
p hat1500-1
p hat1500-2
p hat1500-3
p hat300-1
p hat300-2
p hat300-3
p hat500-1
p hat500-2
p hat500-3
p hat700-1
p hat700-2
p hat700-3

BR
42
13
21
68
34*
16
78
44*
18
57
12
24
58
14
126
26
64
44*
55*
55
65
75
11*
27
59
10
46
68
12*
65
94
8*
25*
36*
9
36
50
11*
44*
62

pd
2
2
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

CPU(s)
0.0127
0.0393
0.023
4.44
0.0001
0.9697
(193.224)
0.0009
181.2339
0.1272
0.0002
0.001
0.0002
0.0004
0.0015
0.0004
0.002
0.001
0.0003
0.0268
0.001
0.0005
<ǫ
0.0201
170.4829
0.0034
0.0024
0.0062
2.7064
0.0061
0.0103
0.0007
0.0002
0.0007
0.001
0.0005
0.0023
0.0194
0.001
0.0015

Steps
15739
9918
8475
1417440
158
50052
(29992770)
845
5505536
72828
24
291
118
45
276
49
301
1077
369
18455
716
402
31
4067
11984412
230
415
1579
126872
730
1828
133
87
476
114
200
1075
1767
251
525

Sols.
18
4
61
70
94
93
(91)
85
93
3
14
1
3
19
3
18
3
4
4
1
1
1
98
100
100
82
87
23
1
90
98
13
42
10
48
14
36
2
72
85

Table 1: DLS-MC performance results, averaged over 100 independent runs, for the complete set of DIMACS benchmark instances. The maximum known clique size for
each instance is shown in the BR column (marked with an asterisk where proven to
be optimal); pd is the optimised DLS-MC penalty delay for each instance; CPU(s)
is the run-time in CPU seconds, averaged over all successful runs, for each instance. Average CPU times less than 0.0001 seconds are shown as < ǫ; ‘Steps’ is
the number of vertices added to the clique, averaged over all successful runs, for
each instance; ‘Sols.’ is the total number of distinct maximum sized cliques found
for each instance. All runs achieved the best known cliques size shown with the
exception of: C2000.9, where 93 of 100 runs were successful giving a maximum
clique size (average clique size, minimum clique size) of 78(77.93, 77); MANN a81,
where 96 of 100 runs obtained 1098 giving 1098(1097.96, 1097); and MANN a45,
where all runs achieved a maximum clique size of 344.

166

Dynamic Local Search for Max-Clique Problem

expected run-time of more than 10 CPU seconds is only required for 8 of the 13 remaining
instances, all of which have at least 800 vertices. Finally, the variation coefficients (stddev/mean) of the run-time distributions (measured in search steps, in order to overcome
inaccuracies inherent to extremely small CPU times) for the instances on which 100% success rate was obtained were found to reach average and maximum values of 0.86 and 1.59,
respectively.
It may be interesting to note that the time-complexity of search steps in DLS-MC
is generally very low. As an indicative example, brock800 1 with 800 vertices, 207 505
edges and a maximum clique size of 23 vertices, DLS-MC performs, on average, 189 235
search steps (i.e., additions to the current clique) per CPU second. Generally, the timecomplexity of DLS-MC steps increases with the size of the improving (NI ) and level (NL )
neighbour sets as well as, to a lesser degree, with the maximum clique size. This relationship
can be seen from Table 2 which shows, for the (randomly generated) DIMACS C∗.9 and
brock∗ 1 instances, how the performance of DLS-MC in terms of search steps per CPU
second decreases as the number of vertices (and hence the size of NI , NL ) increases.
Instance
C125.9
C250.9
C500.9
C1000.9
C2000.9
brock200 1
brock400 1
brock800 1

Vertices
125
250
500
1000
2000
200
400
800

Edges
6963
27984
112332
450079
1799532
14834
59723
207505

BR
34
44
57
68
78
21
27
23

DLS-MC pd
1
1
1
1
1
2
15
45

Steps / Second
1587399
939966
572553
319243
155223
774231
428504
189236

Table 2: Average number of DLS-MC search steps per CPU second (on our reference machine) over 100 runs for the DIMACS C∗.9 and brock∗ 1 instances. The ‘BR’ and
‘DLS-MC pd’ figures from Table 1 are also shown, as these factors have a direct
impact on the performance of DLS-MC. That is, as BR increases, the greater the
overhead in maintaining the sets within DLS-MC; furthermore, larger pd values
cause higher overhead for maintaing penalties, because more vertices tend to be
penalised. The C∗.9 instances are randomly generated with an edge probability
of 0.9, while the brock∗ 1 instances are constructed so as to ‘hide’ the maximum
clique and have considerably lower densities (i.e., average number of edges per
vertex). The scaling of the average number of search steps per CPU second performed by DLS-MC on the C∗.9 instances only, running on our reference machine,
can be approximated as 9 · 107 · n−0.8266 , where n is the number of vertices in the
given graph (this approximation achieves an R2 value of 0.9941).
A more detailed analysis of DLS-MC’s performance in terms of implementation-independent
measures of run-time, such as search steps or iteration counts, is beyond the scope of this
work, but could yield useful insights in the future.
3.2 Comparative Results
The results reported in the previous section demonstrate clearly that DLS-MC achieves
excellent performance on the standard DIMACS benchmark instances. However, a com167

Pullan & Hoos

parative analysis of these results, as compared to the results found in the literature on
other state-of-the-art MAX-CLIQUE algorithms, is not a straight-forward task because of
differences in:
• Computing Hardware: To date, computing hardware has basically been documented in terms of CPU speed which only allows a very basic means of comparison
(i.e., by scaling based on the computer CPU speed which, for example, takes no account of other features, such as memory caching, memory size, hardware architecture,
etc.). Unfortunately, for some algorithms, this was the only realistic option available
to us for this comparison.
• Result Reporting Methodology: Most empirical results on the performance of
MAX-CLIQUE algorithms found in the literature are in the form of statistics on the
clique size obtained after a fixed run-time. To conduct performance comparisons on
such data, care must be taken to avoid inconclusive situations in which an algorithm
A achieves larger clique sizes than another algorithm B, but at the cost of higher runtimes. It is important to realise that the relative performance of A and B can vary
substantially with run-time; while A may reach higher clique sizes than B for relatively
short run-times, the opposite could be the case for longer run-times. Finally, seemingly
small differences in clique size may in fact represent major differences in performance,
since (as in many hard optimisation problems) finding slightly sub-optimal cliques is
typically substantially easier than finding maximal cliques. For example, for C2000.9,
the average time needed to find a clique of size 77 (with 100% success rate) is 6.419
CPU seconds, whereas reaching the maximum clique size of 78 (with 93% success
rate) requires on average (over successful runs only) of 193.224 CPU seconds.
• Termination Criteria: Some MAX-CLIQUE algorithms (such as DAGS) do not
terminate upon reaching a given target clique size, but will instead run for a given
number of search steps or fixed amount of CPU time, even if an optimal clique is
encountered early in the search. It would obviously be highly unfair to directly compare published results for such algorithms with those of DLS-MC, which terminates
as soon as it finds the user supplied target clique size.
Therefore, to confirm that DLS-MC represents a significant improvement over previous
state-of-the-art MAX-CLIQUE algorithms, we conducted further experiments and analyses
designed to yield performance results for DLS-MC that can be more directly compared with
the results of other MAX-CLIQUE algorithms. In particular, we compared DLS-MC with
the following MAX-CLIQUE algorithms: DAGS (Grosso et al., 2004), GRASP (Resende,
Feo, & Smith, 1998) (using the results contained in Grosso et al., 2004), k-opt (Katayama
et al., 2004), RLS (Battiti & Protasi, 2001), GENE (Marchiori, 2002), ITER (Marchiori,
2002) and QUALEX-MS (Busygin, 2002). To rank the performance of MAX-CLIQUE
algorithms and to determine the dominant algorithm for each of our benchmark instances,
we used a set of criteria that are based, primarily, on the quality of the solution and then,
when this is deemed equivalent, on the CPU time requirements of the algorithms. These
criteria are shown, in order of application, in Table 3.

168

Dynamic Local Search for Max-Clique Problem

1. If an algorithm is the only algorithm to find the largest known maximum clique for an instance then it is
ranked as the dominant algorithm for that instance.
2. If more than one algorithm achieves a 100% success rate for an instance then the algorithm with the lowest
average (scaled) CPU time becomes the dominant algorithm for that instance.
3. If a single algorithm achieves a 100% success rate for an instance then that algorithm becomes the dominant
algorithm for that instance.
4. If no algorithm achieves a 100% success rate for an instance, then the algorithm that achieves the largest
size clique, has the highest average clique size and the lowest average CPU time becomes the
dominant algorithm for that instance.
5. If, for any instance, no algorithm meets any of the four criteria listed above, then no conclusion can be
drawn about which is the dominant algorithm for that instance.

Table 3: The criteria used for ranking MAX-CLIQUE algorithms.

Instance
brock200 1
brock200 2
brock200 3
brock200 4
brock400 1
brock400 2
brock400 3
brock400 4
brock800 1
brock800 2
brock800 3
brock800 4
C1000.9
C2000.9
C4000.5
C500.9
gen200 p0.9 44
gen400 p0.9 55
gen400 p0.9 65
gen400 p0.9 75
keller6
MANN a45
p hat1000-3
p hat1500-1
san200 0.7 2
san400 0.7 3
sanr200 0.9

DLS-MC
Clique size
CPU(s)
21
0.0182
12
0.0242
15
0.0367
17
0.0468
27
2.2299
29
0.4774
31
0.1758
33
0.0673
23
56.4971
24
15.7335
25
21.9197
26
8.8807
68
4.44
78(77.93,77)
193.224
18 181.2339
57
0.1272
44
0.001
55
0.0268
65
0.001
75
0.0005
59 170.4829
344
51.9602
68
0.0062
12
2.7064
18
0.0684
22
0.4249
42
0.0127

DAGS
Clique size
SCPU(s)
21
0.256
12
0.064
15
0.064
17(16.8,16)
0.192
27(25.35,24)
1.792
29(28.1,24)
1.792
31(30.7,25)
1.792
33
1.792
23(20.95,20),
10.624
24(20.8,20)
10.752
25(22.2,21)
10.88
26(22.6,20)
10.816
68(65.95,65)
94.848
76(75.4,74)
1167.36
18(17.5,17)
2066.56
56(55.85,55)
8.64
44(41.15,40)
0.576
53(51.8,51)
4.608
65(55.4,51)
4.672
75(55.2,52)
4.992
57(56.4,56)
7888.64
344(343.95) 1229.632
68(67.85,67)
71.872
12(11.75,11)
19.904
18(17.9,17)
0.192
22(21.7,19)
1.28
42(41.85,41)
0.576

GRASP
Clique size
SCPU(s)
21
4.992
12
1.408
14
42.56
17
3.328
25
14.976
25
15.232
31(26.2,25)
14.848
25
15.232
21
32
21
32.96
22(21.85,21)
34.112
21
33.152
67(66.1,65)
154.368
75(74.3,73)
466.368
18(17.75,17)
466.944
56
80.896
44(41.95,41)
11.776
53(52.25,52)
35.264
65(64.3,63)
34.56
74(72.3,69)
36.16
55(53.5,53) 1073.792
336(334.5,334)
301.888
68
237.568
11
23.424
18(16.55,15)
3.264
21(18.8,17)
9.856
42
12.608

Table 4: Performance comparison of DLS-MC, DAGS and GRASP for selected DIMACS
instances. The SCPU columns contain the scaled DAGS and GRASP average
run-times in CPU seconds; DAGS and GRASP results are based on 20 runs per
instance, and DLS-MC results are based on 100 runs per instance. In cases where
the best known result was not found in all runs, clique size entries are in the format
‘maximum clique size (average clique size, minimum clique size)’. DLS-MC is the
dominant algorithm for all instances in this table.

169

Pullan & Hoos

Table 4 contrasts performance results for DAGS and GRASP from the literature (Grosso
et al., 2004) with the respective performance results for DLS-MC. Since the DAGS and
GRASP runs had been performed on a 1.4 GHz Pentium IV CPU, while DLS-MC ran on
our 2.2 GHz Pentium IV reference machine, we scaled their CPU times by a factor or 0.64.
(Note that this is based on the assumption of a linear scaling of run-time with CPU clock
speed; in reality, the speedup is typically significantly smaller.) Using our ranking criteria,
this data shows that DLS-MC dominates both DAGS and GRASP for all the benchmark
instances listed in Table 4. To confirm this ranking, we modified DAGS so it terminated
as soon a given target clique size was reached (this is the termination condition used in
DLS-MC) and performed a direct comparison with DLS-MC on all 80 DIMACS instances,
running both algorithms on our reference machine. As can be seen from the results of
this experiment, shown in Table 5, DLS-MC dominates DAGS on all but one instance (the
exception being san1000).
Table 6 shows performance results for DLS-MC as compared to results for k-opt (Katayama
et al., 2004), GENE (Marchiori, 2002), ITER (Marchiori, 2002) and RLS (Battiti & Protasi,
2001) from the literature. To roughly compensate for differences in CPU speed, we scaled
the CPU times for k-opt, GENE and ITER by a factor of 0.91 (these had been obtained on
a 2.0 GHz Pentium IV) and those for RLS (measured on a 450 MHz Pentium II CPU) by
0.21. Using the ranking criteria in Table 3, RLS is the dominant algorithm for instances
keller6 and MANN a45, k-opt is the dominant algorithm for MANN a81 and DLS-MC is
the dominant algorithm, with the exception of C2000.9, for the remainder of the DIMACS
instances listed in Table 6. To identify the dominant algorithm for C2000.9, a further experiment was performed, running DLS-MC with its maxSteps parameter (which controls
the maximum allowable run-time) reduced to the point where the average clique size for
DLS-MC just exceeded that reported for RLS. In this experiment, DLS-MC reached the
optimum clique size of 78 in 58 of 100 independent runs with an average and minimum
clique size of 77.58 and 77, respectively and an average run-time of 85 CPU sec (taking into
account all runs). This establishes DLS-MC as dominant over RLS and k-opt on instance
C2000.9.
Analagous experiments were performed to directly compare the performance of DLSMC and k-opt on selected DIMACS benchmark instances; the results, shown in Table 7,
confirm that DLS-MC dominates k-opt for these instances.
Finally, Table 8 shows performance results for DLS-MC in comparison with results
for QUALEX-MS from the literature (Busygin, 2002); the CPU times for QUALEX-MS
have been scaled by a factor of 0.64 to compensate for differences in CPU speed (1.4 GHz
Pentium IV CPU vs our 2.2 GHz Pentium IV reference machine). Using the ranking
criteria in Table 3, QUALEX-MS dominates DLS-MC for instances brock400 1, brock800 1,
brock800 2 and brock800 3, while DLS-MC dominates QUALEX-MS for the remaining 76
of the 80 DIMACS instances.

170

Dynamic Local Search for Max-Clique Problem

Instance
brock200 1
brock200 2
brock200 3
brock200 4
brock400 1
brock400 2
brock400 3
brock400 4
brock800 1
brock800 2
brock800 3
brock800 4
DSJC1000 5
DSJC500 5
C1000.9
C125.9
C2000.9
C2000.5
C250.9
C4000.5
C500.9
c-fat200-1
c-fat200-2
c-fat200-5
c-fat500-1
c-fat500-10
c-fat500-2
c-fat500-5
gen200 p0.9 44
gen200 p0.9 55
gen400 p0.9 55
gen400 p0.9 65
gen400 p0.9 75
hamming10-2
hamming10-4
hamming6-2
hamming6-4
hamming8-2
hamming8-4
johnson16-2-4

DLS-MC
Success CPU(s)
100
0.0182
100
0.0242
100
0.0367
100
0.0468
100
2.2299
100
0.4774
100
0.1758
100
0.0673
100
56.4971
100
15.7335
100
21.9197
100
8.8807
100
0.799
100
0.0138
100
4.44
100
0.0001
93
193.224
100
0.9697
100
0.0009
100 181.2339
100
0.1272
100
0.0002
100
0.001
100
0.0002
100
0.0004
100
0.0015
100
0.0004
100
0.002
100
0.001
100
0.0003
100
0.0268
100
0.001
100
0.0005
100
0.0008
100
0.0089
100
<ǫ
100
<ǫ
100
0.0003
100
<ǫ
100
<ǫ

DAGS
Success CPU(s)
93
0.1987
98
0.1252
100
0.1615
82
0.2534
35
3.1418
75
2.3596
92
2.2429
99
1.653
9
20.0102
20
18.747
19
19.1276
45
16.9227
80
7.238
100
0.1139
5
2.87
100
0.0024
5 2.870608
100
17.9247
99
0.1725
−
−
4
16.2064
100
0.0002
100
0.0004
100
0.0012
100
0.0005
100
0.0067
100
0.0009
100
0.0028
14
0.9978
100
0.0267
0
9.0372
27
7.1492
14
8.6018
100
0.1123
100
3.8812
100
0.0003
100
<ǫ
100
0.0039
100
0.0006
100
0.0003

Instance
johnson32-2-4
johnson8-2-4
johnson8-4-4
keller4
keller5
keller6
MANN a27
MANN a45
MANN a81
MANN a9
p hat1000-1
p hat1000-2
p hat1000-3
p hat1500-1
p hat1500-2
p hat1500-3
p hat300-1
p hat300-2
p hat300-3
p hat500-1
p hat500-2
p hat500-3
p hat700-1
p hat700-2
p hat700-3
san1000
san200 0.7 1
san200 0.7 2
san200 0.9 1
san200 0.9 2
san200 0.9 3
san400 0.5 1
san400 0.7 1
san400 0.7 2
san400 0.7 3
san400 0.9 1
sanr200 0.7
sanr200 0.9
sanr400 0.5
sanr400 0.7

DLS-MC
Success CPU(s)
100
<ǫ
100
<ǫ
100
<ǫ
100
<ǫ
100
0.0201
100 170.4829
100
0.0476
100
51.9602
96 264.0094
100
<ǫ
100
0.0034
100
0.0024
100
0.0062
100
2.7064
100
0.0061
100
0.0103
100
0.0007
100
0.0002
100
0.0007
100
0.001
100
0.0005
100
0.0023
100
0.0194
100
0.001
100
0.0015
100
8.3636
100
0.0029
100
0.0684
100
0.0003
100
0.0002
100
0.0015
100
0.1641
100
0.1088
100
0.2111
100
0.4249
100
0.0029
100
0.002
100
0.0127
100
0.0393
100
0.023

DAGS
Success CPU(s)
100
0.0042
100
<ǫ
100
0.0001
100
0.0009
100
0.079
−
−
100
0.1886
94
8.194
−
−
100
0.0003
100
0.0353
100
0.0984
81
37.2
69
15.609
100
0.4025
100
6.3255
100
0.0078
100
0.0033
100
0.0609
100
0.0099
100
0.0215
100
0.4236
100
0.1217
100
0.0415
100
0.1086
100
0.967
100
0.0029
92
0.1001
100
0.0023
100
0.0368
100
0.0572
100
0.0336
100
0.0089
100
0.0402
90
0.5333
100
0.0322
100
0.0239
83
0.3745
93
0.231
100
0.1345

Table 5: Success rates and average CPU times for DLS-MC and DAGS (based on 100 runs
per instance). For the 80 DIMACS instances, DLS-MC had a superior success rate
for 31 instances and, with exception of san1000, required less or the same CPU
time than DAGS for all other instances. Entries of ‘−’ signify that the runs were
terminated because of excessive CPU time requirements. To obtain a meaningful
comparison for DLS-MC and DAGS, for MANN a45 and MANN a81, 344 and
1098 respectively were used as best known results in producing this table. For
both DLS-MC and DAGS, the average CPU time is over successful runs only.
Using the ranking criteria of this study, DAGS is the dominant algorithm for the
san1000 instance, while DLS-MC is the dominant algorithm for all other instances.

171

Pullan & Hoos

DLS-MC
Instance
Clique size
brock200 2
12
brock200 4
17
brock400 2
29
brock400 4
33
brock800 2
24
brock800 4
26
C1000.9
68
C125.9
34
C2000.5
16
C2000.9
78(77.9,77)
C250.9
44
C4000.5
18
C500.9
57
DSJC1000 5
15
DSJC500 5
13
gen200 p0.9 44
44
gen200 p0.9 55
55
gen400 p0.9 55
55
gen400 p0.9 65
65
gen400 p0.9 75
75
hamming10-4
40
hamming8-4
16
keller4
11
keller5
27
keller6
59
MANN a27
126
MANN a45
344
MANN a81 1098(1097.96,1097)
p hat1500-1
12
p hat1500-2
65
p hat1500-3
94
p hat300-1
8
p hat300-2
25
p hat300-3
36
p hat700-1
11
p hat700-2
44
p hat700-3
62

k-opt

RLS

CPU(s)
Clique size
SCPU(s)
Clique size
SCPU(s)
0.0242
11
0.02184
12
2.01705
0.0468
16
0.01911
17
4.09311
0.4774
25(24.6,24)
0.28028 29(26.063,25)
8.83911
0.0673
25
0.18291 33(32.423,25) 22.81398
15.7335
21(20.8,20)
2.16034
21
0.99519
8.8807
21(20.5,20)
2.50796
21
1.40616
4.44
67
6.3063
68
8.7486
0.0001
34
0.00091
34
0.00084
0.9697
16 13.01846
16
2.09496
193.224
77(75.1,74) 66.14608 78(77.575,77) 172.90518
0.0009
44
0.05642
44
0.00609
181.2339
17 65.27885
18 458.44869
0.1272
57(56.1,56)
0.82264
57
0.65604
0.799
15
5.77941
15
1.35513
0.0138
13
0.12103
13
0.04074
0.001
44
0.06643
44
0.00777
0.0003
55
0.00273
55
0.00336
0.0268
53(52.3,51)
0.56238
55
0.25284
0.001
65
0.24934
65
0.0105
0.0005
75
0.16926
75
0.01071
0.0089
40
0.58422
40
0.01638
<ǫ
16
0.00182
16
0.00063
<ǫ
11
0.00091
11
0.00042
0.0201
27
0.07371
27
0.03591
170.4829
57(55.5,55) 125.03218
59 39.86094
0.0476
126
0.03276
126
0.65436
51.9602
344(343.6,343)
5.34716 345(343.6,343)
83.7417
264.0094 1099(1098.1,1098)
84.903
1098 594.4722
2.7064
12 15.43997
12
6.35754
0.0061
65
0.42224
65
0.03318
0.0103
94
2.093
94
0.04032
0.0007
8
0.00637
8
0.00378
0.0002
25
0.00546
25
0.00126
0.0007
36
0.0273
36
0.00441
0.0194
11
0.57876
11
0.03906
0.001
44
0.04914
44
0.00588
0.0015
62
0.08008
62
0.00735

GENE
ITER
Avg.
Avg.
Clique size Clique size
10.5
10.5
15.4
15.5
22.5
23.2
23.6
23.1
19.3
19.1
18.9
19
61.6
61.6
33.8
34
14.2
14.2
68.2
68.7
42.8
43
15.4
15.6
52.2
52.7
13.3
13.5
12.2
12.1
39.7
39.5
50.8
48.8
49.7
49.1
53.7
51.2
60.2
62.7
37.7
38.8
16
16
11
11
26
26.3
51.8
52.7
125.6
126
342.4
343.1
1096.3
1097
10.8
10.4
63.8
63.9
92.4
93
8
8
25
25
34.6
35.1
9.8
9.9
43.5
43.6
60.4
61.8

Table 6: Performance of DLS-MC, k-opt, RLS, GENE and ITER for selected DIMACS
instances. The SCPU columns contain the scaled average run-time in CPU seconds
for k-opt and RLS; DLS-MC and RLS results are based on 100 runs per instance,
and the k-opt, GENE and ITER results are based on 10 runs per instance. Using
the ranking criteria of this study, RLS is the dominant algorithm for instances
MANN a45 and keller6, while DLS-MC is the dominant algorithm for all other
instances.

172

Dynamic Local Search for Max-Clique Problem

DLS-MC
Instance
Clique size CPU(s)
brock400 2 25(24.69,24) 0.1527
brock400 4
25 0.0616
brock800 2 21(20.86,20) 1.7235
brock800 4 21(20.65,20) 1.0058

k-opt
DLS-MC
k-opt
Clique size SCPU(s) Instance Clique size CPU(s) Clique size SCPU(s)
25(24.6,24)
0.280 C1000.9 67(66.07,64) 0.0373
67(66,65)
6.306
25
0.183 C2000.9 77(75.33,74) 0.6317 77(75.1,74)
66.146
21(20.8,20)
2.160 C4000.5
17 1.3005
17
65.279
21(20.5,20)
2.508
keller6 57(55.76,54) 2.6796 57(55.5,55) 125.032

Table 7: Performance of DLS-MC and k-opt where the DLS-MC parameter maxSteps has
been reduced to the point where the clique size results are comparable to those for
k-opt. The CPU(s) values for DLS-MC include the unsuccessful runs; DLS-MC
results are based on 100 runs and k-opt results on 10 runs (per instance).
DLS-MC
QUALEX-MS
DLS-MC
QUALEX-MS
Instance
Clique size CPU(s) Clique size SCPU(s)
Instance
Clique size
CPU(s) Clique size SCPU(s)
brock200 1
21
0.0182
21
0.64 johnson32-2-4
16
<ǫ
16
5.12
brock200 2
12
0.0242
12
< 0.64 johnson8-2-4
4
<ǫ
4
< 0.64
brock200 3
15
0.0367
15
0.64 johnson8-4-4
14
<ǫ
14
< 0.64
brock200 4
17
0.0468
17
< 0.64
keller4
11
<ǫ
11
0.64
brock400 1
27
2.2299
27
1.28
keller5
27
0.0201
26
10.24
brock400 2
29
0.4774
29
1.92
keller6
59 170.4829
53
826.24
brock400 3
31
0.1758
31
1.28
MANN a27
126
0.0476
125
0.64
brock400 4
33
0.0673
33
1.28
MANN a45
344 51.9602
342
10.88
brock800 1
23 56.4971
23
11.52
MANN a81 1098(1097.96,1097) 264.0094
1096
305.28
brock800 2
24 15.7335
24
11.52
MANN a9
16
<ǫ
16
< 0.64
brock800 3
25 21.9197
25
11.52
p hat1000-1
10
0.0034
10
17.92
brock800 4
26
8.8807
26
11.52
p hat1000-2
46
0.0024
45
21.76
C1000.9
68
4.44
64
17.28
p hat1000-3
68
0.0062
65
20.48
C125.9
34
0.0001
34
< 0.64
p hat1500-1
12
2.7064
12
60.8
C2000.5
16
0.9697
16
177.92
p hat1500-2
65
0.0061
64
71.04
C2000.9 78(77.93,77) 193.224
72
137.6
p hat1500-3
94
0.0103
91
69.12
C250.9
44
0.0009
44
0.64
p hat300-1
8
0.0007
8
0.64
C4000.5
18 181.2339
17
1500.8
p hat300-2
25
0.0002
25
0.64
C500.9
57
0.1272
55
2.56
p hat300-3
36
0.0007
35
0.64
c-fat200-1
12
0.0002
12
< 0.64
p hat500-1
9
0.001
9
1.92
c-fat200-2
24
0.001
24
< 0.64
p hat500-2
36
0.0005
36
2.56
c-fat200-5
58
0.0002
58
< 0.64
p hat500-3
50
0.0023
48
2.56
c-fat500-1
14
0.0004
14
0.64
p hat700-1
11
0.0194
11
6.4
c-fat500-10
126
0.0015
126
1.28
p hat700-2
44
0.001
44
7.68
c-fat500-2
26
0.0004
26
1.28
p hat700-3
62
0.0015
62
7.04
c-fat500-5
64
0.002
64
1.28
san1000
15
8.3636
15
16.0
DSJC1000 5
15
0.799
14
23.04 san200 0.7 1
30
0.0029
30
0.64
DSJC500 5
13
0.0138
13
3.2 san200 0.7 2
18
0.0684
18
< 0.64
gen200 p0.9 44
44
0.001
42
< 0.64 san200 0.9 1
70
0.0003
70
< 0.64
gen200 p0.9 55
55
0.0003
55
0.64 san200 0.9 2
60
0.0002
60
0.64
gen400 p0.9 55
55
0.0268
51
1.28 san200 0.9 3
44
0.0015
40
< 0.64
gen400 p0.9 65
65
0.001
65
1.28 san400 0.5 1
13
0.1641
13
1.28
gen400 p0.9 75
75
0.0005
75
1.28 san400 0.7 1
40
0.1088
40
1.92
hamming10-2
512
0.0008
512
24.32 san400 0.7 2
30
0.2111
30
1.28
hamming10-4
40
0.0089
36
28.8 san400 0.7 3
22
0.4249
18
1.28
hamming6-2
32
<ǫ
32
< 0.64 san400 0.9 1
100
0.0029
100
1.28
hamming6-4
4
<ǫ
4
< 0.64
sanr200 0.7
18
0.002
18
0.64
hamming8-2
128
0.0003
128
< 0.64
sanr200 0.9
42
0.0127
41
< 0.64
hamming8-4
16
<ǫ
16
0.64
sanr400 0.5
13
0.0393
13
1.28
johnson16-2-4
8
<ǫ
8
< 0.64
sanr400 0.7
21
0.023
20
1.28

Table 8: Performance of DLS-MC and QUALEX-MS. The SCPU column contains the
scaled run-time for QUALEX-MS in CPU seconds; DLS-MC results are based
on 100 runs per instance. Using the ranking criteria of this study, QUALEX-MS
is the dominant algorithm for instances brock400 1, brock800 1, brock800 2 and
brock800 3, while DLS-MC is the dominant algorithm for all other instances.
173

Pullan & Hoos

Overall, the results from these comparative performance evaluations can be summarised
as follows:
• QUALEX-MS is dominant for the brock400 1, brock800 1, brock800 2 and brock800 3
DIMACS instances.
• RLS is the dominant algorithm for the MANN a45 and keller6 DIMACS instances.
• DAGS is the dominant algorithm for the san1000 DIMACS instance.
• k-opt is the dominant algorithm for the MANN a81 DIMACS instance.
• DLS-MC is the dominant algorithm for the remaining 72 DIMACS instances.
In addition, within the alotted run-time and number of runs, DLS-MC obtained the current best known results for all DIMACS instances with the exceptions of MANN a45 and
MANN a81.

4. Discussion
To gain a deeper understanding of the run-time behaviour of DLS-MC and the efficacy of
its underlying mechanisms, we performed additional empirical analyses. Specifically, we
studied the variability in run-time between multiple independent runs of DLS-MC on the
same problem instance; the role of the vertex penalties in general and, in particular, the
impact of the penalty delay parameter on the performance and behaviour of DLS-MC; and
the frequency of pertubation as well as the role of the perturbation mechanism.
These investigations were performed using two DIMACS instances, C1000.9 and brock800 1.
These instances were selected because, firstly, they are of reasonable size and difficulty. Secondly, C1000.9 is a randomly generated instance where the vertices in the optimal maximum
clique have predominantly higher vertex degree than the average vertex degree (intuitively
it would seem reasonable that, for a randomly generated problem, vertices in the optimal
maximum clique would tend to have higher vertex degrees). For brock800 1, on the other
hand, the vertices in the optimal maximum clique have predominantly lower-than-average
vertex degree. (Note that the DIMACS brock instances were created in an attempt to defeat
greedy algorithms that used vertex degree for selecting vertices Brockington & Culberson,
1996).
This fundamental difference is further highlighted by the results of a quantitative analysis of the maximum cliques for these instances, which showed that, for C1000.9, averaged
over all maximal cliques found by DLS-MC, the average vertex degree of vertices in the maximal cliques is 906 (standard deviation of 9) as compared to 900 (9) when averaged over all
vertices; for brock800 1, the corresponding figures were 515 (11) and 519 (13) respectively.
4.1 Variability in Run-Time
The variability of run-time between multiple independent runs on a given problem is an important aspect of the behaviour of SLS algorithms such as DLS-MC. Following the methology of Hoos and Stützle (2004), we studied this aspect based on run-time distributions
(RTDs) of DLS-MC on our two reference instances.
174

Dynamic Local Search for Max-Clique Problem

As can be seen from the empirical RTD graphs shown in Figure 3 (each based on
100 independent runs that all reached the respective best known clique size), DLS-MC
shows a large variability in run-time. Closer investigation shows that the RTDs are quite
well approximated by exponential distributions (a Kolmogorov-Smirnov goodness-of-fit test
failed to reject the null hypothesis that the sampled run-times stem from the exponential
distributions shown in the figure at a standard confidence level of α = 0.05 with p-values
between 0.16 and 0.62). This observation is consistent with similar results for other highperformance SLS algorithms, e.g., for SAT (Hoos & Stützle, 2000) and scheduling problems
(Watson, Whitley, & Howe, 2005). As a consequence, performing multiple independent
runs of DLS-MC in parallel will result in close-to-optimal parallelisation speedup (Hoos
& Stützle, 2004). Similar observation were made for most of the other difficult DIMACS
instances.
1

1

empirical RLD for DLS-MC
ed[2.5*105]

0.9

0.8

0.8

0.7

0.7

0.6

0.6

P(solve)

P(solve)

0.9

0.5
0.4

0.5
0.4

0.3

0.3

0.2

0.2

0.1

0.1

0
1000

10000

100000

1e+006

empirical RTD for DLS-MC
ed[0.85]

0
0.001

1e+007

0.01

run-time [search steps]
1

1

empirical RLD for DLS-MC
ed[0.7*107]

0.9

0.8

0.8

0.7

0.7

0.6

0.6

P(solve)

P(solve)

0.9

0.5
0.4

0.3
0.2

0.1

0.1
1e+006
1e+007
run-time [search steps]

10

1e+008

1e+009

empirical RTD for DLS-MC
ed[35]

0.4

0.2

100000

1

0.5

0.3

0
10000

0.1
run-time [CPU sec]

0
0.01

0.1

1
10
run-time [CPU sec]

100

1000

Figure 3: Run-time distributions for DLS-MC applied to C1000.9 (top) and brock800 1
(bottom), measured in search steps (left) and CPU seconds (right) on our reference machine (based on 100 independent runs each of which reached the best
known clique size); these empirical RTDs are well approximated by exponential
distributions, labelled ed[m](x) = 1 − 2−x/m in the plots.
4.2 Penalty Delay Parameter and Vertex Penalties
The penalty delay parameter pd specifies the number of penalty increase iterations that must
occur in DLS-MC before there is a penalty decrease (by 1) for all vertices that currently have
175

Pullan & Hoos

Vertex frequency

a penalty. For the MAX-CLIQUE problem, pd basically provides a mechanism for focusing
on lower degree vertices when constructing current cliques. With pd = 1 (i.e., no penalties),
the frequency with which vertices are in the improving neighbour / level neighbour sets will
basically be solely dependent on their degree. Increasing pd overcomes this bias towards
higher degree vertices, as it allows their penalty values to increase (as they are more often
in the current clique), which inhibits their selection for the current clique. This in turn
allows lower degree vertices to become part of the current clique. This effect of the penalty
delay parameter is illustrated in Figure 4, which shows the correlation between the degree
of the vertices and their frequency of being included in the current clique immediately prior
to a perturbation being performed within DLS-MC.

C1000.9 − pd = 1
0.4
0.2

Vertex frequency

0
86

0.4

87

88

89
90
Vertex degree

91

92

93

brock800_1 − pd = 1

0.3
0.2
0.1
0
58

60

62

64
Vertex degree

66

68

70

62

64
Vertex degree

66

68

70

Vertex frequency

0.25
brock800_1 − pd = 45
0.2
0.15
0.1
0.05
58

60

Figure 4: Correlation between the vertex degree and the frequency with which vertices
were present in the clique immediately prior to each DLS-MC perturbation. For
C1000.9 and brock800 1, with pd = 1, the higher degree vertices tend to have a
higher frequency of being present in the clique immediately prior to each DLS-MC
perturbation. For brock800 1, with pd = 45, the frequency of being present in the
clique immediately prior to each DLS-MC perturbation is almost independent of
the vertex degree.
Currently, pd needs to be tuned to a family (or, in the case of the brock instances, a
sub-family) of instances. In general, this could be done in a principled way based on RTD
graphs, but for DLS-MC, which is reasonably robust with regard to the exact value of the
parameter (as shown by Figures 5 and 6), the actual tuning process was a simple, almost
interactive process and did not normally require evaluating RTD graphs. Still, fine-tuning
based on RTD data could possibly result in further, minor performance improvements.

176

Dynamic Local Search for Max-Clique Problem

100

% Success rate

95
90
85
80
75
70

0

10

20

30
Penalty delay

40

50

60

0

10

20

30
Penalty delay

40

50

60

Median processor time

300
250
200
150
100
50
0

Figure 5: Success rate and median CPU time of DLS-MC as a function of the penalty delay
parameter, pd, for the benchmark instance brock800 1. Each data point is based
on 100 independent runs.

Cumulative success rate

100
80

pd = 35
pd = 45
pd = 50

60
40
20
0
4
10

5

6

10

7

10
Steps

8

10

10

Cumulative success rate

100
80

pd = 35
pd = 45
pd = 50

60
40
20
0
−1
10

0

10

1

10
Processor time (seconds)

2

10

Figure 6: Run-time distributions for DLS-MC on brock800 1 for penalty delays of 35, 45
and 50, measuring run-time in search steps (top) and CPU seconds (bottom).
The performance for a penalty delay of 45 clearly dominates that for 35 and 50.

177

Pullan & Hoos

The effect of the penalty delay parameter on the vertex penalties is clearly illustrated in
Figure 7, which shows cumulative distributions of the number of penalised vertices at each
perturbation in DLS-MC, for representative runs of DLS-MC on the DIMACS brock800 1
instance, for varying values of the parameter pd. Note that for brock800 1, the optimal
pd value of 45 corresponds to the point where, on average, about 90% of the vertices have
been penalised. The role of the pd parameter is further illustrated in Figure 8, which shows
the (sorted) frequency with which vertices were present in the current clique immediately
prior to each perturbation for C1000.9 and brock800 1. Note that for both instances,
using higher penalty delay settings significanly reduces the bias towards including certain
vertices in the current clique. As previously demonstrated, without vertex penalties (i.e.,
for pd = 1), DLS-MC prefers to include high-degree vertices in the current clique, which in
the case of problem instances like C1000.9, where optimal cliques tend to consist of vertices
with higher-than-average degrees, is an effective strategy. In instances such as brock800 1,
however, where the optimal clique contains many vertices of lower-than-average degree, the
heuristic bias towards high-degree vertices is misleading and needs to be counteracted, e.g.,
by means of vertex penalties.
100
pd = 5
pd = 10
pd = 15
pd = 20
pd = 25
pd = 30
pd = 35
pd = 40
pd = 45
pd = 50
pd = 55

90

80

Cumulative frequency

70

60

50

40

30

20

10

0

0

100

200

300

400
500
Penalised vertices

600

700

800

Figure 7: Cumulative distributions of the number of penalised vertices measured at each
search perturbation over representative independent runs of DLS-MC on the DIMACS brock800 1 instance as the penalty delay parameter pd is varied (the left
most curve corresponds to pd = 5). Note that for the approx. optimal penalty
delay of pd = 45 (solid line), on average about 90% vertices are penalised (i.e.,
have a penalty value greater than zero).
Generally, by reducing the bias in the cliques visited, vertex penalties help to diversify
the search in DLS-MC. At the same time, penalties do not appear to provide a ‘learning’
mechanism through which DLS-MC identifies those vertices that should be included in
178

Dynamic Local Search for Max-Clique Problem

C1000.9
% frequency vertex in clique

0.5
pd = 1
pd = 10

0.4
0.3
0.2
0.1
0

0

100

200

300

400

500
Vertex

600

700

800

900

1000

brock800_1
% frequency vertex in clique

0.4
pd = 1
pd = 45
0.3

0.2

0.1

0

0

100

200

300

400
Vertex

500

600

700

800

Figure 8: Sorted frequency with which vertices were present in the current clique immediately prior to each DLS-MC perturbation for C1000.9 (top) and brock800 1
(bottom), based on a representative run on each problem instance. Note that by
using penalty delay values pd > 1, the bias towards using certain vertices more
frequently than others is substantially reduced.
the current clique. This is in agreement with recent results for SAPS, a high-performance
dynamic local search algorithm for SAT (Hoos & Stützle, 2004).
4.3 Perturbation Mechanism and Search Mobility
To prevent search stagnation, DLS-MC uses a perturbation mechanism that is executed
whenever its plateau search procedure has failed to lead to a clique that can be further
expanded. Since this mechanism causes major changes in the current clique, it has relatively
high time complexity. It is therefore interesting to investigate how frequently these rather
costly and disruptive perturbation steps are performed. Figure 9 shows the distribution of
the number of improving search steps (i.e., clique expansions) and plateau steps (i.e., vertex
swaps) between successive perturbation phases for a representative run of DLS-MC on the
C1000.9 instance. Analogous results for brock800 1 are shown in Figure 10. These figures
basically show the result of the interactions between the improving and plateau search steps,
the perturbation mechanism and the problem structure.

179

Pullan & Hoos

c1000.9

Cumulative frequency

100
pd = 1
pd = 2
pd = 10

80
60
40
20
0

0

10

20

30

40

50
60
Improving steps

70

80

90

100

20

30

40

50
60
Plateau swaps

70

80

90

100

Cumulative frequency

100
pd = 1
pd = 2
pd = 10

80
60
40
20
0

0

10

Figure 9: Number of improving search steps and plateau swaps between successive perturbation phases of DLS-MC for C1000.9. The graphs show the cumulative distributions of these measures collected over representative independent runs for each
pd value; the solid lines correspond to the approx. optimal penalty delay for this
instance, pd = 1.
brock800_1

Cumulative frequency

100
pd = 1
pd = 2
pd = 45

80
60
40
20
0

0

5

10

15

20
Improving steps

25

30

35

40

10

15

20
Plateau swaps

25

30

35

40

Cumulative frequency

100
pd = 1
pd = 2
pd = 45

80
60
40
20
0

0

5

Figure 10: Number of improving search steps and plateau swaps between successive perturbation phases of DLS-MC for brock800 1. The graphs show the cumulative
distributions of these measures collected over representative independent runs
for each pd value; the solid lines correspond to the approx. optimal penalty delay
for this instance, pd = 45.
180

Dynamic Local Search for Max-Clique Problem

As can be seen from this data, when compared to higher penalty delay values, pd = 1
results in significantly shorter plateau phases and somewhat longer improvement phases.
At the same time, the differences in the behaviour of DLS-MC observed for various penalty
delay values greater than one are relatively small. One explanation for this phenomenon lies
in the fact that for pd = 1, effectively no vertex penalties are used, and consequently, the
selection from the improving and level neighbours sets in each search step is less constrained.
Intuitively, this should make it easier to find exits off plateaus in the underlying search
landscape and to follow gradients for a larger number of search steps.

Whether this renders the search more efficient clearly depends on the topology of the
given search landscape. Instance C1000.9 has at least 70 optimal solutions (see Table 1), and
by construction, these optimal cliques have higher-than-average vertex degree. This suggests
that the respective search landscape has a relatively high fitness-distance correlation, which
would explain why this problem instance is relatively easy to solve and also why using the
less radical perturbation mechanism associated with pd = 1 (which adds a randomly chosen
vertex v to the current clique and removes all vertices not connected to v) provides sufficient
diversification to the search process. Instance brock800 1, on the other hand, appears to
have only a single optimal solution but many near-optimal solutions (i.e., large but nonoptimal cliques that cannot be further extended), since by construction, its optimal clique
has lower-than-average vertex degree. This suggests that the respective search landscape
has relatively low fitness-distance correlation, and therefore, the more radical perturbation
mechanism used for pd > 1 (which restarts clique construction from the most recently
added vertex and uses vertex penalties for diversification) is required in order to obtain
good performance; this hypothesis is also in agreement with the relatively high cost for
solving this problem instance.

To further investigate the efficacy of perturbation in DLS-MC as a diversification mechanism, we measured the relative mobility of the search, defined as the Hamming distance
between the current cliques (i.e., number of different vertices) at consecutive perturbations
divided by two times the maximum clique size, for representative runs of DLS-MC on instances C1000.9 and brock800 1 (this mobility measure is closely related to those used in
previous studies (Schuurmans & Southey, 2000)). As can be seen from Figure 11, there
is a large difference in mobility between the two variants of the perturbation mechanism
for pd = 1 and pd > 1; the former restarts the search from a randomly chosen vertex
and consequently leads to a large variability in Hamming distance to the previous clique,
while the latter restarts from the most recently added vertex, using vertex penalties to
increase search diversification, and hence shows consistently much higher mobility. Note
that when vertex penalties are used (i.e., pd > 1), the pd value has no significant effect on
search mobility. At the same time, as previously observed (see Figure 5), the performance
of DLS-MC does significantly depend on the penalty update delay pd. This demonstrates
that in order to achieve peak performance, the increased mobility afforded by the use of
vertex penalties needs to be combined with the correct amount of additional diversification
achieved by using a specific penalty update delay.
181

Pullan & Hoos

C1000.9

Cumulative frequency

100
Delay 1
Delay 2
Delay 10

80
60
40
20
0

0

0.05

0.1

0.15

0.2

0.25
0.3
Relative mobility

0.35

0.4

0.45

0.5

0.35

0.4

0.45

0.5

brock800_1

Cumulative frequency

100
Delay 1
Delay 2
Delay 45

80
60
40
20
0

0

0.05

0.1

0.15

0.2

0.25
0.3
Relative mobility

Figure 11: Mobility of search between consecutive perturbation phases in DLS-MC for instances C1000.9 (top) and brock800 1 (bottom). Mobility is measured in terms
of relative Hamming distance, i.e., number of different vertices between the respective cliques divided by two times the maximum clique size. The graphs
show the cumulative distributions of relative mobility measurements collected
over representative independent runs for each pd value and problem instance;
the solid lines correspond to the respective approx. optimal pd values.

5. Conclusions and Future Work
We have demonstrated how by applying the general paradigm of dynamic local search to the
maximum clique problem, the state of the art in MAX-CLIQUE solving can be improved.
Our new algorithm, DLS-MC, has some similarity to previous MAX-CLIQUE algorithms, in
particular to the recently introduced DAGS algorithm: Both algorithms use vertex penalties
to guide the heuristic selection of vertices when searching for maximum cliques. However,
unlike DAGS, which has an initial phase of unweighted greedy construction search, DLS-MC
uses and updates the vertex penalties throughout the entire search process. Furthermore,
weight updates in DAGS are monotone while, in DLS-MC, vertex penalties are subject
to increases as well as to occasional decreases, which effectively allows the algorithm to
‘forget’ vertex penalties over time. Furthermore, DLS-MC selects the vertex to be added
to the current clique in each step solely based on its penalty, while vertex selection in
DAGS is based on the total weight of the neighbouring vertices and hence implicitely uses
vertex degree for heuristic guidance. The fact that DLS-MC, although conceptually slightly
simpler, outperforms DAGS on all but one of the standard DIMACS benchmark instances
in combination with its excellent performance compared to other high-performance MAX182

Dynamic Local Search for Max-Clique Problem

CLIQUE algorithms clearly demonstrates the value of the underlying paradigm of dynamic
local search with non-monotone penalty dynamics.
The work presented in this article can be extended in several directions. In particular, it
would be interesting to investigate to which extent the use of multiplicative penalty update
mechanisms in DLS-MC instead of its current additive mechanism can lead to further performance improvements. We also believe that the current implementation of DLS-MC can
be further optimised. For example, for each selection of a vertex to be added to the current
clique, our implementation of DLS-MC performs a complete scan of either the improving
or plateaus sets to build the list of vertices with the lowest penalties; it would probably be
more efficient to maintain this list by means of an incremental update scheme. Another
very interesting direction for future research is to develop mechanisms for automatically
adjusting DLS-MC’s penalty delay parameter during the search, similar to the scheme used
for dynamically adapting the tabu tenure parameter in RLS (Battiti & Protasi, 2001) and
Reactive Tabu Search (Battiti & Tecchiolli, 1994), or the mechanism used for controlling
the noise parameter in Adaptive Novelty+ (Hoos, 2002). Finally, given the excellent performance of DLS-MC on standard MAX-CLIQUE instances reported here suggests that the
underlying dynamic local search method has substantial potential to provide the basis for
high-performance algorithms for other combinatorial optimisation problems, particularly
weighted versions of MAX-CLIQUE and conceptually related clustering problems.

Acknowledgments
The authors would like to thank Liang Zhao for her participation in performing some of the
initial experiments for this paper.

References
Balus, E., & Yu, C. (1986). Finding a maximum clique in an arbitary graph. SIAM Journal
of Computing, 15 (4), 1054–1068.
Battiti, R., & Protasi, M. (2001). Reactive local search for the maximum clique problem.
Algorithmica, 29, 610–637.
Battiti, R., & Tecchiolli, G. (1994). The reactive tabu search. ORSA Journal on Computing,
6 (2), 126–140.
Bomze, I., Budinich, M., Pardalos, P., & Pelillo, M. (1999). The maximum clique problem.
In D.Z. Du, P. P. (Ed.), Handbook of Combinatorial Optimization, Vol. A, pp. 1–74.
Boppana, R., & Halldórsson, M. (1992). Approximating maximum independent sets by
excluding subgraphs. Bit, 32, 180–196.
Brockington, M., & Culberson, J. (1996). Camouflaging independent sets in quasi-random
graphs. In D.S. Johnson, M. T. (Ed.), Cliques, Coloring and Satisfiability: Second
DIMACS Implementation Challenge, Vol. 26 of DIMACS Series. American Mathematical Society.
183

Pullan & Hoos

Busygin, S. (2002). A new trust region technique for the maximum clique problem. Internal
report, http://www.busygin.dp.ua.
Garey, M. R., & Johnson, D. S. (1979). Computers and Intractability: A Guide to the Theory
of N P-Completeness. Freeman, San Francisco, CA, USA.
Grosso, A., Locatelli, M., & Croce, F. D. (2004). Combining swaps and node weights in
an adaptive greedy approach for the maximum clique problem. Journal of Heuristics,
10, 135–152.
Håstad, J. (1999). Clique is hard to approximate within n1−ε . Acta Mathematica, 182,
105–142.
Hoos, H. H. (2002). An adaptive noise mechanism for WalkSAT. In Proceedings of the
Eighteenth National Conference on Artificial Intelligence, pp. 655–660. AAAI Press /
The MIT Press, Menlo Park, CA, USA.
Hoos, H. H., & Stützle, T. (2004). Stochastic Local Search: Foundations and Applications.
Morgan Kaufmann Publishers, USA.
Hoos, H., & Stützle, T. (2000). Local search algorithms for SAT: An empirical evaluation.
In Gent, I., v.Maaren, H., & Walsh, T. (Eds.), SAT 2000, pp. 43–86. IOS Press.
Hutter, F., Tompkins, D. A. D., & Hoos, H. H. (2002). Scaling and probabilistic smoothing: Efficient dynamic local search for SAT. In Hentenryck, P. V. (Ed.), Principles
and Practice of Constraint Programming – CP 2002, Vol. 2470 of Lecture Notes in
Computer Science, pp. 233–248. Springer Verlag, Berlin, Germany.
Ji, Y., Xu, X., & Stormo, G. D. (2004). A graph theoretical approach for predicting common RNA secondary structure motifs including pseudoknots in unaligned sequences.
Bioinformatics, 20 (10), 1591–1602.
Johnson, D., & Trick, M. (Eds.). (1996). Cliques, Coloring and Satisfiability: Second DIMACS Implementation Challenge, Vol. 26 of DIMACS Series. American Mathematical
Society.
Katayama, K., Hamamoto, A., & Narihisa, H. (2004). Solving the maximum clique problem by k-opt local search. In Proceedings of the 2004 ACM Symposium on Applied
computing, pp. 1021–1025.
Marchiori, E. (2002). Genetic, iterated and multistart local search for the maximum clique
problem. In Applications of Evolutionary Computing, Vol. 2279 of Lecture Notes in
Computer Science, pp. 112–121. Springer Verlag, Berlin, Germany.
Pevzner, P. A., & Sze, S.-H. (2000). Combinatorial approaches to finding subtle signals in
DNA sequences. In Proceedings of the Eighth International Conference on Intelligent
Systems for Molecular Biology, pp. 269–278. AAAI Press.
Pullan, W., & Zhao, L. (2004). Resolvent clause weighting local search. In Tawfik, A. Y.,
& Goodwin, S. D. (Eds.), Advances in Artificial Intelligence, 17th Conference of the
Canadian Society for Computational Studies of Intelligence, Vol. 3060 of Lecture Notes
in Computer Science, pp. 233–247. Springer Verlag, Berlin, Germany.
184

Dynamic Local Search for Max-Clique Problem

Resende, M., Feo, T., & Smith, S. (1998). Algorithm 786: FORTRAN subroutine for approximate solution of the maximum independent set problem using GRASP. ACM
Transactions on Mathematical Software, 24, 386–394.
Schuurmans, D., & Southey, F. (2000). Local search characteristics of incomplete SAT
procedures. In Proceedings of the Seventeenth National Conference on Artificial Intelligence, pp. 297–302. AAAI Press / The MIT Press, Menlo Park, CA, USA.
Solnon, C., & Fenet, S. (2004). A study of aco capabilities for solving the maximum clique
problem. Journal of Heuristics, to appear.
Thornton, J., Pham, D. N., Bain, S., & Ferreira, V. (2004). Additive versus multiplicative
clause weighting for SAT. In Proceedings of the 19th National Conference on Artificial
Intelligence (AAAI-04), pp. 191–196. AAAI Press / The MIT Press, Menlo Park, CA,
USA.
Tompkins, D., & Hoos, H. (2003). Scaling and probabilistic smoothing: Dynamic local
search for unweighted MAX-SAT. In Xiang, Y., & Chaib-draa, B. (Eds.), Advances
in Artificial Intelligence, 16th Conference of the Canadian Society for Computational
Studies of Intelligence, Vol. 2671 of Lecture Notes in Computer Science, pp. 145–159.
Springer Verlag, Berlin, Germany.
Watson, J., Whitley, L., & Howe, A. (2005). Linking search space structure, run-time
dynamics, and problem difficulty: A step toward demystifying tabu search. Journal
of Artificial Intelligence, 24, 221–261.

185

Journal of Artificial Intelligence Research 25 (2006) 389–424

Submitted 09/05; published 03/06

On Graphical Modeling of Preference and Importance
Ronen I. Brafman

brafman@cs.stanford.edu

Department of Computer Science
Stanford University
Stanford CA 94305

Carmel Domshlak

dcarmel@ie.technion.ac.il

Faculty of Industrial Engineering and Management
Technion - Israel Institute of Technology
Haifa, Israel 32000

Solomon E. Shimony

shimony@cs.bgu.ac.il

Department of Computer Science
Ben-Gurion University
Beer Sheva, Israel 84105

Abstract
In recent years, CP-nets have emerged as a useful tool for supporting preference elicitation, reasoning, and representation. CP-nets capture and support reasoning with qualitative conditional preference statements, statements that are relatively natural for users
to express. In this paper, we extend the CP-nets formalism to handle another class of very
natural qualitative statements one often uses in expressing preferences in daily life – statements of relative importance of attributes. The resulting formalism, TCP-nets, maintains
the spirit of CP-nets, in that it remains focused on using only simple and natural preference
statements, uses the ceteris paribus semantics, and utilizes a graphical representation of
this information to reason about its consistency and to perform, possibly constrained, optimization using it. The extra expressiveness it provides allows us to better model tradeoffs
users would like to make, more faithfully representing their preferences.

1. Introduction
The ability to make decisions and to assess potential courses of action is a corner-stone of
numerous AI applications, including expert systems, autonomous agents, decision-support
systems, recommender systems, configuration software, and constrained optimization applications. To make good decisions, we must be able to assess and compare different alternatives. Sometimes, this comparison is performed implicitly, as in many recommender
systems (Burke, 2000; Resnick & Varian, 1997). But frequently, explicit information about
the decision-maker’s preferences is required.
In classical decision theory and decision analysis utility functions are used to represent
the decision-maker’s preferences. However, the process of obtaining the type of information
required to generate a good utility function is involved, time-consuming and requires nonnegligible effort on the part of the user (French, 1986). Sometimes such effort is necessary
and possible, but in many applications the user cannot be engaged for a lengthy period of
time and cannot be supported by a human decision analyst. For instance, this is the case in
on-line product recommendation systems and other software decision-support applications.
c
2006
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Brafman, Domshlak, & Shimony

When a utility function cannot be or need not be obtained, one should resort to other,
more qualitative forms of preference representation. Ideally, this qualitative information
should be easily obtainable from the user by non-intrusive means. That is, we should be
able to extract such information from natural and relatively simple statements of preference
provided by the user, and this elicitation process should be amenable to automation. In
addition, automated reasoning about such qualitative preference information should be
semantically effective and computationally efficient.
One framework for preference representation that addresses these concerns is that of
Conditional Preference Networks (CP-nets) (Boutilier et al. 1999, 2004a). CP-nets is a
graphical preference representation model grounded in the notion of conditional preferential independence. In preference elicitation with CP-nets, the decision maker (directly or
indirectly) describes how her preference over the values of one variable depends on the value
of other variables. For example, she may state that her preference for a dessert depends on
the main-course as well as whether or not she had an alcoholic beverage. In turn, her preference for an alcoholic beverage may depend on the main course and the time of day. This
information is described by a graphical structure in which the nodes represent variables of
interest and edges capture direct preferential dependence relations between the variables.
Each node is annotated with a conditional preference table (CPT) describing the user’s
preference over alternative values of this node given different values of the parent nodes.
CP-nets capture a class of intuitive and useful natural language statements of the form “I
prefer the value x0 for variable X given that Y = y0 and Z = z0 ”. Such statements do not
require complex introspection nor a quantitative assessment.
From the practical perspective, there is another class of preference statements that is no
less intuitive or important, yet is not captured by the CP-net model. These statements have
the form: “It is more important to me that the value of X be better than that the value
of Y be better.” We call these relative importance statements. For instance, one might say
“The length of the journey is more important to me than the choice of airline”. A more
refined notion of importance, though still intuitive and easy to communicate, is that of conditional relative importance: “The length of the journey is more important to me than the
choice of airline if I need to give a talk the following day. Otherwise, the choice of airline is
more important.” The latter statement is of the form: “A better assignment for X is more
important than a better assignment for Y given that Z = z0 .” Notice that information
about relative importance is different from information about preferential independence.
For instance, in the example above, user’s preference for an airline does not depend on the
duration of the journey because, e.g., she compares airlines based only on their service,
security levels, and the quality of their frequent flyer program. Informally, using statements
of relative importance the user expresses her preference over compromises that may be
required. Such information is very important in customized product configuration applications (Sabin & Weigel, 1998; Haag, 1998; Freuder & O’Sullivan, 2001), where production,
supply, and other constraints are posed on the product space by the producer, and these
constraints are typically even unknown to the customer. Indeed, in many applications, various resource (e.g., money, time, bandwidth) constraints exist, and the main computational
task is that of finding a solution that is feasible and not preferentially dominated by any
other solution.
390

TCP-Nets

In this paper we consider enhancing the expressive power of CP-nets by introducing
information about importance relations, obtaining a preference-representation structure
which we call TCP-nets (for tradeoffs-enhanced CP-nets). By capturing information about
both conditional preferential independence and conditional relative importance, TCP-nets
provide a richer framework for representing user preferences, allowing stronger conclusions
to be drawn, yet remaining committed to the use of only very intuitive, qualitative information. At the same time, we show that the added relative importance information has
significant impact on both the consistency of the specified relation, and the techniques used
for reasoning about it. Focusing on these computational issues, we show that the graphical structure of the “mixed” set of preference statements captured in a TCP-net can often
be exploited in order to achieve efficiency both in consistency testing and in preferential
reasoning.
This paper is organized as follows: Section 2 describes the notions underlying TCP-nets:
preference relations, preferential independence, and relative importance. In Section 3 we
define TCP-nets, specify their semantics, and provide a number of examples. In Section 4
we characterize a class of conditionally acyclic TCP-nets whose consistency is guaranteed
and then, in Section 5 we discuss the complexity of identifying members of this class. In
Section 6 we present an algorithm for outcome optimization in conditionally acyclic TCPnets, and discuss the related tasks of reasoning about preferences given a TCP-net. We
conclude with a discussion of related and future work in Section 7.

2. Preference Orders, Independence, and Relative Importance
In this section we describe the semantic concepts underlying TCP-nets: preference orders,
preferential independence, conditional preferential independence, as well as relative importance and conditional relative importance.
2.1 Preference and Independence
We model a preference relation as a strict partial order. Thus, we use the terms preference
order and strict partial order interchangeably. A strict partial order is a binary relation
over outcomes that is anti-reflexive, anti-symmetric and transitive. Given two outcomes
o, o0 , we write o  o0 to denote that o is strictly preferred to o0 .
The above choice implies that two outcomes cannot be equally preferred. This choice
follows from the fact that the language of preferences we use in this paper does not allow
statements of indifference (as opposed to incomparability), and thus there is no need for
using weak orderings. Incorporating statements of indifference is pretty straightforward, as
explained by Boutilier et al. (2004a), but introduces much overhead if we were to formally
treat it throughout this paper.
The types of outcomes we are concerned with consist of possible assignments to some set
of variables. More formally, we assume some given set V = {X1 , . . . , Xn } of variables with
corresponding domains D(X1 ), . . . , D(Xn ). The set of possible outcomes is then D(V) =
D(X1 ) × · · · × D(Xn ), where we use D(·) to denote the domain of a set of variables as well.
For example, in the context of the problem of configuring a personal computer (PC), the
variables may be processor type, screen size, operating system etc., where screen size has
the domain {17in, 19in, 21in}, operating system has the domain {LINUX, Windows98,
391

Brafman, Domshlak, & Shimony

WindowsXP}, etc. Each complete assignment to the set of variables specifies an outcome –
a particular PC configuration. Thus, a preference relation over these outcomes specifies a
strict partial order over possible PC configurations.
The number of possible outcomes is exponential in n, while the set of possible orderings
on them is more than doubly exponential in n. Therefore, explicit specification and representation of an ordering is not realistic, and thus we must describe it implicitly using a
compact representation model. The notion of preferential independence plays a key role in
such representations. Intuitively, X ⊂ V is preferentially independent of Y = V − X if and
only if for all assignments to Y, our preference over X values is identical.
Definition 1 Let x1 , x2 ∈ D(X) for some X ⊆ V, and y1 , y2 ∈ D(Y), where Y = V − X.
We say that X is preferentially independent of Y iff, for all x1 , x2 , y1 , y2 we have that
x1 y1  x2 y1 iff x1 y2  x2 y2

(1)

For example, in our PC configuration example, the user may assess screen size to be preferentially independent of processor type and operating system. This could be the case if
the user always prefers a larger screen to a smaller screen, independent of the selection of
processor and/or OS.
Preferential independence is a strong property, and is therefore not very common. A
more refined notion is that of conditional preferential independence. Intuitively, X is conditionally preferentially independent of Y given Z if and only if for every fixed assignment
to Z, the ranking of X values is independent of the value of Y.
Definition 2 Let X, Y and Z be a partition of V and let z ∈ D(Z). X is conditionally
preferentially independent of Y given z iff, for all x1 , x2 , y1 , y2 we have that
x1 y1 z  x2 y1 z iff x1 y2 z  x2 y2 z,

(2)

and X is conditionally preferentially independent of Y given Z iff X is conditionally preferentially independent of Y given every assignment z ∈ D(Z).
Returning to our PC example, the user may assess operating system to be independent of
all other features given processor type. That is, he always prefers LINUX given an AMD
processor and WindowsXP given an Intel processor (e.g., because he might believe that
WindowsXP is optimized for the Intel processor, whereas LINUX is otherwise better). Note
that the notions of preferential independence and conditional preferential independence are
among a number of standard and well-known notions of independence in multi-attribute
utility theory (Keeney & Raiffa, 1976).
2.2 Relative Importance
Although statements of preferential independence are natural and useful, the orderings obtained by relying on them alone are relatively weak. To understand this, consider two
preferentially independent boolean attributes A and B with values a1 , a2 and b1 , b2 , respectively. If A and B are preferentially independent, then we can specify a preference order
over A values, say a1  a2 , independently of the value of B. Similarly, our preference over
392

TCP-Nets

B values, say b1  b2 , is independent of the value of A. From this we can deduce that a1 b1
is the most preferred outcome and a2 b2 is the least preferred outcome. However, we do
not know the relative order of a1 b2 and a2 b1 . This is typically the case when we consider
independent variables: We can rank each one given a fixed value of the other, but often,
we cannot compare outcomes in which both values are different. One type of information
that can address some (though not necessarily all) such comparisons is information about
relative importance. For instance, if we state that A is more important than B, it means
that we prefer an improvement in A over an improvement in B. In that case, we know that
a1 b2  a2 b1 , and can totally order the set of outcomes as a1 b1  a1 b2  a2 b1  a2 b2 .
One may ask why it is important for us to order a1 b2 and a2 b1 – after all, we know
that a1 b1 is the most preferred outcome. However, in many typical scenarios, we have
auxiliary or user constraints that prevent us from providing the user with the most preferred
(unconstrained) outcome. A simple and common example is that of budget constraints,
other resource limitations, such are bandwidth and buffer size (as in the adaptive richmedia systems described by Brafman and Friedman (2005) are also common. In such cases,
it is important to know which attributes the user cares about more strongly, and to try to
maintain good values for these attributes, compromising on the others.
Returning to our PC configuration example, suppose that the attributes operating system and processor type are mutually preferentially independent. We might say that processor type is more important than operating system, e.g, because we believe that the effect
of the processor’s type on system performance is more significant than the effect of the
operating system.
Definition 3 Let a pair of variables X and Y be mutually preferentially independent given
W = V − {X, Y }. We say that X is more important than Y , denoted by X  Y , if for
every assignment w ∈ D(W) and for every xi , xj ∈ D(X), ya , yb ∈ D(Y ), such that xi  xj
given w, we have that:
xi ya w  xj yb w.
(3)
Note that Eq. 3 holds even when yb  ya given w. For instance, when both X and Y
are binary variables, and x1  x2 and y1  y2 hold given w, then X  Y iff we have
x1 y2 w  x2 y1 w for all w ∈ D(W). Notice that this is a strict notion of importance – any
reduction in Y is preferred to any reduction in X. Clearly, this idea can be further refined
by providing an actual ordering over elements of D(XY ), and we discuss this extension
in Section 3.4. In addition, one can consider relative importance assessments among more
than two variables. However, we feel that the benefit of capturing such statements is small:
We believe that statements of relative importance referring to more than two attributes are
not very natural for users to articulate, and their inclusion would significantly reduce the
computational advantages of graphical modeling. Therefore, in this work we focus only on
relative importance statements referring to pairs of attributes.
Relative importance information is a natural enhancement of independence information.
As such, relative importance retains a desirable property - it corresponds to statements that
a naive user would find simple and clear to evaluate and articulate. Moreover, it can be
generalized naturally to a notion of conditional relative importance. For instance, suppose
that the relative importance of processor type and operating system depends on the primary
usage of the PC. For example, when the PC is used primarily for graphical applications, then
393

Brafman, Domshlak, & Shimony

the choice of an operating system is more important than that of a processor because certain
important software packages for graphic design are not available on LINUX. However, for
other applications, the processor type is more important because applications for both
Windows and LINUX exist. Thus, we say that X is more important than Y given z if we
always prefer to reduce the value of Y rather than the value of X, whenever z holds.
Definition 4 Let X and Y be a pair of variables from V, and let Z ⊆ W = V − {X, Y }.
We say that X is more important than Y given z ∈ D(Z) iff, for every assignment w0 on
W0 = V − ({X, Y } ∪ Z) we have:
xi ya zw0  xj yb zw0

(4)

whenever xi  xj given zw0 . We denote this relation by X z Y . Finally, if for some
z ∈ D(Z) we have either X z Y , or Y z X, then we say that the relative importance of
X and Y is conditioned on Z, and write RI(X, Y |Z).

3. TCP-nets
The TCP-net (for Tradeoff-enhanced CP-nets) model is an extension of CP-nets (Boutilier
et al., 2004a) that encodes conditional relative importance statements, as well as the conditional preference statements supported in CP-nets. The primary usage of the TCP-net
graphical structure is in consistency analysis of the provided preference statements, and in
classification of complexity and developing efficient algorithms for various reasoning tasks
over these statements. In particular, as we later show, when this structure is “acyclic”
(for a suitable definition of this notion!), the set of preference statements represented by
the TCP-net is guaranteed to be consistent – that is, there is a strict total order over the
outcomes that satisfies all the preference statements. In what follows we formally define the
TCP-net model. As it subsumes the CP-net model, we will immediately define this more
general model rather than proceed in stages.
3.1 TCP-net Definition
TCP-nets are annotated graphs with three types of edges. The nodes of a TCP-net correspond to the problem variables V. The first type of (directed) edges comes from the
original CP-nets model and captures direct preferential dependencies, that is, such an edge
from X to Y implies that the user has different preferences over Y values given different
values of X. The second (directed) edge type captures relative importance relations. The
existence of such an edge from X to Y implies that X is more important than Y . The third
(undirected) edge type captures conditional importance relations: Such an edge between
nodes X and Y exists if there exists a non-empty variable subset Z ⊆ V − {X, Y } for which
RI(X, Y |Z) holds. Without loss of generality, in what follows, the set Z is assumed to be
the minimal set of variables upon which the relative importance between X and Y depends.
As in CP-nets, each node X in a TCP-net is annotated with a conditional preference
table (CPT). This table associates preferences over D(X) for every possible value assignment
to the parents of X (denoted P a(X)). In addition, in TCP-nets, each undirected edge is
annotated with a conditional importance table (CIT). The CIT associated with such an edge
394

TCP-Nets

(X, Y ) describes the relative importance of X and Y given the value of the corresponding
importance-conditioning variables Z.
Definition 5 A TCP-net N is a tuple hV, cp, i, ci, cpt, citi where:
(1) V is a set of nodes, corresponding to the problem variables {X1 , . . . , Xn }.
(2) cp is a set of directed cp-arcs {α1 , . . . , αk } (where cp stands for conditional preference).
−−−−→
A cp-arc hXi , Xj i is in N iff the preferences over the values of Xj depend on the actual
−−−→
value of Xi . For each X ∈ V, let P a(X) = {X 0 |hX 0 , Xi ∈ cp}.
−−−−→
(3) i is a set of directed i-arcs {β1 , . . . , βl } (where i stands for importance). An i-arc (Xi , Xj )
is in N iff Xi  Xj .
(4) ci is a set of undirected ci-arcs {γ1 , . . . , γm } (where ci stands for conditional importance). A ci-arc (Xi , Xj ) is in N iff we have RI(Xi , Xj |Z) for some Z ⊆ V−{Xi , Xj }.1
We call Z the selector set of (Xi , Xj ) and denote it by S(Xi , Xj ).

(5) cpt associates a CPT with every node X ∈ V, where CP T (X) is a mapping from
D(P a(X)) (i.e., assignments to X’s parent nodes) to strict partial orders over D(X).
(6) cit associates with every ci-arc γ = (Xi , Xj ) a (possibly partial) mapping CIT (γ) from
D (S(Xi , Xj )) to orders over the set {Xi , Xj }.2
A TCP-net in which the sets i and ci (and therefore also cit) are empty, is also a CP-net.
Thus, it is the elements i, ci, and cit that describe absolute and conditional importance of
attributes provided by TCP-nets, beyond the conditional preference information captured
by CP-nets.
3.2 TCP-net Semantics
The semantics of a TCP-net is defined in terms of the set of strict partial orders consistent
with the set of constraints imposed by the preference and importance information captured
by this TCP-net. The intuitive idea is rather straightforward: (1) A strict partial order
 satisfies the conditional preferences for variable X if any two complete assignments that
differ only on the value of X are ordered by  consistently with the ordering on X values
in the CPT of X. Recall that this ordering can depend on the parent of X in the graph.
(2) A strict partial order  satisfies the assertion that X is more important than Y if given
any two complete assignments that differ on the value of X and Y only,  prefers that
assignment which provides X with a better value. (3) A strict partial order  satisfies the
assertion that X is more important than Y given some assignment z to variable set Z if
given any two complete assignments that differ on the value X and Y only, and in (both
of) which Z is assigned z,  prefers that assignment which provides X with a better value.
−−−−→
1. Observe that every i-arc (Xi , Xj ) can be seen as representing RI(Xi , Xj |∅). However, a clear distinction
between i-arcs and ci-arc simplifies specification of many forthcoming notions and claims (e.g., Lemma 3
in Section 4, as well as the related notion of root variables.)
2. That is, the relative importance relation between Xi and Xj may be specified only for certain values of
the selector set.

395

Brafman, Domshlak, & Shimony

This is defined more formally below. We use X
u to denote the preference relation over
the values of X given an assignment u to U ⊇ P a(X).
Definition 6 Consider a TCP-net N = hV, cp, i, ci, cpt, citi.
1. Let W = V − ({X} ∪ P a(X)) and let p ∈ D(P a(X)). A preference (=strict partial)
order  over D(V) satisfies X
p iff xi pw  xj pw, for every w ∈ D(W), whenever
X
xi p xj holds.
2. A preference order  over D(V) satisfies CP T (X) ∈ cpt iff it satisfies X
p for every
assignment p of P a(X).
3. A preference order  over D(V) satisfies X  Y iff for every w ∈ D(W) such that
W = V − {X, Y }, xi ya w  xj yb w whenever xi X
w xj .
4. A preference order  over D(V) satisfies X z Y iff for every w ∈ D(W) such that
W = V − ({X, Y } ∪ Z), xi ya zw  xj yb zw whenever xi X
zw xj .
5. A preference order  over D(V) satisfies CIT (γ) of the ci-arc γ = (X, Y ) ∈ cit if it
satisfies X z Y whenever an entry in the table conditioned on z ranks X as more
important.
A preference order  over D(V) satisfies a TCP-net N = hV, cp, i, ci, cpt, citi iff:
(1) for every X ∈ V,  satisfies CP T (X),
−−−−→
(2) for every i-arc β = (Xi , Xj ) ∈ i,  satisfies X  Y , and
(3) for every ci-arc γ = (Xi , Xj ) ∈ ci,  satisfies CIT (γ).

Definition 7 A TCP-net is satisfiable iff there is some strict partial order  over D(V)
that satisfies it; o  o0 is implied by a TCP-net N iff it holds in all preference orders over
D(V) that satisfy N .
Lemma 1 Preferential entailment with respect to a satisfiable TCP-net is transitive. That is,
if N |= o  o0 and N |= o0  o00 , then N |= o  o00 .
Proof: If N |= o  o0 and N |= o0  o00 , then o  o0 and o0  o00 in all preference orders
satisfying N . As each of these ordering is transitive, we must have o  o00 in all satisfying
orderings. 
Note that, strictly speaking, we should use the term “satisfiable” rather than “consistent” with respect to a set of preference statements, given that we provide a model theory,
and not a proof theory. However, since the corresponding proof theory follows in a completely straightforward manner from our semantics combined with transitivity, this raises
no problem.
396

TCP-Nets

3.3 TCP-net Examples
Having provided the formal specification of the TCP-nets model, let us now illustrate TCPnets with a few examples. For simplicity of presentation, in the following examples all
variables are binary, although the semantics of TCP-nets is given by Definitions 6 and 7
with respect to arbitrary finite domains.
Example 1 (Evening Dress) Figure 1(a) presents a CP-net that consists of three variables
J, P , and S, standing for the jacket, pants, and shirt, respectively. I prefer black to white as
a color for both the jacket and the pants, while my preference for the shirt color (red/white)
is conditioned on the color combination of jacket and pants: If they are of the same color, a
white shirt will make my dress too colorless, therefore, red shirt is preferable. Otherwise, if
the jacket and the pants are of different colors, a red shirt will probably make my evening
dress too flashy, therefore, a white shirt is preferable. The solid lines in Figure 1(c) show
the preference relation induced directly by the information captured by this CP-net; The
top and the bottom elements are the worst and the best outcomes, respectively, and the
arrows are directed from less preferred to more preferred outcomes.
J b  Jw



Jw ∧ Pw ∧ Sw 

Pb  Pw

?>=<
89:;
@ABC
GFED
J/
P
//

//

//


// 
 
?>=<
89:;
S
J b ∧ Pb
J w ∧ Pb
J b ∧ Pw
J w ∧ Pw

Sr  Sw
Sw  Sr
Sw  Sr
Sr  Sw

(a)
?>=<
89:;
GFED
/@ABC

J/
P
//


//

//


// 
 
?>=<
89:;
S




Jw ∧ Pw ∧ Sr 





Jw ∧ Pb ∧ Sr _ _ _ _/ iJb ∧ Pw ∧ Sr 
i
i
i
iiii
iiii
i
i
i
iiii
iiii
i
i
i

tii




Jb ∧ Pw ∧LSw o_ _ _ _Jw ∧ Pb ∧ Sw 
LLL
LLL
LLL
LLL
LLL
 %  

Jb ∧ Pb ∧ Sw 
 }


Jb ∧ Pb ∧ Sr 

(b)

(c)
Figure 1: “Evening Dress” CP-net & TCP-net.

Figure 1(b) depicts a TCP-net that extends this CP-net by adding an i-arc from J to P ,
i.e., having black jacket is (unconditionally) more important than having black pants. This
397

Brafman, Domshlak, & Shimony

induces additional relations among outcomes, captured by the dashed lines in Figure 1(c).
3

The reader may rightfully ask whether the statement of importance in Example 1 is not
redundant: According to my preference, it seems that I will always wear a black suit with
a red shirt. However, while my preferences are clear, various constraints may make some
outcomes, including the most preferred one, infeasible. For instance, I may not have a clean
black jacket, in which case the most preferred feasible alternative is a white jacket, black
pants, and a white shirt. Alternatively, suppose that the only clean clothes I have are velvet
black jacket and white pants, and silk white jacket and black pants. My wife forbids me to
mix velvet and silk, and so I will have to compromise, and to wear either the black (velvet)
jacket with the white (velvet) pants, or the white (silk) jacket with the black (silk) pants. In
this case, the fact that I prefer wearing the preferred jacket to wearing the preferred pants
determines higher desirability for the velvet combination. Now, if my wife has to prepare
my evening dress while I am late at work writing a paper, having this information will help
her to choose among the available options an outfit that I would like most.
Indeed, as noted earlier, many applications involve limited resources, such as money,
time, bandwidth, memory, etc. In many instances, the optimal assignment violates these
resource constraints, and we must compromise and accept a less desirable, but feasible assignment. TCP-nets capture information that allows us make more informed compromises.
Example 2 (Flight to the USA) Figure 2(a) illustrates a more complicated CP-net, describing my preference over the flight options to a conference in the USA, from Israel. This
network consists of five variables, standing for various parameters of the flight:
Day of the Flight The variable D distinguishes between flights leaving a day (D1d ) and
two days (D2d ) before the conference, respectively. Since I am married, and I am
really busy with my work, I prefer to leave on the day before the conference.
Airline The variable A represents the airline. I prefer to fly with British Airways (Aba )
than with KLM (Aklm ).
Departure Time The variable T distinguishes between morning/noon (Tm ) and evening/night
(Tn ) flights. Among flights leaving two days before the conference I prefer an evening/night
flight, because it will allow me to work longer on the day of the flight. However, among
flights leaving a day before the conference I prefer a morning/noon flight, because I
would like to have a few hours before the conference opening in order to rest at the
hotel.
Stop-over The variable S distinguishes between direct (S0s ) and indirect (S1s ) flights,
respectively. On day flights I am awake most of the time and, being a smoker, prefer
a stop-over in Europe (so I can have a smoking break). However, on night flights I
sleep, leading to a preference for direct flights, since they are shorter.
Ticket Class The variable C stands for ticket class. On a night flight, I prefer to sit
in economy class (Ce ) (I don’t care where I sleep, and these seats are significantly
cheaper), while on a day flight I prefer to pay for a seat in business class (Cb ) (Being
awake, I can better appreciate the good seat, food, and wine).
398

TCP-Nets

@ABC
GFED
D

D1d  D2d
D1d
D2d

Tm  Tn
Tn  Tm

Tm
Tn

S1s  S0s
S0s  S1s

Aba  Aklm
Tm
Tn

Cb  Ce
Ce  Cb

@ABC
GFED
D

89:;
?>=<
T0
  000
00

00
 
0

?>=<
89:;
@ABC
GFED
S
C

89:;
?>=<
A

(a)


89:;
?>=<
?>=<
/ 89:;

T0
A
 00
00

00
 
00



T,A
@ABC
GFED
89:;
?>=<

C
S
Tm ∧ Aklm
Tm ∧ Aba
Tn ∧ Aba

SC
C S
SC

(b)

Figure 2: “Flight to the USA” CP-net & TCP-net from Example 2.
The CP-net in Figure 2(a) captures all these preference statements, and the underlying
preferential dependencies, while Figure 2(b) presents a TCP-net that extends this CP-net
to capture relative importance relations between some parameters of the flight. First, there
is an i-arc from T to A, because getting more suitable flying time is more important to me
than getting the preferred airline. Second, there is a ci-arc between S and C, where the
relative importance of S and C depends on the values of T and A:3
1. On a KLM day flight, an intermediate stop in Amsterdam is more important to me
than flying business class (I feel that KLM’s business class does not have a good
cost/performance ratio, while visiting a casino in Amsterdam’s airport sounds to me
like a good idea.)
2. For a British Airways night flight, the fact that the flight is direct is more important
to me than getting a cheaper economy seat (I am ready to pay for business class, in
order not to spend even one minute at Heathrow airport at night).
3. On a British Airways day flight, business class is more important to me than having
a short intermediate break (it is hard to find a nice smoking area at Heathrow).
The CIT of this ci-arc is also shown in Figure 2(b). 3
3.4 Relative Importance with Non-binary Variables
Having read so far, the reader may rightfully ask whether the notion of relative (conditional)
importance ceteris paribus, as specified in Section 2.2 (Eq. 3 and 4), is not too strong when
3. For clarity, the ci-arc in Figure 2(b) is schematically labeled with its importance-conditioning variables
T and A.

399

Brafman, Domshlak, & Shimony

D1d  D2d

D1d
D2d

Tm  Tn
Tn  Tm

@ABC
GFED
D

?>=<
89:;
A


 


?>=<
89:;
T6
 
66

66
 
66

66
6 	

SC

Aba  Aklm

Tm ∧ Aba
Tm ∧ Aklm
Tn ∧ Aba
Tn ∧ Aba

S1s Cb  S0s Cb  S1s Ce  S0s Ce
S1s Cb  S1s Ce  S0s Cb  S0s Ce
S0s Ce  S0s Cb  S1s Ce  S1s Cb
S0s Ce  S0s Cb  S1s Cb
S0s Ce  S1s Ce  S1s Cb

Figure 3: The network obtained by clustering variables S and C in Example 2.

the variables are not binary. For example, consider a more refined notion of departure time
(variable T ) in Example 2, and suppose there are more than two companies flying from
Israel to the USA (variable A). In this case, one may prefer a better flight time, even if
this requires a compromise in the airline, as long as this compromise is not too significant.
For instance, to get a better flight time, one may be willing to compromise and accept any
airline but only among those she ranks in the top i places in this context.
More generally, our notion of importance, as well as some more refined notions of it,
are really means of specifying an ordering over assignments to variable pairs. In a sense,
one could reduce TCP-nets into CP-nets by combining variables between which we have
an importance relation. Thus, for instance, in the “Flight to the USA” example, we could
combine the variables S and C (see Figure 3). The resulting variable, SC will have as its
domain the Cartesian product of the domains of S and C. The preferences for the values
of SC are now conditioned on T , the current parent of S and C, as well as on A, which
belongs to the selector set of their CIT. In general, the selector set (and parents of) a pair of
variables can be viewed as conditioning the preferences over the value combinations for this
pair. Hence, such clustering can help us already in the case of binary variables as certain
orderings over the assignments to two binary variables cannot be specified with a TCP-net.
However, this is clearly more of an issue in the case of non-binary variables, where the
number of combinations of pairs of values is much larger.
The bottom line is that more complex importance relations between pairs of variables
can be captured. The main questions is how. The strict importance relation we use captures
certain such relations in a very compact manner. As such, its specification (e.g., in terms of
natural language statements) is very easy. This does not rule out the possibility of expressing
more refined relations. Various linguistic constructs could be used to express such relations.
However, technically, they can all be captured by clustering the relevant variables, and the
resulting representation would be a TCP-net, or possibly simply a CP-net. Of course, it
is quite possible that some relations have an alternative compact representation that could
help make reasoning with them more efficient than simply collapsing them, and this can be
a useful question for future research to examine.
400

TCP-Nets
?>=<
89:;
D

89:;
?>=<
D


?>=<
89:;
89:;
/ ?>=<

T
A
 000
00

00
 
00


T,A
?>=<
89:;
?>=<
89:;

S
C
Tm ∧ Aklm
Tm ∧ Aba
Tn ∧ Aba


89:;
?>=<
?>=<
/ 89:;
T
A
t
0
t
0
 0
t 
t
00 ttt 


 
tt00
t
t 0 
0 
  tttt
yt
89:;
?>=<
89:;
?>=<
C
S

SC
CS
SC

(a)
()*+
/.-,
D

(b)
()*+
/.-,
D

()*+
/.-,
D


/.-,
()*+
/.-,
/ ()*+
T5
A

 55ooooo




5




o
 



wo
 ooo 5()*+
()*+
/.-,
/ /.-,
C
S


/.-,
()*+
/.-,
/ ()*+
T5
A

 55ooooo




5




o
 



wo
 ooo 5/.-,
()*+
/.-,
()*+
S
C


/.-,
()*+
/.-,
/ ()*+
T5
A

 55ooooo




5




o
 



wo
 ooo 5/.-,
()*+
/.-,
()*+
So
C

(Tm ∧ Aklm )-directed

(Tn ∧ Aklm )-directed

(Tm ∧ Aba )-directed

()*+
/.-,
D


'&%$
!"#
!"#
/ '&%$
T2
qqA
2

q
q
2
 q2 
 
 qxo qqq /.-,
'&%$
!"#
()*+
S
C
(Tn ∧ Aba )-directed

(c)
Figure 4: (a) “Flight to USA” TCP-net. (b) Its dependency graph. (c), Four w-directed
graphs.

4. Conditionally Acyclic TCP-nets
Returning to the notion of TCP-net satisfiability, observe that Definition 7 provides no practical tools for verifying satisfiability of a given TCP-net. Tackling this issue, in this section
we introduce a large class of TCP-nets whose members are guaranteed to be satisfiable. We
refer to this class of TCP-nets as conditionally acyclic.
Let us begin with the notion of the dependency graph induced by a TCP-net.
Definition 8 The dependency graph N ? of TCP-net N contains all the nodes and edges
of N . Additionally, for every ci-arc (Xi , Xj ) in N and every Xk ∈ S(Xi , Xj ), N ? contains
a pair of directed edges (Xk , Xi ) and (Xk , Xj ), if these edges are not already in N .
Figure 4(b) depicts the dependency graph of the TCP-net from the “Flight to USA”
example, repeated for convenience in Figure 4(a). For the next definition, recall that the
selector set of a ci-arc is the set of nodes whose value determines the “direction” of this
arc. Recall also, that once we assign a value to the selector set, we are, in essence, orienting
401

Brafman, Domshlak, & Shimony

all the conditional importance edges. More generally, once all selector sets are assigned, we
transform both N and N ? . This motivates the following definition.
Definition 9 Let S(N ) be the union of all selector sets of N . Given an assignment w to
all nodes in S(N ), the w-directed graph of N ? consists of all the nodes and directed edges
of N ? . In addition it has a directed edge from Xi to Xj if such an edge is not already in
N ? , and (Xi , Xj ) is a ci-arc of N and the CIT for (Xi , Xj ) specifies that Xi  Xj given w.

Figure 4(c) presents all the four w-directed graphs of the TCP-net from the “Flight to
USA” example. Note that, for the KLM night flights, the relative importance of S and C
is not specified, thus there is no edge between S and C in the (Tn ∧ Aklm )-directed graph
of N ? .
Using Definitions 8 and 9, we specify the class of conditionally acyclic TCP-nets, and
show that it is satisfiable4 .
Definition 10 A TCP-net N is conditionally acyclic if, for every assignment w to S(N ),
the induced w-directed graphs of N ? are acyclic.
We now show that every conditionally acyclic TCP-net is satisfiable, and begin with
providing two auxiliary lemmas.
Lemma 2 The property of conditional acyclicity of TCP-nets is hereditary. That is, given two
TCP-nets N = hV, cp, i, ci, cpt, citi and N 0 = hV0 , cp0 , i0 , ci0 , cpt0 , cit0 i, if
1. N is conditionally acyclic, and
2. V0 ⊆ V, cp0 ⊆ cp, i0 ⊆ i, ci0 ⊆ ci, cpt0 ⊆ cpt, cit0 ⊆ cit,
then N 0 is also conditionally acyclic.
Proof:
The proof is straightforward from Definition 10 since removing nodes and/or
edges from N , as well as removing some preference and importance information from CPTs
and CITs of N , can only remove cycles from the w-directed graphs of N ? . Hence, if N is
conditionally acyclic, then so is any subnet of N . 
Lemma 3 Every conditionally acyclic TCP-net N = hV, cp, i, ci, cpt, citi contains at least one
−−→
−−→
variable X ∈ V, such that, for each Y ∈ V \ {X}, we have hY, Xi 6∈ cp, (Y, X) 6∈ i, and
(X, Y ) 6∈ ci.
Proof: To prove the existence of such a root variable X ∈ N , consider the dependency
graph N ? . Since N is conditionally acyclic, there has to be a node X 0 ∈ N ? that has neither
incoming directed nor undirected edges associated with it. The see the latter, observe that
(i) every endpoint of an undirected edge in N ? will also have an incoming directed edge, and
4. The authors would like to thank Nic Wilson for pointing out an error in the original definition of
conditionally acyclic TCP-nets in (Brafman & Domshlak, 2002).

402

TCP-Nets

(ii) there has to be at least one node in N ? with no incoming directed edges, or otherwise
the conditional acyclicity of N will be trivially violated. However, such a node X 0 will also
be a root node in N since the edge set of N ? is a superset of that of N . 
Theorem 1 Every conditionally acyclic TCP-net is satisfiable.
Proof: We prove this constructively by building a satisfying preference ordering. In fact,
our inductive hypothesis will be stronger: any conditionally acyclic TCP-net has a strict
total order that satisfies it. The proof is by induction on the number of problem variables.
The result trivially holds for one variable by definition of CPTs, since we can simply use
any strict total order consistent with its CPT (and trivially satisfying Definition 6.)
Assume that the theorem holds for all conditionally acyclic TCP-nets with fewer than
n variables. Let N be a TCP-net over n variables, and X be one of the root variables of N .
(The existence of such a root X is guaranteed by Lemma 3.) Let D(X) = {x1 , . . . , xk } be
the domain of the chosen root variable X, and let x1 ≺ . . . ≺ xk be a total ordering of D(X)
that is consistent with the (possibly partial) preferential ordering dictated by CP T (X) in
N . For each xi , 1 ≤ i ≤ k, construct a TCP-net Ni , with n − 1 variables V − {X} by
removing X from the original network, and:
−−→
1. For each variable Y , such that there is a cp-arc hX, Y i ∈ N , revise the CPT of Y by
restricting each row to X = xi .
2. For each ci-arc γ = (Y1 , Y2 ), such that X ∈ S(γ), revise the CIT of γ by restricting
each row to X = xi . If, as a result of this restriction, all rows in the new CIT express
the same relative importance between Y1 and Y2 , replace γ in Ni by the corresponding
−−−→
−−−→
i-arc, i.e., either (Y1 , Y2 ) or (Y2 , Y1 ). Alternatively, if the CIT of γ becomes empty,
then γ is simply removed from Ni .
−−→
3. Remove the variable X, together with all cp-arcs of the form hX, Y i, and all i-arcs of
−−→
the form (X, Y ).
From Lemma 2 we have that conditional acyclicity of N implies conditional acyclicity
of all the reduced TCP-nets Ni . Therefore, by the inductive hypothesis we can construct
a preference ordering i for each of the reduced networks Ni . Now we can construct the
preferential ordering for the original network N as follows. Every outcome with X = xj is
ranked as preferred to any outcome with X = xi , for 1 ≤ i < j ≤ k. All the outcomes with
identical X value, xi , are ranked according to the ordering i associated with Ni (ignoring
the value of X). Clearly, by construction, the ordering we defined is a strict total order: it
was obtained by taking a set of strict total orders and ordering them, respectively. From
Definition 6, it is easy to see that this strict total order satisfies N . 
A close look at the proof of Theorem 1 reveals that the key property of conditionally
acyclic TCP-nets is that they induce an “ordering” over the nodes of the network. This
ordering is not fixed, but is context dependent. Different assignments to the variables in
the prefix of this ordering will yield different suffixes. Put differently, the ordering depends
403

Brafman, Domshlak, & Shimony

on the values of the variables, and it captures the relative importance of each variable in
each particular context. In particular, nodes that appear earlier in the ordering are more
important in this particular context.
The above observation helps explain the rationale for our definition of the dependency
graph (Definition 8). In some sense, this graph captures constraints on the ordering of
variables. The TCP-net is conditionally acyclic if these constraints are satisfiable. We use
this perspective to explain some choices made in the definition of the dependency graph
which may seem arbitrary. First, consider the direction of (unconditional) importance edges
from the more important to the less important variable. This simply goes in line with our
desire to use a topological ordering in which the more important variables appear first.
Second, consider the direction of CP-net edges from parent to children. It turns out that in
CP-nets, there is an induced importance relationship between parents and children: parents
are more important than their children (see (Boutilier et al., 2004a)). Thus, edges in the
dependency graph must point from parent to child.
Finally, in order to make sense of this idea of context-dependent ordering, we must
order the variables in the selector set of a ci-arc before the nodes connected by this arc.
The motivation for this last choice may be a bit less clear. The following example shows the
necessity of this (i.e., why Theorem 1 cannot be provided for a stronger notion of TCP-net
acyclicity obtained by defining w-directed graphs over N rather than over N ? ).

c
c
A
B
C

89:;
?>=<
A

AB
BA
aa
bb
a : cc
a : cc

C


@ABC
GFED
B


@ABC
GFED
C

Consider a TCP-net as depicted above. This TCP-net N is defined over three boolean
−−→
variables V = {A, B, C}, and having cp = {hA, Ci}, ci = {(A, B)} with S(A, B) = {C},
and i = ∅. Clearly, the two possible w-directed graphs of N (not of N ? ) are acyclic.
Now, suppose that there exists a strict partial order 0 over D(V) that satisfies N . By
Definition 6, we have
(1) abc 0 abc (from CP T (C)),
(2) abc 0 abc (from CIT ((A, B)) and CP T (B)),
(3) abc 0 abc (from CP T (C)), and
(4) abc 0 abc (from CIT ((A, B)) and CP T (A)).
However, this implies that 0 is not anti-symmetric, contradicting our assumption that 0
is a strict partial order.
404

TCP-Nets

5. Verifying Conditional Acyclicity
In contrast to standard acyclicity in directed graphs, the property of conditional acyclicity
cannot be easily tested in general. Naive verification of the acyclicity of every w-directed
graph can require time exponential in the size of S(N ). Here we study the complexity
of verifying conditional acyclicity, discuss some hard and polynomial subclasses of this
problem, and provide some sufficient and/or necessary conditions for conditional acyclicity
that can be easily checked for certain subclasses of TCP-nets.
Let N be a TCP-net. If there are no cycles in the undirected graph underlying N ?
(i.e., the graph obtained from N ? by making all directed edges into undirected edges),
then clearly all w-directed graphs of N ? are acyclic, and this property of N ? is simple to
check. Alternatively, suppose that the underlying undirected graph of N ? does contain
cycles. If projection of each such cycle back to N ? contains directed arcs oriented in
different directions on the cycle (one “clockwise” and another “counter-clockwise”), then
all w-directed graphs of N ? are still guaranteed to be acyclic. For instance, any subset (of
size > 2) of the variables {T, A, S, C} in our running example in Figure 4 forms a cycle
in the undirected graph underlying N ? , yet each such cycle satisfies the aforementioned
criterion. This sufficient condition for conditional acyclicity can also be checked in (low
order) polynomial time.
The remaining cases are where the dependency graph N ? contains what we define below
as semi-directed cycles, and in the rest of this section we study these cases more closely.
Definition 11 Let A be a mixed set of directed and undirected edges, and AU be the undirected graph underlying A (that is, the graph obtained from A by dropping orientation of its
directed edges.) We say that A is a semi-directed cycle if and only if
(1) AU forms a simple cycle (that is, AU consists of a single connected component with all
vertices having degree 2 w.r.t. AU ).
(2) Not all of the edges in A are directed.
(3) All the directed edges of A point in the same direction along AU (i.e., “clockwise” or
“counter-clockwise”).
Each assignment w to the selector sets of ci-arcs in a semi-directed cycle A of N ?
induces a direction for all these ci-arcs. We say that semi-directed cycle A is conditionally
acyclic if under no such assignment w do we obtain a directed cycle from A. Otherwise,
A is called conditionally directed. Figure 5 illustrates a semi-directed cycle (based on the
variables from our running example) with two possible configurations of its CITs that make
this semi-directed cycle conditionally directed and conditionally acyclic, respectively.
Using these notions, Lemma 4 shows that testing conditional acyclicity for TCP-nets is
naturally decomposable.
Lemma 4 A TCP-net N is conditionally acyclic if and only if every semi-directed cycle of N ?
is conditionally acyclic.
Proof: The proof is straightforward: If there is a variable assignment that makes one of
the semi-directed cycles of N ? conditionally directed, then no other cycle need be examined.
405

Brafman, Domshlak, & Shimony

?>=<
89:;
T

D


D1d

89:;
?>=<
A
O

D1d

SC
AT

(a)


?>=<
89:;
S

D


89:;
?>=<
C

D1d
D1d

SC
T A

(b)

Figure 5: A semi-directed cycle: (a) conditionally directed, and (b) conditionally acyclic.
Conversely, consider one of the semi-directed cycles A of N ? . If no assignment to S(A)
makes A conditionally directed, then additional assignments to variables in other selector
sets do not change this property. 
The decomposition presented by Lemma 4 allows us to prove our first complexity result
for testing conditional acyclicity. Theorem 2 below shows that determining that a TCP-net
is conditionally acyclic is coNP-hard.
Theorem 2 Given a binary-valued TCP-net N , the decision problem: is there a conditionally
directed cycle in N ? , is NP-complete, even if for every ci-arc γ ∈ N we have |S(γ)| = 1.
Proof: The proof of hardness is by reduction from 3-sat. Given a 3-cnf formula F,
construct the following TCP-net N . For every variable Xi and every clause Cj in F,
construct a boolean variable Xi and variable Cj in N , respectively (we retain the same
names, for simplicity). In addition, for every clause Cj , construct three boolean variables
Lj,k , 1 ≤ k ≤ 3, corresponding to the literals appearing in Cj . Let n be the number of
clauses in F. The TCP-net N is somewhat degenerate, since it has no cp-arcs. However,
−−−−−→
it has an i-arc (Cj , Lj,k ) for each clause Cj and every literal Lj,k ∈ Cj . In addition, for
every literal Lj,k ∈ Cj , there is a ci-arc (Lj,k , C(j+1) mod n ), whose selector variable is the
variable Xi represented in Lj,k . The relative importance between Lj,k and C(j+1) mod n on
the selector Xi as follows: if Lj,k is a positive literal, then variable Lj,k is more important
than C(j+1) mod n if Xi is true, and less important if Xi is false. For negative literals, the
dependence on the selector variable is reversed. This completes the construction - clearly a
polynomial-time operation. Figure 6 illustrates the subnet of N corresponding to a clause
Cj = (x1 ∨ x2 ∨ x3 ), where Lj,1 , Lj,2 , Lj,3 correspond to x1 , x2 , x3 , respectively.
We claim that N ? , the dependency graph for the network N we just constructed, has a
conditionally directed cycle just when F is satisfiable5 . It is easy to see that there is a path
from Cj to C(j+1) mod n just when the values of the variables participating in Cj are such
that Cj is satisfied. Thus, an assignment that creates a directed path from C0 to C0 is an
5. In this particular construction, the directed edges in N ? outgoing from the selector variables Xi have
no effect on the existence of conditionally directed cycles in N ? . Therefore, here we can simply consider
the TCP-net N instead of its dependency graph N ? .

406

TCP-Nets

ONML
HIJK
Lj,1
;;
B
;;


;;




X1

;;

;;


;;

X
2
PQRS
HIJK
WVUT
PQRS
 / ONML
Cj+1
Lj,2
Cj
 WVUT
::

::

::


::

::
 X3

::



HIJK
ONML
Lj,3

GFED
@ABC
ONML
HIJK
@ABC
GFED
@ABC
GFED
Lj,1 o
X
X
X1
r 2
sy 3
;;
B
r
s

r
y
s
r

;
y
s

;;
rr 

ssyy
;;
rrr  sssysyy

r

r
;; rr

 sss yy
;r;rr

 sss yyy
r

;

r
r
;   sss yyy

ys yy
xrrr
PQRS
WVUT
PQRS
WVUT
HIJK
ONML
/
Cj+1 yyy
Lj,2
Cj
::
 yyy
::
 yyy

::
 y
::
yyyy

::
 y
::
yy

|yyy
HIJK
ONML
Lj,3

(a)

(b)

Figure 6: (a) TCP-net subnet for Cj = (x1 ∨ x2 ∨ x3 ), and (b) its dependency graph.
assignment that satisfies all clauses, and the problems are equivalent - hence our decision
problem is NP-hard. Deciding existence of a conditional directed cycle is in NP: Indeed,
verifying the existence of a semi-directed cycle A given an assignment to S(A) (the union
of the selector sets of all ci-arcs in A) can be done in polynomial time. Thus, the problem
is NP-complete. 
One reason for the complexity of the general problem, as emerges from the proof of
Theorem 2, is the possibility that the number of semi-directed cycles in the TCP-net dependency graph is exponential in the size of the network. For example, the network in the
reduction has 3n semi-directed cycles, due to the three possible paths generated in each
subnet as depicted in Figure 6(a). Thus, it is natural to consider networks for which the
number of semi-directed cycles is not too large. In what follows, we call a TCP-net N
m-cycle bounded if the number of different semi-directed cycles in its dependency graph N ?
is at most m.
From Lemma 4 it follows that, given an m-cycle bounded TCP-net N , if m is polynomial
in the size of N , then we can reduce testing conditional acyclicity of N ? into separate
tests for conditional acyclicity of every semi-directed cycle A of N ? . A naive check for the
conditional acyclicity of a semi-directed cycle A requires time exponential in the size of S(A)
– where S(A) is the union of the selector sets of all ci-arcs in A. Thus, if S(A) is small for
each semi-directed cycle in N ? , then conditional acyclicity of N ? can be checked quickly. In
fact, often we can determine that a semi-directed cycle A is conditionally directed/acyclic
even more efficiently than enumerating all possible assignments to S(A).
Lemma 5 Let A be a semi-directed cycle in N ? . If A is conditionally acyclic, then it contains
a pair of ci-arcs γi , γj such that S(γi ) ∩ S(γj ) 6= ∅.
In other words, if the selector sets of the ci-arcs in A are all pairwise disjoint, then A
is conditionally directed. Thus, Lemma 5 provides a necessary condition for conditional
acyclicity of A that can be checked in time polynomial in the number of variables.
407

Brafman, Domshlak, & Shimony

Proof (Lemma 5) If all selector sets of the ci-arcs in A are pairwise disjoint, then trivially
there exists an assignment to S(A) orienting all the ci-arcs of A in one direction. 
Before developing sufficient conditions for conditional acyclicity, let us introduce some
useful notation. First, given a ci-arc γ = (X, Y ), we say that an assignment w to a subset
S 0 of S(γ) orients γ if all rows in CIT (γ) consistent with w express the same relative
importance between X and Y , if any. In other words, w orients γ if, given w, the relative
importance between X and Y is independent of S(γ) \ S 0 . Second, if a semi-directed cycle
A contains some directed edges, we refer to their (by definition, unique) direction as the
direction of A.
Lemma 6 A semi-directed cycle A is conditionally acyclic if it contains a pair of ci-arcs γi , γj
such that either:
(a) A contains directed edges, and for every assignment w to S(γi ) ∩ S(γj ), either γi or γj is
oriented by w in the direction opposite to the direction of A.
(b) All edges in A are undirected, and for every assignment w to S(γi ) ∩ S(γj ), γi and γj are
oriented by w in opposite directions with respect to A.

Proof:

Follows immediately from the conditions in the lemma. 

Lemma 6 provides a sufficient condition for conditional acyclicity of A that can be
checked in time exponential in the maximal size of selector set intersection for a pair of
ci-arcs in A. Note that the size of the TCP-net is at least as large as the above exponential
term, because the description of the CIT is exponential in the size of the corresponding
selector set. Thus, checking this condition is only linear in the size of the network.
Definition 12 Given a semi-directed cycle A, let shared(A) denote the union of all pairwise
intersections of the selector sets of the ci-arcs in A:
[
shared(A) =
S(γi ) ∩ S(γj )
γi ,γj ∈A

Lemma 7
(a) If a semi-directed cycle A contains directed edges, then A is conditionally acyclic if and
only if, for each assignment u on shared(A), there exists a ci-arc γu ∈ A that is oriented
by u in the direction opposite to the direction of A.
(b) If a semi-directed cycle A contains only ci-arcs, then A is conditionally acyclic if and only
if, for each assignment u on shared(A), there exist two ci-arcs γu1 , γu2 ∈ A that are oriented
by u in opposite directions with respect to A.

Proof: The sufficiency of the above condition is clear, since it subsumes the condition in
Lemma 6. Thus, we are left with proving necessity. We start with the second case in which
408

TCP-Nets

A contains only ci-arcs. Assume to the contrary that A is conditionally acyclic, but there
exists an assignment u on shared(A) such that no pair of ci-arcs in A are oriented by u in
opposite directions with respect to A.
For each ci-arc γ ∈ A, let S ∗ (γ) = S(γ) \ shared(A). Consider the following disjoint
partition A = Aiu ∪ Aci
u induced by u on A: For each ci-arc γ ∈ A, if u orients γ, then we
have γ ∈ Aiu . Otherwise, if the direction of γ is not independent of S ∗ (γ) given u, we have
γ ∈ Aci
u . We make two observations:
1. Our initial (contradicting) assumption implies that all the (now directed) edges in Aiu
agree on the direction with respect to A.
2. If for some ci-arc γ ∈ A we have S ∗ (γ) = ∅, then we have γ ∈ Aiu , since all the
selectors of γ are instantiated by u.
If we have Aci
u = ∅, then the first observation trivially contradicts our initial assumption
that A is conditionally acyclic. Alternatively, if Aci
u 6= ∅, then, by definition of shared(A),
∗
∗
we have that S (γi ) ∩ S (γj ) = ∅ for each pair of ci-arcs γi , γj ∈ Aci
u . This means that
we can assign each such (non-empty, by the second observation) S ∗ (γi ) independently, and
thus can extend u into an assignment on S(A) that will orient all the ci-arcs in Aci
u either
in the direction of Aiu if Aiu 6= ∅, or in an arbitrary joint direction if Aiu = ∅. Again, this
contradicts our assumption that A is conditionally acyclic. Hence, we have proved that our
condition is necessary for the second case. The proof for the first case in which A contains
some directed edges is similar. 
In general, the size of shared(A) is O(|V|). Since we have to check the set of assignments
over shared(A), this implies that the problem may be hard. Theorem 3 shows that this is
indeed the case.
Theorem 3 Given a binary-valued, 1-cycle bounded TCP-net N , the decision problem: is
there a conditionally directed cycle in N ? , is NP-complete, even if for every ci-arc γ ∈ N we
have |S(γ)| ≤ 3.
Proof: The proof of hardness is by reduction from 3-sat. Given a 3-cnf formula F,
construct the following TCP-net N . For every variable Xi and every clause Cj in F,
construct boolean variables Xi and Cj in N , respectively. In addition, add a single dummy
−−−→
variable C, and an i-arc (C, C1 ). Let n be the number of clauses in F. For 1 ≤ j ≤ n − 1,
we have n − 1 ci-arcs Ej = (Cj , Cj+1 ). In addition, we have ci-arc En = (Cn , C). For all
1 ≤ j ≤ n, the CIT for Ej is determined by clause Cj , as follows. The selector set for Ej is
just the set of variables appearing in Cj , and the relative importance between the variables
of Ej is determined as follows: Cj is less important than Cj+1 just when the values of the
variables in the selector set are such that Cj is false. (For j = n, read C instead of Cj+1 ).
The constructed TCP-net N is 1-cycle bounded, because there is only one semi-directed
cycle in its dependency graph N ? , namely C, C1 , . . . , Cn , C. We claim that this semidirected cycle is conditionally directed just when F is satisfiable. It is easy to see that the
directed path from C to C exists when all the ci-arcs are being directed from Cj to Cj+1 ,
which occurs exactly when the variable assignment makes the clause Cj satisfiable. Hence,
409

Brafman, Domshlak, & Shimony

a directed cycle occurs in N exactly when the assignment makes all clauses satisfiable,
making the two problems equivalent. Thus our decision problem is NP-hard. Finally, as
deciding existence of a conditional directed cycle is in NP (see the proof of Theorem 3), the
problem is NP-complete. 
Observe that the proof of Theorem 3 does not work when the size of all the selector
sets is bounded by 2, because 2-sat is in P. The immediate question is whether in this
latter case the problem becomes tractable, and for binary-valued TCP-nets the answer is
affirmative.
Theorem 4 Given a binary-valued, m-cycle bounded TCP-net N , where m is polynomial in
the size of N and, for every ci-arc γ ∈ N we have |S(γ)| ≤ 2, the decision problem: is there a
conditional directed cycle in N ? , is in P.

Proof: The proof uses a reduction from conditional acyclicity testing to satisfiability. Let
A be a semi-directed cycle with |S(γ)| ≤ k for every ci-arc γ ∈ A. We reduce the conditional
acyclicity testing problem to an equivalent k-sat problem instance. In particular, since 2sat is solvable in linear time (Even, Itai, & Shamir, 1976), together with Lemma 4 this
proves the claim.
First, assume that A has at least one directed edge (either i-arc or cp-arc). By definition
of semi-directed cycles, all directed edges of A point in the same direction, specifying the
only possible cyclic orientation ω of A. For each ci-arc γi ∈ A, let the selector set be
S(γi ) = {Xi,1 , ..., Xi,k }.6 Clearly, A is conditionally directed if and only if all the ci-arcs of
A can be directed consistently with ω.
Given such a semi-directed cycle A, we create a corresponding k-cnf formula F, such
that F is satisfiable just when A is conditionally directed. Let us call all CIT (γi ) entries that
are consistent with ω by the term ω-entries. Since S(γ) = {Xi,1 , ..., Xi,k } and N is binary
valued, we can represent the non-ω entries in CIT (γi ) as a conjunction of disjunctions,
i.e., in CNF form. The number of disjunctions is equal to the number of non-ω entries
in CIT (γi ), and each disjunction is comprised of k literals. Thus, the representation of
CIT (γi ) is a k-CNF formula, of size linear in the size of CIT (γi ). (In fact, the size of the
resulting formula can sometimes be significantly smaller, as one can frequently simplify the
component CNF fragments, but this property is not needed here.)
Finally, compose all the CNF representations of the CIT (γi ), for every γi ∈ A, resulting
in a k-CNF formula of size linear in the combined number of table entries. The construction
of F is clearly a linear-time operation. Likewise, it is easy to see that F is satisfiable just
when there is an assignment to S(A) converting A into a directed cycle.
The minor unresolved issue is with semi-directed cycles consisting of ci-arcs only. Given
such a semi-directed cycle A, we reduce the problem into two sub-problems with a directed
arc. Let A0 and A00 be cycles created from A by inserting one dummy variable and one i-arc
into A – clockwise for A0 , counter-clockwise for A00 . Now, A is conditionally directed if and
only if either A0 or A00 (or both) are conditionally directed. 
6. If |S(γi )| < k, the only impact will be a more compact reduction below.

410

TCP-Nets

To summarize our analysis of verifying conditional acyclicity, one must first identify the
semi-directed cycles in the dependency graph of the TCP-net. Next, one must show that
given each assignment w to the importance-conditioning variables of each semi-directed
cycle, the w-directed graph is acyclic. This problem is coNP-hard in general networks7 ,
but there are interesting classes of networks in which it is tractable. This is the case when
the number of semi-directed cycles is not too large and either the size of shared(A) for
each such cycle or the size of each selector set is not too large. Note that in practice, one
would expect to have small selector sets – statements such as “X is more important than
Y when A = a and B = b and . . . and Z = z” appear to be more complex than what
one would expect to hear. Thus, Lemma 6, Lemma 7 (for semi-directed cycles with small
shared(A)), and Theorem 4 are of more than just theoretical interest. Naturally, extending
the toolbox of TCP-net subclasses that can be efficiently tested for consistency is clearly of
both theoretical and practical interest.

6. Reasoning about Conditionally Acyclic TCP-nets
While automated consistency verification is the core part of the preference elicitation stage,
efficiency of reasoning about user preferences is one of the main desiderata of any model
for preference representation. Of particular importance is the task of preference-based
optimization and constrained optimization, which we discuss in the first part of this section.
Another important task, which provides an important component in the algorithm for
constrained optimization we present, is outcome comparison – discussed in the second part
of this section.
6.1 Generating Optimal Assignments
Following the notation of Boutilier et al. (2004a), if x and y are assignments to disjoint
subsets X and Y of the variable set V, respectively, we denote the combination of x and y
by xy. If X ∩ Y = ∅ and X ∪ Y = V, we call xy a completion of assignment x, and denote
by Comp(x) the set of all completions of x.
One of the central properties of the original CP-net model (Boutilier et al., 2004a)
is that, given an acyclic CP-net N and a (possibly empty) partial assignment x on its
variables, it is simple to determine an outcome consistent with x (a completion of x) that
is preferentially optimal with respect to N . The corresponding linear time forward sweep
procedure is as follows: Traverse the variables in some topological order induced by N , and
set each unassigned variable to its most preferred value given its parents’ values.
Our immediate observation is that this procedure works as is also for conditionally
acyclic TCP-nets: The relative importance relations do not play a role in this case, and
the network is traversed according to a topological order induced by the CP-net part of
the given TCP-net. In fact, Corollary 1 holds for any TCP-net that has no directed cycles
consisting only of cp-arcs.
Corollary 1 Given a conditionally acyclic TCP-net and a (possibly empty) partial assignment x
on its variables, the forward sweep procedure constructs the most preferred outcome in Comp(x).
7. This actually means that when the network is not too large, we can probably solve this in a reasonable
amount of time.

411

Brafman, Domshlak, & Shimony

This strong computational property of outcome optimization with respect to acyclic CPnets (and conditionally acyclic TCP-nets) does not hold if some of the TCP-net variables
are constrained by a set of hard constraints, C. In this case, determining the set of preferentially non-dominated8 feasible outcomes is not trivial. For acyclic CP-nets, a branch and
bound algorithm for determining the optimal feasible outcomes was introduced by Boutilier,
Brafman, Domshlak, Hoos, and Poole (2004b). This algorithm has the important anytime
property – once an outcome is added to the current set of non-dominated outcomes, it is
never removed. An important implication of this property is that the first generated assignment that satisfies the set of hard constraints is also preferentially non-dominated. In other
words, finding just one non-dominated solution in this algorithm boils down to solving the
underlying CSP under certain variable and value ordering strategies.
Here we develop an extension/modification of the algorithm of Boutilier et al. (2004b)
to conditionally acyclic TCP-nets. The extended algorithm Search-TCP retains the anytime property and is shown in Figure 7. The key difference between processing an acyclic
CP-net and a conditionally acyclic TCP-net is that the semantics of the former implicitly induces a single partial order of importance over the variables (where each node precedes its descendants) (Boutilier et al., 2004a), while the semantics of the latter induces a
hierarchically-structured set of such partial orders: Each such partial order corresponds to
a single assignment to the set of selector variables of the network, or, more specifically, to
a certain w-directed graph.
Formally, the problem is defined by a conditionally acyclic TCP-net Norig , and a set of
hard constraints Corig , posed on the variables of Norig . The Search-TCP algorithm (depicted
in Figure 7) is recursive, and each recursive call receives three parameters:
1. A TCP-net N , which is a subnet of the original conditionally acyclic TCP-net Norig ,
2. A set C of hard constraints among the variables of N , which is a subset of the original
set of constraints Corig obtained by restricting Corig to the variables of N , and
3. An assignment K to all the variables in Norig − N . In what follows, we refer to this
assignment K as a context.
The initial call to Search-TCP is done with Norig , Corig , and {}, respectively.
Basically, the Search-TCP algorithm starts with an empty set of solutions, and gradually
extends it by adding new non-dominated solutions to Corig . At each stage of the algorithm,
the current set of solutions serves as a “lower bound” for future candidates; A new candidate
at any point is compared to all solutions generated up to that point. If the candidate is
dominated by no member of the current solution set, then it is added into this set.
The Search-TCP algorithm is guided by the graphical structure of Norig . It proceeds
by assigning values to the variables in a top-down manner, assuring that outcomes are
generated in an order that satisfies (i.e., consistent with) N . On a recursive call to the
Search-TCP procedure with a TCP-net N , the eliminated variable X is one of the root
variables of N (line 1). Recall that, by Lemma 3, conditional acyclicity of N guarantees
the existence of such a root variable X. The values of X are considered according to
8. An outcome o is said to be non-dominated with respect to some preference order  and a set of outcomes
S if there is no other o0 ∈ S such that o0  o.

412

TCP-Nets

Search-TCP (N , C, K)

Input: Conditionally acyclic TCP-net N ,
Hard constraints C on the variables of N ,
Assignment K to the variables of Norig \ N .
Output: Set of all, non-dominated w.r.t. N , solutions for C.
−−→
1. Choose any variable X s.t. there is no cp-arc hY, Xi,
−−→
no i-arc (Y, X), and no (X, Y ) in N .
2. Let x1  . . .  xk be a total order on D(X) consistent with the preference
ordering of D(X) by the assignment on P a(X) in K.
3. Initialize the set of local results by R = ∅
4. for (i = 1; i ≤ k; i + +) do
5. X = xi
6. Strengthen the constraints C by X = xi to obtain Ci
7. if Cj ⊆ Ci for some j < i or Ci is inconsistent then
8.
continue with the next iteration
else
9.
Let K0 be the partial assignment induced by X = xi and Ci
10.
Ni = Reduce (N ,K0 )
11.
Let Ni1 , . . . , Nim be the components of Ni , connected
either by the edges of Ni or by the constraints Ci .
12.
for (j = 1; j ≤ m; j + +) do
13.
Rji = Search-TCP(Nij , Ci , K ∪ K0 )
14.
if Rji 6= ∅ for all j ≤ m then
15.
foreach o ∈ K0 × R1i × · · · × Rm
i do
0
16
if K ∪ o 6 K ∪ o holds for each o0 ∈ R then add o to R
17. return R

Figure 7: The Search-TCP algorithm for conditionally acyclic TCP-net based constrained
optimization.

the preference ordering induced on D(X) by the assignment provided by the context K to
P a(X) (where P a(X) is defined with respect to Norig ). Note that K necessarily contains
some assignment to P a(X) since X is a root variable of the currently considered subnet N
of Norig . Any additional variable assignment X = xi converts the current set of constraints
C into a strictly non-weaker constraint set Ci . As a result of this propagation of X = xi ,
values for some variables (at least, the value of X) are fixed automatically, and this partial
assignment K0 extends the current context K in recursive processing of the next variable.
The Reduce procedure, presented in Figure 8, refines the TCP-net N with respect to K0 :
For each variable assigned by K0 , we reduce both the CPTs and the CITs involving this
variable, and remove this variable from the network. This reduction of the CITs may remove
conditioning of relative importance between some variables, and thus convert some ci-arcs
413

Brafman, Domshlak, & Shimony

into i-arcs, and/or remove some ci-arcs completely. The main point is that, in contrast to
CP-nets, for a pair of X values xi , xj , the variable elimination orderings for processing the
networks Ni and Nj , resulting from propagating Ci and Cj , respectively, may disagree on
the ordering of some variables.
Reduce (N , K0 )
1. foreach {X = xi } ∈ K0 do
−−→
2. foreach cp-arc hX, Y i ∈ N do
3.
Restrict the CPT of Y to the rows dictated by X = xi .
4. foreach ci-arc γ = (Y1 , Y2 ) ∈ N s.t. X ∈ S(γ) do
5.
Restrict the CIT of γ to the rows dictated by X = xi .
6.
if, given the restricted CIT of γ, relative importance
between Y1 and Y2 is independent of S(γ), then
7.
if CIT of γ is not empty then
8.
Replace γ by the corresponding i-arc.
9.
else Remove γ.
10. Remove from N all the edges involving X.
11. return N .

Figure 8: The Reduce procedure.
If the partial assignment K0 causes the current CP-net to become disconnected with
respect to both the edges of the network and the inter-variable hard constraints, then
each connected component invokes an independent search (lines 11-16). This is because
optimization of the variables within such a component is independent of the variables outside
that component. In addition, after strengthening the set of constraints C by X = xi to Ci
(line 6), some pruning takes place in the search tree (lines 7-8): If the set of constraints
Ci is strictly more restrictive than some other set of constraints Cj = C ∪ {X = xj } where
j < i, then the search under X = xi is not continued. The reason for this pruning is that
it can be shown that any feasible outcome a involving X = xi is dominated by (i.e., less
preferable than) some feasible outcome b involving X = xj and thus a cannot be in the
set of non-dominated solutions for the original set of constraints9 . Therefore, the search is
depth-first branch-and-bound, where the set of non-dominated solutions generated so far is
a proper subset of the required set of all the non-dominated solutions for the problem, and
thus it corresponds to the current lower bound.
When the potentially non-dominated solutions for a particular subgraph are returned
with some assignment X = xi , each such solution is compared to all non-dominated solutions
involving more preferred (in the current context K) assignments X = xj , j < i (line 16).
A solution with X = xi is added to the set of the non-dominated solutions for the current
subgraph and context if and only if it passes this non-domination test. From the semantics
9. This pruning was introduced by Boutilier et al. (2004b) for acyclic CP-nets, and it remains valid the
same way for conditionally acyclic TCP-nets. For the proof of soundness of this pruning technique we
refer the reader to Lemma 2 in (Boutilier et al., 2004b).

414

TCP-Nets

of the TCP-nets, given the same context K, a solution involving X = xi can not be preferred
to a solution involving X = xj , j < i. Thus, the generated global set R never shrinks.
Theorem 5 Given a conditionally acyclic TCP-net N and a set of hard constraints C over the
variables of N , an outcome o belongs to the set R generated by the algorithm Search-TCP if
and only if o is consistent with C, and there is no other outcome o0 consistent with C such that
N |= o0  o.
Proof: Let RC be the desired set of all the preferentially non-dominated solution to C.
To prove this theorem, we should show that:
1. Completeness: No preferentially non-dominated solution to C is pruned out, that is,
we have R ⊇ RC , and
2. Soundness: The resulting set R contains no preferentially dominated solution to C,
that is, R ⊆ RC .
(1) The solutions to C are pruned by Search-TCP only in two places, namely at the search
space pruning in lines 7-8, and at the non-dominance test step in line 16. For the first case,
the correctness of the pruning technique used in lines 7-8 is given by Lemma 2 in (Boutilier
et al., 2004b), and thus this pruning does not violate completeness of Search-TCP. For the
second case, if an explicitly generated solution o is rejected due to the failure of its nondominance test, then o 6∈ RC is apparent since the rejection of o here is based on presenting
a concrete solution o0 such that N |= o0  o. Hence, we have R ⊇ RC .
(2) To show R ⊆ RC it is enough to prove that a newly generated solution cannot dominate
an existing solution, that is, if o was added to the generated set of solutions after o0 then
it is not the case that N |= o  o0 . The proof is by induction on the number of problem
variables. First, the claim trivially holds for any one-variable TCP-net, as the order in which
the solutions are examined in line 16 coincides with the total order selected for the single
variable of the network in line 2. Now, assume that the claim holds for all conditionally
acyclic TCP-nets with fewer than n variables. Let N be a TCP-net over n variables, C be
a set of hard constraints on these variables, and X be the root variable of N selected in
line 1. Let R = {o1 , . . . , or } be the output of Search-TCP for these N and C, where the
elements of R are numbered according to the order of their non-dominance examination in
line 16. Now, assume that there exists a pair of assignments oi , oj ∈ R, such that i < j, yet
N |= oj  oi .
First, suppose that oi and oj provide the same value to X, that is oi = xl o0i and
oj = xl o0j , for some xl ∈ D(X). In this case, however, o0i and o0j belong to the output of the
same recursive call to Search-TCP with Nl and Cl , and thus, by our inductive hypothesis, o0i
and o0j are preferentially incomparable. Likewise, Nl is obtained in line 10 by reducing N
with respect to xl , and thus the variables of Nl are preferentially independent of X. Hence,
preferential incomparability of o0i and o0j implies preferential incomparability of oi and oj ,
and thus N |= oj  oi cannot be the case.
Alternatively, suppose that oi and oj provide two different values to X, that is oi = xl o0i
and oj = xm o0j , xl , xm ∈ D(X), where D(X) is numbered according to the total ordering of
415

Brafman, Domshlak, & Shimony

its values selected in line 2. Observe that, by the construction of Search-TCP, i < j trivially
implies l < m. However, using the arguments identical to these in the constructive proof of
Theorem 6, there exists at least one preference order  of the complete assignments to the
variables of N in which we have oi  oj . Hence, it cannot be the case that N |= oj  oi ,
and thus contradiction of our assumption that N |= oj  oi is now complete. 
Note that, if we are interested in getting one non-dominated solution for the given set of
hard constraints (which is often the case), we can output the first feasible outcome generated
by Search-TCP. No comparisons between pairs of outcomes are required because there is
nothing to compare with the first generated solution. However, if we are interested in getting
all, or even a few non-dominated solutions, then the efficiency of preferential comparison
between pairs of outcomes becomes an important factor in the entire complexity of the
Search-TCP algorithm. Hence, in the next section we consider such preferential comparisons
more closely.
6.2 Dominance Testing for TCP-nets
One of the most fundamental queries in any preference-representation formalism is whether
some outcome o dominates (i.e., is strictly preferred to) some other outcome o0 . As discussed
above, such dominance queries are required whenever we wish to generate more than one
non-dominated solution to a set of hard constrains. Much like in CP-nets, a dominance
query hN , o, o0 i with respect to a TCP-net can be treated as a search for an improving
flipping sequence from the (purported) less preferred outcome o0 to the (purported) more
preferred outcome o through a sequence of successively more preferred outcomes, such
that each flip in this sequence is directly sanctioned by the given TCP-net. Formally, an
improving flipping sequence in the context of TCP-nets can be defined as follows:
Definition 13 A sequence of outcomes
o0 = o0 ≺ o1 ≺ · · · ≺ om−1 ≺ om = o
is an improving flipping sequence with respect to a TCP-net N if and only if, for 0 ≤ i < m,
either
1. (CP-flips) outcome oi is different from the outcome oi+1 in the value of exactly one
variable Xj , and oi [j] ≺ oi+1 [j] given the (identical) values of P a(Xj ) in oi and oi+1 ,
or
2. (I-flips) outcome oi is different from the outcome oi+1 in the value of exactly two
variables Xj and Xk , oi [j] ≺ oi+1 [j] and oi [k]  oi+1 [k] given the (identical) values
of P a(Xj ) and P a(Xk ) in oi and oi+1 , and Xj  Xk given RI(Xj , Xk |Z) and the
(identical) values of Z in oi and oi+1 .10
Clearly, each value flip in such a flipping sequence is sanctioned by the TCP-net N , and
the CP-flips are exactly the flips allowed in CP-nets (Boutilier et al., 2004a).
10. We implicitly assumed that neither node is the parent of the other. An implicit consequence of the
standard semantics of conditional preferences is a node is more important than its children. Thus, there
is no need to specify this explicitly.

416

TCP-Nets

Theorem 6 Given a TCP-net N and a pair of outcomes o and o0 , we have that N |= o  o0
if and only if there is an improving flipping sequence with respect to N from o0 to o.
Proof:
⇐= Given an improving flipping sequence F:
o0 = o0 ≺ o1 ≺ · · · ≺ om−1 ≺ om = o
from o0 to o with respect to N , by Definition 13, we have N |= oi  oi+1 for any improving
flip from F. The proposition follows from the transitivity of preferential entailment with
respect to TCP-nets (Lemma 1).
=⇒ Let G be the graph of preferential ordering induced by N , i.e., nodes of G stand for all
outcomes, and there is a directed edge from o1 to o2 if and only if there is an improving
CP-flip or I-flip of o1 to o2 , sanctioned by N . Clearly, directed paths in G are equivalent to
improving flipping sequences with respect to N .
First, we show that any preference ordering  that respects the paths in G (that is,
if there is a path from o1 to o2 in G, then we have o2  o1 ) satisfies N . Assume to the
contrary that ∗ respects the paths in G, and does not satisfy N . Then, by the definition
of satisfiability (Definition 7), there must exist either:
1. Some variable X, assignment p ∈ D(P a(X)), values x, x0 ∈ D(X), and assignment
w to the remaining variables W = V − (X ∪ P a(X)), such that pxw ∗ px0 w, but
CP T (X) dictates that x0  x given p, or
2. Some importance arc ξ between a pair of variables X and Y , assignment z ∈ D(S(ξ))
(if ξ is an i-arc, then S(ξ) = ∅), values x, x0 ∈ D(X), y, y 0 ∈ D(Y ), and assignment w
to the remaining variables W = V − ({X, Y } ∪ S(ξ)), such that pxyw ∗ px0 y 0 w, but
(i) the CP T (X) dictates that x0  x, and (ii) the (possibly empty) CIT of ξ dictates
that X  Y given z.

However, in the first case, if N specifies x0  x given p, there is a CP-flip from px0 w to pxw,
contradicting the fact that ∗ extends G. Similarly, in the second case, if N specify x0  x
given w, and X  Y given z, then there is an I-flip from px0 y 0 w to pxyw, contradicting the
fact that ∗ extends G.
Now, by the construction of G, if there is no improving flipping sequence from o0 to
o, then there is no directed path in G from o0 to o. Therefore, there exist a preference
ordering ∗ respecting the paths in G in which o0 ∗ o. However, based on the above
observation on preference orderings respecting the paths in G, ∗ also satisfies N , which
implies N 6|= o  o0 . 
Various methods can be used to search for a flipping sequence. In particular, we believe that at least some of the techniques, developed for this task with respect to CPnets by Domshlak and Brafman (2002), Domshlak (2002), and Boutilier et al. (2004a)
can be applied to the TCP-net model – an issue left for future research. However, in
general, dominance testing with respect to CP-nets (and thus TCP-nets) is known to be
417

Brafman, Domshlak, & Shimony

NP-hard (Boutilier et al., 2004a), thus in practice one may possibly consider performing
approximate constrained optimization, using the Search-TCP algorithm with a dominance
testing based on one of the tractable refinements of TCP-nets such as those discussed
by Brafman, Domshlak, and Kogan (2004a).

7. Discussion
CP-nets (Boutilier et al., 1999, 2004a) is a relatively new graphical model for representation
and reasoning about preferences. Its development, however, already stimulated research in
several directions (e.g., see (Brafman & Chernyavsky, 2005; Brafman & Dimopoulos, 2004;
Brewka, 2002; Boutilier et al., 2001; Domshlak et al., 2003; Rossi et al., 2004; Lang, 2002;
Wilson, 2004b, 2004a)). In this paper we introduced the qualitative notions of absolute
and conditional relative importance between pairs of variables and extended the CP-net
model to capture the corresponding preference statements. The extended model is called
TCP-nets. We identified a wide class of TCP-nets that are satisfiable, notably the class of
conditionally acyclic TCP-nets, and analyzed complexity and algorithms for testing membership in this class of networks. We also studied reasoning about TCP-nets, focusing on
outcome optimization in conditionally acyclic TCP-nets with and without hard constraints.
Our work opens several directions for future research. First, an important open theoretical question is the precise complexity of dominance testing in TCP-nets. In the context
of CP-nets this problem has been studied by Domshlak (2002), Boutilier et al. (2004a),
Goldsmith et al. (2005). Another question is the consistency of TCP-nets that are not
conditionally acyclic. A preliminary study of this issue in context of cyclic CP-nets has
been done by Domshlak and Brafman (2002) and Goldsmith et al. (2005).
The growing research on preference modeling is motivated by the need for preference
elicitation, representation, and reasoning techniques in diverse areas of AI and user-centric
information systems. In particular, one of the main application areas we have in mind
is this of automatic personalized product configuration (Sabin & Weigel, 1998). Thus, in
the remaining part of this section, we first consider the process of preference elicitation
with TCP-nets, listing a few practical challenges that should be addressed to make this
process appealing to users en-masse. Then, we relate our work to some other approaches
to preference-based optimization.
7.1 Preference Elicitation with TCP-nets (and Other Logical Models of
Preference)
The process of preference elicitation is known to be complex as into account should be taken
not only the formal model of the user’s preferences but also numerous important factors
of human-computer interaction (e.g., see (Faltings, Pu, Torrens, & Viappiani, 2004; Pu &
Faltings, 2004)). In this paper we focus on a formalism for structuring and analyzing the
user’s preferences, although for some (probably offline) applications, this formalism could
actually be used to drive the input process, much like a Bayes network can be used to help
experts express their beliefs.
Depending on the application, a schematic process of constructing a TCP-net would
commence by asking the decision maker to identify the variables of interest, or by presenting
them to the user, if they are fixed. For example, in the application of CP-net to adaptive
418

TCP-Nets

document presentation (Domshlak, Brafman, & Shimony, 2001; Brafman, Domshlak, &
Shimony, 2004b), the content provider chooses a set of content elements, which correspond
to the set of variables. For an online shopper-assistant agent, the variables are likely to
be fixed (e.g., if the agent is an online PC customizer) (Brafman et al., 2004a). Next, the
user is asked to consider for each variable, the value of which other variables influence her
preferences over the values of this variable. At this point cp-arcs and CPTs are introduced.
Next, the user is asked to consider relative importance relations, and the i and ci-arcs are
added. For each ci-arc, the corresponding CIT is filled.
Clearly, one may prefer to keep the preference elicitation process more user-driven, allowing the user simply provide us with a set of preference statements. But if such a set
of statements fits the language expressible by the TCP-nets model, then the specific TCPnet underlying these statements can be constructed from a simple analysis of referents and
conditionals of these statements. Such TCP-net extraction from the statements will be
simpler if these statements will be provided in this or another formal language, or obtained
via some carefully designed, structured user interface. However, for the user it is obviously
more natural to provide these statements in natural language. Hence, an interesting practical question related to elicitation of qualitative preferences is model acquisition from speech
and/or text (Asher & Morreau, 1995; Glass, 1999; Bethard, Yu, Thornton, Hatzivassiloglou,
& Jurafsky, 2004). Observe that the intuitiveness of the qualitative preferential statements
is closely related to the fact that they have a straightforward representation in natural language of everyday life. In addition, collections of typical preferential statements seem to
form a linguistic domain that is a priori constrained in a very special manner. This may
allow us to develop specialized techniques and tools for understanding the corresponding
language. Both offline and online language understanding should be considered, since a user
can either describe her preferences offline, as a self-contained text, or can be asked online, as
a part of interactive process of (possibly mixed) preference elicitation and preference-based
constrained optimization.
Yet another possible approach for eliciting TCP-nets, as well as some alternative logical
models of preferences, would be to allow the user expressing pair-wise comparisons between
completely specified choices, and then construct a TCP-net consistent with this input.
In the scope of quantitative models for preference representation, such an example-based
model generation has been adopted in numerous user-centric optimization systems (e.g.,
see (Linden, Hanks, & Lesh, 1997; Blythe, 2002).) However, devising such a framework for
learning qualitative models of preference seems to be somewhat more challenging. In theory,
nothing prevents us from adopting example-based generation of TCP-nets since the latter
can be seen as just a compact representation of a preference relation over a space of choices.
The question, however, is whether a reasonably small set of pair-wise choice comparisons
can provide us with a sufficient basis for learning not just a TCP-net consistent with these
“training examples”, but a compact TCP-net that will generalize in a justifiable manner
beyond the provided examples. To the best of our knowledge, so far this question has
been studied for no logical preference-representation models, and hence it clearly poses a
challenging venue for future research.11
11. Note that, if we are only interested in compact modeling of pair-wise comparisons between the choices,
then numerous techniques from the area of machine learning can be found useful. For instance, one can
learn a decision tree classifying ordered pairs of choices as “preferred” (first choice to the second choice)

419

Brafman, Domshlak, & Shimony

7.2 Related Work
As we show in Section 6, extending CP-nets to TCP-nets is appealing mainly in the scope of
decision scenarios where the space of all syntactically possible choices is (either explicitly or
implicitly) constrained by some hard constraints. We now review some related approaches
to preference-based optimization that appeared in the literature.
A primary example of preference-based optimization is the soft-constraints formalism
(e.g., see Bistarelli et al. (1997)), developed to model constraint satisfaction problems that
are either over-constrained (and thus unsolvable according to the standard meaning of
satisfaction) (Freuder & Wallace, 1992), or suffer from imprecise knowledge about the actual
constraints (Fargier & Lang, 1993). In this formalism, the constrained optimization problem
is represented as a set of preference orders over assignments to subsets of variables, together
with some operator for combining the preference relations over these subsets of variables to
a preference relation over the assignments to the whole set of variables. Each such subset
of variables corresponds to a soft constraint that can be satisfied to different extent by
different variable assignments. There is much flexibility in how such “local” preference
orders are specified, and how they are combined. Various soft-constraints models, such
as weighted (Bistarelli et al., 1999), fuzzy (Schiex, 1992), probabilistic (Fargier & Lang,
1993), and lexicographic (Fargier et al., 1993) CSPs, are discussed in the literature on soft
constraint satisfaction.
The conceptual difference between our approach and the soft-constraints formalism is
that the latter is based on tightly coupled representation of preferences and constraints,
while our representation of these two paradigms is completely decoupled. Informally, soft
constraints machinery has been developed for optimization of partial constraint satisfaction,
while we are dealing with optimization in the face of constraint satisfaction. For instance, in
personalized product configuration, there are two parties involved typically: the manufacturer and the consumer. The manufacturer brings forth its product expertise, and with it
a set of hard constraints on possible system configurations and the operating environment.
The user expresses her preferences over properties of the final product. Typically numerous
configurations satisfy the production constraints, and the manufacturer strives to provide
the user with maximal satisfaction by finding one of the most preferred, feasible product
configuration. This naturally leads to a decoupled approach.
Freuder and O’Sullivan (2001) proposed a framework of interactive sessions for overconstrained problems. During such a session, if the constraint solver discovers that no
solution for the current set of constraints is available, the user is asked to consider “tradeoffs”. For example, following Freuder and O’Sullivan (2001), suppose that the set of user
requirements for a photo-camera configuration tool is that the weight of the camera should
be less that 10 ounces and the zoom lens should be at least 10X. If no camera meets these
requirements, the user may specify tradeoffs such as “I will increase my weight limit to 14
ounces, if I can have a zoom lens of at least 10X” (possibly using some suggestions automatically generated by the tool). In turn, these tradeoffs are used for refining the current
set of requirements, and the system goes into a new constraint satisfaction process.
or “not preferred”. However, such a classification does not guarantee in general that the resulting binary
relation over the space of choices will be anti-symmetric under the assumption of preference transitivity (a
joint property that considered to be extremely natural in the literature on preference structures (Hansson,
2001).)

420

TCP-Nets

The tradeoffs exploited by Freuder and O’Sullivan (2001) correspond to the information
captured in TCP-nets by the i-arcs. However, instead of treating this information as an
incremental “compromising” update to the set of hard constraints as done by Freuder and
O’Sullivan (2001), in the TCP-net based constrained optimization presented in Section 6,
we exploit this information to guide the constraint solver to the preferable feasible solutions.
On the other hand, the motivation and ideas behind the work of Freuder and O’Sullivan as
well as these in some other works on interactive search (see works, e.g., on interactive goal
programming (Dyer, 1972), and interactive optimization based on example critique (Pu &
Faltings, 2004)) open a venue for future research on interactive preference-based constrained
optimization with TCP-nets, where elicitation of the user preferences is to be interleaved
with the search for feasible solution.
The notion of lexicographic orders/preferences (Fishburn, 1974; Schiex et al., 1995;
Freuder et al., 2003) is closely related to our notion of importance. The idea of a lexicographic ordering is often used in qualitative approaches for multi-criteria decision making.
Basically, it implies that if one item does better than another on the most important (lexicographically earlier) criteria on which they differ, it is considered better overall, regardless
on how poorly it may do on all other criteria. Thus, if we have four criteria (or attributes)
A, B, C, D, thus ordered, and o does well on A but miserably on B, C and D, whereas o0 is
slightly worse on A but much better on all other criteria, o is still deemed better. In terms
of our notion of variable importance, a lexicographic ordering of attributes denotes a special
form of relative importance of an attribute versus a set of attributes. Thus, in the example
above, A is more important than B, C and D combined; B is more important than C and
D combined, and C is more important than D. We note that Wilson (2004b) provides a
nice language that can capture such statements and more. Wilson allows statements of the
form “A = a is preferred to A = a all-else-being-equal, except for B and C.” That is, given
two outcomes that differ on A, B and C only, the one that assigns a to A is preferred to
the one that assigns a0 , regardless of the value of B and C in these outcomes. Hence, this
richer language can in particular capture lexicographic preferences.
While we believe a lexicographic ordering over attributes is typically too strong, we
think the flexibility provided by Wilson’s language could be quite useful. However, once
one starts analyzing relationships between sets of attributes, the utility of graphical models
and their analysis power becomes questionable. Indeed, we are not aware of a graphical
analysis of Wilson’s approach, except for the special case covered by TCP-nets. Moreover,
our intuition is that relative importance of sets will be a notion that users are much less
comfortable specifying in many applications. However, this hypothesis requires empirical
verification, as well as a more general study of the exact expressive power of TCP-nets, i.e.,
characterizing partial orders that are expressible using this language. We believe that this
is an important avenue for future research.
Acknowledgments
Ronen Brafman and Solomon Shimony were partly supported by the Paul Ivanier Center
for Robotics and Production Management. Ronen Brafman was partly supported by NSF
grants SES-0527650 and IIS-0534662. Ronen Brafman’s permanent address is: Department
of Computer Science, Ben Gurion University, Israel.
421

Brafman, Domshlak, & Shimony

References
Asher, N., & Morreau, M. (1995). What some generic sentences mean. In Carlson, G., &
Pelletier, F. J. (Eds.), The Generic Book, pp. 300–338. Chicago University Press.
Bethard, S., Yu, H., Thornton, A., Hatzivassiloglou, V., & Jurafsky, D. (2004). Automatic
extraction of opinion propositions and their holders. In Proceedings of the AAAI
Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications.
Bistarelli, S., Fargier, H., Montanari, U., Rossi, F., Schiex, T., & Verfaillie, G. (1999).
Semiring-based CSPs and valued CSPs: Frameworks, properties, and comparison.
Constraints, 4 (3), 275–316.
Bistarelli, S., Montanari, U., & Rossi, F. (1997). Semiring-based constraint solving and
optimization. Journal of the ACM, 44 (2), 201–236.
Blythe, J. (2002). Visual exploration and incremental utility elicitation. In Proceedings of
the National Conference on Artificial Intelligence (AAAI), pp. 526–532.
Boutilier, C., Bacchus, F., & Brafman, R. I. (2001). UCP-networks: A directed graphical
representation of conditional utilities. In Proceedings of Seventeenth Conference on
Uncertainty in Artificial Intelligence, pp. 56–64.
Boutilier, C., Brafman, R., Domshlak, C., Hoos, H., & Poole, D. (2004a). CP-nets: A tool
for representing and reasoning about conditional ceteris paribus preference statements.
Journal of Artificial Intelligence Research (JAIR), 21, 135–191.
Boutilier, C., Brafman, R., Domshlak, C., Hoos, H., & Poole, D. (2004b). Preference-based
constrained optimization with CP-nets. Computational Intelligence (Special Issue on
Preferences in AI and CP), 20 (2), 137–157.
Boutilier, C., Brafman, R., Hoos, H., & Poole, D. (1999). Reasoning with conditional ceteris
paribus preference statements. In Proceedings of the Fifteenth Annual Conference on
Uncertainty in Artificial Intelligence, pp. 71–80. Morgan Kaufmann Publishers.
Brafman, R., & Chernyavsky, Y. (2005). Planning with goal preferences and constraints. In
Proceedings of the International Conference on Automated Planning and Scheduling,
pp. 182–191, Monterey, CA.
Brafman, R., & Domshlak, C. (2002). Introducing variable importance tradeoffs into CPnets. In Proceedings of the Eighteenth Annual Conference on Uncertainty in Artificial
Intelligence, pp. 69–76, Edmonton, Canada.
Brafman, R., Domshlak, C., & Kogan, T. (2004a). Compact value-function representations
for qualitative preferences. In Proceedings of the Twentieth Annual Conference on
Uncertainty in Artificial Intelligence, pp. 51–58, Banff, Canada.
Brafman, R., Domshlak, C., & Shimony, S. E. (2004b). Qualitative decision making in
adaptive presentation of structured information. ACM Transactions on Information
Systems, 22 (4), 503–539.
Brafman, R. I., & Dimopoulos, Y. (2004). Extended semantics and optimization algorithms
for cp-networks. Computational Intelligence (Special Issue on Preferences in AI and
CP), 20 (2), 218–245.
422

TCP-Nets

Brafman, R. I., & Friedman, D. (2005). Adaptive rich media presentations via preferencebased constrained optimization. In Proceedings of the IJCAI-05 Workshop on Advances in Preference Handling, pp. 19–24, Edinburgh, Scotland.
Brewka, G. (2002). Logic programming with ordered disjunction. In Proceedings of
Eighteenth National Conference on Artificial Intelligence, pp. 100–105, Edmonton,
Canada. AAAI Press.
Burke, R. (2000). Knowledge-based recommender systems. In Kent, A. (Ed.), Encyclopedia
of Library and Information Systems, Vol. 69, pp. 180–200. Marcel Dekker, New York.
Domshlak, C. (2002). Modeling and Reasoning about Preferences with CP-nets. Ph.D.
thesis, Ben-Gurion University, Israel.
Domshlak, C., & Brafman, R. (2002). CP-nets - reasoning and consistency testing. In
Proceedings of the Eighth International Conference on Principles of Knowledge Representation and Reasoning, pp. 121–132, Toulouse, France.
Domshlak, C., Brafman, R., & Shimony, S. E. (2001). Preference-based configuration of
web page content. In Proceedings of the Seventeenth International Joint Conference
on Artificial Intelligence, pp. 1451–1456, Seattle.
Domshlak, C., Rossi, F., Venable, K. B., & Walsh, T. (2003). Reasoning about soft
constraints and conditional preferences: Complexity results and approximation techniques. In Proceedings of the Eighteenth International Joint Conference on Artificial
Intelligence, pp. 215–220, Acapulco, Mexico.
Dyer, J. S. (1972). Interactive goal programming. Management Science, 19, 62–70.
Even, S., Itai, A., & Shamir, A. (1976). On the complexity of timetable and multicommodity
flow problems. SIAM Journal on Computing, 5, 691–703.
Faltings, B., Pu, P., Torrens, M., & Viappiani, P. (2004). Designing example-critiquing interaction. In Proceedings of the International Conference on Intelligent User Interfaces,
pp. 22–29, Funchal, Madeira, Portugal.
Fargier, H., & Lang, J. (1993). Uncertainty in constraint satisfaction problems: A probabilistic approach. In Proceedings of the European Conference on Symbolic and Qualitative
Approaches to Reasoning and Uncertainty, Vol. 747 of LNCS, pp. 97–104.
Fargier, H., Lang, J., & Schiex, T. (1993). Selecting preferred solutions in fuzzy constraint
satisfaction problems. In Proceedings of the First European Congress on Fuzzy and
Intelligent Technologies, pp. 1128–1134.
Fishburn, P. (1974). Lexicographic orders, utilities, and decision rules: A survey. Management Science, 20 (11), 1442–1471.
French, S. (1986). Decision Theory. Halsted Press, New York.
Freuder, E., & O’Sullivan, B. (2001). Generating tradeoffs for interactive constraint-based
configuration. In Proceedings of the 7th International Conference on Principles and
Practice of Constraint Programming, pp. 590–594, Paphos, Cyprus.
Freuder, E. C., & Wallace, R. J. (1992). Partial constraint satisfaction. Artificial Intelligence, 58, 21–70.
423

Brafman, Domshlak, & Shimony

Freuder, E. C., Wallace, R. J., & Heffernan, R. (2003). Ordinal constraint satisfaction. In
Proceedings of the Fifth International Workshop on Soft Constraints.
Glass, J. (1999). Challenges for spoken dialogue systems. In Proceedings of the IEEE ASRU
Workshop, Keystone, CO.
Goldsmith, J., Lang, J., Truszczynski, M., & Wilson, N. (2005). The computational complexity of dominance and consistency in CP-nets. In Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence, pp. 144–149, Edinburgh, Scotland.
Haag, A. (1998). Sales configuration in business processes. IEEE Intelligent Systems and
their Applications, 13 (4), 78–85.
Hansson, S. O. (2001). Preference logic. In Gabbay, D. M., & Guenthner, F. (Eds.),
Handbook of Philosophical Logic (2 edition)., Vol. 4, pp. 319–394. Kluwer.
Keeney, R. L., & Raiffa, H. (1976). Decision with Multiple Objectives: Preferences and Value
Tradeoffs. Wiley.
Lang, J. (2002). From preference representation to combinatorial vote. In Proceedings of
the Eight International Conference on Principles of Knowledge Representation and
Reasoning (KR), pp. 277–288.
Linden, G., Hanks, S., & Lesh, N. (1997). Interactive assessment of user preference models:
The automated travel assistant. In Proceedings of the Sixth International Conference
on User Modeling, pp. 67–78.
Pu, P., & Faltings, B. (2004). Decision tradeoff using example critiquing and constraint
programming. Constraints: An International Journal, 9 (4), 289–310.
Resnick, P., & Varian, H. R. (Eds.). (1997). Special Issue on Recommender Systems, Vol. 40
of Communications of the ACM.
Rossi, F., Venable, K. B., & Walsh, T. (2004). mCP nets: Representing and reasoning with
preferences of multiple agents. In Proceedings of the Nineteenth National Conference
on Artificial Intelligence, pp. 729–734, San Jose, CL.
Sabin, D., & Weigel, R. (1998). Product conguration frameworks - A survey. IEEE Intelligent Systems and their Applications, 13 (4), 42–49.
Schiex, T. (1992). Possibilistic cosntraint satisfaction, or ”How to handle soft constraints”.
In Proceedings of Eighth Conference on Uncertainty in Artificial Intelligence, pp. 269–
275.
Schiex, T., Fargier, H., & Verfaillie, G. (1995). Valued constraint satisfaction problems: Hard
and easy problems. In Proceedings of the Fourteenth International Joint Conference
on Artificial Intelligence, pp. 631–637.
Wilson, N. (2004a). Consistency and constrained optimisation for conditional preferences.
In Proceedings of the Sixteenth European Conference on Artificial Intelligence, pp.
888–894, Valencia.
Wilson, N. (2004b). Extending CP-nets with stronger conditional preference statements.
In Proceedings of the Nineteenth National Conference on Artificial Intelligence, pp.
735–741, San Jose, CL.
424

Journal of Artificial Intelligence Research 25 (2006) 529-576

Submitted 4/05; published 4/06

Asynchronous Partial Overlay: A New Algorithm for Solving
Distributed Constraint Satisfaction Problems
Roger Mailler

mailler@ai.sri.com

SRI International
333 Ravenswood Dr
Menlo Park, CA 94025 USA

Victor R. Lesser

lesser@cs.umass.edu

University of Massachusetts, Department of Computer Science
140 Governors Drive
Amherst, MA 01003 USA

Abstract
Distributed Constraint Satisfaction (DCSP) has long been considered an important
problem in multi-agent systems research. This is because many real-world problems can
be represented as constraint satisfaction and these problems often present themselves in a
distributed form. In this article, we present a new complete, distributed algorithm called
asynchronous partial overlay (APO) for solving DCSPs that is based on a cooperative mediation process. The primary ideas behind this algorithm are that agents, when acting as a
mediator, centralize small, relevant portions of the DCSP, that these centralized subproblems overlap, and that agents increase the size of their subproblems along critical paths
within the DCSP as the problem solving unfolds. We present empirical evidence that shows
that APO outperforms other known, complete DCSP techniques.

1. Introduction
The Distributed constraint satisfaction problem has become a very useful representation
that is used to describe a number of problems in multi-agent systems including distributed
resource allocation (Conry, Kuwabara, Lesser, & Meyer, 1991) and distributed scheduling
(Sycara, Roth, Sadeh, & Fox, 1991). Some researchers in cooperative multi-agent systems
have focused on developing methods for solving these problems that are based on one key assumption. Particularly, the agents involved in the problem solving process are autonomous.
This means that the agents are only willing to exchange information that is directly relevant
to the shared problem and retain the ability to refuse a solution when it obviously conflicts
with some internal goal.
These researchers believe that the focus on agent autonomy precludes the use of centralization because it forces the agents to reveal all of their internal constraints and goals which
may, for reasons of privacy or pure computational complexity, be impossible to achieve.
Several algorithms have been developed with the explicit purpose of allowing the agents
to retain their autonomy even when they are involved in a shared problem which exhibits
interdependencies. Probably the best known algorithms that fit this description can be
found in the work of Yokoo et al. in the form of distributed breakout (DBA) (Yokoo &
Hirayama, 1996), asynchronous backtracking (ABT) (Yokoo, Durfee, Ishida, & Kuwabara,
1992), and asynchronous weak-commitment (AWC) (Yokoo & Hirayama, 2000).
c
°2006
AI Access Foundation. All rights reserved.

Mailler & Lesser

Unfortunately, a common drawback to each of these algorithms is that in an effort to
provide the agents which complete privacy, these algorithms prevent the agents from
making informed decisions about the global effects of changing their local allocation, schedule, value, etc. For example, in AWC, agents have to try a value and wait for another agent
to tell them that it will not work through a nogood message. Because of this, agents never
learn true reason why another agent or set of agents is unable to accept the value, they only
learn that their value in combination with other values doesn’t work.
In addition, these techniques suffer because they have complete distribution of the control. In other words, each agent makes decisions based on its incomplete and often inaccurate
view of the world. The result is that this leads to unnecessary thrashing in the problem
solving because the agents are trying to adapt to the behavior of the other agents, who in
turn are trying to adapt to them. Pathologically, this behavior can be counter-productive to
convergence of the protocol(Fernandez, Bejar, Krishnamachari, Gomes, & Selman, 2003).
This iterative trial and error approach to discovering the implicit and implied constraints
within the problem causes the agents to pass an exponential number of messages and actually reveals a great deal of information about the agents’ constraints and domain values
(Yokoo, Suzuki, & Hirayama, 2002). In fact, in order to be complete, agents using AWC
have to be willing to reveal all of their shared constraints and domain values. The key thing
to note about this statement is that AWC still allows the agents to retain their autonomy
even if they are forced to reveal information about the variables and constraints that form
the global constraint network.
In this paper, we present a cooperative mediation based DCSP protocol, called Asynchronous Partial Overlay (APO). Cooperative mediation represents a new methodology
that lies somewhere between centralized and distributed problem solving because it uses
dynamically constructed, partial centralization. This allows cooperative mediation based
algorithms, like APO, to utilize the speed of current state-of-the-art centralized solvers
while taking advantage of opportunities for parallelism by dynamically identifying relevant
problem structure.
APO works by having agents asynchronously take the role of mediator. When an agent
acts as a mediator, it computes a solution to a portion of the overall problem and recommends value changes to the agents involved in the mediation session. If, as a result of its
recommendations, it causes conflicts for agents outside of the session, it links with them
preventing itself from repeating the mistake in future sessions.
Like AWC, APO provides the agents with a great deal of autonomy by allowing anyone
of them to take over as the mediator when they notice an undesirable state in the current
solution to the shared problem. Further adding to their autonomy, agents can also ignore
recommendations for changing their local solution made by other agents. In a similar
way to AWC, APO is both sound and complete when the agents are willing to reveal the
domains and constraints of their shared variables and allows the agents to obscure the
states, domains, and constraints of their strictly local variables.
In the rest of this article, we present a formalization of the DCSP problem (section
2). In section 3, we describe the underlying assumptions and motivation for this work.
We then present the APO algorithm (section 4.1) and give an example of the protocol’s
execution on a simple 3-coloring problem (section 4.2). We go on to give the proofs for
the soundness and completeness of the algorithm (section 4.3). In section 5, we present
530

Asynchronous Partial Overlay: A New Algorithm for DCSP

the results of extensive testing that compares APO with AWC within the distributed graph
coloring domain and the complete compatibility version of the SensorDCSP domain (Bejar,
Krishnamachari, Gomes, & Selman, 2001) across a variety of metrics including number of
cycles, messages, bytes transmitted, and serial runtime. In each of these cases, we will show
that APO significantly outperforms AWC (Yokoo, 1995; Hirayama & Yokoo, 2000). Section
6 summarizes the article and discusses some future research directions.

2. Distributed Constraint Satisfaction
A Constraint Satisfaction Problem (CSP) consists of the following:

• a set of n variables V = {x1 , . . . , xn }.

• discrete, finite domains for each of the variables D = {D1 , . . . , Dn }.

• a set of constraints R = {R1 , . . . , Rm } where each Ri (di1 , . . . , dij ) is a predicate on
the Cartesian product Di1 × · · · × Dij that returns true iff the value assignments of
the variables satisfies the constraint.

The problem is to find an assignment A = {d1 , . . . , dn |di ∈ Di } such that each of the
constraints in R is satisfied. CSP has been shown to be NP-complete, making some form
of search a necessity.
In the distributed case, DCSP, using variable-based decomposition, each agent is assigned one or more variables along with the constraints on their variables. The goal of each
agent, from a local perspective, is to ensure that each of the constraints on its variables
is satisfied. Clearly, each agent’s goal is not independent of the goals of the other agents
in the system. In fact, in all but the simplest cases, the goals of the agents are strongly
interrelated. For example, in order for one agent to satisfy its local constraints, another
agent, potentially not directly related through a constraint, may have to change the value
of its variable.
In this article, for the sake of clarity, we restrict ourselves to the case where each agent
is assigned a single variable and is given knowledge of the constraints on that variable.
Since each agent is assigned a single variable, we will refer to the agent by the name of
the variable it manages. Also, we restrict ourselves to considering only binary constraints
which are of the form Ri (di1 , di2 ). Since APO uses centralization as its core, it is easy to
see that the algorithm would work if both of these restrictions are removed. This point will
be discussed as part of the algorithm description in section 4.1.4.
Throughout this article, we use the term constraint graph to refer to the graph formed by
representing the variables as nodes and the constraints as edges. Also, a variable’s neighbors
are the variables with which it shares constraints.
531

Mailler & Lesser

3. Assumptions and Motivation
3.1 Assumptions
The following are the assumptions made about the environments and agents for which this
protocol was designed:
1. Agents are situated, autonomous, computing entities. As such, they are capable of
sensing their environment, making local decisions based on some model of intentionality, and acting out their decisions. Agents are rationally and resource bounded.
As a result, agents must communicate to gain information about each others state,
intentions, decisions, etc.
2. Agents within the multi-agent system share one or more joint goals. In this paper,
this goal is Boolean in nature stemming from the DCSP formulation.
3. Because this work focuses on cooperative problem solving, the agents are cooperative.
This does not necessarily imply they will share all of their state, intentions, etc. with
other agents, but are, to some degree, willing to exchange information to solve joint
goals. It also does not imply that they will change their intentions, state, or decisions
based on the demands of another agent. Agents still maintain their autonomy and
the ability to refuse or revise the decisions of other agents based on their local state,
intentions, decisions, etc.
4. Each agent has the capability of computing solutions to the joint goal based on their
potentially limited rationality. This follows naturally from the ability of agents to
make their own decisions, i.e., every agent is capable of computing a solution to its
own portion of the joint goal based on its own desires.
3.2 Motivation for Mediation-Based Problem Solving
The Websters dictionary defines the act of mediating as follows:
Mediate: 1. to act as an intermediary; especially to work with opposing sides
in order to resolve (as a dispute) or bring about (as a settlement). 2. to bring
about, influence, or transmit by acting as an intermediate or controlling agent
or mechanism. (Merriam-Webster, 1995)
By its very definition, mediation implies some degree of centralizing a shared problem in
order for a group of individuals to derive a conflict free solution. Clearly in situations where
the participants are willing (cooperative), mediation is a powerful paradigm for solving
disputes. It’s rather strange, considering this, that very little has been done on looking at
mediation as a cooperative method for solving DCSPs.
Probably, the earliest mediation-based approach for solving conflicts amongst agents in
an airspace management application(Cammarata, McArthur, & Steeb, 1983). This work
investigates using various conflict resolution strategies to deconflict airspace in a distributed
air traffic control system. The author proposes a method for solving disputes where the
involved agents elect a leader to solve the problem. Once elected, the leader becomes
532

Asynchronous Partial Overlay: A New Algorithm for DCSP

S1

S2

{1,2}

{1,2}

Figure 1: A simple distributed problem with two variables.
responsible for recognizing the dispute, devising a plan to correct it, and acting out the
plan. Various election schemes are tested, but unfortunately, the leader only has authority
to modify its own actions in order to resolve the conflicts. This obviously leads to situations
where the plan is suboptimal.
In (Hayden, Carrick, & Yang, 1999), the authors describe mediation as one of a number possible coordination mechanisms. In this work, the mediator acts as a intermediary
between agents and can also act to coordinate their behavior. As an intermediary, the mediator routes messages, provides directory services, etc. This provides for a loose coupling
of the agents, since they only need to know the mediator. The mediator can also act to
coordinate the agents behavior if they have tight interdependencies.
Most of the research work on mediation-based problem solving involved settling disputes
between competitive or semi-competitive agents. Probably one of the best examples of
using mediation in this manner can be found in the PERSUADER system(Sycara, 1988).
PERSUADER was designed to settle conflicts between adversarial parties that are involved
in a labor dispute. PERSUADER uses case-based reasoning to suggest concessions in order
to converge on a satisfactory solution. Another example of using mediation in this way
can be found in in a system called Designer Fabricator Interpreter (DFI) (Werkman, 1990).
In DFI, mediation is used to resolve conflicts as one of a series of problem solving steps.
Whenever the first step fails, in this case iterative negotiation, a mediator agent steps in and
tries to convince the agents to relax their constraints. If this fails, the mediator mandates
a final solution.
There may be several reasons why mediation has not been more deeply explored as
a cooperative problem solving method. First, researchers have focused strongly on using
distributed computing as a way of exploiting concurrency to distribute the computation
needed to solve hard problems (Rao & Kumar, 1993). Because of this, even partially
and/or temporarily centralizing sections of the problem can be viewed as contradictory
to the central goal. Second, researchers have often claimed that part of the power of the
distributed methods lies in the ability of the techniques to solve problems that are naturally
distributed. For example, supply chain problems generally have no central monitoring
authority. Again, directly sharing the reasons why a particular choice is made in the form of
a constraint can seem to contradict the use of distributed methods. Lastly, researchers often
claim that for reasons of privacy or security the problem should be solved in a distributed
fashion. Clearly, sharing information to solve a problem compromises an agents ability to
be private and/or violates its security in some manner.
Although, parallelism, natural distribution, security, and privacy, may all seem like good
justifications for entirely distributed problem solving, in actuality, whenever a problem has
interdependencies between distributed problem solvers, some degree of centralization and
information sharing must take place in order to derive a conflict-free solution.
533

Mailler & Lesser

Consider, as a simple example, the problem in figure 1. In this figure, two problem
solvers, each with one variable, share the common goal of having a different value from one
another. Each of the agents has only two allowable values: {1, 2}. Now, in order to solve
this problem, each agent must individually decide that they have a different value from the
other agent. To do this, at the very least, one agent must transmit its value to the other. By
doing this, it removes half of its privacy (by revealing one of its possible values), eliminates
security (because the other agent could make him send other values by telling him that this
value is no good), and partially centralizes the problem solving (agent S2 has to compute
solutions based on the solution S1 presented and decide if the problem is solved and agent
S1 just relies on S2 to solve it.) In even this simple example, achieving totally distributed
problem solving is impossible.
In fact, if you look at the details of any of the current approaches to solving DCSPs,
you will observe a significant amount of centralization occurring. Most of these approaches
perform this centralization incrementally as the problem solving unfolds in an attempt to
restrict the amount of internal information being shared. Unfortunately, on problems that
have interdependencies among the problem solvers, revealing an agent’s information (such
as the potential values of their variables) is unavoidable. In fact, solutions that are derived
when one or more agents conceals all of the information regarding a shared constraint or
variable are based on incomplete information and therefore may not always be sound.
It follows then, that since you cannot avoid some amount of centralization, mediation
is a natural method for solving problems that contain interdependencies among distributed
problem solvers.

4. Asynchronous Partial Overlay
As a cooperative mediation based protocol, the key ideas behind the creation of the APO
algorithm are
• Using mediation, agents can solve subproblems of the DCSP using internal search.
• These local subproblems can and should overlap to allow for more rapid convergence
of the problem solving.
• Agents should, over time, increase the size of the subproblem they work on along
critical paths within the CSP. This increases the overlap with other agents and ensures
the completeness of the search.
4.1 The Algorithm
Figures 2, 3, 4, 5, and 6 present the basic APO algorithm. The algorithm works by constructing a good list and maintaining a structure called the agent view. The agent view
holds the names, values, domains, and constraints of variables to which an agent is linked.
The good list holds the names of the variables that are known to be connected to the owner
by a path in the constraint graph.
As the problem solving unfolds, each agent tries to solve the subproblem it has centralized within its good list or determine that it is unsolvable which indicates the entire global
problem is over-constrained. To do this, agents take the role of the mediator and attempt
534

Asynchronous Partial Overlay: A New Algorithm for DCSP

procedure initialize
di ← random d ∈ Di ;
pi ← sizeof (neighbors) + 1;
mi ← true;
mediate ← false;
add xi to the good list;
send (init, (xi , pi , di , mi , Di , Ci )) to neighbors;
initList ← neighbors;
end initialize;
when received (init, (xj , pj , dj , mj , Dj , Cj )) do
Add (xj , pj , dj , mj , Dj , Cj ) to agent view;
if xj is a neighbor of some xk ∈ good list do
add xj to the good list;
/ good list
add all xl ∈ agent view ∧ xl ∈
that can now be connected to the good list;
pi ← sizeof (good list);
end if;
if xj ∈
/ initList do
send (init, (xi , pi , di , mi , Di , Ci )) to xj ;
else
remove xj from initList;
check agent view;
end do;
Figure 2: The APO procedures for initialization and linking.

to change the values of the variables within the mediation session to achieve a satisfied
subsystem. When this cannot be achieved without causing a violation for agents outside of
the session, the mediator links with those agents assuming that they are somehow related to
the mediator’s variable. This process continues until one of the agents finds an unsatisfiable
subsystem, or all of the conflicts have been removed.
In order to facilitate the problem solving process, each agent has a dynamic priority
that is based on the size of their good list (if two agents have the same sized good list then
the tie is broken using the lexicographical ordering of their names). Priorities are used by
the agents to decide who mediates a session when a conflicts arises. Priority ordering is
important for two reasons. First, priorities ensure that the agent with the most knowledge
gets to make the decisions. This improves the efficiency of the algorithm by decreasing
the effects of myopic decision making. Second, priorities improve the effectiveness of the
mediation process because lower priority agents expect higher priority agents to mediate.
This improves the likelihood that lower priority agents will be available when a mediation
request is sent.
535

Mailler & Lesser

when received (ok?, (xj , pj , dj , mj )) do
update agent view with (xj , pj , dj , mj );
check agent view;
end do;
procedure check agent view
if initList 6= ∅ or mediate 6=false do
return;
m0i ← hasConf lict(xi );
if m0i and ¬∃j (pj > pi ∧ mj = = true)
if ∃(d0i ∈ Di ) (d0i ∪ agent view does not conflict)
and di conflicts exclusively with lower priority neighbors
di ← d0i ;
send (ok?, (xi , pi , di , mi )) to all xj ∈ agent view;
else
do mediate;
else if mi 6= m0i
mi ← m0i ;
send (ok?, (xi , pi , di , mi )) to all xj ∈ agent view;
end if;
end check agent view;

Figure 3: The procedures for doing local resolution, updating the agent view and the
good list.

4.1.1 Initialization (Figure 2)
On startup, the agents are provided with the value (they pick it randomly if one isn’t
assigned) and the constraints on their variable. Initialization proceeds by having each of
the agents send out an “init” message to its neighbors. This initialization message includes
the variable’s name (xi ), priority (pi ), current value(di ), the agent’s desire to mediate (mi ),
domain (Di ), and constraints (Ci ). The array initList records the names of the agents that
initialization messages have been sent to, the reason for which will become immediately
apparent.
When an agent receives an initialization message (either during the initialization or
through a later link request), it records the information in its agent view and adds the
variable to the good list if it can. A variable is only added to the good list if it is a
neighbor of another variable already in the good list. This ensures that the graph created
by the variables in the good list always remains connected, which focuses the agent’s internal
problem solving on variables which it knows it has an interdependency with. The initList
is then checked to see if this message is a link request or a response to a link request. If an
agent is in the initList, it means that this message is a response, so the agent removes the
536

Asynchronous Partial Overlay: A New Algorithm for DCSP

procedure mediate
pref erences ← ∅;
counter ← 0;
for each xj ∈ good list do
send (evaluate?, (xi , pi )) to xj ;
counter ++;
end do;
mediate ← true;
end mediate;
when receive (wait!, (xj , pj )) do
update agent view with (xj , pj );
counter - -;
if counter == 0 do choose solution;
end do;
when receive (evaluate!, (xj , pj , labeled Dj )) do
record (xj , labeled Dj ) in preferences;
update agent view with (xj , pj );
counter - -;
if counter == 0 do choose solution;
end do;
Figure 4: The procedures for mediating an APO session.
name from the initList and does nothing further. If the agent is not in the initList then it
means this is a request, so a response “init” is generated and sent.
It is important to note that the agents contained in the good list are a subset of the
agents contained in the agent view. This is done to maintain the integrity of the good list
and allow links to be bidirectional. To understand this point, consider the case when a
single agent has repeatedly mediated and has extended its local subproblem down a long
path in the constraint graph. As it does so, it links with agents that may have a very limited
view and therefore are unaware of their indirect connection to the mediator. In order for
the link to be bidirectional, the receiver of the link request has to store the name of the
requester in its agent view, but cannot add them to their good list until a path can be
identified. As can be seen in section 4.3, the bi-directionality of links is important to ensure
the protocol’s soundness.
4.1.2 Checking the agent view (Figure 3)
After all of the initialization messages are received, the agents execute the check agent view
procedure (at the end of figure 2). In this procedure, the current agent view (which contains
the assigned, known variable values) is checked to identify conflicts between the variable
owned by the agent and its neighbors. If, during this check (called hasConflict in the
537

Mailler & Lesser

procedure choose solution
select a solution s using a Branch and Bound search that:
1. satisfies the constraints between agents in the good list
2. minimizes the violations for agents outside of the session
if ¬∃s that satisfies the constraints do
broadcast no solution;
for each xj ∈ agent view do
if xj ∈ pref erences do
if d0j ∈ s violates an xk and xk ∈
/ agent view do
send (init, (xi , pi , di , mi , Di , Ci )) to xk ;
add xk to initList;
end if;
send (accept!, (d0j , xi , pi , di , mi )) to xj ;
update agent view for xj
else
send (ok?, (xi , pi , di , mi )) to xj ;
end if;
end do;
mediate ← false;
check agent view;
end choose solution;
Figure 5: The procedure for choosing a solution during an APO mediation.
figure), an agent finds a conflict with one or more of its neighbors and has not been told by
a higher priority agent that they want to mediate, it assumes the role of the mediator.
An agent can tell when a higher priority agent wants to mediate because of the m i flag
mentioned in the previous section. Whenever an agent checks its agent view it recomputes
the value of this flag based on whether or not it has existing conflicts with its neighbors.
When this flag is set to true it indicates that the agent wishes to mediate if it is given
the opportunity. This mechanism acts like a two-phase commit protocol, commonly seen in
database systems, and ensures that the protocol is live-lock and dead-lock free.
When an agent becomes the mediator, it first attempts to rectify the conflict(s) with
its neighbors by changing its own variable. This simple, but effective technique prevents
mediation sessions from occurring unnecessarily, which stabilizes the system and saves messages and time. If the mediator finds a value that removes the conflict, it makes the change
and sends out an “ok?” message to the agents in its agent view. If it cannot find a nonconflicting value, it starts a mediation session. An “ok?” message is similar to an “init”
message, in that it contains information about the priority, current value, etc. of a variable.
4.1.3 Mediation (Figures 4, 5, and 6)
The most complex and certainly most interesting part of the protocol is the mediation. As
was previously mentioned in this section, an agent decides to mediate if it is in conflict
538

Asynchronous Partial Overlay: A New Algorithm for DCSP

when received (evaluate?, (xj , pj )) do
mj ← true;
if mediate == true or ∃k (pk > pj ∧ mk = = true) do
send (wait!, (xi , pi ));
else
mediate ← true;
label each d ∈ Di with the names of the agents
that would be violated by setting di ← d;
send (evaluate!, (xi , pi , labeled Di ));
end if;
end do;
when received (accept!, (d, xj , pj , dj , mj )) do
di ← d;
mediate ← false;
send (ok?, (xi , pi , di , mi )) to all xj in agent view;
update agent view with (xj , pj , dj , mj );
check agent view;
end do;
Figure 6: Procedures for receiving an APO session.

with one of its neighbors and is not expecting a session request from a higher priority
agent. The mediation starts with the mediator sending out “evaluate?” messages to each
of the agents in its good list. The purpose of this message is two-fold. First, it informs
the receiving agent that a mediation is about to begin and tries to obtain a lock from that
agent. This lock, referred to as mediate in the figures, prevents the agent from engaging
in two sessions simultaneously or from doing a local value change during the course of a
session. The second purpose of the message is to obtain information from the agent about
the effects of making them change their local value. This is a key point. By obtaining this
information, the mediator gains information about variables and constraints outside of its
local view without having to directly and immediately link with those agents. This allows
the mediator to understand the greater impact of its decision and is also used to determine
how to extend its view once it makes its final decision.
When an agent receives a mediation request, it responds with either a “wait!” or
“evaluate!” message. The “wait” message indicates to the requester that the agent is
currently involved in a session or is expecting a request from an agent of higher priority
than the requester, which in fact could be itself. If the agent is available, it labels each of
its domain elements with the names of the agents that it would be in conflict with if it were
asked to take that value. This information is returned in the “evaluate!” message.
The size of the “evaluate!” message is strongly related to the number of variables and the
size of the agent’s domain. In cases where either of these are extremely large, a number of
techniques can be used to reduce the overall size of this message. Some example techniques
539

Mailler & Lesser

include standard message compression, limiting the domain elements that are returned to be
only ones that actually create conflict or simply sending relevant value/variable pairs so the
mediator can actually do the labeling. This fact means that the largest “evaluate!” message
ever actually needed is polynomial in the number of agents (O(V )). In this implementation,
for graph coloring, the largest possible “evaluate!” message is O(|D i | + |V |).
It should be noted that the agents do not need to return all of the names when they
have privacy or security reasons. This effects the completeness of the algorithm, because
the completeness relies on one or more of the agents eventually centralizing the entire
problem in the worst case. As was mentioned in section 3.2, whenever an agent attempts to
completely hide information about a shared variable or constraint in a distributed problem,
the completeness is necessarily effected.
When the mediator has received either a “wait!” or “evaluate!” message from the agents
that it sent a request to, it chooses a solution. The mediator determines that it has received
all of the responses by using the counter variable which is set to the size of the good list
when the “evaluate?” messages are first sent. As the mediator receives either a “wait!” or
“evaluate!” message, it decrements this counter. When it reaches 0, all of the agents have
replied.
Agents that sent a ”wait!” message are dropped from the mediation while for agents
that sent an ”evaluate!” message their labeled domains specified in the message are recorded
and used in the search process. The mediator then uses the current values along with the
labeled domains it has received in the “evaluate!” messages to conduct a centralized search.
Currently, solutions are generated using a Branch and Bound search (Freuder & Wallace,
1992) where all of the constraints in the good list must be satisfied and the number of outside
conflicts are minimized. This is very similar to the the min-conflict heuristic (Minton,
Johnston, Philips, & Laird, 1992). Notice that although this search takes all of the variables
and constraints in its good list into consideration, the solution it generates may not adhere
to the variable values of the agents that were dropped from the session. These variables are
actually considered outside of the session and the impact of not being able to change their
values is calculated as part of the min-conflict heuristic. This causes the search to consider
the current values of dropped variables as weak-constraints on the final solution.
In addition, the domain for each of the variables in the good list is ordered such that
the variable’s current value is its first element. This causes the search to use the current
value assignments as the first path in the search tree and has the tendency to minimize the
changes being made to the current assignments. These heuristics, when combined together,
form a lock and key mechanism that simultaneously exploits the work that was previously
done by other mediators and acts to minimize the number of changes in those assignments.
As will be presented in section 5, these simple feed-forward mechanisms, combined with the
limited centralization needed to solve problems, account for considerable improvements in
the algorithms runtime performance.
If no satisfying assignments are found during this search, the mediator announces that
the problem is unsatisfiable and the algorithm terminates. Once the solution is chosen,
“accept!” messages are sent to the agents in the session, who, in turn, adopt the proposed
answer.
The mediator also sends “ok” messages to the agents that are in its agent view, but for
whatever reason were not in the session. This simply keeps those agents’ agent views up540

Asynchronous Partial Overlay: A New Algorithm for DCSP

to-date, which is important for determining if a solution has been reached. Lastly, using the
information provided to it in the “evaluate!” messages, the mediator sends “init” messages
to any agent that is outside of its agent view, but it caused conflict for by choosing a
solution. This “linking” step extends the mediators view along paths that are likely to be
critical to solving the problem or identifying an over-constrained condition. This step also
ensures the completeness of the protocol.
Although termination detection is not explicitly part of the APO protocol, a technique
similar to (Wellman & Walsh, 1999) could easily be added to detect quiescence amongst
the agents.
4.1.4 Multiple Variables and n-ary Constraints
Removing the restrictions presented in section 2 is a fairly straightforward process. Because
APO uses linking as part of the problem solving process, working with n-ary constraints
simply involves linking with the n agents within the constraint during initialization and
when a post-mediation linking needs to occur. Priorities in this scheme are identical to
those used for binary constraints.
Removing the single agent per variable restriction is also not very difficult and in fact is
one of the strengths of this approach. By using a spanning tree algorithm on initialization,
the agents can quickly identify the interdependencies between their internal variables which
they can then use to create separate good lists for each of the disconnected components
of their internal constraint graph. In essence, on startup, the agents would treat each
of these decomposed problems as a separate problem, using a separate m i flag, priority,
good list, etc. As the problem solving unfolds, and the agent discovers connections between
the internal variables (through external constraints), these decomposed problems could be
merged together and utilize a single structure for all of this information.
This technique has the advantages of being able to ensure consistency between dependent internal variables before attempting to mediate (because of the local checking before
mediation), but allows the agent to handle independent variables as separate problems.
Using a situation aware technique such as this one has been shown to yield the best results
in previous work(Mammen & Lesser, 1998). In addition, this technique allows the agents
to hide variables that are strictly internal. By doing pre-computation on the decomposed
problems, the agents can construct constraints which encapsulate each of the subproblems
as n-ary constraints where n is the number of variables that have external links. These derived constraints can then be sent as part of the “init” message whenever the agent receives
a link request for one of its external variables.
4.2 An Example
Consider the 3-coloring problem presented in figure 7. In this problem, there are 8 agents,
each with a variable and 12 edges or constraints between them. Because this is a 3-coloring
problem, each variable can only be assigned one of the three available colors {Black, Red,
or Blue}. The goal is to find an assignment of colors to the variables such that no two
variables, connected by an edge, have the same color.
In this example, four constraints are in violation: (ND0,ND1), (ND1,ND3), (ND2,ND4),
and (ND6,ND7). Following the algorithm, upon startup each agent adds itself to its
541

Mailler & Lesser

Figure 7: An example of a 3-coloring problem with 8 nodes and 12 edges.
good list and sends an “init” message to its neighbors. Upon receiving these messages,
the agents add each of their neighbors to their good list because they are able to identify a
shared constraint with themselves.
Once the startup has been completed, each of the agents checks its agent view. All of
the agents, except ND5, find that they have conflicts. ND0 (priority 3) waits for ND1 to
mediate (priority 5). ND6 and ND7, both priority 4, wait for ND4 (priority 5, tie with
ND3 broken using lexicographical ordering). ND1, having an equal number of agents in
its good list, but a lower lexicographical order, waits for ND4 to start a mediation. ND3,
knowing it is highest priority amongst its neighbors, first checks to see if it can resolve
its conflict by changing its value, which in this case, it cannot. ND3 starts a session that
involves ND1, ND5, ND6, and ND7. It sends each of them an “evaluate?” message. ND4
being highest priority amongst its neighbors, and unable to resolve its conflict locally, also
starts a session by sending “evaluate?” messages to ND1, ND2, ND6, and ND7.
When each of the agents in the mediation receives the “evaluate?” message, they first
check to see if they are expecting a mediation from a higher priority agent. In this case,
ND1, ND6, and ND7 are expecting from ND4 so tell ND3 to wait. Then they label their
domain elements with the names of the variables that they would be in conflict with as
a result of adopting that value. This information is sent in an “evaluate!” message. The
following are the labeled domains for the agents that are sent to ND4:
• ND1 - Black causes no conflicts; Red conflicts with ND0 and ND3; Blue conflicts with
ND2 and ND4
• ND2 - Black causes no conflicts; Red conflicts with ND0 and ND3; Blue conflicts with
ND4
• ND6 - Black conflicts with ND7; Red conflicts with ND3; Blue conflicts with ND4
• ND7 - Black conflicts with ND6; Red conflicts with ND3; Blue conflicts with ND4

542

Asynchronous Partial Overlay: A New Algorithm for DCSP

Figure 8: The state of the sample problem after ND3 leads the first mediation.
The following are the responses sent to ND3:
• ND1 - wait!
• ND5 - Black causes no conflicts; Red conflicts with ND3; Blue causes no conflicts
• ND6 - wait!
• ND7 - wait!
Once all of the responses are received, the mediators, ND3 and ND4, conduct a branch
and bound searches that attempt to find a satisfying assignment to their subproblems and
minimizes the amount of conflict that would be created outside of the mediation. If either
of them cannot find at least one satisfying assignment, it broadcasts that a solution cannot
be found.
In this example, ND3, with the limited information that it has, computes a satisfying
solution that changes its own color and to remain consistent would have also changed the
colors of ND6 and ND7. Since it was told by ND6 and ND7 to wait, it changes its color,
sends an “accept’ !” message to ND5 and “ok?” messages to ND1, ND6 and ND7. Having
more information, ND4 finds a solution that it thinks will solve its subproblem without
creating outside conflicts. It changes its own color to red, ND7 to blue, and ND1 to black
leaving the problem in the state shown in figure 8.
ND1, ND4, ND5, ND6 and ND7 inform the agents in their agent view of their new
values, then check for conflicts. This time, ND1, ND3, and ND6 notice that their values are
in conflict. ND3, having the highest priority, becomes the mediator and mediates a session
with ND1, ND5, ND6, and ND7. Following the protocol, ND3 sends out the “evaluate?”
messages and the receiving agents label and respond. The following are the labeled domains
that are returned:
• ND1 - Black conflicts with ND3; Red conflicts with ND0 and ND4; Blue conflicts with
ND2
543

Mailler & Lesser

Figure 9: The final solution after ND2 leads the second mediation.
• ND5 - Black conflicts with ND3; Red causes no conflicts; Blue causes no conflicts
• ND6 - Black conflicts with ND3; Red conflicts with ND4; Blue conflicts with ND7
• ND7 - Black conflicts with ND3 and ND6; Red conflicts with ND4; Blue causes no
conflicts
ND3, after receiving these messages, conducts its search and finds a solution that solves
its subproblem. It chooses to change its color to red. ND1, ND3, ND5, ND6, and ND7
check their agent view and find no conflicts. Since, at this point, none of the agents have
any conflict, the problem is solved (see figure 9).
4.3 Soundness and Completeness
In this section we will show that the APO algorithm is both sound and complete. For these
proofs, it is assumed that all communications are reliable, meaning that if a message is sent
from xi to xj that xj will receive the message in a finite amount of time. We also assume
that if xi sends message m1 and then sends message m2 to xj , that m1 will be received
before m2 . Lastly, we assume that the centralized solver used by the algorithm is sound and
complete. Before we prove the soundness and completeness, it helps to have a few principal
lemmas established.
Lemma 1 Links are bidirectional. i.e. If xi has xj in its agent view then eventually xj
will have xi in its agent view.
Proof:
Assume that xi has xj in its agent view and that xi is not in the agent view of xj . In
order for xi to have xj in its agent view, xi must have received an “init” message at some
point from xj . There are two cases.
Case 1: xj is in the initList of xi . In this case, xi must have sent xj an “init” message
first, meaning that xj received an “init” message and therefore has xi in its agent view, a
contradiction.
544

Asynchronous Partial Overlay: A New Algorithm for DCSP

Case 2: xj is not in the initList of xi . In this case, when xi receives the “init” message
from xj , it responds with an “init” message. That means that if the reliable communication
assumption holds, eventually xj will receive xi ’s “init” message and add xi to its agent view.
Also a contradiction.
Lemma 2 If agent xi is linked to xj and xj changes its value, then xi will eventually be
informed of this change and update its agent view.
Proof:
Assume that xi has a value in its agent view for xj that is incorrect. This would mean
that at some point xj altered its value without informing xi . There are two cases:
Case 1: xj did not know it needed to send xi an update. i.e. xi was not in xj ’s
agent view. Contradicts lemma 1.
Case 2: xj did not inform all of the agents in its agent view when it changes its value.
It is clear from the code that this cannot happen. Agents only change their values in the
check agent view, choose solution, and accept! procedures. In each of these cases, it informs
all of the agents within its agent view by either sending an “ok?” or through the “accept!”
message that a change to its value has occurred. A contradiction.
Lemma 3 If xi is in conflict with one or more of its neighbors, does not expect a mediation
from another higher priority agent in its agent view, and is currently not in a session, then
it will act as mediator.
Proof:
Directly from the procedure check agent view.
Lemma 4 If xi mediates a session that has a solution, then each of the constraints between
the agents involved in the mediation will be satisfied.
Proof:
Assume that there are two agents xj and xk (either of them could be xi ), that were
mediated over by xi and after the mediation there is a conflict between xj and xk . There
are two ways this could have happened.
Case 1: One or both of the agents must have a value that xi did not assign to it as
part of the mediation.
Assume that xj and/or xk has a value that xi did not assign. We know that since xi
mediated a session including xj and xk , that xi did not receive a “wait!” message from
either of xj or xk . This means that they could not have been mediating. This also means
that they must have set their mediate flags to true when xi sent them the “evaluate?”
message. Since the only times an agent can change its value is when its mediate flag is
false, it is mediating, or has been told to by a mediator, xj and/or xk could only have
changed their values is if xi told them to, which contradicts the assumption.
Case 2: xi assigned them a value that caused them to be in conflict with one another.
Let’s assume that xi assigned them conflicting values. This means that xi chose a
solution that did not take into account the constraints between xj and xk . But, we know
that xi only chooses satisfying solutions that include all of the constraints between all of
the agents in the good list. This leads to a contradiction.
545

Mailler & Lesser

This lemma is important because it says that once a mediator has successfully concluded
its session, the only conflicts that can exist are on constraints that were outside of the
mediation. This can be viewed as the mediator pushing the constraint violations outside
of its view. In addition, because mediators get information about who the violations are
being pushed to and establish links with those agents, over time, they gain more context.
This is a very important point when considering the completeness of the algorithm.
Theorem 1 The APO algorithm is sound. i.e. It reaches a stable state only if it has either
found an answer or no solution exists.
Proof:
In order to be sound, the agents can only stop when they have reached an answer. The
only condition in which they would stop without having found an answer is if one or more
of the agents is expecting a mediation request from a higher priority agent that does not
send it. In other words, the protocol has deadlocked.
Let’s say we have 3 agents, xi , xj , and xk with pi < pj ∨ pk < pj (i could be equal to k)
and xk has a conflict with xj . There are two cases in which xj would not mediate a session
that included xi , when xi was expecting it to:
Case 1: xi has mj = true in its agent view when the actual value should be false.
Assume that xi has mj = true in its agent view when the true value of mj = false.
This would mean that at some point xj changed the value of mj to false without informing
xi . There is only one place that xj changes the value of mj , this is in the check agent view
procedure (see figure 3). Note that in this procedure, whenever the flag changes value from
true to false, the agent sends an “ok” message to all the agents in its agent view. Since
by lemma 1 we know that xi is in the agent view of xj , xi must have received the message
saying that mj = false, contradicting the assumption.
Case 2: xj believes that xi should be mediating when xi does not believe it should be.
i.e. xj thinks that mi = true and pi > pj .
By the previous case, we know that if xj believes that mi = true that this must be the
case. We only need to show that pi < pj . Let’s say that p0i is the priority that xj believes
xi has and assume that xj believes that p0i > pj when, in fact pi < pj . This means that at
some point xi sent a message to xj informing it that its current priority was p0i . Since we
know that priorities only increase over time (the good list only gets larger), we know that
p0i ≤ pi (xj always has the correct value or underestimates the priority of xi ). Since pj > pi
and pi ≥ p0i then pj > p0i which contradicts the assumption.
This is also an important point when considering how the algorithm behaves. This proof
says that agents always either know or underestimate the true value of their neighbors’
priorities. Because of this, the agents will attempt to mediate when in fact sometimes,
they shouldn’t. The side effect of this attempt, however, is that the correct priorities are
exchanged so the same mistake doesn’t get repeated. The other important thing to mention
is the case were the priority values become equal. In this case, the tie is broken by using
the alphabetical order of names of the agents. This ensures that there is always a way to
break ties.
Definition 1 Oscillation is a condition that occurs when a subset V 0 ⊆ V of the agents are
infinitely cycling through their allowable values without reaching a solution. In other words,
the agents are live-locked
546

Asynchronous Partial Overlay: A New Algorithm for DCSP

By this definition, in order to be considered part of an oscillation, an agent within the
subset must be changing its value (if it’s stable, it’s not oscillating) and it must be connected
to the other members of the subset by a constraint (otherwise, it is not actually a part of
the oscillation).
Theorem 2 The APO algorithm is complete. i.e. if a solution exists, the algorithm will
find it. If a solution does not exist, it it will report that fact.
Proof:
A solution does not exist whenever the problem is over-constrained. If the problem
is over-constrained, the algorithm will eventually produce a good list where the variables
within it and their associated constraints lead to no solution. Since a subset of the variables
is unsatisfiable, the entire problem is unsatisfiable, therefore, no solution is possible. The
algorithm terminates with failure if and only if this condition is reached.
Since we have now shown in Theorem 1 that whenever the algorithm reaches a stable
state, the problem is solved and that when it finds a subset of the variables that is unsatisfiable it terminates, we only need to show that it reaches one of these two states in finite
time. The only way for the agents to not reach a stable state is when one or more of the
agents in the system is in an oscillation.
There are two cases to consider, the easy case is when a single agent is oscillating
(|V 0 | = 1) and the other case is when more than one agent is oscillating (|V 0 | > 1).
Case 1: There is an agent xi that is caught in an infinite loop and all other agents are
stable.
Let’s assume that xi is in an infinite processing loop. That means that no matter what
it changes its value to, it is in conflict with one of its neighbors, because if it changed its
value to something that doesn’t conflict with its neighbors, it would have a solution and
stop. If it changes its value to be in conflict with some xj that is higher priority than it,
then xj will mediate with xi , contradicting the assumption that all other agents are stable.
If xi changes its value to be in conflict with a lower priority agent, then by lemma 3, it will
act as mediator with its neighbors. Since it was assumed that each of the other agents is
in a stable state, then all of the agents in xi ’s good list will participate in the session and
by lemma 4, agent xi will have all of its conflicts removed. This means that xi will be in a
stable state contradicting the assumption that it was in an infinite loop.
Case 2: Two or more agents are in an oscillation.
Let’s say we have a set of agents V 0 ⊆ V that are in an oscillation. Now consider an
agent xi that is within V 0 . We know that the only conditions in which xi changes its value
is when it can do so and solve all of its conflicts (a contradiction because x i wouldn’t be
considered part of the oscillation), as the mediator, or as the receiver of a mediation from
some other agent in V 0 . The interesting case is when an agent acts as the mediator.
Consider the case when xi is the mediator and call the set of agents that it is mediating
over Vi . We know according to definition 1 that after the mediation, that at least one
conflict must be created or remain otherwise the oscillation would stop and the problem
would be solved. In fact, we know that each of the remaining conflicts must contain an
agent from the set V 0 − Vi by lemma 4. We also know that for each violated constraint that
has a member from Vi , that xi will link with any agent that is part of those constraints and
547

Mailler & Lesser

not a member of Vi . The next time xi mediates, the set Vi will include these members and
the number of agents in the set V 0 − Vi is reduced. In fact, whenever xi mediates the set
V 0 − Vi will be reduced (assuming it is not told to wait! by one or more agents. In this
case, it takes longer to reduce this set, but the proof still holds). Eventually, after O(|V | 2 )
mediations, some xi within V 0 must have Vi = V 0 (every agent within the set must have
mediated |V 0 | times in order for this to happen). When this agent mediates it will push the
violations outside of the set V 0 or it will solve the subproblem by lemma 4. Either of these
conditions contradicts the oscillation assumption. Therefore, the algorithm is complete.
QED
It should be fairly clear that, in domains that are exponential, the algorithm’s worsecase runtime is exponential. The space complexity of the algorithm is, however, polynomial,
because the agents only retain the names, priorities, values, and constraints of other agents.

5. Evaluation
A great deal of testing and evaluation has been conducted on the APO algorithm. Almost
exclusively, these test are done comparing the APO algorithm with the currently fastest
known, complete algorithm for solving DCSPs called the Asynchronous Weak Commitment
(AWC) protocol. In this section we will describe the AWC protocol (section 5.1), then will
describe the distributed 3-coloring domain and present results from extensive testing done
in this domain (section 5.2). This testing compares these two algorithms across a variety
of metrics, including the cycle time, number of messages, and serial runtime.
Next, we will describe the tracking domain (section 5.3) and present results from testing
in this domain as well. For this domain, we modified the core search algorithm of APO to
take advantage of the polynomial complexity of this problem. This variant called, APOFlow, will also be described.
5.1 The Asynchronous Weak Commitment (AWC) Protocol
The AWC protocol (Yokoo, 1995) was one of the first algorithms used for solving DCSPs.
Like the APO algorithm, AWC is based on variable decomposition. Also, like APO, AWC
assigns each agent a priority value that dynamically changes. AWC, however, uses the
weak-commitment heuristic (Yokoo, 1994) to assign these priorities values which is where
it gets its name.
Upon startup, each of the agents selects a value for its variable and sends “ok?” messages
to its neighbors (agents it shares a constraint with). This message includes the variables
value and priority (they all start at 0).
When an agent receives an “ok?” message, it updates its agent view and checks its
nogood list for violated nogoods. Each nogood is composed of a set of nogood pairs which
describe the combination of agents and values that lead to an unsatisfiable condition. Initially, the only nogoods in an agent’s nogood list are the constraints on its variable.
When checking its nogood list, agents only check for violations of higher priority nogoods. The priority of a nogood is defined as the priority of the lowest priority variable in
the nogood. If this value is greater than the priority of the agent’s variable, the nogood is
higher priority. Based on the results from this check, one of three things can happen:
548

Asynchronous Partial Overlay: A New Algorithm for DCSP

1. If no higher priority nogoods are violated, the agent does nothing.
2. If there are higher priority nogoods that are violated and this can be repaired by
simply changing the agent’s variable value, then the agent changes its value and sends
out “ok?” messages to the agents in its agent view. If there are multiple possible
satisfying values, then the agent chooses the one that minimizes the number of violated
lower priority nogoods.
3. If there are violated higher priority nogoods and this cannot be repaired by changing
the value of its variable, the agent generates a new nogood. If this nogood is the
same as a previously generated nogood, it does nothing. Otherwise, it then sends this
new nogood to every agent that has a variable contained in the nogood and raises the
priority value of its variable. Finally, it changes its variable value to one that causes
the least amount of conflict and sends out “ok?” messages.
Upon receiving a nogood message from another agent, the agent adds the nogood to
its nogood list and rechecks for nogood violations. If the new nogood includes the names
of agents that are not in its agent view it links to them. This linking step is essential to
the completeness of the search(Yokoo et al., 1992), but causes the agents to communicate
nogoods and ok? messages to agents that are not their direct neighbors in the constraint
graph. The overall effect is an increase in messages and a reduction in the amount of
privacy being provided to the agents because they communicate potential domain values
and information about their constraints through the exchange of ok? and nogood messages
with a larger number of agents.
One of the more recent advances to the AWC protocol has been the addition of resolventbased nogood learning (Hirayama & Yokoo, 2000) which is an adaptation of classical nogood
learning methods (Ginsberg, 1993; Cha & Iwana, 1996; Frost & Dechter, 1994).
The resolvent method is used whenever an agent finds that it needs to generate a new
nogood. Agents only generate new nogoods when each of their domain values are in violation
with at least one higher priority nogood already in their nogood list. The resolvent method
works by selecting one of these higher priority nogoods for each of the domain values and
aggregating them together into a new nogood. This is almost identical to a resolvent in
propositional logic which is why it is referred to as resolvent-based learning. The AWC
protocol used for all of our testing incorporates resolvent-based nogood learning.
5.2 Distributed Graph Coloring
Following directly from the definition for a CSP, a graph coloring problem, also known as
a k-colorability problem, consists of the following:
• a set of n variables V = {x1 , . . . , xn }.
• a set of possible colors for each of the variables D = {D1 , . . . , Dn } where each Di is
has exactly k allowable colors.
• a set of constraints R = {R1 , . . . , Rm } where each Ri (di , dj ) is predicate which implements the “not equals” relationship. This predicate returns true iff the value assigned
to xi differs from the value assigned to xj .
549

Mailler & Lesser

100

APO
AWC

Cycles

80

60

40

20

0
10

20

30

40

50

60

70

80

90

100

Variables

Figure 10: Comparison of the number of cycles needed to solve satisfiable, low-density 3coloring problems of various sizes by AWC and APO.

The problem is to find an assignment A = {d1 , . . . , dn |di ∈ Di } such that each of the
constraints in R is satisfied. Like the general CSP, graph coloring has been shown to be
NP-complete for all values of k > 2.
To test the APO algorithm, we implemented the AWC and APO algorithms and conducted experiments in the distributed 3-coloring domain. The distributed 3-coloring problem is a 3-coloring problem with n variables and m binary constraints where each agent is
given a single variable. We conducted 3 sets of graph coloring based experiments to compare
the algorithms’ computation and communication costs.
5.2.1 Satisfiable Graphs
In the first set of experiments, we created solvable graph instances with m = 2.0n (lowdensity), m = 2.3n (medium-density), and m = 2.7n (high-density) according to the method
presented in (Minton et al., 1992). Generating graphs in this way involves partitioning the
variables into k equal-sized groups. Edges are then added by selecting two of the groups at
random and adding an edge between a random member of each group. This method ensures
that the resulting graphs are satisfiable, but also tests a very limited and very likely easier
subset of the possible graphs. These tests were done because they are traditionally used by
other researchers in DCSPs.
These particular values for m were chosen because they represent the three major regions
within the phase-transition for 3-colorability (Culberson & Gent, 2001). A phase transition
in a CSP is defined based on an order parameter, in this case the average node degree
d. The transition occurs at the point where random graphs created with that order value
yield half satisfiable and half unsatisfiable instances. Values of the order parameter that are
550

Asynchronous Partial Overlay: A New Algorithm for DCSP

200

APO
AWC

Cycles

150

100

50

0
10

20

30

40

50

60

70

80

90

100

Variables

Figure 11: Comparison of the number of cycles needed to solve satisfiable, medium-density
3-coloring problems of various sizes by AWC and APO.

lower than the transition point (more than 50% of the instance are satisfiable) are referred
to being to the left of the transition. The opposite is true of values to the right.
Phase transitions are important because that are strongly correlated with the overall difficulty of finding a solution to the graph (Cheeseman, Kanefsky, & Taylor, 1991; Monasson,
Zecchina, Kirkpatrick, Selman, & Troyansky, 1999; Culberson & Gent, 2001). Within the
phase transition, randomly created instances are typically difficult to solve. Interestingly,
problems to the right and left of the phase transitions tend to be much easier.
In 3-colorability, the value d = 2.0 is to the left of the phase transition. In this region,
randomly created graphs are very likely to be satisfiable and are usually easy to find solve.
At d = 2.3, which is in the middle of the phase transition the graph has about a 50% chance
of being satisfiable and is usually hard to solve. For m = 2.7n, right of the phase transition,
graphs are more than likely to be unsatisfiable and, again, are also easier to solve.
A number of papers (Yokoo & Hirayama, 2000, 1996; Hirayama & Yokoo, 2000) have
reported that m = 2.7n is within the critical phase transition for 3-colorability. This seems
to have been caused by a misinterpretation of previous work in this area(Cheeseman et al.,
1991). Although Cheeseman, Kanefsky, and Taylor reported that m = 2.7n was within the
critical region for 3-colorability, they were using reduced graphs for their analysis.
A reduced graph is one in which the trivially colorable nodes and non-relevant edges
have been removed. For example, one can easily remove any node with just two edges in
a 3-coloring problem because it can always be trivially colored. Additionally, nodes that
possess a unique domain element from their neighbors can also be easily removed.
In later work, Culberson and Gent identified the critical region as being approximately
d = 2.3 and therefore was included in our tests(Culberson & Gent, 2001). One should note,
however, that because phase transitions are typically done on completely random graphs
551

Mailler & Lesser

d

2.0

2.3

2.7

Nodes
15
30
45
60
75
90
Overall
15
30
45
60
75
90
Overall
15
30
45
60
75
90
Overall

APO
Mean
17.82
27.07
39.97
53.24
59.83
80.75

APO
StDev
8.15
17.11
25.79
32.32
35.35
54.30

AWC
Mean
17.38
24.62
43.76
69.96
80.32
82.92

AWC
StDev
12.70
19.23
30.98
49.03
103.76
61.01

15.04
34.01
47.72
92.73
114.02
160.88

5.55
16.81
26.58
72.46
75.84
125.12

20.49
41.30
109.99
135.60
185.84
189.04

11.27
29.58
74.85
146.57
119.94
91.27

13.83
27.28
42.47
52.15
64.54
87.14

3.56
10.10
18.01
23.12
26.26
42.82

20.29
39.99
62.92
86.89
104.09
127.04

10.32
24.08
35.23
43.69
46.62
64.97

p(AW C ≤ AP O)
0.77
0.27
0.34
0.01
0.07
0.81
0.01
0.00
0.04
0.00
0.01
0.00
0.07
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

Table 1: Comparison of the number of cycles needed to solve satisfiable 3-coloring problems
of various sizes and densities by AWC and APO.

552

Asynchronous Partial Overlay: A New Algorithm for DCSP

APO

140

AWC
120

Cycles

100
80
60
40
20
0
10

20

30

40

50

60

70

80

90

100

Variables

Figure 12: Comparison of the number of cycles needed to solve satisfiable, high-density
3-coloring problems of various sizes by AWC and APO.

and by its very definition involves both satisfiable and unsatisfiable instances, it is very
hard to apply the phase-tranisition results to graphs created using the technique described
at the beginning of the section because it only generates satisfiable instances. A detailed
phase transition analysis has not been done on this graph generation technique and in fact,
we believe that these graphs tend to be easier than randomly created satisfiable ones of the
same size and order.
To evaluate the relative strengths and weakness of each of the approaches, we measured
the number of cycles and the number of messages used during the course of solving each
of the problems. During a cycle, incoming messages are delivered, the agent is allowed
to process the information, and any messages that were created during the processing are
added to the outgoing queue to be delivered at the beginning of the next cycle. The actual
execution time given to one agent during a cycle varies according to the amount of work
needed to process all of the incoming messages. The random seeds used to create each
graph instance and variable instantiation were saved and used by each of the algorithms for
fairness.
For this comparison between AWC and APO, we randomly generated 10 graphs of size
n = 15, 30, 45, 60, 75, 90 and m = 2.0n, 2.3n, 2.7n and for each instance generated 10 initial
variable assignments. Therefore, for each combination of n and m, we ran 100 trials making
a total of 1800 trials. The results from this experiment can be seen in figures 10 through 15
and table 1. We should mention that the results of the testing on AWC obtained from these
experiments agree with previous results (Hirayama & Yokoo, 2000) verifying the correctness
of our implementation.
At first glance, figure 10 appears to indicate that for satisfiable low-density graph instances, AWC and APO perform almost identically in terms of cycles to completion. Look553

Mailler & Lesser

APO

AWC

Nodes
15
30
45
60
75
90
15
30
45
60
75
90

% Links
Mean
32.93
17.24
11.97
9.30
7.42
6.51
55.76
32.89
27.00
26.47
21.99
20.11

% Links
StDev
3.52
2.59
1.84
1.30
0.95
0.96
12.31
8.88
7.89
9.12
7.67
7.89

% Central
Mean
60.53
42.53
33.27
29.00
24.60
24.93
79.73
60.97
56.49
56.98
53.19
50.38

% Central
StDev
9.12
8.06
6.61
7.31
6.73
6.16
11.45
10.98
11.57
14.23
13.29
13.78

Table 2: Link statistics for satisfiable, low-density problems.

APO

AWC

Nodes
15
30
45
60
75
90
15
30
45
60
75
90

% Links
Mean
37.46
21.24
14.90
12.85
11.17
10.07
63.19
42.84
52.05
43.69
47.77
44.04

% Links
StDev
3.45
2.85
2.27
2.99
2.45
3.10
12.06
11.83
13.52
14.59
14.10
10.84

% Central
Mean
67.60
51.37
43.67
42.98
43.76
40.47
83.56
72.20
80.67
74.93
79.41
77.98

% Central
StDev
8.26
9.32
11.40
13.06
12.69
14.55
8.96
12.09
11.24
14.56
12.18
10.52

Table 3: Link statistics for satisfiable, medium-density problems.

554

Asynchronous Partial Overlay: A New Algorithm for DCSP

APO

AWC

Nodes
15
30
45
60
75
90
15
30
45
60
75
90

% Links
Mean
43.28
24.29
18.07
13.86
11.85
10.86
78.87
58.03
54.06
53.01
49.63
47.72

% Links
StDev
2.86
2.61
2.06
1.77
1.57
1.85
12.29
11.66
13.44
12.77
11.36
13.81

% Central
Mean
74.53
59.30
52.62
47.78
46.37
50.73
93.60
86.27
82.67
83.47
81.91
80.32

% Central
StDev
8.34
9.75
10.21
11.32
11.32
14.48
7.75
9.54
13.34
11.04
9.93
15.06

Table 4: Link statistics for satisfiable, high-density problems.
ing at the associated table (Table 1), however, reveals that overall, the pairwise T-test
indicates that with 99% confidence, APO outperforms AWC on these graphs.
As the density, or average degree, of the graph increases, the difference becomes more
apparent. Figures 11 and 12 show that APO begins to scale much more efficiently than
AWC. This can be attributed to the ability of APO to rapidly identify strong interdependencies between variables and to derive solutions to them using a centralized search of the
partial subproblem.
Tables 2 through 4, partially verify this statement. As you can see, on average, less than
50% of the possible number of links (n ∗ (n − 1)) are used by APO in solving problems (%
Links column). In addition, the maximum amount of centralization (% Central column)
occuring within any single agent (i.e. the number of agents in its agent view) remains fairly
low. The highest degree of centralization occurs for small, high-density graphs. Intuitively,
this makes a lot of sense because in these graphs, a single node is likely to have a high
degree from the very start. Combine this fact with the dynamic priority ordering and the
result is large amounts of central problem solving.
The most profound differences in the algorithms can be seen in figures 13, 14, and 15 and
table 5. APO uses at least an order of magnitude less messages than AWC. Table 6 shows
that these message savings lead to large savings in the number of bytes being transmitted as
well. Even though APO uses about twice as many bytes per message as AWC (the messages
were not optimized at all), the total amount of information be passed around is significantly
less in almost every case.
Again, looking at the linking structure that AWC produces gives some insights into why
it uses some many more messages than APO. Because agents communicate with all of the
agents they are linked to whenever their value changes, and a large number of changes can
occur in single cycle, AWC can have a tremendous amount of thrashing behavior. APO,
on the other hand, avoids this problem because the process of mediating implicitly creates
555

Mailler & Lesser

d

2.0

2.3

2.7

Nodes
15
30
45
60
75
90
Overall
15
30
45
60
75
90
Overall
15
30
45
60
75
90
Overall

APO
Mean
361.50
1117.15
2078.72
3387.13
4304.22
6742.14

APO
StDev
179.57
844.33
1552.98
2084.69
2651.15
4482.54

AWC
Mean
882.19
2431.71
6926.86
18504.17
21219.01
33125.57

AWC
StDev
967.21
3182.51
7395.22
22281.59
22714.98
39766.56

379.15
1640.08
3299.05
8773.16
14368.87
25826.74

188.69
931.22
2155.45
9613.84
12066.32
29172.66

1205.50
6325.21
44191.89
70104.74
178683.02
201145.37

923.85
6914.79
44693.33
69050.66
173493.21
143236.26

433.64
1623.89
3859.99
5838.36
9507.60
16455.59

164.12
787.59
1921.51
3140.53
4486.04
10679.92

1667.71
9014.02
28964.43
66857.87
116016.71
196239.22

1301.03
8104.34
22900.89
53221.05
82857.63
163722.90

p(AW C ≤ AP O)
0.00
0.00
0.00
0.00
0.00
0.00
0.01
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

Table 5: Comparison of the number of messages needed to solve satisfiable 3-coloring problems of various sizes and densities by AWC and APO.

556

Asynchronous Partial Overlay: A New Algorithm for DCSP

d

2.0

2.3

2.7

Nodes
15
30
45
60
75
90
Overall
15
30
45
60
75
90
Overall
15
30
45
60
75
90
Overall

APO
Mean
10560.76
32314.97
59895.10
97126.79
123565.94
192265.35

APO
StDev
4909.09
23138.55
42740.82
57428.70
73493.72
123384.05

AWC
Mean
11590.38
32409.93
95061.77
259529.42
294502.71
466084.60

AWC
StDev
13765.19
45336.65
106391.51
326087.70
328696.59
581963.72

11370.13
47539.20
95098.49
247417.78
401618.24
712035.13

4951.75
25486.74
59312.25
262844.89
327990.65
782835.83

16260.19
88946.59
644007.01
1018059.11
2626178.31
2935211.45

13237.31
101077.04
675192.26
1029273.23
2606377.80
2138087.17

13415.51
48542.24
112541.02
170174.55
272391.95
465571.42

4280.15
21331.96
52729.10
85705.12
122177.07
288265.82

22393.61
125072.76
405535.81
945039.32
1641250.63
2793725.78

18578.25
116518.80
327349.47
773937.60
1204185.34
2397839.47

p(AW C ≤ AP O)
0.49
0.98
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

Table 6: Comparison of the number of bytes transmitted by APO and AWC for satisfiable
graph instances of various sizes and density.

557

Mailler & Lesser

35000

APO
AWC

30000

Messages

25000
20000
15000
10000
5000
0
10

20

30

40

50

60

70

80

90

100

Variables

Figure 13: Comparison of the number of messages needed to solve satisfiable, low-density
3-coloring problems of various sizes by AWC and APO.

250000

APO
AWC

Messages

200000

150000

100000

50000

0
10

20

30

40

50

60

70

80

90

100

Variables

Figure 14: Comparison of the number of messages needed to solve satisfiable, mediumdensity 3-coloring problems of various sizes by AWC and APO.

regions of stability in the problem landscape while the mediator decides on a solution. In
addition, because APO uses partial centralization to solve problems, it avoids having to use
a large number of messages to discover implied constraints through trial and error.
558

Asynchronous Partial Overlay: A New Algorithm for DCSP

200000

APO
AWC

Messages

150000

100000

50000

0
10

20

30

40

50

60

70

80

90

100

Variables

Figure 15: Comparison of the number of messages needed to solve satisfiable, high-density
3-coloring problems of various sizes by AWC and APO.

% Satisfiable

1
0.8
0.6
0.4
0.2
0
1.8

2

2.2

2.4

2.6

2.8

Density

Figure 16: Phase transition curve for the 60 node randomly generated graphs used in testing.

As we will see in the next two experiments, the high degree of centralization caused
by its unfocused linking degrades AWC’s performance even more when solving randomly
generated, possibly unsatisfiable, graph instances.
559

Mailler & Lesser

800

APO

700

AWC

Cycles

600
500
400
300
200
100
0
1.8

2

2.2

2.4

2.6

2.8

Density

Figure 17: Number of cycles needed to solve completely random 60 variable problems of
various density using AWC and APO.

Density
1.8
2.0
2.1
2.3
2.5
2.7
2.9
Overall

APO
Mean
49.88
88.77
116.79
116.41
56.21
27.62
17.74

APO
StDev
41.98
83.79
107.21
264.65
45.12
25.66
13.69

% APO
Solved
100
100
100
100
100
100
100

AWC
Mean
52.51
189.42
377.54
660.65
640.66
537.99
476.20

AWC
StDev
77.06
237.17
364.55
362.80
335.65
324.87
271.63

% AWC
Solved
100
96
80
55
65
83
92

p(AW C ≤ AP O)
0.62
0.00
0.00
0.00
0.00
0.00
0.00
0.00

Table 7: Number of cycles needed to solve completely random 60 variable problems of
various density using AWC and APO.

560

Asynchronous Partial Overlay: A New Algorithm for DCSP

1.4e+06

APO
AWC

1.2e+06

Messages

1e+06
800000
600000
400000
200000
0
1.8

2

2.2

2.4

2.6

2.8

Density

Figure 18: Number of messages needed to solve completely random 60 variable problems of
various density using AWC and APO.

5.2.2 Random Graphs
In the second set of experiments, we generated completely random 60 node graphs with
average degrees from d = 1.8 to 2.9. This series was conducted to test the completeness of
the algorithms, verify the correctness of their implementations, and to study the effects of
the phase transition on their performance. For each value of d, we generated 200 random
graphs each with a single set of initial values. Graphs were generate by randomly choosing
two nodes and connecting them. If the edge already existed, another pair was chosen. The
phase transition curve for these instances can be seen in figure 16.
In total, 1400 graphs were generated and tested. Due to time constraints, we stopped
the execution of AWC once 1000 cycles had completed (APO never reached 1000). The
results of these experiments are shown in figures 17 and 18 and tables 7 and 8.
On these graphs, APO significantly outperforms AWC on all but the simplest problems
(see figure 17). These results can most directly be attributed to AWC’s poor performance
on unsatisfiable problem instances (Fernandez et al., 2003). In fact, in the region of the
phase transition, AWC was unable to complete 45% of the graphs within the 1000 cycles.
In addition, to solve these problems, AWC uses at least an order of magnitude more
messages that APO. These results can be seen in figure 18. By looking at table 9, it is easy
to see why this occurs. AWC has a very high degree of linking and centralization. In fact,
on d = 2.9 graphs, AWC reaches an average of 93% centralization and 75% of complete
inter-agent linking.
To contrast this, APO has very loose linking throughout the entire phase transition and
centralizes on average around 50% of the entire problem. These results are very encouraging and reinforce the idea that partial overlays and extending along critical paths yields
improvements in the convergence on solutions.
561

Mailler & Lesser

Density
1.8
2.0
2.1
2.3
2.5
2.7
2.9
Overall

APO
Mean
2822.61
7508.33
12642.68
15614.37
8219.74
4196.58
2736.20

APO
StDev
3040.39
9577.86
16193.56
15761.90
7415.76
4201.80
2286.39

AWC
Mean
12741.58
126658.29
356993.39
882813.45
1080277.25
1047001.18
1000217.83

AWC
StDev
47165.70
269976.18
444899.21
566715.73
661909.63
738367.27
699199.90

p(AW C ≤ AP O)
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

Table 8: Number of messages needed to solve completely random 60 variable problems of
various density using AWC and APO.

APO

AWC

Density
1.8
2.0
2.1
2.3
2.5
2.7
2.9
1.8
2.0
2.1
2.3
2.5
2.7
2.9

% Links
Mean
8.09
10.93
13.31
15.56
14.37
13.19
13.12
16.28
34.34
46.52
64.13
70.06
72.95
75.19

% Links
StDev
1.50
3.28
4.49
4.94
3.74
3.32
2.70
6.58
13.44
13.95
11.43
10.51
10.65
9.62

% Central
Mean
26.19
36.92
46.68
55.91
53.86
47.33
45.26
41.08
65.00
75.24
86.06
89.46
91.71
92.78

% Central
StDev
6.94
13.77
15.86
18.18
18.25
19.59
17.55
12.42
14.41
10.66
5.61
4.55
3.80
5.63

Table 9: Link statistics for 60 node random problems.
5.2.3 Runtime Tests
In the third set of experiments, we directly compared the serial runtime performance of
AWC against APO. Serial runtime is measured using the following formula:

serialtime =

cycles
X

X

i=0 a∈agents

562

time(a, i)

Seconds

Asynchronous Partial Overlay: A New Algorithm for DCSP

23900
14160
8390
4970
2940
1740
1030
610
360
210
120
70
40
20
10

APO
AWC
Backtracking

15

25

35

45

55

65

Nodes

Seconds

Figure 19: Comparison of the number of seconds needed to solve random, low-density 3coloring problems of various sizes by AWC, APO, and centralized Backtracking.

92350
51680
28920
16180
9050
5060
2830
1580
880
490
270
150
80
40
20
10

APO
AWC
Backtracking

15

25

35

45

55

65

Nodes

Figure 20: Comparison of the number of seconds needed to solve random, medium-density
3-coloring problems of various sizes by AWC, APO, and centralized Backtracking.

563

Seconds

Mailler & Lesser

3620
2360
1540
1000
650
420
270
170
110
70
40
20
10

APO
AWC
Backtracking

15

25

35

45

55

65

Nodes

Figure 21: Comparison of the number of seconds needed to solve random, high-density 3coloring problems of various sizes by AWC, APO, and centralized Backtracking.

d
2.0

2.3

2.7

Nodes
15
30
45
60
15
30
45
60
15
30
45
60

APO
Mean
0.26
0.78
1.27
2.02
0.18
0.98
2.19
10.51
0.09
0.40
0.66
5.47

APO
StDev
0.21
0.66
1.18
1.74
0.23
0.88
2.26
12.97
0.06
0.39
0.63
5.44

AWC
Mean
2.72
10.52
257.88
890.12
3.96
112.95
1236.27
32616.24
3.51
61.43
460.56
4239.16

AWC
StDev
4.40
18.99
1252.25
4288.43
3.23
144.24
1777.51
62111.52
2.90
59.77
690.98
4114.30

BT
Mean
0.02
1.65
245.14
27183.64
0.03
0.86
150.73
92173.71
0.02
0.29
35.58
2997.04

BT
StDev
0.02
4.14
587.31
56053.26
0.01
1.07
241.02
222327.59
0.01
0.36
57.75
4379.97

Table 10: Comparison of the number of seconds needed to solve random 3-coloring problems
of various sizes and densities using AWC, APO, and centralized Backtracking.

564

Asynchronous Partial Overlay: A New Algorithm for DCSP

Which is just the total accumulated runtime needed to solve the problem when only one
agent is allowed to process at a time.
For these experiments, we again generated random graphs, this time varying the size
and the density of the graph. We generated 25 graphs for the values of n = 15, 30, 45, 60 and
the densities of d = 2.0, 2.3, 2.7, for a total of 300 test cases. To show that the performance
difference in APO and AWC was not caused by the speed of the central solver, we ran a
centralized backtracking algorithm on the same graph instances. Although, APO uses the
branch and bound algorithm, the backtracking algorithm used in this test provides a best
case lower bound on the runtime of APO’s internal solver.
Each of the programs used in this test was run on an identical 2.4GHz Pentium 4 with
768 Mbytes of RAM. These machines where entirely dedicated to the tests so there was a
minimal amount of interference by competing processes. In addition, no computational cost
was assigned to message passing because the simulator passes messages between cycles. The
algorithms were, however, penalized for the amount of time they took to process messages.
Although we realize that the specific implementation of an algorithm can greatly effect its
runtime performance, every possible effort was made to optimize the AWC implementation
used for these experiments in an effort to be fair.
The results of this test series can be seen in figures 19, 20, and 21. You should note
that the scale used for these graphs is logarithmic. From looking at these results, two
things should become apparent. Obviously, the first is that APO outperforms AWC in
every case. Second, APO actually outperforms its own centralized solver on graphs larger
than 45 nodes. This indicates two things. First, the solver that is currently in APO is
very poor and second that APO’s runtime performance is not a direct result of the speed of
centralized solver that it is using. In fact, these tests show that the improved performance
of APO over AWC is caused by APO’s ability to take advantage of the problem’s structure.
If we were to replace the centralized solver used in these tests with a state-of-the-art
solver, we would expect two things. The first is that we would expect the serial runtime of
the APO algorithm to decrease simply from the speedup caused by the centralized solver.
The second, and more importantly, is that the centralized solver would always outperform
APO. This is because current CSP solvers take advantage of problem structure unlike the
solver used in these tests. We are in no way making a claim that APO improves any
centralized solver. We are simply stating that APO outperforms AWC for reasons other
than the speed of its current internal solver.
5.3 Tracking Domain
To test APO’s adaptability to various centralized solvers, we created an implementation
of the complete-compatibility version of the SensorDCSP formulation (Bejar et al., 2001;
Krishnamachari, Bejar, & Wicker, 2002; Fernandez et al., 2003). In this domain, there
are a number of sensors and a number of targets randomly placed within an environment.
Because of range restrictions, only sensors that are within some distance dist can see a
target. The goal is to find an assignment of sensors to targets such that each target has
three sensors tracking it.
Following directly from the definition for a CSP, a SensorDCSP problem consists of the
following:
565

Mailler & Lesser

Figure 22: An example of a tracking problem. There are 30 targets (labeled with their
name) and 224 sensors (black dots). Lines connecting sensors and targets indicate that the sensor is assigned to tracking the target.

566

Asynchronous Partial Overlay: A New Algorithm for DCSP

• a set of n targets T = {T1 , . . . , Tn }.
• a set of possible sensors that can “see” each of the targets D = {D 1 , . . . , Dn }.
• a set of constraints R = {R1 , . . . , Rm } where each Ri (ai , aj ) is predicate which implements the “not intersects” relationship. This predicate returns true iff the sensors
assigned to Ti does not have any elements in common with the sensors assigned to Tj .
The problem is to find an assignment¡ A ¢= {a1 , . . . , an } such that each of the constraints
in R is satisfied and each ai is a set of |Dci | sensors from Di where c = min(|Di |, 3). This
indicates that each target requires 3 sensors, if enough are available, or all of the sensors, if
there are less than 3.
Since, in this implementation, each of the sensors is compatible with one another, the
overall complexity of the problem is polynomial, using a reduction to feasible flow in a
bipartite graph(Krishnamachari, 2002). Because of this, the centralized solver used by
the APO agents was changed to a modified version of the Ford-Fulkerson maximum flow
algorithm (Ford & Fulkerson, 1962; Cormen, Leiserson, & Rivest, 1999), which has been
proven to run in polynomial time.
An example of the tracking problem can be seen in figure 22. In this example, there
are 224 sensors (black dots) placed in an ordered pattern in the environment. There are
30 targets (labeled with their names) which are randomly placed at startup. The lines
connecting sensors to targets indicate that the sensor is assigned to the target. Note that
this instance of the problem is satisfiable.
5.3.1 Modifying APO for the Tracking Domain
Because the tracking domain is so closely related to the general CSP formulation, very few
changes were made to either AWC or APO for these tests. We did, however, decide to test
the adaptability of APO to a new centralized problem solver. To do this, we changed the
centralized problem solver to the Ford Fulkerson max-flow algorithm in figure 23. FordFulkerson works by repeatedly finding paths with remaining capacity through the residual
flow network and augmenting the flows along those paths. The algorithm terminates when
no additional paths can be found. A detailed explanation of the algorithm as well as a proof
of its optimality can be found in (Cormen et al., 1999).
Like mapping bipartite graphs into max-flow, the SensorDCSP problem is also easily
mapped into max-flow. In figures 24 and 25 you can see the mapping of a simple sensor
allocation problem into a max-flow problem. Notice that the capacity of the flow between
the sensors and targets is 1. This ensures that a sensor cannot be used by more than a
single target. Also, notice that the capacity of the targets to t is 3. In fact, this value is to
min(|Di |, 3).
To use this algorithm within APO, the mediator simply translates the problem into
a network flow graph G using the following rules whenever it runs the choose solution
procedure in figure 5:
1. Add the nodes s and t to G.
2. For each Ti ∈ T add a node Ti and an edge (Ti , t) with capacity min(|Di |, 3) to G.
567

Mailler & Lesser

Ford-Fulkerson (G, s, t)
for each edge (u, v) ∈ E[G] do
f [u, v] ← 0;
f [v, u] ← 0;
end do;
while there exists a path p from s to t
in the residual network Gf do
cf (p) ← min{cf (u, v) : (u, v) ∈ p};
for each edge (u, v) ∈ p do
f [u, v] ← f [u, v] + cf (p);
f [v, u] ← −f [u, v];
end do;
end do;
end Ford-Fulkerson;
Figure 23: The Ford-Fulkerson maximum flow algorithm.

S1

S2

T1

S3

S4
T2
S5

S6

Figure 24: A simple sensor to target allocation problem.
3. For each unique sensor Si in the domains of Ti ∈ T , add a node Si , an edge (s, Si )
with capacity 1, and an edge (Si , Ti ) with capacity 1 to G.

568

Asynchronous Partial Overlay: A New Algorithm for DCSP

S1
1

1
S2

1
1

S3

1
1
1

T1

3

s

t
1

1
3

S4
1
S5

1
1

T2

1
S6

Figure 25: The flow network for the simple target allocation problem in figure 24.
It then executes the Ford-Fulkerson algorithm. Once the algorithm finishes, the mediator
checks the residual capacity of the edges between the targets and t. If any of these edges has
residual flow, then the problem is unsatisfiable. Otherwise, the assignment can be derived
by finding all of the (Si , Ti ) edges that have a flow of 1.
One of the nicest characteristics of the Ford-Fulkerson algorithm is that it works regardless of the order that the paths in the residual network are chosen. In our implementation,
we used a breadth-first search which, in addition to identifying paths in the residual network, minimized the cost of the path. Cost in this sense refers to the amount of external
conflict that is created by having a sensor assigned to a target. This modification maintains
the min-conflict heuristic which is an integral part of extending the mediators local view.
5.3.2 Results
To test APO and AWC in this domain, we ran a test series which used a 200f t × 200f t
environment with 224 sensors placed in an ordered grid-based pattern. We chose to place
the sensors in an ordered fashion to reduce the variance obtained within the results. We ran
a test series which varied the sensor to target ratio from 10:1 to 3.8:1 (22 to 59 targets) in
increments of 0.2 which is across the spectrum from mostly satisfiable to mostly unsatisfiable
instances (see figure 26). We then conducted 250 trial runs with a random target placement
for each of these values to get a good statistical sampling.
In total, 6750 test cases were used. For comparison, we measured the number of messages
and cycles that were taken by the algorithms to find a solution. The random seeds used to
place the targets were saved, so APO and AWC were both tested using identical problem
569

Mailler & Lesser

0.9
0.8

% Satisfiable

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
20

25

30

35

40

45

50

55

60

Targets

Figure 26: Phase transition curve for the 224 sensor environment used for testing.

APO
AWC

Cycles

20

15

10

5

0
20

25

30

35

40

45

50

55

60

Targets

Figure 27: Number of cycles needed to solve random target configurations in a field of 224
sensors using AWC and APO.

instances. The correctness of the algorithms was verified by cross-checking the solutions
(satisfiable/unsatisfiable) obtained during these tests, which matched identically.
As can be seen in figure 27 and 28 and tables 11 and 12, APO outperforms AWC on all
but the simplest cases. Part of the reason for this is the minimum 3 cycles it takes APO to
finish a mediation session. In problems that have very sparsely connected interdependencies,
this cost tends to dominate. All-in-all, as the T-tests indicate, APO is significantly better
than AWC in terms of both cycles to completion and number of messages used for problems
in this domain.
570

Asynchronous Partial Overlay: A New Algorithm for DCSP

Targets
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
39
40
41
43
45
47
49
51
53
56
59
Overall

APO
Mean
6.36
6.65
7.12
6.55
6.80
7.09
7.38
7.10
7.55
7.18
6.88
7.62
7.47
7.56
8.08
7.48
7.55
6.45
7.45
5.96
4.80
5.15
4.53
3.52
4.12
3.14
3.28

APO
StDev
2.33
3.39
4.72
3.24
4.28
5.02
5.88
4.89
5.99
6.11
6.31
7.59
7.81
7.49
9.89
8.38
10.87
10.54
13.11
7.79
7.25
8.31
5.90
2.00
5.82
0.59
2.26

AWC
Mean
5.88
7.32
8.83
7.15
9.65
10.85
11.05
9.24
13.15
12.42
11.73
12.32
15.88
14.74
15.70
20.70
16.12
15.74
17.56
16.10
17.61
18.52
15.33
14.34
13.13
10.45
7.46

AWC
StDev
6.61
10.55
19.96
10.56
15.78
19.89
15.89
13.76
25.58
22.22
18.21
24.04
28.82
29.01
25.46
39.63
26.43
21.66
30.98
22.91
28.50
30.27
25.28
22.60
22.55
20.81
10.47

p(AW C ≤ AP O)
0.29
0.36
0.18
0.39
0.01
0.00
0.00
0.02
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

Table 11: Number of cycles needed to solve random target configurations in a field of 224
sensors using AWC and APO.

571

Mailler & Lesser

Targets
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
39
40
41
43
45
47
49
51
53
56
59
Overall

APO
Mean
78.28
89.52
105.53
102.92
116.36
128.58
149.23
145.72
167.50
169.30
174.32
212.59
218.74
221.93
258.41
258.95
303.64
293.24
342.33
274.39
277.26
311.91
303.66
269.37
333.42
296.36
339.21

APO
StDev
32.58
54.01
90.36
57.18
86.58
98.63
144.81
93.24
144.27
152.83
152.89
237.60
246.18
230.44
354.13
342.97
501.10
649.42
724.64
267.95
414.19
405.1
299.13
110.57
390.08
45.54
202.98

AWC
Mean
95.68
133.12
184.19
149.18
245.99
263.32
279.49
231.98
378.89
404.40
362.63
410.05
811.58
613.64
671.00
947.95
815.32
884.32
912.65
1279.97
1334.38
1471.82
1487.65
1571.46
1804.47
1895.23
1765.18

AWC
StDev
133.53
237.74
616.36
298.85
566.03
587.30
493.05
335.98
874.99
997.82
570.41
923.25
2243.79
1422.33
1333.50
2116.98
1373.29
1407.99
1517.10
2194.84
2470.13
2172.13
2503.62
2157.52
2815.94
3731.98
3676.16

p(AW C ≤ AP O)
0.04
0.00
0.04
0.02
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

Table 12: Number of messages needed to solve random target configurations targets in a
field of 224 sensors using AWC and APO.

572

Asynchronous Partial Overlay: A New Algorithm for DCSP

APO

Messages

2000

AWC

1500

1000

500

0
20

25

30

35

40

45

50

55

60

Targets

Figure 28: Number of messages needed to solve random target configurations targets in a
field of 224 sensors using AWC and APO.

6. Conclusions and Future Directions
In this article, we presented a new complete, distributed constraint satisfaction protocol
called Asynchronous Partial Overlay (APO). Like AWC, APO allows the agents to retain
their autonomy because they can obscure or completely hide internal variables and constraints. In addition, agents can refuse solutions posed by a mediator, instead taking over
as the mediator if for some reason they are unhappy with a proposed solution. We also
presented an example of its execution on a simple problem (section 4.2) and proved the
soundness and completeness of the algorithm (section 4.3). Through extensive empirical
testing on 10,250 graph instances from the graph coloring and tracking domain, we also
showed that APO significantly outperforms the currently best known distributed constraint
satisfaction algorithm, AWC (Yokoo, 1995). These tests have shown that APO is better
than AWC in terms of cycles to completion, message usage, and runtime performance. We
have also shown that the runtime characteristics can not be directly attributed to the speed
of the centralized solver.
APO’s performance enhancements can be attributed to a number of things. First,
APO exhibits a hill-climbing nature early in the search which becomes more focused and
controlled as time goes on. Like other hill-climbing techniques this often leads to a satisfiable
solution early in the search. Second, by using partial overlaying of the information that the
agents use in decision making, APO exploits the work that has been previously done by
other mediators. This forms a lock and key mechanism which promotes solution stability.
Lastly, and most importantly, because APO uses dynamic, partial centralization, the agents
work on smaller, highly relevant portions of the overall problem. By identifying these areas
of decomposability, the search space can be greatly reduced which, in some cases, improves
the efficiency of the centralized search algorithm.
There are a vast number of improvements planned for APO in the future. Probably the
most important is to improve the centralized solver that it uses. In this article, an inefficient
solver was chosen to show the strengths of the distributed portions of APO. We expect
573

Mailler & Lesser

that additional improvements in the algorithm’s runtime performance can be obtained by
using a faster centralized search engine. In addition, modern solvers often use methods
like graph reductions, unit propagation and backbone guided search. It is conceivable that
information gained from the centralized search engine could be used to prune the domains
from the variables for consistency reasons and variables from the centralized subproblem
for relevance reasons. We expect this will further focus the efforts of the agents additionally
reducing the search time and communications usage of the algorithm.
Along with these improvements is the selective use of memory for recording nogoods.
Unlike AWC which uses the nogoods to ensure a complete search, APO’s completeness relies
on one of the agents centralizing the entire problem in the worst case. Because of this key
difference, APO can be improved by simply remembering a small, powerful subset of the
nogoods that it discovers from mediation session to session. This would allow the algorithm
to improve future search by exploiting work that it had done previously.
What should be clear is that APO, and the cooperative mediation methodology as a
whole, opens up new areas for future exploration and new questions to be answered in
distributed problem solving. We believe that this work shows a great deal of promise for
addressing a vast number of problems and represents a bridge between centralized and
distributed problem solving techniques.

Acknowledgments
Special thanks to Bryan Horling for his design and implementation of the Farm simulation
environment in which the experiment were run and to Shlomo Zilberstein, Bart Selman,
Neil Immerman, and Jose Vidal for making numerous suggestions during the development
of this work. Lastly, the authors would like to thank the JAIR reviewers for their helpful
feedback and suggestions and Carlos Ansotegui and Jean-Charles Régin for their lengthy
discussion during the final revision to this article.
The effort represented in this paper has been sponsored by the Defense Advanced Research Projects Agency (DARPA) and Air Force Research Laboratory, Air Force Materiel
Command, USAF, under agreement number F30602-99-2-0525. The views and conclusions
contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Defense
Advanced Research Projects Agency (DARPA), Air Force Research Laboratory, or the U.S.
Government. The U.S. Government is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright annotation thereon.

References
Bejar, R., Krishnamachari, B., Gomes, C., & Selman, B. (2001). Distributed constraint
satisfaction in a wireless sensor tracking system. In Workshop on Distributed Constraint Reasoning, International Joint Conference on Artificial Intelligence, Seattle,
Washington.
Cammarata, S., McArthur, D., & Steeb, R. (1983). Strategies of cooperation in distributed
problem solving. In Proceedings of the 8th International Joint Conference on Artificial
574

Asynchronous Partial Overlay: A New Algorithm for DCSP

Intelligence (IJCAI-83), Vol. 2, pp. 767–770.
Cha, B., & Iwana, K. (1996). Adding new clauses for faster local search. In Proceedings of
the Thirteenth National Conference on Artificial Intelligence (AAAI), pp. 332–337.
Cheeseman, P., Kanefsky, B., & Taylor, W. (1991). Where the really hard problems are.
In Proceedings of the 12th International Joint Conference on Artificial Intelligence
(IJCAI-91), pp. 331–337.
Conry, S. E., Kuwabara, K., Lesser, V. R., & Meyer, R. A. (1991). Multistage negotiation
for distributed constraint satisfaction. IEEE Transactions on Systems, Man, and
Cybernetics, 21 (6).
Cormen, T. H., Leiserson, C. E., & Rivest, R. L. (1999). Introduction to Algorithms.
McGraw-Hill.
Culberson, J., & Gent, I. (2001). Frozen development in graph coloring. Theoretical Computer Science, 265 (1–2), 227–264.
Fernandez, C., Bejar, R., Krishnamachari, B., Gomes, C., & Selman, B. (2003). Distributed
Sensor Networks: A Multiagent Perspective, chap. Communication and Computation
in Distributed CSP Algorithms, pp. 299–317. Kluwer Academic Publishers.
Ford, L. R., & Fulkerson, D. (1962). Flows in Networks. Princeton University Press.
Freuder, E. C., & Wallace, R. J. (1992). Partial constraint satisfaction. Artificial Intelligence, 58 (1–3), 21–70.
Frost, D., & Dechter, R. (1994). Dead-end driven learning. In Proceedings of the Twelfth
Natioanl Conference on Artificial Intelligence, pp. 294–300.
Ginsberg, M. L. (1993). Dynamic backtracking. Journal of Artificial Intelligence Research,
1, 25–46.
Hayden, S., Carrick, C., & Yang, Q. (1999). Architectural design patterns for multi-agent
coordination. In Proceedings of the International Conference on Agent Systems, Seattle, WA.
Hirayama, K., & Yokoo, M. (2000). The effect of nogood learning in distributed constraint
satisfaction. In The 20th International Conference on Distributed Computing Systems
(ICDCS), pp. 169–177.
Krishnamachari, B., Bejar, R., & Wicker, S. (2002). Distributed problem solving and the
boundaries of self-configuration in multi-hop wireless networks. In Hawaii International Conference on System Sciences (HICSS-35).
Krishnamachari, B. (2002). Phase Transitions, Structure, and Compleixty in Wireless Networks. Ph.D. thesis, Cornell University, Ithaca, NY.
Mammen, D. L., & Lesser, V. R. (1998). Problem Structure and Subproblem Sharing in
Multi-Agent Systems. Third International Conference on Multi-Agent Systems, 174–
181.
Merriam-Webster (Ed.). (1995). The Merriam-Webster Dictionary (Home and Office edition). Springfield, IL.
575

Mailler & Lesser

Minton, S., Johnston, M. D., Philips, A. B., & Laird, P. (1992). Minimizing conflicts: A
heuristic repair method for constraint satisfaction and scheduling problems. Artificial
Intelligence, 58 (1-3), 161–205.
Monasson, R., Zecchina, R., Kirkpatrick, S., Selman, B., & Troyansky, L. (1999). Determining computational complexity from characteristic ’phase transitions’. Nature, 400,
133–137.
Rao, V. N., & Kumar, V. (1993). On the efficiency of parallel backtracking. IEEE Transactions on Parallel and Distributed Systems, 4 (4), 427–437.
Sycara, K., Roth, S., Sadeh, N., & Fox, M. (1991). Distributed constrained heuristic search.
IEEE Transactions on Systems, Man, and Cybernetics, 21 (6), 1446–1461.
Sycara, K. (1988). Resolving goal conflicts via negotiation. In Proceedings of the Seventh
National Conference on Artificial Intelligence, pp. 245–250.
Wellman, M., & Walsh, W. (1999). Distributed quiescence detection in multiagent negotiation. In In AAAI-99 Workshop on Negotiation: Settling Conflicts and Identifying
Opportunities.
Werkman, K. J. (1990). Knowledge-based model of negotiation using shared perspectives. In
Proceedings of the 10th International Workshop on Distributed Artificial intelligence,
Bandera, TX.
Yokoo, M. (1994). Weak-commitment search for solving constraint satisfaction problems.
In Proceedings of the 12th National Conference on Artificial Intelligence (AAAI-94);
Vol. 1, pp. 313–318, Seattle, WA, USA. AAAI Press, 1994.
Yokoo, M. (1995). Asynchronous weak-commitment search for solving distributed constraint
satisfaction problems.. In Proceedings of the First International Conference on Principles and Practice of Constraint Programming (CP-95), Lecture Notes in Computer
Science 976, pp. 88–102. Springer-Verlag.
Yokoo, M., Durfee, E. H., Ishida, T., & Kuwabara, K. (1992). Distributed constraint satisfaction for formalizing distributed problem solving. In International Conference on
Distributed Computing Systems, pp. 614–621.
Yokoo, M., & Hirayama, K. (1996). Distributed breakout algorithm for solving distributed
constraint satisfaction problems.. In International Conference on Multi-Agent Systems
(ICMAS).
Yokoo, M., & Hirayama, K. (2000). Algorithms for distributed constraint satisfaction: A
review. Autonomous Agents and Multi-Agent Systems, 3 (2), 198–212.
Yokoo, M., Suzuki, K., & Hirayama, K. (2002). Secure distributed constraint satisfaction: Reaching agreement without revealing private information. In Proceeding of the
Eighth International Conference on Principles and Practice of Constraint Programming (CP).

576

Journal of Artificial Intelligence Research 25 (2006) 349-387

Submitted 06/05; published 03/06

Representing Conversations for Scalable Overhearing
Gery Gutnik
Gal A. Kaminka

gutnikg@cs.biu.ac.il
galk@cs.biu.ac.il

Computer Science Department
Bar Ilan University
Ramat Gan 52900, Israel

Abstract
Open distributed multi-agent systems are gaining interest in the academic community
and in industry. In such open settings, agents are often coordinated using standardized
agent conversation protocols. The representation of such protocols (for analysis, validation, monitoring, etc) is an important aspect of multi-agent applications. Recently, Petri
nets have been shown to be an interesting approach to such representation, and radically
different approaches using Petri nets have been proposed. However, their relative strengths
and weaknesses have not been examined. Moreover, their scalability and suitability for
different tasks have not been addressed. This paper addresses both these challenges. First,
we analyze existing Petri net representations in terms of their scalability and appropriateness for overhearing, an important task in monitoring open multi-agent systems. Then,
building on the insights gained, we introduce a novel representation using Colored Petri
nets that explicitly represent legal joint conversation states and messages. This representation approach offers significant improvements in scalability and is particularly suitable
for overhearing. Furthermore, we show that this new representation offers a comprehensive coverage of all conversation features of FIPA conversation standards. We also present
a procedure for transforming AUML conversation protocol diagrams (a standard humanreadable representation), to our Colored Petri net representation.

1. Introduction
Open distributed multi-agent systems (MAS) are composed of multiple, independently-built
agents that carry out mutually-dependent tasks. In order to allow inter-operability of agents
of different designs and implementation, the agents often coordinate using standardized interaction protocols, or conversations. Indeed, the multi-agent community has been investing
a significant effort in developing standardized Agent Communication Languages (ACL) to facilitate sophisticated multi-agent systems (Finin, Labrou, & Mayfield, 1997; Kone, Shimazu,
& Nakajima, 2000; ChaibDraa, 2002; FIPA site, 2003). Such standards define communicative acts, and on top of them, interaction protocols, ranging from simple queries as to the
state of another agent, to complex negotiations by auctions or bidding on contracts. For
instance, the FIPA Contract Net Interaction Protocol (FIPA Specifications, 2003b) defines
a concrete set of message sequences that allows the interacting agents to use the contract
net protocol for negotiations.
Various formalisms have been proposed to describe such standards (e.g., Smith & Cohen,
1996; Parunak, 1996; Odell, Parunak, & Bauer, 2000, 2001b; AUML site, 2003). In particular, AUML–Agent Unified Modelling Language–is currently used in the FIPA-ACL standards
c
°2006
AI Access Foundation. All rights reserved.

Gutnik & Kaminka

(FIPA Specifications, 2003a, 2003b, 2003c, 2003d; Odell, Parunak, & Bauer, 2001a) 1 . UML
2.0 (AUML site, 2003), a new emerging standard influenced by AUML, has the potential to
become the FIPA-ACL standard (and a forthcoming IEEE standard) in the future. However, for the moment, a large set of FIPA specifications remains formalized using AUML.
While AUML is intended for human readability and visualization, interaction protocols
should ideally be represented in a way that is amenable to automated analysis, validation
and verification, online monitoring, etc.
Lately, there is increasing interest in using Petri nets (Petri Nets site, 2003) in modelling
multi-agent interaction protocols (Cost, 1999; Cost, Chen, Finin, Labrou, & Peng, 1999,
2000; Lin, Norrie, Shen, & Kremer, 2000; Nowostawski, Purvis, & Cranefield, 2001; Purvis,
Hwang, Purvis, Cranefield, & Schievink, 2002; Cranefield, Purvis, Nowostawski, & Hwang,
2002; Ramos, Frausto, & Camargo, 2002; Mazouzi, Fallah-Seghrouchni, & Haddad, 2002;
Poutakidis, Padgham, & Winikoff, 2002). There is broad literature on using Petri nets to
analyze the various aspects of distributed systems (e.g. in deadlock detection as shown by
Khomenco & Koutny, 2000), and there has been recent work on specific uses of Petri nets in
multi-agent systems, e.g., in validation and testing (Desel, Oberweis, & Zimmer, 1997), in
automated debugging and monitoring (Poutakidis et al., 2002), in dynamic interpretation of
interaction protocols (Cranefield et al., 2002; de Silva, Winikoff, & Liu, 2003), in modelling
agents behavior induced by their participation in a conversation (Ling & Loke, 2003) and
in interaction protocols refinement allowing modular construction of complex conversations
(Hameurlain, 2003).
However, key questions remain open on the use of Petri nets for conversation representation. First, while radically different approaches to representation using Petri nets have
been proposed, their relative strengths and weaknesses have not been investigated. Second,
many investigations have only addressed restricted subsets of the features needed in representing complex conversations such as those standardized by FIPA (see detailed discussion
of previous work in Section 2). Finally, no procedures have been proposed for translating
human-readable AUML protocol descriptions into the corresponding machine-readable Petri
nets.
This paper addresses these open challenges in the context of scalable overhearing. Here,
an overhearing agent passively tracks many concurrent conversations involving multiple participants, based solely on their exchanged messages, while not being a participant in any of
the overheard conversations itself (Novick & Ward, 1993; Busetta, Serafini, Singh, & Zini,
2001; Kaminka, Pynadath, & Tambe, 2002; Poutakidis et al., 2002; Busetta, Dona, & Nori,
2002; Legras, 2002; Gutnik & Kaminka, 2004a; Rossi & Busetta, 2004). Overhearing is useful in visualization and progress monitoring (Kaminka et al., 2002), in detecting failures in
interactions (Poutakidis et al., 2002), in maintaining organizational and situational awareness (Novick & Ward, 1993; Legras, 2002; Rossi & Busetta, 2004) and in non-obtrusively
identifying opportunities for offering assistance (Busetta et al., 2001, 2002). For instance, an
overhearing agent may monitor the conversation of a contractor agent engaged in multiple
contract-net protocols with different bidders and bid callers, in order to detect failures.
We begin with an analysis of Petri net representations, with respect to scalability and
overhearing. We classify representation choices along two dimensions affecting scalability:
1. (FIPA Specifications, 2003c) is currently deprecated. However, we use this specification since it describes
many important features needed in modelling multi-agent interactions.

350

Representing Conversations for Scalable Overhearing

(i) the technique used to represent multiple concurrent conversations; and (ii) the choice
of representing either individual or joint interaction states. We show that while the runtime complexity of monitoring conversations using different approaches is the same, choices
along these two dimensions have significantly different space requirements, and thus some
choices are more scalable (in the number of conversations) than others. We also argue that
representations suitable for overhearing require the use of explicit message places, though
only a subset of previously-explored techniques utilized those.
Building on the insights gained, the paper presents a novel representation that uses
Colored Petri nets (CP-nets) in which places explicitly denote messages, and valid joint
conversation states. This representation is particularly suited for overhearing as the number
of conversations is scaled-up. We show how this representation can be used to represent
essentially all features of FIPA AUML conversation standards, including simple and complex interaction building blocks, communicative act attributes such as message guards and
cardinalities, nesting, and temporal aspects such as deadlines and duration.
To realize the advantages of machine-readable representations, such as for debugging
(Poutakidis et al., 2002), existing human-readable protocol descriptions must be converted
to their corresponding Petri net representations. As a final contribution in this paper, we
provide a skeleton semi-automated procedure for converting FIPA conversation protocols
in AUML to Petri nets, and demonstrate its use on a complex FIPA protocol. While this
procedure is not fully automated, it takes a first step towards addressing this open challenge.
This paper is organized as follows. Section 2 presents the motivation for our work.
Sections 3 through 6 then present the proposed representation addressing all FIPA conversation features including basic interaction building blocks (Section 3), message attributes
(Section 4), nested & interleaved interactions (Section 5), and temporal aspects (Section 6).
Section 7 ties these features together: It presents a skeleton algorithm for transforming an
AUML protocol diagram to its Petri net representation, and demonstrates its use on a challenging FIPA conversation protocol. Section 8 concludes. The paper rounds up with three
appendixes. The first provides a quick review of Petri nets. Then, to complete coverage of
FIPA interactions, Appendix B provides additional interaction building blocks. Appendix C
presents a Petri net of a complex conversation protocol, which integrates many of the features
of the developed representation technique.

2. Representations for Scalable Overhearing
Overhearing involves monitoring conversations as they progress, by tracking messages that
are exchanged between participants (Gutnik & Kaminka, 2004a). We are interested in representations that can facilitate scalable overhearing, tracking many concurrent conversations,
between many agents. We focus on open settings, where the complex internal state and control logic of agents is not known in advance, and therefore exclude discussions of Petri net
representations which explicitly model agent internals (e.g., Moldt & Wienberg, 1997; Xu
& Shatz, 2001). Instead, we treat agents as black boxes, and consider representations that
commit only to the agent’s conversation state (i.e., its role and progress in the conversation).
The suitability of a representation for scalable overhearing is affected by several facets.
First, since overhearing is based on tracking messages, the representation must be able to
explicitly represent the passing of a message (communicative act) from one agent to another
351

Gutnik & Kaminka

(Section 2.1). Second, the representation must facilitate tracking of multiple concurrent
conversations. While the tracking runtime is bounded from below by the number of messages
(since in any case, all messages are overheard and processed), space requirements may differ
significantly (see Sections 2.2–2.3).
2.1 Message-monitoring versus state-monitoring
We distinguish two settings for tracking the progress of conversations, depending on the
information available to the tracking agent. In the first type of setting, which we refer to
as state monitoring, the tracking agent has access to the internal state of the conversation
in one or more of the participants, but not necessarily to the messages being exchanged.
The other settings involves message monitoring, where the tracking agent has access only to
the messages being exchanged (which are externally observable), but cannot directly observe
the internal state of the conversation in each participant. Overhearing is a form of message
monitoring.
Representations that support state monitoring use places to denote the conversation
states of the participants. Tokens placed in these places (the net marking) denote the
current state. The sending or receiving of a message by a participant is not explicitly
represented, and is instead implied by moving tokens (through transition firings) to the new
state places. Thus, such a representation essentially assumes that the internal conversation
state of participants is directly observable by the monitoring agent. Previous work utilizing
state monitoring includes work by Cost (1999), Cost et al. (1999, 2000), Lin et al. (2000),
Mazouzi et al. (2002), Ramos et al. (2002).
The representation we present in this paper is intended for overhearing tasks, and cannot
assume that the conversation states of overheard agents are observable. Instead, it must
support message monitoring, where in addition to using tokens in state places (to denote
current conversation state), the representation uses message places, where tokens are placed
when a corresponding message is overheard. A conversation-state place and a message
place are connected via a transition to a state place denoting the new conversation state.
Tokens placed in these originating places–indicating a message was received at an appropriate
conversation state–will cause the transition to fire, and for the tokens to be placed in the
new conversation state place. Thus the new conversation state is inferred from "observing"
a message. Previous investigations, that have used explicit message places, include work
by Cost (1999), Cost et al. (1999, 2000), Nowostawski et al. (2001), Purvis et al. (2002),
Cranefield et al. (2002), Poutakidis et al. (2002)2 . These are discussed in depth below.
2.2 Representing a Single Conversation
Two representation variants are popular within those that utilize conversation places (in
addition to message places): Individual state representations use separate places and tokens
for the state of each participant (each role). Thus, the overall state of the conversation is
represented by different tokens marking multiple places. Joint state representations use a
single place for each joint conversation state of all participants. The placement of a token
2. Cost (1999), Cost et al. (1999, 2000) present examples of both state- and message- monitoring representations.

352

Representing Conversations for Scalable Overhearing

within such a place represents the overhearing agent’s belief that the participants are in the
appropriate joint state.
Most previous representations use individual states. In these, different markings distinguish a conversation state where one agent has sent a message, from a state where the other
agent received it. The net for each conversation role is essentially built separately, and is
merged with the other nets, or connected to them via fusion places or similar means.
Cost (1999), Cost et al. (1999, 2000) have used CP-nets with individual state places for
representing KQML and FIPA interaction protocols. Transitions represent message events,
and CP-net features, such as token colors and arc expressions, are used to represent AUML
message attributes and sequence expressions. The authors also point out that deadlines (a
temporal aspect of interaction) can be modelled, but no implementation details are provided.
Cost (1999) also proposed using hierarchical CP-nets to represent hierarchical multi-agent
conversations.
Purvis et al. (2002), Cranefield et al. (2002) represented conversation roles as separate
CP-nets, where places denote both interaction messages and states, while transitions represent operations performed on the corresponding communicative acts such as send, receive,
and process. Special in/out places are used to pass net tokens between the different CP-nets,
through special get/put transitions, simulating the actual transmission of the corresponding
communicative acts.
In principle, individual-state representations require two places in each role, for every
message. For a given message, there would be two individual places for the sender (before
sending and after sending), and similarly two more for each receiver (before receiving and
after receiving). All possible conversation states–valid or not–can be represented. For a
single message and two roles, there are two places for each role (four places total), and four
possible conversation states: message sent and received, sent and not received, not sent but
incorrectly believed to have been received, not sent and not received. These states can be
represented by different markings. For instance, a conversation state where the message has
been sent but not received is denoted by a token in the ’after-sending’ place of the sender
and another token in the ’before-receiving’ place of the receiver. This is summarized in the
following proposition:
Proposition 1 Given a conversation with R roles and a total of M possible messages, an
individual state representation has space complexity of O(M R).
While the representations above all represent each role’s conversation state separately,
many applications of overhearing only require representation of valid conversation states
(message not sent and not received, or sent and received). Indeed, specifications for interaction protocols often assume the use of underlying synchronization protocols to guarantee
delivery of messages (Paurobally & Cunningham, 2003; Paurobally, Cunningham, & Jennings, 2003). Under such an assumption, for every message, there are only two joint states
regardless of the number of roles. For example, for a single message and three roles–a
sender and two receivers, there are two places and two possible markings: A token in a
before sending/receiving place represents a conversation state where the message has not
yet been sent by the sender (and the two receivers are waiting for it), while a token in a
after sending/receiving place denotes that the message has been sent and received by both
receivers.
353

Gutnik & Kaminka

Nowostawski et al. (2001) utilize CP-nets where places denote joint conversation states.
They also utilize places representing communicative acts. Poutakidis et al. (2002) proposed
a representation based on Place-Transition nets (PT-nets)–a more restricted representation
of Petri nets that has no color. They presented several interaction building blocks, which
could then fit together to model additional conversation protocols. In general, the following
proposition holds with respect to such representations:
Proposition 2 Given a conversation with R roles and a total of M possible messages, a
joint state representation that represents only legal states has space complexity of O(M ).
The condition of representing only valid states is critical to the complexity analysis. If all
joint conversation states–valid and invalid–are to be represented, the space complexity would
be O(M R ). In such a case, an individual-state representation would have an advantage. This
would be the case, for instance, if we do not assume the use of synchronization protocols,
e.g., where the overhearing agent may wish to track the exact system state even while a
message is underway (i.e., sent and not yet received).
2.3 Representing Multiple Concurrent Conversations
Propositions 1 and 2 above address the space complexity of representing a single conversation. However, in large scale systems an overhearing agent may be required to monitor
multiple conversations in parallel. For instance, an overhearing agent may be monitoring a
middle agent that is carrying multiple parallel instances of a single interaction protocol with
multiple partners, e.g., brokering (FIPA Specifications, 2003a).
Some previous investigations propose to duplicate the appropriate Petri net representation for each monitored conversation (Nowostawski et al., 2001; Poutakidis et al., 2002). In
this approach, every conversation is tracked by a separate Petri-net, and thus the number
of Petri nets (and their associated tokens) grows with the number of conversations (Proposition 3). For instance, Nowostawski et al. (2001) shows an example where a contract-net
protocol is carried out with three different contractors, using three duplicate CP-nets. This
is captured in the following proposition:
Proposition 3 A representation that creates multiple instances of a conversation Petri net
to represent C conversations, requires O(C) net structures, and O(C) bits for all tokens.
Other investigations take a different approach, in which a single CP-net structure is used
to monitor all conversations of the same protocol. The tokens associated with conversations
are differentiated by their token color (Cost, 1999; Cost et al., 1999, 2000; Lin et al., 2000;
Mazouzi et al., 2002; Cranefield et al., 2002; Purvis et al., 2002; Ramos et al., 2002). For
example, by assigning each token a color of the tuple type hsender, receiveri, an agent can
differentiate multiple tokens in the same place and thus track conversations of different pairs
of agents3 . Color tokens use multiple bits per token; up to log C bits are required to differentiate C conversations. Therefore, the number of bits required to track C conversations
using C tokens is C log C. This leads to the following proposition.
3. See Section 4 to distinguish between different conversations by the same agents.

354

Representing Conversations for Scalable Overhearing

Proposition 4 A representation that uses color tokens to represent C multiple instances of
a conversation, requires O(1) net structures, and O(C log C) bits for all tokens.

Due to the constants involved, the space requirements of Proposition 3 are in practice
much more expensive than those of Proposition 4. Proposition 3 refers to the creation of
O(C) Petri networks, each with duplicated place and transition data structures. In contrast,
Proposition 4 refers to bits required for representing C color tokens on a single CP net.
Moreover, in most practical settings, a sufficiently large constant bound on the number of
conversations may be found, which will essentially reduce the O(log C) factor to O(1).
Based on Propositions 1–4, it is possible to make concrete predictions as to the scalability
of different approaches with respect to the number of conversations, roles. Table 1 shows
the space complexity of different approaches when modelling C conversations of the same
protocol, each with a maximum of R roles, and M messages, under the assumption of
underlying synchronization protocols. The table also cites relevant previous work.

Individual
States
(Proposition 1)
Joint
States
(Proposition 2)

Representing Multiple Conversations (of Same Protocol)
Multiple CP- or PT-nets
Using color tokens, single CP-net
(Proposition 3)
(Proposition 4)
Space: O(M R + C log C)
Cost (1999), Cost et al. (1999, 2000),
Space: O(M RC)
Lin et al. (2000), Cranefield et al. (2002),
Purvis et al. (2002), Ramos et al. (2002),
Mazouzi et al. (2002)
Space: O(M C)
Space: O(M + C log C)
Nowostawski et al. (2001),
This paper
Poutakidis et al. (2002)
Table 1: Scalability of different representations

Building on the insights gained from Table 1, we propose a representation using CP-nets
where places explicitly represent joint conversation states (corresponding to the lower-right
cell in Table 1), and tokens color is used to distinguish concurrent conversations (as in the
upper-right cell in Table 1). As such, it is related to the works that have these features, but
as the table demonstrates, is a novel synthesis.
Our representation uses similar structures to those found in the works of Nowostawski
et al. (2001) and Poutakidis et al. (2002). However, in contrast to these previous investigations, we rely on token color in CP-nets to model concurrent conversations, with space
complexity O(M + C log C). We also show (Sections 3–6) how it can be used to cover a
variety of conversation features not covered by these investigations. These features include
representation of a full set of FIPA interaction building blocks, communicative act attributes
(such as message guards, sequence expressions, etc.), compact modelling of concurrent conversations, nested and interleaved interactions, and temporal aspects.
355

Gutnik & Kaminka

3. Representing Simple & Complex Interaction Building Blocks
This section introduces the fundamentals of our representation, and demonstrates how various simple and complex AUML interaction messages, used in FIPA conversation standards
(FIPA Specifications, 2003c), can be implemented using the proposed CP-net representation. We begin with a simple conversation, shown in Figure 1-a using an AUML protocol
diagram. Here, agent1 sends an asynchronous message msg to agent2 .




	






	



	

 	




	

 	

(a) AUML representation



    
     !"  
# $ %&' (

%)' 
  * "  # $

 ' ( ' 
%
+
  ' 

(b) CP-net representation

Figure 1: Asynchronous message interaction.
To represent agent conversation protocols, we define two types of places, corresponding
to messages and conversation states. The first type of net places, called message places, is
used to describe conversation communicative acts. Tokens placed in message places indicate
that the associated communicative act has been overheard. The second type of net places,
agent places, is associated with the valid joint conversation states of the interacting agents.
Tokens placed in agent places indicate the current joint state of the conversation within the
interaction protocol.
Transitions represent the transmission and receipt of communicative acts between agents.
Assuming underlying synchronization protocols, a transition always originates within a jointstate place and a message place, and targets a joint conversation state (more than one is
possible–see below). Normally, the current conversation state is known (marked with a
token), and must wait the overhearing of the matching message (denoted with a token at
the connected message place). When this token is marked, the transition fires, automatically
marking the new conversation state.
Figure 1-b presents CP net representation of the earlier example of Figure 1-a. The CPnet in Figure 1-b has three places and one transition connecting them. The A1 B1 and the
A2 B2 places are agent places, while the msg place is a message place. The A and B capital
letters are used to denote the agent1 and the agent2 individual interaction states respectively
(we have indicated the individual and the joint interaction states over the AUML diagram
in Figure 1-a, but omit these annotations in later figures). Thus, the A1 B1 place indicates a
joint interaction state where agent1 is ready to send the msg communicative act to agent2
(A1 ) and agent2 is waiting to receive the corresponding message (B1 ). The msg message
place corresponds to the msg communicative act sent between the two agents. Thus, the
transmission of the msg communicative act causes the agents to transition to the A2 B2
356

Representing Conversations for Scalable Overhearing

place. This place corresponds to the joint interaction state in which agent1 has already sent
the msg communicative act to agent2 (A2 ) and agent2 has received it (B2 ).
The CP-net implementation in Figure 1-b also introduces the use of token colors to
represent additional information about interaction states and communicative acts. The
token color sets are defined in the net declaration, i.e. the dashed box in Figure 1-b.
The syntax follows the standard CPN ML notation (Wikstrom, 1987; Milner, Harper, &
Tofte, 1990; Jensen, 1997a). The AGEN T color identifies the agents participating in the
interaction, and is used to construct the two compound color sets.
The INTER-STATE color set is associated with agent places, and represents agents in
the appropriate joint interaction states. It is a record ha1 , a2 i, where a1 and a2 are AGEN T
color elements distinguishing the interacting agents. We apply the INTER-STATE color
set to model multiple concurrent conversations using the same CP-net. The second color
set is M SG, describing interaction communicative acts and associated with message places.
The M SG color token is a record has , ar i, where as and ar correspond to the sender and
the receiver agents of the associated communicative act. In both cases, additional elements,
such as conversation identification, may be used. See Section 4 for additional details.
In Figure 1-b, the A1 B1 and the A2 B2 places are associated with the INTER-STATE
color set, while the msg place is associated with the M SG color set. The place color set
is written in italic capital letters next to the corresponding place. Furthermore, we use
the s and r AGEN T color type variables to denote the net arc expressions. Thus, given
that the output arc expression of both the A1 B1 and the msg places is hs, ri, the s and r
elements of the agent place token must correspond to the s and r elements of the message
place token. Consequently, the net transition occurs if and only if the agents of the message
correspond to the interacting agents. The A2 B2 place input arc expression is hr, si following
the underlying intuition that agent2 is going to send the next interaction communicative
act.
Figure 2-a shows an AUML representation of another interaction building block, synchronous message passing, denoted by the filled solid arrowhead. Here, the msg communicative act is sent synchronously from agent1 to agent2 , meaning that an acknowledgement
on msg communicative act must always be received by agent1 before the interaction may
proceed.
The corresponding CP-net representation is shown in Figure 2-b. The interaction starts
in the A1 B1 place and terminates in the A2 B2 place. The A1 B1 place represents a joint
interaction state where agent1 is ready to send the msg communicative act to agent2 (A1 )
and agent2 is waiting to receive the corresponding message (B1 ). The A2 B2 place denotes
a joint interaction state, in which agent1 has already sent the msg communicative act to
agent2 (A2 ) and agent2 has received it (B2 ). However, since the CP-net diagram represents
synchronous message passing, the msg communicative act transmission cannot cause the
agents to transition directly from the A1 B1 place to the A2 B2 place. We therefore define an
intermediate A01 B10 agent place. This place represents a joint interaction state where agent2
has received the msg communicative act and is ready to send an acknowledgement on it
(B1 ’), while agent1 is waiting for that acknowledgement (A01 ). Taken together, the msg
communicative act causes the agents to transition from the A1 B1 place to the A01 B10 place,
while the acknowledgement on the msg message causes the agents to transition from the
A01 B10 place to the A2 B2 place.
357

Gutnik & Kaminka

	

 	










 
   
	


 	

 

 

	

 	


 




   !"
  #$%  & '
 ( ) ( "
  * %  & '  ( ) ( "
+  ( "

(a) AUML representation

(b) CP-net representation

Figure 2: Synchronous message interaction.
Transitions in a typical multi-agent interaction protocols are composed of interaction
building blocks, two of which have been presented above. Additional interaction buildingblocks, which are fairly straightforward (or have appeared in previous work, e.g., Poutakidis
et al., 2002) are presented in Appendix B. In the remainder of this section, we present two
complex interactions building blocks that are generally common in multi-agent interactions:
XOR-decision and OR-parallel.
We begin with the XOR-decision interaction. The AUML representation to this building
block is shown in Figure 3-a. The sender agent agent1 can either send message msg1 to
agent2 or message msg2 to agent3 , but it can not send both msg1 and msg2 . The non-filled
diamond with an ’x’ inside is the AUML notation for this constraint.
 



	







	


  

	
 

!"#$  %&
  
'()' (
"#$#*%&





!



 
'()' (
)'+(
 

 ,$  %&
() (
	
	
-' . / (



  
 





(a) AUML representation

(b) CP-net representation

Figure 3: XOR-decision messages interaction.
Figure 3-b shows the corresponding CP-net. Again, the A, B and C capital letters
are used to denote the interaction states of agent1 , agent2 and agent3 , respectively. The
358

Representing Conversations for Scalable Overhearing

interaction starts from the A1 B1 C1 place and terminates either in the A2 B2 place or in the
A2 C2 place. The A1 B1 C1 place represents a joint interaction state where agent1 is ready to
send either the msg1 communicative act to agent2 or the msg2 communicative act to agent3
(A1 ); and agent2 and agent3 are waiting to receive the corresponding msg1 /msg2 message
(B1 /C1 ). To represent the A1 B1 C1 place color set, we extend the INTER-STATE color
set to denote a joint interaction state of three interacting agents, i.e. using the INTERSTATE-3 color set. The msg1 communicative act causes the agents to transition to A2 B2
place. The A2 B2 place represents a joint interaction state where agent1 has sent the msg1
message (A2 ), and agent2 has received it (B2 ). Similarly, the msg2 communicative act causes
agents agent1 and agent3 to transition to A2 C2 place. Exclusiveness is achieved since the
single agent token in A1 B1 C1 place can be used either for activating the A1 B1 C1 → A2 B2
transition or for activating the A1 B1 C1 → A2 C2 transition, but not both.
A similar complex interaction is the OR-parallel messages interaction. Its AUML representation is presented in Figure 4-a. The sender agent, agent1 , can send message msg1 to
agent2 or message msg2 to agent3 , or both. The non-filled diamond is the AUML notation
for this constraint.

	

		


 



	









  
 
   
  	

  
	 	

 	

 
		

 

	

 		


(a) AUML representation

  !"
 #$%& '(
)*+) *"
 #$%&%, '(
)*+) *
+)-*"
 . & '(
*+*"
  /) 01 *"

	

		
 

(b) CP-net representation

Figure 4: OR-parallel messages interaction.
Figure 4-b shows the CP-net representation of the OR-parallel interaction. The interaction starts from the A1 B1 C1 place but it can be terminated in the A2 B2 place, or in the
A2 C2 place, or in both. To represent this inclusiveness of the interaction protocol, we define
two intermediate places, the A01 B1 place and the A001 C1 place. The A01 B1 place represents a
joint interaction state where agent1 is ready to send the msg1 communicative act to agent2
(A01 ) and agent2 is waiting to receive the message (B1 ). The A001 C1 place has similar meaning, but with respect to agent3 . As normally done in Petri nets, the transition connecting
the A1 B1 C1 place to the intermediate places duplicates any single token in A1 B1 C1 place
into two tokens going into the A01 B1 and the A001 C1 places. Consequently, the two parts of
the OR-parallel interaction can be independently executed.

4. Representing Interaction Attributes
We now extend our representation to allow additional interaction aspects, useful in describing multi-agent conversation protocols. First, we show how to represent interaction
359

Gutnik & Kaminka

message attributes, such as guards, sequence expressions, cardinalities and content (FIPA
Specifications, 2003c). We then explore in depth the representation of multiple concurrent
conversations (on the same CP net).
Figure 5-a shows a simple agent interaction using an AUML protocol diagram. This
interaction is similar to the one presented in Figure 1-a in the previous section. However,
Figure 5-a uses an AUML message guard-condition–marked as [condition]–that has the
following semantics: the communicative act is sent from agent1 to agent2 if and only if the
condition is true.



 


	

 	



	
  

	

 	

(a) AUML representation

  ! " #$
 !%& " #$

 '( ! ! " #$
  ) !*+,! ! " -
  
./0  !1 .2 0  !$
   3 , " -
 0  !1 0  !1
  
 0!%&10'( ! !$
4. 0  !$ 4.  0!%&$

4. 0'( ! !$
(b) CP-net representation

Figure 5: Message guard-condition
The guard-condition implementation in our Petri net representation uses transition
guards (Figure 5-b), a native feature for CP nets. The AUML guard condition is mapped
directly to the CP-net transition guard. The CP-net transition guard is indicated on the
net inscription next to the corresponding transition using square brackets. The transition
guard guarantees that the transition is enabled if and only if the transition guard is true.
In Figure 5-b, we also extend the color of tokens to include information about the
communicative act being used and its content. We extend the M SG color set definition
to a record hs, r, t, ci, where the s and r elements has the same interpretation as in previous
section (sender and receiver), and the t and c elements define the message type and content,
respectively. The t element is of a new color T Y P E, which determines communicative act
types. The c element is of a new color CON T EN T , which represents communicative act
content and argument list (e.g. reply-to, reply-by and etc).
The addition of new elements also allows for additional potential uses. For instance,
to facilitate representation of multiple concurrent conversations between the same agents
(s and r), it is possible to add a conversation identification field to both the M SG and
INTER-STATE colors. For simplicity, we refrain from doing so in the examples in this
paper.
Two additional AUML communicative act attributes that can be modelled in the CP
representation are message sequence-expression and message cardinality. The sequenceexpressions denote a constraint on the message sent from sender agent. There are a number of
sequence-expressions defined by FIPA conversation standards (FIPA Specifications, 2003c):
m denotes that the message is sent exactly m times; n..m denotes that the message is sent
anywhere from n up to m times; ∗ denotes that the message is sent an arbitrary number of
360

Representing Conversations for Scalable Overhearing

times. An additional important sequence expression is broadcast, i.e. message is sent to all
other agents.
We now explain the representation of sequence-expressions in CP-nets, using broadcast
as an example (Figure 6-b). Other sequence expressions are easily derived from this example.
We define an INTER-STATE-CARD color set. This color set is a tuple (ha1 , a2 i, i) consisting
of two elements. The first tuple element is an INTER-STATE color element, which denotes
the interacting agents as previously defined. The second tuple element is an integer that
counts the number of messages already sent by an agent, i.e. the message cardinality.
This element is initially assigned to 0. The INTER-STATE-CARD color set is applied to
the S1 R1 place, where the S and R capital letters are used to denote the sender and the
receiver individual interaction states respectively and the S1 R1 indicates the initial joint
interaction state of the interacting agents. The two additional colors, used in Figure 6-b, are
the BROADCAST-LIST and the T ARGET colors. The BROADCAST-LIST color defines
the sender broadcast list of the designated receivers, assuming that the sender must have
such a list to carry out its role. The T ARGET color defines indexes into this broadcast list.

  

 

 	


 	

 
 
!!
	



 	
 



'()(  * + ,'()( ./ + ,'()( 01 * * + ,'()( 2* 3  + %'(4 56 *7
56 *# $%&
'()( 08 + 9: 
  '()( 2* 3  30 8 + ; ( 4<':
 " " 
2* 3  7 08	


'()( =  + %'(4  6 *7 6 *7
 	
:6./ 7' 601 * * 

 '()( > 180  2  +  * @ :A,3?
B 5 ) $% + , '()(   + 94%C > 180 3? 2 
	

@ :A !DDD$%3  B 5  6 *- B 5   6= - B 5  608 	

(a) AUML representation

(b) CP-net representation

Figure 6: Broadcast sequence expression.
According to the broadcast sequence-expression semantics, the sender agent sends the
same msg1 communicative act to all the receivers on the broadcast list. The CP-net introduced in Figure 6-b models this behavior.4 The interaction starts from the S1 R1 place,
representing the joint interaction state where sender is ready to send the msg1 communicative act to receiver (S1 ) and receiver is waiting to receive the corresponding msg1
message (R1 ). The S1 R1 place initial marking is a single token, set by the initialization expression (underlined, next to the corresponding place). The initialization expression 1‘(hs, T ARGET (0)i, 0)–given in standard CPN ML notation–determines that the S1 R1
place’s initial marking is a multi-set containing a single token (hs, T ARGET (0)i, 0). Thus,
the first designated receiver is assigned to be the agent with index 0 on the broadcast list,
and the message cardinality counter is initiated to 0.
4. We implement broadcast as an iterative procedure sending the corresponding communicative act separately to all designated recipients.

361

Gutnik & Kaminka

The msg1 message place initially contains multiple tokens. Each of these tokens represents the msg1 communicative act addressed to a different designated receiver on the
broadcast list. In Figure 6-b, the initialization expression, corresponding to the msg1 message place, has been omitted. The S1 R1 place token and the appropriate msg1 place token
together enable the corresponding transition. Consequently, the transition may fire and thus
the msg1 communicative act transmission is simulated.
The msg1 communicative act is sent incrementally to every designated receiver on the
broadcast list. The incoming arc expression (hs, ri, i) is incremented by the transition to
the outgoing (hs, T ARGET (i + 1)i, i + 1) arc expression, causing the receiver agent with
index i + 1 on the broadcast list to be selected. The transition guard constraint i < size,
i.e. i < |broadcast list|, ensures that the msg1 message is sent no more than |broadcast list|
times. The msg1 communicative act causes the agents to transition to the S2 R2 place.
This place represents a joint interaction state in which sender has already sent the msg1
communicative act to receiver and is now waiting to receive the msg2 message (S2 ) and
receiver has received the msg1 message and is ready to send the msg2 communicative act
to sender (R2 ). Finally, the msg2 message causes the agents to transition to the S3 R3
place. The S3 R3 place denotes a joint interaction state where sender has received the msg2
communicative act from receiver and terminated (S3 ), while receiver has already sent the
msg2 message to sender and terminated as well (R3 ).
We use Figure 6-b to demonstrate the use of token color to represent multiple concurrent
conversations using the same CP-net. For instance, let us assume that the sender agent is
called agent1 and its broadcast list contains the following agents: agent2 , agent3 , agent4 ,
agent5 and agent6 . We will also assume that the agent1 has already sent the msg1 communicative act to all agents on the broadcast list. However, it has only received the msg2
reply message from agent3 and agent6 . Thus, the CP-net current marking for the complete
interaction protocol is described as follows: the S2 R2 place is marked by hagent2 , agent1 i,
hagent4 , agent1 i, hagent5 , agent1 i, while the S3 R3 place contains the tokens hagent1 , agent3 i
and hagent1 , agent6 i.
An Example. We now construct a CP-net representation of the FIPA Query Interaction
Protocol (FIPA Specifications, 2003d), shown in AUML form in Figure 7, to demonstrate
how the building blocks presented in Sections 3 and 4 can be put together. In this interaction
protocol, the Initiator requests the P articipant to perform an inform action using one of two
query communicative acts, query-if or query-ref. The P articipant processes the query and
makes a decision whether to accept or ref use the query request. The Initiator may request
the P articipant to respond with either an accept or ref use message, and for simplicity,
we will assume that this is always the case. In case the query request has been accepted,
the P articipant informs the Initiator on the query results. If the P articipant fails, then
it communicates a f ailure. In a successful response, the P articipant replies with one of
two versions of inform (inform-t/f or inform-result) depending on the type of initial query
request.
The CP-net representation of the FIPA Query Interaction Protocol is presented in Figure 8. The interaction starts in the I1 P1 place (we use the I and the P capital letters
to denote the Initiator and the P articipant roles). The I1 P1 place represents a joint
interaction state where (i) the Initiator agent is ready to send either the query-if communicative act, or the query-ref message, to P articipant (I1 ); and (ii) P articipant is wait362

Representing Conversations for Scalable Overhearing

       

  

   	 


   


    
  
  

 
     
  
  



    

 

    

   

Figure 7: FIPA Query Interaction Protocol - AUML representation.

ing to receive the corresponding message (P1 ). The Initiator can send either a query-if
or a query-ref communicative act. We assume that these acts belong to the same class,
the query communicative act class. Thus, we implement both messages using a single
Query message place, and check the message type using the following transition guard:
[#t msg = query-if or #t msg = query-ref]. The query communicative act causes the
interacting agents to transition to the I2 P2 place. This place represents a joint interaction
state in which Initiator has sent the query communicative act and is waiting to receive
a response message (I2 ), and P articipant has received the query communicative act and
deciding whether to send an agree or a ref use response message to Initiator (P2 ). The
ref use communicative act causes the agents to transition to I3 P3 place, while the agree
message causes the agents to transition to I4 P4 place.
The P articipant decision on whether to send an agree or a ref use communicative
act is represented using the XOR-decision building block introduced earlier (Figure 3-b).
The I3 P3 place represents a joint interaction state where Initiator has received a ref use
communicative act and terminated (I3 ) and P articipant has sent a ref use message and
terminated as well (P3 ). The I4 P4 place represents a joint interaction state in which Initiator
has received an agree communicative act and is now waiting for further response from
363

Gutnik & Kaminka

+

+


 

	 




 
 


3  ? 6 ( 9  6  



, B

 

 

  

   ! "


 @A

     !#

' " ("  ) *+,  -.

   
$!  

	 
 
 
 





%

 

 

 
 





%
 

   
	 


' " (" 23 +, *+,  -.


' " ("



 7,/  *8 ' 723 +, * +, .



 

 
 


+, *$4,, *   ' "5 6 7 ) *+,8

6 7 ) *+, .
%
' " (" 9 4 )  ' "5  7 ) *+, 87 ) *+,8

 

: 6  7 ) *+, .

	 





' " (" ,/ *     ! 0  ! 0111.

  

&


&

: 6  79 4 ) .

: 6  7,/ * .

3 ;,

?!" 
 



 





>6 (

 



C  ?!"  D! 6 ?5     ! E
   

 
 


   
	 


	 

=



=

3 ;,

 
 



< <

" C   ?! "     ( 6 ?5
   ! E#

3 ;,

Figure 8: FIPA Query Interaction Protocol - CP-net representation.

P articipant (I4 ) and P articipant has sent an agree message and is now deciding which
response to send to Initiator (P4 ). At this point, the P articipant agent may send one
of the following communicative acts: inform-t/f, inform-result and f ailure. The choice is
represented using another XOR-decision building block, where the inform-t/f and informresult communicative acts are represented using a single Inf orm message place. The f ailure
communicative act causes a transition to the I5 P5 place, while the inf orm message causes
a transition to the I6 P6 place. The I5 P5 place represents a joint interaction state where
P articipant has sent a f ailure message and terminated (P5 ), while Initiator has received
a f ailure and terminated (I5 ). The I6 P6 place represents a joint interaction state in which
P articipant has sent an inf orm message and terminated (P6 ), while Initiator has received
an inf orm and terminated (I6 ).
The implementation of the [query-if ] and the [query-ref ] message guard conditions requires a detailed discussion. These are not implemented in a usual manner in view of the fact
that they depend on the original request communicative act. Thus, we create a special intermediate place that contains the original message type marked "Original M essage T ype"
in the figure. In case an inf orm communicative act is sent, the transition guard verifies
that the inf orm message is appropriate to the original query type. Thus, an inform-t/f
communicative act can be sent only if the original query type has been query-if and an
inform-result message can be sent only if the original query type has been query-ref.
364

Representing Conversations for Scalable Overhearing

5. Representing Nested & Interleaved Interactions
In this section, we extend the CP-net representation of previous sections to model nested
and interleaved interaction protocols. We focus here on nested interaction protocols. Nevertheless, the discussion can also be addressed to interleaved interaction protocols in a similar
fashion.
FIPA conversation standards (FIPA Specifications, 2003c) emphasize the importance of
nested and interleaved protocols in modelling complex interactions. First, this allows reuse of interaction protocols in different nested interactions. Second, nesting increases the
readability of interaction protocols.
The AUML notation annotates nested and interleaved protocols as round corner rectangles (Odell et al., 2001a; FIPA Specifications, 2003c). Figure 9-a shows an example of
a nested protocol5 , while Figure 9-b illustrates an interleaved protocol. Nested protocols
have one or more compartments. The first compartment is the name compartment. The
name compartment holds the (optional) name of the nested protocol. The nested protocol
name is written in the upper left-hand corner of the rectangle, i.e. commitment in Figure 9a. The second compartment, the guard compartment, holds the (optional) nested protocol
guard. The guard compartment is written in the lower left-hand corner of the rectangle, e.g.
[commit] in Figure 9-a. Nested protocols without guards are equivalent to nested protocols
with the [true] guard.
 

 

 

	 

			 




 



	
	

 

	
 	

 



(a) Nested protocol

(b) Interleave protocol

Figure 9: AUML nested and interleaved protocols examples.
Figure 10 describes the implementation of the nested interaction protocol presented in
Figure 9-a by extending the CP-net representation to using hierarchies, relying on standard CP-net methods (see Appendix A). The hierarchical CP-net representation contains
three elements: a superpage, a subpage and a page hierarchy graph. The CP-net superpage
represents the main interaction protocol containing a nested interaction, while the CP-net
subpage models the corresponding nested interaction protocol, i.e. the Commitment Inter5. Figure 9-a appears in FIPA conversation standards (FIPA Specifications, 2003c). Nonetheless, note that
the request-good and the request-pay communicative acts are not part of the FIPA-ACL standards.

365

Gutnik & Kaminka

action Protocol. The page hierarchy graph describes how the superpage is decomposed into
subpages.

#      




	


       

   	 
 
	
 	 	

  	   
 

#    




  

      

  
    

	


 


	

 

 

!"

Figure 10: Nested protocol implementation using hierarchical CP-nets.
Let us consider in detail the process of modelling the nested interaction protocol in
Figure 9-a using a hierarchical CP-net, resulting in the net described in Figure 10. First, we
identify the starting and ending points of the nested interaction protocol. The starting point
of the nested interaction protocol is where Buyer1 sends a Request-Good communicative act
to Seller1 . The ending point is where Buyer1 receives a Request-Pay communicative act
from Seller1 . We model these nested protocol end-points as CP-net socket nodes on the
superpage, i.e. M ain Interaction P rotocol: B11 S11 and Request-Good are input socket
nodes and B13 S13 is an output socket node.
The nested interaction protocol, the Commitment Interaction P rotocol, is represented
using a separate CP-net, following the principles outlined in Sections 3 and 4. This net
is a subpage of the main interaction protocol superpage. The nested interaction protocol
starting and ending points on the subpage correspond to the net port nodes. The B1 S1 and
Request-Good places are the subpage input port nodes, while the B3 S3 place is an output
port node. These nodes are tagged with the IN/OUT port type tags correspondingly.
Then, a substitution transition, which is denoted using HS (Hierarchy and Substitution), connects the corresponding socket places on the superpage. The substitution transition conceals the nested interaction protocol implementation from the net superpage, i.e.
the M ain Interaction P rotocol. The nested protocol name and guard compartments are
mapped directly to the substitution transition name and guard respectively. Consequently,
in Figure 10 we define the substitution transition name as Commitment and the substitution
guard is determined to be [commit].
The superpage and subpage interface is provided using the hierarchy inscription. The
hierarchy inscription is indicated using the dashed box next to the substitution transition. The first line in the hierarchy inscription determines the subpage identity, i.e. the
366

Representing Conversations for Scalable Overhearing

Commitment Interaction P rotocol in our example. Moreover, it indicates that the substitution transition replaces the corresponding subpage detailed implementation on the superpage. The remaining hierarchy inscription lines introduce the superpage and subpage port
assignment. The port assignment relates a socket node on the superpage with a port node
on the subpage. The substitution transition input socket nodes are related to the IN-tagged
port nodes. Analogously, the substitution transition output socket nodes correspond to the
OUT-tagged port nodes. Therefore, the port assignment in Figure 10 assigns the net socket
and port nodes in the following fashion: B11 S11 to B1 S1 , Request-Good to Request-Good
and B13 S13 to B3 S3 .
Finally, the page hierarchy graph describes the decomposition hierarchy (nesting) of
the different protocols (pages). The CP-net pages, the M ain Interaction P rotocol and
the Commitment Interaction P rotocol, correspond to the page hierarchy graph nodes
(Figure 10). The arc inscription indicates the substitution transition, i.e. Commitment.

6. Representing Temporal Aspects of Interactions
Two temporal interaction aspects are specified by FIPA (FIPA Specifications, 2003c). In
this section, we show how timed CP-nets (see also Appendix A) can be applied for modelling
agent interactions that involve temporal aspects, such as interaction duration, deadlines for
message exchange, etc.
A first aspect, duration, is the interaction activity time period. Two periods can be
distinguished: transmission time and response time. The transmission time indicates the
time interval during which a communicative act, is sent by one agent and received by the
designated receiver agent. The response time period denotes the time interval in which
the corresponding receiver agent is performing some task as a response to the incoming
communicative act.
The second temporal aspect is deadlines. Deadlines denote the time limit by which
a communicative act must be sent. Otherwise, the corresponding communicative act is
considered to be invalid. These issues have not been addressed in previous investigations
related to agent interactions modelling using Petri nets.6
We propose to utilize timed CP-nets techniques to represent these temporal aspects of
agent interactions. In doing so, we assume a global clock.7 We begin with deadlines. Figure 11-a introduces the AUML representation of message deadlines. The deadline keyword
is a variation of the communicative act sequence expressions described in Section 4. It
sets a time constraint on the start of the transmission of the associated communicative act.
In Figure 11-a, agent1 must send the msg communicative act to agent2 before the defined
deadline. Once the deadline expires, the msg communicative act is considered to be invalid.
Figure 11-b shows a timed CP-net implementation of the deadline sequence expression.
The timed CP-net in Figure 11-b defines an additional MSG-TIME color set associated with
the net message places. The MSG-TIME color set extends the M SG color set, described in
Section 4, by adding a time stamp attribute to the message token. Thus, the communicative
6. Cost et al. (1999, 2000) mention deadlines without presenting any implementation details.
7. Implementing it, we can use the private clock of an overhearing agent as the global clock for our Petri
net representation. Thus, the time stamp of the message is the overhearer’s time when the corresponding
message was overheard.

367

Gutnik & Kaminka



 


	
	


! ! "#$ % &'
! ! ()# % &'
! ! *+ $#$ % &'
! ! ,$#-./ # % !


	

01 "#$23 1 "#$'

 	
	 ! ! 4 /" % !
 1 "#$2  1 "#$2
   
1()#2 1*+ $#$'
! ! 4 /". ,4 #% 4 /"  '
   
5  1 "#$' 5   1()#'
	

5   1*+ $#$'

 	
5    % &'

(a) AUML representation

(b) CP-net representation

Figure 11: Deadline sequence expression.
act token is a record hs, r, t, ci@[T ts]. The @[..] expression denotes the corresponding token
time stamp, whereas the token time value is indicated starting with a capital ’T’. Accordingly, the described message token has a ts time stamp. The communicative act time limit
is defined using the val deadline parameter. Therefore, the deadline sequence expression
semantics is simulated using the following transition guard: [T ts < T deadline]. This transition guard, comparing the msg time stamp against the deadline parameter, guarantees
that an expired msg communicative act can not be received.
We now turn to representing interaction duration. The AUML representation is shown in
Figure 12-a. The AUML time intensive message notation is used to denote the communicative act transmission time. As a rule communicative act arrows are illustrated horizontally.
This indicates that the message transmission time can be neglected. However, in case the
message transmission time is significant, the communicative act is drawn slanted downwards.
The vertical distance, between the arrowhead and the arrow tail, denotes the message transmission time. Thus, the communicative act msg1 , sent from agent1 to agent2 , has a t1
transmission time.
	
 
 


	
   

 
	
	
    
 !"   
 #$%&   '(

)*+ ,)-+ 
2 2341 1 15 67
$%& % #.

	


#


	

 	
#$%&  / '( 

	
	  . &  '(
2 234 156 7
	

 + , + ,

 + , +!" 
 . &% #.. &  / '( 
	

0)
1+  0)  + 
 	
88
+!" 
0)
	
(a) AUML representation

(b) CP-net representation

Figure 12: Interaction duration.
368

Representing Conversations for Scalable Overhearing

The response time in Figure 12-a is indicated through the interaction thread length.
The incoming msg1 communicative act causes agent2 to perform some task before sending
a response msg2 message. The corresponding interaction thread duration is denoted through
the t2 time period. Thus, this time period specifies the agent2 response time to the incoming
msg1 communicative act.
The CP-net implementation to the interaction duration time periods is shown in Figure 12-b. The communicative act transmission time is illustrated using the timed CP-nets
@+ operator. The net transitions simulate the communicative act transmission between
agents. Therefore, representing a transmission time of t1 , the CP-net transition adds a t1
time period to the incoming message token time stamp. Accordingly, the transition @ + T t1
output arc expression denotes a t1 delay to the time stamp of the outgoing token. Thus,
the corresponding transition takes t1 time units and consequently so does the msg1 communicative act transmission time.
In contrast to communicative act transmission time, the agent interaction response time
is represented implicitly. Previously, we have defined a MSG-TIME color set that indicates
message token time stamps. Analogously, in Figure 12-b we introduce an additional INTERSTATE-TIME color set. This color set is associated with the net agent places and it presents
the possibility to attach time stamps to agent tokens as well. Now, let us assume that A2 B2
and msg2 places contain a single token each. The circled ’1’ next to the corresponding place,
together with the multi-set inscription, indicates the place current marking. Thus, the agent
and the message place tokens have a ts1 and a ts2 time stamps respectively. The ts1 time
stamp denotes the time by which agent2 has received the msg1 communicative act sent
by agent1 . The ts2 time stamp indicates the time by which agent2 is ready to send msg2
response message to agent1 . Thus, the agent2 response time t2 (Figure 12-a) is ts2 − ts1 .

7. Algorithm and a Concluding Example
Our final contribution in this paper is a skeleton procedure for transforming an AUML
conversation protocol diagram of two interacting agents to its CP-net representation. The
procedure is semi-automated–it relies on the human to fill in some details–but also has
automated aspects. We apply this procedure on a complex multi-agent conversation protocol
that involves many of the interaction building blocks already discussed.
The procedure is shown in Algorithm 1. The algorithm input is an AUML protocol
diagram and the algorithm creates, as an output, a corresponding CP-net representation.
The CP-net is constructed in iterations using a queue. The algorithm essentially creates the
conversation net by exploring the interaction protocol breadth-first while avoiding cycles.
Lines 1-2 create and initiate the algorithm queue, and the output CP-net, respectively.
The queue, denoted by S, holds the initiating agent places of the current iteration. These
places correspond to interaction states that initiate further conversation between the interacting agents. In lines 4-5, an initial agent place A1 B1 is created and inserted into the
queue. The A1 B1 place represents a joint initial interaction state for the two agents. Lines
7-23 contain the main loop.
We enter the main loop in line 8 and set the curr variable to the first initiating agent
place in S queue. Lines 10-13 create the CP-net components corresponding to the current
iteration as follows. First, in line 10, message places, associated with curr agent place, are
369

Gutnik & Kaminka

Algorithm 1 Create Conversation Net(input:AU M L,output:CP N )
1: S ← new queue
2: CP N ← new CP − net
3:
4:
5:

A1 B1 ← new agent place with color information
S.enqueue(A1 B1 )

6:
7:
8:

while S not empty do
curr ← S.dequeue()

9:
10:
11:
12:
13:

M P ← CreateM essageP laces(AU M L, curr)
RP ← CreateResultingAgentP laces(AU M L, curr, M P )
(T R, AR) ← CreateT ransitionsAndArcs(AU M L, curr, M P, RP )
F ixColor(AU M L, CP N, M P, RP, T R, AR)

14:
15:
16:
17:
18:
19:
20:
21:
22:
23:

for each place p in RP do
if p was not created in current iteration then
continue
if p is not terminating place then
S.enqueue(p)
S
S
CP N.places = CP N.places M P SRP
CP.transitions = CP N.transitions
TR
S
CP N.arcs = CP N.arcs AR

24:
25:

return CP N

created using the CreateM essageP laces procedure (which we do not detail here). This
procedure extracts the communicative acts that are associated with a given interaction
state, from the AUML diagram. These places correspond to communicative acts, which
take agents from the joint interaction state curr to its successor(s). Then in line 11, the
CreateResultingAgentP laces procedure creates agent places that correspond to interaction
state changes as a result of the communicative acts associated with curr agent place (again
based on the AUML diagram). Then, in CreateT ransitionsAndArcs procedure (line 12),
these places are connected using the principles described in Sections 3–6. Thus, the CP-net
structure (net places, transitions and arcs) is created. Finally, in line 13, the F ixColor procedure adds token color elements to the CP-net structure, to support deadlines, cardinality,
and other communicative act attributes.
Lines 15-19 determine which resulting agent places are inserted into the S queue for
further iteration. Only non-terminating agent places, i.e. places that do not correspond to
interaction states that terminate the interaction, are inserted into the queue in lines 18-19.
However, there is one exception (lines 16-17): a resulting agent place, which has already been
handled by the algorithm, is not inserted back into the S queue since inserting it can cause
an infinite loop. Thereafter, completing the current iteration, the output CP-net, denoted
370

Representing Conversations for Scalable Overhearing

by CP N variable, is updated according to the current iteration CP-net components in lines
21-23. This main loop iterates as long as the S queue is not empty. The resulting CP-net is
returned–line 25.

  	 

	  	



 

  



 



  

   	


	   	





 

 	
 
    
    
   

Figure 13: FIPA Contract Net Interaction Protocol using AUML.
To demonstrate this algorithm, we will now use it on the FIPA Contract Net Interaction
Protocol (FIPA Specifications, 2003b) (Figure 13). This protocol allows interacting agents to
negotiate. The Initiator agent issues m calls for proposals using a cf p communicative act.
Each of the m P articipants may refuse or counter-propose by a given deadline sending either
a ref use or a propose message respectively. A ref use message terminates the interaction.
In contrast, a propose message continues the corresponding interaction.
Once the deadline expires, the Initiator does not accept any further P articipant response messages. It evaluates the received P articipant proposals and selects one, several,
or no agents to perform the requested task. Accepted proposal result in the sending of
accept-proposal messages, while the remaining proposals are rejected using reject-proposal
message. Reject-proposal terminates the interaction with the corresponding P articipant.
On the other hand, the accept-proposal message commits a P articipant to perform the requested task. On successful completion, P articipant informs Initiator sending either an
inform-done or an inform-result communicative act. However, in case a P articipant has
failed to accomplish the task, it communicates a f ailure message.
371

Gutnik & Kaminka

We now use the algorithm introduced above to create a CP-net, which represents the
FIPA Contract Net Interaction Protocol. The corresponding CP-net model is constructed in
four iterations of the algorithm. Figure 14 shows the CP-net representation after the second
iteration of the algorithm, while Figure 15 shows the CP-net representation after the fourth
and final iteration.
The Contract Net Interaction Protocol starts from I1 P1 place, which represents a joint interaction state where Initiator is ready to send a cf p communicative act (I1 ) and P articipant
is waiting for the corresponding cf p message (P1 ). The I1 P1 place is created and inserted
into the queue before the iterations through the main loop begin.
First iteration. The curr variable is set to the I1 P1 place. The algorithm creates
net places, which are associated with the I1 P1 place, i.e. a Cf p message place, and an
I2 P2 resulting agent place. The I2 P2 place denotes an interaction state in which Initiator
has already sent a cf p communicative act to P articipant and is now waiting for its response (I2 ) and P articipant has received the cf p message and is now deciding on an
appropriate response (P2 ). These are created using the CreateM essageP laces and the
CreateResultingAgentP laces procedures, respectively.
Then, the CreateT ransitionsAndArcs procedure in line 12, connects the three places
using a simple asynchronous message building block as shown in Figure 1-b (Section 3).
In line 13, as the color sets of the places are determined, the algorithm also handles the
cardinality of the cf p communicative act, by putting an appropriate sequence expression on
the transition, using the principles presented in Figure 6-b (Section 4). Accordingly, the
color set, associated with I1 P1 place, is changed to the INTER-STATE-CARD color set.
Since the I2 P2 place is not a terminating place, it is inserted into the S queue.
Second iteration. curr is set to the I2 P2 place. The P articipant agent can send, as a
response, either a ref use or a propose communicative act. Ref use and P ropose message
places are created by CreateM essageP laces (line 10), and resulting places I3 P3 and I4 P4 ,
corresponding to the results of the ref use and propose communicative acts, respectively,
are created by CreateResultingAgentP laces (line 11). The I3 P3 place represents a joint
interaction state where P articipant has sent the ref use message and terminated (P3 ), while
Initiator has received it, and terminated (I3 ). The I4 P4 place represents the joint state in
which P articipant has sent the propose message (P4 ), while Initiator has received the
message and is considering its response (I4 ).
In line 12, the I2 P2 , Ref use, I3 P3 , P ropose and I4 P4 places are connected using the
XOR-decision building block presented in Figure 3-b (Section 3). Then, the F ixColor
procedure (line 13), adds the appropriate token color attributes, to allow a deadline sequence
expression (on both the ref use and the propose messages) to be implemented as shown in
Figure 11-b (Section 6). The I3 P3 place denotes a terminating state, whereas the I4 P4
place continues the interaction. Thus, in lines 18-19, only the I4 P4 place is inserted into the
queue, for the next iteration of the algorithm. The state of the net at the end of the second
iteration of the algorithm is presented in Figure 14.
Third iteration. curr is set to I4 P4 . Here, the Initiator response to a P articipant
proposal can either be an accept-proposal or a reject-proposal. CreateM essageP laces procedure in line 10 thus creates the corresponding Accept-Proposal and Reject-Proposal message
places. The accept-proposal and reject-proposal messages cause the interacting agents to
transition to I5 P5 and I6 P6 places, respectively. These agent places are created using the
372

Representing Conversations for Scalable Overhearing

3  0 & ( )4
3  0

8


3  0 ; 4
$% 

& '( )

 

**

 	
   	
 

 

 
 

 




  	

3  0

 

! 

./. 01 "





 
	





 



  	

4

)'<=

&

)<& ' @ 5
4

!"

!

./.01"

 	

# #

) 5  3 . /

"

- -

!"

 	


5 67
&

& ()4
>

/



 	

  	

 

)4

)'<=

3  0 & '@ 5 1! 7

 

	






+ $+ $
' ,

4

 

& '( )

 

3  0

5 67

) 5 3 9 , 9:::7

2 2

)'<=

& ()4
->

?
7

.,3!
&

)?& '@7

3  0 A= ( 5  3  .  & ( )4 ?  & ( ) 4 ?
>
>
!
8 )? 3 ;4 )4 7
>
>

& '() <B =
5 & ( )4
C ! D 67
3  0

5 67
E/ 0
3  0

& '()

5 1.F

& '()


C ! D * :::
E /   & ( ) 4 7 E / 
>
E /  & '@7
>
E / 0 ./. 015 67



<B =

< $7

 A=(7
>

Figure 14: FIPA Contract Net Interaction Protocol using CP-net after the 2nd iteration.
CreateResultingAgentP laces procedure (line 11). The I5 P5 place denotes an interaction
state in which Initiator has sent a reject-proposal message and terminated the interaction (I5 ), while the P articipant has received the message and terminated as well (P5 ). In
contrast, the I6 P6 place represents an interaction state where Initiator has sent an acceptproposal message and is waiting for a response (I6 ), while P articipant has received the
accept-proposal communicative act and is now performing the requested task before sending
a response (P6 ). The Initiator agent sends exclusively either an accept-proposal or a rejectproposal message. Thus, the I4 P4 , Reject-Proposal, I5 P5 , Accept-Proposal and I6 P6 places
are connected using a XOR-decision block (in the CreateT ransitionsAndArcs procedure,
line 12).
The F ixColor procedure in line 13 operates now as follows: According to the interaction
protocol semantics, the Initiator agent evaluates all the received P articipant proposals once
the deadline passes. Only thereafter, the appropriate reject-proposal and accept-proposal
communicative acts are sent. Thus, F ixColor assigns a MSG-TIME color set to the RejectProposal and the Accept-Proposal message places, and creates a [T ts >= T deadline] transition guard on the associated transitions. This transition guard guarantees that Initiator
cannot send any response until the deadline expires, and all valid P articipant responses
have been received. The resulting I5 P5 agent place denotes a terminating interaction state,
whereas the I6 P6 agent place continues the interaction. Thus, only I6 P6 agent place is
inserted into the S queue.
Fourth iteration. curr is set to I6 P6 . This place is associated with three communicative acts: inform-done, inform-result and f ailure. The inform-done and the informresult messages are instances of the inf orm communicative act class. Thus, CreateMessagePlaces (line 10) creates only two message places, Inf orm and F ailure. In line 11,
CreateResultingAgentP laces creates the I7 P7 and I8 P8 agent places. The f ailure communicative act causes interacting agents to transition to I7 P7 agent place, while both inf orm
messages cause the agents to transition to I8 P8 agent place. The I7 P7 place represents a
joint interaction state where P articipant has sent the f ailure message and terminated (P7 ),
373

Gutnik & Kaminka

while Initiator has received a f ailure communicative act and terminated (I7 ). On the other
hand, the I8 P8 place denotes an interaction state in which P articipant has sent the inf orm
message (either inform-done or inform-result) and terminated (P8 ), while Initiator has
received an inf orm communicative act and terminated (I8 ). The inf orm and f ailure communicative acts are sent exclusively. Thus CreateT ransitionsAndArcs (line 12) connects
the I6 P6 , F ailure, I7 P7 , Inf orm and I8 P8 places using a XOR-decision building block.
Then, F ixColor assigns a [#t msg = inform-done or #t msg = inform-result] transition
guard on the transition associated with Inf orm message place. Since both the I7 P7 and
the I8 P8 agent places represent terminating interaction states, they are not inserted into the
queue, which remains empty at the end of the current iteration. This signifies the end of the
conversion. The complete conversation CP-net resulting after this iteration of the algorithm
is shown in Figure 15.
$% 

& '( )

*  * 
 

 	
  	

  

 






, $ , $



.

 


.
 

  	

#



#



 

 



3

  	

'6 4!5  0 1

 


3



8 8

  	
+

+

 
;0 1-



 

  	

=

4  1 
L0 1

& 44!5

)'5D

 

&

)5& 'G 7

5I D

 /-4!
&

)F& 'G?

FE& () =

@ )F4 EC=

7 & ( )=

)=

F
?

J ! K >?

& '( )

7 2/M

& '()

J !K * BBB 5 $?

?

L 0



5I D

EH D( ?

L 0  E& 'G?

!"

L 0 1 /0/ 127 >?

! 7

/0/ 12"
 	

  	


2  
 

 



9 9

)'5D

7 >?

L 0   E& ()=

	
 



& '( )

4  1 

  0 1



 
 	


?

!E




F

0 E& ()=
.

4  1  H D( 7 4  /  E& ()=

!"



 	


) 7 4  / 0 E& ( ) =

=

 

! 7

/0/ 12"

7 >?
&

! 

!"



4  1 

/0/ 12 "

 
 	




)=

)'5D

	




=

4  1  & 'G 7 2! ?

 

!

 	
 

	

  

!"

/0/12"



"


  	

	





4  1 

7 >?

@ ) 7 4 A - ABBB?

4  1  C =

 	 


 

4  1 



  & '( )

' -

4  1  & ( )=







:







<!
7 2  
5

/ 2   <!
7

2  
5- 1!"

 	

:

  	

Figure 15: FIPA Contract Net Interaction Protocol using CP-net after the 4th (and final)
iteration.
The procedure we outline can guide the conversion of many 2-agent conversation protocols in AUML to their CP-net equivalents. However, it is not sufficiently developed to
address the general n-agent case. Appendix C presents a complex example of a 3-agent conversation protocol, which was successfully converted manually, without the guidance of the
algorithm. This example incorporates many advanced features of our CP-net representation
technique and would have been beyond the scope of many previous investigations.
374

Representing Conversations for Scalable Overhearing

8. Summary and Conclusions
Over recent years, open distributed MAS applications have gained broad acceptance both
in the multi-agent academic community and in real-world industry. As a result, increasing attention has been directed to multi-agent conversation representation techniques. In
particular, Petri nets have recently been shown to provide a viable representation approach
(Cost et al., 1999, 2000; Nowostawski et al., 2001; Mazouzi et al., 2002).
However, radically different approaches have been proposed to using Petri nets for modelling multi-agent conversations. Yet, the relative strengths and weaknesses of the proposed
techniques have not been examined. Our work introduces a novel classification of previous investigations and then compares these investigations addressing their scalability and
appropriateness for overhearing tasks.
Based on the insights gained from the analysis, we have developed a novel representation,
that uses CP-nets in which places explicitly represent joint interaction states and messages.
This representation technique offers significant improvements (compared to previous approaches) in terms of scalability, and is particularly suitable for monitoring via overhearing.
We systematically show how this representation covers essentially all the features required
to model complex multi-agent conversations, as defined by the FIPA conversation standards (FIPA Specifications, 2003c). These include simple & complex interaction building
blocks (Section 3 & Appendix B), communicative act attributes and multiple concurrent
conversations using the same CP-net (Section 4), nested & interleaved interactions using
hierarchical CP-nets (Section 5) and temporal interaction attributes using timed CP-nets
(Section 6). The developed techniques have been demonstrated, throughout the paper, on
complex interaction protocols defined in the FIPA conversation standards (see in particular
the example presented in Appendix C). Previous approaches could handle some of these
examples (though with reduced scalability), but only a few were shown to cover all the
required features.
Finally, the paper presented a skeleton procedure for semi-automatically converting an
AUML protocol diagrams (the chosen FIPA representation standard) to an equivalent CPnet representation. We have demonstrated its use on a challenging FIPA conversation protocol, which was difficult to represent using previous approaches.
We believe that this work can assist and motivate continuing research on multi-agent
conversations including such issues as performance analysis, validation and verification (Desel et al., 1997), agent conversation visualization, automated monitoring (Kaminka et al.,
2002; Busetta et al., 2001, 2002), deadlock detection (Khomenco & Koutny, 2000), debugging (Poutakidis et al., 2002) and dynamic interpretation of interaction protocols (Cranefield
et al., 2002; de Silva et al., 2003). Naturally, some issues remain open for future work. For
example, the presented procedure addresses only AUML protocol diagrams representing two
agent roles. We plan to investigate an n-agent version in the future.

Acknowledgments
The authors would like to thank the anonymous JAIR reviewers for many useful and informative comments. Minor subsets of this work were also published as LNAI book chapter
(Gutnik & Kaminka, 2004b). K. Ushi deserves many thanks.
375

Gutnik & Kaminka

Appendix A. A Brief Introduction to Petri Nets
Petri nets (Petri Nets site, 2003) are a widespread, established methodology for representing
and reasoning about distributed systems, combining a graphical representation with a comprehensive mathematical theory. One version of Petri nets is called Place/Transition nets
(PT-nets) (Reisig, 1985). A PT-net is a bipartite directed graph where each node is either
a place or a transition (Figure 16). The net places and transitions are indicated through
circles and rectangles respectively. The PT-net arcs support only place → transition and
transition → place connections, but never connections between two places or between two
transitions. The arc direction determines the input/output characteristics of the place and
the transition connected. Thus, given an arc, P → T , connecting place P and transition T ,
we will say that place P is an input place of transition T and vice versa transition T is an
output transition of place P . The P → T arc is considered to be an output arc of place P
and an input arc of transition T .
















(a) Before firing




(b) After firing

Figure 16: A PT-net example.
A PT-net place may be marked by small black dots called tokens. The arc expression is
an integer, which determines the number of tokens associated with the corresponding arc.
By convention, an arc expression equal to 1 is omitted. A specific transition is enabled if
and only if its input places marking satisfies the appropriate arc expressions. For example,
consider arc P → T to be the only arc to connect place P and transition T . Thus, given
that this arc has an arc expression 2, we will say that transition T is enabled if and only
if place P is marked with two tokens. In case the transition is enabled, it may fire/occur.
The transition occurrence removes tokens from the transition input places and puts tokens
to the transition output places as specified by the arc expressions of the corresponding
input/output arcs. Thus, in Figures 16-a and 16-b, we demonstrate PT-net marking before
and after transition firing respectively.
Although computationally equivalent, a different version of Petri nets, called Colored
Petri nets (CP-nets) (Jensen, 1997a, 1997b, 1997c), offers greater flexibility in compactly
representing complex systems. Similarly to the PT-net model, CP-nets consist of net places,
net transitions and arcs connecting them. However, in CP-nets, tokens are not just single
bits, but can be complex, structured, information carriers. The type of additional information carried by the token, is called token color, and it may be simple (e.g., an integer or a
string), or complex (e.g. a record or a tuple). Each place is declared by a place color set to
376

Representing Conversations for Scalable Overhearing

only match tokens of particular colors. A CP-net place marking is a token multi-set (i.e., a
set in which a member may appear more than once) corresponding to the appropriate place
color set. CP-net arcs pass token multi-sets between the places and transitions. CP-net arc
expressions can evaluate token multi-sets and may involve complex calculation procedures
over token variables declared to be associated with the corresponding arcs.
The CP-net model introduces additional extensions to PT-nets. Transition guards are
boolean expressions, which constrain transition firings. A transition guard associated with
a transition tests tokens that pass through a transition, and will only enable the transition
firings if the guard is successfully matched (i.e., the test evaluates to true). The CP-net
transition guards, together with places color sets and arc expressions, appear as a part of
net inscriptions in the CP-net.
In order to visualize and manage the complexity of large CP-nets, hierarchical CP-nets
(Huber, Jensen, & Shapiro, 1991; Jensen, 1997a) allow hierarchical representations of CPnets, in which sub-CP nets can be re-used in higher-level CP nets, or abstracted away from
them. Hierarchical CP-nets are built from pages, which are themselves CP-nets. Superpages
present a higher level of hierarchy, and are CP-nets that refer to subpages, in addition to
transitions and places. A subpage may also function as a superpage to other subpages. This
way, multiple hierarchy levels can be used in a hierarchical CP-net structure.
The relationship between a superpage and a subpage is defined by a substitution transition, which substitutes a corresponding subpage instance on the CP-net superpage structure
as a transition in the superpage. The substitution transition hierarchy inscription supplies
the exact mapping of the superpage places connected to the substitution transition (called
socket nodes), to the subpage places (called port nodes). The port types determine the
characteristics of the socket node to port node mappings. A complete CP-net hierarchical
structure is presented using a page hierarchy graph, a directed graph where vertices correspond to pages, and directed edges correspond to direct superpage-subpage relationships.
Timed CP-nets (Jensen, 1997b) extend CP-nets to support the representation of temporal aspects using a global clock. Timed CP-net tokens have an additional color attribute
called time stamp, which refers to the earliest time at which the token may be used. Time
stamps can be used by arc expression and transition guards, to enable a timed-transition if
and only if it satisfies two conditions: (i) the transition is color enabled, i.e. it satisfies the
constraints defined by arc expression and transition guards; and (ii) the tokens are ready,
i.e. the time of the global clock is equal to or greater than the tokens’ time stamps. Only
then can the transition fire.

Appendix B. Additional Examples of Conversation Representation
Building Blocks
This appendix presents some additional interaction building blocks to those already described in Section 3. The first is the AND-parallel messages interaction (AUML representation shown in Figure 17-a). Here, the sender agent1 sends both the msg1 message to
agent2 and the msg2 message to agent3 . However, the order of the two communicative acts
is unconstrained.
The representation of AND-parallel in our CP-net representation is shown in Figure 17-b.
The A1 B1 C1 , A2 B2 , A2 C2 , msg1 and msg2 places are defined similarly to Figures 3-b and
377

Gutnik & Kaminka


	

	  




      

 










  





01
 0
	
	

	

	  
 



 



	






 
	
	

  

   
 
 
   
 !"#$  %& ' ()' (
 !"#$#*%& ' ()' ()'+(
 ,$  %& ()(
-'  ./ (

(a) AUML representation

(b) CP-net representation

Figure 17: AND-parallel messages interaction.
4-b in Section 3. However, we also define two additional intermediate agent places, A01 B2 C1
and A001 B1 C2 . The A01 B2 C1 place represents a joint interaction state where agent1 has sent
the msg1 message to agent2 and is ready to send the msg2 communicative act to agent3
(A1 ’), agent2 has received the msg1 message (B2 ) and agent3 is waiting to receive the msg2
communicative act (C1 ). The A001 B1 C2 place represents a joint interaction state in which
agent1 is ready to send the msg1 message to agent2 and has already sent the msg2 communicative act to agent3 (A001 ), agent2 is waiting to receive the msg1 message (B1 ) and agent3
has received the msg2 communicative act (C2 ). These places enable agent1 to send both
communicative acts concurrently. Four transitions connect the appropriate places respectively. The behavior of the transitions connecting A01 B2 C1 → A2 B2 and A001 B1 C2 → A2 C2
is similar to described above. The transitions A1 B1 C1 → A01 B2 C1 and A1 B1 C1 → A001 B1 C2
are triggered by receiving messages msg1 and msg2 , respectively. However, these transitions should not consume the message token since it is used further for triggering transitions
A01 B2 C1 → A2 B2 and A001 B1 C2 → A2 C2 . This is achieved by adding an appropriate message
place as an output place of the corresponding transition.
The second AUML interaction building block, shown in Figure 18-a, is the message
sequence interaction, which is similar to AND-parallel. However, the message sequence
interaction defines explicitly the order between the transmitted messages. Using the 1/msg1
and 2/msg2 notation, Figure 18-a specifies that the msg1 message should be sent before
sending msg2 .
Figure 18-b shows the corresponding CP-net representation. The A1 B1 C1 , A2 B2 , A2 C2 ,
msg1 and msg2 places are defined as before. However, the CP-net implementation presents
an additional intermediate agent place–A01 B2 C1–which is identical to the corresponding
378

Representing Conversations for Scalable Overhearing


	 


 



 	

  	




	


 






 
  





	


 

0




 



	


 




  



   !"
  #$%&  ' ( ) * + )* "
  #$%& %, ' ( ) * +)* +)- * "
  . & ' (  * + * "
/ )  * "



(a) AUML representation

(b) CP-net representation

Figure 18: Sequence messages interaction.
intermediate agent place in Figure 17-b. A01 B2 C1 is defined as an output place of the
A1 B1 C1 → A2 B2 transition. It thus guarantees that the msg2 communicative act can be
sent (represented by the A01 B2 C1 → A2 C2 transition) only upon completion of the msg1
transmission (the A1 B1 C1 → A2 B2 transition).
The last interaction we present is the synchronized messages interaction, shown in Figure 19-a. Here, agent3 simultaneously receives msg1 from agent1 and msg2 from agent2 .
In AUML, this constraint is annotated by merging the two communicative act arrows into
a horizontal bar with a single output arrow.

 	

	 	


	

	 	

 

 








	

	 	
1

	

	

	 	







/
	

	 	

.

.



.
.
/0

.
.
/

   
 !"#$   %& '( )'( 
 !"#$ #*  %& '( )'( )'+( 
 ,$   %&  ( ) ( 
- '  .( 

	

(a) AUML representation

(b) CP-net representation

Figure 19: Synchronized messages interaction.
379

Gutnik & Kaminka

Figure 19-b illustrates the CP-net implementation of synchronized messages interaction.
As in previous examples, we define the A1 C1 , B1 C1 , msg1 , msg2 and A2 B2 C2 places. We
additionally define two intermediate agent places, A2 C10 and B2 C100 . The A2 C10 place represents a joint interaction state where agent1 has sent msg1 to agent3 (A2 ), and agent3 has
received it, however agent3 is also waiting to receive msg2 (C10 ). The B2 C100 place represents
a joint interaction state in which agent2 has sent msg2 to agent3 (B2 ), and agent3 has
received it, however agent3 is also waiting to receive msg1 (C100 ). These places guarantee
that the interaction does not transition to the A2 B2 C2 state until both msg1 and msg2 have
been received by agent3 .

Appendix C. An Example of a Complex Interaction Protocol
We present an example of a complex 3-agent conversation protocol, which was manually converted to a CP-net representation using the building blocks in this paper. The conversation
protocol addressed here is the FIPA Brokering Interaction Protocol (FIPA Specifications,
2003a). This interaction protocol incorporates many advanced conversation features of our
representation such as nesting, communicative act sequence expression, message guards and
etc. Its AUML representation is shown in Figure 20.
The Initiator agent begins the interaction by sending a proxy message to the Broker
agent. The proxy communicative act contains the requested proxied-communicative-act as
part of its argument list. The Broker agent processes the request and responds with either an
agree or a ref use message. Communication of a ref use message terminates the interaction.
If the Broker agent has agreed to function as a proxy, it then locates the agents matching
the Initiator request. If no such agent can be found, the Broker agent communicates
a failure-no-match message and the interaction terminates. Otherwise, the Broker agent
begins m interactions with the matching agents. For each such agent, the Broker informs the
Initiator, sending either an inform-done-proxy or a failure-proxy communicative act. The
failure-proxy communicative act terminates the sub-protocol interaction with the matching
agent in question. The inform-done-proxy message continues the interaction. As the subprotocol progresses, the Broker forwards the received responses to the Initiator agent using
the reply-message-sub-protocol communicative acts. However, there can be other failures
that are not explicitly returned from the sub-protocol interaction (e.g., if the agent executing
the sub-protocol has failed). In case the Broker agent detects such a failure, it communicates
a failure-brokering message, which terminates the sub-protocol interaction.
A CP-net representation of the FIPA Brokering Interaction Protocol is shown in Figure 21. The Brokering Interaction Protocol starts from I1 B1 place. The I1 B1 place represents a joint interaction state where Initiator is ready to send a proxy communicative
act (I1 ) and Broker is waiting to receive it (B1 ). The proxy communicative act causes the
interacting agents to transition to I2 B2 . This place denotes an interaction state in which
Initiator has already sent a proxy message to Broker (I2 ) and Broker has received it (B2 ).
The Broker agent can send, as a response, either a ref use or an agree communicative act.
This CP-net component is implemented using the XOR-decision building block presented
in Section 3. The ref use message causes the agents to transition to I3 B3 place and thus
terminate the interaction. This place corresponds to Broker sending a ref use message
and terminating (B3 ), while Initiator receiving the message and terminating (I3 ). On the
380

Representing Conversations for Scalable Overhearing


  	  

 

	

    	  	
    	 	     
  

		

 		

 	

  	

 		      
   	  	

 		    
   	  	

 	    	

 	   

 !  

    	 
 


   

 		  
     	

	    	  	 !   

 	  	  	
 
 	 " ##


 !  

	  	 

 		

 !    		
 	!	  	
 !    	

Figure 20: FIPA Brokering Interaction Protocol - AUML representation.
other hand, the agree communicative act causes the agents to transition to I4 B4 place,
which represents a joint interaction state in which the Broker has sent an agree message
to Initiator (and is now trying to locate the receivers of the proxied message), while the
Initiator received the agree message.
The Broker agent’s search for suitable receivers may result in two alternatives. First,
in case no matching agents are found, the interaction terminates in the I5 B5 agent place.
This joint interaction place corresponds to an interaction state where Broker has sent the
failure-no-match communicative act (B5 ), and Initiator has received the message and terminated (I5 ). The second alternative is that suitable agents have been found. Then, Broker
starts sending proxied-communicative-act messages to these agents on the established list
of designated receivers, i.e. TARGET-LIST. The first such proxied-communicative-act message causes the interacting agents to transition to I4 B6 P1 place. The I4 B6 P1 place denotes
a joint interaction state of three agents: Initiator, Broker and P articipant (the receiver).
381

Gutnik & Kaminka



 

	 


 71/



 
 


-    8 7 29 7> /


B * 71/    8 7 29 7> /

 

 

2 9KB



 !   "#$% & '(

	 






 

 



$% #/0% % # &  1

 ! 

2 3 "# $%4 2 3 " #$% (

$% #/0% % #/5 &   1
2 3 "# $%4 2 3 " #$%4


9

 


 
 



 ! 

 

	 


 
 


 !  - .$% #$% & '(



 

   
	 


 !  %) # & *  +   +,,,(

  

 
 


2 3 "# $% (

 !  - . 6 & 789 (




 ! 



$% #/0% % #/5/- 6 &
$% # /0% % #/54

 1



- 6 (
 

 

$/
:2 9


IJ 

   


   

H2 7!/


 9/% *

 L

 !  : 0"& 1  3 "# $%4

 

=

3 " #$%49 3%) #4

 71/

 

3-. $% # $% (

-    8 7 29 7> /

G

 
 


 

K  % "#% K? L ? L
	 


   
	 






>2 !  & '(
 !  % " #%&781 % " #%/


K   *  7L

E F


9


 
 
D

G

 !  % " #%/ ; 0% & " #$% < 79 = ,,(

; 0% < 79 = ? ,,, / @(
>2    3: 0"(

 

 
M

>2    * 3 " #$% (

A7  281 9 &B9  C

K  % " #% K7N @L* 7N @L

>2  9 3%)  #(
>2  73- 6(

  *
H2 7!/

 

 
 P

 

 
 
D


 

  *

 71/
-    8 7 29 7> /

 

	 



 

8  /6 8/





  *

	 


 
M





 
 



 

 

 

9

R  /0S/9  !

Q0
   *

R  / 892 9 7 8 /9  !

	 

 
 
D
 

 
M O

 X/ XX (
W
 71/-    8 7 2 9 7> /


 *

 
P T 

 U78



XY Z/

9  !

H2 7!/

 
V 

	 


	 
 

 
 
D

 
 


  *

XY Z/

[
W

 (
[
 (
W


 *


 

9/R (

XY Z/ Z Z (

R   /0S/

 


*! /
: 2 /

  *

0S/
   *

   *

	 


	 


 
 
 D

 
 
D
O





T

9  !

 
 

Figure 21: FIPA Brokering Interaction Protocol - CP-net representation.
The Initiator individual state remains unchanged (I4 ) since the proxied-communicative-act
message starts an interaction between Broker and P articipant. The Broker individual
state (B6 ) denotes that designated agents have been found and the proxied-communicative382

Representing Conversations for Scalable Overhearing

act messages are ready to be sent, while P articipant is waiting to receive the interaction
initiating communicative act (P1 ). The proxied-communicative-act message place is also
connected as an output place of the transition. This message place is used as part of a
CP-net XOR-decision structure, which enables the Broker agent to send either a failure-nomatch or a proxied-communicative-act, respectively. Thus, the token denoting the proxiedcommunicative-act message, must not be consumed by the transition.
Thus, multiple proxied-communicative-act messages are sent to all P articipants. This
is implemented similarly to the broadcast sequence expression implementation (Section 4).
Furthermore, the proxied-communicative-act type is verified against the type of the requested
proxied communicative act, which is obtained from the original proxy message content.
We use the Proxied-Communicative-Act-Type message type place to implement this CPnet component similarly to Figure 8. Each proxied-communicative-act message causes the
interacting agents to transition to both the I4 B7 P1 and the B6 P1 places.
The B6 P1 place corresponds to interaction between the Broker and the P articipant
agents. It represents a joint interaction state in which Broker is ready to send a proxiedcommunicative-act message to P articipant (B6 ), and P articipant is waiting for the message
(P1 ). In fact, the B6 P1 place initiates the nested interaction protocol that results in B10 P3
place. The B10 P3 place represents a joint interaction state where P articipant has sent
the reply-message communicative act and terminated (P3 ), and Broker has received the
message (B10 ). In our example, we have chosen the FIPA Query Interaction Protocol (FIPA
Specifications, 2003d) (Figures 7–8) as the interaction sub-protocol. The CP-net component,
implementing the nested interaction sub-protocol, is modeled using the principles described
in Section 5. Consequently, the interaction sub-protocol is concealed using the Query-SubProtocol substitution transition. The B6 P1 , proxied-communicative-act and B10 P3 places
determine substitution transition socket nodes. These socket nodes are assigned to the CPnet port nodes in Figure 8 as follows. The B6 P1 and proxied-communicative-act places are
assigned to the I1 P1 and query input port nodes, while the B10 P3 place is assigned to the
I3 P3 , I5 P5 and I6 P6 output port nodes.
We now turn to the I4 B7 P1 place. In contrast to the B6 P1 place, this place corresponds to
the main interaction protocol. The I4 B7 P1 place represents a joint interaction state in which
Initiator is waiting for Broker to respond (I4 ), Broker is ready to send an appropriate response communicative act (B7 ), and to the best of the Initiator’s knowledge the interaction
with P articipant has not yet begun (P1 ). The Broker agent can send one of two messages,
either a failure-proxy or an inform-done-proxy, depending on whether it has succeeded to
send the proxied-communicative-act message to P articipant. The failure-proxy message
causes the agents to terminate the interaction with corresponding P articipant agent and to
transition to I6 B8 P1 place. This place denotes a joint interaction state in which Initiator
has received a failure-proxy communicative act and terminated (I6 ), Broker has sent the
failure-proxy message and terminated as well (B8 ) and the interaction with the P articipant
agent has never started (P1 ). On the other hand, the inform-done-proxy causes the agents to
transition to I7 B9 P2 place. The I7 B9 P2 place represents an interaction state where Broker
has sent the inform-done-proxy message (B9 ), Initiator has received it (I7 ), and P articipant
has begun the interaction with the Broker agent (P2 ). Again, this is represented using the
XOR-decision building block.
383

Gutnik & Kaminka

Finally, the Broker agent can either send a reply-message-sub-protocol or a failurebrokering communicative act. The failure-brokering message causes the interacting agents
to transition to I8 B11 P2 place. This place indicates that Broker has sent a failure-brokering
message and terminated (B11 ), Initiator has received the message and terminated (I8 ), and
P articipant has terminated during the interaction with the Broker agent (P2 ). The replymessage-sub-protocol communicative act causes the agents to transition to I9 B12 P3 place.
The I9 B12 P3 place indicates that Broker has sent a reply-message-sub-protocol message and
terminated (B12 ), Initiator has received the message and terminated (I9 ), and P articipant
has successfully completed the nested sub-protocol with the Broker agent and terminated as
well (P3 ). Thus, the B10 P3 place, denoting a successful completion of the nested sub-protocol,
is also the corresponding transition input place.

References
AUML site (2003). Agent unified modeling language, at www.auml.org..
Busetta, P., Dona, A., & Nori, M. (2002). Channelled multicast for group communications.
In Proceedings of AAMAS-02.
Busetta, P., Serafini, L., Singh, D., & Zini, F. (2001). Extending multi-agent cooperation
by overhearing. In Proceedings of CoopIS-01.
ChaibDraa, B. (2002). Trends in agent communication languages. Computational Intelligence, 18 (2), 89–101.
Cost, R. S. (1999). A framework for developing conversational agents. Ph.D. thesis, Department of Computer Science, University of Maryland.
Cost, R. S., Chen, Y., Finin, T., Labrou, Y., & Peng, Y. (1999). Modeling agent conversations
with coloured Petri nets. In Proceedings of the Workshop on Specifying and Implementing Conversation Policies, the Third International Conference on Autonomous Agents
(Agents-99), Seattle, Washington.
Cost, R. S., Chen, Y., Finin, T., Labrou, Y., & Peng, Y. (2000). Using coloured petri nets
for a conversation modeling. In Dignum, F., & Greaves, M. (Eds.), Issues in Agent
Communications, Lecture notes in Computer Science, pp. 178–192. Springer-Verlag.
Cranefield, S., Purvis, M., Nowostawski, M., & Hwang, P. (2002). Ontologies for interaction protocols. In Proceedings of the Workshop on Ontologies in Agent Systems, the
First International Joint Conference on Autonomous Agents & Multi-Agent Systems
(AAMAS-02), Bologna, Italy.
de Silva, L. P., Winikoff, M., & Liu, W. (2003). Extending agents by transmitting protocols
in open systems. In Proceedings of the Workshop on Challenges in Open Agent Systems, the Second International Joint Conference on Autonomous Agents & Multi-Agent
Systems (AAMAS-03), Melbourne, Australia.
Desel, J., Oberweis, A., & Zimmer, T. (1997). Validation of information system models: Petri
nets and test case generation. In Proceedings of the 1997 IEEE International Conference on Systems, Man and Cybernetics: Computational Cybernetics and Simulation,
pp. 3401–3406, Orlando, Florida.
384

Representing Conversations for Scalable Overhearing

Finin, T., Labrou, Y., & Mayfield, J. (1997). KQML as an agent communication language.
In Bradshaw, J. (Ed.), Software Agents. MIT Press.
FIPA site (2003). Fipa - the Foundation for Intelligent Physical Agents, at www.fipa.org..
FIPA Specifications (2003a). Fipa Brokering Interaction Protocol Specification, version H,
at www.fipa.org/specs/fipa0000033/..
FIPA Specifications (2003b). Fipa Contract Net Interaction Protocol Specification, version
H, at www.fipa.org/specs/fipa0000029/..
FIPA Specifications (2003c). Fipa Interaction Protocol Library Specification, version E, at
www.fipa.org/specs/fipa0000025/..
FIPA Specifications (2003d). Fipa Query Interaction Protocol Specification, version H, at
www.fipa.org/specs/fipa0000027/..
Gutnik, G., & Kaminka, G. (2004a). Towards a formal approach to overhearing: Algorithms
for conversation identification. In Proceedings of AAMAS-04.
Gutnik, G., & Kaminka, K. A. (2004b). A scalable Petri net representation of interaction
protocols for overhearing.. In van Eijk, R. M., Huget, M., & Dignum, F. (Eds.), Agent
Communication LNAI 3396: International Workshop on Agent Communication, AC
2004, New York, NY, USA, pp. 50–64. Springer-Verlag.
Hameurlain, N. (2003). MIP-Nets: Refinement of open protocols for modeling and analysis
of complex interactions in multi-agent systems. In Proceedings of the 3rd International
Central and Eastern European Conference on Multi-Agent Systems (CEEMAS-03), pp.
423–434, Prague, Czech Republic.
Huber, P., Jensen, K., & Shapiro, R. M. (1991). Hierarchies in Coloured Petri nets. In
Jensen, K., & Rozenberg, G. (Eds.), High-level Petri Nets: Theory and Application,
pp. 215–243. Springer-Verlag.
Jensen, K. (1997a). Coloured Petri Nets. Basic Concepts, Analysis Methods and Practical
Use, Vol. 1. Springer-Verlag.
Jensen, K. (1997b). Coloured Petri Nets. Basic Concepts, Analysis Methods and Practical
Use, Vol. 2. Springer-Verlag.
Jensen, K. (1997c). Coloured Petri Nets. Basic Concepts, Analysis Methods and Practical
Use, Vol. 3. Springer-Verlag.
Kaminka, G., Pynadath, D., & Tambe, M. (2002). Monitoring teams by overhearing: A
multi-agent plan-recognition approach. JAIR, 17, 83–135.
Khomenco, V., & Koutny, M. (2000). LP deadlock checking using partial order dependencies. In Proceedings of the 11th International Conference on Concurrency Theory
(CONCUR-00), pp. 410–425, Pennsylvania State University, Pennsylvania.
Kone, M. T., Shimazu, A., & Nakajima, T. (2000). The state of the art in agent communication languages. Knowledge and Information Systems, 2, 258–284.
Legras, F. (2002). Using overhearing for local group formation. In Proceedings of AAMAS02.
385

Gutnik & Kaminka

Lin, F., Norrie, D. H., Shen, W., & Kremer, R. (2000). A schema-based approach to specifying conversation policies. In Dignum, F., & Greaves, M. (Eds.), Issues in Agent
Communications, Lecture notes in Computer Science, pp. 193–204. Springer-Verlag.
Ling, S., & Loke, S. W. (2003). MIP-Nets: A compositional model of multi-agent interaction.
In Proceedings of the 3rd International Central and Eastern European Conference on
Multi-Agent Systems (CEEMAS-03), pp. 61–72, Prague, Czech Republic.
Mazouzi, H., Fallah-Seghrouchni, A. E., & Haddad, S. (2002). Open protocol design for
complex interactions in multi-agent systems. In Proceedings of the First International
Joint Conference on Autonomous Agents & Multi-Agent Systems (AAMAS-02), pp.
517–526, Bologna, Italy.
Milner, R., Harper, R., & Tofte, M. (1990). The Definition of Standard ML. MIT Press.
Moldt, D., & Wienberg, F. (1997). Multi-agent systems based on Coloured Petri nets. In
Proceedings of the 18th International Conference on Application and Theory of Petri
Nets (ICATPN-97), pp. 82–101, Toulouse, France.
Novick, D., & Ward, K. (1993). Mutual beliefs of multiple conversants: A computational
model of collaboration in air traffic control. In Proceedings of AAAI-93, pp. 196–201.
Nowostawski, M., Purvis, M., & Cranefield, S. (2001). A layered approach for modeling
agent conversations. In Proceedings of the Second International Workshop on Infrastructure for Agents, MAS and Scalable MAS, the Fifth International Conference on
Autonomous Agents, pp. 163–170, Montreal, Canada.
Odell, J., Parunak, H. V. D., & Bauer, B. (2000). Extending UML in the design of multiagent systems. In Proceedings of the AAAI-2000 Workshop on Agent-Oriented Information Systems (AOIS-00).
Odell, J., Parunak, H. V. D., & Bauer, B. (2001a). Agent UML: A formalism for specifying
multi-agent interactions. In Ciancarini, P., & Wooldridge, M. (Eds.), Agent-Oriented
Software Engineering, pp. 91–103. Springer-Verlag, Berlin.
Odell, J., Parunak, H. V. D., & Bauer, B. (2001b). Representing agent interaction protocols in UML. In Ciancarini, P., & Wooldridge, M. (Eds.), Agent-Oriented Software
Engineering, pp. 121–140. Springer-Verlag, Berlin.
Parunak, H. V. D. (1996). Visualizing agent conversations: Using enhances Dooley graphs
for agent design and analysis. In Proceedings of the Second International Conference
on Multi-Agent Systems (ICMAS-96).
Paurobally, S., & Cunningham, J. (2003). Achieving common interaction protocols in open
agent environments. In Proceedings of the Workshop on Challenges in Open Agent
Systems, the Second International Joint Conference on Autonomous Agents & MultiAgent Systems (AAMAS-03), Melbourne, Australia.
Paurobally, S., Cunningham, J., & Jennings, N. R. (2003). Ensuring consistency in the
joint beliefs of interacting agents. In Proceedings of the Second International Joint
Conference on Autonomous Agents & Multi-Agent Systems (AAMAS-03), Melbourne,
Australia.
386

Representing Conversations for Scalable Overhearing

Petri Nets site (2003). Petri nets world: Online services for the international petri nets
community, at www.daimi.au.dk/petrinets..
Poutakidis, D., Padgham, L., & Winikoff, M. (2002). Debugging multi-agent systems using
design artifacts: The case of interaction protocols. In Proceedings of the First International Joint Conference on Autonomous Agents & Multi-Agent Systems (AAMAS-02),
pp. 960–967, Bologna, Italy.
Purvis, M. K., Hwang, P., Purvis, M. A., Cranefield, S. J., & Schievink, M. (2002). Interaction protocols for a network of environmental problem solvers. In Proceedings
of the 2002 iEMSs International Meeting:Integrated Assessment and Decision Support
(iEMSs 2002), pp. 318–323, Lugano, Switzerland.
Ramos, F., Frausto, J., & Camargo, F. (2002). A methodology for modeling interactions in
cooperative information systems using Coloured Petri nets. International Journal of
Software Engineering and Knowledge Engineering, 12 (6), 619–636.
Reisig, W. (1985). Petri Nets: An Introduction. Springer-Verlag.
Rossi, S., & Busetta, P. (2004). Towards monitoring of group interactions and social roles
via overhearing. In Proceedings of CIA-04, pp. 47–61, Erfurt, Germany.
Smith, I. A., & Cohen, P. R. (1996). Toward a semantics for an agent communications
language based on speech-acts. In Proceedings of AAAI-96.
Wikstrom, A. (1987). Functional Programming using Standard ML. International Series in
Computer Science. Prentice-Hall.
Xu, H., & Shatz, S. M. (2001). An agent-based Petri net model with application to
seller/buyer design in electronic commerce. In Proceedings of the 5th International
Symposium on Autonomous Decentralized Systems (ISAD-01), pp. 11–18, Dallas,
Texas, USA.

387

	
 	 

 
  !#"%$&(')$**+,-$..0/1$+2

34567 8%*9:*&<;>=5
  "8%*$:*+

?@BADCFE(GIH4JDKMLON(PDCQHRSFHT7RVUXWDCFEYPDKYW[Z\N(]^`_INbadceNb^fCFTgW
hjilkmilkgn-opQqsrqut-vYwexzy{n-kg|~}	-

r#kjg-y{xeon-kkr#kgjItbrrtk

xYnsrnqso#
¡£¢¤¦¥F¨0§ © ¢¤ª	«­¬¤¢®	¯°«¢±²¯±
¡£¢¤¦¥F¨0§ © ¢¤ª	³g´¶µg¯·s¯¤

-Yg Q 4 

¸¹`ºs»¦¼½F¾¶»
¿ÁÀÂfÃÄ{ÅsÆ¦ÇÉÈ)Ê0Ê!È ËÌ Â(ÀÂ4ÍÎ0È)ÊÏ0È)Ð4ÊQÑ1ÒsÎ%ÓÊ!Â<ÔÍÂ4ÕÏ0È)Å>ÌÅ>ÕÆÉÏ0Â4ÇÉÖ¶ÒsÎÅ>Ì×QÎ0Â4ØsÎ0Â<Ê0Ê!È ÒsÕÉÖÌ)Å>ÕÕÈ ÕØYÅ>Î0Â(Æ¦ÂÙ
Ú 
Õ Â<ÆÛËÜÝÅIÖÅ>ÎÅ>ÇÉÂÏ0Â4Î0È Þ4Â<ÆuÎ0Â4Ì)Å	ßÅ	Ï0È ÒsÕÝÒ>ÑÏ0ÀÂÉÒsÖ¦Ï0È ÇàÅ>ÌgÐÒÊÏ%Ñ1ÍÕÐÏ0È ÒsÕlÈ ÕÛÏ0ÀÂÉÎ0Â4ØsÎ0Â<Ê0Ê!È ÒsÕÛÊ!Â<Å>ÎÐÀ
Ê!ÖÅsÐÂsá¶â7ÀÂ4Î0ÂfÏ0ÀÂYÖÅ>ÎÅ>ÇÉÂÏ0Â4ÎbãäÒ>åÂ4ÎÊ(ÅàÏ0ÎÅsÆ¦ÂÙ#Ò>åÛË¶ÂÏâÂ4Â4ÕDÏ0ÀÂÉÅsÐ4ÐÍÎÅsÐÜIÅ>ÕÆuÐÒsÇÉÖÍ¦ÏÅ	Ï0È ÒsÕÅ>Ì
ÐÒÊÏæÒ>ÑÁÏ0ÀÂeÀÂ4ÍÎ0È)ÊÏ0È)Ð>çDèFß¦È)ÊÏ0È ÕØ­ÇÉÂÏ0ÀÒ¦ÆÊfÑ1ÒsÎYÐÒsÇÉÖÍ¦Ï0È ÕØ­Ï0ÀÂIÃÄéÀÂ4ÍÎ0È)ÊÏ0È)ÐÉÎ0Â<ÔÍÈ Î0ÂÉÏ0È ÇÉÂeÂßÙ
Ö¶ÒsÕÂ4ÕÏ0È)Å>Ì7È ÕãÛáÌ È ÇÉÈ Ï0È ÕØlÏ0ÀÂ4ÇêÏ0ÒëÊ!ÇàÅ>Ì Ì7ì	Å>Ì ÍÂ<Ê­Ó1ã[íïîs×çð¿ÁÀÂÝÃÄ\ÀÂ4ÍÎ0È)ÊÏ0È)ÐDÐ4Å>ÕÅ>Ì)Ê!ÒlË¶Â
ìÈ Â4âÂ<ÆzÅsÊFÏ0ÀÂ(ÒsÖ¦Ï0È ÇàÅ>ÌÐÒÊÏQÑ1ÍÕÐÏ0È ÒsÕeÈ ÕeÅfÎ0Â4Ì)Å	ßÅ	Ï0È ÒsÕàÒ>ÑÏ0ÀÂ`Ê!Â<Å>ÎÐÀàÊ!ÖÅsÐÂsñ-Ï0ÀÈ)ÊgÖÅ>Ö¶Â4ÎÖÎ0Â<Ê!Â4ÕÏÊ
°0¯ò ó4ô¯·à«4¯ó	°0õöá¶ÅÉÇÉÂÏ0ÀÒ¦ÆIÑ1ÒsÎ(ÐÒsÇÉÖÍ¦Ï0È ÕØÉÏ0ÀÈ)ÊÁÑ1ÍÕÐÏ0È ÒsÕ­ÖÅ>Î!Ï0È)Å>Ì Ì ÜzËÜIÊ!Â<Å>ÎÐÀÈ ÕØÉÈ ÕIÏ0ÀÂæÎ0Â4Ì)Å	ß¦Â<Æ
Ê!ÖÅsÐÂsçF¿ÁÀÂ(Î0Â4Ì)Å	ß¦Â<ÆzÊ!Â<Å>ÎÐÀàÇÉÂÏ0ÀÒ¦ÆáË¶Â<Ð4Å>ÍÊ!Â`È Ï7ÐÒsÇÉÖÍ¦Ï0Â<ÊÁÃ Ä ÒsÕÌ ÜÉÖÅ>Î!Ï0È)Å>Ì Ì ÜsáÈ)ÊÁÐÒsÇÉÖÍ¦ÏÅ	Ï0È ÒsÕ¦Ù
Å>Ì Ì ÜÛÐÀÂ<Å>Ö¶Â4ÎæÅ>ÕÆÛÏ0ÀÂ4Î0ÂÑ1ÒsÎ0ÂÉÍÊ0Å>ËÌ ÂÉÑ1ÒsÎæÀÈ ØsÀÂ4Îæì	Å>Ì ÍÂ<ÊfÒ>ÑÁãÛçI¿ÁÀÂÝÓÐÒsÇÉÖÌ ÂÏ0Â×fÃ÷àÀÂ4ÍÎ0È)ÊÏ0È)ÐÉÈ)Ê
ÐÒsÇøËÈ ÕÂ<Æeâ7È Ï0ÀDÖÅ>Î!Ï0È)Å>ÌÃÄùÀÂ4ÍÎ0È)ÊÏ0È)Ð4Ê4áÑ1ÒsÎÁãVúüûý4þ4þ4þ áÐÒsÇÉÖÍ¦Ï0Â<ÆeËÜzÎ0Â4Ì)Å	ß¦Â<ÆeÊ!Â<Å>ÎÐÀáÎ0Â<Ê!ÍÌ Ï0È ÕØ
È Õ­ÅYÇÉÒsÎ0ÂfÅsÐ4ÐÍÎÅ	Ï0Â`ÀÂ4ÍÎ0È)ÊÏ0È)Ð>ç
¿ÁÀÈ)Ê`ÍÊ!ÂøÒ>ÑQÏ0ÀÂYÎ0Â4Ì)Å	ß¦Â<Æ­Ê!Â<Å>ÎÐÀ­ÇÉÂÏ0ÀÒ¦Æ­Ï0ÒIÈ ÇÉÖÎ0Ò	ìsÂæÒsÕ­Ï0ÀÂàÃ÷øÀÂ4ÍÎ0È)ÊÏ0È)ÐøÈ)Ê(Â4ì	Å>Ì ÍÅ	Ï0Â<Æ­ËÜ
ÐÒsÇÉÖÅ>Î0È ÕØzÏâÒIÒsÖ¦Ï0È ÇàÅ>ÌÏ0Â4ÇÉÖ¶ÒsÎÅ>Ì£ÖÌ)Å>ÕÕÂ4ÎÊ4ñ(¿7ÿ áâ7ÀÈ)ÐÀÛÆ¦ÒÂ<Ê`ÕÒ>ÏfÍÊ!ÂYÈ Ï<á-Å>ÕÆ -4 áâ7ÀÈ)ÐÀ
ÍÊ!Â<ÊÁÈ Ï7ËÍ¦ÏbÈ)ÊÒ>Ï0ÀÂ4Î0â7È)Ê!Â`È)Æ¦Â4ÕÏ0È)Ð4Å>ÌÏ0Òà¿7ÿ çF¿ÁÀÂfÐÒsÇÉÖÅ>Î0È)Ê!ÒsÕzÈ)ÊÁÇàÅsÆ¦Â%ÒsÕzÏ0ÀÂæÆ¦ÒsÇàÅ>È ÕÊÍÊ!Â<ÆeÈ Õ
Ï0ÀÂÁî
²ÕÏ0Â4Î0ÕÅ	Ï0È ÒsÕÅ>ÌÿQÌ)Å>ÕÕÈ ÕØ gÒsÇÉÖ¶ÂÏ0È Ï0È ÒsÕáÈ Õæâ7ÀÈ)ÐÀfË¶Ò>Ï0ÀæÖÌ)Å>ÕÕÂ4ÎÊÖÅ>Î!Ï0È)ÐÈ ÖÅ	Ï0Â<Æç 7Â4Ì)Å	ß¦Â<Æ
Ê!Â<Å>ÎÐÀYÈ)ÊFÑ1ÒsÍÕÆYÏ0ÒfË¶Â(ÐÒÊÏQÂåÂ<ÐÏ0È ìsÂbÈ ÕzÊ!ÒsÇÉÂ7Ò>Ñ¶Ï0ÀÂ<Ê!Â(Æ¦ÒsÇàÅ>È ÕÊ4ásËÍ¦ÏgÕÒ>ÏgÅ>Ì Ìç bÕÅ>Ì Ü¦Ê!È)ÊFÎ0Â4ìsÂ<Å>Ì)Ê£Å
ÐÀÅ>ÎÅsÐÏ0Â4Î0È Þ<Å	Ï0È ÒsÕ%Ò>ÑÏ0ÀÂ7Æ¦ÒsÇàÅ>È ÕÊ-È Õøâ7ÀÈ)ÐÀøÎ0Â4Ì)Å	ß¦Â<ÆæÊ!Â<Å>ÎÐÀøÐ4Å>ÕæË¶ÂÁÂß¦Ö¶Â<ÐÏ0Â<ÆfÏ0Ò`Ë¶ÂÁÐÒÊÏ£ÂåÂ<ÐÏ0È ìsÂsá
È ÕIÏ0Â4Î0ÇàÊÁÒ>Ñ-ÏâÒÉÇÉÂ<ÅsÊ!ÍÎ0Â<ÊÒsÕeÏ0ÀÂfÒsÎ0È ØsÈ ÕÅ>ÌÅ>ÕÆeÎ0Â4Ì)Å	ß¦Â<ÆIÊ!Â<Å>ÎÐÀeÊ!ÖÅsÐÂ<Ê4ç ²ÕIÏ0ÀÂæÆ¦ÒsÇàÅ>È ÕÊÁâ7ÀÂ4Î0Â
Î0Â4Ì)Å	ß¦Â<ÆÊ!Â<Å>ÎÐÀÈ)ÊàÐÒÊÏÉÂåÂ<ÐÏ0È ìsÂsáÂß¦ÖÅ>ÕÆ¦È ÕØlÊ!ÇàÅ>Ì ÌbÊÏÅ	Ï0Â<ÊÉÈ)ÊàÐÒsÇÉÖÍ¦ÏÅ	Ï0È ÒsÕÅ>Ì Ì Ü ÐÀÂ<Å>Ö¶Â4ÎYÏ0ÀÅ>Õ
Âß¦ÖÅ>ÕÆ¦È ÕØÉÌ)Å>Î0ØsÂ%ÊÏÅ	Ï0Â<Ê7Å>ÕÆDÊ!ÇàÅ>Ì ÌÊÏÅ	Ï0Â<ÊÏ0Â4ÕÆeÏ0ÒàÀÅìsÂ%Ê!ÇàÅ>Ì ÌÊ!ÍÐ4ÐÂ<Ê0Ê!ÒsÎ7ÊÏÅ	Ï0Â<Ê4ç

	















 »¦¼(¾¶» 
!#"%$'&)(+*,*+-.!"/&10243+"%57643,898:3,445:';=<6>?&1"%57"%576@AB&"/&10%&CD"FE6.?G8:3,4'&102H1IKJL9M-N3,4C }	- O
JL9M-'@PEQ$45:R2$S3,8:H/6T?G3+0%"%5:R157?G3+"/&CU5:V"%$'&D(+*,*(WRX6>?&1"%57"%576@Y5:HZ3["/&>?6,023,8]\^JL_L!9`\W?G8:3,4'&10@
6,?4"%5:>a3,8Bbc:d1cfecV>a3+g,&H/?G3, O h"a5:HiG3,H/&CT6T"/&>?6,023,8j0%&1;,0%&H%H%576WH/&3+02R2$WEQ57"%$T3,k3,C4>a5:H%H%57iG87&
$'&l'025:H/"%5:RMR13,8:87&Cm $ O JL$'&"/&>?6,023,8'm $ $'&l'025:H/"%5:RM5:H	3,n5:4H/"%3,4RX&6,o4"%$'&`;,&'&1023,8'm'prqtsvuxw+y2(zy1{1{1{:|
o}3,>a5:87~6,o$'&l'025:H/"%5:R1H1@EQ$45:R2$k5:HC'&X'&CTiz~W3D?G3+023,>&1"/&102571&CW0%&8:3'3+"%576k6,oB"%$'&=6,?4"%5:>a3,8QRX6H/"
o}l44RX"%576W6,&10Z"%$'&.H/&3+02R2$SH/?G3,RX& O }	- 5:H5:C'&"%5:R13,8Q"/6[JL9M-D&X'RX&1?4")"%$43+")57"l4H/&H3D0%&RX&"%87~
C'&1,&876,?&C>&1"%$'6^C@4R13,8:87&C.d%X7z%%+d/X@4"/65:>?40%6,&B"%$'&]m $ $'&l'025:H/"%5:Rnq}Y3,H%8:l4>Z@G(+*,*+-3| O
_L&1;,0%&H%H%576?G8:3,4'&102HMR13+0%0%~6l'"3KH/&3+02R2$6,&10jH/&1"%H6,o;,63,8:H1@'H/"%3+0%"%5:';ot0%6>"%$'&Q;,63,8G;57,&5:
"%$'&P?G8:3,445:';?40%6,iG87&> O JL$'&P0%&8:3'3+"%576"%$43+"L87&3,C4H"/6"%$'&m'pr$'&l'025:H/"%5:R1H5:H"/63,H%H%l4>&P"%$43+"j"%$'&
RX6H/"M6,o3,~nH/&1"M6,o>6,0%&j"%$43,s;,63,8:HM&zl43,8:H	"%$'&LRX6H/"M6,o"%$'&L>6H/"MRX6H/"%87~H%l'iGH/&1"	6,oH%571&js O JL$'&
$'&l'025:H/"%5:RR13,l44C'&10%&H/"%5:>a3+"/&]"%$'&nRX6H/"B6,o3;,63,8H/&1"1@57o"%$'&10%&3+0%&5:"/&1023,RX"%5764HP5:,687^5:';>6,0%&
G	2 W f¡2¢¢2£XX¤G¥/2  ¦/§`¨©¦j¦h¨©ª§]«}2¬¬­ª/®¯ L%¨¦¦/%­¥F©°±2²j²³´ª2­¨²+%¨¦¥F©ª¯ ¥!¦Aª2³²+2j¦2¯ ¨¦!µXª¶X¦!4¨©¦
· ­ª²¸`¥!ª²²ª2¨F%¨¯ ª²!¹©¦	¨¦h­[­¦! %®¦/§Y¦/%­¥F©± · ©¯ ¥F©Pº,¦h¨¨¦h­A¥!ª2­­¦!¬,ª²+§¨ª¨©¦¯ ²1¨¯ ¨¯ ª²Y²+§¦h­ »¯ ²¸¨©¦
j¦h¨©ª§± · ¯  zº,¦¦/§Q¯ ²Y¨©¯ ¬+2¬,¦h­/
¼

G½

¾

f¿ ¾

$**+ 	8	0  
 
 " 8

À

4Á

4

"%$43,=sÂ;,63,8:H1@iGl'"Q57"QR13,N'&1,&10Y6,&10%&H/"%5:>a3+"/& O !"%$'&]JL9M-3,4C }	- "/&>?6,023,8?G8:3,4'&102H1@4"%$'&
RX6H/"P6,o3H/&1"P6,oM;,63,8:HB5:HQ"%$'&n>a5:45:>]l4>Ã"/6,"%3,8	&X^&R1l'"%576N"%5:>&,@6,0B>a3+g,&H/?G3,@6,o3,~?G8:3,="%$43+"
3,R2$457&1,&HB"%$'&n;,63,8:H1@iGl'"Y"%$'&H%3,>&0%&8:3'3+"%576.R13,.i&nl4H/&CN"/6C'&10257,&n$'&l'025:H/"%5:R1HQot6,0B0%&1;,0%&H%H%576
?G8:3,445:';EQ57"%$=C45´Ä&10%&"Q?G8:3,=H/"/02l4RX"%l'0%&HY3,4CNRX6H/"P>&3,H%l'0%&H1@Mc Åc:@H/&zl'&"%5:3,8A?G8:3,4HLEQ57"%$=H%l4>
6,o3,RX"%576SRX6H/"%Hq}Y3,H%8:l4>Z@jÆ6'&1"1@LÇÉÈY&XÄ'&10@Y(+*,*Ê|2@Q6,0"/6&H/"%5:>a3+"/&N0%&H/6l'02RX&NRX64H%l4>?4"%576
q}Y3,H%8:l4>ÉÇËÈY&XÄ'&10@M(+*,*'w| ONÌ 6,02>a3,8:87~,@Í"%$'&)m'pÎ$'&l'025:H/"%5:R+@ot6,0n3,~s@	R13,Di&C'&X'&CD3,HK"%$'&
H/68:l'"%576K"/6B3Y0%&8:3'3+"%576n6,o4"%$'&`6,?4"%5:>a3,8^RX6H/"	&zl43+"%576qtg^'6EQn3,H	"%$'&jÆ&8:8:>a3,K&zl43+"%576|EQ$45:R2$
R2$43+023,RX"/&102571&H"%$'&n6,?4"%5:>a3,8RX6H/"Bo}l44RX"%576N6,&10"%$'&aH/&3+02R2$#H/?G3,RX& OnÏ RX6>?G87&1"/&aH/68:l'"%576N"/6Z"%$'&
0%&8:3^&C[&zl43+"%576W5:HRX6>?Gl'"/&CT&X^?G8:5:R157"%87~,@iz~[H/687^5:';3.;,&'&1023,8:571&CTH%$'6,0%"/&H/"a?G3+"%$[?40%6,iG87&>Z@
?402576,0K"/6.H/&3+02R2$3,4CH/"/6,0%&C5:D3="%3+iG87&EQ$45:R2$D5:HKl4H/&CD"/6.R13,8:R1l48:3+"/&$'&l'025:H/"%5:R+3,8:l'&H6,oQH/"%3+"/&H
C4l'025:';nH/&3+02R2$ O
JL$'&]?G3+023,>&1"/&10PsÐ6+Ä&102HB3"/023,C'&XÑ6+Äi&1"FE&1&N"%$'&n3,R1R1l'023,RX~=6,o"%$'&n$'&l'025:H/"%5:R3,4CN57"%HYRX6>nÑ
?Gl'"%3+"%57643,8RX6H/"1I"%$'&)$457;$'&10s@	"%$'&>6,0%&)H%l'i4;,63,85:"/&1023,RX"%5764H]3+0%&"%3+g,&[5:"/6N3,R1RX6l4"3,4C
"%$'&R1876H/&10P"%$'&n$'&l'025:H/"%5:RK5:HQ"/6)"%$'&n"/02l'&nRX6H/"B6,o3,8:8A;,63,8:HB5:N3)H/"%3+"/&,@EQ$45:87&6."%$'&]6,"%$'&10$43,4C@
RX6>?Gl'"%5:';B"%$'&LH/68:l'"%576"/6"%$'&L0%&8:3^&CaRX6H/"M&zl43+"%5765:H?687~^'6>a5:3,8z5:"%$'&QH%571&j6,o"%$'&L?40%6,iG87&>
qt"%$'&=zl4>Ki&10n6,oB3+"/6>aH|]iGl'"n&X^?6'&"%5:3,8j5:[s O Æ&R13,l4H/&"%$'&R1l'0%0%&"a>&1"%$'6^CTot6,0RX6>?Gl'"!Ñ
5:';)"%$'&$'&l'025:H/"%5:R]RX6>?Gl'"/&H3D2Ò+ÓQÔ:1eFH/68:l'"%576."/6"%$'&0%&8:3^&C#RX6H/"&zl43+"%576@A"%$'&a$'&l'025:H/"%5:R
&X'$457iG57"%Hot6,0>6H/"]?G8:3,445:';?40%6,iG87&>aHK3SÕ/C45:>a5:45:H%$45:';)>a3+0%;5:43,8;3,5:4Ö^Ia64RX&)s×;,6z&Hn6,&103
RX&10%"%3,5:="%$'0%&H%$'68:CDqt"F~z?G5:R13,8:87~,@4sØur(|j"%$'&]5:>?40%6,&>&"Qi40%6l';$"Liz~Z"%$'&Kl4H/&K6,oMm'p`ÙAY6,&10m'p
i&RX6>&HjH%>a3,8:87&10`ot6,0L5:4RX0%&3,H%5:';]s O JL$45:HRX6>KiG5:'&H`"/6a>a3+g,&B"%$'&B>&1"%$'6^CZRX6H/"j&XÄ&RX"%57,&,@G5:"%$'&
H/&4H/&`"%$43+""%$'&L$'&l'025:H/"%5:RM0%&C4l4RX&HH/&3+02R2$"%5:>&L>6,0%&`"%$43,n"%$'&j"%5:>&`0%&zl4570%&C]"/6RX6>?Gl'"/&j57"1@6487~
ot6,0YH%>a3,8:8+3,8:l'&HL6,osÚqt"F~z?G5:R13,8:87~,@GsÐÛU(| O Q6E&1,&10@"%$'&]m $ $'&l'025:H/"%5:RP5:HL6,ot"/&="/6z6E&3+g O JL$'&
zl'&H/"%576)3,C4C'0%&H%H/&C$'&10%&P5:H`57o3n>6,0%&B3,R1R1l'023+"/&YÜ3,4CRX6H/"j&XÄ&RX"%57,&YÜ$'&l'025:H/"%5:RLR13,)i&PC'&10257,&C
5:D"%$'&m'pÎot023,>&1E6,0%g O JL$'&5:C'&3=6,oQ0%&8:3^&C[H/&3+02R2$T5:HK"/6RX6>?Gl'"/&m'pÐqtot6,0$457;$'&10]sN|6487~
Ô'+dXe}Ý+t Þ"/63,65:CZ"%$'&B&X^?6'&"%5:3,85:4RX0%&3,H/&5:)RX6>?Gl'"%3+"%57643,8RX6H/" O JL$'&3,87"/&10243+"%57,&BE6l48:C)6,o
RX6l'02H/&Bi&P"/6a3+iG3,4C'6)"%$'&m'prot023,>&1E6,0%g3,4CZ876z6,g3+"j6,"%$'&10Q3+?4?40%63,R2$'&Hj"/6aC'&10257^5:';n3,C4>a5:H%H%5´Ñ
iG87&B$'&l'025:H/"%5:R1H`ot6,0L6,?4"%5:>a3,8"/&>?6,023,8?G8:3,445:';'@ziGl'"j"%$'&10%&3+0%&K'6,"Q>a3,~"/6ai&Bot6l44CI	&X'5:H/"%5:';
>a3+g,&H/?G3,^Ñ6,?4"%5:>a3,84"/&>?6,023,8'?G8:3,4'&102H	&57"%$'&10l4H/&L"%$'&L"/&>?6,023,8Gm $ $'&l'025:H/"%5:RBq%c Åc:@'<`9JB@,ßP5:C43,8
ÇàÈY&XÄ'&10@G(+*,*+-z|6,0L6,i4"%3,5:)&H/"%5:>a3+"/&Hjot0%6>3n"/&>?6,023,8?G8:3,445:';K;,023+?G$q%c Åc:@4JYÈP9Í@\^>a57"%$)Ç
á&8:C@jwâ,â,âz@M3,4CJL9`\z~^H1@MÈP3+0%025:C'6'@MãP43,5:4C45:3^@ÇäÆ`3+0%i&10@(+*,*'w|2@MEQ$45:R2$D3,8:H/6=&4RX6^C'&HK"%$'&)m $
$'&l'025:H/"%5:RY"%$'6l';$=RX6>?Gl'"/&C=5:3aC45´Ä&10%&"Lo}3,H%$4576 O JL$'&KC'6>a3,5:^ÑF5:4C'&1?&4C'&"`$'&l'025:H/"%5:R1Hjl4H/&C
5:6,"%$'&10`"/&>?6,023,8G?G8:3,4'&102H1@H%l4R2$)3,H`åA9`ÈP9æq}å6';Ç Ì 6@G(+*,*ç|6,0`F'JA&Jxq}JA025:4zl43+0%"1@'(+*,*ç|2@
3+0%&al4H/&C."/6&H/"%5:>a3+"/&"%$'&aC45:H/"%3,4RX&"/6Z"%$'&a'&3+0%&H/"KH/68:l'"%576.5:N"%$'&aH/&3+02R2$DH/?G3,RX&,@A023+"%$'&10B"%$43,
"%$'&KRX6H/"]qhÝc}c:@4>a3+g,&H/?G3,|6,o	"%$43+"QH/68:l'"%576 O
JL$'&)0%&8:3'3+"%576Tl44C'&10287~^5:';="%$'&=m'pä$'&l'025:H/"%5:R1HnR13,[i&)&X^?G8:3,5:'&C[5:"/&102>aH6,oY"%$'&ZH/&3+02R2$
H/?G3,RX&,@A023+"%$'&10"%$43,5:."/&102>aHB6,o`H/68:l'"%576.RX6H/"1IK3,~H/&1"B6,o`>6,0%&"%$43,sè;,63,8:H5:HZÕ/H/?G8:57"%Ö)5:"/6
?40%6,iG87&>aHQ6,osÉ;,63,8:HP&3,R2$@EQ$45:R2$.3+0%&H/687,&C.5:4C'&1?&4C'&"%87~#qt"%$'&H/?G8:57"Y5:HP'6,"B3?G3+0%"%57"%57645:';'@
H%5:4RX&+tH%l'iGH/&1"%HY6,o`H%571&nsè3+0%&aH/687,&C| O JL$'&n0%&8:3^&CRX6H/"B&zl43+"%5765:HB3,8:H/6)"%$'&6,?4"%5:>a3,8	RX6H/"
&zl43+"%576Not6,0B"%$45:HQ0%&8:3^&C.H/&3+02R2$#H/?G3,RX&,@EQ$45:R2$=é 8:8ÍR13,8:8Í"%$'&nsêhd%Å,d%2ÝÒ+ëH/?G3,RX& O JL$'&]0%&8:3^&C
H/&3+02R2$>&1"%$'6^C.RX64H%5:H/"%HY5:NH/687^5:';"%$'&n?G8:3,445:';a?40%6,iG87&>ÉqhÝc}c:@H/&3+02R2$45:';ot0%6>Ë"%$'&n"/6,?87&1,&8
;,63,8:H|	5:]"%$'&js)Ñ0%&1;,0%&H%H%576aH/?G3,RX& Oì l'025:';P"%$'&LH/&3+02R2$@?G3+0%"%H	6,o"%$'&LH/68:l'"%576n"/6B"%$'&L0%&8:3^&CRX6H/"
&zl43+"%5763+0%&PC45:H%RX6,&10%&C@'3,4Ca"%$'&H/&P3+0%&PH/"/6,0%&C)5:3"%3+iG87&Yot6,0`8:3+"/&10`l4H/&,@í%l4H/"3,H`5:a"%$'&Q?40%&1^576l4H
3+?4?40%63,R2$ O Æ&R13,l4H/&`"%$'&0%&8:3^&C]H/&3+02R2$]6487~P^5:H%57"%HÔ'+dXez6,o4"%$'&s)Ñ0%&1;,0%&H%H%576]H/&3+02R2$]H/?G3,RX&,@57"ÍR13,
i&L&X^?&RX"/&C"/6Ki&L3+iG87&L"/6nC'6KH/6]>6,0%&Qzl45:R%g^87~]"%$43,>&1"%$'6^C4HM"%$43+"iGl45:8:Cn3KH/68:l'"%576"/6K"%$'&YRX6H/"
&zl43+"%576not6,0M"%$'&j&"%570%&`s)Ñ0%&1;,0%&H%H%576H/?G3,RX& O <64H/&zl'&"%87~,@57"MR13,ni&j3+?4?G8:57&C]ot6,0M$457;$'&10+3,8:l'&H


î>û

î Á(ï^ðzñ7òGó À

4ï %ô :õ Yö 4ï^ð-Gó-N÷æ s4ø- ù ¦Gï^õ¶

  1   f

6,o	s O \^5:4RX&B"%$'&0%&8:3^&C=H/&3+02R2$N5:HLC'6'&ot0%6>Ã"%$'&K;,63,8:HL6,o"%$'&?G8:3,445:';n?40%6,iG87&>Z@'"%$'&K?G3+0%"L6,o
"%$'&)m'pÎH/68:l'"%576"%$43+"]5:HKRX6>?Gl'"/&C5:HK3,8:H/6N8:57g,&87~Ni&a"%$'&>6H/"K0%&87&1+3,"K?G3+0%" O JL$'&RX6>?G87&1"/&
3,4C.?G3+0%"%5:3,8Mm'p$'&l'025:H/"%5:R1HPRX6>?Gl'"/&C.ot6,0KC45´Ä&10%&"Psúiz~N"%$'&"FE6=>&1"%$'6^C4HBR13,i&RX6>KiG5:'&C
iz~>a3'5:>a573+"%576@z0%&H%l487"%5:';]5:3n>6,0%&B3,R1R1l'023+"/&B43,8$'&l'025:H/"%5:R+@^$'6,?&1o}l48:87~3+"j3nRX6>?Gl'"%3+"%57643,8
RX6H/"L"%$43+"Q5:HL'6,"j;,0%&3+"/&10Y"%$43,57"%Hj+3,8:l'& O
JL$45:Hn?G3+?&10>a3+g,&H"FE6>a3,5:WRX6"/0257iGl'"%5764H1I Ì 5702H/"1@`57"?40%6^5:C'&Ha3DC'&1"%3,5:87&CT?40%&H/&"%3+"%576
6,oP"%$'&Z0%&8:3^&CWH/&3+02R2$k>&1"%$'6^CW3,4CT$'6Eä"%$45:H>&1"%$'6^CW5:H3+?4?G8:57&C"/6DR18:3,H%H%5:R13,8L3,4CT"/&>?6,023,8
0%&1;,0%&H%H%576[?G8:3,445:'; ODÏ 87"%$'6l';$["%$'&>&1"%$'6^CT5:Hn?40%&H/&"/&CT5:["%$'&RX6"/&X^"a6,oY?G8:3,445:';'@M57"5:H
zl457"/&N;,&'&1023,8P3,4CV>a3~ki&.3+?4?G8:5:R13+iG87&"/6[6,"%$'&10ZH/&3+02R2$S?40%6,iG87&>aH3,H)E&8:8 O !4C'&1&C@QH%5:>a5:8:3+0
"/&R2$445:zl'&H$43,&Yi&1&3+?4?G8:57&Cn"/6K?G8:3,445:';B3,4Ca6,"%$'&10`H%5:';87&L3+;,&"`H/&3+02R2$?40%6,iG87&>aHQq}9M0257&C457"%5:H1@
wâ,â,çzû`ül4';$43,44HKÇv\^R2$43+&XÄ&10@(+*,*'w|K3,4C#"/6.RX64H/"/023,5:"K6,?4"%5:>a573+"%576VqÈP5:4H/i&10%;NÇÃY3+0%,&1~,@
wâ,â,(zûß&10%o}3,5:8:8:57&,@å&>a3,57"/0%&,@+ÇS\^R2$457&X@'wâ,â,ý| O JL$'&M0%&8:3+"%576K"/6Y"%$'&H/&5:C'&3,HÍ3,4C"/&R2$445:zl'&HÍ5:HA3,8:H/6
C45:H%R1l4H%H/&C O \z&RX64C@57"Í?40%&H/&"%HÍ"%$'&`0%&H%l487"%HA6,o3,]&X^"/&4C'&Cn3,43,87~^H%5:HA6,oG"%$'&0%&8:3+"%57,&`?&10%ot6,02>a3,4RX&
6,oLJL9M-N3,4C }	- 5:#"%$'&C'6>a3,5:4H6,oj"%$'&?G8:3,445:';ZRX6>?&1"%57"%576 O JL$'&?G5:RX"%l'0%&a"%$43+"K&>&10%;,&H
ot0%6>à"%$45:H3,43,87~^H%5:HM5:HH/6>&1EQ$43+"C45´Ä&10%&"Mot0%6>à"%$43+";57,&aiz~"%$'&YRX6>?&1"%57"%576a0%&H%l487"%H O !a?G3+0%"
"%$45:HQ5:HYC4l'&K"/6"%$'&]"%5:>&XÑ?&10/Ñ?40%6,iG87&>ä8:5:>a57"L5:>?6H/&C=5:"%$'&nRX6>?&1"%57"%576@H%5:4RX&K"%$'&n3,C'+3,"%3+;,&
6,o }	- 6,&10aJL9M-N5:H]>a3,5:487~N6þÕ/$43+02C4Ö?40%6,iG87&>aH1@ÍEQ$45:R2$#0%&zl4570%&3N876,"K6,oj"%5:>&"/6.H/687,&ot6,0
i6,"%$=?G8:3,4'&102H O !=?G3+0%"1@57"Y5:HY3,8:H/6i&R13,l4H/&]"%$'&],&102H%576N6,o }	- l4H/&C.5:"%$'&nRX6>?&1"%57"%576NE`3,H
iGl';,;,~ O JL$'&>a3,5:0%&H%l487"6,oj"%$'&3,43,87~^H%5:H1@Í$'6E&1,&10@M5:HK3NR2$43+023,RX"/&102573+"%5766,oj"%$'&C'6>a3,5:4H5:
EQ$45:R2$n0%&8:3^&CaH/&3+02R2$aR13,ai&`&X^?&RX"/&Ca"/6i&jRX6H/"M&XÄ&RX"%57,&,IM5:H%l4R2$aC'6>a3,5:4H1@,&X^?G3,4C45:';PH%>a3,8:8
H/"%3+"/&HM5:H	RX6>?Gl'"%3+"%57643,8:87~KR2$'&3+?&10"%$43,]&X^?G3,4C45:';P8:3+0%;,&`H/"%3+"/&H1@z3,4CnH%>a3,8:8^H/"%3+"/&H"/&4C]"/6$43,&
H%>a3,8:8'H%l4R1RX&H%H/6,0H/"%3+"/&H O h"5:HM3,8:H/6KH%$'6EQ"%$43+"M"%$'&H/&QRX0257"/&1025:3BR13,ai&KqtE&3+g^87~4|	zl43,"%5´G&Ciz~n"FE6
>&3,H%l'0%&H1@5:,687^5:';Y"%$'&`0%&8:3+"%57,&`0%&1;,0%&H%H%576ni4023,4R2$45:';Qo}3,RX"/6,02HM3,4C]"%$'&LH%571&6,oH/"%3+"/&H;,&'&1023+"/&C
iz~0%&1;,0%&H%H%576@45:)"%$'&6,0257;5:43,83,4CZ0%&8:3^&CH/&3+02R2$=H/?G3,RX&H O
ÿ	

Q¼A

½F¾

 

J $'&JL9M-?G8:3,4'&10B4C4HB"/&>?6,023,8?G8:3,4HBot6,0a\^JL_L!9`\=?40%6,iG87&>aHBEQ57"%$#C4l'023+"%57,&3,RX"%5764H O JL$'&
L
?G8:3,4HÍot6l44C3+0%&j6,?4"%5:>a3,8bc:d1cfec>a3+g,&H/?G3,@4Ýc}c:@"%$'&j"/6,"%3,84&X^&R1l'"%576"%5:>&j6,o"%$'&j?G8:3, $ @3,4C"%$'&
?G8:3,4'&10M5:HM3,8:H/6K3+iG87&j"/6K&4H%l'0%&j"%$43+"?G8:3,4HC'6K'6,"M^5768:3+"/&QRX&10%"%3,5:ag^5:4C4H	6,o0%&H/6l'02RX&QRX64H/"/023,5:"%H O
JL$'&n>a3,5:=E6,0%g^5:';?4025:4R157?G87&Hj6,oJL9M-)3+0%&3ot6,02>]l48:3+"%576=6,o30%&1;,0%&H%H%576.H/&3+02R2$.H/?G3,RX&not6,0
"/&>?6,023,8?G8:3,445:';=3,4CD"%$'&m'pÃo}3,>a5:87~.6,oY3,C4>a5:H%H%57iG87&$'&l'025:H/"%5:R1H1@Íi40%6l';$"n"/6,;,&1"%$'&10a"%$'0%6l';$
"%$'&] ìL
Ï  H/&3+02R2$3,87;,6,0257"%$4> O JL$'&H/&,@3,4CN"%$'&]6,&1023,8:83+02R2$457"/&RX"%l'0%&]6,o"%$'&]?G8:3,4'&10@3+0%&ni40257	& G~
C'&H%RX0257i&C]5:K"%$45:HÍH/&RX"%576ûz>6,0%&`C'&1"%3,5:8:HÍ6n"%$'&?G8:3,4'&10	R13,ni&ot6l44C]5:]&3+028:57&10Í?G3+?&102Hjq}Y3,H%8:l4>
ÇËÈY&XÄ'&10@M(+*,*,*^@M(+*,*'w+ûY3,H%8:l4>Z@(+*,*+-,i| O JA6N?40%6^5:C'&aiG3,R%gz;,0%6l44C#ot6,0n3=R187&3+0%&10nC'&H%RX0257?4"%576
6,oÍ0%&8:3^&CH/&3+02R2$=5:"%$'&K'&X^"LH/&RX"%576@GH/&3+02R2$H/?G3,RX&K3,4C$'&l'025:H/"%5:R1H`3+0%&B&X^?G8:3,5:'&CG02H/"jot6,0L"%$'&
H%5:>?G87&10jR13,H/&K6,oH/&zl'&"%5:3,8?G8:3,445:';'@^ot68:876E&CZiz~)"%$'&570L3,C43+?4"%576"/6a"%$'&"/&>?6,023,8R13,H/& O`Ï 8:H/6'@
RX&10%"%3,5:"/&R2$445:R13,8C'&1"%3,5:8:HM"%$43+"`3+?4?&3+05:>?6,0%"%3,"5:a&X^?G8:3,5:45:';B"%$'&Qi&$43^576l'06,o }	- 0%&8:3+"%57,&
"/6JL9M-a5:)"%$'&KRX6>?&1"%57"%576C'6>a3,5:4H`EQ5:8:8Gi&$457;$48:57;$"/&C O
¡1
f¨j©ª §º,¦P²ª2¨¦/§±©ª · ¦!µX¦h­/±¨©+%¨j¨©¦P¬ 2²L2¶X¦!¬+2²¯ Lª¬¨¯ L2 · ¯ ¨©­¦!¬,¦!¥h¨L¨ªK¨©¦P¦!L2²1¨¯ ¥!`¨©+%¨
¹ 4£L2j¦!³´ª2­A¨¦!j¬,ª2­F2z¬ 2²²¯ ²¸± · ©¯ ¥F©B§¯ ¦h­Aªj¦ · ©+%¨A³ ­ªT¨©+%¨A¬,¦!¥!¯ +¦/§Y³´ª2­^¡1 Q 7¦!¦¦!¥
¨¯ ª²)¡1 ¡1 %¤FZ¹zª]L2¶X¦Q¬ 2²j2¥!¥!¦!¬¨F2º ¦P¨ª¨©¦^¡1 P¬ 2²µ2 ¯ §%¨ª2­L¯ ¨j¯ j²¦!¥!¦!%­»a¨ª]¯ ²¦h­¨jªj¦
« · ©¯ ¨¦!¬+2¥!¦!°j¯ ²1¨ª¨©¦	¬ 2²±¯ ²¥h­¦/2¯ ²¸`¨©¦	L2¶X¦!¬+2²Y ¯ ¸©1¨ »1



î>û

À

4Á

4

 Û¶-¶q>qsrtkmxeon-kkr#kg"!#F%$Qgkrn-ofInq>


á&Q3,H%H%l4>&`"%$'&LH/"%3,4C43+02Cn?40%6,?6H%57"%57643,8'\^JL_L!9`\K>6^C'&8z6,o?G8:3,445:'; OAÏ ?G8:3,445:';Q?40%6,iG87&>Îq'&]|
RX64H%5:H/"%HM6,o3H/&1"6,o3+"/6>aH1@^3KH/&1"M6,o3,RX"%5764H3,4Ca"FE6]H%l'iGH/&1"%H6,o3+"/6>aH1I	"%$'6H/&Q"/02l'&Q5:n"%$'&Y5:457"%5:3,8
H/"%3+"/&.'q (^|B3,4C"%$'6H/&0%&zl4570%&C."/6i&a"/02l'&a5:"%$'&a;,63,8H/"%3+"/&.*q )K| ,
O + 3,R2$3,RX"%576.-.5:HC'&H%RX0257i&C
iz~3.H/&1"n6,oQ?40%&RX64C457"%5763+"/6>aH=q /01'q -'|/|2@EQ$45:R2$[$43,&Z"/6$'68:C[5:3.H/"%3+"/&Zot6,0"%$'&Z3,RX"%576["/6
i&a&X^&R1l'"%3+iG87&,@	3,4CDH/&1"%HK6,oj3+"/6>aH]>a3,C'&a"/02l'&N'q -%232'q -'|/|K3,4Co}3,8:H/&N'q 2%154/'q -'|/|Biz~"%$'&3,RX"%576 OZÏ
H/68:l'"%576.?G8:3,.5:H3,&X^&R1l'"%3+iG87&aH/&zl'&4RX&a6,0KH%R2$'&C4l487&n6,oj3,RX"%5764HB&4C45:';Z5:.3H/"%3+"/&aEQ$'&10%&a3,8:8
;,63,83+"/6>aH`$'68:C O JL$'&Q&X'3,RX"?G8:3,aot6,02> C'&1?&4C4H6"%$'&Y>&3,H%l'0%&Q6,?4"%5:>a571&CI	5:a"%$'&PH/&zl'&"%5:3,8
R13,H/&,@3RX6H/"P5:HQ3,H%H/6^R15:3+"/&C="/6&3,R2$.3,RX"%576[q'6	7985:1'q -'<
| ;S*|2@3?G8:3,5:HQ3H/&zl'&4RX&]6,oM3,RX"%5764H1@3,4C
"%$'&KH%l4>6,o	"%$'&570LRX6H/"%HQ5:Hj"%$'&RX6H/"Q6,oÍ"%$'&?G8:3, O
_L&1;,0%&H%H%5765:HK3=?G8:3,445:';>&1"%$'6^CD5:#EQ$45:R2$#"%$'&)H/&3+02R2$ot6,0n3?G8:3,D5:H]>a3,C'&5:#"%$'&H/?G3,RX&
6,o.Õ!?G8:3,S"%3,5:8:H%Ö^@Y?G3+0%"%5:3,8P?G8:3,4H"%$43+"3,R2$457&1,&#"%$'&;,63,8:HZ?40%6^5:C'&CS"%$43+"Z"%$'&?40%&RX64C457"%5764H6,o
"%$'&)?G3+0%"%5:3,8?G8:3,[3+0%&Z>&1" O \z&3+02R2$[&4C4HKEQ$'&[3N?G8:3,D"%3,5:8EQ$'6H/&)?40%&RX64C457"%5764HK3+0%&Z3,870%&3,C'~
H%3+"%5:H!G&CDiz~"%$'&)5:457"%5:3,8MH/"%3+"/&Z5:Hot6l44C O=Ì 6,0aH/&zl'&"%5:3,8?G8:3,445:';'@"%$'&?40%&RX64C457"%5764H?40%6^5:C'&
3H%>l =R157&"aH%l4>a>a3+0%~D6,oY"%$'&Z?G8:3,["%3,5:8 O JL$zl4H1@3#H/&zl'&"%5:3,8`0%&1;,0%&H%H%576TH/"%3+"/&N5:H3H/&1"1<
@ 8@6,o
3+"/6>aH1@0%&1?40%&H/&"%5:';H%l'i4;,63,8:HQ"/6)i&]3,R2$457&1,&C OPÏ N3,RX"%576?-R13,Ni&]l4H/&C="/60%&1;,0%&H%HP3H/"%3+"/
& 8
5´@
Ä 2%154/'q -'| AB8PD
u Cz@'3,4C"%$'&Y0%&H%l487"6,o0%&1;,0%&H%H%5:';B8Y"%$'0%6l';,
$ -a5:E
H 85FGu q 8HG@-%232'q -'|/| IJ/01'q -'| O JL$'&
H/&3+02R2$=H/"%3+0%"%HLot0%6>Î"%$'&H/&1"L6,o	;,63,8:K
H ) 3,4CZ&4C4HjEQ$'&Z3aH/"%3+"/L
& 8MN(5:H`0%&3,R2$'&C O

 OÛ¶-¶q>qsrtkmxeon-kkr#kg"!Dw`bt-n-ofInq>
 

!D"%$'&)R13,H/&Z6,oL"/&>?6,023,8?G8:3,445:';'@A&3,R2$W3,RX"%576[$43,Hn3NC4l'023+"%576Sq'2P04q'-'|;*| O JL$'&?G8:3,5:H
3NH%R2$'&C4l487&,@EQ$'&10%&)3,RX"%5764Hn>a3~D&X^&R1l'"/&Z5:D?G3+023,8:87&8Yq}H%l'i^í/&RX"]"/6.0%&H/6l'02RX&)3,4C[RX6>?G3+"%57iG5:8:57"F~
RX64H/"/023,5:"%H|2@^3,4C"%$'&Q6,i^í/&RX"%57,&Q"/6]>a5:45:>a571&j5:H"%$'&Q"/6,"%3,8G&X^&R1l'"%576a"%5:>&,@6,0>a3+g,&H/?G3, O áS$'&
3,RX"%576=C4l'023+"%5764Hj3+0%&3,8:8&zl43,8"/6Nw+@4"%$'&H/?&R15:3,8R13,H/&K6,oÍ?G3+023,8:87&8?G8:3,445:';K0%&H%l487"%H O

QSRTQSR'UWVYX[Z]\S^.Z"_N`JaBaBbcQSRTQedfZ]g  h3ikj _ml^

JL9M-K3,4C }	- C'6]'6,"H%l'?4?6,0%"M3,~6,o"%$'&Y'&1Ekot&3+"%l'0%&H5:"/0%6^C4l4RX&Ca5:9 ìPì åÍ( O (z@^"%$'&Q?40%6,iG87&>
H/?&R15´R13+"%5768:3,';l43+;,&]ot6,0Q"%$'&n(+*,*+-)RX6>?&1"%57"%576q + C'&87g+3,>?.ÇàQ6+Ä>a3,4@(+*,*+-z| O JL$'&?G8:3,^Ñ
'&102HjH%l'?4?6,0%"`C4l'023+"%57,&B3,RX"%5764H1@'6,iz^576l4H%87~,@'iGl'"`"%$'&H/&3+0%&5:"/&10%?40%&1"/&CZ5:)3>a3,4'&10j"%$43+"LC45´Ä&102H
ot0%6>×"%$'&N9 ìPì åÍ( O w.H/?&R15´R13+"%576þq Ì 6UÇÐå6';'@Q(+*,*ç| OxÌ 6,0)?4023,RX"%5:R13,8Q?Gl'0%?6H/&H1@`JL9M-[3,4C
}	- 3,R1RX&1?4""%$'&9 ìPì åÍ( O waH/~^"%3 o
O n l4>&1025:RnH/"%3+"/&a+3+025:3+iG87&Haq}R13,8:87&CæÕ l'&"%H%ÖZ5:9 ìPì åÍ( O w|
3+0%&KH%l'?4?6,0%"/&C6487~)5:ZRX&10%"%3,5:Zot6,02>aHj6,o	l4H/& O
JL$'&KH/&>a3,"%5:R1HQ"%$43+"YJL9M-3,4C }	- 3,H%H%l4>&ot6,0YC4l'023+"%57,&K3,RX"%5764HY3+0%&K&H%H/&"%5:3,8:87~"%$'6H/&]5:^Ñ
"/0%6^C4l4RX&Caiz~)\^>a57"%$a3,4Cá&8:C.q!wâ,â,â|ot6,0"%$'&YJYÈP9?G8:3,4'&10 O	Ì 6,0j3,3,RX"%576p-n"/6]i&L&X^&R1l'"%3+iG87&
6,&10]3"%5:>&5:"/&10%+3,r
8 q :sy :tN2P04'q -'*| u@A3+"/6>aH5:o/01'q -'|Y>]l4H/"Pi&]"/02l'&3+f
" :@3,4C.?&102H%5:H/"/&"Y?40%&XÑ
RX64C457"%5764Haq}3+"/6>aHK5:,/1v04'q -'|YD
u /01'q -'
| Gw2%154/'q -'|/|B>]l4H/"B0%&>a3,5:."/02l'&6,&10K"%$'&a&"%570%&a5:"/&10%+3,8 O
+ Ä&RX"%Hn6,oL"%$'&)3,RX"%576"%3+g,&Z?G8:3,RX&3+"H/6>&?65:"]5:D"%$'&)5:"/&102576,0]6,oL"%$'&)5:"/&10%+3,8@3,4CR13,[i&
0%&8:57&C6)"/6$'68:C)3+"j"%$'&B&4C)?65:" O JjE6a3,RX"%5764H1@ -3,4Cx-3Ft@43+0%&3,H%H%l4>&C)"/6i&a2Ò+ÓQÔ',e}*Ý yX:X@5:
"%$'&]H/&4H/&]"%$43+"Q"%$'&1~=R13,=i&K&X^&R1l'"/&C.5:6,&1028:3+?4?G5:';5:"/&10%+3,8:HLEQ57"%$'6l'"Q5:"/&10%ot&1025:';aEQ57"%$&3,R2$
6,"%$'&10`5´Ä='&57"%$'&103,RX"%576)C'&87&1"/&H3,3+"/6> "%$43+"`5:H3K?40%&RX64C457"%5766,o6,0`3,C4C'&Caiz~a"%$'&Q6,"%$'&10@Ýc}c:@
5´z
Ä 2%154/'q -'"
| A{/01'q -3F:|M|
u 2%154/'q -'"
| Ap-%232'q -3Ft|D
u C3,4CZ^5:RX&B,&102H%3 O
JL$45:HA5:"/&10%?40%&1"%3+"%576K6,oC4l'023+"%57,&3,RX"%5764HÍ0%&H/?&RX"%HÍ"%$'&nÕ/'6P>6^5:';Y"%3+0%;,&1"%ÖY02l487&6,oG9 ìPì åÍ( O w+@
iGl'"5:3]C45´Ä&10%&"E`3~IM5:4H/"/&3,C6,o0%&zl457025:';B?G8:3,4HM"/6n&X^?G8:5:R157"%87~nH/&1?G3+023+"/&P3,)3,RX"%576C'&1?&4C45:';
6D3=RX64C457"%576ot0%6>Â"%$'&&XÄ&RX"]"%$43+"K&H/"%3+iG8:5:H%$'&HB"%$'&RX64C457"%576@A"%$'&H/&>a3,"%5:R1H0%&zl4570%&HB"%$43+"

~}

î>û

î Á(ï^ðzñ7òGó À

	@5
¢
Ì 57;l'0%&aw+I

	

4ï %ô :õ Yö 4ï^ð-Gó-N÷æ s4ø- ù ¦Gï^õ¶

  1   f

p  	s@ 	~
,5  	s@5 	 ~¡25¢ 

5x 	5 
	2 5¢¢ 5
B5
 C'6>a3,5:| O JL$'&P?G8:3,
Ï "/&>?6,023,8?G8:3,qt"%$'&H/68:l'"%576"/6?40%6,iG87&>%ot0%6>"%$'&K3S]
3,8:H/6.RX6"%3,5:4H]"FE6#3,RX"%5764He3Z3,4C¡ @EQ$45:R2$3+0%&)'6,"]^5:H%57iG87&ai&R13,l4H/&
"%$'&1~$43,&B1&10%6aC4l'023+"%576 OÏ RX"%5764Hos%¢933,4CNs%¢9¡ 
 3+0%&H/&1?G3+023+"/&C)i&R13,l4H/&
6,o3n0%&H/6l'02RX&KRX6>5:RX" O JL$'&>a3+g,&H/?G3,6,o	"%$'&B?G8:3,Z5:HQÊ9,£ ( O
5x5

X^+ëzÅzae¤YÔ7Ýtë[=e}ÝtÓÝtëGeFXd¥++ O JL$45:HY>a3+g,&HC4l'023+"%57,&n3,RX"%5764HBH/"/025:RX"%87~=87&H%HY&X^?40%&H%H%57,&
"%$43,)5:9 ìPì åÍ( O w+@'EQ$'&10%&Y&XÄ&RX"%HjR13,i&PH/?&R15´G&Ca"/6n"%3+g,&B?G8:3,RX&Y&X'3,RX"%87~3+""%$'&BH/"%3+0%"6,0`&4C6,o
3,3,RX"%576 O !a?G3+0%"%5:R1l48:3+0@57"C'6z&H'6,"H%l'?4?6,0%"M3,RX"%5764HM"%$43+"`>a3+g,&Y3KRX64C457"%576"/02l'&Q6487~nC4l'025:';
"%$'&570&X^&R1l'"%576#qhÝc}c:@'3,C4C3,Z3+"/6>Î3+"`"%$'&PH/"%3+0%"`6,oA"%$'&P3,RX"%576Z3,4C)C'&87&1"/&P57"`3+;3,5:)3+"`"%$'&P&4C|2@
EQ$45:R2$?40%&1,&"/&C=JL9M-3,4C }	- ot0%6>ÎH/687^5:';n"%$'&BRX6>?G5:87&C),&102H%5764H`6,oA?40%6,iG87&>aHEQ57"%$)"%5:>&C
5:457"%5:3,88:57"/&1023,8:H O
!)?4025:4R157?G87&,@z57"L5:HjRX&10%"%3,5:487~?6H%H%57iG87&Y"/6C'&1^5:H/&B3"/&>?6,023,80%&1;,0%&H%H%576H/&3+02R2$H/?G3,RX&ot6,0L"%$'&
9 ìPì åÍ( O wQ5:"/&10%?40%&1"%3+"%576n6,oC4l'023+"%57,&L3,RX"%5764H1@z3,87"%$'6l';$H/"%3+"/&H5:n"%$45:HH/?G3,RX&LE6l48:Cni&`o}3+0>6,0%&
RX6>?G87&XZH/"/02l4RX"%l'0%&H1@4C4l'&B"/6a"%$'&K'&1&CZ"/6a0%&1"%3,5:>6,0%&6,o	"%$'&B?G8:3,)"%3,5:85:)"%$'&KH/"%3+"/&)qt&'6l';$Z"/6
5:4R18:l4C'&B"%$'&K&4C?65:"L6,oM3,8:86^Ñ;,65:';3,RX"%5764H| O JL$'&KåA9`ÈP9 q}å6';)Ç Ì 6@(+*,*ç|Q3,4C=JL9`\z~^H
qÈP3+0%025:C'6'@ Ì 6@jÇÂå6';'@Q(+*,*(|?G8:3,4'&102Hi6,"%$kl4H/&"%$'&=9 ìPì åÍ( O wNH/&>a3,"%5:R1H1@L3,4Ck3+0%&=i6,"%$
ÈY023+?G$'?G8:3,.C'&10257+3+"%57,&HB3,4C"%$zl4HR13+0%0%~N6l'"3ZH/&3+02R2$#0%&H/&>KiG8:5:';0%&1;,0%&H%H%576#5:N"%$'&570H/68:l'"%576
&X^"/023,RX"%576?G$43,H/&qt"%$'6l';$Zi6,"%$Z?G8:3,4'&102H&>Ki6^C'~)>6^C45´R13+"%5764H"/6a"%$'&?Gl'0%&87~aiG3,R%gÑFR2$43,5:45:';
H/68:l'"%576)&X^"/023,RX"%576Nl4H/&C5:=ÈY023+?G$'?G8:3,| O Q6E&1,&10@5:Z"%$'&?G8:3,445:';nC'6>a3,5:4H`"%$43+"Y$43,&Ki&1&
l4H/&CW5:T"%$'&"FE6?G8:3,445:';.RX6>?&1"%57"%5764HaH%5:4RX&"%$'&=5:"/0%6^C4l4RX"%576[6,oP"/&>?6,023,8L?G8:3,445:';.5:"/6
9 ìPì åM@`3,4CV3,8:H/65:k>6H/"6,o"%$'&=&X'3,>?G87&NC'6>a3,5:4H"%$43+")$43,&.3+?4?&3+0%&CV5:W"%$'&.8:57"/&1023+"%l'0%&,@
"%$'&)>a3,5:Dl4H/&6,oL"%$'&H/"/0%6';,&109 ìPì åÍ( O w)H/&>a3,"%5:R1H]6,oLC4l'023+"%57,&3,RX"%5764Hn$43,HKi&1&D"/6N&4RX6^C'&
RX&10%"%3,5:=ot&3+"%l'0%&H1@H%l4R2$=3,HQ"%$'&]"%5:>&C=5:457"%5:3,88:57"/&1023,8:HLl4H/&C=5:=H/6>&]C'6>a3,5:,&102H%5764HQ5:"%$'&]8:3,H/"
RX6>?&1"%57"%576@6,0]Õ/'6^ÑF5:'&10%"%ÖPo}3,RX"%HYqhÝc}c:@o}3,RX"%HM"%$43+"C'6'6,"M?&102H%5:H/"	6,&10"%5:>&Ll4487&H%H>a3,5:"%3,5:'&C
iz~[3,W3,RX"%576| O h"a>a3~[,&10%~E&8:8`i&Z&3,H%57&10a"/6D3,C4CTH/6>&6,oY"%$'&H/&ot&3+"%l'0%&HaC4570%&RX"%87~D"/6#"%$'&
"/&>?6,023,8	0%&1;,0%&H%H%576.ot6,02>]l48:3+"%576l4H/&C.iz~N"%$'&aJL9M-3,4C }	- ?G8:3,4'&102H1@"%$'6l';$."%$45:HB$43,HB~,&1"
"/6ai&B?Gl'"j"/6a"%$'&B"/&H/" O
n l4>&1025:RH/"%3+"/&+3+025:3+iG87&H"%$43+"	3+0%&`l4H/&C)qtiz~3,RX"%5764H|5:KRX&10%"%3,5:]H/?&R15´RME`3~^H	3+0%&`5:"/&10%?40%&1"/&C
3,H)0%&H/6l'02RX&Hqt6,0RX6H/">&3,H%l'0%&H1@Y5:VH/&zl'&"%5:3,8Y?G8:3,445:';z|3,4CSH%l'?4?6,0%"/&CWiz~k"%$'&.?G8:3,4'&102H1@
"%$'6l';$=EQ57"%$NH/6>&]0%&H/"/025:RX"%5764H O JL$'&]l4'0%&H/"/025:RX"/&CNl4H/&K6,ozl4>&1025:RH/"%3+"/&n+3+025:3+iG87&HY3,8:876E&C=iz~
9 ìPì åÍ( O w]5:HQ'6,"YH%l'?4?6,0%"/&C O`Ï >6,0%&KC'&1"%3,5:87&C=C45:H%R1l4H%H%576ZR13,=i&ot6l44C=5:Z"%$'&K?G3+?&10L6NJL9M3,4C }	- 5:)"%$'&RX6>?&1"%57"%576Zi6z6,g^87&1"Kq}Y3,H%8:l4>Z@4(+*,*+-,i| O

QSRTQSRTQ¦f^g  "Z § "j h@¨ ^S©"§^ svi Z"_

JA&>?6,023,80%&1;,0%&H%H%576@'í%l4H/"8:57g,&aH/&zl'&"%5:3,8M0%&1;,0%&H%H%576@Í5:H3H/&3+02R2$5:."%$'&H/?G3,RX&6,o`?G8:3,"%3,5:8:H O
Q6E&1,&10@5:)"%$'&B"/&>?6,023,8R13,H/&"%$'&H/&1"j6,o	?40%&RX64C457"%576)3+"/6>aHQ5:Hj'6a876';,&10LH%>l =R157&"j"/6H%l4>nÑ
>a3+02571&Y3?G8:3,a"%3,5:8IH/"%3+"/&H`$43,&Y"/6]i&Qi&L&X^"/&4C'&CEQ57"%$3,RX"%5764HRX64R1l'0%0%&"EQ57"%$"%$'&PH%l'i4;,63,8:H
3,4CT"%$'&"%5:>a5:';6,oP"%$'6H/&=3,RX"%5764Ha0%&8:3+"%57,&"/6D"%$'&=H%l'i4;,63,8:H O <64H%5:C'&10"%$'&&X'3,>?G87&?G8:3,T5:
Ì 57;l'0%&nw+@'H/?&R15´R13,8:87~n"%$'&ZÕ/H/"%3+"/&Öa3+"`"%5:>&B(,Ê+*^IH%5:4RX&Y"%$45:H5:H"%$'&PH/"%3+0%"%5:';]?65:"6,oA3,RX"%576Ns¢%¢ª

«

î>û

À

4Á

4

3@57"%H?40%&RX64C457"%5764H	>]l4H/"i&j;,63,8:HM"/6i&L3,R2$457&1,&C3+"M"%$45:H	?65:" O

Æ`l'"M"%$'&L3,RX"%5764HYq}5:4R18:l4C45:';
'6+Ñ6,?GH|&H/"%3+iG8:5:H%$45:';K"%$'6H/&RX64C457"%5764H`>]l4H/"`i&BRX6>?G3+"%57iG87&PEQ57"%$"%$'&3,RX"%576¬s¢%¢ª­¡ @^EQ$45:R2$
H/"%3+0%"%HKwçl4457"%H`6,oÍ"%5:>&B&3+028:57&10Q3,4C)EQ$'6H/&&X^&R1l'"%576H/?G3,4HL3,RX0%6H%HL"%$45:H`?65:" O
JL$zl4H1@'3]"/&>?6,023,80%&1;,0%&H%H%576)H/&3+02R2$H/"%3+"/&5:H`3]?G3,57<
0 8Pu 'q ®ay ¯]|2@4EQ$'&10%
& ®à5:H`3H/&1"`6,oÍ3+"/6>aH
3,4C{¯x±
u °z'q -  y ²  |2y1{1{1{y'q -3³y ²³4| ´`5:HA3YH/&1"A6,o43,RX"%5764H-3µ'EQ57"%$B"%5:>&5:4RX0%&>&"%m
H ²µ O JL$45:H0%&1?40%&H/&"%H
3n?G3+0%"%5:3,8?G8:3,.qt"%3,5:8t|EQ$'&10%&Y"%$'&3+"/6>aHL5:p®à>]l4H/"j$'68:C)3,4C)&3,R2$3,RX"%576Dq'-3µhy ²µF|5:,¯à$43,H`i&1&
H/"%3+0%"/&
C ²µA"%5:>&Pl4457"%HM&3+028:57&10 9l'"`3,'6,"%$'&10jE`3~,@43,&X^&R1l'"%3+iG87&P?G8:3,.q}H%R2$'&C4l487&|QXÝ	 ¥,H/"%3+"/&
8u'q ®ay ¯]|L3+"Q"%5:>& :j5´Ä"%$'&KO ?G8:3,>a3+g,&HY3,8:8A3+"/6>aHY5:® "/02l'&]3+J" :j3,4C=H%R2$'&C4l487&HQ3,RX"%576¶-3µ3+"
"%5:>f
& :mG·²µ	ot6,0L&3,R2$['q -3µhy ²µFc
| ¸@¯ O
áS$'&&X^?G3,4C45:';)3H/"%3+"/,
& 8uä'q ®ay ¯]|2@	H%l4R1RX&H%H/6,0]H/"%3+"/&L
H 85F	uä'q ®F}y ¯fFf|Y3+0%&RX64H/"/02l4RX"/&Ciz~
R2$'6z6H%5:';q}'6^ÑFC'&1"/&102>a5:45:H/"%5:R13,8:87~4|ot6,0P&3,R2$3+"/6O
> /e¸e®Î3,N&H/"%3+iG8:5:H%$'&10qhÝc}c:@30%&1;l48:3+0B3,RX"%576
6,0j'6+Ñ6,x
? -aEQ57"%f
$ /@¸@-%232'q -'|/|2@GH%l4R2$"%$43+"jR2$'6H/&)3,RX"%5764Hj3+0%&PRX6>?G3+"%57iG87&q}3,H`C'&X'&C5:Z\z&RX"%576
( O ( O w|nEQ57"%$[&3,R2$W6,"%$'&103,4CTEQ57"%$T3,8:8`3,RX"%5764H5:¹¯@`3,4CW3,C'+3,4R15:';."%5:>&Z"/6#"%$'&'&X^"?65:"
EQ$'&10%&B3,Z3,RX"%576H/"%3+0%"%HKq}H%5:4RX&P"%$45:H`5:Hj3n0%&1;,0%&H%H%576)H/&3+02R2$@Õ/3,C'+3,4R15:';Ö3,4C[Õ/'&X^"%Öa3+0%&5:"%$'&
C4570%&RX"%576[6,oY"%$'&i&1;5:445:';=6,oP"%$'&C'&1,&876,?G5:';.?G8:3,| O 9M0%&RX64C457"%5764H]6,oB3,8:8j3,RX"%5764Ha3,4CW'6+Ñ
6,?GHBH/"%3+0%"%5:';3+"B"%$45:HP?65:"Pi&RX6>
& ®FÍEQ$45:87&]0%&>a3,5:45:';)3,RX"%5764HqtEQ57"%$N"%$'&570B"%5:>&5:4RX0%&>&"%H
3,Cí%l4H/"/&C|i&RX6>{
& ¯fF OÏ H/"%3+"/[
& 8Pu 'q ®ay ¯]|j5:H43,857m
o ¯xD
u C3,4Cx®ºMN( O
JL$'&L&X'3,RX"C'&1"%3,5:8:H6,o"%$'&L"/&>?6,023,8'0%&1;,0%&H%H%576H/&3+02R2$3+0%&Y'6,"5:>?6,0%"%3,"ot6,0"%$'&L0%&H/"M6,o"%$45:H
?G3+?&10L3,4C$43,&Ki&1&ZC'&H%RX0257i&C)&8:H/&1EQ$'&10%&aq}Y3,H%8:l4>äÇÈY&XÄ'&10@(+*,*'w| O

QSRTQSR¼» ¨i © } \ ½¾ }"ik¿ \wdKÀ]\



!n3P"/&>?6,023,8z?G8:3,]"%$'&10%&j5:H	l4H%l43,8:87~H/6>&Õ/H%8:3,R%g^Ö^@GÝc}c:@H/6>&L3,RX"%5764HR13,ni&jH%$457ot"/&CKot6,0%E`3+02Cn6,0
iG3,R%gzE`3+02C=5:)"%5:>&BEQ57"%$'6l'"LR2$43,';5:';"%$'&KH/"/02l4RX"%l'0%&B6,0Q>a3+g,&H/?G3,6,o	"%$'&B?G8:3, OMÏ 0257;$"!ÑFH%$457ot"/&C
?G8:3,5:HL6'&]5:EQ$45:R2$=3,8:8H%l4R2$N>6+3+iG87&]3,RX"%5764HY3+0%&]H%R2$'&C4l487&C=3,HY8:3+"/&]3,HQ?6H%H%57iG87& E
O n 6^Ñ0257;$"!Ñ
H%$457ot"/&C?G8:3,4HKR13,Di&a&X'R18:l4C'&Cot0%6>ÉRX64H%5:C'&1023+"%576EQ57"%$'6l'"&4C43,';,&1025:';6,?4"%5:>a3,8:57"F~ OZì 65:';
"%$45:Hn&8:5:>a5:43+"/&Hn0%&C4l44C43,"ni4023,4R2$'&Ha5:["%$'&H/&3+02R2$kH/?G3,RX&,@EQ$45:R2$[6,ot"/&kH/?&1&C4Hl'?W?G8:3,445:';
H%57;45´R13,"%87~ . O
JL$45:HaR13,ki&=3,R2$457&1,&Ckiz~W3+?4?G87~^5:';"%$'&ot68:876EQ5:';#02l487&,I=áS$'&W&X^?G3,4C45:';#3DH/"%3+"/z
& 85FBu
'q ®F}y ¯fFf|EQ57"%$?40%&C'&RX&H%H/6,K0 8Pu 'q ®ay ¯]|2@G3,3,RX"%576-RX6>?G3+"%57iG87&PEQ57"%$Z3,8:83,RX"%5764HL5:p¯à>a3~NëÒ,e
i&Yl4H/&C"/6]&H/"%3+iG8:5:H%$3,3+"/6>Î5:,85F'EQ$'&3,8:84"%$'&P3+"/6>aH`5:o®FG"%$43+E
" -3,C4C4H$43,&Pi&1&6,i4"%3,5:'&C
ot0%6Á
> 8Biz~'6+Ñ6,?GH JL$'&Y0%&3,H/6Z5:H"%$43+<
" -RX6l48:C)$43,&Bi&1&)l4H/&C"/6aH%l'?4?6,0%""%$'&BH%3,>&3+"/6>aHj5:
®a@G3,4CZ"%$zl4HLRX6l48:CZO $43,&Ki&1&ZH%$457ot"/&C)"/6a"%$'&0257;$"Kq}C'&8:3~,&C| O
Ï ;3,5:@MC'&1"%3,5:8:H]R13,i&ot6l44CD&8:H/&1EQ$'&10%&3,4C3+0%&)'6,"n5:>?6,0%"%3," O áS$43+"ÝfK5:>?6,0%"%3,"K"/6
'6,"/&)5:HK"%$43+"]"%$'&0257;$"!ÑFH%$457ot"%5:';Z02l487&a0%&1ot&102HK"/6."%$'&?40%&C'&RX&H%H/6,0n6,oL"%$'&H/"%3+"/&)i&5:';&X^?G3,4C'&C O
JL$45:Ha>&3,4Ha"%$43+"EQ$'&T"%$'&=02l487&5:H3+?4?G8:57&C@"%$'&?6H%H%57iG87&ZH%l4R1RX&H%H/6,02H"/6'@L3,4CW"%$'&10%&1ot6,0%&="%$'&
6,?4"%5:>a3,8	RX6H/"P6,oh@30%&1;,0%&H%H%576.H/"%3+"/&a>a3~=i&nC45´Ä&10%&"PC'&1?&4C45:';6N"%$'&n?G3+"%$N"%$'0%6l';$NEQ$45:R2$
"%$'&)H/"%3+"/&ZE`3,H]0%&3,R2$'&C O JL$zl4H1@"%$'&)876E&10ni6l44C#6"%$'&)RX6H/"n6,oQ3.H/"%3+"/&)6,i4"%3,5:'&CEQ$'&D"%$'&
H/"%3+"/&B5:H&X^?G3,4C'&CiGl'"'6,"`H/687,&C.q}3,H`57"EQ5:8:8'i&Y5:3, ìLÏ H/&3+02R2$|>a3~ai&Y5:+3,8:5:C3,H`3]876E&10
i6l44Cot6,0L"%$'&KH%3,>&KH/"%3+"/&KEQ$'&)0%&3,R2$'&C^5:33aC45´Ä&10%&"j?G3+"%$ O
ÂS Ã­ª[2²Q¦h®¦!¥!¨¯ ª²L¬,ª¯ ²1¨ª2³^µ¯ ¦ · ±¯ ¨L!»jº,¦Í¬­¦h³´¦h­F2º ¦Í¨ª¬ 2¥!¦	2¥h¨¯ ª² · ©ª¦	¦h®¦!¥!¨¯ ª²L¨¯ j¦A¯ ²L¨©¦Í¬ 2²
¯ ²ª2¨¬­¦!¥!¯ ¦! »L¥!ª²}¨­F2¯ ²¦/§Y2¦/%­ »Q2¬,ª¯ º ¦`'  ÄkÅÇÆsÅ ±1 ¦h³ ¨ f©¯ ³ ¨¦/§+¤4­F%¨©¦h­¨©+2²Q%¨¨©¦A %¨¦!}¨¬,ª¯ º ¦Í¨¯ j¦
Ã­ªx¦/%­¥F©¬,ª¯ ²1¨jª2³Íµ¯ ¦ ·T· ©+%¨jL%¨¨¦h­`¯ `¨©+%¨`ª2³Í¨©¦YL2²1»n¬,ª¯ º ¦±º¨`}¨­¥h¨­F2  »as¦ È1¯ µ2 ¦!²1¨/±
¬,ª¯ ¨¯ ª²¯ ²n¨¯ j¦`³´ª2­`2²2¥h¨¯ ª²±4ª² »nª²¦j¯ ¥!ª²¯ §¦h­¦/§B¹©¦j­¦/2ª² · ©1»K­¯ ¸©1¨ f©¯ ³ ¨¯ ²¸B¯ ¦/§n¯ ²}¨¦/§
ª2³4 ¦h³ ¨ f©¯ ³ ¨¯ ²¸L¯ ¨©+%¨A¯ ²B­¦!¸2­¦!¯ ª²¦/%­¥F©±` ¦h³ ¨ f©¯ ³ ¨A­ ¦ · ¯  z¨­¯ ¸¸¦h­	 %¨¦h­'  ÄkÅÇÆsÅ ±§¦!¦!¬,¦h­Í¯ ²Y¨©¦¦/%­¥F©
¨­¦!¦%¤2²+§Q¨©¬­ª%µ¯ §¦ ¦!¦ Éj¥!¯ ¦!²1¨A¬­²¯ ²¸

~Ê

î>û

î Á(ï^ðzñ7òGó À

4ï %ô :õ Yö 4ï^ð-Gó-N÷æ s4ø- ù ¦Gï^õ¶

  1   f

TËOiÛ|Á{rq>qsrÍÌÁo grq	rÍÎq9!B#F%$Qgkrn-ofInq>
å&1"Lm  q+8 |MC'&'6,"/&P"%$'&P6,?4"%5:>a3,8RX6H/"`o}l44RX"%576@Ýc}c:@z"%$'&Po}l44RX"%576"%$43+"j3,H%H%57;4H"/6n&3,R2$H/"%3+"/&8B5:
"%$'&H/&3+02R2$H/?G3,RX&B"%$'&>a5:45:>a3,8GRX6H/"L6,o	3,~?G3+"%$)ot0%6>ÏB
8 "/6a3]43,8H/"%3+"/&q}3aH/"%3+"/&{85FMN4( @45:"%$'&
0%&1;,0%&H%H%576#?G8:3,445:';H/?G3,RX&| O JL$'&ao}l44RX"%576m  q+8 |B5:HKR2$43+023,RX"/&102571&C[iz~"%$'&Æ&8:8:>a3,#&zl43+"%576
q}Æ&8:8:>a3,@wâ,Ê,Ð |2I
*
57o8MN(
m  q8+|MuºÑ
q!w|
>a5:Ò*ÓTÔ9Ò*Õ~Ö*Ö ' Ò , m  q85:F |"twz² q8 y85fF |
EQ$'&10%&r85PS6	,6 q+8 |A5:HÍ"%$'&`H/&1"	6,oH%l4R1RX&H%H/6,0MH/"%3+"/&H	"/6{8 @^Ýc}c:@,"%$'&jH/&1"	6,oGH/"%3+"/&H"%$43+"R13,ni&`RX64H/"/02l4RX"/&C
ot0%6>×K
8 iz~)0%&1;,0%&H%H%576@3,4C¶z² q8 y85fF |j5:Hj"%$'&NÕ/C'&87"%3RX6H/"%Ö^@ÍÝc}c:@G"%$'&K5:4RX0%&3,H/&]5:3,R1R1l4>]l48:3+"/&CNRX6H/"
i&1"FE&1&p8Q3,4C,85F O !"%$'&YH/&zl'&"%5:3,84H/&1"/"%5:';'@z"%$45:H&zl43,8:H"%$'&QRX6H/"6,o"%$'&Q3,RX"%576l4H/&C"/6K0%&1;,0%&H%H
ot0%6>ÁP
8 "/685F OH+ zl43+"%576.wPR2$43+023,RX"/&102571&HLm  q+8 |M6487~6)H/"%3+"/&HJP8 "%$43+"`3+0%&P0%&3,R2$43+iG87&,I"%$'&BRX6H/"`6,o
3,l4'0%&3,R2$43+iG87&H/"%3+"/&]5:HjC'&X'&C)"/6ai&5:^457"/& O
Æ&R13,l4H/&n3,R2$457&1^5:';3a0%&1;,0%&H%H%576=H/"%3+"/&ZqhÝc}c:@H/&1"Q6,o;,63,8:H|<]
8 5:>?G8:57&Hj3,R2$457&1^5:';3,8:8A3+"/6>aHY5:
8 @43,4CZ"%$'&10%&1ot6,0%&K3,~)H%l'iGH/&1"j6,oH8 @'"%$'&6,?4"%5:>a3,8RX6H/"Lo}l44RX"%576ZH%3+"%5:H!G&Hj"%$'&K5:'&zl43,8:57"F~
m  q+8 |cØ Ò*ÓTÙS>a
q(|
ÒsÚ Û 3Ò*ÓT Û Ü m  q8 F |


p
ot6,0`3,~as OÏ H%H%l4>a5:';"%$43+""%$45:H5:'&zl43,8:57"F~5:H3,RX"%l43,8:87~a3,&zl43,8:57"F~a5:HM"%$'&Y0%&8:3'3+"%576"%$43+";57,&H
"%$'&]m'pþ$'&l'025:H/"%5:R1H1IÍ0%&1EL0257"%5:';n&zl43+"%576Dq!w|jl4H%5:';Zq(|j3,HQ3,Z&zl43,8:57"F~0%&H%l487"%Hj5:
*
m p q8+|MuÞß àÝ >a5:
>a3

57o8fMN(
Ò*ÓTÔ9Ò*Õ~Ö*Ö ' Ò , m'pnq85F:|"tw²zq8y85Ff| 57o<á 8>áÛWs
Ò*ÓTÙSÒsÚ Û Ò*ÓkÛ Ü p m'pnq85F:|

qç|

Ï XR 6>?G87&1"/&H/68:l'"%576"/6Q"%$45:H&zl43+"%576@+5:"%$'&ot6,02>æ6,o43,K&X^?G8:5:R157""%3+iG87&6,oGm'pnq8+|ot6,0Í3,8:8H/&1"%HAEQ57"%$
á 8>áAÛ s@ÍR13,#i&aRX6>?Gl'"/&C#iz~H/687^5:';Z3Z;,&'&1023,8:571&CDH%5:';87&XÑFH/6l'02RX&XÑF3,8:8´Ñ"%3+0%;,&1"%HKH%$'6,0%"/&H/"?G3+"%$
?40%6,iG87&> ONÏ +3+0257&1"F~D6,oQ3,87;,6,0257"%$4>aHq}3,8:8+3+025:3+"%5764HK6,oQC'~^43,>a5:Ra?40%6,;,023,>a>a5:';=6,0];,&'&1023,8:571&C
H%$'6,0%"/&H/"A?G3+"%$|R13,Ki&l4H/&C"/6YH/687,&"%$45:H?40%6,iG87&>Z@3,HÍC'&H%RX0257i&CBiz~,@Gc Åc:@+åA5:l1e+7cLq(+*,*(| O JL9M3,4C }	- l4H/&3+3+025:3+"%576#6,o`"%$'&ZÈY&'&1023,8:571&CDÆ&8:8:>a3,^Ñ Ì 6,02CVqÈPÆ Ì |3,87;,6,0257"%$4> O <6>?Gl'"%5:';
3RX6>?G87&1"/&]H/68:l'"%576Z"/6&zl43+"%576qç|L5:Hj?687~^'6>a5:3,85:"%$'&]zl4>Ki&10L6,oM3+"/6>aHQiGl'"L&X^?6'&"%5:3,8
5:s@H%5:>?G87~i&R13,l4H/&K"%$'&nzl4>Ki&10Q6,oMH%l'iGH/&1"%HQ6,oMH%571&KsÉ6,0P87&H%HQ;,0%6EQHQ&X^?6'&"%5:3,8:87~)EQ57"%$s O
JL$45:Hj8:5:>a57"%H"%$'&RX6>?G87&1"/&KH/68:l'"%576Z3+?4?40%63,R2$"/6H%>a3,8:8+3,8:l'&H`6,o	s×q}5:)?4023,RX"%5:RX&,@4sØÛV(| O

QSR¼»R'Uâ[_½b i "_ ^.ãEä j"h À j \ i Z"_ j _"åN\ } ^.æ{^À"§ i# \ i le¦ j]çh ^

JL$'&KH/68:l'"%576Z"/6&zl43+"%576[qç|j5:HQH/"/6,0%&C=5:=3a"%3+iG87&qtEQ$45:R2$ZEQ5:8:8i&0%&1ot&10%0%&C"/63,HQ"%$'&a'	è^dÝfXe}Ý
e3 yX:2| O JL$'&H/"/6,0%&CH/68:l'"%576@A$'6E&1,&10@MRX6>?4025:H/&HB6487~.+3,8:l'&HB6,oQm'pnq 8+|Pot6,0]H/&1"%L
H 8H%l4R2$#"%$43+"
á 8>ázÛks O JA6a6,i4"%3,5:Z"%$'&]$'&l'025:H/"%5:RQ+3,8:l'&6,o3,=3+0%iG57"/023+0%~H/"%3+"/&,@"%$'&K8:3,H/"LR18:3,l4H/&6,o	&zl43+"%576qç|
5:HY&1+3,8:l43+"/&CSÕ!6^ÑF8:5:'&Ö^@3,4C.C4l'025:';a"%$45:HQ&1+3,8:l43+"%576."%$'&]+3,8:l'&]6,o`m'pnq 85F:|Lot6,03,~?85FAH%l4R2$N"%$43+"
á 85F*ázÛWsÂ5:H`6,i4"%3,5:'&CZiz~)876z6,g^5:';57"Ll'?=5:)"%$'&B"%3+iG87& O
!o}3,RX"1@"%$'&L$'&l'025:H/"%5:R"%3+iG87&L5:>?G87&>&"/&C5:JL9M-3,4C }	- 5:H3B;,&'&1023,84>a3+?4?G5:';Pot0%6>àH/&1"%H
6,oG3+"/6>aH	"/6Y"%$'&570	3,H%H/6^R15:3+"/&Cn+3,8:l'&,@,3,4CK"%$'&`$'&l'025:H/"%5:R+3,8:l'&6,oG3YH/"%3+"/<
& 8j5:HA"%$'&`>a3'5:>a3,8+3,8:l'&6,o
3,~KH%l'iGH/&1"6,So 8"%$43+"Í5:HÍH/"/6,0%&C]5:"%$'&"%3+iG87& O !6,"%$'&10	E6,02C4H1@+57>o énq 8+|C'&'6,"/&HÍ"%$'&+3,8:l'&H/"/6,0%&CKot6,0
8@^"%$'&B$'&l'025:H/"%5:RL+3,8:l'&Y6,oÍ3nH/"%3+"/{& 8B5:H;57,&)iz~)mAq 8+|MuS>a3K °vénq 85F:| áê85FM¬8êy énq 85F:|&X'5:H/"%vH ´ O áS$'&

~ë

î>û

À

ì÷

ìí

4Á

4

î

ìí
qtî3iïv| ð

q}3|

Ó

ì÷
×

î

î
q}R|

Ó

ìí ×

î3ïvð ì ÷ ×
î3ïvð
Ó

Ì 57;l'0%&K(zI_L&8:3'3+"%576Z6,oÍ"/&>?6,023,80%&1;,0%&H%H%576H/"%3+"/&H O
3,8:8'3,4Cn6487~]H/&1"%H6,oH%571&`sÎ6,087&H%H3+0%&LH/"/6,0%&Ca5:n"%$'&j"%3+iG87&Kq}3,HM5:H	"%$'&LR13,H/&LEQ$'&am'pV5:HRX6>?Gl'"/&C
RX6>?G87&1"/&87~4|"%$45:HRX65:4R15:C'&HEQ57"%$&1+3,8:l43+"%5:';]"%$'&P8:3,H/"`R18:3,l4H/&Y6,oA&zl43+"%576qç| O Q6E&1,&10@4"%$'&Pl4H/&
6,o3B;,&'&1023,8G$'&l'025:H/"%5:R"%3+iG87&Q5:>?G8:57&HÍ"%$43+"3,HH/6z6a3,H3B+3,8:l'&jot6,0P+ë4ÞL3+"/6> H/&1c
" 8Q5:HMH/"/6,0%&Ca5:"%$'&
"%3+iG87&,@	57"i&RX6>&H]5:>a>&C45:3+"/&87~.5:4R18:l4C'&C5:#3,8:8MH%l'iGH/&zl'&"&1+3,8:l43+"%5764H6,oLH/"%3+"/&H]RX6"%3,5:45:';
8 O !Z?G3+0%"%5:R1l48:3+0@^iz~)H/"/6,025:';?G3+0%"%Hj6,o	"%$'&KH/68:l'"%576)"/6m'p Ó @'ot6,0YH/6>&K$457;$'&10jx
s Ft@G5:)"%$'&ot6,02>Î6,o
l'?C43+"/&HÍ6,oG"%$'&`+3,8:l'&H	6,oH/6>&jH%571&`x
s Fz3+"/6>àH/&1"%H1@"%$'&j$'&l'025:H/"%5:RM&1+3,8:l43+"%5765:>?G8:5:R157"%87~PRX6>?Gl'"/&H
Ó
"%$'&K>a3'5:>]l4>6,om'px3,4C)"%$'&?G3+0%"%5:3,8:87~RX6>?Gl'"/&CNm'p O
JL$'&$'&l'025:H/"%5:RQ"%3+iG87&5:Hj5:>?G87&>&"/&CZ3,HL3JA0257&q}H/&1&)c Åc Ï $'6'@GQ6,?RX0%6,ot"1@GÇ ñY8:8:>a3,@w9â £,ç|
H/6n"%$43+"`"%$'&P&1+3,8:l43+"%5766,oÍ3,)3+"/6>ÃH/&1<
" 8BR13,)i&PC'6'&P5:"%5:>&B8:5:'&3+0`5:"%$'&Bzl4>Ki&106,oÍH%l'iGH/&1"%H
6,f
o 8)"%$43+"&X'5:H/"a5:"%$'&)"%3+iG87v& ò ·
,


&
WH
/6'@"%$'&10%&Z5:HH/6>&)6,&102$'&3,CkRX6>?G3+0%&C["/6D3N"%3+iG87&3,4C
O +
&1+3,8:l43+"%576Z?40%6^RX&C4l'0%&BC'&H%57;'&C)ot6,0Q3n4^&C>a3'5:>a3,8H%l'iGH/&1"LH%571& O



 y

ÍÌ

iÛ|Á{rq>qsr Áo

ÍÎ 9!

grq	r q Iw`bt-n-ofInq>

JA6aC'&X'&Bm'pæot6,0j"/&>?6,023,80%&1;,0%&H%H%576?G8:3,445:';'@6'&'&1&C4H`6487~a"/6aC'&X'&P3nH%l457"%3+iG87&P>&3,H%l'0%&
6,oYH%571&ot6,0n"/&>?6,023,80%&1;,0%&H%H%576H/"%3+"/&Ha3,4C"%$'&?40%6^RX&1&C[3,H5:D"%$'&ZH/&zl'&"%5:3,8R13,H/& O _L&R13,8:8
"%$43+"P3"/&>?6,023,8A0%&1;,0%&H%H%576NH/"%3+"/&RX64H%5:H/"%HQ6,oM"FE6ZRX6>?6'&"%H1"@ 8]u'q ®ay ¯]|2@EQ$'&10%L
& ®Î5:HY3H/&1"
6,oL3+"/6>aH]3,4C.¯Ë3=H/&1"]6,ojH%R2$'&C4l487&CD3,RX"%5764HKEQ57"%$"%5:>&5:4RX0%&>&"%H O JL$'&a6,iz^576l4HKR13,4C45:C43+"/&
5:HB"/6.C'&X'&¶á 8>áó
u á ®pá9t á ¯,á @3,4C#5:4C'&1&C@Íl4H%5:';Z"%$45:H>&3,H%l'0%&5:&zl43+"%576Vqç|3+i6,&0%&H%l487"%H
5:3ZR2$43+023,RX"/&102573+"%576#6,o`3Z876E&10i6l44CNo}l44RX"%576N6"%$'&"/&>?6,023,8	0%&1;,0%&H%H%576H/?G3,RX& O !."%$45:H
R13,H/&,@'$'6E&1,&10@'C4l'&Q"/6]"%$'&Q?40%&H/&4RX&Q6,o3"%5:>&Y5:4RX0%&>&"E²B5:&3,R2$#q ²y -'E
| ¸@¯@z"%$'&YH/&1"6,oH/"%3+"/&H
EQ57"%w
$ á 8>á4Ûæsú5:HQ?6,"/&"%5:3,8:87~Z5:^457"/&,@3,4C="%$'&10%&1ot6,0%&n"%$'&nH/68:l'"%576="/6"%$45:HQ&zl43+"%576.R13,.'6,"Pi&
RX6>?Gl'"/&CZ&X^?G8:5:R157"%87~ O
JA6[6,i4"%3,5:S3[l4H%3+iG87&NRX6H/")&zl43+"%576@Y3o}l'0%"%$'&100%&8:3'3+"%576S5:H'&1&C'&CIH%5:4RX&.3D?G8:3,k"%$43+"
3,R2$457&1,&HK"%$'&H/"%3+"/,
& 8uv'q ®ay ¯]|2@Íot6,L
0 ¯äÁ
u °z'q -  y ²  |2y1{1{1{y'q -3³y ²³'| ´@	3+"K"%5:>B
& :P>]l4H/"K3,R2$457&1,&"%$'&
?40%&RX64C457"%5764H6,oj&3,R2$T3,RX"%576¹-3µL3+"n"%5:>o
& :cGN²µ!@3,4CD"%$'&H/&)>]l4H/"]0%&>a3,5:#"/02l'&)l4"%5:
8 :Kl4487&H%H
£¹©¦`¥! 2¯ æª2³ ¯ ²¦/%­M¨¯ j¦`¦!µ2 +%¨¯ ª²n©ª §Mª² »K²+§¦h­	¨©¦j2j¬¨¯ ª²]ª2³AQ¥!¦h­¨F2¯ ²]­¦!¸ %­¯ ¨t»]ª2³¦!²1¨­¯ ¦!
}¨ª2­¦/§`¯ ²`¨©¦¨F2º ¦ ô4¹©¦A¹­¯ ¦Í§%¨F	}¨­¥h¨­¦}¨ª2­¦!L2¬¬¯ ²¸G¯ ²+§¦h®¦/§º1»}¨­¯ ²¸!±2²+§¨©¦¯ j¬ ¦!j¦!²1¨F%¨¯ ª²
ª2³4¨©¦©¦!­¯ }¨¯ ¥¨F2º ¦M¨­¦/%¨%¨ªV¦h¨	2	}¨­¯ ²¸	¯ ² · ©¯ ¥F©K%¨ªj	2¬¬,¦/%­¯ ²KE
 ®¦/§P ¦h®¯ ¥/2'ª2­F§¦h­/
 õZ©¦!²
2²K%¨ªV¦hH
¨ ö¯ 	}¨ª2­¦/§¯ ²B¨©¦M¨F2º ¦±¦!µX¦h­»B¦h¨Í¨©+%¨	¯ L¬­¦ ®Pª2³ öµ¯ ¦ · ¦/§K2j}¨­¯ ²¸Q¯ ²B¨©¯  · !»B}¨
2 ªº,¦Í}¨ª2­¦/§± · ¯ ¨©Qµ2 ¦Í¢M¯ ³z²ªº,¦h¨¨¦h­µ2 ¦A¯ /µ2¯  2º ¦ 	¦¨ªM¨©¦ · !»j©¦!­¯ }¨¯ ¥Íµ2 ¦!%­¦Í¥!ªj¬¨¦/§
 7º1»P¥!ªj¬ ¦h¨¦ª ¨¯ ª²]ª2³¨©c
¦ ÷øDs¦ È1+%¨¯ ª²Kª2­Mº1»P­¦! %®¦/§¦/%­¥F©,¤F±¨©¯ §ª1¦!	²ª2¨¬­¦!¦!²1¨ML¬­ªº ¦!V¯ ²¥!¦
©· ¦!²¦!µX¦h­B¦h¨¯ }¨ª2­¦/§±G2 ¯ ¨º¦h¨Q 7¯ ²¥! +§¯ ²¸P¨©¦Lº¦h¨¥!ª2­­¦!¬,ª²+§¯ ²¸B¨ª ¦h®¯ ¥/2¬­¦ ®¦!F¤©+/µX¦
2 ­¦/§»Kº,¦!¦!²]}¨ª2­¦/§LÍª · ¦!µX¦h­/±¨©¯ ¯ ¨©¦­¦/2ª² · ©1»¨©¦©¦!­¯ }¨¯ ¥`¨F2º ¦±^¯ ²]¯ ¨M¥!­­¦!²1¨	³´ª2­P±^¯ M²ª2¨
º}¨¯ ¨¨¦Í³´ª2­¨©¦	¨­F2²¬,ª¯ ¨¯ ª²P¨F2º ¦¦/§Q¯ K
² 
Ímùú¦/%­¥F©
1
î

î Á(ï^ðzñ7òGó À

4ï %ô :õ Yö 4ï^ð-Gó-N÷æ s4ø- ù ¦Gï^õ¶

  1   f

C'&87&1"/&CZiz~p-3µ!@'"%$'&6,?4"%5:>a3,8RX6H/"Lo}l44RX"%576H%3+"%5:H!G&H
m  q'®ay¯]|ûØ

&

>a3
m
/01q'-3µ|2yC
	wtw²
qt-z|
Çüý Ú þ ý<, Ôÿ  'Çü Ú þ , Ô ÿ¡Ú'þ  þ ý
m  q'®ay¯]|ûØ m   ®DI
qÊ|
 þ Ôÿ /01q'-3µ|2yC
	K{
Ú
'Çü ,
8 uØq °s/"´y	°zq'-  yw|2yq'- $ y2(|´+|2@MC'&XÑ
Ï #&X'3,>?G87&>a3~#R18:3+0257ot~N"%$'&?4025:4R157?G87&,IK<64H%5:C'&10B"%$'&H/"%3+"/&x)
?G5:RX"/&Cn5: Ì 57;l'0%&j('q}3| OMÏ ?G8:3,]3,R2$457&1^5:';Y"%$45:HÍH/"%3+"/&L3+"	"%5:>&cÍ
: >]l4H/"	3,R2$457&1,&j"%$'&?40%&RX64C457"%5764HA6,o
- $ 3+"H:3G(z@H/6Km  q8+|Í>]l4H/"i&j3+"87&3,H/"m  q /01 q'- $ |2yC |)
t ( O ho3,RX"%576- $ 5:HBÕ/87&1ot"6l'"%Ö^@z3,HM5: Ì 57;l'0%&
('qti|2@57"	R13,]i&H/&1&]"%$43+"	"%$'&`H%3,>&?G8:3,]3,8:H/6P3,R2$457&1,&H	"%$'&í/65:"Í?40%&RX64C457"%5764H6,oG3,RX"%5764H-  3,4C
- $ 3+":¡G.w+@^H/6]m  q+8 |>]l4H/"Mi&Q3+"87&3,H/"m  q /01 q'-  |3IE/01 q'- $ |2yC |3#
t w OÌ 5:43,8:87~,@57oi6,"%$3,RX"%5764H3+0%&
87&1ot"	6l'"Lq Ì 57;l'0%&Q('q}R|/|2@z57"5:H	R187&3+0M"%$43+"	"%$'&j?G8:3,n3,8:H/6B3,R2$457&1,&HMH%5:>]l487"%3,'&16l4H%87~B"%$'&j?40%&RX64C457"%5764H
6,o	"%$'&B"FE63,RX"%5764HL3,4C3+"/6> A
/ @GH/6m  q+8 |`>]l4H/"ji&3+"Q87&3,H/"Ym  q °s/"´<I/01 q'-  |I{/01 q'- $ |2yC | O
Æ~D"/0%&3+"%5:';.5:'&zl43,8:57"%57&Hqt-z|PÜVqÊ|K3,Hn&zl43,8:57"%57&H1@3="/&>?6,023,80%&1;,0%&H%H%576[H/"%3+"/&Z5:HK0%&8:3^&C
"/6[3DH/&1"6,oH/"%3+"/&H)5:WEQ$45:R2$­Ú
¯ u zC @BÝc}c:@jH/"%3+"/&H)RX6"%3,5:45:';D6487~[;,63,8:H3,4Ck'6RX64R1l'0%0%&"
'

3,RX"%5764H O JA6]&3,R2$ZH%l4R2$)H/"%3+"/&,@'0%&8:3'3+"%576.q(|R13,i&Y3+?4?G8:57&C@0%&H%l487"%5:';K5:3,&zl43+"%576)C'&X45:';
"/&>?6,023,8'm'pK@H%5:>a5:8:3+0A"/6aqç| O JL$45:H&zl43+"%576$43,H	3Q457"/&&X^?G8:5:R157"AH/68:l'"%576@,RX6"%3,5:45:';P3,8:8^H/"%3+"/&H
8Ku 'q ®ay C|LEQ57"%¹
$ á ®pá4ÛUs OQÏ ;3,5:@>6,0%&nC'&1"%3,5:8:HQR13,Ni&ot6l44C&8:H/&1EQ$'&10%&)q}Y3,H%8:l4>vÇ ÈY&XÄ'&10@
(+*,*'w| O

Ûi
 ìLÏ 5:H3PE&8:8^g^'6EQa3,C4>a5:H%H%57iG87&$'&l'025:H/"%5:RH/&3+02R2$a3,87;,6,0257"%$4>Ãq}H/&1&nc ÅcB6,0%oh@Gwâ9£,Êz@Gwâ,â,â| O JL$'&
3,87;,6,0257"%$4>E6,0%g^H`iz~)3H/&10257&H`6,oÍRX6H/"!Ñi6l44C'&CZC'&1?4"%$^ÑG02H/"jH/&3+02R2$'&H JL$'&BRX6H/"L0%&1"%l'02'&Ciz~"%$'&


O
:8 3,H/"BRX6>?G87&1"/&C.C'&1?4"%$^ÑG02H/"PH/&3+02R2$#5:HY3Z876E&10Bi6l44C=6N"%$'&RX6H/"B6,o3,~NH/68:l'"%576 O JL$'&10%&1ot6,0%&,@
"%$'&3,87;,6,0257"%$4>ØR13,#&3,H%5:87~Ni&a>6^C45´G&C."/6"%3+g,&3,Dl'?4?&10B8:5:>a57"P6#H/68:l'"%576RX6H/"1@3,4C"/6&X'57"
EQ57"%$)o}3,5:8:l'0%&P64RX&K57"L$43,Hj?40%6,&Z"%$43+"Q'6H/68:l'"%576)EQ57"%$Z3RX6H/"QEQ57"%$45:"%$45:Hj8:5:>a57"&X'5:H/"%H O
Ï )&X^"/&4H%576)6,oA"%$'&B ìL
Ï  3,87;,6,0257"%$4> ot6,0LH/&3+02R2$45:'; K
Ï nY
ì  ãP_æ;,023+?G$4Hj5:H"%$'&>a3,5:"/6z68iz~
EQ$45:R2$="%$'&]0%&8:3^&CNH/&3+02R2$.>&1"%$'6^CN5:HY5:>?G87&>&"/&C O JL$'&K&X^"/&4C'&C.3,87;,6,0257"%$4>Ë5:HQ?40%&H/&"/&CN5:
\z&RX"%576=ç O ( O
 ìLÏ 5:HL3H/6+ÑFR13,8:87&C=8:5:'&3+0QH/?G3,RX&]3,87;,6,0257"%$4>ZI57"QH/"/6,0%&HQ6487~)"%$'&?G3+"%$"/6"%$'&KR1l'0%0%&"Q'6^C'& O
JL$'&3,87;,6,0257"%$4>ÐR13,#i&H/?&1&C'&CDl'?#iz~#l4H%5:';>&>6,0%~5:"%$'&ot6,02>Â6,oL3Z"/023,4H/?6H%57"%576"%3+iG87&,@
EQ$45:R2$0%&RX6,02C4HKl'?C43+"/&C.&H/"%5:>a3+"/&CRX6H/"%HK6,oj'6^C'&HB"%$43+"]$43,&i&1&&X^?G3,4C'&CiGl'"K'6,"KH/687,&C
q}_L&5:'ot&8:CV
Ç N3+02H%8:3,4C@Bwâ,â-z| O JL$'&="%3+iG87&N5:H6,o3#4^&CV8:5:>a57"/&CWH%571&,@LH/6['6,"3,8:8Q&X^?G3,4C'&C
l44H/687,&C#'6^C'&H3+0%&H/"/6,0%&C + O áS$'&'&1,&10]"%$'&H/&3+02R2$#0%&3,R2$'&H]3='6^C'&"%$43+"K5:H5:"%$'&a"%3+iG87&a"%$'&
l'?C43+"/&CRX6H/"j&H/"%5:>a3+"/&Pot6,0j"%$'&B'6^C'&q}C45:H%RX6,&10%&C)EQ$'&"%$'&P'6^C'&YE`3,H`?40%&1^576l4H%87~&X^?G3,4C'&C|M5:H

1Íª2¨¦¨©+%¨ª²¦j¦h¨ª2³¬+%­¦!²1¨©¦!¦!M©+2Mº,¦!¦!²n¯ j¬ ¯ +¦/§] · !»K³ ­ª±÷ !#"%$M¤ ôM¯ ²¥!¦QY}¨F%¨¦Jö'&k !#"%$M¤Í¯ 
Y¬+2¯ ­/±'¯ ¨M©ª §]¯ ²]³72¥h¨º,¦ · ­¯ ¨¨¦!².«'÷    !#"%M
$ ¤¤t°Q¹©!±^¨©¦`¦!j¬¨t»¦h¨¯ ²]¨©¦­¯ ¸©1¨©+2²+§K¯ §¦Lª2³º,ª2¨©
¯ ²¦s1È +2 ¯ ¨¯ ¦!¯ 	¨©¦¦!¥!ª²+§K¬+%­¨ª2³¨©¦}¨F%¨¦±ÄkÅÇÆsÅ ±,¨©¦¦h¨ª2³¥!ª²¥!­­¦!²1¨M2¥h¨¯ ª²($¹©¯ ¯ j¬ ¯ +¦/§B³´ª2­P±
¯ ¨©Yª² »P¯ ²¸ ¦M¬+2¯ ­Aª2³'¬+%­¦!²1¨©¦!¦!!±¯ A¦/§L¨©­ª¸©ª¨¨©¯ ¬+2¬,¦h­/
)·¹©¯ M§ª1¦!#*,+	-  ¦!¥h¨M¥!ªj¬ ¦h¨¦!²¦!	ª2­Mª¬¨¯ L2 ¯ ¨t»ª2³¨©¦¦/%­¥F©±z¯ ²¥!¦`}¨F%¨¦!	¨©+%¨%­¦`²ª2¨}¨ª2­¦/§ :§¦¨ª
¥!ª  ¯ ¯ ª²F¤Í%­¦¯ j¬ »Q­¦f ¦h®¬+2²+§¦/§Y¯ ³¦!²¥!ª²1¨¦h­¦/§B2¸X2¯ ²mt
 ²¹4 £Q2²+§./10 2 ±,¨©¦¨F2º ¦¯ Í¯ j¬ ¦!j¦!²1¨¦/§Y2
L¥! ª¦/§©+2©1¨F2º ¦±ÄkÅÇÆsÅ ±}¨F%¨¦!%­¦}¨ª2­¦/§ª² »K%¨	¨©¦¬,ª¯ ¨¯ ª²K¥!ª2­­¦!¬,ª²+§¯ ²¸Q¨ªL¨©¦!¯ ­	©+2©Kµ2 ¦!` ´¨©
 ª1ª¶¬P¥!ª²¯ }¨Íª² »P¯ ²B`¯ ²¸ ¦©+2©P³´²¥h¨¯ ª²B¥!ªj¬¨F%¨¯ ª²±¬ Í`}¨F%¨¦M¦s1È +2 ¯ ¨t»Y¨¦!}¨¨ªjµX¦h­¯ ³ »Y¨©+%¨¨©¦
}¨ª2­¦/§P}¨F%¨¦M¯ A¯ ²+§¦!¦/§Q¨©¦2j¦2¨©¦Mª²¦º,¦!¯ ²¸j ª1ª¶X¦/§P¬,¤Ft
 ²P¥/2¦ª2³4¥!ª  ¯ ¯ ª²!±¬­¦h³´¦h­¦!²¥!¦M¯ A¸¯ µX¦!²P¨ª
}¨ª2­¯ ²¸`²ª§¦!¥! ª¦h­A¨ª¨©¦Í­ª1ª2¨Íª2³^¨©¦	¦/%­¥F©Y¨­¦!¦j 4A
3 ¦!¯ ²³´¦! §658P7 %­ 2²+§±z:9;29 £1¤F
1=<
î

À

4Á

4

l4H/&C=5:4H/"/&3,CZ6,oM57"%HQ$'&l'025:H/"%5:RP+3,8:l'&,@G3,8:876EQ5:';"%$'&]3,87;,6,0257"%$4>Ã"/63,65:C=0%&XÑFH/&3+02R2$45:';'6^C'&Hj"%$43+"
3+0%&0%&3,R2$43+iG87&^5:3H/&1,&1023,8?G3+"%$4HLC4l'025:';]"%$'&KH%3,>&K57"/&1023+"%576 O

!>Owexzy


JL$'&JL9M-a?G8:3,4'&10j?40%&RX6>?Gl'"/&HL"%$'&"/&>?6,023,8Am $ $'&l'025:H/"%5:RP3,HQC'&H%RX0257i&CZ3+i6,&]3,4Cl4H/&HQ57"L5:
3, ìL
Ï  H/&3+02R2$W5:D"%$'&)"/&>?6,023,80%&1;,0%&H%H%576[H/?G3,RX& O _Q57;$"!ÑFH%$457ot"nR1l'"%Hn3+0%&Zl4H/&C"/6.&8:5:>a5:43+"/&
0%&C4l44C43,"?G3+"%$4Hot0%6> "%$'&QH/&3+02R2$H/?G3,RX&,@z3,4C3B"/023,4H/?6H%57"%576]"%3+iG87&Q5:HMl4H/&C"/6]H/?&1&Cal'?aH/&3+02R2$
q}Y3,H%8:l4>ÇxÈY&XÄ'&10@'(+*,*'w| O JL$'&Y>a3,5:H/"/&1?GH6,o"%$'&Q?G8:3,4'&103+0%&Q6l'"%8:5:'&C5: Ì 57;l'0%&Pýqt6?G3+;,&
(3- £|2@>a3,5:487~"/65:8:8:l4H/"/023+"/&PH%5:>a5:8:3+0257"F~3,4CC45´Ä&10%&4RX&bc:d1cfec"%$'& }	- ?G8:3,4'&10 O

?	A@CB ¼DM  CEGFGb¼!ºs»!¾ºIHKJb¼AHLJNMOF½£¼¾=J
Ì 6,0`>a3,~?G8:3,445:';P?40%6,iG87&>aH"%$'&Ym $ $'&l'025:H/"%5:R`5:HM"/6z6KE&3+g OÏ >6,0%&Y3,R1R1l'023+"/&P$'&l'025:H/"%5:RjR13,i&
6,i4"%3,5:'&Ciz~RX64H%5:C'&1025:';a$457;$'&10L+3,8:l'&HL6,o"%$'&KsÂ?G3+023,>&1"/&10@iGl'"Q3,~>&1"%$'6^Cot6,0PRX6>?Gl'"%5:';
3NRX6>?G87&1"/&)H/68:l'"%576#"/6."%$'&Zm'p&zl43+"%576[H%R13,87&HK&X^?6'&"%5:3,8:87~5:Ds@>a3+g^5:';N57"]5:>?4023,RX"%5:R13,8
ot6,0s ;Ë( O[Ï RX6>?G87&1"/&H/68:l'"%576[5:Hnl4H/&1o}l48i&R13,l4H/&Z57"$'&87?GHnC'&1"/&RX"l4'0%&3,R2$43+iG87&)H/"%3+"/&HNq}5:
?G3+0%"%5:R1l48:3+0@m $ C'&1"/&RX"%H3H%57;45´R13,"Í?G3+0%"6,o"%$'&LH/"%3+"%5:RL>]l'"/&Xn0%&8:3+"%5764H5:n3B?G8:3,445:';Q?40%6,iG87&>|2@
iGl'"3,8:H/6]E`3,H/"/&1o}l484i&R13,l4H/&Y6,ot"/&>a3,~6,o"%$'&Y3+"/6>H/&1"%H`3+0%&Y'6,"0%&87&1+3,"ot6,0&1+3,8:l43+"%5:';]H/"%3+"/&H
3,RX"%l43,8:87~]&4RX6l4"/&10%&CaEQ$45:87&jH/&3+02R2$45:';Bot6,03H/68:l'"%576n"/6K"%$'&j?G8:3,445:';Y?40%6,iG87&> 3+"$43,4C O _L&R13,8:8
"%$43+"P"%$'&$'&l'025:H/"%5:RB&1+3,8:l43+"%576N6,o3)H/"%3+"/&=q}3)H/&1"P6,oM;,63,8:H|Y>a3+g,&HBl4H/&]6,o"%$'&n&H/"%5:>a3+"/&C.RX6H/"B6,o
3,~.H%l'iGH/&1"P6,o"%$'&aH/"%3+"/&a"%$43+"K5:HPg^'6EQWq}H/"/6,0%&C#5:."%$'&a$'&l'025:H/"%5:R"%3+iG87&| OaÏ HB8:3+0%;,&10K3+"/6>ÐH/&1"%H
3+0%&RX64H%5:C'&10%&C@MÝc}c:@A3,Hs 5:4RX0%&3,H/&H1@"%$'&1~.i&RX6>&ai6,"%$>6,0%&zl4>&10%6l4HB3,4C#>6,0%&H/?&R15´R+@
3,4CZ"%$zl4Hj"%$'&Bot023,RX"%576Z6,o	"%$'&KRX6>?G87&1"/&H/68:l'"%576)"%$43+"Q5:HL3,RX"%l43,8:87~)l4H/&1o}l48C'&RX0%&3,H/&H O
JA6Dl4H/&=m'päot6,0$457;$'&10s@`R187&3+0287~3E`3~T5:Ha'&1&C'&CT"/6DRX6>?Gl'"/&Z"%$'&=$'&l'025:H/"%5:R3+"3#RX6H/"
?40%6,?6,0%"%57643+"/&Z"/6"%$'&+3,8:l'&)6,oP"%$'&5:>?40%6,&>&" O _L&8:3^&CWH/&3+02R2$k3,5:>aH"/6D3,R2$457&1,&"%$45:Hniz~
RX6>?Gl'"%5:';B6487~3?G3+0%"M6,o"%$'&Pm'pSH/68:l'"%576@3,4C3B?G3+0%""%$43+"5:HM8:57g,&87~]"/6Ki&L0%&87&1+3,"Mot6,0H/687^5:';
"%$'&;57,&Z?G8:3,445:';]?40%6,iG87&> O

Ë ÛonQPQ¶|W#F¶nÎH!B#F%$Qgkrn-oæInq>

Ï HB&X^?G8:3,5:'&CN&3+028:57&10@"%$'&m'p$'&l'025:H/"%5:R]R13,#i&H/&1&#3,HB"%$'&6,?4"%5:>a3,8RX6H/"o}l44RX"%5765:."%$'&s)Ñ
0%&1;,0%&H%H%576kH/?G3,RX&,@L3#0%&8:3^&CkH/&3+02R2$VH/?G3,RX&=EQ$'&10%&=H/&1"%H6,o>6,0%&="%$43,Ws ;,63,8:H3+0%&NH/?G8:57"a5:"/6
?40%6,iG87&>aHL6,oMsÉ;,63,8:H1@&3,R2$.6,oMEQ$45:R2$N5:HYH/687,&CN5:4C'&1?&4C'&"%87~ O JL$zl4H1@"%$'&]s)Ñ0%&1;,0%&H%H%576NH/?G3,RX&
5:H)3, K
Ï nY
ì  ãP_Ø;,023+?G$IH/"%3+"/&H)EQ57"%$Vs 6,0)ot&1E&103+"/6>aHZ3+0%&DãP_jÑF'6^C'&H)3,4CS3+0%&.&X^?G3,4C'&C
iz~T'6,02>a3,8L0%&1;,0%&H%H%576@EQ$45:87&H/"%3+"/&HEQ57"%$W>6,0%&="%$43,Ws 3+"/6>aH3+0%& K
Ï nYì ÑF'6^C'&H1@`EQ$45:R2$k3+0%&
&X^?G3,4C'&C=iz~=H/687^5:';a&3,R2$H%l'iGH/&1"Q6,oH%571&]s O JL$'&]RX6H/"P6,o3,ãP_jÑF'6^C'&]5:HY>a5:45:>a571&CZ6,&103,8:8
57"%HnH%l4R1RX&H%H/6,02H1@EQ$45:87&"%$'&)RX6H/"n6,oY3, K
Ï nYì ÑF'6^C'&Z5:H]>a3'5:>a571&C ?
O + '3,>?G87&HK6,onqt?G3+0%"]6,o|"%$45:H
;,023+?G$@ot6,0P3)(Ñ0%&1;,0%&H%H%576.H/?G3,RX&,@3+0%&nH%$'6EQN5: Ì 57;l'0%&HQ-)3,4C.ÊNqt"%$'&]&X'3,>?G87&]5:HYC'&H%RX0257i&C5:
C'&1"%3,5:85:Z\z&RX"%576ç O ( O w| OÏ HjR13,)i&BH/&1&@^"%$'&P;,023+?G$)5:H`'6,"jH/"/025:RX"%87~8:3~,&10%&C@45:"%$43+"YãP_jÑF'6^C'&H
>a3~ZH/6>&1"%5:>&HL$43,&]H%l4R1RX&H%H/6,02Hj"%$43+"Q3+0%&K3,8:H/6)ãP_jÑF'6^C'&H O
JL$'&LC45´Ä&10%&"3,87;,6,0257"%$4>aHl4H/&Cn"/6B6,i4"%3,5:aRX6>?G87&1"/&LH/68:l'"%5764HÍ"/6"%$'&Qm'pk&zl43+"%576aR13,a3,8:8^i&
H/&1&3,H+3+025:3+"%5764HM6,o3=Õ!i6,"/"/6>nÑFl'?GÖ]8:3+i&8:5:';B6,o"%$'&Y'6^C'&H6,o"%$45:HM;,023+?G$@zH/"%3+0%"%5:';Kot0%6> '6^C'&H
EQ57"%$ZRX6H/"j1&10%63,4C)?40%6,?G3+;3+"%5:';aRX6H/"%HL"/6?G3+0%&"L'6^C'&Hj3,R1RX6,02C45:';"/6"%$45:Hj>a5:  >a3?4025:4R157?G87& O
JL$'&?40%6,?G3+;3+"%576T5:HnRX6>?G87&1"/&,@QÝc}c:@M?40%6^RX&1&C4Hnl4"%5:8&1,&10%~Uq}H/687+3+iG87&|K'6^C'&)5:"%$'&);,023+?G$[$43,H
i&1&Z8:3+i&87&CEQ57"%$)57"%H`6,?4"%5:>a3,8RX6H/"]q}3,87"%$'6l';$Z5:ZH/6>&B6,oÍ"%$'&3,87;,6,0257"%$4>aH1@'5:4R18:l4C45:';"%$'&]ÈPÆ Ì
1

î î

î Á(ï^ðzñ7òGó À

4ï %ô :õ Yö 4ï^ð-Gó-N÷æ s4ø- ù ¦Gï^õ¶

  1   f

5:>?G87&>&"%3+"%576#l4H/&CDiz~JL9M-N3,4C }	- @	6487~."%$'&RX6H/"%HK6,oYãP_jÑF'6^C'&HK3+0%&)3,RX"%l43,8:87~H/"/6,0%&C| O
_L&8:3^&C=H/&3+02R2$=&X^?G876,0%&Hj"%$'&Ks)Ñ0%&1;,0%&H%H%576H/?G3,RX&]5:3>6,0%&ot6^R1l4H/&Co}3,H%$4576@'EQ57"%$Z"%$'&K3,5:>Ã6,o
C45:H%RX6,&1025:';a"%$'&6,?4"%5:>a3,8ARX6H/"nqt6,0Y3,N5:>?40%6,&C876E&10Yi6l44C|M6,oMH/"%3+"/&HQ0%&87&1+3,"Q"/6"%$'&KH/&3+02R2$
ot6,0j3nH/68:l'"%576"/6n"%$'&P;,63,8:H`6,o"%$'&P;57,&?G8:3,445:';?40%6,iG87&> O JL$45:H5:H`3,R2$457&1,&C)iz~H/&3+02R2$45:';n"%$'&
s)Ñ0%&1;,0%&H%H%576ZH/?G3,RX&Bot6,0Q3,)6,?4"%5:>a3,8H/68:l'"%576"/6a3n?G3+0%"%5:R1l48:3+0jH/"%3+"/&,I"%$'&RX6H/"j6,oÍ"%$45:HjH/68:l'"%576)5:H
"%$'&]m'pþ$'&l'025:H/"%5:RY+3,8:l'&B6,o	"%$43+"QH/"%3+"/& O JL$'&K3,87;,6,0257"%$4>ÎC'&H%RX0257i&CZ5:)"%$'&K'&X^"QH/&RX"%576qt ìLÏ ã  |
R13+0%0257&H6l'"M"%$45:HMH/&3+02R2$DÕ!"/6,?'ÑFC'6EQ4Ö^@^H/"%3+0%"%5:';ot0%6> "%$'&QH/"%3+"/&YRX6,0%0%&H/?64C45:';Y"/6K"%$'&L;,63,8:HM6,o"%$'&
?G8:3,445:';]?40%6,iG87&> O
Q&l'025:H/"%5:R1HjC'&10257,&C)iz~)H/&3+02R2$45:';5:Z3,3+iGH/"/023,RX"%576Z6,oÍ"%$'&H/&3+02R2$=H/?G3,RX&$43,&i&1&H/"%l4C457&C
&X^"/&4H%57,&87~5: Ï Lq}H/&1&c Åc^ÈP3,H%R2$4457;'@wâ Ð+âzû'9M0257&C457"%5:H1@4wâ,â,çzûG<`l487i&102H/6aÇr\^R2$43+&XÄ&10@wâ,â,ý| O !
?G3+0%"%5:R1l48:3+0@57"P$43,HYi&1&.H%$'6EQN"%$43+"BH%l4R2$.$'&l'025:H/"%5:R1HYR13,.6487~Zi&nRX6H/"B&XÄ&RX"%57,&al44C'&10PRX&10%"%3,5:
RX64C457"%5764H1IP"%$'&;,&'&1023,8:571&C"%$'&16,0%&>Ø6,oßM3,87"/6,0%"%3=H/"%3+"/&H"%$43+"K5:."%$'&aRX6l'02H/&6,oj3, Ï H/&3+02R2$
;l45:C'&C)iz~Z3a$'&l'025:H/"%5:RPC'&10257,&CZiz~)H/&3+02R2$45:';aiG8:5:4C487~a5:ZH/6>&K3+iGH/"/023,RX"%5766,o	"%$'&KH/&3+02R2$=H/?G3,RX&,@
&1,&10%~[H/"%3+"/&Z"%$43+"E6l48:CDi&)&X^?G3,4C'&Ciz~3NiG8:5:4CDH/&3+02R2$W5:"%$'&)6,0257;5:43,8H/&3+02R2$WH/?G3,RX&Z>]l4H/"
i&K&X^?G3,4C'&C&57"%$'&10Y5:"%$'&n3+iGH/"/023,RX"PH/?G3,RX&K6,0Piz~Z"%$'& 
Ï  H/&3+02R2$.5:Z"%$'&]6,0257;5:43,8H/?G3,RX&Zq}Q687"/&,@
9Í&10%&1,S@ R5:>a>&10@4T
Ç N3,R ì 643,8:C@Awâ,â,ý| O JL$45:H`5:>?G8:57&H"%$43+"L57oA"%$'&3+iGH/"/023,RX"%5765:Hj3,Z&>Ki&C4C45:';
qt"%$'&nH/&1"Y6,oMH/"%3+"/&HB5:"%$'&]3+iGH/"/023,RX"BH/?G3,RX&]5:HQ"%$'&]H%3,>&n3,HP5:"%$'&K6,0257;5:43,8AH/&3+02R2$.H/?G3,RX&|2@H%l4R2$N3
$'&l'025:H/"%5:RnR13,'&1,&10Ki&RX6H/"K&XÄ&RX"%57,&qtßM3,87"/6,0%"%3^@jw9â £-z| O JL$'&as)Ñ0%&8:3'3+"%576#6,o`"%$'&0%&1;,0%&H%H%576
?G8:3,445:';=H/&3+02R2$WH/?G3,RX&5:Hn3,[&>Ki&C4C45:';'@H%5:4RX&)&1,&10%~[H/"%3+"/&5:"%$'&Z'6,02>a3,80%&1;,0%&H%H%576[H/?G3,RX&
RX6,0%0%&H/?64C4H"/6&X'3,RX"%87~k6'&.H/"%3+"/&Wq}RX6"%3,5:45:';"%$'&.H%3,>&.H/&1")6,oKH%l'i4;,63,8Y3+"/6>aH|5:k"%$'&Ns)Ñ
0%&1;,0%&H%H%576#H/?G3,RX& O !#H/?G57"/&a6,o`"%$45:H1@A"%$'&10%&3+0%&0%&3,H/64H"/6=i&8:57&1,&"%$43+"K0%&8:3^&C#H/&3+02R2$R13,Di&
RX6H/"`&XÄ&RX"%57,&,IJL$'&P3,87;,6,0257"%$4> l4H/&C"/6H/&3+02R2$"%$'&Ps)Ñ0%&1;,0%&H%H%576H/?G3,RX&BC45:H%RX6,&102HBq}3,4C)H/"/6,0%&H`5:
"%$'&j$'&l'025:H/"%5:R"%3+iG87&|"%$'&`"/02l'&jm'pW+3,8:l'&,@+6,0M3P876E&10	i6l44CK6]"%$45:HÍ+3,8:l'&;,0%&3+"/&10M"%$43,]"%$43+"	;57,&
iz~n"%$'&QR1l'0%0%&"M$'&l'025:H/"%5:R"%3+iG87&,@ot6,0M&1,&10%~ãP_jÑF'6^C'&j&X^?G3,4C'&CaC4l'025:';P"%$'&QRX6l'02H/&j6,o"%$'&L0%&8:3^&C
H/&3+02R2$ O JL$'& K
Ï nY
ì  ãP_þH/"/02l4RX"%l'0%&6,o	"%$'&s)Ñ0%&1;,0%&H%H%576H/?G3,RX&,@3,4C"%$'&o}3,RX"Q"%$43+"L"%$'&=Õ!6^ÑF8:5:'&Ö
$'&l'025:H/"%5:R]>a3+g,&Hl4H/&6,o`3,8:8Í0%&87&1+3,"5:'ot6,02>a3+"%576N?40%&H/&"5:."%$'&a$'&l'025:H/"%5:R"%3+iG87&,@A5:>?G8:57&HQ"%$43+"
3,N5:>?40%6,&>&"Q6,o"%$'&]&H/"%5:>a3+"/&CNRX6H/"Y6,oM3,ãP_jÑF'6^C'&K>a3~~^57&8:C5:>a>&C45:3+"/&87~Z3,N5:>?40%6,&C
&H/"%5:>a3+"/&=6,oB"%$'&NRX6H/"6,o>a3,~ K
Ï nYì ÑF'6^C'&H.q}3,8:8QH/"%3+"/&H"%$43+")3+0%&NH%l'?&102H/&1"%H6,oB"%$'&N5:>?40%6,&C
H/"%3+"/&|2@'EQ57"%$'6l'"3,~3,C4C457"%57643,84H/&3+02R2$)&XÄ6,0%" OÌ 5:43,8:87~,@i&R13,l4H/&ãP_jÑF'6^C'&H5:a"%$'&Ys)Ñ0%&1;,0%&H%H%576
H/?G3,RX&Q3+0%&LH/"%3+"/&H6,o8:5:>a57"/&CnH%571&,@&3,R2$'6^C'&j&X^?G3,4H%5765:n"%$'&js)Ñ0%&1;,0%&H%H%576aH/?G3,RX&Q5:H8:57g,&87~K"/6i&
RX6>?Gl'"%3+"%57643,8:87~R2$'&3+?&10"%$43,a"%$'&Y3,&1023+;,&5:"%$'&Y'6,02>a3,840%&1;,0%&H%H%576H/?G3,RX&,@^H%5:4RX&L"%$'&Yzl4>Ki&10
6,oAH%l4R1RX&H%H/6,02H`;,&'&1023+"/&C)EQ$'&0%&1;,0%&H%H%5:';n3nH/"%3+"/&P;,&'&1023,8:87~5:4RX0%&3,H/&HEQ57"%$"%$'&Pzl4>Ki&106,o;,63,8
3+"/6>aHQ5:)"%$'&KH/"%3+"/& O

ËÛiKUI
JA6nH/&3+02R2$)"%$'&Y0%&8:3^&C0%&1;,0%&H%H%576H/?G3,RX&,@ }	- l4H/&H`3,)3,87;,6,0257"%$4> R13,8:87&C ìLÏ ã OÏ H"%$'&P43,>&
H%l';,;,&H/"%H1@L57"5:Ha3,k3,C43+?4"%576W6,oP ìL
Ï  "/6DH/&3+02R2$45:'; ÏKnYì ãP_Ë;,023+?G$4H1@YÝc}c:@j57"aR13+0%0257&Ha6l'"3
C'&1?4"%$^ÑG02H/"1@'57"/&1023+"%57,&KC'&1&1?&45:';H/&3+02R2$ O  ìLÏ ã  5:Hj3,C4>a5:H%H%57iG87&,@^5:)"%$'&H/&4H/&B"%$43+"Q57oA;l45:C'&C)iz~
3,Z3,C4>a5:H%H%57iG87&L$'&l'025:H/"%5:R+@^57"0%&1"%l'024H"%$'&P6,?4"%5:>a3,8H/68:l'"%576RX6H/"j6,o"%$'&BH/"%3+0%"%5:';H/"%3+"/& O !o}3,RX"1@457"
4C4H"%$'&L6,?4"%5:>a3,84RX6H/"6,o&1,&10%~ãP_jÑF'6^C'&L"%$43+"5:HMH/687,&C5:"%$'&QRX6l'02H/&Q6,o"%$'&YH/&3+02R2$ O Q6E&1,&10@
57"]C'6z&HK'6,"]g,&1&1?&'6l';$5:'ot6,02>a3+"%576#ot6,0]"%$'&6,?4"%5:>a3,8H/68:l'"%576#57"%H/&87o`"/6Ni&&X^"/023,RX"/&C@MH/6N57"
R13,Z'6,"i&Pl4H/&C"/6n4CH/68:l'"%5764H"/6 K
Ï nY
ì  ãP_æH/&3+02R2$)?40%6,iG87&>aH O h"E6,0%g^Hot6,0j"%$'&Y?Gl'0%?6H/&Q6,o
5:>?40%6^5:';n"%$'&$'&l'025:H/"%5:R+@'$'6E&1,&10@H%5:4RX&Bot6,0L"%$45:H`6487~"%$'&6,?4"%5:>a3,8M2ÒXeA'&1&C4Hj"/6ai&Pg^'6EQ O
1

î û

À

4Á

4

V ;WYX:Zv[\ V =]K
W_^
V W ;`abv;cedKf5avhg
Vji W :kv	vlvdnm V Wg
Vpo W qm;a V :kv	vlvsrtlcKlu`@;`abv;cAWK^
Vv W
:kv	vlvdX:Zv[wtZxy V =]{:kv	vlvAWg
z
Vj{ W v;kv;l¶:kv	vlvQg
z
Vj| WYX:Zv[wtZxy V =]
W_^
Vj} W tfIf~:l~a V WK^
Vj~ W
;`abv;cdo	;k5hg
V :uW
v;kv;lshg
z
V 	;W tf V ,`v;c¶:ly`abv;c5uauWI^
V W
;`abv;cdo	;k5hg
ViW
v;kv;l?`v;c@;`atkv~;`tlz;`vQg
z
V  o W tf Vu   W_^Yo;Zutlu`cv
VvW
f` V vv:m?:k	@=`f@,:k:mx;m~  =  r;dBW_^
V{W
l5q¶;`v`f=6dX:Zv[\ V =]K
WgxaaeX:Zv[\q;m¶;`vav9p
V|W
tf V l5q?;`v`f=  
Wn^x='lu`@;`abv;c
V}W
v;kv;lel5q¶;`v`f@=g
z
z
V~W
;`abv;cd V aax:k	~x;`abv;cAWg
V ;uW
l5q¶;`v`f@ndB`bvaax=4l5q¶;`v`f@=pQg
V ~;W
tf V ;`abv;cAWK^
V 	W
`v V =]l5q¶;`v`f@Wo:lsy`abv;c5uahg
z
Vi W
v;kv;lel5q¶;`v`f@=g
z
V  o W ave^K[tutlu`cv
V v W
f` V vv:m?=f:l¶:k	 V WWK^
V{ W
tf V cva5 V =]*=WK_m V =W_r;do
W_^
V| W
l5q¶;`vp;mv`tk;m?=6d_cva5 V =]*=WKeX:Zv[wtZxy V =]sncva5 V =]*=WWg
V} W
tf V ;`abv;cAWK^
V~ W
l5q¶;`v`f_dnl5q¶;`vp;mv`tk;m?=g
Vji uW
`v V =]l5q¶;`v`f@Wo:lsy`abv;c5uahg
Vji ;W
v;kv;lIl5q¶;`v`f@=g
z
z
Vji W
ave^
Vjii W
l5q¶;`vp;mv`tk;m?=6d_cva5 V =]*=WKnm V =Wg
z
z
Vji;o W
l5q¶;`v`f@ndB9:ls`bvaax=4l5q¶;`vp;mv`tk;m?=pQg
Vjiv W
`v V =]l5q@;`vs`fW,:lI5kv~	~	t5uahg
Vji{ W
v;kv;lel5q¶;`v`f@=g
z
z
Ì 57;l'0%&KçzIJL$'&B ìLÏ ã



3,87;,6,0257"%$4>ÂqtEQ57"%$ZH/687,&C"%3+iG87&| O
1
î

î Á(ï^ðzñ7òGó À

4ï %ô :õ Yö 4ï^ð-Gó-N÷æ s4ø- ù ¦Gï^õ¶

  1   f

JL$'&`3,87;,6,0257"%$4>r5:HÍH/g,&1"%R2$'&Ca5: Ì 57;l'0%&`ç O JL$'&>a3,5:]C45´Ä&10%&4RX&ot0%6>r ìLÏ 5:HÍ5:K"%$'& ìPÌ \KH%l'i'Ñ
0%6l'"%5:'&,IEQ$'&)&X^?G3,4C45:';n3, ÏKnYì ÑF'6^C'&,@G57"j0%&R1l'02H%57,&87~5:,6,g,&HL"%$'&>a3,5:)?40%6^RX&C4l'0%&P ìLÏ ã  @
023+"%$'&10]"%$43,D"%$'& ìPÌ \o}l44RX"%576 O JL$zl4H1@Íot6,0]&3,R2$TH%l4R1RX&H%H/6,0]"/6.3, ÏKnYì ÑF'6^C'&,@	"%$'&)3,87;,6,0257"%$4>
?&10%ot6,02>aHA3PH/&10257&HA6,oGH/&3+02R2$'&H	EQ57"%$]5:4RX0%&3,H%5:';YRX6H/"	i6l44C@H/"%3+0%"%5:';Yot0%6>þ"%$'&`$'&l'025:H/"%5:R&H/"%5:>a3+"/&
6,oj"%$'&)H%l4R1RX&H%H/6,0n'6^C'&NqtEQ$45:R2$#ot6,0nH/6>&)H%l4R1RX&H%H/6,02H]>a3~#i&H%>a3,8:87&10K"%$43,D"%$43+"K6,oL"%$'& K
Ï nYì Ñ
'6^C'&)57"%H/&87o|3,4CD45:H%$45:';EQ$'&[3.H/68:l'"%5765:H]ot6l44C6,0n"%$'&ZRX6H/"i6l44CD6,oQ"%$'&)?40%&C'&RX&H%H/6,0
K6,Ï ?4nY"%5:ì >aÑF3,'68^RXC'6&BH/"L5:H6,&XoÍ'"%RX$'&1&P&C'&X&^C ?GO 3,JL4C'$4&5:HCZ&'64H^%C'l'&,0%&@4H3,4C"%$4)3+&"`z"%l4$'3,&P8RX"/66aH/"j"%$'0%&B&1"%6,l'?402"%'&5:>aC3,5:8HRX3,687H/E`"Q357~^oHj"%$'3n&876'6E^&1C'0`&Bi5:Hj6l4H/64C87,&6C "%Æ$'~ &
O
H/"/6,025:';]l'?C43+"/&CRX6H/"%H`6,o	ãP_jÑF'6^C'&H5:"%$'&P$'&l'025:H/"%5:Rj"%3+iG87&,@^"%$'&PH/&3+02R2$ZRX6>?Gl'"/&H`3]?G3+0%"6,o"%$'&
m'pÎ$'&l'025:H/"%5:Ra3,H]3NH%5:C'&&XÄ&RX"n3,4C@3,Hn'6,"/&CD&3+028:57&10@	"%$'&+3,8:l'&H]H/"/6,0%&C[5:#"%$'&"%3+iG87&i&RX6>&
5:>a>&C45:3+"/&87~=3+3,5:8:3+iG87&not6,0l4H/&a5:.H%l'iGH/&zl'&"B$'&l'025:H/"%5:RK&1+3,8:l43+"%5764H O  ìLÏ ã  H/"/6,?GHH/&3+02R2$45:';
"%$'&H%l4R1RX&H%H/6,02Hn6,oP3, K
Ï nYì ÑF'6^C'&3,HaH/6z6T3,H6'&Z5:Hnot6l44C"/6#$43,&=3.RX6H/"a;,0%&3+"/&10"%$43,["%$'&
R1l'0%0%&"`i6l44C@^H%5:4RX&Y"%$45:Hj5:>?G8:57&HM"%$'&BRX6H/"L6,oA"%$'& K
Ï nYì ÑF'6^C'&B5:Hj3,8:H/6n;,0%&3+"/&10L"%$43,)"%$'&Bi6l44C O
Q6E&1,&10@H%5:4RX&"%$'&`3,87;,6,0257"%$4>æ?&10%ot6,02>aH0%&1?&3+"/&CnC'&1?4"%$^ÑG02H/"AH/&3+02R2$'&H	EQ57"%$K5:4RX0%&3,H%5:';Li6l44C4H1@
0%&>a3,5:45:';nH%l4R1RX&H%H/6,02HL6,oÍ"%$'& K
Ï nYì ÑF'6^C'&EQ5:8:8G&1,&"%l43,8:87~)3,8:H/6i&H/687,&C O áS$'&3,Zs)ÑFH/68:l'"%576
$43,H`i&1&ot6l44C@^3,8:8H%l4R1RX&H%H/6,02H`"/6&1,&10%~ K
Ï nYì ÑF'6^C'&B3+?4?&3+025:';]5:"%$'&BH/68:l'"%576"/0%&1&$43,&Bi&1&
H/&3+02R2$'&C@3,4C"%$'&570l'?C43+"/&C[RX6H/"%HaH/"/6,0%&C O JL$45:HK&4H%l'0%&H]"%$43+""%$'&)0%&H%l487"%5:';N$'&l'025:H/"%5:R+@`Ýc}c:@
"%$43+"QC'&X'&C)iz~"%$'&K$'&l'025:H/"%5:RQ"%3+iG87&K3+ot"/&10Q"%$'&B0%&8:3^&CH/&3+02R2$=5:H45:H%$'&C@'5:HjH/"%5:8:8RX64H%5:H/"/&" O
Æ&R13,l4H/&"%$'&H%l4R1RX&H%H/6,0]'6^C'&HB6,o K
Ï nYì ÑF'6^C'&H]3+0%&H%l'iGH/&1"%H1@A ìLÏ ã  ot0%&zl'&"%87~.&4RX6l4"/&102H
"%$'&KH%3,>&H/"%3+"/&)q}H/&1"L6,o	;,63,8:H|`>6,0%&"%$43,Z64RX&KC4l'025:';nH/&3+02R2$ O JL$'&3,87;,6,0257"%$4>ÎR13,i&H/?&1&C'&C
l'?@H%57;45´R13,"%87~,@iz~.H/"/6,025:';ZH/687,&C'6^C'&Haqti6,"%$ K
Ï nYì ÑF'6^C'&H3,4CDãP_jÑF'6^C'&H|L"/6,;,&1"%$'&10KEQ57"%$
"%$'&570n6,?4"%5:>a3,8RX6H/"a3,4C[H%$'6,0%"!ÑFR1l'"/"%5:';."%$'&ZH/&3+02R2$[EQ$'&[57"]0%&3,R2$'&Ha3.'6^C'&"%$43+"$43,Hn3,870%&3,C'~
i&1&TH/687,&C 2 O !TC45´Ä&10%&4RX&Z"/6"%$'&876E&10i6l44C4H]H/"/6,0%&Ck5:"%$'&$'&l'025:H/"%5:R"%3+iG87&,@EQ$45:R2$T3+0%&
+3,8:5:C3,8:H/65:"%$'&Px
s F Ñ0%&1;,0%&H%H%576)H/&3+02R2$ZH/?G3,RX&Bot6,0j3,~x
s FS;WsØ3,H`E&8:83,Hj5:"%$'&Y6,0257;5:43,8H/&3+02R2$
H/?G3,RX&,@"%$'&]5:'ot6,02>a3+"%5765:"%$'&]H/687,&C"%3+iG87&]5:HL+3,8:5:CZ6487~)ot6,0Q"%$'&]R1l'0%0%&"Qs)Ñ0%&1;,0%&H%H%576=H/&3+02R2$
q}H%5:4RX&H/"%3+"/&HK6,ojH%571&ax
s Ft@Íot6,0Kx
s F;às 3+0%&0%&8:3^&C#5:"%$'&as)Ñ0%&1;,0%&H%H%576DH/?G3,RX&aiGl'"K'6,"K5:"%$'&
sxF Ñ0%&1;,0%&H%H%576H/?G3,RX&| O
n 6,"/&n"%$43+"B3)H/"%3,4C43+02C="/023,4H/?6H%57"%576"%3+iG87&,@EQ$45:R2$0%&RX6,02C4HPl'?C43+"/&CNRX6H/"P&H/"%5:>a3+"/&HY6,ol4^Ñ
H/687,&CW'6^C'&H1@5:Hn6,oP'6l4H/&5: ìLÏ ã  H%5:4RX&Zl'?C43+"/&C&H/"%5:>a3+"/&H6,oãP_jÑF'6^C'&H3+0%&H/"/6,0%&CW5:
"%$'&$'&l'025:H/"%5:R"%3+iG87&,@EQ$45:87&"%$'&$'&l'025:H/"%5:R&H/"%5:>a3+"/&Z6,oP3, K
Ï nYì ÑF'6^C'&=5:H3,87E`3~^H;57,&Tiz~"%$'&
>a3'5:>]l4>6,o57"%HLH%571&BsÂH%l4R1RX&H%H/6,02H O

»RTQSR'UWVL_Nã j g h ^

Ì 6,03,W5:8:8:l4H/"/023+"%576#6,oY"%$'&Zl4H/&)6,oY0%&8:3^&CTH/&3+02R2$T"/6#5:>?40%6,&$'&l'025:H/"%5:Ra+3,8:l'&H1@RX64H%5:C'&10"%$'&
ot68:876EQ5:';)H%5:>?G87&K?40%6,iG87&>Ëot0%6>Ë"%$'&\^JL_L!9`\Z,&102H%576.6,o"%$'&\^3+"/&8:8:57"/&nC'6>a3,5:@5:"/0%6^C4l4RX&CN5:
"%$'&](+*,*(a?G8:3,445:';nRX6>?&1"%57"%576 O JL$'&B?40%6,iG87&>ÎRX64RX&1024HQ3aH%3+"/&8:8:57"/&EQ$'6H/&B;,63,8Í5:Hj"/63,R1zl4570%&
5:>a3+;,&HM6,oC45´Ä&10%&"3,H/"/0%6'6>a5:R13,84"%3+0%;,&1"%HPqt0%&1?40%&H/&"/&Ciz~n"%$'&Q?40%&C45:R13+"/p
& 5,S,| O JA6]C'6KH/6'@
57"%H5:4H/"/02l4>&">]l4H/"G02H/"i&?6E&10%&CB6q5
",|3,4CR13,8:57i4023+"/&C5q 	ªh,|2@3,4CB"%$'&H%3+"/&8:8:57"/&M>]l4H/"
"%l'02DH/6="%$43+"]57"K5:H?65:"%5:';5:"%$'&C'&H%570%&CDC4570%&RX"%576kq5Qu¡,S,| O !4H/"/02l4>&"KR13,8:57i4023+"%576
0%&zl4570%&H"%$'&`H%3+"/&8:8:57"/&"/6Pi&?65:"%5:';Q3+"3YH/?&R15´RMR13,8:57i4023+"%576"%3+0%;,&1"Qq}5:"%$45:HA&X'3,>?G87&,@,C4570%&RX"%576

 1¹©¦Aª µX¦/§j¨F2º ¦± ¯ ¶X¦A¨©¦¨­F2²¬,ª¯ ¨¯ ª²L¨F2º ¦±¯ ¯ j¬ ¦!j¦!²1¨¦/§`2M¥! ª¦/§L©+2©1¨F2º ¦
t²L¥/2¦Íª2³z¥!ª  ¯ ¯ ª²!±
¨©¦	¬­¦!µ¯ ª »P}¨ª2­¦/§Y²ª§¦¯ A¯ j¬ »Qª%µX¦h­ · ­¯ ¨¨¦!²¹©¯ Aj¦/2²¨©+%¨Aªj¦¦/%­¥F©¦!AL!»Qº,¦	­¦!¬,¦/%¨¦/§±+º¨
§ª1¦!²ª2¨A ¦!¥h¨A¥!ª2­­¦!¥h¨²¦!Aª2³^¨©¦M2 ¸ª2­¯ ¨©P
¡¹©¦L§ªL2¯ ²]¦/§]¯ ²]¨©¯ M¦h®2j¬ ¦`¯ Mªj¦ · ©+%¨¯ j¬ ¯ + ¦/§Y¹©¦`³´ Í ´¨¦!j¬,ª2­F2 ¤§ªL2¯ ²]¯ §¯ ¥!¦/§]¯ ²
 ¦!¥h¨¯ ª²Y£ Â 
1
î

À

4Á

4

{(img d4),(img d5),(img d6)}: 4 (3)

{(img d4),(img d5)}: 4 (3)
(tk_img d4)

{(img d4),(img d6)}: 3

{(point d4),(on),(cal),(img d5)}: 3 + 1

{(img d5),(img d6)}: 3

(tk_img d5)
{(point d5),(on),(cal),(img d4)}: 3 + 1

Ì 57;l'0%&B-'IQ9	3+0%"K6,oj"%$'&)(ÑF_L&1;,0%&H%H%576D"/0%&1&qt&X^?G3,4C'&C#"/6N3=RX6H/"]i6l44C6,oLç|Bot6,0K"%$'&&X'3,>?G87&
\^3+"/&8:8:57"/&?40%6,iG87&> OjK
Ï nYì ÑF'6^C'&HY3+0%&]C'&1?G5:RX"/&Ciz~Z0%&RX"%3,';87&H1@ãP_jÑF'6^C'&HLiz~Z&8:8:57?GH/&H O
JL$'&QRX6H/"6,o&3,R2$'6^C'&Q5:HEL0257"/"/&a3,HKÕ!&H/"%5:>a3+"/&x
C tU3,R1R1l4>]l48:3+"/&C4Ö OMÌ 6,0`'6^C'&HEQ$'6H/&
&H/"%5:>a3+"/&CRX6H/"$43,HMi&1&al'?C43+"/&C3+ot"/&10&X^?G3,4H%576@"%$'&KqmX|Í&H/"%5:>a3+"/&Qi&1ot6,0%&j&X^?G3,^Ñ
H%576Z5:Hj;57,&5:)?G3+0%&"%$'&H%5:H O

¡ z| O

\^5:4RX&"%$45:HQ5:HL"%$'&\^JL_L!9`\,&102H%576=6,o"%$'&]C'6>a3,5:@3,8:8A3,RX"%5764HY3+0%&n3,H%H%l4>&C"/6)$43,&nl4457"

RX6H/" O
JA6g,&1&1?SH%571&=6,oB"%$'&=&X'3,>?G87&N>a3,43+;,&3+iG87&,@L87&1"1é H3,H%H%l4>&N3DRX6>?G87&1"/&.H/68:l'"%576W$43,Hi&1&
RX6>?Gl'"/&CT6487~Dot6,0m3,4C["%$43+"a0%&8:3^&CWH/&3+02R2$W5:Hl4H/&CT"/6DRX6>?Gl'"/&3.?G3+0%"%5:3,8Lm $ H/68:l'"%576 O
Ì 57;l'0%&HA-B3,4CnÊYH%$'6Eæqt?G3+0%"Í6,o|"%$'&j(Ñ0%&8:3^&CnH/?G3,RX&`&X^?G876,0%&CKiz~"%$'&G02H/"	3,4C]H/&RX64Cn57"/&1023+"%576@
0%&H/?&RX"%57,&87~,@'6,o3,Z ìLÏ ã  H/&3+02R2$=H/"%3+0%"%5:';ot0%6>Î"%$'&?40%6,iG87&> ;,63,8:H O
!T"%$'&)G02H/"a57"/&1023+"%576æq Ì 57;l'0%&Z-z|] ìLÏ ãQÑ ìPÌ \W5:HR13,8:87&CTEQ57"%$T3RX6H/"i6l44C6,oBçz@j3,H"%$45:H
5:Ha"%$'&&H/"%5:>a3+"/&CVRX6H/"6,oP"%$'&NH/"%3+0%"%5:';DH/"%3+"/&N;57,&kiz~["%$'&=?40%&RX6>?Gl'"/&CVm$'&l'025:H/"%5:R O JL$'&
0%6z6,"P'6^C'&n5:HY3, K
Ï nYì ÑF'6^C'&,@H/6EQ$'&N57"P5:HQ&X^?G3,4C'&CN ìLÏ ã  5:HYR13,8:87&CNot6,0P&3,R2$H%571&(H%l'iGH/&1"
q}8:5:'&Haq!wÊ|YÜTq!5w £|B5: Ì 57;l'0%&ç| O JL$'&G02H/"H%l4R2$#H%l'iGH/&1""/6i&;,&'&1023+"/&C5:[
H °5¢¤£S¦¥35
Q§´ O JL$45:HH/"%3+"/&B3,8:H/6n$43,H`3,&H/"%5:>a3+"/&CZRX6H/"`6,oÍçz@'H/6] ìLÏ ãQÑ ìPÌ \)5:HR13,8:87&CEQ57"%$a"%$45:Hi6l44C5:
"%$'&KG02H/"Y57"/&1023+"%576@iGl'"L"%$'&]"FE6)?6H%H%57iG87&B0%&1;,0%&H%H%5764HQ6,oM"%$45:HQH/"%3+"/&ni6,"%$N87&3,C="/6)H/"%3+"/&HPEQ57"%$N3
$457;$'&10QRX6H/"Q&H/"%5:>a3+"/&)q}3,&H/"%5:>a3+"/&C=RX6H/"Q6,oçn?G8:l4Hj3,=3,R1R1l4>]l48:3+"/&CNRX6H/"Q6,ojw| O JL$'&'&1EþRX6H/"
5:HQ?40%6,?G3+;3+"/&C.iG3,R%g="/6)"%$'&n?G3+0%&"PH/"%3+"/&,@EQ$'&10%&K"%$'&5:>?40%6,&CNRX6H/"B&H/"%5:>a3+"/&qt-z|L6,o"%$'&n3+"/6>
H/&1
" °5¢¤£S¦¥35¨Q§´5:HKH/"/6,0%&C5:"%$'&$'&l'025:H/"%5:Rn"%3+iG87&3,4C#0%&1"%l'02'&CVq}8:5:'&Hqç,Ê|YÜWqç,ý|
5: Ì 57;l'0%&aç| O \^5:4RX&n"%$45:HP?Gl'"%HP"%$'&&H/"%5:>a3+"/&C#RX6H/"6,o`"%$'&aH/"%3+"/&'6E3+i6,&a"%$'&ai6l44CN6,o"%$'&
 ìLÏ ã  R13,8:8Mq}8:5:'&aqt-z|j5: Ì 57;l'0%&ç|`'6>6,0%&57"/&1023+"%5764HL3+0%&C'6'& O JL$'&'&1EæRX6H/"Y5:H`0%&1"%l'02'&C)"/6
"%$'&B ìLÏ ãQÑ ìPÌ \)?40%6^RX&C4l'0%&P&X^?G3,4C45:';]"%$'&B0%6z6,"L'6^C'&,@'EQ$45:R2$)3,8:H/60%&1"%l'024H`H%5:4RX&B"%$'&B0%6z6,"L'6^C'&
5:Hj3, K
Ï nYì ÑF'6^C'&K3,4C57"j'6Er$43,HL3,l44H/687,&CH%l4R1RX&H%H/6,0]q}8:5:'&Hq!~w Ð,|MÜq!5w £|j5: Ì 57;l'0%&ç| O JL$45:H
45:H%$'&H"%$'&BG02H/"j57"/&1023+"%576 O
!N"%$'&nH/&RX64C57"/&1023+"%576[q Ì 57;l'0%&aÊ|L ìLÏ ãQÑ ìPÌ \.5:HYR13,8:87&CNEQ57"%$N3i6l44C6,o- O h"Y?40%6^RX&1&C4H
8:57g,&j"%$'&`G02H/"1@iGl'"M'6Ek"%$'&j&H/"%5:>a3+"/&CRX6H/"M6,o"%$'& K
& °Qu¡¢¤£S¦¥3
"¦¥3	ªh¦¥35
Ï nYì ÑF'6^C'f
Q§´Q5:HEQ57"%$45:K"%$'&Li6l44C@H/6B"%$45:HM'6^C'&L5:H	&X^?G3,4C'&C O JL$'&`G02H/"MH%571&Q(BH%l'iGH/&1"ot6,0MEQ$45:R2$n ìLÏ ã 
5:H`R13,8:87&C)5:J
H °Qu¡©¤£S¦¥3
"´@EQ57"%$)3,Z5:457"%5:3,84&H/"%5:>a3+"/&CZRX6H/"j6,o`w O JL$'&YG02H/"`57"/&1023+"%576o}3,5:8:H
"/6B4C3H/68:l'"%576not6,0"%$45:HH/"%3+"/&,@ziGl'"MH%5:4RX&j"%$'&L'&1EkRX6H/"6,o(B5:HH/"%5:8:8^EQ57"%$45:]"%$'&ji6l44C5:>?6H/&C
iz~"%$'&K?G3+0%&" K
Ï nYì ÑF'6^C'&,@3H/&RX64C.57"/&1023+"%576.5:HQC'6'&]EQ$45:R2$Z4C4HQ3H/68:l'"%576 O JL$'&]'&1ExRX6H/"
6,oL"%$'&Z3+"/6>èH/&1,
" °Qu¡¢¤£S¦¥3
"´)5:H]H/"/6,0%&C[5:D"%$'&)$'&l'025:H/"%5:Ra"%3+iG87&)3,4C5:3,C4C457"%576@	"%$'&
H/687,&CH/"%3+"/&HBq}3,876';EQ57"%$"%$'&5706,?4"%5:>a3,84H/68:l'"%576aRX6H/"|3+0%&Y3,8:84H/"/6,0%&C5:"%$'&QH/687,&Ca"%3+iG87&]q}8:5:'&H
1}
î

î Á(ï^ðzñ7òGó À

4ï %ô :õ Yö 4ï^ð-Gó-N÷æ s4ø- ù ¦Gï^õ¶

  1   f

{(img d4),(img d5),(img d6)}: 5 (4)

{(img d4),(img d5)}: 5 (4)
(tk_img d4)

(sw_on)
{(point d4),(off)}: 1 (1) + 1

{(point d0),(img d5)}: 3 + 1

{(point d5),(on),(cal),(img d4)}: 4 (3) + 1

{(point d4),(img d5)}: 4 (3)
(turn d0 d4)

...

(turn d1 d4)

{(point d1),(img d5)}: 3 + 1

{(img d5),(img d6)}: 3

(tk_img d5)

{(point d4),(on),(cal),(img d5)}: 4 (3) + 1

{(point d4),(on)}: 2 (1)

{(img d4),(img d6)}: 3

...
(tk_img d5)

...

{(point d4),(point d5),(on),(cal)}: 3 (2) + 1

(turn d6 d4)
...

{(point d6),(off)}: 0 + 2

Ì 57;l'0%&KÊzIQ9	3+0%"K6,oj"%$'&)(ÑF_L&1;,0%&H%H%576D"/0%&1&qt&X^?G3,4C'&C#"/6N3=RX6H/"]i6l44C6,o`-z|Bot6,0K"%$'&&X'3,>?G87&
\^3+"/&8:8:57"/&?40%6,iG87&> OjK
Ï nYì ÑF'6^C'&HY3+0%&]C'&1?G5:RX"/&Ciz~Z0%&RX"%3,';87&H1@ãP_jÑF'6^C'&HLiz~Z&8:8:57?GH/&H O
JL$'&RX6H/"B6,o&3,R2$D'6^C'&n5:HPEL0257"/"/&.3,HÕ!&H/"%5:>a3+"/&·
C t 3,R1R1l4>]l48:3+"/&C4Ö B
O n 6,"/&a"%$43+"B"%$'&
3,R1R1l4>]l48:3+"/&C[RX6H/"a5:HK6487~#3,876';N"%$'&?G3+"%$ot0%6>Ð"%$'&Z'&3+0%&H/"n3,4RX&H/"/6,0 K
Ï nYì ÑF'6^C'& O
6Ì ,0B'6^C'&HQEQ$'6H/&K&H/"%5:>a3+"/&C.RX6H/"P$43,HQi&1&Nl'?C43+"/&C=3+ot"/&10P&X^?G3,4H%576@G"%$'&K&H/"%5:>a3+"/&
i&1ot6,0%&&X^?G3,4H%5765:Hj;57,&=5:)?G3+0%&"%$'&H%5:H1I"%$45:Hj&H/"%5:>a3+"/&K5:4R18:l4C'&Hjl'?C43+"/&HL>a3,C'&K5:
"%$'&?40%&1^576l4H`57"/&1023+"%576Dq}H%$'6EQ=5: Ì 57;l'0%&P-z| O

q (9£|LÜ[qç^w|P5: Ì 57;l'0%&aç| O \^5:4RX&n"%$'&nG02H/"BH%l4R1RX&H%H/6,06,o"%$'& ÏKnYì ÑF'6^C'&aE`3,HH/687,&C.&X^?G3,4H%576
RX6"%5:zl'&HEQ57"%$."%$'&a'&X^"KH%l'iGH/&1"1@°Qu¡©¤£S¦¥35CQ§´ O JL$45:HBH/"%3+"/&$43,HKH/&1,&1023,8?6H%H%57iG87&
0%&1;,0%&H%H%5764H1@^H/6>&Y6,oEQ$45:R2$87&3,C"/6ãP_jÑF'6^C'&HiGl'"H/6>&P"/6 ÏKnYì ÑF'6^C'&H OÏ 8:8@z$'6E&1,&10@'0%&1"%l'02
3N>a5:45:>]l4>×qt&H/"%5:>a3+"/&N
C tÃ3,R1R1l4>]l48:3+"/&C|KRX6H/"n6,oL-'@MH/63,5:>?40%6,&C[RX6H/"Zqtot6,0n"%$'&Z3+"/6>úH/&1"
°Qu¡©¤£S¦¥35CQ§´+|A5:H	H/"/6,0%&C5:K"%$'&L$'&l'025:H/"%5:RM"%3+iG87&`3,4Cn"%$'&`?G3+0%&" KÏ nYì ÑF'6^C'&`0%&>a3,5:4H
l44H/687,&C OÍÏ H%5:>a5:8:3+0?40%6^RX&H%HÍ$43+?4?&4HEQ$'&]57"%HÍH%57iG8:5:';j'6^C'&,%@ °Qu¡¢Q§¦¥3
"¦¥3	ªh¦¥35
¤£S´@5:H&X^?G3,4C'&C@3,4CB"%$'&RX6H/"A6,o^"%$'&M3+"/6>þH/&1" °5¢¤£S¦¥35¨Q§´5:Hl'?C43+"/&CP64RX&>6,0%&,@
"/6Ê O
JL$'&K?40%6^RX&H%HQRX6"%5:zl'&HQ"%$'0%6l';$N3aot&1Ex>6,0%&]57"/&1023+"%5764H1@l4"%5:83,8:8"%$'&]H%571&n(aH%l'iGH/&1"%HQ6,o"%$'&
"/6,?'ÑF87&1,&8Í;,63,8H/&1"P$43,&ni&1&NH/687,&C@"%$'&n>6H/"PRX6H/"%87~=3+"B3RX6H/"P6,E
o Ð OBÏ "P"%$45:HL?65:"1@l'?C43+"/&C
&H/"%5:>a3+"/&HB6,ojý,Ê)H%571&a()3+"/6>ÐH/&1"%H$43,&i&1&H/"/6,0%&C#5:N"%$'&$'&l'025:H/"%5:RK"%3+iG87&,@AH%8:57;$"%87~87&H%HP"%$43,
$43,87o"%$'&Yzl4>Ki&10"%$43+"E6l48:C$43,&Pi&1&H/"/6,0%&C)57o3]RX6>?G87&1"/&Bm $ H/68:l'"%576$43,Ci&1&RX6>?Gl'"/&C O

ËTË ÛonQPQ¶|W#F¶nÎH!Iw`bt-n-ofInq>
H E`3,H"%$'&QR13,H/&YEQ57"%$"%$'&Ym'pS$'&l'025:H/"%5:Rj57"%H/&87oh@3,C43+?4"%5:';0%&8:3^&CH/&3+02R2$a"/6]"%$'&L"/&>?6,023,8GR13,H/&Y5:H
Ï M
H%5:>?G87&Q5:a?4025:4R157?G87&,@iGl'"H/6>&1EQ$43+"`RX6>?G8:5:R13+"/&C5:?4023,RX"%5:RX& OMÌ 5702H/"1@z"%$'&Y0%&8:3'3+"%576)5:"/0%6^C4l4RX&C
iz~K&zl43+"%5764HLqt-z|ÜqÊ|	3+?4?40%6'5:>a3+"/&H3P"/&>?6,023,8z0%&1;,0%&H%H%576H/"%3+"/J
& 8Pu 'q ®ay ¯]|Íiz~]3]1ez6,oH/"%3+"/&H
EQ57"%$'6l'"`3,RX"%5764H1@Ýc}c:@^6,oÍ"%$'&Pot6,02>Â'q ®F}y C| O JA6g,&1&1?=>a3+"/"/&102HLH%5:>?G87&,@z6487~a&zl43+"%576#qÊ|5:Hjl4H/&C
19«
î

À

4Á

4

;ª oQV« `uasWI^
u ytªA\ V« `uasWe^
;`abvKm,¬­I®¯xQ]`v@:lK5kv~	~	t5uahg ; `abv_m,¬­e®¯xQ]`v:lI5kv~	~	t5uahg
` « edX:Zvu\ V« `uasS°`	avWg
sd i g
z
qm; a V lu`rv` «« :l?;`tlc~~;`tl  WK^
X:Zv[\ V« `uasS°`	av=]B:lf~:l­AWg
tf V 
va	v;c « `uaslu`@;`abv;cAWK^
f5v;a¤gI`~t~:l~a « `uasskl;`ab5ua
z
±d[±@,g
z
` « edeX:Zvu\ V« `uasS°`	avWg
z
Ì 57;l'0%&KýzIJL$'&JL9M-a3,4C

}	-

?G8:3,445:';]?40%6^RX&C4l'0%&H O

5:)"%$'&0%&8:3^&CH/&3+02R2$IM"%$43+"Q5:H1@'"%$'&H%571&B6,o3aH/"%3+"/&K5:HLC'&X'&CZ3,H

á´q'®ay¯]|vázu³²² ®|I  /01q'-'|A²² {
²² 'Çü Ú þ , Ôÿ
²²

qý|

m  q'®ay¯]|`ÛVm

q*Ð,|

\z&RX64C@&1,&.H/63H/"%3+"/&]6,oMH%571&]87&H%HQ"%$43,=sÉ>a3~=H/"%5:8:8$43,&n3'6^Ñ&>?4"F
~ ¯ RX6>?6'&"1@3,4C
H%l4R2$]3QH/"%3+"/&`R13,]'6,"Ai&H/"/6,0%&C]5:"%$'&$'&l'025:H/"%5:R	"%3+iG87&PqtEQ$45:R2$K>a3+?GH6487~B3+"/6>xH/&1"%HA"/6Y3,H%H/6^R15:3+"/&C
RX6H/"%H| 
O n &57"%$'&10LR13,)"%$'&P6,?4"%5:>a3,8RX6H/"`6,0L876E&10ji6l44Cot6l44Cot6,0LH%l4R2$)3H/"%3+"/&Bi&PH/"/6,0%&CZ3,H`"%$'&
RX6H/"j6,oA"%$'&BRX6,0%0%&H/?64C45:';]3+"/6>ÃH/&1"qt0257;$"j$43,4CH%5:C'&Y6,oA&zl43+"%576#qý|/|2@GH%5:4RX&Y"%$'&P6,?4"%5:>a3,8RX6H/"
6,oÍ3,R2$457&1^5:';]"%$45:H3+"/6>ÎH/&1"j>a3~i&P876E&10 O Q6E&1,&10@G3]?G8:3,"%$43+"j3,R2$457&1,&<
H ®wI_´ 'Çü Ú þ , Ôÿ /01'q -'|
3+{
" :B3,8:H/6=3,R2$457&1,&H]"%$'&H/"%3+"/&'q ®ay ¯]|3+"]>6H/"]>a3 'Çü Ú þ , Ôÿ ²"%5:>&l4457"%H8:3+"/&10@Í"%$'0%6l';$D5:'&10%"%5:3^@
Ýc}c:@


 ®|I Ú  þ Ôÿ /01q'-'|2ySC 	 G  'Çü >aÚ þ 3, Ô ÿ ² 	
'Çü ,

JL$zl4H1@^"/6>a3,5:"%3,5:)"%$'&3,C4>a5:H%H%57iG5:8:57"F~n6,oÍ"%$'&$'&l'025:H/"%5:RQo}l44RX"%576)C'&X'&C)iz~"%$'&RX6"/&"%HQ6,oÍ"%$'&
$'&l'025:H/"%5:RQ"%3+iG87&,@'"%$'&8:3+0%;,&H/K
" ²K3,>6';3,8:83,RX"%5764HL5:p¯à5:HjH%l'i4"/023,RX"/&CZot0%6>Î"%$'&RX6H/"Li&1ot6,0%&B57"L5:H
H/"/6,0%&C O
ñY'ot6,0%"%l443+"/&87~,@,i6,"%$]6,o4"%$'&H/&jH%5:>?G8:5´R13+"%5764HE&3+g,&"%$'&j$'&l'025:H/"%5:R+3,8:l'&HÍot6l44CKiz~0%&8:3^&C
H/&3+02R2$ O áS$43+"5:HnE6,02H/&,@`H%5:4RX&Z3#RX6H/"l44C'&10/ÑF3+?4?40%6'5:>a3+"%5765:Ha3+?4?G8:57&CDEQ$'&TH/"/6,025:';#H/"%3+"/&H
RX6"%3,5:45:';nRX64R1l'0%0%&"j3,RX"%5764H1@'iGl'"'6,"jC4l'025:';K"%$'&BH/&3+02R2$@'"%$'&P$'&l'025:H/"%5:RQC'&X'&Ciz~a"%$'&P"%3+iG87&
3+ot"/&100%&8:3^&CUH/&3+02R2$æR13,Ui&5:4RX64H%5:H/"/&" O Ï 8:H/6'@Y0257;$"!ÑFH%$457ot")R1l'"%HR13,U'6,"Zi&l4H/&CU5:V"%$'&
0%&8:3^&CH/&3+02R2$ ONÏ H]>&"%576'&CD&3+028:57&10@5:#"%$'&H/&3+02R2$[H/?G3,RX&?402l4'&Ciz~0257;$"!ÑFH%$457ot"KR1l'"%H1@"%$'&
?6H%H%57iG87&KH%l4R1RX&H%H/6,02HY"/6Z3)H/"%3+"/&,@3,4CN"%$'&10%&1ot6,0%&n"%$'&nRX6H/"P0%&1"%l'02'&C=EQ$'&="%$'&H/"%3+"/&5:HY&X^?G3,4C'&C
qt0%&1;,0%&H%H/&C|QiGl'"B'6,"H/687,&C@A>a3~.i&C45´Ä&10%&"BC'&1?&4C45:';6"%$'&n?G3+"%$"%$'0%6l';$.EQ$45:R2$.57"BE`3,H
0%&3,R2$'&C OMÏ ;3,5:@"%$45:HR13,'6,"Mi&jH/"/6,0%&C5:n"%$'&L$'&l'025:H/"%5:R"%3+iG87&,@3,4Ca57"MR13,'6,"i&L57;'6,0%&CaH%5:4RX&
"%$45:HjRX6l48:CZ^5768:3+"/&B"%$'&K3,C4>a5:H%H%57iG5:8:57"F~n6,o	"%$'&$'&l'025:H/"%5:R O
1Ê
î

î Á(ï^ðzñ7òGó À

4ï %ô :õ Yö 4ï^ð-Gó-N÷æ s4ø- ù ¦Gï^õ¶

  1   f

ËyO}	-

JL$'& }	- ?G8:3,445:';L?40%6^RX&C4l'0%&Bq}H%$'6EQ5: Ì 57;l'0%&Lý|ARX64H%5:H/"%H	6,oG"%$'0%&1&j>a3,5:nH/"/&1?GH1IA"%$'&G02H/"	5:HÍ"/6
?40%&RX6>?Gl'"/&L"%$'&L"/&>?6,023,8Gm $ $'&l'025:H/"%5:R+@,"%$'&QH/&RX64Ca"/6K?&10%ot6,02> 3KH/&10257&H6,os)Ñ0%&8:3^&CH/&3+02R2$'&H1@
ot6,0ns uËçzy1{1{1{f@M5:D6,02C'&10n"/65:>?40%6,&"%$'&)$'&l'025:H/"%5:R+@3,4CD"%$'&43,85:Hn3, ìLÏ H/&3+02R2$T5:D"%$'&
"/&>?6,023,8Í0%&1;,0%&H%H%576NH/?G3,RX&,@;l45:C'&C=iz~"%$'&RX6>?Gl'"/&C.$'&l'025:H/"%5:R 
O n 6,"/&n"%$43+"P"%$'&]G02H/"Y3,4C.8:3,H/"
6,o"%$'&Q"%$'0%&1&YH/"/&1?GH3+0%&P5:C'&"%5:R13,8'"/6]"%$'6H/&Q6,oJL9M-'I	"%$'&L6487~C45´Ä&10%&4RX&Y5:H"%$'&Y5:"/&102>&C45:3+"/&YH/"/&1?@
"%$'&BH/&10257&H6,oA0%&8:3^&CH/&3+02R2$'&H O JL$'&Y?Gl'0%?6H/&Q6,o"%$'&H/&BH/&3+02R2$'&Hj5:H"/6C45:H%RX6,&10@'3,4C)H/"/6,0%&B5:"%$'&
$'&l'025:H/"%5:R"%3+iG87&,@5:>?40%6,&C.RX6H/"P&H/"%5:>a3+"/&HP6,oH/"%3+"/&HaqhÝc}c:@3+"/6>ØH/&1"%H|L6,oH%571&]s OÏ HP5:4C45:R13+"/&C
5: Ì 57;l'0%&ýz@0%&8:3^&C.H/&3+02R2$'&H3+0%&R13+0%0257&C=6l'"Pot6,0Psúu çzy1{1{1{f@l4"%5:8ÍH/6>&H/"/6,?4?G5:';RX64C457"%576
5:HjH%3+"%5:H!G&C O JL$'&10%&3+0%&KH/&1,&1023,80%&3,H/643+iG87&H/"/6,?4?G5:';RX64C457"%5764H`"%$43+"QR13,i&Bl4H/&CI
q/|)H/"/6,?NEQ$'&"%$'&n8:3,H/"Qs)Ñ0%&1;,0%&H%H%576=H/&3+02R2$.C'6z&HQ'6,"Y&4RX6l4"/&10P3,~ ÏKnYì ÑF'6^C'&)q}5:EQ$45:R2$
R13,H/&K"%$'&B0%&8:3^&CH/68:l'"%576Z5:HL5:)o}3,RX"Q3aH/68:l'"%576)"/6a"%$'&B6,0257;5:43,8?40%6,iG87&>|2û
q y2|)H/"/6,?N3+"Q3n4^&C#BÔdÝÒ+dÝ;57,&Zsû
q/|)H/"/6,?EQ$'&a"%$'&YRX6H/"6,o"%$'&Qs)ÑFH/68:l'"%576ot6l44Ca5:HM"%$'&YH%3,>&Y3,H"%$43+"6,o"%$'&nqt
s Gw|ÑFH/68:l'"%576
qt6,0Y$'&l'025:H/"%5:RQ&H/"%5:>a3+"/&|2û
q/|)H/"/6,?N3+ot"/&10Y3RX&10%"%3,5:=3,>6l4"j6,o	"%5:>&,@4zl4>Ki&10j6,o	&X^?G3,4C'&C'6^C'&H1@^6,0YH%5:>a5:8:3+0 O
5:>?G87&>&"%H"%$'&aG02H/"K"%$'0%&1& @
O + 3,R2$0%&H%l487"%HK5:D3NC45´Ä&10%&"]RX6^G;l'023+"%576D6,oj"%$'&?G8:3,4'&10@
,3 4Cl4H%l43,8:87~3,8:H/65:Z3C45´Ä&10%&4RX&5:Z?&10%ot6,02>a3,4RX& O !Z"%$'&KRX6>?&1"%57"%576@G3n4^&C8:5:>a57"j3+"Qsvuæç
E`3,Hl4H/&C OB+ 'RX&1?4"BEQ$'&10%&a57"B5:HP&X^?G8:5:R157"%87~H/"%3+"/&C#6,"%$'&10%EQ5:H/&,@"%$45:HB5:HP"%$'&aRX6^G;l'023+"%576l4H/&C5:
"%$'&&X^?&1025:>&"%H`?40%&H/&"/&C5:)"%$'&K'&X^"LH/&RX"%576=3,HLE&8:8 O
}	-

µM¶ F-º+¸·²»ºZ  » J(Fº¹ @CB F»²» ¼
 »  @

 
½

º

JL$45:HH/&RX"%576?40%&H/&"%H`3]RX6>?G3+025:H/66,o"%$'&P0%&8:3+"%57,&Y?&10%ot6,02>a3,4RX&Q6,oAJL9M-n3,4C }	- 6"%$'&PC'6+Ñ
>a3,5:4HQ3,4C?40%6,iG87&>äH/&1"%HQ"%$43+"YE&10%&]l4H/&C=5:"%$'&n(+*,*+-)?G8:3,445:';RX6>?&1"%57"%576@3,4C=3,N3,43,87~^H%5:H
6,o	"%$'&0%&H%l487"%H O JL$'&B0%&H%l487"%H`?40%&H/&"/&C=$'&10%&K3+0%&ot0%6>Î0%&102l4445:';]i6,"%$Z?G8:3,4'&102H`6"%$'&KRX6>?&XÑ
"%57"%576?40%6,iG87&>vH/&1"%H1@	'6,"B"%$'&3,RX"%l43,8M0%&H%l487"%HPot0%6>Ø"%$'&RX6>?&1"%57"%576 O JL$45:HP5:HBot6,0"FE6=0%&3,H/64H1I
Ì 5702H/"1@3,H3,870%&3,C'~n>&"%576'&C@&10%0%6,02HM5:n"%$'& }	- 5:>?G87&>&"%3+"%576a>a3,C'&L57"%H?&10%ot6,02>a3,4RX&L5:n"%$'&
RX6>?&1"%57"%576H/6>&1EQ$43+"E6,02H/&L"%$43,EQ$43+"57"5:H3,RX"%l43,8:87~R13+?G3+iG87&L6,o O \z&RX64C@"%$'&Q0%&1?&3+"/&C02l44H
E&10%&>a3,C'&nEQ57"%$N3)>6,0%&n;,&'&10%6l4HY"%5:>&8:5:>a57"L"%$43,N"%$43+"5:>?6H/&C=C4l'025:';"%$'&nRX6>?&1"%57"%576N"/6
6,i4"%3,5:>6,0%&KC43+"%3a3,4CZ&43+iG87&3ni&1"/"/&10YRX6>?G3+025:H/6 9 OÏ 8:H/6'@4H/6>&&X^?&1025:>&"%H`E&10%&02l4)EQ57"%$
3,87"/&10243+"%57,&LRX6^G;l'023+"%5764HÍ6,o"%$'&`?G8:3,4'&102H OÍì &1"%3,5:87&CC'&H%RX0257?4"%5764HÍ6,oG"%$'&LRX6>?&1"%57"%576nC'6>a3,5:4H
3+0%&;57,&Ziz~)Q6+Ä>a3,4@ + C'&87g+3,>?1e+7cPq(+*,*+-'¦@ ½| O

S ~¾u¿¢ ëtn-r#k
JL$'&S~¾u¿¢YC'6>a3,5:]>6^C'&8:HA"/023,4H/?6,0%"%3+"%576K6,oÕ!iG3+"%R2$'&H%ÖY6,oG?&1"/0%687&l4>æ?40%6^C4l4RX"%HA"%$'0%6l';$

"
y

wIg

3?G57?&8:5:'&)'&1"FE6,0%g O JL$'&>a3,5:WC45´Ä&10%&4RX&ot0%6> 6,"%$'&10"/023,4H/?6,0%"%3+"%576WC'6>a3,5:4Ha5:H"%$43+""%$'&
9¹©¦Q¦h®¬,¦h­¯ j¦!²1¨ · ¦h­¦YL§¦ · ¯ ¨©IÀÁT¨¯ j¦Q ¯ j¯ ¨º,¦h¨ · ¦!¦!²a£]2²+§n¡B©ª­³´ª2­j¦/2¥F©¬­ªº ¦!P±'¨©ª¸©
ª²n ¯ ¸©1¨ »P ª · ¦h­`L2¥F©¯ ²¦¨©+2²n¨©+%¨¦/§]¯ ²]¨©¦L¥!ªj¬,¦h¨¯ ¨¯ ª²3ô`²Â4²1¨¦h­¬­¯ ¦L£X¢¢¢ · ©¯ ¥F©n©+2L%¡
¬­ª1¥!¦!ª2­%Ã
¨  2¢
¢ 7YÅ
 Ä`2²+§a/¢X¡%£76ÆNj¦!jª2­»B¯ ²K¨ª2¨F27j¹©¦` ¨¯ ¬ ¦¬­ª1¥!¦!ª2­Mª¦h­M²ªP§µ2²1¨F2¸¦¨ª
¨©¦	¬ 2²²¦h­!±¯ ²¥!¦¨©¦!¦M%­¦ª2³4¥!ª­¦¯ ²¸ ¦ :¨©­¦/§¦/§±+º¨A%­¦¦/§Q¨ª­²Y¦!µX¦h­F2z¯ ²}¨F2²¥!¦!¯ ²P¬+%­F2  ¦!7±
©ª2­¨¦!²¯ ²¸¨©¦	ª%µX¦h­F2 A«tL2¶X¦!¬+2²°`ª2³^¨©¦¦h®¬,¦h­¯ j¦!²1¨/
1ë
î

À

14

1000

1000

100

a

100

Lower Bound on H*

10000

HSP* : Time (seconds)

HSP*a: Time (seconds)

10000

4Á

4

10

1
1

10

100

1000

TP4: Time (seconds)

q}3|

10000

10

1
1

10

100

qti|

1000

TP4: Time (seconds)

10000

13

12

TP4 (Search)
HSP*a (Rel. Search)
HSP* (Final Search)
a

11

10
10

100

1000

Time (seconds)

q}R|

10000

Ì 57;l'0%&LÐIP\z68:l'"%576)"%5:>&HLot6,0QJL9M-3,4C }	- 6?40%6,iG87&>aHjH/687,&C=5:Z"%$'&S~¾u¿¢aC'6>a3,5:
q}3|YEQ57"%$'6l'"P"%3,'g+3+;,&0%&H/"/025:RX"%576@3,4Ckqti|QEQ57"%$."%3,'g+3+;,&0%&H/"/025:RX"%5764H O JL$'&,&10%"%5:R13,8
8:5:'&B"/6"%$'&0257;$"Q5:)G;l'0%&qti|`5:4C45:R13+"/&Hj"%$'&K"%5:>&XÑ6l'"Q8:5:>a57"qt"%$zl4H1@G"%$'&K"%$'0%&1&?65:"%H
6]"%$'&`8:5:'&RX6,0%0%&H/?64CK"/6Y?40%6,iG87&>r5:4H/"%3,4RX&H	H/687,&CK6487~Biz~ }	- | O q}R| + ,68:l'"%576K6,o
"%$'&P876E&10i6l44C6)H/68:l'"%576RX6H/"`C4l'025:';B0%&8:3^&C3,4C'6,02>a3,8Íq}'6^Ñ0%&8:3^&C|H/&3+02R2$
5:?40%6,iG87&
> ¤Çn6,oA"%$'K
& S~¾u¿¢nC'6>a3,5:qt,&102H%576)EQ57"%$'6l'"`"%3,'g+3+;,&B0%&H/"/025:RX"%576| O
\z"%3+02HÍ5:4C45:R13+"/&MEQ$'&10%&H/68:l'"%5764HA3+0%&ot6l44C Omn 6,"/&"%$43+"	3,8:8"%5:>&H%R13,87&H	3+0%&876,;3+0257"%$4>a5:R O
?G57?&8:5:'&HM>]l4H/"`i&Q8:87&C3+"j3,8:8"%5:>&H1@'H/6nEQ$'&6'&BiG3+"%R2$)&"/&102Hj3]?G57?&nq}5:HnÕ!?Gl4H%$'&C4Ö|3,'6,"%$'&10
iG3+"%R2$N>]l4H/"Q87&3,&]"%$'&?G57?&B3+"Q"%$'&K6,"%$'&10Q&4Cqti&=Õ!?6,?4?&C4Ö| O JL$'&KC'6>a3,5:RX6>&HY5:Z"FE6,&10/Ñ
H%5764H1@^6'&EQ57"%$)0%&H/"/025:RX"%5764H`6WÕ!"%3,'g+3+;,&Ö.q}H/?G3,RX&ot6,0Y5:"/&102>&C45:3+0%~H/"/6,023+;,&|L3,4C)6'&EQ57"%$'6l'"
H%l4R2$Z0%&H/"/025:RX"%5764H O
Ï 87"%$'6l';$'&57"%$'&10ÍJL9M-Q'6,0 }	- 3,R2$457&1,&,&10%~P;,6z6^C0%&H%l487"%H5:B"%$45:HC'6>a3,5:@57"A5:H3,&X'3,>?G87&
6,oP3C'6>a3,5:EQ$'&10%& }	- ?&10%ot6,02>aH]i&1"/"/&10"%$43,WJL9M- O[Ì 57;l'0%&o
H Ð^q}3|n3,4CNÐ^qti|]RX6>?G3+0%&Z"%$'&
02l4"%5:>&H`6,o	"%$'&B"FE6?G8:3,4'&102H6Z"%$'&KH/&1"L6,o	?40%6,iG87&>aH`H/687,&CZiz~Z3+"Q87&3,H/"j6'& O
& Ð^q}R|YRX6>?G3+0%&HP"%$'&]i&$43^576l'0Y6,o"%$'&]"FE6Z?G8:3,4'&102HL6N6'&n&X'3,>?G87&]?40%6,iG87&>Z@ ¤Ç
Ì 57;l'0%
ot0%6>×"%$'&.C'6>a3,5:k,&102H%576kEQ57"%$'6l'""%3,'g+3+;,&.0%&H/"/025:RX"%576@Q5:k>6,0%&.C'&1"%3,5:8 O JL$45:Ha?40%6^5:C'&H3,
5:8:8:l4H/"/023+"%57,&&X'3,>?G87&]6,o0%&8:3^&C.H/&3+02R2$.EQ$'&N57"YE6,0%g^HB3,HP5:"/&4C'&C O \^5:4RX&Ki6,"%$N?G8:3,4'&102HQl4H/&
57"/&1023+"%57,&C'&1&1?&45:';LH/&3+02R2$'&H1@,"%$'&i&H/"Ag^'6EQK876E&10Íi6l44CB6"%$'&RX6H/"Í6,o'"%$'&?40%6,iG87&>æH/68:l'"%576
EQ5:8:8i&5:4RX0%&3,H%5:';'@	H/"%3+0%"%5:';Not0%6>Ð"%$'&)5:457"%5:3,8m $ &H/"%5:>a3+"/&,@"%$'0%6l';$3NH/&10257&HK6,o]qt0%&8:3^&C[3,4C
'6^Ñ0%&8:3^&C|H/&3+02R2$'&H`EQ57"%$)5:4RX0%&3,H%5:';Ki6l44C@^l4"%5:83nH/68:l'"%576)5:Hot6l44CIÍ"%$'&P;,023+?G$)?G876,"%H"%$45:H
&1,68:l'"%576a6,o"%$'&YH/68:l'"%576aRX6H/"`876E&10i6l44C3+;3,5:4H/""%5:>& OÏ HR13,i&QH/&1&@'çÑ0%&1;,0%&H%H%576H/&3+02R2$
0%&3,R2$'&H3.H/68:l'"%576SqtEQ57"%$RX6H/"w(|o}3,H/"/&10"%$43,"%$'&Z'6,02>a3,80%&1;,0%&H%H%576[H/&3+02R2$TC45:H%RX6,&102H]"%$43+"
"%$'&10%&]5:HL'6H/68:l'"%576ZEQ57"%$45:)"%$'&KH%3,>&]RX6H/"Yi6l44C O JL$'&B43,8q}'6^Ñ0%&8:3^&C|0%&1;,0%&H%H%576=H/&3+02R2$
5: }	- 5:HL3,8:H/6ao}3,H/"/&10Q"%$43,"%$43+"L6,oMJL9M-=q}3,HY5:4C45:R13+"/&C)iz~)"%$'&KH%876,?&B6,o	"%$'&KR1l'0%,&|2@C4l'&B"/6"%$'&
$'&l'025:H/"%5:RY5:>?40%6,&>&"%HQH/"/6,0%&CC4l'025:';n"%$'&B0%&8:3^&CH/&3+02R2$ O

"

y 

wIg

¡¢QÈ¾h3

n-kg|

¢ ëtn-r#kgq

< &10%"%3,5:g^5:4C4HM6,oA>6^C'&8GR2$'&R%g^5:';n?40%6,iG87&>aH1@zH%l4R2$3,H"%$'&PC'&1"/&RX"%576)6,oAC'&3,C4876^R%g^H3,4C3,H%H/&10%"%576

^5768:3+"%5764H1@3+0%&&H%H/&"%5:3,8:87~zl'&H/"%5764H6,oY0%&3,R2$43+iG5:8:57"F~Uqt6,0l4'0%&3,R2$43+iG5:8:57"F~4|K5:TH/"%3+"/&XÑ"/023,4H%57"%576
;,023+?G$4H O JL$'r
& ¡¢QÈ¾h3]C'6>a3,5:5:H"%$'&Y0%&H%l487"M6,o"/023,4H%8:3+"%5:';KH%l4R2$)>6^C'&8GR2$'&R%g^5:';]?40%6,iG87&>aH1@ot6,0
H/~^H/"/&>Ã>6^C'&8:H`&X^?40%&H%H/&C5:)"%$'Ê
& É`d/Ò+ÓX7KH/?&R15´R13+"%576Z8:3,';l43+;,&,@G5:"/6a9 ìPì å O JL$'&B?40%6,iG87&>aH

~
î

î Á(ï^ðzñ7òGó À

4ï %ô :õ Yö 4ï^ð-Gó-N÷æ s4ø- ù ¦Gï^õ¶

  1   f

10000

HSP*a: Time (seconds)

HSP*a: Time (seconds)

10000

1000

100

10

10

100

1000

TP4: Time (seconds)

100

1

1

10000

100

TP4: Time (seconds)

q}3|

10000

qti|

Ì 57;l'0%&{£zIP\z68:l'"%576)"%5:>&Hjot6,0QJL9M-3,4C }	- 6Z?40%6,iG87&>aHjH/687,&C=5:#q}3|`"%$'&K¡¢QÈ¾h3aC'6>a3,5:
Tq ¤ËS==,¤Ë¾¢SZH%l'iGH/&1"|3,4CUqti|"%$'&¢DC'6>a3,5:UqvSh¤N5:4H/"%3,4RX&)H%l'iGH/&1"| O JL$'&
,&10%"%5:R13,8Q8:5:'&)"/6#"%$'&Z0257;$"5:4C45:R13+"/&Hn"%$'&"%5:>&XÑ6l'"8:5:>a57"Zqt"%$zl4H1@?65:"%Hn6W"%$'&=8:5:'&
RX6,0%0%&H/?64C)"/6a?40%6,iG87&>Î5:4H/"%3,4RX&HLH/687,&CZ6487~iz~ }	- | O
l4H/&CW5:T"%$'&NRX6>?&1"%57"%576W3+0%&N5:4H/"%3,4RX&H6,oB"FE6C45´Ä&10%&"C'&3,C4876^R%gTC'&1"/&RX"%576k?40%6,iG87&>aHqt"%$'&
Õ/C45:45:';]?G$45:876H/6,?G$'&102H%Ö]3,4CWÕ!6,?4"%5:R13,8"/&87&1;,023+?G$4Ö?40%6,iG87&>aH|M6,o5:4RX0%&3,H%5:';H%571& O
JL$'H
& ¢KC'6>a3,5:n>6^C'&8:HA"%$'&`?40%6,iG87&>þ6,o40%&RX6^G;l'025:';P3Yo}3,l487"F~?6E&10M'&1"FE6,0%g]"/6P0%&H%l'?4?G87~
RX64H%l4>&102Hn3Ä&RX"/&C[iz~#"%$'&)o}3,l487" O ñY4RX&10%"%3,5:"F~[RX64RX&10245:';="%$'&Z5:457"%5:3,8MH/"%3+"/&Z6,oL"%$'&)?40%6,iG87&>
qt"%$'&zl4>Ki&10K3,4C#876^R13+"%576#6,o`o}3,l487"%H|2@Íl4'0%&8:5:3+iG87&n3,RX"%5764HK3,4C?G3+0%"%5:3,8@AH/6>&1"%5:>&H&1,&Do}3,8:H/&,@
6,iGH/&10%+3+"%5764H3+0%&Z5:>?6,0%"%3,"]ot&3+"%l'0%&H6,oQ"%$'&Z3+?4?G8:5:R13+"%576@iGl'"]"%$'&H/&3,H/?&RX"%HnE&10%&H%5:>?G8:5´G&C
3E`3~ot0%6>Ã"%$'&]C'6>a3,5:l4H/&C=5:Z"%$'&KRX6>?&1"%57"%576 O JL$'&KC'6>a3,5:C45:C$'6E&1,&10B>a3+g,&]H%57;45´R13,"
l4H/&=6,o ÏYì åxRX64H/"/02l4RX"%H)3,4Ck"%$'&N'&1EvC'&10257,&Ck?40%&C45:R13+"/&Hot&3+"%l'0%&N6,o9 ìPì åÍ( O ( O JL$'& ÏYì å
RX64H/"/02l4RX"%H3,4CnC'&10257,&C]?40%&C45:R13+"/&H	R13,ni&`RX6>?G5:87&Cn3E`3~,@iGl'"Í6487~K3+"M3,]&X^?6'&"%5:3,8^5:4RX0%&3,H/&
5:k?40%6,iG87&>ÚH%571& O JL$'&10%&1ot6,0%&N6487~T"%$'&.H%>a3,8:87&H/")5:4H/"%3,4RX&HE&10%&3+3,5:8:3+iG87&.5:k?G8:3,5:S\^JL_L!9`\
ot6,02>]l48:3+"%576@3,4C[i&R13,l4H/&Z6,oY"%$45:H]"%$'&1~E&10%&Z"%$'&Z6487~5:4H/"%3,4RX&Hn"%$43+"aJL9M-#3,4C }	- RX6l48:C
3+"/"/&>?4"L"/6H/687,& O
JL$',
& ¡¢QÈ¾h33,4C.¢C'6>a3,5:4H3+0%&'6^Ñ"/&>?6,023,8@5:"%$'&=H/&4H/&Z"%$43+"a3,RX"%576kC4l'023+"%5764H
3+0%&.'6,")RX64H%5:C'&10%&C@jiGl'"'&57"%$'&103+0%&N"%$'&1~kH/"/025:RX"%87~TH/&zl'&"%5:3,8@Ýc}c:@L3,RX"%5764H)R13,V"%3+g,&.?G8:3,RX&
RX64R1l'0%0%&"%87~ O Æ&R13,l4H/&)6,oj"%$45:H1@	JL9M-N3,4C }	- E&10%&02l4D5:#?G3+023,8:87&8@A023+"%$'&10]"%$43,#"/&>?6,023,8@
?G8:3,445:';>6^C'&6D?40%6,iG87&>aH]5:#"%$'&H/&ZC'6>a3,5:4HX * O JL$'&0%&H%l487"%H6,oL"%$'&"FE6?G8:3,4'&102H1@	H%$'6EQ
5: Ì 57;l'0%[
& £z@3+0%&]H%5:>a5:8:3+0j"/6"%$'6H/&K&X'$457iG57"/&C5:Z"%$'f
& S~¾u¿¢C'6>a3,5:I }	- 5:Hji&1"/"/&10Y"%$43,
JL9M-P6,&1023,8:8@H/687^5:';P>6,0%&?40%6,iG87&>aHÍ5:]i6,"%$nC'6>a3,5:4HÍ3,4CnH/687^5:';Y"%$'&`$43+02C'&105:4H/"%3,4RX&HÍo}3,H/"/&10@
EQ$45:87&PJL9M-5:H`o}3,H/"/&10Y3+"QH/687^5:';n&3,H/~Z5:4H/"%3,4RX&H O

"TËOwIg 9Q¾h¤hQ¾ ëtn-r#k
JL$'&L9Q¾h¤hQ¾KC'6>a3,5:>6^C'&8:HH%3+"/&8:8:57"/&H"%3,H/g,&C)EQ57"%$>a3+g^5:';]3,H/"/0%6'6>a5:R13,86,iGH/&10%+3+"%5764H OMÏ
y

H%5:>?G8:5´G&C\^JL_L!9`\,&102H%5766,o"%$'&BC'6>a3,5:E`3,HjC'&H%RX0257i&C5:Z\z&RX"%576ç O ( O w O !"%$'&P;,&'&1023,8C'6+Ñ
/¢ '%­F2  ¦!1¬ 2²²¯ ²¸¯ ¨©¦4¬,¦!¥!¯ 21¥/2¦ª2³¨¦!j¬,ª2­F2X¬ 2²²¯ ²¸¨©+%¨­¦! ¨ · ©¦!²2 2¥h¨¯ ª²z©+/µX¦4²¯ ¨^§­F%¨¯ ª²!
À¦h­¨F2¯ ²Pª¬¨¯ j¯ Ä/%¨¯ ª²³´ª2­A¨©¯ ¥/2¦M%­¦¯ j¬ ¦!j¦!²1¨¦/§ 7¯ §¦!²1¨¯ ¥/2  »+¤¯ ²Yº,ª2¨©P¬ 2²²¦h­!

~,<
î

À

4Á

4

4h

10000

HSP*a: Time

Time (seconds)

1h
1000

100

10

TP4
HSP*

1

CPT
p01

p02

p03

p04

p05

p06

p07

Problem (source of parameters)

1m

1s

a

0.1s
0.1s

p08

q}3|

1s

1m

TP4: Time

1h

4h

qti|

Ì 57;l'0%&KâzIP\z68:l'"%576"%5:>&HYot6,0PJL9M-)3,4C }	  6=?40%6,iG87&>aHYH/687,&C.5:"%$'&,9Q¾h¤hQ¾C'6>a3,5: O
Ì 5:8:87&CVqtiG8:3,R%g4|?65:"%H]0%&1?40%&H/&"n5:4H/"%3,4RX&H]i&876';5:';"/6."%$'&ZRX6>?&1"%57"%576?40%6,iG87&>
H/&1"1@EQ$45:87&0%&>a3,5:45:';=?65:"%Hn3+0%&Zot0%6>É"%$'&H/&1"n6,oY3,C4C457"%57643,8?40%6,iG87&>aH];,&'&1023+"/&C O
"%$'&RX6>?&1"%57"%576
+H/&13,"jR23,$r4CÕ!EQH%5:$'C'6&EQÖZHRX"%6$'8:l4&P>aH/6.58:l':"%N576G";l'%5:0%>&=&q}H3ot|Y6,0%0`&1"%?4$'0%&P&H/H/&&1""`%6,HBoA65:'&4H/"%?43,4R0%6,XiG&H87&;,>v&'&ot10%0263+>v
"/&C)EQ57"%$a"%$'&BH%3,>&
?G3+023,>&1"/&102H.qt;,0%6l'?&Ck5:"/6H%l'iRX68:l4>a4Hniz~T?G8:3,4'&10X| O ãP487~[H/687,&CV5:4H/"%3,4RX&H3+0%&
H%$'6EQ@4H/6a'6,"Q3,8:8RX68:l4>a4Hj$43,&"%$'&H%3,>&Kzl4>Ki&10j6,oÍ?65:"%H OMÌ 57;l'0%&aqti|`RX6>?G3+0%&H
JL9M-n3,4C }	- C4570%&RX"%87~ O JL$'&Y,&10%"%5:R13,88:5:'&Q"/6n"%$'&P0257;$"`5:4C45:R13+"/&H"%$'&Y"%5:>&XÑ6l'"j8:5:>a57"
qt"%$zl4H1@4?65:"%H`6Z"%$'&K8:5:'&P3+0%&K5:4H/"%3,4RX&HLH/687,&CZiz~ }	- iGl'"L'6,"Liz~)JL9M-z| O

>a3,5:@^"%$'&10%&R13,Zi&B>6,0%&P"%$43,Z6'&H%3+"/&8:8:57"/&,@^&3,R2$&zl457?4?&CEQ57"%$)>6,0%&B"%$43,)6'&5:4H/"/02l4>&"1@
3,4CNC45´Ä&10%&"P5:4H/"/02l4>&"%HQ$43,&C45´Ä&10%&"P5:>a3+;5:';R13+?G3+iG5:8:57"%57&H]q}R13,8:87&CVÕ/>6^C'&H%Ö|2@EQ$45:R2$=>a3~
6,&1028:3+?qti&1"FE&1&=5:4H/"/02l4>&"%H3,4C)i&1"FE&1&=H%3+"/&8:8:57"/&H| H
O + 3,R2$Z;,63,85:H`"/6a$43,&K3,Z5:>a3+;,&B"%3+g,&
6,oA3]H/?&R15´RL"%3+0%;,&1"1@45:3]H/?&R15´RQ>6^C'& OÏ H`5:a"%$'&K\^JL_L!9`\n,&102H%576@z"%3+g^5:';n3,5:>a3+;,&Y0%&zl4570%&H
"%$'&a0%&87&1+3,"K5:4H/"/02l4>&"P"/6=i&?6E&10%&C#6D3,4C#R13,8:57i4023+"/&C@A3,4C#"/6=R13,8:57i4023+"/&3,#5:4H/"/02l4>&"
"%$'&H%3+"/&8:8:57"/&>]l4H/"i&?65:"/&C"/6E`3+02C4H]3R13,8:57i4023+"%576"%3+0%;,&1" O JÍl'0245:';)"%5:>&HBi&1"FE&1&C45´Ä&10/Ñ
&"C4570%&RX"%5764HY+3+0%~ OÏ HB3,#3,C4C457"%57643,8ÍRX6>?G8:5:R13+"%576@3+">6H/"B6'&a5:4H/"/02l4>&"Y6i63+02C.&3,R2$
H%3+"/&8:8:57"/&BR13,Zi&P?6E&10%&CZ63+"L3,~"%5:>& O JL$zl4H1@^"/6>a5:45:>a571&Q6,&1023,8:8&X^&R1l'"%576Z"%5:>&P0%&zl4570%&H
3R13+0%&1o}l48ÍH/&87&RX"%576=6,oEQ$45:R2$H%3+"/&8:8:57"/&)q}3,4CN5:4H/"/02l4>&"|"/6l4H/&Kot6,0Y&3,R2$N6,iGH/&10%+3+"%576@3,4C="%$'&
6,02C'&10Q5:)EQ$45:R2$)&3,R2$NH%3+"/&8:8:57"/&BR13+0%0257&Hj6l'"L"%$'&6,iGH/&10%+3+"%5764Hj57"L$43,HLi&1&Z3,H%H%57;'&C O
JL$45:HC'6>a3,5:V5:H)$43+02CVot6,0)i6,"%$SJL9M-T3,4C }	- @Qot6,0H/&1,&1023,8P0%&3,H/64H1I Ì 5702H/"1@Q3,HZ3,870%&3,C'~
>&"%576'&C@z"%$'&BRX6,0%&P6,o"%$'&BC'6>a3,5:5:H3nRX6>KiG5:43+"%5763,)3,H%H%57;4>&"?40%6,iG87&> 3,4C)3]JY\^9ÑF8:57g,&
?40%6,iG87&>Z@^i6,"%$)6,o	EQ$45:R2$Z3+0%&$43+02C)6,?4"%5:>a573+"%576)?40%6,iG87&>aH OÏ 8:H/6'@'"%$'&]m $ $'&l'025:H/"%5:RQ"/&4C4Hj"/6ai&
?G3+0%"%5:R1l48:3+0287~]E&3+ga6)JY\^93,4C0%&8:3+"/&C?40%6,iG87&>aHYqt"%$'&PH%3,>&QE&3+g^'&H%H`$43,H3,8:H/6]i&1&'6,"/&Ciz~
\^>a57"%$=q(+*,*+-z|Mot6,0"%$'&Q?G8:3,445:';Y;,023+?G$$'&l'025:H/"%5:R+@EQ$45:R2$a5:HM&H%H/&"%5:3,8:87~]"%$'&YH%3,>&Y3,Hm $ | O \z&RX64C@
3,RX"%576=C4l'023+"%5764H`5:)"%$45:HjC'6>a3,5:ZC45´Ä&10jiz~Z8:3+0%;,&3,>6l4"%HL3,4C3+0%&K3+"L"%$'&H%3,>&"%5:>&H/?&R15´G&C
EQ57"%$T3.,&10%~[$457;$0%&H/68:l'"%576 O[Ì 6,0&X'3,>?G87&,@5:?40%6,iG87&ó
> > .6'&3,RX"%576k$43,H3C4l'023+"%576[6,o
w+{ (+*+-#3,4C[3,'6,"%$'&103.C4l'023+"%576#6,
o £,(z{ â,â O áS$'&[l4H%5:'; ìL
Ï  EQ57"%$D"/&>?6,023,80%&1;,0%&H%H%576@"%$'&
RX6H/"Bi6l44C="/&4C4HP"/65:4RX0%&3,H/&niz~="%$'&n;R1Ckqt;,0%&3+"/&H/"KRX6>a>6C457^5:H/6,0X|j6,o3,RX"%576#C4l'023+"%5764HY5:

~

î sî

î Á(ï^ðzñ7òGó À

4ï %ô :õ Yö 4ï^ð-Gó-N÷æ s4ø- ù ¦Gï^õ¶

  1   f

&3,R2$D57"/&1023+"%576@&X'RX&1?4"Kot6,0B"%$'&G02H/"Pot&1E57"/&1023+"%5764HX/ O !"%$'&x9Q¾h¤hQ¾ZC'6>a3,5:@"%$'&;R1C#6,o
3,RX"%576ZC4l'023+"%5764H`5:H"F~z?G5:R13,8:87~,&10%~)H%>a3,8:8	qt6)"%$'&P6,02C'&10j6,o * * | O <6>KiG5:'&CEQ57"%$"%$'&PE&3+g^'&H%H
6,oP"%$'&.m $ $'&l'025:H/"%5:R+@EQ$45:R2$W>&3,4Ha"%$'&NC45´Ä&10%&4RX&i&1"FE&1&W"%$'&N5:457"%5:3,8`$'&l'025:H/"%5:R)&H/"%5:>a3+"/&=6,o
"%$'&H/68:l'"%576NRX6H/"aq}>a3+g,&H/?G3,|L6,o3?40%6,iG87&>ä3,4CN"%$'&3,RX"%l43,8	6,?4"%5:>a3,8ÍRX6H/"B5:HY6,ot"/&8:3+0%;,&,@"%$45:H
0%&H%l487"%HÍ5:K3,n3,8:>6H/"	3,H/"/0%6'6>a5:R13,8zzl4>Ki&10Í6,o4 ìL
Ï  57"/&1023+"%5764HAi&5:';Q0%&zl4570%&Ci&1ot6,0%&3YH/68:l'"%576
5:Hot6l44C O JA63,65:C)"%$45:HBq}H/6>&1EQ$43+"j3+0%"%5´R15:3,8t|?40%6,iG87&>Z@^3,RX"%576ZC4l'023+"%5764HE&10%&P0%6l44C'&Cl'?)"/6
"%$'&P'&3+0%&H/"`5:"/&1;,&10j5:a"%$'&Q&X^?&1025:>&"%HC'6'&Y5:a"%$45:HC'6>a3,5: O JL$45:H5:4RX0%&3,H/&H"%$'&P>a3+g,&H/?G3,6,o
"%$'&j?G8:3,4HÍot6l44C@,iGl'"'6,"M,&10%~n>]l4R2$]ÜK6a3,&1023+;,&Qiz~(z{ â Ì@z3,4C3+">6H/"iz~Êz{ â ÌÎq}RX6>?G3+025:H/6
>a3,C'&B6"%$'&B?40%6,iG87&>aH`"%$43+"QRX6l48:C)i&H/687,&CZEQ57"%$)6,0257;5:43,8C4l'023+"%5764H|% $ O
ì l'&"/6Z"%$'&nE&3+g^'&H%HB6,o"%$'&m $ $'&l'025:H/"%5:RK5:N"%$45:HPC'6>a3,5:@"%$'&&XÄ6,0%"5:,&H/"/&C.iz~ }	- 5:
RX6>?Gl'"%5:';B3>6,0%&L3,R1R1l'023+"/&Q$'&l'025:H/"%5:RR13,ai&`&X^?&RX"/&C"/6?G3~n6+Ä	@0%&H%l487"%5:';P5:3Bi&1"/"/&10M6,&1023,8:8
02l4"%5:>&Yot6,0 }	- RX6>?G3+0%&C)"/6aJL9M- O JL$45:H5:Hj5:4C'&1&C"%$'&R13,H/&,I3,87"%$'6l';$ }	- H/687,&H`6487~"%$'&
G,&H%>a3,8:87&H/"P?40%6,iG87&>aHB5:N"%$'&H/&1"q}H%$'6EQ#3,HBiG8:3,R%g.?65:"%HB5: Ì 57;l'0%&aâ'qti|/|2@	JL9M-H/687,&H6487~
ot6l'0L6,oÍ"%$'6H/&,@G3,4C5:HjH%8:57;$"%87~H%876E&10L6>6H/"L6,oÍ"%$'&> O JL$'&H/&B0%&H%l487"%H1@'$'6E&1,&10@3+0%&K'6,"Qzl457"/&
0%&1?40%&H/&"%3+"%57,& O
JL$'
& 9Q¾h¤hQ¾BC'6>a3,5:a$43,H38:3+0%;,&Qzl4>Ki&10M6,o?40%6,iG87&> ?G3+023,>&1"/&102H1IÍ"%$'&Yzl4>Ki&106,o;,63,8:H
3,4C"%$'&Qzl4>Ki&106,oH%3+"/&8:8:57"/&H1@5:4H/"/02l4>&"%HM3,4C"%$'&Q5:4H/"/02l4>&"R13+?G3+iG5:8:57"%57&H1@G1ec:@EQ$45:R2$C'&1"/&10/Ñ
>a5:'&B"%$'&Kzl4>Ki&10L6,o	E`3~^HQ"/63,R2$457&1,&K&3,R2$=;,63,8 O 9M0%6,iG87&>Ã5:4H/"%3,4RX&HQl4H/&C5:Z"%$'&KRX6>?&1"%57"%576
E&10%&Q;,&'&1023+"/&C023,4C'6>a87~,@EQ57"%$+3+0%~^5:';B?G3+023,>&1"/&10`H/&1"/"%5:';HX . O JL$'&LRX6>?&1"%57"%576a?40%6,iG87&> H/&1"1@
EQ$45:R2$D$43,H]"/6N6+Ä&10R2$43,8:87&';5:';=?40%6,iG87&>aH"/6.3=EQ5:C'&+3+0257&1"F~#6,oL?G8:3,4'&102Hqti6,"%$D6,?4"%5:>a3,83,4C
H%l'i6,?4"%5:>a3,8t|EQ$45:87&Bot6,0Y?4023,RX"%5:R13,8A0%&3,H/64HP'6,"Qi&5:';a"/6z6)8:3+0%;,&,@H%R13,87&HYl'?N"%$'&]C45´Ä&10%&"Q?G3+023,>nÑ
&1"/&102HPzl457"/&KH/"/&1&1?G87~,@3,4CZÜ>6,0%&]5:>?6,0%"%3,"%87~ÜZRX6"%3,5:4HQ6487~Z6'&K?40%6,iG87&>ä5:4H/"%3,4RX&Kot6,0Y&3,R2$
H/&1"j6,o	?G3+023,>&1"/&102HLl4H/&C O Q6E&1,&10@G"%$'&$43+02C4'&H%H`6,o	3n?40%6,iG87&>5:4H/"%3,4RX&>a3~)C'&1?&4CZ3,HL>]l4R2$
q}57o'6,">6,0%&|Q6."%$'&n023,4C'6>Ë&87&>&"%HB6,o"%$'&?40%6,iG87&>ä;,&'&1023+"%576kqtEQ$45:R2$.5:4R18:l4C'&,@Mc Åc:@"%$'&
"%l'0245:';]"%5:>&Hji&1"FE&1&"%3+0%;,&1"%HY3,4CZ"%$'&3,RX"%l43,8A3,8:876^R13+"%576)6,oR13+?G3+iG5:8:57"%57&H`3,4CR13,8:57i4023+"%576"%3+0/Ñ
;,&1"%H"/65:4H/"/02l4>&"%H|Q3,HB6"%$'&aH/&1"/"%5:';HB6,o"%$'&aRX6"/0%68:8:3+iG87&n?G3+023,>&1"/&102H O JA6=5:,&H/"%57;3+"/&a"%$'&
5:>?6,0%"%3,4RX&L6,o"%$'&Q023,4C'6>?40%6,iG87&> &87&>&"%Hot6,0?40%6,iG87&>$43+02C4'&H%H1@3,4C"/6]6,i4"%3,5:3i40%63,C'&10
iG3,H%5:HPot6,0"%$'&aRX6>?G3+025:H/6.i&1"FE&1&DJL9M-3,4C }	- @"/&#3,C4C457"%57643,8Í?40%6,iG87&>aHYE&10%&a;,&'&1023+"/&C
q}l4H%5:';"%$'&P3+3,5:8:3+iG87&Q?40%6,iG87&>à;,&'&1023+"/6,0X|ot6,0`&3,R2$6,o"%$'&Q?G3+023,>&1"/&10jH/&1"/"%5:';H`RX6,0%0%&H/?64C45:';B"/6
"%$'&n&57;$"H%>a3,8:87&H/"Y?40%6,iG87&>aHY5:N"%$'&RX6>?&1"%57"%576.H/&1" O JL$'&nC45:H/"/0257iGl'"%5766,oH/68:l'"%576N"%5:>&HYot6,0
JL9M-'@ }	- 3,4CN<`9Jqt"%$'&6487~6,?4"%5:>a3,8"/&>?6,023,8?G8:3,4'&10ji&H%5:C'&HjJL9M-a3,4C }	- "/6?G3+0%"%5:R15´Ñ
¹4£]2²+§_./10
2 ¨­¦/%¨L2¥h¨¯ ª²§­F%¨¯ ª²j2­F%¨¯ ª²+2 êôPº1»n¨©¦Q¸¥/§aª2³A¨ · ª­F%¨¯ ª²+2 'Í2²+§ÎQ¯ `j¦/2²1¨¨©¦
¸2­¦/%¨¦!}¨­F%¨¯ ª²+2Ï¥F©¨©+%¨OÍ&ÑÐÒÏ2²+§Î¸&ÑÓÏ³´ª2­¯ ²1¨¦!¸¦h­ÔÐþ2²+§Ó'¸Íª2¨¦¨©+%¨	¨©¦¬ 2²²¦h­§ª*,+¥!ªj¬¨¦M¨©¦`¸¥/§Kª2³2¥h¨¯ ª²§­F%¨¯ ª²M2²+§¦¨©¯ 	¨ªY¯ ²¥h­¦!j¦!²1¨	¨©¦¥!ª}¨º,ª²+§`¹©¦`º,ª²+§¯ ¯ ²]¦/2¥F©
¯ ¨¦h­F%¨¯ ª²L¯ ²¥h­¦/2¦/§j¨ª¨©¦¥!ª}¨ª2³¨©¦A ¦/2}¨¥!ª}¨ »j²ª§¦¨©+%¨ · 2²ª2¨¦h®¬+2²+§¦/§`§¦¨ªM©+/µ¯ ²¸MM¥!ª}¨2º,ª%µX¦
¨©¦Lº,ª²+§]¯ ²¨©¦L¬­¦!µ¯ ª¯ ¨¦h­F%¨¯ ª² :2¬,¦h­`}¨F2²+§%­F[
§ 
ÍmùúQ¦/%­¥F©,¤F]¹©+%¨¨©¯ M³ ­s¦ È1¦!²1¨ »n©+2¬¬,¦!²M¨ª
º,¦j 7ª²P¨©¦ª2­F§¦h­Íª2³F¤¨©¦¸¥/§Pª2³2¥h¨¯ ª²§­F%¨¯ ª²A¯ Í2² 7²+§¦!¯ ­F2º ¦%¤¦ ¦!¥h¨Aª2³4¨©¦º­F2²¥F©¯ ²¸`­ ¦¦/§Y¨ª
¸¦!²¦h­F%¨¦	¨©¦	¦/%­¥F©P¬+2¥!¦
%¡1 ÕÍ¬¨¯ L2 ¯ ¨t»¥/2²º,¦Q­¦!}¨ª2­¦/§º1»B¨ · ª f}¨F2¸¦Bª¬¨¯ j¯ Ä/%¨¯ ª²¥F©¦!j¦±¯ ² · ©¯ ¥F©a¨©¦YL2¶X¦!¬+2²ª2³Í¨©¦Y²ª~² 
ª¬¨¯ L24ª ¨¯ ª²¯ ¨F2¶X¦!²2¨©¦`¯ ²¯ ¨¯ 2¬¬,¦h­º,ª²+§¯ ²Yº­F2²¥F~© t2²+5§ fº,ª²+§¦/%­¥F©±'¯ ²¸Y¨©¦`ª2­¯ ¸¯ ²+2
2¥h¨¯ ª²§­F%¨¯ ª²G 7¦!¦G	2 P±2¡2¢¢2£º±2³´ª2­^jª2­¦4§¦h¨F2¯  ¤F4¹©¯  · 2z¦/§	¯ ²	¨©¦4¥!ªj¬,¦h¨¯ ¨¯ ª²	³´ª2­z¨©¦ vaavv
§ªL2¯ ²± · ©¦h­¦¨©¦¨ · ªY¦/%­¥F©]}¨F2¸¦!¥!ªº¯ ²¦/§P¨F2¶X¦` ¦!¨¯ j¦¨©+2²]Q¬ 2¯ [
² 
Ímùú`¦/%­¥F©]¯ ²¸Yª2­¯ ¸¯ ²+2
§­F%¨¯ ª²!G¹©¦Í¨ · ª f}¨F2¸¦¥F©¦!j¦Í¯ A2¬¬ ¯ ¥/2º ¦Í¨ª`2²1»Y§ªL2¯ ²±º¨¯ ¨¦ ¦!¥h¨¯ µX¦!²¦!¯ ²Y¸¦!²¦h­F2¯ 2²Yª¬,¦!²
È1¦!}¨¯ ª²
sÂ¹©¦¬­ªº ¦!T¸¦!²¦h­F%¨ª2­Í¥/2²Pº,¦	³´ª²+§Y%¨ mv	 «Ö  « a	tll:l°		¤°	5;m°'v¤°!k×Av;`s « ~~;`tlA 	¹©¦¥!ª~² 
¨­ª  2º ¦	¬+%­F2j¦h¨¦h­%­¦Í¨©¦Í²º,¦h­ª2³'%¨¦!  ¯ ¨¦!!±¨©¦ÍL%®¯ #²º,¦h­ª2³^¯ ²}¨­j¦!²1¨¬,¦h­%¨¦!  ¯ ¨¦±¨©¦
²º,¦h­'ª2³§¯ ¦h­¦!²1¨Gªº¦h­µ%¨¯ ª²`jª§¦!!±¨©¦¨ª2¨F2²º,¦h­'ª2³,¨F%­¸¦h¨!±2²+§¨©¦²º,¦h­4ª2³,ªº¦h­µ%¨¯ ª²j¸ªX2 !

~

î >û

À

4Á

4

?G3+"/&P5:a"%$'&PRX6>?&1"%57"%576|	6&3,R2$6,o"%$'&Y0%&H%l487"%5:';B?40%6,iG87&>H/&1"%H`5:HH%$'6EQ5: Ì 57;l'0%&Pâ'q}3| O JL$'&
5:4H/"%3,4RX&Hj"%$43+"jE&10%&?G3+0%"j6,oÍ"%$'&RX6>?&1"%57"%576)?40%6,iG87&>ÎH/&1"L3+0%&KH%$'6EQ)iz~8:87&CqtiG8:3,R%g4|?65:"%H O
<`87&3+0287~,@'"%$'&+3+025:3+"%5765:)?40%6,iG87&>Î$43+02C4'&H%HL5:HLRX64H%5:C'&1023+iG87&B3,4CZ6,o	"%$'&?40%6,iG87&>aHj5:)"%$'&KRX6>nÑ
?&1"%57"%576[H/&1"aH/6>&3+0%&Z,&10%~&3,H/~[3,4CTH/6>&3+0%&Z,&10%~$43+02C@M0%&8:3+"%57,&Z"/6"%$'&H/&1"6,oY?40%6,iG87&>aH
;,&'&1023+"/&CEQ57"%$)"%$'&KH%3,>&?G3+023,>&1"/&102H O
Ì 57;l'0%&nâ'qti|LRX6>?G3+0%&HPJL9M-)3,4C }	- 6="%$'&]&X^"/&4C'&C=?40%6,iG87&>äH/&1" O }	- H/687,&HBÊ,â ÌÃ6,o
"%$45:H`H/&1"1@'EQ$45:87&YJL9M-H/687,&HLÊ^uw ÌØq}3H%l'iGH/&1"6,oÍ"%$'6H/&BH/687,&C)iz~ }	- | O Q6E&1,&10@3,HjR13,)i&BH/&1&
5:"%$'&YG;l'0%&,@z"%$'&B0%&8:3+"%57,&Y?&10%ot6,02>a3,4RX&P6,oA"%$'&P"FE6?G8:3,4'&102H5:H`3,8:H/6$457;$487~n+3+0257&C@'>]l4R2$Z>6,0%&
H/6a"%$43,Z"%$'&B0%&H%l487"%H`6"%$'&RX6>?&1"%57"%576Z?40%6,iG87&>ÎH/&1"QH%l';,;,&H/"%H O

Q ¢¢% ëtn-r#k
JL$'&[Q¢¢%ZC'6>a3,5:N>6^C'&8:HQ"%$'&n>6,&>&"%HB6,o3,5702RX023+ot"Y6N"%$'&];,0%6l44CN3+"P3,.3,570%?6,0%" O

"

y y

wIg

J $'&
L
;,63,85:HB"/6;l45:C'&a3+0%0257^5:';)3,5702RX023+ot""/6?G3+0%g^5:';Z?6H%57"%5764HP3,4C#C'&1?G3+0%"%5:';3,5702RX023+ot"B"/6=3H%l457"%3+iG87&
02l4E`3~]ot6,0M"%3+g,&16+Ä	@^3,876';B"%$'&L3,570%?6,0%"'&1"FE6,0%gn6,o"%3'57E`3~^H O JL$'&L>a3,5:RX6>?G8:5:R13+"%5765:H	"/6Bg,&1&1?
"%$'&P3,5702RX023+ot"`H%3+ot&87~aH/&1?G3+023+"/&CIM3+"`>6H/"`6'&P3,5702RX023+ot"R13,6^R1R1l'?z~3K02l4E`3~a6,0`"%3'57E`3~H/&1;>&"
3+"Q3,~"%5:>&,@43,4CZC'&1?&4C45:';]6)"%$'&H%571&B6,oÍ"%$'&3,5702RX023+ot"L3,4C)"%$'&K8:3~,6l'"j6,oÍ"%$'&3,570%?6,0%"j'&3+0%iz~
H/&1;>&"%HQ>a3~)i&BiG876^R%g,&C3,HLE&8:8 O
JL9M-ZH/687,&HB6487~wç6l'"P6,o"%$'&Ê+*)?40%6,iG87&>v5:4H/"%3,4RX&HB5:N"%$45:HPC'6>a3,5: O]Ì 6,0"%$'&5:4H/"%3,4RX&H
H/687,&Cniz~]JL9M-'@"%$'&jzl4>Ki&10	6,o'6^C'&HÍ&X^?G3,4C'&Cn5:nH/&3+02R2$a5:HÍ,&10%~nH%>a3,8:8z0%&8:3+"%57,&`"/6B"%$'&jH/68:l'"%576
C'&1?4"%$æqt"%$'6l';$[ot6,0a"%$'&Z8:3+0%;,&105:4H/"%3,4RX&H1@'6^C'&)&X^?G3,4H%576T5:H],&10%~[H%876E@0%&H%l487"%5:';.5:[3.?6z6,0
02l4"%5:>&Z6,&1023,8:8t| O JL$45:H5:>?G8:57&Hn"%$43+"aot6,0"%$'&H/&?40%6,iG87&> 5:4H/"%3,4RX&H"%$'&.m $ $'&l'025:H/"%5:R)5:Ha,&10%~
3,R1R1l'023+"/&,@43,4C"%$zl4H"%$'&1~3+0%&B5:3]H/&4H/&)Õ!&3,H/~^Ö^û4ot6,0jH%l4R2$)5:4H/"%3,4RX&H1@ }	- R13,Z'6,"i&Y&X^?&RX"/&C
"/6Zi&]i&1"/"/&10@H%5:4RX&n"%$'&H/&3+02R2$.&XÄ6,0%"57"B5:,&H/"%HB5:"/6ZRX6>?Gl'"%5:';)3)>6,0%&3,R1R1l'023+"/&$'&l'025:H/"%5:RK5:H
8:3+0%;,&87~]E`3,H/"/&C@iGl'"57"M3,8:H/65:4C45:R13+"/&H	"%$43+"3>6,0%&L3,R1R1l'023+"/&Y$'&l'025:H/"%5:R5:HM'&1&C'&Cn"/6KH/687,&Õ/$43+02C4Ö
?40%6,iG87&>Î5:4H/"%3,4RX&H O
Q6E&1,&10@ }	- H/687,&HP6487z
~ Ða?40%6,iG87&>aH1@3ZH%l'iGH/&1"P6,o"%$'6H/&aH/687,&C.iz~NJL9M-'@A3,4CN"%3+g,&Ho}3+0
>6,0%&B"%5:>&Pot6,0L&3,R2$ OÌ 57;l'0%&w1*4q}3|jH%$'6EQH`"%$'&B"%5:>& }	- H/?&4C4H`5:çÑ0%&1;,0%&H%H%576H/&3+02R2$3,4CZ5:
"%$'&Q43,8Íq}'6^Ñ0%&8:3^&C|H/&3+02R2$)ot6,0&3,R2$)6,o"%$'f
& Q¢¢%5:4H/"%3,4RX&H57"H/687,&H OMÌ 6,0`0%&1ot&10%&4RX&,@^"%$'&
H/&3+02R2$="%5:>&ot6,0YJL9M-5:HL3,8:H/65:4R18:l4C'&C O <`87&3+0287~,@4"%$'&0%&8:3^&C=H/&3+02R2$NRX64H%l4>&HQ3876,"L6,o	"%5:>&K5:
"%$45:HjC'6>a3,5:@43,4CZ6+Ä&102Hj,&10%~Z8:57"/"%87&B5:Z"%$'&BE`3~Z6,o	$'&l'025:H/"%5:RP5:>?40%6,&>&"L5:Z0%&1"%l'02 O JL$43+"L"%$'&
$'&l'025:H/"%5:RB5:>?40%6,&>&"Y5:HQH%>a3,8:8q}R1876H/&K"/6'6^Ñ&X'5:H/"/&"|L5:HL&3,H%5:87~&X^?G8:3,5:'&C@GH%5:4RX&,@G3,HY3,870%&3,C'~
6,iGH/&10%,&C@A"%$'&m $ $'&l'025:H/"%5:R]5:HB3,870%&3,C'~N,&10%~3,R1R1l'023+"/&a6"%$'&H/&a?G3+0%"%5:R1l48:3+0B?40%6,iG87&>v5:4H/"%3,4RX&H O
JL$'&zl'&H/"%576@'"%$'&@45:H`EQ$~"%$'&0%&8:3^&CH/&3+02R2$=5:HjH/6a"%5:>&RX64H%l4>a5:'; O
JL$'&)3+?4?G3+0%&"n0%&3,H/6T5:H]"%$43+"a5:D"%$45:HnC'6>a3,5:@MH/&3+02R2$T5:"%$'&çÑ0%&1;,0%&H%H%576TH/?G3,RX&Z5:Hn>6,0%&
&X^?&4H%57,&]"%$43,H/&3+02R2$5:="%$'&'6,02>a3,8Í0%&1;,0%&H%H%576.H/?G3,RX& O JL$45:HY5:HYRX6"/023+0%~N"/6Z"%$'&n3,H%H%l4>?4"%576
H/"%3+"/&CD5:D\z&RX"%576Dç O w+@A"%$43+""%$'&aRX6H/"6,o`&X^?G3,4C45:';)3H/"%3+"/&H%$'6l48:CNi&aH%>a3,8:87&10B5:."%$'&0%&8:3^&C
0%&1;,0%&H%H%576H/?G3,RX&,@+C4l'&"/6Y3LH%>a3,8:87&10i4023,4R2$45:';`o}3,RX"/6,0 O JÍ3+iG87&Lw1*4qti|C45:H/?G8:3~^HH/6>&R2$43+023,RX"/&1025:H/"%5:R1H
6,o"%$'&]'6,02>a3,8A3,4C.çÑ0%&1;,0%&H%H%576NH/?G3,RX&HQot6,f
0 Q¢¢%)5:4H/"%3,4RXf
& ¤Ç.qt"%$'&nH%>a3,8:87&H/"Q5:4H/"%3,4RX&]'6,"
H/687,&Ciz~JL9M-z| OMì 3+"%3n5:HRX68:87&RX"/&C)C4l'025:';B"%$'&QG02H/"Pqto}3,5:87&C|	57"/&1023+"%5766,o ìL
Ï 
  ìLÏ ã O \z"%3+"/&H
5:#"%$'&)'6,02>a3,80%&1;,0%&H%H%576DH/?G3,RX&ZRX6"%3,5:@	6[3,&1023+;,&,@3N8:3+0%;,&)zl4>Ki&10K6,oQH%l'i4;,63,8:H1@	EQ$45:87&a5:
"%$'&açÑ0%&1;,0%&H%H%576.H/?G3,RX&,@H/"%3+"/&HRX6,0%0%&H/?64C45:';a"/6NãP_jÑF'6^C'&HY3+0%&niz~=C'&X457"%576N8:5:>a57"/&C=5:NH%571& O
<64H/&zl'&"%87~,@'"%$'&Pi4023,4R2$45:';no}3,RX"/6,0L6,oMãP_jÑF'6^C'&Hj5:ZçÑ0%&1;,0%&H%H%576Z5:HjH%>a3,8:87&10q}H%5:4RX&P"%$'&R2$'65:RX&
6,oL&H/"%3+iG8:5:H%$'&10Kot6,0]&3,R2$TH%l'i4;,63,85:H]3=?6,"/&"%5:3,8i4023,4R2$D?65:"|2@	iGl'"K'6,"]iz~D>]l4R2$I"%$'&)>a3,~
H%l'i4;,63,8:Ha5:T"%$'&='6,02>a3,8`0%&1;,0%&H%H%576W5:"/&1023,RX"1@j0%&H%l487"%5:';5:[0%&8:3+"%57,&87~ot&1EËRX64H%5:H/"/&"R2$'65:RX&H O

~1
î

î Á(ï^ðzñ7òGó À

10000

4ï %ô :õ Yö 4ï^ð-Gó-N÷æ s4ø- ù ¦Gï^õ¶

  1   f

n

HSP*a (Rel. Search)
HSP* (Final Search)
a

1000

TP4 (Search)

6,02>a3,8

çÑF_L&1;,0%&H%H%576
ãP_
KÏ nYì

á 8>á

£9£z{ Ð

á 85F á Ø¡á 8>á

w+{ *ç

(z{ Ð+â

i4023,4R2$45:';
o}3,RX"/6,0

w+{ çÐ

w+{ *â

çz{ *

Time (sec.)

100

10

1

0.1

0.01

p01

p02

p03

q}3|

p04

p05

p10

p11

Ð*^{ £

qti|

Ì 57;l'0%&aw1*^I]q}3|JL5:>&WH/?&"D5:àçÑ0%&1;,0%&H%H%576 H/&3+02R2$3,4Cà5:x43,8=q}'6^Ñ0%&8:3^&C|NH/&3+02R2$6
Q¢¢%N5:4H/"%3,4RX&HH/687,&Ciz~ }	- O JL$'&aH/&3+02R2$#"%5:>&ot6,0KJL9M-5:HB3,8:H/6H%$'6EQot6,0
RX6>?G3+025:H/6 
O n 6,"/&L"%$'&L876,;3+0257"%$4>a5:R"%5:>&jH%R13,87&,I	H/&3+02R2$"%5:>&H	ot6,0 }	- 3,4CJL9M-3+0%&
'&3+0287~N5:C'&"%5:R13,8@EQ$45:87&]"%$'&açÑ0%&1;,0%&H%H%576H/&3+02R2$#RX64H%l4>&HBH/&1,&1023,86,02C'&102HP6,o>a3+;+Ñ
457"%l4C'&B>6,0%&B"%5:>& O qti|j<`$43+023,RX"/&1025:H/"%5:R1H`6,o	"%$'&'6,02>a3,83,4CçÑ0%&1;,0%&H%H%576H/?G3,RX&Hjot6,0
Q¢¢%Z5:4H/"%3,4RX& ¤Ç4{I á 8>á'5:HL"%$'&]3,&1023+;,&aH/"%3+"/&nH%571&,Hû á 85F*á Ø¡á 85F á'"%$'&]3,&1023+;,&023+"%5766,o
H%l4R1RX&H%H/6,0LH/"%3+"/&H%571&P"/6n"%$'&BH%571&Y6,oA"%$'&P?40%&C'&RX&H%H/6,0jH/"%3+"/& Oì 3+"%3a5:HRX68:87&RX"/&CZC4l'025:';
"%$'&BG02H/"Kqto}3,5:87&C|57"/&1023+"%576Z6,o	 ìL
Ï 
  ìLÏ ã O

Ï :8 H/6'@"%$'&Z0257;$"!ÑFH%$457ot"R1l'"02l487&,@EQ$45:R2$[&8:5:>a5:43+"/&HH/6>&Z0%&C4l44C43,"ni4023,4R2$'&H1@`5:Hl4H/&CW5:["%$'&
'6,02>a3,8P0%&1;,0%&H%H%576UH/?G3,RX&,@PiGl'"Z'6,"EQ$'&V&X^?G3,4C45:';kãP_jÑF'6^C'&HZ5:UçÑ0%&1;,0%&H%H%576 O Q6E&1,&10@
0%&1;,0%&H%H%576"/&4C4H"/6Y>a3+g,&H/"%3+"/&HPÕ!;,0%6EQÖ^@'Ýc}c:@H%l4R1RX&H%H/6,0	H/"%3+"/&HA;,&'&1023,8:87~BRX6"%3,5:K>6,0%&H%l'i4;,63,8:H
"%$43,)"%$'&570?40%&C'&RX&H%H/6,02H1@43,4CEQ$45:87&Q"%$45:H&XÄ&RX"L5:H`zl457"/&P>6^C'&1023+"/&B5:)'6,02>a3,80%&1;,0%&H%H%576@zEQ$'&10%&
H%l4R1RX&H%H/6,02H`$43,&,@'6)3,&1023+;,&,@ç Ì>6,0%&BH%l'i4;,63,8:H1@^57"5:H`>]l4R2$)>6,0%&Y?40%6'6l44RX&Cot6,0`"%$'&PH%>a3,8:87&10
H/"%3+"/&HMRX6,0%0%&H/?64C45:';Y"/6KãP_jÑF'6^C'&H5:]"%$'&LçÑ0%&1;,0%&H%H%576H/?G3,RX&,@EQ$'6H/&jH%l4R1RX&H%H/6,02HM3+0%&`63,&1023+;,&
(z{ Ð+âe}ÝtÓj8:3+0%;,&10 OMÏ HL30%&H%l487"1@'H%l4R1RX&H%H/6,02HL"/6)ãP_jÑF'6^C'&Hj3+0%&K3,8:8 K
Ï nYì ÑF'6^C'&H1@4EQ57"%$Z3,3,&1023+;,&
6,o3+i6l'J
" £z{ çH%l'i4;,63,8:HL3,4C¶Ð*^{ £H%l4R1RX&H%H/6,02H]q}H%l'iGH/&1"%Hj6,o	H%571&]ç| O
JA6WH%l4>a>a3+02571&,@Y&3,R2$U&X^?G3,4C'&CrãP_jÑF'6^C'&5:UçÑ0%&1;,0%&H%H%576U0%&H%l487"%Hqt^5:3T3,U5:"/&102>&C45:3+"/&
Õ/8:3~,&102Ö=6,o ÏKnYì ÑF'6^C'&H|B5:#3,D3,&1023+;,&)6,oJÐ9Ð{ (='&1EÎãP_jÑF'6^C'&H Op+ ,&#"%$'6l';$D>6H/"K6,o`"%$'&>
q*Ð-'{ ( Ìa|3+0%&`ot6l44C5:]"%$'&j ìLÏ ã  H/687,&Cn"%3+iG87&,@3,4Cn"%$'&10%&1ot6,0%&LC'6é "$43,&L"/6Bi&jH/&3+02R2$'&C@"%$'6H/&
"%$43+"Y0%&>a3,5:~^57&8:C3,=&XÄ&RX"%57,&.Õ2ãP_jÑ"/6+Ñ!ãP_QÖi4023,4R2$45:';ao}3,RX"/6,0P6,oLwâz{ âNq(,Êz{ £Ìä6,c
o Ð9Ð{ (|2@"/6i&
RX6>?G3+0%&C#EQ57"%$."%$'&ai4023,4R2$45:';)o}3,RX"/6,0K6,oBw+{ ç Ð)ot6,0K'6,02>a3,80%&1;,0%&H%H%576 OaÏ ;3,5:@A"%$'&a?40%6,iG87&>Ø5:H
'6,"j"%$'&$457;$)i4023,4R2$45:';]o}3,RX"/6,0Y5:Z57"%H/&87ohI57"L5:H`"%$43+"j"%$'&i4023,4R2$45:';]o}3,RX"/6,0Y5:)"%$'&B0%&8:3^&CZH/&3+02R2$
H/?G3,RX&Y5:HMo}3+0QÝ7Å+'Xd"%$43,57"5:HMot6,0`'6,02>a3,840%&1;,0%&H%H%576@z3,4Ca"%$43+"H/&3+02R2$)5:"%$'&PçÑ0%&1;,0%&H%H%576H/?G3,RX&
5:HMRX64H/&zl'&"%87~n>6,0%&L&X^?&4H%57,&`"%$43,aH/&3+02R2$5:n"%$'&L'6,02>a3,8'0%&1;,0%&H%H%576aH/?G3,RX&,@023+"%$'&10M"%$43,a87&H%H O

~~
î

À

Type I
Type II
Type III

100

a

100

1000

10

1
1

10

100

1000

TP4: Time (seconds)

q}3|

10000

1000

100

a

1000

10000

HSP* : Time (seconds)

HSP* : Time (seconds)

10000

a

HSP* : Time (seconds)

10000

4Á

4

10

1
1

10

100

1000

TP4: Time (seconds)

qti|

10000

10

1
1

10

100

1000

TP4: Time (seconds)

10000

q}R|

Ì 57;l'0%&aw,w+IP\z68:l'"%576."%5:>&HPot6,0KJL9M-3,4C."%$'0%&1&aC45´Ä&10%&"RX6^G;l'023+"%5764HP6,o }	- 6?40%6,iG87&>aH
H/687,&C5:"%$'{
& 3SaC'6>a3,5:IKq}3| }	- EQ57"%$=s)Ñ0%&1;,0%&H%H%576.8:5:>a57"/&C"/6)sÐu ç6487~û
qti|#Õ/l448:5:>a57"/&C4Ö }	- qt?&10%ot6,02>aHas)Ñ0%&1;,0%&H%H%5764Hot6,0)5:4RX0%&3,H%5:';#s l4"%5:8L&57"%$'&10)3
'6^Ñ0%&8:3^&C.H/68:l'"%576N5:HYot6l44C@6,0B"%$'&n&H/"%5:>a3+"/&C.RX6H/"B6,o"%$'&]"/6,?#87&1,&8Í;,63,8:HBC'6z&H
'6,"5:4RX0%&3,H/&|2û`q}R|Õ%çÜ-Ö }	- q}3,87E`3~^HB?&10%ot6,02>aHBçÑQ3,4CN-+Ñ0%&1;,0%&H%H%576| O JL$'&8:5:'&H
"/6"%$'&K0257;$"Y3,4C"/6,?.5:ZG;l'0%&H]qti|j3,4C[q}R|L5:4C45:R13+"/&"%$'&K"%5:>&XÑ6l'"Y8:5:>a57" O Õ/J~z?&K
ÜN%%!Ö0%&1ot&102HP"/6"%$'&aR18:3,H%H%5´R13+"%576N6,o"%$'&?40%6,iG87&>v5:4H/"%3,4RX&HBC'&H%RX0257i&C.5:#\z&RX"%576
- O Ê=qt?G3+;,&](,Ê Ð,| O

"
y

wIg

3S ëtn-r#k

JL$'f
& 3SC'6>a3,5:=>6^C'&8:HL"%$'&[ñÒNJY\R13,8:8AH/&1"!ÑFl'?.?40%6^RX&C4l'0%&ot6,0PC43+"%33+?4?G8:5:R13+"%5764HL5:=>6,iG5:87&
"/&87&1?G$'6'&H O JL$'&)C'6>a3,5:[5:Hn3,RX"%l43,8:87~D3.H%R2$'&C4l48:5:';=?40%6,iG87&>Z@H%5:>a5:8:3+0K"/6?G6EQH%$'6,? O JL$'&ZR13,8:8
H/&1"!ÑFl'??40%6^RX&C4l'0%&QRX64H%5:H/"%HM5:a&57;$"C45:H%RX0%&1"/&QH/"/&1?GHot6,0&3,R2$)3+?4?G8:5:R13+"%576@,6,02C'&10%&Caiz~n?40%&RX&C'&4RX&
RX64H/"/023,5:"%H O JL$'&KC4l'023+"%576)6,o3H/"/&1?NC'&1?&4C4H`6"%$'&"F~z?&6,oH/"/&1?N3,HLE&8:83,HL"%$'&K3+?4?G8:5:R13+"%576 O
áS$'&#H/&1,&1023,8M3+?4?G8:5:R13+"%5764HB3+0%&i&5:';ZH/&1"Kl'?@H/"/&1?GHB?&10%"%3,5:45:';"/6=C45´Ä&10%&"B3+?4?G8:5:R13+"%5764HBR13,
i&L&X^&R1l'"/&C5:?G3+023,8:87&8@H%l'i^í/&RX"M"/6K0%&H/6l'02RX&Q3+3,5:8:3+iG5:8:57"F~IA"%$'&10%&Y3+0%&KwÊzl4>&1025:R`0%&H/6l'02RX&H1@^3,4C
&3,R2$DH/"/&1?#l4H/&H3ZRX&10%"%3,5:3,>6l4"1@EQ$45:R2$C'&1?&4C4HY6."%$'&a3+?4?G8:5:R13+"%576@6,o`H/6>&aH%l'iGH/&1"P6,o0%&XÑ
H/6l'02RX&HMC4l'025:';Q&X^&R1l'"%576=qt6487~nçP6,oG"%$'&wÊP0%&H/6l'02RX&HM3+0%&L3,RX"%l43,8:87~K6,&102H%l'iGH%RX0257i&C| O _L&H/6l'02RX&H
3+0%&=Õ!0%&l4H%3+iG87&Ö^@Ýc}c:@'"%$'&K3,>6l4"Ql4H/&C)i&RX6>&HQ3+3,5:8:3+iG87&3+;3,5:Z64RX&"%$'&KH/"/&1?=$43,H`45:H%$'&C O
Ï "LG02H/"L;8:3,4RX&,@G"%$45:HL3+?4?&3+02HL"/6i&3a?&10%ot&RX"QC'6>a3,5:Zot6,0 }	- IJL$'&?40%&H/&4RX&K6,o	0%&l4H%3+iG87&
0%&H/6l'02RX&#8:5:>a57"%3+"%5764H>a3+g,&H57"Z>6,0%&8:57g,&87~W"%$43+"Z"%$'&10%&3+0%&#$457;$'&10/Ñ6,02C'&10Z>]l'"%l43,8P&X'R18:l4H%5764H
i&1"FE&1&T3,RX"%5764HqhÝc}c:@M"%$'&10%&Z>a3~i&&'6l';$6,oY3N0%&H/6l'02RX&)"/6#R13+0%0%~#6l'"n"FE6#3,RX"%5764Hl4H%5:';
"%$'&0%&H/6l'02RX&RX64R1l'0%0%&"%87~,@iGl'"a'6,"a"%$'0%&1&,@6,0a"%$'0%&1&iGl'"a'6,"ot6l'0@1e| O JL$45:HH%l';,;,&H/"%Ha"%$'&
m'p $'&l'025:H/"%5:R1HL3+0%&]>6,0%&n8:57g,&87~"/6)5:>?40%6,&KEQ57"%$=5:4RX0%&3,H%5:';s@H%5:4RX&]m'p RX64H%5:C'&102HQ3+"P>6H/"Qs
H%l'i4;,63,8:Hj3,4CZ"%$'&10%&1ot6,0%&3+"Q>6H/"LsØRX64R1l'0%0%&"Q3,RX"%5764H OÏ "L"%$'&H%3,>&B"%5:>&,@4C4l'&B"/6a"%$'&H%5:>?G87&
H/"/02l4RX"%l'0%&j6,o?40%&RX&C'&4RX&YRX64H/"/023,5:"%H1@"%$'&Õ!;,0%6EQ5:';ÖH/"%3+"/&H3,4Ca"%$'&L0%&H%l487"%5:';Pi4023,4R2$45:';Po}3,RX"/6,0
iG876EjÑFl'?5:)0%&8:3^&CH/&3+02R2$"%$43+"L6^R1R1l'0%0%&C5:)"%$'{
& Q¢¢%C'6>a3,5:@'3+0%&Kl448:57g,&87~ O
_L&H%l487"%H1@$'6E&1,&10@C45:H%3+;,0%&1&,IQJL9M-)3,4C }	- H/687,&]"%$'&nH%3,>&nH/&1"P6,oM?40%6,iG87&>ä5:4H/"%3,4RX&Hqç,â
6l'"]6,oPÊ+*|2@5:D"%5:>&Hn3,HnH%$'6EQ5: Ì 57;l'0%&.w,wq}3| O JL9M-.5:HKo}3,H/"/&10"%$43, }	- 5:3.>a3í/6,0257"F~#6,o
R13,H/&H1@G"%$'6l';$"%$'&]C45´Ä&10%&4RX&5:Hj0%&8:3+"%57,&87~ZH%>a3,8:8@'EQ$45:87&B5:Z"%$'&KR13,H/&HQEQ$'&10%& }	- 5:HL"%$'&o}3,H/"/&H/"
6,oÍ"%$'&B"FE6'@4"%$'&C45´Ä&10%&4RX&B5:H;,0%&3+"/&10 O JA6aH/&1&BEQ$~,@'"%$'&B?40%6,iG87&>5:4H/"%3,4RX&HjR13,Zi&BC457^5:C'&C5:"/6
"%$'&ot68:876EQ5:';]"%$'0%&1&"F~z?&H1I

~5}
î

î Á(ï^ðzñ7òGó À

4ï %ô :õ Yö 4ï^ð-Gó-N÷æ s4ø- ù ¦Gï^õ¶

  1   f

IT!4H/"%3,4RX&H5:]EQ$45:R2$3,8:8^RX6"/&H/"/&Ca0%&H/6l'02RX&HM3+0%&j3+3,5:8:3+iG87&j5:nH%l>=R157&"zl43,"%57"F~ O JL$45:H
wYpQbn
> &3,4Hj"%$'&10%&5:Hj'60%&H/6l'02RX&RX6>5:RX"j3+"Q3,8:8@'3,4CZ"%$zl4H`"%$43+"Ym  uUm O JL$'&10%&B3+0%&w5£n5:4H/"%3,4RX&Hj6,o

"%$45:H`"F~z?&K3,>6';a"%$'6H/&KH/687,&C@43,4CZ"%$'&1~)3+0%&K5:4C45:R13+"/&C)iz~[ÕÙÖ5: Ì 57;l'0%&aw,w O

u m $ iGl'"m  ; m $ @MÝc}c:@"%$'&10%&3+0%&0%&H/6l'02RX&aRX6>5:RX"%H1@iGl'"

"%$'&H/&N5:,687,&N>6,0%&="%$43,k"%$'0%&1&NRX64R1l'0%0%&"3,RX"%5764H3,4Ck3+0%&N"%$'&10%&1ot6,0%&N'6,"C'&1"/&RX"/&CViz~km . O
JL$'&10%&]3+0%&)w1*)5:4H/"%3,4RX&HQ6,oM"%$45:HL"F~z?&n3,>6';"%$'6H/&H/687,&C@3,4C="%$'&1~=3+0%&n5:4C45:R13+"/&Ciz~kÕ ÛÖ5:
Ì 57;l'0%&aw,w O
. ;þm $
wYpQb8AA Iþ!4H/"%3,4RX&HB5:=EQ$45:R2$.m
O JL$'&10%&n3+0%&Zw,wn5:4H/"%3,4RX&HY6,oM"%$45:HY"F~z?&n3,>6';)"%$'6H/&
H/687,&C@43,4CZ"%$'&1~Z3+0%&K5:4C45:R13+"/&C)iz~[;Õ ÜÖa5: Ì 57;l'0%&aw,w O
ãPB"F~z?&5:4H/"%3,4RX&H1@ }	- R187&3+0287~Q?G3~^HA3,B6,&102$'&3,CKot6,0ARX6>?Gl'"%5:';L3,l44'&RX&H%H%3+025:87~YH/"/0%6';
$'&l'025:H/"%5:R+@^"%$'6l';$Z57"L5:H`0%&8:3+"%57,&87~)H%>a3,8:8 On 6,"/&K"%$43+"j"%$'&H/&K3,R1RX6l4"Qot6,0Q3n"%$45702C)6,oÍ"%$'&5:4H/"%3,4RX&H
5:D"%$'&?40%6,iG87&>ÐH/&1"Zq!5w £=6l'"n6,oYÊ+*|2@3,4C'&3+0287~#$43,87oj6,oL"%$'&)H/687,&C[5:4H/"%3,4RX&H O ãP5:4H/"%3,4RX&H
6,o`"F~z?&HB%Y3,4C%%@ }	- &X^?G3,4C4HBot&1E&10K'6^C'&H5:."%$'&n43,8Lq}'6^Ñ0%&8:3^&C|PH/&3+02R2$#"%$43,#JL9MC'6z&HnC4l'025:';57"%H]H/&3+02R2$@3,Hn>]l4R2$[3,Hç¤- ÌÐot&1E&10n6[3,&1023+;,& O JL$45:HKH%$'6EQHK"%$43+"n"%$'&)$'&l'025:H/"%5:R
5:>?40%6,&>&"`0%&H%l487"%5:';ot0%6>ÎçÑ0%&1;,0%&H%H%576)5:H`3+"j87&3,H/"6,oÍH/6>&P+3,8:l'&,@z"%$'6l';$)5:0%6l';$487~ . 02C6,o
"%$'&K5:4H/"%3,4RX&Hj'6,"L&'6l';$Z"/6RX6>?&4H%3+"/&ot6,0L"%$'&KRX6H/"L6,o	?&10%ot6,02>a5:';]"%$'&0%&8:3^&CZH/&3+02R2$ O
_L&R13,8:8"%$43+""%$'& }	- ?G8:3,4'&105:a"%$'&BRX6>?&1"%57"%576@^3,4C5:"%$'&Y&X^?&1025:>&"%H?40%&H/&"/&C)$'&10%&
H/6.o}3+0@ME`3,Hn0%&H/"/025:RX"/&C"/6?&10%ot6,02>a5:';6487~çÑ0%&1;,0%&H%H%576TH/&3+02R2$ ODÏ ?6H%H%57iG87&a&X^?G8:3,43+"%576ot6,0
"%$'&0%&8:3+"%57,&87~.E&3+g#0%&H%l487"%H"%$'&?G8:3,4'&10?40%6^C4l4RX&H6#"F~z?&%B3,4CD%%P5:4H/"%3,4RX&H]5:H"%$43+"K"%$45:H
0%&H/"/025:RX"%576#?40%&1,&"%HK0%&8:3^&CH/&3+02R2$ot0%6>Âi&5:';o}l48:87~=&X^?G87657"/&C O Q6E&1,&10@M"%$45:H"%$'&16,0%~#C'6z&H
'6,"M$'68:C OÍÌ 57;l'0%&HLw,wqti|Í3,4CZw,wq}R|H%$'6EW0%&H%l487"%HÍot6,0M"FE6K3,87"/&10243+"%57,&LRX6^G;l'023+"%5764H	6,o }	- I	5:
w,wqti|Q3,VÕ/l448:5:>a57"/&C4ÖRX6^G;l'023+"%576@EQ$45:R2$=R13+0%0257&HQ6l'"Ys)Ñ0%&1;,0%&H%H%576NH/&3+02R2$'&HYot6,0PsÐuþçzy1{1{1{
l4"%5:8&57"%$'&103P'6^Ñ0%&8:3^&CnH/68:l'"%576]5:HÍot6l44C@+6,0	"%$'&`6,?4"%5:>a3,8zs)ÑFH/68:l'"%576]RX6H/"5:HÍot6l44CK"/6Bi&"%$'&
H%3,>&j3,H	"%$'&Bqt­
s Gw|ÑFH/68:l'"%576RX6H/"1@3,4Cn5:w,wq}R|Í3)Õ%çÜ-ÖRX6^G;l'023+"%576@+EQ$45:R2$n3,87E`3~^H	?&10%ot6,02>aH
çÑ`3,4CZ-+Ñ0%&1;,0%&H%H%576=H/&3+02R2$'&H O`Ï HQR13,i&KH/&1&=5:)"%$'&G;l'0%&H1@'i6,"%$3,87"/&10243+"%57,&]RX6^G;l'023+"%5764H
5:4R1l'03D8:3+0%;,&106,&102$'&3,CVot6,0"%$'&=0%&8:3^&CVH/&3+02R2$'&H.q}l448:5:>a57"/&C }	- &1,&k"%5:>&Ha6l'"6k"FE6
5:4H/"%3,4RX&HEQ$45:87&C'65:';[ÊÑ0%&1;,0%&H%H%576VH/&3+02R2$| OxÏ 8:H/6'@j"%$'&=;3,5:kot0%6>×"%$'&.3,C4C457"%57643,8L$'&l'025:H/"%5:R
5:'ot6,02>a3+"%576T5:Hzl457"/&ZH%>a3,8:8IZRX6>?G3+025:';#3+;3,5:T"%$'&=zl4>Ki&106,oP'6^C'&Hn&X^?G3,4C'&CW5:["%$'&)43,8
q}'6^Ñ0%&8:3^&C|P0%&1;,0%&H%H%576H/&3+02R2$"/6N"%$'&)zl4>Ki&10K6,oL'6^C'&HK&X^?G3,4C'&C#iz~#JL9M-'@"%$'&l448:5:>a57"/&C
3,4CkçÜ-[RX6^G;l'023+"%5764H&X^?G3,4CT6k3,&1023+;,&.-4w+{ ¤- Ì 3,4CW-4uw Ìúot&1E&10'6^C'&H1@0%&H/?&RX"%57,&87~æqt"/6
i&KRX6>?G3+0%&C="/6"%$'&nç¤- Ìä3,&1023+;,&aH%3^5:';5:&X^?G3,4C'&C='6^C'&HL6,i4"%3,5:'&C=iz~ }	- 0%&H/"/025:RX"/&C"/6
çÑ0%&1;,0%&H%H%576H/&3+02R2$6487~4| O
Ï '6,"%$'&10Q?6H%H%57iG87&P&X^?G8:3,43+"%5765:Hj"%$43+"Q"%$'&K"/023,4H/?6H%57"%576)"%3+iG87&,@4EQ$45:R2$3,8:H/6H/"/6,0%&HYl'?C43+"/&H
6,oB&H/"%5:>a3+"/&CVRX6H/"1@j"%$'6l';$kot6,0EQ$'687&=H/"%3+"/&H023+"%$'&10"%$43,kH%l'iGH/&1"%Ha6,oB;,63,8:H1@j"/6[H/6>&=&X^"/&"
RX6>?&4H%3+"/&HQot6,0Q"%$'&E&3+g,&10P$'&l'025:H/"%5:RBl4H/&CZiz~ZJL9M- O`Ï ;3,5:@$'6E&1,&10@"%$'&&X^?G8:3,43+"%576Z"%l'024H
6l'"'6,""/6[$'68:CINEQ57"%$W"%$'&="/023,4H/?6H%57"%576T"%3+iG87&=C45:H%3+iG87&CW5:Wi6,"%$W?G8:3,4'&102H1@"%$'&NH%3^5:';H5:
zl4>Ki&106,oj'6^C'&HB&X^?G3,4C'&C#5:."%$'&43,8MH/&3+02R2$#iz~ }	- RX6>?G3+0%&C#"/6"%$'&zl4>Ki&106,oj'6^C'&H
&X^?G3,4C'&CDiz~JL9M-N6"F~z?&%K3,4CD%%5:4H/"%3,4RX&Hn5:H]3,RX"%l43,8:87~D87&H%HK"%$43,EQ$'&4CD"/023,4H/?6H%57"%576
"%3+iG87&HB3+0%&l4H/&C@3,&1023+;5:';Z6487~D5w £Ìèqt"%$'6l';$ }	- 5:N"%$45:HY&X^?&1025:>&"YH/687,&HP"FE6?40%6,iG87&>aH
"%$43+"QJL9M-o}3,5:8:H`"/6H/687,&|2@3,4C)"%$'&KC45´Ä&10%&4RX&3,8:H/6i&RX6>&HQ>]l4R2$>6,0%&+3+0257&C O
_L&R13,8:8"%$43+"Y3azl4>Ki&10j6,oH%5:>?G8:5´R13+"%5764HE&10%&K5:"/0%6^C4l4RX&CZ5:Z"%$'&Bot6,02>]l48:3+"%576Z6,o	"/&>?6,023,8
s)Ñ0%&1;,0%&H%H%576@B5:V6,02C'&10"/6W&43+iG87&RX6>?G87&1"/&DH/68:l'"%5764H)"/6W"%$'&0%&8:3^&CæRX6H/"&zl43+"%576U"/6Wi&
RX6>?Gl'"/&CZ3,4C)H/"/6,0%&CZ5:"%$'&B$'&l'025:H/"%5:RL"%3+iG87& O JL$zl4H1@^3]0%&>a3,5:45:';K?6H%H%57iG87&L&X^?G8:3,43+"%576ot6,0j"%$'&

ÚA Ix!4H/"%3,4RX&H5:NEQ$45:R2$m

wYpQb

.

~«
î

À

4Á

4

H%>a3,8:8+3,8:l'&]6,oM"%$'&n5:>?40%6,&>&"Y6,oM"%$'&n$'&l'025:H/"%5:R5:="%$'&K43,8AH/&3+02R2$5:HQ"%$'&H/&nH%5:>?G8:5´R13+"%5764H1@
H%5:4RX&B"%$'&1~)876E&10Q"%$'&B&H/"%5:>a3+"/&HQH/"/6,0%&C=5:)"%$'&K$'&l'025:H/"%5:RQ"%3+iG87& O
<64RX&10245:';`"%$'&M"%5:>&6,&102$'&3,Cot6,00%&8:3^&CH/&3+02R2$@+5:P?G3+0%"%5:R1l48:3+0ot6,0"%$'&M$457;$'&10s)Ñ0%&1;,0%&H%H%576
H/&3+02R2$'&H1@"%$'&&X^?G8:3,43+"%576]3+?4?&3+02HÍ3+;3,5:n"/6Yi&`3Y$457;$'&10Íi4023,4R2$45:';Qo}3,RX"/6,0M5:"%$'&`0%&8:3^&C]H/&3+02R2$
H/?G3,RX&,@"%$'6l';$="%$'&H%57"%l43+"%576N5:HYH/6>&1EQ$43+"PC45´Ä&10%&"Y"%$43,.5:"%$'
& Q¢¢%ZC'6>a3,5: O JL$'&]023+"%576
i&1"FE&1&Z"%$'&H%571&B6,o	H%l4R1RX&H%H/6,0QH/"%3+"/&HQ3,4CZ"%$'&570`?40%&C'&RX&H%H/6,02HL5:Hj876E@43,&1023+;5:';*^{ â Ðn5:Z'6,02>a3,8
0%&1;,0%&H%H%576#3,4C#*^{ â^waot6,0ãP_jÑF'6^C'&H5:#çÑ0%&1;,0%&H%H%576Vq}3,4CDH/"%3~^H0%6l';$487~N"%$'&H%3,>&3,8:H/65:"%$'&
-+Ñ3,4CÊÑ0%&1;,0%&H%H%576H/?G3,RX&H|`H/6 K
Ï nYì ÑF'6^C'&HQ3+0%&B0%&8:3+"%57,&87~)H%R13+02RX& O Æ`l'"j"%$'&K3,&1023+;,&Ki4023,4R2$45:';
o}3,RX"/6,0ot6,0`ãP_jÑF'6^C'&HM5:açÑ0%&1;,0%&H%H%576a5:H(z{ -q}5:4RX0%&3,H%5:';P"/6-'{ 9â £3,4Co£z{ ( ÐB5:n-+ÑÍ3,4CÊÑ0%&1;,0%&H%H%576@
0%&H/?&RX"%57,&87~4|RX6>?G3+0%&C"/63,3,&1023+;,&]6,ojw+{ £,Êa5:Z'6,02>a3,80%&1;,0%&H%H%576 O JL$'&B0%&3,H/65:)"%$'
& 3S
C'6>a3,5:k5:Ha"%$'&N0257;$"!ÑFH%$457ot"R1l'"%H1I.0%&R13,8:8Q"%$43+""%$'&H/&=&8:5:>a5:43+"/&0%&C4l44C43,"ai4023,4R2$'&Hot0%6>×"%$'&
H/&3+02R2$UH/?G3,RX&,@L"%$zl4H0%&C4l4R15:';#"%$'&Ni4023,4R2$45:';Do}3,RX"/6,0@QiGl'"R13,S'6,"i&.l4H/&CkEQ$'&k0%&1;,0%&H%H%5:';
ãP_jÑF'6^C'&Hj5:0%&8:3^&CZH/&3+02R2$=H%5:4RX&P"%$45:Hj>a57;$"jR13,l4H/&B"%$'&RX6>?Gl'"/&C$'&l'025:H/"%5:RL"/6ai&RX6>&5:43,C^Ñ
>a5:H%H%57iG87& O JL$'&Qi4023,4R2$45:';Ko}3,RX"/6,0jot6,0j'6,02>a3,8G0%&1;,0%&H%H%576H/&3+02R2$)EQ57"%$'6l'"0257;$"!ÑFH%$457ot"R1l'"%H`5:H`(z{ ý,â O
JL$'&C45´Ä&10%&4RX&>a3~ZH/&1&>äH%>a3,8:8@^iGl'"L57"L$43,HL3;,0%&3+"Q&XÄ&RX"1IJL9M-EQ57"%$'6l'"j0257;$"!ÑFH%$457ot"jR1l'"%Hjo}3,5:8:H
"/6]H/687,&Q3,8:84iGl'""FE6]"F~z?&L%	3,4Ca%%Í5:4H/"%3,4RX&HYq}5:a>a3,~aR13,H/&H'6,"M&1,&a45:H%$45:';Y"%$'&jG02H/" ìPÌ \
57"/&1023+"%576| O

"!>Oilkgn-opQqsrqun-kg|éItk]Îo#gqsrtkgq
y

!]H/&1,&1023,86,o4"%$'&`RX6>?&1"%57"%576KC'6>a3,5:4H1@ }	- C'6z&HÍ3,R2$457&1,&`i&1"/"/&10Í0%&H%l487"%HA"%$43,]JL9M-'@,5:4C45:R13+"%5:';
"%$43+"M0%&8:3^&CaH/&3+02R2$R13,ai&L3,&	=R157&">&1"%$'6^C6,oRX6>?Gl'"%5:';3>6,0%&L3,R1R1l'023+"/&Y$'&l'025:H/"%5:REQ$45:87&
H/"%3~^5:';k5:S"%$'&Dm'pÐot023,>&1E6,0%g O !U"%$'&#C'6>a3,5:4H)EQ$'&10%&#57"Zo}3,5:8:HÜk"%$'·
& Q¢¢%k3,4C¬3S
C'6>a3,5:4HÜ57"LC'6z&HLH/6i&R13,l4H/&B0%&8:3^&CH/&3+02R2$~^57&8:C4H`30%&8:3+"%57,&87~H%>a3,8:85:>?40%6,&>&"j6,&10Y"%$'&
m $ $'&l'025:H/"%5:R+@'3+"Q3aC45:H/?40%6,?6,0%"%57643+"/&87~a8:3+0%;,&KRX6>?Gl'"%3+"%57643,8RX6H/" O
!"%$'
& Q¢¢%aC'6>a3,5:@z"%$'&P"FE6?40%6,iG87&>aH3+0%&P"%57;$"%87~RX64'&RX"/&CIM"%$'&B$'&l'025:H/"%5:RQ5:>?40%6,&XÑ
>&"Z5:H'&1;8:57;57iG87&=H%5:>?G87~i&R13,l4H/&.0%&8:3^&CVH/&3+02R2$U5:HH/6[&X^?&4H%57,&="%$43+")"%$'&N6487~W?40%6,iG87&>
5:4H/"%3,4RX&H6EQ$45:R2$57"M45:H%$'&HEQ57"%$45:"%$'&Q"%5:>&Y8:5:>a57"3+0%&Q"%$'6H/&Y,&10%~H%5:>?G87&L5:4H/"%3,4RX&Qot6,0EQ$45:R2$
"%$'&)m $ $'&l'025:H/"%5:R5:HK3,870%&3,C'~R1876H/&"/6=?&10%ot&RX" O !"%$'B
& 3SZC'6>a3,5:@A"%$'&0%&3,H/6Dot6,0K"%$'&?6z6,0
$'&l'025:H/"%5:RQ5:>?40%6,&>&"L5:H`H/"%5:8:8H/6>&1EQ$43+"`6,oÍ3>K~^H/"/&10%~IM3zl4>Ki&106,o	$~z?6,"%$'&H/&HE&10%&B"/&H/"/&C@
3,4C0%&1o}l'"/&C ODÏ 0%&>a3,5:45:';=?G8:3,l4H%57iG87&&X^?G8:3,43+"%576[5:H]"%$43+""%$'&ZH%5:>?G8:5´R13+"%5764H5:"/0%6^C4l4RX&C5:
ot6,02>]l48:3+"%576)6,o	"/&>?6,023,8s)Ñ0%&1;,0%&H%H%5763+0%&?G3+0%"%5:R1l48:3+0287~C43,>a3+;5:';a5:)"%$45:HjC'6>a3,5: O
!Zi6,"%$C'6>a3,5:4H1@'$'6E&1,&10@G"%$'&&X^?G8:3,43+"%576Zot6,0L"%$'&0%&8:3+"%57,&87~)8:3+0%;,&B6,&102$'&3,C=ot6,0L0%&8:3^&C
H/&3+02R2$]3+?4?&3+02H"/6Qi&"%$43+"Í57"AH%l^Ä&102Hot0%6>r3L$457;$'&10i4023,4R2$45:';jo}3,RX"/6,0	"%$43,"%$'&'6,02>a3,80%&1;,0%&H%H%576
H/&3+02R2$@	EQ$45:R2$DR13,l4H/&HK&X^?G3,4H%5766,oYãP_jÑF'6^C'&HK5:"%$'&0%&8:3^&CDH/&3+02R2$"/6=i&RX6>?Gl'"%3+"%57643,8:87~
>6,0%&n&X^?&4H%57,&K"%$43,'6^C'&]&X^?G3,4H%576N5:="%$'&'6,02>a3,8	H/&3+02R2$Wqt&1,&57o>a3,~6,o"%$'&n;,&'&1023+"/&C
H%l4R1RX&H%H/6,02H3+0%&Q'6,"H/&3+02R2$'&C| OMÌ 57;l'0%&w(BH%l4>a>a3+02571&HMH/6>&QH/&3+02R2$H/?G3,RX&QR2$43+023,RX"/&1025:H/"%5:R1Hot6,03,8:8
"%$'&YRX6>?&1"%57"%576C'6>a3,5:4H O !"%$'&YC'6>a3,5:4HMEQ$'&10%&Q0%&8:3^&CH/&3+02R2$)5:HM&X^?&4H%57,&,@z"%$45:H5:HMi&R13,l4H/&
H/"%3+"/&Ha"/&4C["/6;,0%6EÃEQ$'&[0%&1;,0%&H%H/&C@QÝc}c:
@ á 85F á Ø¡á 8>áÍ5:Hn8:3+0%;,&3,4CT3,Ha3.0%&H%l487"]"%$'&10%&3+0%&>a3,~
F
Ñ
'6
^
'
C

&
	
H
Q
E
7
5
%
"
]
$
a
>
,
3
~
]
%
H
4
l
1
R
X
R

&
%
H
/
H
,
6
2
0
L
H
q

Q




¢




%
¢
KÏH/?GnY3,RXì &P$43,&B3K$457;$'&10i4023,4R2$45:';o}3,RX"/6,0`"%$43,54:a"|2@+%6,$'0	&Pi'6&,R1023,>al4H/3,&Q84ãP0%&1_j;,0%ÑF&'6H%^H%C'576&HHÍ5:/]"?G3,%$'RX&,&@'0%3,&4C8:3^&3+Cn0%&Q0%&1"%$';,0%&1&0%H%&1H%ot6,5760%&
RX6>?Gl'"%3+"%57643,8:87~.>6,0%&&X^?&4H%57,&a"/6=&X^?G3,4CVTq 3S| O !#"%$'&C'6>a3,5:4HBEQ$'&10%&a0%&8:3^&CDH/&3+02R2$
5:HYH%l4R1RX&H%H/o}l48@G6N"%$'&]6,"%$'&10B$43,4CH
@ á 85F*á Ø¡á 8>áG5:HL"F~z?G5:R13,8:87~R1876H/&]"/6w+@	Ýc}c:@H%>a3,8:8AH/"%3+"/&HH/"%3~=H%>a3,8:8
EQ$'&n0%&1;,0%&H%H/&C@3,4Cn0%&1;,0%&H%H%576n6,oãP_jÑF'6^C'&H5:]"%$'&j0%&8:3^&CH/?G3,RX&Q5:H	RX6>?Gl'"%3+"%57643,8:87~]R2$'&3+?&10
"%$43,'6^C'&a&X^?G3,4H%576D5:"%$'&'6,02>a3,8M0%&1;,0%&H%H%576H/?G3,RX&,@3,H]5:4C45:R13+"/&C#iz~3N876E&10Zqt6,0]0%6l';$487~

~5Ê
î

î Á(ï^ðzñ7òGó À

4ï %ô :õ Yö 4ï^ð-Gó-N÷æ s4ø- ù ¦Gï^õ¶

  1   f

n
á 8>á
á 85F*á Ø¡á 8>á

i4023,4R2$45:';no}3,RX"/6,0

á 8>á
á 85F*á Ø¡á 8>á

6,02>a3,8_L&1;,0%&H%H%576

Q¢¢%
£9z£ { Ð
w+{ *ç
w+{ çÐ
S~¾u¿¢
ýz{ +Ð ý

i4023,4R2$45:';no}3,RX"/6,0

á 8>á
á 85F*á Ø¡á 8>á
á 8>á
á 85F*á Ø¡á 8>á

i4023,4R2$45:';no}3,RX"/6,0

á 8>á
á 85F*á Ø¡á 8>á

i4023,4R2$45:';no}3,RX"/6,0

á 8>á
á 85F*á Ø¡á 8>á

zç { *
(z{ Ð+â
w+{ *â

Ð*^{ £

w+{ (^w
wÊz{:w

(z{ â,â
w+{ ç,Ê
Êz{:wç

Êz{ (

wX-'{ â
w+{:~w Ð
(^w+{ (

(z{ â,â
(z{:~w Ð
çz{ ç+*

ç+*^{ ý

âz{ *Ê
w+{ *£
(-'{ Ê

(z{ â,â
w+{ -(
wÊz{:w

Ð{ Ð+Ê

(z{ â,â
w+{ *ý
Êz{ *'w

Ð{ ç,ç

(z{ Ê,(
*^{ â^w
(z{ -*

ýz{ ç,ý

¡¢QÈ¾h3qT¤ËS==,¤Ë¾¢S|

i4023,4R2$45:';no}3,RX"/6,0

i4023,4R2$45:';no}3,RX"/6,0

çÑF_L&1;,0%&H%H%576
ãP_
KÏ nYì

¢

9Q¾h¤hQ¾
ýz{ £9£
w+{ *+ýz{ â9£
3S
£z{:w1*
*^{ âÐ
w+{ ,£ Ê

Ì 57;l'0%&aw(zIP\z6>&nR2$43+023,RX"/&1025:H/"%5:R1HY6,oM"%$'&]'6,02>a3,8A0%&1;,0%&H%H%576N3,4C.çÑ0%&1;,0%&H%H%576NH/&3+02R2$.H/?G3,RX&HP5:
"%$'&C'6>a3,5:4HRX64H%5:C'&10%&CI"%$'&3,&1023+;,&`H/"%3+"/&`H%571&Yq á 8>á |2@"%$'&3,&1023+;,&023+"%576L6,o4H%l4R1RX&H%H/6,0
H/"%3+"/&KH%571&P"/6"%$'&H%571&P6,oA"%$'&B?40%&C'&RX&H%H/6,0LH/"%3+"/&q á 85F*á Ø¡á 8>á |`3,4C"%$'&Bi4023,4R2$45:';]o}3,RX"/6,0 O
& S~¾u¿¢G5@ ¡¢QÈ¾h34~@ ¢]3,4Cp9Q¾h¤hQ¾PC'6>a3,5:4H1@,"%$'&jzl4>Ki&102HH%$'6EQ
Ì 6,0"%$'c
3+0%&"%$'&3,&1023+;,&HÍ6,&10	H/687,&CB?40%6,iG87&>æ5:4H/"%3,4RX&H OÍÌ 6,0A"%$'
& 3S`C'6>a3,5:@"%$'&3,&1023+;,&
5:HL6,&10BH/687,&C"F~z?&K%`3,4C%%`5:4H/"%3,4RX&HL6487~#q}H/&1&\z&RX"%576=- O Ê| OLÌ 6,0Y"%$'[
& Q¢¢%
C'6>a3,5:@LC43+"%3D5:Hot0%6>×3H%5:';87&Dqto}3,5:87&C|57"/&1023+"%576k6k3H%5:';87&?40%6,iG87&> 5:4H/"%3,4RX&
Tq ¤Ç^| O

~5ë
î

À

4Á

4

&zl43,8t|i4023,4R2$45:';No}3,RX"/6,0 O ãYoQRX6l'02H/&,@"%$'&H/&Dq}3,&1023+;,&C|nzl4>Ki&102Hn3+0%&'6,"n?&10%ot&RX"n?40%&C45:RX"/6,02H
6,oj?&10%ot6,02>a3,4RX&,I5:#"%$'B
& ¡¢QÈ¾h3=C'6>a3,5:@Íot6,0]&X'3,>?G87&,@	"%$'&zá 85F*á Ø¡á 8>á023+"%576.5:HKzl457"/&8:3+0%;,&iGl'"
6l'"/?&10%ot6,02>aHjJL9M-a3,~zE`3~Dq}3,HQH%$'6EQ5: Ì 57;l'0%{
& £'q}3|/| O
}	-
h"5:H5:4H/"/02l4RX"%57,&L"/6n876z6,ga>6,0%&YR1876H/&87~a3+""%$'&PH/"%3+"/&H`5:a"%$'
& Q¢¢%nC'6>a3,5:@z3,4CaEQ$~"%$'&1~
;,0%6EäEQ$'&[0%&1;,0%&H%H/&C OWÌ 6,0a&X'3,>?G87&,@"%$'&WÕ/H/"%3+"/&Ö#6,oP&3,R2$k3,5702RX023+ot"a5:Hq}5:[&3,R2$WE6,028:CTH/"%3+"/&|
C'&H%RX0257i&Ciz~"%$'0%&1&RX6>?6'&"%H1IM6'&B"/&8:8:H`EQ$'&10%&P"%$'&3,5702RX023+ot"L5:H`?6H%57"%576'&C)5:"%$'&'&1"FE6,0%g)6,o
3,570%?6,0%"Í02l4E`3~^HM3,4Cn"%3'57E`3~^H1@6'&jEQ$45:R2$nC4570%&RX"%57657"5:H	o}3,R15:';'@3,4Cn6'&jEQ$'&1"%$'&10M57"5:H	?G3+0%g,&C@
i&5:';)?Gl4H%$'&C.6,0K>6^5:';l44C'&1057"%HB6EQ?6E&101* ò OaÏ 8:>6H/"&1,&10%~.6,?&1023+"/6,0"%$43+"KR2$43,';,&HK6'&
6,o"%$'&H/&n$43,HY3,=&XÄ&RX"P6,0P?40%&RX64C457"%576Z6="%$'&]6,"%$'&10Y"FE6Z3,HQE&8:8 OQÌ 6,0P&X'3,>?G87&,@3,~5:4H/"%3,4RX&
6,o"%$'f
& ÈÝQ¾a6,?&1023+"/6,0@EQ$45:R2$R2$43,';,&HY"%$'&]?6H%57"%576Z6,oM3,N3,5702RX023+ot"1@0%&zl4570%&Hj"%$'&n3,5702RX023+ot"Q"/6i&
>6^5:';3,4Co}3,R15:';n3]?G3+0%"%5:R1l48:3+0`C4570%&RX"%576@^3,4CZ>a3~3,8:H/6R2$43,';,&P"%$'&Po}3,R15:'; O JL$zl4H1@^0%&1;,0%&H%H%5:';
3PH/"%3+"/&LRX6"%3,5:45:';P6487~3P;,63,8'3+"/6> i&876';5:';Q"/6P6'&`6,oG"%$'&jRX6>?6'&"%H5:]>6H/"R13,H/&H0%&H%l487"%H
5:Z3H/"%3+"/&KRX6"%3,5:45:';n;,63,8A3+"/6>aHji&876';5:';]"/63,8:8"%$'0%&1& OMÏ RX64R18:l4H%576)6'&>a3~)C'023Eæ5:H`"%$43+"
H/?G8:57"/"%5:';Q8:3+0%;,&`H/"%3+"/&HLq K
Ï nYì ÑF'6^C'&H|A5:"/6PH%>a3,8:87&10ÍH/"%3+"/&HLqãP_jÑF'6^C'&H|iG3,H/&CK6487~B6]"%$'&`zl4>Ki&10
6D3+"/6>aH]5:H'6,"]3,87E`3~^HK"%$'&a0257;$"KR2$'65:RX& OZÏ D3,87"/&10243+"%57,&aE6l48:Ci&a"/6NC457^5:C'&3+"/6>aH]5:"%$'&
?G8:3,445:';L?40%6,iG87&>x5:"/6P;,0%6l'?GHÍ6,oÕ!0%&8:3+"/&C4ÖB3+"/6>aHM3,4C]"%3+g,&L"%$'&jzl4>Ki&10	6,oG;,0%6l'?GHÍ0%&1?40%&H/&"/&C
5:Z3aH/"%3+"/&K"/6ai&B57"%HLH%571& O
Ï '6,"%$'&10Y6,iGH/&10%+3+"%576"%$43+"YR13,=i&K>a3,C'&]5:HL"%$43+"Q"%$'&KiGl487g6,o"%5:>&]H/?&"Q5:Z0%&8:3^&C=H/&3+02R2$
5:HLH/?&"L5:)"%$'&B43,8H/&3+02R2$N57"/&1023+"%576@'EQ$'&3aH/68:l'"%576)&X'5:H/"%HLEQ57"%$45:"%$'&KR1l'0%0%&"LRX6H/"Qi6l44C O
JL$45:Ha57"/&1023+"%576k5:H'6,"6487~"%$'&N>6H/"&X^?&4H%57,&,@`iGl'"3,8:H/6D"%$'&N87&3,H/"l4H/&1o}l48@`H%5:4RX&0%&8:3+"%57,&87~
ot&1E $'&l'025:H/"%5:R5:>?40%6,&>&"%HP3+0%&nC45:H%RX6,&10%&C5:=57" O JL$45:HQ3,8:H/60%&8:3+"/&HY"/6)"%$'&]i4023,4R2$45:';o}3,RX"/6,0@
H/?&R15´R13,8:87~."%$'&o}3,RX"n"%$43+" K
Ï nYì ÑF'6^C'&Hn$43,&>a3,~D>6,0%&)H%l4R1RX&H%H/6,02H]"%$43,TãP_jÑF'6^C'&H1Iot6,03,
F
Ñ
'6
^
'
C
n
&
/
"
Z
6

i
n
&
/
H

6
7
8
,


&
.
C
,
3
:
8
8	57"%HPH%l4R1RX&H%H/6,02HB>]l4H/"Pi&nH/687,&C@H/6Z5:N"%$'&]43,8Í57"/&1023+"%576@3,8:8	H%l4RÑ
KÏRX&nYH%H/ì 6,02HQ6,oM&1,&10%~
F
Ñ
'6
^
'
C
]
&
3+0%&]H/&3+02R2$'&C O Q6E&1,&10@"%$'&K?Gl'0%?6H/&B6,o0%&8:3^&CNH/&3+02R2$.5:HQ'6,"Q"/6
Ï nYì
K
4C3H/68:l'"%5765:Z"%$'&Ks)Ñ0%&1;,0%&H%H%576=H/?G3,RX&,@GiGl'"L"/6a4CH%571&sÉH/"%3+"/&HnqãP_jÑF'6^C'&H|`EQ$'6H/&KRX6H/"
5:HPl44C'&10%&H/"%5:>a3+"/&C=iz~"%$'&$'&l'025:H/"%5:R+@3,4C.>6,0%&3,R1R1l'023+"/&aRX6H/"P&H/"%5:>a3+"/&HPot6,0B"%$'&H/& O JL$'&10%&1ot6,0%&
57"`>a3~)'6,"j3,RX"%l43,8:87~ai&P'&RX&H%H%3+0%~"/6H/&3+02R2$3,8:8G"%$'&BH%l4R1RX&H%H/6,02H`"/6&1,&10%~ K
Ï nYì ÑF'6^C'& OMÏ Z3,87"/&10/Ñ
43+"%57,&YE6l48:Cai&Y3)qtsy Þ4|Ñ0%&1;,0%&H%H%576)H/&3+02R2$)5:aEQ$45:R2$a6487~"%$'6
& Þ>6H/"nÕ!?40%6>a5:H%5:';ÖH%l4R1RX&H%H/6,02H
"/6Z&1,&10%~ K
F
Ñ
'6
^
'
C
a
&
+
3
%
0

&
X
R

6
4H
%
:
5
'
C
1
&
%
0

&

C

@
^
~
7
5

&
:
8
4
C
:
5
';

,
3
'6
,
%
"
'
$
1
&

0
4
C
:
5

>

&
4H
%576ot6,057"/&1023+"%57,&87~=0%&X45:';"%$'&
Ï nYì
$'&l'025:H/"%5:Rnqt"%$45:HL5:HjC45:H%R1l4H%H/&C)o}l'0%"%$'&10L5:)"%$'&K'&X^"LH/&RX"%576| O
Ì 5:43,8:87~,@j57"H%$'6l48:CTi&?65:"/&Ck6l'""%$43+"&1,&V"%$'6l';$ }	- 5:H1@j6V3,&1023+;,&,@Yi&1"/"/&10"%$43,
JL9M-'@	0%&H%l487"%HB6,oji6,"%$D?G8:3,4'&102HB6#"%$'&)RX6>?&1"%57"%576DC'6>a3,5:4HH/"%5:8:8M3+?4?&3+0K023+"%$'&10]?6z6,0 O JL$'&
6487~Z6,"%$'&10P6,?4"%5:>a3,8"/&>?6,023,8A?G8:3,4'&10Q"/6?G3+0%"%5:R157?G3+"/&]5:"%$'&nRX6>?&1"%57"%576=E`3,H<`9JqtßP5:C43,8ÍÇ
ÈY&XÄ'&10@^(+*,*+-z|2@'EQ$45:R2$@5:>6H/"C'6>a3,5:4H1@3,R2$457&1,&C>]l4R2$ai&1"/"/&100%&H%l487"%H"%$43,JL9M-qt"%$'&L0%&H%l487"%H
6"%$'&a&X^"/&4C'&w
C 9Q¾h¤hQ¾)?40%6,iG87&>ØH/&1"1@	3,4C#3,#5:'ot6,02>a3,8	RX6>?G3+025:H/6i&1"FE&1&DC43+"%3Zot0%6>
"%$'&KRX6>?&1"%57"%576Z3,4CZ"%$'&0%&H%l487"%H`?40%&H/&"/&C=$'&10%&,@G5:4C45:R13+"/&P"%$43+"Q57"j6l'"/?&10%ot6,02>aH }	- 3,HjE&8:8t| O
!["%$'&'6^Ñ"/&>?6,023,
8 ¡¢QÈ¾h3.3,4C.¢C'6>a3,5:4H1@M6,"%$'&10a6,?4"%5:>a3,8`?G8:3,4'&102H]3,8:H/66l'"/?&10%ot6,02>
q}RX6>?&1"%57"%576a0%&H%l487"%HM3+0%&Q?40%&H/&"/&Caiz~aQ6+Ä>a3,43,4C + C'&87g+3,>?h@ ½| O <`9JW3,8:H/6]l4H/&HM"%$'&
}	-
"/&>?6,023,8Ím $ $'&l'025:H/"%5:R+@'iGl'"YH/&3+02R2$'&HP3)<L\^9kot6,02>]l48:3+"%576Z6,o?G3+0%"%5:3,86,02C'&10Y?G8:3,445:';'I$'&l'025:H/"%5:R
&H/"%5:>a3+"/&HY3,4CZ"%$'&KR1l'0%0%&"QRX6H/"Li6l44CZ3+0%&Kot6,02>]l48:3+"/&C3,HQRX64H/"/023,5:"%H1@G3,4C=RX6H/"Li6l44C)^5768:3Ñ
"%5764Hj3+0%&B5:'ot&10%0%&Ciz~RX64H/"/023,5:"j?40%6,?G3+;3+"%576@'3,65:C45:';n"%$'&B'&1&C)"/6&X^?G8:5:R157"%87~&1+3,8:l43+"/&H/"%3+"/&H
3,4C&43+iG8:5:';a&3+028:57&10YC'&1"/&RX"%576 O JL$'&?G3+0%"%5:3,86,02C'&10Yi4023,4R2$45:';a3,4C="%$'&]l4H/&K6,o	& =R157&"Q?40%6,?G3Ñ
!£¹©¦!¦Y«t¥!ªj¬,ª²¦!²1¨°%­¦Í¦h¨ª2³z¬­ª¬,ª¯ ¨¯ ª² · ¯ ¨©j¨©¦A¬­ª¬,¦h­¨t»`¨©+%¨¦h®2¥h¨ »jª²¦Í¬­ª¬,ª¯ ¨¯ ª²Q¯ ²L¨©¦A¦h¨¯ 
¨­¦¯ ²L2²1»­¦/2¥F©+2º ¦ · ª2­ §`}¨F%¨¦9± ÄkÅÇÆsÅ ±X¯ ²µ%­¯ 2²1¨!4¹©!±1¯ ²L2²1»`}¨F%¨¦¦h®2¥h¨ »`ª²¦ V v;lv¶v~5f	
ß ;lvAW ¯ ¨­¦	³´ª2­A¦/2¥F© v~5f	 ±2²+§Qª`ª²n ´¨©¦	¥/2¦ · ©¦!²Q¨©¦M2¯ ­¥h­F%³ ¨Í¯ A2¯ ­º,ª2­²¦M¯ ¬,¦!¥!¯ 2 ¤

5}
î

î Á(ï^ðzñ7òGó À

4ï %ô :õ Yö 4ï^ð-Gó-N÷æ s4ø- ù ¦Gï^õ¶

  1   f

;3+"%5763+0%&Qi6,"%$5:>?6,0%"%3,"Mot6,0"%$'&Q&	=R157&4RX~6,o"%$'&B<`9JW?G8:3,4'&10@,iGl'"57"E6l48:C?40%6,iG3+iG87~n3,8:H/6
i&'&XG"Bot0%6>Â3Z>6,0%&3,R1R1l'023+"/&$'&l'025:H/"%5:R O JL$zl4H1@"%$'&H/&3+02R2$DH%R2$'&>&a6,oQ<`9J 3,4C"%$'&5:C'&3)6,o
5:>?40%6^5:';$'&l'025:H/"%5:R1H"%$'0%6l';$H/&3+02R2$=3+0%&RX6>?G87&>&"%3+0%~,@3,4CZ>a3~)i&B?6H%H%57iG87&Y"/6RX6>KiG5:'& O

à	¶ F·½»,FG  ( F½Fº
JL$'&a5:C'&3)6,o`l4H%5:';ZH/&3+02R2$#"/6=C'&10257,&6,0K5:>?40%6,&a$'&l'025:H/"%5:R1HB5:HB'6,"K'&1E O JL$45:HPH/&RX"%576#0%&1^57&1EQH
3=H/&87&RX"%5766,oj0%&8:3+"/&CD>&1"%$'6^C4H O áS57"%$#"%$'&a&X'RX&1?4"%576#6,oj"%$'&C45:H%R1l4H%H%576N6,oj?G3+"/"/&102DC43+"%3+iG3,H/&
$'&l'025:H/"%5:R1H1@A"%$'&aot6^R1l4H]5:"%$45:HKH/&RX"%5765:HB6$'6E "%$'&H/&.qt6,0nH%5:>a5:8:3+0X|Y>&1"%$'6^C4HKR13,Di&3,C43+?4"/&C
3,4CZ&X^?G87657"/&C)"/65:>?40%6,&0%&8:3^&CZH/&3+02R2$ O

O#F¶nÎáâDnq>¶|mgrq	rÍÎq

ì 1& 0257^5:';$'&l'025:H/"%5:R1HBiz~H/687^5:';3,3+iGH/"/023,RX"/&C@	6,0]0%&8:3^&C@A,&102H%576#6,oj"%$'&H/&3+02R2$D?40%6,iG87&>Â5:H
'6,"Q3'&1Eþ5:C'&3^@G3,4C='&57"%$'&10Q5:Hj"%$'&]5:C'&36,ol4H%5:';aH/&3+02R2$="/6H/687,&C"%$'&K3+iGH/"/023,RX"Q?40%6,iG87&>Ðq}H/&1&
c ÅcÈP3,H%R2$4457;'@wâ Ð+âzû9Í&3+028@w9â £-'û9M0257&C457"%5:H1@wâ,â,ç| O
Ï 0%&RX&"1@'3,4CaH%l4R1RX&H%H/o}l48@+3+025:3,"M6a"%$45:H"%$'&>&Q5:H	Ô',eeFXdëN,e3 y%P'	 è^dÝfXe}ÝXLq<`l487i&102H/6
ÇØ\^R2$43+&XÄ&10@Lwâ,â,ýzûjQ&10243,C' 6ã 87;,~^5`ÇäQ687"/&,@(+*,*,*| O JL$'&H/&3+0%&ZC'&X'&C#iz~D3+iGH/"/023,RX"%5:';.3E`3~
?G3+0%"Í6,oG"%$'&`?40%6,iG87&>r3,4CnH/687^5:';Q6487~"%$'&`?G3+0%"Í"%$43+"	0%&>a3,5:4H`qt"%$'&Õ!?G3+"/"/&1024Ö| O JL$'&`3+iGH/"/023,RX"%576
5:>?G8:5:R157"%87~ZC'&X'&HB3)?40%6í/&RX"%576.ot0%6>v"%$'&?40%6,iG87&>vH/&3+02R2$#H/?G3,RX&a5:"/63ZH%>a3,8:87&10BH/&3+02R2$#H/?G3,RX&,I
6,?4"%5:>a3,8H/68:l'"%576DRX6H/"n5:#"%$45:HK3+iGH/"/023,RX"nH/?G3,RX&)5:H]3=876E&10ni6l44C6D6,?4"%5:>a3,8H/68:l'"%576DRX6H/"n5:
"%$'&B6,0257;5:43,8H/&3+02R2$H/?G3,RX&,@43,4C)iz~>a3+g^5:';n"%$45:H`H/?G3,RX&H%>a3,8:8&'6l';$)"%$'&B6,?4"%5:>a3,8RX6H/"L6,oA&1,&10%~
H/"%3+"/&P5:a"%$'&Y3+iGH/"/023,RX"?40%6,iG87&>H/?G3,RX&PR13,ot6l44Caiz~iG8:5:4CH/&3+02R2$@^3,4CH/"/6,0%&C5:3"%3+iG87&YH/6K"%$43+"
H/"%3+"/&K&1+3,8:l43+"%576R13,i&C'6'&Biz~)3aH%5:>?G87&P"%3+iG87&876z6,g^l'?Dq}$'&4RX&"%$'&K43,>&?G3+"/"/&102C43+"%3+iG3,H/&| O
Q&l'025:H/"%5:RB&H/"%5:>a3+"/&HQot0%6>ä>]l487"%57?G87&3+iGH/"/023,RX"%5764HYR13,=i&KRX6>KiG5:'&Ciz~)"%3+g^5:';"%$'&570Y>a3'5:>]l4>
q}5:DH/6>&R13,H/&HK"%$'&570]H%l4>ZûH/&1& Ì &8:'&10O
@ B6,0%oh@	ÇÃY3,43,@(+*,*+-z| O 9	3+"/"/&102C43+"%3+iG3,H/&)$'&l'025:H/"%5:R1H
$43,&i&1&UH%l4R1RX&H%H/o}l48:87~W3+?4?G8:57&Ck"/6W3Tzl4>Ki&106,onH/"%3,4C43+02CSH/&3+02R2$U?40%6,iG87&>aHq<`l487i&102H/6VÇ
\^R2$43+&XÄ&10@Mwâ,â,ýzû Ì &8:'&10P&1" 3,8 O @(+*,*+-z|2@A3,4C.3,8:H/6"/6ZH/&zl'&"%5:3,8\^JL_L!9`\)?G8:3,445:';Nq + C'&87g+3,>?@
(+*,*'w| O JL$'&a5:C'&3)6,o`?G3+"/"/&102#C43+"%3+iG3,H/&H]>a3~3+?4?&3+0,&10%~H%5:>a5:8:3+0P"/60%&8:3^&CDH/&3+02R2$Vq}5:4C'&1&C@
"/6"%$'&C'&X457"%5766,oÍ"%$'&Km'pr$'&l'025:H/"%5:R1H`5:;,&'&1023,8t|5:"%$43+"L"%$'&P?40%6,iG87&>5:HÕ/H/?G8:57"%Ön5:"/6H%5:>?G87&10
?40%6,iG87&>aHAEQ$45:R2$n3+0%&LH/687,&Cn5:4C'&1?&4C'&"%87~,@+3,4Cn"%$'&jH/68:l'"%576nRX6H/"%Hot6,0	"%$'&H/&Ll4H/&Cn3,H3B$'&l'025:H/"%5:R
&H/"%5:>a3+"/&Not6,0"%$'&NRX6>?G87&1"/&=?40%6,iG87&> O JL$'&10%&N5:H1@j$'6E&1,&10@Y3DRX02l4R15:3,8QC45´Ä&10%&4RX&,@j5:W"%$43+""%$'&
m'pË0%&8:3'3+"%576W?&10%ot6,02>aH"%$45:HH/?G8:57")d%%v è^d2ÍÝ ¥,X Þ+@L"/6D&1,&10%~WH/"%3+"/&N6,oH%571&=>6,0%&"%$43,ks@`EQ$45:87&
"%$'&a3+iGH/"/023,RX"%576."%$43+"C'&X'&HP"%$'&n?G3+"/"/&102#5:.3)?G3+"/"/&102#C43+"%3+iG3,H/&a$'&l'025:H/"%5:RK5:HY4^&CWq}3,8:H/6'@"%$'&
3+iGH/"/023,RX"%576n"%$43+"C'&X'&HÍ3P9 ì Æ5:H	5:K;,&'&1023,8'3jÔd/1Ò ä%e}ÝÒ+ë4@Ýc}c:@,3P>a3,~Ñ"/6+Ñ6'&L>a3+?4?G5:';'@+EQ$45:87&
"%$'&m'p 0%&8:3'3+"%576D5:H3,VXo
Ó y2%+ÝtëzÅ,@`Ýc}c:@A&3,R2$H/"%3+"/&)5:."%$'&a6,0257;5:43,8	0%&1;,0%&H%H%576DH/&3+02R2$DH/?G3,RX&
RX6,0%0%&H/?64C4Hj"/63H%5:';87&H/"%3+"/&n5:Z"%$'&K0%&8:3^&C=H/&3+02R2$NH/?G3,RX&| O<+ ,&N57oÍ&H/"%5:>a3+"/&HQot0%6>ä>]l487"%57?G87&
?G3+"/"/&102.C43+"%3+iG3,H/&Haq}3+iGH/"/023,RX"%5764H|Q3+0%&RX6>KiG5:'&C@"%$'&nRX6>KiG5:43+"%576N6,oM+3,8:l'&Hqtiz~=>a3'5:>a575:';
6,0H%l4>a>a5:';z|B6^R1R1l'02H6487~3+"a"%$'&Z0%6z6,"1@YÝc}c:@"%$'&H/"%3+"/&=i&5:';N&1+3,8:l43+"/&C@`3,4CT'6,"3,876';"%$'&
H/68:l'"%576W?G3+"%$4Hot0%6> "%$45:HH/"%3+"/&5:W"%$'&NC45´Ä&10%&"3+iGH/"/023,RX"%5764H O JL$45:HC45´Ä&10%&4RX&=>&3,4H"%$43+"
5:[H/6>&?40%6,iG87&>aH1@"%$'&=m'pË$'&l'025:H/"%5:RR13,Ti&Z>6,0%&3,R1R1l'023+"/&"%$43,W3,~?G3+"/"/&102WC43+"%3+iG3,H/&6,o
0%&3,H/643+iG87&H%571&aq}Y3,H%8:l4>Z@4Æ6'&1"YÇÈY&XÄ'&10@G(+*,*Êz@G?40%6^5:C'&B3,Z&X'3,>?G87&| O ãPZ"%$'&B6,"%$'&10Q$43,4C@
"%$'&?6H%H%57iG5:8:57"F~6,o`3,C4>a5:H%H%57iG87~ZH%l4>a>a5:';+3,8:l'&HYot0%6>Ø>]l487"%57?G87&K?G3+"/"/&102#C43+"%3+iG3,H/&H>&3,4HP"%$43+"
5:kH/6>&N?40%6,iG87&>aH1@`3RX68:87&RX"%576k6,o3,C4C457"%57,&?G3+"/"/&102SC43+"%3+iG3,H/&H)R13,kot6,02>Ú3D$'&l'025:H/"%5:R>6,0%&
3,R1R1l'023+"/&a"%$43,Dm'p@ot6,0K3,~N0%&3,H/643+iG87&n+3,8:l'&n6,os q}3+;3,5:@AY3,H%8:l4>Z@Æ6'&1"KÇäÈY&XÄ'&10@Í(+*,*Êz@
;57,&K3,Z&X'3,>?G87&| O

5}<
î

À

4Á

4

!H/6>&E`3~^H1@0%&8:3^&C[H/&3+02R2$[$43,H]>6,0%&)5:DRX6>a>6EQ57"%$#"%$'&)5:C'&36,o`Ô',eeFXdëW%+d/X'@
C'&1,&876,?&C5:D"%$'&RX6"/&X^"6,oL"%$'&\z6,g,6,iG3,?Gl'187&,@	EQ$45:R2$3+0%&)>6,0%&)C'~^43,>a5:R=qül4';$43,44H]Ç
\^R2$43+&XÄ&10@4(+*,*'w| O åA57g,&P5:3K?G3+"/"/&102)C43+"%3+iG3,H/&B$'&l'025:H/"%5:R+@z3K?G3+"/"/&102)H/&3+02R2$Z3+iGH/"/023,RX"%H`3E`3~?G3+0%"
6,o'"%$'&?40%6,iG87&>æ3,4CKH/687,&HA"%$'&M0%&>a3,5:45:';Kq}H%>a3,8:8t|?40%6,iG87&>U"/6Q6,i4"%3,5:K3,]5:>?40%6,&CK876E&10Íi6l44C@
iGl'"B"%$'&a?G3+"/"/&102kqhÝc}c:@A"%$'&a?G3+0%"6,o"%$'&a?40%6,iG87&>v"%$43+"K5:HBg,&1?4"iz~."%$'&3+iGH/"/023,RX"%576|P5:HBH/&87&RX"/&C
EQ$'&'&1,&103?G3+0%"%5:R1l48:3+0aH/"%3+"/&=&X^?G3,4H%576rqXÕ/>6,&Ö|a5:HaRX64H%5:C'&10%&C O 9	3+"/"/&1024H"%$43+"$43,&=i&1&
H/&3+02R2$'&C=3+0%&KH/"/6,0%&C@G3,876';EQ57"%$)"%$'&570Ql'?C43+"/&CZRX6H/"1@G3,4CZ"%3+g,&N5:"/6a3,R1RX6l4"Y5:)"%$'&K$'&l'025:H/"%5:R
&1+3,8:l43+"%576=qtiz~a>a3'5:>a573+"%576|A6,o3,~'&1EVH/"%3+"/&Q"%$43+"Y2Ò+ëGe+Ýtë'M"%$'&QH%3,>&L?G3+"/"/&102@&4RX6l4"/&10%&C
iz~n"%$'&YH/&3+02R2$ O JL$'&L?G3+"/"/&1024H&X^?G876,0%&Ciz~n?G3+"/"/&102H/&3+02R2$'&H3+0%&Qot6l44C"%$'0%6l';$3,5:4RX0%&>&"%3,8
?40%6^RX&H%H1IJL$'&LG02H/"M?G3+"/"/&102)RX64H%5:H/"%HM6,o6487~n"%$'&Y?G3+0%"6,o"%$'&Y?40%6,iG87&>ËqXÕ/H/"/6'&Ö|M"%$43+"`5:HC4570%&RX"%87~
3Ä&RX"/&CViz~["%$'&N>6,&.l44C'&10RX64H%5:C'&1023+"%576 O JL$'&='&X^"?G3+"/"/&102W&X^"/&4C4H"%$'&?40%&1^576l4HEQ57"%$
H/"/6'&HK"%$43+"K5:"%$'&R1l'0%0%&"KH/"%3+"/&RX6>5:RX"EQ57"%$"%$'&H/68:l'"%576.ot6l44C#5:"%$'&?40%&RX&C45:';Z?G3+"/"/&102
H/&3+02R2$@G3,4CZ"%$45:Hj5:Hj0%&1?&3+"/&Cl4"%5:8'6a>6,0%&KRX6>5:RX"%HL3+0%&Bot6l44C O
Ï HY>&"%576'&CN5:"%$'&K?40%&1^576l4HQH/&RX"%576@"%$'&]$457;$=RX6>?Gl'"%3+"%57643,8ÍRX6H/"Y6,o"%$'&Ks)Ñ0%&1;,0%&H%H%576
H/&3+02R2$)5:HM6,ot"/&)C4l'&L"/6]"%$'&Yo}3,RX""%$43+" K
Ï nYì ÑF'6^C'&H$43,&B>a3,~aH%l4R1RX&H%H/6,02H1@^3,4C>6H/"6,o"%$'&Q"%5:>&
57"5:Hn'6,"3,RX"%l43,8:87~D'&RX&H%H%3+0%~"/6H/&3+02R2$T"%$'&>è3,8:8ot6,0&1,&10%~ K
Ï nYì ÑF'6^C'&,IZ3,T3,87"/&10243+"%57,&Z5:H]"/6
H/&3+02R2$#6487~="%$'&a>6H/"=Õ!?40%6>a5:H%5:';ÖH%l4R1RX&H%H/6,02H1@AEQ$'&10%&a3)?40%6>a5:H%5:';)H%l4R1RX&H%H/6,0K5:H3,ãP_jÑF'6^C'&
EQ$'6H/&aRX6H/"5:HB8:57g,&87~="/6Zi&l44C'&10%&H/"%5:>a3+"/&C.iz~N"%$'&aR1l'0%0%&"B$'&l'025:H/"%5:R+@3,4C"%$'&10%&1ot6,0%&a8:57g,&87~"/6
5:4RX0%&3,H/&EQ$'&)"%$'&K'6^C'&5:Hj&X^?G3,4C'&C O åA5:>a57"%5:';]"%$'&Kzl4>Ki&10L6,oH%l4R1RX&H%H/6,02HLH/&3+02R2$'&C=ot6,0L&1,&10%~
l4457ot6,02>a87~#"/63+">6H/I
" Þ[0%&H%l487"%Ha5:W3,þqtsy Þ4|Ñ0%&1;,0%&H%H%576VH/?G3,RX&,@j3,4Ck3#H/&10257&Ha6,o
KÏ qtsnYy ì Þ4ÑF|'6Ñ0%^&1C';,&N
0%&H%H%576.H/&3+02R2$'&HYEQ57"%$=5:4RX0%&3,H%5:';asÉ3,4CÑÞR13,Ni&K6,0%;3,4571&CN5:=C45´Ä&10%&"QE`3~^H1Ijot6,0
&X'3,>?G87&,@"%$'&Z?G8:3,4'&10RX6l48:C[?&10%ot6,02> qtsyw|Ñ0%&1;,0%&H%H%576kH/&3+02R2$'&Haot6,0as uÂçzy1{1{1{l4"%5:8jH/6>&
H%l457"%3+iG87&BH/"/6,?4?G5:';RX64C457"%5765:Hj>&1"1@G"%$'&Dqtsy2(|Ñ0%&1;,0%&H%H%576=H/&3+02R2$'&HQot6,0LsvuUçzy1{1{1{f@	1e O JL$45:H
5:H`,&10%~)H%5:>a5:8:3+0"/6a57"/&1023+"%57,&Bi40%63,C'&45:';nH/&3+02R2$qÈP5:4H/i&10%;ÇxY3+0%,&1~,@Íwâ,â,(| O`Ï Z3,87"/&10243+"%57,&5:H
"/6=8:5:>a57"Y"%$'&&X^?G3,4H%576.6,o K
Ï nYì ÑF'6^C'&HK'6^ÑFl4457ot6,02>a87~,@ot6,0&X'3,>?G87&,@"/6=H/&3+02R2$D3,8:8H%l4R1RX&H%H/6,02H
H%3+"%5:H/ot~^5:';BH/6>&LRX0257"/&102576not6,0i&5:';P?40%6>a5:H%5:';'@,H%5:>a5:8:3+0Í"/6"%$'&L57"/&1023+"%57,&jEQ5:C'&45:';YH/"/023+"/&1;,~al4H/&C
5:)"%$'&KRX6"/&X^"Q6,oÍ;3,>&XÑ"/0%&1&nH/&3+02R2$q<`3+1&43,&,@(+*,*'w| O
JL$'&KRX6>5:RX"!ÑFC4570%&RX"/&C=H/"/023+"/&1;,~l4H/&C"/6a4C)?G3+"/"/&1024HY5:)?G3+"/"/&102NH/&3+02R2$'&HYC'&>64H/"/023+"/&HY3
E`3~a"/6nC45:H/"%5:';l45:H%$]?40%6>a5:H%5:';H%l4R1RX&H%H/6,02H O <64H%5:C'&10H/&zl'&"%5:3,84?G8:3,445:';BEQ$'&10%&Y3, K
Ï nYì ÑF'6^C'&
5:HYH%5:>?G87~Z3)H/&1"Y6,o>6,0%&]"%$43,Ns q}H%l'i4;,63,8t|L3+"/6>aH1@3,4CN"%$'&H%l4R1RX&H%H/6,02HP3+0%&n3,8:8	H%571&]súH%l'iGH/&1"%H
6,o"%$45:HQH/&1"1IjH%l'iGH/&1"%HQ>6,0%&]8:57g,&87~"/6)$43,&n3$457;$'&10QRX6H/"Y"%$43,="%$'&K&H/"%5:>a3+"/&K;57,&=iz~=m''
p åPR13,
i&Q5:C'&"%5´G&Caiz~aH/687^5:';B&3,R2$ZH%571&Q
s G#wLH%l'iGH/&1"3,4Ca&X'3,>a5:45:';"%$'&YH/68:l'"%576aot6,0RX6>5:RX"%HEQ57"%$
0%&>a3,5:45:';]3+"/6>aHj5:"%$'&PH/"%3+"/&q}3RX6>5:RX"i&5:';]ot6,0`&X'3,>?G87&P3,Z3,RX"%576)"%$43+"jC'&87&1"/&H`"%$'&B3+"/6>Z@
6,0Q3,RX"%5764Hj5:4RX6>?G3+"%57iG87&YEQ57"%$)3,3,RX"%576Z'&1&C'&CZ"/6&H/"%3+iG8:5:H%$"%$'&3+"/6>| O !)o}3,RX"1@4"%$45:H`>&1"%$'6^C
$43,Hi&1&l4H/&C)5:"%$'&PRX6"/&X^"j6,oÍ3]C45´Ä&10%&"`>&1"%$'6^Cot6,0j5:>?40%6^5:';"%$'&Bm'pæ$'&l'025:H/"%5:R1H"%$'0%6l';$
8:5:>a57"/&CH/&3+02R2$Zq}Y3,H%8:l4>Z@,(+*,*+-3| O \z6>&`R13+0%&>]l4H/"Ai&"%3+g,&]"/6Y&4H%l'0%&M"%$43+"Í"%$'&H/&3+02R2$'&H	'&1&C'&C
"/6]4C"%$'&P?40%6>a5:H%5:';KH/&1"%Hj3+0%&B'6,"`>6,0%&P&X^?&4H%57,&Y"%$43,)H/&3+02R2$45:';]&1,&10%~H/&1"1@'iGl'"`57o"%$'&m '
p å
+3,8:l'&jE`3,HM3,8:H/6RX6>?Gl'"/&Ciz~]0%&8:3^&CaH/&3+02R2$@"%$'&LH%571&jD
s G=wjH%l'iGH/&1"%HLqt6,03+"M87&3,H/"H/6>&j6,o"%$'&>|
$43,&]3,870%&3,C'~i&1&ZH/&3+02R2$'&C@G3,4CRX6>5:RX"%Hjot6l44CZC4l'025:';]?40%&1^576l4HjH/&3+02R2$'&HYR13,Zi&H%3,&C O

ÛiKUI±æQq9[#nUnç wnèqÝw`¶q	Ýn-kg|¢U­g¶Ûilo-t-rquv!t-liKés±êUìëlnÁ
# ¶nÎ 
F
JL$'&a\4<LãY
ñ J ÏKYn ì ãP_à"/0%&1&H/&3+02R2$3,87;,6,0257"%$4>Z@C'&1,&876,?&C.>a3,5:487~Zot6,0BH/&3+02R2$45:';);3,>&n"/0%&1&H1@
"/0257&HA"/6Y0%&C4l4RX&"%$'&`zl4>Ki&10A6,o4'6^C'&HA&1+3,8:l43+"/&C]iz~PG02H/"A"/&H/"%5:';Qot6,0Í&3,R2$'6^C'&57o'57"ÍR13,n3Ä&RX"	"%$'&

5}

î î

î Á(ï^ðzñ7òGó À

4ï %ô :õ Yö 4ï^ð-Gó-N÷æ s4ø- ù ¦Gï^õ¶

  1   f

+3,8:l'&B6,oM57"%Hj?G3+0%&"Li&1ot6,0%&&1+3,8:l43+"%5:';"%$'&]'6^C'&B&X'3,RX"%87~Dq}9Í&3+028@Awâ9£-z| O JL$'&"/&H/"Q5:Hj?&10%ot6,02>&C
iz~3N?40%6^RX&C4l'0%&,@MR13,8:87&C[H%5:>?G87~UÕ/JA&H/"%Ö^@EQ$45:R2$"%3+g,&Ha3,H3+0%;l4>&"%H3.'6^C'&)3,4C[3."%$'0%&H%$'68:C
+3,8:l'&,@3,4CNC'&1"/&102>a5:'&HQ57o"%$'&]+3,8:l'&K6,oM"%$'&]'6^C'&]5:HQ;,0%&3+"/&10qt6,0P&zl43,8t|`"%$43,="%$'&]"%$'0%&H%$'68:C@Giz~
0%&R1l'02H%57,&87~]"/&H/"%5:';K"%$'&Q'6^C'&,é HH%l4R1RX&H%H/6,02HYqt"/6n3KH/?&R15´G&CC'&1?4"%$|2@iGl'"M6487~nl4"%5:8'"%$'&Y5:'&zl43,8:57"F~
5:H?40%6,& O JL$'&j?40%6^RX&C4l'0%&jR13,a&3,H%5:87~]i&L>6^C45´G&C]"/60%&1"%l'02a3P;,0%&3+"/&10+3,8:l'&jot6,0M"%$'&Q'6^C'&jEQ$'&
H%l4R2$3+3,8:l'&5:Hjot6l44C@'"%$'6l';$Z"%$45:HL>a3~ZH/"%5:8:8i&K87&H%H`"%$43,"%$'&K'6^C'&HL3,RX"%l43,8+3,8:l'&,@43,4C57"L$43,H
i&1&)H%$'6EQ"%$43+"`"%$'&BJA&H/"`?40%6^RX&C4l'0%&,@z&4$43,4RX&CEQ57"%$)>&>6,0%~5:"%$'&Pot6,02> 6,oÍ3]"/023,4H/?6H%57"%576
"%3+iG87&,@4R13,)i&Bl4H/&C)57"/&1023+"%57,&87~"/6;57,&3,)&	=R157&"L3,87;,6,0257"%$4> "%$43+"LC'&1"/&102>a5:'&H"%$'&B&X'3,RX"j+3,8:l'&
6,o3'6^C'&q}98:3,3+"1@\^R2$43+&XÄ&10@95 í%8:H1@'Ç ÏKO @Awâ,â,ý| O
JL$'& ìPÌ \aH%l'i40%6l'"%5:'&j6,o ìLÏ ã  5:HM,&10%~aH%5:>a5:8:3+0M"/6n3KC'&1?4"%$^ÑFl4i6l44C'&Cn,&102H%5766,o"%$'&YJA&H/"
?40%6^RX&C4l'0%&,@3,4C"%$zl4H"%$'&j ìLÏ ã  3,87;,6,0257"%$4>à5:HMH%5:>a5:8:3+0	"/6KH%l4R2$a3,a57"/&1023+"%57,&Q3+?4?G8:5:R13+"%576n6,oJA&H/" O
JL$'&B>a3,5:)C45´Ä&10%&4RX&B8:57&H`5:"%$43+"j ìLÏ ãQÑ ìPÌ \Z3+?4?G8:57&H57"/&1023+"%57,&C'&1&1?&45:';Zqtiz~R13,8:8:5:';] ìLÏ ã  |
"/6"%$'&ZH%l4R1RX&H%H/6,02H6,o K
Ï nYì ÑF'6^C'&H1@EQ$'&10%&3,HJA&H/"aR13,8:8:Hn57"%H/&87oL0%&R1l'02H%57,&87~#EQ57"%$"%$'&ZH%3,>&RX6H/"
i6l44C O	Ï HL30%&H%l487"1@^ ìLÏ ã  4C4H"%$'&B6,?4"%5:>a3,8RX6H/"L6,o	3,~)H/687,&CNãP_jÑF'6^C'&,@^EQ$45:R2$ZJA&H/"LC'6z&H
'6,"]qt"%$'6l';$Z"%$'&$457;$'&10LRX6H/"Q0%&1"%l'02'&C)iz~q}>6^C45´G&C|JA&H/"LEQ$'&)"%$'&RX6H/"Q6,o	3a'6^C'&B5:Hj?40%6,&
"/6a&X'RX&1&C="%$'&B"%$'0%&H%$'68:CZ5:HjH/"%5:8:83876E&10Qi6l44C6Z"%$'&K'6^C'&Hj6,?4"%5:>a3,8RX6H/"| O
_L&RX&"%87~,@QÆ6'&1")3,4CSÈY&XÄ'&10q(+*,*+-z|a?40%&H/&"/&CV3#;,&'&1023,8YC'&1?4"%$^ÑG02H/"H/&3+02R2$V3,87;,6,0257"%$4>
ot6,0 K
Ï nY
ì  ãP_jÑ;,023+?G$4H1@	R13,8:87&C#å ìPÌ \@EQ$45:R2$#5:H3,8:H/6,&10%~H%5:>a5:8:3+0B"/6 ìLÏ ã O åA57g,&a ìLÏ ã  @57"
4C4Hj"%$'&6,?4"%5:>a3,8ARX6H/"Q6,o&1,&10%~H/687,&C='6^C'&,@G3,4C=5:>?40%6,&C=876E&10Qi6l44C4Hj6='6^C'&HL"%$43+"Y3+0%&
&X^?G876,0%&C.iGl'"B'6,"H/687,&C O å ìPÌ \@$'6E&1,&10@	H/"/6,?GHH/&3+02R2$45:';)"%$'&aH%l4R1RX&H%H/6,02HB6,o`3, K
Ï nYì ÑF'6^C'&
3,HH/6z63,HP6'&6,o"%$'&>Ø5:HPot6l44CN"/6$43,&a3ZRX6H/"B;,0%&3+"/&10K"%$43,."%$'&R1l'0%0%&"P&H/"%5:>a3+"/&ot6,0B"%$43+"
'6^C'&,@jEQ$'&10%&3,H ìLÏ ã  ?&10%ot6,02>aH57"/&1023+"%57,&.C'&1&1?&45:';#H/&3+02R2$'&H)l4"%5:8L"%$'&N'6^C'&N5:HH/687,&Ck6,0
H%$'6EQa"/6n$43,&P3]RX6H/";,0%&3+"/&10`"%$43,"%$'&YR1l'0%0%&"&H/"%5:>a3+"/&CRX6H/"6,o"%$'&Q?40%&C'&RX&H%H/6,0 K
Ï nYì ÑF'6^C'& O
87;,6,0257"%$4>Øot6,0s)Ñ0%&1;,0%&H%H%576DH/&3+02R2$D$43,&H%$'6EQ#"%$43+"K57"
+5:HK^'6?,&1"K025:>>6,&"0%&%HP	& EQ=57R1"%57$#&"3,K#"%$4h"/3,&1#023+"%57,&ã JA&H/"]3,#&
ìLÏ O=Ï X^?&1025:>&"%3,8MRX6>?G3+025:H/6#i&1"FE&1&D ìLÏ ã  3,4C#"%$'&
å ìPÌ \)3,87;,6,0257"%$4>Î0%&>a3,5:4H`"/6ai&C'6'& O
 ìLÏ ã  @h"/&1023+"%57,&nJA&H/"Y3,4C=å ìPÌ \3,8:8?&10%ot6,02>Î"/6,?'ÑFC'6EQ@C'&1?4"%$^ÑG02H/"L57"/&1023+"%57,&nC'&1&1?&45:';
H/&3+02R2$'&H1@BiGl'"'6'&6,on"%$'&H/&DR2$43+023,RX"/&1025:H/"%5:R1H6,on"%$'&#3,87;,6,0257"%$4>aH3+0%&#&H%H/&"%5:3,8Bot6,0"%$'&#"%$'&570
l4H/&a5:RX6>?Gl'"%5:';3,#5:>?40%6,&C#$'&l'025:H/"%5:R OÏ ~ K
Ï nY
ì  ãP_ H/&3+02R2$D3,87;,6,0257"%$4>ÂR13,#i&l4H/&C"/6
R13+0%0%~#6l'"]"%$'&0%&8:3^&CH/&3+02R2$@M3,Hn876';.3,Hn57"]C45:H%RX6,&102H]"%$'&6,?4"%5:>a3,8RX6H/"Zqt6,0a3=;,0%&3+"/&10a876E&10
i6l44C|`6,o&1,&10%~=&X^?G3,4C'&C#ãP_jÑF'6^C'& OÌ 6,0B&X'3,>?G87&,@H/"%3,4C43+02C Ï ã  q n 5:8:H%H/6@	wâ,9ý £|P3,4CN"%$'&
ÈY&'&1023,8:571&C ì í%57g^H/"/023Z3,87;,6,0257"%$4>vizÑ
~ N3+0%"/&8:8:5	3,4C8=6"%3,43+025Qq!wâ Ð+ç|Pi6,"%$C'6Z"%$45:H1@3,4Ci6,"%$
6+Ä&10Y3?6H%H%57iG5:8:57"F~6,o	"/023,C45:';;,0%&3+"/&10P>&>6,0%~0%&zl4570%&>&"%Hjot6,0Q87&H%HLH/&3+02R2$="%5:>&qt"%$'6l';$"%$'&
0%&H%l487"%H`6,oÆ6'&1"Q3,4CNÈY&XÄ'&10@(+*,*+-'@5:4C45:R13+"/&B"%$43+"j"%$45:HL>a3~Z'6,"ji&B"%$'&KR13,H/&| O
H/&3+02R2$k$43,Hi&1&T>6H/"%87~[5:,&H/"%57;3+"/&CW5:["%$'& Ï n3+0%&36,oP;3,>&?G8:3~^5:'; O JL$'&
KÏ nYãPì _æ ãPqt6,_Ë
O0 N5:^ÑN3G|H/&3+02R2$]H/?G3,RX&HA0%&1?40%&H/&"%5:';L"FE6+Ñ?G8:3~,&10;3,>&HÍ3+0%&H/6>&1EQ$43+"ÍC45´Ä&10%&"
KÏ nYì 
ot0%6>Ã"%$'&Ks)Ñ0%&1;,0%&H%H%576NH/?G3,RX&,I`"%$'&10%&]5:HQ'6RX64RX&1?4"Y6,oMH/68:l'"%576RX6H/"1@6,"%$'&10Y"%$43,"%$'&]E6  876H/"
C45:H/"%5:4RX"%576@M3,4C[ot6,0>6H/";3,>&Ha57"5:H5:'ot&3,H%57iG87&"/6#H/&3+02R2$Tot6,03RX6>?G87&1"/&H/68:l'"%576@M023+"%$'&10
"%$'&H/&3+02R2$3,5:>aH"/6a5:>?40%6,&B"%$'&B3,R1R1l'023,RX~)6,oÍ3$'&l'025:H/"%5:RQ&H/"%5:>a3+"/&B6,oA"%$'&l4H/&1o}l48:'&H%H6,o	3n>6,&
6,0]?6H%57"%576D5:#"%$'&;3,>& O JL$zl4H1@Í;3,>&XÑ"/0%&1&H/&3+02R2$'&Hn3+0%&)C'&1?4"%$^Ñi6l44C'&C@023+"%$'&10n"%$43,RX6H/"!Ñ
i6l44C'&C@3,4C.+3,8:l'&HP3+"B"%$'&a87&3+o'6^C'&HY6,o"%$'&"/0%&1&a3+0%&n;57,&.iz~N3ZH/"%3+"%5:R$'&l'025:H/"%5:Ro}l44RX"%576 O
s)Ñ0%&1;,0%&H%H%576]R13,Ki&Mot6,02>]l48:3+"/&C]5:"%$45:HE`3~,@,iz~B"%3+g^5:';Q"%$'&H%l4>æ6,oG3,R1R1l4>]l48:3+"/&C]3,4CK&H/"%5:>a3+"/&C
RX6H/"]3,HK"%$'&H/"%3+"%5:R$'&l'025:H/"%5:R]o}l44RX"%576 OÏ C'&1?4"%$^Ñi6l44C'&C.H/&3+02R2$DEQ57"%$#3=H/"%3,4C43+02C;3,>&XÑ"/0%&1&
H/&3+02R2$W3,87;,6,0257"%$4> q}9Í&3+028@Qw9â £-'ûQ98:3,3+"n&1"è3,8 O @Qwâ,â,ý|nR13,[i&Zl4H/&C"/6#5:>?40%6,&)"%$'&Z3,R1R1l'023,RX~
6,o"%$'&n&H/"%5:>a3+"/&C#RX6H/"B6,o"%$'&n0%6z6,"'6^C'& O JL$45:H1@$'6E&1,&10@Ao}3,5:8:HY"/6Z3,R2$457&1,&a"%$'&>a3,5:N6,i^í/&RX"%57,&

5}

î sû

À

4Á

4

6,o0%&8:3^&CH/&3+02R2$@AEQ$45:R2$5:HP"/6C45:H%RX6,&10q}3,4C#H/"/6,0%&|P5:>?40%6,&CRX6H/"&H/"%5:>a3+"/&HBot6,0"%$'&aH%571&ns
H/"%3+"/&HP&4RX6l4"/&10%&CC4l'025:';"%$'&H/&3+02R2$@H/6)"%$45:HQ>&1"%$'6^CNE6l48:CN$43,&n"/6)i&]l4H/&CN5:N3C45´Ä&10%&"
E`3~,@c Åc:@3,HP3C'&1?4"%$^Ñi6l44C'&C876z6,gÑF3,$'&3,C="/6)5:>?40%6,&K"%$'&]3,R1R1l'023,RX~6,oM$'&l'025:H/"%5:RB&1+3,8:l43+"%5764H
6,oMH/"%3+"/&HP5:Z"%$'&]'6,02>a3,80%&1;,0%&H%H%576=H/&3+02R2$ O ãP"%$'&K6,"%$'&10P$43,4C@GH%5:4RX&"%$'&]5:>?40%6,&C=$'&l'025:H/"%5:R
+3,8:l'&j5:H5:]"%$45:H>6^C'&`6,ol4H/&j'6,"MH/"/6,0%&CiGl'"	6487~Kl4H/&Cnot6,0M"%$'&jH/"%3+"/&Lot0%6> EQ$45:R2$]"%$'&L876z6,gÑF3,$'&3,C
H/&3+02R2$6,0257;5:43+"/&H1@57"P5:HP'6,"B'&RX&H%H%3+0%~N"/6ZH%5:>?G8:57ot~)"%$'&ns)Ñ0%&1;,0%&H%H%576.H/?G3,RX&,@A3,4C.3?6,"/&"%5:3,8:87~
>6,0%&?6E&10%o}l480%&8:3'3+"%576R13,i&l4H/&C O

í	 ¹  ¾Q·F(º+  º8î ¶ F B ¼!ºAFSï
JL$'&j"FE6K?G8:3,4'&102H		&"/&10%&C5:"%$'&Y(+*,*+-K!"/&10243+"%57643,8498:3,445:';K<6>?&1"%57"%576@JL9M-K3,4C }	- @
3+0%&P,&10%~H%5:>a5:8:3+0IA"%$'&Y6487~C45´Ä&10%&4RX&P5:H"%$43+" }	- 5:,&H/"%HjH/6>&P&XÄ6,0%"j5:"/6nRX6>?Gl'"%5:';n3n>6,0%&
3,R1R1l'023+"/&j$'&l'025:H/"%5:R+@"%$'0%6l';$]3YH/&10257&HA6,o4H/&3+02R2$'&H5:0%&8:3^&CK0%&1;,0%&H%H%576]H/?G3,RX&Hjqt"%$'&s)Ñ0%&1;,0%&H%H%576
H/?G3,RX&H|YEQ$45:R2$3+0%&C'&10257,&C.ot0%6>Ø"%$'&aH%3,>&a0%&8:3'3+"%576#3,HB"%$'&m $ $'&l'025:H/"%5:R]l4H/&Ciz~.JL9M- O !^Ñ
C'&1&C@"%$'&]>6,"%57+3+"%576=ot6,0Y&"/&1025:';i6,"%$=?G8:3,4'&102HjE`3,HY"/6)>a3+g,&nl4H/&K6,oM"%$'&]RX6>?&1"%57"%576N3,HY3,
&X^?&1025:>&"%3,8	&1+3,8:l43+"%576#6,o"%$'&a0%&8:3^&C#H/&3+02R2$D"/&R2$445:zl'&,@Í3,HE&8:83,HKRX6>?G3+025:';Zi6,"%$?G8:3,^Ñ
'&102HQ"/66,"%$'&10BH/"%3+"/&XÑ6,ofÑ"%$'&XÑF3+0%"B6,?4"%5:>a3,8A"/&>?6,023,8?G8:3,4'&102H OQÌ 6,0BH/&1,&1023,8Í0%&3,H/64HY$'6E&1,&10@"%$'&
RX6>?&1"%57"%576Z0%&H%l487"%HLC45:CZ'6,"Q?40%6^5:C'&B"%$'&KRX6>?G87&1"/&K?G5:RX"%l'0%&B6,o	"%$'&K0%&8:3+"%576Zi&1"FE&1& }	- 3,4C
JL9M- O
Ï HPC'&>64H/"/023+"/&C$'&10%&,@ }	- R13,N?40%6^C4l4RX&]i&1"/"/&10P0%&H%l487"%HQ"%$43,.JL9M-)5:NH/6>&n6,o"%$'&nRX6>nÑ
?&1"%57"%576TC'6>a3,5:4H1@M"%$'6l';$W5:T'6#C'6>a3,5:TC'6z&H }	- RX6>?G87&1"/&87~C'6>a5:43+"/&Z6,&10)JL9M- O h"a5:H
>a3,5:487~=6#?40%6,iG87&>aHB"%$43+"]3+0%&$43+02C@Aot6,0Ki6,"%$#?G8:3,4'&102H1@"%$43+""%$'&$'&l'025:H/"%5:Rn5:>?40%6,&>&"K0%&XÑ
H%l487"%5:';jot0%6>r0%&8:3^&CKH/&3+02R2$]~^57&8:C4H3,]3,C'+3,"%3+;,& O !]H/6>&C'6>a3,5:4H1@"%$'&5:>?40%6,&>&"A6,&10"%$'&
m $ $'&l'025:H/"%5:RL5:H`'6,"`&'6l';$)"/6RX6>?&4H%3+"/&Bot6,0`"%$'&P"%5:>&BH/?&"`RX6>?Gl'"%5:';n57" OMÏ >6,0%&BC'&1"%3,5:87&C
3,43,87~^H%5:H	0%&H%l487"/&C5:a3qtE&3+g4|R2$43+023,RX"/&102573+"%576a6,o"%$'&QC'6>a3,5:4H5:nEQ$45:R2$0%&8:3^&CaH/&3+02R2$R13,ai&
&X^?&RX"/&C."/6Zi&RX6H/"B&XÄ&RX"%57,&,IB5:.H%l4R2$C'6>a3,5:4H1@&X^?G3,4C45:';H%>a3,8:8	H/"%3+"/&H5:HPRX6>?Gl'"%3+"%57643,8:87~
R2$'&3+?&10P"%$43,=&X^?G3,4C45:';8:3+0%;,&nH/"%3+"/&H1@3,4CNH%>a3,8:8AH/"%3+"/&HP"/&4C="/6)$43,&H%>a3,8:8AH%l4R1RX&H%H/6,0PH/"%3+"/&H O
!"%$'&C'6>a3,5:4HBEQ$'&10%&a57"5:HB"/6z6=&X^?&4H%57,&,@6#"%$'&a6,"%$'&10]$43,4C@"%$'&RX&"/023,8M?40%6,iG87&>Â5:HP"%$43+"
"%$'&Li4023,4R2$45:';Bo}3,RX"/6,06,os)Ñ0%&1;,0%&H%H%5765:H`Ý7Å+'Xd"%$43,a"%$43+"M6,o'6,02>a3,8'0%&1;,0%&H%H%576aH/&3+02R2$@^H/6"%$43+"
H/&3+02R2$45:';#5:"%$'&Z0%&8:3^&CTH/?G3,RX&=5:HnRX6>?Gl'"%3+"%57643,8:87~kÓaÒ+d%&X^?&4H%57,&#q}zl457"/&RX6"/023+0%~"/6#"%$'&
5:C'&3)6,o6,i4"%3,5:45:';Z3$'&l'025:H/"%5:RK&H/"%5:>a3+"/&aiz~.H/687^5:';Z3WÕ/H%5:>?G8:5´G&C4Öa?40%6,iG87&>| O JjE6=>&3,H%l'0%&H1@
¥!Ý ðc"%$'&Ki4023,4R2$45:';no}3,RX"/6,0Y6,oãP_jÑF'6^C'&HL5:Z0%&8:3^&CH/&3+02R2$=0%&8:3+"%57,&"/6i4023,4R2$45:';o}3,RX"/6,0P5:Z"%$'&
6,0257;5:43,8H/&3+02R2$NH/?G3,RX&]3,4C"%$'&K0%&8:3+"%57,&]H%571&6,o"%$'&KH%l4R1RX&H%H/6,02HQ6,oãP_jÑF'6^C'&HQ5:Z0%&8:3^&C=H/&3+02R2$@
EQ$'&10%&`ot6l44CK"/6Bi&;,6z6^C5:4C45:R13+"/6,02HÍ6,o$'6ETE&8:8 }	- ?&10%ot6,02>&C@,RX6>?G3+0%&Cn"/6BJL9M-'@5:n3Y;57,&
C'6>a3,5: O !a"%$'&Q&X^?&1025:>&"%H1@"%$'&H/&P>&3,H%l'0%&HE&10%&Q"%3+g,&)iz~&X^?G876,025:';B"%$'&YH/&3+02R2$)H/?G3,RX&H1@^iGl'"
57">a3~3,8:H/6i&L?6H%H%57iG87&`"/6]&H/"%5:>a3+"/&Q"%$'&>vq}57o'6,""/6]R13,8:R1l48:3+"/&Y"%$'&>à&X'3,RX"%87~4|ot0%6>à"%$'&YC'6>a3,5:
C'&H%RX0257?4"%576 O
JL$'&B3,43,87~^H%5:H`6,oA"%$'&C'6>a3,5:4Hj5:EQ$45:R2$)0%&8:3^&CZH/&3+02R2$Zo}3,5:8:H`"/6i&P&XÄ&RX"%57,&K3,8:H/6?65:"%H`6l'"
?6H%H%57iG5:8:57"%57&Hot6,0P5:>?40%6,&>&"1@3,4CNH/&1,&1023,8	5:C'&3,HQot6,0P?6,"/&"%5:3,8Í5:>?40%6,&>&"%HQ"/6)"%$'&]>&1"%$'6^C
R13,Ui&Not6l44CS5:V0%&8:3+"/&CÃÕ/5:4RX0%&>&"%3,8:Ö[H/&3+02R2$æH%R2$'&>&H5:V"%$'&8:57"/&1023+"%l'0%& O JL$'&H/&5:4R18:l4C'&
8:5:>a57"%5:';ZH/&3+02R2$"/6.3NH%>a3,8:87&10Kot023,RX"%576D6,oj"%$'&0%&8:3^&CH/?G3,RX&,@Ml4H%5:';=RX6>5:RX"%HK"/6.C4570%&RX"KH/&3+02R2$
"/6a"%$'&KH/"%3+"/&HY>6,0%&K8:57g,&87~"/6i&$43,&K"%$'&570Q$'&l'025:H/"%5:RY+3,8:l'&HL5:>?40%6,&C@43,4C3,87"/&10243+"%57,&KH/&3+02R2$
3,87;,6,0257"%$4>aH`ot6,0 K
Ï nY
ì  ãP_þ;,023+?G$4H O JL$45:H`5:Hj6'&C4570%&RX"%576)ot6,0Lo}l'"%l'0%&C'&1,&876,?G>&"%H O
Ì 5:43,8:87~,@0%&H%l487"%HMR187&3+0287~nC'&>64H/"/023+"/&Q"%$43+"3,87"%$'6l';$5:>?40%6^5:';P"%$'&Y$'&l'025:H/"%5:R`5:>?40%6,&HM?&10/Ñ
ot6,02>a3,4RX&=5:[H/6>&=C'6>a3,5:4H1@3,876'&57"a5:H'6,"&'6l';$T"/6D3,R2$457&1,&=;,6z6^CT?&10%ot6,02>a3,4RX&)0%&8:5:3+iG87~

5}
î

î Á(ï^ðzñ7òGó À

4ï %ô :õ Yö 4ï^ð-Gó-N÷æ s4ø- ù ¦Gï^õ¶

  1   f

3,RX0%6H%HP3,8:8"%$'&nRX6>?&1"%57"%576=C'6>a3,5:4H O <`9JqtßP5:C43,8ÍÇÈY&XÄ'&10@(+*,*+-z|2@"%$'&]6487~)6,"%$'&10P6,?4"%5:>a3,8
"/&>?6,023,8?G8:3,4'&10j"/6a?G3+0%"%5:R157?G3+"/&B5:)"%$'&KRX6>?&1"%57"%576@'3+?4?&3+02Hj"/6a~^57&8:Ci&1"/"/&10Q0%&H%l487"%Hj5:Z>6H/"
RX6>?&1"%57"%576ZC'6>a3,5:4HBqt"%$'6l';$'6?40%&R15:H/&BRX6>?G3+025:H/6Z$43,H`i&1&Z>a3,C'&| O <`9JB@'8:57g,&BJL9M-'@4l4H/&H
"%$'&]"/&>?6,023,8m $ $'&l'025:H/"%5:R+@GiGl'"Y?&10%ot6,02>aHQ3)'6^ÑFC4570%&RX"%57643,8AH/&3+02R2$@3,4C.l4H/&HPRX64H/"/023,5:"Y0%&1?'Ñ
0%&H/&"%3+"%5763,4CN?40%6,?G3+;3+"%576."/&R2$445:zl'&HY"/6Z5:'ot&10PRX6H/"Bi6l44C^5768:3+"%5764H O JL$'&n5:'&	=R157&4RX~6,o
C4570%&RX"%57643,8`H/&3+02R2$W5:"/&>?6,023,8?G8:3,445:';N$43,Hni&1&['6,"/&CTi&1ot6,0%&,I"%$zl4H<`9J3,4C }	- R13,
i&]H%3,5:C"/6)5:>?40%6,&K6="FE6ZC45´Ä&10%&"QE&3+g^'&H%H/&HY6,oMJL9M- O <6>KiG5:45:';"%$'&H/&]5:>?40%6,&>&"%HP5:H
3,'6,"%$'&10YR2$43,8:87&';,&Bot6,0L"%$'&o}l'"%l'0%& O

,ÎQkgt¤ñÛo¶|ksq
e&ò RX"/6,0ÈY&XÄ'&10n$43,HK$43,CD3,D5:>?6,0%"%3,"?G3+0%"K5:"%$'&C'&1,&876,?G>&"K6,oj0%&8:3^&CDH/&3+02R2$H%5:4RX&57"%H
5:4RX&1?4"%576=3,4C"%$'0%6l';$=zl4>&10%6l4HQC45:H%R1l4H%H%5764H O ü,643,HÒB
 +3+024H/"/0A6ã >Z@9Í&10 n ~ziG876>Z@ü Ï !_x3,H%H/6+Ñ
R15:3+"/&Y&C457"/6,0ÃN
 3+025:3 Ì 63,4C"%$'&Y3,'6~^>6l4HM0%&1^57&1E&102H3,8:84?40%6^5:C'&C,&10%~a$'&87?4o}l48'RX6>a>&"%H6
i

C'023+ot"%Hj6,oÍ"%$'&?G3+?&10 OMÏ 8:8o}3,l487"%Hj3+0%&6,o	RX6l'02H/&K>a5:'& O

¶ Fó1F¼F  ¾hF-º

Ï $'6'@ K
Ï O @MQ6,?RX0%6,ot"1@Mü O @ÇYñY8:8:>a3,@ü O q!wâ9£,ç| OCô ,eÑõe}dè4e'è^d%=+ëÑöY ÅÒ+dÝ}etÓ] O[Ï C4C45:H/6^Ñ
á&H%87&1~ O
Æ&8:8:>a3,@4_ O q!wâ,Ê Ð,| O'ô Þ+ë+ÓnÝÉ`d/Ò%Å,d/+ÓnÓnÝtëzÅ O 9M025:4RX&1"/6ñY457,&102H%57"F~)9M0%&H%H O
Æ6'&1"1@Æ O @ÇÈY&XÄ'&10@ O q(+*,*+-z| OKÏ N3,87;,6,0257"%$4>äi&1"/"/&10P"%$43, Ï ã  ½ O !÷É`d/Òc(øhù+etsúK,e}ÝÒ+ë+
û Ò+ë üXd%XëÒ+s
ë öÒýþjöÒöÒöÒýhÿ ù@'?4? O wç-ç]Ü.wç%- Ð O
<`3+1&43,&,@+J O q(+*,*'w| O h"/&1023+"%57,&EQ5:C'&45:'; O !ÊÉ`d/Òc et
 ý2ëGeFXdë,e}ÝÒ+ë+ û Ò+ë üXd%XëjÒ+
ë öYdXe}Ý 1Ý+
ý2ëGeFXt Ý7ÅzXëK
 þjý
	 û öÒýhÿ ù@4?4? O Ê,(,ç]ÜZÊ,9( £ O
<`l487i&102H/6@ü O @LÇè\^R2$43+&XÄ&10@Lü O q!wâ,â,ý| O \z&3+02R2$45:';EQ57"%$k?G3+"/"/&102UC43+"%3+iG3,H/&H O ! û +ë+Ý+ë
û Ò+ë üXd%XëÒ+s
ë öÒý2@'ß68 O w1* £^w6,
o Lú û õG@'?4? O -*(]Ü-4wý O \z?4025:';,&10 O
?@\ O q(+*,*'w| O 98:3,445:';nEQ57"%$Z?G3+"/"/&102=C43+"%3+iG3,H/&H O !±É`d/Ò
c +et
 Eè^d/Ò2Ô4%+ë û Ò+ë üXd%Xë
+ C'&87g+3,Ò+>s
ë É`7+ë4ë4ÝtëzÑ
Å þ û Énÿ ù@4?4? O wçKÜZ(- O
6+Ä>a3,4@jü O q(+*,*+-z| O 9 ìPì åÍ( O (zI`"%$'&N8:3,';l43+;,&Not6,0"%$'&NR18:3,H%H%5:R13,8L?G3+0%"6,o
+ C'&87g+3,!9`><?Ñ@`- \ O @`!ÇÂ4Qet÷

ý2ëGeFXdë,e}ÝÒ+ë+¸ É`7+ë4ë4ÝtëzÅ û Ò+ÓQÔ41e}Ý}e}ÝÒ+
ë YÒÒ ¤:1e}@	?4? O ()Ü#ý O=Ï +3,5:8:3+iG87&a3+"
Ë¡%Oh~ªt%ª>ª
 h¾¢Q¾
Sª¾19¢h O
Ì &8:'&10@ ÏKO @ B6,0%oh@4_ O @ÇàY3,43,@\ O q(+*,*+-z| OjÏ C4C457"%57,&P?G3+"/"/&102=C43+"%3+iG3,H/&K$'&l'025:H/"%5:R1H O 	'Ò è^dë+	1Ò ü
öÒý"!L%+d/XÅ@ øø@( Ð+âKÜZç^5w £ O
@  O @MÇvå6';'@ ì]O q(+*,*ç| O 9 ìPì åÍ( O w+I Ï &X^"/&4H%576["/69 ìPì åUot6,0&X^?40%&H%H%5:';N"/&>?6,023,8
Ì 6¸
?G8:3,445:';nC'6>a3,5:4H O 	'Ò è^dë+	1Ò ü6öÒý"!L%+d/XÅ
@ øhù@ý^wPÜNw(- O
ÈP3+0%025:C'6'@ ÏKO @ Ì 6È@  O @GÇ å6';'@ ì]O q(+*,*(| OÏ "/&>?6,023,8?G8:3,445:';]H/~^H/"/&>Îot6,0QC4l'023+"%57,&B3,RX"%5764H
6,o9 ìPì åÍ( O w O !¨É`d/Ò#1Ò üNet'#
 ,et$
 Eè^d%%Ò2Ô4%+ë û Ò+ë üXd%XëÒ+C
ë öYdXe}Ý 1Ý+#
 ý2ëGeFXt Ý7ÅzXë %
þ û öÒýhÿ ùAø&@4?4? O 9Ê £,ý]Ü)Ê,â+* O
ÈP3+0%025:C'6'@ ÏKO @ãP43,5:4C45:3^@ +LO @ÇàÆ`3+0%i&10@ ÌO q(+*,*'w| O JL5:>&XÑ6,?4"%5:>a3,8?G8:3,445:';n5:Z"/&>?6,023,8?40%6,i'Ñ
87&>aH O !±É`d/Ò
c +et
 Eè^d/Ò2Ô4%+ë û Ò+ë üXd%XëÒ+s
ë É`7+ë4ë4ÝtëzÑ
Å þ û Énÿ ù@4?4? O ç,â ÐÜ-*( O

5}
î

À

4Á

4

ÈP3,H%R2$4457;'@ü O q!wâÐ+â| OTÏ ?40%6,iG87&>úH%5:>a5:8:3+0257"F~3+?4?40%63,R2$T"/6#C'&1^5:H%5:';N$'&l'025:H/"%5:R1H1I Ì 5702H/"n0%&H%l487"%H O
!É`d/Òc+et_ý2ëGeFXdë,e}ÝÒ+ë+'	'Ò+ÝtëGe û Ò+ëüXd%XëKÒ+ëöYdXe}Ý 1Ý+ý2ëGeFXt Ý7ÅzXënþjý
	 û öÒýhÿ )(@^?4? O
ç+*'wPÜZç+3* Ð O
ÈP5:4H/i&10%;'È@  O @ÇY3+0%,&1~,@á O q!wâ,â,(| O h"/&1023+"%57,&i40%63,C'&45:'; O öYdXe}Ý 1Ý+¦ ý2ëGeFXt Ý7ÅzXëX@*Íq(Ñhç|2@
ç,ý ÐKÜZ9ç £,ç O
Y3,H%8:l4>Z@9 O q(+*,*+-3| O !>?40%6^5:';K$'&l'025:H/"%5:R1H	"%$'0%6l';$H/&3+02R2$ O !IÉ`d/Ò+c Eè^d/Ò2Ô4%+ë û Ò+ë üXd%XëBÒ+ë
öÒýþ û öÒýhÿ ù,-@4?4? O w1*ç^wPÜ.w1*ç,( O
Y3,H%8:l4>Z@z9 O q(+*,*+-,i| O JL9M-'é *+-a3,4C }	-ü O !4et_
 ý2ëGeFXdë,e}ÝÒ+ë+S É`7+ë4ë4ÝtëzÅ û Ò+ÓQÔ41e}Ý}e}ÝÒ+.
ë YÒÒ ¤:1e}@
?4? O 9ç £KÜ-* OÏ +3,5:8:3+iG87&B3+#
" Ë¡%h~ªt%ª>ª
 h¾¢Q¾
ª,¾19¢h O
Y3,H%8:l4>Z@B9 O @Æ6'&1"1@KÆ O @]Ç ÈY&XÄ'&10@ O q(+*,*Ê| O n &1E×3,C4>a5:H%H%57iG87&$'&l'025:H/"%5:R1H=ot6,0DC'6>a3,5:^Ñ
5:4C'&1?&4C'&"?G8:3,445:'; O !IÉ`d/Ò¦c øhù+et
 úK,e}ÝÒ+ë+ û Ò+ë üXd%XëÒ+I
ë öÒýþjöÒöÒöÒýhÿ ù@?4? O w,wý,ç
Ü.w,w9ý £ O
Y3,H%8:l4>Z@a9 O @)Ç ÈY&XÄ'&10@ O q(+*,*,*| O Ï C4>a5:H%H%57iG87&S$'&l'025:H/"%5:R1H[ot6,0k6,?4"%5:>a3,8Z?G8:3,445:'; O !
É`d/Ò/c ,et8
 ý2ëGeFXdë,e}ÝÒ+ë+ û Ò+ë üXd%Xë=Ò+ë öYdXe}Ý 1Ý+(
 ý2ëGeFXt Ý7ÅzXën
 É`7+ë4ë4ÝtëzÅ+ë±
 õX'% è^ ê
ÝtëzÑ
Å þjöÒý:ÉÃõ#ÿ ùù
@4?4? O wX-*nÜ.wX-â OÏYÏYÏ `9M0%&H%H O
Y3,H%8:l4>Z@z9 O @4Ç ÈY&XÄ'&10@^ O q(+*,*'w| O Q&l'025:H/"%5:RL?G8:3,445:';BEQ57"%$"%5:>&P3,4C0%&H/6l'02RX&H O !É`d/Ò
c +et
Eè^d/Ò2Ô4%+ë û Ò+ë üXd%XënÒ+±
ë É`7+ë4ë4Ýtëz±
Å þ û Énÿ ù@G?4? O w(^wPÜ.wç,( O
Q&10243,C' 6ã 87;,~^5@	 O @ÇvQ687"/&,@_ O q(+*,*,*| N
O + ^?&1025:>&"%H]EQ57"%$T3,l'"/6>a3+"%5:R13,8:87~RX0%&3+"/&Ck>&>6,0%~Ñ
iG3,H/&CV$'&l'025:H/"%5:R1H O !¢ý2ë É`d/ÒÒ
c öyXe}d/e}ÝÒ+ë %0!Lj ü1Ò+d[
Ó è^7,e}ÝÒ+ë %+ëG
 öÔÔd/Ò,ÝtÓa,e}ÝÒ+ë %04et
ý2ëGeFXdë,e}ÝÒ+ë+L õGÞ+ÓQÔ'ÒÍÝ è^Ó þõQö1!'öNøhùùù
@?4? O 9( £^wPÜZ(,â+* O
Q6+Ä>a3,4@'ü O @'Ç + C'&87g+3,>?@'\ O q(+*,*Ê| O JL$'&PR18:3,H%H%5:R13,8G?G3+0%"6,o!9`<Ñ-'I Ï 6,&10%^57&1E O JA63+?4?&3+0
5:)"%$'2
& 	'Ò è^dë+	1Ò üÒöÒý1!L%+d/Xqt"%$45:HY\z?&R15:3,8JA023,R%g4| O
Q6+Ä>a3,4@`ü O @ + C'&87g+3,>?@`\ O @ + ';87&10%"1@_ O @åA57?6,023,RX&,@ ÌO JL$4;5 &1ò iG3,l^@`\ O @`ÇÂJA0 l'ã ;'@`\ O q(+*,*+-z| O
JA6E`3+02C4HL0%&3,8:5:H/"%5:RQi&4R2$4>a3+0%g^H`ot6,0L?G8:3,445:';'IzJL$'&C'6>a3,5:4H`l4H/&CZ5:)"%$'&R18:3,H%H%5:R13,8?G3+0%"j6,o
!9`<Ñ- O !#4ets
 ý2ëGeFXdë,e}ÝÒ+ë+Ô
 É`7+ë4ë4ÝtëzÅ û Ò+ÓQÔ41e}Ý}e}ÝÒ+
ë YÒÒ ¤:1e}@?4? O ÐÜDwX- OnÏ +3,5:8:3+iG87&]3+"
Ë¡%h~ªt%ª>ª
 h¾¢Q¾
Sª¾19¢h O
Q687"/&,@_ O @9Í&10%&1,Ã
@  O Æ O
@ R5:>a>&10@M_ O  O @`Ç N3,R ì 643,8:C@ ÏKO ü O q!wâ,â,ý| O Y57&1023+02R2$45:R13,8 
Ï  I
\z&3+02R2$45:';#3+iGH/"/023,RX"%576T$457&1023+02R2$457&Hn	& =R157&"%87~ O ! É`d/Ò3
c 4etÚ
 úK,e}ÝÒ+ë+ û Ò+ë üXd%XëNÒ+ë
öYdXe}Ý 1Ý+S ý2ëGeFXt Ý7ÅzXë_
 þjöÒöÒöÒýhÿ (5
@4?4? O Ê,ç+*nÜZÊ,ç,Ê O
ül4';$43,44H1@ ÏKO @jÇú\^R2$43+&XÄ&10@jü O q(+*,*'w| O \z6,g,6,iG3,I + 4$43,4R15:';;,&'&1023,8YH%5:';87&XÑF3+;,&")H/&3+02R2$
>&1"%$'6^C4HLl4H%5:';C'6>a3,5:)g^'6EQ87&C';,& O öYdXe}Ý 1Ý+È ý2ëGeFXt Ý7ÅzXëX6
@ 
ø(Íq!wÑh(|2@(^wâ]Ü)(,Ê^w O
B6,0%oh@'_ O q!w9â £,Ê| Oì &1?4"%$^ÑG02H/"j57"/&1023+"%57,&XÑFC'&1&1?&45:';'I Ï )6,?4"%5:>a3,83,C4>a5:H%H%57iG87&L"/0%&1&H/&3+02R2$ O öYdXe}Ý Mê
1Ý+S ý2ëGeFXt Ý7ÅzXëX¦
@ ø7q!w|2@â ÐÜ.w1*â O
B6,0%oh@_ O q!wâ,â,â| OÏ 0%"%5´R15:3,8	5:"/&8:8:57;,&4RX&H/&3+02R2$D3,87;,6,0257"%$4>aH O !8K+ë3 y%ÒÒ ¤.1Ò üÊöY ÅÒ+dÝ}etÓ]+ë
Ô è'e,e}ÝÒ+ë4@R2$43+? O ç,ý O <`_L<S9M0%&H%H O
9 '%Ò+dÞZ1Ò ü û Ò+ÓQS
åA5:l,@ : O A@ B6z&457;'@\ O @+Ç Ì l'02RX~,@ ì]O q(+*,*(| O \z?&1&C45:';Ll'?"%$'&R13,8:R1l48:3+"%576B6,o4$'&l'025:H/"%5:R1Hot6,0Í$'&l'025:H/"%5:R
H/&3+02R2$^ÑiG3,H/&C?G8:3,445:'; O ! É`d/Ò1
c ;et÷
 úK,e}ÝÒ+ë+ û Ò+ë üXd%XëÒ+Ú
ë öYdXe}Ý 1Ý+Ô
 ý2ëGeFXt Ý7ÅzXë
þjöÒöÒöÒýhÿ ùAø&@4?4? O 3- £-Ü-â^w O
å6';'@ ì]O @AÇ Ì 6L
@  O q(+*,*ç| [
O + ^?G87657"%5:';3;,023+?G$'?G8:3,=ot023,>&1E6,0%gN5:="/&>?6,023,8Í?G8:3,445:'; O !
4et÷
 ý2ëGeFXdë,e}ÝÒ+ë+ û Ò+ë üXd%XëÒ+Ú
ë öKè'eÒ+Óa,eF%
 É`7+ë4ë4ÝtëzÅ#+ë±
 õX'% è^ ÝtëzÅ þjý û öÒÉÃõ#ÿ ù-4,@
?4? O Ê^wYÜZý,( O

5}~}
î

î Á(ï^ðzñ7òGó À

4ï %ô :õ Yö 4ï^ð-Gó-N÷æ s4ø- ù ¦Gï^õ¶

  1   f

N3+0%"/&8:8:5@ ÏKO @ÇC=6"%3,43+025@ñ O q!wâÐ+ç| OÏ C4C457"%57,& ÏKnYì ãP_W;,023+?G$4H O !nÉ`d/Òc+4,d/ý2ëGeFXdë,e}ÝÒ+ë+
	'Ò+ÝtëGe û Ò+ëü Xd%XëÒ+ësY
ö dXe}Ý 1Ý+Èý2ëGeFXt Ý7ÅzXëKþjý
	 û öÒýhÿ <4,@4?4? O wQÜ.w,w O
n 5:8:H%H/6H/@68:l'Kn "%O576ü 4HO q!w!â,±Éý9£ `| d/O Ò\zcL&3+ý =O02R2ý:$4É 5:';û )Ò+?4ëz0%Å,6,d%iG287&>n@G?4ÑFH/? 687^w5:(,';Ê]Ü.3,4Cwç+N* ;3,>&XÑ?G8:3~^5:';)"/0%&1&HPot6,0B>a5:45:>a3,8ARX6H/"
O
O
O
9Í&3+028@Mü O q!wâ9£-z| O 8]	è^dÝfXe}ÝX&>Ãý2ëGeFXt Ý7ÅzXëGeÒõ%+d/XÚõe}d/,eFÅ,Ý6ü1Ò+d û Ò+ÓQÔSè'eFXd_É`d/Ò3yX:XÓ õÒ+ ¥ÝtëzÅ O

Ï 4C C45:H/6^Ñá&H%87&1~ O
98:3,3+"1@ ÏKO @+\^R2$43+&XÄ&10@+ü O @95 í%8:H1@1á O @Ç ÏKO @C O q!wâ,â,ý| O Æ&H/"!ÑG02H/"4^&C^ÑFC'&1?4"%$>a5:45:>a3P3,87;,6,0257"%$4>aH O
öYdXe}Ý 1Ý+S ý2ëGeFXt Ý7ÅzXëX@'; q!wÑh(|2@(,Ê,Ê]ÜZ(,â,ç O
9M0257&C457"%5:H1@ ÏKO+LO q!wâ,â,ç| O N3,R2$45:'&LC45:H%RX6,&10%~]6,oG&XÄ&RX"%57,&Q3,C4>a5:H%H%57iG87&$'&l'025:H/"%5:R1H O7? XÝtëA%+dë4ê
ÝtëzÅ,6
@ 
ø@Aw,~w ÐKÜ.wX-4w O
_L&5:'ot&8:C@ ÏKO @G
Ç N3+02H%8:3,4C@^J O q!wâ,â-z| 
O + 4$43,4RX&C)57"/&1023+"%57,&XÑFC'&1&1?&45:';nH/&3+02R2$ O ý 9 d/+ë'11ê
e}ÝÒ+ë'nÒ+s
ë ÉL,eeFXds
ë öYë+ ÞÝf]+ë ? XÝtë6
 ý2ëGeFXt Ý7ÅzXëX6
@ 5A*q Ð,|2"@ Ð*'wY
Ü Ðzw1* O
\^>a57"%$@ ì]O q(+*,*+-z| O <`$'6z6H%5:';6,i^í/&RX"%57,&H5:[6,&10/ÑFH%l'iGH%RX0257?4"%576[?G8:3,445:'; O !CÉ`d/Ò0
c &4etÚ
 ý2ë4ê
eFXdë,e}ÝÒ+ë+ û Ò+ë üXd%XëaÒ+±
ë öKè'eÒ+Óa,eF%_
 É`7+ë4ë4ÝtëzA
Å @õX'% è^ Ýtëz8
Å þjý û öÒÉÃõ#ÿ ù,-@G?4? O ç,â,çnÜ
-*'w O
\^>a57"%$@ ì]O @Çäá&8:C@ ì]O q!wâ,â,â| O JA&>?6,023,8?G8:3,445:';=EQ57"%$[>]l'"%l43,8&X'R18:l4H%5760%&3,H/645:'; O !
É`d/Ò"c 5+etÑ
 ý2ëGeFXdë,e}ÝÒ+ë+B
 	'Ò+ÝtëGe û Ò+ë üXd%XëZÒ+G
ë öYdXe}Ý 1Ý+O
 ý2ëGeFXt Ý7ÅzXë±
 þjý
	 û öÒýhÿ ((@A?4? O
ç,(,ý]ÜZç,ç,ç O
JA025:4zl43+0%"1@M_ O q(+*,*ç| OrÏ 43,87~z5:';0%&3,R2$43+iG5:8:57"F~[EQ57"%$45:[?G8:3,WH/?G3,RX& O !¨ý û öÒÉÃõ#ÿ ù-4 ô ÒeÒ+d/+
û Ò+ë'1Ò+dXe}ÍÝ è^Ón@?4? O w(,(]Ü.w(,ý O
ßM3,87"/6,0%"%3^O
@  O q!w9â £-z| OÏ 0%&H%l487"P6#"%$'&RX6>?Gl'"%3+"%57643,8MRX6>?G87&X'57"F~.6,oj$'&l'025:H/"%5:R]&H/"%5:>a3+"/&HKot6,0
"%$'& 
ë õ1ÝXëC
@ 4<'@43- £KÜZÊ,â O
Ï  3,87;,6,0257"%$4> O ý2ë ü1Ò+dÓa,e}ÝÒ+÷
ß&10%o}3,5:8:8:57&,@4È O @'å&>a3,57"/0%&,@  O @4Çà\^R2$457&X@'J O q!wâ,â,ý| O _Ql4H%H%5:3,C'68:8H/&3+02R2$)ot6,0LH/687^5:';nRX64H/"/023,5:"
6,?4"%5:>a573+"%576?40%6,iG87&>aH O !ºÉ`d/Ò/
c 4etG
 úK,e}ÝÒ+ë+ û Ò+ë üXd%Xë=Ò+ë öYdXe}Ý 2+¸
 ý2ëGeFXt Ý7ÅzXë
þjöÒöÒöÒýhÿ (5
@4?4? O 5w £^wPÜ.5w £Ð O
ßP5:C43,8@ß O @LÇ ÈY&XÄ'&10@L O q(+*,*+-z| O Æ023,4R2$45:';[3,4CS?402l445:';'I Ï S6,?4"%5:>a3,8Y"/&>?6,023,8P9`ã<`å
?G8:3,4'&10iG3,H/&CV6SRX64H/"/023,5:"?40%6,;,023,>a>a5:'; O !¢É`d/ÒD
c (,etº
 úK,e}ÝÒ+ë+ û Ò+ë üXd%Xë#Ò+ë
öYdXe}Ý 1Ý+S ý2ëGeFXt Ý7ÅzXë_
 þjöÒöÒöÒýhÿ ù,-@4?4? O Ê Ð*nÜZÊ Ð9Ð O

5}«
î

Journal of Artificial Intelligence Research 25 (2006) 457-502

Submitted 11/05; published 4/06

A Continuation Method for Nash Equilibria in Structured
Games
Ben Blum

bblum@cs.berkeley.edu

University of California, Berkeley
Department of Electrical Engineering and Computer Science
Berkeley, CA 94720

Christian R. Shelton

cshelton@cs.ucr.edu

University of California, Riverside
Department of Computer Science and Engineering
Riverside, CA 92521

Daphne Koller

koller@cs.stanford.edu

Stanford University
Department of Computer Science
Stanford, CA 94305

Abstract
Structured game representations have recently attracted interest as models for multiagent artificial intelligence scenarios, with rational behavior most commonly characterized
by Nash equilibria. This paper presents efficient, exact algorithms for computing Nash equilibria in structured game representations, including both graphical games and multi-agent
influence diagrams (MAIDs). The algorithms are derived from a continuation method for
normal-form and extensive-form games due to Govindan and Wilson; they follow a trajectory through a space of perturbed games and their equilibria, exploiting game structure
through fast computation of the Jacobian of the payoff function. They are theoretically
guaranteed to find at least one equilibrium of the game, and may find more. Our approach
provides the first efficient algorithm for computing exact equilibria in graphical games with
arbitrary topology, and the first algorithm to exploit fine-grained structural properties of
MAIDs. Experimental results are presented demonstrating the effectiveness of the algorithms and comparing them to predecessors. The running time of the graphical game
algorithm is similar to, and often better than, the running time of previous approximate
algorithms. The algorithm for MAIDs can effectively solve games that are much larger
than those solvable by previous methods.

1. Introduction
In attempting to reason about interactions between multiple agents, the artificial intelligence
community has recently developed an interest in game theory, a tool from economics. Game
theory is a very general mathematical formalism for the representation of complex multiagent scenarios, called games, in which agents choose actions and then receive payoffs that
depend on the outcome of the game. A number of new game representations have been
introduced in the past few years that exploit structure to represent games more efficiently.
These representations are inspired by graphical models for probabilistic reasoning from the
artificial intelligence literature, and include graphical games (Kearns, Littman, & Singh,
c
2006
AI Access Foundation. All rights reserved.

Blum, Shelton, & Koller

2001), multi-agent influence diagrams (MAIDs) (Koller & Milch, 2001), G nets (La Mura,
2000), and action-graph games (Bhat & Leyton-Brown, 2004).
Our goal is to describe rational behavior in a game. In game theory, a description of the
behavior of all agents in the game is referred to as a strategy profile: a joint assignment of
strategies to each agent. The most basic criterion to look for in a strategy profile is that it be
optimal for each agent, taken individually: no agent should be able to improve its utility by
changing its strategy. The fundamental game theoretic notion of a Nash equilibrium (Nash,
1951) satisfies this criterion precisely. A Nash equilibrium is a strategy profile in which
no agent can improve its payoff by deviating unilaterally — changing its strategy while all
other agents hold theirs fixed. There are other types of game theoretic solutions, but the
Nash equilibrium is the most fundamental and is often agreed to be a minimum solution
requirement.
Computing equilibria can be difficult for several reasons. First, game representations
themselves can grow quite large. However, many of the games that we would be interested
in solving do not require the full generality of description that leads to large representation
size. The structured game representations introduced in AI exploit structural properties
of games to represent them more compactly. Typically, this structure involves locality of
interaction — agents are only concerned with the behavior of a subset of other agents.
One would hope that more compact representations might lead to more efficient computation of equilibria than would be possible with standard game-theoretic solution algorithms
(such as those described by McKelvey & McLennan, 1996). Unfortunately, even with compact representations, games are quite hard to solve; we present a result showing that finding
Nash equilibria beyond a single trivial one is NP-hard in the types of structured games that
we consider.
In this paper, we describe a set of algorithms for computing equilibria in structured
games that perform quite well, empirically. Our algorithms are in the family of continuation
methods. They begin with a solution of a trivial perturbed game, then track this solution as
the perturbation is incrementally undone, following a trajectory through a space of equilibria
of perturbed games until an equilibrium of the original game is found. Our algorithms are
based on the recent work of Govindan and Wilson (2002, 2003, 2004) (GW hereafter),
which applies to standard game representations (normal-form and extensive-form). The
algorithms of GW are of great interest to the computational game theory community in
their own right; Nudelman et al. (2004) have tested them against other leading algorithms
and found them, in certain cases, to be the most effective available. However, as with
all other algorithms for unstructured games, they are infeasible for very large games. We
show how game structure can be exploited to perform the key computational step of the
algorithms of GW, and also give an alternative presentation of their work.
Our methods address both graphical games and MAIDs. Several recent papers have
presented methods for finding equilibria in graphical games. Many of the proposed algorithms (Kearns et al., 2001; Littman, Kearns, & Singh, 2002; Vickrey & Koller, 2002; Ortiz
& Kearns, 2003) have focused on finding approximate equilibria, in which each agent may
in fact have a small incentive to deviate. These sorts of algorithms can be problematic:
approximations must be crude for reasonable running times, and there is no guarantee of
an exact equilibrium in the neighborhood of an approximate one. Algorithms that find
exact equilibria have been restricted to a narrow class of games (Kearns et al., 2001). We
458

A Continuation Method for Nash Equilibria in Structured Games

present the first efficient algorithm for finding exact equilibria in graphical games of arbitrary structure. We present experimental results showing that the running time of our
algorithm is similar to, and often better than, the running time of previous approximate
algorithms. Moreover, our algorithm is capable of using approximate algorithms as starting
points for finding exact equilibria.
The literature for MAIDs is more limited. The algorithm of Koller and Milch (2001)
only takes advantage of certain coarse-grained structure in MAIDs, and otherwise falls
back on generating and solving standard extensive-form games. Methods for related types
of structured games (La Mura, 2000) are also limited to coarse-grained structure, and
are currently unimplemented. Approximate approaches for MAIDs (Vickrey, 2002) come
without implementation details or timing results. We provide the first exact algorithm that
can take advantage of the fine-grained structure of MAIDs. We present experimental results
demonstrating that our algorithm can solve MAIDs that are significantly outside the scope
of previous methods.
1.1 Outline and Guide to Background Material
Our results require background in several distinct areas, including game theory, continuation
methods, representations of graphical games, and representation and inference for Bayesian
networks. Clearly, it is outside the scope of this paper to provide a detailed review of all of
these topics. We have attempted to provide, for each of these topics, sufficient background
to allow our results to be understood.
We begin with an overview of game theory in Section 2, describing strategy representations and payoffs in both normal-form games (single-move games) and extensive-form
games (games with multiple moves through time). All concepts utilized in this paper will
be presented in this section, but a more thorough treatment is available in the standard
text by Fudenberg and Tirole (1991). In Section 3 we introduce the two structured game
representations addressed in this paper: graphical games (derived from normal-form games)
and MAIDs (derived from extensive-form games). In Section 4 we give a result on the complexity of computing equilibria in both graphical games and MAIDs, with the proof deferred
to Appendix B. We next outline continuation methods, the general scheme our algorithms
use to compute equilibria, in Section 5. Continuation methods form a broad computational
framework, and our presentation is therefore necessarily limited in scope; Watson (2000)
provides a more thorough grounding. In Section 6 we describe the particulars of applying
continuation methods to normal-form games and to extensive-form games. The presentation
is new, but the methods are exactly those of GW.
In Section 7, we present our main contribution: exploiting structure to perform the
algorithms of GW efficiently on both graphical games and MAIDs. We show how Bayesian
network inference in MAIDs can be used to perform the key computational step of the GW
algorithm efficiently, taking advantage of finer-grained structure than previously possible.
Our algorithm utilizes, as a subroutine, the clique tree inference algorithm for Bayesian
networks. Although we do not present the clique tree method in full, we describe the
properties of the method that allow it to be used within our algorithm; we also provide
enough detail to allow an implementation of our algorithm using a standard clique tree
package as a black box. For a more comprehensive introduction to inference in Bayesian
459

Blum, Shelton, & Koller

networks, we refer the reader to the reference by Cowell, Dawid, Lauritzen, and Spiegelhalter
(1999). In Section 8, we present running-time results for a variety of graphical games and
MAIDs. We conclude in Section 9.

2. Game Theory
We begin by briefly reviewing concepts from game theory used in this paper, referring to
the text by Fudenberg and Tirole (1991) for a good introduction. We use the notation
employed by GW. Those readers more familiar with game theory may wish to skip directly
to the table of notation in Appendix A.
A game defines an interaction between a set N = {n1 , n2 , . . . , n|N | } of agents. Each agent
n ∈ N has a set Σn of available strategies, where a strategy determines the agent’s behavior
in the game. The precise definition of the set Σn depends on the
Q game representation, as
we discuss below. A strategy profile σ = (σn1 , σn2 , . . . , σn|N | ) ∈ n∈N Σn defines a strategy
σn ∈ Σn for each agent n ∈ N . Given a strategy profile σ, the game defines an expected
payoff Gn (σ) for each agent n ∈ N . We use Σ−n to refer to the set of all strategy profiles
of agents in N \ {n} (agents other than n) and σ−n ∈ Σ−n to refer to one such profile; we
generalize this notation to Σ−n,n0 for the set of strategy profiles of all but two agents. If σ
is a strategy profile, and σn0 ∈ Σn is a strategy for agent n, then (σn0 , σ−n ) is a new strategy
profile in which n deviates from σ to play σn0 , and all other agents act according to σ.
A solution to a game is a prescription of a strategy profile for the agents. In this paper,
we use Nash equilibria as our solution concept — strategy profiles in which no agent can
profit by deviating unilaterally. If an agent knew that the others were playing according
to an equilibrium profile (and would not change their behavior), it would have no incentive
to deviate. Using the notation we have outlined here, we can define a Nash equilibrium
to be a strategy profile σ such that, for all n ∈ N and all other strategies σn0 ∈ Σn ,
Gn (σn , σ−n ) ≥ Gn (σn0 , σ−n ).
We can also define a notion of an approximate equilibrium, in which each agent’s incentive to deviate is small. An -equilibrium is a strategy profile σ such that no agent
can improve its expected payoff by more than  by unilaterally deviating from σ. In other
words, for all n ∈ N and all other strategies σn0 ∈ Σn , Gn (σn0 , σ−n ) − Gn (σn , σ−n ) ≤ .
Unfortunately, finding an -equilibrium is not necessarily a step toward finding an exact
equilibrium: the fact that σ is an -equilibrium does not guarantee the existence of an exact
equilibrium in the neighborhood of σ.
2.1 Normal-Form Games
A normal-form game defines a simultaneous-move multi-agent scenario. Each agent independently selects an action and then receives a payoff that depends on the actions selected
by all of the agents. More precisely, let G be a normal-form game with a set N of agents.
Each agent n ∈ N hasQ
a discrete action set An and a payoff array Gn with entries for every
action profile in A = n∈N An — that is, for joint actions a = (an1 , an2 , . . . , an|N | ) of all
agents. We use A−n to refer to the joint actions of agents in N \ {n}.
460

A Continuation Method for Nash Equilibria in Structured Games

2.1.1 Strategy Representation
If agents are restricted to choosing actions deterministically, an equilibrium is not guaranteed to exist. If, however, agents are allowed to independently randomize over actions, then
the seminal result of game theory (Nash, 1951) guarantees the existence of a mixed strategy
equilibrium. A mixed strategy σn is a probability distribution over An .
The strategy set Σn is therefore defined to be the probability simplex of all mixed
strategies. The support of a mixed strategy is the set of actions in An that have non-zero
probability. A strategy σn for agent n is said to be a pure strategy if it has only a single
action in its support — pure strategies correspond
exactly to the deterministic actions in
Q
An . The set Σ of mixed strategy profiles is n∈N Σn , a product of simplices. A mixed
strategy for a single agent can be represented as a vector of probabilities, one for each
action. For notational simplicity later on, we can concatenate allPthese vectors and regard
a mixed strategy profile σ ∈ Σ as a single m-vector, where m = n∈N |An |. The vector is
indexed by actions in ∪n∈N An , so for an action a ∈ An , σa is the probability that agent
n plays action a. (Note that, for notational convenience, every action is associated with a
particular agent; different agents cannot take the “same” action.)
2.1.2 Payoffs
A mixed strategy profile induces a joint distribution over action profiles, and we can compute
an expectation of payoffs with respect to this distribution. We let Gn (σ) represent the
expected payoff to agent n when all agents behave according to the strategy profile σ. We
can calculate this value by
X
Y
Gn (σ) =
Gn (a)
σ ak .
(1)
a∈A

k∈N

In the most general case (a fully mixed strategy profile, in which every ), this sum includes
every entry in the game array Gn , which is exponentially large in the number of agents.
2.2 Extensive-Form Games
An extensive-form game is represented by a tree. The game proceeds sequentially from
the root. Each non-leaf node in the tree corresponds to a choice either of an agent or of
nature; outgoing branches represent possible actions to be taken at the node. For each
of nature’s choice nodes, the game definition includes a probability distribution over the
outgoing branches (these are points in the game at which something happens randomly in
the world at large). Each leaf z ∈ Z of the tree is an outcome, and is associated with a
vector of payoffs G(z), where Gn (z) denotes the payoff to agent n at leaf z. The choices of
the agents and of nature dictate which path of the tree is followed.
The choice nodes belonging to each agent are partitioned into information sets; each
information set is a set of states among which the agent cannot distinguish. Thus, an agent’s
strategy must dictate the same behavior at all nodes in the same information set. The set
of agent n’s information sets is denoted In , and the set of actions available at information
set i ∈ In is denoted A(i). We define an agent history Hn (y) for a node y in the tree and
an agent n to be a sequence containing pairs (i, a) of the information sets belonging to n
traversed in the path from the root to y (excluding the information set in which y itself is
461

Blum, Shelton, & Koller

0.7

a2

0.8

0.7

b2

b1

0.6

a’1 a’2

(0, 2)

0.9

a1

0.3

b1

0.2

0.1

0.4

1.0

a’3 a’4

(1, −4)

(6, 7)

0.3

b2

0.0

0.5

a’5 a’6

(−6, 0)

(3, 3)

0.5

a’7 a’8

(1, 7)

(8, 0)

(−2, −6)

Figure 1: A simple 2-agent extensive-form game.
contained), and the action selected by n at each one. Since actions are unique to information
sets (the “same” action can’t be taken at two different information sets), we can also omit
the information sets and represent a history as an ordered tuple of actions only. Two nodes
have the same agent-n history if the paths used to reach them are indistinguishable to n,
although the paths may differ in other ways, such as nature’s decisions or the decisions of
other agents. We make the common assumption of perfect recall : an agent does not forget
information known nor choices made at its previous decisions. More precisely, if two nodes
y, y 0 are in the same information set for agent n, then Hn (y) = Hn (y 0 ).
Example 1. In the game tree shown in Figure 1, there are two agents, Alice and Bob. Alice
first chooses between actions a1 and a2 , Bob next chooses b1 or b2 , and then Alice chooses
between two of the set {a01 , a02 , . . . , a08 } (which pair depends on Bob’s choice). Information
sets are indicated by nodes connected with dashed lines. Bob is unaware of Alice’s actions,
so both of his nodes are in the same information set. Alice is aware at the bottom level of
both her initial action and Bob’s action, so each of her nodes is in a distinct information set.
Edges have been labeled with the probability that the agent whose action it is will follow it;
note that actions taken at nodes in the same information set must have the same probability
distribution associated with them. There are eight possible outcomes of the game, each
labeled with a pair of payoffs to Alice and Bob, respectively.

2.2.1 Strategy Representation
Unlike the case of normal-form games, there are several quite different choices of strategy
representation for extensive-form games. One convenient formulation is in terms of behavior
strategies. A behavior profile b assigns to each information set i a distribution over the
462

A Continuation Method for Nash Equilibria in Structured Games

actions a ∈ A(i). The probability that agent n takes action a at information set i ∈ In is
then written b(a|i). If y is a node in i, then we can also write b(a|y) as an abbreviation for
b(a|i).
Our methods primarily employ a variant of the sequence form representation (Koller &
Megiddo, 1992; von Stengel, 1996; Romanovskii, 1962), which is built upon the behavior
strategy representation. In sequence form, a strategy σn for an agent n is represented
as a realization plan, a vector of real values. Each value, or realization probability, in the
realization plan corresponds to a distinct history (or sequence) Hn (y) that agent n has, over
all nodes y in the game tree. Some of these sequences may only be partial records of n’s
behavior in the game — proper prefixes of larger sequences. The strategy representation
employed by GW (and by ourselves) is equivalent to the sequence form representation
restricted to terminal sequences: those which are agent-n histories of at least one leaf node.
We shall henceforth refer to this modified strategy representation simply as “sequence form,”
for the sake of simplicity.
For agent n, then, we consider a realization plan σn to be a vector of the realization
probabilities of terminal sequences. For an outcome z, σ(Hn (z)), abbreviated σn (z), is the
probability that agent n’s choices allow the realization of outcome z —
Q in other words,
the product of agent n’s behavior probabilities along the history Hn (z), (i,a)∈Hn (z) b(a|i).
Several different outcomes may be associated with the same terminal sequence, so that
agent n may have fewer realization probabilities than there are leaves in the tree. The set
of realization plans for agent n is therefore a subset of IR`n , where `n , the number of distinct
terminal sequences for agent n, is at most the number of leaves in the tree.
Example 2. In the example above, Alice has eight terminal sequences, one for each of
a01 , a02 , . . . , a08 from her four information sets at the bottom level. The history for one such
last action is (a1 , a03 ). The realization probability σ(a1 , a03 ) is equal to b(a1 )b(a03 |a1 , b2 ) =
0.1 · 0.6 = 0.06. Bob has only two last actions, whose realization probabilities are exactly his
behavior probabilities.
When all realization probabilities are non-zero, realization plans and behavior strategies
are in one-to-one correspondence. (When some probabilities are zero, many possible behavior strategy profiles might correspond to the same realization plan, as described by Koller
& Megiddo, 1992; this does not affect the work presented here.) From
Q a behavior strategy
profile b, we can easily calculate the realization probability σn (z) = (i,a)∈Hn (z) b(a|i). To
understand the reverse transformation, note that we can also map behavior strategies to
full realization plans defined on non-terminal sequences
(as they were originally defined
Q
by Koller & Megiddo, 1992) by defining σn (h) = (i,a)∈h b(a|i); intuitively, σn (h) is the
probability that agent n’s choices allow the realization of partial sequence h. Using this
observation, we can compute a behavior strategy from an extended realization plan: if (partial) sequence (h, a) extends sequence h by one action, namely action a at information set
i belonging to agent n, then we can compute b(a|i) = σσnn(h,a)
(h) . The extended realization
probabilities can be computed from the terminal realization probabilities by a recursive
procedure starting at the leaves of the tree and working upward: atPinformation set i with
agent-n history h (determined uniquely by perfect recall), σn (h) = a∈A(i) σn (h, a).
As several different information sets can have the same agent-n history h, σn (h) can
be computed in multiple ways. In order for a (terminal) realization plan to be valid, it
463

Blum, Shelton, & Koller

must satisfy the constraint that all choices of information sets with agent-n history h must
give rise to the same value of σn (h). More formally, for each partial sequence h, we have
the
P constraints that for
P all pairs of information sets i1 and i2 with Hn (i1 ) = Hn (i2 ) = h,
σ
(h,
a)
=
a∈A(i1 ) n
a∈A(i2 ) σn (h, a). In the game tree of Example 1, consider Alice’s
realization probability σA (a1 ). It can be expressed as either σA (a1 , a01 ) + σA (a1 , a02 ) =
0.1 · 0.2 + 0.1 · 0.8 or σA (a1 , a03 ) + σA (a1 , a04 ) = 0.1 · 0.6 + 0.1 · 0.4, so these two sums must
be the same.
By recursively defining each realization probability as a sum of realization probabilities for longer sequences, all constraints can be expressed in terms of terminal realization
probabilities; in fact, the constraints are linear in these probabilities. There are several
further constraints: all probabilities must be nonnegative, and, for each agent n, σn (∅) = 1,
where ∅ (the empty sequence) is the agent-n history of the first information set that agent n
encounters. This latter constraint simply enforces that probabilities sum to one. Together,
these linear constraints define a convex polytope Σ of legal terminal realization plans.
2.2.2 Payoffs
If all agents play according to σ ∈ Σ, the payoff to agent n in an extensive-form game is
X
Y
Gn (σ) =
Gn (z)
σk (z) ,
(2)
z∈Z

k∈N

where here we have augmented N to include nature for notational convenience. This is
simply an expected sum of the payoffs over all leaves. For each agent
Q k, σk (z) is the
product of the probabilities controlled by n along the path to z; thus, k∈N σk (z) is the
multiplication of all probabilities along the path to z, which is precisely the probability of
z occurring. Importantly, this expression has a similar multi-linear form to the payoff in a
normal-form game, using realization plans rather than mixed strategies.
Extensive-form games can be expressed (inefficiently) as normal-form games, so they
too are guaranteed to have an equilibrium in mixed strategies. In an extensive-form game
satisfying perfect recall, any mixed strategy profile can be represented by a payoff-equivalent
behavior profile, and hence by a realization plan (Kuhn, 1953).

3. Structured Game Representations
The artificial intelligence community has recently introduced structured representations
that exploit independence relations in games in order to represent them compactly. Our
methods address two of these representations: graphical games (Kearns et al., 2001), a
structured class of normal-form games, and MAIDs (Koller & Milch, 2001), a structured
class of extensive-form games.
3.1 Graphical Games
The size of the payoff arrays required to describe a normal-form game grows exponentially
with the number of agents. In order to avoid this blow-up, Kearns et al. (2001) introduced
the framework of graphical games, a more structured representation inspired by probabilistic graphical models. Graphical games capture local structure in multi-agent interactions,
464

A Continuation Method for Nash Equilibria in Structured Games

allowing a compact representation for scenarios in which each agent’s payoff is only affected
by a small subset of other agents. Examples of interactions where this structure occurs include agents that interact along organization hierarchies and agents that interact according
to geographic proximity.
A graphical game is similar in definition to a normal-form game, but the representation
is augmented by the inclusion of an interaction graph with a node for each agent. The
original definition assumed an undirected graph, but easily generalizes to directed graphs.
An edge from agent n0 to agent n in the graph indicates that agent n’s payoffs depend on
the action of agent n0 . More precisely, we define Famn to be the set of agents consisting
of n itself and its parents in the graph. Agent n’s payoff function Gn is an array indexed
only by the actions of the agents in Famn . Thus, the description of the game is exponential
in the in-degree of the graph and not in the total number of agents. In this case, we use
Σf−n and Af−n to refer to strategy profiles and action profiles, respectively, of the agents in
Famn \ {n}.
Example 3. Suppose 2L landowners along a road running north to south are deciding
whether to build a factory, a residential neighborhood, or a shopping mall on their plots.
The plots are laid out along the road in a 2-by-L grid; half of the agents are on the east side
(e1 , . . . , eL ) and half are on the west side (w1 , . . . , wL ). Each agent’s payoff depends only
on what it builds and what its neighbors to the north, south, and across the road build. For
example, no agent wants to build a residential neighborhood next to a factory. Each agent’s
payoff matrix is indexed by the actions of at most four agents (fewer at the ends of the road)
and has 34 entries, as opposed to the full 32L entries required in the equivalent normal form
game. (This example is due to Vickrey & Koller, 2002.)
3.2 Multi-Agent Influence Diagrams
The description length of extensive-form games can also grow exponentially with the number of agents. In many situations, this large tree can be represented more compactly.
Multi-agent influence diagrams (MAIDs) (Koller & Milch, 2001) allow a structured representation of games involving time and information by extending influence diagrams (Howard
& Matheson, 1984) to the multi-agent case.
MAIDs and influence diagrams derive much of their syntax and semantics from the
Bayesian network framework. A MAID compactly represents a certain type of extensiveform game in much the same way that a Bayesian network compactly represents a joint
probability distribution. For a thorough treatment of Bayesian networks, we refer the
reader to the reference by Cowell et al. (1999).
3.2.1 MAID Representation
Like a Bayesian network, a MAID defines a directed acyclic graph whose nodes correspond
to random variables. These random variables are partitioned into sets: a set X of chance
variables whose values are chosen by nature, represented in the graph by ovals; for each
agent n, a set Dn of decision variables whose values are chosen by agent n, represented
by rectangles; and for each agent n, a set Un of utility variables, represented by diamonds.
Chance and decision variables have, as their domains, finite sets of possible actions. We
refer to the domain of a random variable V by dom(V ). For each chance or decision variable
465

Blum, Shelton, & Koller

V , the graph defines a parent set PaV of those variables on whose values the choice at V
can depend. Utility variables have finite sets of real payoff values for their domains, and
are not permitted to have children in the graph; they represent components of an agent’s
payoffs, and not game state.
The game definition supplies each chance variable X with a conditional probability
distribution (CPD) P (X|PaX ), conditioned on the values of the parent variables of X.
The semantics for a chance variable are identical to the semantics of a random variable in
a Bayesian network; the CPD specifies the probability that an action in dom(X) will be
selected by nature, given the actions taken at X’s parents. The game definition also supplies
a utility function for each utility node U . The utility function maps each instantiation
pa ∈ dom(PaU ) deterministically to a real value U (pa). For notational and algorithmic
convenience, we can regard this utility function as a CPD P (U |PaU ) in which, for each
pa ∈ dom(PaU ), the value U (pa) has probability 1 in P (U |pa) and all other values have
probability 0 (the domain of U is simply the finite set of possible utility values). At the
end of the game, agent n’s total payoff is the sum of the utility received from each Uni ∈ Un
(here i is an index variable). Note that each component Uni of agent n’s payoff depends only
on a subset of the variables in the MAID; the idea is to compactly decompose a’s payoff
into additive pieces.
3.2.2 Strategy Representation
The counterpart of a CPD for a decision node is a decision rule. A decision rule for
a decision variable Dni ∈ Dn is a function, specified by n, mapping each instantiation
pa ∈ dom(PaDni ) to a probability distribution over the possible actions in dom(Dni ). A
decision rule is identical in form to a conditional probability distribution, and we can refer
to it using the notation P (Dni |PaDni ). As with the semantics for a chance node, the decision
rule specifies the probability that agent n will take any particular action in dom(Dni ), having
seen the actions taken at Dni ’s parents. An assignment of decision rules to all Dni ∈ Dn
comprises a strategy for agent n. Once agent n chooses a strategy, n’s behavior at Dni
depends only on the actions taken at Dni ’s parents. PaDni can therefore be regarded as the
set of nodes whose values are visible to n when it makes its choice at Dni . Agent n’s choice
of strategy may well take other nodes into account; but during actual game play, all nodes
except those in PaDni are invisible to n.
Example 4. The extensive-form game considered in Example 1 can be represented by the
MAID shown in Figure 2(a). Alice and Bob each have an initial decision to make without
any information about previous actions; then Alice has another decision to make in which
she is aware of Bob’s action and her own. Alice and Bob each have only one utility node
(the two are condensed into a single node in the graph, for the sake of brevity), whose payoff
structure is wholly general (dependent on every action in the game) and thus whose possible
values are exactly the values from the payoff vectors in the extensive-form game.
Example 5. Figure 2(b) shows a more complicated MAID of a somewhat more realistic
scenario. Here, three landowners along a road are deciding whether to build a store or a
house. Their payoff depends only on what happens adjacent to them along the road. Their
decision proceeds in two stages: the planning stage and the building stage. The second
466

A Continuation Method for Nash Equilibria in Structured Games

A
P1

P2

B

P3

E1

E2

C1

A’

C2

B1

AB

C3

B2

R1

L2

(a)

B3

R2

L3

(b)

Figure 2: (a) A simple MAID equivalent to the extensive form game in Figure 1. (b) A
two-stage road game with three agents.

landowner, for instance, has the two decision variables P2 and B2 . He receives a certain
penalty from the utility node C2 if he builds the opposite of what he had planned to build.
But after planning, he learns something about what his neighbor to the left has planned.
The chance node E1 represents noisy espionage; it transmits the action taken at P1 . After
learning the value of E1 , it may be in the second landowner’s interests to deviate from
his plan, even if it means incurring the penalty. It is in his interest to start a trend that
distinguishes him from previous builders but which subsequent builders will follow: the utility
node L2 rewards him for building the opposite of what was built at B1 , and the utility node
R2 rewards him if the third landowner builds the same thing he does at B3 .
Note that this MAID exhibits perfect recall, because the choice made at a planning stage
is visible to the agent when it makes its next choice at the building stage.
3.2.3 Payoffs
Under a particular strategy profile σ — that is, a tuple of strategies for all players — all
decision nodes have CPDs specified. Since chance and utility nodes are endowed with CPDs
already, the MAID therefore induces a fully-specified Bayesian network Bσ with variables
V = X ∪ D ∪ U and the same directed graph as the MAID. By the chain rule for Bayesian
networks,QBσ induces a joint probability distribution Pσ over all the variables in V by
Pσ (V) = V ∈V P (V |PaV ), with CPDs for chance and utility variables given by the MAID
definition and CPDs for decision variables given by σ. For a game G represented as a
MAID, the expected payoff that agent n receives under σ is the expectation of n’s utility
node values with respect to this distribution:
X
Gn (σ) =
EPσ [Uni ]
Uni ∈Un

=

X

X

Uni ∈Un u∈dom(Uni )

467

u · Pσ (u).

Blum, Shelton, & Koller

We show in Section 7 that this and other related expectations can be calculated efficiently
using Bayesian network inference algorithms, giving a substantial performance increase over
the calculation of payoffs in the extensive-form game.
3.2.4 Extensive Form Strategy Representations in MAIDs
A MAID provides a compact definition of an extensive-form game. We note that, although
this correspondence between MAIDs and extensive form games provides some intuition
about MAIDs, the details of the mapping are not relevant to the remainder of the discussion.
We therefore briefly review this construction, referring to the work of Koller and Milch
(2001) for details.
The game tree associated with a MAID is a full, balanced tree, with each path corresponding to a complete assignment of the chance and decision nodes in the network. Each
node in the tree corresponds either to a chance node or to a decision node of one of the
players, with an outgoing branch for each possible action at that node. All nodes at the
same depth in the tree correspond to the same MAID node. We assume that the nodes
along a path in the tree are ordered consistently with the ordering implied by the directed
edges in the MAID, so that if a MAID node X is a parent of a MAID node Y , the tree
branches on X before it branches on Y . The information sets for tree nodes associated
with a decision node Dni correspond to assignments to the parents PaDni : all tree nodes
corresponding to Dni with the same assignment to PaDni are in a single information set.
We note that, by construction, the assignment to PaDni was determined earlier in the tree,
and so the partition to information sets is well-defined. For example, the simple MAID in
Figure 2(a) expands into the much larger game tree that we saw earlier in Figure 1.
Translating in the opposite direction, from extensive-form games to MAIDs, is not
always as natural. If the game tree is unbalanced, then we cannot simply reverse the above
process. However, with care, it is possible to construct a MAID that is no larger than a
given extensive-form game, and that may be exponentially smaller in the number of agents.
The details are fairly technical, and we omit them here in the interest of brevity.
Despite the fact that a MAID will typically be much more compact than the equivalent
extensive-form game, the strategy representations of the two turn out to be equivalent and
of equal size. A decision rule for a decision variable Dni assigns a distribution over actions to
each joint assignment to PaDni , just as a behavior strategy assigns a distribution over actions
to an information set in an extensive form game — as discussed above, each assignment to
the parents of Dni is an information set. A strategy profile for a MAID — a set of decision
rules for every decision variable — is therefore equivalent to a set of behavior strategies for
every information set, which is simply a behavior profile.
If we make the assumption of perfect recall, then, since MAID strategies are simply
behavior strategies, we can represent them in sequence form. Perfect recall requires that
no agent forget anything that it has learned over the course of the game. In the MAID
formalism, the perfect recall assumption is equivalent to the following constraint: if agent
n has two decision nodes Dni and Dnj , with the second occurring after the first, then all
parents of Dni (the information n is aware of in making decision Dni ) and Dni itself must be
parents of Dnj . This implies that agent n’s final decision node Dnd has, as parents, all of n’s
previous decision nodes and their parents. Then a joint assignment to Dnd ∪ PaDnd precisely
468

A Continuation Method for Nash Equilibria in Structured Games

determines agent n’s sequence of information sets and actions leading to an outcome of the
game — the agent-n history of the outcome.
The realization probability for a particular sequence is computed by multiplying all
behavior strategy probabilities for actions in that sequence. In MAIDs, a sequence corresponds to a joint assignment to Dnd ∪ PaDnd , and the behavior strategy probabilities for
this sequence are entries consistent with this assignment in the decision rules for agent n.
We can therefore derive all of agent n’s realization probabilities at once by multiplying
together, as conditional probability distributions, the decision rules of each of agent n’s decision nodes in the sequence — when multiplying conditional probability distributions, only
those entries whose assignments are consistent with each other are multiplied. Conversely,
given a realization plan, we can derive the behavior strategies and hence the decision rules
according to the method outlined for extensive-form games.
In the simple MAID example in Figure 2(a), the terminal sequences are the same as
in the equivalent extensive-form game. In the road example in Figure 2(b), agent 2 has 8
terminal sequences; one for each joint assignment to his final decision node (B2 ) and its
parents (E1 and P2 ). Their associated realization probabilities are given by multiplying the
decision rules at P2 and at B2 .

4. Computational Complexity
When developing algorithms to compute equilibria efficiently, the question naturally arises
of how well one can expect these algorithms to perform. The complexity of computing
Nash equilibria has been studied for some time. Gilboa and Zemel (1989) first showed
that it is NP-hard to find more than one Nash equilibrium in a normal-form game, and
Conitzer and Sandholm (2003) recently utilized a simpler reduction to arrive at this result
and several others in the same vein. Other recent hardness results pertain to restricted
subclasses of normal-form games (e.g., Chu & Halpern, 2001; Codenotti & Stefankovic,
2005). However, these results apply only to 2-agent normal-form games. While it is true
that proving a certain subclass of a class of problems to be NP-hard also proves the entire
class to be NP-hard (because NP-hardness is a measure of worst-case complexity), such a
proof might tell us very little about the complexity of problems outside the subclass. This
issue is particularly apparent in the problem of computing equilibria, because games can
grow along two distinct axes: the number of agents, and the number of actions per agent.
The hardness results of Conitzer and Sandholm (2003) apply only as the number of actions
per agent increases. Because 2-agent normal-form games are (fully connected) graphical
games, these results apply to graphical games.
However, we are more interested in the hardness of graphical games as the number
of agents increases, rather than the number of actions per agent. It is graphical games
with large numbers of agents that capture the most structure — these are the games for
which the graphical game representation was designed. In order to prove results about the
asymptotic hardness of computing equilibria along this more interesting (in this setting)
axis of representation size, we require a different reduction. Our proof, like a number of
previous hardness proofs for games (e.g., Chu & Halpern, 2001; Conitzer & Sandholm, 2003;
Codenotti & Stefankovic, 2005), reduces 3SAT to equilibrium computation. However, in
these previous proofs, variables in 3SAT instances are mapped to actions (or sets of actions)
469

Blum, Shelton, & Koller

in a game with only 2 players, whereas in our reduction they are mapped to agents. Although
differing in approach, our reduction is very much in the spirit of the reduction appearing in
the work of Conitzer and Sandholm (2003), and many of the corollaries of their main result
also follow from ours (in a form adapted to graphical games).
Theorem 6. For any constant d ≥ 5, and k ≥ 2, the problem of deciding whether a
graphical game with a family size at most d and at most k actions per player has more than
one Nash equilibrium is NP-hard.
Proof. Deferred to Appendix B.
In our reduction, all games that have more than one equilibria have at least one pure
strategy equilibrium. This immediately gives us
Corollary 7. It is NP-hard to determine whether a graphical game has more than one Nash
equilibrium in discretized strategies with even the coarsest possible granularity.
Finally, because graphical games can be represented as (trivial) MAIDs, in which each
agent has only a single parentless decision node and a single utility node, and each agent’s
utility node has, as parents, the decision nodes of the graphical game family of that agent,
we obtain the following corollary.
Corollary 8. It is NP-hard to determine whether a MAID with constant family size at least
6 has more than one Nash equilibrium.

5. Continuation Methods
Continuation methods form the basis of our algorithms for solving each of these structured game representations. We begin with a high-level overview of continuation methods,
referring the reader to the work of Watson (2000) for a more detailed discussion.
Continuation methods work by solving a simpler perturbed problem and then tracing
the solution as the magnitude of the perturbation decreases, converging to a solution for
the original problem. More precisely, let λ be a scalar parameterizing a continuum of
perturbed problems. When λ = 0, the perturbed problem is the original one; when λ = 1,
the perturbed problem is one for which the solution is known. Let w represent the vector
of real values of the solution. For any perturbed problem defined by λ, we characterize
solutions by the equation F (w, λ) = 0, where F is a real-valued vector function of the same
dimension as w (so that 0 is a vector of zeros). The function F is such that w is a solution
to the problem perturbed by λ if and only if F (w, λ) = 0.
The continuation method traces solutions along the level set of solution pairs (w, λ)
satisfying F (w, λ) = 0. Specifically, if we have a solution pair (w, λ), we would like to trace
that solution to a nearby solution. Differential changes to w and λ must cancel out so that
F remains equal to 0.
If (w, λ) changes in the direction of a unit vector u, then F will change
in the direction


∇F · u, where ∇F is the Jacobian of F (which can also be written ∇w F ∇λ F ). We
want to find a direction u such that F remains unchanged, i.e., equal to 0. Thus, we need
to solve the matrix equation
 

 dw
∇ w F ∇λ F
=0.
(3)
dλ
470

A Continuation Method for Nash Equilibria in Structured Games

Equivalently, changes dw and dλ along the path must obey ∇w F ·dw = −∇λ F ·dλ. Rather
than inverting the matrix ∇w F in solving this equation, we use the adjoint adj(∇w F ), which
is still defined when ∇w F has a null space of rank 1. The adjoint is the matrix of cofactors:
the element at (i, j) is (−1)i+j times the determinant of the sub-matrix in which row i and
column j have been removed. When the inverse is defined, adj(∇w F ) = det(∇w F )[∇w F ]−1 .
In practice, we therefore set dw = −adj(∇w F ) · ∇λ F and dλ = det(∇w F ). If the Jacobian
[∇w F ∇λ F ] has a null-space of rank 1 everywhere, the curve is uniquely defined.
The function F should be constructed so that the curve starting at λ = 1 is guaranteed
to cross λ = 0, at which point the corresponding value of w is a solution to the original
problem. A continuation method begins at the known solution for λ = 1 . The null-space of
the Jacobian ∇F at a current solution (w, λ) defines a direction, along which the solution
is moved by a small amount. The Jacobian is then recalculated and the process repeats,
tracing the curve until λ = 0. The cost of each step in this computation is at least cubic in
the size of w, due to the required matrix operations. However, the Jacobian itself may in
general be much more difficult to compute. Watson (2000) provides some simple examples
of continuation methods.

6. Continuation Methods for Games
We now review the work of GW on applying the continuation method to the task of finding equilibria in games. They provide continuation methods for both normal-form and
extensive-form games. These algorithms form the basis for our extension to structured
games, described in the next section. The continuation methods perturb the game by giving agents fixed bonuses, scaled by λ, for each of their actions, independently of whatever
else happens in the game. If the bonuses are large enough (and unique), they dominate the
original game structure, and the agents need not consider their opponents’ actions. There
is thus a unique pure-strategy equilibrium easily determined by the bonuses at λ = 1. The
continuation method can then be used to follow a path in the space of λ and equilibrium
profiles for the resulting perturbed game, decreasing λ until it is zero; at this point, the
corresponding strategy profile is an equilibrium of the original game.
6.1 Continuation Method for Normal-Form Games
We now make this intuition more precise, beginning with normal-form games.
6.1.1 Perturbations
A perturbation vector b is a vector of m values chosen at random, one for each action in the
game. The bonus ba is given to the agent n owning action a for playing a, independently of
whatever else happens in the game. Applying this perturbation to a target game G gives
us a new game, which we denote G ⊕ b, in which, for each a ∈ An , and for any t ∈ A−n ,
(G ⊕ b)n (a, t) = Gn (a, t) + ba . If b is made sufficiently large, then G ⊕ b has a unique
equilibrium, in which each agent plays the pure strategy a for which ba is maximal.
471

Blum, Shelton, & Koller

6.1.2 Characterization of Equilibria
In order to apply Equation (3), we need to characterize the equilibria of perturbed games as
the zeros of a function F . Using a structure theorem of Kohlberg and Mertens (1986), GW
show that the continuation method path deriving from their equilibrium characterization
leads to convergence for all perturbation vectors except those in a set of measure zero. We
present only the equilibrium characterization here; proofs of the characterization and of the
method’s convergence are given by Govindan and Wilson (2003).
We first define an auxiliary vector function V G (σ), indexed by actions, of the payoffs to
each agent for deviating from σ to play a single action. We call V G the deviation function.
The element VaG (σ) corresponding to a single action a, owned by an agent n, is the payoff
to agent n when it deviates from the mixed strategy profile σ by playing the pure strategy
for action a:
X
Y
(4)
VaG (σ) =
Gn (a, t)
σtk .
t∈A−n

k∈N \{n}

It can also be viewed as the component of agent n’s payoff that it derives from action a,
under the strategy profile σ. Since bonuses are given to actions independently of σ, the
effect of bonuses on V G is independent of σ. VaG measures the payoff for deviating and
playing a, and bonuses are given for precisely this deviation, so V G⊕b (σ) = V G (σ) + b.
We also utilize the retraction operator R : IRm → Σ defined by Gül, Pearce, and Stachetti (1993), which maps an arbitrary m-vector w to the point in the space Σ of mixed
strategies which is nearest to w in Euclidean distance. Given this operator, the equilibrium
characterization is as follows.
Lemma 9. (Gül et al., 1993) If σ is a strategy profile of G, then σ = R(V G (σ) + σ) iff σ
is an equilibrium.
Although we omit the proof, we will give some intuition for why this result is true.
Suppose σ is a fully-mixed equilibrium; that is, every action has non-zero probability. For
a single agent n, VaG (σ) must be the same for all actions a ∈ An , because n should not have
any incentive to deviate and play a single one of them. Let Vn be the vector of entries in
V G (σ) corresponding to actions of n, and let σn be defined similarly. Vn is a scalar multiple
of 1, the all-ones vector, and the simplex Σn of n’s mixed strategies is defined by 1T x = 1,
so Vn is orthogonal to Σn . V G (σ) is therefore orthogonal to Σ, so retracting σ + V G (σ) onto
Σ gives precisely σ. In the reverse direction, if σ is a fully-mixed strategy profile satisfying
σ = R(V G (σ) + σ), then V G (σ) must be orthogonal to the polytope of mixed strategies.
Then, for each agent, every pure strategy has the same payoff. Therefore, σ is in fact an
equilibrium. A little more care must be taken when dealing with actions not in the support.
We refer to Gül et al. (1993) for the details.
According to Lemma 9, we can define an equilibrium as a solution to the equation
σ = R(σ + V G (σ)). On the other hand, if σ = R(w) for some w ∈ IRm , we have the
equivalent condition that w = R(w) + V G (R(w)); σ is an equilibrium iff this condition
is satisfied, as can easily be verified. We can therefore search for a point w ∈ IRm which
satisfies this equality, in which case R(w) is guaranteed to be an equilibrium.
The form of our continuation equation is then

F (w, λ) = w − R(w) − V G (R (w)) + λb .
(5)
472

A Continuation Method for Nash Equilibria in Structured Games

We have that V G + λb is the deviation function for the perturbed game G ⊕ λb, so F (w, λ)
is zero if and only if R(w) is an equilibrium of G ⊕ λb. At λ = 0 the game is unperturbed,
so F (w, 0) = 0 iff R(w) is an equilibrium of G.
6.1.3 Computation
The expensive step in the continuation method is the calculation of the Jacobian ∇w F ,
required for the computation that maintains the constraint of Equation (3). Here, we have
that ∇w F = I − (I + ∇V G )∇R, where I is the m × m identity matrix. The hard part is
the calculation of ∇V G . For pure strategies a ∈ An and a0 ∈ An0 , for n0 6= n, the value
at location (a, a0 ) in ∇V G (σ) is equal to the expected payoff to agent n when it plays the
pure strategy a, agent n0 plays the pure strategy a0 , and all other agents act according to
the strategy profile σ:
Y
∂ X
Gn (a, t)
σtk
∂σa0
t∈A−n
k∈N \{n}
X
Y
0
=
Gn (a, a , t)
σ tk .

G
∇Va,a
0 (σ) =

t∈A−n,n0

(6)

k∈N \{n,n0 }

G (σ) = 0.
If both a ∈ An and a0 ∈ An , ∇Va,a
0

Computing Equation
(6) requires a large number of multiplications; the sum is over the
Q
space A−n,n0 = k∈N \{n,n0 } Ai , which is exponentially large in the number of agents.
6.2 Continuation Method for Extensive-Form Games
The same method applies to extensive-form games, using the sequence form strategy representation.
6.2.1 Perturbations
As with normal-form games, the game is perturbed by the bonus vector b. Agent n owning
sequence h is paid an additional bonus bh for playing h, independently of whatever else
happens in the game. Applying this perturbation gives us a new game G ⊕ b in which, for
each z ∈ Z, (G ⊕ b)n (z) = Gn (z) + bHn (z) .
If the bonuses are large enough and unique, GW show that once again the perturbed
game has a unique pure-strategy equilibrium (one in which all realization probabilities are
0 or 1). However, calculating it is not as simple as in the case of normal-form games.
Behavior strategies must be calculated from the leaves upward by a recursive procedure, in
which at each step the agent who owns the node in question chooses the action that results
in the sequence with the largest bonus. Since all actions below it have been recursively
determined, each action at the node in question determines an outcome. The realization
plans can be derived from this behavior profile by the method outlined in Section 2.2.1.
473

Blum, Shelton, & Koller

6.2.2 Characterization of Equilibria
Once more, we first define a vector function capturing the benefit of deviating from a given
strategy profile, indexed by sequences:
X
Y
σk (z),
(7)
VhG (σ) =
Gn (z)
z∈Zh

k∈N \{n}

where Zh is the set of leaves that are consistent with the sequence h. The interpretation of
V G is not as natural as in the case of normal-form games, as it is not possible for an agent
to play one sequence to the exclusion of all others; its possible actions will be partially
determined by the actions of other agents. In this case, VhG (σ) can be regarded as the
portion of its payoff that agent n receives for playing sequence h, unscaled by agent n’s own
probability of playing that sequence. As with normal-form games, the vector of bonuses is
added directly to V G , so V G⊕b = V G + b.
The retraction operator R for realization plans is defined in the same way as for normalform strategies: it takes a general vector and projects it onto the nearest point in the valid
region of realization plans. The constraints defining this space are linear, as discussed in
Section 2.2.1 . We can therefore express them as a constraint matrix C with Cσ = 0 for
all valid profiles σ. In addition, all probabilities must be greater than or equal to zero. To
calculate w, we must find a σ minimizing (w −σ)T (w −σ), the (squared) Euclidean distance
between w and σ, subject to Cσ = 0 and σ ≥ 0. This is a quadratic program (QP), which
can be solved efficiently using standard methods. The Jacobian of the retraction is easily
computable from the set of active constraints.
The equilibrium characterization for realization plans is now surprisingly similar to
that of mixed strategies in normal-form games; GW show that, as before, equilibria are
characterized by σ = R(σ + V G (σ)), where now R is the retraction for sequence form and
V G is the deviation function. The continuation equation F takes exactly the same form as
well.
6.2.3 Computation
The key property of the reduced sequence-form strategy representation is that the deviation function is a multi-linear function of the extensive-form parameters, as shown in
Equation (7). The elements of the Jacobian ∇V G thus also have the same general structure. In particular, the element corresponding to sequence h for agent n and sequence h0
for agent n0 is
Y
∂ X
Gn (z)
σk (z)
∂σh0
z∈Zh
k∈N \{n}
X
Y
=
Gn (z)
σk (z)

G
∇Vh,h
0 (σ) =

z∈Zh,h0

(8)

k∈N \{n,n0 }

where Zh,h0 is the set of leaves that are consistent with the sequences h (for agent n) and
h0 (for agent n0 ). Zh,h0 is the empty set (and hence ∇V G = 0) if h and h0 are incompatible.
Equation (8) is precisely analogous to Equation (6) for normal-form games. We have a sum
over outcomes of the utility of the outcome multiplied by the strategy probabilities for all
474

A Continuation Method for Nash Equilibria in Structured Games

σ
3
2

λ
1

Figure 3: An abstract diagram of the path. The horizontal axis represents λ and the vertical
axis represents the space of strategy profiles (actually multidimensional). The
algorithm starts on the right at λ = 1 and follows the dynamical system until
λ = 0 at point 1, where it has found an equilibrium of the original game. It can
continue to trace the path and find the equilibria labeled 2 and 3.

other agents. Note that this sum is over the leaves of the tree, which may be exponentially
numerous in the number of agents.
One additional subtlety, which must be addressed by any method for equilibrium computation in extensive-form games, relates to zero-probability actions. Such actions induce
a probability of zero for entire trajectories in the tree, possibly leading to equilibria based
on unrealizable threats. Additionally, for information sets that occur with zero probability,
agents can behave arbitrarily without disturbing the equilibrium criterion, resulting in a continuum of equilibria and a possible bifurcation in the continuation path. This prevents our
methods from converging. We therefore constrain all realization probabilities to be greater
than or equal to  for some small  > 0. This is, in fact, a requirement for GW’s equilibrium
characterization to hold. The algorithm thus looks for an -perfect equilibrium (Fudenberg
& Tirole, 1991): a strategy profile σ in which each component is constrained by σs ≥ , and
each agent’s strategy is a best response among those satisfying the constraint. Note that
this is entirely different from an -equilibrium. An -perfect equilibrium always exists, as
long as  is not so large as to make the set of legal strategies empty. An -perfect equilibrium can be interpreted as an equilibrium in a perturbed game in which agents have a small
probability of choosing an unintended action. A limit of -perfect equilibria as  approaches
0 is a perfect equilibrium (Fudenberg & Tirole, 1991): a refinement of the basic notion of
a Nash equilibrium. As  approaches 0, the equilibria found by GW’s algorithm therefore
converge to an exact perfect equilibrium, by continuity of the variation in the continuation
method path. Then for  small enough, there is a perfect equilibrium in the vicinity of the
found -perfect equilibrium, which can easily be found with local search.
475

Blum, Shelton, & Koller

6.3 Path Properties
In the case of normal-form games, GW show, using the structure theorem of Kohlberg and
Mertens (1986), that the path of the algorithm is a one-manifold without boundary with
probability one over all choices for b. They provide an analogous structure theorem that
guarantees the same property for extensive-form games. Figure 3(a) shows an abstract
representation of the path followed by the continuation method. GW show that the path
must cross the λ = 0 hyperplane at least once, yielding an equilibrium. In fact, the path
may cross multiple times, yielding many equilibria in a single run. As the path must
eventually continue to the λ = −∞ side, it will find an odd number of equilibria when run
to completion.
In both normal-form and extensive-form games, the path is piece-wise polynomial, with
each piece corresponding to a different support set of the strategy profile. These pieces are
called support cells. The path is not smooth at cell boundaries due to discontinuities in the
Jacobian of the retraction operator, and hence in ∇w F , when the support changes. Care
must be taken to step up to these boundaries exactly when following the path; at this point,
the Jacobian for the new support can be calculated and the path can be traced into the
new support cell.
In the case of two agents, the path is piece-wise linear and, rather than taking steps, the
algorithm can jump from corner to corner along the path. When this algorithm is applied to
a two-agent game and a particular bonus vector is used (in which only a single entry is nonzero), the steps from support cell to support cell that the algorithm takes are identical to
the pivots of the Lemke-Howson algorithm (Lemke & Howson, 1964) for two-agent generalsum games, and the two algorithms find precisely the same set of solutions (Govindan &
Wilson, 2002). Thus, the continuation method is a strict generalization of the LemkeHowson algorithm that allows different perturbation rays and games of more than two
agents.
This process is described in more detail in the pseudo-code for the algorithm, presented
in Figure 4.
6.4 Computational Issues
Guarantees of convergence apply only as long as we stay on the path defined by the dynamical system of the continuation method. However, for computational purposes, discrete
steps must be taken. As a result, error inevitably accumulates as the path is traced, so that
F becomes slightly non-zero. GW use several simple techniques to combat this problem.
We adopt their techniques, and introduce one of our own: we employ an adaptive step
size, taking smaller steps when error accumulates quickly and larger ones when it does not.
When F is nearly linear (as it is, for example, when very few actions are in the support of
the current strategy profile), this technique speeds computation significantly.
GW use two different techniques to remove error once it has accumulated. Suppose we
are at a point (w, λ) and we wish to minimize the magnitude of F (w, λ) = w − V G (R(w)) +
λb+R(w). There are two values we might change: w, or λb. We can change the first without
affecting the guarantee of convergence, so every few steps we run a local Newton method
search for a w minimizing |F (w, λ)|. If this search does not decrease error sufficiently, then
we perform what GW call a “wobble”: we change the perturbation vector (“wobble” the
476

A Continuation Method for Nash Equilibria in Structured Games

continuation path) to make the current solution consistent. If we set b = [w − V G (R(w)) −
R(w)]/λ, the equilibrium characterization equation is immediately satisfied. Changing the
perturbation vector invalidates any theoretical guarantees of convergence. However, it is
nonetheless an attractive option because it immediately reduces error to zero. Both the
local Newton method and the “wobbles” are described in more detail by Govindan and
Wilson (2003).
These techniques can potentially send the algorithm into a cycle, and in practice they
occasionally do. However, they are necessary for keeping the algorithm on the path. If the
algorithm cycles, random restarts and a decrease in step size can improve convergence. More
sophisticated path-following algorithms might also be used, and in general could improve
the success rate and execution time of the algorithm.
6.5 Iterated Polymatrix Approximation
Because perturbed games may themselves have a large number of equilibria, and the path
may wind back and forth through any number of them, the continuation algorithm can
take a while to trace its way back to a solution to the original game. We can speed up the
algorithm using an initialization procedure based on the iterated polymatrix approximation
(IPA) algorithm of GW. A polymatrix game is a normal-form game in which the payoffs to
an agent n are equal to the sum of the payoffs from a set of two-agent games, each involving
n and another agent. Because polymatrix games are a linear combination of two-agent
normal-form games, they reduce to a linear complementarity problem and can be solved
quickly using the Lemke-Howson algorithm (Lemke & Howson, 1964).
For each agent n ∈ N in a polymatrix game, the payoff array is a matrix B n indexed
n
by the actions of agent n and of each other agent; for actions a ∈ An and a0 ∈ An0 , Ba,a
0
0
0
0
is the payoff n receives for playing a in its game with agent n , when n plays a . Agent
n’s
the payoffs it receives from its games with each other agent,
P total
Ppayoff is the sum of
n
n0 6=n
a∈An ,a0 ∈An0 σa σa0 Ba,a0 . Given a normal-form game G and a strategy profile σ, we
can construct the polymatrix game Pσ whose payoff function has the same Jacobian at σ
as G’s by setting
G
n
(9)
Ba,a
0 = ∇Va,a0 (σ) .
The game Pσ is a linearization of G around σ: its Jacobian is the same everywhere. GW
show that σ is an equilibrium of G if and only if it is an equilibrium of Pσ . This follows
from the equation V G (σ) = ∇V G (σ) · σ/(|N | − 1), which holds for all σ. To see why it
holds, consider the single element indexed by a ∈ An :
X
X
X
Y
(∇V G (σ) · σ)a =
σ a0
Gn (a, a0 , t)
σ tk
n0 ∈N \{n} a0 ∈An0

=

X

X

n0 ∈N \{n}

t∈A−n

k∈N \{n,n0 }

t∈A−n,−n0

Gn (a, t)

Y

σtk

k∈N \{n}

G

= (|N | − 1)V (σ)a .
The equilibrium characterization equation can therefore be written

σ = R σ + ∇V G (σ) · σ (|N | − 1) .
477

Blum, Shelton, & Koller

G and Pσ have the same value of ∇V at σ, and thus the same equilibrium characterization
function. Then σ satisfies one if and only if it satisfies the other.
We define the mapping p : Σ → Σ such that p(σ) is an equilibrium for Pσ (specifically,
the first equilibrium found by the Lemke-Howson algorithm). If p(σ) = σ, then σ is an
equilibrium of G. The IPA procedure of Govindan and Wilson (2004) aims to find such a
fixed point. It begins with a randomly chosen strategy profile σ, and then calculates p(σ)
by running the Lemke-Howson algorithm; it adjusts σ toward p(σ) using an approximate
derivative estimate of p built up over the past two iterations. If σ and p(σ) are sufficiently
close, it terminates with an approximate equilibrium.
IPA is not guaranteed to converge. However, in practice, it quickly moves “near” a
good solution. It is possible at this point to calculate a perturbed game close to the
original game (essentially, one that differs from it by the same amount that G’s polymatrix
approximation differs from G) for which the found approximate equilibrium is in fact an
exact equilibrium. The continuation method can then be run from this starting point to
find an exact equilibrium of the original game. The continuation method is not guaranteed
to converge from this starting point. However, in practice we have always found it to
converge, as long as IPA is configured to search for high quality equilibrium approximations.
Although there are no theoretical results on the required quality, IPA can refine the starting
point further if the continuation method fails. Our results show that the IPA quick-start
substantially reduces the overall running time of our algorithm.
We can in fact use any other approximate algorithm as a quick-start for ours, also
without any guarantees of convergence. Given an approximate equilibrium σ, the inverse
image of σ under R is defined by a set of linear constraints. If we let w := V G (σ) + σ,
then we can use standard QP methods to retract w to the nearest point w0 satisfying these
constraints, and let b := w0 − w. Then σ = R(w0 ) = R(V G (σ) + σ + b), so we are on a
continuation method path. Alternatively, we can choose b by “wobbling”, in which case we
set b := [w − V G (R(w)) − R(w)]/λ.

7. Exploiting Structure
Our algorithm’s continuation method foundation is the same for each game representation,
but the calculation of ∇V G in Step 2(b)i of the pseudo-code in Figure 4 is different for each
and consumes most of the time. Both in normal-form and (in the worst case) in extensiveform games, it requires exponential time in the number of agents. However, as we show in
this section, when using a structured representation such as a graphical game or a MAID,
we can effectively exploit the structure of the game to drastically reduce the computational
time required.
7.1 Graphical Games
Since a graphical game is also a normal-form game, the definition of the deviation function
V G in Equation (4) is the same: VaG (σ) is the payoff to agent n for deviating from σ to
play a deterministically. However, due to the structure of the graphical game, the choice
of strategy for an agent outside the family of n does not affect agent n0 s payoff. This
observation allows us to compute this payoff locally.
478

A Continuation Method for Nash Equilibria in Structured Games

For an input game G:
1. Set λ = 1, choose initial b and σ either by a quick-start procedure (e.g., IPA) or by randomizing. Set
w = V G (σ) + λb + σ.
2. While λ is greater than some (negative) threshold (i.e., there is still a good chance of picking up
another equilibrium):
(a) Initialize for the current support cell: set the steps counter to the number of steps we will take
in crossing the cell, depending on the current amount of error. If F is linear or nearly linear (if,
for example, the strategy profile is nearly pure, or there are only 2 agents), set steps = 1 so we
will cross the entire cell.
(b) While steps ≥ 1:
i. Compute ∇V G (σ).
ii. Set ∇w F (w, λ) = I − (∇V G (σ) + I)∇R(w) (we already know ∇λ F = −b). Set dw =
adj(∇w F ) · b and dλ = det(∇w F ). These satisfy Equation (3).
iii. Set δ equal to the distance we’d have to go in the direction of dw to reach the next support
boundary. We will scale dw and dλ by δ/steps.
iv. If λ will change signs in the course of the step, record an equilibrium at the point where
it is 0.
v. Set w := w + dw(δ/steps) and λ := λ + dλ(δ/steps).
vi. If sufficient error has accumulated, use the local Newton method to find a w minimizing
|F (w, λ)|. If this does not reduce error enough, increase steps, thereby decreasing step
size. If we have already increased steps, perform a “wobble” and reassign b.
vii. Set steps := steps − 1.

Figure 4: Pseudo-code for the cont algorithm.
7.1.1 The Jacobian for Graphical Games
We begin with the definition of V G for normal-form games (modified slightly to account
for the local payoff arrays). Recall that Af−n is the set of action profiles of agents in Famn
other than n, and let A−Famn be the set of action profiles of agents not in Famn . Then
we can divide a sum over full action profiles between these two sets, switching from the
normal-form version of Gn to the graphical game version of Gn , as follows:
X
Y
VaG (σ) =
Gn (a, t)
σ tk
t∈A−n

=

X
u∈Af−n

k∈N \{n}

Gn (a, u)

Y

σuk

X

Y

σv j .

(10)

k∈Famn \{n} v∈A−Famn j∈N \Famn

Note that the latter sum and product simply sum out a probability distribution, and hence
are always equal to 1 due to the constraints on σ. They can thus be eliminated without
changing the value V G takes on valid strategy profiles. However, their partial derivatives
with respect to strategies of agents not in Famn are non-zero, so they enter into the computation of ∇V G .
Suppose we wish to compute a row in the Jacobian matrix corresponding to action a of
agent n. We must compute the entries for each action a0 of each agent n0 ∈ N . In the trivial
G = 0, since σ does not appear anywhere in the expression
case where n0 = n then ∇Va,a
0
a
for VaG (σ). We next compute the entries for each action a0 of each other agent n0 ∈ Famn .
479

Blum, Shelton, & Koller

In this case,
G
∇Va,a
0 (σ) =

X
Y
∂
Gn (a, u)
∂σa0
f

=

Gn (a, u)

u∈Af−n

=

X

∂
∂σa0

Y

Y

σv j

(11)

σuk · 1

k∈Famn \{n}

Y

Gn (a, a0 , t)

t∈Af−n,n0

X

v∈A−Famn j∈N \Famn

k∈Famn \{n}

u∈A−n

X

σuk

k∈Famn

if n0 ∈ Famn .

σ tk ,

(12)

\{n,n0 }

We next compute the entry for a single action a0 of an agent n0 ∈
/ Famn . The derivative in
Equation (11) takes a different form in this case; the variable in question is in the second
summation, not the first, so that we have
X
Y
X
Y
∂
G
∇Va,a
σv j
Gn (a, u)
σuk
0 (σ) =
∂σa0
f
=

X

Gn (a, u)

=

X

Y

σuk

Gn (a, u)

u∈Af−n

Y

X

v∈A−Famn

k∈Famn \{n}

u∈Af−n

v∈A−Famn j∈N \Famn

k∈Famn \{n}

u∈A−n

σuk · 1,

∂
∂σa0

Y

σv j

j∈N \Famn

if n0 6∈ Famn .

(13)

k∈Famn \{n}

Notice that this calculation does not depend on a0 ; therefore, it is the same for each action
of each other agent not in Famn . We need not compute any more elements of the row. We
can copy this value into all other columns of actions belonging to agents not in Famn .
7.1.2 Computational Complexity
Due to graphical game structure, the computation of ∇V G (σ) takes time exponential only
in the maximal family size of the game, and hence takes time polynomial in the number
of agents if the family size is constant. In particular, our methods lead to the following
theorem about the complexity of the continuation method for graphical games.
Theorem 10. The time complexity of computing the Jacobian of the deviation function
∇V G (σ) for a graphical game is O(f df |N | + d2 |N |2 ), where f is the maximal family size
and d is the maximal number of actions per agent.
Proof. Consider a single row in the Jacobian, corresponding to a single action a owned by
a single agent n. There are at most d(f − 1) entries in the row for actions owned by other
G
members of Famn . For one such action a0 , the computation of the Jacobian element ∇Va,a
0
f
−2
according to Equation (12) takes time O(d ). The total cost for all such entries is therefore
O((f − 1)df −1 ). There are then at most d(|N | − f ) entries for actions owned by non-familyG for each such a0 is the same. It can be calculated once in time
members. The value of ∇Va,a
0
f
−1
O(d ), then copied across the row in time d(|N | − f ). All in all, the computational cost
for the row is O(f df −1 + d|N |). There are at most d|N | rows, so the total computational
cost is O(|N |f df + d2 |N |2 ).
480

A Continuation Method for Nash Equilibria in Structured Games

P1

P2

P3

B1

B2

B3

A
B
A’
(a)

(b)

Figure 5: The strategic relevance graphs for the MAIDs in (a) Figure 2(a) and (b) Figure 2(b).

Each iteration of the algorithm calculates ∇V G (σ) once; we have therefore proved that a
single iteration takes time polynomial in |N | if f is constant (in fact, matrix operations make
the complexity cubic in |N |). However, as for normal-form games, there are no theoretical
results about how many steps of the continuation method are required for convergence.
7.2 MAIDs
For graphical games, the exploitation of structure was straightforward. We now turn to
the more difficult problem of exploiting structure in MAIDs. We take advantage of two
distinct sets of structural properties. The first, a coarse-grained structural measure known
as strategic relevance (Koller & Milch, 2001), has been used in previous computational
methods. After decomposing a MAID according to strategic relevance relations, we can
exploit finer-grained structure by using the extensive-form continuation method of GW
to solve each component’s equivalent extensive-form game. In the next two sections, we
describe these two kinds of structure.
7.2.1 Strategic Relevance
Intuitively, a decision node Dni is strategically relevant to another decision node Dnj 0 if agent
n0 , in order to optimize its decision rule at Dnj 0 , needs to know agent n’s decision rule at
Dni . The relevance relation induces a directed graph known as the relevance graph, in which
only decision nodes appear and an edge from node Dnj 0 to node Dni is present iff Dni is
strategically relevant to Dnj 0 . In the event that the relevance graph is acyclic, the decision
rules can be optimized sequentially in any reverse topological order; when all the children
of a node Dni have had their decision rules set, the decision rule at Dni can be optimized
without regard for any other nodes.
When cycles exist in the relevance graph, however, further steps must be taken. Within
a strongly connected component (SCC), a set of nodes for which a directed path between any
two nodes exists in the relevance graph, decision rules cannot be optimized sequentially—
in any linear ordering of the nodes in the SCC, some node must be optimized before one
481

Blum, Shelton, & Koller

of its children, which is impossible. Koller and Milch (2001) show that a MAID can be
decomposed into SCCs, which can then be solved individually.
For example, the relevance graph for the MAID in Figure 2(a), shown in Figure 5(a),
has one SCC consisting of A and B, and another consisting of A0 . In this MAID, we would
first optimize the decision rule at A0 , as the optimal decision rule at A0 does not rely on the
decision rules at A and B — when she makes her decision at A0 , Alice already knows the
actions taken at A and B, so she does not need to know the decision rules that led to them.
Then we would turn A0 into a chance node with CPD specified by the optimized decision
rule and optimize the decision rules at A0 and B. The relevance graph for Figure 2(b),
shown in Figure 5(b), forms a single strongly connected component.
The computational method of Koller and Milch (2001) stops at strategic relevance:
each SCC is converted into an equivalent extensive-form game and solved using standard
methods. Our algorithm can be viewed as an augmentation of their method: after a MAID
has been decomposed into SCCs, we can solve each of these SCCs using our methods, taking
advantage of finer-grained MAID structure within them to find equilibria more efficiently.
The MAIDs on which we test our algorithms (including the road MAID in Figure 2b) all
have strongly connected relevance graphs, so they cannot be decomposed (see Figure 5b
and Figure 10).
7.2.2 The Jacobian for MAIDs
A MAID is equivalent to an extensive-form game, so its deviation function V G is the same
one defined in Equation (8). Now, however, we can compute the payoffs that make up the
Jacobian ∇V G more efficiently. Consider a payoff Gn (z) to agent n for outcome z. The
outcome z is simply an assignment x to all of the variables in the MAID. The realization
probability σn (z) is the product
Q of the probabilities for the decisions of agent n in the
assignment x, so the product k∈N σk (z) of all realization probabilities is simply the joint
probabilityPof the assignment.
The expected payoff agent n will receive under the strategy
Q
profile σ, z∈Z Gn (z) k∈N σk (z), is therefore an expectation of Gn (z). The expectation
is with respect to the distribution Pσ defined by the Bayesian network Bσ whose structure
is the same as the MAID, with decision node CPDs determined by σ.
The entries of ∇V G are not strictly expected payoffs, however. Equation (8) can be
rewritten as
Q
X Gn (z)
k∈N σk (z)
G
∇Vh,h0 (σ) =
.
(14)
σn (z)σn0 (z)
z∈Zh,h0

The expectation is of the quantity Gn (z)/[σn (z)σn0 (z)]. The payoff Gn (z) is the sum of agent
n’s utility nodes. Due to linearity of expectation, we can perform the computation separately
for each of agent n’s utility nodes, and then simply add up the separate contributions.
We can therefore restrict our attention to computing the contribution of a single utility
node Un for each agent n. Furthermore, the value of σn (z) depends only on the values
of the set of nodes D n consisting of n’s decision nodes and their parents. Thus, instead
of computing the probabilities for all assignments to all variables, we need only compute
the marginal joint distribution over Un , D n , and D n0 . From this distribution, we can
compute the contribution of Un to the expectation in Equation (14) for every pair of terminal
sequences belonging to agents n and n0 .
482

A Continuation Method for Nash Equilibria in Structured Games

P1

P2

P3

E1

E2

C1

C2

B1

C3

B2

R1

L2

P1 E1
B1 B2

E1 P2
B1 B2

P2 E2
B2 B3

E2 P3
B2 B3

B3

R2

L3

(a)

(b)

Figure 6: (a) A two-stage road MAID with three agents is shown divided into cliques. Each
of the four cliques is surrounded by a dashed line, and has three decision nodes
and a chance node. (b) The resultant clique tree.

7.2.3 Using Bayesian Network Inference
Our analysis above reduces the required computations significantly. Rather than computing
a separate expectation for every pair of sequences h, h0 , as might at first have seemed
necessary, we need only compute one marginal joint distribution over the variables in {Un }∪
D n ∪ D n0 for every pair of agents n, n0 . This marginal joint distribution is the one defined
by the Bayesian network Bσ . Naively, this computation requires that we execute Bayesian
network inference |N |2 times: once for each ordered pair of agents n, n0 . In fact, we can
exploit the structure of the MAID to perform this computation much more efficiently. The
basis for our method is the standard clique tree algorithm of Lauritzen and Spiegelhalter
(1998). The clique tree algorithm is fairly complex, and a detailed presentation is outside
the scope of this paper. We choose to treat the algorithm as a black box, describing
only those of its properties that are relevant to understanding how it is used within our
computation. We note that these details suffice to allow our method to be implemented
using one of the many off-the-shelf implementations of the clique tree algorithm. A reader
wishing to understand the clique tree algorithm or its derivation in more detail is referred
to the reference by Cowell et al. (1999) for a complete description.
A clique tree for a Bayesian network B is a data structure defined over an undirected
tree with a set of nodes C. Each node Ci ∈ C corresponds to some subset of the variables
in B, typically called a clique. The clique tree satisfies certain important properties. It
must be family preserving: for each node X in B, there exists a clique Ci ∈ C such that
(X ∪ PaX ) ⊆ Ci . It also satisfies a separation requirement: if C2 lies on the unique path
from C1 to C3 , then, in the joint distribution defined by B, the variables in C1 must be
conditionally independent of those in C3 given those in C2 .
The division of the 3-agent road MAID into cliques is shown in Figure 7.2.3(a). This
MAID has 4 cliques. Notice that every family is contained in a clique (including the families
of chance nodes and utility nodes). The clique tree for this MAID is shown in Figure 7.2.3(b).
483

Blum, Shelton, & Koller

Each clique maintains a data structure called a potential, a table with an entry for each
joint assignment to the variables in the clique. A table of this sort is more generally called
a factor. Inference algorithms typically use two basic operations on factors: factor product,
and factor marginalization. If F and G are two factors over the (possibly overlapping) sets
of variables X and Y , respectively, then we can define the product FG to be a new factor
over X ∪ Y . The entry in FG for a particular assignment to the variables in X ∪ Y is the
product of the entries in F and G corresponding to the restriction of the assignment to X
and Y , respectively. This notion of multiplication corresponds to the way that conditional
probability distributions are multiplied. We can also marginalize, or sum, a variable X out
of a factor F over X in the same way in which
Pwe would sum a variable out of a joint
probability distribution. The result is a factor XP
F over the variables in X\{X}. The
entry for a particular assignment to the variables in X F is equal to the sum of all entries
in F compatible with that assignment — one for each value of X.
Because a factor has an entry for every joint assignment to its variables, the size of
the potential for Ci is exponential in |Ci |. The clique tree inference algorithm proceeds by
passing messages, themselves factors, from one clique to another in the tree. The messages
are used to update the potential in the receiving clique by factor multiplication. After a
process in which messages have been sent in both directions over each edge in the tree, the
tree is said to be calibrated ; at this point, the potential of every clique Ci contains precisely
the joint distribution over the variables in Ci according to B (for details, we refer to the
reference by Cowell et al., 1999).
We can use the clique tree algorithm to perform inference over Bσ . Consider the final
decision node for agent n. Due to the perfect recall assumption, all of n’s previous decisions
and all of their parents are also parents of this decision node. The family preservation
property therefore implies that D n is fully contained in some clique. It also implies that
the family of each utility node is contained in a clique. The expectation of Equation (14)
thus requires the computation of the joint distribution over three cliques in the tree: the one
containing PaUn , the one containing D n , and the one containing D n0 . We need to compute
this joint distribution for every pair of agents n, n0 .
The first key insight is that we can reduce this problem to one of computing the joint marginal distribution for all pairs of cliques in the tree. Assume we have computed PB (Ci , Cj )
for every pair of cliques Ci , Cj . Now, consider any triple of cliques Ci , Cj , Ck . There are two
cases: either one of these cliques is on the path between the other two, or not. In the first
case, assume without loss of generality that Cj is on the path from Ci to Ck . In this case, by
the separation requirement, we have that PB (Ci , Cj , Ck ) = PB (Ci , Cj )PB (Cj , Ck )/PB (Cj ).
In the second case, there exists a unique clique C ∗ that lies on the path between any pair
of these cliques. Again, by the separation property, C ∗ renders these cliques conditionally
independent, so we can compute
PB (Ci , Cj , Ck ) =

X PB (Ci , C ∗ )PB (Cj , C ∗ )PB (Ck , C ∗ )
PB (C ∗ )2

C∗

.

(15)

Thus, we have reduced the problem to one of computing the marginals over all pairs of
cliques in a calibrated clique-tree. We can use dynamic programming to execute this process
efficiently. We construct a table that contains PB (Ci , Cj ) for each pair of cliques Ci , Cj . We
construct the table in order of length of the path from Ci to Cj . The base case is when Ci and
484

A Continuation Method for Nash Equilibria in Structured Games

Cj are adjacent in the tree. In this case, we have that PB (Ci , Cj ) = PB (Ci )PB (Cj )/PB (Ci ∩
Cj ). The probability expressions in the numerator are simply the clique potentials in
the calibrated tree. The denominator can be obtained by marginalizing either of the two
cliques. In fact, this expression is computed as a byproduct of the calibration process, so
the marginalization is not required. For cliques Ci and Cj that are not adjacent, we let Ck
be the node adjacent to Cj on the path from Ci to Cj . The clique Ck is one step closer
to Ci , so, by construction, we have already computed P (Ci , Ck ). We can now apply the
separation property again:

PB (Ci , Cj ) =

X PB (Ci , Ck )PB (Ck , Cj )
PB (Ck )

Ck

.

(16)

7.2.4 Computational Complexity
Theorem 11. The computation of ∇V G (σ) can be performed in time O(`2 d3 + u|N |d4 ),
where ` is the number of cliques in the clique tree for G, d is the size of the largest clique
(the number of entries in its potential), |N | is the number of agents, and u is the total
number of utility nodes in the game.
Proof. The cost of calibrating the clique tree for Bσ is O(`d). The cost of computing
Equation (16) for a single pair of cliques is O(d3 ), as we must compute a factor over the
variables in three cliques before summing out. We must perform this computation O(`2 )
times, once for each pair of cliques, for a total cost of O(`2 d3 ). We now compute marginal
joint probabilities over triples of cliques PaUni , D n , D n0 for every utility node Uni and every
agent n0 other than n. There are u(|N | − 1) such triples. Computing a factor over the
variables in three cliques may first require computing a factor over the variables in four
cliques, at a cost of O(d4 ). Given this factor, computing the expected value of the utility
node takes time O(d3 ), which does not affect the asymptotic running time. The total cost for
computing all the marginal joint probabilities and expected utilities is therefore O(u|N |d4 ),
and the total cost for computing ∇V G (σ) is O(`2 d3 + u|N |d4 ).

With this method, we have shown that a single iteration in the continuation method
can be accomplished in time exponential in the induced width of the graph — the number
of variables in the largest clique in the clique tree. The induced width of the optimal clique
tree — the one with the smallest maximal clique — is called the treewidth of the network.
Although finding the optimal clique tree is, itself, an NP-hard problem, good heuristic
algorithms are known (Cowell et al., 1999). In games where interactions between the agents
are highly structured (the road MAID, for example), the size of the largest clique can be a
constant even as the number of agents grows. In this case, the complexity of computing the
Jacobian grows only quadratically in the number of cliques, and hence also in the number
of agents. Note that the matrix adjoint operation takes time cubic in m, which is at least
|N |, so a single step along the path actually has cubic computational cost.
485

Blum, Shelton, & Koller

1800

400
cont
IPA+cont
VK

1600
1400

300

1200

250
seconds

seconds

cont
IPA+cont
VK

350

1000
800

200
150

600

100

400

50

200
0
0

20

40
60
# of agents

80

0

100

10

15

(a)

20

25
30
# of agents

35

40

45

(b)

4

7

x 10

0.01
Cumulative

6

0.009

Terminating run

0.008

5

0.007
seconds/iteration

# of iterations

cont
cubic fit

4
3

0.006
0.005
0.004
0.003

2

0.002
1
0.001
0

10

15

20
# of agents

0
5

25

(c)

10

15
20
# of agents

25

(d)

Figure 7: Results for 2-by-L road game with rock-paper-scissors payoffs: (a) running time.
Results for road game with random payoffs: (b) running time; (c) number of
iterations of cont; (d) average time per iteration of cont.

8. Results
We performed run-time tests of our algorithms on a wide variety of both graphical games
and MAIDs. Tests were performed on an Intel Xeon processor running at 3 GHz with 2
GB of RAM, although the memory was never taxed during our calculations.
8.1 Graphical Games
For graphical games, we compared two versions of our algorithm: cont, the simple continuation method, and IPA+cont, the continuation method with IPA initialization. We tested
the hybrid equilibrium refinement algorithm of Vickrey and Koller (2002) (VK hereafter)
486

A Continuation Method for Nash Equilibria in Structured Games

4

600

5
cont
IPA+cont
VK

500

Cumulative

4.5

Terminating run

4
3.5
# of iterations

400
seconds

x 10

300

200

3
2.5
2
1.5
1

100

0.5
0

5

10

15

20

25
30
# of agents

35

40

0

45

5

10

15

(a)

20
25
# of agents

30

35

40

(b)
4

250

6

x 10

cont

Cumulative

IPA+cont

5

200

Terminating run

VK

# of iterations

seconds

4
150

100

3

2
50

0

1

5

10

15

20
# of agents

25

30

35

0

(c)

10

15

20
# of agents

25

(d)

Figure 8: Results for ring game with random payoffs: (a) running time; (b) number of
iterations of cont. Results for L-by-L grid game with random payoffs: (c) running
time; (d) number of iterations of cont.

for comparison, with the same parameters that they used. The VK algorithm only returns
-equilibria; no exact methods exist which are comparable to our own.
Our algorithms were run on two classes of games defined by Vickrey and Koller (2002)
and two additional classes. The road game of Example 3, denoting a situation in which
agents must build land plots along a road, is played on a 2-by-L grid; each agent has three
actions, and its payoffs depend only on the actions of its (grid) neighbors. Following VK,
we ran our algorithm on road games with additive rock-paper-scissors payoffs: each agent’s
payoffs are a sum of payoffs from independent rock-paper-scissors games with each of its
neighbors. This game is, in fact, a polymatrix game, and hence is very easy to solve using our
methods. In order to test our algorithms on more typical examples, we experimented with
487

Blum, Shelton, & Koller

road games in which the entries of the payoff matrix for each agent were chosen uniformly
at random from [0, 1]. We also experimented with a ring graph with three actions per
agent and random payoffs. Finally, in order to test games with increasing treewidth, we
experimented with grid games with random payoffs. These are defined in the same manner
as the road games, except that the game graph is an L-by-L grid.
For each class of games, we chose a set of game sizes to run on. For each, we selected
(randomly in cases where the payoffs were random) a set of 20 test games to solve. We
then solved each game using cont, IPA+cont, and VK. For cont, we started with a different
random perturbation vector each time and recorded the time and number of iterations
necessary to reach the first equilibrium. For IPA+cont, we started with a different initial
strategy profile for IPA each time and recorded the total time for both IPA and cont to reach
the first equilibrium.
All equilibria found by our algorithm had error at most 10−12 , essentially machine
precision. The hybrid refinement algorithm of VK found -equilibria with average error of
about 10−4 for road games with rock-paper-scissors payoffs, 0.01 for road games and grid
games with random payoffs, and 0.03 for ring games with random payoffs, although the
equilibria had error as high as 0.05 for road games and 0.1 for ring games.
For smaller games, the algorithms always converged to an equilibrium. In some larger
games, cont or IPA detected that they had entered a cycle and terminated without finding
an equilibrium. By maintaining a hash table of support cells they have passed through
already, both cont and IPA are able to detect when they have entered a support cell for the
second time. Although this is not a sure sign that they have entered a cycle, it is a strong
indicator. When potential cycles were detected, the algorithms were restarted with new
random initialization values. Note that cycles in the execution of cont can never arise if
the algorithm does not stray from the path dictated by the theory of GW, so that random
restarts reflect a failure to follow the path accurately.
When an equilibrium was eventually found, the cumulative time for all the random
restarts was recorded. The error bars in the running time graphs show the variance due to
the number of random restarts required, the choices of initialization values, and, for random
games, the choice of game.
Random restarts were required in 29% of the games we tested. On average, 2.2 restarts
were necessary for these games. Note that this figure is skewed by the larger games, which
occasionally required many restarts; the largest games sometimes required 8 or 9 restarts.
In a few large graphical games (10 random road games and 8 random ring games), IPA did
not converge after 10 restarts; in these cases we did not record results for IPA+cont. cont
always found an equilibrium within 10 restarts. Our results are shown in Figures 7(a,b,c,d)
and Figures 8(a,b,c).
For random roads, we also plotted the number of iterations and time per iteration
for cont in Figures 7(c,d). The number of iterations varies based both on the game and
perturbation vector chosen. However, the time per iteration is almost exactly cubic, as
predicted. We note that, when IPA was used as a quick-start, cont invariably converged
immediately (within a second) — all of the time was spent in the IPA algorithm.
In the road games, our methods are more efficient for smaller games, but then become more costly. Due to the polymatrix nature of the rock-paper-scissors road games,
the IPA+cont algorithm solves them immediately with the Lemke-Howson algorithm, and
488

A Continuation Method for Nash Equilibria in Structured Games

is therefore significantly less expensive than VK. In the random ring games, our algorithms
are more efficient than VK for smaller games (up to 20–30 agents), with IPA+cont performing considerably better than cont. However, as with road games, the running time of
our algorithms grows more rapidly than that of VK, so that for larger games, they become
impractical. Nevertheless, our algorithms performed well in games with up to 45 agents
and 3 actions per agent, which were previously intractable for exact algorithms. For the
L-by-L grid games, our algorithm performed much better than the VK algorithm (see Figures 8(c,d)), with and without IPA quick-start. This reflects the fact that the running-time
complexity of our algorithms does not depend on the treewidth of the graph.

# of equilibria

80
60
40
20
0
20
10

15

8
10

6
5

# of players

4
2
# of runs

Figure 9: The number of unique equilibria found as a function of the size of the game and
the number of runs of the algorithm, averaged over ten random ring games.

We also examined the number of equilibria found by the IPA+cont algorithm. We ran
IPA+cont on the ring graphical game for differing numbers of agents. For each number of
agents, we fixed 10 random games, ran the algorithm 10 times on each game, and recorded
the cumulative number of unique equilibria found. The average number of equilibria found
over the 10 games for each number of agents is plotted in figure 9. For small games (with
presumably a small number of equilibria), the number of equilibria found quickly saturated.
For large games, there was an almost linear increase in the number of equilibria found by
each subsequent random restart, implying that each run of the algorithm produced a new
set of solutions.
8.2 MAIDs
The previous computational method for MAIDs (Koller & Milch, 2001) stopped at strategic
relevance: each SCC was converted into an equivalent extensive-form game and solved using
standard methods. Our algorithm takes advantage of further structure once a game has already been decomposed according to strategic relevance. All of our test cases were therefore
selected to have relevance graphs consisting of a single strongly connected component.
489

Blum, Shelton, & Koller

A

NA

B

AB

NB

C

BC
(a)

A

B

C

(b)

Figure 10: (a) The chain game and (b) its strategic relevance graph for the case of three
agents (A, B, and C).

In order to ascertain how much difference our enhancements made, we compared the
results for our MAID algorithm, MAID cont, to those achieved by converting the game to
extensive-form and running both EF cont, the extensive-form version of cont as specified by
GW, and Gambit (McKelvey, McLennan, & Turocy, 2004), a standard game theory software
package. The time required for conversion to extensive form is not included in our results.
We ran our algorithms on two classes of games, with varying sizes. The first, to which
we refer as the chain game, alternates between decision and chance nodes (see Figure 10).
Each decision node belongs to a different agent. Each agent has two utility nodes, each
connected to its own decision node and to a neighbor’s (except for the end agents, who have
one utility node for their single neighbor). There are three actions at each decision node.
All probability tables and payoff matrices are chosen at uniformly at random. The second
class is the two-stage road building game from Example 5, shown in Figure 2(b). In this
class, we chose payoffs carefully, by hand, to ensure non-trivial mixed strategy equilibria.
We ran on chain games of all sizes between 2 and 21, and road games of all sizes between
2 and 9. For each size, we randomly selected 20 perturbation vectors and 20 games (all
20 road games were the same, since payoffs were set by hand, and all 20 chain games had
payoffs randomly assigned). We then tested the algorithms on these games, initialized with
these perturbation vectors, and averaged across test cases. The timing results appear in
Figures 11(a,b). The error bars reflect variance due to the choice of game (in the chain
games), the choice of perturbation vector, and the number of random restarts required.
In some cases, as with the graphical game tests, MAID cont failed to find an equilibrium, terminating early because it detected that it had entered a cycle. In these cases,
it was restarted with a new perturbation vector until it successfully terminated. When
an equilibrium was eventually found, the cumulative time for all the random restarts was
recorded. Over the course of our test runs, only two chain games required a random restart.
Both were of size 7. Our algorithms failed more frequently on road games; the spike for
road games of size 8 reflects the fact that the games of this size required, on average, 1.2
490

A Continuation Method for Nash Equilibria in Structured Games

200

900
cont
EF cont
gambit

180

800

160

cont
EF cont
gambit

700

140

600
seconds

seconds

120
100

500
400

80
300

60

200

40

100

20
0
2

4

6

8

10

12
14
# of agents

16

18

20

0
2

22

3

4

5

(a)

6
# of agents

7

8

9

7

8

9

(b)

1000

0.4
cont
cubic fit

900

0.35

800
0.3
seconds/iteration

# of iterations

700
600
500
400

0.25
0.2
0.15

300
0.1
200
0.05

100
0
2

3

4

5

6
# of agents

7

8

0
2

9

(c)

3

4

5

6
# of agents

(d)

Figure 11: Results for MAIDs: (a) Running times for the chain MAID. Results for two-stage
road MAID: (b) running time; (c) number of iterations; (d) time per iteration.

random restarts before an equilibrium was found. Strangely, MAID cont was much more
successful on the road game of size 9, succeeding without random restarts in all but two
cases.
We tested Gambit and EF cont only on smaller games, because the time and memory
requirements for testing on larger ones were beyond our means. Our results show that, while
EF cont is a faster algorithm than Gambit for extensive-form games, it is inadequate for the
larger MAIDs that we were able to solve with MAID cont. This is not at all surprising; a
road game of size 9 has 26 decision or chance nodes, so the equivalent extensive-form game
tree has 226 ≈ 67 million outcome nodes. For MAIDs of this size, the Bayesian network
inference techniques that we have used become necessary.
491

Blum, Shelton, & Koller

For all MAIDs, realization probabilities were constrained to be at least 10−4 (i.e., we
found -perfect equilibria with  = 10−4 ). The accuracy of these equilibria was within 10−12 ,
or machine precision.
As with graphical games, we recorded the number of iterations until convergence as well
as the time per iteration for MAID cont. The results appear in Figures 11(c,d). The time
per iteration is fit well by a cubic curve, in accordance with our theoretical predictions. The
variance is primarily due to the execution of the retraction operator, whose running time
depends on the number of strategies in the support.

9. Discussion and Conclusions
We have described here two adaptations of the continuation method algorithms of GW,
for the purpose of accelerated execution on structured games. Our results show that these
algorithms represent significant advances in the state of the art of equilibrium computation
for both graphical games and MAIDs.
9.1 Related Work on Graphical Games
In the last few years, several papers have addressed the issue of finding equilibria in structured games. For graphical games, the exact algorithms proposed so far apply only to games
where the interaction structure is an undirected tree, and where each agent has only two
possible actions. Kearns et al. (2001) provide an exponential-time algorithm to compute
all exact equilibria in such a game, and Littman et al. (2002) provide a polynomial-time
algorithm to compute a single exact equilibrium. For this very limited set of games, these
algorithms may be preferable to our own, since they come with running-time guarantees.
However, it is yet to be tested whether these algorithms are, in fact, more efficient in practice. Moreover, our methods are applicable to fully general games, and our results indicate
that they perform well.
More effort has been focused on the computation of -equilibria in general graphical
games. A number of algorithms have recently been proposed for this task. Most of these
use a discretized space of mixed strategies: probabilities must be selected from a grid
in the simplex, which can be made arbitrarily fine. For computational reasons, however,
this grid must typically be quite coarse, as the number of grid points to consider grows
exponentially with the number of actions per agent. Most of these methods (implicitly or
explicitly) define an equilibrium as a set of constraints over the discretized strategy space,
and then use some constraint solving method: Kearns et al. (2001) use a tree-propagation
algorithm (KLS); Vickrey and Koller (2002) use standard CSP variable elimination methods
(VK1); and Ortiz and Kearns (2003) use arc-consistency constraint propagation followed
by search (OK). Vickrey and Koller (2002) also propose a gradient ascent algorithm (VK2),
and provide a hybrid refinement method that can, with further computation, reduce the
equilibrium error.
As with the exact methods, the KLS algorithm is restricted to tree-structured games,
and comes without experimental running time results (although it is guaranteed to run in
polynomial time). Kearns et al. (2001) give a suggestion for working on a non-tree graph
by constructing the junction tree and passing messages therein. However, the necessary
computations are not clear and potentially very expensive.
492

A Continuation Method for Nash Equilibria in Structured Games

The VK1 algorithm is applicable to graphical games of arbitrary topology, with any
number of actions per agent. It takes time exponential in the treewidth of the graph. If
the treewidth is constant, then it scales linearly with the number of agents; however, our
results show that it very quickly becomes infeasible if the treewidth expands (as in the grid
game).
Both of these methods come with complexity guarantees, which depend on the treewidth
of the graph. The others (OK and VK2, as well as our algorithm) are insensitive to treewidth
— a single iteration takes time polynomial in the size of the game representation (and
hence exponential only in the maximum degree of the graph). However, they all require an
unknown number of iterations to converge. Corollary 7 shows that, in general, computation
of equilibria with discretized strategies in games with fixed degree is hard. Thus, the lack
of complexity guarantees for these methods is not surprising.
Nonetheless, experimental results for OK seem promising — they indicate that, on
average, relatively few iterations are required for convergence. Results indicate that OK
is capable of solving grid games of at least 100 agents (although in these cases  was as
large as 0.2, not much better than in a random fully mixed strategy profile). However, no
running time results are provided.
VK2 also exhibits strong experimental results. Vickrey and Koller (2002) have successfully found -equilibria in games of up to 400 agents, with errors of up to 2% of the maximal
payoff.
The main drawback to these algorithms is that they only compute -equilibria. An equilibrium may be sufficient for certain applications: if the utility functions are themselves
approximate, an agent certainly might be satisfied with an -best response; and if we make
the assumption that it is slightly costly for agents to change their minds, each agent might
need an incentive greater than  to deviate. However, -equilibria do bring their own set
of problems. The primary one is that there is no guarantee of an exact equilibrium in the
neighborhood of an -equilibrium. This can make it very difficult to find -equilibria with
small values of ; attempts to refine a given -equilibrium may fail. The lack of a nearby
Nash equilibrium also implies a certain instability. If some agent is unsatisfied with the
-equilibrium, play may deviate quite far from it. Finally, -equilibria are more numerous
than Nash equilibria (uncountably so, in general). This exacerbates the difficulty an agent
faces in choosing which equilibrium to play.
The algorithms for computing -equilibria are frequently faster than our own, especially
when the approximations are crude or the games have more than 50 or so agents. However,
the exact equilibria found by our algorithms are more satisfying solutions, and our results
show that the performance of our algorithm is comparable to that of approximate methods
in most cases. Surprisingly, for many games, running time results show that ours is the
fastest available, particularly in the case of games with large treewidth, such as the grid
game in our test cases. Furthermore, since we can use any approximate equilibrium as a
starting point for our algorithm, advances in approximate methods complement our own
method. The hybrid algorithm of Vickrey and Koller (2002) turns out to be unsuited
to this purpose, as it tends not to remove any pure strategies from the support, but it
is interesting to see whether other methods (including those listed above) might be more
effective. It remains to be seen how small  must be for our methods to reliably refine an
approximate equilibrium.
493

Blum, Shelton, & Koller

9.2 Related Work on MAIDs
Koller and Milch (2001) (KM) define a notion of dependence between agents’ decisions
(s-relevance), and provide an algorithm that can decompose and solve MAIDs based on
this fairly coarse independence structure. Our algorithm is able to exploit finer-grained
structure, resolving an open problem left by KM. In general, our method will not automatically exploit the same structure obtained by decomposing the game into its relevance
components, and so our methods are best regarded as a complement to those of KM; after decomposition according to s-relevance, our algorithm can be applied to find equilibria
efficiently in the decomposed problems. Running time results indicate that our methods
are significantly faster than previous standard algorithms for extensive-form games. This is
unsurprising, since the game representation of our test cases is exponentially larger in the
number of players when converted to extensive-form.
Vickrey (2002) proposes an approximate hill-climbing algorithm for MAIDs that takes
advantage of the same sort of fine-grained structure that we do: Bayesian network inference
is employed to calculate expected utility as one component of the score function for a single
iteration. A constraint-satisfaction approach is also proposed. However, these proposals
were never implemented, so it is hard to determine what quality equilibria they would find
or how quickly they would find them.
La Mura (2000) proposes a continuation method for finding one or all equilibria in
a G net, a representation that is very similar to MAIDs. This proposal only exploits a
very limited set of structural properties (a strict subset of those exploited by KM). This
proposal was also never implemented, and several issues regarding non-converging paths
seem unresolved.
Our algorithm is therefore the first to be able to exploit the finer-grained structure of
a MAID. Moreover, our algorithm, applied in conjunction with the decomposition method
of KM, is able to take advantage of the full known independence structure in a MAID. A
potential drawback is the requirement that strategies be -perturbed. However, decreasing
 incurs no additional computational cost, although there are limits imposed by machine
precision. Perfect equilibria — a highly desirable refinement of Nash equilibria, defined to
be the limit of a sequence of -perturbed equilibria as  goes to zero — can therefore be
computed effectively by our algorithm with little or no additional computational cost. In
this sense, our use of perturbed strategies is advantageous. We have not implemented a
local search algorithm to find an exact perfect equilibrium in the neighborhood of a found
-perturbed equilibrium, although it should be straightforward to do so.
9.3 Conclusion and Further Work
We have presented two related algorithms for computing exact equilibria in structured
games. Our algorithms are based on the methods of GW, but perform the key computational
steps in their methods much more efficiently by exploiting game structure. Our approach
yields the first exact algorithm to take advantage of structure in general graphical games
and the first algorithm to take full advantage of the independence structure of a MAID.
These algorithms are capable of computing exact equilibria in games with large numbers of
agents, which were previously intractable for exact methods.
494

A Continuation Method for Nash Equilibria in Structured Games

Our algorithms come without theoretical running time bounds, but we have noticed certain interesting trends. In both the graphical game and the MAID version of our algorithm,
each iteration executes in time polynomial in the number of agents, so we have examined the
number of iterations required for convergence. Our adaptive step size technique decreases
the number of random restarts required to find an equilibrium, but increases the number
of iterations required to cross a support cell in larger games. When adaptive step size is
disabled, we have noticed that the number of iterations required, averaged across games
with random payoffs, seems to grow approximately linearly. Intuitively, it makes sense that
the number of iterations should be at least linear: starting from a pure strategy profile, a
linear number of actions (in the number of agents) must enter the support in order for us
to reach a general strategy profile. Each support boundary requires at least one iteration of
our algorithm. It is somewhat surprising, however, that the number of iterations required
does not grow more quickly. It is an interesting open problem to analyze the number of
iterations required for convergence.
In very large games, the tendency of our algorithm to cycle increases. This phenomenon
can be attributed, partially, to the cumulative effect of “wobbling”: after a great number
of wobbles, it is possible that the path has been altered sufficiently that it does not pass
through an equilibrium. We have noticed that some games seem intrinsically harder than
others, requiring many random restarts before convergence. For very large games, the
overall running time of our algorithm is therefore quite unpredictable.
Our algorithms might be improved in a number of ways. Most importantly, the continuation method would profit greatly from more sophisticated path-following methods; in
a number of cases, cont or MAID cont failed to find an equilibrium because it strayed too
far from the path. Better path-following techniques might greatly increase the reliability
of our algorithms, particularly if they obviated the need for “wobbles,” which negate GW’s
theoretical guarantee of the convergence of the continuation method.
There are also a number of theoretical questions about the algorithms of GW that
remain unresolved. Nothing is known about the worst-case or average-case running time
of IPA, and no theoretical bounds exist on the number of iterations required by cont. It is
interesting to speculate on how the choice of perturbation ray might affect the execution
of the algorithm. Can the algorithm be directed toward particular equilibria of interest
either by a careful selection of the perturbation ray or by some change in the continuation
method? Is there a way of selecting perturbation rays such that all equilibria will be found?
Is there a way of selecting the perturbation ray so as to speed up the execution time?
Several improvements might be made to MAID cont. We have not adapted IPA for use in
MAIDs, but it should be possible to do so, making use of the generalized Lemke algorithm
of Koller, Megiddo, and von Stengel (1996) to solve intermediate linearized MAIDs. The
computation of ∇V G might also be accelerated using a variant of the all-pairs clique tree
algorithm that only computes the potentials for pairs of sepsets — sets of variables shared
by adjacent cliques — rather than pairs of cliques.
Our work suggests several interesting avenues for further research. In fact, after the
initial publication of these results (Blum, Shelton, & Koller, 2003), at least one further
application of our techniques has already been developed: Bhat and Leyton-Brown (2004)
have shown that an adaptation of cont can be used to efficiently solve a new class of structured games called action-graph games (a generalization of local effect games as presented
495

Blum, Shelton, & Koller

in Leyton-Brown & Tennenholtz, 2003). We believe that these games, and other structured
representations, show great promise as enablers of new applications for game theory. They
have several advantages over their unstructured counterparts: they are well-suited to games
with a large number of agents, they are determined by fewer parameters, making it feasible
for human researchers to fully specify them in a meaningful way, and their built-in structure
makes them a more intuitive medium in which to frame structured, real-world scenarios.
However, to avoid the computational intractability of the general problem, each new class
of structured games requires a new algorithm for equilibrium computation. We hypothesize
that cont and IPA are an excellent starting point for addressing this need.

Acknowledgments. This work was supported by ONR MURI Grant N00014-00-1-0637,
and by Air Force contract F30602-00-2-0598 under DARPA’s TASK program. Special
thanks to Robert Wilson, for kindly taking the time to guide us through the details of his
work with Srihari Govindan, and to David Vickrey, for aiding us in testing our algorithms
alongside his. We also thank the anonymous referees for their helpful comments.
496

A Continuation Method for Nash Equilibria in Structured Games

Appendix A. Table of Notation
Notation for all games
N
set of agents
σn
strategy for agent n
Σn
strategy space for agent n
σ
strategy profile
Σ
space of strategy profiles
σ−n
strategy profile σ restricted to agents other than n
Σ−n
space of strategy profiles for all agents other than n
(σn , σ−n ) strategy profile in which agent n plays strategy σn and all other agents act
according to σ−n
Gn (σ)
expected payoff to agent n under strategy profile σ
G
V (σ)
vector deviation function
R
retraction operator mapping points to closest valid strategy profile
F
continuation method objective function
λ
scale factor for perturbation in continuation method
w
free variable in continuation method
Notation for normal-form games
an
action for agent n
An
set of available actions for agent n
a
action profile
A
set of action profiles
a−n
action profile a restricted to agents other than n
A−n
space of action profiles for agents other than n
Notation for extensive-form games
z
leaf node in game tree (outcome)
Z
set of outcomes
i
information set
In
set of information sets for agent n
A(i)
set of actions available at information set i
Hn (y)
sequence (history) for agent n determined by node y
Zh
set of outcomes consistent with sequence (history) h
b(a|i)
probability under behavior profile b that agent n will choose action a at i
σn (z)
realization probability of outcome z for agent n
Notation for graphical games
Famn
set of agent n and agent n’s parents
f
Σ−n
strategy profiles of agents in Famn other than n
f
A−n
space of action profiles of agents in Famn other than n
Notation for MAIDs
Dni
decision node with index i belonging to agent n
Uni
utility node with index i belonging to agent n
PaX
parents of node X
dom(S) joint domain of variables in set S
497

Blum, Shelton, & Koller

C1

C2

a

C3

a
b

a
b

c

b
c

c

Figure 12: Reduction of the 3SAT instance (¬a ∨ b ∨ c) ∧ (a ∨ ¬b ∨ c) ∧ (¬a ∨ ¬b ∨ ¬c) to a
graphical game.

Appendix B. Proof of Theorem 6
Proof. The proof is by reduction from 3SAT. For a given 3SAT instance, we construct a
graphical game whose equilibria encode satisfying assignments to all the variables.
Let C = {c1 , c2 , . . . , cm } be the clauses of the 3SAT instance in question, and let V =
{v1 , ¬v1 , v2 , ¬v2 , . . . , vn , ¬vn } be the set of literals. If a variable appears in only one clause, it
can immediately be assigned so as to satisfy that clause; therefore, we assume that variables
appear in at least two clauses.
We now construct the (undirected) graphical game. For each clause, ci , we create an
agent Ci connected to Ci−1 and Ci+1 (except C1 and Cm , which only have one clause
neighbor). We also create agents Vi` for each literal ` in ci (there are at most 3). If, for
example, ci is the clause (¬v1 ∨ v2 ), it has agents Vi¬v1 and Viv2 . We connect each of these
to Ci . For every variable v, we group all agents Viv and Vj¬v and connect them in a line,
the same way we connected clauses to each other. The order is unimportant.
Clause agents now have at most 5 neighbors (two clauses on either side of them and three
literals) and literal agents have at most 3 neighbors (two literals on either side of them and
one clause). This completely specifies the game topology. As an example, Figure 12 shows
the graphical game corresponding to the 3SAT problem (¬a∨b∨c)∧(a∨¬b∨c)∧(¬a∨¬b∨¬c).
Now we define the actions and payoff structure. Each agent can be interpreted as a
Boolean variable, and has two actions, true and false, which correspond to the Boolean
values true and false. Intuitively, if a clause Ci plays true, it is satisfied. If an agent
Viv plays true, where v is a non-negated variable, then v is assigned to be true. If Vj¬v
plays true, then v is assigned to be false.
The payoff matrix for a clause agent Ci is designed to ensure that if one clause is
unsatisfied, the entire 3SAT instance is marked as unsatisfied. It can best be expressed in
pseudo-code, as follows:
if any of Ci
’s clause neighbors play false then
1 for playing false
payoff is
0 for playing true
else if at least one of Ci ’s literals plays true (Ci is satisfied) then
498

A Continuation Method for Nash Equilibria in Structured Games


payoff is

2
2

for playing false
for playing true

else
(Ci is unsatisfied)

1 for playing false
payoff is
0 for playing true
end if
The payoff matrix for a literal agent Vi` is designed to encourage agreement with the
other literals along the line for the variable v(`) associated with `. It can be described in
pseudo-code as follows:
if the parent
 clause Ci plays false then
1 for playing consistently with a false assignment to v(`)
payoff is
0 for playing the opposite
else if Vi` ’sliteral neighbors all play consistently with a single assignment to v(`) then
2 for playing consistently with neighbors
payoff is
0 for playing the opposite
else

2 for playing consistently with a false assignment to v(`)
payoff is
0 for playing the opposite
end if
If the formula does have a satisfying assignment, then there is a pure equilibrium in
which each literal is consistent with the assignment and all clauses play true; in fact, all
agents receive higher payoffs in this case than in any other equilibrium, so that satisfying
assignments correspond to equilibria with maximum social welfare.
If the parent clauses all play false, then clearly at equilibrium all non-negated literals
must play false and all negated literals must play true. This is the trivial equilibrium. It
remains to show that the trivial equilibrium is the only equilibrium for unsatisfiable formulas, i.e. that any non-trivial equilibrium can be used to construct a satisfying assignment.
We first prove two simple claims.
Claim 11.1. In any Nash equilibrium, either all clauses play true with probability one or
all clauses play false with probability one.
Proof. In no case is it advantageous for a clause to choose true over false, and if a neighbor
clause takes the action false, it is in fact disadvantageous to do so. Thus, if any clause has a
non-zero probability of playing false at an equilibrium, its neighbors, and consequently all
other clauses, must play false with probability one. Therefore, the only possible equilibria
have all clauses playing false or all clauses playing true.
It follows immediately from this claim that every non-trivial equilibrium has all clauses
playing true with probability one.
Claim 11.2. In any non-trivial Nash equilibrium, in a line of literals for the same variable
v, all those literals that play pure strategies must choose them consistently with a single
assignment to v.
499

Blum, Shelton, & Koller

Proof. Since the equilibrium is non-trivial, all clauses play true. Suppose that one of the
literals, V ` , employs the pure strategy corresponding to a false assignment to v. It suffices
to show that in fact all literals in the line must have pure strategies corresponding to a false
0
0
assignment to v. Consider a neighbor V ` of V ` . Either V ` ’s neighbors (one of which is
V ` ) both play consistently with a false assignment to v, in which case V ` must also play
consistently with a false assignment to v, or its neighbors play inconsistently, in which case
0
the else clause of V ` ’s payoff matrix applies and V ` must, again, play consistently with a
false assignment to v. We may proceed all the way through the line in this manner. All
literals in the line must therefore have pure strategies consistent with a false assignment to
v, so there can be no contradicting literals.

Suppose we have a non-trivial equilibrium. Then by Claim 11.1, all clauses must play
true with probability 1. If all of the literals have pure strategies, it is clear that the
equilibrium corresponds to a satisfying assignment: the literals must all be consistent with
an assignment by Claim 11.2, and the clauses must all be satisfied. Some subtleties arise
when we consider mixed strategy equilibria.
Note first that in each clause, the payoff for choosing true is the same as for choosing false in the case of a satisfying assignment to its literals, and is less in the case of an
unsatisfying assignment. Therefore, if there is any unsatisfying assignment with non-zero
probability, the clause must play false.
Consider a single clause Ci , assumed to be choosing true at equilibrium. The mixed
strategies of Ci ’s literals induce a distribution over their joint
Because Ci plays true,
W actions.
`
each joint action with non-zero probability must satisfy ` Vi . If a literal Vi` has a mixed
strategy, consider what will happen if we change its strategy to either one of the possible
pure strategies (true or false). Some of the joint actions with non-zero probability will
be
but the ones that remain will be a subset of the originals, so will still satisfy
W removed,
` . Essentially, the value of ` does not affect the satisfiability of C , so it can be assigned
V
i
` i
arbitrarily.
Thus, if each literal in a line for a certain variable has a mixed strategy, we can assign
the variable to be either true or false (and give each literal in the line the corresponding
pure strategy) without making any of the clauses connected to these literals unsatisfied. In
fact, we can do this if all literals in a line that have pure strategies are consistent with each
other: if there are indeed literals with pure strategies, we assign the variable according to
them. And by Claim 11.2, this will always be the case.

We observe briefly that this constructed graphical game has only a finite number of
equilibria, even if peculiarities in the 3SAT instance give rise to equilibria with mixed
strategies. If all clauses play false, then there is only one equilibrium. If all clauses play true,
then we can remove them from the graph and trim the payoff matrices of the literals
accordingly. Each line of literals is in this case a generic graphical game, with a finite set
of equilibria. The equilibria of the original game must be a subset of the direct product of
these finite sets.
500

A Continuation Method for Nash Equilibria in Structured Games

References
Bhat, N. A. R., & Leyton-Brown, K. (2004). Computing Nash equilibria of action-graph
games. In Proceedings of the Twentieth International Conference on Uncertainty in
Artificial Intelligence.
Blum, B., Shelton, C., & Koller, D. (2003). A continuation method for Nash equilibria in
structured games. In Proceedings of the Eighteenth International Joint Conference on
Artificial Intelligence, pp. 757–764.
Chu, F., & Halpern, J. (2001). On the np-completeness of finding an optimal strategy in
games with common payoff. International Journal of Game Theory, 30, 99–106.
Codenotti, B., & Stefankovic, D. (2005). On the computational complexity of nash equilibria
for (0,1) bimatrix games. Information Processing Letters, 94, 145–150.
Conitzer, V., & Sandholm, T. (2003). Complexity results about Nash equilibria. In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence,
pp. 765–771.
Cowell, R. G., Dawid, A. P., Lauritzen, S. L., & Spiegelhalter, D. J. (1999). Probabilistic
Networks and Expert Systems. Springer-Verlag.
Fudenberg, D., & Tirole, J. (1991). Game Theory. The MIT Press.
Gilboa, I., & Zemel, E. (1989). Nash and correlated equilibria: Some complexity considerations. Games and Economic Behavior, 1, 80–93.
Govindan, S., & Wilson, R. (2002). Structure theorems for game trees. Proceedings of the
National Academy of Sciences, 99 (13), 9077–9080.
Govindan, S., & Wilson, R. (2003). A global Newton method to compute Nash equilibria.
Journal of Economic Theory, 110, 65–86.
Govindan, S., & Wilson, R. (2004). Computing Nash equilibria by iterated polymatrix
approximation. Journal of Economic Dynamics and Control, 28, 1229–1241.
Gül, F., Pearce, D., & Stachetti, E. (1993). A bound on the proportion of pure strategy
equilibria in generic games. Mathematics of Operations Research, 18, 548–552.
Howard, R. A., & Matheson, J. E. (1984). Influence diagrams. In Howard, R. A., & Matheson, J. E. (Eds.), Readings on the Principles and Applications of Decision Analysis,
Vol. 2, pp. 719–762. Strategic Decision Group. article dated 1981.
Kearns, M., Littman, M. L., & Singh, S. (2001). Graphical models for game theory. In
Proceedings of the Seventeenth International Conference on Uncertainty in Artificial
Intelligence, pp. 253–260.
Kohlberg, E., & Mertens, J.-F. (1986). On the strategic stability of equilibria. Econometrica,
54 (5), 1003–1038.
Koller, D., & Megiddo, N. (1992). The complexity of two-person zero-sum games in extensive
form. Games and Economic Bahavior, 4, 528–552.
Koller, D., Megiddo, N., & von Stengel, B. (1996). Efficient computation of equilibria for
extensive two-person games. Games and Economic Behavior, 14, 247–259.
501

Blum, Shelton, & Koller

Koller, D., & Milch, B. (2001). Multi-agent influence diagrams for representing and solving
games. In Proceedings of the Seventeenth International Joint Conference on Artificial
Intelligence, pp. 1027–1034.
Kuhn, H. W. (1953). Extensive games and the problem of information. In Contributions to
the Theory of Games II, eds. H. W. Kuhn and A. W. Tucker, Vol. 28, pp. 193–216.
Princeton University Press, Princeton, NJ.
La Mura, P. (2000). Game networks. In Proceedings of the Sixteenth International Conference on Uncertainty in Artificial Intelligence, pp. 335–342.
Lauritzen, S. L., & Spiegelhalter, D. J. (1998). Local computations with probabilities on
graphical structures and their application to expert systems. Journal of the Royal
Statistical Society, B 50 (2), 157–224.
Lemke, C. E., & Howson, Jr., J. T. (1964). Equilibrium points in bimatrix games. Journal
of the Society of Applied Mathematics, 12 (2), 413–423.
Leyton-Brown, K., & Tennenholtz, M. (2003). Local-effect games. In Proceedings of the
Eighteenth International Joint Conference on Artificial Intelligence, pp. 772–777.
Littman, M. L., Kearns, M., & Singh, S. (2002). An efficient exact algorithm for singly
connected graphical games. In Advances in Neural Information Processing Systems
14, Vol. 2, pp. 817–823.
McKelvey, R. D., & McLennan, A. (1996). Computation of equilibria in finite games. In
Handbook of Computational Economics, Vol. 1, pp. 87–142. Elsevier Science.
McKelvey, R. D., McLennan, A. M., & Turocy, T. L. (2004). Gambit: Software tools for
game theory, version 0.97.07.. http://econweb.tamu.edu/gambit.
Nash, J. (1951). Non-cooperative games. The Annals of Mathematics, 52 (2), 286–295.
Nudelman, E., Wortman, J., Shoham, Y., & Leyton-Brown, K. (2004). Run the GAMUT:
A comprehensive approach to evaluating game-theoretic algorithms. In Third International Conference on Autonomous Agents and Multi-Agent Systems.
Ortiz, L. E., & Kearns, M. (2003). Nash propagation for loopy graphical games. In Advances
in Neural Information Processing Systems 15, Vol. 1, pp. 793–800.
Romanovskii, I. (1962). Reduction of a game with complete memory to a matrix game.
Doklady Akademii Nauk, SSSR 144, 62–64. [English translation: Soviet Mathematics
3, pages 678–681].
Vickrey, D. (2002). Multiagent algorithms for solving structured games. Undergraduate
honors thesis, Stanford University.
Vickrey, D., & Koller, D. (2002). Multi-agent algorithms for solving graphical games. In
Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI),
pp. 345–351.
von Stengel, B. (1996). Efficient computation of behavior strategies. Games and Economic
Behavior, 14, 220–246.
Watson, L. T. (2000). Theory of globally convergent probability-one homotopies for nonlinear programming. SIAM Journal on Optimization, 11 (3), 761–780.

502

Journal of Artificial Intelligence Research 25 (2006) 1-15

Submitted 08/05; published 01/06

Engineering Note
Engineering a Conformant Probabilistic Planner
Nilufer Onder
Garrett C. Whelan
Li Li

nilufer@mtu.edu
gcwhelan@mtu.edu
lili@mtu.edu

Department of Computer Science
Michigan Technological University
1400 Townsend Drive
Houghton, MI 49931

Abstract
We present a partial-order, conformant, probabilistic planner, Probapop which competed in the blind track of the Probabilistic Planning Competition in IPC-4. We explain
how we adapt distance based heuristics for use with probabilistic domains. Probapop
also incorporates heuristics based on probability of success. We explain the successes and
difficulties encountered during the design and implementation of Probapop.

1. Introduction
Probapop is a conformant probabilistic planner that took part in the probabilistic track of
the 4th International Planning Competition (IPC-4). It was the only conformant planner
that competed. In the conformant probabilistic planning paradigm (Hyafil & Bacchus, 2003)
the actions and the state initialization can be probabilistic, i.e., they can have several possible
outcomes annotated by a probability of occurrence. In addition, the planning problem is
conformant, i.e., the planner has to construct the best plan possible without assuming that
the results of the actions performed can be observed. As an example of a conformant
probabilistic planning problem, consider a student applying for graduate studies. Suppose
that the application needs to include several forms prepared by the student and a single
letter of recommendation written by a professor (one letter is sufficient but more than one
letter is acceptable). Further assume that a typical professor in the student’s department
has 80% probability of sending a letter on time. In such a problem if the student asks
one professor for a letter, the probability of having a complete application is 0.8. If the
student observes that the professor has not sent a letter by the due date, there is no way
to complete the application because it would be too late to ask another professor. Thus,
observation actions are useless and the only way the student can increase the chances of
getting a letter is to ask more than one professor to send in a letter. If 9 professors are
asked, the probability of getting a letter is 0.999997 which is very close to 1. Obviously,
asking too many people is costly, therefore the student has to weigh the benefits of increased
probability against the costs of asking several people.
A conformant probabilistic planner’s task is to find the best sequence of actions when
the possible results of actions have predefined probabilities but cannot be observed. In
that regard, conformant probabilistic planners can be classified as non-observable Markov
c
°2006
AI Access Foundation. All rights reserved.

Onder, Whelan & Li

decision processes (NOMDPs) (Boutilier, Dean, & Hanks, 1999). Fully-observable MDPs
(FOMDP) are the other extreme of MDPs where the agent has complete and cost-free
sensors that indicate the current state. Planners that adopt the FOMDP framework can
generate policies that are functions from states to actions. NOMDP based planners can
only generate unconditional sequences of actions based on a predictive model, because the
environment cannot be observed (Boutilier et al., 1999). The middle ground is partially
observable MDPs (POMDPs) and contingency plans where only some of the domain is observable and the execution of actions may depend on the results of observations (Kaelbling,
Littman, & Cassandra, 1998; Majercik & Littman, 1999; Onder & Pollack, 1999; Hansen
& Feng, 2000; Karlsson, 2001; Hoffmann & Brafman, 2005). There are also conformant
planners which model imperfect actions that may have multiple possible results but do not
model probability information (Ferraris & Giunchiglia, 2000; Bertoli, Cimatti, & Roveri,
2001; Brafman & Hoffmann, 2004).
Our work on Probapop is motivated by the incentive to have partial-order planning
as a viable option for conformant probabilistic planning. The main reasons are threefold.
First, partial-order planners have worked very well with parametric or lifted actions, which
are useful in coding large domains. Second, due to its least commitment strategy in step
ordering, partial-order planning (POP) produces plans that are highly parallelizable. Third,
many planners that can handle rich temporal constraints have been based on the POP
paradigm (Smith, Frank, & Jonsson, 2000). Given these advantages, our intuition in the
design of Probapop was to bring together two paradigms that do not model states explicitly:
POP planners do not represent states because they search in a space of plans, and blind
planners cannot observe the state because no observation actions are available.
Our basic approach is to form base plans by using deterministic partial-order planning
techniques, and then to estimate the best way to improve these plans. Recently, the Repop
(Nguyen & Kambhampati, 2001) and Vhpop (Younes & Simmons, 2003) planners have
demonstrated that the same heuristics that speed up non-partial-order planners can be used
to scale up partial-order planning. We show that distance-based heuristics (McDermott,
1999; Bonet & Geffner, 2001) as implemented using “relaxed” plan graphs in partial-order
planners such as Repop and Vhpop can be employed in probabilistic domains. These
heuristics coupled with selective plan improvement heuristics and incremental planning
techniques result in significant advantages. As a result, Probapop makes partial-order
planning feasible in probabilistic domains. Our work on Probapop has been invaluable
in understanding and identifying the key solutions to issues in probabilistic conformant
planning.

2. Probapop and Partial-Order Planning
For partial-order probabilistic planning, we implemented the Buridan (Kushmerick, Hanks,
& Weld, 1995) probabilistic planning algorithm on top of Vhpop (Younes & Simmons,
2003), a recent partial-order planner. A partially ordered plan π is a 6-tuple, <STEPS,
BIND, ORD, LINKS, OPEN, UNSAFE>, representing sets of actions, binding constraints, ordering constraints, causal links, open conditions, and unsafe links, respectively. A binding
constraint is a constraint between action parameters and other action parameters or ground
literals. An ordering constraint Si ≺ Sj represents the fact that step Si precedes Sj . A
2

Engineering a Conformant Probabilistic Planner

causal link is a triple < Si , p, Sj >, where Si is the producer step, Sj is the consumer step
and p represents the condition supported by Si for Sj . An open condition is a pair < p, S >,
where p is a condition needed by step S. A causal link < Si , p, Sj > is unsafe if the plan
contains a threatening step Sk such that Sk has ¬p among its effects, and Sk may intervene
between Si and Sj . Open conditions and unsafe links are collectively referred to as flaws.
A planning problem is a quadruple < D, I, G, T >, where, D is a domain theory consisting
of (probabilistic) operators, the initial state I is a probability distribution over states, G is
a set of literals that must be true at the end of execution, and T is a termination criterion
such as a probability threshold or a time limit. The objective of the planner is to find the
maximal probability plan that takes the agent from I to G. If several plans have the same
probability of success, then the one with the least number of steps or cost is preferred.
The Probapop algorithm shown in Figure 1 is based on the classical POP algorithm
(Russell & Norvig, 2003; Younes & Simmons, 2003). It first constructs an initial plan by
converting initial and goal into dummy initial and goal steps, and using those as the first
and last steps of a plan with an empty body. It then refines the plans in the search queue
until it meets the termination criterion. The termination criterion that were implemented
include a time limit (e.g., stop after 5 minutes), a memory limit (e.g., stop after 256MB),
a probability threshold (e.g., stop after finding a plan with 0.9 or higher probability), and
lack of significant progress (e.g., stop if the probability of success cannot be increased more
than ²). It is possible to specify multiple termination criterion and use the earliest one that
becomes true. When a termination criterion is met the plan with the highest probability is
returned.
Plan refinement operations involve repairing flaws. An open condition can be closed by
adding a new step from the domain theory, or reusing a step already in the plan. An unsafe
link is handled by the promotion, demotion, or separation (when lifted actions are used)
operations, or by confrontation (Penberthy & Weld, 1992). All of these techniques are part
of the Vhpop implementation. Consider a step Sk threatening a causal link < Si , p, Sj >.
Promotion involves adding an extra ordering constraint such that Sk comes after Sj (Sj ≺ Sk
is added to ORD). Demotion involves adding an extra ordering constraint such that S k
comes before Si (Sk ≺ Si is added to ORD). Separation involves adding an extra inequality
constraint to BIND such that Sk ’s threatening effect can no longer unify with ¬p. Finally,
when actions have multiple effects, confrontation can be used by making a commitment
to non-threatening effects of Sk , i.e., those effects of Sk that do not contain a proposition
that unifies with ¬p. Note that in deterministic domains, an action can have multiple
effects due to multiple secondary preconditions (when conditions). In probabilistic domains,
probabilistic actions always have multiple effects.
The search is conducted using an A* algorithm guided by the ranking function which
provides the f value. As usual for a plan π, f (π) = g(π) + h(π), where g(π) is the cost of
the plan, and h(π) is the estimated cost of completing it. The ranking function is used at
the Merge step of the algorithm to order the plans in the search queue. In the competition
Probapop used a distance based heuristic (ADD) as explained in the next section. For the
flaw selection strategy in the Select-Flaw method, it used Vhpop’s static, which gives
priority to static open conditions, i.e., a condition whose value is not altered by any action
in the domain theory. If the flaws of a plan do not contain any static open conditions
threats are handled next; the lowest priority is given to the remaining open conditions. We
3

Onder, Whelan & Li

function Probapop (D, initial, goal, T)
returns a solution plan, or failure
** plans ← Make-Minimal-Plan(initial, goal)
** BestPlan ← null
** loop do
**** if a termination criterion is met then return BestPlan
**** if plans is empty then return failure
**** plan ← Remove-Front(plans)
**** if Solution?(plan) then return plan
**** plans ← Merge(plans, Refine-Plan(plan))
** end
function Refine-Plan (plan)
returns a set of plans (possibly null)
** if Flaws(plan) is empty then
**** if ProbSuccess (plan) > ProbSuccess (BestPlan)
******* BestPlan ← plan
**** plan ← Reopen-Conditions(plan)
** flaw ← Select-Flaw(plan)
** if flaw is an open condition then choose:
****** return Reuse-Step(plan, flaw)
****** return Add-New-Step(plan, flaw)
** if flaw is a threat then choose:
****** return Demotion(plan, flaw)
****** return Promotion(plan, flaw)
****** return Separation(plan, flaw)
****** return Confrontation(plan, flaw)

Figure 1: The probabilistic POP algorithm.

comment on other heuristics and flaw selection techniques following the discussion of the
competition results.
In the deterministic POP algorithm, a plan is considered to be complete when it has no
flaws, i.e., OPEN = UNSAFE = ∅. In probabilistic domains, there is a possibility that complete
plans that have insufficient probability of success (e.g., below 1 − ²) can be improved.
Probapop improves such plans by conducting a search after reopening the conditions that
can fail as explained in the next section. Probapop can be viewed as first searching for a
plan that is complete in the deterministic sense, and then searching for a way to improve
the plan. In our current implementation, we discard the search queue after finding the
first plan and all the subsequent improvements are made on the first complete plan found.
In the future, we plan to implement multiple search queues in order to be able to jump
between different plans and their improvements. In Figure 2a, we show an initial plan that
corresponds to the student application domain mentioned in the first section. The open
conditions are sending the forms (forms-sent) and getting a letter of reference (letter-sent).
Probapop uses Vhpop guided by the ranking and flaw selection heuristics to produce a
complete plan with 80% probability of success shown in Figure 2b. A straight line shows
a causal link between two actions and a zigzag line refers to a causal link from a plan
4

Engineering a Conformant Probabilistic Planner

fragment that has been omitted for clarity of exposition. Probapop reopens the condition
“letter-sent” (Figure 3a) and resumes its search using the same heuristics to come up with
an improved plan that involves asking two professors as shown in Figure 3b. Assuming
that ASK-PROFx is the only action that has probabilistic effects, the probability of success
is 0.8 for the first complete plan and 0.8 + 0.2 × 0.8 for the second complete plan. Several
such iterations of reopen and search leads Probapop to find a plan with a probability of
0.999997. Such a plan cannot be improved further with single precision arithmetic.
INITIAL

INITIAL

ASK PROF1

forms−sent

letter−sent

forms−sent

letter−sent

GOAL

GOAL

(a)

(b)

Figure 2: Starting with an empty plan and finding a first plan.

INITIAL

INITIAL

ASK PROF1

forms−sent

letter−sent

ASK PROF1

letter−sent

forms−sent

GOAL

letter−sent

ASK PROF2

letter−sent

GOAL

(a)

(b)

Figure 3: Starting with a complete plan and finding an improved plan.

3. Distance Based Ranking in Probapop
The Vhpop deterministic partial order-planner described by Younes and Simmons (2003)
supports distance based heuristics to provide an estimate of the total number of new actions
needed to close an open condition. Before starting to search, the planner builds a planning
graph (Blum & Furst, 1997), which has the literals in the initial state in its first level, and
continues to expand the graph until it reaches a level where all the goal literals are present.
The planning graph is different than Graphplan’s planning graph in the sense that it is
5

Onder, Whelan & Li

relaxed, i.e., delete lists are ignored and thus mutex relationships are not computed (Bonet
& Geffner, 2001).
In order to be able to generate a relaxed planning graph when multiple probabilistic
effects are present, one would need to split into as many plan graphs as there are leaves in
a probabilistic action. To avoid this potential blow up, we split each action in the domain
theory into as many deterministic actions as the number of nonempty effect lists. Each split
action represents a different way the original action would work. In Figure 4, we show an
action A1, which has two probabilistic effects a and b when P and Q are true, one effect c
when P is true and Q is false, and no effect otherwise. Each split action corresponds to one
set of non-empty effects. In Probapop, while the plan graph uses split actions, the plans
constructed always contain the full original action so that the planner can correctly assess
the probability of success. By using the split actions, we can compute a good estimate of
the number of actions needed to complete a plan for use with distance based heuristics.
A1
P
Q

prec: P, Q

prec: P, Q

A1−1

~P

A1−2
b

a

~Q

prec: P, ~Q
0.7

0.3

a

b

c

A1−3
c

Figure 4: Probabilistic action A1 is split into deterministic actions A1-1, A1-2, and A1-3.
An important distinction between deterministic partial-order planning and probabilistic
partial-order planning is multiple support for plan literals. In the deterministic case, an
open condition is permanently removed from the list of flaws once it is resolved. In the
probabilistic case, it can be reopened so that the planner can search for additional steps
that increase the probability of the literal. The Buridan system implements this technique
by reopening all the previously closed conditions of a complete plan and resuming the search
to find another complete plan. Our implementation employs selective reopening (SR) where
only those conditions that are not guaranteed to be achieved are reopened. In other words,
literals supported with a probability of 1 are not reopened. Note that while checking the
probability of literals is costly for probabilistic plans, we save most of the cost by performing
the check during mandatory assessment of complete plans. Obviously, avoiding redundant
searches is an advantage for the planner. In our current implementation we reopen all the
supported literals that have a probability less than 1. We leave the selection from among
this new set of preconditions to the flaw selection heuristic. Our implementation does not
contain any probability based heuristics.
It is important to note that neither the split actions nor the selective reopening technique
change the base soundness and completeness properties of the Buridan algorithm. The split
actions are only used in the relaxed plan graph, and the reopening technique does not block
any alternatives from being sought as they would already be covered by a plan in the search
queue.
6

Engineering a Conformant Probabilistic Planner

4. Probapop in IPC-4
Probapop was among the 7 domain-independent planners that competed in the probabilistic
track of IPC-4. By domain-independent we mean a planner that uses only the PPDDL description of a domain to solve a planning problem and does not employ any previously coded
control information. In Table 1 we show a brief description of these planners (Edelkamp,
Hoffman, Littman, & Younes, 2004; Younes, Littman, Weissman, & Asmuth, 2005; Bonet
& Geffner, 2005; Fern, Yoon, & Givan, 2006; Thiebaux, Gretton, Slaney, Price, & Kabanza,
2006). The competition was conducted as follows: Each planner was given a set of 24
problems written in probabilistic PDDL (PPDDL) and was allotted 5 minutes to solve the
problem. After this, the server simulated a possible way of executing the plan by sending a
sequence of states starting with the initial state and the planners responded to each state
with an action based on the solution they found. 30 simulations were conducted for each
problem. For goal-based problems success was measured by whether the goal was reached
at the end of the simulation. For reward-based problems the total reward was calculated.
The set of 24 problems included both of these types.
The competition included various domains as listed below:
• Blocksworld: Includes the pick up and put down actions where each action can fail.
6 problems with 5, 8, 11, 15, 18 and 21 blocks were given. The goal was to build one
or more towers of blocks.
• Colored Blocksworld: The actions are the same as the Blocksworld domain. Each
block can be one of three colors. The goal towers were specified using existential
quantifiers, e.g., there is a green block on the table, there is a red block on a green
block.
• Exploding Blocksworld: It is similar to the Blocksworld domain but the first put-down
action can permanently destroy the bottom object (block or table). Replanning or
repetition based approaches fail easily due to the irreversible nature of the explosion.
• Boxworld: It is a box transportation problem with load, unload, drive and fly actions.
The drive action can fail taking the truck to a wrong city.
• Fileworld: The objective includes actions to put the papers into files of matching
type. The type of a paper can be found out by using an observation action that has
probabilistics outcomes.
• Tireworld: The actions include moving between several cities and the tire can go flat
during a trip.
• Towers of Hanoise: It is a variation of the Towers of Hanoi problem where discs can
be moved in singles or doubles and discs may slip during a move.
• Zeno travel: It is a travel domain that includes actions related to flying. Some actions
such as boarding and flying can fail.
It should be noted that all of the competition domains were designed for full observability
and needed to be changed to incorporate a blind planner. For instance, the PICKUP action
7

Onder, Whelan & Li

Planner (code)
UMass (C)
NMRDPP (G1)
Classy (J2)
FF-rePlan (J3)
mGPT (P)
Probapop (Q)
CERT (R)

Description
Symbolic heuristic search based on symbolic AO* with loops (LAO*) and
symbolic real-time dynamic programming (RTDP)
Solving decision problems with non-Markovian (and hence Markovian)
rewards
Approximate policy iteration with inductive machine learning using
random-walk problems
Deterministic replanner using Fast Forward
Labeled real-time dynamic programming (LRTDP) with lower bounds
extracted from the deterministic relaxations of the MDP
POP-style plan-space A* search with distance based heuristics and failure
analysis
Heuristic state space search with structured policy iteration algorithm,
factored MDPs, and reachability analysis

Table 1: Domain-independent planners listed in order of competition code.

in the Blocksworld domain has a precondition that requires that the block to be picked
up is not being held by the arm. The action has two probabilistic effects, one resulting
in the block being held, and the other being not held. Because the planner assumes no
observability, a plan involving a PICKUP action cannot be improved because an action
cannot be executed unless its preconditions hold. Thus, the Probapop planner cannot
insert a second PICKUP action to cover the case in which the first one fails. With the help
of the competition organizers, we implemented a workaround such that the actions that are
executed when their conditions do not hold have no effect rather than causing an error.
Probapop (competition name Q) attempted 4 of the 24 problems. The two planners
that attempted most of the problems were Classy (J2) and FF-rePlan (J3). The other
planners attempted between 3 to 10 problems as listed in Table 2. Probapop attempted a
small number of problems due to three reasons. First, when we started building Probapop,
Vhpop’s version was 2.0. The performance of Vhpop was significantly improved with better
memory handling techniques in version 2.2 but we did not have time before the competition
to convert our implementation to the newer version. Second, the competition Blocksworld
domains included universally quantified preconditions which were not supported in Vhpop.
Our implementation of the preconditions including the FORALL keyword was not efficient.
Third, our implementation disables the feature of Vhpop which allows the use of multiple
search queues with different heuristics. This prohibited us from constructing several search
queues each with a different heuristic and using the one that finishes the earliest. We
therefore had to pick a single heuristic to run the competition problems. As a result, we
picked ADD as the ranking metric and static as the flaw selection technique and ran all the
problems with this combination.
After the competition results were announced, we observed that there were three domain
independent planners, namely Classy (J2), FF-rePlan (J3), and mGPT (P), that were
able to solve the largest Blocksworld problems whereas Probapop was only able to solve
the 5-blocks problem (the competition included domains with 5, 8, 11, 15, 18, and 21
8

Engineering a Conformant Probabilistic Planner

Planner
Umass (C)
NMRDPP (G1)
Classy (J2)
FF-rePlan (J3)
mGPT (P)
Probapop (Q)
CERT (R)

# of
problems
4
7
18
24
10
4
3

bw-nc-r-5
30
30
30
30
30
11
30

tire-nr
30
9
–
7
16
7
9

tire-r
30
30
–
30
30
6
0

zeno
30
30
–
0
30
1
27

Table 2: The number of successes in 30 trials obtained by the planners that do not use
domain knowledge. Only the problems attempted by Probapop (Q) are listed.
A dash means that the planner did not attempt that problem. Bw-nc-r-5 is the
Blocksworld problem with 5 blocks. Tire-nr and tire-r are the goal and reward
based problems from the Tireworld domain. Zeno is a problem using the Zeno
travel domain problem.

blocks). Therefore, we looked for ways of improving the performance of Probapop on
these problems. We first reimplemented Probapop on Vhpop’s newer version 2.2. Second,
we brought the language of the competition Blocksworld domain closer to STRIPS. In
particular, we removed the FORALL preconditions and WHEN conditions. For example,
we replaced the PPDDL PICK-UP action shown in Figure 5 with the two actions shown in
Figure 6. However, the version upgrade and the language simplification were not sufficient to
enable Probapop to solve the 8-blocks problem. As explained before, Probapop’s strategy is
to first find a “base plan” and then to improve this plan at possible failure points, therefore
finding the base plan is crucial. We next looked for other heuristics and flaw selection
strategies that can make the Blocksworld problems solvable. We begin discussing these by
explaining Vhpop’s ADD heuristic in more detail.
(:action pick-up-block-from
* :parameters (?top - block ?bottom)
* :effect (when (and (not (= ?top ?bottom)) (on-top-of ?top ?bottom)
****************** (forall (?b - block) (not (holding ?b)))
****************** (forall (?b - block) (not (on-top-of ?b ?top))))
************ (and (decrease (reward) 1)
************ (probabilistic 0.75 (and (holding ?top) (not (on-top-of ?top ?bottom)))
*************** ******** 0.25 (when (not (= ?bottom table))
*************** ************* (and (not (on-top-of ?top ?bottom)) (on-top-of ?top table)))))))

Figure 5: PPDDL’s PICK-UP action
The ADD heuristic achieves good performance by computing the sum of the step costs
of the open conditions from the relaxed planning graph, i.e., the heuristic cost of a plan
is computed as h(π) = hadd (OP EN (π)). The cost of achieving a literal q is the level of
the first action that achieves q: hadd (q) = mina∈GA(q) hadd (a) if GA(q) 6= ∅, where GA(q)
9

Onder, Whelan & Li

(:action pick-up
* :parameters (?x)
* :precondition (and (clear ?x) (ontable ?x) (handempty))
* :effect
* ** (probabilistic 0.75
* **** (and (not (ontable ?x)) (not (clear ?x)) (not (handempty)) (holding ?x))))
(:action unstack
* :parameters (?x ?y)
* :precondition (and (on-top-of ?x ?y) (clear ?x) (handempty))
* :effect
* ** (probabilistic 0.75
* **** (and (holding ?x) (clear ?y) (not (clear ?x)) (not (handempty)) (not (on-top-of ?x ?y)))))

Figure 6: Simplified form of PPDDL’s PICK-UP action.
is an action that has an effect q. Note that hadd (q) is 0 if q holds initially, and is ∞
if q never holds. The level of an action is the first level its preconditions become true:
hadd (a) = 1 + hadd (P REC(a)). The ADDR heuristic is a modification of the ADD heuristic
that takes action reuse into account, thus in addition to the conditions described above, the
heuristic cost of a literal q is 0 if the plan already contains an action that can achieve q.
We observed that ADDR is more effective than ADD for the Blocksworld domain and
tested a variety of flaw selection strategies implemented in Vhpop together with ADDR.
We show the flaw selection strategies we tried in Table 3. We adopt the notation given
by Pollack et al. (1997) and revised by Younes and Simmons (2003). In this notation,
each strategy is an ordered list of selection criteria where LR refers to “least refinements
first”, MCadd refers to “most cost computed using ADD”, and MWadd refers to “most work
using ADD”. Open conditions are divided into three categories for use by some heuristics.
A static open condition is an open condition whose literal can only be provided by the
initial state, i.e., no action has this literal as an effect. A local open condition refers to the
open conditions of the most recently added action and is used to maintain focus on the
achievement of a single goal. An unsafe open condition refers to an open condition whose
causal link would be threatened.
There are five main strategies which prioritize flaws differently. The ucpop strategy
gives priority to threats, the static strategy gives priority to static open conditions, the lcfr
strategy handles flaws in order of least expected cost, the mc strategy orders open conditions
with respect to cost extracted from the relaxed planning graph, and the mw strategy orders
open conditions with respect to expected work extracted from the relaxed planning graph.
A strategy with a “loc” annotation gives priority to local open conditions among the open
conditions, a strategy with a “conf” annotation gives priority to unsafe open conditions
among the open conditions. We refer the reader to the paper by Younes and Simmons
(2003) for a thorough description of these heuristics as well as experimental results with
other domains.
We depict the results of our experiments with the Blocksworld problems in the first and
third lines of Table 4 (the second and fourth lines in Tables 4 and 5 will be explained later).
10

Engineering a Conformant Probabilistic Planner

Strategy
ucpop
static
lcfr
lcfr-loc
lcfr-conf
lcfr-loc-conf
mc
mc-dsep
mc-loc
mc-dsep
mw
mw-dsep
mw-loc
mw-loc-dsep

Description
{n,s} LIFO / {o} LIFO
{t} LIFO / {n,s} LIFO / {o} LIFO
{n,s,o} LR
{n,s,l} LR
{n,s,u} LR / {o} LR
{n,s,u} LR / {l} LR
{n,s} LR / {o} MCadd
{n} LR / {o} MCadd / {s} LR
{n,s} LR / {l} MCadd
{n} LR / {l} MCadd / {s} LR
{n,s} LR / {o} MWadd
{n} LR / {o} MWadd / {s} LR
{n,s} LR / {l} MWadd
{n} LR / {l} MWadd / {s} LR

Table 3: The description of a variety of flaw selection strategies in Vhpop. “n” is a nonseparable threat, “s” is a separable threat, “o” is an open condition, “t” is a static
open condition, “l” is a local open condition, and “u” is an unsafe open condition.

It can be seen that only lcfr and mc strategies work for the problem with 8 blocks. The
larger problems were not solvable. Because the actions are lifted, we tried to make the search
space smaller by delaying separable threats. Peot and Smith (1993) explain that delaying
the separable threats may result in a decreased branching factor because there may be many
ways to add inequality constraints for separation. The delay might also help because the
threat can disappear as more variables are bound. We modified the best working strategies,
namely variants of mc and mw, and implemented the delay of separable threats (in Table 3
these are shown with the dsep suffix.) We show the planning times for the experiments with
and without dsep in Table 5 (we repeat the columns from Table 4 for comparison). The
results show that time improvement can be seen for the 5-blocks problem. The problems
with 8 blocks show an increase in time because each threat must be checked to see if it is
separable. Delaying threats made the 8-blocks problem solvable using the mc-loc, mw, and
mw-loc strategies. However, larger problems were not solvable by any strategy.
The results of our experiments with various heuristics and strategies show that the
search time increases dramatically by going from 5 to 8 blocks and larger problems are not
solvable. We were not able to find a heuristic combination to solve the larger problems. We
noticed that the competition Blocksworld problems list the goal towers from top to bottom
and the planner spends a lot of time with dead end plans when the original goal order is
preserved. If a tower is built from top to bottom, the initial goals almost always have to
be undone to achieve the later goals. We also concluded that such an interaction cannot
be detected with the heuristics we used because they are designed to consider subgoals in
isolation. Koehler and Hoffmann (2000) describe a polynomial time algorithm that can
order goals to minimize the above type of undoing. The algorithm operates on ground
11

Onder, Whelan & Li

5
5o
8
8o

ucpop

static

lcfr

80
0
–
–

70
0
–
–

0
10
55K
–

lcfrloc
60
50
–
–

lcfrconf
570
10
–
–

lcfrloc-conf
90
770
–
–

mc
10
0
13K
42K

mcloc
50
40
–
–

mw
0
0
–
41K

mwloc
20
30
–
–

mwloc-conf
220
120
–
–

Table 4: Time (msec) required to find the base plan for Blocksworld problems with 5 and
8 blocks.

5
5o
8
8o

mc
10
0
13K
42K

mc-dsep
0
0
73K
104K

mc-loc
40
50
–
–

mc-loc-dsep
20
30
22K
–

mw
0
0
–
41K

mw-dsep
0
0
73K
103K

mw-loc
20
30
–
–

mw-loc-dsep
30
20
22K
–

Table 5: Time (msec) required to find the base plan by delaying separable threats for
Blocksworld problems with 5 and 8 blocks.

action descriptions which can be generated from action schemas and was implemented in
the FF planning system (Hoffman & Nebel, 2001). We used this algorithm to order the
top-level goals and repeated all the experiments with this ordering which essentially builds
towers from the bottom to the top. The results for ordered goals are shown in lines 2 and
4 of tables 4 and 5. Ordering the goals had mixed results. For example, for the 8 blocks
problem, it made the lcfr heuristic not usable but the mw heuristic usable. However, the
lowest time increased from 13K to 41K milliseconds and the larger problems were still not
solvable.
Our final strategy was to combine the planning approach used by the FF planner with
POP-style search. In particular, we ordered the top-level goals using FF’s ordering algorithm and ran Vhpop n times for problems with n top level goals. The first problem had
only the first goal and when Vhpop returned a plan, the steps were simulated to find the
resulting state. The second problem had this resulting state as the initial state and goals 1
and 2 so that goal 1 would be preserved or redone and goal 2 would be achieved. When we
used this strategy with the default heuristics of Vhpop to solve the problem with 21 blocks,
the total time was 70 milliseconds with most phases taking 0 milliseconds. Koehler and
Hoffmann (2000) explain that this approach works well for invertible planning problems,
i.e., problems such as the Blocksworld where actions are reversible. In our case, the tradeoff
is the possibility of less optimal plans because a plan for the ith goal is set while working
on the i + 1st goal. The second tradeoff is getting several partially-ordered plans with
breakpoints between problems rather than a single maximally parallel plan. We believe it
is worthwhile to work on an algorithm that combines the individual plans to preserve the
least commitment on ordering. Possible strategies are to causally link action preconditions
12

Engineering a Conformant Probabilistic Planner

to latest producers or use the approach of Edelkamp (2004) and parallelize sequential plans
using critical path analysis.

5. Conclusion and Future Work
We presented the design and implementation of Probapop, a partial-order, probabilistic,
conformant planner. We described the distance-based and condition-probability based
heuristics that we used. We discussed the advantages and disadvantages of using an incremental algorithm where goals are first ordered and submitted one by one. Our short
term plans involve implementing multiple search queues for different base plans and reincorporating the ADL constructs in PPDDL. Our future work involves three threads. In one,
we are looking at improving the performance of Probapop by adding probability information to the planning graph so that the probability of open conditions can be optimistically
estimated. We are also considering the addition of domain specific information (Kuter &
Nau, 2005) to probabilistic domains. In the second thread, we are exploring the middle
ground between no observability and full observability by considering POMDP-like problems in a partial-order setting. Finally, we would like to incorporate hill climbing techniques
into our probabilistic framework. The current Probapop 2.0 software is available through
www.cs.mtu.edu/˜nilufer.

Acknowledgments
This work has been supported by a Research Excellence Fund grant to Nilufer Onder from
Michigan Technological University. We thank JAIR IPC-4 special track editor David E.
Smith, and the anonymous reviewers for their very helpful comments.

References
Bertoli, P., Cimatti, A., & Roveri, M. (2001). Heuristic search + symbolic model checking
= efficient conformant planning. In Proceedings of Eighteenth International Joint
Conference on Artificial Intelligence (IJCAI-01), pp. 467–472.
Blum, A. L., & Furst, M. L. (1997). Fast planning through planning graph analysis. Artificial
Intelligence, 90, 281–300.
Bonet, B., & Geffner, H. (2001). Planning as heuristic search. Artificial Intelligence, 129 (12), 5–33.
Bonet, B., & Geffner, H. (2005). mGPT: A probabilistic planner based on heuristic search.
Journal of Artificial Intelligence Research, 24, 933–944.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision theoretic planning: Structural assumptions and computational leverage. Journal of Artificial Intelligence Research,
11, 1–94.
Brafman, R. I., & Hoffmann, J. (2004). Conformant planning via heuristic forward search: A
new approach. In Proceedings of Fourteenth International Conference on Automated
Planning & Scheduling (ICAPS-04), pp. 355–364.
13

Onder, Whelan & Li

Edelkamp, S. (2004). Extended critical paths in temporal planning. In Workshop on Integrating Planning Into Scheduling International Conference on Automated Planning
and Scheduling (ICAPS-04), pp. 38–45.
Edelkamp, S., Hoffman, J., Littman, M., & Younes, H. (2004). International planning competition. Proceedings of Fourteenth International Conference on Automated Planning
& Scheduling (ICAPS-04).
Fern, A., Yoon, S., & Givan, R. (2006). Approximate policy iteration with a policy language
bias: Solving relational Markov decision processes. Journal of Artificial Intelligence
Research, 25.
Ferraris, P., & Giunchiglia, E. (2000). Planning as satisfiability in nondeterministic domains. In Proceedings of the Seventeenth National Conference on Artificial Intelligence
(AAAI-00), pp. 748–754.
Hansen, E. A., & Feng, Z. (2000). Dynamic programming for POMDPs using a factored
state representation. In Proceedings of the Fifth International Conference on Artificial
Intelligence Planning & Scheduling (AIPS-00), pp. 130–139.
Hoffman, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through
heuristic search. Journal of Artificial Intelligence Research, 14, 253–302.
Hoffmann, J., & Brafman, R. I. (2005). Contingent planning via heuristic forward search
with implicit belief states. In Proceedings of Fifteenth International Conference on
Automated Planning & Scheduling (ICAPS-05), pp. 71–80.
Hyafil, N., & Bacchus, F. (2003). Conformant probabilistic planning via CSPs. In Proceedings of Thirteenth International Conference on Automated Planning & Scheduling
(ICAPS-03), pp. 205–214.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning and acting in
partially observable stochastic domains. Artificial Intelligence, 101, 99–134.
Karlsson, L. (2001). Conditional progressive planning under uncertainty. In Proceedings of
Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-01), pp.
431–436.
Koehler, J., & Hoffmann, J. (2000). On reasonable and forced goal orderings and their use
in an agenda-driven planning algorithm. Journal of Artificial Intelligence Research,
12, 339–386.
Kushmerick, N., Hanks, S., & Weld, D. S. (1995). An algorithm for probabilistic planning.
Artificial Intelligence, 76, 239–286.
Kuter, U., & Nau, D. (2005). Using domain-configurable search control in probabilistic planners. In Proceedings of the Twentieth National Conference on Artificial Intelligence
(AAAI-05).
Majercik, S. M., & Littman, M. L. (1999). Contingent planning under uncertainty via
stochastic satisfiability. In Proceedings of the Sixteenth National Conference on Artificial Intelligence (AAAI-99), pp. 549–556.
McDermott, D. (1999). Using regression-match graphs to control search in planning. Artificial Intelligence, 109 (1-2), 111–159.
14

Engineering a Conformant Probabilistic Planner

Nguyen, X., & Kambhampati, S. (2001). Reviving partial order planning. In Proceedings of
Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-01), pp.
459–464.
Onder, N., & Pollack, M. E. (1999). Conditional, probabilistic planning: A unifying algorithm and effective search control mechanisms. In Proceedings of the Sixteenth
National Conference on Artificial Intelligence (AAAI-99), pp. 577–584.
Penberthy, J. S., & Weld, D. S. (1992). UCPOP: A sound, complete, partial order planner
for ADL. In Proceedings of Third International Conference on Principles of Knowledge
Representation & Reasoning (KR-92), pp. 103–114.
Peot, M. A., & Smith, D. E. (1993). Threat-removal strategies for partial order planning. In
Proceedings of the Eleventh National Conference on Artificial Intelligence (AAAI-93),
pp. 492–499.
Pollack, M. E., Joslin, D., & Paolucci, M. (1997). Flaw selection strategies for partial-order
planning. Journal of Artificial Intelligence Research, 6, 223–262.
Russell, S. J., & Norvig, P. (2003). Artificial Intelligence: A Modern Approach, Second
Edition. Pearson Education, Upper Saddle River, NJ.
Smith, D. E., Frank, J., & Jonsson, A. K. (2000). Bridging the gap between planning and
scheduling. Knowledge Engineering Review, 15 (1), 47–83.
Thiebaux, S., Gretton, C., Slaney, J., Price, D., & Kabanza, F. (2006). Decision-theoretic
planning with non-Markovian rewards. Journal of Artificial Intelligence Research, 25.
Younes, H. L., Littman, M. L., Weissman, D., & Asmuth, J. (2005). The first probabilistic
track of the international planning competition. Journal of Artificial Intelligence
Research, 24, 851–887.
Younes, H., & Simmons, R. (2003). VHPOP: Versatile heuristic partial order planner.
Journal of Artificial Intelligence Research, 20, 405–430.

15

Journal of Artificial Intelligence Research 25 (2006) 315–348

Submitted 08/05; published 03/06

Negotiating Socially Optimal Allocations of Resources
Ulle Endriss

ulle@illc.uva.nl

ILLC, University of Amsterdam
1018 TV Amsterdam, The Netherlands

Nicolas Maudet

maudet@lamsade.dauphine.fr

LAMSADE, Université Paris-Dauphine
75775 Paris Cedex 16, France

Fariba Sadri

fs@doc.ic.ac.uk

Department of Computing, Imperial College London
London SW7 2AZ, UK

Francesca Toni

ft@doc.ic.ac.uk

Department of Computing, Imperial College London
London SW7 2AZ, UK

Abstract
A multiagent system may be thought of as an artificial society of autonomous software
agents and we can apply concepts borrowed from welfare economics and social choice theory
to assess the social welfare of such an agent society. In this paper, we study an abstract
negotiation framework where agents can agree on multilateral deals to exchange bundles
of indivisible resources. We then analyse how these deals affect social welfare for different
instances of the basic framework and different interpretations of the concept of social welfare
itself. In particular, we show how certain classes of deals are both sufficient and necessary
to guarantee that a socially optimal allocation of resources will be reached eventually.

1. Introduction
A multiagent system may be thought of as an artificial society of autonomous software
agents. Negotiation over the distribution of resources (or tasks) amongst the agents inhabiting such a society is an important area of research in artificial intelligence and computer
science (Rosenschein & Zlotkin, 1994; Kraus, 2001; Chavez, Moukas, & Maes, 1997; Sandholm, 1999). A number of variants of this problem have been studied in the literature.
Here we consider the case of an artificial society of agents where, to begin with, each agent
holds a bundle of indivisible resources to which it assigns a certain utility. Agents may then
negotiate with each other in order to agree on the redistribution of some of these resources
to benefit either themselves or the agent society they inhabit.
Rather than being concerned with specific strategies for negotiation, we analyse how the
redistribution of resources by means of negotiation affects the well-being of the agent society
as a whole. To this end, we make use of formal tools for measuring social welfare developed
in welfare economics and social choice theory (Moulin, 1988; Arrow, Sen, & Suzumura,
2002). In the multiagent systems literature, the utilitarian interpretation of the concept of
social welfare is usually taken for granted (Rosenschein & Zlotkin, 1994; Sandholm, 1999;
Wooldridge, 2002), i.e. whatever increases the average welfare of the agents inhabiting a
society is taken to be beneficial for society as well. This is not the case in welfare economics,
c
2006
AI Access Foundation. All rights reserved.

Endriss, Maudet, Sadri, & Toni

for instance, where different notions of social welfare are being studied and compared with
each other. Here, the concept of egalitarian social welfare takes a particularly prominent
role (Sen, 1970; Rawls, 1971; Moulin, 1988; Arrow et al., 2002). In this model, social
welfare is tied to the individual welfare of the weakest member of society, which facilitates
the incorporation of a notion of fairness into the resource allocation process. While the
discussion of the respective advantages and drawbacks of different notions of social welfare
in the social sciences tends to be dominated by ethical considerations,1 in the context
of societies of artificial software agents the choice of a suitable formal tool for modelling
social welfare boils down to a clear-cut (albeit not necessarily simple) technical design
decision (Endriss & Maudet, 2004). Indeed, different applications may call for different
social criteria. For instance, for the application studied by Lemaı̂tre, Verfaillie, and Bataille
(1999), where agents need to agree on the access to an earth observation satellite which has
been funded jointly by the owners of these agents, it is important that each one of them
receives a “fair” share of the common resource. Here, a society governed by egalitarian
principles may be the most appropriate. In an electronic commerce application running on
the Internet where agents have little or no commitments towards each other, on the other
hand, egalitarian principles seem of little relevance. In such a scenario, utilitarian social
welfare would provide an appropriate reflection of the overall profit generated. Besides
utilitarian and egalitarian social welfare, we are also going to discuss notions such as Pareto
and Lorenz optimality (Moulin, 1988), as well as envy-freeness (Brams & Taylor, 1996).
In this paper, we study the effect that negotiation over resources has on society for a
number of different interpretations of the concept of social welfare. In particular, we show
how certain classes of deals regarding the exchange of resources allow us to guarantee that a
socially optimal allocation of resources will be reached eventually. These convergence results
may be interpreted as the emergence of a particular global behaviour (at the level of society)
in reaction to local behaviour governed by the negotiation strategies of individual agents
(which determine the kinds of deals agents are prepared to accept). The work described here
is complementary to the large body of literature on mechanism design and game-theoretical
models of negotiation in multiagent systems (see e.g. Rosenschein & Zlotkin, 1994; Kraus,
2001; Fatima, Wooldridge, & Jennings, 2004). While such work is typically concerned with
negotiation at the local level (how can we design mechanisms that provide an incentive
to individual agents to adopt a certain negotiation strategy?), we address negotiation at a
global level by analysing how the actions taken by agents locally affect the overall system
from a social point of view.
As we shall see, truly multilateral deals involving any number of agents as well as any
number of resources may be necessary to be able to negotiate socially optimal allocations
of resources. This is certainly true as long as we use arbitrary utility functions to model
the preferences of individual agents. In some application domains, however, where utility
functions may be assumed to be subject to certain restrictions (such as being additive), we
are able to obtain stronger results and show that also structurally simpler classes of deals
(in particular, deals involving only a single resource at a time) can be sufficient to negotiate
socially optimal allocations. Nevertheless, for other seemingly strong restrictions on agents’
1. A famous example is Rawls’ veil of ignorance, a thought experiment designed to establish what constitutes
a just society (Rawls, 1971).

316

Negotiating Socially Optimal Allocations of Resources

utility functions (such as the restriction to dichotomous preferences) we are able to show
that no reduction in the structural complexity of negotiation is possible.
Our approach to multiagent resource allocation is of a distributed nature. In general,
the allocation procedure used to find a suitable allocation of resources could be either centralised or distributed. In the centralised case, a single entity decides on the final allocation
of resources amongst agents, possibly after having elicited the agents’ preferences over alternative allocations. Typical examples are combinatorial auctions (Cramton, Shoham, &
Steinberg, 2006). Here the central entity is the auctioneer and the reporting of preferences
takes the form of bidding. In truly distributed approaches, on the other hand, allocations
emerge as the result of a sequence of local negotiation steps. Both approaches have their
advantages and disadvantages. Possibly the most important argument in favour of auctionbased mechanisms concerns the simplicity of the communication protocols required to implement such mechanisms. Another reason for the popularity of centralised mechanisms is
the recent push in the design of powerful algorithms for combinatorial auctions that, for
the first time, perform reasonably well in practice (Fujishima, Leyton-Brown, & Shoham,
1999; Sandholm, 2002). Of course, such techniques are, in principle, also applicable in the
distributed case, but research in this area has not yet reached the same level of maturity
as for combinatorial auctions. An important argument against centralised approaches is
that it may be difficult to find an agent that could assume the role of an “auctioneer” (for
instance, in view of its computational capabilities or in view of its trustworthiness).
The line of research pursued in this paper has been inspired by Sandholm’s work on
sufficient and necessary contract (i.e. deal) types for distributed task allocation (Sandholm,
1998). Since then, it has been further developed by the present authors, their colleagues, and
others in the context of resource allocation problems (Bouveret & Lang, 2005; Chevaleyre,
Endriss, Estivie, & Maudet, 2004; Chevaleyre, Endriss, Lang, & Maudet, 2005a; Chevaleyre,
Endriss, & Maudet, 2005b; Dunne, 2005; Dunne, Laurence, & Wooldridge, 2004; Dunne,
Wooldridge, & Laurence, 2005; Endriss & Maudet, 2004, 2005; Endriss, Maudet, Sadri,
& Toni, 2003a, 2003b). In particular, we have extended Sandholm’s framework by also
addressing negotiation systems without compensatory side payments (Endriss et al., 2003a),
as well as agent societies where the concept of social welfare is given a different interpretation
to that in the utilitarian programme (Endriss et al., 2003b; Endriss & Maudet, 2004). The
present paper provides a comprehensive overview of the most fundamental results, mostly on
the convergence to an optimal allocation with respect to different notions of social welfare,
in a very active and timely area of ongoing research.
The remainder of this paper is organised as follows. Section 2 introduces the basic negotiation framework for resource reallocation we are going to consider. It gives definitions
for the central notions of allocation, deal, and utility, and it discusses possible restrictions
to the class of admissible deals (both structural and in terms of acceptability to individual
agents). Section 2 also introduces the various concepts of a social preference we are going
to consider in this paper. Subsequent sections analyse specific instances of the basic negotiation framework (characterised, in particular, by different criteria for the acceptability
of a proposed deal) with respect to specific notions of social welfare. In the first instance,
agents are assumed to be rational (and “myopic”) in the sense of never accepting a deal
that would result in a negative payoff. Section 3 analyses the first variant of this model
of rational negotiation, which allows for monetary side payments to increase the range of
317

Endriss, Maudet, Sadri, & Toni

acceptable deals. As we shall see, this model facilitates negotiation processes that maximise
utilitarian social welfare. If side payments are not possible, we cannot guarantee outcomes
with maximal social welfare, but it is still possible to negotiate Pareto optimal allocations.
This variant of the rational model is studied in Section 4. Both Section 3 and 4 also investigate how restrictions to the range of utility functions agents may use to model their
preferences can affect such convergence results.
In the second part of the paper we apply our methodology to agent societies where the
concept of social welfare is given a different kind of interpretation than is commonly the
case in the multiagent systems literature. Firstly, in Section 5 we analyse our framework
of resource allocation by negotiation in the context of egalitarian agent societies. Then
Section 6 discusses a variant of the framework that combines ideas from both the utilitarian
and the egalitarian programme and enables agents to negotiate Lorenz optimal allocations
of resources. Finally, Section 7 introduces the idea of using an elitist model of social welfare
for applications where societies of agents are merely a means of enabling at least one agent
to achieve their goal. This section also reviews the concept of envy-freeness and discusses
ways of measuring different degrees of envy.
Section 8 summarises our results and concludes with a brief discussion of the concept of
welfare engineering, i.e. with the idea of choosing tailor-made definitions of social welfare
for different applications and designing agents’ behaviour profiles accordingly.

2. Preliminaries
The basic scenario of resource allocation by negotiation studied in this paper is that of an
artificial society inhabited by a number of agents, each of which initially holds a certain
number of resources. These agents will typically ascribe different values (utilities) to different bundles of resources. They may then engage in negotiation and agree on the reallocation
of some of the resources, for example, in order to improve their respective individual welfare
(i.e. to increase their utility). Furthermore, we assume that it is in the interest of the system
designer that these distributed negotiation processes —somehow— also result in a positive
payoff for society as a whole.
2.1 Basic Definitions
An instance of our abstract negotiation framework consists of a finite set of (at least two)
agents A and a finite set of resources R. Resources are indivisible and non-sharable. An
allocation of resources is a partitioning of R amongst the agents in A.
Definition 1 (Allocations) An allocation of S
resources is a function A from A to subsets
of R such that A(i) ∩ A(j) = { } for i 6= j and i∈A A(i) = R.
For example, given an allocation A with A(i) = {r3 , r7 }, agent i would own resources r3 and
r7 . Given a particular allocation of resources, agents may agree on a (multilateral) deal to
exchange some of the resources they currently hold. In the most general case, any numbers
of agents and resources could be involved in a single deal. From an abstract point of view,
a deal takes us from one allocation of resources to the next. That is, we may characterise
a deal as a pair of allocations.
318

Negotiating Socially Optimal Allocations of Resources

Definition 2 (Deals) A deal is a pair δ = (A, A0 ) where A and A0 are allocations of
resources with A 6= A0 .
The set of agents involved in a deal δ = (A, A0 ) is given by Aδ = {i ∈ A | A(i) 6= A0 (i)}.
The composition of two deals is defined as follows: If δ1 = (A, A0 ) and δ2 = (A0 , A00 ), then
δ1 ◦ δ2 = (A, A00 ). If a given deal is the composition of two deals that concern disjoint sets
of agents, then that deal is said to be independently decomposable.
Definition 3 (Independently decomposable deals) A deal δ is called independently
decomposable iff there exist deals δ1 and δ2 such that δ = δ1 ◦ δ2 and Aδ1 ∩ Aδ2 = { }.
Observe that if δ = (A, A0 ) is independently decomposable then there exists an intermediate
allocation B different from both A and A0 such that the intersection of {i ∈ A | A(i) 6= B(i)}
and {i ∈ A | B(i) 6= A0 (i)} is empty, i.e. such that the union of {i ∈ A | A(i) = B(i)} and
{i ∈ A | B(i) = A0 (i)} is the full set of agents A. Hence, δ = (A, A0 ) not being independently
decomposable implies that there exists no allocation B different from both A and A0 such
that either B(i) = A(i) or B(i) = A0 (i) for all agents i ∈ A (we are going to use this fact in
the proofs of our “necessity theorems” later on).
The value an agent i ∈ A ascribes to a particular set of resources R will be modelled by
means of a utility function, that is, a function from sets of resources to real numbers. We
are going to consider both general utility functions (without any restrictions) and several
more specific classes of functions.
Definition 4 (Utility functions) Every agent i ∈ A is equipped with a utility function
ui : 2R → R. We are going to consider the following restricted classes of utility functions:
• ui is non-negative iff ui (R) ≥ 0 for all R ⊆ R.
• ui is positive iff it is non-negative and ui (R) 6= 0 for all R ⊆ R with R 6= { }.
• ui is monotonic iff R1 ⊆ R2 implies ui (R1 ) ≤ ui (R2 ) for all R1 , R2 ⊆ R.
P
• ui is additive iff ui (R) = r∈R ui ({r}) for all R ⊆ R.
• ui is a 0-1 function iff it is additive and ui ({r}) = 0 or ui ({r}) = 1 for all r ∈ R.
• ui is dichotomous iff ui (R) = 0 or ui (R) = 1 for all R ⊆ R.
Recall that, given an allocation A, the set A(i) is the bundle of resources held by agent i
in that situation. We are usually going to abbreviate ui (A) = ui (A(i)) for the utility value
assigned by agent i to that bundle.
2.2 Deal Types and Rationality Criteria
In this paper we investigate what kinds of negotiation outcomes agents can achieve by using
different classes of deals. A class of deals may be characterised by both structural constraints
(number of agents and resources involved, etc.) and rationality constraints (relating to the
changes in utility experienced by the agents involved).
Following Sandholm (1998), we can distinguish a number of structurally different types
of deals. The most basic are 1-deals, where a single item is passed from one agent to another.
319

Endriss, Maudet, Sadri, & Toni

Definition 5 (1-deals) A 1-deal is a deal involving the reallocation of exactly one resource.
This corresponds to the “classical” form of a contract typically found in the Contract Net
protocol (Smith, 1980). Deals where one agent passes a set of resources on to another
agent are called cluster deals. Deals where one agent gives a single item to another agent
who returns another single item are called swap deals. Sometimes it can also be necessary
to exchange resources between more than just two agents. In Sandholm’s terminology, a
multiagent deal is a deal that could involve any number of agents, where each agent passes
at most one resource to each of the other agents taking part. Finally, deals that combine the
features of the cluster and the multiagent deal type are called combined deals by Sandholm.
These could involve any number of agents and any number of resources. Therefore, every
deal δ, in the sense of Definition 2, is a combined deal. In the remainder of this paper, when
speaking about deals without further specifying their type, we are always going to refer to
combined deals (without any structural restrictions).2
An agent may or may not find a particular deal δ acceptable. Whether or not an agent
will accept a given deal depends on the rationality criterion it applies when evaluating deals.
A selfish agent i may, for instance, only accept deals δ = (A, A0 ) that strictly improve its
personal welfare: ui (A) < ui (A0 ). We call criteria such as this, which only depend on
the utilities of the agent in question, personal rationality criteria. While we do not want
to admit arbitrary rationality criteria, the classes of deals that can be characterised using
personal rationality criteria alone is somewhat too narrow for our purposes. Instead, we are
going to consider rationality criteria that are local in the sense of only depending on the
utility levels of the agents involved in the deal concerned.
Definition 6 (Local rationality criteria) A class ∆ of deals is said to be characterised
by a local rationality criterion iff it is possible to define a predicate Φ over 2A×R×R such
that a deal δ = (A, A0 ) belongs to ∆ iff Φ({(i, ui (A), ui (A0 )) | i ∈ Aδ }) holds true.
That is, Φ is mapping a set of triples of one agent name and two reals (utilities) each to
truth values. The locality aspect comes in by only applying Φ to the set of triples for
those agents whose bundle changes with δ. Therefore, for instance, the class of all deals
that increase the utility of the previously poorest agent is not characterisable by a local
rationality criterion (because this condition can only be checked by inspecting the utilities
of all the agents in the system).
2.3 Socially Optimal Allocations of Resources
As already mentioned in the introduction, we may think of a multiagent system as a society
of autonomous software agents. While agents make their local decisions on what deals to
propose and to accept, we can also analyse the system from a global or societal point of
view and may thus prefer certain allocations of resources over others. To this end, welfare
economics provides formal tools to assess how the distribution of resources amongst the
members of a society affects the well-being of society as a whole (Sen, 1970; Moulin, 1988;
Arrow et al., 2002).
2. The ontology of deal types discussed here is, of course, not exhaustive. It would, for instance, also be of
interest to consider the class of bilateral deals (involving exactly two agents but any number of items).

320

Negotiating Socially Optimal Allocations of Resources

Given the preference profiles of the individual agents in a society (which, in our framework, are represented by means of their utility functions), a social welfare ordering over
alternative allocations of resources formalises the notion of a society’s preferences. Next we
are going to formally introduce the most important social welfare orderings considered in
this paper (some additional concepts of social welfare are discussed towards the end of the
paper). In some cases, social welfare is best defined in terms of a collective utility function.
One such example is the notion of utilitarian social welfare.
Definition 7 (Utilitarian social welfare) The utilitarian social welfare swu (A) of an
allocation of resources A is defined as follows:
X
swu (A) =
ui (A)
i∈A

Observe that maximising the collective utility function swu amounts to maximising the
average utility enjoyed by the agents in the system. Asking for maximal utilitarian social
welfare is a very strong requirement. A somewhat weaker concept is that of Pareto optimality. An allocation is Pareto optimal iff there is no other allocation with higher utilitarian
social welfare that would be no worse for any of of the agents in the system (i.e. that would
be strictly better for at least one agent without being worse for any of the others).
Definition 8 (Pareto optimality) An allocation A is called Pareto optimal iff there is
no allocation A0 such that swu (A) < swu (A0 ) and ui (A) ≤ ui (A0 ) for all i ∈ A.
The first goal of an egalitarian society should be to increase the welfare of its weakest
member (Rawls, 1971; Sen, 1970). In other words, we can measure the social welfare of
such a society by measuring the welfare of the agent that is currently worst off.
Definition 9 (Egalitarian social welfare) The egalitarian social welfare swe (A) of an
allocation of resources A is defined as follows:
swe (A) = min{ui (A) | i ∈ A}
The egalitarian collective utility function swe gives rise to a social welfare ordering over
alternative allocations of resources: A0 is strictly preferred over A iff swe (A) < swe (A0 ). This
ordering is sometimes called the maximin-ordering. The maximin-ordering only takes into
account the welfare of the currently weakest agent, but is insensitive to utility fluctuations
in the rest of society. To allow for a finer distinction of the social welfare of different
allocations we introduce the so-called leximin-ordering.
For a society with n agents, let {u1 , . . . , un } be the set of utility functions for that
society. Then every allocation A determines a utility vector hu1 (A), . . . , un (A)i of length n.
If we rearrange the elements of that vector in increasing order we obtain the ordered utility
vector for allocation A, which we are going to denote by ~u(A). The number ~ui (A) is the ith
element in such a vector (for 1 ≤ i ≤ |A|). That is, ~u1 (A) for instance, is the utility value
assigned to allocation A by the currently weakest agent. We now declare a lexicographic
ordering over vectors of real numbers (such as ~u(A)) in the usual way: ~x lexicographically
precedes ~y iff ~x is a (proper) prefix of ~y or ~x and ~y share a common (proper) prefix of length
k (which may be 0) and we have ~xk+1 < ~yk+1 .
321

Endriss, Maudet, Sadri, & Toni

Definition 10 (Leximin-ordering) The leximin-ordering ≺ over alternative allocations
of resources is defined as follows:
A ≺ A0

~u(A) lexicographically precedes ~u(A0 )

iff

We write A  A0 iff either A ≺ A0 or ~u(A) = ~u(A0 ). An allocation of resources A is called
leximin-maximal iff there is no other allocation A0 such that A ≺ A0 .
Finally, we introduce the concept of Lorenz domination, a social welfare ordering that
combines utilitarian and egalitarian aspects of social welfare. The basic idea is to endorse
deals that result in an improvement with respect to utilitarian welfare without causing a
loss in egalitarian welfare, and vice versa.
Definition 11 (Lorenz domination) Let A and A0 be allocations for a society with n
agents. Then A is Lorenz dominated by A0 iff
k
X

~ui (A) ≤

i=1

k
X

~ui (A0 )

i=1

for all k with 1 ≤ k ≤ n and, furthermore, that inequality is strict for at least one k.
For any k with 1 ≤ k ≤ n, the sum referred to in the above definition is the sum of the
utility values assigned to the respective allocation of resources by the k weakest agents. For
k = 1, this sum is equivalent to the egalitarian social welfare for that allocation. For k = n,
it is equivalent to the utilitarian social welfare. An allocation of resources is called Lorenz
optimal iff it is not Lorenz dominated by any other allocation.
We illustrate some of the above social welfare concepts (and the use of ordered utility
vectors) by means of an example. Consider a society with three agents and two resources,
with the agents’ utility functions given by the following table:
u1 ({ }) = 0
u1 ({r1 }) = 5
u1 ({r2 }) = 3
u1 ({r1 , r2 }) = 8

u2 ({ }) = 0
u2 ({r1 }) = 4
u2 ({r2 }) = 2
u2 ({r1 , r2 }) = 17

u3 ({ }) = 0
u3 ({r1 }) = 2
u3 ({r2 }) = 6
u3 ({r1 , r2 }) = 7

First of all, we observe that the egalitarian social welfare will be 0 for any possible allocation
in this scenario, because at least one of the agents would not get any resources at all. Let
A be the allocation where agent 2 holds the full bundle of resources. Observe that this is
the allocation with maximal utilitarian social welfare. The corresponding utility vector is
h0, 17, 0i, i.e. ~u(A) = h0, 0, 17i. Furthermore, let A0 be the allocation where agent 1 gets
r1 , agent 2 gets r2 , and agent 3 has to be content with the empty bundle. Now we get an
ordered utility vector of h0, 2, 5i. The initial element in either vector is 0, but 0 < 2, i.e.
~u(A) lexicographically precedes ~u(A0 ). Hence, we get A ≺ A0 , i.e. A0 would be the socially
preferred allocation with respect to the leximin-ordering. Furthermore, both A and A0 are
Pareto optimal and neither is Lorenz-dominated by the other. Starting from allocation A0 ,
agents 1 and 2 swapping their respective bundles would result in an allocation with the
ordered utility vector h0, 3, 4i, i.e. this move would result in a Lorenz improvement.
322

Negotiating Socially Optimal Allocations of Resources

3. Rational Negotiation with Side Payments
In this section, we are going to discuss a first instance of the general framework of resource
allocation by negotiation set out earlier. This particular variant, which we shall refer to as
the model of rational negotiation with side payments (or simply with money), is equivalent
to a framework put forward by Sandholm where agents negotiate in order to reallocate
tasks (Sandholm, 1998). For this variant of the framework, our aim will be to negotiate
allocations with maximal utilitarian social welfare.
3.1 Individual Rationality
In this instance of our negotiation framework, a deal may be accompanied by a number
of monetary side payments to compensate some of the agents involved for accepting a loss
in utility. Rather than specifying for each pair of agents how much money the former is
supposed to pay to the latter, we simply say how much money each agent either pays out
or receives. This can be modelled by using what we call a payment function.
Definition 12 (Payment functions) A payment function is a function p from A to real
numbers satisfying the following condition:
X
p(i) = 0
i∈A

Here, p(i) > 0 means that agent i pays the amount of p(i), while p(i) < 0 means that it
receives the amount of −p(i). By definition of a payment function, the sum of all payments
is 0, i.e. the overall amount of money present in the system does not change.3
In the rational negotiation model, agents are self-interested in the sense of only proposing or accepting deals that strictly increase their own welfare (for a justification of this
approach we refer to Sandholm, 1998). This “myopic” notion of individual rationality may
be formalised as follows.
Definition 13 (Individual rationality) A deal δ = (A, A0 ) is called individually rational
iff there exists a payment function p such that ui (A0 ) − ui (A) > p(i) for all i ∈ A, except
possibly p(i) = 0 for agents i with A(i) = A0 (i).
That is, agent i will be prepared to accept the deal δ iff it has to pay less than its gain in
utility or it will get paid more than its loss in utility, respectively. Only for agents i not
affected by the deal, i.e. in case A(i) = A0 (i), there may be no payment at all. For example,
if ui (A) = 8 and ui (A0 ) = 5, then the utility of agent i would be reduced by 3 units if it were
to accept the deal δ = (A, A0 ). Agent i will only agree to this deal if it is accompanied by
a side payment of more than 3 units; that is, if the payment function p satisfies −3 > p(i).
For any given deal, there will usually be a range of possible side payments. How agents
manage to agree on a particular one is not a matter of consideration at the abstract level
at which we are discussing this framework here. We assume that a deal will go ahead
as long as there exists some suitable payment function p. We should point out that this
3. As the overall amount of money present in the system stays constant throughout the negotiation process,
it makes sense not to take it into account for the evaluation of social welfare.

323

Endriss, Maudet, Sadri, & Toni

assumption may not be justified under all circumstances. For instance, if utility functions
are not publicly known and agents are risk-takers, then a potential deal may not be identified
as such, because some of the agents may understate their interest in that deal in order to
maximise their expected payoff (Myerson & Satterthwaite, 1983). Therefore, the theoretical
results on the reachability of socially optimal allocations reported below will only apply
under the assumption that such strategic considerations will not prevent agents from making
mutually beneficial deals.
3.2 An Example
As an example, consider a system with two agents, agent 1 and agent 2, and a set of two
resources R = {r1 , r2 }. The following table specifies the values of the utility functions u1
and u2 for every subset of {r1 , r2 }:
u1 ({ }) = 0
u1 ({r1 }) = 2
u1 ({r2 }) = 3
u1 ({r1 , r2 }) = 7

u2 ({ }) = 0
u2 ({r1 }) = 3
u2 ({r2 }) = 3
u2 ({r1 , r2 }) = 8

Also suppose agent 1 initially holds the full set of resources {r1 , r2 } and agent 2 does not
own any resources to begin with.
The utilitarian social welfare for this initial allocation is 7, but it could be 8, namely if
agent 2 had both resources. As we are going to see next, the simple class of 1-deals alone are
not always sufficient to guarantee the optimal outcome of a negotiation process (if agents
abide to the individual rationality criterion for the acceptability of a deal). In our example,
the only possible 1-deals would be to pass either r1 or r2 from agent 1 to agent 2. In either
case, the loss in utility incurred by agent 1 (5 or 4, respectively) would outweigh the gain
of agent 2 (3 for either deal), so there is no payment function that would make these deals
individually rational. The cluster deal of passing {r1 , r2 } from agent 1 to 2, on the other
hand, would be individually rational if agent 2 paid agent 1 an amount of, say, 7.5 units.
Similarly to the example above, we can also construct scenarios where swap deals or
multiagent deals are necessary (i.e. where cluster deals alone would not be sufficient to
guarantee maximal social welfare). This also follows from Theorem 2, which we are going
to present later on in this section. Several concrete examples are given by Sandholm (1998).
3.3 Linking Individual Rationality and Social Welfare
The following result, first stated in this form by Endriss et al. (2003a), says that a deal (with
money) is individually rational iff it increases utilitarian social welfare. We are mainly going
to use this lemma to give a simple proof of Sandholm’s main result on sufficient contract
types (Sandholm, 1998), but it has also found useful applications in its own right (Dunne
et al., 2005; Dunne, 2005; Endriss & Maudet, 2005).
Lemma 1 (Individually rational deals and utilitarian social welfare) A deal δ =
(A, A0 ) is individually rational iff swu (A) < swu (A0 ).
324

Negotiating Socially Optimal Allocations of Resources

Proof. ‘⇒’: By definition, δ = (A, A0 ) is individually rational iff there exists a payment
function p such that ui (A0 ) − ui (A) > p(i) holds for all i ∈ A, except possibly p(i) = 0 in
case A(i) = A0 (i). If we add up the inequations for all agents i ∈ A we get:
X

(ui (A0 ) − ui (A)) >

i∈A

X

p(i)

i∈A

By definition of a payment function, the righthand side equates to 0 while, by definition of
utilitarian social welfare, the lefthand side equals swu (A0 ) − swu (A). Hence, we really get
swu (A) < swu (A0 ) as claimed.
‘⇐’: Now let swu (A) < swu (A0 ). We have to show that δ = (A, A0 ) is an individually
rational deal. We are done if we can prove that there exists a payment function p such that
ui (A0 ) − ui (A) > p(i) for all i ∈ A. We define the function p : A → R as follows:
p(i) = ui (A0 ) − ui (A) −

swu (A0 ) − swu (A)
|A|

(for i ∈ A)

P
First, observe that p really is a payment function, because we get i∈A p(i) = 0. We also
get ui (A0 ) − ui (A) > p(i) for all i ∈ A, because we have swu (A0 ) − swu (A0 ) > 0. Hence, δ
must indeed be an individually rational deal.
2
Lemma 1 suggests that the function swu does indeed provide an appropriate measure of
social well-being in societies of agents that use the notion of individual rationality (as given
by Definition 13) to guide their behaviour during negotiation. It also shows that individual
rationality is indeed a local rationality criterion in the sense of Definition 6.
3.4 Maximising Utilitarian Social Welfare
Our next aim is to show that any sequence of deals in the rational negotiation model
with side payments will converge to an allocation with maximal utilitarian social welfare;
that is, the class of individually rational deals (as given by Definition 13) is sufficient to
guarantee optimal outcomes for agent societies measuring welfare according to the utilitarian
programme (Definition 7). This has originally been shown by Sandholm (1998) in the
context of a framework where rational agents negotiate in order to reallocate tasks and
where the global aim is to minimise the overall costs of carrying out these tasks.
Theorem 1 (Maximal utilitarian social welfare) Any sequence of individually rational deals will eventually result in an allocation with maximal utilitarian social welfare.
Proof. Given that both the set of agents A as well as the set of resources R are required
to be finite, there can be only a finite number of distinct allocations of resources. Furthermore, by Lemma 1, any individually rational deal will strictly increase utilitarian social
welfare. Hence, negotiation must terminate after a finite number of deals. For the sake
of contradiction, assume that the terminal allocation A does not have maximal utilitarian
social welfare, i.e. there exists another allocation A0 with swu (A) < swu (A0 ). But then, by
Lemma 1, the deal δ = (A, A0 ) would be individually rational and thereby possible, which
contradicts our earlier assumption of A being a terminal allocation.
2
325

Endriss, Maudet, Sadri, & Toni

At first sight, this result may seem almost trivial. The notion of a multilateral deal without
any structural restrictions is a very powerful one. A single such deal allows for any number
of resources to be moved between any number of agents. From this point of view, it is not
particularly surprising that we can always reach an optimal allocation (even in just a single
step!). Furthermore, finding a suitable deal is a very complex task, which may not always
be viable in practice. The crucial point of Theorem 1 is that any sequence of deals will
result in an optimal allocation. That is, whatever deals are agreed on in the early stages of
negotiation, the system will never get stuck in a local optimum and finding an allocation
with maximal social welfare remains an option throughout (provided, of course, that agents
are actually able to identify any deal that is theoretically possible). Given the restriction to
deals that are individually rational for all the agents involved, social welfare must increase
with every single deal. Therefore, negotiation always pays off, even if it has to stop early
due to computational limitations.
The issue of complexity is still an important one. If the full range of deals is too large
to be managed in practice, it is important to investigate how close we can get to finding
an optimal allocation if we restrict the set of allowed deals to certain simple patterns.
Andersson and Sandholm (2000), for instance, have conducted a number of experiments on
the sequencing of certain contract/deal types to reach the best possible allocations within
a limited amount of time. For a complexity-theoretic analysis of the problem of deciding
whether it is possible to reach an optimal allocation by means of structurally simple types
of deals (in particular 1-deals), we refer to recent work by Dunne et al. (2005).
3.5 Necessary Deals
The next theorem improves upon Sandholm’s main result regarding necessary contract
types (Sandholm, 1998), by extending it to the cases where either all utility functions are
monotonic or all utility functions are dichotomous.4 Sandholm’s original result, translated
into our terminology, states that for any system (consisting of a set of agents A and a
set of resources R) and any (not independently decomposable) deal δ for that system, it
is possible to construct utility functions and choose an initial allocation of resources such
that δ is necessary to reach an optimal allocation, if agents only agree to individually
rational deals. All other findings on the insufficiency of certain types of contracts reported
by Sandholm (1998) may be considered corollaries to this. For instance, the fact that,
say, cluster deals alone are not sufficient to guarantee optimal outcomes follows from this
theorem if we take δ to be any particular swap deal for the system in question.
Theorem 2 (Necessary deals with side payments) Let the sets of agents and resources be fixed. Then for every deal δ that is not independently decomposable, there exist
utility functions and an initial allocation such that any sequence of individually rational
deals leading to an allocation with maximal utilitarian social welfare must include δ. This
continues to be the case even when either all utility functions are required to be monotonic
or all utility functions are required to be dichotomous.
4. In fact, our theorem not only sharpens but also also corrects a mistake in previous expositions of this
result (Sandholm, 1998; Endriss et al., 2003a), where the restriction to deals that are not independently
decomposable had been omitted.

326

Negotiating Socially Optimal Allocations of Resources

Proof. Given a set of agents A and a set of resources R, let δ = (A, A0 ) with A 6= A0 be
any deal for this system. We need to show that there are a collection of utility functions
and an initial allocation such that δ is necessary to reach an allocation with maximal social
welfare. This would be the case if A0 had maximal social welfare, A had the second highest
social welfare, and A were the initial allocation of resources.
We first prove the existence of such a collection of functions for the case where all utility
functions are required to be monotonic. Fix  such that 0 <  < 1. As we have A 6= A0 ,
there must be an agent j ∈ A such that A(j) 6= A0 (j). We now define utility functions ui
for agents i ∈ A and sets of resources R ⊆ R as follows:

|R| +  if R = A0 (i) or (R = A(i) and i 6= j)
ui (R) =
|R|
otherwise
Observe that ui is a monotonic utility function for every i ∈ A. We get swu (A0 ) = |R|+·|A|
and swu (A) = swu (A0 ) − . Because δ = (A, A0 ) is not individually decomposable, there
exists no allocation B different from both A and A0 such that either B(i) = A(i) or B(i) =
A0 (i) for all agents i ∈ A. Hence, swu (B) ≤ swu (A) for any other allocation B. That
is, A0 is the (unique) allocation with maximal social welfare and the only allocation with
higher social welfare than A. Therefore, if we were to make A the initial allocation then
δ = (A, A0 ) would be the only deal increasing social welfare. By Lemma 1, this means that
δ would be the only individually rational (and thereby the only possible) deal. Hence, δ is
indeed necessary to achieve maximal utilitarian social welfare.
The proof for the case of dichotomous utility functions is very similar; we only need to
show that a suitable collection of dichotomous utility functions can be constructed. Again,
let j be an agent with A(j) 6= A0 (j). We can use the following collection of functions:

1 if R = A0 (i) or (R = A(i) and i 6= j)
ui (R) =
0 otherwise
We get swu (A0 ) = |A|, swu (A) = swu (A0 )−1 and swu (B) ≤ swu (A) for all other allocations
B. Hence, A is not socially optimal and, with A as the initial allocation, δ would be the
only deal that is individually rational.
2
We should stress that the set of deals that are not independently decomposable includes
deals involving any number of agents and/or any number of resources. Hence, by Theorem 2,
any negotiation protocol that puts restrictions on the structural complexity of deals that
may be proposed will fail to guarantee optimal outcomes, even when there are no constraints
on either time or computational resources. This emphasises the high complexity of our
negotiation framework (see also Dunne et al., 2005; Chevaleyre et al., 2004; Endriss &
Maudet, 2005; Dunne, 2005). The fact that the necessity of (almost) the full range of deals
persists, even when all utility functions are subject to certain restrictions makes this result
even more striking. This is true in particular for the case of dichotomous functions, which
are in some sense the simplest class of utility functions (as they can only distinguish between
“good” and “bad” bundles).
To see that the restriction to deals that are not independently decomposable matters,
consider a scenario with four agents and two resources. If the deal δ of moving r1 from
agent 1 to agent 2, and r2 from agent 3 to agent 4 is individually rational, then so will be
327

Endriss, Maudet, Sadri, & Toni

either one of the two “subdeals” of moving either r1 from agent 1 to agent 2 or r2 from
agent 3 to agent 4. Hence, the deal δ (which is independently decomposable) cannot be
necessary in the sense of Theorem 2 (with reference to our proof above, in the case of δ
there are allocations B such that either B(i) = A(i) or B(i) = A0 (i) for all agents i ∈ A,
i.e. we could get swu (B) = swu (A0 )).
3.6 Additive Scenarios
Theorem 2 is a negative result, because it shows that deals of any complexity may be
required to guarantee optimal outcomes of negotiation. This is partly a consequence of the
high degree of generality of our framework. In Section 2.1, we have defined utility functions
as arbitrary functions from sets of resources to real numbers. For many application domains
this may be unnecessarily general or even inappropriate and we may be able to obtain
stronger results for specific classes of utility functions that are subject to certain restrictions.
Of course, we have already seen that this is not the case for either monotonicity (possibly
the most natural restriction) or dichotomy (possibly the most severe restriction).
Here, we are going to consider the case of additive utility functions, which are appropriate for domains where combining resources does not result in any synergy effects (in the
sense of increasing an agent’s welfare). We refer to systems where all agents have additive
utility functions as additive scenarios. The following theorem shows that for these additive
scenarios 1-deals are sufficient to guarantee outcomes with maximal social welfare.5
Theorem 3 (Additive scenarios) In additive scenarios, any sequence of individually rational 1-deals will eventually result in an allocation with maximal utilitarian social welfare.
Proof. Termination is shown as for Theorem 1. We are going to show that, whenever the
current allocation does not have maximal social welfare, then there is still a possible 1-deal
that is individually rational.
In additive domains, the utilitarian social welfare of a given allocation may be computed
by adding up the appropriate utility values for all the single resources in R. For any
allocation A, let fA be the function mapping each resource r ∈ R to the agent i ∈ A that
holds r in situation A (that is, we have fA (r) = i iff r ∈ A(i)). The utilitarian social welfare
for allocation A is then given by the following formula:
X
swu (A) =
ufA (r) ({r})
r∈R

Now suppose that negotiation has terminated with allocation A and there are no more
individually rational 1-deals possible. Furthermore, for the sake of contradiction, assume
that A is not an allocation with maximal social welfare, i.e. there exists another allocation
A0 with swu (A) < swu (A0 ). But then, by the above characterisation of social welfare for
additive scenarios, there must be at least one resource r ∈ R such that ufA (r) ({r}) <
ufA0 (r) ({r}). That is, the 1-deal δ of passing r from agent fA (r) on to agent fA0 (r) would
increase social welfare. Therefore, by Lemma 1, δ must be an individually rational deal,
i.e. contrary to our earlier assumption, A cannot be a terminal allocation. Hence, A must
be an allocation with maximal utilitarian social welfare.
2
5. This has also been observed by T. Sandholm (personal communication, September 2002).

328

Negotiating Socially Optimal Allocations of Resources

We conclude this section by briefly mentioning two recent results that both extend, in
different ways, the result stated in Theorem 3 (a detailed discussion, however, would be
beyond the scope of the present paper). The first of these results shows that rational
deals involving at most k resources each are sufficient for convergence to an allocation
with maximal social welfare whenever all utility functions are additively separable with
respect to a common partition of R —i.e. synergies across different parts of the partition
are not possible and overall utility is defined as the sum of utilities for the different sets
in the partition (Fishburn, 1970)— and each set in this partition has at most k elements
(Chevaleyre et al., 2005a).
The second result concerns a maximality property of utility functions with respect to
1-deals. Chevaleyre et al. (2005b) show that the class of modular utility functions, which is
only slightly more general than the class of additive functions considered here (namely, it is
possible to assign a non-zero utility to the empty bundle), is maximal in the sense that for no
class of functions strictly including the class of modular functions it would still be possible
to guarantee that agents using utility functions from that larger class and negotiating only
individually rational 1-deals will eventually reach an allocation with maximal utilitarian
social welfare in all cases.

4. Rational Negotiation without Side Payments
An implicit assumption made in the framework that we have presented so far is that every
agent has got an unlimited amount of money available to it to be able to pay other agents
whenever this is required for a deal that would increase utilitarian social welfare. Concretely,
if A is the initial allocation and A0 is the allocation with maximal utilitarian social welfare,
then agent i may require an amount of money just below the difference ui (A0 ) − ui (A) to be
able to get through the negotiation process. In the context of task contracting, for which
this framework has been proposed originally (Sandholm, 1998), this may be justifiable,
but for resource allocation problems it seems questionable to make assumptions about the
unlimited availability of one particular resource, namely money. In this section, we therefore
investigate to what extent the theoretical results discussed in the previous section persist
to apply when we consider negotiation processes without monetary side payments.
4.1 An Example
In a scenario without money, that is, if we do not allow for compensatory payments, we
cannot always guarantee an outcome with maximal utilitarian social welfare. To see this,
consider the following simple problem for a system with two agents, agent 1 and agent 2,
and a single resource r. The agents’ utility functions are defined as follows:
u1 ({ }) = 0
u1 ({r}) = 4

u2 ({ }) = 0
u2 ({r}) = 7

Now suppose agent 1 initially owns the resource. Then passing r from agent 1 to agent 2
would increase utilitarian social welfare by an amount of 3. For the framework with money,
agent 2 could pay agent 1, say, the amount of 5.5 units and the deal would be individually
rational for both of them. Without money (i.e. if p ≡ 0), however, no individually rational
deal is possible and negotiation must terminate with a non-optimal allocation.
329

Endriss, Maudet, Sadri, & Toni

As maximising social welfare is not generally possible, instead we are going to investigate
whether a Pareto optimal outcome (see Definition 8) is possible in the framework without
money, and what types of deals are sufficient to guarantee this.
4.2 Cooperative Rationality
As will become clear in due course, in order to get the desired convergence result, we need
to relax the notion of individual rationality a little. For the framework without money, we
also want agents to agree to a deal, if this at least maintains their utility (that is, no strict
increase is necessary). However, we are still going to require at least one agent to strictly
increase their utility. This could, for instance, be the agent proposing the deal in question.
We call deals conforming to this criterion cooperatively rational.6
Definition 14 (Cooperative rationality) A deal δ = (A, A0 ) is called cooperatively rational iff ui (A) ≤ ui (A0 ) for all i ∈ A and there is an agent j ∈ A such that uj (A) < uj (A0 ).
In analogy to Lemma 1, we still have swu (A) < swu (A0 ) for any deal δ = (A, A0 ) that is
cooperatively rational, but not vice versa. We call the instance of our negotiation framework
where all deals are cooperatively rational (and hence do not include a monetary component)
the model of rational negotiation without side payments.
4.3 Ensuring Pareto Optimal Outcomes
As the next theorem will show, the class of cooperatively rational deals is sufficient to
guarantee a Pareto optimal outcome of money-free negotiation. It constitutes the analogue
to Theorem 1 for the model of rational negotiation without side payments.
Theorem 4 (Pareto optimal outcomes) Any sequence of cooperatively rational deals
will eventually result in a Pareto optimal allocation of resources.
Proof. Every cooperatively rational deal strictly increases utilitarian social welfare (this is
where we need the condition that at least one agent behaves truly individually rational for
each deal). Together with the fact that there are only finitely many different allocations
of resources, this implies that any negotiation process will eventually terminate. For the
sake of contradiction, assume negotiation ends with allocation A, but A is not Pareto
optimal. The latter means that there exists another allocation A0 with swu (A) < swu (A0 )
and ui (A) ≤ ui (A0 ) for all i ∈ A. If we had ui (A) = ui (A0 ) for all i ∈ A, then also
swu (A) = swu (A0 ); that is, there must be at least one j ∈ A with uj (A) < uj (A0 ). But then
the deal δ = (A, A0 ) would be cooperatively rational, which contradicts our assumption of
A being a terminal allocation.
2
Observe that the proof would not have gone through if deals were required to be strictly
rational (without side payments), as this would necessitate ui (A) < ui (A0 ) for all i ∈ A.
Cooperative rationality means, for instance, that agents would be prepared to give away
resources to which they assign a utility value of 0, without expecting anything in return. In
6. Note that “cooperatively rational” agents are still essentially rational. Their willingness to cooperate
only extends to cases where they can benefit others without any loss in utility for themselves.

330

Negotiating Socially Optimal Allocations of Resources

the framework with money, another agent could always offer such an agent an infinitesimally
small amount of money, who would then accept the deal.
Therefore, our proposed weakened notion of rationality seems indeed a very reasonable
price to pay for giving up money.
4.4 Necessity Result
As our next result shows, also for the framework without side payments, deals of any
structural complexity may be necessary in order to be able to guarantee an optimal outcome
of a negotiation.7 Theorem 5 improves upon previous results (Endriss et al., 2003a) by
showing that this necessity property persists also when either all utility functions belong to
the class of monotonic functions or all utility functions belong to the class of dichotomous
functions.
Theorem 5 (Necessary deals without side payments) Let the sets of agents and resources be fixed. Then for every deal δ that is not independently decomposable, there exist
utility functions and an initial allocation such that any sequence of cooperatively rational
deals leading to a Pareto optimal allocation would have to include δ. This continues to be
the case even when either all utility functions are required to be monotonic or all utility
functions are required to be dichotomous.
Proof. The details of this proof are omitted as it is possible to simply reuse the construction
used in the proof of Theorem 2. Observe that the utility functions defined there also
guarantee ui (A) ≤ ui (A0 ) for all i ∈ A, i.e. A is not Pareto optimal, but A0 is. If we were
to make A the initial allocation, then δ = (A, A0 ) would be the only cooperatively rational
deal (as every other deal would decrease social welfare).
2
4.5 0-1 Scenarios
We conclude our study of the rational negotiation framework without side payments by
identifying a class of utility functions where we are able to achieve a reduction in structural
complexity.8 Consider a scenario where agents use additive utility functions that assign
either 0 or 1 to every single resource (this is what we call 0-1 functions).9 This may be
appropriate when we simply wish to distinguish whether or not the agent needs a particular
resource (to execute a given plan, for example). This is, for instance, the case for some of
the agents defined in the work of Sadri, Toni, and Torroni (2001). As the following theorem
shows, for 0-1 scenarios (i.e. for systems where all utility functions are 0-1 functions),
7. This theorem corrects a mistake in the original statement of the result (Endriss et al., 2003a), where the
restriction to deals that are not independently decomposable had been omitted.
8. As dichotomous functions are a special case of the non-negative functions, the full range of (not independently decomposable) deals is also necessary in scenarios with non-negative functions. Interestingly, this
changes when we restrict ourselves to positive utility functions. Now the result of Theorem 5 would not
hold anymore, because any deal that would involve a particular agent (with a positive utility function)
giving away all its resources without receiving anything in return could never be cooperatively rational.
Hence, by Theorem 4, such a deal could never be necessary to achieve a Pareto optimal allocation either.
9. Recall the distinction between 0-1 functions and dichotomous functions. The latter assign either 0 or 1
to each bundle, while the former assign either 0 or 1 to each individual resource (the utilities of bundles
then follow from the fact that 0-1 functions are additive).

331

Endriss, Maudet, Sadri, & Toni

1-deals are sufficient to guarantee convergence to an allocation with maximal utilitarian
social welfare, even in the framework without monetary side payments (where all deals are
required to be cooperatively rational).
Theorem 6 (0-1 scenarios) In 0-1 scenarios, any sequence of cooperatively rational 1deals will eventually result in an allocation with maximal utilitarian social welfare.
Proof. Termination is shown as in the proof of Theorem 4. If an allocation A does not have
maximal social welfare then it must be the case that some agent i holds a resource r with
ui ({r}) = 0 and there is another agent j in the system with uj ({r}) = 1. Passing r from i
to j would be a cooperatively rational deal, so either negotiation has not yet terminated or
we are indeed in a situation with maximal utilitarian social welfare.
2
This result may be interpreted as a formal justification for some of the negotiation strategies
proposed by Sadri et al. (2001).

5. Egalitarian Agent Societies
In this section, we are going to apply the same methodology that we have used to study
optimal outcomes of negotiation in systems designed according to utilitarian principles in
the first part of this paper to the analysis of egalitarian agent societies. The classical
counterpart to the utilitarian collective utility function swu is the egalitarian collective
utility function swe introduced in Definition 9 (Moulin, 1988; Sen, 1970; Rawls, 1971).
Therefore, we are going to study the design of agent societies for which negotiation can be
guaranteed to converge to an allocation of resources with maximal egalitarian social welfare.
Our first aim will be to identify a suitable criterion that agents inhabiting an egalitarian
agent society may use to decide whether or not to accept a particular deal. Clearly, cooperatively rational deals, for instance, would not be an ideal choice, because Pareto optimal
allocations will typically not be optimal from an egalitarian point of view (Moulin, 1988).
5.1 Pigou-Dalton Transfers and Equitable Deals
When searching the economics literature for a class of deals that would benefit society
in an egalitarian system we soon encounter Pigou-Dalton transfers. The Pigou-Dalton
principle states that whenever a utility transfer between two agents takes place which
reduces the difference in utility between the two, then that transfer should be considered
socially beneficial (Moulin, 1988). In the context of our framework, a Pigou-Dalton transfer
(between agents i and j) can be defined as follows.
Definition 15 (Pigou-Dalton transfers) A deal δ = (A, A0 ) is called a Pigou-Dalton
transfer iff it satisfies the following criteria:
• Only two agents i and j are involved in the deal: Aδ = {i, j}.
• The deal is mean-preserving: ui (A) + uj (A) = ui (A0 ) + uj (A0 ).
• The deal reduces inequality: |ui (A0 ) − uj (A0 )| < |ui (A) − uj (A)|.
332

Negotiating Socially Optimal Allocations of Resources

The second condition in this definition could be relaxed to postulate ui (A) + uj (A) ≤
ui (A0 ) + uj (A0 ), to also allow for inequality-reducing deals that increase overall utility.
Pigou-Dalton transfers capture certain egalitarian principles; but are they sufficient as
acceptability criteria to guarantee negotiation outcomes with maximal egalitarian social
welfare? Consider the following example:
u1 ({ }) = 0
u1 ({r1 }) = 3
u1 ({r2 }) = 12
u1 ({r1 , r2 }) = 15

u2 ({ }) = 0
u2 ({r1 }) = 5
u2 ({r2 }) = 7
u2 ({r1 , r2 }) = 17

The first agent attributes a relatively low utility value to r1 and a high one to r2 . Furthermore, the value of both resources together is simply the sum of the individual utilities, i.e.
agent 1 is using an additive utility function (no synergy effects). The second agent ascribes
a medium value to either resource and a very high value to the full set. Now suppose the
initial allocation of resources is A with A(1) = {r1 } and A(2) = {r2 }. The “inequality
index” for this allocation is |u1 (A) − u2 (A)| = 4. We can easily check that inequality is in
fact minimal for allocation A (which means that there can be no inequality-reducing deal,
and certainly no Pigou-Dalton transfer, given this allocation). However, allocation A0 with
A0 (1) = {r2 } and A0 (2) = {r1 } would result in a higher level of egalitarian social welfare
(namely 5 instead of 3). Hence, Pigou-Dalton transfers alone are not sufficient to guarantee
optimal outcomes of negotiations in egalitarian agent societies. We need a more general
acceptability criterion.
Intuitively, agents operating according to egalitarian principles should help any of their
fellow agents that are worse off than they are themselves (as long as they can afford to do
so without themselves ending up even worse). This means, the purpose of any exchange of
resources should be to improve the welfare of the weakest agent involved in the respective
deal. We formalise this idea by introducing the class of equitable deals.
Definition 16 (Equitable deals) A deal δ = (A, A0 ) is called equitable iff it satisfies the
following criterion:
min{ui (A) | i ∈ Aδ } < min{ui (A0 ) | i ∈ Aδ }
Recall that Aδ = {i ∈ A | A(i) 6= A0 (i)} denotes the set of agents involved in the deal δ.
Given that for δ = (A, A0 ) to be a deal we require A 6= A0 , Aδ can never be the empty set
(i.e. the minima referred to in above definition are well-defined). Note that equitability is
a local rationality criterion in the sense of Definition 6.
It is easy to see that any Pigou-Dalton transfer will also be an equitable deal, because
it will always result in an improvement for the weaker one of the two agents concerned.
The converse, however, does not hold (not even if we restrict ourselves to deals involving
only two agents). In fact, equitable deals may even increase the inequality of the agents
concerned, namely in cases where the happier agent gains more utility than the weaker does.
In the literature on multiagent systems, the autonomy of an agent (one of the central
features distinguishing multiagent systems from other distributed systems) is sometimes
equated with pure selfishness. Under such an interpretation of the agent paradigm, our
333

Endriss, Maudet, Sadri, & Toni

notion of equitability would, of course, make little sense. We believe, however, that it is
useful to distinguish different degrees of autonomy. An agent may well be autonomous in
its decision in general, but still be required to follow certain rules imposed by society (and
agreed to by the agent on entering that society).
5.2 Local Actions and their Global Effects
We are now going to prove two lemmas that provide the connection between the local
acceptability criterion given by the notion of equitability and the two egalitarian social
welfare orderings introduced in Section 2.3 (i.e. the maximin-ordering induced by swe as
well as the leximin-ordering).
The first lemma shows how global changes are reflected locally. If a deal happens to
increase (global) egalitarian social welfare, that is, if it results in a rise with respect to the
maximin-ordering, then that deal will in fact be an equitable deal.
Lemma 2 (Maximin-rise implies equitability) If A and A0 are allocations with
swe (A) < swe (A0 ), then δ = (A, A0 ) is an equitable deal.
Proof. Let A and A0 be allocations with swe (A) < swe (A0 ) and let δ = (A, A0 ). Any
agent with minimal utility for allocation A must be involved in δ, because egalitarian social
welfare, and thereby these agents’ individual utility, is higher for allocation A0 . That is,
we have min{ui (A) | i ∈ Aδ } = swe (A). Furthermore, because Aδ ⊆ A, we certainly have
swe (A0 ) ≤ min{ui (A0 ) | i ∈ Aδ }. Given our original assumption of swe (A) < swe (A0 ), we
now obtain the inequation min{ui (A) | i ∈ Aδ } < min{ui (A0 ) | i ∈ Aδ }. This shows that δ
will indeed be an equitable deal.
2
Observe that the converse does not hold; not every equitable deal will necessarily increase
egalitarian social welfare (although an equitable deal will never decrease egalitarian social
welfare either). This is, for instance, not the case if only agents who are currently better off
are involved in a deal. In fact, as argued already at the end of Section 2.2, there can be no
class of deals characterisable by a local rationality criterion (see Definition 6) that would
always result in an increase in egalitarian social welfare.
To be able to detect changes in welfare resulting from an equitable deal we require
the finer differentiation between alternative allocations of resources given by the leximinordering. In fact, as we shall see next, any equitable deal can be shown to result in a strict
improvement with respect to the leximin-ordering.
Lemma 3 (Equitability implies leximin-rise) If δ = (A, A0 ) is an equitable deal, then
A ≺ A0 .
Proof. Let δ = (A, A0 ) be a deal that satisfies the equitability criterion and define α =
min{ui (A) | i ∈ Aδ }. The value α may be considered as partitioning the ordered utility
vector ~u(A) into three subvectors: Firstly, ~u(A) has got a (possibly empty) prefix ~u(A)<α
where all elements are strictly lower than α. In the middle, it has got a subvector ~u(A)=α
(with at least one element) where all elements are equal to α. Finally, ~u(A) has got a suffix
~u(A)>α (which again may be empty) where all elements are strictly greater than α.
334

Negotiating Socially Optimal Allocations of Resources

By definition of α, the deal δ cannot affect agents whose utility values belong to ~u(A)<α .
Furthermore, by definition of equitability, we have α < min{ui (A0 ) | i ∈ Aδ }, which means
that all of the agents that are involved will end up with a utility value which is strictly
greater than α, and at least one of these agents will come from ~u(A)=α . We now collect the
information we have on ~u(A0 ), the ordered utility vector of the second allocation A0 . Firstly,
it will have a prefix ~u(A0 )<α identical to ~u(A)<α . This will be followed by a (possibly empty)
subvector ~u(A0 )=α where all elements are equal to α and which must be strictly shorter than
~u(A)=α . All of the remaining elements of ~u(A0 ) will be strictly greater than α. It follows
that ~u(A) lexicographically precedes ~u(A0 ), i.e. A ≺ A0 holds as claimed.
2
Again, the converse does not hold, i.e. not every deal resulting in a leximin-rise is necessarily
equitable. Counterexamples are deals where the utility value of the weakest agent involved
stays constant, despite there being an improvement with respect to the leximin-ordering at
the level of society.
A well-known result in welfare economics states that every Pigou-Dalton utility transfer
results in a leximin-rise (Moulin, 1988). Given that we have observed earlier that every deal
that amounts to a Pigou-Dalton transfer will also be an equitable deal, this result can now
also be regarded as a simple corollary to Lemma 3.
5.3 Maximising Egalitarian Social Welfare
Our next aim is to prove a convergence result for the egalitarian framework (in analogy to
Theorems 1 and 4). We are going to show that systems where agents negotiate equitable
deals always converge towards an allocation with maximal egalitarian social welfare.
Theorem 7 (Maximal egalitarian social welfare) Any sequence of equitable deals will
eventually result in an allocation of resources with maximal egalitarian social welfare.
Proof. By Lemma 3, any equitable deal will result in a strict rise with respect to the leximinordering ≺ (which is both irreflexive and transitive). Hence, as there are only a finite number
of distinct allocations, negotiation will have to terminate after a finite number of deals. So
suppose negotiation has terminated and no more equitable deals are possible. Let A be the
corresponding terminal allocation of resources. The claim is that A will be an allocation
with maximal egalitarian social welfare. For the sake of contradiction, assume it is not, i.e.
assume there exists another allocation A0 for the same system such that swe (A) < swe (A0 ).
But then, by Lemma 2, the deal δ = (A, A0 ) will be an equitable deal. Hence, there is still
a possible deal, namely δ, which contradicts our earlier assumption of A being a terminal
allocation. This shows that A will be an allocation with maximal egalitarian social welfare,
which proves our claim.
2
From a purely practical point of view, Theorem 7 may be of a lesser interest than the
corresponding results for utilitarian systems, because it does not refer to an acceptability
criterion that only depends on a single agent. Of course, this coincides with our intuitions about egalitarian societies: maximising social welfare is only possible by means of
cooperation and the sharing of information on agents’ preferences.
After having reached the allocation with maximal egalitarian social welfare, it may be
the case that still some equitable deals are possible, although they would not increase social
335

Endriss, Maudet, Sadri, & Toni

welfare any further (but they would still cause a leximin-rise). This can be demonstrated
by means of a simple example. Consider a system with three agents and two resources. The
following table fixes the utility functions:
u1 ({ }) = 0
u1 ({r1 }) = 5
u1 ({r2 }) = 0
u1 ({r1 , r2 }) = 5

u2 ({ }) = 6
u2 ({r1 }) = 7
u2 ({r2 }) = 6.5
u2 ({r1 , r2 }) = 7.5

u3 ({ }) = 8
u3 ({r1 }) = 9
u3 ({r2 }) = 8.5
u3 ({r1 , r2 }) = 9.5

A possible interpretation of these functions would be the following. Agent 3 is fairly well off
in any case; obtaining either of the resources r1 and r2 will not have a great impact on its
personal welfare. The same is true for agent 2, although it is slightly less well off to begin
with. Agent 1 is the poorest agent and attaches great value to r1 , but has no interest in
r2 . Suppose agent 3 initially holds both resources. This corresponds to the ordered utility
vector h0, 6, 9.5i. Passing r1 to agent 1 would lead to a new allocation with the ordered
utility vector h5, 6, 8.5i and increase egalitarian social welfare to 5, which is the maximum
that is achievable in this system. However, there is still another equitable deal that could
be implemented from this latter allocation: agent 3 could offer r2 to agent 2. Of course, this
deal does not affect agent 1. The resulting allocation would then have the ordered utility
vector h5, 6.5, 8i, which corresponds to the leximin-maximal allocation.
To be able to detect situations where a social welfare maximum has already been reached
but some equitable deals are still possible, and to be able to stop negotiation (assuming we
are only interested in maximising swe as quickly as possible), however, we would require a
non-local rationality criterion. No criterion that only takes the welfare of agents involved
in a particular deal into account could be sharp enough to always tell us whether a given
deal would increase the minimum utility in society (see also our discussion after Lemma 2).
We could define a class of strongly equitable deals that are like equitable deals but on top
of that require the (currently) weakest agent to be involved in the deal. This would be
a sharper criterion, but it would also be against the spirit of distributivity and locality,
because every single agent would be involved in every single deal (in the sense of everyone
having to announce their utility in order to be able to determine who is the weakest).
5.4 Necessity Result
As our next theorem will show, if we restrict the set of admissible deals to those that
are equitable, then every single deal δ (that is not independently decomposable) may be
necessary to guarantee an optimal result (that is, no sequence of equitable deals excluding δ
could possibly result in an allocation with maximal egalitarian social welfare). Furthermore,
our theorem improves upon a previous result (Endriss et al., 2003b) by showing that this
holds even when all utility functions are required to be dichotomous.10
Theorem 8 (Necessary deals in egalitarian systems) Let the sets of agents and resources be fixed. Then for every deal δ that is not independently decomposable, there exist
10. This theorem also corrects a mistake in the original statement of the result (Endriss et al., 2003b), where
the restriction to deals that are not independently decomposable had been omitted.

336

Negotiating Socially Optimal Allocations of Resources

utility functions and an initial allocation such that any sequence of equitable deals leading to an allocation with maximal egalitarian social welfare would have to include δ. This
continues to be the case even when all utility functions are required to be dichotomous.
Proof. Given a set of agents A and a set of resources R, let δ = (A, A0 ) be any deal for this
system. As we have A 6= A0 , there will be a (at least one) agent j ∈ A with A(j) 6= A0 (j).
We use this particular j to fix suitable (dichotomous) utility functions ui for agents i ∈ A
and sets of resources R ⊆ R as follows:

ui (R) =

1 if R = A0 (i) or (R = A(i) and i 6= j)
0 otherwise

That is, for allocation A0 every agent assigns a utility value of 1 to the resources it holds.
The same is true for allocation A, with the sole exception of agent j, who only assigns a
value of 0. For any other allocation, agents assign the value of 0 to their set of resources,
unless that set is the same as for either allocation A or A0 . As δ is not independently
decomposable, this will happen for at least one agent for every allocation different from
both A and A0 . Hence, for every such allocation at least one agent will assign a utility value
of 0 to its allocated bundle. We get swe (A0 ) = 1, swe (A) = 0, and swe (B) = 0 for every
other allocation B, i.e. A0 is the only allocation with maximal egalitarian social welfare.
The ordered utility vector of A0 is of the form h1, . . . , 1i, that of A is of the form
h0, 1, . . . , 1i, and that of any other allocation has got the form h0, . . .i, i.e. we have A ≺ A0
and B  A for all allocations B with B 6= A and B 6= A0 . Therefore, if we make A the
initial allocation of resources, then δ will be the only deal that would result in a strict rise
with respect to the leximin-ordering. Thus, by Lemma 3, δ would also be the only equitable
deal. Hence, if the set of admissible deals is restricted to equitable deals then δ is indeed
necessary to reach an allocation with maximal egalitarian social welfare.
2
This result shows, again, that there can be no structurally simple class of deals (such as the
class of deals only involving two agents at a time) that would be sufficient to guarantee an
optimal outcome of negotiation. This is the case even when agents only have very limited
options for modelling their preferences (as is the case for dichotomous utility functions).11
While this negative necessity result is shared with the two other instances of our negotiation framework we have considered, there are currently no positive results on the sufficiency
of 1-deals for restricted domains in the egalitarian setting (see Theorems 3 and 6). For instance, it is not difficult to construct counterexamples that show that even when all agents
are using additive 0-1 functions, complex deals involving all agents at the same time may be
required to reach an allocation with maximal egalitarian social welfare (a concrete example
may be found in Endriss et al., 2003b).
11. However, observe that unlike for our two variants of the framework of rational negotiation, we do not
have a necessity result for scenarios with monotonic utility functions (see Theorems 2 and 5). Using a
collection of monotonic utility functions as in the proof of Theorem 2 would not allow us to draw any
conclusions regarding the respective levels of egalitarian social welfare of A and A0 on the one hand, and
other allocations B on the other.

337

Endriss, Maudet, Sadri, & Toni

6. Negotiating Lorenz Optimal Allocations
In this section, we are going to analyse our framework of resource allocation by negotiation in
view of the notion of Lorenz optimal allocations introduced in Definition 11. We begin with
a somewhat more general discussion of possible local rationality criteria for the acceptability
of a given deal.
6.1 Local Rationality Criteria and Separability
So far, we have studied three different variants of our negotiation framework: (i) rational negotiation with side payments (aiming at maximising utilitarian social welfare); (ii) rational
negotiation without side payments (aiming at Pareto optimal outcomes); and (iii) negotiation in egalitarian agent societies. The first two instances of our framework, where agents
are either individually rational or cooperatively rational, have been natural choices as they
formalise the widely made assumptions that agents are both purely self-interested and myopic (for scenarios with and without monetary side payments, respectively).
The third variant of the framework, which applies to egalitarian agent societies, is attractive for both conceptual and technical reasons. Conceptually, egalitarian social welfare
is of interest, because it has largely been neglected in the multiagent systems literature
despite being the classical counterpart to the widely used notion of utilitarian social welfare. Technically, the analysis of egalitarian agent societies has been interesting, because
egalitarian social welfare does not admit the definition of a local rationality criterion that
directly captures the class of deals resulting in an increase with respect to this metric. This
is why we took the detour via the leximin-ordering to prove termination.
The class of social welfare orderings that can be captured by deals conforming to a
local rationality criterion is closely related to the class of separable social welfare orderings
(Moulin, 1988). In a nutshell, a social welfare ordering is separable iff it only depends on
the agents changing utility whether or not a given deal will result in an increase in social
welfare. Compare this with the notion of a local rationality criterion (Definition 6); here it
only depends on the agents changing bundle whether or not a deal is acceptable. A change
in utility presupposes a change in bundle (but not vice versa). Hence, every separable
social welfare ordering corresponds to a class of deals characterised by a local rationality
criterion (but not vice versa). This means that every separable social welfare ordering gives
rise to a local rationality criterion and proving a general convergence theorem becomes
straightforward.12 The leximin-ordering, for instance, is separable (Moulin, 1988), which is
why we are not going to further discuss this social welfare ordering in this paper.13
Similarly, the ordering over alternative allocations induced by the notion of Lorenz domination (Definition 11) is also separable. Hence, the definition of an appropriate class of
deals and the proof of a general convergence result would not yield any significant new
insights either. However, here the analysis of the effects that some of the previously intro12. Indeed, in the case of the framework of rational negotiation with side payments, the central argument in
the proof of Theorem 1 has been Lemma 1, which shows that individual rationality is in fact equivalent
to the local rationality criterion induced by swu .
13. A suitable rationality criterion would simply amount to a lexicographic comparison of the ordered utility
vectors for the subsociety of the agents involved in the deal in question.

338

Negotiating Socially Optimal Allocations of Resources

duced rationality criteria have on agent societies when social well-being is assessed in terms
of the Lorenz condition is rather instructive, as we are going to see next.
6.2 Lorenz Domination and Existing Rationality Criteria
We are now going to try to establish connections between the global welfare measure induced
by the notion of Lorenz domination on the one hand, and various local criteria on the
acceptability of a proposed deal that individual agents may choose to apply on the other. For
instance, it is an immediate consequence of Definitions 11 and 14 that, whenever δ = (A, A0 )
is a cooperatively rational deal, then A must be Lorenz dominated by A0 . As may easily be
verified, any deal that amounts to a Pigou-Dalton transfer (see Definition 15) will also result
in a Lorenz improvement. On the other hand, it is not difficult to construct examples that
show that this is not the case for the class of equitable deals anymore (see Definition 16).
That is, while some equitable deals will result in a Lorenz improvement, others will not.
Our next goal is to check whether it is possible to combine existing rationality criteria
and define a class of deals that captures the notion of Lorenz improvements in as so far
as, for any two allocations A and A0 such that A is Lorenz dominated by A0 , there exists
a sequence of deals (or possibly even a single deal) belonging to that class leading from
A to A0 . Given that both cooperatively rational deals and Pigou-Dalton transfers always
result in a Lorenz improvement, the union of these two classes of deals may seem like a
promising candidate. In fact, according to a result reported by Moulin (1988, Lemma 2.3),
it is the case that any Lorenz improvement can be implemented by means of a sequence of
Pareto improvements (i.e. cooperatively rational exchanges) and Pigou-Dalton transfers. It
is important to stress that this seemingly general result does not apply to our negotiation
framework. To see this, we consider the following example:
u1 ({ }) = 0
u1 ({r1 }) = 6
u1 ({r2 }) = 1
u1 ({r1 , r2 }) = 7

u2 ({ }) = 0
u2 ({r1 }) = 1
u2 ({r2 }) = 6
u2 ({r1 , r2 }) = 7

u3 ({ }) = 0
u3 ({r1 }) = 1
u3 ({r2 }) = 1
u3 ({r1 , r2 }) = 10

Let A be the allocation in which agent 3 owns both resources, i.e. ~u(A) = h0, 0, 10i and
utilitarian social welfare is currently 10. Allocation A is Pareto optimal, because any other
allocation would be strictly worse for agent 3. Hence, there can be no cooperatively rational
deal that would be applicable in this situation. We also observe that any deal involving only
two agents would at best result in a new allocation with a utilitarian social welfare of 7 (this
would be a deal consisting either of passing both resources on to one of the other agents,
or of passing the “preferred” resource to either agent 1 or agent 2, respectively). Hence, no
deal involving only two agents (and in particular no Pigou-Dalton transfer) could possibly
result in a Lorenz improvement. However, there is an allocation that Lorenz dominates
A, namely the allocation assigning to each one of the first two agents their respectively
preferred resource. This allocation A0 with A0 (1) = {r1 }, A0 (2) = {r2 } and A0 (3) = { } has
got the ordered utility vector h0, 6, 6i.
The reason why the general result reported by Moulin is not applicable to our domain
is that we cannot use Pigou-Dalton transfers to implement arbitrary utility transfers here.
Moulin assumes that every possible utility vector constitutes a feasible agreement. In the
339

Endriss, Maudet, Sadri, & Toni

context of resource allocation, this would mean that there is an allocation for every possible
utility vector. In our framework, where agents negotiate over a finite number of indivisible
resources, however, the range of feasible allocations is limited. For instance, in the above
example there is no feasible allocation with the (ordered) utility vector h0, 4, 6i. In Moulin’s
system, agents could first move from h0, 0, 10i to h0, 4, 6i (a Pigou-Dalton transfer) and then
from h0, 4, 6i to h0, 6, 6i (a Pareto improvement). In our system, on the other hand, this is
not possible.
6.3 Simple Pareto-Pigou-Dalton Deals and 0-1 Scenarios
As we cannot use existing rationality criteria to compose a criterion that captures the notion
of a Lorenz improvement (and that would allow us to prove a general convergence theorem),
we are going to investigate how far we can get in a scenario with restricted utility functions.
Recall our definition of 0-1 scenarios where utility functions can only be used to indicate
whether an agent does or does not need a particular resource. As we shall see next, for 0-1
scenarios, the aforementioned result of Moulin does apply. In fact, we can even sharpen it a
little by showing that only Pigou-Dalton transfers and cooperatively rational deals involving
just a single resource and two agents each are required to guarantee negotiation outcomes
that are Lorenz optimal. We first give a formal definition of this class of deals.
Definition 17 (Simple Pareto-Pigou-Dalton deals) A deal δ is called a simple
Pareto-Pigou-Dalton deal iff it is a 1-deal and either cooperatively rational or a PigouDalton transfer.
We are now going to show that this class of deals is sufficient to guarantee Lorenz optimal
outcomes of negotiations in 0-1 scenarios
Theorem 9 (Lorenz optimal outcomes) In 0-1 scenarios, any sequence of simple
Pareto-Pigou-Dalton deals will eventually result in a Lorenz optimal allocation of resources.
Proof. As pointed out earlier, any deal that is either cooperatively rational or a PigouDalton transfer will result in a Lorenz improvement (not only in the case of 0-1 scenarios).
Hence, given that there are only a finite number of distinct allocations of resources, after
a finite number of deals the system will have reached an allocation where no more simple
Pareto-Pigou-Dalton deals are possible; that is, negotiation must terminate. Now, for the
sake of contradiction, let us assume this terminal allocation A is not optimal, i.e. there
exists another allocation A0 that Lorenz dominates A. Amongst other things, this implies
swu (A) ≤ swu (A0 ), i.e. we can distinguish two cases: either (i) there has been a strict
increase in utilitarian welfare, or (ii) it has remained constant. In 0-1 scenarios, the former
is only possible if there are (at least) one resource r ∈ R and two agents i, j ∈ A such that
ui ({r}) = 0 and uj ({r}) = 1 as well as r ∈ A(i) and r ∈ A0 (j), i.e. r has been moved from
agent i (who does not need it) to agent j (who does need it). But then the 1-deal of moving
only r from i to j would be cooperatively rational and hence also a simple Pareto-PigouDalton deal. This contradicts our assumption of A being a terminal allocation.
Now let us assume that utilitarian social welfare remained constant, i.e. swu (A) =
swu (A0 ). Let k be the smallest index such that ~uk (A) < ~uk (A0 ). (This is the first k for
which the inequality in Definition 11 is strict.) Observe that we cannot have k = |A|, as
340

Negotiating Socially Optimal Allocations of Resources

this would contradict swu (A) = swu (A0 ). We shall call the agents contributing the first
k entries in the ordered utility vector ~u(A) the poor agents and the remaining ones the
rich agents. Then, in a 0-1 scenario, there must be a resource r ∈ R that is owned by
a rich agent i in allocation A and by a poor agent j in allocation A0 and that is needed
by both these agents, i.e. ui ({r}) = 1 and uj ({r}) = 1. But then moving this resource
from agent i to agent j would constitute a Pigou-Dalton transfer (and hence also a simple
Pareto-Pigou-Dalton deal) in allocation A, which again contradicts our earlier assumption
of A being terminal.
2
In summary, we have shown that (i) any allocation of resources from which no simple
Pareto-Pigou-Dalton deals are possible must be a Lorenz optimal allocation and (ii) that
such an allocation will always be reached by implementing a finite number of simple ParetoPigou-Dalton deals. As with our earlier convergence results, agents do not need to worry
about which deals to implement, as long as they are simple Pareto-Pigou-Dalton deals. The
convergence to a global optimum is guaranteed by the theorem.

7. Further Variations
In this section, we are going to briefly consider two further notions of social preference and
discuss them in the context of our framework of resource allocation by negotiation. Firstly,
we are going to introduce the idea of elitist agent societies, where social welfare is tied to
the welfare of the agent that is currently best off. Then we are going to discuss societies
where envy-free allocations of resources are desirable.
7.1 Elitist Agent Societies
Earlier we have discussed the maximin-ordering induced by the egalitarian collective utility
function swe (see Definition 9). This ordering is actually a particular case of a class of social
welfare orderings, sometimes called k-rank dictators (Moulin, 1988), where a particular
agent (the one corresponding to the kth element in the ordered utility vector) is chosen
to be the representative of society. Amongst this class of orderings, another particularly
interesting case is where the welfare of society is evaluated on the basis of the happiest
agent (as opposed to the unhappiest agent, as in the case for egalitarian welfare). We call
this the elitist approach to measuring social welfare.
Definition 18 (Elitist social welfare) The elitist social welfare swel (A) of an allocation
of resources A is defined as follows:
swel (A) = max{ui (A) | i ∈ A}
In an elitist agent society, agents would cooperate in order to support their champion (the
currently happiest agent). While such an approach may seem somewhat unethical as far
as human society is concerned, we believe that it could indeed be very appropriate for
certain societies of artificial agents. For some applications, a distributed multiagent system
may merely serve as a means for helping a single agent in that system to achieve its goal.
However, it may not always be known in advance which agent is most likely to achieve its
goal and should therefore be supported by its peers. A typical scenario could be where a
341

Endriss, Maudet, Sadri, & Toni

system designer launches different agents with the same goal, with the aim that at least one
agent achieves that goal —no matter what happens to the others. As with egalitarian agent
societies, this does not contradict the idea of agents being autonomous entities. Agents may
be physically distributed and make their own autonomous decisions on a variety of issues
whilst also adhering to certain social principles, in this case elitist ones.
From a technical point of view, designing a criterion that would allow agents inhabiting an elitist agent society to decide locally whether or not to accept a particular deal
is very similar to the egalitarian case. In analogy to the case of equitable deals defined
earlier, a suitable deal would have to increase the maximal individual welfare amongst the
agents involved in any one deal. As for the egalitarian case, there can be no class of deals
characterised by a local rationality criterion that would exactly capture the range of deals
resulting in an increase in elitist social welfare (because every agent in the system would
have to be consulted first to determine who is currently best off). To prove convergence,
we would have to resort to an auxiliary social welfare ordering (similarly to the use of the
leximin-ordering in the proof of Theorem 7).
Of course, in many cases there is a much simpler way of finding an allocation with
maximal elitist social welfare. For instance, if all agents use monotonic utility functions,
then moving all resources to the agent assigning the highest utility value to the full bundle
R would be optimal from an elitist point of view. More generally, we can always find an
elitist optimum by checking whose utility function has got the highest peak. That is, while
the highest possible elitist social welfare can easily be determined in a centralised manner,
our distributed approach can still provide a useful framework for studying the process of
actually reaching such an optimal allocation.
7.2 Reducing Envy amongst Agents
Our final example for an interesting approach to measuring social welfare in an agent society
is the issue of envy-freeness (Brams & Taylor, 1996). For a particular allocation of resources,
an agent may be “envious” of another agent if it would prefer that agent’s set of resources
over its own. Ideally, an allocation should be envy-free.
Definition 19 (Envy-freeness) An allocation of resources A is called envy-free iff we
have ui (A(i)) ≥ ui (A(j)) for all agents i, j ∈ A.
Like egalitarian social welfare, this is related to the fair division of resources amongst agents.
Envy-freeness is desirable (though not always achievable) in societies of self-interested agents
in cases where agents have to collaborate with each other over a longer period of time. In
such a case, should an agent believe that it has been ripped off, it would have an incentive
to leave the coalition which may be disadvantageous for other agents or the society as a
whole. In other words, envy-freeness plays an important role with respect to the stability of
a group. Unfortunately, envy-free allocations do not always exist. A simple example would
be a system with two agents and just a single resource, which is valued by both of them.
Then whichever agent holds that single resource will be envied by the other agent.
Furthermore, aiming at agreeing on an envy-free allocation of resources is not always
compatible with, say, negotiating Pareto optimal outcomes. Consider the following example
of two agents with identical preferences over alternative bundles of resources:
342

Negotiating Socially Optimal Allocations of Resources

u1 ({ }) = 0
u1 ({r1 }) = 1
u1 ({r2 }) = 2
u1 ({r1 , r2 }) = 0

u2 ({ }) = 0
u2 ({r1 }) = 1
u2 ({r2 }) = 2
u2 ({r1 , r2 }) = 0

For this example, either one of the two allocations where one agent owns all resources and
the other none would be envy-free (as no agent would prefer the other one’s bundle over
its own). However, such an allocation would not be Pareto optimal. On the other hand,
an allocation where each agent owns a single resource would be Pareto optimal, but not
envy-free (because the agent holding r1 would rather have r2 ).
We should stress that envy is defined on the sole basis of an agent’s private preferences,
i.e. there is no need to take other agents’ utility functions into account. Still, whether an
agent is envious does not just depend on the resources it holds, but also on the resources
it could hold and whether any of the other agents currently hold a preferred bundle. This
somewhat paradoxical situation makes envy-freeness far less amenable to our methodology
than any of the other notions of social welfare we have discussed in this paper.
To be able to measure different degrees of envy, we could, for example, count the number
of agents that are envious for a given allocation. Another option would be to compute for
each agent i that experiences any envy at all the difference between ui (A(i)) and ui (A(j))
for the agent j that i envies the most. Then the sum over all these differences would also
provide an indication of the degree of overall envy (and thereby of social welfare). In the
spirit of egalitarianism, a third option would be identify the degree of envy in society with
the degree of envy experienced by the agent that is the most envious (Lipton, Markakis,
Mossel, & Saberi, 2004). However, it is not possible to define a local acceptability criterion
in terms of the utility functions of the agents involved in a deal (and only those) that
indicates whether the deal in question would reduce envy according to any such a metric.
This is a simple consequence of the fact that a deal may affect the degree of envy experienced
by an agent not involved in the deal at all (because it could lead to one of the participating
agents ending up with a bundle preferred by the non-concerned agent in question).

8. Conclusion
We have studied an abstract negotiation framework where members of an agent society
arrange multilateral deals to exchange bundles of indivisible resources, and we have analysed
how the resulting changes in resource distribution affect society with respect to different
social welfare orderings.
For scenarios where agents act rationally in the sense of never accepting a deal that
would (even temporarily) decrease their level of welfare, we have seen that systems where
side payments are possible can guarantee outcomes with maximal utilitarian social welfare,
while systems without side payments allow, at least, for the negotiation of Pareto optimal
allocations. We have also considered two examples of special domains with restricted utility
functions, namely additive and 0-1 scenarios. In both cases, we have been able to prove the
convergence to a socially optimal allocation of resources also for negotiation protocols that
allow only for deals involving only a single resources and a pair of agents each (so-called
1-deals). In the case of agent societies where welfare is measured in terms of the egalitarian
collective utility function, we have put forward the class of equitable deals and shown that
343

Endriss, Maudet, Sadri, & Toni

negotiation processes where agents use equitability as an acceptability criterion will also
converge towards an optimal state. Another result states that, for the relatively simple
0-1 scenarios, Lorenz optimal allocations can be achieved using one-to-one negotiation by
implementing 1-deals that are either inequality-reducing or that increase the welfare of
both agents involved. We have also discussed the case of elitist agent societies where social
welfare is tied to the welfare of the most successful agent. And finally, we have pointed
out some of the difficulties associated with designing agents that would be able to negotiate
allocations of resources where the degree of envy between the agents in a society is minimal.
Specifically, we have proved the following technical results:
• The class of individually rational deals14 is sufficient to negotiate allocations with
maximal utilitarian social welfare (Theorem 1).
• In domains with additive utility functions, the class of individually rational 1-deals is
sufficient to negotiate allocations with maximal utilitarian social welfare (Theorem 3).
• The class of cooperatively rational deals is sufficient to negotiate Pareto optimal allocations (Theorem 4).
• In domains with 0-1 utility functions, the class of cooperatively rational 1-deals is
sufficient to negotiate allocations with maximal utilitarian social welfare (Theorem 6).
• The class of equitable deals is sufficient to negotiate allocations with maximal egalitarian social welfare (Theorem 7).
• In domains with 0-1 utility functions, the class of simple Pareto-Pigou-Dalton deals
(which are 1-deals) is sufficient to negotiate Lorenz optimal allocations (Theorem 9).
For each of the three convergence results that apply to deals without structural restrictions
(rather than to 1-deals), we have also proved corresponding necessity results (Theorems 2, 5,
and 8). These theorems show that any given deal (defined as a pair of allocations) that is not
independently decomposable may be necessary to be able to negotiate an optimal allocation
of resources (with respect to the chosen notion of social welfare), if deals are required
to conform to the rationality criterion in question. As a consequence of these results,
no negotiation protocol that does not allow for the representation of deals involving any
number of agents and any number of resources could ever enable agents (whose behaviour
is constrained by our various rationality criteria) to negotiate a socially optimal allocation
in all cases. Rather surprisingly, all three necessity results continue to apply even when
agents can only differentiate between resource bundles that they would be happy with and
those they they would not be happy with (using dichotomous utility functions). Theorems 2
and 5 also apply in case all agents are required to use monotonic utility functions.
A natural question that arises when considering our convergence results concerns the
complexity of the negotiation framework. How difficult is it for agents to agree on a deal
and how many deals are required before a system converges to an optimal state? The latter
of these questions has recently been addressed by Endriss and Maudet (2005). The paper
14. Recall that individually rational deals may include monetary side payments.

344

Negotiating Socially Optimal Allocations of Resources

establishes upper bounds on the number of deals required to reach any of the optimal allocations of resources referred to in the four convergence theorems for the model of rational
negotiation (i.e. Theorems 1, 3, 4, and 6). It also discusses the different aspects of complexity involved at a more general level (such as the distinction between the communication
complexity of the system, i.e. the amount of information that agents need to exchange to
reach an optimal allocation, and the computational complexity of the reasoning tasks faced
by every single agent). Dunne (2005) addresses a related problem and studies the number
of deals meeting certain structural requirements (in particular 1-deals) that are required to
reach a given target allocation (whenever this is possible at all —recall that our necessity
results show that excluding certain deal patterns will typically bar agents from reaching
optimal allocations).
In earlier work, Dunne et al. (2005) have studied the complexity of deciding whether
one-resource-at-a-time trading with side payments is sufficient to reach a given allocation
(with improved utilitarian social welfare). This problem has been shown to be NP-hard.
Other complexity results concern the computational complexity of finding a socially optimal allocation, independently from the concrete negotiation mechanism used. As mentioned
earlier, such results are closely related to the computational complexity of the winner determination problem in combinatorial auctions (Rothkopf, Pekec̆, & Harstad, 1998; Cramton
et al., 2006). Recently, NP-completeness results for this optimisation problem have been
derived with respect to several different ways of representing utility functions (Dunne et al.,
2005; Chevaleyre et al., 2004). Bouveret and Lang (2005) also address the computational
complexity of deciding whether an allocation exists that is both envy-free and Pareto optimal.
Besides presenting technical results, we have argued that a wide spectrum of social
welfare orderings (rather than just those induced by the well-known utilitarian collective
welfare function and the concept of Pareto optimality) can be of interest to agent-based
applications. In the context of a typical electronic commerce application, where participating agents have no responsibilities towards each other, a system designer may wish
to ensure Pareto optimality to guarantee that agents get maximal payoff whenever this is
possible without making any of the other agents worse off. In applications where a fair
treatment of all participants is vital (e.g. cases where the system infrastructure is jointly
owned by all the agents), an egalitarian approach to measuring social welfare may be more
appropriate. Many applications are in fact likely to warrant a mixture of utilitarian and
egalitarian principles. Here, systems that enable Lorenz optimal agreements may turn out
to be the technology of choice. Other applications, however, may require social welfare to
be measured in ways not foreseen by the models typically studied in the social sciences.
Our proposed notion of elitist welfare would be such an example. Elitism has little room in
human society, where ethical considerations are paramount, but for a particular computing
application these considerations may well be dropped or changed.
This discussion suggests an approach to multiagent systems design that we call welfare
engineering (Endriss & Maudet, 2004). It involves, firstly, the application-driven choice (or
possibly invention) of a suitable social welfare ordering and, secondly, the design of agent
behaviour profiles and negotiation mechanisms that permit (or even guarantee) socially
optimal outcomes of interactions between the agents in a system. As discussed earlier,
designing agent behaviour profiles does not necessarily contradict the idea of the autonomy
345

Endriss, Maudet, Sadri, & Toni

of an agent, because autonomy always has to be understood as being relative to the norms
governing the society in which the agent operates. We should stress that, while we have been
studying a distributed approach to multiagent resource allocation in this paper, the general
idea of exploring the full range of social welfare orderings when developing agent-based
applications also applies to centralised mechanisms (such as combinatorial auctions).
We hope to develop this methodology of welfare engineering further in our future work.
Other possible directions of future work include the identification of further social welfare
orderings and the definition of corresponding deal acceptability criteria; the continuation of
the complexity-theoretic analysis of our negotiation framework; and the design of practical
trading mechanisms (including both protocols and strategies) that would allow agents to
agree on multilateral deals involving more than just two agents at a time.

Acknowledgments
We would like to thank Jérôme Lang and various anonymous referees for their valuable
comments. This research has been partially supported by the European Commission as
part of the SOCS project (IST-2001-32530).

References
Andersson, M., & Sandholm, T. W. (2000). Contract type sequencing for reallocative negotiation. In Proceedings of the 20th International Conference on Distributed Computing
Systems (ICDCS-2000), pp. 154–160. IEEE.
Arrow, K. J., Sen, A. K., & Suzumura, K. (Eds.). (2002). Handbook of Social Choice and
Welfare, Vol. 1. North-Holland.
Bouveret, S., & Lang, J. (2005). Efficiency and envy-freeness in fair division of indivisible
goods: Logical representation and complexity. In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI-2005), pp. 935–940. Morgan
Kaufmann Publishers.
Brams, S. J., & Taylor, A. D. (1996). Fair Division: From Cake-cutting to Dispute Resolution. Cambridge University Press.
Chavez, A., Moukas, A., & Maes, P. (1997). Challenger: A multi-agent system for distributed
resource allocation. In Proceedings of the 1st International Conference on Autonomous
Agents (Agents-1997), pp. 323–331. ACM Press.
Chevaleyre, Y., Endriss, U., Estivie, S., & Maudet, N. (2004). Multiagent resource allocation
with k-additive utility functions. In Proceedings of the DIMACS-LAMSADE Workshop on Computer Science and Decision Theory, Vol. 3 of Annales du LAMSADE,
pp. 83–100.
Chevaleyre, Y., Endriss, U., Lang, J., & Maudet, N. (2005a). Negotiating over small bundles
of resources. In Proceedings of the 4th International Joint Conference on Autonomous
Agents and Multiagent Systems (AAMAS-2005), pp. 296–302. ACM Press.
Chevaleyre, Y., Endriss, U., & Maudet, N. (2005b). On maximal classes of utility functions
for efficient one-to-one negotiation. In Proceedings of the 19th International Joint
346

Negotiating Socially Optimal Allocations of Resources

Conference on Artificial Intelligence (IJCAI-2005), pp. 941–946. Morgan Kaufmann
Publishers.
Cramton, P., Shoham, Y., & Steinberg, R. (Eds.). (2006). Combinatorial Auctions. MIT
Press.
Dunne, P. E. (2005). Extremal behaviour in multiagent contract negotiation. Journal of
Artificial Intelligence Research, 23, 41–78.
Dunne, P. E., Wooldridge, M., & Laurence, M. (2005). The complexity of contract negotiation. Artificial Intelligence, 164 (1–2), 23–46.
Dunne, P. E., Laurence, M., & Wooldridge, M. (2004). Tractability results for automatic
contracting. In Proceedings of the 16th Eureopean Conference on Artificial Intelligence
(ECAI-2004), pp. 1003–1004. IOS Press.
Endriss, U., & Maudet, N. (2004). Welfare engineering in multiagent systems. In Engineering
Societies in the Agents World IV, Vol. 3071 of LNAI, pp. 93–106. Springer-Verlag.
Endriss, U., & Maudet, N. (2005). On the communication complexity of multilateral trading:
Extended report. Journal of Autonomous Agents and Multiagent Systems, 11 (1), 91–
107.
Endriss, U., Maudet, N., Sadri, F., & Toni, F. (2003a). On optimal outcomes of negotiations over resources. In Proceedings of the 2nd International Joint Conference
on Autonomous Agents and Multiagent Systems (AAMAS-2003), pp. 177–184. ACM
Press.
Endriss, U., Maudet, N., Sadri, F., & Toni, F. (2003b). Resource allocation in egalitarian agent societies. In Secondes Journées Francophones sur les Modèles Formels
d’Interaction (MFI-2003), pp. 101–110. Cépaduès-Éditions.
Fatima, S. S., Wooldridge, M., & Jennings, N. R. (2004). An agenda-based framework for
multi-issues negotiation. Artificial Intelligence, 152 (1), 1–45.
Fishburn, P. C. (1970). Utility Theory for Decision Making. John Wiley and Sons.
Fujishima, Y., Leyton-Brown, K., & Shoham, Y. (1999). Taming the computational complexity of combinatorial auctions: Optimal and approximate approaches. In Proceedings of the 16th International Joint Conference on Artificial Intelligence (IJCAI1999), pp. 548–553. Morgan Kaufmann Publishers.
Kraus, S. (2001). Strategic Negotiation in Multiagent Environments. MIT Press.
Lemaı̂tre, M., Verfaillie, G., & Bataille, N. (1999). Exploiting a common property resource
under a fairness constraint: A case study. In Proceedings of the 16th International Joint
Conference on Artificial Intelligence (IJCAI-1999), pp. 206–211. Morgan Kaufmann
Publishers.
Lipton, R. J., Markakis, E., Mossel, E., & Saberi, A. (2004). On approximately fair allocations of indivisible goods. In Proceedings of the 5th ACM Conference on Electronic
Commerce (EC-2004), pp. 125–131. ACM Press.
Moulin, H. (1988). Axioms of Cooperative Decision Making. Cambridge University Press.
347

Endriss, Maudet, Sadri, & Toni

Myerson, R. B., & Satterthwaite, M. A. (1983). Efficient mechanisms for bilateral trading.
Journal of Economic Theory, 29 (2), 265–281.
Rawls, J. (1971). A Theory of Justice. Oxford University Press.
Rosenschein, J. S., & Zlotkin, G. (1994). Rules of Encounter. MIT Press.
Rothkopf, M. H., Pekec̆, A., & Harstad, R. M. (1998). Computationally manageable combinational auctions. Management Science, 44 (8), 1131–1147.
Sadri, F., Toni, F., & Torroni, P. (2001). Dialogues for negotiation: Agent varieties and dialogue sequences. In Proceedings of the 8th International Workshop on Agent Theories,
Architectures, and Languages (ATAL-2001), pp. 405–421. Springer-Verlag.
Sandholm, T. W. (2002). Algorithm for optimal winner determination in combinatorial au
ctions. Artificial Intelligence, 135, 1–54.
Sandholm, T. W. (1998). Contract types for satisficing task allocation: I Theoretical results.
In Proceedings of the AAAI Spring Symposium: Satisficing Models.
Sandholm, T. W. (1999). Distributed rational decision making. In Weiß, G. (Ed.), Multiagent Systems: A Modern Approach to Distributed Artificial Intelligence, pp. 201–258.
MIT Press.
Sen, A. K. (1970). Collective Choice and Social Welfare. Holden Day.
Smith, R. G. (1980). The contract net protocol: High-level communication and control in a
distributed problem solver. IEEE Transactions on Computers, C-29 (12), 1104–1113.
Wooldridge, M. (2002). An Introduction to MultiAgent Systems. John Wiley and Sons.

348

Journal of Artificial Intelligence Research 25 (2006) 75-118

Submitted 01/05; published 1/06

Approximate Policy Iteration with a Policy Language Bias:
Solving Relational Markov Decision Processes
Alan Fern

afern@cs.orst.edu
School of Electrical Engineering and Computer Science, Oregon State University
Sungwook Yoon
sy@purdue.edu
Robert Givan
givan@purdue.edu
School of Electrical and Computer Engineering, Purdue University

Abstract
We study an approach to policy selection for large relational Markov Decision Processes
(MDPs). We consider a variant of approximate policy iteration (API) that replaces the
usual value-function learning step with a learning step in policy space. This is advantageous
in domains where good policies are easier to represent and learn than the corresponding
value functions, which is often the case for the relational MDPs we are interested in.
In order to apply API to such problems, we introduce a relational policy language and
corresponding learner. In addition, we introduce a new bootstrapping routine for goalbased planning domains, based on random walks. Such bootstrapping is necessary for
many large relational MDPs, where reward is extremely sparse, as API is ineffective in
such domains when initialized with an uninformed policy. Our experiments show that the
resulting system is able to find good policies for a number of classical planning domains
and their stochastic variants by solving them as extremely large relational MDPs. The
experiments also point to some limitations of our approach, suggesting future work.

1. Introduction
Many planning domains are most naturally represented in terms of objects and relations
among them. Accordingly, AI researchers have long studied algorithms for planning and
learning-to-plan in relational state and action spaces. These include, for example, “classical”
STRIPS domains such as the blocks world and logistics.
A common criticism of such domains and algorithms is the assumption of an idealized,
deterministic world model. This, in part, has led AI researchers to study planning and
learning within a decision-theoretic framework, which explicitly handles stochastic environments and generalized reward-based objectives. However, most of this work is based on
explicit or propositional state-space models, and so far has not demonstrated scalability to
the large relational domains that are commonly addressed in classical planning.
Intelligent agents must be able to simultaneously deal with both the complexity arising
from relational structure and the complexity arising from uncertainty. The primary goal
of this research is to move toward such agents by bridging the gap between classical and
decision-theoretic techniques.
In this paper, we describe a straightforward and practical method for solving very large,
relational MDPs. Our work can be viewed as a form of relational reinforcement learning
(RRL) where we assume a strong simulation model of the environment. That is, we assume
access to a black-box simulator, for which we can provide any (relationally represented)
c
2006
AI Access Foundation. All rights reserved.

Fern, Yoon, & Givan

state/action pair and receive a sample from the appropriate next-state and reward distributions. The goal is to interact with the simulator in order to learn a policy for achieving high
expected reward. It is a separate challenge, not considered here, to combine our work with
methods for learning the environment simulator to avoid dependence on being provided
such a simulator.
Dynamic-programming approaches to finding optimal control policies in MDPs (Bellman, 1957; Howard, 1960), using explicit (flat) state space representations, break down
when the state space becomes extremely large. More recent work extends these algorithms
to use propositional (Boutilier & Dearden, 1996; Dean & Givan, 1997; Dean, Givan, &
Leach, 1997; Boutilier, Dearden, & Goldszmidt, 2000; Givan, Dean, & Greig, 2003; Guestrin,
Koller, Parr, & Venkataraman, 2003b) as well as relational (Boutilier, Reiter, & Price, 2001;
Guestrin, Koller, Gearhart, & Kanodia, 2003a) state-space representations. These extensions have significantly expanded the set of approachable problems, but have not yet shown
the capacity to solve large classical planning problems such as the benchmark problems
used in planning competitions (Bacchus, 2001), let alone their stochastic variants. One possible reason for this is that these methods are based on calculating and representing value
functions. For familiar STRIPS planning domains (among others), useful value functions
can be difficult to represent compactly, and their manipulation becomes a bottle-neck.
Most of the above techniques are purely deductive—that is, each value function is guaranteed to have a certain level of accuracy. Rather, in this work, we will focus on inductive
techniques that make no such guarantees in practice. Most existing inductive forms of approximate policy iteration (API) utilize machine learning to select compactly represented
approximate value functions at each iteration of dynamic programming (Bertsekas & Tsitsiklis, 1996). As with any machine learning algorithm, the selection of the hypothesis space,
here a space of value functions, is critical to performance. An example space used frequently
is the space of linear combinations of a human-selected feature set.
To our knowledge, there has been no previous work that applies any form of API to
benchmark problems from classical planning, or their stochastic variants.1 Again, one
reason for this is the high complexity of typical value functions for these large relational
domains, making it difficult to specify good value-function spaces that facilitate learning.
Comparably, it is often much easier to compactly specify good policies, and accordingly
good policy spaces for learning. This observation is the basis for recent work on inductive policy selection in relational planning domains, both deterministic (Khardon, 1999a;
Martin & Geffner, 2000), and probabilistic (Yoon, Fern, & Givan, 2002). These techniques
show that useful policies can be learned using a policy-space bias described by a generic
(relational) knowledge representation language. Here we incorporate those ideas into a variant of API, that achieves significant success without representing or learning approximate
value functions. Of course, a natural direction for future work is to combine policy-space
techniques with value-function techniques, to leverage the advantages of both.
Given an initial policy, our approach uses the simulation technique of policy rollout
(Tesauro & Galperin, 1996) to generate trajectories of an improved policy. These trajectories are then given to a classification learner, which searches for a classifier, or policy, that
“matches” the trajectory data, resulting in an approximately improved policy. These two
1. Recent work in relational reinforcement learning has been applied to STRIPS problems with much simpler
goals than typical benchmark planning domains, and is discussed in Section 8.

76

API with a Policy Language Bias

steps are iterated until no further improvement is observed. The resulting algorithm can be
viewed as a form of API where the iteration is carried out without inducing approximate
value functions.
By avoiding value function learning, this algorithm helps address the representational
challenge of applying API to relational planning domains. However, another fundamental
challenge is that, for non-trivial relational domains, API requires some form of bootstrapping. In particular, for most STRIPS planning domains the reward, which corresponds to
achieving a goal condition, is sparsely distributed and unlikely to be reached by random exploration. Thus, initializing API with a random or uninformed policy, will likely result in no
reward signal and hence no guidance for policy improvement. One approach to bootstrapping is to rely on the user to provide a good initial policy or heuristic that gives guidance
toward achieving reward. Rather, in this work we develop a new automatic bootstrapping
approach for goal-based planning domains, which does not require user intervention.
Our bootstrapping approach is based on the idea of random-walk problem distributions.
For a given planning domain, such as the blocks world, this distribution randomly generates
a problem (i.e., an initial state and a goal) by selecting a random initial state and then
executing a sequence of n random actions, taking the goal condition to be a subset of
properties from the resulting state. The problem difficulty typically increases with n, and
for small n (short random walks) even random policies can uncover reward. Intuitively, a
good policy for problems with walk length n can be used to bootstrap API for problems with
slightly longer walk lengths. Our bootstrapping approach iterates this idea, by starting with
a random policy and very small n, and then gradually increasing the walk length until we
learn a policy for very long random walks. Such long-random-walk policies clearly capture
much domain knowledge, and can be used in various ways. Here, we show that empirically
such policies often perform well on problem distributions from relational domains used in
recent deterministic and probabilistic planning competitions.
An implementation of this bootstrapped API approach took second place of 3 competitors in the hand-tailored track of the 2004 International Probabilistic Planning Competition.2 To our knowledge this is the first machine-learning based system to be entered in
any planning competition, either deterministic or probabilistic.
Here, we give an evaluation of our system on a number of probabilistic and deterministic
relational planning domains, including the AIPS-2000 competition benchmarks, and benchmarks from the hand-tailored track of the 2004 Probabilistic Planning Competition. The
results show that the system is often able to learn policies in these domains that perform
well for long-random-walk problems. In addition, these same policies often perform well on
the planning-competition problem distributions, comparing favorably with the state-of-theart planner FF in the deterministic domains. Our experiments also highlight a number of
limitations of the current system, which point to interesting directions for future work.
The remainder of paper proceeds as follows. In Section 2, we introduce our problem
setup and then, in Section 3, present our new variant of API. In Section 4, we provide some
2. Note, however, that this approach is not hand-tailored. Rather, given a domain definition, our system
learns a policy offline, automatically, which can then be applied to any problem from the domain. We
entered the hand-tailored track because it was the only track that facilitated the use of offline learning,
by providing domains and problem generators before the competition. The other entrants were humanwritten for each domain.

77

Fern, Yoon, & Givan

technical analysis of the algorithm, giving performance bounds on the policy-improvement
step. In Sections 5 and 6, we describe an implemented instantiation of our API approach
for relational planning domains. This includes a description of a generic policy language
for relational domains, a classification learner for that language, and a novel bootstrapping
technique for goal-based domains. Section 7 presents our empirical results, and finally
Sections 8 and 9 discuss related work and future directions.

2. Problem Setup
We formulate our work in the framework of Markov Decision Processes (MDPs). While
our primary motivation is to develop algorithms for relational planning domains, we first
describe our problem setup and approach for a general, action-simulator–based MDP representation. Later, in Section 5, we describe a particular representation of planning domains
as relational MDPs and the corresponding relational instantiation of our approach.
Following and adapting Kearns, Mansour, and Ng (2002) and Bertsekas and Tsitsiklis
(1996), we represent an MDP using a generative model hS, A, T, R, Ii, where S is a finite
set of states, A is a finite, ordered set of actions, and T is a randomized action-simulation
algorithm that, given state s and action a, returns a next state s0 according to some unknown
probability distribution PT (s0 |s, a). The component R is a reward function that maps S × A
to real-numbers, with R(s, a) representing the reward for taking action a in state s, and I
is a randomized initial-state algorithm with no inputs that returns a state s according to
some unknown distribution P0 (s). We sometimes treat I and T (s, a) as random variables
with distributions P0 (·) and PT (·|s, a) respectively.
For an MDP M = hS, A, T, R, Ii, a policy π is a (possibly stochastic) mapping from S to
A. The value function of π, denoted V π (s), represents the expected, cumulative, discounted
reward of following policy π in M starting from state s, and is the unique solution to
V π (s) = E[R(s, π(s)) + γV π (T (s, π(s)))]

(1)

where 0 ≤ γ < 1 is the discount factor. The Q-value function Qπ (s, a) represents the
expected, cumulative, discounted reward of taking action a in state s and then following π,
and is given by
Qπ (s, a) = R(s, a) + γE[V π (T (s, a))]

(2)

We will measure the quality of a policy by the objective function V (π) = E[V π (I)], giving
the expected value obtained by that policy when starting from a randomly drawn initial
state. A common objective in MDP planning and reinforcement learning is to find an
optimal policy π ∗ = argmaxπ V (π). However, no automated technique, including the one
we present here, has to date been able to guarantee finding an optimal policy in the relational
planning domains we consider, in reasonable running time.
It is a well known fact that given a current policy π, we can define a new improved
policy
PI π (s) = argmaxa∈A Qπ (s, a)

(3)

such that the value function of PI π is guaranteed to 1) be no worse than that of π at each
state s, and 2) strictly improve at some state when π is not optimal. Policy iteration is an
78

API with a Policy Language Bias

algorithm for computing optimal policies by iterating policy improvement (PI) from any
initial policy to reach a fixed point, which is guaranteed to be an optimal policy. Each
iteration of policy improvement involves two steps: 1) Policy evaluation where we compute
the value function V π of the current policy π, and 2) Policy selection, where, given V π from
step 1, we select the action that maximizes Qπ (s, a) at each state, defining a new improved
policy.
Finite Horizons. Since our API variant is based on simulation, and must bound the
simulation trajectories by a horizon h, our technical analysis in Section 4 will use the notion
of finite-horizon discounted reward. The h-horizon value function Vhπ is recursively defined
as
V0π (s) = 0,

Vhπ (s) = E[R(s, π(s)) + γVh−1 (T (s, π(s)))]

(4)

giving the expected discounted reward obtained by following π for h steps from s. We also
π (T (s, a))], and the h-horizon
define the h-horizon Q-function Qπh (s, a) = R(s, a) + γE[Vh−1
π
objective function V h (π) = E[Vh (I)]. It is well known, that the effect of using a finite
horizon can be made arbitrarily small. In particular, we have that for all states s and
actions a, the approximation error decreases exponentially with h,
|V π (s) − Vhπ (s)| ≤ γ h Vmax ,
|Qπ (s, a) − Qπh (s, a)| ≤ γ h Vmax , and
Rmax
Vmax =
,
1−γ
where Rmax is the maximum of the absolute value of the reward for any action at any state.
From this we also get that |V h (π) − V (π)| ≤ γ h Vmax .

3. Approximate Policy Iteration with a Policy Language Bias
Exact solution techniques, such as policy iteration, are typically intractable for large statespace MDPs, such as those arising from relational planning domains. In this section, we
introduce a new variant of approximate policy iteration (API) intended for such domains.
First, we review a generic form of API used in prior work, based on learning approximate
value functions. Next, motivated by the fact that value functions are often difficult to learn
in relational domains, we describe our API variant, which avoids learning value functions
and instead learns policies directly as state-action mappings.
3.1 API with Approximate Value Functions
API, as described in Bertsekas and Tsitsiklis (1996), uses a combination of Monte-Carlo
simulation and inductive machine learning to heuristically approximate policy iteration in
large state-space MDPs. Given a current policy π, each iteration of API approximates
policy evaluation and policy selection, resulting in an approximately improved policy π̂.
First, the policy-evaluation step constructs a training set of samples of V π from a small but
representative set of states. Each sample is computed using simulation, estimating V π (s)
for the policy π at each state s by drawing some number of sample trajectories of π starting
79

Fern, Yoon, & Givan

at s and then averaging the cumulative, discounted reward along those trajectories. Next,
the policy-selection step uses a function approximator (e.g., a neural network) to learn an
approximation V̂ π to V π based on the training data. V̂ π then serves as a representation
for π̂, which selects actions using sampled one-step lookahead based on V̂ π , that is
π̂(s) = arg max R(s, a) + γE[V̂ π (T (s, a))].
a∈A

A common variant of this procedure learns an approximation of Qπ rather than V π .
API exploits the function approximator’s generalization ability to avoid evaluating each
state in the state space, instead only directly evaluating a small number of training states.
Thus, the use of API assumes that states and perhaps actions are represented in a factored
form (typically, a feature vector) that facilitates generalizing properties of the training data
to the entire state and action spaces. Note that in the case of perfect generalization (i.e.,
V̂ π (s) = V π (s) for all states s), we have that π̂ is equal to the exact policy improvement
PI π , and thus API simulates exact policy iteration. However, in practice, generalization is
not perfect, and there are typically no guarantees for policy improvement3 —nevertheless,
API often “converges” usefully (Tesauro, 1992; Tsitsiklis & Van Roy, 1996).
The success of the above API procedure depends critically on the ability to represent
and learn good value-function approximations. For some MDPs, such as those arising from
relational planning domains, it is often difficult to specify a space of value functions and
learning mechanism that facilitate good generalization. For example, work in relational
reinforcement learning (Dzeroski, DeRaedt, & Driessens, 2001) has shown that learning
approximate value functions for classical domains, such as the blocks world, can be problematic.4 In spite of this, it is often relatively easy to compactly specify good policies using
a language for (relational) state-action mappings. This suggests that such languages may
provide useful policy-space biases for learning in API. However, all prior API methods are
based on approximating value functions and hence can not leverage these biases. With
this motivation, we consider a form of API that directly learns policies without directly
representing or approximating value functions.
3.2 Using a Policy Language Bias
A policy is simply a classifier, possibly stochastic, that maps states to actions. Our API
approach is based on this view, and is motived by recent work that casts policy selection
as a standard classification learning problem. In particular, given the ability to observe
trajectories of a target policy, we can use machine learning to select a policy, or classifier,
that mimics the target as closely as possible. Khardon (1999b) studied this learning setting
and provided PAC-like learnability results, showing that under certain assumptions, a small
number of trajectories is sufficient to learn a policy whose value is close to that of the
target. In addition, recent empirical work, in relational planning domains (Khardon, 1999a;
Martin & Geffner, 2000; Yoon et al., 2002), has shown that by using expressive languages
3. Under very strong assumptions, API can be shown to converge in the infinite limit to a near-optimal
value function. See Proposition 6.2 of Bertsekas and Tsitsiklis (1996).
4. In particular, the RRL work has considered a variety of value-function representation including relational
regression trees, instance based methods, and graph kernels, but none of them have generalized well over
varying numbers of objects.

80

API with a Policy Language Bias

for specifying state-action mappings, good policies can be learned from sample trajectories
of good policies.
These results suggest that, given a policy π, if we can somehow generate trajectories of
an improved policy, then we can learn an approximately improved policy based on those
trajectories. This idea is the basis of our approach. Figure 1 gives pseudo-code for our API
variant, which starts with an initial policy π0 and produces a sequence of approximately
improved policies. Each iteration involves two primary steps: First, given the current
policy π, the procedure Improved-Trajectories (approximately) generates trajectories of
the improved policy π 0 = PI π . Second, these trajectories are used as training data for the
procedure Learn-Policy, which returns an approximation of π 0 . We now describe each
step in more detail.
Step 1: Generating Improved Trajectories. Given a base policy π, the simulation technique of policy rollout (Tesauro & Galperin, 1996; Bertsekas & Tsitsiklis, 1996)
computes an approximation π̂ to the improved policy π 0 = PI π , where π 0 is the result of
applying one step of policy iteration to π. Furthermore, for a given state s, policy rollout
computes π̂(s) without the need to solve for π 0 at all other states, and thus provides a
tractable way to approximately simulate the improved policy π 0 in large state-space MDPs.
Often π 0 is significantly better than π, and hence so is π̂, which can lead to substantially
improved performance at a small cost. Policy rollout has provided significant benefits in a
number of application domains, including for example Backgammon (Tesauro & Galperin,
1996), instruction scheduling (McGovern, Moss, & Barto, 2002), network-congestion control
(Wu, Chong, & Givan, 2001), and Solitaire (Yan, Diaconis, Rusmevichientong, & Van Roy,
2004).
Policy rollout computes π̂(s), the estimate of π 0 (s), by estimating Qπ (s, a) for each
action a and then taking the maximizing action to be π̂(s) as suggested by Equation 3.
Each Qπ (s, a) is estimated by drawing w trajectories of length h, where each trajectory is
the result of starting at s, taking action a, and then following the actions selected by π for
h − 1 steps. The estimate of Qπ (s, a) is then taken to be the average of the cumulative
discounted reward along each trajectory. The sampling width w and horizon h are specified
by the user, and control the trade off between increased computation time for large values,
and reduced accuracy for small values. Note that rollout applies to both stochastic and
deterministic policies and that due to variance in the Q-value estimates, the rollout policy
can be stochastic even for deterministic base policies.
The procedure Improved-Trajectories uses rollout to generate n length h trajectories
of π̂, each beginning at a randomly drawn initial state. Rather than just recording the
states and actions along each trajectory, we store additional information that is used by
our policy-learning algorithm. In particular, the i’th element of a trajectory has the form
hsi , π(si ), Q̂(si , a1 ), . . . , Q̂(si , am )i, giving the i’th state si along the trajectory, the action
selected by the current (unimproved) policy at si , and the Q-value estimates Q̂(si , a) for
each action. Note that given the Q-value information for si the learning algorithm can
determine the approximately improved action π̂(s), by maximizing over actions, if desired.
Step 2: Learn Policy. Intuitively, we want Learn-Policy to select a new policy
that closely matches the training trajectories. In our experiments, we use relatively simple
learning algorithms based on greedy search within a space of policies specified by a policylanguage bias. In Sections 5.2 and 5.3 we detail the policy-language learning bias used
81

Fern, Yoon, & Givan

by our technique, and the associated learning algorithm. In Section 4 we provide some
technical analysis of an idealized version of this algorithm, providing guidance regarding
the required number of training trajectories. We note that by labeling each training state
in the trajectories with the associated Q-values for each action, rather than simply with the
best action, we enable the learner to make more informed trade-offs, focusing on accuracy
at states where wrong decisions have high costs, which was empirically useful. Also, the
inclusion of π(s) in the training data enables the learner to adjust the data relative to π,
if desired—e.g., our learner uses a bias that focuses on states where large improvement
appears possible.
Finally, we note that for API to be effective, it is important that the initial policy
π0 provide guidance toward improvement, i.e., π0 must bootstrap the API process. For
example, in goal-based planning domains π0 should reach a goal from some of the sampled
states. In Section 6 we will discuss this important issue of bootstrapping and introduce a
new bootstrapping technique.

4. Technical Analysis
In this section, we consider a variant of the policy improvement step of our main API loop,
which learns an improved policy given a base policy π. We show how to select a sampling
width w, horizon h, and training set size n such that, under certain assumptions, the quality
of the learned policy is close to the quality of π 0 the policy iteration improvement. Similar
results have been shown for previous forms of API based on approximate value functions
(Bertsekas & Tsitsiklis, 1996), however, our assumptions are of a much different nature.5
The analysis is divided into two parts. First, following Khardon (1999b), we consider
the sample complexity of policy learning. That is, we consider how many trajectories of a
target policy must be observed by a learner before we can guarantee a good approximation
to the target. Second, we show how to apply this result, which is for deterministic policies,
to the problem of learning from rollout policies, which can be stochastic. Throughout we
assume the context of an MDP M = hS, A, T, R, Ii.
4.1 Learning Deterministic Policies
A trajectory of length h is a sequence (s0 , a0 , s1 , a1 , . . . , ah−1 , sh ) of alternating states si and
actions ai . We say that a deterministic policy π is consistent with a trajectory (s1 , a1 , . . . , sh )
if and only if for each 0 ≤ i < h, π(si ) = ai . We define Dhπ to be a distribution over the set
of all length h trajectories, such that Dhπ (t) is the probability that π generates trajectory
t = (s0 , a0 , s1 , a1 , . . . , ah−1 , sh ) according to the following process: first draw s0 according
to the initial state distribution I, and then draw si+1 from T (si , π(si )) for 0 ≤ i < h. Note
that Dhπ (t) is non-zero only if π is consistent with t.
Our policy improvement step first generates trajectories of the rollout policy π̂ (see Section 3.2), via the procedure Improved-Trajectories, and then learns an approximation
5. In particular, Bertsekas and Tsitsiklis (1996) assumes a bound on the L∞ norm of the value function
approximation, i.e., that at each state the approximation is almost perfect. Rather we assume that
the improved policy π 0 comes from a finite class of policies for which we have a consistent learner.
In both cases policy improvement can be guaranteed given an additional assumption on the minimum
Q-advantage of the MDP (see below).

82

API with a Policy Language Bias

API (n, w, h, M, π0 , γ)
// training set size n, sampling width w, horizon h,
// MDP M = hS, {a1 , . . . , am }, T, R, Ii, initial policy π0 , discount factor γ.
π ← π0 ;
loop
T ← Improved-Trajectories(n, w, h, M, π);
π ← Learn-Policy(T );
until satisfied with π;
// e.g., until change is small
Return π;
Improved-Trajectories(n, w, h, M, π)
// training set size n, sampling width w,
// horizon h, MDP M , current policy π
T ← ∅;
repeat n times // generate n trajectories of improved policy
t ← nil;
s ← state drawn from I; // draw random initial state
for i = 1 to h
hQ̂(s, a1 ), . . . , Q̂(s, am )i ← Policy-Rollout(π, s, w, h, M ); // Qπ (s, a) estimates
t ← t · hs, π(s), Q̂(s, a1 ), . . . , Q̂(s, am ))i; // concatenate new sample onto trajectory
a ← action maximizing Q̂(s, a); // action of the improved policy at state s
s ← state sampled from T (s, a); // simulate action of improved policy
T ← T ∪ t;
Return T ;
Policy-Rollout (π, s, w, h, M )

// Compute Qπ (s, a) estimates hQ̂(s, a1 ), . . . , Q̂(s, am )i

// policy π, state s, sampling width w, horizon h, MDP M
for each action ai in A
Q̂(s, ai ) ← 0;
repeat w times // Q̂(s, ai ) is an average over w trajectories
R ← R(s, ai ); s0 ← a state sampled from T (s, ai ); // take action ai in s
for i = 1 to h − 1 // take h − 1 steps of π accumulating discounted reward in R
R ← R + γ i R(s0 , π(s0 ));
s0 ← a state sampled from T (s0 , π(s0 ))
Q̂(s, ai ) ← Q̂(s, ai ) + R; // include trajectory in average
Q̂(s, ai ) ←

Q̂(s,ai )
;
w

Return hQ̂(s, a1 ), . . . , Q̂(s, am )i

Figure 1: Pseudo-code for our API algorithm. See Section 5.3 for an instantiation of LearnPolicy called Learn-Decision-List.
of π̂. Note that the rollout policy serves as a stochastic approximation of π 0 = PI π the
policy iteration improvement of π. Thus, Improved-Trajectories can be viewed as at0
tempting to draw trajectories from Dhπ , and the learning step can be viewed as learning an
83

Fern, Yoon, & Givan

0

approximation of π 0 . Imagining for the moment that we can draw trajectories from Dhπ ,
a fundamental question is how many trajectories are sufficient to ensure that the learned
policy will be about as good as π 0 . Khardon (1999b) studied this question for the case
of deterministic policies in undiscounted goal-based planning domains (i.e., MDPs where
reward is only received at goal states). Here we give a straightforward adaptation of his
main result to our problem setting where we have general reward functions and measure
the quality of a policy by V (π).
The learning-problem formulation is similar in spirit to the standard framework of Probably Approximately Correct (PAC) learning. In particular, we will assume that the target
policy comes from a finite class of deterministic policies H. For example, H may correspond
to the set of policies that can be described by bounded-length decision lists. In addition,
we assume that the learner is consistent—i.e., it returns a policy from H that is consistent
with all of the training trajectories. Under these assumptions, a relatively small number
of trajectories (logarithmic in |H|) are sufficient to ensure that with high probability the
learned policy is about as good as the target.
Proposition 1. Let H be a finite class of deterministic policies. For any π ∈ H, and any
π
set of n = −1 ln |H|
δ trajectories drawn independently from Dh , there is a 1 − δ probability
that every π̂ ∈ H consistent with the trajectories satisfies V (π̂) ≥ V (π) − 2Vmax ( + γ h ).
The proof of this proposition is in the Appendix. The computational complexity of
finding a consistent policy depends on the policy class H. Polynomial-time algorithms
can be given for interesting classes such as bounded-length decision lists—however, these
algorithms are typically too expensive for the policy classes we consider in practice. Rather,
as described in Section 5.3, we use a learner based on greedy heuristic search, which often
works well in practice.
The assumption that the target policy comes from a fixed size class H will often be
violated. However, as pointed out by Khardon (1999b), it is straightforward to give an
extension of Proposition 1 for the setting where the learner considers increasingly complex
policies until a consistent one is found. In this case, the sample complexity is related to the
encoding size of the target policy rather than the size of H, thus allowing the use of very
large and expressive policy classes without necessarily paying the full sample-complexity
price of Proposition 1.
4.2 Learning from Rollout Policies
The proof of Proposition 1 relies critically on the fact that the policy class H contains only
deterministic policies. However, in our main API loop, the target policies are computed via
rollout and hence are stochastic due to the uncertainty introduced by finite sampling. Thus,
we cannot directly use Proposition 1 in the context of learning from trajectories produced
by rollout. To deal with this problem we describe a variant of Improved-Trajectories
that can reliably generate training trajectories from the deterministic policy π 0 = PI π (see
Equation 3), which is guaranteed to improve on π if improvement is possible.
Given a base policy π, we first define Aπ (s) to be the set of actions that maximize
Qπ (s, a). Note that π 0 (s) = min Aπ (s), where the minimum is taken with respect to the action ordering provided by the MDP. Importantly this policy is deterministic and thus if we
84

API with a Policy Language Bias

can generate trajectories of it, then we can apply the above result to learn a close approximation. In order to generate trajectories of π 0 we slightly modify Improved-Trajectories.
The modification is introduced for analysis only, and our experiments are based on the procedures given in Figure 1. Our modification is to replace the action maximization step of
Improved-Trajectories (second to last statement of the for loop), which chooses the next
action a to execute, with the following two steps
Â(∆, s) ← {a0 | maxa Q̂(s, a) − Q̂(s, a0 ) ≤ ∆}
a ← min Â(∆, s)
where Q̂(s, a) is the estimate of Qπh (s, a) computed by policy rollout using a sampling width
w, and ∆ is a newly introduced parameter.
Note that if Â(∆, s) = Aπ (s), then the selected action a will equal π 0 (s). If this condition is true for every state encountered then the modified Improved-Trajectories will
effectively generate trajectories of π 0 . Thus, we would like to bound the probability that
Â(∆, s) 6= Aπ (s) to a small value by appropriately choosing the sampling width w, the
horizon h, and ∆. Unfortunately, the choice of these parameters depends on the MDP.
That is, given any particular parameter values, there is an MDP such that the event
Â(∆, s) 6= Aπ (s) has a non-negligible probability at some state. For this reason we first
define the Q-advantage ∆∗ of an MDP and show how to select appropriate parameter values
given a lower-bound on ∆∗ .
Given an MDP and policy π, let S 0 be the set of states such that s ∈ S 0 iff there are
two actions a and a0 such that Qπ (s, a) 6= Qπ (s, a0 ), i.e., there are actions with distinct
Q-values. Also for each state in S 0 define a∗1 (s) and a∗2 (s) be a best action and a second
best action respectively as measured by Qπ (s, a). The Q-advantage is defined as ∆∗ =
mins∈S 0 a∗1 (s) − a∗2 (s), which measures the minimum Q-value gap between an optimal and
sub-optimal action over the state space. Given a lower-bound on the Q-advantage of an
MDP the following proposition indicates how to select parameter values to ensure that
Â(∆, s) = Aπ (s) with high probability.
Proposition 2.
have

For any MDP with Q-advantage at least ∆∗ , and any 0 < δ 0 < 1, if we

h > logγ

∆∗
8Vmax

8Vmax
∆∗
∗
∆
2



w >
∆ =

2

ln

|A|
δ0

then for any state s, Â(∆, s) = Aπ (s) with probability at least 1 − δ 0 .
The proof is given in the Appendix. Thus, for parameter values satisfying the above conditions, if our MDP has Q-advantage at least ∆∗ then we are guaranteed that with probability
at least 1 − δ 0 that Â(∆, s) = A(∆, s). This means that Improved-Trajectories will correctly select the action π 0 (s) with probability at least 1 − δ 0 . Note that this proposition
85

Fern, Yoon, & Givan

agrees with the intuition that both h and w should increase with decreasing Q-advantage
and increasing Vmax —and also that w should increase for decreasing δ 0 .6
In order to generate n length h trajectories of π 0 , the modified Improved-Trajectories
routine must compute the set Â(∆, ·) at n · h states, yielding n · h opportunities to make an
error. To ensure that no error is made, the modified procedure sets the sampling width w
δ
. This guarantees that an error free training set is created with probability
using δ 0 = 2nh
δ
at least 1 − 2 .
Combining this observation with the assumption that π 0 ∈ H we can apply Proposition
0
1 as follows. First, generate n = −1 ln 2|H|
δ trajectories of π using the modified Improvedδ
Trajectories routine (with δ 0 = 2nh
). Next, learn a policy π̂ from these trajectories using
a consistent learner. We know that the probability of generating an imperfect training set
is bounded by 2δ , and for the chosen value of n, the failure probability of the learner is
also bounded by 2δ . Thus, we get that with probability at least 1 − δ, the learned policy π̂
satisfies V (π̂) ≥ V (π 0 ) − 2Vmax ( + γ h ), giving an approximation guarantee relative to the
improved policy π 0 . This is summarized by the following proposition.
Proposition 3. Let H be a finite class of deterministic policies, 0 < δ < 1, and 0 <  < 1.
For any MDP with Q-advantage
at least ∆∗ , any policy π such that PI π ∈ H, and any set

−1
−1
of n >  ln 2|H|δ
trajectories produced by modified Improved-Trajectories using
parameters satisfying,
∆ =

∆∗
2

h > logγ

∆∗
8Vmax

8Vmax 2 2nh|A|
w >
ln
∆∗
δ
there is at least a 1 − δ probability that every π̂ ∈ H consistent with the trajectories satisfies
V (π̂) ≥ V (PI π ) − 2Vmax ( + γ h ).




One notable aspect of this result is that there is only a logarithmic dependence on the
number of actions |A| and δ −1 . However, the practical utility is hindered by its dependence
on ∆∗ which is typically not known in practice, and can be exponentially small in the
planning horizon. Unfortunately, this dependence appears to be unavoidable for our type
of approach where we try to learn from trajectories of PI π produced by rollout. This is
because for any particular setting of the above parameters, there is always an MDP with
a small enough Q-advantage, such that the value of the rollout policy is arbitrarily worse
than that of PI π .

5. API for Relational Planning
Our work is motivated by the goal of solving relational MDPs. In particular, we are interested in finding policies for relational MDPs that represent classical planning domains and
6. At first glance it appears that the lower-bound on h decreases with increasing Vmax and decreasing ∆∗ .
However, the opposite is true since the base of the logarithm is the discount factor, which is strictly less
than one. Also note that since ∆∗ is upper-bounded by 2Vmax the bound on h will always be positive.

86

API with a Policy Language Bias

their stochastic variants. Such policies can then be applied to any problem instance from a
planning domain, and hence can be viewed as a form of domain-specific control knowledge.
In this section, we first describe a straightforward way to view classical planning domains
(not just single problem instances) as relationally factored MDPs. Next, we describe our
relational policy space in which policies are compactly represented as taxonomic decision
lists. Finally, we present a heuristic learning algorithm for this policy space.
5.1 Planning Domains as MDPs.
We say that an MDP hS, A, T, R, Ii is relational when S and A are defined by giving a finite
set of objects O, a finite set of predicates P , and a finite set of action types Y . A fact is
a predicate applied to the appropriate number of objects, e.g., on(a, b) is a blocks-world
fact. A state is a set of facts, interpreted as representing the true facts in the state. The
state space S contains all possible sets of facts. An action is an action type applied to the
appropriate number of objects, e.g., putdown(a) is a blocks-world action, and the action
space A is the set of all such actions.
A classical planning domain describes a set of problem instances with related structure,
where a problem instance gives an initial world state and goal. For example, the blocks
world is a classical planning domain, where each problem instance specifies an initial block
configuration and a set of goal conditions. Classical planners attempt to find solutions to
specific problem instances of a domain. Rather, our goal is to solve entire planning domains
by finding a policy that can be applied to all problem instances. As described below, it is
straightforward to view a classical planning domain as a relational MDP where each MDP
state corresponds to a problem instance.
State and Action Spaces. Each classical planning domain specifies a set of action
types Y , world predicates W , and possible world objects O. Together Y and O define the
MDP action space. Each state of the MDP corresponds to a single problem instance (i.e., a
world state and a goal) from the planning domain by specifying both the current world and
the goal. We achieve this by letting the set of relational MDP predicates be P = W ∪ G,
where G is a set of goal predicates. The set of goal predicates contains a predicate for
each world predicate in W , which is named by prepending a ‘g’ onto the corresponding
world predicate name (e.g., the goal predicate gclear corresponds to the world predicate
clear). With this definition of P we see that the MDP states are sets of goal and world
facts, indicating the true world facts of a problem instance and the goal conditions. It
is important to note, as described below, that the MDP actions will only change world
facts and not goal facts. Thus, this large relational MDP can be viewed as a collection of
disconnected sub-MDPs, where each sub-MDP corresponds to a distinct goal condition.
Reward Function. Given an MDP state the objective is to reach another MDP state
where the goal facts are a subset of the corresponding world facts—i.e., reach a world state
that satisfies the goal. We will call such states goal states of the MDP. For example, the
MDP state
{on-table(a), on(a, b), clear(b), gclear(b)}
is a goal state in a blocks-world MDP, but would not be a goal state without the world fact
clear(b). We represent the objective of reaching a goal state quickly by defining R to assign
a reward of zero for actions taken in goal states and negative rewards for actions in all
87

Fern, Yoon, & Givan

other states, representing the cost of taking those actions. Typically, for classical planning
domains, the action costs are uniformly -1, however, our framework allows the cost to vary
across actions.
Transition Function. Each classical planning domain provides an action simulator
(e.g., as defined by STRIPS rules) that, given a world state and action, returns a new world
state. We define the MDP transition function T to be this simulator modified to treat goal
states as terminal and to preserve without change all goal predicates in an MDP state. Since
classical planning domains typically have a large number of actions, the action definitions
are usually accompanied by preconditions that indicate the legal actions in a given state,
where usually the legal actions are a small subset of all possible actions. We assume that
T treats actions that are not legal as no-ops. For simplicity, our relational MDP definition
does not explicitly represent action preconditions, however, we assume that our algorithms
do have access to preconditions and thus only need to consider legal actions. For example,
we can restrict rollout to only the legal actions in a given state.
Initial State Distribution. Finally, the initial state distribution I can be any program
that generates legal problem instances (MDP states) of the planning domain. For example, problem domains from planning competitions are commonly distributed with problem
generators.
With these definitions, a good policy is one that can reach goal states via low-cost
action sequences from initial states drawn from I. Note that here policies are mappings
from problem instances to actions and thus can be sensitive to goal conditions. In this
way, our learned policies are able to generalize across different goals. We next describe a
language for representing such generalized policies.
5.2 Taxonomic Decision List Policies.
For single argument action types, many useful rules for planning domains take the form of
“apply action type A to any object in class C” (Martin & Geffner, 2000). For example, in the
blocks world, “pick up any clear block that belongs on the table but is not on the table”,
or in a logistics world, “unload any object that is at its destination”. Using a concept
language for describing object classes, Martin and Geffner (2000) introduced the use of
decision lists of such rules as a useful learning bias, showing promising experiments in the
deterministic blocks world. With that motivation, we consider a policy space that is similar
to the one used originally by Martin and Geffner, but generalized to handle multiple action
arguments. Also, for historical reasons, our concept language is based upon taxonomic
syntax (McAllester, 1991; McAllester & Givan, 1993), rather than on description logic as
used by Martin and Geffner.
Comparison Predicates. For relational MDPs with world and goal predicates, such
as those corresponding to classical planning domains, it is often useful for polices to compare
the current state with the goal. To this end, we introduce a new set of predicates, called
comparison predicates, which are derived from the world and goal predicates. For each
world predicate p and corresponding goal predicate gp, we introduce a new comparison
predicate cp that is defined as the conjunction of p and gp. That is, a comparison-predicate
fact is true if and only if both the corresponding world and goal predicates facts are true.
88

API with a Policy Language Bias

For example, in the blocks world, the comparison-predicate fact con(a, b) indicates that a
is on b in both the current state and the goal—i.e., on(a, b) and gon(a, b) are true.
Taxonomic Syntax. Taxonomic syntax provides a language for writing class expressions that represent sets of objects with properties of interest and serve as the fundamental
pieces with which we build policies. Class expressions are built from the MDP predicates
(including comparison predicates if applicable) and variables. In our policy representation,
the variables will be used to denote action arguments, and at runtime will be instantiated
by objects. For simplicity we only consider predicates of arity one and two, which we call
primitive classes and relations, respectively. When a domain contains predicates of arity
three or more, we automatically convert them to multiple auxiliary binary predicates. Given
a list of variables X = (x1 , . . . , xk ), class expressions are given by,
C[X] ::= C0 | xi | a-thing | ¬C[X] | (R C[X]) | (min R)
R ::= R0 | R −1 | R∗
where C[X] is a class expression, R is a relation expression, C0 is a primitive class, R0 is
a primitive relation, and xi is a variable in X. Note that, for classical planning domains,
the primitive classes and relations can be world, goal, or comparison predicates. We define the depth d(C[X]) of a class expression C[X] to be one if C[X] is either a primitive
class, a-thing, a variable, or (min R), otherwise we define d(¬C[X]) and d(R C[X]) to be
d(C[X]) + 1, where R is a relation expression and C[X] is a class expression. For a given
relational MDP we denote by Cd [X] the set of all class expressions C[X] that have a depth
of d or less.
Intuitively the class expression (R C[X]) denotes the set of objects that are related
through relation R to some object in the set C[X]. The expression (R∗ C[X]) denotes
the set of objects that are related through some “R chain” to an object in C[X]—this
constructor is important for representing recursive concepts (e.g., the blocks above a). The
expression (min R) denotes the set of objects that are minimal under the relation R.
More formally, let s be an MDP state and O = (o1 , . . . , ok ) be a variable assignment,
which assigns object oi to variable xi . The interpretation of C[X] relative to s and O is a
set of objects and is denoted by C[X]s,O . A primitive class C0 is interpreted as the set of
objects for which the predicate symbol C0 is true in s. Likewise, a primitive relation R0 is
interpreted as the set of all object tuples for which the relation R0 holds in s. The class
expression a-thing denotes the set of all objects in s. The class expression xi , where xi
is a variable, is interpreted to be the singleton set {oi }. The interpretation of compound
expressions is given by,
(¬C[X])s,O = {o | o 6∈ C[X]s,O }
(R C[X])s,O = {o | ∃o0 ∈ C[X]s,O s.t. (o0 , o) ∈ Rs,O }
(min R)s,O = {o | ∃o0 s.t. (o, o0 ) ∈ Rs,O , 6 ∃o0 s.t. (o0 , o) ∈ Rs,O }
(R∗ )s,O = ID ∪ {(o1 , ov ) | ∃o2 , . . . , ov−1 s.t. (oi , oi+1 ) ∈ Rs,O for 1 ≤ i < v}
(R−1 )s,O = {(o, o0 ) | (o0 , o) ∈ Rs,O }
where C[X] is a class expression, R is a relation expression, and ID is the identity relation.
Some examples of useful blocks-world concepts, given the primitive classes clear, gclear,
holding, and con-table, along with the primitive relations on, gon, and con, are:
89

Fern, Yoon, & Givan

• (gon−1 holding) has depth two, and denotes the block that we want under the block
being held.
• (on∗ (on gclear)) has depth three, and denotes the blocks currently above blocks
that we want to make clear.
• (con∗ con-table) has depth two, and denotes the set of blocks in well constructed
towers. To see this note that a block bv is in this class if and only if there exists a
sequence of blocks b1 , . . . , bv such that b1 is on the table in both the goal and the
current state (i.e. con-table(b1 )) and bi+1 is on bi in both the goal and current state
(i.e. con(bi , bi+1 )) for 1 ≤ i < v.
• (gon (con∗ con-table)) has depth three, and denotes the blocks that belong on top
of a currently well constructed tower.
Decision List Policies We represent policies as decision lists of action-selection rules.
Each rule has the form a(x1 , . . . , xk ) : L1 , L2 , . . . Lm , where a is a k-argument action type,
the Li are literals, and the xi are action-argument variables. We will denote the list of
action argument variables as X = (x1 , . . . , xk ). Each literal has the form x ∈ C[X], where
C[X] is a taxonomic syntax class expression and x is an action-argument variable.
Given an MDP state s and a list of action-argument objects O = (o1 , . . . , ok ), we say
that a literal xi ∈ C[X] is true given s and O iff oi ∈ C[X]s,O . We say that a rule
R = a(x1 , . . . , xk ) : L1 , L2 , . . . Lm allows action a(o1 , . . . ok ) in s iff each literal in the rule
is true given s and O. Note that if there are no literals in a rule for action type a, then all
possible actions of type a are allowed by the rule. A rule can be viewed as placing mutual
constraints on the tuples of objects that an action type can be applied to. Note that a
single rule may allow no actions or many actions of one type. Given a decision list of such
rules we say that an action is allowed by the list if it is allowed by some rule in the list,
and no previous rule allows any actions. Again, a decision list may allow no actions or
multiple actions of one type. A decision list L for an MDP defines a deterministic policy
π[L] for that MDP. If L allows no actions in state s, then π[L](s) is the least7 legal action
in s; otherwise, π[L](s) is the least legal action that is allowed by L. It is important to
note that since π[L] only considers legal actions, as specified by action preconditions, the
rules do not need to encode the preconditions, which allows for simpler rules and learning.
In other words, we can think of each rule as implicitly containing the preconditions of its
action type.
As an example of a taxonomic decision list policy consider a simple blocks-world domain
where the goal condition is always to clear off all of the red blocks. The primitive classes
in this domain are red, clear, and holding, and the single relation is on. The following
policy will solve any problem in the domain.
putdown(x1 ) : x1 ∈ holding
pickup(x1 ) : x1 ∈ clear, x1 ∈ (on∗ (on red))
7. The action ordering in a relational MDP is defined lexicographically in terms of orderings on the action
types and objects.

90

API with a Policy Language Bias

The first rule will cause the agent to putdown any block that is being held. Otherwise, if
no block is being held, then find a block x1 that is clear and is above a red block (expressed
by (on∗ (on red))) and pick it up. Appendix B gives examples of more complex policies
that are learned by our system in the experiments.
5.3 Learning Taxonomic Decision Lists
For a given relational MDP, define Rd,l to be the set of action-selection rules that have
a length of at most l literals and whose class expression have depth at most d. Also, let
Hd,l denote the policy space defined by decision lists whose rules are from Rd,l . Since the
number of depth-bounded class expressions is finite there are a finite number of rules, and
hence Hd,l is finite, though exponentially large. Our implementation of Learn-Policy, as
used in the main API loop, learns a policy in Hd,l for user specified values of d and l.
We use a Rivest-style decision-list learning approach (Rivest, 1987)—an approach also
taken by Martin and Geffner (2000) for learning class-based policies. The primary difference
between Martin and Geffner (2000) and our technique is the method for selecting individual
rules in the decision list. We use a greedy, heuristic search, while previous work used an
exhaustive enumeration approach. This difference allows us to find rules that are more
complex, at the potential cost of failing to find some good simple rules that enumeration
might discover.
Recall from Section 3, that the training set given to Learn-Policy contains trajectories
of the rollout policy. Our learning algorithm, however, is not sensitive to the trajectory
structure (i.e., the order of trajectory elements) and thus, to simplify our discussion, we
will take the input to our learner to be a training set D that contains the union of all
the trajectory elements. This means that for a trajectory set that contains n length h
trajectories, D will contain a total of n · h training examples. As described in Section 3,
each training example in D has the form hs, π(s), Q̂(s, a1 ), . . . , Q̂(s, am )i, where s is a state,
π(s) is the action selected in s by the previous policy, and Q̂(s, ai ) is the Q-value estimate
of Qπ (s, ai ). Note that in our experiments the training examples only contain values for
the legal actions in a state.
Given a training set D, a natural learning goal is to find a decision-list policy that for
each training example selects an action with the maximum estimated Q-value. This learning
goal, however, can be problematic in practice as often there are several best (or close to
best) actions as measured by the true Q-function. In such case, due to random sampling,
the particular action that looks best according to the Q-value estimates in the training set
is arbitrary. Attempting to learn a concise policy that matches these arbitrary actions will
be difficult at best and likely impossible.
One approach (Lagoudakis & Parr, 2003) to avoiding this problem is to use statistical
tests to determine the actions that are “clearly the best” (positive examples) and the ones
that are “clearly not the best” (negative examples). The learner is then asked to find a
policy that is consistent with the positive and negative examples. While this approach has
shown some empirical success, it has the potential shortcoming of throwing away most of
the Q-value information. In particular, it may not always be possible to find a policy that
exactly matches the training data. In such cases, we would like the learner to make informed
trade-offs regarding sub-optimal actions—i.e., prefer sub-optimal actions that have larger
91

Fern, Yoon, & Givan

Learn-Decision-List (D, d, l, b)
// training set D, concept depth d, rule length l, beam width b
L ← nil;
while (D is not empty)
R ← Learn-Rule(D, d, l, b);
D ← D − {d ∈ d | R covers d};
L ← Extend-List(L, R); // add R to end of list
Return L;
Learn-Rule(D, d, l, b)
// training set D, concept depth d, rule length l, beam width b
for each action type a

// compute rule for each action type a

Ra ← Beam-Search(D, d, l, b, a);
Return argmaxa Hvalue(Ra , D);
Beam-Search (D, d, l, b, a)
// training set D, concept depth d, rule length l, beam width b, action type a
k ← arity of a; X ← (x1 , . . . , xk );

//

L ← {(x ∈ C) | x ∈ X, C ∈ Cd [X]}; //

X is a sequence of action-argument variables
construct the set of depth bounded candidate literals

B0 ← { a(X) : nil }; i ← 1; // initialize beam to a single rule with no literals
loop
G = Bi−1 ∪ {R ∈ Rd,l | R = Add-Literal(R0 , l), R0 ∈ Bi−1 , l ∈ L};
Bi ← Beam-Select(G, b, D); //

select best b heuristic values

i ← i + 1;
until Bi−1 = Bi ; //

loop until there is no more improvement in heuristic

Return argmaxR∈Bi Hvalue(R, D) //

return best rule in final beam

Figure 2: Pseudo-code for learning a decision list in Hd,l given training data D. The
procedure Add-Literal(R, l) simply returns a rule where literal l is added to the end of
rule R. The procedure Beam-Select(G, w, D) selects the best b rules in G with different
heuristic values. The procedure Hvalue(R, D) returns the heuristic value of rule R relative
to training data D and is described in the text.

Q-values. With this motivation, below we describe a cost-sensitive decision-list learner that
is sensitive to the full set of Q-values in D. The learning goal is roughly to find a decision
list that selects actions with large cumulative Q-value over the training set.
Learning List of Rules. We say that a decision list L covers a training example
hs, π(s), Q̂(s, a1 ), . . . , Q̂(s, am )i if L suggests an action in state s. Given a set of training
examples D, we search for a decision list that selects actions with high Q-value via an
iterative set-covering approach carried out by Learn-Decision-List. Decision-list rules
92

API with a Policy Language Bias

are constructed one at a time and in order until the list covers all of the training examples.
Pseudo-code for our algorithm is given in Figure 2. Initially, the decision list is the null list
and does not cover any training examples. During each iteration, we search for a high quality
rule R with quality measured relative to the set of currently uncovered training examples.
The selected rule is appended to the current decision-list, and the training examples newly
covered by the selected rule are removed from the training set. This process repeats until
the list covers all of the training examples. The success of this approach depends heavily
on the function Learn-Rule, which selects a good rule relative to the uncovered training
examples—typically a good rule is one that selects actions with the best (or close to best)
Q-value and also covers a significant number of examples.
Learning Individual Rules. The input to the rule learner Learn-Rule is a set of
training examples, along with depth and length parameters d and l, and a beam width b.
For each action type a, the rule learner calls the routine Beam-Search to find a good rule
Ra in Rd,l for action type a. Learn-Rule then returns the rule Ra with the highest value
as measured by our heuristic, which is described later in this section.
For a given action type a, the procedure Beam-Search generates a beam B0 , B1 . . .,
where each Bi is a set of rules in Rd,l for action type a. The sets evolve by specializing
rules in previous sets by adding literals to them, guided by our heuristic function. Search
begins with the most general rule a(X) : nil, which allows any action of type a in any state.
Search iteration i produces a set Bi that contains b rules with the highest different heuristic
values among those in the following set8
G = Bi−1 ∪ {R ∈ Rd,l | R = Add-Literal(R0 , l), R0 ∈ Bi−1 , l ∈ L}
where L is the set of all possible literals with a depth of d or less. This set includes the
current best rules (those in Bi−1 ) and also any rule in Rd,l that can be formed by adding
a new literal to a rule in Bi−1 . The search ends when no improvement in heuristic value
occurs, that is when Bi = Bi−1 . Beam-Search then returns the best rule in Bi according
to the heuristic.
Heuristic Function. For a training instance hs, π(s), Q̂(s, a1 ), . . . , Q̂(s, am )i, we define the Q-advantage of taking action ai instead of π(s) in state s by ∆(s, ai ) = Q̂(s, ai ) −
Q̂(s, π(s)). Likewise, the Q-advantage of a rule R is the sum of the Q-advantages of actions
allowed by R in s. Given a rule R and a set of training examples D, our heuristic function
Hvalue(R, D) is equal to the number of training examples that the rule covers plus the
cumulative Q-advantage of the rule over the training examples.9 Using Q-advantage rather
than Q-value focuses the learner toward instances where large improvement over the previous policy is possible. Naturally, one could consider using different weights for the coverage
and Q-advantage terms, possibly tuning the weight automatically using validation data.
8. Since many rules in Rd,l are equivalent, we must prevent the beam from filling up with semantically
equivalent rules. Rather than deal with this problem via expensive equivalence testing we take an ad-hoc,
but practically effective approach. We assume that rules do not coincidentally have the same heuristic
value, so that ones that do must be equivalent. Thus, we construct beams whose members all have
different heuristic values. We choose between rules with the same value by preferring shorter rules, then
arbitrarily.
9. If the coverage term is not included, then covering a zero Q-advantage example is the same as not
covering it. But zero Q-advantage can be good (e.g., the previous policy is optimal in that state).

93

Fern, Yoon, & Givan

6. Random Walk Bootstrapping
There are two issues that are critical to the success of our API technique. First, API is
fundamentally limited by the expressiveness of the policy language and the strength of the
learner, which dictates its ability to capture the improved policy described by the training
data at each iteration. Second, API can only yield improvement if Improved-Trajectories
successfully generates training data that describes an improved policy. For large classical
planning domains, initializing API with an uninformed random policy will typically result
in essentially random training data, which is not helpful for policy improvement. For
example, consider the MDP corresponding to the 20-block blocks world with an initial
problem distribution that generates random initial and goal states. In this case, a random
policy is unlikely to reach a goal state within any practical horizon time. Hence, the
rollout trajectories are unlikely to reach the goal, providing no guidance toward learning an
improved policy (i.e., a policy that can more reliably reach the goal).
Because we are interested in solving large domains such as this, providing guiding inputs
to API is critical. In Fern, Yoon, and Givan (2003), we showed that by bootstrapping API
with the domain-independent heuristic of the planner FF (Hoffmann & Nebel, 2001), API
was able to uncover good policies for the blocks world, simplified logistics world (no planes),
and stochastic variants. This approach, however, is limited by the heuristic’s ability to
provide useful guidance, which can vary widely across domains.
Here we describe a new bootstrapping procedure for goal-based planning domains, based
on random walks, for guiding API toward good policies. Our planning system, which is
evaluated in Section 7, is based on integrating this procedure with API in order to find
policies for goal-based planning domains. For non-goal-based MDPs, this bootstrapping
procedure can not be directly applied, and other bootstrapping mechanisms must be used
if necessary. This might include providing an initial non-trivial policy, providing a heuristic
function, or some form of reward shaping (Mataric, 1994). Below, we first describe the
idea of random-walk distributions. Next, we describe how to use these distributions in the
context of bootstrapping API, giving a new algorithm LRW-API.
6.1 Random Walk Distributions
Throughout we consider an MDP M = hS, A, T, R, Ii that correspond to goal-based planning domains, as described in Section 5.1. Recall that each state s ∈ S corresponds to a
planning problem, specifying a world state (via world facts) and a set of goal conditions (via
goal facts). We will use the terms “MDP state” and “planning problem” interchangeably.
Note that, in this context, I is a distribution over planning problems. For convenience we
will denote MDP states as tuples s = (w, g), where w and g are the sets of world facts and
goal facts in s respectively.
Given an MDP state s = (w, g) and set of goal predicates G, we define s|G to be the
MDP state (w, g 0 ) where g 0 contains those goal facts in g that are applications of a predicate
in G. Given M and a set of goal predicates G, we define the n-step random-walk problem
distribution RW n (M, G) by the following stochastic algorithm:
1. Draw a random state s0 = (w0 , g0 ) from the initial state distribution I.
94

API with a Policy Language Bias

2. Starting at s0 take n uniformly random actions10 , giving a state sequence (s0 , . . . , sn ),
where sn = (wn , g0 ) (recall that actions do not change goal facts). At each uniformly
random action selection, we assume that an extra “no-op” action (that does not change
the state) is selected with some fixed probability, for reasons explained below.
3. Let g be the set of goal facts corresponding to the world facts in wn , so e.g., if
wn = {on(a, b), clear(a)}, then g = {gon(a, b), gclear(a)}. Return the planning
problem (MDP state) (s0 , g)|G as the output.
We will sometimes abbreviate RW n (M, G) by RW n when M and G are clear in context.
Intuitively, to perform well on this distribution a policy must be able to achieve facts
involving the goal predicates that typically result after an n-step random walk from an
initial state. By restricting the set of goal predicates G we can specify the types of facts
that we are interested in achieving—e.g., in the blocks world we may only be interested in
achieving facts involving the “on” predicate.
The random-walk distributions provide a natural way to span a range of problem difficulties. Since longer random walks tend to take us further from an initial state, for small
n we typically expect that the planning problems generated by RW n will become more
difficult as n grows. However, as n becomes large, the problems generated will require far
fewer than n steps to solve—i.e., there will be more direct paths from an initial state to the
end state of a long random walk. Eventually, since S is finite, the problem difficulty will
stop increasing with n.
A question raised by this idea is whether, for large n, good performance on RW n
ensures good performance on other problem distributions of interest in the domain. In
some domains, such as the simple blocks world11 , good random-walk performance does
seem to yield good performance on other distributions of interest. In other domains, such
as the grid world (with keys and locked doors), intuitively, a random walk is very unlikely
to uncover a problem that requires unlocking a sequence of doors. Indeed, since RW n is
insensitive to the goal distribution of the underlying planning domain, the random-walk
distribution may be quite different.
We believe that good performance on long random walks is often useful, but is only
addressing one component of the difficulty of many planning benchmarks. To successfully
address problems with other components of difficulty, a planner will need to deploy orthogonal technology such as landmark extraction for setting subgoals (Hoffman, Porteous, &
Sebastia, 2004). For example, in the grid world, if we could automatically set the subgoal
of possessing a key for the first door, a long random-walk policy could provide a useful
macro for getting that key.
For the purpose of developing a bootstrapping technique for API, we limit our focus
to finding good policies for long random walks. In our experiments, we define “long” by
specifying a large walk length N . Theoretically, the inclusion of the “no-op” action in the
definition of RW ensures that the induced random-walk Markov chain12 is aperiodic, and
10. In practice, we only select random actions from the set of applicable actions in a state si , provided our
simulator makes it possible to identify this set.
11. In the blocks world with large n, RW n generates various pairs of random block configurations, typically
pairing states that are far apart—clearly, a policy that performs well on this distribution has captured
significant information about the blocks world.
12. We don’t formalize this chain here, but various formalizations work well.

95

Fern, Yoon, & Givan

thus that the distribution over states reached by increasingly long random walks converges
to a stationary distribution13 . Thus RW ∗ = limn→∞ RW n is well-defined, and we take
good performance on RW ∗ to be our goal.
6.2 Random-Walk Bootstrapping
For an MDP M , we define M [I 0 ] to be an MDP identical to M only with the initial state
distribution replaced by I 0 . We also define the success ratio SR(π, M [I]) of π on M [I] as
the probability that π solves a problem drawn from I. Also treating I as a random variable,
the average length AL(π, M [I]) of π on M [I] is the conditional expectation of the solution
length of π on problems drawn from I given that π solves I. Typically the solution length of
a problem is taken to be the number of actions, however, when action costs are not uniform,
the length is taken to be the sum of the action costs. Note that for the MDP formulation
of classical planning domains, given in Section 5.1, if a policy π achieves a high V (π) then
it will also have a high success ratio and low average cost.
Given an MDP M and set of goal predicates G, our system attempts to find a good
policy for M [RW N ], where N is selected to be large enough to adequately approximate
RW ∗ , while still allowing tractable completion of the learning. Naively, given an initial
random policy π0 , we could try to apply API directly. However, as already discussed, this
will not work in general, since we are interested in planning domains where RW ∗ produces
extremely large and difficult problems where random policies provide an ineffective starting
point.
However, for very small n (e.g., n = 1), RW n typically generates easy problems, and
it is likely that API, starting with even a random initial policy, can reliably find a good
policy for RW n . Furthermore, we expect that if a policy πn performs well on RW n , then
it will also provide reasonably good, but perhaps not perfect, guidance on problems drawn
from RW m when m is only moderately larger than n. Thus, we expect to be able to find a
good policy for RW m by bootstrapping API with initial policy πn . This suggests a natural
iterative bootstrapping technique to find a good policy for large n (in particular, for n = N ).
Figure 3 gives pseudo-code for the procedure LRW-API which integrates API and
random-walk bootstrapping to find a policy for the long-random-walk problem distribution.
Intuitively, this algorithm can be viewed as iterating through two stages: first, finding a
hard enough distribution for the current policy (by increasing n); and, then, finding a good
policy for the hard distribution using API. The algorithm maintains a current policy π
and current walk length n (initially n = 1). As long as the success ratio of π on RWn is
below the success threshold τ , which is a constant close to one, we simply iterate steps of
approximate policy improvement. Once we achieve a success ratio of τ with some policy π,
the if-statement increases n until the success ratio of π on RW n falls below τ − δ. That is,
when π performs well enough on the current n-step distribution we move on to a distribution
that is slightly harder. The constant δ determines how much harder and is set small enough
so that π can likely be used to bootstrap policy improvement on the harder distribution.
(The simpler method of just increasing n by 1 whenever success ratio τ is achieved will also
13. The Markov chain may not be irreducible, so the same stationary distribution may not be reached from
all initial states; however, we are only considering one initial state, described by I.

96

API with a Policy Language Bias

LRW-API (N, G, n, w, h, M, π0 , γ)
// max random-walk length N , goal predicates G
// training set size n, sampling width w, horizon h,
// MDP M , initial policy π0 , discount factor γ.
π ← π0 ; n ← 1;
loop
c π (n) > τ
if SR

// Find harder n-step distribution for π.
c π (i) < τ − δ, or N if none;
n ← least i ∈ [n, N ] s.t. SR
M 0 = M [RW n (M, G)];
T ← Improved-Trajectories(n, w, h, M 0 , π);
π ← Learn-Policy(T );
until satisfied with π
Return π;

c π (n) estimates the success ratio of π in planning
Figure 3: Pseudo-code for LRW-API. SR
domain D on problems drawn from RW n (M, G) by drawing a set of problems and returning
the fraction solved by π. Constants τ and δ are described in the text.

find good policies whenever this method does. This can take much longer, as it may run
API repeatedly on a training set for which we already have a good policy.)
Once n becomes equal to the maximum walk length N , we will have n = N for all future
iterations. It is important to note that even after we find a policy with a good success ratio
on RW N it may still be possible to improve on the average length of the policy. Thus,
we continue API on this distribution until we are satisfied with both the success ratio and
average length of the current policy.

7. Relational Planning Experiments
In this section, we evaluate the LRW-API technique on relational MDPs corresponding to
deterministic and stochastic classical planning domains. We first give results for a number of
deterministic benchmark domains, showing promising results in comparison with the stateof-the-art planner FF (Hoffmann & Nebel, 2001), while also highlighting limitations of our
approach. Next, we give results for several stochastic planning domains including those
in the domain-specific track of the 2004 International Probabilistic Planning Competition
(IPPC). All of the domain definitions and problem generators used in our experiments are
available upon request.
In all of our experiments, we use the policy learner described in Section 5.3 to learn
taxonomic decision list policies. In all cases, the number of training trajectories is 100, and
policies are restricted to rules with a depth bound d and length bound l. The discount
97

Fern, Yoon, & Givan

factor γ was always one, and LRW-API was always initialized with a policy that selects
random actions. We utilize a maximum-walk-length parameter N = 10, 000 and set τ and
δ equal to 0.9 and 0.1 respectively.
7.1 Deterministic Planning Experiments
We perform experiments in seven familiar STRIPS planning domains including those used
in the AIPS-2000 planning competition, those used to evaluate TL-Plan in Bacchus and
Kabanza (2000), and the Gripper domain. Each domain has a standard problem generator
that accepts parameters, which control the size and difficulty of the randomly generated
problems. Below we list each domain and the parameters associated with them. A detailed
description of these domains can be found in Hoffmann and Nebel (2001).
• Blocks World (n) : the standard blocks worlds with n blocks.
• Freecell (s, c, f, l) : a version of Solitaire with s suits, c cards per suit, f freecells, and
l columns.
• Logistics (a,c,l,p) : the logistics transportation domain with a airplanes, c cities, l
locations, and p packages.
• Schedule (p) : a job shop scheduling domain with p parts.
• Elevator (f, p) : elevator scheduling with f floors and p people.
• Gripper (b) : a robotic gripper domain with b balls.
• Briefcase (i) : a transportation domain with i items.
LRW Experiments. Our first set of experiments evaluates the ability of LRW-API
to find good policies for RW ∗ . Here we utilize a sampling width of one for rollout, since
these are deterministic domains. Recall that in each iteration of LRW-API we compute an
(approximately) improved policy and may also increase the walk length n to find a harder
problem distribution. We continued iterating LRW-API until we observed no further
improvement. The training time per iteration is approximately five hours.14 Though the
initial training period is significant, once a policy is learned it can be used to solve new
problems very quickly, terminating in seconds with a solution when one is found, even for
very large problems.

Figure 4 provides data for each iteration of LRW-API in each of the seven domains
with the indicated parameter settings. The first column, for each domain, indicates the
iteration number (e.g., the Blocks World was run for 8 iterations). The second column
records the walk length n used for learning in the corresponding iteration. The third and
fourth columns record the SR and AL of the policy learned at the corresponding iteration
14. This timing information is for a relatively unoptimized Scheme implementation. A reimplementation in
C would likely result in a 5-10 fold speed-up.

98

n

RW n
SR
AL

iter. #

iter. #

API with a Policy Language Bias

RW ∗
SR
AL

n

Blocks World (20)
1
2
3
4
5
6
7
8

4
14
54
54
54
54
334
334

0.92
0.94
0.56
0.78
0.88
0.98
0.84
0.99
FF

2.0
5.6
15.0
15.0
33.7
25.1
45.6
37.8

0
0.10
0.17
0.32
0.65
0.90
0.87
1
0.96

5
8
30
30
30
30
30
30
30

0.97
0.97
0.65
0.72
0.90
0.81
0.78
0.90
0.93
FF

1.4
2.7
7.0
7.1
6.7
6.7
6.8
6.9
7.7

0.08
0.26
0.78
0.85
0.85
0.89
0.87
0.89
0.93
1

RW ∗
SR
AL

Logistics (1,2,2,6)
0
41.4
42.8
40.2
47.0
43.9
50.1
43.3
49.0

1
2
3
4
5
6
7
8
9
10
···
43
44
45

Freecell (4,2,2,4)
1
2
3
4
5
6
7
8
9

RW n
SR
AL

3.6
6.3
7.0
7.0
6.3
6.6
6.8
6.6
7.9
5.4

5
45
45
45
45
45
45
45
45
45
···
45
45
45

0.86
0.86
0.81
0.86
0.76
0.76
0.86
0.76
0.70
0.81
···
0.74
0.90
0.92
FF

3.1
6.5
6.9
6.8
6.1
5.9
6.2
6.9
6.1
6.1
···
6.4
6.9
6.6

0.25
0.28
0.31
0.28
0.28
0.32
0.39
0.31
0.19
0.25
···
0.25
0.39
0.38
1

11.3
7.2
8.4
8.9
7.8
8.4
9.1
11.0
7.8
7.6
···
9.0
9.3
9.4
13

0.48
1
1

27
34
36

0
0.2
1
1

0
38
30
28

Schedule (20)
1
2

1
4

0.79
1
FF

1
3.45

Briefcase (10)
Elevator (20,10)
1

20

1

4.0

FF

1
1

26
23

1
1

13
13

1
2
3

5
15
15

0.91
0.89
1
FF

1.4
4.2
3.0

Gripper (10)
1

10

1
FF

3.8

Figure 4: Results for each iteration of LRW-API in seven deterministic planning domains.
For each iteration, we show the walk length n used for learning, along with the success ratio
(SR) and average length (AL) of the learned policy on both RW n and RW ∗ . The final
policy shown in each domain performs above τ = 0.9 SR on walks of length N = 10, 000
(with the exception of Logistics), and further iteration does not improve the performance.
For each benchmark we also show the SR and AL of the planner FF on problems drawn
from RW ∗ .

as measured on 100 problems drawn from RW n for the corresponding value of n (i.e.,
the distribution used for learning). When this SR exceeds τ , the next iteration seeks an
increased walk length n. The fifth and sixth columns record the SR and AL of the same
99

Fern, Yoon, & Givan

policy, but measured on 100 problems drawn from the LRW target distribution RW ∗ , which
in these experiments is approximated by RW N for N = 10, 000.
So, for example, we see that in the Blocks World there are a total of 8 iterations, where
we learn at first for one iteration with n = 4, one more iteration with n = 14, four iterations
with n = 54, and then two iterations with n = 334. At this point we see that the resulting
policy performs well on RW ∗ . Further iterations with n = N , not shown, showed no
improvement over the policy found after iteration eight. In other domains, we also observed
no improvement after iterating with n = N , and thus do not show those iterations. We
note that all domains except Logistics (see below) achieve policies with good performance
on RW N by learning on much shorter RW n distributions, indicating that we have indeed
selected a large enough value of N to capture RW ∗ , as desired.
General Observations. For several domains, our learner bootstraps very quickly
from short random-walk problems, finding a policy that works well even for much longer
random-walk problems. These include Schedule, Briefcase, Gripper, and Elevator. Typically, large problems in these domains have many somewhat independent subproblems with
short solutions, so that short random walks can generate instances of all the different typical
subproblems. In each of these domains, our best LRW policy is found in a small number
of iterations and performs comparably to FF on RW ∗ . We note that FF is considered a
very good domain-independent planner for these domains, so we consider this a successful
result.
For two domains, Logistics15 and Freecell, our planner is unable to find a policy with
success ratio one on RW ∗ . We believe that this is a result of the limited knowledge representation we allowed for policies for the following reasons. First, we ourselves cannot write good
policies for these domains within our current policy language. For example, in logistics, one
of the important concept is “the set containing all packages on trucks such that the truck is
in the packages goal city”. However, the domain is defined in such a way that this concept
cannot be expressed within the language used in our experiments. Second, the final learned
decision lists for Logistics and Freecell, which are in Appendix B, contain a much larger
number of more specific rules than the lists learned in the other domains. This indicates
that the learner has difficulty finding general rules, within the language restrictions, that
are applicable to large portions of training data, resulting in poor generalization. Third,
the success ratio (not shown) for the sampling-based rollout policy, i.e., the improved policy
simulated by Improved-Trajectories, is substantially higher than that for the resulting
learned policy that becomes the policy of the next iteration. This indicates that LearnDecision-List is learning a much weaker policy than the sampling-based policy generating
its training data, indicating a weakness in either the policy language or the learning algorithm. For example, in the logistics domain, at iteration eight, the training data for learning
the iteration-nine policy is generated by a sampling rollout policy that achieves success ratio
0.97 on 100 training problems drawn from the same RW 45 distribution, but the learned
iteration-nine policy only achieves success ratio 0.70, as shown in the figure at iteration
nine. Extending our policy language to incorporate the expressiveness that appears to be
required in these domains will require a more sophisticated learning algorithm, which is a
point of future work.
15. In Logistics, the planner generates a long sequence of policies with similar, oscillating success ratio that
are elided from the table with an ellipsis for space reasons.

100

API with a Policy Language Bias

π∗
Domain
Blocks

FF
SR AL
0.81
60
0.28 158

Size
(20)
(50)

SR
1
1

AL
54
151

Freecell

(4,2,2,4)
(4,13,4,8)

0.36
0

15
—

1
0.47

10
112

Logistics

(1,2,2,6)
(3,10,2,30)

0.87
0

6
—

1
1

6
158

Elevator

(60,30)

1

112

1

98

Schedule

(50)

1

175

1

212

Briefcase

(10)
(50)

1
1

30
162

1
0

29
—

Gripper

(50)

1

149

1

149

Figure 5: Results on standard problem distributions for seven benchmarks. Success ratio
(SR) and average length (AL) are provided for both FF and our policy learned for the LRW
problem distribution. For a given domain, the same learned LRW policy is used for each
problem size shown.

In the remaining domain, the Blocks World, the bootstrapping provided by increasingly
long random walks appears particularly useful. The policies learned at each of the walk
lengths 4, 14, 54, and 334 are increasingly effective on the target LRW distribution RW ∗ .
For walks of length 54 and 334, it takes multiple iterations to master the provided level of
difficulty beyond the previous walk length. Finally, upon mastering walk length 334, the
resulting policy appears to perform well for any walk length. The learned policy is modestly
superior to FF on RW ∗ in success ratio and average length.
Evaluation on the Original Problem Distributions. In each domain we denote
by π∗ the best learned LRW policy—i.e., the policy, from each domain, with the highest
performance on RW ∗ , as shown in Figure 4. The taxonomic decision lists corresponding
to π∗ for each domain is given in Appendix B. Figure 5 shows the performance of π∗ , in
comparison to FF, on the original intended problem distributions for each of our domains.
We measured the success ratio of both systems by giving a time limit of 100 seconds to solve
a problem. Here we have attempted to select the largest problem sizes previously used in
evaluation of domain-specific planners, either in AIPS-2000 or in Bacchus and Kabanza
(2000), as well as show a smaller problem size for those cases where one of the planners
we show performed poorly on the large size. In each case, we use the problem generators
provided with the domains, and evaluate on 100 problems of each size.
Overall, these results indicate that our learned, reactive policies are competitive with
the domain-independent planner FF. It is important to remember that these policies are
learned in a domain-independent fashion, and thus LRW-API can be viewed as a general
approach to generating domain-specific reactive planners. On two domains, Blocks World
101

Fern, Yoon, & Givan

and Briefcase, our learned policies substantially outperform FF on success ratio, especially
on large domain sizes. On three domains, Elevator, Schedule, and Gripper, the two approaches perform quite similarly on success ratio, with our approach superior in average
length on Schedule but FF superior in average length on Elevator.
On two domains, Logistics and Freecell, FF substantially outperforms our learned policies on success ratio. We believe that this is partly due to an inadequate policy language,
as discussed above. We also believe, however, that another reason for the poor performance
is that the long-random-walk distribution RW ∗ does not correspond well to the standard
problem distributions. This seems to be particularly true for Freecell. The policy learned
for Freecell (4,2,2,4) achieved a success ratio of 93 percent on RW ∗ , however, for the standard distribution it only achieved 36 percent. This suggests that RW ∗ generates problems
that are significantly easier than the standard distribution. This is supported by the fact
that the solutions produced by FF on the standard distribution are on average twice as long
as those produced on RW ∗ . One likely reason for this is that it is easy for random walks to
end up in dead states in Freecell, where no actions are applicable. Thus the random walk
distribution will typically produce many problems where the goals correspond to such dead
states. The standard distribution on the other hand will not treat such dead states as goals.
7.2 Probabilistic Planning Experiments
Here we present experiments in three probabilistic domains that are described in the probabilistic planning domain language PPDDL (Younes, 2003).
• Ground Logistics (c, p) : a probabilistic version of logistics with no airplanes, with c
cities and p packages. The driving action has a probability of failure in this domain.
• Colored Blocks World (n) : a probabilistic blocks world with n colored blocks, where
goals involve constructing towers with certain color patterns. There is a probability
that moved blocks fall to the floor.
• Boxworld (c, p) : a probabilistic version of full logistics with c cities and p packages.
Transportation actions have a probability of going in the wrong direction.
The Ground Logistics domain is originally from Boutilier et al. (2001), and was also used
for evaluation in Yoon et al. (2002). The Colored Blocks World and Boxworld domains are
the domains used in the hand-tailored track of IPPC in which our LRW-API technique was
entered. In the hand-tailored track, participants were provided with problem generators for
each domain before the competition and were allowed to incorporate domain knowledge into
the planner for use at competition time. We provided the problem generators to LRW-API
and learned policies for these domains, which were then entered into the competition.
We have also conducted experiments in the other probabilistic domains from Yoon et al.
(2002), including variants of the blocks world and a variant of Ground Logistics, some of
which appeared in Fern et al. (2003). However, we do not show those results here since they
are qualitatively identical to the deterministic blocks world results described above and the
Ground Logistics results we show below.
For our three probabilistic domains, we conducted LRW experiments using the same
procedure as above. All parameters given to LRW-API were the same as above except
102

n

SR

RW n
AL

iter. #

iter. #

API with a Policy Language Bias

RW ∗
SR
AL

Boxworld (10,5)
1
10 0.73
4.3
2
10 0.93
2.3
3
20 0.91
4.4
4
40 0.96
6.1
5 170 0.62
30.8
37.9
6 170 0.49
7 170 0.63
29.3
29.1
8 170 0.63
9 170 0.48
36.4
Standard Distribution (15,15)

n

SR

RW n
AL

SR

RW ∗
AL

Ground Logistics (3,4,4,3)
0.03
0.13
0.17
0.31
0.25
0.17
0.21
0.18
0.17
0

61.5
58.4
55.9
50.4
52.2
55.7
55
55.3
55.3
–

1
5 0.95
2
10 0.97
3 160
1
Standard Distribution

2.71
2.06
6.41
(5,7,7,20)

0.17
0.84
1
1

168.9
17.5
7.2
20

Colored Blocks World (10)
1
2
3
4
5

2 0.86
1.7
5 0.89
8.4
40 0.92
11.7
100 0.76
37.5
100 0.94
20.0
Standard Distribution (50)

0.19
0.81
0.85
0.77
0.95
0.95

93.6
40.8
32.7
38.5
21.9
123

Figure 6: Results for each iteration of LRW-API in three probabilistic planning domains.
For each iteration, we show the walk length n used for learning, along with the success ratio
(SR) and average length (AL) of the learned policy on both RW n and RW ∗ . For each
benchmark, we show performance on the standard problem distribution of the policy whose
performance is best on RW ∗ .
that the sampling width used for rollout was set to w = 10, and τ was set to 0.85 in order to
account for the stochasticity in these domains. The results of these experiments are shown
in Figure 6. These tables have the same form as Figure 4 only the last row given for each
domain now gives the performance of π∗ on the standard distribution, i.e., problems drawn
from the domains problem generator. For Colored Blocks World the problem generator
produces problems whose goals are specified using existential quantifiers. For example, a
simple goal may be “there exists blocks x and y such that x is red, y is blue and x is on y”.
Since our policy language cannot directly handle existentially quantified goals we preprocess
the planning problems produced by the problem generator to remove them. This was done
by assigning particular block names to the existential variables, ensuring that the static
properties of a block (in this case color) satisfied the static properties of the variable is
was assigned to. In this domain, finding such an assignment was trivial, and the resulting
assignment was taken to be the goal, giving a planning problem to which our learned policy
was applied. Since the blocks world states are fully connected, the resulting goal is always
guaranteed to be achievable.

For Boxworld, LRW-API is not able to find a good policy for RW ∗ or the standard
distribution. Again, as for deterministic Logistics and Freecell, we believe that this is
primarily because of the restricted policy languages that is currently used by our learner.
Here, as for those domains, we see that the decision list learned for Boxworld contains many
very specific rules, indicating that the learner was not able to generalize well beyond the
103

Fern, Yoon, & Givan

training trajectories. For Ground Logistics, we see that LRW-API quickly finds a good
policy for both RW ∗ and the standard distribution.
For Colored Blocks World, we also see that LRW-API is able to quickly find a good
policy for both RW ∗ and the standard distribution. However, unlike the deterministic
(uncolored) blocks world, here the success ratio is observed to be less than one, solving 95
percent of the problems. It is unclear, why LRW-API is not able to find a “perfect” policy.
It is relatively easy to hand-code a policy for Colored Blocks World using the language of the
learner, hence inadequate knowledge representation is not the answer. The predicates and
action types for this domain are not the same as those in its deterministic counterpart and
other stochastic variants that we have previously considered. This difference apparently
interacts badly with our learners search bias, causing it to fail to find a perfect policy.
Nevertheless, these two results, along with the probabilistic planning results not shown
here, indicate that when a good policy is expressible in our language, LRW-API can
find good policies in complex relational MDPs. This makes LRW-API one of the few
techniques that can simultaneously cope with the complexity resulting from stochasticity
and from relational structure in domains such as these.

8. Related Work
Boutilier et al. (2001) presented the first exact solution technique for relational MDPs
based on structured dynamic programming. However, a practical implementation of the
approach was not provided, primarily due to the need for the simplification of first-order
logic formulas. These ideas, however, served as the basis for a logic-programming-based
system (Kersting, Van Otterlo, & DeRaedt, 2004) that was successfully applied to blocksworld problems involving simple goals and a simplified logistics world. This style of approach
is inherently limited to domains where the exact value functions and/or policies can be
compactly represented in the chosen knowledge representation. Unfortunately, this is not
generally the case for the types of domains that we consider here, particularly as the planning
horizon grows. Nevertheless, providing techniques such as these that directly reason about
the MDP model is an important direction. Note that our API approach essentially ignores
the underlying MDP model, and simply interacts with the MDP simulator as a black box.
An interesting research direction is to consider principled approximations of these techniques that can discover good policies in more difficult domains. This has been considered
by Guestrin et al. (2003a), where a class-based MDP and value function representation was
used to compute an approximate value function that could generalize across different sets
of objects. Promising empirical results were shown in a multi-agent tactical battle domain.
Presently the class-based representation does not support some of the representation features that are commonly found in classical planning domains (e.g., relational facts such as
on(a, b) that change over time), and thus is not directly applicable in these contexts. However, extending this work to richer representations is an interesting direction. Its ability to
“reason globally” about a domain may give it some advantages compared to API.
Our approach is closely related to work in relational reinforcement learning (RRL) (Dzeroski et al., 2001), a form of online API that learns relational value-function approximations. Q-value functions are learned in the form of relational decision trees (Q-trees) and
are used to learn corresponding policies (P -trees). The RRL results clearly demonstrate the
104

API with a Policy Language Bias

difficulty of learning value-function approximations in relational domains. Compared to P trees, Q-trees tend to generalize poorly and be much larger. RRL has not yet demonstrated
scalability to problems as complex as those considered here—previous RRL blocks-world
experiments include relatively simple goals16 , which lead to value functions that are much
less complex than the ones here. For this reason, we suspect that RRL would have difficulty
in the domains we consider, precisely because of the value-function approximation step that
we avoid; however, this needs to be experimentally tested.
We note, however, that our API approach has the advantage of using an unconstrained
simulator, whereas RRL learns from irreversible world experience (pure RL). By using
a simulator, we are able to estimate the Q-values for all actions at each training state,
providing us with rich training data. Without such a simulator, RRL is not able to directly
estimate the Q-value for each action in each training state—thus, RRL learns a Q-tree to
provide estimates of the Q-value information needed to learn the P -tree. In this way, valuefunction learning serves a more critical role when a simulator is unavailable. We believe,
that in many relational planning problems, it is possible to learn a model or simulator
from world experience—in this case, our API approach can be incorporated as the planning
component of RRL. Otherwise, finding ways to either avoid learning or to more effectively
learn relational value-functions in RRL is an interesting research direction.
Researchers in classical planning have long studied techniques for learning to improve
planning performance. For a collection and survey of work on “learning for planning domains” see Minton (1993) and Zimmerman and Kambhampati (2003). Two primary approaches are to learn domain-specific control rules for guiding search-based planners e.g.,
Minton, Carbonell, Knoblock, Kuokka, Etzioni, and Gil (1989), Veloso, Carbonell, Perez,
Borrajo, Fink, and Blythe (1995), Estlin and Mooney (1996), Huang, Selman, and Kautz
(2000), Ambite, Knoblock, and Minton (2000), Aler, Borrajo, and Isasi (2002), and, more
closely related, to learn domain-specific reactive control policies (Khardon, 1999a; Martin
& Geffner, 2000; Yoon et al., 2002).
Regarding the latter, our work is novel in using API to iteratively improve stand-alone
control policies. Regarding the former, in theory, search-based planners can be iteratively
improved by continually adding newly learned control knowledge—however, it can be difficult to avoid the utility problem (Minton, 1988), i.e., being swamped by low utility rules.
Critically, our policy-language bias confronts this issue by preferring simpler policies. Our
learning approach is also not tied to having a base planner (let alone tied to a single particular base planner), unlike most previous work. Rather, we only require a domain simulator.
The ultimate goal of such systems is to allow for planning in large, difficult problems
that are beyond the reach of domain-independent planning technology. Clearly, learning
to achieve this goal requires some form of bootstrapping and almost all previous systems
have relied on the human for this purpose. By far, the most common human-bootstrapping
approach is “learning from small problems”. Here, the human provides a small problem
distribution to the learner, by limiting the number of objects (e.g., using 2-5 blocks in the
blocks world), and control knowledge is learned for the small problems. For this approach to
work, the human must ensure that the small distribution is such that good control knowledge
for the small problems is also good for the large target distribution. In contrast, our long16. The most complex blocks-world goal for RRL was to achieve on(A, B) in an n block environment. We
consider blocks-world goals that involve all n blocks.

105

Fern, Yoon, & Givan

random-walk bootstrapping approach can be applied without human assistance directly to
large planning domains. However, as already pointed out, our goal of performing well on
the LRW distribution may not always correspond well with a particular target problem
distribution.
Our bootstrapping approach is similar in spirit to the bootstrapping framework of “learning from exercises”(Natarajan, 1989; Reddy & Tadepalli, 1997). Here, the learner is provided with planning problems, or exercises, in order of increasing difficulty. After learning
on easier problems, the learner is able to use its new knowledge, or skills, in order to bootstrap learning on the harder problems. This work, however, has previously relied on a
human to provide the exercises, which typically requires insight into the planning domain
and the underlying form of control knowledge and planner. Our work can be viewed as an
automatic instantiation of “learning from exercises”, specifically designed for learning LRW
policies.
Our random-walk bootstrapping is most similar to the approach used in Micro-Hillary
(Finkelstein & Markovitch, 1998), a macro-learning system for problem solving. In that
work, instead of generating problems via random walks starting at an initial state, random
walks were generated backward from goal states. This approach assumes that actions are
invertible or that we are given a set of “backward actions”. When such assumptions hold,
the backward random-walk approach may be preferable when we are provided with a goal
distribution that does not match well with the goals generated by forward random walks.
Of course, in other cases forward random walks may be preferable. Micro-Hillary was
empirically tested in the N × N sliding-puzzle domain; however, as discussed in that work,
there remain challenges for applying the system to more complex domains with parameterized actions and recursive structure, such as familiar STRIPS domains. To the best of our
knowledge, the idea of learning from random walks has not been previously explored in the
context of STRIPS planning domains.
The idea of searching for a good policy directly in policy space rather than value-function
space is a primary motivation for policy-gradient RL algorithms. However, these algorithms
have been largely explored in the context of parametric policy spaces. While this approach
has demonstrated impressive success in a number of domains, it appears difficult to define
such policy spaces for the types of planning problem considered here.
Our API approach can be viewed as a type of reduction from planning or reinforcement
learning to classification learning. That is, we solve an MDP by generating and solving
a series of cost-sensitive classification problems. Recently, there have been several other
proposals for reducing reinforcement learning to classification. Dietterich and Wang (2001)
proposed a reinforcement learning approach based on batch value function approximation.
One of the proposed approximations enforced only that the learned approximation assign
the best action the highest value, which is a type of classifier learning. Lagoudakis and Parr
(2003) proposed a classification-based API approach that is closely related to ours. The primary difference is the form of the classification problem produced on each iteration. They
generate standard multi-class classification problems, whereas we generate cost-sensitive
problems. Bagnell, Kakade, Ng, and Schneider (2003) introduced a closely related algorithm for learning non-stationary policies in reinforcement learning. For a specified horizon
time h, their approach learns a sequence of h policies. At each iteration, all policies are
held fixed except for one, which is optimized by forming a classification problem via policy
106

API with a Policy Language Bias

rollout17 . Finally, Langford and Zadrozny (2004) provide a formal reduction from reinforcement learning to classification, showing that -accurate classification learning implies
near-optimal reinforcement learning. This approach uses an optimistic variant of sparse
sampling to generate h classification problems, one for each horizon time step.

9. Summary and Future Work
We introduced a new variant of API that learns policies directly, without representing
approximate value functions. This allowed us to utilize a relational policy language for
learning compact policy representations. We also introduced a new API bootstrapping
technique for goal-based planning domains. Our experiments show that the LRW-API
algorithm, which combines these techniques, is able to find good policies for a variety of
relational MDPs corresponding to classical planning domains and their stochastic variants.
We know of no previous MDP technique that has been successfully applied to problems
such as these.
Our experiments also pointed to a number of weaknesses of our current approach. First,
our bootstrapping technique, based on long random walks, does not always correspond
well to the problem distribution of interest. Investigating other automatic bootstrapping
techniques is an interesting direction, related to the general problems of exploration and
reward shaping in reinforcement learning. Second, we have seen that limitations of our
current policy language and learner are partly responsible for some of the failures of our
system. In such cases, we must either: 1) depend on the human to provide useful features
to the system, or 2) extend the policy language and develop more advanced learning techniques. Policy-language extensions that we are considering include various extensions to the
knowledge representation used to represent sets of objects in the domain (in particular, for
route-finding in maps/grids), as well as non-reactive policies that incorporate search into
decision-making.
As we consider ever more complex planning domains, it is inevitable that our brute-force
enumeration approach to learning policies from trajectories will not scale. Presently our
policy learner, as well as the entire API technique, makes no attempt to use the definition
of a domain when one is available. We believe that developing a learner that can exploit
this information to bias its search for good policies is an important direction of future work.
Recently, Gretton and Thiebaux (2004) have taken a step in this direction by using logical
regression (based on a domain model) to generate candidate rules for the learner. Developing tractable variations of this approach is a promising research direction. In addition,
exploring other ways of incorporating a domain model into our approach and other modelblind approaches is critical. Ultimately, scalable AI planning systems will need to combine
experience with stronger forms of explicit reasoning.

17. Here the initial state distribution is dictated by the policies at previous time steps, which are held fixed.
Likewise the actions selected along the rollout trajectories are dictated by policies at future time steps,
which are also held fixed.

107

Fern, Yoon, & Givan

Acknowledgments
We would like to thank Lin Zhu for originally suggesting the idea of using random walks
for bootstrapping. We would also like to thank the reviewers and editors for helping to
vastly improve this paper. This work was supported in part by NSF grants 9977981-IIS
and 0093100-IIS.

Appendix A. Omitted Proofs
Proposition 1. Let H be a finite class of deterministic policies. For any π ∈ H, and any
π
set of n = −1 ln |H|
δ trajectories drawn independently from Dh , there is a 1 − δ probability
that every π̂ ∈ H consistent with the trajectories satisfies V (π̂) ≥ V (π) − 2Vmax ( + γ h ).
Proof: We first introduce some basic properties and notation that will be used below. For
any deterministic policy π, if π is consistent with a trajectory t, then Dhπ (t) is entirely
determined by the underlying MDP transition dynamics. This implies that if two deterministic policies π and π 0 are both consistent with a trajectory t then Dhπ (t) = Dhπ̂ (t). We
will denote by v(t) the cumulative discounted reward accumulated by executing trajectory
P
t. For any policy π, we have that V h (π) = t Dhπ (t) · v(t) where the summation is taken
over all length h trajectories (or simply those that are consistent with π). Finally for a set
P
of trajectories Γ we will let Dhπ (Γ) = t∈Γ0 Dhπ (t) giving the cumulative probability of π
generating the trajectories in Γ.
Consider a particular π ∈ H and any π̂ ∈ H that is consistent with the n trajectories of
π. We will let Γ denote the set of all length h trajectories that are consistent with π and
Γ̂ denote the set of trajectories that are consistent with π̂. Following Khardon (1999b) we
first give a standard argument showing that with high probability Dhπ (Γ̂) > 1 − . To see
this consider the probability that π̂ is consistent with all n = −1 ln |H|
δ trajectories of π
δ
π
given that Dh (Γ̂) ≤ 1 − . The probability that this occurs is at most (1 − )n < e−n = |H|
.
δ
Thus the probability of choosing such a π̂ is at most |H| |H|
= δ. Thus, with probability at

least 1 − δ we know that Dhπ (Γ̂) > 1 − . Note that Dhπ (Γ̂) = Dhπ̂ (Γ).
Now given the condition that Dhπ (Γ̂) > 1 −  we show that V h (π̂) ≥ V h (π) − 2Vmax by
considering the difference of the two value functions.
V h (π) − V h (π̂) =

X

Dhπ (t) · v(t) −

t∈Γ

=

X

=

· v(t) +

X

(Dhπ (t) − Dhπ̂ (t)) · v(t) −

t∈Γ∩Γ̂

Dhπ (t)

t∈Γ−Γ̂

≤

Dhπ̂ (t) · v(t)

t∈Γ̂

Dhπ (t)

t∈Γ−Γ̂

X

X

· v(t) + 0 −

t∈Γ̂−Γ

Dhπ̂ (t)

t∈Γ̂−Γ
π̂
− Γ̂) + Dh (Γ̂ − Γ)]
π
Dh (Γ̂) + 1 − Dhπ̂ (Γ)]

Vmax [Dhπ (Γ

= Vmax [1 −

X

≤ 2Vmax
108

X

· v(t)

Dhπ̂ (t) · v(t)

API with a Policy Language Bias

The third lines follows since Dhπ (t) = Dhπ̂ (t) when π and π̂ are both consistent with t. The
last line follows by substituting our assumption of Dhπ (Γ̂) = Dhπ̂ (Γ) > 1− into the previous
line. Combining this result with the approximation due to using a finite horizon,
V (π) − V (π̂) ≤ V h (π) − V h (π̂) + 2γ h Vmax
we get that with probability at least 1 − δ, V (π) − V (π̂) ≤ 2Vmax ( + γ h ), which completes
the proof. 2
Proposition 2.
have

For any MDP with Q-advantage at least ∆∗ , and any 0 < δ 0 < 1, if we

h > logγ

∆∗
8Vmax

8Vmax
∆∗
∗
∆
2



w >
∆ =

2

ln

|A|
δ0

then for any state s, Â(∆, s) = Aπ (s) with probability at least 1 − δ 0 .
Proof: Given a real valued random variable X bounded in absolute value by Xmax and
an average X̂ of w independently drawn samples of X, the
q additive Chernoff bound states
that with probability at least 1 − δ, |E[X] − X̂| ≤ Xmax − wln δ .
Note that Qπh (s, a) is the expectation of the random variable X(s, a) = R(s, a) +
π
γVh−1 (T (s, a)) and Q̂(s, a) is simply an average of w independent samples of X(s, a).
δ0
The Chernoff bound tells us that with probability at least 1 − |A|
, |Qπh (s, a) − Q̂(s, a)| ≤
q

0

δ
Vmax ln |A|−ln
, where |A| is the number of actions. Substituting in our choice of w we
w
∗
get that with probability at least 1 − δ 0 , |Qπh (s, a) − Q̂(s, a)| < ∆8 is satisfied by all actions
simultaneously. We also know that |Qπ (s, a) − Qπh (s, a)| ≤ γ h Vmax , which by our choice
∗
of h gives, |Qπ (s, a) − Qπh (s, a)| < ∆8 . Combining these relationships we get that with
∗
probability at least 1 − δ 0 , |Qπ (s, a) − Q̂(s, a)| < ∆4 holds for all actions simultaneously.
We can use this bound to show that with high probability the Q-value estimates for
∗
actions in Aπ (s) will be within a ∆2 range of each other, and other actions will be outside
of that range. In particular, consider any action a ∈ Aπ (s) and some other action a0 . If
a0 ∈ Aπ (s) then we have that Qπ (s, a) = Qπ (s, a0 ). From the above bound we get that
∗
|Q̂(s, a) − Q̂(s, a0 )| < ∆2 . Otherwise a0 6∈ Aπ (s) and by our assumption about the MDP
Q-advantage we get that Qπ (s, a) − Qπ (s, a0 ) ≥ ∆∗ . Using the above bound this implies
∗
that Q̂(s, a) − Q̂(s, a0 ) > ∆2 . These relationships and the definition of Â(∆, s) imply that
with probability at least 1 − δ 0 we have that Â(∆, s) = Aπ (s). 2

Appendix B. Learned Policies
Below we give the final taxonomic-decision-list policies that were learned for each domain
in our experiments. Rather than write rules in the form a(x1 , . . . , xk ) : L1 ∧ L2 ∧ · · · ∧ Lm
109

Fern, Yoon, & Givan

we drop the variables from the head and simply write, a : L1 ∧ L2 ∧ · · · ∧ Lm . In addition
below we use the notation R−∗ as short-hand for (R−1 )∗ where R is a relation. When interpreting the policies, it is important to remember that for each rule of action type a, the
preconditions for action type a are implicitly included in the constraints. Thus, the rules
will often allow actions that are not legal, but those actions will never be considered by the
system.
Gripper
1. MOVE: (X1 ∈ (NOT (GAT (CARRY−1 GRIPPER)))) ∧ (X2 ∈ (NOT (GAT (AT−1 AT-ROBBY)))) ∧ (X2 ∈ (GAT (NOT
(CAT−1 ROOM)))) ∧ (X1 ∈ (CAT BALL))
2. DROP: (X1 ∈ (GAT−1 AT-ROBBY))
3. PICK: (X1 ∈ (GAT−1 (GAT (CARRY−1 GRIPPER)))) ∧ (X1 ∈ (GAT−1 (NOT AT-ROBBY)))
4. PICK: (X2 ∈ (AT (NOT (GAT−1 ROOM)))) ∧ (X1 ∈ (GAT−1 (NOT AT-ROBBY)))
5. PICK: (X1 ∈ (GAT−1 (NOT AT-ROBBY)))
Briefcase
1. PUT-IN: (X1 ∈ (GAT−1 (NOT IS-AT)))
2. MOVE: (X2 ∈ (AT (NOT (CAT−1 LOCATION)))) ∧ (X2 ∈ (NOT (AT (GAT−1 CIS-AT))))
3. MOVE: (X2 ∈ (GAT IN)) ∧ (X1 ∈ (NOT (CAT IN)))
4. TAKE-OUT: (X1 ∈ (CAT−1 IS-AT))
5. MOVE: (X2 ∈ GIS-AT)
6. MOVE: (X2 ∈ (AT (GAT−1 CIS-AT)))
7. PUT-IN: (X1 ∈ UNIVERSAL)
Schedule
1. DO-IMMERSION-PAINT: (X1 ∈ (NOT (PAINTED−1 X2 ))) ∧ (X1 ∈ (GPAINTED−1 X2 ))
2. DO-DRILL-PRESS: (X1 ∈ (GHAS-HOLEO−1 X3 )) ∧ (X1 ∈ (GHAS-HOLEW−1 X2 ))
3. DO-LATHE: (X1 ∈ (NOT (SHAPE−1 CYLINDRICAL))) ∧ (X1 ∈ (GSHAPE−1 CYLINDRICAL))
4. DO-DRILL-PRESS: (X1 ∈ (GHAS-HOLEW−1 X2 ))
5. DO-DRILL-PRESS: (X1 ∈ (GHAS-HOLEO−1 X3 ))
6. DO-GRIND: (X1 ∈ (NOT (SURFACE-CONDITION−1 SMOOTH))) ∧ (X1 ∈ (GSURFACE-CONDITION−1 SMOOTH))
7. DO-POLISH: (X1 ∈ (NOT (SURFACE-CONDITION−1 POLISHED))) ∧ (X1 ∈ (GSURFACE-CONDITION−1 POLISHED))
8. DO-TIME-STEP:
Elevator
1. DEPART: (X2 ∈ GSERVED)
2. DOWN: (X2 ∈ (DESTIN BOARDED)) ∧ (X2 ∈ (DESTIN GSERVED))
3. UP: (X2 ∈ (DESTIN BOARDED)) ∧ (X2 ∈ (DESTIN GSERVED)) ∧ (X2 ∈ (ABOVE (ORIGIN BOARDED))) ∧ (X1 (NOT
(DESTIN BOARDED)))
4. BOARD: (X2 ∈ (NOT CSERVED)) ∧ (X2 ∈ GSERVED)
5. UP: (X2 ∈ (ORIGIN GSERVED)) ∧ (X2 ∈ (NOT (DESTIN BOARDED))) ∧ (X2 ∈ (NOT (DESTIN GSERVED))) ∧ (X2 ∈
(ORIGIN (NOT CSERVED))) ∧ (X2 ∈ (ABOVE (DESTIN PASSENGER))) ∧ (X1 ∈ (NOT (DESTIN BOARDED)))
6. DOWN: (X2 ∈ (ORIGIN GSERVED)) ∧ (X2 ∈ (ORIGIN (NOT CSERVED))) ∧ (X1 ∈ (NOT (DESTIN BOARDED)))

110

API with a Policy Language Bias

7. UP: (X2 ∈ (NOT (ORIGIN BOARDED))) ∧ (X2 ∈ (NOT (DESTIN BOARDED)))
FreeCell
1. SENDTOHOME: (X1 (CANSTACK−1 (CANSTACK (SUIT−1 (SUIT INCELL))))) ∧ (X5 ∈ (NOT GHOME))
2. MOVE-B: (X2 ∈ (NOT (CANSTACK (ON GHOME)))) ∧ (X2 ∈ (CANSTACK GHOME)) ∧ (X2 ∈ (VALUE−1 (NOT
COLSPACE))) ∧ (X1 ∈ (CANSTACK−1 (SUIT−1 (SUIT BOTTOMCOL))))
3. MOVE: (X1 ∈ (CANSTACK−1 (ON (CANSTACK−1 (ON−1 GHOME))))) ∧ (X3 ∈ (CANSTACK (ON (SUIT−1 (SUIT BOTTOMCOL))))) ∧ (X1 ∈ (ON−1 BOTTOMCOL)) ∧ (X1 ∈ (CANSTACK−1 (ON GHOME))) ∧ (X3 ∈ (ON−1 (CANSTACK−1
(ON−1 (NOT (CANSTACK (VALUE−1 CELLSPACE))))))) ∧ (X1 ∈ (NOT (CANSTACK−1 (SUIT−1 (SUIT INCELL))))) ∧
(X3 ∈ (CANSTACK BOTTOMCOL)) ∧ (X1 ∈ (SUIT−1 (SUIT (ON−1 (NOT (CANSTACK (VALUE−1 CELLSPACE)))))))
∧ (X1 ∈ (VALUE−1 (NOT COLSPACE))) (0 (ON−1 (NOT (CANSTACK−1 (SUIT−1 (SUIT INCELL)))))) ∧ (X1 ∈ (NOT
(CANSTACK−1 CHOME)))
4. SENDTOHOME-B: (X4 ∈ (NOT GHOME))
5. SENDTOHOME: (X1 ∈ (ON−1 (CANSTACK (CANSTACK−1 (SUIT−1 (SUIT INCELL)))))) ∧ (X5 ∈ (NOT GHOME))
6. SENDTOHOME: (X1 (ON−1 (ON−1 GHOME))) ∧ (X1 ∈ (CANSTACK−1 (NOT GHOME))) ∧ (X1 ∈ (CANSTACK−1 (NOT
(ON−1 GHOME)))) (X5 ∈ (NOT GHOME))
7. MOVE-B: (X1 ∈ (NOT (CANSTACK−1 GHOME))) ∧ (X2 ∈ (VALUE−1 (NOT COLSPACE))) ∧ (X1 ∈ (CANSTACK−1
(SUIT−1 (SUIT BOTTOMCOL))))
8. SENDTOFREE: (X1 ∈ (ON−1 (ON−1 GHOME))) ∧ (X1 ∈ (NOT GHOME))
9. SENDTOHOME: (X5 ∈ (CANSTACK−1 (CANSTACK (ON GHOME)))) ∧ (X5 ∈ (NOT GHOME))
10. SENDTOHOME: (0 GHOME) (X5 ∈ (VALUE−1 (NOT COLSPACE))) ∧ (X5 ∈ (NOT (CANSTACK−1 (ON−1 (NOT
GHOME))))) ∧ (X1 ∈ (ON−1 (NOT (ON−1 GHOME)))) ∧ (X5 ∈ (NOT GHOME))
11. NEWCOLFROMFREECELL: (X1 ∈ GHOME)
12. SENDTOHOME: (X5 ∈ (CANSTACK−1 (ON GHOME))) ∧ (X1 ∈ GHOME) ∧ (X5 ∈ (NOT GHOME))
13. MOVE-B: (X1 ∈ (VALUE−1 (VALUE HOME))) ∧ (X2 ∈ (VALUE−1 (NOT COLSPACE))) ∧ (X1 ∈ (CANSTACK−1 (SUIT−1
(SUIT BOTTOMCOL))))
14. SENDTOHOME: (X1 ∈ (CANSTACK−1 (ON−1 (CANSTACK−1 (SUIT−1 (SUIT INCELL)))))) ∧ (X5 ∈ (NOT GHOME))
15. SENDTOHOME: (X1 ∈ (ON−1 (ON−1 (CANSTACK−1 (ON−1 (NOT GHOME)))))) (X5 ∈ (NOT GHOME))
16. SENDTOFREE: (X1 ∈ (CANSTACK−1 (ON (ON−1 GHOME)))) ∧ (X1 ∈ (SUIT−1 (SUIT BOTTOMCOL))) ∧ (X1 ∈ (ON−1
BOTTOMCOL))
17. MOVE: (X3 ∈ (ON−1 (CANSTACK−1 CLEAR))) ∧ (X1 ∈ (ON−1 (CANSTACK (ON−1 (NOT (CANSTACK (VALUE−1
CELLSPACE))))))) ∧ (X3 ∈ (NOT GHOME)) ∧ (X1 ∈ GHOME) ∧ (X3 ∈ (CANSTACK BOTTOMCOL)) ∧ (X3 ∈ (ON−1
(CANSTACK−1 (ON−1 (NOT (CANSTACK (VALUE−1 CELLSPACE))))))) ∧ (X1 ∈ (NOT (CANSTACK−1 (SUIT−1
(SUIT INCELL))))) ∧ (X1 ∈ (ON−1 BOTTOMCOL)) ∧ (X1 ∈ (SUIT−1 (SUIT (ON−1 (NOT (CANSTACK (VALUE−1
CELLSPACE))))))) ∧ (X1 ∈ (VALUE−1 (NOT COLSPACE))) ∧ (X1 ∈ (ON−1 (NOT (CANSTACK−1 (SUIT−1 (SUIT
INCELL)))))) ∧ (X1 ∈ (NOT (CANSTACK−1 CHOME)))
18. MOVE: (X1 ∈ (SUIT−1 (SUIT CHOME))) ∧ (X3 ∈ (NOT GHOME)) ∧ (X3 ∈ (NOT (ON−1 GHOME))) ∧ (X1 ∈ (ON−1
(CANSTACK−1 BOTTOMCOL)))
19. SENDTOHOME: (X1 ∈ (CANSTACK (ON (CANSTACK (ON GHOME))))) ∧ (X1 ∈ GHOME) (X5 ∈ (NOT GHOME))
20. SENDTOHOME: (X1 ∈ (CANSTACK−1 (ON (CANSTACK−1 (ON−1 GHOME))))) ∧ (X1 ∈ (NOT (SUIT−1 (SUIT BOTTOMCOL)))) ∧ (X5 ∈ (NOT GHOME))
21. SENDTOFREE: (X1 ∈ (CANSTACK (ON (CANSTACK (VALUE−1 CELLSPACE))))) ∧ (X1 ∈ (CANSTACK CHOME))
22. SENDTOHOME: (X1 ∈ (CANSTACK−1 (SUIT−1 (SUIT INCELL)))) ∧ (X1 ∈ (ON−1 (NOT (CANSTACK (VALUE−1
CELLSPACE))))) ∧ (X5 ∈ (NOT GHOME))
23. SENDTONEWCOL: (X1 ∈ (CANSTACK (CANSTACK−1 (ON−1 GHOME))))
24. SENDTOFREE: (X1 ∈ (CANSTACK (ON−1 (CANSTACK−1 (ON−1 GHOME))))) ∧ (X1 ∈ (NOT (CANSTACK GHOME)))
∧ (X1 ∈ (NOT (ON−1 GHOME))) ∧ (X1 ∈ (ON−1 (NOT (CANSTACK−1 (SUIT−1 (SUIT INCELL))))))

111

Fern, Yoon, & Givan

25. SENDTOFREE: (X1 ∈ (ON−1 (CANSTACK (CANSTACK−1 (ON−1 GHOME))))) ∧ (X1 ∈ (NOT (CANSTACK BOTTOMCOL))) ∧ (X1 ∈ (NOT (CANSTACK−1 (CANSTACK (ON GHOME)))))
26. SENDTOFREE: (X1 ∈ (CANSTACK (ON−1 (CANSTACK−1 (ON−1 (NOT GHOME)))))) ∧ (X1 ∈ (NOT (CANSTACK
GHOME))) ∧ (X1 ∈ (CANSTACK (NOT (SUIT−1 (SUIT BOTTOMCOL)))))
27. SENDTOHOME: (X1 ∈ (CANSTACK−1 (CANSTACK (ON−1 GHOME)))) ∧ (X1 ∈ (ON−1 (CANSTACK−1 (ON−1 (NOT
GHOME))))) ∧ (X1 ∈ (NOT GHOME)) ∧ (X5 ∈ (NOT GHOME))
28. SENDTOFREE: (X1 ∈ (CANSTACK (ON−1 (CANSTACK−1 (ON−1 (NOT GHOME)))))) ∧ (X1 ∈ (CANSTACK (CANSTACK−1
(ON−1 GHOME)))) ∧ (X1 ∈ (NOT GHOME)) ∧ (X1 ∈ (ON−1 (CANSTACK−1 (ON−1 (NOT (CANSTACK (VALUE−1
CELLSPACE)))))))
29. SENDTOFREE: (X1 ∈ (CANSTACK CHOME)) ∧ (X1 ∈ (SUIT−1 (SUIT (CANSTACK−1 (ON−1 GHOME)))))
30. SENDTOHOME: (X1 ∈ GHOME) ∧ (X1 ∈ (SUIT−1 (SUIT BOTTOMCOL))) ∧ (X1 ∈ (CANSTACK−1 (NOT (ON−1
GHOME)))) ∧ (X5 ∈ (NOT GHOME))
31. SENDTOFREE: (X1 ∈ (CANSTACK−1 (ON−1 GHOME))) ∧ (X1 ∈ (CANSTACK−1 (ON−1 (NOT GHOME))))
32. SENDTOFREE: (X1 ∈ (CANSTACK (ON−1 GHOME))) ∧ (X1 ∈ (NOT GHOME)) ∧ (X1 ∈ (ON−1 (CANSTACK−1 (ON−1
(NOT GHOME)))))
33. SENDTOHOME: (X1 ∈ (ON−1 (CANSTACK−1 BOTTOMCOL))) ∧ (X1 ∈ (CANSTACK−1 (NOT GHOME))) ∧ (X5 ∈
(NOT GHOME))
34. SENDTOFREE: (X1 ∈ (CANSTACK (ON (CANSTACK−1 (ON−1 (NOT GHOME)))))) ∧ (X1 ∈ (NOT (SUIT−1 (SUIT
BOTTOMCOL)))) ∧ (X1 ∈ (NOT GHOME))
35. SENDTOHOME: (X1 ∈ (NOT (CANSTACK−1 GHOME))) ∧ (X1 ∈ (NOT (SUIT−1 (SUIT BOTTOMCOL)))) ∧ (X5 ∈
(NOT GHOME))
36. SENDTOFREE: (X1 ∈ (NOT (ON−1 GHOME))) ∧ (X1 ∈ (CANSTACK (CANSTACK−1 (ON−1 (NOT GHOME)))))
37. SENDTOFREE-B: (X1 ∈ (NOT GHOME))
38. SENDTOFREE: (X1 ∈ UNIVERSAL)
Logistics
1. FLY-AIRPLANE: (X1 ∈ (IN (GAT−1 AIRPORT))) ∧ (X1 ∈ (NOT (IN (GAT−1 (AT AIRPLANE))))) ∧ (X3 ∈ (NOT (GAT
(IN−1 TRUCK)))) ∧ (X1 ∈ (NOT (IN (GAT−1 (NOT AIRPORT)))))
2. LOAD-TRUCK: (X2 ∈ (IN (NOT (GAT−1 (NOT AIRPORT))))) ∧ (X1 ∈ (GAT−1 (GAT (IN−1 TRUCK)))) ∧ (X1 ∈ (NOT
(CAT−1 LOCATION)))
3. DRIVE-TRUCK: (X3 ∈ (AT (AT−1 (GAT (IN−1 TRUCK))))) ∧ (X3 ∈ (IN-CITY−1 (IN-CITY (AT AIRPLANE)))) ∧ (X1
∈ (AT−1 (NOT (GAT (IN−1 TRUCK)))))
4. UNLOAD-TRUCK: (X1 ∈ (GAT−1 (AT (IN OBJ)))) ∧ (X1 ∈ (GAT−1 (AT OBJ))) ∧ (X1 ∈ (NOT (GAT−1 (AT AIRPLANE)))) ∧ (X2 ∈ (AT−1 (GAT (IN−1 TRUCK)))) ∧ (X1 ∈ (GAT−1 (AT TRUCK)))
5. FLY-AIRPLANE: (X3 ∈ (GAT (IN−1 AIRPLANE))) ∧ (X1 ∈ (IN (NOT (GAT−1 (AT TRUCK))))) ∧ (X1 ∈ (AT−1 (NOT
(GAT (IN−1 TRUCK)))))
6. UNLOAD-AIRPLANE: (X2 ∈ (NOT (IN (GAT−1 (NOT AIRPORT))))) ∧ (X1 ∈ (GAT−1 (AT AIRPLANE)))
7. LOAD-TRUCK: (X2 ∈ (IN (NOT (GAT−1 LOCATION)))) ∧ (X1 ∈ (NOT (GAT−1 (AT TRUCK)))) ∧ (X1 ∈ (GAT−1
LOCATION))
8. UNLOAD-TRUCK: (X1 ∈ (GAT−1 (AT TRUCK))) ∧ (X2 ∈ (AT−1 AIRPORT)) ∧ (X2 ∈ (NOT (IN (GAT−1 (NOT AIRPORT))))) ∧ (X1 ∈ (GAT−1 (AT AIRPLANE)))
9. FLY-AIRPLANE: (X3 ∈ (AT (AT−1 (GAT (IN−1 TRUCK))))) ∧ (X1 ∈ (AT−1 (GAT (GAT−1 LOCATION)))) ∧ (X1 ∈
(NOT (AT−1 (CAT OBJ))))
10. DRIVE-TRUCK: (X1 ∈ (IN (GAT−1 LOCATION))) ∧ (X1 ∈ (AT−1 (NOT (GAT (IN−1 TRUCK))))) ∧ (X1 ∈ (AT−1 (NOT
(AT AIRPLANE))))
11. UNLOAD-TRUCK: (X2 ∈ (AT−1 (GAT (GAT−1 (NOT AIRPORT))))) ∧ (X1 ∈ (NOT (GAT−1 AIRPORT)))
12. FLY-AIRPLANE: (X3 ∈ (NOT (GAT (GAT−1 LOCATION)))) ∧ (X1 ∈ (AT−1 (GAT (AT−1 (CAT OBJ))))) ∧ (X3 ∈ (AT
(NOT (GAT−1 (AT AIRPLANE))))) ∧ (X3 ∈ (AT OBJ)) ∧ (X1 ∈ (NOT (IN (GAT−1 AIRPORT)))) ∧ (X3 ∈ (NOT (AT
(IN OBJ))))

112

API with a Policy Language Bias

13. UNLOAD-TRUCK: (X1 ∈ (GAT−1 AIRPORT))
14. LOAD-TRUCK: (X1 ∈ (AT−1 (CAT (GAT−1 (AT AIRPLANE))))) ∧ (X1 ∈ (NOT (GAT−1 LOCATION)))
15. LOAD-TRUCK: (X1 ∈ (GAT−1 (CAT (GAT−1 (AT AIRPLANE))))) ∧ (X1 ∈ (NOT (GAT−1 (AT TRUCK)))) ∧ (X1 ∈
(GAT−1 (AT (GAT−1 (AT AIRPLANE)))))
16. LOAD-TRUCK: (X1 ∈ (GAT−1 (NOT AIRPORT))) ∧ (X1 ∈ (NOT (GAT−1 (AT TRUCK))))
17. FLY-AIRPLANE: (X3 ∈ (AT (GAT−1 (AT AIRPLANE)))) ∧ (X1 ∈ (AT−1 (CAT OBJ)))
18. FLY-AIRPLANE: (X3 ∈ (NOT (GAT (AT−1 (CAT OBJ))))) ∧ (X1 ∈ (AT−1 (GAT (AT−1 (CAT OBJ))))) ∧ (X1 ∈ (AT−1
(GAT (GAT−1 (AT TRUCK)))))
19. LOAD-TRUCK: (X1 ∈ (GAT−1 (AT AIRPLANE))) ∧ (X1 ∈ (NOT (GAT−1 (AT TRUCK)))) ∧ (X1 ∈ (AT−1 (CAT OBJ)))
20. LOAD-AIRPLANE: (X1 ∈ (GAT−1 AIRPORT)) ∧ (X1 ∈ (NOT (CAT−1 LOCATION))) ∧ (X1 ∈ (GAT−1 (NOT (AT
AIRPLANE)))) ∧ (X2 ∈ (NOT (IN (GAT−1 (NOT AIRPORT)))))
21. FLY-AIRPLANE: (X3 ∈ (AT (GAT−1 (AT AIRPLANE)))) ∧ (X3 ∈ (NOT (AT TRUCK)))
22. LOAD-TRUCK: (X1 ∈ (AT−1 (CAT (GAT−1 (NOT AIRPORT))))) ∧ (X1 ∈ (GAT−1 AIRPORT))
23. DRIVE-TRUCK: (X3 ∈ (NOT (AT OBJ))) ∧ (X1 ∈ (NOT (AT−1 (CAT OBJ)))) ∧ (X1 ∈ (AT−1 (GAT (GAT−1 LOCATION))))
24. LOAD-TRUCK: (X1 ∈ (GAT−1 (CAT (CAT−1 AIRPORT)))) ∧ (X1 ∈ (NOT (CAT−1 LOCATION)))
25. FLY-AIRPLANE: (X3 ∈ (AT (GAT−1 (AT AIRPLANE)))) ∧ (X1 ∈ (AT−1 (AT OBJ)))
26. DRIVE-TRUCK: (X1 ∈ (IN OBJ))
27. DRIVE-TRUCK: (X1 ∈ (AT−1 (GAT (GAT−1 AIRPORT)))) ∧ (X3 ∈ (AT (GAT−1 AIRPORT))) ∧ (X1 ∈ (AT−1 (NOT
(AT AIRPLANE))))
28. FLY-AIRPLANE: (X3 ∈ (CAT (GAT−1 (AT TRUCK)))) ∧ (X1 ∈ (AT−1 (GAT (GAT−1 LOCATION))))
29. LOAD-TRUCK: (X1 ∈ (GAT−1 (AT OBJ))) ∧ (X1 ∈ (NOT (CAT−1 LOCATION)))
30. DRIVE-TRUCK: (X3 ∈ (AT (GAT−1 (AT AIRPLANE)))) ∧ (X1 ∈ (NOT (AT−1 (CAT OBJ))))
31. DRIVE-TRUCK: (X3 ∈ (AT AIRPLANE)) ∧ (X3 ∈ (AT (GAT−1 (AT TRUCK))))
32. UNLOAD-AIRPLANE: (X2 ∈ (NOT (AT−1 (CAT OBJ)))) ∧ (X1 ∈ (GAT−1 (NOT AIRPORT)))
33. DRIVE-TRUCK: (X3 ∈ (AT (GAT−1 (AT TRUCK))))
34. LOAD-TRUCK: (X1 ∈ (AT−1 (NOT AIRPORT))) ∧ (X1 ∈ (GAT−1 AIRPORT))
35. FLY-AIRPLANE: (X3 ∈ (AT (GAT−1 LOCATION)))
36. FLY-AIRPLANE: (X1 ∈ (IN OBJ)) ∧ (X3 ∈ (NOT (GAT (GAT−1 LOCATION)))) ∧ (X1 ∈ (NOT (IN (GAT−1 AIRPORT))))
∧ (X3 ∈ (NOT (AT (IN OBJ)))) ∧ (X1 ∈ (AT−1 (GAT (AT−1 (CAT OBJ))))))
37. DRIVE-TRUCK: (X1 ∈ (AT−1 (AT AIRPLANE)))
38. LOAD-AIRPLANE: (X1 ∈ (GAT−1 (NOT AIRPORT)))
Blocks World
1. STACK: (X2 ∈ (GON HOLDING)) ∧ (X2 ∈ (CON−∗ (MIN GON))) ∧ (X1 ∈ (GON−∗ ON-TABLE))
2. PUTDOWN:
3. UNSTACK: (X1 ∈ (ON−∗ (ON (MIN GON)))) ∧ (X2 ∈ (CON−∗ (ON∗ (MIN GON))))
4. UNSTACK: (X2 ∈ (ON−1 (GON CLEAR))) ∧ (X2 ∈ (GON∗ (ON−∗ (MIN GON)))) ∧ (X1 ∈ (ON−∗ (GON ON-TABLE)))
(X1 ∈ (GON−∗ (NOT CLEAR)))
5. PICKUP: (X1 ∈ (GON−1 (CON−∗ (MIN GON)))) ∧ (X1 ∈ (GON−1 CLEAR)) (X1 ∈ (GON−1 (CON−∗ ON-TABLE)))
6. UNSTACK: (X2 ∈ (CON−∗ (GON−1 CLEAR))) ∧ (X1 ∈ (GON−1 (ON−∗ (MIN GON)))) ∧ (X1 ∈ (GON−1 (CON∗
CLEAR)))

113

Fern, Yoon, & Givan

7. UNSTACK: (X1 ∈ (NOT (GON−∗ (MIN GON))))
8. UNSTACK: (X2 ∈ (GON ON-TABLE)) ∧ (X1 ∈ (GON−1 (CON−∗ (MIN GON)))) ∧ (X1 ∈ (GON−1 CLEAR))
9. UNSTACK: (X1 ∈ (NOT (CON−∗ (MIN GON)))) ∧ (X2 ∈ (ON−∗ (GON−1 ON-TABLE))) ∧ (X2 ∈ (GON−∗ (NOT ONTABLE))) ∧ (X1 ∈ (GON∗ (GON−∗ ON-TABLE))) (X1 ∈ (GON−∗ (NOT CLEAR)))
10. UNSTACK: (X2 ∈ (NOT (CON CLEAR))) ∧ (X1 ∈ (GON−1 (CON−∗ ON-TABLE)))
11. UNSTACK: (X1 ∈ (GON−1 CLEAR)) ∧ (X1 ∈ (ON−∗ (ON (MIN GON)))
Ground Logistics
1. LOAD: (X2 ∈ (NOT (IN (GIN−1 CITY)))) ∧ (X1 ∈ (NOT (CIN−1 CITY))) ∧ (X1 ∈ (GIN−1 CITY))
2. UNLOAD: (X1 ∈ (GIN−1 X3 ))
3. DRIVE: (X1 ∈ (IN (GIN−1 X3 )))
4. DRIVE: (X3 ∈ (NOT (GIN BLOCK))) ∧ (X3 ∈ (IN (GIN−1 CITY))) ∧ (X1 ∈ CAR) (X2 ∈ CLEAR)
5. DRIVE: (X3 ∈ (IN (GIN−1 RAIN))) ∧ (X1 ∈ TRUCK)
Colored Blocks World
1. PICK-UP-BLOCK-FROM: (X2 ∈ (NOT (CON-TOP-OF−∗ TABLE))) ∧ (X2 ∈ (GON-TOP-OF−1 (ON-TOP-OF BLOCK)))
2. PUT-DOWN-BLOCK-ON: (X2 ∈ (CON-TOP-OF−1 (CON-TOP-OF−1 BLOCK))) ∧ (X2 ∈ (GON-TOP-OF HOLDING)) ∧
(X2 ∈ (CON-TOP-OF−∗ TABLE))
3. PICK-UP-BLOCK-FROM: (X2 ∈ (NOT (CON-TOP-OF BLOCK))) ∧ (X1 ∈ (ON-TOP-OF−∗ (GON-TOP-OF−1 TABLE)))
∧ (X2 ∈ (GON-TOP-OF∗ (GON-TOP-OF−1 BLOCK))) ∧ (X2 ∈ (NOT (CON-TOP-OF−1 BLOCK))) ∧ (X2 ∈ (ON-TOPOF−1 (GON-TOP-OF BLOCK))) ∧ (X1 ∈ (GON-TOP-OF∗ (GON-TOP-OF−1 BLOCK)))
4. PICK-UP-BLOCK-FROM: (X1 ∈ (NOT (CON-TOP-OF−∗ TABLE))) ∧ (X1 ∈ (GON-TOP-OF−1 (CON-TOP-OF−∗ TABLE))) ∧ (X1 ∈ (GON-TOP-OF−∗ (ON-TOP-OF−1 BLOCK)))
5. PUT-DOWN-BLOCK-ON: (X2 ∈ (CON-TOP-OF−1 (ON-TOP-OF−1 TABLE))) ∧ (X2 ∈ (GON-TOP-OF HOLDING)) ∧ (X2
∈ (CON-TOP-OF−∗ TABLE))
6. PUT-DOWN-BLOCK-ON: (X2 ∈ (CON-TOP-OF (ON-TOP-OF BLOCK))) ∧ (X1 ∈ (GON-TOP-OF−1 (GON-TOP-OF−1
BLOCK)))
7. PUT-DOWN-BLOCK-ON: (X2 ∈ (GON-TOP-OF HOLDING)) ∧ (X2 ∈ (CON-TOP-OF−∗ TABLE))
8. PUT-DOWN-BLOCK-ON: (X2 ∈ TABLE)
9. PICK-UP-BLOCK-FROM: (X2 ∈ (NOT (CON-TOP-OF−∗ TABLE))) ∧ (X2 ∈ (GON-TOP-OF−1 (CON-TOP-OF−∗ TABLE)))
10. PICK-UP-BLOCK-FROM: (X1 ∈ (GON-TOP-OF−1 (CON-TOP-OF−1 TABLE))) ∧ (X2 ∈ TABLE) ∧ (X1 ∈ (GON-TOP-OF
(GON-TOP-OF BLOCK))) ∧ (X1 ∈ (GON-TOP-OF (ON-TOP-OF−1 TABLE)))
11. PICK-UP-BLOCK-FROM: (X2 ∈ (ON-TOP-OF (CON-TOP-OF BLOCK))) ∧ (X1 ∈ (GON-TOP-OF−1 (CON-TOP-OF−1
TABLE)))
12. PICK-UP-BLOCK-FROM: (X2 ∈ (ON-TOP-OF−1 BLOCK)) ∧ (X2 ∈ (NOT (CON-TOP-OF−∗ TABLE))) ∧ (X2 ∈ (GONTOP-OF−∗ (ON-TOP-OF−1 BLOCK))) ∧ (X2 ∈ (GON-TOP-OF∗ (ON-TOP-OF−1 BLOCK)))
13. PICK-UP-BLOCK-FROM: (X1 ∈ (GON-TOP-OF−1 (GON-TOP-OF−1 TABLE)))
Boxworld
1. DRIVE-TRUCK: (X2 ∈ (GBOX-AT-CITY (BOX-AT-CITY−1 X3 ))) ∧ (X3 ∈ (NOT (CAN-FLY (TRUCK-AT-CITY (NOT
PREVIOUS))))) ∧ (X3 ∈ (CAN-DRIVE−1 PREVIOUS)) ∧ (X2 ∈ (NOT (CAN-FLY (TRUCK-AT-CITY (NOT PREVIOUS))))) ∧ (X3 ∈ (NOT (CAN-FLY (BOX-AT-CITY BOX)))) ∧ (X2 ∈ (CAN-DRIVE (CAN-DRIVE (BOX-AT-CITY BOX))))
∧ (X3 ∈ (NOT (CAN-FLY (TRUCK-AT-CITY (BOX-ON-TRUCK (GBOX-AT-CITY−1 CITY))))))
2. UNLOAD-BOX-FROM-TRUCK-IN-CITY: (X1 ∈ (GBOX-AT-CITY−1 (TRUCK-AT-CITY PREVIOUS))) ∧ (X3 ∈ (GBOXAT-CITY BOX)) ∧ (X3 ∈ (NOT (BOX-AT-CITY PREVIOUS))) ∧ (X1 ∈ (GBOX-AT-CITY−1 (CAN-DRIVE−1 (CANDRIVE−1 (CAN-FLY CITY))))) ∧ (X2 ∈ (BOX-ON-TRUCK (GBOX-AT-CITY−1 PREVIOUS)))
3. DRIVE-TRUCK: (X1 ∈ (BOX-ON-TRUCK (GBOX-AT-CITY−1 X3 ))) ∧ (X2 ∈ (NOT (CAN-DRIVE (TRUCK-AT-CITY
(BOX-ON-TRUCK (GBOX-AT-CITY−1 CITY))))))

114

API with a Policy Language Bias

4. DRIVE-TRUCK: (X3 ∈ (CAN-DRIVE (BOX-AT-CITY PREVIOUS))) ∧ (X2 ∈ (CAN-FLY (CAN-DRIVE−1 (BOX-AT-CITY
BOX)))) ∧ (X3 ∈ (CAN-DRIVE (CAN-FLY (TRUCK-AT-CITY TRUCK)))) ∧ (X2 ∈ (NOT (CAN-DRIVE (TRUCK-AT-CITY
(BOX-ON-TRUCK (GBOX-AT-CITY−1 CITY)))))) ∧ (X2 ∈ PREVIOUS) ∧ (X2 ∈ (CAN-DRIVE (CAN-DRIVE X3 ))) ∧ (X3
∈ (NOT (TRUCK-AT-CITY (BOX-ON-TRUCK (GBOX-AT-CITY−1 CITY))))) ∧ (X3 ∈ (NOT (CAN-FLY PREVIOUS))) ∧
(X3 ∈ (CAN-DRIVE (NOT (BOX-AT-CITY BOX)))) ∧ (X2 ∈ (CAN-DRIVE (CAN-DRIVE−1 X3 ))) ∧ (X3 ∈ (CAN-DRIVE
(NOT (TRUCK-AT-CITY TRUCK))))
5. LOAD-BOX-ON-TRUCK-IN-CITY: (X1 ∈ (GBOX-AT-CITY−1 (CAN-DRIVE (TRUCK-AT-CITY TRUCK)))) ∧ (X3 ∈ (NOT
(PLANE-AT-CITY PREVIOUS))) ∧ (X3 ∈ (CAN-DRIVE (CAN-DRIVE−1 (CAN-FLY CITY)))) ∧ (X3 ∈ (CAN-DRIVE−1
(NOT (TRUCK-AT-CITY (NOT PREVIOUS)))))
6. UNLOAD-BOX-FROM-TRUCK-IN-CITY: (X3 ∈ (GBOX-AT-CITY (BOX-ON-TRUCK−1 TRUCK))) ∧ (X3 ∈ (NOT (CANFLY (TRUCK-AT-CITY (BOX-ON-TRUCK (GBOX-AT-CITY−1 CITY)))))) ∧ (X1 ∈ (GBOX-AT-CITY−1 CITY))
7. DRIVE-TRUCK: (X1 ∈ (BOX-ON-TRUCK (GBOX-AT-CITY−1 PREVIOUS))) ∧ (X3 ∈ (CAN-DRIVE (GBOX-AT-CITY
(GBOX-AT-CITY−1 PREVIOUS)))) ∧ (X3 ∈ (NOT (PLANE-AT-CITY PLANE))) ∧ (X2 ∈ (NOT (CAN-FLY (GBOX-ATCITY (GBOX-AT-CITY−1 PREVIOUS)))))
8. FLY-PLANE: (X1 ∈ (BOX-ON-PLANE (GBOX-AT-CITY−1 X3 )))
9. UNLOAD-BOX-FROM-PLANE-IN-CITY: (X1 ∈ (GBOX-AT-CITY−1 PREVIOUS))
10. FLY-PLANE: (X2 ∈ (NOT (CAN-DRIVE (TRUCK-AT-CITY (BOX-ON-TRUCK (GBOX-AT-CITY−1 CITY)))))) ∧ (X2 ∈
(GBOX-AT-CITY BOX)) ∧ (X3 ∈ (NOT (PLANE-AT-CITY PREVIOUS))) ∧ (X1 ∈ (NOT PREVIOUS))
11. LOAD-BOX-ON-PLANE-IN-CITY: (X1 ∈ (GBOX-AT-CITY−1 (CAN-FLY PREVIOUS))) ∧ (X3 ∈ (NOT (TRUCK-AT-CITY
(NOT PREVIOUS)))) ∧ (X3 ∈ (NOT (CAN-DRIVE (TRUCK-AT-CITY (BOX-ON-TRUCK (GBOX-AT-CITY−1 CITY))))))
12. DRIVE-TRUCK: (X1 ∈ (BOX-ON-TRUCK (GBOX-AT-CITY−1 X3 ))) ∧ (X2 ∈ (NOT (CAN-DRIVE (CAN-FLY PREVIOUS)))) ∧ (X2 ∈ (CAN-DRIVE−1 (CAN-FLY CITY)))
13. LOAD-BOX-ON-TRUCK-IN-CITY: (X1 ∈ (GBOX-AT-CITY−1 PREVIOUS))

References
Aler, R., Borrajo, D., & Isasi, P. (2002). Using genetic programming to learn and improve
control knowledge. Artificial Intelligence, 141 (1-2), 29–56.
Ambite, J. L., Knoblock, C. A., & Minton, S. (2000). Learning plan rewriting rules. In
Artificial Intelligence Planning Systems, pp. 3–12.
Bacchus, F. (2001). The AIPS ’00 planning competition. AI Magazine, 22(3)(3), 57–62.
Bacchus, F., & Kabanza, F. (2000). Using temporal logics to express search control knowledge for planning. Artificial Intelligence, 16, 123–191.
Bagnell, J., Kakade, S., Ng, A., & Schneider, J. (2003). Policy search by dynamic programming. In Proceedings of the 16th Conference on Advances in Neural Information
Processing.
Bellman, R. (1957). Dynamic Programming. Princeton University Press.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific.
Boutilier, C., & Dearden, R. (1996). Approximating value trees in structured dynamic
programming. In Saitta, L. (Ed.), International Conference on Machine Learning.
Boutilier, C., Dearden, R., & Goldszmidt, M. (2000). Stochastic dynamic programming
with factored representations. Artificial Intelligence, 121 (1-2), 49–107.
Boutilier, C., Reiter, R., & Price, B. (2001). Symbolic dynamic programming for first-order
MDPs. In International Joint Conference on Artificial Intelligence.
115

Fern, Yoon, & Givan

Dean, T., & Givan, R. (1997). Model minimization in markov decision processes. In National
Conference on Artificial Intelligence, pp. 106–111.
Dean, T., Givan, R., & Leach, S. (1997). Model reduction techniques for computing approximately optimal solutions for Markov decision processes. In Conference on Uncertainty
in Artificial Intelligence, pp. 124–131.
Dietterich, T., & Wang, X. (2001). Batch value function approximation via support vectors.
In Proceedings of the Conference on Advances in Neural Information Processing.
Dzeroski, S., DeRaedt, L., & Driessens, K. (2001). Relational reinforcement learning. Machine Learning, 43, 7–52.
Estlin, T. A., & Mooney, R. J. (1996). Multi-strategy learning of search control for partialorder planning. In National Conference on Artificial Intelligence.
Fern, A., Yoon, S., & Givan, R. (2003). Approximate policy iteration with a policy language bias. In Proceedings of the 16th Conference on Advances in Neural Information
Processing.
Finkelstein, L., & Markovitch, S. (1998). A selective macro-learning algorithm and its
application to the NxN sliding-tile puzzle. Journal of Artificial Intelligence Research,
8, 223–263.
Givan, R., Dean, T., & Greig, M. (2003). Equivalence notions and model minimization in
Markov decision processes. Artificial Intelligence, 147 (1-2), 163–223.
Gretton, C., & Thiebaux, S. (2004). Exploiting first-order regression in inductive policy
selection. In Conference on Uncertainty in Artificial Intelligence.
Guestrin, C., Koller, D., Gearhart, C., & Kanodia, N. (2003a). Generalizing plans to new
environments in relational mdps. In International Joint Conference on Artificial Intelligence.
Guestrin, C., Koller, D., Parr, R., & Venkataraman, S. (2003b). Efficient solution algorithms
for factored mdps. Journal of Artificial Intelligence Research, 19, 399–468.
Hoffman, J., Porteous, J., & Sebastia, L. (2004). Ordered landmarks in planning. Journal
of Artificial Intelligence Research, 22, 215–278.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through
heuristic search. Journal of Artificial Intelligence Research, 14, 263–302.
Howard, R. (1960). Dynamic Programming and Markov Decision Processes. MIT Press.
Huang, Y.-C., Selman, B., & Kautz, H. (2000). Learning declarative control rules for
constraint-based planning. In International Conference on Machine Learning, pp.
415–422.
Kearns, M. J., Mansour, Y., & Ng, A. Y. (2002). A sparse sampling algorithm for nearoptimal planning in large markov decision processes. Machine Learning, 49 (2–3),
193–208.
Kersting, K., Van Otterlo, M., & DeRaedt, L. (2004). Bellman goes relational. In Proceedings
of the Twenty-First International Conference on Machine Learning.
116

API with a Policy Language Bias

Khardon, R. (1999a). Learning action strategies for planning domains. Artificial Intelligence, 113 (1-2), 125–148.
Khardon, R. (1999b). Learning to take actions. Machine Learning, 35 (1), 57–90.
Lagoudakis, M., & Parr, R. (2003). Reinforcement learning as classification: Leveraging
modern classifiers. In International Conference on Machine Learning.
Langford, J., & Zadrozny, B. (2004). Reducing t-step reinforcement learning to classification.
http://hunch.net/∼jl/projects/reductions/RL to class/colt submission.ps.
Martin, M., & Geffner, H. (2000). Learning generalized policies in planning domains using
concept languages. In International Conference on Principles of Knowledge Representation and Reasoning.
Mataric, M. (1994). Reward functions for accelarated learning. In Proceedings of the International Conference on Machine Learning.
McAllester, D., & Givan, R. (1993). Taxonomic syntax for first order inference. Journal of
the ACM, 40 (2), 246–283.
McAllester, D. (1991). Observations on cognitive judgements. In National Conference on
Artificial Intelligence.
McGovern, A., Moss, E., & Barto, A. (2002). Building a basic block instruction scheduler
using reinforcement learning and rollouts. Machine Learning, 49 (2/3), 141–160.
Minton, S. (1988). Quantitative results concerning the utility of explanation-based learning.
In National Conference on Artificial Intelligence.
Minton, S. (Ed.). (1993). Machine Learning Methods for Planning. Morgan Kaufmann.
Minton, S., Carbonell, J., Knoblock, C. A., Kuokka, D. R., Etzioni, O., & Gil, Y. (1989).
Explanation-based learning: A problem solving perspective. Artificial Intelligence, 40,
63–118.
Natarajan, B. K. (1989). On learning from exercises. In Annual Workshop on Computational
Learning Theory.
Reddy, C., & Tadepalli, P. (1997). Learning goal-decomposition rules using exercises. In
International Conference on Machine Learning, pp. 278–286. Morgan Kaufmann.
Rivest, R. (1987). Learning decision lists. Machine Learning, 2 (3), 229–246.
Tesauro, G. (1992). Practical issues in temporal difference learning. Machine Learning, 8,
257–277.
Tesauro, G., & Galperin, G. (1996). On-line policy improvement using monte-carlo search.
In Conference on Advances in Neural Information Processing.
Tsitsiklis, J., & Van Roy, B. (1996). Feature-based methods for large scale DP. Machine
Learning, 22, 59–94.
Veloso, M., Carbonell, J., Perez, A., Borrajo, D., Fink, E., & Blythe, J. (1995). Integrating
planning and learning: The PRODIGY architecture. Journal of Experimental and
Theoretical AI, 7 (1).
Wu, G., Chong, E., & Givan, R. (2001). Congestion control via online sampling. In Infocom.
117

Fern, Yoon, & Givan

Yan, X., Diaconis, P., Rusmevichientong, P., & Van Roy, B. (2004). Solitaire: Man versus
machine. In Conference on Advances in Neural Information Processing.
Yoon, S., Fern, A., & Givan, R. (2002). Inductive policy selection for first-order MDPs. In
Conference on Uncertainty in Artificial Intelligence.
Younes, H. (2003). Extending pddl to model stochastic decision processes. In Proceedings
of the International Conference on Automated Planning and Scheduling Workshop on
PDDL.
Zimmerman, T., & Kambhampati, S. (2003). Learning-assisted automated planning: Looking back, taking stock, going forward. AI Magazine, 24(2)(2), 73–96.

118

Journal of Artificial Intelligence Research 25 (2006) 187-231

Submitted 03/05; published 02/06

An Approach to Temporal Planning and Scheduling in
Domains with Predictable Exogenous Events
Alfonso Gerevini
Alessandro Saetti
Ivan Serina

gerevini@ing.unibs.it
saetti@ing.unibs.it
serina@ing.unibs.it

Dipartimento di Elettronica per l’Automazione
Università degli Studi di Brescia
Via Branze 38, I-25123 Brescia, Italy

Abstract
The treatment of exogenous events in planning is practically important in many realworld domains where the preconditions of certain plan actions are affected by such events.
In this paper we focus on planning in temporal domains with exogenous events that happen
at known times, imposing the constraint that certain actions in the plan must be executed
during some predefined time windows. When actions have durations, handling such temporal constraints adds an extra difficulty to planning. We propose an approach to planning
in these domains which integrates constraint-based temporal reasoning into a graph-based
planning framework using local search. Our techniques are implemented in a planner that
took part in the 4th International Planning Competition (IPC-4). A statistical analysis
of the results of IPC-4 demonstrates the effectiveness of our approach in terms of both
CPU-time and plan quality. Additional experiments show the good performance of the
temporal reasoning techniques integrated into our planner.

1. Introduction
In many real-world planning domains, the execution of certain actions can only occur during
some predefined time windows where one or more necessary conditions hold. For instance,
a car can be refueled at a gas station only when the gas station is open, or a space telescope
can take a picture of a certain planet region only when this region is observable. The truth
of these conditions is determined by some exogenous events that happen at known times,
and that cannot be influenced by the actions available to the planning agent (e.g., the
closing of the gas station or the planet movement).
Several frameworks supporting action durations and time windows have been proposed
(e.g., Vere, 1983; Muscettola, 1994; Laborie & Ghallab, 1995; Schwartz & Pollack, 2004;
Kavuluri & U, 2004; Sanchez, Tang, & Mali, 2004). However, most of them are domaindependent systems or are not fast enough on large-scale problems. In this paper, we propose
a new approach to planning with these temporal features, integrating constraint-based
temporal reasoning into a graph-based planning framework.
The last two versions of the domain definition language of the International planning competition (IPC) support action durations and predictable (deterministic) exogenous
events (Fox & Long, 2003; Edelkamp & Hoffmann, 2004). In PDDL2.1, predictable exogenous events can be implicitly represented (Fox, Long, & Halsey, 2004), while in PDDL2.2
they can be explicitly represented through timed initial literals, one of the two new PDDL
c
°2006
AI Access Foundation. All rights reserved.

Gerevini, Saetti & Serina

features on which the 2004 competition (IPC-4) focused. Timed initial literals are specified
in the description of the initial state of the planning problem through assertions of the form
“(at t L)”, where t is a real number, and L is a ground literal whose predicate does not
appear in the effects of any domain action. The obvious meaning of (at t L) is that L is
true from time t. A set of these assertions involving the same ground predicate defines a
sequence of disjoint time windows over which the timed predicate holds. An example in the
well-known “ZenoTravel” domain (Penberthy, 1993; Long & Fox, 2003a) is
(at
(at
(at
(at

8 (open-fuelstation city1))
12 (not (open-fuelstation city1)))
15 (open-fuelstation city1))
20 (not (open-fuelstation city1))).

These assertions define two time windows over which (open-fuelstation city1) is true,
i.e., from 8 to 12 (excluded) and from 15 to 20 (excluded). A timed initial literal is relevant
to the planning process when it is a precondition of a domain action, which we call a timed
precondition of the action. Each timed precondition of an action can be seen as a temporal
scheduling constraint for the action, defining the feasible time window(s) when the action
can be executed. When actions in a plan have durations and timed preconditions, computing
a valid plan requires planning and reasoning about time to be integrated, in order to check
whether the execution of the planned actions can satisfy their scheduling constraints. If an
action in the plan cannot be scheduled, then the plan is not valid and it must be revised.
The main contributions of this work are: (i) a new representation of temporal plans
with action durations and timed preconditions, called Temporally-Disjunctive Action Graph,
(TDA-graph) integrating disjunctive constraint-based temporal reasoning into a recent
graph-based approach to planning; (ii) a polynomial method for solving the disjunctive temporal reasoning problems that arise in this context; (iii) some new local search techniques
to guide the planning process using our representation; and (iv) an experimental analysis
evaluating the performance of our methods implemented in a planner called lpg-td, which
took part in IPC-4 showing very good performance in many benchmark problems.
The “td” extension in the name of our planner is an abbreviation of “timed initial literals
and derived predicates”, the two main new features of PDDL2.2.1 In lpg-td, the techniques
for handling timed initial literals are quite different from the techniques for handling derived
predicates. The first ones concern representing temporal plans with predictable exogenous
events and fast temporal reasoning for action scheduling during planning; the second ones
concern incorporating a rule-based inference system for efficient reasoning about derived
predicates during planning. Both timed initial literals and derived predicates require to
change the heuristics guiding the search of the planner, but in a radically different way. In
this paper, we focus on timed initial literals, which are by themselves a significant and useful
extension to PDDL2.1. Moreover, an analysis of the results of IPC-4 shows that lpg-td was
top performer in the benchmark problems involving this feature. The treatment of derived
predicates in lpg-td is presented in another recent paper (Gerevini et al., 2005b).
1. Derived predicates allow us to express in a concise and natural way some indirect action effects. Informally, they are predicates which do not appear in the effect of any action, and their truth is determined
by some domain rules specified as part of the domain description.

188

An Approach to Temporal Planning and Scheduling

The paper is organized as follows. In Section 2, after some necessary background, we
introduce the TDA-graph representation and a method for solving the disjunctive temporal
reasoning problems that arise in our context. In Section 3, we describe some new local
search heuristics for planning in the space of TDA-graphs. In Section 4, we present the
experimental analysis illustrating the efficiency of our approach. In Section 5, we discuss
some related work. Finally, in Section 6 we give the conclusions.

2. Temporally Disjunctive Action Graph
Like in partial-order causal-link planning, (e.g., Penberthy & Weld, 1992; McAllester &
Rosenblitt, 1991; Nguyen & Kambhampati, 2001), in our framework we search in a space
of partial plans. Each search state is a partial temporal plan that we represent by a
Temporally-Disjunctive Action Graph (TDA-graph). A TDA-graph is an extension of the
linear action graph representation (Gerevini, Saetti, & Serina, 2003) which integrates disjunctive temporal constraints for handling timed initial literals. A linear action graph is
a variant of the well-known planning graph (Blum & Furst, 1997). In this section, after
some necessary background on linear action graphs and disjunctive temporal constraints,
we introduce TDA-graphs, and we propose some techniques for temporal reasoning in the
context of this representation that will be used in the next section.
2.1 Background: Linear Action Graph and Disjunctive Temporal Constraints
A linear action graph (LA-graph) A for a planning problem Π is a directed acyclic leveled
graph alternating a fact level, and an action level. Fact levels contain fact nodes, each of
which is labeled by a ground predicate of Π. Each fact node f at a level l is associated
with a no-op action node at level l representing a dummy action having the predicate of f
as its only precondition and effect. Each action level contains one action node labeled by
the name of a domain action that it represents, and the no-op nodes corresponding to that
level.
An action node labeled a at a level l is connected by incoming edges from the fact nodes
at level l representing the preconditions of a (precondition nodes), and by outgoing edges
to the fact nodes at level l + 1 representing the effects of a (effect nodes). The initial level
contains the special action node astart , and the last level the special action node aend . The
effect nodes of astart represent the positive facts of the initial state of Π, and the precondition
nodes of aend the goals of Π.
A pair of action nodes (possibly no-op nodes) can be constrained by a persistent mutex
relation (Fox & Long, 2003), i.e., a mutually exclusive relation holding at every level of the
graph, imposing that the involved actions can never occur in parallel in a valid plan. Such
relations can be efficiently precomputed using an algorithm that we proposed in a previous
work (Gerevini et al., 2003).
An LA-graph A also contains a set of ordering constraints between actions in the (partial) plan represented by the graph. These constraints are (i) constraints imposed during
search to deal with mutually exclusive actions: if an action a at level l of A is mutex with
an action node b at a level after l, then a is constrained to finish before the start of b; (ii)
constraints between actions implied by the causal structure of the plan: if an action a is
189

Gerevini, Saetti & Serina

used to achieve a precondition of an action b, then a is constrained to finish before the start
of b.
The effects of an action node can be automatically propagated to the next levels of
the graph through the corresponding no-ops, until there is an interfering (mutex) action
“blocking” the propagation, or the last level of the graph has been reached (Gerevini et al.,
2003). In the rest of the paper, we assume that the LA-graph incorporates this propagation.
A Disjunctive Temporal Problem (DTP) (Stergiou & Koubarakis, 2000; Tsamardinos
& Pollack, 2003) is a pair hP, Ci, where P is a set of time point variables, C is a set of
disjunctive constraints c1 ∨ · · · ∨ cn , ci is of form yi − xi ≤ ki , xi and yi are in P, and ki is a
real number (i = 1...n). When C contains only unary constraints, the DTP is called Simple
Temporal Problem (STP) (Dechter, Meiri, & Pearl, 1991).
A DTP is consistent if and only if the DTP has a solution. A solution of a DTP is an
assignment of real values to the variables of the DTP that is consistent with every constraint
in the DTP. Computing a solution for a DTP is an NP-hard problem (Dechter et al., 1991),
while computing a solution of an STP can be accomplished in polynomial time. Given
an STP with a special “start time” variable s preceding all the others, we can compute a
solution of the STP where each variable has the shortest possible distance from s in O(n · c)
time, for n variables and c constraints in the STP (Dechter et al., 1991; Gerevini & Cristani,
1997). We call such a solution an optimal solution of the STP. Clearly, a DTP is consistent if
and only if we can choose from each constraint in the DTP a disjunct obtaining a consistent
STP, and any solution of such an STP is also a solution of the original DTP.
Finally, an STP is consistent if and only if the distance graph of the STP does not
contain negative cycles (Dechter et al., 1991). The distance graph of an STP hP, Ci is a
directed labeled graph with a vertex labeled p for each p ∈ P, and with an edge from v ∈ P
to w ∈ P labeled k for each constraint w − v ≤ k ∈ C.
2.2 Augmenting the LA-graph with Disjunctive Temporal Constraints
Let p be a timed precondition over a set W (p) of time windows. In the following, x − and x+
indicate the start time and end time of x, respectively, where x is either a time window or an
action. Moreover, al indicates an action node at level l of the LA-graph under consideration.
For clarity of presentation, we will describe our techniques focusing on action preconditions
that must hold during the whole execution of the action (except at the end point of the
action), and on operator effects that hold at the end of the action execution, i.e., on PDDL
conditions of type “over all”, and PDDL effects of type “at end” (Fox & Long, 2003). 2
In order to represent plans where actions have durations and time windows for their
execution, we augment the ordering constraints of an LA-graph with (i) action duration
constraints and (ii) action scheduling constraints. Duration constraints have form
a+ − a− = Dur(a),
where Dur(a) denotes the duration of an action a (for the special actions a start and aend ,
+
−
+
we have Dur(astart ) = Dur(aend ) = 0, since a−
start = astart and aend = aend ). Duration
constraints are supported by the representation proposed in a previous work (Gerevini
2. Our methods and planner support all the types of operator condition and effect that can be specified in
PDDL 2.1 and 2.2.

190

An Approach to Temporal Planning and Scheduling

Level 1
(0)
p1

Level 2

Level 3

Goal level

(−)

p1

p1

p1

p1
p

mutex

p2

(0)

(50)

p5

(50)

p5

(50)

p5

a3

a1
(0)

astart

(0)

p3

p3

p3

(0)

p7

p3
mutex

(−)

astart

p6

(90)

(70)

a1

aend

(70)

p8

(70)

p8

p

p10

[15]
(75)

[50]
(0)

p

(90)

a2
a3

(70)

p8

aend

a2
(0)

p4

(0)

p4

(0)

p4

[70]
(0)

(70)

p9

(70)

p9

(70)

p9

(70)

0

25

50

75

90

125

p9

Figure 1: An example of LA-graph with nodes labeled by T -values (in round brackets),
and the Gantt chart of the actions labeling the nodes of the LA-graph. Square
nodes are action nodes; circle nodes are fact nodes. Action nodes are also marked
by the duration of the represented actions (in square brackets). Unsupported
precondition nodes are labeled “(–)”. Dashed edges form chains of no-ops blocked
by mutex actions. Grey areas in the Gantt chart represent the time windows for
the timed precondition p of a3 .

et al., 2003), while the representation and treatment of scheduling constraints are a major
contribution of this work.
Let π be the plan represented by an LA-graph A. It is easy to see that the set C formed
by the ordering constraints in A and the duration constraints of the actions in π can be
encoded into an STP. For instance, if ai ∈ π is used to support a precondition node of aj ,
−
then a+
i − aj ≤ 0 is in C; if ai and aj are two mutex actions in π, and ai is ordered before aj ,
−
then a+
i − aj ≤ 0 is in C. Moreover, for every action a ∈ π, the following STP-constraints
are in C:
a+ − a− ≤ Dur(a), a− − a+ ≤ −Dur(a),
which are equivalent to a+ − a− = Dur(a). A scheduling constraint imposes the constraint
that the execution of an action must occur during the time windows associated with a timed
precondition of the action. Syntactically, it is a disjunctive constraint c1 ∨ · · · ∨ cn , where
ci is of the form
±
±
(yi± − x±
i ≤ hi ) ∧ (vi − ui ≤ ki ),
± ± ±
u±
i , vi , xi , yi are action start times or action end times, and hi , ki ∈ R . For every action
a ∈ π with a timed precondition p, the following disjunctive constraint is added to C:

191

Gerevini, Saetti & Serina

_

w∈W (p)

¡¡

¢ ¡ +
¢¢ 3
−
−
+
a+
∧ a − a+
.
start − a ≤ −w
start ≤ w

Definition 1 A temporally disjunctive action graph (TDA-graph) is a 4-tuple hA, T , P, Ci
where
• A is a linear action graph;
• T is an assignment of real values to the nodes of A;
• P is the set of time point variables corresponding to the start times and the end times
of the actions labeling the action nodes of A;
• C is a set of ordering constraints, duration constraints and scheduling constraints
involving variables in P.
A TDA-graph hA, T , P, Ci represents the (partial) plan formed by the actions labeling
the action nodes of A with start times assigned by T . Figure 1 gives the LA-graph and
T -values of a simple TDA-graph containing five action nodes (astart , a1 , a2 , a3 , aend ) and
several fact nodes representing ten facts. The ordering constraints and duration constraints
in C are:4
−
+
−
a+
1 − a3 ≤ 0, a2 − a3 ≤ 0,
−
+
+
−
+
a1 − a1 = 50, a2 − a−
2 = 70, a3 − a3 = 15.

Assuming that p is a timed precondition of a3 with windows [25, 50) and [75, 125), the only
scheduling constraint in C is:
−
+
+
+
−
+
+
((a+
start − a3 ≤ −25) ∧ (a3 − astart ≤ 50)) ∨ ((astart − a3 ≤ −75) ∧ (a3 − astart ≤ 125)).

The pair hP, Ci defines a DTP D.5 Let Ds be the set of scheduling constraints in D.
We have that D represents a set Θ of STPs, each of which consists of the constraints in
D − Ds and one disjunct (pair of STP-constraints) for each disjunction in a subset D s0 of
Ds (Ds0 ⊆ Ds ). We call a consistent STP in Θ an induced STP of D. When an induced
STP contains a disjunct for every disjunction in Ds (i.e., Ds0 = Ds ), we say that such a
(consistent) STP is a complete induced STP of D.
The values assigned by T to the action nodes of A are the action start times corresponding to an optimal solution of an induced STP. We call these start times a schedule of the
actions in A. The T value labeling a fact node f of A is the earliest time t = Ta + Dur(a)
3. Note that, if p is an over all timed condition of an action a, then the end of a can be the time when an
exogenous event making p false happens, because in PDDL p is not required to be true at the end of a
(Fox & Long, 2003).
−
+
−
4. For brevity, in our examples we omit the constraints a+
start − ai ≤ 0 and ai − aend ≤ 0, for each action
ai , as well as the duration constraints of astart and aend , which have duration zero.
5. The disjunctive constraints in C are not exactly in DTP-form. However, it is easy to see that every
disjunctive constraint in C can be translated into an equivalent conjunction of constraints in exact DTPform. We use our more compact notation for clarity and efficiency reasons.

192

An Approach to Temporal Planning and Scheduling

such that a supports f in A, and a starts at Ta . If the induced STP from which we derive a
schedule is incomplete, then T may violate the scheduling constraint of some action nodes,
that we say are unscheduled in the current TDA-graph.
The following definitions present the notions of optimality for a complete induced STP
and of optimal schedule, which will be used in the next section.
Definition 2 Given a DTP D with a point variable p, a complete induced STP of D is an
optimal induced STP of D for p iff it has a solution assigning to p a value that is less
than or equal to the value assigned to p by every solution of every other complete induced
STP of D.
Definition 3 Given a DTP D of a TDA-graph G, an optimal schedule for the actions
in G is an optimal solution of an optimal induced STP of D for a−
end .
Note that an optimal solution minimizes the makespan of the represented (possibly
partial) plan. The DTP D of the previous example (Figure 1) has two induced STPs: one
with no time window for p (S1 ), and one including the pair of STP-constraints imposing the
time window [75, 125) to p (S2 ). The STP obtained by imposing the time window [25, 50)
to p is not an induced STP of the DTP, because it is not consistent. S1 is a partial induced
STP of D, while S2 is complete and optimal for the start time of aend . The temporal values
derived from the optimal solution of S2 that are assigned by T to the action nodes of the
+
−
−
−
+
−
TDA-graph are: a−
start = astart = 0, a1 = 0, a2 = 0, a3 = 75, aend = aend = 90.
2.3 Solving the DTP of a TDA-graph
In general, computing a complete induced STP of a DTP (if it exists) is an NP-hard problem
that can be solved by a backtracking algorithm (Stergiou & Koubarakis, 2000; Tsamardinos
& Pollack, 2003). However, given the particular structure of the temporal constraints
forming a TDA-graph, we show that this task can be accomplished in polynomial time with
a backtrack-free algorithm. Moreover, the algorithm computes an optimal induced STP for
a−
end .
In the following, we assume that each time window for a timed precondition is no shorter
than the duration of its action (otherwise, the time window should be removed from those
available for this precondition and, if no time window remains, then the action cannot be
used in any valid plan). Moreover, without loss of generality, we can assume that each
action has at most one timed precondition. It is easy to see that we can always replace a
set of over all timed conditions of an action a with a single equivalent timed precondition,
whose time windows are obtained by intersecting the windows forming the different original
timed conditions of a. Also a set of at start timed conditions and a set of at end timed
conditions can be compiled into single equivalent timed preconditions. This can be achieved
by translating these conditions into conditions of type over all. The idea is similar to the
one presented by Edelkamp (2004), with the difference that we can have more than one
time window associated with a timed condition, while Edelkamp assumes that each timed
condition is associated with a unique time window. Specifically, every at start timed
condition p of an action a can be translated into an equivalent timed condition p 0 of type
over all by replacing the scheduling constraint of p,
193

Gerevini, Saetti & Serina

p

p

Dur(a)

Dur(a)

q

Dur(a)

r

r

x
0

35 40

50

60

80

100

120

150

180

Figure 2: An example of a set of timed conditions compiled into a single timed precondition (x). The solid boxes represent the time windows associated with the timed
conditions p (of type at start), q (of type at end), and r (of type over all) of
an action a. A solid box extended by a dashed box indicates the extension of
the time window in the translation of the corresponding timed condition into an
over all timed condition for a.

_

w∈W (p)

¡¡

¢ ¡ −
¢¢
−
−
+
a+
∧ a − a+
,
start − a < −w
start < w

forcing a− to occur during one or more time windows, with
_

w∈W (p)

¡¡

¢¢ 6
¢ ¡ +
+
−
−
∧ a − a+
a+
start < w + Dur(a) .
start − a < −w

Similarly, every at end timed condition p can be translated into an equivalent over all
timed condition by replacing the scheduling constraint
_

w∈W (p)

¡¡

¢ ¡ +
¢¢
+
−
+
a+
∧ a − a+
,
start − a < −w
start < w

forcing a+ to occur during one or more time windows, with
_

w∈W (p)

¡¡

¢ ¡ +
¢¢
+
−
−
+
a+
.
start − a < −w + Dur(a) ∧ a − astart < w

Clearly, this translation of the timed conditions of each domain action into a single timed
precondition for the action can be accomplished by a preprocessing step in polynomial time.
Figure 2 shows an example. Assume that action a has duration 20 and timed conditions
p of type at start, q of type at end and r of type over all. Let [0, 50) and [100, 150) be
the time windows of p, [35, 80) the time window of q, and finally [40, 60) and [120, 180) the
time windows of r. We can compile these timed conditions into a new timed condition x
with the time window [40, 60).
6. Note that for timed conditions of type at start and at end we need to use “<” instead of “≤”. However,
the properties and algorithms for STPs can be easily generalized to STPs extended with <-constraints
(e.g., Gerevini & Cristani, 1997).

194

An Approach to Temporal Planning and Scheduling

Solve-DTP(X, S)
Input: The set X of meta-variables in the meta CSP of a DTP, a partial solution S of the meta CSP;
Output: Either a solution of the meta CSP or fail.
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.

if X = ∅ then stop and return S;
x ← SelectVariable(X); X 0 ← X − {x};
while D(x) 6= ∅ do
d ← SelectValue(D(x));
S 0 ← S ∪ {x ← d}; D(x) ← D(x) − {d};
D0 (x) ← D(x); /* Saving the domain values */
if ForwardCheck-DTP(X 0 , S 0 ) then
Solve-DTP(X 0 , S 0 );
D(x) ← D 0 (x); /* Restoring the domain values */
return fail; /* backtracking */

ForwardCheck-DTP(X, S)
Input: The set X of meta-variables, a (partial) solution S;
Output: Either true or false.
1.
2.
3.
4.
5.
6.

forall x ∈ X do
forall d ∈ D(x) do
if not Consistency-STP(S ∪ {x ← d}) then
D(x) ← D(x) − {d};
if D(x) = ∅ then return false; /* dead-end */
return true.

Figure 3: Basic algorithm for solving a DTP. D(x) is a global variable whose value is the
current domain of the meta-variable x. Consistency-STP(S) returns true, if the
STP formed by the variable values in the (partial) solution S has a solution, false
otherwise.

As observed by Stergiou and Kourbarakis (2000) and Tsamardinos and Pollack (2003),
a DTP can be seen as a “meta CSP”: the variables of the meta CSP are the constraints
of the original CSP, and the values of these (meta) variables are the disjuncts forming
the constraints of the original CSP. The constraints of the meta CSP are not explicitly
stated. Instead, they are implicitly defined as follows: an assignment θ of values to the
meta-variables satisfies the constraints of the meta CSP iff θ forms a consistent STP (an
induced STP of the DTP). A solution of the meta CSP is a complete induced STP of the
DTP.
Figure 3 shows an algorithm for solving the meta CSP of a DTP (Tsamardinos &
Pollack, 2003), which is a variant of the forward-checking backtracking algorithm for solving
general CSPs. By appropriately choosing the next meta-variable to instantiate (function
SelectVariable) and its value (function SelectValue), we can show that the algorithm finds a
solution with no backtracking (if one exists). Moreover, by a simple modification of Solve195

Gerevini, Saetti & Serina

DTP, we can derive an algorithm that is backtrack free even when the input meta CSP has
no solution. This can be achieved by exploiting the information in the LA-graph A of the
TDA-graph to decompose its DTP D into a sequence of “growing DTPs”
D1 ⊂ D2 ⊂ ... ⊂ Dlast = D
where (i) last is the number of the levels in A, (ii) the variables Vi of Di (i = 1..last) are
all the variables of D corresponding to the action nodes in A up to level i, and (iii) the
constraints of Di are all the constraints of D involving only the variables in Vi . E.g., for the
−
+
−
+
−
+
DTP of Figure 1, the point variables of D3 are a+
start , a1 , a1 , a2 , a2 , a3 , a3 , and the set
of constraints D3 is
−
+
−
+
−
+
−
+
−
{ a+
1 − a3 ≤ 0, a2 − a3 ≤ 0, a1 − a1 = 50, a2 − a2 = 70, a3 − a3 = 15,
−
+
+
+
−
+
+
((a+
start −a3 ≤ −25) ∧ (a3 −astart ≤ 50)) ∨ ((astart −a3 ≤ −75) ∧ (a3 −astart ≤ 125))}.

From the decomposed DTP, we can derive an ordered partition of the set of metavariables in the meta CSP of the original DTP
X = X1 ∪ X2 ∪ ... ∪ Xlast ,
where Xi is the set of the meta-variables corresponding to the constraints in Di − Di−1 ,
if i > 1, and in D1 otherwise. This ordered partition is used to define the order in which
SelectVariable chooses the next variable to instantiate, which is crucial to avoid backtracking. Specifically, every variable with a single domain value (i.e., an ordering constraint,
a duration constraint, or a scheduling constraint with only one time window) is selected
before every variable with more than one possible value (i.e., a scheduling constraint with
more than one time window); moreover, if xi ∈ Xi , xj ∈ Xj and i < j, then xi is selected
before xj .
In order to avoid backtracking, the order in which SelectValue chooses the value for a
meta-variable is very important as well: given a meta-variable with more than one value
(time window) in its current domain, we choose the value corresponding to the earliest
available time window. E.g., if the current domain of the selected meta-variable with m
possible values is
[ ©¡

i=1..m

¢ ¡ +
¢ª
−
+
+
−
a+
,
start − a ≤ −wi ∧ a − astart ≤ wi

then SelectValue chooses the j-th value such that |wj− | < |wh− |, for every h ∈ {1, ..., m},
j ∈ {1, ..., m}, h 6= j.
In the following we give a simple example illustrating the order in which SelectVariable
and SelectValue select the meta-variables and their meta-values, respectively. Consider the
TDA-graph in Figure 1 with the additional time window [150, 200) for the timed precondition p of a3 . The DTP of the extended TDA-graph has six meta-variables (x1 , x2 , . . . , x6 ),
whose domains (the disjuncts of the corresponding constraints of the original CSP) are:
−
x1 : {a+
1 − a3 ≤ 0}
+
−
x2 : {a2 − a3 ≤ 0}

196

An Approach to Temporal Planning and Scheduling

x3 :
x4 :
x5 :
x6 :

−
{a+
1 − a1 = 50}
+
−
{a2 − a2 = 70}
−
{a+
3 − a3 = 15}
+
+
+
+
−
+
+
{(astart − a−
3 ≤ −25) ∧ (a3 − astart ≤ 50), (astart − a3 ≤ −75) ∧ (a3 − astart ≤ 125),
+
−
+
+
(astart − a3 ≤ −150) ∧ (a3 − astart ≤ 200)}.

By exploiting the level structure of the TDA-graph, we derive an ordered partition of the
meta-variables formed by the following sets:
X1 = {x3 }, X2 = {x4 }, X3 = {x1 , x2 , x5 , x6 }.
Since x3 belongs to X1 while x4 belongs to X2 , SelectVariable selects x3 before selecting
x4 . Similarly, the function selects x4 before the meta-variables in X3 . When the algorithm
instantiates x6 , the first meta-value of x6 (i.e., the first time window of the timed precondition of a3 ) has been removed from its domain by forward checking, and SelectValue selects
−
+
+
+
−
+
+
(a+
start − a3 ≤ −75) ∧ (a3 − astart ≤ 125) before (astart − a3 ≤ −150) ∧ (a3 − astart ≤ 200),
because the first meta-value corresponds to a time window starting at time 75, while the
second one corresponds to a time window starting at time 150.
By using these techniques for selecting the next meta-variable to instantiate and its
value, we can prove the following theorem.
Theorem 1 Given a DTP D for a TDA-graph, if the meta CSP X of D is solvable, then
Solve-DTP finds a solution of X with no backtracking. Moreover, this solution is an optimal
induced STP of D for a−
end .
Proof. The proof has two key points: the way meta-variables are selected and instantiated
by SelectVariable and SelectValue, respectively; the particular type of constraints of D, in
which all disjunctive constraints have a specific form encoding a set of disjoint time windows,
and, by construction of D, we have
±
∀j ¬∃i such that i < j and Ω |= a±
j < ai ,

(1)

±
where Ω is the set of ordering constraints and duration constraints in D, and a ±
i (aj ) is
an endpoint of ai (aj ). Because of property (1), Ω cannot imply any restriction on the
maximum distance between an endpoint of ai and endpoint of aj (while, of course, there
can be a lower bound on this distance). I.e., for any positive quantity u we have
±
∀j ¬∃i such that i < j and Ω |= (a±
j − ai ≤ u).

(2)

Let assume that SelectVariable chooses a meta-variable x that cannot be consistently
instantiated to a value in D(x) (and this means that we have reached a backtracking point).
We show that this cannot be the case.
SelectVariable chooses the meta-variables of the STP-constraints of D before any metavariable of a scheduling constraint with more than one value (time window). Let X s be
the set of the meta-variables associated with the scheduling constraints in D. We have
that x must be a meta-variable in X s , because we are assuming that the meta CSP X is
solvable. The use of the forward checking subroutine guarantees that at least one value of x
is consistent with respect to the meta-variables that are instantiated in the current partial
197

Gerevini, Saetti & Serina

solution S. Hence, it should be the case that at step 7 of Solve-DTP ForwardCheck-DTP
returns false for every value d (time window) in D(x), i.e., that for every d ∈ D(x) there
exists another uninstantiated meta-variable x0 ∈ X s such that, for every d0 ∈ D(x0 ), the
check Consistency-STP(S ∪ {x0 ← d0 }) executed by the forward checking subroutine returns
false. However, if X has a solution (D is consistent), this cannot be the case because
(i) the value chosen by SelectValue to instantiate x and the previously instantiated metavariables (step 4) is the earliest available time window in the current domain of the
meta-variable under consideration, which is a “least commitment assignment”, and
(ii) we have at most one scheduling constraint (meta-variable in X s ) for each level of the
TDA-graph.
Let a0 be the action constrained by the scheduling constraint associated with x 0 . Since
SelectVariable selects x before x0 , by (ii) we have that a0 is at a level following the level of the
action constrained by the scheduling constraint associated with x. Thus, by property (2),
we have that if x0 could not be instantiated, then this would be because every time window
of a0 constrains a0 to start “too early”: the current partial solution of X augmented with
any of the possible values of x implies that the start time of a0 should be after the end of
the last time window of a0 . But then, (i) and the assumption that X is solvable guarantee
that this cannot be the case.
Moreover, since the value of every instantiated meta-variable is propagated by forward
checking to the unassigned variables, we have that the first value assigned to any metavariable is the same value assigned to that variable in the solution found for the CSP (if
any) – it is easy to see that if the first value chosen by SelectValue(D(x)) is not feasible
(ForwardCheck-DTP(X 0 , S 0 ) returns false), then every other next value chosen for x is not
feasible.
Finally, since the value chosen by SelectValue for a meta-variable corresponds to the
earliest available window in the current domain of the meta-variable, it follows that the
solution computed by the algorithm is a complete optimal induced STP of D for a −
end . 2
As a consequence of the previous theorem, if Solve-DTP performs backtracking (step 10),
then the input meta CSP has no solution. Thus, we can obtain a general backtrack-free
algorithm for the DTP of any TDA-graph by simply replacing step 10 with
10. stop and return fail.
The correctness of the modified algorithm, which we called Solve-DTP+ , follows from
Theorem 1. The next theorem states that the runtime complexity of Solve-DTP + is polynomial.
Theorem 2 Given a TDA-graph G with DTP D, Solve-DTP+ processes the meta CSP
corresponding to D in polynomial time with respect to the number of action nodes in G and
the maximum number of time windows in a scheduling constraint of D. 7
7. It should be noted that here our main goal is to give a complexity bound that is polynomial. The use of
improved forward checking techniques (e.g., Tsamardinos & Pollack, 2003) could lead to a complexity
bound that is lower than the one given in the proof of the theorem.

198

An Approach to Temporal Planning and Scheduling

Proof. The time complexity depends on the number of times ForwardCheck-DTP is executed, and on its time complexity. D contains a linear number of variables with respect
to the number n of domain action nodes in the LA-graph of the TDA-graph, O(n 2 ) ordering constraints, and O(n) duration constraints and scheduling constraints. Hence, the
meta CSP of D has O(n2 ) meta-variables (one variable for each constraint of the original
CSP). Let ω be the maximum number of time windows in a scheduling constraint of D.
ForwardCheck-DTP is executed at most ω times for each meta-variable x, i.e., O(ω ·n 2 ) times
in total. Consistency-STP decides the satisfiability of an STP involving O(n) variables, which
can be accomplished in O(n3 ) time (Dechter et al., 1991; Gerevini & Cristani, 1997). (Note
that the variables of the STP that is processed by Consistency-STP are the variables of the
original CSP, i.e., they are the starting time and the end time of the actions in the plan.)
Finally, Consistency-STP is run O(ω · n2 ) times during each run of ForwardCheck-DTP. It
follows that the runtime complexity of Solve-DTP+ is O(ω 2 · n7 ). 2
By exploiting the structure of the temporal constraints forming the DTP of a TDAgraph, we can make the following additional changes to Solve-DTP+ improving the efficiency
of the algorithm.
• Instead of starting from an empty assignment S (no meta-variable is instantiated),
initially every meta-variable associated with an ordering constraint or with a duration
constraint is instantiated with its value, and X contains only meta-variables associated
with the scheduling constraints. As observed in the proof of Theorem 1, if the meta
CSP is solvable, the values assigned to the meta-variables by the initial S form a
consistent STP.
• Forward checking is performed only once for each meta-variable. This is because in
the proof of Theorem 1 we have shown that, if the meta CSP is solvable, then the
first value chosen by SelectValue should be feasible (i.e., ForwardCheck-DTP returns
true). Thus, if the first value is not feasible, we can stop the algorithm and return fail
because the meta CSP is not solvable. Moreover, we can omit steps 6 and 9 which
save and restore the domain values of the meta-variables.
• Finally, the improved algorithm can be made incremental by exploiting the particular
way in which we update the DTP of the TDA-graph during planning (i.e., during the
search of a solution TDA-graph described in the next section). As described in the
next section, each search step is either an addition of a new action node to a certain
level l, or the removal of an action node from l. In both cases, it suffices to recompute
the sub-solution for the meta-variables in the subsets Xl , Xl+1 , ..., Xlast . The values
assigned to the other meta-variables is the same as the assignment in the last solution
computed before updating the DTP, and it is part of the input of the algorithm.
Moreover, in order to use the local search techniques described in the next section, we
need another change to the basic algorithm: when the algorithm detects that X has no
solution, instead of returning failure, (i) it keeps processing the remaining meta-variables,
and (ii) when it terminates, it returns the (partial) induced STP Si formed by the values
assigned to the meta-variables. The optimal solution of Si defines the T -assignment of the
TDA-graph.
199

Gerevini, Saetti & Serina

In the next section, SG denotes the induced STP for the DTP of a TDA-graph G computed by our method.

3. Local Search Techniques for TDA-Graphs
A TDA-graph hA, T , P, Ci can contain two types of flaw: unsupported precondition nodes
of A, called propositional flaws, and action nodes of A that are not scheduled by T , called
temporal flaws. If a level of A contains a flaw, we say that this level is flawed. For example,
if the only time window for p in the TDA-graph of Figure 1 were [25, 50), then level 3 would
be flawed, because the start time of a3 would be 70, which violates the scheduling constraint
for a3 imposing that this action must be executed during [25, 50).
A TDA-graph with no flawed level represents a valid plan and is called a solution graph.
In this section, we present new heuristics for finding a solution graph in a search space of
TDA-graphs. These heuristics are used to guide a local search procedure, called Walkplan,
that was originally proposed by Gerevini and Serina (1999) and that is the heart of the
search engine of our planner.
The initial TDA-graph contains only astart and aend . Each search step identifies the
neighborhood N (G) (successor states) of the current TDA-graph G (search state), which is
a set of TDA-graphs obtained from G by adding a helpful action node to A or removing a
harmful action node from A in an attempt to repair the earliest flawed level of G. 8
In the following, for the sake of brevity when we refer to an action node of a TDA-graph,
we are implicitly referring to an action node of the LA-graph of a TDA-graph. Similarly
for the level of a TDA-graph. Moreover, we remind the reader that a l denotes the action
at level l, while la denotes the level of action a.
Definition 4 Given a flawed level l of a TDA-graph G, an action node is helpful for l iff
its insertion into G at a level i ≤ l would remove a propositional flaw at l.
Definition 5 Given a flawed level l of a TDA-graph G, an action node at a level i ≤ l
is harmful for l iff its removal from G would remove a propositional flaw at l, or would
decrease the T -value of al , if al is unscheduled.
Examples of helpful action node and harmful action node
An action node representing an action with effect p1 is helpful for level 3 of the TDA-graph
of Figure 1 if it is added at level 2 or 3 (bear in mind that the insertion of an action node at
level 3 determines an expansion of the TDA-graph postponing a3 to level 4; more details are
given at the end of the examples). Action node a3 of Figure 1 is harmful for level 3, because
its precondition node p1 is unsupported; action node a1 is harmful for level 3, because it
blocks the no-op propagation of p1 at level 1, which would support the precondition node p1
at level 3. Moreover, assuming W (p) = {[25, 50)}, a3 is unscheduled in the plan represented
by the LA-graph. Action node a2 is harmful for level 3, because the removal of a2 from
8. We have designed several flaw selection strategies that are described and experimentally evaluated in a
recent paper (Gerevini, Saetti, & Serina, 2004). The strategy preferring flaws at the earliest level of the
graph tends to perform better than the others, and so it is used as the default strategy of our planner.
More details and a discussion about this strategy are given in the aforementioned paper.

200

An Approach to Temporal Planning and Scheduling

A would decrease the temporal value of a3 . On the contrary, a1 is not harmful for level 3,
because its removal would not affect the possible scheduling of a3 . Notice that an action
node can be both helpful and harmful: a3 is harmful for level 3, and it is helpful for the
goal level (because it supports the precondition node p10 of aend ).
When we add an action node to a level l that is not empty, the LA-graph is extended
by one level, all action nodes from l are shifted forward by one level (i.e., they are moved
to their next level), and the new action is inserted at level l . Similarly, when we remove
an action node from level l, the graph is “shrunk” by removing level l. Some additional
details about this process are given in another paper (Gerevini et al., 2003). Moreover, as
pointed out in the previous section, the addition (removal) of an action node a requires us
to update the DTP of G by adding (removing) the appropriate ordering constraints between
a and other actions in the LA-graph of G, the duration constraint of a, and the scheduling
constraint of a (if any). From the updated DTP, we can use the method described in the
previous section to revise T , and to compute a possibly new schedule of the actions in G
(i.e., an optimal solution of SG ).
The elements in N (G) are evaluated using a heuristic evaluation function E consisting
of two weighted terms, estimating their additional search cost and temporal cost, i.e., the
number of search steps required to repair the new flaws introduced, and their contribution
to the makespan of the represented plan, respectively. An element of N (G) with the lowest
combined cost is then selected using a “noise parameter” randomizing the search to escape
from local minima (Gerevini et al., 2003). In addition, in order to escape local minima, the
new version of our planner uses a short tabu list (Glover & Laguna, 1997). In the rest of
this section, we will focus on the search cost term of E. The techniques that we use for the
evaluation of the temporal cost and the (automatic) setting of the term weights of E are
similar to those that we introduced in a previous work (Gerevini et al., 2003).
The search cost of adding a helpful action node a to repair a flawed level l of G is
estimated by constructing a relaxed temporal plan π achieving
(1) the unsupported precondition nodes of a, denoted by Pre(a)
(2) the propositional flaws remaining at l after adding a, denoted by Unsup(l), and
(3) the supported precondition nodes of other action nodes in G that would become
unsupported by adding a, denoted by Threats(a).
Moreover, we estimate the number of additional temporal flaws that the addition of a and
π to G would determine, i.e., we count the number of
(I) action nodes of G that would become unscheduled by adding a and π to G,
(II) the unsatisfied timed preconditions of a, if a is unscheduled in the TDA-graph extended with a and π,
(III) the action nodes of π with a scheduling constraint that we estimate cannot be satisfied
in the context of π and of G.
The search cost of adding a to G is the number of actions in π plus (I), (II) and (III),
which are new terms of the heuristic evaluation. Note that the action nodes of (I) are
201

Gerevini, Saetti & Serina

aend (90)

Goal level
(90)

p9

p8
[15]

a3 (75)

(70)

p8

Action
b1
b2
b3
b4

(70)

(70)

p10
p

(70)

p9

(70)

p9

p5

(70)

(70)

(70)

(50)

p1

p7

Est lower bound
0
0
15
50

(35)
[5]

q

p9

p8

p6

Relaxed Plan π

Level 3
(−)

Action
b1
b2
b3
b4

N um acts
0
0
1
5

anew (30)

(20)

p5 (50)

p1

p3

Level 2
p1
p1

p5
mutex

p1

(0)

[50]

a1 (0)

Level 1
(0)

(50)

(0)

p2

mutex

p6

p3
p3
p3

(0)

(0)

a2 [70]
(0)
(−)

p4
p4
p4

(30)

q1

q2
[10]
(20)

(0)

b3

(20)

q3

(0)
[15]

p5

(0)

b1 (0)

I
astart (0)

(0)

p3

[100]

q
p4

b4 (50)

(0)

q4

p5

(50)

[20]

b2 (0)

p4

(0)

(50)

p5

Figure 4: An example of relaxed temporal plan π. Square nodes represent action nodes,
while the other nodes represent fact nodes; solid nodes correspond to nodes in
A ∪ {anew }; dotted nodes correspond to the precondition nodes and action nodes
that are considered during the construction of π; the gray dotted nodes are those
selected for their inclusion in π. Action nodes are marked by the duration of
the represented actions (in square brackets) and by their estimated start time (in
round brackets). The meaning of Num acts is described in the text; the lower
bounds on the earliest action start times (Est lower bound) are computed by the
algorithm in Appendix A.

those that would have to be ordered after a (because a is used to achieve one of their
preconditions, or these action nodes are mutex with a) and that, given the estimated end
time of π and the duration of a, would excessively increase their start time. In (II) we
consider the original formulation of the timed preconditions of a (i.e., the formulation before
their possible compilation into one “merged” new precondition, as discussed in Section 2.3).
Finally, to check the scheduling constraint of an action in π, we consider the estimated end
time of the relaxed subplan of π used to achieve the preconditions of this action.
Example of relaxed temporal plan and additional temporal flaws (I–III)
Figure 4 gives an example of π for evaluating the addition of anew at level 2 of the LAgraph on the left side of the figure (the same graph as the one used in Figure 1), which is a
202

An Approach to Temporal Planning and Scheduling

RelaxedTimePlan(G, I, A)
Input: A set of goal facts (G), an initial state for the relaxed plan (I), a set of reusable actions (A);
Output: The set of actions Acts forming a relaxed plan for G from I and the earliest time when all
facts in G can be achieved.
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.

S
Acts ← A; F ← a∈Acts Add (a);
t ← M AX {T (g) | g ∈ G ∩ F or g ∈ G ∩ I};
G ← G − I;
while G − F 6= ∅
g ← a fact in G − F ;
b ← BestAction(g);
hA, t0 i ← RelaxedTimePlan(Pre(b), I, Acts);
T (b) ← ComputeEFT (b, t0 );
t ← M AX{t, T (b)};
forall f ∈ Add(b) ©do
ª
T (f ) ← M IN T (f ),ST (b) + Dur(b) ;
Acts ← A ∪ {b}; F ← a∈Acts Add (a);
return hActs, ti.

Figure 5: Algorithm for computing a relaxed temporal plan. ComputeEFT (b, t 0 ) returns
the estimated earliest finishing time τ of b that is consistent with the scheduling
constraint of b (if any), and such that t0 + Dur(b) ≤ τ (for an example see
Appendix A). Add (a) denotes the set of the positive effects of a.

helpful action node for the unsupported precondition p6 . The goals of π are the unsupported
preconditions q1 and q2 of anew ; while the initial state I of π is formed by the fact nodes that
are supported at level 2. The actions of π are anew , b2 and b3. The numbers in the name
of the actions and facts of the relaxed plan indicate the order in which RelaxedTimePlan
considers them. The estimated start time and end time of b3 are 20 and 30, respectively.
Assume that the timed precondition q of anew has associated with it the time window [0, 20).
Concerning point (I), there is no action node of G that would become unscheduled by adding
anew and π to G. Concerning point (II), anew is unscheduled and has one timed precondition
that is unsatisfied (q). Concerning point (III), we have that b3 cannot be scheduled in the
context of π and the current TDA-graph G. Finally, since π contains three actions, and the
sum of (I), (II) and (III) is 2, we have that the search cost of adding anew to G at level 2
is 5.
The evaluation of a TDA-graph derived by removing a harmful action node a for a
flawed level l is similar, with π achieving
• the precondition nodes supported by a that would become unsupported by removing
a and
• when la < l, the unsupported precondition nodes at level l that do not become supported by removing a.
203

Gerevini, Saetti & Serina

Regarding the second point, note that if l = la , then all flaws at l are eliminated because,
when we remove an action, we also (automatically) remove all its precondition nodes. While,
when la < l, the removal of a could leave some flaws at level l.
Plan π is relaxed in the sense that its derivation ignores the possible (negative) interference between actions in π, and the actions in π may be unscheduled. The derivation of
π takes into account the actions already in the current partial plan (the plan represented
by the TDA-graph G). In particular, the actions of the current plan are used to define an
initial state I for π, which is obtained by applying the actions of G up to level la −1, ordered
according to their corresponding levels. Moreover, each fact f in I is marked by a temporal
value, T (f ), corresponding to the time when f becomes true (and remains so in π) in the
current subplan formed by the actions up to level la − 1.
The relaxed plan π is constructed using a backward process, called RelaxedTimePlan (see
Figure 5), which is an extension of the RelaxedPlan algorithm that we proposed in a previous
work (Gerevini et al., 2003). The algorithm outputs two values: a set of actions forming
a (sub)relaxed plan, and its estimated earliest finishing time (used to defined the temporal
cost term of E). The set of actions Acts forming π is derived by running RelaxedTimePlan
twice: first with goals Pre(a), initial state I and an empty set of reusable actions; then with
goals Unsup(l ) ∪ T hreats(a), initial state I − Threats(a) ∪ Add (a), and a set of reusable
actions formed by the actions computed by the first run plus a.
The main novelty of the extended algorithm for computing π concerns the choice of the
actions forming the relaxed plan. The action b chosen to achieve a (sub)goal g is an action
minimizing the sum of
• the estimated minimum number of additional actions required to support its propositional preconditions from I (Num acts(b, I)),
• the number of supported precondition nodes in the LA-graph that would become
unsupported by adding b to G (Threats(b)),
• the number of timed preconditions of b that we estimate would be unsatisfied in G
extended with π (TimedPre(b));
• the number of action nodes scheduled by T that we estimate would become unscheduled when adding b to G (TimeThreats(b)).
More formally, the action chosen by BestAction(g) at step 6 of RelaxedTimePlan to
achieve a (sub)goal g is an action satisfying
¾
½
0
0
0
0
ARGMIN Num acts(a , I) + |Threats(a )| + |TimedPre(a )| + |TimeThreats(a )| ,
{a0 ∈Ag }

where Ag = {a0 ∈ O | g ∈ Add (a0 ), O is the set of all the domain actions whose preconditions
are reachable from I}.
Num acts(b, I) is computed by the algorithm given in Appendix A; Threats(b) is computed as in our previous method for deriving π (Gerevini et al., 2003), i.e., by considering
the negative interactions (through mutex relations) of b with the precondition nodes that
are supported at levels after al ; TimedPre(b) and TimeThreats(b) are new components of
the action selection method, and they are computed as follows.
204

An Approach to Temporal Planning and Scheduling

In order to compute TimedPre(b), we estimate the earliest start time of b (Est(b)) and
the earliest finishing time of b (Ef t(b)). Using these values, we count the number of the
timed preconditions of b that cannot be satisfied. Ef t(b) is defined as Est(b) + Dur(b),
while Est(b) is the maximum over
• a lower bound on the possible earliest start time of b (Est lower bound of b), computed
by the reachability analysis algorithm given in Appendix A;
• the T -values of the action nodes ci in the current TDA-graph G, with i < la , that are
−
mutex with b because the addition of b to G would occur the addition of c+
i −b ≤ 0
to the DTP of G;
• the maximum over an estimated lower bound on the time when all the preconditions of
b are achieved in the relaxed plan; this estimate is computed from the causal structure
of the relaxed plan, the duration and scheduling constraints of its actions, and the
T -values of the facts in the initial state I.
Example of “TimedPre”
In the example of Figure 4, the estimated start time of b3 is the maximum between 15,
which is the Est lower bound of b3, and 20, which is the maximum time over the estimated
times when the preconditions of b3 are supported (p4 is supported in the initial state of π
at time 0, while q3 is supported at time 20). Notice that a1 is not mutex with b3, and so the
second point in the definition of Est(b3) does not apply here. Since the estimated earliest
start time of b3 is 20 and the duration of b3 is 10, Ef t(b3) = 20 + 10. Thus, if we assume
that q has associated with it the time window [0,20), then the timed precondition q of b3
cannot be scheduled, i.e., q ∈ TimedPre(b3).
In order to compute TimeThreats(b), we use the following notion of time slack between
action nodes.
Definition 6 Given two action nodes ai and aj of a TDA-graph hA, T , P, Ci such that
−
C |= a+
i < aj , Slack(ai , aj ) is the maximum time by which the T -value of ai can be
consistently increased in SG without violating the time window chosen for scheduling aj .
In order to estimate whether an action b is a time threat for an action node a k in the
current TDA-graph extended with the action node a that we are adding for repairing level
l (l < k), we check if
∆(πb , a) > Slack(a, ak )
holds, where πb is the portion of the relaxed plan computed so far, and ∆(πb , a) is the
estimated delay that adding the actions in πb to G would cause to the start time of a.
Examples of time slack and “TimeThreat”
The slack between anew and a3 in the TDA-graph of Figure 4 extended with anew is 35,
because even if anew started at 35, a3 could still be executed during the time window
[75, 125) (imposed by the timed precondition p); while if anew started at 35 + ², then a3
would finish at 125+² (determined by summing the start time of anew , Dur(anew ), Dur(a2 ),
205

Gerevini, Saetti & Serina

and Dur(a3 )), and so the scheduling constraint of a3 would be violated. Assume that we
are evaluating the inclusion of b4 in the relaxed plan of Figure 4 for achieving q2. We have
∆(πb4 , anew ) = 150,
i.e. the estimated delay that the portion of the plan formed by b4 would add to the end
time of anew is 150. Since the slack between anew and a3 is 35,
Slack(anew , a3 ) < ∆(πb4 , anew ),
and so a3 ∈ TimeThreats(b4). On the contrary, since
Slack(anew , a3 ) > ∆(πb3 , anew ) = 30
we have that a3 6∈ TimeThreats(b3).
To conclude this section, we observe that the way we consider scheduling constraints
during the evaluation of the search neighborhood has some similarity with a well-known
technique used in scheduling. For example, suppose that we are evaluating the TDA-graphs
obtained by adding a helpful action node a to one among some alternative possible levels of
the graph, and that the current TDA-graph contains another action node c which is mutex
with a. If the search neighborhood contains two TDA-graphs corresponding to (1) “adding
a to a level before lc ” and (2) “adding a to a level after lc ”, and (1) violates less scheduling
constraints than (2), then, according to points (I)–(III), (1) is preferred to (2). A similar
heuristic method, called constraint-based analysis, has been proposed by Erschler, Roubellat
and Vernhes (1976) to decide whether an action should be scheduled before or after another
conflicting action, and it has been also used in other scheduling work for guiding the search
toward a consistent scheduling of the tasks involved in the problem (e.g., Smith & Cheng,
1993).

4. Experimental Results
We implemented our approach in a planner called lpg-td, which obtained the 2nd prize in
the metric-temporal track (“satisficing planners”) of the 4th International Planning Competition (IPC-4). lpg-td is an incremental planner, in the sense that it produces a sequence
of valid plans each of which improves the quality of the previous ones. Plan quality is
measured using the metric expression that is specified in the planning problem description.
The incremental process of lpg-td is described in another paper (Gerevini et al., 2003).
Essentially, the process iterates the search of a solution graph with an additional constraint
on the lower bound of the plan quality, which is determined by the quality of the previously
generated plans. lpg-td is written in C and is available from http://lpg.ing.unibs.it.
In this section, we present the results of an experimental study with two main goals:
• testing the efficiency of our approach to temporal planning with predictable exogenous
events by comparing the performance of lpg-td and other recent planners that at
IPC-4 attempted the benchmark problems involving timed initial literals (Edelkamp,
Hoffmann, Littman, & Younes, 2004);
206

An Approach to Temporal Planning and Scheduling

Planner

lpg-td
sgplan
p-mep
crikey
lpg-ipc3
downward (diag)
downward
marvin
yahsp
macro-ff
fap
roadmapper
tilsapa
optop

Solved
845
1090
98
364
306
380
360
224
255
189
81
52
63
4

Attempted
1074
1415
588
594
594
432
432
432
279
332
193
186
166
50

Success ratio
79%
77%
17%
61%
52%
88%
83%
52%
91%
57%
42%
28%
38%
8%

Planning capabilities at IPC-4
Propositional + DP, Metric-Temporal +TIL
Propositional + DP, Metric-Temporal +TIL
Propositional, Metric-Temporal +TIL
Propositional, Metric-Temporal
Propositional, Metric-Temporal
Propositional + DP
Propositional + DP
Propositional + DP
Propositional
Propositional
Propositional
Propositional
TIL
TIL

Table 1: Number of problems attempted/solved and success ratio of the (satisficing) planners that took part in IPC-4. “DP” means derived predicates; “TIL” means timed
initial literals; “Propositional” means STRIPS or ADL. The planning capabilities are the PDDL2.2 features in the test problems attempted by each planner at
IPC-4.

• testing the effectiveness of the proposed temporal reasoning techniques integrated
into the planning process to understand, in particular, their impact on the overall
performance of the system, and to compare them with other existing techniques.
For the first analysis, we consider the test problems of the variant of the IPC-4 metrictemporal domains involving timed initial literals. A comparison of lpg-td and other IPC-4
planners considering all the variants of the IPC-4 metric-temporal domains is given in
Appendix B. Additional results are available from the web site of our planner.
For the second experiments, we use new domains and problems obtained by extending
two well-known benchmark domains (and the relative problems) from IPC-3 with timed
initial literals (Long & Fox, 2003a).9
All tests were conducted on an Intel Xeon(tm) 3 GHz, 1 Gbytes of RAM. We ran lpg-td
with the same default settings for every problem attempted.
4.1 LPG-td and Other IPC-4 Planners
In this section, we use the official results of IPC-4 to compare the performance of lpg-td
with those of other planners that took part in the competition. The performance of lpg-td
corresponds to a single run. The CPU-time limit for the run was 30 minutes, after which
termination was forced. lpg-td.s indicates the CPU-time required by our planner to derive
the first plan; lpg-td.bq indicates the best quality plan found within the CPU-time limit.
9. For a description of the IPC-4 domains and of the relative variants, the reader can visit the
official web site of IPC-4 (http://ls5-www.cs.uni-dortmund.de/∼edelkamp/ipc-4/index.html).
The extended versions of the IPC-3 domains used in our experiments are available from
http://zeus.ing.unibs.it/lpg/TestsIPC3-TIL.tgz.

207

Gerevini, Saetti & Serina

Before focusing our analysis on the IPC-4 domains involving timed initial literals, in
Table 1 we give a very brief overview of all the results of the IPC-4 (satisficing) planners, in
terms of planning capabilities and problems attempted/solved by each planner. The table
summarizes the results for all the domain variants of IPC-4. lpg-td and sgplan (Chen, Hsu,
& Wah B., 2004) are the only planners supporting all the major features of PDDL2.1 and
PDDL2.2. Both planners have a good success ratio (close to 80%). downward (Helmert,
2004) and yahsp (Vidal, 2004) have a success ratio better than lpg-td and sgplan, but
they handle only propositional domains (downward supports derived predicates, while
yahsp does not). sgplan attempted more problems than lpg-td because it was also tested
on the “compiled version” of the variants with derived predicates and timed initial literals. 10
Moreover, lpg-td did not attempt the numerical variant of the two versions of the Promela
domain and the ADL variant of PSR-large, because they use equality in some numerical
preconditions or conditional effects, which currently our planner does not support.
Figure 6 shows the performance of lpg-td in the variants of three domains involving
predictable exogenous events with respect to the other (satisficing) planners of IPC-4 supporting timed initial literals: sgplan, p-mep (Sanchez et al., 2004) and tilsapa (Kavuluri
& U, 2004). In Airport (upper plots of the figure), lpg-td solves 45 problems over 50,
sgplan 43, p-mep 12, and tilsapa 7. In terms of CPU-time, lpg-td performs much better
than p-mep and tilsapa. lpg-td is faster than sgplan in nearly all problems (except
problems 1 and 43). In particular, the gap of performance in problems 21–31 is nearly
one order of magnitude. Regarding plan quality, the performance of lpg-td is similar to
the performance of p-mep and tilsapa, while, overall, sgplan finds plan of worse quality
(with the exception of problems 41 and 43, where sgplan performs slightly better, and the
easiest problems where lpg-td and sgplan perform similarly).
lpg-td and tilsapa are the only planners of IPC-4 that attempted the variant of
PipesWorld with timed initial literals (central plots of Figure 6). lpg-td solves 23 problems over 30, while tilsapa solves only 3 problems. In this domain variant lpg-td performs
much better than tilsapa.
In the “flaw version” of Umts (bottom plots of Figure 6), lpg-td solves all 50 problems,
while sgplan solves 27 problems (p-mep and tilsapa did not attempt this domain variant).
Moreover, lpg-td is about one order of magnitude faster than sgplan in every problem
solved. Compared to the other IPC-4 benchmark problems, the Umts problems are generally
easier to solve. In these test problems, the main challenge is finding plans of good quality.
Overall, the best quality plans of lpg-td are much better than sgplan plans, except for
the simplest problems where the two planners generate plans of similar quality. In the basic
version of Umts without flawed actions, sgplan solves all problems as lpg-td, but in terms
of plan quality lpg-td performs much better.
Figure 7 shows the results of the Wilcoxon sign-rank test, also known as the “Wilcoxon
matched pairs test” (Wilcoxon & Wilcox, 1964), comparing the performance of lpg-td and
the planners that attempted the benchmark problems of IPC-4 involving timed initial literals. The same test has been used by Long and Fox (2003a) for comparing the performance
10. Such versions were generated for planners that do not support these features of PDDL2.2. During the
competition we did not test lpg-td with the problems of the compiled domains because the planner
supports the original version of these domains. lpg-td attempted every problem of the (uncompiled)
IPC-4 domains that it could attempt in terms of the planning language it supports.

208

An Approach to Temporal Planning and Scheduling

Airport-Windows

Milliseconds
1e+07

LPG-td.s (45 solved)
P-MEP (12 solved)
SGPlan (43 solved)
TilSapa (7 solved)

1e+06

Airport-Windows

Makespan
1400

LPG-td.bq (45 solved)
P-MEP (12 solved)
SGPlan (43 solved)
TilSapa (7 solved)

1200

1000
100000
800
10000
600
1000
400
100

200

10

0
0

5

10

15

20

25

30

35

40

45

PipesWorldNoTankage-Deadlines

Milliseconds
1e+07

0

5

10

15

20

25

30

35

40

45

PipesWorldNoTankage-Deadlines

Makespan
30

LPG-td.bq (23 solved)
TilSapa (3 solved)

LPG-td.s (23 solved)
TilSapa (3 solved)
1e+06

25

100000

20

10000

15

1000

10

100

5

10

0
0

5

10

15

20

25

30

UmtsFlaw-Windows

Milliseconds
10000

0

5

10

15

20

25

30

UmtsFlaw-Windows

Makespan
1900

LPG-td.s (50 solved)
SGPlan (27 solved)

LPG-td.bq (50 solved)
SGPlan (27 solved)
1850
1800

1000

1750
1700
1650

100

1600
1550
1500

10

1450
0

5

10

15

20

25

30

35

40

45

50

0

5

10

15

20

25

30

35

40

45

Figure 6: CPU-time and plan quality of lpg-td, p-mep, sgplan, and tilsapa for three
IPC-4 domains with timed initial literals. On the x-axis we have the problem
names simplified by numbers. In the plots on the left, on the y-axis we have
CPU-milliseconds (logarithmic scale); in the plots on the right, on the y-axis we
have the plan makespan (the lower the better).
209

50

Gerevini, Saetti & Serina

lpg-td.s vs p-mep
5.841
< 0.001
45

lpg-td.bq vs p-mep
< 0.001
12

CPU-Time Analysis
lpg-td.s vs sgplan
3.162
(0.0016)
197

lpg-td.s vs tilsapa
10.118
< 0.001
136

Plan Quality Analysis
lpg-td.bq vs sgplan lpg-td.bq vs tilsapa
9.837
6.901
< 0.001
< 0.001
154
63

Figure 7: Results of the Wilcoxon test for the performance of lpg-td compared with other
IPC-4 (satisficing) planners in terms of CPU-times and plan quality for the benchmark problems with timed initial literals.

lpg-td.s

sgplan

tilsapa

p-mep

CPU-Time

lpg-td.bq

sgplan

tilsapa

p-mep

A

B:

A is consistently better than B

A

B:

A is better than B a
significant number of times
(confidence level 99.84%)

Plan Quality

Figure 8: Partial order of the performance of the IPC-4 (satisficing) planners according
to the Wilcoxon test for the benchmark problems with timed initial literals. A
dashed arrow indicates that the performance relationship holds with a confidence
level slightly less than 99.9%.

of the IPC-3 planners. For the CPU-time analysis, we consider all the problems attempted
by both the compared planners and solved by at least one of them (when a planner does
not solve a problem, the corresponding CPU-time is the IEEE arithmetic representation of
positive infinity). For the plan quality (makespan) analysis, we consider all the problems
solved by both the compared planners.
210

An Approach to Temporal Planning and Scheduling

In order to carry out the Wilcoxon test, for each planning problem we computed the
difference between the CPU-times of the two planners being compared, defining the samples
of the test for the CPU-time analysis. Similarly, for the test concerning the plan quality
analysis we computed the differences between the makespan of the plans generated by the
two planners. The absolute values of these differences are ranked by increasing numbers
starting from the lowest value. (The lowest value is ranked 1, the next lowest value is
ranked 2, and so on.) Then we sum the ranks of the positive differences, and we sum the
ranks of the negative differences. If the performance of the two planners is not significantly
different, then the number of the positive differences will be approximately equal to the
number of the negative differences, and the sum of the ranks in the set of the positive
differences will be approximately equal to the sum of the ranks in the other set. Intuitively,
the test considers a weighted sum of the number of times one planner performs better than
the other. The sum is weighted because the test uses the performance gap to assign a rank
to each performance difference.
Each cell in Figure 7 gives the result of a comparison between the performance of
lpg-td and another IPC-4 planner. When the number of the samples is sufficiently large,
the T-distribution used by the Wilcoxon test is approximatively a normal distribution.
Therefore, the cells of the figure contain the z-value and the p-value characterizing the
normal distribution. The higher the z-value, the more significant the difference of the
performance is. The p-value represents the level of significance in the performance gap.
We use a confidence level of 99.9%; hence, if the p-value is lower than 0.001, then the
performance of the compared planners is statistically different. When this information
appears on the left (right) side of the cell, the first (second) planner named in the title of
the cell performs better than the other planner.11 For the analysis comparing the CPUtime, the value under each cell is the number of the problems solved by at least one planner;
while for the analysis comparing the plan quality, it is the number of problems solved by
both the planners.
Figure 8 shows a graphical description of the relative performance of the IPC-4 satisficing
planners according to the Wilcoxon test for the benchmark problems with timed initial
literals. A solid arrow from a planner A to a planner B (or a cluster of planners B)
indicates that the performance of A is statistically different from the performance of B,
and that A performs better than B (every planner in B). A dashed arrow from A to B
indicates that A is better than B a significant number of times, but there is no significant
Wilcoxon relationship between A and B with a confidence level of 99.9% (on the other
hand, the relationship holds with a confidence level slightly less than 99.9%). The results
of this analysis say that lpg-td is consistently faster than tilsapa and p-mep, while it is
faster than sgplan a significant number of times. In terms of plan quality, lpg-td performs
consistently better than p-mep, sgplan and tilsapa.
Although lpg-td does not guarantee optimal plans, it is interesting to compare its
performance with the optimal planners that took part in IPC-4, especially to see how good
lpg-td’s plans are. Figure 9 shows the performance of lpg-td and the best results over the
results of all the other optimal IPC-4 planners (“AllOthers-Opt”) in the temporal variants
of Airport and Umts (without flawed actions). The plots for the plan quality (makespan)
11. The p-value in the cell comparing lpg-td and p-mep is omitted because the number of problems solved by
both lpg-td and p-mep is not high enough to approximate the T-distribution to a normal distribution.

211

Gerevini, Saetti & Serina

Airport-Time

Milliseconds
1e+07

Airport-Time

Makespan
1000

LPG-td.s (44 solved)
LPG-td.bq (44 solved)
AllOthers-Opt (21 solved)

LPG-td.s (44 solved)
LPG-td.bq (44 solved)
AllOthers-Opt (21 solved)

900

1e+06

800
700

100000

600
10000

500
400

1000

300
200

100

100
10

0
0

5

10

15

20

25

30

35

40

45

UMTS-Time

Milliseconds
1e+07

0

5

10

15

20

30

35

40

45

UMTS-Time

Makespan
900

LPG-td.s (50 solved)
LPG-td.bq (50 solved)
AllOthers-Opt (38 solved)

25

LPG-td.s (50 solved)
LPG-td.bq (50 solved)
AllOthers-Opt (38 solved)

850

1e+06
800
100000
750

10000

700

650
1000
600
100
550

10

500
0

5

10

15

20

25

30

35

40

45

50

0

5

10

15

20

25

30

35

40

45

50

Figure 9: Performance of lpg-td and the best over all the optimal planners of IPC-4
(AllOthers-Opt) in Airport-Time and Umts-Time: CPU-time in logarithmic scale
(left plots) and plan makespan (right plots). On the x-axis we have the problem
names simplified by numbers.

show that, in nearly every problem of these domains, the best quality plan found by lpg-td
is an optimal solution, and that the first plan found by lpg-td is generally a good solution.
The plots for the CPU-time show that lpg-td finds a plan much more quickly than any
optimal planner, and that the CPU-time required by lpg-td to find the best plan is often
lower than the CPU-time required by AllOthers-Opt (except for problems 12, 16, 18 and
20 of Airport). It should be noted that lpg-td.bq is the last plan over a sequence of
computed plans with increasing quality (and CPU-time). The intermediate plans in this
sequence could already have good quality. In particular, as shown by the plan quality plot
for Airport, the first plan (lpg-td.s) solving problem 12 has near-optimal quality, but it is
computed much more quickly than the lpg-td.bq plan and the AllOthers-Opt plan.
212

An Approach to Temporal Planning and Scheduling

Figure 10: Plan quality distance between the solutions found by lpg-td and the corresponding optimal solutions. On the x-axis, we have some classes of quality distance
(e.g., “10–25%” means that the plan generated by lpg-td is worse than the
optimal plan by a factor between 0.1 and 0.25). On the y-axis, we have the
percentage of solved problems for each of these classes.

Finally, Figure 10 gives the results of a more general analysis on the plan quality distance,
considering all metric-temporal and STRIPS variants of the IPC-4 domains. 12 The analysis
uses only the problems solved by at least one IPC-4 optimal planner. It is also important to
note that we consider only the plans generated by the incremental process of lpg-td using
no more CPU-time than the CPU-time required by the fastest optimal planner (AllOthersOpt). Overall, the results in Figure 10 provide significant empirical evidence supporting
the claim that often an incremental local search approach allows us to compute plans with
very good quality using less or no more CPU-time than an optimal approach. In particular,
the bars for the “0%–1%” class in the plot of the metric-temporal problems show that the
percentage of the test problems in which the best quality plan of lpg-td (lpg-td.bq) is
optimal or nearly optimal (i.e., plan quality is worse than optimal by a factor between
0 and 0.01, with 0 meaning no difference) is about 90%. Moreover, often the first plan
computed by lpg-td (lpg-td.s) has good quality: 60% of all these plans have quality that
is optimal or nearly optimal, and only about 25% of them have a quality that is worse than
the optimal by a factor greater than 0.5.
Interestingly, the plot on the right of Figure 10 shows similar results concerning the good
quality of lpg-td’s plans also for the STRIPS problems of IPC-4 (with a lower percentage
of the lpg-td.s’ plans that are in the 0%–1% class, and a slightly higher percentage of the
lpg-td.bq’s plans that are in the “> 50%” class).
4.2 Temporal Reasoning in LPG-td
We conducted two main experiments. The first was aimed at testing the performance
of lpg-td when the number of windows for the timed initial literals varies in problems
12. For the STRIPS problems, the plan quality metric is the number of the actions in the plan.

213

Gerevini, Saetti & Serina

having the same initial state and goals. The second experiment focused on our temporal
reasoning techniques with the main goals of empirically evaluating their performance, and
understanding their impact on the overall performance of lpg-td.
For these experiments we used two well-known IPC-3 domains, which were modified to
include timed initial literals: Rovers and ZenoTravel. The version of Rovers with timed
initial literals was obtained from the IPC-3 temporal version as follows. In the problem
specification, for each “waypoint”, we added a collection of pairs of timed initial literals of
the type
(at t1 (in sun waypoint0))
(at t2 (not (in sun waypoint0)))

where t1 < t2 . Each of these pairs defines a time window for the involved literal. In the
operator specification file, the “recharge” operator has the precondition
(over all (in sun ?w))

which imposes the constraint that the recharging actions are applied only when the rover is
in the sun (?w is the operator parameter representing the waypoint of the recharging action.)
The modified version of ZenoTravel was obtained similarly. In the problem specification,
for each city we added a collection of pairs of timed initial literals of the type
(at t1 (open-station city0))
(at t2 (not (open-station city0)))

and in the operator specification file, we added the timed precondition
(over all (open-station ?c))

to the “refuel” operator, where ?c is the operator parameter representing the city where
the refuel action is executed.
Given a planning problem Π and a collection of time windows Wφ for a timed literal φ, it
should be noted that, in general, the difficulty of solving Π is affected by three parameters:
the number of windows in Wφ , their size, and the way they are distributed on the time
line.13 We considered two methods for generating test problems taking account of these
parameters (Π indicates an original IPC-3 problem in either the Rovers or ZenoTravel
domain, and n indicates the number of windows in Wφ ):
(I) Let π be the best (shortest makespan) plan among those generated by lpg-td for
solving Π within a certain CPU-time limit, and t the makespan of π. The time
interval [0, t] is divided into 2n − 1 sub-intervals of equal size. The time windows for
each timed literal φ of the extended problem Π0 are the odd “sub-intervals” of [0, t],
i.e.,
´o
nh
´ h
´
h
3t
t
2t
, 2n−1
,
t
.
Wφ = 0, 2n−1
, 2n−1
, . . . , (2n−2)t
2n−1
(II) Let d be the maximum duration of an action in Π with a timed precondition φ. The
time interval θ = [0, d × (2n − 1)] is divided into 2n − 1 sub-intervals of duration d.
13. In general, these parameters influence not only the hardness of temporal reasoning during planning, but
also the logical part of the planning process (i.e., the selection of the actions forming the plan, that in
lpg-td is done using heuristics taking exogenous events into account).

214

An Approach to Temporal Planning and Scheduling

Rovers-Windows

Milliseconds
10000

ZenoTravel-Windows

Milliseconds
1e+06

1 time window per waypoint
10 time windows per waypoint

1 time window per city
10 time windows per city

100000

1000
10000

1000
100

100

10

10
0

2

4

6

8

10

12

14

16

18

20

0

2

4

6

8

10

12

14

16

18

20

Figure 11: Performance of lpg-td in the Rovers and ZenoTravel domains extended with
timed initial literals (1 and 10 time windows for each timed literal). The test
problems were generated using method I. On the x-axis we have the problem
names simplified by numbers; on the y-axis we have CPU-milliseconds (logarithmic scale).

Similarly to method (I), the time windows for φ in the extended problem Π0 are the
odd sub-intervals of θ.
Notice that we can use the first method only when the number of windows is relatively
small because, if there are too many time windows of small size, the extended problem can
become unsolvable (no window is large enough to schedule into it a necessary action with
a timed precondition). The second method was designed to avoid this problem, and it can
be used to test our techniques on planning problems involving many time windows.
Figures 11 and 12 give the results of the first experiment. The CPU-times in these plots
are median values over five runs for each problem. For the results of Figure 11, we use
the IPC-3 test problems modified by method I, while for the results of Figure 12 we use
the IPC-3 test problems modified by method II. In both cases lpg-td solves all problems.
The plots of Figure 11 indicate that the performance degradation when the number of
windows increases from 1 to 10 is generally moderate, except in two cases. The plots of
Figure 12 indicate that, when the number of windows increases exponentially from 1 to
10,000, the approach scales up well for the benchmark problems considered. For instance,
consider the first ZenoTravel problem. With 1 window lpg-td solves this problem in 10
milliseconds, with 10 windows in 20 milliseconds, with 100 windows in 30 milliseconds,
with 1000 windows in about 100 milliseconds, and with 10,000 windows in about 1 second.
Moreover, we observed that the performance degradation is mainly determined by a heavier
pre-processing phase (parsing and instantiation of the operators).
Tables 2 and 3 give some results concerning the experiment about our temporal reasoning
techniques implemented in lpg-td. We consider some of the problems with 10 time windows
(for each timed fluent) used for the tests of Figure 11, and we examine the computational
215

Gerevini, Saetti & Serina

Performance of LPG-td in Rovers-TimeWindows

Milliseconds
100000

Performance of LPG-td in ZenoTravel-TimeWindows

Milliseconds
1e+06

1 time window per waypoint
10 time windows per waypoint
100 time windows per waypoint
1000 time windows per waypoint
10,000 time windows per waypoint

1 time window per city
10 time windows per city
100 time windows per city
1000 time windows per city
10,000 time windows per city

100000

10000

10000
1000
1000

100
100

10

10
0

2

4

6

8

10

12

14

16

18

20

0

2

4

6

8

10

12

14

16

18

20

Figure 12: Performance of lpg-td in the Rovers and ZenoTravel domains extended with
timed initial literals (1–10,000 time windows for each timed literal). The test
problems were generated using method II. On the x-axis we have the problem
names simplified by numbers; on the y-axis we have CPU-milliseconds (logarithmic scale).

cost of temporal reasoning during planning for these problems. In our approach to temporal
planning, each search step defines a set of temporal constraints formed by the ordering and
scheduling constraints in the current TDA-graph. Table 2 gives statistical information about
such DTPs using both the compact constraint representation of lpg-td and the “classical”
DTP representation. For each action in the TDA-graph, we have two temporal variables
(the start/end times of the action), except astart and aend (for which, as we have pointed out,
we can use only one variable). The number of the scheduling constraints and the number
of the ordering constraints depend on which actions are in the current TDA-graph, and on
how these actions are (causally or exclusively) related to each other, respectively (we have
one scheduling constraint for each action with a timed precondition in the TDA-graph).
Notice that our representation of the scheduling constraints is much more compact than
the classical DTP formulation.14
The table also gives information about the average number of DTPs (i.e., search steps)
generated during planning, indicating how many of them are satisfiable (indicated with
“Sat. DTPs”).
Table 3 gives the CPU-time required by our temporal reasoning techniques implemented
in lpg-td (Solve-DTP+ ) and by tsat++ (Armando, Castellini, Giunchiglia, & Maratea,
2004), a state-of-the-art general DTP solver. The DTPs considered here are the same as
those of Table 2, i.e., the sets of the temporal constraints in the TDA-graph at each search
14. The classical DTP-translation of a scheduling constraint contains an exponential number of disjuncts
with respect to the number of time windows in the scheduling constraint. For example, let q be a
timed precondition of a and Wq = {[25, 50), [75, 125)}. The scheduling constraint of a determined by q is
−
+
−
translated into four classical DTP constraints (as abbreviates astart ): (a+
s −a ≤ −25 ∨ as −a ≤ −75),
+
+
−
+
+
+
+
+
−
+
+
≤
50
∨
a
−
a
−
a
≤
−75),
(a
−
a
≤
50
∨
a
≤
125),
(a
−
a
−
a
≤
−25
∨
a
−
a
(a+
s ≤ 125).
s
s
s
s
s

216

An Approach to Temporal Planning and Scheduling

Problems
Rovers
Problem 1
Problem 5
Problem 10
Problem 15
Problem 20
ZenoTravel
Problem 1
Problem 5
Problem 10
Problem 15
Problem 20

Variables
max mean
28
56
94
98
206

18.4
30.0
65.8
58.8
105.0

8
36
114
172
282

6
20
83.4
122.4
194.6

SC with 10 windows (DC)
max
mean
1
2
2
3
4

(1024)
(2048)
(2048)
(3072)
(4096)

1 (1024)
3 (3072)
16 (16,384)
24 (24,576)
42 (43,008)

DTPs
(Sat. DTPs)

0.13 (136.5)
0.33 (341.3)
1.41 (1447)
1.01 (1037)
1.45 (1489)

15 (15)
27 (27)
104 (47)
77 (55)
108 (108)

0.33 (341.3)
0.88 (910.2)
10.5 (10,769)
16.3 (16,673)
24.9 (25,536)

3 (3)
18 (18)
1162 (175)
291 (128)
750 (637)

Table 2: Characteristics of the DTPs generated during planning by lpg-td when solving
some problems in the Rovers and ZenoTravel domains: maximum/mean number of variables (2nd/3rd columns); maximum/mean number of scheduling constraints (“SC”) and of non-unary disjunctions (“DC”) in their DTP-form translation (4th/5th columns); number of DTPs and of satisfiable DTPs solved by lpg-td
(6th column).

step of the planning process. It should be noted that the comparison of Solve-DTP + and
tsat++ is by no means intended to determine which one is better than the other. Indeed
tsat++ was developed to manage a much larger class of DTPs. However, to the best of
our knowledge there exists no other more specialized DTP-solver handling scheduling constraints that we could have used. The goal of this comparison is to experimentally show
that existing general DTP solvers, although designed to work efficiently in the general case,
are not adequate for managing the class of DTPs that arise in our planning framework.
Hence, it is important to develop more specialized techniques which, as empirically demonstrated by the results of Table 3, can be much more efficient. For instance, consider problem
15 in the Rovers domain. As indicated by the last column of Table 2, lpg-td solves this
problem with 77 search steps, which defines 77 DTPs. The data in Table 3 show that
the total CPU-time spent by lpg-td for solving all these temporal reasoning problems is
negligible (< 10−6 seconds), while tsat++ requires 16.8 CPU-seconds in total (note that
the whole temporal planning problem is solved by lpg-td in only 0.25 seconds). 15 Overall,
our specialized temporal reasoning technique is several orders of magnitude faster than an
efficient general DTP, in terms of both CPU-time for solving a single DTP, and CPU-time
for solving all the DTPs that are generated during planning.
15. The CPU-time of tsat++ includes neither the generation of the explicit (classical) DTPs from the TDAgraph, nor the parsing time. Moreover, while tsat++ only decides satisfiability of the input DTPs,
Solve-DTP+ also finds a schedule that is optimal, if the DTP is satisfiable.

217

Gerevini, Saetti & Serina

Problems

Rovers
Problem 1
Problem 5
Problem 10
Problem 15
Problem 20
ZenoTravel
Problem 1
Problem 5
Problem 10
Problem 15
Problem 20

CPU-seconds for Temporal Reasoning
Solve-DTP+
tsat++
max
mean
total
max
mean
total

Total
CPU-Time
of lpg-td

< 10−6
< 10−6
< 10−6
< 10−6
0.01

< 10−6
< 10−6
< 10−6
< 10−6
0.0008

< 10−6
< 10−6
< 10−6
< 10−6
0.03

0.005
0.045
0.54
0.54
3.17

0.002
0.002
0.039
0.028
0.10

0.09
0.14
12.7
16.8
107.1

0.02
0.03
0.30
0.25
3.03

< 10−6
< 10−6
0.01
0.01
0.01

< 10−6
< 10−6
0.00017
0.00014
0.00065

< 10−6
< 10−6
0.2
0.04
0.5

0.001
0.04
2.7
44.6
323.9

0.0003
0.004
9.8
3.9
24.2

0.01
0.21
6018
18,877
177,595

0.02
0.05
22.0
13.9
376.2

Table 3: Performance of Solve-DTP+ and tsat++ for the DTPs generated during planning
by lpg-td when solving some problems in the Rovers and ZenoTravel domains:
maximum, mean and total CPU-seconds. The last column gives the total CPUtime of lpg-td for solving the planning problem. tsat++ was run using its default
settings.

Finally, we experimentally tested the effectiveness of the improvements to Solve-DTP +
for making the algorithm incremental that we have described at the end of Section 2 (such
improvements are included in the implementation of Solve-DTP+ of Table 3). In particular
we observed that, for the problems of Table 3, the average CPU-time of the basic (nonincremental) version of Solve-DTP+ is from one to three orders of magnitude higher than
the incremental version. However, the basic version is still always significantly faster than
tsat++ (from one to four orders of magnitude).

5. Related Work
Several researchers have addressed temporal reasoning in the context of the DTP framework. Some general techniques aimed at efficiently solving a DTP have been proposed
(e.g., Armando et al., 2004; Tsamardinos & Pollack, 2003), but their worst-case complexity
remains exponential. In Section 4, we presented some experimental results indicating that
the simple use of a state-of-the-art DTP solver is not adequate for solving the subclass of
DTPs that arise in our context.
Various planning approaches supporting the temporal features considered in this paper
have been proposed. One of the first planners that was capable of handling predictable
exogenous events is deviser (Vere, 1983), which was developed from nonlin (Tate, 1977).
deviser is a temporal partial order planner using a network of activities called a “plan
network”. Before starting plan generation, the plan network contains the exogenous events
218

An Approach to Temporal Planning and Scheduling

as explicit nodes of the network. During plan generation, the activities added to the network
are ordered with respect to these scheduled events, depending on the relevance of the events
for the activities. A similar explicit treatment of the exogenous events could be also adopted
in the context of the action-graph representation: the initial action graph contains special
action nodes representing the predicted exogenous events. However, this simple method
has some disadvantages with respect to our method, that treats exogenous events at the
temporal level of the representation rather than at the logical (causal) level. In particular,
when there is a high number of timed initial literals, the explicit representation of the
exogenous events in the action graph could lead to very large graphs, causing memory
consumption problems and a possibly heavy CPU-time cost for the heuristic evaluation of
the (possibly very large) search neighborhood.
In the late 80s and early 90s some other temporal planners handling exogenous events
were developed. In general, these systems use input descriptions of the planning problem/domain that are significantly different from the PDDL descriptions accepted by modern
fully-automated planners. One of the most successful among them is hsts (Frederking &
Muscettola, 1992; Muscettola, 1994), a representation and problem solving framework that
provides an integrated view of planning and scheduling. hsts represents predictable exogenous events through “non-controllable state variables”. Both lpg-td and hsts manage
temporal constraints, but the two systems use considerably different approaches to temporal
planning (lpg-td adopts the classical “state-transition view” of change, while hsts adopts
the “histories view” of change, Ghallab, Nau, & Traverso, 2003), and they are based on
different plan representations and search techniques.
zeno (Penberthy, 1993; Penberthy & Weld, 1994) is one of the first domain-independent
planners which supports a rich class of metric-temporal features, including exogenous events.
zeno is a powerful extension of the causal-link partial-order planner ucpop (Penberthy &
Weld, 1992). However, in terms of computational performance, this planner is not competitive with more recent temporal planners.
IxTeT (Ghallab & Laruelle, 1994; Laborie & Ghallab, 1995) is another causal-link planner which uses some techniques and ideas from scheduling, temporal constraint reasoning,
and graph algorithms. IxTeT supports a very expressive language for the temporal description of the actions, including timed preconditions and some features that cannot be
expressed in PDDL2.2. The expressive power of the language is obtained at the cost of increased semantic complexity (Fox & Long, 2005). As observed by Ghallab, Nau and Traverso
(2003), IxTeT embodies a compromise between the expressiveness of complex temporal domains, and the planning efficiency; however, this planner still remains noncompetitive with
the more recent temporal planners.
Smith and Weld (1999) studied an extension of the Graphplan-style of planning for
managing temporal domains. They proposed an extension of their tgp planner that makes
it possible to represent predictable exogenous events. tgp supports only a subclass of the
durative actions expressible in PDDL2.1, which prevents some cases of concurrency that
in PDDL2.1 are admitted. tgp is an optimal planner (under the assumed conservative
model of action concurrency), while lpg-td is a near-optimal (satisficing) planner. A main
drawback of tgp is that it does not scale up adequately.
More recently, Edelkamp (2004) proposed a method for planning with timed initial
literals that is based on compiling the action timed preconditions into a time window as219

Gerevini, Saetti & Serina

sociated with the action, defining the interval during which the action can be scheduled.
He gives an efficient, polynomial algorithm based on critical path analysis for computing
an optimal action schedule from sequential plans generated using the compiled representation. The techniques presented by Edelkamp assume a unique time window for each
timed precondition. The techniques that we propose are more general, in the sense that our
action representation treats multiple time windows associated with a timed precondition,
and our temporal reasoning method computes optimal schedules for partially ordered plans
preserving polynomiality.
Cresswell and Coddington (2004) proposed an extension of the lpgp planner (Long
& Fox, 2003b) to handle timed initial literals, which are represented by special “deadline
actions”. A literal that is asserted to hold at time t is represented by a deadline action
starting at the time of the initial state, and having duration t. The deadline actions in the
plan under construction are translated into particular linear inequalities that, together with
other equalities and inequalities generated from the plan representation, are managed by
a general linear programming solver. lpg-td uses a different representation that does not
encode timed initial literals as special actions, and in which the temporal and scheduling
constraints associated with the actions in the plan are managed by an efficient algorithm
derived by specializing a general DTP solver.
In order to handle problems with timed initial literals in the sapa planner (Do & Kambhampati, 2003), Do, Kambhampati and Zimmerman (2004) proposed a forward search
heuristic based on relaxed plans, which are constructed by exploiting a technique similar to
the time slack analysis used in scheduling (Smith & Cheng, 1993). Given a set of candidate
actions for choosing an action to add to the relaxed plan under construction, this technique
computes the minimum slack between each candidate action and the actions currently in
the relaxed plan. The candidate action with the highest minimum slack is preferred. lpg-td
uses a different time slack analysis, which is exploited in a different way. Our method for
selecting the actions forming the relaxed plan uses the time slacks for counting the number
of scheduling constraints that would be violated when adding a candidate action: we prefer
the candidate actions which cause the lowest number of violations. Moreover, in sapa the
slack analysis is limited to the actions of the relaxed plan, while our method also considers
the actions in the real plan under construction.
dt-pop is a recent planner (Schwartz & Pollack, 2004) extending the POP-style of
planning with an action model involving disjunctive temporal constraints. The language of
dt-pop is elegant and can express a rich class of temporal features, most of which can be
only indirectly (and less elegantly) expressed in PDDL2.2 (Fox et al., 2004). The treatment
of the temporal constraints required to manage predictable exogenous events in dt-pop
appears to be less efficient than in our planner, since dt-pop uses a general DTP solver
enhanced with some efficiency techniques, while lpg-td uses a polynomial solver specialized
for the subclass of DTPs that arise in our representation. dt-pop handles mutex actions
(“threats”) by posting explicit temporal disjunctive constraints imposing disjointness of
the mutex actions, while lpg-td implicitly decides these disjunctions at search time by
choosing the level of the graph where actions are inserted, and asserting the appropriate
precedence constraints. Moreover, the search procedure and heuristics in dt-pop and lpgtd are significantly different.
220

An Approach to Temporal Planning and Scheduling

At IPC-4, the planners that reasoned with timed initial literals are tilsapa (Kavuluri &
U, 2004), sgplan (Chen et al., 2004), p-mep (Sanchez et al., 2004) and lpg-td. For the first
two planners, at the time of writing, to the best of our knowledge in the available literature
there is no sufficiently detailed description to clearly understand their possible similarities
and differences with lpg-td about the treatment of predictable exogenous events. Regarding
p-mep, this planner uses forward state-space search guided by a relaxed plan heuristic which,
differently from the relaxed plans of lpg-td, is constructed without taking account of the
temporal aspects of the relaxed plan and real plan under construction (the makespan of the
constructed relaxed plans is considered only for their comparative evaluation).

6. Conclusions
We have presented some techniques for temporal planning in domains where certain fluents
are made true or false at known times by predictable exogenous events that cannot be
influenced by the actions available to the planner. Such external events are present in many
realistic domains, and a planner has to take them into account to guarantee the correctness
of the synthesized plans, to generate plans of good or optimal quality (makespan), and to
use effective search heuristics for fast planning.
In our approach, the causal structure of the plan is represented by a graph-based representation called TDA-graph, action ordering and scheduling constraints are managed by
efficient constraint-based reasoning, and the plan search is based on a stochastic local search
procedure. We have proposed an algorithm for managing the temporal constraints in a
TDA-graph which is a specialization of a general CSP-based method for solving DTPs.
The algorithm has a polynomial worst-case complexity and, when combined with our plan
representation, in practice it is very efficient. We have also presented some local search
techniques for temporal planning using the new TDA-graph representation. These techniques improve the accuracy of the heuristic methods adopted in the previous version of
lpg, and they extend them to consider action scheduling constraints in the evaluation of the
search neighborhood, which is based on relaxed temporal plans exploiting some (dynamic)
reachability information.
All our techniques are implemented in the planner lpg-td. We have experimentally
investigated the performance of our planner by a statistical analysis of the IPC-4 results
using Wilcoxon’s test. The results of this analysis show that our planner performs very well
compared to other recent temporal planners supporting predictable exogenous events, both
in terms of CPU-time to find a valid plan and quality of the best plan generated. Moreover,
a comparison of the plans computed by lpg-td and those generated by the optimal planners
of IPC-4 shows that very often lpg-td generates plans with very good or optimal quality.
Finally, additional experiments indicate that our temporal reasoning techniques manage the
class of DTPs that arise in our context very efficiently.
Some directions for future work on temporal planning within our framework are: an
extension of the local search heuristics and temporal reasoning techniques to explicitly handle action effects with limited persistence or delays; the treatment of predictable exogenous
events affecting numerical fluents in a discrete or continuous way; the development of tech221

Gerevini, Saetti & Serina

niques supporting controllable exogenous events;16 and the management of actions with
“variable” durations (Fox & Long, 2003), i.e., actions whose durations are specified only by
inequalities constraining their lower or upper bounds, and whose actual duration is decided
by the planner.
Moreover, we intend to study the integration into our framework of the techniques for
goal partitioning and subplan composition that have been successfully used by sgplan
(Chen et al., 2004) in IPC-4, and the application of our approach to plan revision. The
latter has already been partially explored, but only for simple strips domains and using
less powerful search techniques (Gerevini & Serina, 2000).

Acknowledgments
This paper is a revised and extended version of a paper appearing in the Proceedings of
the Nineteenth International Joint Conference on Artificial Intelligence (Gerevini, Saetti, &
Serina, 2005a). The research was supported in part by MIUR Grant anemone. The work
of Ivan Serina was in part carried out at the Department of Computer and Information
Sciences of the University of Strathclyde (Glasgow, UK), and was supported by Marie Curie
Fellowship N HPMF-CT-2002-02149. We would like to thank the anonymous reviewers for
their helpful comments, and Paolo Toninelli who extended the parser of lpg-td to handle
the new language features of PDDL2.2.

Appendix A: Reachability Information
The techniques described in the paper for computing the action evaluation function use
heuristic reachability information about the minimum number of actions required to reach
the preconditions of each domain action (N um acts) and a lower bound on the earliest
finishing time (Ef t) of the reachable actions (the actions whose preconditions are reachable).
In the following, S(l) denotes the state defined by the facts corresponding to the fact nodes
supported at level l of the current TDA-graph. When l = 1, S(l) represents the initial state
of the planning problem (I).
For each action a, lpg-td pre-computes N um acts(a, I), i.e., the estimated minimum
number of actions required to reach the preconditions of a from I, and Ef t(a, I), i.e., the
estimated earliest finishing time of a (if a is reachable from I). Similarly, for each fact f that
is reachable from I, lpg-td computes the estimated minimum number of actions required
to reach f from I (N um acts(f, I)) and the estimated earliest time when f can be made
true by a plan starting from I (Et(f, I)). For l > 1, N um acts(a, S(l)) and Ef t(a, S(l))
can be computed only during search, because they depend on which action nodes are in the
current TDA-graph at the levels preceding l. Since during search many action nodes can be
added and removed, and after each of these operations N um acts(a, S(l)) and Ef t(a, S(l))
could change (if the operation concerns a level preceding l), it is important that they are
computed efficiently.
16. Consider for instance a transportation domain in which a shuttle bus is at the train station for an extra
run to the airport at midnight only if booked in advance. If the shuttle booking is a domain action
available to the planner, then the event “night stop of the shuttle” can be controlled by the planner.

222

An Approach to Temporal Planning and Scheduling

ReachabilityInformation(I, O)
Input: The initial state of the planning problem under consideration (I) and all ground instances
(actions) of the operators (O);
Output: For each action a, an estimate of the number of actions (N um acts(a, I)) required to reach
the preconditions of a from I, an estimate of the earliest finishing time of a from I (Ef t(a, I)).
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.

forall facts f do /* the set of all facts is precomputed by the operator instantiation phase */
if f ∈ I then
N um acts(f, I) ← Et(f, I) ← 0; Action(f, I) ← astart ;
else N um acts(f, I) ← Et(f, I) ← ∞;
forall actions a do N um acts(a, I) ← Ef t(a, I) ← Lf t(a) ← ∞;
F ← I; Fnew ← I; A ← O; Arev ← ∅;
while ( Fnew 6= ∅ or Arev 6= ∅ )
F ← F ∪ Fnew ; Fnew ← ∅; A ← A ∪ Arev ; Arev ← ∅;
while A0 = {a ∈ A | P re(a) ⊆ F } is not empty
a ← an action in A0 ;
t ← ComputeEFT(a, M AX Et(f, I));
f ∈P re(a)

if t < Ef t(a, I) then Ef t(a, I) ← t;
Lf t(a) ← ComputeLFT(a);
if Ef t(a, I) ≤ Lf t(a) then /* a can be scheduled */
ra ← RequiredActions(I, P re(a));
if N um acts(a, I) > ra then N um acts(a, I) ← ra;
forall f ∈ Add(a) do
if Et(f, I) > t then
Et(f, I) ← t;
Arev ← Arev ∪ {a0 ∈ O − A | f ∈ P re(a0 )};
if N um acts(f, I) > (ra + 1) then
N um acts(f, I) ← ra + 1; Action(f, I) ← a;
Fnew ← Fnew ∪ Add(a) − F ;
A ← A − {a};

RequiredActions(I, G)
Input: A set of facts I and a set of action preconditions G;
Output: An estimate of the min number of actions required to achieve all facts in G from I (ACTS).
1.
2.
3.
4.
5.
6.
7.
8.

ACT S ← ∅;
G ← G − I;
while G 6= ∅
g ← an element of G;
a ← Action(g, I);
ACT S ← ACT S ∪ {a}; S
G ← G ∪ P re(a) − I − b∈ACT S Add(b);
return(|ACT S|).

Figure 13: Algorithms for computing heuristic information about the search cost and the
time for reaching a set of facts G from I.

223

Gerevini, Saetti & Serina

Figure 13 gives ReachabilityInformation, the algorithm used by lpg-td for computing
N um acts(a, I), Ef t(a, I), N um acts(f, I) and Et(f, I). ReachabilityInformation is similar
to the reachability algorithm used by the version of lpg that took part in 2002 planning
competition (lpg-ipc3), but with some significant differences. The main differences are:
(i) in order to estimate the earliest finishing time of the domain actions, ReachabilityInformation takes into account the scheduling constraints, which were not considered in
the previous version of the algorithm;
(ii) the algorithm used by lpg-ipc3 applies each domain action at most once, while ReachabilityInformation can apply them more than once.
Notice that (i) improves the accuracy of the estimated finishing time of the actions
(Ef t), which is an important piece of information used during the search neighborhood
evaluation for selecting the actions forming the temporal relaxed plans (see Section 3).
Moreover, (i) allows us to identify some domain actions that cannot be scheduled during
the time windows associated with their timed preconditions, and so these can be pruned
away.
Regarding (ii), during the forward process of computing the reachability information, an
action is re-applied whenever the estimated earliest time of one of its preconditions has been
decreased. This is important for two reasons. On one hand, reconsidering actions already
applied is useful because it can lead to a better estimate of the action finishing times;
on the other hand, this is also necessary to guarantee the correctness of the reachability
algorithm. The latter is because, if we overestimate the earliest finishing time of an action
with a scheduling constraint, then we could incorrectly conclude that the action cannot be
scheduled (and so we would consider the action inapplicable). But if this action is necessary
in any valid plan, then the incorrect estimate of its earliest finishing time could lead to the
incorrect conclusion that the planning problem is unsolvable. In other words, the estimated
finishing time of an action with a scheduling constraint should be a lower bound of its actual
earliest finishing time.
ReachabilityInformation could be used to update N um acts(a, S(l)) and Ef t(a, S(l)) after
each action insertion/removal, for any l > 1 (when l > 1, instead of I, in input the algorithm
has S(l)). However, in order to make the updating process more efficient, the revision is done
in a more selective focused way. Instead of revising the reachability information after each
graph modification (search step), we do so before evaluating the search neighborhood and
choosing the estimated best modification. Specifically, if we are repairing the flawed level l,
we update only the reachability information for the actions and facts at the levels preceding
l that have not been updated yet. (For instance, suppose that at the ith search step we add
an action to level 5, and that at the (i + 1)th step we add another action at level 10. At
the (i + 1)th step we need to consider updating only the reachability information at levels
6–10, since this information at levels 1–5 has already been updated by the ith step.) This is
sufficient because the search neighborhood for repairing the flawed level under consideration
(l) can contain only the graph modifications concerning the levels preceding l.
Before describing the steps of ReachabilityInformation, we need to introduce some notation. Add (a) denotes the set of the positive effects of a; Pre(a) denotes the set of the
(non-timed) preconditions of a; Arev denotes the set of the actions already applied whose
224

An Approach to Temporal Planning and Scheduling

reachability could be revised because the estimated earliest time of some of their preconditions has been revised after their application. Given an action node a and its “current”
earliest start time t computed as the maximum over the earliest times at which its preconditions are reachable, ComputeEFT (a, t) is a function computing the earliest finishing
time τ of a that is consistent with the scheduling constraint of a (if any) and such that
t + Dur(a) ≤ τ .17 ComputeLFT (a) is a function computing the latest finishing time of the
action a, i.e., it returns the upper bound of the last time window during which a can be
scheduled (if one exists), while it returns ∞ if a has no timed precondition.
For example, let a be an action such that all its preconditions are true in the initial
state I (i.e., t = 0), the duration of a is 50, and a has a scheduling constraint imposing that
the action is executed during the interval [25, 100). ComputeEFT (a, t) returns 75, while
ComputeLFT (a, t) returns 100. Thus, the scheduling constraint of a can be satisfied. On
the contrary, if the earliest start time of a is 500, then ComputeEFT (a, t) returns 550 and
a cannot be scheduled during [25, 100).
For the sake of clarity, first we describe the steps of ReachabilityInformation used to derive
N um acts, and then we comment on those for the computation of Ef t. In steps 1–4, for
every fact f , the algorithm initializes N um acts(f, I) to 0, if f ∈ I, and to ∞ otherwise
(indicating that f is not reachable); while, in step 5, N um acts(a, I) is initialized to ∞
(indicating that a is not reachable from I). Then, in steps 7–24 the algorithm iteratively
constructs the set F of the facts that are reachable from I, starting with F = I, and
terminating when F cannot be further extended and the set Arev of the actions to reconsider
is empty. The set A of the available actions is initialized to the set of all possible actions
(step 6); A is reduced by a after its application (step 24), and it is augmented by the set of
actions Arev (step 8) after each action application. When we modify the estimated time at
which a precondition of an action a becomes reachable, a is added to A rev (step 20). The
internal while-loop (steps 9–24) applies the actions in A to the current F , possibly deriving
a new set of facts Fnew in step 23. If Fnew or Arev are not empty, then F is extended with
Fnew , A is extended with Arev , and the internal loop is repeated. When an action a in A0
(the subset of actions currently in A that are applicable to F ) is applied, the reachability
information for its effects are revised as follows. First we estimate the minimum number
ra of actions required to achieve P re(a) from I using the subroutine RequiredActions (step
15). Then we use ra to possibly update N um acts(a, I) and N um acts(f, I) for any effect
f of a (steps 15–16, 21–22). If the number of actions required to achieve the preconditions
of a is lower than the current value of N um acts(a, I), then N um acts(a, I) is set to ra.
Moreover, if the application of a leads to a lower estimate of f , i.e., if ra + 1 is less than the
current value of N um acts(f, I), then N um acts(f, I) is set to ra + 1. In addition, a data
structure indicating the current “best” action to achieve f from I (Action(f, I)) is set to a
(step 22). This information is used by the subroutine RequiredActions.
For any fact f in the initial state, the value of Action(f, I) is astart (step 3). The
subroutine RequiredActions is the same as the one in the reachability algorithm of lpg-ipc3.
The subroutine uses Action to derive ra through a backward process starting from the input
set of action preconditions (G), and ending when G ⊆ I. The subroutine incrementally
constructs a set of actions (ACTS) achieving the facts in G and the preconditions of the
17. If there is no scheduling constraint associated with a, or the existing scheduling constraints cannot be
satisfied by starting the action at t, then ComputeEFT (a, t) returns t + Dur(a).

225

Gerevini, Saetti & Serina

actions already selected (using Action). At each iteration the set G is revised by adding the
preconditions of the last action selected, and removing the facts belonging to I or to the
effects of actions already selected (step 7). Termination of RequiredActions is guaranteed
because every element of G is reachable from I.
We now briefly describe the computation of the temporal information. Eft(a, I), is computed in a way similar to N um acts(a, I). In steps 1–4, ReachabilityInformation initializes
the estimated earliest time (Et(f, I)) when a fact f becomes reachable to 0, if f ∈ I, and
to ∞ otherwise; moreover, the algorithm sets Ef t(a, I) and Lf t(a, I) to ∞. Then, at every
application of an action a in the forward process described above, we estimate the earliest
finishing time Ef t by adding the duration of a to the (current) maximum estimated earliest
time of the preconditions of a, and by taking into account the scheduling constraints of a
using ComputeEFT (a) (step 11). In addition, we compute the latest finishing time Lf t
of a using ComputeLFT (a) (step 13). When the earliest finishing time of an action a is
greater than its latest finishing time, the timed preconditions of a cannot be satisfied from
I, and so steps 15–23 are not executed (see the if-statement of step 14). For any effect f of
a with a current temporal value higher than the earliest finishing time t of a, steps 18–19
set Et(f, I) to t, and step 20 adds a in Arev (because we have decreased the estimated
earliestx time of f , and this revision could decrease the estimated start time of an action
with precondition f ).

Appendix B: Wilcoxon Test for the Metric-Temporal Domains of IPC-4
In this appendix, we present the results of the Wilcoxon sign-rank test on the performance
of lpg-td and the other satisficing IPC-4 planners that attempted the metric-temporal
domains. The performance is evaluated both in terms of CPU-times and plan quality.
Each cell in the first two tables gives the result of a comparison between the performance
of lpg-td and another IPC-4 planner. When the number of samples is sufficiently large, the
T-distribution used by the Wilcoxon test is approximatively a normal distribution. Hence,
in each cell of the Figure we give the z-value and the p-value characterizing the normal
distribution. The higher the z-value, the more significant the difference of the performance
is. The p-value represents the level of significance in the difference of the performance.
We use a confidence level of 99.9%; therefore, if the p-value is lower than 0.001, then the
performance of the two planners is statistically different. When this information appears
on the left (right) side of the cell, the first (second) planner named in the title of the cell
performs better than the other. For the analysis comparing the CPU-time, the value under
each cell is the number of the problems solved by at least one planner; while for the analysis
comparing the plan quality, it is the number of problems solved by both the planners.
The pictures under the tables show the partial order of the performance of the compared
planners in terms of CPU-time and plan quality. A solid edge from a planner A to another
planner B (or a cluster of planners B) indicates that the performance of A is statistically
different from the performance of B, and that A performs better than B (every planner in
B). A dashed edge from A to B indicates that A is better than B a significant number of
times, but there is not significant Wilcoxon relationship between them at a confidence level
of 99.9%.
226

An Approach to Temporal Planning and Scheduling

lpg-td.s vs crikey
11.275
< 0.001
169

Analysis of CPU-Time
lpg-td.s vs p-mep
lpg-td.s vs sgplan
11.132
0.387
< 0.001
(0.699)
215
513

lpg-td.s vs tilsapa
12.324
< 0.001
136

lpg-td.bq vs crikey
10.500
< 0.001
173

Analysis of Plan Quality
lpg-td.bq vs p-mep lpg-td.bq vs sgplan
4.016
16.879
< 0.001
< 0.001
21
452

lpg-td.bq vs tilsapa
6.901
< 0.001
63

lpg-td.s
crikey

p-mep

tilsapa

sgplan

CPU-Time

sgplan
lpg-td.bq
p-mep

crikey

A

B:

A is consistently better than B

tilsapa

A

B:

A is better than B a significant number of times
(confidence level 94.78%)

Plan Quality

227

Gerevini, Saetti & Serina

References
Armando, A., Castellini, C., Giunchiglia, E., & Maratea, M. (2004). A SAT-based decision
procedure for the boolean combination of difference constraints. In Proceedings of the
Seventh International Conference on Theory and Applications of Satisfiability Testing
(SAT-04), Berlin, Heidelberg, New York. Springer-Verlag. SAT 2004 LNCS Volume.
Blum, A., & Furst, M. (1997). Fast planning through planning graph analysis. Artificial
Intelligence, 90, pp. 281–300.
Chen, Y., Hsu, C., & Wah B., W. (2004). SGPlan: Subgoal partitioning and resolution
in planning. In Edelkamp, S., Hoffmann, J., Littman, M., & Younes, H. (Eds.), In
Abstract Booklet of the Competing Planners of ICAPS-04, pp. 30–32.
Cresswell, S., & Coddington, A. (2004). Adapting LPGP to plan with deadlines. In Proceedings of the Sixteenth European Conference on Artificial Intelligence (ECAI-04),
pp. 983–984, Amsterdam, The Netherlands. IOS Press.
Dechter, R., Meiri, I., & Pearl, J. (1991). Temporal constraint networks. Artificial Intelligence, 49, pp. 61–95.
Do, M., B., Kambhampati, S., & Zimmerman, T. (2004). Planning - scheduling connections
through exogenous events. In Proceedings of the ICAPS-04 Workshop on Integrating
Planning into Scheduling, pp. 32–37.
Do, M., & Kambhampati, S. (2003). SAPA: A multi-objective metric temporal planner.
Journal of Artificial Intelligence Research (JAIR), 20, pp. 155–194.
Edelkamp, S. (2004). Extended critical paths in temporal planning. In Proceedings of the
ICAPS-04 Workshop on Integrating Planning into Scheduling, pp. 38–45.
Edelkamp, S., & Hoffmann, J. (2004). PDDL2.2: The language for the classic part of the
4th international planning competition. Technical report 195, Institut für Informatik,
Freiburg, Germany.
Edelkamp, S., Hoffmann, J., Littman, M., & Younes, H. (2004) In Abstract Booklet of the
competing planners of ICAPS-04.
Erschler, J., Roubellat, F., & Vernhes, J. P. (1976). Finding some essential characteristics
of the feasible solutions for a scheduling problem. Operations Research (OR), 24, pp.
772–782.
Fox, M., & Long, D. (2003). PDDL2.1: An extension to PDDL for expressing temporal
planning domains. Journal of Artificial Intelligence Research (JAIR), 20, pp. 61–124.
Fox, M., & Long, D. (2005). Planning in time. In Fisher, M., Gabbay, D., & Vila, L. (Eds.),
Handbook of Temporal Reasoning in Artificial Intelligence, pp. 497–536. Elsevier Science Publishers, New York, NY, USA.
Fox, M., Long, D., & Halsey, K. (2004). An investigation into the expressive power of
PDDL2.1. In Proceedings of the Sixteenth European Conference on Artificial Intelligence (ECAI-04), pp. 338–342, Amsterdam, The Netherlands. IOS Press.
Frederking, R., E., & Muscettola, N. (1992). Temporal planning for transportation planning and scheduling. In IEEE International Conference on Robotics and Automation
(ICRA-92), pp. 1125–1230. IEEE Computer Society Press.
228

An Approach to Temporal Planning and Scheduling

Gerevini, A., & Cristani, M. (1997). On finding a solution in temporal constraint satisfaction
problems. In Proceedings of the Fifteenth International Joint Conference on Artificial
Intelligence (IJCAI-97), Vol. 2, pp. 1460–1465, San Francisco, CA, USA. Morgan
Kaufmann Publishers.
Gerevini, A., Saetti, A., & Serina, I. (2003). Planning through stochastic local search and
temporal action graphs. Journal of Artificial Intelligence Research (JAIR), 20, pp.
239–290.
Gerevini, A., Saetti, A., & Serina, I. (2004). An empirical analysis of some heuristic features
for local search in LPG. In Proceedings of the Fourteenth International Conference
on Automated Planning and Scheduling (ICAPS-04), pp. 171–180, Menlo Park, CA,
USA. AAAI Press.
Gerevini, A., Saetti, A., & Serina, I. (2005a). Integrating planning and temporal reasoning for domains with durations and time windows. In Proceedings of the Nineteenth
International Joint Conference on Artificial Intelligence (IJCAI-05), pp. 1226–1235,
Menlo Park, CA, USA. International Joint Conference on Artificial Intelligence Inc.
Gerevini, A., Saetti, A., Serina, I., & Toninelli, P. (2005b). Fast planning in domains with
derived predicates: An approach based on rule-action graphs and local search. In
Proceedings of the Twentieth National Conference on Artificial Intelligence (AAAI05), pp. 1157–1162, Menlo Park, CA, USA. AAAI Press.
Gerevini, A., & Serina, I. (1999). Fast planning through greedy action graphs. In Proceedings
of the Sixteenth National Conference on Artificial Intelligence (AAAI-99), pp. 503–
510, Menlo Park, CA, USA. AAAI Press/MIT Press.
Gerevini, A., & Serina, I. (2000). Fast plan adaptation through planning graphs: Local and
systematic search techniques. In Proceedings of the Fifth International Conference on
Artificial Intelligence Planning and Scheduling (AIPS-00), pp. 112–121, Menlo Park,
CA, USA. AAAI Press/MIT Press.
Ghallab, M., & Laruelle, H. (1994). Representation and control in IxTeT, a temporal planner. In Proceedings of the Second International Conference on Artificial Intelligence
Planning Systems (AIPS-94), pp. 61–67, Menlo Park, CA, USA. AAAI press.
Ghallab, M., Nau, D., & Traverso, P. (2003). Automated Planning: Theory and Practice.
Morgan Kaufmann Publishers, San Francisco, CA, USA.
Glover, F., & Laguna, M. (1997). Tabu Search. Kluwer Academic Publishers, Boston, USA.
Helmert, M. (2004). A planning heuristic based on causal graph analysis. In Proceedings
of the Fourteenth International Conference on Automated Planning and Scheduling
(ICAPS-04), pp. 161–170, Menlo Park, CA, USA. AAAI Press.
Kavuluri, B. R., & U, S. (2004). Tilsapa - timed initial literals using SAPA. In Edelkamp, S.,
Hoffmann, J., Littman, M., & Younes, H. (Eds.), In Abstract Booklet of the Competing
Planners of ICAPS-04, pp. 46–47.
Laborie, P., & Ghallab, M. (1995). Planning with sharable resource constraints. In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence
(IJCAI-95), Vol. 2, pp. 1643–1651, San Francisco, CA, USA. Morgan Kaufmann Publishers.
229

Gerevini, Saetti & Serina

Long, D., & Fox, M. (2003a). The 3rd international planning competition: Results and
analysis. Journal of Artificial Intelligence Research (JAIR), 20, pp. 1–59.
Long, D., & Fox, M. (2003b). Exploiting a graphplan framework in temporal planning. In
Proceedings of the Thirteenth International Conference on Automated Planning and
Scheduling (ICAPS-03), pp. 52–61, Menlo Park, CA, USA. AAAI Press.
McAllester, D., & Rosenblitt, D. (1991). Systematic nonlinear planning. In Proceedings
of the Ninth National Conference on Artificial Intelligence (AAAI-91), pp. 634–639,
Menlo Park, CA, USA. AAAI Press.
Muscettola, N. (1994). HSTS: Integrating planning and scheduling. In Zweben, & Fox
(Eds.), Intelligent Scheduling, pp. 169–212, San Francisco, CA, USA. Morgan Kaufmann Publishers.
Nguyen, X., & Kambhampati, S. (2001). Reviving partial order planning. In Proceedings of
the Seventeenth International Joint Conference on Artificial Intelligence (IJCAI-01),
Vol. 1, pp. 459–464, San Francisco, CA, USA. Morgan Kaufmann Publishers.
Penberthy, J., & Weld, D. (1992). UCPOP: A sound, complete, partial order planner for
ADL. In Proceedings of the Third International Conference on Principles of Knowledge
Representation and Reasoning (KR’92), pp. 103–114, San Mateo, CA, USA. Morgan
Kaufmann Publishers.
Penberthy, J., & Weld, D. (1994). Temporal planning with continuous change. In Proceedings
of the Twelfth National Conference on Artificial Intelligence (AAAI-94), pp. 1010–
1015, Menlo Park, CA, USA. AAAI Press/MIT Press.
Penberthy, J., S. (1993). Planning with Continuous Change. Ph.D. thesis, University of
Washington, Seattle, WA, USA. Available as technical report UW-CSE-93-12-01.
Sanchez, J., Tang, M., & Mali, A., D. (2004). P-MEP: Parallel more expressive planner. In
Edelkamp, S., Hoffmann, J., Littman, M., & Younes, H. (Eds.), In Abstract Booklet
of the Competing Planners of ICAPS-04, pp. 53–55.
Schwartz, P., J., & Pollack, M., E. (2004). Planning with disjunctive temporal constraints.
In Proceedings of the ICAPS-04 Workshop on Integrating Planning into Scheduling,
pp. 67–74.
Smith, D., & Weld, D. (1999). Temporal planning with mutual exclusive reasoning. In
Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence
(IJCAI-99), pp. 326–337, San Francisco, CA, USA. Morgan Kaufmann Publishers.
Smith, S., & Cheng, C. (1993). Slack-based heuristics for constraint satisfaction scheduling.
In Proceedings of the Eleventh National Conference on Artificial Intelligence (AAAI93), pp. 139–144, Menlo Park, CA, USA. AAAI Press/The MIT press.
Stergiou, K., & Koubarakis, M. (2000). Backtracking algorithms for disjunctions of temporal
constraints. Artificial Intelligence, 120 (1), pp. 81–117.
Tate, A. (1977). Generating project networks. In Proceedings of the Fifth International
Joint Conference on Artificial Intelligence (IJCAI-77), pp. 888–889, Cambridge, MA,
USA. MIT, William Kaufmann.
230

An Approach to Temporal Planning and Scheduling

Tsamardinos, I., & Pollack, M. E. (2003). Efficient solution techniques for disjunctive
temporal reasoning problems. Artificial Intelligence, 151 (1-2), pp. 43–89.
Vere, S. A. (1983). Planning in time: Windows and durations for activities and goals. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 5 (3), pp. 246–267.
Vidal, V. (2004). A lookahead strategy for heuristic search planning. In Proceedings of the
Fourteenth International Conference on Automated Planning and Scheduling (ICAPS04), pp. 150–159, Menlo Park, CA, USA. AAAI Press.
Wilcoxon, F., & Wilcox, R. A. (1964). Some Rapid Approximate Statistical Procedures.
American Cyanamid Co., Pearl River, NY, USA.

231

Journal of Artificial Intelligence Research 25 (2006) 425–456

Submitted 12/04; published 4/06

Logical Hidden Markov Models
Kristian Kersting
Luc De Raedt

kersting@informatik.uni-freiburg.de
deraedt@informatik.uni-freiburg.de

Institute for Computer Science
Albert-Ludwigs-Universität Freiburg
Georges-Koehler-Allee 079
D-79110 Freiburg, Germany

Tapani Raiko

tapani.raiko@hut.fi

Laboratory of Computer and Information Science
Helsinki University of Technology
P.O. Box 5400
FIN-02015 HUT, Finland

Abstract
Logical hidden Markov models (LOHMMs) upgrade traditional hidden Markov models
to deal with sequences of structured symbols in the form of logical atoms, rather than flat
characters.
This note formally introduces LOHMMs and presents solutions to the three central inference problems for LOHMMs: evaluation, most likely hidden state sequence and parameter estimation. The resulting representation and algorithms are experimentally evaluated
on problems from the domain of bioinformatics.

1. Introduction
Hidden Markov models (HMMs) (Rabiner & Juang, 1986) are extremely popular for analyzing sequential data. Application areas include computational biology, user modelling,
speech recognition, empirical natural language processing, and robotics. Despite their successes, HMMs have a major weakness: they handle only sequences of flat, i.e., unstructured symbols. Yet, in many applications the symbols occurring in sequences are structured. Consider, e.g., sequences of UNIX commands, which may have parameters such
as emacs lohmms.tex, ls, latex lohmms.tex, . . .Thus, commands are essentially structured.
Tasks that have been considered for UNIX command sequences include the prediction of
the next command in the sequence (Davison & Hirsh, 1998), the classification of a command
sequence in a user category (Korvemaker & Greiner, 2000; Jacobs & Blockeel, 2001), and
anomaly detection (Lane, 1999). Traditional HMMs cannot easily deal with this type of
structured sequences. Indeed, applying HMMs requires either 1) ignoring the structure of
the commands (i.e., the parameters), or 2) taking all possible parameters explicitly into
account. The former approach results in a serious information loss; the latter leads to a
combinatorial explosion in the number of symbols and parameters of the HMM and as a
consequence inhibits generalization.
The above sketched problem with HMMs is akin to the problem of dealing with structured examples in traditional machine learning algorithms as studied in the fields of inductive logic programming (Muggleton & De Raedt, 1994) and multi-relational learnc
2006
AI Access Foundation. All rights reserved.

Kersting, De Raedt, & Raiko

ing (Džeroski & Lavrač, 2001). In this paper, we propose an (inductive) logic programming
framework, Logical HMMs (LOHMMs), that upgrades HMMs to deal with structure. The
key idea underlying LOHMMs is to employ logical atoms as structured (output and state)
symbols. Using logical atoms, the above UNIX command sequence can be represented
as emacs(lohmms.tex), ls, latex(lohmms.tex), . . . There are two important motivations for
using logical atoms at the symbol level. First, variables in the atoms allow one to make
abstraction of specific symbols. E.g., the logical atom emacs(X, tex) represents all files X
that a LATEX user tex could edit using emacs. Second, unification allows one to share information among states. E.g., the sequence emacs(X, tex), latex(X, tex) denotes that the
same file is used as an argument for both Emacs and LATEX.
The paper is organized as follows. After reviewing the logical preliminaries, we introduce
LOHMMs and define their semantics in Section 3; in Section 4, we upgrade the basic
HMM inference algorithms for use in LOHMMs; we investigate the benefits of LOHMMs in
Section 5: we show that LOHMMs are strictly more expressive than HMMs, that they can
be — by design — an order of magnitude smaller than their corresponding propositional
instantiations, and that unification can yield models, which better fit the data. In Section 6,
we empirically investigate the benefits of LOHMMs on real world data. Before concluding,
we discuss related work in Section 7. Proofs of all theorems can be found in the Appendix.

2. Logical Preliminaries
A first-order alphabet Σ is a set of relation symbols r with arity m ≥ 0, written r/m, and a
set of functor symbols f with arity n ≥ 0, written f/n. If n = 0 then f is called a constant,
if m = 0 then p is called a propositional variable. (We assume that at least one constant
is given.) An atom r(t1 , . . . , tn ) is a relation symbol r followed by a bracketed n-tuple of
terms ti . A term t is a variable V or a functor symbol f(t1 , . . . , tk ) immediately followed by
a bracketed k-tuple of terms ti . Variables will be written in upper-case, and constant, functor and predicate symbols lower-case. The symbol will denote anonymous variables which
are read and treated as distinct, new variables each time they are encountered. An iterative
clause is a formula of the form H ← B where H (called head) and B (called body) are logical
atoms. A substitution θ = {V1 /t1 , . . . , Vn /tn }, e.g. {X/tex}, is an assignment of terms ti
to variables Vi . Applying a substitution σ to a term, atom or clause e yields the instantiated term, atom, or clause eσ where all occurrences of the variables V i are simultaneously
replaced by the term ti , e.g. ls(X) ← emacs(F, X){X/tex} yields ls(tex) ← emacs(F, tex).
A substitution σ is called a unifier for a finite set S of atoms if Sσ is singleton. A unifier θ
for S is called a most general unifier (MGU) for S if, for each unifier σ of S, there exists a
substitution γ such that σ = θγ. A term, atom or clause E is called ground when it contains
no variables, i.e., vars(E) = ∅. The Herbrand base of Σ, denoted as hbΣ , is the set of all
ground atoms constructed with the predicate and functor symbols in Σ. The set G Σ (A) of
an atom A consists of all ground atoms Aθ that belong to hbΣ .

3. Logical Hidden Markov Models
The logical component of a traditional HMM corresponds to a Mealy machine (Hopcroft
& Ullman, 1979), i.e., a finite state machine where the output symbols are associated with
426

Logical Hidden Markov Models

transitions. This is essentially a propositional representation because the symbols used to
represent states and output symbols are flat, i.e. not structured. The key idea underlying
LOHMMs is to replace these flat symbols by abstract symbols. An abstract symbol A is —
by definition — a logical atom. It is abstract in that it represents the set of all ground, i.e.,
variable-free atoms of A over the alphabet Σ, denoted by GΣ (A). Ground atoms then play
the role of the traditional symbols used in a HMMs.
Example 1 Consider the alphabet Σ1 which has as constant symbols tex, dvi, hmm1,
and lohmm1, and as relation symbols emacs/2, ls/1, xdvi/1, latex/2. Then the atom
emacs(File, tex) represents the set {emacs(hmm1, tex), emacs(lohmm1, tex)}. We assume
that the alphabet is typed to avoid useless instantiations such as emacs(tex, tex)).
The use of atoms instead of flat symbols allows us to analyze logical and structured sequences
such as emacs(hmm1, tex), latex(hmm1, tex), xdvi(hmm1, dvi).
O

Definition 1 Abstract transition are expressions of the form p : H ←
− B where p ∈ [0, 1],
and H, B and O are atoms. All variables are implicitly assumed to be universally quantified,
i.e., the scope of variables is a single abstract transition.
The atoms H and B represent abstract states and O represents an abstract output symbol.
O
The semantics of an abstract transition p : H ←
− B is that if one is in one of the states in
GΣ (B), say BθB , one will go with probability p to one of the states in GΣ (HθB ), say HθB θH ,
while emitting a symbol in GΣ (OθB θH ), say OθB θH θO .
latex(File)

Example 2 Consider c ≡ 0.8 : xdvi(File, dvi) ←−−−−−−− latex(File, tex). In general
H, B and O do not have to share the same predicate. This is only due to the nature of our running example. Assume now that we are in state latex(hmm1, tex), i.e.
θB = {File/hmm1}. Then c specifies that there is a probability of 0.8 that the next state
will be in GΣ1 (xdvi(hmm1, dvi)) = {xdvi(hmm1, dvi)} ( i.e., the probability is 0.8 that the
next state will be xdvi(hmm1, dvi)), and that one of the symbols in G Σ1 (latex(hmm1)) =
{latex(hmm1)} ( i.e., latex(hmm1)) will be emitted. Abstract states might also be more
complex such as latex(file(FileStem, FileExtension), User)
The above example was simple because θH and θO were both empty. The situation becomes more complicated when these substitutions are not empty. Then, the resulting
state and output symbol sets are not necessarily singletons. Indeed, for the transilatex(File)

tion 0.8 : emacs(File0 , dvi) ←−−−−−−− latex(File, tex) the resulting state set would be
GΣ1 (emacs(File0 , dvi)) = {emacs(hmm1, tex), emacs(lohmm1, tex)}. Thus the transition
is non-deterministic because there are two possible resulting states. We therefore need a
mechanism to assign probabilities to these possible alternatives.
Definition 2 The selection distribution µ specifies for each abstract state and observation
symbol A over the alphabet Σ a distribution µ(· | A) over GΣ (A).
To continue our example, let µ(emacs(hmm1, tex) | emacs(File0 , tex)) = 0.4 and
µ(emacs(lohmm1, tex) | emacs(File0 , tex)) = 0.6. Then there would be a probability of 0.4 × 0.8 = 0.32 that the next state is emacs(hmm1, tex) and of 0.48 that it is
emacs(lohmm1, tex).
427

Kersting, De Raedt, & Raiko

O

Taking µ into account, the meaning of an abstract transition p : H ←
− B can be summarized as follows. Let BθB ∈ GΣ (B), HθB θH ∈ GΣ (HθB ) and OθB θH θO ∈ GΣ (OθB θH ). Then the
model makes a transition from state BθB to HθB θH and emits symbol OθB θH θO with probability
p · µ(HθB θH | HθB ) · µ(OθB θH θO | OθB θH ).

(1)

To represent µ, any probabilistic representation can - in principle - be used, e.g. a Bayesian
network or a Markov chain. Throughout the remainder of the present paper, however,
we will use a naı̈ve Bayes approach. More precisely, we associate to each argument of a
r/m
r/m
relation r/m a finite domain Di
of constants and a probability distribution Pi
over
r/m
Di . Let vars(A) = {V1 , . . . , Vl } be the variables occurring in an atom A over r/m, and
let σ = {V1 /s1 , . . . Vl /sl } be a substitution grounding A. Each Vj is then considered a
r/m
random variable over the domain Darg(Vj ) of the argument arg(Vj ) it appears first in. Then,
Q
r/m
µ(Aσ | A) = lj=1 Parg(Vj ) (sj ). E.g. µ(emacs(hmm1, tex) | emacs(F, E)), is computed as the
emacs/2

emacs/2

product of P1
(hmm1) and P2
(tex).
Thus far the semantics of a single abstract transition has been defined. A LOHMM
usually consists of multiple abstract transitions and this creates a further complication.
Example 3 Consider

emacs(File)

0.8 : latex(File, tex) ←−−−−−−− emacs(File, tex)

and

emacs(File)

0.4 : dvi(File) ←−−−−−−− emacs(File, User).
These two abstract transitions make
conflicting statements about the state resulting from emacs(hmm1, tex). Indeed, according
to the first transition, the probability is 0.8 that the resulting state is latex(hmm1, tex) and
according to the second one it assigns 0.4 to xdvi(hmm1).
There are essentially two ways to deal with this situation. On the one hand, one might want
to combine and normalize the two transitions and assign a probability of 23 respectively 13 .
On the other hand, one might want to have only one rule firing. In this paper, we chose the
latter option because it allows us to consider transitions more independently, it simplifies
learning, and it yields locally interpretable models. We employ the subsumption (or generality) relation among the B-parts of the two abstract transitions. Indeed, the B-part of
the first transition B1 = emacs(File, tex) is more specific than that of the second transition B2 = emacs(File, User) because there exists a substitution θ = {User/tex} such that
B2 θ = B1 , i.e., B2 subsumes B1 . Therefore GΣ1 (B1 ) ⊆ GΣ1 (B2 ) and the first transition can
be regarded as more informative than the second one. It should therefore be preferred over
the second one when starting from emacs(hmm1, tex). We will also say that the first transition is more specific than the second one. Remark that this generality relation imposes a
partial order on the set of all transitions. These considerations lead to the strategy of only
considering the maximally specific transitions that apply to a state in order to determine
the successor states. This implements a kind of exception handling or default reasoning
and is akin to Katz’s (1987) back-off n-gram models. In back-off n-gram models, the most
detailed model that is deemed to provide sufficiently reliable information about the current
context is used. That is, if one encounters an n-gram that is not sufficiently reliable, then
back-off to use an (n − 1)-gram; if that is not reliable either then back-off to level n − 2, etc.
The conflict resolution strategy will work properly provided that the bodies of all maximally specific transitions (matching a given state) represent the same abstract state. This
428

Logical Hidden Markov Models

start

ls : 0.4

0.45

0.55
emacs(F) : 0.7
ls(U0 )

emacs(F, U)
emacs(F) : 0.3

ls : 0.6
emacs(F) : 0.3

emacs(F0 , U)

latex(F) : 0.2
emacs(F) : 0.1

latex(F) : 0.2
latex(F, tex)

emacs(F, tex)
emacs(F) : 0.6

latex(F) : 0.6

Figure 1: A logical hidden Markov model.

can be enforced by requiring the generality relation over the B-parts to be closed under the
greatest lower bound (glb) for each predicate, i.e., for each pair B1 , B2 of bodies, such that
θ = mgu(B1 , B2 ) exists, there is another body B (called lower bound) which subsumes B1 θ
(therefore also B2 θ) and is subsumed by B1 , B2 , and if there is any other lower bound then
it is subsumed by B. E.g., if the body of the second abstract transition in our example is
emacs(hmm1, User) then the set of abstract transitions would not be closed under glb.
Finally, in order to specify a prior distribution over states, we assume a finite set Υ of
clauses of the form p : H ← start using a distinguished start symbol such that p is the
probability of the LOHMM to start in a state of GΣ (H).
By now we are able to formally define logical hidden Markov models.
Definition 3 A logical hidden Markov model (LOHMM) is a tuple (Σ, µ, ∆, Υ) where Σ is
a logical alphabet, µ a selection probability over Σ, ∆ is a set of abstract transitions, and Υ
is a set of abstract transitions encoding a prior distribution. Let B be the set of all atoms
that occur as body parts of transitions in ∆. We assume B to be closed under glb and require
X
∀B ∈ B :
p = 1.0
(2)
O
p:H←
−B∈∆

and that the probabilities p of clauses in Υ sum up to 1.0 .

HMMs are a special cases of LOHMMs in which Σ contains only relation symbols of arity
zero and the selection probability is irrelevant. Thus, LOHMMs directly generalize HMMs.
LOHMMs can also be represented graphically. Figure 1 contains an example. The underlying language Σ2 consists of Σ1 together with the constant symbol other which denotes a
user that does not employ LATEX. In this graphical notation, nodes represent abstract states
and black tipped arrows denote abstract transitions. White tipped arrows are used to represent meta knowledge. More precisely, white tipped, dashed arrows represent the generality or
subsumption ordering between abstract states. If we follow a transition to an abstract state
with an outgoing white tipped, dotted arrow then this dotted arrow will always be followed.
Dotted arrows are needed because the same abstract state can occur under different cirlatex(File)
cumstances. Consider the transition p : latex(File0 , User0 ) ←−−−−−−− latex(File, User).
429

Kersting, De Raedt, & Raiko

0.6

start

0.45

em(F, U)

µ

em(F, t)

state

abstract state

abstract state
0.4

ls

µ
ls(t)

em(f1 )

em(f1 , t)

1.0

la(F, t)

la(f1 , t)

abstract state

state

ls(U0 )

em(f2 )

state abstract state

0.6

la(f1 )

0.7

em(f2 , o)
state

µ

em(F, U)
abstract state

em(F0 , U)
abstract state

Figure 2: Generating
the
observation
sequence
emacs(hmm1), latex(hmm1),
emacs(lohmm1), ls by the LOHMM in Figure 1. The command emacs is
abbreviated by em, f1 denotes the filename hmm1, f2 represents lohmm1, t denotes
a tex user, and o some other user. White tipped solid arrows indicate selections.

Even though the atoms in the head and body of the transition are syntactically different they
represent the same abstract state. To accurately represent the meaning of this transition we
cannot use a black tipped arrow from latex(File, User) to itself, because this would actulatex(File)

ally represent the abstract transition p : latex(File, User) ←−−−−−−− latex(File, User).
Furthermore, the graphical representation clarifies that LOHMMs are generative models. Let us explain how the model in Figure 1 would generate the observation sequence
emacs(hmm1), latex(hmm1), emacs(lohmm1), ls (cf. Figure 2). It chooses an initial abstract state, say emacs(F, U). Since both variables F and U are uninstantiated, the model
samples the state emacs(hmm1, tex) from GΣ2 using µ. As indicated by the dashed arrow, emacs(F, tex) is more specific than emacs(F, U). Moreover, emacs(hmm1, tex) matches
emacs(F, tex). Thus, the model enters emacs(F, tex). Since the value of F was already
instantiated in the previous abstract state, emacs(hmm1, tex) is sampled with probability
1.0. Now, the model goes over to latex(F, tex), emitting emacs(hmm1) because the abstract
observation emacs(F) is already fully instantiated. Again, since F was already instantiated,
latex(hmm1, tex) is sampled with probability 1.0. Next, we move on to emacs(F 0 , U), emitting latex(hmm1). Variables F0 and U in emacs(F0 , U) were not yet bound; so, values, say
lohmm1 and others, are sampled from µ. The dotted arrow brings us back to emacs(F, U).
Because variables are implicitly universally quantified in abstract transitions, the scope of
variables is restricted to single abstract transitions. In turn, F is treated as a distinct,
new variable, and is automatically unified with F0 , which is bound to lohmm1. In contrast,
variable U is already instantiated. Emitting emacs(lohmm1), the model makes a transition
to ls(U0 ). Assume that it samples tex for U0 . Then, it remains in ls(U0 ) with probability
0.4 . Considering all possible samples, allows one to prove the following theorem.
Theorem 1 (Semantics) A logical hidden Markov model over a language Σ defines a
discrete time stochastic process, i.e., a sequence of random variables hX t it=1,2,... , where the
domain
of Xt is hb(Σ) × hb(Σ). The induced probability measure over the Cartesian product
N
t hb(Σ) × hb(Σ) exists and is unique for each t > 0 and in the limit t → ∞.
Before concluding this section, let us address some design choices underlying LOHMMs.
First, LOHMMs have been introduced as Mealy machines, i.e., output symbols are
associated with transitions. Mealy machines fit our logical setting quite intuitively as they
directly encode the conditional probability P (O, S0 |S) of making a transition from S to S0
430

Logical Hidden Markov Models

emitting an observation O. Logical hidden Markov models define this distribution as
X
P (O, S0 |S) =
p · µ(S0 | HσB ) · µ(O | O0 σB σH )
O0
p:H←−B
O0

where the sum runs over all abstract transitions H ←− B such that B is most specific for S.
Observations correspond to (partially) observed proof steps and, hence, provide information
shared among heads and bodies of abstract transitions. In contrast, HMMs are usually
introduced as Moore machines. Here, output symbols are associated with states implicitly
assuming O and S0 to be independent. Thus, P (O, S0 | S) factorizes into P (O | S) · P (S0 | S).
This makes it more difficult to observe information shared among heads and bodies. In
turn, Moore-LOHMMs are less intuitive and harder to understand. For a more detailed
discussion of the issue, we refer to Appendix B where we essentially show that – as in the
propositional case – Mealy- and Moore-LOHMMs are equivalent.
Second, the naı̈ve Bayes approach for the selection distribution reduces the model complexity at the expense of a lower expressivity: functors are neglected and variables are
treated independently. Adapting more expressive approaches is an interesting future line of
research. For instance, Bayesian networks allow one to represent factorial HMMs (Ghahramani & Jordan, 1997). Factorial HMMs can be viewed as LOHMMs, where the hidden
states are summarized by a 2 · k-ary abstract state. The first k arguments encode the k
state variables, and the last k arguments serve as a memory of the previous joint state. µ
of the i-th argument is conditioned on the i + k-th argument. Markov chains allow one to
sample compound terms of finite depth such as s(s(s(0))) and to model e.g. misspelled
filenames. This is akin to generalized HMMs (Kulp, Haussler, Reese, & Eeckman, 1996), in
which each node may output a finite sequence of symbols rather than a single symbol.
Finally, LOHMMs – as introduced in the present paper – specify a probability distribution over all sequences of a given length. Reconsider the LOHMM in Figure 1. Already the probabilities of all observation sequences of length 1, i.e., ls, emacs(hmm1),
and
P
emacs(lohmm1)) sum up to 1. More precisely, for each t > 0 it holds that x1 ,...,xt P (X1 =
x1 , . .P
. , Xt P
= xt ) = 1.0 . In order to model a distribution over sequences of variable length,
i.e., t>0 x1 ,...,xt P (X1 = x1 , . . . , Xt = xt ) = 1.0 we may add a distinguished end state.
The end state is absorbing in that whenever the model makes a transition into this state,
it terminates the observation sequence generated.

4. Three Inference Problems for LOHMMs
As for HMMs, three inference problems are of interest. Let M be a LOHMM and let
O = O1 , O2 , . . . , OT , T > 0, be a finite sequence of ground observations:
(1) Evaluation: Determine the probability P (O | M ) that sequence O was generated by
the model M .
(2) Most likely state sequence: Determine the hidden state sequence S∗ that has most
likely produced the observation sequence O, i.e. S∗ = arg maxS P (S | O, M ) .
(3) Parameter estimation: Given a set O = {O1 , . . . , Ok } of observation sequences, determine the most likely parameters λ∗ for the abstract transitions and the selection
O | λ) .
distribution of M , i.e. λ∗ = arg maxλ P (O
431

PSfrag replacements
Kersting, De Raedt, & Raiko

sc(1)
abstract selection abstract
transition
transition

selection

abstract selection
sc(2)
transition

sc(Y)
ls(o)
ls(o)

ls(o)

ls(U’)
ls(t)

hc(1)

ls(o)
ls(t)

ls(t)

hc(2)

ls(t)
ls(U’)

start

hc(X)

...

ls(U’)
em(f2,o)

sc(Z)

em(f1,o)

em(F’,U)

em(f1,t)

em(F’,o)

em(f2,t)

latex(f1,t)

latex(f1,t) latex(f2,t)

em(F’,U)

O1

em(F,U)

O2
O2

abstract state

S2

S1

S0

em(F’,o)

states

Figure 3: Trellis induced by the LOHMM in Figure 1. The sets of reachable states at time
0, 1, . . . are denoted by S0 , S1 , . . . In contrast with HMMs, there is an additional
layer where the states are sampled from abstract states.

We will now address each of these problems in turn by upgrading the existing solutions for
HMMs. This will be realized by computing a grounded trellis as in Figure 3. The possible
ground successor states of any given state are computed by first selecting the applicable
abstract transitions and then applying the selection probabilities (while taking into account
the substitutions) to ground the resulting states. This two-step factorization is coalesced
into one step for HMMs.
To evaluate O, consider the probability of the partial observation sequence O 1 , O2 , . . . , Ot
and (ground) state S at time t, 0 < t ≤ T , given the model M = (Σ, µ, ∆, Υ)
αt (S) := P (O1 , O2 , . . . , Ot , qt = S | M )
where qt = S denotes that the system is in state S at time t. As for HMMs, αt (S) can be computed using a dynamic programming approach. For t = 0, we set α0 (S) = P (q0 = S | M ) ,
i.e., α0 (S) is the probability of starting in state S and, for t > 0, we compute αt (S) based
on αt−1 (S0 ):
1: S0 := {start}
2: for t = 1, 2, . . . , T do
3:
St = ∅
4:
foreach S ∈ St−1 do
5:
6:
7:
8:
9:

/* initialize the set of reachable states*/
/* initialize the set of reachable states at clock t*/
O

foreach maximally specific p : H ←
− B ∈ ∆ ∪ Υ s.t. σB = mgu(S, B) exists do
foreach S0 = HσB σH ∈ GΣ (HσB ) s.t. Ot−1 unifies with OσB σH do
if S0 6∈ St then
St := St ∪ {S0 }
αt (S0 ) := 0.0

αt (S0 ) := αt (S0 ) + αt−1 (S) · p ·
P
11: return P (O | M ) = S∈ST αT (S)

10:

432

µ(S0 | HσB ) · µ(Ot−1 | OσB σH )

Logical Hidden Markov Models

where we assume for the sake of simplicity O ≡ start for each abstract transition p : H ←
start ∈ Υ. Furthermore, the boxed parts specify all the differences to the HMM formula:
unification and µ are taken into account.
P
Clearly, as for HMMs P (O | M ) = S∈ST αT (S) holds. The computational complexity
of this forward procedure is O(T · s · (|B| + o · g)) = O(T · s2 ) where s = maxt=1,2,...,T |St | ,
o is the maximal number of outgoing abstract transitions with regard to an abstract state,
and g is the maximal number of ground instances of an abstract state. In a completely
analogous manner, one can devise a backward procedure to compute
βt (S) = P (Ot+1 , Ot+2 , . . . , OT | qt = S, M ) .
This will be useful for solving Problem (3).
Having a forward procedure, it is straightforward to adapt the Viterbi algorithm as a
solution to Problem (2), i.e., for computing the most likely state sequence. Let δt (S)
denote the highest probability along a single path at time t which accounts for the first t
observations and ends in state S, i.e.,
δt (S) =

max

S0 ,S1 ,...,St−1

P (S0 , S1 , . . . , St−1 , St = S, O1 , . . . , Ot−1 |M ) .

The procedure for finding the most likely state sequence basically follows the forward procedure. Instead of summing over all ground transition probabilities in line 10, we maximize
over them. More precisely, we proceed as follows:
1:S0 := {start}
/* initialize the set of reachable states*/
2: for t = 1, 2, . . . , T do
3:
St = ∅
/* initialize the set of reachable states at clock t*/
foreach S ∈ St−1 do
4:
O
5:
foreach maximally specific p : H ←
− B ∈ ∆ ∪ Υ s.t. σB = mgu(S, B) exists do
foreach S0 = HσB σH ∈ GΣ (HσB ) s.t. Ot−1 unifies with OσB σH do
6:
if S0 6∈ St then
7:
8:
St := St ∪ {S0 }
δt (S, S0 ) := 0.0
9:
10:
δt (S, S0 ) := δt (S, S0 ) + δt−1 (S) · p · µ(S0 | HσB ) · µ(Ot−1 | OσB σH )
11:
foreach S0 ∈ St do
12:
δt (S0 ) = maxS∈St−1 δt (S, S0 )
13:
ψt (S0 ) = arg maxS∈St−1 ψt (S, S0 )
Here, δt (S, S0 ) stores the probability of making a transition from S to S0 and ψt (S0 ) (with
ψ1 (S) = start for all states S) keeps track of the state maximizing the probability along
a single path at time t which accounts for the first t observations and ends in state S 0 . The
most likely hidden state sequence S∗ can now be computed as
S∗T +1 = arg max δT +1 (S)
and

S∗t

=

S∈ST +1
ψt (S∗t+1 ) for

t = T, T − 1, . . . , 1 .

One can also consider problem (2) on a more abstract level. Instead of considering all
contributions of different abstract transitions T to a single ground transition from state S
433

Kersting, De Raedt, & Raiko

to state S0 in line 10, one might also consider the most likely abstract transition only. This
is realized by replacing line 10 in the forward procedure with
αt (S0 ) := max(αt (S0 ), αt−1 (S) · p · µ(S0 | HσB ) · µ(Ot−1 | OσB σH )) .
This solves the problem of finding the (20 ) most likely state and abstract transition
sequence:
Determine the sequence of states and abstract transitions GT∗ =
S0 , T0 , S1 , T1 , S2 , . . . , ST , TT , ST+1 where there exists substitutions θi with Si+1 ←
Si ≡ Ti θi that has most likely produced the observation sequence O, i.e.
GT∗ = arg maxGT P (GT | O, M ) .
Thus, logical hidden Markov models also pose new types of inference problems.
For parameter estimation, we have to estimate the maximum likelihood transition
probabilities and selection distributions. To estimate the former, we upgrade the well-known
Baum-Welch algorithm (Baum, 1972) for estimating the maximum likelihood parameters
of HMMs and probabilistic context-free grammars.
For HMMs, the Baum-Welch algorithm computes the improved estimate p of the tranO
sition probability of some (ground) transition T ≡ p : H ←
− B by taking the ratio
p= P

ξ(T)
H0

O0

←−B∈∆∪Υ

ξ(T0 )

(3)

between the expected number ξ(T) of times of making the transitions T at any time given
the model M and an observation sequence O, and the total number of times a transitions
is made from B at any time given M and O.
Basically the same applies when T is an abstract transition. However, we have to be
a little bit more careful because we have no direct access to ξ(T). Let ξ t (gcl, T) be the
GO
probability of following the abstract transition T via its ground instance gcl ≡ p : GH ←−− GB
at time t, i.e.,
ξt (gcl, T) =

αt (GB) · p · βt+1 (GH)
· µ(GH | HσB ) · µ(Ot−1 | OσB σH ) ,
P (O | M )

(4)

where σB , σH are as in the forward procedure (see above) and P (O | M ) is the probability
that the model generated the sequence O. Again, the boxed terms constitute the main
difference to the corresponding HMM formula. In order to apply Equation (3) to compute
improved estimates of probabilities associated with abstract transitions, we set
ξ(T) =

T
X
t=1

ξt (T) =

T X
X

ξt (gcl, T)

t=1 gcl

where the inner sum runs over all ground instances of T.
This leads to the following re-estimation method, where we assume that the sets S i of
reachable states are reused from the computations of the α- and β-values:
434

Logical Hidden Markov Models

1:
2:
3:
4:
5:
6:
7:
8:
9:

/* initialization of expected counts */
foreach T ∈ ∆ ∪ Υ do
ξ(T) := m /* or 0 if not using pseudocounts */
/* compute expected counts */
for t = 0, 1, . . . , T do
foreach S ∈ St do
O

foreach max. specific T ≡ p : H ←
− B ∈ ∆ ∪ Υ s.t. σB = mgu(S, B) exists do
foreach S0 = HσB σH ∈ GΣ (HσB ) s.t. S0 ∈ St+1 ∧ mgu(Ot , OσB σH ) exists do

ξ(T) := ξ(T) + αt (S) · p · βt+1 (S0 ) P (O | M )· µ(S0 | HσB ) · µ(Ot−1 | OσB σH )

Here, equation (4) can be found in line 9. In line 3, we set pseudocounts as small samplesize regularizers. Other methods to avoid a biased underestimate of probabilities and even
zero probabilities such as m-estimates (see e.g., Mitchell, 1997) can be easily adapted.
To estimate the selection probabilities, recall that µ follows a naı̈ve Bayes scheme. Therefore, the estimated probability for a domain element d ∈ D for some domain D is the ratio
between the number of times d is selected and the number of times any d0 ∈ D is selected.
The procedure for computing the ξ-values can thus be reused.
Altogether, the Baum-Welch algorithm works as follows: While not converged, (1) estimate the abstract transition probabilities, and (2) the selection probabilities. Since it is
an instance of the EM algorithm, it increases the likelihood of the data with every update,
and according to McLachlan and Krishnan (1997), it is guaranteed to reach a stationary
point. All standard techniques to overcome limitations of EM algorithms are applicable.
The computational complexity (per iteration) is O(k · (α + d)) = O(k · T · s2 + k · d) where
k is the number of sequences, α is the complexity of computing the α-values (see above),
and d is the sum over the sizes of domains associated to predicates. Recently, Kersting
and Raiko (2005) combined the Baum-Welch algorithm with structure search for model
selection of logical hidden Markov models using inductive logic programming (Muggleton
& De Raedt, 1994) refinement operators. The refinement operators account for different
abstraction levels which have to be explored.

5. Advantages of LOHMMs
In this section, we will investigate the benefits of LOHMMs: (1) LOHMMs are strictly
more expressive than HMMs, and (2), using abstraction, logical variables and unification
can be beneficial. More specifically, with (2), we will show that
(B1) LOHMMs can be — by design — smaller than their propositional instantiations, and
(B2) unification can yield better log-likelihood estimates.
5.1 On the Expressivity of LOHMMs
Whereas HMMs specify probability distributions over regular languages, LOHMMs specify
probability distributions over more expressive languages.

435

Kersting, De Raedt, & Raiko

Theorem 2 For any (consistent) probabilistic context-free grammar (PCFG) G for some
language L there exists a LOHMM M s.t. PG (w) = PM (w) for all w ∈ L.
The proof (see Appendix C) makes use of abstract states of unbounded ’depth’. More
precisely, functors are used to implement a stack. Without functors, LOHMMs cannot
encode PCFGs and, because the Herbrand base is finite, it can be proven that there always
exists an equivalent HMM.
Furthermore, if functors are allowed, LOHMMs are strictly more expressive than PCFGs.
They can specify probability distributions over some languages that are context-sensitive:
1.0 :
stack(s(0), s(0)) ←
a
0.8 :
stack(s(X), s(X)) ←
−
a
0.2 : unstack(s(X), s(X)) ←
−
b
1.0 :
unstack(X, Y) ←
−
c
1.0 :
unstack(s(0), Y) ←
−
end
1.0 :
end ←−−

start
stack(X, X)
stack(X, X)
unstack(s(X), Y)
unstack(s(0), s(Y))
unstack(s(0), s(0))

The LOHMM defines a distribution over {an bn cn | n > 0}.
Finally, the use of logical variables also enables one to deal with identifiers. Identifiers
are special types of constants that denote objects. Indeed, recall the UNIX command
sequence emacs lohmms.tex, ls, latex lohmms.tex, . . . from the introduction. The filename
lohmms.tex is an identifier. Usually, the specific identifiers do not matter but rather the
fact that the same object occurs multiple times in the sequence. LOHMMs can easily deal
with identifiers by setting the selection probability µ to a constant for the arguments in
which identifiers can occur. Unification then takes care of the necessary variable bindings.
5.2 Benefits of Abstraction through Variables and Unification
Reconsider the domain of UNIX command sequences. Unix users oftenly reuse a newly created directory in subsequent commands such as in mkdir(vt100x), cd(vt100x), ls(vt100x) .
Unification should allow us to elegantly employ this information because it allows us to specify that, after observing the created directory, the model makes a transition into a state
where the newly created directory is used:
p1 : cd(Dir, mkdir) ← mkdir(Dir, com)

and

p2 : cd( , mkdir) ← mkdir(Dir, com)

If the first transition is followed, the cd command will move to the newly created directory;
if the second transition is followed, it is not specified which directory cd will move to. Thus,
the LOHMM captures the reuse of created directories as an argument of future commands.
Moreover, the LOHMM encodes the simplest possible case to show the benefits of unification. At any time, the observation sequence uniquely determines the state sequence, and
functors are not used. Therefore, we left out the abstract output symbols associated with
abstract transitions. In total, the LOHMM U , modelling the reuse of directories, consists
of 542 parameters only but still covers more than 451000 (ground) states, see Appendix D
for the complete model. The compression in the number of parameters supports (B1).
To empirically investigate the benefits of unification, we compare U with the variant N
of U where no variables are shared, i.e., no unification is used such that for instance the
436

Logical Hidden Markov Models

first transition above is not allowed, see Appendix D. N has 164 parameters less than U .
We computed the following zero-one win function
(


1 if log PU (O) − log PN (O) > 0
f (O) =
0 otherwise
leave-one-out cross-validated on Unix shell logs collected by Greenberg (1988). Overall,
the data consists of 168 users of four groups: computer scientists, nonprogrammers, novices
and others. About 300000 commands have been logged with an average of 110 sessions
per user. We present here results for a subset of the data. We considered all computer
scientist sessions in which at least a single mkdir command appears. These yield 283 logical
sequences over in total 3286 ground atoms. The LOO win was 81.63%. Other LOO statistics
are also in favor of U :

U
N

training
O)
O) log PPU (O
log P (O
O)
N (O
−11361.0
1795.3
−13157.0

test
log P (O) log PPNU (O)
(O)
−42.8
7.91
−50.7

Thus, although U has 164 parameters more than N , it shows a better generalization performance. This result supports (B2). A pattern often found in U was 1
0.15 : cd(Dir, mkdir) ← mkdir(Dir, com)

and

0.08 : cd( , mkdir) ← mkdir(Dir, com)

favoring changing to the directory just made. This knowledge cannot be captured in N
0.25 : cd( , mkdir) ← mkdir(Dir, com).
The results clearly show that abstraction through variables and unification can be beneficial
for some applications, i.e., (B1) and (B2) hold.

6. Real World Applications
Our intentions here are to investigate whether LOHMMs can be applied to real world
domains. More precisely, we will investigate whether benefits (B1) and (B2) can also be
exploited in real world application domains. Additionally, we will investigate whether
(B3) LOHMMs are competitive with ILP algorithms that can also utilize unification and
abstraction through variables, and
(B4) LOHMMs can handle tree-structured data similar to PCFGs.
To this aim, we conducted experiments on two bioinformatics application domains: protein
fold recognition (Kersting, Raiko, Kramer, & De Raedt, 2003) and mRNA signal structure
detection (Horváth, Wrobel, & Bohnebeck, 2001). Both application domains are multiclass
problems with five different classes each.
1. The sum of probabilities is not the same (0.15 + 0.08 = 0.23 6= 0.25) because of the use of pseudo counts
and because of the subliminal non-determinism (w.r.t. abstract states) in U , i.e., in case that the first
transition fires, the second one also fires.

437

Kersting, De Raedt, & Raiko

6.1 Methodology
In order to tackle the multiclass problem with LOHMMs, we followed a plug-in estimate
approach. Let {c1 , c2 , . . . , ck } be the set of possible classes. Given a finite set of training
examples {(xi , yi )}ni=1 ⊆ X × {c1 , c2 , . . . , cn }, one tries to find f : X → {c1 , c2 , . . . , ck }
f (x) = arg

max

c∈{c1 ,c2 ,...,ck }

P (x | M, λ∗c ) · P (c) .

(5)

with low approximation error on the training data as well as on unseen examples. In
Equation (5), M denotes the model structure which is the same for all classes, λ ∗c denotes
the maximum likelihood parameters of M for class c estimated on the training examples
with yi = c only, and P (c) is the prior class distribution.
We implemented the Baum-Welch algorithm (with pseudocounts m, see line 3) for maximum likelihood parameter estimation using the Prolog system Yap-4.4.4. In all experiments,
we set m = 1 and let the Baum-Welch algorithm stop if the change in log-likelihood was
less than 0.1 from one iteration to the next. The experiments were ran on a Pentium-IV
3.2 GHz Linux machine.
6.2 Protein Fold Recognition
Protein fold recognition is concerned with how proteins fold in nature, i.e., their threedimensional structures. This is an important problem as the biological functions of proteins
depend on the way they fold. A common approach is to use database searches to find proteins (of known fold) similar to a newly discovered protein (of unknown fold). To facilitate
protein fold recognition, several expert-based classification schemes of proteins have been
developed that group the current set of known protein structures according to the similarity
of their folds. For instance, the structural classification of proteins (Hubbard, Murzin, Brenner, & Chotia, 1997) (SCOP) database hierarchically organizes proteins according to their
structures and evolutionary origin. From a machine learning perspective, SCOP induces a
classification problem: given a protein of unknown fold, assign it to the best matching group
of the classification scheme. This protein fold classification problem has been investigated
by Turcotte, Muggleton, and Sternberg (2001) based on the inductive logic programming
(ILP) system PROGOL and by Kersting et al. (2003) based on LOHMMs.
The secondary structure of protein domains2 can elegantly be represented as logical sequences. For example, the secondary structure of the Ribosomal protein L4 is represented as
st(null, 2), he(right, alpha, 6), st(plus, 2), he(right, alpha, 4), st(plus, 2),
he(right, alpha, 4), st(plus, 3), he(right, alpha, 4), st(plus, 1), he(hright, alpha, 6)
Helices of a certain type, orientation and length he(HelixType, HelixOrientation, Length),
and strands of a certain orientation and length st(StrandOrientation, Length) are atoms over
logical predicates. The application of traditional HMMs to such sequences requires one to
either ignore the structure of helices and strands, which results in a loss of information, or to
take all possible combinations (of arguments such as orientation and length) into account,
which leads to a combinatorial explosion in the number of parameters
2. A domain can be viewed as a sub-section of a protein which appears in a number of distantly related
proteins and which can fold independently of the rest of the protein.

438

Logical Hidden Markov Models

end
Block B of length 3

Block s(B) of length 2

Dynamics within block

Dynamics within block

block(B, s(P))

block(s(B), s(P))

block(B, P)

block(s(B), P)
Transition to next block

Transition to next block

block(s(B), s(0))

block(B, s(s(s(0))))

block(B, 0)

block(s(B), 0)

Figure 4: Scheme of a left-to-right LOHMM block model.
The results reported by Kersting et al. (2003) indicate that LOHMMs are well-suited
for protein fold classification: the number of parameters of a LOHMM can by an order of
magnitude be smaller than the number of a corresponding HMM (120 versus approximately
62000) and the generalization performance, a 74% accuracy, is comparable to Turcotte
et al.’s (2001) result based on the ILP system Progol, a 75% accuracy. Kersting et al.
(2003), however, do not cross-validate their results nor investigate – as it is common in
bioinformatics – the impact of primary sequence similarity on the classification accuracy. For
instance, the two most commonly requested ASTRAL subsets are the subset of sequences
with less than 95% identity to each other (95 cut) and with less than 40% identity to each
other (40 cut). Motivated by this, we conducted the following new experiments.
The data consists of logical sequences of the secondary structure of protein domains. As
in the work of Kersting et al. (2003), the task is to predict one of the five most populated
SCOP folds of alpha and beta proteins (a/b): TIM beta/alpha-barrel (fold 1), NAD(P)binding Rossmann-fold domains (fold 2), Ribosomal protein L4 (fold 23), Cysteine hydrolase
(fold 37), and Phosphotyrosine protein phosphatases I-like (fold 55). The class of a/b
proteins consists of proteins with mainly parallel beta sheets (beta-alpha-beta units). The
data have been extracted automatically from the ASTRAL dataset version 1.65 (Chandonia,
Hon, Walker, Lo Conte, P.Koehl, & Brenner, 2004) for the 95 cut and for the 40 cut. As
in the work of Kersting et al. (2003), we consider strands and helices only, i.e., coils and
isolated strands are discarded. For the 95 cut, this yields 816 logical sequences consisting
of in total 22210 ground atoms. The number of sequences in the classes are listed as 293,
151, 87, 195, and 90. For the 40 cut, this yields 523 logical sequences consisting of in total
14986 ground atoms. The number of sequences in the classes are listed as 182, 100, 66, 122,
and 53.
LOHMM structure: The used LOHMM structure follows a left-to-right block topology,
see Figure 4, to model blocks of consecutive helices (resp. strands). Being in a Block of
some size s, say 3, the model will remain in the same block for s = 3 time steps. A similar
idea has been used to model haplotypes (Koivisto, Perola, Varilo, Hennah, Ekelund, Lukk,
Peltonen, Ukkonen, & Mannila, 2002; Koivisto, Kivioja, Mannila, Rastas, & Ukkonen,
2004). In contrast to common HMM block models (Won, Prügel-Bennett, & Krogh, 2004),
439

Kersting, De Raedt, & Raiko

the transition parameters are shared within each block and one can ensure that the model
makes a transition to the next state s(Block ) only at the end of a block; in our example
after exactly 3 intra-block transitions. Furthermore, there are specific abstract transitions
for all helix types and strand orientations to model the priori distribution, the intra- and
the inter-block transitions. The number of blocks and their sizes were chosen according
to the empirical distribution over sequence lengths in the data so that the beginning and
the ending of protein domains was likely captured in detail. This yield the following block
structure
1 2

...

19 20

27 28

...

40 41

46 47

61 62

76 77

where the numbers denote the positions within protein domains. Furthermore, note that
the last block gathers all remaining transitions. The blocks themselves are modelled using
hidden abstract states over
hc(HelixType, HelixOrientation, Length, Block ) and sc(StrandOrientation, Length, Block ) .
Here, Length denotes the number of consecutive bases the structure element consists of.
The length was discretized into 10 bins such that the original lengths were uniformally
distributed. In total, the LOHMM has 295 parameters. The corresponding HMM without
parameter sharing has more than 65200 parameters. This clearly confirms (B1).
Results: We performed a 10-fold cross-validation. On the 95 cut dataset, the accuracy was
76% and took approx. 25 minutes per cross-validation iteration; on the 40 cut, the accuracy
was 73% and took approx. 12 minutes per cross-validation iteration. The results validate
Kersting et al.’s (2003) results and, in turn, clearly show that (B3) holds. Moreover, the
novel results on the 40 cut dataset indicate that the similarities detected by the LOHMMs
between the protein domain structures were not accompanied by high sequence similarity.
6.3 mRNA Signal Structure Detection
mRNA sequences consist of bases (guanine, adenine, uracil, cytosine) and fold intramolecularly to form a number of short base-paired stems (Durbin, Eddy, Krogh, & Mitchison,
1998). This base-paired structure is called the secondary structure, cf. Figures 5 and 6. The
secondary structure contains special subsequences called signal structures that are responsible for special biological functions, such as RNA-protein interactions and cellular transport.
The function of each signal structure class is based on the common characteristic binding
site of all class elements. The elements are not necessarily identical but very similar. They
can vary in topology (tree structure), in size (number of constituting bases), and in base
sequence.
The goal of our experiments was to recognize instances of signal structures classes in
mRNA molecules. The first application of relational learning to recognize the signal structure class of mRNA molecules was described in the works of Bohnebeck, Horváth, and
Wrobel (1998) and of Horváth et al. (2001), where the relational instance-based learner
RIBL was applied. The dataset 3 we used was similar to the one described by Horváth
3. The dataset is not the same as described in the work by Horváth et al. (2001) because we could not obtain
the original dataset. We will compare to the smaller data set used by Horváth et al., which consisted of

440

Logical Hidden Markov Models

et al. (2001). It consisted of 93 mRNA secondary structure sequences. More precisely, it was
composed of 15 and 5 SECIS (Selenocysteine Insertion Sequence), 27 IRE (Iron Responsive
Element), 36 TAR (Trans Activating Region) and 10 histone stem loops constituting five
classes.
The secondary structure is composed of different building blocks such as stacking region,
hairpin loops, interior loops etc. In contrast to the secondary structure of proteins that forms
chains, the secondary structure of mRNA forms a tree. As trees can not easily be handled
using HMMs, mRNA secondary structure data is more challenging than that of proteins.
Moreover, Horváth et al. (2001) report that making the tree structure available to RIBL
as background knowledge had an influence on the classification accuracy. More precisely,
using a simple chain representation RIBL achieved a 77.2% leave-one-out cross-validation
(LOO) accuracy whereas using the tree structure as background knowledge RIBL achieved
a 95.4% LOO accuracy.
We followed Horváth et al.’s experimental setup, that is, we adapted their data representations to LOHMMs and compared a chain model with a tree model.

Chain Representation: In the chain representation (see also Figure 5),
signal
structures
are
described
by
single(TypeSingle, Position, Acid )
or
helical(TypeHelical , Position, Acid , Acid ).
Depending on its type, a structure element is represented by either single/3 or helical/4.
Their first argument
TypeSingle (resp.
TypeHelical ) specifies the type of the structure element, i.e.,
single, bulge3, bulge5, hairpin (resp. stem). The argument Position is the position of the sequence element within the corresponding structure element counted down,
i.e.4 , {n13 (0), n12 (0), . . . , n1 (0)}. The maximal position was set to 13 as this was the
maximal position observed in the data. The last argument encodes the observed nucleotide
(pair).
The used LOHMM structure follows again the left-to-right block structure shown in
Figure 4. Its underlying idea is to model blocks of consecutive helical structure elements. The hidden states are modelled using single(TypeSingle, Position, Acid , Block )
and helical(TypeHelical , Position, Acid , Acid , Block ). Being in a Block of consecutive helical (resp. single) structure elements, the model will remain in the Block or transition to a
single element. The transition to a single (resp. helical) element only occurs at Position
n(0). At all other positions n(Position), there were transitions from helical (resp. single)
structure elements to helical (resp. single) structure elements at Position capturing the dynamics of the nucleotide pairs (resp. nucleotides) within structure elements. For instance,

66 signal structures and is very close to our data set. On a larger data set (with 400 structures) Horváth
et al. report an error rate of 3.8% .
4. nm (0) is shorthand for the recursive application of the functor n on 0 m times, i.e., for position m.

441

Kersting, De Raedt, & Raiko

helical(stem, n(0), c, g).
helical(stem, n(n(0)), c, g).
helical(stem, n(n(n(0))), c, g).
single(bulge5, n(0), a).
single(bulge5, n(n(0)), a).
single(bulge5, n(n(n(0))), g).
helical(stem, n(0), c, g).
helical(stem, n(n(0)), c, g).
single(bulge5, n(0), a).
helical(stem, n(0), a, a).
helical(stem, n(n(0)), u, a).
helical(stem, n(n(n(0))), u, g).
helical(stem, n(n(n(n(0)))), u, a).
helical(stem, n(n(n(n(n(0))))), c, a).
helical(stem, n(n(n(n(n(n(0)))))), u, a).
helical(stem, n(n(n(n(n(n(n(0))))))), a, u).

u
a

u
c
c
c

g
g
g

a
a
g

a
c
c

single(hairpin, n(n(n(0))), a).
single(hairpin, n(n(0)), u).
single(hairpin, n(0), u).

single(bulge3, n(0), a).

g
g

a
a
u
u
u
c
u
a

a
a
g
a
a
a
u

Figure 5: The chain representation of a SECIS signal structure. The ground atoms are
ordered clockwise starting with helical(stem, n(n(n(n(n(n(n(0))))))), a, u) at the
lower left-hand side corner.

the transitions for block n(0) at position n(n(0)) were
pa :he(stem,n(0),X,Y)

a : he(stem, n(0), X, Y, n(0)) ←−−−−−−−−−−−− he(stem, n(n(0)), X, Y, n(0)))
pb :he(stem,n(0),X,Y)

b:

he(stem, n(0), Y, X, n(0)) ←−−−−−−−−−−−− he(stem, n(n(0)), X, Y, n(0)))

c:

he(stem, n(0), X, , n(0)) ←−−−−−−−−−−−− he(stem, n(n(0)), X, Y, n(0)))

d:

he(stem, n(0), , Y, n(0)) ←−−−−−−−−−−−− he(stem, n(n(0)), X, Y, n(0)))

e:

he(stem, n(0), , , n(0)) ←−−−−−−−−−−−− he(stem, n(n(0)), X, Y, n(0)))

pc :he(stem,n(0),X,Y)

pd :he(stem,n(0),X,Y)
pe :he(stem,n(0),X,Y)

In total, there were 5 possible blocks as this was the maximal number of blocks of consecutive
helical structure elements observed in the data. Overall, the LOHMM has 702 parameters.
In contrast, the corresponding HMM has more than 16600 transitions validating (B1).
Results: The LOO test log-likelihood was −63.7, and an EM iteration took on average
26 seconds.
Without the unification-based transitions b-d, i.e., using only the abstract transitions
pa :he(stem,n(0),X,Y)

a : he(stem, n(0), X, Y, n(0)) ←−−−−−−−−−−−− he(stem, n(n(0)), X, Y, n(0)))
e:

pe :he(stem,n(0),X,Y)

he(stem, n(0), , , n(0)) ←−−−−−−−−−−−− he(stem, n(n(0)), X, Y, n(0))),

the model has 506 parameters. The LOO test log-likelihood was −64.21, and an EM iteration took on average 20 seconds. The difference in LOO test log-likelihood is statistically
significant (paired t-test, p = 0.01).
Omitting even transition a, the LOO test log-likelihood dropped to −66.06, and the
average time per EM iteration was 18 seconds. The model has 341 parameters. The
difference in average LOO log-likelihood is statistically significant (paired t-test, p = 0.001).
The results clearly show that unification can yield better LOO test log-likelihoods, i.e.,
(B2) holds.
442

Logical Hidden Markov Models

nucleotide pair((c, g)).
nucleotide pair((c, g)).
nucleotide pair((c, g)).
helical(s(s(s(s(s(0))))), s(s(s(0))), [c], stem, n(n(n(0)))).
nucleotide(a).
nucleotide(a).
nucleotide(g).
single(s(s(s(s(0)))), s(s(s(0))), [], bulge5, n(n(n(0)))).
nucleotide pair((c, g)).
nucleotide pair((c, g)).
helical(s(s(s(0))), s(0), [c, c, c], stem, n(n(0))).
nucleotide(a).
single(s(s(0)), s(0), [], bulge5, n(0)).
nucleotide pair((a, a)).
nucleotide pair((u, a)).
nucleotide pair((u, g)).
nucleotide pair((u, a)).
nucleotide pair((c, a)).
nucleotide pair((u, a)).
nucleotide pair((a, u)).
helical(s(0), 0, [c, c], stem, n(n(n(n(n(n(n(0)))))))).

u
a

u
c
c
c

g
g
g

a
a
g

a
c
c

g
g

single(s(s(s(s(s(s(0)))))), s(s(s(s(s(0))))),
[], hairpin, n(n(n(0)))).
nucleotide(a).
nucleotide(u).
nucleotide(u).

single(s(s(s(s(s(s(s(0))))))), s(s(s(0))),
[], bulge3, n(0)).
nucleotide(a).

a
a
u
u
u
c
u
a

a
a
g
a
a
a
u

0
s(0)
s(s(0))s(s(s(0)))
s(s(s(s(0))))
s(s(s(s(s(s(s(0)))))))
s(s(s(s(s(0))))
s(s(s(s(s(s(0))))))

root(0, root, [c]).

Figure 6: The tree representation of a SECIS signal structure. (a) The logical sequence,
i.e., the sequence of ground atoms representing the SECIS signal structure. The
ground atoms are ordered clockwise starting with root(0, root, [c]) in the lower
left-hand side corner. (b) The tree formed by the secondary structure elements.

Tree Representation: In the tree representation (see Figure 6 (a)), the idea is to capture
the tree structure formed by the secondary structure elements, see Figure 6 (b). Each
training instance is described as a sequence of ground facts over
root(0, root, #Children),
helical(ID, ParentID, #Children, Type, Size),
nucleotide pair(BasePair ),
single(ID, ParentID, #Children, Type, Size),
nucleotide(Base) .
Here, ID and ParentID are natural numbers 0, s(0), s(s(0)), . . . encoding the childparent relation, #Children denotes the number5 of children [], [c], [c, c], . . ., Type is the
type of the structure element such as stem, hairpin, . . ., and Size is a natural number
0, n(0), n(n(0)), . . . Atoms root(0, root, #Children) are used to root the topology. The
maximal #Children was 9 and the maximal Size was 13 as this was the maximal value
observed in the data.
As trees can not easily be handled using HMMs, we used a LOHMM which basically
encodes a PCFG. Due to Theorem 2, this is possible. The used LOHMM structure can be
found in Appendix E. It processes the mRNA trees in in-order. Unification is only used for
parsing the tree. As for the chain representation, we used a Position argument in the hidden
states to encode the dynamics of nucleotides (nucleotide pairs) within secondary structure
5. Here, we use the Prolog short hand notation [·] for lists. A list either is the constant [] representing the
empty list, or is a compound term with functor ./2 and two arguments, which are respectively the head
and tail of the list. Thus [a, b, c] is the compound term .(a, .(b, .(c, []))).

443

Kersting, De Raedt, & Raiko

elements. The maximal Position was again 13. In contrast to the chain representation,
nucleotide pairs such as (a, u) are treated as constants. Thus, the argument BasePair
consists of 16 elements.
Results: The LOO test log-likelihood was −55.56. Thus, exploiting the tree structure
yields better probabilistic models. On average, an EM iteration took 14 seconds. Overall,
the result shows that (B4) holds.
Although the Baum-Welch algorithm attempts to maximize a different objective function, namely the likelihood of the data, it is interesting to compare LOHMMs and RIBL in
terms of classification accuracy.
Classification Accuracy: On the chain representation, the LOO accuracies of all
LOHMMs were 99% (92/93). This is a considerable improvement on RIBL’s 77.2% (51/66)
LOO accuracy for this representation. On the tree representation, the LOHMM also
achieved a LOO accuracy of 99% (92/93). This is comparable to RIBL’s LOO accuracy of
97% (64/66) on this kind of representation.
Thus, already the chain LOHMMs show marked increases in LOO accuracy when compared to RIBL (Horváth et al., 2001). In order to achieve similar LOO accuracies, Horváth
et al. (2001) had to make the tree structure available to RIBL as background knowledge.
For LOHMMs, this had a significant influence on the LOO test log-likelihood, but not on
the LOO accuracies. This clearly supports (B3). Moreover, according to Horváth et al.,
the mRNA application can also be considered a success in terms of the application domain,
although this was not the primary goal of our experiments. There exist also alternative
parameter estimation techniques and other models, such as covariance models (Eddy &
Durbin, 1994) or pair hidden Markov models (Sakakibara, 2003), that might have been
used as well as a basis for comparison. However, as LOHMMs employ (inductive) logic programming principles, it is appropriate to compare with other systems within this paradigm
such as RIBL.

7. Related Work
LOHMMs combine two different research directions. On the one hand, they are related to
several extensions of HMMs and probabilistic grammars. On the other hand, they are also
related to the recent interest in combining inductive logic programming principles with
probability theory (De Raedt & Kersting, 2003, 2004).
In the first type of approaches, the underlying idea is to upgrade HMMs and probabilistic
grammars to represent more structured state spaces.
Hierarchical HMMs (Fine, Singer, & Tishby, 1998), factorial HMMs (Ghahramani &
Jordan, 1997), and HMMs based on tree automata (Frasconi, Soda, & Vullo, 2002) decompose the state variables into smaller units. In hierarchical HMMs states themselves can be
HMMs, in factorial HMMs they can be factored into k state variables which depend on one
another only through the observation, and in tree based HMMs the represented probability
distributions are defined over tree structures. The key difference with LOHMMs is that
these approaches do not employ the logical concept of unification. Unification is essential
444

Logical Hidden Markov Models

because it allows us to introduce abstract transitions, which do not consist of more detailed
states. As our experimental evidence shows, sharing information among abstract states by
means of unification can lead to more accurate model estimation. The same holds for relational Markov models (RMMs) (Anderson, Domingos, & Weld, 2002) to which LOHMMs
are most closely related. In RMMs, states can be of different types, with each type described
by a different set of variables. The domain of each variable can be hierarchically structured.
The main differences between LOHMMs and RMMs are that RMMs do not either support
variable binding nor unification nor hidden states.
The equivalent of HMMs for context-free languages are probabilistic context-free grammars (PCFGs). Like HMMs, they do not consider sequences of logical atoms and do not
employ unification. Nevertheless, there is a formal resemblance between the Baum-Welch
algorithms for LOHMMs and for PCFGs. In case that a LOHMM encodes a PCFG both
algorithms are identical from a theoretical point of view. They re-estimate the parameters
as the ratio of the expected number of times a transition (resp. production) is used and the
expected number of times a transition (resp. production) might have been used. The proof
of Theorem 2 assumes that the PCFG is given in Greibach normal form6 (GNF) and uses a
pushdown automaton to parse sentences. For grammars in GNF, pushdown automata are
common for parsing. In contrast, the actual computations of the Baum-Welch algorithm
for PCFGs, the so called Inside-Outside algorithm (Baker, 1979; Lari & Young, 1990), is
usually formulated for grammars in Chomsky normal form7 . The Inside-Outside algorithm
can make use of the efficient CYK algorithm (Hopcroft & Ullman, 1979) for parsing strings.
An alternative to learning PCFGs from strings only is to learn from more structured data
such as skeletons, which are derivation trees with the nonterminal nodes removed (Levy &
Joshi, 1978). Skeletons are exactly the set of trees accepted by skeletal tree automata (STA).
Informally, an STA, when given a tree as input, processes the tree bottom up, assigning a
state to each node based on the states of that node’s children. The STA accepts a tree iff
it assigns a final state to the root of the tree. Due to this automata-based characterization
of the skeletons of derivation trees, the learning problem of (P)CFGs can be reduced to
the problem of an STA. In particular, STA techniques have been adapted to learning tree
grammars and (P)CFGs (Sakakibara, 1992; Sakakibara et al., 1994) efficiently.
PCFGs have been extended in several ways. Most closely related to LOHMMs are
unification-based grammars which have been extensively studied in computational linguistics. Examples are (stochastic) attribute-value grammars (Abney, 1997), probabilistic feature grammars (Goodman, 1997), head-driven phrase structure grammars (Pollard & Sag,
1994), and lexical-functional grammars (Bresnan, 2001). For learning within such frameworks, methods from undirected graphical models are used; see the work of Johnson (2003)
for a description of some recent work. The key difference to LOHMMs is that only nonterminals are replaced with structured, more complex entities. Thus, observation sequences of
flat symbols and not of atoms are modelled. Goodman’s probabilistic feature grammars are
an exception. They treat terminals and nonterminals as vectors of features. No abstraction
is made, i.e., the feature vectors are ground instances, and no unification can be employed.
6. A grammar is in GNF iff all productions are of the form A ← aV where A is a variable, a is exactly one
terminal and V is a string of none or more variables.
7. A grammar is in CNF iff every production is of the form A ← B, C or A ← a where A, B and C are variables,
and a is a terminal.

445

Kersting, De Raedt, & Raiko

con

mkdir
con
mkdir

mv

ls

cd

mv
con

vt100x

vt100x

ls
new∗

vt100x

vt100x

vt100x

new∗

(a)

cd

vt100x

(b)

vt100x

vt100x

Figure 7: (a) Each atom in the logical sequence mkdir(vt100x), mv(new∗, vt100x),
ls(vt100x), cd(vt100x) forms a tree. The shaded nodes denote shared labels
among the trees. (b) The same sequence represented as a single tree. The predicate con/2 represents the concatenation operator.

Therefore, the number of parameters that needs to be estimated becomes easily very large,
data sparsity is a serious problem. Goodman applied smoothing to overcome the problem.
LOHMMs are generally related to (stochastic) tree automata (see e.g., Carrasco, Oncina, and Calera-Rubio, 2001). Reconsider the Unix command sequence
mkdir(vt100x), mv(new∗, vt100x), ls(vt100x), cd(vt100x) . Each atom forms a tree, see
Figure 7 (a), and, indeed, the whole sequence of atoms also forms a (degenerated) tree,
see Figure 7 (b). Tree automata process single trees vertically, e.g., bottom-up. A state in
the automaton is assigned to every node in the tree. The state depends on the node label
and on the states associated to the siblings of the node. They do not focus on sequential
domains. In contrast, LOHMMs are intended for learning in sequential domains. They
process sequences of trees horizontally, i.e., from left to right. Furthermore, unification
is used to share information between consecutive sequence elements. As Figure 7 (b)
illustrates, tree automata can only employ this information when allowing higher-order
transitions, i.e., states depend on their node labels and on the states associated to
predecessors 1, 2, . . . levels down the tree.
In the second type of approaches, most attention has been devoted to developing highly
expressive formalisms, such as e.g. PCUP (Eisele, 1994), PCLP (Riezler, 1998), SLPs (Muggleton, 1996), PLPs (Ngo & Haddawy, 1997), RBNs (Jaeger, 1997), PRMs (Friedman,
Getoor, Koller, & Pfeffer, 1999), PRISM (Sato & Kameya, 2001), BLPs (Kersting & De
Raedt, 2001b, 2001a), and DPRMs (Sanghai, Domingos, & Weld, 2003). LOHMMs can be
seen as an attempt towards downgrading such highly expressive frameworks. Indeed, applying the main idea underlying LOHMMs to non-regular probabilistic grammar, i.e., replacing
flat symbols with atoms, yields – in principle – stochastic logic programs (Muggleton, 1996).
As a consequence, LOHMMs represent an interesting position on the expressiveness scale.
Whereas they retain most of the essential logical features of the more expressive formalisms,
they seem easier to understand, adapt and learn. This is akin to many contemporary consid446

Logical Hidden Markov Models

erations in inductive logic programming (Muggleton & De Raedt, 1994) and multi-relational
data mining (Džeroski & Lavrač, 2001).

8. Conclusions
Logical hidden Markov models, a new formalism for representing probability distributions
over sequences of logical atoms, have been introduced and solutions to the three central
inference problems (evaluation, most likely state sequence and parameter estimation) have
been provided. Experiments have demonstrated that unification can improve generalization
accuracy, that the number of parameters of a LOHMM can be an order of magnitude smaller
than the number of parameters of the corresponding HMM, that the solutions presented
perform well in practice and also that LOHMMs possess several advantages over traditional
HMMs for applications involving structured sequences.
Acknowledgments The authors thank Andreas Karwath and Johannes Horstmann for
interesting collaborations on the protein data; Ingo Thon for interesting collaboration on
analyzing the Unix command sequences; and Saul Greenberg for providing the Unix command sequence data. The authors would also like to thank the anonymous reviewers for comments which considerably improved the paper. This research was partly supported by the
European Union IST programme under contract numbers IST-2001-33053 and FP6-508861
(Application of Probabilistic Inductive Logic Programming (APrIL) I and II). Tapani Raiko
was supported by a Marie Curie fellowship at DAISY, HPMT-CT-2001-00251.

Appendix A. Proof of Theorem 1
Let M = (Σ, µ, ∆, Υ) be a LOHMM. To show that M specifies a time discrete stochastic
process, i.e., a sequence of random variables hXt it=1,2,... , where the domains of the random
variable Xt is hb(Σ), the Herbrand base over Σ, we define the immediate state operator
TM -operator and the current emission operator EM -operator.
Definition 4 (TM -Operator, EM -Operator ) The operators TM : 2hbΣ → 2hbΣ and EM :
2hbΣ → 2hbΣ are
O

TM (I) = {HσB σH | ∃(p : H ←
− B) ∈ M : BσB ∈ I, HσB σH ∈ GΣ (H)}
O

EM (I) = {OσB σH σO | ∃(p : H ←
− B) ∈ M : BσB ∈ I, HσB σG ∈ GΣ (H)
and OσB σH σO ∈ GΣ (O)}
i+1
i ({start})) with
For each i = 1, 2, 3, . . ., the set TM
({start}) := TM (TM
1
TM ({start}) := TM ({start}) specifies the state set at clock i which forms a random varii ({start}) specifies the possible symbols emitted when transitioning
able Yi . The set UM
from i to i + 1. It forms the variable Ui . Each Yi (resp. Ui ) can be extended to a random
variable Zi (resp. Ui ) over hbΣ :

P (Zi = z) =



i ({start})
0.0 : z 6∈ TM
P (Yi = z) : otherwise

447

Kersting, De Raedt, & Raiko

PSfrag replacements

Z1

Z2

Z3

U1

U2

...

U3

Figure 8: Discrete time stochastic process induced by a LOHMM. The nodes Z i and Ui
represent random variables over hbΣ .

Figure 8 depicts the influence relation among Zi and Ui . Using standard arguments from
probability theory and noting that
P (Ui = Ui | Zi+1 = zi+1 , Zi = zi ) =
and P (Zi+1 | Zi ) =

X

P (Zi+1 , ui | Zi )

P (Zi+1 = zi+1 , Ui = ui | Zi )
P
ui P (Zi+1 , ui | Zi )

ui

where the probability distributions are due to equation (1), it is easy to show that Kolmogorov’s extension theorem (see Bauer, 1991; Fristedt
and Gray, 1997) holds. Thus, M
Nt
specifies a unique probability distribution over
(Z
× Ui ) for each t > 0 and in the
i
i=1
limit t → ∞.


Appendix B. Moore Representations of LOHMMs
For HMMs, Moore representations, i.e., output symbols are associated with states and Mealy
representations, i.e., output symbols are associated with transitions, are equivalent. In this
appendix, we will investigate to which extend this also holds for LOHMMs.
Let L be a Mealy-LOHMM according to definition 3. In the following, we will derive
the notation of an equivalent LOHMM L0 in Moore representation where there are abstract
transitions and abstract emissions (see below). Each predicate b/n in L is extended to b/n+
1 in L0 . The domains of the first n arguments are the same as for b/n. The last argument
will store the observation to be emitted. More precisely, for each abstract transition
o(v1 ,...,vk )

p : h(w1 , . . . , wl ) ←−−−−−− b(u1 , . . . , un )
in L, there is an abstract transition
p : h(w1 , . . . , wl , o(v01 , . . . , v0k )) ← b(u1 , . . . , un , )
in L0 . The primes in o(v01 , . . . , v0k ) denote that we replaced each free 8 variables o(v1 , . . . , vk )
by some distinguished constant symbol, say #. Due to this, it holds that
µ(h(w1 , . . . , wl )) = µ(h(w1 , . . . , wl , o(v01 , . . . , v0k ))) ,
8. A variable X ∈ vars(o(v1 , . . . , vk )) is free iff X 6∈ vars(h(w1 , . . . , wl )) ∪ vars(b(u1 , . . . , un )).

448

(6)

Logical Hidden Markov Models

and L0 ’s output distribution can be specified using abstract emissions which are expressions
of the form
1.0 : o(v1 , . . . , vk ) ← h(w1 , . . . , wl , o(v01 , . . . , v0k )) .
(7)
The semantics of an abstract transition in L0 is that being in
state S0t ∈ GΣ0 (b(u1 , . . . , un , )) the system will make a transition into
S0t+1 ∈ GΣ0 (h(w1 , . . . , wl , o(v01 , . . . , v0k ))) with probability
p · µ(S0t+1 | h(w1 , . . . , wl , o(v01 , . . . , v0k )) | σS0t )

some
state

(8)

where σS0t = mgu(S0t , b(u1 , . . . , un , )). Due to Equation (6), Equation (8) can be rewritten
as
p · µ(S0t+1 | h(w1 , . . . , wl ) | σS0t ) .
Due to equation (7), the system will emit the output symbol ot+1 ∈ GΣ0 (o(v1 , . . . , vk )) in
state S0t+1 with probability
µ(ot+1 | o(v1 , . . . , vk )σS0t+1 σS0t )
where σS0t+1 = mgu(h(w1 , . . . , wl , o(v01 , . . . , v0k )), S0t+1 ). Due to the construction of L0 , there
exists a triple (St , St+1 , Ot+1 ) in L for each triple (S0t , S0t+1 , Ot+1 ), t > 0, in L0 (and vise
versa). Hence,both LOHMMs assign the same overall transition probability.
L and L0 differ only in the way the initialize sequences h(S0t , S0t+1 , Ot+1 it=0,2...,T (resp.
h(St , St+1 , Ot+1 it=0,2...,T ). Whereas L starts in some state S0 and makes a transition to S1
emitting O1 , the Moore-LOHMM L0 is supposed to emit a symbol O0 in S00 before making a
transition to S01 . We compensate for this using the prior distribution. The existence of the
correct prior distribution for L0 can be seen as follows. In L, there are only finitely many
states reachable at time t = 1, i.e, PL (q0 = S) > 0 holds for only a finite set of ground
states S. The probability PL (q0 = s) can be computed similar to α1 (S). We set t = 1 in line
6, neglecting the condition on Ot−1 in line 10, and dropping µ(Ot−1 | OσB σH ) from line 14.
Completely listing all states S ∈ S1 together with PL (q0 = S), i.e., PL (q0 = S) : S ← start ,
constitutes the prior distribution of L0 .
The argumentation basically followed the approach to transform a Mealy machine into
a Moore machine (see e.g., Hopcroft and Ullman, 1979). Furthermore, the mapping of a
Moore-LOHMM – as introduced in the present section – into a Mealy-LOHMM is straightforward.

Appendix C. Proof of Theorem 2
Let T be a terminal alphabet and N a nonterminal alphabet. A probabilistic context-free
grammar (PCFG) G consists of a distinguished start symbol S ∈ N plus a finite set of
∗
productions
P of the form p : X → α, where X ∈ N , α ∈ (N ∪ T ) and p ∈ [0, 1]. For all
X ∈ N , :X→α p = 1. A PCFG defines a stochastic process with sentential forms as states,
and leftmost rewriting steps as transitions. We denote a single rewriting operation of the
grammar by a single arrow →. If as a result of one ore more rewriting operations we are
able to rewrite β ∈ (N ∪ T )∗ as a sequence γ ∈ (N ∪ T )∗ of nonterminals and terminals,
then we write β ⇒∗ γ. The probability of this rewriting is the product of all probability
449

Kersting, De Raedt, & Raiko

values associated to productions used in the derivation. We assume G to be consistent, i.e.,
that the sum of all probabilities of derivations S ⇒∗ β such that β ∈ T ∗ sum to 1.0.
We can assume that the PCFG G is in Greibach normal form. This follows from Abney
et al.’s (1999) Theorem 6 because G is consistent. Thus, every production P ∈ G is of
the form p : X → aY1 . . . Yn for some n ≥ 0. In order to encode G as a LOHMM M , we
introduce (1) for each non-terminal symbol X in G a constant symbol nX and (2) for each
terminal symbol t in G a constant symbol t. For each production P ∈ G, we include an
a
abstract transition of the form p : stack([nY1 , . . . , nYn |S]) ←
− stack([nX|S]), if n > 0, and
a
p : stack(S) ←
− stack([nX|S]), if n = 0. Furthermore, we include 1.0 : stack([s]) ← start
end
and 1.0 : end ←−− stack([]). It is now straightforward to prove by induction that M and G
are equivalent.


Appendix D. Logical Hidden Markov Model for Unix Command
Sequences
The LOHMMs described below model Unix command sequences triggered by mkdir. To
this aim, we transformed the original Greenberg data into a sequence of logical atoms over
com, mkdir(Dir, LastCom), ls(Dir, LastCom), cd(Dir, Dir, LastCom), cp(Dir, Dir, LastCom)
and mv(Dir, Dir, LastCom). The domain of LastCom was {start, com, mkdir, ls, cd, cp, mv}.
The domain of Dir consisted of all argument entries for mkdir, ls, cd, cp, mv in the original
dataset. Switches, pipes, etc. were neglected, and paths were made absolute. This yields
212 constants in the domain of Dir. All original commands, which were not mkdir, ls, cd,
cp, or mv, were represented as com. If mkdir did not appear within 10 time steps before a
command C ∈ {ls, cd, cp,mv}, C was represented as com. Overall, this yields more than
451000 ground states that have to be covered by a Markov model.
The “unification” LOHMM U basically implements a second order Markov model, i.e.,
the probability of making a transition depends upon the current state and the previous
state. It has 542 parameters and the following structure:
com ← start.
mkdir(Dir, start) ← start.

com ← com.
mkdir(Dir, com) ← com.
end ← com.

Furthermore, for each C ∈ {start, com} there are
mkdir(Dir, com)
mkdir( , com)
com
end
ls(Dir, mkdir)
ls( , mkdir)
cd(Dir, mkdir)

←
←
←
←
←
←
←

mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).

cd( , mkdir)
cp( , Dir, mkdir)
cp(Dir, , mkdir)
cp( , , mkdir)
mv( , Dir, mkdir)
mv(Dir, , mkdir)
mv( , , mkdir)

450

←
←
←
←
←
←
←

mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).

Logical Hidden Markov Models

together with for each C ∈ {mkdir, ls, cd, cp, mv} and for each C 1 ∈ {cd, ls} (resp.
C2 ∈ {cp, mv})
mkdir(Dir, com)
mkdir( , com)
com
end
ls(Dir,C1 )
ls( ,C1 )
cd(Dir,C1 )
cd( ,C1 )
cp( , Dir,C1 )
cp(Dir, ,C1 )
cp( , ,C1 )
mv( , Dir,C1 )
mv(Dir, ,C1 )
mv( , ,C1 )

←
←
←
←
←
←
←
←
←
←
←
←
←
←

C1 (Dir,C). mkdir( , com) ←
com ←
C1 (Dir,C).
C1 (Dir,C).
end ←
ls(From,C2 ) ←
C1 (Dir,C).
ls(To,C2 ) ←
C1 (Dir,C).
C1 (Dir,C).
ls( ,C2 ) ←
C1 (Dir,C).
cd(From,C2 ) ←
C1 (Dir,C).
cd(To,C2 ) ←
C1 (Dir,C).
cd( ,C2 ) ←
C1 (Dir,C). cp(From, ,C2 ) ←
C1 (Dir,C).
cp( , To,C2 ) ←
C1 (Dir,C).
cp( , ,C2 ) ←
C1 (Dir,C). mv(From, ,C2 ) ←
mv( , To,C2 ) ←
C1 (Dir,C).
mv( , ,C2 ) ←

C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).

Because all states are fully observable, we omitted the output symbols associated with
clauses, and, for the sake of simplicity, we omitted associated probability values.
The “no unification” LOHMM N is the variant of U where no variables were shared
such as
mkdir( , com) ← cp(From, To,C).
ls(
com ← cp(From, To,C).
cd(
end ← cp(From, To,C). cp( ,
mv( ,

, cp)
, cp)
, cp)
, cp)

←
←
←
←

cp(From, To,C).
cp(From, To,C).
cp(From, To,C).
cp(From, To,C).

Because only transitions are affected, N has 164 parameters less than U , i.e., 378.

Appendix E. Tree-based LOHMM for mRNA Sequences
The LOHMM processes the nodes of mRNA trees in in-order. The structure of the LOHMM
is shown at the end of the section. There are copies of the shaded parts. Terms are
abbreviated using their starting alphanumerical; tr stands for tree, he for helical, si for
single, nuc for nucleotide, and nuc p for nucleotide pair.
The domain of #Children covers the maximal branching factor found in the data, i.e.,
{[c], [c, c], . . . , [c, c, c, c, c, c, c, c, c]}; the domain of Type consists of all types occurring in
the data, i.e., {stem, single, bulge3, bulge5, hairpin}; and for Size, the domain covers
the maximal length of a secondary structure element in the data, i.e., the longest sequence
of consecutive bases respectively base pairs constituting a secondary structure element.
The length was encoded as {n1 (0), n2 (0), . . . , n13 (0)} where nm (0) denotes the recursive
application of the functor n m times. For Base and BasePair , the domains were the 4 bases
respectively the 16 base pairs. In total, there are 491 parameters.
451

my start
1.0 : root(0, root, X)

Copies for tr(Id, [c], [Pa − [C]|R]), tr(Id, [c, c], [Pa − [C]|R]),
and tr(Id, [c, c, c], [Pa − [C]|R])
0.25 : he(s(Id), Pa, [], T, L)
tr(Id, , [Pa − [C]|R])

tr(0, X, [0 − X])

and tr(Id, [c, c, c], [Pa − [C1, C2|Cs]|R])

0.25 : he(s(Id), Pa, B, T, L)

0.25 : he(s(Id), Pa, [], T, L)
0.25 : he(s(Id), Pa, B, T, L)
tr(Id, , [Pa − [C1, C2|Cs]|R])
0.25 : si(s(Id), Pa, B, T, L)
0.25 : si(s(Id), Pa, [], T, L)

0.25 : si(s(Id), Pa, B, T, L)
0.25 : si(s(Id), Pa, [], T, L)
se(T, L, s(Id), B, [s(Id) − B|R])

se(T, L, s(Id), [], R)

Copies for tr(Id, [c], [Pa − [C1, C2|Cs]|R]), tr(Id, [c, c], [Pa − [C1, C2|Cs]|R]),

tree
model

se(T, L, s(Id), [], [Pa − [C2|Cs]|R]) se(T, L, s(Id), B, [s(Id) − B, Pa − [C2|Cs]|R])

Copies for each type single, bulge3, bulge5
Copies for n(n(0)) and n(n(n(0)))

Copies for each length of sequence n(n(0)), n(n(n(0))), n(n(n(n(0))))
se(stem, n(A), Id, B, S)

se(hairpin, n(A), Id, B, S)

Copies for nuc p(a, g), . . . , nuc p(u, u)

Copies for nuc(g), nuc(c), and nuc(u)
0.25 : nuc(a)

0.0625 : nuc p(a, a)

se(hairpin, A, Id, B, S)

se(hairpin, n(0), Id, B, S)

se(stem, A, Id, B, S)

se(hairpin, n(0), s( ), , [])

0.25 : nuc(a)

se(stem, n(0), s( ), , [])

0.0625 : nuc p(a, a)

0.25 : nuc(a)

se(stem, n(0), Id, B, S)

0.0625 : nuc p(a, a)

Copies for nuc p(a, g), . . . , nuc p(u, u)

Copies for nuc(g), nuc(c), and nuc(u)
end

tr(Id, B, S)

sequence
model

Kersting, De Raedt, & Raiko

Figure 9: The mRNA LOHMM structure. The symbol denotes anonymous variables which
are read and treated as distinct, new variables each time they are encountered.
There are copies of the shaded part. Terms are abbreviated using their starting
alphanumerical; tr stands for tree, se for structure element, he for helical,
si for single, nuc for nucleotide, and nuc p for nucleotide pair.

References

452

Abney, S. (1997). Stochastic Attribute-Value Grammars. Computational Linguistics, 23 (4),
597–618.

1.0

start

Logical Hidden Markov Models

Abney, S., McAllester, D., & Pereira, F. (1999). Relating probabilistic grammars and automata. In Proceedings of 37th Annual Meeting of the Association for Computational
Linguistics (ACL-1999), pp. 542–549. Morgan Kaufmann.
Anderson, C., Domingos, P., & Weld, D. (2002). Relational Markov Models and their Application to Adaptive Web Navigation. In Proceedings of the Eighth International
Conference on Knowledge Discovery and Data Mining (KDD-2002), pp. 143–152 Edmonton, Canada. ACM Press.
Baker, J. (1979). Trainable grammars for speech recognition. In Speech communication
paper presented at th 97th Meeting of the Acoustical Society of America, pp. 547–550
Boston, MA.
Bauer, H. (1991). Wahrscheinlichkeitstheorie (4. edition). Walter de Gruyter, Berlin, New
York.
Baum, L. (1972). An inequality and associated maximization technique in statistical estimation for probabilistic functions of markov processes. Inequalities, 3, 1–8.
Bohnebeck, U., Horváth, T., & Wrobel, S. (1998). Term comparison in first-order similarity
measures. In Proceedings of the Eigth International Conference on Inductive Logic
Programming (ILP-98), Vol. 1446 of LNCS, pp. 65–79. Springer.
Bresnan, J. (2001). Lexical-Functional Syntax. Blackwell, Malden, MA.
Carrasco, R., Oncina, J., & Calera-Rubio, J. (2001). Stochastic inference of regular tree
languages. Machine Learning, 44 (1/2), 185–197.
Chandonia, J., Hon, G., Walker, N., Lo Conte, L., P.Koehl, & Brenner, S. (2004). The
ASTRAL compendium in 2004. Nucleic Acids Research, 32, D189–D192.
Davison, B., & Hirsh, H. (1998). Predicting Sequences of User Actions. In Predicting the
Future: AI Approaches to Time-Series Analysis, pp. 5–12. AAAI Press.
De Raedt, L., & Kersting, K. (2003). Probabilistic Logic Learning. ACM-SIGKDD Explorations: Special issue on Multi-Relational Data Mining, 5 (1), 31–48.
De Raedt, L., & Kersting, K. (2004). Probabilistic Inductive Logic Programming. In
Ben-David, S., Case, J., & Maruoka, A. (Eds.), Proceedings of the 15th International
Conference on Algorithmic Learning Theory (ALT-2004), Vol. 3244 of LNCS, pp.
19–36 Padova, Italy. Springer.
Durbin, R., Eddy, S., Krogh, A., & Mitchison, G. (1998). Biological sequence analysis:
Probabilistic models of proteins and nucleic acids. Cambridge University Press.
Džeroski, S., & Lavrač, N. (Eds.). (2001). Relational data mining. Springer-Verlag, Berlin.
Eddy, S., & Durbin, R. (1994). RNA sequence analysis using covariance models. Nucleic
Acids Res., 22 (11), 2079–2088.
453

Kersting, De Raedt, & Raiko

Eisele, A. (1994). Towards probabilistic extensions of contraint-based grammars. In
Dörne, J. (Ed.), Computational Aspects of Constraint-Based Linguistics Decription-II.
DYNA-2 deliverable R1.2.B.
Fine, S., Singer, Y., & Tishby, N. (1998). The hierarchical hidden markov model: analysis
and applications. Machine Learning, 32, 41–62.
Frasconi, P., Soda, G., & Vullo, A. (2002). Hidden markov models for text categorization
in multi-page documents. Journal of Intelligent Information Systems, 18, 195–217.
Friedman, N., Getoor, L., Koller, D., & Pfeffer, A. (1999). Learning probabilistic relational
models. In Proceedings of Sixteenth International Joint Conference on Artificial Intelligence (IJCAI-1999), pp. 1300–1307. Morgan Kaufmann.
Fristedt, B., & Gray, L. (1997). A Modern Approach to Probability Theory. Probability and
its applications. Birkhäuser Boston.
Ghahramani, Z., & Jordan, M. (1997). Factorial hidden Markov models. Machine Learning,
29, 245–273.
Goodman, J. (1997). Probabilistic feature grammars. In Proceedings of the Fifth International Workshop on Parsing Technologies (IWPT-97) Boston, MA, USA.
Greenberg, S. (1988). Using Unix: collected traces of 168 users. Tech. rep., Dept. of
Computer Science, University of Calgary, Alberta.
Hopcroft, J., & Ullman, J. (1979). Introduction to Automata Theory, Languages, and
Computation. Addison-Wesley Publishing Company.
Horváth, T., Wrobel, S., & Bohnebeck, U. (2001). Relational Instance-Based learning with
Lists and Terms. Machine Learning, 43 (1/2), 53–80.
Hubbard, T., Murzin, A., Brenner, S., & Chotia, C. (1997). SCOP : a structural classification
of proteins database. NAR, 27 (1), 236–239.
Jacobs, N., & Blockeel, H. (2001). The Learning Shell: Automated Macro Construction. In
User Modeling 2001, pp. 34–43.
Jaeger, M. (1997). Relational Bayesian networks. In Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence (UAI), pp. 266–273. Morgan Kaufmann.
Katz, S. (1987). Estimation of probabilities from sparse data for hte language model component of a speech recognizer. IEEE Transactions on Acoustics, Speech, and Signal
Processing (ASSP), 35, 400–401.
Kersting, K., & De Raedt, L. (2001a). Adaptive Bayesian Logic Programs. In Rouveirol,
C., & Sebag, M. (Eds.), Proceedings of the 11th International Conference on Inductive
Logic Programming (ILP-01), Vol. 2157 of LNAI, pp. 118–131. Springer.
454

Logical Hidden Markov Models

Kersting, K., & De Raedt, L. (2001b). Towards Combining Inductive Logic Programming
with Bayesian Networks. In Rouveirol, C., & Sebag, M. (Eds.), Proceedings of the
11th International Conference on Inductive Logic Programming (ILP-01), Vol. 2157
of LNAI, pp. 118–131. Springer.
Kersting, K., & Raiko, T. (2005). ’Say EM’ for Selecting Probabilistic Models for Logical
Sequences. In Bacchus, F., & Jaakkola, T. (Eds.), Proceedings of the 21st Conference
on Uncertainty in Artificial Intelligence, UAI 2005, pp. 300–307 Edinburgh, Scotland.
Kersting, K., Raiko, T., Kramer, S., & De Raedt, L. (2003). Towards discovering structural signatures of protein folds based on logical hidden markov models. In Altman,
R., Dunker, A., Hunter, L., Jung, T., & Klein, T. (Eds.), Proceedings of the Pacific Symposium on Biocomputing (PSB-03), pp. 192–203 Kauai, Hawaii, USA. World
Scientific.
Koivisto, M., Kivioja, T., Mannila, H., Rastas, P., & Ukkonen, E. (2004). Hidden Markov
Modelling Techniques for Haplotype Analysis. In Ben-David, S., Case, J., & Maruoka,
A. (Eds.), Proceedings of 15th International Conference on Algorithmic Learning Theory (ALT-04), Vol. 3244 of LNCS, pp. 37–52. Springer.
Koivisto, M., Perola, M., Varilo, T., Hennah, W., Ekelund, J., Lukk, M., Peltonen, L.,
Ukkonen, E., & Mannila, H. (2002). An MDL method for finding haplotype blocks
and for estimating the strength of haplotype block boundaries. In Altman, R., Dunker,
A., Hunter, L., Jung, T., & Klein, T. (Eds.), Proceedings of the Pacific Symposium
on Biocomputing (PSB-02), pp. 502–513. World Scientific.
Korvemaker, B., & Greiner, R. (2000). Predicting UNIX command files: Adjusting to user
patterns. In Adaptive User Interfaces: Papers from the 2000 AAAI Spring Symposium,
pp. 59–64.
Kulp, D., Haussler, D., Reese, M., & Eeckman, F. (1996). A Generalized Hidden Markov
Model for the Recognition of Human Genes in DNA. In States, D., Agarwal, P.,
Gaasterland, T., Hunter, L., & Smith, R. (Eds.), Proceedings of the Fourth International Conference on Intelligent Systems for Molecular Biology,(ISMB-96), pp. 134–
142 St. Louis, MO, USA. AAAI.
Lane, T. (1999). Hidden Markov Models for Human/Computer Interface Modeling. In
Rudström, Å. (Ed.), Proceedings of the IJCAI-99 Workshop on Learning about Users,
pp. 35–44 Stockholm, Sweden.
Lari, K., & Young, S. (1990). The estimation of stochastic context-free grammars using the
inside-outside algorithm. Computer Speech and Language, 4, 35–56.
Levy, L., & Joshi, A. (1978). Skeletal structural descriptions. Information and Control,
2 (2), 192–211.
McLachlan, G., & Krishnan, T. (1997). The EM Algorithm and Extensions. Wiley, New
York.
455

Kersting, De Raedt, & Raiko

Mitchell, T. M. (1997). Machine Learning. The McGraw-Hill Companies, Inc.
Muggleton, S. (1996). Stochastic logic programs. In De Raedt, L. (Ed.), Advances in
Inductive Logic Programming, pp. 254–264. IOS Press.
Muggleton, S., & De Raedt, L. (1994). Inductive logic programming: Theory and methods.
Journal of Logic Programming, 19 (20), 629–679.
Ngo, L., & Haddawy, P. (1997). Answering queries from context-sensitive probabilistic
knowledge bases. Theoretical Computer Science, 171, 147–177.
Pollard, C., & Sag, I. (1994). Head-driven Phrase Structure Grammar. The University of
Chicago Press, Chicago.
Rabiner, L., & Juang, B. (1986). An Introduction to Hidden Markov Models. IEEE ASSP
Magazine, 3 (1), 4–16.
Riezler, S. (1998). Statistical inference and probabilistic modelling for constraint-based
nlp. In Schrder, B., Lenders, W., & und T. Portele, W. H. (Eds.), Proceedings of
the 4th Conference on Natural Language Processing (KONVENS-98). Also as CoRR
cs.CL/9905010.
Sakakibara, Y. (1992). Efficient learning of context-free grammars from positive structural
examples. Information and Computation, 97 (1), 23–60.
Sakakibara, Y. (2003). Pair hidden markov models on tree structures. Bioinformatics,
19 (Suppl.1), i232–i240.
Sakakibara, Y., Brown, M., Hughey, R., Mian, I., Sjolander, K., & Underwood, R. (1994).
Stochastic context-free grammars for tRNA modelling. Nucleic Acids Research,
22 (23), 5112–5120.
Sanghai, S., Domingos, P., & Weld, D. (2003). Dynamic probabilistic relational models.
In Gottlob, G., & Walsh, T. (Eds.), Proceedings of the Eighteenth International Joint
Conference on Artificial Intelligence (IJCAI-03), pp. 992–997 Acapulco, Mexico. Morgan Kaufmann.
Sato, T., & Kameya, Y. (2001). Parameter learning of logic programs for symbolic-statistical
modeling. Journal of Artificial Intelligence Research (JAIR), 15, 391–454.
Schölkopf, B., & Warmuth, M. (Eds.). (2003). Learning and Parsing Stochastic UnificationBased Grammars, Vol. 2777 of LNCS. Springer.
Turcotte, M., Muggleton, S., & Sternberg, M. (2001). The effect of relational background
knowledge on learning of protein three-dimensional fold signatures. Machine Learning,
43 (1/2), 81–95.
Won, K., Prügel-Bennett, A., & Krogh, A. (2004). The Block Hidden Markov Model for Biological Sequence Analysis. In Negoita, M., Howlett, R., & Jain, L. (Eds.), Proceedings
of the Eighth International Conference on Knowledge-Based Intelligent Information
and Engineering Systems (KES-04), Vol. 3213 of LNCS, pp. 64–70. Springer.

456

Journal of Artificial Intelligence Research 25 (2006) 269-314

Submitted 5/05; published 2/06

Distributed Reasoning in a Peer-to-Peer Setting:
Application to the Semantic Web
Philippe Adjiman
Philippe Chatalic
François Goasdoué
Marie-Christine Rousset
Laurent Simon

adjiman@lri.fr
chatalic@lri.fr
fg@lri.fr
mcr@lri.fr
simon@lri.fr

LRI-PCRI, Bâtiment 490
CNRS & Université Paris-Sud XI – INRIA Futurs
91405 Orsay Cedex, France

Abstract
In a peer-to-peer inference system, each peer can reason locally but can also solicit
some of its acquaintances, which are peers sharing part of its vocabulary. In this paper,
we consider peer-to-peer inference systems in which the local theory of each peer is a set of
propositional clauses defined upon a local vocabulary. An important characteristic of peerto-peer inference systems is that the global theory (the union of all peer theories) is not
known (as opposed to partition-based reasoning systems). The main contribution of this
paper is to provide the first consequence finding algorithm in a peer-to-peer setting: DeCA.
It is anytime and computes consequences gradually from the solicited peer to peers that are
more and more distant. We exhibit a sufficient condition on the acquaintance graph of the
peer-to-peer inference system for guaranteeing the completeness of this algorithm. Another
important contribution is to apply this general distributed reasoning setting to the setting
of the Semantic Web through the Somewhere semantic peer-to-peer data management
system. The last contribution of this paper is to provide an experimental analysis of the
scalability of the peer-to-peer infrastructure that we propose, on large networks of 1000
peers.

1. Introduction
Recently peer-to-peer systems have received considerable attention because their underlying infrastructure is appropriate to scalable and flexible distributed applications over
Internet. In a peer-to-peer system, there is no centralized control or hierarchical organization: each peer is equivalent in functionality and cooperates with other peers in order
to solve a collective task. Peer-to-peer systems have evolved from simple keyword-based
peer-to-peer file sharing systems like Napster (http://www.napster.com) and Gnutella
(http://gnutella.wego.com) to semantic peer-to-peer data management systems like
Edutella (Nejdl, Wolf, Qu, Decker, Sintek, & al., 2002) or Piazza (Halevy, Ives, Tatarinov, & Mork, 2003a), which handle semantic data description and support complex queries
for data retrieval. In those systems, the complexity of answering queries is directly related
to the expressivity of the formalism used to state the semantic mappings between peers
schemas (Halevy, Ives, Suciu, & Tatarinov, 2003b).
c
!2006
AI Access Foundation. All rights reserved.

Adjiman, Chatalic, Goasdoué, Rousset, & Simon

In this paper, we are interested in peer-to-peer inference systems in which each peer can
answer queries by reasoning from its local (propositional) theory but can also ask queries
to some other peers with which it is semantically related by sharing part of its vocabulary.
This framework encompasses several applications like peer-to-peer information integration
systems or intelligent agents, in which each peer has its own knowledge (about its data or its
expertise domain) and some partial knowledge about some other peers. In this setting, when
solicited to perform a reasoning task, a peer, if it cannot solve completely that task locally,
must be able to distribute appropriate reasoning subtasks among its acquainted peers. This
leads to a step by step splitting of the initial task among the peers that are relevant to
solve parts of it. The outputs of the different splitted tasks must then be recomposed to
construct the outputs of the initial task.
We consider peer-to-peer inference systems in which the local theory of each peer is
composed of a set of propositional clauses defined upon a set of propositional variables
(called its local vocabulary). Each peer may share part of its vocabulary with some other
peers. We investigate the reasoning task of finding consequences of a certain form (e.g.,
clauses involving only certain variables) for a given input formula expressed using the local
vocabulary of a peer. Note that other reasoning tasks like finding implicants of a certain
form for a given input formula can be equivalently reduced to the consequence finding task.
It is important to emphasize that the problem of distributed reasoning that we consider
in this paper is quite different from the problem of reasoning over partitions obtained
by decomposition of a theory (Dechter & Rish, 1994; Amir & McIlraith, 2000). In that
problem, a centralized large theory is given and its structure is exploited to compute its
best partitioning in order to optimize the use of a partition-based reasoning algorithm. In
our problem, the whole theory (i.e., the union of all the local theories) is not known and the
partition is imposed by the peer-to-peer architecture. As we will illustrate it on an example
(Section 2), the algorithms based on transmitting clauses between partitions in the spirit
of the work of Amir and McIlraith (2000), Dechter and Rish (1994) or del Val (1999) are
not appropriate for our consequence finding problem. Our algorithm splits clauses if they
share variables of several peers. Each piece of a splitted clause is then transmitted to the
corresponding theory to find its consequences. The consequences that are found for each
piece of splitted clause must then be recomposed to get the consequences of the clause that
had been splitted.
The main contribution of this paper is to provide the first consequence finding algorithm
in a peer-to-peer setting: DeCA. It is anytime and computes consequences gradually from
the solicited peer to peers that are more and more distant. We exhibit a sufficient condition on the acquaintance graph of the peer-to-peer inference system for guaranteeing the
completeness of this algorithm.
Another important contribution is to apply this general distributed reasoning setting
to the setting of the Semantic Web through the Somewhere semantic peer-to-peer data
management system. Somewhere is based on a simple data model made of taxonomies
of atomic classes and mappings between classes of different taxonomies, which we think
are the appropriate common semantic support needed for most of the future semantic web
applications. The Somewhere data model can be encoded into propositional logic so that
query answering in Somewhere can be equivalently reduced to distributed reasoning over
logical propositional theories.
270

Distributed Reasoning in a Peer-to-Peer Setting: Application to the Semantic Web

The last contribution of this paper is to provide an experimental analysis of the scalability of our approach, when deployed on large networks. So far, the scalability of a system
like Piazza goes up to about 80 peers. Piazza uses a more expressive language than the
one used in our approach, but its results rely on a wide range of optimizations (mappings
composition, paths pruning Tatarinov & Halevy, 2004) made possible by the centralized
storage of all the schemas and mappings in a global server. In contrast, we have stuck to a
fully decentralized approach and performed our experiments on networks of 1000 peers.
An important point characterizing peer-to-peer systems is their dynamicity: peers can
join or leave the system at any moment. Therefore, it is not feasible to bring all the
information to a single server in order to reason with it locally using standard reasoning
algorithms. Not only would it be costly to gather the data available through the system
but it would be a useless task because of the changing peers connected to the network. The
dynamicity of peer-to-peer inference systems imposes to revisit any reasoning problem in
order to address it in a completely decentralized manner.
The paper is organized as follows. Section 2 defines formally the peer-to-peer inference
problem that we address in this paper. In Section 3, we describe our distributed consequence
finding algorithm and we state its properties. We describe Somewhere in Section 4.
Section 5 reports the experimental study of the scalability of our peer-to-peer infrastructure.
Related work is summarized in Section 6. We conclude with a short discussion in Section 7.

2. Consequence Finding in Peer-to-peer Inference Systems
A peer-to-peer inference system (P2PIS) is a network of peer theories. Each peer P is a
finite set of propositional formulas of a language LP . We consider the case where LP is
the language of clauses without duplicated literals that can be built from a finite set of
propositional variables VP , called the vocabulary of P . Peers can be semantically related by
sharing variables with other peers. A shared variable between two peers is in the intersection
of the vocabularies of the two peers. We do not impose however that all the variables in
common in the vocabularies of two peers are shared by the two peers: two peers may not
be aware of all the variables that they have in common but only of some of them.
In a P2PIS, no peer has the knowledge of the global P2PIS theory. Each peer only knows
its own local theory and the variables that it shares with some other peers of the P2PIS (its
acquaintances). It does not necessarily knows all the variables that it has in common with
other peers (including with its acquaintances). When a new peer joins a P2PIS it simply
declares its acquaintances in the P2PIS, i.e., the peers it knows to be sharing variables with,
and it declares the corresponding shared variables.
2.1 Syntax and Semantics
A P2PIS can be formalized using the notion of acquaintance graph and in the following we
consider P2PIS and acquaintance graphs as equivalent.
271

Adjiman, Chatalic, Goasdoué, Rousset, & Simon

Definition 1 (Acquaintance graph) Let P = {Pi }i=1..n be a collection of clausal theories on their respective vocabularies VPi , let V = ∪i=1..n VPi . An acquaintance graph over V
is a graph Γ = (P, acq) where P is the set of vertices and acq ⊆ V × P × P is a set of
labelled edges such that for every (v, Pi , Pj ) ∈ acq, i %= j and v ∈ VPi ∩ VPj .
A labelled edge (v, Pi , Pj ) expresses that peers Pi and Pj know each other to be sharing
the variable v. For a peer P and a literal l, acq(l, P ) denotes the set of peers sharing with
P the variable of l.
In contrast with other approaches (Ghidini & Serafini, 2000; Calvanese, De Giacomo,
Lenzerini, & Rosati, 2004), we do not adopt an epistemic or modal semantics for interpreting
a P2PIS but we interpret it with the standard semantics of propositional logic.
Definition 2 (Semantics of a P2PIS) Let Γ = (P, acq) be a P2PIS with P = {Pi }i=1..n ,
!
• An interpretation I of P is an assignement of the variables in i=1..n Pi to true or
f alse. In particular, a variable which is common to two theories Pi and Pj of a given
P2PIS is interpreted by the same value in the two theories.
• I is a model of a clause c iff one of the literals of c is evaluated to true in I.
• I is a model of a set of clauses (i.e., a local theory, a union of a local theories, or a
whole P2PIS) iff it is a model of all the clauses of the set.
• A P2PIS is satisfiable iff it has a model.
• The consequence relation for a P2PIS is the standard consequence relation |=: given
a P2PIS P, and a clause c, P |= c iff every model of P is a model of c.
2.2 The Consequence Finding Problem
For each theory P , we consider a subset of target variables T V P ⊆ VP , supposed to represent
the variables of interest for the application, (e.g., observable facts in a model-based diagnosis
application, or classes storing data in an information integration application). The goal is,
given a clause provided as an input to a given peer, to find all the possible consequences
belonging to some target language of the input clause and the union of the peer theories.
The point is that the input clause only uses the vocabulary of the queried peer, but
that its expected consequences may involve target variables of different peers. The target
languages handled by our algorithm are defined in terms of target variables and require that
a shared variable has the same target status in all the peers sharing it. It is worth noting
that this requirement is a local property: the peers sharing variables with a given peer
are its acquaintances and, by definition, they are its direct neighbours in the acquaintance
graph.
Definition 3 (Target Language) Let Γ = (P, acq) be a P2PIS, and for every peer P ,
let T V P be the set of its target variables such that if (v, Pi , Pj ) ∈ acq then v ∈ T V Pi iff
) as the
v ∈ T V Pj . For a set SP of peers of P, we define its target language T arget(SP
!
language of clauses (including the empty clause) involving only variables of P ∈SP T V P .
272

Distributed Reasoning in a Peer-to-Peer Setting: Application to the Semantic Web

The reasoning problem that we are interested in is to compute logical consequences of
an input clause given a P2PIS. It corresponds to the notion of proper prime implicates of
a clause w.r.t. a clausal (distributed) theory, which if formally defined in Definition 4.
Definition 4 (Proper prime implicate of a clause w.r.t. a clausal theory) Let P be
a clausal theory and q be a clause. A clause m is said to be:
• an implicate of q w.r.t. P iff P ∪ {q} |= m.
• a prime implicate of q w.r.t. P iff m is an implicate of q w.r.t. P , and for any other
clause m# implicate of q w.r.t. P , if m# |= m then m# ≡ m.
• a proper prime implicate of q w.r.t. P iff it is a prime implicate of q w.r.t. P but
P %|= m.
The problem of finding prime implicates from a new clause and a theory, a.k.a. Φ-prime
implicates, corresponds exactly to the problem of computing proper prime implicates of
a clause w.r.t. a clausal theory. It has been extensively studied in the centralized case
(see the work of Marquis, 2000, for a survey). Note that deciding whether a clause is a
Φ-prime implicate of a clausal theory is BH2 -complete (Marquis, 2000), i.e., both in N P
and coN P . The problem we address may be viewed as a further refinement, restricting to
the computation of proper prime implicates of a given target language. It corresponds to
(L, Φ)-prime implicates in the work of Marquis (2000) and has the same complexity.
Definition 5 (The consequence finding problem in a P2PIS) Let Γ = (P, acq) be
a P 2P IS, where P = {Pi }i=1..n is a collection of clausal theories with respective target
variables. The consequence finding problem in Γ is, given a peer P , its acquaintances
in Γ,
!
and a clause q ∈ LP , to find the set of proper prime implicates of q w.r.t. i=1..n Pi which
belong to T arget(P).
From an algorithmic point of view, the consequence finding problem in a P2PIS is new
and significantly different from the consequence finding problem in a single global theory.
According to the semantics, in order to be complete, a peer-to-peer consequence finding
algorithm must obtain the same results as any standard consequence finding algorithm
applied to the union of the local theories, but without having it as a global input: it just
has a partial and local input made of the theory of a single peer and of its acquaintances.
The reasoning must be distributed appropriately among the different theories without a
global view of the whole P2PIS. In a full peer-to-peer setting, such a consequence finding
algorithm cannot be centralized (because it would mean that there is a super-peer controlling
the reasoning). Therefore, we must design an algorithm running independently on each peer
and possibly distributing part of reasoning that it controls to acquainted peers: no peer has
control on the whole reasoning.
Among the possible consequences we distinguish local consequences, involving only target
variables of the solicited peer, remote consequences, which involve target variables of a
single peer distant from the solicited peer, and combined consequences which involve target
variables of several peers.
273

Adjiman, Chatalic, Goasdoué, Rousset, & Simon

2.3 Example
The following example illustrates the main characteristics of our message-based distributed
algorithm running on each peer, which will be presented in detail in Section 3.
Let us consider 4 interacting peers. P1 describes a tour operator. Its theory expresses
that its current F ar destinations are either Chile or Kenya. These far destinations are
international destinations (Int) and expensive (Exp). The peer P2 is only concerned with
police regulations and expresses that a passport is required (P ass) for international destinations. P3 focuses on sanitary conditions for travelers. It expresses that, in Kenya,
yellow fever vaccination (Y ellowF ev) is strongly recommended and that a strong protection against paludism should be taken (P alu) when accomodation occurs in Lodges. P4
describes travel accommodation conditions : Lodge for Kenya and Hotel for Chile. It also
expresses that when anti-paludism protection is required, accommodations are equipped
with appropriate anti-mosquito protections (AntiM ). The respective theories of each peer
are described on Figure 1 as the nodes of the acquaintance graph. Shared variables are mentioned as edge labels. Target variables are defined by : T V P1 = {Exp}, T V P2 = {P ass},
T V P3 = {Lodge, Y ellowF ev, P alu} and T V P4 = {Lodge, Hotel, P alu, AntiM }.
P1 :
¬Far ∨ Exp
¬Far ∨ Chile ∨ Kenya
Int

P2 :
¬Int ∨ Pass

¬Far ∨ Int

Kenya

Kenya,Chile

P3 :
¬Kenya ∨ YellowFev
¬Lodge ∨ ¬Kenya ∨ Palu

P4 :
¬Kenya ∨ Lodge
¬Chile ∨ Hotel
¬Palu ∨ AntiM

Lodge,Palu

Figure 1: Acquaintance graph for the tour operator example
We now illustrate the behavior of the algorithm when the input clause Far is provided
to peer P1 by the user. Describing precisely the behavior of a distributed algorithm on a
network of peers is not easy. In the following we present the propagation of the reasoning
as a tree structure, the nodes of which correspond to peers and the branches of which
materialize the different reasoning paths induced by the initial input clause. Edges are
labelled on the left side by literals which are propagated along paths and/or on the right
side by consequences that are transmitted back. A downward arrow on an edge indicates
the step during which a literal is propagated from one peer to its neighbor. For instance,
the initial step can be represented here by the following tree :

F ar
P1 :

Local consequences of a literal propagated on a peer are then explicited within the
peer node. Target literals are outlined using a grey background, as well as transmitted
274

Distributed Reasoning in a Peer-to-Peer Setting: Application to the Semantic Web

back consequences. Vertical arrows preceding consequences distinguish the last returned
consequences from earlier ones. Although the successive trees presented here have increasing
depth, as if all reasoning paths were explored synchronously and in parallel, the reader
should keep in mind that all messages are exchanged in an asynchronous way and that the
order in which consequents are produced cannot be predicted.
In our example, consequences of Far derived by local reasoning on P1 are Exp, Int
and Chile ∨ Kenya. Since Exp is in T arget(P1 ) it is a local consequence of Far. Int is
not a target literal but is shared with P2 , it is therefore transmitted to P2 . The clause
Chile ∨ Kenya is also made of shared variables. Such clauses are processed by our algorithm
using a split/recombination approach. Each shared literal is processed independently, and
transmitted to its appropriate neighbors. Each literal is associated with some queue data
structure, where transmitted back consequences are stored. As soon as at least one consequent has been obtained for each literal, the respective queued consequents of each literal
are recombined, to produce consequences of the initial clause. This recombination process
continues, as new consequences for a literal are produced. Note that since each literal is
processed asynchronously, the order in which the recombined consequences are produced is
unpredictable. Here, the component Chile is transmitted to P4 and Kenya is transmitted to
P3 and P4 . Let us note that the peer P4 appears two times in the tree, because two different
literals are propagated on this peer, which induces two different reasoning paths.

F ar
P1 :
Exp
Int
P2 :

Chile

Int

Chile ∨ Kenya
Kenya Kenya

P4 :

P3 :

P4 :

While Exp is transmitted back to the user as a first (local) consequence of Far.
• The propagation of Int on P2 produces the clause Pass, which is in T arget(P2 ) but is
not shared and therefore, cannot be further propagated.
• The clause Chile, when transmitted to P4 , produces Hotel which is in T arget(P4 ) but
is not shared and cannot be further propagated.
• When transmitted to P3 , the clause Kenya produces YellowFev as well as the clause
¬Lodge ∨ Palu. The three variables are in T arget(P3 ). Lodge and Palu are also shared
variables and therefore, after splitting of the second clause, their corresponding literals
are transmitted (independently) to P4 .
• When transmitted to P4 , Kenya produces Lodge, which is in T arget(P4 ) and is also
shared and therefore further transmitted to P3 .

275

Adjiman, Chatalic, Goasdoué, Rousset, & Simon

↑ Exp

F ar
P1 :
Exp

Int
P2 :
Pass

Chile ∨ Kenya

Int

Chile
P4 :
Hotel

Kenya

Kenya

P3 :
YellowFev
¬Lodge
P4 :

P4 :
¬Lodge ∨ Palu
P alu
P4 :

Lodge
Lodge
P3 :

• The clause Pass, produced on P2 , is transmitted back to P1 as a consequence of Int
and then to the user as a remote consequence of Far.

• The clause Hotel, produced on P4 , is transmitted back to P1 where it is queued as a
consequent of Chile, since it has to be combined with consequences of Kenya.

• The two local consequences of Kenya obtained on P3 contain only target variables.
They are transmitted back to P1 and queued there. They may now be combined
with Hotel to produce two new combined consequences of Far : Hotel ∨ YellowFev and
Hotel ∨ ¬Lodge ∨ Palu, which are transmitted back to the user.
• Similarly on P4 , Lodge is a local target consequent of Kenya, that is transmitted back
to P1 as a consequent of Kenya, where it is combined with Hotel to produce a new
consequence of Far that, in turn, is transmitted back to the user.

Simultaneously, the reasoning further propagates in the network of peers. The propagation of ¬Lodge and Palu on P4 respectively produces ¬Kenya, which is not a target literal
but is shared and thus further propagated on P1 , as well as AntiM, which is a target literal,
but not shared. We do not detail here the propagation of Lodge in the right most branch
of the reasoning tree.
276

Distributed Reasoning in a Peer-to-Peer Setting: Application to the Semantic Web

Exp
↑ Pass

↑ Hotel ∨ YellowFev

F ar

↑ Hotel ∨ ¬Lodge ∨ Palu
↑ Hotel ∨ Lodge

P1 :
Exp

↑ Pass
P2 :
Pass

P4 :
Hotel

Chile ∨ Kenya

Int

↑ YellowFev

↑ Hotel

↑ ¬Lodge ∨ Palu

P3 :

↑ Lodge
P4 :

YellowFev
¬Lodge
P4 :
¬Kenya

¬Lodge ∨ Palu
P alu
P4 :
AntiM

Lodge
Lodge
P3 :
...

¬Kenya
P1 :

Note on the deepest node that P1 is here asked to produce the implicates of ¬Kenya,
while the complementary literal Kenya is still under process. We will see in Section 3 that
such situations are handled in our algorithm by mean of histories keeping track of the
reasoning branches ending up to each transmitted literal. When a same history contains
two complementary literals, the corresponding reasoning branch is closed and the empty
clause ! is returned as a consequence of the literals in the history.

In our example, the consequence produced by P1 for ¬Kenya is thus !, which is sent
back to P4 and then to P3 . After combination on P3 with Palu we thus obtain Palu as a
new consequent of Kenya, which subsumes the previously obtained ¬Lodge ∨ Palu. When
transmitted back to P1 and combined with Hotel we obtain Hotel ∨ Palu which subsumes the
previously obtained consequent Hotel ∨ ¬Lodge ∨ Palu. Since AntiM is not a shared variable
it is the only consequent of Palu on P4 . When transmitted back to P3 for combination with
!, we thus obtain AntiM which, in turn, is returned to P1 for combination with Hotel, thus
giving Hotel ∨ AntiM as a new consequent of Far.
277

Adjiman, Chatalic, Goasdoué, Rousset, & Simon

Exp Pass
Hotel ∨ YellowFev
Hotel ∨ Lodge

↑ Hotel ∨ Palu

F ar

↑ Hotel ∨ AntiM
↑ Hotel ∨ ...

P1 :
Exp

Chile ∨ Kenya

Int

YellowFev
Pass
P2 :
Pass

P4 :
Hotel

Lodge

↑ Palu

Hotel

↑ ...

↑ AntiM

P3 :

P4 :

YellowFev

¬Lodge ∨ Palu
↑ AntiM

↑ !
P4 :
¬Kenya

P4 :
AntiM

Lodge
↑ ...
P3 :
...

↑ !
P1 :

We have not detailed the production of consequences of Lodge on P3 in the right most
branch. The reader could check in a similar way that it also produces AntiM (which has
already been produced through P3 /P4 in the third branch). Eventually, the whole set of
consequences of Far is {Exp, Pass, Hotel∨Lodge, Hotel∨Palu, Hotel∨AntiM, Hotel∨YellowFev}.
Among those consequences, it is important to note that some of them (e.g., Hotel∨YellowFev)
involve target variables from different peers. Such implicates could not be obtained by
partition-based algorithms like those in (Amir & McIlraith, 2000). This is made possible
thanks to the split/recombination strategy of our algorithm.

3. Distributed Consequence Finding Algorithm
The message passing distributed algorithm that we have implemented is described in Section
3.2. We show that it terminates and that it computes the same results as the recursive
algorithm described in Section 3.1. We exhibit a property of the acquaintance graph that
guarantees the completeness of this recursive algorithm, and therefore of the message passing
distributed algorithm (since both algorithms compute the same results).
For both algorithms, we will use the following notations :
• for a literal q, Resolvent(q, P ) denotes the set of clauses obtained by resolution from
the set P ∪ {q} but not from P alone. We call such clauses proper resolvents of q
w.r.t. P ,
• for a literal q, q̄ denotes its complementary literal,
• for a clause c of a peer P , S(c) (resp. L(c)) denotes the disjunction of literals of
c whose variables are shared (resp. not shared) with some acquaintance of P . The
278

Distributed Reasoning in a Peer-to-Peer Setting: Application to the Semantic Web

condition S(c) = ! thus expresses that c does not contain any variable shared with an
acquaintance of P ,
• a history hist is a sequence of triples (l, P, c) (where l is a literal, P a peer, and c
a clause). An history [(ln , Pn , cn ), . . . , (l1 , P1 , c1 ), (l0 , P0 , c0 )] represents a branch of
reasoning initiated by the propagation of the literal l0 within the peer P0 , which either
contains the clause ¬l0 ∨ c0 (in that case c0 may have been splitted into its different
literals among which l1 is propagated in P1 ), or not (in that case l0 = c0 and l0 is
propagated into P1 , and thus l0 = l1 ): for every i ∈ [0..n − 1], ci is a consequence of
li and Pi , and li+1 is a literal of ci , which is propagated in Pi+1 ,
• ! is the distribution operator on sets of clauses: S1 ! · · · ! Sn = {c1 ∨ · · · ∨ cn
|c1 ∈ S1 , . . . , cn ∈ Sn }. If L = {l1 , . . . , lp }, we will use the notation !l∈L Sl to denote
Sl1 ! · · · ! Slp .

3.1 Recursive Consequence Finding Algorithm

Let Γ = (P, acq) be a P2PIS, P one of its peers, and q a literal whose variable belongs
to the vocabulary of P . RCF (q, P ) computes implicates of the literal q w.r.t. P, starting
with the computation of local consequences of q, i.e., implicates of q w.r.t. P , and then
recursively following the acquaintances of the visited peers. To ensure termination, it is
necessary to keep track of the literals already processed by peers. This is done by the
recursive algorithm RCF H(q, SP, hist), where hist is the history of the reasoning branch
ending up to the propagation of the literal q in SP , which is the set of acquaintances of the
last peer added to the history.
Algorithm 1: Recursive consequence finding algorithm
RCF (q, P )
(1)return RCF H(q, {P }, ∅)
RCF H(q, SP, hist)
(1)if there exists P ∈ SP s.t. q ∈ P or if for every P ∈ SP , (q, P, ) ∈ hist return ∅
(2)else if (q̄, , ) ∈ hist return {!}
(3)else for every P ∈ SP local(P ) ← {q} ∪ Resolvent(q, P )
(4)if there exists P ∈ SP s.t. ! ∈ local(P ) return {!}
(5)else for every P ∈ SP local(P ) ← {c ∈ local(P )|L(c) ∈ T arget(P
!)}
(6)if for every P ∈ SP and for every c ∈ local(P ), S(c) = !, return P ∈SP local(P )
(7)else
!
(8) result ← P ∈SP {c ∈ local(P )|S(c) ∈ T arget(P )}
(9) foreach P ∈ SP and c ∈ local(P ) s.t. S(c) %= !
(10) if ¬q ∨ c ∈ P , P ← P \{¬q ∨ c}
(11) foreach literal l ∈ S(c)
(12)
answer(l) ← RCF H(l, acq(l, P ), [(q, P, c)|hist])
(13) disjcomb ← (!l∈S(c) answer(l)) ! {L(c)}
(14) result ← result ∪ disjcomb
(15) return result
279

Adjiman, Chatalic, Goasdoué, Rousset, & Simon

We now establish properties of this algorithm. Theorem 1 states that the algorithm
is guaranteed to terminate and that it is sound. Theorem 2 exhibits a condition on the
acquaintance graph for the algorithm to be complete. For the properties of soundness and
completeness, we consider that the topology and the content of the P2PIS do not change
while the algorithm is running. Therefore, those properties have the following meaning for
a P2PIS: the algorithm is sound (respectively, complete) iff for every P2PIS, the results
returned by RCF (q, P ), where P is any peer of the P2PIS and q any literal whose variable
belongs to the vocabulary of P , are implicates (respectively, include all the proper prime
implicates) of q w.r.t. the union of all the peers in the P2PIS, if there is no change in the
P2PIS while the algorithm is running.
The sufficient condition exhibited in Theorem 2 for the completeness of the algorithm
is a global property of the acquaintance graph: any two peers having a variable in common
must be either acquainted (i.e., must share that variable) or must be related by a path
of acquaintances sharing that variable. First, it is important to emphasize that even if
that property is global, it has not to be checked before running the algorithm. If it is not
verified, the algorithm remains sound but not complete. Second, it is worth noticing that
the modeling/encoding of applications into our general peer-to-peer propositional reasoning
setting may result in acquaintance graphs satisfying that global property by construction. In
particular, as it will be shown in Section 4 (Proposition 2), it is the case for the propositional
encoding of the Semantic Web applications that we deal with in Somewhere.
Theorem 1 Let P be a peer of a P2PIS and q a literal belonging to the vocabulary of P .
RCF (q, P ) is sound and terminates.
Proof: Soundness: We need to prove that every result returned by RCF (q, P ) belongs
to the target language and is an implicate of q w.r.t. P, where P is the union of all the
peers in the P2PIS. For doing so, we prove by induction on the number rc of recursive calls
of RCF H(q, SP, hist) that every result returned by RCF H(q, SP, hist) (where the history
hist, if not empty, is of the form [(ln , Pn , cn ), . . . , (l0 , P0 , c0 )]) is an implicate of q w.r.t.
P ∪ {ln , . . . , l0 } which belongs to the target language.
• rc = 0: either one of the conditions of Line (1), Line (2), Line (4) or Line (6) is
satisfied.
- If the condition in Line (1) is satisfied, the algorithm returns an empty result.
- If either there exists a peer P such that (q̄, , ) ∈ hist or ! ∈ local(P ): in both
cases, ! is returned by the algorithm (in respectively Line (2) and Line (4)) and it is
indeed an implicate of q w.r.t. P ∪ {ln , . . . , l0 } belonging to the target language.

- Let r be a result returned by the algorithm at Line (6): there exists P ∈ SP such
that r ∈ local(P ), and it is obvioulsy an implicate of q w.r.t. P ∪ {ln , . . . , l0 } (as q
or a resolvent of q and of a clause of P ), and it belongs to the target language.

• Suppose the induction hypothesis true for rc ≤ p, and let SP be a set of peers of
a P2PIS and q a literal (belonging to the vocabulary of all the peers of SP ) such
that RCF H(q, SP, hist) requires p + 1 recursive calls to terminate. Let r be a result
returned by RCF H(q, SP, hist).
280

Distributed Reasoning in a Peer-to-Peer Setting: Application to the Semantic Web

- If r ∈ local(P ) for a P ∈ SP and is such that S(r) = ! or S(r) ∈ T arget(P ), it is
obviously an implicate of q w.r.t. P ∪ {ln , . . . , l0 } belonging to the target language.

- If r %∈ local(P ) for any P ∈ SP , it is obtained at Line (13): there exist P ∈ SP and
a clause c of P of the form S(c) ∨ L(c) such that S(c) = ll1 ∨ · · · ∨ llk and r = r1 ∨ · · · ∨
rk ∨ L(c), where every ri is a result returned by RCF H(lli , acq(lli , P ), [(q, P, c)|hist])
(Line (12)). According to the induction hypothesis (the number of recursive calls of
RCF H(lli , acq(lli , P ), [(q, P, c)|hist]) for every lli is less than or equal to p), every
ri belongs to the target language and is an implicate of lli w.r.t. P\{¬q ∨ c} ∪
{q, ln , . . . , l0 }, or, equivalently, an implicate of q w.r.t. P\{¬q ∨ c} ∪ {lli , ln , . . . , l0 }.
Therefore, r1 ∨ · · · ∨ rk belongs to the target language and is an implicate of q w.r.t.
P\{¬q ∨ c} ∪ {S(c), ln , . . . , l0 }. Since L(c) belongs to the target language and c =
S(c)∨L(c), r (i.e, r1 ∨· · ·∨rk ∨L(c)) belongs to the target language, and is an implicate
of q w.r.t. P\{¬q ∨ c} ∪ {c, ln , . . . , l0 }, and a fortiori of q w.r.t. P ∪ {c, ln , . . . , l0 } Since
c ∈ local(P ), c is an implicate of q w.r.t. P, and therefore r is an implicate of q
w.r.t. P ∪ {ln , . . . , l0 }.

Termination: at each recursive call, a new triple (sl, P, c) is added to the history. If the
algorithm did not terminate, the history would be infinite, which is not possible since the
number of peers, literals and clauses within a P2PIS is finite.
!
The following theorem exhibits a sufficient condition for the algorithm to be complete.
Theorem 2 Let Γ = (P, acq) be a P2PIS. If for every P , P # and v ∈ VP ∩ VP ! there exists
a path between P and P # in Γ, all edges of which are labelled with v, then for every literal
q ∈ LP , RCF (q, P ) computes all the proper prime implicates of q w.r.t. P which belong to
T arget(P).

Proof: In fact, we prove that RCF (q, P ) computes at least all the prime proper resolvents
of q w.r.t. P , i.e., the elements of Resolvent(q, P ) that are not strictly subsumed by other
elements of Resolvent(q, P ).
We first show that proper prime implicates of q w.r.t. P are prime proper resolvents of
q w.r.t. P . Let m be a proper prime implicate of q w.r.t. P . By definition P ∪ {q} |= m
and P %|= m. By completeness of resolution w.r.t. prime implicates, m can be obtained by
resolution from the set P ∪ {q} but not from P alone, i.e., it is a proper resolvent of q w.r.t.
P . Let us suppose that m is strictly subsumed by another element m# of Resolvent(q, P ).
This means that P ∪ {q} |= m# |= m and m %≡ m# , which contradicts that m is a prime
implicate of q w.r.t. P .
Now we prove by induction on the maximum number rc of recursive calls involving a
same literal when triggering RCF H(q, SP, hist) that RCF H(q, SP, hist) computes all the
proper prime implicates belonging to the target language of q w.r.t. P(hist), where P(hist)
is obtained from P by replacing each ¬li ∨ ci by li if li %= ci . Thus:
P(hist) = P if hist is empty,
otherwise:
P(hist) = P\{¬li ∨ ci |(li , Pi , ci ) ∈ hist s.t. li %= ci } ∪{li |(li , Pi , ci ) ∈ hist s.t. li %= ci }
281

Adjiman, Chatalic, Goasdoué, Rousset, & Simon

If the history hist is not empty, it is of the form [(ln , Pn , cn ), . . . , (l0 , P0 , c0 )]. According
to the algorithm, when RCF H(q, SP, hist) is triggered, there have been at least n + 1
previous calls of the algorithm RCF H: RCF H(l0 , SP0 , ∅) and RCF H(li , SPi , histi ) for
i ∈ [1..n], where histi =[(li−1 , Pi , ci−1 ) . . . , (l0 , P0 , c0 )]), and Pi ∈ SPi for every i ∈ [0..n].
Since the P2PIS can be cyclic, it may be the case that when we call RCF H(q, SP, hist),
(q, P, q) ∈ hist. If that is the case, there have been previous calls of RCF H involving q,
i.e., of the form RCF H(q, SPi , histi ).
• rc = 0: either one of the conditions of Line (1), Line (2), Line (4) or Line (6) is
satisfied.
- If the first condition is satisfied, since rc = 0, it cannot be the case that for every
P ∈ SP , (q, P, ) ∈ hist, and therefore there exists P ∈ SP such that q ∈ P : in this
case, there is no proper prime implicate of q w.r.t. P(hist), because q ∈ P(hist) and
all the prime implicates of q w.r.t. a theory containing q are consequences of that
theory.
- If either (q̄, , ) ∈ hist or ! ∈ local(P ) for a given peer P of SP : in both cases,
! is the only prime implicate of q w.r.t. P(hist) and therefore, if it is a proper prime
implicate, it is the only one too. It is returned by the algorithm (respectively Line (2)
and Line (4)).
- If for every P ∈ SP , every resolvent of q w.r.t. P has no shared variable with
any acquaintance of P : if P satisfies the property stated in the theorem, this means
that every prime implicate of q w.r.t. P has no variable in common with any other
theory of P. According
to Lemma 1, the set of proper resolvents of q w.r.t. P(hist)
!
is included in P ∈SP local(P ), and thus in particular every proper prime implicate
of q w.r.t. P(hist), which is in the target language, is returned by the algorithm
(Line(6)).
• Suppose the induction hypothesis true for rc ≤ p, and let SP be a set of peers of a
P2PIS satisfying the property stated in the theorem, such that RCF H(q, SP, hist)
requires atmost p + 1 recursive calls involving q. Since there is at least one recursive
call, the condition of Line (1) is not satisfied. Let m be in the target language and a
proper prime implicate of q w.r.t. P(hist), where P(hist) = P\{¬li ∨ ci |(li , Pi , ci ) ∈
hist s.t. li %= ci } ∪{li |(li , Pi , ci ) ∈ hist s.t. li %= ci }. Let us show that m belongs to
the result returned by RCF H(q, SP, hist).
- If m is a proper resolvent of q w.r.t. a given P of SP , then m ∈ local(P ) and is
returned by the algorithm since it is in the target language.
-If m is not a proper resolvent of q w.r.t. a given P of SP , then, according
to
!
Lemma 1, either (i) q has its variable !
in common with clauses in P(hist)\ P ∈SP P ,
or (ii) there exists
! a clause ¬q ∨ c in P ∈SP P such that c has variables in common
with P(hist)\ P ∈SP P and m is a proper resolvent of c w.r.t. P(hist)\{¬q ∨ c} ∪
{q}. In addition, it is a prime proper resolvent of c w.r.t. P(hist)\{¬q ∨ c} ∪ {q}.
Let us suppose that this is not the case. Then there exists some clause m# ∈
Resolvent(c, P(hist)\{¬q ∨ c} ∪ {q}), such that m# |= m and m# %≡ m. By soundness,
P(hist)\{¬q ∨ c} ∪ {q} ∪ {c} |= m# . Since P(hist)\{¬q ∨ c} ∪ {q} ∪ {c} ≡ P(hist) ∪ {q},
282

Distributed Reasoning in a Peer-to-Peer Setting: Application to the Semantic Web

P(hist) ∪ {q} |= m# with m# |= m and m# %≡ m, which contradicts that m is a prime
implicate of q w.r.t. P(hist).

(i) In the first case, according to the property stated in the theorem, the variable of
q is shared with other peers of the P2PIS than those in SP , and therefore q is involved
in an iteration of the loop of Line (9). According to the induction hypothesis (the
number of recursive calls to obtain answer(q) in Line (12) is less than or equal to
p) answer(q) includes the set of proper prime resolvents of q w.r.t. P(hist# ), which
are in the target language, where hist# = [(q, P, q)|hist], and thus P(hist# ) = P(hist).
Therefore, answer(q) includes the set of proper prime resolvents of q w.r.t. P(hist),
in particular m.
(ii) In the second case, according to the property stated in the theorem, c shares
variables with other peers of the P2PIS than those in SP . In addition, since m is
in the target language, the local variables of c are target variables. Therefore c is
involved in an iteration of the loop of Line (9). According to the induction hypothesis
(the number of recursive calls to obtain answer(l) in Line (12) is less than or equal
to p), for every l ∈ S(c), answer(l) includes the set of all proper prime resolvents
of l w.r.t. P(hist# ) and thus in particular, the set of all proper prime implicates
of l w.r.t. P(hist# ) which are in the target language. Since hist# = [(q, P, c)|hist]
with q %= c (because there is no duplicate literals in the clauses that we consider),
P(hist# ) = P(hist)\{¬q ∨ c} ∪ {q}. We can apply Lemma 2 to infer that DisjComp,
which is computed in Line (13), includes the set of proper prime implicates of c w.r.t.
P(hist)\{¬q ∨ c} ∪ {q}, which are in the target language, and in particular m.
!
Lemma 1 Let P be a set of clauses and q a literal. Let P # ⊆ P such that it contains clauses
having a variable in common with q. If m is a proper resolvent of q w.r.t. P , then :
• either m is a proper resolvent of q w.r.t. P # ,
• or the variable of q is common with clauses of P \P # ,
• or there exists a clause ¬q ∨ c of P # such that c has variables in common with clauses
of P \P # and m is a proper resolvent of c w.r.t. P \{¬q ∨ c} ∪ {q}.
Proof: Let m be a proper resolvent of q w.r.t. P . If m is different from q, there exists a
clause ¬q ∨ c in P such that m is a proper resolvent of c w.r.t. P ∪ {q}. Since P \{¬q ∨ c} ∪
{q} ≡ P ∪ {q}, m is a proper resolvent of c w.r.t. P \{¬q ∨ c} ∪ {q}.
• If such a clause does not exist in P # , it exists in P \P # and therefore the variable of q
is common with clauses of P \P # .
• If there exists a clause ¬q ∨ c in P # such that m is a proper resolvent of c w.r.t.
P \{¬q ∨ c} ∪ {q}, and m is not a proper resolvent of q w.r.t. P # , then for every
proof of m there must exist a clause c# in P \P # with which either q or ¬q ∨ c must be
resolved. Therefore, either q or c has variables in common with clauses of P \P # . !
283

Adjiman, Chatalic, Goasdoué, Rousset, & Simon

Lemma 2 Let P be a set of clauses, and let c = l1 ∨ · · · ∨ ln be a clause. For every proper
prime implicate m of c w.r.t. P , there exists m1 , . . . , mn such that m ≡ m1 ∨ · · · ∨ mn , and
for every i ∈ [1..n], mi is a proper prime implicate of li w.r.t. P .

Proof: Let m be a proper prime implicate of c w.r.t. P . For every literal li , let M od(li )
be the set of models of P which make li true. If M od(li ) = ∅, that means that ! is
the only proper prime implicate of li w.r.t. P . For every i such that M od(li ) %= ∅, every model in M od(li ) is a model of P ∪ {c}, and then a model of m ; therefore, m is a
proper implicate of li w.r.t. P , and, by definition of proper prime implicates, there exists
a proper prime implicate mi of li w.r.t. P such that mi |= m. Consequently, there exists
m1 , . . . , mn such that m1 ∨ · · · ∨ mn |= m, and for every i ∈ [1..n], mi is a proper prime
implicate of li w.r.t. P (mi may be !). Since P ∪ {l1 ∨ · · · ∨ ln } |= m1 ∨ · · · ∨ mn , and
m is a proper implicate of l1 ∨ · · · ∨ ln w.r.t. P , we necessarily get that m ≡ m1 ∨ · · · ∨ mn . !

3.2 Message-based Consequence Finding Algorithm
In this section, we exhibit the result of the transformation of the previous recursive algorithm into DeCA: a message-based Decentralized Consequence finding Algorithm running
locally on each peer. DeCA is composed of three procedures, each one being triggered by
the reception of a message. The procedure ReceiveForthMessage is triggered by the
reception of a f orth message m(Sender, Receiver, f orth, hist, l) sent by the peer Sender
to the peer Receiver which executes the procedure: on the demand of Sender, with which
it shares the variable of l, it processes the literal l. The procedure ReceiveBackMessage
is triggered by the reception of a back message m(Sender, Receiver, back, hist, r) sent by
the peer Sender to the peer Receiver which executes the procedure: it processes the consequence r (which is a clause the variables of which are target variables) sent back by Sender
for the literal l (last added in the history) ; it may have to combine it with other consequences of literals being in the same clause as l. The procedure ReceiveFinalMessage
is triggered by the reception of a f inal message m(Sender, Receiver, f inal, hist, true): the
peer Sender notifies the peer Receiver that computation of the consequences of the literal
l (last added in the history) is completed. Those procedures handle two data structures
stored at each peer: cons(l, hist) caches the consequences of l computed by the reasoning
branch corresponding to hist ; final(q, hist) is set to true when the propagation of q within
the reasoning branch of the history hist is completed.
The reasoning is initiated by the user (denoted by a particular peer U ser) sending to a
given peer P a message m(U ser, P, f orth, ∅, q). This triggers on the peer P the local execution of the procedure ReceiveForthMessage(m(U ser, P, f orth, ∅, q)). In the description
of the procedures, since they are locally executed by the peer which receives the message,
we will denote by Self the receiver peer.
284

Distributed Reasoning in a Peer-to-Peer Setting: Application to the Semantic Web

Algorithm 2: DeCA message passing procedure for propagating literals forth
ReceiveForthMessage(m(Sender, Self, f orth, hist, q))
(1) if (q̄, , ) ∈ hist
(2) send m(Self, Sender, back, [(q, Self, !)|hist], !)
(3) send m(Self, Sender, f inal, [(q, Self, true)|hist], true)
(4) else if q ∈ Self or (q, Self, ) ∈ hist
(5) send m(Self, Sender, f inal, [(q, Self, true)|hist], true)
(6) else
(7) local(Self ) ← {q} ∪ Resolvent(q, Self )
(8) if ! ∈ local(Self )
(9)
send m(Self, Sender, back, [(q, Self, !)|hist], !)
(10) send m(Self, Sender, f inal, [(q, Self, true)|hist], true)
(11) else
(12) local(Self ) ← {c ∈ local(Self )| L(c) ∈ T arget(Self )}
(13)
if for every c ∈ local(Self ), S(c) = !
(14)
foreach c ∈ local(Self )
(15)
send m(Self, Sender, back, [(q, Self, c)|hist], c)
(16)
send m(Self, Sender, f inal, [(q, Self, true)|hist], true)
(17) else
(18)
foreach c ∈ local(Self )
(19)
if S(c) = !
(20)
send m(Self, Sender, back, [(q, Self, c)|hist], c)
(21)
else
(22)
foreach literal l ∈ S(c)
(23)
if l ∈ T arget(Self )
(24)
cons(l, [(q, Self, c)|hist]) ← {l}
(25)
else
(26)
cons(l, [(q, Self, c)|hist]) ← ∅
(27)
final(l, [(q, Self, c)|hist]) ← f alse
(28)
foreach RP ∈ acq(l, Self )
(29)
send m(Self, RP, f orth, [(q, Self, c)|hist], l)

Algorithm 3: DeCA message passing procedure for processing the return of consequences
ReceiveBackMessage(m(Sender, Self, back, hist, r))
(1) hist is of the form [(l! , Sender, c! ), (q, Self, c)|hist! ]
(2) cons(l! , hist) ← cons (l! , hist) ∪ {r}
(3) result← !l∈S(c)\{l! } cons(l, hist) ! {L(c) ∨ r}
(4) if hist! = ∅, U ← U ser else U ← the first peer P ! of hist!
(5) foreach cs ∈ result
(6) send m(Self, U, back, [(q, Self, c)|hist!], cs)

Algorithm 4: DeCA message passing procedure for notifying termination
ReceiveFinalMessage(m(Sender, Self, f inal, hist, true))
(1) hist is of the form [(l! , Sender, true), (q, Self, c)|hist!]
(2) final(l! , hist) ← true
(3) if for every l ∈ S(c), final(l, hist) = true
(4) if hist! = ∅ U ← U ser else U ← the first peer P ! of hist!
(5) send m(Self, U, f inal, [(q, Self, true)|hist!], true)
(6) foreach l ∈ S(c)
(7)
cons(l, [(l, Sender, ), (q, Self, c)|hist! ]) ← ∅
285

Adjiman, Chatalic, Goasdoué, Rousset, & Simon

The following theorem states two important results: first, the message-based distributed
algorithm computes the same results as the algorithm of Section 3.1, and thus, is complete
under the same conditions as in Theorem 2 ; second the user is notified of the termination
when it occurs, which is crucial for an anytime algorithm.
Theorem 3 Let r be a result returned by RCF (q, P ). If P receives from the user the message m(U ser, P, f orth, ∅, q), then a message m(P, U ser, back, [(q, P, )], r) will be produced.
If r is the last result returned by RCF (q, P ), then the user will be notified of the termination
by a message m(P, U ser, f inal, [(q, P, true)], true).
Proof: We prove by induction on the number of recursive calls of RCF H(q, SP, hist) that:
(1) for any result r returned by RCF H(q, SP, hist), there exists P ∈ SP such that
P is bound to send a message m(P, S, back, [(q, P, )|hist], r) after receiving the message
m(S, P, f orth, hist, q),
(2) if r is the last result returned by RCF H(q, SP, hist), all the peers P ∈ SP are
bound to send the message m(P, S, f inal, [(q, P, true)|hist], true), where S is the first peer
in the history.
• rc = 0: either one of the conditions of Lines (1), (2), (4) or (6) of the algorithm
RCF H(q, SP, hist) is satisfied. We have shown in the proof of Theorem 2 that if the
conditions of Lines (2) and (4) are satisfied, ! is the only result returned by the algorithm. The condition of Line (2) of the algorithm RCF H(q, SP, hist) corresponds to the
condition of Line (1) of the algorithm ReceiveForthMessage(m(S, P, f orth, hist, q))
for any P of SP , which triggers the sending of a message m(P, S, back, [(q, P, !)|hist], !)
(Line (2)) and of a message m(P, S, f inal, [(q, P, true)|hist], true) (Line(3)). If the condition of Line (4) of the algorithm RCF H(q, SP, hist) is satisfied, there exists P ∈ SP such
that ! ∈ P . That condition corresponds to the condition of Line (8) of the algorithm for
ReceiveForthMessage(m(S, P, f orth, hist, q)), which triggers the sending of a message
m(P, S, back, [(q, P, !)|hist], !) (Line (9) and of a message m(P, S, f inal, [(q, P, true)|hist], true)
(Line (10)). The condition (1) of the algorithm RCF H(q, SP, hist), in which no result is returned (see proof of Theorem 2), corresponds to the condition of Line (4) of the algorithm
ReceiveForthMessage(m(S, P, f orth, hist, q)), for every P ∈ SP , which only triggers
the sending of a final message (Line (5)). Finally, if the condition of Line (6) of the algorithm RCF H(q, SP, hist) is satisfied, there exists P ∈ SP such that r ∈ local(P ). The
condition of Line (6) of the algorithm RCF H(q, SP, hist) corresponds to the condition of
Line (13) in ReceiveForthMessage(m(S, P, f orth, hist, q)), which triggers the sending of
all the messages m(P, S, back, [(q, P, c)|hist], c), where c is a clause of local(P ) (Line (15)),
and in particular the message m(P, S, back, [(q, P, r)|hist], r). It triggers too the sending of
a final message (Line (16)) for P . If r is the last result returned by RCF H(q, SP, hist),
such final messages has been sent by every P ∈ SP .
• Suppose the induction hypothesis true for rc ≤ p, and let Γ = (P, acq) a P2PIS such
that RCF H(q, SP, hist) requires p + 1 recursive calls to terminate.
- If there exists P ∈ SP such that r ∈ local(P ), r is not the last result returned by
the algorithm, and r is one of the clauses c involved in the iteration of the loop of Line
(18) of the algorithm ReceiveForthMessage(m(S, P, f orth, hist, q)), and verifying the
condition of Line (19), which triggers the sending of the message m(P, S, back, [(q, P, r)|
hist], r) (Line (20)).
286

Distributed Reasoning in a Peer-to-Peer Setting: Application to the Semantic Web

-If there exists P ∈ SP and a clause c : l1 ∨ · · · ∨ lk ∨ L(c) of local(P ) such that c is
involved in the iteration of the loop of Line (9) of the algorithm RCF H(q, P, hist), and r
is an element r1 ∨ · · · ∨ rk ∨ L(c) of (!l∈S(c) answer(l)) ! {L(c)} computed at Line (12),
where each answer(l) is obtained as the result of RCF H(l, acq(l, P ), [(q, P, c)|hist]) (Line
(13)), which requires p or less than p recursive calls. By induction, for each literal li ∈ S(c),
there exists RPi ∈ acq(li , P ) such that RPi sends a message m(RPi , P, back, [(li , RPi , ),
(q, P, c)|hist], ri ) if it has received the message m(P, RPi , f orth, [(q, P, c)|hist], li ). The loop
of Line (11) of the algorithm RCF H(q, SP, hist) corresponds to the loop of Line (22) of
the algorithm ReceiveForthMessage(m(S, P, f orth,hist,q)), which triggers the sending
of the messages m(P, RPi , f orth, [(q, P, c)|hist], li ) for each literal li ∈ S(c) (Line (29)).
Therefore, according to the induction hypothesis, for every li ∈ S(c), RPi sends a message
m(RPi , P, back, [(li , RPi , ), (q, P, c)|hist], ri ). When the last of those messages (let us say
m(RPj , P, back, [(lj , RPj , ), (q, P, c)|hist], rj )) is processed, r is produced by Line (3) of
ReceiveBackMessage(m(RPj , P , back, [(lj , RPj , ), (q, P, c)|hist], rj )), and there exists a
peer U such that P is bound to send to it the message m(P, U, back, [(q, P, c)|hist], r) (Line
(6)).
- If r is the last result returned by the algorithm RCF H(q, SP , hist), for every P ∈
SP , for every c ∈ local(P ), for every l ∈ S(c), RCF H(l, acq(l, P ), [(q, P, c)|hist]) has
finished, and, by induction, every peer RP of acq(l, P ) has sent a message m(RP, P, f inal,
[(l, RP, true), (q, P, c)|hist], true). Therefore, the condition of Line (3) of the algorithm
ReceiveFinalMessage(m(RP, P, f inal, [(l, RP, ), (q, P, c)|hist], true)) is satisfied, which
triggers the sending of a message m(P, U, f inal, [(q, P, true)|hist], true) (Line (5)).
!
For sake of simplicity, both recursive and distributed algorithms have been presented
as applying to literals. It does not mean that the formulas that we consider are limited
to literals. Clauses can be handled by splitting them into literals and then using the !
operator to recompose the results obtained for each literal.
It is also important to notice that ! can be returned by our algorithm as a proper prime
implicate because of the lines (1) to (3) and (8) to (10) in ReceiveForthMessage. In
that case, as a corollary of the above theorems, the P2PIS is detected unsatisfiable with
the input clause. Therefore, our algorithm can be exploited for checking the satisfiability
of the P2PIS at each join of a new peer theory.

4. Application to the Semantic Web: the somewhere Peer-to-peer Data
Management System
The Semantic Web (Berners-Lee, Hendler, & Lassila, 2001) envisions a world wide distributed architecture where data and computational resources will easily inter-operate based
on semantic marking up of web resources using ontologies. Ontologies are a formalization of
the semantics of application domains (e.g., tourism, biology, medicine) through the definition of classes and relations modeling the domain objects and properties that are considered
as meaningful for the application. Most of the concepts, tools and techniques deployed so
far by the Semantic Web community correspond to the ”big is beautiful” idea that high
expressivity is needed for describing domain ontologies. As a result, when they are applied,
287

Adjiman, Chatalic, Goasdoué, Rousset, & Simon

the current Semantic Web technologies are mostly used for building thematic portals but
do not scale up to the Web. In contrast, Somewhere promotes a ”small is beautiful”
vision of the Semantic Web (Rousset, 2004) based on simple personalized ontologies (e.g.,
taxonomies of atomic classes) but which are distributed at a large scale. In this vision of
the Semantic Web introduced by Plu, Bellec, Agosto, and van de Velde (2003), no user
imposes to others his own ontology but logical mappings between ontologies make possible
the creation of a web of people in which personalized semantic marking up of data cohabits
nicely with a collaborative exchange of data. In this view, the Web is a huge peer-to-peer
data management system based on simple distributed ontologies and mappings.
Peer-to-peer data management systems have been proposed recently (Halevy et al.,
2003b; Ooi, Shu, & Tan, 2003; Arenas, Kantere, Kementsietsidis, Kiringa, Miller, & Mylopoulos, 2003; Bernstein, Giunchiglia, Kementsietsidis, Mylopoulos, Serafini, & Zaihraheu,
2002; Calvanese et al., 2004) to generalize the centralized approach of information integration systems based on single mediators. In a peer-to-peer data management system, there is
no central mediator: each peer has its own ontology and data or services, and can mediate
with some other peers to ask and answer queries. The existing systems vary according to
(a) the expressive power of their underlying data model and (b) the way the different peers
are semantically connected. Both characteristics have impact on the allowed queries and
their distributed processing.
In Edutella (Nejdl et al., 2002), each peer stores locally data (educational resources)
that are described in RDF relative to some reference ontologies (e.g., Dmoz - http://dmoz.org).
For instance, a peer can declare that it has data related to the concept of the dmoz taxonomy corresponding to the path Computers/Programming/Languages/Java, and that for
such data it can export the author and the date properties. The overlay network underlying Edutella is a hypercube of super-peers to which peers are directly connected. Each
super-peer is a mediator over the data of the peers connected to it. When it is queried, its
first task is to check if the query matches with its schema: if that is the case, it transmits
the query to the peers connected to it, which are likely to store the data answering the
query ; otherwise, it routes the query to some of its neighbour super-peers according to a
strategy exploiting the hypercube topology for guaranteeing a worst-case logarithmic time
for reaching the relevant super-peer.
In contrast with Edutella, Piazza (Halevy et al., 2003b, 2003a) does not consider that
the data distributed over the different peers must be described relatively to some existing
reference schemas. In Piazza, each peer has its own data and schema and can mediate
with some other peers by declaring mappings between its schema and the schemas of those
peers. The topology of the network is not fixed (as the hypercube in Edutella) but
accounts for the existence of mappings between peers: two peers are logically connected if
there exists a mapping between their two schemas. The underlying data model of the first
version of Piazza (Halevy et al., 2003b) is relational and the mappings between relational
peer schemas are inclusion or equivalence statements between conjunctive queries. Such a
mapping formalism encompasses the local-as-views and the global-as-views (Halevy, 2000)
formalisms used in information integration systems based on single mediators. The price
to pay is that query answering is undecidable except if some restrictions are imposed on
the mappings or on the topology of the network (Halevy et al., 2003b). The currently
implemented version of Piazza (Halevy et al., 2003a) relies on a tree-based data model: the
288

Distributed Reasoning in a Peer-to-Peer Setting: Application to the Semantic Web

data is in XML and the mappings are equivalence and inclusion statements between XML
queries. Query answering is implemented based on practical (but not complete) algorithms
for XML query containment and rewriting. The scalability of Piazza so far does not go
up to more than about 80 peers in the published experiments and relies on a wide range of
optimizations (mappings composition, Madhavan & Halevy, 2003, paths pruning, Tatarinov
& Halevy, 2004), made possible by the centralized storage of all the schemas and mappings
in a global server.
In Somewhere, we have made the choice of being fully distributed: there are neither
super-peers (as in Edutella) nor a central server having the global view of the overlay
network (as in Piazza). In addition, we aim at scaling up to thousands of peers. To
make it possible, we have chosen a simple class-based data model in which the data is
a set of resource identifiers (e.g., URIs), the schemas are (simple) definitions of classes
possibly constrained by inclusion, disjunction or equivalence statements, and mappings are
inclusion, disjunction or equivalence statements between classes of different peer schemas.
That data model is in accordance with the W3C recommendations since it is captured by
the propositional fragment of the OWL ontology language (http://www.w3.org/TR/owlsemantics). Note that OWL makes possible, through a declarative import mechanism, to
retrieve ontologies that are physically distributed. Using this transitive mechanism in peer
data management systems amounts in the worst case to centralized on a single peer the
whole set of peer ontologies, and to reason locally. Our feeling is that on very large networks
such a mechanism cannot scale up satisfactorily. Moreover, because of the dynamicity of
peer-to-peer settings, such imports would have to be re-actualized each time that a peer
joins or quit the network.
Section 4.1 defines the Somewhere data model, for which an illustrative example is
given in Section 4.2. In Section 4.3, we show how query rewriting in Somewhere, and thus
query answering, can be reduced by a propositional encoding to distributed reasoning in
propositional logic.
4.1 Somewhere Data model
In Somewhere, a new peer joins the network through some peers that it knows (its acquaintances) by declaring mappings between its own ontology and the ontologies of its
acquaintances. Queries are posed to a given peer using its local ontology. The answers
that are expected are not only instances of local classes but possibly instances of classes of
peers distant from the queried peer if it can be infered from the peer ontologies and the
mappings that they satisfy the query. Local ontologies, storage descriptions and mappings
are defined using a fragment of OWL DL which is the description logic fragment of the
Ontology Web Language recommended by W3C. We call OWL PL the fragment of OWL
DL that we consider in Somewhere, where PL stands for propositional logic. OWL PL is
the fragment of OWL DL reduced to the disjunction, conjunction and negation constructors
for building class descriptions.
4.1.1 Peer ontologies
Each peer ontology is made of a set of class definitions and possibly a set of equivalence,
inclusion or disjointness axioms between class descriptions. A class description is either the
289

Adjiman, Chatalic, Goasdoué, Rousset, & Simon

universal class (/), the empty class (⊥), an atomic class or the union (1), intersection (2)
or complement (¬) of class descriptions.
The name of atomic classes is unique to each peer: we use the notation P :A for identifying an atomic class A of the ontology of a peer P . The vocabulary of a peer P is the set
of names of its atomic classes.
Class descriptions
Logical notation
universal class
/
empty class
⊥
P :A
atomic class
conjunction
D1 2 D2
disjunction
D1 1 D2
¬D
negation

OWL notation
T hing
N othing
classID
intersectionOf (D1 D2)
unionOf (D1 D2)
complementOf (D)

Axioms of class definitions
Logical notation
OWL notation
Complete
P :A ≡ D
Class(P :A complete D)
Partial
P :A 3 D
Class(P :A partial D)

Axioms on class descriptions
Logical notation
OWL notation
equivalence
D1 ≡ D2
EquivalentClasses(D1 D2)
inclusion
D1 3 D2
SubClassOf (D1 D2)
disjointness
D1 2 D2 ≡ ⊥
DisjointClasses(D1 D2)

Taxonomies of atomic classes (possibly enriched by disjointness statements between
atomic classes) are particular cases of the allowed ontologies in Somewhere . Their specification is made of a set of inclusion (and disjointness) axioms involving atomic classes only:
there is no class definition using (conjunction, disjunction or negation) constructors.
4.1.2 Peer storage descriptions
The specification of the data that is stored locally in a peer P is done through the declaration
of atomic extensional classes defined in terms of atomic classes of the peer ontology, and
assertional statements relating data identifiers (e.g., URIs) to those extensional classes. We
restrict the axioms defining the extensional classes to be inclusion statements between an
atomic extensional class and a description combining atomic classes of the ontology. We
impose that restriction in order to fit with a Local-as-Views approach and an open-world
assumption within the information integration setting (Halevy, 2000). We will use the
notation P :V iewA to denote an extensional class V iewA of the peer P .
Storage description
declaration of extensional classes:
OWL notation
Logical notation
P :V iewA 3 C
SubClassOf (P :V iewA C)
assertional statements:
OWL notation
Logical notation
P :V iewA(a)
individual(a type(P :V iewA))
290

Distributed Reasoning in a Peer-to-Peer Setting: Application to the Semantic Web

4.1.3 Mappings
Mappings are disjointness, equivalence or inclusion statements involving atomic classes of
different peers. They express the semantic correspondence that may exist between the
ontologies of different peers.
4.1.4 Schema of a Somewhere network
In a Somewhere network, the schema is not centralized but distributed through the union
of the different peer ontologies and the mappings. The important point is that each peer has
a partial knowledge of the schema: it just knows its own local ontology and the mappings
with its acquaintances.
Let P be a Somewhere peer-to-peer network made of a collection of peers {Pi }i=1..n .
For each peer Pi , let Oi , Vi and Mi be the sets of axioms defining respectively the local
ontology of Pi , the declaration of its extensional classes and the set of mappings stated at Pi
between classes of Oi and classes of
! the ontologies of the acquaintances of Pi . The schema
of P, denoted S(P), is the union i=1..n Oi ∪ Vi ∪ Mi of the ontologies, the declaration of
extensional classes and of the sets of mappings of all the peers.
4.1.5 Semantics
The semantics isis the standard semantics of first order logic defined in terms of interpretations. An interpretation I is a pair (∆I , .I ) where ∆ is a non-empty set, called the domain
of interpretation, and .I is an interpretation function which assigns a subset of ∆I to every
class identifier and an element of ∆I to every data identifier.
An interpretation I is a model of the distributed
! schema of a Somewhere peer-topeer network P = {Pi }i=1..n iff for each axiom in i=1..n Oi ∪ Vi ∪ Mi is satisfied by I.
Interpretations of axioms rely on interpretations of class descriptions which are inductively
defined as follows:
• /I = ∆I , ⊥I = ∅
• (C1 1 C2 )I = C1I ∪ C2I
• (C1 2 C2 )I = C1I ∩ C2I
• (¬C)I = ∆I \C I
Axioms are satisfied if the following holds:
• C 3 D is satisfied in I iff C I ⊆ DI
• C ≡ D is satisfied in I iff C I = DI
• C 2 D ≡ ⊥ is satisfied in I iff C I ∩ D I = ∅
A Somewhere peer-to-peer network is satisfiable iff its (distributed) schema has a
model.
Given a Somewhere peer-to-peer network P = {Pi }i=1..n , a class description C subsumes a class description D iff for any model I of S(P) DI ⊆ C I .
4.2 Illustrative Example
We illustrate the Somewhere data model on a small example of four peers modeling four
persons Ann, Bob, Chris and Dora, each of them bookmarking URLs about restaurants
they know or like, according to their own taxonomy for categorizing restaurants.
291

Adjiman, Chatalic, Goasdoué, Rousset, & Simon

Ann, who is working as a restaurant critic, organizes its restaurant URLs according to
the following classes:
• the class Ann:G of restaurants considered as offering a ”good” cooking, among which
she distinguishes the subclass Ann:R of those which are rated: Ann:R 3 Ann:G
• the class Ann:R is the union of three disjoint classes Ann:S1, Ann:S2, Ann:S3 corresponding respectively to the restaurants rated with 1, 2 or 3 stars:
Ann:R ≡ Ann:S1 1 Ann:S2 1 Ann:S3
Ann:S1 2 Ann:S2 ≡ ⊥ Ann:S1 2 Ann:S3 ≡ ⊥
Ann:S2 2 Ann:S3 ≡ ⊥
• the classes Ann:I and Ann:O, respectively corresponding to Indian and Oriental
restaurants
• the classes Ann:C, Ann:T and Ann:V which are subclasses of Ann:O denoting Chinese, Taı̈ and Vietnamese restaurants respectively: Ann:C 3 Ann:O, Ann:T 3 Ann:O,
Ann:V 3 Ann:O
Suppose that the data stored by Ann that she accepts to make available deals with restaurants of various specialties, and only with those rated with 2 stars among the rated restaurants. The extensional classes declared by Ann are then:
Ann:V iewS2 3 Ann:S2, Ann:V iewC 3 Ann:C, Ann:V iewV 3 Ann:V ,
Ann:V iewT 3 Ann:T , Ann:V iewI 3 Ann:I
Bob, who is fond of Asian cooking and likes high quality, organizes his restaurant URLs
according to the following classes:
• the class Bob:A of Asian restaurants
• the class Bob:Q of high quality restaurants that he knows
Suppose that he wants to make available every data that he has stored. The extensional
classes that he declares are Bob:V iewA and Bob:V iewQ (as subclasses of Bob:A and Bob:Q):
Bob:V iewA 3 Bob:A, Bob:V iewQ 3 Bob:Q
Chris is more fond of fish restaurants but recently discovered some places serving a
very nice cantonese cuisine. He organizes its data with respect to the following classes:
• the class Chris:F of fish restaurants,
• the class Chris:CA of Cantonese restaurants
Suppose that he declares the extensional classes Chris:V iewF and Chris:V iewCA as subclasses of Chris:F and Chris:CA respectively:
Chris:V iewF 3 Chris:F , Chris:V iewCA 3 Chris:CA
Dora organizes her restaurants URLs around the class Dora:DP of her preferred restaurants, among which she distinguishes the subclass Dora:P of pizzerias and the subclass
Dora:SF of seafood restaurants.
Suppose that the only URLs that she stores concerns pizzerias: the only extensional class
that she has to declare is Dora:V iewP as a subclass of Dora:P : Dora:V iewP 3Dora:P
Ann, Bob, Chris and Dora express what they know about each other using mappings
stating properties of class inclusion or equivalence.
292

Distributed Reasoning in a Peer-to-Peer Setting: Application to the Semantic Web

Ann is very confident in Bob’s taste and agrees to include Bob’ selection as good
restaurants by stating Bob:Q 3 Ann:G. Finally, she thinks that Bob’s Asian restaurants
encompass her Oriental restaurant concept: Ann:O 3 Bob:A
Bob knows that what he calls Asian cooking corresponds exactly to what Ann classifies
as Oriental cooking. This may be expressed using the equivalence statement : Bob:A ≡
Ann:O (note the difference of perception of Bob and Ann regarding the mappings between
Bob:A and Ann:O)
Chris considers that what he calls fish specialties is a particular case of Dora seafood
specialties: Chris:F 3 Dora:SF
Dora counts on both Ann and Bob to obtain good Asian restaurants : Bob:A 2 Ann:G
3 Dora:DP
Figure 2 describes the resulting overlay network. In order to alleviate the notations, we
omit the local peer name prefix except for the mappings. Edges are labeled with the class
identifiers that are shared through the mappings between peers.
Dora
ontology :
DP 3 /,
P 3 DP , SF 3 DP,
V iewP 3 P
mappings :
Bob:A 2 Ann:G 3 Dora:DP

Ann:G

Bob:A

Dora:SF

Chris
ontology :
F 3 /, CA 3 /,
V iewF 3 F ,V iewCA 3 CA
mappings :
Chris:F 3 Dora:SF

Bob
ontology :
A 3 /, Q 3 /,
V iewA 3 A,
V iewQ 3 Q
mappings :
Bob:A ≡ Ann:O

Bob:Q,
Bob:A,
Ann:O

Ann
ontology :
G 3 /, O 3 /, I 3 /,
R 3 G,
(S1 1 S2 1 S3) ≡ R,
S1 2 S2 ≡ ⊥,
S1 2 S3 ≡ ⊥,
S2 2 S3 ≡ ⊥,
(C 1 V 1 T ) 3 O,
V iewC 3 C,
V iewV 3 V ,
V iewT 3 T ,
V iewI 3 I,
V iewS2 3 S2
mappings :
Ann:O 3 Bob:A,
Bob:Q 3 Ann:G

Figure 2: The restaurants network
4.3 Query Rewriting in Somewhere through Propositional Encoding
In Somewhere, each user interrogates the peer-to-peer network through one peer of his
choice, and uses the vocabulary of this peer to express his query. Therefore, queries are
logical combinations of classes of a given peer ontology.
The corresponding answer sets are expressed in intention in terms of the combinations
of extensional classes that are rewritings of the query. The point is that extensional classes
293

Adjiman, Chatalic, Goasdoué, Rousset, & Simon

of several distant peers can participate to the rewritings, and thus to the answer of a query
posed to a given peer.
Definition 6 (Rewritings) Given a Somewhere peer-to-peer network P = {Pi }i=1..n , a
logical combination Qe of extensional classes is a rewriting of a query Q iff Q subsumes Qe
w.r.t. P.
Qe is a proper rewriting if there exists some model of I of S(P) such that QIe %= ∅. Qe is a
conjunctive rewriting if it is a rewriting which is a conjunction of extensional classes.
Qe is a maximal (conjunctive) rewriting if there does not exist another (conjunctive) rewriting Q#e of Q (strictly) subsuming Qe w.r.t. P.
In general, finding all answers in a peer data management system is a critical issue
(Halevy et al., 2003b). In our setting however, we are in a case where all the answers can be
obtained using rewritings of the query: it has been shown (Goasdoué & Rousset, 2004) that
when a query has a finite number of maximal conjunctive rewritings, then all its answers
(a.k.a. certain answers) can be obtained as the union of the answer sets of its rewritings.
From the query answering point of view, it is the notion of proper rewriting which is relevant
because it guarantees a non empty set of answers. If a query has no proper rewriting, it
won’t get any answer.
In the Somewhere setting, query rewriting can be equivalently reduced to distributed
reasoning over logical propositional theories by a straighforward propositional encoding of
the query and of the distributed schema of a Somewhere network. It consists in transforming each query and schema statement into a propositional formula using class identifiers as
propositional variables.
The propositional encoding of a class description D, and thus of a query, is the propositional formula P rop(D) obtained inductively as follows:
• P rop(/) = true, P rop(⊥) = f alse
• P rop(A) = A, if A is an atomic class
• P rop(D1 2 D2 ) = P rop(D1 ) ∧ P rop(D2 )
• P rop(D1 1 D2 ) = P rop(D1 ) ∨ P rop(D2 )
• P rop(¬D) = ¬(P rop(D))
The propositional encoding of the schema S of a Somewhere peer-to-peer network P
is the distributed propositional theory P rop(S) made of the formulas obtained inductively
from the axioms in S as follows:
• P rop(C 3 D) = P rop(C) ⇒ P rop(D)
• P rop(C ≡ D) = P rop(C) ⇔ P rop(D)
• P rop(C 2 D ≡ ⊥) = ¬P rop(C) ∨ ¬P rop(D)
From now on, for simplicity, we use the propositional clausal form notation for the
queries and Somewhere peer-to-peer network schemas. As an illustration, let us consider
the propositional encoding of the example presented in Section 4.2. After application of the
transformation rules, conversion of each produced formula in clausal form and suppression
of tautologies, we obtain (Figure 3) a new acquaintance graph where each peer schema is
described as a propositional theory.
We now state two propositions showing that query rewriting in Somewhere can be
reduced to consequence finding in a P2PIS as presented in the previous sections.
294

Distributed Reasoning in a Peer-to-Peer Setting: Application to the Semantic Web

Dora : :
¬Dora:V iewP ∨ Dora:P
¬Dora:P ∨ Dora:DP
¬Dora:SF ∨ Dora:DP
¬Bob:A ∨ ¬Ann:G ∨ Dora:DP

Ann:G

Bob:A

Dora:SF

Bob :
¬Bob:V iewA ∨ Bob:A
¬Bob:V iewQ ∨ Bob:Q
¬Bob:A ∨ Ann:O
¬Ann:O ∨ Bob:A

Chris :
¬Chris:V iewF ∨ Chris:F
¬Chris:V iewCA ∨ Chris:CA
¬Chris:F ∨ Dora:SF

Bob:Q,
Bob:A,
Ann:O

Ann :
¬Ann:S1 ∨ ¬Ann:S2
¬Ann:S1 ∨ ¬Ann:S3
¬Ann:S2 ∨ ¬Ann:S3
¬Ann:S1 ∨ Ann:R
¬Ann:S2 ∨ Ann:R
¬Ann:S3 ∨ Ann:R
¬Ann:R ∨ Ann:S1 ∨ Ann:S2 ∨ Ann:S3
¬Ann:V iewS2 ∨ S2
¬Ann:R ∨ Ann:G
¬Bob:Q ∨ Ann:G
¬Ann:O ∨ Bob:A
¬Ann:C ∨ Ann:O
¬Ann:V ∨ Ann:O
¬Ann:T ∨ Ann:O
¬Ann:V iewC ∨ Ann:C ¬Ann:V iewV ∨ Ann:V
¬Ann:V iewT ∨ Ann:T
¬Ann:V iewI ∨ Ann:I
¬Chris:CA ∨ Ann:C

Chris:CA

Figure 3: The propositional encoding for the restaurant network

Proposition 1 states that the propositional encoding transfers satisfiability and establishes the connection between proper (maximal) conjunctive rewritings and proper (prime)
implicates.
Proposition 2 states that the P2PIS resulting from the propositional encoding of a Somewhere schema fulfills by construction the property exhibited in Theorem 2 as a sufficient
condition for the completeness of the algorithms described in Section 3 that compute proper
prime implicates of a clause w.r.t distributed propositional theories.
Proposition 1 Let P be a Somewhere peer-to-peer network and let P rop(S(P)) be the
propositional encoding of its schema. Let Ve be the set of all the extensional classes.
• S(P) is satisfiable iff P rop(S(P)) is satisfiable.
• qe is a proper maximal conjunctive rewriting of a query q iff ¬P rop(qe) is a proper
prime implicate of ¬P rop(q) w.r.t. P rop(S(P)) such that all its variables are extensional
classes.
Proof: We first exhibit some properties that will be used in the proof of the proposition. Let
P be a Somewhere network, S(P) its schema and P rop(S(P)) its propositional encoding.
For an interpretation I of S(P), and an element o of its domain of interpretation ∆I , we
define the interpretation po (I) of P rop(S(P)) as follows: for every propositional variable
v of P rop(S(P)) (v is the name of an atomic class of S(P)), v po (I) = true iff o ∈ v I . For
an interpretation J of P rop(S(P)), we define the interpretation i(J) of S(P) as follows:
the domain of i(J) is {true}, and for every atomic class A of S(P) (A is the name of a
propositional variable of P rop(S(P))), if AJ = true then Ai(J) = {true} else Ai(J) = ∅.
It is easy to show the following properties, for every interpretation I of S(P) (and every
o ∈ ∆I ), for every interpretation J of P rop(S(P)):
1. for every class description C of S(P):
295

Adjiman, Chatalic, Goasdoué, Rousset, & Simon

a) o ∈ C I ⇔ P rop(C)po (I) = true

b) true ∈ C i(J) ⇔ P rop(C)J = true
2. I is a model of S(P) ⇔ po (I) is a model of P rop(S(P))
3. i(J) is a model of S(P) ⇔ J is a model of P rop(S(P)).
• Suppose that S(P) is satisfiable and let I be a model of it and let o be an element
of ∆I : according to the above Property 2, po (I) is a model of P rop(S(P)), and thus
P rop(S(P)) is satisfiable. In the converse way, let J be a propositional model of
P rop(S(P)). According to the above Property 3, i(J) is a model of S(P), and thus
S(P) is satisfiable.
• Suppose that P rop(¬qe ) is a proper prime implicate of P rop(¬q) w.r.t. P rop(S(P)),
such that all its variables are extensional classes, and let us show that qe is a proper
maximal conjunctive rewriting of q.
Let us first show that qe is a rewriting of q. Suppose that it is not the case: there
exists an interpretation I of S(P) such that qeI %⊆ q I and thus an element of o ∈ ∆I
such that o ∈ qeI and o %∈ q I . According to Property 2, po (I) is a model of P rop(S(P)),
and according to Property 1.a, (P rop(qe ))po (I) = true and (P rop(q))po (I) %= true, i.e.,
(P rop(¬qe ))po (I) = f alse and (P rop(¬q))po (I) = true. This is impossible since it
would contradict the fact that P rop(¬qe ) is a proper prime implicate of {P rop(¬q)} ∪
P rop(S(P)). Therefore, qe must be a rewriting of q.
Let us show now that qe is a conjunctive rewriting of q, i.e., qe is a conjunction of
extensional classes. In S(P), extensional classes only appear in inclusion axioms.
Therefore, the propositional encoding of S(P) ensures that extensional classes only
appear as variables in P rop(S(P)) in negative literals. Resolution being sound and
complete for prime implicate computation, P rop(¬qe ) is obtained as the result of a
finite chain of resolutions, starting from the clauses of P rop(S(P)) and from P rop(¬q).
Since in P rop(S(P)) extensional classes only appear in negative literals, they only
appear in negative literals in the computed implicates, and in particular in P rop(¬qe).
Therefore, qe is a conjunction of extensional classes.
Let us show now that qe is a proper rewriting of q, i.e., that it is satisfiable in some
model of S(P). Since P rop(¬qe) is a proper prime implicate of P rop(¬q) w.r.t.
P rop(S(P)), there exists a model J of P rop(S(P)) s.t. P rop(¬qe )J = f alse and
thus P rop(qe )J = true. By Property 3, i(J) is a model of S(P) and according to the
above Property 1.b, true ∈ (qe )i(J) . Therefore there exists a model of S(P ) in which
qe is satisfiable, and thus qe is a proper rewriting of q.
Finally, let us show that qe is a maximal conjunctive rewriting of q. Suppose that
this is not the case. Then, there exists a conjunctive rewriting qe# of q such that
qe |= qe# and qe %≡ qe# . This means that there exists an interpretation I and an
element o ∈ ∆I such that o ∈ q # Ie , thus o ∈ q I , and o %∈ qeI . According to Property 1.a, P rop(qe# )po (I) = true, P rop(q)po (I) = true, and P rop(qe )po (I) = f alse, i.e.,
P rop(¬qe# )po (I) = f alse, P rop(¬q)po (I) = f alse, and P rop(¬qe )po (I) = true. This is
296

Distributed Reasoning in a Peer-to-Peer Setting: Application to the Semantic Web

impossible since it contradicts that P rop(¬qe ) is a prime implicate of P rop(¬q) w.r.t.
P rop(S(P)). Therefore, qe is a maximal conjunctive rewriting of q.
• Let us now prove the converse direction. Suppose that qe is a proper maximal conjunctive rewriting of a query q and let us show hat P rop(¬qe) is a proper prime implicate
of P rop(¬q) w.r.t. P rop(S(P)). By definition of a proper rewriting, for every model
I of S(P) qeI ⊆ q I , or equivalently (¬q)I ⊆ (¬qe )I and there exists a model I # of S(P)
!
such that qeI %= ∅.

Let us first show that P rop(¬qe ) is an implicate of P rop(¬q) w.r.t P rop(S(P)). Suppose that this is not the case, i.e., that {P rop(¬q)}∪P rop(S(P)) %|= P rop(¬qe ). Then,
there exists a model J of {P rop(¬q)} ∪ P rop(S(P)) such that P rop(¬qe )J = f alse.
According to the above Property 3, i(J) is a model of S(P). According to the above
Property 1.b, true ∈ (¬q)i(J) and (¬qe )i(J) = ∅. This is impossible since it contradicts
that (¬q)i(J) ⊆ (¬qe )i(J) . Therefore, P rop(¬qe ) is an implicate of P rop(¬q) w.r.t.
P rop(S(P)).
Let us now show that P rop(¬qe ) is a proper implicate of P rop(¬q) w.r.t. P rop(S(P)),
i.e., that P rop(¬qe) is not an implicate of P rop(S(P)) alone. By definition of a proper
!
!
rewriting, there exists a model I # of S(P) such that qeI %= ∅. Let o be an element of qeI .
According to Property 2, po (I # ) is a model of P rop(S(P)), and according to Property
!
!
1.a, (P rop(qe ))po (I ) = true, i.e., (P rop(¬qe ))po (I ) = f alse. Therefore, P rop(¬qe ) is
not an implicate of P rop(S(P)).
Finally, let us show that P rop(¬qe ) is a prime implicate of P rop(¬q) w.r.t. P rop(S(P)).

Let us show that if c is a clause such that P rop(S(P )) ∪ {P rop(¬q)} |= c and c |=
P rop(¬qe ), then c ≡ P rop(¬qe ). Since c |= P rop(¬qe ) and P rop(¬qe ) is a disjunction of negation of extensional classes, c is a disjunction of a subset of the literals of P rop(¬qe ). Let qe# be the conjunction of the extensional classes of c, then
c = P rop(¬qe# ). We have proved previously that if P rop(¬qe# ) is an implicate of
P rop(¬q) w.r.t. P rop(S(P )) then qe# is a rewriting of q, and similarly that if P rop(¬qe )
is an implicate of P rop(¬qe# ) w.r.t. P rop(S(P )) then qe# subsumes qe . Therefore, qe# is
a rewriting of qe which subsumes qe . Since qe is a maximal conjunctive rewriting of
q, qe# ≡ qe , thus ¬qe# ≡ ¬qe and P rop(¬qe# ) ≡ P rop(¬qe ), i.e. c ≡ P rop(¬qe ).
!

Proposition 2 Let P be a Somewhere peer-to-peer network and let P rop(S(P)) be the
propositional encoding of its schema. Let Γ = (P rop(S(P)), acq) be the corresponding
P2PIS, where the set of labelled edges acq is such that: (P”:A, P, P’) ∈ acq iff P”:A is
involved in a mapping between P and P’ (i.e, P” = P or P” = P’). For every P , P # and
v ∈ VP ∩ VP ! there exists a path between P and P # in Γ such that all edges of it are labelled
with v.
Proof: Let P and P # be two peers having a variable v in common. Since the vocabularies of
the local ontologies of different peers are disjoint, v is necessarily a variable P ## :A involved
297

Adjiman, Chatalic, Goasdoué, Rousset, & Simon

in a mapping declared between P and some its acquaintances P1 (and thus P ## = P or P ## =
P1 ), or between P # and some of its acquaintances P1# (in this case P ## = P # or P ## = P1# ).
- If v is of the form P ## :A such that P ## = P (respectively P ## = P # ), then P :A is an atomic
class of P (respectively P # :A is an atomic class of P # ) which is involved in a mapping between
P and P # , and therefore, there is an edge (and thus a path) between P and P # labelled with
v (P :A or P # :A respectively) in Γ.
- If v is of the form P ## :A such that P ## is distinct from P and P # , then P ## :A is an atomic
class of P ## , which is involved in a mapping between P ## and P and in a mapping between
P ## and P # . Therefore, there exists an edge between P ## and P labelled with v and an edge
between P ## and P # labelled with v, and thus a path between P and P # such that all edges
of it are labelled with v.
!
From those two propositions, it follows that the message-based distributed consequence
finding algorithm of Section 3.2 can be used to compute the maximal conjunctive rewritings
of a query. This algorithm computes the set of proper prime implicates of a literal w.r.t. a
distributed propositional clausal theory. Therefore, if it is applied to the distributed theory
resulting from the propositional encoding of the schema of a Somewhere network, with the
extensional classes symbols as target variables, and triggered with a literal ¬q, it computes
in fact the negation of the maximal conjunctive rewritings of the atomic query q. This result
also holds for any arbitrary query since, in our setting, the maximal rewritings of such a
query can be obtained by combining the maximal rewritings of its atomic components.
A corollary of these two propositions is that, in our setting, query answering is BH2 complete w.r.t. query complexity and polynomial w.r.t. data complexity.

5. Experimental Analysis
This section is devoted to an experimental study of the performances of the distributed
consequence finding algorithm described in Section 3, when deployed on real peer-to-peer
inference systems. Particularly, the aim of our experiments is to study scalability issues
of the Somewhere infrastructure for the Semantic Web. Our experiments have been
performed on networks of 1000 peers. Our goal is thus to study the practical complexity
of the reasoning on networks of this size and to answer questions such as: how deep and
how wide does the reasoning spread on the network of peers? Does the network cope with
the traffic load? How fast are the results obtained? To what extent do the results integrate
information from distinct peers? etc.
So far, large real corpus of distributed clausal theories are still missing. Since deploying
new real applications at this scale requires a significant amount of time, we have chosen
to perform these experiments on artificially generated instances of peer-to-peer networks.
These instances are characterized by the size and the form of the local theories corresponding
to each peer of the network, as well as by the size and the topology of the acquaintance
graph.
Since our aim is to use the Somewhere infrastructure for Semantic Web applications,
we focus our benchmarking to instances with suitable characteristics. In particular we generate local theories supposed to encode realistic ontologies and mappings, that could be
298

Distributed Reasoning in a Peer-to-Peer Setting: Application to the Semantic Web

written by people to categorize their own data. Acquaintances between peers are generated
in such way that the topology of the resulting graph looks realistic with respect to the
acquaintances between people on the Web. Therefore we focus our generation on acquaintance graphs having the so-called small world property, which is admitted (Newman, 2000)
as being a general property of social networks (including the Web).
In the following, we first detail in Section 5.1 the generation process of our benchmark
instances, the involved parameters and how they have been allowed to vary in our experiments. In Section 5.2, a first series of experiments studies the hardness of local reasoning
within a single peer by evaluating the number and the size of computed proper prime implicates. This allows us to realize the intrinsic complexity of this task and thus, of the
reasoning on large scale networks of such theories. It also helps us to justify the choice
of some parameter values for further experiments. Finally, Section 5.3 reports the experimental results that have been obtained concerning the scalability of Somewhere on large
networks of peers.
5.1 Benchmark Generation
Generating satisfactory instances of peer-to-peer networks for our framework means both
generating realistic propositional theories for each peer, as well as an appropriate structure
for the acquaintance graph. The latter is induced by the variables shared between peer
theories. In the setting of the Semantic Web, they correspond to names of atomic classes
involved in mappings between peer ontologies.
5.1.1 Generation of the local theories
We make the following assumptions on the ontologies and the mappings that are likely to
be deployed at large scale in the future Semantic Web: the ontologies will be taxonomies
of atomic classes (possibly with disjointness statements between pairs of atomic classes) ;
most mappings between such ontologies are likely to state simple inclusion or equivalence
between two atomic classes of two different peers, but we do not want to exclude some more
complex mappings involving logical combination of classes.
As a consequence, the propositional encoding of a taxonomy results in a set of clauses of
length 2. Most mappings result as well in clauses of length 2. The most complex mappings
might result in longer clauses, but since any set of clauses may equivalently be rewritten as
a set of clauses of length 3, we can restrict to the case where these are encoded with clauses
of length 3. Clauses encoding the mappings (called mapping clauses) are thus only clauses
of length 2 and 3, (2-clauses and 3-clauses for short). We denote by %3cnf the ratio of
3-clauses to the total number of mapping clauses. This ratio reflects in some way the degree
of complexity of the mappings. In our experiments we study variations of this parameter
because of its significant impact on the hardness of reasoning.
The local theories are generated in two steps. We first generate a set of m random
2-clauses on a number n of propositional variables and randomly select a number t (t ≤ n)
of target variables (corresponding to the names of extensional classes). Mapping clauses
are then generated, according to a given value %3cnf and added to the theories. Since
mapping clauses induce the structure of the acquaintance graph, the way they are generated
is discussed below. Peer theories are thus composed of only 2-clauses and 3-clauses. In the
299

Adjiman, Chatalic, Goasdoué, Rousset, & Simon

literature on propositional reasoning, such theories correspond to so-called 2 + p formulas,
where p denotes the proportion of 3-clauses in the whole set of clauses (note that p and
%3cnf are different ratios).
5.1.2 Generation of the acquaintance graph
In order to focus on realistic acquaintance graphs, we have chosen to generate random
graphs with “small worlds” properties, as proposed and studied by Watts and Strogatz
(1998), as well as Newman (2000). Such graphs have two properties that are encountered in
social networks: first, a short path length between any pair of nodes, observed in all social
relationship (for instance, the widely accepted “six-degrees of separation” between humans)
and, second, a high clustering ratio, a measurement of the number of common neighbors
shared by two neighbors (for instance, it is likely that two friends share a common subset
of friends).
To generate such graphs, we first generate the pairs of peers that are connected. Following the work of Watts and Strogatz (1998), we start from a so called k-regular ring
structure of np nodes, i.e., a graph the nodes of which may be arranged as a ring, and
such that each node is connected to its k closest neighbors. Edges of this graph are then
randomly rewired, with a given probability pr, by replacing one (or both) of the connected
peers with another peer. It has been shown that between regular graphs (pr = 0) and
uniform random graphs (pr = 1), the graphs generated with pr = 0.1 have ”small world”
properties. All acquaintance graphs used in our experiments have been generated in that
way, with pr = 0.1. Moreover, since our aim is to evaluate the scalability of our approach,
we have chosen to focus on networks of significant size. For all our instances, we have fixed
the number of peers np to 1000 and the number k of edges per peer to 10.
Once the topology of the network has been generated, local theories of each peer are
generated. Portion of the theories encoding taxonomies are first generated as previously
described. Then, for each edge of the generated graph mapping clauses are added. For
simplicity, we have chosen to add a fixed number q of mapping clauses for each edge.
Mapping clauses are randomly generated by picking one variable in each of the two peers
theories and by negating them with probability 0.5. With a %3cnf probability, a third
literal (randomly chosen between the two peers) is added to the clause to produce mapping
clauses of length 3. As a consequence, the average number of variables shared between two
connected peers is (2 + %3cnf ) ∗ q.
5.2 Hardness of Local Reasoning within a Single Peer
In our setting, the first part of the reasoning performed at each peer consists in computing
the proper prime implicates of the received litteral w.r.t. the local theory of the peer. In
order to grasp the hardness of this local reasoning we have first conducted an experimental
study to evaluate the number and the form of such implicates, and also, since our local
theories are 2 + p clausal theories, to evaluate the impact of the ratio p on these values.
These experiments have been performed using a modified version of Zres (Simon & del Val,
2001).
Prime implicates have been already studied for 3-CNF random formulas (Schrag &
Crawford, 1996). This corresponds to the case where p = 100%. We first take this as
300

Distributed Reasoning in a Peer-to-Peer Setting: Application to the Semantic Web

3CNF with m=30 clauses, 1500 experiments

3CNF with m=30 clauses, 1500 experiments

1

700

Number of prime implicates

600

500

Cumulative Distribution Function (CDF)

n=12 vars
n=14 vars
n=16 vars
n=18 vars
n=20 vars
n=22 vars
n=24 vars
n=26 vars
n=28 vars

400

300

200

100

0.9

0.8

0.7

n=12 vars
n=16 vars
n=20 vars
n=24 vars
n=28 vars

0.6

0.5

0.4

0.3

0.2

0.1

0

0

2

4

6

8

10

0
0
10

12

Size of prime implicates

1

10

2

10

3

10

4

10

5

10

6

10

Total number of literals

Figure 4: Prime Implicates in a uniform random 3-CNF theory
a reference for comparison with proper prime implicates. We consider 3-CNF theories of
m = 30 clauses over n variables (n ranging from 12 to 28). The left part of Figure 4 presents
the characteristics of prime implicates for different values of n. Each curve describes how
the prime implicates distribute according to their length. The curves correspond to average
values over 1500 experiments. For instance, for n = 28 variables, there are on average
more than 680 prime implicates of length 7. The right part of Figure 4 describes the size
of the whole set of prime implicates by means of the cumulative distribution function of
its total number L of literals. Each point (x, y) on a curve must be read as “over the
N = 1500 runs, y.N of them led to a L value smaller than x”. This representation is
convenient for exhibiting exponential distributions. The point to stress here is that for
such small formulas, the median value of the size of the whole set of prime implicates
already reaches more than ten thousand literals. On some rare runs (less than 5%), the
set of prime implicates has more than one hundred thousand literals. We can also observe
that augmenting the number of variables increases the difficulty since it results in longer
and longer prime implicates being produced in the final result. Note that a simple random
3CNF formula of only 30 clauses may already generate thousands of prime implicates. Since
computing implicates for random formulas is known to require lots of subsumption checks
(Simon & del Val, 2001), the final set of prime implicates may be very hard to obtain in
practice.
While such results are not new (Schrag & Crawford, 1996), it is interesting to compare
them with those obtained in similar conditions when computing proper prime implicates,
described on Figure 5. We can observe that curve shapes are very similar to those previously
obtained but that the values are one order of magnitude smaller. Note that for n = 28, the
median value of the size of the whole set of proper prime implicates is already about one
thousand of literals. And similarly, for large values of n, a majority of proper prime implicates are long. Intuitively, one may explain this phenomenon by the fact that proper prime
implicates are prime implicates of the initial theory augmented with an additional literal.
But this literal presumably induces clauses reductions in the theory and as a consequence
more subsumptions. The problem thus becomes a simplified – but not very much – version
of the prime implicates problem. From these experiments, a first conclusion is that, even
301

Adjiman, Chatalic, Goasdoué, Rousset, & Simon

3CNF with m=30 clauses, 100.n experiments

3CNF with m=30 clauses, 100.n experiments
1

90

Number of proper prime implicates

80

70

60

Cumulative Distribution Function (CDF)

n=10 vars
n=12 vars
n=14 vars
n=16 vars
n=18 vars
n=20 vars
n=22 vars
n=24 vars
n=26 vars
n=28 vars

50

40

30

20

10

0

0.9

0.8

0.7
n=12 vars
n=16 vars
n=20 vars
n=24 vars
n=28 vars

0.6

0.5

0.4

0.3

0.2

0.1

0

2

4

6

8

10

0
0
10

12

1

10

Size of proper prime implicates

2

10

3

10

4

10

5

10

Total number of literals

Figure 5: Proper Prime Implicates in a uniform random 3-CNF theory
2+p CNF with m=30 clauses, 1000 experiments

2+p CNF with m=30 clauses, 1000 experiments

1

800

Number of prime implicates

700

600

500

Cumulative Distribution Function (CDF)

p =0 %
p =10 %
p =20 %
p =30 %
p =40 %
p =50 %
p =60 %
p =70 %
p =80 %
p =90 %
p =100 %

400

300

200

100

0

0.9

0.8

0.7

0.6
p =0 %
p =10 %
p =20 %
p =30 %
p =40 %
p =50 %
p =60 %
p =70 %
p =80 %
p =90 %
p =100 %

0.5

0.4

0.3

0.2

0.1

0

2

4

6

8

10

0
0
10

12

Size of prime implicates

1

10

2

10

3

10

4

10

5

10

Size of prime implicates

Figure 6: Prime Implicates in a uniform random 2 + p-CNF theory (m = 30, n = 28)
for very small 3-CNF theories, the number of proper prime implicates may be quite large,
and some of them may be quite big.
Let us now focus our further experiments on local 2 + p CNF theories, with smaller
values of p, supposed to better correspond to the encoding of applications in which part of
the knowledge is structured as a tree or as a dag (which is encoded with 2-clauses). Figure
6 describes the prime implicates for a 2 + p CNF theory with m = 30 clauses and n = 28
variables, for values of p ranging from 0% to 100% (the curve corresponding to the case
p = 100% is the same as in Figure 4). Similarly, Figure 7 describes the characteristics of
the proper prime implicates for the different values of p. As previously, we can observe that
the curves have similar shapes. From the cumulative distribution function (CDF) curves, it
appears that the hardness (measured as the total size of the prime/proper prime implicates)
of the 2 + p CNF formula grows exponentially with the value of p. Even for small values of
p the problem may be hard. And as p increases, larger and larger clauses quickly appear in
the result.
Figure 8 studies the characteristics of proper prime implicates for a fixed and small
value of p = 10%, and increasing sizes m of the theory and of the number of variables n.
302

Distributed Reasoning in a Peer-to-Peer Setting: Application to the Semantic Web

2+p CNF with m=30 clauses, 100.n experiments

2+p CNF with m=30 clauses, 100.n experiments

1

100

Number of proper prime implicates

90

80

70

Cumulative Distribution Function (CDF)

p =0 %
p =10 %
p =20 %
p =30 %
p =40 %
p =50 %
p =60 %
p =70 %
p =80 %
p =90 %
p =100 %

60

50

40

30

20

0.8

0.7

p =0 %
p =10 %
p =20 %
p =30 %
p =40 %
p =50 %
p =60 %
p =70 %
p =80 %
p =90 %
p =100 %

0.6

0.5

0.4

0.3

0.2

0.1

10

0

0.9

0

2

4

6

8

10

0
0
10

12

Size of proper prime implicates

1

2

10

3

10

10

4

10

Size of proper prime implicates

Figure 7: Proper Prime Implicates in a uniform random 2+p-CNF theory (m = 30, n = 28)
2+p CNF with p=10%, 1000 experiments

2+p CNF with p=10%, 100.n experiments
1

0.9

Cumulative Distribution Function (CDF)

Cumulative Distribution Function (CDF)

1

0.8

0.7

0.6

0.5
m =30, n =28
m =40, n =37
m =50, n =46
m =60, n =55
m =70, n =64
m =80, n =73
m =90, n =82
m =100, n =91

0.4

0.3

0.2

0.1

0
1
10

0.95

0.9

0.85

0.8

m =30, n =28
m =40, n =37
m =50, n =46
m =60, n =55
m =70, n =64
m =80, n =73
m =90, n =82
m =100, n =91

0.75

0.7

0.65

0.6

0.55

2

10

3

10

4

10

Size of prime implicates

0.5
0
10

1

10

2

10

3

10

Size of proper prime implicates

Figure 8: Size of Prime and Proper Prime Implicates in a uniform random 2+p-CNF theory
for a fixed p
We have chosen a small value of p to focus on the characteristics of larger theories (up to
m = 100 clauses). It is worth noticing that even for m = 100 and p = 10%, the problem
seems simpler than for m = 30 and p = 100% (note that on the right part of Figure 6 the
y-axis has been rescaled to [0.5 − 1]). Such kinds of peer theories seem to have a reasonable
behavior from an integration perspective: half of the queries are very short and only a small
part of the queries are still very hard (the exponential distribution is still observed).
5.3 Scalability of Distributed Reasoning
The previous section has clearly shown that the local computation performed at each peer
may be really hard, even for small and simple theories. When focusing at the level of
whole Somewhere networks, the splitting/recombining strategy followed by our distributed
consequence finding algorithm clearly adds another source of combinatorial explosion. In
order to evaluate the scalability of Somewhere networks, we have performed two kinds of
303

Adjiman, Chatalic, Goasdoué, Rousset, & Simon

experiments. The first one aims at studying the behavior of Somewhere networks during
query processing. It consists in counting the number of messages circulating in a network
and the number of peers that are solicited while processing a query. In particular, we have
measured the distribution of the depth of query processing as well as the potential width of
a query. The depth of processing (depth for short) of a query is the maximum length of the
reasoning branches developed by the distributed algorithm for returning an answer to this
query. The width of the query measures the average number of neighbors that are solicited
by a peer during the reasoning. The second kind of experiments aims at evaluating the
processing time and the number of answers obtained for a query.
In all our experiments, the local theories of the 1000 peers of the network have been
generated as described in Section 5.1, with the fixed values m = n = 70 and t = 40. Those
numbers are close to what we would obtain by encoding taxonomies having the form of
balanced trees, with a depth between 3 and 5, in which each class has between 2 and 3
sub-classes, and the extensional classes of which correspond to the leaves of the tree. Each
peer theories contains in addition 10 ∗ (1 − %3cnf ) ∗ q mapping clauses of length 2 and
10 ∗ q ∗ %3cnf mapping clauses of length 3. Since we have seen in Section 5.2 that the
proportion of 3-clauses in each local theory has a strong impact on the hardness of the local
computing performed for each (sub)query at each peer, we have studied variations of the
two related parameters: q and %3cnf of increasing complexity. Note that the distributed
theories considered in these experiments are quite large since the size of the corresponding
centralized theories ranges from 80 000 up to 130 000 clauses over 70 000 variables.
5.3.1 Behavior of Distributed Query Processing
Let us now study the impact of the number q of mapping clauses per edge and of the ratio
%3cnf of mapping clauses of length 3, on the depth of queries. For this purpose we have
measured, for each pair (q, %3cnf ), the depth of 1000 random queries1 . Since we know (cf.
Section 5.2) that local computing may be sometimes very hard and therefore may require
a lot of time, it has been necessary to introduce an additional timeout parameter. Each
query is thus tagged with its remaining time to live, which, on each peer, is decreased of the
local processing time, before propagating the induced subqueries. For these experiments,
the timeout value has been set to 30 seconds.
Figure 9 shows the cumulative distribution functions corresponding to each pair (q, %3cnf ).
Each point on the figure reports a run, for a distinct query. The four leftmost curves correspond to cases where the query depth remains relatively small. For instance, for q = 2
and %3cnf = 0 none of the 1000 queries has a depth greater than 7. Altogether on the
four leftmost curves none of the queries has a depth greater than 36. This suggests that
our algorithm behaves well on such networks.
When the value of %3cnf increases, queries have longer depths. For instance, with
q = 3 and %3cnf = 20, we can observe that 22% of the queries have a depth greater
than 100 (the maximum being 134). The three rightmost curves have a similar shape,
composed of three phases: a sharp growth, corresponding to small depth queries, followed
1
For convenience, and since time is not here a direct issue (except from the timeout impact), these
experiments have been performed on a single computer, running the 1000 peers. This made the building of
reports relying on peer traces much easier.

304

Distributed Reasoning in a Peer-to-Peer Setting: Application to the Semantic Web

m=70, n=70, t=40, 1000 experiments
Cumulative Distribution Function (CDF)

1

0.9

0.8

0.7

q=2, %3cnf=0
q=2, %3cnf=20
q=3, %3cnf=0
q=3, %3cnf=5
q=3, %3cnf=10
q=3, %3cnf=15
q=3, %3cnf=20

0.6

0.5

0.4

0

20

40

60

80

100

120

140

Depth of queries

Figure 9: Cumulative distribution function of the depth of 1000 queries. q is the number of
mapping clauses per edge, and %3cnf is the ratio of 3-clauses in the mappings.
The y scale has been re-centered on [0.4 − 1.0].

by a plateau, and then a slower growth. The small depth query processing and the ’plateau’
are characteristics of an exponential distribution of values: most of the processing is easy,
but the little remaining is very hard. The slow growth observed is due to the timeout, a sideeffect of which is to bound the query depth. Without such a timeout, previous experiments
suggest that there would exist some queries requiring very long reasoning branches. This
point is outlined on the curve corresponding to the hardest cases (q = 3 and %3cnf = 20)
where there is no query of depth between 20 and 60. This suggests that when hard processing
appears, it is very hard. One may notice here that this last case corresponds to local theories
which are very close to one of cases studied in section 5.2. As a matter of fact, with q = 3
and %3cnf = 20 local theories are very close to a 2 + p theories of m = 100 clauses with
p = 6%.
In other experiments that we not detail here, we have checked that such an exponential
distribution of values cannot be observed when the acquaintance graphs have a strong structure of a ring (this corresponds to a rewiring probability p = 0 in the network generator).
Because such an exponential distribution can be observed on random graphs (corresponding to p = 1), we suspect that such a behavior is due to the short path length between
two peers, a property shared by both random and small world graphs. However, those two
types of graphs differ on their clustering coefficient, a property having a direct impact on
our algorithm behavior.
The depth of a query is the length of the history handled by the algorithm, in which a
given peer can appear several times since the processing of subqueries (resulting of several
splitting of clauses entailed by the initial query) can solicit the same peer several times.
305

Adjiman, Chatalic, Goasdoué, Rousset, & Simon

m=70, n=70, t=40, 20 000 experiments
Cumulative Distribution Function (CDF)

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6
q=2, %3cnf=0
q=3, %3cnf=20
q=3, %3cnf=100
q=5, %3cnf=100

0.55

0.5

0

5

10

15

20

25

Width of queries

Figure 10: Cumulative Distribution Function of queries width without timeouts. Each
curve summarise 20000 runs. The Y scale has been re-centered on [0.5 − 1.0],
the X axis on [0 − 25].

We have also measured the integration degree of queries, which is the number of distinct
peers involved in the query processing. We have observed the same kind of exponential
distributions of values than for the depth of queries, but with 20% smaller values. This
means that about one fifth of the history peers are repeated ones. That phenomenon was
not observed on random acquaintance graphs and this seems closely related to the small
world topology. It is important to point out that such a difference between small world and
random graphs could only be observed on large experimental data, with a large number of
peers.
We have also studied how wide the reasoning propagates in the network of peers during
query processing. For this purpose we have evaluated the average number of neighbors
peers that are solicited by a peer when solving a query. We have estimated this value
by generating 20000 random queries on random peers, and counting for each of them,
the number of induced subqueries to neighbor peers. Figure 10 shows, for different pairs
(q, %3cnf ), the corresponding cumulative distribution functions. For instance, for q = 2
and %3cnf = 0, more than 75% of the queries are solved locally and 15% of the remaining
ones are solved by asking only one neighbor. With q = 5 and %3cnf = 100, about 25% of
the queries solicit at least 10 neighbors. Of course, 25% of the subqueries may also solicit
at least 10 peers and so on.
To summarize, our experiments have pointed out a direct impact of the %3cnf value
on the hardness of query processing, which is not surprising considering the hardness of
clauses of length 3 for prime implicates computation. Those experiments also suggest an
exponential distribution of query depths, due to the short path length between two peers
306

Distributed Reasoning in a Peer-to-Peer Setting: Application to the Semantic Web

in the acquaintance graphs, and with an important repetition of solicited peers, due to the
large clustering coefficient of small world acquaintance graphs.
5.3.2 Time and Number of Answers
We now report a time performance study of our algorithm when it is deployed on a real
cluster of 75 heterogeneous computers2 . Based on the observations of the previous sections,
we have chosen to focus on 5 differents kinds of acquaintance graphs, denoted Very Easy,
Easy, Medium, Hard and Very Hard (see Table 1). One of the main goals of this section
is to estimate where the limits of processing of our algorithm are when it faces with hard
(and even very hard) Somewhere networks. Again, for all these experiments, we have set
the timeout value to 30s.
Network
1st ans.
10th ans.
100th ans.
1000thans.
all
% timeout
#answers
%unsat

Very Easy
q=2
%3cnf = 0
0.04s (100%)
0.06s (14.3%)
–
–
0.07s
0%
5.17
4.4%

Easy
q=3
%3cnf = 20
1.26s (99.6%)
1.37s (25.6%)
2.11s (12.7%)
4.17s (6.80%)
5.56s
13.9%
364
3.64%

Medium
q=3
%3cnf = 100
1.58s (95.6%)
0.99s (33.3%)
0.84s (27.0%)
4.59s (21.2%)
14.6s
37.5%
1006
3.76%

Hard
q=5
%3cnf = 100
1.39s (89.3%)
1.13s (12.0%)
4.09s (10.7%)
11.35s (7.15%)
21.23s
66.9%
1004
1.84%

Very Hard
q = 10
%3cnf = 100
2.66s (49.7%)
5.38s (29.9%)
11.0s (9.0%)
16.6s (1.80%)
27.74s
86.9%
65
1.81%

Table 1: Characteristics of the query processing ranging from very easy to very hard cases.

The values reported in Table 1 are mean values over more than three hundred different
random queries. Each column indicates the time needed to produce respectively the 1st ,
10th , 100th and 1000th answer of a query. The mean time (in seconds) is followed by the
percentage of initial queries that are taken into account in the average. For instance, for a
medium case with q = 3, 12.7% of the queries have produced more than 100 answers, and
the 100th answer was given on average after 2.11 seconds (the average does not take into
account queries that did not produce at least 100 answers). The all row corresponds to
the mean time needed to produce all answers, including queries that lead to timeout, the
percentage of which is reported in the %timeout row. The last two rows report the mean
number of answers and the ratio of proved unsatisfiable queries w.r.t. the Somewhere
network (some unsatisfiable queries w.r.t. the network may not have been counted since
inconsistency might have been found after the timeout).
Unsurprisingly, no timeout occurs for the Very Easy case. It is known that satisfiability
checking and prime implicates computation are tractable for sets of clauses of length 2.
Moreover, the high partitioning of the global theory induced by the low value of q (number
of mappings per peer) is often a witness of “easy” cases for reasoning for centralized theories.
2
All computers were Linux Athlon units, with 1GB of RAM. 26 of them ran at 1.4GHz, 9 at 1.8GHz
and 40 at 2 GHz. Each computer was running around 14 randomly assigned peers

307

Adjiman, Chatalic, Goasdoué, Rousset, & Simon

The point to outline for this case is that there are 5 answers on average, and that they are
produced very quickly by our algorithm (in less than 0.1 seconds).
Let us now point out that, even on Medium and Hard instances, our algorithm produces
a lot of answers. For instance, we have obtained an average of 1006 answers for q = 3, and
of 1004 answers for q = 5. In addition, on those hard instances, 90% of runs produced at
least one answer. It is noticeable that in the Very Hard case (q = 10), half of the queries
produce at least one answer, even if only 13% of them do complete without a timeout. Let
us note yet that checking the satisfiability of the corresponding centralized theories can
also be very hard. As a matter of fact the formula corresponding to the centralized version
of all the distributed theories has n=70 000 variables and m=120 000 clauses, 50 000 of
which are of length 3. The ratio of 3-clauses in those 2 + p theories is thus p = 0.416. It
has been shown (Monasson, Zecchina, Kirkpatrick, Selman, & Troyansky, 1999) that, for
2 + p random formulas, if one does not restrict the locality of variables, the SAT/UNSAT
transition is continuous for p < p0 (p0 = 0.41) and discontinuous for p > p0 , like in 3-SAT
instances. Intuitively, for p > p0 , the random 2 + p-SAT problem shares the characteristics
of the random 3-SAT problems, which is well known as the canonical NP-Complete problem.
Let us recall that our generation model induces a high clustering of variables inside each
peer. Therefore we cannot claim that the corresponding centralized theories have exactly
the same cararacteristics as uniform 2+ p-SAT random formulas. However, if one only focus
on the values of the parameters m and n, for the characteristics of our Very Hard network,
the transition phase between SAT and UNSAT instance occurs at m/n=1.69. Here, we have
m/n=1.71, which is close enough from the transition phase to suspect that this is where
hard instances may be found.
To summarize, when deployed on a real cluster of heterogeneous computers, our algorithm scales very well. Even on Very Hard instances that share some characteristics of a very
large 2 + p-SAT formula at the crossover between the 2-SAT/3-SAT and the SAT/UNSAT
transitions, our algorithm is able to return many answers in a reasonable time.

6. Related work
In Section 6.1, we situate our work w.r.t. existing work related to distributed reasoning or
distributed logics, while in Section 6.2 we summarize the distinguishing points of Somewhere among the existing peer data management systems.
6.1 Related Work on Distributed Reasoning
The message passing distributed algorithm that we have described in Section 3 proceeds
by splitting clauses and distributing the work corresponding to each piece of clause to
appropriate neighbor peers in the network. The idea of splitting formulas into several
parts may be found back as the so called ”splitting rule” in the natural deduction calculus,
introduced in the middle of the 1930’s by two independent works of Gentzen (1935, 1969)
and Jaśkowski (1934). Our algorithm may be viewed as a distributed version of Ordered
Linear Deduction (Chang & Lee, 1973) to produce new target clauses. This principle has
been extended by Siegel (1987) in order to produce all implicates of a given clause belonging
to some target language, and further extended to the first order case by Inoue (1992).
308

Distributed Reasoning in a Peer-to-Peer Setting: Application to the Semantic Web

We have already pointed out the differences between our work and the approach of Amir
and McIlraith (2000). In a peer-to-peer setting, tree decomposition of the acquaintance
graph is not possible. In addition, as we have shown in the introductory example, the
algorithm of Amir and McIlraith (2000) is not complete in the general case for proper
prime implicate computation. However, Goasdoué and Rousset (2003) have shown that
completeness can be guaranteed for a family of P2PIS with peer/super-peers topology. It
describes how to encode a P2PIS with peer/super-peers into a partitioned propositional
theory in order to use the consequence finding algorithm of Amir and McIlraith (2000).
The model-based diagnosis framework for distributed embedded systems (Provan, 2002) is
based on the work of Amir and McIlraith (2000). We think it can benefit from our approach
to apply to a real peer-to-peer setting in which no global knowledge has to be shared.
Other forms of distributed reasoning procedures may be found in multiagent frameworks,
where several agents try to cooperate to solve complex problems. Problems addressed in
this way can generally be decomposed in several interacting subproblems, each of which is
addressed by one agent. This is the case for Distributed Constraint Satisfaction Problems
(DCSP, Yokoo, Durfee, Ishida, & Kuwabara, 1998). Given a set of variables, each of them
being associated to a given domain, and a set of constraints on theses variables, the problem is to assign each variable one value in its respective domain, in such a way that all
constraints are satisfied. In the distributed case, each variable is associated to some agent.
Constraints may either concern a set of variables relevant to a same agent or variables relevant to different agents. In the first case, they may be considered as characterizing the
local theory of the agent, while in the second case they may be assimilated to mapping constraints between agents. Each mapping constraint is assigned to one agent. The problem
addressed in DCSP is easier than the problem of consequence finding since it is a satisfiability problem, which is NP-complete. While centralized CSP are solved using a combination
of backtrack search and consistency techniques, algorithms used to solve DCSP use asynchronous versions of backtracking (Yokoo, Durfee, Ishida, & Kuwabara, 1992; Yokoo et al.,
1998) and consistency techniques (Silaghi, Sam-Haroud, & Faltings, 2000). Basically, agents
proceed by exchanging invalid partial affectations, until converging to a globally consistent
solution. Similar ideas may also be found in distributed ATMS (Mason & Johnson, 1989),
where agents exchange nogood sets in order to converge to a globally consistent set of justifications. Let us note that in contrast with the peer-to-peer vision, such methods aim at
sharing some global knowledge among all agents.
Probabilistic reasoning on bayesian networks (Pearl, 1988) has also given rise several
adaptations suited to distributed reasoning (e.g., the message passing algorithm of Pfeffer
& Tai, 2005). However the problem addressed in this context is different, since it consists in
updating a set of a posteriori beliefs, according to observed evidence and a set of conditional
probabilities. These conditional probabilities are of the form P (x|u1 , ..., un ) and describe
the probability of the event x when u1 and . . . and un are observed. They describe value
interactions that can be viewed as mappings between a conjunction of literals and a single
literal, which are oriented because of the nature or conditioning.
Another work using oriented mappings is the framework of distributed first order logic
(Ghidini & Serafini, 2000), in which a collection of first order theories can communicate
through bridge rules. This approach adopts an epistemic semantics where connections
between peer theories are reflected by mappings between the respective domains of inter309

Adjiman, Chatalic, Goasdoué, Rousset, & Simon

pretation of the involved peers. Based on that work, distributed description logics have been
introduced by Borgida and Serafini (2003) and a distributed tableau method is described
for reasoning in distributed description logics has been proposed by Serafini and Tamilin
(2004b). A reasoning algorithm has been implemented in Drago (Serafini & Tamilin,
2004a) where the bridge rules are restricted to inclusion statements between atomic concepts.
6.2 Related Work on Peer Data Management Systems
As we have pointed it out in Section 4, the Somewhere peer data management system
distinguishes from Edutella (Nejdl et al., 2002) by the fact that there is no need of
super-peers, and from Piazza (Halevy et al., 2003b, 2003a) because it does not require a
central server having the global view of the overlay network.
From the semantic point of view, Somewhereuses a propositional language and mappings correspond to unrestricted formulas. Its semantics is the standard propositional semantics and query answering is always decidable. The peer data management systems
investigated by Halevy et al. (2003b) use a relational language and their mappings correspond to inclusion statements between conjonctive queries. The semantics used in this work
is the standard FOL semantics, for which query answering is shown to be undecidable in the
general case. Restricting to acyclic mappings renders query answering decidable in Piazza,
but checking this property requires some global knowledge on the network topology and is
performed by the central server.
The peer data management system considered in the work of Calvanese et al. (2004)
is similar to that of Halevy et al. (2003b) but proposes an alternative semantics based on
epistemic logic. With that semantics it is shown that query answering is always decidable
(even with cyclic mappings). Answers obtained according to this semantics correspond to a
subset of those that would be obtained according to the standard FOL semantics. However,
to the best of our knowledge, these results are not implemented.
From the systems point of view, the recent work around the coDB peer data management system (Franconi, Kuper, Lopatenko, & Zaihrayeu, 2004) supports dynamic networks
but the first step of the distributed algorithm is to let each node know the network topology.
In contrast, in Somewhere no node does have to know the topology of the network.
The KadoP system (Abiteboul, Manolescu, & Preda, 2004) is an infastructure based
on distributed hash tables for constructing and querying peer-to-peer warehouses of XML
resources semantically enriched by taxonomies and mappings. The mappings that are considered are simple inclusion statements between atomic classes. Compared to KadoP (and
also to Drago, Serafini & Tamilin, 2004a), the mapping language that is dealt with in
Somewhere is more expressive than simple inclusion statements between atomic classes.
It is an important difference which makes Somewhere able to combine elements of answers
coming from different sources for answering a query, which KadoP or Drago cannot do.
Somewhere implements in a simpler setting the (not implemented) vision of peer to
peer data management systems proposed by Bernstein et al. (2002) for the relational model.
310

Distributed Reasoning in a Peer-to-Peer Setting: Application to the Semantic Web

7. Conclusion
The contributions of this paper are both theoretical and practical. We have provided the first
distributed consequence finding algorithm in a peer-to-peer setting, and we have exhibited
a sufficient condition for its completeness. We have developed a P2PIS architecture that
implements this algorithm and for which the first experimental results look promising. This
architecture is used in a joint project with France Télécom, which aims at enriching peerto-peer web applications with reasoning services (e.g., Someone, Plu et al., 2003).
So far, we have restricted our algorithm to deal with a vocabulary-based target language.
However, it can be adapted to more sophisticated target languages (e.g., implicates of a
given, maximal, length, language based on literals and not only variables). This can be
done by adding a simple tag over all messages to encode the desired target language.
Another possible extension of our algorithm is to allow more compact representation
of implicates, as proposed by Simon and del Val (2001). This work relies on an efficient
clause-distribution operator. It can be adapted by extending messages in our algorithm in
order to send compressed sets of clauses instead of one clause as it is the case right now,
without changing the deep architecture of our algorithm.
In the Semantic Web direction, we plan to deal with distributed RDF(S) resources
shared at large scale. RDF(S) (Antoniou & van Harmelen, 2004) is a W3C standard for
annotating web resources, which we think can be encoded in our propositonal setting.
In the distributed reasoning direction, we plan to consider more sophisticated reasoning
in order to deal with a real multi-agent setting, in which possible inconsistencies between
agents must be handled.

Acknowledgnents
This paper is a revised and extended version of an ECAI short paper (Adjiman, Chatalic,
Goasdoué, Rousset, & Simon, 2004) and of an IJCAI paper (Adjiman, Chatalic, Goasdoué,
Rousset, & Simon, 2005).

References
Abiteboul, S., Manolescu, I., & Preda, N. (2004). Constructing and querying peer-to-peer
warehouses of xml resources. In Proceedings of the2nd International VLDB Workshop
on Semantic Web and Databases.
Adjiman, P., Chatalic, P., Goasdoué, F., Rousset, M.-C., & Simon, L. (2004). Distributed
reasoning in a peer-to-peer setting. In de Mántaras, R. L., & Saitta, L. (Eds.), Proceedings of ECAI 2004 (16th European Conference on Artificial Intelligence), pp. 945–946.
ECCAI, IOS Press.
Adjiman, P., Chatalic, P., Goasdoué, F., Rousset, M.-C., & Simon, L. (2005). Scalability
study of peer-to-peer consequence finding. In Proceedings of IJCAI’05 (19th International Joint Conference on Artificial Intelligence), pp. 351–356. IJCAI.
Amir, E., & McIlraith, S. (2000). Partition-based logical reasoning. In Proceedings of KR’00
(7th International Conference on Principles of Knowledge Representation and Reasoning), pp. 389–400, Breckenridge, Colorado, USA. Morgan Kaufmann Publishers.
311

Adjiman, Chatalic, Goasdoué, Rousset, & Simon

Antoniou, G., & van Harmelen, F. (2004). A Semantic Web Primer. The MIT Press.
Arenas, M., Kantere, V., Kementsietsidis, A., Kiringa, I., Miller, R., & Mylopoulos, J.
(2003). The hyperion project: From data integration to data coordination. ACM
SIGMOD Record, 32 (3), 53–58.
Berners-Lee, T., Hendler, J., & Lassila, O. (2001). The semantic web. Scientific American,
284 (5), 35–43.
Bernstein, P., Giunchiglia, F., Kementsietsidis, A., Mylopoulos, J., Serafini, L., & Zaihraheu,
I. (2002). Data management for peer-to-peer computing: A vision. In Proceedings of
WebDB 2002 (5th International Workshop on the Web and Databases).
Borgida, A., & Serafini, L. (2003). Distributed description logics: Assimilating information
from peer sources. Journal of Data Semantics, 1, 153–184.
Calvanese, D., De Giacomo, G., Lenzerini, M., & Rosati, R. (2004). Logical foundations of
peer-to-peer data integration. In Deutsch, A. (Ed.), Proceedings of PODS 2004 (20th
Symposium on Principles of Database Systems), pp. 241–251, Paris, France. ACM.
Chang, C. L., & Lee, R. C. (1973). Symbolic Logic and Mechanical Theorem Proving.
Computer Science Classics. Academic Press.
Dechter, R., & Rish, I. (1994). Directed resolution: the davis-putnam procedure revisited.
In Proceedings of KR’94 (4th International Conference on Principles of Knowledge
Representation and Reasoning), pp. 134–145, Bonn, Germany. Morgan Kaufmann.
del Val, A. (1999). A new method for consequence finding and compilation in restricted
languages. In Proceedings of AAAI 1999 (16th National Conference on Artificial
Intelligence), pp. 259–264, Orlando, FL, USA. AAAI Press / The MIT Press.
Franconi, E., Kuper, G., Lopatenko, A., & Zaihrayeu, I. (2004). Queries and updates in
the coDB peer to peer database system. In Proceedings of VLDB’04 (30th International Conference on Very Large Databases), pp. 1277–1280, Toronto, Canada. Morgan
Kaufmann.
Gentzen, G. (1935). Untersuchungen über das logisches Schließen.
Zeitschrift, 1, 176–210.

Mathematische

Gentzen, G. (1969). Collected Works, edited by M.E. Szabo. North-Holland, Amsterdam.
Ghidini, C., & Serafini, L. (2000). Frontiers Of Combining Systems 2, chap. Distributed
First Order Logics, pp. 121–139. No. 7 in Studies in Logic and Computation. Research
Studies Press Ltd.
Goasdoué, F., & Rousset, M.-C. (2003). Querying distributed data through distributed
ontologies: a simple but scalable approach. IEEE Intelligent Systems, pp. 60–65.
Goasdoué, F., & Rousset, M.-C. (2004). Answering queries using views: a krdb perspective
for the semantic web. ACM Transactions on Internet Technology (TOIT), 4 (3), 255–
288.
Halevy, A. Y. (2000). Logic-based artificial intelligence, chap. Logic-based techniques in
data integration, pp. 575–595. Kluwer Academic Publishers.
312

Distributed Reasoning in a Peer-to-Peer Setting: Application to the Semantic Web

Halevy, A. Y., Ives, Z., Tatarinov, I., & Mork, P. (2003a). Piazza: data management infrastructure for semantic web applications. In Proceedings of WWW 2003 (12th International World Wide Web Conference), pp. 556–567. ACM Press.
Halevy, A. Y., Ives, Z. G., Suciu, D., & Tatarinov, I. (2003b). Schema mediation in peer
data management systems. In Dayal, U., Ramamritham, K., & Vijayaraman, T. M.
(Eds.), Proceedings of ICDE’03 (International Conference on Data Engineering), pp.
505–518, Bangalore, India. IEEE Computer Society.
Inoue, K. (1992). Linear resolution for consequence finding. Artificial Intelligence, 2-3 (56),
301–353.
Jaśkowski, S. (1934). On the rules of suppositions in formal logic. Studia Logica, 1, 5–32.
Reprinted in: S. McCall (ed.), Polish Logic 1920–1939, Clarendon Press, Oxford, pp.
232–258.
Madhavan, J., & Halevy, A. Y. (2003). Composing mappings among data sources. In
Proceedings of VLDB’03 (29th International Conference on Very Large Databases),
pp. 572–583, Berlin, Germany. Morgan Kaufman.
Marquis, P. (2000). Handbook on Defeasible Reasoning and Uncertainty Management Systems, vol 5: Algorithms for Defeasible and Uncertain Reasoning, Vol. 5, chap. Consequence Finding Algorithms, pp. 41–145. Kluwer Academic Publisher.
Mason, C., & Johnson, R. (1989). Distributed Artificial Intelligence II, chap. DATMS: a
framework for distributed assumption based reasoning, pp. 293–317. Pitman.
Monasson, R., Zecchina, R., Kirkpatrick, S., Selman, B., & Troyansky, L. (1999). 2+p-sat:
Relation of typical-case complexity to the nature of the phase transition. Random
Structure and Algorithms, 15, 414–435.
Nejdl, W., Wolf, B., Qu, C., Decker, S., Sintek, M., & al. (2002). Edutella: a p2p networking
infrastructure based on rdf. In Proceedings of WWW 2002 (11th International World
Wide Web Conference), pp. 604–615. ACM.
Newman, M. E. J. (2000). Models of the small world. Journal of Statistical Physics, 101,
819–841.
Ooi, B., Shu, Y., & Tan, K.-L. (2003). Relational data sharing in peer data management
systems. ACM SIGMOD Record, 23 (2).
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann.
Pfeffer, A., & Tai, T. (2005). Asynchronous dynamic bayesian networks. In Bacchus, F.,
& Jaakkola, T. (Eds.), Proceedings of UAI’05 (21th Conference on Uncertainty in
Artificial Intelligence).
Plu, M., Bellec, P., Agosto, L., & van de Velde, W. (2003). The web of people: A dual view
on the www. In Proceedings of WWW 2003 (12th International World Wide Web
Conference), Vol. Alternate Paper Tracks, p. 379, Budapest, Hungary.
Provan, G. (2002). A model-based diagnosis framework for distributed embedded systems.
In Fensel, D., Giunchiglia, F., McGuinness, D. L., & Williams, M.-A. (Eds.), Proceedings of KR’02 (8th International Conference on Principles of Knowledge Representation and Reasoning), pp. 341–352. Morgan Kaufmann.
313

Adjiman, Chatalic, Goasdoué, Rousset, & Simon

Rousset, M.-C. (2004). Small can be beautiful in the semantic web. In McIlaith, S., Plexousakis, D., & van Harmelen, F. (Eds.), Proceedings of ISWC 2004 (3rd International
Semantic Web Conference), Vol. 3298 of Lectures Notes in Computer Science, pp.
6–16, Hiroshima, Japan. Springer Verlag.
Schrag, R., & Crawford, J. (1996). Implicates and prime implicates in random 3-sat. Artificial Intelligence, 81 (1-2), 199–222.
Serafini, L., & Tamilin, A. (2004a). Drago: Distributed reasoning architecture for the semantic web. Tech. rep., ITC-IRST.
Serafini, L., & Tamilin, A. (2004b). Local tableaux for reasoning in distributed description
logics.. In Haarslev, V., & Möller, R. (Eds.), Proceedings of DL’04 (International
Workshop on Description Logics), Vol. 104 of CEUR Workshop Proceedings, Whistler,
British Columbia, Canada.
Siegel, P. (1987). Représentation et utilisation de la connaissance en calcul propositionnel.
Ph.D. thesis, Université d’Aix-Marseille II, Marseille, France.
Silaghi, M.-C., Sam-Haroud, D., & Faltings, B. (2000). Asynchronous search with aggregations. In Proceedings of AAAI 2000 (17th National Conference on Artificial
Intelligence), pp. 917–922. AAAI Press / The MIT Press.
Simon, L., & del Val, A. (2001). Efficient consequence finding. In Nebel, B. (Ed.), Proceedings of IJCAI’01 (17th International Joint Conference on Artificial Intelligence), pp.
359–365, Seattle, Washington, USA. Morgan Kaufmann.
Tatarinov, I., & Halevy, A. Y. (2004). Efficient query reformulation in peer data management
systems. In Proceedings of SIGMOD’04 (International Conference on the Management
of Data), pp. 539–550, New York, NY, USA. ACM Press.
Watts, D. J., & Strogatz, S. H. (1998). Collective dynamics of ’small-world’ networks.
Nature, 393, 440–442.
Yokoo, M., Durfee, E. H., Ishida, T., & Kuwabara, K. (1992). Distributed constraint satisfaction for formalizing distributed problem solving. In Proceedings of ICDS’92 (12th
IEEE International Conference on Distributed Computing Systems), pp. 614–621.
Yokoo, M., Durfee, E. H., Ishida, T., & Kuwabara, K. (1998). The distributed constraint
satisfaction problem: Formalization and algorithms. IEEE Transactions on Knowledge
and Data Engineering, 10 (5), 673–685.

314

Journal of Artificial Intelligence Research 25 (2006) 119-157

Submitted 04/05; published 02/06

Learning in Real-Time Search: A Unifying Framework
Vadim Bulitko
Greg Lee

BULITKO @ UALBERTA . CA
GREGLEE @ CS . UALBERTA . CA

Department of Computing Science
University of Alberta
Edmonton, Alberta T6G 2E8, CANADA

Abstract
Real-time search methods are suited for tasks in which the agent is interacting with an initially
unknown environment in real time. In such simultaneous planning and learning problems, the agent
has to select its actions in a limited amount of time, while sensing only a local part of the environment centered at the agent’s current location. Real-time heuristic search agents select actions using
a limited lookahead search and evaluating the frontier states with a heuristic function. Over repeated experiences, they refine heuristic values of states to avoid infinite loops and to converge to
better solutions. The wide spread of such settings in autonomous software and hardware agents has
led to an explosion of real-time search algorithms over the last two decades. Not only is a potential
user confronted with a hodgepodge of algorithms, but he also faces the choice of control parameters
they use. In this paper we address both problems. The first contribution is an introduction of a simple three-parameter framework (named LRTS) which extracts the core ideas behind many existing
algorithms. We then prove that LRTA*, -LRTA* , SLA*, and γ-Trap algorithms are special cases
of our framework. Thus, they are unified and extended with additional features. Second, we prove
completeness and convergence of any algorithm covered by the LRTS framework. Third, we prove
several upper-bounds relating the control parameters and solution quality. Finally, we analyze the
influence of the three control parameters empirically in the realistic scalable domains of real-time
navigation on initially unknown maps from a commercial role-playing game as well as routing in
ad hoc sensor networks.

1. Motivation
In this paper, we consider a simultaneous planning and learning problem. One motivating application lies with navigation on an initially unknown map under real-time constraints. As an example,
consider a robot driving to work every morning. Imagine the robot to be a newcomer to the town.
The first route the robot finds may not be optimal because traffic jams, road conditions, and other
factors are initially unknown. With the passage of time, the robot continues to learn and eventually
converges to a nearly optimal commute. Note that planning and learning happen while the robot is
driving and therefore are subject to time constraints.
Present-day mobile robots are often plagued by localization problems and power limitations,
but their simulation counter-parts already allow researchers to focus on the planning and learning
problem. For instance, the RoboCup Rescue simulation league (Kitano, Tadokoro, Noda, Matsubara, Takahashi, Shinjou, & Shimada, 1999) requires real-time planning and learning with multiple
agents mapping out an unknown terrain. Pathfinding is done in real time as various crises, involving
fire spread and human victims trapped in rubble, progress while the agents plan.
Similarly, many current-generation real-time strategy games employ a priori known maps. Full
knowledge of the maps enables complete search methods such as A* (Hart, Nilsson, & Raphael,
c
2006
AI Access Foundation. All rights reserved.

B ULITKO & L EE

1968) and Dijkstra’s algorithm (Dijkstra, 1959). Prior availability of the maps allows pathfinding
engines to pre-compute various data to speed up on-line navigation. Examples of such data include
visibility graphs (Woodcock, 2000), influence maps (Pottinger, 2000), space triangulation (Kallmann, Bieri, & Thalmann, 2003), state abstraction hierarchies (Holte, Drummond, Perez, Zimmer,
& MacDonald, 1994; Holte, 1996; Botea, Müller, & Schaeffer, 2004) and route waypoints (Reece,
Krauss, & Dumanoir, 2000). However, the forthcoming generations of commercial and academic
games (Buro, 2002) will require the agent to cope with initially unknown maps via exploration
and learning during the game, and therefore will greatly limit the applicability of complete search
algorithms and pre-computation techniques.
Incremental search methods such as dynamic A* (D*) (Stenz, 1995) and D* Lite (Koenig &
Likhachev, 2002) can deal with initially unknown maps and are widely used in robotics, including
DARPA’s Unmanned Ground Vehicle program, Mars rover, and other mobile robot prototypes (Herbert, McLachlan, & Chang, 1999; Thayer, Digney, Diaz, Stentz, Nabbe, & Hebert, 2000). They
work well when the robot’s movements are slow with respect to its planning speed (Koenig, 2004).
In real-time strategy games, however, the AI engine can be responsible for hundreds to thousands
of agents traversing a map simultaneously and the planning cost becomes a major factor. To illustrate: even at the smaller scale of the six-year old “Age of Empires 2” (Ensemble-Studios, 1999),
60-70% of simulation time is spent in pathfinding (Pottinger, 2000). This gives rise to the following
questions:
1. How can planning time per move, and particularly the first-move delay, be minimized so that
each agent moves smoothly and responds to user requests nearly instantly?
2. Given real-time execution, local sensory information, and initially unknown terrain, how can
the agent learn a near-optimal path and, at the same time, minimize the learning time and
memory required?
The rest of the paper is organized as follows. We first introduce a family of real-time search algorithms designed to address these questions. We then make the first contribution by defining a simple
parameterized framework that unifies and extends several popular real-time search algorithms. The
second contribution lies with a theoretical analysis of the resulting framework wherein we prove
convergence and completeness as well as several performance bounds. Finally, we evaluate the
influence of the control parameters in two different domains: single agent navigation in unknown
maps and routing in ad hoc sensor networks. Detailed pseudocode needed to reimplement the algorithms as well as to follow the theorem proofs is presented in Appendix A. Theorem proofs
themselves are found in Appendix B.

2. Real-time Heuristic Search and Related Work
We situate our survey of real-time heuristic search literature in the framework of agent-centered
search (Koenig, 2001). While traditional off-line search methods first plan a path from start to
goal state and then move the agent along the path, agent-centered search interleaves planning and
execution. Furthermore, the planning is restricted to the areas of the search space around the current
state of the agent such as the physical location of a mobile robot or the current board position in
a game. The agent-centered search methods thus satisfy these two requirements: (i) at all times,
the agent is in a single state which can be changed only via taking actions and, therefore, incurring
execution cost; (ii) the agent can see only the states around its current state.
120

L EARNING IN R EAL -T IME S EARCH : A U NIFYING F RAMEWORK

Real-time heuristic search methods are a subset of agent-centered search methods. They are
distinguished by the following two additional properties: (i) the planning time per move can be
upper-bounded by a user-supplied constant (hence real-time); (ii) they associate a heuristic function
with each state (hence heuristic). The former requirement is motivated by the need to act quickly in
control tasks such as flying a helicopter (Ng, Coates, Diel, Ganapathi, Schulte, Tse, Berger, & Liang,
2004) or playing a real-time strategy computer game (Buro, 2002). The latter property allows the
agent to avoid infinite cycles on a map by learning the proper distances from each state to the goal
state. Unlike learning the map and using it for planning (Stenz, 1995; Koenig & Likhachev, 2002;
Chimura & Tokoro, 1994; Sturtevant, 2005), acquiring high-quality heuristic values lets the agent
select the right action very quickly thereby improving its reaction time. Having the heuristic values
also allows the agent to pick the next best action should the primary choice become unavailable
(e.g., due to a road block on the map). Remarkably, learning state and state-action values is the
prevailing technique in Reinforcement Learning (Sutton, 2005).
Learning real-time search algorithms, such as LRTA* (Korf, 1990), interleave planning and
execution in an on-line decision-making setting. As the planning time per each action executed by
the agent is bounded, these algorithms can be used as control policies for autonomous agents, even
in an unknown and/or non-stationary environment (Koenig & Simmons, 1998; Koenig, 1999; Ishida
& Korf, 1991, 1995; Ishida, 1997). In particular, the ability to make decisions after only a smallscale local search makes these algorithms well suited for routing in large-scale wireless networks of
simple sensors and actuators where each node is aware only of its nearby neighbors and no global
map exists. In such scenarios, not only does the limited amount of computation per node suit the
low computing power and energy requirements, but also the network collectively learns its own
topology over time (Yu, Govindan, & Estrin, 2001; Shang, Fromherz, Zhang, & Crawford, 2003).
Since the pioneering LRTA* (Korf, 1990), research in the field of learning real-time heuristic
search has developed in several major directions. Ishida and Korf (1991) investigated modifications to LRTA* for non-stationary environments. Shimbo and Ishida (2003) studied convergence
to suboptimal solutions as well as mechanisms for bounding the amount of state space exploration.
Furcy and Koenig (2000) considered a different learning mechanism speeding up the convergence.
Russell and Wefald (1991) researched a decision-theoretic approach to balancing partial planning
and execution. Shue and Zamani (1993) and Bulitko (2004) proposed a backtracking component
suggesting yet another way to control the exploration of the environment.
Note that while the original LRTA* algorithm can be viewed as a special case of real-time dynamic programming (Barto, Bradtke, & Singh, 1995), in general, real-time heuristic search methods
have several notable differences from reinforcement learning methods. First, they usually assume
a deterministic environment and thus take advantage of more aggressive value update rules. Second, they employ non-trivial initial heuristics and can converge even faster when such a heuristic
is admissible (Pearl, 1984) by never decreasing heuristic values of states. Third, real-time heuristic
search methods often use more extensive and sophisticated local search methods than the -greedy
control policy commonly used in reinforcement learning. Examples include dynamically selected
lookahead search space in DTA* (Russell & Wefald, 1991), additional heuristic for tie-breaking
in FALCONS (Furcy & Koenig, 2000), and heuristic upper-bounds for safe space exploration in
bounded LRTA* (Shimbo & Ishida, 2003). This improves the quality of decision-making by compensating for inaccuracies in the heuristic function and speeds up the learning process. Finally,
backtracking extensions (Shue & Zamani, 1993; Bulitko, 2004) give the agent another mechanism
to maintain consistency of its heuristic.
121

B ULITKO & L EE

The multitude of learning real-time search algorithms (LRTA*, -LRTA* , δ-LRTA*, FALCONS, eFALCONS, SLA*, γ-Trap, SLA*T, DTA*, etc.) available to the user can be disorienting
since he has to decide not only on the algorithm to use but also on the algorithm parameters which
can have a major impact on performance. The problem is further complicated by the fact that the
empirical studies have been done in different test beds making the results incomparable directly. To
illustrate: Furcy and Koenig (2000) and Shimbo and Ishida (2003) appear to use the same testbed
(the 8-puzzle). Yet, their results on the same algorithm (LRTA*) differ substantially. A closer inspection reveals that two different goal states were used leading to the major discrepancies in the
performance. To compound these problems, most performance metrics have been measured with
the myopic lookahead search of depth one. This is in contrast to game-playing practice where most
competitive systems gain a substantial benefit from a deeper search horizon (Schaeffer, Culberson,
Treloar, Knight, Lu, & Szafron, 1992; Hsu, Campbell, & Hoane, 1995; Buro, 1995).
We take a step towards a unified view of learning in real-time search and make four contributions. First, we introduce a simple three-parameter framework (named LRTS) that includes LRTA*,
-LRTA* , SLA* and γ-Trap as its special cases. A by-product of this generalization is an extension
of the first three algorithms to variable depth lookahead. Second, we prove completeness and convergence for any combination of the parameters. Third, we prove non-trivial theoretical bounds on
the influence of the parameters. Fourth, we present a large-scale empirical evaluation in practically
relevant domains.

3. Problem Formulation
In this section, we formally introduce the learning real-time search problem settings and the metrics
we will be using.
Definition 3.1 The search space is defined as a tuple (S, A, c, s0 , Sg , h0 ) where S is a finite set of
states, A is a finite set of actions, c : S ×A → R+ is the cost function with c(s, a) being the incurred
cost of taking action a in state s, s0 is the initial state, Sg ⊂ S is the set of goal states, and h0 is the
initial heuristic (e.g., Manhattan distance).
We adopt the assumptions of Shue and Zamani (1993), Shimbo and Ishida (2003) that (i) for
every action in every state there exists a reverse action (possibly with a different cost), (ii) every
applicable action leads to another state (i.e., no self-loops), and (iii) at least one goal state in Sg is
reachable from s0 . Assumption (i) is needed for the backtracking mechanism used in SLA* (Shue
& Zamani, 1993). It holds in many combinatorial and path-planning problems. Its applicability
to domains where reversing an action may require a non-trivial sequence of actions is a subject
of future research. Assumption (ii) is adopted for the sake of proof simplicity. In other words,
the results presented in this paper hold even if self-loops (of positive cost) are present. As with
all real-time search algorithms we are unifying, we assume that the environment is stationary and
deterministic. Extension of our algorithms on dynamic/stochastic domains is a subject of current
research.
Definition 3.2 The execution cost of traveling from state s1 to state s2 denoted by dist(s1 , s2 )
is defined as the minimal cumulative execution cost the agent is to incur by traveling from s1
to s2 . Throughout the paper, we will assume that dist satisfies the standard triangle inequality:
∀s1 , s2 , s3 ∈ S [dist(s1 , s3 ) ≤ dist(s1 , s2 ) + dist(s2 , s3 )]. Then, for any state s its true execution
122

L EARNING IN R EAL -T IME S EARCH : A U NIFYING F RAMEWORK

cost h∗ is defined as the minimal execution cost to the nearest goal: h∗ (s) = minsg ∈Sg dist(s, sg ). A
heuristic approximation h to the true execution cost is called admissible iff ∀s ∈ S [h(s) ≤ h∗ (s)].
The value of h in state s will be referred to as the heuristic value of state s. We will assume that
the heuristic value of the initial heuristic function is 0 for any goal state. The latter assumption
holds trivially if the initial heuristic function is admissible. A depth d child of state s is any state s0
reachable from s in the minimum of d actions (denoted by ks, s0 k = d). The depth d neighborhood
of state s is then defined as S(s, d) = {sd ∈ S | ks, sd k = d}.
Definition 3.3 Search agents operate by starting in a fixed start state s0 and executing actions suggested by their control policies. A trial is defined as a sequence of states the algorithm visits between
the start state and the first goal state it encounters. Once a trial is completed, the agent is reset to the
start state and the next trial begins. Final trial is defined as the first trial on which no learning (i.e.,
updates to the heuristic function) occurs.1 A convergence run is the sequence of trials from the first
trial to the final trial.

4. Performance Measures
Each problem instance is fully specified by the search space which includes the start and goal states.
The agent is run until convergence and the following statistics are collected:
execution convergence cost is the sum of execution costs of the actions taken by the agent during
the convergence process (i.e., on the convergence run);
planning cost is the average cost of planning per action during the convergence process. Planning
cost of an action is the number of states the agent considered to decide on taking the action;
total convergence cost is the total planning convergence cost for all actions during convergence
plus the execution convergence cost scaled by a factor called “planning speed”. The scaling
factor represents the amount of planning (measured in the number of nodes considered) the
agent would be able to do in the time it takes to execute a unit of travel. For instance, the
computer on-board an AIBO robodog may be able to plan with 10,000 states in the time it
takes the AIBO to traverse 1 foot of distance on the ground. Correspondingly, the planning
speed will be 10,000 states per foot. This is a commonly used way of combining the execution
and planning costs into a single metric (Russell & Wefald, 1991; Koenig, 2004);2
memory is the total amount of memory (measured in the number of state values) the agent used
to store the heuristic function during the convergence run. Note that the initial heuristic is
usually represented in a compact algorithmic form as opposed to a table. Therefore, memory
is required to store only the heuristic values modified during learning;
first-move delay (lag) is the amount of planning time (measured in milliseconds3 ) the agent takes
before deciding on the first move. This is an important metric in real-time strategy games,
1. Note that if random tie-breaking is used, more learning can actually take place after a learning-free trial. We use fixed
tie-breaking throughout this paper for simplicity.
2. Note that unlike total planning cost, total convergence cost allows us to model domains where some actions are
expensive disproportionally to their running time (e.g., taking damage by running into a wall). Additionally, we use
the standard real-time search framework and assume that planning and execution are not simultaneous but interleaved.
3. All timings are taken on a PowerMac G5 running at 2GHz. Apple gcc 3.3 compiler is used.

123

B ULITKO & L EE

wherein hundreds to thousands of units can be tasked simultaneously and yet have to react to
user’s commands as quickly as possible;
suboptimality of the final solution is defined as the percentage by which the execution cost of
the final trial exceeds that of the best possible solution. For instance, if a pathfinding agent
had the execution cost of 120 on the final trial and the shortest path has the cost of 100, the
suboptimality is 20%.

5. Application Domains
In the paper, we illustrate the algorithms in two realistic testbeds: real-time navigation in unknown
terrain and routing in ad hoc wireless sensor networks. The former domain is more standard to
single-agent search and so we will use it throughout the paper to illustrate our points. The latter
domain is a relative newcomer to the field of real-time heuristic search. We will use it later in
the paper (Section 8) to demonstrate how learning single-search methods can handle a practically
important yet different task.
Note that both domains have the desired attributes listed in the introduction. Indeed, in both
domains, the state space is initially unknown, the agent is in a single current state to be changed via
executing actions, the agent can sense only a local part of the state space centered on the agent’s
current state, the planning time per action should be minimized, and there are repeated trials.
5.1 Real-time navigation in next-generation video games
The agent is tasked to travel from the start state (xs , ys ) to the single goal state (xg , yg ). The
coordinates are on a two-dimensional rectangular grid. In each state, up to eight moves are available
leading to the eight immediate neighbors. Each straight move (i.e.,
√ north, south, west, east) has a
travel cost of 1 while each diagonal move has a travel cost of 2. Each state on the map can be
passable or occupied by a wall. In the latter case, the agent is unable to move into it. Initially,
the map in its entirety is unknown to the agent. In each state (x, y) the agent can see the status
(occupied/free) in the neighborhood of the visibility radius v: {(x0 , y 0 ) | |x0 − x| ≤ v & |y 0 − y| ≤
v}. The initial heuristic we use is the so-called octile distance defined as the length of a shortest
path between the two locations if all cells are passable. It is a translation of Manhattan distance onto
the case of eight moves and can be easily computed in a closed form.
Note that classical A* search is inapplicable due to an initially unknown map. Specifically, it is
impossible for the agent to plan its path through state (x, y) unless it is either positioned within the
visibility radius of the state or has visited this state on a prior trial. A simple solution to this problem
is to generate the initial path under the assumption that the unknown areas of the map contain no
occupied states (the free space assumption of Koenig, Tovey, & Smirnov, 2003). With the octile
distance as the heuristic, the initial path is close to the straight line since the map is assumed to be
empty. The agent follows the existing path until it runs into an occupied state. During the travel,
it updates the explored portion of the map in its memory. Once the current path is blocked, A* is
invoked again to generate a new complete path from the current position to the goal. The process
repeats until the agent arrives at the goal. It is then reset to the start state and a new trial begins. The
convergence run ends when no new states are seen.4
4. Note that this framework can be easily extended to the case of multiple start and goal states.

124

L EARNING IN R EAL -T IME S EARCH : A U NIFYING F RAMEWORK

To increase planning efficiency, several methods of re-using information over subsequent planning episodes have been suggested. Two popular versions are D* (Stenz, 1995) and D* Lite (Koenig
& Likhachev, 2002). Powerful as they are, these enhancements do not reduce the first-move lag time.
Specifically, after the agent is given the destination coordinates, it has to conduct an A* search from
its position to the destination before it can move. Even on small maps, this delay can be substantial.
This is in contrast to LRTA* (Korf, 1990), which only performs a small local search to select the
first move. As a result, several orders of magnitude more agents can respond to a user’s request in
the time it takes one A* agent.
As present-day games circumvent these problem by using fully observable maps and precomputing auxiliary data structures beforehand, we conduct our experiments in a research prototype, called Hierarchical Open Graph (HOG), developed at the University of Alberta by Nathan
Sturtevant. It allows one to load maps from commercial role-playing and real-time strategy games
such as Baldur’s Gate (BioWare, 1999) and WarCraft III (Blizzard, 2002). We use five maps ranging
in size from 214 × 192 states (of which 2765 are traversable) to 235 × 204 (with 16142 traversable
states). The largest map is shown in Figure 1. A total of 1000 problem instances on the five maps
were randomly chosen so that their shortest path lengths fall in the ten bins: 1-10, 11-20, . . . , 91-100
with 100 problems per bin. We will use this suite of 1000 problems throughout the paper.

Figure 1: Left: one of the five test maps from Baldur’s Gate – a commercial role-playing game by
BioWare. Right: a close-up showing the agent as a dot in the middle.

6. LRTS: A Unifying Framework
In this section we will introduce the primary contribution of this paper — the unifying framework
of real-time heuristic search — in an incremental fashion. Namely, we will start with the base
algorithm, LRTA*, and then analyze three extensions: deeper lookahead, optimality weight, and
backtracking. Each extension is illustrated with a hand-traced micro-example and empirical results
in the real-time pathfinding domain. A unifying algorithm constitutes the section’s finale.
6.1 Learning Real-Time A* (LRTA*)
LRTA* introduced by Korf (1990), is the first and best known learning real-time heuristic search
algorithm . The key idea lies with interleaving acting and backing up heuristic values (Figure 2).5
5. For clarity, all pseudocode in this paper describes a single trial only.

125

B ULITKO & L EE

Specifically, in the current state s, LRTA* with a lookahead of one considers the immediate neighbors (lines 4-5 in Figure 2). For each neighbor state, two values are computed: the execution cost
of getting there from the current state (henceforth denoted by g) and the estimated execution cost of
getting to the closest goal state from the neighbor state (henceforth denoted by h). While g is known
precisely, h is a heuristic estimate. LRTA* then travels to the state with the lowest f = g + h value
(line 7). Additionally, it updates the heuristic value of the current state if the minimum f -value is
greater than the heuristic value of the current state (line 6).6
LRTA*
1 initialize the heuristic: h ← h0
2 reset the current state: s ← sstart
3 while s 6∈ Sg do
4
generate children one move away from state s
5
find the state s0 with the lowest f = g + h
6
update h(s) to f (s0 ) if f (s0 ) is greater
7
execute the action to get to s0
8 end while
Figure 2: LRTA* algorithm with a lookahead of one.
Korf (1990) showed that in finite domains where the goal state is reachable from any state and
all action costs are non-zero, LRTA* finds a goal on every trial. Additionally, if the initial heuristic
function is admissible then LRTA* will converge to an optimal solution after a finite number of
trials. Compared to A* search, LRTA* with a lookahead of one has a considerably shorter firstmove lag and finds suboptimal solutions much faster. However, converging to an optimal (e.g.,
lowest execution cost) solution can be expensive in terms of the number of trials and the total
execution cost. Table 1 lists measurements averaged over 1000 convergence runs on the five game
maps in the pathfinding domain (details in Section 5.1). The differences become more pronounced
for larger problems (Figure 3).
Table 1: LRTA* with a lookahead of one vs. A* in pathfinding.
Algorithm
A*
LRTA*

First-move lag
3.09 ms
0.05 ms

Convergence execution cost
159
9346

6.2 Extension 1: Deeper Lookahead Search
LRTA* follows the heuristic landscape to a local minimum. It avoids getting stuck there by raising
the heuristic values and eventually eliminating the local minimum. Thus, local heuristic minima,
caused by inaccuracies in the initial heuristic values, are eliminated through the process of learning.
Ishida (1997) referred to such inaccuracies in the heuristic as “heuristic depressions”. He studied
them for the basic case of LRTA* with a lookahead of one. Heuristic depressions were later generalized for the case of weighted LRTA* and arbitrary lookahead in (Bulitko, 2004) under the name
of γ-traps.
6. This condition is not necessary if the heuristic is consistent as f -values will be non-decreasing along all lookahead
branches. In general, there is no need to decrement the h-value of any state if the initial heuristic is admissible.

126

L EARNING IN R EAL -T IME S EARCH : A U NIFYING F RAMEWORK

4

8

3.5

LRTA*
A*

6
5
4
3
2

A*
2.5
2
1.5
1
0.5

1
0

LRTA*

3
Convergence execution cost

First move lag (ms)

7

x 10

0

10

20

30

40
50
60
70
Shortest solution length

80

90

100

0

0

10

20

30

40
50
60
70
Shortest solution length

80

90

100

Figure 3: Differences between LRTA* and A* as pathfinding problems scale up.
In the process of “filling in” heuristic depressions, the agent can incur substantial execution cost
by moving back and forth inside the depression. This happens due to the fact that LRTA* with a
lookahead of one is myopic and conducts a minimum amount of planning per move. If planning is
cheaper than execution, then a natural solution is to increase the amount of planning per move with
the hope of eliminating heuristic local minima at a lower execution cost.
In LRTA*, additional planning per move is implemented through deeper lookahead search. The
heuristic update and action selection rules introduced in the previous section can be extended to an
arbitrary lookahead depth in a manner inspired by the standard mini-max search in games (Korf,
1990). Korf called the new rule “mini-min” and empirically observed that deeper lookahead decreases the execution cost but increases the planning cost per move. The phenomenon can be illustrated with a hand-traceable example in Figure 4. Each of the six states is shown as a circle with
the actions shown as edges. Each action has the cost of 1. The initial heuristic h0 is admissible but
inaccurate. Heuristic values before and after the first trial are shown as numbers under each state
(circle).
goal

start

goal

h0:

0

1

2

1

1

3

h1:

0

1

2

3

3

3

5 moves

start

h0:

0

1

2

1

1

3

h1:

0

1

2

3

1

3

3 moves

Figure 4: Initial and final heuristics of LRTA* with a lookahead of one (left) and two (right).
Both LRTA* with a lookahead of one and two converge to their final heuristics in one trial. However,
the additional ply of lookahead takes advantage of the two extra heuristic values and, as a result,
reduces the execution cost from 5 to 3 moves. On the other hand, the planning cost of each move
increases from 2 to 4 nodes.
In general, more planning per action “compensates” for inaccuracies in the heuristic function
and allows one to select better moves. Table 2 demonstrates this effect averaged over the 1000
pathfinding problems. The reduction of execution cost due to deeper lookahead becomes more
pronounced in larger problems (Figure 5).
Lookahead search in LRTA*-like algorithms has received considerable attention in the literature.
Russell and Wefald (1991) and Koenig (2004) analyzed selection of the lookahead depth optimal in
terms of the total cost (i.e., a weighted combination of the planning and execution costs). Bulitko,
127

B ULITKO & L EE

Table 2: Effects of deeper lookahead in LRTA*.
Lookahead
1
3
5
7
9

First move lag
0.05 ms
0.26 ms
0.38 ms
0.68 ms
0.92 ms

Execution convergence cost
9346
7795
6559
5405
4423

4

Convergence execution cost

3.5

x 10

d=1

3

d=3
d=5
d=7

2.5
2

d=9

1.5
1
0.5
0

0

20

40

60
Solution length

80

100

Figure 5: Convergence of LRTA* with different lookaheads as problems scale up.
Li, Greiner, and Levner (2003) and Bulitko (2003) examined pathological cases of deeper lookahead
increasing both execution and planning costs.
6.3 Extension 2: Heuristic Weight
Scalability of LRTA* to large problems is determined by two of its attributes. First, it uses the
initial “optimistic” heuristic function for the unknown areas of the search space and more informed,
“realistic” function for the explored areas. This results in the so-called “optimism in the face of uncertainty” where the agent eagerly explores novel parts of the space. Consequently, the convergence
time is lengthened and the solution quality oscillates significantly over consecutive trials (Shimbo
& Ishida, 2003). Second, the complete convergence process can be intractable in terms of running
time and the amount of memory required to store the heuristic because optimal solutions are sought.
Indeed, even simple problems can be intractable in terms of finding optimal solutions (e.g., the
generalized sliding tile puzzle of Ratner & Warmuth, 1986).
A weighted version of LRTA* was proposed by Shimbo and Ishida (2003) as a systematic way
of giving up optimality of the converged solution. The authors argued that optimal solutions are
rarely needed in practice, nor can they be achieved in large domains. Consequently, the user must
settle for suboptimal but satisficing solutions. Weighted LRTA* is the same algorithm (LRTA*)
run with an inadmissible initial heuristic. The inadmissibility is bounded as for every state its h(s)
value is required to be upper-bounded by (1 + )h∗ (s) where h∗ (s) is the true distance to goal. The
resulting -admissibility leads to -optimality of the solution to which such an -LRTA* converges.
By giving up optimality in the -controlled fashion, the algorithm gains faster convergence, smaller
memory requirements, and higher stability. Note that 0-LRTA* is equivalent to the original LRTA*.
Shimbo and Ishida (2003) used Manhattan distance scaled by (1 + ) as the initial heuristic
h0 . The underlying idea can be illustrated in a five-state domain (Figure 6). On the left, the initial
128

L EARNING IN R EAL -T IME S EARCH : A U NIFYING F RAMEWORK

heuristic is listed immediately under the five states (shown as circles). Each action (shown with an
edge) has the execution cost of 1. Every trial of LRTA* will involve four moves and will update
the heuristic values. The values after each trial are listed as successive rows. After 16 moves (4
trials), LRTA* will converge to the perfect heuristic. On the right, we start with the same heuristic
multiplied by (1+) = 2. As shown there, 1-LRTA* takes only one trial (i.e., 4 moves) to converge.
goal

initial heuristic:

final heuristic:

start

0

0

1

1

2

0

1

1

2

2

goal

4 moves

start

initial heuristic:

0

0

2

2

4

final heuristic:

0

1

2

3

4

4 moves

4 moves

0

1

2

2

3

0

1

2

3

3

4 moves

0

1

2

3

4

4 moves

Figure 6: Heuristic values over successive trials of: LRTA* (left) and 1-LRTA* (right).
In general, by scaling the initial (admissible) heuristic by (1 +), the amount of underestimation
of an admissible initial heuristic relative to the perfect heuristic is reduced in many states. Therefore,
the number of updates (line 6 in LRTA*, Figure 2) needed to increase the initial heuristic value of a
state to its final value is reduced. Correspondingly, the execution convergence cost is lowered and
learning is sped up. Figure 7 illustrates the correlation in the pathfinding domain. The discrepancy
is the amount of underestimation in the initial heuristic (1 + )h0 averaged over all states of the map
at hand (s ∈ S):
avg
sgoal , s∈S

h∗ (s, sgoal )−̇(1 + )h0 (s, sgoal )
.
h∗ (s, sgoal )

(6.1)

Here −̇ denotes non-negative subtraction: a−̇b equals a − b if the result is positive and 0 otherwise.
As usual, h∗ is the perfect heuristic and h0 is the octile distance. The heuristics are taken with
respect to the 1000 random goal states (sgoal ) defined in Section 5.1.

Convergence execution cost

10000

8000

6000

4000

2000

0

0

2

4

6

8
10
12
Discrepancy (%)

14

16

18

20

Figure 7: Convergence cost of -LRTA* vs. the weighted initial heuristic discrepancy. The points
on the graph correspond to these values of (1 + ): 3.3, 2.0, 1.5, 1.1, 1.0.
On the negative side, the scaling is uniform over all states and will sometimes result in the scaled
values exceeding the perfect heuristic values. Such inadmissibility can lead to suboptimal solutions
129

B ULITKO & L EE

as illustrated in Figure 8. Specifically, the initial heuristic is admissible (the values of the five states
are shown above the circles in the left part of the figure). All actions (shown as arrows) have the
execution cost of 1. Thus, LRTA* converges to the optimal path (i.e., through the top state). On the
other hand, scaling the heuristic by (1 + ) = 3 (shown on the right) leads 2-LRTA* to converge on
a longer suboptimal path (through the bottom two states).
3

1
2

6

0

0

Start

Start

0

0

Goal

0

Goal

0

Figure 8: 2-LRTA* converges to a suboptimal path.
In summary, scaling an initial (admissible) heuristic by 1 +  tends to reduce the underestimation error of the heuristic thereby speeding up convergence. As the weighted heuristic becomes
progressively more overestimating, suboptimality of the final solutions increases and, eventually,
overall convergence slows down (due to less informative weighted heuristic). Table 3 illustrates the
trends in the pathfinding domain. Once again, the effect is more pronounced for larger problems
(Figure 9).
Note that a similar effect has been observed with weighted A* search wherein a weight w is
put on the heuristic h. When w exceeds 1, fewer nodes are expanded at the cost of suboptimal
solutions (Korf, 1993). Table 4 lists the results of running weighted A* on the pathfinding problems
used in Table 3.
x 10

6

1+ε = 1.25
1.5

1+ε = 1.25
1+ε = 5

5

1+ε = 5

Suboptimality (%)

Convergence execution cost

4

2

1

0.5

4
3
2
1

0

0

20

40
60
Solution length

80

100

0

0

20

40
60
Solution length

80

100

Figure 9: Impact of heuristic weight as pathfinding problems scale up.
6.4 Extension 3: Backtracking
Both LRTA* and -LRTA* learn by updating their heuristic function while advancing in the state
space. SLA* (Shue & Zamani, 1993) introduced a backtracking mechanism. That is, upon making
an update to heuristic value of the current state, the agent backtracks to its previous state. This
provides the agent with two opportunities: (i) to update the heuristic value of the previous state and
(ii) possibly select a different action in the previous state. An alternative scheme would be to update
130

L EARNING IN R EAL -T IME S EARCH : A U NIFYING F RAMEWORK

Table 3: Effects of scaling the initial heuristic in -LRTA* .
Heuristic weight (1 + )
10.0
5.0
3.3
2.0
1.5
1.1
1.0 (LRTA*)

Execution convergence cost
2832
2002
1908
2271
3639
6520
9346

Suboptimality
3.17%
2.76%
2.20%
1.52%
0.97%
0.25%
0.00%

Table 4: Effects of scaling the initial heuristic in weighted A*.
Heuristic weight
10.0
5.0
3.3
2.0
1.5
1.1
1.0 (A*)

Planning convergence cost
1443
1568
1732
2074
3725
5220
10590

Suboptimality
3.48%
3.08%
2.37%
1.69%
1.16%
0.30%
0.00%

heuristic values of previously visited states in memory (i.e., without physically moving the agent
there). This is the approach exploited, for example, by temporal difference methods with eligibility
traces (Watkins, 1989; Sutton & Barto, 1998). It does not give the agent opportunity (ii) above.
Naturally, all backtracking moves are counted in execution cost.
goal

initial heuristic:

final heuristic:

start

0

1

1

2

3

0

1

2

2

3

0

1

2

3

3

0

1

2

3

4

goal

4 moves

start

initial heuristic:

0

1

1

2

3

final heuristic:

0

1

2

3

4

8 moves

4 moves
4 moves

Figure 10: Heuristic values over successive trials of: LRTA* (left) and SLA* (right).
The underlying intuition can be illustrated with a simple example in Figure 10. Once again,
consider a one-dimensional five-state domain. Each action has an execution cost of one. The initial
heuristic is accurate in the left two states and one lower in the right three states. On each trial,
LRTA* will raise the heuristic value of a single state. Therefore, three trials (12 moves) are needed
to make the heuristic perfect. SLA*, on the other hand, gets to the middle state in 2 moves, updates
its value from 1 to 2, backtracks to the second state from the right, increases its value from 2 to 3,
backtracks to the right most state, and increases its value from 3 to 4. The agent will then take 4
moves towards the goal, following the now perfect heuristic. As a result, the first trial is longer (8
vs. 4 moves) but the overall number of moves until convergence is reduced (from 12 to 8).
The SLA* algorithm is nearly identical to LRTA* with a lookahead of one (Figure 2). The only
difference is that upon increasing the state value in line 6, the agent takes an action leading to its
131

B ULITKO & L EE

previous state and not the state s0 as in line 7 of LRTA*. If the previous state does not exist (i.e., the
algorithm reached the start state) then no action is taken but the heuristic value is still updated.
SLA* backtracks every time its heuristic is updated (i.e., learning took place). This causes very
substantial execution cost of the first trial. In fact, as we will prove in Theorem 7.8, all learning
occurs during the first trial. In larger problems, the user may want a suboptimal solution before
the convergence process completes. To address this problem, SLA*T, introduced in (Shue, Li, &
Zamani, 2001), gives the user control over the amount of learning to be done per trial. Namely,
SLA*T backtracks only after it exhausts a user-specified learning quota.
There are two primary effects of backtracking on the metrics we consider in this paper. First,
larger values of learning quota decrease the execution cost on each trial as less backtracking occurs.
In the extreme, the learning quota can be set to infinity, transforming SLA*T into backtracking-free
LRTA* with a lookahead of one. On the other end of the spectrum (with a learning quota of zero),
SLA*T always backtracks upon updating a heuristic value and becomes equivalent to SLA*. More
backtracking tends to speed up the convergence process (i.e., decrease the convergence execution
cost) as fewer trials are needed to converge (recall the example in Figure 10). Empirically, the
first effect of backtracking is most clearly seen in the 8-puzzle (Table 5) as well as the pathfinding
task with small maps (Table 6). It is still an open question why the trend is not observed on larger
pathfinding tasks (Table 7).
Table 5: Effects of backtracking in the 8-puzzle.
Learning quota
0 (SLA*)
7
13
∞ (LRTA*)

First trial execution cost
1846
424
375
371

Convergence execution cost
2321
52766
57816
58002

Memory required
728
18751
20594
25206

Table 6: Effects of backtracking in the pathfinding domain with small maps.
Learning quota
0 (SLA*)
10
50
1000
∞ (LRTA*)

First trial execution cost
434
413
398
390
235

Convergence execution cost
457
487
592
810
935

The second effect of backtracking is reduction in the amount of memory required to store the
learned heuristic values.7 During backtracking, the agent tends to update heuristic values in previously visited states. Such updates do not require additional memory to be allocated. The memorysaving effect is seen across all problem sizes in Table 5 and Table 7.
6.5 Combining the Extensions
The previous sections introduced three extensions of the base algorithm LRTA*. Table 8 lists six
algorithms that use the extensions. The first entry summarizes the arbitrary-depth LRTA* which
7. Recall that the initial heuristic values are not stored in the table but come from an effectively computable function
such as the Manhattan distance.

132

L EARNING IN R EAL -T IME S EARCH : A U NIFYING F RAMEWORK

Table 7: Effects of backtracking in the pathfinding domain with large maps.
Learning quota
0 (SLA*)
10
100
1000
10000
∞ (LRTA*)

Convergence execution cost
9741
10315
11137
10221
9471
9346

Memory required
174
178
207
258
299
303

Table 8: Real-time heuristic methods compared along several dimensions.
Algorithm
LRTA*
-LRTA*
SLA*
SLA*T
γ-Trap
LRTS

Lookahead
arbitrary
arbitrary
one ply
one ply
arbitrary
arbitrary

Learning rule
mini-min, frontier
mini-min, frontier
mini-min, frontier
mini-min, frontier
max of min, all states
max of min, all states

Execution
one action
one action
one action
one action
d actions
d actions

Optimality
optimal
-optimal
optimal
optimal
γ-optimal
γ-optimal

Backtracking
none
none
always
controlled by T
always
controlled by T

uses the mini-min backup rule to propagate the heuristic values from the search frontier to the interior states in the lookahead tree. It executes one action and converges to an optimal solution. No
backtracking is used. Weighted LRTA* (-LRTA* ) converges to an -optimal solution but is identical to LRTA* in all other columns of the table. SLA* and SLA*T are myopic versions of LRTA*
but use backtracking: unlimited or controlled by the learning quota T respectively. Backtracking
was independently introduced in another algorithm, called γ-Trap (Bulitko, 2004), which also used
a heuristic weight parameter γ = 1/(1 + ) to the same effect as weighted LRTA*. Additionally,
γ-Trap employed a more aggressive heuristic update rule, denoted by “max of min” in the table, and
used heuristic values of all states (as opposed to the frontier of the lookahead search only). Instead
of taking a single action between the lookahead search episodes, it applied d actions to amortize the
planning cost.
We will now combine the three extensions into a single algorithm called Learning Real-Time
Search or LRTS. It is shown in the last row of the table and, in a simplified fashion, in Figure 11.
The detailed pseudo-code necessary to re-implement the algorithm as well as to follow the theorem
proofs is found in Figure 20, Appendix A. We will now step through the operation of LRTS and
explain the control parameters.
In line 1, the trial is initialized by setting the current state s of the agent to the start state sstart and
resetting the amount of learning done on the trial (u). As long as the goal state sgoal is not reached,
the LRTS agent interleaves planning (lines 3 and 4), learning (line 5), and execution (lines 6-11).
During the planning state, LRTS uses the model of its environment (implicitly updated when new
states comes into the agent’s radius of visibility) to generate all child states of the current state up
to d moves away. On each lookahead level (i = 1, 2, . . . , d), LRTS finds the most promising state
(si ) that minimizes the weighted estimate of the total distance:
si = arg min

s0 ∈S(s,i)


γ · g(s0 ) + h(s0 ) ,
133

(6.2)

B ULITKO & L EE

where g(s0 ) is the shortest distance from the current state s to its child s0 and h(s0 ) is the heuristic
estimate of the distance from the child state s0 to the goal state sgoal . Throughout this paper we will
be breaking ties in a systematic fashion as detailed later in the paper. Note that the distance g(s0 )
from the current state s to the child state s0 is weighted by the control parameter γ ∈ (0, 1]. The
same behavior can be obtained by multiplying the initial heuristic by 1+ = 1/γ in -LRTA* except
that it makes the initial heuristic inadmissible (Theorem 7.2).
LRTS(d, γ, T )
1 initialize: s ← sstart , u ← 0
2 while s 6∈ Sg do
3
generate children i moves away, i = 1 . . . d
4
on level i, find the state si with the lowest f = γ · g + h
5
update h(s) to max f (si ) if it is greater
1≤i≤d

6
increase the amount of learning u by |∆h|
7
if u ≤ T then
8
execute d moves to get to the state sd
9
else
10
execute d moves to backtrack to the previous state, set u = T
11 end if
12 end while
Figure 11: LRTS algorithm combines the three extensions of LRTA*.
The lookahead search conducted in line 5 has the complexity of O(bd ) if no child state is reached
more than once and b is the branching factor. On maps, however, the complexity is much reduced
to O(dn ) where n is the dimensionality of the map. Thus, in both the path-planning and the sensor
network routing task, the lookahead has a complexity of O(d2 ).
Like γ-Trap , LRTS uses the so-called “max of min” heuristic update (or learning) rule in line 5.
Specifically, if the initial heuristic is admissible, then the agent will want to increase it as aggressively as possible while keeping it admissible. Lemma 7.1 will prove that setting the heuristic value
h of the current state s to the maximum of minima found on each ply preserves the admissibility of
h at all times.
In line 6 the amount of increase to the heuristic value of the current state (h(s)) is then added to
the cumulative amount of learning done so far on the trial (u). If the new amount of learning does
not exceed the learning quota T then the agent will execute d moves to the most promising frontier
state sd seen during the lookahead (line 8). The moves are added to the path so that they can be
“undone” later on during backtracking. If the new amount of learning exceeds the learning quota T
then the agent will backtrack to its previous state (line 10). By doing so, it will “undo” the last d
actions. The last d actions are thus removed from the path traveled so far on the trial. Note that the
backtracking actions count in execution cost.
We now encourage the reader to revisit the hand-traceable examples found in Sections 6.2,
6.3, 6.4 as they show LRTS operation in simple domains. In the weighting example (Section 6.3), γ
should be set to 1/(1+) to enable LRTS to behave as -LRTA* . This concludes the presentation of
the LRTS algorithm and we will now highlight its properties with theoretical and empirical results.
134

L EARNING IN R EAL -T IME S EARCH : A U NIFYING F RAMEWORK

7. Theoretical Results
The LRTS framework includes LRTA*, weighted LRTA*, SLA*, and γ-Trap as special cases (Table 8). This fact is presented formally in the following theorems. All proofs are found in Appendix B. Note that while the following assumptions are not mandatory for real-time search, they
are needed for the proofs. Specifically, the search space is assumed to be deterministic and stationary. Additionally, the agent’s current state is affected only by the agent’s actions. Finally, the agent
possesses a sufficient knowledge of the environment to conduct a local-search (i.e., lookahead) of
depth d in its current state.
Theorem 7.1 LRTS(d = 1, γ = 1, T = ∞) is equivalent to LRTA* with a lookahead of one.
1
, T = ∞) initialized with an admissible heuristic h0 is equivTheorem 7.2 LRTS(d = 1, γ = 1+
alent to -LRTA* initialized with (1 + )h0 .

Theorem 7.3 LRTS(d = 1, γ = 1, T = 0) is equivalent to SLA*.
Theorem 7.4 LRTS(d, γ, T = 0) is equivalent to γ-Trap(d, γ).
In the past, convergence and completeness have been proven for the special cases (LRTA*, LRTA* , and SLA*). We will now prove these properties for any valid values of the lookahead
d ∈ N, d ≥ 1, the heuristic weight γ ∈ R, γ ∈ (0, 1], and the learning quota T ∈ R ∪ {∞}, T ≥ 0.
Definition 7.1 A tie-breaking scheme is employed when two or more states have equal f = g + h
values. Among candidate states with equal f -values, random tie-breaking selects one at random.
This scheme was used in (Shimbo & Ishida, 2003). Systematic tie-breaking used by Furcy and
Koenig (2000) uses a fixed action ordering per current state. The orderings are, however, generated
randomly for each search problem. Finally, fixed tie-breaking always uses the same action ordering.
We use the latter scheme for all our experiments as it is the simplest.
Lemma 7.1 (Admissibility) For any valid T, γ, d and an admissible initial heuristic h0 , the heuristic function h is maintained admissible at all times during LRTS(d, γ, T ) search.
Theorem 7.5 (Completeness) For any valid T, γ, d and an admissible initial heuristic h0 ,
LRTS(d, γ, T ) arrives at a goal state on every trial.
Theorem 7.6 (Convergence) For any valid T, γ, d and an admissible initial heuristic h0 ,
LRTS(d, γ, T ) with systematic or fixed tie-breaking converges to its final solution after a finite
number of trials. It makes zero updates to its heuristic function on the final trial. Each subsequent
trial will be identical to the final trial.
Theorem 7.7 (Suboptimality) For any valid T, γ, d and an admissible initial heuristic h0 , the ex∗
0)
ecution cost of the final trial of LRTS(d, γ, T ) is upper-bounded by h (s
γ . In other words, the
suboptimality of the final solution is no worse than γ1 − 1.
∗

A non-trivial upper-bound of h (sγ0 )+T can be imposed on the solution produced by LRTS on
the first trial. This requires, however, a minor extension of the LRTS algorithm. Thus, we list the
extension and the theorem in Figure 21, Appendix A and Theorem B.1, Appendix B respectively.
135

B ULITKO & L EE

Theorem 7.8 (One trial convergence) For any valid γ, d and an admissible initial heuristic h0 , the
second trial of LRTS(T = 0) with systematic or fixed tie-breaking is guaranteed to be final. Thus,
all learning and exploration are done on the first trial.

8. Combinations of Parameters
We have introduced a three-parameter algorithm unifying several previously proposed extensions
to the base LRTA*. We have also demonstrated theoretically and empirically the influence of each
parameter independently of the other two parameters in Sections 6.2, 6.3, and 6.4. The summary
of the influences is found in Table 9. An up-arrow/down-arrow means an increase/decrease in the
metric. Notation a → p → b indicates that the parameter p increases from a to b. To illustrate: ↓ at
the intersection of the row labeled “convergence execution cost” and the column labeled “1 → d →
∞” states that the convergence execution cost decreases as d goes up.
Table 9: Summary of influences of LRTS parameters in the pathfinding domain.
metric / parameter
convergence execution cost
planning cost per move
convergence memory
first-move lag
suboptimality of final solution

1→d→∞
↓
↑
↓
↑
↓ for small γ

0→γ→1
↓ for small γ, ↑ for large γ
no influence
↑
no influence
↓

0→T →∞
↑ for small T , ↓ for large T
no influence
↑
no influence
no influence

8.1 Interaction among parameters
In this section we consider effects of parameter combinations. Figure 12 shows the impact of the
lookahead depth d as a function of the heuristic weight γ and the learning quota T . Specifically, for
each value of γ and T , the left plot shows the difference between the convergence execution costs
for lookahead values of d = 1 and d = 10:
impact-of-d(γ, T ) = execution-cost(d = 1, γ, T ) − execution-cost(d = 10, γ, T ).
Reduction in conv. execution cost from d=1 to d=10
1

Reduction in conv. memory from d=1 to d=10

4

x 10
3

1

2.5

Heuristic weight (γ)

0.7
2
0.6
1.5

0.5
0.4

1
0.3
0.2

0.5

Heuristic weight (γ)

0.9
0.8

0.9

160

0.8

140

0.7

120

0.6

100

0.5

80

0.4
60
0.3
40

0.2

20

0.1

0.1
0

5

10

50 100 500 1000 5000 100001e+13
Learning quota (T)

(8.1)

0

5

10

50

100 500 1000 5000 10000 1e+13
Learning quota (T)

Figure 12: Impact of lookahead d for different values of heuristic weight γ and learning quota T .
Brighter shades indicate higher impact of increasing d from 1 to 10. Likewise, the right plot
demonstrates the impact of d on the memory required for convergence. Brighter shades indicate
136

L EARNING IN R EAL -T IME S EARCH : A U NIFYING F RAMEWORK

stronger impact of d (i.e., larger differences). Each point is averaged over the 1000 convergence
runs in the pathfinding domain. We observe that larger values of learning quota and higher values of
heuristic weight magnify the reduction of memory due to increasing lookahead depth. Note that the
plot does not show the memory requirements for different values of γ and T but rather the reduction
in the memory requirements when d increases from 1 to 10.
Figure 13 shows the impact of γ as a function of d and T under the same conditions. Learning
quota does not seem to affect the impact of γ on suboptimality of the final solution. However, deeper
lookahead does make γ less effective in that respect. We believe it is because deeper lookahead
compensates for more inaccuracies in the heuristic function. Therefore, weighting the heuristic
with γ makes less difference.
Reduction in suboptimality from gamma=0.3 to gamma=1.0

Reduction in conv. execution cost from gamma=1.0 to gamma=0.3

10

6000

9

5000

9

8

4000

8

7

3000

6

2000

Lookahead depth (d)

Lookahead depth (d)

10

1000

5

0

4

2.1
2
1.9

7
1.8
6
1.7
5
1.6

4

−1000

1.5

3

3
−2000
2

1.4

2

−3000

1.3

1

1
0

5

10

50

0

100 500 1000 5000 10000 1e+13
Learning quota (T)

5

10

50 100 500 1000 5000 100001e+13
Learning quota (T)

Figure 13: Impact of heuristic weight γ for different values of lookahead d and learning quota T .
Figure 14 shows the impact of T as a function of d and γ on the same 1000 pathfinding problems.
Reducing the learning quota T from 500 to 0 results in an increased amount of backtracking. As
discussed previously, more backtracking tends to save memory and speed up convergence. The
figure shows both trends and demonstrates that d and γ affect the impact of backtracking. The left
plot shows that more backtracking is most effective in speeding up convergence for lower heuristic
weights (γ = 0.1). On the contrary, the right plot indicates that higher heuristic weights (close to
γ = 1) make backtracking most effective in reducing the memory required for convergence.
Reduction in conv. execution cost from T=500 to T=0
10

Reduction in conv. memory from T=500 to T=0

4

x 10
1

10

0.5

7
0

6
5

−0.5

4

Lookahead depth (d)

Lookahead depth (d)

8

50

8
7

40

6
30

5
4

20

3

3
−1

2
1
0.1

60

9

9

0.2

0.3

0.4 0.5 0.6 0.7
Heuristic weight (γ)

0.8

0.9

1

10

2
1
0.1

0.2

0.3

0.4
0.5 0.6 0.7
Heuristic weight (γ)

0.8

0.9

1

Figure 14: Impact of learning quota T for different values of heuristic weight γ and lookahead d.
137

B ULITKO & L EE

Overall, the influences are non-trivial and, according to our experiments, domain-specific.
While some can be explained readily, others appear to be a result of the interaction of the three
parameters with the structure of the problem space. Consequently, selection of optimal parameters
for a specific performance metric and a concrete domain is presently a matter of trial and error.
While manual tuning of control parameters is a typical scenario in Artificial Intelligence, future
research will investigate automatic parameter adjustment.
8.2 Minimizing performance measures
In this section, we list parameter combinations optimizing each of the performance measures and
explain the underlying rationale. The best combinations of parameters are sought in the following
parameter space: lookahead d ∈ {1, 2, . . . , 10}, heuristic weight γ ∈ {0.1, 0.2, . . . , 1.0}, and the
learning quota T ∈ {0, 5, 10, 50, 100, 500, 1000, 5000, 10000, ∞}. For each of the one thousand
combinations of the parameters, we ran LRTS until convergence on the test suite of 1000 pathfinding
problems described in Section 5.1. The results are shown in Table 10 and explained below.
Table 10: LRTS control parameters minimizing performance metrics.
Metric
execution convergence cost
planning convergence cost
planning cost per move
total convergence cost (speed < 185.638)
total convergence cost (speed ≥ 185.638)
suboptimality
memory

Lookahead d
10
1
1
1
10
any
10

Heuristic weight γ
0.3
0.3
any
0.3
0.3
1.0
0.3

Learning quota T
∞
∞
any
∞
∞
any
0

Execution convergence cost is lowest when the quality of individual moves is the highest.
Thus, deeper lookahead leading to more planning per move is favored. High values of γ are undesirable as LRTS seeks near-optimal solutions and takes longer to converge. Very low values of γ
are also undesirable as the weighted initial heuristic becomes less and less informative (Table 3).
Hence, the best value lays somewhere in the middle (0.3). Finally, no backtracking is preferred as
then convergence is the fastest on the larger maps used in this experiment (Table 7).
Planning convergence cost is defined as the total planning cost of all moves until convergence
is achieved. Deeper lookahead leads to faster convergence but quadratically increases the planning
cost of every move (cf., Section 6.5). Hence, overall, shallow lookahead is preferred. This also
explains the optimal parameter combination for minimizing planning cost per move which in LRTS
depends on d only.
Total convergence cost. Here we minimize a weighted sum of the planning and execution
costs. This is a common objective previously studied in (Russell & Wefald, 1991; Koenig, 2004).
We adopt the settings of the latter and bring the two metrics (the execution cost and the planning
cost) to a single scalar expressed in the number of states. Namely, our total convergence cost is
defined as the convergence execution cost multiplied by the planning speed plus the total planning
cost (Section 4). For instance, if the agent converged in 1000 units of execution cost, considered
a total of 5000 states while planning (i.e., an average of five states per unit of execution cost), and
the planning speed is 200 (i.e., 200 states can be considered in planning while one unit of travel is
executed), then the total convergence cost is 1000×200+5000 = 205000, clearly dominated by the
138

L EARNING IN R EAL -T IME S EARCH : A U NIFYING F RAMEWORK

execution component. More planning per move tends to decrease the convergence execution cost
and thus is preferred when the planning speed is high (i.e., planning is cheap relative to execution
and the execution cost dominates the total cost). Conversely, less planning per move is preferred
when the planning speed is low. Both Russell and Wefald (1991) and Koenig (2004) used this
reasoning to select the optimal lookahead depth in their LRTA*-based real-time search algorithms.
In the pathfinding domain, the best values of γ and T for fastest convergence are 0.3 and ∞ as
discussed above. The best value of lookahead depends on the planning speed: deeper lookahead (10
vs. 1) is preferred when the planning speed is sufficiently high and more planning per move can be
afforded.
Suboptimality is zero with γ = 1 regardless of d and T .
Memory. The optimal parameter combination (10, 0.3, 0) is consistent with the previous analysis: more backtracking (T = 0) reduces memory consumption, lower heuristic weight γ leads to
less exploration (since LRTS is satisfied with suboptimal solutions), and since heuristic values are
stored d actions apart, higher lookahead is preferred.
Adjusting LRTS control parameters allows the agent to achieve convergence where it has been
previously impossible due to memory limitations. In the following, we adopt the settings of Shimbo
and Ishida (2003) and Bulitko (2004) and consider one hundred 15-puzzles first published by Korf
(1985). It is known that FALCONS and LRTA* with a lookahead of one are unable to converge to
optimal solutions on any of the 100 puzzles with memory limited to 40 million states (Shimbo &
Ishida, 2003). On the other hand, convergence to suboptimal solutions can be achieved with both
-LRTA* (Shimbo & Ishida, 2003) and γ-Trap (Bulitko, 2004).
Table 11: Convergence on Korf’s one hundred 15-puzzles in four million states of memory.
Algorithm
FALCONS
LRTA*
LRTS
LRTS
LRTS

d
1
2
4

T
∞
0
0

max γ
0.29
0.6
0.7

Suboptimality
no convergence on any problem
no convergence on any problem
50%
10%
4%

We make the task more challenging by reducing the memory limit ten fold from forty to four
million states. While we were unable to find a set of d, T, γ parameters that allowed LRTS to
converge to optimal solutions on all 100 instances, the algorithm did converge to solutions that
were, on average, only 4% worse than optimal on all 100 puzzles (Table 11). Lower values of T
and higher values of d increase the memory cost efficiency and allow to increase γ which leads to
higher quality solutions.
In summary, adjusting the control parameters of LRTS significantly affects its performance. Our
analysis of the individual and combined influence of the parameters on several practically relevant
performance metrics gives a practitioner a better intuition for application-specific parameter tuning.
Perhaps more importantly, it opens an interesting line of research on automatic selection of optimal
parameters on the basis of effectively measurable properties of a domain and the target performance
metric.
139

B ULITKO & L EE

8.3 Application Domain #2: Routing in Sensor Networks
A second application of LRTS considered is routing in ad hoc wireless sensor networks. Sensor
nodes generally have limited computational power and energy, thus simple, energy efficient routing
routines are typically used within sensor networks. Various applications exist, including military
(Shang et al., 2003), environmental monitoring (Braginsky & Estrin, 2002) and reconnaissance
missions (Yan, He, & Stankovic, 2003). As such, routing in sensor networks is of interest to a large
community of users.
The Distance Vector Routing (DVR) algorithm is widely used in network routing (Royer & Toh,
1999). In particular, DVR is employed within the Border Gateway Protocol used to route traffic
over the entire Internet. In DVR, each network node maintains a heuristic estimate of its distance
to the destination/goal node. We observe that DVR is conceptually very similar to LRTA* with a
lookahead of one. Thus, applying LRTS in the network routing domain is essentially equivalent
to extending DVR with the heuristic weight γ and the backtracking mechanism controlled by the
learning quota T . Note that there is no analogue of deeper lookahead in DVR since each network
node does not learn an explicit model of the network and, consequently, is not aware of any network
nodes beyond its immediate neighbors. Hence, d is limited to one.
In order to demonstrate the impact of the two DVR extensions, we chose the setup used in a
recent application of real-time heuristic search to network routing (Shang et al., 2003). Specifically,
the network sensor nodes are assumed to be dropped from the sky to monitor territory, landing in a
random dispersion, and thus necessitating ad hoc routing in order to allow for data transmissions.
We consider the case of a fixed destination for each transmission, simulating each source node
providing information to a central information hub (e.g., a satellite up-link).
Note that LRTA* in the pathfinding domain has access to the heuristic values of its neighbors
at no extra cost since all heuristic values are stored in centralized memory. This is not the case in
ad hoc networks, wherein each node maintains an estimate of the number of hops to the goal node
(the h heuristic). A naive application of LRTA* would query the neighbor nodes in order to retrieve
their h values. This would, however, substantially increase the amount of network traffic, thereby
depleting nodes’ energy sources faster and making the network more easily detectable. We adopted
the standard solution where each node not only stores its own heuristic value but also the last known
heuristic values of its immediate neighbors. Then, whenever a node updates its own heuristic value,
it notifies all of its neighbors of the change by broadcasting a short control message. The last major
difference from pathfinding is that the nodes do not know either their geographical position or the
coordinates of the destination node. Thus, the Euclidian distance is not available and the initial
heuristic is initialized to one for all nodes except the destination node where it is set to zero.
In the experiments reported below, N sensor nodes are randomly distributed on an X × Y
two-dimensional grid so that no two nodes occupy the same location. Each node has a fixed transmission radius which determines the number of neighbors a node can broadcast messages to (and
whose heuristic values it has to store). To specify a single problem instance, distinct start (source)
node and goal (destination or hub) nodes are randomly selected from the N nodes. The start node
needs to transmit a data message to the goal node. In doing so, it can send a message to one of its
neighbors who will then re-transmit the message to another node, etc. Each transmission between
two neighboring nodes incurs an execution cost of one hop. Once the message reaches the destination/goal node, a new trial begins with the same start and goal nodes. Convergence occurs when no
heuristic values are updated during the trial.
140

L EARNING IN R EAL -T IME S EARCH : A U NIFYING F RAMEWORK

The objective is to find a shortest route from the start node to the goal node by learning better
heuristic values over repeated trials. We take measurements of four metrics paralleling those used
in the domain of pathfinding:
convergence execution cost is the total number of hops traveled by messages during convergence;
first trial execution cost is the number of hops traveled on the first trial;
suboptimality of the final solution is the excess of the execution cost of the final trial over the
shortest path possible. For instance, if the network converged to a route of 120 hops while the
shortest route is 100 hops, the suboptimality is 20%;
convergence traffic is defined as the total volume of information transmitted throughout the network during convergence. Each data message counts as one. Short control messages (used to
notify node’s neighbors of an update to the node’s heuristic function) count as one third.
Interference is not modeled in this simulation. We assume scheduling is performed independently
of the routing strategy, and that different schedules will not severely affect the performance of the
routing algorithms. Note that, since transmission delay and timings are not modeled, it would be
difficult to model interference accurately.
Four batches of networks of 50, 200, 400, and 800 nodes each were considered. The nodes
were randomly positioned on a square grid of the dimensions 20 × 20, 30 × 30, 50 × 50, and
80 × 80 respectively. Each batch consisted of 100 randomly generated networks. For each network,
25 convergence runs were performed with the parameter values running γ = {0.1, 0.3, 0.5, 0.7, 1}
and T = {0, 10, 100, 1000, 100000}. In the following, we present the results from the batch of
800-node networks. The same trends were observed in smaller networks as well.
We start out by demonstrating the influence of the two control parameters in isolation. Table 12
shows the influence of the heuristic weight γ with no backtracking. As expected, smaller values
of γ speed up the convergence but increase the suboptimality of the final solution. These results
parallel the ones reported in Table 3 for the pathfinding domain.
Table 12: Effects of heuristic weighting in network routing.
Heuristic weight γ
1.0 (DVR)
0.7
0.5
0.3
0.1

Execution convergence cost
8188
8188
8106
7972
7957

Suboptimality
0%
0%
0%
0.29%
0.33%

Table 13 demonstrates the influence of the learning quota T when the heuristic weight γ is set to
one. Smaller values of T increase the amount of backtracking and speed up convergence cost but
lengthen the first trial. This parallels the results in the 8-puzzle (Table 5) as well as in pathfinding
with small maps (Table 6).
We will now consider the impact of parameter combination. The fixed lookahead depth of one
makes the two-dimensional parameter space easier to visualize on contour plots. Thus, instead of
plotting the impact of a parameter on a metric as a function of the other two control parameters, we
141

B ULITKO & L EE

Table 13: Effects of backtracking in network routing.
Learning quota T
105 (DVR)
103
102
10
0

First trial execution cost
549
1188
4958
5956
6117

Convergence execution cost
8188
8032
6455
6230
6138

First trial execution cost (hops)

Convergence execution cost (hops)
1

1

Heuristic weight (γ)

Heuristic weight (γ)

5000
0.7
4000
0.5

3000
2000

0.3

0.7

7500

0.5

7000

0.3
6500

1000
0.1
0

10

100
1000
Learning quota (T)

0.1

100000

0

10

100
1000
Learning quota (T)

100000

Figure 15: First-trial and convergence execution costs in network routing.
will now plot the metric itself. Figure 15 shows the first trial and convergence execution costs for
all combinations of γ and T . Brighter areas indicates higher costs.
Figure 16 shows the suboptimality of the final solution and the total traffic on a convergence run,
averaged over 100 networks of 800 nodes each. Again, brighter areas indicate higher values.
Suboptimality of the final solution (%)

Convergence traffic

1

4

x 10

1

0.2
0.5

0.15
0.1

0.3

Heuristic weight (γ)

Heuristic weight (γ)

0.25
0.7

5
0.7
4.5

0.5

4

0.3

0.05

3.5

0.1

0.1
0

10

100
1000
Learning quota (T)

0

100000

10

100
1000
Learning quota (T)

100000

Figure 16: Suboptimality of the final solution and the total traffic in network routing.
In summary, by extending the DVR algorithm with the heuristic weight and the backtracking mechanisms of LRTS, reduction of the convergence execution cost and the total network traffic can be
achieved. As a result, near-optimal routes are found faster, at a lower energy consumption. This
promising result raises several questions for further research. First, how will additional routing
constraints, such as the ones investigated in (Shang et al., 2003), affect the performance of LRTS?
Second, how will the benefits of LRTS scale to the case of several messages with the same destina142

L EARNING IN R EAL -T IME S EARCH : A U NIFYING F RAMEWORK

tion passing through the network simultaneously? The latter question also applies to path-planning
with multiple memory-sharing units which is a common situation in squad-based games.

9. Summary and Future Research
In this paper, we considered simultaneous planning and learning problems such as real-time
pathfinding in initially unknown environments as well as routing in ad hoc wireless sensor networks. Learning real-time search methods plan by conducting a limited local search and learn by
maintaining and refining a state-based heuristic function. Thus, they can react quickly to user’s
commands and yet converge to better solutions over repeated experiences.
We analyzed three complementary extensions of the base LRTA* algorithm and showed their
effects with hand-traceable examples, theoretical bounds, and extensive empirical evaluation in two
different domains. We then proposed a simple algorithm, called LRTS, unifying the extensions and
studied the problem of choosing optimal parameter combinations.
Current and future work includes extending LRTS with automated state space abstraction mechanisms, function approximation methods for the heuristic, and automated parameter selection techniques. It will also be of interest to investigate the convergence process of a team of LRTS-based
agents with shared memory and extend LRTS for moving targets and stochastic/dynamic environments in video games, robotics, and networking.

Acknowledgements
Input from Valeriy Bulitko, Sven Koenig, Rich Korf, David Furcy, Nathan Sturtevant, Masashi
Shimbo, Rob Holte, Douglas Aberdeen, Reza Zamani, Stuart Russell, and Maryia Kazakevich is
greatly appreciated. Nathan Sturtevant kindly provided and supported Hierarchical Open Graph
simulator. Additionally, Valeriy Bulitko, David Furcy, Sven Koenig, Rich Korf, Scott Thiessen, and
Ilya Levner have taken time to proof read an early draft of this paper. Great thanks to JAIR reviewers
for working with us on improving the manuscript. We are grateful for the support from NSERC, the
University of Alberta, the Alberta Ingenuity Centre for Machine Learning, and Jonathan Schaeffer.

143

B ULITKO & L EE

Appendix A. Detailed Pseudocode
In this section we list pseudocode of the algorithms at the level of detail necessary to implement
them as well as to follow the proofs listed in the Appendix B.
LRTA*
O UTPUT: a series of actions leading from s0 to a goal state
1
2
3
4
5
6
7

if h is undefined then set h to h0
reset the current state: s ← s0
while s 6∈ Sg do
generate depth 1 neighborhood S(s, 1) = {s0 ∈ S | ks, s0 k = 1}
compute h0 (s) = 0 min (dist(s, s0 ) + h(s0 ))
s ∈S(s,1)

if h0 (s) > h(s) then update h(s) ← h0 (s)
update current state s ← arg 0 min (dist(s, s0 ) + h(s0 ))
s ∈S(s,1)

Figure 17: LRTA* algorithm.
SLA*
O UTPUT: a series of actions leading from s0 to a goal state
1
2
3
4
5
6
7
8
9
10
11
12

if h is undefined then set h to h0
reset the current state: s ← s0
reset the path traveled so far: path ← ∅
while s 6∈ Sg do
generate depth 1 neighborhood S(s, 1) = {s0 ∈ S | ks, s0 k = 1}
compute h0 (s) = 0 min (dist(s, s0 ) + h(s0 ))
s ∈S(s,1)

if h0 (s) > h(s) then
update h(s) ← h0 (s)
update current state s ← pop(path) [or remain in state s if path = ∅]
else
push s onto the stack path
update current state s ← arg 0 min (dist(s, s0 ) + h(s0 ))
s ∈S(s,1)

Figure 18: SLA* algorithm.

144

L EARNING IN R EAL -T IME S EARCH : A U NIFYING F RAMEWORK

γ-Trap(d, γ)
I NPUT:
d: lookahead depth
γ: optimality weight
O UTPUT: path: a series of actions leading from s0 to a goal state
1
2
3
4
5
6
7
8
9
10
11
12
13
14

if h is undefined then set h to h0
reset the current state: s ← s0
reset the stack: path ← ∅
reset the cumulative learning amount u ← 0
while s 6∈ Sg do
set d ← min{d, min{k | S(s, k) = ∅}}
reset set G ← ∅
for k=1 to d do
generate depth k neighborhood S(s, k) = {s0 ∈ S | ks, s0 k = k}
if S(s, k) ∩ Sg 6= ∅ then update G ← G ∪ {k}
compute fmin (s, k) = 0 min (γ · dist(s, s0 ) + h(s0 ))
s ∈S(s,k)

compute smin (s, k) = arg

min

s0 ∈S(s,k)

end for
compute h0 (s) ←


 max fmin (s, k)

if G = ∅,

1≤k≤d



max

1≤k≤min{G}

15
16

(γ · dist(s, s0 ) + h(s0 ))

fmin (s, k)

otherwise.

if h0 (s) ≤ h(s) then
push s onto stack path

17

(
if G = ∅,
smin (s, d)
update the current state s ←
arg min fmin (s, k) otherwise.

18
19
20
21
22

else
update h(s) ← h0 (s)
backtrack: s ← pop(path) [or remain in state s if path = ∅].
end if
end while

k∈G

Figure 19: γ-Trap algorithm.

145

B ULITKO & L EE

LRTS(d, γ, T )
I NPUT:
d0 : lookahead depth
γ: optimality weight
T : learning threshold
O UTPUT: path: a series of actions leading from s0 to a goal state
1
2
3
4
5
6
7
8
9
10
11
12
13
14

if h is undefined then set h to h0
reset the current state: s ← s0
reset the stack: path ← ∅
reset the cumulative learning amount u ← 0
while s 6∈ Sg do
set d ← min{d0 , min{k | S(s, k) = ∅}}
reset set G ← ∅
for k=1 to d do
generate depth k neighborhood S(s, k) = {s0 ∈ S | ks, s0 k = k}
if S(s, k) ∩ Sg 6= ∅ then update G ← G ∪ {k}
compute fmin (s, k) = 0 min (γ · dist(s, s0 ) + h(s0 ))
s ∈S(s,k)

compute smin (s, k) = arg

min

s0 ∈S(s,k)

end for
compute h0 (s) ←


 max fmin (s, k)
max

1≤k≤min{G}

23

if G = ∅,

1≤k≤d


15
16
17
18
19
20
21
22

(γ · dist(s, s0 ) + h(s0 ))

fmin (s, k)

otherwise.

if h0 (s) > h(s) then
compute amount of learning on this move: ` ← h0 (s) − h(s)
update h(s) ← h0 (s)
else
reset ` = 0
end if
if u + ` ≤ T then
push s onto stack path
(
if G = ∅,
smin (s, d)
update the current state: s ←
arg min fmin (s, k) otherwise.
k∈G

24
25
26
27
28

accumulate learning amount u ← u + `
else
backtrack: s ← pop(path) [or remain in state s if path = ∅].
end if
end while

Figure 20: LRTS algorithm.
23a
23b
23c

if ∃s0 [s = s0 & s0 ∈ path] then
remove s0 and all following states from path
end if

Figure 21: Additional lines facilitating incremental pruning in LRTS. These are to be inserted between lines 23 and 24 in Figure 20.
146

L EARNING IN R EAL -T IME S EARCH : A U NIFYING F RAMEWORK

Appendix B. Theorem Proofs
Theorem 7.1 LRTS(d = 1, γ = 1, T = ∞) is equivalent to LRTA* with a lookahead of one.
Proof. We will show that in any state s LRTS(d = 1, γ = 1, T = ∞) (i) takes the same action as
LRTA* does and (ii) updates the h function in the same way LRTA* does.
(i) In state s, LRTS(d = 1, γ = 1, T = ∞) goes to state smin (s, 1) (line 23, Figure 20)
which is equivalent to the state arg min (dist(s, s1 ) + h(s1 )) to which LRTA* will move (line
s1 ∈S(s,1)

7, Figure 17).8
(ii) LRTS(d = 1, γ = 1, T = ∞) updates h(s) to h0 (s) in line 17 in Figure 20 when h0 (s) >
h(s) (line 15) which is equivalent to LRTA*’s update under the same condition in line 6, Figure 17.

1
, T = ∞) initialized with an admissible heuristic h0 is equivaTheorem 7.2 LRTS(d = 1, γ = 1+
lent to -LRTA* initialized with (1 + )h0 .

Proof. These two algorithms begin with and maintain different heuristic functions. Let hLRTS
be the
t
1
heuristic function LRTS(d = 1, γ = 1+
, T = ∞) maintains at iteration t.9 Likewise, let h-LRTA*
t
denote the heuristic function -LRTA* maintains at iteration t. We will also denote by sLRTS
the
t
1
, T = ∞) at iteration t. Likewise, s-LRTA*
is
the
current
current state of LRTS(d = 1, γ = 1+
t
state of -LRTA* at iteration t. We show by induction over iteration number t that for ∀t:
h-LRTA*
= (1 + )hLRTS
.
t
t

(B.1)

1
Base step: since -LRTA* initialized with (1 + )h0 and LRTS(d = 1, γ = 1+
, T = ∞) is
initialized with h0 , equation B.1 trivially holds at t = 0.
Inductive step: suppose equation B.1 holds at iteration t. We will show that it holds at iteration
t + 1.
First, we show that both algorithms are bound to be at the same state st . Suppose not: sLRTS
6=
t
-LRTA*
st
. Then since both start in state s0 there must have been the earliest t0 < t such that
-LRTA* . This means that in state s different actions were taken by
sLRTS
= s-LRTA*
but sLRTS
t0
t0
t0
t0 +1 6= st0 +1
LRTS and -LRTA* . LRTS takes action in line 23 of Figure 20 and would move to state:


arg

min

s0 ∈S(st0 ,1)


1
0
LRTS 0
· dist(st0 , s ) + ht0 (s ) .
1+

(B.2)

-LRTA* moves to the following state (line 7 in Figure 17):
arg

min

s0 ∈S(s

t0 ,1)


dist(st0 , s0 ) + h-LRTA*
(s0 ) .
t0

(B.3)

Since h-LRTA*
= (1 + )hLRTS
(as t0 < t and equation B.1 holds at time t by the inductive
t0
t0
hypothesis), the action in B.2 is the same as in B.3.
8. Throughout the paper we assume that tie-breaking is done in a consistent way among all algorithms.
9. One can measure iterations by counting the number of times the while condition is executed (line 5 in Figure 20 and
line 3 in Figure 17).

147

B ULITKO & L EE

The update to h in LRTS(d = 1, γ =
the expression for h0 (s) we arrive at:
hLRTS
t+1 (st )

1
1+ , T


=
=
=
=

min

s0 ∈S(st ,1)

= ∞) occurs in line 17, Figure 20. Substituting


1
0
LRTS 0
dist(st , s ) + ht
(s )
1+


1
min
dist(st , s0 ) + (1 + )hLRTS
(s0 )
t
1 +  s0 ∈S(st ,1)

1
min
dist(st , s0 ) + h-LRTA*
(s0 )
t
0
1 +  s ∈S(st ,1)
1 -LRTA*
h
(st )
1 +  t+1

(B.4)
(B.5)
(B.6)
(B.7)

which concludes the inductive proof. 
Theorem 7.3 LRTS(d = 1, γ = 1, T = 0) is equivalent to SLA*.
Proof. When T = 0, condition u + ` ≤ T in line 21 of Figure 20 will hold if and only if h(s)
is not updated (i.e., h0 (s) ≤ h(s) in line 15). If that is the case the forward move executed in line
23 is equivalent to SLA*’s forward move in lines 11, 12 of Figure 18. If h(s) is indeed updated
then both LRTS and SLA* will backtrack by executing line 26 in Figure 20 and line 9 in Figure 18
respectively. The proof is concluded with the observation that h0 (s) computed by LRTS(d = 1, γ =
1, T = 0) is equivalent to h0 (s) computed by SLA* in line 6 of Figure 18. 
Theorem 7.4 LRTS(d, γ, T = 0) is equivalent to γ-Trap(d, γ).
Proof. Substituting T = 0 in line 21 of Figure 20, we effectively obtain γ-Trap (Figure 19). 
Lemma 7.1 (Admissibility) For any valid T, γ, d and an admissible initial heuristic h0 , the heuristic
function h is maintained admissible at all times during LRTS(d, γ, T ) search.
Proof. We will prove this lemma by induction over the iteration number t. At t = 0 the statement
trivially holds since the initial heuristic h0 is admissible. Suppose ht is admissible then we will
show that ht+1 is admissible. Heuristic ht+1 can be different from ht only in state st as it is updated
in lines 14 and 17 of Figure 20. Combining the expressions together, we obtain:
ht+1 (st ) = max

min (γ dist(st , s0 ) + ht (s0 )),

1≤k≤d s0 ∈S(st ,k)

(B.8)

if no goal states have been discovered during the lookahead (G = ∅) and:
ht+1 (st ) = min

min (γ dist(st , s0 ) + ht (s0 )),

k∈G s0 ∈S(st ,k)

(B.9)

otherwise. Suppose the max min or min min is reached in state s◦ :
ht+1 (st ) = γ dist(st , s◦ ) + ht (s◦ ), s◦ ∈ S(st , m), 1 ≤ m ≤ d.

(B.10)

The shortest path from state st and the closest goal state intersects neighborhood S(st , m) of depth
m. Let us denote state from S(st , m) belonging to the shortest path by s∗ (Figure 22).
148

L EARNING IN R EAL -T IME S EARCH : A U NIFYING F RAMEWORK

S(st,d)
S(st,m)

st

s*

sº

goal

Figure 22: To the proof of admissibility.
≤

Since ht is admissible in all states, γ
arg min (γ dist(st , s) + ht (s)) we conclude that:

1,

and

s◦

was

chosen

as

s∈S(st ,m)

ht+1 (st ) = γ dist(st , s◦ ) + ht (s◦ )
∗

(B.11)

∗

≤ γ dist(st , s ) + ht (s )

(B.12)

∗

≤ dist(st , s ) + h (s )

(B.13)

∗

∗

∗

= h (st ).

(B.14)

Thus, ht+1 is an admissible heuristic which concludes the induction. 
Theorem 7.5 (Completeness) For any valid T, γ, d and an admissible initial heuristic h0 ,
LRTS(d, γ, T ) arrives at a goal state on every trial.
Proof. Since heuristic h is maintained admissible at all times and each update increases its value by
a positive amount lower-bounded by a constant, only a finite number of updates can be made on any
trial. Let the last update be made in state su−1 , u ≥ 0 in the sequence of states s0 , s1 , . . . traversed
on the trial. We will show that the sequence of states su , su+1 , . . . is guaranteed to terminate in
a goal state. In other words, LRTS cannot loop in the finite state space forever avoiding the goal
states. As there are no updates made in su , su+1 , . . . , it must hold that10 for ∀j ≥ u:
h(sj ) ≥

max

min (γ dist(sj , s0 ) + h(s0 )),

1≤k≤d s0 ∈S(sj ,k)

h(sj ) ≥ γ dist(sj , sj+1 ) + h(sj+1 ),
h(sj ) − h(sj+1 ) ≥ γ dist(sj , sj+1 ).

(B.15)
(B.16)
(B.17)

Since γ > 0 and for any states a 6= b [dist(a, b) > 0], it follows that h(su ) > h(su+1 ) > h(su+2 ) >
. . . . As the state space is finite, this series of states must terminate in a goal state with h(sn ) = 0.
Thus, for any valid parameter values, LRTS ends in a goal state on every trial. 
10. Similar reasoning applies to the case of G 6= ∅ when min min is used instead of max min.

149

B ULITKO & L EE

Theorem 7.6 (Convergence) For any valid T, γ, d and an admissible initial heuristic h0 ,
LRTS(d, γ, T ) with systematic or fixed tie-breaking converges to its final solution after a finite
number of trials. It makes zero updates to its heuristic function on the final trial. Each subsequent
trial will be identical to the final trial.
Proof. From the proof of Theorem 7.5 it is clear that there will be a trial with zero updates to the
heuristic function h. We will call this trial the final trial and will demonstrate that each subsequent
trial will be identical to the final trial. Suppose not. Then in the final trial s0 , s1 , . . . , sn there is the
earliest state sj such that the next trial is different from the final trial in state sj+1 . This implies that
on the subsequent trial LRTS took a different action in state sj from the action it took in the same
state on the final trial. As there are no updates to the heuristic function, the action must have been
selected in line 23 of Figure 20. Expanding the expression, we get:11
sj+1 = smin (sj , d) = arg

min (γ dist(sj , s0 ) + h(s0 )).

s0 ∈S(sj ,d)

(B.18)

Since the ties are broken in a systematic or fixed fashion, the choice of state sj+1 is unique which
contradicts with our assumption on existence of a subsequent trial different from the final trial. 
Theorem 7.7 For any valid T, γ, d and an admissible initial heuristic h0 , the converged solution
∗
0)
cost12 of LRTS(d, γ, T ) is upper-bounded by h (s
γ .
Proof. First, let us denote the converged solution cost (i.e., the execution cost of the final trial) by
\ From theorem 7.6 it follows that there will be a final trial with no updates to the heuristic
solution.
function. Suppose s0 , . . . , sn are the states traversed during the final trial. Since no updates are
made to the heuristic function, the condition h0 (si ) ≤ h(si ) in line 15 of Figure 20 must have held
for all si , i = 0, . . . , n. Substituting the expression for h0 (si ) from line 14, we arrive at:13
max

1≤k≤d

min

s0 ∈S(s

i ,k)


γ dist(si , s0 ) + h(s0 ) ≤ h(si ).

(B.19)


γ dist(si , s0 ) + h(s0 )

(B.20)

Since:
si+1 =

min

s0 ∈S(s

i ,d)

we conclude that:
γ dist(si , si+1 ) + h(si+1 ) ≤ h(si )
γ dist(si , si+1 ) ≤ h(si ) − h(si+1 ).

(B.21)
(B.22)

Summing up these telescoping inequalities for i = 0, . . . , n − 1, we derive:
n−1
X
i=0

dist(si , si+1 ) ≤

h(s0 ) − h(sn )
.
γ

11. The case of G 6= ∅ is handled similarly.
12. Henceforth “converged solution cost” is defined as the execution cost of the final trial.
13. Once again, the case of G 6= ∅ is handled similarly.

150

(B.23)

L EARNING IN R EAL -T IME S EARCH : A U NIFYING F RAMEWORK

Since sn ∈ Sg , h(sn ) = 0. The sum of distances travelled is the travel cost of the final trial and is
the converged solution cost by definition.14 Thus, we conclude that the converged solution cost is
∗
0)
upper bounded by h(sγ 0 ) ≤ h (s
γ . 
Theorem 7.8 For any valid γ, d and an admissible initial heuristic h0 , the second trial of LRTS(T =
0) with systematic or fixed tie-breaking is guaranteed to be final. Thus, all learning and exploration
are done on the first trial.
Proof. Consider a sequence of states [s0 , s1 , . . . , sn ] traversed during the first trial. Here s0 is
the start state and sn is the goal state found. Each transition st → st+1 , t = 0, . . . , n − 1 was
carried out via a forward move (line 23 in Figure 20) or backtracking move (line 26). During move
t = 0, . . . , n − 1, the heuristic function may have been changed (line 17) in state st . We will use
ht to denote the heuristic function before the update in line 17. The update results in ht+1 . Note
that the update changes h in state st only so that ht (s) = ht+1 (s) for all s 6= st . In particular, since
st+1 6= st 15 we have ht+1 (st+1 ) = ht (st+1 ).
Since T = 0, all n moves can be divided in two groups: forward moves that do not change
the h function and backtracking moves which do. We will now prune the sequence of states
[s0 , s1 , . . . , sn ] using a backtracking stack. Namely, at the beginning the stack contains s0 . Every
forward move 0 ≤ t ≤ n−1 will add st+1 to the stack. Every backtracking move 0 ≤ t ≤ n−1 will
remove st+1 from the stack. Carrying this procedure for t = 0, . . . , n − 1, we derive the sequence
of states: p0 = [s0 , . . . , sm ] leading from the initial state s0 to the goal state sm . Clearly, m ≤ n.
Observe that since all backtracking moves were removed from the original move sequence, the
only moves left are forward moves with no updates to h. Thus, let us define b
h-value of any state s
visited on the trial as:
(
h(s)
after the most recent backtracking move from state s;
b
h(s) =
initial h(s) if no backtracking move was taken in s during the trial.
Since now T = 0, for any state si , i = 0, . . . , m there have been no updates in h after the last
backtracking
h move fromi that state (if any). In other words, if h is the final heuristic then ∀i =
0, . . . , m h(si ) = b
h(si ) . It also means that in every k-neighborhood (1 ≤ k ≤ d) of each state
0
si ∈ p there was a state s0 with b
h(s0 ) + γ dist(si , s0 ) ≤ b
h(si ).
We now have to show that no updates to h will be done on the second trial. Suppose this is not
the case. Then there must exist the earliest state sj ∈ p0 , 0 ≤ j < m such that LRTS on the second
trial updated h(sj ) to be greater than b
h(sj ). As we reasoned above, the final value of h(sj ) on the
b
first trial was h(sj ) and there was no need to increase it. By the assumption, LRTS did increase
h(sj ) on the second trial. Consequently, at least one state in one of sj ’s neighborhoods was updated
during first trial (as no updates on the second trial had happened yet).
Formally, h(s0 ), where s0 ∈ S(sj , k) for some 1 ≤ k ≤ d, was increased during the first trial
but after state sj was visited in p0 . Additionally, this means that as a result of this update it must
hold that:
hnew (s0 ) + k > h(sj ).

(B.24)

\ =
14. P
It is easy to check that there the sequence of states on the final trial has no repetitions. Therefore, solution
n−1
dist(s
,
s
).
i i+1
i=0
15. Except for possibly the case of backtracking in st = s0 which does not affect the proof.

151

B ULITKO & L EE

s0

S(sj,k)

p'

sj

sm
s'

Figure 23: To the proof of Theorem 7.8.
Thus, on the first trial, state s0 must have been arrived at via a sequence of forward moves starting
in state sj (shown as the dashed curve in Figure 23). During the forward moves with no h-updates,
h(scurrent ) > h(snext ) holds. This means that there was a sequence of states traversed by forward
moves that started with sj and ended with s0 . Since hnew (s0 ) + k > h(sj ), LRTS must have had to
backtrack from s0 to at least sj . Then it would have updated h(sj ) to hnew (sj ) ≥ hnew (s0 ) + k on
the first trial. Consequently, state s0 cannot be the cause of an update to h(sj ) on the second trial
as inequality B.24 will no longer hold, leading to a contradiction. Thus, no updates to h are carried
out on the second trial. 
Let us define solution(i) as the execution cost of a path p between the start and the goal states
such that: (i) p lies fully in the set of states traversed by the agent on trial i and (ii) p minimizes the
execution cost.
Theorem B.1 For any valid T, γ, d and an admissible initial heuristic h0 , the first trial of
LRTS(d, γ, T ) with systematic or fixed tie-breaking and incremental pruning (see Figure 21) re∗
sults in solution(1) ≤ h (sγ0 )+T .
Proof. We will prove this theorem in three steps.
Notation. Consider a sequence of states [s0 , s1 , . . . , sn ] traversed during the first trial. Here
state s0 is the start state and state si is the current state (s) after i-th iteration line 27 in LRTS
(Figure 20). State sn is the goal state reached at the end of the first trial. Within iteration i (0 ≤ i ≤
n−1), two states are computed as follows. At the beginning of the iteration (line 6), si is the current
state s. During the forward move (line 23), state s is set to si+1 = smin (si , d).16 Let us denote the
heuristic value of state si at the beginning of iteration i as hi (si ). After the possible update in line
17, the new heuristic value is hi+1 (si ). Also, let us denote the amount of learning ` during iteration
i as `i . More precisely:
(
hi+1 (si ) − hi (si ) if h0 (s) > h(s) in line 15,
`i =
(B.25)
0
otherwise.
Under this notation, it is easy to see that the following inequality holds:
γ dist(si , si+1 ) + hi (si+1 ) ≤ hi (si ) + `i
16. The case of seeing a goal state, i.e., G 6= ∅, is dealt with in a similar fashion.

152

(B.26)

L EARNING IN R EAL -T IME S EARCH : A U NIFYING F RAMEWORK

when a forward move (line 23) happens. Note that on iteration i, state si is the only state in
which the heuristic value can be updated. Thus, for ∀sj 6= si [hi+1 (sj ) = hi (sj )]. In particular17 ,
hi+1 (si+1 ) = hi (si+1 ) which allows to transform inequality B.26 into:
γ dist(si , si+1 ) + hi+1 (si+1 ) ≤ hi (si ) + `i .

(B.27)

Forward moves. We will now consider the case of revisiting states during forward moves.
Namely, suppose that in the sequence of states [s0 , s1 , . . . , sn ], state si+k = si and k > 0 is the
smallest k such that this equation holds. We will also assume that no backtracking (line 26) took
place between states si and si+k . The pruning module (lines 23a through 23c in Figure 21) will
purge all states between si and si+k . Therefore, move si+k → si+k+1 will immediately follow
move si−1 → si making the new sequence: si−1 → si (= si+k ) → si+k+1 .
For state si−1 , the inequality B.27 is:
γ dist(si−1 , si ) + hi (si ) ≤ hi−1 (si−1 ) + `i−1 .

(B.28)

For state si+k , the inequality B.27 is:
γ dist(si+k , si+k+1 ) + hi+k+1 (si+k+1 ) ≤ hi+k (si+k ) + `i+k .

(B.29)

Remembering that si = si+k and the only increase of h(si ) was possible when the algorithm left
state si , we conclude that hi (si )+`i = hi+k (si+k ). To be consistent with the pruned move sequence
si−1 → si+k → si+k+1 , we re-write inequality B.28 as:
γ dist(si−1 , si+k ) + hi+k (si+k ) − `i ≤ hi−1 (si−1 ) + `i−1 .

(B.30)

Adding B.29 and B.30 and re-grouping terms, we arrive at:
γ [dist(si−1 , si+k ) + dist(si+k , si+k+1 )]
≤ hi−1 (si−1 ) − hi+k+1 (si+k+1 ) + `i−1 + `i + `i+k .

(B.31)

This means that the weighted sum of distances in the sequence of states [si−1 , si+k , si+k+1 ] can be
upper bounded by the differences in h in the first and the last states plus the sum of learning amounts
for each of the three states. It is easy to show that in the absence of backtracking, inequality B.31
generalizes for the entire pruned sequence [s0 , s1 , . . . , sn ] sequence. Note that pruning done to
the sequence will manifest itself as gaps in the enumeration of the states. This is similar to the
subsequence si−1 , si , si+1 , . . . , si+k , si+k+1 used earlier in the proof becoming si−1 , si+k , si+k+1 .
Let us denote the pruned sequence of indices as I. Then inequality B.31 becomes:
γ

X

dist(si , si+1 ) ≤ h0 (s0 ) − hn (sn ) +

n−1
X

`i .

(B.32)

i=0

i∈I

Noticing that h0 (s0 ) ≤ h∗ (s0 ) and hn (sn ) = 0 as sn is a goal state, we arrive at:
γ

X

dist(si , si+1 ) ≤ h∗ (s0 ) +

n−1
X
i=0

i∈I

17. The special case of si = s0 can be dealt with in a simple fashion.

153

`i .

(B.33)

B ULITKO & L EE

Finally, observing that the total amount of learning is upper bounded by the learning quota T and
that the sum of distances along the pruned state sequence is exactly solution(1), we derive the
desired upper bound:
solution(1) ≤

h∗ (s0 ) + T
.
γ

(B.34)

Backtracking. The finale of the proof lies with showing that backtracking (line 26 in LRTS)
does not affect the derivation in the previous section. Suppose, LRTS traversed states si → si+1 →
si+2 → . . . and then backtracked to state si on iteration i+k (i.e., si+k = si ). In the pruned solution
sequence all states between si−1 and si+k will be removed from path (line 26) creating a gap in the
enumeration: . . . , si−1 , si+k , si+k+1 , . . . . Observing that states si−1 , si = si+k , si+k+1 (i) were not
backtracked from and (ii) were not among the states removed from the path during backtracking,18
we conclude that backtracking did not affect the heuristic value of these three states. Therefore, the
previous derivation still holds which concludes the proof of the theorem. 

References
Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning to act using real-time dynamic programming. Artificial Intelligence, 72(1), 81–138.
BioWare (1999). Baldur’s Gate. http://www.bioware.com/games/baldur_gate.
Blizzard (2002). Warcraft 3: Reign of chaos. http://www.blizzard.com/war3.
Botea, A., Müller, M., & Schaeffer, J. (2004). Near Optimal Hierarchical Path-Finding. Journal of
Game Development, 1(1), 7–28.
Braginsky, D., & Estrin, D. (2002). Rumor routing algorithm for sensor networks. In Proceedings
of the First ACM Workshop on Sensor Networks and Applications, pp. 22–31.
Bulitko, V. (2003). Lookahead pathologies and meta-level control in real-time heuristic search.
In Proceedings of the 15th Euromicro Conference on Real-Time Systems, pp. 13–16, Porto,
Portugal.
Bulitko, V., Li, L., Greiner, R., & Levner, I. (2003). Lookahead pathologies for single agent search.
In Proceedings of International Joint Conference on Artificial Intelligence, pp. 1531–1533,
Acapulco, Mexico.
Bulitko, V. (2004). Learning for adaptive real-time search. Tech. rep. http: // arxiv. org / abs / cs.AI
/ 0407016, Computer Science Research Repository (CoRR).
Buro, M. (1995). Probcut: An effective selective extension of the alpha-beta algorithm. ICCA
Journal, 18, 71–76.
Buro, M. (2002). ORTS: A hack-free RTS game environment. In Proceedings of International
Computers and Games Conference, p. 12, Edmonton, Canada.
Chimura, F., & Tokoro, M. (1994). The Trailblazer search: A new method for searching and capturing moving targets. In Proceedings of the National Conference on Artificial Intelligence, pp.
1347–1352.
18. The fact the three states cannot be among the states removed during backtracking is due to the incremental pruning
mechanism in lines 23a, 23b, and 23c of Figure 21. The pruning mechanism clearly separates updates to the heuristic
function made during forward and backward moves.

154

L EARNING IN R EAL -T IME S EARCH : A U NIFYING F RAMEWORK

Dijkstra, E. W. (1959). A note on two problems in connexion with graphs.. Numerische Mathematik,
1, 269–271.
Ensemble-Studios (1999). Age of empires II: Age of kings. http: // www.microsoft.com / games /
age2.
Furcy, D., & Koenig, S. (2000). Speeding up the convergence of real-time search. In Proceedings
of the National Conference on Artificial Intelligence, pp. 891–897.
Hart, P., Nilsson, N., & Raphael, B. (1968). A formal basis for the heuristic determination of
minimum cost paths. IEEE Transactions on Systems Science and Cybernetics, 4(2), 100–107.
Herbert, M., McLachlan, R., & Chang, P. (1999). Experiments with driving modes for urban robots.
In Proceedings of the SPIE Mobile Robots.
Holte, R., Drummond, C., Perez, M., Zimmer, R., & MacDonald, A. (1994). Searching with abstractions: A unifying framework and new high-performance algorithm. In Proceedings of
the Canadian Artificial Intelligence Conference, pp. 263–270.
Holte, R. (1996). Hierarchical A*: Searching abstraction hierarchies efficiently. In Proceedings of
the National Conference on Artificial Intelligence, pp. 530 – 535.
Hsu, F., Campbell, M., & Hoane, A. (1995). Deep Blue system overview. In Proceedings of the 9th
ACM International Conference on Supercomputing, pp. 240–244.
Ishida, T. (1997). Real-time Search for Learning Autonomous Agents. Kluwer Academic Publishers.
Ishida, T., & Korf, R. (1991). Moving target search. In Proceedings of the International Joint
Conference on Artificial Intelligence, pp. 204–210.
Ishida, T., & Korf, R. (1995). A realtime search for changing goals. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 17(6), 609–619.
Kallmann, M., Bieri, H., & Thalmann, D. (2003). Fully dynamic constrained Delaunay triangulations. In Brunnett, G., Hamann, B., Mueller, H., & Linsen, L. (Eds.), Geometric Modelling
for Scientific Visualization, pp. 241 – 257. Springer-Verlag, Heidelberg, Germany.
Kitano, H., Tadokoro, S., Noda, I., Matsubara, H., Takahashi, T., Shinjou, A., & Shimada, S. (1999).
Robocup rescue: Search and rescue in large-scale disasters as a domain for autonomous agents
research. In Proceedings of the IEEE Conference on Man, Systems, and Cybernetics.
Koenig, S. (1999). Exploring unknown environments with real-time search or reinforcement learning. In Proceedings of the Neural Information Processing Systems, pp. 1003–1009.
Koenig, S., & Likhachev, M. (2002). D* Lite. In Proceedings of the National Conference on
Artificial Intelligence, pp. 476–483.
Koenig, S., Tovey, C., & Smirnov, Y. (2003). Performance bounds for planning in unknown terrain.
Artificial Intelligence, 147, 253–279.
Koenig, S. (2001). Agent-centered search. Artificial Intelligence Magazine, 22(4), 109–132.
Koenig, S. (2004). A comparison of fast search methods for real-time situated agents. In Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent
Systems - Volume 2, pp. 864 – 871.
155

B ULITKO & L EE

Koenig, S., & Simmons, R. (1998). Solving robot navigation problems with initial pose uncertainty
using real-time heuristic search. In Proceedings of the International Conference on Artificial
Intelligence Planning Systems, pp. 144 – 153.
Korf, R. (1985). Depth-first iterative deepening : An optimal admissible tree search. Artificial
Intelligence, 27(3), 97–109.
Korf, R. (1990). Real-time heuristic search. Artificial Intelligence, 42(2-3), 189–211.
Korf, R. (1993). Linear-space best-first search. Artificial Intelligence, 62, 41–78.
Ng, A. Y., Coates, A., Diel, M., Ganapathi, V., Schulte, J., Tse, B., Berger, E., & Liang, E. (2004).
Inverted autonomous helicopter flight via reinforcement learning. In Proceedings of the International Symposium on Experimental Robotics.
Pearl, J. (1984). Heuristics. Addison-Wesley.
Pottinger, D. C. (2000). Terrain analysis in realtime strategy games. In Proceedings of Computer
Game Developers Conference.
Ratner, D., & Warmuth, M. (1986). Finding a shortest solution for the N × N extension of the
15-puzzle is intractable. In Proceedings of the National Conference on Artificial Intelligence,
pp. 168–172.
Reece, D., Krauss, M., & Dumanoir, P. (2000). Tactical movement planning for individual combatants. In Proceedings of the 9th Conference on Computer Generated Forces and Behavioral
Representation.
Royer, E., & Toh, C. (1999). A review of current routing protocols for ad hoc mobile wireless
networks. In IEEE Personal Communications, Vol. 6, pp. 46–55.
Russell, S., & Wefald, E. (1991). Do the right thing: Studies in limited rationality. MIT Press.
Schaeffer, J., Culberson, J., Treloar, N., Knight, B., Lu, P., & Szafron, D. (1992). A world championship caliber checkers program. Artificial Intelligence, 53(2-3), 273–290.
Shang, Y., Fromherz, M. P., Zhang, Y., & Crawford, L. S. (2003). Constraint-based routing for
ad-hoc networks. In Proceedings of the IEEE International Conference on Information Technology: Research and Education, pp. 306–310, Newark, NJ, USA.
Shimbo, M., & Ishida, T. (2003). Controlling the learning process of real-time heuristic search.
Artificial Intelligence, 146(1), 1–41.
Shue, L.-Y., Li, S.-T., & Zamani, R. (2001). An intelligent heuristic algorithm for project scheduling
problems. In Proceedings of the Thirty Second Annual Meeting of the Decision Sciences
Institute, San Francisco.
Shue, L.-Y., & Zamani, R. (1993). An admissible heuristic search algorithm. In Proceedings of the
7th International Symposium on Methodologies for Intelligent Systems, Vol. 689 of LNAI, pp.
69–75. Springer Verlag.
Stenz, A. (1995). The focussed D* algorithm for real-time replanning. In Proceedings of the
International Conference on Artificial Intelligence, pp. 1652–1659.
Sturtevant, N. (2005). Path refinement in A* search. Tech. rep., University of Alberta.
Sutton, R. (2005). The value function hypothesis. http:// rlai.cs.ualberta.ca/ RLAI/ valuefunctionhypothesis. html.
156

L EARNING IN R EAL -T IME S EARCH : A U NIFYING F RAMEWORK

Sutton, R., & Barto, A. (1998). Reinforcement Learning: An Introduction. MIT Press.
Thayer, S., Digney, B., Diaz, M., Stentz, A., Nabbe, B., & Hebert, M. (2000). Distributed robotic
mapping of extreme environments. In Proceedings of the SPIE: Mobile Robots XV and Telemanipulator and Telepresence Technologies VII.
Watkins, C. (1989). Learning from Delayed Rewards. Ph.D. thesis, Kings College, Cambridge
University.
Woodcock, S. (2000). Can AI SDKs help?. Game Developer magazine.
Yan, T., He, T., & Stankovic, J. A. (2003). Differentiated surveillance for sensor networks. In
Proceedings of the 1st international conference on Embedded networked sensor systems, pp.
51–62.
Yu, Y., Govindan, R., & Estrin, D. (2001). Geographical and energy aware routing: A recursive data
dissemination protocol for wireless sensor networks. Tech. rep. UCLA/CSD-TR-01-0023,
UCLA Computer Science Department.

157

Journal of Artificial Intelligence Research 25 (2006) 17-74

Submitted 12/04; published 01/06

Decision-Theoretic Planning with non-Markovian Rewards
Sylvie Thiébaux
Charles Gretton
John Slaney
David Price

Sylvie.Thiebaux@anu.edu.au
Charles.Gretton@anu.edu.au
John.Slaney@anu.edu.au
David.Price@anu.edu.au

National ICT Australia &
The Australian National University
Canberra, ACT 0200, Australia

Froduald Kabanza

kabanza@usherbrooke.ca

Département d’Informatique
Université de Sherbrooke
Sherbrooke, Québec J1K 2R1, Canada

Abstract
A decision process in which rewards depend on history rather than merely on the current state is called a decision process with non-Markovian rewards (NMRDP). In decisiontheoretic planning, where many desirable behaviours are more naturally expressed as properties of execution sequences rather than as properties of states, NMRDPs form a more
natural model than the commonly adopted fully Markovian decision process (MDP) model.
While the more tractable solution methods developed for MDPs do not directly apply in the
presence of non-Markovian rewards, a number of solution methods for NMRDPs have been
proposed in the literature. These all exploit a compact specification of the non-Markovian
reward function in temporal logic, to automatically translate the NMRDP into an equivalent MDP which is solved using efficient MDP solution methods. This paper presents
nmrdpp(Non-Markovian Reward Decision Process Planner), a software platform for the
development and experimentation of methods for decision-theoretic planning with nonMarkovian rewards. The current version of nmrdpp implements, under a single interface,
a family of methods based on existing as well as new approaches which we describe in detail. These include dynamic programming, heuristic search, and structured methods. Using
nmrdpp, we compare the methods and identify certain problem features that affect their
performance. nmrdpp’s treatment of non-Markovian rewards is inspired by the treatment
of domain-specific search control knowledge in the TLPlan planner, which it incorporates
as a special case. In the First International Probabilistic Planning Competition, nmrdpp
was able to compete and perform well in both the domain-independent and hand-coded
tracks, using search control knowledge in the latter.

c
2006
AI Access Foundation. All rights reserved.

Thiébaux, Gretton, Slaney, Price & Kabanza

1. Introduction
1.1 The Problem
Markov decision processes (MDPs) are now widely accepted as the preferred model for
decision-theoretic planning problems (Boutilier, Dean, & Hanks, 1999). The fundamental
assumption behind the MDP formulation is that not only the system dynamics but also the
reward function are Markovian. Therefore, all information needed to determine the reward
at a given state must be encoded in the state itself.
This requirement is not always easy to meet for planning problems, as many desirable
behaviours are naturally expressed as properties of execution sequences (see e.g., Drummond, 1989; Haddawy & Hanks, 1992; Bacchus & Kabanza, 1998; Pistore & Traverso,
2001). Typical cases include rewards for the maintenance of some property, for the periodic
achievement of some goal, for the achievement of a goal within a given number of steps
of the request being made, or even simply for the very first achievement of a goal which
becomes irrelevant afterwards.
For instance, consider a health care robot which assists ederly or disabled people by
achieving simple goals such as reminding them to do important tasks (e.g. taking a pill),
entertaining them, checking or transporting objects for them (e.g. checking the stove’s
temperature or bringing coffee), escorting them, or searching (e.g. for glasses or for the
nurse) (Cesta et al., 2003). In this domain, we might want to reward the robot for making
sure a given patient takes his pill exactly once every 8 hours (and penalise it if it fails
to prevent the patient from doing this more than once within this time frame!), we may
reward it for repeatedly visiting all rooms in the ward in a given order and reporting any
problem it detects, it may also receive a reward once for each patient’s request answered
within the appropriate time-frame, etc. Another example is the elevator control domain
(Koehler & Schuster, 2000), in which an elevator must get passengers from their origin to
their destination as efficiently as possible, while attempting to satisfying a range of other
conditions such as providing priority services to critical customers. In this domain, some
trajectories of the elevator are more desirable than others, which makes it natural to encode
the problem by assigning rewards to those trajectories.
A decision process in which rewards depend on the sequence of states passed through
rather than merely on the current state is called a decision process with non-Markovian
rewards (NMRDP) (Bacchus, Boutilier, & Grove, 1996). A difficulty with NMRDPs is that
the most efficient MDP solution methods do not directly apply to them. The traditional way
to circumvent this problem is to formulate the NMRDP as an equivalent MDP, whose states
result from augmenting those of the original NMRDP with extra information capturing
enough history to make the reward Markovian. Hand crafting such an MDP can however
be very difficult in general. This is exacerbated by the fact that the size of the MDP
impacts the effectiveness of many solution methods. Therefore, there has been interest
in automating the translation into an MDP, starting from a natural specification of nonMarkovian rewards and of the system’s dynamics (Bacchus et al., 1996; Bacchus, Boutilier,
& Grove, 1997). This is the problem we focus on.

18

Decision-Theoretic Planning with non-Markovian Rewards

1.2 Existing Approaches
When solving NMRDPs in this setting, the central issue is to define a non-Markovian reward
specification language and a translation into an MDP adapted to the class of MDP solution
methods and representations we would like to use for the type of problems at hand. More
precisely, there is a tradeoff between the effort spent in the translation, e.g. in producing a
small equivalent MDP without many irrelevant history distinctions, and the effort required
to solve it. Appropriate resolution of this tradeoff depends on the type of representations
and solution methods envisioned for the MDP. For instance, structured representations and
solution methods which have some ability to ignore irrelevant information may cope with a
crude translation, while state-based (flat) representations and methods will require a more
sophisticated translation producing an MDP as small as feasible.
Both the two previous proposals within this line of research rely on past linear temporal
logic (PLTL) formulae to specify the behaviours to be rewarded (Bacchus et al., 1996, 1997).
A nice feature of PLTL is that it yields a straightforward semantics of non-Markovian
rewards, and lends itself to a range of translations from the crudest to the finest. The two
proposals adopt very different translations adapted to two very different types of solution
methods and representations. The first (Bacchus et al., 1996) targets classical state-based
solution methods such as policy iteration (Howard, 1960) which generate complete policies
at the cost of enumerating all states in the entire MDP. Consequently, it adopts an expensive
translation which attempts to produce a minimal MDP. By contrast, the second translation
(Bacchus et al., 1997) is very efficient but crude, and targets structured solution methods
and representations (see e.g., Hoey, St-Aubin, Hu, & Boutilier, 1999; Boutilier, Dearden, &
Goldszmidt, 2000; Feng & Hansen, 2002), which do not require explicit state enumeration.
1.3 A New Approach
The first contribution of this paper is to provide a language and a translation adapted to
another class of solution methods which have proven quite effective in dealing with large
MDPs, namely anytime state-based heuristic search methods such as LAO* (Hansen &
Zilberstein, 2001), LRTDP (Bonet & Geffner, 2003), and ancestors (Barto, Bardtke, &
Singh, 1995; Dean, Kaelbling, Kirman, & Nicholson, 1995; Thiébaux, Hertzberg, Shoaff,
& Schneider, 1995). These methods typically start with a compact representation of the
MDP based on probabilistic planning operators, and search forward from an initial state,
constructing new states by expanding the envelope of the policy as time permits. They may
produce an approximate and even incomplete policy, but explicitly construct and explore
only a fraction of the MDP. Neither of the two previous proposals is well-suited to such
solution methods, the first because the cost of the translation (most of which is performed
prior to the solution phase) annihilates the benefits of anytime algorithms, and the second
because the size of the MDP obtained is an obstacle to the applicability of state-based
methods. Since here both the cost of the translation and the size of the MDP it results
in will severely impact on the quality of the policy obtainable by the deadline, we need an
appropriate resolution of the tradeoff between the two.
Our approach has the following main features. The translation is entirely embedded in
the anytime solution method, to which full control is given as to which parts of the MDP
will be explicitly constructed and explored. While the MDP obtained is not minimal, it
19

Thiébaux, Gretton, Slaney, Price & Kabanza

is of the minimal size achievable without stepping outside of the anytime framework, i.e.,
without enumerating parts of the state space that the solution method would not necessarily
explore. We formalise this relaxed notion of minimality, which we call blind minimality in
reference to the fact that it does not require any lookahead (beyond the fringe). This is
appropriate in the context of anytime state-based solution methods, where we want the
minimal MDP achievable without expensive pre-processing.
When the rewarding behaviours are specified in PLTL, there does not appear to be a
way of achieving a relaxed notion of minimality as powerful as blind minimality without
a prohibitive translation. Therefore instead of PLTL, we adopt a variant of future linear
temporal logic (FLTL) as our specification language, which we extend to handle rewards.
While the language has a more complex semantics than PLTL, it enables a natural translation into a blind-minimal MDP by simple progression of the reward formulae. Moreover,
search control knowledge expressed in FLTL (Bacchus & Kabanza, 2000) fits particularly
nicely in this framework, and can be used to dramatically reduce the fraction of the search
space explored by the solution method.
1.4 A New System
Our second contribution is nmrdpp, the first reported implementation of NMRDP solution
methods. nmrdpp is designed as a software platform for their development and experimentation under a common interface. Given a description of the actions in a domain, nmrdpp
lets the user play with and compare various encoding styles for non-Markovian rewards
and search control knowledge, various translations of the resulting NMRDP into MDP, and
various MDP solution methods. While solving the problem, it can be made to record a
range of statistics about the space and time behaviour of the algorithms. It also supports
the graphical display of the MDPs and policies generated.
While nmrdpp’s primary interest is in the treatment of non-Markovian rewards, it is
also a competitive platform for decision-theoretic planning with purely Markovian rewards.
In the First International Probabilistic Planning Competition, nmrdpp was able to enrol
in both the domain-independent and hand-coded tracks, attempting all problems featuring
in the contest. Thanks to its use of search control-knowledge, it scored a second place
in the hand-coded track which featured probabilistic variants of blocks world and logistics
problems. More surprisingly, it also scored second in the domain-independent subtrack consisting of all problems that were not taken from the blocks world and logistic domains. Most
of these latter problems had not been released to the participants prior to the competition.
1.5 A New Experimental Analysis
Our third contribution is an experimental analysis of the factors that affect the performance
of NMRDP solution methods. Using nmrdpp, we compared their behaviours under the
influence of parameters such as the structure and degree of uncertainty in the dynamics,
the type of rewards and the syntax used to described them, reachability of the conditions
tracked, and relevance of rewards to the optimal policy. We were able to identify a number
of general trends in the behaviours of the methods and provide advice concerning which
are best suited in certain circumstances. Our experiments also lead us to rule out one of

20

Decision-Theoretic Planning with non-Markovian Rewards

the methods as systematically underperforming, and to identify issues with the claim of
minimality made by one of the PLTL approaches.
1.6 Organisation of the Paper
The paper is organised as follows. Section 2 begins with background material on MDPs,
NMRDPs, and existing approaches. Section 3 describes our new approach and Section 4
presents nmrdpp. Sections 5 and 6 report our experimental analysis of the various approaches. Section 7 explains how we used nmrdpp in the competition. Section 8 concludes
with remarks about related and future work. Appendix B gives the proofs of the theorems.
Most of the material presented is compiled from a series of recent conference and workshop
papers (Thiébaux, Kabanza, & Slaney, 2002a, 2002b; Gretton, Price, & Thiébaux, 2003a,
2003b). Details of the logic we use to represent rewards may be found in our 2005 paper
(Slaney, 2005).

2. Background
2.1 MDPs, NMRDPs, Equivalence
We start with some notation and definitions. Given a finite set S of states, we write
S ∗ for the set of finite sequences of states over S, and S ω for the set of possibly infinite
state sequences. Where ‘Γ’ stands for a possibly infinite state sequence in S ω and i is a
natural number, by ‘Γi ’ we mean the state of index i in Γ, by ‘Γ(i)’ we mean the prefix
hΓ0 , . . . , Γi i ∈ S ∗ of Γ. Γ; Γ0 denotes the concatenation of Γ ∈ S ∗ and Γ0 ∈ S ω .
2.1.1 MDPs
A Markov decision process of the type we consider is a 5-tuple hS, s0 , A, Pr, Ri, where S is
a finite set of fully observable states, s0 ∈ S is the initial state, A is a finite set of actions
(A(s) denotes the subset of actions applicable in s ∈ S), {Pr(s, a, •) | s ∈ S, a ∈ A(s)} is a
family of probability distributions over S, such that Pr(s, a, s0 ) is the probability of being
in state s0 after performing action a in state s, and R : S 7→ IR is a reward function such
that R(s) is the immediate reward for being in state s. It is well known that such an MDP
can be compactly represented using dynamic Bayesian networks (Dean & Kanazawa, 1989;
Boutilier et al., 1999) or probabilistic extensions of traditional planning languages (see e.g.,
Kushmerick, Hanks, & Weld, 1995; Thiébaux et al., 1995; Younes & Littman, 2004).
A stationary policy for an MDP is a function π : S 7→ A, such that π(s) ∈ A(s) is
the action to be executed in state s. The value Vπ of the policy at s0 , which we seek to
maximise, is the sum of the expected future rewards over an infinite horizon, discounted by
how far into the future they occur:
Vπ (s0 ) = lim E
n→∞

X
n

i



β R(Γi ) | π, Γ0 = s0

i=0

where 0 ≤ β < 1 is the discount factor controlling the contribution of distant rewards.

21

Thiébaux, Gretton, Slaney, Price & Kabanza

.............................
........
...
....
...
....
.

.....................
...
.... 0 ....
.
....
...................

@
R

s

..................................
....
.......
...
...
..

In the initial state s0 , p is false and two actions are
possible: a causes a transition to s1 with probability
0.1, and no change with probability 0.9, while for b the
transition probabilities are 0.5. In state s1 , p is true,
and actions c and d (“stay” and “go”) lead to s1 and s0
respectively with probability 1.
A reward is received the first time p is true, but not
subsequently. That is, the rewarded state sequences are:
hs0 , s1 i
hs0 , s0 , s1 i
hs0 , s0 , s0 , s1 i
hs0 , s0 , s0 , s0 , s1 i
etc.

	

..
..
...
...
...
..
....
...
.
.
.
.
.
.......
.
.
.
.
.
.
.
.
.
.
..................................
................................
...
...
...
...
...
...
....
.
..
..
..
..
...
..
...
.
..
...
.
....
..
.... .................. .......
....... ...
... .......
.....
.
... 1 .....
.
....
...................

a

0.9

b
6@

d 1.0

0.1

0.5

0.5

~s =
I.....
@
...
..
c

.....
...
....
............

...
..

1.0 ..................

Figure 1: A Simple NMRDP
2.1.2 NMRDPs
A decision process with non-Markovian rewards is identical to an MDP except that the
domain of the reward function is S ∗ . The idea is that if the process has passed through
state sequence Γ(i) up to stage i, then the reward R(Γ(i)) is received at stage i. Figure 1
gives an example. Like the reward function, a policy for an NMRDP depends on history,
and is a mapping from S ∗ to A. As before, the value of policy π is the expectation of the
discounted cumulative reward over an infinite horizon:
X

n
i
Vπ (s0 ) = lim E
β R(Γ(i)) | π, Γ0 = s0
n→∞

i=0

e
For a decision process D = hS, s0 , A, Pr, Ri and a state s ∈ S, we let D(s)
stand for
the set of state sequences rooted at s that are feasible under the actions in D, that is:
e
D(s)
= {Γ ∈ S ω | Γ0 = s and ∀i ∃a ∈ A(Γi ) Pr(Γi , a, Γi+1 ) > 0}. Note that the definition
e
of D(s) does not depend on R and therefore applies to both MDPs and NMRDPs.
2.1.3 Equivalence
The clever algorithms developed to solve MDPs cannot be directly applied to NMRDPs.
One way of dealing with this problem is to translate the NMRDP into an equivalent MDP
with an expanded state space (Bacchus et al., 1996). The expanded states in this MDP
(e-states, for short) augment the states of the NMRDP by encoding additional information
sufficient to make the reward history-independent. For instance, if we only want to reward
the very first achievement of goal g in an NMRDP, the states of an equivalent MDP would
carry one extra bit of information recording whether g has already been true. An e-state can
be seen as labelled by a state of the NMRDP (via the function τ in Definition 1 below) and
by history information. The dynamics of NMRDPs being Markovian, the actions and their
probabilistic effects in the MDP are exactly those of the NMRDP. The following definition,
adapted from that given by Bacchus et al. (1996), makes this concept of equivalent MDP
precise. Figure 2 gives an example.

22

Decision-Theoretic Planning with non-Markovian Rewards


..............................
.......
..
....
...
....
.



.................................
....
.......
...
...
..

...................
....
.
..... 0 ....
.... 2 ....
..
..
................
...
..
...
...
....
...
.
.
.
.
.
.......
.
.
.
.
.
.
.
.
.
.
.
.
.
..............................
................................
.
...
...
...
...
...
..
....
.
..
..
..
..
...
.
..
...
.
.
...
.
.
....
..
.... ................. .......
.
....... ...
..... 0 ..............
...
.
.
.
.... 3 ...
...............

@
R ?	
s

a

0.9

0.1

b
6@

d 1.0

~s =

c....
I
@
....
.
.
.....
...
....
...........

1.0

0.5

..... ................
...........
......
...
...
...
....
.
.
..
.............................. ....
.
.
.
.
.
..
.
.
.
.
.
.
..........
..
.....
.
.
.
.
.
.
.
.
.
.
.
.
...
.
...
.
.
.
.
...
....................
....................
...
...
.
.
..... 0 ....
..... 0 ....
.... 1 ....
.... 0 ....
................
...............
...
....
......
.
....
......
........
...
............... ..........................
...
..........
.
..
...
...
..
..
...
.
.
.
....
.
.
.
.
.......
............. ...................

0.5

0.5

0.5...........

d

b

/

c

@

s

a

o

...
..
..
....
............

0.1

s

	

@
I

0.9

Figure 2: An MDP Equivalent to the NMRDP in Figure 1. τ (s00 ) = τ (s02 ) = s0 . τ (s01 ) =
τ (s03 ) = s1 . The initial state is s00 . State s01 is rewarded; the other states are not.

Definition 1 MDP D0=hS 0 , s00 , A0 , Pr0, R0 i is equivalent to NMRDP D = hS, s0 , A, Pr, Ri if
there exists a mapping τ : S 0 7→ S such that:1
1. τ (s00 ) = s0 .
2. For all s0 ∈ S 0 , A0 (s0 ) = A(τ (s0 )).
3. For all s1 , s2 ∈ S, if there is a ∈ A(s1 ) such that Pr(s1 , a, s2 ) > 0, then for all s01 ∈ S 0
such that τ (s01 ) = s1 , there exists a unique s02 ∈ S 0 , τ (s02 ) = s2 , such that for all
a0 ∈ A0 (s01 ), Pr0 (s01 , a0 , s02 ) = Pr(s1 , a0 , s2 ).
e 0 ) and Γ0 ∈ D
e 0 (s0 ) such that τ (Γ0 ) = Γi for
4. For any feasible state sequence Γ ∈ D(s
0
i
all i, we have: R0 (Γ0i ) = R(Γ(i)) for all i.
Items 1–3 ensure that there is a bijection between feasible state sequences in the NMRDP
and feasible e-state sequences in the MDP. Therefore, a stationary policy for the MDP can be
reinterpreted as a non-stationary policy for the NMRDP. Furthermore, item 4 ensures that
the two policies have identical values, and that consequently, solving an NMRDP optimally
reduces to producing an equivalent MDP and solving it optimally (Bacchus et al., 1996):
Proposition 1 Let D be an NMRDP, D0 an equivalent MDP for it, and π 0 a policy for
e 0 ) by π(Γ(i)) = π 0 (Γ0 ),
D0 . Let π be the function defined on the sequence prefixes Γ(i) ∈ D(s
i
0
where for all j ≤ i τ (Γj ) = Γj . Then π is a policy for D such that Vπ (s0 ) = Vπ0 (s00 ).
1. Technically, the definition allows the sets of actions A and A0 to be different, but any action in which
they differ must be inapplicable in reachable states in the NMRDP and in all e-states in the equivalent
MDP. For all practical purposes, A and A0 can be seen as identical.

23

Thiébaux, Gretton, Slaney, Price & Kabanza

2.2 Existing Approaches
Both existing approaches to NMRDPs (Bacchus et al., 1996, 1997) use a temporal logic of
the past (PLTL) to compactly represent non-Markovian rewards and exploit this compact
representation to translate the NMRDP into an MDP amenable to off-the-shelf solution
methods. However, they target different classes of MDP representations and solution methods, and consequently, adopt different styles of translations.
Bacchus et al. (1996) target state-based MDP representations. The equivalent MDP
is first generated entirely – this involves the enumeration of all e-states and all transitions
between them. Then, it is solved using traditional dynamic programming methods such as
value or policy iteration. Because these methods are extremely sensitive to the number of
states, attention is paid to producing a minimal equivalent MDP (with the least number of
states). A first simple translation which we call pltlsim produces a large MDP which can
be post-processed for minimisation before being solved. Another, which we call pltlmin,
directly results in a minimal MDP, but relies on an expensive pre-processing phase.
The second approach (Bacchus et al., 1997), which we call pltlstr, targets structured
MDP representations: the transition model, policies, reward and value functions are represented in a compact form, e.g. as trees or algebraic decision diagrams (ADDs) (Hoey et al.,
1999; Boutilier et al., 2000). For instance, the probability of a given proposition (state
variable) being true after the execution of an action is specified by a tree whose internal
nodes are labelled with the state variables on whose previous values the given variable depends, whose arcs are labelled by the possible previous values (> or ⊥) of these variables,
and whose leaves are labelled with probabilities. The translation amounts to augmenting
the structured MDP with new temporal variables tracking the relevant properties of state
sequences, together with the compact representation of (1) their dynamics, e.g. as trees over
the previous values of relevant variables, and (2) of the non-Markovian reward function in
terms of the variables’ current values. Then, structured solution methods such as structured
policy iteration or the SPUDD algorithm are run on the resulting structured MDP. Neither
the translation nor the solution methods explicitly enumerates the states.
We now review these approaches in some detail. The reader is referred to the respective
papers for additional information.
2.2.1 Representing Rewards with PLTL
The syntax of PLTL, the language chosen to represent rewarding behaviours, is that of
propositional logic, augmented with the operators  (previously) and S (since) (see Emerson, 1990). Whereas a classical propositional logic formula denotes a set of states (a subset
of S), a PLTL formula denotes a set of finite sequences of states (a subset of S ∗ ). A formula
without temporal modality expresses a property that must be true of the current state, i.e.,
the last state of the finite sequence. f specifies that f holds in the previous state (the
state one before the last). f1 S f2 , requires f2 to have been true at some point in the sequence, and, unless that point is the present, f1 to have held ever since. More formally, the
modelling relation |= stating whether a formula f holds of a finite sequence Γ(i) is defined
recursively as follows:
• Γ(i) |= p iff p ∈ Γi , for p ∈ P, the set of atomic propositions

24

Decision-Theoretic Planning with non-Markovian Rewards

• Γ(i) |= ¬f iff Γ(i) 6|= f
• Γ(i) |= f1 ∧ f2 iff Γ(i) |= f1 and Γ(i) |= f2
• Γ(i) |= f iff i > 0 and Γ(i − 1) |= f
• Γ(i) |= f1 S f2 iff ∃j ≤ i, Γ(j) |= f2 and ∀k, j < k ≤ i, Γ(k) |= f1
- f ≡ > S f meaning that f has been true at
From S, one can define the useful operators ♦
- ¬f meaning that f has always been true. E.g, g ∧ ¬  ♦
- g denotes
some point, and f ≡ ¬♦
the set of finite sequences ending in a state where g is true for the first time in the sequence.
- k f for
Other useful abbreviation are k (k times ago), for k iterations of the  modality, ♦
∨ki=1 i f (f was true at some of the k last steps), and k f for ∧ki=1 i f (f was true at all
the k last steps).
Non-Markovian reward functions are described with a set of pairs (fi : ri ) where fi is
a PLTL reward formula and ri is a real, with the semantics that the reward assigned to a
sequence in S ∗ is the sum of the ri ’s for which that sequence is a model of fi . Below, we let
F denote the set of reward formulae fi in the description of the reward function. Bacchus
et al. (1996) give a list of behaviours which it might be useful to reward, together with
their expression in PLTL. For instance, where f is an atemporal formula, (f : r) rewards
with r units the achievement of f whenever it happens. This is a Markovian reward. In
- f : r) rewards every state following (and including) the achievement of f , while
contrast (♦
- f : r) only rewards the first occurrence of f . (f ∧k ¬f : r) rewards the occurrence
(f ∧¬ ♦
of f at most once every k steps. (n ¬  ⊥ : r) rewards the nth state, independently of
its properties. (2 f1 ∧ f2 ∧ f3 : r) rewards the occurrence of f1 immediately followed by
f2 and then f3 . In reactive planning, so-called response formulae which describe that the
achievement of f is triggered by a condition (or command) c are particularly useful. These
- c : r) if every state in which f is true following the first issue of
can be written as (f ∧ ♦
the command is to be rewarded. Alternatively, they can be written as (f ∧ (¬f S c) : r) if
only the first occurrence of f is to be rewarded after each command. It is common to only
- k c : r)
reward the achievement f within k steps of the trigger; we write for example (f ∧ ♦
to reward all such states in which f holds.
From a theoretical point of view, it is known (Lichtenstein, Pnueli, & Zuck, 1985) that
the behaviours representable in PLTL are exactly those corresponding to star-free regular
languages. Non star-free behaviours such as (pp)∗ (reward an even number of states all
containing p) are therefore not representable. Nor, of course, are non-regular behaviours
such as pn q n (e.g. reward taking equal numbers of steps to the left and right). We shall not
speculate here on how severe a restriction this is for the purposes of planning.
2.2.2 Principles Behind the Translations
All three translations into an MDP (pltlsim, pltlmin, and pltlstr) rely on the equivalence f1 S f2 ≡ f2 ∨ (f1 ∧ (f1 S f2 )), with which we can decompose temporal modalities
into a requirement about the last state Γi of a sequence Γ(i), and a requirement about the
prefix Γ(i − 1) of the sequence. More precisely, given state s and a formula f , one can com-

25

Thiébaux, Gretton, Slaney, Price & Kabanza

pute in2 O(||f ||) a new formula Reg(f, s) called the regression of f through s. Regression
has the property that, for i > 0, f is true of a finite sequence Γ(i) ending with Γi = s iff
Reg(f, s) is true of the prefix Γ(i − 1). That is, Reg(f, s) represents what must have been
true previously for f to be true now. Reg is defined as follows:
• Reg(p, s) = > iff p ∈ s and ⊥ otherwise, for p ∈ P
• Reg(¬f, s) = ¬Reg(f, s)
• Reg(f1 ∧ f2 , s) = Reg(f1 , s) ∧ Reg(f2 , s)
• Reg(f, s) = f
• Reg(f1 S f2 , s) = Reg(f2 , s) ∨ (Reg(f1 , s) ∧ (f1 S f2 ))
For instance, take a state s in which p holds and q does not, and take f = (¬q) ∧ (p S q),
meaning that q must have been false 1 step ago, but that it must have held at some point
in the past and that p must have held since q last did. Reg(f, s) = ¬q ∧ (p S q), that is,
for f to hold now, then at the previous stage, q had to be false and the p S q requirement
still had to hold. When p and q are both false in s, then Reg(f, s) = ⊥, indicating that f
cannot be satisfied, regardless of what came earlier in the sequence.
For notational convenience, where X is a set of formulae we write X for X∪{¬x | x ∈ X}.
Now the translations exploit the PLTL representation of rewards as follows. Each expanded
state (e-state) in the generated MDP can be seen as labelled with a set Ψ ⊆ Sub(F ) of
subformulae of the reward formulae in F (and their negations). The subformulae in Ψ must
be (1) true of the paths leading to the e-state, and (2) sufficient to determine the current
truth of all reward formulae in F , as this is needed to compute the current reward. Ideally
the Ψs should also be (3) small enough to enable just that, i.e. they should not contain
subformulae which draw history distinctions that are irrelevant to determining the reward
at one point or another. Note however that in the worst-case, the number of distinctions
needed, even in the minimal equivalent MDP, may be exponential in ||F ||. This happens for
instance with the formula k f , which requires k additional bits of information memorising
the truth of f over the last k steps.
2.2.3 pltlsim
For the choice of the Ψs, Bacchus et al. (1996) consider two cases. In the simple case, which
we call pltlsim, an MDP obeying properties (1) and (2) is produced by simply labelling
each e-state with the set of all subformulae in Sub(F ) which are true of the sequence leading
to that e-state. This MDP is generated forward, starting from the initial e-state labelled
with s0 and with the set Ψ0 ⊆ Sub(F ) of all subformulae which are true of the sequence
hs0 i. The successors of any e-state labelled by NMRDP state s and subformula set Ψ are
generated as follows: each of them is labelled by a successor s0 of s in the NMRDP and by
the set of subformulae {ψ 0 ∈ Sub(F ) | Ψ |= Reg(ψ 0 , s0 )}.
For instance, consider the NMRDP shown in Figure 3. The set F = {q∧p} consists of
a single reward formula. The set Sub(F ) consists of all subformulae of this reward formula,
2. The size ||f || of a reward formula is measured as its length and the size ||F || of a set of reward formulae
F is measured as the sum of the lengths of the formulae in F .

26

Decision-Theoretic Planning with non-Markovian Rewards

start_state
a(0.16)
p

a(1) b(1)

a(0.04) b(0.2)
a(0.16)

a(0.64)

b(0.8)

q

a(0.2) b(1)

a(0.8)
p, q

a(1) b(1)

In the initial state, both p and q are false.
When p is false, action a independently sets
p and q to true with probability 0.8. When
both p and q are false, action b sets q to true
with probability 0.8. Both actions have
no effect otherwise. A reward is obtained
whenever q ∧   p. The optimal policy
is to apply b until q gets produced, making
sure to avoid the state on the left-hand side,
then to apply a until p gets produced, and
then to apply a or b indifferently forever.

Figure 3: Another Simple NMRDP
and their negations, that is Sub(F ) = {p, q, p,   p, q ∧   p, ¬p, ¬q, ¬  p, ¬  p, ¬(q ∧
  p)}. The equivalent MDP produced by pltlsim is shown in Figure 4.
2.2.4 pltlmin
Unfortunately, the MDPs produced by pltlsim are far from minimal. Although they could
be postprocessed for minimisation before invoking the MDP solution method, the above
expansion may still constitute a serious bottleneck. Therefore, Bacchus et al. (1996) consider
a more complex two-phase translation, which we call pltlmin, capable of producing an
MDP also satisfying property (3). Here, a preprocessing phase iterates over all states in
S, and computes, for each state s, a set l(s) of subformulae, where the function l is the
solution of the fixpoint equation l(s) = F ∪ {Reg(ψ 0 , s0 ) | ψ 0 ∈ l(s0 ), s0 is a successor of s}.
Only subformulae in l(s) will be candidates for inclusion in the sets labelling the respective
e-states labelled with s. That is, the subsequent expansion phase will be as above, but taking
Ψ0 ⊆ l(s0 ) and ψ 0 ⊆ l(s0 ) instead of Ψ0 ⊆ Sub(F ) and ψ 0 ⊆ Sub(F ). As the subformulae in
l(s) are exactly those that are relevant to the way feasible execution sequences starting from
e-states labelled with s are rewarded, this leads the expansion phase to produce a minimal
equivalent MDP.
Figure 5 shows the equivalent MDP produced by pltlmin for the NMRDP example in
Figure 3, together with the function l from which the labels are built. Observe how this
MDP is smaller than the pltlsim MDP: once we reach the state on the left-hand side in
which p is true and q is false, there is no point in tracking the values of subformulae, because
q cannot become true and so the reward formula cannot either. This is reflected by the fact
that l({p}) only contains the reward formula.
In the worst case, computing l requires a space, and a number of iterations through S,
exponential in ||F ||. Hence the question arises of whether the gain during the expansion
phase is worth the extra complexity of the preprocessing phase. This is one of the questions
our experimental analysis in Section 5 will try to answer.
2.2.5 pltlstr
The pltlstr translation can be seen as a symbolic version of pltlsim. The set T of
added temporal variables contains the purely temporal subformulae PTSub(F ) of the reward
formulae in F , to which the  modality is prepended (unless already there): T = {ψ | ψ ∈

27

Thiébaux, Gretton, Slaney, Price & Kabanza

start_state
f6,f7,f8,f9,f10
Reward=0
a(0.16)

a(0.04) b(0.2)

a(0.16)

p
f1,f7,f8,f9,f10
Reward=0

a(0.64)

q
f2,f6,f8,f9,f10
Reward=0

a(1) b(1)

p
f1,f3,f4,f7,f10
Reward=0

The following subformulae in Sub(F ) label
the e-states:
f1 : p
f2 : q
f3 : p
f4 :   p
f5 : q ∧   p
f6 : ¬p
f7 : ¬q
f8 : ¬  p
f9 : ¬  p
f10 : ¬(q ∧   p)

a(0.2) b(1)

a(0.8)

p
f1,f3,f7,f9,f10
Reward=0
a(1)

b(0.8)

p, q
f1,f2,f8,f9,f10
Reward=0
b(1)

a(1)

a(1) b(1)

b(1)

p, q
f1,f2,f3,f9,f10
Reward=0
a(1) b(1)
p, q
f1,f2,f3,f4,f5
Reward=1

a(1) b(1)

Figure 4: Equivalent MDP Produced by pltlsim

start_state
f4,f5,f6
Reward=0
a(0.16)
p
f4
Reward=0

a(1) b(1)

a(0.04) b(0.2)

a(0.16)

b(0.8)

q
f4,f5,f6
Reward=0

a(0.64)

The function l is given by:
l({}) = {q ∧   p, p, p}
l({p}) = {q ∧   p}
l({q}) = {q ∧   p, p, p}
l({p, q}) = {q ∧   p, p, p}

a(0.2) b(1)

a(0.8)

The following formulae label the e-states:
f1 : q ∧   p
f2 : p
f3 : p
f4 : ¬(q ∧   p)
f5 : ¬  p
f6 : ¬p

p, q
f3,f4,f5
Reward=0
a(1)

b(1)

p, q
f2,f3,f4
Reward=0
a(1)

b(1)

p, q
f1,f2,f3
Reward=1

a(1) b(1)

Figure 5: Equivalent MDP Produced by pltlmin

28

Decision-Theoretic Planning with non-Markovian Rewards

q

p

1.00

prv prv p

prv p

0.00

1. dynamics of p

1.00

0.00

2. dynamics of   p

0.00

1.00

3. reward

Figure 6: ADDs Produced by pltlstr. prv (previously) stands for 
PTSub(F ), ψ 6= ψ 0 } ∪ {ψ | ψ ∈ PTSub(F )}. By repeatedly applying the equivalence
f1 S f2 ≡ f2 ∨ (f1 ∧ (f1 S f2 )) to any subformula in PTSub(F ), we can express its current
value, and hence that of reward formulae, as a function of the current values of formulae
in T and state variables, as required by the compact representation of the transition and
reward models.
For our NMRDP example in Figure 3, the set of purely temporal variables is PTSub(F ) =
{p,   p}, and T is identical to PTSub(F ). Figure 6 shows some of the ADDs forming
part of the symbolic MDP produced by pltlstr: the ADDs describing the dynamics of
the temporal variables, i.e., the ADDs describing the effects of the actions a and b on their
respective values, and the ADD describing the reward.
As a more complex illustration, consider this example (Bacchus et al., 1997) in which
- (p S (q ∨ r))} ≡ {> S (p S (q ∨ r))}
F = {♦
We have that
PTSub(F ) = {> S (p S (q ∨ r)), p S (q ∨ r), r}
and so the set of temporal variables used is
T = {t1 : (> S (p S (q ∨ r))), t2 : (p S (q ∨ r)), t3 : r}
Using the equivalences, the reward can be decomposed and expressed by means of the
propositions p, q and the temporal variables t1 , t2 , t3 as follows:
> S (p S (q ∨ r))
≡
(p S (q ∨ r)) ∨ (> S (p S (q ∨ r))) ≡
(q ∨ r) ∨ (p ∧ (p S (q ∨ r))) ∨ t1
≡
(q ∨ t3 ) ∨ (p ∧ t2 ) ∨ t1
As with pltlsim, the underlying MDP produced by pltlstr is far from minimal – the
encoded history features do not even vary from one state to the next. However, size is
not as problematic as with state-based approaches, because structured solution methods do
not enumerate states and are able to dynamically ignore some of the variables that become
irrelevant during policy construction. For instance, when solving the MDP, they may be
29

Thiébaux, Gretton, Slaney, Price & Kabanza

able to determine that some temporal variables have become irrelevant because the situation
they track, although possible in principle, is too costly to be realised under a good policy.
This dynamic analysis of rewards contrast with pltlmin’s static analysis (Bacchus et al.,
1996) which must encode enough history to determine the reward at all reachable future
states under any policy.
One question that arises is that of the circumstances under which this analysis of irrelevance by structured solution methods, especially the dynamic aspects, is really effective.
This is another question our experimental analysis will try to address.

3. fltl: A Forward-Looking Approach
As noted in Section 1 above, the two key issues facing approaches to NMRDPs are how
to specify the reward functions compactly and how to exploit this compact representation
to automatically translate an NMRDP into an equivalent MDP amenable to the chosen
solution method. Accordingly, our goals are to provide a reward function specification
language and a translation that are adapted to anytime state-based solution methods. After
a brief reminder of the relevant features of these methods, we consider these two goals
in turn. We describe the syntax and semantics of the language, the notion of formula
progression for the language which will form the basis of our translation, the translation
itself, its properties, and its embedding into the solution method. We call our approach
fltl. We finish the section with a discussion of the features that distinguish fltl from
existing approaches.
3.1 Anytime State-Based Solution Methods
The main drawback of traditional dynamic programming algorithms such as policy iteration
(Howard, 1960) is that they explicitly enumerate all states that are reachable from s0 in
the entire MDP. There has been interest in other state-based solution methods, which may
produce incomplete policies, but only enumerate a fraction of the states that policy iteration
requires.
Let E(π) denote the envelope of policy π, that is the set of states that are reachable
(with a non-zero probability) from the initial state s0 under the policy. If π is defined
at all s ∈ E(π), we say that the policy is complete, and that it is incomplete otherwise.
The set of states in E(π) at which π is undefined is called the fringe of the policy. The
fringe states are taken to be absorbing, and their value is heuristic. A common feature of
anytime state-based algorithms is that they perform a forward search, starting from s0 and
repeatedly expanding the envelope of the current policy one step forward by adding one or
more fringe states. When provided with admissible heuristic values for the fringe states,
they eventually converge to the optimal policy without necessarily needing to explore the
entire state space. In fact, since planning operators are used to compactly represent the
state space, they may not even need to construct more than a small subset of the MDP
before returning the optimal policy. When interrupted before convergence, they return a
possibly incomplete but often useful policy.
These methods include the envelope expansion algorithm (Dean et al., 1995), which
deploys policy iteration on judiciously chosen larger and larger envelopes, using each successive policy to seed the calculation of the next. The more recent LAO∗ algorithm (Hansen
30

Decision-Theoretic Planning with non-Markovian Rewards

& Zilberstein, 2001) which combines dynamic programming with heuristic search can be
viewed as a clever implementation of a particular case of the envelope expansion algorithm,
where fringe states are given admissible heuristic values, where policy iteration is run up to
convergence between envelope expansions, and where the clever implementation only runs
policy iteration on the states whose optimal value can actually be affected when a new fringe
state is added to the envelope. Another example is a backtracking forward search in the
space of (possibly incomplete) policies rooted at s0 (Thiébaux et al., 1995), which is performed until interrupted, at which point the best policy found so far is returned. Real-time
dynamic programming (RTDP) (Barto et al., 1995) is another popular anytime algorithm
which is to MDPs what learning real-time A∗ (Korf, 1990) is to deterministic domains, and
which has asymptotic convergence guarantees. The RTDP envelope is made up of sample
paths which are visited with a frequency determined by the current greedy policy and the
transition probabilities in the domain. RTDP can be run on-line, off-line for a given number
of steps or until interrupted. A variant called LRTDP (Bonet & Geffner, 2003) incorporates
mechanisms that focus the search on states whose value has not yet converged, resulting in
convergence speed up and finite time convergence guarantees.
The fltl translation we are about to present targets these anytime algorithms, although
it could also be used with more traditional methods such as value and policy iteration.
3.2 Language and Semantics
Compactly representing non-Markovian reward functions reduces to compactly representing
the behaviours of interest, where by behaviour we mean a set of finite sequences of states
(a subset of S ∗ ), e.g. the {hs0 , s1 i, hs0 , s0 , s1 i, hs0 , s0 , s0 , s1 i . . .} in Figure 1. Recall that the
reward is issued at the end of any prefix Γ(i) in that set. Once behaviours are compactly
represented, it is straightforward to represent non-Markovian reward functions as mappings
from behaviours to real numbers – we shall defer looking at this until Section 3.6.
To represent behaviours compactly, we adopt a version of future linear temporal logic
(FLTL) (see Emerson, 1990), augmented with a propositional constant ‘$’, intended to be
read ‘The behaviour we want to reward has just happened’ or ‘The reward is received now’.
The language $FLTL begins with a set of basic propositions P giving rise to literals:
L ::= P | ¬P | > | ⊥ | $
where > and ⊥ stand for ‘true’ and ‘false’, respectively. The connectives are classical ∧ and
∨, and the temporal modalities  (next) and U (weak until), giving formulae:
F ::= L | F ∧ F | F ∨ F | F | F U F
Our ‘until’ is weak: f1 U f2 means f1 will be true from now on until f2 is, if ever. Unlike
the more commonly used strong ‘until’, this does not imply that f2 will eventually be true.
It allows us to define the useful operator  (always): f ≡ f U ⊥ (f will always be true
k f for k iterations of the  modality (f will
from now on). We also adopt the notations
Wk
be true in exactly k steps), ♦k f for i=1 i f (f will be true within the next k steps), and
V
k f for ki=1 i f (f will be true throughout the next k steps).
Although negation officially occurs only in literals, i.e., the formulae are in negation
normal form (NNF), we allow ourselves to write formulae involving it in the usual way,
31

Thiébaux, Gretton, Slaney, Price & Kabanza

provided that they have an equivalent in NNF. Not every formula has such an equivalent,
because there is no such literal as ¬$ and because eventualities (‘f will be true some time’)
are not expressible. These restrictions are deliberate. If we were to use our notation and
logic to theorise about the allocation of rewards, we would indeed need the means to say
when rewards are not received or to express features such as liveness (‘always, there will be
a reward eventually’), but in fact we are using them only as a mechanism for ensuring that
rewards are given where they should be, and for this restricted purpose eventualities and
the negated dollar are not needed. In fact, including them would create technical difficulties
in relating formulae to the behaviours they represent.
The semantics of this language is similar to that of FLTL, with an important difference:
because the interpretation of the constant $ depends on the behaviour B we want to reward
(whatever that is), the modelling relation |= must be indexed by B. We therefore write
(Γ, i) |=B f to mean that formula f holds at the i-th stage of an arbitrary sequence Γ ∈ S ω ,
relative to behaviour B. Defining |=B is the first step in our description of the semantics:
(Γ, i) |=B $ iff Γ(i) ∈ B
(Γ, i) |=B >
(Γ, i) 6|=B ⊥
(Γ, i) |=B p, for p ∈ P, iff p ∈ Γi
(Γ, i) |=B ¬p, for p ∈ P, iff p 6∈ Γi
(Γ, i) |=B f1 ∧ f2 iff (Γ, i) |=B f1 and (Γ, i) |=B f2
(Γ, i) |=B f1 ∨ f2 iff (Γ, i) |=B f1 or (Γ, i) |=B f2
(Γ, i) |=B

f

iff (Γ, i + 1) |=B f

(Γ, i) |=B f1 U f2 iff ∀k ≥ i if (∀j, i ≤ j ≤ k (Γ, j) 6|=B f2 ) then (Γ, k) |=B f1
Note that except for subscript B and for the first rule, this is just the standard FLTL
semantics, and that therefore $-free formulae keep their FLTL meaning. As with FLTL, we
say Γ |=B f iff (Γ, 0) |=B f , and |=B f iff Γ |=B f for all Γ ∈ S ω .
The modelling relation |=B can be seen as specifying when a formula holds, on which
reading it takes B as input. Our next and final step is to use the |=B relation to define,
for a formula f , the behaviour Bf that it represents, and for this we must rather assume
that f holds, and then solve for B. For instance, let f be (p → $), i.e., we get rewarded
every time p is true. We would like Bf to be the set of all finite sequences ending with a
state containing p. For an arbitrary f , we take Bf to be the set of prefixes that have to be
rewarded if f is to hold in all sequences:
T
Definition 2 Bf ≡ {B | |=B f }
To understand Definition 2, recall that B contains prefixes at the end of which we get
a reward and $ evaluates to true. Since f is supposed to describe the way rewards will
be received in an arbitrary sequence, we are interested in behaviours B which make $
true in such a way as to make f hold without imposing constraints on the evolution of
the world. However, there may be many behaviours with this property, so we take their

32

Decision-Theoretic Planning with non-Markovian Rewards

intersection,3 ensuring that Bf will only reward a prefix if it has to because that prefix is in
every behaviour satisfying f . In all but pathological cases (see Section 3.4), this makes Bf
coincide with the (set-inclusion) minimal behaviour B such that |=B f . The reason for this
‘stingy’ semantics, making rewards minimal, is that f does not actually say that rewards
are allocated to more prefixes than are required for its truth. For instance, (p → $) says
only that a reward is given every time p is true, even though a more generous distribution
of rewards would be consistent with it.
3.3 Examples
It is intuitively clear that many behaviours can be specified by means of $FLTL formulae.
While there is no simple way in general to translate between past and future tense expressions,4 all of the examples used to illustrate PLTL in Section 2.2 above are expressible
naturally in $FLTL, as follows.
The classical goal formula g saying that a goal p is rewarded whenever it happens is
easily expressed: (p → $). As already noted, Bg is the set of finite sequences of states such
that p holds in the last state. If we only care that p is achieved once and get rewarded at
each state from then on, we write (p → $). The behaviour that this formula represents
is the set of finite state sequences having at least one state in which p holds. By contrast,
the formula ¬p U (p ∧ $) stipulates only that the first occurrence of p is rewarded (i.e. it
specifies the behaviour in Figure 1). To reward the occurrence of p at most once every k
steps, we write ((k+1 p ∧ k ¬p) → k+1 $).
For response formulae, where the achievement of p is triggered by the command c,
we write (c → (p → $)) to reward every state in which p is true following the first
issue of the command. To reward only the first occurrence p after each command, we write
(c → (¬p U (p∧$))). As for bounded variants for which we only reward goal achievement
within k steps of the trigger command, we write for example (c → k (p → $)) to reward
all such states in which p holds.
It is also worth noting how to express simple behaviours involving past tense operators.
To stipulate a reward if p has always been true, we write $ U ¬p. To say that we are rewarded
if p has been true since q was, we write (q → ($ U ¬p)).
Finally, we often find it useful to reward the holding of p until the occurrence of q. The
neatest expression for this is ¬q U ((¬p ∧ ¬q) ∨ (q ∧ $)).
3.4 Reward Normality
$FLTL is therefore quite expressive. Unfortunately, it is rather too expressive, in that it
contains formulae which describe “unnatural” allocations of rewards. For instance, they
may make rewards depend on future behaviours rather than on the past, or they may
3. If there
T is no B such that |=B f , which is the case for any $-free f which is not a logical theorem, then
Bf is ∅ – i.e. S ∗ following normal set-theoretic conventions. This limiting case does no harm, since
$-free formulae do not describe the attribution of rewards.
4. It is an open question whether the set of representable behaviours is the same for $FLTL as for PLTL,
that is star-free regular languages. Even if the behaviours were the same, there is little hope that a
practical translation from one to the other exists.

33

Thiébaux, Gretton, Slaney, Price & Kabanza

leave open a choice as to which of several behaviours is to be rewarded.5 An example of
dependence on the future is p → $, which stipulates a reward now if p is going to hold
next. We call such formula reward-unstable. What a reward-stable f amounts to is that
whether a particular prefix needs to be rewarded in order to make f true does not depend
on the future of the sequence. An example of an open choice of which behavior to reward is
(p → $) ∨ (¬p → $) which says we should either reward all achievements of the goal p
or reward achievements of ¬p but does not determine which. We call such formula rewardindeterminate. What a reward-determinate f amounts to is that the set of behaviours
modelling f , i.e. {B | |=B f }, has a unique minimum. If it does not, Bf is insufficient (too
small) to make f true.
In investigating $FLTL (Slaney, 2005), we examine the notions of reward-stability and
reward-determinacy in depth, and motivate the claim that formulae that are both rewardstable and reward-determinate – we call them reward-normal – are precisely those that
capture the notion of “no funny business”. This is the intuition that we ask the reader to
note, as it will be needed in the rest of the paper. Just for reference then, we define:
Definition 3 f is reward-normal iff for every Γ ∈ S ω and every B ⊆ S ∗ , Γ |=B f iff for
every i, if Γ(i) ∈ Bf then Γ(i) ∈ B.
The property of reward-normality is decidable (Slaney, 2005). In Appendix A we give
some simple syntactic constructions guaranteed to result in reward-normal formulae. While
reward-abnormal formulae may be interesting, for present purposes we restrict attention to
reward-normal ones. Indeed, we stipulate as part of our method that only reward-normal
formulae should be used to represent behaviours. Naturally, all formulae in Section 3.3 are
normal.
3.5 $FLTL Formula Progression
Having defined a language to represent behaviours to be rewarded, we now turn to the
problem of computing, given a reward formula, a minimum allocation of rewards to states
actually encountered in an execution sequence, in such a way as to satisfy the formula.
Because we ultimately wish to use anytime solution methods which generate state sequences
incrementally via forward search, this computation is best done on the fly, while the sequence
is being generated. We therefore devise an incremental algorithm based on a model-checking
technique normally used to check whether a state sequence is a model of an FLTL formula
(Bacchus & Kabanza, 1998). This technique is known as formula progression because it
‘progresses’ or ‘pushes’ the formula through the sequence.
Our progression technique is shown in Algorithm 1. In essence, it computes the modelling relation |=B given in Section 3.2. However,unlike the definition of |=B , it is designed
to be useful when states in the sequence become available one at a time, in that it defers the
evaluation of the part of the formula that refers to the future to the point where the next
state becomes available. Let s be a state, say Γi , the last state of the sequence prefix Γ(i)
5. These difficulties are inherent in the use of linear-time formalisms in contexts where the principle of
directionality must be enforced. They are shared for instance by formalisms developed for reasoning
about actions such as the Event Calculus and LTL action theories (see e.g., Calvanese, De Giacomo, &
Vardi, 2002).

34

Decision-Theoretic Planning with non-Markovian Rewards

that has been generated so far, and let b be a boolean true iff Γ(i) is in the behaviour B to
be rewarded. Let the $FLTL formula f describe the allocation of rewards over all possible
futures. Then the progression of f through s given b, written Prog(b, s, f ), is a new formula
which will describe the allocation of rewards over all possible futures of the next state, given
that we have just passed through s. Crucially, the function Prog is Markovian, depending
only on the current state and the single boolean value b. Note that Prog is computable in
linear time in the length of f , and that for $-free formulae, it collapses to FLTL formula
progression (Bacchus & Kabanza, 1998), regardless of the value of b. We assume that Prog
incorporates the usual simplification for sentential constants ⊥ and >: f ∧ ⊥ simplifies to
⊥, f ∧ > simplifies to f , etc.
Algorithm 1 $FLTL Progression
Prog(true, s, $)
= >
Prog(false, s, $)
= ⊥
Prog(b, s, >)
= >
Prog(b, s, ⊥)
= ⊥
Prog(b, s, p)
= > iff p ∈ s and ⊥ otherwise
Prog(b, s, ¬p)
= > iff p 6∈ s and ⊥ otherwise
Prog(b, s, f1 ∧ f2 ) = Prog(b, s, f1 ) ∧ Prog(b, s, f2 )
Prog(b, s, f1 ∨ f2 ) = Prog(b, s, f1 ) ∨ Prog(b, s, f2 )
Prog(b, s, f )
= f
Prog(b, s, f1 U f2 ) = Prog(b, s, f2 ) ∨(Prog(b, s, f1 ) ∧ f1 U f2 )
Rew(s, f )
$Prog(s, f )

= true iff Prog(false, s, f ) = ⊥
= Prog(Rew(s, f ), s, f )

The fundamental property of Prog is the following. Where b ⇔ (Γ(i) ∈ B):
Property 1 (Γ, i) |=B f iff (Γ, i + 1) |=B Prog(b, Γi , f )
Proof:

See Appendix B.



Like |=B , the function Prog seems to require B (or at least b) as input, but of course
when progression is applied in practice we only have f and one new state at a time of Γ,
and what we really want to do is compute the appropriate B, namely that represented by
f . So, similarly as in Section 3.2, we now turn to the second step, which is to use Prog to
decide on the fly whether a newly generated sequence prefix Γ(i) is in Bf and so should
be allocated a reward. This is the purpose of the functions $Prog and Rew, also given in
Algorithm 1. Given Γ and f , the function $Prog in Algorithm 1 defines an infinite sequence
of formulae hf0 , f1 , . . .i in the obvious way:
f0 = f
fi+1 = $Prog(Γi , fi )
To decide whether a prefix Γ(i) of Γ is to be rewarded, Rew first tries progressing the
formula fi through Γi with the boolean flag set to ‘false’. If that gives a consistent result,
we need not reward the prefix and we continue without rewarding Γ(i), but if the result is
35

Thiébaux, Gretton, Slaney, Price & Kabanza

⊥ then we know that Γ(i) must be rewarded in order for Γ to satisfy f . In that case, to
obtain fi+1 we must progress fi through Γi again, this time with the boolean flag set to the
value ‘true’. To sum up, the behaviour corresponding to f is {Γ(i)|Rew(Γi , fi )}.
To illustrate the behaviour of $FLTL progression, consider the formula f = ¬p U (p ∧ $)
stating that a reward will be received the first time p is true. Let s be a state in which p
holds, then Prog(false, s, f ) = ⊥ ∨ (⊥ ∧ ¬p U (p ∧ $)) ≡ ⊥. Therefore, since the formula has
progressed to ⊥, Rew(s, f ) is true and a reward is received. $Prog(s, f ) = Prog(true, s, f ) =
> ∨ (⊥ ∧ ¬p U (p ∧ $)) ≡ >, so the reward formula fades away and will not affect subsequent
progression steps. If, on the other hand, p is false in s, then Prog(false, s, f ) = ⊥ ∨ (> ∧
¬p U (p∧$)) ≡ ¬p U (p∧$)). Therefore, since the formula has not progressed to ⊥, Rew(s, f )
is false and no reward is received. $Prog(s, f ) = Prog(false, s, f ) = ¬p U (p∧$), so the reward
formula persists as is for subsequent progression steps.
The following theorem states that under weak assumptions, rewards are correctly allocated by progression:
Theorem 1 Let f be reward-normal, and let hf0 , f1 , . . .i be the result of progressing it
through the successive states of a sequence Γ using the function $Prog. Then, provided no
fi is ⊥, for all i Rew(Γi , fi ) iff Γ(i) ∈ Bf .
Proof: See Appendix B



The premise of the theorem is that f never progresses to ⊥. Indeed if fi = ⊥ for some
i, it means that even rewarding Γ(i) does not suffice to make f true, so something must
have gone wrong: at some earlier stage, the boolean Rew was made false where it should
have been made true. The usual explanation is that the original f was not reward-normal.
For instance p → $, which is reward unstable, progresses to ⊥ in the next state if p is
true there: regardless of Γ0 , f0 = p → $ = ¬p ∨ $, Rew(Γ0 , f0 ) = false, and f1 = ¬p,
so if p ∈ Γ1 then f2 = ⊥. However, other (admittedly bizarre) possibilities exist: for
example, although p → $ is reward-unstable, its substitution instance > → $, which
also progresses to ⊥ in a few steps, is logically equivalent to $ and is reward-normal.
If the progression method were to deliver the correct minimal behaviour in all cases
(even in all reward-normal cases) it would have to backtrack on the choice of values for the
boolean flags. In the interest of efficiency, we choose not to allow backtracking. Instead,
our algorithm raises an exception whenever a reward formula progresses to ⊥, and informs
the user of the sequence which caused the problem. The onus is thus placed on the domain
modeller to select sensible reward formulae so as to avoid possible progression to ⊥. It
should be noted that in the worst case, detecting reward-normality cannot be easier than
the decision problem for $FLTL so it is not to be expected that there will be a simple
syntactic criterion for reward-normality. In practice, however, commonsense precautions
such as avoiding making rewards depend explicitly on future tense expressions suffice to
keep things normal in all routine cases. For a generous class of syntactically recognisable
reward-normal formulae, see Appendix A.
3.6 Reward Functions
With the language defined so far, we are able to compactly represent behaviours. The
extension to a non-Markovian reward function is straightforward. We represent such a
36

Decision-Theoretic Planning with non-Markovian Rewards

function by a set6 φ ⊆ $FLTL × IR of formulae associated with real valued rewards. We
call φ a reward function specification. Where formula f is associated with reward r in φ,
we write ‘(f : r) ∈ φ’. The rewards are assumed to be independent and additive, so that
the reward function Rφ represented by φ is given by:
X
{r | Γ(i) ∈ Bf }
Definition 4 Rφ (Γ(i)) =
(f :r)∈φ

E.g, if φ is {¬p U (p ∧ $) : 5.2, (q → $) : 7.3}, we get a reward of 5.2 the first time that p
holds, a reward of 7.3 from the first time that q holds onwards, a reward of 12.5 when both
conditions are met, and 0 otherwise.
Again, we can progress a reward function specification φ to compute the reward at
all stages i of Γ. As before, progression defines a sequence hφ0 , φ1 , . . .i of reward function
specifications, with φi+1 = RProg(Γi , φi ), where RProg is the function that applies Prog to
all formulae in a reward function specification:
RProg(s, φ) = {(Prog(s, f ) : r) | (f : r) ∈ φ}
Then, the total reward received at stage i is simply the sum of the real-valued rewards
granted by the progression function to the behaviours represented by the formulae in φi :
X
{r | Rew(Γi , f )}
(f :r)∈φi

By proceeding that way, we get the expected analog of Theorem 1, which states progression
correctly computes non-Markovian reward functions:
Theorem 2 Let φ be a reward-normal7 reward function specification, and let hφ0 , φ1 . . .i be
the result of progressing it through the successive states
X of a sequence Γ using the function
RProg. Then, provided (⊥ : r) 6∈ φi for any i, then
{r | Rew(Γi , f )} = Rφ (Γ(i)).
(f :r)∈φi

Proof:

Immediate from Theorem 1.



3.7 Translation Into MDP
We now exploit the compact representation of a non-Markovian reward function as a reward
function specification to translate an NMRDP into an equivalent MDP amenable to statebased anytime solution methods. Recall from Section 2 that each e-state in the MDP is
labelled by a state of the NMRDP and by history information sufficient to determine the
immediate reward. In the case of a compact representation as a reward function specification
φ0 , this additional information can be summarised by the progression of φ0 through the
sequence of states passed through. So an e-state will be of the form hs, φi, where s ∈ S is
6. Strictly speaking, a multiset, but for convenience we represent it as a set, with the rewards for multiple
occurrences of the same formula in the multiset summed.
7. We extend the definition of reward-normality to reward specification functions in the obvious way, by
requiring that all reward formulae involved be reward normal.

37

Thiébaux, Gretton, Slaney, Price & Kabanza

a state, and φ ⊆ $FLTL × IR is a reward function specification (obtained by progression).
Two e-states hs, φi and ht, ψi are equal if s = t, the immediate rewards are the same, and
the results of progressing φ and ψ through s are semantically equivalent.8
Definition 5 Let D = hS, s0 , A, Pr, Ri be an NMRDP, and φ0 be a reward function specification representing R (i.e., Rφ0 = R, see Definition 4). We translate D into the MDP
D0 = hS 0 , s00 , A0 , Pr0 , R0 i defined as follows:
1. S 0 ⊆ S × 2$FLTL ×IR
2. s00 = hs0 , φ0 i
3. A0 (hs, φi) = A(s)
Pr(s, a, s0 ) if φ0 = RProg(s, φ)
0
otherwise
0
0
If a 6∈ A (hs, φi), then Pr (hs, φi, a, •) is undefined
X
5. R0 (hs, φi) =
{r | Rew(s, f )}
4. If a ∈ A0 (hs, φi), then Pr0 (hs, φi, a, hs0 , φ0 i) =



(f :r)∈φ

6. For all s0 ∈ S 0 , s0 is reachable under A0 from s00 .
Item 1 says that the e-states are labelled by a state and a reward function specification. Item
2 says that the initial e-state is labelled with the initial state and with the original reward
function specification. Item 3 says that an action is applicable in an e-state if it is applicable
in the state labelling it. Item 4 explains how successor e-states and their probabilities are
computed. Given an action a applicable in an e-state hs, φi, each successor e-state will
be labelled by a successor state s0 of s via a in the NMRDP and by the progression of φ
through s. The probability of that e-state is Pr(s, a, s0 ) as in the NMRDP. Note that the
cost of computing Pr0 is linear in that of computing Pr and in the sum of the lengths of the
formulae in φ. Item 5 has been motivated before (see Section 3.6). Finally, since items 1–5
leave open the choice of many MDPs differing only in the unreachable states they contain,
item 6 excludes all such irrelevant extensions. It is easy to show that this translation leads
to an equivalent MDP, as defined in Definition 1. Obviously, the function τ required for
Definition1 is given by τ (hs, φi) = s, and then the proof is a matter of checking conditions.
In our practical implementation, the labelling is one step ahead of that in the definition:
we label the initial e-state with RProg(s0 , φ0 ) and compute the current reward and the current reward specification label by progression of predecessor reward specifications through
the current state rather than through the predecessor states. As will be apparent below,
this has the potential to reduce the number of states in the generated MDP.
Figure 7 shows the equivalent MDP produced for the $FLTL version of our NMRDP
example in Figure 3. Recall that for this example, the PLTL reward formula was q ∧   p.
In $FLTL, the allocation of rewards is described by ((p ∧ q) → $). The figure also
8. Care is needed over the notion of ‘semantic equivalence’. Because rewards are additive, determining
equivalence may involve arithmetic as well as theorem proving. For example, the reward function specification {(p → $ : 3), (q → $ : 2)} is equivalent to {((p ∧ q) → $ : 5), ((p ∧ ¬q) → $ : 3), ((¬p ∧ q) → $ : 2)}
although there is no one-one correspondence between the formulae in the two sets.

38

Decision-Theoretic Planning with non-Markovian Rewards

start_state
f1
Reward=0
a(0.16)
p
f1,f2
Reward=0
a(1)
p
f1,f2,f3
Reward=0

a(0.04) b(0.2)

a(0.16)

a(0.64)

q
f1
Reward=0

b(1)

a(1) b(1)

b(0.8)

a(0.2) b(1)

a(0.8)

The following formulae label the e-states:
f1 : ((p ∧ q) → $)
f2 : q → $
f3 : q → $

p, q
f1,f2
Reward=0
a(1) b(1)
p, q
f1,f2,f3
Reward=0
a(1) b(1)
p, q
f1,f2,f3
Reward=1

a(1) b(1)

Figure 7: Equivalent MDP Produced by fltl
shows the relevant formulae labelling the e-states, obtained by progression of this reward
formula. Note that without progressing one step ahead, there would be 3 e-states with state
{p} on the left-hand side, labelled with {f1 }, {f1 , f2 }, and {f1 , f2 , f3 }, respectively.
3.8 Blind Minimality
The size of the MDP obtained, i.e. the number of e-states it contains is a key issue for
us, as it has to be amenable to state-based solution methods. Ideally, we would like the
MDP to be of minimal size. However, we do not know of a method building the minimal
equivalent MDP incrementally, adding parts as required by the solution method. And since
in the worst case even the minimal equivalent MDP can be larger than the NMRDP by a
factor exponential in the length of the reward formulae (Bacchus et al., 1996), constructing
it entirely would nullify the interest of anytime solution methods.
However, as we now explain, Definition 5 leads to an equivalent MDP exhibiting a relaxed
notion of minimality, and which is amenable to incremental construction. By inspection,
we may observe that wherever an e-state hs, φi has a successor hs0 , φ0 i via action a, this
means that in order to succeed in rewarding the behaviours described in φ by means of
execution sequences that start by going from s to s0 via a, it is necessary that the future
starting with s0 succeeds in rewarding the behaviours described in φ0 . If hs, φi is in the
minimal equivalent MDP, and if there really are such execution sequences succeeding in
rewarding the behaviours described in φ, then hs0 , φ0 i must also be in the minimal MDP.
That is, construction by progression can only introduce e-states which are a priori needed.
Note that an e-state that is a priori needed may not really be needed: there may in fact
be no execution sequence using the available actions that exhibits a given behaviour. For

39

Thiébaux, Gretton, Slaney, Price & Kabanza

instance, consider the response formula (p → (k q → k $)), i.e., every time trigger p
is true, we will be rewarded k steps later provided q is true then. Obviously, whether p
is true at some stage affects the way future states should be rewarded. However, if the
transition relation happens to have the property that k steps from a state satisfying p, no
state satisfying q can be reached, then a posteriori p is irrelevant, and there was no need to
label e-states differently according to whether p was true or not – observe an occurrence of
this in the example in Figure 7, and how this leads fltl to produce an extra state at the
bottom left of the Figure. To detect such cases, we would have to look perhaps quite deep
into feasible futures, which we cannot do while constructing the e-states on the fly. Hence
the relaxed notion which we call blind minimality does not always coincide with absolute
minimality.
We now formalise the difference between true and blind minimality. For this purpose,
it is convenient to define some functions ρ and µ mapping e-states e to functions from S ∗
to IR intuitively assigning rewards to sequences in the NMRDP starting from τ (e). Recall
from Definition 1 that τ maps each e-state of the MDP to the underlying NMRDP state.
Definition 6 Let D be an NMRDP. Let S 0 be the set of e-states in an equivalent MDP D0
f0 (s0 )
for D. Let e be any reachable e-state in S 0 . Let Γ0 (i) be a sequence of e-states in D
0
e 0 ) obtained under τ in
such that Γ0 (i) = e. Let Γ(i) be the corresponding sequence in D(s
the sense that, for each j ≤ i, Γ(j) = τ (Γ0j ). Then for any ∆ ∈ S ∗ , we define

ρ(e) : ∆ 7→
and


µ(e) : ∆ 7→

R(Γ(i − 1); ∆) if ∆0 = Γi
0
otherwise

e i)
R(Γ(i − 1); ∆) if ∆ ∈ D(Γ
0
otherwise

For any unreachable e-state e, we define both ρ(e)(∆) and µ(e)(∆) to be 0 for all ∆.
Note carefully the difference between ρ and µ. The former describes the rewards assigned
to all continuations of a given state sequence, while the latter confines rewards to feasible
continuations. Note also that ρ and µ are well-defined despite the indeterminacy in the
choice of Γ0 (i), since by clause 4 of Definition 1, all such choices lead to the same values for
R.
Theorem 3 Let S 0 be the set of e-states in an equivalent MDP D0 for D = hS, s0 , A, Pr, Ri.
D0 is minimal iff every e-state in S 0 is reachable and S 0 contains no two distinct e-states s01
and s02 with τ (s01 ) = τ (s02 ) and µ(s01 ) = µ(s02 ).
Proof:

See Appendix B.



Blind minimality is similar, except that, since there is no looking ahead, no distinction can
be drawn between feasible trajectories and others in the future of s:
Definition 7 Let S 0 be the set of e-states in an equivalent MDP D0 for D = hS, s0 , A, Pr, Ri.
D0 is blind minimal iff every e-state in S 0 is reachable and S 0 contains no two distinct estates s01 and s02 with τ (s01 ) = τ (s02 ) and ρ(s01 ) = ρ(s02 ).
40

Decision-Theoretic Planning with non-Markovian Rewards

Theorem 4 Let D0 be the translation of D as in Definition 5. D0 is a blind minimal
equivalent MDP for D.
Proof:

See Appendix B.



The size difference between the blind-minimal and minimal MDPs will depend on the
precise interaction between rewards and dynamics for the problem at hand, making theoretical analyses difficult and experimental results rather anecdotal. However, our experiments
in Section 5 and 6 will show that from a computation time point of view, it is often preferable to work with the blind-minimal MDP than to invest in the overhead of computing the
truly minimal one.
Finally, recall that syntactically different but semantically equivalent reward function
specifications define the same e-state. Therefore, neither minimality nor blind minimality
can be achieved in general without an equivalence check at least as complex as theorem
proving for LTL. In pratical implementations, we avoid theorem proving in favour of embedding (fast) formula simplification in our progression and regression algorithms. This
means that in principle we only approximate minimality and blind minimality, but this
appears to be enough for practical purposes.
3.9 Embedded Solution/Construction
Blind minimality is essentially the best achievable with anytime state-based solution methods which typically extend their envelope one step forward without looking deeper into the
future. Our translation into a blind-minimal MDP can be trivially embedded in any of these
solution methods. This results in an ‘on-line construction’ of the MDP: the method entirely
drives the construction of those parts of the MDP which it feels the need to explore, and
leave the others implicit. If time is short, a suboptimal or even incomplete policy may be
returned, but only a fraction of the state and expanded state spaces might be constructed.
Note that the solution method should raise an exception as soon as one of the reward formulae progresses to ⊥, i.e., as soon as an expanded state hs, φi is built such that (⊥ : r) ∈ φ,
since this acts as a detector of unsuitable reward function specifications.
To the extent enabled by blind minimality, our approach allows for a dynamic analysis of
the reward formulae, much as in pltlstr (Bacchus et al., 1997). Indeed, only the execution
sequences feasible under a particular policy actually explored by the solution method contribute to the analysis of rewards for that policy. Specifically, the reward formulae generated
by progression for a given policy are determined by the prefixes of the execution sequences
feasible under this policy. This dynamic analysis is particularly useful, since relevance of
reward formulae to particular policies (e.g. the optimal policy) cannot be detected a priori.
The forward-chaining planner TLPlan (Bacchus & Kabanza, 2000) introduced the idea
of using FLTL to specify domain-specific search control knowledge and formula progression
to prune unpromising sequential plans (plans violating this knowledge) from deterministic
search spaces. This has been shown to provide enormous time gains, leading TLPlan to
win the 2002 planning competition hand-tailored track. Because our approach is based
on progression, it provides an elegant way to exploit search control knowledge, yet in the
context of decision-theoretic planning. Here this results in a dramatic reduction of the

41

Thiébaux, Gretton, Slaney, Price & Kabanza

fraction of the MDP to be constructed and explored, and therefore in substantially better
policies by the deadline.
We achieve this as follows. We specify, via a $-free formula c0 , properties which we know
must be verified by paths feasible under promising policies. Then we simply progress c0
alongside the reward function specification, making e-states triples hs, φ, ci where c is a $-free
formula obtained by progression. To prevent the solution method from applying an action
that leads to the control knowledge being violated, the action applicability condition (item
3 in Definition 5) becomes: a ∈ A0 (hs, φ, ci) iff a ∈ A(s) and c 6= ⊥ (the other changes are
straightforward). For instance, the effect of the control knowledge formula (p → q) is to
remove from consideration any feasible path in which p is not followed by q. This is detected
as soon as violation occurs, when the formula progresses to ⊥. Although this paper focuses
on non-Markovian rewards rather than dynamics, it should be noted that $-free formulae
can also be used to express non-Markovian constraints on the system’s dynamics, which
can be incorporated in our approach exactly as we do for the control knowledge.
3.10 Discussion
Existing approaches (Bacchus et al., 1996, 1997) advocate the use of PLTL over a finite
past to specify non-Markovian rewards. In the PLTL style of specification, we describe
the past conditions under which we get rewarded now, while with $FLTL we describe the
conditions on the present and future under which future states will be rewarded. While the
behaviours and rewards may be the same in each scheme, the naturalness of thinking in one
style or the other depends on the case. Letting the kids have a strawberry dessert because
they have been good all day fits naturally into a past-oriented account of rewards, whereas
promising that they may watch a movie if they tidy their room (indeed, making sense of the
whole notion of promising) goes more naturally with $FLTL. One advantage of the PLTL
formulation is that it trivially enforces the principle that present rewards do not depend
on future states. In $FLTL, this responsibility is placed on the domain modeller. The best
we can offer is an exception mechanism to recognise mistakes when their effects appear,
or syntactic restrictions. On the other hand, the greater expressive power of $FLTL opens
the possibility of considering a richer class of decision processes, e.g. with uncertainty as
to which rewards are received (the dessert or the movie) and when (some time next week,
before it rains).
At any rate, we believe that $FLTL is better suited than PLTL to solving NMRDPs
using anytime state-based solution methods. While the pltlsim translation could be easily embedded in such a solution method, it loses the structure of the original formulae
when considering subformulae individually. Consequently, the expanded state space easily
becomes exponentially bigger than the blind-minimal one. This is problematic with the
solution methods we consider, because size severely affects their performance in solution
quality. The pre-processing phase of pltlmin uses PLTL formula regression to find sets
of subformulae as potential labels for possible predecessor states, so that the subsequent
generation phase builds an MDP representing all and only the histories which make a difference to the way actually feasible execution sequences should be rewarded. Not only does
this recover the structure of the original formula, but in the best case, the MDP produced
is exponentially smaller than the blind-minimal one. However, the prohibitive cost of the

42

Decision-Theoretic Planning with non-Markovian Rewards

pre-processing phase makes it unsuitable for anytime solution methods. We do not consider that any method based on PLTL and regression will achieve a meaningful relaxed
notion of minimality without a costly pre-processing phase. fltl is an approach based on
$FLTL and progression which does precisely that, letting the solution method resolve the
tradeoff between quality and cost in a principled way intermediate between the two extreme
suggestions above.
The structured representation and solution methods targeted by Bacchus et al. (1997)
differ from the anytime state-based solution methods fltl primarily aims at, in particular
in that they do not require explicit state enumeration at all. Here, non-minimality is not as
problematic as with the state-based approaches. In virtue of the size of the MDP produced,
the pltlstr translation is, as pltlsim, clearly unsuitable to anytime state-based methods.9
In another sense, too, fltl represents a middle way, combining the advantages conferred by
state-based and structured approaches, e.g. by pltlmin on one side, and pltlstr on the
other. From the former fltl inherits a meaningful notion of minimality. As with the latter,
approximate solution methods can be used and can perform a restricted dynamic analysis of
the reward formulae. In particular, formula progression enables even state-based methods
to exploit some of the structure in ‘$FLTL space’. However, the gap between blind and
true minimality indicates that progression alone is insufficient to always fully exploit that
structure. There is a hope that pltlstr is able to take advantage of the full structure of
the reward function, but also a possibility that it will fail to exploit even as much structure
as fltl, as efficiently. An empirical comparison of the three approaches is needed to answer
this question and identify the domain features favoring one over the other.

4. NMRDPP
The first step towards a decent comparison of the different approaches is to have a framework
that includes them all. The Non-Markovian Reward Decision Process Planner, nmrdpp,
is a platform for the development and experimentation of approaches to NMRDPs. it
provides an implementation of the approaches we have described in a common framework,
within a single system, and with a common input language. nmrdpp is available on-line,
see http://rsise.anu.edu.au/~charlesg/nmrdpp. It is worth noting that Bacchus et al.
(1996, 1997) do not report any implementation of their approaches.
4.1 Input language
The input language enables the specification of actions, initial states, rewards, and search
control-knowledge. The format for the action specification is essentially the same as in the
SPUDD system (Hoey et al., 1999). The reward specification is one or more formulae, each
associated with a name and a real number. These formulae are in either PLTL or $FLTL.
Control knowledge is given in the same language as that chosen for the reward. Control
knowledge formulae will have to be verified by any sequence of states feasible under the
generated policies. Initial states are simply specified as part of the control knowledge or as
explicit assignments to propositions.
9. It would be interesting, on the other hand, to use pltlstr in conjunction with symbolic versions of such
methods, e.g. Symbolic LAO* (Feng & Hansen, 2002) or Symbolic RTDP (Feng, Hansen, & Zilberstein,
2003).

43

Thiébaux, Gretton, Slaney, Price & Kabanza

action flip
heads (0.5)
endaction
action tilt
heads (heads (0.9) (0.1))
endaction
heads = ff
[first, 5.0]? heads and ~prv (pdi heads)
[seq, 1.0]? (prv^2 heads) and (prv heads) and ~heads
Figure 8: Input for the Coin Example. prv (previously) stands for  and
-.
pdi (past diamond) stands for ♦
For instance, consider a simple example consisting of a coin showing either heads or
tails (¬heads). There are two actions that can be performed. The flip action changes the
coin to show heads or tails with a 50% probability. The tilt action changes it with 10%
probability, otherwise leaving it as it is. The initial state is tails. We get a reward of 5.0 for
- heads in PLTL) and a reward of 1.0 each
the very first head (this is written heads ∧ ¬  ♦
time we achieve the sequence heads, heads, tails (2 heads ∧ heads ∧ ¬heads in PLTL). In
our input language, this NMRDP is described as shown in Figure 8.
4.2 Common framework
The common framework underlying nmrdpp takes advantage of the fact that NMRDP
solution methods can, in general, be divided into the distinct phases of preprocessing,
expansion, and solving. The first two are optional.
For pltlsim, preprocessing simply computes the set Sub(F ) of subformulae of the reward
formulae. For pltlmin, it also includes computing the labels l(s) for each state s. For
pltlstr, preprocessing involves computing the set T of temporal variables as well as the
ADDs for their dynamics and for the rewards. fltl does not require any preprocessing.
Expansion is the optional generation of the entire equivalent MDP prior to solving.
Whether or not off-line expansion is sensible depends on the MDP solution method used. If
state-based value or policy iteration is used, then the MDP needs to be expanded anyway.
If, on the other hand, an anytime search algorithm or structured method is used, it is
definitely a bad idea. In our experiments, we often used expansion solely for the purpose of
measuring the size of the generated MDP.
Solving the MDP can be done using a number of methods. Currently, nmrdpp provides
implementations of classical dynamic programming methods, namely state-based value and
policy iteration (Howard, 1960), of heuristic search methods: state-based LAO* (Hansen &
Zilberstein, 2001) using either value or policy iteration as a subroutine, and of one structured
method, namely SPUDD (Hoey et al., 1999). Prime candidates for future developments are
(L)RTDP (Bonet & Geffner, 2003), symbolic LAO* (Feng & Hansen, 2002), and symbolic
RTDP (Feng et al., 2003).
44

Decision-Theoretic Planning with non-Markovian Rewards

load coin NMRDP
pltlstr preprocessing

> loadWorld(’coin’)
> preprocess(’sPltl’)
> startCPUtimer
> spudd(0.99, 0.0001)
> stopCPUtimer
> readCPUtimer
1.22000
> iterationCount
1277
> displayDot(valueToDot)
Expected value

18.87

18.62

report number of iterations
display ADD of value function

(prv heads)

(prv (prv pdi heads))

23.87

report solving time

heads

(prv heads)

(prv (prv pdi heads))

solve MDP with SPUDD(β, )

23.62

(prv^2 heads)

(prv pdi heads)

18.25

23.15

(prv pdi heads)

19.25

24.15

display policy

> displayDot(policyToDot)
Optimal policy

heads

(prv heads)

flip

>
>
>
6
>

tilt

pltlmin preprocessing
completely expand MDP
report MDP size

preprocess(’mPltl’)
expand
domainStateSize
printDomain ("") | ’show-domain.rb’
Reward=0
flip(0.5)

flip(0.5)

display postcript rendering of MDP

tilt(0.9)

tilt(0.1)

heads
Reward=5
flip(0.5)
heads
Reward=0
tilt(0.1)

flip(0.5)

tilt(0.9)

tilt(0.9)

flip(0.5)

tilt(0.1)

flip(0.5)

Reward=1
tilt(0.9)

flip(0.5)

tilt(0.9)

flip(0.5)

tilt(0.1)

tilt(0.1)

flip(0.5)
Reward=0
flip(0.5)

flip(0.5)

flip(0.5)

tilt(0.9)

tilt(0.1)

heads
Reward=0

solve MDP with VI(β, )
report number of iterations

> valIt(0.99, 0.0001)
> iterationCount
1277
> getPolicy
...

output policy (textual)

Figure 9: Sample Session
45

Thiébaux, Gretton, Slaney, Price & Kabanza

4.3 Approaches covered
Altogether, the various types of preprocessing, the choice of whether to expand, and the
MDP solution methods, give rise to quite a number of NMRDP approaches, including, but
not limited to those previously mentioned (see e.g. pltlstr(a) below). Not all combinations are possible. E.g., state-based processing variants are incompatible with structured
solution methods (the converse is possible in principle, however). Also, there is at present
no structured form of preprocessing for $FLTL formulae.
pltlstr(a) is an example of an interesting variant of pltlstr, which we obtain by
considering additional preprocessing, whereby the state space is explored (without explicitly
enumerating it) to produce a BDD representation of the e-states reachable from the start
state. This is done by starting with a BDD representing the start e-state, and repeatedly
applying each action. Non-zero probabilities are converted to ones and the result “or-ed”
with the last result. When no action adds any reachable e-states to this BDD, we can
be sure it represents the reachable e-state space. This is then used as additional control
knowledge to restrict the search. It should be noted that without this phase pltlstr makes
no assumptions about the start state, and thus is left at a possible disadvantage. Similar
structured reachability analysis techniques have been used in the symbolic implementation
of LAO* (Feng & Hansen, 2002). However, an important aspect of what we do here is that
temporal variables are also included in the BDD.
4.4 The nmrdpp System
nmrdpp is controlled by a command language, which is read either from a file or interactively. The command language provides commands for the different phases (preprocessing,
expansion, solution) of the methods, commands to inspect the resulting policy and value
functions, e.g. with rendering via DOT (AT&T Labs-Research, 2000), as well as supporting
commands for timing and memory usage. A sample session, where the coin NMRDP is
successively solved with pltlstr and pltlmin is shown in Figure 9.
nmrdpp is implemented in C++, and makes use of a number of supporting libraries.
In particular, it relies heavily on the CUDD package for manipulating ADDs (Somenzi,
2001): action specification trees are converted into and stored as ADDs by the system,
and moreover the structured algorithms rely heavily on CUDD for ADD computations.
The state-based algorithms make use of the MTL – Matrix Template Library for matrix
operations. MTL takes advantage of modern processor features such as MMX and SSE
and provides efficient sparse matrix operations. We believe that our implementations of
MDP solution methods are comparable with the state of the art. For instance, we found
that our implementation of SPUDD is comparable in performance (within a factor of 2) to
the reference implementation (Hoey et al., 1999). On the other hand, we believe that data
structures used for regression and progression of temporal formulae could be optimised.

5. Experimental Analysis
We are faced with three substantially different approaches that are not easy to compare,
as their performance will depend on domain features as varied as the structure in the
transition model, the type, syntax, and length of the temporal reward formula, the presence

46

Decision-Theoretic Planning with non-Markovian Rewards

of rewards unreachable or irrelevant to the optimal policy, the availability of good heuristics
and control-knowledge, etc, and on the interactions between these factors. In this section,
we report an experimental investigation into the influence of some of these factors and try
to answer the questions raised previously:10
1. is the dynamics of the domain the predominant factor affecting performance?
2. is the type of reward a major factor?
3. is the syntax used to describe rewards a major factor?
4. is there an overall best method?
5. is there an overall worst method?
6. does the preprocessing phase of pltlmin pay, compared to pltlsim?
7. does the simplicity of the fltl translation compensate for blind-minimality, or does
the benefit of true minimality outweigh the cost of pltlmin preprocessing?
8. are the dynamic analyses of rewards in pltlstr and fltl effective?
9. is one of these analyses more powerful, or are they rather complementary?
In some cases but not all, we were able to identify systematic patterns. The results in this
section were obtained using a Pentium4 2.6GHz GNU/Linux 2.4.20 machine with 500MB
of ram.
5.1 Preliminary Remarks
Clearly, fltl and pltlstr(a) have great potential for exploiting domain-specific heuristics and control-knowledge; pltlmin less so. To avoid obscuring the results, we therefore
refrained from incorporating these features in the experiments. When running LAO*, the
heuristic value of a state was the crudest possible (the sum of all reward values in the
problem). Performance results should be interpreted in this light – they do not necessarily
reflect the practical abilities of the methods that are able to exploit these features.
We begin with some general observations. One question raised above was whether the
gain during the PLTL expansion phase is worth the expensive preprocessing performed by
pltlmin, i.e. whether pltlmin typically outperforms pltlsim. We can definitively answer
this question: up to pathological exceptions, preprocessing pays. We found that expansion
was the bottleneck, and that post-hoc minimisation of the MDP produced by pltlsim did
not help much. pltlsim is therefore of little or no practical interest, and we decided not to
report results on its performance, as it is often an order of magnitude worse than that of
pltlmin. Unsurprisingly, we also found that pltlstr would typically scale to larger state
spaces, inevitably leading it to outperform state-based methods. However, this effect is not
uniform: structured solution methods sometimes impose excessive memory requirements
which makes them uncompetitive in certain cases, for example where n f , for large n,
features as a reward formula.
10. Here is an executive summary of the answers for the executive reader. 1. no, 2. yes, 3. yes, 4. pltlstr
and fltl, 5. pltlsim, 6. yes, 7. yes and no, respectively, 8. yes, 9. no and yes, respectively.

47

Thiébaux, Gretton, Slaney, Price & Kabanza

5.2 Domains
Experiments were performed on four hand-coded domains (propositions + dynamics) and
on random domains. Each hand-coded domain has n propositions pi , and a dynamics
which makes every state possible and eventually reachable from the initial state in which
all propositions are false. The first two such domains, spudd-linear and spudd-expon
were discussed by Hoey et al. (1999); the two others are our own.
The intention of spudd-linear was to take advantage of the best case behaviour of
SPUDD. For each proposition pi , it has an action ai which sets pi to true and all propositions
pj , 1 ≤ j < i to false. spudd-expon, was used by Hoey et al. (1999) to demonstrate the
worst case behaviour of SPUDD. For each proposition pi , it has an action ai which sets pi
to true only when all propositions pj , 1 ≤ j < i are true (and sets pi to false otherwise), and
sets the latter propositions to false. The third domain, called on/off, has one “turn-on”
and one “turn-off” action per proposition. The “turn-on-pi ” action only probabilistically
succeeds in setting pi to true when pi was false. The turn-off action is similar. The fourth
domain, called complete, is a fully connected reflexive domain. For each proposition pi
there is an action ai which sets pi to true with probability i/(n + 1) (and to false otherwise)
and pj , j 6= i to true or false with probability 0.5. Note that ai can cause a transition to
any of the 2n states.
Random domains of size n also involve n propositions. The method for generating their
dynamics is detailed in appendix C. Let us just summarise by saying that we are able to
generate random dynamics exhibiting a given degree of “structure” and a given degree of
uncertainty. Lack of structure essentially measures the bushiness of the internal part of the
ADDs representing the actions, and uncertainty measures the bushiness of their leaves.
5.3 Influence of Dynamics
The interaction between dynamics and reward certainly affects the performance of the
different approaches, though not so strikingly as other factors such as the reward type (see
below). We found that under the same reward scheme, varying the degree of structure or
uncertainty did not generally change the relative success of the different approaches. For
instance, Figures 10 and 11 show the average run time of the methods as a function of
the degree of structure, resp. degree of uncertainty, for random problems of size n = 6 and
reward n ¬> (the state encountered at stage n is rewarded, regardless of its properties11 ).
Run-time increases slightly with both degrees, but there is no significant change in relative
performance. These are typical of the graphs we obtain for other rewards.
Clearly, counterexamples to this observation exist. These are most notable in cases of
extreme dynamics, for instance with the spudd-expon domain. Although for small values
of n, such as n = 6, pltlstr approaches are faster than the others in handling the reward
n ¬  > for virtually any type of dynamics we encountered, they perform very poorly with
that reward on spudd-expon. This is explained by the fact that only a small fraction of
spudd-expon states are reachable in the first n steps. After n steps, fltl immediately
recognises that reward is of no consequence, because the formula has progressed to >.
pltlmin discovers this fact only after expensive preprocessing. pltlstr, on the other
hand, remains concerned by the prospect of reward, just as pltlsim would.
11.

n $

in $FLTL

48

Average CPU time (sec)

Decision-Theoretic Planning with non-Markovian Rewards

30
25
20
15
10
5

0.1

0.3

0.5

0.7

0.9

1.1

Structure (0:Structured, ... 1:Unstructured)
FLTL

PLTLMIN

PLTLSTRUCT

PLTLSTRUCT(A)

Figure 10: Changing the Degree of Structure

Average CPU time (sec)

35
25
20
15
10
5

0

0.2

0.4

0.6

0.8

1

1.2

Uncertainty (0:Certain, ... 1:Uncertain)
FLTL

PLTLMIN

PLTLSTRUCT

PLTLSTRUCT(A)

Figure 11: Changing the Degree of Uncertainty
5.4 Influence of Reward Types
The type of reward appears to have a stronger influence on performance than dynamics.
This is unsurprising, as the reward type significantly affects the size of the generated MDP:
certain rewards only make the size of the minimal equivalent MDP increase by a constant
number of states or a constant factor, while others make it increase by a factor exponential
in the length of the formula. Table 1 illustrates this. The third column reports the size of
the minimal equivalent MDP induced by the formulae on the left hand side.12
A legitimate question is whether there is a direct correlation between size increase and
(in)appropriateness of the different methods. For instance, we might expect the state-based
methods to do particularly well in conjunction with reward types inducing a small MDP and
12. The figures are not necessarily valid for non-completely connected NMRDPs. Unfortunately, even for
completely connected domains, there does not appear to be a much cheaper way to determine the MDP
size than to generate it and count states.

49

Thiébaux, Gretton, Slaney, Price & Kabanza

type
first time all pi s
pi s in sequence from start state
two consecutive pi s
all pi s n times ago

formula
- ∧ni=1 pi )
(∧ni=1 pi ) ∧ (¬  ♦
(∧ni=1 i pi ) ∧ n ¬  >
n−1
∨i=1
(pi ∧ pi+1 )
n ∧ni=1 pi

size
O(1)||S||
O(n)||S||
O(nk )||S||
O(2n )||S||

fastest
pltlstr(a)
fltl
pltlstr
pltlstr

slowest
pltlmin
pltlstr
fltl
pltlmin

Table 1: Influence of Reward Type on MDP Size and Method Performance

Average CPU time (sec)

1000
600
400
200

2

2.5

3

3.5

4

4.5

5

5.5

n
All APPROACHES prvIn
All APPROACHES prvOut

Figure 12: Changing the Syntax
otherwise badly in comparison with structured methods. Interestingly, this is not always
the case. For instance, in Table 1 whose last two columns report the fastest and slowest
methods over the range of hand-coded domains where 1 ≤ n ≤ 12, the first row contradicts
that expectation. Moreover, although pltlstr is fastest in the last row, for larger values
of n (not represented in the table), it aborts through lack of memory, unlike the other
methods.
The most obvious observations arising out of these experiments is that pltlstr is nearly
always the fastest – until it runs out of memory. Perhaps the most interesting results are
those in the second row, which expose the inability of methods based on PLTL to deal
with rewards specified as long sequences of events. In converting the reward formula to
a set of subformulae, they lose information about the order of events, which then has to
be recovered laboriously by reasoning. $FLTL progression in contrast takes the events one
at a time, preserving the relevant structure at each step. Further experimentation led us
to observe that all PLTL based algorithms perform poorly where reward is specified using
- k f , and k f (f has been true k steps ago, within the last k
formulae of the form k f , ♦
steps, or at all of the last k steps).
5.5 Influence of Syntax
Unsurprisingly, we find that the syntax used to express rewards, which affects the length
of the formula, has a major influence on the run time. A typical example of this effect is
captured in Figure 12. This graph demonstrates how re-expressing prvOut ≡ n (∧ni=1 pi )
50

Decision-Theoretic Planning with non-Markovian Rewards

State count/(2^n)

11
9
7
5
3
1
0

2

4

6

8

10

12

14

n
PLTLMIN
FLTL

Figure 13: Effect of Multiple Rewards on MDP size

Total CPU time (sec)

1500
1000
500

0

2

4

6

8

10

12

14

n
FLTL

PLTLMIN

PLTLSTRUCT

PLTLSTRUCT(A)

Figure 14: Effect of Multiple Rewards on Run Time
as prvIn ≡ ∧ni=1 n pi , thereby creating n times more temporal subformulae, alters the
running time of all PLTL methods. fltl is affected too as $FLTL progression requires two
iterations through the reward formula. The graph represents the averages of the running
times over all the methods, for the complete domain.
Our most serious concern in relation to the PLTL approaches is their handling of reward
specifications containing multiple reward elements. Most notably we found that pltlmin
does not necessarily produce the minimal equivalent MDP in this situation. To demonstrate, we consider the set of reward formulae {f1 , f2 , . . . , fn }, each associated with the
same real value r. Given this, PLTL approaches will distinguish unnecessarily between past
behaviours which lead to identical future rewards. This may occur when the reward at an
e-state is determined by the truth value of f1 ∨ f2 . This formula does not necessarily require
e-states that distinguish between the cases in which {f1 ≡ >, f2 ≡ ⊥} and {f1 ≡ ⊥, f2 ≡ >}
hold; however, given the above specification, pltlmin makes this distinction. For example,

51

Thiébaux, Gretton, Slaney, Price & Kabanza

taking fi = pi , Figure 13 shows that fltl leads to an MDP whose size is at most 3 times
that of the NMRDP. In contrast, the relative size of the MDP produced by pltlmin is
linear in n, the number of rewards and propositions. These results are obtained with all
hand-coded domains except spudd-expon. Figure 14 shows the run-times as a function
of n for complete. fltl dominates and is only overtaken by pltlstr(A) for large values
of n, when the MDP becomes too large for explicit exploration to be practical. To obtain
the minimal equivalent MDP using pltlmin, a bloated reward specification of the form
{( ∨ni=1 (pi ∧nj=1,j6=i ¬pj ) : r), . . . , ( ∧ni=1 pi : n ∗ r)} is necessary, which, by virtue of its
exponential length, is not an adequate solution.
5.6 Influence of Reachability
All approaches claim to have some ability to ignore variables which are irrelevant because
the condition they track is unreachable:13 pltlmin detects them through preprocessing,
pltlstr exploits the ability of structured solution methods to ignore them, and fltl ignores them when progression never exposes them. However, given that the mechanisms for
avoiding irrelevance are so different, we expect corresponding differences in their effects.
On experimental investigation, we found that the differences in performance are best illustrated by looking at response formulae, which assert that if a trigger condition c is reached
then a reward will be received upon achievement of the goal g in, resp. within, k steps.
- k c, and in $FLTL, (c → k (g → $)), resp.
In PLTL, this is written g ∧ k c, resp. g ∧ ♦
(c → k (g → $))
When the goal is unreachable, PLTL approaches perform well. As it is always false, the
goal g does not lead to behavioural distinctions. On the other hand, while constructing the
MDP, fltl considers the successive progressions of k g without being able to detect that it
is unreachable until it actually fails to happen. This is exactly what the blindness of blind
minimality amounts to. Figure 15 illustrates the difference in performance as a function of
the number n of propositions involved in the spudd-linear domain, when the reward is of
the form g ∧ n c, with g unreachable.
fltl shines when the trigger is unreachable. Since c never happens, the formula will
always progress to itself, and the goal, however complicated, is never tracked in the generated MDP. In this situation PLTL approaches still consider k c and its subformulae, only
to discover, after expensive preprocessing for pltlmin, after reachability analysis for pltlstr(a), and never for pltlstr, that these are irrelevant. This is illustrated in Figure 16,
again with spudd-linear and a reward of the form g ∧ n c, with c unreachable.
5.7 Dynamic Irrelevance
Earlier we claimed that one advantage of pltlstr and fltl over pltlmin and pltlsim
is that the former perform a dynamic analysis of rewards capable of detecting irrelevance
of variables to particular policies, e.g. to the optimal policy. Our experiments confirm
this claim. However, as for reachability, whether the goal or the triggering condition in
a response formula becomes irrelevant plays an important role in determining whether a
13. Here we sometimes speak of conditions and goals being ‘reachable’ or ‘achievable’ rather than ‘feasible’,
although they may be temporally extended. This is to keep in line with conventional vocabulary as in
the phrase ‘reachability analysis’.

52

Decision-Theoretic Planning with non-Markovian Rewards

Total CPU time (sec)

350
250
150
100
50

2

4

6

8

10

12

14

n
FLTL

PLTLMIN

PLTLSTRUCT

PLTLSTRUCT(A)

Figure 15: Response Formula with Unachievable Goal

Total CPU time (sec)

350
250
150
100
50

1

3

5

7

9

11

n
FLTL

PLTLMIN

PLTLSTRUCT

PLTLSTRUCT(A)

Figure 16: Response Formula with Unachievable Trigger
pltlstr or fltl approach should be taken: pltlstr is able to dynamically ignore the goal,
while fltl is able to dynamically ignore the trigger.
This is illustrated in Figures 17 and 18. In both figures, the domain considered is
on/off with n = 6 propositions, the response formula is g ∧ n c as before, here with both
g and c achievable. This response formula is assigned a fixed reward. To study the effect of
dynamic irrelevance of the goal, in Figure 17, achievement of ¬g is rewarded by the value
r (i.e. we have (¬g : r) in PLTL). In Figure 18, on the other hand, we study the effect of
dynamic irrelevance of the trigger and achievement of ¬c is rewarded by the value r. Both
figures show the runtime of the methods as r increases.
Achieving the goal, resp. the trigger, is made less attractive as r increases up to the
point where the response formula becomes irrelevant under the optimal policy. When this
happens, the run-time of pltlstr resp. fltl, exhibits an abrupt but durable improvement.
The figures show that fltl is able to pick up irrelevance of the trigger, while pltlstr is able
to exploit irrelevance of the goal. As expected, pltlmin whose analysis is static does not pick
53

Thiébaux, Gretton, Slaney, Price & Kabanza

Total CPU time (sec)

200
150
100
50

0

50

100

150

200

250

300

350

r
PLTLMIN

PLTLSTRUCT

FLTL

PLTLSTRUCT (A)

Average CPU time (sec)

Figure 17: Response Formula with Unrewarding Goal
200
150
100
50

0

50

100

150

200

250

300

350

r
PLTLMIN

FLTL

PLTLSTRUCT

PLTLSTRUCT(A)

Figure 18: Response Formula with Unrewarding Trigger
up either and performs consistently badly. Note that in both figures, pltlstr progressively
takes longer to compute as r increases because value iteration requires additional iterations
to converge.
5.8 Summary
In our experiments with artificial domains, we found pltlstr and fltl preferable to statebased PLTL approaches in most cases. If one insists on using the latter, we strongly
recommend preprocessing. fltl is the technique of choice when the reward requires tracking
a long sequence of events or when the desired behaviour is composed of many elements with
identical rewards. For response formulae, we advise the use of pltlstr if the probability of
reaching the goal is low or achieving the goal is very costly, and conversely, we advise the
use of fltl if the probability of reaching the triggering condition is low or if reaching it is
very costly. In all cases, attention should be paid to the syntax of the reward formulae and
54

Decision-Theoretic Planning with non-Markovian Rewards

in particular to minimising its length. Indeed, as could be expected, we found the syntax
of the formulae and the type of non-Markovian reward they encode to be a predominant
factor in determining the difficulty of the problem, much more so than the features of the
Markovian dynamics of the domain.

6. A Concrete Example
Our experiments have so far focused on artificial problems and have aimed at characterising
the strengths and weaknesses of the various approaches. We now look at a concrete example
in order to give a sense of the size of more interesting problems that these techniques can
solve. Our example is derived from the Miconic elevator classical planning benchmark
(Koehler & Schuster, 2000). An elevator must get a number of passengers from their origin
floor to their destination. Initially, the elevator is at some arbitrary floor and no passenger
is served nor has boarded the elevator. In our version of the problem, there is one single
action which causes the elevator to service a given floor, with the effect that the unserved
passengers whose origin is the serviced floor board the elevator, while the boarded passengers
whose destination is the serviced floor unboard and become served. The task is to plan the
elevator movement so that all passengers are eventually served.14
There are two variants of Miconic. In the ‘simple’ variant, a reward is received each
time a passenger becomes served. In the ‘hard’ variant, the elevator also attempts to
provide a range of priority services to passengers with special requirements: many passengers
will prefer travelling in a single direction (either up or down) to their destination, certain
passengers might be offered non-stop travel to their destination, and finally, passengers
with disabilities or young children should be supervised inside the elevator by some other
passenger (the supervisor) assigned to them. Here we omit the VIP and conflicting group
services present in the original hard Miconic problem, as the reward formulae for those do
not create additional difficulties.
Our formulation of the problem makes use of the same propositions as the PDDL description of Miconic used in the 2000 International Planning Competition: dynamic propositions
record the floor the elevator is currently at and whether passengers are served or boarded,
and static propositions record the origin and destination floors of passengers, as well as the
categories (non-stop, direct-travel, supervisor, supervised) the passengers fall in. However,
our formulation differs from the PDDL description in two interesting ways. Firstly, since
we use rewards instead of goals, we are able to find a preferred solution even when all
goals cannot simultaneously be satisfied. Secondly, because priority services are naturally
described in terms of non-Markovian rewards, we are able to use the same action description for both the simple and hard versions, whereas the PDDL description of hard miconic
requires additional actions (up, down) and complex preconditions to monitor the satisfaction of priority service constraints. The reward schemes for Miconic can be encapsulated
through four different types of reward formula.
1. In the simple variant, a reward is received the first time each passenger Pi is served:
14. We have experimented with stochastic variants of Miconic where passengers have some small probability
of desembarking at the wrong floor. However, we find it more useful to present results for the deterministic
version since it is closer to the Miconic deterministic planning benchmark and since, as we have shown
before, rewards have a far more crucial impact than dynamics on the relative performance of the methods.

55

Thiébaux, Gretton, Slaney, Price & Kabanza

PLTL:

ServedPi ∧   ¬ServedPi

$FLTL:

¬ServedPi U (ServedPi ∧ $)

2. Next, a reward is received each time a non-stop passenger Pi is served in one step
after boarding the elevator:
PLTL:

N onStopPi ∧   ¬BoardedPi ∧   ¬ServedPi ∧ ServedPi

$FLTL:

((N onStopPi ∧ ¬BoardedPi ∧ ¬ServedPi ∧ ServedPi ) → $)

3. Then, a reward is received each time a supervised passenger Pi is served while having
been accompanied at all times inside the elevator by his supervisor15 Pj :
PLTL:
$FLTL:

SupervisedPi ∧ SupervisorPj Pi ∧ ServedPi ∧
  ¬ServedPi ∧ (BoardedPi → BoardedPj )
¬ServedPi U ((BoardedPi ∧ SupervisedPi ∧ ¬(BoardedPj ∧ SupervisorPj Pi )∧
¬ServedPi ) ∨ (ServededPi ∧ $))

4. Finally, reward is received each time a direct travel passenger Pi is served while having
travelled only in one direction since boarding, e.g., in the case of going up:
DirectP
W W i ∧ ServedPi ∧ ¬ServedPi ∧
(( j k>j (AtF loork ∧ AtF loorj )) S (BoardedPi ∧ ¬BoardedPi ))
W W
$FLTL: ((DirectPi ∧ BoardedPi ) → (¬ServedPi U ((¬( j k>i AtF loorj ∧ AtF loork )∧
¬ServedPi ) ∨ (servedPi ∧ $))))

PLTL:

and similarly in the case of going down.
Experiments in this section were run on a Dual Pentium4 3.4GHz GNU/Linux 2.6.11
machine with 1GB of ram. We first experimented with the simple variant, giving a reward
of 50 each time a passenger is first served. Figure 19 shows the CPU time taken by the
various approaches to solve random problems with an increasing number n of floors and
passengers, and Figure 20 shows the number of states expanded when doing so. Each data
point corresponds to just one random problem. To be fair with the structured approach, we
ran pltlstr(a) which is able to exploit reachability from the start state. A first observation
is that although pltlstr(a) does best for small values of n, it quickly runs out of memory.
pltlstr(a) and pltlsim both need to track formulae of the form   ¬ServedPi while
pltlsim does not, and we conjecture that this is why they run out of memory earlier. A
second observation is that attempts at PLTL minimisation do not pay very much here.
While pltlmin has reduced memory because it tracks fewer subformulae, the size of the
MDP it produces is identical to the size of the pltlsim MDP and larger than that of the
fltl MDP. This size increase is due to the fact that PLTL approaches label differently
e-states in which the same passengers are served, depending on who has just become served
(for those passengers, the reward formula is true at the e-state). In contrast, our fltl
implementation with progression one step ahead labels all these e-states with the reward
15. To understand the $FLTL formula, observe that we get a reward iff (BoardedPi ∧ SupervisedPi ) →
(BoardedPj ∧SupervisorPj Pi ) holds until ServedPi becomes true, and recall that the formula ¬q U ((¬p∧
¬q) ∨ (q ∧ $)) rewards the holding of p until the occurrence of q.

56

Decision-Theoretic Planning with non-Markovian Rewards

Total CPU time (sec)

7000
4000
2000
1000

2

4

6

8

10

12

14

n
FLTL
PLTLSIM
PLTLMIN
PLTLSTR(A)

Figure 19: Simple Miconic - Run Time

45

State count/(2^n)

40
35
30
25
20
15
10
5
0
2

4

6

8

10

12

14

n
FLTL
PLTLSIM, PLTLMIN

Figure 20: Simple Miconic - Number of Expanded States
formulae relevant to the passengers that still need to be served, the other formulae having
progressed to >. The gain in number of expanded states materialises into run time gains,
resulting in fltl eventually taking the lead.
Our second experiment illustrates the benefits of using an even extremely simple admissible heuristic in conjunction with fltl. Our heuristic is applicable to discounted stochastic
shortest path problems, and discounts rewards by the shortest time in the future in which
they are possible. Here it simply amounts to assigning a fringe state to a value of 50 times
the number of still unserved passengers (discounted once), and results in avoiding floors at
which no passenger is waiting and which are not the destination of a boarded passenger.
Figures 21 and 22 compare the run time and number of states expanded by fltl when used
in conjunction with value iteration (valIt) to when it is used in conjunction with an LAO*
57

Thiébaux, Gretton, Slaney, Price & Kabanza

Total CPU time (sec)

35000
20000
10000
5000

2

4

6

8

10

12

14

n
FLTL−LAO(h)
FLTL−LAO(u)
FLTL−valIt

Figure 21: Effect of a Simple Heuristic on Run Time

State count/(2^n)

50
40
30
20
10
0
2

4

6

8

10

12

14

n
FLTL−LAO(h)
FLTL−valIt,FLTL−LAO(u)

Figure 22: Effect of a Simple Heuristic on the Number of Expanded States
search informed by the above heuristic (LAO(h)). Uninformed LAO* (LAO*(u), i.e. LAO*
with a heuristic of 50 ∗ n at each node) is also included as a reference point to show the
overhead induced by heuristic search. As can be seen from the graphs, the heuristic search
generates significantly fewer states and this eventually pays in terms of run time.
In our final experiment, we considered the hard variant, giving a reward of 50 as before
for service (1), a reward of 2 for non-stop travel (2), a reward of 5 for appropriate supervision
(3), and a reward of 10 for direct travel (2). Regardless of the number n of floors and
passengers, problems only feature a single non-stop traveller, a third of passengers require
supervision, and only half the passengers care about traveling direct. CPU time and number
of states expanded are shown in Figures 23 and 24, respectively. As in the simple case,
pltlsim and pltlstr quickly run out of memory. Formulae of type (2) and (3) create too
many additional variables to track for these approaches, and the problem does not seem
58

Decision-Theoretic Planning with non-Markovian Rewards

Total CPU time (sec)

14000
8000
4000
2000

2

3

4

5

6

7

n
FLTL
PLTLSIM
PLTLMIN
PLTLSTRUCT(A)

Figure 23: Hard Miconic - Run Time

State count/(2^n)

100
80
60
40
20
0
2

3

4

5

6

7

n
FLTL
PLTLSIM
PLTLMIN

Figure 24: Hard Miconic - Number of Expanded States
to exhibit enough structure to help pltlstr. fltl remains the fastest. Here, this does
not seem to be so much due to the size of the generated MDP which is just slightly below
that of the pltlmin MDP, but rather to the overhead incurred by minimisation. Another
observation arising from this experiment is that only very small instances can be handled
in comparison to the classical planning version of the problem solved by state of the art
optimal classical planners. For example, at the 2000 International Planning Competition,
the PropPlan planner (Fourman, 2000) optimally solved instances of hard Miconic with
20 passengers and 40 floors in about 1000 seconds on a much less powerful machine.

59

Thiébaux, Gretton, Slaney, Price & Kabanza

7. nmrdpp in the Probabilistic Planning Competition
We now report on the behaviour of nmrdpp in the probabilistic track of the 4th International Planning Competition (IPC-4). Since the competition did not feature non-Markovian
rewards, our original motivation in taking part was to further compare the solution methods
implemented in nmrdpp in a Markovian setting. This objective largely underestimated the
challenges raised by merely getting a planner ready for a competition, especially when that
competition is the first of its kind. In the end, we decided that successfully preparing nmrdpp to attempt all problems in the competition using one solution method (and possibly
search control knowledge), would be an honorable result.
The most crucial problem we encountered was the translation of PPDDL (Younes &
Littman, 2004), the probabilistic variant of PDDL used as input language for the competition, into nmrdpp’s ADD-based input language. While translating PPDDL into ADDs
is possible in theory, devising a translation which is practical enough for the need of the
competition (small number of variables, small, quickly generated, and easily manipulable
ADDs) is another matter. mtbdd, the translator kindly made available to participants by
the competition organisers, was not always able to achieve the required efficiency. At other
times, the translation was quick but nmrdpp was unable to use the generated ADDs efficiently. Consequently, we implemented a state-based translator on top of the PDDL parser
as a backup, and opted for a state-based solution method since it did not rely on ADDs
and could operate with both translators.
The version of nmrdpp entered in the competition did the following:
1. Attempt to get a translation into ADDs using mtbdd, and if that proves infeasible,
abort it and rely on the state-based translator instead.
2. Run fltl expansion of the state space, taking search control knowledge into account
when available. Break after 10mn if not complete.
3. Run value iteration to convergence. Failing to achieve any useful result (e.g. because
expansion was not complete enough to even reach a goal state), go back to step 2.
4. Run as many of the 30 trials as possible in the remaining time,16 following the generated policy where defined, and falling back on the non-deterministic search control
policy when available.
With Step 1 we were trying to maximise the instances in which the original ADD-based
nmrdpp version could be run intact. In Step 3, it was decided not to use LAO* because
when run with no good heuristic, it often incurs a significant overhead compared to value
iteration.
The problems featured in the competition can be classified into goal-based or rewardbased problems. In goal-based problems, a (positive) reward is only received when a goal
state is reached. In reward-based problems, action performance may also incur a (usually
negative) reward. Another orthogonal distinction can be made between problems from
16. On each given problem, planners had 15mn to run whatever computation they saw as appropriate (including parsing, pre-processing, and policy generation if any), and execute 30 trial runs of the generated
policy from an initial state to a goal state.

60

Decision-Theoretic Planning with non-Markovian Rewards

domains that were not communicated in advance to the participants and those from domains
that were. The latter consisted of variants of blocks world and logistics (or box world)
problems, and gave the participating planners an opportunity to exploit knowledge of the
domain, much as in the hand-coded deterministic planning track.
We decided to enroll nmrdpp in a control-knowledge mode and in a domain-independent
mode. The only difference between the two modes is that the first uses FLTL search
control knowledge written for the known domains as additional input. Our main concern
in writing the control knowledge was to achieve a reasonable compromise between the size
and effectiveness of the formulae. For the blocks world domain, in which the two actions
pickup-from and putdown-to had a 25% chance of dropping the block onto the table, the
control knowledge we used encoded a variant of the well-known GN1 near-optimal strategy
for deterministic blocks world planning (Slaney & Thiébaux, 2001): whenever possible,
try putting a clear block in its goal position, otherwise put an arbitrary clear block on
the table. Because blocks get dropped on the table whenever an action fails, and because
the success probabilities and rewards are identical across actions, optimal policies for the
problem are essentially made up of optimal sequences of actions for the deterministic blocks
world and there was little need for a more sophisticated strategy.17 In the colored blocks
world domain, where several blocks can share the same color and the goal only refers to the
color of the blocks, the control knowledge selected an arbitrary goal state of the non-colored
blocks world consistent with the colored goal specification, and then used the same strategy
as for the non-colored blocks world. The performance of this strategy depends entirely on
the goal-state selected and can therefore be arbitrarily bad.
Logistics problems from IPC-2 distinguish between airports and other locations within
a city; trucks can drive between any two locations in a city and planes can fly between
any two airports. In contrast, the box world only features cities, some of which have an
airport, some of which are only accessible by truck. A priori, the map of the truck and
plane connections is arbitrary. The goal is to get packages from their city of origin to their
city of destination. Moving by truck has a 20% chance of resulting in reaching one of the
three cities closest to the departure city rather than the intended one. The size of the box
world search space turned out to be quite challenging for nmrdpp. Therefore, when writing
search control knowledge, we gave up any optimality consideration and favored maximal
pruning. We were helped by the fact that the box world generator produces problems with
the following structure. Cities are divided into clusters, all of which are composed of at
least one airport city. Furthermore each cluster has at least one hamiltonian circuit which
trucks can follow. The control knowledge we used forced all planes but one, and all trucks
but one in each cluster to be idle. In each cluster, the truck allowed to move could only
attempt driving along the chosen hamiltonian circuit, picking up and dropping parcels as
it went.
The planners participating in the competition are shown in Table 2. Planners E, G2,
J1, and J2 are domain-specific: either they are tuned for blocks and box worlds, or they use
domain-specific search control knowledge, or learn from examples. The other participating
planners are domain-independent.
17. More sophisticated near-optimal strategies for deterministic blocks world exist (see Slaney & Thiébaux,
2001), but are much more complex to encode and might have caused time performance problems.

61

Thiébaux, Gretton, Slaney, Price & Kabanza

Part.
C
E*
G1
G2*
J1*
J2*
J3
P
Q
R

Description
symbolic LAO*
first-order heuristic search in the fluent calculus
nmrdpp without control knowledge
nmrdpp with control knowledge
interpreter of hand written classy policies
learns classy policies from random walks
version of ff replanning upon failure
mgpt: lrtdp with automatically extracted heuristics
ProbaProp: conformant probabilistic planner
structured reachability analysis and structured PI

Reference
(Feng & Hansen, 2002)
(Karabaev & Skvortsova, 2005)
this paper
this paper
(Fern et al., 2004)
(Fern et al., 2004)
(Hoffmann & Nebel, 2001)
(Bonet & Geffner, 2005)
(Onder et al., 2006)
(Teichteil-Königsbuch & Fabiani, 2005)

Table 2: Competition Participants. Domain-specific planners are starred
dom
prob
G2*
J1*
J2*
E*
J3
G1
R
P
C
Q

5
100
100
100
100
100

bw-c-nr
8
11
100 100
100 100
100 100
100 100
100 100

bw-nc-nr
8
100
100
100
100
100

bx-nr
5-10 10-10
100 100
100
100
100
67
100

expl-bw
11

hanoise
5-3

zeno
1-2-3-7

tire-nr
30-4

9
—

—
50
57

—
100
90
100
100
3

23
30
30
53
?
23

100

3

total
600
600
567
400
632
180
177
153
≥ 100
26

Table 3: Results for Goal-Based Problems. Domain-specific planners are starred. Entries
are the percentage of runs in which the goal was reached. A blank indicates that
the planner was unable to attempt the problem. A — indicates that the planner
attempted the problem but was never able to achieve the goal. A ? indicates that
the result is unavailable (due to a bug in the evaluation software, a couple of the
results initially announced were found to be invalid).
dom
prob
J1*
G2*
E*
J2*
J3
P
C
G1
R
Q

5
497
495
496
497
496

bw-c-r
8
11
487 481
486 480
492 486
486 482
487 482

5
494
495
495
495
494
494
495
495
494
180

8
489
490
490
490
490
488

bw-nc-r
11 15 18
21
480 470 462 458
480 468 352 286
480 468
481 —
466 397

— 455
— 459

bx-r
5-10 10-10 10-15
419
317
129
438 376
—
376
425
184

—
346
—

—
279

file
30-4

tire-r
30-4

36
58

—
—
?
—

—

11

total
5183
4846
2459
4229
4475
2087
≥ 495
495
494
191

Table 4: Results for Reward-Based Problems. Domain-specific planners are starred. Entries
are the average reward achieved over the 30 runs. A blank indicates that the
planner was unable to attempt the problem. A — indicates that the planner
attempted the problem but did not achieve a strictly positive reward. A ? indicates
that the result is unavailable.
62

Decision-Theoretic Planning with non-Markovian Rewards

Tables 3 and 4 show the results of the competition, which we extracted from the competition overview paper (Younes, Littman, Weissmann, & Asmuth, 2005) and from the
competition web site http://www.cs.rutgers.edu/~mlittman/topics/ipc04-pt/. The
first of those tables concerns goal-based problems and the second the reward-based problems. The entries in the tables represent the goal-achievement percentage or average reward achieved by the various planner versions (left-column) on the various problems (top
two rows). Planners in the top part of the tables are domain-specific. Problems from the
known domains lie on the left-hand side of the tables. The colored blocks world problems
are bw-c-nr (goal-based version) and bw-c-r (reward version) with 5, 8, and 11 blocks. The
non-colored blocks world problems are bw-nc-nr (goal-based version) with 8 blocks, and bwnc-r (reward-based version) with 5, 8, 11, 15, 18, and 21 blocks. The box world problems
are bx-nr (goal-based) and bx-r (reward-based), with 5 or 10 cities and 10 or 15 boxes. Problems from the unknown domains lie on the right hand side of the tables. They comprise:
expl-bw, an exploding version of the 11 block blocks world problem in which putting down
a block may destroy the object it is put on, zeno, a probabilistic variant of a zeno travel
domain problem from the IPC-3 with 1 plane, 2 persons, 3 cities and 7 fuel levels, hanoise,
a probabilistic variant of the tower of hanoi problem with 5 disks and 3 rods, file, a problem
of putting 30 files in 5 randomly chosen folders, and tire, a variant a the tire world problem
with 30 cities and spare tires at 4 of them, where the tire may go flat while driving.
Our planner nmrdpp in its G1 or G2 version, was able to attempt all problems, achieving a strictly positive reward in all but 4 of them. Not even ff (J3), the competition overall
winner, was able to successfully attempt that many problems. nmrdpp performed particularly well on goal-based problems, achieving the goal in 100% of the runs except in expl-bw,
hanoise, and tire-nr (note that for these three problems, the goal achievement probability of
the optimal policy does not exceed 65%). No other planner outperformed nmrdpp on that
scale. As pointed out before, ff behaves well on the probabilistic version of blocks and box
world because the optimal policies are very close to those for the deterministic problem –
Hoffmann (2002) analyses the reasons why the ff heuristic works well for traditional planning benchmarks such as blocks world and logistics. On the other hand, ff is unable to
solve the unknown problems which have a different structure and require more substantial
probabilistic reasoning, although these problems are easily solved by a number of participating planners. As expected, there is a large discrepancy between the version of nmrdpp
allowed to use search control (G2) and the domain-independent version (G1). While the
latter performs okay with the unknown goal-based domains, it is not able to solve any of
the known ones. In fact, to except for ff, none of the participating domain-independent
planners were able to solve these problems.
In the reward-based case, nmrdpp with control knoweldge behaves well on the known
problems. Only the human-encoded policies (J1) performed better. Without control knowledge nmrdpp is unable to scale on those problems, while other participants such as ff and
mgpt are. Furthermore nmrdpp appears to perform poorly on the two unknown problems.
In both cases, this might be due to the fact that it fails to generate an optimal policy: suboptimal policies easily have a high negative score in these domains (see Younes et al., 2005).
For r-tire, we know that nmrdpp did indeed generate a suboptimal policy. Additionally, it
could be that nmrdpp was unlucky with the sampling-based policy evaluation process: in

63

Thiébaux, Gretton, Slaney, Price & Kabanza

tire-r in particular, there was a high variance between the costs of various trajectories in
the optimal policy.
Alltogether, the competition results suggest that control knowledge is likely to be essential when solving larger problems (Markovian or not) with nmrdpp, and that, as has
been observed with deterministic planners, approaches making use of control knowledge are
quite powerful.

8. Conclusion, Related, and Future Work
In this paper, we have examined the problem of solving decision processes with nonMarkovian rewards. We have described existing approaches which exploit a compact representation of the reward function to automatically translate the NMRDP into an equivalent
process amenable to MDP solution methods. The computational model underlying this
framework can be traced back to work on the relationship between linear temporal logic
and automata in the areas of automated verification and model-checking (Vardi, 2003;
Wolper, 1987). While remaining in this framework, we have proposed a new representation
of non-Markovian reward functions and a translation into MDPs aimed at making the best
possible use of state-based anytime heuristic search as the solution method. Our representation extends future linear temporal logic to express rewards. Our translation has the
effect of embedding model-checking in the solution method. It results in an MDP of the
minimal size achievable without stepping outside the anytime framework, and consequently
in better policies by the deadline. We have described nmrdpp, a software platform that
implements such approaches under a common interface, and which proved a useful tool in
their experimental analysis. Both the system and the analysis are the first of their kind.
We were able to identify a number of general trends in the behaviours of the methods and
to provide advice as to which are the best suited to certain circumstances. For obvious
reasons, our analysis has focused on artificial domains. Additional work should examine a
wider range of domains of more practical interest, to see what form these results take in that
context. Ultimately, we would like our analysis to help nmrdpp automatically select the
most appropriate method. Unfortunately, because of the difficulty of translating between
PLTL and $FLTL, it is likely that nmrdpp would still have to maintain both a PLTL and
a $FLTL version of the reward formulae.
A detailed comparison of our approach to solving NMRDPs with existing methods (Bacchus et al., 1996, 1997) can be found in Sections 3.10 and 5. Two important aspects of future
work would help take the comparison further. One is to settle the question of the appropriateness of our translation to structured solution methods. Symbolic implementations of
the solution methods we consider, e.g. symbolic LAO* (Feng & Hansen, 2002), as well as
formula progression in the context of symbolic state representations (Pistore & Traverso,
2001) could be investigated for that purpose. The other is to take advantage of the greater
expressive power of $FLTL to consider a richer class of decision processes, for instance with
uncertainty as to which rewards are received and when. Many extensions of the language
are possible: adding eventualities, unrestricted negation, first-class reward propositions,
quantitative time, etc. Of course, dealing with them via progression without backtracking
is another matter.

64

Decision-Theoretic Planning with non-Markovian Rewards

We should investigate the precise relationship between our line of work and recent work
on planning for temporally extended goals in non-deterministic domains. Of particular
interest are ‘weak’ temporally extended goals such as those expressible in the Eagle language
(Dal Lago et al., 2002), and temporally extended goals expressible in π-CTL* (Baral &
Zhao, 2004). Eagle enables the expression of attempted reachability and maintenance goals
of the form “try-reach p” and “try-maintain p”, which add to the goals “do-reach p” and
“do-maintain p” already expressible in CTL. The idea is that the generated policy should
make every attempt at satisfying proposition p. Furthermore, Eagle includes recovery goals
of the form “g1 fail g2 ”, meaning that goal g2 must be achieved whenever goal g1 fails, and
cyclic goals of the form “repeat g”, meaning that g should be achieved cyclically until it
fails. The semantics of these goals is given in terms of variants of Büchi tree automata
with preferred transitions. Dal Lago et al. (2002) present a planning algorithm based on
symbolic model-checking which generates policies achieving those goals. Baral and Zhao
(2004) describe π-CTL*, an alternative framework for expressing a subset of Eagle goals
and a variety of others. π-CTL* is a variant of CTL* which allows for formulae involving
two types of path quantifiers: quantifiers tied to the paths feasible under the generated
policy, as is usual, but also quantifiers more generally tied to the paths feasible under any
of the domain actions. Baral and Zhao (2004) do not present any planning algorithm. It
would be very interesting to know whether Eagle and π-CTL* goals can be encoded as nonMarkovian rewards in our framework. An immediate consequence would be that nmrdpp
could be used to plan for them. More generally, we would like to examine the respective
merits of non-deterministic planning for temporally extended goals and decision-theoretic
planning with non-Markovian rewards.
In the pure probabilistic setting (no rewards), recent related research includes work on
planning and controller synthesis for probabilistic temporally extended goals expressible in
probabilistic temporal logics such as CSL or PCTL (Younes & Simmons, 2004; Baier et al.,
2004). These logics enable expressing statements about the probability of the policy satisfying a given temporal goal exceeding a given threshold. For instance, Younes and Simmons
(2004) describe a very general probabilistic planning framework, involving concurrency, continuous time, and temporally extended goals, rich enough to model generalised semi-Markov
processes. The solution algorithms are not directly comparable to those presented here.
Another exciting future work area is the investigation of temporal logic formalisms for
specifying heuristic functions for NMRDPs or more generally for search problems with
temporally extended goals. Good heuristics are important to some of the solution methods
we are targeting, and surely their value ought to depend on history. The methods we have
described could be applicable to the description and processing of such heuristics. Related
to this is the problem of extending search control knowledge to fully operate under the
presence of temporally extended goals, rewards, and stochastic actions. A first issue is
that branching or probabilistic logics such as CTL or PCTL variants should be preferred
to FLTL when describing search control knowledge, because when stochastic actions are
involved, search control often needs to refer to some of the possible futures and even to
their probabilities.18 Another major problem is that the GOALP modality, which is the
key to the specification of reusable search control knowledge is interpreted with respect to
18. We would not argue, on the other hand, that CTL is necessary for representing non-Markovian rewards.

65

Thiébaux, Gretton, Slaney, Price & Kabanza

a fixed reachability goal19 (Bacchus & Kabanza, 2000), and as such, is not applicable to
domains with temporally extended goals, let alone rewards. Kabanza and Thiébaux (2005)
present a first approach to search control in the presence of temporally extended goals in
deterministic domains, but much remains to be done for a system like nmrdpp to be able
to support a meaningful extension of GOALP.
Finally, let us mention that related work in the area of databases uses a similar approach
to pltlstr to extend a database with auxiliary relations containing sufficient information
to check temporal integrity constraints (Chomicki, 1995). The issues are somewhat different
from those raised by NMRDPs: as there is only ever one sequence of databases, what matters
is more the size of these auxiliary relations than avoiding making redundant distinctions.

Acknowledgements
Many thanks to Fahiem Bacchus, Rajeev Goré, Marco Pistore, Ron van der Meyden, Moshe
Vardi, and Lenore Zuck for useful discussions and comments, as well as to the anonymous
reviewers and to David Smith for their thorough reading of the paper and their excellent
suggestions. Sylvie Thiébaux, Charles Gretton, John Slaney, and David Price thank National ICT Australia for its support. NICTA is funded through the Australian Government’s
Backing Australia’s Ability initiative, in part through the Australian Research Council. Froduald Kabanza is supported by the Canadian Natural Sciences and Engineering Research
Council (NSERC).

Appendix A. A Class of Reward-Normal Formulae
The existing decision procedure (Slaney, 2005) for determining whether a formula is rewardnormal is guaranteed to terminate finitely, but involves the construction and comparison of
automata and is rather intricate in practice. It is therefore useful to give a simple syntactic
characterisation of a set of constructors for obtaining reward-normal formulae even though
not all such formulae are so constructible.
We say that a formula is material iff it contains no $ and no temporal operators – that
is, the material formulae are the boolean combinations of atoms.
We consider four operations on behaviours representable by formulae of $FLTL. Firstly,
a behaviour may be delayed for a specified number of timesteps. Secondly, it may be made
conditional on a material trigger. Thirdly, it may be started repeatedly until a material
termination condition is met. Fourthly, two behaviours may be combined to form their
union. These operations are easily realised syntactically by corresponding operations on
formulae. Where m is any material formula:
delay[f ] =

f

cond[m, f ] = m → f
loop[m, f ] = f U m
union[f1 , f2 ] = f1 ∧ f2
19. Where f is an atemporal formula, GOALP(f ) is true iff f is true of all goal states.

66

Decision-Theoretic Planning with non-Markovian Rewards

We have shown (Slaney, 2005) that the set of reward-normal formulae is closed under delay,
cond (for any material m), loop (for any material m) and union, and also that the closure
of {$} under these operations represents a class of behaviours closed under intersection and
concatenation as well as union.
Many familiar reward-normal formulae are obtainable from $ by applying the four operations. For example, (p → $) is loop[⊥, cond[p, $]]. Sometimes a paraphrase is necessary.
For example, ((p∧q) → $) is not of the required form because of the  in the antecedent
of the conditional, but the equivalent (p → (q → $)) is loop[⊥, cond[p, delay[cond[q, $]]]].
Other cases are not so easy. An example is the formula ¬p U (p∧$) which stipulates a reward
the first time p happens and which is not at all of the form suggested. To capture the same
behaviour using the above operations requires a formula like (p → $) ∧ ((p → $) U p).

Appendix B. Proofs of Theorems
Property 1 Where b ⇔ (Γ(i) ∈ B), (Γ, i) |=B f iff (Γ, i + 1) |=B Prog(b, Γi , f ).
Proof:
Induction on the structure of f . There are several base cases, all fairly trivial.
If f = > or f = ⊥ there is nothing to prove, as these progress to themselves and hold
everywhere and nowhere respectively. If f = p then if f holds in Γi then it progresses to >
which holds in Γi+1 while if f does not hold in Γi then it progresses to ⊥ which does not
hold in Γi+1 . The case f = ¬p is similar. In the last base case, f = $. Then the following
are equivalent:
(Γ, i) |=B f
Γ(i) ∈ B
b
Prog(b, Γi , f ) = >
(Γ, i + 1) |=B Prog(b, Γi , f )
Induction case 1: f = g ∧ h. The following are equivalent:
(Γ, i) |=B f
(Γ, i) |=B g and (Γ, i) |=B h
(Γ, i + 1) |=B Prog(b, Γi , g) and (Γ, i + 1) |=B Prog(b, Γi , h) (by induction hypothesis)
(Γ, i + 1) |=B Prog(b, Γi , g) ∧ Prog(b, Γi , h)
(Γ, i + 1) |=B Prog(b, Γi , f )
Induction case 2: f = g ∨ h. Analogous to case 1.
Induction case 3: f = g. Trivial by inspection of the definitions.
Induction case 4: f = g U h. Then f is logically equivalent to h ∨ (g ∧ (g U h) which by
cases 1, 2 and 3 holds at stage i of Γ for behaviour B iff Prog(b, Γi , f ) holds at stage i+1.

Theorem 1 Let f be reward-normal, and let hf0 , f1 , . . .i be the result of progressing it
through the successive states of a sequence Γ. Then, provided no fi is ⊥, for all i Rew(Γi , fi )
iff Γ(i) ∈ Bf .

67

Thiébaux, Gretton, Slaney, Price & Kabanza

Proof: First, by the definition of reward-normality, if f is reward-normal then Γ |=B f iff
for all i, if Γ(i) ∈ Bf then Γ(i) ∈ B. Next, if Γ |=B f then progressing f through Γ according
to B (that is, letting each bi be true iff Γ(i) ∈ B) cannot lead to a contradiction because
by Property 1, progression is truth-preserving.
It remains, then, to show that if Γ 6|=B f then progressing f through Γ according to B
must lead eventually to ⊥. The proof of this is by induction on the structure of f and as
usual the base case in which f is a literal (an atom, a negated atom or >, ⊥ or $) is trivial.
Case f = g ∧ h. Suppose Γ 6|=B f . Then either Γ 6|=B g or Γ 6|=B h, so by the induction
hypothesis either g or h progresses eventually to ⊥, and hence so does their conjunction.
Case f = g ∨ h. Suppose Γ 6|=B f . Then both Γ 6|=B g and Γ 6|=B h, so by the induction
hypothesis each of g and h progresses eventually to ⊥. Suppose without loss of generality
that g does not progress to ⊥ before h does. Then at some point g has progressed to some
formula g 0 and f has progressed to g 0 ∨ ⊥ which simplifies to g 0 . Since g 0 also progresses to
⊥ eventually, so does f .
Case f = g. Suppose Γ 6|=B f . Let Γ = Γ0 ; ∆ and let B 0 = {γ|Γ0 ; γ ∈ B}. Then
∆ 6|=B 0 g, so by the induction hypothesis g progressed through ∆ according to B 0 eventually
reaches ⊥. But The progression of f through Γ according to B is exactly the same after
the first step, so that too leads to ⊥.
Case f = g U h. Suppose Γ 6|=B f . Then there is some j such that (Γ, j) 6|=B g and for all
i ≤ j, (Γ, i) 6|=B h. We proceed by induction on j. In the base case j = 0, and both Γ 6|=B g
and Γ 6|=B h whence by the main induction hypothesis both g and h will eventually progress
to ⊥. Thus h ∨ (g ∧ f 0 ) progresses eventually to ⊥ for any f 0 , and in particular for f 0 = f ,
establishing the base case. For the induction case, suppose Γ |=B g (and of course Γ 6|=B h).
Since f is equivalent to h ∨ (g ∧ f ) and Γ 6|=B f , Γ 6|=B h and Γ |=B g, clearly Γ 6|=B f . Where
∆ and B 0 are as in the previous case, therefore, ∆ 6|=B 0 f and the failure occurs at stage j − 1
of ∆. Therefore the hypothesis of the induction on j applies, and f progressed through ∆
according to B 0 goes eventually to ⊥, and so f progressed through Γ according to B goes
similarly to ⊥.

Theorem 3 Let S 0 be the set of e-states in an equivalent MDP D0 for D = hS, s0 , A, Pr, Ri.
D0 is minimal iff every e-state in S 0 is reachable and S 0 contains no two distinct e-states s01
and s02 with τ (s01 ) = τ (s02 ) and µ(s01 ) = µ(s02 ).
Proof: Proof is by construction of the canonical equivalent MDP Dc . Let the set of
e 0 ) be partitioned into equivalence classes, where
finite prefixes of state sequences in D(s
e 0 ), R(Γ1(i); ∆) =
Γ1(i) ≡ Γ2(j) iff Γ1i = Γ2j and for all ∆ ∈ S ∗ such that Γ1(i); ∆ ∈ D(s
R(Γ2(j); ∆). Let [Γ(i)] denote the equivalence class of Γ(i). Let E be the set of these
equivalence classes. Let A be the function that takes each [Γ(i)] in E to A(Γi ). For each
Γ(i) and ∆(j) and for each a ∈ A([Γ(i)]), let T ([Γ(i)], a, [∆(j)]) be Pr(Γi , a, s) if [∆(j)] =
[Γ(i); hsi]. Otherwise let T ([Γ(i)], a, [∆(j)]) = 0. Let R([Γ(i)]) be R(Γ(i)). Then note the
following four facts:
1. Each of the functions A, T and R is well-defined.
2. Dc = hE, [hs0 i], A, T , Ri is an equivalent MDP for D with τ ([Γ(i)]) = Γi .

68

Decision-Theoretic Planning with non-Markovian Rewards

3. For any equivalent MDP D00 of D there is a mapping from a subset of the states of
D00 onto E.
4. D0 satisfies the condition that every e-state in S 0 is reachable and S 0 contains no two
distinct e-states s01 and s02 with τ (s01 ) = τ (s02 ) and µ(s01 ) = µ(s02 ) iff Dc is isomorphic
to D0 .
What fact 1 above amounts to is that if Γ1(i) ≡ Γ2(j) then it does not matter which of
the two sequences is used to define A, T and R of their equivalence class. In the cases of
A and T this is simply that Γ1i = Γ2j . In the case of R, it is the special case ∆ = hΓ1i i of
the equality of rewards over extensions.
Fact 2 is a matter of checking that the four conditions of Definition 1 hold. Of these,
conditions 1 (τ ([s0 ]) = s0 ) and 2 (A([Γ(i)]) = A(Γi )) hold trivially by the construction.
e 0 ), we have R([Γ(i)]) = R(Γ(i))
Condition 4 says that for any feasible state sequence Γ ∈ D(s
for all i. This also is given in the construction. Condition 3 states:
For all s1 , s2 ∈ S, if there is a ∈ A(s1 ) such that Pr(s1 , a, s2 ) > 0, then for all
e 0 ) such that Γi = s1 , there exists a unique [∆(j)] ∈ E, ∆j = s2 , such
Γ(i) ∈ D(s
that for all a ∈ A([Γ(i)]), T ([Γ(i)], a, [∆[j]]) = Pr(s1 , a, s2 ).
e 0 ) and Γi = s1 . Then the required ∆(j) is Γ(i); hs2 i,
Suppose Pr(s1 , α, s2 ) > 0, Γ(i) ∈ D(s
and of course A([Γ(i)]) = A(Γi ), so the required condition reads:
[Γ(i); hs2 i] is the unique element X of E with τ (X) = s2 such that for all a ∈
A(Γi ), T ([Γ(i)], a, X) = Pr(s1 , a, s2 ).
To establish existence, we need that if a ∈ A(Γi ) then T ([Γ(i)], a, [Γ(i); hs2 i]) = Pr(Γi , a, s2 ),
which is immediate from the definition of T above. To establish uniqueness, suppose that
τ (X) = s2 and T ([Γ(i)], a, X) = Pr(s1 , a, s2 ) for all actions a ∈ A(Γi ). Since Pr(s1 , α, s2 ) >
0, the transition probability from [Γ(i)] to X is nonzero for some action, so by the definition
of T , X can only be [Γ(i); hs2 i].
Fact 3 is readily observed. Let M be any equivalent MDP for D. For any states s1
and s2 of D, and any state X of M such that τ (X) = s1 there is at most one state Y
of M with τ (Y ) = s2 such that some action a ∈ A(s1 ) gives a nonzero probability of
transition from X to Y . This follows from the uniqueness part of condition 3 of Definition 1
together with the fact that the transition function is a probability distribution (sums to 1).
Therefore for any given finite state sequence Γ(i) there is at most one state of M reached
from the start state of M by following Γ(i). Therefore M induces an equivalence relation
≈M on S ∗ : Γ(i) ≈M ∆(j) iff they lead to the same state of M (the sequences which are not
feasible in M may all be regarded as equivalent under ≈M ). Each reachable state of M has
associated with it a nonempty equivalence class of finite sequences of states of D. Working
through the definitions, we may observe that ≈M is a sub-relation of ≡ (if Γ(i) ≈M ∆(j)
then Γ(i) ≡ ∆(j)). Hence the function that takes the equivalence class under ≈M of each
feasible sequence Γ(i) to [Γ(i)] induces a mapping h (an epimorphism in fact) from the
reachable subset of states of M onto E.
To establish Fact 4, it must be shown that in the case of D0 the mapping can be
reversed, or that each equivalence class [Γ(i)] in Dc corresponds to exactly one element of
69

Thiébaux, Gretton, Slaney, Price & Kabanza

e 0)
D0 . Suppose not (for contradiction). Then there exist sequences Γ1(i) and Γ2(j) in D(s
0
such that Γ1(i) ≡ Γ2(j) but on following the two sequences from s0 we arrive at two different
elements s01 and s02 of D0 with τ (s01 ) = Γ1i = Γ2j = τ (s02 ) but with µ(s01 ) 6= µ(s02 ). Therefore
e
there exists a sequence ∆(k) ∈ D(s)
such that R(Γ1(i − 1); ∆(k)) 6= R(Γ2(j − 1); ∆(k)).
But this contradicts the condition for Γ1(i) ≡ Γ2(j).

Theorem 3 follows immediately from facts 1–4.
Theorem 4 Let D0 be the translation of D as in Definition 5. D0 is a blind minimal
equivalent MDP for D.
Proof: Reachability of all the e-states is obvious, as they are constructed only when
reached. Each e-state is a pair hs, φi where s is a state of D and φ is a reward function
specification. In fact, s = τ (hs, φi) and φ determines a distribution of rewards over all
continuations of the sequences
that reach hs, φi. That is, for all ∆ in S ∗ such that ∆0 = s,
P
the reward for ∆ is (f :r)∈φ {r | ∆ ∈ Bf }. If D0 is not blind minimal, then there exist
distinct e-states hs, φi and hs, φ0 i for which this sum is the same for all ∆. But this makes
φ and φ0 semantically equivalent, contradicting the supposition that they are distinct.


Appendix C. Random Problem Domains
Random problem domains are produced by first creating a random action specification
defining the domain dynamics. Some of the experiments we conducted20 also involved
producing, in a second step, a random reward specification that had desired properties in
relation to the generated dynamics.
The random generation of the domain dynamics takes as parameters the number n
of propositions in the domain and the number of actions to be produced, and starts by
assigning some effects to each action such that each proposition is affected by exactly one
action. For example, if we have 5 actions and 14 propositions, the first 4 actions may affect
3 propositions each, the 5th one only 2, and the affected propositions are all different. Once
each action has some initial effects, we continue to add more effects one at a time, until a
sufficient proportion of the state space is reachable – see “proportion reachable” parameter
below. Each additional effect is generated by picking up a random action and a random
proposition, and producing a random decision diagram according to the “uncertainty” and
“structure” parameters below:
The Uncertainty parameter is the probability of a non zero/one value as a leaf node. An
uncertainty of 1 will result in all leaf nodes having random values from a uniform
distribution. An uncertainty of 0 will result in all leaf nodes having values 0 or 1 with
an equal probability.
The Structure (or influence) parameter is the probability of a decision diagram containing
a particular proposition. So an influence of 1 will result in all decision diagrams
20. None of those are included in this paper, however.

70

Decision-Theoretic Planning with non-Markovian Rewards

including all propositions (and very unlikely to have significant structure), while 0
will result in decision diagrams that do not depend on the values of propositions.
The Proportion Reachable parameter is a lower bound on the proportion of the entire 2n
state space that is reachable from the start state. The algorithm adds behaviour until
this lower bound is reached. A value of 1 will result in the algorithm running until
the actions are sufficient to allow the entire state space to be reachable.
A reward specification can be produced with regard to the generated dynamics such that
a specified number of the rewards are reachable and a specified number are unreachable.
First, a decision diagram is produced to represent which states are reachable and which
are not, given the domain dynamics. Next, a random path is taken from the root of this
decision diagram to a true terminal if we are generating an attainable reward, or a false
terminal if we are producing an unattainable reward. The propositions encountered on this
path, both negated and not, form a conjunction that is the reward formula. This process
is repeated until the desired number of reachable and unreachable rewards are obtained.

References
AT&T Labs-Research (2000). Graphviz. Available from http://www.research.att.com/
sw/tools/graphviz/.
Bacchus, F., Boutilier, C., & Grove, A. (1996). Rewarding behaviors. In Proc. American
National Conference on Artificial Intelligence (AAAI), pp. 1160–1167.
Bacchus, F., Boutilier, C., & Grove, A. (1997). Structured solution methods for nonMarkovian decision processes. In Proc. American National Conference on Artificial
Intelligence (AAAI), pp. 112–117.
Bacchus, F., & Kabanza, F. (1998). Planning for temporally extended goals. Annals of
Mathematics and Artificial Intelligence, 22, 5–27.
Bacchus, F., & Kabanza, F. (2000). Using temporal logic to express search control knowledge
for planning. Artificial Intelligence, 116 (1-2).
Baier, C., Größer, M., Leucker, M., Bollig, B., & Ciesinski, F. (2004). Controller synthesis
for probabilistic systems (extended abstract). In Proc. IFIP International Conference
on Theoretical Computer Science (IFIP TCS).
Baral, C., & Zhao, J. (2004). Goal specification in presence of nondeterministic actions. In
Proc. European Conference on Artificial Intelligence (ECAI), pp. 273–277.
Barto, A., Bardtke, S., & Singh, S. (1995). Learning to act using real-time dynamic programming. Artificial Intelligence, 72, 81–138.
Bonet, B., & Geffner, H. (2003). Labeled RTDP: Improving the convergence of real-time
dynamic programming. In Proc. International Conference on Automated Planning
and Scheduling (ICAPS), pp. 12–21.

71

Thiébaux, Gretton, Slaney, Price & Kabanza

Bonet, B., & Geffner, H. (2005). mGPT: A probabilistic planner based on heuristic search.
Journal of Artificial Intelligence Research, 24, 933–944.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions and computational leverage. In Journal of Artificial Intelligence Research,
Vol. 11, pp. 1–94.
Boutilier, C., Dearden, R., & Goldszmidt, M. (2000). Stochastic dynamic programming
with factored representations. Artificial Intelligence, 121 (1-2), 49–107.
Calvanese, D., De Giacomo, G., & Vardi, M. (2002). Reasoning about actions and planning in LTL action theories. In Proc. International Conference on the Principles of
Knowledge Representation and Reasoning (KR), pp. 493–602.
Cesta, A., Bahadori, S., G, C., Grisetti, G., Giuliani, M., Loochi, L., Leone, G., Nardi, D.,
Oddi, A., Pecora, F., Rasconi, R., Saggase, A., & Scopelliti, M. (2003). The RoboCare
project. Cognitive systems for the care of the elderly. In Proc. International Conference
on Aging, Disability and Independence (ICADI).
Chomicki, J. (1995). Efficient checking of temporal integrity constraints using bounded
history encoding. ACM Transactions on Database Systems, 20 (2), 149–186.
Dal Lago, U., Pistore, M., & Traverso, P. (2002). Planning with a language for extended
goals. In Proc. American National Conference on Artificial Intelligence (AAAI), pp.
447–454.
Dean, T., Kaelbling, L., Kirman, J., & Nicholson, A. (1995). Planning under time constraints in stochastic domains. Artificial Intelligence, 76, 35–74.
Dean, T., & Kanazawa, K. (1989). A model for reasoning about persistance and causation.
Computational Intelligence, 5, 142–150.
Drummond, M. (1989). Situated control rules. In Proc. International Conference on the
Principles of Knowledge Representation and Reasoning (KR), pp. 103–113.
Emerson, E. A. (1990). Temporal and modal logic. In Handbook of Theoretical Computer
Science, Vol. B, pp. 997–1072. Elsevier and MIT Press.
Feng, Z., & Hansen, E. (2002). Symbolic LAO∗ search for factored Markov decision processes. In Proc. American National Conference on Artificial Intelligence (AAAI), pp.
455–460.
Feng, Z., Hansen, E., & Zilberstein, S. (2003). Symbolic generalization for on-line planning.
In Proc. Conference on Uncertainty in Artificial Intelligence (UAI), pp. 209–216.
Fern, A., Yoon, S., & Givan, R. (2004). Learning domain-specific knowledge from random
walks. In Proc. International Conference on Automated Planning and Scheduling
(ICAPS), pp. 191–198.
Fourman, M. (2000). Propositional planning. In Proc. AIPS Workshop on Model-Theoretic
Approaches to Planning, pp. 10–17.
72

Decision-Theoretic Planning with non-Markovian Rewards

Gretton, C., Price, D., & Thiébaux, S. (2003a). Implementation and comparison of solution
methods for decision processes with non-Markovian rewards. In Proc. Conference on
Uncertainty in Artificial Intelligence (UAI), pp. 289–296.
Gretton, C., Price, D., & Thiébaux, S. (2003b). NMRDPP: a system for decision-theoretic
planning with non-Markovian rewards. In Proc. ICAPS Workshop on Planning under
Uncertainty and Incomplete Information, pp. 48–56.
Haddawy, P., & Hanks, S. (1992). Representations for decision-theoretic planning: Utility
functions and deadline goals. In Proc. International Conference on the Principles of
Knowledge Representation and Reasoning (KR), pp. 71–82.
Hansen, E., & Zilberstein, S. (2001). LAO∗ : A heuristic search algorithm that finds solutions
with loops. Artificial Intelligence, 129, 35–62.
Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (1999). SPUDD: stochastic planning using
decision diagrams. In Proc. Conference on Uncertainty in Artificial Intelligence (UAI),
pp. 279–288.
Hoffmann, J. (2002). Local search topology in planning benchmarks: A theoretical analysis.
In Proc. International Conference on AI Planning and Scheduling (AIPS), pp. 92–100.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through
heuristic search. Journal of Artificial Intelligence Research, 14, 253–302.
Howard, R. (1960). Dynamic Programming and Markov Processes. MIT Press, Cambridge,
MA.
Kabanza, F., & Thiébaux, S. (2005). Search control in planning for temporally extended
goals. In Proc. International Conference on Automated Planning and Scheduling
(ICAPS), pp. 130–139.
Karabaev, E., & Skvortsova, O. (2005). A Heuristic Search Algorithm for Solving FirstOrder MDPs. In Proc. Conference on Uncertainty in Artificial Intelligence (UAI),
pp. 292–299.
Koehler, J., & Schuster, K. (2000). Elevator control as a planning problem. In Proc.
International Conference on AI Planning and Scheduling (AIPS), pp. 331–338.
Korf, R. (1990). Real-time heuristic search. Artificial Intelligence, 42, 189–211.
Kushmerick, N., Hanks, S., & Weld, D. (1995). An algorithm for probabilistic planning.
Artificial Intelligence, 76, 239–286.
Lichtenstein, O., Pnueli, A., & Zuck, L. (1985). The glory of the past. In Proc. Conference
on Logics of Programs, pp. 196–218. LNCS, volume 193.
Onder, N., Whelan, G. C., & Li, L. (2006). Engineering a conformant probabilistic planner.
Journal of Artificial Intelligence Research, 25, 1–15.

73

Thiébaux, Gretton, Slaney, Price & Kabanza

Pistore, M., & Traverso, P. (2001). Planning as model-checking for extended goals in
non-deterministic domains. In Proc. International Joint Conference on Artificial Intelligence (IJCAI-01), pp. 479–484.
Slaney, J. (2005). Semi-positive LTL with an uninterpreted past operator. Logic Journal of
the IGPL, 13, 211–229.
Slaney, J., & Thiébaux, S. (2001). Blocks world revisited. Artificial Intelligence, 125,
119–153.
Somenzi, F. (2001).
CUDD: CU Decision Diagram Package.
ftp://vlsi.colorado.edu/pub/.

Available from

Teichteil-Königsbuch, F., & Fabiani, P. (2005). Symbolic heuristic policy iteration algorithms for structured decision-theoretic exploration problems. In Proc. ICAPS workshop on Planning under Uncertainty for Autonomous Systems.
Thiébaux, S., Hertzberg, J., Shoaff, W., & Schneider, M. (1995). A stochastic model of
actions and plans for anytime planning under uncertainty. International Journal of
Intelligent Systems, 10 (2), 155–183.
Thiébaux, S., Kabanza, F., & Slaney, J. (2002a). Anytime state-based solution methods for
decision processes with non-Markovian rewards. In Proc. Conference on Uncertainty
in Artificial Intelligence (UAI), pp. 501–510.
Thiébaux, S., Kabanza, F., & Slaney, J. (2002b). A model-checking approach to decisiontheoretic planning with non-Markovian rewards. In Proc. ECAI Workshop on ModelChecking in Artificial Intelligence (MoChArt-02), pp. 101–108.
Vardi, M. (2003). Automated verification = graph, logic, and automata. In Proc. International Joint Conference on Artificial Intelligence (IJCAI), pp. 603–606. Invited
paper.
Wolper, P. (1987). On the relation of programs and computations to models of temporal
logic. In Proc. Temporal Logic in Specification, LNCS 398, pp. 75–123.
Younes, H. L. S., & Littman, M. (2004). PPDDL1.0: An extension to PDDL for expressing
planning domains with probabilistic effects. Tech. rep. CMU-CS-04-167, School of
Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania.
Younes, H. L. S., Littman, M., Weissmann, D., & Asmuth, J. (2005). The first probabilistic
track of the International Planning Competition. In Journal of Artificial Intelligence
Research, Vol. 24, pp. 851–887.
Younes, H., & Simmons, R. G. (2004). Policy generation for continuous-time stochastic
domains with concurrency. In Proc. International Conference on Automated Planning
and Scheduling (ICAPS), pp. 325–333.

74

