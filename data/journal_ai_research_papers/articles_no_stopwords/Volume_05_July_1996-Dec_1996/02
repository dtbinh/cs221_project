Journal Artificial Intelligence Research 5 (1996) 53-94

Submitted 3/95; published 9/96

Cue Phrase Classification Using Machine Learning
Diane J. Litman

AT&T Labs - Research, 600 Mountain Avenue
Murray Hill, NJ 07974 USA

diane@research.att.com

Abstract

Cue phrases may used discourse sense explicitly signal discourse structure,
sentential sense convey semantic rather structural information. Correctly
classifying cue phrases discourse sentential critical natural language processing
systems exploit discourse structure, e.g., performing tasks anaphora resolution plan recognition. paper explores use machine learning classifying
cue phrases discourse sentential. Two machine learning programs (cgrendel
C4.5) used induce classification models sets pre-classified cue phrases
features text speech. Machine learning shown effective technique
automating generation classification models, improving
upon previous results. compared manually derived classification models already
literature, learned models often perform higher accuracy contain new
linguistic insights data. addition, ability automatically construct classification models makes easier comparatively analyze utility alternative feature
representations data. Finally, ease retraining makes learning approach
scalable exible manual methods.

1. Introduction

Cue phrases words phrases may sometimes used explicitly signal discourse
structure text speech. particular, used discourse sense, cue
phrase explicitly conveys structural information. used sentential sense, cue
phrase instead conveys semantic rather structural information. following examples
(taken spoken language corpus described Section 2) illustrate sample
discourse sentential usages cue phrases \say" \further":
Discourse
\: : : might concept say researcher worked fifteen years
certain project : : : "
\Further, crucial AI probably expert databases well : : : "
Sentential
\: : : let say bears strong resemblance much work that's
done semantic nets even frames."
\: : : place even stranger away : : : "
example, used discourse sense, cue phrase \say" conveys structural
information example beginning. used sentential sense, \say"
convey structural information instead functions verb.
c 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiLitman

ability correctly classify cue phrases discourse sentential critical
natural language processing systems need recognize convey discourse structure,
tasks improving anaphora resolution (Grosz & Sidner, 1986; Reichman, 1985).
Consider following example, taken corpus described
Section 21 :
system attempts hold rules, say expert database expert system,
expect hold rules fact apply us
appropriate situations.
example, cue phrases \say" \then" discourse usages, explicitly
signal boundaries intervening subtopic discourse structure. Furthermore,
referents noun phrases \the system," \an expert database," \an expert
system" possible referents pronoun \it." structural information
conveyed cue phrases, system determine \the system" relevant
interpreting pronoun \it," \an expert database" \an expert system"
occur within embedded (and concluded) subtopic. Without cue phrases,
reasoning required determine referent \the system" intended referent
\it" would much complex.
Correctly classifying cue phrases discourse sentential important natural
language processing tasks well. discourse/sentential distinction used
improve naturalness synthetic speech text-to-speech systems (Hirschberg, 1990).
Text-to-speech systems generate synthesized speech unrestricted text. cue phrase
classified discourse sentential using features input text,
synthesized using different intonational models discourse sentential usages.
addition, explicitly identifying rhetorical relationships, discourse usages cue
phrases used improve coherence multisentential texts natural language
generation systems (Zuckerman & Pearl, 1986; Moser & Moore, 1995). Cue phrases
used reduce complexity discourse processing areas argument
understanding (Cohen, 1984) plan recognition (Litman & Allen, 1987; Grosz & Sidner,
1986).
problem cue phrase classification often noted (Grosz & Sidner,
1986), recently, models classifying cue phrases neither developed evaluated
based careful empirical analyses. Even though literature suggests features
might useful cue phrase classification, quantitative analyses actual
classification algorithms use features (nor suggestions different types
features might combined). systems recognize generate cue phrases simply
assume discourse uses utterance clause initial (Reichman, 1985; Zuckerman &
Pearl, 1986). empirical studies showing intonational prominence
certain word classes varies respect discourse function (Halliday & Hassan, 1976;
Altenberg, 1987), studies investigate cue phrases per se.
address limitations, Hirschberg Litman (1993) conducted several empirical
studies specifically addressing cue phrase classification text speech. Hirschberg
Litman pre-classified set naturally occurring cue phrases, described cue phrase
terms prosodic textual features (the features posited literature easy
1. example described detail Hirschberg Litman (1993).

54

fiCue Phrase Classification Using Machine Learning

automatically code), manually examined data construct classification models
best predicted classifications feature values.
paper examines utility machine learning automating construction
models classifying cue phrases empirical data. set experiments
described use two machine learning programs, cgrendel (Cohen, 1992, 1993)
C4.5 (Quinlan, 1993), induce classification models sets pre-classified cue phrases
features. features, classes training examples used studies
Hirschberg Litman (1993), well additional features, classes training examples, given input machine learning programs. results evaluated
quantitatively qualitatively, comparing error rates content
manually derived learned classification models. experimental results show
machine learning indeed effective technique, automating generation
classification models, improving upon previous results. accuracy
learned classification models often higher accuracy manually derived
models, learned models often contain new linguistic implications. learning
paradigm makes easier compare utility different knowledge sources,
update model given new features, classes, training data.
next section summarizes previous work cue phrase classification. Section 3
describes machine learning approach cue phrase classification taken
paper. particular, section describes four sets experiments use machine
learning automatically induce cue phrase classification models. types inputs
outputs machine learning programs presented, methodologies
used evaluate results. Section 4 presents discusses experimental results,
highlights many benefits machine learning approach. Section 5 discusses
practical utility results paper. Finally, Section 6 discusses use machine
learning studies discourse, Section 7 concludes.

2. Previous Work Classifying Cue Phrases
section summarizes Hirschberg's Litman's empirical studies classification
cue phrases speech text (Hirschberg & Litman, 1987, 1993; Litman & Hirschberg,
1990). Hirschberg's Litman's data (cue phrases taken corpora recorded
transcribed speech, classified discourse sentential, coded using speech-based
text-based features) used create input machine learning experiments. Hirschberg's Litman's results (performance figures manually developed cue
phrase classification models) used benchmark evaluating performance
classification models produced machine learning.
first study Hirschberg Litman investigated usage cue phrase \now"
multiple speakers radio call-in show (Hirschberg & Litman, 1987). classification
model based prosodic features developed based manual analysis \training"
set 48 examples \now", evaluated previously unseen test set 52 examples
\now". follow-up study (Hirschberg & Litman, 1993), Hirschberg Litman tested
classification model larger set cue phrases, namely single word cue phrases
technical keynote address single speaker. corpus yielded 953 instances 34
55

fiLitman

Prosodic Model:

composition intermediate phrase = alone
elseif composition intermediate phrase = :alone
position intermediate phrase = first
accent = deaccented
elseif accent = L*
elseif accent = H*
elseif accent = complex
elseif position intermediate phrase = :first

discourse

discourse

discourse

sentential

sentential

sentential

(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)

Textual Model:

preceding orthography = true
elseif preceding orthography = false

(9)
(10)

discourse
sentential

Figure 1: Decision tree representation manually derived classification models
Hirschberg Litman.
different single word cue phrases derived literature.2 Hirschberg Litman
used cue phrases first 17 minutes corpus develop complementary cue
phrase classification model based textual features (Litman & Hirschberg, 1990),
tested full corpus (Hirschberg & Litman, 1993). first study
referred \now" study, follow-up study \multiple cue phrase" study.
Note term \multiple" means 34 different single word cue phrases (as opposed
cue phrase \now") considered, cue phrases consisting multiple
words (e.g. \by way") considered.
method Hirschberg Litman used develop prosodic textual classification models follows. first separately classified example cue phrase
data discourse, sentential ambiguous listening recording reading
transcription.3 example described set prosodic textual features.4
Previous observations literature correlating discourse structure prosodic information, discourse usages cue phrases initial position clause, contributed
choice features. set classified described examples examined
order manually develop classification models shown Figure 1. models
shown using decision trees ease comparison results C4.5
explained below.
Prosody described using Pierrehumbert's theory English intonation (Pierrehumbert, 1980). Pierrehumbert's theory, intonational contours described sequences
low (L) high (H) tones fundamental frequency (F0) contour (the physical
2. Figure 2 contains list 34 cue phrases. Hirschberg Litman (1993) provide full details regarding
distribution cue phrases. frequent cue phrase \and", occurs 320 times.
next frequent cue phrase \now", occurs 69 times. \But," \like," \or" \so"
occur fifty times. four least frequent cue phrases { \essentially," \otherwise," \since"
\therefore" { occur 2 times.
3. class ambiguous introduced multiple cue phrase study (Hirschberg & Litman, 1993;
Litman & Hirschberg, 1990).
4. Although limited set textual features noted \now" data, analysis \now" data
yield textual classification model.

56

fiCue Phrase Classification Using Machine Learning

correlate pitch). Intonational contours domain intonational phrase.
finite-state grammar describes set tonal sequences intonational phrase.
well-formed intonational phrase consists one intermediate phrases followed
boundary tone. well-formed intermediate phrase one pitch accents followed
phrase accent. Boundary tones phrase accents consist single tone,
pitch accents consist either single tone pair tones. two simple pitch
accents (H* L*) four complex accents (L*+H, L+H*, H*+L, H+L*).
* indicates tone aligned stressed syllable associated lexical item.
Note every stressed syllable accented. Lexical items bear pitch accents
called accented, called deaccented.
Prosody manually transcribed Hirschberg examining fundamental frequency (F0) contour, listening recording. transcription process
performed separately process discourse/sentential classification. produce
F0 contour, recording corpus digitized pitch-tracked using speech analysis software. resulted display F0 x-axis represented time
y-axis represented frequency Hz. Various phrase final characteristics (e.g., phrase accents,
boundary tones, well pauses syllable lengthening) helped identify intermediate
intonational phrases, peaks valleys display F0 contour helped
identify pitch accents. Similar manual transcriptions prosodic phrasing accent
shown reliable across coders (Pitrelli, Beckman, & Hirschberg, 1994).
prosody coded, Hirschberg Litman represented every cue phrase terms
following prosodic features.5 Accent corresponded pitch accent (if any)
associated cue phrase. intonational intermediate phrases
containing cue phrase, feature composition phrase represented whether
cue phrase alone phrase (the phrase contained cue phrase,
cue phrase cue phrases). Position phrase represented whether cue
phrase first (the first lexical item prosodic phrase unit { possibly preceded
cue phrases) not.
textual features used multiple cue phrase study (Hirschberg & Litman, 1993;
Litman & Hirschberg, 1990) extracted automatically transcript. part
speech cue phrase obtained running program tagging words one
approximately 80 parts speech (Church, 1988) transcript.6 Several characteristics
cue phrase's immediate context noted, particular, whether immediately preceded succeeded orthography (punctuation paragraph boundary),
whether immediately preceded succeeded lexical item corresponding
another cue phrase.
background, classification models shown Figure 1 explained.
prosodic model uniquely classifies cue phrase using features composition
intermediate phrase, position intermediate phrase, accent. cue phrase
uttered single intermediate phrase { possibly cue phrases (i.e., line (1)
Figure 1), larger intermediate phrase initial position (possibly preceded
5. features used Figure 1 discussed here.
6. Another syntactic feature - dominating constituent - obtained running parser Fidditch (Hindle,
1989) transcript. However, since feature appear models manually derived
training data (Litman & Hirschberg, 1990), feature pursued.

57

fiLitman

Model
Classifiable Cue Phrases (N=878) Classifiable Non-Conjuncts (N=495)
Prosodic
24.6 3.0
14.7 3.2
Textual
19.9 2.8
16.1 3.4
Default Class
38.8 3.2
40.8 4.4

Table 1: 95% confidence intervals error rates (%) manually derived classification models Hirschberg Litman, testing data (multiple cue phrase corpus).
cue phrases) L* accent deaccented, classified discourse. part
larger intermediate phrase either initial position H* complex accent,
non-initial position, sentential. textual model classifies cue phrases using
single feature preceding orthography.7 cue phrase preceded type
orthography, classified discourse; otherwise, cue phrase classified sentential.
prosodic model used classify cue phrase training data, i.e.,
100 examples \now" model developed, error rate 2.0%.8
error rate textual model training examples multiple cue phrase
corpus 10.6% (Litman & Hirschberg, 1990).
prosodic textual models evaluated quantifying performance
correctly classifying example cue phrases two test sets data, shown rows
labeled \Prosodic" \Textual" Table 1. test set subset 953 examples
multiple cue phrase corpus. first test set (878 examples) consists
classifiable cue phrases, i.e., cue phrases Hirschberg Litman classified
discourse classified sentential. Note cue phrases Hirschberg
Litman classified ambiguous unable agree upon included
classifiable subset. (These cue phrases considered learning experiments
described Section 4.4, however.) second test set, classifiable non-conjuncts
(495 examples), created classifiable cue phrases removing instances
\and", \or" \but". subset considered particularly reliable since 97.2% nonconjuncts classifiable compared 92.1% example cue phrases. error rate
prosodic model 24.6% classifiable cue phrases 14.7% classifiable
non-conjuncts (Hirschberg & Litman, 1993). error rate textual model 19.9%
classifiable cue phrases 16.1% classifiable non-conjuncts (Hirschberg &
Litman, 1993). last row table shows error rates simple \Default Class"
baseline model always predicts frequent class corpus (sentential).
rates 38.8% classifiable cue phrases 40.8% classifiable non-conjuncts.
7. classification model based part-of-speech developed (Litman & Hirschberg, 1990;
Hirschberg & Litman, 1993); however, perform well model based orthography
(the error rate part-of-speech model 36.1% larger test set, opposed 19.9%
orthographic model). Furthermore, model combined orthography part-of-speech performed
comparably simpler orthographic model (Hirschberg & Litman, 1993). Hirschberg Litman
preliminary observations suggesting adjacency cue phrases might prove useful.
8. Following Hirschberg Litman (1993), original 48- 52-example sets (Hirschberg & Litman,
1987) combined.

58

fiCue Phrase Classification Using Machine Learning

Although computed Hirschberg Litman, Table 1 associates margins errors error percentage, used compute confidence intervals (Freedman,
Pisani, & Purves, 1978). (The margin error 2 standard errors 95% confidence
interval using normal table.) lower bound confidence interval computed
subtracting margin error error rate, upper bound computed
adding margin error. Thus, 95% confidence interval prosodic model
classifiable cue phrase test set (21.6%, 27.6%). Analysis confidence intervals
indicates improvement prosodic textual models default
model significant. example, upper bounds error rates prosodic
textual models classifiable cue phrase test set - 27.6% 22.7% - lower
lower bound default class error rate - 35.6%. methodology using statistical inference determine whether differences error rates significant discussed
fully Section 3.3.

3. Experiments using Machine Learning

section describes experiments use machine learning programs C4.5 (Quinlan,
1993) cgrendel (Cohen, 1992, 1993) automatically induce cue phrase classification
models. cgrendel C4.5 similar learning methods
neural networks cart (Brieman, Friedman, Olshen, & Stone, 1984) induce
classification models preclassified examples. program takes following inputs:
names classes learned, names possible values fixed set features,
training data (i.e., set examples class feature values specified).
output program classification model, expressed C4.5 decision tree
cgrendel ordered set if-then rules. cgrendel C4.5 learn
classification models using greedy search guided \information gain" metric.
first group machine learning experiments replicate training testing conditions used Hirschberg Litman (1993) (reviewed previous section), support
direct comparison manual machine learning approaches. second group
experiments evaluate utility training larger amounts data feasible
manual analysis Hirschberg Litman. third set experiments allow
machine learning algorithms distinguish among 34 cue phrases, evaluate utility developing classification models specialized particular cue phrases. fourth set
experiments consider examples multiple cue phrase corpus,
classifiable cue phrases. set experiments attempt predict third classification
unknown, well classifications discourse sentential. Finally, within
four sets experiments, individual experiment learns classification model using
different feature representation training data. experiments consider features
isolation, comparatively evaluate utility individual feature classification.
experiments consider linguistically motivated sets features, gain insight
feature interactions.

3.1 Machine Learning Inputs

section describes inputs machine learning programs, namely,
names classifications learned, names possible values fixed set
59

fiLitman

Classification
Judge1/Judge2
Cue Phrases
Non-Conjuncts

Total
953
509

Classifiable Cue Phrases
Discourse Sentential
D/D
S/S
341
537
202
293

Unknown
?/? D/S S/D D/? S/? ?/D ?/S
59
5
0
0
0
5
6
11
1
0
0
0
0
2

Table 2: Determining classification cue phrases.
features, training data specifying class feature values example
training set.
3.1.1 Classifications

first input learning program specifies names fixed set classifications.
Hirschberg Litman's 3-way classification cue phrases 2 judges (Hirschberg &
Litman, 1993) transformed classifications used machine learning programs
shown Table 2. Recall Section 2 judge classified cue phrase
discourse, sentential, ambiguous; classifications shown D, S, ? Table 2.
discussed Section 2, classifiable cue phrases cue phrases judges
classified either discourse sentential usages. Thus, machine learning
experiments, cue phrase assigned classification discourse judges classified
discourse (D/D, shown column 3 Table 2). Similarly, cue phrase assigned
classification sentential judges classified sentential (S/S, shown column
4). 878 (92.1%) 953 examples full corpus classifiable, 495 (97.2%)
509 non-conjuncts classifiable.
machine learning experiments, third cue phrase classification
considered. particular, cue phrase assigned classification unknown
Hirschberg Litman classified ambiguous (?/?, shown column 5),
unable agree upon classification (D/S, S/D, D/?, S/?, ?/D, ?/S, shown
columns 6-11). full corpus, 59 cue phrases (6.2%) judged ambiguous
judges (?/?). 5 cases (.5%) true disagreement (D/S). 11 cue phrases
(1.2%) judged ambiguous first judge classified second judge (?/D
?/S). conjunctions \and," \or" \but" removed corpus,
11 examples (2.2%) judged ambiguous judges: 3 instances \actually,"
2 instances \because" \essentially," 1 instance \generally," \indeed,"
\like" \now." 1 case (.2%) true disagreement (an instance \like").
2 cue phrases (.4%) - instance \like" \otherwise" - judged ambiguous
first judge.
3.1.2 Features

second component input learning program specifies names potential
values fixed set features. set primitive features considered learning
experiments shown Figure 2. Feature values either numeric value one
fixed set user-defined symbolic values. feature representation shown follows
representation Hirschberg Litman except noted. Length intonational phrase (P60

fiCue Phrase Classification Using Machine Learning

Prosodic Features

{ length intonational phrase (P-L): integer.
{ position intonational phrase (P-P): integer.
{ length intermediate phrase (I-L): integer.
{ position intermediate phrase (I-P): integer.
{ composition intermediate phrase (I-C): only, cue phrases, other.
{ accent (A): H*, L*, L*+H, L+H*, H*+L, H+L*, deaccented, ambiguous.
{ accent* (A*): H*, L*, complex, deaccented, ambiguous.
Textual Features
{ preceding cue phrase (C-P): true, false, NA.
{ succeeding cue phrase (C-S): true, false, NA.
{ preceding orthography (O-P): comma, dash, period, paragraph, false, NA.
{ preceding orthography* (O-P*): true, false, NA.
{ succeeding orthography (O-S): comma, dash, period, false, NA.
{ succeeding orthography* (O-S*): true, false, NA.
{ part-of-speech (POS): article, coordinating conjunction, cardinal numeral, subordinating conjunction,
preposition, adjective, singular mass noun, singular proper noun, intensifier, adverb, verb base form,
NA.

Lexical Feature

{ token (T): actually, also, although, and, basically, because, but, essentially, except, finally, first, further,

generally, however, indeed, like, look, next, no, now, ok, or, otherwise, right, say, second, see, similarly,
since, so, then, therefore, well, yes.

Figure 2: Representation features, use C4.5 cgrendel.
L) length intermediate phrase (I-L) represent number words intonational
intermediate phrases containing cue phrase, respectively. feature coded
\now" data, coded (although used) later multiple cue phrase
data. Position intonational phrase (P-P) position intermediate phrase (I-P) use
numeric values rather earlier symbolic values (e.g., first Figure 1). Composition
intermediate phrase (I-C) replaces value alone (meaning phrase contained
example cue phrase, example plus cue phrases) Figure 1
primitive values cue phrases (whose disjunction equivalent
alone); I-C uses value rather :alone (as used Figure 1). Accent
(A) uses value ambiguous represent cases prosodic analysis yields
disjunction (e.g., \H*+L H*"). Accent* (A*) re-represents symbolic values
feature accent (A) using abstract level description. particular, L*+H,
L+H*, H*+L, H+L* represented separate values single value {
superclass complex { A*. useful abstractions often result learning
process, A* explicitly represented advance prosodic feature representation
potential automated (see Section 5).
textual features, value NA (not applicable) ects fact 39 recorded
examples included transcription, done independently
61

fiLitman

studies performed Hirschberg Litman (1993). coding used Hirschberg
Litman, preceding cue phrase (C-P) succeeding cue phrase (C-S) represented actual
cue phrase (e.g., \and") preceding succeeding cue phrase; value
true encodes cases. prosodic feature set A*, preceding orthography*
(O-P*) succeeding orthography* (O-S*) re-represent symbolic values
preceding orthography (O-P) succeeding orthography (O-S), respectively, using
abstract level description (e.g., comma, dash, period represented separate values
O-S single value true O-S*). done reliability coding
detailed transcriptions orthography known. Part-of-speech (POS) represents
part speech assigned cue phrase Church's program tagging part speech
unrestricted text (Church, 1988); program assign approximately 80 different
values, subset values actually assigned cue phrases
transcripts corpora shown figure. Finally, lexical feature token (T)
new study, represents actual cue phrase described.
3.1.3 Training Data

final input learning program training data, i.e., set examples
class feature values specified. Consider following utterance, taken
multiple cue phrase corpus (Hirschberg & Litman, 1993):
Example 1 [(Now) (now welcomed here)] it's time get
business conference.
utterance contains two cue phrases, corresponding two instances \now".
brackets parentheses illustrate intonational intermediate phrases, respectively,
contain example cue phrases. Note single intonational phrase contains
examples, example uttered different intermediate phrase.
interested feature length intonational phrase (P-L), two examples would
represented training data follows:
P-L Class
9 discourse
9 sentential
first column indicates value assigned feature P-L, second column
indicates example classified. Thus, length intonational phrase
containing first instance \now" 9 words, example cue phrase classified
discourse usage. interested feature composition intermediate
phrase (I-C), two examples would instead represented training data follows:
I-C Class
discourse
sentential
is, intermediate phrase containing first instance \now" contains
cue phrase \now", intermediate phrase containing second instance \now"
contains \now" well 7 lexical items cue phrases. Note
value P-L examples, value I-C different.
62

fiCue Phrase Classification Using Machine Learning

3.2 Machine Learning Outputs

output machine learning programs classification models. C4.5 model
expressed decision tree, consists either leaf node (a class assignment),
decision node (a test feature, one branch subtree possible outcome
test). following example illustrates non-graphical representation decision
node testing feature n possible values:
test1 : : :
:::

elseif testn

:::

Tests form \feature operator value"9 . \Feature" name feature (e.g.
accent), \value" valid value feature (e.g., deaccented). features
symbolic values (e.g., accent), one branch symbolic value, operator
\=" used. features numeric values (e.g., length intonational phrase),
two branches, comparing numeric value threshold value; operators
\" \>" used. Given decision tree, cue phrase classified starting
root tree following appropriate branches leaf reached. Section 4
shows example decision trees produced C4.5.
cgrendel classification model expressed ordered set if-then rules
following form:
test1 ^ : : : ^ testk class
\if" part rule conjunction tests values (varying) features,
tests form \feature operator value." C4.5, \feature" name
feature, \value" valid value feature. Unlike C4.5, operators = =
6
used features symbolic values, used features numeric
values. \then" part rule specifies class assignment (e.g, discourse). Given set
if-then rules, cue phrase classified using rule whose \if" part satisfied.
two rules rules disagree class example, cgrendel
applies one two con ict resolution strategies (chosen user): choose first rule,
choose rule accurate data. experiments reported use
second strategy. rules, cgrendel assigns default class. Section 4
shows example rules produced cgrendel.
C4.5 cgrendel learn classification models using greedy search guided
\information gain" metric. C4.5 uses divide conquer process: training examples
recursively divided subsets (using tests discussed above), subsets
belong single class. test chosen divide examples maximizes
metric called gain ratio (a local measure progress, consider
subsequent tests); metric based information theory discussed detail
Quinlan (1993). test selected, backtracking. Ideally, set chosen
tests result small final decision tree. cgrendel generates set if-then rules
using method called separate conquer (to highlight similarity divide
conquer):
9. additional type test may invoked C4.5 option.

63

fiLitman

Many rule learning systems generate hypotheses using greedy strategy
rules added rule set one one effort form small cover
positive examples; rule, turn created adding one condition
another antecedent rule consistent negative
data. (Cohen, 1993)
Although cgrendel claimed two advantages C4.5, advantages
come play experiments reported here. First, if-then rules appear easier
people understand decision trees (Quinlan, 1993). However, cue phrase
classification task, decision trees produced C4.5 quite compact thus easily
understood. Furthermore, rule representation derived C4.5 decision trees,
using program C4.5rules. Second, cgrendel allows users exploit prior knowledge
learning problem, constraining syntax rules learned. However,
prior knowledge exploited cue phrase experiments. main reason using
C4.5 cgrendel increase reliability comparisons machine
learning manual results. particular, comparable results obtained using
C4.5 cgrendel, performance differences learned manually
derived classification models less likely due specifics particular learning
program, likely ect learned/manual distinction.

3.3 Evaluation

output machine learning experiment classification model
learned training data. learned models qualitatively evaluated examining linguistic content, comparing manually derived models
Figure 1. learned models quantitatively evaluated examining error
rates testing data comparing error rates error
rates shown Table 1. error rate classification model computed using
model predict classifications set examples classifications already
known, comparing predicted known classifications. cue phrase domain,
error rate computed summing number discourse examples misclassified
sentential number sentential examples misclassified discourse, dividing
total number examples.
error rates learned classification models estimated using two methodologies. Train-and-test error rate estimation (Weiss & Kulikowski, 1991) \holds out" test
set examples, seen training completed. is, model
developed examining training examples; error model estimated
using model classify test examples. evaluation method used
Hirschberg Litman. resampling method cross-validation (Weiss & Kulikowski,
1991) estimates error rate using multiple train-and-test experiments. example, 10fold cross-validation, instead dividing examples training test sets once, 10 runs
learning program performed. total set examples randomly divided 10
disjoint test sets; run thus uses 90% examples test set training
remaining 10% testing. Note iteration cross-validation,
learning process begins scratch; thus new classification model learned
training sample. estimated error rate obtained averaging error rate test64

fiCue Phrase Classification Using Machine Learning

ing portion data 10 runs. method make sense
humans, computers truly ignore previous iterations. sample sizes hundreds
(the classifiable subset multiple cue phrase sample classifiable non-conjunct
subset provide 878 495 examples, respectively) 10-fold cross-validation often provides
better performance estimate hold-out method (Weiss & Kulikowski, 1991).
major advantage cross-validation examples eventually used testing,
almost examples used given training run.
best performing learned models identified comparing error rates
error rates learned models manually derived error rates.
determine whether fact error rate E1 lower another error rate E2
significant, statistical inference used. particular, confidence intervals two
error rates computed, 95% confidence level. error rate estimated using
single error rate test set (i.e., train-and-test methodology), confidence
interval computed using normal approximation binomial distribution (Freedman
et al., 1978). error rate estimated using average multiple error
rates (i.e., cross-validation methodology), confidence interval computed using
t-Table (Freedman et al., 1978). upper bound 95% confidence interval E1
lower lower bound 95% confidence interval error rate E2,
difference E1 E2 assumed significant.10

3.4 Experimental Conditions
section describes conditions used set machine learning experiments.
experiments differ use training testing corpora, methods estimating error
rates, features classifications used. actual results experiments
presented Section 4.
3.4.1 Four Sets Experiments

learning experiments conceptually divided four sets. experiment
first set estimates error rate using train-and-test method, training
testing samples used Hirschberg Litman (1993) (the \now" data
two subsets multiple cue phrase corpus, respectively). allows direct comparison
manual machine learning approaches. However, prosodic experiments
conducted Hirschberg Litman (1993) replicated. textual training testing
conditions replicated original training corpus (the first 17 minutes
multiple cue phrase corpus) (Litman & Hirschberg, 1990) subset of, rather disjoint
from, test corpus (the full 75 minutes multiple cue phrase corpus) (Hirschberg &
Litman, 1993).
contrast, experiment second set uses cross-validation estimate error
rate. Furthermore, training testing samples taken multiple cue
phrase corpus. experiment uses 90% examples multiple cue phrase
data training, remaining 10% testing. Thus experiment second
set trains much larger amounts data (790 classifiable examples, 445 classifiable
10. Thanks William Cohen suggesting methodology.

65

fiLitman

prosody
hl93features
phrasing
length
position
intonational
intermediate
text
adjacency
orthography
preceding
succeeding
speech-text

P-L
X
X
X
X

P-P I-L I-P
X X X
X
X X X
X
X
X
X
X X

I-C A*
X X X
X X X
X
X

C-P

X
X
X

X

X

X

X

X

X

X

X

C-S O-P

X
X
X
X

O-P* O-S

O-S* POS

X

X

X

X

X
X

X
X

X

X

X

X

X
X

X
X

X

X

Table 3: Multiple feature sets components.
non-conjuncts) experiment first set (100 \nows"). reliability
testing compromised due use cross-validation (Weiss & Kulikowski, 1991).
experiment third set replicates experiment second set, exception learning program allowed distinguish cue phrases.
done adding feature representing cue phrase (the feature token Figure 2)
experiment second set. Since potential use lexical feature
noted used Hirschberg Litman (1993), experiments provide qualitatively new linguistic insights data. example, features may
used differently predict classifications different cue phrases sets cue phrases.
Finally, experiment fourth set replicates experiment first, second,
third set, exception 953 examples multiple cue phrase corpus
considered. practice, learned cue phrase classification model
likely used classify cue phrases, even dicult human judges
classify. experiments fourth set allow learning programs attempt
learn class unknown, addition classes discourse sentential.
3.4.2 Feature Representations within Experiment Sets

Within four sets experiments, individual experiment represents
data using different subset available features. First, data represented
14 single feature sets, corresponding prosodic textual feature shown
Figure 2. experiments comparatively evaluate utility individual feature
classification. representations Example 1 shown illustrate data
represented using single feature set P-L, using single feature set I-C.
Second, data represented 13 multiple feature sets shown Table 3.
sets contains linguistically motivated subset least 2 14 features.
first 7 sets use prosodic features. Prosody considers prosodic features
coded example cue phrase. Hl93features considers coded features
used model shown Figure 1. Phrasing considers features
intonational intermediate phrases containing example cue phrase (i.e., length
66

fiCue Phrase Classification Using Machine Learning

Example 1 [(

) (now welcomed here)] it's time get business conference.
P-P I-L I-P I-C

A*
C-P C-S O-P O-P* O-S O-S* POS Class
1
1
1 H*+L complex f

par.
f
f
adv. disc.
2
8
1 H*
H*

f
f
f
f
f
adv. sent.


P-L
9
9

Figure 3: Representation Example 1 feature set speech-text.
phrase, position example phrase, composition phrase). Length position
consider one features, respect intonational
intermediate phrase. Conversely, intonational intermediate consider one type
phrase, consider features. next 5 sets use textual features. Text
considers textual features. Adjacency orthography consider single textual
feature, consider preceding succeeding immediate context. Preceding
succeeding consider contextual features relating orthography cue phrases,
limit context. last set, speech-text, uses prosodic textual features.
Figure 3 illustrates two example cue phrases Example 1 would represented
using speech-text. Consider feature values first example cue phrase. Since
example first lexical item intonational intermediate phrases
contain it, position phrases (P-P I-P) 1. Since intermediate phrase
containing cue phrase contains lexical items, length (I-L) 1 word
composition (I-C) cue phrase. values A* indicate
intonational phrase described sequence tones, complex pitch accent H*+L
associated cue phrase. respect textual features, utterance
transcribed began new paragraph. Thus example cue phrase
preceded another cue phrase (C-P), preceded form orthography (O-P
O-P*). Since example cue phrase immediately followed another instance
\now" transcription, cue phrase succeeded another cue phrase (C-S)
succeeded orthography (O-S O-S*). Finally, output part
speech tagging program run transcript corpus yields value adverb
cue phrase's part speech (POS).
first set experiments replicate prosodic experiments conducted
Hirschberg Litman (1993); cue phrases represented using subset feature sets consist prosodic features. second set experiments, examples
represented using 27 different feature sets (the 14 single feature sets 13
multiple feature sets). third set experiments, examples represented using 27
tokenized feature sets, constructed adding lexical feature token Figure 2 (the
cue phrase described) 14 single 13 multiple feature sets
second set experiments. tokenized feature sets referred using names
single multiple feature sets, concatenated \+". following illustrates
two cue phrases Example 1 would represented using P-L+:
P-L
Class
9 discourse
9 sentential
67

fiLitman

representation similar P-L representation shown earlier, except second
column indicates value assigned feature token (T).

4. Results

section examines results running two learning programs { C4.5 cgrendel { four sets cue phrase classification experiments described above. learned
classification models compared classification models shown Figure 1,
error rates learned classification models compared error
rates shown Table 1 error rates learned models.
seen, results suggest machine learning useful automating generation
linguistically viable classification classification models, generating classification models
perform lower error rates manually developed hypotheses, adding
body linguistic knowledge regarding cue phrases.

4.1 Experiment Set 1: Replicating Hirschberg Litman

first group experiments replicate training, testing, evaluation conditions
used Hirschberg Litman (1993), order investigate well machine learning
performs comparison manual development cue phrase classification models.
Figure 4 shows best performing prosodic classification models learned two
machine learning programs; top figure replicates manually derived prosodic
model Figure 1 ease comparison. prosodic features used
represent 100 training examples \now" (i.e., example represented using
feature set prosody Table 3)11, classification models learned shown
manually derived model top Figure 4. Note using learning
programs, decision tree learned smaller feature sets phrasing
position used represent \now" data. bottom portion figure shows
classification models learned examples represented using
single prosodic feature position intonational phrase (P-P); model
learned examples represented using multiple feature set intonational.
Recall C4.5 represents learned classification model decision tree.
level tree (shown indentation) specifies test single feature, branch
every possible outcome test. branch either lead assignment class,
another test. example, C4.5 classification model learned prosody classifies
cue phrases using two features position intonational phrase (P-P) position
intermediate phrase (I-P). Note available features prosody (recall
Table 3) used decision tree. tree initially branches based value
feature position intonational phrase.12 first branch leads class assignment
discourse. second branch leads test feature position intermediate phrase.
first branch test leads class assignment discourse, second branch
leads sentential. C4.5 produces unsimplified pruned decision trees. goal
11. Experiment Set 1, feature set prosody contain features P-L I-L. Recall
phrasal length coded later multiple cue phrase study.
12. ease comparison Figure 1, original symbolic representation feature value used
rather integer representation shown Figure 2.

68

fiCue Phrase Classification Using Machine Learning

Manually derived prosodic model (repeated Figure 1):

composition intermediate phrase = alone
elseif composition intermediate phrase = :alone
position intermediate phrase = first
accent = deaccented
elseif accent = L*
elseif accent = H*
elseif accent = complex
elseif position intermediate phrase = :first

discourse

discourse

discourse

sentential

sentential

sentential

(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)

Decision tree learned prosody, phrasing, position using C4.5:

position intonational phrase = first
elseif position intonational phrase = :first
position intermediate phrase = first
elseif position intermediate phrase = :first
discourse

discourse
sentential

Ruleset learned prosody, phrasing, position using CGRENDEL:

(position intonational phrase 6= first) ^ (position intermediate phrase 6= first)
default discourse

sentential

Decision tree learned P-P intonational using C4.5:

position intonational phrase = first
elseif position intonational phrase = :first

discourse
sentential

Ruleset learned P-P intonational using CGRENDEL:

position intonational phrase 6= first
default discourse

sentential

Figure 4: Example C4.5 cgrendel classification models learned different prosodic
feature representations \now" data.

69

fiLitman

Model
Classifiable Cue Phrases (N=878) Classifiable Non-Conjuncts (N=495)
P-P
18.3 2.6
16.6 3.4
prosody
27.3 3.0
17.8 3.4
phrasing
27.3 3.0
17.8 3.4
position
27.3 3.0
17.8 3.4
intonational
18.3 2.6
16.6 3.4
manual prosodic
24.6 3.0
14.7 3.2

Table 4: 95%-confidence intervals error rates (%) best performing cgrendel
prosodic classification models, testing data. (Training data \now" corpus;
testing data multiple cue phrase corpus.)
pruning process take complex decision tree may overfitted
training data, produce tree comprehensible whose accuracy
comprised (Quinlan, 1993). Since almost trees improved pruning (Quinlan,
1993), simplified decision trees considered paper.
contrast, cgrendel represents learned classification model set if-then
rules. rule specifies conjunction tests various features, results
assignment class. example, cgrendel ruleset learned prosody classifies
cue phrases using two features position intonational phrase (P-P) position
intermediate phrase (I-P) (the two features used C4.5 decision tree).
values features first, if-then rule applies cue phrase classified
sentential. value either feature first, default applies cue phrase
classified discourse.
examination learned classification models Figure 4 shows
comparable content portion manually derived model classifies cue
phrases solely phrasal position (line (8)). particular, classification models
say cue phrase initial phrasal position classify sentential.
hand, manually derived model assigns class sentential given
initial phrasal position conjunction certain combinations phrasal composition
accent; learned classification models instead classify cue phrase discourse
cases. shown, discrimination manually obtained model
significantly improve performance compared learned classification
models, fact one case significantly degrades performance.
error rates learned classification models \now" training data
developed follows: 6% models learned prosody, phrasing
position, 9% models learned P-P intonational. Recall
Section 2 error rate manually developed prosodic model Figure 1
training data 2%.
Table 4 presents 95% confidence intervals error rates best performing
cgrendel prosodic classification models. ease comparison, row labeled \manual
prosodic" presents error rates manually developed prosodic model Figure 1
two test sets, originally shown Table 1. table includes
cgrendel models whose performance matches exceeds manual performance.
70

fiCue Phrase Classification Using Machine Learning

Comparison error rates learned manually developed models suggests
machine learning effective technique automating development cue phrase
classification models. particular, within test set, 95% confidence interval
error rate classification models learned multiple feature sets prosody,
phrasing, position overlaps confidence interval error rate
manual prosodic model. true error rates P-P intonational
classifiable non-conjunct test set. Thus, machine learning supports automatic construction variety cue phrase classification models achieve similar performance
manually constructed models.
results P-P intonational classifiable cue phrase test set
shown italics, suggest machine learning may useful improving
performance. Although simple classification model learned P-P intonational performs worse manually derived model training data, tested
classifiable cue phrases, learned model (with upper bound error rate 20.9%)
outperforms manually developed model (with lower bound error rate 21.6%).
suggests manually derived model might overfitted training data,
i.e., prosodic feature set useful classifying \now" generalize
cue phrases. noted above, use simplified learned classification models helps
guard overfitting learning approach. ease inducing classification
models many different sets features using machine learning supports generation
evaluation wide variety hypotheses (e.g. P-P, high performing
optimal performing model training data).
Note manual prosodic manual performs significantly better smaller test
set (which contain cue phrases \and", \or", \but"). contrast,
performance improvement P-P intonational smaller test set significant.
suggests manually derived model generalize well learned
models.
Finally, feature sets shown Table 4, decision trees produced C4.5 perform
error rates rulesets produced cgrendel, test sets. Recall
Figure 4 C4.5 decision trees cgrendel rules fact semantically
equivalent feature set. fact comparable results obtained using C4.5
cgrendel adds extra degree reliability experiments. particular,
duplication results suggests ability match perhaps even improve
upon manual performance using machine learning due specifics either
learning program.

4.2 Experiment Set 2: Using Different Training Sets
second group experiments evaluate utility training larger amounts
data. done using 10-fold cross-validation estimate error, run
90% examples sample used training (and 10 runs,
examples used testing). addition, experiments second set take
training testing data multiple-cue phrase corpus, contrast previous
set experiments training data taken \now" corpus.
seen, changes improve results, learned classification models
71

fiLitman

Model
Classifiable Cue Phrases (N=878) Classifiable Non-Conjuncts (N=495)
P-L
33.0 5.9
(33.2 1.9)
P-P
16.1 3.5
18.8 4.2
I-L
25.5 3.7
(25.6 2.8)
I-P
25.9 4.9
19.4 3.1
I-C
(36.5 5.4)
(35.2 3.4)

28.6 3.6
(30.2 3.1)
A*
28.3 4.3
(28.4 1.7)
prosody
15.5 2.6
17.2 3.1
hl93features
29.4 3.3
18.2 4.2
phrasing
16.1 3.4
19.6 3.9
length
26.1 3.8
(27.4 3.4)
position
18.2 2.3
19.4 2.8
intonational
17.0 4.0
20.6 3.6
intermediate
21.9 2.3
19.4 5.7
manual prosodic
24.6 3.0
14.7 3.2

Table 5: 95%-confidence intervals error rates (%) cgrendel prosodic classification models, testing data. (Training testing done multiple
cue phrase corpus using cross-validation.)
perform lower comparable error rates compared manually developed
models.
4.2.1 Prosodic Models

Table 5 presents error rates classification models learned cgrendel,
28 different prosodic experiments. (For Experiment Sets 2 3, C4.5 error rates
presented Appendix A.) numeric cell shows 95% confidence interval
error rate, equal error percentage obtained cross-validation margin
error ( 2.26 standard errors, using t-Table). top portion table considers
models learned single prosodic feature sets (Figure 2), middle portion
considers models learned multiple feature sets (Table 3), last row
considers manually developed prosodic model. error rates shown italics indicate
performance learned classification model exceeds performance
manual model (given test set). error rates shown parentheses indicate
opposite case - performance manual model exceeds performance
learned model. cases omitted Table 4.
Experiment Set 1, comparison error rates learned manually
developed models suggests machine learning effective technique
automating development cue phrase classification models, improving
performance. evaluated classifiable cue phrase test set, five learned models
improved performance compared manual model; models except I-C
perform least comparably manual model. Note Experiment Set 1, two
learned models outperformed manual model, five learned models performed
least comparably. ability use large training sets thus appears advantage
automated approach.
72

fiCue Phrase Classification Using Machine Learning

Manually derived prosodic model (repeated Figure 1):

composition intermediate phrase = alone
elseif composition intermediate phrase = :alone
position intermediate phrase = first
accent = deaccented
elseif accent = L*
elseif accent = H*
elseif accent = complex
elseif position intermediate phrase = :first

discourse

discourse

discourse

sentential

sentential

sentential

(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)

Decision tree learned P-P using C4.5:

position intonational phrase 1
elseif position intonational phrase > 1

discourse
sentential

Ruleset learned P-P using CGRENDEL:

position intonational phrase 2
default discourse

sentential

Decision tree learned prosody using C4.5:

position intonational phrase 1
position intermediate phrase 1
elseif position intermediate phrase > 1
elseif position intonational phrase > 1
length intermediate phrase 1
elseif length intermediate phrase > 1

discourse
sentential

discourse
sentential

Ruleset learned prosody using CGRENDEL:

(position intonational phrase 2) ^ (length intermediate phrase 2)
(7 position intonational phrase 4) ^ (length intonational phrase 10)
(length intermediate phrase 2) ^ (length intonational phrase 7) ^ (accent = H*)
(length intermediate phrase 2) ^ (length intonational phrase 9) ^ (accent = H*+L)
(length intermediate phrase 2) ^ (accent = deaccented)
(length intermediate phrase 8) ^ (length intonational phrase 9) ^ (accent = L*)
sentential

sentential

sentential
sentential

sentential

sentential

default discourse

Figure 5: Example C4.5 cgrendel classification models learned different prosodic
feature representations classifiable cue phrases multiple cue phrase
corpus.
tested classifiable non-conjuncts (where error rate manually
derived model decreases), machine learning useful automating improving
performance. might ect fact manually derived theories already achieve
optimal performance respect examined features less noisy subcorpus,
and/or automatically derived theory subcorpus based smaller
training set used larger subcorpus.
examination best performing learned classification models shows
quite comparable content relevant portions prosodic model Figure 1,
often contain linguistic insights. Consider classification model learned
single feature position intonational phrase (P-P), shown near top Figure 5.
73

fiLitman

learned classification models say cue phrase initial
position intonational phrase, classify sentential; otherwise classify discourse.
Note correspondence line (8) manually derived prosodic model. note
classification models comparable13 P-P classification models learned
Experiment Set 1 (shown Figure 4), despite differences training data.
fact single prosodic feature position intonational phrase (P-P) classify cue
phrases least well complicated manual multiple feature learned models
new result learning experiments.
Figure 5 illustrates complex classification models learned using prosody,
largest prosodic feature set. C4.5 model similar lines (1) (8) manual
model. (The length value 1 equivalent composition value alone.) ruleset
induced prosody cgrendel, first 2 if-then rules correlate sentential status
(among things) non-initial position14 , second 2 rules H* H*+L
accents; rules similar lines (6)-(8) Figure 1. However, last 2 if-then rules
ruleset correlate accent L* sentential status phrase
certain length, lines (4) (5) Figure 1 provide different interpretation
take length account. Recall length coded Hirschberg Litman
test data. Length thus never used generate revise prosodic model.
utility length new result experiment set.
Although shown, models learned phrasing, position, intonational
outperform manual model. seen Table 3, models correspond
feature sets supersets P-P subsets prosody.
4.2.2 Textual Models

Table 6 presents error rates classification models learned cgrendel,
24 different textual experiments. Unlike experiments involving prosodic feature sets,
none learned textual models perform significantly better manually derived
model. However, results suggest machine learning still effective technique
automating development cue phrase classification models. particular, five
learned models (O-P, O-P*, text, orthography, preceding) perform comparably
manually derived model, test sets. Note five models learned
five textual feature sets include either feature O-P O-P* (recall Figure 2
Table 3). models perform significantly better remaining learned
textual models.
Figure 6 shows best performing learned textual models. Note similarity
manually derived model. prosodic results, best performing single feature
models perform comparably learned multiple features. fact, cgrendel,
rulesets learned multiple feature sets orthography preceding identical
rulesets learned single features O-P O-P*, even though features
available use. (The corresponding error rates Table 6 identical due
13. different feature values two figures ect fact phrasal position represented
\now" corpus using symbolic values (as Figure 1), multiple cue phrase corpus using
integers (as Figure 2).
14. Tests \feature x" \feature y" merged figure simplicity, e.g., \y feature
x."

74

fiCue Phrase Classification Using Machine Learning

Model
Classifiable Cue Phrases (N=878) Classifiable Non-Conjuncts (N=495)
C-P
(40.7 6.2)
(40.2 4.5)
C-S
(41.3 5.9)
(39.8 4.2)
O-P
20.6 5.7
17.6 3.3
O-P*
18.4 3.7
17.2 2.4
O-S
(34.1 6.3)
(30.2 1.8)
O-S*
(35.2 5.5)
(32.6 3.0)
POS
(37.7 4.1)
(38.2 4.6)
text
18.8 4.2
19.0 3.6
adjacency
(39.7 5.7)
(40.2 3.4)
orthography
18.9 3.4
18.8 3.0
preceding
18.8 3.8
17.6 3.2
succeeding
(33.9 6.0)
(30.0 2.7)
manual textual
19.9 2.8
16.1 3.4

Table 6: 95%-confidence intervals error rates (%) cgrendel textual classification models, testing data. (Training testing done multiple
cue phrase corpus using cross-validation.)
Manually derived textual model (repeated Figure 1):
preceding orthography = true discourse
elseif preceding orthography = false sentential
Decision tree learned O-P*, text, orthography, preceding using C4.5:

preceding orthography* = NA
elseif preceding orthography* = false
elseif preceding orthography* = true

discourse
sentential
discourse

Ruleset learned O-P, O-P*, orthography, preceding using CGRENDEL:

preceding orthography* = false
default discourse

sentential

Ruleset learned text using CGRENDEL:

preceding orthography* = false
part-of-speech = article
default discourse

sentential

sentential

Figure 6: Example C4.5 cgrendel classification models learned different textual
feature representations classifiable cue phrases multiple cue phrase
corpus.
estimation using cross-validation.) cgrendel model text incorporates feature
part-of-speech. C4.5, models text, orthography preceding identical O-P*.
4.2.3 Prosodic/Textual Models

Table 7 presents error rates classification models learned cgrendel
data represented using speech-text, complete set prosodic textual features (recall
75

fiLitman

Model
Classifiable Cue Phrases (N=878) Classifiable Non-Conjuncts (N=495)
speech-text
15.9 3.2
14.6 4.6
manual prosodic
24.6 3.0
14.7 3.2
manual textual
19.9 2.8
16.1 3.4

Table 7: 95%-confidence intervals error rates (%) cgrendel prosodic/textual
classification model, testing data. (Training testing done multiple cue phrase corpus using cross-validation.)
Table 3). Since Hirschberg Litman develop similar classification model
combined types features, comparison last two rows show error rates
separate prosodic textual models. learned model compared
manual prosodic model, using classifiable cue phrases testing, learning result
significant performance improvement. consistent results discussed above,
several learned prosodic models performed better manually derived prosodic
model test set. performance speech-text significantly better worse
performance either best prosodic textual learned models (Tables 5 6,
respectively).
Figure 7 shows C4.5 cgrendel hypotheses learned speech-text. C4.5
model classifies cue phrases using prosodic textual features performed best
isolation (position intonational phrase preceding orthography*, discussed above),
conjunction additional feature length intermediate phrase (which appears
model learned prosody Figure 5). line (9) manually derived
textual model, learned model associates presence preceding orthography
class discourse. Unlike line (10), however, cue phrases preceded orthography
may classified either discourse sentential, based prosodic feature values (which
available use textual model). branch learned decision tree
corresponding last three lines similar lines (1), (2), (8) manually
derived prosodic model. (Recall length value 1 equivalent composition value
alone.)
cgrendel model uses similar features used C4.5 well prosodic
feature accent (also used prosody Figure 5), textual features part-of-speech
(also used text Figure 6) preceding cue phrase. C4.5, unlike line (10)
manually derived textual model, cgrendel model classifies cue phrases lacking
preceding orthography sentential conjunction certain feature values.
Unlike line (9) manual model, learned model classifies cue phrases
preceding orthography sentential (if orthography comma, feature values
present). Finally, third fifth learned rules elaborate line (6) additional
prosodic well textual features, first last learned rules elaborate line (8).

4.3 Experiment Set 3: Adding Feature token

experiment third group replicates experiment second group,
exception data representation includes lexical feature token
76

fiCue Phrase Classification Using Machine Learning

Manually derived prosodic model (repeated Figure 1):

composition intermediate phrase = alone
elseif composition intermediate phrase = :alone
position intermediate phrase = first
accent = deaccented
elseif accent = L*
elseif accent = H*
elseif accent = complex
elseif position intermediate phrase = :first

discourse

discourse

discourse

sentential

sentential

sentential

Manually derived textual model (repeated Figure 1):

preceding orthography = true
elseif preceding orthography = false

(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)

discourse
sentential

Decision tree learned speech-text using C4.5:

position intonational phrase 1
preceding orthography* = NA
elseif preceding orthography* = true
elseif preceding orthography* = false
length intermediate phrase > 12
elseif length intermediate phrase 12
length intermediate phrase 1
elseif length intermediate phrase > 1
elseif position intonational phrase > 1
length intermediate phrase 1
elseif length intermediate phrase > 1
discourse

discourse

discourse
discourse
sentential

discourse

sentential

Ruleset learned speech-text using CGRENDEL:

(preceding orthography = false) ^ (4 position intonational phrase 6) ^
(preceding orthography = false) ^ (length intermediate phrase 2)
(preceding orthography = false) ^ (length intonational phrase 7) ^ (preceding cue phrase = NA)
^ (accent = H*)
(preceding orthography = comma) ^ (length intermediate phrase 5) ^ (length intonational phrase 17)
^ (part-of-speech = adverb)
(preceding orthography = comma) ^ (3 length intonational phrase 8) ^ (accent = H*)
(preceding orthography = comma) ^ (3 length intermediate phrase 8)
^ (length intonational phrase 15)
(position intonational phrase 2) ^ (length intermediate phrase 2)
^ (preceding cue phrase = NA)
sentential

sentential

sentential

sentential

sentential

sentential

default discourse

sentential

Figure 7: C4.5 cgrendel classification models learned prosodic/textual feature representation classifiable cue phrases multiple cue phrase corpus.

77

fiLitman

Model
Classifiable Cue Phrases (N=878) Classifiable Non-Conjuncts (N=495)
P-L+
21.8 4.6
17.4 2.7
P-P+
16.7 2.8
14.8 5.0
I-L+
20.3 3.4
16.0 3.3
I-P+
25.1 4.1
17.0 3.6
I-C+
27.0 3.6
18.4 3.4
A+
19.8 3.2
12.8 3.1
A*+
18.6 3.8
15.4 2.8
prosody+
16.7 2.9
15.8 3.1
hl93features+
24.0 4.5
17.4 4.3
phrasing+
14.5 3.3
12.6 3.3
length+
18.6 2.0
16.2 3.5
position+
15.6 3.3
13.0 3.9
intonational+
15.1 2.2
16.6 4.6
intermediate+
18.5 3.7
16.6 4.0
manual prosodic
24.6 3.0
14.7 3.2

Table 8: 95%-confidence intervals error rates (%) cgrendel prosodic, tokenized classification models, testing data. (Training testing done
multiple cue phrase corpus using cross-validation.)
Figure 2. experiments investigate performance changes classification models allowed treat different cue phrases differently. seen, learning
tokenized feature sets often improves performance learned classification
models. addition, classification models contain new linguistic information regarding particular tokens (e.g., \so").
4.3.1 Prosodic Models

Table 8 presents error learned classification models test sets
multiple cue phrase corpus, tokenized prosodic feature sets. Again, error
rates italics indicate performance learned classification model meaningfully
exceeds performance \manual prosodic" model (which consider feature
token).
One way improvement obtained adding feature token seen
comparing performance learned manually derived models. Table 8, six
cgrendel classification models lower (italicized) error rates manual model.
Table 5, five models italicized. Thus, adding feature token results
additional learned model - length+ - outperforming manually derived model.
Conversely, Table 8, learned models perform significantly worse manually
derived manual. contrast, Table 5, several non-tokenized models perform worse
manual model (I-C larger test set, P-L, I-L, I-C, A, A*, length
non-conjunct test set).
improvement obtained adding feature token seen comparing
performance tokenized (Table 8) non-tokenized (Table 5) versions
model other. convenience, cases tokenization yields improvement
highlighted Table 9. table shows error rate tokenized versions
feature sets significantly lower error non-tokenized versions, P-L, I-C,
78

fiCue Phrase Classification Using Machine Learning

Model
P-L
I-L
I-C

A*
length

Classifiable Cue Phrases (N=878)
Non-Tokenized Tokenized (+)
33.0 5.9
21.8 4.6
36.5 5.4
27.0 3.6
28.6 3.6
19.8 3.2
28.3 4.3
18.6 3.8
26.1 3.8
18.6 2.0

Classifiable Non-Conjuncts (N=495)
Non-Tokenized
Tokenized (+)
33.2 1.9
17.4 2.7
25.6 2.8
16.0 3.3
35.2 3.4
18.4 3.4
30.2 3.1
12.8 3.1
28.4 1.7
15.4 2.8
27.4 3.4
16.2 3.5

Table 9: Cases adding feature token improves performance prosodic
model.
A, A*, length test sets, I-L non-conjunct test set. Note
overlap feature sets Table 9 discussed previous paragraph.
Figure 8 shows several tokenized single feature prosodic classification models. first
cgrendel model figure shows ruleset learned P-L+, reduces
33.2% 1.9% error rate P-L (length intonational phrase) 17.4% 2.7%,
trained tested using classifiable non-conjuncts (Table 9). Note first rule
uses prosodic feature (like rules Experiment Sets 1 2), fact
similar line (1) manual model. (Recall length value 1 equivalent
composition value alone.) However, unlike rules previous experiment sets,
next 5 rules use prosodic feature lexical feature token. unlike
rules previous experiment sets, remaining rules classify cue phrases using
feature token. Examination learned rulesets Figures 8 9 shows
cue phrases often appear last type rule. cue phrases,
example, \finally", \however", \ok", fact always discourse usages multiple
cue phrase corpus. cue phrases, classifying cue phrases using token
corresponds classifying cue phrases using default class (the frequent type
usage multiple cue phrase corpus). Recall use non-tokenized default class
model Table 1.
second example shows ruleset learned I-C+ (composition intermediate
phrase+). first rule corresponds line (1) manually derived model.15
next six rules classify particular cue phrases discourse, independently value I-C.
Note although model cue phrase \say" classified using token,
previous model sophisticated strategy classifying \say" could found.
third example shows cgrendel ruleset learned A+ (accent+). first
rule corresponds line (5) manually derived prosodic model. contrast line
(4), however, cgrendel uses deaccenting predict discourse tokens \say"
\so." token \finally", \however", \now" \ok", discourse assigned (for
accents). deaccented cases, sentential assigned (using default). Similarly,
contrast line (7), complex accent L+H* predicts discourse cue phrases
\further" \indeed" (and \finally", \however", \now" \ok"), sentential
otherwise.
15. discussed relation Figure 2, I-C values cue phrases multiple cue phrase
corpus replace value alone \now" corpus.

79

fiLitman

Manually derived prosodic model (repeated Figure 1):

composition intermediate phrase = alone
elseif composition intermediate phrase = :alone
position intermediate phrase = first
accent = deaccented
elseif accent = L*
elseif accent = H*
elseif accent = complex
elseif position intermediate phrase = :first

(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)

discourse

discourse

discourse

sentential

sentential

sentential

Ruleset learned P-L+ using CGRENDEL:

length intonational phrase 1
(7 length intonational phrase 11) ^ (token = although)
(9 length intonational phrase 16) ^ (token = indeed)
(length intonational phrase 20) ^ (token = say)
(11 length intonational phrase 13) ^ (token = then)
(length intonational phrase = 5) ^ (token = well)
token = finally
token =
token = however
token =
token = ok
token = otherwise
token =
discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

default sentential

Ruleset learned I-C+ using CGRENDEL:

composition intermediate phrase =
token = finally
token = however
token =
token = ok
token = say
token =

discourse

discourse

discourse

discourse

discourse

discourse

discourse

default sentential

Ruleset learned A+ using CGRENDEL:

accent = L*
(accent = deaccented) ^ (token = say)
(accent = deaccented) ^ (token = so)
(accent = L+H*) ^ (token = further)
(accent = L+H*) ^ (token = indeed)
token = finally
token = however
token =
token = ok
discourse

discourse

discourse
discourse

discourse

discourse

discourse

discourse

discourse

default sentential

Figure 8: Example cgrendel classification models learned different tokenized,
prosodic feature representations classifiable non-conjuncts multiple
cue phrase corpus.

80

fiCue Phrase Classification Using Machine Learning

Model
Classifiable Cue Phrases (N=878) Classifiable Non-Conjuncts (N=495)
C-P+
(28.2 3.9)
16.4 4.6
C-S+
(28.9 3.6)
17.2 4.0
O-P+
17.5 4.4
10.0 3.1
O-P*+
17.7 2.9
12.2 2.9
O-S+
26.9 4.7
18.4 3.9
O-S*+
(27.3 3.5)
16.0 3.2
POS+
(27.4 3.6)
17.2 3.9
text+
18.4 3.0
12.0 2.6
adjacency+
(28.6 4.1)
15.2 3.1
orthography+
17.6 3.0
13.6 3.9
preceding+
17.0 4.1
13.6 2.6
succeeding+
25.6 3.9
18.0 4.5
manual textual
19.9 2.8
16.1 3.4

Table 10: 95%-confidence intervals error rates (%) cgrendel textual, tokenized classification models, testing data. (Training testing done
multiple cue phrase corpus using cross-validation.)
summarize, new prosodic results Experiment Set 3 features relating
length, composition, accent, useful (in isolation) predicting classification cue phrases, fact quite useful predicting class individual cue
phrases subsets cue phrases. (Recall result Experiment Sets 1 2
without token, prosodic feature position intonational phrase useful
isolation.)
4.3.2 Textual Models

Table 10 presents error learned classification models test sets
multiple cue phrase corpus, tokenized textual feature sets. Experiment Set 2 (Table 6), none cgrendel classification models lower (italicized)
error rates manual model. However, adding feature token improve
performance many learned rulesets, following models (unlike
non-tokenized counterparts) longer outperformed manual model: O-S+
succeeding+ larger test set, C-P+, C-S+, O-S+, O-S*+, POS+, adjacency+,
succeeding+ non-conjunct test set.
improvement obtained adding feature token seen comparing
performance tokenized (Table 10) non-tokenized (Table 6) versions
model other, shown Table 11. table shows error rates
tokenized versions feature sets significantly lower error nontokenized versions, C-P, C-S, POS, adjacency test sets, O-P, O-S,
O-S*, text, succeeding non-conjunct test set. Note overlap feature
sets Table 11 discussed previous paragraph.
Figure 9 shows several tokenized single textual feature classification models. first
cgrendel model shows ruleset learned C-P+ (preceding cue phrase+),
reduces 40.2% 4.5% error rate C-P 16.4% 4.6% trained tested using
classifiable non-conjuncts (Table 11). ruleset correlates preceding cue phrases
discourse usages \indeed", omitted transcriptions \further", \now", \so"
81

fiLitman

Manually derived textual model (repeated Figure 1):

preceding orthography = true
elseif preceding orthography = false

discourse
sentential

Ruleset learned C-P+ using CGRENDEL:

(preceding cue phrase = true) ^ (token = indeed)
(preceding cue phrase = NA) ^ (token = further)
(preceding cue phrase = NA) ^ (token = now)
(preceding cue phrase = NA) ^ (token = so)
token = although
token = finally
token = however
token = ok
token = say
token = similarly

discourse
discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

default sentential

discourse

Ruleset learned O-P+ using CGRENDEL:

preceding orthography = false
(preceding orthography = comma) ^ (token = then)
sentential

default discourse

sentential

Ruleset learned O-S+ using CGRENDEL:

succeeding orthography = comma
(succeeding orthography = false) ^ (token = so)
succeeding orthography = NA
token = although
token = finally
token =
token = ok
token = say
discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

default sentential

Ruleset learned POS+ using CGRENDEL:

(part-of-speech = adverb) ^ (token = finally)
(part-of-speech = singular proper noun) ^ (token = further)
(part-of-speech = adverb) ^ (token = however)
(part-of-speech = adverb) ^ (token = indeed)
(part-of-speech = subordinating conjunction) ^ (token = so)
token = although
token =
token = say
token = ok
discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

default sentential

Figure 9: Example cgrendel classification models learned different tokenized, textual
feature representations classifiable non-conjuncts multiple cue phrase
corpus.

82

fiCue Phrase Classification Using Machine Learning

Model
C-P
C-S
O-P
O-S
O-S*
POS
text
adjacency
succeeding

Classifiable Cue Phrases (N=878)
Non-Tokenized Tokenized (+)
40.7 6.2
28.2 3.9
41.3 5.9
28.9 3.6
37.7 4.1
27.4 3.6
39.7 5.7
28.6 4.1
-

Classifiable Non-Conjuncts (N=495)
Non-Tokenized
Tokenized (+)
40.2 4.5
16.4 4.6
39.8 4.2
17.2 4.0
17.6 3.3
10.0 3.1
30.2 1.8
18.4 3.9
32.6 3.0
16.0 3.2
38.2 4.6
17.2 3.9
19.0 3.6
12.0 2.6
40.2 3.4
15.2 3.1
30.0 2.7
18.0 4.5

Table 11: Cases adding feature token improves performance textual
model.
discourse usages. classifications rest cue phrases predicted using
feature token.
second example shows cgrendel ruleset learned O-P+ (preceding orthography+). ruleset correlates preceding orthography sentential usages cue
phrases (as manually derived model learned models Experiment
Set 2). Unlike models, however, cue phrase \then" classified sentential,
even preceded orthography (namely, comma).
third example shows cgrendel ruleset learned O-S+ (succeeding orthography). ruleset correlates presence succeeding commas discourse usages
cue phrases, except cue phrase \so", classified discourse usage without
succeeding orthography. model correlates cue phrases omitted
transcript discourse usages. classifications rest cue phrases
predicted using feature token.
last example shows cgrendel ruleset learned POS+ (part-of-speech+).
ruleset classifies certain cue phrases discourse usages depending part-ofspeech token, well independently part-of-speech.
Finally, Figure 10 shows classification model learned text+, largest tokenized textual feature set. Note three four features used tokenized, single
textual feature models Figure 9 incorporated tokenized, multiple textual
feature model.
summarize, new textual results Experiment Set 3 features based adjacent cue phrases, succeeding orthography, part-of-speech, useful (in isolation)
predicting classification cue phrases, fact quite useful conjunction
feature token. (Recall result Experiment Set 2 without token,
textual features preceding orthography preceding orthography* useful
isolation.)
4.3.3 Prosodic/Textual Models

Table 12 presents error rates classification models learned cgrendel
data represented using speech-text+, complete set prosodic textual
83

fiLitman

Ruleset learned text+ using CGRENDEL:

preceding orthography = false
(preceding orthography = comma) ^ (token = although)
(preceding orthography = comma) ^ (token = no)
(preceding orthography = comma) ^ (token = then)
(succeeding orthography = false) ^ (preceding cue phrase = NA) ^ (token = similarly)
token = actually
token = first
token = since
token = yes
sentential

sentential

sentential

sentential

sentential

sentential

sentential

sentential

sentential

default discourse

Figure 10: cgrendel classification model learned tokenized, multiple textual feature
representation classifiable non-conjuncts multiple cue phrase corpus.
Model
Classifiable Cue Phrases (N=878) Classifiable Non-Conjuncts (N=495)
speech-text+
16.9 3.4
16.6 4.1
manual prosodic
24.6 3.0
14.7 3.2
manual textual
19.9 2.8
16.1 3.4

Table 12: 95%-confidence intervals error rates (%) cgrendel
prosodic/textual, tokenized classification models, testing data. (Training
testing done multiple cue phrase corpus using cross-validation.)
features. Experiment Set 2, performance speech-text+ better
performance either best learned (tokenized) prosodic textual models (Tables 8
10, respectively).
Comparison Tables 7 12 shows feature set speech-text, tokenization improve performance. contrast prosodic textual feature
sets, tokenization improves performance many learned models (namely
shown Tables 9 11).

4.4 Experiment Set 4: Adding Classification ambiguous
practice, cue phrase classification model classify cue phrases
recording text, \classifiable." experiments fourth set
replicate experiments Experiment Sets 1, 2, 3, exception 953 cue
phrases multiple cue phrase corpus used. means cue phrases
classified discourse, sentential, well unknown (defined Table 2). Experiment
Set 4 investigates whether machine learning explicitly recognize new class unknown.
Recall studies Hirschberg Litman attempt predict class
unknown, occur \now" training corpus. Thus Experiment Set 1,
class unknown similarly learned training data. However, unknown
examples added testing data Experiment Set 1. Obviously performance
degrade, models must incorrectly classify unknown example either discourse
84

fiCue Phrase Classification Using Machine Learning

sentential. example, tested full corpus 953 example cue phrases,
95% confidence intervals error rates P-P intonational 24.8% 2.8%;
recall tested subset corpus corresponding 878 classifiable cue
phrases, error 18.3% 2.6% (Table 4).
Unfortunately, results rerunning Experiment Sets 2 3 show promising
results classifying cue phrases unknown. Despite presence 75 examples
unknown, learned models still classify unknown cue phrases discourse
sentential. example, cgrendel used learning, 2 possible 27 nontokenized models16 (phrasing speech-text) contain rules predict class unknown.
Furthermore, models contains one rule unknown,
rules applies 2 possible 953 examples! Similarly, four possible 27
tokenized models (length+, phrasing+, prosody+, speech-text+) contain least one rule
class unknown. compared training testing using classifiable
cue phrases corpus, error rate full corpus typically (but always)
significantly higher. best performing model Experiment Set 4 speech-text+,
22.4% 4.1% error rate (95% confidence interval).
sum, Experiment Set 4 addressed problem previously unexplored
literature - ability develop classification models predict discourse
sentential usages cue phrases, usages human judges find dicult classify.
Unfortunately, results experiments suggest learning classify cue
phrases unknown dicult problem. Perhaps training data (recall
75 examples unknown) additional features better results could
obtained.

4.5 Discussion
experimental results suggest machine learning useful tool automating
generation classification models improving upon manually derived results.
Experiment Sets 1 2 performance many learned classification models
comparable performance manually derived models. addition, tested
classifiable cue phrases, several learned prosodic classification models (as well
learned prosodic/textual model) outperform Hirschberg Litman's manually derived
prosodic model. Experiment Set 3 shows learning tokenized feature sets even
improves performance, especially non-conjunct test set. tokenized
non-tokenized learned models perform least well manually derived models.
Many tokenized learned models outperform non-tokenized counterparts.
textual classification models outperform better prosodic classification models, advantage textual feature values obtained directly
transcript, determining values prosodic features requires manual analysis. (See, however, Section 5 discussion feasibility automating prosodic
analysis. addition, transcript may always available.) hand, almost
high performing textual models dependent orthography. manual tran16. Recall Experiment Sets 2 3 constructed 14 prosodic models, 12 textual models, 1
prosodic/textual model.

85

fiLitman

scriptions prosodic features shown reliable across coders (Pitrelli et al.,
1994), corresponding results reliability orthography.
Examination best performing learned models shows often comparable content relevant portions manually derived models. Examination
models provides new contributions cue phrase literature. example,
Experiment Sets 1 2 demonstrate utility classifying cue phrases based
single prosodic feature - phrasal position.17 Experiment Set 2 demonstrates utility
prosodic feature length textual feature preceding cue phrase classifying
cue phrases - conjunction prosodic textual features. Finally, results
Experiment Set 3 demonstrate even though many features useful
classifying cue phrases, may nonetheless informative tokenized
form. true prosodic features based phrasal length, phrasal composition,
accent, textual features based adjacent cue phrases, succeeding position,
part-of-speech.18

5. Utility
results machine learning experiments quite promising, compared
manually derived classification models already literature, learned classification
models often perform comparable higher accuracy. Thus, machine learning
appears effective technique automating generation classification models.
However, given experiments reported still rely manually created training
data, discussion practical utility results order.
Even given manually created training data, results established Hirschberg
Litman (1993) - obtained using even less automation experiments paper
- already practical import. particular, manually derived cue phrase
classification models used improve naturalness synthetic speech text-tospeech system (Hirschberg, 1990). Using text-based model, text-to-speech system
classifies cue phrase text synthesized either discourse sentential
usage. Using prosodic model, system conveys usage synthesizing
cue phrase appropriate type intonation. speech synthesis could
improved (and output made varied) using one higher performing
learned prosodic models presented paper.
results paper could directly applied area text generation.
example, Moser Moore (1995) concerned implementation cue selection placement strategies natural language generation systems. systems could
enhanced using text-based models cue phrase classification (particularly
17. empirical studies performed Holte (1993) show many datasets, accuracy
single feature rules decision trees often competitive accuracy complex learned
models.
18. contrast, prosodic features phrasal composition accent previously known useful
conjunction phrasal position (Hirschberg & Litman, 1993), part-ofspeech known useful conjunction orthography (Hirschberg & Litman, 1993).
Length, adjacent cue phrases, succeeding position used either manually derived
models (Hirschberg & Litman, 1993) (although length adjacent cue phrases shown useful
- conjunction prosodic textual features - Experiment Set 2).

86

fiCue Phrase Classification Using Machine Learning

tokenized models) additionally specify preceding succeeding orthography, part-ofspeech, adjacent cue phrases appropriate discourse usages.
Finally, results paper could fully automated, could used
natural language understanding systems, enhancing ability recognize discourse
structure. results obtained Litman Passonneau (1995) Passonneau
Litman (in press) suggest algorithms use cue phrases (in conjunction
features) predict discourse structure outperform algorithms take cue phrases
account. particular, Litman Passonneau develop several algorithms explore
features cue phrases, prosody referential noun phrases best combined
predict discourse structure. Quantitative evaluations results show best
performing algorithms incorporate use discourse usages cue phrases (where cue
phrases classified discourse using phrasal position). discussed Section 1,
discourse structure useful performing tasks anaphora resolution plan
recognition. Recent work shown discourse structure recognized,
used improve retrieval text (Hearst, 1994) speech (Sti eman, 1995).
Although prosodic features manually labeled Hirschberg Litman,
recent results suggesting least aspects prosody automatically
labeled directly speech. example, Wightman Ostendorf (1994) develop
algorithm able automatically recognize prosodic phrasing 85-86% accuracy
(measured comparing automatically derived labels hand-marked labels); accuracy slightly less human-human accuracy. Recall experimental results
paper show models learned single feature position intonational
phrase - could automatically computed given automatic prosodic phrasing algorithm - perform least well learned prosodic model. Similarly,
accenting versus deaccenting automatically labeled 88% accuracy (Wightman
& Ostendorf, 1994), sophisticated labeling scheme distinguishes
four types accent classes (and somewhat similar prosodic feature accent* used
paper) labeled 85% accuracy (Ostendorf & Ross, press). Recall
Experiment Set 3 tokenized models learned using accent* classify cue phrases
good results.
Although textual features automatically extracted transcript, transcript manually created. Many natural language understanding systems
deal speech all, thus begin textual representations. spoken language systems transcription process typically automated using speech recognition
system (although introduces sources error).

6. Related Work
paper compared results obtained using machine learning previously
existing manually-obtained results, used machine learning tool developing theories given new linguistic data (as models resulting Experiment Set 3,
new feature token considered). Siegel (1994) similarly uses machine learning
(in particular, genetic learning algorithm) classify cue phrases previously unstudied set textual features: feature corresponding token, well textual features
containing lexical orthographic item immediately left 4 positions
87

fiLitman

right example. Siegel's input consists one judge's non-ambiguous examples
taken data used Hirschberg Litman (1993) well additional examples;
output form decision trees. Siegel reports 21% estimated error rate,
half corpus used training half testing. Siegel McKeown (1994)
propose method developing linguistically viable rulesets, based partitioning
training data produced induction.
Machine learning used several areas discourse analysis. example, learning used develop rules structuring discourse multi-utterance
segments. Grosz Hirschberg (1992) use classification regression tree system
cart (Brieman et al., 1984) construct decision trees classifying aspects discourse
structure intonational feature values. Litman Passonneau (1995) Passonneau
Litman (in press) use system C4.5 construct decision trees classifying utterances discourse segment boundaries, using features relating prosody, referential noun
phrases, cue phrases. addition, C4.5 used develop anaphora resolution
algorithms, training corpora tagged appropriate discourse information (Aone &
Bennett, 1995). Similarly, McCarthy Lehnert (1995) use C4.5 learn decision trees
classify pairs phrases coreferent not. Soderland Lehnert (1994) use
machine learning program ID3 (a predecessor C4.5) support corpus-driven knowledge
acquisition information extraction. Machine learning often results algorithms
outperform manually derived alternatives (Litman & Passonneau, 1995; Passonneau & Litman, press; Aone & Bennett, 1995; McCarthy & Lehnert, 1995), although statistical
inference always used evaluate significance performance differences.
Finally, machine learning used great success many areas
natural language processing. discussed above, work researchers discourse
analysis concentrated direct application existing symbolic learning approaches
(e.g., C4.5), comparison learning manual methods. researchers
areas natural language processing addressed issues,
addition applied much wider variety learning approaches, concerned
development learning methods particularly designed language processing.
recent survey learning natural language (Wermter, Riloff, & Scheler, 1996) illustrates
type learning approaches used modified (in particular,
symbolic, connectionist, statistical, hybrid approaches), well scope
problems proved amenable use learning techniques (e.g., grammatical
inference, syntactic disambiguation, word sense disambiguation).

7. Conclusion
paper demonstrated utility machine learning techniques cue phrase
classification. Machine learning supports automatic generation linguistically viable
classification models. compared manually derived models already literature,
many learned models contain new linguistic insights perform least
high (if higher) accuracy. addition, ability automatically construct classification models makes easier comparatively analyze utility alternative feature
representations data. Finally, ease retraining makes learning approach
scalable extensible manual methods.
88

fiCue Phrase Classification Using Machine Learning

first set experiments presented used machine learning programs

cgrendel (Cohen, 1992, 1993) C4.5 (Quinlan, 1993) induce classification models

preclassified cue phrases features used training data
Hirschberg Litman (1993). results evaluated testing data
methodology used Hirschberg Litman (1993). second group experiments
used method cross-validation train test testing data used
Hirschberg Litman (1993). third set experiments induced classification models
using new feature token. fourth set experiments induced classification models
using new classification unknown.
experimental results indicate several learned classification models (including
extremely simple one feature models) significantly lower error rates models
developed Hirschberg Litman (1993). One possible explanation handbuilt classification models derived using small training sets; new data became
available, data used testing updating original models. contrast, machine learning conjunction cross-validation (Experiment Set 2) supported
building classification models using much larger amount data training.
Even learned models derived using small training set (Experiment
Set 1), results showed learning approach helped guard overfitting
training data.
prosodic classification model developed Hirschberg Litman demonstrated utility combining phrasal position phrasal composition accent,
best performing prosodic models Experiment Sets 1 2 demonstrated phrasal
position fact even useful predicting cue phrases used itself.
high performing classification models Experiment Set 2 demonstrated utility classifying cue phrases based prosodic feature length textual feature
preceding cue phrase, combination features.
machine learning approach made easy retrain new training examples became available (Experiment Set 2), machine learning made easy retrain
new features become available. particular, value feature token
added representations Experiment Set 2, trivial relearn
models (Experiment Set 3). Allowing learning programs treat cue phrases individually improved accuracy learned classification models, added
body linguistic knowledge regarding cue phrases. Experiment Set 3 demonstrated
useful classifying cue phrases, prosodic features based
phrasal length, phrasal composition, accent, textual features based adjacent
cue phrases, succeeding position, part-of-speech, fact useful used
conjunction feature token.
final advantage machine learning approach ease inducing classification models many different sets features supports exploration comparative
utility different knowledge sources. especially useful understanding tradeoffs accuracy model set features considered.
example, might worth effort code feature automatically obtainable
expensive automatically obtain adding feature results significant
improvement performance.
89

fiLitman

sum, results paper suggest machine learning useful tool
cue phrase classification, amount data precludes effective human analysis,
exibility afforded easy retraining needed (e.g., due additional training
examples, new features, new classifications), and/or analysis goal gain better
understanding different aspects data.
Several areas future work remain. First, still room performance improvement. error rates best performing learned models, even though outperform
manually derived models, perform error rates teens. Note
features coded discussed Hirschberg Litman (1993) considered
paper. may possible lower error rates considering new types
prosodic textual features (e.g., contextual textual features (Siegel, 1994),
features proposed connection general topic discourse
structure), and/or using different kinds learning methods. Second, Experiment Set
4 (and previous literature) show yet, models predicting
cue phrase usage classified unknown, rather discourse sentential.
Again, may possible improve performance existing learned models
considering new features and/or learning methods, perhaps performance could improved providing training data. Finally, currently open question whether
textual models developed here, based transcripts speech, applicable
written texts. Textual models thus need developed using written texts training
data. Machine learning continue useful tool helping address
issues.

Appendix A. C4.5 Results Experiment Sets 2 3
Tables 13, 14 15 present C4.5 error rates Experiment Sets 2 3. C4.5
results Experiment Set 2 shown \Non-Tokenized" columns. comparison
Tables 13 5 shows except larger test set, C4.5 prosodic error rates
fall within cgrendel confidence intervals. similar comparison Tables 14 6
shows except O-P larger test set, C4.5 textual error rates fall within
cgrendel confidence intervals. Finally, comparison Tables 15 7 shows
C4.5 error rate speech-text falls within cgrendel confidence interval. fact
comparable cgrendel C4.5 results generally obtained suggests ability
automate well improve upon manual performance due specifics
either learning program.
C4.5 results Experiment Set 3 shown \Tokenized" columns Tables 13, 14 15. Comparison Tables 8, 10 12 shows error rates C4.5
cgrendel similar Experiment Set 2. However, error rates reported
tables use default C4.5 cgrendel options running learning programs. Comparable performance two learning programs fact generally
achieved overriding one default C4.5 options. detailed Quinlan (1993),
default C4.5 approach { creates separate subtree possible feature value
{ might appropriate many values feature. situation characterizes feature token. C4.5 default option changed allow feature values
grouped one branch decision tree, problematic C4.5 error rates
90

fiCue Phrase Classification Using Machine Learning

Model
P-L
P-P
I-L
I-P
I-C

A*
prosody
hl93features
phrasing
length
position
intonational
intermediate

Classifiable Cue Phrases (N=878)
Non-Tokenized Tokenized (+)
32.5
31.7
16.2
18.4
25.6
26.8
25.9
26.3
36.5
36.6
40.7
40.7
28.3
26.7
16.0
15.2
30.2
29.0
15.9
15.2
24.8
24.4
18.1
18.0
16.8
16.6
21.2
22.3

Classifiable Non-Conjuncts (N=495)
Non-Tokenized
Tokenized (+)
32.2
31.4
18.8
19.0
25.6
25.6
19.4
18.8
35.8
32.8
29.6
29.2
28.8
31.2
19.4
16.0
18.8
18.8
18.0
17.4
26.2
24.2
19.6
17.6
18.8
19.8
21.6
18.4

Table 13: Error rates (%) C4.5 prosodic classification models, testing data. (Training
testing done multiple cue phrase corpus using cross-validation.)
Model
C-P
C-S
O-P
O-P*
O-S
O-S*
POS
text
adjacency
orthography
preceding
succeeding

Classifiable Cue Phrases (N=878)
Non-Tokenized Tokenized (+)
40.7
39.3
40.7
39.9
40.7
35.7
18.4
20.3
35.0
31.6
34.4
32.5
40.7
34.7
19.0
20.6
40.9
39.4
18.9
19.3
18.7
19.3
34.1
32.9

Classifiable Non-Conjuncts (N=495)
Non-Tokenized
Tokenized (+)
39.2
33.6
39.2
39.2
18.6
14.6
17.2
15.0
31.8
31.8
31.0
32.4
41.8
31.8
20.0
15.0
40.6
43.6
17.8
18.0
19.2
16.0
30.0
31.8

Table 14: Error rates (%) C4.5 textual classification models, testing data. (Training
testing done multiple cue phrase corpus using cross-validation.)
indeed improve. example, A+ error rate classifiable non-conjuncts changes
29.2% (Table 13) 11%, within 12.8% 3.1% cgrendel confidence
interval (Table 8).

Acknowledgements
would thank William Cohen Jason Catlett helpful comments regarding
use cgrendel C4.5, Sandra Carberry, Rebecca Passonneau, three
anonymous JAIR reviewers helpful comments paper. would
91

fiLitman

Model
speech-text

Classifiable Cue Phrases (N=878)
Non-Tokenized Tokenized (+)
15.3
13.6

Classifiable Non-Conjuncts (N=495)
Non-Tokenized
Tokenized (+)
16.8
17.6

Table 15: Error rates (%) C4.5 prosodic/textual classification model, testing data.
(Training testing done multiple cue phrase corpus using crossvalidation.)
thank William Cohen, Ido Dagan, Julia Hirschberg, Eric Siegel comments
preliminary version paper (Litman, 1994).

References

Altenberg, B. (1987). Prosodic Patterns Spoken English: Studies Correlation
Prosody Grammar Text-to-Speech Conversion, Vol. 76 Lund Studies
English. Lund University Press, Lund.
Aone, C., & Bennett, S. W. (1995). Evaluating automated manual acquisition
anaphora resolution strategies. Proceedings Thirty-Third Annual Meeting
Association Computational Linguistics (ACL).
Brieman, L., Friedman, J., Olshen, R., & Stone, C. (1984). Classification Regression
Trees. Monterey, CA: Wadsworth Brooks.
Church, K. W. (1988). stochastic parts program noun phrase parser unrestricted
text. Proceedings Second Conference Applied Natural Language Processing.
Cohen, R. (1984). computational theory function clue words argument understanding. Proceedings Tenth International Conference Computational
Linguistics (COLING).
Cohen, W. W. (1992). Compiling knowledge explicit bias. Proceedings
Ninth International Conference Machine Learning.
Cohen, W. W. (1993). Ecient pruning methods separate-and-conquer rule learning
systems. Proceedings Thirteenth International Joint Conference Artificial
Intelligence (IJCAI).
Freedman, D., Pisani, R., & Purves, R. (1978). Statistics. W. W. Norton Company.
Grosz, B., & Hirschberg, J. (1992). intonational characteristics discourse structure. Proceedings International Conference Spoken Language Processing
(ICSLP).
Grosz, B. J., & Sidner, C. L. (1986). Attention, intentions, structure discourse.
Computational Linguistics, 12 (3), 175{204.
92

fiCue Phrase Classification Using Machine Learning

Halliday, M. A. K., & Hassan, R. (1976). Cohesion English. Longman.
Hearst, M. A. (1994). Multi-paragraph segmentation expository text. Proceedings
Thirty-Second Annual Meeting Association Computational Linguistics
(ACL).
Hindle, D. M. (1989). Acquiring disambiguation rules text. Proceedings
Twenty-Seventh Annual Meeting Association Computational Linguistics
(ACL).
Hirschberg, J. (1990). Accent discourse context: Assigning pitch accent synthetic
speech. Proceedings Eighth National Conference Artificial Intelligence
(AAAI).
Hirschberg, J., & Litman, D. (1987). let's talk \now": Identifying cue phrases
intonationally. Proceedings Twenty-Fifth Annual Meeting Association
Computational Linguistics (ACL).
Hirschberg, J., & Litman, D. (1993). Empirical studies disambiguation cue phrases.
Computational Linguistics, 19 (3), 501{530.
Holte, R. C. (1993). simple classification rules perform well commonly used
datasets. Machine Learning, 11 (1), 63{90.
Litman, D., & Hirschberg, J. (1990). Disambiguating cue phrases text speech.
Proceedings Thirteenth International Conference Computational Linguistics
(COLING).
Litman, D. J. (1994). Classifying cue phrases text speech using machine learning.
Proceedings Twelfth National Conference Artificial Intelligence (AAAI).
Litman, D. J., & Allen, J. F. (1987). plan recognition model subdialogues conversation. Cognitive Science, 11, 163{200.
Litman, D. J., & Passonneau, R. J. (1995). Combining multiple knowledge sources
discourse segmentation. Proceedings Thirty-Third Annual Meeting
Association Computational Linguistics (ACL).
McCarthy, J. F., & Lehnert, W. G. (1995). Using decision trees coreference resolution.
Proceedings Fourteenth International Joint Conference Artificial Intelligence
(IJCAI).
Moser, M., & Moore, J. D. (1995). Investigating cue selection placement tutorial
discourse. Proceedings Thirty-Third Annual Meeting Association
Computational Linguistics (ACL).
Ostendorf, M., & Ross, K. (in press). multi-level model recognition intonation labels.
Y. Sagisaka, N. C., & Higuchi, N. (Eds.), Computing Prosody. Springer-Verlag.
Passonneau, R. J., & Litman, D. J. (in press). Discourse segmentation human
automated means. Computational Linguistics, 23.
93

fiLitman

Pierrehumbert, J. B. (1980). Phonology Phonetics English Intonation. Ph.D.
thesis, Massachusetts Institute Technology. Distributed Indiana University
Linguistics Club.
Pitrelli, J., Beckman, M., & Hirschberg, J. (1994). Evaluation prosodic transcription
labeling reliability ToBI framework. Proceedings International Conference Spoken Language Processing (ICSLP).
Quinlan, J. R. (1993). C4.5 : Programs Machine Learning. San Mateo, CA: Morgan
Kaufmann.
Reichman, R. (1985). Getting Computers Talk Me: Discourse Context,
Focus, Semantics. Cambridge, MA: MIT Press.
Siegel, E. V. (1994). Competitively evolving decision trees fixed training cases
natural language processing. K. E. Kinnear, J. (Ed.), Advances Genetic
Programming. Cambridge, MA: MIT Press.
Siegel, E. V., & McKeown, K. R. (1994). Emergent linguistic rules automatic
grouping training examples: Disambiguating clue words decision trees.
Proceedings Twelfth National Conference Artificial Intelligence (AAAI).
Soderland, S., & Lehnert, W. (1994). Corpus-driven knowledge acquisition discourse
analysis. Proceedings Twelfth National Conference Artificial Intelligence
(AAAI).
Sti eman, L. J. (1995). discourse analysis approach structured speech. Working
Notes AAAI Spring Symposium Series: Empirical Methods Discourse Interpretation Generation.
Weiss, S. M., & Kulikowski, C. (1991). Computer Systems Learn: Classification
Prediction Methods Statistics, Neural Nets, Machine Learning, Expert
Systems. San Mateo, CA: Morgan Kaufmann.
Wermter, S., Riloff, E., & Scheler, G. (1996). Connectionist, Statistical Symbolic Approaches Learning Natural Language Processing. Berlin, Germany: SpringerVerlag.
Wightman, C. W., & Ostendorf, M. (1994). Automatic labeling prosodic patterns. IEEE
Transactions Speech Audio Processing, 2 (4), 469{481.
Zuckerman, I., & Pearl, J. (1986). Comprehension-driven generation meta-technical
utterances math tutoring. Proceedings Fifth National Conference
Artificial Intelligence (AAAI).

94


