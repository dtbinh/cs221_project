journal artificial intelligence

submitted published

cue phrase classification machine learning
diane j litman

labs mountain avenue
murray hill nj usa

diane att com

abstract

cue phrases may used discourse sense explicitly signal discourse structure
sentential sense convey semantic rather structural information correctly
classifying cue phrases discourse sentential critical natural language processing
systems exploit discourse structure e g performing tasks anaphora resolution plan recognition explores use machine learning classifying
cue phrases discourse sentential two machine learning programs cgrendel
c used induce classification sets pre classified cue phrases
features text speech machine learning shown effective technique
automating generation classification improving
upon previous compared manually derived classification already
literature learned often perform higher accuracy contain
linguistic insights data addition ability automatically construct classification makes easier comparatively analyze utility alternative feature
representations data finally ease retraining makes learning
scalable exible manual methods

introduction

cue phrases words phrases may sometimes used explicitly signal discourse
structure text speech particular used discourse sense cue
phrase explicitly conveys structural information used sentential sense cue
phrase instead conveys semantic rather structural information following examples
taken spoken language corpus described section illustrate sample
discourse sentential usages cue phrases say
discourse
might concept say researcher worked fifteen years
certain project
crucial ai probably expert databases well
sentential
let say bears strong resemblance much work
done semantic nets even frames
place even stranger away
example used discourse sense cue phrase say conveys structural
information example beginning used sentential sense say
convey structural information instead functions verb
c ai access foundation morgan kaufmann publishers rights reserved

filitman

ability correctly classify cue phrases discourse sentential critical
natural language processing systems need recognize convey discourse structure
tasks improving anaphora resolution grosz sidner reichman
consider following example taken corpus described
section
system attempts hold rules say expert database expert system
expect hold rules fact apply us
appropriate situations
example cue phrases say discourse usages explicitly
signal boundaries intervening subtopic discourse structure furthermore
referents noun phrases system expert database expert
system possible referents pronoun structural information
conveyed cue phrases system determine system relevant
interpreting pronoun expert database expert system
occur within embedded concluded subtopic without cue phrases
reasoning required determine referent system intended referent
would much complex
correctly classifying cue phrases discourse sentential important natural
language processing tasks well discourse sentential distinction used
improve naturalness synthetic speech text speech systems hirschberg
text speech systems generate synthesized speech unrestricted text cue phrase
classified discourse sentential features input text
synthesized different intonational discourse sentential usages
addition explicitly identifying rhetorical relationships discourse usages cue
phrases used improve coherence multisentential texts natural language
generation systems zuckerman pearl moser moore cue phrases
used reduce complexity discourse processing areas argument
understanding cohen plan recognition litman allen grosz sidner

cue phrase classification often noted grosz sidner
recently classifying cue phrases neither developed evaluated
careful empirical analyses even though literature suggests features
might useful cue phrase classification quantitative analyses actual
classification use features suggestions different types
features might combined systems recognize generate cue phrases simply
assume discourse uses utterance clause initial reichman zuckerman
pearl empirical studies showing intonational prominence
certain word classes varies respect discourse function halliday hassan
altenberg studies investigate cue phrases per se
address limitations hirschberg litman conducted several empirical
studies specifically addressing cue phrase classification text speech hirschberg
litman pre classified set naturally occurring cue phrases described cue phrase
terms prosodic textual features features posited literature easy
example described detail hirschberg litman



ficue phrase classification machine learning

automatically code manually examined data construct classification
best predicted classifications feature values
examines utility machine learning automating construction
classifying cue phrases empirical data set experiments
described use two machine learning programs cgrendel cohen
c quinlan induce classification sets pre classified cue phrases
features features classes training examples used studies
hirschberg litman well additional features classes training examples given input machine learning programs evaluated
quantitatively qualitatively comparing error rates content
manually derived learned classification experimental
machine learning indeed effective technique automating generation
classification improving upon previous accuracy
learned classification often higher accuracy manually derived
learned often contain linguistic implications learning
paradigm makes easier compare utility different knowledge sources
update model given features classes training data
next section summarizes previous work cue phrase classification section
describes machine learning cue phrase classification taken
particular section describes four sets experiments use machine
learning automatically induce cue phrase classification types inputs
outputs machine learning programs presented methodologies
used evaluate section presents discusses experimental
highlights many benefits machine learning section discusses
practical utility finally section discusses use machine
learning studies discourse section concludes

previous work classifying cue phrases
section summarizes hirschberg litman empirical studies classification
cue phrases speech text hirschberg litman litman hirschberg
hirschberg litman data cue phrases taken corpora recorded
transcribed speech classified discourse sentential coded speech
text features used create input machine learning experiments hirschberg litman performance figures manually developed cue
phrase classification used benchmark evaluating performance
classification produced machine learning
first study hirschberg litman investigated usage cue phrase
multiple speakers radio call hirschberg litman classification
model prosodic features developed manual analysis training
set examples evaluated previously unseen test set examples
follow study hirschberg litman hirschberg litman tested
classification model larger set cue phrases namely single word cue phrases
technical keynote address single speaker corpus yielded instances


filitman

prosodic model

composition intermediate phrase alone
elseif composition intermediate phrase alone
position intermediate phrase first
accent deaccented
elseif accent l
elseif accent h
elseif accent complex
elseif position intermediate phrase first

discourse

discourse

discourse

sentential

sentential

sentential










textual model

preceding orthography true
elseif preceding orthography false




discourse
sentential

figure decision tree representation manually derived classification
hirschberg litman
different single word cue phrases derived literature hirschberg litman
used cue phrases first minutes corpus develop complementary cue
phrase classification model textual features litman hirschberg
tested full corpus hirschberg litman first study
referred study follow study multiple cue phrase study
note term multiple means different single word cue phrases opposed
cue phrase considered cue phrases consisting multiple
words e g way considered
method hirschberg litman used develop prosodic textual classification follows first separately classified example cue phrase
data discourse sentential ambiguous listening recording reading
transcription example described set prosodic textual features
previous observations literature correlating discourse structure prosodic information discourse usages cue phrases initial position clause contributed
choice features set classified described examples examined
order manually develop classification shown figure
shown decision trees ease comparison c
explained
prosody described pierrehumbert theory english intonation pierrehumbert pierrehumbert theory intonational contours described sequences
low l high h tones fundamental frequency f contour physical
figure contains list cue phrases hirschberg litman provide full details regarding
distribution cue phrases frequent cue phrase occurs times
next frequent cue phrase occurs times
occur fifty times four least frequent cue phrases essentially otherwise since
therefore occur times
class ambiguous introduced multiple cue phrase study hirschberg litman
litman hirschberg
although limited set textual features noted data analysis data
yield textual classification model



ficue phrase classification machine learning

correlate pitch intonational contours domain intonational phrase
finite state grammar describes set tonal sequences intonational phrase
well formed intonational phrase consists one intermediate phrases followed
boundary tone well formed intermediate phrase one pitch accents followed
phrase accent boundary tones phrase accents consist single tone
pitch accents consist single tone pair tones two simple pitch
accents h l four complex accents l h l h h l h l
indicates tone aligned stressed syllable associated lexical item
note every stressed syllable accented lexical items bear pitch accents
called accented called deaccented
prosody manually transcribed hirschberg examining fundamental frequency f contour listening recording transcription process
performed separately process discourse sentential classification produce
f contour recording corpus digitized pitch tracked speech analysis software resulted display f x axis represented time
axis represented frequency hz phrase final characteristics e g phrase accents
boundary tones well pauses syllable lengthening helped identify intermediate
intonational phrases peaks valleys display f contour helped
identify pitch accents similar manual transcriptions prosodic phrasing accent
shown reliable across coders pitrelli beckman hirschberg
prosody coded hirschberg litman represented every cue phrase terms
following prosodic features accent corresponded pitch accent
associated cue phrase intonational intermediate phrases
containing cue phrase feature composition phrase represented whether
cue phrase alone phrase phrase contained cue phrase
cue phrase cue phrases position phrase represented whether cue
phrase first first lexical item prosodic phrase unit possibly preceded
cue phrases
textual features used multiple cue phrase study hirschberg litman
litman hirschberg extracted automatically transcript part
speech cue phrase obtained running program tagging words one
approximately parts speech church transcript several characteristics
cue phrase immediate context noted particular whether immediately preceded succeeded orthography punctuation paragraph boundary
whether immediately preceded succeeded lexical item corresponding
another cue phrase
background classification shown figure explained
prosodic model uniquely classifies cue phrase features composition
intermediate phrase position intermediate phrase accent cue phrase
uttered single intermediate phrase possibly cue phrases e line
figure larger intermediate phrase initial position possibly preceded
features used figure discussed
another syntactic feature dominating constituent obtained running parser fidditch hindle
transcript however since feature appear manually derived
training data litman hirschberg feature pursued



filitman

model
classifiable cue phrases n classifiable non conjuncts n
prosodic


textual


default class



table confidence intervals error rates manually derived classification hirschberg litman testing data multiple cue phrase corpus
cue phrases l accent deaccented classified discourse part
larger intermediate phrase initial position h complex accent
non initial position sentential textual model classifies cue phrases
single feature preceding orthography cue phrase preceded type
orthography classified discourse otherwise cue phrase classified sentential
prosodic model used classify cue phrase training data e
examples model developed error rate
error rate textual model training examples multiple cue phrase
corpus litman hirschberg
prosodic textual evaluated quantifying performance
correctly classifying example cue phrases two test sets data shown rows
labeled prosodic textual table test set subset examples
multiple cue phrase corpus first test set examples consists
classifiable cue phrases e cue phrases hirschberg litman classified
discourse classified sentential note cue phrases hirschberg
litman classified ambiguous unable agree upon included
classifiable subset cue phrases considered learning experiments
described section however second test set classifiable non conjuncts
examples created classifiable cue phrases removing instances
subset considered particularly reliable since nonconjuncts classifiable compared example cue phrases error rate
prosodic model classifiable cue phrases classifiable
non conjuncts hirschberg litman error rate textual model
classifiable cue phrases classifiable non conjuncts hirschberg
litman last row table shows error rates simple default class
baseline model predicts frequent class corpus sentential
rates classifiable cue phrases classifiable non conjuncts
classification model part speech developed litman hirschberg
hirschberg litman however perform well model orthography
error rate part speech model larger test set opposed
orthographic model furthermore model combined orthography part speech performed
comparably simpler orthographic model hirschberg litman hirschberg litman
preliminary observations suggesting adjacency cue phrases might prove useful
following hirschberg litman original example sets hirschberg litman
combined



ficue phrase classification machine learning

although computed hirschberg litman table associates margins errors error percentage used compute confidence intervals freedman
pisani purves margin error standard errors confidence
interval normal table lower bound confidence interval computed
subtracting margin error error rate upper bound computed
adding margin error thus confidence interval prosodic model
classifiable cue phrase test set analysis confidence intervals
indicates improvement prosodic textual default
model significant example upper bounds error rates prosodic
textual classifiable cue phrase test set lower
lower bound default class error rate methodology statistical inference determine whether differences error rates significant discussed
fully section

experiments machine learning

section describes experiments use machine learning programs c quinlan
cgrendel cohen automatically induce cue phrase classification
cgrendel c similar learning methods
neural networks cart brieman friedman olshen stone induce
classification preclassified examples program takes following inputs
names classes learned names possible values fixed set features
training data e set examples class feature values specified
output program classification model expressed c decision tree
cgrendel ordered set rules cgrendel c learn
classification greedy search guided information gain metric
first group machine learning experiments replicate training testing conditions used hirschberg litman reviewed previous section support
direct comparison manual machine learning approaches second group
experiments evaluate utility training larger amounts data feasible
manual analysis hirschberg litman third set experiments allow
machine learning distinguish among cue phrases evaluate utility developing classification specialized particular cue phrases fourth set
experiments consider examples multiple cue phrase corpus
classifiable cue phrases set experiments attempt predict third classification
unknown well classifications discourse sentential finally within
four sets experiments individual experiment learns classification model
different feature representation training data experiments consider features
isolation comparatively evaluate utility individual feature classification
experiments consider linguistically motivated sets features gain insight
feature interactions

machine learning inputs

section describes inputs machine learning programs namely
names classifications learned names possible values fixed set


filitman

classification
judge judge
cue phrases
non conjuncts

total



classifiable cue phrases
discourse sentential







unknown
















table determining classification cue phrases
features training data specifying class feature values example
training set
classifications

first input learning program specifies names fixed set classifications
hirschberg litman way classification cue phrases judges hirschberg
litman transformed classifications used machine learning programs
shown table recall section judge classified cue phrase
discourse sentential ambiguous classifications shown table
discussed section classifiable cue phrases cue phrases judges
classified discourse sentential usages thus machine learning
experiments cue phrase assigned classification discourse judges classified
discourse shown column table similarly cue phrase assigned
classification sentential judges classified sentential shown column
examples full corpus classifiable
non conjuncts classifiable
machine learning experiments third cue phrase classification
considered particular cue phrase assigned classification unknown
hirschberg litman classified ambiguous shown column
unable agree upon classification shown
columns full corpus cue phrases judged ambiguous
judges cases true disagreement cue phrases
judged ambiguous first judge classified second judge
conjunctions removed corpus
examples judged ambiguous judges instances actually
instances essentially instance generally indeed
case true disagreement instance
cue phrases instance otherwise judged ambiguous
first judge
features

second component input learning program specifies names potential
values fixed set features set primitive features considered learning
experiments shown figure feature values numeric value one
fixed set user defined symbolic values feature representation shown follows
representation hirschberg litman except noted length intonational phrase p

ficue phrase classification machine learning

prosodic features

length intonational phrase p l integer
position intonational phrase p p integer
length intermediate phrase l integer
position intermediate phrase p integer
composition intermediate phrase c cue phrases
accent h l l h l h h l h l deaccented ambiguous
accent h l complex deaccented ambiguous
textual features
preceding cue phrase c p true false na
succeeding cue phrase c true false na
preceding orthography p comma dash period paragraph false na
preceding orthography p true false na
succeeding orthography comma dash period false na
succeeding orthography true false na
part speech pos article coordinating conjunction cardinal numeral subordinating conjunction
preposition adjective singular mass noun singular proper noun intensifier adverb verb base form
na

lexical feature

token actually although basically essentially except finally first

generally however indeed look next ok otherwise right say second see similarly
since therefore well yes

figure representation features use c cgrendel
l length intermediate phrase l represent number words intonational
intermediate phrases containing cue phrase respectively feature coded
data coded although used later multiple cue phrase
data position intonational phrase p p position intermediate phrase p use
numeric values rather earlier symbolic values e g first figure composition
intermediate phrase c replaces value alone meaning phrase contained
example cue phrase example plus cue phrases figure
primitive values cue phrases whose disjunction equivalent
alone c uses value rather alone used figure accent
uses value ambiguous represent cases prosodic analysis yields
disjunction e g h l h accent represents symbolic values
feature accent abstract level description particular l h
l h h l h l represented separate values single value
superclass complex useful abstractions often learning
process explicitly represented advance prosodic feature representation
potential automated see section
textual features value na applicable ects fact recorded
examples included transcription done independently


filitman

studies performed hirschberg litman coding used hirschberg
litman preceding cue phrase c p succeeding cue phrase c represented actual
cue phrase e g preceding succeeding cue phrase value
true encodes cases prosodic feature set preceding orthography
p succeeding orthography represent symbolic values
preceding orthography p succeeding orthography respectively
abstract level description e g comma dash period represented separate values
single value true done reliability coding
detailed transcriptions orthography known part speech pos represents
part speech assigned cue phrase church program tagging part speech
unrestricted text church program assign approximately different
values subset values actually assigned cue phrases
transcripts corpora shown figure finally lexical feature token
study represents actual cue phrase described
training data

final input learning program training data e set examples
class feature values specified consider following utterance taken
multiple cue phrase corpus hirschberg litman
example welcomed time get
business conference
utterance contains two cue phrases corresponding two instances
brackets parentheses illustrate intonational intermediate phrases respectively
contain example cue phrases note single intonational phrase contains
examples example uttered different intermediate phrase
interested feature length intonational phrase p l two examples would
represented training data follows
p l class
discourse
sentential
first column indicates value assigned feature p l second column
indicates example classified thus length intonational phrase
containing first instance words example cue phrase classified
discourse usage interested feature composition intermediate
phrase c two examples would instead represented training data follows
c class
discourse
sentential
intermediate phrase containing first instance contains
cue phrase intermediate phrase containing second instance
contains well lexical items cue phrases note
value p l examples value c different


ficue phrase classification machine learning

machine learning outputs

output machine learning programs classification c model
expressed decision tree consists leaf node class assignment
decision node test feature one branch subtree possible outcome
test following example illustrates non graphical representation decision
node testing feature n possible values
test


elseif testn



tests form feature operator value feature name feature e g
accent value valid value feature e g deaccented features
symbolic values e g accent one branch symbolic value operator
used features numeric values e g length intonational phrase
two branches comparing numeric value threshold value operators
used given decision tree cue phrase classified starting
root tree following appropriate branches leaf reached section
shows example decision trees produced c
cgrendel classification model expressed ordered set rules
following form
test testk class
part rule conjunction tests values varying features
tests form feature operator value c feature name
feature value valid value feature unlike c operators

used features symbolic values used features numeric
values part rule specifies class assignment e g discourse given set
rules cue phrase classified rule whose part satisfied
two rules rules disagree class example cgrendel
applies one two con ict resolution strategies chosen user choose first rule
choose rule accurate data experiments reported use
second strategy rules cgrendel assigns default class section
shows example rules produced cgrendel
c cgrendel learn classification greedy search guided
information gain metric c uses divide conquer process training examples
recursively divided subsets tests discussed subsets
belong single class test chosen divide examples maximizes
metric called gain ratio local measure progress consider
subsequent tests metric information theory discussed detail
quinlan test selected backtracking ideally set chosen
tests small final decision tree cgrendel generates set rules
method called separate conquer highlight similarity divide
conquer
additional type test may invoked c option



filitman

many rule learning systems generate hypotheses greedy strategy
rules added rule set one one effort form small cover
positive examples rule turn created adding one condition
another antecedent rule consistent negative
data cohen
although cgrendel claimed two advantages c advantages
come play experiments reported first rules appear easier
people understand decision trees quinlan however cue phrase
classification task decision trees produced c quite compact thus easily
understood furthermore rule representation derived c decision trees
program c rules second cgrendel allows users exploit prior knowledge
learning constraining syntax rules learned however
prior knowledge exploited cue phrase experiments main reason
c cgrendel increase reliability comparisons machine
learning manual particular comparable obtained
c cgrendel performance differences learned manually
derived classification less likely due specifics particular learning
program likely ect learned manual distinction

evaluation

output machine learning experiment classification model
learned training data learned qualitatively evaluated examining linguistic content comparing manually derived
figure learned quantitatively evaluated examining error
rates testing data comparing error rates error
rates shown table error rate classification model computed
model predict classifications set examples classifications already
known comparing predicted known classifications cue phrase domain
error rate computed summing number discourse examples misclassified
sentential number sentential examples misclassified discourse dividing
total number examples
error rates learned classification estimated two methodologies train test error rate estimation weiss kulikowski holds test
set examples seen training completed model
developed examining training examples error model estimated
model classify test examples evaluation method used
hirschberg litman resampling method cross validation weiss kulikowski
estimates error rate multiple train test experiments example fold cross validation instead dividing examples training test sets runs
learning program performed total set examples randomly divided
disjoint test sets run thus uses examples test set training
remaining testing note iteration cross validation
learning process begins scratch thus classification model learned
training sample estimated error rate obtained averaging error rate test

ficue phrase classification machine learning

ing portion data runs method make sense
humans computers truly ignore previous iterations sample sizes hundreds
classifiable subset multiple cue phrase sample classifiable non conjunct
subset provide examples respectively fold cross validation often provides
better performance estimate hold method weiss kulikowski
major advantage cross validation examples eventually used testing
almost examples used given training run
best performing learned identified comparing error rates
error rates learned manually derived error rates
determine whether fact error rate e lower another error rate e
significant statistical inference used particular confidence intervals two
error rates computed confidence level error rate estimated
single error rate test set e train test methodology confidence
interval computed normal approximation binomial distribution freedman
et al error rate estimated average multiple error
rates e cross validation methodology confidence interval computed
table freedman et al upper bound confidence interval e
lower lower bound confidence interval error rate e
difference e e assumed significant

experimental conditions
section describes conditions used set machine learning experiments
experiments differ use training testing corpora methods estimating error
rates features classifications used actual experiments
presented section
four sets experiments

learning experiments conceptually divided four sets experiment
first set estimates error rate train test method training
testing samples used hirschberg litman data
two subsets multiple cue phrase corpus respectively allows direct comparison
manual machine learning approaches however prosodic experiments
conducted hirschberg litman replicated textual training testing
conditions replicated original training corpus first minutes
multiple cue phrase corpus litman hirschberg subset rather disjoint
test corpus full minutes multiple cue phrase corpus hirschberg
litman
contrast experiment second set uses cross validation estimate error
rate furthermore training testing samples taken multiple cue
phrase corpus experiment uses examples multiple cue phrase
data training remaining testing thus experiment second
set trains much larger amounts data classifiable examples classifiable
thanks william cohen suggesting methodology



filitman

prosody
hl features
phrasing
length
position
intonational
intermediate
text
adjacency
orthography
preceding
succeeding
speech text

p l
x
x
x
x

p p l p
x x x
x
x x x
x
x
x
x
x x

c
x x x
x x x
x
x

c p

x
x
x

x

x

x

x

x

x

x

x

c p

x
x
x
x

p

pos

x

x

x

x

x
x

x
x

x

x

x

x

x
x

x
x

x

x

table multiple feature sets components
non conjuncts experiment first set nows reliability
testing compromised due use cross validation weiss kulikowski
experiment third set replicates experiment second set exception learning program allowed distinguish cue phrases
done adding feature representing cue phrase feature token figure
experiment second set since potential use lexical feature
noted used hirschberg litman experiments provide qualitatively linguistic insights data example features may
used differently predict classifications different cue phrases sets cue phrases
finally experiment fourth set replicates experiment first second
third set exception examples multiple cue phrase corpus
considered practice learned cue phrase classification model
likely used classify cue phrases even dicult human judges
classify experiments fourth set allow learning programs attempt
learn class unknown addition classes discourse sentential
feature representations within experiment sets

within four sets experiments individual experiment represents
data different subset available features first data represented
single feature sets corresponding prosodic textual feature shown
figure experiments comparatively evaluate utility individual feature
classification representations example shown illustrate data
represented single feature set p l single feature set c
second data represented multiple feature sets shown table
sets contains linguistically motivated subset least features
first sets use prosodic features prosody considers prosodic features
coded example cue phrase hl features considers coded features
used model shown figure phrasing considers features
intonational intermediate phrases containing example cue phrase e length


ficue phrase classification machine learning

example

welcomed time get business conference
p p l p c


c p c p p pos class


h l complex f

par
f
f
adv disc


h
h

f
f
f
f
f
adv sent


p l



figure representation example feature set speech text
phrase position example phrase composition phrase length position
consider one features respect intonational
intermediate phrase conversely intonational intermediate consider one type
phrase consider features next sets use textual features text
considers textual features adjacency orthography consider single textual
feature consider preceding succeeding immediate context preceding
succeeding consider contextual features relating orthography cue phrases
limit context last set speech text uses prosodic textual features
figure illustrates two example cue phrases example would represented
speech text consider feature values first example cue phrase since
example first lexical item intonational intermediate phrases
contain position phrases p p p since intermediate phrase
containing cue phrase contains lexical items length l word
composition c cue phrase values indicate
intonational phrase described sequence tones complex pitch accent h l
associated cue phrase respect textual features utterance
transcribed began paragraph thus example cue phrase
preceded another cue phrase c p preceded form orthography p
p since example cue phrase immediately followed another instance
transcription cue phrase succeeded another cue phrase c
succeeded orthography finally output part
speech tagging program run transcript corpus yields value adverb
cue phrase part speech pos
first set experiments replicate prosodic experiments conducted
hirschberg litman cue phrases represented subset feature sets consist prosodic features second set experiments examples
represented different feature sets single feature sets
multiple feature sets third set experiments examples represented
tokenized feature sets constructed adding lexical feature token figure
cue phrase described single multiple feature sets
second set experiments tokenized feature sets referred names
single multiple feature sets concatenated following illustrates
two cue phrases example would represented p l
p l
class
discourse
sentential


filitman

representation similar p l representation shown earlier except second
column indicates value assigned feature token



section examines running two learning programs c cgrendel four sets cue phrase classification experiments described learned
classification compared classification shown figure
error rates learned classification compared error
rates shown table error rates learned
seen suggest machine learning useful automating generation
linguistically viable classification classification generating classification
perform lower error rates manually developed hypotheses adding
body linguistic knowledge regarding cue phrases

experiment set replicating hirschberg litman

first group experiments replicate training testing evaluation conditions
used hirschberg litman order investigate well machine learning
performs comparison manual development cue phrase classification
figure shows best performing prosodic classification learned two
machine learning programs top figure replicates manually derived prosodic
model figure ease comparison prosodic features used
represent training examples e example represented
feature set prosody table classification learned shown
manually derived model top figure note learning
programs decision tree learned smaller feature sets phrasing
position used represent data bottom portion figure shows
classification learned examples represented
single prosodic feature position intonational phrase p p model
learned examples represented multiple feature set intonational
recall c represents learned classification model decision tree
level tree shown indentation specifies test single feature branch
every possible outcome test branch lead assignment class
another test example c classification model learned prosody classifies
cue phrases two features position intonational phrase p p position
intermediate phrase p note available features prosody recall
table used decision tree tree initially branches value
feature position intonational phrase first branch leads class assignment
discourse second branch leads test feature position intermediate phrase
first branch test leads class assignment discourse second branch
leads sentential c produces unsimplified pruned decision trees goal
experiment set feature set prosody contain features p l l recall
phrasal length coded later multiple cue phrase study
ease comparison figure original symbolic representation feature value used
rather integer representation shown figure



ficue phrase classification machine learning

manually derived prosodic model repeated figure

composition intermediate phrase alone
elseif composition intermediate phrase alone
position intermediate phrase first
accent deaccented
elseif accent l
elseif accent h
elseif accent complex
elseif position intermediate phrase first

discourse

discourse

discourse

sentential

sentential

sentential










decision tree learned prosody phrasing position c

position intonational phrase first
elseif position intonational phrase first
position intermediate phrase first
elseif position intermediate phrase first
discourse

discourse
sentential

ruleset learned prosody phrasing position cgrendel

position intonational phrase first position intermediate phrase first
default discourse

sentential

decision tree learned p p intonational c

position intonational phrase first
elseif position intonational phrase first

discourse
sentential

ruleset learned p p intonational cgrendel

position intonational phrase first
default discourse

sentential

figure example c cgrendel classification learned different prosodic
feature representations data



filitman

model
classifiable cue phrases n classifiable non conjuncts n
p p


prosody


phrasing


position


intonational


manual prosodic



table confidence intervals error rates best performing cgrendel
prosodic classification testing data training data corpus
testing data multiple cue phrase corpus
pruning process take complex decision tree may overfitted
training data produce tree comprehensible whose accuracy
comprised quinlan since almost trees improved pruning quinlan
simplified decision trees considered
contrast cgrendel represents learned classification model set
rules rule specifies conjunction tests features
assignment class example cgrendel ruleset learned prosody classifies
cue phrases two features position intonational phrase p p position
intermediate phrase p two features used c decision tree
values features first rule applies cue phrase classified
sentential value feature first default applies cue phrase
classified discourse
examination learned classification figure shows
comparable content portion manually derived model classifies cue
phrases solely phrasal position line particular classification
say cue phrase initial phrasal position classify sentential
hand manually derived model assigns class sentential given
initial phrasal position conjunction certain combinations phrasal composition
accent learned classification instead classify cue phrase discourse
cases shown discrimination manually obtained model
significantly improve performance compared learned classification
fact one case significantly degrades performance
error rates learned classification training data
developed follows learned prosody phrasing
position learned p p intonational recall
section error rate manually developed prosodic model figure
training data
table presents confidence intervals error rates best performing
cgrendel prosodic classification ease comparison row labeled manual
prosodic presents error rates manually developed prosodic model figure
two test sets originally shown table table includes
cgrendel whose performance matches exceeds manual performance


ficue phrase classification machine learning

comparison error rates learned manually developed suggests
machine learning effective technique automating development cue phrase
classification particular within test set confidence interval
error rate classification learned multiple feature sets prosody
phrasing position overlaps confidence interval error rate
manual prosodic model true error rates p p intonational
classifiable non conjunct test set thus machine learning supports automatic construction variety cue phrase classification achieve similar performance
manually constructed
p p intonational classifiable cue phrase test set
shown italics suggest machine learning may useful improving
performance although simple classification model learned p p intonational performs worse manually derived model training data tested
classifiable cue phrases learned model upper bound error rate
outperforms manually developed model lower bound error rate
suggests manually derived model might overfitted training data
e prosodic feature set useful classifying generalize
cue phrases noted use simplified learned classification helps
guard overfitting learning ease inducing classification
many different sets features machine learning supports generation
evaluation wide variety hypotheses e g p p high performing
optimal performing model training data
note manual prosodic manual performs significantly better smaller test
set contain cue phrases contrast
performance improvement p p intonational smaller test set significant
suggests manually derived model generalize well learned

finally feature sets shown table decision trees produced c perform
error rates rulesets produced cgrendel test sets recall
figure c decision trees cgrendel rules fact semantically
equivalent feature set fact comparable obtained c
cgrendel adds extra degree reliability experiments particular
duplication suggests ability match perhaps even improve
upon manual performance machine learning due specifics
learning program

experiment set different training sets
second group experiments evaluate utility training larger amounts
data done fold cross validation estimate error run
examples sample used training runs
examples used testing addition experiments second set take
training testing data multiple cue phrase corpus contrast previous
set experiments training data taken corpus
seen changes improve learned classification


filitman

model
classifiable cue phrases n classifiable non conjuncts n
p l


p p


l


p


c








prosody


hl features


phrasing


length


position


intonational


intermediate


manual prosodic



table confidence intervals error rates cgrendel prosodic classification testing data training testing done multiple
cue phrase corpus cross validation
perform lower comparable error rates compared manually developed

prosodic

table presents error rates classification learned cgrendel
different prosodic experiments experiment sets c error rates
presented appendix numeric cell shows confidence interval
error rate equal error percentage obtained cross validation margin
error standard errors table top portion table considers
learned single prosodic feature sets figure middle portion
considers learned multiple feature sets table last row
considers manually developed prosodic model error rates shown italics indicate
performance learned classification model exceeds performance
manual model given test set error rates shown parentheses indicate
opposite case performance manual model exceeds performance
learned model cases omitted table
experiment set comparison error rates learned manually
developed suggests machine learning effective technique
automating development cue phrase classification improving
performance evaluated classifiable cue phrase test set five learned
improved performance compared manual model except c
perform least comparably manual model note experiment set two
learned outperformed manual model five learned performed
least comparably ability use large training sets thus appears advantage
automated


ficue phrase classification machine learning

manually derived prosodic model repeated figure

composition intermediate phrase alone
elseif composition intermediate phrase alone
position intermediate phrase first
accent deaccented
elseif accent l
elseif accent h
elseif accent complex
elseif position intermediate phrase first

discourse

discourse

discourse

sentential

sentential

sentential










decision tree learned p p c

position intonational phrase
elseif position intonational phrase

discourse
sentential

ruleset learned p p cgrendel

position intonational phrase
default discourse

sentential

decision tree learned prosody c

position intonational phrase
position intermediate phrase
elseif position intermediate phrase
elseif position intonational phrase
length intermediate phrase
elseif length intermediate phrase

discourse
sentential

discourse
sentential

ruleset learned prosody cgrendel

position intonational phrase length intermediate phrase
position intonational phrase length intonational phrase
length intermediate phrase length intonational phrase accent h
length intermediate phrase length intonational phrase accent h l
length intermediate phrase accent deaccented
length intermediate phrase length intonational phrase accent l
sentential

sentential

sentential
sentential

sentential

sentential

default discourse

figure example c cgrendel classification learned different prosodic
feature representations classifiable cue phrases multiple cue phrase
corpus
tested classifiable non conjuncts error rate manually
derived model decreases machine learning useful automating improving
performance might ect fact manually derived theories already achieve
optimal performance respect examined features less noisy subcorpus
automatically derived theory subcorpus smaller
training set used larger subcorpus
examination best performing learned classification shows
quite comparable content relevant portions prosodic model figure
often contain linguistic insights consider classification model learned
single feature position intonational phrase p p shown near top figure


filitman

learned classification say cue phrase initial
position intonational phrase classify sentential otherwise classify discourse
note correspondence line manually derived prosodic model note
classification comparable p p classification learned
experiment set shown figure despite differences training data
fact single prosodic feature position intonational phrase p p classify cue
phrases least well complicated manual multiple feature learned
learning experiments
figure illustrates complex classification learned prosody
largest prosodic feature set c model similar lines manual
model length value equivalent composition value alone ruleset
induced prosody cgrendel first rules correlate sentential status
among things non initial position second rules h h l
accents rules similar lines figure however last rules
ruleset correlate accent l sentential status phrase
certain length lines figure provide different interpretation
take length account recall length coded hirschberg litman
test data length thus never used generate revise prosodic model
utility length experiment set
although shown learned phrasing position intonational
outperform manual model seen table correspond
feature sets supersets p p subsets prosody
textual

table presents error rates classification learned cgrendel
different textual experiments unlike experiments involving prosodic feature sets
none learned textual perform significantly better manually derived
model however suggest machine learning still effective technique
automating development cue phrase classification particular five
learned p p text orthography preceding perform comparably
manually derived model test sets note five learned
five textual feature sets include feature p p recall figure
table perform significantly better remaining learned
textual
figure shows best performing learned textual note similarity
manually derived model prosodic best performing single feature
perform comparably learned multiple features fact cgrendel
rulesets learned multiple feature sets orthography preceding identical
rulesets learned single features p p even though features
available use corresponding error rates table identical due
different feature values two figures ect fact phrasal position represented
corpus symbolic values figure multiple cue phrase corpus
integers figure
tests feature x feature merged figure simplicity e g feature
x



ficue phrase classification machine learning

model
classifiable cue phrases n classifiable non conjuncts n
c p


c


p


p








pos


text


adjacency


orthography


preceding


succeeding


manual textual



table confidence intervals error rates cgrendel textual classification testing data training testing done multiple
cue phrase corpus cross validation
manually derived textual model repeated figure
preceding orthography true discourse
elseif preceding orthography false sentential
decision tree learned p text orthography preceding c

preceding orthography na
elseif preceding orthography false
elseif preceding orthography true

discourse
sentential
discourse

ruleset learned p p orthography preceding cgrendel

preceding orthography false
default discourse

sentential

ruleset learned text cgrendel

preceding orthography false
part speech article
default discourse

sentential

sentential

figure example c cgrendel classification learned different textual
feature representations classifiable cue phrases multiple cue phrase
corpus
estimation cross validation cgrendel model text incorporates feature
part speech c text orthography preceding identical p
prosodic textual

table presents error rates classification learned cgrendel
data represented speech text complete set prosodic textual features recall


filitman

model
classifiable cue phrases n classifiable non conjuncts n
speech text


manual prosodic


manual textual



table confidence intervals error rates cgrendel prosodic textual
classification model testing data training testing done multiple cue phrase corpus cross validation
table since hirschberg litman develop similar classification model
combined types features comparison last two rows error rates
separate prosodic textual learned model compared
manual prosodic model classifiable cue phrases testing learning
significant performance improvement consistent discussed
several learned prosodic performed better manually derived prosodic
model test set performance speech text significantly better worse
performance best prosodic textual learned tables
respectively
figure shows c cgrendel hypotheses learned speech text c
model classifies cue phrases prosodic textual features performed best
isolation position intonational phrase preceding orthography discussed
conjunction additional feature length intermediate phrase appears
model learned prosody figure line manually derived
textual model learned model associates presence preceding orthography
class discourse unlike line however cue phrases preceded orthography
may classified discourse sentential prosodic feature values
available use textual model branch learned decision tree
corresponding last three lines similar lines manually
derived prosodic model recall length value equivalent composition value
alone
cgrendel model uses similar features used c well prosodic
feature accent used prosody figure textual features part speech
used text figure preceding cue phrase c unlike line
manually derived textual model cgrendel model classifies cue phrases lacking
preceding orthography sentential conjunction certain feature values
unlike line manual model learned model classifies cue phrases
preceding orthography sentential orthography comma feature values
present finally third fifth learned rules elaborate line additional
prosodic well textual features first last learned rules elaborate line

experiment set adding feature token

experiment third group replicates experiment second group
exception data representation includes lexical feature token


ficue phrase classification machine learning

manually derived prosodic model repeated figure

composition intermediate phrase alone
elseif composition intermediate phrase alone
position intermediate phrase first
accent deaccented
elseif accent l
elseif accent h
elseif accent complex
elseif position intermediate phrase first

discourse

discourse

discourse

sentential

sentential

sentential

manually derived textual model repeated figure

preceding orthography true
elseif preceding orthography false












discourse
sentential

decision tree learned speech text c

position intonational phrase
preceding orthography na
elseif preceding orthography true
elseif preceding orthography false
length intermediate phrase
elseif length intermediate phrase
length intermediate phrase
elseif length intermediate phrase
elseif position intonational phrase
length intermediate phrase
elseif length intermediate phrase
discourse

discourse

discourse
discourse
sentential

discourse

sentential

ruleset learned speech text cgrendel

preceding orthography false position intonational phrase
preceding orthography false length intermediate phrase
preceding orthography false length intonational phrase preceding cue phrase na
accent h
preceding orthography comma length intermediate phrase length intonational phrase
part speech adverb
preceding orthography comma length intonational phrase accent h
preceding orthography comma length intermediate phrase
length intonational phrase
position intonational phrase length intermediate phrase
preceding cue phrase na
sentential

sentential

sentential

sentential

sentential

sentential

default discourse

sentential

figure c cgrendel classification learned prosodic textual feature representation classifiable cue phrases multiple cue phrase corpus



filitman

model
classifiable cue phrases n classifiable non conjuncts n
p l


p p


l


p


c








prosody


hl features


phrasing


length


position


intonational


intermediate


manual prosodic



table confidence intervals error rates cgrendel prosodic tokenized classification testing data training testing done
multiple cue phrase corpus cross validation
figure experiments investigate performance changes classification allowed treat different cue phrases differently seen learning
tokenized feature sets often improves performance learned classification
addition classification contain linguistic information regarding particular tokens e g
prosodic

table presents error learned classification test sets
multiple cue phrase corpus tokenized prosodic feature sets error
rates italics indicate performance learned classification model meaningfully
exceeds performance manual prosodic model consider feature
token
one way improvement obtained adding feature token seen
comparing performance learned manually derived table six
cgrendel classification lower italicized error rates manual model
table five italicized thus adding feature token
additional learned model length outperforming manually derived model
conversely table learned perform significantly worse manually
derived manual contrast table several non tokenized perform worse
manual model c larger test set p l l c length
non conjunct test set
improvement obtained adding feature token seen comparing
performance tokenized table non tokenized table versions
model convenience cases tokenization yields improvement
highlighted table table shows error rate tokenized versions
feature sets significantly lower error non tokenized versions p l c


ficue phrase classification machine learning

model
p l
l
c


length

classifiable cue phrases n
non tokenized tokenized











classifiable non conjuncts n
non tokenized
tokenized













table cases adding feature token improves performance prosodic
model
length test sets l non conjunct test set note
overlap feature sets table discussed previous paragraph
figure shows several tokenized single feature prosodic classification first
cgrendel model figure shows ruleset learned p l reduces
error rate p l length intonational phrase
trained tested classifiable non conjuncts table note first rule
uses prosodic feature rules experiment sets fact
similar line manual model recall length value equivalent
composition value alone however unlike rules previous experiment sets
next rules use prosodic feature lexical feature token unlike
rules previous experiment sets remaining rules classify cue phrases
feature token examination learned rulesets figures shows
cue phrases often appear last type rule cue phrases
example finally however ok fact discourse usages multiple
cue phrase corpus cue phrases classifying cue phrases token
corresponds classifying cue phrases default class frequent type
usage multiple cue phrase corpus recall use non tokenized default class
model table
second example shows ruleset learned c composition intermediate
phrase first rule corresponds line manually derived model
next six rules classify particular cue phrases discourse independently value c
note although model cue phrase say classified token
previous model sophisticated strategy classifying say could found
third example shows cgrendel ruleset learned accent first
rule corresponds line manually derived prosodic model contrast line
however cgrendel uses deaccenting predict discourse tokens say
token finally however ok discourse assigned
accents deaccented cases sentential assigned default similarly
contrast line complex accent l h predicts discourse cue phrases
indeed finally however ok sentential
otherwise
discussed relation figure c values cue phrases multiple cue phrase
corpus replace value alone corpus



filitman

manually derived prosodic model repeated figure

composition intermediate phrase alone
elseif composition intermediate phrase alone
position intermediate phrase first
accent deaccented
elseif accent l
elseif accent h
elseif accent complex
elseif position intermediate phrase first










discourse

discourse

discourse

sentential

sentential

sentential

ruleset learned p l cgrendel

length intonational phrase
length intonational phrase token although
length intonational phrase token indeed
length intonational phrase token say
length intonational phrase token
length intonational phrase token well
token finally
token
token however
token
token ok
token otherwise
token
discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

default sentential

ruleset learned c cgrendel

composition intermediate phrase
token finally
token however
token
token ok
token say
token

discourse

discourse

discourse

discourse

discourse

discourse

discourse

default sentential

ruleset learned cgrendel

accent l
accent deaccented token say
accent deaccented token
accent l h token
accent l h token indeed
token finally
token however
token
token ok
discourse

discourse

discourse
discourse

discourse

discourse

discourse

discourse

discourse

default sentential

figure example cgrendel classification learned different tokenized
prosodic feature representations classifiable non conjuncts multiple
cue phrase corpus



ficue phrase classification machine learning

model
classifiable cue phrases n classifiable non conjuncts n
c p


c


p


p








pos


text


adjacency


orthography


preceding


succeeding


manual textual



table confidence intervals error rates cgrendel textual tokenized classification testing data training testing done
multiple cue phrase corpus cross validation
summarize prosodic experiment set features relating
length composition accent useful isolation predicting classification cue phrases fact quite useful predicting class individual cue
phrases subsets cue phrases recall experiment sets
without token prosodic feature position intonational phrase useful
isolation
textual

table presents error learned classification test sets
multiple cue phrase corpus tokenized textual feature sets experiment set table none cgrendel classification lower italicized
error rates manual model however adding feature token improve
performance many learned rulesets following unlike
non tokenized counterparts longer outperformed manual model
succeeding larger test set c p c pos adjacency
succeeding non conjunct test set
improvement obtained adding feature token seen comparing
performance tokenized table non tokenized table versions
model shown table table shows error rates
tokenized versions feature sets significantly lower error nontokenized versions c p c pos adjacency test sets p
text succeeding non conjunct test set note overlap feature
sets table discussed previous paragraph
figure shows several tokenized single textual feature classification first
cgrendel model shows ruleset learned c p preceding cue phrase
reduces error rate c p trained tested
classifiable non conjuncts table ruleset correlates preceding cue phrases
discourse usages indeed omitted transcriptions


filitman

manually derived textual model repeated figure

preceding orthography true
elseif preceding orthography false

discourse
sentential

ruleset learned c p cgrendel

preceding cue phrase true token indeed
preceding cue phrase na token
preceding cue phrase na token
preceding cue phrase na token
token although
token finally
token however
token ok
token say
token similarly

discourse
discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

default sentential

discourse

ruleset learned p cgrendel

preceding orthography false
preceding orthography comma token
sentential

default discourse

sentential

ruleset learned cgrendel

succeeding orthography comma
succeeding orthography false token
succeeding orthography na
token although
token finally
token
token ok
token say
discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

default sentential

ruleset learned pos cgrendel

part speech adverb token finally
part speech singular proper noun token
part speech adverb token however
part speech adverb token indeed
part speech subordinating conjunction token
token although
token
token say
token ok
discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

default sentential

figure example cgrendel classification learned different tokenized textual
feature representations classifiable non conjuncts multiple cue phrase
corpus



ficue phrase classification machine learning

model
c p
c
p


pos
text
adjacency
succeeding

classifiable cue phrases n
non tokenized tokenized










classifiable non conjuncts n
non tokenized
tokenized



















table cases adding feature token improves performance textual
model
discourse usages classifications rest cue phrases predicted
feature token
second example shows cgrendel ruleset learned p preceding orthography ruleset correlates preceding orthography sentential usages cue
phrases manually derived model learned experiment
set unlike however cue phrase classified sentential
even preceded orthography namely comma
third example shows cgrendel ruleset learned succeeding orthography ruleset correlates presence succeeding commas discourse usages
cue phrases except cue phrase classified discourse usage without
succeeding orthography model correlates cue phrases omitted
transcript discourse usages classifications rest cue phrases
predicted feature token
last example shows cgrendel ruleset learned pos part speech
ruleset classifies certain cue phrases discourse usages depending part ofspeech token well independently part speech
finally figure shows classification model learned text largest tokenized textual feature set note three four features used tokenized single
textual feature figure incorporated tokenized multiple textual
feature model
summarize textual experiment set features adjacent cue phrases succeeding orthography part speech useful isolation
predicting classification cue phrases fact quite useful conjunction
feature token recall experiment set without token
textual features preceding orthography preceding orthography useful
isolation
prosodic textual

table presents error rates classification learned cgrendel
data represented speech text complete set prosodic textual


filitman

ruleset learned text cgrendel

preceding orthography false
preceding orthography comma token although
preceding orthography comma token
preceding orthography comma token
succeeding orthography false preceding cue phrase na token similarly
token actually
token first
token since
token yes
sentential

sentential

sentential

sentential

sentential

sentential

sentential

sentential

sentential

default discourse

figure cgrendel classification model learned tokenized multiple textual feature
representation classifiable non conjuncts multiple cue phrase corpus
model
classifiable cue phrases n classifiable non conjuncts n
speech text


manual prosodic


manual textual



table confidence intervals error rates cgrendel
prosodic textual tokenized classification testing data training
testing done multiple cue phrase corpus cross validation
features experiment set performance speech text better
performance best learned tokenized prosodic textual tables
respectively
comparison tables shows feature set speech text tokenization improve performance contrast prosodic textual feature
sets tokenization improves performance many learned namely
shown tables

experiment set adding classification ambiguous
practice cue phrase classification model classify cue phrases
recording text classifiable experiments fourth set
replicate experiments experiment sets exception cue
phrases multiple cue phrase corpus used means cue phrases
classified discourse sentential well unknown defined table experiment
set investigates whether machine learning explicitly recognize class unknown
recall studies hirschberg litman attempt predict class
unknown occur training corpus thus experiment set
class unknown similarly learned training data however unknown
examples added testing data experiment set obviously performance
degrade must incorrectly classify unknown example discourse


ficue phrase classification machine learning

sentential example tested full corpus example cue phrases
confidence intervals error rates p p intonational
recall tested subset corpus corresponding classifiable cue
phrases error table
unfortunately rerunning experiment sets promising
classifying cue phrases unknown despite presence examples
unknown learned still classify unknown cue phrases discourse
sentential example cgrendel used learning possible nontokenized phrasing speech text contain rules predict class unknown
furthermore contains one rule unknown
rules applies possible examples similarly four possible
tokenized length phrasing prosody speech text contain least one rule
class unknown compared training testing classifiable
cue phrases corpus error rate full corpus typically
significantly higher best performing model experiment set speech text
error rate confidence interval
sum experiment set addressed previously unexplored
literature ability develop classification predict discourse
sentential usages cue phrases usages human judges dicult classify
unfortunately experiments suggest learning classify cue
phrases unknown dicult perhaps training data recall
examples unknown additional features better could
obtained

discussion
experimental suggest machine learning useful tool automating
generation classification improving upon manually derived
experiment sets performance many learned classification
comparable performance manually derived addition tested
classifiable cue phrases several learned prosodic classification well
learned prosodic textual model outperform hirschberg litman manually derived
prosodic model experiment set shows learning tokenized feature sets even
improves performance especially non conjunct test set tokenized
non tokenized learned perform least well manually derived
many tokenized learned outperform non tokenized counterparts
textual classification outperform better prosodic classification advantage textual feature values obtained directly
transcript determining values prosodic features requires manual analysis see however section discussion feasibility automating prosodic
analysis addition transcript may available hand almost
high performing textual dependent orthography manual tran recall experiment sets constructed prosodic textual
prosodic textual model



filitman

scriptions prosodic features shown reliable across coders pitrelli et al
corresponding reliability orthography
examination best performing learned shows often comparable content relevant portions manually derived examination
provides contributions cue phrase literature example
experiment sets demonstrate utility classifying cue phrases
single prosodic feature phrasal position experiment set demonstrates utility
prosodic feature length textual feature preceding cue phrase classifying
cue phrases conjunction prosodic textual features finally
experiment set demonstrate even though many features useful
classifying cue phrases may nonetheless informative tokenized
form true prosodic features phrasal length phrasal composition
accent textual features adjacent cue phrases succeeding position
part speech

utility
machine learning experiments quite promising compared
manually derived classification already literature learned classification
often perform comparable higher accuracy thus machine learning
appears effective technique automating generation classification
however given experiments reported still rely manually created training
data discussion practical utility order
even given manually created training data established hirschberg
litman obtained even less automation experiments
already practical import particular manually derived cue phrase
classification used improve naturalness synthetic speech text tospeech system hirschberg text model text speech system
classifies cue phrase text synthesized discourse sentential
usage prosodic model system conveys usage synthesizing
cue phrase appropriate type intonation speech synthesis could
improved output made varied one higher performing
learned prosodic presented
could directly applied area text generation
example moser moore concerned implementation cue selection placement strategies natural language generation systems systems could
enhanced text cue phrase classification particularly
empirical studies performed holte many datasets accuracy
single feature rules decision trees often competitive accuracy complex learned

contrast prosodic features phrasal composition accent previously known useful
conjunction phrasal position hirschberg litman part ofspeech known useful conjunction orthography hirschberg litman
length adjacent cue phrases succeeding position used manually derived
hirschberg litman although length adjacent cue phrases shown useful
conjunction prosodic textual features experiment set



ficue phrase classification machine learning

tokenized additionally specify preceding succeeding orthography part ofspeech adjacent cue phrases appropriate discourse usages
finally could fully automated could used
natural language understanding systems enhancing ability recognize discourse
structure obtained litman passonneau passonneau
litman press suggest use cue phrases conjunction
features predict discourse structure outperform take cue phrases
account particular litman passonneau develop several explore
features cue phrases prosody referential noun phrases best combined
predict discourse structure quantitative evaluations best
performing incorporate use discourse usages cue phrases cue
phrases classified discourse phrasal position discussed section
discourse structure useful performing tasks anaphora resolution plan
recognition recent work shown discourse structure recognized
used improve retrieval text hearst speech sti eman
although prosodic features manually labeled hirschberg litman
recent suggesting least aspects prosody automatically
labeled directly speech example wightman ostendorf develop
able automatically recognize prosodic phrasing accuracy
measured comparing automatically derived labels hand marked labels accuracy slightly less human human accuracy recall experimental
learned single feature position intonational
phrase could automatically computed given automatic prosodic phrasing perform least well learned prosodic model similarly
accenting versus deaccenting automatically labeled accuracy wightman
ostendorf sophisticated labeling scheme distinguishes
four types accent classes somewhat similar prosodic feature accent used
labeled accuracy ostendorf ross press recall
experiment set tokenized learned accent classify cue phrases
good
although textual features automatically extracted transcript transcript manually created many natural language understanding systems
deal speech thus begin textual representations spoken language systems transcription process typically automated speech recognition
system although introduces sources error

related work
compared obtained machine learning previously
existing manually obtained used machine learning tool developing theories given linguistic data resulting experiment set
feature token considered siegel similarly uses machine learning
particular genetic learning classify cue phrases previously unstudied set textual features feature corresponding token well textual features
containing lexical orthographic item immediately left positions


filitman

right example siegel input consists one judge non ambiguous examples
taken data used hirschberg litman well additional examples
output form decision trees siegel reports estimated error rate
half corpus used training half testing siegel mckeown
propose method developing linguistically viable rulesets partitioning
training data produced induction
machine learning used several areas discourse analysis example learning used develop rules structuring discourse multi utterance
segments grosz hirschberg use classification regression tree system
cart brieman et al construct decision trees classifying aspects discourse
structure intonational feature values litman passonneau passonneau
litman press use system c construct decision trees classifying utterances discourse segment boundaries features relating prosody referential noun
phrases cue phrases addition c used develop anaphora resolution
training corpora tagged appropriate discourse information aone
bennett similarly mccarthy lehnert use c learn decision trees
classify pairs phrases coreferent soderland lehnert use
machine learning program id predecessor c support corpus driven knowledge
acquisition information extraction machine learning often
outperform manually derived alternatives litman passonneau passonneau litman press aone bennett mccarthy lehnert although statistical
inference used evaluate significance performance differences
finally machine learning used great success many areas
natural language processing discussed work researchers discourse
analysis concentrated direct application existing symbolic learning approaches
e g c comparison learning manual methods researchers
areas natural language processing addressed issues
addition applied much wider variety learning approaches concerned
development learning methods particularly designed language processing
recent survey learning natural language wermter riloff scheler illustrates
type learning approaches used modified particular
symbolic connectionist statistical hybrid approaches well scope
proved amenable use learning techniques e g grammatical
inference syntactic disambiguation word sense disambiguation

conclusion
demonstrated utility machine learning techniques cue phrase
classification machine learning supports automatic generation linguistically viable
classification compared manually derived already literature
many learned contain linguistic insights perform least
high higher accuracy addition ability automatically construct classification makes easier comparatively analyze utility alternative feature
representations data finally ease retraining makes learning
scalable extensible manual methods


ficue phrase classification machine learning

first set experiments presented used machine learning programs

cgrendel cohen c quinlan induce classification

preclassified cue phrases features used training data
hirschberg litman evaluated testing data
methodology used hirschberg litman second group experiments
used method cross validation train test testing data used
hirschberg litman third set experiments induced classification
feature token fourth set experiments induced classification
classification unknown
experimental indicate several learned classification including
extremely simple one feature significantly lower error rates
developed hirschberg litman one possible explanation handbuilt classification derived small training sets data became
available data used testing updating original contrast machine learning conjunction cross validation experiment set supported
building classification much larger amount data training
even learned derived small training set experiment
set showed learning helped guard overfitting
training data
prosodic classification model developed hirschberg litman demonstrated utility combining phrasal position phrasal composition accent
best performing prosodic experiment sets demonstrated phrasal
position fact even useful predicting cue phrases used
high performing classification experiment set demonstrated utility classifying cue phrases prosodic feature length textual feature
preceding cue phrase combination features
machine learning made easy retrain training examples became available experiment set machine learning made easy retrain
features become available particular value feature token
added representations experiment set trivial relearn
experiment set allowing learning programs treat cue phrases individually improved accuracy learned classification added
body linguistic knowledge regarding cue phrases experiment set demonstrated
useful classifying cue phrases prosodic features
phrasal length phrasal composition accent textual features adjacent
cue phrases succeeding position part speech fact useful used
conjunction feature token
final advantage machine learning ease inducing classification many different sets features supports exploration comparative
utility different knowledge sources especially useful understanding tradeoffs accuracy model set features considered
example might worth effort code feature automatically obtainable
expensive automatically obtain adding feature significant
improvement performance


filitman

sum suggest machine learning useful tool
cue phrase classification amount data precludes effective human analysis
exibility afforded easy retraining needed e g due additional training
examples features classifications analysis goal gain better
understanding different aspects data
several areas future work remain first still room performance improvement error rates best performing learned even though outperform
manually derived perform error rates teens note
features coded discussed hirschberg litman considered
may possible lower error rates considering types
prosodic textual features e g contextual textual features siegel
features proposed connection general topic discourse
structure different kinds learning methods second experiment set
previous literature yet predicting
cue phrase usage classified unknown rather discourse sentential
may possible improve performance existing learned
considering features learning methods perhaps performance could improved providing training data finally currently open question whether
textual developed transcripts speech applicable
written texts textual thus need developed written texts training
data machine learning continue useful tool helping address
issues

appendix c experiment sets
tables present c error rates experiment sets c
experiment set shown non tokenized columns comparison
tables shows except larger test set c prosodic error rates
fall within cgrendel confidence intervals similar comparison tables
shows except p larger test set c textual error rates fall within
cgrendel confidence intervals finally comparison tables shows
c error rate speech text falls within cgrendel confidence interval fact
comparable cgrendel c generally obtained suggests ability
automate well improve upon manual performance due specifics
learning program
c experiment set shown tokenized columns tables comparison tables shows error rates c
cgrendel similar experiment set however error rates reported
tables use default c cgrendel options running learning programs comparable performance two learning programs fact generally
achieved overriding one default c options detailed quinlan
default c creates separate subtree possible feature value
might appropriate many values feature situation characterizes feature token c default option changed allow feature values
grouped one branch decision tree problematic c error rates


ficue phrase classification machine learning

model
p l
p p
l
p
c


prosody
hl features
phrasing
length
position
intonational
intermediate

classifiable cue phrases n
non tokenized tokenized





























classifiable non conjuncts n
non tokenized
tokenized





























table error rates c prosodic classification testing data training
testing done multiple cue phrase corpus cross validation
model
c p
c
p
p


pos
text
adjacency
orthography
preceding
succeeding

classifiable cue phrases n
non tokenized tokenized

























classifiable non conjuncts n
non tokenized
tokenized

























table error rates c textual classification testing data training
testing done multiple cue phrase corpus cross validation
indeed improve example error rate classifiable non conjuncts changes
table within cgrendel confidence
interval table

acknowledgements
would thank william cohen jason catlett helpful comments regarding
use cgrendel c sandra carberry rebecca passonneau three
anonymous jair reviewers helpful comments would


filitman

model
speech text

classifiable cue phrases n
non tokenized tokenized



classifiable non conjuncts n
non tokenized
tokenized



table error rates c prosodic textual classification model testing data
training testing done multiple cue phrase corpus crossvalidation
thank william cohen ido dagan julia hirschberg eric siegel comments
preliminary version litman

references

altenberg b prosodic patterns spoken english studies correlation
prosody grammar text speech conversion vol lund studies
english lund university press lund
aone c bennett w evaluating automated manual acquisition
anaphora resolution strategies proceedings thirty third annual meeting
association computational linguistics acl
brieman l friedman j olshen r stone c classification regression
trees monterey ca wadsworth brooks
church k w stochastic parts program noun phrase parser unrestricted
text proceedings second conference applied natural language processing
cohen r computational theory function clue words argument understanding proceedings tenth international conference computational
linguistics coling
cohen w w compiling knowledge explicit bias proceedings
ninth international conference machine learning
cohen w w ecient pruning methods separate conquer rule learning
systems proceedings thirteenth international joint conference artificial
intelligence ijcai
freedman pisani r purves r statistics w w norton company
grosz b hirschberg j intonational characteristics discourse structure proceedings international conference spoken language processing
icslp
grosz b j sidner c l attention intentions structure discourse
computational linguistics


ficue phrase classification machine learning

halliday k hassan r cohesion english longman
hearst multi paragraph segmentation expository text proceedings
thirty second annual meeting association computational linguistics
acl
hindle acquiring disambiguation rules text proceedings
twenty seventh annual meeting association computational linguistics
acl
hirschberg j accent discourse context assigning pitch accent synthetic
speech proceedings eighth national conference artificial intelligence
aaai
hirschberg j litman let talk identifying cue phrases
intonationally proceedings twenty fifth annual meeting association
computational linguistics acl
hirschberg j litman empirical studies disambiguation cue phrases
computational linguistics
holte r c simple classification rules perform well commonly used
datasets machine learning
litman hirschberg j disambiguating cue phrases text speech
proceedings thirteenth international conference computational linguistics
coling
litman j classifying cue phrases text speech machine learning
proceedings twelfth national conference artificial intelligence aaai
litman j allen j f plan recognition model subdialogues conversation cognitive science
litman j passonneau r j combining multiple knowledge sources
discourse segmentation proceedings thirty third annual meeting
association computational linguistics acl
mccarthy j f lehnert w g decision trees coreference resolution
proceedings fourteenth international joint conference artificial intelligence
ijcai
moser moore j investigating cue selection placement tutorial
discourse proceedings thirty third annual meeting association
computational linguistics acl
ostendorf ross k press multi level model recognition intonation labels
sagisaka n c higuchi n eds computing prosody springer verlag
passonneau r j litman j press discourse segmentation human
automated means computational linguistics


filitman

pierrehumbert j b phonology phonetics english intonation ph
thesis massachusetts institute technology distributed indiana university
linguistics club
pitrelli j beckman hirschberg j evaluation prosodic transcription
labeling reliability tobi framework proceedings international conference spoken language processing icslp
quinlan j r c programs machine learning san mateo ca morgan
kaufmann
reichman r getting computers talk discourse context
focus semantics cambridge mit press
siegel e v competitively evolving decision trees fixed training cases
natural language processing k e kinnear j ed advances genetic
programming cambridge mit press
siegel e v mckeown k r emergent linguistic rules automatic
grouping training examples disambiguating clue words decision trees
proceedings twelfth national conference artificial intelligence aaai
soderland lehnert w corpus driven knowledge acquisition discourse
analysis proceedings twelfth national conference artificial intelligence
aaai
sti eman l j discourse analysis structured speech working
notes aaai spring symposium series empirical methods discourse interpretation generation
weiss kulikowski c computer systems learn classification
prediction methods statistics neural nets machine learning expert
systems san mateo ca morgan kaufmann
wermter riloff e scheler g connectionist statistical symbolic approaches learning natural language processing berlin germany springerverlag
wightman c w ostendorf automatic labeling prosodic patterns ieee
transactions speech audio processing
zuckerman pearl j comprehension driven generation meta technical
utterances math tutoring proceedings fifth national conference
artificial intelligence aaai




