journal artificial intelligence

submitted published

exploiting causal independence bayesian network inference
nevin lianwen zhang

lzhang cs ust hk

department computer science
university science technology hong kong

david poole

poole cs ubc ca

department computer science university british columbia
main mall vancouver b c canada v z

abstract
method proposed exploiting causal independencies exact bayesian network inference bayesian network viewed representing factorization joint probability
multiplication set conditional probabilities present notion causal independence enables one factorize conditional probabilities combination even
smaller factors consequently obtain finer grain factorization joint probability
formulation causal independence lets us specify conditional probability variable given
parents terms associative commutative operator sum max
contribution parent start simple bayesian network inference
given evidence query variable uses factorization posterior distribution
query extended exploit causal independence empirical
studies cpcs networks medical diagnosis method efficient
previous methods allows inference larger networks previous

introduction
reasoning uncertain knowledge beliefs long recognized important
issue ai shortliffe buchanan duda et al several methodologies
proposed including certainty factors fuzzy sets dempster shafer theory probability theory
probabilistic far popular among alternatives mainly due
knowledge representation framework called bayesian networks belief networks pearl
howard matheson
bayesian networks graphical representation dependencies amongst random variables
bayesian network bn dag nodes representing random variables arcs representing
direct influence independence encoded bayesian network variable
independent non descendents given parents
bayesian networks aid knowledge acquisition specifying probabilities needed
network structure sparse number probabilities required much less
number required independencies structure exploited computationally
make inference faster pearl lauritzen spiegelhalter jensen et al shafer
shenoy
definition bayesian network constrain variable depends parents
often however much structure probability functions exploited knowledge acquisition inference one case dependencies depend particular
values variables dependencies stated rules poole trees boutilier

c ai access foundation morgan kaufmann publishers rights reserved

fiz hang p oole

et al multinets geiger heckerman another function
described binary operator applied values parent variables
latter known causal independencies seek exploit
causal independence refers situation multiple causes contribute independently
common effect well known example noisy gate model good knowledge
engineers specific causal independence simplifying knowledge acquisition henrion olesen et al olesen andreassen heckerman
first formalize general concept causal independence formalization later refined
heckerman breese
kim pearl showed use noisy gate speed inference special
kind bns known polytrees dambrosio showed two level bns
binary variables general bns olesen et al heckerman proposed two ways
causal independencies transform network structures inference transformed
networks efficient original networks see section
proposes method exploiting special type causal independence see section still covers common causal independence noisy gates noisy maxgates noisy gates noisy adders special cases method following
observation bn viewed representing factorization joint probability multiplication list conditional probabilities shachter et al zhang poole li
dambrosio type causal independence studied leads factorization conditional probabilities section finer grain factorization joint probability
obtained propose extend exact inference exploit conditional
independencies make use finer grain factorization provided causal independence
state art exact inference called clique tree propagation ctp lauritzen
spiegelhalter jensen et al shafer shenoy proposes another called variable elimination section related spi shachter et al li
dambrosio extends make use finer grain factorization see sections
rather compiling secondary structure finding posterior probability
variable query oriented needs part network relevant query given
observations work necessary answer query chose instead ctp
simplicity carry inference large networks ctp cannot
deal
experiments section performed two cpcs networks provided pradhan
networks consist nodes respectively contain abundant causal independencies best one could terms exact inference would first
transform networks jensen et al heckermans technique apply ctp
experiments computer ran memory constructing clique trees transformed
networks occurs one cannot answer query however extended able answer almost randomly generated queries twenty less observations
findings networks
one might propose first perform jensen et al heckermans transformation apply
experiments significantly less efficient extended
begin brief review concept bayesian network issue inference


fie xploiting c ausal ndependence



b ayesian n etwork nference

bayesian networks
assume domain characterized set random variables beliefs represented bayesian network bn annotated directed acyclic graph nodes represent
random variables arcs represent probabilistic dependencies amongst variables use
terms node variable interchangeably associated node conditional probability variable given parents
addition explicitly represented conditional probabilities bn implicitly represents
conditional independence assertions let x x xn enumeration nodes bn
node appears children let xi set parents node xi
bayesian network represents following independence assertion
variable xi conditionallyindependent variables fx x xi g given
values parents
conditional independence assertions conditional probabilities together entail joint probability variables chain rule

p x x xn





n


n



p xi jx x xi
p xi jx




second equation true conditional independence assertions conditional probabilities p xi jxi given specification bn consequently one
theory arbitrary probabilistic reasoning bn
inference
inference refers process computing posterior probability p x jy set x
query variables obtaining observations list observed variables
corresponding list observed values often x consists one query variable
theory p x jy obtained marginal probability p x turn
computed joint probability p x x xn summing variables outside
x one one practice viable summing variable joint probability requires exponential number additions
key efficient inference lies concept factorization factorization joint
probability list factors functions one construct joint probability
factor function set variables number say factor contains variable factor function variable say factor variables depends
suppose f f factors f factor contains variables x xi yj
write f x xi yj f factor variables yj z zk
yj variables common f f product f f factor
function union variables namely x xi yj z zk defined

f f x xi yj z zk f x xi yj f yj z zk





fiz hang p oole

c

b



e

e





e

figure bayesian network
let f x xi function variable x xi setting say x f x xi particular
value yields f x x xi function variables x xi
f x xi factor sum variable say x resulting factor variables
x xi defined

x



x

f x xi f x x xi f x ffm x xi

ffm possible values variable x
equation bn viewed representing factorization joint probability
example bayesian network figure factorizes joint probability p b c e e e
following list factors

p p b p c p e ja b c p e ja b c p e je e
multiplying factors yields joint probability
suppose joint probability p z z zm factorized multiplication list factors f f fm obtaining p z zm summing z p z z zm requires exponential number additions obtaining factorization p z zm often
done much less computation consider following procedure
procedure sum f z




inputs f list factors z variable
output list factors

remove f factors say f fk contain z
add factor

p qk f f return f
z

theorem suppose joint probability p z z zm factorized multiplication
list f factors sum f z returns list factors whose multiplicationis p z zm


fie xploiting c ausal ndependence



b ayesian n etwork nference

proof suppose f consists factors f f fm suppose z appears factors
f f fk

p z zm





x
z

p z z zm


xy
z



k
xy
z






k



theorem follows
variables appear factors f f fk participated computation sum f z
often small portion variables inference bn
tractable many cases even general np hard cooper

variable elimination
discussions previous section present simple computing p x jy
intuitions underlying dambrosios symbolic probabilistic inference
spi shachter et al li dambrosio first appeared zhang poole
essentially dechter bucket elimination belief assessment
called variable elimination sums variables list
factors one one ordering variables outside x summed required
input called elimination ordering
procedure f x





inputs f list conditional probabilities bn
x list query variables
list observed variables
corresponding list observed values
elimination ordering variables outside x
output p x jy

set observed variables factors corresponding observed values
empty

remove first variable z
b call sum f z endwhile

set h multiplication factors f
h function variables x

return h x

p h x renormalization
x

theorem output f x indeed p x jy
proof consider following modifications procedure first remove step factor
h produced step function variables x add step step sets
observed variables h observed values


fiz hang p oole

let f function variable variables use f jy denote
f let f g h z three functions variables
evident

f g jy f jy g jy
x
x

h z jy h z jy
z

z

consequently modifications change output procedure
according theorem modifications factor produced step simply marginal
probability p x consequently output exactly p x jy
complexity measured number numerical multiplications numerical summations performs optimal elimination ordering one least complexity finding optimal elimination ordering np complete arnborg et al
commonly used heuristics include minimum deficiency search bertele brioschi maximum cardinality search tarjan yannakakis kjrulff empirically shown
minimum deficiency search best existing heuristic use minimum deficiency search
experiments found better maximum cardinality search




versus clique tree propagation

clique tree propagation lauritzen spiegelhalter jensen et al shafer shenoy
compilation step transforms bn secondary structure called clique tree
junction tree secondary structure allows ctp compute answers queries one
query variable fixed set observations twice time needed answer one query
clique tree many applications desirable property since user might want compare
posterior probabilities different variables
ctp takes work build secondary structure observations received
bayesian network reused cost building secondary structure amortized
many cases observation entails propagation though network
given observations processes one query time user wants posterior
probabilities several variables sequence observations needs run
variables observation sets
cost terms number summations multiplications answering single query
observations order magnitude ctp particular clique
tree propagation sequence encodes elimination ordering elimination ordering approximately summations multiplications factors ctp
discrepancy actually form marginals cliques works conditional probabilities directly observations make simpler observed variables eliminated
start observation ctp requires propagation evidence
query oriented prune nodes irrelevant specific queries geiger et al
lauritzen et al baker boult ctp hand clique tree structure
kept static run time hence allow pruning irrelevant nodes
ctp encodes particular space time tradeoff another ctp particularly suited
case observations arrive incrementally want posterior probability node


fie xploiting c ausal ndependence



b ayesian n etwork nference

cost building clique tree amortized many cases suited
one queries single query variable observations given
unfortunately large real world networks ctp cannot deal due time
space complexities see section two examples networks still answer
possible queries permits pruning irrelevant variables

causal independence
bayesian networks place restriction node depends parents unfortunately
means general case need specify exponential number parents
number conditional probabilities node many cases structure
probability tables exploited acquisition inference one case
investigate known causal independence
one interpretation arcs bn represent causal relationships parents c c cm
variable e viewed causes jointly bear effect e causal independence refers
situation causes c c cm contribute independently effect e
precisely c c cm said causally independent w r effect e exist
random variables frame e set possible values e

probabilistically depends ci conditionally independent cj
j given ci
exists commutative associative binary operator
e
independence notion pearl let
given z first condition

frame e

x jz mean x independent

fc cm mgjc
similarly variables entails cj jc j jc cj j
j
refer contribution ci e less technical terms causes causally independent w r common effect individual contributions different causes independent
total influence effect combination individual contributions
call variable e convergent variable independent contributions different sources collected combined lack better name non convergent variables
simply called regular variables call base combination operator e
definition causal independence given slightly different given heckerman breese srinivas however still covers common causal independence
noisy gates good pearl noisy max gates dez noisy
gates noisy adders dagum galper special cases one see following examples
example lottery buying lotteries affects wealth amounts money spend
buying different kinds lotteries affect wealth independently words causally


fiz hang p oole

independent w r change wealth let c ck denote amounts money spend
buying k types lottery tickets let k changes wealth due buying
different types lottery tickets respectively depends probabilistically ci
conditionally independent cj j given ci let e total change wealth
due lottery buying e k hence c ck causally independent w r e
base combination operator e numerical addition example instance causal independence model called noisy adders
c ck amounts money spend buying lottery tickets lottery
c ck causally independent w r e winning one ticket reduces
chance winning thus conditionally independent given c however
ci represent expected change wealth buying tickets lottery would
causally independent probabilistically independent would arcs ci
example alarm consider following scenario different motion sensors
connected burglary alarm one sensor activates alarm rings different
sensors could different reliability treat activation sensor random variable
reliability sensor reflected assume sensors fail independently
assume alarm caused sensor activation alarm
base combination operator logical operator example instance causal
independence model called noisy gate
following example instance causal independence know
example contract renewal faculty members university evaluated teaching
service purpose contract renewal faculty members contract renewed renewed without pay raise renewed pay raise renewed double pay raise depending
whether performance evaluated unacceptable least one three areas acceptable
areas excellent one area excellent least two areas
let c c c fractions time faculty member spends teaching
service respectively let represent evaluation gets ith area take values
depending whether evaluation unacceptable acceptable excellent variable
depends probabilistically ci reasonable assume conditionally independent
cj j given ci
let e represent contract renewal variable take values depending
whether contract renewed renewed pay raise renewed pay raise
renewed double pay raise e base combination operator given
following table






























called exception independence assumption pearl
called accountability assumption pearl assumption satisfied introducing
node represent causes henrion



fie xploiting c ausal ndependence



b ayesian n etwork nference

fractions time faculty member spends three areas causally independent
w r contract renewal
traditional formulation bayesian network need specify exponential
number parents number conditional probabilities variable causal independence
number conditional probabilities p jci linear causal independence
reduce complexity knowledge acquisition henrion pearl olesen et al
olesen andreassen following sections causal independence
exploited computational gain
conditional probabilities convergent variables
allows us exploit structure bayesian network providing factorization joint probability distribution section causal independence used factorize
joint distributioneven initial factors form p ejc cm
want break simpler factors need table exponential
following proposition shows causal independence used

proposition let e node bn let c c cm parents e c c cm
causally independent w r e conditional probability p ejc cm obtained
conditional probabilities p jci

p e ffjc cm

x

ffk

p jc p ffmjcm



value e base combination operator e
proof definition causal independence entails independence assertions

fc cmgjc jc
axiom weak union pearl p jfc cmg thus
mutually independent given fc cmg
definition causal independence fc cm gjc
p jfc c cmg p jc
thus

p e ffjc cm
p ffjc cm
x

p ffm jc cm

x

p jc cm p jc cm p ffm jc cm

x

p jc p jc p ffm jcm


k

k

k

next four sections develop exploiting causal independence inference
thanks anonymous reviewer helping us simplify proof



fiz hang p oole

causal independence heterogeneous factorizations
section shall first introduce operator combining factors contain convergent
variables operator basic ingredient developed next three sections operator shall rewrite equation form convenient use
inference introduce concept heterogeneous factorization
consider two factors f g let e ek convergent variables appear f
g let list regular variables appear f g let b list variables
appear f let c list variables appear g b c contain
convergent variables well regular variables suppose base combination operator
ei combination f
g f g function variables e ek variables
b c defined

f
g e x
ek ffkx
b c


f e ek ffk b


ffk k ffk ffk

g e ek ffk c



value ffi ei shall sometimes write f
g f e ek b
g e ek c
make explicit arguments f g
note base combination operators different convergent variables different
following proposition exhibits basic properties combination operator

proposition f g share convergent variables f
g simply multiplication f g operator
commutative associative
proof first item obvious commutativity
follows readily commutativity
multiplication base combination operators shall prove associativity
special
case general case proved following line reasoning
suppose f g h three factors contain one variable e variable convergent need f
g
h f
g
h let base combination operator e
associativity value e

f
g
h e








x

f
g e h e
x
x

f e g e h e

x
f e g e h e

x
x
f e
g e h e






note base combination operators summations indexed convergent variable associated operator use binary operator associated corresponding convergent variable
examples ease exposition use one base combination operator one type
base combination operator e g may use sum max different variables network
keep track operators associated convergent variables however complicate
description



fie xploiting c ausal ndependence

x







b ayesian n etwork nference

f e g
h e

f
g
h e



proposition hence proved
following propositions give properties
correspond operations
exploited proofs straight forward omitted
proposition suppose f g factors variable z appears f g

x

xz
z

x

fg





f
g









z
x
z

f g
f
g

proposition suppose f g h factors g h share convergent variables

g f
h gf
h



rewriting equation
noticing contribution variable possible values e define functions

e ci

e ci p ffjci
value e shall refer contributing factor ci e
operator
rewrite equation follows
p ejc cm
mi e ci



interesting notice similarity equation equation equation
conditional independence allows one factorize joint probability factors involve less
variables equation causal independence allows one factorize conditional probability
factors involve less variables however ways factors combined
different two equations
heterogeneous factorizations
consider bayesian network figure factorizes joint probability p b c e e e
following list factors

p p b p c p e ja b c p e ja b c p e je e
say factorization homogeneous factors combined way
e multiplication
suppose ei convergent variables conditional probabilities factorized follows

p e ja b c
p e ja b c
p e je e





f e
f e b
f e c
f e
f e b
f e c
f e e
f e e


fiz hang p oole

factor f e instance contributing factor e
say following list factors

f e f e b f e c f e f e b f e c f e e f e e
p p b p c

constitute heterogeneous factorization p b c e e e joint probability
obtained combining factors proper order multiplication operator

word heterogeneous signify fact different factor pairs might combined different ways call fij heterogeneous factor needs combined
fik operator
combined factors multiplication contrast
call factors p p b p c homogeneous factors
shall refer heterogeneous factorization heterogeneous factorization represented
bn figure obvious heterogeneous factorization finer grain
homogeneous factorization represented bn

flexible heterogeneous factorizations deputation
extends exploit finer grain factorization compute answer query
summing variables one one factorization
correctness guaranteed fact factors homogeneous factorization
combined multiplication order distributivity multiplication summations see proof theorem
according proposition operator
distributive summations however factors
heterogeneous factorization cannot combined arbitrary order example consider heterogeneous factorization correct combine f e f e b

combine f e e f e e
correct combine f e f e e

want combine latter two multiplication combined sibling heterogeneous factors
overcome difficulty transformation called deputation performed bn
transformation change answers queries heterogeneous factorization
represented transformed bn flexible following sense
heterogeneous factorization joint probability flexible
joint probability


multiplication homogeneous factors

combination
heterogeneous factors



property allows us carry multiplication homogeneous factors arbitrary order
since
associative commutative combination heterogeneous factors arbitrary order conditions proposition satisfied exchange multiplication
combination
guarantee conditions proposition elimination ordering needs
constrained sections
heterogeneous factorization p b c e e e given end previous section
flexible consider combining heterogeneous factors since operator
commutative


fie xploiting c ausal ndependence



b ayesian n etwork nference

c

b


e

e

e

e
e
e

figure bn figure deputation convergent variables
associative one first combine fik obtaining conditional probability
ei combine resulting conditional probabilities combination

p e ja b c
p e ja b c
p e je e
multiplication

p e ja b c p e ja b c p e je e
convergent variables e e appear one factor consequently equation
hold factorization flexible arises convergent variable shared two factors siblings example want combine
f e f e e
order tackle introduce deputation
variable heterogeneous factor contains single convergent variable
deputation transformation one apply bn make heterogeneous factorization represented bn flexible let e convergent variable depute e make copy
e e make parents e parents e replace e e contributing factors e make
e parent e set conditional probability p eje follows

p eje






e e
otherwise



shall call e deputy e deputy variable e convergent variable definition
variable e convergent deputation becomes regular variable deputation
shall refer regular variable contrast shall refer variables regular
deputation old regular variables conditional probability p e je homogeneous
factor definition sometimes called deputing function written e e since
ensures e e take value
deputation bn obtained bn deputing convergent variables deputation
bn deputy variables convergent variables deputy variables convergent variables


fiz hang p oole

figure shows deputation bn figure factorizes joint probability

p b c e e e e e e
homogeneous factors

p p b p c e e e e e e
heterogeneous factors

f e f e b f e c f e f e b f e c f e e f e e
factorization three important properties
heterogeneous factor contains one one convergent variable recall ei
longer convergent variables deputies
convergent variable e appears one one homogeneous factor namely
deputing function e e
except deputing functions none homogeneous factors contain convergent
variables
properties shared factorization represented deputation bn
proposition heterogeneous factorization represented deputation bn flexible
proof consider combination
heterogeneous factors deputation bn since
combination operator
commutative associative carry combination following two steps first convergent deputy variable e combine heterogeneous factors contain e yielding conditional probability p e je e combine resulting
conditional probabilities follows first property mentioned different convergent variables e e p e je p e je share convergent variables hence
combination p e je multiplication consequently combination

heterogeneous factors deputation bn multiplication conditional
probabilities convergent variables therefore








joint probability variables deputation bn
multiplication conditional probabilities variables


multiplication conditional probabilities regular variables



multiplication homogeneous factors

multiplication conditional probabilities convergent variables

combination
heterogeneous factors
proposition hence proved
deputation change answer query precisely
proposition posterior probability p x jy bn deputation


fie xploiting c ausal ndependence



b ayesian n etwork nference

proof let r e e lists old regular regular deputy variables deputation bn respectively suffices p r e original bn
deputation bn regular variable e let e deputy easy see quantity
p e e p e j deputation bn p ej original bn hence
e
e
e




p r ex deputation bn

p r e e
e
x


p rjr p eje p e je
e r r
e

ex

p rjr e e p e je
r r
e e e



p rjr p eje










r r



proposition proved



e e

p r e original bn

tidy heterogeneous factorizations
far encountered heterogeneous factorizations correspond bayesian networks
following intermediate heterogeneous factorizations necessarily correspond bns property combine form appropriate marginal probabilities general intuition heterogeneous factors must combine sibling heterogeneous factors multiplied factors containing original convergent variable
previous section mentioned three properties heterogeneous factorization represented deputation bn used first property factorization flexible
two properties qualify factorization tidy heterogeneous factorization defined
let z z zk list variables deputation bn convergent deputy
variable e fz z zk g corresponding regular variable e flexible heterogeneous factorization p z z zk said tidy

convergent deputy variable e fz z zk g factorization contains deputing function e e homogeneous factor involves e

except deputing functions none homogeneous factors contain convergent
variables
stated earlier heterogeneous factorization represented deputation bn tidy
certain conditions given theorem one obtain tidy factorization p z zk
summing z tidy factorization p z z zk following procedure
procedure sum f f z



inputs f list homogeneous factors
f list heterogeneous factors
z variable


fiz hang p oole



output list heterogeneous factors list homogeneous factors

remove f factors contain z multiply resulting say f
factors set f nil

remove f factors contain z combine
resulting
say g factors set g nil

p f f

z
p
else add heterogeneous factor z fg f
return f f

g nil add homogeneous factor



theorem suppose list homogeneous factors f list heterogeneous factors f constitute tidy factorization p z z zk z convergent variable old regular
variable regular variable whose deputy list fz zk g procedure
sum f f z returns tidy heterogeneous factorization p z zk
proof theorem quite long hence given appendix

causal independence inference
task compute p x jy bn according proposition
deputation bn
elimination ordering consisting variables outside x legitimate deputy
variable e appears corresponding regular variable e ordering found
minor adaptations minimum deficiency search maximum cardinality search
following computes p x jy deputation bn called
extension
procedure f f x





inputs f list homogeneous factors
deputation bn
f list heterogeneous factors
deputation bn
x list query variables
list observed variables
corresponding list observed values
legitimate elimination ordering
output p x jy

set observed variables factors observed values
empty

remove first variable z
f f sum f f z endwhile


fie xploiting c ausal ndependence



b ayesian n etwork nference

set h multiplication factors f
combination
factors f
h function variables x
return h x

p h x renormalization
x

theorem output f f x indeed p x jy
proof consider following modifications first remove step factor
h produced step function variables x add step step sets
observed variables h observed values shall first modifications
change output output modified
p x jy
let f g h z three functions variables evident

f g jy f jy g jy
x
x

h z jy h z jy
z

z

regular variable

f
g jy f jy
g jy
consequently modifications change output procedure
since elimination ordering legitimate case deputy variable e
summed neither corresponding regular variable e let z zk remaining variables time execution e fz zk g implies e fz zk g fact factorization represented deputation bn tidy
enable us repeatedly apply theorem conclude modifications factor created
step simply marginal probability p x consequently output p x jy
example
subsection illustrates walking example consider computing p e je
deputation bayesian network shown figure suppose elimination ordering b
c e e e e first step

f fp p b p c e e e e e e g
f e f e b f e c f e f e b f e c f e e f e e g
procedure enters loop sums variables one one
summing
f fp b p c e e e e e e g
f e b f p e c f e b f e c f e e f e e e e g
e e p f e f e
summing b
f fp c e e e e e e g
f e c f p e c f e e f e e e e e e g
e e b p b f e b f e b


fiz hang p oole

summing c
f e e e e e e g
f e e fp
e e e e e e e e g


e e c p c f e c f e c
summing e
f e e e e g
f e e fp
e e e e g
e e e e e e e
e e
e e

summing e
f e e g
f e e fp
e e e e g
e e e e e e e

summing e
f e e g
f e e p e e g
e e e f e e e e
finally summing e
f
f f e g p
e e e e f e e
e e procedure enters step

p
nothing example finally procedure returns e e e
p e je required probability






comparing
comparing notice summing variable combine
factors contain variable however factorization latter works
finer grain factorization used former running example latter works
factorization initially consists factors contain two variables factorization former uses initially include factors contain five variables hand latter
uses operator
expensive multiplication consider instance calculating
f e
g e b suppose e convergent variable variables binary operation requires numerical multiplications numerical summations hand
multiplying f e g e b requires numerical multiplications

despite expensiveness operator
efficient shall provide
empirical evidence support claim section see simple example true
consider bn figure e convergent variable suppose variables binary
computing p e elimination ordering c c c c requires
numerical multiplications
numerical additions hand computing p e deputation bn shown figure
elimination ordering c c c c e requires
numerical multiplications numerical additions
note summing e requires numerical multiplications summing
ci four heterogeneous factors containing argument e combining


fie xploiting c ausal ndependence

c

c

c



b ayesian n etwork nference

c

c

c

c

c

e

e

e


c



c

c
e

c
e

c

c

c

c

e

e

e

e



e




figure bn deputation transformations
pairwise requires multiplications resultant factor needs multiplied deputing
factor e e requires numerical multiplications

previous methods
two methods proposed previously exploiting causal independence speed inference general bns olesen et al heckerman use causal independence
transform topology bn transformation conventional ctp
used inference
shall illustrate methods bn figure let base combination
operator e let denote contribution ci e let e ci contributing factor ci
e
parent divorcing method olesen et al transforms bn one figure
transformation variables regular variables e e
possible values e conditional probabilities e e given

p e jc c f e c
f e c
p e jc c f e c
f e c

conditional probability e given

p e ffje e
value e e e shall use pd refer first
performs parent divorcing transformation uses inference


fiz hang p oole

temporal transformation heckerman converts bn one figure
variables regular transformation newly introduced variables
possible values e conditional probability e given

p e ffjc f c
value e conditional probability ei e stands e given

p ei ffjei ci

x



e ci

possible value ei ei shall use tt refer first
performs temporal transformation uses inference
factorization represented original bn includes factor contain five variables
factors transformed bns contain three variables general transformations lead finer grain factorizations joint probabilities pd tt
efficient
however pd tt efficient shall provide empirical evidence support
claim next section illustrate considering calculating p e
figure elimination ordering c c c c e e would require
numerical multiplications numerical additions
figure elimination ordering c e c e c e c would require
numerical multiplications numerical additions cases
numerical multiplications additions performed differences drastic
complex networks shown next section
saving example may seem marginal may reasonable conjecture
olesons method produces families three elements marginal saving hope
producing factors two elements rather cliques three elements however interacting
causal variables make difference extreme example use olesons
method bn figure produce network figure triangulation
network least one clique four elements yet produce factor
two elements
note far computing p e networks shown figure concerned
efficient pd pd efficient tt tt efficient experiments
true general
exactly number operations required determine p e clique tree propagation
network clique tree figure three cliques one containing fc c e g one containing fc c e g
containing fe e eg first clique contains elements construct requires multiplications
message needs sent third clique marginal e obtained summing c
c similarly second clique third clique elements requires multiplications construct
order extract p e clique need sum e e shown one reason
efficient ctp never constructs factor three variables example note however
advantage ctp cost building cliques amortized many queries
note need produce two variables represent noisy b need two variables noise
applied case independent note noise network e b c
need create one variable e e would variable least perfectly correlated
case would need complicated example point



fie xploiting c ausal ndependence





b ayesian n etwork nference

b

e

c



e

e

e





e

figure applying olesons method bn figure

experiments
cpcs networks multi level multi valued bns medicine created pradhan
et al computer patient case simulation system cpcs pm developed
parker miller two cpcs networks used experiments one
consists nodes arcs contains nodes among largest
bns use present time
cpcs networks contain abundant causal independencies matter fact non root
variable convergent variable base combination operator max good test cases
inference exploit causal independencies
ctp approaches versus approaches
seen previous section one kind exploiting causal independencies
use transform bns thereafter inference including ctp
used inference
found coupling network transformation techniques ctp able carry
inference two cpcs networks used experiments computer ran memory
constructing clique trees transformed networks reported next subsection however combination network transformation techniques able answer
many queries
proposed method exploiting causal independencies observed
causal independencies lead factorization joint probability finer grain
factorization entailed conditional independencies alone one extend inference including ctp exploit finer grain factorization extended
obtained called able answer almost queries two
cpcs networks conjecture however extension ctp would able carry
inference two cpcs networks resources takes answer
query bn extension ctp would take construct clique tree
obtained ftp camis stanford edu pub pradhan
v txt cpcs networks std



file names cpcs lm sm k













number queries

number queries

z hang p oole


pd
tt


number queries


cpu time seconds














pd
tt



cpu time seconds














pd
tt


cpu time seconds

figure comparisons node bn

bn seen next subsection queries two cpcs networks
able answer
summary ctp approaches would able deal two cpcs
networks approaches different extents
comparisons approaches
subsection provides experimental data compare approaches namely pd tt
compare approaches determine much gained
exploiting causal independencies
node network three types queries one query variable five ten fifteen
observations respectively considered fifty queries randomly generated query
type query passed nodes irrelevant pruned general observations mean less irrelevant nodes hence greater difficulty answer query
cpu times spent answering queries recorded
order get statistics cpu time consumption limited ten seconds
memory consumption limited ten megabytes
statistics shown figure charts curve instance displays
time statistics queries five observations points x axis represent cpu times
















b ayesian n etwork nference

number queries

number queries

e xploiting c ausal ndependence


pd
tt



cpu time seconds












pd
tt


cpu time seconds

figure comparisons node bn
seconds time point corresponding point axis represents number fiveobservation queries answered within time
see able answer queries pd tt able answer
ten observation fifteen observation queries able answer majority
queries
get feeling average performances regard curves representing functions instead x integration along axis curve pd instance
roughly total amount time pd took answer ten observation queries pd
able answer dividing total number queries answered one gets average time pd
took answer ten observation query
clear average performed significantly better pd tt turn
performed much better average performance pd five ten observation queries
roughly tt slightly better fifteen observation queries
node network two types queries five ten observations considered
fifty queries generated type space time limits imposed
node networks moreover approximations made real numbers smaller
regarded zero since approximations comparisons
fair
statistics shown figure curves hardly visible
close axis
see average performed significantly better pd pd performed significantly better tt tt performed much better
one might notice tt able answer thirty nine ten observation queries
pd able due limit memory consumption see
next subsection memory consumption limit increased twenty megabytes able
answer forty five ten observation queries exactly ten seconds
effectiveness
established efficient exploiting causal
independencies section investigate effective


fiz hang p oole

node bn
number queries

number queries

node bn


















cpu time seconds




















cpu time seconds

figure time statistics
experiments carried two cpcs networks answer question
node network four types queries one query variable five ten fifteen twenty
observations respectively considered fifty queries randomly generated query
type statistics times took answer queries given left chart figure
collecting statistics ten mb memory limit ten second cpu time limit
imposed guide excessive resource demands see fifty five observation queries
network answered less half second forty eight ten observation queries
forty five fifteen observation queries forty twenty observation queries answered one
second however one twenty observation query able answer within
time memory limits
node network three types queries one query variable five ten fifteen
observations respectively considered fifty queries randomly generated query
type unlike previous section approximations made twenty mb memory limit
forty second cpu time limit imposed time statistics shown right hand side
chart see able answer queries majority queries
answered little time however three fifteen observation queries able
answer

conclusions
concerned exploit causal independence exact bn inference previous approaches olesen et al heckerman use causal independencies transform
bns efficiency gained inference easier transformed bns original
bns
method proposed basic idea bayesian network
viewed representing factorization joint probability multiplication list
conditional probabilities studied notion causal independence enables one
factorize conditional probabilities combination even smaller factors consequently
obtain finer grain factorization joint probability
propose extend inference make use finer grain factorization
extended called experiments shown extended algo

fie xploiting c ausal ndependence



b ayesian n etwork nference

rithm significantly efficient one first performs olesen et al heckermans
transformation apply
choice instead widely known ctp due ability work
networks ctp cannot deal matter fact ctp able deal networks
used experiments even olesen et al heckermans transformation hand
able answer almost randomly generated queries majority queries
answered little time would interesting extend ctp make use finer grain
factorization mentioned
seen previous section queries especially node network
took long time answer queries able answer
queries approximation must employed approximation technique comparing
node network technique captures extent heuristic ignoring
minor distinctions future work developing way bound error technique
anytime technique

acknowledgements
grateful malcolm pradhan gregory provan sharing us cpcs networks
thank jack breese bruce dambrosio mike horsch runping qi glenn shafer
valuable discussions ronen brafman chris geib mike horsch anonymous reviewers helpful comments mr tak yin chan great help experimentations
supported nserc grant ogpoo institute robotics intelligent
systems hong kong council grant hkust e sino software center
grant ssrc eg

appendix proof theorem
theorem suppose list homogeneous factors f list heterogeneous factors f constitute tidy factorization p z z zk z convergent variable old regular
variable regular variable whose deputy list fz zk g procedure
sum f f z returns tidy heterogeneous factorization p z zk
proof suppose f fr heterogeneous factors g gs homogeneous
factors suppose f fl g gm factors contain z

p z zk









x

p z z zk

z

x
z


rj fj

x





gi


lj fj

rj l fj







gi



x l



j fj
gi

rj l fj
gi
z


z





gi



fiz hang p oole



x





l
r


j fj gi

j l fj
gi
z





equation due proposition equation follows proposition matter
q g due
fact z convergent variable convergent variable

first condition tidiness condition proposition satisfied z appear
fl fr hand z old regular variable regular variable whose
q g contains convergent variables due
deputy appear list z zk

second condition tidiness condition proposition satisfied thus proved
sum f f z yields flexible heterogeneous factorization p z zk
let e convergent variable list z zk z cannot corresponding regular variable e hence factor e e touched sum f f z consequently
factor created sum f f z heterogeneous factor
homogeneous factor contain convergent variable factorization returned tidy
suppose sum f f z create homogeneous factor heterogeneous factors f contain z z convergent variable say e e e homop
geneous factor contain e factor e e e contain convergent
variables z old regular variable regular variable whose deputy list z
zk factors contain z contain convergent variables hence factor
contain convergent variables theorem thus proved


references
arnborg corneil g proskurowski complexity finding embedding
k tree siam j alg disc meth
baker boult pruning bayesian networks efficient computation proc sixth
conf uncertainty artificial intelligence pp cambridge mass
bertele u brioschi f nonserial dynamic programming vol mathematics
science engineering academic press
boutilier c friedman n goldszmidt koller context specific independence
bayesian networks e horvitz f jensen ed proc twelthth conf uncertainty
artificial intelligence pp portland oregon
cooper g f computational complexity probabilistic inference bayesian belief
networks artificial intelligence
dagum p galper additive belief network heckerman mamdani
ed proc ninth conf uncertainty artificial intelligence pp washington c
dambrosio local expression languages probabilistic dependence international journal approximate reasoning
dambrosio b symbolic probabilistic inference large bn networks r lopez de
mantaras poole ed proc tenth conf uncertainty artificial intelligence pp
seattle


fie xploiting c ausal ndependence



b ayesian n etwork nference

dechter r bucket elimination unifying framework probabilistic inference e
horvits f jensen ed proc twelthth conf uncertainty artificial intelligence pp
portland oregon
dez f j parameter adjustment bayes networks generalized noisy gate
heckerman mamdani ed proc ninth conf uncertainty artificial intelligence
pp washington c
duda r hart p e nilsson n j subjective bayesian methods rule inference systems proc afips nat comp conf pp
geiger heckerman knowledge representation inference similarity networks
bayesian multinets artificial intelligence
geiger verma pearl j separation theorems henrion
et al ed uncertainty artificial intelligence pp north holland york
good causal calculus british journal philosophy science
heckerman causal independence knowledge acquisition inference proc
ninth conference uncertainty artificial intelligence pp
heckerman breese j look causal independence proc tenth
conference uncertainty artificial ingelligence pp
henrion practical issues constructing belief networks l kanal levitt
j lemmer ed uncertainty artificial intelligence pp north holland
howard r matheson j e influence diagrams howard r matheson
j eds principles applications decision analysis pp strategic decisions group ca
jensen f v lauritzen l olesen k g bayesian updating causal probabilistic
networks local computations computational statistics quaterly
kim j pearl j computational model causal diagnostic reasoning inference
engines proc eighth international joint conference artificial intelligence pp
karlsruhe germany
kjrulff u triangulation graphs giving small total state space tech rep
r department mathematics computer science strandvejen dk aalborg
denmark
lauritzen l dawid p larsen b n leimer h g independence properties
directed markov fields networks
lauritzen l spiegelhalter j local computations probabilities graphical
structures application expert systems journal royal statistical society
series b


fiz hang p oole

li z dambrosio b efficient inference bayes networks combinatorial optimization international journal approximate reasoning
olesen k g andreassen specification large expert systems
causal probabilistic networks artificial intelligence medicine
olesen k g kjrulff u jensen f falck b andreassen andersen k
munin network median nerve case study loops applied artificial intelligence

parker r miller r causal knowledge creat simulated patient cases cpsc
project extension internist proc th symp comp appl medical care pp
los alamitos ca ieee comp soc press
pearl j probabilistic reasoning intelligent systems networks plausible inference
morgan kaufmann san mateo ca
poole probabilistic horn abduction bayesian networks artificial intelligence

pradhan provan g middleton b henrion knowledge engineering large
belief networks r lopez de mantaras poole ed proc tenth conf uncertainty
artificial intelligence pp seattle
shachter r dambrosio b del favero b symbolic probabilistic inference
belief networks proc th national conference artificial intelligence pp
boston mit press
shafer g shenoy p probability propagation annals mathematics artificial
intelligence
shortliffe e buchanan g b model inexact reasoning medicine math biosci

srinivas generalization noisy model proc ninth conference uncertainty artificial intelligence pp
tarjan r e yannakakis simple linear time test chordality graphs
test acyclicity hypergraphs selectively reduce acyclic hypergraphs siam j comput

zhang n l poole simple bayesian network computations proc
tenth canadian conference artificial intelligence pp




