Journal Artificial Intelligence Research 5 (1996) 301-328

Submitted 4/96; published 12/96

Exploiting Causal Independence Bayesian Network Inference
Nevin Lianwen Zhang

LZHANG @ CS . UST. HK

Department Computer Science,
University Science & Technology, Hong Kong

David Poole

POOLE @ CS . UBC . CA

Department Computer Science, University British Columbia,
2366 Main Mall, Vancouver, B.C., Canada V6T 1Z4

Abstract
new method proposed exploiting causal independencies exact Bayesian network inference. Bayesian network viewed representing factorization joint probability
multiplication set conditional probabilities. present notion causal independence enables one factorize conditional probabilities combination even
smaller factors consequently obtain finer-grain factorization joint probability. new
formulation causal independence lets us specify conditional probability variable given
parents terms associative commutative operator, or, sum max,
contribution parent. start simple algorithm Bayesian network inference
that, given evidence query variable, uses factorization find posterior distribution
query. show algorithm extended exploit causal independence. Empirical
studies, based CPCS networks medical diagnosis, show method efficient
previous methods allows inference larger networks previous algorithms.

1. Introduction
Reasoning uncertain knowledge beliefs long recognized important research
issue AI (Shortliffe & Buchanan, 1975; Duda et al., 1976). Several methodologies
proposed, including certainty factors, fuzzy sets, Dempster-Shafer theory, probability theory.
probabilistic approach far popular among alternatives, mainly due
knowledge representation framework called Bayesian networks belief networks (Pearl, 1988;
Howard & Matheson, 1981).
Bayesian networks graphical representation (in)dependencies amongst random variables.
Bayesian network (BN) DAG nodes representing random variables, arcs representing
direct influence. independence encoded Bayesian network variable
independent non-descendents given parents.
Bayesian networks aid knowledge acquisition specifying probabilities needed.
network structure sparse, number probabilities required much less
number required independencies. structure exploited computationally
make inference faster (Pearl, 1988; Lauritzen & Spiegelhalter, 1988; Jensen et al., 1990; Shafer &
Shenoy, 1990).
definition Bayesian network constrain variable depends parents.
Often, however, much structure probability functions exploited knowledge acquisition inference. One case dependencies depend particular
values variables; dependencies stated rules (Poole, 1993), trees (Boutilier

c 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiZ HANG & P OOLE

et al., 1996) multinets (Geiger & Heckerman, 1996). Another function
described using binary operator applied values parent variables.
latter, known causal independencies, seek exploit paper.
Causal independence refers situation multiple causes contribute independently
common effect. well-known example noisy OR-gate model (Good, 1961). Knowledge
engineers using specific causal independence models simplifying knowledge acquisition (Henrion, 1987; Olesen et al., 1989; Olesen & Andreassen, 1993). Heckerman (1993)
first formalize general concept causal independence. formalization later refined
Heckerman Breese (1994).
Kim Pearl (1983) showed use noisy OR-gate speed inference special
kind BNs known polytrees; DAmbrosio (1994, 1995) showed two level BNs
binary variables. general BNs, Olesen et al. (1989) Heckerman (1993) proposed two ways
using causal independencies transform network structures. Inference transformed
networks efficient original networks (see Section 9).
paper proposes new method exploiting special type causal independence (see Section 4) still covers common causal independence models noisy OR-gates, noisy MAXgates, noisy AND-gates, noisy adders special cases. method based following
observation. BN viewed representing factorization joint probability multiplication list conditional probabilities (Shachter et al., 1990; Zhang & Poole, 1994; Li &
DAmbrosio, 1994). type causal independence studied paper leads factorization conditional probabilities (Section 5). finer-grain factorization joint probability
obtained result. propose extend exact inference algorithms exploit conditional
independencies make use finer-grain factorization provided causal independence.
state-of-art exact inference algorithm called clique tree propagation (CTP) (Lauritzen &
Spiegelhalter, 1988; Jensen et al., 1990; Shafer & Shenoy, 1990). paper proposes another algorithm called variable elimination (VE ) (Section 3), related SPI (Shachter et al., 1990; Li
& DAmbrosio, 1994), extends make use finer-grain factorization (see Sections 6, 7,
8). Rather compiling secondary structure finding posterior probability
variable, query-oriented; needs part network relevant query given
observations, work necessary answer query. chose instead CTP
simplicity carry inference large networks CTP cannot
deal with.
Experiments (Section 10) performed two CPCS networks provided Pradhan.
networks consist 364 421 nodes respectively contain abundant causal independencies. paper, best one could terms exact inference would first
transform networks using Jensen et al.s Heckermans technique apply CTP.
experiments, computer ran memory constructing clique trees transformed
networks. occurs one cannot answer query all. However, extended algorithm able answer almost randomly generated queries twenty less observations
(findings) networks.
One might propose first perform Jensen et al.s Heckermans transformation apply
. experiments show significantly less efficient extended algorithm.
begin brief review concept Bayesian network issue inference.
302

fiE XPLOITING C AUSAL NDEPENDENCE



B AYESIAN N ETWORK NFERENCE

2. Bayesian Networks
assume problem domain characterized set random variables. Beliefs represented Bayesian network (BN) annotated directed acyclic graph, nodes represent
random variables, arcs represent probabilistic dependencies amongst variables. use
terms node variable interchangeably. Associated node conditional probability variable given parents.
addition explicitly represented conditional probabilities, BN implicitly represents
conditional independence assertions. Let x1 , x2 , ..., xn enumeration nodes BN
node appears children, let xi set parents node xi .
Bayesian network represents following independence assertion:
variable xi conditionallyindependent variables fx1 ; x2; : : :; xi,1 g given
values parents.
conditional independence assertions conditional probabilities together entail joint probability variables. chain rule, have:

P (x1; x2; : : :; xn)

=

=

n

i=1
n

i=1

P (xi jx1; x2; : : :; xi,1)
P (xi jx );


(1)

second equation true conditional independence assertions. conditional probabilities P (xi jxi ) given specification BN. Consequently, one can,
theory, arbitrary probabilistic reasoning BN.
2.1 Inference
Inference refers process computing posterior probability P (X jY =Y0 ) set X
query variables obtaining observations =Y0 . list observed variables
Y0 corresponding list observed values. Often, X consists one query variable.
theory, P (X jY =Y0 ) obtained marginal probability P (X; ), turn
computed joint probability P (x1 ; x2; : : :; xn) summing variables outside
X [Y one one. practice, viable summing variable joint probability requires exponential number additions.
key efficient inference lies concept factorization. factorization joint
probability list factors (functions) one construct joint probability.
factor function set variables number. say factor contains variable factor function variable; say factor variables depends.
Suppose f1 f2 factors, f1 factor contains variables x1 ; : : :; xi; y1; : : :; yj
write f1 (x1 ; : : :; xi; y1; : : :; yj ) f2 factor variables y1 ; : : :; yj ; z1; : : :; zk ,
y1 ; : : :; yj variables common f1 f2 . product f1 f2 factor
function union variables, namely x1 ; : : :; xi; y1; : : :; yj ; z1; : : :; zk , defined by:

f1 f2)(x1; : : :; xi; y1; : : :; yj ; z1; : : :; zk) = f1(x1; : : :; xi; y1; : : :; yj )f2(y1; : : :; yj ; z1; : : :; zk )

(

303

fiZ HANG & P OOLE

c

b



e

e

2

1

e3

Figure 1: Bayesian network.
Let f (x1 ; : : :; xi ) function variable x1; : : :; xi . Setting, say x1 f (x1 ; : : :; xi) particular
value yields f (x1 =ff; x2; : : :; xi), function variables x2 ; : : :; xi.
f (x1; : : :; xi) factor, sum variable, say x1 , resulting factor variables
x2 ; : : :; xi, defined

X

(

x1

f )(x2; : : :; xi) = f (x1 =ff1 ; x2; : : :; xi) + + f (x1=ffm; x2; : : :; xi)

ff1 ; : : :; ffm possible values variable x1.
equation (1), BN viewed representing factorization joint probability.
example, Bayesian network Figure 1 factorizes joint probability P (a; b; c; e1; e2; e3)
following list factors:

P (a); P (b); P (c); P (e1ja; b; c); P (e2ja; b; c); P (e3je1; e2 ):
Multiplying factors yields joint probability.
Suppose joint probability P (z1 ; z2; : : :; zm ) factorized multiplication list factors f1 , f2 , ..., fm . obtaining P (z2 ; : : :; zm ) summing z1 P (z1 ; z2; : : :; zm ) requires exponential number additions, obtaining factorization P (z2 ; : : :; zm ) often
done much less computation. Consider following procedure:
Procedure sum-out(F ; z ):




Inputs: F list factors; z variable.
Output: list factors.

1. Remove F factors, say f1 , ..., fk , contain z ,
2. Add new factor

P Qk f F return F .
z i=1

Theorem 1 Suppose joint probability P (z1 ; z2; : : :; zm) factorized multiplication
list F factors. sum-out(F ; z1 ) returns list factors whose multiplicationis P (z2 ; : : :; zm ).
304

fiE XPLOITING C AUSAL NDEPENDENCE



B AYESIAN N ETWORK NFERENCE

Proof: Suppose F consists factors f1 , f2 , ..., fm suppose z1 appears factors
f1, f2, ..., fk .

P (z2 ; : : :; zm)

=

=

X
z1

P (z1 ; z2; : : :; zm )


XY
z1 i=1

= [

k
XY
z1 i=1




][

i=k+1

]:

theorem follows. 2
variables appear factors f1 , f2 , ..., fk participated computation sum-out(F ; z1 ),
often small portion variables. inference BN
tractable many cases, even general problem NP-hard (Cooper, 1990).

3. Variable Elimination Algorithm
Based discussions previous section, present simple algorithm computing P (X jY =Y0 ).
algorithm based intuitions underlying DAmbrosios symbolic probabilistic inference
(SPI) (Shachter et al., 1990; Li & DAmbrosio, 1994), first appeared Zhang Poole (1994).
essentially Dechter (1996)s bucket elimination algorithm belief assessment.
algorithm called variable elimination (VE ) sums variables list
factors one one. ordering variables outside X [Y summed required
input. called elimination ordering.
Procedure (F ; X; Y; Y0; )





Inputs: F list conditional probabilities BN;
X list query variables;
list observed variables;
Y0 corresponding list observed values;
elimination ordering variables outside X [Y .
Output: P (X jY =Y0 ).

1. Set observed variables factors corresponding observed values.
2. empty,

(a) Remove first variable z ,
(b) Call sum-out(F ; z ). Endwhile

3. Set h = multiplication factors F .
/* h function variables X . */

4. Return h(X )=

P h(X ). /* Renormalization */
X

Theorem 2 output VE(F ; X; Y; Y0; ) indeed P (X jY =Y0 ).
Proof: Consider following modifications procedure. First remove step 1. factor
h produced step 3 function variables X . Add new step step 3 sets
observed variables h observed values.
305

fiZ HANG & P OOLE

Let f (y; A) function variable variables A. use f (y; A)jy=ff denote
f (y=ff; A). Let f (y; ,), g (y; ,), h(y; z; ,) three functions variables.
evident

f (y; ,)g (y; ,)jy=ff = f (y; ,)jy=ff g(y; ,)jy=ff;
X
X
[
h(y; z; ,)]jy=ff = [h(y; z; ,)jy=ff ]:
z

z

Consequently, modifications change output procedure.
According Theorem 1, modifications factor produced step 3 simply marginal
probability P (X; ). Consequently, output exactly P (X jY =Y0 ). 2
complexity measured number numerical multiplications numerical summations performs. optimal elimination ordering one results least complexity. problem finding optimal elimination ordering NP-complete (Arnborg et al., 1987).
Commonly used heuristics include minimum deficiency search (Bertele & Brioschi, 1972) maximum cardinality search (Tarjan & Yannakakis, 1984). Kjrulff (1990) empirically shown
minimum deficiency search best existing heuristic. use minimum deficiency search
experiments found better maximum cardinality search.
3.1



versus Clique Tree Propagation

Clique tree propagation (Lauritzen & Spiegelhalter, 1988; Jensen et al., 1990; Shafer & Shenoy,
1990) compilation step transforms BN secondary structure called clique tree
junction tree. secondary structure allows CTP compute answers queries one
query variable fixed set observations twice time needed answer one query
clique tree. many applications desirable property since user might want compare
posterior probabilities different variables.
CTP takes work build secondary structure observations received.
Bayesian network reused, cost building secondary structure amortized
many cases. observation entails propagation though network.
Given observations, processes one query time. user wants posterior
probabilities several variables, sequence observations, needs run
variables observation sets.
cost, terms number summations multiplications, answering single query
observations using order magnitude using CTP. particular clique
tree propagation sequence encodes elimination ordering; using elimination ordering results approximately summations multiplications factors CTP (there
discrepancy, actually form marginals cliques, works conditional probabilities directly). Observations make simpler (the observed variables eliminated
start algorithm), observation CTP requires propagation evidence.
query oriented, prune nodes irrelevant specific queries (Geiger et al., 1990;
Lauritzen et al., 1990; Baker & Boult, 1990). CTP, hand, clique tree structure
kept static run time, hence allow pruning irrelevant nodes.
CTP encodes particular space-time tradeoff, another. CTP particularly suited
case observations arrive incrementally, want posterior probability node,
306

fiE XPLOITING C AUSAL NDEPENDENCE



B AYESIAN N ETWORK NFERENCE

cost building clique tree amortized many cases. suited
one-off queries, single query variable observations given once.
Unfortunately, large real-world networks CTP cannot deal due time
space complexities (see Section 10 two examples). networks, still answer
possible queries permits pruning irrelevant variables.

4. Causal Independence
Bayesian networks place restriction node depends parents. Unfortunately
means general case need specify exponential (in number parents)
number conditional probabilities node. many cases structure
probability tables exploited acquisition inference. One case
investigate paper known causal independence.
one interpretation, arcs BN represent causal relationships; parents c1; c2; : : :; cm
variable e viewed causes jointly bear effect e. Causal independence refers
situation causes c1 ; c2; : : :; cm contribute independently effect e.
precisely, c1; c2; : : :; cm said causally independent w.r.t. effect e exist
random variables 1; 2; : : :; frame, i.e., set possible values, e

1. i, probabilistically depends ci conditionally independent cj
j given ci ,
2. exists commutative associative binary operator
e = 1 2 : : : .
Using independence notion Pearl (1988), let
given Z , first condition is:

frame e

(X; jZ ) mean X independent

(1; fc2; : : :; cm; 2; : : :; mgjc1)
similarly variables. entails (1; cj jc1) (1; j jc1) cj j
j 6= 1.
refer contribution ci e. less technical terms, causes causally independent w.r.t. common effect individual contributions different causes independent
total influence effect combination individual contributions.
call variable e convergent variable independent contributions different sources collected combined (and lack better name). Non-convergent variables
simply called regular variables. call base combination operator e.
definition causal independence given slightly different given Heckerman Breese (1994) Srinivas (1993). However, still covers common causal independence
models noisy OR-gates (Good, 1961; Pearl, 1988), noisy MAX-gates (Dez, 1993), noisy
AND-gates, noisy adders (Dagum & Galper, 1993) special cases. One see following examples.
Example 1 (Lottery) Buying lotteries affects wealth. amounts money spend
buying different kinds lotteries affect wealth independently. words, causally
307

fiZ HANG & P OOLE

independent w.r.t. change wealth. Let c1; : : :; ck denote amounts money spend
buying k types lottery tickets. Let 1; : : :; k changes wealth due buying
different types lottery tickets respectively. Then, depends probabilistically ci
conditionally independent cj j given ci . Let e total change wealth
due lottery buying. e=1 + +k . Hence c1 ; : : :; ck causally independent w.r.t. e.
base combination operator e numerical addition. example instance causal independence model called noisy adders.
c1 ; : : :; ck amounts money spend buying lottery tickets lottery,
c1 ; : : :; ck causally independent w.r.t. e, winning one ticket reduces
chance winning other. Thus, 1 conditionally independent 2 given c1. However,
ci represent expected change wealth buying tickets lottery, would
causally independent, probabilistically independent (there would arcs ci s).
Example 2 (Alarm) Consider following scenario. different motion sensors
connected burglary alarm. one sensor activates, alarm rings. Different
sensors could different reliability. treat activation sensor random variable.
reliability sensor reflected . assume sensors fail independently1.
Assume alarm caused sensor activation2. alarm=1 _ _m ;
base combination operator logical operator. example instance causal
independence model called noisy OR-gate.
following example instance causal independence models know:
Example 3 (Contract renewal) Faculty members university evaluated teaching, research,
service purpose contract renewal. faculty members contract renewed, renewed without pay raise, renewed pay raise, renewed double pay raise depending
whether performance evaluated unacceptable least one three areas, acceptable
areas, excellent one area, excellent least two areas.
Let c1 , c2, c3 fractions time faculty member spends teaching, research,
service respectively. Let represent evaluation gets ith area. take values 0, 1,
2 depending whether evaluation unacceptable, acceptable, excellent. variable
depends probabilistically ci. reasonable assume conditionally independent
cj j given ci .
Let e represent contract renewal result. variable take values 0, 1, 2, 3 depending
whether contract renewed, renewed pay raise, renewed pay raise,
renewed double pay raise. e=1 23, base combination operator given
following table:

0
1
2
3

0
0
0
0
0

1
0
1
2
3

2
0
2
3
3

3
0
3
3
3

1. called exception independence assumption Pearl (1988).
2. called accountability assumption Pearl (1988). assumption always satisfied introducing
node represent causes (Henrion, 1987).

308

fiE XPLOITING C AUSAL NDEPENDENCE



B AYESIAN N ETWORK NFERENCE

So, fractions time faculty member spends three areas causally independent
w.r.t. contract renewal result.
traditional formulation Bayesian network need specify exponential,
number parents, number conditional probabilities variable. causal independence,
number conditional probabilities P (i jci ) linear m. causal independence
reduce complexity knowledge acquisition (Henrion, 1987; Pearl, 1988; Olesen et al., 1989;
Olesen & Andreassen, 1993). following sections show causal independence
exploited computational gain.
4.1 Conditional Probabilities Convergent Variables
allows us exploit structure Bayesian network providing factorization joint probability distribution. section show causal independence used factorize
joint distributioneven further. initial factors algorithm form P (ejc1; : : :; cm).
want break simpler factors need table exponential m.
following proposition shows causal independence used this:

Proposition 1 Let e node BN let c1 ; c2; : : :; cm parents e. c1; c2; : : :; cm
causally independent w.r.t. e, conditional probability P (ejc1; : : :; cm) obtained
conditional probabilities P (i jci)

P (e=ffjc1; : : :; cm) =

X

ff1 :::ffk =ff

P (1 =ff1 jc1): : :P (m =ffmjcm );

(2)

value e. base combination operator e.
Proof:3 definition causal independence entails independence assertions

(1; fc2; : : :; cmgjc1) (1; 2jc1):
axiom weak union (Pearl, 1988, p. 84), (1; 2jfc1; : : :; cmg). Thus
mutually independent given fc1; : : :; cmg.
have, definition causal independence (1; fc2; : : :; cm gjc1),
P (1jfc1; c2; : : :; cmg) = P (1jc1)
Thus have:

P (e=ffjc1; : : :; cm)
= P (1 =ffjc1; : : :; cm)
X
=
P (1=ff1 ; : : :; m=ffm jc1; : : :; cm)
ff1 :::ff =ff
X
=
P (1=ff1 jc1; : : :; cm )P (2=ff2 jc1; : : :; cm) P (m=ffm jc1; : : :; cm)
ff1 :::ff =ff
X
=
P (1=ff1 jc1)P (2=ff2jc2) P (m =ffm jcm)
ff1 :::ff =ff
2
k

k

k

next four sections develop algorithm exploiting causal independence inference.
3. Thanks anonymous reviewer helping us simplify proof.

309

fiZ HANG & P OOLE

5. Causal Independence Heterogeneous Factorizations
section, shall first introduce operator combining factors contain convergent
variables. operator basic ingredient algorithm developed next three sections. Using operator, shall rewrite equation (2) form convenient use
inference introduce concept heterogeneous factorization.
Consider two factors f g . Let e1 , ..., ek convergent variables appear f
g , let list regular variables appear f g , let B list variables
appear f , let C list variables appear g . B C contain
convergent variables, well regular variables. Suppose base combination operator
ei. Then, combination f
g f g function variables e1, ..., ek variables
A, B , C . defined by:4

f
g (e1=X
ff1 ; : : :; ek =ffkX
; A; B; C )
=
:::
f (e1=ff11; : : :; ek =ffk1 ; A; B)
ff11 1 ff12 =ff1

ffk1 k ffk2 =ffk

g (e1=ff12; : : :; ek=ffk2; A; C );

(3)

value ffi ei . shall sometimes write f
g f (e1 ; : : :; ek ; A; B )
g (e1; : : :; ek ; A; C )
make explicit arguments f g .
Note base combination operators different convergent variables different.
following proposition exhibits basic properties combination operator
.
Proposition 2 1. f g share convergent variables, f
g simply multiplication f g . 2. operator
commutative associative.
Proof: first item obvious. commutativity
follows readily commutativity
multiplication base combination operators. shall prove associativity
special
case. general case proved following line reasoning.
Suppose f , g , h three factors contain one variable e variable convergent. need show (f
g )
h=f
(g
h). Let base combination operator e.
associativity , have, value e,

f
g )
h(e=ff)

(

=
=
=
=

X

f
g (e=ff4)h(e=ff3)
X
X
[
f (e=ff1 )g(e=ff2)]h(e=ff3 )
ff4 ff3 =ff ff1 ff2 =ff4
X
f (e=ff1)g (e=ff2 )h(e=ff3 )
ff1 ff2 ff3 =ff
X
X
f (e=ff1)[
g (e=ff2 )h(e=ff3)]
ff4 ff3 =ff

ff1 ff4 =ff

ff2ff3 =ff4

4. Note base combination operators summations indexed. convergent variable associated operator, always use binary operator associated corresponding convergent variable.
examples, ease exposition, use one base combination operator. one type
base combination operator (e.g., may use or, sum max different variables network),
keep track operators associated convergent variables. will, however, complicate
description.

310

fiE XPLOITING C AUSAL NDEPENDENCE

X

=

ff1 ff4 =ff



B AYESIAN N ETWORK NFERENCE

f (e=ff1)g
h(e=ff4)

f
(g
h)(e=ff):

=

proposition hence proved.2
following propositions give properties
correspond operations
exploited algorithm . proofs straight forward omitted.
Proposition 3 Suppose f g factors variable z appears f g ,

X

Xz
z

X

fg)

=

(

f
g )

=

(

(

(

z
X
z

f )g;
f )
g:

Proposition 4 Suppose f , g h factors g h share convergent variables,

g (f
h) = (gf )
h:

(4)

5.1 Rewriting Equation 2
Noticing contribution variable possible values e, define functions

fi(e; ci)

fi(e=ff; ci ) = P (i =ffjci);
value e. shall refer contributing factor ci e.
using operator
, rewrite equation (2) follows
P (ejc1; : : :; cm) =
mi=1 (e; ci):

(5)

interesting notice similarity equation (1) equation (5). equation (1)
conditional independence allows one factorize joint probability factors involve less
variables, equation (5) causal independence allows one factorize conditional probability
factors involve less variables. However, ways factors combined
different two equations.
5.2 Heterogeneous Factorizations
Consider Bayesian network Figure 1. factorizes joint probability P (a; b; c; e1; e2; e3)
following list factors:

P (a); P (b); P (c); P (e1ja; b; c); P (e2ja; b; c); P (e3je1; e2 ):
say factorization homogeneous factors combined way,
i.e., multiplication.
suppose ei convergent variables. conditional probabilities factorized follows:

P (e1ja; b; c)
P (e2ja; b; c)
P (e3 je1 ; e2)

=
=
=

f11 (e1; a)
f12 (e1 ; b)
f13 (e1; c);
f21 (e2; a)
f22 (e2 ; b)
f23 (e2; c);
f31 (e3; e1)
f32 (e3; e2 );
311

fiZ HANG & P OOLE

factor f11(e1 ; a), instance, contributing factor e1 .
say following list factors

f11(e1 ; a); f12(e1; b); f13(e1; c); f21(e2 ; a); f22(e2 ; b); f23(e2 ; c); f31(e3 ; e1); f32(e3 ; e2);
P (a); P (b); P (c)
(6)
constitute heterogeneous factorization P (a; b; c; e1; e2; e3) joint probability
obtained combining factors proper order using either multiplication operator
.
word heterogeneous signify fact different factor pairs might combined different ways. call fij heterogeneous factor needs combined
fik operator
combined factors multiplication. contrast,
call factors P (a), P (b), P (c) homogeneous factors.
shall refer heterogeneous factorization heterogeneous factorization represented
BN Figure 1. obvious heterogeneous factorization finer grain
homogeneous factorization represented BN.

6. Flexible Heterogeneous Factorizations Deputation
paper extends exploit finer-grain factorization. compute answer query
summing variables one one factorization .
correctness guaranteed fact factors homogeneous factorization
combined (by multiplication) order distributivity multiplication summations (see proof Theorem 1).
According Proposition 3, operator
distributive summations. However, factors
heterogeneous factorization cannot combined arbitrary order. example, consider heterogeneous factorization (6). correct combine f11(e1 ; a) f12 (e1; b) using
,
combine f31 (e3; e1 ) f32 (e3; e2 ) using
, correct combine f11 (e1; a) f31 (e3; e1)

. want combine latter two multiplication, combined sibling heterogeneous factors.
overcome difficulty, transformation called deputation performed BN.
transformation change answers queries. heterogeneous factorization
represented transformed BN flexible following sense:
heterogeneous factorization joint probability flexible if:
joint probability
=

multiplication homogeneous factors

combination (by
) heterogeneous factors:

(7)

property allows us carry multiplication homogeneous factors arbitrary order,
since
associative commutative, combination heterogeneous factors arbitrary order. conditions Proposition 4 satisfied, exchange multiplication
combination
. guarantee conditions Proposition 4, elimination ordering needs
constrained (Sections 7 8).
heterogeneous factorization P (a; b; c; e1; e2; e3) given end previous section
flexible. Consider combining heterogeneous factors. Since operator
commutative
312

fiE XPLOITING C AUSAL NDEPENDENCE



B AYESIAN N ETWORK NFERENCE

c

b


e1

e2

e1

e2
e3
e3

Figure 2: BN Figure 1 deputation convergent variables.
associative, one first combine, i, fik s, obtaining conditional probability
ei , combine resulting conditional probabilities. combination

P (e1 ja; b; c)
P (e2 ja; b; c)
P (e3je1 ; e2)
multiplication

P (e1 ja; b; c)P (e2ja; b; c)P (e3je1; e2)
convergent variables e1 e2 appear one factor. Consequently, equation
(7) hold factorization flexible. problem arises convergent variable shared two factors siblings. example, want combine
f11 (e1; a) f31 (e3 ; e1) using
. order tackle problem introduce new deputation
variable heterogeneous factor contains single convergent variable.
Deputation transformation one apply BN make heterogeneous factorization represented BN flexible. Let e convergent variable. depute e make copy
e0 e, make parents e parents e0 , replace e e0 contributing factors e, make
e0 parent e, set conditional probability P (eje0 ) follows:

P (eje0 ) =

(

1
0

e = e0
otherwise

(8)

shall call e0 deputy e. deputy variable e0 convergent variable definition.
variable e, convergent deputation, becomes regular variable deputation.
shall refer new regular variable. contrast, shall refer variables regular
deputation old regular variables. conditional probability P (e0 je) homogeneous
factor definition. sometimes called deputing function written (e0; e) since
ensures e0 e always take value.
deputation BN obtained BN deputing convergent variables. deputation
BN, deputy variables convergent variables deputy variables convergent variables.
313

fiZ HANG & P OOLE

Figure 2 shows deputation BN Figure 1. factorizes joint probability

P (a; b; c; e1; e01; e2; e02; e3; e03)
homogeneous factors

P (a); P (b); P (c); I1(e01; e1); I2(e02; e2); I3(e03; e3);
heterogeneous factors

f11(e01 ; a); f12(e01; b); f13(e01 ; c); f21(e02 ; a); f22(e02; b); f23(e02; c); f31(e03; e1); f32(e03; e2):
factorization three important properties.
1. heterogeneous factor contains one one convergent variable. (Recall ei
longer convergent variables deputies are.)
2. convergent variable e0 appears one one homogeneous factor, namely
deputing function (e0; e).
3. Except deputing functions, none homogeneous factors contain convergent
variables.
properties shared factorization represented deputation BN.
Proposition 5 heterogeneous factorization represented deputation BN flexible.
Proof: Consider combination,
, heterogeneous factors deputation BN. Since
combination operator
commutative associative, carry combination following two steps. First convergent (deputy) variable e0 , combine heterogeneous factors contain e0 , yielding conditional probability P (e0 je ) e0 . combine resulting
conditional probabilities. follows first property mentioned different convergent variables e01 e02, P (e01 je1 ) P (e02 je2 ) share convergent variables. Hence
combination P (e0 je )s multiplication them. Consequently, combination,

, heterogeneous factors deputation BN multiplication conditional
probabilities convergent variables. Therefore,
0

0

0

0

joint probability variables deputation BN
= multiplication conditional probabilities variables
=

multiplication conditional probabilities regular variables

=

multiplication homogeneous factors

multiplication conditional probabilities convergent variables

combination (by
) heterogeneous factors:
proposition hence proved. 2
Deputation change answer query. precisely,
Proposition 6 posterior probability P (X jY =Y0 ) BN deputation.
314

fiE XPLOITING C AUSAL NDEPENDENCE



B AYESIAN N ETWORK NFERENCE

Proof: Let R, E , E 0 lists old regular, new regular, deputy variables deputation BN respectively. suffices show P (R; E ) original BN
deputation BN. new regular variable e, let e0 deputy. easy see quantity
P (e0; e)P (e0j ) deputation BN P (ej ) original BN. Hence,
e
e
e
0

0

P (R; EX) deputation BN
=
P (R; E; E 0)
E
X

=
P (rjr ) [P (eje)P (e0 je )]
E r2R
2E

eX
=
P (rjr ) [ (e0; e)P (e0je )]
r2R
e2E e


=
P (rjr ) P (eje )
0

0

0

0

0

r2R

=

proposition proved.

2

e2 E

P (R; E ) original BN:

7. Tidy Heterogeneous Factorizations
far, encountered heterogeneous factorizations correspond Bayesian networks.
following algorithm, intermediate heterogeneous factorizations necessarily correspond BNs. property combine form appropriate marginal probabilities. general intuition heterogeneous factors must combine sibling heterogeneous factors multiplied factors containing original convergent variable.
previous section, mentioned three properties heterogeneous factorization represented deputation BN, used first property show factorization flexible.
two properties qualify factorization tidy heterogeneous factorization, defined below.
Let z1 , z2 , ..., zk list variables deputation BN convergent (deputy)
variable e0 fz1; z2 ; : : :; zk g, corresponding new regular variable e. flexible heterogeneous factorization P (z1 ; z2; : : :; zk ) said tidy

1. convergent (deputy) variable e02fz1; z2; : : :; zk g, factorization contains deputing function (e0; e) homogeneous factor involves e0 .

2. Except deputing functions, none homogeneous factors contain convergent
variables.
stated earlier, heterogeneous factorization represented deputation BN tidy.
certain conditions, given Theorem 3, one obtain tidy factorization P (z2 ; : : :; zk )
summing z1 tidy factorization P (z1 ; z2; : : :; zk ) using following procedure.
Procedure sum-out1(F1 ; F2; z )



Inputs: F1 list homogeneous factors,
F2 list heterogeneous factors,
z variable.
315

fiZ HANG & P OOLE



Output: list heterogeneous factors list homogeneous factors.

1. Remove F1 factors contain z , multiply resulting in, say, f .
factors, set f =nil.

2. Remove F2 factors contain z , combine using
resulting
in, say, g . factors, set g =nil.

P f F .
1
z
P
Else add new (heterogeneous) factor z fg F2.
Return (F1; F2).

3. g =nil, add new (homogeneous) factor
4.
5.

Theorem 3 Suppose list homogeneous factors F1 list heterogeneous factors F2 constitute tidy factorization P (z1 ; z2; : : :; zk ). z1 either convergent variable, old regular
variable, new regular variable whose deputy list fz2; : : :; zk g, procedure
sum-out1(F1 ; F2; z1) returns tidy heterogeneous factorization P (z2 ; : : :; zk ).
proof theorem quite long hence given appendix.

8. Causal Independence Inference
task compute P (X jY =Y0 ) BN. According Proposition 6,
deputation BN.
elimination ordering consisting variables outside X [Y legitimate deputy
variable e0 appears corresponding new regular variable e. ordering found
using, minor adaptations, minimum deficiency search maximum cardinality search.
following algorithm computes P (X jY =Y0 ) deputation BN. called 1
extension .
Procedure 1 (F1; F2; X; Y; Y0; )





Inputs: F1 list homogeneous factors
deputation BN;
F2 list heterogeneous factors
deputation BN;
X list query variables;
list observed variables;
Y0 corresponding list observed values;
legitimate elimination ordering.
Output: P (X jY =Y0 ).

1. Set observed variables factors observed values.
2. empty,

Remove first variable z .
(F1; F2) = sum-out1(F1; F2; z). Endwhile
316

fiE XPLOITING C AUSAL NDEPENDENCE



B AYESIAN N ETWORK NFERENCE

3. Set h=multiplication factors F1
combination (by
) factors F2.
/* h function variables X . */
4. Return h(X )=

P h(X ). /* renormalization */
X

Theorem 4 output 1 (F1; F2; X; Y; Y0; ) indeed P (X jY =Y0 ).
Proof: Consider following modifications algorithm. First remove step 1. factor
h produced step 3 function variables X . Add new step step 3 sets
observed variables h observed values. shall first show modifications
change output algorithm show output modified algorithm
P (X jY =Y0 ).
Let f (y; ,), g (y; ,), h(y; z; ,) three functions variables. evident

f (y; ,)g (y; ,)jy=ff = f (y; ,)jy=ff g(y; ,)jy=ff;
X
X
[
h(y; z; ,)]jy=ff = [h(y; z; ,)jy=ff ]:
z

z

regular variable,

f (y; ,)
g (y; ,)jy=ff = f (y; ,)jy=ff
g (y; ,)jy=ff :
Consequently, modifications change output procedure.
Since elimination ordering legitimate, always case deputy variable e0
summed out, neither corresponding new regular variable e. Let z1 , ..., zk remaining variables time execution algorithm. Then, e0 2fz1 ; : : :; zk g implies e2fz1 ; : : :; zk g. fact factorization represented deputation BN tidy
enable us repeatedly apply Theorem 3 conclude that, modifications, factor created
step 3 simply marginal probability P (X; ). Consequently, output P (X jY =Y0 ). 2
8.1 Example
subsection illustrates 1 walking example. Consider computing P (e2 je3=0)
deputation Bayesian network shown Figure 2. Suppose elimination ordering is: a, b,
c, e01, e02, e1, e03 . first step VE1 ,

F1 = fP (a); P (b); P (c); I1(e01; e1); I2(e02; e2); I3(e03; e3=0)g;
F2 = ff11(e01; a); f12(e01; b); f13(e01; c); f21(e02; a); f22(e02; b); f23(e02; c); f31(e03; e1); f32(e03; e2)g:
procedure enters while-loop sums variables one one.
summing a,
F1 = fP (b); P (c); I1(e01; e1); I2(e02; e2); I3(e03; e3=0)g;
F2 = ff12(e01; b); f13P(e01; c); f22(e02; b); f23(e02; c); f31(e03; e1); f32(e03; e2); 1(e01; e02)g;
1 (e01; e02) = P (a)f11(e01 ; a)f21(e02 ; a).
summing b,
F1 = fP (c); I1(e01; e1); I2(e02; e2); I3(e03; e3=0)g;
F2 = ff13(e01; c); f23P(e02; c); f31(e03; e1); f32(e03; e2); 1(e01; e02); 2(e01; e02)g;
2 (e01; e02) = b P (b)f12(e01 ; b)f22(e02; b).
317

fiZ HANG & P OOLE

summing c,
F1 = fI1(e01; e1); I2(e02; e2); I3(e03; e3=0)g;
F2 = ff31(e03; e1); fP
32(e03 ; e2); 1(e01 ; e02); 2(e01 ; e02); 3(e01 ; e02)g;
0
0
3 (e1; e2) = c P (c)f23 (e02; c)f13(e01 ; c).
summing e01 ,
F1 = fI2(e02; e2); I3(e03; e3=0)g;
F2 = ff31(e03; e1); fP
32(e03 ; e2); 4(e1 ; e02)g;
4 (e1; e02) = e I1(e01 ; e1)[ 1(e01; e02)
2 (e01; e02)
3 (e01; e03)].
1
summing e02 ,
F1 = fI3(e03; e3=0)g;
F2 = ff31(e03; e1); fP
32(e03 ; e2); 5(e1 ; e2)g;
5 (e1; e2) = e I2(e02 ; e2) 4(e1 ; e02).
2
summing e1 ,
F1 = fI3(e03; e3=0)g;
F2 = ff32(e03; e2); P6(e03; e2)g;
6 (e03; e2) = e1 f31(e03 ; e1) 5(e1 ; e2).
Finally, summing e03 ,
F 1 = ;;
F2 = f 7(e2)g; P
7 (e2) = e I3(e03 ; e3=0)[f32(e03; e2 )
6(e03; e2 )]. procedure enters step 3,
3
P
nothing example. Finally, procedure returns 7(e2 )= e2 7(e2 ),
P (e2je3 =0), required probability.
0

0

0

8.2 Comparing 1
comparing 1 , notice summing variable, combine
factors contain variable. However, factorization latter works
finer grain factorization used former. running example, latter works
factorization initially consists factors contain two variables; factorization former uses initially include factors contain five variables. hand, latter
uses operator
expensive multiplication. Consider, instance, calculating
f (e; a)
g(e; b). Suppose e convergent variable variables binary. operation requires 24 numerical multiplications 24 , 23 numerical summations. hand,
multiplying f (e; a) g (e; b) requires 23 numerical multiplications.

Despite expensiveness operator
, 1 efficient . shall provide
empirical evidence support claim Section 10. see simple example true,
consider BN Figure 3(1), e convergent variable. Suppose variables binary.
Then, computing P (e) using elimination ordering c1 , c2, c3, c4 requires 25 + 24 +
23 + 22=60 numerical multiplications (25 , 24) + (24 , 23 ) + (23 , 22 ) + (22 , 2)=30
numerical additions. hand, computing P (e) deputation BN shown Figure 3(2)
1 using elimination ordering c1, c2, c3 , c4, e0 requires 22 + 22 + 22 + 22 +
(322 + 22)=32 numerical multiplications 2 + 2 + 2 + 2 + (32 + 2)=16 numerical additions.
Note summing e0 requires 322 + 22 numerical multiplications summing
ci s, four heterogeneous factors, containing argument e0 . Combining
318

fiE XPLOITING C AUSAL NDEPENDENCE

c1

c3

c2



B AYESIAN N ETWORK NFERENCE

c1

c4

c3

c2

c4

e

e

e
(1)

c1

(2)

c3

c2
e1

c4
e2

c1

c2

c3

c4

e1

e2

e

e

3

e
(4)

(3)

Figure 3: BN, deputation transformations.
pairwise requires 322 multiplications. resultant factor needs multiplied deputing
factor (e0; e), requires 22 numerical multiplications.

9. Previous Methods
Two methods proposed previously exploiting causal independence speed inference general BNs (Olesen et al., 1989; Heckerman, 1993). use causal independence
transform topology BN. transformation, conventional algorithms CTP
used inference.
shall illustrate methods using BN Figure 3(1). Let base combination
operator e, let denote contribution ci e, let (e; ci) contributing factor ci
e.
parent-divorcing method (Olesen et al., 1989) transforms BN one Figure 3(3).
transformation, variables regular new variables e1 e2
possible values e. conditional probabilities e1 e2 given

P (e1 jc1; c2)=f1(e; c1)
f2 (e; c2);
P (e2 jc3; c4)=f3(e; c3)
f4 (e; c4):

conditional probability e given

P (e=ffje1=ff1; e2=ff2 ) = 1 ff=ff1 ff2,
value e, ff1 e1 , ff2 e2 . shall use PD refer algorithm first
performs parent-divorcing transformation uses inference.
319

fiZ HANG & P OOLE

temporal transformation Heckerman (1993) converts BN one Figure 3(4).
variables regular transformation newly introduced variables
possible values e. conditional probability e1 given

P (e1=ffjc1) = f1(1=ff; c1);
value e1 . i=2; 3; 4, conditional probability ei (e4 stands e) given

P (ei =ffjei,1 =ff1 ; ci) =

X

ff1 ff2 =ff

fi(e=ff2; ci);

possible value ei ff1 ei,1 . shall use TT refer algorithm first
performs temporal transformation uses inference.
factorization represented original BN includes factor contain five variables,
factors transformed BNs contain three variables. general, transformations lead finer-grain factorizations joint probabilities. PD TT
efficient .
However, PD TT efficient 1 . shall provide empirical evidence support
claim next section. illustrate considering calculating P (e).
Figure 3(3) using elimination ordering c1, c2, c3 , c4, e1 , e2 would require 23 + 22 +
23 + 22 + 23 + 22 =36 numerical multiplications 18 numerical additions.5
Figure 3(4) using elimination ordering c1 , e1 , c2, e2 , c3, e3 , c4 would require 22 + 23 + 22 +
23 + 22 + 23 + 22 =40 numerical multiplications 20 numerical additions. cases,
numerical multiplications additions performed 1 . differences drastic
complex networks, shown next section.
saving example may seem marginal. may reasonable conjecture that,
Olesons method produces families three elements, marginal saving hope
for; producing factors two elements rather cliques three elements. However, interacting
causal variables make difference extreme. example, use Olesons
method BN Figure 1, produce6 network Figure 4. triangulation
network least one clique four elements, yet 1 produce factor
two elements.
Note far computing P (e) networks shown Figure 3 concerned, 1
efficient PD, PD efficient TT, TT efficient . experiments
show true general.
5. exactly number operations required determine P (e) using clique-tree propagation
network. clique tree Figure 3(3) three cliques, one containing fc1 ; c2 ; e1 g, one containing fc3 ; c4 ; e2 g,
containing fe1 ; e2 ; eg. first clique contains 8 elements; construct requires 22 + 23 = 12 multiplications.
message needs sent third clique marginal e1 obtained summing c1
c2 . Similarly second clique. third clique 8 elements requires 12 multiplications construct.
order extract P (e) clique, need sum e1 e2 . shown one reason 1
efficient CTP VE; 1 never constructs factor three variables example. Note however,
advantage CTP cost building cliques amortized many queries.
6. Note need produce two variables represent noisy b. need two variables noise
applied case independent. Note noise network e1 = b c
need create one variable, e1 e2 would variable (or least perfectly correlated).
case would need complicated example show point.

320

fiE XPLOITING C AUSAL NDEPENDENCE





B AYESIAN N ETWORK NFERENCE

b

e

c

11

e21

e

e

2

1

e3

Figure 4: result Applying Olesons method BN Figure 1.

10. Experiments
CPCS networks multi-level, multi-valued BNs medicine. created Pradhan
et al. (1994) based Computer-based Patient Case Simulation system (CPCS-PM) developed
Parker Miller (1987). Two CPCS networks7 used experiments. One
consists 422 nodes 867 arcs, contains 364 nodes. among largest
BNs use present time.
CPCS networks contain abundant causal independencies. matter fact, non-root
variable convergent variable base combination operator MAX. good test cases
inference algorithms exploit causal independencies.
10.1 CTP-based Approaches versus -based Approaches
seen previous section, one kind approach exploiting causal independencies
use transform BNs. Thereafter, inference algorithms, including CTP ,
used inference.
found coupling network transformation techniques CTP able carry
inference two CPCS networks used experiments. computer ran memory
constructing clique trees transformed networks. reported next subsection, however, combination network transformation techniques able answer
many queries.
paper proposed new method exploiting causal independencies. observed
causal independencies lead factorization joint probability finer-grain
factorization entailed conditional independencies alone. One extend inference algorithms, including CTP , exploit finer-grain factorization. paper extended
obtained algorithm called 1 . 1 able answer almost queries two
CPCS networks. conjecture, however, extension CTP would able carry
inference two CPCS networks all. resources 1 takes answer
query BN extension CTP would take construct clique tree
7. Obtained ftp://camis.stanford.edu/pub/pradhan.
V1.0.txt CPCS-networks/std1.08.5.

321

file names CPCS-LM-SM-K0-

fi50
45
40
35
30
25
20
15
10
5
0

Number queries

Number queries

Z HANG & P OOLE

"5ve1"
"5pd"
"5tt"
"5ve"

Number queries

0 1 2 3 4 5 6 7 8 9
CPU time seconds

50
45
40
35
30
25
20
15
10
5
0

"10ve1"
"10pd"
"10tt"
"10ve"

0 1 2 3 4 5 6 7 8 9
CPU time seconds

50
45
40
35
30
25
20
15
10
5
0

"15ve1"
"15pd"
"15tt"

0 1 2 3 4 5 6 7 8 9 10
CPU time seconds

Figure 5: Comparisons 364-node BN.

BN are, seen next subsection, queries two CPCS networks
1 able answer.
summary, CTP based approaches would able deal two CPCS
networks, -based approaches (to different extents).
10.2 Comparisons -based Approaches
subsection provides experimental data compare -based approaches namely PD, TT,
1 . compare approaches determine much gained
exploiting causal independencies.
364-node network, three types queries one query variable five, ten, fifteen
observations respectively considered. Fifty queries randomly generated query
type. query passed algorithms nodes irrelevant pruned. general, observations mean less irrelevant nodes hence greater difficulty answer query.
CPU times algorithms spent answering queries recorded.
order get statistics algorithms, CPU time consumption limited ten seconds
memory consumption limited ten megabytes.
statistics shown Figure 5. charts, curve 5ve1, instance, displays
time statistics 1 queries five observations. Points X-axis represent CPU times
322

fi50
45
40
35
30
25
20
15
10
5
0



B AYESIAN N ETWORK NFERENCE

Number queries

Number queries

E XPLOITING C AUSAL NDEPENDENCE

"5ve1"
"5pd"
"5tt"
"5ve"

0 1 2 3 4 5 6 7 8 9
CPU time seconds

40
35
30
25
20
15
10
5
0

"10ve1"
"10pd"
"10tt"

0 1 2 3 4 5 6 7 8 9 10
CPU time seconds

Figure 6: Comparisons 422-node BN.
seconds. time point, corresponding point Y-axis represents number fiveobservation queries answered within time 1 .
see 1 able answer queries, PD TT able answer
ten-observation fifteen-observation queries. able answer majority
queries.
get feeling average performances algorithms, regard curves representing functions , instead x. integration, along -axis, curve 10PD, instance,
roughly total amount time PD took answer ten-observation queries PD
able answer. Dividing total number queries answered, one gets average time PD
took answer ten-observation query.
clear average, 1 performed significantly better PD TT, turn
performed much better . average performance PD five- ten-observation queries
roughly TT, slightly better fifteen-observation queries.
422-node network, two types queries five ten observations considered
fifty queries generated type. space time limits imposed
364-node networks. Moreover, approximations made; real numbers smaller 0.00001
regarded zero. Since approximations algorithms, comparisons
fair.
statistics shown Figure 6. curves 5ve1 10ve1 hardly visible
close -axis.
see average, 1 performed significantly better PD, PD performed significantly better TT, TT performed much better .
One might notice TT able answer thirty nine ten-observation queries,
1 PD able to. due limit memory consumption. see
next subsection, memory consumption limit increased twenty megabytes, 1 able
answer forty five ten-observation queries exactly ten seconds.
10.3 Effectiveness 1
established 1 efficient -based algorithm exploiting causal
independencies. section investigate effective 1 is.
323

fiZ HANG & P OOLE

422-node BN
Number queries

Number queries

364-node BN
50
45
40
35
30
25
20
15
10
5
0

"5ve1"
"10ve1"
"15ve1"
"20ve1"

0 1 2 3 4 5 6 7 8 9 10
CPU time seconds

50
45
40
35
30
25
20
15
10
5
0

"5ve1"
"10ve1"
"15ve1"

0

5 10 15 20 25 30 35 40
CPU time seconds

Figure 7: Time statistics 1 .
Experiments carried two CPCS networks answer question.
364-node network, four types queries one query variable five, ten, fifteen, twenty
observations respectively considered. Fifty queries randomly generated query
type. statistics times 1 took answer queries given left chart Figure
7. collecting statistics, ten MB memory limit ten second CPU time limit
imposed guide excessive resource demands. see fifty five-observation queries
network answered less half second. Forty eight ten-observation queries,
forty five fifteen-observation queries, forty twenty-observation queries answered one
second. is, however, one twenty-observation query 1 able answer within
time memory limits.
364-node network, three types queries one query variable five, ten, fifteen,
observations respectively considered. Fifty queries randomly generated query
type. Unlike previous section, approximations made. twenty MB memory limit
forty-second CPU time limit imposed. time statistics shown right hand side
chart. see 1 able answer queries majority queries
answered little time. are, however, three fifteen-observation queries 1 able
answer.

11. Conclusions
paper concerned exploit causal independence exact BN inference. Previous approaches (Olesen et al., 1989; Heckerman, 1993) use causal independencies transform
BNs. Efficiency gained inference easier transformed BNs original
BNs.
new method proposed paper. basic idea. Bayesian network
viewed representing factorization joint probability multiplication list
conditional probabilities. studied notion causal independence enables one
factorize conditional probabilities combination even smaller factors consequently
obtain finer-grain factorization joint probability.
propose extend inference algorithms make use finer-grain factorization.
paper extended algorithm called . Experiments shown extended algo324

fiE XPLOITING C AUSAL NDEPENDENCE



B AYESIAN N ETWORK NFERENCE

rithm, 1 , significantly efficient one first performs Olesen et al.s Heckermans
transformation apply .
choice instead widely known CTP algorithm due ability work
networks CTP cannot deal with. matter fact, CTP able deal networks
used experiments, even Olesen et al.s Heckermans transformation. hand,
1 able answer almost randomly generated queries majority queries
answered little time. would interesting extend CTP make use finer-grain
factorization mentioned above.
seen previous section, queries, especially 422-node network,
took 1 long time answer. queries 1 able answer.
queries, approximation must. employed approximation technique comparing
algorithms 422-node network. technique captures, extent, heuristic ignoring
minor distinctions. future work, developing way bound error technique
anytime algorithm based technique.

Acknowledgements
grateful Malcolm Pradhan Gregory Provan sharing us CPCS networks.
thank Jack Breese, Bruce DAmbrosio, Mike Horsch, Runping Qi, Glenn Shafer
valuable discussions, Ronen Brafman, Chris Geib, Mike Horsch anonymous reviewers helpful comments. Mr. Tak Yin Chan great help experimentations.
Research supported NSERC Grant OGPOO44121, Institute Robotics Intelligent
Systems, Hong Kong Research Council grant HKUST658/95E Sino Software Research Center
grant SSRC95/96.EG01.

Appendix A. Proof Theorem 3
Theorem 3 Suppose list homogeneous factors F1 list heterogeneous factors F2 constitute tidy factorization P (z1 ; z2; : : :; zk ). z1 either convergent variable, old regular
variable, new regular variable whose deputy list fz2 ; : : :; zk g, procedure
sum-out1(F1 ; F2; z1) returns tidy heterogeneous factorization P (z2 ; : : :; zk ).
Proof: Suppose f1 , ..., fr heterogeneous factors g1, ..., gs homogeneous
factors. suppose f1 , ..., fl , g1, ..., gm factors contain z1 .

P (z2 ; : : :; zk )

=

=

=

=

X

P (z1 ; z2; : : :; zk)

z1

X
z1


rj=1fj

X



i=1

gi


lj=1fj )
(
rj=l+1 fj )]







gi
i=1 i=m+1


X l


[(
j =1 fj
gi)
(
rj=l+1 fj )]
gi
z1
i=1
i=m+1
z1

[(

325

gi

(9)

fiZ HANG & P OOLE

=

X





l
r
[(

j=1 fj gi)
(
j=l+1fj )]
gi ;
z1
i=1
i=m+1

(10)

equation (10) due Proposition 3. Equation (9) follows Proposition 4. matter
Q g due
fact, z1 convergent variable, convergent variable
i=1
first condition tidiness. condition Proposition 4 satisfied z1 appear
fl+1 , ..., fr . hand, z1 old regular variable new regular variable whose
Q g contains convergent variables due
deputy appear list z2 , ..., zk ,
i=1
second condition tidiness. condition Proposition 4 satisfied. thus proved
sum-out1(F1 ; F2; z1) yields flexible heterogeneous factorization P (z2 ; : : :; zk ).
Let e0 convergent variable list z2 , ..., zk . z1 cannot corresponding new regular variable e. Hence factor (e0; e) touched sum-out1(F1 ; F2; z1). Consequently,
show new factor created sum-out1(F1 ; F2; z1) either heterogeneous factor
homogeneous factor contain convergent variable, factorization returned tidy.
Suppose sum-out1(F1 ; F2; z1) create new homogeneous factor. heterogeneous factors F1 contain z1 . z1 convergent variable, say e0, (e0; e) homoP
geneous factor contain e0 . new factor e (e0; e), contain convergent
variables. z1 old regular variable new regular variable whose deputy list z2 ,
..., zk , factors contain z1 contain convergent variables. Hence new factor
contain convergent variables. theorem thus proved. 2
0

References
Arnborg, S., Corneil, D. G., & Proskurowski, A. (1987). Complexity finding embedding
k-tree. SIAM J. Alg. Disc. Meth., 8(2), 277284.
Baker, M., & Boult, T. (1990). Pruning Bayesian networks efficient computation. Proc. Sixth
Conf. Uncertainty Artificial Intelligence, pp. 257264 Cambridge, Mass.
Bertele, U., & Brioschi, F. (1972). Nonserial dynamic programming, Vol. 91 Mathematics
Science Engineering. Academic Press.
Boutilier, C., Friedman, N., Goldszmidt, M., & Koller, D. (1996). Context-specific independence
Bayesian networks. E. Horvitz F. Jensen (Ed.), Proc. Twelthth Conf. Uncertainty
Artificial Intelligence, pp. 115123 Portland, Oregon.
Cooper, G. F. (1990). computational complexity probabilistic inference using Bayesian belief
networks. Artificial Intelligence, 42(2-3), 393405.
Dagum, P., & Galper, A. (1993). Additive belief-network models. D. Heckerman A. Mamdani
(Ed.), Proc. Ninth Conf. Uncertainty Artificial Intelligence, pp. 9198 Washington D.C.
DAmbrosio (1995). Local expression languages probabilistic dependence. International Journal Approximate Reasoning, 13(1), 6181.
DAmbrosio, B. (1994). Symbolic probabilistic inference large BN2O networks. R. Lopez de
Mantaras D. Poole (Ed.), Proc. Tenth Conf. Uncertainty Artificial Intelligence, pp.
128135 Seattle.
326

fiE XPLOITING C AUSAL NDEPENDENCE



B AYESIAN N ETWORK NFERENCE

Dechter, R. (1996). Bucket elimination: unifying framework probabilistic inference. E.
Horvits F. Jensen (Ed.), Proc. Twelthth Conf. Uncertainty Artificial Intelligence, pp.
211219 Portland, Oregon.
Dez, F. J. (1993). Parameter adjustment bayes networks. generalized noisy or-gate. D.
Heckerman A. Mamdani (Ed.), Proc. Ninth Conf. Uncertainty Artificial Intelligence,
pp. 99105 Washington D.C.
Duda, R. O., Hart, P. E., & Nilsson, N. J. (1976). Subjective Bayesian methods rule-based inference systems. Proc. AFIPS Nat. Comp. Conf., pp. 10751082.
Geiger, D., & Heckerman, D. (1996). Knowledge representation inference similarity networks
Bayesian multinets. Artificial Intelligence, 82, 4574.
Geiger, D., Verma, T., & Pearl, J. (1990). d-separation: theorems algorithms. M. Henrion
et. al. (Ed.), Uncertainty Artificial Intelligence 5, pp. 139148. North Holland, New York.
Good, I. (1961). causal calculus (i). British Journal Philosophy Science, 11, 305318.
Heckerman, D. (1993). Causal independence knowledge acquisition inference. Proc.
Ninth Conference Uncertainty Artificial Intelligence, pp. 122127.
Heckerman, D., & Breese, J. (1994). new look causal independence. Proc. Tenth
Conference Uncertainty Artificial Ingelligence, pp. 286292.
Henrion, M. (1987). practical issues constructing belief networks. L. Kanal T. Levitt
J. Lemmer (Ed.), Uncertainty Artificial Intelligence, pp. 161174. North-Holland.
Howard, R. A., & Matheson, J. E. (1981). Influence diagrams. Howard, R. A., & Matheson,
J. (Eds.), Principles Applications Decision Analysis, pp. 720762. Strategic Decisions Group, CA.
Jensen, F. V., Lauritzen, S. L., & Olesen, K. G. (1990). Bayesian updating causal probabilistic
networks local computations. Computational Statistics Quaterly, 4, 269282.
Kim, J., & Pearl, J. (1983). computational model causal diagnostic reasoning inference
engines. Proc. Eighth International Joint Conference Artificial Intelligence, pp.
190193 Karlsruhe, Germany.
Kjrulff, U. (1990). Triangulation graphs - algorithms giving small total state space. Tech. rep.
R 90-09, Department Mathematics Computer Science, Strandvejen, DK 9000 Aalborg,
Denmark.
Lauritzen, S. L., Dawid, A. P., Larsen, B. N., & Leimer, H. G. (1990). Independence properties
directed markov fields. Networks, 20, 491506.
Lauritzen, S. L., & Spiegelhalter, D. J. (1988). Local computations probabilities graphical
structures application expert systems. Journal Royal Statistical Society,
Series B, 50(2), 157224.
327

fiZ HANG & P OOLE

Li, Z., & DAmbrosio, B. (1994). Efficient inference Bayes networks combinatorial optimization problem. International Journal Approximate Reasoning, 11(1), 5581.
Olesen, K. G., & Andreassen, S. (1993). Specification models large expert systems based
causal probabilistic networks. Artificial Intelligence Medicine, 5, 269281.
Olesen, K. G., Kjrulff, U., Jensen, F., Falck, B., Andreassen, S., & Andersen, S. K. (1989).
munin network median nerve - case study loops. Applied Artificial Intelligence,
3, 384403.
Parker, R., & Miller, R. (1987). Using causal knowledge creat simulated patient cases: CPSC
project extension Internist-1. Proc. 11th Symp. Comp. Appl. Medical Care, pp.
473480 Los Alamitos, CA. IEEE Comp Soc Press.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference.
Morgan Kaufmann, San Mateo, CA.
Poole, D. (1993). Probabilistic Horn abduction Bayesian networks. Artificial Intelligence, 64(1),
81129.
Pradhan, M., Provan, G., Middleton, B., & Henrion, M. (1994). Knowledge engineering large
belief networks. R. Lopez de Mantaras D. Poole (Ed.), Proc. Tenth Conf. Uncertainty
Artificial Intelligence, pp. 484490 Seattle.
Shachter, R. D., DAmbrosio, B. D., & Del Favero, B. D. (1990). Symbolic probabilistic inference
belief networks. Proc. 8th National Conference Artificial Intelligence, pp. 126131
Boston. MIT Press.
Shafer, G., & Shenoy, P. (1990). Probability propagation. Annals Mathematics Artificial
Intelligence, 2, 327352.
Shortliffe, E., & Buchanan, G. B. (1975). model inexact reasoning medicine. Math. Biosci.,
23, 351379.
Srinivas, S. (1993). generalization noisy-or model. Proc. Ninth Conference Uncertainty Artificial Intelligence, pp. 208215.
Tarjan, R. E., & Yannakakis, M. (1984). Simple linear time algorithm test chordality graphs,
test acyclicity hypergraphs, selectively reduce acyclic hypergraphs. SIAM J. Comput.,
13, 566579.
Zhang, N. L., & Poole, D. (1994). simple approach Bayesian network computations. Proc.
Tenth Canadian Conference Artificial Intelligence, pp. 171178.

328


