Journal Artificial Intelligence Research 52 (2015) 445-475

Submitted 09/14; published 03/15

Modeling Lifespan Discourse Entities
Application Coreference Resolution
Marie-Catherine de Marneffe

MCDM @ LING . OHIO - STATE . EDU

Linguistics Department
Ohio State University
Columbus, OH 43210 USA

Marta Recasens

RECASENS @ GOOGLE . COM

Google Inc.
Mountain View, CA 94043 USA

Christopher Potts

CGPOTTS @ STANFORD . EDU

Linguistics Department
Stanford University
Stanford, CA 94035 USA

Abstract
discourse typically involves numerous entities, mentioned once. Distinguishing die one mention (singleton) lead longer lives
(coreferent) would dramatically simplify hypothesis space coreference resolution models,
leading increased performance. realize gains, build classifier predicting
singleton/coreferent distinction. models feature representations synthesize linguistic insights
factors affecting discourse entity lifespans (especially negation, modality, attitude
predication) existing results benefits surface (part-of-speech n-gram-based)
features coreference resolution. model effective right, feature representations help identify anchor phrases bridging anaphora well. Furthermore, incorporating
model two different state-of-the-art coreference resolution systems, one rule-based
learning-based, yields significant performance improvements.

1. Introduction
Karttunen imagined text interpreting system designed keep track individuals, is,
events, objects, etc., mentioned text and, individual, record whatever said
(Karttunen, 1976, p. 364). used term discourse referent describe abstract individuals.
discourse referents easily mapped specific entities world, proper
names. Others indeterminate sense compatible many different real-world
entities, indefinites train. either case, discourse referents enter anaphoric
relations discourse; even know exactly real-world object train picks
heard train distance . . . , nonetheless refer subsequent pronouns
ascribe properties (. . . loud horn).
discourse referents enjoy repeat appearances discourse. lead long lives
appear wide variety discourse contexts, whereas others never escape birthplaces,
dying one mention. central question paper factors influence
lifespan discourse referent. focus noun phrases, direct identifiers
discourse referents English. specifically, seek predict whether given discourse
c
2015
AI Access Foundation. rights reserved.

fiDE

ARNEFFE , R ECASENS & P OTTS

referent coreferent (mentioned multiple times given discourse) singleton (mentioned
once). ability make distinction based properties noun phrases used
identify referents (henceforth, mentions) would benefit coreference resolution models, simplifying hypothesis space consider predicting anaphoric links, could improve
performance tasks require accurately tracking discourse entities, including textual entailment (Delmonte, Bristot, Piccolino Boniforti, & Tonelli, 2007; Giampiccolo, Magnini, Dagan,
& Dolan, 2007) discourse coherence (Hobbs, 1979; Grosz, Joshi, & Weinstein, 1995; Kehler,
2002; Barzilay & Lapata, 2008; Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, & Webber, 2008).
existing literature provides numerous generalizations relevant singleton/coreferent
distinction. known, example, internal syntax morphology phrase used
establish discourse referent provide important clues lifespan referent (Prince,
1981a, 1981b; Wang, McCready, & Asher, 2006). Information structuring important; certain
grammatical discourse roles correlate long lifespans (Chafe, 1976; Hobbs, 1979; Walker,
Joshi, & Prince, 1997; Beaver, 2004). Features based insights long integrated
coreference resolution systems. contribution explore interaction
features semantic operators negation, modals, attitude predicates (know, certain,
wonder). interactions Karttunens primary focus (Karttunen, 1973, 1976),
long dominated work dynamic approaches linguistic meaning (Kamp, 1981; Heim, 1982,
1992; Roberts, 1990; Groenendijk & Stokhof, 1991; Bittner, 2001). Here, highlight importance interactions predicting lifespans discourse referents actual text.
approach capitalizes results Durrett Klein (2013) Hall, Durrett,
Klein (2014) concerning power surface features natural language processing (NLP)
tasks. authors show large sets easily extracted part-of-speech (POS) n-grambased features achieve results least good achieved hand-engineered
linguistic features. therefore investigate contribution surface features predicting
lifespan discourse entities. find surface features alone substantial predictive value
task, adding specialized linguistic features leads reliable performance gains.
suggests linguistic constraints relevant lifespan prediction go beyond
approximated surface-level information given available data.
first step analysis bring insights linguistic theories together
single logistic regression model lifespan model assess predictive power real
data. show linguistic features generally behave existing literature leads us
expect, model effective predicting whether given mention singleton
coreferent. second step bring surface features obtain predictive model.
provide initial assessment engineering value making singleton/coreferent distinction
incorporating lifespan model two different, state-of-the-art coreference resolution
systems: rule-based Stanford coreference system (Lee, Peirsman, Chang, Chambers, Surdeanu,
& Jurafsky, 2011) learning-based Berkeley coreference system (Durrett & Klein, 2013).
both, adding features results significant improvement precision CoNLL-2011
CoNLL-2012 Shared Task data, across standardly used coreference resolution measures,
see reliable boosts recall well.
article subsumes extends work Recasens, de Marneffe, Potts (2013).
specific differences follows. First, freed NAACLs tight space constraints, provide
much in-depth linguistic analysis various features lifespan model, include
details throughout. Second, examine contribution surface features lifespan
446

fiM ODELING L IFESPAN ISCOURSE E NTITIES

model. Third, assess value lifespan model predicting phrases act
anchors bridging anaphora. Fourth, give fuller evaluation coreference applications
model, incorporate best lifespan model learning-based system (the Berkeley
coreference system), complementing previous results rule-based Stanford coreference
system. Fifth, use recent version CoNLL scorer (v8.0), includes results
according BLANC fixes bug incorrectly boosted B3 CEAF scores points.
Sixth, benefit Kummerfeld Kleins (2013) error analysis tool gain deeper insights
errors lifespan model helps with.

2. Linguistic Insights
section briefly summarizes previous research anaphora resolution, discourse structure,
discourse coherence linguistic literature. goal obtain clear picture
lifespan discourse referent shaped features mentions local morphosyntactic features features syntactic semantic environments
occur. insights gather section inform design feature extraction functions
lifespan model (Section 5) turn shape contributions Stanford Berkeley
coreference systems (Section 8).
Karttunen (1976) primarily concerned ways semantic scope
indefinite influences lifespan associated discourse referent. three-sentence discourse
(1), indefinite exam question sentence 1 text-level scope. result, associated
discourse referent free lead long life, linking mention text-level
(sentence 2) one embedded negation (sentence 3).
(1)

Kim read exam question. hard. didnt understand it.

contrast, Karttunen observed, indefinite interpreted scope negation,
typically available anaphoric reference inside negative environment, (2),
outside it, (3). (We use # mark discourses incoherent intended construal.)
(2)

Kim didnt understand exam question even reading twice.

(3)

Kim didnt understand exam question. # hard.

course, (3) coherent construal exam question interpreted taking widescope respect negation (there question Kim didnt understand). inverse scope
readings often disfavored, become salient modifiers certain particular included (Fodor & Sag, 1982; Schwarzschild, 2002), mention contains
positive polarity item, is, item tons resists scoping negation
semantically (Baker, 1970; Israel, 1996, 2001):
(4)

Kim didnt understand particular exam question. pondered hours avail.

(5)

Kim didnt understand exam question. pondered hours avail.

Conversely, using negative polarity item (NPI) inside indefinite mention essentially ensures narrow-scope reading (Ladusaw, 1996; Israel, 2004), leads impossibleto-resolve anaphoric link simple variants (3):
(6)

Kim didnt understand exam question. # hard.
447

fiDE

ARNEFFE , R ECASENS & P OTTS

pattern Karttunen saw semantic scope anaphoric potential intimately related: given mention participate anaphoric relationships within scope,
outside it. Broadly speaking, familiar quantificational binding logical languages
(Cresswell, 2002) variable scope control structures programming languages (Muskens,
van Benthem, & Visser, 1997). Thus, indefinite text-level scope free reign, whereas one
inside scope operator negation restricted links span outer boundaries scopal environment. semantic generalizations might directly
reflected surface syntax, interpretive preferences internal morphosyntactic features
mention help disambiguate intended logical form.
Karttunen (1976) immediately generalized observations negation discourse reference modal auxiliaries non-factive attitude predicates want claim. following
based original examples:
(7)

Bill make kite. # long string.

(8)

John wants catch fish. # see here?

(9)

Sandy claims Jesse bought bicycle. # green frame.

negation, pattern makes intuitive sense. Bills abilities regarding kite construction
involve specific kite, hence first sentence (7) automatically establish
right sort discourse referent. Similarly, wanting catch fish guarantee salience (or
even existence) fish, Sandy might unreliable source bicycle status
outside semantic scope claim.
(7)(9) cohere indefinite interpreted outside scope relevant semantic
operator. relative preferences surface inverse scope harder characterize
negation, influenced complex ways semantics pragmatics
attitude predicate, reliability source information, nature conversational issues goals. example, speaker (9) regards Sandy reliable source
regarding Jesses bike buying, bicycle likely attain text-level scope by-product
Jesse bought bicycle becoming text-level commitment. Karttunen (1973) discusses patterns, observing that, many contexts, pragmatic pressures encourage embedded content become
elevated text level way. De Marneffe, Manning, Potts (2012) study newspaper data
extremely common pattern attitude verbs tend function evidential markers source embedded content (Rooryck, 2001; Simons, 2007).
see later attitude predicates seem encourage long lifespans OntoNotes data (the
majority news-like), arguably result pragmatic factors.
far restricted attention anaphoric links indefinite establishes new
discourse referent pronoun refers it. observations carry directly links indefinites definite noun phrases, linguistic theories treat roughly pronouns additional
descriptive content (for discussion, see work Elbourne, 2008). mention-patterns tend
quite different, though. discourse referents established definites named entities,
interactions negation operators simpler definites named entities
interact scopally operators (but see work Aloni, 2000, related issues
involving presupposition intensionality). Thus, anaphoric connections unconstrained
factors discussing. Conversely, truly quantified phrases student every linguist severely limited, interaction operators
448

fiM ODELING L IFESPAN ISCOURSE E NTITIES

deficiencies comes establishing discourse referents. cases
expressions establish new discourse referents, seem infrequent unusual (Wang
et al., 2006).
Cross-cutting considerations factors long central studies coreference anaphora within computational linguistics NLP. instance, animate nouns
generally likely lead long discourse lives, whereas mentions refer abstract objects quantities, percentages, measures tend singleton. assume
statistical patterns derive, narrow linguistic constraints, rather general cognitive
biases concerning people conceptualize discuss different kinds objects. However,
evidence biases make way grammars specific languages
form morpho-semantic phenomena obviation (Aissen, 1997) differential object marking
(Aissen, 2003).
syntactic environment phrases occur modulate anaphoric potential hence lifespans. example, Prince (1981b) reports semantically indefinite
phrases using this, guy back row, highly likely referred
subsequent clause. Similarly, Chafe (1976) shows information structuring choices
predictive whether given noun phrase serve antecedent later referential devices.
close correlations syntactic topic position leading long discourse life (Grosz et al., 1995; Beaver, 2004); focused evaluation ideas handling
coreference, see work Beaver (2007).
seek incorporate observations lifespan model. additional patterns literature pursue, infrequent
data. example, Karttunen (1976) identified natural class counterexamples basic scope generalizations: certain sequences intensional predicates support exceptional anaphoric
links, phenomenon later studied systematically heading modal subordination
(Roberts, 1990, 1996):
(10)

Frank wants marry rich linguist. # kind.

(11)

Frank wants marry rich linguist. kind.

addition, mentions inside parenthetical clauses less likely introduce long-term discourse
referents, due likelihood parenthetical clause conveys secondary content
compared main clause hosts (Potts, 2005). Thus, anaphoric links
parentheticals possible (AnderBois, Brasoveanu, & Henderson, 2010; Potts, 2012), seem
arise relatively rarely, valuable piece practical advice appositive-rich texts scientific
papers unfortunately one could put action here.
Karttunens observations helped set agenda dynamic approaches semantics next
decades (Kamp, 1981; Heim, 1982; Groenendijk & Stokhof, 1991). literature refined
extended observations numerous ways. Taken together, findings suggest intensional
operators negation interact complex ways discourse anaphora. default, expect
phrases introduced scope operators lead short lifespans, possible
take wide-scope respect operators, broadens range anaphoric links
establish. readings favored disfavored pragmatics situation well
lexical syntactic nature phrases involved. follows, seek model
interactions use inform lifespan model.
449

fiDE

ARNEFFE , R ECASENS & P OTTS

3. Previous Engineering Efforts Quantitative Evaluations
insights inspired NLP researchers try predict roles different mentions
play coreference chains. Previous work area subdivided detecting four
different targets: non-referential mentions, non-anaphoric mentions, discourse-new mentions,
non-antecedent mentions. terminology always used consistent way linguistics NLP, believe results ultimately brought together. Here, aim
clarify terminology find common insights behind various features used.
first single singleton/coreferent detection task such, work finds
important antecedents existing literature.
3.1 Non-referential Mentions
noun phrases refer discourse referent rather fill syntactic position.
English, canonical example non-referential NP expletive pronoun it, obvious
succeed. lexical NPs introduce discourse referent either,
linguist Pat linguist: mention Pat introduce discourse referent, linguist
simply predicates something her. Detecting non-referential uses plays role coreference
resolution: since NPs pick discourse referents (new existing), cannot enter
anaphoric relations kind consideration here.
Early work non-referentiality detection focuses pronoun it, aiming distinguish referential uses non-referential ones. Paice Husk (1987) develop rule-based system, Evans
(2001) uses supervised approach, Muller (2006) focuses use spoken dialog.
studies mainly employ lexico-syntactic features immediate surrounding context
pronoun. Similarly, Bergsma, Lin, Goebel (2008) explore system uses Web-count features derived Google n-grams data (Brants & Franz, 2006) capture frequent
subjects replace pronoun it: referential cases (e.g., able to), words
frequent n-grams, able China able to, whereas non-referential
cases, pronoun likely frequent subject (e.g., important to).
recently, Bergsma Yarowsky (2011) develop NADA system, improves
Bergsma et al. (2008) incorporating lexical features. lexical features indicate presence
absence strings specific positions around pronoun: three-grams five-grams
spanning pronoun; two tokens pronoun five tokens pronoun
positions; token within twenty tokens right pronoun; token within ten
tokens left pronoun named entity belongs following list: that, this,
and, said, says, it, It, its, itself. Using types features, lexical Web-count, achieve
85% accuracy different datasets.
Byron Gegg-Harrison (2004) apply linguistic insights highlighted Karttunen
(Section 2) special case pronoun resolution, seeking discard non-referential indefinite
NPs set potential antecedents pronouns. use hard filter non-referential
mentions, looking presence indefinites, negation, apposition (hand-labeled), modals, adjectival phrases predication adjuncts (tagged -CLR Penn Treebank), predicates copular
verbs (tagged -PRD), noun phrases express value. found removing nonreferential mentions gave small boost performance pronoun resolution.
450

fiM ODELING L IFESPAN ISCOURSE E NTITIES

3.2 Non-anaphoric Mentions
Non-anaphoric NPs whose interpretation depend previous mention
text. example, phrase new Scorsese movie stars De Niro (12) (while manifesting
many kinds context dependence) depend overt phrases order capture
descriptive content. contrast, movie (13) crucially links back previous sentence
descriptive content; superficially involves predicate movie, construed
additional property seen speaker previous night.
(12)

Last night, watched new Scorsese movie stars De Niro.

(13)

Last night, watched movie read paper. movie directed Scorsese.

direct correspondence anaphora coreferentiality. Coreferent mentions
non-anaphoric (as text containing multiple tokens phrase White House),
anaphoric mentions coreferent non-coreferent (van Deemter & Kibble, 2000). Cases
bridging anaphora (Clark, 1975) (14) involve non-coreferent anaphora. Here, ceiling
interpreted ceiling room mentioned previous sentence, thus anaphoric
room without coreferent phrase discourse.
(14)

looked room. ceiling high.

return cases Section 6, use lifespan model characterize sense
bridging anchors room lead longer lifespans count strictly coreferent
mentions would suggest.
Poesio, Uryupina, Vieira, Alexandrov-Kabadjov, Goulart (2004) Poesio, AlexandrovKabadjov, Vieira, Goulart, Uryupina (2005) summarize previous approaches non-anaphoricity
detection, refer discourse-new detectors. Vieira Poesio (2000) focus definite NPs use syntactic heuristics based pre- post-modification distinguish
anaphoric non-anaphoric NPs. Modification good indicator anaphoricity; heavily modified phrases new Scorsese movie stars De Niro tend non-anaphoric, whereas short
phrases general descriptive content movie tend anaphoric. Bean Riloff (1999)
focus definite NPs: addition syntactic heuristics based pre- post-modification,
use techniques mining lists likely non-anaphoric NPs (such presence NPs first
sentence document). Compared Vieira Poesio (2000), obtain substantially higher
recall (with recall precision figures around 80%).
non-anaphoricity detector, Poesio et al. (2005) use head feature (distance
NPs identical heads), syntactic features (e.g., occurring inside appositive copular clause,
post-modified), capitalization mention, presence mention first sentence
Web page, position mention text, probability mention definite
computed Web using technique Uryupina (2003). find important
features head feature definiteness probabilities.
3.3 Discourse-New Mentions
Discourse-new mentions introduce new entity discourse (Prince, 1981b;
Fraurud, 1990). entity might singleton involve chain coreferring mentions
first phrase discourse-new one rest considered discourse-old. Cast
451

fiDE

ARNEFFE , R ECASENS & P OTTS

information status task, goal discourse-new mention detection find discourse referents
previously available hearer/reader; e.g., see work Nissim (2006).
Ng Cardie (2002) develop discourse-new classifier targets every kind NP using
variety feature types: lexical (string head matching, conjunction), morpho-syntactic (definiteness, quantification, number), grammatical (appositional copular context, modifier structure,
proper-noun embedding), shallow semantic (e.g., WordNet features). incorporate
classifier coreference resolution system, pre-filtering NPs tagged discoursenew. However, pre-filtering ultimately hurts coreference resolution system performance: even
though precision increases, recall drops considerably. Section 8.2.3, report similar results
model instantiated discourse-new pre-filtering, find recall drop
avoided filtering applied mention analysis tagged discourse-new
antecedent candidate tagged singleton.
Ng Cardies (2002) work cast non-anaphoricity detection, model perhaps
better described trying distinguish coreferent mentions singleton initiate
coreference chains. specifically, write, positive instance created NP
involved coreference chain head chain (Ng & Cardie, 2002, p. 3),
picks non-initial members coreference chains. Conversely, negative instance created
remaining NPs (Ng & Cardie, 2002, p. 3), i.e., without antecedents.
Uryupina (2009) proposes discourse-new mention detector kind NP. classifier
relies features falling three categories defines: lexical (number words mention), syntactic (POS, number, person, determiner, pre- post-modification), semantic (gender, semantic class), salience (grammatical role, position sentence paragraph).
addition, includes Karttunens features implemented Byron Gegg-Harrison
(2004). classifier checks mentions identical heads, distance these.
syntactic head features deliver improvements majority baseline (which marks
NP discourse-new), performing almost well features together. Uryupina notes,
however, features, especially based Karttunens ideas,
designed discourse-new mention detection.
Ng Cardie (2002) Uryupina (2009) integrated discourse-new detector
coreference resolution system pipeline manner. joint approach discourse-new detection
coreference resolution, see work Denis Baldridge (2007).
3.4 Non-antecedent Mentions
Uryupina (2009) observes, coreference resolution, matters fact NPs
unavailable antecedents. therefore builds classifier marks NPs likely antecedents
not. system based features discourse-new detector described
(Section 3.3). non-antecedenthood detection, syntactic semantic features lead
significant precision improvement majority baseline (which marks NP nonantecedent), syntactic features alone performing well features together.
3.5 Approach: Singletons
model cross-cuts four categories. Unlike previous models non-referentality,
restricted pronouns indefinite NPs, tries identify kind non-referential NP
well referential NP whose referent mentioned (i.e., singleton). Thus,
452

fiM ODELING L IFESPAN ISCOURSE E NTITIES

Dataset

Docs

Tokens

Training
Development
Test

2,802
343
348

1.3M
160K
170K

ENTIONS
Coreferent Singletons
152,974
18,855
19,407

181,274
23,140
23,657

Table 1: CoNLL-2012 Shared Task data statistics. added singletons (noun phrases annotated coreferent), account 55% referents development set.

non-referential NPs fall singleton class. hand, strict correspondence singleton/coreferent distinction non-anaphoric/anaphoric distinction,
since anaphoricity based whether mention relies previous one interpretation,
whereas singleton/coreferent divide based long lifespan entity is. Similarly,
discourse-new mentions either coreferent singleton classification, depending
whether entity mentioned not.
terms feature representations, tried stay close possible Karttunens
original insights: extract features full syntactic parses, seeking remain faithful
underlying semantic relationships involved, include feature interaction terms capture
complex set dependencies reviewed Section 2. approach allows us evaluate
linguistic ideas quantitatively assess practical contributions full coreference
systems.

4. Data
data used throughout paper come CoNLL-2012 Shared Task data (Pradhan, Moschitti, Xue, Uryupina, & Zhang, 2012), included 1.6M English words OntoNotes
v5.0 (Pradhan & Xue, 2009) several common layers annotation (coreference, parse trees,
named-entity tags, etc.). OntoNotes corpus contains documents seven different domains:
broadcast conversation (20%), broadcast news (13%), magazine (7%), newswire (21%), telephone
conversation (13%), weblogs newsgroups (15%), pivot text (11%). genres
news-like, exception pivot texts (which come New Testament)
telephone conversations. used training, development, test splits defined shared
task (Table 1). Since coreference annotations OntoNotes contain singleton mentions, automatically marked singleton noun phrases annotated coreferent.
excluded verbal mentions.
mark singleton noun phrases annotated coreferent, definition
singletons includes non-referential noun phrases raining, president served
president two terms (Section 3.1). makes practical sense: starting point
coreference resolution systems take noun phrases possible candidates coreference
subsequently find clusters coreferent one another. phrases
accurately identify singleton, phrases exclude clustering step,
translate directly performance gains.
453

fiDE

ARNEFFE , R ECASENS & P OTTS

Referents

23140

2369
Singleton

2

797

415

236

436

140

61

92

3

4

5

6-10

11-15

16-20

>20

Mentions

Figure 1: Distribution referent lifespans 2012 OntoNotes development set.

5. Predicting Lifespans Linguistic Features
describe model predicting lifespan discourse referents using linguistic
factors proposed Section 2. model makes binary distinction discourse referents
part coreference chain (singleton) part one (coreferent).
distribution lifespans data shown Figure 1.
plot gives number entities associated single mention, number associated
two mentions, forth. fact singletons dominate data suggests binary singleton/coreferent division natural one. propensity toward singletons highlights
relevance detecting singletons coreference system. Following Bergsma Yarowsky
(2011), use logistic regression model, shown perform well range
NLP tasks. fit logistic regression model R (R Development Core Team, 2013) training data, coding singletons 0 coreferent mentions 1. Thus, throughout following
tables coefficient estimates, positive values favor coreferent mentions negative values favor
singletons. turn describing motivating features model.
5.1 Morphosyntax Mention
Table 2 summarizes features model concern internal morphology syntactic
structure mention, giving coefficient estimates. tables, indicated otherwise, coefficient estimates significant p < 0.001. use indicate significance
p < 0.05, indicate estimates p 0.05. morphosyntactic features include type
(pronoun, proper noun, common noun), animacy, named-entity tag, person, number, quantification (definite, indefinite, quantified), number modifiers mention. Many
common coreference systems (Recasens & Hovy, 2009), model highlights influence
lifespans. available, used gold annotations derive features, since primary
goal shed light relevance features claimed influence lifespans.
454

fiM ODELING L IFESPAN ISCOURSE E NTITIES

morphosyntactic features operationalized using static lists lexicons well
Stanford dependencies output Stanford parser (version 2.0.3; de Marneffe, MacCartney, &
Manning, 2006) gold constituent trees. features extracted following way:
Type type feature captures whether mention pronoun, proper noun, common noun.
value determined gold POS tag mention named-entity tag.
Animacy set animacy values (animate, inanimate, unknown) using static list pronouns, named-entity tags (e.g., PERSON animate whereas LOCATION not), dictionary
bootstrapped Web (Ji & Lin, 2009).
Person Person values (1, 2, 3) assigned pronouns (identified POS tag), using
static list. Mentions pronouns get value 0.
Number number value (singular, plural, unknown) based static list pronouns,
POS tags, Bergsma Lins (2006) static dictionary, named-entity tags. (Mentions marked
named entity considered singular exception organizations,
singular plural get value unknown.)
Quantification discussed Section 2, indefinites definites given referential
semantics pairs naturally discourse anaphora, whereas anaphoric possibilities truly
quantified terms restricted. operationalize quantification decide whether mention
definite, indefinite, quantified, use dependencies find possible determiners, possessors,
numerical quantifiers mention. mention definite named entity,
possessor (e.g., car Johns car definite), determiner definite (the), demonstrative,
possessive. mention quantified numerical quantifier (e.g., two cars)
determiner all, both, neither either. mentions indefinite.
Number modifiers added feature counting many modifiers mention has, seeking capture correlation specificity referentiality. modifiers, counted adjectival,
participial, infinitival, prepositional modifiers well relative clause modifiers, noun compounds, possessives. (Thus, four modifiers phrase modern multifunctional
business center costing 60 million yuan.)
Named entities model includes named-entity features 18 OntoNotes entitytypes, NER = true non-named-entities. used gold entity-type annotation.
Table 2 summarizes coefficient estimates obtain features. broad terms,
picture one would expect taxonomy given new defined Prince (1981b)
assumed throughout dynamic semantics (Kamp, 1981; Heim, 1982): pronouns depend anaphoric
connections previous mentions disambiguation thus likely coreferent.
corroborated positive coefficient estimate Type = pronoun.
quantified phrases participate discourse anaphora (Partee, 1987; Wang et al., 2006),
accounting association quantifiers singletons (as measured negative
coefficient estimate Quantifier = quantified).
negative coefficient indefinites initially surprising. seen Section 2, theories
stretching back Karttunen (1976) say indefinites excel establishing new discourse entities
frequent participants coreference chains, association
455

fiDE

ARNEFFE , R ECASENS & P OTTS

Feature

Coefficient

Feature

Coefficient

Type = pronoun
Type = proper noun
Animacy = inanimate
Animacy = unknown
Person = 1
Person = 2
Person = 3
Number = singular
Number = unknown
Quantifier = indefinite
Quantifier = quantified
Number modifiers
NER = DATE
NER = EVENT
NER = FACILITY

1.17
1.89
1.36
0.39
1.04
0.13
1.62
0.61
0.17
1.43
1.25
0.39
1.83
2.89
2.94

= GPE
NER = LANGUAGE
NER = LAW
NER = LOCATION
NER = MONEY
NER = NORP
NER =
NER = ORDINAL
NER = ORGANIZATION
NER = PERCENT
NER = PERSON
NER = PRODUCT
NER = QUANTITY
NER = TIME
NER = WORK ART

3.46
2.56
2.85
2.83
0.05
0.82
4.17
0.90
3.39
0.88
2.28
2.64
0.02
1.53
2.42

NER





Table 2: Internal morphosyntactic features lifespan model. indicates non-significant coefficient (p 0.05); sign indicates significant coefficient (p < 0.001).

chains negative. return Section 5.3, argue interactions semantic
operators explain fact.
behavior named-entity (NER) features closely aligned previous models
theoretical discussion above. rule, named entities behave Type = proper noun associating coreferent mentions. exceptions MONEY, ORDINAL, NORP (for nationalities
religions), PERCENT, QUANTITY, seem intuitively unlikely participate coreference chains. person, number, animacy features together suggest singular animates
excellent coreferent noun phrases.
one real surprise us concerns feature Number modifiers. Inspired observations Fodor Sag (1982) Schwarzschild (2002), expected feature positively
correlate coreferent. reasoning increased modification would likely result
increased specificity, thereby making associated discourse referent identifiable
distinctive. opposite seems hold data. However, hesitate conclude
original hypothesis mistaken. Rather, suspect model insufficiently sensitive
interactions modifier counts lexical semantics modifiers themselves.
5.2 Grammatical Role Mention
Synthesizing much work Centering Theory information structuring, hypothesized
coreferent mentions likely appear core verbal arguments favor sentence-initial (topictracking) positions (Ward & Birner, 2004). capture insights, used grammatical
relation mention given Stanford dependencies gold constituents, sentence
position mention.
456

fiM ODELING L IFESPAN ISCOURSE E NTITIES

Feature

Coefficient

Feature

Coefficient

Sentence Position = end
Sentence Position = first
Sentence Position = last
Sentence Position = middle
coordination

0.22
0.03
0.31
0.11
0.48

Relation = noun argument
Relation =
Relation = root
Relation = subject
Relation = verb argument

0.56
0.67
0.61
0.65
0.32



Table 3: Grammatical role features lifespan model. indicates non-significant coefficient
(p 0.05); sign indicates significant coefficient (p < 0.001).

Sentence position Sentence position determined based raw string: first indicates
mention first word sentence, end last word, begin, middle, last
indicate whether mention situated first, second, last third sentence, respectively.
Relation distinguish among grammatical relations, check whether mention subject, adjunct (which includes prepositional objects, adverbial modifiers, temporal modifiers),
verb argument (which includes direct indirect objects, clausal complements, adjectival complements attributes), noun argument (which includes relative clauses, appositions, possessives, noun compounds, adjectival modifiers).
coordination indicated whether mention conjunct see whether
inside coordinate phrase affects coreference ways go beyond grammatical role
containing phrase.
coefficient estimates Table 3 support general hypotheses: arguments make good discourse
referents, subjects best all, whereas sentence-final positions disfavor coreference. addition,
note model identifies negative correlation coordination coreference.
5.3 Semantic Environment Mention
Table 4 highlights complex interactions discourse anaphora semantic operators introduced Section 2. interactions focus logical semantics since Karttunen
(1976), whose guiding observation semantic: indefinite interpreted inside scope negation, modal, attitude predicate generally unavailable anaphoric reference outside
scope operator. Heim (1992) relates anaphoric properties NPs scope-taking
entailments attitude predications.
direct access semantic scope, expect syntactic scope correlate
strongly semantic scope. therefore used dependency representations define features
capturing syntactic scope negation, modal auxiliaries, broad range attitude predicates
(181 verbs 374 nouns Saur, 2008). Technically, given mention, produce
negation, modal attitude verb feature according presence pre-defined negation
modality markers (such not, can, may) attitude predicates (e.g., accuse, allege, doubt, say)
dependency path. example, NP relief given negation feature
financial storm shows sign relief today, since scope sign. Similarly,
mention scientific technological companies scope modal auxiliary would
457

fiDE

ARNEFFE , R ECASENS & P OTTS

Feature

Coefficient

Presence negation
Presence modality
attitude verb
AttitudeVerb * (Type = pronoun)
AttitudeVerb * (Type = proper noun)
AttitudeVerb * (Quantifier = indefinite)
AttitudeVerb * (Quantifier = quantified)
Modal * (Type = pronoun)
Modal * (Type = proper noun)
Modal * (Quantifier = indefinite)
Modal * (Quantifier = quantified)
Negation * (Type = pronoun)
Negation * (Type = proper noun)
Negation * (Quantifier = indefinite)
Negation * (Quantifier = quantified)
Negation * (Number modifiers)

0.18
0.22
0.10
0.41
0.10
0.19
0.10
0.13
0.35
0.00
0.17
1.07
0.30
0.36
0.39
0.11








Table 4: Semantic environment features interactions lifespan model. indicates nonsignificant coefficient (p 0.05); sign indicates significant coefficient (p < 0.001);
indicates significance p < 0.05.

attitude verb said firms Taiwan said would establish scientific technological
companies zone, receives modal attitude verb features.
Table 4 summarizes models semantic environment features interactions. interaction terms added model follow previous linguistic literature: expect scope
semantic operators (negation, modality attitude predicate) interact internal syntax mention, specifically type definiteness/quantification. results
beautifully aligned guiding linguistic hypotheses. First, negation modality
negatively correlate coreference, expected given constraints impose lifespans.
Interacting semantic features internal syntax mentions yields
expected results: since proper names pronouns scope-taking, largely unaffected
environment features, whereas indefinites, affected scope, emerge even
restricted, Karttunen others would predict.
coefficient values attitude predicates interactions seem anomalous light
semantics items. Section 2, noted non-factive attitude predicates say
cannot offer semantic guarantees mentions scope survive outside scope.
might lead one think biased long-lived mentions, fact see
opposite. However, observed pragmatic factors often facilitate exceptional anaphoric
dependencies attitude predications. Karttunen (1973) referred leakiness
predicates information introduced scope seems often percolate text level
wide range contexts (Rooryck, 2001; Simons, 2007; Harris & Potts, 2009). Since lifespan
458

fiM ODELING L IFESPAN ISCOURSE E NTITIES

# F EATURES
L INGUISTIC
URFACE
C OMBINED
C ONFIDENT

123
73,393
73,516
73,516

INGLETON
Recall Precision F1
80.2
80.2
81.1
56.0

77.5
79.9
80.8
89.8

78.8
80.0
80.9
69.0

C OREFERENT
Recall Precision F1
71.4
75.3
76.4
48.2

74.6
75.6
76.6
90.7

73.0
75.4
76.5
62.9

ACCURACY
76.3
78.0
79.0
52.5

Table 5: Recall, precision, F1 accuracy three different sets features OntoNotes
development set. C ONFIDENT C OMBINED model singleton predicted
Pr < 0.2 coreferent Pr > 0.8.

model trained real usage data, surprising reflects pragmatic factors rather
lexical semantics (de Marneffe et al., 2012).
noted earlier, features Table 4 standardly used coreference systems. Uryupina
(2009) notes Karttunen features implemented (see Section 3) significantly improve performance discourse-new mention non-antecedent detectors. Contrary
Uryupina, adding features Table 4 model incorporates features described
Table 2 Table 3 results significantly better model (likelihood ratio test, p < 0.001).
accuracy CoNLL-2012 development set improves adding Karttunen features
(McNemars test, p < 0.001).
5.4 Results
highlighted above, lifespan model built OntoNotes data confirms claims
Karttunen others concerning semantic operators interact specific kinds mention.
novel quantitative evidence theories. model successfully learns tease
singleton coreferent mentions apart, suggesting practical value NLP applications.
first row Table 5 summarizes linguistic model performance development set
OntoNotes data described Section 4, giving precision, recall, F1 measures singleton
coreferent mentions. accuracy model 76.3%. majority baseline, predicting
mentions singletons, leads accuracy 55.1%.

6. Extension Bridging
lifespan model suggests new perspective bridging anaphora, discussed briefly
Section 3.2 using example (14), repeated here:
(15)

looked room. ceiling high.

anchor phrase room superficially singleton discourse, intuitive lifespan
longer: makes salient discourse referent ceiling room, ceiling
second sentence refers to. bridging relationship keeps room alive discourse
referent, extending lifespan, though way read directly text. Together
basic tenets lifespan model, observations suggest testable hypothesis
459

fiDE

ARNEFFE , R ECASENS & P OTTS

bridging: even bridging anchors superficially singleton (henceforth, singleton anchors),1
lifespan model tend classify coreferent, since model designed
detect later mentions per se, rather capture abstract information roles
entities play discourse.
OntoNotes contain annotations bridging anaphora, evaluating hypothesis
straightforward. However, Hou, Markert, Strube (2013) annotated 50 WSJ texts
OntoNotes bridging information, yielding annotations 663 bridging anchors. these, 145
singleton anchors sense identify (Section 4) thus used assess
models ability detect abstract sense bridging anchors long-lived.
Ideally, would simply run trained lifespan model examples. proves ineffective, though, (outside Hou et al.s data) OntoNotes annotations treat singleton
anchors singleton, meaning trained lifespan model optimized data obscure
distinction interest. Nonetheless, expect feature representations form backbone
lifespan model able distinguish true singletons singleton anchors given right
kind training data. small number relevant bridging annotations poses obstacles
pursuing idea, sought navigate around follows: using annotated corpus
Hou et al., extract 145 singleton anchors sample additional 145 true
singletons documents (from total 5,804 cases). yields data set
confident makes relevant distinction. randomly divide data set 80%
training data 20% testing data, conduct standard classifier evaluation. use logistic
regression classifier, employing recursive feature elimination cross-validation (Guyon, Weston,
& Barnhill, 2002), implemented Pedregosa et al. (2011), try find compact model
effective small data set. model used `2 regularizer penalty 0.5, though
`1 regularization changes penalty delivered essentially results,
without recursive feature elimination step.
train test sets small, performance varies greatly depending nature
true singleton sample, repeat experiment 1,000 times average results.
procedure, lifespan feature representations achieve mean F1 65% (standard error 0.002;
mean precision 62%, mean recall 0.69%), indicating lifespan-based features sensitive
distinction singleton anchors true singletons. finding bolsters design
lifespan feature representations shows lifespan deeper abstract
merely counting referents. Given right kind annotations, believe model could
extended provide even fuller treatment bridging, governed partly mix
linguistic contextual factors (Hawkins, 1978; Prince, 1981b; Schwarz, 2009).

7. Predicting Lifespans Surface Features
Durrett Klein (2013) Hall et al. (2014) showed that, tasks coreference resolution
parsing, large quantity surface-level information implicitly model linguistic features, capture patterns data easily identified manually.
Given large amount annotated data available OntoNotes corpus, might expect
sufficient amount surface-level data capture linguistic insights hand-engineered
1. bridging anchors literal coreferent mentions, looked room. empty,
ceiling high., room coreferent addition providing discourse support ceiling.
set aside cases bridging experiments.

460

fiM ODELING L IFESPAN ISCOURSE E NTITIES

lifespan model defined above. therefore tested model using POS tags n-grams
fares lifespan task.
used following features surface model: lemmas words mention,
POS tags words mention, POS tag head mention, lemma
POS tags two words preceding following mention (with dummy BEGIN END
words mark beginning end sentences). suggested Durrett Klein (2013),
features might capture information encoded NER tag, number, person, sentence position.
surface models performance reported second row Table 5. models
Table 5, `2 regularization penalty chosen via five-fold cross-validation training data.
linguistic model, using tuned `2 regularization penalty rather default one makes
almost difference, substantially improves performance models features.
additionally experimented different algorithms feature selection, found
results invariably best, models, retained full sets features. last
row table gives performance model combine linguistic
surface features evaluate whether surface features alone cover information captured
linguistic features, whether linguistic features additional predictive value.
surface model performs better linguistic-only model, especially coreferent
category. However, small number linguistically-motivated features yields results
range obtained large number features surface model, might
importance tasks small amount annotated data available,
bridging experiment Section 6. (The obvious trade-off surface features easier
specify implement.) shown C OMBINED row Table 5, combined surface
feature set, linguistically-motivated features give statistically significant boost performance.
suggests surface features miss certain long-distance interactions discourse
anaphora semantic operators interactions linguistic features explicitly encode.
best model predicting lifespan combined one. Instead using standard 0.5
threshold decision boundary, make use full distribution returned logistic regression model rely confident decisions. resulting C ONFIDENT model
C OMBINED one predicts singleton Pr < 0.2 coreferent Pr > 0.8. threshold values
reported best trade-off found precision score close 0.90 without losing
much recall. expected, using highly confident model, increase precision,
though cost recall. kind model preferred depend application; noted
Ng (2004) Uryupina (2009), incorporating lifespan model downstream NLP
applications, often want highly accurate predictions, favors model C ONFIDENT.

8. Application Coreference Resolution
assess value lifespan model NLP applications, incorporate best
feature combination two state-of-the art coreference resolution systems: Stanford system
(Lee et al., 2011) Berkeley system (Durrett & Klein, 2013). cases, original
model serves baseline, focus extent lifespan model contributes
improvements baseline. allows us quantify power effectiveness
lifespan model two different systems rule-based one (Stanford) learning-based
one (Berkeley).
461

fiDE

ARNEFFE , R ECASENS & P OTTS

8.1 Evaluation Measures
evaluate incorporation lifespan model coreference systems, use English
development test sets CoNLL-2011 CoNLL-2012 Shared Tasks. Although
CoNLL shared tasks evaluated systems multi-mention (i.e., non-singleton) entities,
still expect lifespan model help: stopping singletons linked multi-mention
entities, expect see increase precision. evaluation uses measures given
CoNLL scorer:
MUC (Vilain, Burger, Aberdeen, Connolly, & Hirschman, 1995): Link-based metric
measures many links gold system partitions common.
B3 (Bagga & Baldwin, 1998): Mention-based metric measures proportion mention
overlap gold predicted entities.
CEAF-3 (Luo, 2005): Mention-based metric that, unlike B3 , enforces one-to-one alignment gold predicted entities.
CEAF-4 (Luo, 2005): entity-based version metric.
CoNLL (Denis & Baldridge, 2009; Pradhan, Ramshaw, Marcus, Palmer, Weischedel, & Xue,
2011): Average MUC, B3 CEAF-4 .
BLANC (Recasens & Hovy, 2011): Link-based metric takes mean coreference
non-coreference links, thereby rewarding (but over-rewarding) singletons.
use new CoNLL coreference scorer (Pradhan, Luo, Recasens, Hovy, Ng, & Strube, 2014,
version 8.0), fixes bug previous versions concerning way gold predicted mentions
aligned evaluating automatically predicted mentions. new scorer modify
either gold system output, implements measures originally proposed, extends
BLANC successfully handle predicted mentions, following Luo, Pradhan, Recasens, Hovy
(2014).
8.2 Incorporating Lifespan Model Stanford Coreference System
Stanford system highest-scoring system CoNLL-2011 Shared Task (Pradhan
et al., 2011), part highest-scoring system (Fernandes, dos Santos, & Milidiu,
2012) CoNLL-2012 Shared Task (Pradhan et al., 2012). rule-based system includes
total ten rules (or sieves) entity coreference, exact string match pronominal
resolution. sieves applied highest lowest precision, rule adding coreference
links. coreference resolution sieve, documents mentions traversed left right.
prune search space, mention already linked another one previous sieve,
mention first textual order considered subsequent sieves. Furthermore,
mentions headed indefinite pronoun (e.g., some, other) start indefinite
determiner (a, an) discarded antecedent exact string.
mention compared previous mentions text coreferent antecedent found
(according current sieve) beginning text reached. Candidates sorted using
left-to-right breadth-first traversal parse tree, favors subjects syntactic salience
general.
lifespan model improve coreference resolution two different ways: (i) mentions classified singletons considered either antecedents coreferent, (ii) mentions
462

fiM ODELING L IFESPAN ISCOURSE E NTITIES

classified coreferent linked mention(s). successfully predicting singletons (i), enhance systems precision; successfully predicting coreferent mentions (ii),
improve systems recall. focus (i) use lifespan model detecting
singletons. decision motivated two factors. First, given large number singletons
(Figure 1), likely see gain performance discarding singletons. Second,
multi-sieve nature Stanford coreference system make straightforward decide
antecedent mention linked even know coreferent.
integrate singleton model Stanford coreference system, depart previous
work letting sieve consider whether pair mentions coreferent mentions
classified singletons C ONFIDENT model mentions named entity.
this, discard 29% NPs consideration. Experiments development set yielded
higher performance taking account named entities. Performance higher
C ONFIDENT model TANDARD model.
therefore use lifespan model help coreference resolution pre-filtering step
coreference resolution, discarding mentions tagged singletons lifespan model. Previous
work incorporating non-referentiality discourse-new detection module pre-processing
step coreference resolution shown mixed results, discussed Section 3. general
arguments pipeline vs. joint approaches apply here: pipeline approaches prevent recovering
errors earlier pipeline, joint approaches tend increase model complexity associated
optimization challenges, easily allow separating different modules, makes
feature design error analysis difficult well. case, context Stanford
systems sieve-architecture, natural add lifespan model pre-filtering step.
8.2.1 R ESULTS
Table 6 summarizes performance Stanford system CoNLL-2011 CoNLL-2012
development test sets. evaluate incorporation lifespan model realistic setting,
use automatic parses, POS NER tags provided CoNLL documents.
scores automatically predicted mentions. baseline Stanford coreference system,
w/ Lifespan system extended lifespan model discard singletons, explained
above. Stars indicate statistically significant difference (Wilcoxon signed-rank test, p < 0.05)
according jackknifing (10 partitions development set test set, balanced
different domains2 corpus). expected, lifespan model significantly increases precision
(up +4.0 points) decreases recall (by 0.7 points). Overall, however, gain precision
higher loss recall, obtain significant improvement 0.41.5 points F1
score evaluation measures.
8.2.2 E RROR NALYSIS
Kummerfeld Klein (2013) provide useful tool automatically analyzing categorizing
errors made coreference resolution systems. tool identifies seven intuitive error types: span
error, conflated entities (entity mentions corefer clustered together), extra entity
(entities gold data added), extra mention (the system incorrectly introduces
2. mentioned Section 4, OntoNotes corpus contains documents seven different domains coreference
performance shown vary highly depending domain (Pradhan et al., 2012).

463

fiDE

CoNLL
F1

Stanford
2011 DEV SET
Baseline
w/ Lifespan
Discourse-new

R

ARNEFFE , R ECASENS & P OTTS

MUC
P

F1

R

B3
P

F1

R

CEAF-4
P
F1

51.49
52.23*
51.52

58.00* 55.97 56.97
57.57 57.72* 57.65*
56.30 58.98* 57.61*

48.01* 49.81 48.89
47.45 51.62* 49.45*
45.51 52.33* 48.68

54.27* 44.03 48.62
53.46 46.27* 49.60*
48.63 47.93* 48.28

2011 TEST SET
Baseline
50.55
w/ Lifespan
51.58*
Discourse-new 51.26*

60.09* 56.09 58.02
59.75 58.32* 59.03*
58.92 59.71* 59.31*

47.57* 47.91 47.74
47.06 50.18* 48.57*
45.72 51.06* 48.25*

52.28* 40.90 45.89
51.42 43.50* 47.13*
47.41 45.1* 46.22

2012 DEV SET
Baseline
w/ Lifespan
Discourse-new

55.26
55.77*
53.63

61.36* 65.26 63.25
60.99 66.70* 63.72*
60.71 63.27 61.96

48.35* 57.05 52.34
47.87 58.57* 52.68*
47.25 54.42 50.58

53.86* 47.01 50.20
53.10 48.91* 50.92*
49.35 47.41* 48.36

2012 TEST SET
Baseline
53.31
w/ Lifespan
54.58*
Discourse-new 53.01

62.05* 61.35 61.70
61.31 65.61* 63.39*
61.22 62.73* 61.97

48.00* 52.66 50.22
46.91 57.05* 51.49*
46.72 53.62* 49.93

52.29* 44.36 48.00
51.03 46.87* 48.86*
48.38 45.92* 47.12

(a)

Stanford

R

CEAF-3
P
F1

R

BLANC
P
F1

2011 DEV SET
Baseline
w/ Lifespan
Discourse-new

57.11* 52.50 54.71
56.55 54.43* 55.47*
54.02 55.67* 54.83

45.04* 46.84 45.14
44.37 48.65* 45.85*
42.59 49.57* 45.60

2011 TEST SET
Baseline
w/ Lifespan
Discourse-new

55.57* 49.56 52.39
55.04 51.80* 53.37*
53.2
53.08* 53.14*

46.46* 47.51 46.12
45.98 49.53* 47.06*
44.87 50.82* 47.33*

2012 DEV SET
Baseline
w/ Lifespan
Discourse-new

56.59* 57.22 56.90
56.11 58.75* 57.40*
55.00 56.18 55.58

48.78* 56.47 51.94
48.23 57.94* 52.36*
48.11 54.12 50.73

2012 TEST SET
Baseline
w/ Lifespan
Discourse-new

56.12* 53.46 54.76
54.98 56.69* 55.82*
54.43 54.78* 54.60

49.08* 54.48 50.88
47.69 59.15* 52.28*
47.95 55.81* 51.14*

(b)

Table 6: Performance Stanford system CoNLL-2011 CoNLL-2012 development
test sets. Scores (v8.0 CoNLL scorer) automatically predicted mentions,
using CoNLL automatic annotations. Stars w/ Lifespan Discourse-new
rows indicate significant difference baseline (Wilcoxon signed-rank test, p <
0.05).
464

fiM ODELING L IFESPAN ISCOURSE E NTITIES

Error

they1

Gold
scientists1
they1
family2
they2

Extra entity

various major Hong Kong media
media




Extra mention

book
book



book


Conflated entities

System
scientists1
they1

(a) Errors affecting precision.

Error

System
scientists1
they1
family2
they1

Gold
scientists1
they1
family2
they2

Missing entity




network


Missing mention

two mothers



two mothers

two mothers lost loved ones

Divided entity

(b) Errors affecting recall.

Table 7: Illustration error types provided Kummerfeld Kleins (2013) system: errors
made Stanford coreference system CoNLL-2012 development set.

mention coreferent cluster),3 divided entity (an entity split two different
clusters),4 missing entity (the system fails detect entity), missing mention (an entity
missing one mentions). Table 7 illustrates error types interested in,5 showing errors
made Stanford system, separated affecting precision affecting recall.
ran Kummerfeld Kleins (2013) system Stanford output quantify improvement obtained incorporating lifespan model coreference system CoNLL-2012
development set. Figure 2 shows difference errors original Stanford coreference
system system lifespan model integrated. lifespan model generally
reduces errors affecting precision, notably getting rid spurious entities (Extra
entity). top three errors Table 7 precision-related fixed integrating lifespan model Stanford system. hand, bottom two errors recall-related
3. distinction two categories conflated entities extra mention makes sense corpus
OntoNotes singletons annotated: former occurs system clusters one mentions
multi-mention entity incorrect entity, whereas latter occurs system incorrectly clusters
others mention truly part singleton entity (and annotated gold).
4. conflated-entities error divided-entity error often co-occur.
5. span error category relevant comparison here: systems (with without lifespan) work
predicted mentions.

465

fiDE

ARNEFFE , R ECASENS & P OTTS

Stanford alone

Conflated entities

Extra entity

Extra mention

lifespan

897

Stanford alone

728

lifespan

Stanford alone

535

lifespan

523

Stanford alone

Divided entities

Missing entity

Missing mention

1635
1607

lifespan

2038
2021

830

Stanford alone

877

lifespan

Stanford alone

1154

lifespan

1158

Figure 2: Number errors Stanford coreference system (with without lifespan
model) CoNLL-2012 development set.

introduced lifespan model. However, cumulative gain error reduction across error
categories results significant improvement overall coreference performance.
8.2.3 U SING L IFESPAN ODEL ISCOURSE -N EW ENTION C LASSIFIER
discussed Section 3.3, previous work (Ng & Cardie, 2002; Uryupina, 2009) reports
loss coreference resolution performance pre-filtering discourse-new mentions, i.e., singleton mentions well mentions start coreference chain. mimic pre-filtering,
incorporate lifespan model Stanford system following way: mentions
model classify singletons considered every sieve hypothesized corefer
previous mention, discourse-new mentions removed consideration.
so, see performance loss, shown Discourse-new rows Table 6. clear significant gains across measures, compared performance
standard Stanford system (Baseline rows). improvements see Table 6 result
pre-filtering pairs mentions lifespan model classifies singletons. stricter
constraint seems balance loss pre-filtering many mentions early stage.
8.3 Incorporating Lifespan Model Berkeley Coreference System
Berkeley coreference system (Durrett & Klein, 2013; Durrett, Hall, & Klein, 2013) currently
highest scoring coreference system publicly available. uses mention-synchronous
framework: mention, system either chooses one antecedent decides mention
starts new cluster (perhaps leading singleton cluster). log-linear model features
extracted mentions decide whether mentions anaphoric, features
extracted pairs mentions decide whether pairs corefer. baseline compare
466

fiM ODELING L IFESPAN ISCOURSE E NTITIES

takes best feature set, FINAL one, reported Durrett Klein (2013),
combines large number lexicalized surface features well semantic features.
incorporate lifespan model Berkeley system, use probabilities
mentions given lifespan model. pair mentions, add lifespan features
adding lifespan probability mention. add singleton feature mentions
lifespan probability 0.2, coreferent feature mentions lifespan
probability 0.8. Unlike Stanford architecture, exploiting coreferent predictions
straightforward (Section 8.2), learning-based setup Berkeley system allows us
make use lifespan probabilities without focusing singleton-class prediction.
Instead incorporating lifespan probabilities lifespan model, tried adding
Berkeley system features lifespan model already present Berkeley
system (i.e., features Table 3 Table 4). However, lead significant
improvements CoNLL 2012 development data, CoNLL 2012 test data.
Moreover, overall results less good incorporating probabilities manner
described above.
8.3.1 R ESULTS
Table 8 shows results Berkeley system CoNLL 2011 2012 development
test sets. Stanford system, scores automatically predicted mentions.
use automatic POS tags, parse trees, NER annotations provided CoNLL data
training testing. restrict training training data only.6 baseline FINAL
Berkeley coreference system, w/ Lifespan system extended lifespan,
singleton coreferent features, explained above. Significance computed way
Stanford system (we created 10 partitions development set test set, balanced
different domains corpus).
learning-based context Berkeley system, lifespan model increases precision
well recall, leading final improvement CoNLL score 1.0 2.0 points. Since
use lifespan model predicting singleton coreferent mentions, manage improve precision recall. provides additional empirical support splitting coreference
resolution entity-lifespan task predicts mentions refer long-lived entities
discourse coreference task focuses establishing coreference links
mentions.
8.3.2 E RROR NALYSIS
Parallel analysis Stanford coreference system output, ran Kummerfeld Kleins
(2013) system Berkeley output. Figure 3 shows difference errors original Berkeley coreference system (FINAL feature set) system enhanced lifespan
model. enhanced system commits fewer errors affecting precision (upper part Figure 3),
6. tried training gold POS tags, parse trees, NER annotations provided CoNLL data,
using automatic annotations test time. make difference original Berkeley system.
incorporating linguistic features (either lifespan probabilities features lifespan model
already Berkeley system), setting lead significant improvements baseline. However,
improvements hold consistently across development test sets: compared results obtained
training automatic annotations, training gold improves performance linguistically informed systems
test set.

467

fiDE

Berkeley

CoNLL
F1

R

ARNEFFE , R ECASENS & P OTTS

MUC
P

F1

R

B3
P

F1

R

CEAF-4
P
F1

2011 DEV SET
Baseline
59.72
w/ Lifespan 61.03*

62.67 70.22 66.23
64.78* 72.24* 68.30*

52.19 62.54 56.90
54.65* 63.28* 58.65*

53.77* 58.43 56.00
52.89 59.83* 56.15

2011 TEST SET
Baseline
59.06
w/ Lifespan 59.65*

64.14 71.68 67.70
64.96* 73.29* 68.87*

50.81 61.31 55.56
51.78* 62.38* 56.59*

51.66* 56.34 53.90
49.89 57.62* 53.48

2012 DEV SET
Baseline
61.49
w/ Lifespan 63.42*

69.06 71.32 70.17
70.76* 74.30* 72.49*

57.10 60.55 58.78
59.35* 62.79* 61.02*

55.20* 55.80 55.50
54.74 58.94* 56.76*

2012 TEST SET
Baseline
61.06
w/ Lifespan 62.15*

69.17 71.96 70.54
70.42* 74.07* 72.20*

55.77 60.50 58.04
56.87* 62.21* 59.42*

53.82* 55.37 54.58
52.64 57.20* 54.83

(a)

Berkeley

R

CEAF-3
P
F1

R

BLANC
P
F1

2011 DEV SET
Baseline
w/ Lifespan

58.82 65.37 61.92
59.29* 66.36* 62.63*

50.38 59.93 54.73
52.83* 62.92* 57.37*

2011 TEST SET
Baseline
w/ Lifespan

56.71
56.37

63.01 59.70
63.96* 59.93

49.11 59.67 53.88
50.66* 61.87* 55.68*

2012 DEV SET
Baseline
w/ Lifespan

62.29
62.65

64.01 63.14
66.18* 64.37*

60.32 60.79 60.53
62.19* 63.80* 62.86*

2012 TEST SET
Baseline
w/ Lifespan

60.83 63.12 61.95
61.05* 64.68* 62.81*

57.70 61.79 59.68
58.92* 63.93* 61.32*

(b)

Table 8: Performance Berkeley system CoNLL 2011 CoNLL 2012 development
test sets. Scores (v8.0 CoNLL scorer) automatically predicted mentions,
using CoNLL automatic annotations. Stars indicate significant difference (Wilcoxon
signed-rank test, p < 0.05).

significantly category. However, cumulative gains result significant
improvement overall precision. Globally, lifespan model fixes errors brings in.
468

fiM ODELING L IFESPAN ISCOURSE E NTITIES

Berkeley alone

Conflated entities

Extra entity

Extra mention

lifespan

Berkeley alone
lifespan

579
533

594

Berkeley alone
lifespan

508

Berkeley alone

Divided entities

Missing entity

Missing mention

1448
1412

lifespan

Berkeley alone

818

lifespan

820

Berkeley alone
lifespan

1669
1572

829
941

Figure 3: Number errors Berkeley coreference system (with without lifespan
model) CoNLL 2012 development set.

9. Conclusion
factors determine fate given discourse referent? nature (its internal morphosyntax) nurture (the broader syntactic semantic environments mentions)? lifespan
model (Section 5) suggests nature, nurture, interactions important. model
validates existing linguistic generalizations discourse anaphora (Section 2), provides new
insights previous engineering efforts similar direction (Section 3). show
linguistically-motivated features bring improvement top surface features (Section 7), demonstrating automatic language processing rely machine learning big data.
lifespan model performs well right, achieving 79% accuracy predicting whether
given mention singleton coreferent. alone could ramifications tracking topics,
identifying protagonists, discourse coherence. paper, demonstrated benefits
lifespan model coreference resolution. incorporated lifespan model two
different coreference resolution systems showed yields improvements practical
statistical significance cases (Section 8).
Stepping back, hope provided compelling illustration efforts theoretical
linguistics NLP complement other, developing models assessing
scientific engineering contexts.

Acknowledgments
thank Jefferson Barlew, Greg Durrett, Micha Elsner, Gregory Kierstead, Craige Roberts, Michael
White, Stanford NLP Group, anonymous reviewers helpful suggestions earlier drafts paper. research supported part ONR grant No. N00014-10-1-0109
ARO grant No. W911NF-07-1-0216.
469

fiDE

ARNEFFE , R ECASENS & P OTTS

References
Aissen, J. (1997). syntax obviation. Language, 73(4), 705750.
Aissen, J. (2003). Differential object marking: Iconicity vs. economy. Natural Language
Linguistic Theory, 21(3), 435483.
Aloni, M. (2000). Quantification Conceptual Covers. Ph.D. thesis, University Amsterdam.
AnderBois, S., Brasoveanu, A., & Henderson, R. (2010). Crossing appositive/at-issue meaning
boundary. Li, N., & Lutz, D. (Eds.), Proceedings Semantics Linguistic Theory 20,
pp. 328346. CLC Publications.
Bagga, A., & Baldwin, B. (1998). Algorithms scoring coreference chains. Proceedings
LREC 1998 Workshop Linguistic Coreference, pp. 563566.
Baker, C. L. (1970). Double negatives. Linguistic Inquiry, 1(2), 169186.
Barzilay, R., & Lapata, M. (2008). Modeling local coherence: entity-based approach. Computational Linguistics, 34(1), 134.
Bean, D. L., & Riloff, E. (1999). Corpus-based identification non-anaphoric noun phrases.
Proceedings 37th Annual Meeting Association Computational Linguistics,
pp. 373380. ACL.
Beaver, D. (2004). optimization discourse anaphora. Linguistics Philosophy, 27(1),
356.
Beaver, D. I. (2007). Corpus pragmatics: Something old, something new. Paper presented
annual meeting Texas Linguistic Society.
Bergsma, S., & Lin, D. (2006). Bootstrapping path-based pronoun resolution. Proceedings
21st International Conference Computational Linguistics 44th Annual Meeting
Association Computational Linguistics, pp. 3340. ACL.
Bergsma, S., Lin, D., & Goebel, R. (2008). Distributional identification non-referential pronouns.
Proceedings 46th Annual Meeting Association Computational Linguistics:
Human Language Technologies, pp. 1018. ACL.
Bergsma, S., & Yarowsky, D. (2011). NADA: robust system non-referential pronoun detection. Hendrickx, I., Lalitha Devi, S., Branco, A., & Mitkov, R. (Eds.), Anaphora Processing
Applications, Vol. 7099 Lecture Notes Computer Science, pp. 1223. Springer.
Bittner, M. (2001). Surface composition bridging. Journal Semantics, 18(2), 127177.
Brants, T., & Franz, A. (2006). Google Web 1T 5gram corpus version 1.1. LDC2006T13.
Byron, D. K., & Gegg-Harrison, W. (2004). Eliminating non-referring noun phrases coreference resolution. Proceedings Discourse Anaphora Reference Resolution Conference, pp. 2126.
Chafe, W. L. (1976). Givenness, contrastiveness, definiteness, subjects, topics, point view.
Li, C. N. (Ed.), Subject Topic, pp. 2555. Academic Press.
Clark, H. H. (1975). Bridging. Schank, R. C., & Nash-Webber, B. L. (Eds.), Theoretical Issues
Natural Language Processing, pp. 169174. ACM.
470

fiM ODELING L IFESPAN ISCOURSE E NTITIES

Cresswell, M. J. (2002). Static semantics dynamic discourse. Linguistics Philosophy, 25(5
6), 545571.
de Marneffe, M.-C., MacCartney, B., & Manning, C. D. (2006). Generating typed dependency
parses phrase structure parses. Proceedings Fifth International Conference
Language Resources Evaluation, pp. 449454. ACL.
de Marneffe, M.-C., Manning, C. D., & Potts, C. (2012). happen? pragmatic complexity
veridicality assessment. Computational Linguistics, 38(2), 301333.
Delmonte, R., Bristot, A., Piccolino Boniforti, M. A., & Tonelli, S. (2007). Entailment anaphora
resolution RTE3. Proceedings ACL-PASCAL Workshop Textual Entailment
Paraphrasing, pp. 4853.
Denis, P., & Baldridge, J. (2007). Joint determination anaphoricity coreference resolution
using integer programming. Human Language Technologies 2007: Conference
North American Chapter Association Computational Linguistics; Proceedings
Main Conference, pp. 236243. ACL.
Denis, P., & Baldridge, J. (2009). Global joint models coreference resolution named entity
classification. Procesamiento del Lenguaje Natural, 42, 8796.
Durrett, G., Hall, D., & Klein, D. (2013). Decentralized entity-level modeling coreference
resolution. Proceedings 51st Annual Meeting Association Computational
Linguistics (Volume 1: Long Papers), pp. 114124. ACL.
Durrett, G., & Klein, D. (2013). Easy victories uphill battles coreference resolution.
Proceedings 2013 Conference Empirical Methods Natural Language Processing,
pp. 19711982. ACL.
Elbourne, P. (2008). Demonstratives individual concepts. Linguistics Philosophy, 31(4),
409466.
Evans, R. (2001). Applying machine learning toward automatic classification it. Literary
Linguistic Computing, 16(1), 4557.
Fernandes, E., dos Santos, C., & Milidiu, R. (2012). Latent structure perceptron feature induction unrestricted coreference resolution. Joint Conference EMNLP CoNLL
- Shared Task, pp. 4148. ACL.
Fodor, J. D., & Sag, I. A. (1982). Referential quantificational indefinites. Linguistics
Philosophy, 5(3), 355398.
Fraurud, K. (1990). Definiteness processing noun phrases natural discourse. Journal
Semantics, 7(4), 395433.
Giampiccolo, D., Magnini, B., Dagan, I., & Dolan, B. (2007). third PASCAL recognizing
textual entailment challenge. Proceedings ACL-PASCAL Workshop Textual Entailment Paraphrasing, pp. 19.
Groenendijk, J., & Stokhof, M. (1991). Dynamic predicate logic. Linguistics Philosophy, 14(1),
39100.
Grosz, B. J., Joshi, A. K., & Weinstein, S. (1995). Centering: framework modeling local
coherence discourse. Computational Linguistics, 21(2), 203225.
471

fiDE

ARNEFFE , R ECASENS & P OTTS

Guyon, I., Weston, J., & Barnhill, S. (2002). Gene selection cancer classification using support
vector machines. Machine Learning, 46(13), 389422.
Hall, D., Durrett, G., & Klein, D. (2014). Less grammar, features. Proceedings 52nd
Annual Meeting Association Computational Linguistics (Volume 1: Long Papers),
pp. 228237. ACL.
Harris, J. A., & Potts, C. (2009). Perspective-shifting appositives expressives. Linguistics
Philosophy, 32(6), 523552.
Hawkins, J. A. (1978). Definiteness Indefiniteness. Croom Helm.
Heim, I. (1982). Semantics Definite Indefinite Noun Phrases. Ph.D. thesis, UMass
Amherst.
Heim, I. (1992). Presupposition projection semantics attitude verbs. Journal Semantics,
9(2), 183221.
Hobbs, J. R. (1979). Coherence coreference. Cognitive Science, 3(1), 6790.
Hou, Y., Markert, K., & Strube, M. (2013). Global inference bridging anaphora resolution.
Proceedings 2013 Conference North American Chapter Association
Computational Linguistics: Human Language Technologies, pp. 907917. ACL.
Israel, M. (1996). Polarity sensitivity lexical semantics. Linguistics Philosophy, 19(6),
619666.
Israel, M. (2001). Minimizers, maximizers, rhetoric scalar reasoning. Journal Semantics, 18(4), 297331.
Israel, M. (2004). pragmatics polarity. Horn, L., & Ward, G. (Eds.), Handbook
Pragmatics, pp. 701723. Blackwell.
Ji, H., & Lin, D. (2009). Gender animacy knowledge discovery web-scale n-grams
unsupervised person mention detection. Proceedings 23rd Pacific Asia Conference
Language, Information Computation, pp. 220229.
Kamp, H. (1981). theory truth discourse representation. Groenendijk, J., Janssen,
T. M. V., & Stockhof, M. (Eds.), Formal Methods Study Language, pp. 277322.
Mathematical Centre.
Karttunen, L. (1973). Presuppositions compound sentences. Linguistic Inquiry, 4(2), 169193.
Karttunen, L. (1976). Discourse referents. McCawley, J. D. (Ed.), Syntax Semantics, Vol. 7:
Notes Linguistic Underground, pp. 363385. Academic Press.
Kehler, A. (2002). Coherence, Reference, Theory Grammar. CSLI.
Kummerfeld, J. K., & Klein, D. (2013). Error-driven analysis challenges coreference resolution. Proceedings 2013 Conference Empirical Methods Natural Language
Processing, pp. 265277. ACL.
Ladusaw, W. A. (1996). Negation polarity items. Lappin, S. (Ed.), Handbook Contemporary Semantic Theory, pp. 321341. Blackwell.
472

fiM ODELING L IFESPAN ISCOURSE E NTITIES

Lee, H., Peirsman, Y., Chang, A., Chambers, N., Surdeanu, M., & Jurafsky, D. (2011). Stanfords
multi-pass sieve coreference resolution system CoNLL-2011 shared task. Proceedings 15th Conference Computational Natural Language Learning: Shared Task, pp.
2834. ACL.
Luo, X. (2005). coreference resolution performance metrics. Proceedings Human Language Technology Conference Conference Empirical Methods Natural Language
Processing, pp. 2532. ACL.
Luo, X., Pradhan, S., Recasens, M., & Hovy, E. (2014). extension BLANC system mentions. Proceedings 52nd Annual Meeting Association Computational
Linguistics, pp. 2429. ACL.
Muller, C. (2006). Automatic detection nonreferential spoken multi-party dialog. Proceedings European Chapter Association Computational Linguistics, pp. 4956.
ACL.
Muskens, R., van Benthem, J., & Visser (1997). Dynamics. van Benthem, J., & ter Meulen, A.
(Eds.), Handbook Logic Language, pp. 587648. Elsevier.
Ng, V. (2004). Learning noun phrase anaphoricity improve coreference resolution: Issues
representation optimization. Proceedings 42nd Annual Meeting Association
Computational Linguistics, pp. 152159. ACL.
Ng, V., & Cardie, C. (2002). Identifying anaphoric non-anaphoric noun phrases improve
coreference resolution. Proceedings 19th International Conference Computational Linguistics, pp. 17. ACL.
Nissim, M. (2006). Learning information status discourse entities. Proceedings 2006
Conference Empirical Methods Natural Language Processing, pp. 94102.
Paice, C. D., & Husk, G. D. (1987). Towards automatic recognition anaphoric features
English text: impersonal pronoun it. Computer Speech & Language, 2(2), 109132.
Partee, B. H. (1987). Noun phrase interpretation type-shifting principles. Groenendijk,
J., de Jong, D., & Stokhof, M. (Eds.), Studies Discourse Representation Theory
Theory Generalized Quantifiers, pp. 115143. Foris Publications.
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M.,
Perrot, M., & Duchesnay, E. (2011). Scikit-learn: Machine learning Python. Journal
Machine Learning Research, 12, 28252830.
Poesio, M., Alexandrov-Kabadjov, M., Vieira, R., Goulart, R., & Uryupina, O. (2005).
discourse-new detection help definite description resolution?. Proceedings 6th International Workshop Computational Semantics, pp. 236246.
Poesio, M., Uryupina, O., Vieira, R., Alexandrov-Kabadjov, M., & Goulart, R. (2004). Discoursenew detectors definite description resolution: survey preliminary proposal.
Harabagiu, S., & Farwell, D. (Eds.), ACL 2004: Workshop Reference Resolution
Applications, pp. 4754. ACL.
Potts, C. (2005). Logic Conventional Implicatures. Oxford University Press.
473

fiDE

ARNEFFE , R ECASENS & P OTTS

Potts, C. (2012). Conventional implicature expressive content. Maienborn, C., von Heusinger,
K., & Portner, P. (Eds.), Semantics: International Handbook Natural Language Meaning, Vol. 3, pp. 25162536. Mouton de Gruyter.
Pradhan, S., Luo, X., Recasens, M., Hovy, E., Ng, V., & Strube, M. (2014). Scoring coreference partitions predicted mentions: reference implementation. Proceedings
52nd Annual Meeting Association Computational Linguistics, pp. 3035. ACL.
https://github.com/conll/reference-coreference-scorers.
Pradhan, S., Moschitti, A., Xue, N., Uryupina, O., & Zhang, Y. (2012). Conll-2012 shared task:
Modeling multilingual unrestricted coreference ontonotes. Joint Conference EMNLP
CoNLL - Shared Task, pp. 140. ACL.
Pradhan, S., Ramshaw, L., Marcus, M., Palmer, M., Weischedel, R., & Xue, N. (2011). CoNLL2011 shared task: Modeling unrestricted coreference OntoNotes. Proceedings
Fifteenth Conference Computational Natural Language Learning: Shared Task, pp. 127.
ACL.
Pradhan, S. S., & Xue, N. (2009). Ontonotes: 90% solution. Proceedings Human Language Technologies: 2009 Annual Conference North American Chapter Association Computational Linguistics, Companion Volume: Tutorial Abstracts, pp. 1112.
ACL.
Prasad, R., Dinesh, N., Lee, A., Miltsakaki, E., Robaldo, L., Joshi, A., & Webber, B. (2008).
Penn Discourse Treebank 2.0. Proceedings Sixth International Language Resources
Evaluation, pp. 29612968. European Language Resources Association.
Prince, E. (1981a). inferencing indefinite NPs. Webber, B. L., Sag, I., & Joshi,
A. (Eds.), Elements Discourse Understanding, pp. 231250. Cambridge University Press.
Prince, E. F. (1981b). Toward taxonomy givennew information. Cole, P. (Ed.), Radical
Pragmatics, pp. 223255. Academic Press.
R Development Core Team (2013). R: Language Environment Statistical Computing. R
Foundation Statistical Computing.
Recasens, M., de Marneffe, M.-C., & Potts, C. (2013). life death discourse entities:
Identifying singleton mentions. Human Language Technologies: 2013 Annual Conference North American Chapter Association Computational Linguistics, pp.
627633. ACL.
Recasens, M., & Hovy, E. (2009). deeper look features coreference resolution.
Lalitha Devi, S., Branco, A., & Mitkov, R. (Eds.), Anaphora Processing Applications,
Vol. 5847 Lecture Notes Computer Science, pp. 2942. Springer.
Recasens, M., & Hovy, E. (2011). BLANC: Implementing Rand index coreference evaluation. Natural Language Engineering, 17(4), 485510.
Roberts, C. (1990). Modal Subordination, Anaphora, Distributivity. Garland.
Roberts, C. (1996). Anaphora intensional contexts. Lappin, S. (Ed.), Handbook Contemporary Semantic Theory, pp. 215246. Blackwell.
Rooryck, J. (2001). Evidentiality, Part II. Glot International, 5(5), 161168.
474

fiM ODELING L IFESPAN ISCOURSE E NTITIES

Saur, R. (2008). Factuality Profiler Eventualities Text. Ph.D. thesis, Brandeis University.
Schwarz, F. (2009). Two Types Definites Natural Language. Ph.D. thesis, UMass Amherst.
Schwarzschild, R. (2002). Singleton indefinites. Journal Semantics, 19(3), 289314.
Simons, M. (2007). Observations embedding verbs, evidentiality, presupposition. Lingua,
117(6), 10341056.
Uryupina, O. (2003). High-precision identification discourse new unique noun phrases.
Proceedings 41st Annual Meeting Association Computational Linguistics
Student Research Workshop, pp. 8086. ACL.
Uryupina, O. (2009). Detecting anaphoricity antecedenthood coreference resolution. Procesamiento del lenguaje natural, 42, 113120.
van Deemter, K., & Kibble, R. (2000). coreferring: Coreference MUC related annotation
schemes. Computational linguistics, 26(4), 629637.
Vieira, R., & Poesio, M. (2000). empirically based system processing definite descriptions.
Computational Linguistics, 26(4), 539593.
Vilain, M., Burger, J., Aberdeen, J., Connolly, D., & Hirschman, L. (1995). model-theoretic
coreference scoring scheme. Proceedings 6th Message Understanding Conference,
pp. 4552. Morgan Kaufman.
Walker, M. A., Joshi, A. K., & Prince, E. F. (Eds.). (1997). Centering Discourse. Oxford University Press.
Wang, L., McCready, E., & Asher, N. (2006). Information dependency quantificational subordination. von Heusinger, K., & Turner, K. (Eds.), Semantics Meets Pragmatics, pp.
267304. Elsevier.
Ward, G., & Birner, B. (2004). Information structure non-canonical syntax. Horn, L. R., &
Ward, G. (Eds.), Handbook Pragmatics, pp. 153174. Blackwell Publishing Ltd.

475


