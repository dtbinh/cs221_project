journal artificial intelligence

submitted published

agnostic pointwise competitive selective classification
yair wiener
ran el yaniv

wyair tx technion ac il
rani cs technion ac il

computer science department
technion israel institute technology
haifa israel

abstract
pointwise competitive classifier class f required classify identically
best classifier hindsight f noisy agnostic settings present strategy
learning pointwise competitive classifiers finite training sample provided
classifier abstain prediction certain region choice interesting hypothesis classes families distributions
measure

rejected region

shown diminishing rate polylog log
high probability sample size standard confidence parameter
smoothness parameters bernstein type condition associated excess loss class
related f loss exact implementation proposed learning strategy
dependent erm oracle hard compute agnostic case thus
consider heuristic approximation procedure svms empirically
consistently outperforms traditional rejection mechanism
distance decision boundary

introduction
given labeled training set class f possible select f
finite training sample model whose predictions identical best model
hindsight classical statistical learning theory surely preclude
possibility within standard model changing rules game possible
indeed consider game classifier allowed abstain prediction without
penalty region choice k classification reject option game
assuming noise free realizable setting shown el yaniv wiener
one train perfect classifier never errs whenever willing predict
abstaining render perfect classification vacuous shown
quite broad set specified underlying distribution family
hypothesis class perfect realizable classification achievable rejection rate
diminishes quickly zero training sample size
general perfect classification cannot achieved noisy setting
objective achieve pointwise competitiveness property ensuring prediction
every non rejected test point identical prediction best predictor hindsight
class consider pointwise competitive selective classification
generalize el yaniv wiener agnostic case particular
pointwise competitive classification achievable high probability
learning strategy called low error selective strategy less given training sample sm

c

ai access foundation rights reserved

fiwiener el yaniv

hypothesis class f less outputs pointwise competitive selective classifier f g
f x standard classifier g x selection function qualifies
predictions dont knows see definitions section classifier f simply
taken empirical risk minimizer erm classifier f pointwise competitiveness
achieved g follows standard concentration inequalities
true risk minimizer f achieves empirical error close f thus high
probability f belongs class low empirical error hypotheses left
set g x allows prediction label x f x
hypotheses low error class unanimously agree label x
simpler realizable setting el yaniv wiener low error class simply reduces
version space
bulk analysis sections concerns coverage bounds less
namely showing measure region classifier f g refuses classify
diminishes quickly high probability training sample size grows see section
formal definition provide several general distribution dependent coverage
bounds particular corollaries respectively high probability
bounds coverage f g classifier f g form


f g polylog log

linear unknown distribution p x x feature space points
labels whose marginal p x finite mixture gaussians axis
aligned rectangles p x whose marginal p x product distribution
bernstein class smoothness parameters depending hypothesis class
underlying distribution loss function case
outset efficient implementation less seems reach
required track supremum empirical error possibly infinite hypothesis
subset general might intractable overcome computational difficulty
propose reduction calculating two constrained erms
given test point x calculate erm training sample sm
constraint label x one positive label constraint one negative
thresholding difference empirical error two constrained erms
equivalent tracking supremum entire infinite hypothesis subset
reduction introduce section disbelief principle motivates heuristic implementation less relies constrained svms mimics optimal
behavior
section present numerical examples medical classification
examine empirical performance compare performance
widely used selective classification method rejection distance
decision boundary

pointwise competitive selective classification preliminary
definitions
let x feature space example dimensional vectors rd
output space standard classification goal learn classifier f x


fiagnostic pointwise competitive selective classification

finite training sample labeled examples sm xi yi
assumed sampled
unknown underlying distribution p x x classifier
selected hypothesis class f let bounded loss function
selective classification el yaniv wiener learning receives sm
required output selective classifier defined pair f g f f
classifier g x selection function serving qualifier f follows
x x f g x f x iff g x otherwise classifier outputs dont know
general performance selective predictor characterized terms two quantities coverage risk coverage f g f g ep g x true risk f g
respect loss function average loss f restricted region activity
qualified g normalized coverage r f g ep f x g x f g
easy verify g therefore f g r f g reduces
famil pm
iar risk functional r f ep f x classifier f let r f f xi yi
standard empirical error f sample sm let f argminf f r f
empirical risk minimizer erm let f argminf f r f true risk minimizer
respect unknown distribution p x clearly true risk minimizer f unknown selective classifier f g called pointwise competitive x x
g x f x f x

low error selective strategy less
hypothesis class f hypothesis f f distribution p sample sm real number
r define true empirical low error sets


v f r f f r f r f r



n

v f r f f r f r f r



throughout denote slack standard uniform deviation
bound given terms training sample size confidence parameter
vc dimension class f


ln
ln





following theorem slight extension statement made bousquet boucheron
lugosi p
theorem bousquet et al let loss function f hypothesis
class whose vc dimension probability least
choice sm p hypothesis f f satisfies
r f r f
similarly r f r f conditions


formally f classifier r f inf f f r f inf f f p x f x f x
existence measurable f guaranteed sufficient considerations see hanneke
pp



fiwiener el yaniv

remark use theorem particular vc bounds classification
loss mandatory developing theory presented similar
theories developed types bounds e g rademacher compression
bounds learning
let g f disagreement set hanneke el yaniv wiener w r g
defined
dis g x x f f g f x f x

let us motivate low error selective strategy less whose pseudo code appears
strategy strategy define whenever empirical risk minimizer erm exists
example case loss standard uniform deviation bound
one theorem one training error true risk minimizer f
cannot far training error empirical risk minimizer f therefore

guarantee high probability empirical low error class v f r applied
appropriately chosen r includes true risk minimizer f selection function
g constructed accept subset domain x hypotheses
empirical low error set unanimously agree strategy formulates idea call
strategy rather lacks implementation details indeed
clear outset strategy implemented
strategy agnostic low error selective strategy less
input sm
output pointwise competitive selective classifier h g w p

set f erm
f sm e f empirical risk minimizer f w r sm
set g v f see eq
construct g g x x x dis g
f f
begin analysis less following lemma establishes pointwise competitiveness section develop general coverage bounds terms undetermined
disagreement coefficient section present distribution dependent bounds
rely disagreement coefficient
lemma pointwise competitiveness let loss function f hypothesis
class whose vc dimension let given let f g selective classifier
chosen less probability least f g pointwise competitive
selective classifier
proof theorem probability least
r f r f
clearly since f minimizes true error r f r f applying theorem
know probability least
r f r f


fiagnostic pointwise competitive selective classification

union bound follows probability least
r f r f
hence probability least


f v f g

definition less constructs selection function g x equals one iff x
x dis g thus x x g x hypotheses g agree
particular f f agree therefore f g pointwise competitive high probability

general coverage bounds less terms disagreement
coefficient
require following definitions facilitate coverage analysis f f
r define set b f r hypotheses reside ball radius r around f





b f r f f pr f x f x r
xp

g f distribution p denote g disagreement set
g see g pr dis g let r disagreement coefficient hanneke
hypothesis class f respect target distribution p
r f r sup

r r

b f r

r



disagreement coefficient utilized later analysis see discussion
characteristics corollary associated excess loss class class f
loss function massart mendelson bartlett mendelson philips
defined
xl f x f x f x f f
whenever f fixed abbreviate xl xl f x xl said
bernstein class respect p every h xl
satisfies
eh eh

bernstein classes arise many natural situations see e g koltchinskii bartlett
mendelson bartlett wegkamp example conditional probability
p x bounded away satisfies tsybakovs noise conditions
excess loss function bernstein class bartlett mendelson tsybakov


data generated unknown deterministic hypothesis limited noise p x
bounded away



specifically loss assumption proposition work tsybakov equivalent

bernstein class condition equation
tsybakov noise
parameter



fiwiener el yaniv

following sequence lemmas theorems assume binary hypothesis class f
vc dimension underlying distribution p x loss
function xl denotes associated excess loss class extended
loss functions similar techniques used beygelzimer dasgupta
langford
figure schematically depict hypothesis class f gray area target
hypothesis filled black circle outside f best hypothesis class f
distance two points diagram relates distance two hypothesis
marginal distribution p x first observation excess loss class
bernstein class set low true error depicted figure resides
within larger ball centered around f see figure b

figure set low true error resides within ball around f b
lemma xl bernstein class respect p r


v f r b f r

proof f v f r definition e f x e f x r
linearity expectation
e f x f x r
since xl bernstein
e f x f x e f x f x
n

e f x f x eh eh
e f x f x


e f x f x r therefore definition f b f r




fiagnostic pointwise competitive selective classification

far seen set low true error resides within ball around f
would prove high probability set low empirical error depicted
figure resides within set low true error see figure b emphasize
distance hypotheses figure empirical error
distance figure b true error

figure set low empirical error resides within set low true error b
lemma r probability least
v f r v f r
proof f v f r definition r f r f r since f minimizes empirical
error know r f r f theorem twice applying union bound
see probability least
r f r f



r f r f

therefore
r f r f r

f v f r

shown high probability set low empirical error subset
certain ball around f therefore probability least two hypotheses set
low empirical error disagree bounded probability
least two hypotheses ball around f disagree fortunately
latter bounded disagreement coefficient established following lemma



fiwiener el yaniv

lemma r probability least
v f r r r
r disagreement coefficient f respect p applied r
see
proof applying lemmas get probability least


v f r b f r

therefore



v f r b f r

definition disagreement coefficient r r b f r r r
recalling thus observing r r
r proof complete
position state first coverage bound selective classifier
constructed less bound given terms disagreement coefficient
corollary let f hypothesis class theorem assume xl
bernstein class w r p let f g selective classifier constructed less
probability least f g pointwise competitive selective classifier

f g r

r disagreement coefficient f respect p r

proof
lemma
probability least f g pointwise competitive set

g v f construction f f selection function g x equals
one iff x x dis g thus definition coverage f g e g x g
therefore applications lemma union bound imply probability
least f g pointwise competitive coverage satisfies
f g e g x g r

noting r monotone non increasing r know coverage bound
corollary clearly applies quantity discussed numerous papers shown finite settings including thresholds r
distribution hanneke linear
separators origin
rd uniform distribution sphere hanneke linear
separators rd smooth data distribution bounded away zero c f
c f unknown constant depends target hypothesis friedman
cases application corollary sufficient guarantee pointwisecompetitiveness bounded coverage converges one unfortunately many
hypothesis classes distributions disagreement coefficient infinite hanneke
fortunately disagreement coefficient r grows slowly respect r
shown wang sufficient smoothness conditions corollary sufficient
guarantee bounded coverage



fiagnostic pointwise competitive selective classification

distribution dependent coverage bounds less
section establish distribution dependent coverage bounds less starting
point bounds following corollary
corollary let f hypothesis class theorem assume f disagreement coefficient
r polylog r

w r distribution p xl bernstein class w r distribution
let f g selective classifier chosen less probability least
f g pointwise competitive coverage satisfies



polylog

f g
log



proof plugging coverage bound corollary immediately yields

corollary states fast coverage bounds less cases disagreement coefficient grows slowly respect r recent disagreement active
learning selective prediction wiener et al wiener established tight relations disagreement coefficient empirical quantity called version space
compression set size quantity analyzed el yaniv wiener
context realizable selective classification known distribution dependent
bounds plan rest section introduce version space compression set size discuss relation disagreement coefficient
apply agnostic setting
interested solving agnostic case consider moment
realizable setting utilize known used analysis specifically
assume f f p f x x x x x
x p given training sample sm let vsf induced version space e
set hypotheses consistent given sample sm version space compression
set size denoted n sm n f sm defined size smallest subset
sm inducing version space hanneke b el yaniv wiener
function sm clearly n sm random variable specific realization sm
value unique
define version space compression set size minimal bound
bn min b n p n sm b



rely following lemma wiener et al sake self containment
provide proof appendix


disagreement coefficient grow ploy logarithmically r still r
still possible prove lower bound coverage specifically r r one

f g



fiwiener el yaniv

lemma wiener et al realizablecase bn
polylog log



bn polylog r polylog r

obviously statement lemma holds well defined within realizable
setting version space compression set size defined setting turn
back agnostic setting consider arbitrary underlying distribution p x
recall agnostic setting let f x denote measurable classifier
r f inf f f r f inf f f p x f x f x guaranteed
exist sufficient assumptions see hanneke section call f infimal
best hypothesis f w r p clearly several different infimal hypotheses
note however xl bernstein class respect p assume
lemma ensures infimal hypotheses identical measure
zero
definitions version space version space compression set size naturally
generalized agnostic setting respect infimal hypothesis wiener et al
follows let f infimal hypothesis f w r p agnostic version space
sm
vsf sm f f f x sm f x f x

agnostic version space compression set size denoted n sm n f sm f defined
size smallest subset sm inducing agnostic version space vsf sm f
finally extend definition version space compression set minimal bound
agnostic setting follows
bn f min b n p n f sm f b

key observation allows surprisingly easy utilization lemma
agnostic setting disagreement coefficient depends hypothesis class
f marginal distribution p x infimal hypothesis f therefore
take agnostic learning consider realizable projection whereby points
labeled f marginal distribution p x two
essentially disagreement coefficients idea initially observed
hanneke wiener formulate slight variation
formulation work wiener hanneke el yaniv
define disagreement agnostic setting respect infimal hypothesis f agnostic learning f p define realizable
projection f p follows let f f f f infimal hypothesis
agnostic define p distribution marginal p x p x
p f x x x x x easy verify f p realizable
learning e f f pp x f x x x x x
lemma realizable projection given agnostic learning f p let
f p realizable projection let r r associated disagreement coefficients agnostic realizable projection respectively r r

proof first note depend respectively p p via f
marginal distributions p x p x since f f f f readily get
r r


fiagnostic pointwise competitive selective classification

let us summarize derivation given agnostic f p consider
realizable projection f p bn polylog log bn
polylog realizable lemma r polylog r
lemma holds original agnostic therefore corollary
applies obtain fast coverage bound less w r f p
agnostic coverage bounds less obtained following known bounds
realizable version space compression set size first one el yaniv wiener
applies learning linear separators mixture gaussian
distributions following theorem direct application lemma work
el yaniv wiener
theorem el yaniv wiener n n let x rd f space
linear separators rd p distribution marginal rd mixture
n multivariate normal distributions constant cd n depending
n otherwise independent p
bn cd n log
applying theorem together lemma lemma corollary immediately
yields following
corollary assume conditions theorem assume xl bernstein class w r p x let f g selective classifier constructed less
probability least f g pointwise competitive selective classifier



f g polylog log
second version space compression set size bound concerns realizable learning
axis aligned rectangles product densities rn bounds previously
proposed wiener hanneke el yaniv el yaniv wiener
state without proof recent bound wiener hanneke el yaniv giving
version space compression set size bound learning whose positive class
bounded away zero

theorem wiener et al n let x rd p
marginal distribution rd product densities rd marginals
continuous cdfs f space axis aligned rectangles f rd
p x f x



bn
ln



application theorem together lemma lemma corollary yields following corollary
corollary n let x rd let p x underlying
distribution marginal p x product densities rd marginals
continuous cdfs let f space axis aligned rectangles f rd p x f x
assume xl bernstein class w r p x let f g


fiwiener el yaniv

selective classifier constructed less probability least f g
pointwise competitive selective classifier


f g polylog log

erm oracles disbelief principle
outset efficient construction selection function g prescribed less seems
reach required verify point x question whether
hypotheses low error class agree label moreover g computed
entire domain luckily possible compute g lazy manner
compute g x calculating two constrained erms given test point x
calculate erm training sample sm constraint label x one
positive label constraint one negative thresholding difference
empirical error two constrained erms equivalent tracking supremum
entire infinite hypothesis subset following lemma establishes reduction
lemma let f g selective classifier chosen less observing training
sample sm let f empirical risk minimizer sm let x point x
define


n
fx argmin r f f x sign f x
f f

e empirical risk minimizer forced label x opposite f x
g x



r fx r f

proof first note according definition v see eq


r fx r f fx v f





prove first direction assume rhs holds
get f fx v however construction f x fx x x dis v
g x
prove direction assume r fx r f
assumption prove f v f x f x therefore x
x dis v entailing g x indeed assume contradiction exists
f v f x fx x f x construction holds
r f r fx r f
f v contradiction
lemma tells us order decide point x rejected need measure
empirical error r fx special empirical risk minimizer fx constrained
label x opposite h x error sufficiently close r h classifier cannot
sure label x must reject thus provided compute
erms decide whether predict reject individual test point x x


fiagnostic pointwise competitive selective classification

without actually constructing g entire domain x figure illustrates principle
dimensional example hypothesis class class linear classifiers r
source distribution two normal distributions negative samples represented
blue circles positive samples red squares usual f denotes empirical

figure constrained erm
risk minimizer let us assume want classify point x point classified
positive f therefore force point negative calculate restricted
erm depicted doted line marked fx difference empirical risk f
fx large enough point x rejected however want classify
point x difference empirical risk f fx quite large point
classified positive
equation motivates following definition disbelief index df x sm
individual point x specifically x x define disbelief index w r sm
f
x df x sm r fx r f
observe x large whenever model sensitive label x sense
forced bend best model fit opposite label x model
substantially deteriorates giving rise large disbelief index large x
interpreted disbelief possibility x labeled differently
case definitely predict label x unforced model conversely
x small model indifferent label x sense committed
label case abstain prediction x notice less
specific application thresholded disbelief index
note similar technique erm oracle enforce arbitrary
number example constraints used dasgupta hsu monteleoni
beygelzimer hsu langford zhang context active learning
disbelief index difference empirical risk importance weighted
empirical risk see beygelzimer et al two erm oracles different constraints
used estimate prediction confidence



fiwiener el yaniv







test error

test error





























c











c

figure rc curve technique depicted red compared rejection
distance decision boundary depicted dashed green line rc curve
right figure zooms lower coverage regions left curve

practical applications selective prediction desirable allow control
trade risk coverage words desirable able
develop entire risk coverage rc curve classifier hand see e g el yaniv
wiener let user choose cutoff point along curve accordance
practical considerations constraints disbelief index facilitates exploration
risk coverage trade curve classifier follows given pool test points
rank test points according disbelief index points low index
rejected first thus ranking provides means constructing risk coverage
trade curve ignoring moment implementation details discussed
section typical rc curve generated less depicted figure red curve
dashed green rc curve computed traditional distance techniques
rejection see discussion common technique section right graph zoom
section entire rc curve depicted left graph dashed horizontal line
test error f entire domain dotted line bayes error
high coverage values two techniques statistically indistinguishable coverage
less get significant advantage less clear case
estimation error reduced test error goes significantly optimal
test error f low coverage values
interestingly disbelief index generates rejection regions fundamentally different obtained traditional distance techniques rejection see
section illustrate point still ignoring implementation details consider
figure depict rejection regions training sample points sampled
mixture two identical normal distributions centered different locations
height map figure correspond disbelief index magnitude distance
decision boundary b reflect confidence regions technique according
confidence measure


learning synthetic used generating figure



fiagnostic pointwise competitive selective classification



b

figure linear classifier confidence height map disbelief index b distance
decision boundary

figure svm polynomial kernel confidence height map disbelief index
b distance decision boundary

intuitively explain height map figure recall disbelief index
difference empirical error erm restricted erm test
point resides high density region expect forcing wrong label point
large increase training error denser area
larger disbelief index therefore higher classification confidence
second synthetic source distribution consider even striking x
distributed uniformly labels sampled according
following conditional distribution

x sin x
p x x x
else
thick red line depicts decision boundary bayes classifier hight maps
figure depict rejection regions obtained approximation less



fiwiener el yaniv

traditional distance decision boundary technique training sample
points sampled distribution averaged iterations hypothesis
class used training svm polynomial kernel degree qualitative
difference two techniques particular nice fit disbelief
principle technique compared svm quite surprising

figure rc curves svm linear kernel method solid red rejection
distance decision boundary dashed green horizontal axis c
represents coverage

heuristic procedure svm empirical performance
computation constrained erm oracle efficiently achieved case
realizable learning linear see e g el yaniv wiener case
linear regression wiener el yaniv however noisy setting computation
linear erm oracle reduced variant max fls c max fls
strict non strict inequalities amaldi kann unfortunately



fiagnostic pointwise competitive selective classification

max fls apx complete within factor c max fls max ind set hard
cannot approximated efficiently moreover extensions
classes including axis aligned hyper rectangles showing approximating erm
classes np hard ben david et al
present known hardness related lower
bounds hold half spaces nice distributions gaussian mixtures note
tauman kalai et al studied agnostically learning halfspaces
distributional assumptions particular showed data distribution
uniform dimensional unit sphere hyper cube related distributions

possible agnostically learn accurate halfspaces time poly however
known particular distributions elicit effective pointwise competitive
learning contrary uniform distribution unit sphere among
worst possible distributions pointwise competitive classification disagreement
active learning unless one utilizes homogeneous halfspaces see discussion e g el yaniv
wiener

figure svm linear kernel maximum coverage distance rejection
technique allows error rate method specific coverage



fiwiener el yaniv

discussed computational hurdles recall much applied
machine learning many applications quite well heuristic
approximations rather formal ones practical performance objective
clever heuristics tricks sometimes make difference point
therefore switch theory practice aiming implementing rejection method
inspired disbelief principle see well work real world
approximate erm follows support vector machines svms use
high c value experiments penalize training errors small
margin see definitions svm parameters e g chang lin way
solution optimization tend get closer erm order estimate
r fx restrict svm optimizer consider hypotheses classify
point x specific way accomplish use weighted svm unbalanced data
add point x another training point weight times larger weight
training points combined thus penalty misclassification x large
optimizer finds solution doesnt violate constraint
another face disbelief index noisy statistic highly
depends sample sm overcome noise use robust statistics first
k bootstrap sampling
generate odd number k different samples sm


used k sample calculate disbelief index test points
point take median measurements final index note
finite training sample disbelief index discrete variable often case
several test points share disbelief index cases use confidence
measure tie breaker experiments use distance decision boundary
break ties focusing svms linear kernel compared rc risk coverage
curves achieved proposed method achieved svm rejection
distance decision boundary latter common practical
applications selective classification implementation used libsvm chang
lin
tested standard medical diagnosis uci repository including datasets used grandvalet rakotomamonjy keshet canu
transformed nominal features numerical ones standard way binary indicator attributes normalized attribute independently dynamic
range preprocessing employed iteration choose uniformly
random non overlapping training set samples test set samples
dataset svm trained entire training set test samples sorted
according confidence distance decision boundary disbelief index
figure depicts rc curves technique red solid line rejection
distance decision boundary green dashed line linear kernel datasets
averaged iterations error bars standard error exception
hepatitis dataset methods statistically indistinguishable
datasets proposed method exhibits significant advantage traditional
would highlight performance proposed method
pima dataset traditional cannot achieve error less


due size hepatitis dataset test set limited samples



fiagnostic pointwise competitive selective classification

figure rc curves svm rbf kernel method solid red rejection
distance decision boundary dashed green

rejection rate test error decreases monotonically zero rejection
rate furthermore clear advantage method large range rejection rates
evident haberman dataset
sake fairness note running time presented
substantially longer traditional technique performance substantially improved many unlabeled samples available
case rejection function evaluated unlabeled samples generate
labeled sample rejection classifier trained sample
figure depicts maximum coverage distance rejection technique
allows error rate method specific coverage example let us
assume method error rate coverage


haberman dataset contains survival data patients undergone surgery breast cancer
estimated cases breast cancer united states society
improvement affects lives women



fiwiener el yaniv

figure svm rbf kernel maximum coverage distance rejection
technique allows error rate method specific coverage

distance rejection technique achieves error maximum coverage
point red line thus red line bellow diagonal
technique advantage distance rejection visa versa
example consider haberman dataset observe regardless rejection rate
distance technique cannot achieve error technique coverage
lower
figures depict obtained rbf kernel case statistically
significant advantage technique observed datasets

related work
pointwise competitive classification unique extreme instance classification
abstention option idea emerged pattern recognition community
first proposed studied years ago chow generated lots interest



fiagnostic pointwise competitive selective classification

fumera et al tortorella santos pereira pires fumera roli
pietraszek bounsiar et al landgrebe et al herbei wegkamp
hellman el yaniv pidan bartlett wegkamp wegkap freund
et al taking broader perspective pointwise competitive selective prediction
particular classification particular instance broader concept confidencerated learning whereby learner must formally quantify confidence prediction
achieving effective confidence rated prediction including abstention longstanding
challenging goal number disciplines communities let us first discuss
prominent approaches confidence rated prediction note
related present work
knows knows kwik framework studied reinforcement learning li
littman walsh strehl littman li littman similar notion
pointwise competitiveness studied coverage rates analyzed li et al
li however kwik limited realizable model concerned
adversarial setting target hypothesis training data selected
adversary positive kwik adversarial setting apply
statistical pointwise competitive prediction setting training examples sampled
adversarial setting precludes non trivial coverage interesting hypothesis
classes currently addressed pointwise competitive prediction deficiency comes
surprise kwik adversarial setting much challenging statistical
pointwise competitive prediction assumptions
conformal prediction framework vovk gammerman shafer provides
hedged predictions allowing possibility multi labeled predictions guarantees
user desired confidence rate asymptotic sense conformal prediction mainly concerned online probabilistic setting rather predicting single label
sample point conformal predictor assign multiple labels user defined confidence level error rate asymptotically guaranteed interpreting
multi labeled predictions rejection compare pointwise competitive prediction sense conformal prediction construct online predictors reject option
asymptotic performance guarantees important differences conformal predictions pointwise competitive prediction pointed
approaches provide hedged predictions use different notions hedging whereas
pointwise competitive prediction goal guarantee high probability
training sample predictor agrees best predictor class
points accepted domain goal conformal predictions provide guarantees
average error rate average taken possible samples test
points sense conformal prediction cannot achieve pointwise competitiveness
addition conformal prediction utilizes different notion error one used
pointwise competitive model pointwise competitive prediction focused performance guarantees error rate covered accepted examples conformal
prediction provides guarantee examples including multiple predictions none increasing multi labeled prediction rate uncertain prediction


noted vovk et al impossible achieve conditional probability error equal given
observed examples unconditional probability error equals therefore implicitly
involves averaging different data sequences vovk et al p



fiwiener el yaniv

error rate decreased arbitrarily small value case
pointwise competitive prediction error notion covered examples bounded
bayes error covered region finally conformal prediction mentions
notion efficiency similar coverage best knowledge
finite sample established another interesting scheme vicinity
confidence rated learning guaranteed error machine gem campi
gem model reject option considered correct answer means risk
reduced arbitrarily conformal prediction
pointwise competitive classification special case pointwise competitive prediction
el yaniv wiener wiener el yaniv el yaniv wiener
wiener wiener et al pointwise competitive selective classification first
addressed el yaniv wiener realizable case studied
pointwise competitiveness termed perfect classification present article
extends pointwise competitive classification noisy
number theoretical studies general selective classification
pointwise competitive freund et al studied simple ensemble method binary
classification given hypothesis class f method outputs weighted average
hypotheses f weight hypothesis exponentially depends
individual training error abstains prediction whenever weighted
average individual predictions close zero able bound probability
misclassification r f conditions proved bound
r f f rejection rate less strategy viewed extreme
variation freund et al method include ensemble hypotheses
sufficiently low empirical error abstain weighted average predictions
definitive risk coverage bounds asymptotically tighter
excess risk bounds developed herbei wegkamp model
rejection incurs cost bound applies empirical risk minimizer
hypothesis class ternary hypotheses whose output reject see
extensions wegkap bartlett wegkamp
rejection mechanism svms distance decision boundary perhaps
widely known used rejection technique routinely used medical applications mukherjee et al guyon et al mukherjee papers proposed
alternative techniques rejection case svms include taking reject
area account optimization fumera roli training two svm classifiers
asymmetric cost sousa mora cardoso hinge loss bartlett
wegkamp grandvalet et al proposed efficient implementation svm
reject option double hinge loss empirically compared
two selective classifiers one proposed bartlett wegkamp
traditional rejection distance decision boundary experiments
statistically significant advantage method compared traditional
high rejection rates
pointwise selective classification strongly tied disagreement active learning
realizable case el yaniv wiener presented reduction stream
active learning cal cohn et al pointwise competitive
classification reduction roughly states rejection rate reciprocal



fiagnostic pointwise competitive selective classification

coverage less polylog f p actively learnable
cal exponential speedup consequence reduction resulted first
exponential speedup bounds cal general linear finite mixture
gaussians direction showing exponential speedup cal implies
rejection rate less realizable setting recently established wiener
wiener hanneke el yaniv two different techniques
version space compression set size extensively utilized present
work introduced implicitly hanneke b special case extended
teaching dimension context version space compression set called
minimal specifying set introduced explicitly el yaniv wiener
context selective classification proved el yaniv wiener
special case extended teaching dimension hanneke b relations
disagreement coefficient version space compression set size first discussed
el yaniv wiener sharp ties two quantities stated
lemma others recently developed wiener hanneke el yaniv


concluding remarks
existence pointwise competitive classification quite fascinating striking
feature classifier definition pointwise competitive predictor free
estimation error cannot overfit means hypothesis class
expressive still protected overfitting however without
effective coverage bounds pointwise competitive classifier may refuse predict
times
current recent studies selective prediction el yaniv wiener
active learning wiener hanneke el yaniv place version space
compression set size center stage leading quantity drive
intuition domains present known technique able prove fast
coverage pointwise competitive classification exponential label complexity speedup
disagreement active learning general linear fixed mixture
gaussians axis aligned rectangles product distributions possible
extend beyond linear classifiers axis aligned rectangles interesting
distribution families example plausible existing axis aligned
rectangles extended decision trees
formal relationship active learning pointwise competitive classification
el yaniv wiener wiener wiener et al created powerful synergy
allows migrating two currently formal connection manifested via two links first within realizable setting equivalence
less classification fast coverage cal active learning exponential speedup second link consists bounds relate underlying complexity
measures disagreement coefficient active learning version space compression
set size pointwise competitive classification number non established relations significantly substantiate interaction two could
considered example possible prove direct equivalence less



fiwiener el yaniv

pointwise competitive agnostic classification fast coverage rates less active
learning exponential speedup expect resolution question
interesting implications example relationship could potentially facilitate
migration interesting techniques devised active learning
pointwise competitive framework immediate candidate beygelzimer et al builds ideas dasgupta et al b beygelzimer et al
resembling implementation proposed less via calls constrained erm
oracle works without tracking version space final choice
hypothesis well querying component instead querying relies
erm oracle enforces one example constrain thus importanceweighting technique resembles disbelief principle outline
regard interesting consider migrate ideas active
learning emerging online learning branch orabona cesa bianchi
cesa bianchi et al dekel et al required online batch
conversion techniques zhang kakade tewari cesa bianchi gentile
dekel
less strategy requires unanimous vote among hypotheses low empirical
error subset hypothesis class considering e g linear subset
hypotheses uncountable case even finite size huge clearly
less extremely radical defensive strategy immediate question arises
whether less unanimity requirement relaxed majority vote
achieve pointwise competitiveness strong majority vote instead unanimity
besides greater flexibility general voting scheme may lead different types
interesting learning relaxation potentially ease computational
complexity implementing less discussed bottleneck agnostic
classification example relaxed voting scheme might utilize hypothesis
sampling classical example related context celebrated query bycommittee qbc strategy seung et al freund et al fine et al giladbachrach gilad bachrach et al however strict pointwise competitiveness
advocated easy see strong majority vote sufficient indeed consider
f differs hypotheses f single point x unless probability
point large typical case high probability point part
training set sm therefore majority vote even strong label
opposite f hence worst case even strong majority sufficient pointwise
competitiveness natural compromise pointwise competitiveness objective one
revert standard excess risk bounds bartlett et al whereby compare
overall average performance predictor r f optimal predictor r f
pointwise regard work freund mansour schapire discussed
section excess risk bound r f g r f


hyper parameter coverage bound f g r f ln f
considering excess risk bounds f possible beat risk coverage
bounds relaxed voting scheme rejection would optimal bounds
fully agnostic setting better bounds devised specific distributions
gaussian mixtures note freund et al strategy interesting



fiagnostic pointwise competitive selective classification

final aggregated predictor general outside f principle significantly
outperform f f bound elicit behavior emphasizes
potential usefulness ensembles applied rejection scheme
final predictor recall less strategy final predictor belongs f
thus considering ensembles allowing excess risk bounds even
ambitious goals strictly beating f average

acknowledgments
thank anonymous referees good comments grateful steve hanneke helpful insightful discussions warmly thank intel collaborative
institute computational intelligence icri ci israel science foundation
isf generous support

appendix proofs
proof lemma relies following lemma wiener et al whose
proof provided sake self containment

lemma wiener et al realizable case r







r max max bn
r
r r


proof prove r




b f r


max bn


r
r



follows taking supremum sides r r
fix r let r define sm sm xi yi
define dm dis vsf sm b f r p xi dm sm p dm
b f r clearly holds otherwise suppose b f r xi
dis vsf sm must xi yi csm

n sm


x




dis vsf sm xi



fiwiener el yaniv

therefore
p n sm b f r


x

p
dis vsf
xi b f r


p
p





x

dm xi





x

b f r

dis b f r xi





p




x



dis b f r xi







p





x

p



dis b f r xi


x



x



x












dm xi

dm xi




x

p

dis b f r xi




x


b f r
dis b f r xi



dm xi


x







b f r




dis b f r xi b f r





b f r
dis b f r xi


dis b f r xi






x









dis b f r xi b f r


b f r









dis b f r xi







dm xi





b f r



since considering case b f r chernoff bound implies


x

exp b f r e
p
dis b f r xi b f r




furthermore markovs inequality implies
p


x



dis b f r xi







dm xi



b f r


mb f r e

hp






dm xi

mb f r

since xi values exchangeable





h h
ii x
x
x





e e dm xi fism
e
e
dm xi
















fiagnostic pointwise competitive selective classification

shown hanneke least
r b f r
particular b f r must r implies
r r


x

e
dm xi mb f r




altogether established
p n sm b f r

mb f r mb f r
e
mb f r


e




thus since n sm bn
probability least must


b f r


bn
b f r

r

proof lemma assuming bn polylog log holds exists
constant
bn non bn polylog



bn thus bn
polylog therefore
increasing bn








max bn
max polylog polylog


r
r
r
lemma





r max
max bn


r








polylog
max bn

r
r

references
amaldi e kann v complexity approximability finding maximum
feasible subsystems linear relations theoretical computer science
bartlett p l jordan mcauliffe j convexity classification risk
bounds journal american statistical association
bartlett p mendelson discussion ims medallion lecture local
rademacher complexities oracle inequalities risk minimization v koltchinskii annals statistics



fiwiener el yaniv

bartlett p mendelson philips p local complexities empirical risk
minimization colt proceedings workshop computational learning
theory morgan kaufmann publishers
bartlett p wegkamp classification reject option hinge loss
journal machine learning
ben david eiron n long p difficulty approximately maximizing
agreements journal computer system sciences
beygelzimer dasgupta langford j importance weighted active learning
proceedings th annual international conference machine learning pp
acm
beygelzimer hsu langford j zhang agnostic active learning without
constraints advances neural information processing systems
beygelzimer dasgupta langford j importance weighted active learning
proceedings th annual international conference machine learning pp
acm
beygelzimer hsu langford j zhang agnostic active learning without
constraints arxiv preprint arxiv
bounsiar grall e beauseroy p kernel rejection method supervised classification international journal computational intelligence
bousquet boucheron lugosi g introduction statistical learning
theory advanced lectures machine learning vol lecture notes
computer science pp springer
campi classification guaranteed probability error mach learn

cesa bianchi n gentile c improved risk tail bounds line
information theory ieee transactions
cesa bianchi n gentile c orabona f robust bounds classification via
selective sampling proceedings th annual international conference
machine learning pp acm
chang c lin c libsvm library support vector machines acm
transactions intelligent systems technology software available
http www csie ntu edu tw cjlin libsvm
chow c optimum character recognition system decision function ieee
trans computer
chow c optimum recognition error reject trade ieee trans
information theory
cohn atlas l ladner r improving generalization active learning
machine learning
dasgupta hsu monteleoni c general agnostic active learning nips



fiagnostic pointwise competitive selective classification

dasgupta monteleoni c hsu j b general agnostic active learning
advances neural information processing systems pp
dekel online batch learning cutoff averaging nips
dekel gentile c sridharan k robust selective sampling single
multiple teachers colt pp
el yaniv r pidan selective prediction financial trends hidden
markov nips pp
el yaniv r wiener foundations noise free selective classification
journal machine learning
el yaniv r wiener agnostic selective classification neural information
processing systems nips
el yaniv r wiener active learning via perfect selective classification
journal machine learning
el yaniv r wiener version space compression set size
applications vovk v papadopoulos h gammerman eds measures
complexity festschrift alexey chervonenkis springer berlin
fine gilad bachrach r shamir e query committee linear separation
random walks theoretical computer science
freund mansour schapire r generalization bounds averaged classifiers annals statistics
freund seung h shamir e tishby n selective sampling query
committee machine learning
friedman e active learning smooth proceedings nd
annual conference learning theory
fumera g roli f support vector machines embedded reject option
pattern recognition support vector machines first international workshop pp

fumera g roli f giacinto g multiple reject thresholds improving
classification reliability lecture notes computer science
gilad bachrach r pac beyond ph thesis hebrew university
jerusalem
gilad bachrach r navot tishby n query committee made real
nips
grandvalet rakotomamonjy keshet j canu support vector machines reject option nips pp mit press
guyon weston j barnhill vapnik v gene selection cancer classification support vector machines machine learning
hanneke bound label complexity agnostic active learning icml
pp



fiwiener el yaniv

hanneke b teaching dimension complexity active learning proceedings th annual conference learning theory colt vol lecture
notes artificial intelligence pp
hanneke theoretical foundations active learning ph thesis carnegie
mellon university
hanneke statistical theory active learning unpublished
hanneke activized learning transforming passive active improved label
complexity journal machine learning
hellman nearest neighbor classification rule reject option ieee
trans systems sc cyb
herbei r wegkamp classification reject option canadian journal
statistics
kakade tewari generalization ability online strongly convex
programming advances neural information processing systems
nips pp
koltchinskii v ims medallion lecture local rademacher complexities
oracle inequalities risk minimization annals statistics
landgrebe tax paclk p duin r interaction classification
reject performance distance reject option classifiers pattern recognition
letters
li l littman l reducing reinforcement learning kwik online regression
annals mathematics artificial intelligence
li l littman walsh knows knows framework self aware
learning proceedings th international conference machine learning pp
acm
li l unifying framework computational reinforcement learning theory ph
thesis rutgers state university jersey
massart p applications concentration inequalities statistics annales
de la faculte des sciences de toulouse vol pp universite paul sabatier
mendelson improving sample complexity global data information
theory ieee transactions
mukherjee chapter classifying microarray data support vector machines
scientists university pennsylvania school medicine school
engineering applied science kluwer academic publishers
mukherjee tamayo p slonim verri golub mesirov j p poggio
support vector machine classification microarray data tech rep ai memo
massachusetts institute technology
orabona f cesa bianchi n better selective sampling
proceedings th international conference machine learning icml
pp



fiagnostic pointwise competitive selective classification

pietraszek optimizing abstaining classifiers roc analysis proceedings twenty second international conference machine learning icml pp

santos pereira c pires optimal reject rules roc curves pattern
recognition letters
seung h opper sompolinsky h query committee proceedings
fifth annual workshop computational learning theory colt pp
society c cancer facts figures
sousa r mora b cardoso j ordinal data method classification
reject option icmla pp ieee computer society
strehl l littman l online linear regression application
model reinforcement learning advances neural information processing
systems pp
tauman kalai klivans mansour servedio r agnostically learning
halfspaces siam j comput
tortorella f optimal reject rule binary classifiers lecture notes computer
science
tsybakov optimal aggregation classifiers statistical learning annals
mathematical statistics
vovk v gammerman shafer g algorithmic learning random world
springer york
wang l smoothness disagreement coefficient label complexity agnostic
active learning jmlr
wegkap lasso type classifiers reject option electronic journal
statistics
wiener theoretical foundations selective prediction ph thesis technion
israel institute technology
wiener el yaniv r pointwise tracking optimal regression function
advances neural information processing systems pp
wiener hanneke el yaniv r compression technique analyzing
disagreement active learning arxiv preprint arxiv
zhang data dependent concentration bounds sequential prediction learning theory pp springer




