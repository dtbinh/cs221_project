Journal Artificial Intelligence Research 52 (2015) 179-201

Submitted 5/14; published 1/15

Agnostic Pointwise-Competitive Selective Classification
Yair Wiener
Ran El-Yaniv

wyair@tx.technion.ac.il
rani@cs.technion.ac.il

Computer Science Department
Technion Israel Institute Technology
Haifa 32000, Israel

Abstract
pointwise competitive classifier class F required classify identically
best classifier hindsight F. noisy, agnostic settings present strategy
learning pointwise-competitive classifiers finite training sample provided
classifier abstain prediction certain region choice. interesting hypothesis classes families distributions,
measure

rejected region
/2
shown diminishing rate 1 (polylog(m) log(1/)/m) 2
, high probability, sample size, standard confidence parameter, 1 , 2
smoothness parameters Bernstein type condition associated excess loss class
(related F 0/1 loss). Exact implementation proposed learning strategy
dependent ERM oracle hard compute agnostic case. thus
consider heuristic approximation procedure based SVMs, show empirically
algorithm consistently outperforms traditional rejection mechanism based
distance decision boundary.

1. Introduction
Given labeled training set class models F, possible select F, based
finite training sample, model whose predictions always identical best model
hindsight? classical results statistical learning theory surely preclude
possibility within standard model, changing rules game possible.
Indeed, consider game classifier allowed abstain prediction without
penalty region choice (a.k.a classification reject option). game,
assuming noise free realizable setting, shown El-Yaniv Wiener (2010)
one train perfect classifier never errs whenever willing predict.
always abstaining render perfect classification vacuous, shown
quite broad set problems (each specified underlying distribution family
hypothesis class), perfect realizable classification achievable rejection rate
diminishes quickly zero training sample size.
general, perfect classification cannot achieved noisy setting. paper,
objective achieve pointwise competitiveness, property ensuring prediction
every non-rejected test point identical prediction best predictor hindsight
class. consider pointwise-competitive selective classification
generalize results El-Yaniv Wiener (2010) agnostic case. particular,
show pointwise-competitive classification achievable high probability
learning strategy called low error selective strategy (LESS). Given training sample Sm

c
2015
AI Access Foundation. rights reserved.

fiWiener & El-Yaniv

hypothesis class F, LESS outputs pointwise-competitive selective classifier (f, g),
f (x) standard classifier, g(x) selection function qualifies
predictions dont knows (see definitions Section 2). classifier f simply
taken empirical risk minimizer (ERM) classifier, f. Pointwise competitiveness
achieved g follows. Using standard concentration inequalities, show
true risk minimizer, f , achieves empirical error close f. Thus, high
probability f belongs class low empirical error hypotheses. left
set g(x) allows prediction label x, f(x),
hypotheses low error class unanimously agree label x.
simpler, realizable setting (El-Yaniv & Wiener, 2010), low error class simply reduces
version space.
bulk analysis (in Sections 3, 4 5) concerns coverage bounds LESS,
namely, showing measure region classifier (f, g) refuses classify,
diminishes quickly, high probability, training sample size grows (see Section 2
formal definition). provide several general distribution-dependent coverage
bounds. particular, show (in Corollaries 12 14, respectively) high probability
bounds coverage (f, g) classifier (f, g, ) form,


(f, g) 1 1 (polylog(m) log(1/)/m)2 /2 ,

linear models (unknown) distribution P (X, ), X feature space points
labels, whose marginal P (X) finite mixture Gaussians, axis
aligned rectangles P (X, ) whose marginal P (X) product distribution,
1 , 2 Bernstein class smoothness parameters depending hypothesis class
underlying distribution (and loss function, 0/1 case).
outset, efficient implementation LESS seems reach
required track supremum empirical error possibly infinite hypothesis
subset, general might intractable. overcome computational difficulty,
propose reduction problem problem calculating (two) constrained ERMs.
given test point x, calculate ERM training sample Sm
constraint label x (one positive label constraint one negative). show
thresholding difference empirical error two constrained ERMs
equivalent tracking supremum entire (infinite) hypothesis subset. Based
reduction introduce Section 6 disbelief principle motivates heuristic implementation LESS, relies constrained SVMs, mimics optimal
behavior.
Section 7 present numerical examples medical classification problems
examine empirical performance new algorithm compare performance
widely used selective classification method rejection, based distance
decision boundary.

2. Pointwise-Competitive Selective Classification: Preliminary
Definitions
Let X feature space, example, d-dimensional vectors Rd ,
output space. standard classification, goal learn classifier f : X Y, using
172

fiAgnostic Pointwise-Competitive Selective Classification

finite training sample labeled examples, Sm = {(xi , yi )}m
i=1 , assumed sampled
i.i.d. unknown underlying distribution P (X, ) X Y. classifier
selected hypothesis class F. Let : [0, 1] bounded loss function.
selective classification (El-Yaniv & Wiener, 2010), learning algorithm receives Sm
required output selective classifier, defined pair (f, g), f F
classifier, g : X {0, 1} selection function, serving qualifier f follows.
x X , (f, g)(x) = f (x) iff g(x) = 1. Otherwise, classifier outputs dont know.
general performance selective predictor characterized terms two quantities: coverage risk. coverage (f, g) (f, g) , EP [g(x)] . true risk (f, g),
respect loss function , average loss f restricted region activity
qualified g, normalized coverage, R(f, g) , EP [(f (x), y) g(x)] /(f, g).
easy verify g 1 (and therefore (f, g) = 1), R(f, g) reduces
famil1 Pm
iar risk functional R(f ) , EP [(f (x), y)]. classifier f , let R(f ) , i=1 (f (xi ), yi ),
standard empirical error f sample Sm . Let f = argminf F R(f )
empirical risk minimizer (ERM), let f = argminf F R(f ) true risk minimizer
respect unknown distribution P (X, ).1 Clearly, true risk minimizer f unknown. selective classifier (f, g) called pointwise-competitive x X ,
g(x) > 0, f (x) = f (x).

3. Low Error Selective Strategy (LESS)
hypothesis class F, hypothesis f F, distribution P , sample Sm , real number
r > 0, define true empirical low-error sets,


V(f, r) , f F : R(f ) R(f ) + r
(1)


n

V(f, r) , f F : R(f ) R(f ) + r .

(2)

Throughout paper denote (m, , d) slack standard uniform deviation
bound, given terms training sample size, m, confidence parameter, ,
VC-dimension, d, class F,


+ ln 2
2d ln 2me

(m, , d) , 2
.
(3)

following theorem slight extension statement made Bousquet, Boucheron,
Lugosi (2004, p. 184).
Theorem 1 (Bousquet et al., 2004). Let 0/1 loss function F, hypothesis
class whose VC-dimension d. 0 < < 1, probability least 1
choice Sm P , hypothesis f F satisfies
R(f ) R(f ) + (m, , d).
Similarly, R(f ) R(f ) + (m, , d) conditions.
1

formally, f classifier R(f ) = inf f F R(f ) inf f F P ((x, y) : f (x) 6= f (x)) = 0.
existence (measurable) f guaranteed sufficient considerations (see Hanneke, 2012,
pp. 1511-2).

173

fiWiener & El-Yaniv

Remark 2. use Theorem 1 and, particular, VC bounds classification problems
(0/1 loss) mandatory developing theory presented paper. Similar
theories developed using types bounds (e.g., Rademacher compression
bounds) learning problems.
Let G F. disagreement set (Hanneke, 2007a; El-Yaniv & Wiener, 2010) w.r.t. G
defined
DIS(G) , {x X : f1 , f2 G s.t. f1 (x) 6= f2 (x)} .
(4)
Let us motivate low-error selective strategy (LESS) whose pseudo-code appears
Strategy 1. strategy define whenever empirical risk minimizer (ERM) exists,
example, case 0/1 loss. Using standard uniform deviation bound,
one Theorem 1, one show training error true risk minimizer, f ,
cannot far training error empirical risk minimizer, f. Therefore,

guarantee, high probability, empirical low error class V f, r (applied
appropriately chosen r) includes true risk minimizer f . selection function
g constructed accept subset domain X , hypotheses
empirical low-error set unanimously agree. Strategy 1 formulates idea. call
strategy rather algorithm lacks implementation details. Indeed,
clear outset strategy implemented.
Strategy 1 Agnostic low-error selective strategy (LESS)
Input: Sm , m, ,
Output: pointwise-competitive selective classifier (h, g) w.p. 1

1: Set f = ERM
(F, Sm ), i.e., f empirical risk minimizer F w.r.t. Sm
2: Set G = V f, 2(m, /4, d) (see Eq. (2) (3))
3: Construct g g(x) = 1 x {X \ DIS (G)}
4: f = f
begin analysis LESS. following lemma establishes pointwise competitiveness. Section 4 develop general coverage bounds terms undetermined
disagreement coefficient. Then, Section 5 present distribution-dependent bounds
rely disagreement coefficient.
Lemma 3 (pointwise competitiveness). Let 0/1 loss function F, hypothesis
class whose VC-dimension d. Let > 0 given let (f, g) selective classifier
chosen LESS. Then, probability least 1 /2, (f, g) pointwise competitive
selective classifier.
Proof. Theorem 1, probability least 1 /4,
R(f ) R(f ) + (m, /4, d) .
Clearly, since f minimizes true error, R(f ) R(f). Applying Theorem 1,
know probability least 1 /4,
R(f) R(f) + (m, /4, d) .
174

fiAgnostic Pointwise-Competitive Selective Classification

Using union bound, follows probability least 1 /2,
R(f ) R(f) + 2 (m, /4, d) .
Hence, probability least 1 /2,


f V f, 2 (m, /4, d) , G.

definition, LESS constructs selection function g(x) equals one iff x
X \DIS (G) . Thus, x X , g(x) = 1, hypotheses G agree,
particular f f agree. Therefore (f, g) pointwise-competitive high probability.

4. General Coverage Bounds LESS Terms Disagreement
Coefficient
require following definitions facilitate coverage analysis. f F
r > 0, define set B(f, r) hypotheses reside ball radius r around f ,





B(f, r) , f F : Pr f (X) 6= f (X) r .
XP

G F, distribution P , denote G volume disagreement set
G (see (4)), G , Pr {DIS(G)}. Let r0 0. disagreement coefficient (Hanneke,
2009) hypothesis class F respect target distribution P
(r0 ) , f (r0 ) = sup

r>r0

B(f , r)
.
r

(5)

disagreement coefficient utilized later analysis. See discussion
characteristics Corollary 7. associated excess loss class class F
loss function (Massart, 2000; Mendelson, 2002; Bartlett, Mendelson, & Philips, 2004)
defined
XL(F, )(x, y) , {(f (x), y) (f (x), y) : f F} .
Whenever F fixed abbreviate XL = XL(F, )(x, y). XL said
(1 , 2 )-Bernstein class respect P (where 0 < 2 1 1 1), every h XL
satisfies
Eh2 1 (Eh)2 .
(6)
Bernstein classes arise many natural situations (see, e.g., Koltchinskii, 2006; Bartlett &
Mendelson, 2006; Bartlett & Wegkamp, 2008). example, conditional probability
P (Y |X) bounded away 1/2, satisfies Tsybakovs noise conditions2 ,
excess loss function Bernstein class (Bartlett & Mendelson, 2006; Tsybakov, 2004).3
2

data generated unknown deterministic hypothesis limited noise P (Y |X)
bounded away 1/2.

3

Specifically, 0/1 loss, Assumption Proposition 1 work Tsybakov (2004), equivalent

Bernstein class condition Equation (6) 2 = 1+
, Tsybakov noise
parameter.

175

fiWiener & El-Yaniv

following sequence lemmas theorems assume binary hypothesis class F
VC-dimension d, underlying distribution P X {1}, 0/1 loss
function. Also, XL denotes associated excess loss class. results extended
loss functions 0/1 similar techniques used Beygelzimer, Dasgupta,
Langford (2009).
Figure 1 schematically depict hypothesis class F (the gray area), target
hypothesis (filled black circle outside F), best hypothesis class f .
distance two points diagram relates distance two hypothesis
marginal distribution P (X). first observation excess loss class
(1 , 2 )-Bernstein class, set low true error (depicted Figure 1 (a)) resides
within larger ball centered around f (see Figure 1 (b)).

Figure 1: set low true error (a) resides within ball around f (b).
Lemma 4. XL (1 , 2 )-Bernstein class respect P , r > 0


V(f , r) B f , 1 r2 .

Proof. f V(f , r) then, definition, E {I(f (X) 6= )} E {I(f (X) 6= )} + r.
linearity expectation have,
E {I(f (X) 6= ) I(f (X) 6= )} r.
Since XL (1 , 2 )-Bernstein,
E {I(f (X) 6= f (X))} = E {|I(f (X) 6= ) I(f (X) 6= )|}
n

= E ((f (X), ) (f (X), ))2 , Eh2 1 (Eh)2
, 1 (E {I(f (X) 6= ) I(f (X) 6= )})2 .


(7), E {I(f (X) 6= f (X))} 1 r2 . Therefore, definition, f B f , 1 r2 .
176

(7)

fiAgnostic Pointwise-Competitive Selective Classification

far seen set low true error resides within ball around f .
would prove high probability set low empirical error (depicted
Figure 2 (a)) resides within set low true error (see Figure 2 (b)). emphasize
distance hypotheses Figure 2 (a) based empirical error,
distance Figure 2 (b) based true error.

Figure 2: set low empirical error (a) resides within set low true error (b).
Lemma 5. r > 0, 0 < < 1, probability least 1 ,
V(f, r) V (f , 2 (m, /2, d) + r) .
Proof. f V(f, r), then, definition, R(f ) R(f) + r. Since f minimizes empirical
error, know R(f) R(f ). Using Theorem 1 twice, applying union bound,
see probability least 1 ,
R(f ) R(f ) + (m, /2, d)



R(f ) R(f ) + (m, /2, d).

Therefore,
R(f ) R(f ) + 2 (m, /2, d) + r,

f V (f , 2 (m, /2, d) + r) .

shown that, high probability, set low empirical error subset
certain ball around f . Therefore, probability least two hypotheses set
low empirical error disagree bounded probability
least two hypotheses ball around f disagree other. Fortunately,
latter bounded disagreement coefficient established following lemma.

177

fiWiener & El-Yaniv

Lemma 6. r > 0 0 < < 1, probability least 1 ,
V(f, r) 1 (2 (m, /2, d) + r)2 (r0 ),
(r0 ) disagreement coefficient F respect P , applied r0 =
(2(m, /2, d))2 (see (5)).
Proof. Applying Lemmas 5 4 get probability least 1 ,


V(f, r) B f , 1 (2 (m, /2, d) + r)2 .

Therefore,



V(f, r) B f , 1 (2 (m, /2, d) + r)2 .

definition disagreement coefficient (5), r > r0 , B(f , r ) (r0 )r .
Recalling 1 1 thus observing r = 1 (2 (m, /2, d) + r)2 > (2 (m, /2, d))2 =
r0 , proof complete.
position state first coverage bound selective classifier
constructed LESS. bound given terms disagreement coefficient.
Corollary 7. Let F hypothesis class Theorem 1, assume XL
(1 , 2 )-Bernstein class w.r.t. P . Let (f, g) selective classifier constructed LESS.
Then, probability least 1 , (f, g) pointwise competitive selective classifier

(f, g) 1 1 (4 (m, /4, d))2 (r0 ),

(r0 ) disagreement coefficient F respect P , r0 = (2(m, /4, d))2 .

Proof.
Lemma 3,
probability least 1/2, (f, g) pointwise-competitive. Set

G , V f , 2 (m, /4, d) . construction, f = f, selection function g(x) equals
one iff x X \ DIS (G). Thus, definition coverage, (f, g) = E{g(X)} = 1 G.
Therefore, applications Lemma 6 union bound imply probability
least 1 , (f, g) pointwise-competitive coverage satisfies,
(f, g) = E{g(X)} = 1 G 1 1 (4 (m, /4, d))2 (r0 ),

Noting (r) monotone non-increasing r, know coverage bound
Corollary 7 clearly applies (0). quantity (0) discussed numerous papers shown finite various settings including thresholds R
distribution ((0) = 2) (Hanneke, 2009), linear
separators origin
Rd uniform distribution sphere ((0) d) (Hanneke, 2009), linear
separators Rd smooth data distribution bounded away zero ((0) c(f )d,
c(f ) unknown constant depends target hypothesis) (Friedman,
2009). cases, application Corollary 7 sufficient guarantee pointwisecompetitiveness bounded coverage converges one. Unfortunately many
hypothesis classes distributions disagreement coefficient (0) infinite (Hanneke,
2009). Fortunately, disagreement coefficient (r) grows slowly respect 1/r (as
shown Wang, 2011, sufficient smoothness conditions), Corollary 7 sufficient
guarantee bounded coverage.

178

fiAgnostic Pointwise-Competitive Selective Classification

5. Distribution-Dependent Coverage Bounds LESS
section establish distribution-dependent coverage bounds LESS. starting
point bounds following corollary.
Corollary 8. Let F hypothesis class Theorem 1, assume F disagreement coefficient
(r0 ) = (polylog (1/r0 ))
(8)
w.r.t. distribution P , XL (1 , 2 )-Bernstein class w.r.t. distribution.
Let (f, g) selective classifier chosen LESS. Then, probability least 1 ,
(f, g) pointwise competitive coverage satisfies,
!


polylog(m)
1 2 /2
(f, g) 1 1
log
.


Proof. Plugging (8) coverage bound Corollary 7 immediately yields result.

Corollary 8 states fast coverage bounds LESS cases disagreement coefficient grows slowly respect 1/r0 .4 Recent results disagreement-based active
learning selective prediction (Wiener et al., 2014; Wiener, 2013) established tight relations disagreement coefficient empirical quantity called version space
compression set size. quantity analyzed El-Yaniv Wiener (2010)
context realizable selective classification, known distribution-dependent
bounds it. plan rest section introduce version space compression set size, discuss relation disagreement coefficient, show
apply results agnostic setting.
interested solving agnostic case, consider moment
realizable setting utilize known results used analysis. Specifically,
assume f F P(Y = f (x)|X = x) = 1 x X ,
(X, ) P . Given training sample Sm , let VSF ,S induced version space, i.e.,
set hypotheses consistent given sample Sm . version space compression
set size, denoted n(Sm ) = n(F, Sm ), defined size smallest subset
Sm inducing version space (Hanneke, 2007b; El-Yaniv & Wiener, 2010).
function Sm , clearly n(Sm ) random variable, specific realization Sm
value unique.
(0, 1], define version space compression set size minimal bound
Bn (m, ) , min {b N : P(n(Sm ) b) 1 } .

(9)

rely following lemma (Wiener et al., 2014). sake self-containment
provide proof appendix.
4

disagreement coefficient grow ploy-logarithmically 1/r0 still o(1/r0 ),
still possible prove lower bound coverage. Specifically, (r0 ) = ((1/r0 ) ) < 1, one

show (f, g) 1 O(1/( m)2 (1) ).

179

fiWiener & El-Yaniv

Lemma 9 (Wiener et al., 2014). realizablecase, Bn (m,
) = (polylog(m) log (1/)),

1
1
Bn m, 20 = (polylog(m)), (r0 ) = polylog r0 .

Obviously, statement Lemma 9 holds (and well defined) within realizable
setting (the version space compression set size defined setting). turn
back agnostic setting consider arbitrary underlying distribution P X Y.
Recall agnostic setting, let f : X denote (measurable) classifier
R(f ) = inf f F R(f ) inf f F P ((x, y) : f (x) 6= f (x)) = 0, guaranteed
exist sufficient assumptions (see Hanneke, 2012, Section 6.1); call f infimal
(best) hypothesis (of F, w.r.t. P ). Clearly several different infimal hypotheses.
note, however, XL (1 , 2 )-Bernstein class respect P (as assume
paper), Lemma 4 ensures infimal hypotheses identical measure
zero.
definitions version space version space compression set size naturally
generalized agnostic setting respect infimal hypothesis (Wiener et al.,
2014) follows. Let f infimal hypothesis F w.r.t. P . agnostic version space
Sm
VSF ,Sm ,f , {f F : (x, y) Sm , f (x) = f (x)}.

agnostic version space compression set size, denoted n(Sm ) = n(F, Sm , f ), defined
size smallest subset Sm inducing agnostic version space VSF ,Sm ,f .
Finally, extend definition version space compression set minimal bound
agnostic setting follows.
Bn (m, , f ) , min{b N : P(n(F, Sm , f ) b) 1 }.

key observation allows surprisingly easy utilization Lemma 9
agnostic setting disagreement coefficient depends hypothesis class
F marginal distribution P (X). Using infimal hypothesis f therefore
take agnostic learning problem consider realizable projection, whereby points
labeled f marginal distribution P (X). two problems
(essentially) disagreement coefficients. idea initially observed
Hanneke (2013) Wiener (2013). formulate slight variation
formulation work Wiener, Hanneke, El-Yaniv (2014).
define disagreement agnostic setting (5) respect infimal hypothesis f . agnostic learning problem (F, P ) define realizable
projection (F , P ) follows. Let F , F {f } f infimal hypothesis
agnostic problem. Define P distribution marginal P (X) = P (X),
P(Y = f (x)|X = x) = 1 x X . easy verify (F , P ) realizable
learning problem, i.e., f F PP (X,Y ) (Y = f (x)|X = x) = 1 x X .
Lemma 10 (Realizable projection). Given agnostic learning problem, (F, P ), let
(F , P ) realizable projection. Let (r0 ) (r0 ) associated disagreement coefficients agnostic realizable projection problems, respectively. Then, (r0 ) (r0 ).

Proof. First note depend, respectively, P P via f
marginal distributions P (X) = P (X). Since F F {f } = F , readily get
(r0 ) (r0 ).
180

fiAgnostic Pointwise-Competitive Selective Classification

Let us summarize derivation. Given agnostic problem (F, P ), consider
realizable projection (F , P ). Bn (m, ) = (polylog(m) log (1/)) (or Bn (m, 1/20) =
(polylog(m))) realizable problem, Lemma 9, (r0 ) = (polylog (1/r0 )),
which, Lemma 10, holds original agnostic problem. Therefore, Corollary 7
applies obtain fast coverage bound LESS w.r.t. (F, P ).
New agnostic coverage bounds LESS obtained using following known bounds
(realizable) version space compression set size. first one, El-Yaniv Wiener
(2010), applies problem learning linear separators mixture Gaussian
distributions. following theorem direct application Lemma 32 work
El-Yaniv Wiener (2010).
Theorem 11 (El-Yaniv & Wiener, 2010). d, n N, let X Rd , F space
linear separators Rd , P distribution marginal Rd mixture
n multivariate normal distributions. Then, constant cd,n > 0 (depending
d, n, otherwise independent P ) 2,
Bn (m, 1/20) cd,n (log(m))d1 .
Applying Theorem 11, together Lemma 10, Lemma 9 Corollary 8, immediately
yields following result.
Corollary 12. Assume conditions Theorem 11. Assume XL (1 , 2 )Bernstein class w.r.t. P (X, ). Let (f, g) selective classifier constructed LESS.
Then, probability least 1 , (f, g) pointwise competitive selective classifier



(f, g) 1 1 (polylog(m) log(1/)/m)2 /2 .
second version space compression set size bound concerns realizable learning
axis-aligned rectangles product densities Rn . bounds previously
proposed Wiener, Hanneke, El-Yaniv (2014) El-Yaniv Wiener (2010, 2012).
state (without proof) recent bound (Wiener, Hanneke, & El-Yaniv, 2014) giving
version space compression set size bound learning problem (whose positive class
bounded away zero).

Theorem 13 (Wiener et al., 2014). d, N , (0, 1), let X Rd . P
marginal distribution Rd product densities Rd marginals
continuous CDFs, F space axis-aligned rectangles f Rd
P ((x, y) : f (x) = 1) ,

8d
8d
Bn (m, )
ln
.


again, application Theorem 13, together Lemma 10, Lemma 9 Corollary 8 yields following corollary.
Corollary 14. d, N , (0, 1), let X Rd . Let P (X, ) underlying
distribution marginal P (X) product densities Rd marginals
continuous CDFs. Let F space axis-aligned rectangles f Rd P ((x, y) : f (x) =
1) , Assume XL (1 , 2 )-Bernstein class w.r.t. P (X, ). Let (f, g)
181

fiWiener & El-Yaniv

selective classifier constructed LESS. Then, probability least 1 , (f, g)
pointwise competitive selective classifier


(f, g) 1 1 (polylog(m) log(1/)/m)2 /2 .

6. ERM Oracles Disbelief principle
outset, efficient construction selection function g prescribed LESS seems
reach required verify, point x question, whether
hypotheses low error class agree label. Moreover, g computed
entire domain. Luckily, possible compute g lazy manner show
compute g(x) calculating (two) constrained ERMs. given test point x,
calculate ERM training sample Sm constraint label x (one
positive label constraint one negative). show thresholding difference
empirical error two constrained ERMs equivalent tracking supremum
entire (infinite) hypothesis subset. following lemma establishes reduction.
Lemma 15. Let (f, g) selective classifier chosen LESS observing training
sample Sm . Let f empirical risk minimizer Sm . Let x point X
define


n
fx , argmin R(f ) | f (x) = sign f(x) ,
f F

i.e., empirical risk minimizer forced label x opposite f(x).
g(x) = 0



R(fx ) R(f) 2 (m, /4, d) .

Proof. First note according definition V (see Eq (2)),


R(fx ) R(f) 2 (m, /4, d) fx V f, 2 (m, /4, d) .

(10)

(11)

prove first direction (=) (10), assume RHS (10) holds. (11),
get f, fx V. However, construction, f(x) = fx (x), x DIS(V)
g(x) = 0.
prove direction (=), assume R(fx ) R(f) > 2 (m, /4, d).
assumption, prove f V, f (x) = f(x), therefore, x
X \ DIS(V), entailing g(x) = 1. Indeed, assume contradiction exists
f V f (x) = fx (x) 6= f(x). construction, holds
R(f ) R(fx ) > R(f) + 2 (m, /4, d) ,
f 6 V. Contradiction.
Lemma 15 tells us order decide point x rejected need measure
empirical error R(fx ) special empirical risk minimizer, fx , constrained
label x opposite h(x). error sufficiently close R(h), classifier cannot
sure label x must reject it. Thus, provided compute
ERMs, decide whether predict reject individual test point x X ,
182

fiAgnostic Pointwise-Competitive Selective Classification

without actually constructing g entire domain X . Figure 3 illustrates principle
2-dimensional example. hypothesis class class linear classifiers R2
source distribution two normal distributions. Negative samples represented
blue circles positive samples red squares. usual, f denotes empirical

Figure 3: Constrained ERM.
risk minimizer. Let us assume want classify point x1 . point classified
positive f. Therefore, force point negative calculate restricted
ERM (depicted doted line marked fx1 ). difference empirical risk f
fx1 large enough, point x1 rejected. However, want classify
point x2 , difference empirical risk f fx2 quite large point
classified positive.
Equation (11) motivates following definition disbelief index DF (x, Sm )
individual point X . Specifically, x X , define disbelief index w.r.t. Sm
F,
D(x) , DF (x, Sm ) , R(fx ) R(f).
Observe D(x) large whenever model sensitive label x sense
forced bend best model fit opposite label x, model
substantially deteriorates, giving rise large disbelief index. large D(x)
interpreted disbelief possibility x labeled differently.
case definitely predict label x using unforced model. Conversely,
D(x) small, model indifferent label x sense, committed
label. case abstain prediction x. Notice LESS
specific application thresholded disbelief index.
note similar technique using ERM oracle enforce arbitrary
number example-based constraints used Dasgupta, Hsu, Monteleoni (2007a)
Beygelzimer, Hsu, Langford, Zhang (2010), context active learning.
disbelief index, difference empirical risk (or importance weighted
empirical risk, see Beygelzimer et al., 2010) two ERM oracles (with different constraints)
used estimate prediction confidence.

183

fiWiener & El-Yaniv

0.1

0.16
0.14

0.09
test error

test error

0.12
0.1
0.08

0.08
0.07

0.06
0.04

0.06

0.02
0

0.2

0.4

0.6

0.8

0.05
0.1

1

c

0.2

0.3

0.4

0.5

0.6

c

Figure 4: RC curve technique (depicted red) compared rejection based
distance decision boundary (depicted dashed green line). RC curve
right figure zooms lower coverage regions left curve.

practical applications selective prediction desirable allow control
trade-off risk coverage; words, desirable able
develop entire risk-coverage (RC) curve classifier hand (see, e.g., El-Yaniv &
Wiener, 2010) let user choose cutoff point along curve accordance
practical considerations constraints. disbelief index facilitates exploration
risk-coverage trade-off curve classifier follows. Given pool test points
rank test points according disbelief index, points low index
rejected first. Thus, ranking provides means constructing risk-coverage
trade-off curve. Ignoring moment implementation details (which discussed
Section 7), typical RC curve generated LESS depicted Figure 4 (red curve)5 .
dashed green RC curve computed using traditional distance-based techniques
rejection (see discussion common technique Section 8) right graph zoom
section entire RC curve (depicted left graph). dashed horizontal line
test error f entire domain dotted line Bayes error.
high coverage values two techniques statistically indistinguishable, coverage
less 60% get significant advantage LESS. clear case
estimation error reduced, test error goes significantly optimal
test error f low coverage values.
Interestingly, disbelief index generates rejection regions fundamentally different obtained traditional distance-based techniques rejection (see
Section 8). illustrate point (and still ignoring implementation details), consider
Figure 5 depict rejection regions training sample 150 points sampled
mixture two identical normal distributions (centered different locations).
height map figure, correspond disbelief index magnitude (a), distance
decision boundary (b), reflect confidence regions technique according
confidence measure.
5

learning problem synthetic problem used generating Figure 6.

184

fiAgnostic Pointwise-Competitive Selective Classification

(a)

(b)

Figure 5: Linear classifier. Confidence height map using (a) disbelief index; (b) distance
decision boundary.

Figure 6: SVM polynomial kernel. Confidence height map using (a) disbelief index;
(b) distance decision boundary.

intuitively explain height map Figure 5(a), recall disbelief index
difference empirical error ERM restricted ERM. test
point resides high density region, expect forcing wrong label point
result large increase training error. result, denser area is,
larger disbelief index, therefore, higher classification confidence.
second synthetic 2D source distribution consider even striking. X
distributed uniformly [0, 3] [2, 2] labels sampled according
following conditional distribution

0.95, x2 sin(x1 );
P (Y = 1|X = (x1 , x2 )) ,
0.05, else.
thick red line depicts decision boundary Bayes classifier. hight maps
Figure 6 depict rejection regions obtained (our approximation of) LESS

185

fiWiener & El-Yaniv

traditional (distance decision boundary) technique training sample 50
points sampled distribution (averaged 100 iterations). hypothesis
class used training SVM polynomial kernel degree 5. qualitative
difference two techniques, particular, nice fit disbelief
principle technique compared SVM quite surprising.

Figure 7: RC curves SVM linear kernel. method solid red, rejection
based distance decision boundary dashed green. Horizontal axis (c)
represents coverage.

7. Heuristic Procedure Using SVM Empirical Performance
computation (constrained) ERM oracle efficiently achieved case
realizable learning linear models (see, e.g., El-Yaniv & Wiener, 2010) case
linear regression (Wiener & El Yaniv, 2012). However, noisy setting computation
linear ERM oracle reduced variant MAX FLS C MAX FLS
problems (with strict non-strict inequalities) (Amaldi & Kann, 1995). Unfortunately,

186

fiAgnostic Pointwise-Competitive Selective Classification

MAX FLS APX-complete (within factor 2). C MAX FLS MAX IND SET-hard,
cannot approximated efficiently all. Moreover, extensions results
classes, including axis-aligned hyper-rectangles, showing approximating ERM
classes NP-hard (Ben-David et al., 2003).
present known hardness results (and related lower
bounds) hold half spaces nice distributions Gaussian (mixtures), note
Tauman Kalai et al. (2008) studied problem agnostically learning halfspaces
distributional assumptions. particular, showed data distribution
uniform d-dimensional unit sphere (or hyper-cube, related distributions),
4
possible agnostically learn -accurate halfspaces time poly(d1/ ). However,
known particular distributions elicit effective pointwise competitive
learning. contrary, uniform distribution unit sphere among
worst possible distributions pointwise-competitive classification (and disagreement-based
active learning) unless one utilizes homogeneous halfspaces (see discussion in, e.g., El-Yaniv
& Wiener, 2010).

Figure 8: SVM linear kernel. maximum coverage distance-based rejection
technique allows error rate method specific coverage.

187

fiWiener & El-Yaniv

discussed computational hurdles, recall much applied
machine learning research many applications quite well heuristic
approximations (rather formal ones). practical performance objective,
clever heuristics tricks sometimes make difference. point paper
therefore switch theory practice, aiming implementing rejection method
inspired disbelief principle see well work real world problems.
approximate ERM follows. Using support vector machines (SVMs) use
high C value (105 experiments) penalize training errors small
margin (see definitions SVM parameters in, e.g. Chang & Lin, 2011). way
solution optimization problem tend get closer ERM. order estimate
R(fx ) restrict SVM optimizer consider hypotheses classify
point x specific way. accomplish use weighted SVM unbalanced data.
add point x another training point weight 10 times larger weight
training points combined. Thus, penalty misclassification x large
optimizer finds solution doesnt violate constraint.
Another problem face disbelief index noisy statistic highly
depends sample Sm . overcome noise use robust statistics. First
1 , 2 , . . . k ) using bootstrap sampling
generate odd number k different samples (Sm


(we used k = 11). sample calculate disbelief index test points
point take median measurements final index. note
finite training sample disbelief index discrete variable. often case
several test points share disbelief index. cases use confidence
measure tie breaker. experiments use distance decision boundary
break ties. Focusing SVMs linear kernel compared RC (Risk-Coverage)
curves achieved proposed method achieved SVM rejection based
distance decision boundary. latter approach common practical
applications selective classification. implementation used LIBSVM (Chang &
Lin, 2011).
tested algorithm standard medical diagnosis problems UCI repository, including datasets used Grandvalet, Rakotomamonjy, Keshet, Canu (2008).
transformed nominal features numerical ones standard way using binary indicator attributes. normalized attribute independently dynamic
range [0, 1]. preprocessing employed. iteration choose uniformly
random non-overlapping training set (100 samples) test set (200 samples)
dataset.6 SVM trained entire training set, test samples sorted
according confidence (either using distance decision boundary disbelief index).
Figure 7 depicts RC curves technique (red solid line) rejection based
distance decision boundary (green dashed line) linear kernel 6 datasets.
results averaged 500 iterations (error bars show standard error). exception
Hepatitis dataset, methods statistically indistinguishable,
datasets proposed method exhibits significant advantage traditional
approach. would highlight performance proposed method
Pima dataset. traditional approach cannot achieve error less 8%
6

Due size Hepatitis dataset test set limited 29 samples.

188

fiAgnostic Pointwise-Competitive Selective Classification

Figure 9: RC curves SVM RBF kernel. method solid red rejection
based distance decision boundary dashed green.

rejection rate, approach test error decreases monotonically zero rejection
rate. Furthermore, clear advantage method large range rejection rates
evident Haberman dataset.7 .
sake fairness, note running time algorithm (as presented
here) substantially longer traditional technique. performance algorithm substantially improved many unlabeled samples available.
case rejection function evaluated unlabeled samples generate new
labeled sample. new rejection classifier trained sample.
Figure 8 depicts maximum coverage distance-based rejection technique
allows error rate method specific coverage. example, let us
assume method error rate 10% coverage 60%
7

Haberman dataset contains survival data patients undergone surgery breast cancer.
estimated 207,090 new cases breast cancer united states 2010 (Society, 2010)
improvement 1% affects lives 2000 women.

189

fiWiener & El-Yaniv

Figure 10: SVM RBF kernel. maximum coverage distance-based rejection
technique allows error rate method specific coverage.

distance-based rejection technique achieves error maximum coverage 40%.
point (0.6, 0.4) red line. Thus, red line bellow diagonal
technique advantage distance-based rejection visa versa.
example, consider Haberman dataset, observe regardless rejection rate,
distance-based technique cannot achieve error technique coverage
lower 80%.
Figures 9 10 depict results obtained RBF kernel. case statistically
significant advantage technique observed datasets.

8. Related Work
Pointwise-competitive classification unique extreme instance classification
abstention option, idea emerged pattern recognition community,
first proposed studied 50 years ago Chow (1957, 1970), generated lots interest

190

fiAgnostic Pointwise-Competitive Selective Classification

(Fumera et al., 2001; Tortorella, 2001; Santos-Pereira & Pires, 2005; Fumera & Roli, 2002;
Pietraszek, 2005; Bounsiar et al., 2006; Landgrebe et al., 2006; Herbei & Wegkamp, 2006;
Hellman, 1970; El-Yaniv & Pidan, 2011; Bartlett & Wegkamp, 2008; Wegkap, 2007; Freund
et al., 2004). Taking broader perspective, pointwise-competitive selective prediction (and
particular, classification) particular instance broader concept confidencerated learning, whereby learner must formally quantify confidence prediction.
Achieving effective confidence-rated prediction (including abstention) longstanding
challenging goal number disciplines research communities. Let us first discuss
prominent approaches confidence-rated prediction note
related present work.
knows-what-it-knows (KWIK) framework studied reinforcement-learning (Li,
Littman, & Walsh, 2008; Strehl & Littman, 2007; Li & Littman, 2010) similar notion
pointwise competitiveness studied, coverage rates analyzed (Li et al., 2008;
Li, 2009). However, KWIK limited realizable model concerned
adversarial setting target hypothesis training data selected
adversary. positive results KWIK adversarial setting apply
statistical pointwise-competitive prediction setting (where training examples sampled
i.i.d.), adversarial setting precludes non trivial coverage interesting hypothesis
classes currently addressed pointwise-competitive prediction. deficiency comes
surprise KWIK adversarial setting much challenging statistical
pointwise-competitive prediction assumptions.
conformal prediction framework (Vovk, Gammerman, & Shafer, 2005) provides
hedged predictions allowing possibility multi-labeled predictions guarantees
user-desired confidence rate asymptotic sense. Conformal prediction mainly concerned online probabilistic setting. Rather predicting single label
sample point, conformal predictor assign multiple labels. user-defined confidence level error rate asymptotically guaranteed. interpreting
multi-labeled predictions rejection, compare pointwise-competitive prediction. sense, conformal prediction construct online predictors reject option
asymptotic performance guarantees. important differences conformal predictions pointwise-competitive prediction pointed out.
approaches provide hedged predictions, use different notions hedging. Whereas
pointwise-competitive prediction goal guarantee high probability
training sample predictor agrees best predictor class
points accepted domain, goal conformal predictions provide guarantees
average error rate, average taken possible samples test
points.8 sense, conformal prediction cannot achieve pointwise competitiveness.
addition, conformal prediction utilizes different notion error one used
pointwise-competitive model. pointwise-competitive prediction focused performance guarantees error rate covered (accepted) examples, conformal
prediction provides guarantee examples (including multiple predictions none all). increasing multi-labeled prediction rate (uncertain prediction),
8

noted Vovk et al.: impossible achieve conditional probability error equal given
observed examples, unconditional probability error equals . Therefore, implicitly
involves averaging different data sequences... (Vovk et al., 2005, p. 295).

191

fiWiener & El-Yaniv

error rate decreased arbitrarily small value. case
pointwise-competitive prediction error notion covered examples, bounded
Bayes error covered region. Finally, conformal prediction mentions
notion efficiency, similar coverage but, best knowledge,
finite sample results established. Another interesting scheme vicinity
confidence-rated learning guaranteed error machine (GEM) (Campi, 2010).
GEM model reject option considered correct answer, means risk
reduced arbitrarily (as conformal prediction).
Pointwise-competitive classification special case pointwise-competitive prediction
(El-Yaniv & Wiener, 2010, 2011; Wiener & El Yaniv, 2012; El-Yaniv & Wiener, 2012;
Wiener, 2013; Wiener et al., 2014). Pointwise-competitive selective classification first
addressed El-Yaniv Wiener (2010) realizable case studied (in
paper pointwise-competitiveness termed perfect classification). present article
extends pointwise-competitive classification noisy problems
number theoretical studies (general) selective classification (not
pointwise-competitive). Freund et al. (2004) studied simple ensemble method binary
classification. Given hypothesis class F, method outputs weighted average
hypotheses F, weight hypothesis exponentially depends
individual training error. algorithm abstains prediction whenever weighted
average individual predictions close zero. able bound probability
misclassification 2R(f ) + (m) and, conditions, proved bound
5R(f ) + (F, m) rejection rate. LESS strategy viewed extreme
variation Freund et al. method. include ensemble hypotheses
sufficiently low empirical error abstain weighted average predictions
definitive ( 6= 1). risk coverage bounds asymptotically tighter.
Excess risk bounds developed Herbei Wegkamp (2006) model
rejection incurs cost [0, 1/2]. bound applies empirical risk minimizer
hypothesis class ternary hypotheses (whose output {1, reject}). See
various extensions Wegkap (2007) Bartlett Wegkamp (2008).
rejection mechanism SVMs based distance decision boundary perhaps
widely known used rejection technique. routinely used medical applications (Mukherjee et al., 1998; Guyon et al., 2002; Mukherjee, 2003). papers proposed
alternative techniques rejection case SVMs. include taking reject
area account optimization (Fumera & Roli, 2002), training two SVM classifiers
asymmetric cost (Sousa, Mora, & Cardoso, 2009), using hinge loss (Bartlett &
Wegkamp, 2008). Grandvalet et al. (2008) proposed efficient implementation SVM
reject option using double hinge loss. empirically compared results
two selective classifiers: one proposed Bartlett Wegkamp (2008)
traditional rejection based distance decision boundary. experiments
statistically significant advantage either method compared traditional
approach high rejection rates.
Pointwise selective classification strongly tied disagreement-based active learning.
realizable case, El-Yaniv Wiener (2012) presented reduction stream-based
active learning CAL algorithm Cohn et al. (1994) pointwise-competitive
classification. reduction roughly states rejection rate (the reciprocal

192

fiAgnostic Pointwise-Competitive Selective Classification

coverage) LESS O(polylog(m/)/m) problem (F, P ) actively learnable
CAL exponential speedup. consequence reduction resulted first
exponential speedup bounds CAL general linear models finite mixture
Gaussians. direction, showing exponential speedup CAL implies
rejection rate LESS (in realizable setting) recently established Wiener
(2013) Wiener, Hanneke, El-Yaniv (2014) (using two different techniques).
version space compression set size, extensively utilized present
work, introduced implicitly Hanneke (2007b) special case extended
teaching dimension, context, version space compression set called
minimal specifying set. introduced explicitly El-Yaniv Wiener (2010)
context selective classification, proved El-Yaniv Wiener (2012)
special case extended teaching dimension Hanneke (2007b). Relations
disagreement coefficient version space compression set size first discussed
El-Yaniv Wiener (2012). Sharp ties two quantities, stated
Lemma 9, others recently developed Wiener, Hanneke, El-Yaniv
(2014).

9. Concluding Remarks
find existence pointwise-competitive classification quite fascinating. striking
feature classifier that, definition, pointwise-competitive predictor free
estimation error cannot overfit. means hypothesis class
expressive still protected overfitting. However, without
effective coverage bounds pointwise-competitive classifier may refuse predict
times.
current paper, recent studies selective prediction (El-Yaniv & Wiener,
2015) active learning (Wiener, Hanneke, & El-Yaniv, 2014), place version space
compression set size center stage, leading quantity drive results
intuition domains. present, known technique able prove fast
coverage pointwise-competitive classification exponential label complexity speedup
disagreement-based active learning general linear models fixed mixture
Gaussians axis aligned rectangles product distributions.. possible
extend results beyond linear classifiers axis aligned rectangles interesting
distribution families? example, plausible existing results axis-aligned
rectangles extended decision trees.
formal relationship active learning pointwise-competitive classification
(El-Yaniv & Wiener, 2012; Wiener, 2013; Wiener et al., 2014) created powerful synergy
allows migrating results two models. Currently, formal connection manifested via two links. first, within realizable setting, equivalence
LESS-based classification fast coverage CAL-based active learning exponential speedup. second link consists bounds relate underlying complexity
measures: disagreement coefficient active learning, version space compression
set size pointwise-competitive classification. number non-established relations significantly substantiate interaction two problems could
considered. example, possible prove direct equivalence LESS-based

193

fiWiener & El-Yaniv

pointwise-competitive agnostic classification fast coverage rates LESS-based active
learning exponential speedup? expect resolution question
various interesting implications. example, relationship could potentially facilitate
migration interesting algorithms techniques devised active learning
pointwise-competitive framework. immediate candidate algorithm Beygelzimer et al. (2010), builds ideas Dasgupta et al. (2007b) Beygelzimer et al.
(2009). Resembling implementation proposed LESS via calls (a constrained) ERM
oracle, algorithm works without tracking version space final choice
hypothesis well querying component. Instead, querying, relies
ERM oracle enforces one example-based constrain. Thus, importanceweighting technique based resembles disbelief principle outline here.
regard, interesting consider migrate ideas active
learning algorithms emerging online learning branch (Orabona & Cesa-Bianchi,
2011; Cesa-Bianchi et al., 2009; Dekel et al., 2010) using, required, online batch
conversion techniques (Zhang, 2005; Kakade & Tewari, 2009; Cesa-Bianchi & Gentile, 2008;
Dekel, 2008).
LESS strategy requires unanimous vote among hypotheses low empirical
error subset hypothesis class. considering, e.g., linear models, subset
hypotheses uncountable, case (even finite) size huge. Clearly,
LESS extremely radical defensive strategy. immediate question arises
whether LESS unanimity requirement relaxed majority vote.
achieve pointwise competitiveness (strong) majority vote instead unanimity?
Besides greater flexibility general voting scheme, may lead different types
interesting learning algorithms, relaxation potentially ease computational
complexity implementing LESS (which, discussed above, bottleneck agnostic
classification). example, relaxed voting scheme might utilize hypothesis
sampling, classical example related context celebrated query-bycommittee (QBC) strategy (Seung et al., 1992; Freund et al., 1997; Fine et al., 2002; GiladBachrach, 2007; Gilad-Bachrach et al., 2005). However, strict pointwise competitiveness
advocated, easy see strong majority vote sufficient. Indeed, consider
f differs hypotheses F single point X . Unless probability
point large (not typical case), high probability point part
training set Sm , therefore, majority vote (even strong) label
opposite f . Hence, worst case, even strong majority sufficient pointwise
competitiveness. natural compromise pointwise competitiveness objective, one
revert standard excess-risk bounds (Bartlett et al., 2006) whereby compare
overall average performance predictor, R(f ), optimal predictor, R(f )
(not pointwise). regard, work Freund, Mansour, Schapire (2004) discussed
Section 8, result excess-risk bound R(f, g) 2R(f
) + 1/(m1/2)

( hyper-parameter) coverage bound (f, g) 1 5R(f ) ln |F|/ m1/2 .
Considering excess-risk bounds f , possible beat risk coverage
bounds using relaxed voting scheme rejection? would optimal bounds
fully agnostic setting? better bounds devised specific distributions
Gaussian mixtures? note Freund et al. strategy interesting

194

fiAgnostic Pointwise-Competitive Selective Classification

final aggregated predictor general outside F can, principle, significantly
outperform f F (the bound elicit behavior). emphasizes
potential usefulness ensembles, applied rejection scheme,
final predictor. Recall LESS strategy final predictor always belongs F.
Thus, considering ensembles allowing excess-risk bounds, even
ambitious goals, strictly beating f average.

Acknowledgments
thank anonymous referees good comments, grateful Steve Hanneke helpful insightful discussions. Also, warmly thank Intel Collaborative
Research Institute Computational Intelligence (ICRI-CI), Israel Science Foundation
(ISF) generous support.

Appendix A. Proofs
proof Lemma 9 relies following Lemma 16 (Wiener et al., 2014), whose
proof provided sake self-containment.

Lemma 16 (Wiener et al., 2014). realizable case, r0 (0, 1),



1
1
,
, 512 .
(r0 ) max max 16Bn
r 20
r(r0 ,1)


Proof. prove that, r (0, 1),




B(f , r)
1
1
max 16Bn
,
, 512 .
r
r 20

(12)

result follows taking supremum sides r (r0 , 1).
Fix r (0, 1), let = 1/r, {1, . . . , m}, define Sm\i = Sm \ {(xi , yi )}.
define Dm\i = DIS(VSF ,Sm\i B(f , r)) m\i = P(xi Dm\i |Sm\i ) = P (Dm\i Y).
B(f , r)m 512, (12) clearly holds. Otherwise, suppose B(f , r)m > 512. xi
DIS(VSF ,Sm\i ), must (xi , yi ) CSm .

n(Sm )


X
i=1

1

DIS(VSF ,Sm\i ) (xi ).

195

fiWiener & El-Yaniv

Therefore,
P {n(Sm ) (1/16)B(f , r)m}
)
(m
X

P
DIS(VSF ,S
) (xi ) (1/16)B(f , r)m
1

P
=P

m\i

i=1
(m
X

Dm\i (xi )

1

i=1
(m
X

(1/16)B(f , r)m

DIS(B(f ,r)) (xi )

1

i=1

=P

(


X



DIS(B(f ,r)) (xi )



1

1

+P

1

i=1

X

P



DIS(B(f ,r)) (xi )
i=1

X

i=1
(m
X

i=1
(m
X
1

1

)



1

1

Dm\i (xi )

Dm\i (xi )




X

+P

DIS(B(f ,r)) (xi )

i=1


X

1
B(f , r)m,
DIS(B(f ,r)) (xi )
16
1

Dm\i (xi )


X

1

1

i=1

(1/16)B(f , r)m

)
7

DIS(B(f ,r)) (xi ) < B(f , r)m
8



1
B(f , r)m,
DIS(B(f ,r)) (xi )
16

DIS(B(f ,r)) (xi )

)




X
i=1

)

1

)
7

DIS(B(f ,r)) (xi ) B(f , r)m
8

< (7/8)B(f , r)m

i=1

(

1



DIS(B(f ,r)) (xi )

i=1



1

Dm\i (xi )



)

(13/16)B(f , r)m

.

Since considering case B(f , r)m > 512, Chernoff bound implies
!

X

exp {B(f , r)m/128} < e4 .
P
DIS(B(f ,r)) (xi ) < (7/8)B(f , r)m
1

i=1

Furthermore, Markovs inequality implies
P


X

1

DIS(B(f ,r)) (xi )

i=1



1

Dm\i (xi )

!

(13/16)B(f , r)m


mB(f , r) E

hP


i=1

1

Dm\i (xi )

(13/16)mB(f , r)

Since xi values exchangeable,
#
"m



h h
ii X
X
X





E E Dm\i (xi )fiSm\i =
E m\i = m\m .
E
Dm\i (xi ) =
1

i=1

1

i=1

i=1

196



.

fiAgnostic Pointwise-Competitive Selective Classification

shown (Hanneke, 2012) least
m(1 r)m1 B(f , r).
particular, B(f , r)m > 512, must r < 1/511 < 1/2, implies
(1 r)1/r1 1/4,
#
"m
X

E
Dm\i (xi ) (1/4)mB(f , r).
1

i=1

Altogether, established
P (n(Sm ) (1/16)B(f , r)m) <

mB(f , r) (1/4)mB(f , r)
+ e4
(13/16)mB(f , r)
12
=
+ e4 < 19/20.
13


1
Thus, since n(Sm ) Bn m, 20
probability least 19/20, must


B(f , r)
1
.
Bn m,
> (1/16)B(f , r)m (1/16)
20
r

Proof Lemma 9. Assuming Bn (m, ) = polylog(m) log 1 holds, exists
constant 1 (0, 1/20)
Bn (m, ) non Bn (m, 1 ) = (polylog(m)).

1
1
Bn (m, 1 ), thus Bn m, 20
= (polylog(m)). Therefore,
increasing , Bn m, 20






1
1
max Bn m,
= max polylog(m) = polylog
,
20
r0
m1/r0
m1/r0
using Lemma 16 have,




1
(r0 ) max
max 16Bn m,
, 512
20
m1/r0





1
1
.
= polylog
528 + 16 max Bn m,
20
r0
m1/r0

References
Amaldi, E., & Kann, V. (1995). complexity approximability finding maximum
feasible subsystems linear relations. Theoretical computer science, 147 (1), 181210.
Bartlett, P. L., Jordan, M. I., & McAuliffe, J. D. (2006). Convexity, classification, risk
bounds. Journal American Statistical Association, 101 (473), 138156.
Bartlett, P., & Mendelson, S. (2006). Discussion 2004 IMS medallion lecture: Local
rademacher complexities oracle inequalities risk minimization V. koltchinskii. Annals Statistics, 34, 26572663.

197

fiWiener & El-Yaniv

Bartlett, P., Mendelson, S., & Philips, P. (2004). Local complexities empirical risk
minimization. COLT: Proceedings Workshop Computational Learning
Theory, Morgan Kaufmann Publishers.
Bartlett, P., & Wegkamp, M. (2008). Classification reject option using hinge loss.
Journal Machine Learning Research, 9, 18231840.
Ben-David, S., Eiron, N., & Long, P. (2003). difficulty approximately maximizing
agreements. Journal Computer System Sciences, 66 (3), 496514.
Beygelzimer, A., Dasgupta, S., & Langford, J. (2009). Importance weighted active learning.
Proceedings 26th Annual International Conference Machine Learning, pp.
4956. ACM.
Beygelzimer, A., Hsu, D., Langford, J., & Zhang, T. (2010). Agnostic active learning without
constraints. Advances Neural Information Processing Systems 23.
Beygelzimer, A., Dasgupta, S., & Langford, J. (2009). Importance weighted active learning.
Proceedings 26th annual international conference machine learning, pp.
4956. ACM.
Beygelzimer, A., Hsu, D., Langford, J., & Zhang, T. (2010). Agnostic active learning without
constraints. arXiv preprint arXiv:1006.2588.
Bounsiar, A., Grall, E., & Beauseroy, P. (2006). kernel based rejection method supervised classification. International Journal Computational Intelligence, 3, 312321.
Bousquet, O., Boucheron, S., & Lugosi, G. (2004). Introduction statistical learning
theory. Advanced Lectures Machine Learning, Vol. 3176 Lecture Notes
Computer Science, pp. 169207. Springer.
Campi, M. (2010). Classification guaranteed probability error. Mach. Learn., 80 (1),
6384.
Cesa-Bianchi, N., & Gentile, C. (2008). Improved risk tail bounds on-line algorithms.
Information Theory, IEEE Transactions on, 54 (1), 386390.
Cesa-Bianchi, N., Gentile, C., & Orabona, F. (2009). Robust bounds classification via
selective sampling. Proceedings 26th Annual International Conference
Machine Learning, pp. 121128. ACM.
Chang, C., & Lin, C. (2011). LIBSVM: library support vector machines. ACM
Transactions Intelligent Systems Technology, 2, 27:127:27. Software available
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
Chow, C. (1957). optimum character recognition system using decision function. IEEE
Trans. Computer, 6 (4), 247254.
Chow, C. (1970). optimum recognition error reject trade-off. IEEE Trans.
Information Theory, 16, 4136.
Cohn, D., Atlas, L., & Ladner, R. (1994). Improving generalization active learning.
Machine Learning, 15 (2), 201221.
Dasgupta, S., Hsu, D., & Monteleoni, C. (2007a). general agnostic active learning algorithm. NIPS.

198

fiAgnostic Pointwise-Competitive Selective Classification

Dasgupta, S., Monteleoni, C., & Hsu, D. J. (2007b). general agnostic active learning
algorithm. Advances neural information processing systems, pp. 353360.
Dekel, O. (2008). online batch learning cutoff-averaging.. NIPS.
Dekel, O., Gentile, C., & Sridharan, K. (2010). Robust selective sampling single
multiple teachers.. COLT, pp. 346358.
El-Yaniv, R., & Pidan, D. (2011). Selective prediction financial trends hidden
markov models. NIPS, pp. 855863.
El-Yaniv, R., & Wiener, Y. (2010). foundations noise-free selective classification.
Journal Machine Learning Research, 11, 16051641.
El-Yaniv, R., & Wiener, Y. (2011). Agnostic selective classification. Neural Information
Processing Systems (NIPS).
El-Yaniv, R., & Wiener, Y. (2012). Active learning via perfect selective classification.
Journal Machine Learning Research, 13, 255279.
El-Yaniv, R., & Wiener, Y. (2015). version space compression set size
applications. Vovk, V., Papadopoulos, H., & Gammerman, A. (Eds.), Measures
Complexity: Festschrift Alexey Chervonenkis. Springer, Berlin.
Fine, S., Gilad-Bachrach, R., & Shamir, E. (2002). Query committee, linear separation
random walks. Theoretical Computer Science, 284 (1), 2551.
Freund, Y., Mansour, Y., & Schapire, R. (2004). Generalization bounds averaged classifiers. Annals Statistics, 32 (4), 16981722.
Freund, Y., Seung, H., Shamir, E., & Tishby, N. (1997). Selective sampling using query
committee algorithm. Machine Learning, 28, 133168.
Friedman, E. (2009). Active learning smooth problems. Proceedings 22nd
Annual Conference Learning Theory.
Fumera, G., & Roli, F. (2002). Support vector machines embedded reject option.
Pattern Recognition Support Vector Machines: First International Workshop, pp.
811919.
Fumera, G., Roli, F., & Giacinto, G. (2001). Multiple reject thresholds improving
classification reliability. Lecture Notes Computer Science, 1876.
Gilad-Bachrach, R. (2007). PAC Beyond. Ph.D. thesis, Hebrew University
Jerusalem.
Gilad-Bachrach, R., Navot, A., & Tishby, N. (2005). Query committee made real.
NIPS.
Grandvalet, Y., Rakotomamonjy, A., Keshet, J., & Canu, S. (2008). Support vector machines reject option. NIPS, pp. 537544. MIT Press.
Guyon, I., Weston, J., Barnhill, S., & Vapnik, V. (2002). Gene selection cancer classification using support vector machines.. Machine Learning, 389422.
Hanneke, S. (2007a). bound label complexity agnostic active learning. ICML,
pp. 353360.

199

fiWiener & El-Yaniv

Hanneke, S. (2007b). Teaching dimension complexity active learning. Proceedings 20th Annual Conference Learning Theory (COLT), Vol. 4539 Lecture
Notes Artificial Intelligence, pp. 6681.
Hanneke, S. (2009). Theoretical Foundations Active Learning. Ph.D. thesis, Carnegie
Mellon University.
Hanneke, S. (2013). statistical theory active learning. Unpublished.
Hanneke, S. (2012). Activized learning: Transforming passive active improved label
complexity. Journal Machine Learning Research, 98888, 14691587.
Hellman, M. (1970). nearest neighbor classification rule reject option. IEEE
Trans. Systems Sc. Cyb., 6, 179185.
Herbei, R., & Wegkamp, M. (2006). Classification reject option. Canadian Journal
Statistics, 34 (4), 709721.
Kakade, S., & Tewari, A. (2009). generalization ability online strongly convex
programming algorithms. Advances Neural Information Processing Systems
(NIPS), pp. 801808.
Koltchinskii, V. (2006). 2004 IMS medallion lecture: Local rademacher complexities
oracle inequalities risk minimization. Annals Statistics, 34, 25932656.
Landgrebe, T., Tax, D., Paclk, P., & Duin, R. (2006). interaction classification
reject performance distance-based reject-option classifiers. Pattern Recognition
Letters, 27 (8), 908917.
Li, L., & Littman, M. L. (2010). Reducing reinforcement learning kwik online regression.
Annals Mathematics Artificial Intelligence, 217237.
Li, L., Littman, M., & Walsh, T. (2008). Knows knows: framework self-aware
learning. Proceedings 25th international conference Machine learning, pp.
568575. ACM.
Li, L. (2009). unifying framework computational reinforcement learning theory. Ph.D.
thesis, Rutgers, State University New Jersey.
Massart, P. (2000). applications concentration inequalities statistics. Annales
de la Faculte des Sciences de Toulouse, Vol. 9, pp. 245303. Universite Paul Sabatier.
Mendelson, S. (2002). Improving sample complexity using global data. Information
Theory, IEEE Transactions on, 48 (7), 19771991.
Mukherjee, S. (2003). Chapter 9. classifying microarray data using support vector machines.
scientists University Pennsylvania School Medicine School
Engineering Applied Science. Kluwer Academic Publishers.
Mukherjee, S., Tamayo, P., Slonim, D., Verri, A., Golub, T., Mesirov, J. P., & Poggio, T.
(1998). Support vector machine classification microarray data. Tech. rep., AI Memo
1677, Massachusetts Institute Technology.
Orabona, F., & Cesa-Bianchi, N. (2011). Better algorithms selective sampling.
Proceedings 28th International Conference Machine Learning (ICML-11),
pp. 433440.

200

fiAgnostic Pointwise-Competitive Selective Classification

Pietraszek, T. (2005). Optimizing abstaining classifiers using ROC analysis. Proceedings Twenty-Second International Conference Machine Learning(ICML), pp.
665672.
Santos-Pereira, C., & Pires, A. (2005). optimal reject rules ROC curves. Pattern
Recognition Letters, 26 (7), 943952.
Seung, H., Opper, M., & Sompolinsky, H. (1992). Query committee. Proceedings
Fifth Annual Workshop Computational Learning theory (COLT), pp. 287294.
Society, A. C. (2010). Cancer facts & figures 2010..
Sousa, R., Mora, B., & Cardoso, J. (2009). ordinal data method classification
reject option. ICMLA, pp. 746750. IEEE Computer Society.
Strehl, A. L., & Littman, M. L. (2007). Online linear regression application
model-based reinforcement learning. Advances Neural Information Processing
Systems, pp. 14171424.
Tauman Kalai, A., Klivans, A., Mansour, Y., & Servedio, R. (2008). Agnostically learning
halfspaces. SIAM J. Comput., 37 (6), 17771805.
Tortorella, F. (2001). optimal reject rule binary classifiers. Lecture Notes Computer
Science, 1876, 611620.
Tsybakov, A. (2004). Optimal aggregation classifiers statistical learning. Annals
Mathematical Statistics, 32, 135166.
Vovk, V., Gammerman, A., & Shafer, G. (2005). Algorithmic Learning Random World.
Springer, New York.
Wang, L. (2011). Smoothness, disagreement coefficient, label complexity agnostic
active learning. JMLR, 22692292.
Wegkap, M. (2007). Lasso type classifiers reject option. Electronic Journal
Statistics, 1, 155168.
Wiener, Y. (2013). Theoretical Foundations Selective Prediction. Ph.D. thesis, Technion
Israel Institute Technology.
Wiener, Y., & El Yaniv, R. (2012). Pointwise tracking optimal regression function.
Advances Neural Information Processing Systems 25, pp. 20512059.
Wiener, Y., Hanneke, S., & El-Yaniv, R. (2014). compression technique analyzing
disagreement-based active learning. arXiv preprint arXiv:1404.1504.
Zhang, T. (2005). Data dependent concentration bounds sequential prediction algorithms. Learning Theory, pp. 173187. Springer.

201


