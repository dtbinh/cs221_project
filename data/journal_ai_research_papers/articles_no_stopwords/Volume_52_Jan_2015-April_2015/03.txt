Journal Artificial Intelligence Research 52 (2015) 543-600

Submitted 09/14; published 04/15

Distributed Evaluation Nonmonotonic Multi-context Systems
Minh Dao-Tran
Thomas Eiter
Michael Fink
Thomas Krennwallner

DAO @ KR . TUWIEN . AC .
EITER @ KR . TUWIEN . AC .
FINK @ KR . TUWIEN . AC .
TKREN @ KR . TUWIEN . AC .

Institute fur Informationssysteme, TU Wien
Favoritenstrasse 9-11, A-1040 Vienna, Austria

Abstract
Multi-context Systems (MCSs) formalism systems consisting knowledge bases
(possibly heterogeneous non-monotonic) interlinked via bridge rules, global
system semantics emerges local semantics knowledge bases (also called contexts)
equilibrium. MCSs related formalisms inherently targeted distributed settings, truly distributed algorithms evaluation available. address shortcoming present suite algorithms includes basic algorithm DMCS, advanced version DMCSOPT exploits topology-based optimizations, streaming algorithm
DMCS-STREAMING computes equilibria packages bounded size. algorithms behave quite differently several respects, experienced thorough experimental evaluation
system prototype. experimental results, derive guideline choosing appropriate
algorithm running mode particular situations, determined parameter settings.

1. Introduction
last decade, increasing interest systems comprise information
multiple knowledge bases. includes wide range application fields data integration, multi-agent systems, argumentation many others. picture concrete real-world
application, may consider METIS (Velikova et al., 2014), industrial prototype system facilitating timely human decision making maritime control. application, human operators
need support determine whether ship entering port might hide identity illegal activities might high risk environmental hazard. access risks, METIS relies
number heterogeneous external information sources commercial ship database IHS
Fairplay,1 ship tracking websites,2 news items history pollution events ship may
involved in.
rise Word Wide Web distributed systems propelled development,
date several AI-based formalisms available host multiple, possibly distributed knowledge
bases compound system. Well-known formalisms distributed SAT solving (Hirayama
& Yokoo, 2005), distributed constraint satisfaction (Faltings & Yokoo, 2005; Yokoo & Hirayama,
2000), distributed ontologies different flavors (Homola, 2010), MWeb (Analyti, Antoniou, &
Damasio, 2011), different approaches multi-context systems (Giunchiglia & Serafini, 1994;
Ghidini & Giunchiglia, 2001; Brewka, Roelofsen, & Serafini, 2007; Brewka & Eiter, 2007; Bikakis
1. www.ihs.com/products/maritime-information/
2. marinetraffic.com, myship.com
c
2015
AI Access Foundation. rights reserved.

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

Figure 1: Pinpointing Joker
& Antoniou, 2010) rooted McCarthys (1993) work; among them, focus Heterogeneous Nonmonotonic Multi-context Systems (MCSs) (Brewka & Eiter, 2007).
generalization previous proposals, MCSs powerful formalism specify systems
knowledge bases may different formats reasoning powers, ranging simple
query answering relational database reasoning description logic knowledge bases (see
Baader et al., 2003), well nonmonotonic formalisms default logic (Reiter, 1980)
answer set programs (Gelfond & Lifschitz, 1991). allow heterogeneous knowledge bases
deal impedance mismatch them, MCSs abstract knowledge bases plain mathematical structures; top, special bridge rules interlink knowledge bases, bridge rule
adds formula knowledge base, depending certain beliefs knowledge bases. Hence
semantics knowledge base associated bridge rules, forms context, depends
contexts, possibly cyclic manner. Based this, MCSs equilibrium semantics
terms global states every context adopts abstract local model, called belief set,
conformant local models adopted contexts addition obeys
bridge rules. following simple example, paraphrase Ghidini Giunchiglias
(2001) Magic Box, illustrates power idea,
Example 1 Suppose computer game, players Batman Robin chased player Joker
partially occluded area, shown Figure 1; Robin wounded cannot read distance
objects. Neither Batman Robin tell Jokers exact position 33 box: Batman
assure columns 2 3, Robin tell row 1. However,
exchange partial knowledge, pinpoint Joker row 1 column 1.
model Batman Robin contexts whose local knowledge bases include information Jokers position, exchanged using bridge rules, row (X) (2 :
row (X)). Batman, informally imports Robins knowledge (context 2) row
positions; full encoding given Example 2. equilibrium emerging MCS discloses
Jokers position Batman Robin.
544

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

Although MCSs related formalisms inherently target distributed systems, truly distributed algorithms computing equilibria MCSs available. Brewka Eiter (2007)
encoded equilibria HEX-programs (Eiter, Ianni, Schindlauer, & Tompits, 2005),
evaluated using dlvhex solver. However, approach elegantly offers full heterogeneity, fully centralized needs technical assumptions. Roelofsen, Serafini, Cimatti (2004)
proposed earlier algorithm check satisfiability homogeneous, monotonic MCS
centralized control accesses contexts parallel (hence truly distributed). Bikakis
Antoniou (2010) instead gave distributed algorithm defeasible multi-context systems;
however, latter homogeneous (possibly nonmonotonic) contexts particular type
semantics, algorithm serves query answering model building.
lack distributed algorithms evaluating MCSs based local context handlers due
several obstacles:
abstract view local semantics belief sets limits algorithm global level
interference knowledge bases evaluation process context.
Towards real life applications, certain levels information hiding security required
(e.g. information exchange knowledge bases companies) selected
information transferred contexts via well-defined interfaces. prevents context
getting insight neighbors optimization, instance learn conflicts (i.e.,
joint beliefs leading contradiction) across contexts.
MCS system topology, i.e., structure context linkage, might unknown context;
disables decomposing system efficient, modular evaluation.
bridge rules might fuel cyclic information flow group contexts. Even
context easy evaluate (e.g., knowledge bases acyclic logic programs), global cycles
require nontrivial care.
article, address obstacles present results towards efficient distributed evaluation MCSs. main contributions suite generic algorithms DMCS, DMCSOPT,
DMCS-STREAMING work truly distributed, implementation system prototype.
detail, contributions follows.
1.1 Algorithms Optimization Techniques
(1) first, basic algorithm DMCS aims fully distributed setting deal obstacles
generic way: contexts exchange belief sets call history (i.e., access path
traversing bridge rules), information. global level, belief states formed
tuples belief sets; context bridge rules must respect belief sets neighbors
computing belief sets using local solver knowledge base. Cycles detected
call history, context gets request finds call history; break cycle,
guessing technique used checks return path.
(2) localizing contexts knowledge system information exchange, DMCS
fairly easily adapt context changes (additions deletions), time faces
scalability issues. enhance performance optimized version DMCSOPT, disclose
meta-level information contexts, viz. (i) topology context dependencies, exploited
decomposing MCS sub-MCSs (blocks) linked block-tree, (ii) interface contexts, optimizing data transfer blocks. (i) breaks cycles
545

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

advance (ii) significantly reduces duplicate local evaluation; yields remarkable performance gain.
(3) Still DMCS DMCSOPT compute equilibria MCS, escape
scalability memory issues, multiple local belief sets lead combinatorial explosion
global level. thus consider computing equilibria streaming mode; end, contexts pass
belief sets one shot parents gradually small packages. Memory blowup
thus avoided moreover contexts continue earlier wait answers
neighbors. approach seems user-friendly equilibria gradually appear rather
once, possibly long time; one may quit computation seeing sufficiently
many results (i.e., equilibria).
1.2 Implementation Experiments
implemented algorithms system prototype. assess effects optimization techniques, set benchmarking system conducted comprehensive experiments
MCSs various topologies interlinking. results confirm expectation optimization techniques general; nutshell, (i) decomposition technique clearly improves
performance non-streaming mode; (ii) streaming worthwhile may still find answers
non-streaming times out; (iii) streaming, choosing package size important;
(iv) system topology important optimization techniques show drastic improvements
specific topologies; (v) sometimes, techniques yield gain incur overhead.
results work provide truly distributed algorithms evaluating MCSs,
distributed versions non-monotonic knowledge base formalisms
(e.g., distributed answer set programs), underlying principles techniques might
exploited related contexts. Furthermore, may provide basis evaluation extensions generalizations MCSs, non-ground MCSs (Fink, Ghionna, & Weinzierl, 2011),
managed MCSs (Brewka, Eiter, Fink, & Weinzierl, 2011), supported MCS (Tasharrofi & Ternovska,
2014), reactive MCSs (Goncalves, Knorr, & Leite, 2014; Brewka, Ellmauthaler, & Puhrer, 2014).
1.3 Organization
remainder article organized follows. next section provides preliminaries
Multi-context Systems. Section 3 introduces basic distributed algorithm DMCS, Section 4 develops optimized algorithm DMCSOPT; Section 5 presents streaming algorithm DMCS-STREAMING. Experimental results prototype implementation reported
Section 6. Section 7, consider related works, Section 8 summarize address
open issues. increase readability, proofs moved Appendix.

2. Preliminaries
sections briefly introduces preliminaries needed rest article.
2.1 Multi-context Systems
First, present formalization Heterogeneous Nonmonotonic Multi-context Systems (MCSs)
proposed Brewka Eiter (2007) described Brewka, Eiter, Fink (2011),
546

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

serves base work. idea behind MCSs allow different logics used
different contexts, model information flow among contexts via bridge rules. notion
logic defined follows.
Definition 1 (cf. Brewka & Eiter, 2007) logic L = (KBL , BSL , ACCL ) composed
following components:
1. KBL set well-formed knowledge bases L, consists set
elements called formulas;
2. BSL set possible belief sets, BSL set elements called beliefs;

3. ACCL : KBL 2BSL function describing semantics logic, assigning
element KBL set acceptable sets beliefs.
notion logic generic, abstracts formation agents beliefs bare
minimum. Structure formulas (both knowledge base belief sets) dismissed,
viewed naked elements. Likewise particular inference mechanism associated
knowledge base, logical properties imposed belief sets; term belief
reflects statements held agent might epistemic basis, without going
detail. assignment acceptable beliefs sets knowledge base, intuitively
set beliefs agent willing adopt given knowledge base, captures logics
(e.g., nonmonotonic logics) multiple even acceptable belief sets possible.
abstract model allows us capture range different logics knowledge representation
reasoning, including classical logic, modal logics, epistemic logics, spatial logics, description
logics etc, nonmonotonic logics default logic (Reiter, 1980) answer set programs
(Gelfond & Lifschitz, 1991), different varieties settings. comparison formalisms
given Brewka et al. (2011). example, classical (propositional predicate logic) may
modeled follows:
KB: set (well-formed) sentences signature ,
BS: set deductively closed sets -sentences, (i.e., Cn(S) = S, Cn()
denotes deductive closure),
ACC(kb): singleton containing deductive closure kb, i.e., ACC(kb) = {Cn(kb)}.
example nonmonotonic logics, (disjunctive) logic programs answer set semantics (Gelfond & Lifschitz, 1991) modeled
KB: set logic programs signature ,
BS: set consistent sets literals ,
ACC(kb): set (kb) answer sets kb according Gelfond Lifschitz (1991).3
refer setting, used repeatedly sequel, Answer Set Programming
(ASP). Note answer sets knowledge base kb amount particular 3-valued models kb;
intuitively, positive literal p answer set S, p known true, negative
3. common, exclude inconsistent answer sets admitted Gelfond Lifschitz (1991).

547

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

literal p S, p known false, known means literal present
fact derivable rules; neither p p S, truth value p unknown.
MCS modeling possible worlds (scenarios) view via answer sets, generated
answer set solver. However, ASP implementations capture inference (truth
query respectively answer sets) forms belief set formation.
Bridge rules. Based logics, bridge rules introduced provide uniform way interlinking
heterogeneous information sources follows.
Definition 2 (cf. Brewka & Eiter, 2007) Let L = {L1 , . . . , Ln } (multi-)set logics. Lk bridge rule L, 1 k n, form
(c1 : p1 ), . . . , (cj : pj ), (cj+1 : pj+1 ), . . . , (cm : pm )

(1)

(i) 1 m, ci {1, . . . , n} pi element belief set Lci ,
(ii) kb KBk , holds kb {s} KBk .
Informally, bridge rules refer bodies contexts (identified ci ) thus add
information contexts knowledge base depending believed disbelieved
contexts. contrast Giunchiglias (1992) multi-context systems, single, global set
bridge rules; context knows bridge rules.
means connecting contexts available, MCSs formally defined.
Definition 3 (Brewka & Eiter, 2007) multi-context system (MCS) = (C1 , . . . , Cn ) consists
collection contexts Ci = (Li , kb , br ) Li = (KBi , BSi , ACCi ) logic, kb
KBi knowledge base, br set Li -bridge rules {L1 , . . . , Ln }.
Example 2 (contd) scenario Example 1 formalized MCS = (C1 , C2 ),
contexts L1 , L2 instances Answer Set Programming, and:


col (X) see col (X).
kb 1 = F F1
R,
col (X) see col (X).


row (X) (2 : row (X)).
br 1 =
row (X) covered row (X) (2 : see row (X)), (1 : row (X)).


row (X) see row (X).
kb 2 = F F2
R,
row (X) see row (X).


col (X) (1 : col (X)).
br 2 =
,
col (X) covered col (X) (1 : see col (X)), (2 : col (X)).

F = {row (1). row (2). row (3). col (1). col (2). col (3).},
F1 = {see col (2). see col (3).},
F2 = {see row (1).},
548

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS


R=





joker row (X).
joker col (X).

row (X) joker
row (X) joker
col (X) joker



col (X) joker




in, row (X), row (X).
in, row (X), row (Y ), X 6= Y.
in, col (X), col (X).
in, col (X), col (Y ), X 6= Y.





.




Here, X variables used schematic rules, range rows resp. columns (i.e.,
1,2,3). Intuitively, C1 formalizes Batmans knowledge scene C2 Robin.
knowledge bases kb 1 kb 2 , facts F represent box size 3 3, F1 F2 state
Batman Robin see, viz. Joker columns 2 3 respectively
row 1. next two rules simply map sensed locations respective facts. Informally, rules
R make guess row column Joker is, concluded box (first
two rules); may lead multiple belief sets. Importantly, Batman adjusts knowledge base
depending beliefs communicated Robin (bridge rules br 1 ) vice versa (bridge rules br 2 ).
convenience, introduce following notation conventions. MCS =
(C1 , . .
. , Cn ), denote Bi set
beliefs occur belief sets context Ci , i.e.,
Bi = SBSi S, let BM = ni=1 Bi (simply B, understood). Without loss
generality, assume distinct contexts Ci Cj , Bi Bj = , bridge
atom form (i : bi ) appearing bridge rule , holds bi Bi .
2.2 Semantics Multi-context Systems
semantics MCS defined terms special belief states, sequences =
(S1 , . . . , Sn ) Si element BSi . Intuitively, Si belief set
knowledge base kb ; however, bridge rules must respected. end, kb augmented
conclusions bridge rules applicable. precisely, bridge rule r form (1)
applicable S, pi Sci , 1 j, pk
/ Sck , j + 1 k m. denote
head (r) head r, app(R, S) set bridge rules r R applicable S.
Then,
Definition 4 (Brewka & Eiter, 2007) belief state = (S1 , . . . , Sn ) MCS = (C1 , . . . ,
Cn ) equilibrium, Si ACCi (kb {head (r) | r app(br , S)}), 1 n.
equilibrium thus belief state contains context acceptable belief set,
given belief sets contexts.
Example 3 (contd) MCS Example 2 single equilibrium = (S1 , S2 )
S1 = F F1 F3 S2 = F F2 F3 F3 = {joker in, row (1), row (2),
row (3), col (1), col (2), col (3)}. equilibrium indeed reflects intuition
scenario Example 1, Batman Robin together infer location Joker,
single one cannot accomplish task without communication.
Example 4 Let = (C1 , C2 , C3 , C4 ) MCS Li ASP logics, signatures
1 = {a}, 2 = {b}, 3 = {c, d, e}, 4 = {f, g}. Suppose
kb 1 = , br 1 = {a (2 : b), (3 : c)};
549

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

kb 2 = , br 2 = {b (4 : g)};
kb 3 = {c d; c}, br 3 = {c e (4 : f )};
kb 4 = {f g }, br 4 = .
One check = ({a}, {b}, {c, d}, {g}) equilibrium .
computation equilibria given MCS realized declarative implementation using HEX-programs (Eiter et al., 2005) evaluated using dlvhex system.4
idea translate MCS HEX-program (i) disjunctive facts guessing
truth values beliefs, (ii) HEX-rules capturing bridge rules, (iii) constraints external
atoms capturing acceptability functions. details concrete implementation
approach, refer reader MCS-IE system (Bogl, Eiter, Fink, & Schuller, 2010).
article, pursue sophisticated approach, i.e., design implement distributed
algorithms, compute equilibria MCSs. evaluation, centralized component
controls communication contexts. context independently runs instance
algorithm communicates exchange beliefs well detect break
cycles. novel contributions described next sections.

3. Basic Algorithm (DMCS)
section introduces first, basic, truly distributed algorithm evaluating equilibria
MCS. algorithm takes general setting input, is, context minimal
knowledge whole system; words, knows interface direct neighbors (parents child contexts) topological information metadata
system. setting, concentrate distributeness. Section 4 shifts focus towards
optimization techniques metadata provided.
Taking local stance, consider context Ck compute parts (potential) equilibria
system contain coherent information contexts reachable Ck .
3.1 Basic Notions
start basic concepts. import closure formally captures reachability.
Definition 5 (Import Closure) Let = (C1 , . . . , Cn ) MCS. import neighborhood
context Ck , k {1, . . . , n}, set
In(k) = {ci | (ci : pi ) B(r), r br k }.
Furthermore, import closure IC (k) Ck smallest set (i) k (ii)
S, In(i) S.
Equivalently, define import closure constructively IC (k) = {k}

IC 0 (k) = In(k), IC j+1 (k) = iIC j (k) In(i).



j0 IC

j

(k),

Example 5 Consider Example 4. In(1) = {2, 3}, In(2) = In(3) = {4}, In(4) =
; import closure C1 IC (1) = {1, 2, 3, 4} (see Figure 2).
4. www.kr.tuwien.ac.at/research/systems/dlvhex/

550

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

C1

C1

IC (1)

In(1)
C2

C2

C3

C3

C4

C4
(a) Import neighborhood C1

(b) Import closure C1

Figure 2: Import neighborhood Import closure
S=

S1

...



...



...

Sj

...

Sn

=



...



...

Ti

...

Tj

...

Tn

./ =

S1

...



...

Ti

...

Sj (= Tj )

...

Figure 3: Joining partial belief states
Based import closure define partial equilibria.
Definition
6 (Partial Belief States Equilibria) Let = (C1 , . . . , Cn ) MCS, let
Sn

/ i=1 BSi . sequence = (S1 , . . . , Sn ) Si BSi {}, 1 n,
partial belief state (PBS) , partial equilibrium (PE) w.r.t. Ck , k {1, . . . , n},
IC (k) implies Si ACCi (kb {head (r) | r app(br , S)}), 6 IC (k) implies
Si = , 1 n.
Note IC (k) essentially defines subsystem 0 connected bridge rules. use
PEs instead equilibria 0 keep original MCS intact.
combining partial belief states = (S1 , . . . , Sn ) = (T1 , . . . , Tn ), define
join ./ partial belief state (U1 , . . . , Un )

Si , Ti = Si = Ti ,
Ui =
, 1 n
Ti , Ti 6= Si = ,
(see Figure 3). Note ./ void, couples Si , Ti BSi different. Naturally,
join two sets partial belief states ./ = {S ./ | S, }.
Example 6 Consider two sets partial belief states:
= { (, {b}, , {f, g}) , (, {b}, , {f, g}) }
= {(, , {c, d, e}, {f, g}), (, , {c, d, e}, {f, g}), (, , {c, d, e}, {f, g})} .
551

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

join given

./ =

(, {b}, {c, d, e}, {f, g}), (, {b}, {c, d, e}, {f, g}),
(, {b}, {c, d, e}, {f, g})


.

3.2 Basic Algorithm
Given MCS starting context Ck , aim finding PEs w.r.t. Ck distributed
way. end, design algorithm DMCS whose instances run independently node
context communicate exchanging sets partial belief states.
provides method distributed model building, DMCS algorithm applied
MCS provided appropriate solvers respective context logics available. main
feature DMCS, compute projected partial equilibria, i.e., PEs projected relevant
part beliefs showing Ck import closure. exploited specific tasks
like, e.g., local query answering consistency checking. computing projected PEs,
information communicated contexts minimized, keeping communication cost low.
sequel, present basic version algorithm, abstracting low-level implementation issues; overall MCS structure assumed unknown context nodes. idea
follows: starting context Ck , visit import closure expanding import neighborhood
context Ci depth-first search (DFS), leaf context reached cycle detected, finding current context set hist already visited contexts. leaf context simply
computes local belief sets, transforms partial belief states, returns result
parent (invoking context, Figure 4a). case cycle (Figure 4c), context Ci detects
cycle must break it, (i) guessing belief sets export interface, (ii) transforming
guesses partial belief states, (iii) returning invoking context.
intermediate context Ci produces partial belief states joined, i.e., consistently
combined, partial belief states neighbors; enable this, Ci returns local belief sets,
joined results neighbors (Figure 4b).
computing projected PEs, algorithm offers parameter V called relevant interface
must fulfill conditions w.r.t. import closure next discuss.
Notation. Given (partial) belief state set V B beliefs, denote S|V restriction
V, i.e., (partial) belief state 0 = (S1 |V , . . . , Sn |V ), Si |V = Si V Si 6= ,
|V = ; set (partial) belief states, let S|V = {S|V | S}. Next,
Definition 7 (Recursive Import Interface) MCS = (C1 , . . . , Cn ) k {1, . . . , n},
call V(k) = {pi | (ci : pi ) B(r), r brk } import interface context Ck V (k) =

iIC (k) V(i) recursive import interface context Ck .
correct relevant interface V, two extremal cases: (1) V = V (k) (2) V = VB = B.
(1), DMCS basically checks consistency import closure Ck computing PEs
projected interface beliefs. (2), computes PEs w.r.t. Ck . between, providing fixed
interface V, problem-specific knowledge (such query variables) and/or infrastructure information
exploited keep computations focused relevant projections partial belief states.
projections partial belief states cached every context recomputation
recombination belief states local belief sets kept minimum.
assume context Ck background process (or daemon Unix terminology)
waits incoming requests form (V, hist), upon starts computation outlined
552

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

Si =



SS` ./Sj

lsolve(S)

(V, hist)

})
{i


(V, hist)

Sj



`

Ci

C`

(V

,h






Cj

C`
lsolve((, . . . , )) =
(a) Leaf context

(b) Intermediate context

V
Ci

hi

st

=

{.
.

.,

i,

Cj

..

.}

C`
Ct
(c) Cycle breaking

Figure 4: Basic distributed algorithm - casewise
Algorithm 1. process serves purpose keeping cache c(k) persistent.
write Ci .DMCS(V, hist) specify send (V, hist) process context Ci wait
return message.
Algorithm 1 uses following primitives:
function lsolve(S) (Algorithm 2): augments knowledge base kb current context
heads bridge rules br applicable w.r.t. partial belief state S, computes
local belief sets using function ACC, combines local belief set S, returns
resulting set partial belief states;
function guess(V, Ck ): guesses possible truth assignments relevant interface w.r.t.
Ck , i.e., Bk V.5
DMCS proceeds following way:

(a) check cache appropriate partial belief state;
5. order relate beliefs Bk , V either vector sets, variables V prefixed context ids;
simplicity, kept V set without assumptions.

553

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

Algorithm 1: DMCS(V, hist) Ck = (Lk , kb k , br k )
Input: V: relevant interface, hist: visited contexts
Data: c(k): static cache
Output: set accumulated partial belief states
(a)

c(k) empty return c(k)
:=

(b)
(c)

(d)

k hist
:= guess(V, Ck )
else
:= {(, . . . , )} hist := hist {k}
foreach In(k)
, Ti =
:= ./ Ci .DMCS(V, hist)

(e)

foreach := lsolve(T )

(f)

c(k) := S|V

// cyclic: guess local beliefs w.r.t. V
// acyclic: collect neighbor beliefs add local ones

return S|V
Algorithm 2: lsolve(S) Ck = (Lk , kb k , br k )
Input: S: partial belief state = (S1 , . . . , Sn )
Output: set locally acceptable partial belief states
:= ACCk (kb k {head (r) | r app(brk , S)})
return {(S1 , , . . . , Sk1 , Tk , Sk+1 , . . . , Sn ) | Tk T}

(b) check cycle;
(c) cycle detected, guess partial belief states relevant interface context
running DMCS;
(d) cycle detected, import neighbor contexts needed, request partial belief
states neighbors join them;
(e) compute local belief states given partial belief states collected neighbors;
(f) cache current (projected) partial belief state.
next examples illustrate evaluation runs DMCS finding partial equilibria
different MCS. start acyclic run.
Example 7 Reconsider Example 4. Suppose user invokes C1 .DMCS(V, ),
V = {a, b, c, f, g}, trigger evaluation process. Next, C1 forwards (d) requests C2
C3 , call C4 . called first time, C4 calculates (e) belief sets
assembles set partial belief states
S4 = {(, , , {f, g}), (, , , {f, g})} .
554

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

V

c(1) : S1
C1
S10 |V

2 |V

c(3) : S3

c(2) : S2
C2

C3
3 |V

Figure 5: cyclic topology
caching S4 |V (f), C4 returns S4 |V = S4 one contexts C2 , C3 whose request arrived
first. second call, C4 simply returns S4 |V context cache.
C2 C3 next call lsolve (in (e)) two times each, results S2 = resp. S3 =
S, Example 6.
= { (, {b}, , {f, g}) , (, {b}, , {f, g}) }
= {(, , {c, d, e}, {f, g}), (, , {c, d, e}, {f, g}), (, , {c, d, e}, {f, g})} .
Thus,
S2 |V

= { (, {b}, , {f, g}) , (, {b}, , {f, g}) }

S3 |V

= {(, , {c}, {f, g}), (, , {c}, {f, g}), (, , {c}, {f, g})} .

C1 , computing (d)
S2 |V ./ S3 |V = {(, {b}, {c}, {f, g}), (, {b}, {c}, {f, g}), (, {b}, {c}, {f, g})}
calls lsolve (e) thrice compute final result:
S1 |V = {({a}, {b}, {c}, {f, g}), ({a}, {b}, {c}, {f, g}), ({a}, {b}, {c}, {f, g})} .
next example illustrates run DMCS cyclic topology.
Example 8 Let = (C1 , C2 , C3 ) MCS Li ASP logic,
kb 1 = , br 1 = {a (2 : b)};
kb 2 = , br 2 = {b (3 : c)};
kb 3 = , br 3 = {c (1 : a)}.
Figure 5 shows cyclic topology . Suppose user sends request C1 calling C1 .DMCS(V, ) V = {a, b, c}. step (d) Algorithm 1, C1 calls C2 .DMCS(V, {1}),
context C2 issues call C3 .DMCS(V, {1, 2}), thus C3 invokes C1 .DMCS(V, {1, 2, 3}).
point, instance DMCS C1 detects cycle (b) guesses partial belief states
S10 = {({a}, , ), ({a}, , )}
555

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

1 V. Then, following dotted lines Figure 5, set S10 |V = S10 return value
request C3 , joins initial empty belief state (, , ), gives us
calls lsolve(T ) (e), resulting
S3 = {({a}, , {c, d}), ({a}, , {c, d}), ({a}, , {c, d})} .
next step C3 return S3 |V back C2 , proceed C3 before. result
set belief states
S2 = {({a}, {b}, {c}), ({a}, {b}, {c}), ({a}, {b}, {c})} ,
sent back C1 S2 |V . Notice belief state ({a}, {b}, {c}) inconsistent
C1 , eventually eliminated C1 evaluates S2 |V lsolve.
Next, C1 join S2 |V (, , ), yields S2 |V , use result call lsolve.
union gives us
S1 = {({a}, {b}, {c}), ({a}, {b}, {c})} ,
sent back user final result.
Given MCS = (C1 , . . . , Cn ) context Ck , using recursive import interface Ck ,
i.e., V (k), relevant interface safe (lower) bound correctness Algorithm 1.
follows, let , Ck , V (k) above.
Theorem 1 (Correctness DMCS partial equilibrium) every V V (k), holds
0 Ck .DMCS(V, ) iff partial equilibrium w.r.t. Ck 0 = S|V .
compute partial equilibria Ck use VB . holds using VB preserves
belief sets returned step (e), projection step (f) takes effect.
Corollary 2 partial equilibrium w.r.t. Ck iff Ck .DMCS(VB , ).
assumption single root context C1 , i.e., IC (1)
2 n, DMCS computes equilibria.
Corollary 3 MCS single root context C1 , equilibrium iff
C1 .DMCS(VB , ).
analysis algorithm yields following upper bound computational complexity
communication activity.
Proposition 4 Let = (C1 , . . . , Cn ) MCS. run DMCS context Ck
interface V, holds
(1) total number calls lsolve exponentially bound n |V|, i.e., O(2n|V| ).
(2) number messages exchanged contexts Ci , IC (k), bounded
2 |E(k)|, E(k) = {(i, cj ) | IC (k), r bri , (cj : pj ) B(r)}.
556

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

3.3 Discussion
Algorithm DMCS naturally proceeds forward import direction context Ck . Thus, starting
there, computes partial equilibria cover Ck contexts import closure.
contexts ignored; fact, unknown contexts closure. partial
equilibria may exist Ck import closure, whole MCS could equilibrium,
because, e.g., (P1) contexts access beliefs Ck closure get inconsistent, (P2)
isolated context subsystem inconsistent.
Enhancements DMCS may deal situations: (P1), context neighborhood
may include importing supporting contexts. Intuitively, Ci imports Cj , Ci
must register Cj . carefully adapting DMCS, solve (P1). However, (P2) remains;
needs knowledge global system topology.
suitable assumption manager exists every context Ci system
reach ask whether isolated inconsistent context subsystem exists; confirms this,
Ci DMCS instance simply returns , eliminating partial equilibria.
improve decentralization information hiding, weaken manager assumption
introducing routers. Instead asking M, context Ci queries assigned router R, collects
topology information needed Ci looks cache. information exchange Ci
R flexible, depending system setting, could contain contexts import information
Ci , isolated inconsistent contexts.
advantage topological information Ci recognize cyclic acyclic
branches upfront; invocation order neighborhood optimized, starting
acyclic branches entering cyclic subsystems. caching mechanism adapted
acyclic branches, intermediate results complete cache meaningful even across
different evaluation sessions.
setting, safe assuming V (k) V. needed resp. Ck
import closure join-contexts, i.e., contexts least two parents. access
path information context, could calculate V fly adjust
MCS traversal. particular, tree- ring-shaped , restrict V locally shared
interface Ck import neighbors, i.e., restricting V bridge atoms br k .
presence join-contexts, V must made big enough, e.g. using path information. Furthermore,
join-contexts may eliminated virtually splitting them, orthogonal parts contexts
accessed. way, scalability many contexts achieved.
Next, present optimization techniques using topological information system.

4. Topology-Based Optimization Algorithm (DMCSOPT)
basic version, Algorithm DMCS uses metadata apart minimal information
context must know: interface every neighboring context. scalability
issues tracked following problems:
(1) contexts unaware context dependencies system beyond neighbors, thus
treat neighbors equally. Specifically, cyclic dependencies remain undetected context,
seeing invocation chain, requests models context chain. Furthermore, context
Ci know whether neighbor Cj already requested models another neighbor Cj 0
would passed Ci ; hence, Ci makes possibly superfluous request Cj 0 .
557

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

(2) context Ci returns local models combined results neighbors. case
multiple models, result size become huge system size number neighbors
increases. fact, one main performance obstacles.
section address optimizations increase scalability distributed MCS evaluation. Resorting methods graph theory, aim decomposing, pruning, improved
cycle breaking dependencies MCSs. Focusing (1), describe decomposition method
using biconnected components inter-context dependencies. Based break cycles
prune acyclic parts ahead create acyclic query plan. address (2), foster partial view
system, often sufficient satisfactory answer, compromise partial
information performance. thus define set variables import dependency
system project models context bare minimum remain meaningful.
manner, omit needless information circumvent excessive model combinations.
proceed follows. introducing running example superficial explanation
optimization it, present details techniques Section 4.2. Section 4.3 introduces
notion query plans, used Section 4.4 describe algorithm DMCSOPT
intertwines decomposition pruning variable projection performance gains MCS evaluation.
4.1 Running Scenario
first present scenario Example 9 running example section.
Example 9 (Scientists Group) group four scientists, Alice, Bob, Charlie, Demi meets
conference closing arrange travel back home. options going train car
(which slower); use train, bring along food. Alice group
leader finally decides, based information gets Bob Charlie.6
Alice prefers go car, would object Bob Charlie want go train.
Charlie daughter, Fiona; mind either option, Fiona sick wants
fastest transport get home. Demi got married, husband, Eddie, wants back
soon, even sooner would come soon; Demi tries yield husbands plea.
Charlie charge buying provisions go train. might choose either salad
peanuts; notably, Alice allergic nuts. options beverages coke juice. Bob
modest; agrees choice Charlie Demi transport dislikes coke. Charlie
Demi want bother others personal matters communicate
preferences, sufficient reaching agreement.

Example 10 scenario Example 9 encoded MCS = (C1 , . . . , C6 ),
Alice = 1, Bob = 2, etc lexicographical order Li ASP logics. knowledge bases kbi
bridge rules bri follows:




car 1 train 1 .
train 1 (2 : train 2 ), (3 : train 3 ).
C1 : kb 1 =
br 1 =
;
nuts 1 .
nuts 1 (3 : peanuts 3 ).
6. Similar scenarios already investigated realm multi-agent systems (on social answer set programming see, e.g., Buccafurri & Caminiti, 2008). aim introducing new semantics scenarios;
example serves plain MCS showcase algorithms.

558

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

B1

1
2

1
2

3
4

3
4
5

4

B2

6

3

4

B3
3
6

5
(b) Diamond-ring block tree

(a) Diamond-ring

Figure 6: Topologies decomposition scientist group example

C2 : kb 2

C3 : kb 3

C4 : kb 4
C5 : kb 5
C6 : kb 6



car 2 (3 : car 3 ), (4 : car 4 ).
= { car 2 , train 2 .} br 2 = train 2 (3 : train 3 ), (4 : train 4 ), ;


(3 : coke 3 ).


car 3 train 3 .








train 3 urgent 3 .
urgent 3 (6 : sick 6 ).
=
;
br 3 =
salad 3 peanuts 3 train 3 .
train 3 (4 : train 4 )





coke 3 juice 3 train 3




= car 4 train 4 br 4 = train 4 (5 : sooner 5 ) ;




= sooner 5 soon 5 br 5 = soon 5 (4 : train 4 ) ;


= sick 6 fit 6 br 6 = .

context dependencies shown Fig. 6a. three equilibria, namely:
({train 1 }, {train 2 }, {train 3 , urgent 3 , juice 3 , salad 3 }, {train 4 },
{soon 5 , sooner 5 }, {sick 6 });
({train 1 }, {train 2 }, {train 3 , juice 3 , salad 3 }, {train 4 }, {soon 5 , sooner 5 }, {fit 6 });
({car 1 }, {car 2 }, {car 3 }, {car 4 }, , {fit 6 }).
Example 11 Consider MCS = (C1 , . . . , C7 ) context dependencies drawn Figure 7a. user queries C1 cares local belief sets C1 ,
evaluation process, C4 discard local belief sets C5 C6 answering call
C2 C3 . However, C1 calls C2 (or C3 ), invoked context must carry local belief sets
C4 answers C1 . reason belief sets C4 cause inconsistent joins C1
partial belief states returned C2 C3 , C5 C7 contribute directly
computing local belief sets C4 . Note belief sets C4 C7 play role determining
applicability bridge rules C1 .
559

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

query

query
C1
C2

C1
C3

C2

C4

C4

,S

C7
(a) Original Topology

C3

C7

(, , S3 )
(b) Triangle

C6

)

C2

C5

3

(

,S

2,

,

C6

S3

)

C1

(

C5

C3

(c) Transitive Reduction

Figure 7: Topology Example 11 (two stacked zig-zag diamonds)
Now, take sub-system including C1 , C2 , C3 , assuming C1 bridge rules atoms
(2 : p2 ) (3 : p3 ) body, C2 atoms (3 : p3 ). is, C1 depends C2
C3 , C2 depends C3 (see Fig. 7b). straightforward approach evaluate MCS asks
C1 belief sets C2 C3 . C2 depends C3 , would need another query
C2 C3 evaluate C2 w.r.t. belief sets C3 . shows evident redundancy, C3
need compute belief sets twice. Simple caching strategies could mellow second belief
state building C3 ; nonetheless, C1 asks C3 , context transmit belief states back,
thus consuming network resources.
Moreover, C2 asks PEs C3 , receive set PEs covers belief sets
C3 addition contexts C3 import closure. excessive C1 view,
needs know (2 : p2 ) (3 : p3 ). However, C1 needs belief states C2
C3 reply C2 : C2 reports belief sets (which consistent w.r.t. C3 ), C1 cant
align belief sets received C2 received C3 . Realizing C2 reports
belief sets C3 , call C3 must made.
4.2 Decomposition Nonmonotonic MCS
Based observations above, present optimization strategy pursues two orthogonal
goals: (i) prune dependencies MCS cut superfluous transmissions, belief state building,
joining belief states; (ii) minimize content transmissions. start defining
topology MCS.
Definition 8 (Topology) topology MCS = (C1 , . . . , Cn ) directed graph GM =
(V, E), V = {C1 , . . . , Cn } resp. V = {1, . . . , n} (i, j) E iff rule br
atom (j:p) body.
first optimization technique made three graph operations. get coarse view
topology splitting biconnected components, form tree representation MCS.
Then, edge removal techniques yield acyclic structures.
560

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

sequel, use standard terminology graph theory (see Bondy & Murty, 2008);
graphs directed default. may view undirected graphs directed graphs
edges (u, v), (v, u) undirected edge {u, v}.
graph G edges E(G), denote G\S maximal subgraph G
edges S. Suppose V 0 V (G) nonempty. subgraph G0 = (V 0 , E 0 ) G
vertex set V 0 edge set E = {(u, v) E(G) | u, v V (G)} subgraph induced
V 0 , denoted G[V 0 ]. induced subgraph G[V \ V 0 ] denoted G\V 0 ; results G
deleting vertices V 0 together incident edges. V 0 = {v}, write G\v G\{v}.
Two vertices u v G said connected, (directed) path u v
G, i.e., sequence vertices u = v1 , v2 , . . . , vn = v, (vi , vi+1 ) E(G), =
1, . . . , n1; path trivial n = 1. undirected graph G, connectedness equivalence
relation V (G). Thus partition V (G) nonempty subsets V1 , V2 , . . . , Vw
two vertices u v G connected iff belong set Vi .
subgraphs G[V1 ], G[V2 ], . . . , G[Vw ] called components G. w = 1 (i.e., G exactly
one component), G connected; otherwise, G disconnected.
directed graph G strongly connected, vertices u, v V (G) path u v
vice versa exists. strongly connected components G subgraphs G[V1 ], . . . , G[Vm ]
unique partition graph G pairwise disjoint induced subgraphs (i.e., Vi Vj = )
strongly connected.
Furthermore, directed graph G weakly connected, turning edges undirected edges
yields connected graph. vertex c weakly connected graph G cut vertex, G\c
disconnected. biconnected graph weakly connected graph without cut vertices.
block graph G maximal
biconnected
subgraph G. Given set blocks B,


union blocks B defined B = BB B, union two graphs G1 = (V1 , E1 )
G2 = (V2 , E2 ) defined G1 G2 = (V1 V2 , E1 E2 ).
Let (G) = (B C, E) denote undirected bipartite graph, called block tree graph G,

(i) B set blocks G,
(ii) C set cut vertices G,
(iii) (B, c) E B B c C iff c V (B).
Note (G) forest graph G rooted tree G weakly connected.
Example 12 Consider graph Figure 7a. One check 4 cut vertex
two blocks, viz. subgraphs induced {1, 2, 3, 4} {4, 5, 6, 7}.
next example shows block tree scenario Example 9.
Example 13 topology GM Example 10 shown Figure 6a. two cut vertices,
namely 3 4; thus block tree (GM ) (Figure 6b) contains blocks B1 , B2 , B3 ,
subgraphs GM induced {1, 2, 3, 4}, {4, 5}, {3, 6}, respectively.
topological sort directed graph linear ordering vertices every
directed edge (u, v) vertex u vertex v, u comes v ordering.
561

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

Pruning. acyclic topologies, triangle presented Figure 7b, exploit minimal
graph representation avoid unnecessary calls contexts, namely, transitive reduction
graph GM . Recall Aho, Garey, Ullman (1972) graph G transitive
reduction directed graph G whenever following two conditions satisfied:
(i) directed path vertex u vertex v G iff directed path u v
G,
(ii) graph fewer edges G satisfying condition (i).
Note G unique G acyclic. instance, graph Figure 7c unique transitive
reduction one Figure 7a.
Ear decomposition Another essential part optimization strategy break cycles removing edges. end, use ear decompositions cyclic graphs. block may multiple
cycles necessarily strongly connected; thus first decompose blocks strongly
connected components. Using Tarjans algorithm (Tarjan, 1972) task, one gets byproduct topological sort directed acyclic graph formed strongly connected components.
yield sequence nodes r1 , . . . , rs used entry points component. next
step break cycles.
ear decomposition strongly connected graph G rooted node r sequence P =
hP0 , . . . , Pm subgraphs G
(i) G = P0 Pm ,
(ii) P0 simple cycle (i.e., repeated edges vertices) r V (P0 ),
(iii) Pi (i > 0) non-trivial path (without cycles) whose endpoint ti P0 Pi1 ,
nodes not.
Let cb(G, P ) set edges containing (`0 , r) P0 last edge (`i , ti ) Pi , > 0.
Here, `0 vertex belonging edge root node r simple cycle P0 .
Example 14 Take, example, strongly connected graph G Figure 8a. ear decomposition G rooted node 1 P = hP0 , P1 , P2 , P3
VP0 = {1, 2, 3}, EP0 = {(1, 2), (2, 3), (3, 1)},

VP1 = {2, 4, 3}, EP1 = {(2, 4), (4, 3)},

VP2 = {2, 5, 3}, EP2 = {(2, 5), (5, 3)},

VP3 = {1, 4}, EP4 = {(1, 4)}.

last edges Pi dashed. form set cb(G, P ) = {(3, 1), (4, 3), (5, 3), (1, 4)}.
Removing edges results acyclic topology Figure 8b.
Intuitively, ear decomposition used remove cycles original system .
resulting acyclic topology, algorithms evaluating MCSs designed conveniently.
trade edge (`, t) removed , context C` , despite leaf
context, guess values variables Ct . following example shows application
optimization techniques running scenario.
Example 15 (contd) Block B1 (GM ) acyclic, transitive reduction gives B1
edges {(1, 2), (2, 3), (3, 4)}. B2 cyclic, hB2 ear decomposition rooted 4;
removing cb(B2 , hB2 i) = {(5, 4)}, obtain B20 edges {(4, 5)}. B3 acyclic already
reduced. Fig. 6b shows final result (dotted edges removed).
562

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

1

2

1

3

2

3

4

4

5

5

(a) strongly connected component

(b) Acyclic topology

Figure 8: Ear decomposition example
graph-theoretic concepts introduced here, particular transitive reduction acyclic blocks
ear decomposition cyclic blocks, used implement first optimization MCS
evaluation outlined above. Intuitively, block, apply ear decomposition get rid cycles (with trade-off guessing), use transitive reduction minimize communication.
Given transitive reduction B acyclic block B B, total order V (B ) , one
evaluate respective contexts reverse order total order computing PEs
context Ck : first context simply computes local belief sets whichrepresented
set partial belief states S0 constitutes initial set partial belief states T0 . iteration
step 1, Ti computed joining Ti1 local belief sets Si considered context Ci .
Given final Tk , Tk |V (k) set PEs Ck (restricted contexts V (B )).
Refined recursive import. Next, define second part optimization strategy
handles minimization information needed transmission two neighboring contexts
Ci Cj . purpose, refine notion recursive import interface (Definition 7)
context w.r.t. particular neighbor given (sub-)graph.
Definition 9 Given MCS = (C1 , . . . , Cn ) subgraph G GM ,
edge (i, j)


E(G), recursive import interface Ci Cj w.r.t. G V (i, j)G = V (i) `G|j B`
G|j contains nodes G reachable j.7
Example 16 (contd) MCS Example 10, V (1) = {train 2 , train 3 , peanuts 3 ,
car 3 , coke 3 , car 4 , train 4 , sooner 5 , sick 6 }. focus block B1 , refined recursive
import interface V (1, 2)B obtained removing bridge atoms contexts
1
blocks B2 B3 , yielding {train 2 , train 3 , peanuts 3 , car 3 , coke 3 , car 4 , train 4 }.
Algorithms. Algorithms 3 4 combine optimization techniques outlined above. Intuitively,
OptimizeTree takes input block tree parent cut vertex cp root cut vertex cr . traverses DFS manner calls OptimizeBlock every block. call results collected
7. Note V (k) defined Definition 7.

563

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

Algorithm 3: OptimizeTree(T = (B C, E), cp , cr )
Input: : block tree, cp : identifiesSlevel , cr : identifies
level cp

Output: F : removed edges B, v: labels ( B)\F
B 0 := , F := , v :=
cp = cr
B 0 := {B B | cr V (B)}
else
B 0 := {B B | (B, cp ) E}

(a)

(b)

// initialize siblings B0 return values

foreach sibling block B B 0
// sibling blocks B parent cp
E := OptimizeBlock(B, cp )
// prune block
0
C := {c C | (B, c) E c 6= cp }
// children cut vertices B
B 0 := B\E, F := F E
foreach edge (i, j) B 0 doS
// setup interface pruned B

v(i, j) := V (i, j)B 0 cC 0 V (cp )|Bc (`,t)E V (cp )|Bt
foreach child cut vertex c C 0
// accumulate children
(F 0 , v 0 ) := OptimizeTree(T \B, c, cp )
F := F F 0 , v := v v 0
return (F, v)

set F removed edges; blocks processed, final result OptimizeTree
pair (F, v) v labeling remaining edges. OptimizeBlock takes graph G calls
CycleBreaker cyclic G, decomposes G strongly connected components, creates
ear decomposition P component Gc , breaks cycles removing edges cb(Gc , P ).
resulting acyclic subgraph G, OptimizeBlock computes transitive reduction G
returns edges removed G. OptimizeTree continues computing labeling v remaining edges, building recursive import interface, keeping relevant
interface beliefs child cut vertices removed edges. Example 20 (Appendix B) illustrates
Algorithms 3 4 detailed run MCS Example 10.
Formally, following property holds.
Proposition 5 Given MCS context Ck k cut vertex topology GM ,
OptimizeTree(T (GM ), k, k) returns pair (F, v)
(i) subgraph G GM \F induced IC (k) acyclic,
(ii) block B G (i, j) E(B), holds v(i, j) V (i, j)B .
Regarding computational cost computation, obtain:
Proposition 6 Given MCS context Ck k cut vertex topology GM ,
OptimizeTree(T (GM ), k, k) runs polynomial (quadratic) time size (GM ) resp. GM .

564

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

Algorithm 4: OptimizeBlock(G : graph, r : context id)

(c)
(d)

F :=
G cyclic
F := CycleBreaker(G, r)

// ear decomposition strongly connected components

Let G transitive reduction G\F
return E(G) \ E(G )

// removed edges G

4.3 Query Plan
Given topology MCS, need represent stripped version contains
minimal dependencies contexts interface beliefs need transferred contexts. representation query plan used execution processing.
Syntactically, query plans following form.
Definition 10 (Query Plan) query plan MCS w.r.t. context Ck labeled subgraph
GM induced IC (k) E() E(GM ), edge labels v : E(G) 2 .
MCS context Ck , every query plan suitable evaluating M; however,
following query plan fact effective.
Definition 11 (Effective Query Plan) Given MCS context Ck , effective query plan
respect Ck k = (V (G), E(G)\F, v) G subgraph GM induced
IC (k) (F, v) = OptimizeTree(T (GM ), k, k).
next use k MCS evaluation, tacitly assume query plans effective.
4.4 Evaluation Query Plans
present algorithm DMCSOPT, based DMCS exploits optimization
techniques above. idea DMCSOPT follows: start context Ck traverse
given query plan k expanding outgoing edges k context, DFS,
leaf context Ci reached. context simply computes local belief sets, transforms belief
sets partial belief states, returns result parent. Ci (j : p) bridge rules
bodies context Cj query plan (this means broke cycle removing last
edge Cj ), possible truth assignments import interface Cj considered.
result context Ci set partial belief states, amounts join, i.e.,
consistent combination, local belief sets results neighbors; final result
obtained Ck . keep recomputation recombination belief states local belief sets
minimum, partial belief states cached every context.
Algorithm 5 shows distributed algorithm, DMCSOPT, instance context Ck .
input id c predecessor context (which process awaits), proceeds based
(acyclic) query plan r w.r.t. context Cr , i.e., starting context system. algorithm
maintains cache(k) cache Ck (which kept persistent).
Ci .DMCSOPT(c): send id c DMCSOPT context Ci wait result.
guess(V): guess possible truth assignments interface beliefs V.
565

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

Algorithm 5: DMCSOPT(c : context id predecessor) Ck = (Lk , kb k , br k )
Data: r : query plan w.r.t. starting context Cr label v, cache(k): cache
Output: set accumulated partial belief states
(a)

(b)
(c)

(d)

(e)

cache(k) empty
:= cache(k)
else
:= {(, . . . , )}
foreach (k, i) E(r ) := ./ Ci .DMCSOPT(k)

// neighbor belief sets

In(k) s.t. (k, i)
/ E(r ) Ti =
:= guess(v(c, k)) ./
// guess removed dependencies r
:=
foreach := lsolve(T )
// get local beliefs w.r.t.
cache(k) :=
(c, k) E(r ) (i.e., Ck non-root)
return S|v(c,k)
else
return

lsolve(S) (Algorithm 2): given partial belief state S, augment kbk heads
bridge rules brk applicable w.r.t. (=: kb0k ), compute local belief sets ACC(kb0k ),
merge S; return resulting set partial belief states.
steps Algorithm 5 explained follows:
(a)+(b) check cache, empty get neighbor contexts query plan, request
partial belief states neighbors join them;
(c) (i : p) bridge rules brk (k, i)
/ E(r ), neighbor delivered
belief sets Ci step (b) (i.e., Ti = ), call guess interface v(c, k)
join result (intuitively, happens edges removed
cycles);
(d) compute local belief states given partial belief states collected neighbors;
(e) return locally computed belief states project variables v(c, k) nonroot contexts; point mask parts belief states needed
contexts lying different block (GM ).
Theorem 7 shows DMCSOPT sound complete.
Theorem 7 Let Ck context MCS , let k query plan Definition 11 let
b = {p v(k, j) | (k, j) E(k )}. Then,
V
(i) 0 Ck .DMCSOPT(k), exists partial equilibrium w.r.t. Ck
0 = S|Vb ;
(ii) partial equilibrium w.r.t. Ck , exists 0 Ck .DMCSOPT(k)
0 = S|Vb .
566

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

5. Streaming Equilibria (DMCS-STREAMING)
Algorithm DMCSOPT shows substantial improvements DMCS; however, sizes
local knowledge bases context interfaces increase, suffers bottlenecks.
stems way models exchanged contexts. Suppose context C1
accesses several neighbors C2 , . . . , Cm acyclic information flow, Ci , n,
ni PEs. Ci computes DMCS resp. DMCSOPT local models, must join PEs
neighbors; may lead n2 n3 nm many PEs, input
local solver. may take considerable time exhaust memory, even
local model computation starts.
Note however instead neighbor would transfer portion PEs,
computation C1 avoid memory blowup. Moreover, strategy helps reduce
inactive running time C1 waiting neighbors return PEs, C1 already start
local computing neighbors producing models.
general, indispensable trade computation time, due recomputations, less
memory eventually partial equilibria C1 shall computed. idea underlying
streaming evaluation method distributed MCS. particularly useful user interested
obtaining instead answers system, realistic scenarios
current evaluation algorithm manage output resource constraints
practice equilibrium all.
section, turn idea concrete streaming algorithm DMCS-STREAMING
computing partial equilibria. main features briefly summarized follows:
algorithm fully distributed, i.e., instances components run every context
communicate, thus cooperating level peers;
invoked context Ci , algorithm streams (i.e. computes) k 1 partial equilibria
Ci time; particular, setting k = 1 allows consistency checking MCS
(sub-)system.
issuing follow-up invocations one may compute next k partial equilibria context C1
equilibria exist; i.e., evaluation scheme complete.
local buffers used storing exchanging local models (partial belief states)
contexts, avoiding space explosion problem.
section mainly studies streaming aspect algorithm, simplify presentation omit interface contexts. principles presented applied
DMCS DMCSOPT adapting interface pruning topology preprocessing
time. Furthermore, assume work acyclic MCSs. Treatment cyclic cases easily
achieved adding guessing code solving component DMCS DMCSOPT.
best knowledge, similar streaming algorithm neither developed
particular case computing equilibria MCS, generally computing models
distributed knowledge bases. Thus, results obtained interest setting
heterogeneous MCS, relevant general model computation reasoning
distributed (potentially homogeneous) knowledge bases e.g. distributed SAT instances.
567

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

request (k1 , k2 )

k belief states

Handler

Ci

Cj1

Solver

Joiner

..
.

Output

Cjm

Figure 9: DMCS-STREAMING architecture
Algorithm 6: Handler(k1 , k2 : package range) Ci
Output.k1 := k1 , Output.k2 := k2 ,
Solver.k2 := k2 , Joiner.k := k2 k1 + 1
call Solver

5.1 Basic Streaming Procedure
basic idea follows: pair neighboring contexts communicate multiple rounds,
request effect receive k PEs. communication window k PEs
ranges k1 -th PE k2 -th (= k1 + k 1) PE. parent context Ci requests child
context Cj pair (k1 , k2 ) receive time later package k PEs; receiving
indicates Cj fewer k1 models. parallelized version discussed Section 5.2.
Important subroutines new algorithm DMCS-STREAMING take care receiving
requests parents, receiving joining answers neighbors, local solving returning
results parents. reflected four components: Handler, Solver, Output, Joiner
(only active non-leaf contexts); see Figure 9 architectural overview.
components except Handler (shown Algorithm 6) communicate using message queues:
Joiner j queues store partial equilibria j neighbors, Solver one queue hold joined
PEs Joiner, Output queue carry results Solver. bound space usage,
queue limit number entries. queue full (resp., empty), enqueuing writer
(resp., dequeuing reader) automatically blocked. Furthermore, getting element removes
queue, makes room PEs queue later. property frees us
synchronization technicalities.
Algorithms 7 8 show Solver Joiner work. use following primitives:
lsolve(S): works lsolve DMCS DMCSOPT, addition may return one
answer time may able tell whether models left. Moreover, require
results lsolve returned fixed order, regardless called. property
key guarantee correctness algorithm.
get first(`1 , `2 , k): send neighbor c`1 c`2 request first k partial equilibria, i.e., k1 = 1 k2 = k; return models, store respective queues
return true; otherwise, return false (some neighbor inconsistent).
568

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

Algorithm 7: Solver() Ci
Data: Input queue: q, maximal number models: k2

(a)
(b)

count := 0
count < k2
Ci leaf :=
else call Joiner pop q
= count := k2

(c)

count < k2
pick next model ? lsolve(S)
? 6=
push ? Output.q
count := count + 1
else break
refresh() push Output.q

get next(`, k): request next k equilibria neighbor Cc` ; Cc` sends back models, store queue q` return true; otherwise, return false neighbor already
exhaustively returned PEs previous request. Note subroutine needs keep
track range already asked neighbor, maintaining set counters.
counter w.r.t. neighbor Cc` initialized 0 increased time get next(`, k) called.
value t, request Cc` asks tth package k models, i.e., models range
given k1 = (t 1) k + 1 k2 = k. get first(`1 , `2 , k) called, counters
range [`1 , `2 ] reset 0.
refresh(): reset counters flags Joiner starting states, e.g., first join true,
counters 0.
process context Ci triggered message parent, contains
range (k1 , k2 ) arrives Handler. latter notifies Solver compute k2 models Output
collect range (k1 , k2 ) return parent. Furthermore, sets package
size Joiner k = k2 k1 + 1 case Ci needs query neighbors (cf. Algorithm 6).
Solver receives notification Handler, first prepares input local solver.
Ci leaf context, input gets empty set assigned Step (a); otherwise, Solver triggers
Joiner (Step (b)) input neighbors. Fed input them, lsolve used Step (c)
compute k2 results send output queue.
Joiner, activated intermediate contexts discussed, gathers partial equilibria neighbors fixed ordering stores joined, consistent input local buffer.
communicates one input time Solver upon request. fixed joining order guaranteed always asking first package k models neighbors beginning Step (d).
subsequent rounds, begin finding first neighbor Cc` return models
(Step (e)), reset query ask first packs k models neighbors Cc1 Cc`1 .
neighbors run models Step (f), joining process ends sends Solver.
Note procedure guarantees models missed, lead consider combinations (inputs Solver) multiple times. Using cache helps mitigate
569

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

Algorithm 8: Joiner() Ci
Data: Queue q1 , . . . , queue qj In(i) = {c1 , . . . , cj }, buffer partial equilibria: buf ,
flag first join
true
buf empty
pop buf , push Solver.q
return

(d)

(e)

(f)

first join
get first(1, j, k) = false
push Solver.q
return
else first join := false
else
` := 1
get next(`, k) = false ` j ` := ` + 1
1 < ` j
get first(1, ` 1, k)
else ` > j
push Solver.q
return
S1 q1 , . . . , Sj qj add S1 ./ ./ Sj buf
C1

C2

C4

C3

C5

C6

C7

Figure 10: Binary tree MCS

recomputation, unlimited buffering quickly exceeds memory limits, recomputation
inevitable part trading computation time less memory.
Output component simply reads queue receives reaches k2 models
(cf. Algorithm 9). Upon reading, throws away first k1 1 models keeps ones
k1 onwards. Eventually, fewer k1 models returned Solver, Output
return parent.
Example 17 Let = (C1 , . . . , Cn ) MCS given integer > 0, n =
2m+1 1 contexts, let ` > 0 integer. Let contexts ASP logics. < 2m ,
570

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

Algorithm 9: Output() Ci
Data: Input queue: q, starting model: k1 , end model: k2
buf := count := 0
count < k1
pick Output.q
= count := k2 + 1
else count := count + 1
count < k2 + 1
wait Output.q
= count := k2 + 1
else
count := count + 1
add buf
buf empty
send parent
else
send content buf parent

context Ci = (Li , kbi , bri )
(
kbi =

{aji



aji

ti | 1 j `} bri =

)


ti (2i : aj2i ),
fi1j` ,
ti (2i + 1 : aj2i+1 )

(2)

2m , let Ci
kbi = {aji aji | 1 j `} bri = .

(3)

Intuitively, binary tree-shaped MCS depth `+1 size alphabet
context. Figure 10 shows MCS n = 7 contexts depth = 2; internal contexts
knowledge bases bridge rules (2), leaf contexts (3). directed
edges show dependencies bridge rules. system equilibria = (S1 , . . . , Sn )
Si = {aki , ti }, 1 k `.
compute one PE using DMCS DMCSOPT, one needs transfer packages 2`
PEs context parent (as context Ci computes subsets {a1i , . . . , a`i }).
intermediate context receives 2` results children, whose join leads 22` inputs
lsolve; invokes lsolve often returns 2` models parent,
wait this.
hand, DMCS-STREAMING needs transfer single PE pair
connected contexts, significant saving. Indeed, consider e.g. = 1, ` = 5, i.e.,
= (C1 , C2 , C3 ). Querying C1 package size k = 1 first causes query forwarded
C2 pair k1 = k2 = 1. C2 leaf context, invokes local solver eventually gets five
different models. However, returns one PE C1 , say (, {a12 }, ). Note t2 projected
among atoms C2 accessed C1 . happens C3 , assume
571

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

return (, , {a23 }) C1 . root context C1 , two PEs neighbors consistently
combined (, {a12 }, {a23 }). Feeding local solver, C1 obtains five models, returns
one them, say = ({a11 , t1 }, {a12 }, {a23 }).
following proposition shows correctness algorithm.
Proposition 8 Let = (C1 , . . . , Cn ) MCS, {1, . . . , n} let k 1 integer.
input (1, k) Ci .Handler, Ci .Output returns k different partial equilibria respect Ci ,
fact k least k partial equilibria exist.
5.2 Parallelized Streaming
one might expect, strategy ignoring k1 models collecting next k
likely effective. reason context uses one Solver,
general serve one parent, i.e., requests different ranges models size k.
new parent context requests models, refresh state Solver Joiner
redo scratch. unavoidable, unless context satisfies specific property one
parent call it.
way address problem parallelization. idea serve parent suite
Handler, Joiner, Solver Output. basic interaction units still shown
Figure 9, notable difference component runs individual thread.
significant change Solver control Joiner waits queue get new input
local solving process. Joiner independently queries neighbors, combines PEs
neighbors, puts results Solver queue.
effect waste recomputation time unused models. However, parallelization limits practice. DMCSOPT may run memory, unlimited parallel
instances streaming algorithm exceed number threads/processes operating
system support; happens contexts reach others many alternative paths,
stacked diamond topology: number threads exponential number connected contexts, prohibits scaling large system sizes. However, real-world applications number
paths might still ok.
compromise two extremes bounded parallel algorithm. idea create
fixed-size pool multiple threads components share among contexts; incoming requests cannot served resources available, algorithm continues basic
streaming procedure. realization remains future work.

6. Experimental Evaluation
implemented algorithms using C ++ system prototype called DMCS,
available online.8 space reasons, omit detailed presentation refer work
Bairakdar, Dao-Tran, Eiter, Fink, Krennwallner (2010b), Dao-Tran (2014, ch. 7). Briefly,
main components global architecture (i) command-line frontend dmcs user
access system; (ii) demons daemon represent nodes contain (a set of) contexts;
(iii) manager dmcsm containing meta-information MCS (topology, interfaces)
8. http://www.kr.tuwien.ac.at/research/systems/dmcs,
https://github.com/DistributedMCS/dmcs/

572

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

C1

C2

C1

C3

C2

C3
C2

C1

C4

C2

C5

C3

C4

C6

C5

C3
C1

C6
C4

C4

C5

C6

C7

(a) Binary Tree (T)

C7

C7

(b) Diamond (D)

(c) Zig-Zag (Z)

C4

(d) Ring (R)

Figure 11: Topologies testing DMCS algorithms
helper dmcsgen generating configurations optimized components. Contexts implemented groups threads communicate concurrent message queues.
system two main command-line tools, viz. running algorihms test case generation, respectively. allows switch different algorithms modes simply changing
command-line arguments.
turn experimental evaluation DMCS various aspects. Next describe
benchmarks set up, go runs results interpretation.
6.1 Benchmark Setup
idea analyze strong weak points algorithm respect different parameters,
namely system topology, system size, local theory (i.e., knowledge base) size, interface size.
Specifically, considered MCSs topologies Figure 11, including:
Binary Tree (T): Binary trees grow balanced, i.e., every level except last one complete.
topology, edge needs removed form optimal topology; every
intermediate node cut-vertex, import interface query plan drastically reduced,
leading extreme performance improvement.
(Stack of) Diamond(s) (D): diamond consists four nodes connecting C1 C4 Figure 11b. stack diamonds combines multiple diamonds row, i.e., stacking diamonds
tower 3m + 1 contexts. Similar Binary Tree, edge removed constructing
query plan. W.r.t. topology, every context connecting two diamonds cut-vertex.
such, import interface query plan refined every diamond; avoids
significantly repetition partial PEs evaluation.
(Stack of) Zig-Zag Diamond(s) (Z): zig-zag diamond diamond connection
two middle contexts, depicted contexts C2 C4 Figure 11c. stack zigzag diamonds built above. topology interesting removing two edges per
block, query plan turns linear topology.
Ring (R): ring (Figure 11d). query plan removes connection context Cn C1
carries interface way back C1 . topology requires guess573

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

a1

a2

a3

a4

a5

a6

a7

a8

Figure 12: Local theory structure
ing checking DMCS algorithm; thus quite unpredictable algorithm
performs better general.
quantitative parameters represented tuple P = (n, s, b, r),
n system size (number contexts),
local theory size (number ground atoms local theory),
b number local atoms used bridge atoms contexts,
words, number interface atoms,
r maximal number bridge rules. generator generates bridge rule iterating
1 r 50% chance; hence average r/2 bridge rules generated. allow
bridge bodies size 1 2.
test configuration formulated X/(n, s, b, r) X {T, D, Z, R} represents topology n, s, b, r integers representing quantitative (i.e., size-related) parameters.
would run several instances one configuration, final formulation test instance
Xi /(n, s, b, r), index test instance.
Inside context, local theories structured follows. Context Ci ground atoms
indicated ai,1 , . . . , ai,s . Rules form ai,j ai,k k = j + 1, j odd;
otherwise, randomly choose k j 1 j + 1 probability 50% possibility.
case k > rule exist. example context local theory size 8
illustrated dependency graph Figure 12. Here, bold arrows stand fixed
rules dashed arrows stands rules decided randomization. corresponding local
theory figure is:


a1 a2
a2 a1

a3 a4
a4 a3

a4 a5
a5 a6

a6 a7
a7 a8


.

setting, local context 2m answer sets, [0, s/2].
Furthermore, one obtain deterministic contexts (having one answer set) disallowing
cycles structure local theories.
6.2 Experiments
conducted experiments host system using 4-core Intel(R) Xeon(R) CPU 3.0GHz processor 16GB RAM, running Ubuntu Linux 12.04.1. Furthermore, used DLV [build BEN/Sep
28 2011 gcc 4.3.3] back-end ASP solver.
ran comprehensive set benchmarks setup described Section 6.1.
parameter space P = (n, s, b, r) huge, singled initial probing phase following
values experiments:
574

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

1000

DMCS
DMCSOPT

100

10

1

0.1

0.01
-5

T7,10,5,5
0

5

D7,10,5,5
10

Z7,10,5,5
15

R7,10,5,5
20

T10,10,5,5
25

D10,10,5,5
30

35

Z10,10,5,5
40

R10,10,5,5
45

50

Figure 13: DMCS vs. DMCSOPT non-streaming mode
system size n, depending topology:
T:
D:

n {7, 10, 15, 31, 70, 100}
n {4, 7, 10, 13, 25, 31}

Z:
R:

n {4, 7, 10, 13, 25, 31, 70}
n {4, 7, 10, 13, 70}

s, b, r fixed either 10, 5, 5 20, 10, 10, respectively.
combination topology X parameters P = (n, s, b, r) denoted X(n, s, b, r) X n,s,b,r
(used figures). parameter setting tested five instances. instance,
measured total running time total number returned partial equilibria DMCS,
DMCSOPT non-streaming streaming mode. latter mode, DMCS-STREAMING,
asked k answers, k {1, 10, 100}. parameter influences size packages
transferred contexts (at k partial equilibria transferred one message).
streaming mode, asking one PE may require multiple rounds get answers,
interest see fast first answers arrive compared answers. thus compared
running time tasks k = 10 k = 100.
6.3 Observations Interpretations
Figures 13-17 summarize results experiments. Run times seconds timeout
600 seconds. data, several interesting properties observed. organize
analysis along following aspects: (1) comparing DMCS DMCSOPT, (2) comparing
streaming non-streaming mode, (3) effect package size, (4) role topologies,
(5) behavior algorithms deterministic contexts.
6.3.1 DMCS VS . DMCSOPT
Figure 13 shows running time DMCS DMCSOPT computing partial equilibria, i.e.,
non-streaming mode, five instances respective parameter settings. Clearly DMCSOPT
outperforms DMCS. explained fact computing answers, DMCS
always produces partial equilibria DMCSOPT, one PE returned DMCSOPT
575

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

100

100
DMCS-1st
DMCSOPT-1st
DMCS-10
DMCSOPT-10

DMCS-1st
DMCSOPT-1st
DMCS-100
DMCSOPT-100

10

10

1

1

0.1

T1

T2

T3

T4

T5

D1

(a) (25, 10, 5, 5)

D2

D3

D4

D5

(b) D(10, 10, 5, 5)

1000

100
DMCS-1st
DMCSOPT-1st
DMCS-10
DMCSOPT-10

DMCS-1st
DMCSOPT-1st
DMCS-10
DMCSOPT-10

100

10

10

1

1

0.1

0.1

Z1

Z2

Z3

Z4

Z5

R1

(c) Z(10, 10, 5, 5)

R2

R3

R4

R5

(d) R(4, 10, 5, 5)

Figure 14: DMCS vs. DMCSOPT streaming mode
obtained projecting many partial equilibria returned DMCS imported interface.
Furthermore, intermediate results transferred one message, makes difference
terms number communications algorithms. such, DMCS must spend
time processing possibly exponentially input; hence, unsurprisingly, consistently
slower DMCSOPT.
However, observation streaming mode different. Figure 14 shows running time
DMCS DMCSOPT streaming mode compute first 100 respectively 10 unique partial
equilibria (25, 10, 5, 5) respectively D(10, 10, 5, 5), Z(10, 10, 5, 5) R(4, 10, 5, 5).
first view, DMCSOPT consistently slower DMCS, one might question correctness
results. However, surprise: PE returned DMCSOPT correspond several PEs returned DMCS. Hence, batch first k unique answers DMCS
corresponds smaller number (few) unique answers DMCSOPT.
Therefore, comparing DMCS DMCSOPT streaming mode measuring runtime
compute first k answers fair. thus took time algorithms finished first
round answers (denoted DMCS-1st DMCSOPT-1st Figure 14). setting,
observed following:
majority cases DMCSOPT finishes first round faster DMCS, however
40% instances, way around; shows effect using query plan;
576

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

however, cases DMCS wins. explained follows. First all, streaming
mode, transfer packages k partial equilibria time; therefore, effect reducing
amount total work done always apply non-streaming mode. Furthermore,
every context, compute k PEs project output interface returning
results. According strategy, context Ci returns k1 partial equilibria non-streaming
mode k2 partial equilibria streaming another context Cj , might happen k2 much
smaller k1 hence provide enough input Cj compute k PEs. Therefore, Cj
issue requests Ci asking packages k PEs, e.g., [k + 1, 2k], [2k + 1, 3k],
etc; costs DMCSOPT time even compute first batch PEs root context.
Another approach compute always k unique partial equilibria returning parent
context. However, strategy risks compute even local models k unique partial
equilibria found.
Overall, much difference running time DMCSOPT slower DMCS, except
instance R3 (Figure 14d). however comes different reason: cyclic topology
guess-and-check effects, play much important role choosing DMCS
DMCSOPT (see Section 6.3.4).
6.3.2 TREAMING VS . N - STREAMING DMCS
compare streaming non-streaming algorithm (DMCS resp. DMCSOPT).
Figure 15 shows results DMCS (15a), results DMCSOPT compute
first 10 resp. 100 PEs small systems/local knowledge bases (15b) large systems/local theories (15c). Excluding Ring (which behaves abnormally due guess-and-check)
one see that:
DMCS, streaming mode definitely worth pursuing since DMCS non-streaming
mode times many cases (see Figure 13), streaming mode still could find
answers reasonable time.
DMCSOPT, situation bit different, streaming loses non-streaming
small instances. due recomputation streaming mode pays transferring
chunks partial equilibria contexts; furthermore, duplications answers.
one moves larger systems local knowledge bases, streaming mode starts gaining
back. However, always win, recomputation still significantly takes time cases.
Summing up, system small enough, one try non-streaming mode
avoids recomputation duplication PEs different rounds computation. large
systems, streaming rescue us timing out. Even pay recomputation, still
helps cases results needed, e.g. brave query answering (membership
query PE).
6.3.3 E FFECTS PACKAGE IZE TREAMING ODE
considerations raise question optimal number PEs transferred
return messages contexts. analyze experimental results streaming
mode package sizes 1, 10, 100 give hints this.
Figure 16 shows average time compute 1 PE DMCSOPT streaming mode
respect three package sizes. One see transferring single PE get first answer
577

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

1000

Non-Streaming
Streaming-10
Streaming-100

100

10

1

0.1

0.01
-5

T10,10,5,5

0

5

D10,10,5,5
10

15

Z10,10,5,5

20

R4,10,5,5
25

30

(a) DMCS
1000

Non-Streaming
Streaming-10
Streaming-100

100

10

1

0.1

0.01
-5

T10,10,5,5

0

5

D10,10,5,5
10

15

Z10,10,5,5

20

R4,10,5,5
25

30

(b) DMCSOPT small systems local theories
1000

Non-Streaming
Streaming-10
Streaming-100

100

10

1

0.1

0.01
-5

0

T31,20,10,10

5

D10,20,10,10
10

15

Z10,20,10,10

20

R4,20,10,10
25

30

(c) DMCSOPT large systems local theories

Figure 15: Non-streaming vs. streaming DMCS DMCSOPT
acceptable cases, particular guessing needed. Moving size 1 small
package size 10 sometimes better, one save communication time (sending
package 10 partial equilibria vs. sending ten times package single PE). setting
(small package sizes 10) effective communication big factor,
578

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

1000

Streaming-1
Streaming-10
Streaming-100

100
10
1
0.1
0.01
0.001
-5

0

T100,20,10,10

5

D25,20,10,10

10

Z70,20,10,10
15

R4,20,10,10
20

25

Figure 16: Average time DMCSOPT find one partial equilibrium streaming mode, varying
package size
happens real applications contexts located physically distributed nodes.
cases, computing 10 partial equilibria faster computing 1 PE 10 consecutive times.
Furthermore, package size 1 safe cases guessing applied, e.g.,
R3 (4, 20, 10, 10). cases, large enough package size might help cover correct
guess; general, guarantee coverage. thoroughly solve problem,
one needs apply conflict learning whole MCS evaluation.
Also, interesting see package size 100, DMCSOPT usually times out.
reason many duplications DMCSOPT stuck local search branch
promises fewer 100 partial equilibria, algorithm lose time without finding
new unique answers eventually time out.
find good package size p specific setting (topology, system size, local theory size),
one may run system training set apply binary search p.
6.3.4 E FFECT OPOLOGY
quick glance plots Figures 1316 reveals pattern algorithms, especially
optimizations, perform better tree zigzag diamond, depending DMCS
DMCSOPT, worst ring.
system topology plays important role here. aspects affect performance
algorithms (i) number connections, (ii) structure block trees cut vertices,
(iii) acyclicity vs. cyclicity.
Regarding (i), topology introduces number connections based system size. Tree
fewer connections Diamond Zigzag, reduces communication
local solving time fewer requests made; performance DMCS topologies
proves observation. one follows argument, Ring must offer best performance.
However, actually case due aspect (iii) shortly analyze below.
Concerning (ii), tree ultimately optimized every intermediate node cut vertex.
Hence, applying query plan DMCSOPT, strip beliefs PEs sent
child contexts parent context. words, local beliefs context Ci needed
transferred back parents. drastically decreases amount information
579

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

1000
DMCS-100
DMCSOPT-100

100

10

1
-2

0

R4,10,5,5
2

4

6

R7,10,5,5
8

10

12

R4,20,10,10
14

16

18

Figure 17: DMCS vs. DMCSOPT streaming mode package size 100 ring
communicated, importantly, number calls lsolve. Due special property,
DMCSOPT performs extremely well tree topology, scales hundreds contexts.
Comparing Diamond Zigzag, number cut vertices. However, Zigzag
converted linear topology optimal query plan (cf. Figure 11c), therefore
processed much faster Diamond. Figure 16, DMCSOPT scales Zigzag 70 contexts
average time compute one answer still better one diamond 25 contexts.
Regarding (iii), Ring cyclic topology topologies acyclic. Hence
algorithms must guess-and-check context topology. Making
right guess important, even important reducing communication calls local
solvers. result running DMCS DMCSOPT topology (Figure 17) follow
pattern; absolutely depends specific instance whether sequential guessing
luckily arrives result. Therefore, frequently see DMCS outperforms DMCSOPT
streaming mode, cases, guessing root context (after detecting cycle)
effective guessing parent root context according optimal query plan.
Based observations, one come best strategy evaluate different types
topologies. dealing MCSs arbitrary topologies, looks natural decompose
parts familiar topologies efficient strategies known, combine
strategies overall evaluation method. Studying beyond scope work
interesting issue future research.
6.3.5 B EHAVIOR ETERMINISTIC C ONTEXTS
considered algorithms MCSs consisting possibly non-deterministic contexts,
i.e., one acceptable belief set per knowledge base. intriguing see
algorithms behave contexts always exactly one accepted belief set per knowledge base;
might underlying logic genuinely deterministic accepted belief set
clear (e.g., closure classical logic) among multiple candidates particular belief set chosen
(in implementations typically first best solution computed, e.g. SAT solving ASP).
observed that:
non-cyclic topologies, performance difference DMCS DMCSOPT,
smaller interface used DMCSOPT reduce number intermediate PEs
transferred contexts, one partial equilibrium computed every context.
580

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

1000

DMCS
DMCSOPT
MCS-IE

100

10

1

0.1

0.01
-5

T7,10,5,5
0

5

D7,10,5,5
10

Z7,10,5,5
15

R7,10,5,5
20

T10,10,5,5
25

D10,10,5,5
30

35

Z10,10,5,5
40

R10,10,5,5
45

50

Figure 18: DMCS vs. DMCSOPT streaming mode package size 100 ring
cyclic topology (Ring), guessing plays main role. Hence depends individual
instance whether DMCS DMCSOPT wins, case non-deterministic contexts (cf.
Section 6.3.4).
non-streaming mode much faster streaming (on DMCS DMCSOPT);
reasonable request partial equilibria redundant.
6.3.6 C OMPARISON MCS-IE P2P-DR
Systems close DMCS MCS-IE (Bogl et al., 2010)9 P2P-DR (Bikakis, Antoniou, & Hassapis, 2011). former plugin dlvhex system originally developed compute
explanations inconsistency Multi-context Systems, includes mode computing
equilibria MCS. However, MCS-IE implemented centralized approach. Figure 18
presents run time DMCS, DMCSOPT comparison MCS-IE computing partial equilibria respective configurations. shows MCS-IE outperforms DMCS since
inherits powerful decomposition technique dlvhex; however, decomposition based
topological information DMCSOPT turns efficient, localizes interface beliefs communicate blocks contexts, specific MCS
exploited general decomposition technique dlvhex.
P2P-DR supports distributed query answering multi-context systems based defeasible
logic; details, see Section 7. present comparison DMCS P2P-DR.
converted benchmark P2P-DRs style converting local knowledge bases
bridge rules defeasible local meta rules, added fixed trust order contexts.
queried root context atom appearing one answers DMCS-STREAMING
package size 10. turned P2P-DR always found answers around 0.25 seconds,
regardless tested instance. behavior explained follows. find answers
query atom, algorithm P2P-DR first evaluates local theory. determine truth
value query, terminates; otherwise algorithm consults neighbors get evidence
reasoning. local knowledge base structure, converted P2P-DRs defeasible
theories, allows local decision, system works local theory root context
9. http://www.kr.tuwien.ac.at/research/systems/mcsie/tut/

581

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

every test case, thus results almost constant execution time. Even asking neighbours
necessary, P2P-DR general may much faster DMCS, query answering process
inherently deterministic low-complexity logic; turn, formalism less expressive.
detailed study issue remains future work.
6.3.7 UMMARY
Summing up, analysis experimental results shows clear winner among
algorithms (DMCS vs. DMCSOPT) different running modes (streaming vs. non-streaming,
different package size) different topologies. distill guideline choose
setup fits specific instances practice, including issues open investigation,
briefly stated follows:
choose DMCSOPT DMCS non-streaming mode, except cyclic topologies;
streaming mode, choose appropriate package size carefully (e.g., binary search
training instances;
decompose random topologies parts whose topologies effective strategies evaluate,
study combine strategies systems.

7. Related Work
section, resume discussion related work. Starting multi-context systems,
provide details work Roelofsen et al. (2004), Bikakis et al. (2011) consider
work. move related formalisms SAT, CSP ASP.
Roelofsen et al. (2004) described evaluation monotone MCS classical theories using
SAT solvers contexts parallel. used (co-inductive) fixpoint strategy check MCS
satisfiability, centralized process iteratively combines results SAT solvers. Apart
truly distributed, extension nonmonotonic MCS non-obvious; furthermore,
caching technique used.
Serafini, Borgida, Tamilin (2005) Serafini Tamilin (2005) developed distributed
tableaux algorithms reasoning distributed ontologies, regarded multi-context
systems special bridge rules. algorithms serve decide whether system consistent, provided cyclic context dependencies exist (in technical terms, distributed TBox
acyclic); DRAGO system (Serafini & Tamilin, 2005) implements approach OWL ontologies. Compared ours, work tailored specific class multi-context systems resp.
knowledge bases, without nonmonotonic negation cyclic dependencies (which challenging); furthermore, targets query answering rather model building, sense dual
problem.
related work regards distributed evaluation system P2P-DR Bikakis
et al. (2011). developed distributed algorithm query evaluation multi-context system
framework specifically based (propositional) defeasible logic. framework, contexts built using defeasible rules exchange literals via bridge rules, trust order
contexts supplied. knowledge base context has, terminology, single
accepted belief set contains literals concluded; global system semantics given
terms (unique) three-valued assignment literals, determined using algorithm: whether literal l provably (not) logical conclusion system, whether remains
582

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

open. Apart tailored particular logic preference mechanisms evaluating interlinked contexts, applying algorithm model building straightforward; particular,
produces unique belief sets, dealing nondeterminism multiple equilibria possible.
work computing equilibria distributed multi-context systems clearly related
work solving constraint satisfaction problems (CSP) SAT solving distributed setting;
Yokoo Hirayama (2000) survey algorithms distributed CSP solving, usually developed setting node (agent) holds exactly one variable, constraints
binary, communication done via messages, every node holds constraints
involved. adopted later works (Gao, Sun, & Zhang, 2007) generalized
(Yokoo & Hirayama, 2000). relation topology-based optimization techniques Section 4,
biconnected components used Baget Tognetti (2001) decompose CSP problems.
decomposition used localize computation single solution components undirected constraint graphs. Along lines, approach based directed dependencies,
allows us use query plan MCS evaluation.
predominant solution methods CSP backtracking algorithms. Bessiere, Bouyakhf,
Mechqrane, Wahbi (2011) took step backtracking dynamic total ordering agents guided nogoods. approach, however, allows cyclic dependency
contexts. Hirayama Yokoo (2005) presented suite algorithms solving distributed SAT (DisSAT), based random assignment improvement flips reduce conflicts.
However, algorithms geared towards finding single model, extension streaming
multiple (or all) models straightforward; works distributed CSP SAT,
similar.
Finally, (distributed) SAT CSP solving concerns monotonic systems (removal clauses
resp. constraints preserves satisfiability), MCSs evaluation concerns nonmonotonic systems,
even contexts monotonic (e.g., clause sets); makes efficient evaluation difficult,
important structural properties search space cannot exploited.
Adjiman, Chatalic, Goasdoue, Rousset, Simon (2006) present framework peer-to-peer
inference systems, local theories propositional clause sets share atoms special algorithm consequence finding available. pursue dual problem model building,
applying needs straightforward; furthermore, dealing non-monotonic
systems, peer-to-peer systems Adjiman et al. monotonic.
Moving ASP, Pontelli, Son, Nguyens (2011) ASP-PROLOG shares MCS idea
integrating several knowledge bases, called modules, possibly different semantics. However, restricted module semantics ASP Prolog (that is, least Herbrand model),
ASP-PROLOG pursues query answering instead model building.
streaming, answer set streaming algorithm HEX-programs (which generalize ASP
external information access) given Eiter, Fink, Ianni, Krennwallner, Schuller
(2011). Despite similarities Algorithm DMCS-STREAMING, rather different: monolithic programs syntactically decomposed modules answer sets computed modular
fashion; fully distributed combines partial models lower components input
upper components straightforwardly; moreover, may use exponential space components.
583

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

8. Conclusion
considered distributed evaluation Multi-context Systems (MCSs) introduced
Brewka Eiter (2007) general formalism interlink possibly nonmonotonic heterogeneous knowledge bases. presented suite generic algorithms compute equilibria, i.e., semantics MCS fully distributed manner, using local solvers knowledge
bases contexts. contains basic algorithm DMCS, advanced version DMCSOPT
uses topology-based optimizations, streaming variant DMCS-STREAMING computing
partial equilibria gradually. believe underlying principles techniques might
exploited related contexts, particular distributed evaluation non-monotonic
knowledge base formalisms.
algorithms implemented prototype system available open source.8
top implementation, conducted comprehensive experiments compare performance algorithms gave insight analysis results. points advantages,
disadvantages well time/memory trade algorithms different situations
depending parameters system topology, local interface theory size, number
equilibria desired user. Based this, user choose setting (algorithm mode)
fits need best finding (partial) equilibria MCS. extensive treatment given
Dao-Tran (2014).
work open issues. Several issues remain investigation. One improvement algorithms. Here, experimental results Ring topology strongly suggest
incorporate conflict learning, proved valuable ASP SAT solving, DMCS
DMCSOPT; expect cyclic topologies benefit better guided guessing process. Another issue concerns semantics variants MCSs. former, grounded
equilibria considered Dao-Tran (2014), akin answer sets logic programs
applicable MCSs satisfy certain algebraic conditions; characterized answer
sets using (adapted) loop formula approach (Lee & Lifschitz, 2003). Dealing supported
equilibria (Tasharrofi & Ternovska, 2014), however, open.
Regarding MCS variants, managed MCSs (Brewka et al., 2011) generalize bridge rules derive
operations (commands) management function applied knowledge bases; seems
possible generalize algorithms setting, efficient realization straightforward. Another generalization MCS concerns dynamic data: areas sensor networks, social
networks, smart city applications, data may change even continuously arrive nodes,
motivates reactive stream processing MCSs (Goncalves et al., 2014; Brewka et al., 2014).
Last least, allowing contexts evolve via interation users changes environment valuable extention. Extending algorithms settings interesting
challenging.
Finally, extending work query answering MCSs, user poses query
context receives results derived (partial) equilibria another natural issue.
need building whole equilibria, better performance may achieved.

Acknowledgments
research supported Austrian Science Fund (FWF) projects P20841
P26471.
584

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

C1000

C1

C2

C2000

C3

C1

C3000

Figure 19: Introducing guess context(s)
thank reviewers pointing corrections constructive suggestions
helped improve presentation work, thank Antonis Bikakis providing us
P2P-DR system experimental comparison.

Appendix A. Proofs
Proof Theorem 1
prove theorem, first prove following Lemmas 9 10. latter aims simplifying
proof cyclic case, based notion converting cyclic MCSs acyclic ones.
Lemma 9 context Ck partial belief state MCS = (C1 , . . . , Cn ),
app(brk , S) = app(brk , S|V ) VB V V (k).
Proof r app(brk , S), (ci : pi ) B + (r) : pi Sci
(cj : pj ) B (r) : pj
/ Scj . need show pi Sci |Vci pj
/ Scj |Vcj . Indeed:
V VB Vcj VBj Scj |Vcj Scj . Therefore, pj
/ Scj pj
/ Scj |Vcj .
Now, assume pi
/ Sci |Vci . fact pi Sci , follows pi
/ Vci , hence

pi
/ V (k). contradiction fact pi occurs bridge rule body.
Therefore, r app(brk , S|V ).

next Lemma 10 based following notions convert cyclic MCSs acyclic
ones show corresponding equilibria. intuition (illustrated Figure 19
Examples 18, 19) introduce additional context Ck take care guessing every cycle
breaker Ck . Then, bridge rules Ck parents modified point Ck .
formally realize idea starting function ren renames part bridge rules.
Definition 12 Let Ck context MCS , let V interface running DMCS.
renaming function ren defined follows:

atom a: ren(a, k, V) =

ag


Bk V
otherwise


context index c: ren(c, k, V) =

c
c

c {1, . . . , n}
otherwise

bridge atom (ci : pi ): ren((ci : pi ), k, V) = (ren(ci , k, V) : ren(pi , k, V))

585

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

bridge body B = {(c1 : p1 ) . . . (cj : pj )}:
ren(B, k, V) = {ren((ci : pi ), k, V) | (ci : pi ) B}
bridge rule r = head (r) B(r):
ren(r, k, V) = head (r) ren(B(r), k, V)
set bridge rules br : ren(br , k, V) = {ren(r, k, V) | r br }
context Ci = (Li , kb , br ) : ren(Ci , k, V) = (Li , kb , ren(bri , k, V)).
Example 18 Let us slightly modify MCS = (C1 , C2 , C3 ) Example 8 follows:
kb 1 = {e e}, br 1 = {a (1 : e), (2 : b)};
kb 2 = , br 2 = {b (3 : c)};
kb 3 = , br 3 = {c (1 : a)}.
Applying function ren contexts C1 C3 results following bridge rules wrt. interface V = {a, b, c, e}:
ren(br 1 , 1, V) = {a (1 : eg ), (2 : b)},
ren(br 3 , 1, V) = {c (1 : ag )}.
two contexts Ci Cj , former called parent latter respect interface
V, denoted parent(Ci , Cj , V) iff exists bridge rule r br exists (c : p)
B(r) p Bj V.
set contexts {Cc1 , Cc2 , . . . , Cc` } MCS called cycle w.r.t. interface V iff
^
parent(Cc` , C1 , V)
parent(Cci , Cci+1 , V)
1i`1

holds. One pick arbitrary context set cycle-breaker. Given MCS ,
several ways choose (finite) set contexts cycle-breakers. Algorithm DMCS,
Step (d) practically establishes cycle-breakers based order elements In(k)
iterated. next definition, interested particular set cycle-breakers.
Definition 13 Given MCS = (C1 , . . . , Cn ), let CB rM = {Cc1 , . . . , Ccj } set cyclebreakers based application DMCS starting context Cr . conversion
equal acyclic ? based CB rM interface V done follows:

ren(Ci , i, V) Ci CB rM
0
0
Let Ci = (Li , kb , br ) =
Ci
otherwise
Let Ci00 = (Li , kb , br 00i ) = Ck CBM ren(Ci0 , k, V)10
00
br {a (i : ag ) | Bi V}
000
Let Ci000 = (Li , kb , br 000
)

br
=


br 00i

Ci CB rM
otherwise

Cj CB rM , introduce Cj = (Lj , kb j , br j ) br j = kb j = {ag ag |
Bj V}. ? = (C1000 , . . . , Cn000 , Cc1 , . . . , Ccj ).
10. order composing function ren different parameters k matter here.

586

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

Example 19 (contd) Let MCS Example 18 CB rM = {C1 }. Then, conversion Definition 13 gives ? = (C1000 , C2000 , C3000 , C1 ), where:
g
kb 1 = {e e}, br 000
1 = {a (1 : e ), (2 : b).

(1 : ag ).

e (1 : eg ).};

kb 2 = , br 000
2 = {b (3 : c)};
g
kb 3 = , br 000
3 = {c (1 : )};

kb 1 = {eg eg .

ag ag .}, br 1 = .

Lemma 10 Let MCS ? conversion acyclic MCS Definition 13.
equilibria ? 1-1 correspondence.
Proof (Sketch) Let (R1 ) (R2 ) runs DMCS ? , respectively. Due
selection CB rM construct ? , (R1 ) (R2 ) order visiting contexts,
except (R1 ) revisits cycle-breaker Ck CB rM , counterpart (R2 ) visits Ck .
corresponding locations:
(R1 ) calls guess(V, Ck ) Step (c),
(R2 ) calls lsolve({, . . . , }) Step (e) since Ck leaf context.
construction local knowledge base Ck gives us exactly guess Ck . Furthermore,
guesses passed parent contexts Ck later unified additional
bridge rules (k : ag ) introduced br 000
k . Therefore, belief combinations (Step (d)) done
Ck executed input runs (R1 ) (R2 ). correspondence equilibria
hence follows.

Proof (Theorem 1) Thanks Lemma 10, need prove Theorem 1 acyclic
case automatically get result cyclic case.
() start showing soundness DMCS. Let 0 Ck .DMCS(V, ) V V (k).
show partial equilibrium acyclic w.r.t. Ck 0 = S|V .
proceed structural induction topology .
Base case: Ck leaf In(k) = brk = k
/ hist. means (d)
executed, hence, (e), lsolve runs exactly (, . . . , ), get result set belief
states = lsolve((, . . . , )) = {(, . . . , , Tk , , . . . , ) | Tk ACCk (kbk )}. show
0 S|V . Towards contradiction, assume partial equilibrium = (S1 , . . . , Sn )
w.r.t. Ck 0 = S|V . In(k) = , get IC (k) = {k}, thus partial belief
state (, . . . , , Tk , , . . . , ) (where Tk ACCk (kbk )) partial equilibrium w.r.t. Ck .
Contradiction.
Induction step: assume context Ck import neighborhood In(k) = {i1 , . . . , im }
i1 = Ci1 .DMCS(V, hist {k}),
..
.
im = Cim .DMCS(V, hist {k}).
587

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

induction hypothesis, every 0ij ij , exists partial equilibrium ij
w.r.t. Cij ij |V = 0ij .
Let = Ck .DMCS(V, hist). need show every 0 S, partial equilibrium w.r.t. Ck 0 = S|V . Indeed, since In(k) 6= , Step (d) executed; let
= i1 ./ ./ im
result combining partial belief states
Scalling DMCS Ci1 , . . . , Cim . Furthermore,
?
?
Step (e), = |V = {lsolve(S) | }. Eventually, 0 S|V .
Since every DMCS Ci1 , . . . , Cim returns partial equilibria w.r.t. Cij projected V,
every partial equilibrium w.r.t. Cij projected V. acyclic visited
contexts In(k), thus Lemma 9 get every , app(brk , ) gives us
applicable bridge rules r regardless Tj = , j
/ In(k). Hence, , lsolve(T )
returns partial belief states, component projected V except kth component.
every preserves applicability rules Lemma 9, get every 0 S|V ,
exists partial equilibrium w.r.t. Ck 0 = S|V .
() give proof completeness DMCS structural induction topology
acyclic . Let = (S1 , . . . , Sn ) partial equilibrium w.r.t. Ck let 0 = S|V .
show 0 Ck .DMCS(V, ).
Base case: Ck leaf context. Then, executing Ck .DMCS(V, ), Step (d) ignored
Step (e) called input (, . . . , ), lsolve((, . . . , )) gives us belief sets Ck .
equilibrium w.r.t. Ck , S; hence, 0 = S|V returned Ck .DMCS(V, ).
Induction case: suppose import neighborhood context Ck In(k) = {i1 , . . . , im }. Let
restriction every context Cij In(k) denoted ij , where:

S` ` IC (ij )
0
0
0
ij
= (S1 , . . . , Sn ) S` =

otherwise
Informally speaking, restriction keeps belief sets contexts reachable Cij
sets non-reachable contexts . induction hypothesis, ij |V computed
Cij .DMCS(V, ) ij In(k). show S|V computed Ck .DMCS(V, ).
Indeed, considering acyclic , holds ij |V returned call
Cij .DMCS(V, {k}), k plays role calls Cij neighbors. means
step (d), contains = Si1 ./ . . . ./ Sim Sij appears position ij S.
Since partial equilibrium w.r.t. Ck , Sk ACCk (kbk {head (r) |
r app(brk , S)}). Furthermore, choosing V V (k), Lemma 9 tells us applicability
bridge rules preserved projection belief sets V. gives us Sk lsolve(T )
step (e), hence 0 = S|V returned Ck .DMCS(V, ).

Proof Proposition 4
(1) context Ck , let number calls local solver denoted c(k). number
calculated computation Step (d), bounded maximal number
combined partial belief sets neighbors. Formally speaking:
c(k) iIn(k) 2|VBi | 2|In(k)||V| 2n|V| .
588

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

Hence whole MCS, upper bound calls lsolve run DMCS
c = 1kn c(k) n 2n|V|
(2) context Ck MCS = (C1 , . . . , Cn ), set E(k) contains dependencies
contexts Ci IC (k). visit (i, j) E(k) exactly twice DFS-traversal :
calling Cj .DMCS(V, hist) Ci , retrieving S|V Cj Ci . Furthermore,
caching technique Step (a) prevents recomputation already visited nodes, thus prevents
recommunication subtree visited node. claim hence follows.

Proof Proposition 5
Item (i) trivial see since CycleBreaker applied Algorithm 4. prove item (ii), let us look
two cases edge (`, t) removed original topology Step (a) Algorithm 3:
(`, t) removed CycleBreaker: causes certain nodes graph cannot reach
via `. However, interface Ct provides already attached v(i, j) via V (cp )|Bt .
(`, t) removed transitive reduction: change reachability
nodes; therefore, interface Ct provides already included V (i, j)B 0 .


argument gives us property (ii).
Proof Proposition 6
First, estimate complexity compute v(i, j) loop (a).
[
[
v(i, j) := V (i, j)B 0
V (cp )|Bc
V (cp )|Bt
cC 0

(`,t)E

one hand, refined recursive import V (i, j)0B defined (Definition 9):
V (i, j)0B = {V (i)

[

B` }

`B 0 |j

B 0 |j contains nodes reachable j.
hand, since sets possible beliefs different contexts disjoint,

[
cC 0

V (cp )|Bc

[

V (cp )|Bt = V (cp )|ScC0 Bc S(`,t)E Bt

(`,t)E


Since recursive import interface node k defined V (k) = iIC (k) V(i),
expression compute v(i, j) end combination set intersection, union, projection.
implementation sets using hash set, is, look takes O(1), operators
implemented linear time. Therefore, v(i, j) computed linear time total number
beliefs contexts system.
Given GM , block tree graph (GM ) constructed linear time (Vats & Moura,
2010). Ear-decomposition (Step (c)) done linear time (Valdes, Tarjan, & Lawler,
589

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

1982). Transitive reduction (Step (d)) computed quadratic time respect number
edges block.
OptimizeTree(T (GM ), k, k) iterates blocks. Assume blocks B1 . . . ,
Bm , Bi contains ni edges, n =
i=1 ni total number edges original
graph. Let ti time process block Bi . bound total processing time
assessed follows:



X
X
X
2
t=
ti
ni (
ni )2 = n2 .
i=1

i=1

i=1

Therefore, ignore loop (a), OptimizeTree done quadratic time size
original input, i.e., size GM .

Proof Theorem 7
prove this, need Proposition 11 claim partial equilibria returned DMCS
DMCSOPT correspondence. first, need following supportive notion.

Definition 14 Let Ck context MCS , let k query plan Definition 11.
block B k , block interface B, whose root vertex cB ,
VB = {p v(i, j) | (i, j) E(B)} BcB .
Let Ci context B. self-recursive import interface Ci B
[
V (i)B = Bi
V (i, `)B .
(i,`)E(k )

Proposition 11 Let Ck context MCS
, let k query plan Definition 11
Ck belongs block B k let V = Bk VB . Then,
(i) 0 DMCSOPT(k) called Cc (c, k) E(k ) c = k, exists
partial equilibrium Ck .DMCS(V, ) 0 = S|V (c,k)B (c, k) E(k )
0 = S|V (k)B c = k;
(ii) Ck .DMCS(V, ), exists DMCSOPT(k) called Cc
0 = S|V (c,k)B (c, k) E(k ) 0 = S|V (k)B c = k.
detailed proof Proposition 11 given next section, give proof Theorem 7.
Proof (Theorem 7) (i) Let 0 Ck .DMCSOPT(k) result DMCSOPT. Proposi00
0
00
tion 11 (i)
Sc = k, exists Ck .DMCS(V, ) = |V (k)B ,
choose V = Bk VB . Note V (k) V V collects bridge atoms blocks,
might contain blocks reachable k. Theorem 1, exists partial equilibrium
00 = S|V . Thus,
0 = (S|V )|V (k)B
= S|V (k)B
V (k)B V
b V (k)B
= S|Vb
V
(ii) Let partial equilibrium MS. Theorem 1, exists 00 Ck .DMCS(V, )
00 = S|V choose V = Bk VB . above, V (k) V. Proposition 11 (ii)
c = k, exists 0 Ck .DMCSOPT(k) 0 = 00 |V (k)B . above,
0 = S|Vb .

590

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

Proof Proposition 11
support proof Proposition 11, need following lemmas.
Lemma 12 Assume context Ck import neighborhood In(k) = {i1 , . . . , im }, (k, ij ) removed original topology OptimizeBlock(B, cB ),
0i1

= DMCSOPT(k) Ci1
..
.

i1

= Ci1 .DMCS(VB , )
..
.

0im

= DMCSOPT(k) Cim

im

= Cim .DMCS(VB , )

every partial equilibrium 0 0ij , exists ij 0 = S|V (k,ij )B .
Let 0 = 0i1 ./ . . . ./ 0im = i1 ./ . . . ./ SSim . Then, 0 0 , exists
2
V (k, ij )B .
0 = |Vinput (1,m) Vinput (`1 , `2 ) = `j=`
1
Proof prove induction number neighbors In(k).
Base case: In(k) = {i}, claim trivially holds.
Induction case: In(k) = {i1 , . . . , i` }, U 0 = 0i1 ./ . . . ./ 0i`1 , U = i1 ./ . . . ./ i`1 ,
U 0 U 0 , exists U U U 0 = U |Vinput(1,`1) . need show
0 U 0 ./ 0i` , exists U ./ i` 0 = |Vinput (1,`) .
Assume opposite holds, i.e., exists = U 0 ./ 0 U 0 U 0 0 0i` ,
U U, i` U 0 = U |Vinput (1,`1) 0 = S|V (k,i` )B ,
U ./ void.
means exists context Ct reachable Ck two different ways, one via i`
via one i1 , . . . , i`1 Ut 6= , St 6= , Ut 6= St , either
(i) Ut0 = St0 = ,
(ii) Ut0 = St0 6=
Case (i) cannot happen Ct reachable Ck , hence Vinput (1, ` 1) Bt 6=
V (k, i` ) Bt 6= .
Concerning case (ii), Ut |Vinput (1,`1) = St |V (k,i` ) 6= , hence exists
Ut \ Ut |Vinput (1,`1)
/ St |V (k,i` ) . means Vinput (1, ` 1) Bt 6= V (k, i` ) Bt .


However, Definition 9 recursive import interface, V (k, ix )B = V (k)
`B|k B` , B|ix contains nodes B reachable ix . follows V (k, i` )

V (k, ij ) 1 j ` 1 reaches t, share projection Bt , hence Vinput (1, `
1) Bt = V (k, i` ) Bt .
reach contradiction, therefore Lemma 12 proved.


Lemma 13 join operator ./ following properties, given arbitrary belief states S, , U
size: (i) ./ = (ii) ./ = ./ (iii) ./ (T ./ U ) = (S ./ ) ./ U .
properties hold sets belief states.
Proof first two properties trivial prove. prove associativity.
Let R = ./ (T ./ U ) W = (S ./ ) ./ U . Consider joins left right.
position (1 n), Ri Wi determined locally comparing Si , Ti Ui .
591

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

Si =




Ti =


N

Ui =

N




N

N

N





N



N

N

N



N

N

N

Si = Ti


N
N
N
N
N
N

N


N
N

Ti = Ui

N
N

N

N
N
N
N

N

N

Ui = Si

N

N
N
N

N
N
N

N
N
N

Ri

Ui
Ti
Ti
void
Si
Si
void
Si
void
Si
void
void
void

Wi

Ui
Ti
Ti
void
Si
Si
void
Si
void
Si
void
void
void

Table 1: Possible cases joining position

reach inconsistency, process terminates void returned; otherwise, conclude value
Ri , Wi continue next position. final join returned position n processed
without inconsistency.
possible combination Si , Ti , Wi shown Table 1. One see always
outcome Ri Wi . Therefore, end either R = W
void . concludes join operator ./ commutative.

Lemma 14 Let Ci Cj two contexts block executing
OptimizeTree directed path Ci Cj . Suppose = DMCSOPT(k) Ci
j = DMCSOPT(k) Cj . = ./ j .

Proof use cache DMCSOPT change result disregarded, i.e.,
assume without loss generality cache(k) = DMCSOPT. Indeed, cache(k) filled
result computation empty (i.e., Ck accessed first time),
never changed DMCSOPT returns cache(k), i.e., value computation
empty cache(k).
assumption, Lemma 14 proven taking path Ci = Cp1 , . . . , Cph =
Cj connects Ci Cj , arguing index ` {1, . . . , h}, holds p` = p` ./
j (?). Indeed, show induction path.
Base case: ` = h, statement (?) holds ph ./ j = j ./ j = j identity
(Lemma (13), (i)).
Induction case: consider ` < h, suppose already established induction hypothesis
p`+1 = p`+1 ./ j .
592

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

definition p` DMCSOPT, holds p` = lsolve(T )11 is,
statements (b) (c), form = p`+1 ./ 0 ; holds edge (p` , p`+1 )
E, ./ commutative associative (Lemma (13), (ii) (iii)). induction
hypothesis, get
= p`+1 ./ 0 = (S p`+1 ./ j ) ./ 0 = j ./ (S p`+1 ./ 0 ),
is, form j ./ 00 .
Next, lsolve(T ) change value component interpretation
defined j ; is, lsolve(T ) ./ j = lsolve(T ). means p` = lsolve(T ) = lsolve(T ) ./
j = p` ./ j , proves statement (?) holds `.
Eventually, get ` = 1 = p1 = p1 ./ j = ./ j .

Based Lemma 14, following result.
Lemma 15 Assume import neighborhood context Ck In(k) = {i1 , . . . , im },
ij = DMCSOPT(k) Cij , 1 j m. Furthermore, suppose edge (k, ij ) removed
optimization process (1 j m), Ci` neighbor Ck exists path
k ij i` optimized topology. i` = i` ./ ij ; words, input
DMCSOPT Ck affected removal (k, ij ).
Proof Since Cij Ci` direct children Ck , follows belong block.
Therefore, Lemma 14 i` = i` ./ ij .

Proof (Proposition 11) proceed structural induction block tree MCS . First,
consider case topology single block B. case, interface passed
DMCS V = VB .
Base case: Ck leaf. compare call DMCSOPT(k) Ck Ck .DMCS(V, ),
V = V (k)B = Bk . Algorithm 1 returns local belief sets Ck projected V Algorithm 5 returns plain local belief sets, claim follows V = V (k)B = Bk .
Induction case: Assume import neighborhood context Ck In(k) = {i1 , . . . , im },
0i1

= DMCSOPT(k) Ci1
..
.

i1

= Ci1 .DMCS(VB , )
..
.

0im

= DMCSOPT(k) Cim

im

= Cim .DMCS(VB , )

every partial equilibrium 0 0ij , exists ij 0 = S|V (k,ij )B .
two cases. First, edge (k, ij ) removed optimization procedure. Then,
Lemma 12, correspondence input DMCSOPT DMCS Ck .
hand, assume edge (k, ij ) removed optimization process.
removal either transitive reduction ear decomposition. former case, Lemma 15
shows input Ck affected removal edge. latter case, removal
one three possibilities illustrated Figure 20, assuming context C1 gets called:
(i) (6, 1), last edge simple cycle P0 = {1, 2, 3, 4, 5, 6}
11. abuse notation, write lsolve(T )





lsolve(T )

593

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

1

12
11

10

9

2

6

3

5
4

8

7

Figure 20: Possible cycle breakings
(ii) (9, 6), last edge path P1 = {5, 7, 8, 9, 6}
(iii) (12, 2), last edge path P2 = {3, 10, 11, 12}
Cases (i) (iii) differ case (ii) sense cycle recognized DMCS
case (ii), cycle detected along corresponding path.
Now, consider (k, ij ) removed situations similar cases (i) (iii), DMCSOPT
issue guess Step (c) Algorithm 5 v(k, ij ), includes V (cB )|Bij = VB Bij .
hand, DMCS recognize cycle Cij issue guess VB Bij Step (c)
Algorithm 1. Therefore, guess fed equally Ck .
(k, ij ) removed situations similar case (ii), guesses Ck interface
Cij eventually filtered combined local belief states computed Cij ,
starting node path containing (k, ij ) last edge (in ear decomposition).
Figure 20, node 5.
cases, whenever input 0 lsolve DMCSOPT(k) called
Cc , input lsolve Ck .DMCS(VB , ). Therefore, claim output holds.
Proposition 11 holds single leaf block, one see upper blocks
need import interface
beliefs cut vertices (also root contexts lower blocks).
setting V = Bk VB , results DMCSOPT DMCS projected interface
cut vertices identical. Therefore, upper blocks receive input regarding
interfaces cut vertices running algorithms. therefore final results projected
V (k)B end same.


Proof Proposition 8
Note components Handler Output simply take care communication part
DMCS-STREAMING. Output makes sure models sent back invokers correspondence request Handler got. routines Joiner Solver main
components play role Step (b) (d) Algorithm 5, respectively.
594

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

T1,1

T1,1
T1,1

T1,1

./

T2,1

./



./

./

T2,1

./



./

./

T2,1

./



./

./

T2,1

./



./


T1,1

T1,1

T1,1

./

T2,1

./



./

./

T2,1

./



./

./

T2,p2

./



./

Tm,1



./

Tm,pm



./

Tm,1



./

Tm,pm



Tm1,pm1

Tm1,pm1


./

Tm,1



./

Tm,pm



Tm1,1

./

Tm,1



./

Tm,pm



./

Tm,1




./

T2,p2

./



./


T1,p1

Tm1,2

Tm1,2

./




T1,1

Tm1,1

Tm1,1

Tm1,pm1


./

T2,1

./



./



Tm1,p1


T1,p1

./

T2,p2

./



./

Tm1,pm1

./

Tm,pm

T1,1

T1,1

T1,1

T1,1

T1,p1

./

T2,1

./



./

Tm1,1


./

F (m, m)



./

T2,1

./



./

./

F (m, m)



./

T2,p2

./



./

./

F (m, m)



./

T2,p2

./



./

./

F (m, m)



./

T2,p2

./



./

Tm1,pm1

Tm1,1

Tm1,pm1

Tm1,pm1

./

F (m, m)

T1,1

T1,1

T1,p1

./

T2,1

./



./



./

T2,p2

./



./

./

T2,p2

./



./

F (m 1, m)

F (m 1, m)

F (m 1, m)

=

[T1,1 ./ F (2, m)]







[T1,p1 ./ F (2, m)]

=

F (1, m).

=

=



Table 2: Accumulation Joiner

prove correctness DMCS-STREAMING, need show input lsolve
complete sense Step (e) Algorithm 8 exhaustively executed, full join
partial equilibria neighboring contexts delivered.
595

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

Formally, assume current contexts import neighborhood {1, 2, . . . , m}. Assume
neighbor Ci 1 m, full partial equilibria Ti returned packages size
k denoted Ti,1 , . . . , Ti,pi , is, Ti = Ti,1 . . . Ti,pi . correctness algorithm,
assume Ti,1 , . . . , Ti,pi fixed partition Ti . possible when, example, lsolve
always returns answers fixed order. need show accumulation join
Algorithm 8 actually T1 ./ . . . ./ Tm .
Indeed, possible join T1,i1 ./ T2,i2 ./ . . . ./ Tm,im considered Joiner, performs
lexicographical traversal suitable combinations. Formally speaking, let F (p, q), q <
q, denote join result neighbors p q, is, F (p, q) = Tp ./ Tp+1 ./
.1 . . ./ Tq .
According lexicographical order, accumulation Joiner pj=1
[T1,j ./
F (2, m)] = F (1, m) demonstrated Table 2.
shows input lsolve complete. Hence, DMCS-STREAMING correct.


Appendix B. Detailed Run OptimizeTree
Example 20 illustrate call OptimizeTree(T = (B C, E), cp , cr ) block set B =
{B1 , B2 , B3 }, B1 = {1, 2, 3, 4}, B2 = {4, 5}, B3 = {3, 6}, C = {1, 3, 4}, E = {(B1 , 1), (B2 , 4),
(B3 , 3)}, cp = cr = 1.
local knowledge bases presented Example 10, have:
B1 = {car 1 , train 1 , nuts 1 }
B2 = {car 2 , train 2 }
B3 = {car 3 , train 3 , salad 3 , peanuts 3 , coke 3 , juice 3 , urgent 3 }

B4 = {car 4 , train 4 }
B5 = {soon5 , sooner 5 }
B6 = {fit 6 , sick 6 }

Since cp = cr , start B 0 = {B1 }. F = v = .
call OptimizeBlock(B1 , 1). Since B1 acyclic, transitive reduction applied.
get B1 = ({1, 2, 3, 4}, {(1, 2), (2, 3), (3, 4)}). subroutine returns E = {(1, 3), (2, 4)}.
child cut vertices B1 C 0 = {3, 4}; update F {(1, 3), (2, 4)}.
Next, update label edges (i, j) B1 . this, let us enumerate recursive
import interfaces, starting import interface, every node 1 6:
V(1) = {train 2 , train 3 , peanuts 3 }
V(2) = {car 3 , coke 3 , train 3 , car 4 , train 4 }
V (1)
V (2)
V (3)
V (4)
V (5)
V (6)

V(3) = {train 4 , sick 6 }
V(5) = {train 4 }

V(4) = {sooner 5 }
V(6) =

{train 2 , train 3 , peanuts 3 , car 3 , coke 3 , car 4 , train 4 , sooner 5 , sick 6 }
{train 3 , car 3 , coke 3 , train 4 , car 4 , sooner 5 , sick 6 }
{train 4 , sooner 5 , sick 6 }
{train 4 , sooner 5 }
{train 4 , sooner 5 }


Now, let us compute V (1, 2)B1 = V (1) `B1 |2 B` . B1 |2 = {3, 4}, thus
=
=
=
=
=
=

V (1, 2)B1 = V (1) (B3 B4 ) = {train 2 , train 3 , peanuts 3 , car 3 , coke 3 , car 4 , train 4 }
Similarly, B1 |3 = B1 |4 = {4}, have:
V (2, 3) = V (2) B3 = {car 4 , train 4 }
V (3, 4) = V (3) B4 = {train 4 }
596

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

removed edges updated labels stored respectively F v block B1
summarized as:
F = {(1, 3), (2, 4)}
v(1, 2) =

V (1, 2)



V (1)|B3



V (1)|B4

v(2, 3) = V (2, 3) V (1)|B3 V (1)|B4
v(3, 4) = V (3, 4) V (1)|B3 V (1)|B4




train 2 , train 3 , peanuts 3 ,
=
car 3 , coke 3 , car 4 , train 4
= {train 3 , peanuts 3 , car 3 , coke 3 , car 4 , train 4 }
= {train 3 , peanuts 3 , car 3 , coke 3 , car 4 , train 4 }

Next, call OptimizeTree(T \ B1 , 3, 1) OptimizeTree(T \ B1 , 4, 1), eventually process
blocks B2 B3 manner above. two calls respectively return:
F 0 = {(5, 4)}
v 0 (4, 5) = {sooner 5 }

F 00 =
v 00 (3, 6) = {train 4 , sick 6 }

Combining results together, OptimizeTree(T, 1, 1) returns set removed edges
F = {(1, 2), (3, 4), (5, 4)}
updated labels v remaining edges blocks
v(1, 2)
v(2, 3)
v(3, 4)
v(4, 5)
v(3, 6)

=
=
=
=
=

{train 2 , train 3 , peanuts 3 , car 3 , coke 3 , car 4 , train 4 }
{train 3 , peanuts 3 , car 3 , coke 3 , car 4 , train 4 }
{train 3 , peanuts 3 , car 3 , coke 3 , car 4 , train 4 }
{sooner 5 }
{train 4 , sick 6 }

References
Adjiman, P., Chatalic, P., Goasdoue, F., Rousset, M.-C., & Simon, L. (2006). Distributed reasoning
peer-to-peer setting: Application semantic web. J. Artif. Intell. Res., 25, 269314.
Aho, A. V., Garey, M. R., & Ullman, J. D. (1972). Transitive Reduction Directed Graph.
SIAM J. Comput., 1(2), 131137.
Analyti, A., Antoniou, G., & Damasio, C. V. (2011). MWeb: principled framework modular
web rule bases semantics. ACM Trans. Comput. Log., 12(2), 17.
Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.). (2003).
Description Logic Handbook. Cambridge University Press.
Baget, J.-F., & Tognetti, Y. S. (2001). Backtracking biconnected components constraint
graph. Nebel, B. (Ed.), Proceedings Seventeenth International Joint Conference
Artificial Intelligence, IJCAI 2001, Seattle, Washington, USA, August 4-10, 2001, pp. 291
296. Morgan Kaufmann.
Bairakdar, S. E.-D., Dao-Tran, M., Eiter, T., Fink, M., & Krennwallner, T. (2010a). Decomposition
distributed nonmonotonic multi-context systems. Janhunen, T., & Niemela, I. (Eds.),
Logics Artificial Intelligence - 12th European Conference, JELIA 2010, Helsinki, Finland,
September 13-15, 2010. Proceedings, Vol. 6341 Lecture Notes Computer Science, pp.
2437. Springer.
597

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

Bairakdar, S. E.-D., Dao-Tran, M., Eiter, T., Fink, M., & Krennwallner, T. (2010b). DMCS
solver distributed nonmonotonic multi-context systems. Janhunen, T., & Niemela, I.
(Eds.), Logics Artificial Intelligence - 12th European Conference, JELIA 2010, Helsinki,
Finland, September 13-15, 2010. Proceedings, Vol. 6341 Lecture Notes Computer Science, pp. 352355. Springer.
Bessiere, C., Bouyakhf, E., Mechqrane, Y., & Wahbi, M. (2011). Agile asynchronous backtracking
distributed constraint satisfaction problems. IEEE 23rd International Conference
Tools Artificial Intelligence, ICTAI 2011, Boca Raton, FL, USA, November 7-9, 2011,
pp. 777784.
Bikakis, A., & Antoniou, G. (2010). Defeasible contextual reasoning arguments ambient
intelligence. IEEE Transactions Knowledge Data Engineering, 22(11), 14921506.
Bikakis, A., Antoniou, G., & Hassapis, P. (2011). Strategies contextual reasoning conflicts
ambient intelligence. Knowl. Inf. Syst., 27(1), 4584.
Bogl, M., Eiter, T., Fink, M., & Schuller, P. (2010). MCS-IE system explaining inconsistency
multi-context systems. Logics Artificial Intelligence - 12th European Conference,
JELIA 2010, Helsinki, Finland, September 13-15, 2010. Proceedings, Vol. 6341 Lecture
Notes Computer Science, pp. 356359. Springer.
Bondy, A., & Murty, U. S. R. (2008). Graph Theory, Vol. 244 Graduate Texts Mathematics.
Springer.
Brewka, G., Eiter, T., Fink, M., & Weinzierl, A. (2011). Managed multi-context systems. Walsh,
T. (Ed.), Proceedings 22nd International Joint Conference Artificial Intelligence
(IJCAI-11), pp. 786791. AAAI Press/IJCAI.
Brewka, G., Ellmauthaler, S., & Puhrer, J. (2014). Multi-context systems reactive reasoning
dynamic environments. Ellmauthaler, S., & Puhrer, J. (Eds.), Proceedings International Workshop Reactive Concepts Knowledge Representation (ReactKnow) 2014, pp.
2330. Tech.Rep. 1, Computer Science Institute, Univ. Leipzig, ISSN 1430-3701.
Brewka, G., & Eiter, T. (2007). Equilibria heterogeneous nonmonotonic multi-context systems.
Proceedings Twenty-Second AAAI Conference Artificial Intelligence, July 22-26,
2007, Vancouver, British Columbia, Canada, pp. 385390. AAAI Press.
Brewka, G., Eiter, T., & Fink, M. (2011). Nonmonotonic Multi-Context Systems: Flexible Approach Integrating Heterogeneous Knowledge Sources. Balduccini, M., & Son, T. C.
(Eds.), Logic Programming, Knowledge Representation, Nonmonotonic Reasoning - Essays Dedicated Michael Gelfond Occasion 65th Birthday, Vol. 6565 Lecture Notes Computer Science, pp. 233258. Springer.
Brewka, G., Roelofsen, F., & Serafini, L. (2007). Contextual default reasoning. Veloso, M. M.
(Ed.), IJCAI 2007, Proceedings 20th International Joint Conference Artificial Intelligence, Hyderabad, India, January 6-12, 2007, pp. 268273.
Buccafurri, F., & Caminiti, G. (2008). Logic programming social features. Theory Practice
Logic Programming, 8(5-6), 643690.
Dao-Tran, M. (2014). Distributed Nonmonotonic Multi-Context Systems: Algorithms Efficient
Evaluation. Ph.D. thesis, Faculty Informatics, Vienna University Technology, Austria.
598

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

Dao-Tran, M., Eiter, T., Fink, M., & Krennwallner, T. (2010). Distributed nonmonotonic multicontext systems. Lin, F., Sattler, U., & Truszczynski, M. (Eds.), Principles Knowledge Representation Reasoning: Proceedings Twelfth International Conference,
KR 2010, Toronto, Ontario, Canada, May 9-13, 2010. AAAI Press.
Dao-Tran, M., Eiter, T., Fink, M., & Krennwallner, T. (2011). Model streaming distributed multicontext systems. Mileo, A., & Fink, M. (Eds.), 2nd International Workshop Logicbased Interpretation Context: Modeling Applications, Vol. 738 CEUR Workshop
Proceedings, pp. 1122.
Eiter, T., Fink, M., Ianni, G., Krennwallner, T., & Schuller, P. (2011). Pushing efficient evaluation
hex programs modular decomposition. Delgrande, J. P., & Faber, W. (Eds.), 11th International Conference Logic Programming Nonmonotonic Reasoning (LPNMR 2011),
Vancouver, BC, Canada, May 16-19, 2011, Vol. 6645 Lecture Notes Computer Science,
pp. 93106. Springer.
Eiter, T., Ianni, G., Schindlauer, R., & Tompits, H. (2005). uniform integration higher-order
reasoning external evaluations answer-set programming. IJCAI, pp. 9096.
Faltings, B., & Yokoo, M. (2005). Introduction: Special issue distributed constraint satisfaction.
Artif. Intell., 161(1-2), 15.
Fink, M., Ghionna, L., & Weinzierl, A. (2011). Relational information exchange aggregation
multi-context systems. Delgrande, J. P., & Faber, W. (Eds.), 11th International Conference Logic Programming Nonmonotonic Reasoning (LPNMR 2011), Vancouver, BC,
Canada, 16-19 May, 2011, Vol. 6645 Lecture Notes Computer Science, pp. 120133.
Springer.
Gao, J., Sun, J., & Zhang, Y. (2007). improved concurrent search algorithm distributed CSPs.
Australian Conference Artificial Intelligence, pp. 181190.
Gelfond, M., & Lifschitz, V. (1991). Classical negation logic programs disjunctive databases.
New Generation Comput., 9(3/4), 365386.
Ghidini, C., & Giunchiglia, F. (2001).
Local models semantics, contextual reasoning=locality+compatibility. Artif. Intell., 127(2), 221259.
Giunchiglia, F. (1992). Contextual Reasoning. Epistemologia, Special Issue Linguaggi e le
Macchine, 345, 345364.
Giunchiglia, F., & Serafini, L. (1994). Multilanguage hierarchical logics or: without
modal logics. Artif. Intell., 65(1), 2970.
Goncalves, R., Knorr, M., & Leite, J. (2014). Evolving multi-context systems. Schaub, T.,
Friedrich, G., & OSullivan, B. (Eds.), Proceedings 21st Eureopean Conference
Artificial Intelligence, ECAI2014, Prague, Czech Republic, August 18-23, 2014. IOS Press.
Hirayama, K., & Yokoo, M. (2005). distributed breakout algorithms. Artif. Intell., 161(12),
89115.
Homola, M. (2010). Semantic Investigations Distributed Ontologies. Ph.D. thesis, Comenius
University, Bratislava, Slovakia.
599

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

Lee, J., & Lifschitz, V. (2003). Loop formulas disjunctive logic programs. Palamidessi, C.
(Ed.), Logic Programming, 19th International Conference, ICLP 2003, Mumbai, India, December 9-13, 2003, Proceedings, Lecture Notes Computer Science, pp. 451465. Springer.
McCarthy, J. (1993). Notes formalizing context. Bajcsy, R. (Ed.), Proceedings 13th
International Joint Conference Artificial Intelligence. Chambery, France, August 28 September 3, 1993, pp. 555562. Morgan Kaufmann.
Pontelli, E., Son, T., & Nguyen, N.-H. (2011). Combining answer set programming prolog:
ASP-PROLOG system. Balduccini, M., & Son, T. (Eds.), Logic Programming, Knowledge
Representation, Nonmonotonic Reasoning, Vol. 6565, pp. 452472. Springer Berlin Heidelberg.
Reiter, R. (1980). logic default reasoning. Artificial Intelligence, 13, 81132.
Roelofsen, F., Serafini, L., & Cimatti, A. (2004). Many hands make light work: Localized satisfiability multi-context systems. de Mantaras, R. L., & Saitta, L. (Eds.), Proceedings
16th Eureopean Conference Artificial Intelligence, ECAI2004, including Prestigious
Applicants Intelligent Systems, PAIS 2004, Valencia, Spain, August 22-27, 2004, pp. 5862.
IOS Press.
Serafini, L., Borgida, A., & Tamilin, A. (2005). Aspects distributed modular ontology reasoning. Nineteenth International Joint Conference Artificial Intelligence (IJCAI 2005),
pp. 570575. AAAI Press.
Serafini, L., & Tamilin, A. (2005). Drago: Distributed reasoning architecture semantic web.
Gomez-Perez, A., & Euzenat, J. (Eds.), Semantic Web: Research Applications,
Second European Semantic Web Conference, ESWC 2005, Heraklion, Crete, Greece, May 29
- June 1, 2005, Proceedings, Lecture Notes Computer Science, pp. 361376. Springer.
Tarjan, R. E. (1972). Depth-First Search Linear Graph Algorithms. SIAM J. Comput., 1(2),
146160.
Tasharrofi, S., & Ternovska, E. (2014). Generalized multi-context systems.. Baral, C., Giacomo,
G. D., & Eiter, T. (Eds.), Principles Knowledge Representation Reasoning: Proceedings Fourteenth International Conference, KR 2014, Vienna, Austria, July 20-24, 2014.
AAAI Press.
Valdes, J., Tarjan, R. E., & Lawler, E. L. (1982). recognition series parallel digraphs. SIAM
J. Comput., 11(2), 298313.
Vats, D., & Moura, J. M. F. (2010). Graphical models block-tree graphs. CoRR, abs/1007.0563.
Velikova, M., Novak, P., Huijbrechts, B., Laarhuis, J., Hoeksma, J., & Michels, S. (2014).
Integrated Reconfigurable System Maritime Situational Awareness. ECAI 2014 - 21st
European Conference Artificial Intelligence, 18-22 August 2014, Prague, Czech Republic
- Including Prestigious Applications Intelligent Systems (PAIS 2014), pp. 11971202.
Yokoo, M., & Hirayama, K. (2000). Algorithms distributed constraint satisfaction: review.
Autonomous Agents Multi-Agent Systems, 3(2), 185207.

600


