Journal Artificial Intelligence Research 52 (2015) 1-95

Submitted 07/14; published 01/15

Coherent Predictive Inference Exchangeability
Imprecise Probabilities
Gert de Cooman
Jasper De Bock

gert.decooman@UGent.be
jasper.debock@UGent.be

Ghent University, SYSTeMS Research Group
TechnologieparkZwijnaarde 914
9052 Zwijnaarde, Belgium

Mrcio Alves Diniz

marcio.alves.diniz@gmail.com

Federal University Carlos, Department Statistics
Rod. Washington Luis, km 235
Carlos, Brazil

Abstract
Coherent reasoning uncertainty represented general manner
coherent sets desirable gambles. context allow indecision, leads
approach mathematically equivalent working coherent conditional
probabilities. allow indecision, leads general foundation coherent
(imprecise-)probabilistic inference. framework, given finite category set,
coherent predictive inference exchangeability represented using Bernstein
coherent cones multivariate polynomials simplex generated category set.
powerful generalisation de Finettis Representation Theorem allowing
imprecision indecision.
define inference system map associates Bernstein coherent cone
polynomials every finite category set. Many inference principles encountered
literature interpreted, represented mathematically, restrictions
maps. discuss, particular examples, two important inference principles: representation
insensitivitya strengthened version Walleys representation invarianceand specificity.
show infinity inference systems satisfy two principles,
amongst discuss particular skeptically cautious inference system, inference
systems corresponding (a modified version of) Walley Bernards Imprecise Dirichlet
Multinomial Models (IDMM), skeptical IDMM inference systems, Haldane
inference system. prove latter produces posterior inferences
would obtained using Haldanes improper prior, implying infinity
proper priors produce coherent posterior inferences Haldanes improper one.
Finally, impose additional inference principle allows us characterise uniquely
immediate predictions IDMM inference systems.

1. Introduction
paper deals predictive inference categorical variables. therefore concerned
(possibly infinite) sequence variables Xn assume values finite set
categories A. observed number n them, found that, say X1 = x1 ,
X2 = x2 , . . . , Xn = xn , consider subjects belief model next n variables
Xn+1 , . . . Xn+n . probabilistic traditionand want build tradition
2015 AI Access Foundation. rights reserved.

fiDe Cooman, De Bock, & Diniz

context paperthis belief modelled conditional predictive probability
mass function pn (|x1 , . . . , xn ) set possible values. probability mass
functions used prediction estimation, statistical inferences, decision
making involving uncertain values variables. sense, predictive inference lies
heart statistics, generally, learning uncertainty. reason,
crucial importance dealing uncertainty Artificial Intelligence,
instance, intelligent systems learn multinomial probabilities, Markov
transition probabilities, rates occurrence phenomena, local probabilities Bayesian
credal networks on. refer synthesis Geisser (1993) collection
essays Zabell (2005) good introductions predictive inference underlying
issues present paper concerned with.
connects predictive probability mass functions various values n, n
(x1 , . . . , xn ) requirements time consistency coherence. former requires
n1 n2 , pn1 (|x1 , . . . , xn ) obtained pn2 (|x1 , . . . , xn )
usual marginalisation procedure; latter essentially demands conditional
probability mass functions connected time-consistent unconditional probability
mass functions Bayess Rule.
common assumption variables Xn exchangeable, meaning
roughly subject believes order observed, present
themselves, influence decisions inferences make regarding
variables. assumption, analysis consequences, goes back de Finetti
(1937) (see Cifarelli & Regazzini, 1996). famous Representation Theorem states,
essence, time-consistent coherent conditional unconditional predictive
probability mass functions associated countably infinite exchangeable sequence
variables completely characterised by1 completely characterisea unique
probability measure Borel sets simplex probability mass functions A,
called representation.2
leads us central problem predictive inference: since infinity
probability measures simplex, one subject choose particular
context, given choice motivated justified? subjectivists de
Finettis persuasion might answer question needs answer: subjects personal
predictive probabilities entirely his, time consistency coherence
requirements heed. Earlier scholars, Laplace Bayes, would
call subjectivists, invoked Principle Indifference justify using specific class
predictive mass functions. Proponents logicist approach predictive inference would
try enunciating general inference principles order narrow down, hopefully eliminate
entirely, possible choices representing probability measures simplex.
logicians W. E. Johnson (1924) and, much systematic fashion, Rudolf Carnap (1952)
1. . . . unless observed sequence probability zero.
2. Actually, order clarify connection shall later on, essence de Finettis
argument representation coherent prevision set multinomial polynomialsor
equivalently, continuous real functionson simplex (De Cooman, Quaeghebeur, & Miranda,
2009b). (finitely additive) coherent prevision, extended uniquely far set
lower semicontinuous functions, determine unique (countably additive) probability
measure Borel sets simplex, F. Riesz Representation Theorem (De Cooman &
Miranda, 2008a; Troffaes & De Cooman, 2014).

2

fiCoherent Predictive Inference Exchangeability

tried develop axiom system predictive inference based reasonable inference
principles. Carnaps first group axioms related called coherence,
suggested, weak single particular predictive
model. second group consisted invariance axioms, including exchangeability.
included axiom instantial relevance, translating intuitive principle predictive
inferences actually learn experience. last axiom, predictive irrelevance,
proposed earlier Johnson called sufficientness postulate Good (1965).
Armed axioms, Carnap able derive continuum probabilistic inference
rules, closely related Dirichlet multinomial model Imprecise Dirichlet
Multinomial Model (IDMM) proposed Walley (1996) Walley Bernard (1999),
discuss Appendices C D, respectively.
point view holds middle ground subjectivist logicist positions:
possible subject make assessments certain predictive probabilities,
combine certain inference principles finds reasonable, suit
purpose problem hand. Indeed, inference systems introduce discuss
Section 6, notion conservative coherent inferenceor natural extensionwe
associate them, provide elegant framework tools making conservative coherent
predictive inferences combine (local) subjective probability assessments (general)
inference principles. work Section 15 characterising immediate predictions
IDMM constitutes exercise inor example forprecisely that.
idea conservative probabilistic inference brings us believe
main contribution paper. central idea de Finettis (1975) approach
probabilitybut course implicit Markov Chebyshev inequalitiesthat
subject makes probability assessments, consider bounds so-called
precise probability models. Calculating conservative tightest bounds indeed
de Finettis (1975) Fundamental Theorem Prevision (see Lad, 1996) about.
theory imprecise probabilities, brought synthesis Williams (1976) Walley
(1991, 2000), going back Boole (1952) Keynes (1921), crucial contributions
quite number statisticians philosophers (Smith, 1961; Levi, 1980; Seidenfeld,
Schervish, & Kadane, 1999), looks conservative probabilistic inference precisely
way: calculate efficiently possible consequencesin sense
conservative tightest boundsof making certain probability assessments. may local
assessments, inequalities imposed probabilities previsions certain events
variables, structural assessments, independence, exchangeability.
One advantage imprecise probability models allow imprecision,
words, use partial probability assessments using bounding inequalities rather
equalities. Another, related, advantage allow indecision modelled
explicitly: loosely stated, imposed bounds probabilities allow one
probability model solution, may well two actions, first
higher expected utility one compatible probability model, smaller another
compatible probability model, meaning neither action robustly preferred other.
current stated model beliefs, subject undecided
actions. Section 2, give concise overview relevant ideas, models techniques
field imprecise probabilities. much extensive detailed recent overview
area research published Augustin, Coolen, De Cooman, Troffaes (2014).
3

fiDe Cooman, De Bock, & Diniz

present paper, then, described application ideas imprecise probabilities predictive inference. aim studyand develop general framework
dealing withconservative coherent predictive inference using imprecise probability models.
Using models allow us represent subjects indecision, believe
natural state knowing, learned little, problem hand.
seems important theories learning uncertainty general, predictive
inference particular, least allow us (i) start conservative, imprecise
indecisive models little learned, (ii) become precise decisive
observations come in. shall see abstract notion inference system
introduce on, allows forbut necessarily forcesuch behaviour,
shall give number examples concrete inference systems display it.
work builds on, manages reach much than, earlier paper
one authors (De Cooman, Miranda, & Quaeghebeur, 2009a). One reason
so, earlier work deals immediate prediction models, shall
see on, predictive inference using imprecise probabilities completely determined
immediate prediction, contrary expect using precise probabilities.
main reason position use powerful mathematical
language represent imprecise-probabilistic inferences: Walleys (2000) coherent sets
desirable gambles. Earlier imprecise probability models (Boole, 1952, 1961; Koopman, 1940)
centred lower upper probability bounds eventsor propositions. Later (Walley,
1991, Section 2.7), became apparent language events lower upper
probabilities lacking power expression: much expressive theory uses random
variables lower previsions expectations. successful theory coherent lower
previsions quite well developed (Walley, 1991; Augustin et al., 2014; Troffaes &
De Cooman, 2014). faces number problems, mathematical well
conceptual complexity, especially dealing conditioning independence,
fact that, case many approaches probability, shall see
Section 2.5, issues conditioning sets (lower) probability zero.
attractive solution problems offered Walley (2000), form
coherent sets desirable gambles, inspired earlier ideas (Smith, 1961; Williams, 1975b;
Seidenfeld, Schervish, & Kadane, 1995). Here, primitive notions probabilities
events, expectations random variables. focus rather whether gamble,
risky transaction, desirable subjectstrictly preferred zero transaction,
status quo. basic belief model probability measure lower prevision,
set desirable gambles. course, stating gamble desirable leads
particular lower prevision assessment: provides lower bound zero prevision
gamble. explain prefer use sets desirable gambles basic uncertainty
models Section 2.
summary, then, aim paper use sets desirable gambles extend
existing probabilistic theory predictive inference. Let us explain detail
intend go this. basic building blocks introduced Sections 27.
already indicated above, give overview relevant notions results concerning
imprecise probability model choicecoherent sets desirable gamblesin Section 2.
particular, explain use conservative inference well conditioning;
4

fiCoherent Predictive Inference Exchangeability

derive commonly used models, lower previsions lower probabilities,
them; relate precise probability models.
Section 3, explain describe subjects beliefs sequence
variables terms predictive sets desirable gambles, derived notion predictive
lower previsions. imprecise probability models generalise above-mentioned predictive
probability mass functions pn (|x1 , . . . , xn ), constitute basic tools shall
working with. explain proper formulations above-mentioned
time consistency coherence requirements general context.
Section 4, discuss number inference principles believe could reasonably
imposed predictive inferences, show represent mathematically
terms predictive sets desirable gambles lower previsions. Pooling invarianceor
Walley (1996) called Representation Invariance Principle (RIP)and renaming
invariance seem reasonable requirements type predictive inference, category
permutation invariance seems natural thing require starting state
complete ignorance. Taken together, constitute call representation insensitivity.
means predictive inferences remain essentially unchanged transform
set categories, words essentially insensitive choice
representationthe category set. Another inference principle look imposes so-called
specificity property: predictive inference specific, certain type question
involving restricted number categories, general model replaced
specific model deals categories interest, produce
relevant inferences (Bernard, 1997).
next important step taken Section 5, recall literature (De
Cooman et al., 2009b; De Cooman & Quaeghebeur, 2012) deal exchangeability
predictive inference models imprecise. recall de Finettis Representation
Theorem significantly generalised. case, time-consistent coherent
predictive sets desirable gambles completely characterised set (multivariate)
polynomials simplex probability mass functions category set.3
set polynomials must satisfy number properties, taken together define
notion Bernstein coherence. Without becoming technical point, conclusion
section that, general context, precise-probabilistic notion
representing probability measure simplex probability mass functions replaced
Bernstein coherent set polynomials simplex. set polynomials serves
completely purpose representing probability measure: completely determines,
conveniently densely summarises, predictive inferences. reason
rest developments paper expressed terms Bernstein coherent
sets polynomials.
introduce coherent inference systems Section 6 maps associate
finite set categories Bernstein coherent set polynomials simplex probability
mass functions set. coherent inference system way fixing completely
coherent predictive inferences possible category sets. reasons introducing
coherent inference systems twofold. First all, inference principles Section 4 impose
connections predictive inferences different category sets, represent
3. contradistinction de Finettis version, version problems conditioning observed
sequences (lower) probability zero.

5

fiDe Cooman, De Bock, & Diniz

inference principles mathematically restrictions coherent inference systems,
main topic Section 7. Secondly, allows us extend method natural extensionor
conservative inferenceintroduced Section 2.2, take account principles
predictive inference, generally, predictive inference multiple category sets once.
leads method combining (local) predictive probability assessments (global)
inference principles produce conservative predictive inferences compatible
them.
first illustration power methodology, look immediate prediction
Section 8: implications representation insensitivity specificity
predictive inference single next observation? show approach allows us
streamline, simplify significantly extend previous attempts direction De
Cooman et al. (2009a).
material Sections 914 shows, producing explicit examples,
quite different typeseven uncountable infinitiesof coherent inference systems
representation insensitive and/or specific. discuss vacuous nearly vacuous
inference systems Sections 9 10, skeptically cautious inference system Section 11,
family IDMM inference systems Section 12, family skeptical IDMM inference
systems Section 13, Haldane inference system Section 14. inference
systems, apart IDMM, appear first time. Also, believe
first publish detailed explicitas well still elegantproof IDMM
inference systems indeed representation insensitive specific. already
mentioned here, however, IDMM inference systems based modified,
arguably better behaved, version models originally introduced Walley Bernard
(see Walley, 1996; Walley & Bernard, 1999; Bernard, 2005); refer Appendix
explanation, proof original IDMM specific that, contrary
often claimed, satisfy so-called nestedness property.
results disprove conjecture (Bernard, 2007; De Cooman et al., 2009a)
IDMM inference systemsour version original oneare ones, even
conservative ones, satisfy representation insensitivity specificity.
show Section 15 IDMM family immediate predictionswhich
version original oneare definite sense conservative ones
representation insensitive specific, satisfy another requirement,
called concave surprise.
conclusion (Section 16) point number surprising consequences
results, discuss avenues research.
order make paper self-contained possible, included number
appendices additional discussion. help reader find way many
notions notations need paper, Appendix provides list common
ones, short hint meaning, introduced. Appendix B provides
useful necessary background theory multivariate polynomials simplices,
important part Bernstein basis polynomials there. discussion IDMM
inference systems relies quite heavily Dirichlet densities simplices, expectation
operators associated them. discuss important relevant properties
Appendix C. Appendix contains discussion original IDM IDMM models,
proposed Walley Bernard (see Walley, 1991, 1996; Walley & Bernard, 1999; Bernard,
6

fiCoherent Predictive Inference Exchangeability

2005), show claims make model need
carefully formulated. stated above, main reason introducing,
Section 12, modified version IDMM models, suffer
shortcomings, produces immediate prediction models original version.
Finally, effort make lengthy paper readable possible, moved
proofs, additional technical discussion, Appendix E.

2. Imprecise Probability Models
section, give concise overview imprecise probability models representing,
making inferences decisions under, uncertainty. suggested Introduction,
shall focus sets desirable gambles uncertainty models choice.
Let us briefly summarise next section why, present paper, work
sets basic uncertainty models conservative probabilistic inference. reader
wants dispense motivation proceed Section 2.2, introduce
mathematics behind models. later sections, shall course briefly mention
derived results terms familiar language (lower) previsions probabilities.
2.1 Sets Desirable Gambles?
First all, number examples literature (Moral, 2005; Couso & Moral, 2011; De
Cooman & Quaeghebeur, 2012; De Cooman & Miranda, 2012) shown working
making inferences using models general expressive.
simpler elegant mathematical point view, intuitive
geometrical interpretation (Quaeghebeur, 2014). shall see Sections 2.4 3
marginalisation conditioning especially straightforward, issues
conditioning sets (lower) probability zero.
Also, become apparent discussion Section 2.2, explained
detail Moral Wilson (1995) De Cooman Miranda (2012),
similarity accepting gamble one hand accepting proposition true
other, gives logical flavour conservative probabilistic inference. Indeed,
strong analogy two, connects conservative probabilistic inferencealso
called natural extension fieldwith logical deduction: classical propositional
logic looking smallest deductively closed set contains number given
propositions, imprecise probabilities context looking smallest coherent set
desirable gambles contains number given gambles. context analogy,
precise probability models closely related complete, maximal, deductively closed
setsperfect information states. clear indication precise probability models
well suited dealing conservative inference, need
broader context imprecise probability models natural language setting
this. summary, working sets desirable gambles encompasses subsumes
special cases classical (or precise) probabilistic inference inference classical
propositional logic; see detailed discussion De Cooman Miranda (2012).
Finally, briefly explain Section 5, De Cooman Quaeghebeur (2012)
shown working sets coherent desirable gambles especially illuminating
context modelling exchangeability assessments: exposes simple geometrical meaning
7

fiDe Cooman, De Bock, & Diniz

notion exchangeability, leads simple particularly elegant proof
significant generalisation de Finettis (1937) Representation Theorem exchangeable
random variables.
summary, work sets desirable gambles powerful,
expressive general models hand, intuitive work withthough
unfortunately less familiar people closely involved field, and,
importantly, avoid problems conditioning sets (lower) probability
zero. details, refer work Walley (2000), Moral (2005), Couso Moral
(2011), De Cooman Quaeghebeur (2012), Quaeghebeur (2014).
2.2 Coherent Sets Desirable Gambles Natural Extension
consider variable X assumes values finite4 possibility space A. model
subjects beliefs value X looking gambles variable
subject finds desirable, meaning strictly prefers5 zero gamblethe status
quo. general approach, extends usual rationalist subjectivist
approach probabilistic modelling allow indecision imprecision.
gamble real-valued function f A. interpreted uncertain reward f (X)
depends value X, expressed units predetermined linear utility.
represents reward subject gets transaction first actual value x
X determined, subject receives amount utility f (x)which may
negative, meaning pay it. Throughout paper, use device writing f (X)
want make clear variable X gamble f depends on.
Events subsets possibility space A. event B associate
special gamble IB , called indicator, assumes value 1 B 0 elsewhere.
denote set gambles L(A). linear space point-wise
addition gambles, point-wise multiplication gambles real numbers.
subset L(A), posi(A) set positive linear combinations gambles A:
posi(A) :=

{
n

}
k fk : fk A, k R>0 , n N .

(1)

k=1

Here, N set natural numbers (without zero), R>0 set positive real
numbers. convex cone gambles subset L(A) closed positive linear
combinations, meaning posi(A) = A.
two gambles f g A, write f g (x A)f (x) g(x), f > g
f g f 6= g. gamble f > 0 called positive. gamble g 0 called non-positive.
4. sake simplicity, restrict discussion finite possibility spaces,
really need purposes paper. limited number remarks on, shall
occasion mention related notions infinite possibility spaces, give ample references
guide interested reader relevant literature.
5. want point notion strict preferenceor preference without indifferencecommonly
used preference modelling, confused Walleys (1991, Section 3.7.7) notion strict
desirability, one many ways construct lower prevision set gambles
strictly preferred zero gamble; see discussion near end Section 2.5.
details, refer recent paper Quaeghebeur, De Cooman, Hermans (2014).

8

fiCoherent Predictive Inference Exchangeability

L>0 (A) denotes convex cone positive gambles, L0 (A) convex cone
non-positive gambles.
collect gambles subject finds desirablestrictly prefers6 zero gamble
set desirable gambles, shall take sets basic uncertainty models.
course, satisfy certain rationality criteria:
Definition 1 (Coherence). set desirable gambles L(A) called coherent
satisfies following requirements:
D1. 0
/ D;
D2. L>0 (A) D;
D3. = posi(D).
D(A) denotes set coherent sets desirable gambles A.
Requirement D3 turns convex cone. Due D2, includes L>0 (A); D1D3,
avoids non-positivity:
D4. f 0 f
/ posi(D), equivalently L0 (A) posi(D) = .
L>0 (A) smallest coherent subset L(A). so-called vacuous model therefore
reflects minimal commitments part subject: knows absolutely nothing
likelihood different outcomes, strictly prefer zero
gambles never decrease wealth possibility increasing it.
D1 D2 , subject set desirable gambles D1 conservative, less
committal, subject set desirable gambles D2 , simply latter strictly
prefers zero gambles former does, possibly more. inclusion relation
imposes natural partial ordering sets desirable gambles, simple interpretation
least conservative as.
non-empty family coherent sets desirable gambles Di , I, intersection
iI Di still coherent. simple result underlies notion (conservative) coherent
inference. subject gives us assessmenta set L(A) gambles
finds desirablethen tells us exactly assessment extended coherent
set desirable gambles, construct smallestand therefore least committal
conservativesuch set:
Theorem 2 (Natural Extension, De Cooman & Quaeghebeur, 2012). Let L(A),
define natural extension by:7

EA :=
{D D(A) : D} .
following statements equivalent:
(i) avoids non-positivity: L0 (A) posi(A) = ;
(ii) included coherent set desirable gambles;
6. See footnote 5.

7. usual, expression, let = L(A).

9

fiDe Cooman, De Bock, & Diniz

(iii) EA 6= L(A);
(iv) set desirable gambles EA coherent;
(v) EA smallest coherent set desirable gambles includes A.
(and hence all) equivalent statements holds, EA = posi(L>0 (A) A).
Moreover, coherent 6= L(A) EA = A.
2.3 Maximal Coherent Sets Desirable Gambles
element D(A) called maximal strictly included element
D(A), words, adding gamble f makes sure longer extend
set {f } set still coherent:
(D0 D(A))(D D0 = D0 ).
M(A) denotes set maximal elements D(A). coherent set desirable gambles
maximal non-zero gambles f A, f
/ f (see Couso &
Moral, 2011 case finite A, De Cooman & Quaeghebeur, 2012 infinite
case). Coherence natural extension described completely terms maximal
elements:
Theorem 3 (Couso & Moral, 2011; De Cooman & Quaeghebeur, 2012). set avoids
non-positivity
maximal M(A) D. Moreover,

EA = {D M(A) : D}.
2.4 Conditioning Sets Desirable Gambles
Let us suppose subject coherent set desirable gambles A, expressing
beliefs value variable X assumes A. ask so-called
updated set DcB desirable gambles B would be, receive additional
informationand nothing morethat X actually belongs subset B A.
updating, conditioning, rule sets desirable gambles states that:
g DcB gIB gambles g B.

(2)

states gamble g desirable subject observe X B
called-off gamble gIB desirable him. called-off gamble gIB
gamble variable X gives zero rewardis called offunless X B,
case reduces gamble g new possibility space B. updated set DcB
set desirable gambles B still coherent, provided (De Cooman
& Quaeghebeur, 2012). See discussions Moral (2005), Couso Moral (2011), De
Cooman Quaeghebeur (2012), De Cooman Miranda (2012) Quaeghebeur (2014)
detailed information updating sets desirable gambles.
2.5 Coherent Lower Previsions
use coherent sets desirable gambles introduce derived concepts, coherent
lower previsions, probabilities.
10

fiCoherent Predictive Inference Exchangeability

Given coherent set desirable gambles D, functional P defined L(A)
P (f ) := sup { R : f D} f L(A),

(3)

coherent lower prevision (Walley, 1991, Thm. 3.8.1). means lower
envelope expectations associated set probability mass functions,8 or,
equivalently, satisfies following coherence properties (Walley, 1991, 2000; De
Cooman & Quaeghebeur, 2012; Miranda & De Cooman, 2014; Troffaes & De Cooman, 2014):
P1. P (f ) min f gambles f A;
P2. P (f + g) P (f ) + P (g) gambles f, g A;
P3. P (f ) = P (f ) gambles f real 0.
used notation min f := min {f (x) : x A}; max f defined similarly.
conjugate upper prevision P defined P (f ) := inf { R : f D} = P (f ).
following properties implied P1P3:
P4. max f P (f ) P (f ) min f gambles f A;
P5. P (f + ) = P (f ) + P (f + ) = P (f ) + gambles f R.
gamble f , P (f ) called lower prevision f , follows Equation (3)
interpreted subjects supremum desirable price buying gamble f .
event B, P (IB ) denoted P (B), called lower probability B;
interpreted subjects supremum desirable rate betting B. Similarly
upper previsions upper probabilities.
lower prevision associated vacuous set desirable gambles L>0 (A) given
P (f ) = min f . called vacuous lower prevision, point-wise smallest,
conservative, coherent lower previsions.
coherent conditional model DcB, B non-empty subset A, induces conditional lower prevision P (|B) L(B), invoking Equation (3):
P (g|B) := sup { R : g DcB} = sup { R : [g ]IB D}
gambles g B. (4)
difficult show (Walley, 1991) P P (|B) related following
coherence condition:
P ([g P (g|B)]IB ) = 0 g L(B),
(GBR)
called Generalised Bayes Rule. rule allows us infer P (|B) uniquely P ,
provided P (B) > 0. Otherwise, usually infinity coherent lower previsions
P (|B) coherent P sense satisfy (GBR), equivalently,
coherent set desirable gambles leads P P (|B). Two
8. statement valid working finite A. infinite A, similar results shown
hold (Walley, 1991; De Cooman & Quaeghebeur, 2012; Miranda & De Cooman, 2014; Troffaes &
De Cooman, 2014), expectations involved coherent previsionsexpectation operators
associated finitely additive probability measures. See discussion Section 2.6.

11

fiDe Cooman, De Bock, & Diniz

particular conditioning rules, namely natural regular extension (Walley, 1991; Miranda
& De Cooman, 2014), always produce conditional lower previsions satisfy GBR,
therefore coherent P . P (B) > 0but necessarily P (B) = 0!they
always produce point-wise smallest largest coherent conditional lower previsions,
respectively (Miranda, 2009; Miranda & De Cooman, 2014).9
Many different coherent sets desirable gambles lead coherent lower prevision
P , typically differ boundaries. sense, coherent sets desirable
gambles informative coherent lower previsions: gamble positive lower
prevision always desirable one negative lower prevision never, gamble
zero lower prevision lies border set desirable gambles, lower
prevision generally provide information desirability gambles.
border behaviour importantand dealing conditioning events
zero (lower) probability (Walley, 2000; Moral, 2005; Couso & Moral, 2011; Quaeghebeur,
2014)it useful work sets desirable gambles rather lower previsions,
Equations (2) (4) tell us, allow us derive unique conditional models
unconditional ones: coherent set desirable gambles corresponds unique
conditional set desirable gambles DcB unique conditional lower prevision P (|B),
non-empty event B. smallest set desirable gambles induces given coherent
lower prevision, called associated set strictly desirable gambles (Walley, 1991)
given {f L(A) : f > 0 P (f ) > 0}. See papers Walley (2000) Quaeghebeur
(2014) additional discussion sets desirable gambles informative
coherent lower previsions.
2.6 Linear Previsions Credal Sets
coherent lower upper prevision coincide gambles, real
functional P defined L(A) P (f ) := P (f ) = P (f ) f L(A) coherent prevision.
Since assumed finite,10 means corresponds
expectation

operator associated probability mass function p: P (f ) = xA f (x)p(x) =: Ep (f )
f L(A), p(x) := P (I{x} ) x A. happens particular lower
upper previsions induced maximal coherent set desirable gambles. Indeed,
boundary behaviour, so-called precise probability models P correspond maximal
coherent sets desirable gambles; see discussions Williams (1975a), Miranda
Zaffalon (2011, Proposition 6) Couso Moral (2011, Section 5) information.
coherent previsions P , Generalised Bayes Rule (GBR) reduces Bayess Rule:
P (gIB ) = P (B)P (g|B) g L(B),

(BR)

indicating central probabilistic updating rule special case Equation (2).
9. conditional lower previsions Section 12 IDMM produced regular extension.
models Sections 11, 13 14 lower previsions amongst them, nearly cases
different conditional lower previsions, even though cases natural regular extensions
coincidethey vacuous there.
10. already hinted footnote 8, similar things still said infinite A, would unduly
complicate discussion. details, see work Walley (1991), Troffaes De Cooman
(2014) Miranda De Cooman (2014).

12

fiCoherent Predictive Inference Exchangeability

assumed finite, define so-called credal set M(P ) associated
coherent lower prevision P as:
M(P ) := {p : (f L(A))Ep (f ) P (f )} ,
closed convex subset so-called simplex probability mass
functions A.11 P lower envelope M(P ): P (f ) = min {Ep (f ) : p M(P )}
f L(A) (Walley, 1991; Miranda & De Cooman, 2014; Troffaes & De Cooman,
2014). sense, convex closed sets precise probability models seen
imprecise probability models, mathematically equivalent coherent lower
previsions. therefore less general powerful coherent sets desirable
gambles, suffer problems conditioning events (lower) probability
zero.12

3. Predictive Inference
Predictive inference, specific sense focussing here, considers number
variables X1 , . . . , Xn assuming values category set Awe define category set
non-empty finite set.13 follows, shall occasion use many different
category sets, shall use italic capitals A, B, C, D, . . . refer them.
start discussion predictive inference models general representationally powerful language: coherent sets desirable gambles, introduced previous
section. on, shall pay attention specific derived models,
predictive lower previsions, predictive lower probabilities.
Predictive inference assumes generally number n observations made,
= (x1 , . . . , xn ) first n variables X1 , . . . , Xn . Based
know values
n c
values
subject posterior predictive model DA
observation sample ,
n
n
coherent set
next n variables Xn+1 , . . . , Xn+n assume . DA c
desirable gambles f (Xn+1 , . . . , Xn+n ) . assume n N.
hand, want allow n N0 := N {0}, set natural numbers
zero: want able deal case previous observations
n prior predictive model.14 course,
made. case, call corresponding model DA
technically speaking, n + n n.
said, subject may prior, unconditional model, obn
servations yet made. general form, coherent set DA
11. See Section 5.2 explicit definition .
12. Using sets full conditional measures (Dubins, 1975; Cozman, 2013), rather sets probability
mass functions, leads imprecise probability model related sets desirable gambles (Couso
& Moral, 2011), problems conditioning sets lower probability zero either,
feel less elegant mathematically complicated.
13. formal reasons, include trivial case category sets single element, case
certain value variables assume.
14. terms posterior prior association predictive models indicate whether previous
observations made. But, order avoid well-known issues temporal coherence
(Zaffalon & Miranda, 2013), assuming prior posterior models based
subjects beliefs observations made, posterior models refer hypothetical
future situations.

13

fiDe Cooman, De Bock, & Diniz

n
desirable gambles f (X1 , . . . , Xn ) , n N. may coherent sets DA
n
desirable gambles f (X1 , . . . , Xn ) , n natural number
n n must related following
n n; sets DA

marginalisation, time consistency, requirement:15
n
n
f (X1 , . . . , Xn ) DA
f (X1 , . . . , Xn ) DA
gambles f .

(5)

expression, throughout paper, identify gamble f cylindrical
extension f 0 , defined f 0 (x1 , . . . , xn , . . . , xn ) := f (x1 , . . . , xn ) (x1 , . . . , xn ) .
introduce marginalisation operator margn () := L(An ), time consistency
n = marg (D n ) = n L(An ).
condition rewritten simply DA
n


n posterior (conditional) ones n c
Prior (unconditional) predictive models DA
must
related following updating requirement:
n
n
f (Xn+1 , . . . , Xn+n )I{}
f (Xn+1 , . . . , Xn+n ) DA
c
(X1 , . . . , Xn ) DA

gambles f , (6)
special case Equation (2): gamble f (Xn+1 , . . . , Xn+n ) desirable observ gamble f (Xn+1 , . . . , Xn+n )I{}
ing sample
(X1 , . . . , Xn ) desirable
observations made. called-off gamble f (Xn+1 , . . . , Xn+n )I{}
(X1 , . . . , Xn )

gamble gives zero rewardis called offunless first n observations ,
case reduces gamble f (Xn+1 , . . . , Xn+n ) remaining variables
Xn+1 , . . . , Xn+n . updating requirement generalisation Bayess Rule updating,
fact reduces sets desirable gambles lead (precise) probability
mass functions, described Section 2.6 proved detail Walley (2000)
De Cooman Miranda (2012). contrary Bayess Rule probability mass
functions, updating rule (6) coherent sets desirable gambles clearly suffer
problems conditioning event (lower) probability zero: allows us infer
unique conditional model unconditional one, regardless (lower upper)
probability conditioning event. refer work De Cooman Miranda
(2012) detailed discussions marginalisation updating sets desirable gambles
many-variable context.
explained Section 2.5, use relationship (3) derive prior (unconditional)
n through:
predictive lower previsions P nA () L(An ) prior set DA
n
P nA (f ) := sup { R : f DA
} gambles f 1 n n,

L(An ) posterior
posterior (conditional) predictive lower previsions P nA (|)
n
through:
sets DA c
{
}
n
:= sup R : f DA
gambles f .
P nA (f |)
c
15. See related discussion notion De Cooman Miranda (2008b) De Cooman
Quaeghebeur (2012); confused temporal consistency discussed Goldstein
(1983, 1985) Zaffalon Miranda (2013).

14

fiCoherent Predictive Inference Exchangeability

on, shall want condition predictive lower previsions additional
information (Xn+1 , . . . , Xn+n ) B n , proper subset B A. Using ideas
Sections 2.4 2.5, leads instance following lower prevision:
{
}
n
B n ) := sup R : [g ]IB n DA
gambles g B n ,
P nA (g|,
c
(7)
conditioned event B n .
lower prevision P nA (|)

4. Principles Predictive Inference
far, introduced coherence, marginalisation updating basic rationality
requirements prior posterior predictive inference models must satisfy. could
envisaged requirementsother inference principlescan imposed
inference models. want show deal additional
requirements theory conservative predictive inference, discuss, way
examples, number additional conditions, suggested number
authors reasonable properties ofor requirements forpredictive inference models.
want stress considering requirements examples, want
defend using circumstances, mean suggest always reasonable
useful. are: inference principles might want impose, whose
implications conservative predictive inference might therefore want investigate.
4.1 Pooling Invariance
first consider Walleys (1996) notion representation invariance, prefer call
pooling invariance. Consider set categories A, partition B non-empty
partition classes. course consider partition B set categories well.
Therefore, order streamline discussion notation, shall henceforth denote
Bas stated before, want use italic capitals category sets. elements
subset C Acorresponds single new category, consists original
categories x C pooledconsidered one. Denote (x) unique element
partition B original category x belongs to. leads us consider surjective
(onto) map B.
say gamble g differentiate pooled categories when:
g() = g() , (k {1, . . . , n})(xk ) = (yk ),
means gamble f B n that:
( )g() = f ((x1 ), . . . , (xn )).
idea underlying formulaor requirementis sample = (x1 , . . . , xn )
, corresponds sample := ((x1 ), . . . , (xn )) B n pooled categories. Pooling
invariance requires gambles g = f differentiate pooled
categories, make difference whether make predictive inferences using set
original categories A, using set pooled categories B. formally, terms
predictive lower previsions:

15

fiDe Cooman, De Bock, & Diniz

= P nB (f |)

P nA (f ) = P nB (f ) P nA (f |)
,
n, n N considered, gambles f B n
alternatively, generally, terms predictive sets desirable gambles:
n
n
n
n
f DB

f DA
f DB
f DA
c
c

.
n, n N considered, gambles f B n
Pooling invariance seems reasonable principle uphold cases category
set known full detail. case useful start limited set broadly
defined categories, allow creation new ones, pooling splitting old categories
observations proceed. context, recall Walleys (1996) example:
closed bag containing coloured marbles, probability drawing red marble
it? information, subject idea colours
marbles bag, making difficult construct suitable detailed category set
experiment. draws bag, predictive inference model used
respects pooling invariance, inferences made red marbles uses
category set {red, yellow, blue, other} using category
set {red, non-red}, colours different red pooled together single
category. appears pooling invariance typically useful principle, instance,
sampling species problems, one wants assess prevalence given species
certain area.
special case pooling invariance, called embedding invariance,16 concentrates case without prior observations. terms lower previsions:
P nA (f ) = P nB (f ) n N considered, gambles f B n ,
alternatively, generally, terms sets desirable gambles:
n
n
f DA
f DB
n N considered, gambles f B n .

4.2 Renaming Invariance
Besides pooling invariance, may require renaming invariance: long confusion
arise, matter subjects predictive inferences names, labels,
gives different categories.
may seem trivial even mention, far know, always implicitly
taken granted predictive inference. well devote attention
here, order distinguish category permutation invariance discussed
shortly, easily confused pay proper attention.
renaming bijection (a one-to-one onto map) set original categories
set renamed categories C, clearly distinguish elements
C, sample = (x1 , . . . , xn ) original categories,
corresponds sample renamed categories := ((x1 ), . . . , (xn )). gamble
16. Walley calls underlying requirement (lower) probability event depend
possibility space embedded, Embedding Principle (Walley, 1991, Section 5.5.1).

16

fiCoherent Predictive Inference Exchangeability

f set C n renamed samples, corresponds gamble f set
original samples. Clearly, require make difference whether
make predictive inferences using set original categories A, using set renamed
categories C. formally, terms predictive lower previsions:
= P nC (f |)

P nA (f ) = P nC (f ) P nA (f |)
,
n, n N considered, gambles f C n
alternatively, generally, terms predictive sets desirable gambles:
n
n
n
n
f DC

f DA
f DC
f DA
c
c

.
n, n N considered, gambles f C n
4.3 Category Permutation Invariance
shall especially interested predictive inference subject starts state
prior ignorance. state, reason distinguish different elements
set categories chosen. formalise idea, consider permutation
$ elements A.17 sample , corresponds permuted sample
$ := ($(x1 ), . . . , $(xn )). gamble f , corresponds permuted
gamble f $ . subject reason distinguish categories z
images $(z), make sense require following category permutation invariance:18
= P nA (f |$)

P nA (f $) = P nA (f ) P nA (f $|)
,
n, n N considered, gambles f
alternatively, generally, terms predictive sets desirable gambles:
n
n
n
n
f DA

f $ DA
f DA
f $ DA
c
c$

.
n, n N considered, gambles f
Formally, requirement closely resembles renaming invariance, whereas latter
trivial requirement, category permutation invariance symmetry requirement
categories justified subject reason distinguish
them, may instance justified starts state prior ignorance.
draw attention difference two somewhat loose manner: category
permutation invariance allows confusion new old categories, something
renaming invariance carefully avoids.
see principle could reasonable, recall Walleys (1996) bag marbles
example, introduced discussing pooling invariance. Since, drawn
17. permutation $ elements A, words categories, contrasted
permutations order observations, i.e. time set {1, . . . , n}, considered discussion
exchangeability, Section 5.
18. requirement related notion (weak) permutation invariance De Cooman Miranda
(2007) studied much detail paper dealing symmetry uncertainty modelling. goes
back Walleys (1991, Section 5.5.1) Symmetry Principle.

17

fiDe Cooman, De Bock, & Diniz

marbles bag, subject idea marbles coloured, state
complete prior ignorance. Therefore, starts sample space {red, non-red},
observes outcomes draws, say twice non-red, consider probability
obtaining red marble next draw. due symmetry originating complete
ignorance, permute categories, calling red marbles non-red
non-red ones red, situation looking completely before,
therefore probability obtaining non-red marble next draw observing
twice red, must observing red one, observing non-red twice.
principle reminiscent Axiom A8 proposed Carnap (1952) system
inductive logic. course, reasonable principle subject prior
knowledge problem would, instance, allow impose ordering
categories.
4.4 Representation Insensitivity
shall call representation insensitivity combination pooling, renaming category
permutation invariance. means predictive inferences remain essentially unchanged
transform set categories, words insensitive
choice representationthe category set. difficult see representation
insensitivity formally characterised follows. Consider two category sets
so-called relabelling map : onto, i.e.
= (A) := {(x) : x A}. sample , corresponds transformed
sample := ((x1 ), . . . , (xn )) Dn . gamble f Dn corresponds
gamble f .
4.4.1 Representation Insensitivity
category sets onto map : D, n, n N
gambles f Dn :
considered,
= P nD (f |),

P nA (f ) = P nD (f ) P nA (f |)

(RI1)

alternatively, generally, terms predictive sets desirable gambles:
n
n
n
n
f DD

f DA
f DD
f DA
c
c.

(RI2)

weaker combination pooling, renaming category permutation
invariance models prior observations.
4.4.2 Prior Representation Insensitivity
category sets onto map : D, n N considered
gambles f Dn :
P nA (f ) = P nD (f ),
(EI1)
alternatively, generally, terms sets desirable gambles:
n
n
f DA
f DD
.

18

(EI2)

fiCoherent Predictive Inference Exchangeability

4.5 Specificity
turn another, rather peculiar view intuitively appealing, potential property predictive inferences. Assume addition observing sample observations
n observations category set A, subject comes know determine

way n following observations belong proper subset B A, nothing
elsewe might suppose instance observation (Xn+1 , . . . , Xn+n ) made,
imperfect, allows conclude (Xn+1 , . . . , Xn+n ) B n .
impose following requirement, uses models conditioned
event B n . conditional models introduced Equations (2) (4); see
discussion leading Equation (7), near end Section 3.
4.5.1 Specificity

category sets B B A, n, n N considered,
gambles f B n :
B n ) = P nB (f |
B ),
P nA (f |B n ) = P nB (f ) P nA (f |,

(SP1)

alternatively, generally, terms predictive sets desirable gambles:
n
n
n
n
f DB
B,
f IB n DA
f DB
f IB n DA
c
c

(SP2)

B tuple observations obtained eliminating tuple
observawhere
B empty tuple, observations
tions B. expressions,
B, posterior predictive model simply taken reduce prior predictive

model.
Specificity means predictive inferences subject makes
ones would get focussing category set B, time discarding
previous observations producing values outside B, effect retaining observations
inside B! knowing future observations belong B allows
subject ignore previous observations happened lie outside B. term
specificity context seems proposed Bernard (1997, 2005), based
work Rouanet Lecoutre (1983). so-called specific inference approach, questions,
inferences decisions involving restricted number categories, general
model replaced specific model deals categories interest,
specificity respected, general specific models produce
inferences. Specificity seems relevant principle analysing categorical data
described tree structures, case of, instance, patients classified
according symptoms (Bernard, 1997).
give simple example involving, again, Walleys bag marbles, subject
may observed, drawings, green, red, blue white marbles. asked
probability drawing red marble next, observer already seen
is, informs us either green redperhaps due bad lighting conditions
shes colour blind. subject uses specific inference model, disregard
previous observations involving colours green red.
19

fiDe Cooman, De Bock, & Diniz

4.6 Prior Near-Ignorance
use notion near-ignorance defined Walley (1991, p. 521) give following
definition prior near-ignorance context predictive inference; see related
discussions Walley (1991, Section 5.3.2), Walley (1997, Section 3) Walley Bernard
(1999, Section 2.3). refer paper Piatti, Zaffalon, Trojani, Hutter (2009)
interesting discussion prior near-ignorance may produce undesirable results
certain contexts.
4.6.1 Prior Near-Ignorance
prior model single variable Xk assuming values arbitrary category set
vacuous, category set A, n N considered, 1 k n gambles
f A:
P nA (extnk (f )) = min f,
alternatively, generally, terms sets desirable gambles:
n
extnk (f ) DA
f > 0,

extnk (f ) denotes cylindrical extension f gamble . defined
extnk (f )(x1 , . . . , xn ) := f (xk ) (x1 , . . . , xn ) . perhaps intuitive, less
formally correct, notation gamble f (Xk ).
Theorem 4. Prior representation insensitivity implies prior near-ignorance.
simple result implies model whose predictive previsions precise
prior representation insensitive, let alone representation insensitive, prior model
immediate predictions vacuous. shall see Section 14
nevertheless possible representation insensitive coherent inferences deploy precise
posterior predictive previsions.

5. Adding Exchangeability Picture
now, remainder paper, going add two additional assumptions.
first assumption is, principle, upper bound number
variables take account. words, considering n variables
X1 , . . . , Xn , always envisage looking one variable Xn+1 . effectively
means dealing countably infinite sequence variables X1 , . . . , Xn , . . .
assume values category set A.
n coherent
predictive inference models, means sequence DA
sets desirable gambles , n N. sequence course time-consistent
sense Requirement (5), meaning
n1
n2
n2
(n1 , n2 N)(n1 n2 DA
= margn1 (DA
) = DA
L(An1 )).

second assumption sequence variables exchangeable, means,
roughly speaking, subject believes order variables observed,
20

fiCoherent Predictive Inference Exchangeability

present themselves, influence decisions inferences make regarding
them.19
section, explain succinctly deal assumptions technically,
consequences predictive models interested in. detailed
discussion derivation results presented here, refer papers De Cooman
et al. (2009b) De Cooman Quaeghebeur (2012).
begin useful notation, employed numerous times
follows. Consider element RA . consider A-tuple, many (real)
components
x R categories x A. subset B A, denote
B := xB x sum components B.
5.1 Permutations, Count Vectors Hypergeometric Distribution
Consider arbitrary n N. denote = (x1 , . . . , xn ) generic, arbitrary element .
P n set permutations index set {1, . . . , n}. permutation ,
associate permutation , denoted , defined ()k := x(k) ,
words, (x1 , . . . , xn ) := (x(1) , . . . , x(n) ). Similarly, lift permutation
L(An ) letting f := f , ( f )() := f ().
permutation invariant atoms [] := { : P n }, smallest permutation invariant subsets . introduce counting map : NAn : 7 (),
count vector () A-tuple components
Tz () := |{k {1, . . . , n} : xk = z}| z A,
set possible count vectors n observations given
{
}
NAn := NA
0 : = n .

(8)

(9)

Tz () number times category z appears sample . = (),
[] = { : () = }, atom [] completely determined single count
vector elements, therefore denoted [].
consider linear expectation operator HynA (|) associated uniform
distribution invariant atom []:
HynA (f |) :=


1
f () gambles f ,
|[]|

(10)

[]

number elements () := |[]| invariant atom [] given
multinomial coefficient:
(
) ( )

n
n!
:=
() =
=
.
(11)


zA mz !
expectation operator Equation (10) characterisesor one associated
(multivariate) hyper-geometric distribution (Johnson, Kotz, & Balakrishnan, 1997, Section 39.2), associated random sampling without replacement urn n balls
19. Exchangeability assumed Carnaphis Axiom A7and Johnson (1924), named
permutation postulate.

21

fiDe Cooman, De Bock, & Diniz

types z A, whose composition characterised count vector . borne
0
fact that, , 0 n0 n 0 = (),
{
( 0 )/() 0
HynA (I{} |) =
0
otherwise
probability randomly selecting, without replacement, sequence n0 balls types
urn n balls whose composition determined count vector . See
running example concrete illustration.
hyper-geometric expectation operator seen linear transformation
HynA linear space L(An ) generally much lower-dimensional linear space
L(NAn ), turning gamble f so-called count gamble HynA (f ) := HynA (f |)
count vectors.
Running Example. order make argumentation, notions introduce
discuss, tangible concrete, shall use simple running example,
shall come back repeatedly number sections. notations assumptions made
maintained throughout series.
Consider (potentially infinite) sequence coin flips, whose successive outcomes
denote variables X1 , X2 , . . . Xn , . . . assuming values category set {H , }.
make somewhat interesting usual run-of-the-mill example, assume
stepfor coin flipNathalie selects coin bag three coins, hands
Arthur, proceeds flip it. coin put back bag next
step. subject whose beliefs modelling, may may know something
nature coins, Nathalie choosing coins subsequent flips:
might choose completely random, might specific deterministic
mechanism selecting them, . . .
= (H , , H , H ) first n = 4 observed coin flips. count
Consider sequence
corresponds sequence given components
vector ()
TH ((H , , H , H )) = 3 TT ((H , , H , H )) = 1,
= (3, 1), letting first component always refer H ,
denote
on. corresponding permutation invariant atom
[(H , , H , H )] = [(3, 1)] = {(T , H , H , H ), (H , , H , H ), (H , H , , H ), (H , H , H , )}
4!
3!1!

4
= 4 elements. set possible count vectors given N{H
,T } =
:= {(H , ), (T , H )} {H , }2
{(0, 4), (1, 3), (2, 2), (3, 1), (4, 0)}. Consider event HT
two different outcomes first two observations,

((3, 1)) =

1
1
Hy4{H ,T } (IHT
|(3, 1)) = (1 + 1 + 0 + 0) =
4
2
probability observing two different outcomes two random draws without replacement urn containing three balls marked H one ball marked , whose
composition therefore determined count vector (3, 1).

22

fiCoherent Predictive Inference Exchangeability

5.2 Multinomial Distribution
Next, consider simplex probability mass functions A:

{
}
:= RA : 0 = 1 , where, before: :=
x .

(12)

xA

probability mass function A, corresponds following multinomial
expectation operator MnnA (|):20
MnnA (f |) :=




f ()



zTz () gambles f ,

(13)

zA

characterises multinomial distribution, associated n independent trials
experiment possible outcomes probability mass function . Observe
)


( 1
f ()) ()
zmz
MnnA (f |) =
()
n
zA
NA
[]


n
mz
=
HyA (f |)()
z = CoMnnA (HynA (f )|),
n
NA

zA

used so-called count multinomial expectation operator:21


CoMnnA (g|) :=
g()()
zmz gambles g NAn .
n
NA

(14)

zA

Running Example. Consider n = 4 independent trials experiment possible outcomes
category set {H , } probability mass function = (H , ).
3
2 2
3
2
Mn4{H ,T } (IHT
|(H , )) = 2H + 4H + 2H = 2H (H + ) = 2H ,

. Observe, way, Mnn
gives probability event HT
|(H , )) =
{H ,T } (IHT
2H n 2.
gamble fHT
:= IHT
observation sequences (X1 , . . . , X4 ), corresponds
4
count gamble gHT
:= Hy{H ,T } (fHT
|) given by:
gHT
(0, 4) = 0 gHT
(1, 3) =

1
2
1
gHT
gHT
gHT
(2, 2) =
(3, 1) =
(4, 0) = 0,
2
3
2



1
2 2 2
1 3
3
CoMn4{H ,T } (g|(H , )) = 4H
+ 6H
+ 4H
= 2H
2
3
2
leads polynomial before, should.



20. avoid confusion, make (perhaps non-standard) distinction multinomial expectation,
associated sequences observations, count multinomial expectation, associated
count vectors.
21. See footnote 20.

23

fiDe Cooman, De Bock, & Diniz

5.3 Multivariate Polynomials

Let us introduce notation NA := mN NAm set possible count vectors
corresponding samples least one observation. Equation (9), let n = 0,
turns NA0 singleton
containing null count vector 0, whose

components zero. mN0 NAm = NA {0} set possible count vectors.
count vector NA {0}, consider (multivariate) Bernstein basis
polynomial BA, degree , defined by:
(
)

mz
mz
BA, () := ()
z =
z .
(15)

zA

zA

particular, course, BA,0 = 1.
linear combination p Bernstein basis polynomials degree n 0 (multivariate)
polynomial , whose degree deg(p) n.22 denote linear space
polynomials degree n V n (A). course, polynomials degree zero simply real
constants. gathered relevant useful information multivariate polynomials
Appendix B. follows discussion that, n 0, introduce
linear isomorphism CoMnnA linear spaces L(NAn ) V n (A):
gamble g
NAn , corresponds polynomial CoMnnA (g) := CoMnnA (g|) = N n g()BA,

V n (A), conversely, polynomial p V n (A) unique gamble bnp NAn
p = CoMnnA (bnp ).23 Observe particular, n 0 NAn :
CoMnnA ({}|) = BA, () .
(16)

denote V (A) := nN0 V n (A) linear space (multivariate) polynomials ,
arbitrary degree.
set HA V (A) polynomials called Bernstein coherent satisfies
following properties:
B1. 0
/ HA ;
B2. V + (A) HA ;
B3. posi(HA ) = HA .
Here, V + (A) set Bernstein positive polynomials : polynomials p
n deg(p) bnp > 0. follows Proposition 28 Appendix B
V + (A) subset set V ++ (A) polynomials p p() > 0
interior int(A ) := { : (x A)x > 0} . consequence B1B3,
find set V0 (A) := V + (A) Bernstein negative polynomials that:
B4. V0 (A) HA = .
22. degree may smaller n sum Bernstein basis polynomials fixed degree
one. Strictly speaking, polynomials p restrictions multivariate polynomials q RA ,
called representations p. p, multiple representations, possibly different degrees.
smallest degree called degree deg(p) p.
23. Strictly speaking, Equation (14) defines count multinomial expectation operator CoMnn

n > 0, clear definition extends trivially case n = 0.

24

fiCoherent Predictive Inference Exchangeability

Finally, every Bernstein coherent set HA polynomials induces lower prevision
H V (A) defined by:
H (p) := sup { R : p HA } p V (A).

(17)

lower prevision coherent, mathematical sense satisfies coherence
requirements P1P3.24
5.4 Exchangeability Representation Theorem
ready deal exchangeability. shall give definition coherent sets
desirable gambles generalises de Finettis (1937, 1975) definition, allows
significant generalisation Representation Theorem.
First all, fix n N. subject considers variables X1 , . . . , Xn
exchangeable distinguish gamble f permuted
version f , words, gamble f f equivalent zero gamble foror
indifferent tohim. means so-called set indifferent gambles:
{
}
n
:= f f : f L(An ) P n .
IA
n , set must compatible
subject coherent set desirable gambles DA
n , sense must satisfy rationality
set indifferent gambles IA
n
n
n
requirement DA + IA = DA ; see detailed explanations justifications De Cooman
Quaeghebeur (2012) Quaeghebeur et al. (2014) so-called desiring sweetened
n ,
deals requirement. say sequence X1 , . . . , Xn , model DA
exchangeable.
Next, countably infinite sequence variables X1 , . . . , Xn . . . called exchangeable
n, n N
finite subsequences X1 , . . . , Xn are, n N. means models DA
exchangeable. course time-consistent.
formulate powerful generalisation de Finettis (1937, 1975) Representation
Theorem, straightforward compilation various results proved De Cooman
Quaeghebeur (2012):

Theorem 5 (Representation Theorem, De Cooman & Quaeghebeur, 2012). sequence
n desirable gambles , n N coherent, time-consistent exchangeable
sets DA
Bernstein coherent set HA polynomials
NA
[]:

n N, gambles f ,
n
n
MnnA (f )BA,
f DA
MnnA (f ) HA f DA
c
HA .

case representation HA unique given HA :=



(18)

n
n
nN MnA (DA ).

follows Condition (18) HA completely determines predictive inferences
n
sequence variables X1 , . . . , Xn , . . . , fixes prior predictive models DA
24. Actually, suitably adapted version, underlying possibility space need longer finite
(Walley, 1991; Troffaes & De Cooman, 2014), domain restricted polynomials
(De Cooman & Quaeghebeur, 2012).

25

fiDe Cooman, De Bock, & Diniz

n c.
25 tells us representation HA set
posterior predictive models DA
polynomials plays role probability measure, density, distribution
function, precise-probabilistic case.
Indeed, corresponding coherent lower prevision H V (A) given Equation (17),
shown determine convex closed (compact) set

M(H ) := {HA : (p V (A))HA (p) H (p)}
coherent previsions HA V (A) (Walley, 1991; De Cooman et al., 2009b; De Cooman &
Quaeghebeur, 2012; Troffaes & De Cooman, 2014). pointed footnote 2and
come back footnote 36each coherent prevision HA uniquely
determines -additive probability measure Borel sets , therefore set
polynomials HA , via M(H ), uniquely determines set probability measures. But,
argued before, HA informative H M(H ), problems
conditioning sets lower probability zero: Bernstein coherent set polynomials
HA determines unique lower prevision H , therefore M(H ) unique set
probability measuresand densities absolutely continuouson simplex ,
converse necessarilyand usually notthe case. set probability densities
used define coherent set polynomialswe provide example
Section 12but generally one coherent set polynomials
leads set densities, updating behaviour different sets
polynomials different conditioning events lower probability zero.
n c
depend
Condition (18) tells us posterior predictive models DA
count vector
= ():
count vectors sufficient
observed sequence
statistics exchangeability. reason, shall denote posterior
n c
n c.
well DA
Also, every then, shall use
predictive models DA
n
n
DA c0 alternative notation DA .
immediate interesting consequence Theorem 5 updating observations
preserves exchangeability: observing values first n variables, count
remaining sequence variables Xn+1 , Xn+2 , . . . still exchangeable,
vector ,
Condition (18) tells us representation given Bernstein coherent set
defined by:
polynomials HA c
:= {p V (A) : BA,
HA c
p HA } .

(19)

compare Expressions (2) (6), tells us that, essentially, Bernstein
basis polynomials serve likelihood functions updating sets polynomials. use
refer coherent lower prevision V (A) derived HA c
means
H (|)
= 0, find HA c0 = HA H (|0) = H .
Equation (17). special case
related following version Generalised
Observe H H (|)
Bayes Rule:

H ([p H (p|)]B
(20)
) = 0 p V (A).
A,
completely determined HA . One consider HA prior model
Clearly, HA c
plays role posterior derived it.
parameter space , HA c
25. contrasted usual precise-probabilistic version, posterior predictive
models uniquely determined observed sequences non-zero probability; see footnote 3.

26

fiCoherent Predictive Inference Exchangeability

see Condition (18) Equation (19) thatsimilarly happens preciseprobabilistic settingthe multinomial distribution serves direct link one
n and, hand,
hand, prior HA prior predictive inference models DA
n c.
posterior predictive inference models DA
Recalling
posterior HA c

NA {0}:
convention = 0, summarise follows: n N
{
}
n
= f L(An ) : MnnA (f ) HA c

DA
c
(21)
and, immediate consequence:
{
}
= sup R : MnnA (f ) HA c
f L(An )
P nA (f |)

(22)

or, equivalently:
= H (MnnA (f )|)
f L(An ).
P nA (f |)

(23)

practical point view, Equation (23) often easier work Equa often admit simpler expression
tion (22), shall see on, H (|)
compare Equations (45), (54), (61) (69) Equations (49), (55), (65)
HA c;
always uniquely determined H : relaand (73), respectively. But, H (|)
uniquely H prior lower probability
tion (20) allows us determine H (|)

H (BA,
)

observing

non-zero.
Therefore,
sets polynomials HA


uniquely. quite dramatic
fundamental models, allow us determine HA c
illustration this, shall Sections 11, 13 14 come across number
quite different inference systemswith different HA give rise prior H

different posterior H (|)!
Running Example. assume subject assesses sequence coin flips
exchangeable, finds desirable gamble type I{H } (Xn ), fixed
(0, 1]; upper probability observing heads coin flip . Since
infer Equation (13) N n, MnN
{H ,T } (I{H } (Xn )|) = H , infer
Theorem 5 assessment corresponds following coherent set polynomials:
{
}
H := 1 p+ + 2 ( H ) : p+ V + ({H , }), 1 , 2 R0 max{1 , 2 } > 0 ,
smallest Bernstein coherent set polynomials contains polynomial
H ; explanation, see discussions De Cooman et al. (2009b)
De Cooman Quaeghebeur (2012). followsafter manipulationsfrom
Equation (17) Proposition 28 corresponding lower prevision V ({H , })
completely determined following optimisation:
H (p) = sup

min [p() + (H )]

0 {H ,T }

given
Hence, lower probability event HT
H (2H ) = sup min [2x(1 x) + (x )] = 0,
0 x[0,1]

upper probability
H (2H ) = H (2H ) = inf max [2x(1 x) (x )]
0 x[0,1]

27

fiDe Cooman, De Bock, & Diniz

=

{
2(1 )
1
2

12
otherwise.


tells us exchangeability alone already guarantees upper probability HT
1
2 . three coins bag assumed biased towards heads, < 12 ,
upper probability drops 12 .

finish section representation, want stress polynomials
given behavioural interpretation gambles may may desirable:
merely mathematical representational tools help us characterise
gambles observation sequences desirable.26 Similarly, set polynomials HA
lower prevision H merely mathematical tools allow convenient
representation predictive models observation sequences.
Running Example. illustrate polynomial representation much convenient
efficient, recall want make inferences sequence coin flips
length n, need work sets desirable gambles {H , }n , words,
cones 2n -dimensional space. work polynomial representations,
led consider cones polynomials degree n, constitute linear
space spanned n + 1 Bernstein basis polynomials degree n, therefore
n + 1-dimensional. Working polynomial representations therefore leads
dramaticexponentialreduction complexity.


6. Reasoning Inference Systems
seen previous section that, fix category set A, predictive inferences
exchangeable sequences assuming values completely determined Bernstein
coherent set HA polynomials . way associating Bernstein
coherent set HA every possible set categories A, would completely fix predictive
inferences. leads us following definition.
Definition 6 (Inference Systems). denote F collection category sets, i.e. finite
non-empty sets. inference system map maps category set F
set polynomials (A) = HA . inference system called coherent
category sets F, (A) Bernstein coherent set polynomials .
So, coherent inference system way systematically associate coherent predictive
inferences category set. Since inference principles Section 4 impose connections
predictive inferences different category sets, see interpret
inference principlesor rather, represent mathematicallyas properties of,
restrictions on, coherent inference systems. shall Section 7,
provides one important motivation introducing systems. Another, equally
26. makes operational, behavioural sense consider notion accepting polynomial, finding
desirable. much classical case, de Finetti (1975) probability distributions
simplex used mathematical representations, direct behavioural
meaningalthough Bayesians less careful foundations de Finetti might care make
distinction.

28

fiCoherent Predictive Inference Exchangeability

important reason so, allows us extend method natural extension
conservative inferenceintroduced Section 2.2, take account inference
principles predictive inference, generally, predictive inference multiple category
sets once.
see comes about, let us show conservative reasoning
inference systems. two inference systems 1 2 , say 1 less committal
conservativethan 2 , write 1 v 2
(A F)1 (A) 2 (A).
simply means predictive inferences category set less committal
first second inference system. denote set inference
systems, clearly set partially ordered v. Actually, complete lattice,
infimum supremum non-empty family , given by:
(

)
(
)


inf (A) =
(A) sup (A) =
(A) category sets A.
iI

iI

iI

iI

denote C set coherent inference systems:
C := { : (A F)(A) Bernstein coherent} .

(24)

clear C complete meet-semilattice, meaning closed arbitrary
non-empty infima:27
(i I)i C inf C.
(25)
iI

bottom structurethe conservative coherent inference systemis called
vacuous inference system V , coherent inference system given by:
V (A) = V + (A) category sets A.
shall come back detail vacuous inference system Section 9.
property (25) allows us conservative reasoning coherent inference systems.
Suppose, instance, collection category sets F F, assessments
form set polynomials AA V (A), F. Then, exists,
conservative coherent inference system compatible assessments given
by:
= inf { C : (A F)AA (A)} .
And, course, exist set polynomials AA included
Bernstein coherent set polynomials HA A, F. case, difficult
see, given discussion Section 5.3, (A) = posi(V + (A) AA ) F
(A) = V + (A) F \ F.
27. necessarily closed suprema, however, union Bernstein coherent sets polynomials
need Bernstein coherent.

29

fiDe Cooman, De Bock, & Diniz

7. Representation Insensitivity Specificity Exchangeability
Let us investigate form inference principles representation insensitivity (RI2)
specificity (SP2) take predictive inference exchangeability, inference
completely characterised Bernstein coherent sets polynomials. allow us
reformulate principles constraints onor properties ofinference systems.
7.1 Representation Insensitivity
recall notations assumptions Section 4.4. surjective (onto) map
: associate surjective map R : RA RD letting:
R ()z :=



x

RA z D.

(26)

xA : (x)=z

map allows us give following elegant characterisation representation insensitivity.
Theorem 7. coherent inference system representation insensitive
category sets onto map : D, p V (D)
NA {0}:
(p R )BA, (A) pBD,R () (D).
(RI3)
Running Example. Assume coins bag actually rather thick, implying
non-negligible chance fall one flat sides,
remain upright. denote new state U , new category set
:= {H , , U }. consider new flat state F , meaning either heads tails,
consider, instead A, category set := {F , U } distinguish
heads tails. relabelling map (H ) := (T ) := F (U ) := U
identifies proper relations categories D.
Suppose want say something lower probability event
observing U one flip H other, immediately observing
UF
sequence (H , U , H , ) count vector = (2, 1, 1)the last count three
refers number U observation sequence. A-domain, gamble IUF

n
28
expressed polynomial q = Mn{H ,T ,U } (IUF
), n 2 given by:
q() = 2(H + )U {H ,T ,U } .
2 belong
want find whether polynomials type [2(H + )U ]12H
U
({H , , U }); see Equation (18).
hand, seen previously, D-domain, gamble IUF

expressed polynomial p given p() = 2F U {F ,U } . Observe
q = p R . count vector = (2, 1, 1) A-domain corresponds count vector
R () = (3, 1) D-domain, first component refers number F
second number U s. here, need check whether polynomials type
[2F U ]43F U belong ({F , U }).

28. similar contexts, easy check polynomial remains n 2.

30

fiCoherent Predictive Inference Exchangeability

nice thing representation insensitivity makes checking whether
2
polynomials type [2(H + )U ]12H
U belong ({H , , U }) Adomain equivalent checking whether polynomials type [2F U ]43F U belong
({F , U }) D-domain.

interestingly, representation insensitivity preserved taking arbitrary nonempty infima coherent inference systems, allows us look conservative
representation insensitive coherent inference system compatible assessment
F, way straightforward extension discussion near end Section 6.
Theorem 8. Consider non-empty family , representation insensitive coherent
inference systems. infimum inf iI representation insensitive coherent
inference system well.
7.2 Specificity
Next, turn specificity, recall notations assumptions Section 4.5. Let us
define surjective restriction map rB : RA RB by:
rB ()z := z RA z B,

(27)

particular, rB () count vector B obtained restricting B (indices
the) components count vector A. define one-to-one injection map
iA : RB RA by:
{
x x B
iA ()x :=
RB x A.
(28)
0
otherwise
map used define following one-to-one maps IrB,A : V (B) V (A),
r N0 , follows:

IrB,A (p) :=
bdeg(p)+r
()BA,iA () polynomials p V (B).
(29)
p
deg(p)+r

NB

derive meaning following observation. polynomial p B
equivalently represented Bernstein basis B degree deg(p) + r.
interpret different representations polynomials , longer equivalent,
lead different polynomials IrB,A (p), r N0 . following propositions clarify
exactly effect operator IrB,A is.
Proposition 9. polynomial p B r N0 : IrB,A (p) iA = p.
introduce following notation, B > 0: |+
B := rB ()/B .
Observe |+


whenever

>
0.
B
B
B
Proposition 10. Consider polynomial p B , r N0 .
deg(p) + r = 0 p = c R, IrB,A (p|) = c. Otherwise, deg(p) + r > 0:
{
deg(p)+r
B
p(|+
B > 0
B)
IrB,A (p|) =
0
otherwise.
31

fiDe Cooman, De Bock, & Diniz

maps IrB,A allow us give following elegant characterisation specificity:
Theorem 11. coherent inference system specific category sets
B B A, p V (B), NA {0} r N0 :
IrB,A (p)BA, (A) pBB,rB () (B).

(SP3)

Running Example. Suppose, before, made observation (H , U , H , ),
count vector = (2, 1, 1). interested posterior lower probability
, somebody told us neither two subsequent coin flipsafter
event HT
first fourresulted U . specific inference system, allowed consider
predictive inference problem reduced category space B = {H , }, rather
category space = {H , , U }. then, B-space, use reduced count
vector rB () = (2, 1), obtained leaving number observed U s. polynomials
lead consider here, therefore type [2H ]32H , want
know whether belong ({F , U }).
A-space, polynomial p() = 2H , whose degree deg(p) = 2,
transformed polynomials
]
[

H
r
IB,A (p|) = 2
(H + )2+r = [2H (H + )2 ](H + )r
H + H +
r N0 . follows argumentation proof Theorem 11 original
problem requires us check whether polynomials type
2
[2H (H + )2 ](H + )r 12H
U

({H , , U }). Specificity allows us look problem B-space,
easier.

Observe close formal similarity conditions (RI3) (SP3).
therefore surprise us specificity, too, preserved taking arbitrary non-empty
infima inference systems.
Theorem 12. Consider non-empty family , specific coherent inference systems.
infimum inf iI specific coherent inference system well.
Let us denote Crs set coherent inference systems representation
insensitive specific. follows Theorems 8 12 Crs , C, closed
arbitrary non-empty infima, perform conservative reasoning, much
way discussed near end Section 6.

8. Immediate Prediction
inference system , look special case immediate prediction,
given category set A, observing sample n 0 variables count vector
NAn , want express beliefs value next observation Xn+1

assume A. specific case predictive inference n = 1, Condition (18)
NAn :
simplified somewhat, gambles f
1
1
BA,
f DA
SA (f ) (A) f DA
c
SA (f ) (A),

32

fiCoherent Predictive Inference Exchangeability

let
so-called sampling expectation SA (f ) linear polynomial given
SA (f |) := xA f (x)x .
reason NA1 = {x : x A} x count vector corresponding
single observation category x, words, exz = xz z [Kronecker
delta]. Hence, x :
( )
( )
1
1
x
x
f
(z)
=
f
(x)

B
()
=
zez = x ,
Hy1A (f |x ) =
A,
x
x

x
zA

z[ ]

leading to:
Mn1A (f |) =



Hy1A (f |x )BA,x () =

1
x NA



f (x)x = SA (f |).

(30)

xA

matter straightforward verification that, due Bernstein coherence HA ,
1 c
coherent set desirable gambles A,
so-called immediate prediction model DA
NA {0}. induces following predictive lower previsions:
every count vector
{
}
1
= sup R : f DA

P 1A (f |)
c
= sup { R : [SA (f ) ]BA,
(A)} .

(31)

Immediate prediction context exchangeable imprecise probability models
studied detail De Cooman et al. (2009a). Lower previsions, rather
sets desirable gambles, model choice paper, that,
authors encountered problems conditioning sets (lower) probability zero. fact,
problems provided motivation dealing much general
problem (not necessarily immediate) predictive inference using sets desirable gambles
present paper. section, want illustrate many results proved
made stronger (and easier proofs, borne Appendix E.3)
present context.
requirement (RI2) representation insensitivity reduces following simpler
requirement immediate prediction models: category sets
onto map : D, gambles f NA {0}:
1
1
f DA
c f DD
cR ().

(RI4)

Similarly, requirement (SP2) specificity reduces following simpler requirement
immediate prediction models: category sets B B A,
gambles f B NA {0}:
1
1
f IB DA
c f DB
crB ().

(SP4)

Let us show simple characterisation immediate prediction
models satisfy representation insensitivity. get there, observe consider
gamble g category set (surjective) pooling map finite subset
g(A) Ralso category set. corresponding Rg : RA Rg(A) given by:

Rg ()r =
x r g(A).
xA : g(x)=r

33

fiDe Cooman, De Bock, & Diniz

simple idea allows intriguing reformulation representation insensitivity
requirement immediate prediction models:
Proposition 13. immediate prediction models associated coherent inference
system representation insensitive category sets A, gambles g
count vectors NA {0}:
1
1
g DA
c idg(A) Dg(A)
cRg ().

(RI5)

Here, non-empty set B, denote idB identity map B, defined idB (z) := z
z B.
Proposition 13 tells us whether gamble desirable depends values
assumesand assumedand number times
values observed pastor rather would observing
g(Xk ) rather Xk .
Let us focus happens events. Consider event B nontrivial meaning B neither empty equal A. real gamble
IB assumes two values, 1 , see applying Proposition 13
NA {0}:
1
1
IB DA
c id{1,} D{1,}
c(mB , mA\B ),

therefore
{
}
1
P 1A (B|) = sup R : IB DA
c
{
}
1
= sup R : id{1,} D{1,}
c(mB , mA\B ) =: (mA , mB ),

(32)
(33)

meaning that, representation insensitivity, predictive lower probability non-trivial
event B depends number times mB observed past
experiments, total number observations . thing holds predictive
upper probability 1 (mA , mB ). precise predictive probabilities, similar property
known Johnsons sufficientness postulate (Johnson, 1924; Zabell, 1982).
representation insensitive{coherent inference system,
see define
}
2
so-called lower probability function : (n, k) N0 : k n [0, 1] Equation (33),
completely characterises one-step-ahead predictive lower upper probabilities29
non-trivial events count vectors. shall use representation insensitivity
specificity requirements try say lower probability function.
following theorem strengthens, simplifies, extends similar results De Cooman et al.
(2009a).
Theorem 14. Consider representation insensitive coherent inference system .
associated lower probability function following properties:
L1. bounded: 0 (n, k) 1 n, k N0 k n.
L2. super-additive second argument: (n, k + `) (n, k) + (n, `)
n, k, ` N0 k + ` n.
29. . . . necessarily predictive lower upper previsions . . .

34

fiCoherent Predictive Inference Exchangeability

L3. (n, 0) = 0 n N0 .
L4. (n, k) k(n, 1) n(n, 1) 1 n, k N0 1 k n.
L5. non-decreasing second argument: (n, k) (n, `) n, k, ` N0
k ` n.
L6. (n, k) (n + 1, k) + (n, k)[(n + 1, k + 1) (n + 1, k)] n, k N0
k n.
L7. non-increasing first argument: (n + 1, k) (n, k) n, k N0
k n.
L8. Suppose (n, 1) > 0 n N, let sn :=
1
. sn 0 sn+1 sn .
(n, 1) = n+s
n

1
(n,1)

n, equivalently,

moreover specific, following properties:
n
L9. Consider real (0, 1) suppose (1, 1) , (n, n) 1+n

1
n N0 . consequence, consider > 0 suppose (1, 1) 1+s ,
n
(n, n) n+s
n N0 .

know Theorem 4 representation insensitive coherent inference systems
near-ignorant, meaning vacuous therefore completely indecisive
single observation prior observations made. borne
Theorem 14.L3. Let us define imprecision function
(n, k) := 1 (n, n k) (n, k) n, k N0 k n.

(34)

1

clear P (B|) P 1A (B|) = (mA , mB ) width probability interval
event B observed mB times. representation
insensitive coherent inference system whose imprecision function (n, k) satisfies following
property:
}
(n + 1, k) (n, k)
0 k n,
(35)
(n + 1, k + 1) (n, k)
imprecision increase total number observations increases. suggests
representation insensitive coherent inference systems display
desirable behaviour mentioned Introduction: conservative little
learned, never become less precise observations come in. following
sections, intendamongst thingsto take closer look whether behaviour
present number systems.
Immediate prediction important predictive inference precise probabilities,
Law Total Probability guarantees completely determined immediate
predictions. Perhaps surprisingly, case predictive inference imprecise
probabilities: Appendix provides counterexample. points
limitations scope earlier work De Cooman et al. (2009a). reason,
leave immediate prediction models are, rest paper concentrate
general notion inference system.
35

fiDe Cooman, De Bock, & Diniz

9. Vacuous Inference System
following sections, provide explicit interesting examples representation insensitive, specific coherent inference systems. begin simplest
one: vacuous inference system V , introduced Section 6 smallest,
conservative, coherent inference system. associates category set
smallest Bernstein coherent set V (A) = HV,A := V + (A) containing Bernstein positive
polynomialsthe ones guaranteed anyway, Bernstein coherence alone.
deduce Proposition 30 Appendix B that:
= HV,A = V + (A)
NA {0},
HV,A c
Proposition 28 Appendix B that:
{
}
= H V,A (p) = sup R : p V + (A)
H V,A (p|)
= min p = min p() p V (A).


predictive models inference system straightforward find,
NA {0},
follow directly Equations (21) (23). n N
deduce that:
{
}
n
n
= f L(An ) : MnnA (f ) V + (A) ,
DV,A
= DV,A
c
(36)

= min MnnA (f |) f L(An ).
P nV,A (f ) = P nV,A (f |)


(37)

particular:
1
1
= L>0 (A),
DV,A
= DV,A
c

P 1V,A (f )

=


P 1V,A (f |)

(38)

= min f f L(A),

(39)


V (n, k) = 0 n, k N0 k n.

(40)

conservative exchangeable predictive models are, arise
making assessments exchangeability alone. gather Equations (36)
(40), interesting, involve non-trivial commitments,
allow learning observations. borne corresponding
imprecision function, given by:
V (n, k) = 1 n, k N0 k n.
Running Example. seen Mnn{H ,T } (IHT
|) = 2H n 2,
therefore
n
P nV,{H ,T } (IHT
) = P V,{H ,T } (IHT
|(3, 1)) =

min

Mnn{H ,T } (IHT
|) =

max

Mnn{H ,T } (IHT
|) =

{H ,T }

min

{H ,T }

2H = 0


n

n

P V,{H ,T } (IHT
) = P V,{H ,T } (IHT
|(3, 1)) =

{H ,T }

36

1
2H = .
{H ,T }
2
max

fiCoherent Predictive Inference Exchangeability

shows vacuous inference model produce completely vacuous inferences:
allows us find consequences making assessments exchangeability.
allow us change lower upper probabilities previsions
new observations come in.

Even though makes non-trivial inferences, vacuous inference system satisfies
representation insensitivity, specific.
Theorem 15. vacuous inference system V coherent representation insensitive.
Let us show means counterexample V specific,
Running Example. Let us go back inferences category space = {H , , U }
reduced category space B = {H , }. Consider polynomial p() = 2H H + 2T
{H ,T } . polynomial Bernstein positiveso p V + ({H , })because
p() = (2H H + 2T )(H + ) = 3H + 3T
expansion Bernstein basis degree 3 positive. let us consider
corresponding polynomial {H ,T ,U } :
2
2
q() := I0B,A (p|) = H
H +
.

(41)

polynomial Bernstein positive: easy see every n N0 ,
2
2
q() = (H
H +
)(H + + U )n
n . q = I0 (p)
always term H U
/ V + ({H , , U }), infer
B,A
Theorem 11 V cannot specific.


following sections, shall prove infinity committal,
specific representation insensitive coherent inference systems. begin introducing
slightly modified version vacuous inference system coherent, representation
insensitive specific.

10. Nearly Vacuous Inference System
Let us introduce nearly vacuous inference system NV reason name
become clear presentlyby:
NV (A) := HNV,A := V ++ (A) := {p V (A) : ( int(A ))p() > 0}
category sets A.
Since V ++ (A) consists polynomials positive int(A ), deduce
NA {0}: HNV,A c
= HNV,A = V ++ (A)
Proposition 28 Appendix B that,
that:
= H NV,A (p) =
H NV,A (p|)

p() = min p() p V (A).

inf
int(A )

37



fiDe Cooman, De Bock, & Diniz

Since know Proposition 28 Appendix B, counterexample following it,
generally speaking V + (A) V ++ (A), see inference system less conservative
vacuous one. case vacuous inference system, predictive models
nearly vacuous inference system straightforward find, follow directly
NA {0}, deduce that:
Equations (21) (23). n N
{
}
n
n
= f L(An ) : MnnA (f ) V ++ (A) ,
DNV,A
= DNV,A
c

= min MnnA (f |) f L(An ).
P nNV,A (f ) = P nNV,A (f |)


particular:
1
1
= L>0 (A),
DNV,A
= DNV,A
c

= min f f L(A).
P 1NV,A (f ) = P 1NV,A (f |)
see immediate prediction models, predictive lower previsions,
inference system exactly ones vacuous inference systems.30
allow learning observations.
Interestingly, contrast vacuous inference system, nearly vacuous
inference system specific, already tells us Crs 6= .
Theorem 16. nearly vacuous inference system NV coherent, representation insensitive specific: NV Crs .

11. Skeptically Cautious Inference System
construct rather simple inference system quite intuitive slightly
informative vacuous nearly vacuous ones. Suppose subject uses
following system making inferences based sequence n > 0 observations count
category set A. skeptical believes future,
vector ,
observe categories seen previously, categories set:
:= {x : mx > 0} .
A[]

(42)

cautious, beliefs already observed categories
observed future, nearly vacuous. explain this, assume first
particular, n future observations, vacuous beliefs count vector
observe set
{
}
n
n


NA : (y \ A[])my = 0 = NA[

]
holds possible observing count vector ,

future count vectors
31 Lemma 47 Appendix B,
namely count vectors observation outside A[].
30. first example shows immediate prediction models completely determine
inference system. shall come across another example Appendix D.
31. last equality equation actually device allows us identify count vectors
zero, count vectors A[].
shall using repeatedly,
whose components outside A[]
without explicit mention, rest paper.

38

fiCoherent Predictive Inference Exchangeability

would lead us associate following set polynomials count vector NA :
{
}
+
n
V[]
(A) := p V (A) : (n deg(p)) bnp |NA[]
>0
{
}
+
= p V (A) : p|A[] V (A[]) .
But, already know vacuous models V + (A) lead specific systems,
whereas nearly vacuous models V ++ (A) do, modify slightly, rather
associate following set polynomials count vector NA :
{
}
++
V[]
(A) := p V (A) : p|A[] V ++ (A[]) .
++
polynomials V[]
(A) desirable representation32 observing sample
count vector , infer Equation (19) subject considers desirable
representation polynomials in:
{
}
++
++
V[]
(A)BA, = pBA, : p V[]
(A) .

thus led consider following assessment:

++
ASC,A :=
V[]
(A)BA, ,
NA

set positive linear combinations:
HSC,A

:= posi (ASC,A ) =

{
`

pk BA,k : ` N, nk N, k

NAnk , pk

}



++
V[
(A)
k]

. (43)

k=1

following proposition guarantees sets HSC,A appropriate conservative
models summarise exchangeable inferences skeptically cautious subject.
Proposition 17. HSC,A smallest Bernstein coherent set polynomials
includes ASC,A .
shows inference system SC , defined SC (A) := HSC,A category
sets A, coherent. shall call skeptically cautious inference system.
want find updating works system. end, introduce
slight generalisation set defined Equation (43). Consider NA {0}, let
HSC,A, :=

{
`

pk BA,k : ` N, nk N0 , + nk > 0, k

NAnk , pk

}



++
V[+
(A)
k]

,

k=1

(44)
see that, particular, HSC,A = HSC,A, = 0.
sets HSC,A, following interesting characterisation:
32. stated before, polynomials direct behavioural indirect representational meaning,
conveniently condensed representations desirable gambles observation sequences. Hence
caution using term desirable representation.

39

fiDe Cooman, De Bock, & Diniz

Proposition 18. NA {0}:
{
}
HSC,A, = p V (A) \ {0} : (K min SA, (p))p|K V ++ (K) ,

(45)


{
}
SA, (p) := =
6 K : A[] K p|K 6= 0 .

(46)

min SA, (p) mean set minimal, non-dominating, elements SA, (p),
min SA, (p) := {C SA, (p) : (K SA, (p))(K C K = {C)}. formally extend
}
Equation (42) include case = 0, A[0] = SA,0 (p) = =
6 K : p|K 6= 0 .
Proposition 19. NA {0}: HSC,A c = HSC,A, .
combining result Equation (21), deriveadmittedly rather involved
expressions predictive sets desirable gambles skeptically cautious inference
NA {0}:
system. n N
{
}
n
= f L(An ) : MnnA (f ) HSC,A,
c
(47)
DSC,A
.
NA :
immediate prediction, expressions simplify significantly.
{
}
1
1
= f L(A) : f |A[]
DSC,A
= L>0 (A) DSC,A
c
> 0 L>0 (A).

(48)

NA :
lower previsions derived HSC,A,
tractable.
=
H SC,A (p) = min p(x ) H SC,A (p|)
xA

min p() p V (A),

A[]


(49)

where, x A, x degenerate probability mass function assigns
probability mass x.
predictive lower previsions skeptically cautious inference system
NA :
easily obtained combining Equations (49) (23). n N
=
P nSC,A (f |)

min MnnA (f |) f L(An )

A[]


(50)


P nSC,A (f ) = min f (x, x, . . . , x) f L(An ).

(51)

= min f (x) f L(A).
P 1SC,A (f ) = min f P 1SC,A (f |)

(52)

xA

particular:

xA[]

lower probability function given by:
{
1 k = n > 0
SC (n, k) =
0 otherwise

n, k N0 k n,

corresponding imprecision function by:
{
1 n = 0 0 < k < n
SC (n, k) =
0 otherwise
40

n, k N0 k n.

fiCoherent Predictive Inference Exchangeability

Running Example. before, Mnn{H ,T } (IHT
|) = 2H n 2. take
account {H , }[(3, 1)] = {H , }, get:
n
P nSC,{H ,T } (IHT
) = P SC,{H ,T } (IHT
|(3, 1)) =

min

Mnn{H ,T } (IHT
|) =

{H ,T }

max

Mnn{H ,T } (IHT
|) =

{H ,T }

{H ,T }

min

2H = 0

max

1
2H = .
2


n

n

P SC,{H ,T } (IHT
) = P SC,{H ,T } (IHT
|(3, 1)) =

{H ,T }

categories observed count vector (3, 1)meaning {H , }[(3, 1)] =
{H , }we find inferences vacuous inference system.

Interestingly, coherent inference system SC satisfies representation insensitivity specificity.
Theorem 20. skeptically cautious inference system SC coherent, representation
insensitive specific: SC Crs .

12. IDMM Inference Systems
Imprecise Dirichlet Modelsor IDMs, shortare family parametric inference models
introduced Walley (1996) conveniently chosen sets Dirichlet densities diA (|)
constant prior weight s:
{
}
{diA (|) : KsA } , KsA := RA
(53)
>0 : = = {s : int(A )} ,
value (so-called) hyperparameter R>0 category set A. Dirichlet
densities diA (|) defined int(A ); see Appendix C explicit definition
extensive discussion.
IDMs generalise Imprecise Beta models introduced earlier Walley (1991).
later paper, Walley Bernard (1999) focussed closely related family predictive
inference models, called Imprecise Dirichlet Multinomial Modelsor IDMMs,
short.33 refer papers, recent overview paper Bernard
(2005) extensive motivating discussion IDM(M)s, inferences properties.
precise Dirichlet models expectations, related Dirichlet multinomial
models, gathered Appendix C important facts, properties results,
necessary proper understanding present discussion IDM(M)s context
inference systems.
One reasons Walley (1996) suggesting IDM reasonable model
precisely satisfies pooling34 invariance properties discussed Section 4.1.
discussed emphasis Walley Bernard (1999) Bernard (2005),
know detailed explicit formulations properties literature,
proofs seen fairly sketchy. Bernard (1997, 2005) suggests IDM
33. later paper, Walley Bernard (1999) clearly distinguish name parametric IDMs
predictive IDMMs, earlier paper Walley (1996), types models referred
IDMs.
34. Walley uses term representation invariance rather pooling invariance.

41

fiDe Cooman, De Bock, & Diniz

underlying precise Dirichlet models satisfy so-called specificity property,
tried translate present context predictive inference Section 4.5.
present section, use ideas behind Walley Bernards IDM(M)s construct
interesting family coherent inference systems, give detailed formal proof
Appendix E fact inference systems indeed representation insensitive
specific. Interestingly, shall need slightly modified version Walleys IDM(M)
make things work. reason Walleys original version, described
Expression (53), number less desirable properties, seem either
unknown to, ignored by, Walley Bernard. describe shortcomings
detail Appendix D. present purposes, suffices mention that, contrary
often claimed, contradistinction new version, inferences using original
version IDM(M) necessarily become conservative (or less committal)
hyperparameter increases.
version, rather using hyperparameter sets KsA , consider sets
{
}
sA := RA
>0 : < R>0 .
Observe
{
}
sA = s0 : s0 R>0 , s0 < int(A ) =



0

KsA .

0<s0 <s

R>0 , category set A, consider following set polynomials
p, positive Dirichlet expectation DiA (p|) hyperparameters sA :

:= {p V (A) : ( sA ) DiA (p|) > 0} .
HIDM,A

shall see Theorem 21 set Bernstein coherent. call inference
system sIDM , defined by:

sIDM (A) := HIDM,A
category sets A,

IDMM inference system hyperparameter > 0. corresponding updated models
NA {0}, given by:
are,

= {p V (A) : ( sA ) DiA (p|
+ ) > 0}
HIDM,A
c

(54)

= inf DiA (p|
+ ) p V (A).
H sIDM,A (p|)

(55)




Using expressions, predictive models IDMM inference system straightforward find; suffices apply Equations (21) (23). n N
NA {0}:

{
}
s,n
= f L(An ) : ( sA ) DiA (MnnA (f )|
+ ) > 0 ,
DIDM,A
c
(56)

= inf DiA (MnnA (f )|
+ ) f L(An ),
P s,n
IDM,A (f |)


42

(57)

fiCoherent Predictive Inference Exchangeability

where, using notations introduced Appendix C:
+ ) = DiMnnA (HynA (f )|
+ )
DiA (MnnA (f )|
( )

n
1
n

=
HyA (f |)
(mx + x )(mx ) .
(n)

(mA + )
n
xA

N

(58)



general, expressions seem forbidding, immediate prediction models
NA {0}:
manageable enough.
{
}
1
s,1
= f L(A) : f >
(59)
DIDM,A c
f (x)mx ,

xA

1


P s,1
(f
|
)
=
f (x)mx +
min f f L(A),
(60)
IDM,A
+
+
xA



k
n, k N0 k n.
n+s
corresponding imprecision function given by:
sIDM (n, k) =

sIDM (n, k) =


n, k N0 k n,
n+s

decreasing first constant second argument, implies
satisfies Condition (35). suggests IDMM inference systems conservative
little learned, become precise observations come in.
Running Example. before, Mnn{H ,T } (IHT
|) = 2H n 2, find that,
using results Appendix C:
)
(

Di{H ,T } Mnn{H ,T } (IHT
|) =

2H
.
(H + )(H + + 1)

difficult verify using Equation (57) 0 < s:
s,n

P s,n
) = 0 P IDM,{H ,T } (IHT
) =
IDM,{H ,T } (IHT

1
.
21+s

observing count vector (3, 1), find manipulations that:
2(3 + )
2(3 + s)
=
,
0<<s (4 + )(5 + )
(4 + s)(5 + s)

P s,n
|(3, 1)) = inf
IDM,{H ,T } (IHT
similarly:




6

1+s
(4 + s)(5 + s)
s,n
P IDM,{H ,T } (IHT
|(3, 1)) =

1
4+s


25+s

2
2.

Observe infinitely large s, recover inferences vacuous system.
43



fiDe Cooman, De Bock, & Diniz

Interestingly, immediate prediction models version IDMM inference
system coincide Walleys original version. Hence, many practical applications concerned immediate prediction only, approaches yield identical
results.
IDMM inference systems constitute uncountably infinite family coherent
inference systems, satisfies representation insensitivity specificity
requirements.
Theorem 21. R>0 , IDMM inference system sIDM coherent, representation
insensitive specific: sIDM Crs .

Since Crs closed non-empty infima, infimum
IDM IDM , > 0
still coherent, representation insensitive specific, conservative
IDMM inference systems. given by:
{
}
+++

(A) := p V (A) : ( RA
IDM (A) = V
>0 ) DiA (p|) > 0 ,

although set generally strictly includes sets V + (A) V ++ (A), associated
immediate prediction models predictive lower previsions shown coincide
ones vacuous nearly vacuous inference systems.

13. Skeptical IDMM Inference Systems
combine ideas previous two sections: suppose subject uses
following system making inferences based sequence n > 0 observations
category set A. Section 11, skeptical
count vector ,
believes future, observe categories seen previously,
rather cautious completely vacuous
categories set A[].
beliefs already observed categories observed future,
uses IDMM-like inference them, described Section 12.
turns done quite simply replacing, characterisation (45)
sets HSC,A, skeptically cautious inference system, nearly vacuous models

V ++ (K) appropriate IDMM models HIDM,K
crK (). define, category
set A, NA {0} R>0 , following set polynomials:
{
}


:= p V (A) \ {0} : (K min SA, (p))p|K HIDM,K
HSI,A,
crK () ,
(61)
recall K min SA, (p), A[] K therefore K[rK ()] =
A[] K = A[], rK () essentially count vectors. let


:= HSI,A,
HSI,A
= 0, words:
}
{


:= p V (A) \ {0} : (K min SA,0 (p))p|K HIDM,K
HSI,A
,
{
}
where, again, SA,0 (p) = =
6 K : p|K 6= 0 . remainder section, show

sets polynomials HSI,A
indeed lead definition reasonable potentially
useful type inference system. begin coherence.

Proposition 22. HSI,A
Bernstein coherent set polynomials .

44

fiCoherent Predictive Inference Exchangeability


shows inference system sSI , given sSI (A) := HSI,A
category sets A,

coherent. call SI skeptical IDMM inference system hyperparameter s.
want find updating works inference system. following
proposition really come surprise.


Proposition 23. NA {0}: HSI,A
c = HSI,A,
.

combining Equation (21), obtain followingagain, rather involved
predictive sets desirable gambles skeptical IDMM inference systems. n N
NA {0}:

{
}
s,n

= f L(An ) : MnnA (f ) HSI,A,
DSI,A
c
(62)
.


rather abstract, case
Although expressions HSI,A
c
NA :
corresponding lower previsions.

H sSI,A (p) = min p(x ) p V (A)
xA

(63)


=
H sSI,A (p|)

inf

sA[]


+ )
DiA[]
|rA[]
(p|A[]
()


p V (A).
= H sIDM,A[]
|rA[]
())
(p|A[]


(64)
(65)

Combining Equation (23), immediately obtain following predictive lower
NA :
previsions skeptical IDMM inference systems. n N
n
P s,n
SI,A (f ) = min f (x, x, . . . , x) f L(A )
xA


=
P s,n
SI,A (f |)

inf

sA[]


n
+ )
DiA[]
(MnA[]
()
(f |A[]
n )|rA[]

f L(An ).
= P s,n
())
n |rA[]
(f |A[]
IDM,A[]

(66)

immediate prediction models skeptical IDMM inference systems surprisingly
manageable:
s,1
DSI,A
= L>0 (A) P s,1
SI,A (f ) = min f f L(A)

NA :
and,
{
}
1
s,1
= f L(A) : f |A[]
DSI,A c
f (x)mx L>0 (A)
>


(67)


xA[]


1

s,1
=
(f |)
f (x)mx +
min f (x) f L(A).
P SI,A
+
+ xA[]


xA[]

45

(68)

fiDe Cooman, De Bock, & Diniz

lower probability function given by:
{
k
k < n n = 0

SI (n, k) = n+s
1
k = n > 0
corresponding imprecision function by:
{

n = 0 0 < k < n

SI (n, k) = n+s
0
otherwise

n, k N0 k n,

n, k N0 k n.

consider case n > 0, see sSI (n, n) = 0 sSI (n + 1, n) =
imprecision function satisfy Condition (35).


n+1+s

> 0,

Running Example. {H , }[(3, 1)] = {H , }, infer Equation (66)
IDMM inference systems.
inferences event HT

coherent inference systems sSI satisfy representation insensitivity
specificity.
Theorem 24. R>0 , corresponding skeptical IDMM inference system
coherent, representation insensitive, specific: sSI Crs .

Since Crs closed non-empty infima, infimum
SI SI , > 0 still
coherent, representation insensitive specific, conservative
skeptical IDMM inference systems. shown associated immediate prediction
models predictive lower previsions coincide ones skeptically cautious
inference system.

14. Haldane Inference System
already know discussion near-ignorance following Theorem 4 representation insensitive coherent inference system fully precise, immediate prediction
models observations made, must completely vacuous. ask
whether representation insensitive (and specific) inference systems whose
posterior predictive lower previsions become precise (linear) previsions. problem
address section. shall first construct inference system, show
system is, definite sense, unique linear posterior predictive previsions.
use family IDMM inference systems sIDM , R>0 , define inference
system H committal them:



HIDM,A
=
sIDM (A) category sets A.
H (A) = HH,A :=
sR>0

sR>0

call H Haldane inference system, reasons become clear
section.
Theorem 25. Haldane inference system H coherent, representation insensitive
specific: H Crs .
46

fiCoherent Predictive Inference Exchangeability

Due representation insensitivity, Haldane system satisfies prior near-ignorance.
implies making observation, immediate prediction model vacuous,
far away precise probability model possible. show
that, making even single observation, inferences become precise-probabilistic:
coincide inferences generated Haldane (improper) prior.
get there, first take look models involving sets desirable gambles.
NA {0}:



= {p V (A) : (s R>0 )( sA ) DiA (p|
+ ) > 0} =

HH,A c
HIDM,A
c.
sR>0
(69)
corresponding predictive models easily derived applying Equation (21).
NA {0}:
n N
{
}
n
= f L(An ) : (s R>0 )( sA ) DiA (MnnA (f )|
+ ) > 0
DH,A
c

s,n

=
DIDM,A
c.
(70)
sR>0

immediate prediction models obtained combining Equations (70) (59).
NA :

{
}

1
1
= f L(A) :
DH,A = L>0 (A) DH,A c
f (x)mx > 0 L>0 (A).
xA

turns expressions corresponding lower previsions much
NA {0}:
manageable. First all, find
+ ) = lim H sIDM,A (p|)
p V (A).
inf DiA (p|

= lim
H H,A (p|)

s+0 sA

s+0

(71)

= 0, simplifies to:
particular,
H H,A (p) = min p(x ) p V (A),
xA

(72)

NA , find linear previsions:35
whereas
= H H,A (p|)
= HH,A (p|)
= DiA (p|)
p V (A).
H H,A (p|)

(73)

corresponding predictive models easily derived applying Equation (23).
NA {0}:
n N
= lim
P nH,A (f |)

+ ) = lim P s,n
f L(An ).
inf DiA (MnnA (f )|
IDM,A (f |)

s+0 sA

s+0

(74)
= 0:
particular,
P nH,A (f ) = min f (x, x, . . . , x) f L(An ),
xA

35. Dirichlet expectations DiA (|) strictly speaking defined RA
>0 , argue
Appendix C, continuously extended components zero, others strictly
positive.

47

fiDe Cooman, De Bock, & Diniz

NA :


P nH,A (f |)

=

n

P H,A (f |)

=

n

PH,A
(f |)

=


n
NA

( )
(nx )
n
xA mx
.
(n)



HynA (f |)

(75)



NA :
immediate prediction models, find

mx
1
=
P 1H,A (f ) = min f PH,A
(f |)
f (x)
f L(A),

xA

lower probability function given by:
{
k
n > 0
H (n, k) = n
n, k N0 k n.
0
n = 0
corresponding imprecision function given by:
{
1 n = 0
H (n, k) =
n, k N0 k n,
0 n > 0
satisfies Condition (35), suggests Haldane inference system displays
albeit extreme interesting mannerthe desirable behaviour mentioned
Introduction: conservative little learned, never become less
precise observations come in.
Running Example. use Equation (74) results previously obtained
IDMM inference systems find
n

n
P nH,{H ,T } (IHT
) = 0 PH,{H ,T } (IHT
|(3, 1)) =
) = P H,{H ,T } (IHT

3
.
10

want point first equalities contradict prior near-ignorance
Haldane inference system, pertains immediate predictions: predictions
single future observations.

precise posterior predictive previsions Equation (75) exactly ones
would found formally apply Bayess rule multinomial likelihood
Haldanes improper prior (Haldane,
1945; Jeffreys, 1998; Jaynes, 2003), whose density
function int(A ) proportional xA x1 . This, course, use Haldanes name
inference system produces them. argumentation shows nothing
wrong posterior predictive previsions, based coherent inferences.
fact, analysis shows infinity precise proper priors simplex
that, together multinomial likelihood, coherent posterior predictive
previsions: every coherent prevision V (A) dominates coherent lower prevision
H H,A V (A).36 binomial parametric inferences Haldane prior, Walley (1991,
Section 7.4.8) comes related conclusion completely different manner.
36. immediate consequence F. Riesz Representation Theorem coherent prevision
restriction polynomials expectation operator unique -additive probability measure
Borel sets ; see instance discussion De Cooman Miranda (2008a)
footnote 2.

48

fiCoherent Predictive Inference Exchangeability

simple argument show Haldane posterior predictive previsions
precise ones compatible representation insensitivity. Indeed,
shown representation insensitive coherent inference system precise
posterior predictive previsions, lower probability function must satisfy (n, k) = k/n
n > 0 0 k n,37 straightforward prove, using Bayess Theorem go
immediate prediction general predictive inference, posterior predictive
previsions must Haldanes.

15. Characterisation IDMM Immediate Predictions
lower probability function (n, k) representation insensitive coherent inference
system gives lower probability observing non-trivial event observed k
times n trials.
suppose subject specifies single lower probability, namely value
(1, 1) [0, 1]: probability observing something (again) observed (once)
single trial. ask conservative consequences
assessment are, take representation insensitivity specificity granted.
words, conservative representation insensitive specific coherent inference
system (at least) given value (1, 1) lower probability function?
question makes sense representation insensitive specific coherent inference
systems constitute complete meet-semilattice Statement (25) Theorems 8 12.38
Clearly, (1, 1) = 0, smallest representation insensitive specific coherent
inference system, know discussion Sections 9 10, must
immediate prediction models predictive lower previsions (nearly) vacuous
inference system. consider case 0 < (1, 1) < 1,39 words, use
parametrisation turn convenient purposes, that:
(1, 1) =

1
1
positive real number :=
1.
1+s
(1, 1)

(76)

Let us denote conservative inference system , lower probability
1
function , assumption (1, 1) 1+s
. follows Theorem 14.L9
n

(n, n) n+s n N0 . since IDMM inference system sIDM , Equation (60)
n
tells us sIDM (n, n) = n+s
, since assumption sIDM (n, n) (n, n), conclude
that:
n
(n, n) = sIDM (n, n) =
n N0 .
(77)
n+s
surmised (Bernard, 2007; De Cooman et al., 2009a) IDMM inference
system hyperparameter could smallest, conservative, representation
1
insensitive specific coherent inference system given value (1, 1) = 1+s
.
fact, trying prove made us start research present paper.
conjecture turns false: apart lower bound (77) (n, n),
37. suffices exploit additivity precise probabilities symmetry implied representation
insensitivity; explicit proof, see paper De Cooman et al. (2009a, Thm. 7).
38. See discussion near end Section 7.
39. surmise, prove here, conservative representation insensitive specific
coherent inference system corresponding (1, 1) = 1 might skeptically cautious one.

49

fiDe Cooman, De Bock, & Diniz

representation insensitivity specificity impose lower bounds (n, k) k < n.
see this, consider inference system sMC := inf{SC , sIDM }, Statement (25)
Theorems 8, 12, 20 21 coherent, representation insensitive specific: sMC Crs .
lower probability function sMC satisfies:
{
n
n
min{1, n+s
} = n+s
k = n > 0
sMC (n, k) = min{SC (n, k), sIDM (n, k)} =
k
min{0, n+s } = 0
otherwise,
substantiating claim made above. See Figure 1, depicted lower (and
upper) probability functions Haldane system H , IDMM system sIDM , sMC
n

inference system inf{4s
SI , IDM }. latter three share value n+s (n, n),
n 0. conjecture sMC could smallest, conservative, representation
1
insensitive specific coherent inference system given value (1, 1) = 1+s
, offer
proof this.
(n, k)
1
n
n+s


n+s

0
0

1

2

...

n1

n

k

Figure 1: Lower upper probability functions: H Haldane system (dark grey,

4), sIDM IDMM system hyperparameter (blue, ?), min{4s
SI , IDM }
(orange, ) sMC = min{SC , sIDM } (red, ). specific plot made
n = 10 = 2.
means want characterise IDMM inference systems way
conservative ones, need add, besides coherence, representation insensitivity
specificity, another requirement preserved taking infima. One possible
candidate this, shall prove job inspired Figure 1,
following requirement.
Let us define subjects surprise event supremum rate betting
opposite event, words, lower probability opposite event. surprise
highclose onewhen subject believes strongly event occur,
lowclose zerowhen subject strong beliefs occur.
50

fiCoherent Predictive Inference Exchangeability

allows us associate so-called surprise function (n, k) := (n, n k)
lower probability function, (n, k) subjects surprise observing non-trivial
event observed k n times before.
follows Theorem 14.L5 representation insensitive system, surprise
function non-increasing second argument:
(n, k) := (n, k + 1) (n, k) = (n, n k 1) (n, n k) 0 0 k n 1.
fairly intuitive property: often event observed before,
smaller surprise seeing again.
shall say representation insensitive system concave surprise
2 (n, k) := (n, k + 1) (n, k) 0 0 k n 2,
where, course, 2 (n, k) = (n, n k 2) 2(n, n k 1) + (n, n k).
difficult see concave surprise preserved taking non-empty infima
inference systems, makes sense go looking smallest (most conservative)
coherent representation insensitive specific coherent inference system concave
surprise, satisfies additional local assessments, (76).
Looking Figure 1 makes us suspect IDMM inference system sIDM might
system, again, offer proof conjecture. however provide proof
following, related (probably) weaker, statement, focusses immediate
prediction only:
Theorem 26. immediate prediction models P 1A (|), NA {0} smallest
(most conservative) coherent representation insensitive specific coherent inference system
concave surprise satisfies (76), coincide ones IDMM inference
system sIDM hyperparameter s.

16. Conclusion
believe first paper tries deal systematic fashion principles
predictive inference exchangeability using imprecise probability models. Two salient
features approach (i) consistently use coherent sets desirable gambles
uncertainty models choice; (ii) notion inference system allows us
derive conservative predictive inference method combining local predictive probability
assessments general inference principles.
first feature allows us, contradistinction approaches
probability theory, avoid problems determining unique conditional models
unconditional ones conditioning events (lower) probability zero. set
n c

polynomials HA completely determines prior posterior predictive models DA
n
n
even (lower) prior probability P ([])
= H (BA,
P (|),
) observing
zero. approach using lower previsions probabilities would make
count vector
much complicated involved, impossible. Interestingly, provide
perfect illustration fact using results Sections 11, 13 14.40 three
40. Something similarly dramatic happens Sections 9 10: inference systems
immediate prediction models (predictive) lower previsions, one specific
not.

51

fiDe Cooman, De Bock, & Diniz

inference systems described therethe skeptically cautious, skeptical IDMM
Haldane systemshave, given category set A, three different sets polynomials
HA . Nevertheless, gather Equations (49), (63) (72),
lower prevision H therefore prior predictive models P nA . count vector
NA prior lower probability:


= H (BA,
P nA ([])
) = min BA,
(x ) = 0.
xA


zero lower probability makes sure posterior lower previsions H (|)
uniquely determined prior lower prevision
posterior predictive models P nA (|)
H : infer Equations (49), (65) (73) indeed different
three types inference systems. fail see could come withlet alone
proved necessary results forthese three systems relying lower prevision credal
set theory.
canand musttake line argumentation even further. Theorem 4,
inference system satisfies (prior) representation insensitivity near-vacuous prior
predictive models, therefore, time consistency coherence [monotonicity], see
n
= 0
prior predictive lower previsions must satisfy H (BA,
) = P ([])
NA well. simply means impossible (prior) representation insensitive

coherent inference system lower prevision H uniquely determine conditional
therefore systematic way dealing inference
lower previsions H (|).
systems must able resolveor deal withthis non-unicity way. believe
approach involving coherent sets desirable gambles one mathematically
elegant ways this.
second feature allowed us, example, characterise IDMM immediate
predictions conservative ones satisfying number inference principles.
approach follow canat least principlealso used types inference
systems inference principles. key requirement inference principle
make amenable approach that, formulated property inference
system, preserved taking arbitrary non-empty infima. three inference
principles considering aboverepresentation insensitivity, specificity
concave surprisehave property, nothing prevents analysis
approach extended inference principle too.
complications see, point, technical mathematical nature. reader
doubt noticed proofs results later sections quite involved
technical, rely quite heavily properties polynomials simplex. feel
present paper made headway mathematical territory, instance
new discussion Bernstein positivity polynomials near Proposition 28
Appendix B. Conclusions paper De Cooman Quaeghebeur (2012),
characterisation Bernstein positivity mentioned open problem interesting
practical applications inferencenatural extensionunder exchangeability.
much remains open exploration, determined study mathematical
structure properties polynomials would certainly help alleviating technical
difficulties working inference principles inference systems.
paper opened feel interesting line
research foundations predictive inference, nevertheless provided answers
52

fiCoherent Predictive Inference Exchangeability

number ofif allopen problems formulated Conclusions earlier paper
De Cooman et al. (2009a), tried deal representation insensitivity immediate
prediction. first example: asked whether representation insensitive
coherent inference systems whose lower probability functions additive second
argument? suffices look Figure 1 see answer is, clearly, yes. Another
question was: representation insensitive coherent inference systems
mixing predictive systems?41 follows Equation (68) answer yes:
skeptical IDMM inference systems provides example. Finally, use infimum
sMC skeptically cautious inference system SC IDMM inference system sIDM ,
mentioned briefly Section 15, answer two questions. representation
insensitive coherent inference systems inequality Theorem 14.L6 strict?
representation insensitive coherent inference systems whose behaviour
gambles completely determined lower probability function? inference
system sMC provides positive answer questions.
inference systems mentioned above, apart IDMM Haldane
systems, appear first time. may appear contrived perhaps
even artificial, found useful constructing (counter)examples,
shaping intuition, building new models, Figure 1 argumentation clearly
indicate. might wonder whether representation insensitive and/or
specific coherent inference systems, cannot produced appropriately chosen infima
examples introduced here. suggest, candidates consideration,
inference systems derived using Walleys (1997) bounded derivative model,
inference systems constructed using sets infinitely divisible distributions,
recently proposed Mangili Benavoli (2013). framework provided here, well
simple characterisation results Theorems 7 11, quite useful addressing
similar problems.
end, want draw attention simple direct, quite appealing,
consequence argumentation Section 14: infinity precise proper
priors that, together multinomial likelihood, coherent Haldane posterior
predictive previsions. So, need improper priors justify posteriors,
proper priors job perfectly well. (precise-)probabilistic conclusion
follows easily looking problem using general powerful language
imprecise probabilities. Moreover, seen properties representation
insensitivity cannot satisfied precise probabilistic models. Finally, entire framework
conservative predictive inference using inference principles would impossible develop
within limitative context precise probabilities. shows distinct
advantages using imprecise probability models dealing predictive inference.

Acknowledgements
Gert de Coomans research partially funded project number 3G012512
Research Foundation Flanders (FWO). Jasper De Bock PhD Fellow Research
41. Loosely speaking: cannot written (specific kind of) convex mixture Haldane inference
system IDMM inference system; see paper De Cooman et al. (2009a, Section 5)
information.

53

fiDe Cooman, De Bock, & Diniz

Foundation Flanders wishes acknowledge financial support. Marcio Diniz
supported FAPESP (So Paulo Research Foundation), project 2012/14764-0
wishes thank SYSTeMS Research Group Ghent University hospitality
support sabbatical visit there. authors would thank three anonymous
reviewers many insightful comments suggestions aimed making paper
easier read cleaning misunderstandings. special thank great
Arthur Van Camp enthusiasm everything and, particular, helping us check
little examples.

Appendix A. Notation
appendix, provide list commonly used important notation,
defined first introduced.
notation

meaning

introduced

A, B, C,
IB
X, Xn
n
n
posi(A)
L(A)
L>0 (A)
L0 (A)




n
DA

category sets, events
indicator event B
variable, variable time n
number already observed variables
number observed variables
cone generated
set gambles
set positive gambles
set non-positive gambles
observed sample
observed count vector
prior predictive set desirable gambles
category set n future observations
posterior predictive set desirable gambles
prior predictive lower prevision
posterior predictive lower prevision
pooling map relabelling map
renaming bijection
category permutation
sample observations outside B eliminated
counting map
set count vectors n observations
set count vectors, zero
hypergeometric expectation operator
multinomial coefficient count vector
multinomial expectation operator
simplex probability mass functions
sum components x x B
Bernstein basis polynomial
set polynomials degree n

Section 1
Section 2.2
Section 1
Section 1
Section 1
Equation (1)
Section 2.2
Section 2.2
Section 2.2
Section 3
Section 5.4
Section 3

n c,
n c
DA

DA
n
P ()
P nA (|)

P nA (|),


$
B


NAn
NA , NA {0}
HynA (|)
()
MnnA (|)

B
BA,
V n (A)

54

Section 3
Section 3
Section 3
Sections 4.1&4.4
Section 4.2
Sections 4.3
Section 4.5
Equation (8)
Equation (9)
Section 5.3
Equation (10)
Equation (11)
Equation (13)
Equation (12)
Equation (12)
Equation (15)
Section 5.3

fiCoherent Predictive Inference Exchangeability

V (A)
V + (A)
V ++ (A)
HA

HA c
HA

H (|)
F

C
Crs
R
rB
iA
IrB,A
SA



subscript
subscript
subscript
subscript
subscript
subscript
subscript

A[]
++
V[]
(A)

V
NV
SC
IDM
SI
H
OI

diA (|)
DiA (|)
DiMnnA (|)
bnp

set polynomials
set Bernstein positive polynomials
set polynomials
positive int(A )
representing set polynomials
updated representing set polynomials
lower prevision induced HA

lower prevision induced HA c
set category sets
inference system
set coherent inference systems
set coherent inference systems
representation insensitive specific
extended relabelling map
restriction map
injection map
extended injection map
sampling expectation
lower probability function
imprecision function
surprise function
related vacuous inference system
related nearly vacuous inference system
related skeptically cautious inference system
related IDMM inference systems
related skeptical IDMM inference systems
related Haldane inference system
related original IDMM inference systems
categories already observed
set polynomials
positive int(A[] )
Dirichlet density
Dirichlet expectation operator
Dirichlet multinomial expectation operator
expansion polynomial p
Bernstein basis degree n

Section 5.3
Section 5.3
Section 10
Theorem 5
Equation (19)
Equation (17)
Equation (20)
Definition 6
Definition 6
Equation (24)
Theorem 12
Equation (26)
Equation (27)
Equation (28)
Equation (29)
Section 8
Equation (33)
Equation (34)
Section 15
Section 9
Section 10
Section 11
Section 12
Section 13
Section 14
Appendix
Equation (42)
Section 11
Appendix
Appendix
Appendix
Appendix

C
C
C
B

Appendix B. Multivariate Bernstein Basis Polynomials
n 0 NAn corresponds
Bernstein basis polynomial
(multivariate)

x
:=
degree n , given BA, ()
() xA x , . polynomials
number interesting properties (see instance Prautzsch, Boehm, & Paluszny, 2002,
Chapters 10 11), list here:
BB1. set {BA, :
NAn } Bernstein basis polynomials fixed degree n linearly
independent: N n BA, = 0, = 0 NAn .


55

fiDe Cooman, De Bock, & Diniz

NAn } Bernstein basis polynomials fixed degree n forms
BB2. set {BA, :
partition unity: N n BA, = 1.


BB3. Bernstein basis polynomials non-negative, strictly positive interior
int(A ) .
BB4. set {BA, : NAn } Bernstein basis polynomials fixed degree n forms
basis linear space polynomials whose degree n.
Property BB4 follows BB1 BB2.42 follows BB4 that:
BB5. polynomial p unique expansion terms Bernstein basis polynomials
called Bernstein expansionof fixed degree n deg(p),
words, unique count gamble bnp NAn that:

p() =
bnp ()BA, () .

(78)

n
NA

tells us [also use BB2 BB3] p() convex combination Bernstein
coefficients bnp (), NAn whence:
min bnp min p p() max p max bnp .

(79)

following proposition adds detail picture.
Proposition 27. polynomial p :
lim [min bnp , max bnp ] = [min p, max p] = p(A ).

n+
ndeg(p)

Proof Proposition 27. Since bnp converges uniformly polynomial p n +
(Trump & Prautzsch, 1996), sense
( )



lim maxn p
bnp () = 0,
n+ NA
n
ndeg(p)

find
lim

n+
ndeg(p)

min bnp min p =

lim

[
]
minn bnp () min p

n+ NA
ndeg(p)

[
( )]
minn bnp () p
n+ NA
n
ndeg(p)
( )



lim maxn p
bnp () = 0,
n+ NA
n



lim

ndeg(p)

therefore limn+,ndeg(p) min bnp min p. Furthermore, Statement (79), see
limn+,ndeg(p) min bnp min p. Hence indeed limn+,ndeg(p) min bnp = min p. proof
equality completely analogous.
42. see how: clearly polynomials definition linear combinations Bernstein basis polynomials,
possibly different degrees. terms, use BB2 raise degree common higher
degree nmultiply appropriate version 1. shows Bernstein basis polynomials
fixed degree n generating polynomials lower degrees. independent BB1.

56

fiCoherent Predictive Inference Exchangeability

Using results, prove number useful relations Bernstein
positivity polynomial positivity (the interior of) simplex. related
property first proved Hausdorff univariate case (Hausdorff, 1923, p. 124).
Proposition 28. Let p polynomial . Consider following statements:
(i) ( )p() > 0;
(ii) p V + (A), meaning n deg(p) bnp > 0;
(iii) p V ++ (A), meaning ( int(A ))p() > 0;
(iv) ( )p() 0.
(i)(ii)(iii)(iv).
Proof Proposition 28. first implication direct consequence Proposition 27:
infer (i) continuity p min p > 0 therefore, Proposition 27,
limn+,ndeg(p) min bnp = min p > 0, implies (ii).
prove (ii)(iii), assume n deg(p) bnp > 0,
consider int(A ). since BA, () > 0 NAn [BB3], since
assumption bnp 0 bnp () > 0 NAn , see
p() =



bnp ()BA, () bnp ()BA, () > 0.

n
NA

third implication immediate consequence continuity p.
following counterexample shows necessarily V + (A) = V ++ (A).
Running Example. go back polynomial q {H ,T ,U } defined Equation (41):
2
2
q() = H
H +
= (H )2 + H {H ,T ,U } .

already argued polynomial Bernstein positive. Nevertheless,
obviously positive interior {H ,T ,U } .

quite easy trace effect Bernstein expansion multiplying
Bernstein basis polynomial:
Proposition 29. polynomials p , natural n deg(p), NA {0}
NAn+mA :

()
bn ( )

p
n+mA
(

)()
bpBA, () =

0
otherwise.
Proof Proposition 29. Observe that:
(
)

n
pBA, =
bp ()BA, BA, =
bnp ()BA, BA,
n
NA

n
NA

57

fiDe Cooman, De Bock, & Diniz

=



bnp ()

n
NA

( + )
BA,+ ,
()()

use uniqueness (Bernstein) basis expansion.
allows us prove following simple interesting result Bernstein positivity:
Proposition 30. Consider NA {0} polynomial p . Then:
pBA, V + (A) p V + (A).
Proof Proposition 30. First, assume pBA, V + (A), natural n
n

deg(p) bn+m
pBA, > 0. follows Proposition 29 bp > 0,
therefore p V + (A).
Assume, conversely, p V + (A), n deg(p) bnp > 0.
+

follows Proposition 29 bn+m
pBA, > 0, therefore pBA, V (A).

Appendix C. Dirichlet Distribution
density diA (|) Dirichlet distribution hyperparameter RA
>0 given by:
diA (|) :=


(A )
xx 1 int(A ),
(
)
x
xA
xA

polynomial p define corresponding expectation as:43


(A )
p()
DiA (p|) :=
xx 1 d.
(
)
x

xA
xA

particular,


(

)


(A )
xx 1
(x )

xA
xA
xA
( )
( )

n
(A )
(mx + x )
1
n
=
x (mx ) ,
=
(n + )
(x )
(n)

DiA (BA, |) =

n


xmx

(80)

xA

xA

using ascending factorial (r) := (+r)
() = ( + 1) . . . ( + r 1), R
r N0 .
Dirichlet distribution used prior combination multinomial
likelihood, leading so-called Dirichlet multinomial distribution, described
follows. probability observing (a sample n 0 observations with) count vector
NA {0} multinomial process Dirichlet prior density diA (|) given by:

n
DiMnA ({}|) :=
CoMnnA ({}|) diA (|)


43. integrals section interpreted multiple Riemann integrals.

58

fiCoherent Predictive Inference Exchangeability


BA, () diA (|) = DiA (BA, |),

=


second equality follows Equation (16). Therefore, generally, take
expansion polynomial p Bernstein basis polynomials degree n deg(p):
DiA (p|) =



bnp () DiA (BA, |) =

n
NA

= DiMnnA



bnp () DiMnnA (I{} |)

n
NA

(
n
NA

)

n
bp ()I{} = DiMnnA (bnp |),

Dirichlet multinomial expectation count gamble bnp . general
useful relationship Dirichlet expectation polynomial p, Dirichlet
multinomial expectation Bernstein expansion bnp . Although expectations
strictly speaking defined RA
>0 , extend definition continuously
elements RA
\
{0}

taking
appropriate
limits, Equation (80) indicates.

C.1 Special Properties Dirichlet Distribution
recall interesting properties Dirichlet distribution. begin
updating property:
Proposition 31 (Updating). category set A, polynomial p V (A), count
vector NA {0} RA
>0 :
DiA (pBA, |) = DiA (BA, |) DiA (p| + ).
Proof Proposition 31.

DiA (pBA, |) =
p()BA, () diA (|)

(
)


mx (A )
xx 1
=
p()
x
(
)

x

xA
xA
xA
(
)


(mx + x )

(A )
=
p() diA (| + )
(mA + )
(x )

xA

= DiA (BA, |) DiA (p| + ),
last equality follows Equation (80).
Next, turn so-called renaming property:
Proposition 32 (Renaming). category sets C bijective
(one-to-one onto) map : C, polynomial p V (C) RA
>0 :
DiA (p R |) = DiC (p|R ()).
59

fiDe Cooman, De Bock, & Diniz

Proof Proposition 32. Due linear nature Dirichlet expectation, clearly
suffices prove property Bernstein basis polynomials p = BC, ,
NC {0}. Observe R bijection too. Then, using Equation (80), let := R ()
:= R1 (), z = 1 (z) mz = n1 (z) z C, = C
nA = mC , get:
(
)
(mz + z )
mC
(C )
DiC (BC, |R ()) = DiC (BC, |) =
(mC + C )
(z )
zC
( )
(n1 (z) + 1 (z) )
(A )
nA
=
(nA + )
(1 (z) )
zC
( )
(nx + x )
(A )
nA
=
= DiA (BA, |),
(nA + )
(x )
xA

take account int(A ):
(BC, R )() = BC, (R ())
(
)
(
)
(
)
mC mz
mC
mC n1 (z)
mz
=
1 (z)
1 (z) =
R ()z =



zC
zC
zC
( )
nA
xnx = BA, (),
=

xA

see indeed DiC (BC, |R ()) = DiA (BC, R |).
so-called pooling property generalises renaming property:
Proposition 33 (Pooling). category sets onto
map : D, polynomial p V (D) RA
>0 :
DiA (p R |) = (p|R ()).
Proof Proposition 33. Due linear nature Dirichlet expectation, suffices
prove property Bernstein basis polynomials p = BD, , ND {0}.
Also, take account renaming property Proposition 32, enough consider
following special case, non-empty set different categories b,
c belonging it, let := {b, c} := {d}, define letting
(x) := x x (b) = (c) := d.
one hand, taking account Equation (80), letting := R () :
(
)
(mz + z )
mD
(D )
(BD, |R ()) = (BD, |) =
(mD + )
(z )
zD
(
)
mD
(D )
(md + ) (mz + z )
=
.
(81)
(mD + ) (d )
(z )
zDo

hand,

60

fiCoherent Predictive Inference Exchangeability

DiA (BD, R |)
(
)
)
(
mD
md
mz
(b + c )
z
diA (|)
=


zDo
)
(


(A )
mD

=
(b + c )md bb 1 cc 1
zmz +z 1

xA (x )
zDo
(
)
)
(



md

(A )
mD

bk+b 1 cmd k+c 1
zmz +z 1
=
k
(
)

x

xA
zDo
k=0

)
(
)
(



(A )
md (k + b )(md k + c ) zDo (mz + z )
mD

=
.

k
(mD + )
xA (x )
k=0

So, compare results recall = , z = z z = b + c ,
see must prove that:
)
md (

1
md
(md + b + c )
=
(k + b )(md k + c )
(b + c )
(b )(c )
k
k=0

equivalently, using ascending factorials:
(b + c )(md ) =

)
md (

md
b (k) c (md k) .
k

(82)

k=0

see proving pooling property essentially equivalent proving Equation (82),
binomial theorem ascending factorials. well-known result,
follows fact ascending factorials Sheffer sequences binomial type (Sheffer,
1939). completeness, give proof here, easy,
shown hold prove pooling property particular case
= {a}, category different b, c d. = {a, b, c} = {a, d},
case rewrite Equation (81) as:
(BD, |R ())
(
)
+ md
(a + b + c )
(md + b + c ) (ma + )
=

(ma + md + + b + c ) (b + c )
(a )
whereas
DiA (BD, R |) =
let
1 (
:=
0
1

(1

0

(1

=

1a

)md bb 1 (1

)md ama +a 1

(

(
)
+ md (a + b + c )


(a )(b )(c )



1a

b )c 1 ama +a 1 db

bb 1 (1

b )

c 1

)
da

)

db da
( 1
)
1
md +b +c 1 +a 1
b 1
c 1
=
(1 )


(1 t)
dt da
0

0

0

0

61

fiDe Cooman, De Bock, & Diniz

= B(ma + , md + b + c )B(b , c ) =

(ma + )(md + b + c ) (b )(c )
,
(ma + md + + b + c ) (b + c )

using well-known evaluation Beta function terms Gamma functions.
Finally, look properties related restriction.
Proposition 34 (Restriction). category sets B B A,
polynomial p V (B), RA
>0 r N0 :
DiA (IrB,A (p)|) =

(deg(p) + r + B )
(A )
DiB (p|rB ()).
(deg(p) + r + )
(B )

Proof Proposition 34. Let n := deg(p) + r, due linearity Dirichlet
expectation operator, Equations (29) (80):

DiA (IrB,A (p)|) =
bnp () DiA (BA,iA () |)
n
NB



( )
n
(A )
xB (nx + x )
xA\B (x )


=
(n + )
n
xA\B (x )
xB (x )
NB
( )

n
(A ) (nx + x )
=
bnp ()
(n + )
(x )
n


bnp ()

NB

=


n
NB

=

xB

bnp ()

(A ) (n + B )
DiB (BB, |rB ())
(n + ) (B )

(A ) (n + B )
DiB (p|rB ()),
(n + ) (B )

concluding proof.

Appendix D. Original IDMM Inference System Walley
Bernard
IDMM inference system sIDM , introduced Section 12, differs one
originally proposed Walley Bernard (1999).44 appendix, discuss original
IDMM inference system, denote sOI , explain related ours,
illustrate advantages version one Walley Bernard.
D.1 Defining Original IDMM Inference System
R>0 , category set A, consider following set polynomials:

:= {p V (A) : ( KsA ) DiA (p|) > 0}
HOI,A

= {p V (A) : ( int(A )) DiA (p|s) > 0} .
44. Strictly speaking, Walley Bernard propose inference system sense, rather
collection prior posterior predictive lower previsions category set A. inference system
call original IDMM inference system one produces predictive lower previsions.

62

fiCoherent Predictive Inference Exchangeability

reasons become clear shortly, call inference system sOI defined

sOI (A) := HOI,A
category sets A,

original IDMM inference system hyperparameter > 0. Updating done much
NA {0}:
way inference system sIDM Section 12.

= {p V (A) : ( int(A )) DiA (p|
+ s) > 0} ,
HOI,A
c

compared Equation (54). leave exercise reader
check sOI coherent representation insensitive.45 However, illustrated
counterexample Section D.3, sOI specific.
predictive models sOI easily derived mimicking approach used
Section 12 derive predictive models sIDM ; see Equations (56) (57).
NAn :
n N0 , n N
{
}
s,n
= f L(An ) : ( int(A )) DiA (MnnA (f )|
+ s) > 0 ,
DOI,A
c

(83)

=
P s,n
OI,A (f |)

(84)


inf
int(A )

+ s) gambles f .
DiA (MnnA (f )|

latter expression motivates refer sOI original IDMM inference system:
predictive lower previsions coincide proposed Walley Bernard (1999).
Using Equation (83) n = 1, mimicking argument proof Equation (59)
Appendix E.7, see
s,1
=
DOI,A
c

{
f L(A) : f >

}
1
s,1

NA {0}.
f (x)mx = DIDM,A
c

xA

tells us IDMM original IDMM immediate prediction
models. corresponding immediate predictive lower previsions original IDMM
well-known course identical ones produced version IDMM
inference system, given Equation (60). However, examples next section
illustrate, equality extend beyond immediate prediction: IDMM
original IDMM different coherent inference systems, leads us general
important conclusion coherent inference systems completely determined
immediate prediction models.
Nevertheless, approaches closely related; comparing Equations (84) (57),
NAn
see n N0 , n N
0

,n
= inf0 P sOI,A
gambles f .
P s,n
(f |)
IDM,A (f |)
0<s <s

45. proof similar one sIDM [see Theorem 21].

63

(85)

fiDe Cooman, De Bock, & Diniz

D.2 Original IDMM Inference System Monotone
hyperparameter original IDMM inference system usually interpreted
degree caution. Higher values often claimed produce inferences
cautious less informative. following quote Walley Bernard (1999,
Section 2.4) makes explicit:
B event concerning future observations, IDMM(s) produces intervals
posterior probabilities [P (B|), P (B|)] nested become wider
increases. means inferences produced two IDMMs different
values always consistent other, effect increasing
simply make inferences cautious less informative.
Similar statements found related papers Walley (1996, Section 2.5) Bernard
(2005, Section 4.6). Although indeed true many inferences, including many important
onesfor example, immediate predictions, hold event concerning
future observations, illustrated following example, lower probability
event concerning two future observations shown initially increase s.
Example 1. Consider situation possibility space consists two elements
only, say heads (H) tails (T ), observed once, n = 2
= (mH , mT ) = (1, 1). interested predictive lower probability

next two trials, heads tails observed once: n = 2 looking
= {(H, ), (T, H)},
= (mH , mT ) = (1, 1).
predictive lower probability event []
original IDMM inference system, following formula provides closed-form
expression:
=
P s,n
|)
OI,A (I[]

+ s) = inf DiA (BA,
+ s)
DiA (MnnA (I[]
|
)|
int(A )
( )
1
n
(mx + stx )(mx )
= inf
(n)


int(A ) (n + s)
xA
inf

int(A )

= inf

0<t<1

2(1 + st)(1 + s(1 t))
2(1 + s)
=
.
(2 + s)(3 + s)
(2 + s)(3 + s)

initially increases s; see Figure 2.
conclude P s,n
|)
OI,A (I[]

(86)


version IDMM inference system, statement made aforementioned
quote hold event concerning future observations. follows trivially
Equation (85). illustrate next example.
Example 2. Consider problem Example 1. time, solve using version
IDMM. result depicted Figure 2, function hyperparameter s.
s,n
P IDM,A
non-increasing function s. Indeed,
contrast P s,n
(I[]
|),
|)
OI,A (I[]

0 ,n
1

=
(I[]
0 < < 1
P sOI,A

|)
slim
0 0
3
s,n
=
P IDM,A (I[]
|)

2(1 + s)

P s,n
=
1
|)
OI,A (I[]
(2 + s)(3 + s)
closed-form expression find combining Equations (85) (86).
64



fiCoherent Predictive Inference Exchangeability

0.36


P s,n
|)
OI,A (I[]

0.34
0.32


P s,n
|)
IDM,A (I[]

0.3
0.28

0

0.5

1

1.5

2

Figure 2: Lower probability observing two different outcomes next two experiments, given possibility space consists two categories,
already observed once: solutions according sOI (solid line) sIDM
(dashed solid line); see Examples 1 2 information.

Clearly, inferences sOI sIDM differ: suffices compare results
Examples 1 2; see Figure 2 well. Therefore, seems clear Walleys (1996, p. 51)
statement [. . . ] allowed vary 0 s, produces exactly
inferences IDM = s. equivalently, sOI sIDM produce
inferences, taken apply immediate prediction only.
D.3 Original IDMM Inference System Specific
announced Theorem 21, version IDMM inference system specific.
show that, least values hyperparameter s, true original
version.
NBn . B f L(B n ):
Consider n N0 , n N
s,n
( int(A )) DiA (MnnA (f IB n )|iA ()
+ s) > 0
f IB n DOI,A
ciA ()

+ srB ()) > 0,
( int(A )) DiB (MnnB (f )|
last equivalence consequence Propositions 41 34 fact
= .
particular B A, hard see sB = {srB () : int(A )},
rB (iA ())
implies that:
s,n
s,n
( sB ) DiB (MnnB (f )|
+ ) > 0 f DIDM,B

f IB n DOI,A
ciA ()
c,

therefore also:

B n ) = inf DiB (MnnB (f )|
+ ) = P s,n
P s,n
OI,A (f |iA (),
IDM,B (f |).
B

65

fiDe Cooman, De Bock, & Diniz

hand, due Equation (SP1), sOI specific, would that:
B n ) = P s,n


= P s,n
P s,n
OI,A (f |iA (),
OI,B (f |rB (iA ()))
OI,B (f |).
P s,n

Hence, order sOI specific, necessary P s,n
OI,B (|)
IDM,B (|)
coincide. illustrated examples previous section, necessarily
case. Therefore, sOI always specific. counterexample provided,
difference occurs < 1 only, whereas practice, usually chosen either 1
2 (Walley & Bernard, 1999, Section 2.4). would interesting see whether similar
counterexamples constructed 1.
original IDMM inference systems specific, apparently contradicts Theorem 11 De Cooman et al. (2009a), seems state are. fact,
theorem states original IDMM immediate prediction models satisfy weaker
specificity condition, tailored immediate prediction only. Since immediate prediction
models original IDMM IDMM coincide, contradiction.

Appendix E. Proofs Additional Results Technical
E.1 Proofs Results Section 4
Proof Theorem 4. sake notational simplicity, use intuitive notation f (Xk )
extnk (f ). give proof general definition, terms sets desirable
gambles. proof lower previsions follows immediately.
Consider category set A, n N, 1 k n gamble f
n may assume without loss generality singleton.
f (Xk ) DA
already implies f 6 0, coherence [D4]. Hence particular f 6= 0 max f > 0.
Assume ex absurdo f 6> 0, must f (a) < 0. Define
gamble g letting g(a) := f (a) g(x) := max f > 0 x \ {a}.
g f therefore g(Xk ) f (Xk ), implies, coherence [use D2 D3],
n . let := max f f (a) > 0 := f (a)/ > 0, define
g(Xk ) DA
n , > 0.
gamble h := g/ = + IA\{a} , also, coherence [D3], h(Xk ) DA
consider natural number N 2, follows repeatedly applying pooling
n
renaming invariance appropriate manner + I{a1 } (Zk ) D{a
,
1 ,...,aN }
Zk variable assumes value a1 Xk 6= assumes value
{a2 , . . . , } Xk = a. repeatedly applying category permutation invariance, find
n
+ I{a` } (Zk ) D{a
` {1, . . . , N }. Coherence [D3] tells us
1 ,...,aN }
N
n
N + 1 = `=1 [ + I{a` } (Zk )] D{a
. leads contradiction coherence
1 ,...,aN }
[D4] choose N large enough.
E.2 Proofs Results Section 7
Proposition 35. n N : () = R ( ()).
Proof Proposition 35. Consider z D,
Tz () = |{k {1, . . . , n} : (xk ) = z}| =


yA : (y)=z

66

|{k {1, . . . , n} : xk = y}|

fiCoherent Predictive Inference Exchangeability



=

Ty () = R ( ())z ,

yA : (y)=z

concluding proof.
Lemma 36. n N, NAn Dn :

1
1
I{} () =

().
()
(R ()) [R ()]
[]


Proof Lemma 36. Consider map : Dn R defined := [] I{} .
permutation index set {1, . . . , n} Dn , see


() =
I{} () =
I{(1 )} ()
[]

[]



=



I{} () =

[]

I{} () = (),

[]

tells us permutation invariant thereforeconstant atoms
[], NDn . means that, obvious notations, = N n ()I[] .

() > 0 implies [] = , therefore,
Proposition 35, () = () = R ( ()) = R () therefore [R ()]. tells
us () = 0 unless = R () therefore = (R ())I[R ()] .
plug f := 1 Equation (87), see


() =
() =
(R ())I[R ()] () = (R ())(R ()).
Dn

Dn

Lemma 37. n N NDn :


BD, R =

BA,

n : R ()=
NA


Proof Lemma 37. ,
( ) (
)nz
n
(BD, R )() =
x

zD x1 ({z})
( )
( )

n
nz
=
z


nz
z
zD N

( )
n
=

=

1 ({z})

(


n : R ()=
NA



n:
NA

R ()=

(

n


67

xA

z

xmx

x1 ({z})

xmx

) (

xA

)

concluding proof.



xmx =

zD

)

nz

|1 ({z})


n:
NA

R ()=

BA, (),

fiDe Cooman, De Bock, & Diniz

lemma allows us prove two related propositions.
Proposition 38. n N gambles f Dn : MnnA (f ) = MnnD (f ) R .
Proof Proposition 38. First all, count vector NAn
HynA (f |) =



1
1
f () =
I{} ()f ()
()
()
n
[]

[]



=

f ()

Dn

I{} ()

(87)

[]

1

()
f ()
(R ()) [R ()]
n



=



=

1
()



1
(R ())



f () = HynD (f |R ()),

[R ()]

fourth equality follows Lemma 36. Therefore indeed:


MnnA (f ) =
HynA (f |)BA, =
HynD (f |R ())BA,
n
NA

=



n
NA

HynD (f |)

n
ND

=





BA,

n : R ()=
NA


HynD (f |)(BD, R ) = MnnD (f ) R ,

n
ND

fourth equality follows Lemma 37.
Proposition 39. polynomials p n N0 n deg(p):
bnpR = bnp R .
Proof Proposition 39. find expanding p appropriate Bernstein basis:
(
)

n
p R =
bp ()BD, R =
bnp ()(BD, R )
n
ND

=



bnp ()

n
ND

=



n
ND



BA, =





bnp (R ())BA,

n
n : R ()=
ND
NA


n : R ()=
NA


(bnp R )()BA, ,

n
NA

third equality follows Lemma 37. desired result follows
uniqueness expansion (Bernstein) basis.
Proof Theorem 7. Fix category sets onto map : D,
gamble f Dn . use notation HA := (A)
n, n N,
68

fiCoherent Predictive Inference Exchangeability

HD := (D), transform Condition (RI2) using equivalence Condition (18).
:= ():

one hand, letting
n
f DA
MnnA (f ) HA MnnD (f ) R HA
n
n
n
BA,
f DA
c
MnA (f ) HA BA,
(MnD (f ) R ) HA ,

second equivalences follow Proposition 38. hand, recalling
= R ( ())
= R ()
Proposition 35:
()
n
f DD
MnnD (f ) HD
n
n
BD,R ()
f DD
c
MnD (f ) HD .

tells us equivalences Condition (RI2) rewritten as:
MnnD (f ) R HA MnnD (f ) HD
n
n
BA,
(MnD (f ) R ) HA BD,R ()
MnD (f ) HD .

proof complete observe (and recall discussion Section 5.3
Appendix B) varying n N f L(Dn ), let p := MnnD (f ) range
, let
:= ()
range
polynomials , varying n N
count vectors NA .
Proof Theorem 8. Let, ease notation := inf iI , coherent using Equation (25). Consider category sets onto map : D,
p V (D) NA {0}. Then, using representation insensitivity
coherent Theorem 7:
(p R )BA, (A) (i I)(p R )BA, (A)
(i I)pBD,R () (D) pBD,R () (D),
concludes proof.
Proposition 40. , (B ) = rB ( ()).
Proof Proposition 40. Immediate, since B sample whose components belong
B, category B, number times occurs B exactly
number times occurs .
Proof Proposition 9. Consider B , let, simplicity notation = iA ().
since NBn , n := deg(p) + r
(
)
( )
n
n
iA ()x
BA,iA () () =
x
=
nx x = BB, (),
iA ()

xA

xB

see indeed:
IrB,A (p|) =



bnp ()BA,iA () () =

n
NB


n
NB

69

bnp ()BB, () = p().

fiDe Cooman, De Bock, & Diniz

Proof Proposition 10. deg(p) + r = 0, r = 0 p = c R, trivially
IrB,A (p|) = I0B,A (c)() = c. let us assume deg(p) + r > 0. First all, observe
deg(p)+r

NB

:
(
)
(
)
deg(p) + r iA ()x
deg(p) + r nx
BA,iA () () =
x
=
x
iA ()

xA
xB
{
deg(p)+r
B
BB, (|+
B > 0
B)
=
0
otherwise.

(88)

therefore already follows Condition (29) IrB,A (p|) = 0 B = 0. Let us therefore
assume B > 0. Condition (29) Equation (88) tell us that:

IrB,A (p|) =
bdeg(p)+r
()BA,iA () ()
p
deg(p)+r

NB

deg(p)+r



=

bdeg(p)+r
()B
p

BB, (|+
B)

deg(p)+r

NB

deg(p)+r

deg(p)+r



= B

bdeg(p)+r
()BB, (|+
p
B ) = B

p(|+
B ),

deg(p)+r

NB

concludes proof.
Proposition 41. n N gambles f B n :
MnnA (f IB n ) = IrB,A (MnnB (f )), r := n deg(MnnB (f )).
Proof Proposition 41. First all, count vector NAn thatwith
slight abuse notation:
HynA (f IB n |) =


1
1
(f IB n )() =
()
()
[]



f ()

[]B n

zero unless = iA () NBn . case, since obviously () = (),
[iA ()] B n []again slight abuse notation:
HynA (f IB n |iA ()) =

1
f () = HynB (f |).
()
[]

Therefore, recall Condition (29):


HynB (f |)BA,iA ()
MnnA (f IB n ) =
HynA (f IB n |iA ())BA,iA () =
n
NB

n
NB

= IrB,A (MnnB (f )),
r := n deg(MnnB (f )).
70

fiCoherent Predictive Inference Exchangeability

Proof Theorem 11. Fix category sets B B A, n, n N,
gamble f B n . use notation HA := (A) HB := (B),

transform Condition (SP2) using equivalence Condition (18). one hand,
:= ()
r := n deg(MnnB (f )):
letting
n
f IB n DA
MnnA (f IB n ) HA IrB,A (MnnB (f )) HA
n
n
r
n
BA,
f IB n DA
c
MnA (f IB n ) HA BA,
IB,A (MnB (f )) HA ,

second equivalences follow Proposition 41. hand, recalling
B ) = rB ( ())
= rB ()
Proposition 40:
(
n
f DB
MnnB (f ) HB
n
n
B BB,rB ()
f DB
c
MnB (f ) HB .

tells us equivalences Condition (SP2) rewritten as:
IrB,A (MnnB (f )) HA MnnB (f ) HB
r
n
n
BA,
IB,A (MnB (f )) HA BB,rB ()
MnB (f ) HB .

proof complete recall discussion Section 5.3 Appendix B
varying n N f L(B n ), let p := MnnB (f ) = CoMnnB (HynB (f )) range
polynomials B r = n deg(MnnB (f )) range elements N0 ,
, let
:= ()
range count vectors NA .
varying n N
Proof Theorem 12. Let, ease notation := inf iI , coherent using
Equation (25). Consider category sets B B A, p V (B),
NA {0} r N0 . Then, using specificity :
IrB,A (p)BA, (A) (i I)IrB,A (p)BA, (A)
(i I)pBB,rB () (B) pBB,rB () (B),
concludes proof.
E.3 Proofs Results Section 8
Proof Proposition 13. sufficiency, fix category set A, gamble g A, count
vector NA {0}. Condition (RI4) := g(A), := g, f := idD yields Condition (RI5).
necessity, fix category sets onto map : D,
gamble f D, count vector NA {0}. Observe (f )(A) = f (D)
r f (D)



Rf ()r =
mx =
mx
xA : (f )(x)=r

zD : f (z)=r xA : (x)=z



=

zD : f (z)=r

71

R ()z = Rf (R ())r ,

fiDe Cooman, De Bock, & Diniz

Rf = Rf R . infer invoking Condition (RI5) twice that:
1
1
f DA
c id(f )(A) D(f
)(A) cRf ()
1
idf (D) Df1 (D) cRf (R ()) f DD
cR (),

concluding proof.
Proof Theorem 14. arguments proof rely heavily following expression
lower probability function:
{
}
1
(n, k) = sup R : I{a} D{a,b}
c(k, n k)
}
{
= sup R : ak bnk [a ] ({a, b})
(89)
related expressions equivalent representation insensitivity
Bernstein coherence [B3]. expressions follow Equations (32) (33), Bernstein
coherence [B3] representation insensitivity form (RI4).
L1. Immediate Bernstein coherence fact (n, k) lower probability:
use Equation (89), B2 B4.
L2. Fix non-negative integers n, k ` k + ` n. Consider real
< (n, k) < (n, `), follows applying Equation (89) Condition (RI4)
xk y` znk` [x ] ({x, y, z}) xk y` znk` [y ] ({x, y, z}), whence,
Bernstein coherence [B3], xk y` znk` [(x + ) ( + )] ({x, y, z}). Applying
Equation (89) Condition (RI4) tells us uk+` znk` [u ( + )] ({u, z}),
whence + (n, k + `).
L3, L4 L5 immediate consequences L1 L2.
L6. Consider category set := {a, b} count vector := k
mb := n k. Define gamble g g(a) := (n + 1, k + 1) g(b) := (n + 1, k).
g(a) g(b) L5, therefore coherence [P5 P3] predictive lower
prevision P 1A (|) tells us P 1A (g|) = g(b) + [g(a) g(b)]P 1A ({a}|) = (n + 1, k) +
(n, k)[(n + 1, k + 1) (n + 1, k)] [see Equation (33)]. clearly suffices prove
P 1A (g|) (n, k) = P 1A ({a}|). Consider < P 1A (g|), follows using
Equation (31) that:
ak bnk [g(a)a + g(b)b ] (A).
(90)
Also, > 0, ak+1 bnk [a g(a) + ] (A) ak bn+1k [a g(b) + ] (A),
therefore, coherence [B3], recalling + b = 1,
(A) 3 ak+1 bnk [a g(a) + ] + ak bn+1k [a g(b) + ]
= ak bnk [a g(a)a g(b)b + ]. (91)
Combining Statements (90) (91) using coherence [B3], leads ak bnk [a + ]
(A), whence (n, k) , completes proof.
L7. Use L1 L5 find (n, k)[(n + 1, k + 1) (n + 1, k)] 0, use L6.
L8. sn 0 follows L4, need prove sn+1 sn , equivalently,
(n, 1) (n + 1, 1)[1 + (n, 1)]. Indeed:
(n, 1) (n + 1, 1) + (n, 1)[(n + 1, 2) (n + 1, 1)]
72

fiCoherent Predictive Inference Exchangeability

(n + 1, 1) + (n, 1)[2(n + 1, 1) (n + 1, 1)]
= (n + 1, 1) + (n, 1)(n + 1, 1),
first inequality follows L6 k = 1, second L4 L1.
L9. inequalities hold trivially n = 0, due L1. consider n N,
category sets := {x, y} B := {x1 , x2 , . . . , xn , y}. Let 0 < < := > 0.
Since (1, 1) > , see x [x ] (A), equivalently, x [x (1 ) ] (A),
since x + = 1. Representation insensitivity [use Equation (89) Condition (RI4)]
tells us xk [xk (1
) ] ({xk , y}), specificity [use Theorem 11] allows us
infer ( nk=1 xk
)[xk (1 )
n ] (B), k {1, . . . , n}.
n
infer coherence [B3] ( k=1 xk )[ k=1 xk (1 ) ny ] (B), apply
representation insensitivity get xn [x (1 ) ny ] (A). Since = 1 x ,
n
equivalent xn [x (1 + n) n] (A). shows (n, n) 1+n
, using
Equation (89). rest proof immediate.
E.4 Proofs Results Section 9
Proof Theorem 15. V coherent obvious, category set F,
V (A) = V + (A) Bernstein coherent set polynomials .
prove representation insensitivity, use Theorem 7. Consider category sets
onto map : D, p V (D) NA {0}.
indeed
(p R )BA, V + (A) p R V + (A) p V + (D) pBD,R () V + (D),
first last equivalences follow Proposition 30, second one
Lemma 48 K = A.
E.5 Proofs Results Section 10
Proof Theorem 16. V coherent obvious, category set F,
V (A) = V ++ (A) obviously convex cone includes V + (A) [Proposition 28]
contain zero polynomial: V ++ (A) therefore Bernstein coherent set polynomials
.
prove representation insensitivity, use Theorem 7. Consider category sets
onto map : D, p V (D) NA {0}.
indeed
(p R )BA, V ++ (A) ( int(A ))p(R ())BA, () > 0
( int(A ))p(R ()) > 0
( int(D ))p() > 0
( int(D ))p()BD,R () () > 0 pBD,R () V ++ (D),
second fourth equivalences follow Bernstein positivity Bernstein
basis polynomials Proposition 28, third one Lemma 48 K = A.
73

fiDe Cooman, De Bock, & Diniz

prove specificity, use Theorem 11. Consider category sets B
B A, p V (B), NA {0} r N0 . indeed:
IrB,A (p)BA, V ++ (A) ( int(A ))IrB,A (p|)BA, () > 0
( int(A ))IrB,A (p|) > 0
( int(B ))p() > 0
( int(B ))p()BB,rB () () > 0 pBB,rB () V ++ (B),
second fourth equivalences follow Bernstein positivity Bernstein
basis polynomials Proposition 28, third one Lemma 52 K = A.
E.6 Proofs Results Section 11
Below, use convenient device identifying, proper subset B A, element
B unique corresponding element = iA () whose components outside
B zero:
(x B)x = x (x \ B)x = 0.
observe that, using convention, identify int(A[] ) subset ,
characterise follows:
: int(A[] ) (x A)(x > 0 mx > 0).
Proof Proposition 17. clearly suffices prove V + (A) HSC,A 0
/ HSC,A .
first statement easy prove ASC,A trivially includes non-constant
Bernstein basis polynomials, Proposition 28. Since V + (A) consists finite, strictly positive
linear combinations non-constant Bernstein basis polynomials, immediately
V + (A) HSC,A .
prove second statement, suppose ex absurdo 0 HSC,A . implies
++
finitely many nk > 0, count vectors k NAnk pk V[
(A)
k]

0 = k pk BA,k . always possible find (at least) one count vector, 1 say,
A[k ] 6 A[1 ] k. words, either A[k ] = A[1 ]
A[k ] \ A[1 ] 6= . consider int(A[1 ] ). A[k ] \ A[1 ] 6= ,
++
BA,k () = 0. A[k ] = A[1 ], BA,k () > 0, moreover, since pk V[
(A),
k]

pk () > 0. Hence 0 = k pk ()BA,k () > 0, contradiction.
Lemma 42. Consider NA {0} p HSC,A, , ` N, nk N0

++
+ nk > 0, k NAnk pk V[+
(A) p = `k=1 pk BA,k .
k]

SA, (p) = {K : A[ + k ] K k {1, . . . , `}}
therefore
min SA, (p) = min {A[ + k ] : k {1, . . . , `}} .
Proof Lemma 42. second statement trivial, given first. restrict
attention proving first statement.
Assume first A[ + r ] K r {1, . . . , `}. clearly K 6= ,
since + nr > 0. may assume without loss generality A[ + r ] minimal
74

fiCoherent Predictive Inference Exchangeability

element set {A[ + k ] : k {1, . . . , `}}. Consider int(A[+r ] ), whence
K . k {1, . . . , `} A[ + k ] = A[ + r ]and
++
clearly least one kwe see pk () > 0 since pk V[+
(A),
k]
BA,k () > 0, whence (pk BA,k )() > 0. k must A[ + k ] \
A[ + r ] 6= , therefore (pk BA,k )() = 0 since BA,k () = 0. guarantees

p() = `k=1 (pk BA,k )() > 0, whence indeed K SA, (p), since already know
K , A[] A[ + r ] K K 6= .
Assume, conversely, K SA, (p), implies 6= K A[] K,
K p() 6= 0. Observe A[ + k ] = A[]A[k ],
assume ex absurdo A[ + k ] * K therefore A[k ] * K k {1, . . . , `}.
Fix k {1, . . . , `}, x A[
/ K, therefore x = 0,
k ] x
whence BA,k () = 0. shows p() = `k=1 (pk BA,k )() = 0, contradiction.
Lemma 43. Consider NA {0}, p V (A) n N n deg(p).
NAn :
bnp () 6= 0 (K min SA, (p))K \ A[] A[].
Proof Lemma 43. Fix NAn . prove contraposition, suppose
K min SA, (p), K \ A[] * A[] therefore K * A[ + ], since
A[ + ] = A[] A[]. Hence, A[ + ]
/ SA, (p). Since moreover 6= A[ + ]
A[] A[ + ], infer Equation (46) p|B = 0, let, ease
notation, B := A[ + ]. rewrite [see Lemma 47]:
0 = p|B =


n
NA

bnp ()BA, |B =


n
NB

bnp ()BA, |B =



bnp ()BB, .

n
NB

Due uniqueness Bernstein expansion, possible bnp () = 0
n
n
NA[+]
. concludes proof since, clearly, NA[+]
.

Proof Proposition 18. First, assume p HSC,A, , implying p = `k=1 pk BA,k
++
` N, nk N0 + nk > 0, k NAnk pk V[+
(A). already
k]
follows Lemma 42 p =
6 0 min SA, (p) = min {A[ + k ] : k {1, . . . , `}}.
Consider K min {A[ + k ] : k {1, . . . , `}} int(K ).
k, either A[ + k ] = K A[ + k ] \ K 6= . A[ + k ] = Kwhich
happens least one k, due choice Kthen pk () > 0 BA,k () > 0.
A[ + k ] \ K 6= , since A[] K, A[k ] \ K =
6 , implying BA,k () = 0.
Hence, p() > 0. Since holds int(K ), find p|K V ++ (K).
Assume, conversely, p V (A)\{0}
p|K V ++ (K)
K n min SA, (p).
n
Fix n N n deg(p), p = N n bp ()BA, = bp ()BA, ,

{
}
:= NAn : bnp () 6= 0 . Since p 6= 0, infer Equation (46) min SA, (p) 6=
[observe SA, (p)]. know Lemma 43 , least
one K min SA, (p) K \ A[] A[]. Let us pick K, call
K . let, K min SA, (p), MK := { : K = K}, found
way divide disjoint subsets MK , one every K min SA, (p)
75

fiDe Cooman, De Bock, & Diniz


may empty, K \ A[] A[] MK , = Kmin SA, (p) MK


therefore p = Kmin SA, (p) MK bnp ()BA, .
fix K min SA, (p), construct count vector K letting (mK )x := 1
x K \ A[] (mK )x := 0 otherwise. Notice K NAnK , nK number
elements |K \ A[]| set K \ A[], therefore nK n. Consider MK ,
since (mK )x = 1 implies x A[] therefore x 1, see :
BA, () = ()



xx = ()

xA[]



xx (mK )x

xA[]



x(mK )x

xA[]

= (K, )BA,K ()BA,K (),

n
1 ( )1 . Hence, rewrite
(K, ) := ()(

)
K
K
MK bp ()BA,

:= MK (K, )bnp ()BA,K . way, find p =

pK BA,K , pK
Kmin SA, (p) BA,K pK .
Hence, fix K min SA, (p) 6= , left prove + nK > 0
++
pK V[+
(A). Assume first, ex absurdo, + nK = 0. particular
K]
++
K = , contradicts K SA, (p). remains prove pK V[+
(A).
K]
Consider int(A[+K ] ). derive K min SA, (p) SA, (p)
A[] K. Since A[K ] = K \A[], implies A[ + K ] = A[]A[K ] =
A[] (K \ A[]) = K, therefore int(K ). K 0 min SA, (p) \ {K},
K0 \ K =
6 therefore BA,K 0 () = 0. Hence, p() = BA,K ()pK (). know
p() > 0 p|K V ++ (K) BA,K () > 0 A[K ] = K \ A[] K.
conclude indeed pK () > 0.
Lemma 44. NA {0} p V (A):
SA, (p) = SA,0 (pBA, ) therefore min SA, (p) = min SA,0 (pBA, ).
Proof Lemma 44. First, assume K SA, (p). 6= K A, A[] K
p|K 6= 0. last inequality continuity polynomials, infer
int(K ) p() 6= 0. Since A[] K, find p()BA, () 6= 0
therefore (pBA, )|K 6= 0.
Assume, conversely, K SA,0 (pBA, ). =
6 K (pBA, )|K 6= 0.
last inequality implies K (pBA, )() 6= 0 therefore
BA, () 6= 0 p() 6= 0. BA, () 6= 0, derive A[] K
p() 6= 0, derive p|K 6= 0.
Proof Proposition 19. way HSC,A HSC,A, constructed [see defining
expressions (43) (44)], clearly suffices prove HSC,A c HSC,A, . Consider
therefore p V (A) pBA, HSC,A , Proposition 18, implies
pBA, 6= 0 (pBA, )|K V ++ (K) K min SA,0 (pBA, ). set
prove p HSC,A, . Applying Proposition 18 again, since, clearly, p 6= 0,
see suffices show p|K V ++ (K) K min SA, (p). consider
K min SA, (p). Then, Lemma 44, K min SA,0 (pBA, ), already argued
(pBA, )|K V ++ (K). Hence indeed p|K V ++ (K).
76

fiCoherent Predictive Inference Exchangeability


Proof Equation (48). Combining Equations (47) (30), see that,
NA {0}:
1
= {f L(A) : SA (f ) HSC,A,
DSC,A
c
(92)
}.
Also, f L(A) =
6 K A:
SA (f ) = 0 f = 0, SA (f )|K V ++ (K) f |K > 0 SA (f )|K = 0 f |K = 0. (93)
= 0. f L(A):
start case
min SA,0 (SA (f )) = {{x} : x f (x) 6= 0} ,
1
Statement (93). Hence, Proposition 18 Equations (92) (93): DSC,A
=
L>0 (A).
NA . f L(A):
Next, consider

{

{A[]}
f |A[]
6 0
=
min SA,
(SA (f )) =
{x} : x \ A[]
f (x) 6= 0} f |A[]
{A[]
=0

(94)

Equation (93). recall Proposition 18 Equations (92) (93)
1

consider two cases: f |A[]
6= 0 f |A[]
= 0. f |A[]
6= 0, f DSC,A c
{if f 6= 0 [which redundant]
f |A[]
> 0 or, equivalently [since f |A[]
6= 0],
}
1

f h L(A) : h|A[]
> 0 L>0 (A). f |A[]
= 0, f DSC,A c
or, equivalently [since f |A[]
f 6={0 f (x) 0 }
x \ A[]
= 0],
f h L(A) : h|A[]
> 0 L>0 (A).
Proof Equation (49). start first part Equation (49). Due Equation (17)
Proposition 19, suffices prove that, p V (A), minxA p(x ) > 0 p
HSC,A,0 minxA p(x ) < 0 p
/ HSC,A,0 .

First, assume minxA p(x ) < 0. p(y ) < 0.
Hence, since p|{y} = p(y ) < 0, find {y} min SA,0 (p) therefore
p
/ HSC,A,0 , Proposition 18.
Next, assume minxA p(x ) > 0. p|{x} = p(x ) > 0 x A, implying
min SA,0 (p) = {{x} : x A} therefore also, since p 6= 0, p HSC,A,0 ,
Proposition 18.
turn second part Equation (49). Due Equation (17) Proposition 19,
NA p V (A), minA[]
suffices prove that,
p() > 0 p

HSC,A,
p() < 0 p
/ HSC,A,
minA[]
.

First, assume minA[]
p()
<
0.

int(A[]
)

++


p() < 0, implying p|A[]
6= 0 p|A[]

/ V (A[]).
Hence, find A[]


min SA,
/ HSC,A,
(p) therefore p
, Proposition 18.

Next, assume minA[]
p()
>
0.

p|A[]
=
6 0 p|A[]
V ++ (A[]).



therefore also, since p 6= 0, p HSC,A,
Hence, find min SA,
(p) = {A[]}
,
Proposition 18.
77

fiDe Cooman, De Bock, & Diniz

Proof Equation (52). first part Equation (52) trivial consequence Equa NA f L(A). Then, combining
tion (51). second part, consider
Equations (50) (30):


= min
f (x)x = min f (x).
P 1SC,A (f |)
f (x)x = min
A[]


xA

A[]



xA[]


xA[]

Lemma 45. Consider category sets onto map : D,
p V (D) =
6 K A. (p R )|K 6= 0 p|(K) 6= 0.
Proof Lemma 45. First, assume p|(K) 6= 0, (K)
p() 6= 0. choose K R () = . Then, clearly, (p R )() =
p(R ()) = p() 6= 0 therefore (p R )|K 6= 0.
Assume, conversely, (pR )|K 6= 0, K (pR )() 6= 0.
let := R (), (K) p() = p(R ()) = (p R )() 6= 0. Hence,
p|(K) 6= 0.
Lemma 46. Consider category sets onto map : D,
p V (D), NA {0}.
{
}
SA, (p R ) = K : A[] K (K) SD,R () (p) ,
therefore
(SA, (p R )) = SD,R () (p) (min SA, (p R )) = min SD,R () (p).
Proof Lemma 46. start proving first statement. First, assume K SA, (p
R ), implying =
6 K A, A[] K (p R )|K 6= 0. 6= (K) D,
D[R ()] = (A[]) (K) and, Lemma 45, p|(K) 6= 0. Hence, (K) SD,R () (p).
Conversely, assume K A, A[] K (K) SD,R () (p). =
6 (K),
implies =
6 K, p|(K) 6= 0, which, Lemma 45, implies (p R )|K 6= 0.
Hence, K SA, (p R ).
first statement implies (SA, (p R )) SD,R () (p) therefore, order
prove second statement, suffices show SD,R () (p) (SA, (p R )) or,
equivalently, every L SD,R () (p), K SA, (p R )
(K) = L. choose L SD,R () (p) let K := {x : (x) L} = 1 (L).
(K) = L onto, since (A[]) = D[R ()] L, follows A[] K.
Hence, first statement, K SA, (p R ).
prove third statement, first assume K min SA, (p R ), implying
K SA, (p R ) that, K 0 SA, (p R ), K 0 6 K. second statement,
(K) SD,R () (p). prove (K) min SD,R () (p), assume ex absurdo
L SD,R () (p) L (K). Let K 0 := {x K : (x) L} = K 1 (L).
K 0 K (K 0 ) = L, therefore, Lemma 45, (p R )|K 0 6= 0,
K 0 6= p|L =
6 0. Since L SD,R () (p), see (A[]) = D[R ()] L
therefore A[] 1 (L). Since K SA, (p R ), know A[] K,
therefore A[] K 1 (L) = K 0 . tells us K 0 SA, (p R ), contradiction.
Assume, conversely, L min SD,R () (p), implying L SD,R () (p). Then,
78

fiCoherent Predictive Inference Exchangeability

second statement, K 0 SA, (p R ) (K 0 ) = L. Hence,
K min SA, (p R ) K K 0 therefore (K) (K 0 ) = L. Since
L min SD,R () (p) since, due second statement, (K) SD,R () (p),
(K) 6 L therefore (K) = L.
Lemma 47. Let =
6 K let p polynomial . n deg(p):
bnp| = bnp |NKn .
K

Proof Lemma 47. follows

p() =
bnp ()BA, ()
n
NA

K :
p|K () =


n
NA

=





bnp ()BA, (iA ()) =

bnp ()BA, (iA ())

n : A[]K
NA

bnp |NKn ()BK, (),

n
NK

completes proof.
Lemma 48. Consider category sets onto map : D,
p V (D) =
6 K A. Then:
(i) (p R )|K V + (K) p|(K) V + ((K));
(ii) (p R )|K V ++ (K) p|(K) V ++ ((K)).
Proof Lemma 48. first statement follows fact that, n deg(p):
bn(pR )|

K

n
> 0 bn(pR ) |NKn > 0 (bnp R )|NKn > 0 bnp |N(K)
> 0 bnp|

> 0,
(K)

first last equivalence due Lemma 47, second equivalence follows
n) = Nn
Proposition 39, third equivalence holds R (NK
(K) .
turn second statement, prove following statements
equivalent:
(a) ( int(K ))p(R ()) > 0;
(b) ( int((K) ))p() > 0.
First assume (a) holds, consider int((K) ). prove p() > 0.
1
construct K follows.
Consider z (K). x ({z}) K, choose
x > 0 way xK : (x)=z x = z . way, found K
satisfying R () = , moreover x > 0 x K, whence int(K ).
infer (a) indeed p() = p(R ()) > 0.
Assume, conversely, (b) holds, consider int(K ). Then, z D,
R ()z > 0 z (K) R ()z = 0 otherwise. means R () int((K) )
infer (b) indeed p(R ()) > 0.
79

fiDe Cooman, De Bock, & Diniz

Proposition 49. SC representation insensitive.
Proof Proposition 49. use characterisation representation insensitivity Theorem 7. Consider category sets onto map : D,
p V (D) NA {0}. Then, Proposition 19, need prove
p R HSC,A, p HSC,D,R () .
First, assume p HSC,D,R () , which, Proposition 18, implies p 6= 0
p|L V ++ (L) L min SD,R () (p). Applying Lemma 45 K = A, infer
p =
6 0 p R 6= 0. Consider K min SA, (p R ). Then, Lemma 46,
(K) min SD,R () (p), implying that, due assumption, p|(K) V ++ ((K)). Since
K 6= , apply Lemma 48 find (p R )|K V ++ (K). Hence, Proposition 18,
p R HSC,A, .
Assume, conversely, pR HSC,A, , which, Proposition 18, implies pR 6= 0
(p R )|K V ++ (K) K min SA, (p R ). Applying Lemma 45,
K = A, infer p R 6= 0 p 6= 0. Now, consider L min SD,R () (p),
Lemma 46, K min SA, (p R ) (K) = L. Since K =
6 and,
assumption, (p R )|K V ++ (K), infer Lemma 48 p|L V ++ (L). Hence,
Proposition 18, p HSC,D,R () .
Lemma 50. Consider category sets B B A, p V (B),
K K B 6= r N0 . IrB,A (p)|K 6= 0 p|KB 6= 0.
Proof Lemma 50. may assume without loss generality r + deg(p) > 0,
proof trivial otherwise.
First, assume p|KB =
6 0, means KB
p() 6= 0. := iA () K , infer Proposition 9 IrB,A (p|) = p() 6= 0
therefore IrB,A (p)|K 6= 0.
Assume, conversely, IrB,A (p)|K 6= 0, means, due continuity polynomials, int(K ) IrB,A (p|) 6= 0. infer K B 6=
+
B > 0, Proposition 10 guarantees p(|+
B ) 6= 0. Since |B KB , find
p|KB 6= 0.
Lemma 51. Consider category sets B B A, p V (B)
r N0 r + deg(p) > 0, NA {0}.
{
}
SA, (IrB,A (p)) = K : A[] K K B SB,rB () (p) ,
therefore
{
}
SB,rB () (p) = K B : K SA, (IrB,A (p))

{
}
min SB,rB () (p) = K B : K min SA, (IrB,A (p)) .
Proof Lemma 51. begin first statement. First, assume K SA, (IrB,A (p))
therefore 6= K A, A[] K IrB,A (p)|K =
6 0. K implies
KB B, A[] K implies B[rB ()] = A[]B KB. Moreover, IrB,A (p)|K =
6 0
together Proposition 10 r + deg(p) > 0 implies K B 6= , turn,
Lemma 50, implies p|KB 6= 0. Hence, K B SB,rB () (p). Conversely, assume
80

fiCoherent Predictive Inference Exchangeability

K A, A[] K K B SB,rB () (p). K B 6= , implying K 6= ,
p|KB 6= 0, which, Lemma 50, implies IrB,A (p)|K 6= 0. Hence, K SA, (IrB,A (p)).
order prove second statement, clearly suffices show SB,rB () (p)
{K B : K SA, (IrB,A (p))}, since converse inclusion follows directly first
statement. consider L SB,rB () (p) let K := L A[]. K A, A[] K
K B = L (A[] B) = L B[rB ()] = L. Hence, first statement, indeed
K SA, (IrB,A (p)).
prove third statement, first assume K min SA, (IrB,A (p)), implying
particular K SA, (IrB,A (p)). Then, second statement, K B SB,rB () (p).
prove K B min SB,rB () (p), consider L SB,rB () (p) L K B,
let K 0 := L A[]. Then, argument identical one used proof second
statement, K 0 B = L K 0 SA, (IrB,A (p)). However, since K 0 B = L K B
K 0 \ B = A[] \ B K \ B, find K 0 = (K 0 B) (K 0 \ B) (K B) (K \ B) = K,
therefore K 0 = K, assumption. Hence indeed L = K 0 B = K B. Assume,
conversely, L min SB,rB () (p), implying L SB,rB () (p). Then, second
statement, K 0 SA, (IrB,A (p)) K 0 B = L,
K min SA, (IrB,A (p)) K K 0 therefore K B K 0 B = L. Since
L min SB,rB () (p) and, second statement, K B SB,rB () (p),
K B = L.
Lemma 52. Consider category sets B B A, p V (B), K
K B 6= r N0 . IrB,A (p)|K V ++ (K) p|KB V ++ (K B).
Proof Lemma 52. may assume without loss generality r + deg(p) > 0,
proof trivial otherwise. Using Proposition 10, considering that, since K B =
6 , B > 0
int(K ), suffices prove following statements equivalent:
(a) ( int(K ))p(|+
B ) > 0;
(b) ( int(KB ))p() > 0.
First assume (a) holds, consider int(KB ). prove p() > 0.
construct
K follows. x K \ B, choose x > 0 way

:= xK\B x < 1, always possible. x K B, let x := (1)x > 0.
follows construction B = 1 > 0, |+
B = int(K ),
infer (a) indeed p() = p(|+
)
>
0.
B
Assume, conversely, (b) holds, consider int(K ). B > 0
+
K B =
6 0 therefore, z B, (|+
B )z > 0 z K B. Hence |B int(KB ),
+
infer (b) p(|B ) > 0.
Proposition 53. SC specific.
Proof Proposition 53. use characterisation specificity Theorem 11. Consider
category sets B B A, p V (B), NA {0}, r N0 .
Then, Proposition 19, need prove IrB,A (p) HSC,A, p HSC,B,rB () .
First, assume p HSC,B,rB () , which, Proposition 18, implies p 6= 0
p|L V ++ (L) L min SB,rB () (p). Applying Lemma 50 K = A, infer
p 6= 0 IrB,A (p) 6= 0. Consider K min SA, (IrB,A (p)), Lemma 51,
81

fiDe Cooman, De Bock, & Diniz

K B min SB,rB () (p), implying that, due assumption, p|KB V ++ (K B).
Since K B 6= 0, apply Lemma 52 find IrB,A (p)|K V ++ (K). Hence,
Proposition 18, IrB,A (p) HSC,A, .
Assume, conversely, IrB,A (p) HSC,A, , which, Proposition 18, implies
r
IB,A (p) 6= 0 IrB,A (p)|K V ++ (K) K min SA, (IrB,A (p)). Lemma 50
K = A, IrB,A (p) 6= 0, infer p 6= 0. Consider L min SB,rB () (p),
then, Lemma 51, K min SA, (IrB,A (p)) KB = L. Since therefore
K B 6= 0 since, assumption, IrB,A (p)|K V ++ (K), infer Lemma 52
p|L V ++ (L). Hence, Proposition 18, p HSC,B,rB () .
Proof Theorem 20. immediate consequence Propositions 17 [coherence], 49
[representation insensitivity] 53 [specificity].
E.7 Proofs Results Section 12
NA {0} p V (A).
Proof Equation (54). Consider


BA, p HIDM,A
p HIDM,A
c
( sA ) DiA (BA, p|) > 0

+ ) > 0
( sA ) DiA (BA, |) DiA (p|
+ ) > 0,
( sA ) DiA (p|
third equivalence follows Updating Property Dirichlet expectation
[Proposition 31].
NA {0}. Then, combining Equations (56)
Proof Equation (59). Consider
(58) n = 1:
{
}

mx + x
s,1
= f L(A) : ( sA )
DIDM,A
c
f (x)
>0 .
+
xA

consider f L(A). sA :

xA

f (x)



mx + x
x
1
>0
f (x)(mx + x ) > 0
>
f (x)
f (x)mx .
+


xA

xA

Combining equations above, letting c := 1s
find that:

xA



xA f (x)mx

s,1
(s0 (0, s))( int(A ))
f DIDM,A
c

ease notation,

s0
f (x)tx > c.


(95)

xA

f c, f (y) < c therefore, Statement (95),
s,1
[choose s0 ty close enough 1, respectively]. f = c, due
f
/ DIDM,A
c
s,1
Finally, let us
definition c, f = c = 0. Hence, Statement (95), f
/ DIDM,A
c.
see happens
f > c. clearly c 0. Consider s0 (0,
s) int(A ).
0
0
since f > c, xA f (x)tx > c therefore also, since c 0, ss xA f (x)tx > ss c c.
s,1
Statement (95).
Hence f DIDM,A
c
82

fiCoherent Predictive Inference Exchangeability

NA {0} f L(A). combining
Proof Equation (60). Consider
Equations (57) (58):

mx + x
mx + s0 tx
= inf
inf
f (x)

+ s0 (0,s) int(A )
+ s0
xA
xA
(
)


1
s0
= inf
f (x)mx +
inf
f (x)tx
+ s0 int(A )
s0 (0,s) + s0
xA
xA
(
)

1
s0
= inf
f (x)mx +
min f
+ s0
s0 (0,s) + s0
xA


1
f (x)mx +
min f,
=
+
+

= inf
P s,1
IDM,A (f |)



f (x)

xA

last equality follows min f

mx
xA f (x) ,



property convex combinations.

Proof Theorem 21. coherence, fix category set A, must prove

HIDM,A
satisfies requirements B1B3 Bernstein coherence. trivial

definition HIDM,A
, linearity Dirichlet expectation operator, fact
Dirichlet expectation Bernstein basis polynomial positive.
Next, turn representation insensitivity, use characterisation Theorem 7.
Consider category sets onto map : D, p V (D)
NA {0}. Then, using Pooling Property [Proposition 33] Dirichlet
expectation Equation (54), find indeed:

(p R )BA, HIDM,A
( sA ) DiA (p R | + ) > 0

( sA ) (p|R ( + )) > 0

( sD ) (p|R () + ) > 0 pBD,R () HIDM,D
,

third equivalence follows equality sD = R (sA ).
Finally, turn specificity, use characterisation Theorem 11. Consider
category sets B B A, p V (B), NA {0}
r N0 . Then, using Restriction Property [Proposition 34] Dirichlet expectation
Equation (54), find indeed:

IrB,A (p)BA, HIDM,A
( sA ) DiA (IrB,A (p)| + ) > 0

( sA ) DiB (p|rB ( + )) > 0

( sB ) DiB (p|rB () + ) > 0 pBB,rB () HIDM,B
,

third equivalence follows sB = rB (sA ).
E.8 Proofs Results Section 13

Lemma 54. p1 , p2 HSI,A
: SA,0 (p1 + p2 ) = SA,0 (p1 ) SA,0 (p2 ).

83

fiDe Cooman, De Bock, & Diniz

Proof Lemma 54. First, consider K SA,0 (p1 + p2 ), meaning 6= K
(p1 + p2 )|K 6= 0. Assume, ex absurdo, K
/ SA,0 (p1 ) K
/ SA,0 (p2 ). p1 |K = 0
p2 |K = 0 therefore (p1 + p2 )|K = 0, contradiction. Hence indeed
K SA,0 (p1 ) SA,0 (p2 ).
Next, consider K SA,0 (p1 ) SA,0 (p2 ), implying =
6 K A.
least one K 0 min(SA,0 (p1 ) SA,0 (p2 )) K 0 K, assume without
loss generality K 0 SA,0 (p1 ). Since K 0 min(SA,0 (p1 ) SA,0 (p2 )),
L 6 K 0 L SA,0 (p1 ) SA,0 (p2 ), therefore K 0 min SA,0 (p1 ). already tells us

0
p1 |K 0 HIDM,K
0 . two possibilities. first one K SA,0 (p2 ),

then, much way above, find p2 |K 0 HIDM,K 0 . Hence,


due Bernstein coherence [B3] HIDM,K
0 , (p1 + p2 )| 0 = p1 | 0 + p2 | 0 HIDM,K 0 .
K
K
K
0
0
second possibility K
/ SA,0 (p2 ), p2 |K 0 = 0 since K 6= , find,

too, (p1 + p2 )|K 0 = p1 |K 0 + p2 |K 0 = p1 |K 0 HIDM,K
0 . cases, therefore,


(p1 + p2 )|K 0 HIDM,K 0 , Bernstein coherence [B1] HIDM,K
0 allows us conclude
6 0. Since K 0 K, find (p1 + p2 )|K 6= 0 therefore
(p1 + p2 )|K 0 =
K SA,0 (p1 + p2 ).


Proof Proposition 22. Since 0
/ HSI,A
, left prove V + (A) HSI,A
that,



> 0 p, p1 , p2 HSI,A , p HSI,A p1 + p2 HSI,A .

First, consider > 0 p HSI,A
. Then, clearly, SA,0 (p) = SA,0 (p) therefore
min SA,0 (p) = min SA,0 (p). K min SA,0 (p), K min SA,0 (p),


which, since p HSI,A
, implies p|K HIDM,K
therefore, due Bernstein


coherence HIDM,K , (p)|K = (p|K ) HIDM,K
. Furthermore, since p 6= 0

p 6= 0, therefore p HSI,A .

Next, consider p1 , p2 HSI,A
. p1 6= 0 p2 6= 0, implying SA,0 (p1 ) 6=
SA,0 (p2 ) 6= , therefore SA,0 (p1 ) SA,0 (p2 ) 6= . Applying Lemma 54, find
SA,0 (p1 + p2 ) 6= , K 6= K A, (p1 + p2 )|K =
6 0
therefore p1 + p2 6= 0. K 0 min SA,0 (p1 + p2 ), equivalently, due Lemma 54,
K 0 min(SA,0 (p1 ) SA,0 (p2 )). Then, applying reasoning second part


proof Lemma 54, find (p1 + p2 )|K 0 HIDM,K
0 . Hence, p1 + p2 HSI,A .

Since already shown HSI,A closed taking positive linear combinations,
since V + (A) consists positive linear combinations Bernstein basis polynomials,

need show HSI,A
contains Bernstein basis polynomials order prove
+

V (A) HSI,A . consider NA {0}. Then, K 6= K A,
BA, |K = BK,rK () A[] K, BA, |K = 0 otherwise. implies

SA,0 (BA, ) = { =
6 K : A[] K} that, due Bernstein coherence HIDM,K
,


BA, |K = BK,rK () HIDM,K K SA,0 (BA, ). Hence, BA, |K HIDM,K

K min SA,0 (BA, ). Since BA, 6= 0, find indeed BA, HSI,A
.


Proof Proposition 23. first prove HSI,A
c HSI,A,
. Consider p V (A)


pBA, HSI,A , meaning pBA, 6= 0 (pBA, )|K HIDM,K


K min SA,0 (pBA, ). set prove p HSI,A, . Since, clearly, p 6= 0, suffices

show p|K HIDM,K
crK () K min SA, (p). consider K min SA, (p),
implying A[] K therefore K[rK ()] = A[]. infer

Lemma 44 K min SA,0 (pBA, ), tells us (pBA, )|K HIDM,K
. Since

(pBA, )|K = p|K BA, |K = p|K BK,rK () , find p|K HIDM,K crK ().

84

fiCoherent Predictive Inference Exchangeability




Next, prove HSI,A,
HSI,A
c. Consider p HSI,A,
, meaning

p 6= 0 p|K HIDM,K crK () K min SA, (p). set prove


pBA, HSI,A
or, equivalently, pBA, =

6 0 (pBA, )|K HIDM,K
K min SA,0 (pBA, ). Since p 6= 0, continuity polynomials guarantees
int(A ) p() 6= 0 therefore (pBA, )() 6= 0. know already
pBA, 6= 0. Consider K min SA,0 (pBA, ). Then, Lemma 44, K min SA, (p),


implying A[] K p|K HIDM,K
crK () therefore p|K BK,rK () HIDM,K
.

Since moreover p|K BK,rK () = (pBA, )|K , find indeed (pBA, )|K HIDM,K .

Proof Equation (63). Due Equation (17), suffices prove that, p V (A),


minxA p(x ) > 0 p HSI,A
minxA p(x ) < 0 p
/ HSI,A
.

First, assume minxA p(x ) < 0. p(y ) < 0.
Hence, since p|{y} = p(y ) < 0, find {y} min SA,0 (p) therefore also, due


Bernstein coherence HIDM,{y}
[see Theorem 21], p|{y}
/ HIDM,{y}
,

infer p
/ HSI,A .
Next, assume minxA p(x ) > 0. p|{x} = p(x ) > 0 x A, implying

min SA,0 (p) = {{x} : x A} that, x A, p|{x} HIDM,{x}
,


Bernstein coherence HIDM,{x} . Hence, since p 6= 0, find p HSI,A
.
Proof Equations (64) (65). Equation (65) follows directly Equation (55).
prove Equation (64). Due Equation (17) Proposition 23, suffices prove that,
NA p V (A):



> 0 p HSI,A,
<0p
c(p, )
/ HSI,A,
c(p, )
,



where, ease notation, let
:=
c(p, )

inf

sA[]


+ ).
DiA[]
|rA[]
(p|A[]
()


< 0, implying DiA[]
+ ) < 0
First, assume c(p, )
|rA[]
(p|A[]
()


A[]
6= 0 and, Equation (54), p|A[]

/
therefore p|A[]





HIDM,A[]
(p) therefore
(). Hence, find A[] min SA,
crA[]

p
/ HSI,A,
.


> 0, implying p|A[]
Next, assume c(p, )
6= 0 and, Equation (54),



therefore
p|A[]

H
cr
(
).
Hence,

find

min

(p) = {A[]}
A,

A[]

IDM,A[]


p HSI,A,
.
NA :
Proof Equation (67). combining Equations (62) (30), see that,
{
}
s,1

= f L(A) : SA (f ) HSI,A,
DSI,A
c
(96)
.

Consider f L(A) distinguish two cases: f |A[]
6= 0 f |A[]
= 0.
f |A[]
6= 0 [and therefore f 6= 0],
s,1

SA (f )|A[]
f DSI,A
c
HIDM,A[
()
crA[]
]


SA[]
(f |A[]
) HIDM,A[]
()
crA[]

85

fiDe Cooman, De Bock, & Diniz

s,1
f |A[]
DIDM,A[]
()
crA[]

1
1
f |A[]
>

f
(x)


f
>

f (x)mx f > 0,
x


|A[]



xA[]


xA[]

first equivalence due Statement (93) Equations (94), (61) (96).
second equivalence follows definition SA SA[]
third one due
Equations (21) (30). fourth equivalence consequence Equation (67)
final equivalence holds f > 0 redundant, given f |A[]
6= 0.
f |A[]
= 0, [again, using Statement (93) Equations (94), (61) (96)]
s,1
f 6= 0 x \ A[]:

f DSI,A
c

f (x) = 0 SA (f )|A[]{x}
HIDM,A[
crA[]{x}
().


]{x}


Since f |A[]
crA[]{x}
() Bernstein coherent [Theorem 21],
= 0 HIDM,A[]{x}


latter statement equivalent f (x) > 0. Hence, find that:
s,1
f 6= 0 (x \ A[])f

f DSI,A
c
(x) 0

f >0
f |A[]
>

1
f (x)mx f > 0,


xA[]

second third equivalences consequences f |A[]
= 0.
Lemma 55. Consider category sets onto map : D,
p V (D), NA {0}, 6= K A[] K.


(p R )|K HIDM,K
crK () p|(K) HIDM,(K)
cr(K) (R ()).
Proof Lemma 55. Let := K, := (K), := |K p := p|(K) .
onto map , p V (D ) p R = p|(K) R|K = (p R )|K .

Since A[] K, identify element := rK () NK
therefore
result follows representation insensitivity IDMM inference system
hyperparameter s, R ( ) = R|K (rK ()) = r(K) (R ()):





p R HIDM,A
c p HIDM,D cR ( ).

Proposition 56. sSI representation insensitive.
Proof Proposition 56. use characterisation representation insensitivity Theorem 7. Consider category sets onto map : D,
p V (D) NA {0}. Then, Proposition 23, need prove


p R HSI,A,
p HSI,D,R
.
()


First, assume p HSI,D,R () , meaning p 6= 0 p|L HIDM,L
crL (R ())
L min SD,R () (p). Applying Lemma 45 K = A, infer p 6= 0
p R 6= 0. Consider K min SA, (p R ), 6= K A[] K. Then,
Lemma 46, (K) min SD,R () (p), implying that, due assumption, p|(K)
86

fiCoherent Predictive Inference Exchangeability



HIDM,(K)
cr(K) (R ()). Applying Lemma 55, find (p R )|K HIDM,K
crK ().

Hence, p R HSI,A, .

Assume, conversely, p R HSI,A,
, meaning p R 6= 0 (p R )|K

HIDM,K crK () K min SA, (p R ). Applying Lemma 45 K = A, infer
p R 6= 0 p 6= 0. Consider L min SD,R () (p). Lemma 46,
K min SA, (p R ) (K) = L. Since 6= K A, A[] K

and, assumption, (p R )|K HIDM,K
crK (), infer Lemma 55


p|L HIDM,L crL (R ()). Hence, p HSI,D,R () .

Lemma 57. Consider category sets B B A, p V (B),
NA {0}, r N0 K K B 6= A[] K.


IrB,A (p)|K HIDM,K
crK () p|KB HIDM,KB
crKB ().
Proof Lemma 57. Let := K, B := K B, p := p|KB r = deg(p) deg(p ) + r.
B , p V (B ), r r 0, r + deg(p ) = r + deg(p),


bdeg(p)+r
()BA,iA ()|K =
bdeg(p)+r
()BA,iA ()|K
IrB,A (p)|K =
p
p
deg(p)+r

deg(p)+r

NB

=



NB
B[]K


bdeg(p)+r
()BK,iK () = IrB ,A (p ),
p|
KB

deg(p)+r

NKB

third equality follows unicity Bernstein expansion polynomial.
Since A[] K, identify element := rK () NK {0} therefore
result follows specificity IDMM inference system hyperparameter s,
rB ( ) = rKB ():







IrB ,A (p ) HIDM,A
c p HIDM,B crB ( ).

Proposition 58. sSI specific.
Proof Proposition 58. use characterisation specificity Theorem 11. Consider
category sets B B A, p V (B), NA {0}


r N0 . Then, Proposition 23, need prove IrB,A (p) HSI,A,
p HSI,B,r
.
B ()
clear Propositions 10 22 assume without loss generality
r + deg(p) > 0.


First, assume p HSI,B,r
, implying p =
6 0 p|L HIDM,L
crBL ()
B ()
L min SB,rB () (p). Applying Lemma 50 K = A, infer p 6= 0
IrB,A (p) 6= 0. Consider K min SA, (IrB,A (p)). infer Lemma 51 K

crKB ().
B min SB,rB () (p), implying that, due assumption, p|KB HIDM,KB
r

Since K B 6= 0 A[] K, IB,A (p)|K HIDM,K crK () Lemma 57. Hence,

IrB,A (p) HSI,A,
.

Assume, conversely, IrB,A (p) HSI,A,
, implies IrB,A (p) 6= 0
r

IB,A (p)|K HIDM,K crK () K min SA, (IrB,A (p)). Applying Lemma 50
K = A, infer IrB,A (p) 6= 0 p 6= 0. Consider L min SB,rB () (p).
Lemma 51, K min SA, (IrB,A (p)) K B = L. Since K B 6= 0,
87

fiDe Cooman, De Bock, & Diniz


A[] K and, assumption, IrB,A (p)|K HIDM,K
crK (), infer Lemma 57


p|KB HIDM,KB crKB (), words, p|L HIDM,L
crBL (). Hence,

p HSI,B,rB () .

Proof Theorem 24. immediate consequence Propositions 22 [coherence], 56
[representation insensitivity] 58 [specificity].
E.9 Proofs Results Section 14
Proof Theorem 25. begin coherence. Consider category set A,

prove HH,A Bernstein coherent. B1, recall 0
/ HIDM,A
> 0,
+

therefore 0
/ HH,A . Similarly, B2, recall V (A) HIDM,A > 0,
+
therefore V (A) HH,A . B3, consider n N k R>0 pk HH,A

k {1, . . . ,
n}. > 0 pk HIDM,A
k {1, . . . , n},
n


n therefore k=1 k pk HIDM,A , Bernstein coherence [Theorem 21]. Hence indeed
k=1 k pk HH,A .
Next, turn representation insensitivity, use characterisation Theorem 7.
Consider category sets onto map : D, p V (D)
NA {0}. find indeed:

(p R )BA, HH,A (s R>0 )(p R )BA, HIDM,A

(s R>0 )pBD,R () HIDM,D
pBD,R () HH,D ,

second equivalence follows representation insensitivity IDMM
inference systems [Theorem 21].
Finally, turn specificity, use characterisation Theorem 11. Consider
category sets B B A, p V (B), NA {0} r N0 .
find indeed:

IrB,A (p)BA, HH,A (s R>0 )IrB,A (p)BA, HIDM,A

(s R>0 )pBB,rB () HIDM,B
pBB,rB () HH,B ,

second equivalence follows specificity IDMM inference systems [Theorem 21].
NA {0} p V (A):
Proof Equation (69).

pBA,
p HH,A c
HH,A (s R>0 )pBA,
HIDM,A


(s R>0 )p HIDM,A
c.

Combined Equation (54), yields desired result.
NA {0} p V (A):
Proof Equation (71).
= sup { R : p HH,A c}

H H,A (p|)
{
}


= sup sup R : p HIDM,A
c
sR>0

88

fiCoherent Predictive Inference Exchangeability

+ ) = lim
inf DiA (p|

= sup

+ ),
inf DiA (p|

s+0 sA

sR>0

second equality due Equation (69), third one due Equation (54).
Proof Equation (72). Consider p V (A) apply Equation (71):
H H,A (p) = lim

inf DiA (p|) = lim

s+0 sA

inf DiA (p|s0 )

inf

s+0 int(A ) s0 (0,s)

(97)

fix n max{deg(p), 1} int(A ). Using Equation (80), find
NAn :
(

1

0

DiA (BA, |s ) =

s0 (n)

)
( )
n 0 (mx )
n
1
(m )
(s tx )
= (n)
(s0 tx ) x ,
0



xA
xA[]

x A[]:
(mx )

(s0 tx )

= (s0 tx )(s0 tx + 1) . . . (s0 tx + mx 1) = s0 tx (mx 1)![1 + O(s0 )]

similarly:
1
s0 (n)

=

s0 (n

1
[1 + O(s0 )].
1)!

Hence, find
(
0

DiA (BA, |s ) =

xA[] tx (mx

1)!

)

(n 1)!

s0|A[]|1 [1 + O(s0 )].

consider two cases: |A[]| > 1 |A[]| = 1 [since n 1, cases
exhaustive]. |A[]| > 1, DiA (BA, |s0 ) = O(s0 ). |A[]| = 1 or, equivalently,
x = nx , DiA (BA,nx |s0 ) = tx [1 + O(s0 )]. combine
Equation (78), find
DiA (p|s0 ) =



bnp () DiA (BA, |s0 ) =

n
NA



bnp (nx )tx + O(s0 ).

xA

Furthermore, due Equation (78):
bnp (nx ) =



bnp ()BA, (x ) = p(x ) x A.

n
NA

Hence, conclude
DiA (p|s0 ) =



p(x )tx + O(s0 ),

xA

which, combined Equation (97), leads desired result.
89

fiDe Cooman, De Bock, & Diniz

NA p V (A) use Equation (71):
Proof Equation (73). Consider
= lim
H H,A (p|)

= lim sup DiA (p|
+ ). (98)
+ ) H H,A (p|)
inf DiA (p|

s+0 sA

s+0



Bernstein coherent [Theorem 25], follows H H,A (|)
coherent
Since HH,A c
super-additive, conjugate upper
lower prevision. implies H H,A (|)
sub-additive. Hence, suffices prove equalities Equation (73)
prevision H H,A (|)
Bernstein basis polynomial p = BA, , NA {0}. sA
gather Equation (80) Appendix B that:
( )
n
+ ) =
(mx + x )(nx ) .
DiA (BA, |
(n)
(mA + )
xA
1

Observe that:
x)
(mx + x )(nx ) = (mx + x )(mx + x + 1) . . . (mx + x + nx 1) = m(n
[1 + O(x )],
x

similarly, since > 0:
1
(mA + )

(n)

1

=

(n)



[1 + O(A )]

Therefore:
( )
(nx )

n
xA mx
+ ) =
DiA (BA, |
[1
+
O(
)]
[1 + O(x )],

(n)


xA



which, using Equation (98), leads to:46
( )
(nx )
n
xA mx
= H H,A (BA, |)
=

H H,A (BA, |)
= DiA (BA, |).
(n)




E.10 Proofs Results Section 15
Proof Theorem 26. already argued smallest inference
system , shall denote lower probability function . First, assume n 2.
denote (n, k) := (n, k + 1) (n, k), follows assumptions
(n, k + 1) (n, k) 0 k n 2.

(99)

first going prove induction implies
(n, k)

k
(n, n) 0 k n.
n

46. See footnote 35.

90

(100)

fiCoherent Predictive Inference Exchangeability

Observe inequality holds trivially k = 0 [Theorem 14.L1]. assume
inequality holds k = `, ` {0, . . . , n 1}. must show holds
k = ` + 1. Assume, ex absurdo, not, therefore
(n, ` + 1) <

`+1
1
(n, n) (n, `) + (n, n),
n
n

(101)

second inequality follows induction hypothesis.
(n, n) = (n, ` + 1) +

n1


(n, m) (n, ` + 1) + (n ` 1)(n, `)

m=`+1

<

`+1
n`1
(n, n) +
(n, n) = (n, n),
n
n

first inequality follows Equation (99), second first
second inequalities Equation (101). contradiction, completes proof
induction (100).
infer (100), Theorem 14.L9 assumption (76)
(n, k)

k n
k
=
0 k n.
nn+s
n+s

(102)

observe inequality holds trivially n {0, 1}. get predictive
lower prevision P 1A (h|) gamble h A:

[h(x) min h]P 1A (I{x} |)
P 1A (h|) = min h + P 1A (h min h|) min h +
xA

= min h +



[h(x) min h](n, mx )

xA

min h +


xA

[h(x) min h]

mx
= P s,1
IDM,A (h|),
n+s

first equality first inequality follow coherence [P5, P2 P3]
P 1A (|), second equality representation insensitivity [Equation (33)],
second inequality Equation (102). converse inequality, observe IDMM
inference system sIDM coherent, representation insensitive, specific Theorem 21,
clearly concave surprise, satisfies assumption (76), therefore dominates smallest
inference system.

References
Augustin, T., Coolen, F. P. A., De Cooman, G., & Troffaes, M. C. M. (Eds.). (2014).
Introduction Imprecise Probabilities. John Wiley & Sons.
Bernard, J.-M. (1997). Bayesian analysis tree-structured categorized data. Revue Internationale de Systmique, 11, 1129.
Bernard, J.-M. (2005). introduction imprecise Dirichlet model multinomial
data. International Journal Approximate Reasoning, 39, 123150.
91

fiDe Cooman, De Bock, & Diniz

Bernard, J.-M. (2007). personal conversation..
Boole, G. (1847, reprinted 1961). Laws Thought. Dover Publications, New York.
Boole, G. (2004, reprint work originally published Watts & Co., London, 1952).
Studies Logic Probability. Dover Publications, Mineola, NY.
Carnap, R. (1952). continuum inductive methods. University Chicago Press.
Cifarelli, D. M., & Regazzini, E. (1996). De Finettis contributions probability statistics.
Statistical Science, 11, 253282.
Couso, I., & Moral, S. (2011). Sets desirable gambles: conditioning, representation,
precise probabilities. International Journal Approximate Reasoning, 52 (7), 1034
1055.
Cozman, F. G. (2013). Independence full conditional probabilities: Structure, factorization, non-uniqueness, bayesian networks. International Journal Approximate
Reasoning, 54 (9), 12611278.
De Cooman, G., & Miranda, E. (2007). Symmetry models versus models symmetry.
Harper, W. L., & Wheeler, G. R. (Eds.), Probability Inference: Essays Honor
Henry E. Kyburg, Jr., pp. 67149. Kings College Publications.
De Cooman, G., & Miranda, E. (2008a). F. Riesz Representation Theorem finite
additivity. Dubois, D., Lubiano, M. A., Prade, H., Gil, M. A., Grzegorzewski,
P., & Hryniewicz, O. (Eds.), Soft Methods Handling Variability Imprecision
(Proceedings SMPS 2008), pp. 243252. Springer.
De Cooman, G., & Miranda, E. (2008b). Weak strong laws large numbers coherent
lower previsions. Journal Statistical Planning Inference, 138 (8), 24092432.
De Cooman, G., & Miranda, E. (2012). Irrelevant independent natural extension
sets desirable gambles.. Journal Artificial Intelligence Research, 45, 601640.
De Cooman, G., Miranda, E., & Quaeghebeur, E. (2009a). Representation insensitivity
immediate prediction exchangeability. International Journal Approximate
Reasoning, 50 (2), 204216.
De Cooman, G., & Quaeghebeur, E. (2012). Exchangeability sets desirable gambles.
International Journal Approximate Reasoning, 53 (3), 363395. Special issue
honour Henry E. Kyburg, Jr.
De Cooman, G., Quaeghebeur, E., & Miranda, E. (2009b). Exchangeable lower previsions.
Bernoulli, 15 (3), 721735.
de Finetti, B. (1937). La prvision: ses lois logiques, ses sources subjectives. Annales de
lInstitut Henri Poincar, 7, 168. English translation Kyburg Jr. Smokler
(1964).
de Finetti, B. (1970). Teoria delle Probabilit. Einaudi, Turin.
de Finetti, B. (19741975). Theory Probability: Critical Introductory Treatment. John
Wiley & Sons, Chichester. English translation de Finettis (1970) book, two volumes.
Dubins, L. E. (1975). Finitely additive conditional probabilities, conglomerability
disintegrations. Annals Probability, 3, 8899.
92

fiCoherent Predictive Inference Exchangeability

Geisser, S. (1993). Predictive Inference: Introduction. Chapman & Hall.
Goldstein, M. (1983). prevision prevision. Journal American Statistical
Society, 87, 817819.
Goldstein, M. (1985). Temporal coherence. Bernardo, J. M., DeGroot, M. H., Lindley,
D. V., & Smith, A. F. M. (Eds.), Bayesian Statistics, Vol. 2, pp. 231248. North-Holland,
Amsterdam. discussion.
Good, I. J. (1965). Estimation Probabilities: Essay Modern Bayesian Methods.
MIT Press.
Haldane, J. B. S. (1945). method estimating frequencies. Biometrika, 33, 222225.
Hausdorff, F. (1923). Momentprobleme fr ein endliches Intervall. Mathematische Zeitschrift,
13, 220248.
Jaynes, E. T. (2003). Probability Theory: Logic Science. Cambridge University Press.
Jeffreys, H. (1998). Theory Probability. Oxford Classics series. Oxford University Press.
Reprint third edition (1961), corrections.
Johnson, N. L., Kotz, S., & Balakrishnan, N. (1997). Discrete Multivariate Distributions.
Wiley Series Probability Statistics. John Wiley Sons, New York.
Johnson, W. E. (1924). Logic, Part III. Logical Foundations Science. Cambridge
University Press. Reprinted Dover Publications 1964.
Keynes, J. M. (1921). Treatise Probability. Macmillan, London.
Koopman, B. O. (1940). Axioms Algebra Intuitive Probability. Annals
Mathematics, Second Series, 41 (2), 269292.
Kyburg Jr., H. E., & Smokler, H. E. (Eds.). (1964). Studies Subjective Probability. Wiley,
New York. Second edition (with new material) 1980.
Lad, F. (1996). Operational Subjective Statistical Methods: Mathematical, Philosophical
Historical Introduction. John Wiley & Sons.
Levi, I. (1980). Enterprise Knowledge. MIT Press, London.
Mangili, F., & Benavoli, A. (2013). New prior near-ignorance models simplex.
Cozman, F., Denux, T., Destercke, S., & Seidenfeld, T. (Eds.), ISIPTA 13
Proceedings Eighth International Symposium Imprecise Probability: Theories
Applications, pp. 213222. SIPTA.
Miranda, E. (2009). Updating coherent lower previsions finite spaces. Fuzzy Sets
Systems, 160 (9), 12861307.
Miranda, E., & De Cooman, G. (2014). Introduction Imprecise Probabilities, chap. Lower
previsions. John Wiley & Sons.
Miranda, E., & Zaffalon, M. (2011). Notes desirability conditional lower previsions.
Annals Mathematics Artificial Intelligence, 60 (3-4), 251309.
Moral, S. (2005). Epistemic irrelevance sets desirable gambles. Annals Mathematics
Artificial Intelligence, 45, 197214.
93

fiDe Cooman, De Bock, & Diniz

Moral, S., & Wilson, N. (1995). Revision rules convex sets probabilities. Coletti,
G., Dubois, D., & Scozzafava, R. (Eds.), Mathematical Models Handling Partial
Knowledge Artificial Intelligence, pp. 113128. Plenum Press, New York.
Piatti, A., Zaffalon, M., Trojani, F., & Hutter, M. (2009). Limits learning categorical
latent variable prior near-ignorance. International Journal Approximate
Reasoning, 50 (4), 597611.
Prautzsch, H., Boehm, W., & Paluszny, M. (2002). Bzier B-Spline Techniques. Springer,
Berlin.
Quaeghebeur, E. (2014). Introduction Imprecise Probabilities, chap. Desirability. John
Wiley & Sons.
Quaeghebeur, E., De Cooman, G., & Hermans, F. (2014). Accept & reject statement-based
uncertainty models. International Journal Approximate Reasoning. Accepted
publication.
Rouanet, H., & Lecoutre, B. (1983). Specific inference ANOVA: significance tests
Bayesian procedures. British Journal Mathematical Statistical Psychology,
36 (2), 252268.
Seidenfeld, T., Schervish, M. J., & Kadane, J. B. (1995). representation partially ordered
preferences. Annals Statistics, 23, 21682217. Reprinted collection
Seidenfeld et al. (1999, pp. 69129).
Seidenfeld, T., Schervish, M. J., & Kadane, J. B. (1999). Rethinking Foundations
Statistics. Cambridge University Press, Cambridge.
Sheffer, I. M. (1939). properties polynomial sets type zero. Duke Mathematical
Journal, 5, 590622.
Smith, C. A. B. (1961). Consistency statistical inference decision. Journal
Royal Statistical Society, Series A, 23, 137.
Troffaes, M. C. M., & De Cooman, G. (2014). Lower Previsions. Wiley.
Trump, W., & Prautzsch, H. (1996). Arbitrary degree elevation Bzier representations.
Computer Aided Geometric Design, 13, 387398.
Walley, P. (1991). Statistical Reasoning Imprecise Probabilities. Chapman Hall,
London.
Walley, P. (1996). Inferences multinomial data: learning bag marbles. Journal
Royal Statistical Society, Series B, 58, 357. discussion.
Walley, P. (1997). bounded derivative model prior ignorance real-valued
parameter. Scandinavian Journal Statistics, 24 (4), 463483.
Walley, P. (2000). Towards unified theory imprecise probability. International Journal
Approximate Reasoning, 24, 125148.
Walley, P., & Bernard, J.-M. (1999). Imprecise probabilistic prediction categorical data.
Tech. rep. CAF-9901, Laboratoire Cognition et Activites Finalises, Universit de
Paris 8.
94

fiCoherent Predictive Inference Exchangeability

Williams, P. M. (1975a). Coherence, strict coherence zero probabilities. Proceedings
Fifth International Congress Logic, Methodology Philosophy Science,
Vol. VI, pp. 2933. Dordrecht. Proceedings 1974 conference held Warsaw.
Williams, P. M. (1975b). Notes conditional previsions. Tech. rep., School Mathematical
Physical Science, University Sussex, UK. See revised journal version
Williams (2007).
Williams, P. M. (1976). Indeterminate probabilities. Przelecki, M., Szaniawski, K., &
Wojcicki, R. (Eds.), Formal Methods Methodology Empirical Sciences, pp.
229246. Reidel, Dordrecht. Proceedings 1974 conference held Warsaw.
Williams, P. M. (2007). Notes conditional previsions. International Journal Approximate
Reasoning, 44, 366383.
Zabell, S. L. (1982). W. E. Johnsons sufficientness postulate. Annals Statistics, 10,
10901099. Reprinted collection Zabell (2005).
Zabell, S. L. (2005). Symmetry Discontents: Essays History Inductive Probability. Cambridge Studies Probability, Induction, Decision Theory. Cambridge
University Press, Cambridge, UK.
Zaffalon, M., & Miranda, E. (2013). Probability time. Artificial Intelligence, 198, 151.

95


