journal artificial intelligence

submitted published

computing convex coverage sets
faster multi objective coordination
diederik roijers
shimon whiteson
frans oliehoek

roijers uva nl
whiteson uva nl
f oliehoek uva nl

informatics institute
university amsterdam
amsterdam netherlands

abstract
article propose multi objective coordination graphs mocogs key efficiency compute convex coverage
set ccs instead pareto coverage set pcs ccs sufficient solution
set large class important characteristics facilitate
efficient solutions propose two main computing ccs mo cogs
convex multi objective variable elimination cmove computes ccs performing
series agent eliminations seen solving series local multi objective
subproblems variable elimination linear support vels iteratively identifies single
weight vector w lead maximal possible improvement partial ccs
calls variable elimination solve scalarized instance w vels
faster cmove small medium numbers objectives compute
approximate ccs fraction runtime addition propose variants
methods employ tree search instead variable elimination achieve
memory efficiency analyze runtime space complexities methods prove
correctness compare empirically naive baseline existing
pcs method terms memory usage runtime
focusing ccs methods achieve much better scalability number
agents current state art

introduction
many real world domains maintenance scharpff spaan
volker de weerdt traffic light control pham et al multiple agents
need coordinate actions order maximize common utility key coordinating
efficiently domains exploiting loose couplings agents guestrin koller
parr kok vlassis agents actions directly affect subset
agents
multi agent coordination complicated fact many domains agents need
balance multiple objectives roijers vamplew whiteson dazeley example agents might maximize performance computer network minimizing
power consumption tesauro das chan kephart lefurgy levine rawson
maximize cost efficiency maintenance tasks road network minimizing traffic
delays roijers scharpff spaan oliehoek de weerdt whiteson
c

ai access foundation rights reserved

firoijers whiteson oliehoek

figure mining company example
however presence multiple objectives per se necessitate use
specialized multi objective solution methods scalarized e
utility function converted scalar utility function may solvable
existing single objective methods conversion involves two steps roijers et al
first step specify scalarization function
definition scalarization function f function maps multi objective utility
solution decision u scalar utility uw
uw f u w
w weight vector parameterizes f
second step define single objective version decision
utility solution equals scalarized utility original uw
unfortunately scalarizing solving possible
w may known advance example consider company mines different
resources figure depict company faces morning one van
per village needs transport workers village nearby mine
resources mined different mines yield different quantities resource per worker
market prices per resource vary stochastic process every price change
alter optimal assignment vans expected price variation increases
passage time maximize performance thus critical act latest
possible price information since computing optimal van assignment takes time redoing
computation every price change highly undesirable
settings need multi objective method computes advance optimal solution possible prices w call set coverage set cs many
cases w revealed solution must executed case solution
automatically selected cs given w cases w never made explicit
instead human involved decision making selects one solution
cs perhaps basis constraints preferences difficult formalize
objectives roijers et al cases cs typically
much smaller complete set solutions selecting optimal joint action
cs typically much easier selecting directly complete set solutions


ficomputing ccss faster multi objective coordination

article consider multi objective methods made efficient require coordination multiple loosely coupled agents particular address multi objective coordination graphs mo cogs one shot multi agent decision loose couplings expressed graphical model mo cogs form
important class decision used model variety realworld delle fave stranders rogers jennings marinescu rollon
many sequential decision modeled series mo cogs
common single objective guestrin et al kok vlassis oliehoek
spaan dibangoye amato
key efficiency mo cog methods propose compute convex
coverage set ccs instead pareto coverage set pcs ccs subset
pcs sufficient solution multi objective linear scalarization
function example mining company example figure f linear since
total revenue simply sum quantity resource mined times price per
unit however even f nonlinear stochastic solutions allowed ccs
sufficient
ccs previously considered solution concept mo cogs
computing ccs requires running linear programs whilst computing pcs requires
pairwise comparisons solutions however key insight article loosely
coupled systems ccss easier compute pcss two reasons first ccs
typically much smaller subset pcs loosely coupled settings efficient methods
work solving series local subproblems focusing ccs greatly reduce size
subproblems second focusing ccs makes solving mo cog equivalent
finding optimal piecewise linear convex pwlc scalarized value function
efficient techniques adapted reasons argue ccs often
concept choice mo cogs
propose two approaches exploit insights solve mo cogs efficiently
existing methods delle fave et al dubus gonzales perny marinescu
razak wilson rollon larrosa first deals multiple
objectives level individual agents second deals global
level
first extends rollon larrosa refer
pareto multi objective variable elimination pmove computes local pareto
sets agent elimination compute ccs instead call resulting
convex multi objective variable elimination cmove
second abstract call optimistic linear support
ols much faster small medium numbers objectives furthermore ols
precise case stochastic strategies ccs deterministic strategies sufficient
vamplew dazeley barker kelarev case deterministic strategies linearity
scalarization function makes ccs sufficient roijers et al
article synthesizes extends already reported two conference papers specifically
cmove section previously published adt roijers whiteson oliehoek
b vels section aamas roijers whiteson oliehoek
memory efficient methods computing ccss section novel contribution article
original article called multi objective bucket elimination mobe however
use pmove consistent names mentioned article



firoijers whiteson oliehoek

used produce bounded approximation ccs ccs
enough time compute full ccs ols generic method employs single objective
solvers subroutine article consider two implementations subroutine
variable elimination subroutine yields variable elimination linear support
vels particularly fast small moderate numbers objectives
memory efficient cmove however memory highly limited reduction
memory usage may enough cases search mateescu
dechter instead yields tree search linear support tsls
slower vels much memory efficient
prove correctness cmove ols analyze runtime space
complexities methods methods better guarantees
pcs methods cmove ols complementary e trade offs exist
variants
furthermore demonstrate empirically randomized realistic cmove vels scale much better previous empirically confirm trade offs cmove ols ols used
bounded approximation save additional orders magnitude runtime
even small finally even memory highly limited tsls
still solve large
rest article structured follows first provide formal definition
model well overview existing solution methods section
presenting naive section sections analyze runtime
space complexities compare empirically
existing end section finally conclude section
overview contributions findings suggestions future

background
section formalize multi objective coordination graph mo cog
however describe single objective version coordination graph
cog mo cog extension variable elimination
solving cogs methods present section build different ways
single objective coordination graphs
coordination graph cog guestrin et al kok vlassis tuple hd ui

n set n agents
ai joint action space cartesian product finite action
spaces agents joint action thus tuple containing action agent
ha


u u u set scalar local payoff functions limited
scope e depends onlypa subset agents total team payoff sum
local payoffs u e ue ae


ficomputing ccss faster multi objective coordination

figure cog agents local payoff functions b eliminating agent
adding u c eliminating agent adding u























table payoff matrices u left u right two possible
actions per agent denoted dot bar

agents share payoff function u abuse notation e index local
payoff function ue denote subset agents scope ae thus local joint
action e joint action subset agents
decomposition u local payoff functions represented factor
graph bishop bipartite graph containing two types vertices agents variables
local payoff functions factors edges connecting local payoff functions
agents scope
figure shows factor graph example cog team payoff function
decomposes two local payoff functions two agents scope
u


x

ue ae u u

e

local payoff functions defined table factor graph illustrates loose
couplings decomposition local payoff functions particular
agents choice action directly depends immediate neighbors e g
agent knows agent action choose action without considering agent
variable elimination
discuss variable elimination several multi objective
extensions rollon larrosa rollon build including cmove section use subroutine ols section
exploits loose couplings expressed local payoff functions efficiently
compute optimal joint action e joint action maximizing u first forward


firoijers whiteson oliehoek

pass eliminates agents turn computing value agents best
response every possible joint action neighbors values used construct
local payoff function encodes value best response replaces agent
payoff functions participated original agents
eliminated backward pass assembles optimal joint action constructed
payoff functions present slight variant payoff tagged
action generates obviating need backwards pass two
equivalent variant amenable multi objective extension present
section
eliminates agents graph predetermined order shows
pseudocode elimination single agent first determines set local
payoff functions connected ui neighboring agents ni lines
definition set neighboring local payoff functions ui set local
payoff functions agent scope
definition set neighboring agents ni set agents
scope one local payoff functions ui
constructs payoff function computing value agent best
response possible joint action ani agents ni lines
loops joint actions ani line ani loops actions ai
available agent line ai ai computes local payoff agent
responds ani ai line tags total payoff ai action generates
line order able retrieve optimal joint action later already
tags present appends ai way entire joint action incrementally
constructed maintains value best response taking maximum
payoffs line finally eliminates agent payoff functions ui replaces
newly constructed local payoff function line
elimve u








input cog u agent
ui set local payoff functions involving
ni set neighboring agents
unew factor taking joint actions ni ani input
foreach ani ani

foreach ax
ai
v
uj ani ai
uj ui








tag v ai
v
end
unew ani max
end
return u ui unew



ficomputing ccss faster multi objective coordination

consider example figure table optimal payoff maximizes sum
two payoff functions
max u max u u




eliminates agent first pushes maximization inward
goes local payoff functions involving agent case u




max u max u max u






solves inner maximization replaces local payoff function u
depends agent neighbors thereby eliminating agent

max u max u u




leads factor graph depicted figure b values u u
u optimal payoffs actions
agent given payoffs shown table ultimately want optimal joint
action optimal payoff tags payoff u action agent
generates e think u value tag pair denote pair
parentheses subscript u u
next eliminates agent yielding factor graph shown figure c




max u max max u u max u








appends tags agent existing tags agent yielding following
tagged payoff values u maxa u u
u finally maximizing yields optimal
payoff optimal action contained tags
runtime complexity exponential number agents
induced width often much less number agents
theorem computational complexity n amax w amax
maximal number actions single agent w induced width e maximal
number neighboring agents agent plus one agent moment
eliminated guestrin et al
theorem space complexity n amax w
space complexity arises every agent elimination local payoff
function created amax w fields possible input actions since impossible
tell priori many local payoff functions exist given time
execution need multiplied total number local payoff
functions created execution n
designed minimize runtime methods focus memory efficiency
instead mateescu dechter discuss memory efficiency section
fact proven best runtime guarantees within large class rosenthal




firoijers whiteson oliehoek























table two dimensional payoff matrices u left u right
multi objective coordination graphs
multi objective coordination

graph mo cog tuple hd ui
u u u set dimensional local p
payoff functions
total team payoff sum local vector valued payoffs u e ue ae use
ui indicate value th objective denote set possible joint action
values v table shows two dimensional mo cog structure
single objective example section multi objective payoffs
solution mo cog coverage set cs joint actions associated values
u contains least one optimal joint action possible parameter vector w
scalarization function f definition cs subset undominated set
definition undominated set u mo cog set joint actions
associated payoff values optimal w scalarization function f


u v u u v wa uw uw
care least one optimal joint action every w rather
optimal joint actions lossless subset u suffices
definition coverage set cs cs v subset u possible w
least one optimal solution cs e
wa


u cs v uw uw

note cs necessarily unique typically seek smallest possible cs
convenience assume payoff vectors cs contain values associated
joint actions suggested tagging scheme described section
payoff vectors v cs depends know
scalarization function f minimal assumption f monotonically increasing e
value one objective ui increases uj stay constant scalarized value
u cannot decrease assumption ensures objectives desirable e else
equal better
definition pareto front undominated set arbitrary strictly monotonically
increasing scalarization functions f


p f v u u v u p u
p indicates pareto dominance p dominance greater equal objectives
strictly greater least one objective


ficomputing ccss faster multi objective coordination

order optimal scalarized values necessary compute entire
pf e g two joint actions equal payoffs need retain one
definition pareto coverage set pcs p cs v p f v coverage set
arbitrary strictly monotonically increasing scalarization functions f e

u p cs v u p u u u
computing p dominance requires pairwise comparison payoff vectors feng
zilberstein
highly prevalent scenario addition f monotonically increasing
know linear parameter vectors w weights
values individual objectives multiplied f w u mining example
figure resources traded open market resources positive unit
price case scalarization linear combination amount resource
mined weights correspond price per unit resource many
examples linear scalarization functions exist literature e g lizotte bowling
murphy assume linear scalarization monotonically increasing
represent without loss generality convex combination objectives e
weights positive sum case convex coverage set ccs
needed subset convex hull ch
definition convex hull ch undominated set linear non decreasing
scalarizations f u w w u


ch v u u v wa w u w u
ch contains solutions attain optimal value least one weight
vectors ch c dominated contrast p domination c domination cannot
tested pairwise comparisons take two payoff vectors
c dominate payoff vector note ch contains solutions needed guarantee optimal scalarized value value contain multiple solutions optimal
one specific weight lossless subset ch respect linear scalarizations
called convex coverage set ccs e ccs retains least one u maximizes
scalarized payoff w u every w
definition convex coverage set ccs ccs v ch v cs linear nondecreasing scalarizations e

wa u ccs v w u w u
since linear non decreasing functions specific type monotonically increasing function ccs subset smallest possible pcs
previously mentioned css pcs ccs may unique example
two joint actions equal payoff vectors need one
make pcs ccs
p dominance often called pairwise dominance pomdp literature
note term convex hull overloaded graphics convex hull superset mean
convex hull article



firoijers whiteson oliehoek

figure ccs filled circles left solid black lines right versus pcs filled circles
squares left dashed solid black lines right twelve random
dimensional payoff vectors

practice pcs ccs often equal pf ch however
proposed article guaranteed produce pcs ccs
necessarily entire pf ch pcss ccss sufficient solutions
terms scalarized value say solve mo cogs
figure left values joint actions u represented points valuespace two objective mo cog joint action value ccs
pcs b however pcs ccs weight
linear scalarization bs value would optimal shown figure right
scalarized value strategies plotted function weight first objective
w w c neither ccs pcs pareto dominated
many multi objective methods e g delle fave et al dubus et al marinescu et al rollon simply assume pcs appropriate solution
concept however argue choice cs depends one assume
utility defined respect multiple objectives e scalarization function used scalarize vector valued payoffs argue many situations
scalarization function linear cases one use ccs
addition shape f choice solution concept depends whether
deterministic joint actions considered whether stochastic strategies permitted stochastic strategy assigns probabilitypto joint action
probabilities joint actions together sum aa value stochastic strategy linear
p combination value vectors joint actions

mixture u
aa u therefore optimal values monotonically
increasing f lie convex upper surface spanned strategies ccs
indicated lines figure left therefore optimal values monotonically
increasing f including nonlinear ones constructed taking mixture policies
ccs vamplew et al
article considers methods computing ccss sections
computed efficiently pcss furthermore ccss typically much


ficomputing ccss faster multi objective coordination

smaller particularly important final selection joint done
group humans compare possible alternatives solution set
methods presented article variable elimination sections
tree search ts section exact solution
methods cogs
cmove propose section differs another
multi objective refer pmove rollon larrosa
produces ccs rather pcs alternative messagepassing max plus pearl kok vlassis however
guaranteed exact tree structured cogs multi objective methods build
max plus delle fave et al limitation unless
preprocess cog form clique tree gai network dubus et al
tree structured graphs message passing produce optimal solutions
similar runtime guarantees note pmove existing multi objective methods
message passing produce pcs rather ccs
section take different multi objective coordination
outer loop explain applicable computing ccs
pcs considerable advantages terms runtime memory usage

non graphical
naive way compute ccs ignore graphical structure calculate set
possible payoffs joint actions v prune away c dominated joint actions
first translate set value set factors vsfs f vsf f function
mapping local joint actions sets payoff vectors initial vsfs constructed
local payoff functions
f e ae ue ae
e vsf maps local joint action singleton set containing actions
local payoff define v terms f cross sum operator vsfs
f joint action

v f
f e ae
f e f

cross sum two sets b contains possible vectors made
summing one payoff vector set
b b b b
ccs calculated applying pruning operator cprune described
removes c dominated vectors set value vectors v

ccs v f cprune v f cprune
f e ae

f e f

non graphical ccs simply computes righthand side equation e
computes v f explicitly looping actions action looping
local vsfs pruning set ccs


firoijers whiteson oliehoek

ccs contains least one payoff vector maximizes scalarized value every
w
w



arg max w u





u ccs v f w u w u

aa

every w solution part ccs achieves
value maximizing solution moreover value solutions given
dot product thus finding ccs analogous faced partially observable
markov decision processes pomdps feng zilberstein optimal vectors
corresponding value vectors u beliefs corresponding weight vectors
w must found therefore employ pruning operators pomdp literature
describes implementation cprune feng
zilberstein one modification order improve runtime guarantees cprune
first pre prunes candidate solutions u pcs pprune line
pprune computes pcs u p cs running pairwise comparisons next
partial ccs u constructed follows random vector u u selected line
u tries weight vector w u better vectors
u line solving linear program w cprune
finds best vector v w u moves u line weight
u better c dominated thus removed u u line
cprune u

















input set payoff vectors u
u pprune u
u
notempty u
select random u u
w findweight u u
w null
weight u optimal
remove u u
end
else
v arg maxuu w u
u u v
u u v
end
end
return u

runtime cprune defined
u p cs p cs p ccs



p ccs polynomial size ccs number objectives
runtime linear program tests c domination


ficomputing ccss faster multi objective coordination

pprune u













input set payoff vectors u
u
u
u first element u
foreach v u
v p u
u v continue v instead u
end
end
remove u vectors p dominated u u
add u u
end
return u

findweight u u
max x
x w

subject w u u x u u

x

wi



x return w else return null

key downside non graphical requires explicitly enumerating
possible joint actions calculating payoffs associated one consequently
intractable small numbers agents number joint actions grows
exponentially number agents
theorem time complexity computing ccs mo cog containing local
payoff functions following non graphical equation
amax n amax n p cs p cs p ccs
proof first v computed looping vsfs joint action summing
vectors length maximum size action space agent amax
amax n joint actions v contains one payoff vector joint action v input
cprune
next two sections present two approaches compute ccss efficiently
first pushed cprune operator equation cross sum
union max operator pushed summation call
inner loop uses pruning operators agent eliminations
inner loop second inspired linear support cheng


firoijers whiteson oliehoek

pomdp pruning operator requires finding optimal solution certain w instead performing maximization entire set v original linear
support use finite number scalarized instances
mo cog avoiding explicit calculation v call outer loop
creates outer loop around single objective method
calls subroutine

convex variable elimination mo cogs
section exploit loose couplings calculate ccs inner loop e pushing pruning operators cross sum union
operators equation cmove extension rollon larrosas
pareto extension refer pmove rollon larrosa
analyzing cmoves complexity terms local convex coverage sets
yields much better runtime complexity guarantees non graphical
computing ccss presented section
exploiting loose couplings inner loop
non graphical computing ccs expensive computing pcs
shown section mo cogs compute ccs
much efficiently exploiting mo cogs graphical structure particular
solve mo cog series local subproblems eliminating agents
manipulating set vsfs f describe mo cog key idea compute
local ccss lccss eliminating agent instead single best response
computing lccs prunes away many vectors possible
minimizes number payoff vectors calculated global level
greatly reduce computation time describe elim operator eliminating agents
used cmove section
first need update definition neighboring local payoff functions definition
neighboring vsfs
definition set neighboring vsfs set local payoff functions
agent scope
neighboring agents ni agent agents scope vsf
except corresponding definition possible local joint action
ni compute lccs contains payoffs c undominated responses
agent best response values words ccs subproblem
arises considering fixing specific local joint action ani compute
lccs must consider payoff vectors subproblem vi prune dominated
ones
definition fix actions
ani ai set payoff vectors
l
subproblem vi ani ai f e f e ae ae formed ai
appropriate part ani
definition define lccs ccs vi


ficomputing ccss faster multi objective coordination

definition local ccs lccs c undominated subset vi ani
lccsi ani ccs vi ani

lccss create vsf f conditioned actions
agents ni
ani f ani lccs ani
elim operator replaces vsfs f factor
elim f f f ani
theorem elim preserves ccs f ccs v f ccs v elim f
proof implication equation e joint actions
w scalarized value maximal vector valued payoff
u w u w u ccs maximal scalarized
payoff cannot lost elim

function distributes local payoff functions w u
p linear scalarization
p
w e ue ae e w ue ae thus eliminating agent divide set vsfs
non neighbors nn agent participate neighbors ni

x
x
w u
w ue ae
w ue ae
enn

eni

following equation ccs contains maxaa w u w elim pushes
maximization
max w u max
aa

ai ai

x

w ue ae max

ai ai

enn

x

w ue ae

eni

elim
agent factors term f ani satisfies w f ani maxai
p replaces
e
eni w u ae per definition thus preserving maximum scalarized value w
thereby preserving ccs
instead lccs could compute local pcs lpcs pcs
computation vi instead ccs computation note since lccs lpcs vi
elim reduces size respect vi would
possible considered p dominance therefore focusing ccs greatly
reduce sizes local subproblems since solution local subproblem input
next agent elimination size subsequent local subproblems reduced
lead considerable speed ups


firoijers whiteson oliehoek

convex multi objective variable elimination
present convex multi objective variable elimination cmove
implements elim cprune cmove iteratively eliminates agents none
left however implementation elim computes ccs outputs correct
joint actions payoff vector ccs rather single joint action cmove
extension rollon larrosas pareto extension refer
pmove rollon larrosa
important difference cmove pmove cmove computes ccs typically leads much smaller subproblems thus much better
computational efficiency addition identify three places pruning take
place yielding flexible different trade offs finally use tagging scheme instead backwards pass section
presents abstract version cmove leaves pruning operators
unspecified section cmove first translates set vector set
factors vsfs f line next cmove iteratively eliminates agents elim line
elimination order determined techniques devised single objective
koller friedman
cmove u prune prune prune q










input set local payoff functions u elimination order q queue containing
agents
f create one vsf every local payoff function u
ani ani
q dequeue
f elim f prune prune
end
f retrieve final factor f
f
return prune

shows implementation elim parameterized two pruning operators prune prune corresponding two different pruning locations inside
operator computes lccsi computelccsi ani prune prune
elim f prune prune









input set vsfs f agent
ni set neighboring agents
subset vsf scope
f ani vsf
foreach ani ani
f ani computelccs ani prune prune
end
f f f
return f



ficomputing ccss faster multi objective coordination

computelccsi implemented follows first define cross sum prune
prune b lccsi applies operator sequentially
operator ab


f e ae

computelccsi ani prune prune prune
e
ai

f

operator leading incremental
prune applied cross sum two sets via
pruning cassandra littman zhang prune applied coarser level
union cmove applies elim iteratively agents remain resulting ccs
note agents left f line agents condition
case consider actions neighbors single empty action
pruning applied end agents eliminated
call prune increasing level coarseness thus three pruning operators incremental pruning prune pruning union actions eliminated
agent prune pruning agents eliminated prune reflected
agents eliminated final factor taken set
factors line single set contained factor retrieved line note
use empty action denote field final factor agents
scope finally prune called
consider example figure payoffs defined table apply
cmove first cmove creates vsfs f f u u eliminate agent
creates vsf f computing lccss every tagging element
set action agent generates cmove first generates
set since vectors optimal w neither
removed pruning thus f cmove first generates
cprune determines dominated consequently removes
yielding f cmove adds f graph removes f
agent yielding factor graph shown figure b
cmove eliminates agent combining f f create f f
cmove must calculate lccs
f f f f
first cross sum yields second yields pruning
union yields f similarly taking union yields
lccs f adding f
graph figure c
finally cmove eliminates agent since neighboring agents left ai
contains empty action cmove takes union f f since
dominate latter pruned leaving ccs

cmove variants
several ways implement pruning operators lead correct instantiations cmove pprune cprune used
long prune prune cprune note prune computes ccs prune
necessary


firoijers whiteson oliehoek

article consider basic cmove use prune prune
prunes prune cprune well incremental cmove uses cprune
prune prune latter invests effort intermediate pruning
smaller cross sums resulting speedup however vectors
pruned intermediate steps additional speedup may occur
creates unnecessary overhead empirically investigate variants
section
one could consider pruning operators contain prior knowledge
range possible weight vectors information available could easily
incorporated changing pruning operators accordingly leading even smaller lccss
thus faster article however focus case
prior knowledge available
analysis
analyze correctness complexity cmove
theorem move correctly computes ccs
proof proof works induction number agents base case original
mo cog f e ae f singleton set since elim preserves ccs
see theorem necessary vectors lost last agent eliminated
one factor remains since conditioned agent actions
lccs computation must contain one set ccs
theorem computational complexity cmove
n amax wa wf r r r



wa induced agent width e maximum number neighboring agents connected via factors agent eliminated wf induced factor width e
maximum number neighboring factors agent eliminated r r r
cost applying prune prune prune operators
proof cmove eliminates n agents one computes lccs joint
action eliminated agents neighbors field vsf cmove computes
amax wa fields per iteration calling prune equation adjacent factor
prune taking union actions eliminated agent prune called
exactly eliminating agents line
unlike non graphical cmove exponential wa number
agents respect similar pmove rollon
however earlier complexity make effect pruning explicit instead
complexity bound makes use additional constraints limit total
number possible different value vectors specifically analysis pmove
payoff vectors integer valued maximum value objectives practice
compute pcs first prune prune compute ccs prune
however useful small pcs cheaper compute ccs



ficomputing ccss faster multi objective coordination

bounds loose even impossible define e g payoff values
real valued one objectives therefore instead give description
computational complexity makes explicit dependence effectiveness
pruning even though complexity bounds better worst case e
pruning possible allow greater insight runtimes
evaluate apparent analysis experimental section
theorem demonstrates complexity cmove depends heavily runtime
pruning operators turn depends sizes input sets input
set prune union returned series applications prune
prune uses output last application prune therefore need balance
effort lower level pruning higher level pruning occurs less
often dependent output lower level bigger lccss
gained lower level pruning
theorem space complexity cmove
n amax wa lccsmax amax emax
lccsmax maximum size local ccs original number vsfs
emax maximum scope size original vsfs
proof cmove computes local ccs vsf joint action eliminated agents neighbors maximally wa neighbors maximally n
factors payoff vector stores real numbers
vsfs created initialization cmove vsfs
exactly one payoff vector containing real numbers per joint action agents scope
maximally amax emax joint actions
pmove space complexity p ccsmax instead lccsmax
lccs subset corresponding lpcs cmove thus strictly
memory efficient pmove
note theorem rather loose upper bound space complexity
vsfs original exist time however possible predict
priori many vsfs exist time resulting space complexity
bound basis vsfs exist point execution cmove
empirical evaluation
test efficiency cmove compare runtimes pmove
non graphical varying numbers agents objectives
analyze runtimes correspond sizes pcs ccs
use two types experiments first experiments done random mocogs directly control variables second experiment use
mining day realistic benchmark structured random mo cogs
still randomized
compare pmove prune pprune rather prune prune pprune
proposed original article rollon larrosa found former option slightly
consistently faster



firoijers whiteson oliehoek



b

c

figure runtimes ms log scale nongraphical method pmove cmove
standard deviation mean error bars b corresponding number vectors
pcs ccs c corresponding spread induced width

random graphs
generate random mo cogs employ procedure takes input n number
agents number payoff dimensions number local payoff functions
ai action space size agents agents procedure
starts fully connected graph local payoff functions connecting two agents
local payoff functions randomly removed ensuring graph
remains connected local payoff functions remain values different
objectives local payoff function real numbers drawn independently
uniformly interval compare set randomly
generated mo cogs separate value n ai
compare basic cmove incremental cmove pmove non graphical
method test random mo cogs number agents ranging
average number factors per agent held n number
objectives experiment run ghz intel core computer gb
memory figure shows averaged mo cogs number agents
runtime figure non graphical method quickly explodes cmove
variants slower pmove small numbers agents runtime grows much
slowly pmove agents cmove variants faster
pmove average agents one mo cogs generated caused pmove
time basic cmove maximum runtime incremental
cmove explained differences size solutions e
pcs ccs figure b pcs grows much quickly number
agents ccs two objective incremental cmove seems
consistently slower basic cmove
cmoves runtime grows much slowly nongraphical method
still exponential number agents counterintuitive since worst case
complexity linear number agents explained induced width
mo cogs runtime cmove exponential figure c see
induced width increases linearly number agents random graphs


ficomputing ccss faster multi objective coordination

figure runtimes ms non graphical method pmove cmove log scale
standard deviation mean error bars left corresponding number vectors
pcs ccs right increasing numbers agents objectives

therefore conclude two objective mo cogs non graphical method
intractable even small numbers agents runtime cmove increases
much less number agents pmove
test runtime behavior changes higher number objectives run
experiment average number factors per agent held n
increasing numbers agents remaining experiments
described section executed xeon l ghz computer gb
memory figure left shows experiment averaged mo cogs
number agents note plot induced widths
change number objectives demonstrate number
agents grows cmove becomes key containing computational cost solving
mo cog cmove outperforms nongraphical method agents onwards
agents basic cmove times faster cmove significantly better
pmove though one order magnitude slower agents ms basic
ms incremental versus ms average runtime grows much slowly
pmove agents cmove variants faster pmove
agents basic cmove almost one order magnitude faster versus
average difference increases every agent
runtime cmove exponential induced width increases
number agents n n average
random mo cog generation procedure however cmoves runtime polynomial
size ccs size grows exponentially shown figure right
fact cmove much faster pmove explained sizes pcs
ccs former grows much faster latter agents average pcs
size average ccs size agents average pcs size risen
average ccs size
figure left compares scalability number objectives
random mo cogs n averaged mo cogs cmove
outperforms nongraphical method interestingly nongraphical method


firoijers whiteson oliehoek

figure runtimes ms non graphical method pmove cmove logscale
standard deviation mean error bars left corresponding number vectors
pcs ccs right increasing numbers objectives

several orders magnitude slower grows slowly starts
grow exponent pmove explained fact
time takes enumerate joint actions payoffs remains approximately constant
time takes prune increases exponentially number objectives
cmove order magnitude slower pmove ms basic
incremental versus ms however cmove variants already
faster pmove dimensions respectively times faster
happens ccs grows much slowly pcs shown figure
right difference incremental basic cmove decreases number
dimensions increases factor trend indicates
pruning every cross sum e prune becomes relatively better higher
numbers objectives although unable solve instances many
objectives within reasonable time expect trend continue incremental
cmove would faster basic cmove many objectives
overall conclude random graphs cmove key solving mo cogs
within reasonable time especially size increases number
agents number objectives
mining day
mining day mining company mines gold silver objectives set mines
local payoff functions located mountains see figure mine workers live
villages foot mountains company one van village agents
transporting workers must determine every morning mine van
go actions however vans travel nearby mines graph connectivity workers
efficient workers mine efficiency bonus per
worker amount resource mined per worker x w x
base rate per worker w number workers mine base rate
gold silver properties mine since company aims maximize revenue
best strategy depends fluctuating prices gold silver maximize revenue


ficomputing ccss faster multi objective coordination

figure runtimes ms basic incremental cmove pmove log scale
standard deviation mean error bars left corresponding number vectors
pcs ccs right increasing numbers agents

mining company wants use latest possible price information lose time
recomputing optimal strategy every price change therefore must calculate
ccs
generate mining day instance v villages agents randomly assign
workers village connect mines village connected mines
greater equal index e village connected mines connected
mines last village connected mines thus number mines
v base rates per worker resource mine drawn uniformly
independently interval
order compare runtimes basic incremental cmove pmove
realistic benchmark generate mining day instances varying numbers
agents note include non graphical method runtime mainly
depends number agents thus considerably faster
random graphs runtime shown figure left cmove
pmove able tackle agents however runtime
pmove grows much quickly cmove two objective setting
basic cmove better incremental cmove basic cmove pmove
runtimes around agents agents basic cmove runs
pmove even though incremental cmove worse basic cmove
runtime still grows much slowly pmove beats pmove
many agents
difference pmove cmove relationship
number agents sizes ccs grows linearly pcs grows
polynomially shown figure right induced width remains around regardless
number agents demonstrate ccs grows slowly
pcs number agents cmove solve mo cogs efficiently
pmove number agents increases


firoijers whiteson oliehoek

linear support mo cogs
section present variable elimination linear support vels vels
method computing ccs mo cogs several advantages cmove
moderate numbers objectives runtime complexity better anytime
e time vels produces intermediate become better
better approximations ccs therefore provided maximum scalarized
error vels compute optimal ccs
rather dealing multiple objectives inner loop cmove vels
deals outer loop employs subroutine vels thus builds
ccs incrementally iteration outer loop vels adds one
vector partial ccs vector vels selects single w one offers
maximal possible improvement passes w inner loop inner loop
vels uses section solve single objective coordination graph cog
scalarizing mo cog w selected outer loop joint
action optimal cog multi objective payoff added
partial ccs
departure point creating vels chengs linear support cheng chengs
linear support originally designed pruning pomdps unfortunately
rarely used pomdps practice runtime exponential
number states however number states pomdp corresponds number
objectives mo cog realistic pomdps typically many states many
mo cogs handful objectives therefore mo cogs scalability
number agents important making chengs linear support attractive starting
point developing efficient mo cog solution method
building chengs linear support section create abstract
call optimistic linear support ols builds ccs incrementally
ols takes arbitrary single objective solver input seen generic
multi objective method ols chooses w iteration
finite number iterations improvements partial ccs made
ols terminate furthermore bound maximum scalarized error
intermediate used bounded approximations ccs
section instantiate ols single objective solver
yielding vels effective mo cog
optimistic linear support
ols constructs ccs incrementally adding vectors initially empty partial ccs
definition partial ccs subset ccs turn subset v
ccs v
define scalarized value function corresponding convex upper surface
shown bold figure b
definition scalarized value function partial ccs function takes
weight vector w input returns maximal attainable scalarized value


ficomputing ccss faster multi objective coordination



b

c



figure possible payoff vectors objective mo cog b ols finds two payoff
vectors extrema red vertical lines corner weight wc
found maximal possible improvement ccs shown dotted line
c ols finds vector adds two corner weights q
ols calls solvecog corner weights two iterations finds
vectors ensuring ccs ccs
payoff vector
us w max w u
u

similarly define set maximizing joint actions
definition optimal joint action set function respect function
gives joint actions maximize scalarized value
w arg max w u
u

note w set w multiple joint actions provide
scalarized value
definitions describe optimistic linear support ols ols adds
vectors partial ccs finding vectors called corner weights corner
weights weights us w definition changes slope directions
must thus weights w definition consists multiple payoff vectors every
corner weight prioritized maximal possible improvement finding payoff
vector corner weight maximal possible improvement ols knows
partial ccs complete example process given figure
corner weights searched payoff vectors indicated
red vertical lines
ols shown optimal payoff corner weight ols
assumes access function called solvecog computes best payoff vector
given w leave implementation solvecog abstract section
discuss implement solvecog ols takes input mo cog solved
maximal tolerable error
first describe ols initialized section define corner weights
formally describe ols identifies section finally describe


firoijers whiteson oliehoek

ols solvecog

























input
coggccs
agent eliminate

partial
w set checked weights
q empty priority queue
foreach extremum weight simplex
q add add extrema infinite priority
end
q isempty timeout
w q pop
u solvecog w
u
wdel remove corner weights made obsolete u q store
wdel w wdel corner weights removed adding u
wu newcornerweights u wdel
u
foreach w wu
r w calculate improvement maxvaluelp w w
r w
q add w r w
end
end
end
w w w
end
return highest r w left q

ols prioritizes corner weights used bound error
stopping ols done finding full ccs section
initialization
ols starts initializing partial ccs contain payoff vectors
ccs discovered far line well set visited weights w line
adds extrema weight simplex e points
weight one objective priority queue q infinite priority line
extrema popped priority queue ols enters main loop line
w highest priority selected line solvecog called
w line u best payoff vector w
example figure b shows two payoff vectors dimensional mocog found applying solvecog extrema weight simplex
vectors must part ccs optimal
least one w one solvecog returned solution extrema
weight simplex set weights w ols tested far marked vertical
red line segments


ficomputing ccss faster multi objective coordination

corner weights
evaluated extrema consists number objectives payoff vectors
associated joint actions however many weights simplex yet
contain optimal payoff vector therefore identifying vector u add
line ols must determine weights add q chengs linear support
ols identifying corner weights weights corners convex
upper surface e points pwlc surface us w changes slope define
corner weights precisely must first define p polyhedral subspace weight
simplex us w bertsimas tsitsiklis corner weights
vertices p defined set linear inequalities
definition set known payoff vectors define polyhedron
x
p x x wi
wi


matrix elements row vectors augmented column vector
setpof linear inequalities x supplemented simplex constraints
wi wi vector x w wd u consists weight vector
scalarized value weights corner weights weights contained
vertices p form w wd u
note due simplex constraints p dimensional furthermore
extrema weight simplex special cases corner weights
identifying u ols identifies corner weights change polyhedron p
adding u fortunately require recomputation corner weights
done incrementally first corner weights q u yields better
value currently known deleted queue line function
newcornerweights u wdel line calculates corner weights involve u
solving system linear equations see u intersects boundaries
relevant subset present vectors
newcornerweights u wdel line first calculates set relevant payoff
vectors arel taking union maximizing vectors weights wdel
arel



w

wwdel

w contains fewer payoff vectors boundary weight simplex
involved boundaries stored possible subsets size vectors
boundaries taken subset weight payoff vectors
boundaries intersect u computed solving system
linear equations intersection weights subsets together form set candidate
corner weights wcan newcornerweights u wdel returns subset wcan
inside weight simplex u higher scalarized value payoff
fact implementation optimize step caching w w q



firoijers whiteson oliehoek

vector already figure b shows one corner weight labelled wc
practice arel small systems linear equations need solved
calculating corner weights wu line u added line
cheng showed finding best payoff vector corner weight adding
partial ccs e solvecog w guarantees best improvement
theorem cheng maximum value
max

min w u w v

w uccs vs

e maximal improvement adding vector one corner weights
cheng
theorem guarantees correctness ols corner weights checked
payoff vectors thus maximal improvement must ols found
full ccs
prioritization
chengs linear support assumes corner weights checked inexpensively
reasonable assumption pomdp setting however since solvecog expensive
operation testing corner weights may feasible mo cogs therefore unlike
chengs linear support ols pops one w q tested per iteration making
ols efficient thus critically depends giving w suitable priority adding
q end ols prioritizes corner weight w according maximal possible
improvement upper bound improvement us w upper bound computed
respect ccs optimistic hypothetical ccs e best case scenario
final ccs given current partial ccs w set weights already
tested solvecog key advantage ols chengs linear support
priorities computed without calling solvecog obviating need run solvecog
corner weights
definition optimistic hypothetical ccs ccs set payoff vectors yields
highest possible scalarized value possible w consistent finding vectors
weights w
figure b denotes ccs dotted line note ccs
superset value uccs w us w weights w

given w maxvaluelp finds scalarized value uccs
w solving
max w v
subject w v us w
however theory possible construct partial ccs corner weight
payoff vectors adel



ficomputing ccss faster multi objective coordination

us w vector containing us w w w note abuse notation
w case matrix whose rows consist weight vectors set
w
ccs define maximal possible improvement
w uccs w us w
figure b shows wc dashed line use maximal relative possible improvement r w w uccs w priority corner weight w wu

figure b r wc
corner weight w identified line

added q priority r w long r w lines
wc figure b added q popped element
q solvecog wc generates vector yielding
illustrated figure c corner weights
points intersects testing weights illustrated
figure payoff vectors causing ols terminate maximal
improvement corner weights thus due theorem ccs upon
termination ols called solvecog weights resulting exactly payoff
vectors ccs payoff vectors v displayed grey dashed black
lines figure never generated
variable elimination linear support
exact cog used implement solvecog naive
explicitly compute values joint actions v select joint action maximizes
value
solvecog w arg max w u
u v

implementation solvecog combination ols yields
refer non graphical linear support ngls ignores graphical structure
flattening cog standard multi objective cooperative normal form game
main downside computational complexity solvecog linear v
equal exponential number agents making feasible
mo cogs agents
contrast use section implement solvecog better
call resulting variable elimination linear support vels dealt
multiple objectives outer loop ols vels relies exploit graphical
structure inner loop yielding much efficient method ngls
analysis
analyze computational complexity vels
implementation ols reduces size lp subset weights w
joint actions involved w w found optimal lead slight

overestimation uccs
w



firoijers whiteson oliehoek

theorem runtime vels
ccs wccs n amax w cnw cheur
w induced width running ccs size ccs wccs
number corner weights uccs w cnw time costs run newcornerweights
cheur cost computation value optimistic ccs maxvaluelp
proof since n amax w runtime theorem runtime vels
quantity plus overhead per corner weight cnw cheur multiplied number
calls count calls consider two cases calls adding
vector vector instead confirm
optimality scalarized value weight former size final ccs
ccs latter number corner weights final ccs wccs
overhead ols e computing corner weights cnw calculating
maximal relative improvement cheur small compared solvecog calls
practice newcornerweights u wdel computes solutions small set
linear equations equations maxvaluelp w w computes solutions
linear programs polynomial size inputs
number corner weights smaller ccs runtime
vels thus n amax w ccs number corner weights twice ccs
minus constant solvecog finds payoff vector one corner weight
removed three corner weights added loose bound wccs

total number possible combinations payoff vectors boundaries ccs


however obtain tighter bound observing counting number corner
weights given ccs equivalent vertex enumeration dual
facet enumeration e counting number vertices given corner weights kaibel
pfetsch
theorem arbitrary wccs bounded
avis devroye

ccs b
c

ccs



ccs b
c


ccs

proof follows directly mcmullens upper bound theorem facet enumeration henk richter gebert ziegler mcmullen
reasoning used prove theorems used establish following
corollary runtime vels
ccs wccs n amax w cnw cheur ccs size ccs
wccs number corner weights uccs w
practice vels often test corner weights polyhedron spanned
ccs cannot guaranteed general section empirically
ccs decreases rapidly increases
reduction footnote used small subset w used making even smaller



ficomputing ccss faster multi objective coordination

figure left runtimes pmove cmove vels different values
varying numbers agents n n factors actions per agent
objectives right corresponding sizes ccss
theorem space complexity vels ccs wccs n amax w

proof ols needs store every corner weight vector length queue
wccs ols needs store every vector vectors length
furthermore solvecog called memory usage added memory
usage outer loop ols memory usage n amax w theorem
ols adds memory requirements vels almost memory
efficient thus considerably memory efficient cmove theorem
empirical evaluation
empirically evaluate vels comparison cmove pmove longer
compare non graphical method clearly dominated cmove
pmove refer cmove section mean basic cmove
fastest tested scenarios use random graphs mining day
benchmark experiments section run ghx intel core computer
gb memory
random graphs
test vels randomly generated mo cogs use mo cog generation
procedure section determine scalability exact approximate
vels compares pmove cmove tested random mo cogs
increasing numbers agents average number factors per agent held
n number objectives figure shows
averaged mo cogs number agents note runtimes left
axis log scale set sizes right
demonstrate vels efficient cmove two objective
random mo cogs runtime exact vels average times less


firoijers whiteson oliehoek

cmove cmove solves random mo cogs agents average whilst
exact vels handle agents
already large gain achieve even lower growth rate permitting
small agents permitting error margin yields gain
order magnitude reducing runtime permitting error reduces
runtime thus reduce runtime vels factor
retaining accuracy compared cmove agents vels
times faster
speedups explained slower growth ccs figure right
small numbers agents size ccs grows slightly slowly
size full ccs however certain number agents onwards size
ccs grows marginally size full ccs keeps growing
ccs grew payoff vectors payoff vectors agents
marginally agents contrast full ccs grew
vectors agents keeps growing agents
similar picture holds ccs grows rapidly vectors
vectors agents grows slowly agents stabilizes
reach vectors agents agents full ccs grows
vectors vectors making almost times large ccs
times larger ccs
test scalability vels respect number objectives tested
random mo cogs constant number agents factors n n
increased number objectives compare
scalability cmove kept number agents n number local
payoff functions small order test limits scalability number
objectives number actions per agent figure left plots number
objectives runtime log scale ccs grows exponentially
number objectives figure right runtime cmove exponential
number objectives vels however linear number corner weights
exponential size ccs making vels doubly exponential exact vels
faster cmove approximate vels
times faster however even approximate vels
slower cmove
unlike number agents grows size ccs figure right
stabilize number objectives grows seen following table
ccs
























therefore conclude vels compute ccs faster cmove objectives
less cmove scales better number objectives vels however scales
better number agents


ficomputing ccss faster multi objective coordination

figure left runtimes cmove vels varying numbers objectives right size ccs varying numbers objectives

figure left plot runtimes cmove vels different values
varying n right loglogplot runtime vels
agent mining day instances varying values
mining day
compare cmove vels mining day benchmark generation procedure section generated mining day instances increasing n
averaged runtimes figure left agents cmove reached runtime
exact vels compute complete ccs mo cog agents
time indicates vels greatly outperforms cmove structured
objective mo cog moreover allow error takes
compute ccs agents speedup order magnitude
measure additional speedups obtainable increasing test vels
large generated mining day instances n
averaged instances per value instances exact vels runs
n n n average expected increasing
leads greater speedups figure right however close e


firoijers whiteson oliehoek

ccs close full ccs speedup small increased beyond certain
value dependent n decline becomes steady shown line log log plot
increases factor runtime decreases factor
thus vels compute exact ccs unprecedented
numbers agents well structured addition small
values enable large speedups increasing leads even bigger improvements
scalability

memory efficient methods
cmove vels designed minimize runtime required compute ccs
however cases bottleneck may memory instead memory efficient methods
cogs related recently received considerable attention dechter
mateescu marinescu mateescu dechter section
outer loop method vels naturally memory efficient
therefore solve much larger mo cogs inner loop method cmove
memory restricted addition cmove vels modified
produce even memory efficient variants
tree search
begin background tree search dechter mateescu
marinescu mateescu dechter yeoh felner koenig class
solving single objective cogs tuned provide better space
complexity guarantees however improvement space complexity comes
price e runtime complexity worse mateescu dechter background
provide brief broader overview tree search cogs related
please see work dechter marinescu multi objective
versions work marinescu
tree search work converting graph pseudo tree pt
agent need know actions ancestors descendants pt
take order select action example agent node pt two
subtrees agents conditionally independent
agents given ancestors figure shows pt coordination
graph figure
next tree search perform tree search
search tree aost agent aost node children nodes
corresponding one agent actions turn children nodes
nodes corresponding agent children pt action nodes
agent agents nodes agents actions appear
tree multiple times figure b shows aost graph figure
specific joint action constructed traversing tree starting root
selecting one alternative childen node e one action agent
continuing children node example figure b joint
action indicated grey retrieve value joint action must
first define value nodes


ficomputing ccss faster multi objective coordination

figure pseudo tree b corresponding search tree
definition value node vai representing action ai agent
sum local payoff functions scope ai together node
ancestors actions specifies action agent scope local payoff functions
example figure b total payoff cog u u
u value grey node u u payoff function
agent scope together ancestral nodes grey node
completes joint local action u
retrieve optimal action must define value subtree aost
definition value subtree v ti rooted node aost
maximum value subtrees rooted node children value
subtree v tai rooted node ai aost value ai definition
plus sum value subtrees rooted node children ai
memory efficient way retrieve optimal joint action aost
euler touring e performing depth first search computing values
subtrees generating nodes fly deleting evaluated memory
usage minimized refer simply tree search ts
earlier sections implementation employs tagging scheme tagging value
subtree actions maximize
ts single objective method extended compute pcs
yielding call pareto ts pts marinescu define pts must
update definition set pareto optimal payoffs refer subtree value
set intermediate pcs ipcs
definition intermediate pcs subtree ip cs ti rooted node
pcs union intermediate pcss children ch
ip cs ti pprune



aj ch



ip cs taj

firoijers whiteson oliehoek

intermediate pcs subtree ip cs tai rooted node ai pcs
value ai definition plus cross sum intermediate pcss subtrees
rooted node children ai



ip cs tj vai
ip cs tai pprune
jch ai

thus pts replaces max operator ts pruning operator pmove replaces
max operator pruning operator
memory efficient ccs
propose two memory efficient computing ccs straightforward variants cmove vels
first call convex ts cts simply replaces pprune cprune
definition thus cts pts different pruning operator
seen cmove replaced ts advantage cts pts
analogous cmove pmove highly beneficial compute local
ccss instead local pcss intermediate coverage sets input next
subproblem sequential search scheme regardless whether scheme ts
cts memory efficient cmove still requires computing intermediate
coverage sets take space typically large ccs
size bounded total number joint actions
second addresses employing ols ts singleobjective solver subroutine solvecog yielding tree search linear support tsls thus
tsls vels replaced ts tsls outer loop method
runs ts sequence requiring memory used ts overhead
outer loop consists partial ccs definition priority queue
consequently tsls even memory efficient cts
analysis
ts much better space complexity e linear number agents n
theorem time complexity ts n amax n number agents
amax maximal number actions single agent depth pseudo
tree uses linear space n
proof number nodes aost bounded n amax tree creates
maximally amax children node every node exactly one child
number nodes would bounded amax pt deep however
branching pt node multiple children branch increases
size aost amax nodes exactly n agents
pt happen n times node aost ts performs
summation scalars maximization scalars ts performs depth first
search n nodes need exist point execution


ficomputing ccss faster multi objective coordination

tss memory usage usually lower required store original singleobjective memory amax emax number local payoff
functions amax maximal size action space single agent
emax maximal size scope single local payoff function
pt depth different constant induced width w typically
larger however bounded w
theorem given mo cog induced width w exists pseudo tree
depth w log n dechter mateescu
thus combining theorems shows agents ts
much memory efficient relatively small runtime penalty
time space complexity ts establish following
corollaries time space complexity cts tsls
corollary time complexity cts n amax r r runtime
cprune
proof n amax bounds number nodes aost node aost
cprune called
runtime cprune terms size input given equation note
size input cprune depends size intermediate ccss
children node case node input size iccsmax c
c maximum number children node nodes
amax iccsmax
corollary space complexity cts n iccsmax iccsmax
maximum size intermediate ccs execution cts
proof ts n nodes aost need exist point
execution node contains intermediate ccs
cts thus much memory efficient cmove space complexity
exponential induced width theorem
corollary time complexity tsls ccs w ccs n amax cnw
cheur w log n
proof proof theorem time complexity
replaced ts
terms memory usage outer loop ols large advantage
inner loop overhead outer loop consists partial
ccs definition priority queue vels theorem thus much better
space complexity cmove theorem tsls advantage cts
vels cmove therefore tsls low memory usage since requires
memory used ts plus overhead outer loop
note c turn upper bounded n loose bound



firoijers whiteson oliehoek

corollary space complexity tsls ccs w ccs n
w log n
proof proof theorem space complexity
replaced ts
mentioned section ts memory efficient member class
tree search members class offer different trade offs
time space complexity possible create inner loop
corresponding outer loop basis time
space complexity analyses performed similar manner
corollaries advantages outer loop methods compared corresponding
inner loop methods however remain tsls cts therefore
article focus comparing memory efficient inner loop method
memory efficient outer loop method
empirical evaluation
section compare cts tsls cmove vels use
random graphs mining day benchmark obtain pts cts tsls
use heuristic cmove vels generate elimination order
transform pt w log n holds whose existence guaranteed
theorem procedure suggested bayardo miranker
random graphs
first test random graphs employing generation procedure
section connections agents graphs generated
randomly induced width varies different average induced
width increases number local payoff functions even ratio
local payoff factors number agents remains constant
order test sizes different mo cog solution methods
handle within limited memory generate random graphs two objectives varying
number agents n n local payoff functions previous sections
limited maximal available memory kb imposed timeout
figure shows vels scale agents within given memory constraints non memory efficient methods particular pmove cmove
handle agents respectively given induced width w
must store amax w local css agents induced width figure c
agents induced width vels handle agents
induced width memory demands come running
inner loop outer loop adds little overhead need store one payoff
local payoff function agent elimination whereas pmove
cmove must store local coverage sets thus outer loop vels
instead inner loop cmove already yields significant improvement
sizes tackled limited memory


ficomputing ccss faster multi objective coordination



b

c

figure runtimes ms tsls vels cts cmove pmove random objective mo cogs varying numbers agents n n local payoff
factors b runtimes approximate tsls varying amounts allowed error
compared exact vels parameters c
corresponding induced widths mo cogs b

however scaling beyond agents requires memory efficient figure
shows cts tsls require runtime handle agents
within memory constraints fact unable generate mo cog enough
agents cause methods run memory tsls faster cts case
times faster reasons vels faster cmove
however speed advantage outer loop allow
bit error scalarized value trade accuracy runtime figure b
agents exact tsls average runtime times slower
vels however runtime times slower
times slower times slower furthermore
relative increase runtime number agents increases less higher thus
approximate version tsls highly attractive method cases memory
runtime limited
mining field
compare performance cmove vels tsls variation mining
day call mining field longer consider cls consistently higher
runtime tsls worse space complexity use mining field order ensure
interesting memory restricted setting mining day see section
induced width depends parameter specifying connectivity villages
increase number agents factors therefore whether
vels memory efficient enough handle particular instance depends primarily
parameter number agents
mining field villages situated along mountain ridge placed
grid number agents thus n use random placement mines
ensuring graph connected induced width connected grid
generate grid graphs larger instances higher induced width


firoijers whiteson oliehoek

village



mine

b

c

figure example mining field instance additional mines
marked b runtimes ms tsls varying amounts allowed
error vels cmove objective mining field instances
varying numbers additional mines grid size c
corresponding induced widths mining field instances

induced width thus longer depends connectivity parameter increases
number agents factors graph
example mining field instance provided figure choose distance
adjacent villages grid unit length map place
mines local payoff functions connect agents arbitrary tree agent
local payoff functions mines figures mines span tree unmarked
connected mines black edges require factors build tree
add additional mines independently placing random point
map inside grid mine placed connect villages within
r radius mine map chose therefore maximum
connectivity factor mine created fashion figure mines
marked rewards per mine per worker well number workers per
village generated way mining day
compare runtimes memory requirements cmove vels tsls
mining field tested instance agents mb available memory
tsls use three different values exact use time limit
minutes increase number additional mines factors
total onwards steps
setup possible solve instances pmove
ran memory fact pmove succeeded tree shaped
e one without additional factors figures b c
remaining methods cmove runs memory additional factors factors
total contrast vels runs memory additional factors induced
width
compared random graph section induced widths
cmove vels handle lower mining field suspect


ficomputing ccss faster multi objective coordination

grid shaped number factors highest induced
width need exist parallel execution higher
tsls run memory tested instances face
unable generate instances tsls run memory however
run time tsls first exceeds time limit additional mines
happens tsls ran time
differences runtime tsls vels larger random graphs
therefore difficult compensate slower runtime tsls choosing
higher much slower tsls compared vels thus seems depend
structure mo cog
mining field confirm conclusion random graph experiments
outer loop vels instead inner loop cmove yields
significant improvement sizes tackled limited memory
futhermore tsls used solve sizes beyond vels handle
within limited memory approximate version tsls appealing choice cases
memory runtime limited

conclusions future work
article proposed exploit loose couplings compute ccs
multi objective coordination graphs showed exploiting loose couplings
key solving mo cogs many agents particular showed theoretically
empirically computing ccs considerable advantages computing pcs
terms runtime memory usage experiments consistently shown
runtime pcs methods grows lot faster ccs methods
cmove deals multiple objectives inner loop e computes local ccss
looping agents contrast vels deals multiple objectives
outer loop e identifies weights maximal improvement upon partial ccs
made solves scalarized single objective weights yielding
anytime addition cts tsls memory efficient variants
methods proved correctness analyzed complexity
cmove vels complementary methods cmove scales better number
objectives vels scales better number agents compute ccs leading large additional speedups furthermore vels memory efficient
cmove fact vels uses little memory single objective
however memory restricted vels cannot applied tsls provides
memory efficient alternative tsls considerably slower vels
loss compensated allowing error
numerous possibilities future work mentioned section ols
generic method applied multi objective fact together
authors already applied ols large multi objective mdps showed
ols extended permit non exact single objective solvers roijers et al
future work intend investigate approximate methods mo cogs approximate single objective solvers cogs e g lp relaxation methods sontag
globerson jaakkola attempt optimal balance


firoijers whiteson oliehoek

levels approximation inner outer loop respect runtime guarantees
empirical runtimes
many methods exist single objective coordination graphs single parameter
controls trade memory usage runtime furcy koenig rollon
corresponding multi objective inner loop version
computes pcs marinescu devised would interesting
create inner outer loop methods methods compute ccs
instead compare performance particular shown ols requires
little extra memory usage compared single objective solvers would interesting
investigate much extra memory could used single objective solver inside ols
comparison corresponding inner loop method
addition work mo cogs aim extend work sequential
settings particular look developing efficient method multiagent multi objective mdps better exploiting loosely couplings first try
develop approximate version sparse cooperative q learning kok vlassis
b however may possible general effects agent
agents via state impossible bound general therefore hope identify
broadly applicable subclass multi agent momdps approximate
method yields substantial speed compared exact methods

acknowledgements
thank rina dechter introducing us memory efficient methods cogs
mo cogs radu marinescu tips memory efficient methods implementation would thank maarten inja well anonymous reviewers valuable feedback supported nwo dtc ncap
nwo catch projects nwo innovational incentives scheme veni frans oliehoek affiliated
university amsterdam university liverpool

references
avis devroye l estimating number vertices polyhedron information processing letters
bayardo r j j miranker p space time trade solving constraint
satisfaction ijcai proceedings fourteenth international
joint conference artificial intelligence
bertsimas tsitsiklis j introduction linear optimization athena scientific
bishop c pattern recognition machine learning springer
cassandra littman zhang n incremental pruning simple fast exact
method partially observable markov decision processes uai proceedings
thirteenth conference uncertainty artificial intelligence pp


ficomputing ccss faster multi objective coordination

cheng h partially observable markov decision processes ph
thesis university british columbia vancouver
dechter r reasoning probabilistic deterministic graphical exact vol synthesis lectures artificial intelligence machine
learning morgan claypool publishers
dechter r mateescu r search spaces graphical artificial
intelligence
delle fave f stranders r rogers jennings n bounded decentralised
coordination multiple objectives proceedings tenth international joint
conference autonomous agents multiagent systems pp
dubus j gonzales c perny p choquet optimization gai networks
multiagent multicriteria decision making adt proceedings first
international conference algorithmic decision theory pp
feng z zilberstein region incremental pruning pomdps uai
proceedings twentieth conference uncertainty artificial intelligence pp
furcy koenig limited discrepancy beam search ijcai proceedings nineteenth international joint conference artificial intelligence pp

guestrin c koller parr r multiagent factored mdps
advances neural information processing systems nips
henk richter gebert j ziegler g basic properties convex polytopes
handbook discrete computational geometry ch pp crc
press boca
kaibel v pfetsch e algorithmic polytope theory
algebra geometry software systems pp springer
kok j r vlassis n sparse cooperative q learning proceedings
twenty first international conference machine learning icml york ny
usa acm
kok j r vlassis n max plus multiagent decision
making coordination graphs robocup robot soccer world cup ix pp

kok j vlassis n b collaborative multiagent reinforcement learning payoff
propagation journal machine learning
koller friedman n probabilistic graphical principles techniques mit press
lizotte bowling murphy efficient reinforcement learning multiple
reward functions randomized clinical trial analysis proceedings th
international conference machine learning icml pp


firoijers whiteson oliehoek

marinescu r razak wilson n multi objective influence diagrams
uai proceedings twenty eighth conference uncertainty artificial
intelligence
marinescu r search strategies combinatorial optimization graphical ph thesis university california irvine
marinescu r exploiting decomposition multi objective constraint optimization principles practice constraint programming cp pp
springer
marinescu r efficient approximation multi objective constraint
optimization adt proceedings second international conference
algorithmic decision theory pp springer
mateescu r dechter r relationship search variable
elimination uai proceedings twenty first conference uncertainty
artificial intelligence pp
mcmullen p maximum numbers faces convex polytope mathematika

oliehoek f spaan j dibangoye j amato c heuristic search
identical payoff bayesian games aamas proceedings ninth international joint conference autonomous agents multiagent systems pp

pearl j probabilistic reasoning intelligent systems networks plausible inference morgan kaufmann
pham brys taylor e brys drugan bosman p cock
lazar c demarchi l steenhoff et al learning coordinated
traffic light control proceedings adaptive learning agents workshop
aamas vol pp
roijers scharpff j spaan j oliehoek f de weerdt whiteson
bounded approximations linear multi objective uncertainty icaps proceedings twenty fourth international conference
automated scheduling pp
roijers vamplew p whiteson dazeley r survey multiobjective sequential decision making journal artificial intelligence

roijers whiteson oliehoek f b computing convex coverage sets
multi objective coordination graphs adt proceedings third international conference algorithmic decision theory pp
roijers whiteson oliehoek f linear support multi objective
coordination graphs aamas proceedings thirteenth international
joint conference autonomous agents multi agent systems pp
rollon e multi objective optimization graphical ph thesis universitat politecnica de catalunya barcelona


ficomputing ccss faster multi objective coordination

rollon e larrosa j bucket elimination multiobjective optimization journal heuristics
rosenthal nonserial dynamic programming optimal proceedings
ninth annual acm symposium theory computing pp acm
scharpff j spaan j volker l de weerdt uncertainty coordinating infrastructural maintenance proceedings th annual
workshop multiagent sequencial decision making certainty
sontag globerson jaakkola introduction dual decomposition
inference optimization machine learning
tesauro g das r chan h kephart j lefurgy c levine w rawson f
managing power consumption performance computing systems
reinforcement learning advances neural information processing systems
nips
vamplew p dazeley r barker e kelarev constructing stochastic mixture policies episodic multiobjective reinforcement learning tasks advances
artificial intelligence pp
yeoh w felner koenig bnb adopt asynchronous branch andbound dcop journal artificial intelligence




