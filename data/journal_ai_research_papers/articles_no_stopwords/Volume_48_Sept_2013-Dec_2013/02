journal artificial intelligence

submitted published

survey multi objective sequential decision making
diederik roijers

roijers uva nl

informatics institute
university amsterdam
amsterdam netherlands

peter vamplew

p vamplew ballarat edu au

school science
information technology engineering
university ballarat
ballarat victoria australia

shimon whiteson

whiteson uva nl

informatics institute
university amsterdam
amsterdam netherlands

richard dazeley

r dazeley ballarat edu au

school science
information technology engineering
university ballarat
ballarat victoria australia

abstract
sequential decision making multiple objectives arise naturally practice pose unique challenges decision theoretic learning
largely focused single objective settings article surveys designed sequential decision making multiple objectives though
growing body literature subject little makes explicit circumstances special methods needed solve multi objective therefore
identify three distinct scenarios converting single objective
one impossible infeasible undesirable furthermore propose taxonomy
classifies multi objective methods according applicable scenario nature
scalarization function projects multi objective values scalar ones type
policies considered factors determine nature optimal solution single policy convex hull pareto front taxonomy
survey literature multi objective methods learning finally
discuss key applications methods outline opportunities future work

introduction
sequential decision commonly modeled markov decision processes mdps
bellman occur range real world tasks robot control kober
peters game playing szita clinical management patients peek
military aberdeen thiebaux zhang control elevators crites
barto power systems ernst glavic wehenkel water supplies
bhattacharya lobbrecht solomantine therefore development
c

ai access foundation rights reserved

firoijers vamplew whiteson dazeley

automatically solving given model mdp e g
via dynamic programming methods bellman b learning interaction
unknown mdp e g via temporal difference methods sutton barto
important challenge artificial intelligence
topics desirability undesirability actions
effects codified single scalar reward function typically objective
autonomous agent interacting mdp maximize expected possibly
discounted sum rewards time many tasks scalar reward function
natural e g financial trading agent could rewarded monetary gain
loss holdings recent time period however many tasks
naturally described terms multiple possibly conflicting objectives e g
traffic control system minimize latency maximize throughput autonomous
vehicle minimize travel time fuel costs multi objective
widely examined many areas decision making zeleny cochrane vira
haimes stewart diehl haimes roijers whiteson oliehoek
growing albeit fragmented literature addressing multi objective decisionmaking sequential settings
article present survey devised
settings begin section formalizing multi objective mdp
momdp section motivate multi objective perspective decisionmaking little existing literature multi objective makes explicit
multi objective beneficial crucially cases cannot trivially reduced
single objective solved standard address
describe three motivating scenarios multi objective
section present novel taxonomy organizes multi objective
terms underlying assumptions nature resulting solutions key
difficulty existing literature authors considered many different types
often without making explicit assumptions involved differ
authors scope applicability resulting methods taxonomy
aims fill void
sections survey momdp learning methods respectively organizing according taxonomy identifying key differences
approaches examined learning areas section surveys applications
methods covering specific applications general classes
momdp methods applied section discusses future directions field
gaps literature identified sections section concludes

background
finite single objective markov decision process mdp tuple hs r
finite set states
finite set actions
transition function specifying state action
next state probability next state occurring


fia survey multi objective sequential decision making

r reward function specifying state action next
state expected immediate reward
probability distribution initial states
discount factor specifying relative importance immediate rewards
goal agent acts environment maximize expected return
rt function rewards received timestep onwards typically
return additive boutilier dean hanks e sum rewards
infinite horizon mdp return typically infinite sum term discounted
according

x
k rt k
rt
k

rt reward obtained time parameter thus quantifies relative
importance short term long term rewards
contrast finite horizon mdp return typically undiscounted finite sum
e certain number timesteps process terminates reward
obtained single multi objective methods developed finite horizon
discounted infinite horizon average reward settings puterman sake
brevity formalize infinite horizon discounted reward mdps article
agents policy determines actions selects timestep broadest
sense policy condition everything known agent state indepedent
value function v specifies expected return following initial state
v e r



policy stationary e conditions current state
formalized specifies state action probability
taking action state specify state value function policy
v e rt st
st bellman equation restates expectation recursively
stationary policies
x
x
r v
v





note bellman equation forms heart standard solution
dynamic programming bellman b temporal difference methods sutton
barto explicitly relies assumption additive returns important
explain section multi objective settings interfere
additivity property making learning methods rely bellman
equation inapplicable
formalizations settings see example overview van otterlo wiering



firoijers vamplew whiteson dazeley

state value functions induce partial ordering policies e better
equal value greater states


v v
special case stationary policy deterministic stationary policy one
action chosen probability every state deterministic stationary policy
seen mapping states actions single objective mdps
least one optimal policy e stationary deterministic
theorem additive infinite horizon single objective mdp exists deterministic stationary optimal policy see e g howard boutilier et al
one optimal policy exists share value function known
optimal value function v max v bellman optimality equation defines
optimal value function recursively
x
r v
v max




note maximizes actions equation makes use fact
optimal deterministic stationary policy optimal policy maximizes
value every state policy optimal regardless initial state distribution
however state independent value equation may well different different
initial state distributions state value function translated back
state independent value function equation
x
v
v
ss

multi objective mdp momdp mdp reward function r
n describes vector n rewards one objective instead scalar
similarly value function v momdp specifies expected cumulative discounted
reward vector

x
k rk

v e
k

rt vector rewards received time difference single
objective value equation multi objective value equation policy
return underlying sum rewards vector rather scalar
stationary policies define multi objective value state
v e


x

k rt k st



k

single objective mdp state value functions impose partial ordering

policies compared different states e g possible v v v
multi objective mdps confused mixed observability mdps ong png hsu lee
sometimes abbreviated momdp



fia survey multi objective sequential decision making



v given state ordering complete e v must greater

equal less v true state independent value functions
contrast momdp presence multiple objectives means value
function v state vector expected cumulative rewards instead scalar
value functions supply partial ordering even given state example


possible state vi vi vj vj similarly


state independent value functions may vi vi vj vj consequently
unlike mdp longer determine values optimal without additional
information prioritize objectives information provided
form scalarization function discuss following sections
though focus article momdp variants constraints
specified objectives see e g feinberg shwartz altman
goal agent maximize regular objectives meeting constraints
objectives constrained objectives fundamentally different regular
objectives explicitly prioritized regular objectives e policy
fails meet constraint inferior policy meets constraints regardless
well policies maximize regular objectives

motivating scenarios
momdp setting received considerable attention immediately obvious useful addition standard mdp specialized
needed fact researchers argue modeling explicitly multiobjective necessary scalar reward function adequate sequential
decision making tasks direct formulation perspective suttons reward
hypothesis states mean goals purposes well
thought maximization expected value cumulative sum received scalar
signal reward
view imply multi objective exist indeed
would difficult claim since easy think naturally possess
multiple objectives instead implication reward hypothesis resulting
momdps converted single objective mdps additive returns
conversion process would involve two steps first step specify scalarization
function
definition scalarization function f function projects multi objective
value v scalar value
vw f v w
w weight vector parameterizing f
example f may compute linear combination values case element
w quantifies relative importance corresponding objective setting discussed section second step define single objective mdp
http rlai cs ualberta ca rlai rewardhypothesis html



firoijers vamplew whiteson dazeley

figure three motivating scenarios momdps unknown weights scenario
b decision support scenario c known weights scenario

additive returns expected return equals scalarized value
vw
though rarely ever makes issue explicit momdps rests
premise exist tasks one conversion steps impossible
infeasible undesirable section discuss three scenarios occur
see figure
first scenario call unknown weights scenario figure occurs
w unknown moment learning must occur consider example
public transport system aims minimize latency e time commuters
need reach destinations pollution costs addition assume resulting
momdp scalarized converting objective monetary cost economists
compute cost lost productivity due commuting pollution incurs tax
must paid pollution credits purchased given price assume credits
traded open market therefore price constantly fluctuates transport
system complex may infeasible compute plan every day given latest
prices scenario preferable use multi objective method
computes set policies price one policies optimal
see learning phase figure computationally
expensive computing single optimal policy given price needs done
done advance computational resources available
time select policy current weights e price pollution


fia survey multi objective sequential decision making

credits used determine best policy set selection phase finally
selected policy employed task execution phase
unknown weights scenario scalarization impossible learning
trivial policy actually needs used w known time
contrast second scenario call decision support scenario figure
b scalarization infeasible throughout entire decision making process
difficulty specifying w even f example economists may able accurately
compute cost lost productivity due commuting user may fuzzy
preferences defy meaningful quantification example transport system could
made efficient building train line obstructs beautiful view
human designer may able quantify loss beauty difficulty specifying
exact scalarization especially apparent designer single person
committee legislative body whose members different preferences agendas
system momdp method used calculate optimal solution set
respect known constraints f w figure b shows decision support
scenario proceeds similarly unknown weights scenario except selection
phase user users select policy set according arbitrary preferences
rather explicit scalarization according given weights
cases one still argue scalarization learning
possible principle example loss beauty quantified measuring
resulting drop housing prices neighborhoods previously enjoyed unobstructed
view however difficulty scalarization may impractical
importantly forces users express preferences way may
inconvenient unnatural selecting w requires weighing hypothetical
trade offs much harder choosing set actual alternatives
well understood phenomenon field decision analysis clemen
standard workflow involves presenting alternatives soliciting preferences
subfields decision analysis multiple criteria decision making multiattribute utility theory focus multiple objectives dyer fishburn steuer wallenius
zionts reasons momdps provide critical decision
support rather forcing users specify w advance
prune policies would optimal w offer users range
alternatives select according preferences whose relative importance
easily quantified
third scenario call known weights scenario figure c assume
w known time learning thus scalarization possible
feasible however may undesirable difficulty second step
conversion particular f nonlinear resulting single objective mdp
may additive returns see section optimal policy may
non stationary see section stochastic see section cannot occur
single objective additive infinite horizon mdps see theorem consequently mdp
difficult solve standard methods applicable converting mdp
one additive returns may help cause blowup state space


firoijers vamplew whiteson dazeley

leaves intractable therefore even though scalarization possible
w known may still preferable use methods specially designed momdps
rather convert single objective mdp contrast unknown
weights decision support scenarios known weights scenario momdp
method produces one policy executed e separate selection
phase shown figure c
note figure assumes line scenario learning occurs
execution however multi objective methods employed line settings
learning interleaved execution line version
unknown weights scenario weights better characterized dynamic rather
unknown line scenario agent must already seen weights timesteps
since prerequisite execution timesteps however
weights change time agent may yet know weights used
timestep learning phase timestep

taxonomy
far described momdp formalism proposed three motivating scenarios
section discuss constitutes optimal solution unfortunately
simple answer question depends several critical factors therefore
propose taxonomy shown table categorizes momdps according
factors describes nature optimal solution category
taxonomy call utility contrast many
multi objective papers follow axiomatic optimality momdps
utility rests following premise execution phases
scenarios section one policy selected collapsing value vector policy
scalar utility scalarization function application scalarization
function may implicit hidden e g may embedded thought process
user nonetheless occurs scalarization function part notion utility
e agent maximize therefore set optimal solution
possible weight setting scalarization function solved momdp
utility derives optimal solution set assumptions
made scalarization function policies user allows whether need
one multiple policies
contrast axiomatic begins axiom optimal solution set
pareto front see section limiting demonstrate
section settings solution concepts suitable
thus take utility makes possible derive solution
concept rather assuming pareto front fact correct solution
since non additive returns depend agents entire history immediate reward function
converted mdp may depend history thus state representation converted
mdp must augmented include
example axiomatic multi objective reinforcement learning see survey
liu xu hu



fia survey multi objective sequential decision making

single policy
known weights
deterministic
linear
scalarization

multiple policies
unknown weights decision support

stochastic

one deterministic stationary
policy

monotonically one
increasing
deterministic
scalarization
non stationary
policy

deterministic

stochastic

convex coverage set
deterministic stationary policies


one mixture
policy two

deterministic
stationary
policies

pareto
coverage set
deterministic
non stationary
policies

convex
coverage set
deterministic
stationary
policies

table momdp taxonomy showing critical factors
nature resulting optimal solution columns describe whether
necessitates single policy multiple ones whether policies
must deterministic specification allowed stochastic rows
describe whether scalarization function linear combination rewards
whether cannot assumed scalarization function merely
monotonically increasing function contents cell describe
optimal solution given setting looks

concept utility provides justification allows
appropriate solution concept derived instead
taxonomy categorizes classes assumptions scalarization function policies user allows whether one multiple policies
required leads different solution concepts underscoring importance
carefully considering choice solution concept available information
discuss three factors constitute taxonomy following order
section discuss first factor whether one multiple policies sought choice
follows directly motivating scenario applicable known weights
scenario figure c implies single policy unknown weights decision
support scenarios figure b imply multiple policy section
discuss second factor whether scalarization function linear combination
rewards merely monotonically increasing function section discuss
third factor whether stochastic deterministic policies permitted
goal taxonomy cover momdps remaining simple
intuitive however due diversity momdps
fit neatly taxonomy note discrepancies discussing
sections


firoijers vamplew whiteson dazeley

single versus multiple policies
following vamplew et al first distinguish
one policy sought ones multiple policies sought case holds
depends three motivating scenarios discussed section applies
unknown weights decision support scenarios solution momdp
consists multiple policies though two scenarios conceptually quite different
algorithmic perspective identical reason characterized strict separation decision making process two phases
learning phase execution phase though line settings agent may go
back forth two
learning phase w unavailable consequently learning must return single policy set policies corresponding
multi objective values set contain policies suboptimal
scalarizations e interested undominated policies
definition momdp scalarization function f set undominated
policies u subset possible policies exists w
scalarized value maximal


u w vw vw



u sufficient solve e w contains policy optimal
scalarized value however may contain redundant policies optimal
weights optimal policy set w policies removed
still ensuring set contains optimal policy w fact order solve
need subset undominated policies possible w
least one policy set optimal sometimes called coverage set cs becker
zilberstein lesser goldman
definition momdp scalarization function f set cs
coverage set subset u every w contains policy maximal
scalarized value e









cs u w cs vw vw

note u automatically coverage set however u unique cs
need multiple policies value u contains
coverage set need contain one addition given cs

may exist policy
cs v different v cs
scalarized value cs w optimal
contrast single objective mdps momdps whether policy cs
depend initial state distribution thus important accurately specify
formulating momdp
ideally momdp smallest cs however
might harder finding one smaller u section specialize
coverage set two classes scalarization functions


fia survey multi objective sequential decision making

execution phase single policy chosen set returned
learning phase executed unknown weights scenario assume w revealed
learning complete execution begins selecting policy
requires maximizing scalarized value policy returned set
argmax vw
cs

decision support scenario set manually inspected user select
policy execution informally making implicit trade objectives
known weights scenario w known learning begins therefore
returning multiple policies unnecessary however mentioned section discussed
section scalarization yield single objective mdp difficult
solve
linear versus monotonically increasing scalarization functions
second critical factor affecting constitutes optimal solution momdp
nature scalarization function section discuss two types scalarization
function linear combinations rewards merely
monotonically increasing functions
linear scalarization functions
common assumption scalarization function e g natarajan tadepalli
barrett narayanan f linear e computes weighted sum
values objective
definition linear scalarization function computes inner product weight vector
w value vector v
vw w v

element w specifies much one unit value corresponding objective
contributes scalarized value elements weight vector w positive real
numbers constrained sum
linear scalarization functions simple intuitive way scalarize one common
situation applicable rewards easily translated monetary
value example consider mining task different policies yield different expected
quantities minerals prices per kilo minerals fluctuate daily
task formulated momdp objective corresponding different
mineral element v reflects expected number kilos mineral
mined scalarized value vw corresponds monetary value
everything mined vw computed w corresponding
normalized current price per kilo mineral becomes known
single policy setting w known presence multiple objectives poses
difficulties given linear f instead f simply applied reward vector


firoijers vamplew whiteson dazeley

momdp inner product computed f distributes addition
single objective mdp additive returns infinite horizon setting leads
vw w v w e


x

k rt k e


x

k w rt k



k

k

since single objective mdp additive returns solved standard methods yielding single policy reflected box labeled table due theorem
determinstic stationary policy suffices however multi objective still
preferable case e g v may easier estimate vw large continuous
momdps function approximation required see section
multiple policy setting however know w learning
therefore want coverage set f linear u automatically
coverage set consists convex hull substituting equation definition
undominated set definition obtain definition convex hull
definition momdp convex hull ch subset
exists w linearly scalarized value maximal


ch w w v w v



figure illustrates concept convex hull stationary deterministic policies
point plot represents multi objective value given policy two objective
momdp axes represent reward dimensions convex hull shown set
filled circles connected lines form convex surface given linear f
scalarized value policy linear function weights illustrated
figure b x axis represents weight dimension w w
axis scalarized value policies select policy need know
values convex hull policies form upper surface scalarized value
illustrated black solid lines correspond three convex hull policies
figure upper surface forms piecewise linear convex function functions
well known literature partially observable markov decision processes
pomdps whose relationship momdps discuss section
u ch contain superfluous policies however define
convex coverage set ccs specification coverage set f linear
reflected box table explain policies set deterministic
stationary section
definition momdp set ccs convex coverage set
subset ch every w contains policy whose linearly scalarized value
maximal e



ccs ch w ccs w v w v
note term convex hull slightly different meaning multi objective literature
standard geometric definition geometry convex hull finite set points euclidean
space minimal subset points expressed convex
combination points convex hull multi objective setting interested
particular subset geometric convex hull points convex combinations strictly
bigger dimensions point e points optimal weight



fia survey multi objective sequential decision making



b

figure example convex hull pareto front point represents
multi objective value given policy line b represents linearly
scalarized value policy across values w convex hull shown black
filled circles black lines b pareto front consists filled
points circles squares dashed solid black lines
b unfilled points grey lines b dominated

deterministic stationary policies difference ch ccs may
often small therefore terms often used interchangeably however case
non stationary stochastic policies difference quite significant ch
contain infinitely many policies possible construct finite ccs
section
monotonically increasing scalarization functions
linear scalarization functions intuitive simple adequate
expressing users preferences example suppose mining task mentioned
two minerals mined three policies available
sends mining equipment location first mineral mined
location second mineral mined location
minerals mined suppose owner equipment prefers e g
least partially appeases clients different interests however may case
location corresponding fewer minerals convex hull contains
thus owners preference implies implicitly explicitly
employs nonlinear scalarization function
consider case f nonlinear corresponds common
notion relationship reward utility class possibly nonlinear scalarizations strictly monotonically increasing scalarization functions functions
adhere constraint policy changed way value increases


firoijers vamplew whiteson dazeley

one objectives without decreasing objectives scalarized
value increases
definition scalarization function f strictly monotonically increasing






vi vi vi vi w vw vw



linear scalarization functions non zero positive weights included class
functions condition left hand side equation commonly known
pareto dominance pareto
definition policy pareto dominates another policy value least
high objectives strictly higher least one objective






v p v vi vi vi vi



demanding f strictly monotonically increasing quite minimal constraint
requires things equal getting reward certain objective
better fact difficult think f violates constraint without
employing highly unnatural notion reward
three observations order strictly monotonically increasing scalarization
functions related concept pareto dominance first unlike linear case
necessarily know exact shape f instead know belongs
particular class functions solution concept follows thus applies strictly
monotonically increasing f cases stronger assumptions f made
specific solution concepts possible however except linearity aware
properties f exploited solving momdps
second notions optimality introduced section longer appropriate
reason even though vector valued returns still additive equation
scalarized returns may f may longer linear example consider
well known tchebycheff scalarization function perny weng
x
v p w max wi pi vi
wi pi vi

n

n

p optimistic reference point w weights arbitrarily small positive
constant greater note sum righthand side makes function
strictly monotonically increasing p p r r
k
w f v w e
k f rt k w
loss additivity scalarized returns applying nonlinear
f important consequences methods applied section
third still identify prune policies optimal w
strictly monotonically increasing f even though may nonlinear consider three
addition f strictly monotonically increasing assumptions made
policies pruned coverage set thus computing value every policy coverage
set required selection phase likely intractable
definition differs slightly perny weng multiplied express
maximization instead minimization sake consistency rest article



fia survey multi objective sequential decision making

labeled policies figure note figure b apply scalarization
function longer linear b higher value one objective lower
value therefore cannot tell whether b ought preferred without
knowing w however c lower value objectives thus paretodominates c p c f strictly monotonically increasing scalarized value
greater c w thus discard c
defer full discussion constitutes optimal solution
momdp strictly monotonically increasing scalarization function e boxes
table depends whether single multiple policy setting
applies whether deterministic stochastic policies considered
addressed section
however already observe given strictly monotonically increasing f
use pareto front set viable policies pareto front consists policies
pareto dominated
definition momdp pareto front set policies
pareto dominated policy


p f v p v



note p f set undominated policies u specific strictly
monotonically increasing f already seen special case linear f
u ch subset p f example figure pareto
front consists convex hull plus b however strictly monotonically increasing
f know policy p f dominated respect f e
p f u strictly monotonically increasing f

p f cannot exist w optimal since definition exists

v p v since f strictly monotonically increasing implies

vw vw
however know f strictly monotonically increasing cannot settle
subset p f exist strictly monotonically increasing f
u p f perny weng u p f
tchebycheff function equation strictly monotonically increasing therefore
cannot discard policies p f retain undominated set u
strictly monotonically increasing f
pareto coverage set pcs minimal size constructed retaining one
policy policies identical vector values p f formally define
pcs follows
definition momdp set p cs pareto coverage set subset
p f every policy contains policy dominates
equal value e




p cs p f p cs v p v v v
deterministic stationary policies difference p cs p f
may minor note p f automatically p cs papers
literature therefore take p f solution


firoijers vamplew whiteson dazeley

slightly relax constraint f without change policies
p cs specifically define monotonically increasing scalarization

function function following property holds vi vi w vw

vw relaxation influences set undominated policies policies
p f dominated strictly monotonically increasing f need
monotonically increasing f consider example f v w
monotonically increasing strictly monotonically increasing function
dominated policies every policy scalarized value however
scalarized value policy p f cannot greater scalarized
function policy p cs use p cs non strict monotonically
increasing f therefore article focus monotonically increasing f
broader class functions
p f even p cs may prohibitively large contain
many policies whose values differ negligible amounts chatterjee et al brazdil
et al introduce slack parameter use define approximate pareto
front p f p f contains values policies every possible policy

policy p f vi vi weakening
requirements domination yields smaller set calculated
efficiently
another option finding smaller set p f making additional assumptions
scalarization function example perny weng goldsmith hanna
introduce notion fairness objectives leading lorentz optimality
additional assumption sum values objectives stays
making difference two objectives smaller yields higher scalarized value
course strong assumption apply broadly pareto optimality however
apply help reduce size optimal solution set
deterministic versus stochastic policies
third critical factor affecting constitutes optimal solution momdp
whether deterministic polices considered stochastic ones allowed
applications reason exclude stochastic policies priori
cases stochastic policies clearly undesirable even unethical example
policy determines clinical treatment patient e g work lizotte bowling
murphy shortreed laber lizotte stroup pineau murphy
flipping coin determine course action may inappropriate denote

set deterministic policies
set stationary policies sets subsets




policies finally set policies deterministic


stationary intersection sets denoted
ds
single objective mdps factor critical due theorem
restrict search deterministic stationary policies e optimal attainable value

v however
attainable deterministic stationary policy maxm v max



ds

situation complex momdps section discuss focus
stochastic deterministic policies affects setting considered taxonomy


fia survey multi objective sequential decision making

deterministic stochastic policies linear scalarization
functions
f linear similar theorem holds momdps due following
corollary

corollary momdp ccs
ds ccs

proof f linear translate momdp single objective mdp
possible w done treating inner product reward vector w
rewards leaving rest since inner product distributes
addition scalarized returns remain additive equation thus every w
exists translation single objective mdp optimal deterministic
stationary policy must exist due theorem hence w exists optimal
deterministic stationary policy therefore exists ccs
ds optimal



w consequently cannot exist ds w v w v

thus ccs
ds ccs
ccs
ds thus sufficient solving momdps linear f even stochastic non stationary policies allowed reflected box table
applies box since optimal policy case member ccs
ds
e one best given known w
unfortunately analogous corollary holds momdps monotonically increasing f rest section discuss consequences
nature optimal momdp solution boxes table
multiple deterministic policies monotonically increasing
scalarization functions
multiple policy setting deterministic policies allowed f nonlinear
non stationary policies may better best stationary ones
theorem infinite horizon momdps deterministic non stationary policies paretodominate deterministic stationary policies undominated deterministic stationary policies white
see consider following momdp denoted adapted example
white one state three actions yield rewards
respectively allow deterministic stationary policies
three possible policies
ds corresponding taking
one actions pareto optimal policies following
state independent values equation v v
v however consider set possibly non stationary


policies
including non stationary ones construct policy ns ds
alternates starting whose value vns
consequently ns p thus cannot restrict


firoijers vamplew whiteson dazeley

attention stationary policies consequently multiple deterministic policies case
monotonically increasing f need p cs
includes non stationary
policies shown box table
addition consider broader class policies another consequence
defining policy indirectly via value function longer possible standard
single objective methods optimal policy found local action selection
respect value function e every state policy selects action
maximizes expected value however local selection yield non stationary policy value function must non stationary e must condition current
timestep standard finite horizon setting different value function computed timestep possible infinite horizon setting
discuss address difficulty sections
multiple stochastic policies monotonically increasing
scalarization functions
multiple policy setting stochastic non stationary policies e full set
allowed cannot consider deterministic stationary policies however
employ stochastic stationary policies instead deterministic non stationary ones
particular employ mixture policy vamplew dazeley barker kelarev
takes set n deterministic
policies selects th policy set
p
probability pi n
p


leads values linear combination

values constituent policies previous example replace ns
policy chooses probability p otherwise resulting following
values


p p



v p v p v




fortunately necessary explicitly represent entire p cs explicitly
instead sufficient compute ccs
ds necessary stochastic policies
create p cs easily constructed making mixture policies
policies ccs
ds
corollary infinite horizon discounted momdp infinite set mixture policies
pm constructed policies ccs
ds set pm

p cs vamplew et al
proof construct policy value vector convex surface e g
thereblack lines figure mixing policies ccs
ds e g black dots
fore construct mixture policy dominates policy value
surface e g b contradiction cannot policy
white shows infinite horizon discounted setting arguments hold
finite horizon average reward settings
note mix policies adjacent line pair policies
mix convex surface e g mixing policy represented leftmost black dot
figure policy represented rightmost black dot lead optimal policies
line connecting two points convex surface



fia survey multi objective sequential decision making

convex surface would optimal w f linear consequently corollary would deterministic stationary policy least
equal value since convex surface spans values ccs
ds leads
contradiction thus policy pareto dominate mixture policy convex
surface
thanks corollary sufficient compute ccs
ds solve momdp
reflected box table surprising consequence fact knowledge
made explicit literature pareto optimality though common
solution concept associated multi objective actually necessary one
specific setting
observation multiple policy setting f monotonically increasing
deterministic policies considered box table requires computing pareto coverage set f linear stochastic policies allowed ccs
ds suffices
wakuta proves sufficiency ccs
ds monotonically increasing
scalarizations multiple stochastic policies box table infinite horizon
momdps different way instead mixture policies corollary uses stationary randomizations deterministic stationary policies wakuta togawa
provide similar proof average reward case
note common consider non stationary stochastic policies f
nonlinear policies typically condition current state current state
time agents reward history however setting policies condition
reward history dominate example suppose two
objectives take positive values f simply selects smaller two e
f v w mini vi suppose given state two actions available
yields rewards respectively finally suppose agent arrive
state one two reward histories whose discounted sums
policy conditions discounted reward histories outperform policies
e optimal policy selects action yielding reward history sums
action yielding reward history sums
single objective mdps markov property additive returns sufficient restrict
attention policies ignore history multi objective case scalarized returns
longer additive therefore optimal policy depend history examples
methods exploit fact steering mannor shimkin
reward augmented state thresholded lexicographic ordering method geibel
discussed section
single deterministic stochastic policies monotonically
increasing scalarization functions
remains address single policy setting monotonically increasing f
nature optimal solution case follows directly reasoning given
multiple policy setting
deterministic policies considered single policy sought may
non stationary reflected box table reasons elucidated whites


firoijers vamplew whiteson dazeley

example hard define non stationary policy local action selection
due risk circular dependencies q values
stochastic policies allowed optimal policy may stochastic
represented mixture policy two deterministic stationary policies
reflected box table reasons given corollary cases
policies potentially benefit conditioning reward history

momdps
section survey key approaches momdps e computing
optimal policy coverage set undominated policies given complete model
momdp following taxonomy presented section first consider single policy
methods turn multiple policy methods linear monotonically increasing
scalarization functions
single policy
known weights scenario w known begins single
policy optimal w must discovered since momdp transformed
single objective mdp f linear see section focus single policy
nonlinear f
discussed section nonlinear f cause scalarized return nonadditive consequently single objective dynamic programming linear programming
methods exploit assumption additive returns employing bellman equation applicable however different linear programming formulations singlepolicy momdps possible key feature methods
produce stochastic policies discussed section optimal
scalarization function nonlinear aware single policy
methods work arbitrary nonlinear f methods developed two special
cases particular perny weng propose linear programming method
momdps scalarized tchebycheff function mentioned section
tchebycheff function w pareto optimal policy optimal
single policy pareto front addition ogryczak perny
weng propose analogous method ordered weighted regret metric
metric calculates regret objective respect estimated ideal reference
point sorts descending order calculates weighted sum weights
descending order
researchers proposed single policy methods momdps constraints
feinberg shwartz consider momdps one regular objective objectives inequality constraints feasible policy exists setting
deterministic stationary finite number timesteps n
prior timestep n random actions must performed call n
policy pareto optimal values achieved n policies propose
linear programming finds approximate policies setting
general momdps constraints considered particular altman
proposes several linear programming approaches settings


fia survey multi objective sequential decision making

furnkranz hullermeier cheng park propose framework mdps
qualitative reward signals related momdps fit neatly
taxonomy qualitative reward signals indicate preference policies actions
without directly ascribing numeric value since preferences induce partial
ordering policies policy iteration method authors propose setting
may applicable momdps nonlinear f pareto dominance induces partial
orderings however authors note multi objective tasks generally numeric
feedback exploited thus suggest quantitative momdps
viewed subset preference mdps methods designed specifically
momdps may efficient general preference methods
multiple policy linear scalarization functions
multiple policy setting linear f seek ccs
ds note however
distinction convex hull convex coverage set usually made
literature
one might argue explicitly multi objective methods necessary setting one could repeatedly run single objective methods obtain ccs
ds
however since infinitely many possible w obvious possible values w covered might possible devise way run single objective
methods finite number times still guarantee ccs
ds produced however
would nontrivial corresponding would essence
multi objective method happens use single objective methods subroutines
one attempted minimally sized ccs
e convex
coverage set deterministic necessarily stationary policies originally proposed
white kim translate momdp partially observable markov
decision process pomdp sondik intuitive way think translation
imagine fact one true objective agent unaware
objectives momdp modeled pomdp defining state
tuple hs state momdp n indicates
true objective observations thus identify exactly give information
note translation momdps pomdps one way every
pomdp translated equivalent momdp
typically agent interacting pomdp maintains belief e probability
distribution states pomdp derived momdp belief decomposed belief belief former degenerative
known latter vector size n th element specifies probability
th objective true one vector analogous w linear f
fact reason figure b resembles piecewise linear value functions often
depicted pomdps difference whether x axis interpreted w
belief
white kim finite horizon case solution every belief
exactly solution w solutions resulting pomdp
exactly original momdp infinite horizon case difficult
infinite horizon pomdps undecidable madani hanks condon however


firoijers vamplew whiteson dazeley

sufficiently large horizon solution finite horizon pomdp used
approximate solution infinite horizon momdp
solve resulting pomdp white kim propose combination sondiks
one pass smallwood sondik policy iteration pomdps sondik
however pomdp method used long
require initial belief pomdp state would correspond initializing
momdp state w computes optimal policy every
possible belief recently developed exact methods e g cassandra littman
zhang kaelbling littman cassandra meet conditions
could thus employed approximate point pomdp methods spaan vlassis
pineau gordon thrun meet conditions could
adapted compute approximate convex hull choosing prior distribution
weights could sample online pomdp methods ross pineau
paquet chaib draa applicable plan given belief
converting pomdp thus allows use pomdp methods
solving momdps linear f however inefficient
exploit characteristics distinguish momdps general pomdps e
part state known observations give information
example methods compute policies trees e g kaelbling et al
exploit fact deterministic policies stationary functions state
needed momdps linear f furthermore mentioned general infinite
horizon pomdps undecidable momdps fact possible compute
ccs
ds exactly
reasons researchers developed specialized methods
setting viswanathan aggarwal nair propose linear programming
episodic momdps wakuta togawa propose policy iteration
three phases first phase uses policy iteration narrow set
possibly optimal policies second phase uses linear programs check optimality
since necessarily give definitive answer third phase uses another linear
program handle undetermined solutions left second phase
barrett narayanan propose convex hull value iteration chvi computes ch
every state chvi extends conventional value iteration storing
ds
set vectors q state action pair representing convex hull policies involving action sets vectors correspond q values single objective
setting contain optimal q values possible w backup operation
performed q hulls next state propagated back possible next

state possible actions considered e union convex hulls q
taken weighted probability occurring taking action state
procedure similar witness kaelbling et al pomdps
lizotte et al propose value iteration finite horizon setting
computes different value function timestep addition uses piecewise
linear spline representation value functions authors prove offers asymptotic
time space complexity improvements representation used chvi
enables application momdps continuous states however


fia survey multi objective sequential decision making

applicable two objectives limitation addressed
authors subsequent work lizotte bowling murphy extends
arbitrary number objectives provides detailed implementation
case three objectives
multiple policy monotonically increasing scalarization
functions
section consider momdps monotonically increasing f discussed section stochastic policies allowed mixture policies deterministic
stationary policies sufficient therefore focus case deterministic
policies allowed consider methods compute p cs
include
non stationary policies distinction p f


p
cs

usually
made literature
linear case scalarizing every w obtaining p cs
singleobjective methods problematic infinitely many w consider unlike
linear case additional difficulty scalarized returns may longer
additive make single objective methods inapplicable
daellenbach kluyver present multi objective routing tasks
essentially deterministic momdps uses dynamic programming conjunction augmented state space non pareto dominated policies iteratively
number iterations equals maximum number steps route finds undominated sub policies parallel authors use two alternative explicit
scalarization functions call weighted minsum weighted minmax operators first values solutions translated objective value
becomes fractional difference optimal values objective across
solutions value objective multiplied positive weight finally
minimum sum minsum minimum maximal value minmax
weighted fractional differences chosen scalarization note
scalarization functions monotonically increasing objectives optimal value
objective individually depend scalarization function
white extends work proposing dynamic programming method
approximately solves infinite horizon momdps repeatedly backing according multi objective version bellman equation since policies
non stationary size pareto front grows rapidly number backups applied
however white notes number need large acceptable approximations reached nonetheless feasible small momdps
wiering de jong address difficulty dynamic programming method
called con modp deterministic momdps computes optimal stationary policies
con modp works enforcing consistency dp updates policy consistent
suggests action timesteps given state inconsistent policy
inconsistent one state action pair con modp makes consistent forcing
current action taken time current state visited inconsistency runs
deeper policy discarded


firoijers vamplew whiteson dazeley

contrast gong proposes linear programming finds paretofront stationary policies however authors note suitable
small momdps number constraints decision variables
linear program increase rapidly state space grows
mentioned section one way cope intractably large pareto fronts
compute instead approximate pareto front much smaller chatterjee
et al propose linear programming method computes approximate front
infinite horizon momdp chatterjee propose analogous
average reward setting cases stationary stochastic policies shown
sufficient
another way improve scalability setting give whole
state space instead plan line agents current state monte carlo tree
search kocsis szepesvari approaches proven
successful e g game go gelly silver increasingly popular
single objective mdps wang sebag propose monte carlo tree search method
deterministic momdps single objective tree search methods typically optimistically
explore tree selecting actions maximize upper confidence bound
value estimates multi objective variant respect scalar
multi objective value function whose definition hypervolume indicator induced proposed action together set pareto optimal policies computed
far hypervolume indicator zitzler thiele laumanns fonseca da fonseca
measures hypervolume pareto dominated set points since pareto
front maximizes hypervolume indicator optimistic action selection strategy focuses
tree search branches likely compliment existing archive

learning momdps
methods reviewed section assume model transition reward
dynamics momdp known cases model directly available
multi objective reinforcement learning morl used instead
one way carry morl take model e use agents
interaction environment learn model transition reward function
momdp apply multi objective methods described
section though seems well suited morl papers
considered e g lizotte et al discuss opportunities future work
model morl section instead work morl focused
model free methods model transition reward function never explicitly
learned
section survey key morl approaches majority
methods single policy setting multiple policy methods developed first glance may seem multiple policy methods unlikely effective
learning setting since finding policies would increase sample costs
computational costs former typically much scarcer resource however modelbased methods obviate issue enough samples gathered learn
useful model finding policies optimal weights requires computation model

fia survey multi objective sequential decision making

free methods practical multiple policy setting employ policy
learning sutton barto precup sutton dasgupta makes possible learn one policy data gathered another way policies
multiple weight settings optimized data
single policy learning methods
known weights scenario morl aims learn single policy
optimal given weights discussed section linear scalarization
equivalent learning optimal policy single objective mdp standard
temporal difference td methods sutton q learning watkins
easily applied
however even though specialized methods needed address setting
nonetheless commonly studied setting morl linear scalarization
uniform weights e elements w equal forms basis work karlsson
ferreira bianchi ribeiro aissani beldjilali trentesaux
shabani amongst others non uniform weights used authors
castelletti et al guo et al perez et al majority
work uses td methods work line although castelletti et al extend line
fitted q iteration ernst geurts wehenkel multiple objectives
cases change made underlying rl rather
scalarizing reward function learning scalar value function resulting
single objective mdp vector valued value function learned original momdp
scalarized selecting actions argument
values individual objectives may easier learn scalarized value particularly
function approximation employed tesauro et al example function
approximator ignore state variables irrelevant objective reducing
size state space thereby speeding learning
discussed section linear scalarization may appropriate scenarios vamplew yearwood dazeley berry demonstrate empirically
practical consequences morl therefore morl methods work
nonlinear scalarization functions substantial importance unfortunately illustrated
section coping setting especially challenging since
td methods bellman equation inherently incompatible
nonlinear scalarization functions due non additive nature scalarized returns
four main classes single policy morl methods non linear scalarization
arisen differ deal issue first class simply applies td
methods without modification approaches resign heuristics guaranteed converge impose restrictions environment ensure
convergence second class modifies td state representation
issue non additive returns avoided third class uses td methods
learn multiple policies linear scalarization different values w
forms stochastic non stationary meta policy optimal respect
nonlinear scalarization fourth class uses policy search methods


firoijers vamplew whiteson dazeley

make use bellman equation hence directly applied combination
nonlinear scalarizations
first class includes methods model multi agent system
one agent per objective agent learns recommends actions basis
return objective global switch selects winning agent whose recommended action followed current state examples include simple winner takes
agent whose recommended action highest q value selected
sophisticated approaches w learning humphrys selected
action one incur loss followed one key weakness
approaches pointed russell zimdars allow
selection actions optimal single objective offer good compromise
multiple objectives another key weakness since actions selected
different timesteps may recommended different agents resulting behavior corresponds policy combines elements learned agent combination
may optimal even single objective e may pareto dominated perform
arbitrarily poorly
td used directly nonlinear scalarization functions allow
consideration actions optimal regards individual
objectives scalarization functions fuzzy logic proposed
discrete actions zhao chen hu continuous
actions lin chung widely cited nonlinear scalarization
gabor kalmar szepesvari designed tasks constraints
must satisfied objectives lexicographic ordering objectives defined
threshold value specified objectives except last state action values
objective exceed corresponding threshold clamped threshold value
prior applying lexicographic ordering thus thresholded lexicographic ordering
tlo scalarization maximizes performance last objective subject
meeting constraints objectives specified thresholds
methods combining td nonlinear scalarization may converge suitable
policy certain conditions converge suboptimal policy even fail
converge conditions example issabekov vamplew demonstrate empirically tlo fail converge suitable policy episodic tasks
constrained objective receives non zero rewards timestep end
episode general methods combination td nonlinear scalarization
must regarded heuristic nature applicable restricted classes
second class avoids caused non additive scalarized returns modifying td state representation knowledge two approaches
proposed geibel address limitations tlo members
class require reward accumulated objective current episode
stored first local decision making scalarized value
sum cumulative reward current state action values eliminates
non additive returns yields policy non stationary respect
observed state meaning may converge second augments state representation cumulative reward converges
correct policy learns slowly due increase size state space


fia survey multi objective sequential decision making

third class uses td methods learn policies linear scalarizations
policy selection mechanism nonlinear scalarization used form
meta policy base policies multiple directions reinforcement learning
mdrl mannor shimkin uses
context line learning non episodic tasks user specifies target region within
long term average reward lie initial active policy chosen arbitrarily
followed average reward moves outside target region agent
specified reference state point direction current average reward
vector closest point target set calculated policy whose direction best
matches target direction selected active policy way average reward
steered towards users specified target region underlying base policies
utilize linear scalarization nature policy selection mechanism means
overall non stationary policy formed base policies optimal nonlinear
scalarization specified users defined target set vamplew et al suggest
similar episodic tasks td used first learn policies optimal
linear scalarization range different w stochastic mixture policy
constructed optimal regards nonlinear scalarization
fourth class uses policy search directly learn policy without learning value function single policy morl policy search approaches
focused policy gradient methods sutton mcallester singh mansour kohl
stone kober peters methods policy iteratively adjusted
direction gradient value respect parameters usually probability
distributions actions per state policy shelton proposes
first learns optimal policy individual objective used base policies form initial mixture policy stochastically selects base policy start
episode hill climbing method weighted convex combination
normalized objective gradients iteratively improves mixture policy
directly fit taxonomy returns never scalarized instead
weights used step direction relative current policy parameters
practical perspective behavior akin single policy rl nonlinear
scalarization function converges single pareto optimal policy need lie
convex hull uchibe doya propose policy gradient method
morl called constrained policy gradient rl cpgrl uses gradient projection technique policies whose average reward satisfies constraints one
objectives sheltons cpgrl learns stochastic policies works
nonlinear scalarization functions
multiple policy learning linear scalarization functions
unknown weights decision support scenarios f linear morl
aim learn ccs possible policies simple inefficient used
castelletti et al run td multiple times different values w
simplest case runs conducted sequentially gradually build approximate
ccs natarajan tadepalli showed made efficient
reusing policies learned earlier runs similar w


firoijers vamplew whiteson dazeley

improves greatly sample costs learning policy w similar already
visited previous runs however many samples typically still required good
approximate ccs obtained
sophisticated approximating convex coverage set learn multiple policies parallel several proposed achieve within
td learning framework hiraoka yoshida mishima similar
chvi barrett narayanan see section
learns parallel optimal value function w convex hull representation
prone infinite growth number vertices convex hull polygons
threshold margin applied hull representations iteration eliminating points contribute little hulls hypervolume hiraoka et al present
adapt margins learning improve efficiency note many
parameters must tuned effective performance mukai kuroe iima present
similar extension chvi learning context address problematic growth
number values stored pruning vectors q value update vector
selected random set vectors stored given state action pair
others lying within threshold distance deleted
approaches hiraoka et al mukai et al designed
line learning contrast multi objective fitted q iteration mofqi castelletti
pianosi restelli line learning multiple policies mofqi
multi objective extension fitted q iteration fqi ernst et al
uses combination historical data single step transition dynamics
environment initial function approximator q learning update rule construct
dataset maps state action pairs expected return dataset used
train improved function approximator process repeats values
function approximator converge mofqi provides computationally efficient extension
fqi multiple objectives including w input function approximator
constructing expanded training data set containing training instances randomly
generated ws since learned function generalizes across weight space addition
state action space used construct policy w
discussed section lizotte et al lizotte et al describe valueiteration convex hull policies finite horizon tasks note
method applied learning context estimating model state transition
probabilities immediate rewards basis experience environment
demonstrated task analyzing randomized drug trial data producing
estimates historical data gathered clinical trials
multiple policy learning monotonically increasing scalarization
functions
f nonlinear morl unknown weights decision support scenarios aim learn pcs linear scalarization case simplest
run single objective multiple times varying w shelton demonstrates policy gradient vamplew et al
tlo method gabor et al however requires


fia survey multi objective sequential decision making

f explicitly known learning may undesirable decision
support scenario
knowledge currently methods learning multiple policies
nonlinear f value function might seem possible adapt convex
hull methods chvi pareto dominance operators place convex hull
calculations straightforward scalarized values policies
certain state non additive cannot restrict stationary policies want
deterministic pareto optimal policies mentioned section however
bellman equation chvi work additivity resulting sufficiency
deterministic policies required discuss options developing multiple policy learning
methods nonlinear f sections
given extensive multi objective evolutionary moeas
coello coello lamont van veldhuizen tan khor lee sathikannan
drugan thierens evolutionary methods rl whiteson surprisingly little work evolutionary approaches morl methods populationbased well suited approximating pareto fronts would thus seem natural
fit f nonlinear knowledge handa b first apply moeas
morl extending estimation distribution eda evolutionary handle multiple objectives eda rl handa uses conditional random fields crf
represent probabilistic policies initial set policies used generate set
episodes best episodes set selected crfs likely produce trajectories generated policies formed crfs constitute
next generation handa b extends eda rl momdps paretodominance fitness metric select best episodes
soh demiris apply moeas morl policies represented
stochastic finite state controllers sfsc optimized two different moeas
nsga standard evolutionary mcma eda use sfscs gives
rise large search space necessitating addition local search operator local
search generates random w uses scalarize rewards performs gradient
search sfsc empirical comparisons multi objective variants three pomdp
benchmarks demonstrate evolutionary methods generally superior purely
local search local search combined evolution usually outperforms
purely evolutionary methods one papers directly consider partially
observable momdps

momdp applications
multi objective methods learning employed wide range
applications simulation real world settings section survey
applications sake brevity list comprehensive instead aims
provide illustrative range examples first discuss use multi objective
methods specific applications second discuss identified broader
classes multi objective methods play useful role


firoijers vamplew whiteson dazeley

specific applications
important factor driving interest multi objective decision making increasing
social political emphasis environmental concerns decisions must
made trade economic social environmental objectives reflected
fact substantial proportion applications multi objective methods
environmental component
perhaps extensively researched application water reservoir control considered castelletti et al castelletti pianosi soncini sessa
castelletti et al castelletti pianosi restelli general
task control policy releasing water dam balancing multiple uses
reservoir including hydroelectric production flood mitigation management
hydroelectric power production examined shabani another environmental application forest management balance economic benefits
timber harvesting environmental aesthetic objectives demonstrated
simulation gong bone dragicevic
several researchers considered environmentally motivated applications concerning management energy consumption saves system developed kwak
et al controls aspects commercial building lighting heating airconditioning computer systems provide suitable trade energy consumption comfort buildings occupants simulation indicate
saves reduce energy consumption approximately compared manual control
system maintaining slightly improving occupant comfort tesauro et al
liu et al consider controlling computing server
objectives minimizing response time user requests power consumption
guo et al apply morl develop broker agent electricity market
broker sets caps group agents sit hierarchy manage energy
consumption device level must balance energy cost system stability
shelton examines application morl developing broker agents
however case agents task financial rather environmental acting
market maker sets buy sell prices resources market aim balance
objectives maximizing profit minimizing spread difference buy
sell prices lead larger trades
computing communications applications widely considered perez
et al apply morl allocation resources jobs cloud computing scenario objectives maximizing system responsiveness utilization resources
fairness amongst different classes user comsa et al consider maximize
system throughput ensure user equity context long term evolution mobile
communications packet scheduling protocol tong brown use constraint
scalarization address tasks call access control routing broadband multimedia network system aims maximize profit function throughput
satisfying constraints quality service metrics capacity constraints fairness constraints uses methods similar gabor et al zheng li qiu gong
sheltons model market directly model trading spread used proxy




fia survey multi objective sequential decision making

use constrained morl methods make routing decisions cognitive radio
network aiming minimize average transmission delay maintaining acceptably
low packet loss rate
industrial mechanical control important application single objective mdp
methods explored momdp researchers aoki kimura kobayashi
apply distributed rl control sewage flow system exploiting systems hierarchical structure solution minimizes violation stock levels node
flow system smoothing variation flow source aissani et al
apply morl maintenance scheduling within manufacturing plant minimize time
taken complete maintenance tasks machine downtime aissani beldjilali
trentesaux build work applying simulation real petroleum refinery demonstrating ability adapt unscheduled corrective maintenance required
due equipment failures control wet clutch heavy duty transmission systems
examined van vaerenbergh et al twin objectives minimizing
engagement time making transition smooth
robotics popular application momdps though work far
simulation rather real robots maravall de lope consider control
two limbed brachiating robot objectives moving desired direction
avoiding collisions nojima kojima kubota attempt balance
objectives progress target collision avoidance agent makes use
predefined behavioral modules target tracing collision avoidance wall following
morl used dynamically adjust weighting modules meisner
identifies social robots promising application momdp methods behavior
inherently multi objective must carry task without causing anxiety
discomfort humans
morl applied control traffic infrastructure yang wen
apply control freeway ramps vehicle management systems aiming
maximize throughput equity freeway system multiple agents
shared policies used action selection occurring via negotiation agents
similarly dusparic cahill apply morl control traffic lights intersections
urban environment minimize waiting time two different classes vehicles yin
duan li zhang houli zhiheng yi apply morl traffic
light control novelty lies considering different objectives
current state road system minimizing vehicle stops prioritized traffic
free flowing minimizing waiting time emphasized system medium load
minimizing queue length intersections targeted system congested
lizotte et al consider medical application prescribing appropriate
drug regime patient achieve acceptable trade drugs effectiveness severity side effects system learns multiple policies
many robotic applications may ideal avoid collisions completely environments
may possible e g presence moving obstacles whose velocity faster
robot difficult predict may case humans human controlled vehicles
reducing likelihood impact collisions may reasonable attempting
collision free policy see example holenstein badreddin pervez ryu




firoijers vamplew whiteson dazeley

static data produced randomized controlled drug trials selection best
treatment specific patient made doctor patients individual circumstances application excellent example stochastic
approaches mixture policies inappropriate policy maximizes symptom relief side effects one patient minimizes side effects
symptom relief next patient may appear give excellent averaged
across episodes however experience individual patient likely regarded
undesirable
applications within broader learning tasks
addition specific applications discussed several authors identified
general classes tasks multi objective sequential decision making applied
probabilistic risk aware
cheng subrahmanian westerberg argue decision making uncertainty inherently multi objective nature even single reward
considered profit environmental uncertainty means expected value
alone insufficient support good decision making decision maker must consider
variance return similarly bryce states probabilistic
inherently multi objective due need optimize cost probability
success plan criticizes approaches aggregate factors bound one
optimize arguing favor explicitly multi objective methods
aptly named probabilistic multi objective bryce cushing
kambhampati demonstrates might achieved describing method
multi objective dynamic programming belief states multi objective extension
looping ao search set pareto optimal plans recent work
kolobov mausam weld teichteil konigsbuch b examine extension stochastic shortest path ssp methods dead end states exist
ssp methods assume least one policy exists guaranteed reach goal
presence dead ends policy exists authors propose
aim maximize probability reaching goal minimize cost
paths found goal
bryce notes probabilistic plan fails environment enters non goal
absorbing state hence multi objective probabilistic strong parallels
risk aware rl carried geibel geibel wysotzki
add second reward signal indicating transition environment error
state defourny ernst wehenkel provide useful insights incorporation risk awareness mdp methods review range criteria proposed
constraining risk note many nonlinear produce non additive
scalarized returns incompatible local decision making methods
bellman equation recommend custom risk control requirements
mostly enforced heuristically altering policy optimization procedures checking
compliance policies initial requirements multi policy momdp methods treating risk additional objective would satisfy requirement iden

fia survey multi objective sequential decision making

tified coverage set risk aware metric used select best policy
however measures risk may expressed directly discounted cumulative
rewards example agent may wish minimize variance expected return particular reward signal rather discounted cumulative value methods
multi objective probabilistic model checking courcoubetis yannakakis
forejt kwiatkowska norman parker qu forejt kwiatkowska parker
teichteil konigsbuch evaluate whether system modelled mdp satisfies multiple possibly conflicting properties may suitable tasks
multi agent systems
use mdps within multi agent systems widely explored bosoniu babuska
schutter several authors proposed approaches strongly related
momdps multi agent system agent objective effective
overall performance must consider actions affect agents
agents completely self interested framed momdp
treating effects agents additional objectives example mouaddib
uses multi objective dynamic programming facilitate cooperation multiple agents
whose underlying goals may conflicting state action pair agent stores
three values local utility gain agents receive penalty inflicts
agents policy agent established converting vector values
regret ratios applying leximin ordering ratios
dusparic cahill compare application morl multi agent tasks
multi agent methods evolutionary ant colony dusparic
cahill extend w learning humphrys agent learns
local policies one objectives remote policies one
local policy neighboring agents timestep local policies
active remote policies agent nominate actions winning action selected
combining action values across nominating policies weighting term applied
values remote policies determine level cooperation agent offers neighbor
experimental urban traffic control simulator substantial improvement
level cooperation non zero work similar schneider wong
moore riedmiller addresses use multiple agents distributed
network power distribution grid aim maximize global reward
formed combination agents local reward demonstrate
agent focuses local reward policies learned may maximize global
reward performance improved agent perform linearly scalarized
learning local reward rewards neighboring agents
multi objective optimization reinforcement learning
reinforcement learning primarily applied sequential decision making tasks dynamic
environment however employed control search mechanisms static optimization tasks scheduling carchrae beck multi objective optimization
static tasks design well established field majority work


firoijers vamplew whiteson dazeley

employed mathematical evolutionary approaches coello coello et al
authors explored application reinforcement learning contexts
mariano morales b investigate use rl methods ant q
q learning search mechanism optimization multi objective design tasks
values decision variables considered current state actions defined
alter values variables multiple agents explore state space parallel
agents divided families family focuses single objective
end episode final states found agent evaluated undominated solutions kept archive agents discovered solutions rewarded
increasing likelihood similar policies followed future method
shown work small number test evolutionary multi objective
optimization literature liao wu jiang apply rl search static control
settings power generation system objectives reducing fuel usage ensuring voltage stability propose rl formulated specifically tasks
high dimensional state spaces compare performance evolutionary
multi objective finding rl method discovers fronts
accurate better distributed improving speed search
note effectively apply rl multi objective optimization assumptions usually made nature environment example liao et al require
action increases decreases value precisely one state variable
methods likely limited applicability general morl
described earlier

future work
section enumerate possibilities future multi objective
learning
model methods
mentioned section little work model approaches
morl given breadth methods momdps could employed
model morl methods subroutines surprising knowledge
work area lizotte et al model momdps
transition probabilities reward function derived historical data
spline multi objective value iteration applied model general
learning seems negligibly harder single objective setting since
estimates reward function learned separately learning
transition function generally considered hard part model learning identical
single objective setting especially multiple policy scenarios model approaches
morl could greatly reduce sample costs model learned entire
ccs pcs computed line without requiring additional samples


fia survey multi objective sequential decision making

learning multiple policies monotonically increasing scalarization
functions value functions
mentioned section aware methods use value function
learn multiple policies pcs stochastic policies permitted
easier learn ccs
ds use mixture policies
vamplew et al stationary randomizations wakuta policies
ccs see section however deterministic policies permitted
difficult one option could use finite horizon approximation
infinite horizon backwards horizon expected
reward timesteps go approximates infinite horizon value better better
mentioned section similar approaches used pomdp
setting another way good approximations non stationary policies could
learn stationary policies perhaps extending con mdp wiering de jong
learning setting prefix timesteps non stationary policy
many objective sequential decision making
majority reviewed article theoretical applied deals
momdps objectives mirrors state early evolutionary multiobjective focused almost exclusively two three
objectives however last decade growing interest evolutionary
methods called many objective least four sometimes
fifty objectives ishibuchi tsukamoto nojima
shown many perform well objectives scale poorly
number objectives necessitating special many objective setting
many objective mdps received little consideration far numerous real world control naturally modeled way example
fleming et al point many objective control commonly arise
engineering give example jet engine control system eight objectives
many objective considered evolutionary computation seems likely
least methods explored far scale poorly number objectives example multi policy momdp described lizotte
et al limited two objectives
key challenge posed many objective number undominated
solutions typically grows exponentially number objectives particularly
problematic multiple policy momdp methods fleming et al note one
effective approaches used many objective evolutionary computation
incorporate user preferences restrict search space small region interest
particular recommend interactive preference articulation user interactively steers system towards desirable solution optimization vamplew
et al raise possibility incorporating morl
aware actually done


firoijers vamplew whiteson dazeley





b

c





figure momdp two objectives four states
expectation scalarized return
section defined scalarized value vw applying scalarization function f multi objective value v according w e vw f v w
since v expectation means scalarization function applied
expectation computed e
vw f v w f e


x

k rk w

k

formulation refer scalarization expected return ser
standard literature however option possible define
vw expectation scalarized return esr
vw e f


x

k rk w

k

definition used critically affect policies preferred example
consider following momdp illustrated figure four states b c
two objectives agent starts state two possible actions transits
state b c probability transits state probability
actions lead reward states b c one action
leads deterministic reward b c
scalarization function multiplies two objectives together thus ser
vw v v
esr
vw e


x
k

k rk


x
k




k rk

fia survey multi objective sequential decision making

rki reward th objective timestep k w needed example
since f involves constants multi objective
values v v
ser leads scalarized values v v
consequently preferred esr however v
v thus preferred
intuitively ser formulation appropriate policy used many times
return accumulates across episodes e g user policy
time scalarizing expected reward makes sense preferable
expectation accumulate return objectives however policy
used times return accumulate across episodes e g
episode conducted different user esr formulation appropriate
case expected return scalarization interest preferable
yield zero scalarized return given episode
knowledge literature momdps employs esr formulation
even though many real world scenarios seems appropriate
example medical application lizotte et al mentioned section
patient gets one episode treat illness thus clearly interested
maximizing esr ser thus believe developing methods momdps
esr formulation critical direction future

conclusions
article presented survey designed sequential decision making multiple objectives
order make explicit circumstances special methods needed
solve multi objective identified three distinct scenarios converting
single objective one impossible infeasible undesirable well
providing motivation need multi objective methods scenarios represent
three main ways methods applied practice
proposed taxonomy classifies multi objective methods according applicable scenario scalarization function projects multi objective values scalar
ones type policies considered showed factors determine
nature optimal solution single policy coverage set convex
pareto taxonomy utility sees scalarization
function part utility thus part definition contrasts
called axiomatic usually assumes pareto front appropriate
solution showed utility used justify choice
solution set following line thought observed observation computing
pareto front often necessary many cases convex coverage set
deterministic stationary policies sufficient
taxonomy surveyed literature multi objective methods
learning interesting observation learning methods use modelfree rather model identifying latter understudied class


firoijers vamplew whiteson dazeley

methods another part taxonomy yet widely studied learning
case monotonically increasing scalarization functions
discussed key applications momdp methods motivation importance
methods applications identified diverse range fields including environmental management financial markets information communications technology
control industrial processes robotic systems traffic infrastructure addition connections identified multi objective sequential decision making broad
areas probabilistic model checking multi agent systems
general multi objective optimization
finally outlined several opportunities future work include understudied
areas model methods learning monotonically increasing scalarization settings
many objective sequential decision making reformulation objective
momdps expectation scalarized return particularly important
optimize policy executed

acknowledgments
would thank matthijs spaan frans oliehoek matthijs snel marie manner
samy sa well anonymous reviewers valuable feedback work
supported netherlands organisation scientific nwo decisiontheoretic control network capacity allocation project

references
aberdeen thiebaux zhang l decision theoretic military operations
proc icaps vol pp
aissani n beldjilali b trentesaux efficient effective reactive scheduling manufacturing system sarsa multi objective agents mosim th
conference internationale de modelisation et simulation pp
aissani n beldjilali b trentesaux dynamic scheduling maintenance
tasks pretroleum industry reinforcement engineering applications
artificial intelligence
altman e constrained markov decision processes chapman hall crc
london
aoki k kimura h kobayashi distributed reinforcement learning
bi directional decision making multi criteria control multi stage flow systems
th conference intelligent autonomous systems vol pp
barrett l narayanan learning optimal policies multiple criteria
proceedings th international conference machine learning pp
york ny usa acm
becker r zilberstein lesser v goldman c v transition independent
decentralized markov decision processes proc nd intl joint conf
autonomous agents multi agent systems


fia survey multi objective sequential decision making

bellman r e markov decision process journal mathematical mech

bellman r b dynamic programming princeton university press
bhattacharya b lobbrecht h solomantine p neural networks reinforcement learning control water systems journal water resources
management
bone c dragicevic gis intelligent agents multiobjective natural
resource allocation reinforcement learning transactions gis

bosoniu l babuska r schutter b comprehensive survey multiagent
reinforcement learning ieee transactions systems man cybernetics part
c applications reviews
boutilier c dean hanks decision theoretic structural assumptions computational leverage journal artificial intelligence

brazdil brozek v chatterjee k forejt v kucera two views
multiple mean payoff objectives markov decision processes corr abs
bryce value probabilistic plans workshop reality check
scheduling uncertainty icaps
bryce cushing w kambhampati probabilistic multiobjective technical report arizona state university
carchrae beck j c applying machine learning low knowledge control
optimization computational intelligence
cassandra littman l zhang n l incremental pruning simple fast
exact method partially observable markov decision processes proceedings
thirteenth conference uncertainty artificial intelligence pp
castelletti pianosi f restelli multiobjective reinforcement learning
water resources systems operation pareto frontier approximation
single run water resources
castelletti corani g rizzolli soncini sessa r weber e reinforcement learning operational management water system ifac workshop
modeling control environmental issues pp
castelletti galelli restelli soncini sessa r tree reinforcement learning optimal water reservoir operation water resources
w
castelletti pianosi f restelli multi objective fitted q iteration pareto
frontier approximation one single run international conference networking
sensing control pp
castelletti pianosi f restelli tree fitted q iteration multiobjective markov decision processes ieee world congress computational
intelligence


firoijers vamplew whiteson dazeley

castelletti pianosi f soncini sessa r water reservoir control economic social environmental constraints automatica
chatterjee k markov decision processes multiple long run average objectives
fsttcs vol lncs pp
chatterjee k majumdar r henzinger markov decision processes
multiple objectives proceedings rd annual conference theoretical
aspects computer science stacs pp berlin heidelberg springerverlag
cheng l subrahmanian e westerberg multiobjective decision processes
uncertainty applications formulations solution strategies industrial
engineering chemistry
clemen r making hard decisions introduction decision analysis
edition south western college pub
coello coello c lamont g b van veldhuizen evolutionary solving multi objective kluwer academic publishers
comsa aydin zhang kuonen p wagen j f multi objective resource scheduling lte networks reinforcement learning international journal
distributed systems technologies
courcoubetis c yannakakis markov decision processes regular events
ieee transactions automatic control
crites r h barto g improving elevator performance reinforcement
learning touretzky mozer c hasselmo e eds advances
neural information processing systems pp mit press
daellenbach h g kluyver c note multiple objective dynamic
programming journal operational society
defourny b ernst wehenkel l risk aware decision making dynamic
programming nips workshop model uncertainty risk rl
diehl haimes influence diagrams multiple objectives tradeoff analysis systems man cybernetics part systems humans ieee
transactions
drugan thierens stochastic pareto local search pareto neighbourhood
exploration perturbation strategies journal heuristics
dusparic cahill v distributed w learning multi policy optimization selforganizing systems third ieee international conference self adaptive
self organizing systems pp
dusparic cahill v multi policy optimization self organizing systems
soar lncs pp
dyer j fishburn p c steuer r e wallenius j zionts multiple criteria decision making multiattribute utility theory next ten years management
science


fia survey multi objective sequential decision making

ernst geurts p wehenkel l tree batch mode reinforcement learning
journal machine learning
ernst glavic wehenkel l power systems stability control reinforcement learning framework ieee transactions power systems
feinberg e shwartz constrained markov decision weighted
discounted rewards mathematics operations
ferreira l bianchi r ribeiro c multi agent multi objective reinforcement
learning heuristically accelerated reinforcement learning brazilian
robotics symposium latin american robotics symposium pp
fleming p purshouse r lygoe r many objective optimization engineering design perspective evolutionary multi criterion optimization lecture notes
computer science vol pp
forejt v kwiatkowska norman g parker qu h quantitative
multi objective verification probabilistic systems tools
construction analysis systems pp springer berlin heidelberg
forejt v kwiatkowska parker pareto curves probabilistic model
checking automated technology verification analysis pp
springer berlin heidelberg
furnkranz j hullermeier e cheng w park h preference reinforcement learning formal framework policy iteration machine
learning
gabor z kalmar z szepesvari c multi criteria reinforcement learning
fifteenth international conference machine learning pp
geibel p wysotzki f risk sensitive reinforcement learning applied control
constraints journal artificial intelligence
geibel p reinforcement learning bounded risk proceeding th
international conference machine learning pp
geibel p reinforcement learning mdps constraints european conference machine learning vol pp
gelly silver monte carlo tree search rapid action value estimation
computer go artificial intelligence
gong p multiobjective dynamic programming forest resource management
forest ecology management
guo zeman li r reinforcement learning setting multiobjective goals energy demand management international journal agent technologies systems
handa h eda rl estimation distribution reinforcement learning acm sigevo genetic evolutionary computation conference
pp


firoijers vamplew whiteson dazeley

handa h b solving multi objective reinforcement learning eda rl acquisition strategies proceedings ninth internatonal conference
intelligent sysems design applications pp
hiraoka k yoshida mishima parallel reinforcement learning weighted
multi criteria model adaptive margin cognitive neurodynamics
holenstein badreddin e collision avoidance behavior mobile
robot design robotics automation proceedings ieee international conference pp ieee
houli zhiheng l yi z multiobjective reinforcement learning traffic
signal control vehicular ad hoc network eurasip journal advances
signal processing
howard r dynamic programming markov decision processes mit press
humphrys action selection methods reinforcement learning proceedings fourth international conference simulation adaptive behavior pp

ishibuchi h tsukamoto n nojima evolutionary many objective optimisation short review ieee congress evolutionary computation pp
issabekov r vamplew p empirical comparison two common multiobjective reinforcement learning ai th australasian joint
conference artificial intelligence pp
kaelbling l p littman l cassandra r acting
partially observable stochastic domains artificial intelligence
karlsson j learning solve multiple goals ph thesis university rochester
kober j peters j policy search motor primitives robotics machine
learning
kober j peters j reinforcement learning robotics survey wiering
otterlo eds reinforcement learning vol adaptation learning
optimization pp springer berlin heidelberg
kocsis l szepesvari c bandit monte carlo th european
conference machine learning pp springer
kohl n stone p policy gradient reinforcement learning fast quadrupedal
locomotion proceedings ieee international conference robotics
automation pp
kolobov mausam weld theory goal oriented mdps dead
ends proceedings twenty eighth conference uncertainty artificial
intelligence
kwak j varakantham p maheswarn r tambe jazizadeh f kavulya g klein
l becerik gerber b hayes wood w saves sustainable multiagent application conserve building energy considering occupants th international conference autonomous agents multiagent systems pp


fia survey multi objective sequential decision making

liao h wu q jiang l multi objective optimization reinforcement learning
power system dispatch voltage stability innovative smart grid technologies
conference europe
lin c chung f reinforcement neuro fuzzy combiner multiobjective
control ieee transactions systems man cyberbetics part b

liu c xu x hu multiobjective reinforcement learning comprehensive
overview systems man cybernetics part c applications reviews ieee
transactions pp
liu w tan qiu q enhanced q learning dynamic power
management performance constraints date pp
lizotte j bowling murphy efficient reinforcement learning
multiple reward functions randomized clinical trial analysis th international
conference machine learning pp
lizotte j bowling murphy linear fitted q iteration multiple
reward functions journal machine learning
madani hanks condon undecidability probabilistic
infinite horizon partially observable markov decision proceedings
national conference artificial intelligence aaai pp
mannor shimkin n steering multi criteria reinforcement
learning neural information processing systems pp
mannor shimkin n geometric multi criterion reinforcement
learning journal machine learning
maravall de lope j reinforcement learning method dynamic obstacle
avoidance robotic mechanisms computational intelligent systems applied
proceedings th international flins conference pp singapore world scientific
mariano c morales e moaq ant q multiple objective
optimization gecco proceedings genetic evolutionary
computation conference pp
mariano c morales e solution multiple objective
optimization reinforcement learning advances artificial
intelligence international joint conference th ibero american conference ai
th brazilian symposium springer
mariano c morales e b distributed reinforcement learning
multiple objective optimisation lecture notes ai vol proceedings mexican international conference artficial intelligence pp
springer
meisner e learning controllers human robot interaction ph thesis
rensselaer polytechnic institute


firoijers vamplew whiteson dazeley

mouaddib collective multi objective proceedings ieee
workshop distributed intelligent systems collective intelligence applications dis pp washington dc usa ieee computer society
mukai kuroe iima h multi objective reinforcement learning method
acquiring pareto optimal policies simultaneously ieee international conference systems man cybernetics pp
natarajan tadepalli p dynamic preferences multi criteria reinforcement
learning international conference machine learning pp
nojima kojima f kubota n local episode learning multiobjective behavior coordination mobile robot dynamic environments
th ieee international conference fuzzy systems vol pp
ogryczak w perny p weng p minimizing ordered weighted regrets
multiobjective markov decision processes nd international conference
algorithmic decision theory pp
ong c png w hsu lee w uncertainty robotic
tasks mixed observability international journal robotics

pareto v manuel deconomie politique giard paris
peek n b explicit temporal decisiontheoretic clinical
management artificial intelligence medicine
perez j germain renaud c kegl b loomis c responsive elastic computing international conference autonomic computing pp
perny p weng p finding compromise solutions multiobjective markov
decision processes ecai multidisciplinary workshop advances preference
handling pp
perny p weng p goldsmith j hanna j p approximation lorenz optimal
solutions multiobjective markov decision processes workshops twentyseventh aaai conference artificial intelligence
pervez ryu j safe physical human robot interaction past present
future journal mechanical science technology
pineau j gordon g thrun anytime point approximations large
pomdps journal artificial intelligence
precup sutton r dasgupta policy temporal difference learning
function approximation proceedings th international conference
machine learning pp
puterman l markov decision processes discrete stochastic dynamic programming john wiley sons inc
roijers whiteson oliehoek f computing convex coverage sets
multi objective coordination graphs adt proceedings third international conference algorithmic decision theory appear


fia survey multi objective sequential decision making

ross pineau j paquet chaib draa b online
pomdps journal artificial intelligence
russell zimdars l q decomposition reinforcement learning agents
proceedings th international conference machine learning pp
schneider j wong w k moore riedmiller distributed value functions proceedings th international conference machine learning pp
san francisco ca morgan kaufmann
shabani n incorporating flood control rule curves columbia river hydroelectric system multireservoir reinforcement learning optimization model masters
thesis university british columbia
shelton c r importance sampling reinforcement learning multiple objectives ai technical report mit
shortreed laber e lizotte stroup pineau j murphy informing
sequential clinical decision making reinforcement learning empirical study
machine learning
smallwood r sondik e optimal control partially observable markov
processes finite horizon operations
soh h demiris evolving policies multi reward partially observable
markov decision processes mr pomdps gecco proceedings th
annual conference genetic evolutionary computation pp
sondik e optimal control partially observable processes finite horizon
ph thesis stanford university stanford california
sondik e optimal control partially observable markov processes
infinite horizon discounted costs operations
spaan vlassis n perseus randomized point value iteration
pomdps journal artificial intelligence
stewart j critical survey status multiple criteria decision making
theory practice omega
sutton r learning predict methods temporal differences machine
learning
sutton r barto g introduction reinforcement learning st edition
mit press cambridge usa
sutton r mcallester singh mansour policy gradient methods
reinforcement learning function approximation nips pp
szita reinforcement learning games wiering otterlo eds
reinforcement learning vol adaptation learning optimization pp
springer berlin heidelberg
tan k c khor e f lee h sathikannan r evolutionary
advanced goal priority specification multi objective optimization journal
artificial intelligence


firoijers vamplew whiteson dazeley

teichteil konigsbuch f path constrained markov decision processes bridging
gap proceedings twentieth european conference artificial intelligence
teichteil konigsbuch f b stochastic safest shortest path proceedings twenty sixth aaai conference artificial intelligence
tesauro g das r chan h kephart j lefurgy c levine w rawson f
managing power consumption performance computing systems
reinforcement learning neural information processing systems
tong h brown x reinforcement learning call admission control
routing quality service constraints multimedia networks machine learning
uchibe e doya k constrained reinforcement learning intrinsic
extrinsic rewards pp theory novel applications machine learning
tech vienna austria
vamplew p dazeley r barker e kelarev constructing stochastic mixture
policies episodic multiobjective reinforcement learning tasks ai nd
australasian conference artificial intelligence pp
vamplew p dazeley r berry dekker e issabekov r empirical evaluation methods multiobjective reinforcement learning machine learning

vamplew p yearwood j dazeley r berry limitations scalarisation multi objective reinforcement learning pareto fronts ai st
australasian joint conference artificial intelligence pp springer
van otterlo wiering reinforcement learning markov decision processes reinforcement learning state art chap pp springer
van vaerenbergh k rodriguez gagliolo vrancx p nowe stoev j
goossens pinte g symens w improving wet clutch engagement
reinforcement learning international joint conference neural networks
ijcnn
vira c haimes multiobjective decision making theory methodology
north holland
viswanathan b aggarwal v v nair k p k multiple criteria markov
decision processes tims studies management science
wakuta k togawa k solution procedures markov decision processes optimization journal mathematical programming operations

wakuta k note structure value spaces vector valued markov decision
processes mathematical methods operations
wang w sebag hypervolume indicator dominance reward multiobjective monte carlo tree search machine learning
watkins c j c h learning delayed rewards ph thesis cambridge
university


fia survey multi objective sequential decision making

white c c kim k solution procedures solving vector criterion markov
decision processes large scale systems
white multi objective infinite horizon discounted markov decision processes
journal mathematical analysis applications
whiteson evolutionary computation reinforcement learning wiering
van otterlo eds reinforcement learning state art chap
pp springer berlin
wiering de jong e computing optimal stationary policies multiobjective markov decision processes ieee international symposium approximate dynamic programming reinforcement learning pp ieee
yang z wen k multi objective optimization freeway traffic flow via
fuzzy reinforcement learning method rd international conference advanced
computer theory engineering vol pp
yin duan h li z zhang multi objective reinforcement learning
traffic signal coordinate control th world conference transport
zeleny cochrane j l multiple criteria decision making vol mcgrawhill york
zhao chen q hu w multi objective reinforcement learning
mosdmp unknown environment proceedings th world congress
intelligent control automation pp
zheng k li h qiu r c gong multi objective reinforcement learning
routing cognitive radio networks walking random maze international
conference computing networking communications pp
zitzler e thiele l laumanns fonseca c da fonseca v g performance assessment multiobjective optimizers analysis review evolutionary
computation ieee transactions




