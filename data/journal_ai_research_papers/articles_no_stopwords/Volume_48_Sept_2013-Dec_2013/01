journal artificial intelligence

submitted published

learning optimal bayesian networks
shortest path perspective
changhe yuan

changhe yuan qc cuny edu

department computer science
queens college city university york
queens ny usa

brandon malone

brandon malone cs helsinki

department computer science
helsinki institute information technology
fin university helsinki finland

abstract
learning bayesian network structure optimizes scoring function
given dataset viewed shortest path implicit state space search
graph perspective highlights importance two issues development
search strategies solving shortest path design heuristic functions guiding search introduces several techniques addressing
issues one search learns optimal bayesian network structure
searching promising part solution space others mainly
two heuristic functions first heuristic function represents simple relaxation
acyclicity constraint bayesian network although admissible consistent heuristic may introduce much relaxation loose bound second heuristic
function reduces amount relaxation avoiding directed cycles within groups
variables empirical methods constitute promising
learning optimal bayesian network structures

introduction
bayesian networks graphical represent uncertain relations
random variables domain compactly intuitively bayesian network directed
acyclic graph nodes represent random variables arcs lack
represent dependence conditional independence relations variables
relations quantified set conditional probability distributions one
variable conditioning parents overall bayesian network represents joint
probability distribution variables
applying bayesian networks real world typically requires building graphical
representations one popular use score methods
high scoring structures given dataset cooper herskovits heckerman
score learning shown np hard however chickering
due complexity early area mainly focused developing approximation greedy hill climbing approaches heckerman bouckaert
chickering friedman nachman peer unfortunately solutions
found methods unknown quality recent years several exact learning algoc

ai access foundation rights reserved

fiyuan malone

rithms developed dynamic programming koivisto sood ott
imoto miyano silander myllymaki singh moore branch
bound de campos ji integer linear programming cussens jaakkola
sontag globerson meila hemmecke lindner studeny methods
guaranteed optimal solutions able finish successfully however
efficiency scalability leave much room improvement
view learning bayesian network structure optimizes scoring function given dataset shortest path idea
represent solution space learning implicit state space search graph
shortest path start goal nodes graph corresponds
optimal bayesian network perspective highlights importance two orthogonal
issues development search strategies solving shortest path
design admissible heuristic functions guiding search present several
techniques address issues firstly search developed learn
optimal bayesian network focusing searching promising parts solution
space secondly two heuristic functions introduced guide search tightness
heuristic determines efficiency search first heuristic represents simple relaxation acyclicity constraint bayesian networks
variable chooses optimal parents independently heuristic estimate may
contain many directed cycles loose bound second heuristic named
k cycle conflict heuristic form relaxation tightens bound
avoiding directed cycles within groups variables finally traversing
search graph need calculate cost arc visited corresponds
selecting optimal parents variable candidate set present two data
structures storing querying costs candidate parent sets one set
full exponential size data structures called parent graphs stored hash tables
answer query constant time sparse representation parent
graph stores optimal parent sets improve space efficiency
empirically evaluated empowered different combinations
heuristic functions parent graph representations set uci machine learning
datasets even simple heuristic full parent graph representation often achieve better efficiency scalability existing approaches
learning optimal bayesian networks k cycle conflict heuristic sparse parent
graph representation enabled achieve even greater efficiency
scalability indicate proposed methods constitute promising
learning optimal bayesian network structures
remainder structured follows section reviews
learning optimal bayesian networks reviews related work section introduces
shortest path perspective learning formulation search graph
discussed detail section introduces two data structures developed compute
store optimal parent sets pairs variables candidate sets data structures used query cost arc search graph section presents
search developed two heuristic functions guiding
studied theoretical properties section presents empirical evaluating
several existing approaches finally section concludes


filearning optimal bayesian networks

background
first provide brief summary related work learning bayesian networks
learning bayesian network structures
bayesian network directed acyclic graph dag g represents joint probability
distribution set random variables v x x xn directed arc xi
xj represents dependence two variables say xi parent xj
use paj stand parent set xj dependence relation xj paj
quantified conditional probability distribution p xj paj joint probability
distribution represented g factorized product
q conditional probability
distributions network e p x xn ni p xi pai addition
compact representation bayesian networks provide principled approaches solving
inference tasks including belief updating probable explanation maximum
posteriori assignment pearl relevant explanation yuan liu lu lim
yuan lim littman yuan lim lu b
given dataset dn data point di vector values
variables v learning bayesian network task finding network structure
best fits work assume variable discrete finite number
possible values data point missing values
roughly three main approaches learning score learning
constraint learning hybrid methods score learning methods evaluate
quality bayesian network structures scoring function selects one
best score cooper herskovits heckerman methods basically
formulate learning combinatorial optimization work well
datasets many variables may fail optimal solutions large
datasets discuss detail next section
take constraint learning methods typically use statistical testings
identify conditional independence relations data build bayesian network
structure best fits independence relations pearl spirtes glymour
scheines cheng greiner kelly bell liu de campos huete xie
geng constraint methods mostly rely local statistical testings
often scale large datasets however sensitive accuracy
statistical testings may work well insufficient noisy data
comparison score methods work well even datasets relatively data
points hybrid methods aim integrate advantages previous two approaches
use combinations constraint score methods solving learning
dash druzdzel acid de campos tsamardinos brown aliferis
perrier imoto miyano one popular strategy use constraint
learning create skeleton graph use score learning high scoring
network structure subgraph skeleton tsamardinos et al perrier et al
work consider bayesian model averaging methods aim
estimate posterior probabilities structural features edges rather model
selection heckerman friedman koller dash cooper


fiyuan malone

score learning
score learning methods rely scoring function score evaluating quality
bayesian network structure search strategy used structure g optimizes
score therefore score methods two major elements scoring functions
search strategies
scoring functions
many scoring functions used measure quality network structure
bayesian scoring functions define posterior probability distribution
network structures conditioning data structure highest
posterior probability presumably best structure scoring functions best
represented bayesian dirichlet score bd heckerman geiger chickering
variations e g k cooper herskovits bayesian dirichlet score
score equivalence bde heckerman et al bayesian dirichlet score score
equivalence uniform priors bdeu buntine scoring functions often
form trading goodness fit structure data complexity
structure goodness fit measured likelihood structure given
data amount information compressed structure data
scoring functions belonging category include minimum description length mdl
equivalently bayesian information criterion bic rissanen suzuki lam
bacchus akaike information criterion aic akaike bozdogan
factorized normalized maximum likelihood function nml fnml silander roos kontkanen myllymaki mutual information tests score mit de campos
scoring functions decomposable score network
decomposed sum node scores heckerman
optimal structure g may unique multiple bayesian network structures may share optimal score two network structures said belong
equivalence class chickering represent set probability distributions possible parameterizations score equivalent scoring functions assign
score structures equivalence class scoring functions
score equivalent
mainly use mdl score work let ri number states xi npai
number data points consistent pai pai nxi pai number data
points constrained xi xi mdl defined follows lam bacchus

dl g

x

dl xi pai



often use optimal instead optimal throughout





filearning optimal bayesian networks


log n
k xi pai

x
nxi pai
h xi pai
nxi pai log

npai
xi pai

k xi pai ri
rl
xl pai

dl xi pai h xi pai





goal bayesian network minimum mdl score however
methods means restricted mdl decomposable scoring function
bic bdeu fnml used instead without affecting search strategy
demonstrate test bdeu experimental section one slight difference
mdl scoring functions latter scores need maximized
order optimal solution rather straightforward translate
maximization minimization simply changing sign scores
sometimes use costs refer scores represent distances
nodes search graph
local search strategies
given n variables n n n directed acyclic graphs dags size
solution space grows exponentially number variables surprising
score structure learning shown np hard chickering due
complexity early focused mainly developing approximation heckerman bouckaert popular search strategies used include greedy hill
climbing stochastic search genetic etc
greedy hill climbing methods typically begin initial network e g empty
network randomly generated structure repeatedly apply single edge operations
including addition deletion reversal finding locally optimal network extensions include tabu search random restarts glover limiting
number parents parameters variable friedman et al searching
space equivalence classes chickering searching space variable
orderings teyssier koller searching constraints extracted
data tsamardinos et al optimal reinsertion moore wong
adds different operator variable removed network optimal parents
selected variable reinserted network parents
parents selected ensure network still valid bayesian network
stochastic search methods markov chain monte carlo simulated annealing
applied high scoring structure heckerman de campos
puerta myers laskey levitt methods explore solution space
non deterministic transitions neighboring network structures favoring
better solutions stochastic moves used hope escape local optima
better solutions
optimization methods genetic hsu guo perry stilson
larranaga kuijpers murga yurramendi ant colony optimization meth

fiyuan malone

ods de campos fernndez luna gmez puerta daly shen
applied learning bayesian network structures well unlike previous methods
work one solution time population methods maintain set candidate solutions throughout search step create next generation
solutions randomly reassembling current solutions genetic
generating solutions information collected incumbent solutions
ant colony optimization hope obtain increasingly better populations solutions
eventually good network structure
local search methods quite robust face large learning
many variables however guarantee optimal solution worse
quality solutions typically unknown
optimal search strategies
recently multiple exact developed learning optimal bayesian networks several dynamic programming proposed observation
bayesian network least one leaf ott et al singh moore
leaf variable child variables bayesian network order optimal
bayesian network set variables v sufficient best leaf leaf
choice x best possible bayesian network constructed letting x choose optimal
parent set pax v x letting v x form optimal subnetwork
best leaf choice one minimizes sum score x pax score v x
scoring function score formally
score v min score v x bestscore x v x
xv




bestscore x v x

min
score x pax
pax v x



given recurrence relation dynamic programming works follows first finds optimal structures single variables trivial starting
base cases builds optimal subnetworks increasingly larger variable
sets optimal network found v dynamic programming
optimal bayesian network n n time space koivisto sood ott
et al silander myllymaki singh moore recent
improved memory complexity trading longer running times reduced memory consumption parviainen koivisto taking advantage layered structure
present within dynamic programming lattice malone yuan hansen b malone
yuan hansen bridges
branch bound bb proposed de campos ji
learning bayesian networks first creates cyclic graph allowing
variable obtain optimal parents variables best first search strategy
used break cycles removing one edge time uses
approximation estimate initial upper bound solution pruning
occasionally expands worst nodes search frontier hope


filearning optimal bayesian networks

figure order graph four variables
better networks update upper bound completion finds optimal
network structure subgraph initial cyclic graph ran
memory finding solution switch depth first search strategy
suboptimal solution
integer linear programming ilp used learn optimal bayesian network
structures cussens jaakkola et al learning cast integer
linear program polytope exponential number facets outer bound
approximation polytope solved solution relaxed
integral guaranteed optimal structure otherwise cutting planes branch
bound subsequently applied optimal structure recently
similar method proposed optimal structure searching space
equivalence classes hemmecke et al
several methods considered optimal constraints enforce
network structure example optimal parents selected variable k
finds optimal network structure particular variable ordering cooper herskovits
methods developed ordyniak szeider kojima perrier imoto
miyano optimal network structure must subgraph given super
graph

shortest path perspective
section introduces shortest path perspective learning bayesian
network structure given dataset
order graph
state space graph learning bayesian networks basically hasse diagram containing
subsets variables domain figure visualizes state space graph
learning four variables top node empty set layer


fiyuan malone

start search node bottom node complete set layer n
goal node n number variables domain arc u u x
represents generating successor node adding variable x existing set
variables u u called predecessor u x cost arc equal score
selecting optimal parent set x u e bestscore x u example arc
x x x x x cost equal bestscore x x x node layer
ni successors many ways add variable predecessors
many leaf choices define expanding node u generating successors
nodes u
search graph thus defined path start node goal node defined
sequence nodes arc nodes next node
sequence path corresponds ordering variables order
appearance example path traversing nodes x x x x x x
x x x x stands variable ordering x x x x call
search graph order graph cost path defined sum costs
arcs path shortest path path minimum total cost
order graph
given shortest path reconstruct bayesian network structure noting
arc path encodes choice optimal parents one variables
preceding variables complete path represents ordering
variables therefore putting together optimal parent choices generates valid
bayesian network construction bayesian network structure optimal
finding shortest path
methods applied solve shortest path dynamic programming
considered evaluate order graph top sweep order graph silander
myllymaki malone et al b layer layer dynamic programming finds
optimal subnetwork variables contained node order graph
previous layers example three ways construct bayesian
network node x x x x x subnetwork x leaf
x x subnetwork x leaf x x subnetwork x
leaf top sweep makes sure optimal subnetworks already found
x x x x x x need select optimal parents
leaves identify leaf produces optimal network x x x
evaluation reaches node last layer shortest path equivalently optimal
bayesian network found global variable set
drawback dynamic programming need compute
bestscore candidate parent sets variable n variables
n nodes order graph n parent scores computed
variable totally n n scores number variables increases computing storing
order parent graphs quickly becomes infeasible
propose apply hart nilsson raphael
solve shortest path uses heuristic function evaluate quality
search nodes expand promising search node search step


filearning optimal bayesian networks

guidance heuristic functions needs explore part search
graph finding optimal solution however comparison dynamic programming
overhead calculating heuristic values maintaining priority queue
actual relative performance dynamic programming thus depends
efficiency calculating heuristic values tightness values felzenszwalb
mcallester klein manning

finding optimal parent sets
introducing solving shortest path first discuss
obtain cost bestscore x u arc u u x visit
order graph recall arc involves selecting optimal parents variable
candidate set need consider subsets candidate set finding subset
best score section introduce two data structures related methods
computing storing optimal parent sets scores pairs variable candidate
parent set
exact learning bayesian network structures need calculate
optimal parent sets scores present reasonable calculation
note however applicable vice versa
parent graph
use data structure called parent graph compute costs arcs order graph
variable parent graph parent graph variable x hasse diagram
consisting subsets variables v x node u stores optimal parent
set pax u minimizes score x pa x well bestscore x u
example figure b shows sample parent graph x contains best scores
subsets x x x obtain figure b however first need calculate
preliminary graph figure contains raw score subset u parent
set x e score x u equation shows scores calculated
counts particular instantiations parent child variables
use ad tree moore lee collect counts dataset
compute scores ad tree unbalanced tree structure contains two types
nodes ad tree nodes varying nodes ad tree node stores number data points
consistent particular variable instantiation varying node used instantiate
state variable full ad tree stores counts data points consistent
partial instantiations variables sample ad tree two variables shown
figure n variables states number ad tree nodes ad tree
n grows even faster size order parent graph moore lee
described sparse ad tree significantly reduces space complexity readers
referred details pseudo code assumes sparse ad tree
used
given ad tree ready calculate raw scores score x figure
exponential number scores parent graph however parent
sets possibly optimal bayesian network certain parent sets discarded
without ever calculating values according following theorems tian


fiyuan malone

figure sample parent graph variable x raw scores score x
parent sets first line node gives parent set second
line gives score set parents x b optimal
scores bestscore x candidate parent set second line
node gives optimal score subset variables first line
parents x c optimal parent sets scores pruned parent
sets shown gray parent set pruned predecessors
better score

x
x
c
vary
v
x

vary
v
x

x
x

x
x

x
x

x
x

c

c

c

c

vary
x

vary
x

x
x

x
x

x
x

x
x

c

c

c

c

figure ad tree
use theorems compute necessary mdl scores scoring functions
bdeu similar pruning rules de campos ji provides
pseudo code calculating raw scores
theorem optimal bayesian network mdl scoring function
n
variable log log
n parents n number data points


filearning optimal bayesian networks

score calculation
input ad sparse ad tree input data v input variables
output score x u pair x v u v x
function calculatemdlscores ad v

xi v

calculatescores xi ad

end
end function























function calculatescores xi ad
n
k log log
prune due theorem
n
u u v x u k
parent sets size k
prune f alse
u
k xi u score xi u
prune true
prune due theorem
break
end
end
prune true
score xi u log n k xi u
complexity term
instantiation xi u xi u
log likelihood term
cf amily getcount xi u ad
cp arents getcount u ad
score xi u score xi u cf amily log cf amily
score xi u score xi u cf amily log cp arents
end
end
end
end
end function

theorem let u two candidate parent sets x u k xi
dl xi u supersets cannot possibly optimal parent sets
x
computing raw scores compute parent graph according following
theorem appeared many earlier papers e g see work teyssier
koller de campos ji theorem simply means parent set
optimal subset better score
theorem let u two candidate parent sets x u
score x u score x optimal parent set x candidate set


fiyuan malone

computing parent graphs
input necessary score x u x v u v x
output full parent graphs containing bestscore x u
function calculatefullparentgraphs v score

x v

layer n
propagate best scores graph

u u v x u layer

calculatebestscore x u score

end

end

end
end function









function calculatebestscore x u score
bestscore x u score x u
u
bestscore x u bestscore x u
bestscore x u bestscore x u
end
end
end function

function getbestscore x u

return bestscore x u
end function

propagate best scores

query bestscore x u



therefore generate successor node u u parent graph x
check whether score x u smaller bestscore x u let parent
graph node u record optimal parent set otherwise bestscore x u
smaller propagate optimal parent set u u propagation
must following teyssier koller
theorem let u two candidate parent sets x u must
bestscore x bestscore x u
pseudo code propagating scores computing parent graph outlined
figure b shows parent graph optimal scores propagating
best scores top bottom
search order graph whenever visit arc u u x
score looking parent graph variable x example need
optimal parents x x x look node x x x parent graph
optimal parent set score make look ups efficient use hash
tables organize parent graphs query answered constant time


filearning optimal bayesian networks

parentsx
scoresx

x x


x


x





table sorted scores parent sets x pruning parent sets
possibly optimal
parentsx

parentsx
x
x
parentsx

parentsx
x

x x




x




x









table parentsx xi bit vectors x line xi indicates corresponding parent set includes variable xi indicates otherwise note
pruning none optimal parent sets include x

sparse parent graphs
full parent graph variable x exhaustively enumerates subsets v x
stores bestscore x u subsets naively requires storing
n n scores parent sets silander myllymaki theorem however
number optimal parent sets often far smaller full size figure b shows
optimal parent set may shared several candidate parent sets full parent
graph representation allocate space repetitive information candidate sets
resulting waste time space
address limitations introduce sparse representation parent graphs
related scanning techniques querying optimal parent sets full parent
graphs begin calculating pruning scores described last section due
theorems parent sets pruned without evaluated
therefore create full parent graphs instead creating
hasse diagrams sort optimal parent scores variable x list
maintain parallel list stores associated optimal parent sets call sorted
lists scoresx parentsx table shows sorted lists optimal scores
parent graph figure b essence allows us store efficiently process
scores figure c
optimal parent set x candidate set u simply scan
list x starting beginning soon first parent set subset
u optimal parent score bestscore x u trivially true due
following theorem
theorem first subset u parentsx optimal parent set x u
scanning lists optimal parent sets inefficient done properly
since scanning arc visited order graph inefficiency
scanning large impact search


fiyuan malone

parentsx
validx

parentsx
x

validx

x x




x




x









table performing bitwise operation exclude parent sets
include x validx bit vector means parent set
include x used selecting optimal parents first set bit
indicates best possible score parent set

parentsx
validx

parentsx
x

validx

x x




x




x









table performing bitwise operation exclude parent sets
include x x validnew
x bit vector means parent
set includes neither x x initial validx bit vector already excluded
x finding validnew
x required excluding x

ensure efficiency propose following scanning technique variable
x first initialize working bit vector length kscoresx k called validx
indicates parent scores scoresx usable create n bit vectors
length kscoresx k one variable v x bit vector variable
denoted parentsyx contains parent sets contain others
table shows bit vectors example table exclude variable
candidate parent perform bit operation validnew
validx parentsyx
x
validx bit vector contains parent sets subsets v
first set bit corresponds bestscore x v table shows example excluding
x set possible parents x first set bit bit vector
corresponds bestscore x v x want exclude x candidate
parent bit vector last step becomes current bit vector step

bit operation applied validnew
validx parentsx
x
x first set
bit corresponds bestscore x v x x table demonstrates
operation important note exclude one variable time example
excluding x wanted exclude x rather x could take validnew

x

validx parentsx


operations

described


createsparseparentgraph

x
getbestscore functions
pruning duplicate scores sparse representation requires much less
memory storing possible parent sets scores long kscores x k
c n n requires less memory memory efficient dynamic programming
malone et al b
experimentally kscoresx k almost


filearning optimal bayesian networks

sparse parent graph
input necessary score x u x v u v x
output sparse parent graphs containing optimal parent sets scores
function createsparseparentgraph x score

x v

scorest parentst sort score x
sort scores preferring low cardinality

scoresx parentsx
initialize possibly optimal scores

scorest

prune f alse

j scoresx
check better subset pattern exists

contains parentst parentsx j scoresx scorest

prune true

break

end

end

prune true

append scoresx parentsx parentst parentst

end

end

scoresx
set bit vectors efficient querying

parentsx

set parentsyx

end

end

end
end function









function getbestscore x u
valid allscoresx
v u
valid valid parentsyx
end
f sb f irstsetbit valid
return scoresx f sb
end function

query bestscore x u

return first score set bit

smaller c n n several orders magnitude offers
usually substantial memory savings compared previous best approaches
sparse representation extra benefit improving time efficiency well
full representation create complete exponential size parent graphs
even though many nodes parent graph share optimal parent choices
sparse representation avoid creating nodes makes creating sparse
parent graphs much efficient


fiyuan malone

search
ready tackle shortest path order graph section
presents search well two admissible heuristic functions guiding


apply well known state space search method hart et al
solve shortest path order graph main idea
use evaluation function f measure quality search nodes expand
one lowest f cost exploration order graph node u
f u decomposed sum exact past cost g u estimated future cost
h u g u cost measures shortest distance start node u
h u cost estimates far away u goal node therefore f cost provides
estimated total cost best possible path passes u
uses open list usually priority queue store search frontier
closed list store expanded nodes initially open list contains start node
closed list empty search step node lowest f cost
open list say u selected expansion generate successor nodes expanding
u however need first check whether goal node yes shortest path
goal found construct bayesian network path terminate
search
u goal expand generate successor nodes successor
considers one possible way adding variable say x leaf existing
subnetwork variables u u x g cost calculated
sum g cost u cost arc u arc cost well
optimal parent set pax x u retrieved xs parent graph h cost
computed heuristic function describe shortly record
following information g cost h cost x pax
clear order graph multiple paths node
perform duplicate detection see whether node representing set variables
already generated check duplicates search space blows
order graph size n order tree size n first check whether
duplicate already exists closed list check whether duplicate
better g cost yes discard immediately represents worse path
otherwise remove duplicate closed list place open list
happens found better path lower g cost reopen node future
search
duplicate found closed list need check open list
duplicate found simply add open list otherwise compare
g costs duplicate duplicate lower g cost discarded
otherwise replace duplicate lower g cost means better
path found
delay calculation h duplicate detection avoid unnecessary calculations
nodes pruned



filearning optimal bayesian networks

search
input full sparse parent graphs containing bestscore x u
output optimal bayesian network g
function main

start

score start p

push open start v bestscore v

isempty open

u pop open

u goal
shortest path found

print best score score v

g construct network shortest path

return g

end

put closed u

x v u
generate successors

g bestscore x u score u

contains closed u x
closed list dd

g score u x
reopen node

delete closed u x

push open u x g h

score u x g

end

else

contains open u x g score u x open list dd

update open u x g h

score u x g

end

end

end

end
end function

successor nodes generated place node u closed
list indicates node already expanded expanding top node open
list called one search step performs step repeatedly goal
node selected expansion moment shortest path start state
goal state found
shortest path found reconstruct optimal bayesian network
structure starting goal node tracing back shortest path reaching
start node since node path stores leaf variable optimal parent set
putting optimal parent sets together generates valid bayesian network structure
pseudo code shown


fiyuan malone

simple heuristic function
provides different theoretical guarantees depending properties
heuristic function h function h admissible h cost never greater
true cost goal words optimistic given admissible heuristic
function guaranteed shortest path goal node
selected expansion pearl let u node order graph first consider
following simple heuristic function h
definition
h u

x

bestscore x v x



xv u

heuristic function allows remaining variable choose optimal parents
variables design reflects principle exact cost relaxed
used admissible bound original pearl case
original learn bayesian network directed acyclic graph equation
relaxes ignoring acyclicity constraint directed cyclic graphs
allowed heuristic function easily proven admissible following theorem
proofs theorems found appendix
theorem h admissible
turns h even nicer property heuristic function consistent
node u successor h u h c u c u stands cost
arc u given consistent heuristic f cost monotonically non decreasing
following path order graph f cost node less
equal f cost goal node follows immediately consistent heuristic
guaranteed admissible consistent heuristic guaranteed
shortest path node u u selected expansion duplicate
found closed list duplicate must optimal g cost node
discarded immediately following simple heuristic equation
consistent
theorem h consistent
heuristic may seem expensive compute requires computing bestscore x v
x variable x however scores easily found querying parent
graphs stored array repeated use takes linear time calculate
heuristic start node subsequent computation h however takes constant
time simply subtract best score newly added variable
heuristic value parent node
improved admissible heuristic
simple heuristic function defined equation referred hsimple hereafter relaxes
acyclicity constraint bayesian networks completely hsimple may introduce
many directed cycles loose bound introduce another heuristic
section tighten heuristic first use toy example motivate heuristic
describe two specific approaches computing heuristic


filearning optimal bayesian networks

x

x

x

x

figure directed graph representing heuristic estimate start search node

motivating example
hsimple heuristic estimate start node order graph allows variable
choose optimal parents variables suppose optimal parent sets
x x x x x x x x x x x x respectively parent
choices shown directed graph figure since acyclicity constraint
ignored directed cycles introduced e g x x however know
final solution cannot cycles three cases possible x x x
parent x x cannot parent x x parent x neither
true theorem third case cannot provide better value
first two cases one variables must fewer candidate parents
unclear one better take minimum
get lower bound consider case delete arc x x rule
x parent x let x rechoose optimal parents remaining
variables x x must check parent sets including x deletion
arc alone cannot produce bound best parent set x
x x necessarily x total bound x x computed summing
together original bound x bound x call total bound
b case handled similarly call total bound b joint cost
x x c x x must optimistic compute minimum b b
effectively considered possible ways break cycle obtained tighter
heuristic value heuristic clearly admissible still allow cycles among
variables
often hsimple introduces multiple cycles heuristic estimate figure
cycle x x cycle shares x earlier cycle x x
say cycles overlap one way break cycles set parent set x
x however introduces cycle x x described detail
shortly partition variables exclusive groups break cycles within
group example x x different groups break cycle


fiyuan malone

k cycle conflict heuristic
idea generalized compute joint cost variable group
size k avoiding cycles within group node u order graph
calculate heuristic value partitioning variables v u several exclusive
groups sum costs together name resulting technique k cycle conflict
heuristic note simple heuristic hsimple special case heuristic
simply contains costs individual variables k
heuristic application additive pattern database technique felner
korf hanan pattern databases culberson schaeffer
computing admissible heuristic solving relaxed consider
puzzle square tiles numbered randomly placed
box one position left empty configuration tiles called state
goal slide tiles one time destination configuration tile slide
empty position beside position puzzle relaxed
contain tiles tiles removed relaxation multiple
states original map one state abstract state space relaxed
share positions remaining tiles abstract state called
pattern cost pattern equal smallest cost sliding remaining
tiles destination positions cost provides lower bound state
original state space maps pattern costs patterns stored
pattern database
relax different ways obtain multiple pattern databases
solutions several relaxed independent said
exclusive puzzle relax contain tiles relaxation
solved independently previous one share puzzle
movements concrete state original state space positions tiles
map pattern first pattern database positions tiles map
different pattern second pattern database costs patterns added
together obtain admissible heuristic hence name additive pattern databases
learning pattern defined group variables cost
optimal joint cost variables avoiding directed cycles
decomposability scoring function implies costs two exclusive patterns
added together obtain admissible heuristic
explicitly break cycles computing cost pattern
following theorem offers straightforward
theorem cost pattern u c u equal shortest distance v u
goal node order graph
consider example figure cost pattern x x equal
shortest distance x x goal order graph figure
furthermore difference c u sum simple heuristic values
variables u indicates amount improvement brought avoiding cycles within
pattern differential score called h thus used quality measure
ordering patterns choosing patterns likely tighter
heuristic


filearning optimal bayesian networks

dynamic k cycle conflict heuristic
two slightly different versions k cycle conflict heuristic first version
named dynamic k cycle conflict heuristic compute costs groups variables
size k store single pattern database according theorem
heuristic computed finding shortest distances nodes
last k layers order graph goal
compute heuristic breadth first search backward search
order graph k layers search starts goal node expands order
graph backward layer layer reverse arc u x u cost arc
u u x e bestscore x u reverse g cost u updated whenever
path lower cost found breadth first search ensures node u obtain
exact reverse g cost previous layer expanded g cost cost pattern
v u compute differential score h pattern time
pattern better differential score subset patterns
discarded pruning significantly reduce size pattern database improve
query efficiency computing dynamic k cycle conflict heuristic
shown
heuristic created calculate heuristic value search node
follows node u partition remaining variables v u set exclusive
patterns sum costs together heuristic value since prune superset
patterns partition however potentially many ways
partition ideally want one highest total cost represents
tightest heuristic value finding optimal partition formulated
maximum weighted matching felner et al k define
undirected graph vertex represents variable edge two
variables represents pattern containing variables weight equal
cost pattern goal select set edges graph two
edges share vertex total weight edges maximized matching
solved n time n number vertices papadimitriou steiglitz

k add hyperedges matching graph connecting
k vertices represent larger patterns goal becomes select set edges
hyperedges maximize total weight however three dimensional higher order
maximum weighted matching np hard garey johnson means
solve np hard calculating heuristic value
alleviate potential inefficiency greedily select patterns quality
consider node u unsearched variables v u choose pattern highest
differential cost patterns subsets v u repeat step
remaining variables variables covered total cost chosen patterns
used heuristic value u hdynamic function gives pseudocode
computing heuristic value
dynamic k cycle conflict heuristic introduced example dynamically partitioned pattern database felner et al patterns dynamically
selected search refer dynamic pattern database short


fiyuan malone

dynamic k cycle conflict heuristic
input full sparse parent graphs containing bestscore x u
output pattern database p patterns size k
function createdynamicpd k

p v

h v

l k
perform bfs k levels

u p dl

expand u l

checksave u

p v u p dl u

end

end

x p save
remove superset patterns improvement

delete p x

end

sort p h
sort patterns decreasing costs
end function
























function expand u l
x u
g p dl u bestscore x u x
g p dl u x p dl u x g
end
end function

duplicate detection

function checksave u
p
h u g v u bestscore v
x v u
check improvement subset patterns
h u h u x save u
end
end function
function hdynamic u
h
ru
p
r
rr
h h p
end
end
return h
end function

calculate heuristic value u

greedily best subset pattern r



filearning optimal bayesian networks

potential drawback dynamic pattern databases even greedy
method computing heuristic value still much expensive simple heuristic
equation consequently search time longer even though tighter pattern
database heuristic pruning fewer expanded nodes
static k cycle conflict heuristic
address inefficiency dynamic pattern database computing heuristic values
introduce another version named static k cycle conflict heuristic statically
partitioned pattern database technique felner et al idea partition
variables several static exclusive groups create separate pattern database
group consider variables x x divide variables
two groups x x x x group say x x create
pattern database contains costs subsets x x store
hash table refer heuristic static pattern database short
create static pattern databases follows static grouping v vi need
compute pattern database group vi resembles order graph containing
subsets vi use breadth first search create graph starting
node
vi cost arc u x u graph equal bestscore x j vj u
means variables groups valid candidate parents ensure
efficient retrieval static pattern databases stored hashtables nothing pruned
gives pseudocode creating static pattern databases
much simpler use static pattern databases compute heuristic value consider
search node x x x unsearched variables x x x x x simply
divide variables two patterns x x x x x according static
grouping look respective pattern databases sum costs together
heuristic value moreover since search step processes one variable
one pattern affected requires score lookup therefore heuristic value
calculated incrementally hstatic function provides pseudocode
naively calculating heuristic value
properties k cycle conflict heuristic
versions k cycle conflict heuristic remain admissible although avoid
cycles within pattern cannot prevent cycles across different patterns following theorem proves
theorem k cycle conflict heuristic admissible
understanding consistency heuristic slightly complex first
look static pattern database involve selecting patterns dynamically
following theorem shows static pattern database still consistent
theorem static pattern database version k cycle conflict heuristic remains
consistent
dynamic pattern database search step needs solve maximum weighted
matching select set patterns compute heuristic value


fiyuan malone

static k cycle conflict heuristics

input full sparse parent graphs containing bestscore x u vi partition v
output full pattern database p vi
function createstaticpd vi

p

l fivi
perform bfs vi


u p dl

expand u l vi
u

p u p dl

end

end
end function














function expand u l vi
x vi u

u bestscore x u
g p dl
j vj


g p dl u x p dl u x g
end
end function
function hstatic u
h
vi v
h h p u vi
end
return h
end function

duplicate detection

sum p separately

following dynamic k cycle conflict heuristic consistent closely
following theorem work edelkamp schrodl
theorem dynamic pattern database version k cycle conflict heuristic remains consistent
however theorem assumes use shortest distances nodes
abstract space use greedy method solve maximum weighted
matching longer guarantee shortest paths
may lose consistency property dynamic pattern database thus necessary
reopen duplicate node closed list better path found

experiments
evaluated search set benchmark datasets uci repository bache lichman datasets variables data
points discretized variables two states mean values deleted


filearning optimal bayesian networks

e
e

full

largest layer

sparse

e
e

size

e
e
e
e
e
e
e

figure number parent sets scores stored full parent graphs
full largest layer parent graphs memory efficient dynamic programming largest layer sparse representation sparse

data points missing values search implemented java
compared branch bound bb de campos ji dynamic programming dp silander myllymaki integer linear programming
gobnilp cussens used latest versions software
source code time experiments well default parameter settings
version gobnilp scip bb dp calculate mdl
use bic score uses equivalent calculation mdl confirmed
found bayesian networks belong
equivalence class experiments performed ghz intel xeon gb
ram running suse linux enterprise server version
full vs sparse parent graphs
first evaluated memory savings made possible sparse parent graphs comparison full parent graphs particular compared maximum number
scores stored variables typical dynamic programming stores scores possible parent sets variables
memory efficient dynamic programming malone et al b stores possible parent
sets one layer parent graphs variables size largest layer
software package source code named urlearning learning implementing
downloaded http url cs qc cuny edu software urlearning html
http www ecse rpi edu cvrl structlearning html
http b course hiit bene
http www cs york ac uk aig sw gobnilp



fiyuan malone

parent graphs indication space requirement sparse representation
stores optimal parent sets variables
figure shows memory savings sparse representation benchmark
datasets clear number optimal parent scores stored sparse representation typically several orders magnitude smaller full representation
furthermore due theorem increasing number data points increases maximum number candidate parents therefore number candidate parent sets increases
number data points increases however many parent sets pruned
sparse representation theorem number variables affects
number candidate parent sets consequently number optimal parent scores
increases function number data points number variables
amount pruning data dependent though easily predictable
practice number data points affect number unique scores much
number variables
pattern database heuristics
pattern database heuristic two versions static dynamic pattern databases
parameterized different ways tested parameterizations
heuristics two datasets named autos flag chose
two datasets large enough number variables better
demonstrate effect pattern database heuristics dynamic pattern database
varied k static pattern databases tried groupings
autos dataset groupings flag dataset obtained
groupings simply dividing variables datasets several consecutive blocks
sparse parent graphs shown figure
full parent graphs ran memory datasets
full parent graphs used sparse representations achieved much better
scalability able solve autos heuristic flag
best heuristics sparse parent graphs hereafter experiments
assume use sparse parent graphs
pattern database heuristics improved efficiency scalability significantly simple heuristic static pattern database grouping
ran memory flag dataset pattern database heuristics enabled finish successfully dynamic pattern database k helped reduce
number expanded nodes significantly datasets setting k helped even
however increasing k resulted increased search time sometimes
even increased number expanded nodes shown believe larger k
better pattern database occasional increase expanded nodes
greedy strategy used choose patterns fully utilize better heuristic
longer search time understandable though less efficient compute
heuristic value larger pattern databases inefficiency gradually overtook
benefit therefore k seems best parametrization dynamic pattern
database general static pattern databases able test much larger


filearning optimal bayesian networks

e

running time

size pattern database

e

e
e
e
e













autos

e

running time

size pattern database

e

e
e
e
e













x

x

f lag
figure comparison enhanced different heuristics hsimple hdynamic k
hstatic groupings autos dataset
groupings flag dataset size pattern database
means number patterns stored running time means search time
seconds indicated pattern database strategy x means
memory

groups need enumerate groups certain size
suggest fewer larger groups tend tighter heuristic
sizes static pattern databases typically much larger dynamic
pattern databases however time needed create pattern databases still negligible comparison search time cases thus cost effective try compute
larger affordable size static pattern databases achieve better search efficiency
best static pattern databases typically helped achieve better
efficiency dynamic pattern databases even number expanded nodes
larger reason calculating heuristic values much efficient
static pattern databases


fiyuan malone


bb scoring

dp scoring

scoring

scoring time











figure comparison scoring time bb dp label
x axis consists dataset name number variables number
data points

simple heuristic
first tested hsimple heuristic competing roughly two
phases computing optimal parent sets scores scoring phase searching bayesian
network structure searching phase therefore compare two
parts running time scoring time search time figure shows scoring times
bb dp gobnilp included assumes optimal scores
provided input label horizontal axis shows dataset number
variables number data points ad tree method used
seems efficient computing parent scores
scoring part dp often order magnitude slower others
somewhat misleading however scoring searching parts dp
tightly integrated work dp done
scoring part little work left search shortly search time
dp typically short
figure reports search time benchmark
datasets difficult take long even fail optimal
solutions therefore terminate early runs seconds
dataset bb succeeded two datasets voting
hepatitis within time limit datasets several orders
magnitude faster bb major difference bb formulation
search space bb searches space directed cyclic graphs
maintains directed acyclic graph search indicate better
search space directed acyclic graphs
search time needed dp often shorter
explained earlier reason heavy lifting dp done


filearning optimal bayesian networks


bb

dp

gobnilp



search time









x x

x

x

x

x

x

x

x x

x




total running time

dp total time

total time









b
figure comparison search time seconds bb dp gobnilp
b total running time dp x means corresponding
finish within time limit seconds ran memory
case

scoring part add scoring search time together shown figure b
several times faster dp datasets except adult voting
gobnilp left search part main difference
dp explores part order graph dynamic programming fully
evaluates graph however step search overhead
cost computing heuristic function maintaining priority queue one step


fiyuan malone

expensive similar dynamic programming step pruning
outweigh overhead slower dynamic programming adult
voting large number data points makes pruning technique
theorem less effective although dp perform pruning due
simplicity highly streamlined optimized performing
calculations dp faster search two
datasets however efficient dp datasets
datasets number data points large comparison number
variables pruning significantly outweighs overhead example
runs faster mushroom dataset comparing total running time even though
mushroom data points
comparison gobnilp shows advantages able optimal bayesian networks datasets well within
time limit gobnilp failed learn optimal bayesian networks three datasets
including letter image mushroom reason gobnilp formulates
learning integer linear program whose variables correspond optimal
parent sets variables even though datasets many variables
many optimal parent sets integer programs many variables
solvable within time limit hand gobnilp
quite efficient many datasets even though dataset may many
variables gobnilp solve efficiently long number optimal parent sets
small much efficient datasets hepatitis heart although
opposite true datasets adult statlog
pattern database heuristics
since static pattern databases seem work better dynamic pattern databases
cases tested static pattern database sp dp gobnilp
datasets used figure well several larger datasets used simple
static grouping n n datasets n number variables
bb excluded solve additional dataset
shown figure
benefits brought pattern databases rather obvious
datasets able finish sp typically order magnitude
faster addition sp able solve three larger datasets sensor autos flag
failed running time datasets pretty short
indicates memory consumption parent graphs reduced
able use memory order graph solve search rather
easily
dp able solve one dataset autos able solve
somewhat surprising given pruning capability explanation
stores search information ram fail ram exhausted dp
described silander myllymaki stores intermediate
computer files hard disks able scale larger datasets


filearning optimal bayesian networks



search time

dp

gobnilp



sp







x

x

x

xxx

x

x xx x x

figure comparison search time seconds dp gobnilp sp
x means corresponding finish within time
limit seconds ran memory case

gobnilp able solve autos horse flag failed sensors sensors
dataset data points number optimal parent sets large almost
shown figure gobnilp begins difficulty solving datasets
optimal parent scores particular computing environment
gobnilp quite efficient datasets able solve autos flag
solve horse dataset figure clear
reason number optimal parent sets small dataset
pruning
gain insight performance looked amount pruning
different layers order graph plot figure detailed numbers
expanded nodes versus numbers unexpanded nodes layer order
graph two datasets mushroom parkinsons use datasets
largest datasets solved sp manifest different
pruning behaviors top two figures simple heuristic
bottom two sp
mushroom plain needed expand small portion search nodes
layer indicates heuristic function quite tight dataset effective
pruning started early th layer parkinsons however plain
successful pruning nodes first layers heuristic function appeared
loose expand nodes layers heuristic function became
tighter latter layers enabled prune increasing percentage search
nodes help pattern database heuristic however sp helped prune many


e
expanded

e

unexpanded

expandedvsunexpandednodes

expandedvsunexpandednodes

yuan malone

e
e
e
e
e
e
e













layer







e
expanded

e
e
e
e
e
e
e
e




mushroom










layer















b parkinsons
e

e
expanded

unexpanded

expandedvsunexpandednodes

expandedvsunexpandednodes

unexpanded

e
e
e
e
e
e
e
e











layer











c sp mushroom

expanded

unexpanded

e
e
e
e
e
e
e
e











layer



sp parkinsons

figure number expanded unexpanded nodes layer order
graph mushroom parkinsons different heuristics

search nodes parkinsons pruning became effective early th layer
sp helped prune nodes mushroom although benefit clear
already quite effective dataset
factors affecting learning difficulty
several factors may affect difficulty dataset bayesian network learning
including number variables number data points number
optimal parent sets analyzed correlation factors search
times replaced occurrence time order
make analysis possible caution though may underestimation
figure shows excluded bb finished two
datasets dp sp important factor determining efficiency
number variables correlations search time numbers
variables greater however seems negative correlation
search time number data points intuitively increasing number
data points make dataset difficult explanation preexisting negative correlation number data points number variables
datasets tested analysis shows correlation


filearning optimal bayesian networks



variables

data records

optimal parent sets



correlation







dp

gobnilp



sp




figure correlation search time several factors
may affect difficulty learning including number
variables number data points dataset number optimal
parent sets

since search time strong positive correlation number variables
seemingly negative correlation search time number data points
becomes less surprising
comparison efficiency gobnilp affected number optimal
parent sets correlation high close positive correlation
number data points efficiency explained earlier
data points often leads optimal parent sets finally correlation
number variables almost zero means difficulty dataset gobnilp
determined number variables
insights quite important provide guideline choosing suitable
given characteristic dataset many optimal parent sets
many variables better way around true gobnilp
better
effect scoring functions
analyses far mainly mdl score decomposable scoring
functions used correctness search strategies
heuristic functions affected scoring function however different scoring
functions may different properties example theorem property mdl
score cannot use pruning technique scoring functions consequently
number optimal parent sets tightness heuristic practical performance
may affected
verify hypothesis tested bdeu scoring function heckerman
equivalent sample size set since scoring phase common
exact focus experiment comparing number optimal parent
sets resulted scoring functions search time sp gobnilp


fiyuan malone

optimal ps mdl

optimal ps bdeu

size












gobnilp mdl

gobnilp bdeu

mdl

bdeu

search time









xx

xx

x

xx

xx

x

b
figure comparison number optimal parent sets b search time
sp gobnilp datasets two scoring functions mdl
bdeu

datasets horse flag included optimal parent sets
unavailable figure shows
main observation number optimal parent sets differ mdl
bdeu bdeu score tends allow larger parent sets mdl
larger number optimal parent sets datasets difference around
order magnitude datasets imports autos
comparison search time shows sp affected much gobnilp increase number optimal parent sets efficiency finding
optimal parent set affected sp slowed slightly
datasets significant change mushroom dataset took sp
seconds solve dataset mdl seconds bdeu comparison
gobnilp affected much able solve datasets imports autos effi

filearning optimal bayesian networks

ciently mdl failed solve within hours bdeu remained
unable solve letter image mushroom sensors within time limit

discussions conclusions
presents shortest path perspective learning optimal bayesian
networks optimize given scoring function uses implicit order graph represent
solution space learning shortest path start
goal nodes graph corresponds optimal bayesian network perspective
highlights importance two orthogonal directions one direction
develop search solving shortest path main contribution
made line solving shortest path learning
optimal bayesian network guided heuristic functions focuses
searching promising parts solution space finding optimal bayesian
network
second equally important direction development search heuristics
introduced two admissible heuristics shortest path first heuristic
estimates future cost completely relaxing acyclicity constraint bayesian networks shown admissible consistent second heuristic
k cycle conflict heuristic developed additive pattern database technique
unlike simple heuristic variable allowed choose optimal parents independently heuristic tightens estimation enforcing acyclicity constraint
within small groups variables two specific approaches computing
heuristic one named dynamic k cycle conflict heuristic computes costs
groups variables size k search dynamically partition
remaining variables exclusive patterns calculating heuristic value
named static k cycle conflict heuristic partitions variables several static
exclusive groups computes separate pattern database group sum
costs static pattern databases obtain admissible heuristic heuristics
remain admissible consistent although consistency dynamic k cycle conflict
may sacrificed due greedy method used select patterns
tested empowered different search heuristics set uci
machine learning datasets pattern database heuristics
contributed significant improvements efficiency scalability typically efficient dynamic
programming shares similar formulation comparison gobnilp integer
programming less sensitive number optimal parent sets number
data points scoring functions sensitive number variables
datasets advantages believe methods represent promising
learning optimal bayesian network structures
exact learning optimal bayesian networks still limited relatively
small scaling learning needed e g incorporating domain
expert knowledge learning means approximation methods still useful
domains many variables nevertheless exact valuable
serve basis evaluate different approximation methods


fiyuan malone

quality assurance promising direction develop
best properties approximation exact
good solutions quickly given enough resources converge optimal
solution malone yuan

acknowledgments
supported nsf grants iis eps iis
academy finland finnish centre excellence computational inference
coin part previously presented ijcai yuan
malone wu uai yuan malone

appendix proofs
following proofs theorems
proof theorem
proof note optimal parent set x u subset u
subset best score sorting unique parent scores makes sure
first found subset must satisfy requirements stated theorem

proof theorem
proof heuristic function h clearly admissible allows remaining variable
choose optimal parents variables v chosen parent set must
superset parent set variable optimal directed acyclic graph
consisting remaining variables due theorem heuristic lower
bound cost

proof theorem
proof successor node u let u
x
h u
bestscore x v x
xv u



x

bestscore x v x

xv u x

bestscore u
h c u
inequality holds fewer variables used select optimal parents hence
h consistent

proof theorem
proof theorem proven noting avoiding cycles variables
u equivalent finding optimal ordering variables best joint score


filearning optimal bayesian networks

different paths v u goal node correspond different orderings
variables among shortest path hence corresponds optimal ordering
proof theorem
proof node u assume remaining variables v u partitioned exclusive
sets v vp decomposability scoring function h u
p
p
c vi computing c vi allow directed cycles within vi


variables v vi valid candidate parents however cost pattern c vi
must optimal definition pattern databases argument used
proof theorem h u cost cannot worse total cost v u
cost optimal directed acyclic graph consisting variables u allowable
parents otherwise simply arrange variables patterns
order optimal directed acyclic graph get cost therefore heuristic
still admissible
note previous argument relies optimality pattern costs
patterns chosen greedy strategy used dynamic pattern database
affects patterns selected therefore theorem holds dynamic
static pattern databases

proof theorem
proof recall static pattern databases node partitions v vi
heuristic value node u follows
h u

x

c v u vi



v u vi pattern ith static pattern database successor
node u let u without lost generality let v u vj heuristic
value node
h

x

c v u vi c v u vj

j

cost u
c u bestscore u
definition pattern database know c v u vj best possible
joint score variables pattern u searched therefore
c v u vj c v u vj bestscore j vi vj v u
c v u vj bestscore u
last inequality holds u j vi vj v u following
immediately follows
h u h c u


fiyuan malone

hence static k cycle conflict heuristic consistent

proof theorem
proof heuristic values calculated dynamic pattern database considered shortest distances nodes abstract space abstract space consists
set nodes e subsets v however additional arcs added
node nodes k additional variables
consider shortest path p two nodes u goal v original solution
space path remains valid path may longer shortest path u
v additional arcs
let g u v shortest distance u v abstract space
successor node u must following
g u v g u g v



recall g u v g v heuristic values original solution
space g u equal arc cost c u original space therefore
following
h u c u h

hence dynamic k cycle conflict heuristic consistent



references
acid de campos l hybrid methodology learning belief networks
benedict international journal approximate reasoning
akaike h information theory extension maximum likelihood principle
proceedings second international symposium information theory pp

bache k lichman
http archive ics uci edu ml



uci

machine

learning

repository

bouckaert r r properties bayesian belief network learning
proceedings tenth conference uncertainty artificial intelligence pp
seattle wa morgan kaufmann
bozdogan h model selection akaikes information criterion aic general
theory analytical extensions psychometrika
buntine w theory refinement bayesian networks proceedings seventh
conference uncertainty artificial intelligence pp san francisco
ca usa morgan kaufmann publishers inc
cheng j greiner r kelly j bell liu w learning bayesian networks
data information theory artificial intelligence



filearning optimal bayesian networks

chickering transformational characterization equivalent bayesian network
structures proceedings th annual conference uncertainty artificial
intelligence uai pp san francisco ca morgan kaufmann publishers
chickering learning bayesian networks np complete learning
data artificial intelligence statistics v pp springer verlag
chickering learning equivalence classes bayesian network structures
journal machine learning
cooper g f herskovits e bayesian method induction probabilistic
networks data machine learning
culberson j c schaeffer j pattern databases computational intelligence

cussens j bayesian network learning cutting planes proceedings
twenty seventh conference annual conference uncertainty artificial intelligence uai pp corvallis oregon auai press
daly r shen q learning bayesian network equivalence classes ant colony
optimization journal artificial intelligence
dash cooper g model averaging prediction discrete bayesian
networks journal machine learning
dash h druzdzel j hybrid anytime construction
causal sparse data proceedings fifteenth annual conference
uncertainty artificial intelligence uai pp san francisco ca
morgan kaufmann publishers inc
de campos c p ji q efficient learning bayesian networks constraints
journal machine learning
de campos c p ji q properties bayesian dirichlet scores learn bayesian
network structures fox poole eds aaai pp aaai press
de campos l scoring function learning bayesian networks
mutual information conditional independence tests journal machine learning

de campos l fernndez luna j gmez j puerta j ant colony
optimization learning bayesian networks international journal approximate
reasoning
de campos l huete j f learning belief networks
independence criteria international journal approximate reasoning



fiyuan malone

de campos l puerta j stochastic local learning belief
networks searching space orderings benferhat besnard p
eds ecsqaru vol lecture notes computer science pp
springer
edelkamp schrodl heuristic search theory applications morgan
kaufmann
felner korf r hanan additive pattern database heuristics journal
artificial intelligence
felzenszwalb p f mcallester generalized architecture journal
artificial intelligence
friedman n koller bayesian network structure bayesian
structure discovery bayesian networks machine learning

friedman n nachman peer learning bayesian network structure
massive datasets sparse candidate laskey k b prade h
eds proceedings fifteenth conference conference uncertainty artificial
intelligence uai pp morgan kaufmann
garey r johnson computers intractability guide
theory np completeness w h freeman co york ny usa
glover f tabu search tutorial interfaces
hart p e nilsson n j raphael b formal basis heuristic determination minimum cost paths ieee trans systems science cybernetics

heckerman geiger chickering learning bayesian networks
combination knowledge statistical data machine learning
heckerman tutorial learning bayesian networks holmes jain
l eds innovations bayesian networks vol studies computational
intelligence pp springer berlin heidelberg
hemmecke r lindner studeny characteristic imsets learning
bayesian network structure international journal approximate reasoning

hsu w h guo h perry b b stilson j permutation genetic
variable ordering learning bayesian networks data langdon w b
cant paz e mathias k e roy r davis poli r balakrishnan k honavar
v rudolph g wegener j bull l potter schultz c miller j f
burke e k jonoska n eds gecco pp morgan kaufmann


filearning optimal bayesian networks

jaakkola sontag globerson meila learning bayesian network
structure lp relaxations proceedings th international conference
artificial intelligence statistics aistats pp chia laguna resort
sardinia italy
klein manning c parsing fast exact viterbi parse selection
proceedings human language conference north american association
computational linguistics hlt naacl pp
koivisto sood k exact bayesian structure discovery bayesian networks
journal machine learning
kojima k perrier e imoto miyano optimal search clustered
structural constraint learning bayesian network structure journal machine
learning
lam w bacchus f learning bayesian belief networks
mdl principle computational intelligence
larranaga p kuijpers c h murga r h yurramendi learning
bayesian network structures searching best ordering genetic ieee transactions systems man cybernetics part

malone b yuan c evaluating anytime learning optimal bayesian
networks proceedings th conference uncertainty artificial intelligence uai pp seattle washington
malone b yuan c hansen e bridges improving scalability optimal bayesian network learning frontier breadth first branch bound search
proceedings th conference uncertainty artificial intelligence uai
pp barcelona catalonia spain
malone b yuan c hansen e b memory efficient dynamic programming
learning optimal bayesian networks proceedings th aaai conference
artificial intelligence aaai pp san francisco ca
moore lee cached sufficient statistics efficient machine learning
large datasets journal artificial intelligence
moore wong w k optimal reinsertion search operator accelerated accurate bayesian network structure learning international
conference machine learning pp
myers j w laskey k b levitt learning bayesian networks
incomplete data stochastic search laskey k b prade h
eds proceedings fifteenth conference conference uncertainty artificial
intelligence uai pp morgan kaufmann


fiyuan malone

ordyniak szeider complexity exact bayesian
structure learning gruwald p spirtes p eds proceedings th
conference conference uncertainty artificial intelligence uai pp
auai press
ott imoto miyano finding optimal small gene networks
pacific symposium biocomputing pp
papadimitriou c h steiglitz k combinatorial optimization
complexity prentice hall inc upper saddle river nj usa
parviainen p koivisto exact structure discovery bayesian networks
less space proceedings twenty fifth conference uncertainty artificial
intelligence montreal quebec canada auai press
pearl j heuristics intelligent search strategies computer solving
addison wesley longman publishing co inc boston usa
pearl j probabilistic reasoning intelligent systems networks plausible inference morgan kaufmann publishers inc
perrier e imoto miyano finding optimal bayesian network given
super structure journal machine learning
rissanen j modeling shortest data description automatica
silander myllymaki p simple finding globally optimal bayesian network structure proceedings nd annual conference
uncertainty artificial intelligence uai pp auai press
silander roos kontkanen p myllymaki p factorized normalized
maximum likelihood criterion learning bayesian network structures proceedings
th european workshop probabilistic graphical pgm pp

singh moore w finding optimal bayesian networks dynamic programming tech rep cmu cald carnegie mellon university
spirtes p glymour c scheines r causation prediction search second
edition mit press
suzuki j learning bayesian belief networks minimum description
length principle efficient b b technique international
conference machine learning pp
teyssier koller ordering search simple effective
learning bayesian networks proceedings twenty first annual conference
uncertainty artificial intelligence uai pp auai press


filearning optimal bayesian networks

tian j branch bound mdl learning bayesian networks
uai proceedings th conference uncertainty artificial intelligence
pp san francisco ca usa morgan kaufmann publishers inc
tsamardinos brown l aliferis c max min hill climbing bayesian
network structure learning machine learning
xie x geng z recursive method structural learning directed acyclic
graphs journal machine learning
yuan c lim h littman l relevant explanation computational
complexity approximation methods annals mathematics artificial intelligence
yuan c lim h lu c b relevant explanation bayesian networks
journal artificial intelligence jair
yuan c liu x lu c lim h relevant explanation properties
evaluations proceedings th conference uncertainty
artificial intelligence uai pp montreal canada
yuan c malone b improved admissible heuristic learning optimal
bayesian networks proceedings th conference uncertainty artificial
intelligence uai pp catalina island ca
yuan c malone b wu x learning optimal bayesian networks
search proceedings nd international joint conference artificial intelligence ijcai pp helsinki finland




