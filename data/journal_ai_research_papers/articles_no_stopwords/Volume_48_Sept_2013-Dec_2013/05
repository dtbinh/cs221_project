journal artificial intelligence

submitted published

unsupervised sub tree alignment
tree tree translation
tong xiao
jingbo zhu

xiaotong mail neu edu cn
zhujingbo mail neu edu cn

college information science engineering
northeastern university
wenhua road heping district
shenyang china

abstract
article presents probabilistic sub tree alignment model application
tree tree machine translation unlike previous work resort surface heuristics expensive annotated data instead derive unsupervised model infer
syntactic correspondence two languages importantly developed model
syntactically motivated rely word alignments product
model outputs sub tree alignment matrix encoding large number diverse alignments
syntactic structures machine translation systems efficiently extract translation rules often filtered due errors best alignment
experimental proposed outperforms three state art
baseline approaches alignment accuracy grammar quality applied
machine translation yields bleu improvement ter reduction nist machine translation evaluation corpora tree binarization
fuzzy decoding even outperforms state art hierarchical phrase system

introduction
recent years witnessed increasing interest syntax methods many artificial intelligence ai natural language processing nlp applications ranging
text summarization machine translation mt particular syntax
intensively investigated statistical machine translation smt approaches include
string tree mt galley hopkins knight marcu galley graehl knight marcu
deneefe wang thayer tree string mt liu liu lin huang kevin
joshi tree tree mt eisner zhang jiang aw li tan li
liu lu liu chiang train tree string tree tree pairs
seek model translation equivalency relations learned parsed data part
focus syntax mt tree tree use synchronous context free grammars synchronous tree substitution grammars received growing interest showing
promising several well established evaluation tasks zhang et al liu
et al chiang example recent studies chiang demonstrated
modern tree tree systems significantly outperform hierarchical phrase
counterpart large scale chinese english arabic english translation
tree tree mt translation broadly regarded transformation
source language syntax tree target language syntax tree model process
c

ai access foundation rights reserved

fixiao zhu

tree tree systems resort general framework synchronous grammars
pair trees generated derivations synchronous grammar rules translation
rules model goal translation build underlying derivations
pairs trees output target string encoded likely derivation figure
shows intuitive example illustrate generation process tree pair
sample grammar source target language sentences associated
phrase structure trees generated automatic parsers
previous work shown acquisition good translation rules one essential factors contributing success syntax systems deneefe knight wang
marcu date several groups addressed issue rule acquisition designed effective extract high coverage grammars bilingual
parsed data zhang et al liu et al chiang despite differences
detailed modeling approaches rely syntactic alignments align tree nodes
syntactic parse tree one language tree nodes alignments
could employed standard tree tree rule extraction liu et al
chiang
current tree tree heavily depend syntactic alignments two
languages alignments induced indirectly word alignments tree tree
systems sensitive word alignment behavior unfortunately word alignments
general far perfect viewpoint syntactic alignment fossum knight
abney cases even one spurious word alignment prevent large
number desirable rules extraction example figure shows tree totree translation rules extracted word alignment produced giza
alignment incorrectly aligns source word past tense marker chinese
target word spurious word alignment produces incorrect rule
dt blocks extraction high level syntactic transfer rules
ip nn vp np vp
obviously desirable solution directly infer node correspondences
source target parse trees namely sub tree alignment syntactic parse trees
explain underlying structure sentences well performing alignment sub tree level
make benefits high level structural information syntactic categorization
example consider alignment figure b links nodes two parse
trees chinese english rather aligning word level example
confident align vp sub tree spanning source tree
vp sub tree spanning drastically fallen target tree therefore
phrase structure tree leaf nodes words sentence internal tree nodes followed
leaf nodes labeled part speech pos tags tree nodes labeled syntactic
categories defined treebanks see appendix meanings pos tags syntactic categories used
work nlp many well developed parsers available automatic parsing several
good quality phrase structure treebanks across languages used train parsing
penn english chinese treebanks marcus santorini marcinkiewicz xue xia chiou
palmer note addition phrase structure syntax popular formalisms
e g dependency syntax used syntax mt discussion different formalisms
syntactic parsing beyond scope article instead focus tree tree mt
phrase structure trees throughout work
chinese english follow subject verb object structure verb phrases chinese
sentence frequently aligned verb phrases english translation



fiunsupervised sub tree alignment tree tree translation

np

vp

np

vp

prp

vbd





target language side
english



vp
vbn

pp

satisfied
pp


np



r

r

dt

nns



answers

r


ta


huida

p

nn


biaoshi


manyi

vv

nn

pp

pn

pp

vp

np

vp

np

vp

source language side
chinese

r

dui

ip

synchronous grammar used
id
r
r
r
r

source language side
np pn
pp p nn
vp pp vp vv nn
ip np vp

target language side
np prp
pp np dt nns answers
vp vbd vp vbn satisfied pp
np vp

figure example derivation tree tree translation rules rules represented
aligned pairs tree fragments linked dotted lines subscripts
language sides grammar rules indicate alignments frontier nonterminals language side derivation round head lines link
frontier non terminals rewritten translation
know child nodes source language vp likely aligned child
nodes target language vp means two vps aligned
children aligned outside vp sub tree structure e prevent
alignment chinese tree node english tree node dt due
inconsistency vp vp alignment case correctly aligned vbp


fixiao zhu




np
dt

np

vp

nns

vbp

dt

advp

imports

rb

vp

nns

vbp

advp

imports

vbn

rb

drastically fallen

drastically fallen








vv



ad

nn

vbn




vp



vv



ad

nn

vp



vp
vp

ip

ip

minimal rules extracted

minimal rules extracted
r

dt

r

ad rb drastically

r

nn nns imports

r

vv vbn fallen

r

ad rb drastically

r

vbp

r

vv vbn fallen

r

nn np dt nns imports

r

ip nn vp ad vp vv

r

vp ad vp vv
vp vbp advp rb vbn

np dt nns vp vbp advp rb vbn

r

word alignment extracted rules

ip nn vp np vp

b sub tree alignment extracted rules

figure tree tree translation rules extracted via word alignment sub tree alignment b dashed lines represent word alignment links dotted lines
represent sub tree alignment node alignment links
bad rule dt ruled desirable rules
extracted sub tree alignment including desirable rules blocked
figure
actually researchers aware sub tree alignment tried
explore solutions tinsley zhechev hearne way sun zhang tan b
example proposed judge whether two nodes aligned
work alignment confidence first calculated lexical translation probabilities
classifiers trained labeled data final alignment determined according
node level alignment score however inference sub tree alignment approaches
relies heuristic essentially optimized within unified
probabilistic framework
moreover alignment applied tree tree translation systems
suffer another translation rules extracted best alignment
zhang et al liu et al chiang significantly affects


fiunsupervised sub tree alignment tree tree translation

rule set coverage rate due alignment errors simple solution issue use
k best alignments instead however k best alignments often variations many
redundancies differ alignment links obviously inefficient
extract rules similar alignments
article address sub tree alignment issue principled way investigate
methods effectively apply sub tree alignment tree tree mt particular
develop unsupervised learning probabilistic sub tree alignment
model bi lingual parsed data
investigate different methods integrating sub tree alignment tree tree
machine translation specifically develop sub tree alignment matrix encoding
exponentially large number diverse sub tree alignments extract multiple
alternative translation rules alignment posteriors sub tree alignment
matrix
advantages three fold first rely
heuristic labeled data second developed sub tree alignment model
structure model used mt e synchronous tree
substitution grammars means mt systems directly make benefits subtree alignment model especially rule extraction mt parameter estimation third
accessing sub tree alignment matrix encodes large number alignments
efficiently obtain rules often filtered due errors within best k best alignment experiment chinese english subtree alignment translation tasks sub tree alignment significantly outperforms
three state art baselines machine translation obtains significant
improvements tree tree system rule quality translation quality
example yields bleu improvement ter reduction nist mt
evaluation corpora finally system even outperforms state art hierarchical
phrase system equipped tree binarization wang knight marcu b
fuzzy decoding chiang techniques
rest article structured follows section briefly introduces subtree alignment task section describes unsupervised sub tree alignment
section investigates effective methods applying alignment model tree totree translation section presents experimental evaluation
reviewing related work section interesting issues discussed section
finally article concluded summary section

statement
general sub tree alignment defined task alignment
nodes tree nodes another tree restrict machine translation article sub tree alignment actually task must tightly coupled
specific applications example addition machine translation
work term tree refers data structure defined recursively collection nodes
starting root node node list edges pointing nodes children
constraint edge duplicated points root knuth



fixiao zhu

nlp tasks make benefits sub tree alignment including sentence simplification cohn lapata woodsend lapata paraphrasing das smith
question answering wang smith mitamura parser adaptation
projection smith eisner
ideally would sub tree alignment system language independent
application independent given parallel corpus training examples able
learn alignment model use infer syntactic correspondence tree
pairs broadly speaking alignments paired linguistic tree structures
regarded instances sub tree alignment example alignment performed
dependency trees eisner nakazawa kurohashi phrase structure
trees tinsley et al sun et al b
although sub tree alignment includes number tasks seek alignments syntactic tree structures particularly interested aligning tree
nodes phrase structure trees work focus phrase structure sub tree alignment phrase structure parsing one popular syntactic analysis
formalisms several state art full parsing tools developed many
languages phrase structure trees basis many successful syntax mt systems alternatives dependency trees benefit mt systems
constituency interest relatively larger portion mt community state art performance recent tree tree systems zhang et al
liu et al chiang
natural language processing phrase structure parse tree ordered rooted
tree represents syntactic structure sentence according phrase structure grammars constituency grammars describe way words combine form
phrases sentences chiswell hodges generally phrase structure parse trees
distinguish terminal non terminal nodes leaf nodes labeled terminal categories words internal nodes labeled non terminal categories
grammar phrasal categories example english parse tree figure b
imports terminal nodes np nns two non terminals indicating
noun phrase plural form nouns respectively following description
experiments take penn treebank standard tree annotation
choose penn treebank one popular tree annotated corpora
used syntactic parsing good quality quantity several languages
chinese english
definition sub tree alignment defined alignments
non terminals source target language phrase structure parse trees
formally given source language parse tree target language parse tree
sub tree alignment denoted short set node node links
node pair u v good alignment follow three criteria
tinsley et al
u v aligned indicating alignment
note non terminals followed leaf nodes called pre terminals
labeled part speech tags e g nns node followed terminal node imports
thus pre terminal
contrast word alignment regarded alignments terminals two languages



fiunsupervised sub tree alignment tree tree translation

u aligned v descendants u aligned descendants v
u aligned v ancestors u aligned ancestors v
criteria prevent aligning constituents cross property
similar bi parsing formalisms synchronous context free
grammars synchronous tree substitution grammars advantage enables
use powerful synchronous grammars modeling sub tree alignment
shown next section constraints take synchronous
tree substitution grammars basis proposed model
according tinsley et al work alignments satisfying criteria
called well formed alignments alignment ill formed violates
criteria work focus well formed alignments hence sub tree alignment
task stated given pair parse trees search likely wellformed alignment
arg max p





set well formed alignments p viewed
alignment model predicts probability every alignment given
follows describe sub tree alignment tree tree translation including alignment model training inference methods effective
use model tree tree mt systems

unsupervised sub tree alignment
section present unsupervised sub tree alignment model first define
base model sub tree alignment framework synchronous tree substitution
grammars describe model parameterization training inference methods
base model
fundamental question sub tree alignment define correspondence
nodes source language parse tree nodes target language parse tree
address issue synchronous tree substitution grammars stsgs
widely adopted model transformation process source target language
parse trees mt zhang et al liu et al chiang general
framework stsgs chiang knight assumed pair source
target parse trees simultaneously generated derivation stsg rules
tree tree transfer rules example grammar figure stsg
rules used generate pair sentences formally stsg system
hns nt ws wt ns nt sets non terminals source target
languages ws wt sets terminals words source target languages
finite set productions production stsg rewrite rule denoted r
pair source target language non terminals snt tnt
hsnt tnt hsr tr r


fixiao zhu

sr source language tree fragment whose frontier nodes words ws
non terminals ns labeled x tr corresponding target language tree fragment
r set alignments connect frontier non terminals sr
frontier non terminals tr example r figure
snt ip
tnt
sr ip nn x vp ad x vp vv x x
tr np dt x nns x vp vbp advp rb x vbn x
r
note non terminals left hand side rule actually roots
corresponding tree fragments right hand side means rule contains
exactly information matter whether root nodes snt tnt explicitly
represented following parts article use hsr tr r simpler representation stsg rules beyond stsg rules written
compact form alignment r encoded numbers assigned frontier
non terminals sr tr example figures subscripts language
sides stsg rules indicate aligned pairs frontier non terminals
stsg model frontier non terminals called substitution nodes
applying stsgs rewrite aligned pair substitution nodes tree fragment
pair encoded stsg rule constraint operation labels
substituted non terminals must match root labels rewrite rules example
round head lines figure substitution operations used derivation
stsg rules parse tree pair generate corresponding derivations generation process trivial start pair root symbols repeatedly rewrite pairs non terminal symbols stsg rules example tree pair
figure b start root labels source target language parse trees
superscript indicates node index tree
h ip
apply rule r
ip

h ip nn vp np vp
r

ip

represents operation rewrites aligned node pair ip
r

r denoted ip process proceeds repeatedly rewriting
remaining frontier non terminals get complete source target language trees



fiunsupervised sub tree alignment tree tree translation

nn np


r

vp vp


r

h ip nn vp np dt nns imports vp
h ip nn vp ad





vp vv



np dt nns imports vp vbp
ad rb


r

h ip nn vp ad vp vv



np dt nns imports vp vbp


vv



advp rb



vbn





advp rb drastically vbn



vbn

h ip nn vp ad vp vv
r

np dt nns imports vp vbp



advp rb drastically vbn fallen
vbp


r

h ip nn vp ad vp vv
np dt nns imports vp vbp
advp rb drastically vbn fallen

process rewrite rule indicates node alignment importantly
derivations model two nice properties first node u
source language target language parse tree one node targetlanguage source language parse tree aligned u second hierarchical
structure behind alignment avoids links constituents cross
consequently well formed sub tree alignment derivation
encodes alignment means sub tree alignment essentially
finding likely stsg derivation thus sub tree
alignment task see equation restated finding likely derivation
given pair parse trees
model derivation probability follow formulation adopted statistical
word alignment brown pietra pietra mercer vogel ney tillmann
transformation source language tree target language tree described
following equation
x
p
p

dd

set derivations transforming say aligning nodes
nodes p probability transforming
derivation parameters model use notation
p express dependence model parameters general optimal
value learned parsed parallel data training criteria example
context unsupervised learning optimize model parameters maximizing
probability observed data known maximum likelihood training
given set optimal parameters best sub tree alignment determined
p
choosing derivation p greatest since p p




fixiao zhu

p constant given finding best derivation
finding derivation make p large possible hence reach
fundamental equation sub tree alignment
arg max p



dd

formulation implies three fundamental issues sub tree alignment including
modeling derivation probability e p learning model parameters e
finding best alignment given learned model e arg max operation
following parts section describe solutions issues
parameterization
simplest case alignment model one parameter instance derivation
however model would unmanageable set parameters since number
derivations exponential length input sentences choose simple
solution issue decomposes base model product trainable submodels start assumption rules conditionally independent given
source language parse tree probability p defined product
rule probabilities conciseness drop subscript

p
p r

rd

nevertheless complex tree tree mappings still extremely large number
rules causes computational degenerate analysis
data control number parameters reasonable level decompose
rule probability simpler probability factors independence assumptions
first assume generation rule r independent input tree
conditioned source language side rule
p r p r sr



note strong assumption generation synchronous grammar
rule depends source language side similar used statistical
modeling machine translation brown et al koehn och marcu galley
et al chiang generation atomic alignment translation units
conditioned associated source language words tree fragments rather
whole input sentence tree smt independence assumptions phrases
translation rules generally used decompose parallel corpus manageable units
parameter estimation successfully used modern smt
systems adopt similar assumption ease parameter estimation process
model
decompose p r sr additional assumptions since r hsr tr r
p r sr written another form chain rule
degenerate analysis refers case complex overfitting
poor generalization ability unseen data



fiunsupervised sub tree alignment tree tree translation

p r sr p sr tr r sr
p r sr tr p tr sr



equation indicates two sub including reordering model frontier nonterminals p r sr tr tree fragment translation model p tr sr
model p r sr tr view frontier non terminal reordering aligning
elements two vectors non terminals let vnt function returns
vector leaf non terminals given tree fragment r defines alignment
non terminals vnt sr vnt tr example r figure
frontier non terminal vectors sr tr
vnt sr nn ad vv
vnt tr dt nns rb vbn
r indicates alignment vnt sr vnt tr say
nn aligned nns ad aligned rb opt simple model
selecting r non terminal reordering probability condition
frontier non terminal vectors language sides follows
p r tr sr preorder r vnt sr vnt tr



turn modeling tree fragment translation p tr sr e
second sub model defined equation define tree fragment consists
two parts words lex e terminals tree structure tree without lexicons
involved example r figure target language tree fragment contains
two elements lex tr tree tr
lex tr
tree tr np dt x nns x vp vbp advp rb x vbn x
let root function returns root given tree fragment write
p tr sr
p tr sr p lex tr tree tr sr
p root tr sr
p tree tr root tr sr
p lex tr tree tr root tr sr



worth noting equation approximation choose
one many ways p tr sr written product series
reordering model defined ensures arbitrary alignments handled might
large model sparse parameter distributions big tree fragments involved
considering issue choose several pruning methods better control rule size sub tree
alignment system see section pruning settings work



fixiao zhu

conditional probabilities simply assert equation generating targetlanguage tree fragment source language tree fragment first choose root
symbol target language tree fragment given source language tree fragment
probability p root tr sr choose tree structure target language
tree fragment given root symbol source language tree fragment probability
p tree tr root tr sr choose target language terminals associated
tree fragment given target language tree structure target language root symbol
source language tree fragment probability p lex tr tree tr root tr sr
another note equation actually reduce model complexity
example p lex tr tree tr root tr sr essentially indicates combinations source
target language tree fragments simpler model required feasible solution
parameter estimation introduce additional assumptions relax
conditions probabilities reduce number parameters reasonable level
p root tr sr depends root sr e
p root tr sr pnt root tr root sr



assumption implies node correspondence source targetlanguage parse trees
p tree tr root tr sr depends root tr e
p tree tr root tr sr ptree tree tr root tr



second assumption monolingual model generating target language
tree structures generation tree fragment conditioned
root viewed analogy generative model used standard tsgs
p lex tr tree tr root tr sr depends source words lex sr e
p lex tr tree tr root tr sr plex lex tr lex sr



allows us directly model terminal correspondence two languages
substitute equations equation get
p tr sr pnt root tr root sr
ptree tree tr root tr
plex lex tr lex sr
equations equation finally written




fiunsupervised sub tree alignment tree tree translation

id

rule

probability

r

ad rb drastically

pnt rb ad plex drastically

r

vv vbn fallen

pnt vbn vv plex fallen

r

vbp

pnt vbp plex

r

nn

pnt np nn plex imports

np dt nns imports

ptree np dt nns np

vp ad vp vv

pnt vp vp ptree vp vbp advp rb vbn vp

vp vbp advp rb vbn

preorder ad vv vbp rb vbn

ip nn vp np vp

pnt ip ptree np vp

r
r

preorder nn vp np vp

table rule probabilities sample derivation r r r r r r figure b
p



pnt root tr root sr

rd

ptree tree tr root tr
plex lex tr lex sr
preorder r vnt sr vnt tr



simplified model generative story described section takes
rule generation probability product four probability factors pnt nonterminal mapping probability roughly captures syntactic correspondence subtrees two languages ptree probability generating tree structure
plex probability terminal mappings two language sides
rule preorder probability frontier non terminal reordering encoded
rule see table rule probabilities sample derivation
model parameters assumed multinomial distributions calculation pnt ptree preorder straightforward directly used
without decompositions assumptions calculate plex choose
form adopted popular word alignment och ney thayer ettelaie knight marcu munteanu och tipu probability defined
product word translation probabilities
l


x
plex tl sm plength l
pw ti sj





j

ti target word sj source word plength used control number
target words produced given number source words pw word translation
probability sub model principle something rather similar conventional
word translation tables ibm brown et al
node deletion insertion
word sub tree deletion insertion common real world alignment translation
tasks add flexibility modeling allow production empty


fixiao zhu

sub trees source target language side rule model formally
rule whose target language side empty sub tree probability defined
p r pnt root root sr
ptree tree root
plex lex lex sr
preorder vnt sr vnt



special symbol indicates nothing factors pnt root root sr
plex lex lex sr model deletion probability different levels tree fragment
ptree tree root probability generating empty tree fragment factor
preorder vnt sr vnt regards special reordering pattern aligns
frontier non terminals source side virtual node null obviously values
ptree tree root preorder vnt sr vnt simply
similarly rule whose source side empty sub tree probability defined
p r pnt root tr root
ptree tree tr root tr
plex lex tr lex
preorder vnt vnt tr



value preorder vnt vnt tr
worthwhile note word deletion insertion important
mt spite relatively less discussion recent studies tree tree translation
actually analogy null alignment used ibm
brown et al word phrase removing words alignment
leave space correctly aligning words sentence even
necessary sub tree alignment alignment respect syntactic
constraints language sides e g sub tree alignments allowed break
constraints imposed neighbouring parts tree cases cannot obtain
correct alignment tree pair due one two bad nodes
necessarily aligned valid node counterpart tree instead nodes
skipped alignment thus impose bad constraints parts
tree node deletion insertion allowed especially true align sentence
pairs flat tree structures free translations work found node
deletion insertion operations necessary achieve satisfactory sub tree alignment
therefore used implementation default
training
turn training discussed section focus unsupervised
learning model parameters optimal values parameters estimated given
note current phrase approaches koehn et al och ney allow null aligned
words appear boundary phrase viewed way implicit modeling
word insertion deletion



fiunsupervised sub tree alignment tree tree translation

collection tree pairs without annotation sub tree level alignment work
choose two approaches estimating parameters sub tree alignment model including
maximum likelihood estimation mle bayesian
maximum likelihood training
mle one popular methods parameter estimation statistical
basic idea given model set parameters mle method selects values
parameters generate distribution gives highest probability observed
data mle general parameter estimation widely adopted
many ai nlp tasks part speech tagging case sub tree alignment
mle simply described finding optimal values parameters lead
maximum probability aligning tree nodes source language parse tree
target language parse tree formally given set tree pairs sn tn
objective mle training defined
arg max


n


x

p ti si



dd si ti

choose expectation maximization em dempster laird rubin
solve optimization basically em
iterative training method finding maximum likelihood estimates model parameters
assumed observed data depends latent variables
performs iteratively calling two sub routines namely expectation e step
maximization step e step calculates expected value likelihood
function associated parameters observed data respect distribution
latent variables given observed data current estimates parameters
step seeks parameters maximize expected likelihood found
e step
applying em case view input pairs parse
trees sn tn observed data underlying derivations rules latent
variables distributions pnt ptree preorder plex e plength
pw unknown parameters see figure pseudo code training
pnt denoted tnt snt directly applicable estimation
parameters model skip description learning remaining parameters
detailed description em training model parameters
refer reader appendix
snt tnt represent source language non terminal symbol
target language non terminal symbol u v represent source language tree node
target language tree node ec represents expected count given variable
p k represents derivation probability parameters obtained
k th round em iteration e step accumulates expected
count pairs parse trees step finds maximum likelihood estimate
quantity nontrivial part computation
expected count e step roughly speaking physical meaning right hand
side line relative probability derivation contains rule r root node


fixiao zhu

function trainmodelwithem sn tn

set tnt snt initial model
k k

foreach non terminal symbol pair snt tnt

ec tnt snt
e step

foreach tree pair sequence sn tn

foreach node pair u v symbol pair tnt snt

foreach rule r rooted
p u v
p

rooted u v

ec tnt snt rd r p

p k
step

foreach non terminal symbol pair snt tnt




k

tnt snt

p

k

ec tnt snt
ec

nt

nt

nt

k

return tnt snt
figure em training pnt

p
pair u v numerator rd r rooted u v p k probability sum
p
derivations involve r denominator p k
overall probability alignment however brute force computation
expected counts inefficient requires sum possible derivations
whose number exponential length input sentences
work use bilingual version inside outside probabilities manning
schutze avoid naive enumeration possible derivations computing
probabilities inside probability u v denoted u v measures
likely generate sub tree pair inside node pair u v outside probability
denoted u v dual inside probability measures likely generate
remaining parts tree pair start symbols formulation
used monolingual parsing manning schutze u v u v defined
following recursive forms


x

u v
p r
p q

r root r u v

u v

x

p q yield r



root r p r

r u v yield r



p q





p q yield r
p q u v

root r abbreviation node pair root sr root tr yield r
set aligned frontier non terminal pairs yielded r recursive
definitions u v u v efficiently computed dynamic programming
inside outside probabilities easy address computation
mentioned let u v denote probability tree node u aligned
tree node v probability expressed inside outside fashion


fiunsupervised sub tree alignment tree tree translation

u v

x

p

u v
aligned

u v u v



way overall alignment probability e denominator
right hand side line simply written
x

p root root root root





numerator right hand side line let us view another angle
e step expected count accumulated rules whose root u v
rules rooted u v indicate node alignment u v lines
principle imply probability derivations aligning u v precisely
node alignment probability u v probability written simple
form inside outside probabilities
x
x
p u v
r r rooted rd
u v

u v u v



together equation e step efficiently implemented
replacing lines following equation
ec tnt snt

u v u v
root root root root



snt tnt symbol pair u v note snt tnt e step step
u v u v
increases ec tnt snt sum root root
root root node pairs u v
whose symbols snt tnt means snt tnt aligned different positions
input tree pair method considers alignment snt tnt multiple
times updates ec tnt snt accordingly
worth noting several methods initializing model parameters em style training begins example model initialized
uniform random distributions work initialize parameters sub tree
alignment model model obtained word alignment standard way adopted many unsupervised simpler model used good
starting point training process helpful optimization procedure
sensitive initial setting model parameters e g em non convex objective functions experiments found giza word alignment parameter
initialization resulted better performance fewer iterations convergence
uniform initial distributions word alignment obtained unsupervised
manner change training condition thus chose
method initializing model parameters implementation


fixiao zhu

bayesian
mle one standard approaches training unsupervised well
known tendency overfit data overfitting becomes severe
complex since parameters fit training data better
case stsgs likely degenerate analysis data e rare
big rules dominate ml solution stsgs considered noisy
generalize poorly unseen data cohn blunsom liu gildea
natural solution incorporate constraints proper priors
training process take bayesian alternative solution
training
unlike mle bayesian plug single optimum point estimate
parameter distribution data point instead account uncertainty
value parameter bayesian parameters assumed
drawn probability distributions priors parameters extra prior
distributions called hyperparameters denoted parameters
model viewed mathematically multinomials choose dirichlet distributions
ferguson prior model parameters advantage dirichlet
distributions conjugate multinomial distributions inference
priors easier
following previous description use denote model parameters
multinomial outcomes k e k probability outcome k k
multinomial distribution sample set outcomes x xn probability
p xi k k dirichlet prior distribution multinomials sample
prior actually set parameter values therefore distribution
modeled
xi multinomial



dirichlet



equation means xi distributed according multinomial parameters
similarly equation read distributed according dirichlet distribution parameters k hyperparameter vector corresponding
outcomes work use symmetric dirichlet prior e k share
value use represent single hyperparameter instead hyperparameter
vector
model compute conditional distribution observation
xn given previous observations x xn hyperparameter follows
z
p xn x xn p xn x xn p

big advantage bayesian introduce prior distribution
unknown parameters model meant capture knowledge beliefs
model seeing data neal especially important case
need bias towards preferred situations example expect
model favor high frequency rules dislike rare big rules goal


fiunsupervised sub tree alignment tree tree translation

easily achieved bayesian appropriate choice priors say
dirichlet prior low concentration parameter however introduction priors
generally makes intractable estimate posterior analytically practical systems
bayesian widely used solution use approximate methods
seek compromise exact inference computational resources work
choose variational bayes approximate inference variational bayes good method
preserves benefits introducing prior tractable inference procedure
attias beal successfully applied several nlp related
hidden markov hmms ibm beal riley gildea
one good thing variational bayes seen extension
em resembles usual forms used em resulting procedure looks lot
em modified step convenient implementation
follow presented previous work beal riley gildea
variational bayesian applied similar tasks need
slight change step original em presented section
original em see figure step normalizes expected counts
collected e step standard mle variational bayesian version step
slightly modifies formula performs inexact normalization passing counts
function f x exp x

tnt snt

f ec tnt snt
p
f ec nt snt



nt

x digamma function johnson approximate effect
subtracting argument choice controls behavior estimation
set low value performs estimation way anti smoothing
subtracted rule counts small counts corresponding rare events
penalized heavily large counts corresponding frequent events affected
much example low values make equation favor non terminal pairs
aligned frequently distrust non terminal pairs aligned rarely
way variational bayesian method could control overfitting caused abusing
rare events hand larger used smoothing required
method applicable training parameters model
requires replacement step figure variational bayesian step
equation implementation variational bayes training
perform additional round normalization without variational bayes normalize rule
probabilities sum one
additional normalization process makes posterior probabilities directly comparable
obtained training methods em training note convert
bayesian inference probability distributions good explanation probability
factors model hand technical trick pseudo bayesian procedure
bayesian inference exactly though shows good empirical study one
remove additional round normalization pure bayesian changes
affect overall pipeline practical standpoint



fixiao zhu

function decode



getinsideoutsideprobabilities

foreach node u bottom order

foreach node v bottom order

u v u v u v

foreach tree fragment sr rooted u

foreach tree fragment tr rooted v

foreach frontier non terminal alignment sr ts

r createrule
q r tr

score p r p q yield r p p q

score p u v

u v createderivation r p q p q yield r
return
function getinsideoutsideprobabilities
foreach node u bottom order

foreach node v bottom order

set u v according equation
foreach node u top order

foreach node v top order

set u v according equation
return
figure decoding proposed sub tree alignment model best
posterior outputs
decoding
inference model straightforward simplest case inferring best subtree alignment given set learned parameters first visit every node pair u v
bottom fashion compute posterior probability aligning sub tree pair
rooting u v procedure dynamic program used trainer
select derivation maximum sub tree alignment probability
input tree pair generate list k best derivations similar manner
addition best k best output model able output alignment
posterior probability every pair tree nodes need record
probability u v node pair obtain inside outside probabilities
note outputting alignment posterior probabilities commonly used statistical
word phrasal aligners provides flexible way making use alignment
downstream components rule extraction system presented
next sections tree tree mt systems make great benefits posteriorbased alignment output effective rule extraction method well
better translation
figure depicts pseudo code decoding best posteriorbased outputs x data structure records best derivation rooted x x x x data structures record inside
probabilities output probabilities alignment posterior probabilities respectively cre

fiunsupervised sub tree alignment tree tree translation

aterule creates rule pair tree fragments sr tr frontier non terminal
alignment calculates rule probability createderivation builds derivation
input rules output access best alignment traversing
root root access alignment posterior
given pair trees outer two loops iterates pair
nodes two trees resulting time complexity represents
n
size input tree generating pairs tree fragments requires ntree
tree
maximum number tree fragments given tree node computing alignment
sr tr requires l l maximum number leaf non terminals

rule therefore time complexity ntree
l quadratic
size input trees note actual time complexity could
high potential alignments considered example ntree generally
exponential function depth input tree fragment deep tree could
extremely large space alignments make practical sub tree alignment
systems pruning techniques taken account work example
implementation restrict depth tree fragment reasonable number see section
addition commonly used phrasal alignment related tasks consider
word alignments pruning discard sub tree alignments violate certain
number word alignments example throw away sub tree alignments
two word alignment links outside spans covered aligned sub trees

applying sub tree alignment tree tree translation
sub tree alignment obtained current tree tree systems directly learn translation rules node aligned tree pairs section investigate methods applying
sub tree alignment tree tree rule extraction
rule extraction best k best sub tree alignments
data several methods developed tree tree rule extraction zhang et al
liu et al chiang popular ghkm
method extends idea extracting syntactic translation rules string tree pairs
galley et al ghkm extraction first compute set minimallysized translation rules explain mappings source language tree
target language tree respecting alignment reordering
two languages larger rules learned composing two minimal rules
example figure b r r two minimal rules extracted according sub tree
alignment compose rules form larger rule
ip nn vp np dt nns imports vp
work use tree tree version ghkm extraction described
liu et al work see figure pseudo code rule extraction
best sub tree alignment choose method widely used
tree tree systems note rule extraction tree tree translation generally
restricted performed best sub tree alignment ghkm


fixiao zhu

function onebestextract

foreach node u

foreach node v

foreach tree fragment pair sr tr

rooted u v

onetoonealign sr tr

empty

r createrule sr tr

rules add r

return rules
function onetoonealign sr tr
frontier non terminals sr tr

alignments

return frontier alignment sr tr
else

return

function matrixextract

foreach node u

foreach node v

isextractable u v

next loop

foreach tree fragment pair sr tr

rooted u v

foreach frontier alignment

sr tr

isextractable

r createrule sr tr

rules add r
return rules
function isextractable
foreach alignment p q

probability p q pmin

return false
return true

best extraction

b matrix extraction

figure best matrix rule extraction
extraction method employed list k best sub tree alignments provided
k best extraction need repeat procedure best extraction
sub tree alignment k best list
rule extraction sub tree alignment matrices
previous work pointed current mt systems suffer error propagation due
alignment errors made within best alignment venugopal zollmann smith
stephan sub tree alignment early stage step training pipeline
errors best alignment likely propagated translation rule extraction
parameter estimation translation model though alleviated
k best alignments limited scope k best alignments still inefficient
learning translation rules example preliminary experiment shows
extracted rules redundant best alignments involved
instead present simple efficient method namely matrix rule extraction method use posterior output aligner represent
sub tree alignment compact structure call sub tree alignment matrix alignment
matrix short liu xia xiao liu b de gispert pino byrne
see figure two example sub tree alignment matrices made pair sentence segments matrices entry indexed pair source target nodes
score entry posterior probability alignment corresponding node pair e u v probability defined equation probability
straightforwardly accessible output inference described section
principle u v viewed measure sub tree alignment confidence higher
value indicates confident alignment two nodes way


fihave

rb



vbn

drastically





fallen





vv



ad









vp



ad













vp





vv





vv


vb
n



vb
p
ad
vp

rb



ad
vp

vp
vp

vp





vp

advp

vb

vbp

vp



vp

p
ad
vp

rb

vb
n

unsupervised sub tree alignment tree tree translation



fixed alignment

possible alignment

matrix best alignment

matrix posterior

sub tree alignment matrices sample sub tree pair
minimal rules
extracted matrix posterior
r
ad rb drastically
r
vv vbn fallen
r
vbp
r
vp ad vp vv
vp vbp advp rb vbn
r vp vv vbn fallen
r vp ad vp vp vbp advp

minimal rules
extracted matrix best
r ad rb drastically
r vv vbn fallen
r vbp
r vp ad vp vv
vp vbp advp rb vbn


b rules extracted best alignment alignment posterior
figure matrix representation sub tree alignment sample rules extracted
matrix shows case best sub tree alignment matrix shows
case sub tree alignment posterior
access possible sub tree alignments different probabilities rather limited
number
extract rules sub tree alignment matrix method simple
collect rules associated entry matrix core
method essential used best k best extraction difference
best k best extraction matrix method considers possible node
pairs extraction rather visiting see figure b pseudocode sub tree alignment matrix rule extraction represents
sub tree alignment matrix pair trees compared extracting rules
k best alignments method efficiently obtain additional rules whose extraction
blocked k best extraction example right side figure b two rules
r r extracted cannot obtained best alignment
prevent extraction great number noisy rules low alignment probabilities


fixiao zhu

prune away rules whose alignment probabilities pre specified threshold
formally given pair nodes u v rule extraction executed u v
satisfies
u v
pmin

root root
expression measures relative probability alignment u v respect
sum probabilities possible derivations pmin empirical threshold control
often rules pruned larger pmin means rules thrown away
work set default therefore entries zero score figure
denoted dot excluded rule extraction
however discarding rules relatively low probabilities turn incompleteness extracted rules might unable transform given source
parse tree even training set nonetheless severe
case experiments observed parse tree pairs training
corpus could recovered extracted rules pmin chose default value
contribution translation accuracy low confidence rules limited
generally less bleu points
another note sub tree alignment matrix extraction advantage
method follows general well developed framework syntax mt
e word syntactic alignment rule extraction parameter estimation mt decoding
need replace rule extraction component sub tree alignment matrixbased system preserve components pipeline means still
use heuristics obtain additional useful rules sub tree alignment
matrix extraction rule composing galley et al spmt extraction
marcu wang echihabi knight posterior probability encoded
matrix used better estimation mt oriented features
note basis stsg model rules sub tree
alignment model resemble general forms translation rules used tree tree mt
systems alternative simple way rule induction directly infer translation rules sub tree alignment model take corresponding rule probabilities
features translation model mt decoding however tree tree mt
method suffers several first sub tree alignment model requires computation possible aligned tree fragments high time complexity
training decoding procedures aggressive pruning used
reasonable size search space e g consider relatively small tree fragments
implementation acceptable running speed side effect many relatively large
rules e g composed rules spmt rules absent sub tree alignment model
available use traditional alignment extraction heuristics pipeline
engineering standpoint efficient directly infer translation rules
sub tree alignment model compared inferring rules pruned fixed subtree alignment matrix plus heuristics second rule probability optimization
objective sub tree alignment different used mt systems example use generative model maximum likelihood bayesian sub tree
see section detailed discussion parameter estimation issue



fiunsupervised sub tree alignment tree tree translation

alignment use discriminative model minimum error rate training mt many
features employed mt decoder considered sub tree alignment model
issues might lead unsatisfactory mt performance shown experiments see section directly inferring translation rules sub tree alignment
model achieve promising
learning features machine translation
previous work syntax mt proved syntax systems make great
benefits mt oriented features even necessarily well explained
syntactic parsing viewpoint e g phrase translation probabilities however
features available word sub tree alignment model instead
need learn features additional step parameter estimation mt
follow commonly used framework estimates values mtoriented features extracted rule set mle procedure simple
translation rules extracted obtain maximum likelihood relative frequency
estimate parameters according definition feature function
however traditional tree tree systems rule extracted tree pair
count unit one used calculate values features
might enlarge influence noisy rules extracted sub tree alignment
matrices e g rule high alignment probability equal weight rule
low alignment probability thus unreasonably large impact mt systems
desired solution rule extracted derivation low probability
penalized accordingly feature learning motivated idea use fractional counts
estimate appearance rule mi huang given node pair u v
alignment probability rule r rooted u v defined denoted
r u v
x
r u v
p

dd
rd

r u v regarded probability sum derivations involving r u v
rewrite equation inside outside fashion

r u v u v
p q p r

p q yield r

define probability r involved derivations
x
r
r u v



u v

equation sum probabilities r node pairs means
rule probability considered multiple times particular derivations contain
r r fractional count r defined
c r

r
root root





fixiao zhu

equation reflects probability likely r involved derivation given
pair trees set bilingual parse trees c r accumulated tree pair
obviously c r used estimate parameters mt model
translation rules weighted parameter estimation procedure proceed usual
weight counts work c r employed learn five features used
mt decoder including bi directional phrase conditional translation probabilities
marcu et al three syntax conditional probabilities mi huang
let function returns sequence frontier nodes input tree fragment
probabilities computed following equations
p

r sr sr tr tr c r
p
pphrase tr sr


r sr sr c r
p

r sr sr tr tr c r
p
pphrase sr tr


r tr c r
r

c r

p r root r

p

p r sr

p

r root r root r c r

c r
r sr sr

p r tr

c r

c r
p

r tr tr

c r







experiments
evaluation first experimented chinese english sub tree
alignment task tested effectiveness state art tree tree mt system
baselines
three unsupervised sub tree alignment methods chosen baselines experiments
wordalign wordalign ghkm method galley et al
uses word alignments infer syntactic correspondences implementation
giza toolkit grow diag final method used obtain
symmetric word alignment sentence pairs sub tree alignments
heuristically induced selecting node correspondences consistent
word alignment e sub tree alignments violate word
alignments chose method widely adopted modern
tree tree systems
wordalign second baseline essentially wordalign
difference wordalign improved word alignment system
link deletion techniques fossum et al basic idea delete harmful
alignment links initial word alignment e g deleting link
figure experiments considered likely
deletion top common chinese words including


fiunsupervised sub tree alignment tree tree translation

top common english words including

heuristicalgin heuristicalgin implementation proposed
tinsley et al work method alignment confidence every node
pair first computed lexical translation probabilities used obtain
node correspondences via heuristic method require
training process successfully adopted several translation tasks
french english translation chosen another baseline comparison
experimental setup
settings experiments described follows
data preparation
bilingual corpus consists million sentence pairs mentioned
used giza grow diag final heuristics generate best k best word
alignments used baseline word alignment parse trees
chinese english generated berkeley parser publicly available
corpus used evaluate sub tree alignment consists node aligned
sentence pairs gold standard parse trees language sides ldc e
included bilingual data corpus divided two parts
held set used finding appropriate setting hyperparameters sentences
articles test set used evaluating sub tree alignment systems
sentences articles mt experiments gram language model trained
xinhua portion gigaword corpus addition english part ldc
bilingual training data used nist mt evaluation corpus development
set sentences newswire portion nist mt evaluation corpora
test set sentences
sub tree alignment
parameters sub tree alignment model initialized add one smoothing
rule set extracted word alignments e wordalign baseline
model trained parse trees bilingual corpus em
variational bayes vb implementation vb training
hyperparameters assumed share value leads setting
ldc category ldc e ldc ldc e ldc ldc e ldc e
ldc e ldc e ldc e ldc see http www ldc upenn edu
details
note ldc e corpus reused gold standard parse trees provided chinese
english treebanks
available http www nlplab com resources nodealigned bitreebank html
ldc category english gigaword corpus ldc
although could adopt different hyperparameters finer control priors model parameters found setting hyperparameters value could lead satisfactory
performance



fixiao zhu

optimal value held set default trained model em
variational em iterations speed training process avoid degenerate
analysis caused large rules restricted rules reasonable sizes rules five frontier non terminals depth three rules
five frontier non terminals considered tree fragments depth one
restrict number frontier non terminals involved flat tree structures
used associated height one tree fragments besides discarded sub tree
alignment every node pair whose terminals aligned outside corresponding
spans two times wordalign
machine translation
used niutrans open source toolkit xiao zhu zhang li build
tree tree mt system rule extraction used extension ghkm method
extract minimal tree tree transformation rules liu et al obtained larger
rules composing two three minimal rules galley et al used cky style
decoder cube pruning huang chiang beam search decode chinese
sentences default beam size set addition features described
equations used several features mt system including
gram language model rule number bonus target length bonus two binary
features lexicalized rule low frequency rule marcu et al features
combined log linear fashion optimized minimum error rate training mert
och

following part section present experimental including evaluations sub tree alignments extracted rules mt systems
several improved methods effective use tree tree mt
evaluation alignments
first evaluated alignment quality sub tree alignment approaches terms
precision p recall r f score see table three baseline
systems sub tree alignment system measures vb system
significantly improves overall recall f score slightly degrading precision
compared wordalign vb training outperforms em counterpart due priors introduced learning process interesting observation
though em training model suffers degenerate analysis
data extremely bad experiment phenomenon due
restriction size tree fragment training described section
restricted translation rules reasonable size tree fragments several ways e g
let predicted number alignments system output correct number correct alignments
system output gold number alignments gold standard measure precision recall

precisionrecall
correct
f score defined precision predicted
recall correct
f

gold
precision recall
parameter controls preference recall e precision e
nlp tasks set indicating equal weights recall precision



fiunsupervised sub tree alignment tree tree translation

entry
overall
np np
nn nn
vp vp
pu
ip
pu
np nn
np pp
nn nns
nr nnp
nn np
pp pp
nn jj
p
qp np

wordalign
p
r
f

















wordalign
p
r
f

















heuristicalgin
p
r
f

















em
p
r
f

















vb
p
r
f

















table evaluation sub tree alignment system baselines
measures reported percentage
set parameter maximum depth constraints reduce number rules
involved training prevents use rare large rules indicates
fact tree fragment size constraint actually important efficiency
crucial learning discussed previous work without constraints
imposing proper prior solution em degenerate marcu wong denero
gillick zhang klein
addition table shows common types sub tree alignment expected vb system achieves best f score cases
interestingly observed obtains significantly better performance
handling pp prepositional phrase alignment seems difficult
baselines due unclear boundary indicators aligning pp structures attribute
better use syntactic information language sides model
generally ignored traditional surface heuristics word alignments
evaluation extracted rules
applied sub tree alignment tree tree system study impact
sub tree alignment mt discussed section rule extraction downstream
component sub tree alignment current tree tree mt pipeline therefore
chose evaluate quality rules obtained sub tree alignment
determine goodness extracted grammars computed rule precision recall
f scores baseline approaches test set used
best alignment quality evaluation make gold standard grammar chose
method used fossum et al work grammar automatically
generated manually annotated alignment rules extracted
annotated sub tree alignments regarded gold standard computing
evaluation scores table shows evaluation grammars extracted


fimatrix

best

xiao zhu

entry
wordalign
wordalign
heuristicalgin
em
vb
vb pmin
vb pmin
vb pmin
vb pmin






rule p










rule r










rule f










table evaluation rules obtained sub tree alignment approaches
measures reported percentage
different sub tree alignment approaches see improvements persist
sub tree alignments employed translation rule extraction vb
produces grammars higher rule f score three baselines
addition best extraction studied rule extraction behaves
sub tree alignment matrix extraction method table shows
sub tree matrix extraction method different choices pruning parameter
pmin see smaller values pmin grammars higher rule recall
better rule f scores achieved adjusting pmin seeking good balance
rule precision rule recall e g pmin
scores informative measure grammar quality investigated differences rule sets obtained model compared
baseline approaches following levenberg dyer blunsoms method figure
shows probable rules frequency obtained bilingual corpus
vb alignment appear model wordalign
alignment vice versa asked two annotators sub tree alignment estimate
rule quality syntactic correspondence adequacy frontier node sequence
two languages sides rule labeled good judges considered good quality figure see eight top rules extracted
absent wordalign grammar good rules contrast
four top rules baseline model good quality sense human
preference furthermore examined top probable rules appear
two grammars individually top rules extracted proposed model
better quality good rules contrast top ranking
rules induced wordalign alignment good translation rules
evaluation translations
evaluated translations generated mt system different sub tree alignment approaches since vb training shows best performance previous
experiments chose default setting following experiments
table shows evaluation translation quality estimated caseinsensitive ibm version bleu papineni roukos ward zhu ter snover


fiunsupervised sub tree alignment tree tree translation













top highest probability rules mt absent wordalign
np dnp nn vp advp vp vbd improved
np pu np cd nn pu np dt cd two nns sessions
np np qp np nn np np x np cd np nns represents
np nn np vp vb desire nnp
np pu np nr nn pu np np nnp nn independence
np vp dec np nn nn
np adjp adjp jj ideological nn struggle
np np qp np np adjp jj np nn
np np dt jj important nn thinking sbar whnp
np ip deg np nn nn np adjp adjp jj practical
nn significance
np np pu np qp nn pu np adjp nn
np np dt jj nn idea pp np dt cd nns represents
np pu nn pu vp vbg joining np dt nn

top highest probability rules mt wordalign absent

lcp qp lc adjp jj

np dnp nn vp advp vbp changes

np nn nn np cd three nns links x

np dnp ip dec np nn np adjp nn significance

vp advp vp vv np nn nn
vp advp vp vp vv np dt jj mass nn

ip np vp vv np nn nn np np nns pp np

np vp deg nn adjp jj

np np pu nt pu np nn nr
np np prp qp cd nn speech

vp vp advp vp vv cc vv np
vp advp vp vp vb strengthen cc vb improve np
np np pu nn pu np
np np np nn taiwan nn independence nns

figure top highest probability rules built proposed sub tree alignment
wordalign baseline grammar top rules
wordalign baseline grammar obtained proposed
sub tree alignment good translation rule
dorr schwartz makhoul micciula weischedel significance test performed bootstrap resampling method koehn moreover efficiency
rule extraction reported terms rule set size extraction time comparison
report rule extraction word alignment matrices liu et al b
wordalign wordalign
table indicates outperforms baselines bleu ter
measures best best extraction addition matrix method
much efficient k best method example compared best extraction extracting rules sub tree alignment matrices times efficient however
rules counted unit one parameter estimation translation model
alignment matrices significant bleu improvements ter reductions comparison best counterpart see rows marked unitcount
many additionally extracted rules utilized real translation
example observed rules used generating final best


fixiao zhu

entry
wordalign best
wordalign best
heuristicalgin best
wordalign best
wordalign best
heuristicalgin best
wordalign matrix
wordalign matrix
best unitcount
best unitcount
matrix unitcount
best posterior
best posterior
matrix posterior

dev

test

bleu ter

bleu ter





























































rule set
size















efficiency
rule sec
















table evaluation translations different alignment approaches bleu higher
better ter lower better unitcount means take rule
occurrence unit one parameter estimation posterior means use
rule posterior probabilities fractional counts parameter estimation
significantly better three best baselines p
translations indeed extracted alignments seen best
alignments thus indicates fact naively increasing number rules might
effective improving translation quality
last three rows table alignment posterior probabilities
parameter estimation e method described section see alignment
posterior probabilities helpful improving translation quality system
weight rules confidence entries unitcount vs entries
posterior sub tree alignment matrices rule extraction alignment
posterior probabilities parameter estimation finally achieves bleu
improvement ter reduction best case baselines even
outperforms word alignment matrix counterpart bleu points
ter points significant p
effectiveness proposed demonstrated terms bleu
ter scores rule set size figure compares
baseline approaches different numbers unique rules extracted clearly
number unique rules proposed sub tree alignment leads better translations baselines
impact alignment grammar quality mt performance
experiments demonstrate effectiveness proposed terms
different measures individually next natural question sub tree alignment
adjusted pmin obtain grammars different sizes
approaches used different k best lists rule extraction



fiunsupervised sub tree alignment tree tree translation



ter

bleu







heuristicalgin
wordalign
wordalign




heuristicalgin
wordalign
wordalign
































rule set size million

rule set size million

figure bleu ter rule set size
rule extraction affect translation quality study issue important
optimize upstream systems mt decoding select appropriate evaluation
metrics good prediction mt performance
therefore carried another set experiments compares translation
quality different sub tree alignment rule extraction settings generate diverse
sub tree alignment rule extraction varied values pmin
sub tree alignment rule extraction respectively way obtained ensembles
sub tree alignments grammars different precision recall scores chose
f score evaluation metric sub tree alignment system rule
extraction system instead fixing varied value since
parameter control bias towards precision recall choosing different values
helpful seeking good tradeoff precision recall
appropriate evaluation measure sub tree alignment rule extraction predicting
mt performance well
figures plot f scores measures mt performance sub tree alignment rule extraction figure see rule f score correlates best
translation quality measures indicates mt system prefers rule
recall biased metrics agrees observation figure mt system
make benefits rules hand curves figure
better correlation sub tree alignment f f score translation quality measures implying preference relatively higher sub tree alignment recall
reasonable framework node alignment links aligned
tree fragments rules extracted high recall sub tree alignment generally
big grammar high rule recall thus better bleu ter com example larger value generally higher alignment precision small value
prefers higher alignment recall rule extraction larger value pmin generally leads grammar
higher rule precision choosing smaller pmin generate grammar higher rule recall



fixiao zhu



sub tree alignment f

sub tree alignment f







f
f
f
f
f


















f
f
f
f
f









bleu





ter









rule f

rule f

figure bleu ter sub tree alignment f measure




f
f
f
f
f












f
f
f
f
f








bleu







ter

figure bleu ter rule f measure

puted pearsons correlation coefficients sub tree alignment rule f score
bleu ter sub tree alignment f correlation coefficients bleu ter
respectively rule f correlation coefficients bleu
ter respectively good correlations translation quality measures another interesting observation mt performance
sensitive change rule f score change sub tree alignment
f score may lie rule extraction direct upstream step decoding
impacts output mt systems contrast sub tree alignment front end
step mt pipeline indirect effect actual translation process


fiunsupervised sub tree alignment tree tree translation

system
hierarchical phrase
tree best word alignment wordalign

word alignment matrix
tree sub tree alignment matrix

dev

test

bleu ter

bleu ter





















table mt evaluation rules obtained alignment approaches
bleu higher better ter lower better significantly better
hierarchical phrase baseline p
improvements
previous work pointed straightforward implementation tree tree mt
suffers rules derivations rule extraction
decoding process chiang advance tree tree system compare
state art employed tree binarization wang et al b fuzzy
decoding chiang system alignment equipped
general framework tree tree translation trivial conduct another set
experiments investigate effectiveness stronger system table
shows bleu ter scores system enhanced methods
comparison report state art mt system implements
hierarchical phrase model chiang tree tree system extracts
rules word alignment matrices liu et al b table indicates superiority
tree binarization fuzzy decoding involved significantly
outperforms hierarchical phrase system bleu points ter points
tree tree system word alignment matrices bleu points
ter points
discussed section transfer rules sub tree alignment model resembles
general form stsgs directly used mt instead resorting
explicit step rule extraction use rules sub tree alignment model
mt decoding e sub tree alignment cast grammar induction step therefore
built another system directly acquires translation rules sub tree alignment
step need output rules derivation forest generated
alignment model rule probabilities obtained inside output
probabilities pruning performed throwing away rules whose probability
pmin addition rule probability reused n gram language model rule number
bonus target length bonus lexicalized rule low frequency rule indicators
base tree tree system additional features fair comparison obtain good
reasonable employed fuzzy decoding tree binarizaiton experiment
choose setting previous experiments gold standard alignment annotation penn treebank style trees difficult evaluate alignment grammar
quality binarized trees due lack benchmark data experiments first conducted
experiments individual tasks see sections studied correlations simple
reasonable setting consistent sub tree alignment mt see section
investigated effectiveness advanced tree tree system see section
implementation parse trees binarized head fashion



fixiao zhu

entry
baseline explicit rule extraction
rules sub tree alignment model
rules sub tree alignment model mert
baseline sub tree alignment features

dev

test

bleu ter

bleu ter





















table mt evaluation obtaining rules sub tree alignment model
obtaining rules traditional rule extraction pipeline
table compares sub tree alignment matrix rule extraction inducing rules alignment model row vs row unfortunately straightforwardly
inferring rules probabilities sub tree alignment model underperforms
baseline might attributed several reasons first due large derivation
space cannot enumerate relatively large tree fragments sub tree alignment
step instead access tree fragments limited depths contrast baseline system extracts basic rules sub tree alignment matrices obtains
large rules heuristics e g rule composing additional rules obtained
baseline framework rule extraction general useful modern syntax
systems galley et al marcu et al deneefe et al second rule
probability sub tree alignment model defined product probability factors
good generation story however mt systems usually use features required form generative model features shown equations
consequence many well developed features used baseline system
available sub tree alignment model third sub tree alignment model
trained maximizing likelihood criteria consistent
adopted mt system e minimizing evaluation related error rate function
study issues improved system two ways first treated four
probability factors sub tree alignment model see equation different
features mt decoder tuned weights mert row table shows
method achieves better system employing unweighted probability
factors however performance still worse baseline indicates
mt oriented features rule extraction heuristics crucial success
tree tree system finally added probabilistic factors sub tree alignment
model baseline system additional features shown last row table
enhanced system yields modest bleu improvements baseline ter
improvement achieved give us two interesting messages rule
extraction heuristics mt oriented features objectives learning key factors contributing good tree tree system better use sub tree alignment
model upstream module rule extraction decoding rather
simple step grammar induction
last issue investigate whether sub tree alignment model make
benefits labeled data although focus unsupervised learning work
proposed model require strictly unsupervised condition instead
enhanced use labeled data idea simple combine probability
factors sub tree alignment model log linear weighted fashion means


fiunsupervised sub tree alignment tree tree translation

entry
unweighted
weighted weights learned labeled data

dev

test

bleu ter

bleu ter













table comparison unweighted weighted sub tree alignment
probability factors sub tree alignment model taken real valued feature
functions feature weights learned labeled data supervised methods
way unweighted generative model e factor weight one
transformed weighted model e factor individual weight note
weighted model almost form used smt systems
difference smt model language model needed targetlanguage side fixed sub tree alignment step avoid bias towards
many rules added rule number additional feature model
training test divided node aligned gold standard data two parts
first sentences selected weight training remaining sentences
selected testing system learn feature weights supervised manner chose
mert one powerful tools training log linear error
function used mert defined one minus sub tree alignment f score
sentence test data tree annotation penn treebanks
weighted model achieves alignment f score rule f score
respectively better unweighed unsupervised model
obtains alignment f score rule f score
data set finally tested mt performance best setting e sub tree alignment
matrix rule extraction tree binarization fuzzy decoding table shows
weighted sub tree alignment model leads better bleu score tuning set
promising improvements test data size labeled corpus
small expect better labeled data available worth studying
sophisticated supervised methods learn better weights kernel
methods sun et al b supervised semi supervised learning focus
work leave interesting issues future investigations

related work
syntax approaches widely adopted machine translation last
ten years many successful syntactic mt systems developed shown good
several translation tasks eisner galley et al liu et al
huang et al zhang et al liu et al chiang despite
differences modeling implementation details require alignment
step acquire syntactic correspondence source target languages
standard smt syntax mt systems use word alignments infer syntactic
alignments string tree tree tree pairs however word alignments generally
good quality viewpoint syntactic alignment makes sense directly
sub tree aligned data binarized trees reused weights learned penn
treebank style trees four probability factors sub tree alignment model



fixiao zhu

induce sub tree level alignments pairs sentences syntactic information
language side especially true tree tree mt
actually need alignment sub trees two languages rather surface
alignment words several lines work address syntactic alignment
make better use alignment tree tree translation
word sub tree alignment machine translation
earliest efforts syntactic alignment focus enhancing word alignment
syntactic information date several groups fraser marcu denero
klein may knight fossum et al haghighi blitzer denero
klein burkett blitzer klein riesa irvine marcu proposed
syntax augmented advance word alignment systems although
achieved promising improvements still address alignment word level
discussed section methods might desirable choices learning
correspondence tree nodes two languages alternative straightforward solution researchers tried infer sub tree level alignments pairs syntactic
trees example imamura groves hearne way tinsley et al
defined several scoring functions measure similarity source
target sub trees aligned tree nodes greedy approaches
though simple implement derived principled way example
explicit optimization procedure general framework statistical learning instead model parameters obtained additional alignment
lexicons another line sun et al b attempted address sub tree alignment supervised semi supervised used
tree kernels syntactic features advance sub tree alignment system
showed promising chinese english translation tasks however still
relies heuristic inferring node correspondences two parse trees
beyond train tree kernels requires additional labeled data
generally expensive build unlike studies derive sub tree model
principled way develop unsupervised sub tree alignment framework tree tree
mt
unsupervised syntactic alignment
previous studies resort labeled data sub tree alignment
earliest eisners work designed unsupervised
modeling sub tree alignment stsg formalism however since
detailed derivation model decomposition provided model computationally
expensive even difficult applied current tree tree systems complex tree
structures involved gildea applied stsgs tree tree tree string
alignment developed loosely tree alignment method address issue
parse tree isomorphism bitext work targets word alignment rather
modern syntactic mt systems recently nakazawa kurohashi proposed
bayesian sub tree alignment dependency trees tested
japanese english mt system actually model much common model


fiunsupervised sub tree alignment tree tree translation

presented work example apply unsupervised learning methods
bayesian sub tree alignment hand two studies differ
important aspects first nakazawa kurohashi restricted sub tree
alignment dependency trees different aligning tree nodes
phrase structure trees since phrase structure trees involve complex structures
syntactic categories alignment phrase structure trees relatively
difficult dependency counterpart second model makes benefits
recent advances stsgs directly applicable current state art tree totree systems
another related work presented pauls klein chiang knights
work factored node string alignment model components
generates target side synchronous rule source side moreover probability
rule fragment factored lexical structural component work actually
model proposed model two variants theme appear
obvious differences first focus sub tree alignment tree tree
translation pauls et al addressed alignment issue tree string stringto tree translation model parse language sides independently rather
parsing one side projecting syntactic categories inference faster
work since need consider possible parse trees unparsed side
alignment second permutation model presented work general
order handle non itg trees third investigate methods effective use
sub tree alignment mt particular present rule extraction obtaining
additional translation rules sub tree alignment posteriors rather learning rules
best sub tree alignment
rule extraction alignment
machine translation word syntactic alignments used extract translation rules
phrases traditional pipeline rule phrase extraction best alignment considered suffers limited scope single alignment
efficiently obtain diverse alignment parsing packed data structures adopted
improve best pipeline mt systems recent years mi huang liu et al
zhang zhang li aw tan example liu et al b de gispert et al
used alignment posterior probabilities phrase hierarchical phrase extraction
development sub tree alignment matrices actually motivated similar idea
word alignment matrices difference work use sub tree
language sides infer alignment posterior probabilities probabilities
calculated word phrase level previous work liu et al b de gispert et al
moreover knowledge effectiveness sub tree alignment matrix
systematically studied case tree tree translation
note presented work something similar synchronous grammar induction example model stsg
formalism used mt recent studies bayesian blunsom cohn
dyer osborne cohn blunsom levenberg et al shown
promising directly learning synchronous grammars bilingual data hierar

fixiao zhu

chical phrase string tree systems rather extracting synchronous grammar
rules explicit word syntactic alignment step however rare see related
work tree tree mt principle article different previous work synchronous grammar induction example aim work learn sub tree
alignment model applied many potential applications except mt
sentence compression paraphrasing test summarization jing cohn lapata
unlike synchronous grammar induction alignment implicitly encoded
learning process treat sub tree alignment separate task eases
development tuning alignment system actually resort mt
systems slow difficult optimize another advantage
make benefits compact rather used mt great
number rules involved take implementation instance alignment model
learned relatively small set grammar rules rules limited depths
mt system accesses much larger grammar many additional rules involved
rule composing method efficient alignment system likely
alleviate degenerate analysis data cost degrading mt performance

discussion
underlying assumption proposed model sub tree alignments
achieved constraints imposed neighboring parts tree see section
makes sense standpoint linguistically motivated yet turn
faces constraints make difficult align sentences trees correctly
particularly free translations several reasons explain
works nice tree tree mt suffer greatly
structure divergence languages first model flexible allows
node deletion insertion alignment means levels tree
necessary require every node aligned valid node language side
instead nodes dropped needed advantage method
cannot confidently align node node counterpart tree align
virtual node enforce bad constraints aligning parts tree
useful flat tree structures partial translations
syntactically well formed second main purpose infer
sub tree alignment probabilities used pruning sub tree alignment matrices
extracting rules mt systems though alignment required training
sub tree alignment model actually access large number alignment
alternatives rule extraction even cannot appear derivation
due alignment constraints third model work phrase structure
trees instead penn treebank style trees difficult alignment
cases sub tree alignment system works well binarized trees shows promising
improvements baselines note tree binarization effective method
alleviate structure divergence especially chinese english translation
might interesting investigate methods dealing differences
syntactic structures languages forest methods mi huang
liu et al


fiunsupervised sub tree alignment tree tree translation

another note implemented naively speed sub tree alignment system slow since model needs calculation alignment probability pairs tree fragments fortunately thought computation several
optimizations make system much efficient practice first described
work pruning methods employed restrict number tree fragments
reasonable level experiments system pruning achieved speed
sentence second single core intel xeon ghz cpu another way speed
improvement parallel processing good property em style
e step easily implemented parallel computation environment
need divide training data set number smaller parts run
inside outside parts parallel e map procedure expected
counts model parameters accumulated parts e
reduce procedure step performed usual implementation
used threads parallel training running time one training iteration
million sentence corpus hours note system speed
expected powerful distributed infrastructures available e g clusters
hadoop difficult scale handle millions sentence pairs
current training framework

conclusions
proposed unsupervised probabilistic sub tree alignment tree totree translation factoring alignment model several components resulting
model easily learned em variational bayesian
investigated different ways applying proposed model tree tree
translation particular developed sub tree alignment matrix encodes
exponentially large number alignments representation sub tree alignment
desirable rules extracted efficiently k best sub tree alignment
experiments showed proposed model achieved significant improvements
alignment quality grammar quality several baselines nist chineseenglish evaluation corpora achieved bleu improvement ter reduction
top state art tree tree system improved mt system even significantly
outperformed state art hierarchical phrase system equipped tree
binarization fuzzy decoding

acknowledgments
work supported part national science foundation china grants
natural science foundation youth china grant
china postdoctoral science foundation grant specialized fund doctoral program higher education grant
fundamental funds central universities grant n
authors would thank anonymous reviewers pertinent insightful comments keh yih su great help improving early version article ji


fixiao zhu

helpful discussions chunliang zhang tongran liu language refinement
corresponding author article jingbo zhu

appendix part speech tags phrase structure labels
work annotation pos tagging phrase structure parsing follows standard defined penn english chinese treebanks marcus et al xue et al
see tables lists pos tags constituent labels used example
trees article
pos tag
ad

nn
nr
p
pn
pu
vv

description
adverb
aspect particle
noun except proper nouns temporal nouns
proper noun
preposition
pronoun
punctuation
verb except stative verbs copulas main
verbs

table chinese pos tags used examples
pos tag
dt

jj
nnp
nns
prp
rb
vbd
vbn
vbp



description
determiner
preposition
adjective
proper noun singular
noun plural
personal pronoun
adverb
verb past tense
verb past participle
verb non rd person singular present
comma
period

table english pos tags used examples
syntactic label
ip
np
pp
qp
vp

description
single clause
noun phrase
preposition phrase
quantity phrase
verb phrase

table chinese constituent labels used examples



fiunsupervised sub tree alignment tree tree translation

syntactic label
advp
np
pp

vp

description
adverb phrase
noun phrase
preposition phrase
sentence
verb phrase

table english constituent labels used examples
distribution
pnt
ptree
preorder

notation
tnt snt
ttree tnt
tvnt svnt

plength
pw

l
tw sw

description
snt tnt source target language non terminal symbols
ttree target language tree fragment
svnt tvnt vectors non terminal symbols source
target languages
l numbers source target terminals words
sw tw source target terminals words

table notations model parameters

appendix b em training sub tree alignment model
described section proposed sub tree alignment model five types parameters including non terminal mapping probability pnt target language treefragment generation probability ptree frontier non terminal reordering probability
preorder word number probability plength word mapping probability pw
convenience use set notations denote model parameters following description see table symbol list
follow framework em training described figure
see figure complete version em parameters model
ec represents expected count input variable x x
function returns variable x takes value x otherwise k r u v
k rule probability see equation probability subtree alignment see equation k indicates
probabilities calculated parameters k th iteration tree vnt
lex functions return tree structure frontier non terminal vector
terminal sequence input tree fragment respectively see section
basic idea e step check rule r given pair tree nodes u
r u v
v update ec relative probability root root
applied
update rules parameters tnt snt ttree tnt tvnt svnt l see lines
exception tw sw defined equation pw ti sj direct
product factor pinstead use sum terminals source language treefragment e
j pw ti sj follow ibm model make
p lex
update magnitude proportional pw ti sj j r pw ti sj refer reader
brown et al work detailed derivation expected count ibm model
worth noting performs parameter update
different choices u v r e step means rule instance involved
particular derivation one time e g tree fragment appears


fixiao zhu

function trainmodelwithem sn tn





initialize tnt snt ttree tnt tvnt svnt l tw sw
k k

set ec model parameters
e step

foreach tree pair sequence sn tn

foreach node pair u v

foreach rule r rooted u v
k r u v snt u tnt v
k root root



ec tnt snt





ec ttree tnt

k r u v tnt v ttree tree tr
k root root



ec tvnt svnt

k r u v svnt vnt sr tvnt vnt tr
k root root



ec l

k r u v lex sr l lex tr
k root root



foreach word pair sj ti position j lex sr lex tr



k
p
ti sj
sw sj tw ti
r p k
w
j
j

k r u v p lex sw



ec tw sw

k root root

step












foreach non terminal symbol pair snt tnt
k

tnt snt



ec tnt snt
p

nt



ec

nt snt

foreach target non terminal symbol tnt tree fragment structure ttree
ec ttree tnt

k

ttree tnt

p

tree



ec

tree tnt

foreach pair non terminal symbol vectors svnt tvnt
ec tvnt svnt

k

tvnt svnt

p

vnt



ec

vnt svnt

foreach pair word numbers l
k

l



ec l
p

l

ec l



foreach pair words sw tw
k

tw sw

k



ec tw sw
p

w

k

ec



w sw

k

k

k

return tnt snt ttree tnt tvnt svnt l tw sw
figure em training model parameters

different positions update corresponding parameters would carried
multiple times
another note em expected counts parameters
efficiently calculated inside outside probabilities according lines


fiunsupervised sub tree alignment tree tree translation

parameters efficient ways example discussed
section expected count tnt snt obtained without checking individual
rule omit loop r case technique considered
speed sub tree alignment system

references
attias h variational bayesian framework graphical solla
leen k k eds advances neural information processing systems
pp mit press
beal j variational approximate bayesian inference masters
thesis university college london
blunsom p cohn dyer c osborne gibbs sampler phrasal
synchronous grammar induction proceedings joint conference th
annual meeting acl th international joint conference natural
language processing afnlp acl ijcnlp pp suntec singapore
brown p e pietra pietra v j mercer r l mathematics
statistical machine translation parameter estimation computational linguistics

burkett blitzer j klein joint parsing alignment weakly synchronized grammars human language technologies annual conference north american chapter association computational linguistics
hlt naacl pp los angeles california usa
chiang hierarchical phrase model statistical machine translation
proceedings rd annual meeting association computational linguistics acl pp ann arbor michigan usa
chiang hierarchical phrase translation computational linguistics

chiang learning translate source target syntax proceedings
th annual meeting association computational linguistics acl
pp uppsala sweden
chiang knight k introduction synchronous grammars tutorials
st international conference computational linguistics th annual
meeting association computational linguistics coling acl
chiswell hodges w mathematical logic oxford university press
cohn blunsom p bayesian model syntax directed tree string grammar induction proceedings conference empirical methods natural
language processing emnlp pp singapore
cohn lapata sentence compression tree transduction journal
artificial intelligence
das smith n paraphrase identification probabilistic quasi synchronous
recognition proceedings joint conference th annual meeting


fixiao zhu

acl th international joint conference natural language processing
afnlp acl ijcnlp pp suntec singapore
de gispert pino j byrne w hierarchical phrase translation grammars extracted alignment posterior probabilities proceedings
conference empirical methods natural language processing emnlp pp
cambridge usa
dempster laird n rubin maximum likelihood incomplete data via
em journal royal statistical society series b methodological

deneefe knight k wang w marcu syntax mt
learn phrase mt proceedings joint conference empirical methods natural language processing computational natural language
learning emnlp conll pp prague czech republic
denero j gillick zhang j klein generative phrase underperform surface heuristics proceedings workshop statistical machine
translation wmt pp york city usa
denero j klein tailoring word alignments syntactic machine translation proceedings th annual meeting association computational
linguistics acl pp prague czech republic
eisner j learning non isomorphic tree mappings machine translation
companion proceedings st annual meeting association
computational linguistics acl pp sapporo japan
ferguson bayesian analysis nonparametric annals
statistics
fossum v knight k abney syntax improve word alignment
precision syntax machine translation proceedings third workshop
statistical machine translation wmt pp columbus ohio usa
fraser marcu getting structure right word alignment leaf
proceedings joint conference empirical methods natural language
processing computational natural language learning emnlp conll pp
prague czech republic
galley graehl j knight k marcu deneefe wang w thayer
scalable inference training context rich syntactic translation proceedings st international conference computational linguistics
th annual meeting association computational linguistics colingacl pp sydney australia
galley hopkins knight k marcu whats translation rule
susan dumais roukos eds proceedings human language
technology conference north american chapter association computational linguistics hlt naacl pp boston massachusetts usa


fiunsupervised sub tree alignment tree tree translation

gildea loosely tree alignment machine translation proceedings
st annual meeting association computational linguistics acl pp
sapporo japan
groves hearne way robust sub sentential alignment phrasestructure trees proceedings th international conference computational
linguistics coling pp geneva switzerland
haghighi blitzer j denero j klein better word alignments
supervised itg proceedings joint conference th annual
meeting acl th international joint conference natural language
processing afnlp acl ijcnlp pp suntec singapore
huang l chiang better k best parsing proceedings ninth international workshop parsing technology iwpt pp vancouver british
columbia canada
huang l kevin k joshi statistical syntax directed translation
extended domain locality proceedings th conference association
machine translation americas amta pp cambridge massachusetts
usa
imamura k hierarchical phrase alignment harmonized parsing proceedings
th nlp pacific rim symposium pp
jing h sentence reduction automatic text summarization proceedings
th applied natural language processing conference pp
johnson doesnt em good hmm pos taggers proceedings
joint conference empirical methods natural language processing
computational natural language learning emnlp conll pp prague
czech republic
knuth art computer programming fundamental addisonwesley
koehn p statistical significance tests machine translation evaluation lin
wu eds proceedings conference empirical methods
natural language processing emnlp pp barcelona spain
koehn p och f marcu statistical phrase translation proceedings human language technology conference north american
chapter association computational linguistics hlt naacl pp
edmonton canada
levenberg dyer c blunsom p bayesian model learning scfgs
discontiguous rules proceedings joint conference empirical methods natural language processing computational natural language learning
emnlp conll pp jeju island korea
liu gildea bayesian learning phrasal tree string templates proceedings conference empirical methods natural language processing
emnlp pp singapore


fixiao zhu

liu liu q lin tree string alignment template statistical machine
translation proceedings st international conference computational
linguistics th annual meeting association computational linguistics coling acl pp sydney australia
liu lu liu q improving tree tree translation packed forests
proceedings joint conference th annual meeting acl
th international joint conference natural language processing afnlp
acl ijcnlp pp suntec singapore
liu xia xiao x liu q b weighted alignment matrices statistical
machine translation proceedings conference empirical methods
natural language processing emnlp pp singapore
manning c schutze h foundations statistical natural language processing mit press
marcu wang w echihabi knight k spmt statistical machine translation syntactified target language phrases proceedings conference
empirical methods natural language processing emnlp pp sydney
australia
marcu wong phrase joint probability model statistical machine translation proceedings conference empirical methods
natural language processing emnlp pp
marcus p santorini b marcinkiewicz building large annotated
corpus english penn treebank computational linguistics
may j knight k syntactic alignment machine translation
proceedings joint conference empirical methods natural language processing computational natural language learning emnlp conll
pp prague czech republic
mi h huang l forest translation rule extraction proceedings
conference empirical methods natural language processing emnlp
pp honolulu hawaii usa
nakazawa kurohashi bayesian subtree alignment model dependency trees proceedings th international joint conference natural language
processing ijcnlp pp chiang mai thailand
neal r philosophy bayesian inference http www cs toronto edu radford
res bayes ex html
och f minimum error rate training statistical machine translation proceedings st annual meeting association computational linguistics
acl pp sapporo japan
och f ney h alignment template statistical machine translation computational linguistics
papineni k roukos ward zhu w bleu method automatic
evaluation machine translation proceedings th annual meeting


fiunsupervised sub tree alignment tree tree translation

association computational linguistics acl pp philadelphia pennsylvania usa
pauls klein chiang knight k unsupervised syntactic alignment
inversion transduction grammars proceedings human language technologies annual conference north american chapter association
computational linguistics hlt naacl pp los angeles california
usa
riesa j irvine marcu feature rich language independent syntax
alignment statistical machine translation proceedings conference
empirical methods natural language processing emnlp pp edinburgh scotland uk
riley gildea improving performance giza variational
bayes tech rep university rochester
smith eisner j parser adaptation projection quasi synchronous
grammar features proceedings conference empirical methods
natural language processing emnlp pp singapore
snover dorr b schwartz r makhoul j micciula l weischedel r
study translation error rate targeted human annotation tech rep
lamp tr cs tr umiacs tr university maryland college
park bbn technologies
sun j zhang tan c l discriminative induction sub tree alignment
limited labeled data proceedings rd international conference
computational linguistics coling pp beijing china
sun j zhang tan c l b exploring syntactic structural features sub tree
alignment bilingual tree kernels proceedings th annual meeting
association computational linguistics acl pp uppsala sweden
thayer ettelaie e knight k marcu munteanu och f tipu q
isi usc mt system proceedings international workshop spoken language
translation pp
tinsley j zhechev v hearne way robust language pair independent
sub tree alignment proceedings machine translation summit xi pp
copenhagen denmark
venugopal zollmann smith n stephan v wider pipelines n best
alignments parses mt training proceedings eighth conference
association machine translation americas amta pp
vogel ney h tillmann c hmm word alignment statistical translation proceedings rd international conference computational linguistics coling pp
wang smith n mitamura jeopardy model quasisynchronous grammar qa proceedings joint conference empirical methods natural language processing computational natural language
learning emnlp conll pp prague czech republic


fixiao zhu

wang w knight k marcu b binarizing syntax trees improve syntaxbased machine translation accuracy proceedings joint conference
empirical methods natural language processing computational natural
language learning emnlp conll pp prague czech republic
woodsend k lapata learning simplify sentences quasi synchronous
grammar integer programming proceedings conference empirical methods natural language processing emnlp pp edinburgh
scotland uk
xiao zhu j zhang h li q niutrans open source toolkit phrasebased syntax machine translation proceedings th annual meeting association computational linguistics system demonstrations acl
pp jeju island korea
xue n xia f chiou f palmer penn chinese treebank phrase
structure annotation large corpus natural language engineering
zhang h zhang li h aw tan c l forest tree sequence
string translation model proceedings joint conference th annual
meeting acl th international joint conference natural language
processing afnlp acl ijcnlp pp suntec singapore
zhang jiang h aw li h tan c l li tree sequence alignmentbased tree tree translation model proceedings th annual meeting
association computational linguistics human language techonologies
acl hlt pp columbus ohio usa




