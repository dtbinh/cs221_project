Journal Artificial Intelligence Research 48 (2013) 733-782

Submitted 04/13; published 11/13

Unsupervised Sub-tree Alignment
Tree-to-Tree Translation
Tong Xiao
Jingbo Zhu

xiaotong@mail.neu.edu.cn
zhujingbo@mail.neu.edu.cn

College Information Science Engineering
Northeastern University
3-11, Wenhua Road, Heping District
Shenyang, China

Abstract
article presents probabilistic sub-tree alignment model application
tree-to-tree machine translation. Unlike previous work, resort surface heuristics expensive annotated data, instead derive unsupervised model infer
syntactic correspondence two languages. importantly, developed model
syntactically-motivated rely word alignments. by-product,
model outputs sub-tree alignment matrix encoding large number diverse alignments
syntactic structures, machine translation systems efficiently extract translation rules often filtered due errors 1-best alignment.
Experimental results show proposed approach outperforms three state-of-the-art
baseline approaches alignment accuracy grammar quality. applied
machine translation, approach yields +1.0 BLEU improvement -0.9 TER reduction NIST machine translation evaluation corpora. tree binarization
fuzzy decoding, even outperforms state-of-the-art hierarchical phrase-based system.

1. Introduction
Recent years witnessed increasing interest syntax-based methods many Artificial Intelligence (AI) Natural Language Processing (NLP) applications ranging
text summarization Machine Translation (MT). particular, syntax-based models
intensively investigated Statistical Machine Translation (SMT). Approaches include
string-to-tree MT (Galley, Hopkins, Knight, & Marcu, 2004; Galley, Graehl, Knight, Marcu,
DeNeefe, Wang, & Thayer, 2006), tree-to-string MT (Liu, Liu, & Lin, 2006; Huang, Kevin,
& Joshi, 2006) tree-to-tree MT (Eisner, 2003; Zhang, Jiang, Aw, Li, Tan, & Li, 2008;
Liu, Lu, & Liu, 2009a; Chiang, 2010), train tree-string/tree-tree pairs
seek model translation equivalency relations learned parsed data. part
focus syntax-based MT, tree-to-tree models use synchronous context free grammars synchronous tree substitution grammars received growing interest, showing
promising results several well-established evaluation tasks (Zhang et al., 2008; Liu
et al., 2009a; Chiang, 2010). example, recent studies (Chiang, 2010) demonstrated
modern tree-to-tree systems significantly outperform hierarchical phrase-based
counterpart large scale Chinese-English Arabic-English translation.
tree-to-tree MT, translation problem broadly regarded transformation
source-language syntax tree target-language syntax tree. model process,
c
2013
AI Access Foundation. rights reserved.

fiXiao & Zhu

tree-to-tree systems resort general framework synchronous grammars,
pair trees generated derivations synchronous grammar rules (or translation
rules). model, goal translation build underlying derivations
pairs trees output target string encoded likely derivation. Figure
1 shows intuitive example illustrate generation process tree pair using
sample grammar, source target-language sentences associated
phrase structure trees generated using automatic parsers.1
Previous work shown acquisition good translation rules one essential factors contributing success syntax-based systems (DeNeefe, Knight, Wang,
& Marcu, 2007). date, several research groups addressed issue rule acquisition designed effective algorithms extract high-coverage grammars bilingual
parsed data (Zhang et al., 2008; Liu et al., 2009a; Chiang, 2010). Despite differences
detailed modeling, approaches rely syntactic alignments align tree nodes
syntactic parse tree one language tree nodes other, alignments
could employed standard tree-to-tree rule extraction algorithms (Liu et al., 2009a;
Chiang, 2010).
current tree-to-tree models heavily depend syntactic alignments two
languages, alignments induced indirectly word alignments tree-to-tree
systems sensitive word alignment behavior. Unfortunately, word alignments
general far perfect viewpoint syntactic alignment (Fossum, Knight,
& Abney, 2008). cases, even one spurious word alignment prevent large
number desirable rules extraction. example, Figure 2(a) shows tree-totree translation rules extracted using word alignment produced GIZA++.
alignment incorrectly aligns source word (a past tense marker Chinese)
target word the. spurious word alignment produces incorrect rule AS()
DT(the) blocks extraction high-level syntactic transfer rules,
IP(NN1 VP2 ) S(NP1 VP2 ).
Obviously, desirable solution directly infer node correspondences
source target parse trees, namely sub-tree alignment. syntactic parse trees
explain underlying structure sentences well, performing alignment sub-tree level
make benefits high-level structural information syntactic categorization.
example, consider alignment Figure 2(b). links nodes two parse
trees (in Chinese English), rather aligning word level. example,
confident align VP sub-tree (spanning ) source tree
VP sub-tree (spanning drastically fallen) target tree.2 therefore
1. phrase structure tree, leaf nodes words sentence. internal tree nodes followed
leaf nodes labeled Part-Of-Speech (POS) tags, tree nodes labeled syntactic
categories defined treebanks (see appendix meanings POS tags syntactic categories used
work). NLP, many well-developed parsers available automatic parsing. Also, several
good-quality phrase structure treebanks across languages used train parsing models,
Penn English Chinese Treebanks (Marcus, Santorini, & Marcinkiewicz, 1993; Xue, Xia, Chiou,
& Palmer, 2005). Note addition phrase structure syntax, popular formalisms
(e.g., dependency syntax) used syntax-based MT. discussion different formalisms
syntactic parsing beyond scope article. instead focus tree-to-tree MT based
phrase structure trees throughout work.
2. Chinese English follow subject-verb-object structure. verb phrases Chinese
sentence frequently aligned verb phrases English translation.

734

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

NP

VP

NP

VP

PRP

VBD





Target-language Side
(English)



VP
VBN

PP

satisfied
PP


NP



r1

r4

DT

NNS



answers

r2


(ta)


(huida)

P

NN


(biaoshi)


(manyi)

VV

NN

PP

PN

PP

VP

NP

VP

NP

VP

Source-language Side
(Chinese)

r3

(dui)

IP

Synchronous Grammar Used
ID
r1
r2
r3
r4

Source-language Side
NP(PN())
PP(P() NN())
VP(PP1 VP(VV() NN()))
IP(NP1 VP2 )

Target-language Side
NP(PRP(he))
PP(IN(with) NP(DT(the) NNS(answers)))
VP(VBD(was) VP(VBN(satisfied) PP1 ))
S(NP1 VP2 )

Figure 1: Example derivation tree-to-tree translation rules. rules represented
aligned pairs tree-fragments (linked dotted lines). subscripts
language sides grammar rules indicate alignments frontier nonterminals. language side derivation, round-head lines link
frontier non-terminals rewritten translation.
know child nodes source-language VP likely aligned child
nodes target-language VP. means two VPs aligned,
children aligned outside VP sub-tree structure, i.e., prevent
alignment Chinese tree node English tree node DT due
inconsistency VP-VP alignment. case, correctly aligned VBP.
735

fiXiao & Zhu




NP
DT

NP

VP

NNS

VBP

DT

ADVP

imports

RB

VP

NNS

VBP

ADVP

imports

VBN

RB

drastically fallen

drastically fallen








VV



AD

NN

VBN




VP



VV



AD

NN

VP



VP
VP

IP

IP

(Minimal) Rules Extracted

(Minimal) Rules Extracted
r1

AS() DT(the)

r3

AD() RB(drastically)

r2

NN() NNS(imports)

r4

VV() VBN(fallen)

r3

AD() RB(drastically)

r6

AS() VBP(have)

r4

VV() VBN(fallen)

r7

NN() NP(DT(the) NNS(imports))

r5

IP(NN1 VP(AD2 VP(VV3 AS4 )))

r8

VP(AD1 VP(VV2 AS3 ))
VP(VBP3 ADVP(RB1 VBN2 ))

S(NP(DT4 NNS1 ) VP(VBP(have) ADVP(RB2 VBN3 )))

r9

(a) word alignment extracted rules

IP(NN1 VP2 ) S(NP1 VP2 )

(b) sub-tree alignment extracted rules

Figure 2: Tree-to-tree translation rules extracted via word alignment (a) sub-tree alignment (b). dashed lines represent word alignment links, dotted lines
represent sub-tree alignment (or node alignment) links.
result, bad rule AS() DT(the) ruled out, desirable rules
extracted using sub-tree alignment (including desirable rules blocked
Figure 2(a)).
Actually, researchers aware sub-tree alignment problem tried
explore solutions (Tinsley, Zhechev, Hearne, & Way, 2007; Sun, Zhang, & Tan, 2010b,
2010a). example, proposed judge whether two nodes aligned not.
work, alignment confidence first calculated using lexical translation probabilities
classifiers trained labeled data, final alignment determined according
node-level alignment score. However, inference sub-tree alignment approaches
relies heuristic algorithms, models essentially optimized within unified
probabilistic framework.
Moreover, alignment result applied tree-to-tree translation, systems
suffer another problem translation rules extracted using 1-best alignment
(Zhang et al., 2008; Liu et al., 2009a; Chiang, 2010). problem significantly affects
736

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

rule-set coverage rate due alignment errors. simple solution issue use
k-best alignments instead. However, k-best alignments often variations many
redundancies. differ alignment links. obviously inefficient
extract rules similar alignments.
article address sub-tree alignment issue principled way investigate
methods effectively apply sub-tree alignment result tree-to-tree MT. particular,
develop unsupervised approach learning probabilistic sub-tree alignment
model bi-lingual parsed data.
investigate different methods integrating sub-tree alignment tree-to-tree
machine translation. Specifically, develop sub-tree alignment matrix encoding
exponentially large number diverse sub-tree alignments, extract multiple
alternative translation rules using alignment posteriors sub-tree alignment
matrix.
advantages approach three-fold. First, approach rely
heuristic algorithms labeled data. Second, developed sub-tree alignment model
structure model used MT, i.e., based synchronous tree
substitution grammars. means MT systems directly make benefits subtree alignment model, especially rule extraction MT parameter estimation. Third,
accessing sub-tree alignment matrix encodes large number alignments,
efficiently obtain rules often filtered due errors within 1best/k-best alignment result. experiment approach Chinese-English subtree alignment translation tasks. sub-tree alignment, significantly outperforms
three state-of-the-art baselines. machine translation, approach obtains significant
improvements tree-to-tree system rule quality translation quality.
example, yields +1.0 BLEU improvement -0.9 TER reduction NIST MT
evaluation corpora. Finally, system even outperforms state-of-the-art hierarchical
phrase-based system equipped tree binarization (Wang, Knight, & Marcu, 2007b)
fuzzy decoding (Chiang, 2010) techniques.
rest article structured follows. Section 2 briefly introduces subtree alignment task. Section 3 describes unsupervised approach sub-tree alignment.
Then, Section 4 investigates effective methods applying alignment model tree-totree translation. Then, Section 5 presents experimental evaluation approach.
reviewing related work Section 6, interesting issues discussed Section 7.
Finally, article concluded summary Section 8.

2. Problem Statement
general, sub-tree alignment defined task find alignment
nodes tree nodes another tree.3 restrict machine translation article, sub-tree alignment actually task must tightly coupled
specific applications. example, addition machine translation,
3. work term tree refers data structure defined recursively collection nodes
starting root node. node list edges pointing nodes (or children),
constraint edge duplicated points root (Knuth, 1997).

737

fiXiao & Zhu

NLP tasks make benefits sub-tree alignment, including sentence simplification (Cohn & Lapata, 2009; Woodsend & Lapata, 2011), paraphrasing (Das & Smith,
2009), question answering (Wang, Smith, & Mitamura, 2007a), parser adaptation
projection (Smith & Eisner, 2009).
Ideally, would sub-tree alignment system language independent
application independent. Given parallel corpus training examples, able
learn alignment model use infer syntactic correspondence tree
pairs. Broadly speaking, alignments paired linguistic tree structures
regarded instances sub-tree alignment. example, alignment performed
dependency trees (Eisner, 2003; Nakazawa & Kurohashi, 2011) phrase structure
trees (Tinsley et al., 2007; Sun et al., 2010b).
Although sub-tree alignment problem includes number tasks seek alignments syntactic tree structures, particularly interested aligning tree
nodes phrase structure trees work. focus phrase structure sub-tree alignment because: 1) phrase structure parsing one popular syntactic analysis
formalisms. Several state-of-the-art full parsing models/tools developed many
languages; 2) phrase structure trees basis many successful syntax-based MT systems. alternatives, dependency trees, benefit MT systems,
constituency-based models interest relatively larger portion MT community show state-of-the-art performance recent tree-to-tree systems (Zhang et al.,
2008; Liu et al., 2009a; Chiang, 2010).
natural language processing, phrase structure parse tree ordered rooted
tree. represents syntactic structure sentence according phrase structure grammars (or constituency grammars) describe way words combine form
phrases sentences (Chiswell & Hodges, 2007). Generally, phrase structure parse trees
distinguish terminal non-terminal nodes. leaf nodes labeled terminal categories (or words), internal nodes labeled non-terminal categories
grammar (or phrasal categories). example, English parse tree Figure 2(b),
imports terminal, nodes NP NNS two non-terminals indicating
noun phrase plural form nouns respectively.4 following description
experiments, take Penn Treebank standard tree annotation.
choose Penn Treebank one popular tree-annotated corpora
used syntactic parsing good quality quantity several languages,
Chinese English.
Based definition, sub-tree alignment defined alignments
non-terminals source target-language (phrase structure) parse trees.5
formally, given source-language parse tree target-language parse tree ,
sub-tree alignment (denoted A(S, ) short) set node-to-node links
. node pair (u, v) (S, ), good alignment follow three criteria
(Tinsley et al., 2007):
1. u (or v) aligned (indicating 1-to-1 alignment).
4. Note non-terminals always followed leaf nodes called pre-terminals
labeled part-of-speech tags. E.g., NNS node followed terminal node imports
thus pre-terminal.
5. contrast, word alignment regarded alignments terminals two languages.

738

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

2. u aligned v, descendants u aligned descendants v.
3. u aligned v, ancestors u aligned ancestors v.
criteria prevent aligning constituents cross other. property
similar bi-parsing formalisms, synchronous context free
grammars synchronous tree substitution grammars. advantage enables
use powerful synchronous grammars modeling sub-tree alignment problem.
shown next section, based constraints take synchronous
tree substitution grammars basis proposed model.
According Tinsley et al.s (2007) work, alignments satisfying criteria
called well-formed alignments. alignment ill-formed violates
criteria. work focus well-formed alignments. Hence sub-tree alignment
task stated as: given pair parse trees (S, ), search likely wellformed alignment
= arg max P(A | S, )

(1)

A(S,T )

(S, ) set well-formed alignments, P(A | S, ) viewed
alignment model predicts probability every alignment given .
follows, describe approach sub-tree alignment tree-to-tree translation, including alignment model, training inference methods, effective
use model tree-to-tree MT systems.

3. Unsupervised Sub-tree Alignment
section present unsupervised sub-tree alignment model. first define
base model sub-tree alignment framework synchronous tree substitution
grammars, describe model parameterization, training inference methods.
3.1 Base Model
fundamental question sub-tree alignment define correspondence
nodes source-language parse tree nodes target-language parse tree.
address issue using Synchronous Tree Substitution Grammars (STSGs)
widely adopted model transformation process source target-language
parse trees MT (Zhang et al., 2008; Liu et al., 2009a; Chiang, 2010). general
framework STSGs (Chiang & Knight, 2006), assumed pair source
target parse trees simultaneously generated using derivation STSG rules (or
tree-to-tree transfer rules). example, grammar Figure 1 STSG
rules used generate pair sentences. formally, STSG system
hNs , Nt , Ws , Wt , i, Ns Nt sets non-terminals source target
languages, Ws Wt sets terminals (or words) source target languages,
finite set productions. production STSG rewrite rule (denoted r)
pair source target-language non-terminals (snt , tnt ):
hsnt , tnt hsr , tr , r
739

fiXiao & Zhu

sr source-language tree-fragment, whose frontier nodes either words Ws
non-terminals Ns (labeled x); tr corresponding target-language tree-fragment;
r set 1-to-1 alignments connect frontier non-terminals sr
frontier non-terminals tr . example, r5 Figure 2(a),
snt = IP
tnt =
sr = IP(NN:x VP(AD:x VP(VV:x AS:x)))
tr = S(NP(DT:x NNS:x) VP(VBP(have) ADVP(RB:x VBN:x)))
r = {1-2, 2-3, 3-4, 4-1}
Note non-terminals left-hand side rule actually roots
corresponding tree-fragments right hand side. means rule contains
exactly information matter whether root nodes (snt , tnt ) explicitly
represented not. following parts article use hsr , tr , r simpler representation STSG rules. Beyond this, STSG rules written
compact form alignment r encoded numbers assigned frontier
non-terminals sr tr . example, Figures 1 2, subscripts language
sides STSG rules indicate aligned pairs frontier non-terminals.
STSG model, frontier non-terminals called substitution nodes.
applying STSGs, rewrite aligned pair substitution nodes tree-fragment
pair encoded STSG rule. constraint operation labels
substituted non-terminals must match root labels rewrite rules. example,
round-head lines Figure 1 show substitution operations used derivation.
using STSG rules, parse tree pair generate corresponding derivations. generation process trivial: start pair root symbols repeatedly rewrite pairs non-terminal symbols using STSG rules. example, tree pair
Figure 2(b), start root labels source target-language parse trees
(the superscript indicates node index tree)
h IP[1] , S[1]
apply rule r9 .
IP[1] S[1]

h IP(NN[2] VP[3] ), S(NP[2] VP[3] )
r9

IP[1] S[1]

represents operation rewrites aligned node pair IP[1]
r9

S[1] r9 (denoted IP[1] S[1] ). process proceeds repeatedly rewriting
remaining frontier non-terminals get complete source target-language trees,
so:
740

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

NN[2] NP[2]


r7

VP[3] VP[3]


r8

h IP(NN() VP[3] ), S(NP(DT(the) NNS(imports)) VP[3] )
h IP(NN() VP(AD

[4]

[5]

VP(VV

AS[6] ))),

S(NP(DT(the) NNS(imports)) VP(VBP
AD[4] RB[4]


r3

h IP(NN() VP(AD() VP(VV

[5]

S(NP(DT(the) NNS(imports)) VP(VBP
[5]

VV

[6]

ADVP(RB

[4]

VBN[5] )))

AS[6] ))),

[6]

ADVP(RB(drastically) VBN[5] )))

[5]

VBN

h IP(NN() VP(AD() VP(VV() AS[6] ))),
r4

S(NP(DT(the) NNS(imports)) VP(VBP

[6]

ADVP(RB(drastically) VBN(fallen))))
AS[6] VBP[6]


r6

h IP(NN() VP(AD() VP(VV() AS()))),
S(NP(DT(the) NNS(imports)) VP(VBP(have)
ADVP(RB(drastically) VBN(fallen))))

process, rewrite rule indicates node alignment. importantly,
derivations model two nice properties: first, node u
source-language (or target-language) parse tree, one node targetlanguage (or source-language) parse tree aligned u; second, hierarchical
structure behind alignment avoids links constituents cross other.
Consequently, well-formed sub-tree alignment A, always find derivation
encodes alignment A. means sub-tree alignment problem essentially
problem finding likely STSG derivation. Thus sub-tree
alignment task (see Equation (1)) restated finding likely derivation
given pair parse trees.
model derivation probability, follow formulation adopted statistical
word alignment (Brown, Pietra, Pietra, & Mercer, 1993; Vogel, Ney, & Tillmann, 1996).
transformation source-language tree target-language tree described
following equation.
X
P(T | S) =
P (T, d|S)
(2)
dD(S,T )

D(S, ) set derivations transforming (say, aligning nodes
nodes ), P (T, d|S) probability transforming using
derivation D(S, ), parameters model. use notation
P () express dependence model parameters. general, optimal
value learned parsed parallel data training criteria. example,
context unsupervised learning, optimize model parameters maximizing
probability observed data (known maximum likelihood training).
Given set optimal parameters , best sub-tree alignment (S, ) determined
P (T,d|S)
choosing derivation P (d | S, ) greatest. Since P (d | S, ) = P (T |S)


741

fiXiao & Zhu

P (T | S) constant given (S, ) , finding best derivation
finding derivation make P (T, | S) large possible. Hence reach
fundamental equation sub-tree alignment.
= arg max P (T, | S)

(3)

dD(S,T )

formulation implies three fundamental issues sub-tree alignment, including
modeling derivation probability (i.e., P (T, d|S)), learning model parameters (i.e., )
finding best alignment given learned model (i.e., arg max operation).
following parts section, describe solutions issues.
3.2 Parameterization
simplest case, alignment model one parameter instance derivation.
However, model would unmanageable set parameters since number
derivations exponential length input sentences. choose simple
solution issue decomposes base model product trainable submodels. start assumption rules conditionally independent given
source-language parse tree S, probability P(T, | S) defined product
rule probabilities (for conciseness, drop subscript on).

P(T, | S)
P(r | S)
(4)
rd

Nevertheless complex tree-to-tree mappings still result extremely large number
rules, causes computational problem degenerate analysis
data.6 control number parameters reasonable level, decompose
rule probability simpler probability factors independence assumptions.
First assume generation rule r independent input tree S,
conditioned source-language side rule, is,
P(r | S) P(r | sr )

(5)

Note strong assumption generation synchronous grammar
rule depends source-language side. similar used statistical
modeling machine translation (Brown et al., 1993; Koehn, Och, & Marcu, 2003; Galley
et al., 2004; Chiang, 2005) generation atomic alignment/translation units
conditioned associated source-language words tree-fragments, rather
whole input sentence tree. SMT, independence assumptions based phrases
translation rules generally used decompose parallel corpus manageable units
parameter estimation. successfully used modern SMT
systems, adopt similar assumption ease parameter estimation process
model.
decompose P(r | sr ) additional assumptions. Since r = hsr , tr , r i,
P(r | sr ) written another form using chain rule:
6. degenerate analysis refers case using models complex results overfitting
poor generalization ability unseen data.

742

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

P(r | sr ) = P(sr , tr , r | sr )
= P(r | sr , tr ) P(tr | sr )

(6)

Equation (6) indicates two sub-models, including reordering model frontier nonterminals P(r | sr , tr ), tree-fragment translation model P(tr | sr ).
model P(r | sr , tr ), view frontier non-terminal reordering problem aligning
elements two vectors non-terminals. Let vnt() function returns
vector leaf non-terminals given tree-fragment. r defines 1-to-1 alignment
non-terminals vnt(sr ) vnt(tr ). example, r5 Figure 2(a),
frontier non-terminal vectors sr5 tr5 are:
vnt(sr5 ) = (NN, AD, VV, AS)
vnt(tr5 ) = (DT, NNS, RB, VBN)
r5 = {1-2, 2-3, 3-4, 4-1} indicates alignment vnt(sr5 ) vnt(tr5 ), say,
NN aligned NNS, AD aligned RB on. opt simple model
selecting r . models non-terminal reordering probability condition
frontier non-terminal vectors language sides, follows7
P(r | tr , sr ) Preorder (r | vnt(sr ), vnt(tr ))

(7)

turn problem modeling tree-fragment translation P(tr | sr ) (i.e.,
second sub-model defined Equation (6)). define tree-fragment consists
two parts: words lex() (i.e., terminals ) tree structure tree() without lexicons
involved. example, r5 Figure 2(a), target-language tree-fragment contains
two elements lex(tr5 ) tree(tr5 ):
lex(tr5 ) =
tree(tr5 ) = S(NP(DT:x NNS:x) VP(VBP ADVP(RB:x VBN:x)))
Let root() function returns root given tree-fragment. write
P(tr | sr ) as:
P(tr | sr ) = P(lex(tr ), tree(tr ) | sr )
= P(root(tr ) | sr )
P(tree(tr ) | root(tr ), sr )
P(lex(tr ) | tree(tr ), root(tr ), sr )

(8)

worth noting Equation (8) approximation. choose
one many ways P(tr | sr ) written product series
7. reordering model defined ensures arbitrary 1-to-1 alignments handled. might
result large model sparse parameter distributions big tree-fragments involved.
considering issue, choose several pruning methods better control rule size sub-tree
alignment system. See Section 5.2.2 pruning settings work.

743

fiXiao & Zhu

conditional probabilities. simply assert equation generating targetlanguage tree-fragment source-language tree-fragment, first choose root
symbol target-language tree-fragment given source-language tree-fragment (in
probability P(root(tr ) | sr )). choose tree-structure target-language
tree-fragment given root symbol source-language tree-fragment (in probability
P(tree(tr ) | root(tr ), sr )). choose target-language terminals associated
tree-fragment given target-language tree-structure, target-language root symbol
source-language tree-fragment (in probability P(lex(tr ) | tree(tr ), root(tr ), sr )).
Another note Equation (8) actually reduce model complexity.
example, P(lex(tr ) | tree(tr ), root(tr ), sr ) essentially indicates combinations source
target-language tree-fragments. simpler model required feasible solution
parameter estimation. this, introduce additional assumptions relax
conditions probabilities reduce number parameters reasonable level.
1. P(root(tr ) | sr ) depends root(sr ), i.e.,
P(root(tr ) | sr ) Pnt (root(tr ) | root(sr ))

(9)

assumption implies node correspondence source targetlanguage parse trees.
2. P(tree(tr ) | root(tr ), sr ) depends root(tr ), i.e.,
P(tree(tr ) | root(tr ), sr ) Ptree (tree(tr ) | root(tr ))

(10)

second assumption results monolingual model generating target-language
tree-structures, generation tree-fragment conditioned
root. viewed analogy generative model used standard TSGs.
3. P(lex(tr ) | tree(tr ), root(tr ), sr ) depends source words lex(sr ), i.e.,
P(lex(tr ) | tree(tr ), root(tr ), sr ) Plex (lex(tr ) | lex(sr ))

(11)

allows us directly model terminal correspondence two languages.
Then, substitute Equations (9)-(11) Equation (8), get
P(tr | sr ) Pnt (root(tr ) | root(sr ))
Ptree (tree(tr ) | root(tr ))
Plex (lex(tr ) | lex(sr ))
using Equations (6), (7) (12), Equation (4) finally written as:
744

(12)

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

id

rule

probability

r3

AD() RB(drastically)

Pnt (RB | AD) Plex (drastically | )

r4

VV() VBN(fallen)

Pnt (VBN | VV) Plex (fallen | )

r6

AS() VBP(have)

Pnt (VBP | AS) Plex (have | )

r7

NN()

Pnt (NP | NN) Plex (the imports | )

NP(DT(the) NNS(imports))

Ptree (NP(DT NNS) | NP)

VP(AD1 VP(VV2 AS3 ))

Pnt (VP | VP) Ptree (VP(VBP ADVP(RB VBN)) | VP)

VP(VBP3 ADVP(RB1 VBN2 ))

Preorder (1-2, 2-3, 3-1 | (AD, VV, AS), (VBP, RB, VBN))

IP(NN1 VP2 ) S(NP1 VP2 )

Pnt (S | IP) Ptree (S(NP VP) | S)

r8
r9

Preorder (1-1, 2-2 | (NN, VP), (NP, VP))

Table 1: Rule probabilities sample derivation = {r3 , r4 , r6 , r7 , r8 , r9 } Figure 2(b)
P(T, | S)



Pnt (root(tr ) | root(sr ))

rd

Ptree (tree(tr ) | root(tr ))
Plex (lex(tr ) | lex(sr ))
Preorder (r | vnt(sr ), vnt(tr ))

(13)

simplified model generative story described section. takes
rule generation probability product four probability factors: Pnt () nonterminal mapping probability, roughly captures syntactic correspondence subtrees two languages; Ptree () probability generating tree structure
; Plex () probability terminal mappings two language sides
rule; Preorder () probability frontier non-terminal reordering encoded
rule. See Table 1 rule probabilities sample derivation.
model parameters assumed multinomial distributions. calculation Pnt (), Ptree () Preorder () straightforward: directly used
without decompositions assumptions. calculate Plex (), choose
form adopted popular models word alignment (Och & Ney, 2004; Thayer, Ettelaie, Knight, Marcu, Munteanu, Och, & Tipu, 2004), probability defined
product word-based translation probabilities:
l


1 X
Plex (t1 ...tl | s1 ...sm ) Plength (l | m)
Pw (ti | sj )

i=1

(14)

j=1

ti target word, sj source word. Plength () used control number
target words produced given number source words. Pw () word translation
probability. sub-model principle something rather similar conventional
word-based translation tables, IBM Models (Brown et al., 1993).
3.3 Node Deletion Insertion
Word (or sub-tree) deletion/insertion common real-world alignment translation
tasks. add flexibility modeling problem, allow production empty
745

fiXiao & Zhu

sub-trees either source target-language side rule model. formally,
rule whose target-language side empty sub-tree, probability defined as:
P(r | S) Pnt (root() | root(sr ))
Ptree (tree() | root())
Plex (lex() | lex(sr ))
Preorder ( | vnt(sr ), vnt())

(15)

special symbol indicates nothing. Factors Pnt (root() | root(sr ))
Plex (lex() | lex(sr )) model deletion probability different levels tree-fragment.
Ptree (tree() | root()) probability generating empty tree-fragment. Factor
Preorder ( | vnt(sr ), vnt()) regards special reordering pattern aligns
frontier non-terminals source side virtual node NULL. Obviously, values
Ptree (tree() | root()) Preorder ( | vnt(sr ), vnt()) simply 1.
Similarly, rule whose source side empty sub-tree, probability defined as:
P(r | S) Pnt (root(tr ) | root())
Ptree (tree(tr ) | root(tr ))
Plex (lex(tr ) | lex())
Preorder ( | vnt(), vnt(tr ))

(16)

value Preorder ( | vnt(), vnt(tr )) 1.
worthwhile note word deletion insertion problems important
MT spite relatively less discussion recent studies tree-to-tree translation.
actually analogy NULL-alignment used IBM Models
(Brown et al., 1993). word/phrase-based models, removing words alignment
leave space correctly aligning words sentence.8 even
necessary (1-to-1) sub-tree alignment alignment respect syntactic
constraints language sides, e.g., sub-tree alignments allowed break
constraints imposed neighbouring parts tree. cases, cannot obtain
correct 1-to-1 alignment tree pair due one two bad nodes
necessarily aligned valid node counterpart tree. Instead, nodes
skipped alignment thus impose bad constraints parts
tree node deletion/insertion allowed. especially true align sentence
pairs flat tree structures free translations. work found node
deletion insertion operations necessary achieve satisfactory sub-tree alignment
result. therefore used implementation default.
3.4 Training
turn training problem. discussed Section 3.1, focus unsupervised
learning model parameters, is, optimal values parameters estimated given
8. Note current phrase-based approaches (Koehn et al., 2003; Och & Ney, 2004) allow NULL-aligned
words appear boundary phrase, viewed way implicit modeling
word insertion/deletion problem

746

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

collection tree pairs without annotation sub-tree level alignment. work
choose two approaches estimating parameters sub-tree alignment model, including
Maximum Likelihood Estimation (MLE) approach Bayesian approach.
3.4.1 Maximum Likelihood Training
MLE one popular methods parameter estimation statistical models.
basic idea that, given model set parameters, MLE method selects values
parameters generate distribution gives highest probability observed
data. MLE general approach parameter estimation widely adopted
many AI NLP tasks, part-of-speech tagging. case sub-tree alignment,
MLE simply described finding optimal values parameters lead
maximum probability aligning tree nodes source-language parse tree
target-language parse tree. formally, given set tree pairs {(S1 , T1 ), ..., (Sn , Tn )},
objective MLE-based training defined be:
= arg max


n


X

P (Ti , | Si )

(17)

i=1 dD(Si ,Ti )

choose Expectation Maximization (EM, Dempster, Laird, & Rubin, 1977)
algorithm solve optimization problem. Basically EM algorithm
iterative training method finding maximum likelihood estimates model parameters,
assumed observed data depends latent variables. algorithm
performs iteratively calling two sub-routines, namely Expectation (E)-step
Maximization (M)-step. E-step, calculates expected value likelihood
function associated parameters observed data, respect distribution
latent variables given observed data current estimates parameters.
M-step, seeks parameters maximize expected likelihood found
E-step.
applying EM algorithm case, view input pairs parse
trees {(S1 , T1 ), ..., (Sn , Tn )} observed data, underlying derivations rules latent
variables, distributions Pnt (), Ptree (), Preorder () Plex () (i.e., Plength ()
Pw ()) unknown parameters. See Figure 3 pseudo-code training algorithm
Pnt () (denoted tnt |snt ). algorithm directly applicable estimation
parameters model, skip description learning remaining parameters
here. detailed description EM-based training model parameters
refer reader appendix.
algorithm, snt tnt represent source-language non-terminal symbol
target-language non-terminal symbol, u v represent source-language tree node
target-language tree node, EC() represents expected count given variable,
P(k) (T, | S) represents derivation probability based parameters obtained
k-th round. EM iteration, E-step algorithm accumulates expected
count pairs parse trees. Then, M-step finds maximum likelihood estimate
using quantity. nontrivial part algorithm computation
expected count E-step. Roughly speaking, physical meaning right-hand
side line 9 relative probability derivation contains rule r (with root node
747

fiXiao & Zhu

1: Function TrainModelWithEM ({(S1 , T1 ), ..., (Sn , Tn )})
(0)
2: Set {tnt |snt } = initial model
3: k = 0 K 1
4:
Foreach non-terminal symbol pair (snt , tnt )
5:
EC(tnt |snt ) = 0
6: E-step:
6:
Foreach tree pair (S, ) sequence {(S1 , T1 ), ..., (Sn , Tn )}
7:
Foreach node pair (u, v) symbol pair (tnt , snt ) (S, )
8:
Foreach rule r rooted
P (u, v)
P

rooted (u,v)
9:
EC(tnt |snt ) + = d: rd r P
0
d0 P (k) (T,d |S)
8: M-step:
10:
Foreach non-terminal symbol pair (snt , tnt )

11:
12:

(k+1)

tnt |snt =

P

(k) (T,d|S)

EC(tnt |snt )
EC(t0 |s )

t0nt

nt

nt

(K)

return {tnt |snt }
Figure 3: EM-based training algorithm (for Pnt ())

P
pair (u, v)). numerator d: rd r rooted (u,v) P(k) (T, | S) probability sum
P
derivations involve r , denominator d0 P(k) (T, d0 | S)
overall probability alignment . However, brute-force computation
expected counts inefficient requires sum possible derivations
whose number exponential length input sentences.
work use bilingual version inside outside probabilities (Manning
& Schutze, 1999) avoid naive enumeration possible derivations computing
various probabilities. inside probability (u, v) (denoted (u, v)) measures
likely generate sub-tree pair inside node pair (u, v). outside probability
(denoted (u, v)) dual inside probability. measures likely generate
remaining parts tree pair (S, ) start symbols. formulation
used monolingual parsing (Manning & Schutze, 1999), (u, v) (u, v) defined
using following recursive forms:


X

(u, v) =
P(r | S)
(p, q)
(18)
r:root(r)=(u,v)

(u, v) =

X

(p,q)yield(r)



(root(r)) P(r | S)

r:(u,v)yield(r)



(p, q)



(19)

(p,q)yield(r)
(p,q)6=(u,v)

root(r) abbreviation node pair (root(sr ), root(tr )), yield(r)
set aligned frontier non-terminal pairs yielded r. Based recursive
definitions, (u, v) (u, v) efficiently computed dynamic programming.
using inside outside probabilities, easy address computation
problem mentioned above. Let (u, v) denote probability tree node u aligned
tree node v. probability expressed inside-outside fashion:
748

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

(u, v) =

X

P(T, | S)

d: (u,v)
aligned

= (u, v) (u, v)

(20)

way, overall alignment probability (i.e., denominator
right-hand side line 9) simply written as:
X

P(T, | S) = (root(S), root(T )) (root(S), root(T ))

(21)



numerator right-hand side line 9, let us view another angle.
E-Step algorithm, expected count accumulated rules whose root (u, v).
rules rooted (u, v) indicate node alignment u v, lines
8-9 principle imply probability derivations aligning u v, precisely
node alignment probability (u, v). probability written simple
form using inside outside probabilities:
X
X
P(T, | S) = (u, v)
r: r rooted d: rd
(u,v)

= (u, v) (u, v)

(22)

Together result Equation (21), E-step efficiently implemented
replacing lines 8-9 following equation
EC(tnt |snt ) + =

(u, v) (u, v)
(root(S), root(T )) (root(S), root(T ))

(23)

(snt , tnt ) symbol pair (u, v). Note (snt , tnt ), E-step step
(u,v)(u,v)
increases EC(tnt |snt ) sum (root(S),root(T
))(root(S),root(T )) node pairs (u, v)
whose symbols snt tnt . means (snt , tnt ) aligned different positions
input tree pair, method considers alignment (snt , tnt ) multiple
times updates EC(tnt |snt ) accordingly.
worth noting several methods initializing model parameters EM-style training begins. example, model initialized
uniform random distributions. work initialize parameters sub-tree
alignment model model obtained using word alignment result. standard way adopted many unsupervised models simpler model used good
starting point training process. helpful optimization procedure
sensitive initial setting model parameters (e.g., EM non-convex objective functions). experiments found using GIZA++ word alignment parameter
initialization resulted better performance fewer iterations convergence
uniform initial distributions. word alignment obtained unsupervised
manner, change training condition approach. Thus chose
method initializing model parameters implementation.
749

fiXiao & Zhu

3.4.2 Bayesian Approach
MLE one standard approaches training unsupervised models, well
known tendency overfit data. overfitting problem becomes severe
complex models since parameters fit training data better.
case STSGs, likely result degenerate analysis data, i.e., rare
big rules dominate ML solution STSGs, considered noisy
generalize poorly unseen data (Cohn & Blunsom, 2009; Liu & Gildea, 2009).
natural solution problem incorporate constraints proper priors
training process. take Bayesian approach alternative solution
training problem.
Unlike MLE, Bayesian approach plug single optimum point estimate
parameter distribution data point, instead account uncertainty
value parameter. Bayesian models, parameters assumed
drawn probability distributions priors. parameters extra prior
distributions called hyperparameters, denoted . parameters
model viewed mathematically multinomials, choose Dirichlet distributions
(Ferguson, 1973) prior model parameters. advantage using Dirichlet
distributions conjugate multinomial distributions inference
priors easier.
Following previous description, use denote model parameters
multinomial outcomes {1, ..., K} (i.e., k probability outcome k {1, ..., K}).
multinomial distribution sample set outcomes {x1 , ..., xn } probability
P(xi = k) = k . Dirichlet prior distribution multinomials, sample
prior actually set parameter values . Therefore distribution
modeled as:
xi | Multinomial()

(24)

| Dirichlet()

(25)

Equation (24) means xi distributed according multinomial parameters
. Similarly, Equation (25) read distributed according Dirichlet distribution parameters . = {1 , ..., K } hyperparameter vector corresponding
outcomes. work use symmetric Dirichlet prior ( i.e., 1 , ..., K share
value), use represent single hyperparameter instead hyperparameter
vector.
Using model, compute conditional distribution new observation
xn+1 given previous observations {x1 , ..., xn } hyperparameter , follows:
Z
P(xn+1 | x1 , ..., xn , ) = P(xn+1 | x1 , ..., xn , ) P( | )
(26)
big advantage Bayesian approach introduce prior distribution
unknown parameters model, meant capture knowledge beliefs
model seeing data (Neal, 1998). especially important case
need bias towards preferred situations. example, expect
model favor high frequency rules dislike rare big rules. goal
750

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

easily achieved using Bayesian approach appropriate choice priors, say,
Dirichlet prior low concentration parameter . However, introduction priors
generally makes intractable estimate posterior analytically. practical systems
based Bayesian approach, widely-used solution use approximate methods
seek compromise exact inference computational resources. work
choose Variational Bayes approximate inference. Variational Bayes good method
preserves benefits introducing prior tractable inference procedure
(Attias, 2000; Beal, 2003). successfully applied several NLP-related models,
Hidden Markov Models (HMMs) IBM Models (Beal, 2003; Riley & Gildea,
2010). One good thing Variational Bayes seen extension
EM algorithm resembles usual forms used EM. resulting procedure looks lot
EM algorithm modified M-step, convenient implementation.
follow approach presented previous work (Beal, 2003; Riley & Gildea, 2010)
variational Bayesian algorithms applied similar tasks. need
slight change M-step original EM algorithm presented Section 3.4.1.
original EM algorithm (see Figure 3), M-step normalizes expected counts
collected E-step standard MLE. variational Bayesian version M-step
slightly modifies formula performs inexact normalization passing counts
function f (x) = exp((x)).

tnt |snt =

f (EC(tnt |snt ) + )
P
f ( t0 (EC(t0nt |snt ) + ))

(27)

nt

(x) digamma function (Johnson, 2007). approximate effect
subtracting 0.5 argument. choice controls behavior estimation.
set low value, performs estimation way anti-smoothing.
0.5 subtracted rule counts, small counts corresponding rare events
penalized heavily, large counts corresponding frequent events affected
much. example, low values make Equation (27) favor non-terminal pairs
aligned frequently distrust non-terminal pairs aligned rarely.
way, variational Bayesian method could control overfitting caused abusing
rare events. hand, larger used smoothing required.
method applicable training parameters model.
requires replacement M-step Figure 3 variational Bayesian M-step (as
Equation (27)). implementation, variational Bayes-based training,
perform additional round normalization without variational Bayes normalize rule
probabilities sum one.9
9. additional normalization process makes posterior probabilities directly comparable
obtained training methods, EM-based training. Note convert result
Bayesian inference probability distributions good explanation various probability
factors model. hand, technical trick results pseudo-Bayesian procedure
Bayesian inference exactly though shows good results empirical study. One
remove additional round normalization pure Bayesian approach. changes
affect overall pipeline approach (from practical standpoint).

751

fiXiao & Zhu

1: Function Decode
(S, )

2:
[], [] = GetInsideOutsideProbabilities (S, )
3:
Foreach node u bottom-up order
4:
Foreach node v bottom-up order
5:
[u, v] = [u, v] [u, v]
6:
Foreach tree-fragment sr rooted u
7:
Foreach tree-fragment tr rooted v
8:
Foreach frontier non-terminal alignment sr ts
9:
r = CreateRule(s
Q r , tr , a)
10:
score = P(r | S) (p,q)yield(r) P(d[p, q])
11:
score > P(d[u, v])
12:
d[u, v] = CreateDerivation(r, {d[p, q] : (p, q) yield(r)})
13: return (d[], [])
14: Function GetInsideOutsideProbabilities (S, )
15: Foreach node u bottom-up order
16:
Foreach node v bottom-up order
17:
Set [u, v] according Equation (18)
18: Foreach node u top-down order
19:
Foreach node v top-down order
20:
Set [u, v] according Equation (19)
21: return ([], [])
Figure 4: Decoding algorithm proposed sub-tree alignment model 1-best
posterior-based outputs
3.5 Decoding
Inference model straightforward. simplest case inferring 1-best subtree alignment. Given set learned parameters, first visit every node pair (u, v)
bottom-up fashion, compute posterior probability aligning sub-tree pair
rooting (u, v). procedure dynamic program used trainer.
select derivation maximum sub-tree alignment probability
input tree pair. Also, generate list k-best derivations similar manner.
addition 1-best/k-best output, model able output alignment
posterior probability every pair tree nodes. this, need record
probability (u, v) node pair obtain inside outside probabilities.
Note outputting alignment posterior probabilities commonly used statistical
word phrasal aligners. provides flexible way making use alignment result
downstream components, rule extraction system. presented
next sections, tree-to-tree MT systems make great benefits posteriorbased alignment output, results effective rule extraction method well
better translation results.
Figure 4 depicts pseudo-code decoding algorithm 1-best posteriorbased outputs. algorithm, d[x, y] data structure records best derivation rooted (x, y). [x, y], [x, y] [x, y] data structures record inside
probabilities, output probabilities alignment posterior probabilities, respectively. Cre752

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

ateRule() creates rule pair tree-fragments (sr , tr ) frontier non-terminal
alignment a, calculates rule probability. CreateDerivation() builds derivation
using input rules. output, access 1-best alignment traversing
d[root(S), root(T )], access alignment posterior [].
Given pair trees (S, ), outer two loops algorithm iterates pair
nodes two trees, resulting time complexity O(|S| |T |) | | represents
2 ), N
size input tree. Generating pairs tree-fragments requires O(Ntree
tree
maximum number tree-fragments given tree node. Computing alignment
(sr , tr ) requires O(L!) L maximum number leaf non-terminals
2
rule. Therefore time complexity algorithm O(|S| |T | Ntree
L!), quadratic
size input trees. Note actual time complexity algorithm could
high potential alignments considered. example, Ntree generally
exponential function depth input tree-fragment, deep tree could
results extremely large space alignments. make practical sub-tree alignment
systems, pruning techniques taken account work. example,
implementation, restrict depth tree-fragment reasonable number (see Section
5.2.2). addition, commonly-used phrasal alignment related tasks, consider
word alignments pruning discard sub-tree alignments violate certain
number word alignments. example, throw away sub-tree alignments
two word alignment links outside spans covered aligned sub-trees.

4. Applying Sub-tree Alignment Tree-to-Tree Translation
sub-tree alignment obtained, current tree-to-tree systems directly learn translation rules node-aligned tree pairs. section investigate methods applying
sub-tree alignment tree-to-tree rule extraction.
4.1 Rule Extraction Using 1-best/k-best Sub-tree Alignments
data, several methods developed tree-to-tree rule extraction (Zhang et al.,
2008; Liu et al., 2009a; Chiang, 2010). popular GHKM-like
method extends idea extracting syntactic translation rules string-tree pairs
(Galley et al., 2004). GHKM-like extraction, first compute set minimallysized translation rules explain mappings source-language tree
target-language tree respecting alignment reordering
two languages. Larger rules learned composing two minimal rules.
example, Figure 2(b), r7 r9 two minimal rules extracted according sub-tree
alignment. compose rules form larger rule, this:
IP(NN() VP1 ) S(NP(DT(the) NNS(imports)) VP1 )
work use tree-to-tree version GHKM-like extraction described
Liu et al.s (2009a) work. See Figure 5(a) pseudo-code rule extraction
1-best sub-tree alignment. choose method widely used
tree-to-tree systems. Note rule extraction tree-to-tree translation generally
restricted performed 1-best sub-tree alignment result. GHKM-like
753

fiXiao & Zhu

1: Function OneBestExtract (S, , A)
2:
Foreach node u
3:
Foreach node v
4:
Foreach tree-fragment pair (sr , tr )
4:
rooted (u, v)
5:
= OneToOneAlign(sr , tr , A)
6:
empty
7:
r = CreateRule(sr , tr , a)
8:
rules.Add(r)
9:
return rules
10: Function OneToOneAlign(sr , tr , A)
11: frontier non-terminals (sr , tr )
11:
1-to-1 alignments
12:
return frontier alignment (sr , tr )
13: Else
14:
return

1: Function MatrixExtract (S, , )
2:
Foreach node u
3:
Foreach node v
4:
IsExtractable({(u, v)}, )
5:
next loop
6:
Foreach tree-fragment pair (sr , tr )
6:
rooted (u, v)
7:
Foreach frontier alignment
7:
(sr , tr )
8:
IsExtractable(a, )
9:
r = CreateRule(sr , tr , a)
10:
rules.Add(r)
11: return rules
12: Function IsExtractable(a, )
13: Foreach alignment (p, q)
14:
probability (p, q) < Pmin
15:
return false
16: return true

(a) 1-best Extraction

(b) Matrix-based Extraction

Figure 5: 1-best matrix-based rule extraction algorithms
extraction method employed list k-best sub-tree alignments provided.
k-best extraction need repeat procedure 1-best extraction
sub-tree alignment k-best list.
4.2 Rule Extraction Using Sub-tree Alignment Matrices
Previous work pointed current MT systems suffer error propagation due
alignment errors made within 1-best alignment (Venugopal, Zollmann, Smith, &
Stephan, 2008). sub-tree alignment early-stage step training pipeline,
errors 1-best alignment likely propagated translation rule extraction
parameter estimation translation model. Though problem alleviated
using k-best alignments, limited scope k-best alignments still results inefficient
learning translation rules. example, preliminary experiment shows 95.8%
extracted rules redundant 100-best alignments involved.
instead present simple efficient method, namely matrix-based rule extraction. method, use posterior-based output aligner represent
sub-tree alignment compact structure - call sub-tree alignment matrix alignment
matrix short (Liu, Xia, Xiao, & Liu, 2009b; de Gispert, Pino, & Byrne, 2010).
See Figure 6(a) two example sub-tree alignment matrices made pair sentence segments. matrices, entry indexed pair source target nodes.
score entry posterior probability alignment corresponding node pair, i.e., (u, v) probability defined Equation (20). probability
straightforwardly accessible output inference algorithm described Section
3.5. principle (u, v) viewed measure sub-tree alignment confidence: higher
value indicates confident alignment two nodes. way
754

fihave

RB

[4]

VBN

drastically



1

fallen





VV[4]

AS[5]

AD[2]

.1

AS[5]

4]

5]

VP[1]

.1

AD[2]

.8

.1

.1

.6

.1

.2

VP[3]

.3

.7

VV[4]

.4

AS[5]

1 VV[4]
1

VB
N[

1]

VB
P [2]
AD
VP [
3]
RB [

.9

AD[2]
VP[3]

VP[3]
VP[1]

VP[1]

1

[5]

VP [

ADVP[3]

VB

VBP[2]

VP [

1]

VP[1]

P [2]
AD
VP [
3]
RB [
4]
VB
N [5]

Unsupervised Sub-tree Alignment Tree-to-Tree Translation

.6

= fixed alignment

= possible alignment

Matrix 1: 1-best alignment

Matrix 2: posterior

(a) Sub-tree alignment matrices sample sub-tree pair
Minimal Rules
Extracted Matrix 2 (posterior)
r3
AD() RB(drastically)
r4
VV() VBN(fallen)
r6
AS() VBP(have)
r8
VP(AD1 VP(VV2 AS3 ))
VP(VBP3 ADVP(RB1 VBN2 ))
r10 VP(VV() AS()) VBN(fallen)
r11 VP(AD1 VP2 ) VP(VBP1 ADVP2 )

Minimal Rules
Extracted Matrix 1 (1-best)
r3 AD() RB(drastically)
r4 VV() VBN(fallen)
r6 AS() VBP(have)
r8 VP(AD1 VP(VV2 AS3 ))
VP(VBP3 ADVP(RB1 VBN2 ))

...
(b) Rules extracted using 1-best alignment alignment posterior
Figure 6: Matrix-based representation sub-tree alignment sample rules extracted.
Matrix 1 shows case 1-best sub-tree alignment, Matrix 2 shows
case sub-tree alignment posterior.
access possible sub-tree alignments (with different probabilities), rather limited
number them.
extract rules using sub-tree alignment matrix. method simple:
collect rules associated entry matrix. core algorithm
method essential used 1-best/k-best extraction. difference
1-best/k-best extraction matrix-based method considers possible node
pairs extraction, rather visiting only. See Figure 5(b) pseudocode sub-tree alignment matrix-based rule extraction algorithm, represents
sub-tree alignment matrix pair trees (S, ). Compared extracting rules
k-best alignments, method efficiently obtain additional rules whose extraction
blocked k-best extraction. example, right side Figure 6(b) two new rules
r10 r11 extracted, cannot obtained 1-best alignment result.
prevent extraction great number noisy rules low alignment probabilities,
755

fiXiao & Zhu

prune away rules whose alignment probabilities pre-specified threshold.
formally, given pair nodes (u, v), rule extraction executed (u, v)
satisfies:
(u, v)
< Pmin
(28)
(root(S), root(T ))
expression measures relative probability alignment (u, v) respect
sum probabilities possible derivations. Pmin empirical threshold control
often rules pruned (a larger Pmin means rules thrown away).
work, set 107 default. Therefore, entries zero score Figure
6(a) (denoted dot) excluded rule extraction.
However, discarding rules relatively low probabilities turn results incompleteness problem, is, extracted rules might unable transform given source
parse-tree, even training set. Nonetheless, problem severe
case. experiments observed parse-tree pairs (over 90%) training
corpus could recovered extracted rules Pmin chose default value,
contribution translation accuracy low confidence rules limited
(generally less 0.1 BLEU points).
Another note sub-tree alignment matrix-based extraction. advantage
method follows general well-developed framework syntax-based MT,
i.e., word/syntactic alignment + rule extraction/parameter estimation + MT decoding.
need replace rule extraction component sub-tree alignment matrixbased system, preserve components pipeline. means still
use heuristics obtain additional useful rules result sub-tree alignment
matrix-based extraction, rule composing (Galley et al., 2006) SPMT extraction
(Marcu, Wang, Echihabi, & Knight, 2006). Also, posterior probability encoded
matrix used better estimation various MT-oriented features.10
Note basis approach STSG model, rules sub-tree
alignment model resemble general forms translation rules used tree-to-tree MT
systems. So, alternative simple way rule induction, directly infer translation rules sub-tree alignment model take corresponding rule probabilities
features translation model MT decoding. However, tree-to-tree MT
method suffers several problems. First, sub-tree alignment model requires computation possible aligned tree-fragments, results high time complexity
training decoding procedures. result, aggressive pruning used
reasonable size search space, e.g., consider relatively small tree-fragments
implementation acceptable running speed. side effect, many relatively large
rules (e.g., composed rules SPMT rules) absent sub-tree alignment model,
available use traditional alignment + extraction heuristics pipeline.
engineering standpoint, efficient directly infer translation rules
sub-tree alignment model, compared inferring rules using pruned fixed subtree alignment matrix plus heuristics. Second, rule probability optimization
objective sub-tree alignment different used MT systems. example, use generative model maximum-likelihood/Bayesian approach sub-tree
10. See Section 4.3 detailed discussion parameter estimation issue.

756

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

alignment, use discriminative model minimum error rate training MT. Many
features employed MT decoder considered sub-tree alignment model.
issues might lead unsatisfactory MT performance. shown experiments (see Section 5.3.5), directly inferring translation rules sub-tree alignment
model achieve promising results.
4.3 Learning Features Machine Translation
previous work syntax-based MT, proved syntax-based systems make great
benefits MT-oriented features, even necessarily well explained
syntactic parsing viewpoint (e.g., phrase-based translation probabilities). However,
features available word/sub-tree alignment model. Instead
need learn features using additional step parameter estimation MT.
this, follow commonly-used framework estimates values various MToriented features extracted rule set using MLE. procedure simple:
translation rules extracted, obtain maximum-likelihood (or relative-frequency)
estimate parameters according definition feature function.
However, traditional tree-to-tree systems rule extracted tree pair
count unit one, used calculate values various features.
approach might enlarge influence noisy rules extracted sub-tree alignment
matrices. E.g., rule high alignment probability equal weight rule
low alignment probability, thus unreasonably large impact MT systems.
desired solution rule extracted derivation low probability
penalized accordingly feature learning. Motivated idea, use fractional counts
estimate appearance rule (Mi & Huang, 2008). Given node pair (u, v)
(S, ), alignment probability rule r rooted (u, v) defined (denoted
(r; u, v)):
X
(r; u, v) =
P(T, | S)
(29)
dD(S,T )
rd

(r; u, v) regarded probability sum derivations involving r (u, v).
Also, rewrite Equation (29) inside-outside fashion:

(r; u, v) = (u, v)
(p, q) P(r | S)
(30)
(p,q)yield(r)

define probability r involved derivations (S, ) as:
X
(r) =
(r; u, v)

(31)

u,v

Equation (31) sum probabilities r node pairs. means
rule probability considered multiple times particular derivations contain
r once. using (r), fractional count r defined be:
c(r) =

(r)
(root(S), root(T ))

757

(32)

fiXiao & Zhu

Equation (32) reflects probability likely r involved derivation given
pair trees. set bilingual parse trees, c(r) accumulated tree pair.
Obviously, c(r) used estimate parameters MT model, is,
translation rules weighted, parameter estimation procedure proceed usual,
weight counts. work c(r) employed learn five features used
MT decoder, including bi-directional phrase-based conditional translation probabilities
(Marcu et al., 2006) three syntax-based conditional probabilities (Mi & Huang, 2008).
Let () function returns sequence frontier nodes input tree-fragment.
probabilities computed following equations:
P
00
r00 :(sr00 )=(sr )(tr00 )=(tr ) c(r )
P
Pphrase (tr | sr ) =
(33)
0
r0 :(sr0 )=(sr ) c(r )
P
00
r00 :(sr00 )=(sr )(tr00 )=(tr ) c(r )
P
Pphrase (sr | tr ) =
(34)
0
r0 :(t 0 )=(tr ) c(r )
r

c(r)

P(r | root(r)) =

P

P(r | sr ) =

P

r0 :root(r0 )=root(r) c(r

c(r)
r0 :sr0 =sr

P(r | tr ) =

c(r0 )

c(r)
P

r0 :tr0 =tr

c(r0 )

0)

(35)
(36)
(37)

5. Experiments
evaluation, first experimented approach Chinese-English sub-tree
alignment task, tested effectiveness state-of-the-art tree-to-tree MT system.
5.1 Baselines
Three unsupervised sub-tree alignment methods chosen baselines experiments.
WordAlign-1 : WordAlign-1 based GHKM-like method (Galley et al., 2004)
uses word alignments infer syntactic correspondences. implementation,
GIZA++ toolkit grow-diag-final-and method used obtain
symmetric word alignment sentence pairs. sub-tree alignments
heuristically induced selecting node correspondences consistent
word alignment result (i.e., sub-tree alignments violate word
alignments). chose method widely adopted modern
tree-to-tree systems.
WordAlign-2 : second baseline essentially WordAlign-1.
difference WordAlign-1 improved word alignment system using
link-deletion techniques (Fossum et al., 2008). basic idea delete harmful
alignment links initial word alignment result (e.g., deleting link
Figure 2(a)). experiments considered likely
deletion top-10 common Chinese words (including {, , , ,
758

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

, , , , , }) top-10 common English words (including {the,
of, and, to, in, a, is, that, for, on}).
HeuristicAlgin: HeuristicAlgin re-implementation approach proposed
Tinsley et al.s (2007) work. method alignment confidence every node
pair first computed lexical translation probabilities, used obtain
node correspondences via heuristic algorithm. method require
training process successfully adopted several translation tasks,
French-English translation, chosen another baseline comparison.
5.2 Experimental Setup
settings experiments described follows.
5.2.1 Data Preparation
bilingual corpus consists 1.06 million sentence pairs.11 mentioned above,
used GIZA++ grow-diag-final-and heuristics generate 1-best/k-best word
alignments, used baseline word alignment results. parse trees
Chinese English generated using Berkeley Parser.12 publicly available
corpus used evaluate sub-tree alignment result.13 consists 736 node-aligned
sentence pairs (with gold-standard parse trees language sides) LDC2003E07
included bilingual data. corpus divided two parts:
held-out set used finding appropriate setting hyperparameters (99 sentences
articles 301-309), test set used evaluating sub-tree alignment systems (637
sentences articles 001-066). MT experiments, 5-gram language model trained
Xinhua portion Gigaword corpus addition English part LDC
bilingual training data.14 used NIST 2003 MT evaluation corpus development
set (919 sentences) newswire portion NIST 2004-2006 MT evaluation corpora
test set (3,486 sentences).
5.2.2 Sub-tree Alignment
parameters sub-tree alignment model initialized add-one smoothing
rule-set extracted using word alignments (i.e., WordAlign-1 baseline). Then,
model trained parse trees bilingual corpus using EM algorithm
Variational Bayes (VB) approach. implementation VB-based training,
hyperparameters assumed share value.15 leads setting = 0.01
11. LDC category: LDC2003E14, LDC2005T10, LDC2003E07, LDC2005T06, LDC2005E83, LDC2006E26,
LDC2006E34, LDC2006E85, LDC2006E92 LDC2004T08. See http://www.ldc.upenn.edu/
details.
12. Note LDC2003E07 corpus reused gold-standard parse trees provided Chinese
English treebanks.
13. Available http://www.nlplab.com/resources/nodealigned-bitreebank.html
14. LDC category English Gigaword corpus: LDC2003T05
15. Although could adopt different hyperparameters finer control priors model parameters, found setting hyperparameters value could lead satisfactory
performance.

759

fiXiao & Zhu

optimal value held-out set. default, trained model 5 EM
Variational EM iterations. speed-up training process avoid degenerate
analysis caused large rules, restricted rules reasonable sizes rules five frontier non-terminals depth three. rules
five frontier non-terminals, considered tree-fragments depth one
restrict number frontier non-terminals involved, is, flat tree structures,
used associated height-one tree-fragments. Besides, discarded sub-tree
alignment every node pair whose terminals aligned outside corresponding
spans two times WordAlign-1.
5.2.3 Machine Translation
used NiuTrans open-source toolkit (Xiao, Zhu, Zhang, & Li, 2012) build
tree-to-tree MT system. rule extraction, used extension GHKM method
extract minimal tree-to-tree transformation rules (Liu et al., 2009a) obtained larger
rules composing two three minimal rules (Galley et al., 2006). used CKY-style
decoder cube pruning (Huang & Chiang, 2005) beam search decode Chinese
sentences. default beam size set 50. addition features described
Equations (33)-(37), used several features MT system, including
5-gram language model, rule number bonus, target length bonus two binary
features - lexicalized rule low frequency rule (Marcu et al., 2006). features
combined log-linear fashion optimized using Minimum Error Rate Training (MERT,
Och, 2003).
5.3 Results
following part section, present experimental results, including evaluations sub-tree alignments, extracted rules, MT systems. Also, show results
several improved methods effective use approach tree-to-tree MT.
5.3.1 Evaluation Alignments
First evaluated alignment quality various sub-tree alignment approaches terms
precision (P), recall (R) F-1 score.16 See Table 2 results three baseline
systems sub-tree alignment system. measures, VB-based system
significantly improves overall recall F-1 score, slightly degrading precision
compared WordAlign-1/2. Also, VB-based training outperforms EM-based counterpart due priors introduced learning process. interesting observation
that, though EM training model suffers degenerate analysis
data, show extremely bad results experiment. phenomenon due
restriction size tree-fragment training. described Section 5.2.2,
restricted translation rules reasonable-size tree-fragments several ways (e.g.,
16. Let predicted number alignments system output, correct number correct alignments
system output, gold number alignments gold-standard. measure precision, recall
2
)precisionrecall
correct
F- score defined as: precision = predicted
, recall = correct
F- = (1+
.
gold
2 precision+recall
parameter controls preference recall (i.e., > 1) precision (i.e., 0 < 1).
NLP tasks set 1, indicating equal weights recall precision.

760

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

Entry
Overall
NP NP
NN NN
VP VP
PU ,
IP
PU .
NP NN
NP PP
NN NNS
NR NNP
NN NP
PP PP
NN JJ
P
QP NP

WordAlign-1
P
R
F-1
75.4 63.6 69.0
86.9 59.0 70.3
83.9 75.6 79.5
75.3 61.9 68.0
84.7 76.5 80.4
92.5 87.5 89.9
98.5 98.7 98.6
77.6 71.5 74.4
46.8 63.8 54.0
84.2 76.0 79.9
69.4 41.2 51.8
65.1 59.8 62.4
84.6 68.4 75.7
90.7 76.5 83.0
81.5 69.8 75.2
72.2 64.9 68.3

WordAlign-2
P
R
F-1
76.3 64.5 69.9
87.0 62.1 72.5
83.6 77.0 80.2
74.9 61.9 67.8
84.7 76.5 80.4
92.3 87.7 89.9
98.5 98.7 98.6
83.0 71.5 76.8
49.7 63.5 55.7
83.8 76.5 80.0
69.9 41.7 52.3
65.9 61.0 63.3
85.1 69.2 76.3
90.4 76.3 82.7
82.3 68.3 74.7
72.6 65.2 68.7

HeuristicAlgin
P
R
F-1
65.7 67.7 66.7
79.1 73.6 76.3
76.2 74.1 75.1
71.3 71.8 71.6
69.6 71.3 70.3
90.6 90.7 90.6
98.5 98.7 98.6
59.3 77.5 67.2
53.1 31.9 39.8
77.8 75.5 76.7
63.4 57.4 60.2
71.8 50.4 59.2
79.3 72.6 75.8
83.9 81.7 82.8
81.2 72.2 76.4
67.1 65.6 66.4

(EM)
P
R
F-1
79.8 46.2 58.5
84.2 48.2 61.3
81.8 63.7 71.6
80.5 49.0 60.9
82.7 67.7 74.5
90.4 76.8 83.0
94.7 86.8 90.6
75.4 62.3 68.2
43.2 42.1 42.6
83.7 58.3 68.7
57.7 41.5 48.3
70.5 48.1 57.2
85.6 56.9 68.4
84.2 69.1 75.9
79.9 57.9 67.1
78.3 42.2 54.8

(VB)
P
R
F-1
72.6 75.1 73.8
88.7 75.3 81.4
81.1 79.9 80.5
75.7 75.8 75.7
82.5 80.7 81.6
90.0 92.4 91.2
98.5 98.5 98.6
75.9 78.9 77.4
54.5 70.1 61.3
81.0 77.6 79.2
67.2 56.3 61.3
71.1 67.7 69.4
85.4 82.5 83.9
85.9 81.2 83.5
84.7 76.1 80.2
74.9 71.7 73.3

Table 2: Evaluation results sub-tree alignment system baselines.
measures reported percentage.
set parameter maximum depth). constraints reduce number rules
involved training, prevents use rare large rules. result indicates
fact tree-fragment size constraint actually important efficiency
crucial learning. discussed previous work, without constraints
imposing proper prior, solution EM degenerate (Marcu & Wong, 2002; DeNero,
Gillick, Zhang, & Klein, 2006).
addition, Table 2 shows result 15 common types sub-tree alignment. expected, VB-based system achieves best F-1 score cases.
interestingly, observed approach obtains significantly better performance
handling PP (Prepositional Phrase) alignment seems difficult problem
baselines due unclear boundary indicators aligning PP structures. attribute
better use syntactic information language sides model,
generally ignored traditional models based surface heuristics word alignments.
5.3.2 Evaluation Extracted Rules
applied sub-tree alignment result tree-to-tree system study impact
sub-tree alignment MT. discussed Section 4, rule extraction downstream
component sub-tree alignment current tree-to-tree MT pipeline. therefore
chose evaluate quality rules obtained various sub-tree alignment results.
determine goodness extracted grammars, computed rule precision, recall,
F-1 scores approach baseline approaches test set used
(1-best) alignment quality evaluation. make gold-standard grammar, chose
method used Fossum et al.s (2008) work grammar automatically
generated manually-annotated alignment result, is, rules extracted using
annotated sub-tree alignments regarded gold-standard computing various
evaluation scores. Table 3 shows evaluation result grammars extracted
761

fimatrix

1best

Xiao & Zhu

Entry
WordAlign-1
WordAlign-2
HeuristicAlgin
(EM)
(VB)
(VB + Pmin
(VB + Pmin
(VB + Pmin
(VB + Pmin

= 105 )
= 106 )
= 107 )
= 108 )

Rule P
51.9
52.3
55.8
61.9
54.9
79.6
53.0
41.3
34.9

Rule R
60.8
61.8
55.3
49.2
65.2
34.5
70.0
75.6
79.5

Rule F-1
55.9
56.6
55.5
54.8
59.6
48.2
60.3
53.4
48.5

Table 3: Evaluation results rules obtained various sub-tree alignment approaches.
measures reported percentage.
different sub-tree alignment approaches. see improvements persist
sub-tree alignments employed translation rule extraction. VB-based approach
produces grammars higher rule F-1 score three baselines.
addition 1-best extraction, studied rule extraction behaves
sub-tree alignment matrix-based extraction method. Table 3 shows result
sub-tree matrix-based extraction method different choices pruning parameter
Pmin . see smaller values Pmin result grammars higher rule recall. Also,
better rule F-1 scores achieved adjusting Pmin seeking good balance
rule precision rule recall , e.g., Pmin = 106 107 .
scores informative measure grammar quality, investigated differences rule sets obtained model compared
baseline approaches, following Levenberg, Dyer, Blunsoms (2012) method. Figure 7
shows probable rules (frequency 2) obtained bilingual corpus using
VB-based alignment approach appear model WordAlign-2
alignment vice versa. asked two annotators sub-tree alignment estimate
rule quality based syntactic correspondence adequacy frontier node sequence
two languages sides. rule labeled good judges considered good quality. figure, see eight top-10 rules extracted
using approach absent WordAlign-2 grammar good rules. contrast,
four top-10 rules baseline model good quality sense human
preference. Furthermore, examined top-100 probable rules appear
two grammars individually. Again, top-100 rules extracted using proposed model
better quality. results 61 good rules. contrast, 44% top-ranking
rules induced using WordAlign-2 alignment good translation rules.
5.3.3 Evaluation Translations
evaluated translations generated MT system different sub-tree alignment approaches. Since VB-based training shows best performance previous
experiments, chose default setting approach following experiments.
Table 4 shows evaluation result translation quality estimated using caseinsensitive IBM-version BLEU4 (Papineni, Roukos, Ward, & Zhu, 2002) TER (Snover,
762

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation


1*
2*
3*
4
5*
6*
7*
8*
9*
10

top-10 highest probability rules (for MT) approach absent WordAlign-2
NP(DNP1 NN()) VP(ADVP1 VP(VBD(improved)))
NP(PU( ) NP(CD() NN()) PU()) NP(DT(the) CD(two) NNS(sessions))
NP(NP(QP1 NP(NN())) NP2 ) NP(X2 NP(CD1 NP(NNS(represents))))
NP(NN() NP1 ) VP(VB(desire) NNP1 )
NP(PU() NP(NR1 NN()) PU()) NP(() NP(NNP1 NN(independence)) ())
NP(VP1 DEC() NP(NN() NN()))
NP(ADJP(ADJP1 JJ(ideological)) NN(struggle))
NP(NP(QP1 NP2 ) NP(ADJP(JJ()) NP(NN())))
NP(NP(DT(the) JJ(important) NN(thinking)) IN(of) SBAR(WHNP1 S2 ))
NP(IP1 DEG() NP(NN() NN())) NP(ADJP(ADJP1 JJ(practical))
NN(significance))
NP(NP(PU() NP(QP1 NN()) PU()) NP(ADJP2 NN()))
NP(NP(DT(the) JJ2 NN(idea)) PP(IN(of) NP(DT(the) CD1 NNS(represents))))
NP(PU() NN() PU1 ) VP(VBG(joining) NP(DT(the) NN1 ))

top-10 highest probability rules (for MT) WordAlign-2 absent approach
1
LCP(QP1 LC()) ADJP(JJ1 )
2*
NP(DNP1 NN()) VP(ADVP1 VBP(changes))
3*
NP(NN() NN1 ) NP(CD(three) NNS(links) X1 )
4
NP(DNP(IP1 DEC()) NP(NN())) NP(ADJP1 NN(significance))
5
VP(ADVP1 VP(VV2 NP(NN() NN3 )))
VP(ADVP1 VP(VP(VV2 ) NP(DT(the) JJ(mass) NN3 )))
6
IP(NP1 VP(VV() NP(NN() NN2 ))) NP(NP(NNS1 ) PP(IN(for) NP2 ))
7
NP(VP1 DEG() NN()) ADJP(JJ1 )
8
NP(NP(PU() NT1 PU()) NP(NN()) NR())
NP(NP(PRP$(his)) QP(CD1 ) NN(speech))
9*
VP(VP(ADVP1 VP(VV() CC() VV())) NP2 )
VP(ADVP1 VP(VP(VB(strengthen) CC(and) VB(improve)) NP2 ))
10* NP(NP(PU() NN() PU()) NP1 )
NP(NP(() NP(NN(taiwan) NN(independence)) ()) NNS1 )

Figure 7: top-10 highest probability rules built proposed sub-tree alignment
approach WordAlign-2 baseline grammar, top-10 rules
WordAlign-2 baseline grammar obtained using proposed
sub-tree alignment approach. * = good translation rule.
Dorr, Schwartz, Makhoul, Micciula, & Weischedel, 2005), significance test performed using bootstrap resampling method (Koehn, 2004). Moreover, efficiency
rule extraction reported terms rule-set-size/extraction-time. comparison,
report result rule extraction using word alignment matrices (Liu et al., 2009b)
WordAlign-1 WordAlign-2.
Table 4 indicates approach outperforms baselines BLEU TER
measures 1-best 30-best extraction. addition, matrix-based method
much efficient k-best method. example, compared 30-best extraction, extracting rules sub-tree alignment matrices 9 times efficient. However,
rules counted unit one parameter estimation translation model,
using alignment matrices show significant BLEU improvements TER reductions comparison 30-best counterpart (see rows marked unitcount).
many additionally extracted rules utilized real translation.
example, observed 7.3% rules used generating final (1-best)
763

fiXiao & Zhu

Entry
WordAlign-1 (1-best)
WordAlign-2 (1-best)
HeuristicAlgin (1-best)
WordAlign-1 (30-best)
WordAlign-2 (30-best)
HeuristicAlgin (30-best)
WordAlign-1 (matrix)
WordAlign-2 (matrix)
(1-best + unitcount)
(30-best + unitcount)
(matrix + unitcount)
(1-best + posterior)
(30-best + posterior)
(matrix + posterior)

Dev

Test

BLEU4[%] TER[%]

BLEU4[%] TER[%]

36.2
36.2
35.7
36.3
36.4
35.4
36.9*
36.8*
36.7*
36.8*
36.9*
36.9*
37.0*
37.4**

34.2
34.2
33.8
34.4
34.6*
33.9
35.0*
35.1*
34.9*
35.0*
35.3**
35.0*
35.2**
35.6**

57.0
56.9
57.2
57.2
57.0
57.2
56.5
56.6
56.6
56.6
56.3*
56.4*
56.2**
55.9**

58.3
58.1
58.3
58.2
58.0
58.0
57.9*
57.7*
57.8
57.6*
57.5*
57.4*
57.0**
57.1**

Rule-set
size
24.8M
25.3M
22.7M
32.7M
33.0M
32.4M
50.2M
53.8M
27.0M
37.4M
54.9M
27.0M
37.4M
54.9M

Efficiency
(rule/sec)

75.4
75.9
72.0
3.8
3.9
3.8
35.8
37.9
78.8
4.1
37.7
78.8
4.1
37.7

Table 4: Evaluation translations different alignment approaches. BLEU, higher
better. TER, lower better. unitcount means take rule
occurrence unit one parameter estimation, posterior means use
rule posterior probabilities fractional counts parameter estimation. * **
= significantly better three 1-best baselines (p < 0.05 0.01).
translations indeed extracted alignments seen 30-best
alignments. thus indicates fact naively increasing number rules might
effective improving translation quality.
last three rows Table 4 show result using alignment posterior probabilities
parameter estimation (i.e., method described Section 4.3). see alignment
posterior probabilities helpful improving translation quality system
weight rules confidence (entries unitcount vs. entries
posterior ). using sub-tree alignment matrices rule extraction alignment
posterior probabilities parameter estimation, approach finally achieves +1.0 BLEU
improvement -0.9 TER reduction 30-best case baselines. even
outperforms word alignment matrix-based counterpart +0.5 BLEU points -0.6
TER points (both significant p < 0.05).
Further, effectiveness proposed approach demonstrated terms BLEU
TER scores rule-set size. Figure 8 compares approach
baseline approaches different numbers unique rules extracted.17 Clearly,
number unique rules, proposed sub-tree alignment approach leads better translations baselines.
5.3.4 Impact Alignment Grammar Quality MT Performance
experiments demonstrate effectiveness proposed approach terms
different measures individually. next natural question sub-tree alignment
17. this, adjusted Pmin obtain grammars different sizes approach.
approaches, used different k-best lists rule extraction.

764

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

43

1 - TER[%]

BLEU4[%]

36

35

34

HeuristicAlgin
WordAlign-2
WordAlign-1


42

HeuristicAlgin
WordAlign-2
WordAlign-1

41

33
10

20

30

40

50

60

70

10

20

30

40

50

60

70

Rule-set size (million)

Rule-set size (million)

Figure 8: BLEU 1-TER rule-set size
rule extraction affect translation quality. study issue important
optimize upstream systems MT decoding select appropriate evaluation
metrics good prediction MT performance.
therefore carried another set experiments compares translation
quality different sub-tree alignment rule extraction settings. generate diverse
sub-tree alignment rule extraction results, varied values Pmin
sub-tree alignment rule extraction respectively. way, obtained ensembles
sub-tree alignments grammars different precision recall scores.18 chose
F- score evaluation metric sub-tree alignment system rule
extraction system. Instead fixing 1, varied value 0.5 3. Since
parameter control bias towards precision recall, choosing different values
helpful seeking good tradeoff precision recall. find
appropriate evaluation measure sub-tree alignment rule extraction predicting
MT performance well.
Figures 9 10 plot F- scores measures MT performance sub-tree alignment rule extraction. Figure 10, see rule F-3 score correlates best
translation quality measures, indicates MT system prefers rule
recall-biased metrics. agrees observation Figure 8 MT system
make benefits rules. hand, curves Figure 9 show
better correlation sub-tree alignment F-2/F-3 score translation quality measures, implying preference relatively higher sub-tree alignment recall. result
reasonable framework node alignment links result aligned
tree-fragments (or rules) extracted. high-recall sub-tree alignment generally results
big grammar high rule recall, thus better BLEU TER results. com18. example, larger value generally results higher alignment precision, small value
prefers higher alignment recall. rule extraction, larger value Pmin generally leads grammar
higher rule precision, choosing smaller Pmin generate grammar higher rule recall.

765

fiXiao & Zhu

80

sub-tree alignment F-[%]

sub-tree alignment F-[%]

80

70

60

F-0.50
F-0.75
F-1.00
F-2.00
F-3.00

50

40
33.5

34

34.5

35

35.5

70

60

F-0.50
F-0.75
F-1.00
F-2.00
F-3.00

50

40

36

41

BLEU4[%]

42

43

1 - TER[%]

70

70

60

60

rule F-[%]

rule F-[%]

Figure 9: BLEU 1-TER sub-tree alignment F- measure

50
40

F-0.50
F-0.75
F-1.00
F-2.00
F-3.00

30
20
34

34.5

35

50
40

F-0.50
F-0.75
F-1.00
F-2.00
F-3.00

30
20

35.5

41.5

BLEU4[%]

42

42.5

43

1 - TER[%]

Figure 10: BLEU 1-TER rule F- measure

puted Pearsons correlation coefficients sub-tree alignment/rule F-3 score
BLEU/TER. sub-tree alignment F-3, correlation coefficients BLEU TER
0.971 -0.962 respectively. rule F-3, correlation coefficients BLEU
TER 0.983 -0.963 respectively. show good correlations translation quality measures. Another interesting observation MT performance
sensitive change rule F- score change sub-tree alignment
F- score. may lie rule extraction direct upstream step decoding
impacts output MT systems. contrast, sub-tree alignment front-end
step MT pipeline indirect effect actual translation process.
766

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

System
Hierarchical phrase-based
Tree 1-best word alignment (WordAlign-2)

Word alignment matrix
tree Sub-tree alignment matrix

Dev

Test

BLEU4[%] TER[%]

BLEU4[%] TER[%]

37.2
36.8
37.0
37.9*

35.2
35.3
35.2
36.0**

57.3
56.4*
56.3*
55.8**

58.0
57.7
57.5*
56.8**

Table 5: MT Evaluation results rules obtained various alignment approaches.
BLEU, higher better. TER, lower better. * ** = significantly better
hierarchical phrase-based baseline (p < 0.05 0.01).
5.3.5 Improvements
Previous work pointed straightforward implementation tree-to-tree MT
suffers problems rules derivations either rule extraction
decoding process (Chiang, 2010). advance tree-to-tree system compare
state-of-the-art, employed tree binarization (Wang et al., 2007b) fuzzy
decoding (Chiang, 2010) system. alignment approach equipped
general framework tree-to-tree translation, trivial conduct another set
experiments investigate effectiveness approach stronger system.19 Table
5 shows BLEU TER scores system enhanced methods.20
comparison, report result state-of-the-art MT system implements
hierarchical phrase-based model (Chiang, 2007) tree-to-tree system extracts
rules using word alignment matrices (Liu et al., 2009b). Table 5 indicates superiority
approach tree binarization fuzzy decoding involved. significantly
outperforms hierarchical phrase-based system (+0.7 BLEU points -1.2 TER points)
tree-to-tree system based word alignment matrices (+0.8 BLEU points
-0.7 TER points).
discussed Section 4, transfer rules sub-tree alignment model resembles
general form STSGs directly used MT. Instead resorting
explicit step rule extraction, use rules sub-tree alignment model
MT decoding, i.e., sub-tree alignment cast grammar induction step. therefore
built another system directly acquires translation rules sub-tree alignment
step. this, need output rules derivation forest generated
alignment model. rule probabilities obtained using inside output
probabilities, pruning performed throwing away rules whose probability
Pmin . addition rule probability, reused n-gram language model, rule number
bonus, target length bonus, lexicalized rule low frequency rule indicators
base tree-to-tree system additional features fair comparison. obtain good
reasonable result, employed fuzzy decoding tree binarizaiton experiment.
19. choose setting previous experiments gold-standard alignment annotation Penn Treebank-style trees only. difficult evaluate alignment grammar
quality binarized trees due lack benchmark data. experiments first conducted
experiments individual tasks (see Sections 5.3.1-5.3.3), studied correlations simple
reasonable setting consistent result sub-tree alignment MT (see Section 5.3.4).
investigated effectiveness approach advanced tree-to-tree system (see Section 5.3.5).
20. implementation parse trees binarized head-out fashion.

767

fiXiao & Zhu

Entry
Baseline (explicit rule extraction)
Rules sub-tree alignment model
Rules sub-tree alignment model + MERT
Baseline + sub-tree alignment features

Dev

Test

BLEU4[%] TER[%]

BLEU4[%] TER[%]

37.9
36.2
36.7
38.2

36.0
34.3
34.9
36.1

55.8
57.9
57.3
55.8

56.8
58.8
58.0
56.9

Table 6: MT Evaluation results obtaining rules sub-tree alignment model
obtaining rules traditional rule extraction pipeline
Table 6 compares results sub-tree alignment matrix-based rule extraction inducing rules alignment model (Row 1 vs. Row 2). Unfortunately, straightforwardly
inferring rules probabilities sub-tree alignment model underperforms
baseline. might attributed several reasons. First, due large derivation
space, cannot enumerate relatively large tree-fragments sub-tree alignment
step, instead access tree-fragments limited depths. contrast, baseline system extracts basic rules using sub-tree alignment matrices obtains
large rules heuristics (e.g., rule composing). additional rules obtained
baseline framework rule extraction general useful modern syntax-based
systems (Galley et al., 2006; Marcu et al., 2006; DeNeefe et al., 2007). Second, rule
probability sub-tree alignment model defined product probability factors
good generation story. However, MT systems usually use features required form generative model, features shown Equations (33)-(37).
consequence, many well-developed features used baseline system
available sub-tree alignment model. Third, sub-tree alignment model
trained maximizing likelihood criteria, consistent
adopted MT system (i.e., minimizing evaluation-related error rate function).
study issues, improved system two ways. First, treated four
probability factors sub-tree alignment model (See Equation (13)) different
features MT decoder, tuned weights using MERT. Row 3 Table 6 shows
method achieves better results system employing unweighted probability
factors. However, performance still worse baseline, indicates
MT-oriented features rule extraction heuristics crucial success
tree-to-tree system. Finally added probabilistic factors sub-tree alignment
model baseline system additional features. shown last row Table 6,
enhanced system yields modest BLEU improvements baseline, TER
improvement achieved. results give us two interesting messages - 1) rule
extraction heuristics, MT-oriented features objectives learning key factors contributing good tree-to-tree system; 2) better use sub-tree alignment
model upstream module rule extraction decoding, rather using
simple step grammar induction.
last issue investigate whether sub-tree alignment model make
benefits labeled data. Although focus unsupervised learning work,
proposed model require strictly unsupervised condition. Instead
enhanced use labeled data. idea simple: combine probability
factors sub-tree alignment model log-linear weighted fashion. means
768

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

Entry
Unweighted
Weighted (weights learned labeled data)

Dev

Test

BLEU4[%] TER[%]

BLEU4[%] TER[%]

37.9
38.3

36.0
36.1

55.8
55.6

56.8
56.9

Table 7: Comparison unweighted weighted sub-tree alignment models
probability factors sub-tree alignment model taken real valued feature
functions, feature weights learned labeled data supervised methods.
way, unweighted generative model (i.e., factor weight one)
transformed weighted model (i.e., factor individual weight). Note
weighted model almost form used SMT systems.
difference SMT model language model needed targetlanguage side fixed sub-tree alignment step. avoid bias towards
many rules, added rule number additional feature new model.
training test, divided node-aligned gold-standard data two parts -
first 310 sentences selected weight training, remaining 327 sentences
selected testing system. learn feature weights supervised manner, chose
MERT one powerful tools training log-linear models. error
function used MERT defined one minus sub-tree alignment F-1 score.
327-sentence test data (with tree annotation Penn Treebanks),
weighted model achieves alignment F-1 score 75.4% rule F-1 score 60.0%,
respectively. result better unweighed (and unsupervised) model
obtains alignment F-1 score 72.4% rule F-1 score 59.2%
data set. Finally tested MT performance best setting (i.e., sub-tree alignment
matrix-based rule extraction + tree binarization + fuzzy decoding).21 Table 7 shows
weighted sub-tree alignment model leads better BLEU score tuning set
show promising improvements test data. size labeled corpus
small, expect better results labeled data available. worth studying
sophisticated supervised methods learn better weights, kernel-based
methods (Sun et al., 2010b). supervised/semi-supervised learning focus
work, leave interesting issues future investigations.

6. Related Work
Syntax-based approaches widely adopted machine translation last
ten years. Many successful syntactic MT systems developed shown good
results several translation tasks (Eisner, 2003; Galley et al., 2004, 2006; Liu et al.,
2006; Huang et al., 2006; Zhang et al., 2008; Liu et al., 2009a; Chiang, 2010). Despite
differences modeling implementation details, models require alignment
step acquire syntactic correspondence source target languages.
standard SMT, syntax-based MT systems use word alignments infer syntactic
alignments string-tree/tree-tree pairs. However, word alignments generally
good quality viewpoint syntactic alignment. makes sense directly
21. sub-tree-aligned data binarized trees, reused weights learned Penn
Treebank-style trees four probability factors sub-tree alignment model.

769

fiXiao & Zhu

induce sub-tree level alignments pairs sentences syntactic information
either language side both. especially true tree-to-tree MT
actually need alignment sub-trees two languages, rather surface
alignment words. several lines work address syntactic alignment
problem make better use various alignment results tree-to-tree translation.
6.1 Word Sub-tree Alignment Machine Translation
earliest efforts syntactic alignment focus enhancing word alignment models
syntactic information. date, several research groups (Fraser & Marcu, 2007; DeNero
& Klein, 2007; May & Knight, 2007; Fossum et al., 2008; Haghighi, Blitzer, DeNero, &
Klein, 2009; Burkett, Blitzer, & Klein, 2010; Riesa, Irvine, & Marcu, 2011) proposed
syntax-augmented models advance word alignment systems. Although models
achieved promising improvements, still address alignment problem word level.
discussed Section 1, methods might desirable choices learning
correspondence tree nodes two languages. alternative straightforward solution, researchers tried infer sub-tree level alignments pairs syntactic
trees. example, Imamura (2001), Groves, Hearne, Way (2004), Tinsley et al.
(2007) defined several scoring functions measure similarity source
target sub-trees, aligned tree nodes greedy algorithms. approaches,
though simple implement, derived principled way. example,
models explicit optimization procedure, general framework statistical learning. Instead, model parameters obtained using additional alignment
models lexicons. another line research, Sun et al. (2010a, 2010b) attempted address sub-tree alignment problem supervised/semi-supervised models. used
tree kernels various syntactic features advance sub-tree alignment system
showed promising results Chinese-English translation tasks. However, approach still
relies heuristic algorithms inferring node correspondences two parse trees.
Beyond this, train tree kernels, approach requires additional labeled data
generally expensive build. Unlike studies, derive sub-tree model
principled way develop unsupervised sub-tree alignment framework tree-to-tree
MT.
6.2 Unsupervised Syntactic Alignment
previous studies resort labeled data sub-tree alignment.
earliest Eisners (2003) work. designed unsupervised approach
modeling sub-tree alignment problem STSG formalism. However, since
detailed derivation model decomposition provided, model computationally
expensive, even difficult applied current tree-to-tree systems complex tree
structures involved. Gildea (2003) applied STSGs tree-to-tree/tree-to-string
alignment. developed loosely tree-based alignment method address issue
parse-tree isomorphism bitext. work targets word alignment rather
modern syntactic MT systems. Recently Nakazawa Kurohashi (2011) proposed
Bayesian approach sub-tree alignment dependency trees, tested
Japanese-English MT system. Actually model much common model
770

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

presented work. example, apply unsupervised learning methods
Bayesian models sub-tree alignment. hand, two studies differ
important aspects. First, Nakazawa Kurohashi (2011) restricted sub-tree
alignment dependency trees, different aligning tree nodes
phrase structure trees. Since phrase structure trees involve complex structures
syntactic categories, alignment problem phrase structure trees relatively
difficult dependency-based counterpart. Second, model makes benefits
recent advances STSGs directly applicable current state-of-the-art tree-totree systems.
Another related work presented Pauls, Klein, Chiang, Knights
(2010) work. factored node-to-string alignment model components
generates target side synchronous rule source side. Moreover, probability
rule fragment factored lexical structural component work. Actually,
model proposed model two variants theme. appear
obvious differences them. First, focus sub-tree alignment tree-to-tree
translation, Pauls et al. (2010) addressed alignment issue tree-to-string/stringto-tree translation. model, parse language sides independently, rather
parsing one side projecting syntactic categories. result, inference faster
work since need consider possible parse trees unparsed side
alignment. Second, permutation model presented work general
order handle non-ITG trees. Third, investigate methods effective use
sub-tree alignment MT. particular, present rule extraction approach obtaining
additional translation rules using sub-tree alignment posteriors, rather learning rules
1-best sub-tree alignment.
6.3 Rule Extraction Using Various Alignment Results
machine translation, word syntactic alignments used extract translation rules
phrases. traditional pipeline rule phrase extraction, 1-best alignment result considered, suffers limited scope single alignment.
efficiently obtain diverse alignment/parsing results, packed data structures adopted
improve 1-best pipeline MT systems recent years (Mi & Huang, 2008; Liu et al., 2009a;
Zhang, Zhang, Li, Aw, & Tan, 2009). example, Liu et al. (2009b) de Gispert et al.
(2010) used alignment posterior probabilities phrase hierarchical phrase extraction.
development sub-tree alignment matrices actually motivated similar idea
word alignment matrices. difference work use sub-tree
language sides infer alignment posterior probabilities, probabilities
calculated word/phrase-level previous work (Liu et al., 2009b; de Gispert et al.,
2010). Moreover, knowledge, effectiveness sub-tree alignment matrix
systematically studied case tree-to-tree translation.
Note approach presented work something similar synchronous grammar induction. example, model results STSG
formalism used MT. Recent studies Bayesian models (Blunsom, Cohn,
Dyer, & Osborne, 2009; Cohn & Blunsom, 2009; Levenberg et al., 2012) shown
promising results directly learning synchronous grammars bilingual data hierar771

fiXiao & Zhu

chical phrase-based string-to-tree systems, rather extracting synchronous grammar
rules based explicit word/syntactic alignment step. However rare see related
work tree-to-tree MT. principle article different previous work synchronous grammar induction. example, aim work learn sub-tree
alignment model, applied many potential applications except MT,
sentence compression paraphrasing test summarization (Jing, 2000; Cohn & Lapata,
2009). Unlike synchronous grammar induction alignment implicitly encoded
learning process, treat sub-tree alignment separate task. eases
development tuning alignment system actually resort MT
systems slow difficult optimize. Another advantage approach
make benefits compact models, rather used MT great
number rules involved. Take implementation instance. alignment model
learned relatively small set grammar rules (rules limited depths),
MT system accesses much larger grammar many additional rules involved
rule composing. method result efficient alignment system likely
alleviate degenerate analysis data, cost degrading MT performance.

7. Discussion
underlying assumption proposed model 1-to-1 sub-tree alignments
achieved based constraints imposed neighboring parts tree (see Section
2). makes sense standpoint linguistically-motivated models, yet turn
faces problem constraints make difficult align sentences/trees correctly,
particularly free translations. several reasons explain
approach works nice tree-to-tree MT suffer greatly
structure divergence languages. First, model flexible allows
node deletion/insertion alignment. means levels tree
necessary require every node aligned valid node language side,
instead nodes dropped needed. advantage method
cannot confidently align node node counterpart tree, align
virtual node enforce bad constraints aligning parts tree.
useful flat tree structures partial translations
syntactically well-formed. Second, main purpose approach infer
sub-tree alignment probabilities used pruning sub-tree alignment matrices
extracting rules MT systems. Though 1-to-1 alignment required training
sub-tree alignment model, actually access large number alignment
alternatives rule extraction, even cannot appear derivation
due alignment constraints. Third, model work phrase structure
trees. Instead Penn Treebank-style trees difficult alignment
cases, sub-tree alignment system works well binarized trees shows promising
improvements various baselines. Note tree binarization effective method
alleviate structure divergence problem, especially Chinese-English translation.
Also, might interesting investigate methods dealing differences
syntactic structures languages, forest-based methods (Mi & Huang,
2008; Liu et al., 2009a).
772

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

Another note approach. implemented naively, speed sub-tree alignment system slow since model needs calculation alignment probability pairs tree-fragments. Fortunately, thought computation, several
optimizations make system much efficient practice. First, described
work, pruning methods employed restrict number tree-fragments
reasonable level. experiments, system pruning achieved speed 1.82.2
sentence/second single core Intel Xeon 3.16-GHz CPU. Another way speed
improvement parallel processing. good property EM-style algorithms
E-step easily implemented parallel computation environment.
need divide training data set number smaller parts, run
inside-outside algorithm parts parallel (i.e., Map procedure). expected
counts model parameters accumulated results parts (i.e.,
Reduce procedure). M-step performed usual. implementation
used 40 threads parallel training. running time one training iteration
1-million-sentence corpus 14-17 hours. Note system speed-up
expected powerful distributed infrastructures available (e.g., clusters +
Hadoop), difficult scale approach handle millions sentence pairs
using current training framework.

8. Conclusions
proposed unsupervised probabilistic sub-tree alignment approach tree-totree translation. factoring alignment model several components, resulting
model easily learned using EM algorithm variational Bayesian approach.
Also, investigated different ways applying proposed model tree-to-tree
translation. particular, developed sub-tree alignment matrix encodes
exponentially large number alignments. representation sub-tree alignment,
desirable rules extracted efficiently using k-best sub-tree alignment
result. experiments showed proposed model achieved significant improvements
alignment quality grammar quality several baselines. NIST ChineseEnglish evaluation corpora, achieved +1.0 BLEU improvement -0.9 TER reduction
top state-of-the-art tree-to-tree system. improved MT system even significantly
outperformed state-of-the-art hierarchical phrase-based system equipped tree
binarization fuzzy decoding.

Acknowledgments
work supported part National Science Foundation China (Grants
61073140 61272376), Natural Science Foundation Youth China (Grant
61300097), China Postdoctoral Science Foundation (Grant 2013M530131), Specialized Research Fund Doctoral Program Higher Education (Grant 20100042110031),
Fundamental Research Funds Central Universities (Grant N100204002).
authors would thank anonymous reviewers pertinent insightful comments, Keh-Yih Su great help improving early version article, Ji
773

fiXiao & Zhu

helpful discussions, Chunliang Zhang Tongran Liu language refinement.
corresponding author article Jingbo Zhu.

Appendix A. Part-of-speech Tags Phrase Structure Labels
work annotation POS tagging phrase structure parsing follows standard defined Penn English Chinese Treebanks (Marcus et al., 1993; Xue et al.,
2005). See Tables 8-11 lists POS tags constituent labels used example
trees article.
POS Tag
AD

NN
NR
P
PN
PU
VV

Description
Adverb
Aspect Particle
Noun (except proper nouns temporal nouns)
Proper Noun
Preposition
Pronoun
Punctuation
Verb (except stative verbs, copulas, main
verbs , )

Table 8: Chinese POS tags used examples
POS Tag
DT

JJ
NNP
NNS
PRP
RB
VBD
VBN
VBP
,
.

Description
Determiner
Preposition
Adjective
Proper noun (singular)
Noun (plural)
Personal Pronoun
Adverb
Verb (past tense)
Verb (past participle)
Verb (non-3rd person singular present)
Comma
Period

Table 9: English POS tags used examples
Syntactic Label
IP
NP
PP
QP
VP

Description
Single Clause
Noun Phrase
Preposition Phrase
Quantity Phrase
Verb Phrase

Table 10: Chinese constituent labels used examples

774

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

Syntactic Label
ADVP
NP
PP

VP

Description
Adverb Phrase
Noun Phrase
Preposition Phrase
Sentence
Verb Phrase

Table 11: English constituent labels used examples
Distribution
Pnt ()
Ptree ()
Preorder ()

Notation
tnt |snt
ttree |tnt
tvnt |svnt

Plength ()
Pw ()

l|m
tw |sw

Description
snt tnt source target-language non-terminal symbols
ttree target-language tree-fragment
svnt tvnt vectors non-terminal symbols source
target-languages
l numbers source target terminals (or words)
sw tw source target terminals (or words)

Table 12: Notations model parameters

Appendix B. EM-based Training Sub-tree Alignment Model
described Section 3, proposed sub-tree alignment model five types parameters, including non-terminal mapping probability Pnt (), target-language treefragment generation probability Ptree (), frontier non-terminal reordering probability
Preorder (), word number probability Plength () word mapping probability Pw ().
convenience use new set notations denote model parameters following description. See Table 12 symbol list.
follow framework EM-based training described Figure
3. See Figure 11 complete version EM algorithm parameters model.
algorithm, EC() represents expected count input variable. (X = x)
0-1 function returns 1 variable X takes value x, 0 otherwise. (k) (r; u, v)
(k) (S, ) rule probability (see Equation (29)) probability subtree alignment (see Equation (20)), k indicates
probabilities calculated based parameters k-th iteration. tree(), vnt()
lex() functions return tree-structure, frontier non-terminal vector,
terminal sequence input tree-fragment, respectively (see Section 3.2).
basic idea E-step check rule r (given pair tree nodes u
(r;u,v)
v) update EC() relative probability (root(S),root(T
)) . applied
update rules parameters tnt |snt , ttree |tnt , tvnt |svnt l|m (see lines 8-11).
exception tw |sw . defined Equation (14), Pw (ti | sj ) direct
product factor,Pinstead use sum terminals source-language treefragment (i.e.,
j=1 Pw (ti | sj )). follow result IBM Model 1 make
P|lex(s )|
update magnitude proportional Pw (ti | sj )/ j 0 =1 r Pw (ti | sj 0 ). refer reader
Brown et al.s (1993) work detailed derivation expected count IBM Model
1. worth noting algorithm performs parameter update based
different choices (u, v) r E-step. means rule instance involved
particular derivation one time (e.g., tree-fragment appears
775

fiXiao & Zhu

1: Function TrainModelWithEM ({(S1 , T1 ), ..., (Sn , Tn )})
(0)
(0)
(0)
(0)
(0)
2: Initialize {tnt |snt , ttree |tnt , tvnt |svnt , l|m , tw |sw }
3: k = 0 K 1
4:
Set EC() = 0 model parameters
6: E-step:
5:
Foreach tree pair (S, ) sequence {(S1 , T1 ), ..., (Sn , Tn )}
6:
Foreach node pair (u, v) (S, )
7:
Foreach rule r rooted (u, v)
(k) (r;u,v)(snt =u)(tnt =v)
(k) (root(S),root(T ))

8:

EC(tnt |snt )

+=

9:

EC(ttree |tnt ) + =

(k) (r;u,v)(tnt =v)(ttree =tree(tr ))
(k) (root(S),root(T ))

10:

EC(tvnt |svnt ) + =

(k) (r;u,v)(svnt =vnt(sr ))(tvnt =vnt(tr ))
(k) (root(S),root(T ))

11:

EC(l|m )

(k) (r;u,v)(m=|lex(sr )|)(l=|lex(tr )|)
(k) (root(S),root(T ))

12:

Foreach word pair (sj , ti ) position (j, i) (lex(sr ), lex( tr ))

+=

(k)
P
(ti |sj )
(sw =sj )(tw =ti )
r P(k) (t |s )
w
j0
j 0 =1

(k) (r;u,v) P|lex(sw)|

13:

EC(tw |sw ) + =

(k) (root(S),root(T ))

10: M-step:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:

Foreach non-terminal symbol pair (snt , tnt )
(k+1)

tnt |snt

=

EC(tnt |snt )
P

t0nt



EC t0

nt |snt

Foreach target non-terminal symbol tnt tree-fragment structure ttree
EC(ttree |tnt )

(k+1)

ttree |tnt =

P

t0tree



EC t0

tree |tnt

Foreach pair non-terminal symbol vectors (svnt , tvnt )
EC(tvnt |svnt )

(k+1)

tvnt |svnt =

P

t0vnt



EC t0

vnt |svnt

Foreach pair word numbers (m, l)
(k+1)

l|m

=

EC(l|m )
P

l0

EC l0 |m



Foreach pair words (sw , tw )
(k+1)

tw |sw

(K)

=

EC(tw |sw )
P

t0w

(K)

EC t0



w |sw

(K)

(K)

(K)

return {tnt |snt , ttree |tnt , tvnt |svnt , l|m , tw |sw }
Figure 11: EM-based training algorithm model parameters

different positions), update corresponding parameters would carried
multiple times.
Another note EM algorithm. expected counts parameters
efficiently calculated using inside outside probabilities according lines 8-11
776

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

13. parameters efficient ways. example, discussed
Section 3.4.1, expected count tnt |snt obtained without checking individual
rule, is, omit loop r case. technique considered
speed-up sub-tree alignment system.

References
Attias, H. (2000). variational bayesian framework graphical models. Solla, S. A.,
Leen, T. K., & K., M. (Eds.), Advances Neural Information Processing Systems 12,
pp. 209215. MIT Press.
Beal, M. J. (2003). Variational algorithms approximate bayesian inference. Masters
thesis, University College London.
Blunsom, P., Cohn, T., Dyer, C., & Osborne, M. (2009). gibbs sampler phrasal
synchronous grammar induction. Proceedings Joint Conference 47th
Annual Meeting ACL 4th International Joint Conference Natural
Language Processing AFNLP (ACL-IJCNLP), pp. 782790, Suntec, Singapore.
Brown, P. E., Pietra, S. A. D., Pietra, V. J. D., & Mercer, R. L. (1993). mathematics
statistical machine translation: Parameter estimation. Computational Linguistics,
19, 263311.
Burkett, D., Blitzer, J., & Klein, D. (2010). Joint parsing alignment weakly synchronized grammars. Human Language Technologies: 2010 Annual Conference North American Chapter Association Computational Linguistics
(HLT:NAACL), pp. 127135, Los Angeles, California, USA.
Chiang, D. (2005). hierarchical phrase-based model statistical machine translation.
Proceedings 43rd Annual Meeting Association Computational Linguistics (ACL), pp. 263270, Ann Arbor, Michigan, USA.
Chiang, D. (2007). Hierarchical phrase-based translation. Computational Linguistics, 33,
4560.
Chiang, D. (2010). Learning translate source target syntax. Proceedings
48th Annual Meeting Association Computational Linguistics (ACL),
pp. 14431452, Uppsala, Sweden.
Chiang, D., & Knight, K. (2006). introduction synchronous grammars. Tutorials
21st International Conference Computational Linguistics 44th Annual
Meeting Association Computational Linguistics (COLING-ACL).
Chiswell, I., & Hodges, W. (2007). Mathematical Logic. Oxford University Press.
Cohn, T., & Blunsom, P. (2009). Bayesian model syntax-directed tree string grammar induction. Proceedings 2009 Conference Empirical Methods Natural
Language Processing (EMNLP), pp. 352361, Singapore.
Cohn, T., & Lapata, M. (2009). Sentence compression tree transduction. Journal
Artificial Intelligence Research, 34, 637674.
Das, D., & Smith, N. A. (2009). Paraphrase identification probabilistic quasi-synchronous
recognition. Proceedings Joint Conference 47th Annual Meeting
777

fiXiao & Zhu

ACL 4th International Joint Conference Natural Language Processing
AFNLP (ACL-IJCNLP), pp. 468476, Suntec, Singapore.
de Gispert, A., Pino, J., & Byrne, W. (2010). Hierarchical phrase-based translation grammars extracted alignment posterior probabilities. Proceedings 2010
Conference Empirical Methods Natural Language Processing (EMNLP), pp.
545554, Cambridge, MA, USA.
Dempster, A., Laird, N., & Rubin, D. (1977). Maximum likelihood incomplete data via
em algorithm. Journal Royal Statistical Society. Series B (Methodological),
39, 138.
DeNeefe, S., Knight, K., Wang, W., & Marcu, D. (2007). syntax-based MT
learn phrase-based MT?. Proceedings 2007 Joint Conference Empirical Methods Natural Language Processing Computational Natural Language
Learning (EMNLP-CoNLL), pp. 755763, Prague, Czech Republic.
DeNero, J., Gillick, D., Zhang, J., & Klein, D. (2006). generative phrase models underperform surface heuristics. Proceedings Workshop Statistical Machine
Translation (WMT), pp. 3138, New York city, USA.
DeNero, J., & Klein, D. (2007). Tailoring word alignments syntactic machine translation. Proceedings 45th Annual Meeting Association Computational
Linguistics (ACL), pp. 1724, Prague, Czech Republic.
Eisner, J. (2003). Learning non-isomorphic tree mappings machine translation.
Companion Volume Proceedings 41st Annual Meeting Association
Computational Linguistics (ACL), pp. 205208, Sapporo, Japan.
Ferguson, T. S. (1973). bayesian analysis nonparametric problems. Annals
Statistics, 1, 209230.
Fossum, V., Knight, K., & Abney, S. (2008). Using syntax improve word alignment
precision syntax-based machine translation. Proceedings Third Workshop
Statistical Machine Translation (WMT), pp. 4452, Columbus, Ohio, USA.
Fraser, A., & Marcu, D. (2007). Getting structure right word alignment: LEAF.
Proceedings 2007 Joint Conference Empirical Methods Natural Language
Processing Computational Natural Language Learning (EMNLP-CoNLL), pp. 51
60, Prague, Czech Republic.
Galley, M., Graehl, J., Knight, K., Marcu, D., DeNeefe, S., Wang, W., & Thayer, I. (2006).
Scalable inference training context-rich syntactic translation models. Proceedings 21st International Conference Computational Linguistics
44th Annual Meeting Association Computational Linguistics (COLINGACL), pp. 961968, Sydney, Australia.
Galley, M., Hopkins, M., Knight, K., & Marcu, D. (2004). Whats translation rule?.
Susan Dumais, D. M., & Roukos, S. (Eds.), Proceedings 2004 Human Language
Technology Conference North American Chapter Association Computational Linguistics (HLT:NAACL), pp. 273280, Boston, Massachusetts, USA.
778

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

Gildea, D. (2003). Loosely tree-based alignment machine translation. Proceedings
41st Annual Meeting Association Computational Linguistics (ACL), pp.
8087, Sapporo, Japan.
Groves, D., Hearne, M., & Way, A. (2004). Robust sub-sentential alignment phrasestructure trees. Proceedings 20th International Conference Computational
Linguistics (COLING), pp. 10721078, Geneva, Switzerland.
Haghighi, A., Blitzer, J., DeNero, J., & Klein, D. (2009). Better word alignments
supervised itg models. Proceedings Joint Conference 47th Annual
Meeting ACL 4th International Joint Conference Natural Language
Processing AFNLP (ACL-IJCNLP), pp. 923931, Suntec, Singapore.
Huang, L., & Chiang, D. (2005). Better k-best parsing. Proceedings Ninth International Workshop Parsing Technology (IWPT), pp. 5364, Vancouver, British
Columbia, Canada.
Huang, L., Kevin, K., & Joshi, A. (2006). Statistical syntax-directed translation
extended domain locality. Proceedings 7th Conference Association
Machine Translation Americas (AMTA), pp. 6673, Cambridge, Massachusetts,
USA.
Imamura, K. (2001). Hierarchical phrase alignment harmonized parsing. Proceedings
6th NLP Pacific Rim Symposium, pp. 377384.
Jing, H. (2000). Sentence reduction automatic text summarization. Proceedings
6th Applied Natural Language Processing Conference, pp. 310315.
Johnson, M. (2007). doesnt EM find good HMM POS-taggers?. Proceedings
2007 Joint Conference Empirical Methods Natural Language Processing
Computational Natural Language Learning (EMNLP-CoNLL), pp. 296305, Prague,
Czech Republic.
Knuth, D. (1997). Art Computer Programming: Fundamental Algorithms. AddisonWesley.
Koehn, P. (2004). Statistical significance tests machine translation evaluation. Lin,
D., & Wu, D. (Eds.), Proceedings 2004 Conference Empirical Methods
Natural Language Processing (EMNLP), pp. 388395, Barcelona, Spain.
Koehn, P., Och, F., & Marcu, D. (2003). Statistical phrase-based translation. Proceedings 2003 Human Language Technology Conference North American
Chapter Association Computational Linguistics (HLT:NAACL), pp. 4854,
Edmonton, Canada.
Levenberg, A., Dyer, C., & Blunsom, P. (2012). bayesian model learning scfgs
discontiguous rules. Proceedings 2012 Joint Conference Empirical Methods Natural Language Processing Computational Natural Language Learning
(EMNLP-CoNLL), pp. 223232, Jeju Island, Korea.
Liu, D., & Gildea, D. (2009). Bayesian learning phrasal tree-to-string templates. Proceedings 2009 Conference Empirical Methods Natural Language Processing
(EMNLP), pp. 13081317, Singapore.
779

fiXiao & Zhu

Liu, Y., Liu, Q., & Lin, S. (2006). Tree-to-string alignment template statistical machine
translation. Proceedings 21st International Conference Computational
Linguistics 44th Annual Meeting Association Computational Linguistics (COLING-ACL), pp. 609616, Sydney, Australia.
Liu, Y., Lu, Y., & Liu, Q. (2009a). Improving tree-to-tree translation packed forests.
Proceedings Joint Conference 47th Annual Meeting ACL
4th International Joint Conference Natural Language Processing AFNLP
(ACL-IJCNLP), pp. 558566, Suntec, Singapore.
Liu, Y., Xia, T., Xiao, X., & Liu, Q. (2009b). Weighted alignment matrices statistical
machine translation. Proceedings 2009 Conference Empirical Methods
Natural Language Processing (EMNLP), pp. 10171026, Singapore.
Manning, C. D., & Schutze, H. (1999). Foundations Statistical Natural Language Processing. MIT Press.
Marcu, D., Wang, W., Echihabi, A., & Knight, K. (2006). Spmt: Statistical machine translation syntactified target language phrases. Proceedings 2006 Conference
Empirical Methods Natural Language Processing (EMNLP), pp. 4452, Sydney,
Australia.
Marcu, D., & Wong, D. (2002). phrase-based,joint probability model statistical machine translation. Proceedings 2002 Conference Empirical Methods
Natural Language Processing (EMNLP), pp. 133139.
Marcus, M. P., Santorini, B., & Marcinkiewicz, M. A. (1993). Building large annotated
corpus english: penn treebank. Computational Linguistics, 19, 313330.
May, J., & Knight, K. (2007). Syntactic re-alignment models machine translation.
Proceedings 2007 Joint Conference Empirical Methods Natural Language Processing Computational Natural Language Learning (EMNLP-CoNLL),
pp. 360368, Prague, Czech Republic.
Mi, H., & Huang, L. (2008). Forest-based translation rule extraction. Proceedings
2008 Conference Empirical Methods Natural Language Processing (EMNLP),
pp. 206214, Honolulu, Hawaii, USA.
Nakazawa, T., & Kurohashi, S. (2011). Bayesian subtree alignment model based dependency trees. Proceedings 5th International Joint Conference Natural Language
Processing (IJCNLP), pp. 794802, Chiang Mai, Thailand.
Neal, R. (1998). Philosophy Bayesian Inference. http://www.cs.toronto.edu/radford/
res-bayes-ex.html.
Och, F. (2003). Minimum error rate training statistical machine translation. Proceedings 41st Annual Meeting Association Computational Linguistics
(ACL), pp. 160167, Sapporo, Japan.
Och, F., & Ney, H. (2004). alignment template approach statistical machine translation. Computational Linguistics, 30, 417449.
Papineni, K., Roukos, S., Ward, T., & Zhu, W. (2002). Bleu: method automatic
evaluation machine translation. Proceedings 40th Annual Meeting
780

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

Association Computational Linguistics (ACL), pp. 311318, Philadelphia, Pennsylvania, USA.
Pauls, A., Klein, D., Chiang, D., & Knight, K. (2010). Unsupervised syntactic alignment
inversion transduction grammars. Proceedings Human Language Technologies: 2010 Annual Conference North American Chapter Association
Computational Linguistics (HLT:NAACL), pp. 118126, Los Angeles, California,
USA.
Riesa, J., Irvine, A., & Marcu, D. (2011). Feature-rich language-independent syntax-based
alignment statistical machine translation. Proceedings 2011 Conference
Empirical Methods Natural Language Processing (EMNLP), pp. 497507, Edinburgh, Scotland, UK.
Riley, D., & Gildea, D. (2010). Improving performance giza++ using variational
bayes. Tech. rep., University Rochester.
Smith, D. A., & Eisner, J. (2009). Parser adaptation projection quasi-synchronous
grammar features. Proceedings 2009 Conference Empirical Methods
Natural Language Processing (EMNLP), pp. 822831, Singapore.
Snover, M., Dorr, B., Schwartz, R., Makhoul, J., Micciula, L., & Weischedel, R. (2005).
Study Translation Error Rate Targeted Human Annotation. Tech. rep.
LAMP-TR-126,CS-TR-4755,UMIACS-TR-2005-58, University Maryland, College
Park BBN Technologies.
Sun, J., Zhang, M., & Tan, C. L. (2010a). Discriminative induction sub-tree alignment
using limited labeled data. Proceedings 23rd International Conference
Computational Linguistics (COLING), pp. 10471055, Beijing, China.
Sun, J., Zhang, M., & Tan, C. L. (2010b). Exploring syntactic structural features sub-tree
alignment using bilingual tree kernels. Proceedings 48th Annual Meeting
Association Computational Linguistics (ACL), pp. 306315, Uppsala, Sweden.
Thayer, I., Ettelaie, E., Knight, K., Marcu, D., Munteanu, D., Och, F., & Tipu, Q. (2004).
isi/usc mt system. Proceedings International Workshop Spoken Language
Translation 2004, pp. 5960.
Tinsley, J., Zhechev, V., Hearne, M., & Way, A. (2007). Robust language pair-independent
sub-tree alignment. Proceedings Machine Translation Summit XI, pp. 467474,
Copenhagen, Denmark.
Venugopal, A., Zollmann, A., Smith, N. A., & Stephan, V. (2008). Wider pipelines: n-best
alignments parses mt training. Proceedings Eighth Conference
Association Machine Translation Americas (AMTA), pp. 192201.
Vogel, S., Ney, H., & Tillmann, C. (1996). Hmm-based word alignment statistical translation. Proceedings 16rd International Conference Computational Linguistics (COLING), pp. 836841.
Wang, M., Smith, N. A., & Mitamura, T. (2007a). Jeopardy model? quasisynchronous grammar QA. Proceedings 2007 Joint Conference Empirical Methods Natural Language Processing Computational Natural Language
Learning (EMNLP-CoNLL), pp. 2232, Prague, Czech Republic.
781

fiXiao & Zhu

Wang, W., Knight, K., & Marcu, D. (2007b). Binarizing syntax trees improve syntaxbased machine translation accuracy. Proceedings 2007 Joint Conference
Empirical Methods Natural Language Processing Computational Natural
Language Learning (EMNLP-CoNLL), pp. 746754, Prague, Czech Republic.
Woodsend, K., & Lapata, M. (2011). Learning simplify sentences quasi-synchronous
grammar integer programming. Proceedings 2011 Conference Empirical Methods Natural Language Processing (EMNLP), pp. 409420, Edinburgh,
Scotland, UK.
Xiao, T., Zhu, J., Zhang, H., & Li, Q. (2012). Niutrans: open source toolkit phrasebased syntax-based machine translation. Proceedings 50th Annual Meeting Association Computational Linguistics System Demonstrations (ACL),
pp. 1924, Jeju Island, Korea.
Xue, N., Xia, F., Chiou, F.-d., & Palmer, M. (2005). penn chinese treebank: Phrase
structure annotation large corpus. Natural Language Engineering, 11, 207238.
Zhang, H., Zhang, M., Li, H., Aw, A., & Tan, C. L. (2009). Forest-based tree sequence
string translation model. Proceedings Joint Conference 47th Annual
Meeting ACL 4th International Joint Conference Natural Language
Processing AFNLP (ACL-IJCNLP), pp. 172180, Suntec, Singapore.
Zhang, M., Jiang, H., Aw, A., Li, H., Tan, C. L., & Li, S. (2008). tree sequence alignmentbased tree-to-tree translation model. Proceedings 46th Annual Meeting
Association Computational Linguistics: Human Language Techonologies
(ACL:HLT), pp. 559567, Columbus, Ohio, USA.

782


