Journal Artificial Intelligence Research 48 (2013) 1-22

Submitted 12/12; published 10/13

Natural Language Inference Arabic Using Extended Tree
Edit Distance Subtrees
Maytham Alabbas

maytham.alabbas@gmail.com

Department Computer Science, University Basrah,
Basrah, Iraq

Allan Ramsay

Allan.Ramsay@manchester.ac.uk

School Computer Science, University Manchester,
Manchester, M13 9PL, UK

Abstract
Many natural language processing (NLP) applications require computation similarities pairs syntactic semantic trees. Many researchers used tree edit
distance task, technique suffers drawback deals single node operations only. extended standard tree edit distance algorithm
deal subtree transformation operations well single nodes. extended algorithm subtree operations, TED+ST, effective flexible standard
algorithm, especially applications pay attention relations among nodes (e.g.
linguistic trees, deleting modifier subtree cheaper sum deleting
components individually). describe use TED+ST checking entailment
two Arabic text snippets. preliminary results using TED+ST encouraging compared two string-based approaches standard algorithm.

1. Introduction
Tree edit distance widely used component natural language processing (NLP)
systems attempt determine whether one text snippet supports inference another
(roughly speaking, whether first entails second), distance pairs
dependency trees taken measure likelihood one entails other.
extend standard algorithm calculating distance two trees allowing
operations apply subtrees, rather single nodes. extension improves
performance technique Arabic around 5% F-score around 4%
accuracy compared number well-known techniques. relative performance
standard techniques Arabic testset replicates results reported
techniques English testsets. applied extended version tree edit
distance, TED+ST, English RTE-2 testset, outperforms standard
algorithm.
Tree edit distance generalisation standard string edit distance metric,
measures similarity two strings. used underpin several NLP
applications information extraction (IE), information retrieval (IR) natural
language inference (NLI). edit distance two trees defined minimum
cost sequence edit operations transform one tree another. numerous
approaches calculating edit distance trees, reported Selkow (1977), Tai
c
!2013
AI Access Foundation. rights reserved.

fiAlabbas & Ramsay

(1979), Zhang Shasha (1989), Klein (1998), Demaine, Mozes, Rossman, Weimann
(2009) Pawlik Augsten (2011). chosen work Zhang-Shashas
algorithm (Zhang & Shasha, 1989) intermediate structures produced
algorithm allow us detect respond operations subtrees. refer
standard tree edit distance algorithm throughout rest article, mean ZhangShashas algorithm, use short form ZS-TED.
ultimate goal develop NLI system Arabic (Alabbas, 2011).1 NLI
problem determining whether natural language hypothesis h reasonably inferred
natural language premise p. challenges NLI quite different
encountered formal deduction: emphasis informal reasoning, lexical semantic
knowledge, variability linguistic expression, rather long chains formal
reasoning (MacCartney, 2009). recent, better-known, formulation NLI
task recognising textual entailment challenge (RTE), described Dagan Glickman
(2004) task determining, two text snippets premise p hypothesis h, whether
. . . typically, human reading p would infer h likely true. According
authors, entailment holds truth h, interpreted typical language user,
inferred meaning p. popular method used recent years
tasks use tree edit distance, compares sentence pairs finding minimal
cost sequence editing operations transform tree representation one sentence
tree (Kouylekov, 2006; Heilman & Smith, 2010). Approximate tree matching
kind allows users match parts two trees, rather demanding complete
match every element tree. However, one main drawbacks tree edit
distance transformation operations applied solely single nodes (Kouylekov,
2006). Kouylekov Magnini (2005) used standard tree edit distance, uses
transformation operations (insert, delete exchange) solely single nodes, check
entailment two dependency trees. hand, Heilman Smith
(2010) extended available operations standard tree edit distance INSERT-CHILD,
INSERT-PARENT, DELETE-LEAF, DELETE-&-MERGE, RELABEL-NODE RELABEL-EDGE.
authors identify three new operations, MOVE-SUBTREE, means move node X
tree last child left/right side node (s.t.
descendant X ), NEW-ROOT MOVE-SIBLING, enable succinct edit sequences
complex transformation. extended set edit operations allows certain combinations
basic operations treated single steps, hence provides shorter (and therefore
cheaper) derivations. fine-grained distinctions between, instance, different kinds
insertions make possible assign different weights different variations
operation. Nonetheless, operations continue operate individual nodes rather
subtrees (despite name, even MOVE-SUBTREE appears defined operation
nodes rather subtrees). solved problem extending basic
version algorithm costs operations insert/delete/exchange subtrees
derived appropriate function costs operations parts.
makes TED+ST effective flexible standard algorithm, especially
applications pay attention relations among nodes (e.g deleting modifier subtree,
linguistic trees, cheaper sum deleting components individually).
1. particular, Modern standard Arabic (MSA). refer Arabic throughout article,
mean MSA.

2

fiNatural Language Inference Arabic

rest paper organised follows: Zhang-Shashas algorithm, ZS-TED,
explained Section 2. Section 3 presents TED+ST. Section 4 describes dependency trees
matching. Dataset preparation explained Section 5. experimental results
discussed Section 6. Conclusions given Section 7.

2. Zhang-Shashas TED Algorithm
approach extends ZS-TED, uses dynamic programming provide O(n4 )
algorithm finding optimal sequence node-based edit operations transforming
one tree another. section contains brief recapitulation algorithma
detailed description given Bille (2005).
Ordered trees trees left-to-right order among siblings significant.
Approximate tree matching allows us match tree parts another tree.
three operations, namely deleting, inserting exchanging node,
transform one ordered tree another. nonnegative real cost associated
edit operation. costs changed match requirements specific applications.
Deleting node x means attaching children parent x. Insertion inverse
deletion. means inserted node becomes parent consecutive sub-sequence
left right order parent. Exchanging node alters label. editing
operations illustrated Figure 1 (Bille, 2005).

(a)

l1

l2

(b)

l1

l1

l2

(b)

l1

l1
l2

Figure 1: (a) Relabeling node label (l1 l2 ). (b) Deleting node labeled (l2 ).
(c) Inserting node labeled l2 child node labeled l1 ( l2 ).

operation associated cost allowed single nodes only. Selecting
good set costs operations hard dealing complex problems.
3

fiAlabbas & Ramsay

alterations costs choosing different combination lead
drastic changes tree edit distance performance (Mehdad & Magnini, 2009).
ZS-TED algorithm, tree nodes compared using postorder traversal,
visits nodes tree starting leftmost leaf descendant root proceeding
leftmost descendant right sibling leaf, right siblings,
parent leaf tree root. last node visited always
root. example postorder traversal leftmost leaf descendant tree
shown Figure 2. figure, two trees, T1 m=7 nodes T2 n=7
nodes. subscript node considered order node postorder
tree. So, postorder T1 e,f,b,g,c,d,a postorder T2 g,c,y,z,x,d,a.
leftmost leaf descendant subtrees T1 headed nodes e,f,b,g,c,d,a
1,2,1,4,4,6,1 respectively, similarly leftmost leaf descendants g,c,y,z,x,d,a T2
1,1,3,4,3,3,1.
a7

a7
c5

b3
e1

f2

d6

g4

c2

d6

g1

x5
y3

T1

z4

T2

Figure 2: Two trees T1 T2 postorder traversal.
descendants node, least cost mapping calculated
node encountered, order least cost mapping selected right away.
achieve this, algorithm pursues keyroots tree, defined
set contains root tree plus nodes left sibling. Concentrating
keyroots critical dynamic nature algorithm, since subtrees
rooted keyroots allow problem split independent subproblems
general kind. keyroots tree decided advance, permitting algorithm
distinguish tree distance (the distance two nodes considered
context left siblings trees T1 T2 ) forest distance (the distance
two nodes considered separately siblings ancestors
descendants) (Kouylekov, 2006). illustration, keyroots tree Figure 2
marked bold.
node, computation find least cost mapping (the tree distance)
node first tree one second depends solely mapping nodes
children. find least cost mapping node, then, one needs recognise least cost
mapping keyroots among children, plus cost leftmost child.
nodes numbered according postorder traversal, algorithm proceeds
following steps (Kouylekov, 2006): (i) mappings leaf keyroots determined;
(ii) mappings keyroots next higher level decided recursively; (iii)
root mapping found. Algorithm 1 shows pseudocode ZS-TED algorithm (Zhang
& Shasha, 1989). matrices F used recording results individual
4

fiNatural Language Inference Arabic

subproblems: used store tree distance trees rooted pairs nodes
two trees, F used store forest distance sequences nodes.
F used temporary store tree edit distance pairs keyroots
calculated. extended standard algorithm, computes cost
cheapest edit sequence, records edit operations themselves.
involves adding two new matrices, DPATH FDPATH, hold appropriate sequences
edit operationsDPATH hold edit sequences trees rooted pairs nodes
FDPATH hold edit sequences forests. DPATH permanent arrays,
whereas F FDPATH reinitialised pair keyroots.
algorithm iterates keyroots, split two main stages pair
keyroots: initialisation phase (lines 312) deals first row column,
assume every cell first row reached appending insert operation
cell left every cell first column reached appending delete operation
cell it, appropriate costs. exactly parallel initialisation
standard dynamic time warping algorithm calculating string edit distance, though
treating task matching subsets subtrees rooted T1 [x] T2 [y]
string matching problem nodes two trees sequences enumerated
post-order.
second stage (lines 1337) traces cost edit sequence transforming
sub-sequence sequence nodes dominated T1 [x] sub-sequence sequence
nodes dominated T2 [x], considering whether nodes reached
cell left insert, cell delete, cell diagonally
left either match exchange x. two cases considered
here:
two sequences consideration trees (tested line 15), know
considered every possible way exchanging one other, hence
record cost F D, edit sequence FDPATH
DPATH. case, calculate cost moving along diagonal inspection
two nodes. See Figure 3 illustration notion.
ii one sequences forest retrieve cost moving along diagonal
DPATH, store cost F edit sequence FDPATH.
cases, gather set {cost, path} pairs result considering insert/delete/exchange operations preceding sub-sequences, choose best
pair store various arrays. similar corresponding element
string edit algorithm, added complication calculating tree edit costs
sequences pair keyroots involves calculating costs edit sequences
pairs sub-sequences nodes roots. results pairs keyroots
stored permanently, utilised calculations sub-sequences next
stage.
Bille (2005) provides detailed worked examples calculation costs transforming one tree another. Figure 4 shows FDPATH grows algorithm iterates
keyroots trees T1 T2 Figure 2. figure, cells representing
5

fiAlabbas & Ramsay

Algorithm 1 pseudocode Zhang-Shashas TED algorithm edit sequences
[i, j]
ith jth nodes post-order enumeration tree (T [i, i] written [i])
l(i)
leftmost leaf descendant subtree rooted
K(T )
keyroots tree T, K(T ) = {k : k1 > k l(k1 ) = l(k)}
D[i, j]
tree distance two nodes T1 [i] T2 [j]
F D[T1 [i, i1 ], T2 [j, j1 ]]
forest distance nodes i1 T1 nodes j j1 T2
DP H[i, j]
edit sequence trees rooted two nodes T1 [i] T2 [j]
F DAT H[T1 [i, i1 ], T2 [j, j1 ]] edit sequence forests covered nodes i1 T1 nodes j j1 T2
(T1 [i] )
cost deleting ith node T1
( T2 [j])
cost inserting jth node T2 T1
(T1 [i] T2 [j])
cost exchanging ith node T1 jth node T2
m, n
number nodes T1 T2 respectively
best
choose best cost path set options
1: x 1 |K1 (T1 )|
2:
1 |K2 (T2 )|
3:
F D[, ] 0
4:
F DP H[, ]
5:
l1 (x) x
6:
F D[T1 [l1 (x), i], ] F D[T1 [l1 (x), i-1], ] + (T1 [i] )
7:
F DP H[T1 [l1 (x), i], ] F DP H[T1 [l1 (x), i-1], ] +
8:
end
9:
j l2 (y)
10:
F D[, T2 [l2 (y), j]] F D[, T2 [l2 (y), y-1]] + ( T2 [j])
11:
F DP H[, T2 [l2 (y), j]] F DP H[, T2 [l2 (y), y-1]] +
12:
end
13:
l1 (x) x
14:
j l2 (y)
15:
(l1 (i) == l1 (x) l2 (j) == l2 (y))
16:
cost, path best({F D[T1 [l1 (x), i-1], T2 [l2 (y), j]] + (T1 [i] ),
17:
F DP H[T1 [l1 (x), i-1], T2 [l2 (y), j]] + d},
18:
{F D[T1 [l1 (x), i], T2 [l2 (y), j-1]] + ( T2 [j]),
19:
F DP H[T1 [l1 (x), i], T2 [l2 (y), j-1]] + i},
20:
{F D[T1 [l1 (x), i-1], T2 [l2 (y), j-1]] + (T1 [i] T2 [j])),
21:
F DP H[T1 [l1 (x), i-1], T2 [l2 (y), j-1]] + m/x})
22:
F D[T1 [l1 (x), i], T2 [l2 (y), j]] cost
23:
D[i, j] cost
24:
F DP H[T1 [l1 (x), i], T2 [l2 (y), j]] path
25:
DP H[i, j] path
26:
else
27:
cost, path best({F D[T1 [l1 (x), i-1], T2 [l2 (y), j]] + (T1 [i] ),
28:
F DP H[T1 [l1 (x), i-1], T2 [l2 (y), j]] + d},
29:
{F D[T1 [l1 (x), i], T2 [l2 (y), j-1]] + ( T2 [j]),
30:
F DP H[T1 [l1 (x), i], T2 [l2 (y), j-1]] + i},
31:
{F D[T1 [l1 (x), i-1], T2 [l2 (y), j-1]] + D[i, j]),
32:
F DP H[T1 [l1 (x), i-1], T2 [l2 (y), j-1]] + DP H[i][j]})
33:
F D[T1 [l1 (x), i], T2 [l1 (y), j]] cost
34:
F DP H[T1 [l1 (x), i], T2 [l1 (y), j]] path
35:
end
36:
end
37:
end
38:
end
39: end
40: return D[n, m], DP H[n, m]

6

fiNatural Language Inference Arabic

i-1,j-1

i,j-1

i-1,j
x/m





i,j

Figure 3: edit operation direction used algorithm. arc implies edit
operation labeled: insertion, deletion, x exchanging
operation (matching).

optimal sequence edit operations transform T1 T2 highlighted bold,
final optimal path shown last cell (at final row column).
T1
e
f
b
g
c



T2

dd
ddd
dddd
ddddd
dddddd
ddddddd

g

x
xd
xdd
dddm
dddmd
dddmdd
dddmddd

c
ii
xi
xid
xdx
dddmi
dddmm
dddmmd
dddmmdd


iii
iix
xix
xdxi
xdxx
dddmmi
dddmmx
dddmmxd

z
iiii
iiix
iixx
iixxd
xdxxi
dddmmii
dddmmxi
dddmmxid

x
iiiii
iiixi
xiiix
iixxx
xdxixi
dddmmiii
dddmmxii
dddmmxiid


iiiiii
iiixii
xiiiix
iixxxi
xdxixii
xdxixix
dddmmiiim
dddmmiiimd


iiiiiii
iiixiii
xiiiixi
iixxxii
iixxxiid
xdxixixi
dddmmiiimi
dddmmiiimm

FDPATH

Figure 4: Computing optimal path trees Figure 2.
mapping two trees found final sequence edit operations
mapping nodes corresponding match operation only.
final distance 6 represents final values (at final row column) D.2
last value DPATH represents final sequence edit operations, namely dddmmiiimm. According path, define alignment two postorder trees.
alignment two trees T1 T2 obtained inserting gap symbol (i.e. _)
either T1 T2 , according type edit operation, resulting strings 1
2 length sequence edit operations. gap symbol inserted
2 edit operation delete (d), whereas inserted 1 edit
operation insert (i). Otherwise, nodes T1 T2 inserted 1 2
respectively. following optimal alignment T1 T2 :
1:
2:

e

_

f

_

b

_

g

g

c

c

_



_

z

_

x









2. simplicity here, assume single operation cost 1 except matching cost
0, described Zhang Shasha (1989).

7

fiAlabbas & Ramsay

means:
d:
d:
d:
m:
m:
i:
i:
i:
m:
m:

Delete (e) T1
Delete (f ) T1
Delete (b) T1
Leave (g) without change
Leave (c) without change
Insert (y) T1
Insert (z ) T1
Insert (x ) T1
Leave (d) without change
Leave (a) without change

final mapping T1 T2 shown Figure 5. mapping figure
insertion, deletion, matching exchanging operations shown single, double,
single dashed double dashed outline respectively. matching nodes (or subtrees)
linked dashed arrows.

a7
c5

b3
e1

f2

a7
d6

g4

c2

d6

g1

x5
y3

T1

z4

T2

Figure 5: ZS-TED, mapping T1 T2 .

3. Extended TED Subtree Operations
main weakness ZS-TED algorithm able perform transformations
subtrees (i.e. delete subtree, insert subtree exchange subtree). output ZSTED lowest cost sequence operations single nodes. extend find
lowest cost sequence operations nodes subtrees, TED+ST, follows:
1. Run ZS-TED compute standard alignment results (Algorithm 1);
2. Go alignment group subtree operations. sequence identical
operations applies set nodes comprising subtree, replaced
8

fiNatural Language Inference Arabic

single operation, whose cost determined appropriate function costs
individual nodes (Algorithm 2). variety functions could applied here,
depending application. using algorithm textual entailment
use costs Figure 8, derived used Punyakanok, Roth,
Yih (2004), illustration current section simply take cost
subtree operation half sum costs individual operations
make up.
noted apply technique modify Zhang-Shashas
O(n4 ) algorithm, could applied algorithm finding tree edit distance,
e.g. Kleins O(n3 logn ) algorithm (Klein, 1998), Demaine et al. O(n3 ) algorithm (Demaine,
Mozes, Rossman, & Weimann, 2009) Pawlik Augsten O(n3 ) algorithm (Pawlik &
Augsten, 2011), since extension operates output original algorithm.
additional time cost O(n2 ) negligible since less time cost available
tree edit distance algorithm.
3.1 Find Sequence Subtree Edit Operations
Extending ZS-TED cover subtree operations give us flexibility comparing
trees (especially linguistic trees). key algorithm find maximal
sequences identical edit operations correspond subtrees. sequence nodes
postorder corresponds subtree following conditions satisfied: (i) first
node leaf; (ii) leftmost sibling last node sequence (i.e. root
subtree) first node sequence. two conditions
checked constant time, since leftmost sibling node determined node
advance. hence find maximal sequences corresponding subtrees scanning
forwards sequence node operations find sequences identical operations,
scanning backwards sequence find point
covers subtree. involves potentially O(n2 ) stepsn forward steps find sequences
identical operations, possibly n-1 backward steps time find sub-sequences
corresponding subtrees. example, sequence nodes e,f,b tree T1 Figure 2
subtree e leaf leftmost last node b 1, represents
first node e. hand, sequence nodes g,c,d tree
subtree g leaf, leftmost last node 6, represents itself,
first node g.
Algorithm 2 contains pseudocode find optimal sequence single subtree
edit operations transforming T1 T2 . Ep=1..L {d, i, x, m} algorithm
optimal sequence node edits transforming T1 T2 , obtained applying
technique Section 2, 1 2 alignments T1 T2 obtained
applying sequence node edits.
shown Algorithm 2, find optimal single subtree edit operations sequence
transforms T1 T2 , maximal sequence identical operations checked see
whether contains subtree(s) not. Checking whether sequence corresponds
subtree depends type edit operation, according following rules: (i)
operation d, sequence checked first tree; (ii) operation i,
sequence checked second tree; (iii) otherwise, sequence checked
9

fiAlabbas & Ramsay

Algorithm 2 pseudocode find subtree edit operations
E
L
S1, S2

sequence edit operations transform tree T1 tree T2 , Ep=1..L {d,i,x,m}
length sequence edit operations E
optimal alignment T1 T2 respectively, length 1 = 2 = L

1: repeat
2:
ERoot EL
3:
F L
4:
repeat
5:
(F 2 EF 1 == ERoot)
6:
F F 1
7:
end
8:
(F == L)
9:
LL1
10:
ERoot EL
11:
F L
12:
end
13:
(F < L F 2 EF 1 )= ERoot) (L = 0)
14:
F0 F
15:
(F < L)
16:
(F < L)
17:
IsSubtree true
18:
(F < L IsSubtree)
1
19:
(ERoot =d SF1 ..SL
subtree)
2
2
20:
(ERoot =i SF ..SL subtree)
1
2
21:
((ERoot {x,m}) (SF1 ..SL
SF2 ..SL
subtrees))
22:
Replace EF ..EL1 +
23:
LF 1
24:
F F0
25:
else
26:
IsSubtree f alse
27:
end
28:
end
29:
F F +1
30:
end
31:
L L1
32:
F F0
33:
end
34:
L F0 1
35: (L 0)
36: return E

trees. that, sequence operations corresponds subtree, symbols
sequence replaced + except last one (which represents root
subtree). Otherwise, checking starts sub-sequence original, explained below.
instance, let us consider Eh , ..., Et , 1 h < L, 1 < L, h < t, sequence
edit operation, i.e. Ek=h..t {d, i, x, m}. Let us consider h0 = h, firstly
check nodes Sh1 , ..., St1 Sh2 , ..., St2 see whether heads subtrees.
Ek d, nodes Sh1 , ..., St1 checked, nodes Sh2 , ..., St2 checked,
otherwise, nodes Sh1 , ..., St1 Sh2 , ..., St2 checked. edit operations Eh , ..., Et1
replaced + sequence corresponds subtree. Then, start checking
beginning another sequence left subtree Eh , ..., Et , i.e. = h 1.
10

fiNatural Language Inference Arabic

Otherwise, checking applied sequence starting next position, i.e.
h = h+ 1. checking continued h = t. that, (t h) sequences
start different positions end position contain subtree, checking
starts beginning new sequence, i.e. h = h0 = 1. process
repeated h = t.
explain subtree operations applied, let us consider two trees T1
T2 Figure 2.
According TED+ST, cost 3 sequence operation follows:
sequence d, result. sequences consist three subtrees
(i.e. three deleted nodes, first two matched nodes three inserted nodes):
ddd mm iii mm. So, final result is: ++d +m ++i mm. means:
++d:
+m:
++i:
m:
m:

Delete subtree (e,f,b) T1
Leave subtree (g,c) without change
Insert subtree (y,z,x ) T1
Leave (d) without change
Leave (a) without change

final mapping T1 T2 obtained using TED+ST shown Figure 6.

a7
c5

b3
e1

f2

a7
d6

g4

c2

d6

g1

x5
y3

T1

z4

T2

Figure 6: TED+ST, mapping T1 T2 .

4. Matching Dependency Trees
mentioned above, main goal design textual entailment (TE) system Arabic
check whether one text snippet (i.e. premise p) entails another text (i.e. hypothesis h).
match p h dependency tree pairs effectively, use TED+ST. enables us
find minimum edit operations transform one tree another. allows us
sensitive fact links dependency tree carry linguistic information
relations complex units, hence ensure paying attention
relations compare two trees. instance, enables us pay attention
11

fiAlabbas & Ramsay

fact operations involving modifiers, particular, applied subtree
whole rather individual elements. Thus, transform tree D1 tree D2
Figure 7 deleting park single operation, removing modifier whole,
rather three operations removing in, park one one, using costs
Figure 8 initial test edit operations experiments. costs
updated version costs used Punyakanok et al. (2004).3 authors found
using tree edit distance gives better results bag-of-word scoring methods,
applied question answering.4
saw

saw




man





park

man



D1

D2

Figure 7: Two dependency trees, D1 D2 .

using costs Figure 8, cost transferring D1 D2 according ZS-TED
19 (i.e. one stop word (5) two words (14)), whereas according TED+ST
operations 0. Therefore, easy decide D1 entails D2 , whereas reverse
true. exploited subset/superset relations encoded Arabic WordNet (AWN)
(Black, Elkateb, Rodriguez, Alkhalifa, Vossen, Pease, & Fellbaum, 2006) comparing
items tree. Roughly speaking, comparing one tree another requires us swap
two lexical items, happier item source tree synonym
hyponym one target treesince wombat hyponym animal, swapping
wombat premise saw wombat zoo animal saw animal
zoo truth-preserving exchange.
Approaches make use lexical relations kind cope fact
words often multiple meanings. follow Hobbs (2005) assuming W1
sense hyponym sense W2 sentence involving W1 entail
similar sentence involving W2 shown (1).
(1) p.
h.

saw peach yesterdays party.
saw attractive woman yesterdays party.

3. stop words list contains common Arabic words (e.g. particle
( +,- ./0# "#! $ Almdyr mwl director indeed busy
"#! $ indeed). instance, %&'! )*
( +,- ./0# Almdyr mwl director busy.
entails %&'! )*
4. transcription Arabic examples document follows Habash-Soudi-Buckwalter (HSB) transliteration scheme (Habash, Soudi, & Buckwalter, 2007) transcribing Arabic symbols.

12

fiNatural Language Inference Arabic

Cost

Single node

Subtree (more one node)

Delete:

X stop word cost 5,
else cost 7
stop word cost
5,
else cost 100
X subsumed cost 0,
elseif X stop word cost 5,
elseif subsumed (or
antonym of) X cost 100
else cost 50

0

Insert:

Exchange:

double sum costs parts

S1 identical S2 cost 0
else half sum costs parts

Figure 8: Edit operation costs.
(1p), instance, word peach ambiguous,5 shade pink tinged yellow
(hypernym: Pink) Downy juicy fruit sweet yellowish whitish flesh (hypernym:
Drupe, edible fruit, stone fruit) attractive seductive looking woman (hypernym:
Adult female, women) cultivated temperate regions (hypernym: Fruit tree).
context (1h), however, human reader would assume second interpretation
peach intended, despite fact general fairly unusual usage.
reflects widely accepted view contextual information key lexical
disambiguation. Within RTE task, premise provides context disambiguation
hypothesis, hypothesis provides context disambiguation premise.
Almost human reader would, instance, accept (2p) entails (2h), despite
potential ambiguity word bank.
(2) p.
h.

money tied bank.
cannot easily spend money.

5. Dataset Preparation
order train test TE system Arabic, need appropriate dataset.
knowledge, datasets available Arabic, develop one.
followed one procedures used collecting premise-hypothesis pairs
RTE tasks, slight alteration. premises RTE collected variety
sources, e.g. newswire text. contain one two sentences tend fairly long
(e.g. averaging 25 words RTE1, 28 words RTE2, 30 words RTE3 39 words
RTE4). contrast, hypotheses quite short single sentences (averaging 11 words
RTE1, 8 words RTE2 7 words RTE3 RTE4), manually constructed
premise. first three RTE Challenges presented binary classification
task yes balanced numbers yes problems. Beginning RTE4,
three-way classifications (yes, no, contradict, distinguish cases
h contradicts p h compatible with, entailed p).
5. See Sages dictionary online: http://www.sequencepublishing.com/thesageonline.php. WordNet
provides senses (and more) peach.

13

fiAlabbas & Ramsay

dataset, want produce set p-h pairs handpartly
lengthy tedious process, importantly hand-coded datasets liable
embody biases introduced developer. dataset used training system,
rules extracted little unfolding information explicitly
supplied developers. used testing test examples
developers chosen, likely biased, albeit unwittingly, towards
way think problem.
set Arabic p-h pairs TE task created semi-automatic technique
two stages. first stage (Section 5.1) responsible automatically collecting
p-h pairs news websites, second stage (Section 5.2) uses online annotation
system allows annotators annotate collected pairs manually. stages
explained detail below.
5.1 Collecting p-h Pairs
collected candidate p-h pairs automatically so-called headline-lead paragraph
technique (Burger & Ferro, 2005) web (e.g. newspaper corpora, pairing
first paragraph article, p, headline, h). based observation
news articles headline often partial paraphrase first paragraph
article, conveying thus comparable meaning. use updated version headlinelead paragraph strategy improve quality p-h pair.
key idea pose queries search engine automatically filter
responses text snippets might entail query. pairs manually annotated entailment/non-entailment, texts automatically collected
freely occurring natural texts. eliminates possibility (indeed likelihood)
unconscious bias introduced hypotheses manually generated.
built corpus p-h pairs using headlines websites Arabic newspapers
TV channels queries input Google via standard Google API,
selecting first paragraph, usually represents related text snippet(s)
article headline (Burger & Ferro, 2005), first 10 returned pages.
technique produces large number potential pairs without bias either
premises hypotheses. improve quality pairs resulted query,
use two conditions filter results: (i) length headline must least five
words, avoid small headlines; (ii) fewer 80% words headline
appear premise, avoid similar sentences.
problem p h similar would little
learn used training phase TE system; would
almost worthless test pairvirtually TE system get pair right,
serve discriminatory test pair. therefore eliminate excessively similar
p-h pairs training testing, assess terms number shared
uncommon words.
order overcome problem, matched headlines one source stories
another. Major stories typically covered range outlets, usually variations emphasis wording. Stories different sources linked looking
common words headlinesit unlikely two stories about, in14

fiNatural Language Inference Arabic

stance, neanderthals news time, straightforward matching based
low frequency words proper names likely find articles topic.
terminology structure first text snippets articles, however, likely
quite different. Thus using headline one source first text snippet
article story another source likely produce p-h pairs
unduly similar. therefore link headline one newspaper related
sentences another.
5.2 Annotating p-h Pairs
pairs collected first stage still marked-up human annotators,
least process collecting nearly bias-free possible. pairs cover
number subjects politics, business, sport general news. annotation
performed eight expert non-expert human annotators identify different pairs
positive entailment examples yes, p judged entails h, negative examples
no, entailment hold. annotators follow nearly annotation
guidelines used building RTE task dataset (Dagan, Glickman, & Magnini,
2006).
pair annotated three annotators. inter-annotator agreement (where
annotators agree) around 74% compared 89% annotator agrees
least one co-annotator. suggests annotators found difficult task.
fact 74% agreement annotations produced three independent
annotators taken account sets upper bound reasonable expect
automatic system carrying task. human annotators agree
three quarters cases, unlikely computer-based system achieve
much 75% agreement given pair annotators.6

6. Experiments
check effectiveness TED+ST, used check entailment p-h
Arabic pairs text snippets compared results two string-based approaches
(bag-of-words Levenshtein distance) ZS-TED set pairs. Checking
whether one Arabic text snippet entails another, however, particularly challenging
Arabic ambiguous languages, English. instance, Arabic
written without diacritics (short vowels), often leading multiple ambiguities. makes
morphological analysis difficult (i.e. single written form may easily correspond
many ten different lexemes, see Alabbas & Ramsay, 2011a, 2011b, 2012a, 2012c).
preliminary testing dataset contains 600 pairs, binary annotated yes (a 50-50
split) using technique explained Section 5. distribution pairs p length
summarised Table 1, h average length around 10 words average
common words p h around 4 words. average length sentence
dataset 25 words per sentence, sentences containing 40+ words.
6. dataset, including dependency-tree analysis CoNLL format, available online appendices article http://www.cs.man.ac.uk/~ramsay/ArabicTE/

15

fiAlabbas & Ramsay

ps length
<20
20-29
30-39
>39
Total

#pairs
175
329
87
9
600

yes
83
171
43
3
300


92
158
44
6
300

Table 1: Distribution sentence lengths testset.

order check entailment p-h pairs, follow three steps. First,
sentence preprocessed tagger parser order convert elements p-h
pair dependency trees. dependency tree tree words vertices syntactic
relations dependency relations. vertex therefore single parent, except
root tree. dependency relation holds dependent, i.e. syntactically
subordinate vertex, head, i.e. another vertex dependent. Thus
dependency structure represented head-dependent relation vertices
classified dependency types SBJ subject, OBJ object, ATT attribute, etc.
carried number experiments state-of-the-art taggers
AMIRA (Diab, 2009), MADA (Habash, Rambow, & Roth, 2009) in-house maximumlikelihood (MXL) tagger (Ramsay & Sabtan, 2009) parsers MALTParser (Nivre,
Hall, Nilsson, Chanev, Eryigit, Kbler, Marinov, & Marsi, 2007) MSTParser (McDonald, Lerman, & Pereira, 2006).7 experiments show particular merging MADA
(97% accuracy) MSTParser gives better results (around 81% labelled accuracy)
tagger:parser combinations (Alabbas & Ramsay, 2012b). therefore use
MADA+MSTParser current experiments.
converting p-h pairs dependency trees, matched dependency trees using
ZS-TED TED+ST algorithms, two string-based algorithms (bag-of-words
Levenshtein distance) provide baseline. tree edit distance algorithms used edit
operation costs defined Figure 8 find cost matching p-h pairs.
bag-of-words measures similarity p h number common words
(either surface forms lemma forms), divided length h.
four algorithms use AWN lexical resource order take account synonymy
hyponymy relations calculating cost edit.
carried two kinds experiments using algorithms: first simple
yes/no experiment, using single threshold decide whether premise similar enough
hypothesis safe say entailed it, second two thresholds
could say yes/dont know/no. results experiments given below.

7. parsers data-driven dependency parsers. Arabic usually trained Arabic
dependency treebank, Prague Arabic Dependency Treebank (PADT) (Smr, Bielicky, Kouilov,
Krmar, Haji, & Zemnek, 2008), version Penn Arabic Treebank (PATB) (Maamouri
& Bies, 2004) converted dependency trees: scoring parsers matter counting
dependency links.

16

fiNatural Language Inference Arabic

6.1 Binary Decision (yes no)
p entails h cost matching less (more case bag-of-words) threshold.
results experiments, terms precision (P), recall (R) F-score (F)
yes class overall accuracy, shown Table 2. table shows substantial
improvement obtained using TED+ST bag-of-words (F-score TED+ST
around 1.16 times F-score bag-of-words, accuracy 1.09 times better)
ZS-TED (around 1.06 times better F-score 1.04 times better total accuracy).
Method
Bag-of-words
Levenshtein distance
ZS-TED
TED+ST

Pyes
63.5%
64.7%
65.9%
69.7%

Ryes
43.7%
44.1%
51.2%
54.5%

Fyes
0.518
0.525
0.576
0.612

Accuracy
59.3%
60.2%
62.5%
65.5%

Table 2: Performance TED+ST compared string-based algorithms ZS-TED,
binary decision.
Although primarily interested Arabic, carried parallel sets experiments English RTE2 testset, using Princeton English WordNet (PWN)
resource deciding whether word premise may exchanged one hypothesis. tree edit distance algorithms work dependency tree analyses input texts, used set analysed using Minipar (Lin, 1998), downloaded
http://u.cs.biu.ac.il/~nlp/RTE2 Datasets/RTE2 Preprocessed Datasets.html.
RTE2 testset contains around 800 p-h pairs, number Minipar analyses
multiple heads hence correspond well-formed trees,
number cases segmentation algorithm used produces multi-word expressions. eliminating problematic pairs kind left 730 pairs, split
evenly positive negative examples. Since mainly concerned
difference ZS-TED TED+ST, omitted Levenshtein distance
simply kept basic bag-of-words algorithm baseline. Previous authors
shown tree edit distance consistently outperforms string-based approaches
dataset, need replicate result here.
Method
Bag-of-words
ZS-TED
TED+ST

Pyes
53.2%
52.9%
53.2%

Ryes
50.1%
62.5%
66.8%

Fyes
0.516
0.573
0.59

Accuracy
52.1%
53.5%
55.8%

Table 3: Performance TED+ST compared simple bag-of-words ZS-TED,
binary decision, RTE2 dataset.
pattern Table 3 similar Table 2. ZS-TED better bag-of-words,
TED+ST improvement ZS-TED. experiments textual entailment
tasks report accuracy: certain situations may important decisions
trustworthy (high precision, Table 2) sure captured
17

fiAlabbas & Ramsay

many positive examples possible (high recall8 , Table 3), good balance
(high F-score). easy change balance precision recall,
simply changing threshold used determining whether safe say
p entails hwe could chosen thresholds Table 3 increased precision
decreased recall, results closely matched Table 2. key point
sets experiments, F-scores improve move string-based
measures ZS-TED use TED+ST; remarkably
similar two datasets, despite fact collected different means,
different languages, parsed using different parsers.
6.2 Making Three-way Decision (yes, unknown)
task use two thresholds, one trigger positive answer cost matching
lower lower threshold (exceeds higher one bag-of-words algorithm)
trigger negative answer cost matching exceeds higher one (mutatis
mutandis bag-of-words). Otherwise, result unknown. reason making
three-way decision drive systems make precise distinctions. Note
distinguishing {h entails p, h p compatible, h contradicts p},
{h entails p, dont know whether h entails p, h entail p}.
subtle distinction, reflecting systems confidence judgement,
extremely useful deciding act decision.
results experiment, terms precision (P), recall (R) F-score (F),
shown Table 4. Again, shows large improvement using TED+ST
bag-of-words (F-score around 1.10 times better) ZS-TED (F-score around 1.06 times
better).
Method
Bag-of-words
Levenshtein distance
ZS-TED
TED+ST

P
58.9%
61.4%
65.1%
67.4%

R
56.7%
58.0%
56.0%
60.2%

F
0.578
0.597
0.602
0.636

Table 4: Performance TED+ST compared string-based algorithms ZS-TED,
three-way decision.
scores three-way decision RTE2 dataset lower Arabic
dataset, TED+ST outperforms ZS-TED three measures.

7. Conclusion
presented extended version, TED+ST, tree edit distance solved
one main drawbacks standard tree edit distance, supports
8. might useful, instance, TED+ST used low cost filter question-answering
system, results query search engine might filtered TED+ST passed
system employing full semantic analysis deep reasoning, high precision
time-consuming.

18

fiNatural Language Inference Arabic

Method
Bag-of-words
ZS-TED
TED+ST

P
50.8%
52.3%
54.3%

R
48.3%
50.2%
52.7%

F
0.495
0.512
0.535

Table 5: Performance TED+ST compared simple bag-of-word ZS-TED,
three-way decision, RTE2 dataset.

edit operations (i.e. delete, insert exchange) single nodes. TED+ST deals
subtree transformation operations well operations single nodes: leads useful
improvements performance standard algorithm determining entailment.
key subtrees tend correspond single information units. treating
operations subtrees less costly corresponding set individual node operations,
TED+ST concentrates entire information units, appropriate granularity
individual words considering entailment relations.
current findings, preliminary, quite encouraging. fact results
original testset, particularly improvement F-score, replicated testset
control parser used produce dependency trees
p-h pairs provides evidence robustness approach. anticipate
cases accurate parser (our parser Arabic attains around 81%
accuracy PATB, Minipar reported attain 80% Suzanne corpus)
would improve performance ZS-TED TED+ST.
currently experimenting different scoring algorithms ZS-TED TED+
ST. performance variant tree edit distance depends critically costs
various operations, thresholds used deciding whether h entails
p, therefore investigating use various optimisation algorithms choosing
weights thresholds. intend use Arabic lexical resources,
OpenOffice Arabic dictionary MS Word Arabic dictionary, provide us
information relations words, information AWN,
useful, sparse comparison PWN (Habash, 2010).

Acknowledgments
would thank reviewers valuable comments, particular reviewer
suggested evaluating approach English dataset well Arabic one.
extra work provided support belief robustness approach
degree anticipate.
would extend thanks annotators time effort
put annotating experimental dataset. Maytham Alabbas owes deepest gratitude
Iraqi Ministry Higher Education Scientific Research financial support
PhD study. Allan Ramsays contribution work partially supported Qatar
National Research Fund (grant NPRP 09 - 046 - 6 - 001).
19

fiAlabbas & Ramsay

References
Alabbas, M. (2011). ArbTE: Arabic textual entailment. Proceedings Second Student
Research Workshop associated RANLP 2011, pp. 4853, Hissar, Bulgaria. RANLP
2011 Organising Committee.
Alabbas, M., & Ramsay, A. (2011a). Evaluation combining data-driven dependency
parsers Arabic. Proceeding 5th Language & Technology Conference: Human
Language Technologies (LTC11), pp. 546550, Pozna, Poland.
Alabbas, M., & Ramsay, A. (2011b). Evaluation dependency parsers long Arabic
sentences. Proceeding International Conference Semantic Technology
Information Retrieval (STAIR11), pp. 243248, Putrajaya, Malaysia. IEEE.
Alabbas, M., & Ramsay, A. (2012a). Arabic treebank: phrase-structure trees dependency trees. META-RESEARCH Workshop Advanced Treebanking 8th
International Conference Language Resources Evaluation (LREC), pp. 6168,
Istanbul, Turkey.
Alabbas, M., & Ramsay, A. (2012b). Combining black-box taggers parsers modern standard Arabic. Federated Conference Computer Science Information
Systems (FedCSIS-2012), pp. 19 26, Wroclaw, Poland. IEEE.
Alabbas, M., & Ramsay, A. (2012c). Improved POS-tagging Arabic combining diverse
taggers. Proceedings 8th Artificial Intelligence Applications Innovations
(AIAI), pp. 107116, Halkidiki, Greece. Springer.
Bille, P. (2005). survey tree edit distance related problems. Theoretical Computer
Science, 337 (1-3), 217239.
Black, W., Elkateb, S., Rodriguez, H., Alkhalifa, M., Vossen, P., Pease, A., & Fellbaum, C.
(2006). Introducing Arabic WordNet project. Proceedings 3rd International WordNet Conference (GWC-06), pp. 295299, Jeju Island, Korea.
Burger, J., & Ferro, L. (2005). Generating entailment corpus news headlines.
Proceedings ACL Workshop Empirical Modeling Semantic Equivalence
Entailment, pp. 4954, Ann Arbor, Michigan, USA. Association Computational
Linguistics.
Dagan, I., & Glickman, O. (2004). Probabilistic textual entailment: generic applied modeling language variability. PASCAL Workshop Learning Methods Text
Understanding Mining, pp. 2629, Grenoble, France.
Dagan, I., Glickman, O., & Magnini, B. (2006). PASCAL recognising textual entailment challenge. Quionero-Candela, J., Dagan, I., Magnini, B., & dAlch Buc, F.
(Eds.), Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, Recognising Tectual Entailment, Vol. 3944 Lecture Notes
Computer Science, pp. 177190. Springer Berlin, Heidelberg.
Demaine, E., Mozes, S., Rossman, B., & Weimann, O. (2009). optimal decomposition
algorithm tree edit distance. ACM Transactions Algorithms (TALG), 6 (1),
2:12:19.
20

fiNatural Language Inference Arabic

Diab, M. (2009). Second generation tools (AMIRA 2.0): fast robust tokenization, POS
tagging, base phrase chunking. Proceedings 2nd International Conference
Arabic Language Resources Tools, pp. 285288, Cairo, Eygpt. MEDAR
Consortium.
Habash, N. (2010). Introduction Arabic Natural Language Processing. Synthesis Lectures
Human Language Technologies. Morgan & Claypool Publishers.
Habash, N., Rambow, O., & Roth, R. (2009). MADA+TOKAN: toolkit Arabic tokenization, diacritization, morphological disambiguation, POS tagging, stemming
lemmatization. Proceedings 2nd International Conference Arabic Language
Resources Tools, Cairo, Eygpt. MEDAR Consortium.
Habash, N., Soudi, A., & Buckwalter, T. (2007). Arabic transliteration. Arabic Computational Morphology, 1522.
Heilman, M., & Smith, N. (2010). Tree edit models recognizing textual entailments, paraphrases, answers questions. Human Language Technologies: 2010 Annual
Conference North American Chapter Association Computational Linguistics, pp. 10111019, Los Angeles, California, USA. Association Computational
Linguistics.
Hobbs, J. R. (2005). handbook pragmatics, chap. Abduction Natural Language
Understanding, pp. 724740. Blackwell Publishing.
Klein, P. (1998). Computing edit-distance unrooted ordered trees. Proceedings 6th Annual European Symposium Algorithms (ESA 98), pp. 91102,
Venice, Italy. Springer-Verlag.
Kouylekov, M. (2006). Recognizing Textual Entailment Tree Edit Distance: application
Question Answering Information Extraction. Ph.D. thesis, DIT, University
Trento, Italy.
Kouylekov, M., & Magnini, B. (2005). Recognizing textual entailment tree edit distance algorithms. Proceedings the1st Challenge Workshop Recognising Textual
Entailment, pp. 1720, Southampton, UK.
Lin, D. (1998). Dependency-based evaluation minipar. Workshop Evaluation
Parsing systems, pp. 317330. Springer.
Maamouri, M., & Bies, A. (2004). Developing Arabic treebank: methods, guidelines,
procedures, tools. Proceedings Workshop Computational Approaches
Arabic Script-based Languages, pp. 29, Geneva, Switzerland.
MacCartney, B. (2009). Natural Language Inference. Ph.D. thesis, Department Computer
Science, Stanford University, USA.
McDonald, R., Lerman, K., & Pereira, F. (2006). Multilingual dependency parsing
two-stage discriminative parser. 10th Conference Computational Natural Language Learning (CoNLL-X), New York, USA.
Mehdad, Y., & Magnini, B. (2009). Optimizing textual entailment recognition using particle
swarm optimization. Proceedings 2009 Workshop Applied Textual Inference (TextInfer 09), pp. 3643, Suntec, Singapore. Association Computational
Linguistics.
21

fiAlabbas & Ramsay

Nivre, J., Hall, J., Nilsson, J., Chanev, A., Eryigit, G., Kbler, S., Marinov, S., & Marsi,
E. (2007). MaltParser: language-independent system data-driven dependency
parsing. Natural Language Engineering, 13 (02), 95135.
Pawlik, M., & Augsten, N. (2011). RTED: robust algorithm tree edit distance.
Proceedings VLDB Endowment, 5 (4), 334345.
Punyakanok, V., Roth, D., & Yih, W. (2004). Natural language inference via dependency
tree mapping: application question answering. Computational Linguistics, 6,
110.
Ramsay, A., & Sabtan, Y. (2009). Bootstrapping lexicon-free tagger Arabic. Proceedings 9th Conference Language Engineering (ESOLEC2009), pp. 202215,
Cairo, Egypt.
Selkow, S. (1977). tree-to-tree editing problem. Information Processing Letters, 6 (6),
184186.
Smr, O., Bielicky, V., Kouilov, I., Krmar, J., Haji, J., & Zemnek, P. (2008). Prague
Arabic dependency treebank: word million words. Proceedings Workshop Arabic Local Languages (LREC 2008), pp. 1623, Marrakech, Morocco.
Tai, K. (1979). tree-to-tree correction problem. Journal ACM (JACM), 26 (3),
422433.
Zhang, K., & Shasha, D. (1989). Simple fast algorithms editing distance
trees related problems. SIAM Journal Computing, 18 (6), 12451262.

22


