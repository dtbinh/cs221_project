Journal Artificial Intelligence Research 48 (2013) 67-113

Submitted 3/13; published 10/13

Survey Multi-Objective Sequential Decision-Making
Diederik M. Roijers

d.m.roijers@uva.nl

Informatics Institute
University Amsterdam
Amsterdam, Netherlands

Peter Vamplew

p.vamplew@ballarat.edu.au

School Science,
Information Technology Engineering
University Ballarat
Ballarat, Victoria, Australia

Shimon Whiteson

s.a.whiteson@uva.nl

Informatics Institute
University Amsterdam
Amsterdam, Netherlands

Richard Dazeley

r.dazeley@ballarat.edu.au

School Science,
Information Technology Engineering
University Ballarat
Ballarat, Victoria, Australia

Abstract
Sequential decision-making problems multiple objectives arise naturally practice pose unique challenges research decision-theoretic planning learning,
largely focused single-objective settings. article surveys algorithms designed sequential decision-making problems multiple objectives. Though
growing body literature subject, little makes explicit circumstances special methods needed solve multi-objective problems. Therefore,
identify three distinct scenarios converting problem single-objective
one impossible, infeasible, undesirable. Furthermore, propose taxonomy
classifies multi-objective methods according applicable scenario, nature
scalarization function (which projects multi-objective values scalar ones), type
policies considered. show factors determine nature optimal solution, single policy, convex hull, Pareto front. Using taxonomy,
survey literature multi-objective methods planning learning. Finally,
discuss key applications methods outline opportunities future work.

1. Introduction
Sequential decision problems, commonly modeled Markov decision processes (MDPs)
(Bellman, 1957a), occur range real-world tasks robot control (Kober &
Peters, 2012), game playing (Szita, 2012), clinical management patients (Peek, 1999),
military planning (Aberdeen, Thiebaux, & Zhang, 2004), control elevators (Crites
& Barto, 1996), power systems (Ernst, Glavic, & Wehenkel, 2004), water supplies
(Bhattacharya, Lobbrecht, & Solomantine, 2003). Therefore, development algorithms
c
2013
AI Access Foundation. rights reserved.

fiRoijers, Vamplew, Whiteson & Dazeley

automatically solving problems, either planning given model MDP (e.g.,
via dynamic programming methods, Bellman, 1957b) learning interaction
unknown MDP (e.g., via temporal-difference methods, Sutton & Barto, 1998),
important challenge artificial intelligence.
research topics, desirability undesirability actions
effects codified single, scalar reward function. Typically, objective
autonomous agent interacting MDP maximize expected (possibly
discounted) sum rewards time. many tasks, scalar reward function
natural, e.g., financial trading agent could rewarded based monetary gain
loss holdings recent time period. However, many tasks
naturally described terms multiple, possibly conflicting objectives, e.g.,
traffic control system minimize latency maximize throughput; autonomous
vehicle minimize travel time fuel costs. Multi-objective problems
widely examined many areas decision-making (Zeleny & Cochrane, 1982; Vira &
Haimes, 1983; Stewart, 1992; Diehl & Haimes, 2004; Roijers, Whiteson, & Oliehoek, 2013)
growing, albeit fragmented, literature addressing multi-objective decisionmaking sequential settings.
article, present survey algorithms devised
settings. begin Section 2 formalizing problem multi-objective MDP
(MOMDP). Then, Section 3, motivate multi-objective perspective decisionmaking. Little existing literature multi-objective algorithms makes explicit
multi-objective approach beneficial and, crucially, cases cannot trivially reduced
single-objective problem solved standard algorithms. address this,
describe three motivating scenarios multi-objective algorithms.
Then, Section 4, present novel taxonomy organizes multi-objective problems
terms underlying assumptions nature resulting solutions. key
difficulty existing literature authors considered many different types
problems, often without making explicit assumptions involved, differ
authors, scope applicability resulting methods. taxonomy
aims fill void.
Sections 5 6 survey MOMDP planning learning methods, respectively, organizing according taxonomy identifying key differences
approaches examined planning learning areas. Section 7 surveys applications
methods, covering specific applications general classes problems
MOMDP methods applied. Section 8 discusses future directions field
based gaps literature identified Sections 5 6, Section 9 concludes.

2. Background
finite single-objective Markov decision process (MDP) tuple hS, A, T, R, , where:
finite set states,
finite set actions,
: [0, 1] transition function specifying, state, action,
next state, probability next state occurring,
68

fiA Survey Multi-Objective Sequential Decision-Making

R : reward function, specifying, state, action, next
state, expected immediate reward,
: [0, 1] probability distribution initial states,
[0, 1) discount factor specifying relative importance immediate rewards.
goal agent acts environment maximize expected return
Rt , function rewards received timestep onwards. Typically,
return additive (Boutilier, Dean, & Hanks, 1999), i.e., sum rewards.
infinite horizon MDP, return typically infinite sum, term discounted
according :

X
k rt+k+1 ,
Rt =
k=0

rt reward obtained time t. parameter thus quantifies relative
importance short-term long-term rewards.
contrast, finite horizon MDP, return typically undiscounted finite sum,
i.e., certain number timesteps, process terminates reward
obtained. single- multi-objective methods developed finite horizon,
discounted infinite horizon, average reward settings (Puterman, 1994), sake
brevity formalize infinite horizon discounted reward MDPs article.1
agents policy determines actions selects timestep. broadest
sense, policy condition everything known agent. state-indepedent
value function V specifies expected return following initial state:
V = E[R0 | ].

(1)

policy stationary, i.e., conditions current state,
formalized : [0, 1]: specifies, state action, probability
taking action state. specify state value function policy :
V (s) = E[Rt | , st = s],
st = s. Bellman equation restates expectation recursively
stationary policies:
X
X
(s, a, )[R(s, a, ) + V (s )].
V (s) =
(s, a)




Note Bellman equation, forms heart standard solution algorithms
dynamic programming (Bellman, 1957b) temporal-difference methods (Sutton
& Barto, 1998), explicitly relies assumption additive returns. important
because, explain Section 4.2.2, multi-objective settings interfere
additivity property, making planning learning methods rely Bellman
equation inapplicable.
1. formalizations settings, see example overview Van Otterlo Wiering (2012).

69

fiRoijers, Vamplew, Whiteson & Dazeley

State value functions induce partial ordering policies, i.e., better
equal value greater states:


s, V (s) V (s).
special case stationary policy deterministic stationary policy, one
action chosen probability 1 every state. deterministic stationary policy
seen mapping states actions: : A. single-objective MDPs,
always least one optimal policy , i.e., : , stationary deterministic.
Theorem 1. additive infinite-horizon single-objective MDP, exists deterministic stationary optimal policy (see e.g., Howard, 1960; Boutilier et al., 1999).
one optimal policy exists, share value function, known
optimal value function V (s) = max V (s). Bellman optimality equation defines
optimal value function recursively:
X
(s, a, )[R(s, a, ) + V (s )].
V (s) = max




Note that, maximizes actions, equation makes use fact
optimal deterministic stationary policy. optimal policy maximizes
value every state, policy optimal regardless initial state distribution .
However, state-independent value (Equation 1) may well different different
initial state distributions. Using , state value function translated back
state-independent value function (Equation 1):
X
V =
(s)V (s).
sS

multi-objective MDP (MOMDP)2 MDP reward function R :
n describes vector n rewards, one objective, instead scalar.
Similarly, value function V MOMDP specifies expected cumulative discounted
reward vector:

X
k rk+1 | ],
(2)
V = E[
k=0

rt vector rewards received time t. difference single
objective value (Equation 1) multi-objective value (Equation 2) policy
return, underlying sum rewards, vector rather scalar.
stationary policies, define multi-objective value state:
V (s) = E[


X

k rt+k+1 | , st = s].

(3)

k=0

single-objective MDP, state value functions impose partial ordering

policies compared different states, e.g., possible V (s) > V (s) V (s ) <
2. Multi-objective MDPs confused mixed-observability MDPs (Ong, Png, Hsu, & Lee,
2010), sometimes abbreviated MOMDP.

70

fiA Survey Multi-Objective Sequential Decision-Making



V (s ). given state, ordering complete, i.e., V (s) must greater than,

equal to, less V (s). true state-independent value functions.
contrast, MOMDP, presence multiple objectives means value
function V (s) state vector expected cumulative rewards instead scalar.
value functions supply partial ordering, even given state. example,


possible that, state s, Vi (s) > Vi (s) Vj (s) < Vj (s). Similarly,


state-independent value functions, may Vi > Vi Vj < Vj . Consequently,
unlike MDP, longer determine values optimal without additional
information prioritize objectives. information provided
form scalarization function, discuss following sections.
Though focus article, MOMDP variants constraints
specified objectives (see e.g., Feinberg & Shwartz, 1995; Altman, 1999).
goal agent maximize regular objectives meeting constraints
objectives. Constrained objectives fundamentally different regular
objectives explicitly prioritized regular objectives, i.e., policy
fails meet constraint inferior policy meets constraints, regardless
well policies maximize regular objectives.

3. Motivating Scenarios
MOMDP setting received considerable attention, immediately obvious useful addition standard MDP specialized algorithms
needed. fact, researchers argue modeling problems explicitly multiobjective necessary, scalar reward function adequate sequential
decision-making tasks. direct formulation perspective Suttons reward
hypothesis, states mean goals purposes well
thought maximization expected value cumulative sum received scalar
signal (reward).3
view imply multi-objective problems exist. Indeed,
would difficult claim, since easy think problems naturally possess
multiple objectives. Instead, implication reward hypothesis resulting
MOMDPs always converted single-objective MDPs additive returns.
conversion process would involve two steps. first step specify scalarization
function.
Definition 1. scalarization function f , function projects multi-objective
value V scalar value.
Vw (s) = f (V (s), w),
w weight vector parameterizing f .
example, f may compute linear combination values, case element
w quantifies relative importance corresponding objective (this setting discussed Section 4.2.1). second step define single-objective MDP
3. http://rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html

71

fiRoijers, Vamplew, Whiteson & Dazeley

Figure 1: three motivating scenarios MOMDPs: (a) unknown weights scenario,
(b) decision support scenario, (c) known weights scenario.

additive returns that, s, expected return equals scalarized value
Vw (s).
Though rarely, ever, makes issue explicit, research MOMDPs rests
premise exist tasks one conversion steps impossible,
infeasible, undesirable. section, discuss three scenarios occur
(see Figure 1).
first scenario, call unknown weights scenario (Figure 1a), occurs
w unknown moment planning learning must occur. Consider example
public transport system aims minimize latency (i.e., time commuters
need reach destinations) pollution costs. addition, assume resulting
MOMDP scalarized converting objective monetary cost: economists
compute cost lost productivity due commuting pollution incurs tax
must paid pollution credits purchased given price. Assume credits
traded open market therefore price constantly fluctuates. transport
system complex, may infeasible compute new plan every day given latest
prices. scenario, preferable use multi-objective planning method
computes set policies that, price, one policies optimal
(see planning learning phase Figure 1a). computationally
expensive computing single optimal policy given price, needs done
done advance, computational resources available.
Then, time select policy, current weights, i.e., price pollution
72

fiA Survey Multi-Objective Sequential Decision-Making

credits, used determine best policy set (the selection phase). Finally,
selected policy employed task (the execution phase).
unknown weights scenario, scalarization impossible planning learning
trivial policy actually needs used w known time.
contrast, second scenario, call decision support scenario (Figure
1b), scalarization infeasible throughout entire decision-making process
difficulty specifying w, even f . example, economists may able accurately
compute cost lost productivity due commuting. user may fuzzy
preferences defy meaningful quantification. example, transport system could
made efficient building new train line obstructs beautiful view,
human designer may able quantify loss beauty. difficulty specifying
exact scalarization especially apparent designer single person
committee legislative body whose members different preferences agendas.
system, MOMDP method used calculate optimal solution set
respect known constraints f w. Figure 1b shows, decision support
scenario proceeds similarly unknown weights scenario except that, selection
phase, user users select policy set according arbitrary preferences,
rather explicit scalarization according given weights.
cases, one still argue scalarization planning learning
possible principle. example, loss beauty quantified measuring
resulting drop housing prices neighborhoods previously enjoyed unobstructed
view. However, difficulty scalarization may impractical
but, importantly, forces users express preferences way may
inconvenient unnatural. selecting w requires weighing hypothetical
trade-offs, much harder choosing set actual alternatives.
well understood phenomenon field decision analysis (Clemen, 1997),
standard workflow involves presenting alternatives soliciting preferences.
subfields decision analysis multiple criteria decision-making multiattribute utility theory focus multiple objectives (Dyer, Fishburn, Steuer, Wallenius, &
Zionts, 1992). reasons, algorithms MOMDPs provide critical decision
support. Rather forcing users specify w advance, algorithms
prune policies would optimal w. Then, offer users range
alternatives select according preferences whose relative importance
easily quantified.
third scenario, call known weights scenario (Figure 1c), assume
w known time planning learning thus scalarization possible
feasible. However, may undesirable difficulty second step
conversion. particular, f nonlinear, resulting single-objective MDP
may additive returns (see Section 4.2.2). result, optimal policy may
non-stationary (see Section 4.3.2) stochastic (see Section 4.3.3), cannot occur
single-objective, additive, infinite-horizon MDPs (see Theorem 1). Consequently, MDP
difficult solve, standard methods applicable. Converting MDP
one additive returns may help either cause blowup state space,
73

fiRoijers, Vamplew, Whiteson & Dazeley

leaves problem intractable.4 Therefore, even though scalarization possible
w known, may still preferable use methods specially designed MOMDPs
rather convert problem single-objective MDP. contrast unknown
weights decision support scenarios, known weights scenario, MOMDP
method produces one policy, executed, i.e., separate selection
phase, shown Figure 1c.
Note Figure 1 assumes off-line scenario: planning learning occurs once,
execution. However, multi-objective methods employed on-line settings
planning learning interleaved execution. on-line version
unknown weights scenario, weights better characterized dynamic, rather
unknown. on-line scenario, agent must already seen weights timesteps
> 1 since prerequisite execution timesteps 1, . . . , 1. However,
weights change time, agent may yet know weights used
timestep planning learning phase timestep.

4. Problem Taxonomy
far, described MOMDP formalism proposed three motivating scenarios
it. section, discuss constitutes optimal solution. Unfortunately,
simple answer question, depends several critical factors. Therefore,
propose problem taxonomy, shown Table 1, categorizes MOMDPs according
factors describes nature optimal solution category.
taxonomy based call utility-based approach, contrast many
multi-objective papers follow axiomatic approach optimality MOMDPs.
utility-based approach rests following premise: execution phases
scenarios Section 3, one policy selected collapsing value vector policy
scalar utility, using scalarization function. application scalarization
function may implicit hidden, e.g., may embedded thought-process
user, nonetheless occurs. scalarization function part notion utility,
i.e., agent maximize. Therefore, find set optimal solution
possible weight setting scalarization function, solved MOMDP.
utility-based approach derives optimal solution set assumptions
made scalarization function, policies user allows, whether need
one multiple policies.
contrast, axiomatic approach begins axiom optimal solution set
Pareto front (see Section 4.2.2).5 approach limiting because, demonstrate
section, settings solution concepts suitable.
Thus, take utility-based approach makes possible derive solution
concept, rather assuming it. Pareto front fact correct solution
4. Since non-additive returns depend agents entire history, immediate reward function
converted MDP may depend history thus state representation converted
MDP must augmented include it.
5. example axiomatic approach multi-objective reinforcement learning, see survey
Liu, Xu, Hu (2013).

74

fiA Survey Multi-Objective Sequential Decision-Making

single policy
(known weights)
deterministic
linear
scalarization

multiple policies
(unknown weights decision support)

stochastic

one deterministic stationary
policy (1)

monotonically one
increasing
deterministic
scalarization
non-stationary
policy (3)

deterministic

stochastic

convex coverage set
deterministic stationary policies
(2)

one mixture
policy two

deterministic
stationary
policies (4)

Pareto
coverage set
deterministic
non-stationary
policies (5)

convex
coverage set
deterministic
stationary
policies (6)

Table 1: MOMDP problem taxonomy showing critical factors problem
nature resulting optimal solution. columns describe whether
problem necessitates single policy multiple ones, whether policies
must deterministic (by specification) allowed stochastic. rows
describe whether scalarization function linear combination rewards
or, whether cannot assumed scalarization function merely
monotonically increasing function them. contents cell describe
optimal solution given setting looks like.

concept, utility-based approach provides justification it. not, allows
appropriate solution concept derived instead.
taxonomy categorizes problem classes based assumptions scalarization function, policies user allows, whether one multiple policies
required. show leads different solution concepts, underscoring importance
carefully considering choice solution concept based available information.
discuss three factors constitute taxonomy following order.
Section 4.1, discuss first factor: whether one multiple policies sought, choice
follows directly motivating scenario applicable. known weights
scenario (Figure 1c) implies single-policy approach unknown weights decision
support scenarios (Figure 1a 1b) imply multiple-policy approach. Section 4.2,
discuss second factor: whether scalarization function linear combination
rewards merely monotonically increasing function them. Section 4.3, discuss
third factor: whether stochastic deterministic policies permitted.
goal taxonomy cover research MOMDPs remaining simple
intuitive. However, due diversity research MOMDPs, research
fit neatly taxonomy. note discrepancies discussing research
Sections 5 6.
75

fiRoijers, Vamplew, Whiteson & Dazeley

4.1 Single versus Multiple Policies
Following approach Vamplew et al. (2011), first distinguish problems
one policy sought ones multiple policies sought. case holds
depends three motivating scenarios discussed Section 3 applies.
unknown weights decision support scenarios, solution MOMDP
consists multiple policies. Though two scenarios conceptually quite different,
algorithmic perspective identical. reason characterized strict separation decision-making process two phases: planning
learning phase execution phase (though on-line settings, agent may go
back forth two).
planning learning phase, w unavailable. Consequently, planning learning algorithm must return single policy set policies (and corresponding
multi-objective values). set contain policies suboptimal
scalarizations, i.e. interested undominated policies.
Definition 2. MOMDP scalarization function f , set undominated
policies, U (m ), subset possible policies exists w
scalarized value maximal:


U (m ) = { : w( ) Vw Vw }.

(4)

U (m ) sufficient solve m, i.e., w, contains policy optimal
scalarized value. However, may contain redundant policies that, optimal
weights, optimal policy set w. policies removed
still ensuring set contains optimal policy w. fact, order solve
m, need subset undominated policies that, possible w,
least one policy set optimal. sometimes called coverage set (CS) (Becker,
Zilberstein, Lesser, & Goldman, 2003).
Definition 3. MOMDP scalarization function f , set CS(m )
coverage set subset U (m ) if, every w, contains policy maximal
scalarized value, i.e., if:









CS( ) U ( ) (w)() CS( ) ( ) Vw Vw .
(5)
Note U (m ) automatically coverage set. However, U (m ) unique, CS(m )
need be. multiple policies value, U (m ) contains
them, coverage set need contain one. addition, given CS(m ),

may exist policy
/ CS(m ) V different V CS(m )
scalarized value CS(m ) w optimal.
contrast single-objective MDPs, MOMDPs whether policy CS(m )
depend initial state distribution . thus important accurately specify
formulating MOMDP.
Ideally, MOMDP algorithm find smallest CS(m ). However,
might harder finding one smaller U (m ). Section 4.2, specialize
coverage set two classes scalarization functions.
76

fiA Survey Multi-Objective Sequential Decision-Making

execution phase, single policy chosen set returned planning
learning phase executed. unknown weights scenario, assume w revealed
planning learning complete execution begins. Selecting policy
requires maximizing scalarized value policy returned set:
= argmax Vw .
CS(m )

decision support scenario, set manually inspected user(s), select
policy execution informally, making implicit trade-off objectives.
known weights scenario, w known planning learning begins. Therefore,
returning multiple policies unnecessary. However, mentioned Section 3 discussed
Section 4.2.2, scalarization yield single-objective MDP difficult
solve.
4.2 Linear versus Monotonically Increasing Scalarization Functions
second critical factor affecting constitutes optimal solution MOMDP
nature scalarization function. section, discuss two types scalarization
function: linear combinations rewards merely
monotonically increasing functions them.
4.2.1 Linear Scalarization Functions
common assumption scalarization function (e.g., Natarajan & Tadepalli, 2005;
Barrett & Narayanan, 2008), f linear, i.e., computes weighted sum
values objective.
Definition 4. linear scalarization function computes inner product weight vector
w value vector V
Vw = w V .
(6)
element w specifies much one unit value corresponding objective
contributes scalarized value. elements weight vector w positive real
numbers constrained sum 1.
Linear scalarization functions simple intuitive way scalarize. One common
situation applicable rewards easily translated monetary
value. example, consider mining task different policies yield different expected
quantities various minerals. prices per kilo minerals fluctuate daily,
task formulated MOMDP, objective corresponding different
mineral. element V reflects expected number kilos mineral
mined scalarized value Vw corresponds monetary value
everything mined. Vw computed w, corresponding
(normalized) current price per kilo mineral, becomes known.
single-policy setting, w known, presence multiple objectives poses
difficulties given linear f . Instead, f simply applied reward vector
77

fiRoijers, Vamplew, Whiteson & Dazeley

MOMDP. inner product computed f distributes addition, result
single-objective MDP additive returns. infinite horizon setting leads to:
Vw = w V = w E[


X

k rt+k+1 ] = E[


X

k (w rt+k+1 )].

(7)

k=0

k=0

Since single-objective MDP additive returns, solved standard methods, yielding single policy, reflected box labeled (1) Table 1. Due Theorem
1, determinstic stationary policy suffices. However, multi-objective approach still
preferable case, e.g., V may easier estimate Vw large continuous
MOMDPs function approximation required (see Section 6.1).
multiple policy setting, however, know w planning learning
therefore want find coverage set. f linear, U (m ), automatically
coverage set, consists convex hull. Substituting Equation 6 definition
undominated set (Definition 2), obtain definition convex hull:
Definition 5. MOMDP m, convex hull (CH) subset
exists w linearly scalarized value maximal:


CH(m ) = { : w( ) w V w V }.

(8)

Figure 2a illustrates concept convex hull stationary deterministic policies.
point plot represents multi-objective value given policy two-objective
MOMDP. axes represent reward dimensions. convex hull shown set
filled circles, connected lines form convex surface.6 Given linear f ,
scalarized value policy linear function weights. illustrated
Figure 2b, x-axis represents weight dimension 0 (w[1] = 1 w[0]),
y-axis scalarized value policies. select policy, need know
values convex hull policies, form upper surface scalarized value,
illustrated black solid lines, correspond three convex hull policies
Figure 2a. upper surface forms piecewise linear convex function. functions
well-known literature partially-observable Markov decision processes
(POMDPs), whose relationship MOMDPs discuss Section 5.2.
U (m ), CH(m ) contain superfluous policies. However, define
convex coverage set (CCS) specification coverage set f linear.
reflected box (2) Table 1 (we explain policies set deterministic
stationary Section 4.3.1).
Definition 6. MOMDP m, set CCS(m ) convex coverage set
subset CH(m ) if, every w, contains policy whose linearly scalarized value
maximal, i.e., if:



CCS(m ) CH(m ) (w)() CCS(m ) ( ) w V w V . (9)
6. Note term convex hull slightly different meaning multi-objective literature
standard geometric definition. geometry, convex hull finite set points Euclidean
space minimal subset points expressed convex
combination points convex hull. multi-objective setting, interested
particular subset geometric convex hull; points convex combinations strictly
bigger (in dimensions) point S, i.e., points optimal weight.

78

fiA Survey Multi-Objective Sequential Decision-Making

(a)

(b)

Figure 2: Example convex hull Pareto front. point (a) represents
multi-objective value given policy line (b) represents linearly
scalarized value policy across values w. convex hull shown black
filled circles (a), black lines (b). Pareto front consists filled
points (circles squares) (a), dashed solid black lines
(b). unfilled points (a) (grey lines (b)) dominated.

deterministic stationary policies, difference CH(m ) CCS(m ) may
often small. Therefore, terms often used interchangeably. However, case
non-stationary stochastic policies, difference quite significant, CH
contain infinitely many policies, possible construct finite CCS, show
Section 4.3.1.
4.2.2 Monotonically Increasing Scalarization Functions
linear scalarization functions intuitive simple, always adequate
expressing users preferences. example, suppose mining task mentioned
above, two minerals mined three policies available: 1
sends mining equipment location first mineral mined, 2
location second mineral mined, 3 location
minerals mined. Suppose owner equipment prefers 3 , e.g.,
least partially appeases clients different interests. However, may case that,
location corresponding 3 fewer minerals, convex hull contains
1 2 . Thus, owners preference 3 implies she, implicitly explicitly,
employs nonlinear scalarization function.
Here, consider case f nonlinear, corresponds common
notion relationship reward utility. class possibly nonlinear scalarizations strictly monotonically increasing scalarization functions. functions
adhere constraint policy changed way value increases
79

fiRoijers, Vamplew, Whiteson & Dazeley

one objectives, without decreasing objectives, scalarized
value increases.
Definition 7. scalarization function f strictly monotonically increasing if:






(i, Vi Vi i, Vi > Vi ) (w, Vw > Vw ).

(10)

Linear scalarization functions (with non-zero positive weights) included class
functions. condition left-hand side Equation 10 commonly known
Pareto dominance (Pareto, 1896).
Definition 8. policy Pareto-dominates another policy value least
high objectives strictly higher least one objective:






V P V i, Vi Vi i, Vi > Vi .

(11)

Demanding f strictly monotonically increasing quite minimal constraint,
requires that, things equal, getting reward certain objective
always better. fact, difficult think f violates constraint without
employing highly unnatural notion reward.7
Three observations order strictly monotonically increasing scalarization
functions related concept Pareto dominance. First, unlike linear case,
necessarily know exact shape f . Instead, know belongs
particular class functions. solution concept follows thus applies strictly
monotonically increasing f . cases stronger assumptions f made,
specific solution concepts possible. However, except linearity, aware
properties f exploited solving MOMDPs.
Second, notions optimality introduced Section 4.2.1 longer appropriate.
reason that, even though vector-valued returns still additive (Equation 2),
scalarized returns may f may longer linear. example, consider
well-known Tchebycheff scalarization function (Perny & Weng, 2010)8 :
X
(V , p, w) = max wi |pi Vi |
wi |pi Vi |,
(12)
i1...n

i1...n

p optimistic reference point, w weights, arbitrarily small positive
constant greater 0. Note sum righthand side makes function
strictly monotonically increasing. Now, p = (3, 3),P = 0.01, r1 = (0, 3), r2 = (3, 0),
k
w = (0.5, 0.5) = 1, f (V , w) = 0 E[
k=0 f (rt+k+1 , w)] = (1.515) +
(1.515) = 3.03. loss additivity scalarized returns applying nonlinear
f important consequences methods applied, show Section 4.3.2.
Third, still identify prune policies optimal w
strictly monotonically increasing f , even though may nonlinear. Consider three
7. addition, f strictly monotonically increasing assumptions made,
policies pruned coverage set. Thus, computing value every policy coverage
set, required selection phase, likely intractable.
8. definition differs slightly Perny Weng (2010): multiplied 1 express
maximization instead minimization, sake consistency rest article.

80

fiA Survey Multi-Objective Sequential Decision-Making

labeled policies Figure 2a (note Figure 2b apply, scalarization
function longer linear). B higher value one objective, lower
value other. therefore cannot tell whether B ought preferred without
knowing w. However, C lower value objectives, thus Paretodominates C: P C. f strictly monotonically increasing, scalarized value
greater C w thus discard C.
now, defer full discussion constitutes optimal solution
MOMDP strictly monotonically increasing scalarization function (i.e., boxes (3)-(6)
Table 1) depends, whether single multiple policy setting
applies, whether deterministic stochastic policies considered,
addressed Section 4.3.
However, already observe that, given strictly monotonically increasing f ,
use Pareto front set viable policies. Pareto front consists policies
Pareto dominated.
Definition 9. MOMDP m, Pareto front set policies
Pareto dominated policy :


P F (m ) = { : ( ), V P V }.

(13)

Note P F (m ) set undominated policies U (m ) specific strictly
monotonically increasing f . already seen special case linear f ,
U (m ) = CH(m ), subset P F (m ). (For example, Figure 2, Pareto
front consists convex hull plus B.) However, strictly monotonically increasing
f , know policy P F (m ) dominated respect f , i.e.,
6 P F (m ) 6 U (m ). because, strictly monotonically increasing f

/ P F (m ), cannot exist w optimal, since definition exists

V P V and, since f strictly monotonically increasing, implies

Vw > Vw .
However, know f strictly monotonically increasing, cannot settle
subset P F (m ) either, exist strictly monotonically increasing f
U (m ) = P F (m ). Perny Weng (2010) show U (m ) = P F (m )
Tchebycheff function (Equation 12), strictly monotonically increasing. Therefore,
cannot discard policies P F (m ) retain undominated set U (m )
strictly monotonically increasing f .
Pareto coverage set (PCS) minimal size constructed retaining one
policy policies identical vector values P F (m ). formally define
PCS follows:
Definition 10. MOMDP m, set P CS(m ) Pareto coverage set subset
P F (m ) if, every policy , contains policy either dominates
equal value , i.e., if:




P CS(m ) P F (m ) ( )() P CS(m ) (V P V V = V ) . (14)
Again, deterministic stationary policies difference P CS(m ) P F (m )
may minor. Note P F (m ) automatically P CS(m ). papers
literature therefore take P F (m ) solution.
81

fiRoijers, Vamplew, Whiteson & Dazeley

slightly relax constraint f , without change policies
P CS(m ). Specifically, define monotonically increasing scalarization

function function following property holds: (i, Vi Vi ) (w, Vw

Vw ). relaxation influences set undominated policies: policies
P F (m ) always dominated strictly monotonically increasing f , need
monotonically increasing f . Consider example f (V , w) = 0,
monotonically increasing strictly monotonically increasing. function
dominated policies, every policy scalarized value. However,
scalarized value policy 6 P F (m ) cannot greater scalarized
function policy P CS(m ), use P CS(m ) (non-strict) monotonically
increasing f . Therefore, article, focus monotonically increasing f ,
broader class functions.
P F (m ), even P CS(m ), may prohibitively large contain
many policies whose values differ negligible amounts, Chatterjee et al. (2006) Brazdil
et al. (2011) introduce slack parameter , use define -approximate Pareto
front, P F (m ). P F (m ) contains values policies every possible policy

policy P F (m ) Vi (s) + Vi (s). weakening
requirements domination, approach yields smaller set calculated
efficiently.
Another option finding smaller set P F (m ) making additional assumptions
scalarization function. example, Perny, Weng, Goldsmith, Hanna (2013)
introduce notion fairness objectives, leading Lorentz optimality.
additional assumption sum values objectives stays same,
making difference two objectives smaller yields higher scalarized value.
course strong assumption apply broadly Pareto optimality. However,
apply, help reduce size optimal solution set.
4.3 Deterministic versus Stochastic Policies
third critical factor affecting constitutes optimal solution MOMDP
whether deterministic polices considered stochastic ones allowed.
applications reason exclude stochastic policies priori,
cases stochastic policies clearly undesirable even unethical. example,
policy determines clinical treatment patient, e.g., work Lizotte, Bowling,
Murphy (2010) Shortreed, Laber, Lizotte, Stroup, Pineau, Murphy (2011),
flipping coin determine course action may inappropriate. denote

set deterministic policies
set stationary policies . sets subsets




policies: . Finally set policies deterministic


stationary intersection sets, denoted
DS = .
single-objective MDPs, factor critical because, due Theorem 1,
restrict search deterministic stationary policies, i.e. optimal attainable value

V . However,
attainable deterministic stationary policy: maxm V = max



DS

situation complex MOMDPs. section, discuss focus
stochastic deterministic policies affects setting considered taxonomy.
82

fiA Survey Multi-Objective Sequential Decision-Making

4.3.1 Deterministic Stochastic Policies Linear Scalarization
Functions
f linear, result similar Theorem 1 holds MOMDPs due following
corollary:

Corollary 1. MOMDP m, CCS(m
DS ) CCS( ).

Proof. f linear, translate MOMDP single-objective MDP,
possible w. done treating inner product reward vector w
new rewards, leaving rest problem is. Since inner product distributes
addition, scalarized returns remain additive (Equation 7). Thus, every w
exists translation single-objective MDP, optimal deterministic
stationary policy must exist, due Theorem 1. Hence, w exists optimal
deterministic stationary policy. Therefore, exists CCS(m
DS ) optimal



w. Consequently, cannot exist \ DS w V > w V

thus CCS(m
DS ) CCS( ).
CCS(m
DS ) thus sufficient solving MOMDPs linear f , even stochastic non-stationary policies allowed. reflected box (2) Table 1.
applies box (1) since optimal policy case member CCS(m
DS ),
i.e., one best given known w.
Unfortunately, result analogous Corollary 1 holds MOMDPs monotonically increasing f . rest section, discuss consequences
nature optimal MOMDP solution boxes (3)-(6) Table 1.
4.3.2 Multiple Deterministic Policies Monotonically Increasing
Scalarization Functions
multiple-policy setting deterministic policies allowed f nonlinear,
non-stationary policies may better best stationary ones.
Theorem 2. infinite-horizon MOMDPs, deterministic non-stationary policies Paretodominate deterministic stationary policies undominated deterministic stationary policies (White, 1982).
see why, consider following MOMDP, denoted m1, adapted example
White (1982). one state three actions a1 , a2 , a3 , yield rewards
(3, 0), (0, 3), (1, 1), respectively. allow deterministic stationary policies,
three possible policies 1 , 2 , 3 m1
DS , corresponding always taking
one actions, Pareto optimal. policies following
state-independent values (Equation 2): V1 = (3/(1 ), 0), V2 = (0, 3/(1 )),
V3 = (1/(1 ), 1/(1 )). However, consider set possibly non-stationary
m1
m1
policies m1
(including non-stationary ones), construct policy ns \ DS
alternates a1 a2 , starting a1 , whose value Vns = (3/(1
2 ), 3/(1 2 )). Consequently, ns P 3 > 0.5 thus cannot restrict
83

fiRoijers, Vamplew, Whiteson & Dazeley

attention stationary policies.9 Consequently, multiple deterministic policies case
monotonically increasing f , need find P CS(m
), includes non-stationary
policies, shown box (5) Table 1.
addition consider broader class policies, another consequence
defining policy indirectly via value function longer possible. standard
single-objective methods, optimal policy found local action selection
respect value function: i.e., every state, policy selects action
maximizes expected value. However, local selection yield non-stationary policy, value function must non-stationary, i.e., must condition current
timestep. standard finite-horizon setting, different value function computed timestep, possible infinite-horizon setting.
discuss address difficulty Sections 5 6.
4.3.3 Multiple Stochastic Policies Monotonically Increasing
Scalarization Functions
multiple policy setting stochastic non-stationary policies, i.e., full set ,
allowed, cannot consider deterministic stationary policies. However,
employ stochastic stationary policies instead deterministic non-stationary ones.
particular, employ mixture policy (Vamplew, Dazeley, Barker, & Kelarev, 2009)
takes set N deterministic
policies, selects i-th policy set,
P
probability pi , N
p
=
1.
leads values linear combination
i=0
values constituent policies. previous example, replace ns
policy chooses 1 probability p1 2 otherwise, resulting following
values:


3p1 3(1 p1 )

1
2
V = p1 V + (1 p1 )V =
,
.
1
1
Fortunately, necessary explicitly represent entire P CS(m ) explicitly.
Instead, sufficient compute CCS(m
DS ). necessary stochastic policies
create P CS(m ) easily constructed making mixture policies
policies CCS(m
DS ).
Corollary 2. infinite horizon discounted MOMDP, infinite set mixture policies
PM constructed policies CCS(m
DS ), set PM ,

P CS( ) (Vamplew et al., 2009).
Proof. construct policy value vector convex surface, e.g.,
10 Thereblack lines Figure 2a, mixing policies CCS(m
DS ), e.g., black dots.
fore, always construct mixture policy dominates policy value
surface, e.g., B. show contradiction cannot policy
9. White (1982) shows infinite-horizon discounted setting, arguments hold
finite-horizon average-reward settings.
10. Note always mix policies adjacent; line pair policies
mix convex surface. E.g. mixing policy represented leftmost black dot
Figure 2a policy represented rightmost black dot lead optimal policies,
line connecting two points convex surface.

84

fiA Survey Multi-Objective Sequential Decision-Making

convex surface. was, would optimal w f linear. Consequently, Corollary 1, would deterministic stationary policy least
equal value. since convex surface spans values CCS(m
DS ), leads
contradiction. Thus, policy Pareto-dominate mixture policy convex
surface.
Thanks Corollary 2, sufficient compute CCS(m
DS ) solve MOMDP,
reflected box (6) Table 1. surprising consequence fact, knowledge
made explicit literature, Pareto optimality, though common
solution concept associated multi-objective problems, actually necessary one
specific problem setting:
Observation 1. multiple policy setting f monotonically increasing
deterministic policies considered (box (5) Table 1), requires computing Pareto coverage set. either f linear stochastic policies allowed, CCS(m
DS ) suffices.
Wakuta (1999) proves sufficiency CCS(m
DS ) monotonically increasing
scalarizations multiple stochastic policies (box (6) Table 1) infinite horizon
MOMDPs, different way. Instead mixture policies Corollary 2, uses stationary randomizations deterministic stationary policies. Wakuta Togawa (1998)
provide similar proof average reward case.
Note that, common consider non-stationary stochastic policies f
nonlinear, policies typically condition current state, current state
time, agents reward history. However, setting, policies condition
reward history dominate not. example, suppose two
objectives take positive values f simply selects smaller two, i.e.,
f (V , w) = mini Vi . Suppose that, given state, two actions available,
yields rewards (4, 4) (0, 5) respectively. Finally, suppose agent arrive
state one two reward histories, whose discounted sums either (5, 0) (3, 3).
policy conditions discounted reward histories outperform policies
not, i.e., optimal policy selects action yielding (4, 4) reward history sums
(3, 3) action yielding (0, 5) reward history sums (5, 0). So,
single objective MDPs Markov property additive returns sufficient restrict
attention policies ignore history, multi-objective case, scalarized returns
longer additive therefore optimal policy depend history. Examples
methods exploit fact steering approach (Mannor & Shimkin, 2001)
reward-augmented-state thresholded lexicographic ordering method Geibel (2006),
discussed Section 6.1.
4.3.4 Single Deterministic Stochastic Policies Monotonically
Increasing Scalarization Functions
remains address single-policy setting monotonically increasing f .
nature optimal solution case follows directly reasoning given
multiple-policy setting.
deterministic policies considered, single policy sought may
non-stationary, reflected box (3) Table 1, reasons elucidated Whites
85

fiRoijers, Vamplew, Whiteson & Dazeley

example. Again, hard define non-stationary policy local action selection,
due risk circular dependencies Q-values.
stochastic policies allowed, optimal policy may stochastic,
represented mixture policy two deterministic stationary policies,
reflected box (4) Table 1, reasons given Corollary 2. cases,
policies potentially benefit conditioning reward history.

5. Planning MOMDPs
section, survey key approaches planning MOMDPs, i.e., computing
optimal policy coverage set undominated policies given complete model
MOMDP. Following taxonomy presented Section 4, first consider single-policy
methods turn multiple-policy methods linear monotonically increasing
scalarization functions.
5.1 Single-Policy Planning
known weights scenario, w known planning begins, single
policy, optimal w, must discovered. Since MOMDP transformed
single-objective MDP f linear (see Section 4.2.1), focus single-policy
planning nonlinear f .
discussed Section 4.2.2, nonlinear f cause scalarized return nonadditive. Consequently, single-objective dynamic programming linear programming
methods, exploit assumption additive returns employing Bellman equation, applicable. However, different linear programming formulations singlepolicy planning MOMDPs possible. key feature methods
produce stochastic policies, which, discussed Section 4, optimal
scalarization function nonlinear. aware single-policy planning
methods work arbitrary nonlinear f , methods developed two special
cases. particular, Perny Weng (2010) propose linear programming method
MOMDPs scalarized using Tchebycheff function mentioned Section 4.2.2.
Tchebycheff function always w Pareto-optimal policy optimal,
approach find (single) policy Pareto front. addition, Ogryczak, Perny,
Weng (2011) propose analogous method ordered weighted regret metric.
metric calculates regret objective respect estimated ideal reference
point, sorts descending order, calculates weighted sum weights
descending order.
researchers proposed single-policy methods MOMDPs constraints.
Feinberg Shwartz (1995) consider MOMDPs one regular objective objectives inequality constraints. show feasible policy exists setting,
deterministic stationary finite number timesteps N that,
prior timestep N , random actions must performed. call (M, N )
policy, show Pareto-optimal values achieved (M, N ) policies, propose
linear programming algorithm finds -approximate policies setting.
general MOMDPs constraints considered. particular, Altman (1999)
proposes several linear programming approaches settings.
86

fiA Survey Multi-Objective Sequential Decision-Making

Furnkranz, Hullermeier, Cheng, Park (2012) propose framework MDPs
qualitative reward signals, related MOMDPs fit neatly
taxonomy. Qualitative reward signals indicate preference policies actions
without directly ascribing numeric value them. Since preferences induce partial
ordering policies, policy iteration method authors propose setting
may applicable MOMDPs nonlinear f , Pareto dominance induces partial
orderings. However, authors note multi-objective tasks generally numeric
feedback exploited. Thus, suggest quantitative MOMDPs
viewed subset preference-based MDPs, methods designed specifically
MOMDPs may efficient general preference-based methods.
5.2 Multiple-Policy Planning Linear Scalarization Functions
multiple-policy setting linear f , seek CCS(m
DS ). Note however,
distinction convex hull convex coverage set usually made
literature.
One might argue explicitly multi-objective methods necessary setting, one could repeatedly run single-objective methods obtain CCS(m
DS ).
However, since infinitely many possible w, obvious possible values w covered. might possible devise way run single-objective
methods finite number times still guarantee CCS(m
DS ) produced. However,
would nontrivial result corresponding algorithm would essence
multi-objective method happens use single-objective methods subroutines.
One approach attempted find minimally sized CCS(m
), i.e., convex
coverage set deterministic necessarily stationary policies, originally proposed
White Kim (1980), translate MOMDP partially observable Markov
decision process (POMDP) (Sondik, 1971). intuitive way think translation
imagine fact one true objective agent unaware
objectives MOMDP is. modeled POMDP defining state
tuple hs1 , s2 s1 state MOMDP s2 {1 . . . n} indicates
true objective. observations thus identify s1 exactly give information
s2 . Note translation MOMDPs POMDPs one-way only. every
POMDP translated equivalent MOMDP.
Typically, agent interacting POMDP maintains belief, i.e., probability
distribution states. POMDP derived MOMDP, belief decomposed belief s1 belief s2 . former degenerative s1
known. latter vector size n i-th element specifies probability
i-th objective true one. vector analogous w linear f .
fact, reason Figure 2b resembles piecewise linear value functions often
depicted POMDPs; difference whether x-axis interpreted w
belief.
White Kim (1980) show that, finite horizon case, solution every belief
exactly solution w, solutions resulting POMDP
exactly original MOMDP. infinite horizon case difficult
infinite horizon POMDPs undecidable (Madani, Hanks, & Condon, 1999). However,
87

fiRoijers, Vamplew, Whiteson & Dazeley

sufficiently large horizon, solution finite horizon POMDP used
approximate solution infinite horizon MOMDP.
solve resulting POMDP, White Kim (1980) propose combination Sondiks
one-pass algorithm (Smallwood & Sondik, 1973) policy iteration POMDPs (Sondik,
1978). However, POMDP planning method used long (1)
require initial belief POMDP state (which would correspond initializing
MOMDP state w) (2) computes optimal policy every
possible belief. recently developed exact methods, e.g., Cassandra, Littman,
Zhang (1997) Kaelbling, Littman, Cassandra (1998), meet conditions
could thus employed. Approximate point-based POMDP methods (Spaan & Vlassis,
2005; Pineau, Gordon, & Thrun, 2006) meet conditions (1) (2) could
adapted compute approximate convex hull, choosing prior distribution
weights could sample. Online POMDP planning methods (Ross, Pineau,
Paquet, & Chaib-draa, 2008) applicable plan given belief.
Converting POMDP thus allows use POMDP methods
solving MOMDPs linear f . However, approach inefficient
exploit characteristics distinguish MOMDPs general POMDPs, i.e.,
part state, s1 , known observations give information
s2 . example, methods compute policies trees, e.g., (Kaelbling et al., 1998)
exploit fact deterministic policies stationary functions state
needed MOMDPs linear f . Furthermore, mentioned before, general infinite
horizon POMDPs undecidable, MOMDPs fact possible compute
CCS(m
DS ) exactly.
reasons, researchers developed specialized planning methods
setting. Viswanathan, Aggarwal, Nair (1977) propose linear programming approach
episodic MOMDPs. Wakuta Togawa (1998) propose policy iteration approach
three phases. first phase uses policy iteration narrow set
possibly optimal policies. second phase uses linear programs check optimality.
Since necessarily give definitive answer, third phase uses another linear
program handle undetermined solutions left second phase.
Barrett Narayanan (2008) propose convex hull value iteration (CHVI), computes CH(m
), every state. CHVI extends conventional value iteration storing
DS
set vectors, Q (s, a) state-action pair, representing convex hull policies involving action. sets vectors correspond Q-values single-objective
setting; contain optimal Q-values possible w. backup operation
performed, Q-hulls next state propagated back s. possible next

state , possible actions considered (i.e. union convex hulls Q (s , )
taken), weighted probability occurring taking action state s.
procedure similar witness algorithm (Kaelbling et al., 1998) POMDPs.
Lizotte et al. (2010) propose value-iteration approach finite-horizon setting
computes different value function timestep. addition, uses piecewise
linear spline representation value functions. authors prove offers asymptotic
time space complexity improvements representation used CHVI
enables application algorithm MOMDPs continuous states. However,
88

fiA Survey Multi-Objective Sequential Decision-Making

algorithm applicable problems two objectives. limitation addressed
authors subsequent work (Lizotte, Bowling, & Murphy, 2012) extends
algorithm arbitrary number objectives provides detailed implementation
case three objectives.
5.3 Multiple-Policy Planning Monotonically Increasing Scalarization
Functions
section, consider planning MOMDPs monotonically increasing f . discussed Section 4.3, stochastic policies allowed, mixture policies deterministic
stationary policies sufficient. Therefore, focus case deterministic
policies allowed consider methods compute P CS(m
), include
non-stationary policies. distinction P F (m
)

P
CS(m

) usually
made literature.
linear case, scalarizing every w obtaining P CS(m
) singleobjective methods problematic. infinitely many w consider but, unlike
linear case, additional difficulty scalarized returns may longer
additive, make single-objective methods inapplicable.
Daellenbach Kluyver (1980) present algorithm multi-objective routing tasks
(essentially deterministic MOMDPs). approach uses dynamic programming conjunction augmented state space find non-Pareto-dominated policies iteratively,
number iterations equals maximum number steps route. algorithm finds undominated sub-policies parallel. authors use two alternative explicit
scalarization functions, call weighted minsum weighted minmax operators. First, values solutions translated : objective, new value
becomes fractional difference optimal values objective across
solutions. Then, value objective multiplied positive weight. Finally,
either minimum sum (minsum) minimum maximal value (minmax )
new weighted fractional differences chosen scalarization. Note
scalarization functions monotonically increasing objectives, optimal value
objective individually depend scalarization function.
White (1982) extends work proposing dynamic programming method
approximately solves infinite horizon MOMDPs. repeatedly backing according multi-objective version Bellman equation. Since policies
non-stationary, size Pareto front grows rapidly number backups applied.
However, White notes number need large acceptable approximations reached. Nonetheless, approach feasible small MOMDPs.
Wiering De Jong (2007) address difficulty dynamic programming method
called CON-MODP deterministic MOMDPs computes optimal stationary policies.
CON-MODP works enforcing consistency DP updates: policy consistent
suggests action timesteps given state. inconsistent policy
inconsistent one state-action pair, CON-MODP makes consistent forcing
current action taken time current state visited. inconsistency runs
deeper, policy discarded.
89

fiRoijers, Vamplew, Whiteson & Dazeley

contrast, Gong (1992) proposes linear programming approach finds Paretofront stationary policies. However, authors note, approach suitable
small MOMDPs number constraints decision variables
linear program increase rapidly state space grows.
mentioned Section 4.2.2, one way cope intractably large Pareto fronts
compute instead -approximate Pareto front, much smaller. Chatterjee
et al. (2006) propose linear programming method computes -approximate front
infinite horizon MOMDP, Chatterjee (2007) propose analogous algorithm
average reward setting. cases, stationary stochastic policies shown
sufficient.
Another way improve scalability setting give planning whole
state space instead plan on-line agents current state, using Monte Carlo tree
search approach (Kocsis & Szepesvari, 2006). approaches, proven
successful, e.g., game Go (Gelly & Silver, 2011), increasingly popular
single-objective MDPs. Wang Sebag (2013) propose Monte Carlo tree search method
deterministic MOMDPs. Single-objective tree search methods typically optimistically
explore tree selecting actions maximize upper confidence bound
value estimates. multi-objective variant same, respect scalar
multi-objective value function whose definition based hypervolume indicator induced proposed action together set Pareto optimal policies computed
far. hypervolume indicator (Zitzler, Thiele, Laumanns, Fonseca, & da Fonseca, 2003)
measures hypervolume Pareto-dominated set points. Since Pareto
front maximizes hypervolume indicator, optimistic action selection strategy focuses
tree search branches likely compliment existing archive.

6. Learning MOMDPs
methods reviewed Section 5 assume model transition reward
dynamics MOMDP known. cases model directly available,
multi-objective reinforcement learning (MORL) used instead.
One way carry MORL take model-based approach, i.e., use agents
interaction environment learn model transition reward function
MOMDP apply multi-objective planning methods described
Section 5. Though approach seems well suited MORL, papers
considered it, (e.g., Lizotte et al., 2010, 2012). discuss opportunities future work
model-based MORL Section 8.1. Instead, work MORL focused
model-free methods, model transition reward function never explicitly
learned.
section, survey key MORL approaches. majority
methods single-policy setting, multiple-policy methods developed. first glance, may seem multiple-policy methods unlikely effective
learning setting, since finding policies would increase sample costs,
computational costs, former typically much scarcer resource. However, modelbased methods obviate issue: enough samples gathered learn
useful model, finding policies optimal weights requires computation. Model90

fiA Survey Multi-Objective Sequential Decision-Making

free methods practical multiple-policy setting employ off-policy
learning (Sutton & Barto, 1998; Precup, Sutton, & Dasgupta, 2001), makes possible learn one policy using data gathered another. way, policies
multiple weight settings optimized using data.
6.1 Single-Policy Learning Methods
known weights scenario, MORL algorithm aims learn single policy
optimal given weights. discussed Section 5.1, linear scalarization
equivalent learning optimal policy single-objective MDP standard
temporal-difference (TD) methods (Sutton, 1988) Q-learning (Watkins, 1989)
easily applied.
However, even though specialized methods needed address setting,
nonetheless commonly studied setting MORL. Linear scalarization
uniform weights, i.e., elements w equal, forms basis work Karlsson
(1997), Ferreira, Bianchi, Ribeiro (2012), Aissani, Beldjilali, Trentesaux (2008)
Shabani (2009) amongst others, non-uniform weights used authors
Castelletti et al. (2002), Guo et al. (2009) Perez et al. (2009). majority
work uses TD methods, work on-line, although Castelletti et al. (2010) extend off-line
Fitted Q-Iteration (Ernst, Geurts, & Wehenkel, 2005) multiple objectives.
cases, change made underlying RL algorithm that, rather
scalarizing reward function learning scalar value function resulting
single-objective MDP, vector-valued value function learned original MOMDP
scalarized selecting actions. argument approach
values individual objectives may easier learn scalarized value, particularly
function approximation employed (Tesauro et al., 2007). example, function
approximator ignore state variables irrelevant objective, reducing
size state space thereby speeding learning.
discussed Section 4.2.2, linear scalarization may appropriate scenarios. Vamplew, Yearwood, Dazeley, Berry (2008) demonstrate empirically
practical consequences MORL. Therefore, MORL methods work
nonlinear scalarization functions substantial importance. Unfortunately, illustrated
Section 4.2.2, coping setting especially challenging, since algorithms
TD methods based Bellman equation inherently incompatible
nonlinear scalarization functions due non-additive nature scalarized returns.
Four main classes single-policy MORL methods using non-linear scalarization
arisen, differ deal issue. first class simply applies TD
methods without modification. approaches either resign heuristics guaranteed converge impose restrictions environment ensure
convergence. second class modifies either TD algorithm state representation
issue non-additive returns avoided. third class uses TD methods
learn multiple policies using linear scalarization different values w,
forms stochastic non-stationary meta-policy optimal respect
nonlinear scalarization. fourth class uses policy-search methods,
91

fiRoijers, Vamplew, Whiteson & Dazeley

make use Bellman equation hence directly applied combination
nonlinear scalarizations.
first class includes methods model problem multi-agent system,
one agent per objective. agent learns recommends actions basis
return objective. global switch selects winning agent, whose recommended action followed current state. Examples include simple winner-takes-all
approach agent whose recommended action highest Q-value selected,
sophisticated approaches W-learning (Humphrys, 1996) selected
action one incur loss followed. One key weakness
approaches pointed Russell Zimdars (2003): allow
selection actions that, optimal single objective, offer good compromise
multiple objectives. Another key weakness that, since actions selected
different timesteps may recommended different agents, resulting behavior corresponds policy combines elements learned agent. combination
may optimal even single objective, i.e., may Pareto dominated perform
arbitrarily poorly.
TD used directly nonlinear scalarization functions allow
consideration actions, optimal regards individual
objectives. Scalarization functions based fuzzy logic proposed problems
discrete actions Zhao, Chen, Hu (2010) problems continuous
actions Lin Chung (1999). widely cited approach nonlinear scalarization
Gabor, Kalmar, Szepesvari (1998), designed tasks constraints
must satisfied objectives. lexicographic ordering objectives defined
threshold value specified objectives except last. State-action values
objective exceed corresponding threshold clamped threshold value
prior applying lexicographic ordering. Thus, thresholded lexicographic ordering
(TLO) approach scalarization maximizes performance last objective subject
meeting constraints objectives specified thresholds.
methods combining TD nonlinear scalarization may converge suitable
policy certain conditions, converge suboptimal policy even fail
converge conditions. example, Issabekov Vamplew (2012) demonstrate empirically TLO fail converge suitable policy episodic tasks
constrained objective receives non-zero rewards timestep end
episode. general, methods based combination TD nonlinear scalarization
must regarded heuristic nature, applicable restricted classes problems.
second class avoids problems caused non-additive scalarized returns modifying either TD algorithm state representation. knowledge, two approaches
proposed Geibel (2006) address limitations TLO members
class. require reward accumulated objective current episode
stored. first algorithm, local decision-making based scalarized value
sum cumulative reward current state-action values. eliminates
problem non-additive returns, yields policy non-stationary respect
observed state, meaning algorithm may converge. second approach augments state representation cumulative reward. approach converges
correct policy learns slowly, due increase size state space.
92

fiA Survey Multi-Objective Sequential Decision-Making

third class uses TD methods learn policies based linear scalarizations.
policy selection mechanism based nonlinear scalarization used form
meta-policy base policies. Multiple Directions Reinforcement Learning
(MDRL) algorithm Mannor Shimkin (2001, 2004) uses approach
context on-line learning non-episodic tasks. user specifies target region within
long-term average reward lie. initial active policy chosen arbitrarily
followed average reward moves outside target region agent
specified reference state. point, direction current average reward
vector closest point target set calculated, policy whose direction best
matches target direction selected active policy. way, average reward
steered towards users specified target region. underlying base policies
utilize linear scalarization, nature policy-selection mechanism means
overall non-stationary policy formed base policies optimal nonlinear
scalarization specified users defined target set. Vamplew et al. (2009) suggest
similar approach episodic tasks, TD used first learn policies optimal
linear scalarization range different w, stochastic mixture policy
constructed optimal regards nonlinear scalarization.
fourth class uses policy-search algorithms directly learn policy without learning value function. single-policy MORL, research policy-search approaches
focused policy-gradient methods (Sutton, McAllester, Singh, & Mansour, 2000; Kohl &
Stone, 2004; Kober & Peters, 2011). methods, policy iteratively adjusted
direction gradient value respect parameters (usually probability
distributions actions per state) policy. Shelton (2001) proposes algorithm
first learns optimal policy individual objective. used base policies form initial mixture policy stochastically selects base policy start
episode. hill-climbing method based weighted convex combination
normalized objective gradients iteratively improves mixture policy. approach
directly fit taxonomy returns never scalarized. Instead,
weights used find step direction relative current policy parameters.
practical perspective, behavior akin single-policy RL using nonlinear
scalarization function, converges single Pareto-optimal policy need lie
convex hull. Uchibe Doya (2009) propose policy-gradient method
MORL called Constrained Policy Gradient RL (CPGRL) uses gradient projection technique find policies whose average reward satisfies constraints one
objectives. Sheltons approach, CPGRL learns stochastic policies works
nonlinear scalarization functions.
6.2 Multiple-Policy Learning Linear Scalarization Functions
unknown weights decision support scenarios, f linear, MORL algorithms
aim learn CCS possible policies. simple inefficient approach used
Castelletti et al. (2002) run TD multiple times different values w.
simplest case, runs conducted sequentially gradually build approximate
CCS. Natarajan Tadepalli (2005) showed approach made efficient
reusing policies learned earlier runs similar w. show
93

fiRoijers, Vamplew, Whiteson & Dazeley

improves greatly sample costs learning policy w similar already
visited previous runs. However, many samples typically still required good
approximate CCS obtained.
sophisticated approach approximating convex coverage set learn multiple policies parallel. Several algorithms proposed achieve within
TD learning framework. approach Hiraoka, Yoshida, Mishima (2009) similar
CHVI planning algorithm Barrett Narayanan (2008) (see Section 5.2)
learns parallel optimal value function w, using convex hull representation.
approach prone infinite growth number vertices convex hull polygons,
threshold margin applied hull representations iteration, eliminating points contribute little hulls hypervolume. Hiraoka et al. (2009) present
algorithm adapt margins learning improve efficiency, note many
parameters must tuned effective performance. Mukai, Kuroe, Iima (2012) present
similar extension CHVI learning context. address problematic growth
number values stored pruning vectors Q-value update: vector
selected random set vectors stored given state-action pair
others lying within threshold distance deleted.
approaches Hiraoka et al. (2009) Mukai et al. (2012) designed
on-line learning. contrast, Multi-Objective Fitted Q-Iteration (MOFQI) (Castelletti,
Pianosi, & Restelli, 2011, 2012) off-line approach learning multiple policies. MOFQI
multi-objective extension Fitted Q-Iteration (FQI) algorithm (Ernst et al., 2005)
uses combination historical data single-step transition dynamics
environment, initial function approximator, Q-learning update rule construct
dataset maps state-action pairs expected return. dataset used
train improved function approximator process repeats values
function approximator converge. MOFQI provides computationally efficient extension
FQI multiple objectives including w input function approximator
constructing expanded training data set containing training instances randomly
generated ws. Since learned function generalizes across weight space addition
state-action space, used construct policy w.
discussed Section 5.3, Lizotte et al. (2010) Lizotte et al. (2012) describe valueiteration algorithm find convex hull policies finite horizon tasks. note
method applied learning context estimating model state transition
probabilities immediate rewards basis experience environment.
approach demonstrated task analyzing randomized drug trial data producing
estimates historical data gathered clinical trials.
6.3 Multiple-Policy Learning Monotonically Increasing Scalarization
Functions
f nonlinear, MORL algorithms unknown weights decision support scenarios aim learn PCS. linear scalarization case, simplest approach
run single-objective algorithms multiple times varying w. Shelton (2001) demonstrates approach policy-gradient algorithm, Vamplew et al. (2011)
TLO method Gabor et al. (1998). approach however, requires
94

fiA Survey Multi-Objective Sequential Decision-Making

f explicitly known learning algorithm, may undesirable decision
support scenario.
knowledge, currently methods learning multiple policies
nonlinear f using value-function approach. might seem possible adapt convex
hull methods CHVI using Pareto-dominance operators place convex-hull
calculations, straightforward. scalarized values policies
certain state non-additive, cannot restrict stationary policies want
find deterministic Pareto-optimal policies (as mentioned Section 4.3.2). However,
Bellman equation CHVI work, additivity, resulting sufficiency
deterministic policies, required. discuss options developing multiple-policy learning
methods nonlinear f Sections 8.1 8.2.
Given extensive research multi-objective evolutionary algorithms (MOEAs)
(Coello Coello, Lamont, & Van Veldhuizen, 2002; Tan, Khor, Lee, & Sathikannan, 2003;
Drugan & Thierens, 2012) evolutionary methods RL (Whiteson, 2012), surprisingly little work evolutionary approaches MORL. methods populationbased, well suited approximating Pareto fronts, would thus seem natural
fit f nonlinear. knowledge, Handa (2009b) first apply MOEAs
MORL, extending Estimation Distribution (EDA) evolutionary algorithms handle multiple objectives. EDA-RL (Handa, 2009a) uses Conditional Random Fields (CRF)
represent probabilistic policies. initial set policies used generate set
episodes. best episodes set selected CRFs likely produce trajectories generated. policies formed CRFs constitute
next generation. Handa (2009b) extends EDA-RL MOMDPs using Paretodominance based fitness metric select best episodes.
Soh Demiris (2011) apply MOEAs MORL. Policies represented
Stochastic Finite State Controllers (SFSC) optimized using two different MOEAs:
NSGA2, standard evolutionary algorithm, MCMA, EDA. use SFSCs gives
rise large search space, necessitating addition local search operator. local
search generates random w, uses scalarize rewards, performs gradient-based
search SFSC. Empirical comparisons multi-objective variants three POMDP
benchmarks demonstrate evolutionary methods generally superior purely
local-search approach, local search combined evolution usually outperforms
purely evolutionary methods. one papers directly consider partially
observable MOMDPs.

7. MOMDP Applications
Multi-objective methods planning learning employed wide range
applications, simulation real-world settings. section, survey
applications. sake brevity, list comprehensive instead aims
provide illustrative range examples. First, discuss use multi-objective
methods specific applications. Second, discuss research identified broader
classes problems multi-objective methods play useful role.
95

fiRoijers, Vamplew, Whiteson & Dazeley

7.1 Specific Applications
important factor driving interest multi-objective decision-making increasing
social political emphasis environmental concerns. more, decisions must
made trade economic, social, environmental objectives. reflected
fact substantial proportion applications multi-objective methods
environmental component.
Perhaps extensively researched application water reservoir control problem considered Castelletti et al. (2002), Castelletti, Pianosi, Soncini-Sessa (2008),
Castelletti et al. (2011, 2012) Castelletti, Pianosi, Restelli (2013). general
task find control policy releasing water dam balancing multiple uses
reservoir, including hydroelectric production flood mitigation. Management
hydroelectric power production examined Shabani (2009). Another environmental application forest management balance economic benefits
timber harvesting environmental aesthetic objectives, demonstrated
simulation Gong (1992) Bone Dragicevic (2009).
Several researchers considered environmentally-motivated applications concerning management energy consumption. SAVES system developed Kwak
et al. (2012) controls various aspects commercial building (lighting, heating, airconditioning, computer systems) provide suitable trade-off energy consumption comfort buildings occupants. Simulation results indicate
SAVES reduce energy consumption approximately 30% compared manual control
system, maintaining slightly improving occupant comfort. Tesauro et al.
(2007) Liu et al. (2010) consider problem controlling computing server,
objectives minimizing response time user requests power consumption.
Guo et al. (2009) apply MORL develop broker agent electricity market.
broker sets caps group agents sit hierarchy manage energy
consumption device level, must balance energy cost system stability.
Shelton (2001) examines application MORL developing broker agents.
However, case agents task financial rather environmental, acting
market maker sets buy sell prices resources market. aim balance
objectives maximizing profit minimizing spread (the difference buy
sell prices) lead larger volume trades11 .
Computing communications applications widely considered. Perez
et al. (2009) apply MORL allocation resources jobs cloud computing scenario, objectives maximizing system responsiveness, utilization resources,
fairness amongst different classes user. Comsa et al. (2012) consider maximize
system throughput ensure user equity context Long Term Evolution mobile
communications packet scheduling protocol. Tong Brown (2002) use constraint-based
scalarization address tasks call access control routing broadband multimedia network. system aims maximize profit (a function throughput)
satisfying constraints quality service metrics (capacity constraints fairness constraints), uses methods similar Gabor et al. (1998). Zheng, Li, Qiu, Gong
11. Sheltons model market directly model trading volume, spread used proxy
volume.

96

fiA Survey Multi-Objective Sequential Decision-Making

(2012) use constrained MORL methods make routing decisions cognitive radio
network, aiming minimize average transmission delay maintaining acceptably
low packet loss rate.
Industrial mechanical control, important application single-objective MDP
methods, explored MOMDP researchers. Aoki, Kimura, Kobayashi
(2004) apply distributed RL control sewage flow system, exploiting systems hierarchical structure find solution minimizes violation stock levels node
flow system, smoothing variation flow source. Aissani et al. (2008)
apply MORL maintenance scheduling within manufacturing plant minimize time
taken complete maintenance tasks machine downtime. Aissani, Beldjilali,
Trentesaux (2009) build work applying simulation real petroleum refinery demonstrating ability adapt unscheduled corrective maintenance required
due equipment failures. control wet clutch heavy-duty transmission systems
examined Van Vaerenbergh et al. (2012). twin objectives minimizing
engagement time, making transition smooth.
Robotics popular application MOMDPs, though work far
simulation rather real robots. Maravall de Lope (2002) consider control
two-limbed brachiating robot, objectives moving desired direction
avoiding collisions12 . Nojima, Kojima, Kubota (2003) attempt balance
objectives progress target collision avoidance. agent makes use
predefined behavioral modules target tracing, collision avoidance, wall following,
MORL used dynamically adjust weighting modules. Meisner (2009)
identifies social robots promising application MOMDP methods: behavior
inherently multi-objective must carry task without causing anxiety
discomfort humans.
MORL applied control traffic infrastructure. Yang Wen
(2010) apply control freeway on-ramps vehicle management systems, aiming
maximize throughput equity freeway system. Multiple agents
shared policies used, action selection occurring via negotiation agents.
Similarly, Dusparic Cahill (2009) apply MORL control traffic lights intersections
urban environment minimize waiting time two different classes vehicles. Yin,
Duan, Li, Zhang (2010) Houli, Zhiheng, Yi (2010) apply MORL traffic
light control. novelty approach lies considering different objectives based
current state road system; minimizing vehicle stops prioritized traffic
free-flowing; minimizing waiting time emphasized system medium load;
minimizing queue length intersections targeted system congested.
Lizotte et al. (2010, 2012) consider medical application: prescribing appropriate
drug regime patient achieve acceptable trade-off drugs effectiveness severity side effects. system learns multiple policies based
12. many robotic applications may ideal avoid collisions completely, environments
may possible (e.g., presence moving obstacles whose velocity faster
robot, difficult predict, may case humans human-controlled vehicles)
reducing likelihood impact collisions may reasonable attempting
find collision-free policy. See example Holenstein Badreddin (1991) Pervez Ryu
(2008).

97

fiRoijers, Vamplew, Whiteson & Dazeley

static data produced randomized controlled drug trials. selection best
treatment specific patient made doctor based patients individual circumstances. application excellent example problem stochastic
approaches mixture policies inappropriate. policy maximizes symptom relief side effects one patient minimizes side effects
symptom relief next patient may appear give excellent results averaged
across episodes. However, experience individual patient likely regarded
undesirable.
7.2 Applications within Broader Planning Learning Tasks
addition specific applications discussed above, several authors identified
general classes tasks multi-objective sequential decision-making applied.
7.2.1 Probabilistic Risk-Aware Planning
Cheng, Subrahmanian, Westerberg (2005) argue decision-making uncertainty inherently multi-objective nature. Even single reward
considered (such profit), environmental uncertainty means expected value
alone insufficient support good decision-making; decision-maker must consider
variance return. Similarly, Bryce (2008) states probabilistic planning
inherently multi-objective due need optimize cost probability
success plan. criticizes approaches either aggregate factors bound one
optimize other, arguing favor explicitly multi-objective methods.
aptly named Probabilistic Planning Multi-objective! paper Bryce, Cushing,
Kambhampati (2007) demonstrates might achieved, describing method based
multi-objective dynamic programming belief states, multi-objective extension
Looping AO* search algorithm find set Pareto-optimal plans. Recent work
Kolobov, Mausam, Weld (2012) Teichteil-Konigsbuch (2012b) examine extension stochastic shortest path (SSP) methods problems dead-end states exist.
SSP methods assume least one policy exists guaranteed reach goal;
presence dead-ends policy exists, authors propose algorithms
aim maximize probability reaching goal minimize cost
paths found goal.
Bryce (2008) notes probabilistic plan fails environment enters non-goal
absorbing state. Hence, multi-objective probabilistic planning strong parallels
research risk-aware RL carried Geibel (2001) Geibel Wysotzki (2005),
add second reward signal indicating transition environment error
state. Defourny, Ernst, Wehenkel (2008) provide useful insights incorporation risk-awareness MDP methods. review range criteria proposed
constraining risk, note many nonlinear produce non-additive
scalarized returns incompatible local decision-making methods based
Bellman equation. recommend custom risk-control requirements
mostly enforced heuristically, altering policy optimization procedures checking
compliance policies initial requirements. Multi-policy MOMDP methods treating risk additional objective would satisfy requirement: iden98

fiA Survey Multi-Objective Sequential Decision-Making

tified coverage set, risk-aware metric used select best policy.
However, measures risk may expressed directly discounted cumulative
rewards. example, agent may wish minimize variance expected return particular reward signal rather discounted cumulative value. Methods
based multi-objective probabilistic model checking (Courcoubetis & Yannakakis, 1998;
Forejt, Kwiatkowska, Norman, Parker, & Qu, 2011; Forejt, Kwiatkowska, & Parker, 2012;
Teichteil-Konigsbuch, 2012a), evaluate whether system modelled MDP satisfies multiple, possibly conflicting, properties, may suitable tasks.
7.2.2 Multi-Agent Systems
use MDPs within multi-agent systems widely explored (Bosoniu, Babuska,
& Schutter, 2008), several authors proposed approaches strongly related
MOMDPs. multi-agent system, agent objective, effective
overall performance must consider actions affect agents.
agents completely self-interested, problem framed MOMDP
treating effects agents additional objectives. example, Mouaddib (2006)
uses multi-objective dynamic programming facilitate cooperation multiple agents
whose underlying goals may conflicting. state-action pair, agent stores
three values: local utility, gain agents receive, penalty inflicts
agents. policy agent established converting vector values
regret ratios applying leximin ordering ratios.
Dusparic Cahill (2010) compare application MORL multi-agent tasks
multi-agent methods evolutionary ant-colony algorithms. Dusparic
Cahill (2009) extend W-Learning algorithm Humphrys (1996). agent learns
local policies (one objectives) remote policies (one
local policy neighboring agents). timestep, local policies
active remote policies agent nominate actions, winning action selected
combining action values across nominating policies. weighting term applied
values remote policies determine level cooperation agent offers neighbor.
Experimental results urban traffic control simulator show substantial improvement
level cooperation non-zero. work similar Schneider, Wong,
Moore, Riedmiller (1999), addresses use multiple agents distributed
network power distribution grid, aim maximize global reward
formed combination agents local reward. demonstrate
agent focuses local reward, policies learned may maximize global
reward, performance improved agent perform linearly scalarized
learning using local reward rewards neighboring agents.
7.2.3 Multi-Objective Optimization using Reinforcement Learning
Reinforcement learning primarily applied sequential decision-making tasks dynamic
environment. However employed control search mechanisms static optimization tasks scheduling (Carchrae & Beck, 2005). Multi-objective optimization
static tasks design well-established field and, majority work
99

fiRoijers, Vamplew, Whiteson & Dazeley

employed mathematical evolutionary approaches (Coello Coello et al., 2002),
authors explored application reinforcement learning contexts.
Mariano Morales (1999, 2000b, 2000a) investigate use RL methods (Ant-Q
Q-learning) search mechanism optimization multi-objective design tasks.
values decision variables considered current state, actions defined
alter values variables. Multiple agents explore state space parallel.
Agents divided families, family focuses single objective.
end episode, final states found agent evaluated. Undominated solutions kept archive agents discovered solutions rewarded,
increasing likelihood similar policies followed future. method
shown work small number test problems evolutionary multi-objective
optimization literature. Liao, Wu, Jiang (2010) apply RL search static control
settings power generation system objectives reducing fuel usage ensuring voltage stability. propose RL algorithm formulated specifically tasks
high-dimensional state spaces, compare performance evolutionary
multi-objective algorithm, finding RL method discovers fronts
accurate better distributed, improving speed search.
Note effectively apply RL multi-objective optimization, assumptions usually made nature environment. example, Liao et al. (2010) require
action increases decreases value precisely one state variable. result,
methods likely limited applicability general MORL problems
described earlier.

8. Future Work
section, enumerate possibilities future research multi-objective
planning learning.
8.1 Model-Based Methods
mentioned Section 6, little work model-based approaches
MORL. Given breadth planning methods MOMDPs, could employed
model-based MORL methods subroutines, surprising. knowledge,
work area Lizotte et al. (2010, 2012), model MOMDPs
transition probabilities reward function derived historical data,
spline-based multi-objective value iteration approach applied model. general,
learning models seems negligibly harder single-objective setting, since
estimates reward function learned separately. problem learning
transition function, generally considered hard part model learning, identical
single-objective setting. Especially multiple-policy scenarios, model-based approaches
MORL could greatly reduce sample costs: model learned, entire
CCS PCS computed off-line, without requiring additional samples.
100

fiA Survey Multi-Objective Sequential Decision-Making

8.2 Learning Multiple Policies Monotonically Increasing Scalarization
Functions using Value Functions
mentioned Section 6.3, aware methods use value function
approach learn multiple policies PCS. stochastic policies permitted,
problem easier learn CCS(m
DS ), use either mixture policies
(Vamplew et al., 2009) stationary randomizations (Wakuta, 1999) policies
CCS (see Section 4.3.3). However, deterministic policies permitted,
problem difficult. One option could use finite-horizon approximation
infinite horizon problem. planning backwards planning horizon, expected
reward timesteps go approximates infinite-horizon value better better
. mentioned Section 5.2, similar approaches used POMDP
setting. Another way find good approximations non-stationary policies could
learn stationary policies (perhaps extending CON-MDP (Wiering & De Jong, 2007)
learning setting), prefix timesteps non-stationary policy.
8.3 Many-Objective Sequential Decision-Making
majority research reviewed article, theoretical applied, deals
MOMDPs objectives. mirrors state early evolutionary multiobjective research, focused almost exclusively problems two three
objectives. However, last decade growing interest evolutionary
methods so-called many-objective problems, least four sometimes
fifty objectives (Ishibuchi, Tsukamoto, & Nojima, 2008). research
shown many algorithms perform well objectives scale poorly
number objectives, necessitating special algorithms many-objective setting.
many-objective MDPs received little consideration far, numerous real-world control problems naturally modeled way. example,
Fleming et al. (2005) point many-objective control problems commonly arise
engineering, give example jet engine control system eight objectives.
many-objective problems considered evolutionary computation, seems likely
least methods explored far scale poorly number objectives. example, multi-policy MOMDP planning algorithm described Lizotte
et al. (2010) limited problems two objectives.
key challenge posed many-objective problems number undominated
solutions typically grows exponentially number objectives. particularly
problematic multiple-policy MOMDP methods. Fleming et al. (2005) note one
effective approaches used many-objective evolutionary computation
incorporate user preferences restrict search space small region interest.
particular, recommend interactive preference articulation user interactively steers system towards desirable solution optimization. Vamplew
et al. (2011) raise possibility incorporating approach MORL,
aware research actually done so.
101

fiRoijers, Vamplew, Whiteson & Dazeley

(3,0)

(0,3)

B

C


(1,1)


Figure 3: MOMDP two objectives four states.
8.4 Expectation Scalarized Return
Section 3, defined scalarized value Vw (s) result applying scalarization function f multi-objective value V (s) according w, i.e., Vw (s) = f (V (s), w).
Since V (s) expectation, means scalarization function applied
expectation computed, i.e.,
Vw (s) = f (V (s), w) = f (E[


X

k rk | , s0 = s], w).

k=0

formulation, refer scalarization expected return (SER)
standard literature. However, option. possible define
Vw (s) expectation scalarized return (ESR):
Vw (s) = E[f (


X

k rk , w) | , s0 = s]

k=0

definition used critically affect policies preferred. example,
consider following MOMDP, illustrated Figure 3. four states (A, B, C,
D) two objectives. agent starts state two possible actions: a1 transits
state B C, probability 0.5, a2 transits state probability 1.
actions lead (0, 0) reward. states B, C one action,
leads deterministic reward (3, 0) B, (0, 3) C, (1, 1) D.
scalarization function multiplies two objectives together. Thus, SER,
Vw (s) = V1 (s)V2 (s),
ESR,
Vw (s) = E[


X
k=0

k rk1


X
k=0

102


k rk2 | , s0 = s],

fiA Survey Multi-Objective Sequential Decision-Making

rki reward i-th objective timestep k (w needed example
since f involves constants). 1 (A) = a1 2 (A) = a2 , multi-objective
values V1 (A) = (1.5/(1 ), 1.5/(1 )) V2 (A) = (/(1 ), /(1 )).
SER, leads scalarized values V 1 (A) = (1.5/(1 ))2 V 2 (A) =
(/(1 ))2 consequently 1 preferred. ESR, however, V 1 (A) = 0
V 2 (A) = (/(1 ))2 thus 2 preferred.
Intuitively, SER formulation appropriate policy used many times
return accumulates across episodes, e.g., user using policy
time. Then, scalarizing expected reward makes sense 1 preferable
expectation accumulate return objectives. However, policy
used times return accumulate across episodes, e.g.,
episode conducted different user, ESR formulation appropriate.
case, expected return scalarization interest 2 preferable
1 always yield zero scalarized return given episode.
knowledge, literature MOMDPs employs ESR formulation,
even though many real-world scenarios seems appropriate.
example, medical application Lizotte et al. (2010) mentioned Section 7,
patient gets one episode treat illness, thus clearly interested
maximizing ESR, SER. Thus, believe developing methods MOMDPs
ESR formulation critical direction future research.

9. Conclusions
article presented survey algorithms designed sequential decision-making problems multiple objectives.
order make explicit circumstances special methods needed
solve multi-objective problems, identified three distinct scenarios converting
problem single-objective one impossible, infeasible, undesirable. well
providing motivation need multi-objective methods, scenarios represent
three main ways methods applied practice.
proposed taxonomy classifies multi-objective methods according applicable scenario, scalarization function (which projects multi-objective values scalar
ones), type policies considered. showed factors determine
nature optimal solution, single policy, coverage set (convex
Pareto). taxonomy based utility-based approach, sees scalarization
function part utility, thus part problem definition. contrasts
so-called axiomatic approach, usually assumes Pareto front appropriate
solution. showed utility-based approach used justify choice
solution set. Following line thought, observed (Observation 1) computing
Pareto front often necessary, many cases convex coverage set
deterministic stationary policies sufficient.
Using taxonomy, surveyed literature multi-objective methods planning
learning. interesting observation learning methods use modelfree rather model based approach, identifying latter understudied class
103

fiRoijers, Vamplew, Whiteson & Dazeley

methods. Another part taxonomy yet widely studied learning
case monotonically increasing scalarization functions.
discussed key applications MOMDP methods motivation importance
methods. Applications identified diverse range fields including environmental management, financial markets, information communications technology,
control industrial processes, robotic systems traffic infrastructure. addition connections identified multi-objective sequential decision-making broad
areas research probabilistic planning model-checking, multi-agent systems
general multi-objective optimization.
Finally, outlined several opportunities future work, include understudied
areas (model-based methods, learning monotonically increasing scalarization settings,
many-objective sequential decision-making), reformulation objective
MOMDPs Expectation Scalarized Return particularly important
optimize policy executed once.

Acknowledgments
would thank Matthijs Spaan, Frans Oliehoek, Matthijs Snel, Marie D. Manner
Samy Sa, well anonymous reviewers, valuable feedback. work
supported Netherlands Organisation Scientific Research (NWO): DecisionTheoretic Control Network Capacity Allocation Problems (#612.001.109) project.

References
Aberdeen, D., Thiebaux, S., & Zhang, L. (2004). Decision-theoretic military operations
planning. Proc. ICAPS, Vol. 14, pp. 402411.
Aissani, N., Beldjilali, B., & Trentesaux, D. (2008). Efficient effective reactive scheduling manufacturing system using Sarsa-multi-objective agents. MOSIM08: 7th
Conference Internationale de Modelisation et Simulation, pp. 698707.
Aissani, N., Beldjilali, B., & Trentesaux, D. (2009). Dynamic scheduling maintenance
tasks pretroleum industry: reinforcement approach. Engineering Applications
Artificial Intelligence, 22, 10891103.
Altman, E. (1999). Constrained Markov Decision Processes. Chapman Hall/CRC,
London.
Aoki, K., Kimura, H., & Kobayashi, S. (2004). Distributed reinforcement learning using
bi-directional decision making multi-criteria control multi-stage flow systems.
8th Conference Intelligent Autonomous Systems, Vol. 2004.03, pp. 281290.
Barrett, L., & Narayanan, S. (2008). Learning optimal policies multiple criteria.
Proceedings 25th International Conference Machine Learning, pp. 4147,
New York, NY, USA. ACM.
Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2003). Transition-Independent
Decentralized Markov Decision Processes. Proc. 2nd Intl Joint Conf.
Autonomous Agents & Multi-Agent Systems.
104

fiA Survey Multi-Objective Sequential Decision-Making

Bellman, R. E. (1957a). Markov decision process. Journal Mathematical Mech., 6,
679684.
Bellman, R. (1957b). Dynamic Programming. Princeton University Press.
Bhattacharya, B., Lobbrecht, A. H., & Solomantine, D. P. (2003). Neural networks reinforcement learning control water systems. Journal Water Resources Planning
Management, 129 (6), 458465.
Bone, C., & Dragicevic, S. (2009). GIS intelligent agents multiobjective natural
resource allocation: reinforcement learning approach. Transactions GIS, 13 (3),
253272.
Bosoniu, L., Babuska, R., & Schutter, B. D. (2008). comprehensive survey multiagent
reinforcement learning. IEEE Transactions Systems, Man, Cybernetics - Part
C: Applications Reviews, 38 (2), 156172.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions computational leverage. Journal Artificial Intelligence Research,
11, 194.
Brazdil, T., Brozek, V., Chatterjee, K., Forejt, V., & Kucera, A. (2011). Two views
multiple mean-payoff objectives Markov decision processes. CoRR, abs/1104.3489.
Bryce, D. (2008). value(s) probabilistic plans. Workshop Reality Check
Planning Scheduling Uncertainty, ICAPS-08.
Bryce, D., Cushing, W., & Kambhampati, S. (2007). Probabilistic planning multiobjective!. Technical report 08-006, Arizona State University.
Carchrae, T., & Beck, J. C. (2005). Applying machine learning low-knowledge control
optimization algorithms. Computational Intelligence, 21 (4), 372387.
Cassandra, A., Littman, M. L., & Zhang, N. L. (1997). Incremental pruning: simple, fast,
exact method partially observable markov decision processes. Proceedings
Thirteenth conference Uncertainty artificial intelligence, pp. 5461.
Castelletti, A., Pianosi, F., & Restelli, M. (2013). multiobjective reinforcement learning
approach water resources systems operation: Pareto frontier approximation
single run. Water Resources Research.
Castelletti, A., Corani, G., Rizzolli, A., Soncini-Sessa, R., & Weber, E. (2002). Reinforcement learning operational management water system. IFAC Workshop
Modeling Control Environmental Issues, pp. 325330.
Castelletti, A., Galelli, S., Restelli, M., & Soncini-Sessa, R. (2010). Tree-based reinforcement learning optimal water reservoir operation. Water Resources Research,
46 (W09507).
Castelletti, A., Pianosi, F., & Restelli, M. (2011). Multi-objective Fitted Q-Iteration: Pareto
frontier approximation one single run. International Conference Networking,
Sensing Control, pp. 260265.
Castelletti, A., Pianosi, F., & Restelli, M. (2012). Tree-based Fitted Q-iteration multiobjective Markov decision processes. IEEE World Congress Computational
Intelligence.
105

fiRoijers, Vamplew, Whiteson & Dazeley

Castelletti, A., Pianosi, F., & Soncini-Sessa, R. (2008). Water reservoir control economic, social environmental constraints. Automatica, 44, 15951607.
Chatterjee, K. (2007). Markov decision processes multiple long-run average objectives.
FSTTCS, Vol. LNCS 4855, pp. 473484.
Chatterjee, K., Majumdar, R., & Henzinger, T. A. (2006). Markov decision processes
multiple objectives. Proceedings 23rd Annual conference Theoretical
Aspects Computer Science, STACS06, pp. 325336, Berlin, Heidelberg. SpringerVerlag.
Cheng, L., Subrahmanian, E., & Westerberg, A. (2005). Multiobjective decision processes
uncertainty: Applications, formulations solution strategies. Industrial
Engineering Chemistry Research, 44 (8), 24052415.
Clemen, R. T. (1997). Making Hard Decisions: Introduction Decision Analysis (2
edition). South-Western College Pub.
Coello Coello, C. A., Lamont, G. B., & Van Veldhuizen, D. A. (2002). Evolutionary Algorithms Solving Multi-Objective Problems. Kluwer Academic Publishers.
Comsa, I., Aydin, M., Zhang, S., Kuonen, P., & Wagen, J.-F. (2012). Multi objective resource scheduling LTE networks using reinforcement learning. International Journal
Distributed Systems Technologies, 3 (2), 3957.
Courcoubetis, C., & Yannakakis, M. (1998). Markov decision processes regular events.
IEEE Transactions Automatic Control, 43 (10), 13991418.
Crites, R. H., & Barto, A. G. (1996). Improving elevator performance using reinforcement
learning. Touretzky, D. S., Mozer, M. C., & Hasselmo, M. E. (Eds.), Advances
Neural Information Processing Systems 8, pp. 10171023. MIT Press.
Daellenbach, H. G., & Kluyver, C. A. D. (1980). Note multiple objective dynamic
programming. Journal Operational Research Society, 31, 591594.
Defourny, B., Ernst, D., & Wehenkel, L. (2008). Risk-aware decision making dynamic
programming. NIPS 2008 Workshop Model Uncertainty Risk RL.
Diehl, M., & Haimes, Y. Y. (2004). Influence diagrams multiple objectives tradeoff analysis. Systems, Man Cybernetics, Part A: Systems Humans, IEEE
Transactions on, 34 (3), 293304.
Drugan, M. M., & Thierens, D. (2012). Stochastic pareto local search: Pareto neighbourhood
exploration perturbation strategies. Journal Heuristics, 18 (5), 727766.
Dusparic, I., & Cahill, V. (2009). Distributed W-learning: Multi-policy optimization selforganizing systems. Third IEEE International Conference Self-Adaptive
Self-Organizing Systems, pp. 2029.
Dusparic, I., & Cahill, V. (2010). Multi-policy optimization self-organizing systems.
SOAR 2009, LNCS 6090, pp. 101126.
Dyer, J. S., Fishburn, P. C., Steuer, R. E., Wallenius, J., & Zionts, S. (1992). Multiple criteria decision making, multiattribute utility theory: next ten years. Management
Science, 38 (5), 645654.
106

fiA Survey Multi-Objective Sequential Decision-Making

Ernst, D., Geurts, P., & Wehenkel, L. (2005). Tree-based batch mode reinforcement learning.
Journal Machine Learning Research, 6, 503556.
Ernst, D., Glavic, M., & Wehenkel, L. (2004). Power systems stability control: Reinforcement learning framework. IEEE Transactions Power Systems, 19 (1), 427435.
Feinberg, E. A., & Shwartz, A. (1995). Constrained Markov decision models weighted
discounted rewards. Mathematics Operations Research, 20 (2), 302320.
Ferreira, L., Bianchi, R., & Ribeiro, C. (2012). Multi-agent multi-objective reinforcement
learning using heuristically accelerated reinforcement learning. 2012 Brazilian
Robotics Symposium Latin American Robotics Symposium, pp. 1420.
Fleming, P., Purshouse, R., & Lygoe, R. (2005). Many-objective optimization: engineering design perspective. Evolutionary Multi-Criterion Optimization: Lecture Notes
Computer Science, Vol. 3410, pp. 1432.
Forejt, V., Kwiatkowska, M., Norman, G., Parker, D., & Qu, H. (2011). Quantitative
multi-objective verification probabilistic systems. Tools Algorithms
Construction Analysis Systems, pp. 112127. Springer Berlin Heidelberg.
Forejt, V., Kwiatkowska, M., & Parker, D. (2012). Pareto curves probabilistic model
checking. Automated Technology Verification Analysis, pp. 317332.
Springer Berlin Heidelberg.
Furnkranz, J., Hullermeier, E., Cheng, W., & Park, S.-H. (2012). Preference-based reinforcement learning: formal framework policy iteration algorithm. Machine
Learning, 89 (1-2), 123156.
Gabor, Z., Kalmar, Z., & Szepesvari, C. (1998). Multi-criteria reinforcement learning.
Fifteenth International Conference Machine Learning, pp. 197205.
Geibel, P., & Wysotzki, F. (2005). Risk-sensitive reinforcement learning applied control
constraints. Journal Artificial Intelligence Research, 24, 81108.
Geibel, P. (2001). Reinforcement learning bounded risk. Proceeding 18th
International Conference Machine Learning, pp. 162169.
Geibel, P. (2006). Reinforcement learning MDPs constraints. European Conference Machine Learning, Vol. 4212, pp. 646653.
Gelly, S., & Silver, D. (2011). Monte-carlo tree search rapid action value estimation
computer go. Artificial Intelligence, 175 (11), 18561875.
Gong, P. (1992). Multiobjective dynamic programming forest resource management.
Forest Ecology Management, 48, 4354.
Guo, Y., Zeman, A., & Li, R. (2009). reinforcement learning approach setting multiobjective goals energy demand management. International Journal Agent Technologies Systems, 1 (2), 5570.
Handa, H. (2009a). EDA-RL: Estimation distribution algorithms reinforcement learning problems. ACM/SIGEVO Genetic Evolutionary Computation Conference,
pp. 405412.
107

fiRoijers, Vamplew, Whiteson & Dazeley

Handa, H. (2009b). Solving multi-objective reinforcement learning problems EDA-RL acquisition various strategies. Proceedings Ninth Internatonal Conference
Intelligent Sysems Design Applications, pp. 426431.
Hiraoka, K., Yoshida, M., & Mishima, T. (2009). Parallel reinforcement learning weighted
multi-criteria model adaptive margin. Cognitive Neurodynamics, 3, 1724.
Holenstein, A. A., & Badreddin, E. (1991). Collision avoidance behavior-based mobile
robot design. Robotics Automation, 1991. Proceedings., 1991 IEEE International Conference on, pp. 898903. IEEE.
Houli, D., Zhiheng, L., & Yi, Z. (2010). Multiobjective reinforcement learning traffic
signal control using vehicular ad hoc network. EURASIP Journal Advances
Signal Processing.
Howard, R. A. (1960). Dynamic programming Markov decision processes. MIT Press.
Humphrys, M. (1996). Action selection methods using reinforcement learning. Proceedings Fourth International Conference Simulation Adaptive Behavior, pp.
135144.
Ishibuchi, H., Tsukamoto, N., & Nojima, Y. (2008). Evolutionary many-objective optimisation: short review. IEEE Congress Evolutionary Computation, pp. 24192426.
Issabekov, R., & Vamplew, P. (2012). empirical comparison two common multiobjective reinforcement learning algorithms. AI2012: 25th Australasian Joint
Conference Artificial Intelligence, pp. 626636.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning acting
partially observable stochastic domains. Artificial Intelligence, 101, 99134.
Karlsson, J. (1997). Learning Solve Multiple Goals. Ph.D. thesis, University Rochester.
Kober, J., & Peters, J. (2011). Policy search motor primitives robotics. Machine
Learning, 12, 171203.
Kober, J., & Peters, J. (2012). Reinforcement learning robotics: survey. Wiering,
M., & Otterlo, M. (Eds.), Reinforcement Learning, Vol. 12 Adaptation, Learning,
Optimization, pp. 579610. Springer Berlin Heidelberg.
Kocsis, L., & Szepesvari, C. (2006). Bandit based monte-carlo planning. 17th European
Conference Machine Learning, pp. 282293. Springer.
Kohl, N., & Stone, P. (2004). Policy gradient reinforcement learning fast quadrupedal
locomotion. Proceedings IEEE International Conference Robotics
Automation, pp. 26192624.
Kolobov, A., Mausam, & Weld, D. S. (2012). theory goal-oriented mdps dead
ends. Proceedings Twenty-Eighth Conference Uncertainty Artificial
Intelligence.
Kwak, J., Varakantham, P., Maheswarn, R., Tambe, M., Jazizadeh, F., Kavulya, G., Klein,
L., Becerik-Gerber, B., Hayes, T., & Wood, W. (2012). SAVES: sustainable multiagent application conserve building energy considering occupants. 11th International Conference Autonomous Agents Multiagent Systems, pp. 2128.
108

fiA Survey Multi-Objective Sequential Decision-Making

Liao, H., Wu, Q., & Jiang, L. (2010). Multi-objective optimization reinforcement learning
power system dispatch voltage stability. Innovative Smart Grid Technologies
Conference Europe.
Lin, C.-T., & Chung, I.-F. (1999). reinforcement neuro-fuzzy combiner multiobjective
control. IEEE Transactions Systems, Man Cyberbetics - Part B, 29 (6), 726
744.
Liu, C., Xu, X., & Hu, D. (2013). Multiobjective reinforcement learning: comprehensive
overview. Systems, Man, Cybernetics, Part C: Applications Reviews, IEEE
Transactions on, PP (99), 113.
Liu, W., Tan, Y., & Qiu, Q. (2010). Enhanced q-learning algorithm dynamic power
management performance constraints. DATE10, pp. 602605.
Lizotte, D. J., Bowling, M., & Murphy, S. A. (2010). Efficient reinforcement learning
multiple reward functions randomized clinical trial analysis. 27th International
Conference Machine Learning, pp. 695702.
Lizotte, D. J., Bowling, M., & Murphy, S. A. (2012). Linear fitted-q iteration multiple
reward functions. Journal Machine Learning Research, 13, 32533295.
Madani, O., Hanks, S., & Condon, A. (1999). undecidability probabilistic planning
infinite-horizon partially observable Markov decision problems. Proceedings
National Conference Artificial Intelligence (AAAI), pp. 541548.
Mannor, S., & Shimkin, N. (2001). steering approach multi-criteria reinforcement
learning. Neural Information Processing Systems, pp. 15631570.
Mannor, S., & Shimkin, N. (2004). geometric approach multi-criterion reinforcement
learning. Journal Machine Learning Research, 5, 325360.
Maravall, D., & de Lope, J. (2002). reinforcement learning method dynamic obstacle
avoidance robotic mechanisms. Computational Intelligent Systems Applied
Research: Proceedings 5th International FLINS Conference, pp. 485494, Singapore. World Scientific.
Mariano, C., & Morales, E. (1999). MOAQ Ant-Q algorithm multiple objective
optimization problems. GECCO-99: Proceedings Genetic Evolutionary
Computation Conference, pp. 894901.
Mariano, C., & Morales, E. (2000a). new approach solution multiple objective
optimization problems based reinforcement learning. Advances Artificial
Intelligence, International Joint Conference, 7th Ibero-American Conference AI,
15th Brazilian Symposium. Springer.
Mariano, C., & Morales, E. (2000b). new distributed reinforcement learning algorithm
multiple objective optimisation problems. Lecture Notes AI Vol 1952: Proceedings Mexican International Conference Artficial Intelligence, pp. 212223.
Springer.
Meisner, E. M. (2009). Learning Controllers Human-Robot Interaction. Ph.D. thesis,
Rensselaer Polytechnic Institute.
109

fiRoijers, Vamplew, Whiteson & Dazeley

Mouaddib, A.-I. (2006). Collective multi-objective planning. Proceedings IEEE
Workshop Distributed Intelligent Systems: Collective Intelligence Applications (DIS06), pp. 4348, Washington, DC, USA. IEEE Computer Society.
Mukai, Y., Kuroe, Y., & Iima, H. (2012). Multi-objective reinforcement learning method
acquiring Pareto optimal policies simultaneously. IEEE International Conference Systems, Man Cybernetics, pp. 19171923.
Natarajan, S., & Tadepalli, P. (2005). Dynamic preferences multi-criteria reinforcement
learning. International Conference Machine Learning, pp. 601608.
Nojima, Y., Kojima, F., & Kubota, N. (2003). Local episode-based learning multiobjective behavior coordination mobile robot dynamic environments.
12th IEEE International Conference Fuzzy Systems, Vol. 1, pp. 307312.
Ogryczak, W., Perny, P., & Weng, P. (2011). minimizing ordered weighted regrets
multiobjective Markov decision processes. 2nd International Conference
Algorithmic Decision Theory, pp. 190204.
Ong, S. C., Png, S. W., Hsu, D., & Lee, W. S. (2010). Planning uncertainty robotic
tasks mixed observability. International Journal Robotics Research, 29 (8),
10531068.
Pareto, V. (1896). Manuel dEconomie Politique. Giard, Paris.
Peek, N. B. (1999). Explicit temporal models decisiontheoretic planning clinical
management. Artificial Intelligence Medicine, 15 (2), 135154.
Perez, J., Germain-Renaud, C., Kegl, B., & Loomis, C. (2009). Responsive elastic computing. International Conference Autonomic Computing, pp. 5564.
Perny, P., & Weng, P. (2010). finding compromise solutions multiobjective Markov
decision processes. ECAI Multidisciplinary Workshop Advances Preference
Handling, pp. 5560.
Perny, P., Weng, P., Goldsmith, J., & Hanna, J. P. (2013). Approximation lorenz-optimal
solutions multiobjective markov decision processes. Workshops TwentySeventh AAAI Conference Artificial Intelligence.
Pervez, A., & Ryu, J. (2008). Safe physical human robot interaction-past, present
future. Journal Mechanical Science Technology, 22 (3), 469483.
Pineau, J., Gordon, G., & Thrun, S. (2006). Anytime point-based approximations large
POMDPs. Journal Artificial Intelligence Research, 27 (1), 335380.
Precup, D., Sutton, R. S., & Dasgupta, S. (2001). Off-policy temporal-difference learning
function approximation. Proceedings 18th International Conference
Machine Learning, pp. 417424.
Puterman, M. L. (1994). Markov decision processes: Discrete stochastic dynamic programming. John Wiley & Sons, Inc.
Roijers, D. M., Whiteson, S., & Oliehoek, F. A. (2013). Computing convex coverage sets
multi-objective coordination graphs. ADT 2013: Proceedings Third International Conference Algorithmic Decision Theory. appear.
110

fiA Survey Multi-Objective Sequential Decision-Making

Ross, S., Pineau, J., Paquet, S., & Chaib-draa, B. (2008). Online planning algorithms
POMDPs. Journal Artificial Intelligence Research, 32, 663704.
Russell, S., & Zimdars, A. L. (2003). Q-decomposition reinforcement learning agents.
Proceedings 20th International Conference Machine Learning, pp. 656663.
Schneider, J., Wong, W.-K., Moore, A., & Riedmiller, M. (1999). Distributed value functions. Proceedings 16th International Conference Machine Learning, pp.
371378, San Francisco, CA. Morgan Kaufmann.
Shabani, N. (2009). Incorporating flood control rule curves Columbia River hydroelectric system multireservoir reinforcement learning optimization model. Masters
thesis, University British Columbia.
Shelton, C. R. (2001). Importance sampling reinforcement learning multiple objectives. AI Technical Report 2001-003, MIT.
Shortreed, S., Laber, E., Lizotte, D., Stroup, T., Pineau, J., & Murphy, S. (2011). Informing
sequential clinical decision-making reinforcement learning: empirical study.
Machine Learning, 84, 109136.
Smallwood, R., & Sondik, E. (1973). optimal control partially observable Markov
processes finite horizon. Operations Research, 21 (5), 10711088.
Soh, H., & Demiris, Y. (2011). Evolving policies multi-reward partially observable
Markov decision processes (MR-POMDPs). GECCO11 Proceedings 13th
Annual Conference Genetic Evolutionary Computation, pp. 713720.
Sondik, E. (1971). optimal control partially observable processes finite horizon.
Ph.D. thesis, Stanford University, Stanford, California.
Sondik, E. (1978). optimal control partially observable Markov processes
infinite horizon: Discounted costs. Operations Research, 26 (2), 282304.
Spaan, M., & Vlassis, N. (2005). Perseus: Randomized point-based value iteration
POMDPs. Journal Artificial Intelligence Research, 24 (1), 195220.
Stewart, T. J. (1992). critical survey status multiple criteria decision making
theory practice. Omega, 20 (5), 569586.
Sutton, R. S. (1988). Learning predict methods temporal differences. Machine
Learning, 3 (1), 944.
Sutton, R. S., & Barto, A. G. (1998). Introduction Reinforcement Learning (1st edition).
MIT Press, Cambridge, MA, USA.
Sutton, R., McAllester, D., Singh, S., & Mansour, Y. (2000). Policy gradient methods
reinforcement learning function approximation. NIPS, pp. 10571063.
Szita, I. (2012). Reinforcement learning games. Wiering, M., & Otterlo, M. (Eds.),
Reinforcement Learning, Vol. 12 Adaptation, Learning, Optimization, pp. 539
577. Springer Berlin Heidelberg.
Tan, K. C., Khor, E. F., Lee, T. H., & Sathikannan, R. (2003). evolutionary algorithm
advanced goal priority specification multi-objective optimization. Journal
Artificial Intelligence Research, 18, 183215.
111

fiRoijers, Vamplew, Whiteson & Dazeley

Teichteil-Konigsbuch, F. (2012a). Path-constrained markov decision processes: bridging
gap. Proceedings Twentieth European Conference Artificial Intelligence.
Teichteil-Konigsbuch, F. (2012b). Stochastic safest shortest path problems. Proceedings Twenty-Sixth AAAI Conference Artificial Intelligence.
Tesauro, G., Das, R., Chan, H., Kephart, J. O., Lefurgy, C., Levine, D. W., & Rawson, F.
(2007). Managing power consumption performance computing systems using
reinforcement learning. Neural Information Processing Systems.
Tong, H., & Brown, T. X. (2002). Reinforcement learning call admission control
routing quality service constraints multimedia networks. Machine Learning, 49, 111139.
Uchibe, E., & Doya, K. (2009). Constrained Reinforcement Learning Intrinsic
Extrinsic Rewards, pp. 155166. Theory Novel Applications Machine Learning.
I-Tech, Vienna, Austria.
Vamplew, P., Dazeley, R., Barker, E., & Kelarev, A. (2009). Constructing stochastic mixture
policies episodic multiobjective reinforcement learning tasks. AI09: 22nd
Australasian Conference Artificial Intelligence, pp. 340349.
Vamplew, P., Dazeley, R., Berry, A., Dekker, E., & Issabekov, R. (2011). Empirical evaluation methods multiobjective reinforcement learning algorithms. Machine Learning,
84 (1-2), 5180.
Vamplew, P., Yearwood, J., Dazeley, R., & Berry, A. (2008). limitations scalarisation multi-objective reinforcement learning Pareto fronts. AI08: 21st
Australasian Joint Conference Artificial Intelligence, pp. 372378. Springer.
Van Otterlo, M., & Wiering, M. (2012). Reinforcement learning markov decision processes. Reinforcement Learning: State Art, chap. 1, pp. 342. Springer.
Van Vaerenbergh, K., Rodriguez, A., Gagliolo, M., Vrancx, P., Nowe, A., Stoev, J.,
Goossens, S., Pinte, G., & Symens, W. (2012). Improving wet clutch engagement
reinforcement learning. International Joint Conference Neural Networks,
IJCNN 2012.
Vira, C., & Haimes, Y. Y. (1983). Multiobjective decision making: theory methodology.
No. 8. North-Holland.
Viswanathan, B., Aggarwal, V. V., & Nair, K. P. K. (1977). Multiple criteria Markov
decision processes. TIMS Studies Management Science, 6, 263272.
Wakuta, K., & Togawa, K. (1998). Solution procedures Markov decision processes. Optimization: Journal Mathematical Programming Operations Research, 43 (1),
2946.
Wakuta, K. (1999). note structure value spaces vector-valued Markov decision
processes.. Mathematical Methods Operations Research, 49 (1), 7785.
Wang, W., & Sebag, M. (2013). Hypervolume indicator dominance reward based multiobjective monte-carlo tree search. Machine Learning, 127.
Watkins, C. J. C. H. (1989). Learning Delayed Rewards. Ph.D. thesis, Cambridge
University.
112

fiA Survey Multi-Objective Sequential Decision-Making

White, C. C., & Kim, K. M. (1980). Solution procedures solving vector criterion Markov
decision processes. Large Scale Systems, 1, 129140.
White, D. (1982). Multi-objective infinite-horizon discounted Markov decision processes.
Journal Mathematical Analysis Applications, 89 (2), 639 647.
Whiteson, S. (2012). Evolutionary computation reinforcement learning. Wiering,
M. A., & van Otterlo, M. (Eds.), Reinforcement Learning: State Art, chap. 10,
pp. 325352. Springer, Berlin.
Wiering, M., & De Jong, E. (2007). Computing optimal stationary policies multiobjective Markov decision processes. IEEE International Symposium Approximate Dynamic Programming Reinforcement Learning, pp. 158165. IEEE.
Yang, Z., & Wen, K. (2010). Multi-objective optimization freeway traffic flow via
fuzzy reinforcement learning method. 3rd International Conference Advanced
Computer Theory Engineering, Vol. 5, pp. 530534.
Yin, S., Duan, H., Li, Z., & Zhang, Y. (2010). Multi-objective reinforcement learning
traffic signal coordinate control. 1th World Conference Transport Research.
Zeleny, M., & Cochrane, J. L. (1982). Multiple criteria decision making, Vol. 25. McGrawHill New York.
Zhao, Y., Chen, Q., & Hu, W. (2010). Multi-objective reinforcement learning algorithm
MOSDMP unknown environment. Proceedings 8th World Congress
Intelligent Control Automation, pp. 31903194.
Zheng, K., Li, H., Qiu, R. C., & Gong, S. (2012). Multi-objective reinforcement learning
based routing cognitive radio networks: Walking random maze. International
Conference Computing, Networking Communications, pp. 359363.
Zitzler, E., Thiele, L., Laumanns, M., Fonseca, C. M., & da Fonseca, V. G. (2003). Performance assessment multiobjective optimizers: analysis review. Evolutionary
Computation, IEEE Transactions on, 7 (2), 117132.

113


