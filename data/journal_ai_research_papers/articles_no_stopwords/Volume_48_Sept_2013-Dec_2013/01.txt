Journal Artificial Intelligence Research 48 (2013) 23-65

Submitted 04/13; published 10/13

Learning Optimal Bayesian Networks:
Shortest Path Perspective
Changhe Yuan

changhe.yuan@qc.cuny.edu

Department Computer Science
Queens College/City University New York
Queens, NY 11367 USA

Brandon Malone

brandon.malone@cs.helsinki.fi

Department Computer Science
Helsinki Institute Information Technology
Fin-00014 University Helsinki, Finland

Abstract
paper, learning Bayesian network structure optimizes scoring function
given dataset viewed shortest path problem implicit state-space search
graph. perspective highlights importance two research issues: development
search strategies solving shortest path problem, design heuristic functions guiding search. paper introduces several techniques addressing
issues. One A* search algorithm learns optimal Bayesian network structure
searching promising part solution space. others mainly
two heuristic functions. first heuristic function represents simple relaxation
acyclicity constraint Bayesian network. Although admissible consistent, heuristic may introduce much relaxation result loose bound. second heuristic
function reduces amount relaxation avoiding directed cycles within groups
variables. Empirical results show methods constitute promising approach
learning optimal Bayesian network structures.

1. Introduction
Bayesian networks graphical models represent uncertain relations
random variables domain compactly intuitively. Bayesian network directed
acyclic graph nodes represent random variables, arcs lack
represent dependence/conditional independence relations variables.
relations quantified set conditional probability distributions, one
variable conditioning parents. Overall, Bayesian network represents joint
probability distribution variables.
Applying Bayesian networks real-world problems typically requires building graphical
representations problems. One popular approach use score-based methods
find high-scoring structures given dataset (Cooper & Herskovits, 1992; Heckerman,
1998). Score-based learning shown NP-hard, however (Chickering, 1996).
Due complexity, early research area mainly focused developing approximation algorithms greedy hill climbing approaches (Heckerman, 1998; Bouckaert,
1994; Chickering, 1995; Friedman, Nachman, & Peer, 1999). Unfortunately solutions
found methods unknown quality. recent years, several exact learning algoc
2013
AI Access Foundation. rights reserved.

fiYuan & Malone

rithms developed based dynamic programming (Koivisto & Sood, 2004; Ott,
Imoto, & Miyano, 2004; Silander & Myllymaki, 2006; Singh & Moore, 2005), branch
bound (de Campos & Ji, 2011), integer linear programming (Cussens, 2011; Jaakkola,
Sontag, Globerson, & Meila, 2010; Hemmecke, Lindner, & Studeny, 2012). methods
guaranteed find optimal solutions able finish successfully. However,
efficiency scalability leave much room improvement.
paper, view problem learning Bayesian network structure optimizes scoring function given dataset shortest path problem. idea
represent solution space learning problem implicit state-space search graph,
shortest path start goal nodes graph corresponds
optimal Bayesian network. perspective highlights importance two orthogonal
research issues: development search strategies solving shortest path problem,
design admissible heuristic functions guiding search. present several
techniques address issues. Firstly, A* search algorithm developed learn
optimal Bayesian network focusing searching promising parts solution
space. Secondly, two heuristic functions introduced guide search. tightness
heuristic determines efficiency search algorithm. first heuristic represents simple relaxation acyclicity constraint Bayesian networks
variable chooses optimal parents independently. result, heuristic estimate may
contain many directed cycles result loose bound. second heuristic, named
k-cycle conflict heuristic, based form relaxation tightens bound
avoiding directed cycles within groups variables. Finally, traversing
search graph, need calculate cost arc visited, corresponds
selecting optimal parents variable candidate set. present two data
structures storing querying costs candidate parent sets. One set
full exponential-size data structures called parent graphs stored hash tables
answer query constant time. sparse representation parent
graph stores optimal parent sets improve space efficiency.
empirically evaluated A* algorithm empowered different combinations
heuristic functions parent graph representations set UCI machine learning
datasets. results show even simple heuristic full parent graph representation, A* often achieve better efficiency and/or scalability existing approaches
learning optimal Bayesian networks. k-cycle conflict heuristic sparse parent
graph representation enabled algorithm achieve even greater efficiency
scalability. results indicate proposed methods constitute promising approach
learning optimal Bayesian network structures.
remainder paper structured follows. Section 2 reviews problem
learning optimal Bayesian networks reviews related work. Section 3 introduces
shortest path perspective learning problem. formulation search graph
discussed detail. Section 4 introduces two data structures developed compute
store optimal parent sets pairs variables candidate sets. data structures used query cost arc search graph. Section 5 presents
A* search algorithm. developed two heuristic functions guiding algorithm
studied theoretical properties. Section 6 presents empirical results evaluating
algorithm several existing approaches. Finally, Section 7 concludes paper.
24

fiLearning Optimal Bayesian Networks

2. Background
first provide brief summary related work learning Bayesian networks.
2.1 Learning Bayesian Network Structures
Bayesian network directed acyclic graph (DAG) G represents joint probability
distribution set random variables V = {X1 , X2 , ..., Xn }. directed arc Xi
Xj represents dependence two variables; say Xi parent Xj .
use PAj stand parent set Xj . dependence relation Xj PAj
quantified using conditional probability distribution, P (Xj |PAj ). joint probability
distribution represented G factorized product
Q conditional probability
distributions network, i.e., P (X1 , ..., Xn ) = ni=1 P (Xi |PAi ). addition
compact representation, Bayesian networks provide principled approaches solving
various inference tasks, including belief updating, probable explanation, maximum
Posteriori assignment (Pearl, 1988), relevant explanation (Yuan, Liu, Lu, & Lim,
2009; Yuan, Lim, & Littman, 2011a; Yuan, Lim, & Lu, 2011b).
Given dataset = {D1 , ..., DN }, data point Di vector values
variables V, learning Bayesian network task finding network structure
best fits D. work, assume variable discrete finite number
possible values, data point missing values.
roughly three main approaches learning problem: score-based learning,
constraint-based learning, hybrid methods. Score-based learning methods evaluate
quality Bayesian network structures using scoring function selects one
best score (Cooper & Herskovits, 1992; Heckerman, 1998). methods basically
formulate learning problem combinatorial optimization problem. work well
datasets many variables, may fail find optimal solutions large
datasets. discuss approach detail next section,
approach take. Constraint-based learning methods typically use statistical testings
identify conditional independence relations data build Bayesian network
structure best fits independence relations (Pearl, 1988; Spirtes, Glymour, &
Scheines, 2000; Cheng, Greiner, Kelly, Bell, & Liu, 2002; de Campos & Huete, 2000; Xie &
Geng, 2008). Constraint-based methods mostly rely results local statistical testings,
often scale large datasets. However, sensitive accuracy
statistical testings may work well insufficient noisy data.
comparison, score-based methods work well even datasets relatively data
points. Hybrid methods aim integrate advantages previous two approaches
use combinations constraint-based and/or score-based methods solving learning
problem (Dash & Druzdzel, 1999; Acid & de Campos, 2001; Tsamardinos, Brown, & Aliferis,
2006; Perrier, Imoto, & Miyano, 2008). One popular strategy use constraint-based
learning create skeleton graph use score-based learning find high-scoring
network structure subgraph skeleton (Tsamardinos et al., 2006; Perrier et al.,
2008). work, consider Bayesian model averaging methods aim
estimate posterior probabilities structural features edges rather model
selection (Heckerman, 1998; Friedman & Koller, 2003; Dash & Cooper, 2004).
25

fiYuan & Malone

2.2 Score-Based Learning
Score-based learning methods rely scoring function Score(.) evaluating quality
Bayesian network structure. search strategy used find structure G optimizes
score. Therefore, score-based methods two major elements, scoring functions
search strategies.
2.2.1 Scoring Functions
Many scoring functions used measure quality network structure.
Bayesian scoring functions define posterior probability distribution
network structures conditioning data, structure highest
posterior probability presumably best structure. scoring functions best
represented Bayesian Dirichlet score (BD) (Heckerman, Geiger, & Chickering, 1995)
variations, e.g., K2 (Cooper & Herskovits, 1992), Bayesian Dirichlet score
score equivalence (BDe) (Heckerman et al., 1995), Bayesian Dirichlet score score
equivalence uniform priors (BDeu) (Buntine, 1991). scoring functions often
form trading goodness fit structure data complexity
structure. goodness fit measured likelihood structure given
data amount information compressed structure data.
Scoring functions belonging category include minimum description length (MDL)
(or equivalently Bayesian information criterion, BIC) (Rissanen, 1978; Suzuki, 1996; Lam
& Bacchus, 1994), Akaike information criterion (AIC) (Akaike, 1973; Bozdogan, 1987),
(factorized) normalized maximum likelihood function (NML/fNML) (Silander, Roos, Kontkanen, & Myllymaki, 2008), mutual information tests score (MIT) (de Campos,
2006). scoring functions decomposable, is, score network
decomposed sum node scores (Heckerman, 1998).
optimal structure G may unique multiple Bayesian network structures may share optimal score1 . Two network structures said belong
equivalence class (Chickering, 1995) represent set probability distributions possible parameterizations. Score-equivalent scoring functions assign
score structures equivalence class. scoring functions
score equivalent.
mainly use MDL score work. Let ri number states Xi , Npai
number data points consistent PAi = pai , Nxi ,pai number data
points constrained Xi = xi . MDL defined follows (Lam & Bacchus, 1994).

DL(G) =

X

DL(Xi |PAi ),



1. often use optimal instead optimal throughout paper.

26

(1)

fiLearning Optimal Bayesian Networks


log N
K(Xi |PAi ),
2
X
Nxi ,pai
H(Xi |PAi ) =
Nxi ,pai log
,
Npai
xi ,pai

K(Xi |PAi ) = (ri 1)
rl .
Xl PAi

DL(Xi |PAi ) = H(Xi |PAi ) +

(2)
(3)
(4)

goal find Bayesian network minimum MDL score. However,
methods means restricted MDL; decomposable scoring function,
BIC, BDeu, fNML, used instead without affecting search strategy.
demonstrate that, test BDeu experimental section. One slight difference
MDL scoring functions latter scores need maximized
order find optimal solution. rather straightforward translate
maximization minimization problems simply changing sign scores. Also,
sometimes use costs refer scores, represent distances
nodes search graph.
2.2.2 Local Search Strategies
Given n variables, O(n2n(n1) ) directed acyclic graphs (DAGs). size
solution space grows exponentially number variables. surprising
score-based structure learning shown NP-hard (Chickering, 1996). Due
complexity, early research focused mainly developing approximation algorithms (Heckerman, 1998; Bouckaert, 1994). Popular search strategies used include greedy hill
climbing, stochastic search, genetic algorithm, etc..
Greedy hill climbing methods typically begin initial network, e.g., empty
network randomly generated structure, repeatedly apply single edge operations,
including addition, deletion, reversal, finding locally optimal network. Extensions approach include tabu search random restarts (Glover, 1990), limiting
number parents parameters variable (Friedman et al., 1999), searching
space equivalence classes (Chickering, 2002), searching space variable
orderings (Teyssier & Koller, 2005), searching constraints extracted
data (Tsamardinos et al., 2006). optimal reinsertion algorithm (OR) (Moore & Wong,
2003) adds different operator: variable removed network, optimal parents
selected, variable reinserted network parents.
parents selected ensure new network still valid Bayesian network.
Stochastic search methods Markov Chain Monte Carlo simulated annealing
applied find high-scoring structure (Heckerman, 1998; de Campos &
Puerta, 2001; Myers, Laskey, & Levitt, 1999). methods explore solution space
using non-deterministic transitions neighboring network structures favoring
better solutions. stochastic moves used hope escape local optima find
better solutions.
optimization methods genetic algorithms (Hsu, Guo, Perry, & Stilson,
2002; Larranaga, Kuijpers, Murga, & Yurramendi, 1996) ant colony optimization meth27

fiYuan & Malone

ods (de Campos, Fernndez-Luna, Gmez, & Puerta, 2002; Daly & Shen, 2009)
applied learning Bayesian network structures well. Unlike previous methods
work one solution time, population-based methods maintain set candidate solutions throughout search. step, create next generation
solutions randomly reassembling current solutions genetic algorithms,
generating new solutions based information collected incumbent solutions
ant colony optimization. hope obtain increasingly better populations solutions
eventually find good network structure.
local search methods quite robust face large learning problems
many variables. However, guarantee find optimal solution. worse,
quality solutions typically unknown.
2.2.3 Optimal Search Strategies
Recently multiple exact algorithms developed learning optimal Bayesian networks. Several dynamic programming algorithms proposed based observation
Bayesian network least one leaf (Ott et al., 2004; Singh & Moore, 2005).
leaf variable child variables Bayesian network. order find optimal
Bayesian network set variables V, sufficient find best leaf. leaf
choice X, best possible Bayesian network constructed letting X choose optimal
parent set PAX V\{X} letting V\{X} form optimal subnetwork.
best leaf choice one minimizes sum Score(X, PAX ) Score(V\{X})
scoring function Score(.). formally, have:
Score(V) = min {Score(V \ {X}) + BestScore(X, V \ {X})},
XV

(5)


BestScore(X, V \ {X}) =

min
Score(X, PAX ).
PAX V\{X}

(6)

Given recurrence relation, dynamic programming algorithm works follows. first finds optimal structures single variables, trivial. Starting
base cases, algorithm builds optimal subnetworks increasingly larger variable
sets optimal network found V. dynamic programming algorithms
find optimal Bayesian network O(n2n ) time space (Koivisto & Sood, 2004; Ott
et al., 2004; Silander & Myllymaki, 2006; Singh & Moore, 2005). Recent algorithms
improved memory complexity either trading longer running times reduced memory consumption (Parviainen & Koivisto, 2009) taking advantage layered structure
present within dynamic programming lattice (Malone, Yuan, & Hansen, 2011b; Malone,
Yuan, Hansen, & Bridges, 2011a).
branch bound algorithm (BB) proposed de Campos Ji (2011)
learning Bayesian networks. algorithm first creates cyclic graph allowing
variable obtain optimal parents variables. best-first search strategy
used break cycles removing one edge time. algorithm uses
approximation algorithm estimate initial upper bound solution pruning.
algorithm occasionally expands worst nodes search frontier hope find
28

fiLearning Optimal Bayesian Networks

Figure 1: order graph four variables.
better networks update upper bound. completion, algorithm finds optimal
network structure subgraph initial cyclic graph. algorithm ran
memory finding solution, switch using depth-first search strategy
find suboptimal solution.
Integer linear programming (ILP) used learn optimal Bayesian network
structures (Cussens, 2011; Jaakkola et al., 2010). learning problem cast integer
linear program polytope exponential number facets. outer bound
approximation polytope solved. solution relaxed problem
integral, guaranteed optimal structure. Otherwise, cutting planes branch
bound algorithms subsequently applied find optimal structure. Recently
similar method proposed find optimal structure searching space
equivalence classes (Hemmecke et al., 2012).
Several methods considered optimal constraints enforce
network structure. example, optimal parents selected variable, K2
finds optimal network structure particular variable ordering (Cooper & Herskovits,
1992). methods developed (Ordyniak & Szeider, 2010; Kojima, Perrier, Imoto, &
Miyano, 2010) find optimal network structure must subgraph given super
graph.

3. Shortest Path Perspective
section introduces shortest path perspective problem learning Bayesian
network structure given dataset.
3.1 Order Graph
state space graph learning Bayesian networks basically Hasse diagram containing
subsets variables domain. Figure 1 visualizes state space graph
learning problem four variables. top-most node empty set layer
29

fiYuan & Malone

0 start search node, bottom-most node complete set layer n
goal node, n number variables domain. arc U U {X}
represents generating successor node adding new variable {X} existing set
variables U; U called predecessor U {X}. cost arc equal score
selecting optimal parent set X U, i.e., BestScore(X, U). example, arc
{X1 , X2 } {X1 , X2 , X3 } cost equal BestScore(X3 , {X1 , X2 }). node layer
ni successors many ways add new variable, predecessors
many leaf choices. define expanding node U generating successors
nodes U.
search graph thus defined, path start node goal node defined
sequence nodes arc nodes next node
sequence. path corresponds ordering variables order
appearance. example, path traversing nodes , {X1 }, {X1 , X2 }, {X1 , X2 , X3 },
{X1 , X2 , X3 , X4 } stands variable ordering X1 , X2 , X3 , X4 . call
search graph order graph. cost path defined sum costs
arcs path. shortest path path minimum total cost
order graph.
Given shortest path, reconstruct Bayesian network structure noting
arc path encodes choice optimal parents one variables
preceding variables, complete path represents ordering
variables. Therefore, putting together optimal parent choices generates valid
Bayesian network. construction, Bayesian network structure optimal.
3.2 Finding Shortest Path
Various methods applied solve shortest path problem. Dynamic programming
considered evaluate order graph using top sweep order graph (Silander
& Myllymaki, 2006; Malone et al., 2011b). Layer layer, dynamic programming finds
optimal subnetwork variables contained node order graph based
results previous layers. example, three ways construct Bayesian
network node {X1 , X2 , X3 }: using {X2 , X3 } subnetwork X1 leaf, using
{X1 , X3 } subnetwork X2 leaf, using {X1 , X2 } subnetwork X3
leaf. top-down sweep makes sure optimal subnetworks already found
{X2 , X3 }, {X1 , X3 }, {X1 , X2 }. need select optimal parents
leaves identify leaf produces optimal network {X1 , X2 , X3 }.
evaluation reaches node last layer, shortest path and, equivalently, optimal
Bayesian network found global variable set.
drawback dynamic programming approach need compute
BestScore(.) candidate parent sets variable. n variables,
2n nodes order graph, 2n1 parent scores computed
variable, totally n2n1 scores. number variables increases, computing storing
order parent graphs quickly becomes infeasible.
paper, propose apply A* algorithm (Hart, Nilsson, & Raphael, 1968)
solve shortest path problem. A* uses heuristic function evaluate quality
search nodes expand promising search node search step.
30

fiLearning Optimal Bayesian Networks

guidance heuristic functions, A* needs explore part search
graph finding optimal solution. However, comparison dynamic programming,
A* overhead calculating heuristic values maintaining priority queue.
actual relative performance dynamic programming A* thus depends
efficiency calculating heuristic values tightness values (Felzenszwalb
& McAllester, 2007; Klein & Manning, 2003).

4. Finding Optimal Parent Sets
introducing algorithm solving shortest path problem, first discuss
obtain cost BestScore(X, U) arc U U {X} visit
order graph. Recall arc involves selecting optimal parents variable
candidate set. need consider subsets candidate set finding subset
best score. section, introduce two data structures related methods
computing storing optimal parent sets scores pairs variable candidate
parent set.
exact algorithms learning Bayesian network structures need calculate
optimal parent sets scores. present reasonable approach calculation
paper. Note, however, approach applicable algorithms, vice versa.
4.1 Parent Graph
use data structure called parent graph compute costs arcs order graph.
variable parent graph. parent graph variable X Hasse diagram
consisting subsets variables V \ {X}. node U stores optimal parent
set PAX U minimizes Score(X, PA X ) well BestScore(X, U) itself.
example, Figure 2(b) shows sample parent graph X1 contains best scores
subsets {X2 , X3 , X4 }. obtain Figure 2(b), however, first need calculate
preliminary graph Figure 2(a) contains raw score subset U parent
set X1 , i.e., Score(X1 , U). Equation 3 shows, scores calculated based
counts particular instantiations parent child variables.
use AD-tree (Moore & Lee, 1998) collect counts dataset
compute scores. AD-tree unbalanced tree structure contains two types
nodes, AD-tree nodes varying nodes. AD-tree node stores number data points
consistent particular variable instantiation; varying node used instantiate
state variable. full AD-tree stores counts data points consistent
partial instantiations variables. sample AD-tree two variables shown
Figure 3. n variables states each, number AD-tree nodes AD-tree
(d+1)n . grows even faster size order parent graph. Moore Lee (1998)
described sparse AD-tree significantly reduces space complexity. Readers
referred paper details. pseudo code assumes sparse AD-tree
used.
Given AD-tree, ready calculate raw scores Score(X1 , .) Figure 2(a).
exponential number scores parent graph. However, parent
sets possibly optimal Bayesian network; certain parent sets discarded
without ever calculating values according following theorems Tian (2000).
31

fiYuan & Malone

Figure 2: sample parent graph variable X1 . (a) raw scores Score(X1 , .)
parent sets. first line node gives parent set, second
line gives score using set parents X1 . (b) optimal
scores BestScore(X1 , .) candidate parent set. second line
node gives optimal score using subset variables first line
parents X1 . (c) optimal parent sets scores. pruned parent
sets shown gray. parent set pruned predecessors
better score.

X1 = *
X2 = *
C = 50
Vary
V
X1

Vary
V
X2

X1 = 0
X2 = *

X1 = 1
X2 = *

X1 = *
X2 = 0

X1 = *
X2 = 1

C = 20

C = 30

C = 25

C = 25

Vary
X2

Vary
X2

X1 = 0
X2 = 0

X1 = 0
X2 = 1

X1 = 1
X2 = 0

X1 = 1
X2 = 1

C = 15

C=5

C = 10

C = 20

Figure 3: AD-tree.
use theorems compute necessary MDL scores. scoring functions
BDeu similar pruning rules (de Campos & Ji, 2011). Algorithm 1 provides
pseudo code calculating raw scores.
Theorem 1 optimal Bayesian network based MDL scoring function,
2N
variable log( log
N ) parents, N number data points.
32

fiLearning Optimal Bayesian Networks

Algorithm 1 Score Calculation Algorithm
Input: AD sparse AD-tree input data; V input variables.
Output: Score(X, U) pair X V U V \ {X}
1: function calculateMDLScores(AD, V)
2:
Xi V
3:
calculateScores(Xi , AD)
4:
end
5: end function
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:

function calculateScores(Xi, AD)
2N
k 0 log( log
Prune due Theorem 1
N )
U U V \ {X}& |U| == k
parent sets size k
prune f alse
U
K(Xi |U) - Score(Xi , U \ {Y }) > 0
prune true
Prune due Theorem 2
break
end
end
prune ! = true
Score(Xi , U) log2 N K(Xi |U)
Complexity term
instantiation xi , u Xi , U
Log likelihood term
cF amily GetCount({xi } u,AD)
cP arents GetCount(u, AD)
Score(Xi , U) Score(Xi , U) - cF amily log cF amily
Score(Xi , U) Score(Xi , U) + cF amily log cP arents
end
end
end
end
end function

Theorem 2 Let U two candidate parent sets X, U S, K(Xi |S)
DL(Xi |U) > 0. supersets cannot possibly optimal parent sets
X.
computing raw scores, compute parent graph according following
theorem appeared many earlier papers, e.g., see work Teyssier
Koller (2005), de Campos Ji (2010). theorem simply means parent set
optimal subset better score.
Theorem 3 Let U two candidate parent sets X U S,
Score(X, U) Score(X, S). optimal parent set X candidate set.
33

fiYuan & Malone

Algorithm 2 Computing parent graphs
Input: necessary Score(X, U), X V&U V \ {X}
Output: Full parent graphs containing BestScore(X, U)
1: function calculateFullParentGraphs(V, Score(., .))
2:
X V
3:
layer 0 n
Propagate best scores graph
4:
U U V \ {X}& |U| == layer
5:
calculateBestScore(X, U, Score(., .))
6:
end
7:
end
8:
end
9: end function
10:
11:
12:
13:
14:
15:
16:
17:

function calculateBestScore(X, U, Score(., .))
BestScore(X, U) Score(X, U)
U
BestScore(X, U \ {Y }) < BestScore(X, U)
BestScore(X, U) BestScore(X, U \ {Y })
end
end
end function

function getBestScore(X, U)
19:
return BestScore(X, U)
20: end function

Propagate best scores

Query BestScore(X, U)

18:

Therefore, generate successor node U{Y } U parent graph X,
check whether Score(X, U {Y }) smaller BestScore(X, U). so, let parent
graph node U{Y } record optimal parent set. Otherwise BestScore(X, U)
smaller, propagate optimal parent set U U{Y }. propagation,
must following (Teyssier & Koller, 2005).
Theorem 4 Let U two candidate parent sets X U S. must
BestScore(X, S) BestScore(X, U).
pseudo code propagating scores computing parent graph outlined
Algorithm 2. Figure 2(b) shows parent graph optimal scores propagating
best scores top bottom.
search order graph, whenever visit new arc U U {X},
find score looking parent graph variable X. example, need find
optimal parents X1 {X2 , X3 }, look node {X2 , X3 } X1 parent graph
find optimal parent set score. make look-ups efficient, use hash
tables organize parent graphs query answered constant time.
34

fiLearning Optimal Bayesian Networks

parentsX1
scoresX1

{X2 , X3 }
5

{X3 }
6

{X2 }
8

{}
10

Table 1: Sorted scores parent sets X1 pruning parent sets
possibly optimal.
parentsX1
2
parentsX
X1
X3
parentsX1
4
parentsX
X1

{X2 , X3 }
1
1
0

{X3 }
0
1
0

{X2 }
1
0
0

{}
0
0
0

Table 2: parentsX (Xi ) bit vectors X1 . 1 line Xi indicates corresponding parent set includes variable Xi , 0 indicates otherwise. Note
that, pruning, none optimal parent sets include X4 .

4.2 Sparse Parent Graphs
full parent graph variable X exhaustively enumerates subsets V \ {X}
stores BestScore(X, U) subsets. Naively, approach requires storing
n2n1 scores parent sets (Silander & Myllymaki, 2006). Theorem 3, however,
number optimal parent sets often far smaller full size. Figure 2(b) shows
optimal parent set may shared several candidate parent sets. full parent
graph representation allocate space repetitive information candidate sets,
resulting waste time space.
address limitations, introduce sparse representation parent graphs
related scanning techniques querying optimal parent sets. full parent
graphs, begin calculating pruning scores described last Section. Due
Theorems 1 2, parent sets pruned without evaluated.
Therefore, create full parent graphs. Also, instead creating
Hasse diagrams, sort optimal parent scores variable X list,
maintain parallel list stores associated optimal parent sets. call sorted
lists scoresX parentsX . Table 1 shows sorted lists optimal scores
parent graph Figure 2(b). essence, allows us store efficiently process
scores Figure 2(c).
find optimal parent set X candidate set U, simply scan
list X starting beginning. soon find first parent set subset
U, find optimal parent score BestScore(X, U). trivially true due
following theorem.
Theorem 5 first subset U parentsX optimal parent set X U.
Scanning lists find optimal parent sets inefficient done properly.
Since scanning arc visited order graph, inefficiency
scanning large impact search algorithm.
35

fiYuan & Malone

parentsX1
validX1
3
parentsX
X1
new
validX1

{X2 , X3 }
1
0
0

{X3 }
1
0
0

{X2 }
1
1
1

{}
1
1
1

Table 3: result performing bitwise operation exclude parent sets
include X3 . 1 validX1 bit vector means parent set
include X3 used selecting optimal parents. first set bit
indicates best possible score parent set.

parentsX1
validX1
3
parentsX
X1
new
validX1

{X2 , X3 }
0
0
0

{X3 }
0
1
0

{X2 }
1
0
0

{}
1
1
1

Table 4: result performing bitwise operation exclude parent sets
include either X3 X2 . 1 validnew
X1 bit vector means parent
set includes neither X2 X3 . initial validX1 bit vector already excluded
X3 , finding validnew
X1 required excluding X2 .

ensure efficiency, propose following scanning technique. variable
X, first initialize working bit vector length kscoresX k called validX 1s.
indicates parent scores scoresX usable. Then, create n 1 bit vectors
length kscoresX k, one variable V \ {X}. bit vector variable
denoted parentsYX contains 1s parent sets contain 0s others.
Table 2 shows bit vectors example Table 1. Then, exclude variable
candidate parent, perform bit operation validnew
validX & parentsYX . new
X
validX bit vector contains 1s parent sets subsets V \ {Y }.
first set bit corresponds BestScore(X, V \ {Y }). Table 3 shows example excluding
X3 set possible parents X1 , first set bit new bit vector
corresponds BestScore(X1 , V \ {X3 }). want exclude X2 candidate
parent, new bit vector last step becomes current bit vector step,
2
bit operation applied: validnew
validX & parentsX
X
X1 . first set
bit result corresponds BestScore(X1 , V \ {X2 , X3 }). Table 4 demonstrates
operation. Also, important note exclude one variable time. example,
if, excluding X3 , wanted exclude X4 rather X2 , could take validnew

X
4
validX & parentsX
.

operations

described


createSparseParentGraph

X
getBestScore functions Algorithm 3.
pruning duplicate scores, sparse representation requires much less
memory storing possible parent sets scores. long kscores(X)k <
C(n 1, n2 ), requires less memory memory-efficient dynamic programming
algorithm (Malone et al., 2011b).
Experimentally, show kscoresX k almost
36

fiLearning Optimal Bayesian Networks

Algorithm 3 Sparse Parent Graph Algorithms
Input: necessary Score(X, U), X V&U V \ {X}
Output: Sparse parent graphs containing optimal parent sets scores
1: function createSparseParentGraph(X, Score(., .))
2:
X V
3:
scorest , parentst sort(Score(X, ))
Sort scores, preferring low cardinality
4:
scoresX , parentsX
Initialize possibly optimal scores
5:
= 0 |scorest |
6:
prune f alse
7:
j = 0 |scoresX |
Check better subset pattern exists
8:
contains(parentst(i), parentsX (j))&scoresX (i) scorest (i)
9:
prune true
10:
Break
11:
end
12:
end
13:
prune ! = true
14:
Append scoresX , parentsX parentst(i), parentst (i)
15:
end
16:
end
17:
= 0 |scoresX |
Set bit vectors efficient querying
18:
parentsX (i)
19:
set(parentsYX (i))
20:
end
21:
end
22:
end
23: end function
24:
25:
26:
27:
28:
29:
30:
31:

function getBestScore(X, U)
valid allScoresX
V \ U
valid valid& parentsYX
end
f sb f irstSetBit(valid)
return scoresX [f sb]
end function

Query BestScore(X, U)

Return first score set bit

always smaller C(n 1, n2 ) several orders magnitude. approach offers
(usually substantial) memory savings compared previous best approaches.
sparse representation extra benefit improving time efficiency well.
full representation, create complete exponential-size parent graphs,
even though many nodes parent graph share optimal parent choices.
sparse representation, avoid creating nodes, makes creating sparse
parent graphs much efficient.
37

fiYuan & Malone

5. A* Search Algorithm
ready tackle shortest path problem order graph. section
presents search algorithm well two admissible heuristic functions guiding
algorithm.
5.1 Algorithm
apply well known state space search method, A* algorithm (Hart et al., 1968),
solve shortest path problem order graph. main idea algorithm
use evaluation function f measure quality search nodes always expand
one lowest f cost exploration order graph. node U,
f (U) decomposed sum exact past cost, g(U), estimated future cost,
h(U). g(U) cost measures shortest distance start node U,
h(U) cost estimates far away U goal node. Therefore, f cost provides
estimated total cost best possible path passes U.
A* uses open list (usually priority queue) store search frontier,
closed list store expanded nodes. Initially open list contains start node,
closed list empty. search step, node lowest f -cost
open list, say U, selected expansion generate successor nodes. expanding
U, however, need first check whether goal node. yes, shortest path
goal found; construct Bayesian network path terminate
search.
U goal, expand generate successor nodes. successor
considers one possible way adding new variable, say X, leaf existing
subnetwork variables U, = U {X}. g cost calculated
sum g-cost U cost arc U S. arc cost well
optimal parent set PAX X U retrieved Xs parent graph. h cost
computed heuristic function describe shortly. record
following information2 : g cost, h cost, X, PAX .
clear order graph multiple paths node.
perform duplicate detection see whether node representing set variables
already generated before. check duplicates, search space blows
order graph size 2n order tree size n!. first check whether
duplicate already exists closed list. so, check whether duplicate
better g cost S. yes, discard immediately, represents worse path.
Otherwise, remove duplicate closed list, place open list.
happens found better path lower g cost, reopen node future
search.
duplicate found closed list, need check open list.
duplicate found, simply add open list. Otherwise, compare
g costs duplicate S. duplicate lower g cost, discarded.
Otherwise, replace duplicate S. Again, lower g cost means better
path found.
2. delay calculation h duplicate detection avoid unnecessary calculations
nodes pruned.

38

fiLearning Optimal Bayesian Networks

Algorithm 4 A* Search Algorithm
Input: full sparse parent graphs containing BestScore(X, U)
Output: optimal Bayesian network G
1: function main(D)
2:
start
3:
Score(start) 0 P
4:
push(open, start, V BestScore(Y, V \ {Y })
5:
!isEmpty(open)
6:
U pop(open)
7:
U goal
shortest path found
8:
print(The best score + Score(V))
9:
G construct network shortest path
10:
return G
11:
end
12:
put(closed, U)
13:
X V \ U
Generate successors
14:
g BestScore(X, U) + Score(U)
15:
contains(closed, U {X})
Closed list DD
16:
g < Score(U {X})
reopen node
17:
delete(closed, U {X})
18:
push (open, U {X}, g + h)
19:
Score(U {X}) g
20:
end
21:
else
22:
contains(open, U {X}) & g < Score(U {X}) Open list DD
23:
update(open, U {X}, g + h)
24:
Score(U {X}) g
25:
end
26:
end
27:
end
28:
end
29: end function

successor nodes generated, place node U closed
list, indicates node already expanded. Expanding top node open
list called one search step. A* algorithm performs step repeatedly goal
node selected expansion. moment shortest path start state
goal state found.
shortest path found, reconstruct optimal Bayesian network
structure starting goal node tracing back shortest path reaching
start node. Since node path stores leaf variable optimal parent set,
putting optimal parent sets together generates valid Bayesian network structure.
pseudo code A* algorithm shown Algorithm 4.
39

fiYuan & Malone

5.2 Simple Heuristic Function
A* algorithm provides different theoretical guarantees depending properties
heuristic function h. function h admissible h cost never greater
true cost goal; words, optimistic. Given admissible heuristic
function, A* algorithm guaranteed find shortest path goal node
selected expansion (Pearl, 1984). Let U node order graph. first consider
following simple heuristic function h.
Definition 1
h(U) =

X

BestScore(X, V\{X}).

(7)

XV\U

heuristic function allows remaining variable choose optimal parents
variables. design reflects principle exact cost relaxed problem
used admissible bound original problem (Pearl, 1984). case,
original problem learn Bayesian network directed acyclic graph. Equation 7
relaxes problem ignoring acyclicity constraint, directed cyclic graphs
allowed. heuristic function easily proven admissible following theorem.
proofs theorems paper found Appendix A.
Theorem 6 h admissible.
turns h even nicer property. heuristic function consistent if,
node U successor S, h(U) h(S) + c(U, S), c(U, S) stands cost
arc U S. Given consistent heuristic, f cost monotonically non-decreasing
following path order graph. result, f cost node less
equal f cost goal node. follows immediately consistent heuristic
guaranteed admissible. consistent heuristic, A* algorithm guaranteed
find shortest path node U U selected expansion. duplicate
found closed list, duplicate must optimal g cost, new node
discarded immediately. show following simple heuristic Equation 7
consistent.
Theorem 7 h consistent.
heuristic may seem expensive compute requires computing BestScore(X, V\
{X}) variable X. However, scores easily found querying parent
graphs stored array repeated use. takes linear time calculate
heuristic start node. subsequent computation h, however, takes constant
time simply subtract best score newly added variable
heuristic value parent node.
5.3 Improved Admissible Heuristic
simple heuristic function defined Equation 7, referred hsimple hereafter, relaxes
acyclicity constraint Bayesian networks completely. result, hsimple may introduce
many directed cycles result loose bound. introduce another heuristic
section tighten heuristic. first use toy example motivate new heuristic,
describe two specific approaches computing heuristic.
40

fiLearning Optimal Bayesian Networks

X1

X2

X3

X4

Figure 4: directed graph representing heuristic estimate start search node.

5.3.1 Motivating Example
hsimple , heuristic estimate start node order graph allows variable
choose optimal parents variables. Suppose optimal parent sets
X1 , X2 , X3 , X4 {X2 , X3 , X4 }, {X1 , X4 }, {X2 }, {X2 , X3 } respectively. parent
choices shown directed graph Figure 4. Since acyclicity constraint
ignored, directed cycles introduced, e.g., X1 X2 . However, know
final solution cannot cycles; three cases possible X1 X2 : (1) X2
parent X1 (so X1 cannot parent X2 ), (2) X1 parent X2 , (3) neither
true. Based Theorem 4, third case cannot provide better value
first two cases one variables must fewer candidate parents.
(1) (2), unclear one better, take minimum
get lower bound. Consider case (1). delete arc X1 X2 rule
X1 parent X2 . let X2 rechoose optimal parents remaining
variables {X3 , X4 }, is, must check parent sets including X1 . deletion
arc alone cannot produce new bound best parent set X2
{X3 , X4 } necessarily {X4 }. total bound X1 X2 computed summing
together original bound X1 new bound X2 . call total bound
b1 . Case (2) handled similarly; call total bound b2 . joint cost
X1 X2 , c(X1 , X2 ), must optimistic, compute minimum b1 b2 .
Effectively considered possible ways break cycle obtained tighter
heuristic value. new heuristic clearly admissible, still allow cycles among
variables.
Often, hsimple introduces multiple cycles heuristic estimate. Figure 4
cycle X2 X4 . cycle shares X2 earlier cycle X1 X2 ;
say cycles overlap. One way break cycles set parent set X2
{X3 }; however, introduces new cycle X2 X3 . described detail
shortly, partition variables exclusive groups break cycles within
group. example, X2 X3 different groups, break cycle.
41

fiYuan & Malone

5.3.2 K-Cycle Conflict Heuristic
idea generalized compute joint cost variable group
size k avoiding cycles within group. node U order graph,
calculate heuristic value partitioning variables V \ U several exclusive
groups sum costs together. name resulting technique k-cycle conflict
heuristic. Note simple heuristic hsimple special case new heuristic,
simply contains costs individual variables (k=1).
new heuristic application additive pattern database technique (Felner,
Korf, & Hanan, 2004). Pattern databases (Culberson & Schaeffer, 1998) approach
computing admissible heuristic problem solving relaxed problem. Consider
15-puzzle problem. 15 square tiles numbered 1 15 randomly placed 4
4 box one position left empty. configuration tiles called state.
goal slide tiles one time destination configuration. tile slide
empty position beside position. 15 puzzle relaxed
contain tiles 1-8 tiles removed. relaxation, multiple
states original problem map one state abstract state space relaxed
problem share positions remaining tiles. abstract state called
pattern; cost pattern equal smallest cost sliding remaining
tiles destination positions. cost provides lower bound state
original state space maps pattern. costs patterns stored
pattern database.
relax problem different ways obtain multiple pattern databases.
solutions several relaxed problems independent, problems said
exclusive. 15-puzzle, relax contain tiles 9-15. relaxation
solved independently previous one share puzzle
movements. concrete state original state space, positions tiles 1-8
map pattern first pattern database, positions tiles 9-15 map
different pattern second pattern database. costs patterns added
together obtain admissible heuristic, hence name additive pattern databases.
learning problem, pattern defined group variables, cost
optimal joint cost variables avoiding directed cycles them.
decomposability scoring function implies costs two exclusive patterns
added together obtain admissible heuristic.
explicitly break cycles computing cost pattern.
following theorem offers straightforward approach so.
Theorem 8 cost pattern U, c(U), equal shortest distance V \ U
goal node order graph.
consider example Figure 4. cost pattern {X1 , X2 } equal
shortest distance {X3 , X4 } goal order graph Figure 1.
Furthermore, difference c(U) sum simple heuristic values
variables U indicates amount improvement brought avoiding cycles within
pattern. differential score, called h , thus used quality measure
ordering patterns choosing patterns likely result tighter
heuristic.
42

fiLearning Optimal Bayesian Networks

5.3.3 Dynamic K-Cycle Conflict Heuristic
two slightly different versions k-cycle conflict heuristic. first version
named dynamic k-cycle conflict heuristic, compute costs groups variables
size k store single pattern database. According Theorem 8,
heuristic computed finding shortest distances nodes
last k layers order graph goal.
compute heuristic using breadth-first search backward search
order graph k layers. search starts goal node expands order
graph backward layer layer. reverse arc U {X} U cost arc
U U {X}, i.e., BestScore(X, U). reverse g cost U updated whenever new
path lower cost found. Breadth-first search ensures node U obtain
exact reverse g cost previous layer expanded. g cost cost pattern
V \ U. compute differential score, h , pattern time.
pattern better differential score subset patterns
discarded. pruning significantly reduce size pattern database improve
query efficiency. algorithm computing dynamic k-cycle conflict heuristic
shown Algorithm 5.
heuristic created, calculate heuristic value search node
follows. node U, partition remaining variables V \ U set exclusive
patterns, sum costs together heuristic value. Since prune superset
patterns, always find partition. However, potentially many ways
partition. Ideally want find one highest total cost, represents
tightest heuristic value. problem finding optimal partition formulated
maximum weighted matching problem (Felner et al., 2004). k = 2, define
undirected graph vertex represents variable, edge two
variables represents pattern containing variables weight equal
cost pattern. goal select set edges graph two
edges share vertex total weight edges maximized. matching problem
solved O(n3 ) time, n number vertices (Papadimitriou & Steiglitz,
1982).
k > 2, add hyperedges matching graph connecting
k vertices represent larger patterns. goal becomes select set edges
hyperedges maximize total weight. However, three-dimensional higher-order
maximum weighted matching problem NP-hard (Garey & Johnson, 1979). means
solve NP-hard problem calculating heuristic value.
alleviate potential inefficiency, greedily select patterns based quality.
Consider node U unsearched variables V \ U. choose pattern highest
differential cost patterns subsets V \ U. repeat step
remaining variables variables covered. total cost chosen patterns
used heuristic value U. hdynamic function Algorithm 5 gives pseudocode
computing heuristic value.
dynamic k-cycle conflict heuristic introduced example dynamically partitioned pattern database (Felner et al., 2004) patterns dynamically
selected search algorithm. refer dynamic pattern database short.
43

fiYuan & Malone

Algorithm 5 Dynamic k-cycle Conflict Heuristic
Input: full sparse parent graphs containing BestScore(X, U)
Output: pattern database P patterns size k
1: function createDynamicPD(k)
2:
P D0 (V) 0
3:
h (V) 0
4:
l = 1 k
Perform BFS k levels
5:
U P Dl1
6:
expand(U, l)
7:
checkSave(U)
8:
P D(V \ U) P Dl1 (U)
9:
end
10:
end
11:
X P \ save
Remove superset patterns improvement
12:
delete P D(X)
13:
end
14:
sort(P : h )
Sort patterns decreasing costs
15: end function
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35:
36:
37:
38:

function expand(U, l)
X U
g P Dl1 (U) + BestScore(X, U \ {X})
g < P Dl (U \ {X}) P Dl (U \ {X}) g
end
end function

Duplicate detection

function checkSave(U)
P
h (U) g V\U BestScore(Y, V \ {Y })
X V \ U
Check improvement subset patterns
h (U) > h (U {X}) save(U)
end
end function
function hdynamic (U)
h0
RU
P
R
RR\S
h h + P D(S)
end
end
return h
end function

Calculate heuristic value U

Greedily find best subset pattern R

44

fiLearning Optimal Bayesian Networks

potential drawback dynamic pattern databases that, even greedy
method, computing heuristic value still much expensive simple heuristic
Equation 7. Consequently, search time longer even though tighter pattern
database heuristic results pruning fewer expanded nodes.
5.3.4 Static K-Cycle Conflict Heuristic
address inefficiency dynamic pattern database computing heuristic values,
introduce another version named static k-cycle conflict heuristic based statically
partitioned pattern database technique (Felner et al., 2004). idea partition
variables several static exclusive groups, create separate pattern database
group. Consider problem variables {X1 , ..., X8 }. divide variables
two groups, {X1 , ..., X4 } {X5 , ..., X8 }. group, say {X1 , ..., X4 }, create
pattern database contains costs subsets {X1 , ..., X4 } store
hash table. refer heuristic static pattern database short.S
create static pattern databases follows. static grouping V = Vi , need
compute pattern database group Vi resembles order graph containing
subsets Vi . use breadth first search create graph starting
node
Vi . cost arc U{X} U graph equal BestScore(X, ( j6=i Vj )U),
means variables groups valid candidate parents. ensure
efficient retrieval, static pattern databases stored hashtables; nothing pruned
them. Algorithm 6 gives pseudocode creating static pattern databases.
much simpler use static pattern databases compute heuristic value. Consider
search node {X1 , X4 , X8 }; unsearched variables {X2 , X3 , X5 , X6 , X7 }. simply
divide variables two patterns {X2 , X3 } {X5 , X6 , X7 } according static
grouping, look respective pattern databases, sum costs together
heuristic value. Moreover, since search step processes one variable,
one pattern affected requires new score lookup. Therefore, heuristic value
calculated incrementally. hstatic function Algorithm 6 provides pseudocode
naively calculating heuristic value.
5.3.5 Properties K-Cycle Conflict Heuristic
versions k-cycle conflict heuristic remain admissible. Although avoid
cycles within pattern, cannot prevent cycles across different patterns. following theorem proves result.
Theorem 9 k-cycle conflict heuristic admissible.
Understanding consistency new heuristic slightly complex. first
look static pattern database involve selecting patterns dynamically.
following theorem shows static pattern database still consistent.
Theorem 10 static pattern database version k-cycle conflict heuristic remains
consistent.
dynamic pattern database, search step needs solve maximum weighted
matching problem select set patterns compute heuristic value.
45

fiYuan & Malone

Algorithm 6 Static k-cycle Conflict Heuristics

Input: full sparse parent graphs containing BestScore(X, U), Vi partition V
Output: full pattern database P Vi
1: function createStaticPD(Vi )
2:
P D0i () 0
3:
l = 1 fiVi
Perform BFS Vi

4:
U P Dl1
5:
expand(U, l, Vi )
(U)
6:
P (U) P Dl1
7:
end
8:
end
9: end function
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:

function expand(U, l, Vi )
X Vi \ U

(U) + BestScore(X, U
g P Dl1
j6=i Vj )


g < P Dl (U X) P Dl (U X) g
end
end function
function hstatic (U)
h0
Vi V
h h + P (U Vi )
end
return h
end function

Duplicate detection

Sum P separately

following, show dynamic k-cycle conflict heuristic consistent closely
following Theorem 4.1 work Edelkamp Schrodl (2012).
Theorem 11 dynamic pattern database version k-cycle conflict heuristic remains consistent.
However, theorem assumes use shortest distances nodes
abstract space. use greedy method solve maximum weighted
matching problem, longer guarantee find shortest paths. result,
may lose consistency property dynamic pattern database. thus necessary
A* reopen duplicate node closed list better path found.

6. Experiments
evaluated A* search algorithm set benchmark datasets UCI repository (Bache & Lichman, 2013). datasets 29 variables 30, 162 data
points. discretized variables two states using mean values deleted
46

fiLearning Optimal Bayesian Networks

1.00E+10
1.00E+09

Full

Largest Layer

Sparse

1.00E+08
1.00E+07

Size

1.00E+06
1.00E+05
1.00E+04
1.00E+03
1.00E+02
1.00E+01
1.00E+00

Figure 5: number parent sets scores stored full parent graphs
(Full), largest layer parent graphs memory-efficient dynamic programming (Largest Layer), sparse representation (Sparse).

data points missing values. A* search algorithm implemented Java3 .
compared algorithm branch bound (BB)4 (de Campos & Ji, 2011), dynamic programming (DP)5 (Silander & Myllymaki, 2006), integer linear programming
(GOBNILP) algorithms6 (Cussens, 2011). used latest versions software
source code time experiments well default parameter settings;
version 1.1 GOBNILP 2.1.1 SCIP. BB DP calculate MDL,
use BIC score, uses equivalent calculation MDL. results confirmed
algorithms found Bayesian networks either belong
equivalence class. experiments performed 2.66 GHz Intel Xeon 16GB
RAM running SUSE Linux Enterprise Server version 10.
6.1 Full vs Sparse Parent Graphs
first evaluated memory savings made possible sparse parent graphs comparison full parent graphs. particular, compared maximum number
scores stored variables algorithm. typical dynamic programming algorithm stores scores possible parent sets variables.
memory-efficient dynamic programming (Malone et al., 2011b) stores possible parent
sets one layer parent graphs variables, size largest layer
3. software package source code named URLearning (You Learning) implementing A*
algorithm downloaded http://url.cs.qc.cuny.edu/software/URLearning.html.
4. http://www.ecse.rpi.edu/cvrl/structlearning.html
5. http://b-course.hiit.fi/bene
6. http://www.cs.york.ac.uk/aig/sw/gobnilp/

47

fiYuan & Malone

parent graphs indication space requirement. sparse representation
stores optimal parent sets variables.
Figure 5 shows memory savings sparse representation benchmark
datasets. clear number optimal parent scores stored sparse representation typically several orders magnitude smaller full representation.
Furthermore, due Theorem 1, increasing number data points increases maximum number candidate parents. Therefore, number candidate parent sets increases
number data points increases; however, many new parent sets pruned
sparse representation Theorem 3. number variables affects
number candidate parent sets. Consequently, number optimal parent scores
increases function number data points number variables.
results show, amount pruning data-dependent, though, easily predictable.
practice, find number data points affect number unique scores much
number variables.
6.2 Pattern Database Heuristics
new pattern database heuristic two versions: static dynamic pattern databases;
parameterized different ways. tested various parameterizations
new heuristics A* algorithm two datasets named Autos Flag. chose
two datasets large enough number variables better
demonstrate effect pattern database heuristics. dynamic pattern database,
varied k 2 4. static pattern databases, tried groupings 9-9-8 13-13
Autos dataset groupings 10-10-9 15-14 Flag dataset. obtained
groupings simply dividing variables datasets several consecutive blocks.
results based sparse parent graphs shown Figure 6. show
results full parent graphs A* ran memory datasets
full parent graphs used. sparse representations, A* achieved much better
scalability, able solve Autos heuristic Flag
best heuristics using sparse parent graphs. Hereafter experiments results
assume use sparse parent graphs.
Also, pattern database heuristics improved efficiency scalability A* significantly. A* either simple heuristic static pattern database grouping
10-10-9 ran memory Flag dataset. pattern database heuristics enabled A* finish successfully. dynamic pattern database k = 2 helped reduce
number expanded nodes significantly datasets. Setting k = 3 helped even
more. However, increasing k 4 resulted increased search time, sometimes
even increased number expanded nodes (not shown). believe larger k always
results better pattern database; occasional increase expanded nodes
greedy strategy used choose patterns fully utilize better heuristic.
longer search time understandable though, less efficient compute
heuristic value larger pattern databases, inefficiency gradually overtook
benefit. Therefore, k = 3 seems best parametrization dynamic pattern
database general. static pattern databases, able test much larger
48

fiLearning Optimal Bayesian Networks

1.00E+04

Running Time

Size Pattern Database

1.00E+05

1.00E+03
1.00E+02
1.00E+01
1.00E+00

500
450
400
350
300
250
200
150
100
50
0

Autos

1.00E+04

Running Time

Size Pattern Database

1.00E+05

1.00E+03
1.00E+02
1.00E+01
1.00E+00

500
450
400
350
300
250
200
150
100
50
0

X

X

F lag
Figure 6: comparison A* enhanced different heuristics (hsimple , hdynamic k =
2, 3, 4, hstatic groupings 9-9-8 13-13 Autos dataset
groupings 10-10-9 15-14 Flag dataset). Size Pattern Database
means number patterns stored. Running Time means search time
(in seconds) using indicated pattern database strategy. X means
memory.

groups need enumerate groups certain size. results
suggest fewer larger groups tend result tighter heuristic.
sizes static pattern databases typically much larger dynamic
pattern databases. However, time needed create pattern databases still negligible comparison search time cases. thus cost effective try compute
larger affordable-size static pattern databases achieve better search efficiency.
results show best static pattern databases typically helped A* achieve better
efficiency dynamic pattern databases, even number expanded nodes
larger. reason calculating heuristic values much efficient
using static pattern databases.
49

fiYuan & Malone

10000
BB Scoring

DP Scoring

A* Scoring

Scoring Time

1000

100

10

1

0.1

Figure 7: comparison scoring time BB, DP, A* algorithms. label
X-axis consists dataset name, number variables, number
data points.

6.3 A* Simple Heuristic
first tested A* hsimple heuristic. competing algorithm roughly two
phases, computing optimal parent sets/scores (scoring phase) searching Bayesian
network structure (searching phase). therefore compare algorithms based two
parts running time: scoring time search time. Figure 7 shows scoring times
BB, DP, A*. GOBNILP included assumes optimal scores
provided input. label horizontal axis shows dataset, number
variables, number data points. results show AD-tree method used
A* algorithm seems efficient approach computing parent scores.
scoring part DP often order magnitude slower others.
result somewhat misleading, however. scoring searching parts DP
tightly integrated algorithms. result, work DP done
scoring part; little work left search. show shortly, search time
DP typically short.
Figure 8(a) reports search time algorithms. benchmark
datasets difficult algorithms take long even fail find optimal
solutions. therefore terminate algorithm early runs 7,200 seconds
dataset. results show BB succeeded two datasets, Voting
Hepatitis, within time limit. datasets, A* algorithm several orders
magnitude faster BB. major difference A* BB formulation
search space. BB searches space directed cyclic graphs, A* always
maintains directed acyclic graph search. results indicate better
search space directed acyclic graphs.
results show search time needed DP algorithm often shorter
A*. explained earlier, reason heavy lifting DP done
50

fiLearning Optimal Bayesian Networks

10000
BB

DP

GOBNILP

A*

Search Time

1000

100

10

1

X X

X

X

X

X

X

X

X X

X

(a)
10000

Total Running Time

DP Total Time

A* Total Time

1000

100

10

1

(b)
Figure 8: comparison (a) search time (in seconds) BB, DP, GOBNILP, A*
(b) total running time DP A*. X means corresponding
algorithm finish within time limit (7,200 seconds) ran memory
case A*.
.
scoring part. add scoring search time together, shown Figure 8(b),
A* several times faster DP datasets except Adult Voting (Again,
GOBNILP left search part). main difference A*
DP A* explores part order graph, dynamic programming fully
evaluates graph. However, step A* search algorithm overhead
cost computing heuristic function maintaining priority queue. One step
51

fiYuan & Malone

A* expensive similar dynamic programming step. pruning
outweigh overhead, A* slower dynamic programming. Adult
Voting large number data points, makes pruning technique
Theorem 1 less effective. Although DP algorithm perform pruning, due
simplicity, algorithm highly streamlined optimized performing
calculations. DP algorithm faster A* search two
datasets. However, A* algorithm efficient DP datasets.
datasets, number data points large comparison number
variables. pruning significantly outweighs overhead A*. example,
A* runs faster Mushroom dataset comparing total running time even though
Mushroom 8,000 data points.
comparison GOBNILP A* shows advantages. A* able find optimal Bayesian networks datasets well within
time limit. GOBNILP failed learn optimal Bayesian networks three datasets,
including Letter, Image, Mushroom. reason GOBNILP formulates
learning problem integer linear program whose variables correspond optimal
parent sets variables. Even though datasets many variables,
many optimal parent sets, integer programs many variables
solvable within time limit. hand, results show GOBNILP
quite efficient many datasets. Even though dataset may many
variables, GOBNILP solve efficiently long number optimal parent sets
small. much efficient A* datasets Hepatitis Heart, although
opposite true datasets Adult Statlog.
6.4 A* Pattern Database Heuristics
Since static pattern databases seem work better dynamic pattern databases
cases, tested A* static pattern database (A*,SP) A*, DP, GOBNILP
datasets used Figure 8 well several larger datasets. used simple
static grouping n2 n2 datasets, n number variables.
results BB excluded solve additional dataset. results
shown Figure 9.
benefits brought pattern databases A* rather obvious.
datasets A* able finish, A*,SP typically order magnitude
faster. addition, A*,SP able solve three larger datasets: Sensor, Autos, Flag,
A* failed them. running time datasets pretty short,
indicates memory consumption parent graphs reduced, A*
able use memory order graph solve search problems rather
easily.
DP able solve one dataset, Autos, A* able solve.
somewhat surprising given A* pruning capability. explanation A*
stores search information RAM, fail RAM exhausted. DP
algorithm described Silander Myllymaki (2006) stores intermediate results
computer files hard disks, able scale larger datasets A*.
52

fiLearning Optimal Bayesian Networks

1000

Search Time

DP

GOBNILP

A*

A*, SP

100

10

1

X

X

X

XXX

X

X XX X X

Figure 9: comparison search time (in seconds) DP, GOBNILP, A*, A*,SP.
X means corresponding algorithm finish within time
limit (7,200 seconds) ran memory case A*.

GOBNILP able solve Autos, Horse, Flag, failed Sensors. Sensors
dataset 5, 456 data points. number optimal parent sets large, almost
106 shown Figure 5. GOBNILP begins difficulty solving datasets
8, 000 optimal parent scores particular computing environment. again,
GOBNILP quite efficient datasets able solve Autos Flag.
algorithm solve Horse dataset. Figure 5, clear
reason number optimal parent sets small dataset.
6.5 Pruning A*
gain insight performance A*, looked amount pruning
A* different layers order graph. plot Figure 10 detailed numbers
expanded nodes versus numbers unexpanded nodes layer order
graph two datasets: Mushroom Parkinsons. use datasets
largest datasets solved A* A*,SP, manifest different
pruning behaviors. top two figures show results A* simple heuristic,
bottom two show A*,SP algorithm.
Mushroom, plain A* needed expand small portion search nodes
layer, indicates heuristic function quite tight dataset. effective
pruning started early 6th layer. Parkinsons, however, plain A*
successful pruning nodes. first 13 layers, heuristic function appeared
loose. A* expand nodes layers. heuristic function became
tighter latter layers enabled A* prune increasing percentage search
nodes. help pattern database heuristic, however, A*,SP helped prune many
53

fi1.60E+06
Expanded

1.40E+06

Unexpanded

ExpandedvsUnexpandedNodes

ExpandedvsUnexpandedNodes

Yuan & Malone

1.20E+06
1.00E+06
8.00E+05
6.00E+05
4.00E+05
2.00E+05
0.00E+00
0

2

4

6

8

10

12 14
Layer

16

18

20

1.60E+06
Expanded

1.40E+06
1.20E+06
1.00E+06
8.00E+05
6.00E+05
4.00E+05
2.00E+05
0.00E+00
0

22

(a) A* Mushroom

2

4

6

8

10 12 14
Layer

16

18

20

22

18

20

22

(b) A* Parkinsons
1.60E+06

1.60E+06
Expanded

Unexpanded

ExpandedvsUnexpandedNodes

ExpandedvsUnexpandedNodes

Unexpanded

1.40E+06
1.20E+06
1.00E+06
8.00E+05
6.00E+05
4.00E+05
2.00E+05
0.00E+00
0

2

4

6

8

10 12
Layer

14

16

18

20

22

(c) A*,SP Mushroom

Expanded

Unexpanded

1.40E+06
1.20E+06
1.00E+06
8.00E+05
6.00E+05
4.00E+05
2.00E+05
0.00E+00
0

2

4

6

8

10 12 14
Layer

16

(d) A*,SP Parkinsons

Figure 10: number expanded unexpanded nodes A* layer order
graph Mushroom Parkinsons using different heuristics.

search nodes Parkinsons; pruning became effective early 6th layer.
A*,SP helped prune nodes Mushroom, although benefit clear
A* already quite effective dataset.
6.6 Factors Affecting Learning Difficulty
Several factors may affect difficulty dataset Bayesian network learning
algorithms, including number variables, number data points, number
optimal parent sets. analyzed correlation factors search
times algorithms. replaced occurrence time 7,200 order
make analysis possible (we caution though may results underestimation).
Figure 11 shows results. excluded results BB finished two
datasets. DP, A*, A*,SP, important factor determining efficiency
number variables, correlations search time numbers
variables greater 0.58. However, seems negative correlation
search time number data points. Intuitively, increasing number
data points make dataset difficult. explanation preexisting negative correlation number data points number variables
datasets tested; analysis shows correlation 0.61.
54

fiLearning Optimal Bayesian Networks

1

Variables

Data!Records

Optimal!Parent!Sets

0.8

Correlation

0.6
0.4
0.2
0
0.2

DP

GOBNILP

A*

A*,!SP

0.4
0.6

Figure 11: correlation search time algorithms several factors
may affect difficulty learning problem, including number
variables, number data points dataset, number optimal
parent sets.

Since search time strong positive correlation number variables,
seemingly negative correlation search time number data points
becomes less surprising.
comparison, efficiency GOBNILP affected number optimal
parent sets; correlation high close 0.8. Also, positive correlation
number data points efficiency. because, explained earlier,
data points often leads optimal parent sets. Finally, correlation
number variables almost zero, means difficulty dataset GOBNILP
determined number variables.
insights quite important, provide guideline choosing suitable
algorithm given characteristic dataset. many optimal parent sets
many variables, A* better algorithm; way around true, GOBNILP
better.
6.7 Effect Scoring Functions
analyses far based mainly MDL score. decomposable scoring
functions used A* algorithm, correctness search strategies
heuristic functions affected scoring function. However, different scoring
functions may different properties. example, Theorem 1 property MDL
score. cannot use pruning technique scoring functions. Consequently,
number optimal parent sets, tightness heuristic, practical performance
various algorithms may affected.
verify hypothesis, tested BDeu scoring function (Heckerman, 1998)
equivalent sample size set 1.0. Since scoring phase common
exact algorithms, focus experiment comparing number optimal parent
sets resulted scoring functions, search time A*,SP GOBNILP
55

fiYuan & Malone

Optimal PS, MDL

Optimal PS, Bdeu

Size

10000000
1000000
100000
10000
1000
100
10
1

(a)
10000
GOBNILP, MDL

GOBNILP, BDeu

A*, MDL

A*, BDeu

Search Time

1000

100

10

1

XX

XX

X

XX

XX

X

(b)
Figure 12: comparison (a) number optimal parent sets, (b) search time
A*,SP GOBNILP various datasets two scoring functions, MDL
BDeu.

datasets; Horse Flag included optimal parent sets
unavailable. Figure 12 shows results.
main observation number optimal parent sets differ MDL
BDeu. BDeu score tends allow larger parent sets MDL results
larger number optimal parent sets datasets. difference around
order magnitude datasets Imports Autos.
comparison search time shows A*,SP affected much GOBNILP. increase number optimal parent sets, efficiency finding
optimal parent set affected, A*,SP slowed slightly
datasets. significant change Mushroom dataset. took A*,SP 2
seconds solve dataset using MDL, 115 seconds using BDeu. comparison,
GOBNILP affected much more. able solve datasets Imports Autos effi56

fiLearning Optimal Bayesian Networks

ciently using MDL, failed solve within 3 hours using BDeu. remained
unable solve Letter, Image, Mushroom, Sensors within time limit.

7. Discussions Conclusions
paper presents shortest-path perspective problem learning optimal Bayesian
networks optimize given scoring function. uses implicit order graph represent
solution space learning problem shortest path start
goal nodes graph corresponds optimal Bayesian network. perspective
highlights importance two orthogonal directions research. One direction
develop search algorithms solving shortest path problem. main contribution
made line A* algorithm solving shortest path problem learning
optimal Bayesian network. Guided heuristic functions, A* algorithm focuses
searching promising parts solution space finding optimal Bayesian
network.
second equally important research direction development search heuristics.
introduced two admissible heuristics shortest path problem. first heuristic
estimates future cost completely relaxing acyclicity constraint Bayesian networks. shown admissible consistent. second heuristic,
k-cycle conflict heuristic, developed based additive pattern database technique.
Unlike simple heuristic variable allowed choose optimal parents independently, new heuristic tightens estimation enforcing acyclicity constraint
within small groups variables. two specific approaches computing
new heuristic. One approach named dynamic k-cycle conflict heuristic computes costs
groups variables size k. search, dynamically partition
remaining variables exclusive patterns calculating heuristic value.
approach named static k-cycle conflict heuristic partitions variables several static
exclusive groups, computes separate pattern database group. sum
costs static pattern databases obtain admissible heuristic. heuristics
remain admissible consistent, although consistency dynamic k-cycle conflict
may sacrificed due greedy method used select patterns.
tested A* algorithm empowered different search heuristics set UCI
machine learning datasets. results show pattern database heuristics
contributed significant improvements efficiency scalability A* algorithm. results show A* algorithm typically efficient dynamic
programming shares similar formulation. comparison GOBNILP, integer
programming algorithm, A* less sensitive number optimal parent sets, number
data points, scoring functions, sensitive number variables
datasets. advantages, believe methods represent promising approach
learning optimal Bayesian network structures.
Exact algorithms learning optimal Bayesian networks still limited relatively
small problems. scaling learning needed, e.g., incorporating domain
expert knowledge learning. means approximation methods still useful
domains many variables. Nevertheless, exact algorithms valuable
serve basis evaluate different approximation methods
57

fiYuan & Malone

quality assurance. Also, promising research direction develop algorithms
best properties approximation exact algorithms, is,
find good solutions quickly and, given enough resources, converge optimal
solution (Malone & Yuan, 2013).

Acknowledgments
research supported NSF grants IIS-0953723, EPS-0903787, IIS-1219114
Academy Finland (Finnish Centre Excellence Computational Inference Research
COIN, 251170). Part research previously presented IJCAI-11 (Yuan,
Malone, & Wu, 2011) UAI-12 (Yuan & Malone, 2012).

Appendix A. Proofs
following proofs theorems paper.
A.1 Proof Theorem 5
Proof: Note optimal parent set X U subset U,
subset best score. Sorting unique parent scores makes sure
first found subset must satisfy requirements stated theorem.

A.2 Proof Theorem 6
Proof: Heuristic function h clearly admissible, allows remaining variable
choose optimal parents variables V. chosen parent set must
superset parent set variable optimal directed acyclic graph
consisting remaining variables. Due Theorem 4, heuristic results lower
bound cost.

A.3 Proof Theorem 7
Proof: successor node U, let \ U.
X
h(U) =
BestScore(X, V\{X})
XV\U



X

BestScore(X, V\{X})

XV\U,X6=Y

+BestScore(Y, U)
= h(S) + c(U, S).
inequality holds fewer variables used select optimal parents . Hence,
h consistent.

A.4 Proof Theorem 8
Proof: theorem proven noting avoiding cycles variables
U equivalent finding optimal ordering variables best joint score.
58

fiLearning Optimal Bayesian Networks

different paths V \ U goal node correspond different orderings
variables, among shortest path hence corresponds optimal ordering.
A.5 Proof Theorem 9
Proof: node U, assume remaining variables V \ U partitioned exclusive
sets V1 , ..., Vp . decomposability scoring function, h(U) =
p
P
c(Vi ). computing c(Vi ), allow directed cycles within Vi .
i=1

variables V \ Vi valid candidate parents, however. cost pattern, c(Vi ),
must optimal definition pattern databases. argument used
proof Theorem 6, h(U) cost cannot worse total cost V \ U, is,
cost optimal directed acyclic graph consisting variables (with U allowable
parents also). Otherwise, simply arrange variables patterns
order optimal directed acyclic graph get cost. Therefore, heuristic
still admissible.
Note previous argument relies optimality pattern costs,
patterns chosen. greedy strategy used dynamic pattern database
affects patterns selected. Therefore, theorem holds dynamic
static pattern databases.

A.6 Proof Theorem 10
Proof: Recall using static pattern databases node partitions V = Vi ,
heuristic value node U follows.
h(U) =

X

c((V \ U) Vi ),



(V \U) Vi pattern ith static pattern database. Then, successor
node U, let \ U. Without lost generality, let (V \ U) Vj . heuristic
value node
h(S) =

X

c((V \ U) Vi ) + c((V \ U) (Vj \ {Y })).

i6=j

Also, cost U
c(U, S) = BestScore(Y, U).
definition pattern database, know c((V\U)Vj ) best possible
joint score variables pattern U searched. Therefore,
c((V \ U) Vj ) c(V \ U) Vj \ {Y }) + BestScore(Y, (i6=j Vi ) (Vj \ (V \ U))
c((V \ U) (Vj \ {Y })) + BestScore(Y, U).
last inequality holds U (i6=j Vi ) (Vj \ (V \ U)). following
immediately follows.
h(U) h(S) + c(U, S).
59

fiYuan & Malone

Hence, static k-cycle conflict heuristic consistent.

A.7 Proof Theorem 11
Proof: heuristic values calculated dynamic pattern database considered shortest distances nodes abstract space. abstract space consists
set nodes, i.e., subsets V. However, additional arcs added
node nodes k additional variables.
Consider shortest path p two nodes U goal V original solution
space. path remains valid path, may longer shortest path U
V additional arcs.
Let g (U, V) shortest distance U V abstract space.
successor node U, must following.
g (U, V) g (U, S) + g (S, V).

(8)

Now, recall g (U, V) g (S, V) heuristic values original solution
space, g (U, S) equal arc cost c(U, S) original space. therefore
following.
h(U) c(U, S) + h(S).
(9)
Hence, dynamic k-cycle conflict heuristic consistent.



References
Acid, S., & de Campos, L. M. (2001). hybrid methodology learning belief networks:
BENEDICT. International Journal Approximate Reasoning, 27 (3), 235262.
Akaike, H. (1973). Information theory extension maximum likelihood principle.
Proceedings Second International Symposium Information Theory, pp.
267281.
Bache, K., & Lichman, M.
http://archive.ics.uci.edu/ml.

(2013).

UCI

machine

learning

repository.

Bouckaert, R. R. (1994). Properties Bayesian belief network learning algorithms.
Proceedings Tenth Conference Uncertainty Artificial Intelligence, pp.
102109, Seattle, WA. Morgan Kaufmann.
Bozdogan, H. (1987). Model selection Akaikes information criterion (AIC): general
theory analytical extensions. Psychometrika, 52, 345370.
Buntine, W. (1991). Theory refinement Bayesian networks. Proceedings seventh
conference (1991) Uncertainty artificial intelligence, pp. 5260, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Cheng, J., Greiner, R., Kelly, J., Bell, D., & Liu, W. (2002). Learning Bayesian networks
data: information-theory based approach. Artificial Intelligence, 137 (1-2),
4390.
60

fiLearning Optimal Bayesian Networks

Chickering, D. (1995). transformational characterization equivalent Bayesian network
structures. Proceedings 11th annual conference uncertainty artificial
intelligence (UAI-95), pp. 8798, San Francisco, CA. Morgan Kaufmann Publishers.
Chickering, D. M. (1996). Learning Bayesian networks NP-complete. Learning
Data: Artificial Intelligence Statistics V, pp. 121130. Springer-Verlag.
Chickering, D. M. (2002). Learning equivalence classes Bayesian-network structures.
Journal Machine Learning Research, 2, 445498.
Cooper, G. F., & Herskovits, E. (1992). Bayesian method induction probabilistic
networks data. Machine Learning, 9, 309347.
Culberson, J. C., & Schaeffer, J. (1998). Pattern databases. Computational Intelligence,
14, 318334.
Cussens, J. (2011). Bayesian network learning cutting planes. Proceedings
Twenty-Seventh Conference Annual Conference Uncertainty Artificial Intelligence (UAI-11), pp. 153160, Corvallis, Oregon. AUAI Press.
Daly, R., & Shen, Q. (2009). Learning Bayesian network equivalence classes ant colony
optimization. Journal Artificial Intelligence Research, 35, 391447.
Dash, D., & Cooper, G. (2004). Model averaging prediction discrete Bayesian
networks. Journal Machine Learning Research, 5, 11771203.
Dash, D. H., & Druzdzel, M. J. (1999). hybrid anytime algorithm construction
causal models sparse data. Proceedings Fifteenth Annual Conference
Uncertainty Artificial Intelligence (UAI99), pp. 142149, San Francisco, CA.
Morgan Kaufmann Publishers, Inc.
de Campos, C. P., & Ji, Q. (2011). Efficient learning Bayesian networks using constraints.
Journal Machine Learning Research, 12, 663689.
de Campos, C. P., & Ji, Q. (2010). Properties Bayesian Dirichlet scores learn Bayesian
network structures. Fox, M., & Poole, D. (Eds.), AAAI, pp. 431436. AAAI Press.
de Campos, L. M. (2006). scoring function learning Bayesian networks based
mutual information conditional independence tests. Journal Machine Learning
Research, 7, 21492187.
de Campos, L. M., Fernndez-Luna, J. M., Gmez, J. A., & Puerta, J. M. (2002). Ant colony
optimization learning Bayesian networks. International Journal Approximate
Reasoning, 31 (3), 291311.
de Campos, L. M., & Huete, J. F. (2000). new approach learning belief networks
using independence criteria. International Journal Approximate Reasoning, 24 (1),
11 37.
61

fiYuan & Malone

de Campos, L. M., & Puerta, J. M. (2001). Stochastic local algorithms learning belief
networks: Searching space orderings. Benferhat, S., & Besnard, P.
(Eds.), ECSQARU, Vol. 2143 Lecture Notes Computer Science, pp. 228239.
Springer.
Edelkamp, S., & Schrodl, S. (2012). Heuristic Search - Theory Applications. Morgan
Kaufmann.
Felner, A., Korf, R., & Hanan, S. (2004). Additive pattern database heuristics. Journal
Artificial Intelligence Research, 22, 279318.
Felzenszwalb, P. F., & McAllester, D. A. (2007). generalized A* architecture. Journal
Artificial Intelligence Research, 29, 153190.
Friedman, N., & Koller, D. (2003). Bayesian network structure: Bayesian
approach structure discovery Bayesian networks. Machine Learning, 50 (1-2),
95125.
Friedman, N., Nachman, I., & Peer, D. (1999). Learning Bayesian network structure
massive datasets: sparse candidate algorithm. Laskey, K. B., & Prade, H.
(Eds.), Proceedings Fifteenth Conference Conference Uncertainty Artificial
Intelligence (UAI-99), pp. 206215. Morgan Kaufmann.
Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide
Theory NP-Completeness. W. H. Freeman & Co., New York, NY, USA.
Glover, F. (1990). Tabu search: tutorial. Interfaces, 20 (4), 7494.
Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). formal basis heuristic determination minimum cost paths. IEEE Trans. Systems Science Cybernetics, 4 (2),
100107.
Heckerman, D., Geiger, D., & Chickering, D. M. (1995). Learning Bayesian networks:
combination knowledge statistical data. Machine Learning, 20, 197243.
Heckerman, D. (1998). tutorial learning Bayesian networks. Holmes, D., & Jain,
L. (Eds.), Innovations Bayesian Networks, Vol. 156 Studies Computational
Intelligence, pp. 3382. Springer Berlin / Heidelberg.
Hemmecke, R., Lindner, S., & Studeny, M. (2012). Characteristic imsets learning
Bayesian network structure. International Journal Approximate Reasoning, 53 (9),
13361349.
Hsu, W. H., Guo, H., Perry, B. B., & Stilson, J. A. (2002). permutation genetic algorithm
variable ordering learning Bayesian networks data. Langdon, W. B.,
Cant-Paz, E., Mathias, K. E., Roy, R., Davis, D., Poli, R., Balakrishnan, K., Honavar,
V., Rudolph, G., Wegener, J., Bull, L., Potter, M. A., Schultz, A. C., Miller, J. F.,
Burke, E. K., & Jonoska, N. (Eds.), GECCO, pp. 383390. Morgan Kaufmann.
62

fiLearning Optimal Bayesian Networks

Jaakkola, T., Sontag, D., Globerson, A., & Meila, M. (2010). Learning Bayesian network
structure using LP relaxations. Proceedings 13th International Conference
Artificial Intelligence Statistics (AISTATS), pp. 358365, Chia Laguna Resort,
Sardinia, Italy.
Klein, D., & Manning, C. D. (2003). A* parsing: Fast exact Viterbi parse selection.
Proceedings Human Language Conference North American Association
Computational Linguistics (HLT-NAACL), pp. 119126.
Koivisto, M., & Sood, K. (2004). Exact Bayesian structure discovery Bayesian networks.
Journal Machine Learning Research, 5, 549573.
Kojima, K., Perrier, E., Imoto, S., & Miyano, S. (2010). Optimal search clustered
structural constraint learning Bayesian network structure. Journal Machine
Learning Research, 11, 285310.
Lam, W., & Bacchus, F. (1994). Learning Bayesian belief networks: approach based
MDL principle. Computational Intelligence, 10, 269293.
Larranaga, P., Kuijpers, C. M. H., Murga, R. H., & Yurramendi, Y. (1996). Learning
Bayesian network structures searching best ordering genetic algorithms. IEEE Transactions Systems, Man, Cybernetics, Part A, 26 (4), 487
493.
Malone, B., & Yuan, C. (2013). Evaluating anytime algorithms learning optimal Bayesian
networks. Proceedings 29th Conference Uncertainty Artificial Intelligence (UAI-13), pp. 381390, Seattle, Washington.
Malone, B., Yuan, C., Hansen, E., & Bridges, S. (2011a). Improving scalability optimal Bayesian network learning frontier breadth-first branch bound search.
Proceedings 27th Conference Uncertainty Artificial Intelligence (UAI-11),
pp. 479488, Barcelona, Catalonia, Spain.
Malone, B., Yuan, C., & Hansen, E. A. (2011b). Memory-efficient dynamic programming
learning optimal Bayesian networks. Proceedings 25th AAAI Conference
Artificial Intelligence (AAAI-11), pp. 10571062, San Francisco, CA.
Moore, A., & Lee, M. S. (1998). Cached sufficient statistics efficient machine learning
large datasets. Journal Artificial Intelligence Research, 8, 6791.
Moore, A., & Wong, W.-K. (2003). Optimal reinsertion: new search operator accelerated accurate Bayesian network structure learning. International
Conference Machine Learning, pp. 552559.
Myers, J. W., Laskey, K. B., & Levitt, T. S. (1999). Learning Bayesian networks
incomplete data stochastic search algorithms. Laskey, K. B., & Prade, H.
(Eds.), Proceedings Fifteenth Conference Conference Uncertainty Artificial
Intelligence (UAI-99), pp. 476485. Morgan Kaufmann.
63

fiYuan & Malone

Ordyniak, S., & Szeider, S. (2010). Algorithms complexity results exact Bayesian
structure learning. Gruwald, P., & Spirtes, P. (Eds.), Proceedings 26th
Conference Conference Uncertainty Artificial Intelligence (UAI-10), pp. 401
408. AUAI Press.
Ott, S., Imoto, S., & Miyano, S. (2004). Finding optimal models small gene networks.
Pacific Symposium Biocomputing, pp. 557567.
Papadimitriou, C. H., & Steiglitz, K. (1982). Combinatorial optimization: algorithms
complexity. Prentice-Hall, Inc., Upper Saddle River, NJ, USA.
Parviainen, P., & Koivisto, M. (2009). Exact structure discovery Bayesian networks
less space. Proceedings Twenty-Fifth Conference Uncertainty Artificial
Intelligence, Montreal, Quebec, Canada. AUAI Press.
Pearl, J. (1984). Heuristics: intelligent search strategies computer problem solving.
Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA.
Pearl, J. (1988). Probabilistic reasoning intelligent systems: networks plausible inference. Morgan Kaufmann Publishers Inc.
Perrier, E., Imoto, S., & Miyano, S. (2008). Finding optimal Bayesian network given
super-structure. Journal Machine Learning Research, 9, 22512286.
Rissanen, J. (1978). Modeling shortest data description. Automatica, 14, 465471.
Silander, T., & Myllymaki, P. (2006). simple approach finding globally optimal Bayesian network structure. Proceedings 22nd Annual Conference
Uncertainty Artificial Intelligence (UAI-06), pp. 445452. AUAI Press.
Silander, T., Roos, T., Kontkanen, P., & Myllymaki, P. (2008). Factorized normalized
maximum likelihood criterion learning Bayesian network structures. Proceedings
4th European Workshop Probabilistic Graphical Models (PGM-08), pp. 257
272.
Singh, A., & Moore, A. W. (2005). Finding optimal Bayesian networks dynamic programming. Tech. rep. CMU-CALD-05-106, Carnegie Mellon University.
Spirtes, P., Glymour, C., & Scheines, R. (2000). Causation, prediction, search (second
edition). MIT Press.
Suzuki, J. (1996). Learning Bayesian belief networks based minimum description
length principle: efficient algorithm using B&B technique. International
Conference Machine Learning, pp. 462470.
Teyssier, M., & Koller, D. (2005). Ordering-based search: simple effective algorithm
learning Bayesian networks. Proceedings Twenty-First Annual Conference
Uncertainty Artificial Intelligence (UAI-05), pp. 584590. AUAI Press.
64

fiLearning Optimal Bayesian Networks

Tian, J. (2000). branch-and-bound algorithm MDL learning Bayesian networks.
UAI 00: Proceedings 16th Conference Uncertainty Artificial Intelligence,
pp. 580588, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
Tsamardinos, I., Brown, L., & Aliferis, C. (2006). max-min hill-climbing Bayesian
network structure learning algorithm. Machine Learning, 65, 3178.
Xie, X., & Geng, Z. (2008). recursive method structural learning directed acyclic
graphs. Journal Machine Learning Research, 9, 459483.
Yuan, C., Lim, H., & Littman, M. L. (2011a). relevant explanation: Computational
complexity approximation methods. Annals Mathematics Artificial Intelligence, 61, 159183.
Yuan, C., Lim, H., & Lu, T.-C. (2011b). relevant explanation Bayesian networks.
Journal Artificial Intelligence Research (JAIR), 42, 309352.
Yuan, C., Liu, X., Lu, T.-C., & Lim, H. (2009). Relevant Explanation: Properties,
algorithms, evaluations. Proceedings 25th Conference Uncertainty
Artificial Intelligence (UAI-09), pp. 631638, Montreal, Canada.
Yuan, C., & Malone, B. (2012). improved admissible heuristic learning optimal
Bayesian networks. Proceedings 28th Conference Uncertainty Artificial
Intelligence (UAI-12), pp. 924933, Catalina Island, CA.
Yuan, C., Malone, B., & Wu, X. (2011). Learning optimal Bayesian networks using A*
search. Proceedings 22nd International Joint Conference Artificial Intelligence (IJCAI-11), pp. 21862191, Helsinki, Finland.

65


