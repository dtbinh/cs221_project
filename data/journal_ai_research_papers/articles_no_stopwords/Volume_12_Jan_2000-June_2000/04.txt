Journal Articial Intelligence Research 12 (2000) 105-147

Submitted 10/99; published 3/00

Robust Agent Teams via Socially-Attentive Monitoring

Gal A. Kaminka
Milind Tambe

galk@isi.edu
tambe@isi.edu

Information Sciences Institute Computer Science Department
University Southern California
4676 Admiralty Way
Los Angeles, CA 90292, USA

Abstract
Agents dynamic multi-agent environments must monitor peers execute individual group plans. key open question much monitoring agents'
states required eective: Monitoring Selectivity Problem. investigate
question context detecting failures teams cooperating agents, via SociallyAttentive Monitoring, focuses monitoring failures social relationships
agents. empirically analytically explore family socially-attentive
teamwork monitoring algorithms two dynamic, complex, multi-agent domains,
varying conditions task distribution uncertainty. show centralized scheme
using complex algorithm trades correctness completeness requires monitoring
teammates. contrast, simple distributed teamwork monitoring algorithm results
correct complete detection teamwork failures, despite relying limited, uncertain
knowledge, monitoring key agents team. addition, report design
socially-attentive monitoring system demonstrate generality monitoring several coordination relationships, diagnosing detected failures, on-line o-line
applications.

1. Introduction
Agents complex, dynamic, multi-agent environments must able detect, diagnose,
recover failures run-time (Toyama & Hager, 1997).

instance, robot's

grip may slippery, opponents' behavior may intentionally dicult predict, communications may fail, etc. Examples environments include virtual environments
training (Johnson & Rickel, 1997; Calder, Smith, Courtemanche, Mar, & Ceranowicz, 1993),
high-delity distributed simulations (Tambe, Johnson, Jones, Koss, Laird, Rosenbloom, &
Schwamb, 1995; Kitano, Tambe, Stone, Veloso, Coradeschi, Osawa, Matsubara, Noda, &
Asada, 1997), multi-agent robotics (Parker, 1993; Balch, 1998). rst key step
process execution-monitoring (Doyle, Atkinson, & Doshi, 1986; Ambros-Ingerson &
Steel, 1988; Cohen, Amant, & Hart, 1992; Reece & Tate, 1994; Atkins, Durfee, & Shin,
1997; Veloso, Pollack, & Cox, 1998).
Monitoring execution multi-agent settings requires agent monitor peers,
since correct execution depends state peers (Cohen & Levesque,
1991; Jennings, 1993; Parker, 1993; Jennings, 1995; Grosz & Kraus, 1996; Tambe, 1997).
Monitoring peers particular importance teams, since team-members rely
work closely together related tasks:

c 2000 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiKaminka Tambe

Monitoring allows team-members coordinate actions plans team-

mates, help teammates cooperate without interference. example, drivers
cars convoy cannot drive without monitoring cars convoy,
disband convoy, help drivers cars break down.
Monitoring allows team-members use peers dynamic information sources,

learning new information. instance, driver convoy sees cars
front suddenly turn left, infer existence obstacle
milestone despite directly seeing herself.
Previous work investigated dierent ways monitoring context teams cooperating agents. example, theoretical work SharedPlans (Grosz & Kraus, 1999)

passive monitoring, agent notied proposition
changes (e.g., via communications), active monitoring, agent actively seeks
distinguished

nd proposition changes (e.g., via observations inference unobservable
attributes). Practical implementations investigated use passive monitoring via
communications (Jennings, 1995), active monitoring via plan-recognition (Huber & Durfee, 1995), active implicit monitoring via environment (Fenster, Kraus, & Rosenschein,
1995), dierent combinations methods (Parker, 1993; Jennings, 1993; Tambe,
1997; Lesh, Rich, & Sidner, 1999).

approach clearly superior another:

Passive

monitoring generally perceived less costly active monitoring, less
reliable (Grosz & Kraus, 1999; Huber & Durfee, 1995; Kaminka & Tambe, 1998).
Regardless monitoring method, bandwidth computational limitations prohibit
monitoring agent monitoring agents full extent, time (Jennings,
1995; Durfee, 1995; Grosz & Kraus, 1996). Thus key open question much monitoring agents required eective (in teams) (Jennings, 1993; Grosz & Kraus,
1996, 1999). call challenging problem

Monitoring Selectivity Problem,

i.e.,

problem selectivity observing others inferring state (based observations) monitoring.

Although raised past, framework

minimal constraints answers provided (Jennings, 1993; Grosz & Kraus, 1996).
instance, theory SharedPlans requires agents verify intentions
conict teammates (Grosz & Kraus, 1996). However, methods
verication take place left investigation (Grosz & Kraus, 1996, p.
308). Section 8 provides details related work.
paper begins address monitoring selectivity problem teams, investigating monitoring requirements eective failure detection.

focus investigation

detecting failures social relationships ideally hold agents monitored team. call monitoring social relationships

socially-attentive monitoring,

dierentiate types monitoring, monitoring failures
progress agents towards goals. Here, term social relationship used denote
relation attributes multiple agents' states. Socially-attentive monitoring convoy
example involves verifying agents common destination heading,
beliefs driving convoy mutual, etc. instance, agents observed
head dierent directions, clearly common heading. dierent
monitoring whether chosen (common) heading leads towards (agreed upon)
destination.

106

fiRobust Agent Teams via Socially-Attentive Monitoring

Monitoring relationships team (socially-attentive monitoring) critical task
monitoring team-members.

Failures maintain team's relationships often lead

catastrophic failures part team, lack cooperative behavior lack
coordination. failures often result individual agent failures, failures
agent's sensors actuators. Thus socially-attentive monitoring covers large class
failures, promotes robust individual operation.
explore socially-attentive monitoring algorithms detecting teamwork failures various conditions uncertainty.

analytically show despite presence

uncertainty actual state monitored agents, centralized

active

monitoring

scheme guarantee failure detection either sound incomplete, complete
unsound. However, requires reasoning multiple hypotheses actual
state monitored agents, monitoring agents team.

show active

distributed teamwork monitoring results sound complete detection capabilities,
despite using much simpler algorithm. distributed algorithm: (a) uses single,
possibly incorrect hypothesis actual state monitored agents, (b) involves monitoring key agents team, necessarily team-members. Using transformation
analytical constructs, show analogous results centralized failure-detection
mutual-exclusion coordination relationships.
conduct empirical investigation socially-attentive monitoring teams.
present implemented general socially-attentive monitoring framework
expected ideal social relationships maintained agents compared
actual social relationships. Discrepancies detected possible failures diagnosed. apply framework two dierent complex, dynamic, multi-agent domains,
service monitoring various social relationships, on-line o-line.
domains involve multiple interacting agents collaborative adversarial settings,
uncertainties perception action.

one domain, provide empirical results

active monitoring conrm analytical results. another domain show
o-line socially-attentive monitoring provide quantitative teamwork quality feedback
designer. provide initial diagnosis procedures detected failures.
focus explorations practical algorithms guarantees performance real-world applications. algorithms present seek complement use
passive communications-based monitoring (which unreliable many domains) explore
use unintrusive key-hole plan-recognition alternative. However, rule
use communicationswe simply seek provide techniques work even
communications fail.

analytical guarantees failure-detection soundness

completeness hold whether monitoring done communications plan-recognition.
paper organized follows: Section 2 presents motivating examples background. Section 3 presents socially-attentive monitoring framework. Section 4 explores
monitoring selectivity centralized teamwork monitoring.

Section 5 explores monitoring

selectivity distributed teamwork monitoring. Section 6 demonstrates generality
framework applying o-line conguration. Section 7 presents investigations additional relationship models. Section 8 presents related work, Section 9 concludes.
two appendices contain proofs theorems presented (Appendix A), pseudo-code
socially-attentive monitoring algorithms (Appendix B).

107

fiKaminka Tambe

2. Motivation Background
monitoring selectivity problem paper addresseshow much monitoring required
failure-detection teamsrose growing frustration signicant software
maintenance eorts two application domains.

ModSAF domain, high-

delity battleeld virtual environment (Calder et al., 1993), involved
development synthetic helicopter pilots (Tambe et al., 1995).

RoboCup soccer

simulation domain (Kitano et al., 1997) involved developing synthetic soccer players (Marsella, Adibi, Al-Onaizan, Kaminka, Muslea, Tallis, & Tambe, 1999).
environments domains dynamic complex, many uncertainties:
behavior agents (some adversarial, cooperative), unreliable communications sensors, actions may execute intended, etc.

Agents

environments therefore presented countless opportunities failure despite
designers' best eorts.
examples may serve illustrate. following two examples actual failures
occurred ModSAF domain.

use two illustrate explore

socially-attentive monitoring throughout paper:

Example 1.

Here, team three helicopter pilot agents specied way-

point (a given position), one team-members,

attackers )

towards enemy, teammates (

scout,

forward

land wait signal.

agents monitored way-point. However, due unexpected sensor failure, one
attackers failed sense way-point.

attacker correctly landed,

failing attacker continued forward scout (see Figure 1 screen shot
illustrating failure).

Example 2.

dierent run, three agents reached way-point detected it,

scout gone forward identied enemy. sent message waiting
attackers join attack enemy. One attackers receive message,
remained behind indenitely scout attacker continued
mission alone.
collected dozens similar reports ModSAF RoboCup domain.
general, failures dicult anticipate design time, due huge number
possible states. agents therefore easily nd novel states
foreseen developer, monitoring conditions communications place
proved insucient: none failure cases reported agents involved detect, let
alone correct, erroneous behavior. agent believed agents acting
coordination it, since communication received agents indicate
otherwise. However, agents violating collaboration relationships them,
agents came disagree plan executeda collaboration relationship
failure occurred.

Preliminary empirical results show upwards 30% failures

reported involved relationship violations (relationship failures).
Human observers, however, typically quick notice failures,
clear social misbehavior agents cases. able infer failure
occurred despite knowing exactly happened. instance, seeing attacker
continuing ahead despite teammates' switching dierent plan (which human

108

fiRobust Agent Teams via Socially-Attentive Monitoring

Enemy

Scout (ahead) failing attacker (trailing)

Landing attacker

Figure 1: plan-view display (the ModSAF domain) illustrating failure Example 1.
thick wavy lines contour lines.

observers inferred fact one teammates, attacker, landed)
sucient observer detect something gone amisswithout knowing
dierent plan was.
analysis showed agents monitoring suciently. However, naive solution continuous communications agents clearly impractical since: (i) agents operating hostile environment; (ii) communications
overheads would prohibitive; (iii) fact, communications equipment broke cases. therefore sought practical ways achieve
quick detection failure, based limited ambiguous knowledge available
monitoring agent.

3. Socially-Attentive Monitoring
begin overview general structure socially-attentive monitoring system, shown Figure 2. consists of: (1) social relationship knowledge-base containing
models relationships hold among monitored agents, enabling generation

expected ideal behavior

terms relationships (Section 3.1); (2) agent

team modeling component, responsible collecting representing knowledge
monitored agents'

actual behavior

(Section 3.2); (3) relationship failure-detection compo-

nent monitors violations relationships among monitored agents contrasting
expected actual behavior (Section 3.3); (4) relationship diagnosis component
veries failures, provides explanation (Section 3.4). resulting

109

fiKaminka Tambe

explanation (diagnosis) used recovery, e.g., negotiations system (Kraus,
Sycara, & Evenchik, 1998), general (re)planner (Ambros-Ingerson & Steel, 1988).

Expected
Attribute
Values

agents monitor,
agent attributes

Social Relationships Knowledge-Base
Expected Behavior
Relationship
Diagnosis

Relationship
Failure
Detections

Detected
Failure

Observations,
Communications

Actual
Behavior

Actual Values

Monitored Agent

Agent/Team Modeling Component

Diagnosis

Socially-Attentive Monitoring System

Monitored Agent

Figure 2: general structure socially-attentive monitoring system.

3.1 Knowledge-Base Relationship Models
take relationship among agents relation state attributes. relationship
model thus species dierent attributes agent's state related
agents multi-agent system. attributes include beliefs held
agents, goals, plans, actions, etc. example, many teamwork relationship models
require team-members mutual belief joint goal (Cohen & Levesque, 1991;
Jennings, 1995).

spatial formation relationship (Parker, 1993; Balch, 1998) species

relative distances, velocities maintained group agents (in
domain, helicopter pilots).

Coordination relationships may specify temporal relationships

hold among actions two agents, e.g., business contractors (Malone &
Crowston, 1991).

relationships

social

explicitly specify multiple

agents act believe maintain relationships
them.
relationship knowledge-base contains models relationships supposed
hold system, species agents participating relationships.
knowledge-base guides agent-modeling component selecting agents monitored,
attributes state need represented (for detection diagnosis).
used failure detection component generate expectations contrasted
actual relationships maintained agents. provides diagnosis component
detailed information agents' states' attributes related, drive diagnosis
process.

implementation socially-attentive monitoring teams uses four types

relationships:


formations, role-similarity, mutual exclusion, teamwork.

teamwork

monitoring



use



STEAM

(Tambe,

1997)

general

domain-

independent model teamwork, based Cohen Levesque's Joint Intentions
Framework (Levesque, Cohen, & Nunes, 1990; Cohen & Levesque, 1991) Grosz, Sidner,
Kraus's SharedPlans (Grosz & Sidner, 1990; Grosz & Kraus, 1996, 1999).

110

However,

fiRobust Agent Teams via Socially-Attentive Monitoring

teamwork models may used instead STEAM. Although STEAM used
pilot soccer agents generate collaborative behavior, reused independently
service monitoring, i.e., monitored agents assumed team, STEAM
used monitoring teamwork. STEAM teamwork models (e.g.,

Cohen &

Levesque, 1991; Jennings, 1995; Rich & Sidner, 1997) require mutual belief team members joint goals plans. characteristic used monitor teamwork
system. relationship models used secondary monitoring role.
discussed greater length Section 7.

3.2 Knowledge Monitored Agents Team
agent modeling component responsible acquiring maintaining knowledge

actual relations exist
ideal expected relations.

monitored agents. knowledge used construct
agents' states' attributes, compared

section, describe plan-recognition capabilities agent-modeling component
implementation experiments, i.e., extent knowledge could maintained
monitored agents' plans necessary. Later sections show fact limited, possibly
inaccurate, knowledge sucient eective failure detection. Thus implementations may
use optimized agent-modeling algorithms rather full capabilities. Section 3.4
discuss additional agent-modeling capabilities, necessary diagnosis.

3.2.1 Representation

monitoring teamwork relationships, found representing agents terms
selected hierarchical reactive plans enables quick monitoring state,
facilitates inference monitored agents' beliefs, goals, unobservable actions,
since capture agents' decision processes.
representation, reactive plans (Firby, 1987; Newell, 1990) form single decomposition hierarchy (a tree) represents alternative controlling processes agent.
reactive plan hierarchy (hereafter referred simply plan) selection
conditions (also referred preconditions) applicable, termination conditions used terminate suspend plans. given moment, agent
executing single path (root leaf ) hierarchy.

path composed

plans dierent levels.
Figure 3 presents small portion hierarchy, created ModSAF domain.
case Example 1, prior way-point,

agents

executing path be-

execute-mission highest-level plan, fly-flight-plan, fly-route,
traveling low-level. Upon reaching way-point, supposed switch
fly-flight-plan descendents wait-at-point. attackers would
select just-wait child wait-at-point, scout would select scout-forward
ginning

descendents. course, failing attacker detect way-point

fly-flight-plan selection conditions wait-at-point
failing attacker continued execute fly-flight-plan

termination conditions
satised
descendents.

111

fiKaminka Tambe

Execute-Mission
Fly Flight Plan (F)
Fly Route

Wait Point (W)

Join Scout (J)

Ordered Halt (H)

Low Level

Wait

Scout Forward
Traveling

Nap Earth

Contour

Figure 3: Portion Hierarchical Reactive Plan Library ModSAF Domain (Team plans
boxed. explained Section 3.3).

3.2.2 Acquisition

practical perspective, agents may cooperatively report monitoring
agent state using communications, requires communication channels
suciently fast, reliable secure.

unfortunately possible many realistic

domains, examples demonstrate (Section 2).
Alternatively, monitor may use plan-recognition infer agents' unobservable state
observable behavior. approach unintrusive robust face communication failures. course, monitor may still benet focused communications
agents, would critically dependent them.
enable plan-recognition using reactive plans (our chosen representation),
employed reactive plan-recognition algorithm called RESL (REal-time Situated Leastcommitments). key capability required allow explicit maintenance hierarchical
plan hypotheses matching agent's observed behavior, pruning hypotheses
deemed incorrect useless monitoring purposes.

RESL works expanding

entire plan-library hierarchy modeled agent, tagging paths matching
observed behavior agent modeled (see Appendix B pseudo-code
algorithm). Heuristics external knowledge may used eliminate paths (hypotheses)
deemed inappropriateindeed heuristics explored shortly. RESL's
basic approach similar previous work reactive plan recognition (Rao, 1994)
team-tracking (Tambe, 1996), used successfully ModSAF domain,
share many RESL's properties.

However, RESL adds belief-inference capabilities

used diagnosis process, discussed (Section 3.4).
Figure 4 gives simplied presentation plan hierarchies variation Example
1, agents correctly detected way-point, i.e., failure occurred (note
plans intermediate levels abstracted gure). scout
(Figure 4a) two attackers (Figures 4b, 4c) switched
(denoted

fly-flight-plan plan

F) wait-at-point plan (denoted W). outside observer using RESL

infers explanations agent's behavior observing agents. scout continues

112

fiRobust Agent Teams via Socially-Attentive Monitoring

low-level, one possible ight-methods
wait-at-point (W) plans. Thus

ahead, speed altitude matching


fly-flight-plan (F)



tagged possible hypotheses scout's executing plan hierarchy.

Similarly,

just-wait
ordered-halt (H)

attackers land, RESL recognizes executing
plan used service either

W



plan.

plana plan

helicopters ordered headquarters land immediately.

H



W

Thus

tagged explanations attackers' states (at second level

hierarchies). agents, RESL identies plan
plan.

However,

execute-mission

top-level

illustration, actual executing paths agents marked lled

arrows.

individual modeling

hypotheses match observed behavior marked

using dashed arrows. outside observer, course, way knowing
possible hypotheses correct.

Execute Mission

Wait-at-Point (W)

Execute Mission

Execute Mission

Fly-Flight-Plan (F)

Wait-at-Point (W)

Ordered-Halt (H)

Wait-at-Point (W)

Low-Level

Just-Wait

Just-Wait

Just-Wait

Low-Level

(b)

(a)

Ordered-Halt (H)

Just-Wait

(c)

Figure 4: Scout (a) Attackers' (b, c) actual recognized

abbreviated

reactive plan

hierarchies.

individual modeling hypotheses acquired individual agent (using planrecognition implementation, potentially communications), monitoring
agent must combine create team-modeling hypotheses state team
whole. monitoring agent selects single individual modeling hypothesis
individual agent combines single team-modeling hypothesis. Several
team-modeling hypotheses possible given multiple hypotheses individual agents.
instance, Figure 4, team-hypotheses

execution-mission

top-

level plan, eight dierent team-hypotheses dierentiated
second-level plan:

(W,W,W), (W,W,H), (W,H,W), (W,H,H), (F,W,W), (F,W,H), (F,H,W), (F,H,H).

observer member team, knows executing itself, would still
multiple-hypotheses teammates' states. instance, attacker Figure 4b
monitoring teammates, hypotheses second level would (W,W,W), (W,W,H),

(F,W,W), (F,W,H).

avoid explicitly representing combinatorial number hypotheses, RESL explicitly
maintains candidate hypotheses agent individually, combinations
individual models team hypotheses. Instead, combinations implicitly represented. Thus number hypotheses explicitly maintained grows linearly number
agents.

113

fiKaminka Tambe

3.3 Relationship Violation Detection
failure-detection component detects violations social relationships
hold among agents.

done comparing ideal expected relationships

actual maintenance agents. teamwork specically, relationship model requires
team-members always agree

team plan jointly executed team, similarly

Joint Responsibility (Jennings, 1995), SharedPlans (Grosz & Kraus, 1996).
requirement fails actuality (i.e., agents executing dierent team plans)
teamwork failure occurred.
basic teamwork failure detection algorithm follows.
plan-hierarchies processed top-down manner.

monitored agents'

detection component uses

teamwork model tag specic plans team plans, explicitly representing joint activity
team (these plans boxed Figures 3, 5 4).

team-plans equal depths

hierarchies used create team-modeling hypotheses. hypothesis,
plans dierent agents compared detect disagreements. dierence found
indication failure.

dierences found, comparison reaches individual

plans (non-team, therefore non-boxed gures) failure detected. Individual plans,
may chosen agent individually service team plans boxed
gures, handled using relationships discussed Section 7
instance, suppose failing attacker Example 1 monitoring attacker. Figure 5 shows view hierarchical plan left. path
right represents state attacker (who landed). state inferred example observations made monitoring attacker (here,
assuming plan-recognition process resulted one correct hypothesis
agent. discuss realistic settings below). Figure 5, dierence would
detected marked arrow two plans second level top.

fly-flight-plan team-plan (on
wait-at-point team-plan (on right).

failing attacker executing

left),

attacker executing

disagreement

team-plan executed failure teamwork.

Execute Mission

Execute Mission

Fly-Flight-Plan

Wait-at-Point

Fly-Route
Traveling
Low-Level

Just-Wait

Figure 5: Comparing two hierarchical plans. top-most dierence level 2.

114

fiRobust Agent Teams via Socially-Attentive Monitoring

Detecting disagreements dicult multiple team-modeling hypotheses, since
may imply contradictory results respect failure detection: hypotheses may imply failure occurred team, others may not. Unfortunately,
expected realistic applications. instance, Figure 4 (Section 3.2) shows several hypotheses possible based observations. However, one hypotheses,
(W,W,W), implies failure occurredall agents agreement team-plan

executingwhile another hypothesis, (F,W,H), implies failures occurred.

limit reasoning small number team hypotheses, restricting
failure-detection capabilities, use disambiguation heuristic ranks team-modeling
hypotheses level

coherence

represent. heuristic provided initial

solution. Later sections examine additional heuristics.

Denition 1.



coherence

level multi-agent modeling hypothesis dened

ratio number agents modeled number plans contained hypothesis.
denition results partial ordering hypotheses set, least coherent
hypothesis (one assigns agent dierent plan team-mates),
coherent hypothesis (that assigns plan team members).

instance,

hypothesis (F,W,H) would lowest level coherence, 1, since implies complete

breakdown teamworkevery agent executing dierent plan. hypothesis (W,W,W)
would coherence level 3, highest level coherence group three agents,
since assigned plan. Ranked would hypothesis
(W,W,H), single teamwork failure (disagreement W H) coherence level
3/2.
detection component selects single maximally-coherent team-modeling hypothesis
(ties broken randomly).

intuition using coherence failures agree occur

despite agents' attempts teamwork. Thus expect agreements disagreements team.

coherence level team-hypothesis inversely related

number teamwork failures implied hypothesis. Selecting maximally-coherent hypothesis therefore corresponds minimum-number-of-failures heuristic commonly used
diagnosis (Hamscher, Console, & de Kleer, 1992).
case depicted Figure 4, complete detection process may conceptu-

1

alized follows .

Suppose one attackers, whose hierarchy described

Figure 4b, monitoring team.

First, collects plan hypotheses top

hierarchy agent (including ). case, {execute-mission},

{execute-mission}, {execute-mission}. one team-modeling hypothesis built
these: (execute-mission,

execute-mission, execute-mission).

Since hypoth-

esis shows disagreement occurs level, process continues second level.
Here, hypotheses rst agent left {F,W}, monitoring second agent
(since knows state) one possibility {W}, third agent {W,H}.

saw above, maximally team-coherent hypothesis (W,W,W) selected. Since
indicate failure, process continues third level.

agents

executing individual plans, comparison process stops. Algorithm 2 Appendix
B provides greater details process.
1. implementations may make use optimized algorithms heuristics integrated
agent-modeling algorithm.

115

fiKaminka Tambe

sub-teams introduced, dierence team-plans may explained
agents question part dierent sub-teams.

Sub-team members still

agree joint sub-team plans, may dier one
sub-team next. now, let us assume teams consideration

teams, dened Denition 2.

simple

make denition service later analytical results

appear condition. return issue sub-teams Section 7.1.

Denition 2.

say team

simple,

plan-hierarchy involves dierent

team plans executed dierent sub-teams.
Intuitively, idea simple team, members team jointly execute
team plans hierarchy. denition somewhat similar denition


ground team

(Kinny, Ljungberg, Rao, Sonenberg, Tidhar, & Werner, 1992),

allow sub-team members team joint plan dierent
members.

3.4 Relationship Diagnosis
diagnosis component constructs explanation detected failure, identifying
failure state facilitating recovery.

diagnosis given terms set agent

belief dierences (inconsistencies) explains failure maintain relationship.
starting point process detected failure (e.g., dierence team-plans).
diagnosis process compares beliefs agents involved produce set
inconsistent beliefs explain failure.
Two problems exist practical applications procedure.

First, monitoring

agent likely access beliefs held monitored agents, since
feasible practice communicate agents' beliefs other. Second,
agent real-world domain may many beliefs, many vary among
agents, though irrelevant diagnosis. Thus relevant knowledge
may simply accessible, may hidden mountains irrelevant facts.
gain knowledge beliefs monitored agents without relying communications,
diagnosis process uses process belief ascription. agent-modeling component (using RESL implementation) maintains knowledge selection termination
conditions recognized plans (hypotheses). recognized plan hypothesis, modeling component infers termination conditions plan believed false
monitored agent (since terminated plan). found useful
use additional heuristic, infer selection conditions (preconditions)
plan

begun execution

true. idea plan selected

execution, preconditions likely hold, least short period time.
heuristic involves explicit assumption part system new plan
recognized soon begins execution.

Designers domains need verify

assumption holds.
agent i, inferred termination selection conditions make set beliefs
Bi agent. instance, suppose agent hypothesized switched

executing

fly-flight-plan



wait-at-point.

RESL infers agent believes

way-point detected (a selection condition

116

wait-at-point).

addition,

fiRobust Agent Teams via Socially-Attentive Monitoring

RESL infers agent believes enemy seen, order

wait-at-point).

received base halt mission (negated termination conditions

determine facts relevant failure, diagnosis component uses
teamwork model. teamwork model dictates beliefs agents hold must
mutually believed agents team.

dierence detected

beliefs certain failure, team members agree issues agreement
mandatory participation team.

teamwork model thus species

beliefs contained Bi sets mutual, therefore consistent:

[

Bi 6`?



inconsistency detected, diagnosis procedure looks contradictions (disagreements) would cause dierence team-plan selection. dierence beliefs serves
diagnosis, allowing monitoring agent initiate process recovery, e.g.,
negotiating conicting beliefs (Kraus et al., 1998).
example, shown Section 3.3, two attackers Example 1 (Section 2) dier
choice team-plan: One attacker continuing execution

fly-flight-plan

plan, helicopters formation. attacker detected waypoint, terminated

fly-flight-plan

wait-at-point,

switched

landing immedi-

ately (Figure 5). failing attacker monitors team-mate, detects dierence
team-plans (Section 3.3), detected dierence passed diagnosis. failing
attacker makes following inferences:
1.

Fly-flight-plan three termination conditions:

(a) seeing enemy, (b) detecting

way-point, (c) receiving order halt. failing attacker (left hierarchy
Figure 5) knows belief none conditions hold, thus
B1

2.

Wait-at-point

=

f:W ayP oint; :E nemy; :H altOrderg

one selection condition:

way-point detected.



termination condition scout sent message join it, identied
enemy's position. diagnosis component case therefore infers
attacker (right hierarchy Figure 5)
B2

=

fW ayP oint; :S coutM essageReceivedg

Then,
B1 [ B2





=

f:W ayP oint; W ayP oint; :E nemy; :S coutM essageReceived; :H altOrderg

inconsistent.



inconsistency

(disagreement





attackers)



f:W ayP oint; W ayP ointg, i.e., contradictory beliefs W aypoint. Thus failing

attacker knows team-mate seen way-point. choose quietly adapt
belief, thereby terminating

fly-flight-plan

117

selecting

wait-at-point,



fiKaminka Tambe

may choose recovery actions, negotiating attacker whether
way-point reached.
found diagnosis procedures useful many failures detected
socially-attentive monitoring (see Section 4 evaluation discussion).
since paper focuses monitoring selectivity problem

However,

detection, leave

investigation diagnosis procedures future work.

4. Monitoring Selectivity Centralized Teamwork Monitoring
Using socially-attentive framework Section 3 systematically examine failure
permutations Examples 1 2 (Section 2) centralized teamwork monitoring
conguration, single team-member monitoring team.

vary agents

failing (attacker, attacker scout, etc.) role monitoring agent (attacker
scout). report empirical results detecting diagnosing failures cases.
Using empirical results guide, explore centralized teamwork monitoring analytically. show even monitoring uncertainty, centralized teamwork monitoring
provide either sound complete detection results (but both).
starting point exploration, monitoring agent uses single maximallycoherent team-modeling hypothesis discussed Section 3.3. begin Example 2.
normal order execution

W,

execution
enemy's position.

wait-at-point (W),

followed

join-scout (J).



two attackers land wait scout visually identify

Upon identication, scout sends message join it,

triggers selection

J

plan, termination

W

plan. executing

J,

scout hovers low altitude, waiting attackers join it. failures
part attackers (they cannot receive message) part scout (it
cannot send it). failures arise, instance, radio broken team-members
range. agent fails, continues execute

W

instead switching

J.

Table 1 summarizes permutations Example 2. permutation number appear
rst column.

next three columns show actual plans selected three

agents A1, A2 A3 permutation.

second-to-last column shows whether

relationship failure occurred case, i.e., whether disagreement exists
agents. Finally, last column details physical conditions case.
possible failure permutations: case 1, none agents failed.
attacker failed.

cases 2 3 one

case 4 scout failed send message attackers failed

receive it. case 5 scout identify enemy's position (so message sent,
three agents continue execute
since attacker switch

J

W

plan). permutations possible,

plan without scout.

instance, case 2 Table 1 corresponds Example 2. scout (A3) detected
enemy, switched plan

J,

sent message attackers join it. One attacker

(A2) received message, switched plan

J, began ying towards scout.

However,

remaining attacker (A1) failed receive message, maintains position,
continuing execute

W failing switch J. Since agents longer agreement

team plan jointly executed, teamwork failure occurred. Condition
monitors used original failure case monitor scout's message. However
failures communications resulted monitoring conditions rendered useless.

118

fiRobust Agent Teams via Socially-Attentive Monitoring

Case

#

Actual Executing Plans

Relationship Failure

Physical

?

Attacker A1

Attacker A2

Scout A3

Occurred

Failure

1

J

J

J

-

-

2

W

J

J

+

A1 fails receive

3

J

W

J

+

A2 fails receive

4

W

W

J

+

A3's message lost

5

W

W

W

-

Enemy identied

Table 1: possible failure permutations broken radio-link scenario (Example 2).

One key issue raised case 5 Table 1. Here, due scout's inability identify
enemy's position (perhaps due failure scout's part, perhaps enemy
simply there), three helicopter pilots remain agreement enemy
identied. Here, even though clearly pilots failing make progress towards
task goals (the scout continues search enemy indenitely), relationship failure
taking place, since agents maintaining teamwork relationship failing
make progress. clearly demonstrates failures necessarily relationship
failures.
Table 2 presents results scout monitoring teammates Example 2, using
maximally team-coherent hypothesis basis detection. rst column shows
case number, reference Table 1. next three columns show scout's (A3's)
hypothesis plan agent executing according maximal coherence
heuristic. next two columns show whether failure detected, whether
diagnosed correctly. last column shows detection class (discussed below).

Case

#

A3's Hypothesized Executing Plans

Relationship Failure

Diagnosis

Detection

Detected?

Success?

Class

J

-

n/a

True Negative

J

+

+

True Positive

W

J

+

+

True Positive

H

J

+

-

True Positive

W

W

-

n/a

True Negative

Attacker A1

Attacker A2

Scout A3

1

J

J

2

W

J

3

J

4

H

5

W

Table 2: Scout's (A3) monitoring results permutations Example 2.
example, case 2 Table 2 shows results scout monitoring original
failure Example 2 (Section 2). Using RESL, selecting maximally-coherent hypothesis, scout hypothesizes non-moving attacker executing
moving attacker executing
selected plan

J

J (case 2, column 3).

W (case

2, column 2),

scout course knows

(case 2, column 4). violation teamwork relationship thus

detected (case 2, column 5), since A1's

W

agreement rest team's

J.

Furthermore, diagnosis successful identifying cause failure, i.e.,
fact enemy's position identied scout, knowledge
passed failing attacker (case 2, column 6).

119

fiKaminka Tambe

last column Table 2 shows detection class failure. detection class
case one of: true positive, true negative, false positive, false negative.
correspond following possible monitoring outcomes: true positive outcome
relationship failure actually occurred, detected. true negative
failure occurred, system correctly reports none detected.
false positive failure occurred, system nevertheless incorrectly
detects one, false negative failure occurred, system fails
detect it. Table 2 shows permutations Example 2 teamwork monitoring
techniques encounter problematic false positive false negative cases.
closer look results hints key contribution paper addressing
monitoring selectivity problem: Eective failure detection take place despite use
uncertain, limited, knowledge monitored agents. case 4 Table 2, monitoring agent able detect failure

involved.

despite wrong state agents

scout believes two attackers executing

W. H

plan, actually executing,

H (ordered-halt)

selected command received

headquarters halt execution hover place.

scout's perspective, hov-

ering attacker therefore inferred executing

H



W.

Thus two equally-ranked

maximally-coherent hypotheses exist: two attackers either executing
executing

H.

W



random selection made, case resulted wrong hypothe-

sis selected. Nevertheless, violation teamwork relationships detected,
neither

H



W

agrees scout's

J.

However, last column case 4 shows (in Table 2), diagnosis procedures
sensitive selection team-modeling hypothesis. hypothesis used case
correctly reect true state agents, despite scout's success
detect failure case, diagnosis procedures fail provide correct diagnosis (the
diagnosis successful two failure cases). phenomenon repeats
empirical results provide below:

diagnosis failed whenever hypothesis chosen

incorrect, although sucient detection.

intend explore ways improve

accuracy hypotheses future work, therefore address issue
paper anymore. failure detection capabilities signicant improvement
themselves, since agents know certainty failure occurred, even
diagnosis incorrect.
Many social physical failures successfully captured using team-coherence
heuristic monitoring selectivity. fact, permutations Example 2, matter
one agents monitor, failures maintain relationship (i.e., physical
failures except one team remains agreement) detected reliably,
although sometimes diagnosis failed.

result especially surprising considering

single agent monitoring. Previous monitoring methods (condition monitors
communications) unable detect failures, despite used three
agents .
Tables 3 4 present empirical results, basis Example 1.

Table 3

presents failure permutations Example 1 format Table 1. normal or-

fly-flight-plan
wait-at-point (W) plan,

der execution plans follows: agents jointly execute
(F) plan detect way-point. switch

two attackers land scout continues ahead identify enemy.

120

fiRobust Agent Teams via Socially-Attentive Monitoring

failures part agents detect way-point, thus switch


W

plan.

Case

#

Actual Executing Plans

Relationship Failure

Physical

?

Attacker A1

Attacker A2

Scout A3

Occurred

Failure

1

W

W

W

-

-

2

F

W

W

+

A1 vision fails

3

W

F

W

+

A2 vision fails

4

F

F

W

+

A1, A2 vision fails

5

W

W

F

+

A3 vision fails

6

F

W

F

+

A1, A3 vision fails

7

W

F

F

+

A2, A3 vision fails

8

F

F

F

-

A1,2,3 vision fails

Table 3: failure permutations undetected way-point scenario (Example 1).

Case

#

A1's Hypothesized Executing Plans

Relationship Failure

Detection

Attacker A1

Attacker A2

Scout A3

Detected?

Class

W

W

W

-

True Negative

2

F

W

F

+

True Positive

3

W

F

W

+

-

True Positive

False Negative
False Negative

1

4
5

F
W

F
W

F
W

6

F

W

F

+

True Positive

7

W

F

F

+

True Positive

8

F

F

F

-

True Negative

Table 4: Attacker's (A1) monitoring results permutations Example 1.

Table 4 present monitoring results permutations Example 1.



attacker A1 monitoring team using maximally team-coherent hypothesis
detecting failures. results show A1 successful detecting teamwork failures
two (cases 4-5, highlighted bold face).
two false outcomes false negatives. cases, monitoring
attacker A1 picked incorrect hypothesis scout, since scout's actions lead
ambiguous interpretations. scout forward (to scout enemy) detected
way-point (plan

W), (then would ying formationplan F).

use maximal team-coherence heuristic causes A1 prefer hypothesis
scout agreement attackers fact not. example, case 4, two
attackers failed detect way-point executing

F. Observing scout,
F W. However, believing

monitoring attacker A1 sure whether scout executing
scout executing

F results maximally-coherent

team-modeling hypothesis (all

agents agreement), believing scout executing

121

W

results less

fiKaminka Tambe

coherent hypothesis. Thus A1 selects wrong hypothesis, case fails detect
teamwork failure.
maximal team-coherence heuristic detect failures despite using incorrect hypotheses. Unfortunately, hypotheses lead false-negatives seen
Table 4. However, none experiments resulted false-positive result, i.e., result
system detected failure reality none occurred. Thus heuristic
provided sound results cases. able formally prove property holds
general maximal team-coherence heuristic used.
First, address matter notation. Let agent monitor agent B ,
executing plan P .

denote (A; B =P ) set agent-modeling hypotheses

A's agent-modeling component constructs based B 's observable behavior
execution P . words, (A; B =P ) A's set plans match B 's observable
behavior.

Note monitors itself, direct access state

(A; A=P )

=fP g. Using modeling notation, make following denitions

ground assumptions underlying knowledge used monitoring:

Denition 3.

agent-modeling

Given monitoring agent A, monitored agent B , say A's
agent B

complete

plan P may executed B, P 2

(A; B=P ).

set (A; B=P ) typically include matching hypotheses besides correct
hypothesis P, guaranteed include P. Following denition

individual

agent-

modeling completeness, dene group-wide team-modeling completeness:

Denition 4.
A's

Let agent monitoring team agents B1 ; ; Bn . say

team-modeling

team

complete

A's agent-modeling B1 ; ; Bn

complete.
Denition 4 critical guarantee capabilities explore analytically
section next. generally holds use RESL ModSAF RoboCup
domains, make explicit service applications techniques
domains.
Armed denitions, formalize failure detection capabilities suggested empirical evidence Theorem 1.

Theorem 1. Let monitoring agent

monitor simple team . A's team-modeling
complete, uses maximally team-coherent hypothesis detection,
teamwork failure detection results sound.

Proof.

show failure occurred, none detected, thus

failure detected fact failure. Let a1 ; : : : ; agent members .
agent ai executing plan Pi (1 n). Thus collectively, group executing

(P1 ; : : : ; Pn ). failure occurred, agents executing plan

P0 ,

i.e., 8i; Pi = P0 . Since A's team-modeling complete, correct hypothesis (P0 ; : : : ; P0 )
going set team-modeling hypotheses H .

Since maximally team-

coherent hypothesis, either selected, dierent hypothesis

coherence level



selected. hypothesis coherence level correct

one implies failure detected. Thus detection procedure sound.

122

fiRobust Agent Teams via Socially-Attentive Monitoring

Despite uncertainty knowledge used, sound failure-detection guaranteed using
maximal team-coherence heuristic.

one answer monitoring selectivity

problem. However, seen Table 4, failures may pass undetected using
heuristic (i.e., may result false-negatives).
may therefore unfortunately

complete guaranteed

incomplete.

Detection using maximal team-coherence

may prefer monitoring system

detect teamwork failures.

incoherence

therefore experimented maximal team-

heuristic, inverse

maximal team-coherence heuristic. heuristic prefers hypotheses suggest
failures, rather less.



Table 5 gives monitoring attacker A1's view team,

incoherent

similar Table 4, using maximally team-

hypothesis. shows indeed

using maximally team-incoherent hypothesis lead false-negative detections
cases 4 5 (in contrast cases Table 4).
Case

Relationship Failure

Detection

Attacker A1

Scout A3

Detected?

+

Class

1

W

Attacker A2

False Positive

2

F

H

W

+

True Positive

3

W

F

F

+

True Positive

F

F

W

+

True Positive

#

A1's Hypothesized Executing Plans

4
5

H

F

W

H

F

+

True Positive

6

F

H

W

+

True Positive

7

W

8

F

F

F

F

+

W

+

True Positive

False Positive

Table 5: Attacker's (A1) monitoring results permutations Example 1, using team-

incoherence.

Guided results, formally show team-incoherence heuristic leads
detection procedure

complete.

Theorem 2. Let monitoring agent monitor simple team . A's team-modeling
complete, uses maximally team-incoherent hypothesis detection,
teamwork failure detection results complete.
Proof. Analogous Theorem 1, proof provided appendix A.
However, successes oset false positive outcomes cases 1 8 Table 5.
cases, failures occurred, monitoring system falsely reported detected
failures. practice, may lead costly processing many false alarms.
Ideally, detection capabilities sound



complete. Unfortunately,

show coherence-based disambiguation scheme exists results sound
complete detection. show Theorem 3 provide sound complete detection,
disambiguation method inconsistent: Given set possible matching
hypotheses, sometimes rank one hypothesis top, sometimes another.

Theorem 3. Let

H complete team-modeling hypotheses set, modeling simple team.
exists disambiguation scheme (1) uses coherence alone basis

123

fiKaminka Tambe

disambiguation H , (2) deterministic selection, (3) results sound
complete failure detection.
Proof.

Let disambiguation scheme leads complete sound detection

uses knowledge coherence hypotheses selecting disambiguated hypothesis. Suppose contradiction deterministic, thus consistent, selection
hypothesis H , i.e., given H , set candidate hypotheses, applies
deterministic procedure choose one hypothesis based coherence.

Since

use knowledge outside coherence candidate hypotheses, given
set candidates, always choose hypothesis.

Let

monitoring agent using . Let B monitored agent, whose actions identical
executing team plans P1 ; P2 .
P2 , (Am ; B =P1 )

=

Thus, cannot determine whether B executing P1

(Am ; B =P2 )

=

fP1 ; P2 g. B executing P1 , 's

H

=

f(P1 ; P1 );

hypotheses set

(P1 ; P2 )g

Since leads complete sound detection, choose (P1 ; P1 ). Now,
B executing P1 P2 , respectively, matching hypothesis set H dened

above. must select (P1 ; P2 ). Since set candidate hypothesis H
used case, information supplied, must non-deterministic
selection disambiguated hypothesis, contradicting assumption.
empirical analytical results show use single disambiguated hypothesis leads improved, imperfect, failure-detection results, compared monitoring
conditions communications previously used. empirical results Tables 2, 4,
5 establish benets teamwork monitoring technique: physical failures
detected. However, analytical results (Theorems 1, 2, 3) show results less
perfect. algorithms either sound complete, both. complete monitoring, would require additional procedures dierentiate true positives
false ones, e.g., focused communication. procedures often expensive.
reduce need costly verication letting go insistence single
hypothesis, focusing instead maintaining two hypotheses: maximally-coherent hypothesis maximally-incoherent hypothesis.

Table 6 shows portion full set

team-hypotheses available attacker A1 monitoring team. total number
hypotheses presented table 24, many 4 co-existing single case,
thus maintaining full set hypotheses would expensive. However, two inverse
heuristicsteam-coherence incoherencerepresent two extremes space
hypotheses.

agree failure exists, failure actually occurred, since

team-coherent hypothesis guarantees soundness (Theorem 1). agree failure
exists, failure took place, since team-incoherent hypothesis guarantees completeness (Theorem 2). disagree (i.e., team-coherent hypothesis imply
failure, team-incoherent hypothesis does), monitoring system cannot sure
either way, must revert back verication.
revised detection algorithm oers signicant computational savings compared
single team-incoherent hypothesis approach. complete unsound, signicantly

124

fiRobust Agent Teams via Socially-Attentive Monitoring

Case

#
1

2

3
4
5

6

7
8

A1's Hypothesized Executing Plans

Relationship Failure

Detection

Scout A3

Detected?

Class

H

F

+

False Positive

H

W

+

False Positive

W

W

F

+

False Positive

W

W

W

-

True Negative

F

H

F

+

True Positive

F

H

W

+

True Positive

F

W

F

+

True Positive

F

W

W

+

True Positive

W

F

F

+

True Positive

W

F

W

+

True Positive

F

F

W

+

True Positive

F

F

F

-

False Negative

W

H

F

+

True Positive

W

H

W

+

True Positive

W

W

F

+

True Positive

W

W

W

-

False Negative

F

H

W

+

True Positive

Attacker A1

Attacker A2

W
W

F

H

F

+

True Positive

F

W

W

+

True Positive

F

W

F

+

True Positive

W

F

F

+

True Positive

W

F

W

+

True Positive

F

F

W

+

False Positive

F

F

F

-

True Negative

Table 6: portion attacker's (A1) monitoring hypotheses implied results
ranking used select single hypothesis case.

reduces need verication, since least team-coherent hypothesis implies
failures, verication necessary. requires representing two hypotheses,
thus still computationally cheaper maintaining exponential number hypotheses.
example, using maximally team-incoherent hypothesis permutations Example
1 results need verify eight cases seen (5). However, combine
hypothesis maximally team-coherent hypothesis (e.g., Table 4),
need verify four (50% ) cases. cases 2, 3, 6, 7 agreement
two hypotheses failure occurred, thus verication required.
monitoring agent therefore address monitoring selectivity problem balancing
resource usage guaranteed performance monitoring algorithm used.
Either simpler single-hypothesis algorithms would utilize one hypothesis
case, detection capabilities guaranteed sound complete, both.
complex algorithm, two hypotheses would reasoned case,

125

fiKaminka Tambe

algorithm would complete require verication fewer cases compared
simple-hypothesis complete algorithm.

5. Monitoring Selectivity Distributed Teamwork Monitoring
section focuses monitoring selectivity exploiting key opportunity execution monitoring multi-agent environmentsit monitored agents
distributed, monitoring agents distributed well. begin
simple scheme selecting single maximally team-coherent hypothesis. Since centralized
teamwork monitoring successful addressing permutations Example 2, focus
permutations Example 1 (Table 3), centralized teamwork monitoring
attacker resulted false-negative detections (cases 4-5 Table 4).
distributed teamwork monitoring scheme, single attacker monitor
teammates, scout (and attacker) engage monitoring. Table
7 presents monitoring results failure permutations, scout
monitoring agent.

nd scout successfully detects two failure cases

attacker failed detect, compensating attackers' monitoring mistakes. Furthermore, since scout used maximal-coherence heuristic, detection sound
verication required. reason scout's success attackers' actions
case, although ambiguous, support hypothesis matched
scout's plan. words, regardless plan attackers executing
two cases, dierent plan executed scout.
Case

Relationship Failure

Detection

Detected?

Class

W

-

True Negative

F

+

True Positive

F

W

+

True Positive

F

W

+

True Positive

H

H

F

+

True Positive

F

H

F

+

True Positive

7

H

F

F

+

True Positive

8

F

F

F

-

True Negative

#

A3's Hypothesized Executing Plans
Attacker A1

Attacker A2

Scout A3

1

W

W

2

F

W

3

W

4

F

5
6

Table 7: Scout's (A3) monitoring results permutations Example 1, using teamcoherence.

Thus agents engaged monitoring permutations Example 1, detection would
sound complete. actual failure cases (and those) would least one
team-member detects failure. attempt formally dene general conditions
phenomenon holds.

Denition 5.

say two team-plans P1 ; P2 ,

observably-dierent roles

R1 ; R2

given agent B fullls roles R1 ; R2 two plans, resp., monitoring agent
(dierent B ) (A; B =P1 ) \ (A; B =P2 )

observably-dierent roles P1 P2 , call B

126

=

key agent.

;.

say B

fiRobust Agent Teams via Socially-Attentive Monitoring

Intuitively, B key agent observably dierent roles two plans
monitoring agent dierentiate B 's behavior executing P1 executing
P2 . instance, attackers observably dierent roles

W

(in land).

F

(in y)

However, observably dierent roles

require land. scout observably dierent roles


H

W H,
W (ying)

(landing).

key-agent basis conditions self-monitoring team
detect failure agent using team-coherence. rst prove lemma
conditions single given agent detect failure. use lemma
prove conditions least one agent given team detect failure.

Lemma 1. Suppose simple team

self-monitoring (all members team monitor
other) using maximally team-coherent heuristic (and assumption
agent, team-modeling complete). Let A1 ; A2 monitoring agents members
executing P1 ; P2 , respectively. A1 would detect failure maintaining teamwork
relationships agent A2 , A2 key-agent P1 ; P2 .

Proof.

See appendix A.

A1 knows executing P1 .

A2 executing P2 , key-agent P1 P2 ,

A1 guaranteed notice dierence exists A2 , since A2
acting observably dierent would executing P1 . Note, however,
A2 may may detect dierence, since A2 's perspective, A1 's behavior may

may explained P2 . A2 detect dierence A1 's roles P1 P2
observably-dierent. However, since A1 detected failure, alert
teammates, diagnose failure, choose corrective action.
want guarantee teamwork failure always detected least
one agent, must make sure possible combination plans,
least one key-agent whose roles observably dierent.

lemma shows

agents monitoring agent notice failure one occurs. aim, dene
observably-partitioned set plans employed team.

Denition 6.

set P team-plans said

observably-partitioned

two plans

Pi ; Pj 2 P exists key-agent Aij . set Aij agents called

set P .

key agents

instance, set team-plans helicopter pilots team using
examples (Fly-Flight-Plan (F),

Wait-at-Point (W), Ordered-Halt (H), Join-Scout
(J)) observably-partitioned. attackers land W H, F J. scout
lands J H, ies W F. Table 8 shows agents observably dierent
roles two plans set. instance, nding cell intersection

H

row

W

column, nd scout observably dierent roles two

plans. Indeed, scout lands command received halt execution (H), ies
scout enemy's position executing

W.

Here, since agents observably-

dierent roles least two plans, key agents set {
teamattackers scout.

127

W , F , H, J }

includes members

fiKaminka Tambe

Fly-Flight-Plan (F)

Wait-at-Point (W)

Ordered-Halt (H)

-

Attackers

Attackers

Scout

Attackers

-

Scout

Scout Attackers

F
W
H
J

Join-Scout (J)

Attackers

Scout

-

Attackers

Scout

Scout Attackers

Attackers

-

Table 8: Observable partitioning helicopter pilot team ModSAF.

Theorem 4. simple team (1) employs observably-partitioned set team-plans O,

team-members monitor members key agents set O, (2) using complete teammodeling (3) maximally team-coherent hypotheses, teamwork failure detection
results sound complete.
Proof.

theorem 1 know detection would sound. show complete,

prove least one agent detect dierence others whenever
team-members executing plan (i.e., failure occurring). Suppose
team currently divided team-plans must executed, i.e., agents
ai ; aj team executing team plans Pi ; Pj , respectively, Pi 6= Pj .

Thus failure occurred. Let K key agents set . Since team observablypartitioned, Pi ; Pj exists least one key agent a1 2 K . three cases:

case (i).

a1 executing Pi . case agent executing Pj would detect dierence

a1 would therefore detect failure (lemma 1).

case (ii).

a1 executing Pj . case agent executing Pi would detect dierence

a1 would therefore detect failure (lemma 1).

case (iii).

a1 executing plan Q. roles must observably dierent Q

Pi , Q Pj (or both), thus agent executing Pj and/or Pi would detect
failure. case a1 's roles observably dierent Q Pi



Q

Pj impossible, since monitoring agent
(Am ; a1 =Pi ) \ (Am ; a1 =P j ) fQg 6= ;

Contradicting a1 key agent Pi ; Pj .
Since three cases, least one agent would detect failure one occurred.
Therefore, failure detection complete. Since sound seen, detection
sound complete.
theorem shows distributed teamwork monitoring result

sound complete
key

failure-detection, using simple algorithm. team-member monitors

agents 2 , using maximally team-coherent

hypothesis. detects failure, certainly

one occurred. agent detects failure, indeed failure occurred.
simple distributed algorithm, attention-focusing features guaranteed
soundness completeness contrasts complex centralized algorithm
discussed previous section (Section 4). algorithm's eectiveness relies
2. monitoring team-member know key agents are, knows exist,
monitor team-members. increases monitoring, sound complete failure detection
still guaranteed.

128

fiRobust Agent Teams via Socially-Attentive Monitoring

condition observably-partitioned set plans, distribution monitoring.
corollary Theorems 3 4 key agents available distributed
case, failure detection either sound complete, both. even key agents
available, centralized teamwork monitoring still complete sound.
Fortunately, observable-partitioning dicult property design: Teams
often composed agents role plan, general,
roles observable dierences them. instance, helicopter pilot team
ModSAF domain typically executes set plans property, Table 8
demonstrates.
team, however, observably-partitioned, may case two agents
executing dierent plan, agent able detect using teamcoherence heuristic. minimal case occurs two agents, A1 A2
executing plans P1 P2 , respectively, P1 P2 observably dierent,

(A2 ; A1 =P1 ) \ (A1 ; A2 =P 2)

=

fP1 ; P2 g

result A1 A2 believing agreement them.
check situation made part plan design process, marking

points

risky

execution detection either sound complete (Theorem 3),

verication (e.g., communications) prescribed pro-actively. Or, check could
inserted protocol run-time analysisthe agent would simulate other's
hypotheses matching actions, detect risky points dynamically.

6. Using Socially-Attentive Monitoring O-Line Conguration
demonstrate generality socially-attentive monitoring framework,
section examines re-use teamwork monitoring domains diagnosis recovery
every failure infeasible execution. Examples domains include team
sports, military human team training (Volpe, Cannon-Bowers, & Salas, 1996),
multi-agent domains.

dynamic nature domain, hard real-time deadlines,

complexity agents involved (e.g., human team members) make diagnosis recovery
dicult.

Even failure diagnosed, often late eective recovery.



environments, monitoring agent often concerned trends performance.
information important long-term design evaluation analysis, need
necessarily calculated on-line. results analysis meant feedback
agents' designer (coach supervisor, humans).
end, developing o-line socially-attentive monitoring system called
Teamore (TEAmwork MOnitoring REview). Teamore currently uses execution traces

monitored agents perform monitoring, rather using plan recognition. Thus
need worry uncertainty plan-recognition, real-time performance. Instead, knows certainty agent's plans execution. Teamore
accumulates several quantitative measures related teamwork, including Average-Timeto-Agreement measure (ATA, short), measure level agreement team.
build failure detection algorithm, aggregate failures quantitatively.
focus ATA measure.

129

fiKaminka Tambe

Teamore denes

switch

time interval beginning point team-

member (at least one) selects new team plan execution team, ending
point team agreement team-plan executed.



perfect teamwork, team-members select new team-plan jointly, always remain
agreement.

realistic scenario, agents take longer switch,

initially teamwork failure occur. rst team-member select new plan
disagreement teammates, either rejoins executing
original plan, join selecting new plan. switch begins detected
failure ends failures detected.
Figure 6 shows illustration switch. three agents begin initial state
agreement joint execution Plan 1 (lled line). Agent 1 rst agent switch
Plan 2 (dotted line), followed Agent 3, nally Agent 2.

switch

interval begins instance Agents 1 selected Plan 2, time three agents
regained agreement (but time Plan 2).

Switch
Legend:
Plan 1

Agent 3

Plan 2
Agent 2
Agent 1
Time
Figure 6: illustration switch. three agents switch plan 1 plan 2.
Teamore keeps track lengths time failures detected

resolved. ATA measure average switch length (in time ticks) per complete
team run (e.g., mission ModSAF, game RoboCup). perfect team would
switches length zero, therefore ATA 0. worst team would one
beginning task execution end it, would agree
team plan executed. instance, RoboCup game lasts 6000 ticks.
worst possible team would one switch game, length 6000. Thus
ATA scale RoboCup goes 0 (perfect) 6000 (worst).
used ATA measure analyze series games two RoboCup simulationleague teams, ISIS'97 ISIS'98 (Marsella et al., 1999) xed opponent, Andhill'97
(Andou, 1998).

games, varied use communications teams

evaluate design decisions use communications. approximately half games,
players allowed use communications service teamwork. half,
communications agents disabled. ISIS'97 played approximately 15 games
settings, ISIS'98 played 30 games communication settings.
Table 9 shows mean ATA values games, two sub-teams (each
three members) ISIS'97 ISIS'98 (ATA values calculated separately subteam). rst column shows sub-team results refer row. second

130

fiRobust Agent Teams via Socially-Attentive Monitoring

columns shows mean ATA sub-team, communications used.
third column shows mean ATA communications used. next column shows
size ATA reductionthe drop mean ATA values communications
introduced.

last column shows probability null hypothesis two-tailed

t-test dierence ATA means. probability dierence due
chance, thus smaller numbers indicate greater signicance.

ISIS

Mean ATA

Mean ATA

ATA

t-test prob.

sub-team

comm.

Comm.

Reduction

null-hypothesis

32.80

5.79

27.01

7.13e-13

'97 Goalies
'97 Defenders

57.5

6.81

50.69

.45e-10

'98 Goalies

13.28

3.65

9.63

9.26e-16

'98 Defenders

12.99

3.98

9.01

7.13e-5

Table 9: Average-Time-to-Agreement (ATA) games Andhill'97.

Clearly, signicant dierence emerges communicating noncommunicating versions sub-team.

ATA values indicate sharing infor-

mation way communications signicantly decreases time takes team-members
come agreement selected plan. result agrees intuitions
role communications, sense, may surprising.
However, ATA reduction magnitudes indicate ISIS'98 may much less sensitive loss communications ISIS'97. dierences ATA values ISIS'97
approximately triple, nearly four times, great ISIS'98.

explanation

phenomenon ISIS'98 composed players improved capabilities monitoring environment (such better knowledge environment).

ISIS'98

therefore dependent communications teams, ISIS'97, composed
players lesser environment monitoring capabilities. ISIS'98 players better able
select correct plan without relying teammates.

Thus, would able

maintain level performance communications used. contrast,
ISIS'97 players rely passing information (monitoring other)
communications, took much longer establish agreement communications available.
validate hypothesis suggested ATA measurements looking overall
team-performance games, measured score dierence end game.
Table 10 shows mean score dierence series games Andhill'97.
rst column lists communications settings (with without).
third columns show

mean

second

score-dierence games ISIS'97 ISIS'98.



bottom row summarizes results t-tests run set games, determine
signicance level dierence mean score-dierences. score-dierence
results corroborate ATA results. dierence mean score-dierence indeed
statistically signicant ISIS'97 games, signicant ISIS'98 games. supports
explanation situationally aware ISIS'98 indeed better able handle
loss communications ISIS'97.

131

fiKaminka Tambe

ISIS'97

ISIS'98

Communication Used

-3.38

-1.53

Communication Used

-4.36

-2.13

t-test p/null hypothesis

p=0.032

p=0.13

Table 10: ISIS'97 ISIS'98 mean score dierence Andhill'97, changing communications settings

general lesson emerging experiments trade-o exists addressing monitoring selectivity problem. knowledge maintained teammates
(here, via communications) traded, extent, knowledge maintained
environment.

designer therefore range alternative capabilities

choose agents. Dierent domains may better facilitate implicit coordination monitoring environment, others require agents rely communications explicit
knowledge team-members handle coordination.
ATA results support additional conclusions, especially combined general
performance measure score-dierence.

illustrate, consider plots

actual data games. Figure 7 plots ATA values four variants,
Goalies sub-team.

graph plots approximately 60 data-points.

see Figure

7 communications used, ISIS'97's ATA values still generally better
ISIS'98's ATA without communications. Thus, despite importance, individual situational
awareness able fully compensate lack communications.

Average Time Agreement (ATA)

90
80
70
60
50

40
30
20
10
0
ISIS98/Comm.

ISIS97/Comm.

ISIS98/No-Comm. ISIS97/No-Comm.

Goalies Sub-Team
(a) ATA Values Goalies subteam

Figure 7: ATA values Goalies sub-teams games Andhill'97.

132

fiRobust Agent Teams via Socially-Attentive Monitoring

Teamore demonstrates reuse teamwork monitoring techniques developed

earlier sections o-line conguration. designer ISIS'97 set agents
use communications, since signicant improvement score-dierence.
contrast, without communications, ISIS'98 players able maintain
collaboration. Thus communications takes precious resources, relatively safely
eliminated ISIS'98 agents' design, development eorts directed
components agents.

7. Beyond Teamwork
presented general socially-attentive monitoring framework detect failures
maintaining agreement joint team plans.

However, eective operation teams often

relies additional relationships, briey address section.

7.1 Richer Agreement Model: Agreeing Disagree
teamwork model requires joint execution team plans. service agreed-upon
joint plans, agents may sometimes agree execute dierent sub-plans individually, split
sub-teams execute dierent sub-team plans. Two examples may serve illustrate.

Example 3.

ModSAF domain, helicopters engage enemy repeatedly following

masking ), popping (unmask-

following three steps: hiding behind hill trees (

ing ),

shooting missiles enemy, back hiding. variations

plan, required make sure two helicopters shooting time.
course, due limits communications, helicopters fail unmask time.

Example 4.

RoboCup domain, 11 players ISIS'97 ISIS'98 (Marsella

et al., 1999) divided four sub-teams: mid-elders, attackers, defenders, goalies
(the goalie two close defenders). division sub-teams modeled agents

play team plan (see Figure 8). Mid-elders
must select midfield plan, goalies must select defend-goal plan, etc. Again, ideally
attacker would never select plan attack, defender would select
plan defend, etc. However, due communication failures, players may sometimes
selecting one four team plans service

accidently abandon intended sub-team, execute team-plan another sub-team.
[WinGame]
[Play]

[Attack]
[Simple
Advance]
.....
Scoregoal

[Flank
Attack]
...
Pass

[Interrupt]
...

[Defend]
......

[Midfield]
...........

[DefendGoal]

[Careful
defense]
Intercept

kickout

[Simplegoal
defense]
.....
Reposition

Figure 8: Portion plan-hierarchy used ISIS RoboCup agents.

examples, certain dierences agents agreed upon
sign correct execution, failure. Indeed, lack dierence selected plans

133

fiKaminka Tambe

would indicate failure cases. use term

mutual-exclusion coordination

refer relationships. Example 3, ideally two pilots executing
plan time.



shooting

Example 4, two members dierent sub-teams (e.g.,

attacker defender) executing plan service

play (e.g., defend).



examples demonstrate, clear need monitoring mutual-exclusion coordination.
results previous sections re-used service socially-attentive monitoring
mutual-exclusion relationships. require transformation implementation
theory. hierarchies compared usual manner, except failures signied
equalities, rather dierences. instance, attacker staying team's
half eld, teammates may come suspect mistakenly defected
attackers' sub-team believes defender.
analytical results inverted well. maximal team-coherence heuristic
lead completeness, since prefers hypotheses contain equalities among agents,
failures mutual-exclusion coordination. maximal team-incoherence heuristic lead sound detection, prefers hypotheses imply equalities
occurred. properties proven formally.

Theorem 5. Let monitoring agent

monitor mutual-exclusion relationships group
agents G. A's modeling G complete, uses maximally team-incoherent
hypothesis detection, failure detection results sound.

Proof.

Provided appendix A.

Theorem 6. Let monitoring agent

monitor mutual-exclusion relationships group
agents G. A's modeling G complete, uses maximally team-coherent
hypothesis detection, failure detection results complete.
Proof.



Provided appendix A.

Thus mutual-exclusion relationships, teamwork relationships, guaranteed failuredetection results may still provided despite use limited, uncertain knowledge
monitored agents. centralized teamwork monitoring algorithms easily transformed monitoring mutual-exclusion relationships. Unfortunately, results
distributed case (Theorem 4) cannot easily transformed, since rely
property observable-partitioning, associated dierences, equalities.
leave issue future work.

7.2 Monitoring Using Role-Similarity Relationships
section applies socially-attentive monitoring role-similarity relationships, monitoring individual performance within teams. particular, service team-plans agents
may select individual sub-plans, necessitate agreement team-members,
constrained agents' roles.

fly-flight-plan

instance, service executing team-plan

(Figure 3) pilots individually select individual plans set

velocity heading within constraints formation ight method specied
mission.
Role similarity relationships specify ways given individual plans similar,
extent.

Two agents role executing dissimilar plans

134

fiRobust Agent Teams via Socially-Attentive Monitoring

considered violation role-similarity relationships. enables sociallyattentive monitoring system detect failure role-execution. monitor individual plans
agent executing, compares selection agents

role,

similarly method used teamwork. plans considered similar
role-similarity relationship model, failure detected.

Otherwise, failure may

occurred, diagnosis component called verify provide explanation.
Let us illustrate failure ModSAF domain system able
detect using role-similarity relationship:

Example 5.

team three helicopters take base head

mission. However, one pilot agents failed correctly process mission statement.
therefore kept helicopter hovering base, teammates left execute
mission themselves.
failures detected using role-similarity relationship monitoring. agreed-upon
team-plan selected agents, problem teamwork relationship
detected. team-plan involved agent selecting individual methods ight,
determine altitude velocity.

agents diered.

failing helicopter

remained hovering, teammates moved forward. Using role similarity relationship,
failing helicopter compared selected plan teammate (who shared
role subordinate formation), realized plans dissimilar enough
announce possible failure.
Unfortunately, actual similarity metrics seem domain- task-specic,
thus easy re-use across domains. Furthermore, detected failures necessarily real failures, detected failures weight.

currently

investigating ways address challenging issues.

8. Related Work
investigation socially-attentive monitoring, relationship knowledge
maintained agents' states monitoring eectiveness builds research dierent subelds multi-agent systems. address sub-elds section, explain
investigation related existing literature.

8.1 Related Work Teamwork
Previous work teamwork recognized monitoring agents critical teams.
Past investigations raised monitoring selectivity problem, addressed
depth. Building upon investigations, paper begins provide in-depth
answers problem.
theory SharedPlans (Grosz & Kraus, 1996, 1999) touches teamwork monitoring selectivity problem several ways, provides initial answers. First,
theory requires agents know teammates capable carrying tasks
team.

authors note agents must communicate enough plans

convince teammates ability carry actions (Grosz & Kraus, 1996,
p. 314). Second, theory requires agents mutual-belief shared recipe,

135

fiKaminka Tambe

state requires agents reason innite recursion agent's beliefs.

Un-

fortunately, attainment mutual belief undecidable theory (Halpern & Moses, 1990)
hence must approximated practice (Jennings, 1995; Rich & Sidner, 1997).
approximations may still impose strong monitoring requirements. Third, theory introduces
intention-that construct service coordination helpful behavior, implying monitoring others' progress assess need behavior (Grosz & Kraus, 1996,
Axiom A5-A7).

Fourth, SharedPlans requires intentions agent must con-

ict (Grosz & Kraus, 1996, Axiom A1), since intentions (in particular,
intentions-that) may involve attitudes agents, monitoring others
detect avoid conicts implied. authors point theoretically
conicts detected, infeasible practice (Grosz & Kraus, 1996, p. 307).
suggest conict detection prevention investigated problem-specic manner
within minimal constraints (i.e., monitoring capabilities, mutual-belief, progress, lack
conicts) provided SharedPlans framework (p. 308 314).
Joint-Intentions (Levesque et al., 1990; Cohen & Levesque, 1991) requires agent
privately comes believe joint-goal either achieved, unachievable, irrelevant,
must commit entire team mutually believe case. theory
SharedPlans, Joint-Intentions' use mutual belief approximated practice,
imposes strong monitoring requirements. Thus, monitoring selectivity problem
raised practical implementations Joint-Intentions.
Jennings hypothesized two central constructs cooperative multi-agent coordination

commitments

made agents,

conventions, rules used monitor

commitments (Jennings, 1993). conventions used decide information needs
monitored agents, monitored. instance, convention may
require agent report teammates changes privately detects respect
attainability team goal.

Jennings raises monitoring selectivity problem

provides example specic conventions high- low-bandwidth situations
knowledge communicated agents bandwidth available.
However, Jennings explore in-depth question conventions selected, trade-os guarantees associated selection particular
conventions. instance, guarantees eects using low-bandwidth
convention example.
theoretical investigations described raise monitoring selectivity problem (implicitly explicitly). work builds upon address problem depth,
context socially-attentive monitoring teams. paper reports soundness
and/or completeness properties teamwork relationship failure-detection analytically guaranteed, despite uncertainty knowledge acquired monitored agents.
analytical guarantees applicable plan-recognition communications,
corroborated empirical results.
Building theoretical work, practical teamwork systems include (Jennings, 1995; Rich
& Sidner, 1997) (Tambe, 1997).

Jennings' investigation Joint-Responsibility

teamwork model GRATE* (Jennings, 1995) builds Joint-Intentions, similarly
implementation, requires agents agree team-plans execute.
However, GRATE* used industrial settings foolproof communications
assumed (Jennings, 1995, p.

211), thus passive monitoring (via communica-

136

fiRobust Agent Teams via Socially-Attentive Monitoring

tions) used. Although Jennings provides evaluation GRATE*'s performance
respect communication delays, guarantees provided respect failure detection. GRATE* maintains knowledge agents acquaintances models,
used keep track team-members' capabilities (in service forming
teams). However, question much knowledge used models
left unaddressed.
Rich Sidner investigate COLLAGEN collaborative user-interface system,
communications reliable (Rich & Sidner, 1997). However, human-usability
perspective, limiting amount communications still desirable.

address is-

sue, recent empirical work Lesh, Sidner Rich (1999) utilizes plan recognition
COLLAGEN; focus work using collaborative settings make
plan-recognition tractable.

instance, ambiguities plan-recognition may resolved

asking user clarication. Work COLLAGEN investigate much
knowledge maintained eective collaborative dialogue user. contrast,
able provide guarantees failure-detection results algorithms. Also,
analysizing dialogue plans

risky points

may allow systems COLLAGEN

decide whether use communications clarication regardless plan-recognition ambiguity.
STEAM (Tambe, 1997) maintains limited information ability team-members
carry roles. STEAM allows team-members reason explicitly
cost communication deciding whether communicate not. work signicantly
extends capabilities via plan-recognition, provides analytically-guaranteed faultdetection results. Furthermore, teamwork failure-detection capabilities useful
trigger STEAM's re-planning capabilities.

8.2 Related Work Coordination
Huber (1995) investigated use probabilistic plan-recognition service active teamwork monitoring, motivated unreliability costs passive communications-based
monitoring military applications. Washington explores observation-based coordination using Markov Models (Washington, 1998), focusing making computations tractable.
contrast Huber Washington, work focuses monitoring selectivity problem.
showed strengths limitations centralized distributed approaches guaranteed failure-detection results using coherence-based disambiguation plan-recognition
hypotheses.
Durfee (1995) discusses various methods reducing amount knowledge agents
need consider coordinating others. methods discussed involve pruning parts
nested models, using communications, using hierarchies abstractions, etc.
focus work methods modeling limited, focus
work question much modeling required guaranteed performancethe
monitoring selectivity problem. provide analytical guarantees trade-os involved
using limited knowledge agents failure-detection purposes.
Sugawara Lesser (1998) report use comparative reasoning/analysis techniques service learning specializing coordination rules system distributed agents coordinate diagnosing faulty network. investigation focused

137

fiKaminka Tambe

optimizing coordination rules minimize ineciency redundancy agent's coordinating messages. Upon detecting sub-optimal coordination (via fault model), agents
exchange information local views system problem solving activity,
construct global view. compare local view global view nd
critical values/attributes missing local view therefore gave rise
sub-optimal performance problem. values attributes used constructing
situation-specic rules optimize coordination particular situations.

example,

network diagnosis agents may learn rule guides choose coordination strategy one agent performs diagnosis shares result rest
diagnosis agents. work socially-attentive monitoring similarly uses comparison
agents views drive monitoring process. However, use comparison
product relationship monitoring.

Sugawara Lesser's work

viewed letting agents incrementally optimize monitoring requirements,
results analytically explore level monitoring required eective failure-detection,
dierent congurations. teamwork monitoring technique addresses uncertainty
acquired information, construct global view attributes systemas
would extremely expensive. Instead, technique focuses triggering failure detection via contrasting plans, incrementally expanding search dierences
diagnosis process.
Robotics literature raised monitoring selectivity problem.

Parker (1993)

investigated monitoring selectivity problem dierent perspective, formationmaintenance task. empirically examined eects combining socially-attentive information (which referred local) knowledge team's goals, concludes
fault-tolerant strategy one agents monitor well
progress towards goals.

Kuniyoshi et al.

(Kuniyoshi, Rougeaux, Ishii, Kita, Sakane,

& Kakikura, 1994) present framework cooperation observations, robots
visually attend others prerequisite coordination. framework presents several
standard attentional templates, i.e., monitors whom. dene team attentional
structure one agents monitor other.

work focuses mon-

itoring selectivity problem within socially-attentive monitoring teamwork relationships,
provides analytical well empirical results. treat attentional templates
product relationships hold system. results show monitoring
teams may necessarily require monitoring team-members.

8.3 Related Work
Horling et al.

(Horling, Lesser, Vincent, Bazzan, & Xuan, 1999) present distributed

diagnosis system multi-agent intelligent home environment.The system uses faultmodels identify failures ineciencies components, guide recovery. Schroeder
Wagner (1997) proposed distributed diagnosis technique cooperating agents
receive requests tests diagnoses, send responses agents.



construct global diagnosis based local ones produce receivewith
assumption conicts occur.

Frohlich Nejdl (1996) investigates scheme

multiple diagnosis agents cooperate via blackboard architecture diagnosing
physical system. agents may use dierent diagnosis models systems, centralized

138

fiRobust Agent Teams via Socially-Attentive Monitoring

conict-resolution agent employed handle conicts diagnoses found. three
approaches address monitoring selectivity problem.
social measures related ATA. Goldberg Mataric (1997) in-

interference amount time robots
social entropy (Bailey, 1990) measure be-

vestigate multi-robot foraging task measure
spend avoiding other. Balch (1998) uses

havioral diversity

multi-agent tasks soccer, foraging, formation-maintenance.

investigations focus characterizing heterogeneity multi-agent systems relation
performance. contrast, focus work providing useful feedback
designer.

Possible correlation task performance ATA values remains

investigated.

9. Conclusions Future Work
work presented paper motivated practical concerns. begun
investigation monitoring selectivity problem result observation failures
continue occur despite agents' use monitoring conditions communications.
Analysis failures revealed agents suciently informed other's
state. need monitor one's teammates recognized repeatedly past
(Jennings, 1993; Grosz & Kraus, 1996; Tambe, 1997), monitoring selectivity problem
question much monitoring requiredremained largely unaddressed (Jennings,
1993; Grosz & Kraus, 1996).
provide key answers monitoring selectivity problem.

Within context

socially-attentive monitoring teams, demonstrate teamwork relationship failures
detected eectively even uncertain, limited, knowledge team-members' states.
show analytically centralized active teamwork monitoring provides failure-detection
either complete unsound, sound incomplete. However, centralized teamwork monitoring requires multiple hypotheses monitoring team-members.



contrast, distributed active teamwork monitoring results complete sound failuredetection, despite using simpler algorithm monitoring key agents team.
Using implemented general framework socially-attentive monitoring, empirically validate results ModSAF domain. provide initial results monitoring mutual-exclusion role-similarity relationships, initial diagnosis procedures.
demonstrate generality framework applying RoboCup
domain, show useful quantitative analysis generated o-line.
ModSAF RoboCup dynamic, complex, multi-agent domains involve many uncertainties perception action.
attempted demonstrate results techniques applied
domains.

explicitly pointed necessary conditions theorems hold,

observable-partitioning team-modeling completeness. presented diagnosis
algorithm sensitive accuracy knowledge used, may require assuming
plans recognized soon selected. conditions veried
designer target application domain. Reactive plans (our chosen representation)
commonly used many dynamic multi-agent domains. focus monitoring agreements
joint plans stems centrality similar notions agreement agent human
teamwork literature (Jennings, 1995; Grosz & Kraus, 1996; Volpe et al., 1996; Tambe, 1997).

139

fiKaminka Tambe

made several references additional areas would conduct
investigations.

One important topic plan investigate depth strong

requirements distributed teamwork monitoring algorithm terms observability.
order provide soundness completeness guarantees, distributed algorithm relies
ability team-members monitor key agents. investigating ways
relax requirement still providing guaranteed results. addition, diagnosis
procedures extended formalized, would investigate ways
alleviate sensitivity procedures choice team-modeling hypothesis.

Acknowledgments
article partially based AAAI-98 paper (Kaminka & Tambe, 1998),
Agents-99 paper (Kaminka & Tambe, 1999) authors. research supported part NSF Grant ISI-9711665, part AFOSR contract #F49620-97-10501. thank Je Rickel, George Bekey, Victor Lesser, Dan O'Leary, David Pynadath
many useful comments. anonymous reviewers thanks helping us crystallize ideas contributions revisions paper.

Appendix A. Proofs
Theorem. (# 2, page 123). Let monitoring agent monitor simple team . A's
team-modeling complete, uses maximally team-coherent hypothesis detection, teamwork failure detection results sound.
Proof.

show failure occurs detected, thus failures

detected. Let a1 ; : : : ; agent members . agent ai executing
plan Pi (1 n).

Thus collectively, group executing (P1 ; : : : ; Pn ).

failure

occurred, two agents ak ; aj ; 1 j; k n aj executing plan
Pj ak executing plan Pk Pj 6= Pk .

Since A's team-modeling complete,

correct hypothesis (P1 ; : : : ; Pj ; : : : ; Pk ; : : : Pn ) set team-modeling hypotheses.
Since choose maximally team-incoherent hypothesis, either choose correct
hypothesis, incoherent hypothesis implying failure occurred,
select hypothesis greater incoherence hypothesis (or equivalent level).
case, failure would detected, detection procedure complete.

Lemma. (#

1, page 127). Suppose simple team self-monitoring (all members
team monitor other) using maximally team-coherent heuristic (and
assumption agent, team-modeling complete). monitoring agent A1
member executing P1 would detect failure maintaining teamwork relationships
agent A2 (also member ) executing dierent plan P2 , A2 observably
dierent role P1 P2 .

Proof.

A1 knows executing P1 .

Since members monitor

themselves, A1 monitoring A2 , observably dierent role P1 P2 . Since A2
executing P2 , following observably dierent role, P1 2
= (A1 ; A2 =P2 ). Therefore
perspective A1 , cannot case assigns P1
hypothesis, therefore

team-modeling

agent-modeling

hypothesis A1 A1 executing

140

fiRobust Agent Teams via Socially-Attentive Monitoring

P1 , A2 executing plan P1 . words, A1 's perspective

team-coherent hypothesis, dierence would detected A1 A2 .

Theorem. (# 5, page 134). Let monitoring agent monitor mutual-exclusion relation-

ships group agents G. A's modeling G complete, uses maximally
team-incoherent hypothesis detection, failure detection results sound.
Proof.

show failure occurred, none detected, thus

failure detected fact failure.
G.

Let a1 ; : : : ; agent members

agent ai executing plan Pi (1 n). Thus collectively, group

executing (P1 ; : : : ; Pn ). failure occurred, agent executing dierent
plan (i 6= j ) Pi 6= Pj ). Since A's group-modeling complete, correct hypothesis
going set group-modeling hypotheses H . Since maximally incoherent
hypothesis, either selected, dierent hypothesis

level

selected.

coherence

hypothesis coherence level correct one

implies failure detected. Thus detection procedure sound.

Theorem. (# 6, page 134). Let monitoring agent monitor mutual-exclusion relation-

ships group agents G. A's modeling G complete, uses maximally
team-coherent hypothesis detection, failure detection results complete.
Proof.

show failure occurs detected, thus procedure

complete. Let a1 ; : : : ; agent members G. agent ai executing
plan Pi (1 n).

Thus collectively, group executing (P1 ; : : : ; Pn ).

failure

occurred, two agents ak ; aj ; 1 j; k n aj executing plan
Pj ak executing plan Pk Pj

=

Pk .

Since A's group-modeling complete,

correct hypothesis (P1 ; : : : ; Pj ; : : : ; Pk ; : : : Pn ) set group-modeling hypotheses.
Since choose maximally team-coherent hypothesis, either choose correct
hypothesis, coherent hypothesis implying failure occurred,
select hypothesis greater coherence hypothesis (or equivalent level).
case, failure would detected. Therefore, detection procedure complete.

Appendix B. Socially-Attentive Monitoring Algorithms
bring algorithms (in pseudo-code) RESL plan-recognition algorithm,
comparison test supporting detection simple non-simple teams,
monitoring algorithms centralized distributed cases.

B.1 RESL
RESL works rst expanding complete operator hierarchy agents modeled, tagging plans non-matching. plans' preconditions termination conditions
agged non-matching well. plans' actions set used expectations
behavior. initializing plan-recognition hierarchy monitored agent, observations agent continuously matched actions expected plans.
Plans whose expectations match observations tagged matching, ags
propagated along hierarchy, down, complete paths hierarchy

141

fiKaminka Tambe

agged matching not. paths specify possible matching interpretations
observations. addition, precondition termination conditions agged true
not, signifying inferred appropriate belief modeled agents.

process

described Algorithm 1.

Algorithm 1 RESL's main loop, matching

observation making inferences given

plan-recognition hierarchy (a single agent).
1.

Get observations agent

2.

plan set expected observations:
(a) Compare observations expectations
(b) succeed, ag plan matching successfully, otherwise ag plan failing match

3.

plan agged matching successfully
(a) Flag parents matching successfully // propagate matching

4.

plan whose children (all them) agged failing match
(a) Flag failing match // propagate non-matching

B.2 Detection Failure, Centralized Distributed Teamwork Monitoring
Algorithm 2 shows comparison hierarchical plans carried out. limit
simple-teams. algorithm accepts input two sets hierarchical plan hypotheses, two associated agents (for clarity, algorithms assume two agents.
generalization n agents straightforward). algorithm accepts policy ag,

Policy.



OPTIMISTIC

policy causes algorithm use maximal team-coherence

provide sound, incomplete detection.

PESSIMISTIC policy causes algorithm use

maximal team-incoherence provide complete, unsound detection.

hierarchy_1 hierarchy_2. two agents
agent_2. algorithm makes use predicate Sub-team,
true two agents (Agent1, Agent2) belong dierent sub-teams given
level hierarchy (Depth).
set hierarchical plans marked

marked

agent_1



aid Algorithm 2, dene centralized distributed failure detection algorithms.

centralized teamwork monitoring algorithm (Algorithm 3)

utilizes Algorithm 2 twice, checking failures

PESSIMISTIC



OPTIMISTIC

policies. results policies agree, certain. results agree,
(i.e.,

PESSIMISTIC

policy causes failure detected,

OPTIMISTIC

policy

causes failure detected), monitoring agent cannot certain failure
taken place, therefore needs verify failure.

Algorithm 3 therefore returns

FAILURE, NO_FAILURE, POSSIBLE_FAILURE.
distributed monitoring algorithm given pseudo-code form,
nothing call Algorithm 2

142

OPTIMISTIC

policy parameter. power

fiRobust Agent Teams via Socially-Attentive Monitoring

Algorithm 2 Hierarchical comparison two agents, allowing sub-teams.
1.

Set Depth 0 //

2.

plans depth Depth team-plans
(a)

look top-most dierence rst

Policy == OPTIMISTIC
i. Let Plan_1, Plan_2 maximally team-coherent plans level Depth
hierarchy_1 hierarchy_2, respectively.
ii. else Let Plan_1, Plan_2 maximally team-incoherent plans level Depth
hierarchy_1 hierarchy_2, respectively.

(b)

Plan_1 equal Plan_2
i. return FAILURE
ii. else bottom hierarchies reached, return NO_FAILURE, otherwise increase Depth go 2.

3.

one plan team-plan, return FAILURE, else return NO_FAILURE.

Algorithm 3 Centralized Teamwork Monitoring, applying optimistic pessimistic
views.
1.

2.

Let Optimistic_Result = Detect(agent_1, agent_2, hierarchies_1,
hierarchies_2, OPTIMISTIC)
/* algorithm 2 */
Let Pessimistic_Result = Detect(agent_1, agent_2, hierarchies_1,
hierarchies_2, PESSIMISTIC)
/* algorithm 2 */

3.

Optimistic_Result == Pessimistic_Result

4.

return Optimistic_Result /*

5.

else return POSSIBLE_FAILURE

either

143

FAILURE,



NO_FAILURE */

fiKaminka Tambe

derived fact members team using monitor key agents
team.

References
Ambros-Ingerson, J. A., & Steel, S. (1988). Intergrating planning, execution monitoring.


Proceedings Seventh National Conference Articial Intelligence (AAAI-88)

Minneapolis/St. Paul, MN. AAAI Press.
Andou, T. (1998). Renement soccer agents' positions using reinforcement learning.
Kitano, H. (Ed.),

RoboCup-97: Robot soccer world cup 1, Vol. LNAI 1395, pp. 373388.

Springer-verlag.
Atkins, E. M., Durfee, E. H., & Shin, K. G. (1997). Detecting reacting unplanned-

Proceedings Fourteenth National Conference Articial
Intelligence (AAAI-97), pp. 571576 Providence, RI. AAAI Press.
world states.

Bailey, K. D. (1990).
Balch, T. (1998).

Social Entropy Theory.

State University New York Press.

Behavioral Diversity Learning Robot Teams.

Ph.D. thesis, Georgia

Institute Technology.
Calder, R. B., Smith, J. E., Courtemanche, A. J., Mar, J. M. F., & Ceranowicz, A. Z. (1993).

Modsaf behavior simulation control. Proceedings Third Conference
Computer Generated Forces Behavioral Reresentation Orlando, Florida. Institute
Simulation Training, University Central Florida.

Cohen, P. R., Amant, R. S., & Hart, D. M. (1992).

Early warnings plan failure, false

positives, envelopes: Experiments model.

Tech. rep. CMPSCI Technical

Report 92-20, University Massachusetts.
Cohen, P. R., & Levesque, H. J. (1991). Teamwork.

Nous, 35.

Doyle, R. J., Atkinson, D. J., & Doshi, R. S. (1986). Generating perception requests
expectations verify execution plans.



Conference Articial Intelligence (AAAI-86).
Durfee, E. H. (1995).

Blissful ignorance:

Proceedings Fifth National

Knowing enough coordinate well.



Proceedings First International Conference Multiagent Systems (ICMAS-95),
pp. 406413.

Fenster, M., Kraus, S., & Rosenschein, J. S. (1995).

Coordination without communica-

Proceedings First
International Conference Multiagent Systems (ICMAS-95), pp. 102108 California,
tion: Experimental validation focal point techniques.
USA.

Firby, R. J. (1987). investigation reactive planning complex domains.

ceedings Sixth National Conference Articial Intelligence (AAAI-87).
144

Pro-

fiRobust Agent Teams via Socially-Attentive Monitoring

Frohlich, P., & Nejdl, W. (1996). Resolving conicts distributed diagnosis. Wahlster,
W. (Ed.),

12th Europeach Conference Articial Intelligence (ECAI-96).

John

Wiley & Sons, Inc.
Goldberg, D., & Mataric, M. J. (1997).

Interference tool designing evaluat-

Proceedings Fourteenth National Conference
Articial Intelligence (AAAI-97), pp. 637642 Providence, RI. AAAI Press.
ing multi-robot controllers.

Grosz, B. J., & Kraus, S. (1999). evolution sharedplans. Wooldridge, M., & Rao,
A. (Eds.),

Foundations Theories Rational Agency, pp. 227262.

Grosz, B. J., & Kraus, S. (1996). Collaborative plans complex group actions.

Intelligence, 86, 269358.

Articial

Grosz, B. J., & Sidner, C. L. (1990). Plans discourse. Cohen, P. R., Morgan, J., &
Pollack, M. (Eds.),

Intentions Communication, pp. 417445. MIT Press, Cambridge,

MA.
Halpern, J. Y., & Moses, Y. (1990). Knowledge common knowledge distributed
environment.

distributed computing, 37 (3), 549587.

Hamscher, W., Console, L., & de Kleer, J. (Eds.). (1992).

nosis.

Readings Model-Based Diag-

Morgan Kaufmann Publishers, San Mateo, CA.

Horling, B., Lesser, V. R., Vincent, R., Bazzan, A., & Xuan, P. (1999).

Diagnosis

integral part multi-agent adaptability. Tech. rep. CMPSCI Technical Report 199903, University Massachusetts/Amherst.
Huber, M. J., & Durfee, E. H. (1995).

acting together: Without communication.



Working Notes AAAI Spring Symposium Representing Mental States
Mechanisms, pp. 6071 Stanford, CA.

Jennings, N. R. (1993). Commitments conventions: foundations coordination
multi-agent systems.

Knowledge Engineering Review, 8 (3), 223250.

Jennings, N. R. (1995). Controlling cooperative problem solving industrial multi-agent
systems using joint intentions.

Articial Intelligence, 75 (2), 195240.

Johnson, W. L., & Rickel, J. (1997). STEVE: animated pedagogical agent procedural
training virtual environments.

SIGART Bulletin, 8 (1-4), 1621.

Kaminka, G. A., & Tambe, M. (1998). What's wrong us? Improving robustness

Proceedings Fifteenth National Conference Articial
Intelligence (AAAI-98), pp. 97104 Madison, WI. AAAI Press.
social diagnosis.

Kaminka, G. A., & Tambe, M. (1999).

I'm OK, You're OK, We're OK: Experiments

distributed centralized social monitoring diagnosis.



Proceedings

Third International Conference Autonomous Agents (Agents-99) Seattle, WA. ACM
Press.

145

fiKaminka Tambe

Kinny, D., Ljungberg, M., Rao, A., Sonenberg, E., Tidhar, G., & Werner, E. (1992). Planned
team activity.

Castelfranchi, C., & Werner, E. (Eds.),

Articial Social Systems,

Lecture notes AI 830, pp. 227256. Springer Verlag, New York.

Kitano, H., Tambe, M., Stone, P., Veloso, M., Coradeschi, S., Osawa, E., Matsubara, H.,

Proceedings International Joint Conference Articial Intelligence (IJCAI-97)

Noda, I., & Asada, M. (1997). RoboCup synthetic agent challenge '97.
Nagoya, Japan.

Kraus, S., Sycara, K., & Evenchik, A. (1998). Reacing agreements negotiations:
logical model implementation.

articial intelligence, 104 (1-2), 169.

Kuniyoshi, Y., Rougeaux, S., Ishii, M., Kita, N., Sakane, S., & Kakikura, M. (1994). Cooperation observation framework basic task patterns.

International Conference Robotics Automation,

IEEE

pp. 767773 San-Diego, CA.

IEEE Computer Society Press.
Lesh, N., Rich, C., & Sidner, C. L. (1999). Using plan recognition human-computer col-

Proceedings Seventh International Conference User Modelling
(UM-99) Ban, Canada.
laboration.

Levesque, H. J., Cohen, P. R., & Nunes, J. H. T. (1990). acting together.

Eigth National Conference Articial Intelligence (AAAI-90)

Proceedings

Menlo-Park,

CA. AAAI Press.
Malone, T. W., & Crowston, K. (1991).
tion.

Toward interdisciplinary theory coordina-

Tech. rep. CCS TR#120 SS WP# 3294-91-MSA, Massachusetts Institute

Technology.
Marsella, S. C., Adibi, J., Al-Onaizan, Y., Kaminka, G. A., Muslea, I., Tallis, M., & Tambe,
M. (1999).

teammate:

Experiences acquired design robocup

Proceedings Third International Conference Autonomous Agents
(Agents-99) Seattle, WA. ACM Press.

teams..

Newell, A. (1990).

Unied Theories Cognition.

Harvard University Press, Cambridge,

Massachusetts.

Proceedings
IEEE Robotics Automation Conference, pp. 582587 Atlanta, GA.

Parker, L. E. (1993). Designing control laws cooperative agent teams.

Rao, A. S. (1994). Means-end plan recognition towards theory reactive recognition.

Proceedings International Conference Knowledge Representation Reasoning (KR-94), pp. 497508.

Reece, G. A., & Tate, A. (1994). Synthesizing protection monitors causal structure.


Proceedings Articial Intelligence Planning Systems (AIPS-94) Chicago, IL.

Rich, C., & Sidner, C. L. (1997).

COLLAGEN: agents collaborate people.

Johnson, W. L. (Ed.), Proceedings First International Conference Autonomous Agents (Agents-97), pp. 284291 Marina del Rey, CA. ACM Press.

146

fiRobust Agent Teams via Socially-Attentive Monitoring

Proceedings
First International Conference Autonomous Agents (Agents-97), pp. 268275

Schroeder, M., & Wagner, G. (1997). Distributed diagnosis vivid agents.
Marina del Rey, CA. ACM Press.

Sugawara, T., & Lesser, V. R. (1998). Learning improve coordinated actions cooperative
distributed problem-solving environments.

Machine Learning, 33 (2/3), 129153.

Tambe, M. (1996). Tracking dynamic team activity.

ence Articial Intelligence (AAAI).

Tambe, M. (1997). Towards exible teamwork.

7, 83124.

Proceedings National Confer-

Journal Articial Intelligence Research,

Tambe, M., Johnson, W. L., Jones, R., Koss, F., Laird, J. E., Rosenbloom, P. S., & Schwamb,
K. (1995).

16 (1).

Intelligent agents interactive simulation environments.

AI Magazine,

Proceedings
Fourteenth National Conference Articial Intelligence (AAAI-97), pp. 39 Provi-

Toyama, K., & Hager, G. D. (1997). rst don't succeed....
dence, RI.

Veloso, M., Pollack, M. E., & Cox, M. T. (1998). Rationale-based monitoring planning
dynamic environments.

(AIPS-98) Pittsburgh, PA.

Proceedings Articial Intelligence Planning Systems

Volpe, C. E., Cannon-Bowers, J. A., & Salas, E. (1996). impact cross-training
team functioning: empirical investigation.

human factors, 38 (1), 87100.

Markov tracking agent coordination. Proceedings
Second International Conference Autonomous Agents (Agents-98), pp. 7077 Min-

Washington, R. (1998).

neapolis/St. Paul, MN. ACM Press.

147


