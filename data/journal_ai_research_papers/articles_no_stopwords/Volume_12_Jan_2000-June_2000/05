journal artificial intelligence

submitted published

model inductive bias learning
jonathan baxter

j onathan baxter anu edu au

school information sciences engineering
australian national university canberra australia

abstract
major machine learning inductive bias choose learners hypothesis space large enough contain solution learnt yet small
enough ensure reliable generalization reasonably sized training sets typically bias
supplied hand skill insights experts model automatically
learning bias investigated central assumption model learner embedded
within environment related learning tasks within environment learner sample
multiple tasks hence search hypothesis space contains good solutions
many environment certain restrictions set hypothesis
spaces available learner hypothesis space performs well sufficiently
large number training tasks perform well learning novel tasks environment explicit bounds derived demonstrating learning multiple tasks within
environment related tasks potentially give much better generalization learning single
task

introduction
often hardest machine learning task initial choice hypothesis space
large enough contain solution hand yet small enough ensure
good generalization small number examples mitchell suitable bias
found actual learning task often straightforward existing methods bias generally
require input human expert form heuristics domain knowledge example
selection appropriate set features despite successes methods
clearly limited accuracy reliability experts knowledge extent
knowledge transferred learner thus natural search methods
automatically learning bias
introduce analyze formal model bias learning builds upon
pac model machine learning variants vapnik valiant blumer
ehrenfeucht haussler warmuth haussler typically take
training data
following general form learner supplied hypothesis space
drawn independently according underlying distribution

information contained learners goal select hypothesis
minimizing measure
expected loss respect example case squared loss
learners
bias represented choice contain good solution
regardless much data learner receives cannot learn
course best way bias learner supply containing single optimal hypothesis finding hypothesis precisely original learning



fifi



















c ai access foundation morgan kaufmann publishers rights reserved





fibaxter

pac model distinction bias learning ordinary learning put differently
pac model model process inductive bias simply takes hypothesis space
given proceeds overcome assume instead
faced single learning task learner embedded within environment
related learning tasks learner supplied family hypothesis spaces

appropriate entire environment
goal bias e hypothesis space
simple example handwritten character recognition preprocessing stage
identifies removes small rotations dilations translations image character
advantageous recognizing characters set individual character recognition
viewed environment learning set
form distinguish characters distinguish b characters
preprocessor represents bias appropriate environment
likely many currently unknown biases appropriate
environment would able learn automatically

dc

b

many examples learning viewed belonging environments related example individual face recognition belongs
essentially infinite set related learning individual face recognition set individual spoken word recognition forms another large environment
set fingerprint recognition printed chinese japanese character recognition stock price prediction even medical diagnostic prognostic
multitude diseases predicted pathology tests constitute
environment related learning
many cases environments normally modeled instead treated
single multiple category learning example recognizing group faces would
normally viewed single learning multiple class labels one face
group multiple individual learning however reliable classifier
individual face group constructed easily combined produce
classifier whole group furthermore viewing faces environment related
learning presented bias learnt good
learning novel faces claim cannot made traditional
point goes heart model concerned adjusting learners
bias performs better fixed set learning process fact
ordinary learning richer hypothesis space components labelled bias
able varied instead suppose learner faced potentially infinite stream
tasks adjusting bias subset tasks improves learning performance
future yet unseen tasks
bias appropriate environment must learnt sampling
many tasks single task learnt bias extracted likely specific
task rest general theory bias learning developed upon idea
learning multiple related tasks loosely speaking formal stated section
two main conclusions theory presented

e

learning multiple related tasks reduces sampling burden required good generalization
least number examples required per task basis


fia odel nductive b ias l earning

e

bias learnt sufficiently many training tasks likely good learning novel
tasks drawn environment

second point shows form meta generalization possible bias learning ordinarily say learner generalizes well seeing sufficiently many training examples
produces hypothesis high probability perform well future examples
task however bias learner generalizes well seeing sufficiently many training tasks produces hypothesis space high probability contains good solutions novel tasks another
term used process learning learn thrun pratt
main theorems stated agnostic setting
necessarily contain
hypothesis space solutions environment give improved
bounds realizable case sample complexity bounds appearing stated
terms combinatorial parameters related complexity set hypothesis spaces
available bias learner boolean learning pattern classification parameters
bias learning analogue vapnik chervonenkis dimension vapnik blumer et al

application general theory learning appropriate set neuralnetwork features environment related tasks formulated bias learning
case continuous neural network features able prove upper bounds number
training tasks number examples training task required ensure set features
works well training tasks high probability work well novel tasks drawn

environment upper bound number tasks scales
measure complexity possible feature sets available learner upper

number
bound number examples task scales
examples required learn task true set features correct bias already
known number tasks thus case see number related tasks
learnt increases number examples required task good generalization decays
minimum possible boolean neural network feature maps able matching
lower bound number examples required per task form





f jilkmgonpq

p

f hg
f jir

g

related work
large body previous algorithmic experimental work machine learning
statistics literature addressing inductive bias learning improving generalization
multiple task learning approaches seen special cases least
closely aligned model described others orthogonal without
completely exhaustive section present overview main contributions see thrun
pratt chapter comprehensive treatment

e

hierarchical bayes earliest approaches bias learning come hierarchical bayesian
methods statistics berger good gelman carlin stern rubim
contrast bayesian methodology present takes essentially empirical
process modeling bias learning however model mixture
hierarchical bayesian information theoretic ideas presented baxter
similar conclusions found empirical study showing utility
hierarchical bayes domain containing large number related tasks given
heskes


fie

baxter

e

early machine learning work rendell seshu tcheng vbms variable bias
management system introduced mechanism selecting amongst different learning
tackling learning stabb shift better bias utgoff another early scheme adjusting bias unlike vbms stabb
primarily focussed searching bias applicable large domains use
environment related tasks may interpreted environment
analogous tasks sense conclusions one task arrived analogy
sufficiently many tasks early discussion analogy context see russell particular observation analogous
sampling burden per task reduced
metric approaches metric used nearest neighbour classification vector
quantization determine nearest code book vector represents form inductive bias
model present extra assumptions tasks
environment specifically marginal input space distributions identical
differ conditional probabilities assign class labels shown
optimal metric distance measure use vector quantization onenearest neighbour classification baxter b baxter bartlett metric
learnt sampling subset tasks environment used
distance measure learning novel tasks drawn environment bounds
number tasks examples task required ensure good performance novel
tasks given baxter bartlett along experiment metric
successfully trained examples subset japanese characters used
fixed distance measure learning yet unseen characters
similar described thrun mitchell thrun
neural networks output trained match labels novel task simultaneously
forced match gradient derivative information generated distance metric
trained previous related tasks performance novel tasks improved substantially
use derivative information

e

e

note many adaptive metric techniques used machine learning
focus exclusively adjusting metric fixed set rather learning
metric suitable learning novel related tasks bias learning
feature learning learning internal representations adaptive metric techniques
many approaches feature learning focus adapting features fixed task
rather learning features used novel tasks one cases features
learnt subset tasks explicit aim novel tasks
intrator edelman low dimensional representation learnt set
multiple related image recognition tasks used successfully learn novel tasks
kind experiments reported baxter chapter baxter b
baxter bartlett nature
bias learning inductive logic programming ilp predicate invention refers process ilp whereby predicates thought useful classification task hand
added learners domain knowledge predicates background domain knowledge learning novel tasks predicate invention may viewed form


fia odel nductive b ias l earning

inductive bias learning preliminary chess domain reported
khan muggleton parson

e

e

improving performance fixed reference task multi task learning caruana
trains extra neural network outputs match related tasks order improve generalization
performance fixed reference task although explicitly identify
extra bias generated related tasks way used learn novel tasks
example exploiting bias provided set related tasks improve generalization
performance similar approaches include suddarth kergosien suddarth
holden abu mostafa

e

bias computational complexity consider inductive bias samplecomplexity perspective learnt bias decrease number examples required
novel tasks good generalization natural alternative line enquiry runningtime computational complexity learning may improved training
related tasks early neural networks vein contained sharkey
sharkey pratt
reinforcement learning many control tasks appropriately viewed elements sets
related tasks learning navigate different goal states learning set
complex motor control tasks number papers reinforcement learning literature
proposed sharing information related tasks improve average
generalization performance across tasks singh ring learning bias
set tasks improve performance future tasks sutton thrun schwartz


overview
section bias learning model formally defined main sample complexity
given showing utility learning multiple related tasks feasibility bias learning
sample complexity controlled size certain covering numbers
associated set hypothesis spaces available bias learner much way
sample complexity learning boolean functions controlled vapnik chervonenkis
dimension vapnik blumer et al section upper bounds
sample complexity required good generalization learning multiple tasks learning
inductive bias
general section specialized case feature learning neural networks section training features gradient descent presented
special case able matching lower bounds sample complexity
multiple task learning section present concluding remarks directions future
many proofs quite lengthy moved appendices
interrupt flow main text
following tables contain glossary mathematical symbols used


fibaxter

symbol



u



description
input space
output space
distribution
learning task
loss function
hypothesis space
hypothesis
error hypothesis distribution
training set
learning
empirical error training set
set learning tasks
distribution learning tasks
family hypothesis spaces
loss hypothesis space environment
sample
empirical loss
bias learning
function induced
set
average

set
set
function probability distributions
set
pseudo metric
pseudo metric
covering number
capacity
covering number
capacity
sequence hypotheses
sequence distributions
average loss
average loss
set feature maps
output class composed feature maps
hypothesis space associated
loss function class associated
covering number
capacity
pseudo metric feature maps
covering number

st




v

w x

z


p
w

v
b

b
b

b fifi c b

fifi rc b

fifi rc b
dqb c
bc
c
fifi c b
b
b
tea
e
e
cb
fa g
f
e

h jijfi e f
e
k jijfi e c
ea
cb
h jijfi c b flg

c
jijfi b
ap b

n
p
g
n

w

p
psr q
pb
h jijfi p b f
pb
k hijfi p b
pb
ft uvxw q qzy
h jijfi f uv w






u


fifi c

fifi c



qp

q qzy

q

z

first referenced












































fia odel nductive b ias l earning

hsymbol

jijfi f uv w description
covering number
k uv jijfi

capacity

neural network hypothesis space

restricted vector

growth function

vapnik chervonenkis dimension

restricted matrix
p
restricted matrix
growth function
f pq
dimension function
f
upper dimension function
f c
lower dimension function cof
n
g
optimal performance

f


c metric

rc
average

c



c


set

c
permutations integer pairs
j
permuted
f
u

empirical
metric functions
w g
n
optimal average error

first referenced





















bias learning model
section bias learning model formally introduced motivate definitions first
describe main features ordinary single task supervised learning
single task learning
computational learning theory supervised learning usually include following ingredients

e

input space

e



ss
e loss function u
probability distribution

e

hypothesis space



output space





set hypotheses functions





example learn recognize images marys face neural network
would set images typically represented subset
component
pixel intensity would set
distribution would peaked images
different faces correct class labels learners hypothesis space would class
neural networks mapping input space

loss case would discrete loss









u zfi l














fibaxter

u
u l





loss function allows us present unified treatment pattern recognition
real valued function learning e g regression
usually

goal learner select hypothesis
minimum expected loss

ct


b u f qfir

minimizing
course learner know cannot search
practice learner samples repeatedly according distribution
generate training set


fifi j

ct hence general
information contained learner produces hypothesis
v
learner simply map set training samples hypothesis space
v dt

v
stochastic learners treated assuming distribution valued

many seek minimize empirical loss defined


w x u



course intelligent things data simply minimizing empirical
errorfor example one add regularisation terms avoid fitting
however learner chooses hypothesis uniform bound

probability large deviation

bound learners genas function empirical loss training set
whether
eralization error
bound holds depends upon richness conditions ensuring convergence

well understood boolean function learning
discrete
loss convergence controlled vc dimension

w x





w x

c



w x
bfi

suppose
probability distribution


fifi fibe isanygenerated

times according let
f probabilityby atsampling
least choice training set
c satisfy






f


w

x ffko f k

theorem let

proofs may found vapnik blumer et al
reproduced

aj













vc dimension class boolean functions
largest integer exists subset
restriction contains boolean functions





fia odel nductive b ias l earning





w x

theorem provides conditions deviation


actually small
likely small guarantee true error
governed choice contains solution small error learner minimizes
error training set high probability
small however bad choice
mean hope achieving small error thus bias learner model
represented choice hypothesis space



bias learning model
main extra assumption bias learning model introduced learner embedded environment related tasks sample environment generate multiple
training sets belonging multiple different tasks model ordinary single task

bias learning
learning learning task represented distribution
model environment learning represented pair
set
e set possible learning
probability distributions
distribution controls learning learner likely see example
learner face recognition environment highly peaked face recognition type
whereas learner character recognition environment peaked
character recognition type introduction view environments
sets individual classification rather single multiple class classification
recall last paragraph previous section learners bias represented
choice hypothesis space enable learner learn bias supply family

set hypothesis spaces
putting together formally learning learn bias learning consists





z



z

z



z

z



e

input space

e

loss function



output space

u
e environment z

distribution
e

hypothesis space family



separable metric spaces

set probability distributions





u

u

c

assume loss function range
assume bounded





set functions





z





equivalently rescaling

bias governed learner uses hypothesis space example circumstances
learner may choose use full power neural network example early stopping simplicity
abstract away features assume uses entire hypothesis space

domain algebra subsets suitable one purposes borel algebra
generated
topology weak convergence assume separable metric spaces
separable metric space prohorov metric metrizes topology weak convergence parthasarathy
existence measures
see appendix discussion
particularly proof part lemma






















q



fibaxter

define goal bias learner hypothesis space
following loss

c

minimizing

f z

u f f z
z

way small high probability contains good solution
z
drawn random according sense measures appropriate
z
bias embodied environment
z
general learner know able minimizing

z

times according yield
c


fifi





e sample times according yield
b


e resulting p training setshenceforth called p sample generated
processare supplied learner sequel p sample denoted
written matrix






j










c
c
c c c
c
p sample simply p training sets
fifia sampled p different learning tasks
c

task selected according environmental probability distribution z
size training set kept primarily facilitate analysis
c
information contained learner must choose hypothesis space

directly however learner sample environment following way

e

sample

p

one way would learner
defined

minimizing empirical loss

w





c



c





w p
r w x





note
simply average best possible empirical error achievable
training set function biased estimate
unbiased estimate
would require choosing minimal average error distributions
defined

ordinary learning likely intelligent things training data
minimizing denoting set
samples
general bias
learner map takes
samples input produces hypothesis spaces

output



v

p


c c
r

p

c
v




c


ss c

p

sc



fia odel nductive b ias l earning

v

stated deterministic bias learner however trivial extend stochastic
learners
note concerned sample complexity properties bias
learner discuss issues computability
since searching entire hypothesis spaces within family hypothesis spaces
extra representational question model bias learning present
represented searched defer
ordinary learning family
discussion section main sample complexity model bias learning
introduced specific case learning set features suitable environment
related learning see section
regardless learner chooses hypothesis space uniform bound

probability large deviation

compute
upper bound
bound bias learners generalization error

view question generalization within bias learning model becomes many
tasks many examples task required ensure

close high probability uniformly
informally many tasks
many examples task required ensure hypothesis space good solutions
training tasks contain good solutions novel tasks drawn environment
turns kind uniform convergence bias learning controlled size
certain function classes derived hypothesis space family much way
vc dimension hypothesis space
controls uniform convergence case boolean
function learning theorem size measures auxiliary definitions needed
state main theorem introduced following subsection

v



v

v

v



c
p

w

w





w

c






define b
b u fir

hypothesis space hypothesis space family define
bff b b ct j

c

c define
fifi c b dt
sequence p hypotheses
fifi
c


fifi rc b

fifi c c p u



db

c b hypothesis space family define
use denote
fifi
cb b
fifi c b
fifi c ct j

covering numbers

definition hypothesis

define

cb cb






fibaxter


b
b
p



b

first part definition hypotheses
turned functions
mapping
composition loss function
collection
functions original hypotheses come
often called loss function class
case interested average loss across tasks hypotheses
chosen fixed hypothesis space motivates definition

finally
collection
restriction
belong single
hypothesis space


c
b

c

definition


c b

c

define

hypothesis space family



te

e r

define

db

fifi c

p c
b



e b
e c j


cb
e controls large p sample must ensure
size

w close uniformly c size defined terms
certain covering
cb numbers neede define measure distance
elements
elements
n
fifi c sequence p probability distributions dt
definition let
c
db b c b define

flg dqb yb db

fifi c c yb

fifi c c

f


f c c c
z
e e c e define
similarly distribution

f e
e e
e f z



fg f pseudo metrics cb e respectively
easily verified
e f
te te
edc e


definition cover set
fifi
f te e ti note require te contained
h

e f
e measurable functions
let jijfi denote size smallest
e
cover define capacity

k jilfi e
h jilfi e f


h jijfi cb f g defined similar
supremum probability measures
c
fg place f define capacity b
way
k jijfi cb g h jilfi cb fg

supremum sequences p probability measures
r

pseudo metric metric without condition




fia odel nductive b ias l earning

uniform convergence bias learners
enough machinery state main theorem theorem hypothesis space
family required permissible permissibility discussed detail appendix note
weak measure theoretic condition satisfied almost real world hypothesis space
families logarithms base









z


p






z
c

p


fififip










































p

k e






p

number examples task satisfies
k cb







pffi
c satisfy
probability least p sample
w k

theorem suppose
separable metric spaces let probability distribution set distributions
suppose
sample generated
sampling times according give
sampling times
generate

let
permissible
hypothesis space family number tasks satisfies











proof see appendix

k jijfi cb
p

several important points note theorem

k j ijfi e


cs


provided capacities

finite theorem shows bias
bound generalisation error

learner selects hypothesis spaces
terms
sufficiently large
samples bias learners
exact value
involves finding smallest error hypothesis
training sets upper bound
found example
gradient descent error function still give upper bound
see
section brief discussion achieved feature learning setting

w w
ap

w









p

c



w




close uniformly
order learn bias sense
number tasks number examples task
must
sufficiently large intuitively reasonable bias learner must see
sufficiently many tasks confident nature environment sufficiently
many examples task confident nature task

c
z


w

learner found
small value
use

learn novel tasks drawn according one following theorem bounding
sample complexity required good generalisation learning proof
similar proof bound theorem





fibaxter



fifi


ilfi ijfi


k b




ct satisfy
probability least
w x k ij
k
capacity jilfi appearing equation defined analogous
b fito
f b b fashion
capacities definition use pseudo metric

yb qfir f important thing note theorem number
ex

theorem let
training set generated sampling
according distribution let permissible hypothesis space


number training examples satisfies

amples required good generalisation learning novel tasks proportional logarithm capacity learnt hypothesis space contrast learner
bias learning reason select one hypothesis space

consequently would view candidate solution hypothesis
hypothesis spaces
thus sample complexity proportional
capacity
general considerably larger capacity
learning learner learnt learn environment
individual
sense needs far smaller training sets learn novel tasks

z

c

c
b b

c


w k


r

w




learnt hypothesis space
small value
theorem tells us
probability least
expected value
novel task
less
course rule really bad performance tasks
however probability generating bad tasks bounded particular
note
expected value function
markovs
inequality



e












w


e

e




ki










probability



ijfi

keeping accuracy confidence parameters
fixed note number examples
required task good generalisation obeys

f p k ijfi cb

c
k jilfi b increases sublinearly p upper bound number
provided

examples required task decrease number tasks increases shows
suitably constructed hypothesis space families possible share information
tasks discussed theorem


fia odel nductive b ias l earning

choosing hypothesis space family


p






w


c
p

theorem provides conditions

close guarantee
actually small governed choice contains hypothesis
space small value
learner able
minimizing error
sample e minimizing
sufficiently large theorem enthe
sures high probability
small however bad choice mean
hope finding small error sense choice represents hyper bias
learner
note sample complexity point view optimal hypothesis space family choose
contains good solutions
one containing single minimal hypothesis space
environment least set high probability
bias learning choice made hypothesis
spaces output bias learning guaranteed good hypothesis space
environment since hypothesis space minimal learning within environment require smallest possible number examples however scenario
analagous trivial scenario ordinary learning learning contains
single optimal hypothesis learnt case learning done
bias learning done correct hypothesis space already known
extreme contains single hypothesis space consisting possible functions
bias learning impossible bias learner cannot produce
restricted hypothesis space output hence cannot produce hypothesis space improved
sample complexity requirements yet unseen tasks
focussing two extremes highlights minimal requirements successful bias
must strictly smaller space
learning occur hypothesis spaces
functions
small skewed none contain good solutions
large majority environment
may seem simply replaced selecting right bias e selecting
right hypothesis space equally difficult selecting right hyper bias e
right hypothesis space family however many cases selecting right hyper bias far
easier selecting right bias example section see feature selection
may viewed bias selection selecting right features extremely
difficult one knows little environment intelligent trial error typically best
one however bias learning scenario one specify set features
exist loosely parameterised set features example neural networks learn
features sampling multiple related tasks






w








z







c





learning multiple tasks

p

z


may learner interested learning learn wants learn fixed set
previous section assume learner starts
tasks environment
hypothesis space family receives
sample generated
distributions
time however learner simply looking hypotheses
contained hypothesis space average generalization
error hypotheses minimal denoting
writing


p

fifi rc

p


c

p


fifi c







p

n
c

fibaxter

c






g p


c


p u f



empirical loss
c


w p w x


c
p u




c prove uniform bound
regardless learner chooses
fifi
g



c perform
w
probability large deviation
fifi
well training sets high probability perform well future examples

error given

tasks

n

fifi c p









p

theorem let
probability distributions
let
according let

sample generated sampling times
permissible hypothesis space family number examples task satisfies





k
cb






ffp




c c satisfy
probability least choice

g w ffk
c
k b
recall definition meaning jilfi
proof omitted follow proof bound theorem
bound theorem virtually identical bound theorem note
depends inversely number tasks p assuming first part max
k cb
expression dominate one whether helps depends rate growth

function p following lemma shows growth small enough ensure
never worse learning multiple tasks least terms upper bound number
examples required per task




k li b k h ilfi cb k li b
c

lemma hypothesis space family





fia odel nductive b ias l earning




r

c
b
proof let denote set functions
fifi c
k hijfi cb member
k jijfi
c recall definition

b
c





hypothesis space


k
k

ilfi

b
h

j





lemma appendix b

right hand inequality follows
n meafor first inequality
let probability measure let
c
sure obtained first copy
cb flg product
b c ignoring
b

elements product let
cover
pick

c b c fg fifi b
fifi c b construction
let
fifi
flg b
c b f
b establishes first inequality
k ji b
k jijfi cb p k ijfi b


keeping accuracy parameters fixed plugging see upper
bound number examples required task never increases number tasks
best decreases f npq although upper bound provides strong hint
lemma

learning multiple related tasks advantageous number examples required per task
basis section shown feature learning types behavior possible
decrease
advantage
dependence



f npq

ni

ni

theorems bounds sample complexity scale
behavior
improved
empirical loss guaranteed zero e realizable
case behavior interested relative deviation empirical
true loss rather absolute deviation formal theorems along lines stated appendix


feature learning
use restricted feature sets nearly ubiquitous method encoding bias many areas
machine learning statistics including classification regression density estimation
section choosing set features environment
related tasks recast bias learning explicit bounds

calculated general feature classes section bounds applied
learning neural network feature set section

k e fiai

k cb fiai

feature learning model
consider following quote vapnik
classical estimating multidimensional functional dependencies
following belief
real life exists small number strong features simple
functions say linear combinations approximate well unknown function
therefore necessary carefully choose low dimensional feature space
use regular statistical techniques construct approximation


fibaxter

q


q














q
q
q

q
p

q c
ptr q b r q c p


p
r



c


q q j
carefully choosing right features q equivalent bias learning
c hence provided
right hypothesis space
k e
k cb learner embedded within
environment related tasks capacities fiai fiai finite theorem tells
us feature set q learnt rather carefully chosen represents important
simplification choosing set features often difficult part machine learning

k e
k cb
section give theorem bounding fiai fiai general feature classes
theorem specialized neural network classes section
p
note forced function class feature maps q although
p
necessary indeed variants follow obtained allowed vary
q

general set strong features may viewed function
mapping input
typically lower dimensional space let
set feature
space

maps may viewed set features
must

carefully chosen quote general simple functions features may
represented class functions mapping
define hypothesis


hypothesis space family
space

capacity bounds general feature classes


q





p
u
c p
q
u
fir u
b
pb
b
p b r r q c p b q co
pb
k jilfi p b
e h jilfi p b f

f
zcbfir
supremum probability measures gf
cbfi f
cbfi
define capacity p ofb first define pseudo metric f uvxw
pulling back h metric follows
ft uv w q q u v r q r q qfir f




f
ft
easily verified uvxw pseudo metric note uv w well defined suprep
b
mum integrand must measurable guaranteed theh hypothesis space family
ft
p ib r q q c permissible lemma
part define jilfi uv w

pb
f


smallest cover pseudo metric space u v w capacity respect

k uv jilfi
h jilfi f uv w

supremum probability measures state main theorem



notationally easier view feature maps mapping
absorb loss function definition viewing


cb

via cb
previously latter function would
map
denoted follows drop subscript cause confusion
class belongs still denoted


definitions let
define capacity
usual way

section



fia odel nductive b ias l earning

ii
k

theorem let




hypothesis space family equation

k j ilfi cb
k jijfi e

k h
p b c k u v ji
k uv hilfi

ijfiai
fiai





proof see appendix b
learning neural network features

f

general set features may viewed map typically high dimensional input

much smaller dimensional space
jlk
section consider approximatspace
ing feature map one hidden layer neural network input nodes j output nodes
qp r

n

figure denote set feature maps

r
bounded subset ts u number weights parameters first two layers
set previous section
feature n

j defined











f


fifi c

fifi
b


n vwx b kyb b
z






b
output

output
node first hidden layer cb
fifi b


node parameters th feature v sigmoid squashing function v
fifi u computes
first layer hidden node






v wx

z
k













hidden nodes parameters assume v lipschitz weight

fifi


vector entire feature map thus
p

fifi


fifi b
fifi b
b

b
b
b
b b








uf
u
total number feature parameters u kffk j k
p
arguments sake assume simple functions features class previous


section squashed affine maps sigmoid function v keeping
p
neural network flavor features thus setting feature weights generates
hypothesis space








n k


f


c r



ed





r

bounded subset set hypothesis spaces
b q p c r
k h lmgnh k h hkporq
lipschitz exists constant g h j
vcb







fibaxter

multiple output classes
n

k

l

feature
map


input

p

p

p

figure neural network feature learning feature map implemented first two
hidden layers output nodes correspond different tasks
sample node network computes squashed linear function nodes
previous layer




fifi




feature
hypothesis space family restrictions output layer weights
p


weights restriction lipschitz squashing function needed obtain finite upper
bounds covering numbers theorem
finding good set features environment
equivalent finding good hyp
turn means finding good set feature map parameters
pothesis space
theorem correct set features may learnt finding hypothesis space
small error sufficiently large
sample specializing squared loss present
framework empirical loss
equation given

z

c

p


c













b
b

w p
tsu w vwrvwv x k
z vyb b
n k f
since sigmoid function v range restrict outputs range


lgorithms



f inding



g ood et



f eatures

provided squashing function v differentiable gradient descent small variation
p
backpropagation compute derivatives used feature weights minimizing
least local minimum extra difficulty ordinary gradient
descent appearance definition
solution perform gradient
p
node feature weights
descent output parameters


details see baxter b baxter chapter empirical supporting
theoretical presented given

r

w


fifi


fia odel nductive b ias l earning

ample c omplexity b ounds


k j ijfi cb



n eural n etwork f eature l earning

size ensuring resulting features good learning novel tasks
environment given theorem compute logarithm covering
numbers



k j ijfi e
ap c hypothesis space family form
theorem let







v b
n k
f
c






n
n neural network u weights mapping

p
feature weights output weights
fifi bounded squashing function v
u



lipschitz squared loss output space bounded subset
exist constants independent ilfi u j
k jijfi cb j kp k u


k jijfi e u ri

recall specialized squared loss
proof see appendix b
noting neural network hypothesis space family
theorem gives following theorem



permissible plugging





theorem let
hypothesis space family hypothesis space

set squashed linear maps composed neural network feature map suppose
number features j total number feature weights w assume feature weights
sample
output weights bounded squashing function v lipschitz let
generated environment




z

p f u k r

p





f j k k u p k p r
c satisfy
probability least
w k il






fibaxter

iscussion



f k



npq

keeping accuracy confidence parameters fixed upper bound number
examples required task behaves j
u
learner simply learning
fixed tasks rather learning learn upper bound applies recall
theorem

p


f

p




upper bound
note away feature map altogether u
becomes j independent apart less important term terms
upper bound learning tasks becomes hard learning one task extreme
fix output weights effectively j
number examples required
task decreases u
thus range behavior number examples required
decrease number
task possible improvement
tasks increases recall discussion end section

p



f npq

p

f npq

feature map learnt achieved techniques outlined baxter
b baxter bartlett baxter chapter output weights
estimated learn novel task keeping accuracy parameters fixed requires
j examples thus number tasks learnt increases upper bound
number examples required task decays minimum possible j

f

f

small number strong features assumption correct j small however
typically little idea features confident neural
network capable implementing good feature set need large implying
uj
j
u
decreases rapidly increasing uj least
terms upper bound number examples required per task learning small feature
sets ideal application bias learning however upper bound number
tasks fare well scales u

f k

npq

p

f


special case multi task framework one marginal distribution input
task fifip varies tasks conditional
space
distribution output space example would multi class face
l fifip p number faces recognized
recognition

marginal distribution simply natural distribution images faces
case every example havein addition sample th tasks conditional
distribution samples remaining p conditional distributions
view p training sets containing examples one large training set multi class
tp examples altogether bound theorem states tp
f p j k u proportional total number parameters network would
c omparison



raditional ultiple c lass c lassification

expect haussler
specialized traditional multiple class single task framework theorem consistent bounds already known however already argued face
recognition really single task multiple class appropriately viewed
example classified large margin naive parameter counting improved upon bartlett




fia odel nductive b ias l earning

p

p

potentially infinite collection distinct binary classification case goal
bias learning single output network classify subset faces
well learn set features reliably used fixed preprocessing distinguishing single face faces thing provided theorem tells us
provided trained output neural network sufficiently many examples sufficiently
many tasks confident common feature map learnt tasks good
learning yet unseen task provided task drawn distribution
generated training tasks addition learning task requires estimating j
output node parameters task vastly easier estimating parameters
entire network sample computational complexity perspective since
high confidence learnt features good learning novel tasks drawn
environment features candidate study learn
nature environment claim could made features learnt
small set tasks guarantee generalization novel tasks likely features
would implement idiosyncrasies specific tasks rather invariances apply across
tasks

p

p



p

viewed bias feature learning perspective rather traditional class
classification perspective bound number examples required task takes
somewhat different meaning tells us provided large e collecting examples
large number tasks really need collect examples would
examples vs j examples
otherwise collect feature map already known j u
tells us burden imposed feature learning made negligibly small least
viewed perspective sampling burden required task

p

k np

learning multiple tasks boolean feature maps



p



ignoring accuracy confidence parameters theorem shows number
examples required task learning tasks common neural network feature map
j
u
bounded
j number features u number
adjustable parameters feature map since
j examples required learn single task
true features known shows upper bound number examples
required task decays order minimum possible number tasks increases
suggests learning multiple tasks advantageous truly convincing need

prove lower bound form proving lower bounds real valued setting
complicated fact single example convey infinite amount information
one typically make extra assumptions targets
corrupted
noise process rather concern complications section restrict
attention boolean hypothesis space families meaning hypothesis
maps

measure error discrete loss


otherwise

f k

npq

f

p

c



c

u fir u fir

sample complexity learning p tasks boolean hypothesis space family
f pq give nearly matching upper
type parameter
controlled vc dimension
f pq derive bounds f pq hypothesis space
lower bounds involving

family considered previous section lipschitz sigmoid function v replaced hard
threshold linear threshold networks


fibaxter

f

well bound number examples required per task good generalization across
tasks theorem shows features performing well u
tasks generalize well
novel tasks u number parameters feature map given many feature
learning u likely quite large recall note section would useful
know
u
tasks fact necessary without restrictions environmental
distributions generating tasks unfortunately yet able lower
bound
empirical evidence suggesting practice upper bound number
tasks may weak example baxter bartlett reported experiments
set neural network features learnt subset japanese characters turned
good enough classifying unseen characters even though features contained
several hundred thousand parameters similar may found intrator edelman
experiments reported thrun thrun pratt chapter
gap experiment theory may another example looseness inherent
general bounds may analysis tightened particular bound
number tasks insensitive size class output functions class section
may looseness arisen

zf



p

u pper l ower b ounds
pace families



l earning tasks



b oolean h ypothesis


fifi c

b
fifi ct j


clearly say shatters growth function defined
l
size largest set shattered
vapnik chervonenkis dimension
j

first recall concepts theory boolean function learning let
class

set binary vectors obtainable
boolean functions
applying functions



important theory learning boolean functions sauers lemma sauer
make use
lemma sauers lemma boolean function class

positive integers





f


f




generalize concepts learning

p



tasks boolean hypothesis space family

fia odel nductive b ias l earning

definition let
input space
matrices



matrices
denote p
bec boolean hypothesis
c space
family
c
c




define set binary










z
c ct








c c
rc c





p
p define
p l

c c





p







note
matrix
say shatters
c

f pq p j
define

define

lemma



p




let

f

f
f f
f pq f p f


f k f
p
proof first inequality trivial definitions get second term maximum
c f construct matrix
second
inequality choose
c




f
c
whose rows length shattered clearly
shatters

first term maximum take sequence
fifi shattered hypothesis

space consisting union hypothesis spaces distribute elements equally
among rows throw away leftovers set matrices











c










c

c

















c

f
size
np subset
c c
lemma

p f pq








fibaxter

p p p

fifi c




f pq p f qp

p


fifi rc

proof observe
collection boolean
obtained first choosing functions

functions sequences
applying
first examples
second examples
definition

hence follows lemma applied


c



k cb fiai

p


one follows proof theorem particular proof theorem appendix
clear

may replaced
boolean
e
case making replacement theorem choices
discussion

following theorem obtain following bound probability large deviation
empirical true performance boolean setting

n
fifi c p
let


p



let b

c c g w ffk ij p p n


corollary conditions theorem number examples task


probability distributions
theorem let

sample generated sampling times
according
permissible boolean hypothesis space family


satisfies

f pq k p


probability least choice
g w ffk



c c


satisfy


proof applying theorem require

p p n

satisfied

f pq f pq k p

fim
used lemma
k k ifi



f pqni satisfied
fii setting il
f pq k p




fia odel nductive b ias l earning

corollary shows learning
requires

p

tasks hypothesis space family

f f pq k p r

c









p

examples task ensure high probability average true error hypotheses
selects
within average empirical error sample give
theorem showing learning required produce hypotheses whose average
true error within best possible error achievable
arbitrary sequence
distributions
within
factor number examples equation
necessary

sequence
probability distributions
define






fifi c

g c




n
c p
g c r g

c p






contains least two
pbe afi boolean
hypothesis space family
fifi let v c learning taking input p c samples
c





c
producing output p hypotheses
c c
mn mn
f pq km p




c
n

fifi probability least
exist distributions

random choice
g v c j g c ffk

theorem let
functions

proof see appendix c

ni
f pq

l inear hreshold n etworks

p
f pq

factor sample complexity
theorems within constants
learning tasks boolean hypothesis space family controlled complexity parameter
section derive bounds
hypothesis space families constructed
thresholded linear combinations boolean feature maps specifically assume
form given squashing function v replaced hard
threshold


v
otherwise



rfi





ry

dont restrict range feature output layer weights note case
proof theorem carry constants theorem depend
lipschitz bound v



f u

theorem let hypothesis space family form given
hard threshold sigmoid function v recall parameters j input dimension
number hidden nodes feature map number features output nodes feature map


fibaxter

u f k k j u k number adjustable parameters feature
f pq u k j k j k u kz
p

proof recall
c eachm p c ts denotes feature map parameters p
c


let
denote matrix









c
c
set binary p matrices obtainable composing thresholded linear
note


respectively let u
map

functions elements
restriction function must applied
element row functions may differ rows slight abuse notation
define

p





ap c









c
c
fix
sauers
lemma node first hidden layer feature map computes

f functions p input vectors thus
tpqnb k
tpqn f k
distinct functions input output first hidden layer
p points fixing first hidden layer
u b
parameters node second layer b
feature map computes tpqn k functions image produced output
u

first hidden layer thus second hidden layer computes tpqn k
functions output first hidden layer p points total
b
tp b



p
p f kr u k

number functions computable row
possible matrix



thresholded linear combination output feature map n j k hence
c
obtainable applying linear threshold functions
number binary sign assignments
thus
rows n j k
b

c

b

p f tkp u tkp p tkp
j


q convex function hence ifigfi
ik u gy
k

j
q j k u k j k u k jrq jiffk u q hgffk q
b
b
u


k

k








j


j ik u g kc
g
u k g f k p j k shows
substituting
c

u



p

k

k






p k j p k

u

j



fia odel nductive b ias l earning

hence

tp j k u k





k
k


p
j
u k p j k
p c definition f pq observe


u kn u k p j k j k u k shows
setting tp j k

u k
satisfied u np k j k j k
f
u
theorem let theorem following extra restrictions j
f

j
f pq u p k j k



f
f
proof bound apply lemma present setting contains
f
u
three layer linear threshold networks input nodes hidden nodes first hidden layer j


u

hidden nodes second hidden layer one output node theorem bartlett



lf u k u j k

f
restrictions stated greater u n hence
f ufi

u

n

f

j

j choose feature weight assignment feature map
j
identity j components input vector insensitive setting reminaing
components hence generate j
points
whose image feature map
j
shattered linear threshold output node


k

f k

combining theorem corrolary shows

f u p k j k k p
examples task suffice learning p tasks linear threshold hypothesis space family
combining theorem theorem shows

u p k j k k p
learning fail set p tasks
conclusion
inductive bias one broad significance machine learning
introduced formal model inductive bias learning applies learner able
sample multiple related tasks proved provided certain covering numbers computed
set hypothesis spaces available bias learner finite hypothesis space
contains good solutions sufficiently many training tasks likely contain good solutions
novel tasks drawn environment
specific case learning set features showed number examples
j
u
required task task training set obeys
j number

p

f k



npq



fibaxter

features u measure complexity feature class showed bound
essentially tight boolean feature maps constructed linear threshold networks addition
proved number tasks required ensure good performance features novel
tasks u showed good set features may found gradient
descent
model represents first step towards formal model hierarchical approaches
learning modelling learners uncertainty concerning environment probabilistic terms
shown learning occur simultaneously base levellearn tasks
handand meta levellearn bias transferred novel tasks technical
perspective assumption tasks distributed probabilstically allows performance guarantees proved practical perspective many domains
viewed probabilistically distributed sets related tasks example speech recognition
may decomposed along many different axes words speakers accents etc face recognition
represents potentially infinite domain related tasks medical diagnosis prognosis
pathology tests yet another example domains benefit
tackled bias learning
natural avenues enquiry include

e

f



alternative constructions although widely applicable specific example feature
learning via gradient descent represents one possible way generating searching
hypothesis space family would interesting investigate alternative methods
including decision tree approaches approaches inductive logic programming khan
et al whether general learning techniques boosting applied
bias learning setting

e





automatically determining hypothesis space family model
structure
fixed apriori represents hyper bias bias learner would
interesting see extent structure learnt

e

e



automatically determining task relatedness ordinary learning usually little doubt whether individual example belongs learning task
analogous question bias learning whether individual learning task belongs
given set related tasks contrast ordinary learning
clear cut answer examples discussed speech
face recognition task relatedness question cases medical
clear grouping large subset tasks together related tasks could
clearly detrimental impact bias learning multi task learning emprical evidence support caruana thus automatically determining
task relatedness potentially useful avenue context see silver
mercer thrun osullivan note question task relatedness
clearly meaningful relative particular hypothesis space family example
possible collections tasks related contains every possible hypothesis space





extended hierarchies extension two level arbitrarily deep hierarchies
see langford interesting question extent hierarchy
inferred data somewhat related question automatic induction
structure graphical


fia odel nductive b ias l earning

acknowledgements
work supported times australian postgraduate award shell australia postgraduate fellowship u k engineering physical sciences council grants
k k australian postdoctoral fellowship along way many people
contributed helpful comments suggestions improvement including martin anthony
peter bartlett rich caruana john langford stuart russell john shawe taylor sebastian thrun
several anonymous referees

appendix uniform convergence
theorem provides bound uniform probability large deviation

p

obtain general follow haussler introduce
following parameterized class metrics



c


e


main theorem uniform bound probability large values

p

e



theorem follow corollary

rather

e
better bounds realizable case
appendix
p



lemma following three properties



easily established

e







n

l


p e


e






e



e







p







ease exposition dealing explicitly hypothesis spaces
q
j
containing functions
constructing loss functions q mapping


j
q
however general view



loss function

j
q function abstract set

ignore particular construction

terms loss function remainder section unless otherwise stated
j
considerably
hypothesis spaces sets functions mapping




convenient transpose notation c
samples writing
training sets columns
instead rows




equation prior discussion

fiff
recalling definition






transposition lives
following definition generalizes quantities




setting

definition let
functions mapping


let orsetssimply
map
denote











































































j





fibaxter

let denote set functions given
elements

equivalently element
writing
rows define






recall equation
define similarly product probability measure


recall equation
necessarily form
define

define
recall equation class functions mapping


supremum product probability measures

size smallest cover recall definition
















































































































j

































j



























following theorem main rest uniform convergence
derived

ba c






edf g
ji g
h
gqp
r g st vu g w


permissible class functions mapping
theorem let


j








let
generated
independent trials





according


product
probability
measure







k ml








n





p






























following immediate corollary use later

yx z u g h r bac gd fe g ih

corollary conditions theorem








k l










j















p














gp g




proof theorem
proof via double symmetrization argument kind given chapter pollard
borrowed ideas proof theorem haussler


fia odel nductive b ias l earning

f irst ymmetrization





let












k







extra piece notation
bottom half viz













top half

l





l















following lemma first symmetrization trick relate probability large deviation
empirical estimate loss true loss probability large deviation
two independent empirical estimates loss



mi g





lemma let permissible set functions


j



probability measure


k l











n



po

gp
n rq


cfn





p








yd k l










p











j

let





ts g p










q
rq ts g uo g
q g zd
w
q uo g
q uo g

proof note first permissibility guarantees measurability suprema
p








lemma part triangle inequality
















thus







q
k v

vo ts g
xw
k v





















q


























































chebyshevs
inequality fixed


k v











q




g
k l













df g












gives


















r



g


r




















gd p



substituting last expression right hand side



fibaxter

econd ymmetrization
second symmetrization trick bounds probability large deviation two empirical
estimates loss e right hand side computing probability large deviation elements randomly permuted first second sample following
definition introduces appropriate permutation group purpose















h let

f l

l

permissible set functions
lemma let
mapping
let w
statement theorem fix
st cover



rows
g

k l h j q ts g p
k v h q

ts gr


h chosen uniformly random
q ts g
proof fix let

g st without loss
already done choose







generality assume form


ffu

fiff





ffu



ffp q



ffu q



ffp q

ts

ffu q

ts
q q







definition integers
let
denote set permutations








sequence pairs integers

























































j




















p

















































q



p





















j























































cp




















































j


























j
























































































fia odel nductive b ias l earning



p

hence triangle inequality



q
q ts
q
q


q g r construction
g r thus
implies
rq ts
v h xw
v h xw


p




































c

c












p































q
q ts

ts g assumption
gd
q gry































































gives



function written form

k mv b q ts gr yd vu g

h chosen uniformly random


proof f
q

ts





p




q ts











ffu

let
simplify notation denote
fiff
fiff pair


lff independent random variable










fiff
probability



lff

fiff probability
k mv h q gr




l




fiffyl
k h q

ts gr

ffp


ffp



fiff
k
fiff gr

ffu

ffu


bounded ranges


hofor zero mean independent random variables
effdings inequality devroye gyorfi lugosi


k
h yd vu







bound probability term right hand side





lemma let























j

























j



























































j



q



























































j











































































fibaxter


fiff


fiff


fiff yd vu g
ffu
fiffv
k l
fiff gr

ffp
fiff


ffp

ffu

let

ffp
fiff
fiff
ffu
fiff
fiff hence

vu g
ffu

lff ffu

lff yd vu g


r hence
giving value
minimized setting
k v f q ts gr yd vu g


noting range












j






































j





e





dj















j

















e




































j

j



j






























required



give
k l h j q ts g p
yd g st
vu w g

empirical distribution
note
simply




recall definition hence
puts point mass


n q g p
k l h

yd g st vu w g
random choice
fiff independently identically distributed

ever swaps
fiff
swaps
fiff drawn according another component
drawn according distribution thus integrate respect choice
write
j q g p
k l

yd g st vu w g
p utting


fixed




ogether


lemmas



































g























e



e





e







j

































































j



















p

















applying lemma expression gives theorem










j



fia odel nductive b ias l earning

proof theorem





another piece notation required proof hypothesis space



measures
let




























k




probability





another empirical
note used rather indicate


estimate


c
generated se sampling process addition
sample



although supplied learner
quence probability measures






means
notion used
following lemma

n
probability generating sequence measures environment

c
sample according
holds





lemma



k l















f n





k l

k l












x
p















g p dg





















p






proof follows directly triangle inequality





g p dg













g p g




treat two inequalities lemma separately

nequality





following lemma replace supremum

lemma


k ml











inequality supremum

f j
qg p


f n
k


















p




















p ep










gh



fibaxter










p





g
ji

n



e

proof suppose
let satisfy







definition


equality suppose first




e
exists
hence property







e




metric
exists

pick arbitrary




e
satisfying inequality definition










assumption compatibility
ordering




p




p



reals
say triangle inequality







u



g








g g g





g g


thus

g g g satisfying inequality
g


found choosing
shows exists



instead
identical argument run role
interchanged thus cases

g w

g
p p




p


















qc p
e
p















p




































qp






















completes proof lemma

nature c
sampling process





j
k


ay

k
















p










gh



p g h
h permissible assumed

permissibility lemma appendix hence
satisfies conditions corollary
g g g corollary
combining lemma equation substituting g
















gq






































gives following lemma sample size required ensure holds

x z u l g f g g g p
f j



lemma



k l






























p
















g

g
g




x z u l g g g g p

nequality


















g p dg


f






note




e expectation

distributed according bound left hand side apply corollary


replaced replaced replaced

respectively
replaced replaced note
permissible whenever lemma
thus































g



fia odel nductive b ias l earning

inequality satisfied
putting together lemma lemma equation proved following

general version theorem

g g
yx z u l g g g g p
g p


x
z
u
l
g

g
g

j
k l
gp g


get theorem observe





r
setting g
maximizing g
gives
substituting g
p


sample gentheorem let permissible hypothesis space family let c
n










erated environment


























p
















































































ed









theorem gives theorem


g

realizable case







g g
g
g
g g
g
ji g g

corollary conditions theorem
x z u l g g g g g g p
g p



x
z
u
l
g

g g
g g

g
n
k l
p g
g


bounds particularly useful know
set g
g
maximizes g






theorem sample complexity
scales
improved






ep





instead requiring
require

















see observe















setting
theorem treating constant
gives





















































z b
composition two function classes note



write
r




form given written



r























recalling definition









p





appendix b proof theorem










p











































































define

fibaxter

bt z thus setting b




following two lemmas enable us bound






lemma let
form














proof fix measure

let minimum size cover
















definition
b let
measure
defined

set



measurable
algebra
measurable



f


let

let

definition


andsize cover
note

minimum
lemma











shown
cover
given
choose
proved
h v



line follows
first line follows triangle inequality second






f









facts


thus
cover
follows
definition following lemma
recalling definition
lemma




let

proof fix product probability measure


covers let given
choose




























follows
thus cover













j



j















































l



























c






















































c

























c



e

















































































































j











































































































e









































































fia odel nductive b ias l earning

b bounding








j

e e
lemma




similar techniques used prove lemmas
satisfy

lemma





























































shown







equations together imply inequality




hypothesis space
wish prove


x

family form


f note f corresponds



defined
probability measure induces probability measure
zz
algebra
note
bounded positive functions

arbitrary set




















let f
let probability measure space probability measures






two elements corresponding hypothesis spaces












guaranteed
permissibility lemma part apthe measurability







pendix
f


e


b bounding
































































j

















































































































































gives inequality
















































fibaxter

b proof theorem
order prove bounds theorem apply theorem neural network
hypothesis space family equation case structure




g

g g g g










bounded

subset
lipschitz squashing function feature class

set one hidden layer neural networks inputs hidden nodes outputs
squashing function weights
fiff bounded subset lipschitz
restriction bounded restrictions weights ensure lipschitz
b



classes hence exists

w



norm

case loss function squared loss
hence probability measures

onnow recall assumed
output space

l


yd






marginal distribution
derived similarly

probability measures



yd

define
e e

supremum probability measures borel subsets






e






size


smallest
cover



metric
similarly
set

e e
supremum probability measures equations imply






applying theorem haussler











w

















j











































j


















































































































































j















j































































































substituting two expressions applying theorem yields theorem



fia odel nductive b ias l earning

appendix c proof theorem
proof follows similar argument one presented anthony bartlett
ordinary boolean function learning
first need technical lemma



g

k




k g r n n

denote number occurences random sequence
proof let


function viewed decision rule e observations tries guess














whether probability
bayes

dis wd
optimal
rule isd otherwise
decision
estimator
hence
k g k g


k g
kd g







half probability binomial
random variable least wd
sluds inequality slud
k g k


g









lemma let random variable uniformly distributed





let

valued random variables g




function mapping





















































































































































































n





















j

tates inequality tate states
normal


k
















n









shattered row let
set
let



distributions

contained










fiff









th row ofd f


let





achieved sequence







note
optimal error


fiff

fiff

contains sequence shatters optimal error







ffu
combining last two inequalities completes proof


















c


























c
















c




































c











c





























g



















c













fibaxter









p













fiff

fiff

sample let element
fiff array








equal number occurrences
fiff
uniformly random generate select

output learning
sample


fiff

lff c b

fiff

lff
b
fiff
fiff
lff

ffu
probability generating configuration
fiff sampling

process sum possible configurations lemma

lff
fiff
fiff er df g h nn ji



c













































c



























































fiff




k




c













































j

j





b
























c

r























































r

ffp
l n l
n


k






















h
n n ij











g











df

g



k k

g








plugging shows


k



c

valued random variable g














lff

g
l n l



g nr
k k n






jensens inequality since
implies











c

















hence



















































c





g


fia odel nductive b ias l earning











since inequality holds random choice must hold specific choice


sequence distributions
hence learning

k










setting

assuming equality get











g




g



e





g






g















e



k



g g



ensures

g















g



solving substituting expressions g shows satisfied
provided
g g g

g
r r since g r g g assuming
setting
g somefor
yd becomes




r right hand side approximately maximized
subject constraint
qpsr point value exceeds
cdf thus g

r
e


g























c




















e










j





c























c













c





q

























g

contains least two
g
obtain dependence theorem observe assumption
ut two distributions
functions
hence exists

let









concentrated



let
b h product distributions

v

generated note

one
learning
chooses wrong hypothesis


k



















































































































































fibaxter





generate
n
nr x w w n


choose uniformly random
cording lemma shows

k
g
least














































c











sample

ac





r











ji g r combining two constraints
x z u finishes proof


c



f g g


provided














p


appendix measurability
order theorems hold full generality impose constraint called
permissibility hypothesis space family permissibility introduced pollard
ordinary hypothesis classes definition similar dudleys image admissible
suslin dudley extending definition cover hypothesis space families
throughout section assume functions map complete separable metric
j
let denote borel algebra topological space section
space
view set probability measures topological space equipping
topology weak convergence algebra generated topology
following two definitions taken minor modifications pollard













j





valued functions indexed set
definition set
r

j





definition set
















zff

q







permissible indexed set







exists function



analytic subset polish space





function
algebra









j

indexing




analytic subset polish space




measurable respect product


simply continuous image borel subset
another polish space analytic subsets polish space include borel sets
important projections analytic sets analytic measured complete
measure space whereas projections borel sets necessarily borel hence cannot
measured borel measure details see dudley section
lemma











j

permissible








permissible

proof omitted
define permissibility hypothesis space families
topological space called polish metrizable complete separable metric space



fia odel nductive b ias l earning

w







definition hypothesis space family
permissible exist sets





analytic subsets polish spaces respectively function


measurable respect




ba



fe hg





e






zff







dc










j





let
analytic subset polish space let
measure space

denote analytic subsets following three facts analytic sets taken
pollard appendix c

fe hg





b









complete





e






e

projection onto



contains product algebra

c set





























recall definition definition following lemma assume

n
complete
completed respect probability measure
respect environmental measure





lemma permissible hypothesis space family
permissible


f permissible

permissible






measurable

measurable

permissible
simply set
proof absorbed loss function hypotheses

fold products
thus follows lemma
g



































immediate definitions permissible proved
identical argument used measurable suprema section pollard appendix
c


j


j
function
defined
note borel measurable








borel measurable kechris chapter permissibility






measurable
automatically implies permissibility



r



j
appropriate way prove
let indexed




j





e





e



fubinis theorem
define




j
e






measurable function let
defined

e
indexes
appropriate way
permissible provided


measurable analyticity becomes important let
shown




e e
property b analytic sets



contains


e




e








set
projection
onto
property c
n

analytic

assumed complete
measurable property thus
measurable function permissibility
follows



kj








fiff
qp

r fiy
g
c c

p




g





w

lj










c

c

f f















c

fibaxter

references
abu mostafa method learning hints hanson j cowan j giles
c l eds advances neural information processing systems pp san mateo
ca morgan kaufmann
anthony bartlett p l neural network learning theoretical foundations cambridge university press cambridge uk
bartlett p l lower bounds vc dimension multi layer threshold networks
proccedings sixth acm conference computational learning theory pp
york acm press summary appeared neural computation
bartlett p l sample complexity pattern classification neural networks
size weights important size network ieee transactions
information theory
baxter j learning internal representations ph thesis department mathematics statistics flinders university south australia copy available
http wwwsyseng anu edu au jon papers thesis ps gz



baxter j b learning internal representations proceedings eighth international
conference computational learning theory pp acm press copy available
http wwwsyseng anu edu au jon papers colt ps gz



baxter j bayesian information theoretic model learning learn via multiple task
sampling machine learning
baxter j b canonical distortion measure vector quantization function approximation proceedings fourteenth international conference machine learning
pp morgan kaufmann
baxter j bartlett p l canonical distortion measure feature space nn
classification advances neural information processing systems pp mit
press
berger j statistical decision theory bayesian analysis springer verlag
york
blumer ehrenfeucht haussler warmuth k learnability vapnikchervonenkis dimension journal acm
caruana r multitask learning machine learning
devroye l gyorfi l lugosi g probabilistic theory pattern recognition
springer york
dudley r course empirical processes vol lecture notes mathematics pp springer verlag
dudley r real analysis probability wadsworth brooks cole california


fia odel nductive b ias l earning

gelman carlin j b stern h rubim b eds bayesian data analysis
chapman hall
good j history hierarchical bayesian methodology bernardo j
groot h lindley v smith f eds bayesian statistics ii university
press valencia
haussler decision theoretic generalizations pac model neural net
learning applications information computation
heskes solving huge number similar tasks combination multi task learning
hierarchical bayesian shavlik j ed proceedings th international
conference machine learning icml pp morgan kaufmann
intrator n edelman make low dimensional representation suitable
diverse tasks connection science
kechris classical descriptive set theory springer verlag york
khan k muggleton parson r repeat learning predicate invention page
c ed proceedings th international workshop inductive logic programming
ilp lnai pp springer verlag
langford j c staged learning tech rep cmu school computer science
http www cs cmu edu jcl ltol staged latest ps



mitchell need biases learning generalisations dietterich g
shavlik j eds readings machine learning morgan kaufmann
parthasarathy k r probabiliity measures metric spaces academic press london
pollard convergence stochastic processes springer verlag york
pratt l discriminability transfer neural networks hanson j
cowan j giles c l eds advances neural information processing systems
pp morgan kaufmann
rendell l seshu r tcheng layered concept learning dynamically variable
bias management proceedings tenth international joint conference artificial
intelligence ijcai pp ijcai inc
ring b continual learning reinforcement environments r oldenbourg verlag
russell use knowledge analogy induction morgan kaufmann
sauer n density families sets journal combinatorial theory

sharkey n e sharkey j c adaptive generalisation transfer knowledge
artificial intelligence review


fibaxter

silver l mercer r e parallel transfer task knowledge dynamic
learning rates measure relatedness connection science
singh transfer learning composing solutions elemental sequential tasks machine learning
slud e distribution inequalities binomial law annals probability
suddarth c holden c symolic neural systems use hints developing complex systems international journal man machine studies
suddarth c kergosien l rule injection hints means improving network performance learning time proceedings eurasip workshop neural
networks portugal eurasip
sutton r adapting bias gradient descent incremental version delta bar delta
proceedings tenth national conference artificial intelligence pp mit
press
tate r f double inequality normal distribution annals mathematical
statistics
thrun learning n th thing easier learning first advances neural
information processing systems pp mit press
thrun mitchell learning one thing proceedings international
joint conference artificial intelligence pp morgan kaufmann
thrun osullivan j discovering structure multiple learning tasks tc saitta l ed proceedings th international conference machine
learning icml pp morgen kaufmann
thrun pratt l eds learning learn kluwer academic
thrun schwartz finding structure reinforcement learning tesauro g
touretzky leen eds advances neural information processing systems vol
pp mit press
utgoff p e shift bias inductive concept learning machine learning artificial
intelligence pp morgan kaufmann
valiant l g theory learnable comm acm
vapnik v n estimation dependences empirical data springer verlag
york
vapnik v n nature statistical learning theory springer verlag york




