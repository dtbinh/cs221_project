Journal Artificial Intelligence Research 12 (2000) 387-416

Submitted 12/99; published 6/00

Application Reinforcement Learning Dialogue
Strategy Selection Spoken Dialogue System Email
Marilyn A. Walker

walker@research.att.com

AT&T Shannon Laboratory
180 Park Ave., Bldg 103, Room E103
Florham Park, NJ 07932

Abstract

paper describes novel method spoken dialogue system learn
choose optimal dialogue strategy experience interacting human users.
method based combination reinforcement learning performance modeling spoken dialogue systems. reinforcement learning component applies Q-learning
(Watkins, 1989), performance modeling component applies PARADISE evaluation framework (Walker et al., 1997) learn performance function (reward) used
reinforcement learning. illustrate method spoken dialogue system named
elvis (EmaiL Voice Interactive System), supports access email phone.
conduct set experiments training optimal dialogue strategy corpus 219
dialogues human users interact elvis phone. test
strategy corpus 18 dialogues. show elvis learn optimize strategy
selection agent initiative, reading messages, summarizing email folders.

1. Introduction
past several years, become possible build spoken dialogue systems
communicate humans telephone real time. Systems exist tasks
finding good restaurant nearby, reading email, perusing classified advertisements
cars sale, making travel arrangements (Seneff, Zue, Polifroni, Pao, Hetherington, Goddeau, & Glass, 1995; Baggia, Castagneri, & Danieli, 1998; Sanderman, Sturm,
den Os, Boves, & Cremers, 1998; Walker, Fromer, & Narayanan, 1998). systems
realized examples real time, goal-oriented interactions humans
computers. Yet spite 30 years research algorithms dialogue management task-oriented dialogue systems, (Carbonell, 1971; Winograd, 1972; Simmons &
Slocum, 1975; Bruce, 1975; Power, 1974; Walker, 1978; Allen, 1979; Cohen, 1978; Pollack,
Hirschberg, & Webber, 1982; Grosz, 1983; Woods, 1984; Finin, Joshi, & Webber, 1986;
Carberry, 1989; Moore & Paris, 1989; Smith & Hipp, 1994; Kamm, 1995) inter alia,
design dialogue manager real-time, implemented systems still art
science (Sparck-Jones & Galliers, 1996). paper describes novel method,
experiments validate method, spoken dialogue system learn
experience human users optimize choice dialogue strategy.
dialogue manager spoken dialogue system processes user's utterance
chooses real time information communicate human user
communicate it. choice makes called strategy. dialogue manager
naturally formulated state machine, state dialogue defined set
c 2000 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiWalker

state variables representing observations user's conversational behavior, results
accessing various information databases, aspects dialogue history. Transitions
states driven system's dialogue strategy. typical system,
large number potential strategy choices state dialogue.
example, consider one choice faced elvis (EmaiL Voice Interactive System)
spoken dialogue system supports access user's email phone. elvis provides
verbal summaries user's email folders, many ways summarize folder
(Sparck-Jones, 1993, 1999). summary could consist simple statement
number messages different folders, e.g., 5 new messages, could provide
much detail messages particular folder, e.g., messages
Kim, one message meeting, second interviewing Antonio.
elvis must decide many properties message mention, message's
status, sender, subject message.1
Decision theoretic planning applied problem choosing among dialogue
strategies, associating utility U strategy (action) choice positing
spoken dialogue systems adhere Maximum Expected Utility Principle
(Keeney & Raiffa, 1976; Russell & Norvig, 1995),

Maximum Expected Utility Principle: optimal action one maximizes expected utility outcome states.

Thus, elvis act optimally choosing strategy state Si maximizes U (Si ).
formulation however simply leaves us problem derive utility
values U (Si ) dialogue state Si . Several reinforcement learning algorithms based
dynamic programming specify way calculate U (Si ) terms utility successor
state Sj (Bellman, 1957; Watkins, 1989; Sutton, 1991; Barto, Bradtke, & Singh, 1995),
utility final state dialogue known, would possible calculate
utilities earlier states, thus determine policy selects optimal
dialogue strategies.
Previous work suggested possible treat dialogue strategy selection
stochastic optimization problem way (Walker, 1993; Biermann & Long, 1996; Levin,
Pieraccini, & Eckert, 1997; Mellish, Knott, Oberlander, & O'Donnell, 1998). However
(Walker, 1993), argued lack performance function assigning utility
final state dialogue critical methodological limitation. seemed
three main possibilities simple reward function: user satisfaction, task completion,
measure user effort elapsed time dialogue number
user turns. appeared simple reward functions fail
capture essential aspects system's performance. example, level user
effort complete dialogue task system, domain task dependent. Moreover, high
levels effort, e.g., requirement users confirm system's understanding
utterance, necessarily lead concomitant increases task completion,
1. strategies implemented elvis summarized Figure 1. Note due practical
constraints, implemented strategy choices subset states, elvis uses fixed
strategy states. Section 2, describe detail strategy choices elvis explores
addition choices summarization, namely choices among strategies controlling dialogue
initiative reading multiple messages.

388

fiReinforcement Learning ELVIS System

lead significant decreases user satisfaction (Shriberg, Wade, & Price, 1992; Danieli &
Gerbino, 1995; Kamm, 1995; Baggia et al., 1998). Furthermore, user satisfaction alone fails
ect fact system successful unless helps user complete
task. concluded relationship measures interesting
complex method deriving appropriate performance function
necessary precursor applying stochastic optimization algorithms spoken dialogue
systems. (Walker, Litman, Kamm, & Abella, 1997a), proposed paradise method
learning performance function corpus human-computer dialogues.
work, apply paradise model learn performance function elvis,
use calculating utility final state dialogue experiments
applying reinforcement learning elvis's selection dialogue strategies. Section 2 describes implementation version elvis randomly explores alternate strategies
initiative, reading messages, summarizing email folders. Section 3 describes
experimental design first use exploratory version elvis collect
training corpus conversations 73 human users carrying set three email
tasks. Section 4 describes apply reinforcement learning corpus 219 dialogues optimize elvis's dialogue strategy decisions. test optimized policy
experiment six new users interact elvis complete set
tasks, show learned policy performs significantly better exploratory
policy used training phase.

2. ELVIS Spoken Dialogue System
started process designing elvis conducting Wizard-of-Oz experiment
recorded dialogues six people accessing email remotely talking
human playing part spoken dialogue system. purpose
experiment identify basic functionality implemented elvis.
analysis resulting dialogues suggested elvis needed support contentbased access email messages specification subject sender field, verbal
summaries email folders, reading body email message, requests help
repetition messages (Walker et al., 1997b, 1998).
Given requirements, implemented elvis using general-purpose platform
spoken dialogue systems (Kamm et al., 1997). platform consists dialogue
manager (described detail Section 2.2), speech recognizer, audio server
voice recordings text-to-speech (TTS), interface computer running
Elvis telephone network, modules specifying rules spoken language
understanding application specific functions.
speech recognizer speaker-independent Hidden Markov Model (HMM) system,
context-dependent phone models telephone speech, constrained grammars
defining vocabulary permitted point dialogue (Rabiner, Juang, &
Lee, 1996). platform supports barge-in, user interrupt system;
barge-in important application user interrupt system
reading long email message.
audio server switch voice recordings text-to-speech (TTS)
integrate voice recordings TTS. TTS technology concatenative diphone synthe389

fiWalker

sis (Sproat & Olive, 1995). Elvis uses TTS since would possible pre-record,
concatenate, words necessary realizing content email messages.
spoken language understanding (SLU) module consists set rules specifying
vocabulary allowable utterances, associated set rules translating
user's utterance domain-specific semantic representation meaning. syntactic
rules converted FSM network used directly speech recognizer
(Mohri, Pereira, & Riley, 1998). semantic rule associated syntactic
rule maps user's utterance directly application specific template consisting
application function name arguments. templates converted directly
application specific function calls specified application module. understanding
module supports dynamic grammar generation loading recognizer
vocabulary must change interaction, e.g., support selection email messages
content fields sender subject.
application module provides application specific functions, e.g., functions accessing message attributes subject sender, functions making realizable
speech used instantiate variables spoken language generation.

2.1 ELVIS's Dialogue Manager Strategies

's dialogue manager based state machine one dialogue strategies
explored state. state dialogue manager defined set
state variables representing various items information dialogue manager uses
deciding next. state variables encode various observations user's
conversational behavior, results processing user's speech spoken
language understanding (SLU) module, results accessing information databases
relevant application, well certain aspects dialogue history. dialogue
strategy specification system say; Elvis represented
template variables must instantiated current context. states
system always executes dialogue strategy states alternate strategies
explored. strategies implemented Elvis summarized Figure 1.
complete specification dialogue strategy executed state called
policy dialogue system.
develop version Elvis supported exploring number possible policies,
implemented several different choices particular states system. goal
implement strategy choices states optimal strategy obvious priori.
purpose illustrating dialogue strategies explored, consider situation
user attempting execute following task (one tasks used
experimental data collection described Section 3):
Elvis

Task 1.1: working home morning plan go directly meeting

go work. Kim said would send message telling
meeting is. Find Meeting Time Meeting Place.

complete task, user needs find message Kim meeting
inbox listen it. many possible strategies Elvis could use help
user accomplish task. Below, first describe dialogue strategies Figure 1
390

fiReinforcement Learning ELVIS System

Strategy Type
Initiative
Summarization

Choices
Explored?
yes
yes

Reading

yes

Request-Info



Provide-Info
Help




Timeout



Rejection



Strategy Choices
System-Initiative (SI), Mixed-Initiative (MI)
SummarizeBoth (SB), SummarizeSystem (SS),
SummarizeChoicePrompt (SCP)
Read-First (RF), Read-Summary-Only (RSO),
Read-Choice-Prompt (RCP)
AskUserName, Ask-Which-Selection (AskWS),
Ask-Selection-Criteria (AskSC),
Read-Message
AskUserName-Help, SI-Top-Help, MI-Top-Help,
Read-Message-Help, AskWS-Help, AskSC-Help
AskUserName-Timeout,
Read-Timeout,
SI-Top-Timeout, MI-Top-Timeout, Read-MessageTimeout, AskWS-Timeout, AskSC-Timeout
AskUserName-Reject,
SITop-Reject, MI-Top-Reject, AskWS-Reject, AskSCReject, Read-Message-Reject

Figure 1: ELVIS's Dialogue Strategies. ELVIS explores choices Initiative, Summarization
Read Strategies uses fixed strategies elsewhere.
Elvis makes choices among, describe detail complete state machine,
dialogue strategies Figure 1 used states choice among
dialogue strategies, space policies Elvis execute. provide several
detailed examples dialogues generated Elvis's dialogue manager's state
machine.
2.1.1 Strategy Choices ELVIS

One strategy choice experiments involved exploration two different strategies
managing dialogue initiative: system-initiative mixed-initiative. implementation Elvis tested here, choice initiative made early dialogue
style initiative used throughout dialogue whenever could
choice two order avoid confusing user.
Previous research suggested system-initiative strategy Figure 2
Elvis directive user might best strategy, given current state
art speech recognition (Kamm, 1995). Figure 2, Elvis's utterances labelled
name dialogue strategy Elvis using produce utterance, e.g.,
utterance E1 labelled SI-Top, name system-initiative top level strategy.
choice system-initiative strategy affects utterances E2 E3; utterances
Elvis continues dialogue requesting particular items information. general,
system-initiative strategy constrains user say point dialogue,
requesting particular items information one time, user constrained
providing information asked for. reason strategy
recommended given current state art speech recognition guides
391

fiWalker

E1: Hi, ELVIS here...You 5 new unread messages
inbox. Say Read, Summarize, say Help options.
U1: Read.
E2: Select Sender, Subject order arrival?
U2: Sender.
E3: Sender?
U3: Kim.
E4: messages Kim, there's 1 message \Interviewing
Antonio" 1 message \Meeting Today." first message
titled, \Interviewing Antonio." says, \I'd interview him.
could go along lunch. Kim."
U4: Next.
E5: message meeting today. says meeting
morning 10:00 2d516. See there, Kim.
U5: I'm done here.

SI-Top
Ask-Which-Selection
Ask-Selection-Criterion
Read-First (RF)

Read-Message

Figure 2: System-Initiative (SI) Dialogue Strategy: ELVIS explicitly prompts user
say point dialogue. ELVIS's utterances labelled
strategy names.
user saying things system likely understand, recognizer
vocabulary state constrained responses question asked.
strategy might dispreferred users however, could tedious user
constrained providing one item information time. dialogue Figure 2,
takes four user utterances access message Kim needed complete Task 1.1.
research suggested better strategy mixed-initiative strategy allows
user take initiative except user fails say anything system
trouble understanding user (Smith & Hipp, 1994; Levin, Pieraccini, Eckert,
Fabbrizio, & Narayanan, 1999; Webber & Joshi, 1982; Walker & Whittaker, 1990).
dialogue Figure 3 illustrates mixed-initiative dialogue strategy. Elvis's
utterances Figure 3 labelled name dialogue strategy Elvis using
produce utterance, e.g., utterance E1 labelled MI-Top, name mixedinitiative top level strategy. Note unlike SI-Top strategy, MI-Top strategy
volunteer information tell user say. Instead,
mixed-initiative strategy allows user take control dialogue specify exactly
wants single utterance, utterance U1 Figure 3. However,
evidence user trouble confused, user says
nothing utterance U2, mixed-initiative strategy take initiative ask
user information tell user say. utterance E3 Figure 3
Read-Timeout strategy triggered fact user's response delayed longer
pre-set timeout threshhold, system tells user actions available
point dialogue. general, mixed-initiative strategy support
ecient dialogues; dialogue Figure 2, takes two user utterances access
message Kim needed complete Task 1.1. However eciency gain may offset
increased likelihood misunderstandings user confusion.
392

fiReinforcement Learning ELVIS System

E1: Hi, ELVIS here. I've got mail.
U1: Read messages Kim.
E2: messages Kim, there's 1 message \Interviewing
Antonio" 1 message \Meeting Today." first message
titled, \Interviewing Antonio." says, \I'd interview him. could
go along lunch. Kim."
U2: (User says nothing within 5 seconds system takes initiative)
E3: say repeat, repeat last message, say next,
read next message folder. say, I'm done
exit folder.
U3: Next message.
E4: message meeting today. says meeting
morning 10:00 2d516. See there, Kim.
U4: I'm done here.

MI-Top
Read-First (RF)

Read-Timeout
ReadMessage

Figure 3: Mixed-Initiative (MI) Dialogue Strategy: ELVIS leaves user take
initiative, unless user seems trouble system
Summarize Strategy
Summarize-Both (SB)

Summarize-System (SS)
Summarize-Choice-Prompt
(SCP)

Example Prompt
top level inbox, Kim, there's 1 message
\Lunch." Michael, there's 1 message \Evaluation
group meeting." Noah, there's 1 message \Call
Tomorrow" 1 message \Interviewing Antonio."
Owen, there's 1 message \Agent Personality."
top level inbox, there's 1 message Kim, 2 messages
Noah, 1 message Michael, 1 message Owen.
E: Summarize subject, sender, both?
U: Subject.
E: top level inbox, there's 1 message \Lunch," 1
message \Interviewing Antonio," 1 message \Call
Tomorrow," 1 message \Evaluation Group Meeting,"
1 message \Agent Personality."

Figure 4: Alternate Summarization Strategies response request \Summarize
messages"
different type strategy choice involves Elvis's decisions present information user. mentioned many different ways summarize
set items user wants information about. Elvis explores set alternate summarization strategies illustrated Figure 4; strategies vary message attributes
included summary messages current folder. Summarize-Both
strategy (SB) uses sender subject attributes summary. employing Summarize-System strategy (SS), Elvis summarizes subject sender
based current context. instance, user top level inbox, Elvis
summarize sender, user situated folder containing messages par393

fiWalker

ticular sender, Elvis summarize subject, summary sender would provide
new information. Summarize-Choice-Prompt (SCP) strategy asks user specify
relevant attributes summarize by. See Figure 4.
Another type information presentation choice occurs request user
read subset messages, e.g., Read messages Kim, results multiple
matching messages. strategies explored Elvis summarized Figure 5. One
choice Read-First strategy (RF) involves summarizing messages
Kim, taking initiative read first one. Elvis used read strategy
dialogues Figures 2 3. alternate strategy reading multiple matching
messages Read-Summary-Only (RSO) strategy, Elvis provides information
allows users refine selection criteria. Another strategy reading multiple
messages Read-Choice-Prompt (RCP) strategy, Elvis explicitly tells user
say order refine message selection criteria. See Figure 5.
Read Strategy
Read-First (RF)
Read-Summary-Only
(RSO)
Read-Choice-Prompt
(RCP)

Example Prompt
messages Kim, there's 1 message \Interviewing Antonio" 1 message \Meeting Today." first message
titled, \Interviewing Antonio." says, \I'd interview him.
could go along lunch. Kim."
messages Kim, there's 1 message \Interviewing Antonio" 1 message \Meeting Today."
messages Kim, there's 1 message \Interviewing Antonio" 1 message \Meeting Today." hear messages,
say, \Interviewing Antonio" \Meeting."

Figure 5: Alternate Read Strategies response request \Read messages
Kim"
remainder Elvis's dialogue strategies, summarized Figure 1, fixed, i.e.
multiple versions strategies explored experiments presented here.
2.1.2 ELVIS's Dialogue State Machine

mentioned above, dialogue strategy choice system makes, particular
state, say say it. policy dialogue system complete
specification strategy execute system state. state defined set
state variables. Ideally, state representation corresponds dialogue model
summarizes dialogue history compactly, retains relevant information
dialogue interaction far. notion dialogue model retaining relevant
information formally known reinforcement learning state representation
satisfies Markov Property. state representation satisfying Markov Property one
probability particular state particular reward r
action prior state estimated function action
prior state, function complete dialogue history (Sutton & Barto, 1998).
precisely,
Pr(st+1 = s0; rt+1 = rjst ; ) = Pr(st+1 = s0 ; rt+1 = rjst; ; rt ; st,1 ; at,1 ; rt,1 ; : : : R1; s0 ; a0 )
394

fiReinforcement Learning ELVIS System

s0 ; r; st .
Markov Property guaranteed state representation encodes everything
system able observe everything happened dialogue far.
However, representation would complex estimate model probability
various state transitions, systems complex spoken dialogue system must
general utilize state representations compact possible.2 However state
representation impoverished, system lose much relevant information
work well.
Operations Variable
KnowUserName
InitStrat
SummStrat
ReadStrat
TaskProgress
CurrentUserGoal
NumMatches
WhichSelection
KnowSelectionCriteria
Confidence
Timeout
Help
Cancel

Abbrev
(U)
(I)
(S)
(R)
(P)
(G)
(M)
(W)
(SC)
(C)
(T)
(H)
(L)

Possible Values
0,1
0,SI,MI
0,SS,SCP,SB
0,RF,RSO,RCP
0,1,2
0,Read,Summarize
0,1,N>1
0,Sender (Snd),Subject (Sub),InOrder (InO)
0,1
0,1
0,1
0,1
0,1

Figure 6: Operations variables possible values define operations vector
controlling aspects ELVIS's behavior. abbreviations variable
names values used column headers Operations Variables
Figures 7, 8 9.
's state space representation must obviously discriminate among states
various strategy choices explored, addition, must state variables
capture distinctions number states Elvis always executes
strategy. state variables Elvis keeps track possible values given
Figure 6. KnowUserName (U) variable keeps track whether Elvis knows user's
name not. InitStrat (I), SummStrat (S) ReadStrat (R) variables keep track
whether Elvis already employed particular initiative strategy, summarize strategy
reading strategy current dialogue, so, strategy selected.
variable needed Elvis employs one strategies, strategy
used consistently throughout rest dialogue order avoid confusing user.
TaskProgress (P) variable tracks much progress user made completing
experimental task. CurrentUserGoal (G) variable corresponds system's belief
Elvis

2. respects driven implementation requirements since system development maintenance impossible without compact state representations.

395

fiWalker

U
0
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

Operations Variables
Action Choices

R P G
W SC C H L
0 0
0 0 0
0
0 0 0 0 0 0 AskUserName
0 0
0 0 0
0
0 0 1 0 0 0 SI-Top, MI-Top
SI 0
0 0 0
0
0 0 1 0 1 0 SI-Top-Help
SI 0
0 0 0
0
0 0 0 0 0 0 SI-Top-Reject
SI 0
0 0
0
0 0 1 0 0 0 SS,SB,SCP
SI 0
0 0 R
0
0 0 1 0 0 0 AskWS
SI 0
0 0 R
0
0 0 0 0 0 0 AskWS-Reject
SI 0
0 0 R
0 Snd 0 1 0 0 0 AskSC
SI 0
0 0 R
0 Snd 0 1 1 0 0 AskSC-TimeOut
SI 0
0 0 R N>1 Snd 1 1 0 0 0 RF,RSO,RCP
SI 0 RCP 0 R
1 Snd 1 1 0 0 0 ReadMessage
SI 0 RCP 1 0
0
0 1 0 0 0 0 SI-Top
MI 0
0 0 0
0
0 0 1 0 1 0 MI-Top-Help
MI 0
0 0 0
0
0 0 0 0 0 0 MI-Top-Reject
MI 0
0 0
0
0 0 1 0 0 0 SS,SB,SCP
MI SS
0 0 R N>1 Snd 1 1 0 0 0 RF,RSO,RCP
MI SS RF 0 R
1 Snd 1 1 0 0 0 ReadMessage

Figure 7: portion ELVIS's operations state machine using full operations vector
control ELVIS's behavior
user's current goal is. WhichSelection (W) variable tracks whether
system knows type selection criteria user would use read
messages. KnowSelectionCriteria (SC) variable tracks whether system believes
understood either sender name subject name use select messages.
NumMatches (M) variable keeps track many messages match user's selection
criteria. Confidence (C) variable threshholded variable indicating whether
speech recognizer's confidence understood user said pre-set
threshhold. Timeout (T) variable represents system's belief user didn't
say anything allotted time. Help (H) variable represents system's belief
user said Help, leads system providing context-specific help messages.
Cancel (L) variable represents system's belief user said Cancel, leads
system resetting state state last user utterance processed.
Thus 110,592 possible states used control operation system, although
states occur.3
order reader achieve better understanding range Elvis's capabilities way operations vector used, Figure 7 shows portion Elvis's state
machine generate sample system mixed-initiative dialogue interactions
Figures 8 9. figures provides state representation strategy choices made state sample dialogues. example, row two Figure
7 shows system acquires user's name (KnowUserName (U) = 1)
high confidence (Confidence (C) = 1), explore system-initiative (SI-Top)
3. example system knows user name, none variable values change
initial value.

396

fiReinforcement Learning ELVIS System

mixed-initiative (MI-Top) strategies. Figure 8 illustrates dialogue SI strategy
chosen Figure 9 illustrates dialogue MI-Top strategy chosen.
discuss detail dialogue Figure 8 generated state machine
Figure 7.
Figure 8, first row shows Elvis's strategy AskUserName executed
initial state dialogue operations variables set 0. Elvis's
utterance E1 surface realization strategy's execution. Note according
state machine Figure 7, strategy choices initial state
dialogue. user responds name SLU module returns user's
name dialogue manager high confidence (Confidence (C) = 1). dialogue
manager updates operations variables KnowUserName(U) = 1 Confidence (C)
= 1, shown row two Figure 8. Now, according state machine Figure 7,
two choices strategy, system-initiative strategy whose initial action SI-Top
mixed-initiative strategy whose initial action MI-Top. Figure 8 illustrates one
potential path SI-Top strategy chosen; Elvis's utterance E2 realization
SI-Top strategy. user responds utterance Help processed
SLU, dialogue manager receives input information SLU believes
user said Help (Help (H) = 1) high confidence (Confidence (C) = 1). dialogue
manager updates operations variables ect information SLU well
fact executed system-initiative strategy (InitStrat (I) = SI). results
operations vector shown adjacent Elvis's utterance E3. third row state
machine Figure 7 shows state, Elvis choice strategies, Elvis
simply executes SI-Top-Help strategy, realized utterance E3. user
responds saying Read (utterance U3) dialogue manager updates operations
variables results SLU module saying believes user said Read
(Goal (G) = R) high confidence (Confidence (C) = 1). state machine Figure 7
specifies state Elvis execute AskWhichSelection (AskWS) strategy,
corresponds Elvis's utterance E4 Figure 8. time, however, user
responds system's query word Sender (utterance U4), SLU module
confident understanding (Confidence (C) = 0) operations variable
updated confidence value. According state machine Figure 7, strategy
Elvis executes state AskWS-Reject strategy, specific rejection strategy
designed executed context goal acquire value WhichSelection
operations variable (see list operations variables Figure 6). realization
AskWS-Reject strategy utterance E5. user responds saying Sender
(utterance U5) time SLU module understands user's utterance high
confidence. dialogue manager updates WhichSelection (W) variable Snd
confidence variable, resulting operations vector shown adjacent utterance
E6. state machine Figure 7 specifies state Elvis execute
AskSelectionCriterion (AskSC) strategy realized utterance E6. user says
nothing (utterance U6) SLU module detects silence pre-set timeout
threshhold high confidence. dialogue manager updates operations variables
ect observation (Timeout (T) = 1, Confidence (C) = 1). state machine
Figure 7 specifies AskSC-Timeout strategy executed state;
context specific strategy re-attempts system's initial goal providing information
397

fiWalker

Operations Variables

Utterance

U
0 0

R
0 0

P
0

G
0

W
0 0

SC C
0
0 0

1

0

0

0

0

0

0

0

0

1

0

1

SI

0

0

0

0

0

0

0

1

0

1

SI

0

0

0

R

0

0

0

1

0

1

SI

0

0

0

R

0

0

0

0

0

1

SI

0

0

0

R

0

Snd

0

1

0

1

SI

0

0

0

R

0

Snd

0

1

1

1

SI

0

0

0

R

N

Snd

1

1

0

1

SI

0

RCP

0

R

1

Snd

1

1

0

1

SI

0

RCP

1

0

0

0

0

1

0

H L
0 0 E1: Hi, ELVIS here,
that?
U1: Susan.
0 0 E2: Hi Susan, Please wait.
I'll get mail (10 seconds).
Okay Susan, 5 new
unread messages inbox. Say Read, Summarize
say Help options.
U2: Help.
1 0 E3: access messages,
say read summarize.
switch active messages,
say new messages, messages. want repeat
message summary, say repeat. always say cancel
help. Say Goodbye Elvis,
hang up.
U3: Read.
0 0 E4: Select Sender, Subject
order Arrival?
U4: Sender
0 0 E5: didn't understand you.
want select messages
sender, subject order
arrival?
U5: Sender
0 0 E6: Sender?
U6: (user says nothing)
0 0 E7: senders are, Kim,
Noah, Michael, Owen.
sender want?
U7: Kim.
0 0 E8: messages Kim,
there's 1 message \Interviewing Antonio" 1 message \Meeting Today."
hear messages, say, \Interviewing Antonio" \Meeting."
U8: Meeting.
0 0 E9: message
meeting today. says meeting morning 10:00
2d516. See there, Kim.
U9: I'm done here.
0 0 E10: back
toplevel inbox. Say Read, Summarize say Help
options.

Strategy
Name
AskUserName
SI-Top

SI-Top-Help

AskWS
AskWSReject
AskSC
AskSCTimeOut
RCP

ReadMessage

SI-Top

Figure 8: System-Initiative Dialogue, completing Task 1.1 Figure 11, illustrating
ELVIS's ability provide help, use timeout confidence information
intended help user re-asking original query, realized
utterance E7. user responds name sender (utterance U7)
understood SLU high confidence (KnowSelectionCriteria (SC) = 1, Confidence =
398

fiReinforcement Learning ELVIS System

1). Elvis retrieves messages mail server matching selection criteria,
multiple matches found (NumMatches = N, per list operations variables
Figure 6). time row ten state machine Figure 7 specifies state
choice dialogue strategies, namely choice Read-First (RF), ReadSummary-Only (RSO) Read-Choice-Prompt (RCP) strategies illustrated Figure 5.
Elvis randomly chooses explore RCP strategy, realized utterance E8.
information user needs complete Task 1.1 provided utterance E9
user responds utterance U8 saying Meeting (and SLU understands high
confidence). row utterance E9 Figure 8 shows updated operations vector
ecting fact system executed RCP strategy; ReadStrat (R) variable
used enforce fact implementation Elvis, particular reading,
strategy selected, used consistently throughout dialogue avoid confusing
user. last exchange Figure 8, SLU module's confident understanding
user's utterance U9, I'm done here, results resetting G,M,W, SC variables
dialogue manager updates variable TaskProg (P) 1 ect progress
experimental task. Figure 7 shows that, state, system one strategy;
since InitStrat variable set SI, system executes SI-Top strategy,
realized context utterance E10.
dialogue Figure 9 illustrates potential dialogue Elvis MI-Top
strategy selected rather SI-Top strategy user name acquired.
reader may track path dialogue utilizing state machine Figure 7.
Note operations vector Elvis utilizes needed make Elvis fully operational system provides functionality equired support users. dialogues
Figures 8 9 show Elvis provides:

Context-Specific Help strategies: illustrated strategies SI-Top-Help MITop-Help, supported Help variable.

Timeout strategies: system uses taking initiative context, trig-

gered delay user's response going time threshhold, illustrated
strategy AskSC-TimeOut, supported Timeout variable.

Rejection strategies: backoff dialogue actions become directive user
ASR returns confidence value lower confidence threshhold,
illustrated strategies AskWS-Reject MI-Top-Reject, supported
Confidence variable.

However, operations vector state representation needed control operation
system necessarily required even optimal applying reinforcement learning
(Barto et al., 1995; Sutton & Barto, 1998). Sometimes may advantageous aggregrate states purpose applying reinforcement learning, even possible
guarantee state representation obeys Markov property (Sutton & Barto,
1998). Note many states defined Figure 7 alternate strategies
explored. example, Confidence variable used system determine whether
apply backoff rejection strategy, vary rejection strategies.
incentive aggregrating states ignoring state variables irrelevant learning
399

fiWalker

Operations Variables

Utterance

U
0 0


0

R
0

P G
0 0

W
0 0

SC C
0
0


0

H L
0 0

1

0

0

0

0

0

0

0

0

0

0

1

0

E1: Hi, ELVIS here, AskUserName
that?
U1: Susan.
E2: Hi Susan, Please wait. I'll
get mail (delay 10
seconds).

1

MI 0

0

0

0

0

0

0

1

0

1

0

1

MI 0

0

0

0

0

0

0

0

0

0

0

1

MI 0

0

0



0

0

0

1

0

0

0

1

MI SS

0

0

R

N

Snd

1

1

0

0

0

1

MI SS

RF

1

R

1

Snd

1

1

0

0

0

Strategy
Name

Okay Susan, I've got mail.
U2: Help.
E3: read summarize messages using values
sender subject field.
example, say, read
message Lee
meeting, or, read messages
Kim, or, summarize
messages. access messages
current folder,
say next, previous, first, last.
want repeat message summary, say repeat.
always say cancel
help. exit current folder
say, I'm done here. hangup,
say, Goodbye Elvis .
U3: Summarize messages.
E4: didn't understand you.
want
messages?
U4: Summarize them.
E5: top level inbox,
there's 2 messages Kim,
1 message Noah, 1 message Michael, 1 message Owen.
U5: Read messages
Kim
E6: messages Kim,
there's one message Interviewing Antonio one message Meeting Today.
first message says (user barges
in)
U6: Next message
E7: Next message.
message meeting today.
says meeting morning
10:00 2d516. See there,
Kim.

MI-Top
MI-Top-Help

MI-TopReject
SS

RF

ReadMessage

Figure 9: Mixed Initiative Dialogue completing Task 1.1 Figure 11, illustrating ELVIS's
ability provide help, use timeout confidence information
reduction state space size; means fewer dialogue samples needed
collect large enough sample state/action pairs purpose applying reinforcement
learning. perspective, goal aggregrate state space way
distinguish states different dialogue strategies explored.
400

fiReinforcement Learning ELVIS System

However, additional constraint state aggregration. Reinforcement learning4
backs rewards received final states dialogue sf earlier states si
strategy choices explored. However algorithm distinguish strategy choices
trajectory si sf distinct strategy choice. words,
two actions point lead state, without local reward, Q-values
two actions equal.
UserName (U) Init (I) TaskProg (P) UserGoal (G)
0,1
0,SI,MI 0,1,2,
0,R,S
Figure 10: Reinforcement Learning State Variables Values
Figure 10 specifies subset state variables given Figure 6 developed represent state space purpose applying reinforcement learning.
combination state variables compact, provides distinct trajectories
different strategy choices. reduced state space 18 states, supports dialogue
optimization policy space 2 312 = 1062882 different policies. policies
prima facie candidates optimal policies support human users
completing set experimental email tasks.

3. Experimental Design
Experimental dialogues training testing phase collected via experiments human users interacted Elvis complete three representative application tasks required access email messages three different email inboxes.
collected data 73 users performing three tasks (219 dialogues) training Elvis,
tested learned policy corpus six users performing
three tasks (18 dialogues).
Instructions users given set web pages, one page experimental dialogue. web page dialogue contained brief general description
functionality system, list hints talking system, description
tasks user supposed complete, information call Elvis.
page contained form specifying information acquired Elvis
dialogue, survey, filled task completion, designed probe user's
satisfaction Elvis. Users read instructions oces calling Elvis
oce phone.
three calls Elvis made sequence, conversation consisted
two task scenarios system user exchanged information criteria
selecting messages information within message. tasks given Figure
11, where, e.g., Task 1.1 Task 1.2 done conversation Elvis.
motivation asking caller complete multiple tasks call create subdialogue
structure experimental dialogues (Litman, 1985; Grosz & Sidner, 1986).
4. applied without local rewards.

401

fiWalker











Task 1.1: working home morning plan go directly meeting
go work. Kim said would send message telling
meeting is. Find Meeting Time Meeting Place.
Task 1.2: second task involves finding information different message. Yesterday
evening, told Lee might want call morning. Lee said would send
message telling reach him. Find Lee's Phone Number.
Task 2.1: got work, went directly meeting. Since people
late, you've decided call ELVIS check mail see meetings may
scheduled. Find day, place time scheduled meetings.
Task 2.2: second task involves finding information different message. Find
need call anyone. so, find number call.
Task 3.1: expecting message telling Discourse Discussion Group
meet. Find place time meeting.
Task 3.2: second task involves finding information different message. secretary
taken phone call left message. Find called
reach them.

Figure 11: Sample Task Scenarios





Dialogue Eciency Metrics: elapsed time, system turns, user turns
Dialogue Quality Metrics mean recognition score, timeouts, rejections, helps, cancels,

bargeins, timeout%, rejection%, help%, cancel%, bargein%
Task Success Metrics: task completion per survey
User Satisfaction: sum TTS Performance, ASR Performance, Task Ease, Interaction
Pace, User Expertise, System Response, Expected Behavior, Comparable Interface, Future
Use.

Figure 12: Metrics collected spoken dialogues.
collect number different measures dialogue via four different methods:
(1) dialogues recorded; (2) dialogue manager logs state
system enters dialogue strategy Elvis selects state; (3) dialogue
manager logs information calculating number dialogue quality dialogue eciency
metrics summarized Figure 12 described detail below; (4) end
dialogue, users fill web page forms support calculation task success
user satisfaction measures. explain use measures paradise
framework reinforcement learning.
402

fiReinforcement Learning ELVIS System

dialogue eciency metrics calculated dialogue recordings
system logs. length recording used calculate elapsed time seconds
(ET) beginning end interaction. Measures number System
Turns, number User Turns, calculated basis system logging
everything said everything heard user say.
dialogue quality measures derived recordings, system logs
hand-labeling. number system behaviors affect quality resulting dialogue
automatically logged. included number timeout prompts (timeouts)
played user didn't respond quickly expected, number recognizer
rejections (rejects) system's confidence understanding low said
something I'm sorry didn't understand you. User behaviors system perceived
might affect dialogue quality logged: included number times
system played one context specific help messages believed
user said Help (helps), number times system reset context
returned earlier state believed user said Cancel (cancels).
recordings used check whether users barged system utterances,
labeled per-state basis (bargeins).
Another measure dialogue quality recognizer performance whole dialogue,
calculated terms concept accuracy. recording user's utterance compared
logged recognition result calculate concept accuracy measure utterance
hand. Concept accuracy measure semantic understanding system, rather
word word understanding. example, utterance Read messages
Kim contains two concepts, read function, sender:kim selection criterion.
system understood user said Read, concept accuracy would 0.5. Mean
concept accuracy calculated whole dialogue used, conjunction
ASR rejections, compute Mean Recognition Score (MRS) dialogue.
goal generate models performance generalize across systems
tasks, thought important introduce metrics likely generalize.
eciency metrics seemed unlikely generalize since, e.g., elapsed time
complete task depends dicult task is. research suggested
dialogue quality metrics likely generalize (Litman, Walker, & Kearns, 1999),
thought raw counts likely task specific. Thus normalized
dialogue quality metrics dividing raw counts total number utterances
dialogue. resulted timeout%, rejection%, help%, cancel%, bargein%
metrics.
web page forms basis calculating Task Success User Satisfaction
measures. Users reported perceptions whether completed task
(Comp).5 provide objective evidence fact completed
task filling form information acquired Elvis.6
5. Yes,No responses converted 1,0.
6. supports alternative way calculating Task Success objectively using Kappa statistic
compare information users filled key task (Walker et al., 1997a). However
earlier results indicated user's perception task success better predictor
overall satisfaction, simply use perceived task success measured Comp.

403

fiWalker











ELVIS easy understand conversation? (TTS Performance)
conversation, ELVIS understand said? (ASR Performance)
conversation, easy find message wanted? (Task Ease)
pace interaction ELVIS appropriate conversation? (Interaction Pace)
conversation, know could say point dialogue? (User
Expertise)
often ELVIS sluggish slow reply conversation? (System
Response)
ELVIS work way expected conversation? (Expected Behavior)
conversation, ELVIS's voice interface compare touch-tone interface
voice mail? (Comparable Interface)
current experience using ELVIS get email, think you'd use
ELVIS regularly access mail away desk? (Future Use)

Figure 13: User Satisfaction Survey
order calculate User Satisfaction, users asked evaluate system's performance user satisfaction survey Figure 13. question responses
five point Likert scale simply required yes, yes, no, maybe responses.
survey questions probed number different aspects users' perceptions
interaction Elvis order focus user task rating system,
(Shriberg et al., 1992; Jack, Foster, & Stentiford, 1992; Love, Dutton, Foster, Jack, &
Stentiford, 1994). multiple choice survey response mapped range 1
5. values responses summed, resulting User Satisfaction
measure dialogue possible range 8 40.

4. Training Testing Optimized Dialogue Strategy
Given experimental training data, first apply paradise estimate performance
function Elvis linear combination metrics described above. apply
performance function dialogue training corpus estimate utility
final state dialogue apply Q-learning using utility. Finally test
learned policy new population users.

4.1

paradise

Performance Modeling

first step developing performance model spoken dialogue systems specification causal model performance illustrated Figure 14 (Walker et al., 1997a).
According model, system's primary objective maximize user satisfaction.
404

fiReinforcement Learning ELVIS System

MAXIMIZE USER SATISFACTION

MAXIMIZE TASK
SUCCESS

MINIMIZE COSTS

EFFICIENCY
MEASURES

Figure 14:

QUALITATIVE
MEASURES

's structure objectives spoken dialogue performance.

paradise

Task success various costs associated interaction contributors user satisfaction. Task success measured quantitatively number
ways: could represented continuous variable representing quality solution
boolean variable representing binary task completion. Dialogue costs two types:
dialogue eciency quality. Eciency costs measures system's eciency
helping user complete task, number utterances completion
dialogue. Dialogue quality costs intended capture aspects system
may strong effects user's perception system, number times
user repeat utterance order make system understand utterance.
Given model, performance metric dialogue system estimated
experimental data applying multivariate linear regression user satisfaction
dependent variable task success, dialogue quality, dialogue eciency measures
independent variables.7 stepwise linear regression training data measures
discussed above, showed Comp, MRS, BargeIn% Rejection% significant
contributors User Satisfaction, accounting 39% variance R-Squared (F
(4,192)=30.7, p <.0001).8
Performance = :27 Comp + :54 MRS , :09 BargeIn% + :15 Rejection%
tested well performance function generalize unseen test dialogues
tenfold cross-validation, randomly sampling 90% training dialogues
testing goodness fit performance model remaining 10% dialogues
7. One advantage approach performance function derived, longer necessary
collect user satisfaction reports users, opens possibility estimating reward
function fully automatic measures. latter possibility might useful online calculation
reward function calculating local reward.
8. normalize metrics regression magnitude coecients directly
indicates contribution factor User Satisfaction (Cohen, 1995; Walker et al., 1997a).

405

fiWalker

training set. average R2 training set 37% standard error
.005, average R2 held-out 10% dialogues 38% standard
error .06. Since average R2 test set statistically indistinguishable
training set, assume performance model generalize new Elvis dialogues.

4.2 Training Optimized Policy

Given learned performance function described above, apply function
measures logged dialogue Di , thereby replacing range measures single performance value Pi , used utility (reward) final state
dialogue.9 apply reinforcement learning Pi utility final state
dialogue Di (Bellman, 1957; Sutton, 1991; Tesauro, 1992; Russell & Norvig, 1995;
Watkins, 1989). utility action state Si , U (a; Si ) (its Q-value),
calculated terms utility successor state Sj , obeying recursive equation:
U (a; Si ) = R(a; Si ) + Mija max
U (a0 ; Sj )


X

0

j

R(a; Si ) immediate reward received action Si , strategy
finite set strategies admissable state Si, Mija probability
reaching state Sj strategy selected state Si . experiments reported here,
reward associated state, R(Si ), zero. addition, since reliable priori
prediction user action particular state possible (for example user may say
Help speech recognizer may fail understand user), state transition model
Mija estimated logged state-strategy history dialogue.
utility values estimated within desired threshold using value iteration,
updates estimate U (a; Si ), based updated utility estimates neighboring
states, equation becomes:

Un+1 (a; Si ) = R(Si) +

XM
j


ij

max
Un(a0 ; Sj )

0

Un (a; Si ) utility estimate state Si n iterations (Sutton &
Barto, 1998) pp. 101. Value iteration stops difference Un (a; Si )
Un+1 (a; Si ) threshold, utility values associated states
strategy selections made.10 value iteration completed optimal policy
obtained selecting action maximal Q-value dialogue state.
Figure 15 enumerates subset states aggregrated state space used
reinforcement learning potential actions defining policy space. strategy
greatest Q-value state training indicated boldface Figure 15.
optimized policy tested fixed policy operation Elvis.
states task, System-Initiative strategy Figure 2 predicted
optimal initiative strategy, Read-First strategy Figure 5 predicted
best performance Read strategies. Figure 15 shows, learned strategy
9. dialogue treated unique final state.
10. experimenting various threshholds, used threshold 5% performance range
dialogues.

406

fiReinforcement Learning ELVIS System

State Variables
U P G
0 0 0 0
1 0 0 0
1 SI 0
1 SI 0 R
1 SI 1 0
1 SI 1
1 SI 1 R
1 SI 2 0
1 SI 2
1 SI 2 R
1 MI 0
1 MI 0 R
1 MI 1 0
1 MI 1
1 MI 1 R
1 MI 2 0
1 MI 2
1 MI 2 R

Strategy Choices
AskUserName
SI-Top, MI-Top
SS,SB,SCP
RF,RSO,RCP
SI-Top
SS,SB,SCP
RF,RSO,RCP
SI-Top
SS,SB,SCP
RF,RSO,RCP
SS,SB,SCP
RF,RSO,RCP
MI-Top
SS,SB,SCP
RF,RSO,RCP
MI-Top
SS,SB,SCP
RF,RSO,RCP

Figure 15: subset state space defines policy class explored experiments. learned policy indicated boldface.
summarization varies according state task. different summarization
strategies illustrated Figure 4. policy learned use SummarizeBoth strategy beginning dialogue (when TaskProg = 0), switch
using Summarize-System strategy later phases dialogue. strategy makes
sense terms giving user complete information messages inbox
beginning dialogue.

4.3 Testing Optimized Policy

first constructed deterministic version Elvis implemented learned policy
discussed above, one variation. variation based fact decision
whether use Summarize-Both Summarize-System summarization strategy
conditioned value TaskProg variable. However, intended utilize optimized version system situations would access TaskProg
variable, namely situations task user attempting perform
control experimenter. examined Q-values summarization strategies course dialogue, found Summarize-System strategy
greatest average Q-value, strongly preferred Summarize-Both strategy
except initial phase dialogue, Q-value Summarize-Both
407

fiWalker

slightly greater. Thus implemented learned policy (see Figure 15),
exception Summarize-System strategy used throughout dialogue.11
terms operations state machine Figure 7, implementation learned
policy means choices SI-Top MI-Top strategies replaced
SI-Top strategy, choices different read strategies different states
replaced Read-First (RF) strategy choices different summarization
strategies different states replaced Summarize-System (SS) strategy.
tested policy six new users never used Elvis before.
users conversed Elvis perform set six email tasks used
training phase, described Figure 10 above. addition, identical performance
measures collected testing dialogue training dialogue. Overall performance
measures training test dialogues given Table 1, training data split
terms System-Initiative, Mixed-Initiative overall means. table shows
versions Elvis high levels task completion, important testing
utility reinforcement learning. Statistical analysis results indicated statistically
significant increase User Satisfaction training test (F= = 4.07 p = .047).

5. Discussion Future Work

paper proposes novel method dialogue system learn choose
optimal dialogue strategy tests experiments Elvis, dialogue system
supports access email phone, strategies initiative, reading summarizing messages. reported experiments Elvis learned System-Initiative
strategy higher utility Mixed-Initiative strategy, Read-First best
read strategy, Summarize-System generally best summary strategy.
tested policy Elvis learned new set users performing set
tasks showed learned policy resulted statistically significant increase
user satisfaction test set dialogues.
Previous work treated system's choice dialogue strategy stochastic
optimization problem (Walker, 1993; Biermann & Long, 1996; Levin & Pieraccini, 1997;
Levin et al., 1997). knowledge, Walker (1993) first proposed reinforcement
learning algorithms could applied dialogue strategy selection. simulation experiments reported Walker (1993, 1996), dialogues two agents artificial world
used test dialogue strategies optimal various conditions.
experiments varied: (1) dialogue agent's resource bounds; (2) performance function used assess agent's performance. experiments showed strategies
optimal one set assumptions performance function could
highly ecacious performance function ected fact dialogue agent
resource bounded. Walker (1993) suggested optimal dialogue strategy could
11. Obviously choice strategy test risked testing non-optimal policy. alternative
would try future work utilize SummStrat state variable operations vector
state representation reinforcement learning simply distinguish states summarize
strategy selected (no summary produced) states least one summary
produced. analysis dialogue phase carries through, policy
learned use Summarize-Both strategy first summary dialogue afterwards
use Summarize-System strategy.

408

fiReinforcement Learning ELVIS System

Measure Train SI Train MI Overall Train Test
Comp
.87
.80
.85
.94
User Turns
21.5
17.0
20.0 25.8
System Turns
24.2
21.2
23.1 29.2
Elapsed time (sec) 339.14
296.18
311.56 368.5
Mean recognition score
.88
.72
.82
.81
TimeOuts
2.7
4.2
3.0
3.3
TimeOut%
.11
.19
.13
.11
Cancs
.34
.02
.26
.00
Canc%
.02
.00
.01
.00
Help Requests
.67
.92
0.66 1.11
Help%
.03
.05
.03
.04
BargeIns
3.6
3.6
3.7
7.8
BargeIn%
.08
.09
.18
.30
Rejects
.9
1.6
1.1
1.4
Reject%
.04
.08
.05
.05
User satisfaction
28.9
25.0
27.5 31.7
Table 1: Performance measure means per dialogue Training Testing Dialogues. SI
= System-Initiative, MI = Mixed-Initiative
learned via reinforcement learning, appropriate performance function could determined, described experiment using genetic algorithms learn optimal dialogue
strategy. subsequent work, utilized here, paradise model proposed way
learn appropriate performance function (Walker et al., 1997a). addition, related work
utilizing Elvis, varied reward function, applied reinforcement learning
algorithms, carried Fromer (Fromer, 1998).
Biermann Long (1996), proposed use similar techniques context
learning optimal dialogue strategies multi-modal dialogue tutor. goal tutor
instruct students taking first programming class tutor interacted
students highlighting parts code printing text screen telling
wrong program. Biermann Long describe planned experiment
system would vary instructional style, system's reward would
amount time system's instructions student's response. reward
function based assumption delayed response suggested greater cognitive
load student, cognitive load minimized instructional setting.
Levin colleagues proposed treating dialogue systems Markov Decision Processes suggested system designers could determine appropriate objective
function might (Levin et al., 1997; Levin & Pieraccini, 1997). carried series
experiments simulated user interacted implemented spoken dialogue
system travel planning exchanging messages semantic meaning level.
showed system could learn strategy choices level database interaction,
409

fiWalker

e.g., system query database determined many
constraints necessary order find ights matched user's goals.
Stochastic optimization techniques applied similar problems textbased dialogue interaction graphical user interfaces. Mellish colleagues applied
stochastic optimization problem determining content structure
system's utterances ILEX system, interactive museum tour guide (Mellish et al.,
1998). work tested user population performance (reward)
measure based heuristics good text plans formulated experts. Christensen
colleagues applied genetic algorithms design graphical user interface
automated teller machine. goal automatically learn best layout sequence
interaction screens intracting user (Christensen, Marks, & Shieber, 1994).
work, Levin colleagues, user population simulated.
Here, method optimizing dialogue strategy selection illustrated evaluating strategies managing initiative information presentation interaction
human callers. applied paradise performance model derive empirically motivated performance function, combines subjective user preferences objective
system performance measures single function. would impossible predict priori dialogue factors uence usability dialogue system,
degree. performance equation shows task success dialogue quality measures
primary contributors system performance. Furthermore, contrast assuming priori model, use dialogues real user-system interactions provide
realistic estimates Mija , state transition model used learning algorithm.
impossible predict priori transition frequencies, given imperfect nature
spoken language understanding, unpredictability user behavior.
use method introduces several open issues possible areas future
work. First, results learning algorithm dependent representation
state space. spoken dialogue systems, system designers construct state space
decide state variables system needs monitor, whereas applications reinforcement learning (e.g. backgammon), state space pre-defined.
experiments reported here, fixed state representation carried experiments
particular state representation. However future work hope able learn
aspects state history represented using similar techniques
described (Langkilde, Walker, Wright, Gorin, & Litman, 1999). example, may
beneficial system represent additional state variables representing
dialogue history, order Elvis able learn dialogue strategies ect
aspects dialogue history.
Second, advance actually running experiments, clear much experience
system need determine strategy better. experiments reported here,
able show improvement policy converged initiative
read strategies yet converged appropriate summarization strategy.
possible local rewards nonzero optimal policy could
learned less training data. future work, hope explore interaction
training set size use local reward.
Third, experimental data based fixing particular experimental parameters.
experiments based short-term interactions novice users, might
410

fiReinforcement Learning ELVIS System

expect users email system would engage many interactions
system, preferences system interaction strategies could change time
user expertise. means performance function might change time.
used fixed set tasks representative domain, possible
aspects policies learned might sensitive experimental tasks. Another
limitation experiments carried scenario email folder
small number messages: strategies tested might optimal
email folder contains hundreds messages.
Fourth, optimal strategy potentially dependent various system parameters.
example, ReadFirst strategy takes initiative read message, might result
messages read user wasn't interested in, since user barge-in
system utterances, little overhead taking decision. system
support barge-in, results might different.
Fifth, learned policy depends reward function. example, since Elvis
fully functional system, users complete experimental task version
system using strategies explored. means used
task completion reward function, reinforcement learning would predicted
differences different strategies. hand, using
paradise performance function, utilized reward function fit data
Elvis's performance, evidence reward function may generalize
systems (Walker, Kamm, & Litman, 2000).
Sixth, experiments report limited way demonstrate
utility reinforcement learning dialogue strategy optimization. traditional
way selecting best dialogue strategies would experiments treated
dialogue strategy selection factor, standard statistical hypothesis testing would
used compare performance different strategies. scale experiment
small enough imaginable space policies could possibly
tested traditional way. However, primary goal experiments reported
simply test feasibility methods, required working
detail many issues state strategy representation discussed above.
many details worked out, methods presented applied
much complex dialogue strategy optimization problems, varying initiative
depending dialogue state (Chu-Carroll & Brown, 1997; Webber & Joshi, 1982),
exploring combinations strategies information presentation, summarization (SparckJones, 1999), error recovery (Hirschman & Pao, 1993), database query (Levin et al., 1997),
cooperative responses (Joshi, Webber, & Weischedel, 1986; Finin et al., 1986; Chu-Carroll &
Carberry, 1994), content selection generation (McKeown, 1985; Kittredge, Korelsky,
& Rambow, 1991), inter alia.
Finally, learning algorithm report off-line algorithm, i.e. Elvis
collects set dialogues decides optimal strategy result. contrast,
possible Elvis learn on-line, course dialogue, methods
developed performance function automatically calculated approximated.
primary goal experiments reported explore application
reinforcement learning spoken dialogue systems identify open issues
discussed above. current work, exploring issues several ways.
411

fiWalker

codified notion state estimator systematically vary state
representation order explore effect state representation value function
optimal policy (Singh, Kearns, Litman, & Walker, 1999). process
using reinforcement learning conduct set experiments spoken dialogue system
accessing information activities New Jersey. experiments explore
number different reward functions explore much broader range strategies
user initiative, reprompting user, confirming system's understanding.

6. Acknowledgements
received many useful questions comments research presented
initial results invited talk given AAAI 1997 Providence, R.I. design
implementation basic functionality Elvis done collaboration J. Fromer,
G. DiFabbrizio, D. Hindle C. Mestel. Initial experiments reinforcement learning
Elvis done collaboration J. Fromer S. Narayanan. work
benefited discussions W. Eckert, C. Kamm, M. Kearns, E. Levin, D. Litman, D.
McAllester, R. Pieraccini, R. Sutton, S. Singh. Special thanks S. Whittaker, J.
Wiebe four reviewers detailed comments earlier versions manuscript.
v

References
Allen, J. F. (1979). Plan-Based Approach Speech Act Recognition. Tech. rep., University Toronto.
Baggia, P., Castagneri, G., & Danieli, M. (1998). Field trials italian arise train
timetable system. Interactive Voice Technology Telecommunications Applications, IVTTA, pp. 97{102.
Barto, A., Bradtke, S. J., & Singh, S. P. (1995). Learning act using real-time dynamic
programming. Artificial Intelligence Journal, 72(1-2), 81{138.
Bellman, R. E. (1957). Dynamic Programming. Princeton University Press, Princeton, N.J.
Biermann, A. W., & Long, P. M. (1996). composition messages speech-graphics
interactive systems. Proceedings 1996 International Symposium Spoken
Dialogue, pp. 97{100.
Bruce, B. (1975). Belief systems language understanding. Tech. rep. AI-21, Bolt,
Berenak Newman.
Carberry, S. (1989). Plan recognition use understanding dialogue. Kobsa,
A., & Wahlster, W. (Eds.), User Models Dialogue Systems, pp. 133{162. Springer
Verlag, Berlin.
Carbonell, J. R. (1971). Mixed-initiative man-computer dialogues. Tech. rep. 1970, Bolt
Beranek Newman, Cambridge, MA.
412

fiReinforcement Learning ELVIS System

Christensen, J., Marks, J., & Shieber, S. (1994). Placing text labels maps diagrams.
Heckbert, P. (Ed.), Graphics Gems IV. Academic Press.
Chu-Carroll, J., & Brown, M. K. (1997). Tracking initiative collaborative dialogue interactions. Proceedings 35th Annual Meeting Association Computational
Linguistics, pp. 262{270.
Chu-Carroll, J., & Carberry, S. (1994). plan-based model response generation
collaborative task-oriented dialogue. AAAI 94, pp. 799{805.
Cohen, P. R. (1995). Empirical Methods Artificial Intelligence. MIT Press, Boston.
Cohen, P. R. (1978). knowing say: Planning speech acts. Tech. rep. 118,
University Toronto; Department Computer Science.
Danieli, M., & Gerbino, E. (1995). Metrics evaluating dialogue strategies spoken
language system. Proceedings 1995 AAAI Spring Symposium Empirical
Methods Discourse Interpretation Generation, pp. 34{39.
Finin, T. W., Joshi, A. K., & Webber, B. L. (1986). Natural language interactions
artificial experts. Proceedings IEEE, 74(7), 921{938.
Fromer, J. C. (1998). Learning optimal discourse strategies spoken dialogue system.
Tech. rep., MIT AI Lab M.S. Thesis.
Grosz, B. J. (1983). Team: transportable natural language interface system. Proc. 1st
Applied ACL, Association Computational Linguistics, Santa Monica, Ca.
Grosz, B. J., & Sidner, C. L. (1986). Attention, intentions structure discourse.
Computational Linguistics, 12, 175{204.
Hirschman, L., & Pao, C. (1993). cost errors spoken language system. Proceedings Third European Conference Speech Communication Technology,
pp. 1419{1422.
Jack, M., Foster, J. C., & Stentiford, F. W. (1992). Intelligent dialogues automated telephone services. International Conference Spoken Language Processing, ICSLP,
pp. 715 { 718.
Joshi, A. K., Webber, B., & Weischedel, R. M. (1986). aspects default reasoning
interactive discourse. Tech. rep. MS-CIS-86-27, University Pennsylvania.
Kamm, C., Narayanan, S., Dutton, D., & Ritenour, R. (1997). Evaluating spoken dialog systems telecommunication services. 5th European Conference Speech
Technology Communication, EUROSPEECH 97, pp. 2203{2206.
Kamm, C. (1995). User interfaces voice applications. Roe, D., & Wilpon, J.
(Eds.), Voice Communication Humans Machines, pp. 422{442. National
Academy Press.
413

fiWalker

Keeney, R., & Raiffa, H. (1976). Decisions Multiple Objectives: Preferences Value
Tradeoffs. John Wiley Sons.
Kittredge, R., Korelsky, T., & Rambow, O. (1991). need domain communication
knowledge. Computational Intelligence, 7 (4), 305{314.
Langkilde, I., Walker, M., Wright, J., Gorin, A., & Litman, D. (1999). Automatic prediction
problematic human-computer dialogues May Help You?. Proceedings
IEEE Workshop Automatic Speech Recognition Understanding, ASRUU99.
Levin, E., & Pieraccini, R. (1997). stochastic model computer-human interaction
learning dialogue strategies. EUROSPEECH 97.
Levin, E., Pieraccini, R., & Eckert, W. (1997). Learning dialogue strategies within
Markov Decision Process framework. Proc. IEEE Workshop Automatic Speech
Recognition Understanding.
Levin, E., Pieraccini, R., Eckert, W., Fabbrizio, G. D., & Narayanan, S. (1999). Spoken
language dialogue: theory practice. Proc. IEEE Workshop Automatic
Speech Recognition Understanding, ASRUU99.
Litman, D. (1985). Plan recognition discourse analysis: integrated approach
understanding dialogues. Tech. rep. 170, University Rochester.
Litman, D. J., Walker, M. A., & Kearns, M. J. (1999). Automatic detection poor speech
recognition dialogue level. Proceedings Thirty Seventh Annual Meeting
Association Computational Linguistics, pp. 309{316.
Love, S., Dutton, R. T., Foster, J. C., Jack, M. A., & Stentiford, F. W. M. (1994). Identifying salient usability attributes automated telephone services. International
Conference Spoken Language Processing, ICSLP, pp. 1307{1310.
McKeown, K. R. (1985). Discourse strategies generating natural language text. Artificial
Intelligence, 27 (1), 1{42.
Mellish, C., Knott, A., Oberlander, J., & O'Donnell, M. (1998). Experiments using stochastic search text planning. Proceedings International Conference Natural
Language Generation, pp. 97{108.
Mohri, M., Pereira, F. C. N., & Riley, M. D. (1998). Fsm library { general purpose finitestate machine software tools..
Moore, J. D., & Paris, C. L. (1989). Planning text advisory dialogues. Proc. 27th
Annual Meeting Association Computational Linguistics.
Pollack, M., Hirschberg, J., & Webber, B. (1982). User participation reasoning process
expert systems. Proceedings First National Conference Artificial Intelligence,
pp. pp. 358{361.
Power, R. (1974). Computer Model Conversation. Ph.D. thesis, University Edinburgh.
414

fiReinforcement Learning ELVIS System

Rabiner, L. R., Juang, B. H., & Lee, C. H. (1996). overview automatic speech
recognition. Lee, C. H., Soong, F. K., & Paliwal, K. K. (Eds.), Automatic Speech
Speaker Recognition, Advanced Topics, pp. 1{30. Kluwer Academic Publishers.
Russell, S., & Norvig, P. (1995). Artificial Intelligence: Modern Approach. Prentiss Hall,
Englewood Cliffs, N.J.
Sanderman, A., Sturm, J., den Os, E., Boves, L., & Cremers, A. (1998). Evaluation
dutchtrain timetable information system developed ARISE project.
Interactive Voice Technology Telecommunications Applications, IVTTA, pp. 91{
96.
Seneff, S., Zue, V., Polifroni, J., Pao, C., Hetherington, L., Goddeau, D., & Glass, J. (1995).
preliminary development displayless PEGASUS system. ARPA Spoken
Language Technology Workshop.
Shriberg, E., Wade, E., & Price, P. (1992). Human-machine problem solving using spoken language systems (SLS): Factors affecting performance user satisfaction.
Proceedings DARPA Speech NL Workshop, pp. 49{54.
Simmons, R., & Slocum, J. (1975). Generating english discourse semantic networks.
CACM, 15 (10), 891{905.
Singh, S., Kearns, M. S., Litman, D. J., & Walker, M. A. (1999). Reinforcement learning
spoken dialogue systems. Proc. NIPS99.
Smith, R. W., & Hipp, D. R. (1994). Spoken Natural Language Dialog Systems: Practical
Approach. Oxford University Press.
Sparck-Jones, K. (1993). might summary?. Proceedings Information
Retrieval 93: Von der Modellierung zur Anwendung, pp. 9{26 Universitatsverlag Knstanz.
Sparck-Jones, K. (1999). Automatic summarizing; factors directions. Mani, I., &
Maybury, M. (Eds.), Advances Automatic Text Summarization. MIT Press.
Sparck-Jones, K., & Galliers, J. R. (1996). Evaluating Natural Language Processing Systems.
Springer.
Sproat, R., & Olive, J. (1995). approach text-to-speech synthesis. Kleijn, W. B.,
& Paliwal, K. K. (Eds.), Speech Coding Synthesis, pp. 611{633. Elsevier.
Sutton, R. S. (1991). Planning incremental dynamic programming. Proceedings Ninth
Conference Machine Learning, pp. 353{357. Morgan-Kaufmann.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning. MIT Press.
Tesauro, G. (1992). Practical Issues Temporal Difference Learning. Machine Learning,
8 (3{4), 257{277.
Walker, D. (1978). Understanding Spoken Language. Elsevier, North-Holland, New York.
415

fiWalker

Walker, M., Fromer, J., Fabbrizio, G. D., Mestel, C., & Hindle, D. (1998). say:
Evaluating spoken language interface email. Proceedings Conference
Computer Human Interaction (CHI 98), pp. 582{589.
Walker, M. A., Litman, D., Kamm, C. A., & Abella, A. (1997a). PARADISE: general
framework evaluating spoken dialogue agents. Proceedings 35th Annual
Meeting Association Computational Linguistics, ACL/EACL 97, pp. 271{
280.
Walker, M., Hindle, D., Fromer, J., Fabbrizio, G. D., & Mestel, C. (1997b). Evaluating
competing agent strategies voice email agent. Proceedings European
Conference Speech Communication Technology, EUROSPEECH97.
Walker, M. A. (1993). Informational Redundancy Resource Bounds Dialogue. Ph.D.
thesis, University Pennsylvania.
Walker, M. A. (1996). Effect Resource Limits Task Complexity Collaborative
Planning Dialogue. Artificial Intelligence Journal, 85 (1{2), 181{243.
Walker, M. A., Fromer, J. C., & Narayanan, S. (1998). Learning optimal dialogue strategies: case study spoken dialogue agent email. Proceedings 36th
Annual Meeting Association Computational Linguistics, COLING/ACL 98,
pp. 1345{1352.
Walker, M. A., Kamm, C. A., & Litman, D. J. (2000). Towards developing general models
usability PARADISE. Natural Language Engineering: Special Issue Best
Practice Spoken Dialogue Systems.
Walker, M. A., & Whittaker, S. (1990). Mixed initiative dialogue: investigation
discourse segmentation. Proc. 28th Annual Meeting ACL, pp. 70{79.
Watkins, C. J. (1989). Models Delayed Reinforcement Learning. Ph.D. thesis, Cambridge
University.
Webber, B., & Joshi, A. (1982). Taking initiative natural language database interaction: Justifying why. Coling 82, pp. 413{419.
Winograd, T. (1972). Understanding Natural Language. Academic Press, New York, N.Y.
Woods, W. A. (1984). Natural language communication machines: ongoing goal.
Reitman, W. (Ed.), Artificial Intelligence Applications Business, pp. 195{209.
Ablex Publishing Corp, Norwood, N.J.

416


