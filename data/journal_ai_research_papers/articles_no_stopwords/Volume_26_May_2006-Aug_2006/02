Journal Artificial Intelligence Research 26 (2006) 101-126

Submitted 8/05; published 5/06

Domain Adaptation Statistical Classifiers
Hal Daume III
Daniel Marcu

hdaume@isi.edu
marcu@isi.edu

Information Sciences Institute
University Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292 USA

Abstract
basic assumption used statistical learning theory training data
test data drawn underlying distribution. Unfortunately, many
applications, in-domain test data drawn distribution related,
identical, out-of-domain distribution training data. consider
common case labeled out-of-domain data plentiful, labeled in-domain data
scarce. introduce statistical formulation problem terms simple mixture
model present instantiation framework maximum entropy classifiers
linear chain counterparts. present efficient inference algorithms special
case based technique conditional expectation maximization. experimental
results show approach leads improved performance three real world tasks
four different data sets natural language processing domain.

1. Introduction
generalization properties current statistical learning techniques predicated
assumption training data test data come underlying
probability distribution. Unfortunately, many applications, assumption inaccurate.
often case plentiful labeled data exists one domain (or coming one
distribution), one desires statistical model performs well another related,
identical domain. Hand labeling data new domain costly enterprise, one
often wishes able leverage original, out-of-domain data building model
new, in-domain data. seek eliminate annotation in-domain
data, instead seek minimize amount new annotation effort required achieve
good performance. problem known domain adaptation transfer.
paper, present novel framework understanding domain adaptation
problem. key idea framework treat in-domain data drawn
mixture two distributions: truly in-domain distribution general domain
distribution. Similarly, out-of-domain data treated drawn mixture
truly out-of-domain distribution general domain distribution. apply
framework context conditional classification models conditional linear-chain
sequence labeling models, inference may efficiently solved using technique
conditional expectation maximization. apply model four data sets varying degrees divergence in-domain out-of-domain data obtain
c
2006
AI Access Foundation. rights reserved.

fiDaume III & Marcu

predictive accuracies higher large number baseline systems second
model proposed literature problem.
domain adaptation problem arises frequently natural language processing domain, millions dollars spent annotating text resources
morphological, syntactic semantic information. However, resources
based text news domain (in cases, Wall Street Journal). sort
language appears text Wall Street Journal highly specialized is,
circumstances, poor match domains. instance,
recent surge interest performing summarization (Elhadad, Kan, Klavans, & McKeown, 2005) information extraction (Hobbs, 2002) biomedical texts, summarization
electronic mail (Rambow, Shrestha, Chen, & Lauridsen, 2004), information extraction
transcriptions meetings, conversations voice-mail (Huang, Zweig, & Padmanabhan,
2001), among others. Conversely, machine translation domain, parallel
resources machine translation system depend parameter estimation drawn
transcripts political meetings, yet translation systems often targeted news
data (Munteanu & Marcu, 2005).

2. Statistical Domain Adaptation
multiclass classification problem, one typically assumes existence training set
= {(xn , yn ) X : 1 n N }, X input space finite set.
assumed (xn , yn ) drawn fixed, unknown base distribution p
training set independent identically distributed, given p. learning problem
find function f : X obtains high predictive accuracy (this typically
done either explicitly minimizing regularized empirical error, maximizing
probabilities model parameters).
2.1 Domain Adaptation
context domain adaptation, situation becomes complicated. assume
given two sets training data, (o) D(i) , out-of-domain indomain data sets, respectively. longer assume single fixed,
known distribution drawn, rather assume (o) drawn
distribution p(o) D(i) drawn distribution p(i) . learning problem
find function f obtains high predictive accuracy data drawn p (i) . (Indeed,
model turn symmetric respect (i) D(o) , contexts
consider obtaining good predictive model (i) makes intuitive sense.)
assume |D (o) | = N (o) |D(i) | = N (i) , typically N (i) N (o) .
before, assume N (o) out-of-domain data points drawn iid p(o)
N (i) in-domain data points drawn iid p(i) .
Obtaining good adaptation model requires careful modeling relationship
p(i) p(o) . two distributions independent (in obvious intuitive
sense), out-of-domain data (o) useless building model p(i) may
well ignore it. hand, p(i) p(o) identical, adaptation
necessary simply use standard learning algorithm. practical problems,
though, p(i) p(o) neither identical independent.
102

fiDomain Adaptation Statistical Classifiers

2.2 Prior Work
relatively little prior work problem, nearly focused
specific problem domains, n-gram language models generative syntactic parsing
models. standard approach used treat out-of-domain data prior knowledge
estimate maximum posterior values model parameters prior
distribution. approach applied successfully language modeling (Bacchiani
& Roark, 2003) parsing (Roark & Bacchiani, 2003). parsing domain, Hwa
(1999) Gildea (2001) shown simple techniques based using carefully chosen
subsets data parameter pruning improve performance adapted
parser. models assume data distribution p (D | ) parameters prior
distribution parameters p ( | ) hyper-parameters . estimate
hyperparameters out-of-domain data find maximum posteriori
parameters in-domain data, prior fixed.
context conditional discriminative models, domain adaptation
work aware model Chelba Acero (2004). model
uses out-of-domain data estimate prior distribution, context
maximum entropy model. Specifically, maximum entropy model trained
out-of-domain data, yielding optimal weights problem. weights used
mean weights Gaussian prior learned weights in-domain data.
Though effective experimentally, practice estimating prior distribution
out-of-domain data fixing estimation in-domain data leaves much
desired. Theoretically, strange estimate fix prior distribution data;
made apparent considering form models. Denoting in-domain data
parameters (i) , respectively, out-of-domain data parameters
D(o) , obtain following form prior estimation models:




= arg max p | arg max p () p




(o)



(i)
|
p |

(1)

One would difficult time rationalizing optimization problem anything
experimental performance. Moreover, models unusual
treat in-domain data out-of-domain data identically. Intuitively,
difference two sets data; simply come different, related distributions.
Yet, prior-based models highly asymmetric respect two data sets.
makes generalization one domain data set difficult. Finally,
see, model propose paper, alleviates problems,
outperforms experimentally.
second generic approach domain adaptation problem build
domain model use predictions features domain data.
successfully used context named entity tagging (?). approach attractive
makes assumptions underlying classifier; fact, multiple classifiers
used.
103

fiDaume III & Marcu

2.3 Framework
paper, propose following relationship in-domain out-ofdomain distributions. assume instead two underlying distributions,
actually three underlying distributions, denote q (o) , q (g) q (i) .
consider p(o) mixture q (o) q (g) , consider p(i) mixture q (i)
q (g) . One intuitively view q (o) distribution distribution data truly
out-of-domain, q (i) distribution data truly in-domain q (g) distribution
data general domains. Thus, knowing q (g) q (i) sufficient build
model in-domain data. out-of-domain data help us providing
information q (g) available considering in-domain data.
example, part-of-speech tagging, assignment tag determiner (DT)
word likely general decision, independent domain. However,
Wall Street Journal, monitor almost always verb (VB), technical documentation
likely noun. q (g) distribution account case the/DT,
q (o) account monitor/VB q (i) account monitor/NN.

3. Domain Adaptation Maximum Entropy Models
domain adaptation framework outlined Section 2.3 completely general
applied statistical learning model. section apply loglinear conditional maximum entropy models linear chain counterparts, since
models proved quite effective many learning tasks. first review maximum
entropy framework, extend domain adaptation problem; finally
discuss domain adaptation linear chain maximum entropy models.
3.1 Maximum Entropy Models
maximum entropy framework seeks conditional distribution p (y | x) closest
(in sense KL divergence) uniform distribution matches set training data respect feature function expectations (Della Pietra, Della Pietra, &
Lafferty, 1997). introducing one Lagrange multiplier feature function ,
optimization problem results probability distribution form:
p (y | x ; ) =

1
Z,x

h

exp > f (x, y)

(2)

P
Here, u> v denotes scalar product two vectors u v, given by: u> v = ui vi .
normalization constant Eq (2), Z,x , obtained summing exponential
possible classes 0 Y. probability distribution known exponential
distribution Gibbs distribution. learning (or optimization) problem find
vector maximizes likelihood Eq (2). practice, prevent over-fitting, one
typically optimizes penalized (log) likelihood, isotropic Gaussian prior mean
0 covariance matrix 2 placed parameters (Chen & Rosenfeld, 1999).
graphical model standard maximum entropy model depicted left
Figure 1. figure, circular nodes correspond random variables square nodes
104

fiDomain Adaptation Statistical Classifiers

correspond fixed variables. Shaded nodes observed training data empty
nodes hidden unobserved. Arrows denote conditional dependencies.
general, feature functions f (x, y) may arbitrary real-valued functions; however,
paper restrict attention binary features. practice, harsh
restriction: many problems natural language domain naturally employ binary
features (for real valued features, binning techniques applied). Additionally,
notational convenience, assume features (x, y) written product
form gi (y)hi (x) arbitrary binary functions g outputs binary features h
inputs. latter assumption means consider x binary vector
xi = hi (x); following simplify notation significantly (the extension full
case straightforward, messy, therefore considered remainder
paper). considering x vector, may move class dependence parameters
consider matrix y,i weight hi class y. write
refer column vector corresponding class y. x considered
column vector, write > x shorthand dot product x weights
class y. modified notation, may rewrite Eq (2) as:
p (y | x ; ) =

1
Z,x

h

exp > x

(3)

Combining Gaussian prior weights, obtain following form
log posterior data set:


N

h
X
X
1
yn > xn log
l = log p ( | D, ) = 2 > +
exp y0 > xn + const
2
0
n=1

(4)



parameters estimated using convex optimization technique; practice,
limited memory BFGS (Nash & Nocedal, 1991; Averick & More, 1994) seems good
choice (Malouf, 2002; Minka, 2003) use algorithm experiments
described paper. order perform calculations, one must able compute
gradient Eq (4) respect , available closed form.
3.2 Maximum Entropy Genre Adaptation Model
Extending maximum entropy model account in-domain out-of-domain
data framework described earlier requires addition several extra model param(i) (i)
eters. particular, in-domain data point (xn , yn ), assume existence
(i)
(i)
(i) (i)
binary indicator variable zn . value zn = 1 indicates (xn , yn ) drawn q (i)
(i)
(the truly in-domain distribution), value zn = 0 indicates drawn q (g)
(o) (o)
(the general-domain distribution). Similarly, out-of-domain data point (x n , yn ),
(o)
(o)
assume binary indicator variable zn , zn = 1 means data point drawn
(o)
q (the truly out-of-domain distribution) value 0 means drawn
q (g) (the general-domain distribution). course, indicator variables
observed data, must infer values automatically.
105

fiDaume III & Marcu

2

2





g

yni

yn
xn

xni

zni

N





yno

N

xno









zno



g





N





Figure 1: (Left) standard logistic regression model; (Right) Mega Model.
According model, zn binary random variables assume
drawn Bernoulli distribution parameter (i) (for in-domain) (o) (for outof-domain). Furthermore, assume three vectors, (i) , (o) (g)
corresponding q (i) , q (o) q (g) , respectively. instance, zn = 1, assume
(i)
xn classified using (i) . Finally, model binary vectors xn (respec(o)
tively xn s) drawn independently Bernoulli distributions parameterized
(i)
(g) (respectively, (o) (g) ). Again, zn = 1, assume xn
drawn according (i) . corresponds nave Bayes assumption generative
probabilities xn vectors. Finally, place common Beta prior nave Bayes
parameters, . Allowing range {i, o, g}, full hierarchical model is:
()

f | a, b
(i)
zn | (i)
(i)

(i)

(i)

(i)

(i)

(g)

xnf | zn , f , f
(i)

() | 2
(o)
zn | (o)

Bet(a, b)
Ber( (i) )
z (i)

Ber( fn )

(i)

(i)

yn | xn , zn , (i) , (g) Gibbs(xn , zn )

(o)

(o)

(o)

(o)

(o)

(g)

xnf | zn , f , f
(o)

Nor(0, 2 I)
Ber( (o) )

(5)

z (o)

Ber( fn )

(o)

(o)

yn | xn , zn , (o) , (g) Gibbs(xn , zn )

term model Maximum Entropy Genre Adaptation Model (the Mega
Model). corresponding graphical model shown right Figure 1. generative story in-domain data point x(i) follows:
1. Select whether x(i) truly in-domain general-domain indicate
z (i) {i, g}. Choose z (i) = probability (i) z (i) = g probability
1 (i) .
(i)

2. component f x(i) , choose xf 1 probability zf
z (i)

probability 1 f .
(i)

3. Choose class according Eq (3) using parameter vector z .
106

(i)

0

fiDomain Adaptation Statistical Classifiers

story out-of-domain data points identical, uses truly out-of-domain
general-domain parameters, rather truly in-domain parameters generaldomain parameters.
3.3 Linear Chain Models
straightforward extension maximum entropy classification model maximum
entropy Markov model (MEMM) (McCallum, Freitag, & Pereira, 2000) obtained
assuming targets yn sequences labels. canonical example model
part speech tagging: word sequence assigned part speech tag.
introducing first order Markov assumption tag sequence, one obtains linear chain
model viewed discriminative counterpart standard (generative)
hidden Markov model. parameters models estimated using
limited memory BFGS. extension Mega Model linear chain framework
similarly straightforward, assumption label (part speech tag)
indicator variable z (versus global indicator variable z entire tag sequence).
techniques described herein may applied conditional random field
framework Lafferty, McCallum, Pereira (2001), fixes bias problem
MEMM performing global normalization rather per-state normalization. is,
however, subtle difficulty direct application CRFs. Specifically, one would need
decide single z variable would assigned entire sentence, word
individually. MEMM case, natural one z per word. However,
CRF would computationally expensive. remainder, continue
use MEMM model efficiency purposes.

4. Conditional Expectation Maximization
Inference Mega Model slightly complex standard maximum entropy models. However, inference solved efficiently using conditional expectation
maximization (CEM), variant standard expectation maximization (EM) algorithm
(Dempster, Laird, & Rubin, 1977), due Jebara Pentland (1998). high level, EM
useful computing generative models hidden variables, CEM useful
computing discriminative models hidden variables; Mega Model belongs
latter family, CEM appropriate choice.
standard EM family algorithms maximizes joint likelihood data.
particular, (xn , yn )N
n=1 data z (discrete) hidden variable, M-step EM
proceeds maximizing bound given Eq (6)
log p (x, | ) = log

X
z

p (z, x, | ) = log Ezp( | x;) p (x, | z; )

(6)

Eq (6), Ez denotes expectation. One may apply Jensens inequality
equation, states f (E{x}) E{f (x)} whenever f convex. Taking f = log,
able decompose log expectation expectation log. typically
separates terms makes taking derivatives solving resolution optimization problem tractable. Unfortunately, EM cannot directly applied conditional models (such
107

fiDaume III & Marcu

Mega Model) form Eq (7) models result M-step
requires maximization equation form given Eq (8).
log p (y | x; ) = log
l = log

X
z

X
z

p (z, | x; ) = log Ezp( | x,) p (y | x, z; )
p (z, x, | ) log

X
z

p (z, x | )

(7)
(8)

Jensens inequality applied first term Eq (8), maximized
readily standard EM. However, applying Jensens inequality second term would
lead upper bound likelihood, since term appears negated.
conditional EM solution (Jebara & Pentland, 1998) bound change
log-likelihood iterations, rather log-likelihood itself. change loglikelihood written Eq (9), denotes parameters iteration t.


lc = log p | x; log p | x; t1

(9)

rewriting conditional distribution p (y | x) p (x, y) divided p (x),
express lc log joint distribution difference minus log marginal
distribution. Here, apply Jensens inequality first term (the joint difference),
second (because appears negated). Fortunately, Jensens
bound employ. standard variational upper bound logarithm function is:
log x x 1; leads lower bound negation, exactly desired.
bound attractive reasons: (1) tangent logarithm; (2) tight;
(3) makes contact current operating point (according maximization
previous time step); (4) simply linear function; (5) terminology
calculus variations, variational dual logarithm; see (Smith, 1998).
Applying Jensens inequality first term Eq (9) variational dual
second term, obtain change log-likelihood moving model parameters
t1 time 1 time (which shall denote Qt ) bounded l Qt ,
Qt defined Eq (10), h = E{z | x; } z = 1 1 E{z | x; }
z = 0, expectations taken respect parameters previous iteration.


P

X
p z, x, |
z p z, x |

+1
(10)
Q =
hz log
P
t1 )
p (z, x, | t1 )
z p (z, x |
zZ

applying two bounds (Jensens inequality variational bound),
removed sums logs, hard deal analytically. full derivation
given Appendix A. remaining expression lower bound change likelihood,
maximization result maximization likelihood.
MAP variant standard EM, change E-step priors
placed parameters. assumption standard EM wish maximize
p ( | x, y) p () p (y | , x) prior probability ignored, leaving
likelihood term parameters given data. MAP estimation, make
assumption instead use true prior p (). so, need add factor
log p () definition Qt Eq (10).
108

fiDomain Adaptation Statistical Classifiers

t1
jn,z
n

mt1
n


= log p xn , yn , zn | t1
n,zn

P
t1 1
=
n,zn ,f 0
z n p x n , zn |



xnf
1xnf
zn
zn

1


f =1
f
f

xnf
1xnf
Q
zn
zn

=
1


0
f 6=f
f
f

=

QF

Table 1: Notation used Mega Model equations.
important note although make use full joint distribution p (x, y, z),
objective function model conditional. joint distribution used
process creating bound: overall optimization maximize conditional likelihood labels given input. particular, bound using full joint likelihood
holds parameters marginal.

5. Parameter Estimation Mega Model
made explicit Eq (10), relevant distributions performing CEM full joint
distributions input variables x, output variables y, hidden variables z.
Additionally, require marginal distribution x variables z variables.
Finally, need compute expectations z variables. derive expectation
step section present final solution maximization step class
variables. derivation equations maximization given Appendix B.
Q bound complete conditional likelihood Mega Modelis given below:





P
(i)
(i)
(i)
(i)
(i)
z
,
x
p
p
z
,
x
,

(i)
X
n
n
n
n
n
z


+ 1
P n
h(i)
Qt =
n log
(i)
(i)
(i)
0 z (i) , x(i)
0
p z n , x n , yn
(i) p
n
n
n=1 z (i)
zn
n






P
(o)
(o) (o)
(o)
(o)
(o)
N
p zn , xn , yn
(o) p zn , xn
X X
z

+ 1

P n
h(o)
+
n log
(o)
(o)
(o)
(o)
(o)
0
0
p z n , x n , yn
z n , xn
(o) p
n=1 z (o)
z
N (i)
X



(11)

n

n

equation, p0 () probability distribution previous iteration. first
term Eq (11) bound in-domain data, second term bound
out-of-domain data. optimizations described section, nearly
identical terms in-domain parameters out-of-domain parameters. brevity,
explicitly write equations in-domain parameters; corresponding
out-of-domain equations easily derived these. Moreover, reduce notational
overload, elide superscripts denoting in-domain out-of-domain obvious
context. notational brevity, use notation depicted Table 1.
5.1 Expectation Step
E-step concerned calculating hn given current model parameters. Since zn
{0, 1}, easily find hn = p (zn = 1|), calculated follows:
109

fiDaume III & Marcu

p (zn = z | xn , yn , , , )
p (zn = z | ) p (xn | , zn = z) p (yn | , zn = z)
= P
z p (zn = z | ) p (xn | , zn = z) p (yn | , zn = z)

h
1
z (1 )1z n,z
exp zyn > xn
Zxn ,z

(12)

Here, Z partition function before. easily calculated z {0, 1}
expectation found dividing value z = 1 sum both.
5.2 M-Step
shown Appendix B.1, directly compute
value solving simple

quadratic equation. compute + a2 b, where:
=
b =
5.3 M-Step

PN

t1
n=1 2hn mn (n,0 n,1 )
PN
2 n=1 mt1
n (n,0 n,1 )
PN
n=1 hn
PN
t1
n=1 mn (n,0 n,1 )

1



Viewing Qt function , easy see optimization variable convex.
analytical solution available, gradient Qt respect (i)
seen identical gradient standard maximum entropy posterior, Eq (4),
data point weighted according posterior probability, (1 h n ). may
thus use identical optimization techniques computing optimal variables standard
maximum entropy models; difference data points weighted.
similar story holds (o) . case (g) , obtain standard maximum entropy
(i)
gradient, computed N (i) + N (o) data points, xn weighted hn
(o)
(o)
xn weighted hn . shown Appendix B.2.
5.4 M-Step
case , cannot obtain analytical solution finding maximizes
Qt . However, compute simple derivatives Qt respect single component
(i)
f maximized analytically. shown Appendix B.3, compute f

+ a2 b, where:
PN



n=1 1 hn + jn,0 (1 )n,0,f
=
P
2 N
n=1 jn,0 (1 )n,0,f
PN
(1 hn ) xnf
1+
b = PN n=1
n=1 jn,0 (1 )n,0,f

110



fiDomain Adaptation Statistical Classifiers

Algorithm MegaCEM
()
()
Initialize f = 0.5, f = 0, () = 0.5 {g, i, o} f .
parameters havent converged iterations remain
{- Expectation Step -}
n = 1..N (i)
(i)
Compute in-domain marginal probabilities, mn
(i)
Compute in-domain expectations, hn , Eq (12)
end
n = 1..N (o)
(o)
Compute out-of-domain marginal probabilities, mn
(o)
Compute out-of-domain expectations, hn Eq (12)
end
{- Maximization Step -}
Analytically update (i) (o) according equations shown Section 5.2
Optimize (i) , (o) (g) using BFGS
Iterations remain and/or havent converged
Update according derivation Section 5.4
end
end
return , ,
Figure 2: full training algorithm Mega Model.
case (o) identical. (g) , difference must replace
sum data points two sums, one in-domain out-of-domain
points; and, before, 1 hn must replaced hn ; made explicit
Appendix. Thus, optimize variables, simply iterate optimize
component analytically, given above, convergence.
5.5 Training Algorithm
full training algorithm depicted Figure 2. Convergence properties CEM
algorithm ensure converge (local) maximum posterior space. local
optima become problem practice, one alternatively use stochastic optimization
algorithm, temperature applied enabling optimization jump local
optima early on. However, explore idea work. context
application, extension required.
5.6 CEM Convergence
One immediate question conditional EM model described many
EM iterations required model converge. experiments, 5 iterations
111

fiDaume III & Marcu

Convergence CEM Optimization
22
20
18

Negative Log Likelihood (*1e6)

16
14
12
10
8
6
4
2
0

0

1

2
3
Number Iterations

4

5

Figure 3: Convergence training algorithm.

CEM sufficient, often 2 3 necessary. make clear,
Figure 3, plotted negative complete log likelihood model first
data set, described Section 6.2. three separate maximizations full
training algorithm (see Figure 2); first involves updating variables, second
involves optimizing variables third involves optimizing variables.
compute likelihood steps.
Running total 5 CEM iterations still relatively efficient model. dominating expense weighted maximum entropy optimization, which, 5 CEM iterations,
must computed 15 times (each iteration requires optimization three
sets variables). worst take 15 times amount time train model
complete data set (the union in-domain out-of-domain data), practice
resume optimization ending point previous iteration, causes
subsequent optimizations take much less time.
5.7 Prediction
training supplied us model parameters, subsequent task apply
parameters unseen data obtain class predictions. assume test data indomain (i.e., drawn either Q(i) Q(g) notation introduction),
obtain decision rule form given Eq (13) new test point x.

= arg max p (y | x; )
yY
X
= arg max
p (z | x; ) p (y | x, z; )
yY

= arg max
yY

z

X
z

p (z | ) p (x | z; ) p (y | x, z; )
112

fiDomain Adaptation Statistical Classifiers



= arg max

F




(g)

f

xf

(g)

1 f

1xf





h
(g)
exp > x

Zx,(g)
h



>x
F
xf
1xf exp (i)


(i)
(i)

+ (1 )
f
1 f
Zx,(i)
yY

f =1

(13)

f =1

Thus, decision rule simply select class highest probability according maximum entropy classifiers, weighted linearly marginal probabilities
new data point drawn Q(i) versus Q(g) . sense, model
seen linearly interpolating in-domain model general-domain model,
interpolation parameter input specific.

6. Experimental Results
section, describe result applying Mega Model several datasets
varying degrees divergence in-domain out-of-domain data. However,
describing data results, discuss systems compare.
6.1 Baseline Systems
Though little literature problem thus real systems
compare, several obvious baselines, describe section.
OnlyI: model obtained simply training standard maximum entropy model
in-domain data. completely ignores out-of-domain data serves
baseline case data unavailable.
OnlyO: model obtained training standard maximum entropy model
out-of-domain data, completely ignoring in-domain data. serves baseline
expected performance without annotating new data. gives sense close
out-of-domain distribution in-domain distribution.
LinI: model obtained linearly interpolating OnlyI OnlyO systems.
interpolation parameter estimated held-out (development) in-domain data.
means that, practice, extra in-domain data would need annotated order create
development set; alternatively, cross-validation could used.
Mix: model obtained training maximum entropy model union
out-of-domain in-domain data sets.
MixW: model obtained training maximum entropy model union
out-of-domain in-domain data sets, out-of-domain data downweighted effectively equinumerous in-domain data.
Feats: model uses out-of-domain data build one classifier uses
classifiers predictions features in-domain data, described ? (?).
113

fiDaume III & Marcu

Prior: adaptation model described Section 2.2, out-of-domain
data used estimate prior in-domain classifier. case maximum
entropy models consider here, weights learned out-of-domain data used
mean Gaussian prior distribution placed weights training
in-domain data, described Chelba Acero (2004).
cases, tune model hyperparameters using performance development data.
development data taken random 20% training data cases.
appropriate hyperparameters found, 20% folded back training set.
6.2 Data Sets
evaluate models three different problems. first two problems come
Automatic Content Extraction (ACE) data task. data selected ACE
program specifically looks data different domains. third problem
tackled Chelba Acero (2004), required annotate data themselves.
6.2.1 Mention Type Classification
first problem, Mention Type, subcomponent entity mention detection
task (an extension named entity tagging task, wherein pronouns nominals
marked, addition simple names). assume extents mentions
marked simply need identify type, one of: Person, Geo-political Entity,
Organization, Location, Weapon Vehicle. out-of-domain data, use newswire
broadcast news portions ACE 2005 training data; in-domain data, use
Fisher conversations data. example out-of-domain sentence is:
again, prime battleground constitutional allocation power
nom
nam
federal governmentnom
gpe statesgpe , Congressorg
bar
federal regulatory agenciesorg .
example in-domain sentence is:
nom
pro
nom
mypro
per wifeper Iper transported across continent gpe
whq pro
whereloc Iper born

use 23k out-of-domain examples (each mention corresponds one example), 1k
in-domain examples 456 test examples. Accuracy computed 0/1 loss. use
standard feature functions employed named entity models, include lexical items,
stems, prefixes suffixes, capitalization patterns, part-of-speech tags, membership
information gazetteers locations, businesses people. accuracies reported
result running ten fold cross-validation.
6.2.2 Mention Tagging
second problem, Mention Tagging precursor Mention Type task,
attempt tag entity mentions raw text. use standard Begin/In/Out
encoding use maximum entropy Markov model perform tagging (McCallum
et al., 2000). out-of-domain data, use newswire broadcast news
114

fiDomain Adaptation Statistical Classifiers

data; in-domain data, use broadcast news data transcribed
automatic speech recognition. in-domain data lacks capitalization, punctuation, etc.,
contains transcription errors (speech recognition word error rate approximately
15%). tagging task, 112k out-of-domain examples (in context tagging,
example single word), 5k in-domain examples 11k test examples.
Accuracy F-measure across segmentation. use features mention
type identification task. scores reported ten fold cross-validation.
6.2.3 Recapitalization
final problem, Recap, task recapitalizing text. Following Chelba Acero
(2004), use maximum entropy Markov model, possible tags are:
Lowercase, Capitalized, Upper Case, Punctuation Mixed case. out-of-domain
data task comes Wall Street Journal, two separate in-domain data sets
come broadcast news text CNN/NPR ABC Primetime, respectively. use
3.5m out-of-domain examples (one example one word). CNN/NPR data, use
146k in-domain training examples 73k test examples; ABC Primetime data,
use 33k in-domain training examples 8k test examples. use identical features
Chelba Acero (2004). order maintain comparability results described
Chelba Acero (2004), perform cross-validation experiments: use
train/test split described paper.
6.3 Feature Selection
maximum entropy models used classification adept dealing
many irrelevant and/or redundant features, nave Bayes generative model, use
model distribution input variables, overfit features. turned
problem Mention Type Mention Tagging problems,
Recap problems, caused errors. alleviate problem, Recap
problem only, applied feature selection algorithm features used nave
Bayes model (the entire feature set used maximum entropy model). Specifically,
took 10k top features according information gain criteria predict indomain versus out-of-domain (as opposed feature selection class label); Forman
(2003) provides overview different selection techniques.1
6.4 Results
results shown Table 2, see training in-domain data
always outperforms training out-of-domain data. linearly interpolated model
improve base models significantly. Placing data one bag helps,
clear advantage re-weighting domain data. Prior model
Feats model perform roughly comparably, Prior model edging
small margin.2 model outperforms Prior model Feats model.
1. value 10k selected arbitrarily initial run model development data;
tuned optimize either development test performance.
2. numbers result Prior model data Chelba Acero (2004) differ slightly
reported paper. two potential reasons this. First, numbers

115

fiDaume III & Marcu

|D(o) |

|D(i) |
Accuracy
OnlyO
OnlyI
LinI
Mix
MixW
Feats
Prior
MegaM
% Reduction
Mix
Prior

Mention
Type
23k
1k

Mention
Tagging
112k
5k

Recap
ABC
3.5m
8k

Recap
CNN
3.5m
73k

Average
-

57.6
81.2
81.5
84.9
81.3
87.8
87.9
92.1

78.3
83.5
83.8
80.9
81.0
84.2
85.1
88.2

95.5
97.4
97.7
96.4
97.6
97.8
97.9
98.1

94.6
94.7
94.9
95.0
93.5
96.1
95.9
96.8

81.5
89.2
89.5
89.3
88.8
91.5
91.7
93.9

47.7
34.7

38.2
20.8

52.8
19.0

36.0
22.0

43.0
26.5

Table 2: Experimental results; first set rows show sizes in-domain
out-of-domain training data sets. second set rows (Accuracy) show
performance various models four tasks. last two rows (%
Reduction) show percentage reduction error rate using Mega Model
baseline model (Mix) best alternative method (Prior).

applied McNemars test (Gibbons & Chakraborti, 2003, section 14.5) gage statistical significance results, comparing results Prior model
Mega Model (for mention tagging experiment, compute McNemars test simple
Hamming accuracy rather F-score; suboptimal, know
compute statistical significance F-score). mention type task, difference
statistical significant p 0.03 level; mention tagging task, p 0.001;
recapitalization tasks, difference ABC data significant p 0.06
level, CNN/NPR data significant p 0.004 level.
mention type task, improved baseline model trained in-domain
data accuracy 81.2% 92.1%, relative improvement 13.4%. mention
tagging, improve 83.5% F-measure 88.2%, relative improvement 5.6%.
ABC recapitalization task (for much in-domain data available), increase
performance 95.5% 98.1%, relative improvement 2.9%. CNN/NPR
recapitalization task (with little in-domain data), increase performance 94.6%
96.8%, relative improvement 2.3%.

reported based using 20m examples; consider 3.5m example case. Second,
likely subtle differences training algorithms used. Nevertheless, whole, relative
improvements agree paper.

116

fiDomain Adaptation Statistical Classifiers

Mention Type Identification Task

Mention Tagging Task

90

95

OnlyOut
Chelba
MegaM

90

OnlyOut
Chelba
MegaM

85
85

80

Accuracy

Fmeasure

80

75

75
70

65

70
60

65
0
10

1

10

2

3

10
10
Amount Domain Data Used (log scale)

55
1
10

4

10

2

10
Amount Domain Data Used (log scale)

Figure 4: Learning curves Prior MegaM models.
6.5 Learning Curves
particular interest amount annotated in-domain data needed see marked
improvement OnlyO baseline well adapted system. show Figure 4
learning curves Mention Type Mention Tagging problems. Along x-axis,
plot amount in-domain data used; along y-axis, plot accuracy. plot
three lines: flat line OnlyO model use in-domain data,
curves Prior MegaM models. see, model maintains accuracy
models, Prior curve actually falls baseline
type identification task.3

7. Model Introspection
seen previous sections Mega Model routinely outperforms competing models. Despite clear performance improvement, question remains open regarding
internal workings models. (i) variable captures degree indomain data set truly in-domain. z variables model aim capture,
test data point, whether general domain in-domain. section, discuss
particular values parameters model learns variables.
present two analyses. first (Section 7.1), inspect models inner workings
Mention Type task Section 6.2.1. analysis, look specifically
expected values hidden variables found model. second analysis
(Section 7.2), look ability model judge degree relatedness, defined
variables.
3. Fisher data personal conversations. hence much higher degree first
second person pronouns news. (The baseline always guesses person achieves 77.8%
accuracy.) able intelligently use out-of-domain data in-domain model
unsure, performance drops, observed Prior model.

117

3

10

fiDaume III & Marcu

Pre-context
home trenton
veterans administration
know american
gives
capable getting
fisher thing calling


. . . Entity . . .
. . . new jersey . . .
. . . hospital . . .
. . . government. . .
...

...
. . . anything . . .
...

...
...
kid
...

Post-context
thats

chills

ha ha screwed


True
GPE
ORG
ORG
PER
WEA
PER
PER

Hyp
GPE
LOC
ORG
PER
PER
PER
PER

p (z = I)
0.02
0.11
0.17
0.71
0.92
0.93
0.98

Table 3: Examples test data Mention Type task. True column
correct entity type Hyp column models prediction. final
column probability example truly in-domain model.

7.1 Model Expectations
focus discussion, consider Mention Type task, Section 6.2.1.
Table 3, shown seven test-data examples Mention Type task. Precontext text appears entity post-context text
appears after. report true class class model hypothesizes. Finally,
report probability example truly in-domain, according model.
see, three examples model thinks general domain new
jersey, hospital government. believes me, anything kid
in-domain. general, probabilities tend skewed toward 0 1,
uncommon nave Bayes models. shown two errors data. first,
model thinks hospital location truly organization.
difficult distinction make: training data, hospitals often used locations.
second example error anything capable getting anything
here. long-distance context example discussion biological warfare
Saddam Hussein, anything supposed refer type biological warhead.
model mistakingly thinks person. error likely due fact
model identifies word anything likely truly in-domain (the word
common newswire). learned truly in-domain entities people.
Thus, lacking evidence otherwise, model incorrectly guesses anything person.
interesting observe model believes entity gives
chills closer general domain fisher thing calling ha ha
screwed up. likely occurs context ha ha occurred anywhere
out-of-domain training data, twice in-domain training data. unlikely
example would misclassified otherwise (me fairly clearly person),
example shows model able take context account deciding domain.
decisions made model, shown Table 3 seem qualitatively reasonable.
numbers perhaps excessively skewed, ranking believable. in-domain
data primarily conversations random (not necessarily news worthy) topics,
hence highly colloquial. Contrastively, out-of-domain data formal news.
model able learn entities new jersey government
news words kid.
118

fiDomain Adaptation Statistical Classifiers

(i)
(o)

Mention
Type
0.14
0.11

Mention
Tagging
0.41
0.45

Recap
CNN
0.36
0.40

Recap
ABC
0.51
0.69

Table 4: Values variables discovered Mega Model algorithm.
7.2 Degree Relatedness
section, analyze values found model. Low values (i)
(o) mean in-domain data significantly different out-of-domain data;
high values mean similar. high value means
general domain model used cases. tasks Mention Type,
values middling around 0.4. Mention Type, (i) 0.14 (o) 0.11,
indicating significant difference in-domain out-of-domain
data. exact values tasks shown Table 4.
values make intuitive sense. distinction conversation data
news data (for Mention Type task) significantly stronger difference
manually automatically transcribed newswire (for Mention Tagging task).
values reflect qualitative distinction. rather strong difference
values recapitalization tasks expected priori. However, post hoc
analysis shows result reasonable. compute KL divergence unigram
language model out-of-domain data set in-domain data sets.
KL divergence CNN data 0.07, divergence ABC data 0.11.
confirms ABC data perhaps different baseline out-of-domain
CNN data, reflected values.
interested cases little difference in-domain
out-of-domain data. simulate case, performed following experiment.
consider Mention Type task, use training portion out-ofdomain data. randomly split data half, assigning half in-domain
out-of-domain. theory, model learn may rely general
domain model. performed experiment ten fold cross-validation found
average value selected model 0.94. strictly less
one, show model able identify similar domains.

8. Conclusion Discussion
paper, presented Mega Model domain adaptation discriminative (conditional) learning framework. described efficient optimization algorithms
based conditional EM technique. experimentally shown, four data sets,
model outperforms large number baseline systems, including current state
art model, requiring significantly less in-domain data.
Although focused specifically discriminative modeling maximum entropy
framework, believe novel, basic idea work foundedto break
in-domain distribution p(i) out-of-domain distribution p(o) three distributions, q (i) ,
119

fiDaume III & Marcu

q (o) q (g) general. particular, one could perform similar analysis case
generative models obtain similar algorithms (though case generative model,
standard EM could used). model could applied domain adaptation
language modeling machine translation.
exception work described Section 2.2, previous work in-domain adaptation quite rare, especially discriminative learning framework. substantial literature language modeling/speech community, adaptation
concerned based adapting new speakers (Iyer, Ostendorf, & Gish,
1997; Kalai, Chen, Blum, & Rosenfeld, 1999). learning perspective, Mega
Model similar mixture experts model. model seen constrained experts model, three experts, constraints specify in-domain
data come one two experts, out-of-domain data come
one two experts (with single expert overlapping two). attempts
build discriminative mixture experts models make heuristic approximations order
perform necessary optimization (Jordan & Jacobs, 1994), rather apply conditional
EM, gives us strict guarantees monotonically increase data (incomplete)
log likelihood iteration training.
domain adaptation problem closely related multitask learning (also known
learning learn inductive transfer). multitask learning, one attempts learn
function solves many machine learning problems simultaneously. related problem
discussed Thrun (1996), Caruana (1997) Baxter (2000), among others.
similarity multitask learning domain adaptation deal
data drawn related, distinct distributions. primary difference domain
adaptation cares predicting one label type, multitask learning cares
predicting many.
various sub-communities natural language processing family begin continue branch domains newswire, importance developing models
new domains without annotating much new data become important.
Mega Model first step toward able migrate simple classification-style models (classifiers maximum entropy Markov models) across domains. Continued research
area adaptation likely benefit work done active learning
learning large amounts unannotated data.

Acknowledgments
thank Ciprian Chelba Alex Acero making data available. thank Ryan
McDonald pointing Feats baseline, previously considered.
thank Kevin Knight Dragos Munteanu discussions related project.
paper greatly improved suggestions reviewers, including reviewers
previous, shorter version. work partially supported DARPA-ITO grant N6600100-1-9814, NSF grant IIS-0097846, NSF grant IIS-0326276, USC Dean Fellowship
Hal Daume III.
120

fiDomain Adaptation Statistical Classifiers

Appendix A. Conditional Expectation Maximization
appendix, derive Eq (10) Eq (7) making use Jensens inequality
variational bound. interested reader referred work Jebara Pentland
(1998) details. discussion consider bound change log
likelihood iteration 1 iteration t, l c , given Eq (14):



p | x;
p x, | /p |
l = log
= log
p (y | x; t1 )
p (x, | t1 ) /p (y | t1 )



p x, y;
p x;
= log
log
p (x, y; t1 )
p (x; t1 )
c

(14)
(15)

Here, effectively rewritten log-change ratio conditionals
difference log-change ratio joints log-change ratio
marginals. may rewrite Eq (15) introducing hidden variables z as:


P
P


z p x, z;
z p x, y, z;
log P
l = log P
t1 )
t1 )
z p (x, y, z;
z p (x, z;
c

(16)

apply Jensens inequality first term Eq (16) obtain:

c

l

X
z

"

#


P

p x, y, z | t1
p x, y, z;
z p x, z;
P
log
log P
t1 )
t1 )
p (x, y, z; t1 )
z 0 p (x, y, z |
z p (x, z;
|
{z
}

(17)

hx,y,z,t1

Eq (17), expression denoted hx,y,z,t1 joint expectation z
previous iterations parameter settings. Unfortunately, cannot apply Jensens inequality remaining term Eq (17) appears negated. applying
variational dual (log x x 1) term, obtain following, final bound:
c



l Q =

X
z



P

p x, y, z;
z p x, z;
+1
hx,y,z,t1 log
P
t1 )
p (x, y, z; t1 )
z p (x, z;

(18)

Applying bound Eq (18) distributions chosen model yields Eq (10).

Appendix B. Derivation Estimation Equations
Given model structure parameterization Mega Modelgiven Section 3.2,
Eq (5), obtain following expression joint probability data:
121

fiDaume III & Marcu



p x, y, z | () , () ,


F
N



zn
zn
Ber(xnf | f )Gibbs(yn | xn , )
Ber(zn | )
=


n=1
f =1



F
N

1xnf
xnf


=
1 zfn
zfn
zn (1 )1zn

n=1
f =1
!
h
X
1
h
exp zynn > xn
exp zcn > xn

c

(19)

marginal distribution obtained removing last two terms (the exp
sum exps) final equation. Plugging Eq (19) Eq (10) using notation
Eq (12), obtain following expression Qt :
Qt =

X


+

N
X

n=1



log Nor(() ; 0, 2 I) +
"

X
zn

(

F
X

f =1



()

log Bet( f ; a, b)

hn zn log + (1 zn ) log(1 ) + log n,zn
+

F
X

xnf log zynn

f =1

zn
mt1
n (1

)

1zn

log

X
c

n,zn + 1

#

exp

h

zcn > xn






jn,z
n

)
(20)

well analogous term out-of-domain data. j defined Table 1.
B.1 M-Step
computing , simply differentiate Qt (see Eq (20)) respect , obtaining:
N

Qt X hn 1 hn
=
+
+ mt1
n (n,0 n,1 )


1

(21)

n=1

solving 0 leads directly quadratic expression form:

0 =

2

"
"

N
X

mt1
n (n,0

n=1

+ 1 1 +

N
X

n=1

n,1 )

#

2hn mt1
n (n,0 n,1 )
122



#

fiDomain Adaptation Statistical Classifiers

+

0

"



N
X

n=1

hn

#

(22)

Solving directly gives desired update equation.
B.2 M-Step
optimizing (i) , rewrite Qt , Eq (20), neglecting irrelevant terms, as:

Qt [] =

N
X

n=1

(1 hn )


F
X


f =1

xnf yn ,f log

X
c

h

exp c > xn





+ log Nor(; 0, 2 I)

(23)

Eq (23), bracketed expression exactly log-likelihood term obtained
standard logistic regression models. Thus, optimization Q respect (i)
(o) performed using weighted version standard logistic regression optimization,
weights defined (1hn ). case (g) , obtain weighted logistic regression
model, N (i) + N (o) data points, weights defined hn .
B.3 M-Step
case (i) (o) , rewrite Eq (20) remove irrelevant terms, as:
Qt [ (i) ] =

F
X

f =1

log Bet(f ; a, b) +

N
X


n=1

(1 hn ) log n,0 mt1
n (1 )n,0



(24)

Due presence product term , cannot compute analytical solution
maximization problem. However, take derivatives component-wise (in F )
obtain analytical solutions (when combined prior). admits iterative
solution maximizing Qt maximizing component separately convergence.
Computing derivatives Qt respect f requires differentiating n,0 respect
f ; convenient form (recalling notation Table 1:


n,0 = [n,0,f ]
{xnf f + (1 xnf )(1 f )} = n,0,f
f
f

(25)

Using result, maximize Qt respect f solving:
"
N
X
xnf (1 f ) (1 xnf )f
h
(1 hn )
Qf
=
f
f (1 f )
n=1
#

(26)

1
f (1 f )
#
N
X
jn,0 (1 )n,0,f
f )

jn,0 (1 )n,0,f +

=

"
N
X
1
(1 hn ) (xnf
1+
f (1 f )
n=1

123

n=1

fiDaume III & Marcu

Equating zero yields quadratic expression form:

0 = (f )2
+ (f )

1

"

N
X

jn,0 (1
n=1
" N
X


"

)n,0,f

#

1 hn + jn,0 (1 )n,0,f

n=1
N
X

+ (f )0 1 +

n=1

(1 hn ) xnf

#



#
(27)
(o)

final equation solved analytically. similar expression arises f .
(g)

case f , obtain quadratic form sums entire data set
hn replacing occurrences (1 hn ):

0 =

+

+






N (i)
2 X
(i)
(i)
(g)

jn,1 (i)


n,1,f

f

n=1

n=1

(g)

f

1

"




(g) 0

f

+

(o)
N
X

N (i)



X

n=1

X

n=1

jn,1 (o) n,1,f


(i)
h(i)
n + jn,1 n,1,f

N (i)

1+

(i)

(i)

(o)

(o)

N (o)
(i)

h(i)
n xnf +

X

n=1



(o)




h(o)
n xnf

(o)
N
X

n=1

(o)

(o)

(o)
h(o)
n + jn,1 n,1,f



#
(28)

Again, solved analytically. values j, m, , ,, defined Table 1.

References
Averick, B. M., & More, J. J. (1994). Evaluation large-scale optimization problems
vector parallel architectures. SIAM Journal Optimization, 4.
Baxter, J. (2000). model inductive bias learning. Journal Artificial Intelligence
Research, 12 , 149198.
Bacchiani, M., & Roark, B. (2003). Unsupervised langauge model adaptation. Proceedings
International Conference Acoustics, Speech Signal Processing (ICASSP).
Caruana, R. (1997). Multitask learning: knowledge-based source inductive bias. Machine Learning, 28 , 4175.
Chelba, C., & Acero, A. (2004). Adaptation maximum entropy classifier: Little data
help lot. Proceedings Conference Empirical Methods Natural
Language Processing (EMNLP), Barcelona, Spain.
Chen, S., & Rosenfeld, R. (1999). Gaussian prior smoothing maximum entropy
models. Tech. rep. CMUCS 99-108, Carnegie Mellon University, Computer Science
Department.
124

fiDomain Adaptation Statistical Classifiers

Della Pietra, S., Della Pietra, V. J., & Lafferty, J. D. (1997). Inducing features random
fields. IEEE Transactions Pattern Analysis Machine Intelligence, 19 (4), 380
393.
Dempster, A., Laird, N., & Rubin, D. (1977). Maximum likelihood incomplete data
via EM algorithm. Journal Royal Statistical Society, B39.
Elhadad, N., Kan, M.-Y., Klavans, J., & McKeown, K. (2005). Customization unified
framework summarizing medical literature. Journal Artificial Intelligence
Medicine, 33 (2), 179198.
Forman, G. (2003). extensive empirical study feature selection metrics text classification. Journal Machine Learning Research, 3, 12891305.
Gibbons, J. D., & Chakraborti, S. (2003). Nonparametric Statistical Inference. Marcel
Dekker, Inc.
Gildea, D. (2001). Corpus variation parser performance. Proceedings Conference Empirical Methods Natural Language Processing (EMNLP).
Hobbs, J. R. (2002). Information extraction biomedical text. Journal Biomedical
Informatics, 35 (4), 260264.
Huang, J., Zweig, G., & Padmanabhan, M. (2001). Information extraction voicemail.
Proceedings Conference Association Computational Linguistics
(ACL).
Hwa, R. (1999). Supervised grammar induction using training data limited constituent
information. Proceedings Conference Association Computational
Linguistics (ACL), pp. 7379.
Iyer, R., Ostendorf, M., & Gish, H. (1997). Using out-of-domain data improve in-domain
language models. IEEE Signal Processing, 4 (8).
Jebara, T., & Pentland, A. (1998). Maximum conditional likelihood via bound maximization
CEM algorithm. Advances Neural Information Processing Systems
(NIPS).
Jordan, M., & Jacobs, R. (1994). Hierarchical mixtures experts EM algorithm.
Neural Computation, 6, 181214.
Kalai, A., Chen, S., Blum, A., & Rosenfeld, R. (1999). On-line algorithms combining
language models. ICASSP.
Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional random fields: Probabilistic
models segmenting labeling sequence data. Proceedings International
Conference Machine Learning (ICML).
Malouf, R. (2002). comparison algorithms maximum entropy parameter estimation.
Proceedings CoNLL.
McCallum, A., Freitag, D., & Pereira, F. (2000). Maximum entropy Markov models
information extraction segmentation. Proceedings International Conference Machine Learning (ICML).
125

fiDaume III & Marcu

Minka, T. P. (2003). comparison numerical optimizers logistic regression. http:
//www.stat.cmu.edu/~minka/papers/logreg/.
Munteanu, D., & Marcu, D. (2005). Improving machine translation performance exploiting non-parallel corpora. Computational Linguistics, appear.
Nash, S., & Nocedal, J. (1991). numerical study limited memory BFGS method
truncated Newton method large scale optimization. SIAM Journal
Optimization, 1, 358372.
Rambow, O., Shrestha, L., Chen, J., & Lauridsen, C. (2004). Summarizing email threads.
Proceedings Conference North American Chapter Association
Computational Linguistics (NAACL) Short Paper Section.
Roark, B., & Bacchiani, M. (2003). Supervised unsupervised PCFG adaptation
novel domains. Proceedings Conference North American Chapter
Association Computational Linguistics Human Language Technology
(NAACL/HLT).
Smith, D. R. (1998). Variational Methods Optimization. Dover Publications, Inc., Mineola, New York.
Thrun, S. (1996). learning n-th thing easier learning first. Advances
Neural Information Processing Systems (NIPS).

126


