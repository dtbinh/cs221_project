journal artificial intelligence

submitted published

domain adaptation statistical classifiers
hal daume iii
daniel marcu

hdaume isi edu
marcu isi edu

information sciences institute
university southern california
admiralty way suite
marina del rey ca usa

abstract
basic assumption used statistical learning theory training data
test data drawn underlying distribution unfortunately many
applications domain test data drawn distribution related
identical domain distribution training data consider
common case labeled domain data plentiful labeled domain data
scarce introduce statistical formulation terms simple mixture
model present instantiation framework maximum entropy classifiers
linear chain counterparts present efficient inference special
case technique conditional expectation maximization experimental
leads improved performance three real world tasks
four different data sets natural language processing domain

introduction
generalization properties current statistical learning techniques predicated
assumption training data test data come underlying
probability distribution unfortunately many applications assumption inaccurate
often case plentiful labeled data exists one domain coming one
distribution one desires statistical model performs well another related
identical domain hand labeling data domain costly enterprise one
often wishes able leverage original domain data building model
domain data seek eliminate annotation domain
data instead seek minimize amount annotation effort required achieve
good performance known domain adaptation transfer
present novel framework understanding domain adaptation
key idea framework treat domain data drawn
mixture two distributions truly domain distribution general domain
distribution similarly domain data treated drawn mixture
truly domain distribution general domain distribution apply
framework context conditional classification conditional linear chain
sequence labeling inference may efficiently solved technique
conditional expectation maximization apply model four data sets varying degrees divergence domain domain data obtain
c

ai access foundation rights reserved

fidaume iii marcu

predictive accuracies higher large number baseline systems second
model proposed literature
domain adaptation arises frequently natural language processing domain millions dollars spent annotating text resources
morphological syntactic semantic information however resources
text news domain cases wall street journal sort
language appears text wall street journal highly specialized
circumstances poor match domains instance
recent surge interest performing summarization elhadad kan klavans mckeown information extraction hobbs biomedical texts summarization
electronic mail rambow shrestha chen lauridsen information extraction
transcriptions meetings conversations voice mail huang zweig padmanabhan
among others conversely machine translation domain parallel
resources machine translation system depend parameter estimation drawn
transcripts political meetings yet translation systems often targeted news
data munteanu marcu

statistical domain adaptation
multiclass classification one typically assumes existence training set
xn yn x n n x input space finite set
assumed xn yn drawn fixed unknown base distribution p
training set independent identically distributed given p learning
function f x obtains high predictive accuracy typically
done explicitly minimizing regularized empirical error maximizing
probabilities model parameters
domain adaptation
context domain adaptation situation becomes complicated assume
given two sets training data domain indomain data sets respectively longer assume single fixed
known distribution drawn rather assume drawn
distribution p drawn distribution p learning
function f obtains high predictive accuracy data drawn p indeed
model turn symmetric respect contexts
consider obtaining good predictive model makes intuitive sense
assume n n typically n n
assume n domain data points drawn iid p
n domain data points drawn iid p
obtaining good adaptation model requires careful modeling relationship
p p two distributions independent obvious intuitive
sense domain data useless building model p may
well ignore hand p p identical adaptation
necessary simply use standard learning practical
though p p neither identical independent


fidomain adaptation statistical classifiers

prior work
relatively little prior work nearly focused
specific domains n gram language generative syntactic parsing
standard used treat domain data prior knowledge
estimate maximum posterior values model parameters prior
distribution applied successfully language modeling bacchiani
roark parsing roark bacchiani parsing domain hwa
gildea shown simple techniques carefully chosen
subsets data parameter pruning improve performance adapted
parser assume data distribution p parameters prior
distribution parameters p hyper parameters estimate
hyperparameters domain data maximum posteriori
parameters domain data prior fixed
context conditional discriminative domain adaptation
work aware model chelba acero model
uses domain data estimate prior distribution context
maximum entropy model specifically maximum entropy model trained
domain data yielding optimal weights weights used
mean weights gaussian prior learned weights domain data
though effective experimentally practice estimating prior distribution
domain data fixing estimation domain data leaves much
desired theoretically strange estimate fix prior distribution data
made apparent considering form denoting domain data
parameters respectively domain data parameters
obtain following form prior estimation




arg max p arg max p p










p



one would difficult time rationalizing optimization anything
experimental performance moreover unusual
treat domain data domain data identically intuitively
difference two sets data simply come different related distributions
yet prior highly asymmetric respect two data sets
makes generalization one domain data set difficult finally
see model propose alleviates
outperforms experimentally
second generic domain adaptation build
domain model use predictions features domain data
successfully used context named entity tagging attractive
makes assumptions underlying classifier fact multiple classifiers
used


fidaume iii marcu

framework
propose following relationship domain ofdomain distributions assume instead two underlying distributions
actually three underlying distributions denote q q g q
consider p mixture q q g consider p mixture q
q g one intuitively view q distribution distribution data truly
domain q distribution data truly domain q g distribution
data general domains thus knowing q g q sufficient build
model domain data domain data help us providing
information q g available considering domain data
example part speech tagging assignment tag determiner dt
word likely general decision independent domain however
wall street journal monitor almost verb vb technical documentation
likely noun q g distribution account case dt
q account monitor vb q account monitor nn

domain adaptation maximum entropy
domain adaptation framework outlined section completely general
applied statistical learning model section apply loglinear conditional maximum entropy linear chain counterparts since
proved quite effective many learning tasks first review maximum
entropy framework extend domain adaptation finally
discuss domain adaptation linear chain maximum entropy
maximum entropy
maximum entropy framework seeks conditional distribution p x closest
sense kl divergence uniform distribution matches set training data respect feature function expectations della pietra della pietra
lafferty introducing one lagrange multiplier feature function
optimization probability distribution form
p x


z x

h

exp f x



p
u v denotes scalar product two vectors u v given u v ui vi
normalization constant eq z x obtained summing exponential
possible classes probability distribution known exponential
distribution gibbs distribution learning optimization
vector maximizes likelihood eq practice prevent fitting one
typically optimizes penalized log likelihood isotropic gaussian prior mean
covariance matrix placed parameters chen rosenfeld
graphical model standard maximum entropy model depicted left
figure figure circular nodes correspond random variables square nodes


fidomain adaptation statistical classifiers

correspond fixed variables shaded nodes observed training data empty
nodes hidden unobserved arrows denote conditional dependencies
general feature functions f x may arbitrary real valued functions however
restrict attention binary features practice harsh
restriction many natural language domain naturally employ binary
features real valued features binning techniques applied additionally
notational convenience assume features x written product
form gi hi x arbitrary binary functions g outputs binary features h
inputs latter assumption means consider x binary vector
xi hi x following simplify notation significantly extension full
case straightforward messy therefore considered remainder
considering x vector may move class dependence parameters
consider matrix weight hi class write
refer column vector corresponding class x considered
column vector write x shorthand dot product x weights
class modified notation may rewrite eq
p x


z x

h

exp x



combining gaussian prior weights obtain following form
log posterior data set


n

h
x
x

yn xn log
l log p
exp xn const


n





parameters estimated convex optimization technique practice
limited memory bfgs nash nocedal averick seems good
choice malouf minka use experiments
described order perform calculations one must able compute
gradient eq respect available closed form
maximum entropy genre adaptation model
extending maximum entropy model account domain domain
data framework described earlier requires addition several extra model param
eters particular domain data point xn yn assume existence



binary indicator variable zn value zn indicates xn yn drawn q

truly domain distribution value zn indicates drawn q g

general domain distribution similarly domain data point x n yn


assume binary indicator variable zn zn means data point drawn

q truly domain distribution value means drawn
q g general domain distribution course indicator variables
observed data must infer values automatically


fidaume iii marcu









g

yni

yn
xn

xni

zni

n





yno

n

xno









zno



g





n





figure left standard logistic regression model right mega model
according model zn binary random variables assume
drawn bernoulli distribution parameter domain outof domain furthermore assume three vectors g
corresponding q q q g respectively instance zn assume

xn classified finally model binary vectors xn respec
tively xn drawn independently bernoulli distributions parameterized

g respectively g zn assume xn
drawn according corresponds nave bayes assumption generative
probabilities xn vectors finally place common beta prior nave bayes
parameters allowing range g full hierarchical model


f b

zn










g

xnf zn f f




zn

bet b
ber
z

ber fn





yn xn zn g gibbs xn zn











g

xnf zn f f



ber



z

ber fn





yn xn zn g gibbs xn zn

term model maximum entropy genre adaptation model mega
model corresponding graphical model shown right figure generative story domain data point x follows
select whether x truly domain general domain indicate
z g choose z probability z g probability



component f x choose xf probability zf
z

probability f


choose class according eq parameter vector z






fidomain adaptation statistical classifiers

story domain data points identical uses truly domain
general domain parameters rather truly domain parameters generaldomain parameters
linear chain
straightforward extension maximum entropy classification model maximum
entropy markov model memm mccallum freitag pereira obtained
assuming targets yn sequences labels canonical example model
part speech tagging word sequence assigned part speech tag
introducing first order markov assumption tag sequence one obtains linear chain
model viewed discriminative counterpart standard generative
hidden markov model parameters estimated
limited memory bfgs extension mega model linear chain framework
similarly straightforward assumption label part speech tag
indicator variable z versus global indicator variable z entire tag sequence
techniques described herein may applied conditional random field
framework lafferty mccallum pereira fixes bias
memm performing global normalization rather per state normalization
however subtle difficulty direct application crfs specifically one would need
decide single z variable would assigned entire sentence word
individually memm case natural one z per word however
crf would computationally expensive remainder continue
use memm model efficiency purposes

conditional expectation maximization
inference mega model slightly complex standard maximum entropy however inference solved efficiently conditional expectation
maximization cem variant standard expectation maximization em
dempster laird rubin due jebara pentland high level em
useful computing generative hidden variables cem useful
computing discriminative hidden variables mega model belongs
latter family cem appropriate choice
standard em family maximizes joint likelihood data
particular xn yn n
n data z discrete hidden variable step em
proceeds maximizing bound given eq
log p x log

x
z

p z x log ezp x p x z



eq ez denotes expectation one may apply jensens inequality
equation states f e x e f x whenever f convex taking f log
able decompose log expectation expectation log typically
separates terms makes taking derivatives solving resolution optimization tractable unfortunately em cannot directly applied conditional


fidaume iii marcu

mega model form eq step
requires maximization equation form given eq
log p x log
l log

x
z

x
z

p z x log ezp x p x z
p z x log

x
z

p z x




jensens inequality applied first term eq maximized
readily standard em however applying jensens inequality second term would
lead upper bound likelihood since term appears negated
conditional em solution jebara pentland bound change
log likelihood iterations rather log likelihood change loglikelihood written eq denotes parameters iteration


lc log p x log p x



rewriting conditional distribution p x p x divided p x
express lc log joint distribution difference minus log marginal
distribution apply jensens inequality first term joint difference
second appears negated fortunately jensens
bound employ standard variational upper bound logarithm function
log x x leads lower bound negation exactly desired
bound attractive reasons tangent logarithm tight
makes contact current operating point according maximization
previous time step simply linear function terminology
calculus variations variational dual logarithm see smith
applying jensens inequality first term eq variational dual
second term obtain change log likelihood moving model parameters
time time shall denote qt bounded l qt
qt defined eq h e z x z e z x
z expectations taken respect parameters previous iteration


p

x
p z x
z p z x



q
hz log
p

p z x
z p z x
zz

applying two bounds jensens inequality variational bound
removed sums logs hard deal analytically full derivation
given appendix remaining expression lower bound change likelihood
maximization maximization likelihood
map variant standard em change e step priors
placed parameters assumption standard em wish maximize
p x p p x prior probability ignored leaving
likelihood term parameters given data map estimation make
assumption instead use true prior p need add factor
log p definition qt eq


fidomain adaptation statistical classifiers


jn z
n

mt
n


log p xn yn zn
n zn

p


n zn f
z n p x n zn



xnf
xnf
zn
zn




f
f
f

xnf
xnf
q
zn
zn






f f
f
f



qf

table notation used mega model equations
important note although make use full joint distribution p x z
objective function model conditional joint distribution used
process creating bound overall optimization maximize conditional likelihood labels given input particular bound full joint likelihood
holds parameters marginal

parameter estimation mega model
made explicit eq relevant distributions performing cem full joint
distributions input variables x output variables hidden variables z
additionally require marginal distribution x variables z variables
finally need compute expectations z variables derive expectation
step section present final solution maximization step class
variables derivation equations maximization given appendix b
q bound complete conditional likelihood mega modelis given





p





z

x
p
p
z

x



x
n
n
n
n
n
z



p n
h
qt
n log



z x

p z n x n yn
p
n
n
n z
zn
n






p





n
p zn xn yn
p zn xn
x x
z



p n
h

n log







p z n x n yn
z n xn
p
n z
z
n
x





n

n

equation p probability distribution previous iteration first
term eq bound domain data second term bound
domain data optimizations described section nearly
identical terms domain parameters domain parameters brevity
explicitly write equations domain parameters corresponding
domain equations easily derived moreover reduce notational
overload elide superscripts denoting domain domain obvious
context notational brevity use notation depicted table
expectation step
e step concerned calculating hn given current model parameters since zn
easily hn p zn calculated follows


fidaume iii marcu

p zn z xn yn
p zn z p xn zn z p yn zn z
p
z p zn z p xn zn z p yn zn z

h

z z n z
exp zyn xn
zxn z



z partition function easily calculated z
expectation found dividing value z sum
step
shown appendix b directly compute
value solving simple

quadratic equation compute b

b
step

pn


n hn mn n n
pn
n mt
n n n
pn
n hn
pn

n mn n n





viewing qt function easy see optimization variable convex
analytical solution available gradient qt respect
seen identical gradient standard maximum entropy posterior eq
data point weighted according posterior probability h n may
thus use identical optimization techniques computing optimal variables standard
maximum entropy difference data points weighted
similar story holds case g obtain standard maximum entropy

gradient computed n n data points xn weighted hn


xn weighted hn shown appendix b
step
case cannot obtain analytical solution finding maximizes
qt however compute simple derivatives qt respect single component

f maximized analytically shown appendix b compute f

b
pn



n hn jn n f

p
n
n jn n f
pn
hn xnf

b pn n
n jn n f





fidomain adaptation statistical classifiers

megacem


initialize f f g f
parameters havent converged iterations remain
expectation step
n n

compute domain marginal probabilities mn

compute domain expectations hn eq
end
n n

compute domain marginal probabilities mn

compute domain expectations hn eq
end
maximization step
analytically update according equations shown section
optimize g bfgs
iterations remain havent converged
update according derivation section
end
end
return
figure full training mega model
case identical g difference must replace
sum data points two sums one domain domain
points hn must replaced hn made explicit
appendix thus optimize variables simply iterate optimize
component analytically given convergence
training
full training depicted figure convergence properties cem
ensure converge local maximum posterior space local
optima become practice one alternatively use stochastic optimization
temperature applied enabling optimization jump local
optima early however explore idea work context
application extension required
cem convergence
one immediate question conditional em model described many
em iterations required model converge experiments iterations


fidaume iii marcu

convergence cem optimization




negative log likelihood e

















number iterations





figure convergence training

cem sufficient often necessary make clear
figure plotted negative complete log likelihood model first
data set described section three separate maximizations full
training see figure first involves updating variables second
involves optimizing variables third involves optimizing variables
compute likelihood steps
running total cem iterations still relatively efficient model dominating expense weighted maximum entropy optimization cem iterations
must computed times iteration requires optimization three
sets variables worst take times amount time train model
complete data set union domain domain data practice
resume optimization ending point previous iteration causes
subsequent optimizations take much less time
prediction
training supplied us model parameters subsequent task apply
parameters unseen data obtain class predictions assume test data indomain e drawn q q g notation introduction
obtain decision rule form given eq test point x

arg max p x
yy
x
arg max
p z x p x z
yy

arg max
yy

z

x
z

p z p x z p x z


fidomain adaptation statistical classifiers



arg max

f




g

f

xf

g

f

xf





h
g
exp x

zx g
h



x
f
xf
xf exp






f
f
zx
yy

f



f

thus decision rule simply select class highest probability according maximum entropy classifiers weighted linearly marginal probabilities
data point drawn q versus q g sense model
seen linearly interpolating domain model general domain model
interpolation parameter input specific

experimental
section describe applying mega model several datasets
varying degrees divergence domain domain data however
describing data discuss systems compare
baseline systems
though little literature thus real systems
compare several obvious baselines describe section
onlyi model obtained simply training standard maximum entropy model
domain data completely ignores domain data serves
baseline case data unavailable
onlyo model obtained training standard maximum entropy model
domain data completely ignoring domain data serves baseline
expected performance without annotating data gives sense close
domain distribution domain distribution
lini model obtained linearly interpolating onlyi onlyo systems
interpolation parameter estimated held development domain data
means practice extra domain data would need annotated order create
development set alternatively cross validation could used
mix model obtained training maximum entropy model union
domain domain data sets
mixw model obtained training maximum entropy model union
domain domain data sets domain data downweighted effectively equinumerous domain data
feats model uses domain data build one classifier uses
classifiers predictions features domain data described


fidaume iii marcu

prior adaptation model described section domain
data used estimate prior domain classifier case maximum
entropy consider weights learned domain data used
mean gaussian prior distribution placed weights training
domain data described chelba acero
cases tune model hyperparameters performance development data
development data taken random training data cases
appropriate hyperparameters found folded back training set
data sets
evaluate three different first two come
automatic content extraction ace data task data selected ace
program specifically looks data different domains third
tackled chelba acero required annotate data
mention type classification
first mention type subcomponent entity mention detection
task extension named entity tagging task wherein pronouns nominals
marked addition simple names assume extents mentions
marked simply need identify type one person geo political entity
organization location weapon vehicle domain data use newswire
broadcast news portions ace training data domain data use
fisher conversations data example domain sentence
prime battleground constitutional allocation power
nom
nam
federal governmentnom
gpe statesgpe congressorg
bar
federal regulatory agenciesorg
example domain sentence
nom
pro
nom
mypro
per wifeper iper transported across continent gpe
whq pro
whereloc iper born

use k domain examples mention corresponds one example k
domain examples test examples accuracy computed loss use
standard feature functions employed named entity include lexical items
stems prefixes suffixes capitalization patterns part speech tags membership
information gazetteers locations businesses people accuracies reported
running ten fold cross validation
mention tagging
second mention tagging precursor mention type task
attempt tag entity mentions raw text use standard begin
encoding use maximum entropy markov model perform tagging mccallum
et al domain data use newswire broadcast news


fidomain adaptation statistical classifiers

data domain data use broadcast news data transcribed
automatic speech recognition domain data lacks capitalization punctuation etc
contains transcription errors speech recognition word error rate approximately
tagging task k domain examples context tagging
example single word k domain examples k test examples
accuracy f measure across segmentation use features mention
type identification task scores reported ten fold cross validation
recapitalization
final recap task recapitalizing text following chelba acero
use maximum entropy markov model possible tags
lowercase capitalized upper case punctuation mixed case domain
data task comes wall street journal two separate domain data sets
come broadcast news text cnn npr abc primetime respectively use
domain examples one example one word cnn npr data use
k domain training examples k test examples abc primetime data
use k domain training examples k test examples use identical features
chelba acero order maintain comparability described
chelba acero perform cross validation experiments use
train test split described
feature selection
maximum entropy used classification adept dealing
many irrelevant redundant features nave bayes generative model use
model distribution input variables overfit features turned
mention type mention tagging
recap caused errors alleviate recap
applied feature selection features used nave
bayes model entire feature set used maximum entropy model specifically
took k top features according information gain criteria predict indomain versus domain opposed feature selection class label forman
provides overview different selection techniques

shown table see training domain data
outperforms training domain data linearly interpolated model
improve base significantly placing data one bag helps
clear advantage weighting domain data prior model
feats model perform roughly comparably prior model edging
small margin model outperforms prior model feats model
value k selected arbitrarily initial run model development data
tuned optimize development test performance
numbers prior model data chelba acero differ slightly
reported two potential reasons first numbers



fidaume iii marcu




accuracy
onlyo
onlyi
lini
mix
mixw
feats
prior
megam
reduction
mix
prior

mention
type
k
k

mention
tagging
k
k

recap
abc

k

recap
cnn

k

average






























































table experimental first set rows sizes domain
domain training data sets second set rows accuracy
performance four tasks last two rows
reduction percentage reduction error rate mega model
baseline model mix best alternative method prior

applied mcnemars test gibbons chakraborti section gage statistical significance comparing prior model
mega model mention tagging experiment compute mcnemars test simple
hamming accuracy rather f score suboptimal know
compute statistical significance f score mention type task difference
statistical significant p level mention tagging task p
recapitalization tasks difference abc data significant p
level cnn npr data significant p level
mention type task improved baseline model trained domain
data accuracy relative improvement mention
tagging improve f measure relative improvement
abc recapitalization task much domain data available increase
performance relative improvement cnn npr
recapitalization task little domain data increase performance
relative improvement

reported examples consider example case second
likely subtle differences training used nevertheless whole relative
improvements agree



fidomain adaptation statistical classifiers

mention type identification task

mention tagging task





onlyout
chelba
megam



onlyout
chelba
megam






accuracy

fmeasure



























amount domain data used log scale












amount domain data used log scale

figure learning curves prior megam
learning curves
particular interest amount annotated domain data needed see marked
improvement onlyo baseline well adapted system figure
learning curves mention type mention tagging along x axis
plot amount domain data used along axis plot accuracy plot
three lines flat line onlyo model use domain data
curves prior megam see model maintains accuracy
prior curve actually falls baseline
type identification task

model introspection
seen previous sections mega model routinely outperforms competing despite clear performance improvement question remains open regarding
internal workings variable captures degree indomain data set truly domain z variables model aim capture
test data point whether general domain domain section discuss
particular values parameters model learns variables
present two analyses first section inspect inner workings
mention type task section analysis look specifically
expected values hidden variables found model second analysis
section look ability model judge degree relatedness defined
variables
fisher data personal conversations hence much higher degree first
second person pronouns news baseline guesses person achieves
accuracy able intelligently use domain data domain model
unsure performance drops observed prior model







fidaume iii marcu

pre context
home trenton
veterans administration
know american
gives
capable getting
fisher thing calling


entity
jersey
hospital
government



anything




kid


post context
thats

chills

ha ha screwed


true
gpe
org
org
per
wea
per
per

hyp
gpe
loc
org
per
per
per
per

p z








table examples test data mention type task true column
correct entity type hyp column prediction final
column probability example truly domain model

model expectations
focus discussion consider mention type task section
table shown seven test data examples mention type task precontext text appears entity post context text
appears report true class class model hypothesizes finally
report probability example truly domain according model
see three examples model thinks general domain
jersey hospital government believes anything kid
domain general probabilities tend skewed toward
uncommon nave bayes shown two errors data first
model thinks hospital location truly organization
difficult distinction make training data hospitals often used locations
second example error anything capable getting anything
long distance context example discussion biological warfare
saddam hussein anything supposed refer type biological warhead
model mistakingly thinks person error likely due fact
model identifies word anything likely truly domain word
common newswire learned truly domain entities people
thus lacking evidence otherwise model incorrectly guesses anything person
interesting observe model believes entity gives
chills closer general domain fisher thing calling ha ha
screwed likely occurs context ha ha occurred anywhere
domain training data twice domain training data unlikely
example would misclassified otherwise fairly clearly person
example shows model able take context account deciding domain
decisions made model shown table seem qualitatively reasonable
numbers perhaps excessively skewed ranking believable domain
data primarily conversations random necessarily news worthy topics
hence highly colloquial contrastively domain data formal news
model able learn entities jersey government
news words kid


fidomain adaptation statistical classifiers




mention
type



mention
tagging



recap
cnn



recap
abc



table values variables discovered mega model
degree relatedness
section analyze values found model low values
mean domain data significantly different domain data
high values mean similar high value means
general domain model used cases tasks mention type
values middling around mention type
indicating significant difference domain domain
data exact values tasks shown table
values make intuitive sense distinction conversation data
news data mention type task significantly stronger difference
manually automatically transcribed newswire mention tagging task
values reflect qualitative distinction rather strong difference
values recapitalization tasks expected priori however post hoc
analysis shows reasonable compute kl divergence unigram
language model domain data set domain data sets
kl divergence cnn data divergence abc data
confirms abc data perhaps different baseline domain
cnn data reflected values
interested cases little difference domain
domain data simulate case performed following experiment
consider mention type task use training portion ofdomain data randomly split data half assigning half domain
domain theory model learn may rely general
domain model performed experiment ten fold cross validation found
average value selected model strictly less
one model able identify similar domains

conclusion discussion
presented mega model domain adaptation discriminative conditional learning framework described efficient optimization
conditional em technique experimentally shown four data sets
model outperforms large number baseline systems including current state
art model requiring significantly less domain data
although focused specifically discriminative modeling maximum entropy
framework believe novel basic idea work foundedto break
domain distribution p domain distribution p three distributions q


fidaume iii marcu

q q g general particular one could perform similar analysis case
generative obtain similar though case generative model
standard em could used model could applied domain adaptation
language modeling machine translation
exception work described section previous work domain adaptation quite rare especially discriminative learning framework substantial literature language modeling speech community adaptation
concerned adapting speakers iyer ostendorf gish
kalai chen blum rosenfeld learning perspective mega
model similar mixture experts model model seen constrained experts model three experts constraints specify domain
data come one two experts domain data come
one two experts single expert overlapping two attempts
build discriminative mixture experts make heuristic approximations order
perform necessary optimization jordan jacobs rather apply conditional
em gives us strict guarantees monotonically increase data incomplete
log likelihood iteration training
domain adaptation closely related multitask learning known
learning learn inductive transfer multitask learning one attempts learn
function solves many machine learning simultaneously related
discussed thrun caruana baxter among others
similarity multitask learning domain adaptation deal
data drawn related distinct distributions primary difference domain
adaptation cares predicting one label type multitask learning cares
predicting many
sub communities natural language processing family begin continue branch domains newswire importance developing
domains without annotating much data become important
mega model first step toward able migrate simple classification style classifiers maximum entropy markov across domains continued
area adaptation likely benefit work done active learning
learning large amounts unannotated data

acknowledgments
thank ciprian chelba alex acero making data available thank ryan
mcdonald pointing feats baseline previously considered
thank kevin knight dragos munteanu discussions related project
greatly improved suggestions reviewers including reviewers
previous shorter version work partially supported darpa ito grant n nsf grant iis nsf grant iis usc dean fellowship
hal daume iii


fidomain adaptation statistical classifiers

appendix conditional expectation maximization
appendix derive eq eq making use jensens inequality
variational bound interested reader referred work jebara pentland
details discussion consider bound change log
likelihood iteration iteration l c given eq



p x
p x p
l log
log
p x
p x p



p x
p x
log
log
p x
p x
c




effectively rewritten log change ratio conditionals
difference log change ratio joints log change ratio
marginals may rewrite eq introducing hidden variables z


p
p


z p x z
z p x z
log p
l log p


z p x z
z p x z
c



apply jensens inequality first term eq obtain

c

l

x
z






p

p x z
p x z
z p x z
p
log
log p


p x z
z p x z
z p x z

z




hx z

eq expression denoted hx z joint expectation z
previous iterations parameter settings unfortunately cannot apply jensens inequality remaining term eq appears negated applying
variational dual log x x term obtain following final bound
c



l q

x
z



p

p x z
z p x z

hx z log
p

p x z
z p x z



applying bound eq distributions chosen model yields eq

appendix b derivation estimation equations
given model structure parameterization mega modelgiven section
eq obtain following expression joint probability data


fidaume iii marcu



p x z


f
n



zn
zn
ber xnf f gibbs yn xn
ber zn



n
f



f
n

xnf
xnf



zfn
zfn
zn zn

n
f

h
x

h
exp zynn xn
exp zcn xn

c



marginal distribution obtained removing last two terms exp
sum exps final equation plugging eq eq notation
eq obtain following expression qt
qt

x




n
x

n



log


x
zn



f
x

f





log bet f b

hn zn log zn log log n zn


f
x

xnf log zynn

f

zn
mt
n



zn

log

x
c

n zn



exp

h

zcn xn






jn z
n




well analogous term domain data j defined table
b step
computing simply differentiate qt see eq respect obtaining
n

qt x hn hn


mt
n n n






n

solving leads directly quadratic expression form








n
x

mt
n n

n



n
x

n

n



hn mt
n n n






fidomain adaptation statistical classifiers









n
x

n

hn





solving directly gives desired update equation
b step
optimizing rewrite qt eq neglecting irrelevant terms

qt

n
x

n

hn


f
x


f

xnf yn f log

x
c

h

exp c xn





log



eq bracketed expression exactly log likelihood term obtained
standard logistic regression thus optimization q respect
performed weighted version standard logistic regression optimization
weights defined hn case g obtain weighted logistic regression
model n n data points weights defined hn
b step
case rewrite eq remove irrelevant terms
qt

f
x

f

log bet f b

n
x


n

hn log n mt
n n





due presence product term cannot compute analytical solution
maximization however take derivatives component wise f
obtain analytical solutions combined prior admits iterative
solution maximizing qt maximizing component separately convergence
computing derivatives qt respect f requires differentiating n respect
f convenient form recalling notation table


n n f
xnf f xnf f n f
f
f



maximize qt respect f solving

n
x
xnf f xnf f
h
hn
qf

f
f f
n





f f

n
x
jn n f
f

jn n f




n
x

hn xnf

f f
n



n

fidaume iii marcu

equating zero yields quadratic expression form

f
f





n
x

jn
n
n
x




n f



hn jn n f

n
n
x

f

n

hn xnf









final equation solved analytically similar expression arises f
g

case f obtain quadratic form sums entire data set
hn replacing occurrences hn












n
x


g

jn


n f

f

n

n

g

f








g

f




n
x

n



x

n

x

n

jn n f



h
n jn n f

n











n


h
n xnf

x

n








h
n xnf


n
x

n






h
n jn n f






solved analytically values j defined table

references
averick b j j evaluation large scale optimization
vector parallel architectures siam journal optimization
baxter j model inductive bias learning journal artificial intelligence

bacchiani roark b unsupervised langauge model adaptation proceedings
international conference acoustics speech signal processing icassp
caruana r multitask learning knowledge source inductive bias machine learning
chelba c acero adaptation maximum entropy classifier little data
help lot proceedings conference empirical methods natural
language processing emnlp barcelona spain
chen rosenfeld r gaussian prior smoothing maximum entropy
tech rep cmucs carnegie mellon university computer science
department


fidomain adaptation statistical classifiers

della pietra della pietra v j lafferty j inducing features random
fields ieee transactions pattern analysis machine intelligence

dempster laird n rubin maximum likelihood incomplete data
via em journal royal statistical society b
elhadad n kan klavans j mckeown k customization unified
framework summarizing medical literature journal artificial intelligence
medicine
forman g extensive empirical study feature selection metrics text classification journal machine learning
gibbons j chakraborti nonparametric statistical inference marcel
dekker inc
gildea corpus variation parser performance proceedings conference empirical methods natural language processing emnlp
hobbs j r information extraction biomedical text journal biomedical
informatics
huang j zweig g padmanabhan information extraction voicemail
proceedings conference association computational linguistics
acl
hwa r supervised grammar induction training data limited constituent
information proceedings conference association computational
linguistics acl pp
iyer r ostendorf gish h domain data improve domain
language ieee signal processing
jebara pentland maximum conditional likelihood via bound maximization
cem advances neural information processing systems
nips
jordan jacobs r hierarchical mixtures experts em
neural computation
kalai chen blum rosenfeld r line combining
language icassp
lafferty j mccallum pereira f conditional random fields probabilistic
segmenting labeling sequence data proceedings international
conference machine learning icml
malouf r comparison maximum entropy parameter estimation
proceedings conll
mccallum freitag pereira f maximum entropy markov
information extraction segmentation proceedings international conference machine learning icml


fidaume iii marcu

minka p comparison numerical optimizers logistic regression http
www stat cmu edu minka papers logreg
munteanu marcu improving machine translation performance exploiting non parallel corpora computational linguistics appear
nash nocedal j numerical study limited memory bfgs method
truncated newton method large scale optimization siam journal
optimization
rambow shrestha l chen j lauridsen c summarizing email threads
proceedings conference north american chapter association
computational linguistics naacl short section
roark b bacchiani supervised unsupervised pcfg adaptation
novel domains proceedings conference north american chapter
association computational linguistics human language technology
naacl hlt
smith r variational methods optimization dover publications inc mineola york
thrun learning n th thing easier learning first advances
neural information processing systems nips




