Journal Artificial Intelligence Research 26 (2006) 191246

Submitted 01/05; published 07/06

Fast Downward Planning System
Malte Helmert

HELMERT @ INFORMATIK . UNI - FREIBURG . DE

Institut fur Informatik
Albert-Ludwigs-Universitat Freiburg
Georges-Kohler-Allee, Gebaude 052
79110 Freiburg, Germany

Abstract
Fast Downward classical planning system based heuristic search. deal general deterministic planning problems encoded propositional fragment PDDL2.2, including
advanced features ADL conditions effects derived predicates (axioms).
well-known planners HSP FF, Fast Downward progression planner, searching
space world states planning task forward direction. However, unlike PDDL planning systems, Fast Downward use propositional PDDL representation planning
task directly. Instead, input first translated alternative representation called multivalued planning tasks, makes many implicit constraints propositional planning
task explicit. Exploiting alternative representation, Fast Downward uses hierarchical decompositions planning tasks computing heuristic function, called causal graph heuristic,
different traditional HSP-like heuristics based ignoring negative interactions
operators.
article, give full account Fast Downwards approach solving multi-valued
planning tasks. extend earlier discussion causal graph heuristic tasks involving
axioms conditional effects present novel techniques search control used
within Fast Downwards best-first search algorithm: preferred operators transfer idea helpful actions local search global best-first search, deferred evaluation heuristic functions
mitigates negative effect large branching factors search performance, multi-heuristic
best-first search combines several heuristic evaluation functions within single search algorithm
orthogonal way. describe efficient data structures fast state expansion (successor
generators axiom evaluators) present new non-heuristic search algorithm called focused
iterative-broadening search, utilizes information encoded causal graphs novel
way.
Fast Downward proven remarkably successful: classical (i. e., propositional,
non-optimising) track 4th International Planning Competition ICAPS 2004, following
footsteps planners LPG. experiments show performs
well benchmarks earlier planning competitions provide insights
usefulness new search enhancements.

1. Introduction
Consider typical transportation planning task: postal service must deliver number parcels
respective destinations using vehicle fleet cars trucks. Let us assume car
serves locations one city, different cities connected via highways
served trucks. sake simplicity, let us assume travelling segment
road highway incurs cost. highly realistic assumption, purposes
exposition do. number parcels, posted arbitrary locations
c
2006
AI Access Foundation. rights reserved.

fiH ELMERT

p2

B

F

c2





E

c1

G

C

c3

p1

Figure 1: transportation planning task. Deliver parcel p 1 C G parcel p2 F E,
using cars c1 , c2 , c3 truck t. cars may use inner-city roads (thin edges),
truck may use highway (thick edge).

arbitrary destinations. Moreover, cities varying size, one several cars
within city, one several trucks connecting cities. Cars never leave
city. Fig. 1 shows example task kind two cities, three cars single truck.
two parcels delivered, one (p 1 ) must moved two cities,
(p2 ) stay within initial city.
astute reader familiar planning literature noticed
essentially describing L OGISTICS domain, standard benchmark classical planning systems,
extended roadmaps complete graphs. (Part of) propositional STRIPS-like encoding
task shown Fig. 2.
would human planners go solving tasks kind? likely, would use
hierarchical approach: p1 , clear parcel needs moved cities,
possible using truck. Since example city access highway one
location, see must first load parcel car initial location, drop
first citys highway access location, load truck, drop citys highway
access location, load car city, finally drop destination.
commit high-level plan delivering p 1 without worrying lower-level aspects
path planning cars. obvious us good solution structure,
since parcel change location clearly defined ways (Fig. 3). figure
shows reasonable plans getting p 2 destination require loading car
initial city dropping target location. point ever loading
truck cars left city.
say committed (partially ordered, movements two parcels
interleaved) high-level plan shown Fig. 5. need complete plan choose
linearization high-level steps fill movements vehicle fleet them.
thus decomposed planning task number subproblems. parcel scheduling
problem (where, vehicles, parcel loaded unloaded) separated
path planning problem vehicle fleet (how move point X Y).
192

fiT FAST OWNWARD P LANNING YSTEM

Variables:
at-p1-a, at-p1-b, at-p1-c, at-p1-d, at-p1-e,
at-p2-a, at-p2-b, at-p2-c, at-p2-d, at-p2-e,
at-c1-a, at-c1-b, at-c1-c, at-c1-d,
at-c2-a, at-c2-b, at-c2-c, at-c2-d,
at-c3-e, at-c3-f, at-c3-g,
at-t-d, at-t-e,
in-p1-c1, in-p1-c2, in-p1-c3, in-p1-t,
in-p2-c1, in-p2-c2, in-p2-c3, in-p2-t
Init:
at-p1-c, at-p2-f, at-c1-a, at-c2-b, at-c3-g,
Goal:
at-p1-g, at-p2-e
Operator drive-c1-a-d:
PRE: at-c1-a ADD: at-c1-d DEL: at-c1-a
Operator drive-c1-b-d:
PRE: at-c1-b ADD: at-c1-d DEL: at-c1-b
Operator drive-c1-c-d:
PRE: at-c1-c ADD: at-c1-d DEL: at-c1-c
...
Operator load-c1-p1-a:
PRE: at-c1-a, at-p1-a ADD: in-p1-c1 DEL:
Operator load-c1-p1-b:
PRE: at-c1-b, at-p1-b ADD: in-p1-c1 DEL:
Operator load-c1-p1-c:
PRE: at-c1-c, at-p1-c ADD: in-p1-c1 DEL:
...
Operator unload-c1-p1-a:
PRE: at-c1-a, in-p1-c1 ADD: at-p1-a DEL:
Operator unload-c1-p1-b:
PRE: at-c1-b, in-p1-c1 ADD: at-p1-b DEL:
Operator unload-c1-p1-c:
PRE: at-c1-c, in-p1-c1 ADD: at-p1-c DEL:
...

at-p1-f, at-p1-g,
at-p2-f, at-p2-g,

at-t-e

at-p1-a
at-p1-b
at-p1-c

in-p1-c1
in-p1-c1
in-p1-c1

Figure 2: Part typical propositional encoding transportation planning task (no actual
PDDL syntax).

193

fiH ELMERT

c1




B

C



E

F

c2

G

c3

Figure 3: Domain transition graph parcels p 1 p2 . Indicates parcel change
state. example, arcs correspond actions
loading/unloading parcel location truck t.

B
F







E

E

G
C

Figure 4: Domain transition graphs cars c 1 c2 (left), truck (centre), car c3 (right).
Note graph corresponds part roadmap traversed
respective vehicle.

load
c1-p1-c

unload
c1-p1-d

load
t-p1-d

unload
t-p1-e

load
c3-p2-f

unload
c3-p2-e

load
c3-p1-e

Figure 5: High-level plan transportation planning task.
194

unload
c3-p1-g

fiT FAST OWNWARD P LANNING YSTEM

c1

c2

c3

p1

p2



Figure 6: Causal dependencies transportation planning task.
graph search problems, corresponding graphs shown Fig. 3 Fig. 4.
Graphs kind formally introduced domain transition graphs Section 5.
course graph search problems interact, limited ways: State
transitions parcels associated conditions regarding vehicle fleet, need
considered addition actual path planning Fig. 3. example, parcel change
state location inside car c 1 car c1 location A. However, state transitions
vehicles associated conditions parts planning task, hence
moving vehicle one location another indeed easy finding path associated
domain transition graph. say parcels causal dependencies vehicles
operators change state parcels preconditions state
vehicles. Indeed, causal dependencies task, since parcels depend
parcels vehicles depend anything except (Fig. 6). set causal
dependencies planning task visualized causal graph.
argue humans often solve planning tasks hierarchical fashion outlined preceding paragraphs, algorithmic approaches action planning usefully apply similar
ideas. Indeed, show following section, first introduce domain transition graphs causal graphs. However, earlier work almost exclusively focused acyclic
causal graphs, good reason: causal graph planning task exhibits cycle, hierarchical decomposition possible, subproblems must solved achieve
operator precondition necessarily smaller original task. far aware,
first (Helmert, 2004) present general planning algorithm focuses exploiting hierarchical information causal graphs. However, causal graph heuristic requires
acyclicity; general case, considers relaxed planning problem operator
preconditions ignored break causal cycles.
Knowing cycles causal graphs undesirable, take closer look transportation
planning task. Let us recall informal definition causal graphs: causal graph planning
task contains vertex state variable arcs variables occur preconditions
variables occur effects operator. far, may given impression
causal graph example task well-behaved shape shown Fig. 6. Unfortunately,
closer look STRIPS encoding Fig. 2, see case: correct
causal graph, shown Fig. 7, looks messy. discrepancy intuitive actual
graph due fact informal account human-style problem solving, made
use (non-binary) state variables location car c 1 state parcel p1 ,
STRIPS-level state variables correspond (binary) object-location propositions parcel p 1
195

fiH ELMERT

Figure 7: Causal graph STRIPS encoding transportation planning task.
location A. would much nicer given multi-valued encoding planning
task explicitly contains variable location car c 1 similar properties. Indeed,
nice looking acyclic graph Fig. 6 causal graph multi-valued encoding shown
Fig. 8.
provided intuition underlying concepts, let us state design goal
Fast Downward planning system: develop algorithm efficiently solves general
propositional planning tasks exploiting hierarchical structure inherent causal graphs.
need overcome three major obstacles undertaking:
First, propositionally encoded planning tasks usually unstructured causal graphs.
However, intuitive dependencies often become visible encodings multi-valued
state variables. exploit fact automated PDDL planning system, devised
automatic algorithm translating (or reformulating) propositional tasks multi-valued
ones. translation algorithm considered independently rest planner; fact, used part planning systems (van den Briel, Vossen, &
Kambhampati, 2005). keep article focused, discuss translation algorithm
here, referring earlier work central ideas (Edelkamp & Helmert, 1999).
Instead, consider output, multi-valued planning task, base formalism.
Second, matter clever encoding is, planning tasks completely hierarchical nature. deal causal cycles, consider relaxations causal
dependencies ignored use solutions relaxed problem within heuristic search
algorithm.
Third, even planning tasks solved hierarchically, finding solution difficult (indeed, still PSPACE-complete). reason, heuristic function considers
fragment task time, namely subproblems induced single state variable
predecessors causal graph. Even planning problem still NP-complete,
196

fiT FAST OWNWARD P LANNING YSTEM

Variables:
p1, p2 {at-a, at-b, at-c, at-d, at-e, at-f, at-g,
in-c1, in-c2, in-c3, in-t}
c1, c2 {at-a, at-b, at-c, at-d}
c3
{at-e, at-f, at-g}

{at-d, at-e}
Init:
p1 = at-c, p2 = at-f
c1 = at-a, c2 = at-b, c3 = at-g, = at-e
Goal:
p1 = at-g, p2 = at-e
Operator drive-c1-a-d:
PRE: c1 = at-a EFF: c1 = at-d
Operator drive-c1-b-d:
PRE: c1 = at-b EFF: c1 = at-d
Operator drive-c1-c-d:
PRE: c1 = at-c EFF: c1 = at-d
...
Operator load-c1-p1-a:
PRE: c1 = at-a, p1 = at-a EFF: p1 = in-c1
Operator load-c1-p1-b:
PRE: c1 = at-b, p1 = at-b EFF: p1 = in-c1
Operator load-c1-p1-c:
PRE: c1 = at-c, p1 = at-c EFF: p1 = in-c1
...
Operator unload-c1-p1-a:
PRE: c1 = at-a, p1 = in-c1 EFF: p1 = at-a
Operator unload-c1-p1-b:
PRE: c1 = at-b, p1 = in-c1 EFF: p1 = at-b
Operator unload-c1-p1-c:
PRE: c1 = at-c, p1 = in-c1 EFF: p1 = at-c
...
Figure 8: Part encoding transportation planning task multi-valued state variables.

197

fiH ELMERT

content incomplete solution algorithm within heuristic solver. solution
algorithm theoretical shortcomings never failed us practice.
introduced rationale approach, discuss related work next section.
followed overview general architecture Fast Downward planning system
Section 3. planning system consists three components: translation, knowledge compilation,
search. translation component converts PDDL2.2 tasks multi-valued planning tasks,
formally introduce Section 4. knowledge compilation component discussed
Section 5, search component Section 6. conclude presentation experimental
results Section 7 discussion Section 8.

2. Related Work
planning system based heuristic forward search, Fast Downward clearly related
heuristic planners HSP (Bonet & Geffner, 2001) (Hoffmann & Nebel, 2001)
architectural level. However, section focus work related conceptual level,
i. e., work uses similar forms hierarchical decomposition causal graphs work uses
similar forms search domain transition graphs.
2.1 Causal Graphs Abstraction
term causal graph first appears literature work Williams Nayak (1997),
general idea considerably older. approach hierarchically decomposing planning tasks
arguably old field AI Planning itself, first surfaced Newell Simons
(1963) work General Problem Solver.
Still, took long time notions evolve modern form. Sacerdotis (1974)
ABSTRIPS algorithm introduced concept abstraction spaces STRIPS-like planning tasks.
abstraction space STRIPS task state space abstracted task, obtained
removing preconditions operators original task belong given set
propositions (which abstracted away). 1 solve planning task, ABSTRIPS first generates
plan abstracted task, refines plan inserting concrete plans abstract
plan steps bridge gap abstract states satisfying operator preconditions
ignored abstract level. idea easily generalized several levels abstraction forming abstraction hierarchy, abstract level top almost
preconditions ignored, successively introducing preconditions every layer final
layer hierarchy equals original planning task.
One problem approach planning general guarantee
abstract plans bear resemblance reasonable concrete plans. example, abstraction spaces
chosen badly, quite possible finding concrete plan satisfies precondition
first operator abstract plan difficult solving original goal concrete level.
shortcomings spawned large amount research properties abstraction hierarchies
generated automatically.
1. later work authors, propositions abstracted away removed operator effects.
makes difference subtle cases require presence axioms; distinguish
two kinds abstraction here.

198

fiT FAST OWNWARD P LANNING YSTEM

Tenenberg (1991) gives one first formal accounts properties different kinds
abstraction. Among contributions, defines so-called upward solution property,
informally stated as: exists concrete solution, exists abstract
solution. Rather surprisingly, abstractions considered time satisfied basic
property, without one would loathe call given state space abstraction another
state space.
limitation upward solution property states relationship concrete
abstract plan all. ABSTRIPS-style hierarchical planning successful, abstract
plan must bear resemblance concrete one; otherwise little point trying
refine it. Indeed, Tenenberg introduces stronger versions upward solution property,
relevant Fast Downward Knoblocks (1994) work ordered monotonicity property.
abstraction space satisfies ordered monotonicity property if, roughly speaking, concrete
solution derived abstract solution leaving actions abstract plan
intact relevant concrete plan. Clearly, important property ABSTRIPSlike hierarchical planning.
Knoblocks article causal graphs first surface (although introduce name
them). Translated terminology, Knoblock proves following relationship
useful abstractions causal graphs: causal graph contains path variable
abstracted away variable abstracted away, abstraction ordered
monotonicity property. particular, means acyclic causal graphs, possible devise
abstraction hierarchy one new variable introduced level.
Besides theoretical contributions, Knoblock presents planning system called ALPINE
computes abstraction hierarchy planning task causal graph exploits
within hierarchical refinement planner. Although planning method different,
derivation abstraction hierarchy similar Fast Downwards method generating
hierarchical decompositions planning tasks (Section 5.2).
itself, ordered monotonicity property sufficient guarantee good performance
hierarchical planning approach. guarantees every concrete solution obtained
natural way abstract solution, guarantee abstract solutions
refined concrete ones. guarantee provided downward refinement property,
introduced Bacchus Yang (1994).
downward refinement property rarely guaranteed actual planning domains,
Bacchus Yang develop analytical model performance hierarchical planning situations given abstract plan refined certain probability p < 1. Based
analysis, present extension ALPINE called HIGHPOINT, selects abstraction hierarchy high refinement probability among satisfy ordered monotonicity
property. practice, feasible compute refinement probability, HIGHPOINT approximates value based notion k-ary necessary connectivity.
2.2 Causal Graphs Unary STRIPS Operators
Causal graphs first given name Jonsson Backstrom (1995, 1998b), call
dependency graphs. study fragment propositional STRIPS negative conditions
interesting property plan existence decided polynomial time, minimal
solutions task exponentially long, polynomial planning algorithm exists.
199

fiH ELMERT

present incremental planning algorithm polynomial delay, i. e., planning algorithm
decides within polynomial time whether given task solution, and, so, generates
solution step step, requiring polynomial time two subsequent steps. 2
fragment STRIPS covered Jonsson Backstroms algorithm called 3S
defined requirement causal graph task acyclic state variables
static, symmetrically reversible, splitting. Static variables easy
guarantee never change value solution plan. variables detected
compiled away easily. Symmetrically reversible variables operator
makes true corresponding operator identical preconditions makes
false, vice versa. words, variable symmetrically reversible iff domain
transition graph undirected. Finally, variable v splitting iff removal causal graph
weakly disconnects positive successors (those variables appear effects operators
v precondition) negative successors (those variables appear effects
operators v precondition).
Williams Nayak (1997) independently prove incremental (or, setting, reactive)
planning polynomial problem STRIPS-like setting causal graphs acyclic
operators reversible. operators reversible (according definition Williams
Nayak), variables symmetrically reversible (according definition Jonsson
Backstrom), actually special case previous result. However, Williams Nayaks
work applies general formalism propositional STRIPS, approaches
directly comparable.
recently, Domshlak Brafman provide detailed account complexity finding plans propositional STRIPS (with negation) formalism unary operators acyclic
graphs (Domshlak & Brafman, 2002; Brafman & Domshlak, 2003). 3 Among results,
prove restriction unary operators acyclic graphs reduce complexity
plan existence: problem PSPACE-complete, unrestricted propositional STRIPS
planning (Bylander, 1994). show singly connected causal graphs, shortest plans
cannot exponentially long, problem still NP-complete. even restricted class
causal graphs, namely polytrees bounded indegree, present polynomial planning algorithm. generally, analysis relates complexity STRIPS planning unary domains
number paths causal graph.
2.3 Multi-Valued Planning Tasks
exception Williams Nayaks paper, work discussed far exclusively deals
propositional planning problems, state variables assume values binary domain. observed introduction, question propositional vs. multi-valued encodings
usually strong impact connectivity causal graph task. fact, apart
trivial OVIE domain, none common planning benchmarks exhibits acyclic causal graph
2. However, guarantee length generated solution polynomially related length
optimal solution; might exponentially longer. Therefore, algorithm might spend exponential time tasks
solved polynomial time.
3. According formal definition causal graphs Section 5.2, operators several effects always induce
cycles causal graph, acyclic causal graph implies unary operators. researchers define causal graphs
differently, name properties explicitly here.

200

fiT FAST OWNWARD P LANNING YSTEM

considering propositional representation. contrast, multi-valued encoding
introductory example acyclic causal graph.
Due dominance PDDL (and previously, STRIPS) formalism, non-binary state variables studied often classical planning literature. One important exceptions rule work SAS + planning formalism, papers Backstrom
Nebel (1995) Jonsson Backstrom (1998a) relevant Fast Downward.
SAS+ planning formalism basically equivalent multi-valued planning tasks introduce
Section 4 apart fact include derived variables (axioms) conditional
effects. Backstrom Nebel analyse complexity various subclasses SAS + formalism discover three properties (unariness, post-uniqueness single-valuedness) together
allow optimal planning polynomial time. One three properties (unariness) related
acyclicity causal graphs, one (post-uniqueness) implies particularly simple shape domain
transition graphs (namely, post-unique tasks, domain transition graphs must simple cycles
trees).
Backstrom Nebel analyse domain transition graphs formally. Indeed, term
introduced later article Jonsson Backstrom (1998a), refines earlier results
introducing five additional restrictions SAS + tasks, related properties
domain transition graphs.
Neither two articles discusses notion causal graphs. Indeed, earlier work
aware includes causal graphs domain transition graphs central concepts
article Domshlak Dinitz (2001) state-transition support (STS) problem,
essentially equivalent SAS+ planning unary operators. context STS, domain
transition graphs called strategy graphs causal graphs called dependence graphs,
apart minor details, semantics two formalisms identical. Domshlak Dinitz
provide map complexity STS problem terms shape causal graph,
showing problem NP-complete worse almost non-trivial cases. One interesting
result causal graph simple chain n nodes variables three-valued,
length minimal plans already grow (2 n ). contrast, propositional tasks
causal graph shape admit polynomial planning algorithms according result Brafman
Domshlak (2003), causal graphs polytrees constant indegree bound
(namely, bound 1).
summarize conclude discussion related work, observe central concepts Fast Downward causal graph heuristic, causal graphs domain transition
graphs, firmly rooted previous work. However, Fast Downward first attempt marry
hierarchical problem decomposition use multi-valued state variables within general planning framework. first attempt apply techniques similar Knoblock (1994)
Bacchus Yang (1994) within heuristic search planner.
significance latter point underestimated: classical approaches
hierarchical problem decomposition, imperative abstraction satisfies ordered monotonicity property, important probability able refine abstract plan
concrete plan high, analysis Bacchus Yang shows. Unfortunately, non-trivial
abstraction hierarchies rarely ordered monotonic, even rarely guarantee high refinement probabilities. Within heuristic approach, must-haves turn nice-to-haves:
abstraction hierarchy ordered monotonic abstract plan considered heuristic
evaluator refinable, merely reduces quality heuristic estimate, rather caus201

fiH ELMERT

Translation





Normalization
Invariant synthesis
Grounding
Translation MPT

Knowledge
Compilation
Domain transition
graphs
Causal graph
Successor generator
Axiom evaluator

Search






Causal graph heuristic
heuristic
Greedy best-first search
Multi-heuristic best-first search
Focused iterative-broadening search

Figure 9: three phases Fast Downwards execution.
ing search fail (in worst case) spend long time trying salvage non-refinable abstract
plans (in much better case).

3. Fast Downward
describe overall architecture planner. Fast Downward classical planning
system based ideas heuristic forward search hierarchical problem decomposition.
deal full range propositional PDDL2.2 (Fox & Long, 2003; Edelkamp & Hoffmann,
2004), i. e., addition STRIPS planning, supports arbitrary formulae operator preconditions
goal conditions, deal conditional universally quantified effects derived
predicates (axioms).
name planner derives two sources: course, one sources Hoffmanns successful (Fast Forward) planner (Hoffmann & Nebel, 2001). FF, Fast
Downward heuristic progression planner, i. e., computes plans heuristic search space
world states reachable initial situation. However, compared FF, Fast Downward uses
different heuristic evaluation function called causal graph heuristic. heuristic evaluator proceeds downward far tries solve planning tasks hierarchical fashion
outlined introduction. Starting top-level goals, algorithm recurses
causal graph remaining subproblems basic graph search tasks.
Similar FF, planner shown excellent performance: original implementation
causal graph heuristic, plugged standard best-first search algorithm, outperformed previous champions area, LPG (Gerevini, Saetti, & Serina, 2003), set STRIPS
benchmarks first three international planning competitions (Helmert, 2004). Fast Downward followed footsteps LPG winning propositional, non-optimizing
track 4th International Planning Competition ICAPS 2004 (referred IPC4
on).
mentioned introduction, Fast Downward solves planning task three phases (Fig. 9):
translation component responsible transforming PDDL2.2 input nonbinary form amenable hierarchical planning approaches. applies number normalizations compile away syntactic constructs disjunctions
directly supported causal graph heuristic performs grounding axioms operators. importantly, uses invariant synthesis methods find groups related propo202

fiT FAST OWNWARD P LANNING YSTEM

sitions encoded single multi-valued variable. output translation
component multi-valued planning task, defined following section.
knowledge compilation component generates four kinds data structures play
central role search: Domain transition graphs encode how, conditions,
state variables change values. causal graph represents hierarchical dependencies different state variables. successor generator efficient data
structure determining set applicable operators given state. Finally, axiom
evaluator efficient data structure computing values derived variables.
knowledge compilation component described Section 5.
search component implements three different search algorithms actual planning.
Two algorithms make use heuristic evaluation functions: One well-known
greedy best-first search algorithm, using causal graph heuristic. called multiheuristic best-first search, variant greedy best-first search tries combine several
heuristic evaluators orthogonal way; case Fast Downward, uses causal
graph heuristics. third search algorithm called focused iterative-broadening
search; closely related Ginsberg Harveys (1992) iterative broadening.
heuristic search algorithm sense use explicit heuristic evaluation
function. Instead, uses information encoded causal graph estimate usefulness operators towards satisfying goals task. search component described
Section 6.

4. Multi-Valued Planning Tasks
Let us formally introduce problem planning multi-valued state variables.
formalism based SAS+ planning model (Backstrom & Nebel, 1995; Jonsson & Backstrom,
1998a), extends axioms conditional effects.
Definition 1 Multi-valued planning tasks (MPTs)
multi-valued planning task (MPT) given 5-tuple = hV, 0 , s? , A, Oi following
components:
V finite set state variables, associated finite domain v . State variables partitioned fluents (affected operators) derived variables (computed
evaluating axioms). domains derived variables must contain undefined value .
partial variable assignment partial state V function subset V
s(v) Dv wherever s(v) defined. partial state called extended state
defined variables V reduced state state defined fluents V.
context partial variable assignments, write v = variable-value pairing
(v, d) v 7 d.
s0 state V called initial state.
s? partial variable assignment V called goal.
finite set (MPT) axioms V. Axioms triples form hcond, v, di,
cond partial variable assignment called condition body axiom, v derived
203

fiH ELMERT

variable called affected variable, v called derived value v. pair
(v, d) called head axiom written v := d.
axiom set partitioned totally ordered set axiom layers 1 Ak
within layer, affected variable may associated single
value axiom heads bodies. words, within layer, axioms
affected variable different derived values forbidden, variable appears
axiom head, may appear different value body. called
layering property.
finite set (MPT) operators V. operator hpre, effi consists partial
variable assignment pre V called precondition, finite set effects eff. Effects
triples hcond, v, di, cond (possibly empty) partial variable assignment called
effect condition, v fluent called affected variable, v called new
value v.
axioms effects, use notation cond v := place hcond, v, di.
provide formal semantics MPT planning, first need formalize axioms:
Definition 2 Extended states defined state
Let state MPT axioms A, layered 1 Ak . extended state defined
s, written A(s), result 0 following algorithm:
algorithm evaluate-axioms(A1 , . . . , Ak , s):
variable
( v:
s(v) v fluent variable
s0 (v) :=

v derived variable
{1, . . . , k}:
exists axiom (cond v := d) cond s0 s0 (v) 6= d:
Choose axiom cond v := d.
s0 (v) :=
words, axioms evaluated layer-by-layer fashion using fixed point computations,
similar semantics stratified logic programs. easy see layering
property Definition 1 guarantees algorithm terminates produces deterministic
result. defined semantics axioms, define state space MPT:
Definition 3 MPT state spaces
state space MPT = hV, s0 , s? , A, Oi, denoted S(), directed graph. vertex
set set states V, contains arc (s, 0 ) iff exists operator hpre, effi
that:
pre A(s),
s0 (v) = effects cond v := eff cond A(s),
s0 (v) = s(v) fluents.
204

fiT FAST OWNWARD P LANNING YSTEM

Finally, define MPT planning problem:
Definition 4 MPT planning
MPT-P LAN E X following decision problem: Given MPT initial state 0 goal
s? , S() contain path s0 state s0 s? A(s0 )?
MPT-P LANNING following search problem: Given MPT initial state 0 goal
s? , compute path S() s0 state s0 s? A(s0 ), prove none exists.
MPT-P LAN E X problem easily shown PSPACE-hard generalizes plan
existence problem propositional STRIPS, known PSPACE-complete (Bylander,
1994). easy see addition multi-valued domains, axioms conditional effects
increase theoretical complexity MPT planning beyond propositional STRIPS. Thus,
conclude formal introduction MPT planning stating MPT-P LAN E X PSPACEcomplete, turn practical side things following section.

5. Knowledge Compilation
purpose knowledge compilation component set stage search algorithms
compiling critical information planning task number data structures efficient access. contexts, computations kind often called preprocessing. However,
preprocessing nondescript word mean basically anything. reason,
prefer term puts stronger emphasis role module: rephrase critical information planning task way directly useful search algorithms.
three building blocks Fast Downward (translation, knowledge compilation, search),
least time-critical part, always requiring less time translation dominated search
trivial tasks.
Knowledge compilation comprises three items. First foremost, compute domain
transition graph state variable. domain transition graph state variable encodes
circumstances variable change value, i. e., values domain
transitions values, operators axioms responsible transition, conditions state variables associated transition. Domain
transition graphs described Section 5.1. central concept computation
causal graph heuristic, described Section 6.1.
Second, compute causal graph planning task. domain transition graphs encode dependencies values given state variable, causal graph encodes dependencies
different state variables. example, given location planning task unlocked
means key carried agent, variable representing lock state
location dependent variable represents whether key carried.
dependency encoded arc causal graph. domain transition graphs, causal
graphs central concept computation causal graph heuristic, giving name.
causal graph heuristic requires causal graphs acyclic. reason, knowledge compilation component generates acyclic subgraph real causal graph cycles occur.
amounts relaxation planning task operator preconditions ignored.
addition usefulness causal graph heuristic, causal graphs key concept
focused iterative-broadening search algorithm introduced Section 6.5. discuss causal
graphs Section 5.2.
205

fiH ELMERT

Third, compute two data structures useful forward-searching algorithm
MPTs, called successor generators axiom evaluators. Successor generators compute set
applicable operators given world state, axiom evaluators compute values derived
variables given reduced state. designed job quickly possible,
especially important focused iterative-broadening search algorithm, compute heuristic estimates thus requires basic operations expanding search node
implemented efficiently. data structures discussed Section 5.3.
5.1 Domain Transition Graphs
domain transition graph state variable representation ways variable
change value, conditions must satisfied value changes allowed. Domain transition graphs introduced Jonsson Backstrom (1998a) context
SAS+ planning. formalization domain transition graphs generalizes original definition
planning tasks involving axioms conditional effects.
Definition 5 Domain transition graphs
Let = hV, s0 , s? , A, Oi multi-valued planning task, let v V state variable .
domain transition graph v, symbols DTG(v), labelled directed graph vertex
set Dv . v fluent, DTG(v) contains following arcs:
effect cond v := d0 operator precondition pre pre cond
contains condition v = d, arc 0 labelled pre cond \ {v = d}.
effect cond v := d0 operator precondition pre pre cond
contain condition v = v , arc Dv \ {d0 } d0
labelled pre cond.
v derived variable, DTG(v) contains following arcs:
axiom cond v := d0 cond contains condition v = d, arc
d0 labelled cond \ {v = d}.
axiom cond v := d0 cond contain condition v =
Dv , arc Dv \ {d0 } d0 labelled cond.
Arcs domain transition graphs called transitions. labels referred
conditions transition.
Domain transition graphs weighted, case transition associated
non-negative integer weight. Unless stated otherwise, assume transitions derived
operators weight 1 transitions derived axioms weight 0.
definition somewhat lengthy, informal content easy grasp: domain transition graph v contains transition 0 exists operator axiom
change value v d0 . transition labelled conditions state
variables must true transition shall applied. Multiple transitions
values using different conditions allowed occur frequently.
already seen domain transition graphs introductory section (Figs. 3 4), although introduced informally show arc labels usually associated
206

fiT FAST OWNWARD P LANNING YSTEM

= open
(1, 1)

(2, 1)

(3, 1)

(2, 1)

(2, 2)

(3, 2)

r=

r = (1, 1), k = carried

closed

r = (2, 2), k = carried

r=

open

(1, 2)

(1,

1)

1)

2)
(1,
r=

2)
(1,

r = (2, 1)

(1,

r = (2, 1)

r=

r=

r=

(3, 1)

1)
(3,

r=
r=

carried

r = (2, 2)

(1, 2)

(1, 1)

r = (2, 2)

= open

= open

(3,

(3,
(3,

2)

1)

2)

(3, 2)

r = (3, 1), k = carried
(2, 2)

Figure 10: Domain transition graphs G RID task. Top left: DTG(r) (robot); right: DTG(k)
(key); bottom left: DTG(d) (door).

transitions. Fig. 10 shows examples simple task G RID domain, featuring 3 2 grid single initially locked location centre upper row, unlockable
single key. MPT encoding task, three state variables: variable r
Dr = { (x, y) | x {1, 2, 3}, {1, 2} } encodes location robot, variable k
Dk = Dr {carried} encodes state key, variable = {closed, open}
encodes state initially locked grid location.
operators MPT unary (i. e., single effect) leave aside axioms
moment, strong correspondence state space MPT
domain transition graphs. Since vertices domain transition graphs correspond values state
variables, given state represented selecting one vertex domain transition graph, called
active vertex state variable. Applying operator means changing active vertex
state variable performing transition corresponding domain transition graph.
Whether transition allowed depends condition, checked
active vertices domain transition graphs.
Let us use G RID example illustrate correspondence. Consider initial state
robot location (1, 1), key location (3, 2), door locked. represent
placing pebbles appropriate vertices three domain transition graphs. want
move pebble domain transition graph key location (2, 1). done
moving robot pebble vertex (1, 2), (2, 2), (3, 2), moving key pebble vertex
carried, moving robot pebble back vertex (2, 2), moving door pebble open, moving
robot pebble vertex (2, 1) finally moving key pebble vertex (2, 1).
207

fiH ELMERT

= open, r = (1, 1)
= open, r = (3, 1)

= open, r = (1, 1)

= closed


>



r = (2, 1)

>

r = (1, 2)

= open, r = (3, 1)

r = (2, 2)
r = (3, 2)

Figure 11: Domain transition graphs freezing variable G RID task, normal (left)
extended (right). Note extended graph shows change state
freezing (>) freezing ().

example shows plan execution viewed simultaneous traversal domain
transition graphs (cf. Domshlak & Dinitz, 2001). important notion Fast Downward
causal graph heuristic computes heuristic estimates solving subproblems
planning task looking paths domain transition graphs basically way described.
mentioned before, view MPT planning completely accurate unary tasks
without axioms, domain transition graphs indeed complete representation
state space. non-unary operators, would need link certain transitions different domain
transition graphs belong operator. could executed together.
axioms, would need mark certain transitions mandatory, requiring taken
whenever possible. (This intended rough analogy leaves details layered
axioms.)
previous work (Helmert, 2004), successfully applied view planning
STRIPS tasks. Extending notion plans conditional effects provides challenges domain transition graphs always consider planning operators one effect time,
case effect condition simply seen part operator precondition. However, axioms
provide challenge easily overlooked. want change value fluent
d0 , domain transition graph contains important information; find path 0
try find associated conditions achieved. Consider problem
derived state variable. Let us assume unlocking location G RID example leads
drought, causing robot freeze enters horizontally adjacent location. could encode
new derived variable f (for freezing) domain f = {>, }, defined axioms
= open, r = (1, 1) f := > = open, r = (3, 1) f := >. domain transition graph
DTG(f ) depicted Fig. 11 (left).
problem domain transition graph tell us change
state variable f > . general, MPTs derived STRIPS tasks derived
predicates occur negatively condition, domain transition graph contain sufficient
information changing value derived variable true false. Derived variables
208

fiT FAST OWNWARD P LANNING YSTEM

never assume value due derivation value; negation failure semantics,
assume value default value derived. want reason
ways setting value derived variable , need make information explicit.
logical notation, whether derived variable assumes given value triggering
axiom given layer determined formula disjunctive normal form, one disjunct
axiom setting value. example, axioms = open, r = (1, 1) f := >
= open, r = (3, 1) f := > correspond DNF formula (d = open r = (1, 1)) (d =
open r = (3, 1)). want know rules trigger, must negate formula,
leading CNF formula (d 6= open r 6= (1, 1))(d 6= open r 6= (3, 1)). able encode
information domain transition graph, need replace inequalities equalities
translate formula back DNF. Since transformations increase formula size
dramatically, apply simplifications along way, removing duplicated dominated disjuncts.
result case DNF formula = closed r = (2, 1) r = (1, 2) r = (2, 2) r =
(3, 2).
domain transition graph derived variable enriched contain possible
ways causing variable assume value called extended domain transition graph,
shown G RID example Fig. 11 (right). Since computing extended domain transition
graph costly always necessary, knowledge compilation component scans
conditions planning task (axioms, operator preconditions effect conditions, goal)
occurrences pairings type v = derived variables v. Extended domain transition
graphs computed derived variables required.
Note negative occurrences derived variables cascade: u, v w derived
variables domain {>, } condition v = present operator precondition,
moreover v defined axiom u = >, w = > v := >, v assumes value
whenever u w do, would require extended domain transition graphs u w well.
hand, multiple layers negation failure cancel out: derived
variable v occurs conditions form v = never positive form defined
axiom u = , w = v := >, necessarily require extended domain transition
graphs u w.
general, whether need extended domain transition graphs derived variable
determined following rules:
v derived variable condition v = 6= appears operator
precondition, effect condition goal, v used positively.
v derived variable condition v = appears operator precondition,
effect condition goal, v used negatively.
v derived variable condition v = 6= appears body
axiom whose head used positively (negatively), v used positively (negatively).
v derived variable condition v = appears body axiom
whose head used positively (negatively), v used negatively (positively).
knowledge compilation component computes extended domain transition graphs derived variables used negatively (standard) domain transition graphs state
variables. Normal domain transition graphs computed going set axioms
209

fiH ELMERT

set operator effects following Definition 5, reasonably straight-forward; computation extended domain transition graphs outlined above. Therefore, algorithmic
aspects topic require discussion.
5.2 Causal Graphs
Causal graphs introduced informally introduction. formal definition.
Definition 6 Causal graphs
Let multi-valued planning task variable set V. causal graph , symbols
CG(), directed graph vertex set V containing arc (v, v 0 ) iff v 6= v 0 one
following conditions true:
domain transition graph v 0 transition condition v.
set affected variables effect list operator includes v v 0 .
first case, say arc induced transition condition. second case say
induced co-occurring effects.
course, arcs induced transition conditions arcs induced co-occurring effects
mutually exclusive. causal graph arc generated reasons.
Informally, causal graph contains arc source variable target variable changes
value target variable depend value source variable. arcs
included dependency form effect source variable. agrees
definition dependency graphs Jonsson Backstrom (1998b), although authors
distinguish two different ways arc graph introduced using
labelled arcs.
Whether co-occurring effects induce arcs causal graph depends intended semantics: arcs included, set causal graph ancestors anc(v) variable
v precisely variables relevant goal change value v. Plans
goal computed without considering variables outside anc(v), eliminating variables outside anc(v) planning task simplifying axioms operators accordingly.
call achievability definition causal graphs, causal graphs encode variables
important achieving given assignment state variable.
However, achievability definition, planner considers anc(v) generating
action sequence achieves given valuation v may modify variables outside anc(v), i. e.,
generated plans side effects could destroy previously achieved goals otherwise
negative impact overall planning. Therefore, prefer definition, call
separability definition causal graphs.
5.2.1 ACYCLIC C AUSAL G RAPHS
Following separability definition causal graphs, solving subproblem variables anc(v)
always possible without changing values outside anc(v). leads us following
observation.
210

fiT FAST OWNWARD P LANNING YSTEM

Observation 7 Acyclic causal graphs strongly connected domain transition graphs
Let MPT CG() acyclic, domain transition graphs strongly connected,
derived variables, trivially false conditions occur operators goals.
solution.
trivially false conditions, mean conditions kind {v = d, v = 0 } 6= d0 .
Note similarity Observation 7 results Williams Nayak (1997) planning domains unary operators, acyclic causal graphs reversible transitions. separability
definition causal graphs, acyclic causal graphs imply unariness operators operators
several effects introduce causal cycles. Moreover, strong connectedness domain transition
graphs closely related Williams Nayaks reversibility property, although weaker
requirement.
truth observation easily seen inductively: planning task one state
variable domain transition graph strongly connected, state (of one variable)
transformed state applying graph search techniques. planning task
several state variables causal graph acyclic, pick sink causal graph, i. e.,
variable v without outgoing arcs, check goal defined variable. not,
remove variable task, thus reducing problem one fewer state variables,
solved recursively. yes, search path 0 (v) s? (v) domain transition graph
v, guaranteed exist graph strongly connected. yields high-level
plan setting v s? (v) fleshed recursively inserting plans setting
variables predecessors v causal graph values required transitions
form high-level plan. desired value v set, v eliminated
planning task remaining problem solved recursively.
algorithm shown Fig. 12. Although backtrack-free, require exponential
time execute generated plans exponentially long. unavoidable; even
MPTs satisfy conditions Observation 7, shortest plans exponentially long.
family planning tasks property given proof Theorem 4.4 article
Backstrom Nebel (1995).
method solving multi-valued planning tasks essentially planning refinement:
begin constructing abstract skeleton plan, merely path domain transition
graph, lower level abstraction adding operators satisfy preconditions required
transitions taken path. Strong connectedness domain transition graphs guarantees
every abstract plan actually refined concrete plan. precisely Bacchus
Yangs (1994) downward refinement property (cf. Section 2.1).
5.2.2 G ENERATING



P RUNING C AUSAL G RAPHS

usefulness causal graphs planning refinement limited acyclic case. Consider subset V 0 task variables contains causal graph descendants. general,
restrict task V 0 removing occurrences variables initial state, goal,
operators axioms, obtain abstraction original problem satisfies Knoblocks
(1994) ordered monotonicity property (Section 2.1).
Unfortunately, one major problem approach requirement include causal
graph descendants quite limiting. uncommon causal graph planning task
strongly connected, case technique allow us abstract away variables
211

fiH ELMERT

algorithm solve-easy-MPT(V, s0 , s? , O):
s? = :
{ goal empty: empty plan solution. }
return hi.
else:
Let v V variable occurring preconditions effect conditions O.
{ variable always exists causal graph task acyclic. }
V 0 := V \ {v}.
0 := { | affect v }.
plan := hi
s? (v) defined:
Let t1 , . . . , tk path transitions DTG(v) 0 (v) s? (v).
{ t1 , . . . , tk high-level plan reaches goal v,
ignores preconditions variables. }
{t1 , . . . , tk }:
{ Recursively find plan achieves conditions t. }
Let cond condition operator associated t.
Let s00 state reached executing plan, restricted V 0 .
Extend plan solve-easy-MPT(V 0 , s00 , cond, 0 ).
Extend plan o.
{ dealing v, recursively plan goals remaining variables. }
Let s00 state reached executing plan, restricted V 0 .
s0? := s? restricted V 0 .
Extend plan solve-easy-MPT(V 0 , s00 , s0? , 0 ).
return plan
Figure 12: Planning algorithm MPTs acyclic causal graph strongly connected domain
transition graphs.

212

fiT FAST OWNWARD P LANNING YSTEM

all. However, heuristic approach, free simplify planning task. particular,
ignoring operator preconditions purposes heuristic evaluation, make
arbitrary causal graph acyclic. Clearly, aspects real task ignore, worse
expect heuristic approximate actual goal distance. Considering this, aim ignore
little information possible. explain done.
knowledge compilation component begins causal graph processing generating
full causal graph (Definition 6). One consequence separability definition causal graphs
state variables ancestors variables mentioned goal completely
irrelevant. Therefore, computed graph, compute causal graph ancestors
variables goal. state variables found goal ancestors eliminated planning task causal graph, associated operators axioms removed. 4
Afterwards, compute pruned causal graph, acyclic subgraph causal graph
vertex set. try fashion important causal dependencies retained
whenever possible. specifically, apply following algorithm.
First, compute strongly connected components causal graph. Cycles occur
within strongly connected components, component dealt separately. Second,
connected component, compute total order vertices, retaining
arcs (v, v 0 ) v v 0 . v v 0 , say v 0 higher level v. total order
computed following way:
1. assign weight arc causal graph. weight arc n induced
n axioms operators. lower cumulated weight incoming arcs vertex,
fewer conditions ignored assigning low level vertex.
2. pick vertex v minimal cumulated weight incoming arcs select
lowest level, i. e., set v v 0 vertices v 0 strongly connected component.
3. Since v dealt with, remove vertex incident arcs consideration
rest ordering algorithm.
4. remaining problem solved iteratively applying technique order
vertices single vertex remains.
reader notice pruning choices within strongly connected component
performed greedy algorithm. could try find sets arcs minimal total weight
eliminating arcs results acyclic graph. However, NP-equivalent problem,
even case unweighted graphs (Garey & Johnson, 1979, problem GT8).
generating pruned causal graph, prune domain transition graphs removing transition labels DTG(v) conditions variables v 0 v v 0 .
conditions ignored heuristic computation. Finally, simplify domain transition
graphs removing dominated transitions: 0 transitions two values
variable, condition proper subset condition 0 , transition
easier apply t0 , remove t0 . Similarly, several transitions identical
conditions, keep one them.
4. simplification closely related Knoblocks criterion problem-specific ordered monotonicity property
(Knoblock, 1994).

213

fiH ELMERT

t1

t2

a1

p1

p2

a2

Figure 13: Causal graph L OGISTICS task. State variables ai encode locations
trucks airplanes, state variables p locations packages.

f1

p1

f2

f3

f1

f2

f3

l1

l2

l1

l2

c1

c2

c1

c2

p2

p1

p2

Figure 14: Causal graph YSTERY task (left) relaxed version task (right). State
variables encode fuel location, state variables l ci encode locations
remaining capacities trucks, state variables p encode locations packages.

5.2.3 C AUSAL G RAPH E XAMPLES
give impression types causal graphs typically found standard benchmarks
effects pruning, show examples increasing graph complexity.
first simplest example, Fig. 13 shows causal graph task L OGISTICS
domain, featuring two trucks, two airplanes two packages. seen, graph acyclic,
requires pruning causal graph heuristic. Since L OGISTICS tasks feature strongly
connected domain transition graphs, even solved polynomial solve-easy-MPT
algorithm.
slightly complicated example, next figure, Fig. 14, shows task YS TERY domain three locations, two trucks two packages. causal graph contains
number cycles, mostly local. pruning arcs vertices l fj , ignore
214

fiT FAST OWNWARD P LANNING YSTEM

r

r



l



k1

k2

l

k1

k2

Figure 15: Causal graph G RID task (left) relaxed version task (right). State
variable r encodes location robot, encodes status robot arm (empty
carrying key), l encodes status locked location (locked open), k 1
k2 encode locations two keys.

fact must move trucks certain locations want use fuel location.
using fuel useful thing do, big loss information. pruning arcs
vertices pi cj , ignore fact vehicles increase decrease current
capacity unloading loading packages. Compared heuristics based ignoring delete effects, great loss information, since ignoring delete effects YSTERY domain
almost amounts ignoring capacity fuel constraints altogether. pruning arcs,
eliminate cycles causal graph, YSTERY domain considered fairly
well-behaved.
worse case shown Fig. 15, shows example G RID domain
arbitrary number locations, single one locked. two keys, one
unlock locked location. Eliminating cycles requires minor relaxations regarding
status robot arm (empty non-empty), one major simplification, namely
elimination arc l r representing fact robot enter locked
location unlocked.
(nearly) worst-case example, consider task B LOCKSWORLD domain (no figure).
typical MPT encoding uses one state variable h encoding whether hand empty
two state variables per block task: i-th block, encodes whether block
lying table, bi encodes block lying top it, clear held
arm. causal graph task, variable h ingoing arcs outgoing arcs
state variables, state variables b connected directions.
state variables ti slightly simpler connection structure, connected h
bi value i. relaxation problem eliminates cycles causal
graph loses large amount information, surprising EPOT domain,
includes B LOCKSWORLD subproblem, one precursor Fast Downward fared
worst (Helmert, 2004). Still, pointed planners ignore delete effects
similar problems B LOCKSWORLD-like domains, comparison causal
graph heuristics article shows.
215

fiH ELMERT

5.3 Successor Generators Axiom Evaluators
addition good heuristic guidance, forward searching planning system needs efficient methods
generating successor states applied benchmark suite international
planning competitions. domains, causal graph heuristic popular methods
heuristic provide excellent goal estimates, yet still planning time-consuming
long plans vast branching factors.
variant best-first search implemented Fast Downward compute heuristic
estimate state generated. Essentially, heuristic evaluations computed
closed nodes, computation deferred nodes search frontier. domains
strong heuristic guidance large branching factors, number nodes frontier
far dominate number nodes closed set. case point, consider problem instance
ATELLITE #29. solving task, default configuration Fast Downward computes
heuristic estimates 67 597 world states adding 107 233 381 states frontier. Clearly,
determining set applicable operators quickly critical importance scenario.
ATELLITE tasks, almost 1 000 000 ground operators, try
avoid individually checking operator applicability. Similarly, biggest PSR tasks,
100 000 axioms must evaluated state compute values derived
variables, computation must made efficient. purposes, Fast Downward uses two
data structures called successor generators axiom evaluators.
5.3.1 UCCESSOR G ENERATORS
Successor generators recursive data structures similar decision trees. internal nodes
associated conditions, likened decisions decision tree, leaves
associated operator lists likened set classified samples decision tree
leaf. formally defined follows.
Definition 8 Successor generators
successor generator MPT = hV, 0 , s? , A, Oi tree consisting selector nodes
generator nodes.
selector node internal node tree. associated variable v V called
selection variable. Moreover, |D v |+1 children accessed via labelled edges, one edge labelled
v = value Dv , one edge labelled >. latter edge called dont care
edge selector.
generator node leaf node tree. associated set operators called
set generated operators.
operator must occur exactly one generator node, set edge labels
leading root node (excluding dont care edges) must equal precondition o.
Given successor generator MPT state , compute set
applicable operators traversing successor generator follows, starting root:
selector node selection variable v, follow edge v = s(v) dont care edge.
generator node, report generated operators applicable.
216

fiT FAST OWNWARD P LANNING YSTEM

algorithm evaluate-axiom-layer(s, ):
axiom Ai :
a.counter := |a.cond|
variable v:
axiom Ai condition v = s(v) body:
a.counter := a.counter 1
exists axiom Ai a.counter = 0 yet considered:
Let hv, di head axiom.
s(v) 6= d:
s(v) :=
axiom Ai condition v = body:
a.counter := a.counter 1
Figure 16: Computing values derived variables given planning state.
build successor generator , apply top-down algorithm considers task
variables arbitrary order v1 v2 vn . root node, choose v1 selection variable classify set operators according preconditions respect v 1 . Operators
precondition v1 = represented child root accessed edge
corresponding label, operators without preconditions v 1 represented child
root accessed dont care edge. children root, choose v 2 selection
variable, grandchildren v3 , on.
one exception rule avoid creating unnecessary selection nodes: operator
certain branch tree condition v , vi considered selection variable
branch. construction branch ends variables considered,
stage generator node created operators associated branch.
5.3.2 XIOM E VALUATORS
Axiom evaluators simple data structure used efficiently implementing well-known
marking algorithm propositional Horn logic (Dowling & Gallier, 1984), extended modified
layered logic programs correspond axioms MPT. consist two parts.
Firstly, indexing data structure maps given variable/value pairing given axiom layer
set axioms given layer whose body pairing appears. Secondly, set counters,
one axiom, counts number conditions axiom yet derived.
Within Fast Downward, axioms evaluated two steps. First, derived variables set
default value . Second, algorithm evaluate-axiom-layer (Fig. 16) executed axiom
layer sequence determine final values derived variables.
assume reader familiar enough marking algorithm require much
explanation, point test whether axiom ready trigger implemented means queue axioms put soon counter reaches 0. actual
implementation evaluate-axiom-layer within Fast Downward initializes axiom counters slightly
efficiently indicated pseudo-code. However, minor technical detail,
turn remaining piece Fast Downwards architecture, search component.
217

fiH ELMERT

6. Search
Unlike translation knowledge compilation components, single
mode execution, search component Fast Downward perform work various alternative ways. three basic search algorithms choose from:
1. Greedy best-first search: standard textbook algorithm (Russell & Norvig, 2003),
modified technique called deferred heuristic evaluation mitigate negative influence wide branching. extended algorithm deal preferred operators, similar FFs helpful actions (Hoffmann & Nebel, 2001). discuss greedy best-first
search Section 6.3. Fast Downward uses algorithm together causal graph
heuristic, discussed Section 6.1.
2. Multi-heuristic best-first search: variation greedy best-first search evaluates
search states using multiple heuristic estimators, maintaining separate open lists each.
variant greedy best-first search, supports use preferred operators. Multiheuristic best-first search discussed Section 6.4. Fast Downward uses algorithm
together causal graph heuristics, discussed Sections 6.1 6.2.
3. Focused iterative-broadening search: simple search algorithm use
heuristic estimators, instead reduces vast set search possibilities focusing
limited operator set derived causal graph. experimental algorithm;
future, hope develop basic idea algorithm robust method.
Focused iterative-broadening search discussed Section 6.5.
two heuristic search algorithms, second choice must made regarding use
preferred operators. five options supported planner:
1. use preferred operators.
2. Use helpful transitions causal graph heuristic preferred operators.
3. Use helpful actions heuristic preferred operators.
4. Use helpful transitions preferred operators, falling back helpful actions
helpful transitions current search state.
5. Use helpful transitions helpful actions preferred operators.
five options combined two heuristic search algorithms,
total eleven possible settings search component, ten using one
heuristic algorithms one using focused iterative-broadening search.
addition basic settings, search component configured execute several
alternative configurations parallel making use internal scheduler. configurations
Fast Downward participated IPC4 made use feature running one configuration
heuristic search algorithms parallel focused iterative-broadening search. heuristic
search algorithm, configuration Fast Downward employed greedy best-first search helpful
transitions, falling back helpful actions necessary (option 4.). configuration Fast Diagonally Downward employed multi-heuristic best-first search using helpful transitions helpful
actions preferred operators (option 5.).
218

fiT FAST OWNWARD P LANNING YSTEM

avoid confusion complete Fast Downward planning system particular
configuration called Fast Downward, refer IPC4 planner configurations FD
FDD rest paper. name planning system whole never abbreviated.
6.1 Causal Graph Heuristic
causal graph heuristic centrepiece Fast Downwards heuristic search engine. estimates cost reaching goal given search state solving number subproblems
planning task derived looking small windows (pruned) causal graph.
additional intuitions design heuristic discussion theoretical aspects,
refer article heuristic first introduced (Helmert, 2004).
6.1.1 C ONCEPTUAL V IEW



C AUSAL G RAPH H EURISTIC

state variable v pair values d, 0 Dv , causal graph heuristic computes
heuristic estimate cost v (d, d0 ) cost changing value v 0 , assuming
state variables carry values current state. (This simplification. Cost
estimates computed state variables v values never required.
ignore fact discussing heuristic conceptual level.) heuristic estimate
given state sum costs cost v (s(v), s? (v)) variables v goal
condition s? (v) defined.
Conceptually, cost estimates computed one variable other, traversing (pruned)
causal graph bottom-up fashion. bottom-up, mean start variables
predecessors causal graphs; call order computation bottom-up
consider variables change state accord low-level, variables
whose state transitions require help variables complex transition semantics
thus considered high-level. Note figures depicting causal graphs, high-level
variables typically displayed near bottom.
variables without predecessors causal graph, cost v (d, d0 ) simply equals cost
shortest path d0 (pruned) domain transition graph DTG(v). variables,
cost estimates computed graph search domain transition graph. However,
conditions transitions must taken account path planning, addition
counting number transitions required reach destination value, consider costs
achieving value changes variables necessary set transition conditions.
important point computing values cost v (d, d0 ), completely consider
interactions state variable v predecessors causal graph. changing
value d0 requires several steps steps associated condition
variable v 0 , realize v 0 must assume values required conditions sequence.
example, v represents package transportation task must moved B
means vehicle located C, recognize vehicle must first move C
B order drop package B. different way HSPor FF-based heuristics work examples. However, consider interactions
immediate predecessors v causal graph. Interactions occur via several graph layers
captured heuristic estimator.
essence, compute cost v (d, d0 ) solving particular subproblem MPT, induced
variable v predecessors pruned causal graph. subproblem, assume
219

fiH ELMERT

algorithm compute-costs-bottom-up(, s):
variable v , traversing pruned causal graph bottom-up order:
Let V 0 set immediate predecessors v pruned causal graph.
pair values (d, d0 ) Dv Dv :
Generate planning task v,d,d0 following components:
Variables: V 0 {v}.
Initial state: v = v 0 = s(v 0 ) v 0 V 0 .
Goal: v = d0 .
Axioms operators:
1. corresponding transitions pruned DTG v.
2. variables v 0 V 0 values e, e0 Dv0 , operator
precondition v 0 = e, effect v 0 = e0 cost cost0v (e, e0 ).
{ Note variables v 0 V 0 considered previously,
cost values known. }
Set costv (d, d0 ) cost plan solves v,d,d0 .
Figure 17: compute-costs-bottom-up algorithm, high-level description causal graph
heuristic.

v initially set d, want v assume value 0 , state variables carry
value current state. call planning problem local subproblem v, 0 ,
local subproblem v leave target value 0 open.
formalization intuitive notions cost estimates generated, consider
pseudo-code Fig. 17. reflect way heuristic values actually computed
within Fast Downward; algorithm figure would far expensive evaluate
search state. However, computes cost values Fast Downward does, provided
algorithm generating plans last line algorithm one one used
real cost estimator.
6.1.2 C OMPUTATION



C AUSAL G RAPH H EURISTIC

actual computation causal graph heuristic traverses causal graph top-down direction starting goal variables, rather bottom-up starting variables without causal
predecessors. fact, top-down traversal causal graph reason Fast Downwards
name.
Computing cost estimates top-down traversal implies algorithm computing
plans local subproblems given variable, typically yet know costs changing
state causal predecessors. algorithm compute-costs addresses evaluating
cost values dependent variables recursive invocations itself.
given variable-value pairing v = d, always compute costs cost v (d, d0 ) values
d0 Dv time, similar way Dijkstras algorithm computes shortest path
single source single destination vertex, single source possible destination
vertices. Computing costs values 0 (much) expensive computing
220

fiT FAST OWNWARD P LANNING YSTEM

one values, cost values determined, cache re-use
needed later parts computation heuristic value
current state.
fact, similarity shortest path problems superficial runs quite deeply.
ignore recursive calls computing cost values dependent variables, compute-costs basically implementation Dijkstras algorithm single-source shortest path problem
domain transition graphs. difference regular algorithm lies fact
know cost using arc advance. Transitions derived variables base cost
0 transitions fluents base cost 1, addition base cost, must pay
cost achieving conditions associated transition. However, cost achieving
given condition v 0 = e0 depends current value e state variable time transition
taken. Thus, compute real cost transition know values
dependent state variables relevant situation.
course, many different ways taking transitions domain transition graphs,
potentially leading different values dependent state variables. first introduced
causal graph heuristic, showed deciding plan existence local subproblems NPcomplete (Helmert, 2004), content approach lead complete
planning algorithm, long works well subproblems face practice.
approach chosen achieve value state variable v local subproblem
v quickly possible, following greedy policy. context Dijkstra algorithm,
means start finding cheapest possible plan make transition
value d0 . found cheapest possible plan d0 , commit it, annotating
vertex d0 domain transition graph local state obtained applying plan d0
current state. next step, look cheapest possible plan achieve another value 00 ,
either considering transitions start initial value d, considering transitions
continue plan d0 moving neighbour d0 . process iterated vertices
domain transition graph reached progress possible.
implementation follows Dijkstras algorithm (Fig. 18). implemented priority queue vector buckets maximal speed use cache avoid generating
costv (d, d0 ) value twice state. addition this, use global cache shared
throughout whole planning process need compute values cost v (d, d0 ) variables v ancestors pruned causal graph once. (Note cost v (d, d0 ) depends
current values ancestors v.)
Apart technical considerations, Fig. 18 gives accurate account
Fast Downwards implementation causal graph heuristic. details, including
complexity considerations worked-out example, refer original description
algorithm (Helmert, 2004).
6.1.3 TATES

NFINITE

H EURISTIC VALUE

noted Fast Downward uses incomplete planning algorithm determining solutions
local planning problems. Therefore, states cost v (s(v), s? (v)) = even though
goal condition v = s? (v) still reached. means cannot trust infinite values
returned causal graph heuristic. experience, states infinite heuristic evaluation
still possible reach goal rare, indeed treat states dead ends.
221

fiH ELMERT

algorithm compute-costs(, s, v, d):
Let V 0 set immediate predecessors v pruned causal graph .
Let DTG pruned domain transition graph v.
costv (d, d) := 0
costv (d, d0 ) := d0 Dv \ {d}
local-state := restricted V 0
unreached := Dv
unreached contains value d0 Dv cost v (d, d0 ) < :
Choose value d0 unreached minimizing cost v (d, d0 ).
unreached := unreached \ {d0 }
transition DTG leading 0 d00 unreached:
transition-cost := 0 v derived variable; 1 v fluent
pair v 0 = e0 condition t:
e := local-state d0 (v 0 )
call compute-costs(, s, v 0 , e).
transition-cost := transition-cost + cost v0 (e, e0 )
costv (d, d0 ) + transition-cost < cost v (d, d00 ):
costv (d, d00 ) := costv (d, d0 ) + transition-cost
local-state d00 := local-state d0
pair v 0 = e0 condition t:
local-state d00 (v 0 ) := e0
Figure 18: Fast Downwards implementation causal graph heuristic: compute-costs algorithm computing estimates cost v (d, d0 ) values d0 Dv state
MPT .

222

fiT FAST OWNWARD P LANNING YSTEM

turns states search frontier dead ends, cannot make progress
causal graph heuristic. case, use sound dead-end detection routine verify
heuristic assessment. turns frontier states indeed dead ends, report
problem unsolvable. Otherwise, search restarted heuristic (cf. Section 6.2),
sound purposes dead-end detection. 5
dead-end detection routine originally developed STRIPS-like tasks. However,
extending full MPTs easy; fact, changes core algorithm required, works
level domain transition graphs still sound applied tasks conditional
effects axioms. Since central aspect Fast Downward, discuss here,
referring earlier work instead (Helmert, 2004).
6.1.4 H ELPFUL RANSITIONS
Inspired Hoffmanns successful use helpful actions within planner (Hoffmann &
Nebel, 2001), extended algorithm computing causal graph heuristic
addition heuristic estimate, generates set applicable operators considered useful
steering search towards goal.
compute helpful actions FF, Hoffmanns algorithm generates plan relaxed planning task defined current search state considers operators helpful belong
relaxed plan applicable current state.
approach follows similar idea. computing heuristic estimate cost v (s(v), s? (v))
variable v goal condition defined, look domain transition graph
v trace path transitions leading s(v) ? (v) gave rise cost estimate.
particular, consider first transition path, starting s(v). transition corresponds
applicable operator, consider operator helpful transition continue check
next goal. transition correspond applicable operator associated
conditions form v 0 = e0 currently satisfied, recursively look helpful transitions domain transition graph variable v 0 , checking path
generated computation cost v0 (s(v 0 ), e0 ).
recursive process continues found helpful transitions. Unlike case
FF, helpful actions found non-goal states, might find helpful
transition all. may case transition correspond applicable operator
even though associated conditions; happen operator preconditions
represented pruned domain transition graph due cycles causal graph. Even so,
found helpful transitions useful tool guiding best-first search algorithms.
6.2 Heuristic
heuristic named Hoffmanns planning algorithm name, context
originally introduced (Hoffmann & Nebel, 2001). based notion
relaxed planning tasks ignore negative interactions. context MPTs, ignoring negative
interactions means assume state variable hold several values simultaneously.
operator effect axiom sets variable v value original task corresponds
5. practice, never observed causal graph heuristic fail solvable task. Therefore, fallback
mechanism used unsolvable tasks ICONIC -F ULL ADL domain recognized
dead-end detection technique.

223

fiH ELMERT

effect axiom adds value range values assumed v relaxed task.
condition v = original task corresponds condition requiring element
set values currently assumed v relaxed task.
easy see applying operator solvable relaxed planning task never render
unsolvable. lead operators applicable goals true,
significant effect all. reason, relaxed planning tasks solved efficiently, even
though optimal solutions still NP-hard compute (Bylander, 1994). plan relaxation
planning task called relaxed plan task.
heuristic estimates goal distance world state generating relaxed plan
task reaching goal world state. number operators generated plan
used heuristic estimate. implementation heuristic necessarily
generate same, even equally long, relaxed plan FF. experiments, turn
problematic, implementations appear equally informative.
heuristic originally introduced ADL domains, extending tasks involving derived predicates straight-forward. One possible extension simply assume
derived predicate initially set default value treat axioms relaxed operators cost
0. slightly complicated, accurate approach, derived variables initialized
actual value given world state, allowing relaxed planner achieve value
(or values) applying transitions extended domain transition graph derived
variable. followed second approach.
addition heuristic estimates, heuristic exploited restricting biasing
choice operators apply given world state s. set helpful actions consists
operators relaxed plan computed applicable state. mentioned
introduction section, Fast Downward configured treat helpful actions
preferred operators.
wealth work heuristic literature, discuss further.
thorough treatment, point references (Hoffmann & Nebel, 2001; Hoffmann,
2001, 2002, 2005).
6.3 Greedy Best-First Search Fast Downward
Fast Downward uses greedy best-first search closed list default search algorithm.
assume reader familiar algorithm refer literature details (Russell &
Norvig, 2003).
implementation greedy best-first search differs textbook algorithm two ways.
First, treat helpful transitions computed causal graph heuristic helpful actions computed heuristic preferred operators. Second, performs deferred heuristic evaluation
reduce influence large branching factors. turn describing two search
enhancements.
6.3.1 P REFERRED PERATORS
make use helpful transitions computed causal graph heuristic helpful actions computed heuristic, variant greedy best-first search supports use so-called preferred operators. set preferred operators given state subset set applicable
operators state. operators considered preferred depends settings
224

fiT FAST OWNWARD P LANNING YSTEM

search component, discussed earlier. intuition behind preferred operators randomly
picked successor state likely closer goal generated preferred operator, case call preferred successor. Preferred successors considered
non-preferred ones average.
search algorithm implements preference maintaining two separate open lists, one
containing successors expanded states one containing preferred successors exclusively.
search algorithm alternates expanding regular successor preferred successor.
even iterations consider one open list, odd iterations other. matter
open list state taken from, successors placed first open list, preferred
successors additionally placed second open list. (Of course could limit first open
list contain non-preferred successors; however, typically total number successors
vast number preferred successors tiny. Therefore, cheaper add successors
first open list detect duplicates upon expansion scan list successors
determining element whether preferred.)
Since number preferred successors smaller total number successors,
means preferred successors typically expanded much earlier others. especially
important domains heuristic guidance weak lot time spent exploring plateaus.
faced plateaus, Fast Downwards open lists operate first-in-first-out fashion. (In
words: constant heuristic function, search algorithm behaves breadth-first
search.) Preferred operators typically offer much better chances escaping plateaus since
lead significantly lower effective branching factors.
6.3.2 EFERRED H EURISTIC E VALUATION
Upon expanding state s, textbook version greedy best-first search computes heuristic
evaluation successor states sorts open list accordingly.
wasteful many successors heuristic evaluations costly, two conditions often
true heuristic search approaches planning.
second modification comes play. successor better heuristic
estimate generated early leads promising path towards goal, would
avoid generating successors. Let us assume 1000 successors, 0 ,
10th successor generated, better heuristic estimate s. Furthermore, let us
assume goal reached 0 path non-increasing heuristic estimates.
would avoid computing heuristic values 990 later successors altogether.
Deferred heuristic evaluation achieves computing heuristic estimates successors expanded state immediately. Instead, successors placed open list
together heuristic estimate state s, heuristic estimates computed
expanded, time used sorting successors open
list, on. general, state sorted open list according heuristic evaluation
parent, initial state exception. fact, need put successor
state open list, since require representation want evaluate
heuristic estimate. Instead, save memory storing reference parent state
operator transforming parent state successor state open list.
might clear approach lead significant savings time, since deferred
evaluation means information available later. potential savings become
225

fiH ELMERT

apparent considering deferred heuristic evaluation together use preferred operators:
improving successor s0 state reached preferred operator, likely
expanded (via second open list) long successors even siblings
s. situation described above, exists non-increasing path 0 goal,
heuristic evaluations never computed successors s. fact, deferred heuristic
evaluation significantly improve search performance even preferred operators
used, especially tasks branching factors large heuristic estimate informative.
first glance, deferred heuristic evaluation might appear related another technique reducing effort expanding node within best-first search algorithm, namely Partial
Expansion (Yoshizumi, Miura, & Ishida, 2000). However, algorithm designed reducing
space requirements best-first search expense additional heuristic evaluations:
expanding node, Partial Expansion computes heuristic value successors,
stores open queue whose heuristic values fall certain relevance threshold.
later iterations, might turn threshold chosen low, case node
needs re-expanded heuristic values successors re-evaluated. general,
Partial Expansion never compute fewer heuristic estimates standard , usually
require less memory.
However, heuristic search approaches planning (and certainly Fast Downward), heuristic evaluations usually costly time memory storing open closed lists
limiting factor. thus willing trade memory time opposite way: Deferred
heuristic evaluation normally leads node expansions higher space requirements
standard best-first search heuristic values used guiding search less informative (they evaluate predecessor search node rather node itself). However, heuristic
computations required nodes actually removed open queue rather
nodes fringe, latter usually significantly numerous.
6.4 Multi-Heuristic Best-First Search
alternative greedy best-first search, Fast Downward supports extended algorithm called
multi-heuristic best-first search. algorithm differs greedy best-first search use
multiple heuristic estimators, based observation different heuristic estimators different weaknesses. may case given heuristic sufficient directing search
towards goal except one part plan, gets stuck plateau. Another heuristic
might similar characteristics, get stuck another part search space.
Various ways combining heuristics proposed literature, typically adding
together taking maximum individual heuristic estimates. believe often
beneficial combine different heuristic estimates single numerical value. Instead,
propose maintaining separate open list heuristic estimator, sorted according
respective heuristic. search algorithm alternates expanding state
open list. Whenever state expanded, estimates calculated according heuristic,
successors put open list.
Fast Downward configured use multi-heuristic best-first search, computes estimates causal graph heuristic heuristic, maintaining two open lists. course,
approach combined use preferred operators; case, search algorithm
maintains four open lists, heuristic distinguishes normal preferred successors.
226

fiT FAST OWNWARD P LANNING YSTEM

algorithm reach-one-goal(, v, d, cond):
{0, 1, . . . , max-threshold}:
Let set operators whose modification distance respect v
.
Assign cost c operator modification distance c
respect v.
Call uniform-cost-search algorithm closed list, using operator set ,
find state satisfying {v = d} cond.
return plan uniform-cost-search succeeded.
Figure 19: reach-one-goal procedure reaching state v = d. value max-threshold
equal maximal modification distance operator respect v.

6.5 Focused Iterative-Broadening Search
focused iterative-broadening search algorithm experimental piece Fast Downwards search arsenal. present form, algorithm unsuitable many planning domains,
especially containing comparatively different goals. Yet think might contain
nucleus successful approach domain-independent planning different
current methods, include completeness source inspiration.
algorithm intended first step towards developing search techniques emphasize
idea using heuristic criteria locally, limiting set operators apply, rather globally,
choosing states expand global set open states. made first experiments
direction observing large boost performance obtained using preferred
operators heuristic search. algorithm performed surprisingly well standard
benchmark domains, performing badly others.
name suggests, algorithm focuses search concentrating one goal time,
restricting attention operators supposedly important reaching goal:
Definition 9 Modification distances
Let MPT, let operator , let v variable .
modification distance respect v defined minimum, variables
v 0 occur affected variables effect list o, distance v 0 v CG().
example, operators modify v directly modification distance 0 respect
v, operators modify variables occur preconditions operators modifying v
modification distance 1, on. assume order change value variable,
operators low modification distance respect variable useful.
Fig. 19 shows reach-one-goal procedure achieving single goal MPT. time
being, assume cond parameter always . procedure makes use assumption
high modification distance implies low usefulness two ways. First, operators high
modification distance respect goal variable considered higher associated
cost, hence applied less frequently. Second, operators whose modification distance beyond
certain threshold forbidden completely. Instead choosing threshold priori, algorithm
227

fiH ELMERT

first tries find solution lowest possible threshold 0, increasing threshold 1
whenever previous search failed. uniform-cost-search algorithm mentioned Fig. 19
standard textbook method (Russell & Norvig, 2003).
Although ignorant fact time algorithm conceived, core idea
reach-one-goal new: Ginsberg Harvey (1992) present search technique called iterative
broadening, based idea repeatedly sequence uninformed searches
ever-growing set operators. work demonstrates superiority iterative broadening standard depth-bounded search empirically analytically reasonable
assumption choices made branching point equally important. 6 original iterative broadening algorithm applies scenarios without knowledge problem domain,
chooses set operators may applied every search node randomly, rather
using heuristic information causal graph case. However, Ginsberg Harvey
already discuss potential incorporation heuristics operator selection. introduction operator costs (in form modification distances) new, fairly straightforward
extension heuristic information available.
focused iterative-broadening search algorithm based reach-one-goal method;
idea achieve goals planning task one other, using reach-one-goal
algorithm core subroutine satisfying individual goals. Since obvious good
order achieving goals would be, one invocation reach-one-goal started goal
parallel. one-goal solver focuses (supposedly) relevant operators reaching
particular goal, hope number states considered goal reached small.
one one-goal solvers reaches goal, resulting plan reported sub-searches
stopped. overall search algorithm commits part plan; situation
first goal reached considered new initial state.
situation, try satisfy second goal, starting parallel invocations
reach-one-goal possible second goal. course, lead situation
search algorithm oscillates goals, first achieving goal a, abandoning favour goal
b, without sign making real progress. Therefore, demand reach-one-goal achieves
second goal addition one reached first, setting cond argument accordingly.
two goals reached, sub-searches stopped, sub-searches third
goal started, on, goals reached.
sense, focusing technique similar beam search algorithm (Lowerre, 1976),
performs fixed number concurrent searches avoid committing particular path
search space early. Beam search uses heuristic function evaluate branches
search abandoned new branches spawned. focused iterativebroadening search appear use heuristic evaluations first glance, number satisfied
goals state used evaluation criterion essentially way. One important difference beam search use modification distances relative particular goal, means
different beams explore state space qualitatively different ways.
one final twist: motivate reach-one-goal needlessly wander away satisfied goals, forbid applying operators undo previously achieved goals cond.
old idea called goal protection (Joslin & Roach, 1989). well-known protecting
6. See original analysis precise definition equally important (Ginsberg & Harvey, 1992). Ginsberg
Harveys assumption certainly valid practice, find much convincing competing model
goal states uniformly distributed across search fringe.

228

fiT FAST OWNWARD P LANNING YSTEM

algorithm reach-one-goal(, v, d, cond):
{0, 1, . . . , max-threshold}:
Let set operators whose modification distance respect v
affect state variable occurring cond.
Assign cost c operator modification distance c
respect v.
Call uniform-cost-search algorithm closed list, using operator set ,
find state satisfying {v = d} cond.
return plan uniform-cost-search succeeded.
{0, 1, . . . , max-threshold}:
Let set operators whose modification distance respect v
.
Assign cost c operator modification distance c
respect v.
Call uniform-cost-search algorithm closed list, using operator set ,
find state satisfying {v = d} cond.
return plan uniform-cost-search succeeded.
Figure 20: reach-one-goal procedure reaching state v = (corrected).
goals renders search algorithm incomplete, even state spaces operators reversible
local search approaches focused iterative-broadening search would otherwise complete.
particular, search must fail planning tasks serializable (Korf, 1987). Therefore,
first solution attempt fails, algorithm restarted without goal protection. complete
procedure shown Fig. 20, concludes discussion Fast Downwards search component.

7. Experiments
evaluate performance Fast Downward, specifically differences various
configurations search component, performed number experiments set
benchmarks previous international planning competitions. purpose experiments compare Fast Downward state art PDDL planning, contrast
performance different search algorithms Fast Downward (greedy best-first search
without preferred operators, multi-heuristic best-first search without preferred operators,
focused iterative-broadening search).
clearly state purpose experiments, let us point two areas worthy study
choose investigate here:
compare causal graph heuristic heuristics, HSP
heuristics. comparison would require evaluating different heuristics within otherwise identical planning systems. performed experiment (Helmert,
2004) thus prefer dedicate section evaluation complete Fast Downward
planning system, rather heuristic function.
229

fiH ELMERT

give final answer question Fast Downward performs well badly
domains analyse. observe bad performance, try give plausible
explanation this, conduct full-blown study heuristic quality spirit
Hoffmanns work h+ heuristics (Hoffmann, 2005). believe
much could learned investigation, major undertaking would go
beyond scope article.
aim section evaluate Fast Downward planner whole,
number algorithmic questions address. example, one might wonder (if
any) speed-up obtained using successor generators simpler methods test
operator applicability whenever node expanded. Another question concerns extent
deferred heuristic evaluation affects search performance. keep section reasonable
length, discuss either questions here. However, conducted experiments
addressing them, include results electronic appendix paper. 7
7.1 Benchmark Set
benchmark set use consists propositional planning tasks fully automated
tracks first four international planning competitions hosted AIPS 1998, AIPS 2000, AIPS
2002 ICAPS 2004. set benchmark domains shown Fig. 21. Altogether, benchmark suite comprises 1442 tasks. (The numbers Fig. 21 add 1462, 20 ATELLITE
instances introduced IPC3 part benchmark set IPC4,
count once.)
distinguish three classes domains:
STRIPS domains: domains feature derived predicates conditional effects,
conditions appearing goal operators conjunctions positive literals.
ADL domains: domains make use conditional effects operator and/or contain
general conditions simple conjunctions goals operators. However,
require axioms.
PDDL2.2 domains: domains use full range propositional PDDL2.2, including
features present ADL domains axioms.
IPC4, domains presented different formulations, meaning realworld task encoded several different ways. Participants asked work one
formulation per domain, able choose preferred formulation given domain freely.
example, IRPORT domain available STRIPS formulation ADL formulation.
However, organizers strictly follow rule considering different encodings
real-world task different formulations, rather different domains proper. Namely,
PSR-M IDDLE P ROMELA domains, encodings without axioms available,
considered different domains grounds encodings without axioms
7. See http://www.jair.org/. short summary successor generators speed search two
orders magnitude extreme cases largest ATELLITE tasks, little impact performance
time. Deferred heuristic evaluation beneficial domains, speed-ups one order
magnitude common, somewhat beneficial majority domains, speed-ups 2 4,
rarely detrimental performance.

230

fiT FAST OWNWARD P LANNING YSTEM

Competition

Domain

Class

Number tasks

IPC1 (AIPS 1998)

SSEMBLY
G RID
G RIPPER
L OGISTICS
OVIE
YSTERY
MP RIME

ADL
STRIPS
STRIPS
STRIPS
STRIPS
STRIPS
STRIPS

30
5
20
35
30
30
35

IPC2 (AIPS 2000)

B LOCKSWORLD
F REECELL
L OGISTICS
ICONIC -STRIPS
ICONIC -S IMPLE ADL
ICONIC -F ULL ADL
CHEDULE

STRIPS
STRIPS
STRIPS
STRIPS
ADL
ADL
ADL

35
60
28
150
150
150
150

IPC3 (AIPS 2002)

EPOT
RIVERLOG
F REECELL
ROVERS
ATELLITE
Z ENOTRAVEL

STRIPS
STRIPS
STRIPS
STRIPS
STRIPS
STRIPS

22
20
20
20
20
20

IPC4 (ICAPS 2004)

IRPORT
P ROMELA -O PTICALT ELEGRAPH
P ROMELA -P HILOSOPHERS
P IPESWORLD -N OTANKAGE
P IPESWORLD -TANKAGE
PSR-S MALL
PSR-M IDDLE
PSR-L ARGE
ATELLITE

STRIPS
PDDL2.2
PDDL2.2
STRIPS
STRIPS
STRIPS
PDDL2.2
PDDL2.2
STRIPS

50
48
48
50
50
50
50
50
36

Figure 21: Planning domains first four international planning competitions.

231

fiH ELMERT

much larger hence likely difficult solve. apply formulation vs. encoding view
strictly thus consider one PSR-M IDDLE domain one domain two
P ROMELA variants, P ROMELA -P HILOSOPHERS P ROMELA -O PTICALT ELEGRAPH.
IPC1 benchmark set, tasks solvable except 11 YSTERY instances.
IPC2 benchmark set, tasks solvable except 11 ICONIC -F ULL ADL instances.
IPC3 benchmarks solvable. IPC4, checked instances P IPESWORLD TANKAGE domain, assume tasks solvable.
run heuristic search modes, Fast Downward proves unsolvability
unsolvable YSTERY ICONIC -F ULL ADL tasks using dead-end detection routine described earlier article causal graph heuristic (Helmert, 2004), cases
ICONIC -F ULL ADL domain exhaustively searching states finite heuristic.
course, unsolvable task proved unsolvable planner, report successfully
solved instance experimental results.
7.2 Experimental Setup
discussed Section 6, eleven possible configurations Fast Downwards search
component. However, equally reasonable. example, use FFs helpful
actions, would seem wasteful use heuristic estimate, since two calculated
together. Therefore, greedy best-first search setup, exclude configurations
helpful actions always computed. multi-heuristic best-first search setup, exclude
configurations one type preferred operators considered, other, since
would seem arbitrary choice. leaves us six different configurations
planner:
1. G: Use greedy best-first search without preferred operators.
2. G + P: Use greedy best-first search helpful transitions preferred operators.
3. G + P+ : Use greedy best-first search helpful transitions preferred operators. Use
helpful actions preferred operators states helpful transitions.
4. M: Use multi-heuristic best-first search without preferred operators.
5. + P: Use multi-heuristic best-first search helpful transitions helpful actions
preferred operators.
6. F: Use focused iterative-broadening search.
apply planner configurations 1442 benchmark tasks, using
computer 3.066 GHz Intel Xeon CPU machine used IPC4 set
memory limit 1 GB timeout 300 seconds.
compare Fast Downward state art, try solve benchmark
best-performing planners literature. Unfortunately, involves intricacies:
planners publicly available, others cover restricted subset PDDL2.2.
main experiment, thus partition benchmark domains three sets depending
planners available comparison.
232

fiT FAST OWNWARD P LANNING YSTEM

Domain

Task

Configuration

F REECELL (IPC2)
G RID
MP RIME
PSR-L ARGE
ATELLITE (IPC4)

probfreecell-10-1
prob05
prob14
p30-s179-n30-l3-f30
p33-HC-pfile13

M+P


G+P
M+P

Preprocessing
9.30
10.04
22.38
43.43
180.74

Search
298.64
291.01
291.67
265.29
169.09

Figure 22: Tasks could solved configuration Fast Downward search
timeout 300 seconds, total processing timeout 300 seconds.
column preprocessing shows total time translation knowledge compilation.

7.3 Translation Knowledge Compilation vs. Search
course, results report Fast Downward include time spent three components
planner: translation, knowledge compilation, search. Therefore, following presentation results, consider task solved total processing time 300 seconds.
However, investigated tasks solved timeout 300 seconds
search component alone, allowing components use arbitrary amount resources.
turns makes difference five cases, could solved
total time 310 seconds (Fig. 22). one five cases, ATELLITE instance
exorbitant size, search take less time two phases combined. results show
search component time-critical part Fast Downward practice. Therefore,
report separate performance results individual components.
7.4 STRIPS Domains IPC13
Let us present results main experiment. abstain listing runtimes individual planning tasks due prohibitively large amount data. available electronic
appendix article.8 Instead, report following information:
Tables showing number tasks solved planner within 300 second timeout.
Here, present individual results domain.
Graphs showing number tasks solved given time planner. Here,
present separate results domain, would require many graphs.
discuss plan lengths; observations regard similar made
original implementation causal graph heuristic (Helmert, 2004).
Fig. 23 shows number unsolved tasks STRIPS domains IPC13.
Figs. 24 25 show number tasks solved planner within given time bound
0 300 seconds. addition six configurations Fast Downward consideration,
table includes four columns.
heading Any, include results hypothetical meta-planner guesses
best six configuration Fast Downward input task executes Fast Downward
8. http://www.jair.org/

233

fiH ELMERT

Domain

#Tasks

G

B LOCKSWORLD
EPOT
RIVERLOG
F REECELL (IPC2)
F REECELL (IPC3)
G RID
G RIPPER
L OGISTICS (IPC1)
L OGISTICS (IPC2)
ICONIC -STRIPS
OVIE
YSTERY
MP RIME
ROVERS
ATELLITE (IPC3)
Z ENOTRAVEL

35
22
20
60
20
5
20
35
28
150
30
30
35
20
20
20

0
12
2
4
0
1
0
1
0
0
0
1
0
2
1
0

0
13
0
4
0
2
0
0
0
0
0
2
0
0
0
0

0
13
0
12
5
1
0
0
0
0
0
1
0
0
0
0

Total

550

24

21

32

G+P G+P+

M+P

F



CG



LPG

0
12
1
11
1
1
0
4
0
0
0
0
2
0
0
0

0
8
0
12
2
0
0
0
0
0
0
0
0
0
0
0

17
11
1
40
14
4
0
26
0
0
0
13
14
2
6
0

0
7
0
3
0
0
0
0
0
0
0
0
0
0
0
0

0
14
3
2
0
1
0
0
0
0
0
1
1
3
0
0

4
3
5
3
2
0
0
0
0
0
0
12
3
0
0
0

0
0
0
55
19
1
0
4
0
0
0
15
7
0
0
0

32

22

148

10

25

32

101

Figure 23: Number unsolved tasks STRIPS domains IPC1, IPC2, IPC3.
PSfrag replacements
FDD (Fast Downward)

550 (100%)

FD (Fast Downward)
YAHSP
Macro-FF

495 (90%)

LPG-TD
CG

LPG

Solved Tasks

SGPlan

440 (80%)

(Fast Downward)
G + P (Fast Downward)
+ P (Fast Downward)
G (Fast Downward)
G + P+ (Fast Downward)
(Fast Downward)
F (Fast Downward)

385 (70%)

0s

50s

100s

150s
Search Time

200s

250s

300s

Figure 24: Number tasks solved vs. runtime STRIPS domains IPC1, IPC2 IPC3.
graph shows results various configurations Fast Downward.

234

fiT FAST OWNWARD P LANNING YSTEM

PSfrag replacements
FDD (Fast Downward)

550 (100%)

FD (Fast Downward)
YAHSP
Macro-FF

495 (90%)

LPG-TD

Solved Tasks

SGPlan

440 (80%)

G + P+ (Fast Downward)
(Fast Downward)
G + P (Fast Downward)
CG

LPG

385 (70%)
G (Fast Downward)
+ P (Fast Downward)
(Fast Downward)
F (Fast Downward)

0s

50s

100s

150s
Search Time

200s

250s

300s

Figure 25: Number tasks solved vs. runtime STRIPS domains IPC1, IPC2 IPC3.
graph shows results CG, LPG hypothetical planner
always chooses best configuration Fast Downward. result greedy
best-first search helpful transitions repeated ease comparison Fig. 24.

235

fiH ELMERT

setting. heading CG, report results first implementation
causal graph heuristic (Helmert, 2004). 9 Finally, LPG refer well-known planners
(Hoffmann & Nebel, 2001; Gerevini et al., 2003) fully-automated tracks IPC2
IPC3. chosen comparison benchmark set showed best
performance far publicly available planners experimented with. LPG, uses
randomized search strategy, attempted solve task five times report median result.
results show excellent performance Fast Downward set benchmarks. Compared CG, already shown solve tasks LPG benchmark set
(Helmert, 2004), get another slight improvement half planner configurations. One
configurations, multi-heuristic best-first search using preferred operators, solves benchmarks
domains except EPOT F REECELL. Even importantly, number tasks
solved Fast Downward configurations small 10. Note planning competitions typically allowed planner spend 30 minutes task; time constraints,
could allocate five minutes six configurations Fast Downward, getting results
least good reported planner. Results might even better
cleverer allocation scheme.
Even configuration using focused iterative-broadening search performs comparatively well
benchmarks, although cannot compete planners. surprisingly,
version planner difficulties domains many dead ends (F REECELL, YSTERY,
MP RIME) goal ordering important (B LOCKSWORLD, EPOT). fares comparatively badly domains large instances, namely L OGISTICS (IPC1) ATELLITE.
reader keep mind LPG excellent planning systems;
planners experimented with, including awarded prizes first three planning competitions, none solved benchmarks group focused iterative-broadening
search.
one domain proves quite resistant Fast Downwards solution attempts configuration EPOT. already observed initial experiments causal graph heuristic
(Helmert, 2004), believe one key problem Fast Downward, unlike FF,
use goal ordering techniques, important domain. fact domain
includes B LOCKSWORLD-like subproblem problematic, gives rise dense causal
graphs demonstrated Section 5.2.3.
7.5 ADL Domains IPC13
Second, present results ADL domains first three planning competitions.
much smaller group previous, including four domains. time, cannot consider
CG LPG, since neither CG publicly available version LPG supports ADL domains.
Therefore, compare exclusively. Again, report number unsolved tasks
domain (Fig. 26) present graphs showing quickly tasks solved (Figs. 27 28).
results look good first group domains. Results ICONIC
domains good, even improving FF. However, greedy best-first search performs
badly SSEMBLY domain, configurations perform badly CHEDULE domain.
9. Apart missing support ADL axioms, CG similar Fast Downward using greedy best-first search
preferred operators (configuration G). translation knowledge compilation components essentially
identical. older search component mainly differs Fast Downward use deferred heuristic
evaluation.

236

fiT FAST OWNWARD P LANNING YSTEM

Domain

#Tasks

SSEMBLY
ICONIC -S IMPLE ADL
ICONIC -F ULL ADL
CHEDULE
Total

30
150
150
150
480

G
28
0
9
134
171

G+P G+P+
27
0
8
93
128

25
0
9
93
127

3
0
9
132
144

M+P

F





0
0
8
28
36

30
0
90
113
233

0
0
6
25
31

0
0
12
0
12

Figure 26: Number unsolved tasks ADL domains IPC1, IPC2 IPC3.

PSfrag replacements
FDD (Fast Downward)

480 (100%)

FD (Fast Downward)

432 (90%)

YAHSP
Macro-FF

384 (80%)

LPG-TD
CG

LPG

Solved Tasks

SGPlan

336 (70%)
288 (60%)

(Fast Downward)

240 (50%)
+ P (Fast Downward)
G + P+ (Fast Downward)
G + P (Fast Downward)
(Fast Downward)
G (Fast Downward)
F (Fast Downward)

192 (40%)
144 (30%)
0s

50s

100s

150s
Search Time

200s

250s

300s

Figure 27: Number tasks solved vs. runtime ADL domains IPC1, IPC2 IPC3.
graph shows results various configurations Fast Downward.

237

fiH ELMERT

PSfrag replacements
FDD (Fast Downward)

480 (100%)

FD (Fast Downward)

432 (90%)

YAHSP
Macro-FF

384 (80%)

LPG-TD
CG

LPG

G + P+ (Fast Downward)

Solved Tasks

SGPlan

336 (70%)
288 (60%)
240 (50%)

G + P (Fast Downward)
G (Fast Downward)

(Fast Downward)
F (Fast Downward)

192 (40%)


(Fast Downward)
+ P (Fast Downward)

144 (30%)
0s

50s

100s

150s
Search Time

200s

250s

300s

Figure 28: Number tasks solved vs. runtime ADL domains IPC1, IPC2 IPC3.
graph shows results hypothetical planner always
chooses best configuration Fast Downward. result multi-heuristic bestfirst search preferred operators repeated ease comparison Fig. 27.

238

fiT FAST OWNWARD P LANNING YSTEM

Currently, good explanation SSEMBLY behaviour. CHEDULE domain, weak performance seems related missing goal ordering techniques:
many CHEDULE tasks, several goals defined object satisfied
certain order. instance, objects cylindrical, polished painted,
three goals must satisfied precisely order: making object cylindrical reverts effects
polishing painting, polishing reverts effect painting. recognising constraints, heuristic search algorithm assumes close goal object already
polished painted cylindrical, loathe transform object cylindrical shape
would undo already achieved goals. rudimentary manual goal ordering,
ignoring painting goals goals satisfied, number tasks solved
multi-heuristic best-first search preferred operators drops 28 3. three failures
appear due remaining ordering problems regard cylindrical polished objects.
7.6 Domains IPC4
Third finally, present results IPC4 domains. Here, compare FF:
benchmarks, perform well best planners competition. Besides, several
IPC4 competitors extensions hybrids using part bigger system, FFbased planning well-represented even limit attention IPC4 planners.
comparison, chose four successful competition participants besides Fast Downward,
namely LPG-TD, SGPlan, Macro-FF YAHSP (cf. results Hoffmann & Edelkamp, 2005).
Similar previous two experiments, report number unsolved tasks domain
(Fig. 29) present graphs showing quickly tasks solved (Figs. 30 31).
Fast Downward competitive planners across domains, better others
some. P IPESWORLD domains ones planners noticeably better two competition versions Fast Downward. case YAHSP
P IPESWORLD domain variants SGPlan P IPESWORLD -N OTANKAGE . P IPESWORLD
domain hierarchical nature; might domain decomposition approach causal graph heuristic appropriate. results heuristic search
configurations P ROMELA -O PTICALT ELEGRAPH domain extremely bad require investigation.
Interestingly, focused iterative-broadening search performs well benchmarks suite. One reasons many tasks IPC4 suite,
many individual goals easy serialize solved mostly independently. 10
Comparing configuration G G + P + especially + P, observe using preferred operators useful benchmarks, even two previous
experiments.
final remark, observe implemented meta-planner calling six
Fast Downward configurations round-robin fashion, would obtain planning system
could solve 54 IPC4 benchmarks within 6 5 = 30 minute timeout. almost
par top performer IPC4, Fast Diagonally Downward, solved 52 IPC4
benchmarks timeout. Thus, benchmark set exploring different
planner configurations definitely pays off.
10. devised experiment shows property artificially violated simple goal reformulation,
performance algorithm degrades quickly; see electronic appendix details.

239

fiH ELMERT

Domain

#Tasks

IRPORT
P IPESWORLD -N OTANKAGE
P IPESWORLD -TANKAGE
P ROMELA -O PTICALT ELEGRAPH
P ROMELA -P HILOSOPHERS
PSR-S MALL
PSR-M IDDLE
PSR-L ARGE
ATELLITE (IPC4)
Total

50
50
50
48
48
50
50
50
36
432

G
28
24
36
48
0
0
0
22
8
166

G+P G+P+

M+P

F



30
25
36
47
0
0
0
20
0
158

14
7
17
46
0
0
0
22
3
109

0
10
34
13
21
1
22
39
22
162

0
7
14
13
0
0
0
20
0
54

17
23
36
48
0
0
0
22
0
146

18
14
34
47
16
0
0
23
8
160

Domain

FD

FDD

LPG-TD

Macro-FF

SGPlan

YAHSP

IRPORT
P IPESWORLD -N OTANKAGE
P IPESWORLD -TANKAGE
P ROMELA -O PTICALT ELEGRAPH
P ROMELA -P HILOSOPHERS
PSR-S MALL
PSR-M IDDLE
PSR-L ARGE
ATELLITE (IPC4)
Total

0
11
34
22
0
0
0
22
0
89

0
7
19
22
0
0
0
22
3
73

7
10
29
37
1
2
0
50
1
137

30
12
29
31
36
50
19
50
0
257

6
0
20
29
0
6
4
39
6
110

17
0
13
36
19
3
50
50
0
188

Figure 29: Number unsolved tasks IPC4 domains. Results various configurations
Fast Downward listed upper part, results competition participants
lower part. FD FDD denote versions Fast Downward participated IPC4 names Fast Downward Fast Diagonally Downward
(cf. Section 6).

240

fiT FAST OWNWARD P LANNING YSTEM

PSfrag replacements
FDD (Fast Downward)

432 (100%)

FD (Fast Downward)

389 (90%)

YAHSP

346 (80%)

Macro-FF
SGPlan

CG

LPG

302 (70%)
Solved Tasks

LPG-TD

(Fast Downward)

259 (60%)
216 (50%)
173 (40%)
130 (30%)
+ P (Fast Downward)
G + P+ (Fast Downward)
G + P (Fast Downward)
(Fast Downward)
F (Fast Downward)
G (Fast Downward)

86 (20%)
43 (10%)
0s

50s

100s

150s
Search Time

200s

250s

300s

Figure 30: Number tasks solved vs. runtime IPC4 domains. graph shows results
various configurations Fast Downward.
PSfrag replacements
432 (100%)
389 (90%)
346 (80%)

CG

LPG

G + P+ (Fast Downward)
G + P (Fast Downward)
G (Fast Downward)

Solved Tasks

302 (70%)
259 (60%)
216 (50%)
173 (40%)
130 (30%)

(Fast Downward)
FDD (Fast Downward)
FD (Fast Downward)
SGPlan
LPG-TD
YAHSP
Macro-FF

86 (20%)
43 (10%)

+ P (Fast Downward)
(Fast Downward)
F (Fast Downward)

0s

50s

100s

150s
Search Time

200s

250s

300s

Figure 31: Number tasks solved vs. runtime IPC4 domains. graph shows results
hypothetical planner always chooses best configuration Fast
Downward, competition configurations Fast Downward best four
participants.

241

fiH ELMERT

7.7 Conclusions Experiment
interpret experimental results? first conclusion Fast Downward
clearly competitive state art. especially true configuration using
multi-heuristic best-first search preferred operators (M+P), outperforms competing
planning systems set STRIPS domains IPC13 domains IPC4.
problems CHEDULE domain, would true remaining
group benchmarks, ADL domains IPC13.
regard second objective investigation, evaluating relative strengths
different planner configurations, M+P configuration emerges clear-cut winner. 23
29 domains, configuration solves tasks, unlike configurations,
one domain (P ROMELA -O PTICALT ELEGRAPH) performs badly. conclude
multi-heuristic best-first search use preferred operators promising extensions
heuristic planners.
particularly true preferred operators. Indeed, M+P configuration, two
variants greedy best-first search preferred operators show next best overall performance,
terms number domains among top performers terms
total number tasks solved. Comparing G G+P, ten domains variant
using preferred operators solves tasks one using them; opposite true five
domains. Comparing M+P, difference even striking, preferred operator
variant outperforming fifteen domains, worse two (in
solves one task less). convincing arguments use preferred operators.

8. Summary Discussion
turn discussion, let us briefly summarize contributions article. motivating starting point, explained planning tasks often exhibit simpler structure expressed
multi-valued state variables, rather traditional propositional representations.
introduced Fast Downward, planning system based idea converting tasks multivalued formalism exploiting causal information underlying encodings.
Fast Downward processes PDDL planning tasks three stages. skipped first
stages, translation, automatically transforms PDDL task equivalent multi-valued
planning task nicer causal structure. explained inner workings second stage,
knowledge compilation, demonstrating depth kind knowledge planner extracts
problem representation, discussing causal graphs, domain transition graphs, successor generators axiom evaluators. discussion Fast Downwards search component,
introduced heuristic search algorithms, use technique deferred heuristic evaluation
reduce number states heuristic goal distance estimate must computed.
addition greedy best-first search, Fast Downward employs multi-heuristic best-first search
algorithm usefully integrate information two heuristic estimators, namely causal graph
heuristic heuristic. heuristic search algorithms utilize preference information
operators. introduced Fast Downwards experimental focused iterative-broadening
search algorithm, based idea pruning set operators consider
successor states likely lead towards specific goal.
thus tried give complete account Fast Downward planning systems approach
solving multi-valued planning tasks, including motivation, architecture, algorithmic founda242

fiT FAST OWNWARD P LANNING YSTEM

tions. previous section, demonstrated empirical behaviour, showing good performance
across whole range propositional benchmarks previous planning competitions.
Among novel algorithms search enhancements discussed article, two
aspects Fast Downward consider central importance would
emphasize. One use multi-valued state variables PDDL-style planning.
believe multi-valued representations much structured hence much amenable
automated reasoning purposes heuristic evaluation, problem decomposition,
aspects planning goal ordering extraction landmarks. central
idea use hierarchical decompositions within heuristic planning framework. Hierarchical
approaches domain-independent planning considerable potential, since work
Knoblock (1994) Bacchus Yang (1994), little work published. Fast Downward, hope renew interest area, believe promising ground
advances automated planning.
future, several aspects Fast Downward would investigate
further. First, intend experiment search techniques along lines focused
iterative-broadening search, emphasize heuristically evaluating operator usefulness rather
heuristically evaluating states.
Second, would come efficient heuristic multi-valued planning tasks
require pruning cycles causal graph. Initial experiments direction
shown difficult achieve goal without losing performance Fast Downwards
heuristic estimator, perhaps better heuristic accuracy outweigh worse per-state performance
many cases.
Third, want investigate far performance planner could improved
encoding domains differently. cases, merging set state variables
closely interrelated single state variable whose domain product domains
original state variables might beneficial. Also, want test hand-tailored encodings lead
better performance automatically derived ones, so, large performance gap is.
Fourth finally, would evaluate behaviour causal graph heuristic
specific planning domains empirically theoretically, following Hoffmanns work
heuristic (Hoffmann, 2001, 2002, 2005). Hopefully, give indication
expect good performance causal graph heuristic advisable look
approaches.

Acknowledgements
author wishes thank Silvia Richter, member Fast Downward team
4th International Planning Competition, part implementing planner valuable
advice before, throughout, competition. deserves thanks helping
experiments, proof-reading article, suggesting number improvements.
anonymous reviewers article handling editor, Maria Fox, made number
useful suggestions led significant improvements.
work partly supported German Research Council (DFG) within Graduate
Programme Mathematical Logic Applications part Transregional Collaborative
Research Centre Automatic Verification Analysis Complex Systems (SFB/TR 14 AVACS).
See www.avacs.org information.
243

fiH ELMERT

References
Bacchus, F., & Yang, Q. (1994). Downward refinement efficiency hierarchical problem
solving. Artificial Intelligence, 71(1), 43100.
Backstrom, C., & Nebel, B. (1995). Complexity results SAS + planning. Computational Intelligence, 11(4), 625655.
Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129(1), 533.
Brafman, R. I., & Domshlak, C. (2003). Structure complexity planning unary operators.
Journal Artificial Intelligence Research, 18, 315349.
Bylander, T. (1994). computational complexity propositional STRIPS planning. Artificial
Intelligence, 69(12), 165204.
Domshlak, C., & Brafman, R. I. (2002). Structure complexity planning unary operators.
Ghallab, M., Hertzberg, J., & Traverso, P. (Eds.), Proceedings Sixth International
Conference Artificial Intelligence Planning Scheduling (AIPS 2002), pp. 3443. AAAI
Press.
Domshlak, C., & Dinitz, Y. (2001). Multi-agent off-line coordination: Structure complexity.
Cesta, A., & Borrajo, D. (Eds.), Pre-proceedings Sixth European Conference
Planning (ECP01), pp. 277288, Toledo, Spain.
Dowling, W. F., & Gallier, J. H. (1984). Linear-time algorithms testing satisfiability
propositional Horn formulae. Journal Logic Programming, 1(3), 367383.
Edelkamp, S., & Helmert, M. (1999). Exhibiting knowledge planning problems minimize
state encoding length. Fox, M., & Biundo, S. (Eds.), Recent Advances AI Planning.
5th European Conference Planning (ECP99), Vol. 1809 Lecture Notes Artificial
Intelligence, pp. 135147, New York. Springer-Verlag.
Edelkamp, S., & Hoffmann, J. (2004). PDDL2.2: language classical part 4th
International Planning Competition. Tech. rep. 195, Albert-Ludwigs-Universitat Freiburg,
Institut fur Informatik.
Fox, M., & Long, D. (2003). PDDL2.1: extension PDDL expressing temporal planning
domains. Journal Artificial Intelligence Research, 20, 61124.
Garey, M. R., & Johnson, D. S. (1979). Computers Intractability Guide Theory
NP-Completeness. Freeman.
Gerevini, A., Saetti, A., & Serina, I. (2003). Planning stochastic local search temporal
action graphs LPG. Journal Artificial Intelligence Research, 20, 239290.
Ginsberg, M. L., & Harvey, W. D. (1992). Iterative broadening. Artificial Intelligence, 55, 367383.
Helmert, M. (2004). planning heuristic based causal graph analysis. Zilberstein, S., Koehler,
J., & Koenig, S. (Eds.), Proceedings Fourteenth International Conference Automated
Planning Scheduling (ICAPS 2004), pp. 161170. AAAI Press.
Hoffmann, J. (2001). Local search topology planning benchmarks: empirical analysis.
Nebel, B. (Ed.), Proceedings 17th International Joint Conference Artificial Intelligence (IJCAI01), pp. 453458. Morgan Kaufmann.
244

fiT FAST OWNWARD P LANNING YSTEM

Hoffmann, J. (2002). Local search topology planning benchmarks: theoretical analysis.
Ghallab, M., Hertzberg, J., & Traverso, P. (Eds.), Proceedings Sixth International Conference Artificial Intelligence Planning Scheduling (AIPS 2002), pp. 92100. AAAI
Press.
Hoffmann, J. (2005). ignoring delete lists works: Local search topology planning benchmarks. Journal Artificial Intelligence Research, 24, 685758.
Hoffmann, J., & Edelkamp, S. (2005). deterministic part IPC-4: overview. Journal
Artificial Intelligence Research, 24, 519579.
Hoffmann, J., & Nebel, B. (2001). planning system: Fast plan generation heuristic
search. Journal Artificial Intelligence Research, 14, 253302.
Jonsson, P., & Backstrom, C. (1995). Incremental planning. Ghallab, M., & Milani, A. (Eds.),
New Directions AI Planning: EWSP 95 3rd European Workshop Planning, Vol. 31
Frontiers Artificial Intelligence Applications, pp. 7990, Amsterdam. IOS Press.
Jonsson, P., & Backstrom, C. (1998a). State-variable planning structural restrictions: Algorithms complexity. Artificial Intelligence, 100(12), 125176.
Jonsson, P., & Backstrom, C. (1998b). Tractable plan existence imply tractable plan generation. Annals Mathematics Artificial Intelligence, 22(3), 281296.
Joslin, D., & Roach, J. (1989). theoretical analysis conjunctive-goal problems. Artificial
Intelligence, 41(1), 97106. Research Note.
Knoblock, C. A. (1994). Automatically generating abstractions planning. Artificial Intelligence,
68(2), 243302.
Korf, R. E. (1987). Planning search: quantitative approach. Artificial Intelligence, 33(1),
6588.
Lowerre, B. T. (1976). HARPY Speech Recognition System. Ph.D. thesis, Computer Science
Department, Carnegie-Mellon University, Pittsburgh, Pennsylvania.
Newell, A., & Simon, H. A. (1963). GPS: program simulates human thought. Feigenbaum,
E. A., & Feldman, J. (Eds.), Computers Thought, pp. 279293. Oldenbourg.
Russell, S., & Norvig, P. (2003). Artificial Intelligence Modern Approach. Prentice Hall.
Sacerdoti, E. D. (1974). Planning hierarchy abstraction spaces. Artificial Intelligence, 5,
115135.
Tenenberg, J. D. (1991). Abstraction planning. Allen, J. F., Kautz, H. A., Pelavin, R. N.,
& Tenenberg, J. D., Reasoning Plans, chap. 4, pp. 213283. Morgan Kaufmann, San
Mateo.
van den Briel, M., Vossen, T., & Kambhampati, S. (2005). Reviving integer programming approaches AI planning: branch-and-cut framework. Biundo, S., Myers, K., & Rajan,
K. (Eds.), Proceedings Fifteenth International Conference Automated Planning
Scheduling (ICAPS 2005), pp. 310319. AAAI Press.
Williams, B. C., & Nayak, P. P. (1997). reactive planner model-based executive. Pollack,
M. E. (Ed.), Proceedings 15th International Joint Conference Artificial Intelligence
(IJCAI97), pp. 11781195. Morgan Kaufmann.
245

fiH ELMERT

Yoshizumi, T., Miura, T., & Ishida, T. (2000). partial expansion large branching factor problems. Kautz, H., & Porter, B. (Eds.), Proceedings Seventeenth National
Conference Artificial Intelligence (AAAI-2000), pp. 923929. AAAI Press.

246


