Journal Artificial Intelligence Research 26 (2006) 134

Submitted 11/05; published 05/06

Logic Reasoning Evidence
Joseph Y. Halpern

halpern@cs.cornell.edu

Cornell University, Ithaca, NY 14853 USA

Riccardo Pucella

riccardo@ccs.neu.edu

Northeastern University, Boston, 02115 USA

Abstract
introduce logic reasoning evidence essentially views evidence
function prior beliefs (before making observation) posterior beliefs (after
making observation). provide sound complete axiomatization logic,
consider complexity decision problem. Although reasoning logic
mainly propositional, allow variables representing numbers quantification
them. expressive power seems necessary capture important properties evidence.

1. Introduction
Consider following situation, essentially taken Halpern Tuttle (1993)
Fagin Halpern (1994). coin tossed, either fair double-headed. coin
lands heads. likely coin double-headed? coin tossed
20 times lands heads time? Intuitively, much likely coin
double-headed latter case former. likelihood
measured? cannot simply compute probability coin double-headed;
assigning probability event requires prior probability coin
double-headed. example, coin chosen random barrel
one billion fair coins one double-headed coin, still overwhelmingly likely
coin fair, sequence 20 heads unlucky. However, problem
statement, prior probability given. show given prior probability
coin double-headed increases significantly result seeing 20 heads. But,
intuitively, seems able say seeing 20 heads row provides
great deal evidence favor coin double-headed without invoking prior.
great deal work trying make intuition precise,
review.
main feature coin example involves combination probabilistic outcomes (e.g., coin tosses) nonprobabilistic outcomes (e.g., choice
coin). great deal work reasoning systems combine
probabilistic nondeterministic choices; see, example, Vardi (1985), Fischer Zuck
(1988), Halpern, Moses, Tuttle (1988), Halpern Tuttle (1993), de Alfaro (1998),
He, Seidel, McIver (1997). However, observations suggest attempt
formally analyze situation one frameworks, essentially permit
modeling probabilities, able directly capture intuition
increasing likelihood. see plays out, consider formal analysis situation
Halpern-Tuttle (1993) framework. Suppose Alice nonprobabilistically chooses

c
2006
AI Access Foundation. rights reserved.

fiHalpern & Pucella

one two coins: fair coin probability 1/2 landing heads, double-headed coin
probability 1 landing heads. Alice tosses coin repeatedly. Let k formula
stating: kth coin toss lands heads. probability k according Bob,
know coin Alice chose, even probability Alices choice?
According Halpern-Tuttle framework, modeled considering
set runs describing states system point time, partitioning
set two subsets, one coin used. set runs fair coin
used, probability k 1/2; set runs double-headed coin
used, probability k 1. setting, conclusion drawn
(PrB (k ) = 1/2) (PrB (k ) = 1). (This course probability Bobs point
view; Alice presumably knows coin using.) Intuitively, seems reasonable:
fair coin chosen, probability kth coin toss lands heads, according
Bob, 1/2; double-headed coin chosen, probability 1. Since Bob
know coins used, said.
suppose that, 101st coin toss, Bob learns result first 100
tosses. Suppose, moreover, landed heads. probability
101st coin toss lands heads? analysis, still either 1/2 1, depending
coin used.
hardly useful. make matters worse, matter many coin tosses Bob
witnesses, probability next toss lands heads remains unchanged.
answer misses important information. fact first 100 coin
tosses heads strong evidence coin fact double-headed. Indeed,
straightforward computation using Bayes Rule shows prior probability
coin double-headed , observing 100 tosses land heads,
probability coin double-headed becomes

+ 2100 (1 )

=

2100
.
2100 + (1 )

However, note possible determine posterior probability coin
double-headed (or 101st coin toss heads) without prior probability .
all, Alice chooses double-headed coin probability 10100 , still
overwhelmingly likely coin used fact fair, Bob unlucky
see unrepresentative sequence coin tosses.
None frameworks described reasoning nondeterminism probability takes issue evidence account. hand, evidence
discussed extensively philosophical literature. Much discussion occurs
philosophy science, specifically confirmation theory, concern historically assess support evidence obtained experimentation lends various
scientific theories (Carnap, 1962; Popper, 1959; Good, 1950; Milne, 1996). (Kyburg (1983)
provides good overview literature.)
paper, introduce logic reasoning evidence. logic extends
logic defined Fagin, Halpern Megiddo (1990) (FHM on) reasoning
likelihood expressed either probability belief. logic first-order quantification
reals (so includes theory real closed fields), FHM logic,
reasons shortly become clear. add observations states, provide
2

fiA Logic Reasoning Evidence

additional operator talk evidence provided particular observations.
refine language talk prior probability hypotheses posterior
probability hypotheses, taking account observation states. lets us
write formulas talk relationship prior probabilities, posterior
probabilities, evidence provided observations.
provide sound complete axiomatization logic. obtain
axiomatization, seem need first-order quantification fundamental way. Roughly
speaking, ensuring evidence operator appropriate properties
requires us assert existence suitable probability measures. seem possible without existential quantification. Finally, consider complexity
satisfiability problem. complexity problem full language requires exponential
space, since incorporates theory real closed fields, exponential-space
lower bound known (Ben-Or, Kozen, & Reif, 1986). However, show satisfiability problem propositional fragment language, still strong enough
allow us express many properties interest, decidable polynomial space.
reasonable ask point bother logic evidence.
claim many decisions practical applications made basis evidence.
take example security, consider enforcement mechanism used detect
react intrusions computer system. enforcement mechanism analyzes
behavior users attempts recognize intruders. Clearly mechanism wants
make sensible decisions based observations user behaviors. this?
One way think enforcement mechanism accumulating evidence
hypothesis user intruder. accumulated evidence used
basis decision quarantine user. context, clear
reasonable way assign prior probability whether user intruder. want
specify behavior systems prove meet specifications,
helpful logic allows us this. believe logic propose
first so.
rest paper organized follows. next section, formalize notion
evidence captures intuitions outlined above. Section 3, introduce logic
reasoning evidence. Section 4, present axiomatization logic
show sound complete respect intended models. Section 5,
discuss complexity decision problem logic. Section 6, examine
alternatives definition weight evidence use. ease exposition,
paper, consider system two time points:
observation. Section 7, extend work dynamic systems,
multiple pieces evidence, obtained different points time. proofs technical
results found appendix.

2. Measures Confirmation Evidence
order develop logic reasoning evidence, need first formalize
appropriate notion evidence. section, review various formalizations
literature, discuss formalization use. Evidence studied depth
philosophical literature, name confirmation theory. Confirmation theory aims

3

fiHalpern & Pucella

determining measuring support piece evidence provides hypothesis.
mentioned introduction, many different measures confirmation proposed
literature. Typically, proposal judged degree satisfies
various properties considered appropriate confirmation. example, may
required piece evidence e confirms hypothesis h e makes h
probable. desire enter debate class measures confirmation
appropriate. purposes, confirmation functions inappropriate:
assume given prior set hypotheses observations.
marginalization, prior hypotheses, exactly information
want assume. One exception measures evidence use
log-likelihood ratio. case, rather prior hypotheses observations,
suffices probability h observations hypothesis h: intuitively,
h (ob) probability observing ob h holds. Given observation ob, degree
confirmation provides hypothesis h


h (ob)
,
l(ob, h) = log
h (ob)
h represents hypothesis h (recall approach applies
two hypotheses). Thus, degree confirmation ratio
two probabilities. use logarithm critical here. Using ensures
likelihood positive observation confirms hypothesis. approach
advocated Good (1950, 1960), among others.1
One problem log-likelihood ratio measure l defined
used reason evidence discriminating two competing hypotheses,
namely hypothesis h holding hypothesis h holding. would
measure confirmation along lines log-likelihood ratio measure,
handle multiple competing hypotheses. number generalizations,
example, Pearl (1988) Chan Darwiche (2005). focus generalization given Shafer (1982) context Dempster-Shafer theory evidence
based belief functions (Shafer, 1976); studied Walley (1987).
description taken mostly Halpern Fagin (1992). measure
confirmation number nice properties take advantage, much work
presented paper adapted different measures confirmation.
start finite set H mutually exclusive exhaustive hypotheses; thus,
exactly one hypothesis holds given time. Let set possible observations
(or pieces evidence). simplicity, assume finite. case loglikelihood, assume that, hypotheses h H, probability measure
h h (ob) probability ob hypothesis h holds. Furthermore,
assume observations relevant hypotheses: every observation
ob O, must hypothesis h h (ob) > 0. (The measures h often
called likelihood functions literature.) define evidence space (over H O)
1. Another related approach, Bayes factor approach, based taking ratio odds rather
likelihoods (Good, 1950; Jeffrey, 1992). remark literature, confirmation usually taken
respect background knowledge. ease exposition, ignore background knowledge
here, although easily incorporated framework present.

4

fiA Logic Reasoning Evidence

tuple E = (H, O, ), function assigns every hypothesis h H
likelihood function (h) = h . (For simplicity, usually write h (h),
function clear context.)
Given evidence space E, define weight observation ob lends hypothesis h, written (ob, h),
h (ob)
.
h0 H h0 (ob)

(ob, h) = P

(1)

measure always lies 0 1; intuitively, (ob, h) = 1, ob fully
confirms h (i.e., h certainly true ob observed), (ob, h) = 0, ob
disconfirms h (i.e.,
P h certainly false
P ob observed). Moreover, fixed observation
ob

(ob)
>
0,
h
hH
hH (ob, h) = 1, thus weight evidence
looks probability measure ob. useful technical
consequences, one interpret probability measure. Roughly speaking,
weight (ob, h) likelihood h right hypothesis light observation
ob.2 advantages known measures confirmation (a)
applicable given prior probability distribution hypotheses, (b)
applicable two competing hypotheses, (c) fairly
intuitive probabilistic interpretation.
important problem statistical inference (Casella & Berger, 2001) choosing
best parameter (i.e., hypothesis) explains observed data. prior
parameters, best parameter typically taken one maximizes
likelihood data given parameter. Since normalized likelihood
function, parameter maximizes likelihood maximize . Thus,
interested maximizing likelihood, need normalize evidence
do. return issue normalization Section 6.3
Note H = {h1 , h2 }, sense generalizes log-likelihood ratio
measure. precisely, fixed observation ob, (ob, ) induces relative order
hypotheses l(ob, ), fixed hypothesis h, (, h) induces relative
order observations l(, h).
Proposition 2.1: ob, (ob, hi ) (ob, h3i ) l(ob, hi )
l(ob, h3i ), = 1, 2, h, ob, ob 0 , (ob, h) (ob 0 , h)
l(ob, h) l(ob 0 , h).
2. could taken log ratio make parallel log-likelihood ratio l defined earlier,
technical advantages weight evidence number 0 1.
3. Another representation evidence similar characteristics Shafers original representation evidence via belief functions (Shafer, 1976), defined
wES (ob, h) =

h (ob)
.
maxhH h (ob)

measure known statistical hypothesis testing generalized likelihood-ratio statistic.
another generalization log-likelihood ratio measure l. main difference wES
behave one considers combination evidence, discuss later section.
Walley (1987) Halpern Fagin (1992) point out, gives intuitive results case.
remark parameter (hypothesis) maximized likelihood maximizes wES , wES
used statistical inference.

5

fiHalpern & Pucella

Although (ob, ) behaves probability measure hypotheses every observation ob, one think probability; weight evidence combined
hypothesis, instance, generally sum weights individual hypotheses (Halpern & Pucella, 2005a). Rather, (ob, ) encoding evidence.
evidence? Halpern Fagin (1992) suggested evidence thought
function mapping prior probability hypotheses posterior probability, based
observation made. precise sense viewed function
maps prior probability 0 hypotheses H posterior probability ob based
observing ob, applying Dempsters Rule Combination (Shafer, 1976). is,
ob = 0 (ob, ),

(2)

combines two probability distributions H get new probability distribution
H defined follows:
P
1 (h)2 (h)
(1 2 )(H) = PhH
.
hH 1 (h)2 (h)
(Dempsters Rule Combination used combine belief functions. definition
complicated considering arbitrary belief functions, special case
belief functions fact probability measures, takes form give here.)
Bayes Rule standard way updating prior probability based observation,
applicable joint probability distribution hypotheses
observations (or, equivalently, prior hypotheses together likelihood
functions h h H), something want assume given.
particular, willing assume given likelihood functions,
willing assume given prior hypotheses. Dempsters Rule
Combination essentially simulates effects Bayes Rule. relationship
Dempsters Rule Bayes Rule made precise following well-known theorem.
Proposition 2.2: (Halpern & Fagin, 1992) Let E = (H, O, ) evidence space. Suppose
P probability H P (H {ob} | {h} O) = h (ob) h H
ob O. Let 0 probability H induced marginalizing P ; is, 0 (h) =
P ({h} O). ob O, let ob = 0 (ob, ). ob (h) = P ({h} | H {ob}).
words, joint probability hypotheses observations, Dempsters Rule Combination gives us result straightforward
application Bayes Rule.
Example 2.3: get feel measure evidence used, consider
variation two-coins example introduction. Assume coin chosen
Alice either double-headed fair, consider sequences hundred tosses coin.
Let = {m : 0 100} (the number heads observed), let H = {F, D}, F
coin fair, coin double-headed. probability spaces associated
hypotheses generated following probabilities simple observations m:



1 100
1 = 100
F (m) = 100
(m) =
0 otherwise.
2

6

fiA Logic Reasoning Evidence

(We extend additivity whole set O.) Take E = (H, O, ), (F ) = F
(D) = . observation 6= 100, weight favor F given

1 100
(m, F ) =

0

2100

1 100
+ 2100


= 1,

means support unconditionally provided F ; indeed,
sequence tosses cannot appear double-headed coin. Thus, 6= 100, get

0
= 0.
(m, D) =
1 100
0 + 2100
happens hundred coin tosses heads? straightforward check

1
1
1
2100
100
w
(100,
D)
=
;
(100, F ) = 2 1 =
=
E
1
1 + 2100
1 + 2100
1 + 2100
1 + 2100

time overwhelmingly evidence favor F .
Note assumed prior probability. Thus, cannot talk
probability coin fair double-headed. quantitative assessment
evidence favor one hypotheses. However, assume prior probability
coin fair heads observed 100 tosses, probability
coin fair 1 6= 100; = 100 then, applying rule combination,
posterior probability coin fair /( + (1 )2100 ).

u
characterize weight functions using small number properties? precisely,
given sets H O, function f H [0, 1], properties f
ensure likelihood functions f = E = (H, O, )?
saw earlier, fixed observation ob, f essentially acts probability measure
H. However, sufficient guarantee f weight function. Consider
following example, = {ob 1 , ob 2 } H = {h1 , h2 , h3 }:
f (ob 1 , h1 ) = 1/4
f (ob 1 , h2 ) = 1/4
f (ob 1 , h3 ) = 1/2

f (ob 2 , h1 ) = 1/4
f (ob 2 , h2 ) = 1/2
f (ob 2 , h3 ) = 1/4.

straightforward check f (ob 1 , ) f (ob 2 , ) probability measures H,
evidence space E = (H, O, ) f = . Indeed, assume
h1 , h2 , h3 . definition weight evidence, fact f
weight evidence, get following system equations:
h1 (ob 1 )
h1 (ob 1 )+h2 (ob 1 )+h3 (ob 1 )
h2 (ob 1 )
h1 (ob 1 )+h2 (ob 1 )+h3 (ob 1 )
h3 (ob 1 )
h1 (ob 1 )+h2 (ob 1 )+h3 (ob 1 )

h1 (ob 2 )
h1 (ob 2 )+h2 (ob 2 )+h3 (ob 2 )
h2 (ob 2 )
h1 (ob 2 )+h2 (ob 2 )+h3 (ob 2 )
h3 (ob 2 )
h1 (ob 2 )+h2 (ob 2 )+h3 (ob 2 )

= 1/4
= 1/4
= 1/2

= 1/4
= 1/2
= 1/4.

immediate exist 1 2 hi (ob j ) = j f (ob j , hi ),
= 1, 2, 3. Indeed, j = h1 (ob j ) + h2 (ob j ) + h3 (ob j ), j = 1, 2. Moreover, since hi
probability measure, must
hi (ob 1 ) + hi (ob 2 ) = 1 f (ob 1 , hi ) + 2 f (ob 2 , hi ) = 1,
7

fiHalpern & Pucella

= 1, 2, 3. Thus,
1 /4 + 2 /4 = 1 /4 + 2 /2 = 1 /2 + 4 /4 = 1.
constraints easily seen unsatisfiable.
argument generalizes arbitrary functions f ; thus, necessary condition f
weight function exists observation ob h (ob ) =
f (ob , h) hypothesis h probability measure, is, 1 f (ob 1 , h) + +
k f (ob k , h) = 1. fact, combined constraint f (ob, ) probability
measure fixed ob, condition turns sufficient, following theorem
establishes.
Theorem 2.4: Let H = {h1 , . . . , hm } = {ob 1 , . . . , ob n }, let f real-valued
function domain H f (ob, h) [0, 1]. exists evidence space
E = (H, O, ) f = f satisfies following properties:
WF1. every ob O, f (ob, ) probability measure H.
P
WF2. exists x1 , . . . , xn > 0 that, h H, ni=1 f (ob , h)xi = 1.
characterization fundamental completeness axiomatization
logic introduce next section. characterization complicated fact
weight evidence essentially normalized likelihood: likelihood
observation given particular hypothesis normalized using sum likelihoods
observation, possible hypotheses. One consequence this, already
mentioned above, weight evidence always 0 1, superficially
behaves probability measure. Section 6, examine issue normalization
carefully, describe changes framework would occur
take unnormalized likelihoods weight evidence.
Let E = (H, O, ) evidence space. Let set sequences observations
hob 1 , . . . , ob k O.4 Assume observations independent, is, basic
hypothesis h, take h (hob 1 , . . . , ob k i), probability observing particular sequence
observations given h, h (ob 1 ) h (ob k ), product probability making
observation sequence. Let E = (H, , ). assumption, well
known Dempsters Rule Combination used combine evidence setting;
is,
(hob 1 , . . . , ob k i, ) = (ob 1 , ) (ob k , )
(Halpern & Fagin, 1992, Theorem 4.3). easy exercise check weight
provided sequence observations hob 1 , . . . , ob k expressed terms
weight individual observations:
(ob 1 , h) (ob k , h)
.
1 0
k 0
h0 H (ob , h ) (ob , h )

(hob 1 , . . . , ob k i, h) = P

(3)

4. use superscript rather subscripts index observations sequence observations
confused basic observations ob 1 , . . . , ob n O.

8

fiA Logic Reasoning Evidence

let 0 prior probability hypotheses, hob 1 ,...,ob k probability
hypotheses observing ob 1 , . . . , ob k , verify
hob1 ,...,ob k = 0 (hob 1 , . . . , ob k i, ).
Example 2.5: Consider variant Example 2.3, take coin tosses individual observations, rather number heads turn one hundred coin
tosses. before, assume coin chosen Alice either double-headed fair. Let
= {H, }, result individual coin toss, H coin landed heads
coin landed tails. Let H = {F, D}, F coin fair,
coin double-headed. Let E = (H, , ). probability measure h associated
hypothesis h generated following probabilities simple observations:
F (H) =

1
2

(H) = 1.

Thus, example, F (hH, H, T, Hi) = 1/16, (hH, H, Hi) = 1, H (hH, H, T, Hi) =
0.
easily verify results similar obtained Example 2.3.
instance, weight observing favor F given
(T, F ) =

1
2

0+

1
2

= 1,

indicates observing provides unconditional support F ; doubleheaded coin cannot land tails.
sequences observations? weight provided sequence hob 1 , . . . , ob k
hypothesis h given Equation (3). Thus, H = hH, . . . , Hi, sequence hundred
coin tosses, check
(H, F ) =

1

1
2100
1
+ 2100

=

1
1 + 2100

(H, D) =

Unsurprisingly, result Example 2.3.

2100
1
.
=
1
1 + 2100
1 + 2100

u

3. Reasoning Evidence
introduce logic Lfo -ev reasoning evidence, inspired logic introduced
FHM reasoning probability. logic lets us reason weight evidence
observations hypotheses; moreover, able talk relationship
prior probabilities, evidence, posterior probabilities, provide operators reason
prior posterior probabilities hypotheses. remark
somewhat agnostic whether priors exist given (or
known) whether prior exist all. beyond scope paper
enter debate whether always appropriate assume existence prior.
Although definition evidence makes sense even priors exist, logic
implicitly assumes priors (although may known), since provide
9

fiHalpern & Pucella

operators reasoning prior. make use operators
examples below. However, fragment logic use operators
appropriate prior-free reasoning.
logic propositional features first-order features. take probability propositions weight evidence observations hypotheses, view
probability evidence propositions, allow first-order quantification numerical quantities, probabilities evidence. logic essentially considers two
time periods, thought time observation made
time observation made. section, assume exactly one observation
made. (We consider sequences observations Section 7.) Thus, talk
probability formula observation made, denoted Pr0 (), probability
observation, denoted Pr(), evidence provided observation ob
hypothesis h, denoted w(ob, h). course, want able use logic
relate quantities.
Formally, start two finite sets primitive propositions, h = {h1 , . . . , hnh }
representing hypotheses, = {ob 1 , . . . , ob } representing observations. Let
Lh (h ) propositional sublanguage hypothesis formulas obtained taking primitive propositions h closing negation conjunction; use range
formulas sublanguage.
basic term form Pr0 (), Pr(), w(ob, h), hypothesis formula,
ob observation, h hypothesis. said, interpret Pr0 ()
prior probability , Pr() posterior probability , w(ob, h) weight
evidence observation ob hypothesis h. may seem strange allow language
talk prior probability hypotheses, although said
want assume prior known. could, course, simplify syntax
include formulas form Pr0 () Pr(). advantage
that, even prior known, given view evidence function priors
posteriors, make statements prior probability h 2/3, ob
observed, weight evidence ob h 3/4, posterior probability h
6/7;
Pr0 (h) = 1/2 ob w(ob, h) = 3/4 Pr(h) = 6/7.
polynomial term form t1 + + tn , term ti product integers,
basic terms, variables (which range reals). polynomial inequality formula
form p c, p polynomial term c integer. Let Lfo -ev (h , )
language obtained starting primitive propositions h
polynomial inequality formulas, closing conjunction, negation, firstorder quantification. Let true abbreviation arbitrary propositional tautology
involving hypotheses, h1 h1 ; let false abbreviation true.
definition, true false considered part sublanguage Lh (h ).
clear allow integer coefficients appear polynomial
terms, fact express polynomial terms rational coefficients crossmultiplying.
instance, 31 Pr() + 12 Pr(0 ) 1 represented polynomial inequality formula
2Pr() + 3Pr(0 ) 6. difficulty giving semantics polynomial terms
use arbitrary real coefficients, need restriction integers order make use
10

fiA Logic Reasoning Evidence

results theory real closed fields axiomatization Section 4
complexity results Section 5.
use obvious abbreviations needed, ( ),
, x x(), Pr() Pr() c Pr() + (1)Pr() c, Pr() Pr()
Pr() Pr() 0, Pr() c Pr() c, Pr() < c (Pr() c), Pr() = c
(Pr() c) (Pr() c) (and analogous abbreviations inequalities involving Pr0
w).
Example 3.1: Consider situation given Example 2.3. Let , observations,
consist primitive propositions form heads[m], integer 0
100, indicating heads 100 tosses appeared. Let h consist two
primitive propositions fair doubleheaded. computations Example 2.3
written follows:
w(heads[100], fair) = 1/(1 + 2100 ) w(heads[100], doubleheaded) = 2100 /(1 + 2100 ).
capture fact weight evidence observation maps prior
probability posterior probability Dempsters Rule Combination. example,
following formula captures update prior probability hypothesis fair
upon observation hundred coin tosses landing heads:
Pr0 (fair) = w(heads[100], fair) = 1/(1 + 2100 ) Pr(fair) = /( + (1 )2100 ).
develop deductive system derive conclusions next section.


u

consider semantics. formula interpreted world specifies
hypothesis true observation made, well evidence space interpret
weight evidence observations probability distribution hypotheses
interpret prior probabilities talk updating based evidence. (We need
include posterior probability distribution, since computed prior
weights evidence using Equation (2).) evidential world tuple w = (h, ob, , E),
h hypothesis, ob observation, probability distribution h , E
evidence space h .
interpret propositional formulas Lh (h ), associate hypothesis formula
set [[]] hypotheses, induction structure :
[[h]] = {h}
[[]] = h [[]]
[[1 2 ]] = [[1 ]] [[2 ]].
interpret first-order formulas may contain variables, need valuation v
assigns real number every variable. Given evidential world w = (h, ob, , E)
valuation v, assign polynomial term p real number [p]w,v straightforward way:
[x]w,v = v(x)
[a]w,v =
[Pr0 ()]w,v = ([[]])
11

fiHalpern & Pucella

[Pr()]w,v = ( (ob, ))([[]])
[w(ob 0 , h0 )]w,v = (ob 0 , h0 )
[t1 t2 ]w,v = [t1 ]w,v [t2 ]w,v
[p1 + p2 ]w,v = [p1 ]w,v + [p2 ]w,v .
Note that, interpret Pr(), posterior probability observed ob (the
observation world w), use Equation (2), says posterior obtained
combining prior probability (ob, ).
define means formula true (or satisfied) evidential world
w valuation v, written (w, v) |= , follows:
(w, v) |= h w = (h, ob, , E) ob, , E
(w, v) |= ob w = (h, ob, , E) h, , E
(w, v) |= (w, v) 6|=
(w, v) |= (w, v) |= (w, v) |=
(w, v) |= p c [p]w,v c
(w, v) |= x (w, v 0 ) |= v 0 agree v variables x.
(w, v) |= true v, write simply w |= . easy check
closed formula (that is, one free variables), (w, v) |=
(w, v 0 ) |= , v, v 0 . Therefore, given closed formula , (M, w, v) |= , fact
w |= . typically concerned closed formulas. Finally, w |=
evidential worlds w, write |= say valid. next section,
characterize axiomatically valid formulas logic.
Example 3.2: following formula valid, is, true evidential worlds:
|= (w(ob, h1 ) = 2/3 w(ob, h2 ) = 1/3) (Pr0 (h1 ) 1/100 ob) Pr(h1 ) 2/101.
words, evidential worlds weight evidence observation ob
hypothesis h1 2/3 weight evidence observation ob hypothesis h2 1/3,
must case prior probability h1 least 1/100 ob actually
observed, posterior probability h1 least 2/101. shows extent
reason evidence independently prior probabilities.

u
logic imposes restriction prior probabilities used models.
implies, instance, formula
fair Pr0 (fair) = 0
satisfiable: exists evidential world w formula true w.
words, consistent hypothesis true, despite prior probability
true 0. simple matter impose restriction models
h true world, (h) > 0 prior world.
12

fiA Logic Reasoning Evidence

conclude section remarks concerning semantic model. semantic model implicitly assumes prior probability known likelihood
functions (i.e., measures h ) known. course, many situations
uncertainty both. Indeed, motivation focusing evidence precisely deal
situations prior known. Handling uncertainty prior easy
framework, since notion evidence independent prior hypotheses.
straightforward extend model allowing set possible worlds, different
prior each, using evidence space them. extend
logic knowledge operator, statement known true true
worlds. allows us make statements know prior hypothesis
h . Since observation ob provides evidence 3/4 h, know
posterior h given ob (3)/(2 + 1) (3)/(2 + 1).
Dealing uncertainty likelihood functions somewhat subtle.
understand issue, suppose one two coins chosen tossed. bias
coin 1 (i.e., probability coin 1 lands heads) 2/3 3/4; bias coin
2 1/4 1/3. uncertainty probability coin 1
picked (this uncertainty prior) uncertainty bias
coin (this uncertainty likelihood functions). problem that,
deal this, must consider possible worlds possibly different evidence
space world. obvious define weight evidence. explore
issue detail companion paper (Halpern & Pucella, 2005a).

4. Axiomatizing Evidence
section present sound complete axiomatization AX(h , ) logic.
axiomatization divided four parts. first part, consisting
following axiom inference rule, accounts first-order reasoning:
Taut. substitution instances valid formulas first-order logic equality.
MP. infer .
Instances Taut include, example, formulas form ,
arbitrary formula logic. includes formulas (x) x free
. particular, (x(h)) h hypotheses h , similarly observations
. Note Taut includes substitution instances valid formulas first-order logic
equality; words, valid formula first-order logic equality
free variables replaced arbitrary terms language (including Pr0 (), Pr(),
w(ob, h)) instance Taut. Axiom Taut replaced sound complete
axiomatization first-order logic equality, given, instance, Shoenfield (1967)
Enderton (1972).
second set axioms accounts reasoning polynomial inequalities, relying
theory real closed fields:
RCF. instances formulas valid real closed fields (and, thus, true reals),
nonlogical symbols +, , <, 0, 1, 1, 2, 2, 3, 3, . . . .

13

fiHalpern & Pucella

Formulas valid real closed fields include, example, fact addition
reals associative, xyz((x+y)+z = x+(y +z)), 1 identity multiplication,
x(x1 = x), formulas relating constant symbols, k = 1+ +1 (k times)
1 + 1 = 0. Taut, could replace RCF sound complete axiomatization
real closed fields (cf. Fagin et al., 1990; Shoenfield, 1967; Tarski, 1951).
third set axioms essentially captures fact single hypothesis
single observation holds per state.
H1. h1 hnh .
H2. hi hj 6= j.
O1. ob 1 ob .
O2. ob ob j 6= j.
axioms illustrate subtlety logic. propositional logics,
parameterized primitive propositions, case, h . However, axiomatizations propositional logics typically depend exact set primitive
propositions, does. Clearly, axiom H1 sound hypothesis primitives
exactly h1 , . . . , hnh . Similarly, axiom O1 sound observation primitives
exactly ob 1 , . . . , ob . therefore important us identify primitive propositions
talking axiomatization AX(h , ).
last set axioms concerns reasoning probabilities evidence proper.
axioms probability taken FHM.
Pr1. Pr0 (true) = 1.
Pr2. Pr0 () 0.
Pr3. Pr0 (1 2 ) + Pr0 (1 2 ) = Pr0 (1 ).
Pr4. Pr0 (1 ) = Pr0 (2 ) 1 2 propositional tautology.
Axiom Pr1 simply says event true probability 1. Axiom Pr2 says probability nonnegative. Axiom Pr3 captures finite additivity. possible express
countable additivity logic. hand, FHM, need
axiom countable additivity. Roughly speaking, establish next section,
formula satisfiable all, satisfiable finite structure. Similar axioms capture
posterior probability formulas:
Po1. Pr(true) = 1.
Po2. Pr() 0.
Po3. Pr(1 2 ) + Pr(1 2 ) = Pr(1 ).
Po4. Pr(1 ) = Pr(2 ) 1 2 propositional tautology.

14

fiA Logic Reasoning Evidence

Finally, need axioms account behavior evidence operator w.
properties? one thing, weight function acts essentially probability
hypotheses, fixed observation, except restricted taking weight
evidence basic hypotheses only. gives following axioms:
E1. w(ob, h) 0.
E2. w(ob, h1 ) + + w(ob, hnh ) = 1.
Second, evidence connects prior posterior beliefs via Dempsters Rule Combination, (2). captured following axiom. (Note that, since
division language, crossmultiply clear denominator.)
E3. ob (Pr0 (h)w(ob, h) = Pr(h)Pr0 (h1 )w(ob, h1 ) + + Pr(h)Pr0 (hnh )w(ob, hnh )).
quite enough. saw Section 2, property WF2 Theorem 2.4
required function evidence function. following axiom captures WF2
logic:
E4. x1 . . . xno (x1 > 0 xno > 0 w(ob 1 , h1 )x1 + + w(ob , h1 )xno = 1
w(ob 1 , hnh )x1 + + w(ob , hnh )xno = 1).
Note axiom E4 axiom requires quantification. Moreover, axioms E3
E4 depend h .
example, show h h0 distinct hypotheses h , formula
(w(ob, h) = 2/3 w(ob, h0 ) = 2/3)
provable. First, RCF, following valid formula theory real closed fields
provable:
xy(x = 2/3 = 2/3 x + > 1).
Moreover, (x, y) first-order logic formula two free variables x y,
(xy((x, y))) (w(ob, h), w(ob, h0 ))
substitution instance valid formula first-order logic equality, hence
instance Taut. Thus, MP, prove
w(ob, h) = 2/3 w(ob, h0 ) = 2/3 w(ob, h) + w(ob, h0 ) > 1,
provably equivalent (by Taut MP) contrapositive
w(ob, h) + w(ob, h0 ) 1 (w(ob, h) = 2/3 w(ob, h0 ) = 2/3).
argument similar above, using RCF, Taut, MP, E1, E2, derive
w(ob, h) + w(ob, h0 ) 1,
MP, obtain desired conclusion: (w(ob, h) = 2/3 w(ob, h0 ) = 2/3).
15

fiHalpern & Pucella

Theorem 4.1: AX(h , ) sound complete axiomatization Lfo -ev (h , )
respect evidential worlds.
usual, soundness straightforward, prove completeness, suffices show
formula consistent AX(h , ), satisfiable evidential structure. However, usual approach proving completeness modal logic, involves
considering maximal consistent sets canonical structures work. problem
maximal consistent sets formulas satisfiable. example,
maximal consistent set formulas includes Pr() > 0 Pr() 1/n
n = 1, 2, . . . . clearly unsatisfiable. proof follows techniques developed
FHM.
express axiom E4, needed quantification logic.
fact representation evidence normalized nontrivial effect logic: E4
corresponds property WF2, essentially says function weight evidence
function one find normalization factor. interesting question whether
possible find sound complete axiomatization propositional fragment
logic (without quantification variables). this, need give quantifier-free
axioms replace axiom E4. amounts asking whether simpler property
WF2 Theorem 2.4 characterizes weight evidence functions. remains
open question.

5. Decision Procedures
section, consider decision problem logic, is, problem
deciding whether given formula satisfiable. order state problem precisely,
however, need deal carefully fact logic parameterized sets
h primitive propositions representing hypotheses observations.
logics, choice underlying primitive propositions essentially irrelevant. example,
propositional formula contains primitive propositions set
true respect truth assignments , remains true respect
truth assignments set 0 . monotonicity property hold here.
example, already observed, axiom H1 clearly depends set hypotheses
observations; longer valid set changed. true O1, E3,
E4.
means careful, stating decision problems, role
h algorithm. straightforward way deal assume
satisfiability algorithm gets input h , , formula Lfo -ev (h , ).
Lfo -ev (h , ) contains full theory real closed fields, unsurprisingly difficult
decide. decision procedure, use exponential-space algorithm Ben-Or,
Kozen, Reif (1986) decide satisfiability real closed field formulas. define
length || number symbols required write , count
length coefficient 1. Similarly, define kk length longest
coefficient appearing f , written binary.
Theorem 5.1: procedure runs space exponential || kk deciding,
given h , whether formula Lfo -ev (h , ) satisfiable evidential world.
16

fiA Logic Reasoning Evidence

essentially best do, since Ben-Or, Kozen, Reif (1986) prove
decision problem real closed fields complete exponential space, logic
contains full language real closed fields.
assumed algorithm takes input set primitive propositions
h , really affect complexity algorithm. precisely,
given formula Lfo -ev set hypotheses observations,
still decide whether satisfiable, is, whether sets h primitive
propositions containing primitive propositions evidential world w
satisfies .
Theorem 5.2: procedure runs space exponential || kk deciding
whether exists sets primitive propositions h Lfo -ev (h , )
satisfiable evidential world.
main culprit exponential-space complexity theory real closed fields,
add logic able even write axiom E4 axiomatization AX(h , ).5 However, interested axiomatizations, simply
verifying properties probabilities weights evidence, consider following
propositional (quantifier-free) fragment logic. before, start sets h
hypothesis observation primitives, form sublanguage Lh hypothesis
formulas. Basic terms form Pr0 (), Pr(), w(ob, h), hypothesis
formula, ob observation, h hypothesis. quantifier-free polynomial term
form a1 t1 + + tn , ai integer ti product
basic terms. quantifier-free polynomial inequality formula form p c,
p quantifier-free polynomial term, c integer. instance, quantifier-free
polynomial inequality formula takes form Pr0 () + 3w(ob, h) + 5Pr0 ()Pr(0 ) 7.
Let Lev (h , ) language obtained starting primitive propositions
h quantifier-free polynomial inequality formulas, closing conjunction negation. Since quantifier-free polynomial inequality formulas polynomial
inequality formulas, Lev (h , ) sublanguage Lfo -ev (h , ). logic Lev (h , )
sufficiently expressive express many properties interest; instance, certainly
express general connection priors, posteriors, evidence captured axiom
E3, well specific relationships prior probability posterior probability
weight evidence particular observation, Example 3.1. Reasoning
propositional fragment logic Lev (h , ) easier full language.6
5. Recall axiom E4 requires existential quantification. Thus, restrict sublanguage
consisting formulas single block existential quantifiers prefix position. satisfiability
problem sublanguage shown decidable time exponential size formula
(Renegar, 1992).
6. preliminary version paper (Halpern & Pucella, 2003), examined quantifier-free fragment
Lfo -ev (h , ) uses linear inequality formulas, form a1 t1 + + tn c,
ti basic term. claimed problem deciding, given h , whether formula
fragment satisfiable evidential world NP-complete. claimed result
followed small-model theorem: satisfiable, satisfiable evidential world
small number hypotheses observations. small-model theorem true, argument
satisfiability problem NP implicitly assumed numbers associated
probability measure evidence space evidential world small. true

17

fiHalpern & Pucella

Theorem 5.3: procedure runs space polynomial || kk deciding,
given h , whether formula Lev (h , ) satisfiable evidential world.
Theorem 5.3 relies Cannys (1988) procedure deciding validity quantifierfree formulas theory real closed fields. general case, complexity
unaffected whether decision problem takes input sets h
primitive propositions.
Theorem 5.4: procedure runs space polynomial || kk deciding
whether exists sets primitive propositions h Lev (h , )
satisfiable evidential world.

6. Normalized Versus Unnormalized Likelihoods
weight evidence used throughout paper generalization log-likelihood
ratio advocated Good (1950, 1960). pointed earlier, measure confirmation essentially normalized likelihood: likelihood observation given particular
hypothesis normalized sum likelihoods observation, possible hypotheses. would change take (unnormalized) likelihoods h
weight evidence? things would simplify. example, WF2
consequence normalization, corresponding axiom E4, axiom
requires quantification.
main argument normalizing likelihood normalizing probability measures. probability, using normalized likelihood, weight
evidence always 0 1, provides absolute scale judge
reports evidence. impact psychologicalit permits one use
rules thumb situations, since numbers obtained independent context use. Thus, instance, weight evidence 0.95 one situation corresponds
amount evidence weight evidence 0.95 different situation;
acceptable decision based weight evidence first situation ought
acceptable situation well. importance uniform scale
depends, course, intended applications.
sake completeness, describe changes framework required
use unnormalized likelihoods weight evidence. Define wEu (ob, h) = h (ob).
general. Even though formula involves linear inequality formulas, every evidential world
satisfies axiom E3. constraint enables us write formulas exist models
probabilities weights evidence rational. example, consider formula
Pr0 (h1 ) = w(ob 1 , h1 ) Pr0 (h2 ) = 1 Pr0 (h1 ) Pr(h1 ) = 1/2 w(ob 1 , h2 ) = 1/4
evidential world satisfying formula must satisfy
Pr0 (h1 ) = w(ob 1 , h1 ) = 1/8(1


17)

irrational. exact complexity fragment remains open. use techniques
show PSPACE, matching lower bound. (In particular, may indeed
NP.) re-examine fragment logic Section 6, different interpretation weights
evidence.

18

fiA Logic Reasoning Evidence

First, note update prior probability 0 via set likelihood functions h
using form Dempsters Rule Combination. precisely, define 0 wEu (ob, )
probability measure defined
0 (h)h (ob)
.
0
h0 H 0 (h )h0 (ob)

(0 wEu (ob, ))(h) = P

logic introduced Section 3 applies well new interpretation
weights evidence. syntax remains unchanged, models remain evidential worlds,
semantics formulas simply take new interpretation weight evidence
account. particular, assignment [p]w,v uses definition wEu ,
becomes
[Pr()]w,v = ( wEu (ob, ))([[]])
[w(ob 0 , h0 )]w,v = wEu (ob 0 , h0 ).
axiomatization new logic slightly different somewhat simpler
one Section 3. particular, E1 E2, say w(ob, h) acts probability
measure fixed ob, replaced axioms say w(ob, h) acts probability
measure fixed h:
E10 . w(ob, h) 0.
E20 . w(ob 1 , h) + + w(ob , h) = 1.
Axiom E3 unchanged, since wEu updated essentially way . Axiom E4
becomes unnecessary.
complexity decision procedure? Section 5, complexity
decision problem full logic Lfo -ev (h , ) remains dominated complexity reasoning real closed fields. course, now, express full axiomatization
unnormalized likelihood interpretation weight evidence Lev (h , ) fragment, decided polynomial space. advantage unnormalized
likelihood interpretation weight evidence, however, leads useful fragment
Lev (h , ) perhaps easier decide.
Suppose interested reasoning exclusively weights evidence,
prior posterior probability. kind reasoning actually underlies
many computer science applications involving randomized algorithms (Halpern & Pucella,
2005b). before, start sets h hypothesis observation primitives,
form sublanguage Lh hypothesis formulas. quantifier-free linear term
form a1 w(ob 1 , h1 ) + + w(ob n , hn ), ai integer, ob
observation, hi hypothesis. quantifier-free linear inequality formula
form p c, p quantifier-free linear term c integer. example,
w(ob 0 , h) + 3w(ob, h) 7 quantifier-free linear inequality formula.
Let Lw (h , ) language obtained starting primitive propositions
h quantifier-free linear inequality formulas, closing conjunction
negation. Since quantifier-free linear inequality formulas polynomial inequality
formulas, Lw (h , ) sublanguage Lfo -ev (h , ). Reasoning Lw (h , )
easier full language, possibly easier Lev (h , ) fragment.
19

fiHalpern & Pucella

Theorem 6.1: problem deciding, given h , whether formula Lw (h , )
satisfiable evidential world NP-complete.
general case, complexity unaffected whether decision
problem takes input sets h primitive propositions.
Theorem 6.2: problem deciding, formula , whether exists sets
primitive propositions h Lw (h , ) satisfiable
evidential world NP-complete.

7. Evidence Dynamic Systems
evidential worlds considered essentially static, model
situation single observation made. Considering static worlds lets
us focus relationship prior posterior probabilities hypotheses
weight evidence single observation. related paper (Halpern & Pucella,
2005b), consider evidence context randomized algorithms; use evidence
characterize information provided by, example, randomized algorithm primality
says number prime. framework work dynamic; sequences
observations made time. section, extend logic reason
evidence sequences observations, using approach combining evidence described
Section 2.
subtleties involved trying find appropriate logic reasoning
situations Example 2.5. important one relationship
observations time. way illustration, consider following example. Bob
expecting email Alice stating rendezvous take place. Calm
pressure, Bob reading waits. assume Bob concerned
time. purposes example, one three things occur given point
time:
(1) Bob check received email;
(2) Bob checks received email, notices received email
Alice;
(3) Bob checks received email, notices received email Alice.
view world affected events? (1), clear that,
things equal, Bobs view world change: observation made.
Contrast (2) (3). (2), Bob make observation, namely
yet received Alices email. fact checks indicates wants observe
result. (3), makes observation, namely received email Alice.
cases, check yields observation, use update view
world. case (2), essentially observed nothing happened, emphasize
observation, distinguished case Bob
even check whether email arrived, explicit set evidence
space.

20

fiA Logic Reasoning Evidence

discussion motivates models use section. characterize
agents state observations made, including possibly nothing
happened observation. Although explicitly model time, easy incorporate
time framework, since agent observe times clock ticks. models
section admittedly simple, already highlight issues involved reasoning
evidence dynamic systems. long agents forget observations,
loss generality associating agents state sequence observations. do,
however, make simplifying assumption evidence space used
observations sequence. words, assume evidence space fixed
evolution system. many situations interest, external world changes.
possible observations may depend state world, may likelihood functions.
intrinsic difficulties extending model handle state changes,
additional details would obscure presentation.
ways, considering dynamic setting simplifies things. Rather talking
prior posterior probability using different operators, need single
probability operator represents probability hypothesis current time.
express analogue axiom E3 logic, need able talk
probability next time step. done adding next-time operator
logic, holds current time holds next time step.7
extend logic talk weight evidence sequence observations.
-ev
define logic Lfo
dyn follows. Section 3, start set primitive
propositions h , respectively representing hypotheses observations.
Again, let Lh (h ) propositional sublanguage hypotheses formulas obtained
taking primitive propositions h closing negation conjunction; use
range formulas sublanguage.
basic term form Pr() w(ob, h), hypothesis formula,
ob = hob 1 , . . . , ob k nonempty sequence observations, h hypothesis.
ob = hob 1 i, write w(ob 1 , h) rather w(hob 1 i, h). before, polynomial term
form t1 + + tn , term ti product integers, basic terms, variables
(which intuitively range reals). polynomial inequality formula form
-ev
p c, p polynomial term c integer. Let Lfo
dyn (h , ) language
obtained starting primitive propositions h polynomial
inequality formulas, closing conjunction, negation, first-order quantification,
application operator. use abbreviations Section 3.
semantics logic involves models dynamic behavior. Rather
considering individual worlds, consider sequences worlds,
call runs, representing evolution system time. model infinite
run, run describes possible dynamic evolution system. before, run
records observations made hypothesis true run, well
probability distribution describing prior probability hypothesis initial
state run, evidence space E h interpret w. define
evidential run r map natural numbers (representing time) histories
7. Following discussion above, time steps associated new observations. Thus, means
true next time step, is, next observation. simplifies presentation
logic.

21

fiHalpern & Pucella

system time. history time records relevant information
runthe hypothesis true, prior probability hypotheses, evidence
space E observations made time m. Hence, history
form h(h, , E ), ob 1 , . . . , ob k i. assume r(0) = h(h, , E )i h, ,
E , r(m) = h(h, , E ), ob 1 , . . . , ob > 0. define point run
pair (r, m) consisting run r time m.
associate propositional formula Lh (h ) set [[]] hypotheses,
Section 3.
order ascribe semantics first-order formulas may contain variables,
need valuation v assigns real number every variable. Given valuation v,
evidential run r, point (r, m), r(m) = h(h, , E ), ob 1 , . . . , ob i, assign
polynomial term p real number [p]r,m,v using essentially approach
Section 3:
[x]r,m,v = v(x)
[a]r,m,v =
[Pr()]r,m,v = ( (hob 1 , . . . , ob i, )))([[]])
r(m) = h(h, , E ), ob 1 , . . . , ob
[w(ob, h0 )]r,m,v = (ob, h0 )
r(m) = h(h, , E ), ob 1 , . . . , ob
[t1 t2 ]r,m,v = [t1 ]r,m,v [t2 ]r,m,v
[p1 + p2 ]r,m,,v = [p1 ]r,m,v + [p2 ]r,m,v .
define means formula true (or satisfied) point (r, m)
evidential run r valuation v, written (r, m, v) |= , using essentially
approach Section 3:
(r, m, v) |= h r(m) = h(h, , E ), . . .i
(r, m, v) |= ob r(m) = h(h, , E ), . . . , obi
(r, m, v) |= (r, m, v) 6|=
(r, m, v) |= (r, m, v) |= (r, m, v) |=
(r, m, v) |= p c [p]r,m,v c
(r, m, v) |= (r, + 1, v) |=
(r, m, v) |= x (r, m, v 0 ) |= valuations v 0 agree v variables
x.
(r, m, v) |= true v, simply write (r, m) |= . (r, m) |= points
(r, m) r, write r |= say valid r. Finally, r |=
evidential runs r, write |= say valid.
straightforward axiomatize new logic. axiomatization shows
capture combination evidence directly logic, pleasant property.
22

fiA Logic Reasoning Evidence

axioms Section 3 carry immediately. Let axiomatization AXdyn (h , )
consists following axioms inference rules: first-order reasoning (Taut, MP), reasoning polynomial inequalities (RCF), reasoning hypotheses observations
(H1,H2,O1,O2), reasoning probabilities (Po14 only, since Pr0
language), reasoning weights evidence (E1, E2, E4), well new axioms
present.
Basically, axiom needs replacing E3, links prior posterior
probabilities, since needs expressed using operator. Moreover,
need axiom relate weight evidence sequence observation weight
evidence individual observations, given Equation (3).
E5. ob x( (Pr(h) = x)
Pr(h)w(ob, h) = xPr(h1 )w(ob, h1 ) + + xPr(hnh )w(ob, hnh )).
E6. w(ob 1 , h) w(ob k , h) = w(hob 1 , . . . , ob k i, h)w(ob 1 , h1 ) w(ob k , h1 ) + +
w(hob 1 , . . . , ob k i, h)w(ob 1 , hnh ) w(ob k , hnh ).
get complete axiomatization, need axioms inference rules capture
properties temporal operator .
T1. ( ) .
T2. .
T3. infer .
Finally, need axioms say truth hypotheses well value polynomial
terms containing occurrences Pr time-independent:
T4. .
T5. (p c) p c p contain occurrence Pr.
T6. (x) x( ).
-ev
Theorem 7.1: AXdyn (h , ) sound complete axiomatization Lfo
dyn (h , )
respect evidential runs.

8. Conclusion
literature, reasoning effect observations typically done context
prior probability set hypotheses condition
observations made obtain new probability hypotheses reflects effect
observations. paper, presented logic evidence lets us reason
weight evidence observations, independently prior probability
hypotheses. logic expressive enough capture logical form relationship
prior probability hypotheses, weight evidence observations,
result posterior probability hypotheses. capture reasoning
involve prior probabilities.
23

fiHalpern & Pucella

logic essentially propositional, obtaining sound complete axiomatization seems require quantification reals. adds complexity
logicthe decision problem full logic exponential space. However, interesting potentially useful fragment, propositional fragment, decidable polynomial
space.
Acknowledgments. preliminary version paper appeared Proceedings
Nineteenth Conference Uncertainty Artificial Intelligence, pp. 297304, 2003.
work mainly done second author Cornell University. thank Dexter
Kozen Nimrod Megiddo useful discussions. Special thanks Manfred Jaeger
careful reading paper subsequent comments. Manfred found bug
proof satisfiability problem quantifier-free fragment Lfo -ev (h , )
uses linear inequality formulas NP-complete. comments led us discuss
issue normalization. thank reviewers, whose comments greatly improved
paper. work supported part NSF grants CTC-0208535, ITR-0325453,
IIS-0534064, ONR grant N00014-01-10-511, DoD Multidisciplinary
University Research Initiative (MURI) program administered ONR grants
N00014-01-1-0795 N00014-04-1-0725, AFOSR grant F49620-02-1-0101.

Appendix A. Proofs
Proposition 2.1: ob, (ob, hi ) (ob, h3i ) l(ob, hi )
l(ob, h3i ), = 1, 2, h, ob, ob 0 , (ob, h) (ob 0 , h)
l(ob, h) l(ob 0 , h).
Proof. Let ob arbitrary observation. result follows following argument:
(ob, hi ) (ob, h3i )
iff hi (ob)/(hi (ob) + h3i (ob)) h3i (ob)/(hi (ob) + h3i (ob))
iff hi (ob)hi (ob) h3i (ob)h3i (ob)
iff hi (ob)/h3i (ob) h3i (ob)/hi (ob)
iff l(ob, hi ) l(ob, h3i ).
similar argument establishes result hypotheses.


u

Theorem 2.4: Let H = {h1 , . . . , hm } = {ob 1 , . . . , ob n }, let f real-valued
function domain H f (ob, h) [0, 1]. exists evidence space
E = (H, O, h1 , . . . , hm ) f = f satisfies following properties:
WF1. every ob O, f (ob, ) probability measure H.
P
WF2. exists x1 , . . . , xn > 0 that, h H, ni=1 f (ob , h)xi = 1.
Proof. () Assume f = evidence space E = (H, O, h1 , . . . , hm ).
routine verify WF1, fixed ob O, wEP
(ob, ) probability measure H.
verify WF2, note simply take xi = h0 H h0 (ob ).
() Let f function H [0, 1] satisfies WF1 WF2. Let
x1 , . . . , xnh positive reals guaranteed WF2. straightforward verify
24

fiA Logic Reasoning Evidence

taking h (ob ) = f (ob , h)/xi h H yields evidence space E f =
.
u

following lemmas useful prove completeness axiomatizations
paper. results depend soundness axiomatization AX(h , ).
Lemma A.1: AX(h , ) sound axiomatization logic Lfo -ev (h , ) respect evidential worlds.
Proof. easy see axiom valid evidential worlds.


u

Lemma A.2: hypothesis formulas , h1 hk provable AX(h , ),
[[]] = {h1 , . . . , hk }.
Proof. Using Taut, show provably equivalent formula 0 disjunctive
normal form. Moreover, axiom H2, assume without loss generality
disjuncts 0 consists single hypothesis. Thus, h1 hk . easy
induction structure shows hypothesis formula evidential world w,
w |= iff w |= h h [[]]. Moreover, follows immediately
soundness axiomatization (Lemma A.1) h1 . . . hk provable iff
evidential worlds w, w |= iff w |= hi {1, . . . , k}. Thus, h1 . . . hk
provable iff [[]] = {h1 , . . . , hk }.

u
easy consequence Lemma A.2 1 provably equivalent 2
[[1 ]] = [[2 ]].
Lemma A.3: Let hypothesis formula. formulas
P
Pr(h)
Pr() =
h[[]]

Pr0 () =

P

Pr0 (h)

h[[]]

provable AX(h , ).
Proof. Let h = {h1 , . . . , hnh } = {ob 1 , . . . , ob }. prove result Pr.
proceed induction size [[]]. base case, assume |[[]]| = 0.
Lemma A.2, implies provably equivalent false. Po4, Pr() =
Pr(false), easy check Pr(false) = 0 provable using Po1, Po3, Po4,
thus Pr() = 0, required. |[[]]| = n + 1 > 0, [[]] = {hi1 , . . . , hin+1 },
Lemma A.2, provably equivalent hi1 hin+1 . Po4, Pr() = Pr( hin+1 ) +
Pr( hin+1 ). easy check hin+1 provably equivalent hin+1 (using
H2), similarly hin+1 provably equivalent hi1 hin . Thus, Pr() =
hin ]]| = n, induction
provable. Since |[[hi1P
Pr(hin+1 ) + Pr(hi1 hin ) P
hypothesis, Pr(hi1 hin ) = h{hi ,...,hin } Pr(h) = h[[]]{hi } Pr(h). Thus, Pr() =
1
n+1
P
P
Pr(hin+1 ) + h[[]]{hi } Pr(h), is, Pr() = h[[]] Pr(h), required.
n+1

argument applies mutatis mutandis Pr0 , using axioms Pr14 instead
Po14.
u

25

fiHalpern & Pucella

Theorem 4.1: AX(h , ) sound complete axiomatization logic respect evidential worlds.
Proof. Soundness established Lemma A.1. prove completeness, recall following definitions. formula consistent axiom system AX(h , )
provable AX(h , ). prove completeness, sufficient show
consistent, satisfiable, is, exists evidential world w valuation v
(w, v) |= .
body paper, let h = {h1 , . . . , hnh } = {ob 1 , . . . , ob }. Let
consistent formula. way contradiction, assume unsatisfiable. reduce
formula equivalent formula language real closed fields. Let u1 , . . . , unh ,
v1 , . . . , vno , x1 , . . . , xnh , y1 , . . . , yno , z11 , . . . , zn1 h , . . . , z1no , . . . , znnho new variables, where,
intuitively,
ui gets value 1 hypothesis hi holds, 0 otherwise;
vi gets value 1 observation ob holds, 0 otherwise;
xi represents Pr0 (hi );
yi represents Pr(hi );
zi,j represents w(ob , hj ).
Let v represent list new variables. Consider following formulas. Let h
formula saying exactly one hypothesis holds:
(u1 = 0 u1 = 1) (unh = 0 unh = 1) u1 + + unh = 1.
Similarly, let formula saying exactly one observation holds:
(v1 = 0 v1 = 1) (vno = 0 vnh = 1) v1 + + vnh = 1.
Let pr formula expresses Pr0 probability measure:
pr = x1 0 xnh 0 x1 + + xnh = 1.
Similarly, let po formula expresses Pr probability measure:
po = y1 0 ynh 0 y1 + + ynh = 1.
Finally, need formulas saying w weight evidence function. formula
w ,p simply says w satisfies WF1, is, acts probability measure fixed
observation:
z1,1 0 z1,nh 0 zno ,1 0 zno ,nh 0
z1,1 + + z1,nh = 1 zno ,1 + + zno ,nh = 1.
formula w ,f says w satisfies WF2:
w1 , . . . , wno (w1 > 0 wno > 0 z1,1 w1 + + zno ,1 wno = 1
z1,nh w1 + + zno ,nh wno = 1)
26

fiA Logic Reasoning Evidence

w1 , . . . , wno new variables.
Finally, formula w ,up captures fact weights evidence viewed updating prior probability posterior probability, via Dempsters Rule Combination:
(v1 = 1 (x1 z1,1 = y1 x1 z1,1 + + y1 xnh z1,nh
xnh z1,nh = ynh x1 z1,1 + + ynh xnh z1,nh ))

(vno = 1 (x1 zno ,1 = y1 x1 zno ,1 + + y1 xnh zno ,nh
xnh zno ,nh = ynh x1 zno ,1 + . . . ynh xnh zno ,nh )).
Let formula language real closed fields obtained replacing
occurrence primitive proposition
hi ui = 1, occurrencePof ob vi =
P
1, occurrence Pr0 () hi [[]] xi , occurrence Pr() hi [[]] yi ,
occurrence w(ob , hj ) zi,j , occurrence integer coefficient k 1 + + 1
(k times). Finally, let 0 formula v(h pr po w ,p w ,f w ,up ).
easy see unsatisfiable evidential worlds, 0 false
interpreted real numbers. Therefore, 0 must formula valid real closed
fields, hence instance RCF. Thus, 0 provable. straightforward show,
using Lemma A.3, provable, contradicting fact consistent.
Thus, must satisfiable, establishing completeness.

u
mentioned beginning Section 5, Lfo -ev monotone respect
validity: axiom H1 depends set hypotheses observations, general
longer valid set changed. true O1, E3, E4. do,
however, form monotonicity respect satisfiability, following lemma
shows.
Lemma A.4: Given h , let formula Lfo -ev (h , ), let H h
hypotheses observations occur . satisfiable
evidential world h , satisfiable evidential world 0h
0o , |0h | = |H| + 1 |0o | = |O| + 1.
Proof. two steps, clarify presentation. First, show
add single hypothesis observation h preserve satisfiability .
means second step assume h 6= H 6= O. Assume
satisfied evidential world w = (h, ob, , E) h , exists
v (w, v) |= . Let 0h = h {h }, h new hypothesis h ,
let 0o = {ob }, ob new observation . Define evidential
world w0 = (h, ob, 0 , E 0 ) 0h 0o , E 0 0 defined follows. Define
probability measure 0 taking:
(
(h) h h
0
(h) =
0
h = h .

27

fiHalpern & Pucella

Similarly, define evidence space E 0 = (0h , 0o , 0 ) derived E = (h , , ) taking:


h (ob) h h ob



0
h h ob = ob
0h (ob) =

0
h = h ob



1
h = h ob ob .
Thus, 0h extends existing h assigning probability 0 new observation ob ;
contrast, new probability 0h assigns probability 1 new observation ob .
check (w0 , v) |= .
second step collapse hypotheses observations appear
one hypotheses appear H O, previous step
guaranteed exist. previous step, assume h 6= H 6= O.
Assume satisfiable evidential world w = (h, ob, , E) h , is,
exists v (w, v) |= . Pick hypothesis observation h
follows, depending hypothesis h observation ob w. Let h h h 6 H,
otherwise, let h arbitrary element h H; let 0h = H {h }. Similarly, let ob
ob ob 6 O, otherwise, let ob arbitrary element O; let 0o = {ob }.
Let w0 = (h, ob, 0 , E 0 ) evidential world 0h 0o obtained w follows.
Define probability measure 0 taking:
(
(h)
h H
0 (h) = P
0

h0 h H (h ) h = h .
Define E 0 = (0h , 0o , 0 ) derived E = (h , , ) taking:


h H ob

h (ob)


P 0
0
h (ob )
h H ob = ob
0h (ob) = Pob

h = h ob

h0 h H h0 (ob)

P

P
0


h0 h H
ob 0 h0 (ob ) h = h ob = ob .
check induction (w0 , v) |= .


u

Theorem 5.1: procedure runs space exponential || kk deciding,
given h , whether formula Lfo -ev (h , ) satisfiable evidential world.
Proof. Let formula Lfo -ev (h , ). Lemma A.4, satisfiable
construct probability measure 0h = H {h } (where H set hypotheses
appearing , h 6 H) probability measures h1 , . . . , hm 0o = {ob }
(where set observations appearing ob 6 O) E = (0h , 0o , ),
w = (h, ob, , E) (w, v) |= h, ob, v.
aim derive formula 0 language real closed fields asserts
existence probability measures. precisely, adapt construction
formula 0 proof Theorem 4.1. one change need make
ensure 0 polynomial size , construction proof
28

fiA Logic Reasoning Evidence

Theorem 4.1 guarantee. culprit fact encode integer constants k
1+ +1. straightforward modify construction use efficient
representation integer constants, namely, binary representation. example,
write 42 2(1 + 22 (1 + 22 )), expressed language real closed fields
(1 + 1)(1 + (1 + 1)(1 + 1)(1 + (1 + 1)(1 + 1))). check k coefficient length
k (when written binary), written term length O(k) language
real closed fields. Thus, modify construction 0 proof Theorem 4.1
integer constants k represented using binary encoding. easy see
|0 | polynomial || kk (since |0h | |0o | polynomial ||).
use exponential-space algorithm Ben-Or, Kozen, Reif (1986) 0 : 0
satisfiable, construct required probability measures, satisfiable;
otherwise, probability measures exist, unsatisfiable.

u
Theorem 5.2: procedure runs space exponential || kk deciding
whether exist sets primitive propositions h Lfo -ev (h , )
satisfiable evidential world.
Proof. Let h1 , . . . , hm hypotheses appearing , ob 1 , . . . , ob n hypotheses
appearing . Let h = {h1 , . . . , hm , h } = {ob 1 , . . . , ob n , ob }, h ob
hypothesis observation appearing . Clearly, |h | |o | polynomial
||. Lemma A.4, satisfiable evidential world, satisfiable evidential
world h . Theorem 5.1, algorithm determine satisfied
evidential world h runs space exponential || kk.

u
Theorem 5.3: procedure runs space polynomial || kk deciding,
given h , whether formula Lev (h , ) satisfiable evidential world.
Proof. proof result similar Theorem 5.1. Let formula
Lev (h , ). Lemma A.4, satisfiable exists probability measure
0h = H {h } (where H set hypotheses appearing , h 6 H), probability
measures h1 , . . . , hm 0o = {ob } (where set observations appearing
ob 6 O), hypothesis h, observation o, valuation v (w, v) |= ,
w = (h, ob, , E) E = (0h , 0o , ).
derive formula 0 language real closed fields asserts existence
probability measures adapting construction formula 0
proof Theorem 4.1. proof Theorem 5.1, need make sure 0
polynomial size , construction proof Theorem 4.1
guarantee. modify construction use efficient representation
integer constants, namely, binary representation. example, write 42
2(1 + 22 (1 + 22 )), expressed language real closed fields (1 + 1)(1 +
(1 + 1)(1 + 1)(1 + (1 + 1)(1 + 1))). check k coefficient length k
(when written binary), written term length O(k) language
real closed fields. modify construction 0 proof Theorem 4.1
integer constants k represented using binary encoding. easy see |0 |
polynomial || kk (since |0h | |0o | polynomial ||). key
notice resulting formula 0 written x1 . . . xn (00 ) quantifierfree formula 00 . form, apply polynomial space algorithm Canny (1988)
29

fiHalpern & Pucella

00 : 00 satisfiable, construct required probability measures,
satisfiable; otherwise, probability measures exist, unsatisfiable.
u

Theorem 5.4: procedure runs space polynomial || kk deciding
whether exists sets primitive propositions h Lev (h , )
satisfiable evidential world.
Proof. Let h1 , . . . , hm hypotheses appearing , ob 1 , . . . , ob n hypotheses
appearing . Let h = {h1 , . . . , hm , h } = {ob 1 , . . . , ob n , ob }, h ob
hypothesis observation appearing . Clearly, |h | |o | polynomial
||. Lemma A.4, satisfiable evidential world, satisfiable evidential
world h . Theorem 5.3, algorithm determine satisfied
evidential world h runs space polynomial || kk.

u
proofs Theorem 6.1 6.2 rely following small model result, variation
Lemma A.4.
Lemma A.5: Given h , let formula Lfo -ev (h , ), let H h
hypotheses observations occur . satisfiable
evidential world h , satisfiable evidential world 0h
0o |0h | = |H| + 1 |0o | = |O| + 1, where, h 0h ob 0o ,
likelihood h (ob) rational number size O(|| kk + || log(||)).
Proof. Let formula satisfiable evidential world h . Lemma A.4,
satisfiable evidential world 0h 0o , |0h | = |H|+1 |0o | = |O|+1.
force likelihoods small, adapt Theorem 2.6 FHM, says
formula f FHM logic satisfiable, satisfiable structure
probability assigned state structure rational number size O(|f | kf k+
|f | log(|f |)). formulas Lw (0h , 0o ) formulas FHM logic. result
adapts immediately, yields required bounds size likelihoods.

u
Theorem 6.1: problem deciding, given h , whether formula Lw (h , )
satisfiable evidential world NP-complete.
Proof. establish lower bound, observe reduce propositional satisfiability
satisfiability Lw (h , ). precisely, let f propositional formula,
p1 , . . . , pn primitive propositions appearing f . Let = {ob 1 , . . . , ob n , ob }
set observations, observation ob corresponds primitive proposition pi ,
ob another (distinct) observation; let h arbitrary set hypotheses, let
h arbitrary hypothesis h . Consider formula f obtained replacing every
occurrence pi f w(ob , h) > 0. straightforward verify f satisfiable
f satisfiable Lw (h , ). (We need extra observation ob take
care case f satisfiable model p1 , . . . , pn false. case,
w(ob 1 , h) = w(ob n , h) = 0, take w(ob , h) = 1.) establishes lower
bound,
upper bound straightforward. Lemma A.5, evidential world h
guessed time polynomial |h | + |o | + || kk, since prior probability
world requires assigning value |h | hypotheses, evidence space requires
30

fiA Logic Reasoning Evidence

|h | likelihood functions, assigning value |o | observations, size polynomial
|| kk. verify world satisfies time polynomial || kk + |h | + |h |.
establishes problem NP.

u
Theorem 6.2: problem deciding, formula , whether exists sets
primitive propositions h Lw (h , ) satisfiable
evidential world NP-complete.
Proof. lower bound, reduce decision problem Lw (h , ) fixed
h . Let h = {h1 , . . . , hm } = {ob 1 , . . . , ob n }, let formula
Lw (h , ). check satisfiable evidential world h
(h1 hm ) (ob 1 ob n ) satisfiable evidential world arbitrary
0h 0o . Thus, Theorem 6.1, get lower bound.
upper bound, Lemma A.5, satisfiable, satisfiable evidential
world h , h = H {h }, H consists hypotheses appearing ,
= {ob }, consists observations appearing , h ob new
hypotheses observations. Thus, |h | || + 1, |o | || + 1. proof
Theorem 6.1, world guessed time polynomial || kk + |h | + |o |,
therefore time polynomial || kk. verify world satisfies time
polynomial || kk, establishing problem NP.

u
-ev
Theorem 7.1: AXdyn (h , ) sound complete axiomatization Lfo
dyn (h , )
respect evidential runs.
Proof. easy see axiom valid evidential runs. prove completeness,
follow procedure proof Theorem 4.1, showing consistent,
satisfiable, is, exists evidential run r valuation v
(r, m, v) |= point (r, m) r.
body paper, let h = {h1 , . . . , hnh } = {ob 1 , . . . , ob }. Let
consistent formula. first step process reduce formula canonical
form respect operator. Intuitively, push every occurrence
polynomial inequality formulas present formula. easy see axioms
inference rules T1T6 used establish provably equivalent formula
0 every occurrence form subformulas n (ob) n (p c),
p polynomial term contains least one occurrence Pr operator. use
notation n . . . , n-fold application . write 0 . Let N
maximum coefficient 0 .
way contradiction, assume 0 (and hence ) unsatisfiable. proof
Theorem 4.1, reduce formula 0 equivalent formula language real
closed fields. Let u1 , . . . , unh , v10 , . . . , vn0 , . . . , v1N , . . . , vnNo , y10 , . . . , yn0 , . . . , y1N , . . . , ynNo ,
zhi1 ,...,ik i,1 , . . . , zhi1 ,...,ik i,nh (for every sequence hi1 , . . . , ik i) new variables, where, intuitively,
ui gets value 1 hypothesis hi holds, 0 otherwise;
vin gets value 1 observation ob holds time n, 0 otherwise;
yin represents Pr(hi ) time n;
31

fiHalpern & Pucella

zhi1 ,...,ik i,j represents w(hob i1 , . . . , ob ik i, hj ).
main difference construction proof Theorem 4.1 variables vin representing observations every time step n, rather variables representing observations time step, variables yin representing hypothesis probability
every time step, rather variables representing prior posterior probabilities,
variables zhi1 ,...,ik i,j representing weight evidence sequences observations, rather
variables representing weight evidence single observations. Let v represent
list new variables. consider formulas proof Theorem 4.1,
modified account new variables, fact reasoning multiple
time steps. specifically, formula h unchanged. Instead , consider formulas 1o , . . . , N
saying exactly one observation holds time time step,
given by:
(v1n = 0 v1n = 1) (vnno = 0 vnnh = 1) v1n + + vnnh = 1.
Let 0o = 1o N
.
Similarly, instead pr po , consider formulas 1p , . . . , N
p expressing Pr
probability measure time step, np given by:
y1n 0 ynnh 0 y1n + + ynnh = 1.
Let p = 1p N
p .
Similarly, consider w ,p w ,f , except replace variables zi,j zhii,j ,
reflect fact consider sequences observations. formula w ,up , capturing
update prior probability posterior probability given E5, replaced
formulas 1w ,up , . . . , N
w ,up representing update probability time step,
n
w ,up given obvious generalization w ,up :
z1,nh
(v1n = 1 (y1n1 z1,1 = y1n y1n1 z1,1 + + y1n ynn1
h
n1
n1
n
z1,nh ))
ynh z1,nh = ynh y1 z1,1 + + ynnh ynn1
h

zno ,nh
(vnno = 1 (y1n1 zno ,1 = y1n y1n1 zno ,1 + + y1n ynn1
h
n n1 z
n n1 z
ynn1
z
+
.
.
.

=

,nh
,1
,nh )).
nh 1
nh nh
h
Let 0w ,up = 1w ,up N
w ,up .
Finally, need new formula w ,c capturing relationship weight
evidence sequence observations, weight evidence individual
observations, capture axiom E6:
^
zhi1 i,h1 zhik i,h1 = zhi1 ,...,ik i,h1 zhi1 i,h1 zhik i,h1
1kN
+ + zhi1 ,...,ik i,h1 zhi1 i,hnh zhik i,hnh
1i1 ,...,ik
^

zhi1 i,hnh zhik i,hnh = zhi1 ,...,ik i,hnh zhi1 i,h1 zhik i,h1
1kN
+ + zhi1 ,...,ik i,hnh zhi1 i,hnh zhik i,hnh .
1i1 ,...,ik

32

fiA Logic Reasoning Evidence

Let formula language real closed fields obtained replacing
occurrence primitive proposition hi ui = 1, occurrence n ob
vin = 1, within
polynomial inequality formula n (p c), replacing occurrence
P
Pr() hi [[]] yin , occurrence w(hob i1 , . . . , ob ik i, hj ) zhi1 ,...,ik i,j ,
occurrence integer coefficient k 1 + + 1 (k times). Finally, let 0 formula
v(h 0o p w ,p w ,f 0w ,up w ,c ).
easy see unsatisfiable evidential systems, 0 false
real numbers. Therefore, 0 must formula valid real closed fields, hence
instance RCF. Thus, 0 provable. straightforward show, using obvious
variant Lemma A.3 provable, contradicting fact consistent.
Thus, must satisfiable, establishing completeness.

u

References
Ben-Or, M., Kozen, D., & Reif, J. H. (1986). complexity elementary algebra
geometry. Journal Computer System Sciences, 32 (1), 251264.
Canny, J. F. (1988). algebraic geometric computations PSPACE. Proc. 20th
Annual ACM Symposium Theory Computing (STOC88), pp. 460467.
Carnap, R. (1962). Logical Foundations Probability (Second edition). University
Chicago Press.
Casella, G., & Berger, R. L. (2001). Statistical Inference (Second edition). Duxbury.
Chan, H., & Darwiche, A. (2005). revision probabilistic beliefs using uncertain
evidence. Artificial Intelligence, 163, 6790.
de Alfaro, L. (1998). Formal Verification Probabilistic Systems. Ph.D. thesis, Stanford
University. Available Technical Report STAN-CS-TR-98-1601.
Enderton, H. B. (1972). Mathematical Introduction Logic. Academic Press.
Fagin, R., & Halpern, J. Y. (1994). Reasoning knowledge probability. Journal
ACM, 41 (2), 340367.
Fagin, R., Halpern, J. Y., & Megiddo, N. (1990). logic reasoning probabilities.
Information Computation, 87 (1/2), 78128.
Fischer, M. J., & Zuck, L. D. (1988). Reasoning uncertainty fault-tolerant distributed systems. Technical report YALEU/DCS/TR643, Yale University.
Good, I. J. (1950). Probability Weighing Evidence. Charles Griffin & Co. Ltd.
Good, I. J. (1960). Weights evidence, corroboration, explanatory power, information
utility experiments. Journal Royal Statistical Society, Series B, 22,
319331.
Halpern, J. Y., & Fagin, R. (1992). Two views belief: belief generalized probability
belief evidence. Artificial Intelligence, 54, 275317.
Halpern, J. Y., Moses, Y., & Tuttle, M. R. (1988). knowledge-based analysis zero
knowledge. Proc. 20th Annual ACM Symposium Theory Computing
(STOC88), pp. 132147.
33

fiHalpern & Pucella

Halpern, J. Y., & Pucella, R. (2003). logic reasoning evidence. Proc. 19th
Conference Uncertainty Artificial Intelligence (UAI03), pp. 297304.
Halpern, J. Y., & Pucella, R. (2005a). Evidence uncertain likelihoods. Proc. 21th
Conference Uncertainty Artificial Intelligence (UAI05), pp. 243250.
Halpern, J. Y., & Pucella, R. (2005b). Probabilistic algorithmic knowledge. Logical Methods
Computer Science, 1 (3:1).
Halpern, J. Y., & Tuttle, M. R. (1993). Knowledge, probability, adversaries. Journal
ACM, 40 (4), 917962.
He, J., Seidel, K., & McIver, A. (1997). Probabilistic models guarded command
language. Science Computer Programming, 28 (23), 171192.
Jeffrey, R. C. (1992). Probability Art Judgement. Cambridge University Press.
Kyburg, Jr., H. E. (1983). Recent work inductive logic. Machan, T., & Lucey, K.
(Eds.), Recent Work Philosophy, pp. 87150. Rowman & Allanheld.
Milne, P. (1996). log[p(h|eb)/p(h|b)] one true measure confirmation. Philosophy
Science, 63, 2126.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference. Morgan Kaufmann.
Popper, K. R. (1959). Logic Scientific Discovery. Hutchinson.
Renegar, J. (1992). computational complexity geometry first order theory
reals. Journal Symbolic Computation, 13 (3), 255352.
Shafer, G. (1976). Mathematical Theory Evidence. Princeton University Press.
Shafer, G. (1982). Belief functions parametric models (with commentary). Journal
Royal Statistical Society, Series B, 44, 322352.
Shoenfield, J. R. (1967). Mathematical Logic. Addison-Wesley, Reading, Mass.
Tarski, A. (1951). Decision Method Elementary Algebra Geometry (2nd edition).
Univ. California Press.
Vardi, M. Y. (1985). Automatic verification probabilistic concurrent finite-state programs.
Proc. 26th IEEE Symposium Foundations Computer Science (FOCS85),
pp. 327338.
Walley, P. (1987). Belief function representations statistical evidence. Annals Statistics,
18 (4), 14391465.

34


