Journal Artificial Intelligence Research 33 (2008) 615-655

Submitted 09/08; published 12/08

Latent Relation Mapping Engine:
Algorithm Experiments
Peter D. Turney

peter.turney@nrc-cnrc.gc.ca

Institute Information Technology
National Research Council Canada
Ottawa, Ontario, Canada, K1A 0R6

Abstract
Many AI researchers cognitive scientists argued analogy core
cognition. influential work computational modeling analogy-making
Structure Mapping Theory (SMT) implementation Structure Mapping Engine
(SME). limitation SME requirement complex hand-coded representations.
introduce Latent Relation Mapping Engine (LRME), combines ideas
SME Latent Relational Analysis (LRA) order remove requirement handcoded representations. LRME builds analogical mappings lists words, using
large corpus raw text automatically discover semantic relations among words.
evaluate LRME set twenty analogical mapping problems, ten based scientific
analogies ten based common metaphors. LRME achieves human-level performance
twenty problems. compare LRME variety alternative approaches
find able reach level performance.

1. Introduction
faced problem, try recall similar problems faced
past, transfer knowledge past experience current
problem. make analogy past situation current situation,
use analogy transfer knowledge (Gentner, 1983; Minsky, 1986; Holyoak & Thagard,
1995; Hofstadter, 2001; Hawkins & Blakeslee, 2004).
survey computational modeling analogy-making, French (2002) cites
Structure Mapping Theory (SMT) (Gentner, 1983) implementation Structure
Mapping Engine (SME) (Falkenhainer, Forbus, & Gentner, 1989) influential
work modeling analogy-making. SME, analogical mapping : B
source target B. source familiar, known, concrete,
whereas target relatively unfamiliar, unknown, abstract. analogical mapping
used transfer knowledge source target.
Gentner (1983) argues two kinds similarity, attributional similarity
relational similarity. distinction attributes relations may understood terms predicate logic. attribute predicate one argument,
large(X), meaning X large. relation predicate two arguments,
collides with(X, ), meaning X collides .
Structure Mapping Engine prefers mappings based relational similarity
mappings based attributional similarity (Falkenhainer et al., 1989). example, SME
able build mapping representation solar system (the source)
c
2008
National Research Council Canada. Reprinted permission.

fiTurney

representation Rutherford-Bohr model atom (the target). sun mapped
nucleus, planets mapped electrons, mass mapped charge. Note
mapping emphasizes relational similarity. sun nucleus different
terms attributes: sun large nucleus small. Likewise,
planets electrons little attributional similarity. hand, planets revolve
around sun electrons revolve around nucleus. mass sun attracts
mass planets charge nucleus attracts charge electrons.
Gentner (1991) provides evidence children rely primarily attributional similarity
mapping, gradually switching relational similarity mature. uses
terms mere appearance refer mapping based mostly attributional similarity, analogy
refer mapping based mostly relational similarity, literal similarity refer
mixture attributional relational similarity. Since use analogical mappings solve
problems make predictions, focus structure, especially causal relations,
look beyond surface attributes things (Gentner, 1983). analogy
solar system Rutherford-Bohr model atom illustrates importance
going beyond mere appearance, underlying structures.
Figures 1 2 show LISP representations used SME input analogy
solar system atom (Falkenhainer et al., 1989). Chalmers, French,
Hofstadter (1992) criticize SMEs requirement complex hand-coded representations.
argue hard work done human creates high-level
hand-coded representations, rather SME.
(defEntity sun :type inanimate)
(defEntity planet :type inanimate)
(defDescription solar-system
entities (sun planet)
expressions (((mass sun) :name mass-sun)
((mass planet) :name mass-planet)
((greater mass-sun mass-planet) :name >mass)
((attracts sun planet) :name attracts-form)
((revolve-around planet sun) :name revolve)
((and >mass attracts-form) :name and1)
((cause and1 revolve) :name cause-revolve)
((temperature sun) :name temp-sun)
((temperature planet) :name temp-planet)
((greater temp-sun temp-planet) :name >temp)
((gravity mass-sun mass-planet) :name force-gravity)
((cause force-gravity attracts-form) :name why-attracts)))

Figure 1: representation solar system SME (Falkenhainer et al., 1989).
Gentner, Forbus, colleagues attempted avoid hand-coding
recent work SME.1 CogSketch system generate LISP representations
simple sketches (Forbus, Usher, Lovett, Lockwood, & Wetzel, 2008). Gizmo system
generate LISP representations qualitative physics models (Yan & Forbus, 2005).
Learning Reader system generate LISP representations natural language text
(Forbus et al., 2007). systems require LISP input.
1. Dedre Gentner, personal communication, October 29, 2008.

616

fiThe Latent Relation Mapping Engine

(defEntity nucleus :type inanimate)
(defEntity electron :type inanimate)
(defDescription rutherford-atom
entities (nucleus electron)
expressions (((mass nucleus) :name mass-n)
((mass electron) :name mass-e)
((greater mass-n mass-e) :name >mass)
((attracts nucleus electron) :name attracts-form)
((revolve-around electron nucleus) :name revolve)
((charge electron) :name q-electron)
((charge nucleus) :name q-nucleus)
((opposite-sign q-nucleus q-electron) :name >charge)
((cause >charge attracts-form) :name why-attracts)))

Figure 2: Rutherford-Bohr model atom SME (Falkenhainer et al., 1989).

However, CogSketch user interface requires person draws sketch identify basic components sketch hand-label terms knowledge
base derived OpenCyc. Forbus et al. (2008) note OpenCyc contains
58,000 hand-coded concepts, added hand-coded concepts OpenCyc,
order support CogSketch. Gizmo system requires user hand-code physical
model, using methods qualitative physics (Yan & Forbus, 2005). Learning Reader
uses 28,000 phrasal patterns, derived ResearchCyc (Forbus
et al., 2007). evident SME still requires substantial hand-coded knowledge.
work present paper effort avoid complex hand-coded representations. approach combine ideas SME (Falkenhainer et al., 1989) Latent
Relational Analysis (LRA) (Turney, 2006). call resulting algorithm Latent Relation Mapping Engine (LRME). represent semantic relation two terms
using vector, elements derived pattern frequencies large corpus
raw text. semantic relations automatically derived corpus, LRME
require hand-coded representations relations. needs list terms
source list terms target. Given two lists, LRME uses corpus
build representations relations among terms, constructs mapping
two lists.
Tables 1 2 show input output LRME analogy solar
system Rutherford-Bohr model atom. Although human effort involved
constructing input lists, considerably less effort SME requires input
(contrast Figures 1 2 Table 1).
Scientific analogies, analogy solar system RutherfordBohr model atom, may seem esoteric, believe analogy-making ubiquitous
daily lives. potential practical application work task identifying
semantic roles (Gildea & Jurafsky, 2002). Since roles relations, attributes,
appropriate treat semantic role labeling analogical mapping problem.
example, Judgement semantic frame contains semantic roles judge,
evaluee, reason, Statement frame contains roles speaker, addressee, message, topic, medium (Gildea & Jurafsky, 2002). task identifying
617

fiTurney

Source
planet
attracts
revolves
sun
gravity
solar system
mass

Target B
revolves
atom
attracts
electromagnetism
nucleus
charge
electron

Table 1: representation input LRME.
Source
solar system
sun
planet
mass
attracts
revolves
gravity

Mapping








Target B
atom
nucleus
electron
charge
attracts
revolves
electromagnetism

Table 2: representation output LRME.
semantic roles automatically label sentences roles, following examples (Gildea & Jurafsky, 2002):
[Judge She] blames [Evaluee Government] [Reason failing enough
help].
[Speaker We] talked [Topic proposal] [Medium phone].
training set labeled sentences testing set unlabeled sentences,
may view task labeling testing sentences problem creating analogical
mappings training sentences (sources) testing sentences (targets). Table 3 shows blames Government failing enough help. might
mapped blame company polluting environment. mapping
found, transfer knowledge, form semantic role labels, source
target.
Source

blames
government
failing
help

Mapping






Target B

blame
company
polluting
environment

Table 3: Semantic role labeling analogical mapping.
Section 2, briefly discuss hypotheses behind design LRME.
precisely define task performed LRME, specific form analogical mapping,
618

fiThe Latent Relation Mapping Engine

Section 3. LRME builds Latent Relational Analysis (LRA), hence summarize LRA
Section 4. discuss potential applications LRME Section 5.
evaluate LRME, created twenty analogical mapping problems, ten science analogy problems (Holyoak & Thagard, 1995) ten common metaphor problems (Lakoff &
Johnson, 1980). Table 1 one science analogy problems. intended solution
given Table 2. validate intended solutions, gave colleagues lists
terms (as Table 1) asked generate mappings lists. Section 6
presents results experiment. Across twenty problems, average agreement
intended solutions (as Table 2) 87.6%.
LRME algorithm outlined Section 7, along evaluation twenty
mapping problems. LRME achieves accuracy 91.5%. difference
performance human average 87.6% statistically significant.
Section 8 examines variety alternative approaches analogy mapping task.
best approach achieves accuracy 76.8%, approach requires hand-coded partof-speech tags. performance significantly LRME human performance.
Section 9, discuss questions raised results preceding
sections. Related work described Section 10, future work limitations considered
Section 11, conclude Section 12.

2. Guiding Hypotheses
section, list assumptions guided design LRME.
results present paper necessarily require assumptions, might
helpful reader, understand reasoning behind approach.
1. Analogies semantic relations: Analogies based semantic relations
(Gentner, 1983). example, analogy solar system Rutherford-Bohr model atom based similarity semantic relations
among concepts involved understanding solar system semantic
relations among concepts involved Rutherford-Bohr model atom.
2. Co-occurrences semantic relations: Two terms interesting, significant semantic relation tend co-occur within relatively
small window (e.g., five words) relatively large corpus (e.g., 1010 words).
interesting semantic relation causes co-occurrence co-occurrence reliable
indicator interesting semantic relation (Firth, 1957).
3. Meanings semantic relations: Meaning relations among
words individual words. Individual words tend ambiguous polysemous.
putting two words pair, constrain possible meanings. putting
words sentence, multiple relations among words sentence,
constrain possible meanings further. focus word pairs (or tuples), instead
individual words, word sense disambiguation less problematic. Perhaps word
sense apart relations words (Kilgarriff, 1997).
4. Pattern distributions semantic relations: many-to-many mapping semantic relations patterns two terms co-occur.
example, relation CauseEffect(X, ) may expressed X causes ,
619

fiTurney

X, due X, X, on. Likewise, pattern
X may expression CauseEffect(X, ) (sick bacteria)
OriginEntity(X, ) (oranges Spain). However, given X , statistical distribution patterns X co-occur reliable signature
semantic relations X (Turney, 2006).
extent LRME works, believe success lends support hypotheses.

3. Task
paper, examine algorithms generate analogical mappings. simplicity,
restrict task generating bijective mappings; is, mappings injective
(one-to-one; instance two terms source map term
target) surjective (onto; source terms cover target terms;
target term left mapping). assume entities
mapped given input. Formally, input algorithms two sets terms,
B.
= {hA, Bi}

(1)

Since mappings bijective, B must contain number terms, m.
= {a1 , a2 , . . . , }

(2)

B = {b1 , b2 , . . . , bm }

(3)

term, ai bj , may consist single word (planet) compound two words
(solar system). words may part speech (nouns, verbs, adjectives, adverbs).
output bijective mapping B.
= {M : B}

(4)

(ai ) B

(5)

(A) = {M (a1 ), (a2 ), . . . , (am )} = B

(6)

algorithms consider accept batch multiple independent mapping
problems input generate mapping one output.
= {hA1 , B1 , hA2 , B2 , . . . , hAn , Bn i}

(7)

= {M1 : A1 B1 , M2 : A2 B2 , . . . , Mn : Bn }

(8)

Suppose terms arbitrary order a.
= ha1 , a2 , . . . ,
mapping function : B, given a, determines unique ordering b B.
620

(9)

fiThe Latent Relation Mapping Engine

b = hM (a1 ), (a2 ), . . . , (am )i

(10)

Likewise, ordering b B, given a, defines unique mapping function . Since
m! possible orderings B, m! possible mappings B. task
search m! mappings find best one. (Section 6 shows
relatively high degree consensus mappings best.)
Let P (A, B) set m! bijective mappings B. (P stands permutation, since mapping corresponds permutation.)
P (A, B) = {M1 , M2 , . . . , Mm! }

(11)

= |A| = |B|

(12)

m! = |P (A, B)|

(13)

following experiments, 7 average 9 most, m! usually around
7! = 5, 040 9! = 362, 880. feasible us exhaustively search P (A, B).
explore two basic kinds algorithms generating analogical mappings, algorithms
based attributional similarity algorithms based relational similarity (Turney,
2006). attributional similarity two words, sima (a, b) <, depends
degree correspondence properties b. correspondence
is, greater attributional similarity. relational similarity two
pairs words, simr (a : b, c : d) <, depends degree correspondence
relations : b c : d. correspondence is, greater relational
similarity. example, dog wolf relatively high degree attributional similarity,
whereas dog : bark cat : meow relatively high degree relational similarity.
Attributional mapping algorithms seek mapping (or mappings) maximizes
sum attributional similarities terms corresponding
terms B. (When multiple mappings maximize sum, break tie
randomly choosing one them.)
= arg max


X

sima (ai , (ai ))

(14)

P (A,B) i=1

Relational mapping algorithms seek mapping (or mappings) Mr maximizes
sum relational similarities.
Mr = arg max

X

X

simr (ai : aj , (ai ) : (aj ))

(15)

P (A,B) i=1 j=i+1

(15), assume simr symmetrical. example, degree relational similarity
dog : bark cat : meow degree relational similarity
bark : dog meow : cat.
simr (a : b, c : d) = simr (b : a, : c)

(16)

assume simr (a : a, b : b) interesting; example, may constant
value b. Therefore (15) designed always less j.
621

fiTurney

Let scorer (M ) scorea (M ) defined follows.

scorer (M ) =
scorea (M ) =

X

X

simr (ai : aj , (ai ) : (aj ))

i=1 j=i+1

X

sima (ai , (ai ))

(17)

(18)

i=1

Mr may defined terms scorer (M ) scorea (M ).
Mr = arg max scorer (M )

(19)

P (A,B)

= arg max scorea (M )

(20)

P (A,B)

Mr best mapping according simr best mapping according sima .
Recall Gentners (1991) terms, discussed Section 1, mere appearance (mostly attributional similarity), analogy (mostly relational similarity), literal similarity (a mixture
attributional relational similarity). take Mr abstract model mapping based analogy model mere appearance. literal similarity,
combine Mr , take care normalize scorer (M ) scorea (M )
combine them. (We experiment combining Section 9.2.)

4. Latent Relational Analysis
LRME uses simplified form Latent Relational Analysis (LRA) (Turney, 2005, 2006)
calculate relational similarity pairs words. briefly describe past
work LRA present LRME.
LRA takes input set word pairs generates output relational
similarity simr (ai : bi , aj : bj ) two pairs input.
= {a1 : b1 , a2 : b2 , . . . , : bn }

(21)

= {simr : <}

(22)

LRA designed evaluate proportional analogies. Proportional analogies form
: b :: c : d, means b c d. example, mason : stone :: carpenter : wood
means mason stone carpenter wood. mason artisan works
stone carpenter artisan works wood.
consider proportional analogies special case bijective analogical mapping,
defined Section 3, |A| = |B| = = 2. example, a1 : a2 :: b1 : b2 equivalent
M0 (23).
= {a1 , a2 } , B = {b1 , b2 } , M0 (a1 ) = b1 , M0 (a2 ) = b2 .
definition scorer (M ) (17), following result M0 .
622

(23)

fiThe Latent Relation Mapping Engine

scorer (M0 ) = simr (a1 : a2 , M0 (a1 ) : M0 (a2 )) = simr (a1 : a2 , b1 : b2 )

(24)

is, quality proportional analogy mason : stone :: carpenter : wood given
simr (mason : stone, carpenter : wood).
Proportional analogies may evaluated using attributional similarity.
definition scorea (M ) (18), following result M0 .
scorea (M0 ) = sima (a1 , M0 (a1 )) + sima (a2 , M0 (a2 )) = sima (a1 , b1 ) + sima (a2 , b2 )

(25)

attributional similarity, quality proportional analogy mason : stone :: carpenter :
wood given sima (mason, carpenter) + sima (stone, wood).
LRA handles proportional analogies. main contribution LRME extend
LRA beyond proportional analogies bijective analogies > 2.
Turney (2006) describes ten potential applications LRA: recognizing proportional
analogies, structure mapping theory, modeling metaphor, classifying semantic relations,
word sense disambiguation, information extraction, question answering, automatic thesaurus generation, information retrieval, identifying semantic roles. Two
applications (evaluating proportional analogies classifying semantic relations) experimentally evaluated, state-of-the-art results.
Turney (2006) compares performance relational similarity (24) attributional
similarity (25) task solving 374 multiple-choice proportional analogy questions
SAT college entrance test. LRA used measure relational similarity variety
lexicon-based corpus-based algorithms used measure attributional similarity.
LRA achieves accuracy 56% 374 SAT questions, significantly
different average human score 57%. hand, best performance
attributional similarity 35%. results show attributional similarity better
random guessing, good relational similarity. result consistent
Gentners (1991) theory maturation human similarity judgments.
Turney (2006) applies LRA task classifying semantic relations nounmodifier expressions. noun-modifier expression phrase, laser printer,
head noun (printer) preceded modifier (laser). task identify semantic
relation noun modifier. case, relation instrument;
laser instrument used printer. set 600 hand-labeled noun-modifier pairs
five different classes semantic relations, LRA attains 58% accuracy.
Turney (2008) employs variation LRA solving four different language tests,
achieving 52% accuracy SAT analogy questions, 76% accuracy TOEFL synonym
questions, 75% accuracy task distinguishing synonyms antonyms, 77%
accuracy task distinguishing words similar, words associated,
words similar associated. core algorithm used
four tests, tuning parameters particular test.

5. Applications LRME
Since LRME extension LRA, every potential application LRA potential
application LRME. advantage LRME LRA ability handle bijective
623

fiTurney

analogies > 2 (where = |A| = |B|). section, consider kinds
applications might benefit ability.
Section 7.2, evaluate LRME science analogies common metaphors,
supports claim two applications benefit ability handle larger sets
terms. Section 1, saw identifying semantic roles (Gildea & Jurafsky, 2002)
involves two terms, believe LRME superior LRA
semantic role labeling.
Semantic relation classification usually assumes relations binary; is,
semantic relation connection two terms (Rosario & Hearst, 2001; Nastase
& Szpakowicz, 2003; Turney, 2006; Girju et al., 2007). Yuret observed binary relations may linked underlying n-ary relations.2 example, Nastase Szpakowicz
(2003) defined taxonomy 30 binary semantic relations. Table 4 shows six binary relations Nastase Szpakowicz (2003) covered one 5-ary relation,
Agent:Tool:Action:Affected:Theme. Agent uses Tool perform Action. Somebody
something Affected Action. whole event summarized Theme.
Nastase Szpakowicz (2003)
Relation
Example
agent
student protest
purpose
concert hall
beneficiary
student discount
instrument
laser printer
object
metal separator
object property sunken ship

Agent:Tool:Action:Affected:Theme
Agent:Action
Theme:Tool
Affected:Action
Tool:Agent
Affected:Tool
Action:Affected

Table 4: six binary semantic relations Nastase Szpakowicz (2003)
viewed different fragments one 5-ary semantic relation.

SemEval Task 4, found easier manually tag datasets expanded
binary relations underlying n-ary relations (Girju et al., 2007). believe
expansion would facilitate automatic classification semantic relations. results
Section 9.3 suggest applications LRA discussed Section 4
might benefit able handle bijective analogies > 2.

6. Mapping Problems
evaluate algorithms analogical mapping, created twenty mapping problems,
given Appendix A. twenty problems consist ten science analogy problems, based
examples analogy science Chapter 8 Holyoak Thagard (1995), ten
common metaphor problems, derived Lakoff Johnson (1980).
tables Appendix show intended mappings twenty problems. validate mappings, invited colleagues Institute Information
Technology participate experiment. experiment hosted web server
2. Deniz Yuret, personal communication, February 13, 2007. observation context
work building datasets SemEval 2007 Task 4 (Girju et al., 2007).

624

fiThe Latent Relation Mapping Engine

(only accessible inside institute) people participated anonymously, using web
browsers offices. 39 volunteers began experiment 22
went way end. analysis, use data 22 participants
completed mapping problems.
instructions participants Appendix A. sequence problems
order terms within problem randomized separately participant,
remove effects due order. Table 5 shows agreement intended
mapping mappings generated participants. Across twenty problems,
average agreement 87.6%, higher agreement figures many
linguistic annotation tasks. agreement impressive, given participants
minimal instructions training.
Type

science
analogies

common
metaphors

Mapping
A1
A2
A3
A4
A5
A6
A7
A8
A9
A10
M1
M2
M3
M4
M5
M6
M7
M8
M9
M10

Source Target
solar system atom
water flow heat transfer
waves sounds
combustion respiration
sound light
projectile planet
artificial selection natural selection
billiard balls gas molecules
computer mind
slot machine bacterial mutation
war argument
buying item accepting belief
grounds building reasons theory
impediments travel difficulties
money time
seeds ideas
machine mind
object idea
following understanding
seeing understanding

Average

Agreement
90.9
86.9
81.8
79.0
79.2
97.4
74.7
88.1
84.3
83.6
93.5
96.1
87.9
100.0
77.3
89.0
98.7
89.1
96.6
78.8
87.6


7
8
8
8
7
7
7
8
9
5
7
7
6
7
6
7
7
5
8
6
7.0

Table 5: average agreement intended mappings mappings
22 participants. See Appendix details.

column labeled gives number terms set source terms
mapping problem (which equal number terms set target terms).
average problem, = 7. third column Table 5 gives mnemonic summarizes
mapping (e.g., solar system atom). Note mnemonic used input
algorithms, mnemonic shown participants experiment.
agreement figures Table 5 individual mapping problem averages
mappings problem. Appendix gives detailed view, showing
agreement individual mapping mappings. twenty problems contain
total 140 individual mappings (20 7). Appendix shows every one 140
625

fiTurney

mappings agreement 50% higher. is, every case, majority
participants agreed intended mapping. (There two cases agreement
exactly 50%. See problems A5 Table 14 M5 Table 16 Appendix A.)
select mapping chosen majority 22 participants,
get perfect score twenty problems. precisely, try m! mappings
problem, select mapping maximizes sum number participants
agree individual mapping mappings, score
100% twenty problems. strong support intended mappings
given Appendix A.
Section 3, applied Genters (1991) categories mere appearance (mostly attributional similarity), analogy (mostly relational similarity), literal similarity (a mixture
attributional relational similarity) mappings Mr , Mr
best mapping according simr best mapping according sima . twenty
mapping problems chosen analogy problems; is, intended mappings
Appendix meant relational mappings, Mr ; mappings maximize relational
similarity, simr . tried avoid mere appearance literal similarity.
Section 7 use twenty mapping problems evaluate relational mapping
algorithm (LRME), Section 8 use evaluate several different attributional
mapping algorithms. hypothesis LRME perform significantly better
attributional mapping algorithms twenty mapping problems,
analogy problems (not mere appearance problems literal similarity problems).
expect relational attributional mapping algorithms would perform approximately
equally well literal similarity problems, expect mere appearance problems
would favour attributional algorithms relational algorithms, test
latter two hypotheses, primary interest paper analogy-making.
goal test hypothesis real, practical, effective, measurable
difference output LRME output various attributional mapping algorithms. skeptic might claim relational similarity simr (a : b, c : d)
reduced attributional similarity sima (a, c) + sima (b, d); therefore relational mapping
algorithm complicated solution illusory problem. slightly less skeptical claim
relational similarity versus attributional similarity valid distinction cognitive
psychology, relational mapping algorithm capture distinction. test
hypothesis refute skeptical claims, created twenty analogical mapping
problems, show LRME handles problems significantly better
various attributional mapping algorithms.

7. Latent Relation Mapping Engine
Latent Relation Mapping Engine (LRME) seeks mapping Mr maximizes
sum relational similarities.
Mr = arg max

X

X

simr (ai : aj , (ai ) : (aj ))

(26)

P (A,B) i=1 j=i+1

search Mr exhaustively evaluating possibilities. Ties broken randomly. use simplified form LRA (Turney, 2006) calculate simr .
626

fiThe Latent Relation Mapping Engine

7.1 Algorithm
Briefly, idea LRME build pair-pattern matrix X, rows correspond
pairs terms columns correspond patterns. example, row xi: might
correspond pair terms sun : solar system column x:j might correspond
pattern X centered . patterns, wild card, match
single word. value element xij X based frequency pattern
x:j , X instantiated terms pair xi: . example,
take pattern X centered instantiate X : pair sun : solar system,
pattern sun centered solar system , thus value element
xij based frequency sun centered solar system corpus. matrix
X smoothed truncated singular value decomposition (SVD) (Golub & Van Loan,
1996) relational similarity simr two pairs terms given cosine
angle two corresponding row vectors X.
detail, LRME takes input set mapping problems generates
output corresponding set mappings.

= {hA1 , B1 , hA2 , B2 , . . . , hAn , Bn i}

(27)

= {M1 : A1 B1 , M2 : A2 B2 , . . . , Mn : Bn }

(28)

following experiments, twenty mapping problems (Appendix A) processed
one batch (n = 20).
first step make list R contains pairs terms input I.
mapping problem hA, Bi I, add R pairs ai : aj , ai aj
members A, 6= j, pairs bi : bj , bi bj members B, 6= j.
|A| = |B| = m, m(m 1) pairs m(m 1) pairs B.3
typical pair R would sun : solar system. allow duplicates R; R list
pair types, pair tokens. twenty mapping problems, R list 1,694 pairs.
pair r R, make list S(r) phrases corpus contain
pair r. Let ai : aj terms pair r. search corpus phrases
following form:
[0 1 words] ai [0 3 words] aj [0 1 words]

(29)

ai : aj R, aj : ai R, find phrases members pairs
orders, S(ai : aj ) S(aj : ai ). search template (29) used
Turney (2008).
following experiments, search corpus 5 1010 English words (about 280
GB plain text), consisting web pages gathered web crawler.4 retrieve phrases
3. m(m 1) here, m(m 1)/2, need pairs orders. want
calculate simr one order pairs, always less j (26); however, ensure
simr symmetrical, (16), need make matrix X symmetrical, rows
matrix orders every pair.
4. corpus collected Charles Clarke University Waterloo. provide copies
corpus request.

627

fiTurney

corpus, use Wumpus (Buttcher & Clarke, 2005), efficient search engine
passage retrieval large corpora.5
1,694 pairs R, find total 1,996,464 phrases corpus, average
1,180 phrases per pair. pair r = sun : solar system, typical phrase
S(r) would sun centered solar system illustrates.
Next make list C patterns, based phrases found. pair
r R, r = ai : aj , found phrase S(r), replace ai X
replace aj . remaining words may either left replaced
wild card symbol . replace ai aj X, replace
remaining words wild cards leave are. n remaining
words s, ai aj replaced, generate 2n+1 patterns s, add
patterns C. add new patterns C; is, C list pattern types,
pattern tokens; duplicates C.
example, pair sun : solar system, found phrase sun centered solar
system illustrates. replace ai : aj X : , X centered
illustrates. three remaining words, generate eight patterns,
X illustrates, X centered , X illustrates, on.
patterns added C. replace ai : aj : X, yielding centered X
illustrates. gives us another eight patterns, centered X . Thus
phrase sun centered solar system illustrates generates total sixteen patterns,
add C.
revise R, make list pairs correspond rows frequency
matrix F. remove pairs R phrases found corpus,
terms either order. Let ai : aj terms pair r. remove
r R S(ai : aj ) S(aj : ai ) empty. remove rows
would correspond zero vectors matrix F. reduces R 1,694 pairs 1,662
pairs. Let nr number pairs R.
Next revise C, make list patterns correspond columns
frequency matrix F. following experiments, stage, C contains millions
patterns, many efficient processing standard desktop computer. need
reduce C manageable size. select patterns shared
pairs. Let c pattern C. Let r pair R. phrase S(r),
pattern generated identical c, say r one
pairs generated c. sort patterns C descending order number
pairs R generated pattern, select top tnr patterns
sorted list. Following Turney (2008), set parameter 20; hence C reduced
top 33,240 patterns (tnr = 20 1,662 = 33,240). Let nc number patterns
C (nc = tnr ).
rows R columns C defined, build frequency matrix
F. Let ri i-th pair terms R (e.g., let ri sun : solar system) let cj
j-th pattern C (e.g., let cj X centered ). instantiate X
pattern cj terms ri ( sun centered solar system ). element fij F
frequency instantiated pattern corpus.
5. Wumpus developed Stefan Buttcher available http://www.wumpus-search.org/.

628

fiThe Latent Relation Mapping Engine

Note need search corpus instantiated pattern
fij , order find frequency. process creating pattern, keep track
many phrases generated pattern, pair. get frequency fij
checking record patterns generated ri .
next step transform matrix F raw frequencies form X
enhances similarity measurement. Turney (2006) used log entropy transformation,
suggested Landauer Dumais (1997). kind tf-idf (term frequency
times inverse document frequency) transformation, gives weight elements
matrix statistically surprising. However, Bullinaria Levy (2007) recently
achieved good results new transformation, called PPMIC (Positive Pointwise Mutual
Information Cosine); therefore LRME uses PPMIC. raw frequencies F used
calculate probabilities, calculate pointwise mutual information
(PMI) element matrix. element negative PMI set zero.

fij
pij = Pnr Pnc

j=1 fij

i=1

(30)

Pnc

j=1 fij
pi = Pnr Pnc

(31)

Pnr
f
Pncij
= Pnr i=1

(32)

i=1

pj

i=1



j=1 fij
j=1 fij

pij
pi pj



pmiij = log

pmiij pmiij > 0
xij =
0 otherwise

(33)
(34)

Let ri i-th pair terms R (e.g., let ri sun : solar system) let cj
j-th pattern C (e.g., let cj X centered ). (33), pij estimated probability
pattern cj instantiated pair ri ( sun centered solar system ), pi
estimated probability ri , pj estimated probability cj . ri cj
statistically independent, pi pj = pij (by definition independence), thus
pmiij zero (since log(1) = 0). interesting semantic relation
terms ri , pattern cj captures aspect semantic relation,
expect pij larger would ri cj indepedent; hence find
pij > pi pj , thus pmiij positive. (See Hypothesis 2 Section 2.)
hand, terms completely different domains may avoid other, case
find pmiij negative. PPMIC designed give high value xij
pattern cj captures aspect semantic relation terms ri ; otherwise,
xij value zero, indicating pattern cj tells us nothing
semantic relation terms ri .
experiments, F density 4.6% (the percentage nonzero elements)
X density 3.8%. lower density X due elements negative PMI,
transformed zero PPMIC.
629

fiTurney

smooth X applying truncated singular value decomposition (SVD) (Golub
& Van Loan, 1996). use SVDLIBC calculate SVD X.6 SVDLIBC designed
sparse (low density) matrices. SVD decomposes X product three matrices
UVT , U V column orthonormal form (i.e., columns orthogonal
unit length, UT U = VT V = I) diagonal matrix singular values
(Golub & Van Loan, 1996). X rank r, rank r. Let k ,
k < r, diagonal matrix formed top k singular values, let Uk Vk
matrices produced selecting corresponding columns U V. matrix
Uk k VkT matrix rank k best approximates original matrix X, sense
minimizes approximation errors. is, X = Uk k VkT minimizes kX XkF
matrices X rank k, k . . . kF denotes Frobenius norm (Golub & Van
Loan, 1996). may think matrix Uk k VkT smoothed compressed version
original matrix X. Following Turney (2006), set parameter k 300.
relational similarity simr two pairs R inner product two
corresponding rows Uk k VkT , rows normalized unit length.
simplify calculations dropping Vk (Deerwester, Dumais, Landauer, Furnas, & Harshman,
1990). take matrix Uk k normalize row unit length. Let W
resulting matrix. let Z WWT , square matrix size nr nr . matrix contains
cosines combinations two pairs R.
mapping problem hA, Bi I, let : a0 pair terms let b : b0
pair terms B. Suppose ri = : a0 rj = b : b0 , ri rj
i-th j-th pairs R. simr (a : a0 , b : b0 ) = zij , zij element i-th
row j-th column Z. either : a0 b : b0 R, S(a : a0 ), S(a0 : a),
S(b : b0 ), S(b0 : b) empty, set similarity zero. Finally, mapping
problem I, output map Mr maximizes sum relational similarities.

Mr = arg max

X

X

simr (ai : aj , (ai ) : (aj ))

(35)

P (A,B) i=1 j=i+1

simplified form LRA used calculate simr differs LRA used Turney
(2006) several ways. LRME, use synonyms generate alternate forms
pairs terms. LRME, morphological processing terms. LRME uses
PPMIC (Bullinaria & Levy, 2007) process raw frequencies, instead log entropy.
Following Turney (2008), LRME uses slightly different search template (29) LRME
sets number columns nc tnr , instead using constant. Section 7.2,
evaluate impact two changes (PPMIC nc ), tested
changes, mainly motivated desire increased efficiency
simplicity.
7.2 Experiments
implemented LRME Perl, making external calls Wumpus searching corpus
SVDLIBC calculating SVD. used Perl Net::Telnet package interprocess
6. SVDLIBC work Doug Rohde available http://tedlab.mit.edu/dr/svdlibc/.

630

fiThe Latent Relation Mapping Engine

communication Wumpus, PDL (Perl Data Language) package matrix manipulations (e.g., calculating cosines), List::Permutor package generate permutations
(i.e., loop P (A, B)).
ran following experiments dual core AMD Opteron 64 computer, running
64 bit Linux. running time spent searching corpus phrases. took
16 hours 27 minutes Wumpus fetch 1,996,464 phrases. remaining steps
took 52 minutes, SVD took 10 minutes. running time could cut half
using RAID 0 speed disk access.
Table 6 shows performance LRME baseline configuration. comparison,
agreement 22 volunteers intended mapping copied Table 5.
difference performance LRME (91.5%) human participants
(87.6%) statistically significant (paired t-test, 95% confidence level).

Mapping
A1
A2
A3
A4
A5
A6
A7
A8
A9
A10
M1
M2
M3
M4
M5
M6
M7
M8
M9
M10
Average

Source Target
solar system atom
water flow heat transfer
waves sounds
combustion respiration
sound light
projectile planet
artificial selection natural selection
billiard balls gas molecules
computer mind
slot machine bacterial mutation
war argument
buying item accepting belief
grounds building reasons theory
impediments travel difficulties
money time
seeds ideas
machine mind
object idea
following understanding
seeing understanding

Accuracy
LRME Humans
100.0
90.9
100.0
86.9
100.0
81.8
100.0
79.0
71.4
79.2
100.0
97.4
71.4
74.7
100.0
88.1
55.6
84.3
100.0
83.6
71.4
93.5
100.0
96.1
100.0
87.9
100.0
100.0
100.0
77.3
100.0
89.0
100.0
98.7
60.0
89.1
100.0
96.6
100.0
78.8
91.5
87.6

Table 6: LRME baseline configuration, compared human performance.
Table 6, column labeled Humans average 22 people, whereas LRME
column one algorithm (it average). Comparing average several scores
individual score (whether individual human algorithm) may give
misleading impression. results individual person, typically several
100% scores scores 55-75% range. average mapping problem seven
terms. possible exactly one term mapped incorrectly;
incorrect mappings, must two incorrect mappings. follows
nature bijections. Therefore score 5/7 = 71.4% uncommon.
631

fiTurney

Table 7 looks results another perspective. column labeled LRME wrong
gives number incorrect mappings made LRME twenty problems.
five columns labeled Number people N wrong show, various values N ,
may 22 people made N incorrect mappings. average mapping problem,
15 22 participants perfect score (N = 0); remaining 7 participants, 5
made two mistakes (N = 2). Table 7 shows clearly Table 6 LRMEs
performance significantly different (individual) human performance. (For yet
another perspective, see Section 9.1).

Mapping
A1
A2
A3
A4
A5
A6
A7
A8
A9
A10
M1
M2
M3
M4
M5
M6
M7
M8
M9
M10
Average

LRME
wrong
0
0
0
0
2
0
2
0
4
0
2
0
0
0
0
0
0
2
0
0
1

Number people N wrong
N =0 N =1 N =2 N =3 N 4
16
0
4
2
0
14
0
5
0
3
9
0
9
2
2
9
0
9
0
4
10
0
7
2
3
20
0
2
0
0
8
0
6
6
2
13
0
8
0
1
11
0
7
2
2
13
0
9
0
0
17
0
5
0
0
19
0
3
0
0
14
0
8
0
0
22
0
0
0
0
9
0
11
0
2
15
0
4
3
0
21
0
1
0
0
18
0
2
1
1
19
0
3
0
0
13
0
3
3
3
15
0
5
1
1


7
8
8
8
7
7
7
8
9
5
7
7
6
7
6
7
7
5
8
6
7

Table 7: Another way viewing LRME versus human performance.
Table 8, examine sensitivity LRME parameter settings. first row
shows accuracy baseline configuration, Table 6. next eight rows show
impact varying k, dimensionality truncated singular value decomposition,
50 400. eight rows show effect varying t, column factor,
5 40. number columns matrix (nc ) given number rows (nr
= 1,662) multiplied t. second last row shows effect eliminating singular
value decomposition LRME. equivalent setting k 1,662, number
rows matrix. final row gives result PPMIC (Bullinaria & Levy,
2007) replaced log entropy (Turney, 2006). LRME sensitive
manipulations: None variations Table 8 perform significantly differently
baseline configuration (paired t-test, 95% confidence level). (This necessarily mean
manipulations effect; rather, suggests larger sample problems
would needed show significant effect.)
632

fiThe Latent Relation Mapping Engine

Experiment
baseline configuration

varying k

varying

dropping SVD
log entropy

k
300
50
100
150
200
250
300
350
400
300
300
300
300
300
300
300
300
1662
300


20
20
20
20
20
20
20
20
20
5
10
15
20
25
30
35
40
20
20

nc
33,240
33,240
33,240
33,240
33,240
33,240
33,240
33,240
33,240
8,310
16,620
24,930
33,240
41,550
49,860
58,170
66,480
33,240
33,240

Accuracy
91.5
89.3
92.8
91.3
92.6
90.6
91.5
90.6
90.6
86.9
94.0
94.0
91.5
90.1
90.6
89.5
91.7
89.7
83.9

Table 8: Exploring sensitivity LRME various parameter settings modifications.

8. Attribute Mapping Approaches
section, explore variety attribute mapping approaches twenty mapping
problems. approaches seek mapping maximizes sum
attributional similarities.
= arg max


X

sima (ai , (ai ))

(36)

P (A,B) i=1

search exhaustively evaluating possibilities. Ties broken randomly. use variety different algorithms calculate sima .
8.1 Algorithms
following experiments, test five lexicon-based attributional similarity measures
use WordNet:7 HSO (Hirst & St-Onge, 1998), JC (Jiang & Conrath, 1997), LC (Leacock & Chodrow, 1998), LIN (Lin, 1998), RES (Resnik, 1995). five implemented
Perl package WordNet::Similarity,8 builds WordNet::QueryData9 package. core idea behind treat WordNet graph measure semantic
distance two terms length shortest path graph.
Similarity increases distance decreases.
7. WordNet developed team Princeton available http://wordnet.princeton.edu/.
8. Ted Pedersens WordNet::Similarity package http://www.d.umn.edu/tpederse/similarity.html.
9. Jason Rennies WordNet::QueryData package http://people.csail.mit.edu/jrennie/WordNet/.

633

fiTurney

HSO works nouns, verbs, adjectives, adverbs, JC, LC, LIN, RES
work nouns verbs. used WordNet::Similarity try possible parts speech
possible senses input word. Many adjectives, true valuable,
noun verb senses WordNet, JC, LC, LIN, RES still able
calculate similarity them. raw form word found WordNet,
WordNet::Similarity searches morphological variations word.
multiple similarity scores, multiple parts speech multiple senses, select
highest similarity score. similarity score, word WordNet,
JC, LC, LIN, RES could find alternative noun verb form
adjective adverb, set score zero.
evaluate two corpus-based attributional similarity measures: PMI-IR (Turney,
2001) LSA (Landauer & Dumais, 1997). core idea behind word
characterized company keeps (Firth, 1957). similarity two terms
measured similarity statistical distributions corpus. used corpus
Section 7 along Wumpus implement PMI-IR (Pointwise Mutual Information
Information Retrieval). LSA (Latent Semantic Analysis), used online
demonstration.10 selected Matrix Comparison option General Reading
1st year college (300 factors) topic space term-to-term comparison type. PMI-IR
LSA work parts speech.
eighth similarity measure based observation intended mappings
map terms part speech (see Appendix A). Let POS(a) partof-speech tag assigned term a. use part-of-speech tags define measure
attributional similarity, simPOS (a, b), follows.

100 = b
10 POS(a) = POS(b)
(37)
simPOS (a, b) =

0 otherwise
hand-labeled terms mapping problems part-of-speech tags (Santorini,
1990). Automatic taggers assume words tagged embedded
sentence, terms mapping problems sentences, tags
ambiguous. used knowledge intended mappings manually disambiguate
part-of-speech tags terms, thus guaranteeing corresponding terms
intended mapping always tags.
first seven attributional similarity measures above, created seven
similarity measures combining simPOS (a, b). example, let simHSO (a, b)
Hirst St-Onge (1998) similarity measure. combine simPOS (a, b) simHSO (a, b)
simply adding them.
simHSO+POS (a, b) = simHSO (a, b) + simPOS (a, b)

(38)

values returned simPOS (a, b) range 0 100, whereas values returned
simHSO (a, b) much smaller. chose large values (37) getting POS tags
match weight similarity measures. manual POS tags
10. online demonstration LSA work team University Colorado Boulder.
available http://lsa.colorado.edu/.

634

fiThe Latent Relation Mapping Engine

high weight simPOS (a, b) give unfair advantage attributional mapping
approach, relational mapping approach afford generous.
8.2 Experiments
Table 9 presents accuracy various measures attributional similarity.
best result without POS labels 55.9% (HSO). best result POS labels 76.8%
(LIN+POS). 91.5% accuracy LRME (see Table 6) significantly higher
76.8% accuracy LIN+POS (and thus, course, significantly higher everything else
Table 9; paired t-test, 95% confidence level). average human performance 87.6%
(see Table 5) significantly higher 76.8% accuracy LIN+POS (paired t-test,
95% confidence level). summary, humans LRME perform significantly better
variations attributional mapping approaches tested.
Algorithm
HSO
JC
LC
LIN
RES
PMI-IR
LSA
POS (hand-labeled)
HSO+POS
JC+POS
LC+POS
LIN+POS
RES+POS
PMI-IR+POS
LSA+POS

Reference
Hirst St-Onge (1998)
Jiang Conrath (1997)
Leacock Chodrow (1998)
Lin (1998)
Resnik (1995)
Turney (2001)
Landauer Dumais (1997)
Santorini (1990)
Hirst St-Onge (1998)
Jiang Conrath (1997)
Leacock Chodrow (1998)
Lin (1998)
Resnik (1995)
Turney (2001)
Landauer Dumais (1997)

Accuracy
55.9
54.7
48.5
48.2
43.8
54.4
39.6
44.8
71.1
73.6
69.5
76.8
71.6
72.8
65.8

Table 9: accuracy attribute mapping approaches wide variety measures
attributional similarity.

9. Discussion
section, examine three questions suggested preceding results.
difference science analogy problems common metaphor
problems? advantage combining relational attributional mapping approaches? advantage relational mapping approach attributional
mapping approach?
9.1 Science Analogies versus Common Metaphors
Table 5 suggests science analogies may difficult common metaphors.
supported Table 10, shows agreement 22 participants
intended mapping (see Section 6) varies science problems metaphor
635

fiTurney

problems. science problems lower average performance greater variation
performance. difference science problems metaphor problems
statistically significant (paired t-test, 95% confidence level).
Participant
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
Average
Standard deviation

20
72.6
88.2
90.0
71.8
95.7
83.4
79.6
91.9
89.7
80.7
94.5
90.6
93.2
97.1
86.6
80.5
93.3
86.5
92.9
90.4
82.7
96.2
87.6
7.2

Average Accuracy
10 Science 10 Metaphor
59.9
85.4
85.9
90.5
86.3
93.8
56.4
87.1
94.2
97.1
83.9
82.9
73.6
85.7
95.0
88.8
90.0
89.3
81.4
80.0
95.7
93.3
87.4
93.8
89.6
96.7
94.3
100.0
88.5
84.8
80.2
80.7
89.9
96.7
78.9
94.2
96.0
89.8
84.1
96.7
74.9
90.5
94.9
97.5
84.6
90.7
10.8
5.8

Table 10: comparison difficulty science problems versus metaphor problems 22 participants. numbers bold font scores
scores LRME.
average science problem terms (7.4) average metaphor problem
(6.6), might contribute difficulty science problems. However, Table 11
shows clear relation number terms problem (m
Table 5) level agreement. believe people find metaphor problems
easier science problems common metaphors entrenched
language, whereas science analogies peripheral.
Table 12 shows 16 algorithms studied perform slightly worse science
problems metaphor problems, difference statistically significant
(paired t-test, 95% confidence level). hypothesize attributional mapping approaches performing well enough sensitive subtle differences science
analogies common metaphors.
Incidentally, tables give us another view performance LRME comparison human performance. first row Table 12 shows performance LRME
636

fiThe Latent Relation Mapping Engine

Num terms
5
6
7
8
9

Agreement
86.4
81.3
91.1
86.5
84.3

Table 11: average agreement among 22 participants function number
terms problems.

Algorithm
LRME
HSO
JC
LC
LIN
RES
PMI-IR
LSA
POS
HSO+POS
JC+POS
LC+POS
LIN+POS
RES+POS
PMI-IR+POS
LSA+POS
Average
Standard deviation

20
91.5
55.9
54.7
48.5
48.2
43.8
54.4
39.6
44.8
71.1
73.6
69.5
76.8
71.6
72.8
65.8
61.4
14.7

Average Accuracy
10 Science 10 Metaphor
89.8
93.1
57.4
54.3
57.4
52.1
49.6
47.5
46.7
49.7
39.0
48.6
49.5
59.2
37.3
41.9
42.1
47.4
66.9
75.2
78.1
69.2
70.8
68.2
68.8
84.8
70.3
72.9
65.7
79.9
69.1
62.4
59.9
62.9
15.0
15.3

Table 12: comparison difficulty science problems versus metaphor problems 16 algorithms.

science metaphor problems. Table 10, marked bold font cases
human scores greater LRMEs scores. 20 problems, 8
cases; 10 science problems, 8 cases; 10 metaphor problems, 10 cases. evidence LRMEs performance
significantly different human performance. LRME near middle range
performance 22 human participants.
9.2 Hybrid Relational-Attributional Approaches
Recall definitions scorer (M ) scorea (M ) given Section 3.
637

fiTurney

scorer (M ) =
scorea (M ) =

X

X

simr (ai : aj , (ai ) : (aj ))

i=1 j=i+1

X

sima (ai , (ai ))

(39)

(40)

i=1

combine scores simply adding multiplying them, scorer (M )
scorea (M ) may quite different scales distributions values; therefore
first normalize probabilities.
scorer (M )
Mi P (A,B) scorer (Mi )

(41)

scorea (M )
Mi P (A,B) scorea (Mi )

(42)

probr (M ) = P
proba (M ) = P

probability estimates, assume scorer (M ) 0 scorea (M ) 0.
necessary, constant value may added scores, ensure negative.
combine scores adding multiplying probabilities.


Mr+a = arg max probr (M ) + proba (M )

(43)

P (A,B)



Mra = arg max probr (M ) proba (M )

(44)

P (A,B)

Table 13 shows accuracy LRME combined LIN+POS (the best attributional mapping algorithm Table 9, accuracy 76.8%) HSO (the best
attributional mapping algorithm use manual POS tags, accuracy
55.9%). try adding multiplying probabilities. own, LRME
accuracy 91.5%. Combining LRME LIN+POS increases accuracy 94.0%,
improvement statistically significant (paired t-test, 95% confidence level). Combining LRME HSO results decrease accuracy. decrease significant
probabilities multiplied (85.4%), significant probabilities
added (78.5%).
summary, experiments show significant advantage combining LRME
attributional mapping. However, possible larger sample problems would
show significant advantage. Also, combination methods explored (addition
multiplication probabilities) elementary. sophisticated approach,
weighted combination, may perform better.
9.3 Coherent Relations
hypothesize LRME benefits kind coherence among relations.
hand, attributional mapping approaches involve kind coherence.
638

fiThe Latent Relation Mapping Engine

Components
Relational Attributional
LRME
LIN+POS
LRME
LIN+POS
LRME
HSO
LRME
HSO

Combination
add probabilities
multiply probabilities
add probabilities
multiply probabilities

Accuracy
94.0
94.0
78.5
85.4

Table 13: performance four different hybrids relational attributional mapping
approaches.

Suppose swap two terms mapping. Let original mapping
let 0 new mapping, 0 (a1 ) = (a2 ), 0 (a2 ) = (a1 ), 0 (ai ) = (ai )
> 2. attributional similarity, impact swap score mapping
limited. Part score affected.

scorea (M ) = sima (a1 , (a1 )) + sima (a2 , (a2 )) +


X

sima (ai , (ai ))

(45)

sima (ai , (ai ))

(46)

i=3

scorea (M 0 ) = sima (a1 , (a2 )) + sima (a2 , (a1 )) +


X
i=3

hand, relational similarity, impact swap limited
way. change part mapping affects whole score. kind global
coherence relational similarity lacking attributional similarity.
Testing hypothesis LRME benefits coherence somewhat complicated,
need design experiment coherence effect isolated
effects. this, move terms outside accuracy calculation.
Let : B one twenty mapping problems, intended
mapping = |A| = |B|. Let A0 randomly selected subset size m0 . Let B 0
(A0 ), subset B maps A0 .

A0

(47)

0

B B
0

(48)
0

B = (A )

m0 = A0 = B 0
0

<m

(49)
(50)
(51)

two ways might use LRME generate mapping 0 : A0 B 0
new reduced mapping problem, internal coherence total coherence.
1. Internal coherence: select 0 based hA0 , B 0 alone.
639

fiTurney

A0 = {a1 , ..., am0 }

(52)

0

B = {b1 , ..., bm0 }

(53)
m0

0 = arg max

m0

X X

P (A0 ,B 0 ) i=1 j=i+1

simr (ai : aj , (ai ) : (aj ))

(54)

case, 0 chosen based relations internal hA0 , B 0 i.
2. Total coherence: select 0 based hA, Bi knowledge 0
must satisfy constraint 0 (A0 ) = B 0 . (This knowledge embedded
internal coherence.)

= {a1 , ..., }

(55)

B = {b1 , ..., bm }


P (A, B) = | P (A, B) (A0 ) = B 0
X

X
0
= arg max
simr (ai : aj , (ai ) : (aj ))
0

(56)
(57)
(58)

P 0 (A,B) i=1 j=i+1

case, 0 chosen using relations internal hA0 , B 0
relations hA, Bi external hA0 , B 0 i.

Suppose calculate accuracy two methods based subproblem hA0 , B 0 i. first might seem advantage total coherence,
must explore larger space possible mappings internal coherence (since
|P 0 (A, B)| larger |P (A0 , B 0 )|), additional terms explores
involved calculating accuracy. However, hypothesize total coherence
higher accuracy internal coherence, additional external relations
help select correct mapping.
test hypothesis, set m0 3 randomly generated ten new reduced
mapping problems twenty problems (i.e., total 200 new problems size
3). average accuracy internal coherence 93.3%, whereas average accuracy
total coherence 97.3%. difference statistically significant (paired t-test, 95%
confidence level).
hand, attributional mapping approaches cannot benefit total
coherence, connection attributes hA0 , B 0
attributes outside. decompose scorea (M ) two independent parts.
640

fiThe Latent Relation Mapping Engine

A00 = \ A0
0

(59)
00

A=A


P (A, B) = | P (A, B) (A0 ) = B 0
X
0 = arg max
sima (ai , (ai ))

(60)

0

(61)
(62)

P 0 (A,B)



= arg max
P 0 (A,B)


X

ai

sima (ai , (ai )) +

A0

X
ai

sima (ai , (ai ))

(63)

A00

two parts optimized independently. Thus terms external
hA0 , B 0 influence part 0 covers hA0 , B 0 i.
Relational mapping cannot decomposed independent parts way,
relations connect parts. gives relational mapping approaches inherent
advantage attributional mapping approaches.
confirm analysis, compared internal total coherence using LIN+POS
200 new problems size 3. average accuracy internal coherence
88.0%, whereas average accuracy total coherence 87.0%. difference
statistically significant (paired t-test, 95% confidence level). (The reason
difference that, two mappings score, break ties randomly.
causes random variation accuracy.)
benefit coherence suggests make analogy mapping problems easier
LRME adding terms. difficulty new terms cannot randomly
chosen; must fit logic analogy overlap existing terms.
course, important difference relational attributional mapping approaches. believe important difference relations
reliable general attributes, using past experiences make
predictions future (Hofstadter, 2001; Gentner, 2003). Unfortunately, hypothesis difficult evaluate experimentally hypothesis coherence.

10. Related Work
French (2002) gives good survey computational approaches analogy-making,
perspective cognitive science (where emphasis well computational systems
model human performance, rather well systems perform). sample
systems survey add mentioned.
French (2002) categorizes analogy-making systems symbolic, connectionist, symbolicconnectionist hybrids. Gardenfors (2004) proposes another category representational
systems AI cognitive science, calls conceptual spaces. spatial geometric systems common information retrieval machine learning (Widdows, 2004;
van Rijsbergen, 2004). influential example Latent Semantic Analysis (Landauer &
Dumais, 1997). first spatial approaches analogy-making began appear around
time Frenchs (2002) survey. LRME takes spatial approach analogy-making.
641

fiTurney

10.1 Symbolic Approaches
Computational approaches analogy-making date back Analogy (Evans, 1964)
Argus (Reitman, 1965). systems designed solve proportional analogies
(analogies |A| = |B| = 2; see Section 4). Analogy could solve proportional
analogies simple geometric figures Argus could solve simple word analogies.
systems used hand-coded rules able solve limited range problems
designers anticipated coded rules.
French (2002) cites Structure Mapping Theory (SMT) (Gentner, 1983) Structure
Mapping Engine (SME) (Falkenhainer et al., 1989) prime examples symbolic
approaches:
SMT unquestionably influential work date modeling
analogy-making applied wide range contexts ranging
child development folk physics. SMT explicitly shifts emphasis analogymaking structural similarity source target domains. Two
major principles underlie SMT:
relation-matching principle: good analogies determined mappings relations attributes (originally identical predicates
mapped)
systematicity principle: mappings coherent systems relations
preferred mappings individual relations.
structural approach intended produce domain-independent mapping process.
LRME follows principles. LRME uses relational similarity; attributional similarity involved (see Section 7.1). Coherent systems relations preferred
mappings individual relations (see Section 9.3). However, spatial (statistical,
corpus-based) approach LRME quite different symbolic (logical, hand-coded)
approach SME.
Martin (1992) uses symbolic approach handle conventional metaphors. Gentner,
Bowdle, Wolff, Boronat (2001) argue novel metaphors processed analogies,
conventional metaphors recalled memory without special processing. However,
line conventional novel metaphor unclear.
Dolan (1995) describes algorithm extract conventional metaphors
dictionary. semantic parser used extract semantic relations Longman
Dictionary Contemporary English (LDOCE). symbolic algorithm finds metaphorical
relations words, using extracted relations.
Veale (2003, 2004) developed symbolic approach analogy-making, using WordNet lexical resource. Using spreading activation algorithm, achieved score
43.0% set 374 multiple-choice lexical proportional analogy questions SAT
college entrance test (Veale, 2004).
Lepage (1998) demonstrated symbolic approach proportional analogies
used morphology processing. Lepage Denoual (2005) apply similar approach
machine translation.
642

fiThe Latent Relation Mapping Engine

10.2 Connectionist Approaches
Connectionist approaches analogy-making include ACME (Holyoak & Thagard, 1989)
LISA (Hummel & Holyoak, 1997). symbolic approaches, systems use handcoded knowledge representations, search mappings takes connectionist approach, nodes weights incrementally updated time,
system reaches stable state.
10.3 Symbolic-Connectionist Hybrid Approaches
third family examined French (2002) hybrid approaches, containing elements
symbolic connectionist approaches. Examples include Copycat (Mitchell,
1993) Tabletop (French, 1995). Much work Fluid Analogies Research
Group (FARG) concerns symbolic-connectionist hybrids (Hofstadter & FARG, 1995).
10.4 Spatial Approaches
Marx, Dagan, Buhmann, Shamir (2002) present coupled clustering algorithm,
uses feature vector representation find analogies collections text. example,
given documents Buddhism Christianity, finds related terms, {school,
Mahayana, Zen} Buddhism {tradition, Catholic, Protestant} Christianity.
Mason (2004) describes CorMet system extracting conventional metaphors
text. CorMet based clustering feature vectors represent selectional preferences
verbs. Given keywords source domain laboratory target domain finance,
able discover mappings liquid income container institution.
Turney, Littman, Bigham, Shnayder (2003) present system solving lexical
proportional analogy questions SAT college entrance test, combines thirteen
different modules. Twelve modules use either attributional similarity symbolic
approach relational similarity, one module uses spatial (feature vector) approach
measuring relational similarity. module worked much better
modules; therefore, studied detail Turney Littman (2005).
relation pair words represented vector, elements pattern
frequencies. similar LRME, one important difference Turney
Littman (2005) used fixed, hand-coded set 128 patterns, whereas LRME automatically
generates variable number patterns given corpus (33,240 patterns
experiments here).
Turney (2005) introduced Latent Relational Analysis (LRA), examined
thoroughly Turney (2006). LRA achieves human-level performance set 374
multiple-choice proportional analogy questions SAT college entrance exam. LRME
uses simplified form LRA. similar simplification LRA used Turney (2008),
system processing analogies, synonyms, antonyms, associations. contribution
LRME go beyond proportional analogies, larger systems analogical mappings.
10.5 General Theories Analogy Metaphor
Many theories analogy-making metaphor either involve computation
suggest general principles concepts specific particular computational
643

fiTurney

approach. design LRME influenced several theories type (Gentner,
1983; Hofstadter & FARG, 1995; Holyoak & Thagard, 1995; Hofstadter, 2001; Gentner,
2003).
Lakoff Johnson (1980) provide extensive evidence metaphor ubiquitous
language thought. believe system analogy-making able
handle metaphorical language, ten analogy problems derived
Lakoff Johnson (1980). agree claim metaphor merely
involve superficial relation couple words; rather, involves systematic set
mappings two domains. Thus analogy problems involve larger sets words,
beyond proportional analogies.
Holyoak Thagard (1995) argue analogy-making central daily thought,
especially finding creative solutions new problems. ten scientific analogies
derived examples analogy-making scientific creativity.

11. Limitations Future Work
Section 4, mentioned ten applications LRA, Section 5 claimed
results experiments Section 9.3 suggest LRME may perform better LRA
ten applications, due ability handle bijective analogies > 2.
focus future work testing hypothesis. particular, task semantic
role labeling, discussed Section 1, seems good candidate application LRME.
input LRME simpler input SME (compare Figures 1 2
Section 1 Table 1), still human effort involved creating input.
LRME immune criticism Chalmers, French, Hofstadter (1992),
human generates input work computer makes
mappings, although trivial matter find right mapping 5,040 (7!)
choices.
future work, would relax requirement hA, Bi must bijection
(see Section 3), adding irrelevant words (distractors) synonyms. mapping
algorithm forced decide terms include mapping terms
leave out.
would develop algorithm take proportional analogy (m = 2)
input (e.g., sun:planet::nucleus:electron) automatically expand larger analogy
(m > 2, e.g., Table 2). is, would automatically search corpus new terms
add analogy.
next step would give computer topic source domain (e.g.,
solar system) topic target domain (e.g., atomic structure), let work
rest own. might possible combining ideas LRME ideas
coupled clustering (Marx et al., 2002) CorMet (Mason, 2004).
seems analogy-making triggered people encounter problem
(Holyoak & Thagard, 1995). problem defines target us, immediately
start searching source. Analogical mapping enables us transfer knowledge
source target, hopefully leading solution problem. suggests
input ideal analogical mapping algorithm would simply statement
644

fiThe Latent Relation Mapping Engine

problem (e.g., structure atom?). Ultimately, computer might
find problems well. input would large corpus.
algorithms considered perform exhaustive search set
possible mappings P (A, B). acceptable sets small, here,
problematic larger problems. future work, necessary use
heuristic search algorithms instead exhaustive search.
takes almost 18 hours LRME process twenty mapping problems (Section 7).
better hardware changes software, time could significantly
reduced. even greater speed, algorithm could run continuously, building large
database vector representations term pairs, ready create mappings
soon user requests them. similar vision Banko Etzioni (2007).
LRME, LRA LSA (Landauer & Dumais, 1997), uses truncated singular value
decomposition (SVD) smooth matrix. Many algorithms proposed
smoothing matrices. past work LRA (Turney, 2006), experimented
Nonnegative Matrix Factorization (NMF) (Lee & Seung, 1999), Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999), Iterative Scaling (IS) (Ando, 2000), Kernel
Principal Components Analysis (KPCA) (Scholkopf, Smola, & Muller, 1997).
interesting results small matrices (around 1000 2000), none algorithms
seemed substantially better truncated SVD, none scaled matrix
sizes (1,662 33,240). However, believe SVD unique,
future work likely discover smoothing algorithm efficient effective
SVD. results Section 7.2 show significant benefit SVD. Table 8
hints PPMIC (Bullinaria & Levy, 2007) important SVD.
LRME extracts knowledge many fragments text. Section 7.1, noted
found average 1,180 phrases per pair. information 1,180
phrases combined vector, represent semantic relation pair.
quite different relation extraction (for example) Automatic Content Extraction
(ACE) Evaluation.11 task ACE identify label semantic relation single
sentence. Semantic role labeling involves labeling single sentence (Gildea & Jurafsky,
2002).
contrast LRME ACE analogous distinction cognitive
psychology semantic episodic memory. Episodic memory memory
specific event ones personal past, whereas semantic memory memory basic facts
concepts, unrelated specific event past. LRME extracts relational information
independent specific sentence, semantic memory. ACE concerned
extracting relation specific sentence, episodic memory. cognition, episodic
memory semantic memory work together synergistically. experience event,
use semantic memory interpret event form new episodic memory,
semantic memory constructed past experiences, accumulated
episodic memories. suggests synergy combining LRME-like
semantic information extraction algorithms ACE-like episodic information extraction
algorithms.
11. ACE annual event began 1999. Relation Detection Characterization (RDC)
introduced ACE 2001. information, see http://www.nist.gov/speech/tests/ace/.

645

fiTurney

12. Conclusion
Analogy core cognition. understand present analogy past.
predict future analogy past present. solve problems searching
analogous situations (Holyoak & Thagard, 1995). daily language saturated
metaphor (Lakoff & Johnson, 1980), metaphor based analogy (Gentner et al.,
2001). understand human language, solve human problems, work humans,
computers must able make analogical mappings.
best theory analogy-making Structure Mapping Theory (Gentner, 1983),
Structure Mapping Engine (Falkenhainer et al., 1989) puts much burden
analogy-making human users (Chalmers et al., 1992). LRME attempt
shift burden onto computer, remaining consistent general
principles SMT.
shown LRME able solve bijective analogical mapping problems
human-level performance. Attributional mapping algorithms (at least, tried
far) able reach level. supports SMT, claims relations
important attributes making analogical mappings.
still much research done. LRME takes load human
user, formulating input LRME easy. paper incremental step
towards future computers make surprising useful analogies minimal
human assistance.

Acknowledgments
Thanks colleagues Institute Information Technology participating
experiment Section 6. Thanks Charles Clarke Egidio Terra corpus.
Thanks Stefan Buttcher making Wumpus available giving advice use.
Thanks Doug Rohde making SVDLIBC available. Thanks WordNet team
Princeton University WordNet, Ted Pedersen WordNet::Similarity Perl package,
Jason Rennie WordNet::QueryData Perl package. Thanks LSA team
University Colorado Boulder use online demonstration LSA.
Thanks Deniz Yuret, Andre Vellino, Dedre Gentner, Vivi Nastase, Yves Lepage, Diarmuid
Seaghdha, Roxana Girju, Chris Drummond, Howard Johnson, Stan Szpakowicz,
anonymous reviewers JAIR helpful comments suggestions.

Appendix A. Details Mapping Problems
appendix, provide detailed information twenty mapping problems.
Figure 3 shows instructions given participants experiment
Section 6. instructions displayed web browsers. Tables 14, 15, 16,
17 show twenty mapping problems. first column gives problem number
(e.g., A1) mnemonic summarizes mapping (e.g., solar system atom).
second column gives source terms third column gives target terms.
mappings shown tables intended mappings. fourth column
shows percentage participants agreed intended mappings. example,
646

fiThe Latent Relation Mapping Engine

Systematic Analogies Metaphors
Instructions
presented twenty analogical mapping problems, ten based scientific
analogies ten based common metaphors. typical problem look this:
horse
legs
hay
brain
dung













?
?
?
?
?

may click drop-down menus above, see options available.
task construct analogical mapping; is, one-to-one mapping
items left items right. example:
horse
legs
hay

car
wheels
gasoline





brain
dung

driver
exhaust




mapping expresses analogy horse car. horses legs
cars wheels. horse eats hay car consumes gasoline. horses brain controls
movement horse cars driver controls movement car. horse
generates dung waste product car generates exhaust waste product.
duplicate items answers right-hand side.
duplicates missing items (question marks), get error message
submit answer.
welcome use dictionary work problems, would find
helpful.
find instructions unclear, please continue exercise.
answers twenty problems used standard evaluating output
computer algorithm; therefore, proceed confident
understand task.
Figure 3: instructions participants experiment Section 6.

647

fiTurney

Mapping
A1
solar system
atom

A2
water flow
heat transfer

A3
waves
sounds

A4
combustion
respiration

A5
sound
light

Source
solar system
sun
planet
mass
attracts
revolves
gravity
Average agreement:
water
flows
pressure
water tower
bucket
filling
emptying
hydrodynamics
Average agreement:
waves
shore
reflects
water
breakwater
rough
calm
crashing
Average agreement:
combustion
fire
fuel
burning
hot
intense
oxygen
carbon dioxide
Average agreement:
sound
low
high
echoes
loud
quiet
horn
Average agreement:










Target
atom
nucleus
electron
charge
attracts
revolves
electromagnetism










heat
transfers
temperature
burner
kettle
heating
cooling
thermodynamics










sounds
wall
echoes
air
insulation
loud
quiet
vibrating










respiration
animal
food
breathing
living
vigorous
oxygen
carbon dioxide









light
red
violet
reflects
bright
dim
lens

Agreement
86.4
100.0
95.5
86.4
90.9
95.5
81.8
90.9
86.4
95.5
86.4
72.7
72.7
95.5
95.5
90.9
86.9
86.4
77.3
95.5
95.5
81.8
63.6
100.0
54.5
81.8
72.7
95.5
90.9
72.7
59.1
77.3
77.3
86.4
79.0
86.4
50.0
54.5
100.0
90.9
77.3
95.5
79.2

POS
NN
NN
NN
NN
VBZ
VBZ
NN
NN
VBZ
NN
NN
NN
VBG
VBG
NN
NNS
NN
VBZ
NN
NN
JJ
JJ
VBG
NN
NN
NN
VBG
JJ
JJ
NN
NN
NN
JJ
JJ
VBZ
JJ
JJ
NN

Table 14: Science analogy problems A1 A5, derived Chapter 8 Holyoak
Thagard (1995).

648

fiThe Latent Relation Mapping Engine

Mapping
A6
projectile
planet

A7
artificial selection
natural selection

A8
billiard balls
gas molecules

A9
computer
mind

A10
slot machine
bacterial mutation

Source
projectile
trajectory
earth
parabolic
air
gravity
attracts
Average agreement:
breeds
selection
conformance
artificial
popularity
breeding
domesticated
Average agreement:
balls
billiards
speed
table
bouncing
moving
slow
fast
Average agreement:
computer
processing
erasing
write
read
memory
outputs
inputs
bug
Average agreement:
slot machines
reels
spinning
winning
losing
Average agreement:










Target
planet
orbit
sun
elliptical
space
gravity
attracts









species
competition
adaptation
natural
fitness
mating
wild










molecules
gas
temperature
container
pressing
moving
cold
hot











mind
thinking
forgetting
memorize
remember
memory
muscles
senses
mistake







bacteria
genes
mutating
reproducing
dying

Agreement
100.0
100.0
100.0
100.0
100.0
90.9
90.9
97.4
100.0
59.1
59.1
77.3
54.5
95.5
77.3
74.7
90.9
72.7
81.8
95.5
77.3
86.4
100.0
100.0
88.1
90.9
95.5
100.0
72.7
54.5
81.8
72.7
90.9
100.0
84.3
68.2
72.7
86.4
90.9
100.0
83.6

POS
NN
NN
NN
JJ
NN
NN
VBZ
NNS
NN
NN
JJ
NN
VBG
JJ
NNS
NN
NN
NN
VBG
VBG
JJ
JJ
NN
VBG
VBG
VB
VB
NN
NNS
NNS
NN
NNS
NNS
VBG
VBG
VBG

Table 15: Science analogy problems A6 A10, derived Chapter 8 Holyoak
Thagard (1995).

649

fiTurney

Mapping
M1
war
argument

M2
buying item
accepting belief

M3
grounds building
reasons theory

M4
impediments travel
difficulties

M5
money
time

Source
war
soldier
destroy
fighting
defeat
attacks
weapon
Average agreement:
buyer
merchandise
buying
selling
returning
valuable
worthless
Average agreement:
foundations
buildings
supporting
solid
weak
crack
Average agreement:
obstructions
destination
route
traveller
travelling
companion
arriving
Average agreement:
money
allocate
budget
effective
cheap
expensive
Average agreement:










Target
argument
debater
refute
arguing
acceptance
criticizes
logic









believer
belief
accepting
advocating
rejecting
true
false








reasons
theories
confirming
rational
dubious
flaw









difficulties
goal
plan
person
problem solving
partner
succeeding








time
invest
schedule
efficient
quick
slow

Agreement
90.9
100.0
90.9
95.5
90.9
95.5
90.9
93.5
100.0
90.9
95.5
100.0
95.5
95.5
95.5
96.1
72.7
77.3
95.5
90.9
95.5
95.5
87.9
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
95.5
86.4
86.4
86.4
50.0
59.1
77.3

POS
NN
NN
VB
VBG
NN
VBZ
NN
NN
NN
VBG
VBG
VBG
JJ
JJ
NNS
NNS
VBG
JJ
JJ
NN
NNS
NN
NN
NN
VBG
NN
VBG
NN
VB
NN
JJ
JJ
JJ

Table 16: Common metaphor problems M1 M5, derived Lakoff Johnson (1980).

650

fiThe Latent Relation Mapping Engine

Mapping
M6
seeds
ideas

M7
machine
mind

M8
object
idea

M9
following
understanding

M10
seeing
understanding

Source
seeds
planted
fruitful
fruit
grow
wither
blossom
Average agreement:
machine
working
turned
turned
broken
power
repair
Average agreement:
object
hold
weigh
heavy
light
Average agreement:
follow
leader
path
follower
lost
wanders
twisted
straight
Average agreement:
seeing
light
illuminating
darkness
view
hidden
Average agreement:










Target
ideas
inspired
productive
product
develop
fail
succeed









mind
thinking
awake
asleep
confused
intelligence
therapy







idea
understand
analyze
important
trivial










understand
speaker
argument
listener
misunderstood
digresses
complicated
simple








understanding
knowledge
explaining
confusion
interpretation
secret

Agreement
90.9
95.5
81.8
95.5
81.8
100.0
77.3
89.0
95.5
100.0
100.0
100.0
100.0
95.5
100.0
98.7
90.9
81.8
81.8
95.5
95.5
89.1
100.0
100.0
100.0
100.0
86.4
90.9
95.5
100.0
96.6
68.2
77.3
86.4
86.4
68.2
86.4
78.8

POS
NNS
VBD
JJ
NN
VB
VB
VB
NN
VBG
JJ
JJ
JJ
NN
NN
NN
VB
VB
JJ
JJ
VB
NN
NN
NN
JJ
VBZ
JJ
JJ
VBG
NN
VBG
NN
NN
JJ

Table 17: Common metaphor problems M6 M10, derived Lakoff Johnson
(1980).

651

fiTurney

problem A1, 81.8% participants (18 22) mapped gravity electromagnetism.
final column gives part-of-speech (POS) tags source target terms.
used Penn Treebank tags (Santorini, 1990). assigned tags manually.
intended mappings tags chosen mapped terms tags.
example, A1, sun maps nucleus, sun nucleus tagged NN.
POS tags used experiments Section 8. POS tags used LRME
shown participants experiment Section 6.

References
Ando, R. K. (2000). Latent semantic space: Iterative scaling improves precision interdocument similarity measurement. Proceedings 23rd Annual ACM SIGIR
Conference Research Development Information Retrieval (SIGIR-2000), pp.
216223.
Banko, M., & Etzioni, O. (2007). Strategies lifelong knowledge extraction web.
Proceedings 4th International Conference Knowledge Capture (K-CAP
2007), pp. 95102.
Bullinaria, J., & Levy, J. (2007). Extracting semantic representations word cooccurrence statistics: computational study. Behavior Research Methods, 39 (3),
510526.
Buttcher, S., & Clarke, C. (2005). Efficiency vs. effectiveness terabyte-scale information retrieval. Proceedings 14th Text REtrieval Conference (TREC 2005),
Gaithersburg, MD.
Chalmers, D. J., French, R. M., & Hofstadter, D. R. (1992). High-level perception, representation, analogy: critique artificial intelligence methodology. Journal
Experimental & Theoretical Artificial Intelligence, 4 (3), 185211.
Deerwester, S. C., Dumais, S. T., Landauer, T. K., Furnas, G. W., & Harshman, R. A.
(1990). Indexing latent semantic analysis. Journal American Society
Information Science (JASIS), 41 (6), 391407.
Dolan, W. B. (1995). Metaphor emergent property machine-readable dictionaries. Proceedings AAAI 1995 Spring Symposium Series: Representation
Acquisition Lexical Knowledge: Polysemy, Ambiguity Generativity, pp. 2732.
Evans, T. (1964). heuristic program solve geometric-analogy problems. Proceedings
Spring Joint Computer Conference, pp. 327338.
Falkenhainer, B., Forbus, K. D., & Gentner, D. (1989). structure-mapping engine:
Algorithm examples. Artificial Intelligence, 41 (1), 163.
Firth, J. R. (1957). synopsis linguistic theory 19301955. Studies Linguistic
Analysis, pp. 132. Blackwell, Oxford.
Forbus, K., Usher, J., Lovett, A., Lockwood, K., & Wetzel, J. (2008). Cogsketch: Opendomain sketch understanding cognitive science research education.
Proceedings Fifth Eurographics Workshop Sketch-Based Interfaces Modeling, Annecy, France.
652

fiThe Latent Relation Mapping Engine

Forbus, K. D., Riesbeck, C., Birnbaum, L., Livingston, K., Sharma, A., & Ureel, L. (2007).
prototype system learns reading simplified texts. AAAI Spring Symposium
Machine Reading, Stanford University, California.
French, R. (1995). Subtlety Sameness: Theory Computer Model AnalogyMaking. MIT Press, Cambridge, MA.
French, R. M. (2002). computational modeling analogy-making. Trends Cognitive
Sciences, 6 (5), 200205.
Gardenfors, P. (2004). Conceptual Spaces: Geometry Thought. MIT Press.
Gentner, D. (1983). Structure-mapping: theoretical framework analogy. Cognitive
Science, 7 (2), 155170.
Gentner, D. (1991). Language career similarity. Gelman, S., & Byrnes, J.
(Eds.), Perspectives Thought Language: Interrelations Development, pp.
225277. Cambridge University Press.
Gentner, D. (2003). smart. Gentner, D., & Goldin-Meadow, S. (Eds.),
Language Mind: Advances Study Language Thought, pp. 195235.
MIT Press.
Gentner, D., Bowdle, B. F., Wolff, P., & Boronat, C. (2001). Metaphor analogy.
Gentner, D., Holyoak, K. J., & Kokinov, B. N. (Eds.), analogical mind: Perspectives Cognitive Science, pp. 199253. MIT Press, Cambridge, MA.
Gildea, D., & Jurafsky, D. (2002). Automatic labeling semantic roles. Computational
Linguistics, 28 (3), 245288.
Girju, R., Nakov, P., Nastase, V., Szpakowicz, S., Turney, P., & Yuret, D. (2007). Semeval2007 task 04: Classification semantic relations nominals. Proceedings
Fourth International Workshop Semantic Evaluations (SemEval 2007), pp.
1318, Prague, Czech Republic.
Golub, G. H., & Van Loan, C. F. (1996). Matrix Computations (Third edition). Johns
Hopkins University Press, Baltimore, MD.
Hawkins, J., & Blakeslee, S. (2004). Intelligence. Henry Holt.
Hirst, G., & St-Onge, D. (1998). Lexical chains representations context detection
correction malapropisms. Fellbaum, C. (Ed.), WordNet: Electronic
Lexical Database, pp. 305332. MIT Press.
Hofmann, T. (1999). Probabilistic Latent Semantic Indexing. Proceedings 22nd
Annual ACM Conference Research Development Information Retrieval (SIGIR 99), pp. 5057, Berkeley, California.
Hofstadter, D. (2001). Epilogue: Analogy core cognition. Gentner, D., Holyoak,
K. J., & Kokinov, B. N. (Eds.), Analogical Mind: Perspectives Cognitive
Science, pp. 499538. MIT Press.
Hofstadter, D., & FARG (1995). Fluid Concepts Creative Analogies: Computer Models
Fundamental Mechanisms Thought. Basic Books, New York, NY.
653

fiTurney

Holyoak, K., & Thagard, P. (1989). Analogical mapping constraint satisfaction. Cognitive
Science, 13, 295355.
Holyoak, K., & Thagard, P. (1995). Mental Leaps. MIT Press.
Hummel, J., & Holyoak, K. (1997). Distributed representations structure: theory
analogical access mapping. Psychological Review, 104, 427466.
Jiang, J. J., & Conrath, D. W. (1997). Semantic similarity based corpus statistics
lexical taxonomy. Proceedings International Conference Research
Computational Linguistics (ROCLING X), pp. 1933, Tapei, Taiwan.
Kilgarriff, A. (1997). dont believe word senses. Computers Humanities, 31,
91113.
Lakoff, G., & Johnson, M. (1980). Metaphors Live By. University Chicago Press.
Landauer, T. K., & Dumais, S. T. (1997). solution Platos problem: latent semantic analysis theory acquisition, induction, representation knowledge.
Psychological Review, 104 (2), 211240.
Leacock, C., & Chodrow, M. (1998). Combining local context WordNet similarity
word sense identification. Fellbaum, C. (Ed.), WordNet: Electronic Lexical
Database. MIT Press.
Lee, D. D., & Seung, H. S. (1999). Learning parts objects nonnegative matrix
factorization. Nature, 401, 788791.
Lepage, Y. (1998). Solving analogies words: algorithm. Proceedings 36th
Annual Conference Association Computational Linguistics, pp. 728735.
Lepage, Y., & Denoual, E. (2005). Purest ever example-based machine translation: Detailed
presentation assessment. Machine Translation, 19 (3), 251282.
Lin, D. (1998). information-theoretic definition similarity. Proceedings 15th
International Conference Machine Learning (ICML-98).
Martin, J. H. (1992). Computer understanding conventional metaphoric language. Cognitive Science, 16 (2), 233270.
Marx, Z., Dagan, I., Buhmann, J., & Shamir, E. (2002). Coupled clustering: method
detecting structural correspondence. Journal Machine Learning Research, 3,
747780.
Mason, Z. (2004). CorMet: computational, corpus-based conventional metaphor extraction system. Computational Linguistics, 30 (1), 2344.
Minsky, M. (1986). Society Mind. Simon & Schuster, New York, NY.
Mitchell, M. (1993). Analogy-Making Perception: Computer Model. MIT Press, Cambridge, MA.
Nastase, V., & Szpakowicz, S. (2003). Exploring noun-modifier semantic relations.
Fifth International Workshop Computational Semantics (IWCS-5), pp. 285301,
Tilburg, Netherlands.
Reitman, W. R. (1965). Cognition Thought: Information Processing Approach. John
Wiley Sons, New York, NY.
654

fiThe Latent Relation Mapping Engine

Resnik, P. (1995). Using information content evaluate semantic similarity taxonomy.
Proceedings 14th International Joint Conference Artificial Intelligence
(IJCAI-95), pp. 448453, San Mateo, CA. Morgan Kaufmann.
Rosario, B., & Hearst, M. (2001). Classifying semantic relations noun-compounds
via domain-specific lexical hierarchy. Proceedings 2001 Conference
Empirical Methods Natural Language Processing (EMNLP-01), pp. 8290.
Santorini, B. (1990). Part-of-speech tagging guidelines Penn Treebank Project. Tech.
rep., Department Computer Information Science, University Pennsylvania.
(3rd revision, 2nd printing).
Scholkopf, B., Smola, A. J., & Muller, K.-R. (1997). Kernel principal component analysis.
Proceedings International Conference Artificial Neural Networks (ICANN1997), pp. 583588, Berlin.
Turney, P. D. (2001). Mining Web synonyms: PMI-IR versus LSA TOEFL.
Proceedings Twelfth European Conference Machine Learning (ECML-01),
pp. 491502, Freiburg, Germany.
Turney, P. D. (2005). Measuring semantic similarity latent relational analysis. Proceedings Nineteenth International Joint Conference Artificial Intelligence
(IJCAI-05), pp. 11361141, Edinburgh, Scotland.
Turney, P. D. (2006). Similarity semantic relations. Computational Linguistics, 32 (3),
379416.
Turney, P. D. (2008). uniform approach analogies, synonyms, antonyms, associations. Proceedings 22nd International Conference Computational
Linguistics (Coling 2008), pp. 905912, Manchester, UK.
Turney, P. D., & Littman, M. L. (2005). Corpus-based learning analogies semantic
relations. Machine Learning, 60 (13), 251278.
Turney, P. D., Littman, M. L., Bigham, J., & Shnayder, V. (2003). Combining independent
modules solve multiple-choice synonym analogy problems. Proceedings
International Conference Recent Advances Natural Language Processing
(RANLP-03), pp. 482489, Borovets, Bulgaria.
van Rijsbergen, C. J. (2004). Geometry Information Retrieval. Cambridge University
Press, Cambridge, UK.
Veale, T. (2003). analogical thesaurus. Proceedings 15th Innovative Applications Artificial Intelligence Conference (IAAI 2003), pp. 137142, Acapulco,
Mexico.
Veale, T. (2004). WordNet sits SAT: knowledge-based approach lexical analogy.
Proceedings 16th European Conference Artificial Intelligence (ECAI 2004),
pp. 606612, Valencia, Spain.
Widdows, D. (2004). Geometry Meaning. Center Study Language
Information, Stanford, CA.
Yan, J., & Forbus, K. D. (2005). Similarity-based qualitative simulation. Proceedings
27th Annual Meeting Cognitive Science Society, Stresa, Italy.

655


