Journal Artificial Intelligence Research 33 (2008) 1-31

Submitted 05/08; published 09/08

Anytime Induction Low-cost, Low-error Classifiers:
Sampling-based Approach
Saher Esmeir
Shaul Markovitch

esaher@cs.technion.ac.il
shaulm@cs.technion.ac.il

Computer Science Department
TechnionIsrael Institute Technology
Haifa 32000, Israel

Abstract
Machine learning techniques gaining prevalence production wide range
classifiers complex real-world applications nonuniform testing misclassification
costs. increasing complexity applications poses real challenge resource
management learning classification. work introduce ACT (anytime
cost-sensitive tree learner), novel framework operating complex environments.
ACT anytime algorithm allows learning time increased return lower
classification costs. builds tree top-down exploits additional time resources
obtain better estimations utility different candidate splits. Using sampling
techniques, ACT approximates cost subtree candidate split favors
one minimal cost. stochastic algorithm, ACT expected able
escape local minima, greedy methods may trapped. Experiments
variety datasets conducted compare ACT state-of-the-art cost-sensitive
tree learners. results show majority domains ACT produces significantly
less costly trees. ACT exhibits good anytime behavior diminishing returns.

1. Introduction
Traditionally, machine learning algorithms focused induction models
low expected error. many real-word applications, however, several additional constraints
considered. Assume, example, medical center decided use machine learning techniques build diagnostic tool heart disease. comprehensibility
decision tree models (Hastie, Tibshirani, & Friedman, 2001, chap. 9) makes preferred choice base tool. Figure 1 shows three possible trees. first tree
(upper-left) makes decisions using results cardiac catheterization (heart cath).
tree expected highly accurate. Nevertheless, high costs risks associated heart cath procedure make decision tree impractical. second tree
(lower-left) dispenses need cardiac catheterization reaches decision based
single, simple, inexpensive test: whether patient complains chest pain.
tree would highly accurate: people experience chest pain
indeed healthy. tree, however, distinguish costs different types
errors. false positive prediction might result extra treatments, false negative
prediction might put persons life risk. Therefore, third tree (right) preferred, one
attempts minimize test costs misclassification costs simultaneously.
c
2008
AI Access Foundation. rights reserved.

fiEsmeir & Markovitch

heart cath


normal

chest pain


yes

yes

blood
pressure

alerting



blood
pressure


yes

cardiac
stress

normal

normal



normal

normal

heart cath

yes





yes

yes

alerting

yes

normal

alerting

normal

heart cath

chest pain


yes

alerting

Figure 1: Three possible decision trees diagnosis heart diseases. upper-left tree
bases decision solely heart cath therefore accurate prohibitively
expensive. lower-left tree dispenses need heart cath reaches
decision using single, simple, inexpensive test: whether patient
complains chest pain. tree would highly accurate
distinguish costs different error types. third (right-hand)
tree preferable: attempts minimize test costs misclassification costs
simultaneously.

cost(a1-10) = $$

a9
a10
0

1

a7

a10
1

a6

a9

0

cost(a1-8) = $$
cost(a9,10) = $$$$$$

a1

0

a9
1

1

a4
0

0

a4
1

1

0

Figure 2: Left: example difficulty greedy learners might face. Right: example
importance context-based feature evaluation.

Finding tree lowest expected total cost least NP-complete.1
cost insensitive case, greedy heuristic used bias search towards low-cost trees.
Decision Trees Minimal Cost (DTMC), greedy method attempts minimize
1. Finding smallest consistent tree, easier problem, NP-complete (Hyafil & Rivest, 1976).

2

fiAnytime Induction Low-cost, Low-error Classifiers

types costs simultaneously, recently introduced (Ling, Yang, Wang, &
Zhang, 2004; Sheng, Ling, Ni, & Zhang, 2006). tree built top-down, greedy split
criterion takes account testing misclassification costs used. basic
idea estimate immediate reduction total cost split, prefer
split maximal reduction. split reduces cost training data,
induction process stopped.
Although efficient, DTMC approach trapped local minimum
produce trees globally optimal. example, consider concept costs
described Figure 2 (left). 10 attributes, a9 a10 relevant.
cost a9 a10 , however, significantly higher others. high costs may
hide usefulness a9 a10 , mislead learner repeatedly splitting a18 ,
would result large, expensive tree. problem would intensified a9
a10 interdependent, low immediate information gain (e.g., a9 a10 ).
case, even costs uniform, local measure might fail recognize relevance
a9 a10 .
DTMC appealing learning resources limited. However, requires
fixed runtime cannot exploit additional resources escape local minima. many
real-life applications, willing wait longer better tree induced (Esmeir &
Markovitch, 2006). example, importance model saving patients lives may
convince medical center allocate 1 month learn it. Algorithms exploit
additional time produce better solutions called anytime algorithms (Boddy & Dean,
1994).
ICET algorithm (Turney, 1995) pioneer non-greedy search tree
minimizes test misclassification costs. ICET uses genetic search produce new
set costs reflects original costs contribution attribute
reducing misclassification costs. builds tree using EG2 algorithm (Nunez,
1991) evolved costs instead original ones. EG2 greedy cost-sensitive
algorithm builds tree top-down evaluates candidate splits considering
information gain yield measurement costs. not, however, take
account misclassification cost problem.
ICET shown significantly outperform greedy tree learners, producing trees
lower total cost. ICET use additional time resources produce generations
hence widen search space costs. genetic operations randomized,
ICET likely escape local minima EG2 original costs might
trapped. Nevertheless, two shortcomings limit ICETs ability benefit extra time.
First, search phase, uses greedy EG2 algorithm build final tree.
EG2 prefers attributes high information gain (and low test cost),
usefulness highly relevant attributes may underestimated greedy measure
case hard-to-learn concepts attribute interdependency hidden. result
expensive trees. Second, even ICET overcomes problem randomly
reweighting attributes, searches space parameters globally, regardless
context tree. imposes problem attribute important one subtree
useless another. better understand shortcomings, consider concept described
tree Figure 2 (right). 10 attributes similar costs. value a1
determines whether target concept a7 a9 a4 a6 . interdependencies result
3

fiEsmeir & Markovitch

low gain attributes. ICET assigns costs globally, attributes
similar costs well. Therefore, ICET able recognize one relevant
context. irrelevant attributes cheaper, problem intensified
model might end relying irrelevant attributes.
Recently, introduced cost-insensitive LSID3 algorithm, induce
accurate trees allocated time (Esmeir & Markovitch, 2007a). algorithm evaluates candidate split estimating size smallest consistent tree
it. estimation based sampling space consistent trees, size
sample determined advance according allocated time. LSID3 designed,
however, minimize test misclassification costs. work build LSID3
propose ACT, anytime cost-sensitive tree learner exploit additional time
produce lower-cost trees. Applying sampling mechanism cost-sensitive setup,
however, trivial imposes three major challenges: (1) produce sample,
(2) evaluate sampled trees, (3) prune induced trees. Section
3 show obstacles may overcome.
Section 4 report extensive set experiments compares ACT several
decision tree learners using variety datasets costs assigned human experts
automatically. results show ACT significantly better majority
problems. addition, ACT shown exhibit good anytime behavior diminishing
returns.

2. Cost-Sensitive Classification
Offline concept learning consists two stages: learning stage, set labeled
examples used induce classifier; classification stage, induced
classifier used classify unlabeled instances. two stages involve different types
costs (Turney, 2000). primary goal work trade learning speed
reduction test misclassification costs. make problem well defined, need
specify: (1) misclassification costs represented, (2) test costs calculated,
(3) combine types cost.
answer questions, adopt model described Turney (1995). problem
|C| different classes, misclassification cost matrix |C| |C| matrix whose Mi,j
entry defines penalty assigning class ci instance actually belongs
class cj . Typically, entries main diagonal classification cost matrix (no error)
zero.
classifying example e using tree , propagate e tree along
single path root one leaves. Let (T, e) set tests along
path. denote cost() cost administering test . testing cost e
P
therefore tcost(T, e) = cost(). Note use sets notation tests
appear several times charged once. addition, model described Turney
(1995) handles two special test types, namely grouped delayed tests.
Grouped Tests. tests share common cost, would charge
once. Typically, test extra (possibly different) cost. example, consider
tree path tests cholesterol level glucose level. values measured,
blood test needed. Taking blood samples measure cholesterol level clearly lowers
4

fiAnytime Induction Low-cost, Low-error Classifiers

cost measuring glucose level. Formally, test possibly belongs group.2
first test group administered, charge full cost. another
test group already administered earlier decision path,
charge marginal cost.
Delayed Tests. Sometimes outcome test cannot obtained immediately, e.g.,
lab test results. tests, called delayed tests, force us wait outcome
available. Alternatively, Turney (1995) suggests taking account possible outcomes:
delayed test encountered, tests subtree administered
charged for. result delayed test available, prediction hand.
One problem setup follows paths subtree, regardless
outcome non-delayed costs. Moreover, possible distinguish delays
different tests impose: example, one result might ready several minutes
another days. work handle delayed tests,
explain ACT modified take account.
test misclassification costs measured, important question
remains: combine them? Following Turney (1995), assume
cost types given scale. general model would require utility function
combines types. Qin, Zhang, Zhang (2004) presented method handle
two kinds cost scales setting maximal budget one kind cost minimizing
one. Alternatively, patient preferences elicited summarized utility
function (Lenert & Soetikno, 1997).
Note algorithm introduce paper adapted cost model.
important property cost-sensitive setup maximizing generalization accuracy,
goal existing learners, viewed special case: accuracy
objective, test costs ignored misclassification cost uniform.

3. ACT Algorithm
ACT, proposed anytime framework induction cost-sensitive decision trees, builds
recently introduced LSID3 algorithm. LSID3 adopts general top-down scheme
induction decision trees (TDIDT): starts entire set training examples,
partitions subsets testing value attribute, recursively builds
subtrees. Unlike greedy inducers, LSID3 invests time resources making better split
decisions. every candidate split, LSID3 attempts estimate size resulting
subtree split take place. Following Occams razor (Blumer, Ehrenfeucht, Haussler, & Warmuth, 1987; Esmeir & Markovitch, 2007b), favors one smallest
expected size.
estimation based biased sample space trees rooted evaluated
attribute. sample obtained using stochastic version ID3 (Quinlan, 1986),
call SID3. SID3, rather choosing attribute maximizes information
gain (as ID3), choose splitting attribute semi-randomly. likelihood
attribute chosen proportional information gain. Due randomization,
2. model test may belong single group. However, easy extend work allow
tests belong several groups.

5

fiEsmeir & Markovitch

Procedure LSID3-Choose-Attribute(E, A, r)
r = 0
Return ID3-Choose-Attribute(E, A)
Foreach
Foreach vi domain(a)
Ei {e E | a(e) = vi }
mini
Repeat r times
SID3(Ei , {a})
mini min (mini , Size(T ))
P|domain(a)|
totala i=1
mini
Return totala minimal
Figure 3: Attribute selection LSID3
repeated invocations SID3 result different trees. candidate attribute a, LSID3
invokes SID3 r times form sample r trees rooted a, uses size smallest
tree sample evaluate a. Obviously, r larger, resulting size estimations
expected accurate, improving final tree. Consider, example, 3-XOR
concept several additional irrelevant attributes. LSID3 prefer one relevant
attributes root, one trees samples relevant attributes must
smallest. probability event increases increase sample size.
LSID3 contract anytime algorithm parameterized r, sample size. Additional
time resources utilized forming larger samples. Figure 3 lists procedure
attribute selection applied LSID3. Let = |E| number examples
n = |A| number attributes. runtime complexity LSID3 O(rmn3 ). LSID3
shown exhibit good anytime behavior diminishing returns. applied
hard concepts, produced significantly better trees ID3 C4.5.
ACT takes sampling approach LSID3. three major components
LSID3 need replaced order adapt cost-sensitive problems are: (1)
sampling space trees, (2) evaluating tree, (3) pruning tree.
3.1 Obtaining Sample
LISD3 uses SID3 bias samples towards small trees. ACT, however, would
bias sample towards low-cost trees. purpose, designed stochastic version
EG2 algorithm, attempts build low cost trees greedily. EG2, tree
built top-down, test maximizes ICF chosen splitting node, where,
ICF () =

2I() 1
.
(cost () + 1)w

information gain (as ID3). parameter w [0, 1] controls bias
towards lower cost attributes. w = 0, test costs ignored ICF relies solely
6

fiAnytime Induction Low-cost, Low-error Classifiers

Procedure SEG2-Choose-Attribute(E, A)
Foreach
(a) Information-Gain(E, a)
c (a) Cost(a)
2I(a) 1
p (a) (c(a)+1)
w
Choose attribute random A;
attribute a, probability
selecting proportional p (a)
Return
Figure 4: Attribute selection SEG2
information gain. Larger values w strengthen effect test costs ICF.
discuss setting value w Section 3.5.
stochastic EG2 (SEG2), choose splitting attributes semi-randomly, proportionally
ICF. SEG2 stochastic, expect able escape local minima
least trees sample. Figure 4 formalizes attribute selection component
SEG2. obtain sample size r, ACT uses EG2 SEG2 r 1 times. EG2
SEG2 given direct access context-based costs, i.e., attribute already
tested, cost zero another attribute belongs group
tested, group discount applied.
3.2 Evaluating Subtree
LSID3 cost-insensitive learning algorithm. such, main goal maximize
expected accuracy learned tree. Occams razor states given two consistent
hypotheses, smaller one likely accurate. Following Occams razor, LSID3
uses tree size preference bias favors splits expected reduce final
size.
cost-sensitive setup, however, goal minimize expected total cost
classification. Therefore, rather choosing attribute minimizes size,
would choose one minimizes total cost. Given decision tree, need
come procedure estimates expected cost using tree classify
future case. cost two components: test cost misclassification cost.
3.2.1 Estimating Test Costs
Assuming distribution future cases would similar learning
examples, estimate test costs using training data. Given tree, calculate
average test cost training examples use estimate test cost new
cases. (sub)tree built E, set training examples, denote average
cost traversing example E
tcost(T, E) =

1 X
tcost(T, e).
eE
7

fiEsmeir & Markovitch


estimated test cost unseen example e therefore tcost(T,
e ) = tcost(T, E).
Observe costs calculated relevant context. attribute already
tested upper nodes, charge testing again. Similarly, attribute
group g already tested, apply group discount attributes
g. delayed attribute encountered, sum cost entire subtree.

3.2.2 Estimating Misclassification Costs
go estimating cost misclassification obvious. tree size
longer used heuristic predictive errors. Occams razor allows comparison
two consistent trees provides means estimating accuracy. Moreover, tree size
measured different currency accuracy hence cannot easily incorporated
cost function.
Rather using tree size, propose different estimator: expected error
(Quinlan, 1993). leaf training examples, misclassified,
expected error defined upper limit probability error, i.e., EE(m, s, cf ) =
bin (m, s), cf confidence level U bin upper limit confidence
Ucf
interval binomial distribution. expected error tree sum expected
errors leaves.
Originally, expected error used C4.5s error-based pruning predict whether
subtree performs better leaf. Although lacking theoretical basis, shown
experimentally good heuristic. ACT use expected error approximate
misclassification cost. Assume problem |C| classes misclassification cost
matrix . Let c class label leaf l. Let ml total number examples
l mil number examples l belong class i. penalties
predictive errors uniform (Mi,j = mc), estimated misclassification cost l
(l) = EE(ml , ml mc , cf ) mc.
mcost
l

problem nonuniform misclassification costs, mc replaced cost
actual errors leaf expected make. errors obviously unknown
learner. One solution estimate error type separately using confidence intervals
multinomial distribution multiply associated cost:
(l) =
mcost

X

mul
Ucf
(ml , mil , |C|) mc.

i6=c

approach, however, would result overly pessimistic approximation, mainly
many classes. Alternatively, compute expected error uniform
case propose replacing mc weighted average penalty classifying
instance c belongs another class. weights derived proportions
mil
ml mc using generalization Laplaces law succession (Good, 1965, chap. 4):
l

(l) = EE(ml , ml mc , cf )
mcost
l

X
i6=c



!

mil + 1
Mc,i .
ml mcl + |C| 1

Note problem C classes, average C 1 possible penalties
Mc,c = 0. Hence, problem two classes c1 , c2 leaf marked c1 , mc
8

fiAnytime Induction Low-cost, Low-error Classifiers

Procedure ACT-Choose-Attribute(E, A, r)
r = 0
Return EG2-Choose-Attribute(E, A)
Foreach
Foreach vi domain(a)
Ei {e E | a(e) = vi }
EG2(a, Ei , {a})
mini Total-Cost(T, Ei )
Repeat r 1 times
SEG2(a, Ei , {a})
mini min (mini , Total-Cost(T, Ei ))
P|domain(a)|
mini
totala Cost(a) + i=1
Return totala minimal
Figure 5: Attribute selection ACT
would replaced M1,2 . classifying new instance, expected misclassification
cost tree built examples sum expected misclassification costs
leaves divided m:
1 X

mcost(l),
mcost(T
)=

L set leaves . Hence, expected total cost classifying
single instance is:



total(T,
E) = tcost(T,
E) + mcost(T
).

alternative approach intend explore future work estimate cost
sampled trees using cost set-aside validation set. approach attractive
mainly training set large one afford setting aside significant part
it.
3.3 Choosing Split
decided sampler tree utility function, ready formalize
tree growing phase ACT. tree built top-down. procedure selecting
splitting test node listed Figure 5 illustrated Figure 6. give
detailed example ACT chooses splits explain split selection procedure
modified numeric attributes.
3.3.1 Choosing Split: Illustrative Examples
ACTs evaluation cost-senstive considers test error costs simultaneously
take account different error penalties. illustrate let us consider
two-class problem mc = 100$ (uniform) 6 attributes, a1 , . . . , a6 , whose costs
10$. training data contains 400 examples, 200 positive 200
negative.
9

fiEsmeir & Markovitch



G2
SE
st( 4.9
=

co
)

cost(EG2)
=4.7
cost(SEG2)
=5.1

cost(EG2)
=8.9

Figure 6: Attribute evaluation ACT. Assume cost current context
0.4. estimated cost subtree rooted therefore 0.4 + min(4.7, 5.1) +
min(8.9, 4.9) = 10.

Test costs
a1 10
a2 10
a3 10
a4 10
a5 10
a6 10
MC costs
FP 100
FN 100

r=1

T1

T2

a1
a3

5 +
95 EE = 7.3

-

a2
a4

a5

95 +
5
EE = 7.3

+

5 +
95 EE = 7.3

-

95 +
5
EE = 7.3

+

mcost (T1) = 7.3*4*100$ / 400 = 7.3$
tcost (T1) = 20$
total(T1) = 27.3$

0 +
50 EE = 1.4

-

a6

100 +
50 EE = 54.1

+

0 +
50 EE = 1.4

-

100 +
50 EE = 54.1

+

mcost (T2) = (1.4*2 + 54.1*2) * 100$ / 400 = 27.7$
tcost (T2) = 20$
total (T2) = 47.7$

Figure 7: Evaluation tree samples ACT. leftmost column defines costs: 6
attributes identical cost uniform error penalties. 1 sampled a1
2 a2 . EE stands expected error. total cost T1
lower, ACT would prefer split a1 .

Assume choose a1 a2 , r = 1. Let trees
Figure 7, denoted 1 2, sampled a1 a2 respectively. expected
error costs T1 T2 are:3

mcost(T
1) =


mcost(T
2) =

=

1
4 7.3
(4 EE (100, 5, 0.25)) 100$ =
100$ = 7.3$
400
400
1
(2 EE (50, 0, 0.25) 100$ + 2 EE (150, 50, 0.25) 100$)
400
2 1.4 + 2 54.1
100$ = 27.7$
400

test error costs involved, ACT considers sum. Since test
cost trees identical (20$), ACT would prefer split a1 . If, however, cost
3. example set cf 0.25, C4.5. Section 3.5 discuss tune cf .

10

fiAnytime Induction Low-cost, Low-error Classifiers

Test costs
a1 40
a2 10
a3 10
a4 10
a5 10
a6 10
MC costs
FP 100
FN 100

r=1

T1

T2

a1
a3

-

a4

a5

95 +
5
EE = 7.3

5 +
95 EE = 7.3

95 +
5
EE = 7.3

5 +
95 EE = 7.3

a2

-

+

+

mcost (T1) = 7.3*4*100$ / 400 = 7.3$
tcost (T1) = 50$
total(T1) = 57.3$

a6

-

100 +
50 EE = 54.1

0 +
50 EE = 1.4

100 +
50 EE = 54.1

0 +
50 EE = 1.4

-

+

+

mcost (T2) = (1.4*2 + 54.1*2) * 100$ / 400 = 27.7$
tcost (T2) = 20$
total (T2) = 47.7$

Figure 8: Evaluation tree samples ACT. leftmost column defines costs: 6
attributes identical cost (except expensive a1 ) uniform error
penalties. 1 sampled a1 2 a2 . total cost T2
lower, ACT would prefer split a2 .

Test costs
a1 10
a2 10
a3 10
a4 10
a5 10
a6 10
MC costs
FP 1
FN 199

r=1

T1

T2

a1
a3

5 +
95 EE = 7.3

-

a2
a4

a5

95 +
5
EE = 7.3

+

5 +
95 EE = 7.3

-

95 +
5
EE = 7.3

+

mcost (T1) = (7.3*2*1$ + 7.3*2*199$) / 400 = 7.3$
tcost (T1) = 20$
total (T1) = 27.3$

0 +
50 EE = 1.4

-

a6

100 +
50 EE = 54.1

+

0 +
50 EE = 1.4

-

100 +
50 EE = 54.1

+

mcost (T2) = (54.1*2*1$ + 1.4*2*199$) / 400 = 1.7$
tcost (T2) = 20$
total (T2) = 21.7$

Figure 9: Evaluation tree samples ACT. leftmost column defines costs: 6
attributes identical cost nonuniform error penalties. 1 sampled
a1 2 a2 . total cost T2 lower, ACT would prefer
split a2 .

a1 40$, Figure 8, tcost(T 1) would become 50$ total cost 1 would
become 57.3$, 2 would remain 47.7$. Hence, case ACT would split
a2 .
illustrate ACT handles nonuniform error penalties, let us assume cost
attributes 10$, cost false positive (FP ) 1$ cost
false negative (F N ) 199$. Let trees Figure 9, denoted 1 2, sampled
a1 a2 respectively. first example, misclassification costs play role
test costs trees same. Although average misclassification
11

fiEsmeir & Markovitch

cost 100$, ACT evaluates trees differently:

mcost(T
1) =

=


mcost(T
2) =

=

1
(2 EE (100, 5, 0.25) 1$ + 2 EE (100, 5, 0.25) 199$)
400
2 7.3 1$ + 2 7.3.1 199$
= 7.3$
400
1
(2 EE (50, 0, 0.25) 199$ + 2 EE (100, 50, 0.25) 1$)
400
2 1.4 199$ + 2 54.1 1$
= 1.7$
400

Therefore, nonuniform setup, ACT would prefer a2 . makes sense
given setup prefer trees may result false positives reduce number
expensive false negatives.
3.3.2 Choosing Split Attributes Numeric
selection procedure formalized Figure 5 must modified slightly attribute numeric: rather iterating values attribute take, first pick
r tests (split points) highest information gain invoke EG2
split point. guarantees numeric nominal attributes get resources.
Chickering, Meek, Rounthwaite (2001) introduced several techniques generating
small number candidate split points dynamically little overhead. future
intend apply techniques select r points, evaluated
single invocation EG2.
3.4 Cost-Sensitive Pruning
Pruning plays important role decision tree induction. cost-insensitive environments,
main goal pruning simplify tree order avoid overfitting training
data. subtree pruned resulting tree expected yield lower error.
test costs taken account, pruning another important role: reducing
test costs tree. Keeping subtree worthwhile expected reduction misclassification costs larger cost tests subtree. misclassification
cost zero, makes sense keep split tree. If, hand, misclassification cost much larger test costs, would expect similar behavior
cost-insensitive setup.
handle challenge, propose novel approach cost-sensitive pruning.
error-based pruning (Quinlan, 1993), scan tree bottom-up. compare
expected total cost subtree leaf. leaf expected perform better,
subtree pruned.
cost subtree estimated described Section 3.2. Formally, let E
set training examples reach subtree , let size E. Assume
examples E belong default class.4 Let L set leaves .
4. misclassification costs uniform, default class majority class. Otherwise, class
minimizes misclassification cost node.

12

fiAnytime Induction Low-cost, Low-error Classifiers

prune leaf if:
X
1


mcost(l).
EE(m, s, cf ) mc tcost(T,
E) +



assumes uniform misclassification cost mc. case nonuniform penalties,
multiply expected error average misclassification cost.
alternative approach post-pruning early stopping growing phase.
example, one could limit depth tree, require minimal number examples
child (as C4.5), prevent splitting nodes splitting criterion fails exceed
predetermined threshold (as DTMC). Obviously, pre-pruning condition
applied part post-pruning procedure. advantage post-pruning, however,
ability estimate effect split entire subtree it,
immediate successors (horizon effect).
Consider example 2-XOR problem b. Splitting neither b would
positive gain hence growing would stopped. pre-pruning allowed,
optimal tree would found would post-pruned utility
splits correctly measured. Frank (2000) reports comprehensive study pruning
decision trees, compared pre- post-pruning empirically cost-insensitive
setup. findings show advantage post-pruning variety UCI datasets
significant. pre-pruning computationally efficient, Frank concluded
that, practice, might viable alternative post-pruning. Despite results,
decided use post-pruning ACT, following reasons:
1. Several concepts represented UCI repository may appear real-world
problems. example, parity functions naturally arise real-world problems,
Drosophila survival concept (Page & Ray, 2003).
2. costs involved, horizon effect may appear frequently high
costs may hide good splits.
3. anytime setup user willing wait longer order obtain good tree.
Since post-pruning takes even less time induction single greedy tree,
extra cost post-pruning minor.
future plan add pre-pruning parameter allow early stopping
resources limited. Another interesting direction future work would postprune final tree pre-prune lookahead trees form samples. would
reduce runtime cost less accurate estimations utility candidate
split.
3.5 Setting Parameters ACT
addition r, sample size, ACT parameterized w, controls weight
test costs EG2, cf , confidence factor used pruning error
estimation. ICET tunes w cf using genetic search. ACT considered three different
alternatives: (1) keeping EG2s C4.5s default values w = 1 cf = 0.25, (2) tuning
13

fiEsmeir & Markovitch

values using cross-validation, (3) setting values priori, function
problem costs.
first solution simplest, exploit potential adapting
sampling mechanism specific problem costs. Although tuning values using
grid search would achieve good results, may costly terms runtime. example,
5 values parameter used 5-fold cross-validation, would need
run ACT 125 times sake tuning alone. anytime setup time could
invested invoke ACT larger r hence improve results. Furthermore,
algorithm would able output valid solution tuning stage finished.
Alternatively, could try tune parameters invoking much faster EG2,
results would good optimal values EG2 necessarily
good ACT.
third approach, chose experiments, set w cf advance,
according problem specific costs. w set inverse proportionally misclassification cost: high misclassification cost results smaller w, reducing effect attribute
costs split selection measure. exact formula is:
w = 0.5 + ex ,
x average misclassification cost (over non-diagnoal entries ) divided
C, cost take tests. Formally,
x=

P

Mi,j
.
(|C| 1) |C| C
i6=j

C4.5 default value cf 0.25. Larger cf values result less pruning. Smaller
cf values lead aggressive pruning. Therefore, ACT set cf value
range [0.2, 0.3]; exact value depends problem cost. test costs dominant,
prefer aggressive pruning hence low value cf . test costs negligible,
prefer prune less. value cf used estimate expected error.
Again, test costs dominant, afford pessimistic estimate error,
misclassification costs dominant, would prefer estimate closer
error rate training data. exact formula setting cf is:
x1
cf = 0.2 + 0.05(1 +
).
x+1

4. Empirical Evaluation
conducted variety experiments test performance behavior ACT.
First introduce novel method automatic adaption existing datasets costsensitive setup. describe experimental methodology motivation. Finally
present discuss results.
4.1 Datasets
Typically, machine learning researchers use datasets UCI repository (Asuncion &
Newman, 2007). five UCI datasets, however, assigned test costs.5 include
5. Costs datasets assigned human experts (Turney, 1995).

14

fiAnytime Induction Low-cost, Low-error Classifiers

datasets experiments. Nevertheless, gain wider perspective,
developed automatic method assigns costs existing datasets. method
parameterized with:
1. cr, cost range.
2. g, number desired groups percentage number attributes.
problem |A| attributes, g |A| groups. probability attribute
1
, probability belong
belong groups g|A|+1
groups.
3. d, number delayed tests percentage number attributes.
4. , group discount percentage minimal cost group (to ensure
positive costs).
5. , binary flag determines whether costs drawn randomly, uniformly ( = 0)
semi-randomly ( = 1): cost test drawn proportionally information
gain, simulating common case valuable features tend higher costs.
case assume cost comes truncated normal distribution,
mean proportional gain.
Using method, assigned costs 25 datasets: 20 arbitrarily chosen UCI datasets6
5 datasets represent hard concepts used previous research. Appendix gives detailed descriptions datasets.
Due randomization cost assignment process, set parameters
defines infinite space possible costs. 25 datasets sampled space
4 times
cr = [1, 100], g = 0.2, = 0, = 0.8, = 1.
parameters chosen attempt assign costs manner similar
real costs assigned. total, 105 datasets: 5 assigned human experts
100 automatically generated costs.7
Cost-insensitive learning algorithms focus accuracy therefore expected perform well testing costs negligible relative misclassification costs. However,
testing costs significant, ignoring would result expensive classifiers. Therefore,
evaluating cost-sensitive learners requires wide spectrum misclassification costs.
problem 105, created 5 instances, uniform misclassification costs
mc = 100, 500, 1000, 5000, 10000. Later on, consider nonuniform misclassification
costs.
4.2 Methodology
start experimental evaluation comparing ACT, given fixed resource allocation, several cost-sensitive cost-insensitive algorithms. Next compare
anytime behavior ACT ICET. Finally, evaluate algorithms
6. chosen UCI datasets vary size, type attributes, dimension.
7. additional 100 datasets available http://www.cs.technion.ac.il/esaher/publications/cost.

15

fiEsmeir & Markovitch

two modifications problem instances: random test cost assignment nonuniform
misclassification costs.
4.2.1 Compared Algorithms
ACT compared following algorithms:
C4.5 : cost-insensitive greedy decision tree learner. algorithm reimplemented following details (Quinlan, 1993) default parameters
used.
LSID3 : cost-insensitive anytime decision tree learner. uses extra time
induce trees higher accuracy. able, however, exploit additional allotted
time reduce classification costs.
IDX : greedy top-down learner prefers splits maximize
c (Norton, 1989).
algorithm take account misclassification costs. IDX implemented top C4.5, modifying split selection criteria.
2

CSID3 : greedy top-down learner prefers splits maximize Ic (Tan &
Schlimmer, 1989). algorithm take account misclassification costs.
CSID3 implemented top C4.5, modifying split selection criteria.
EG2 : greedy top-down learner prefers splits maximize

2I() 1

w

(cost()+1)
(Nunez, 1991). algorithm take account misclassification costs.
EG2 implemented top C4.5, modifying split selection criteria.

DTMC : DTMC implemented following original pseudo-code (Ling et al.,
2004; Sheng et al., 2006). However, original pseudo-code support continuous attributes multiple class problems. added support continuous
attributes, C4.5s dynamic binary-cut discretization, cost reduction
replacing gain ratio selecting cutting points. extension multiple class problems straightforward. Note DTMC post-prune trees
pre-prunes them.
ICET : ICET reimplemented following detailed description given
Turney (1995). verify results reimplementation, compared
reported literature. followed experimental setup
used 5 datasets. results indeed similar: basic version ICET
achieved average cost 50.8 reimplementation vs. 50 reported originally.
One possible reason slight difference may initial population
genetic algorithm randomized, genetic operators process
partitioning data training, validating, testing sets. paper, Turney
introduced seeded version ICET, includes true costs initial
population, reported perform better unseeded version. Therefore,
use seeded version comparison. parameters ICET
default ones.
16

fiAnytime Induction Low-cost, Low-error Classifiers

4.2.2 Normalized Cost
Turney (1995) points out, using average cost classification dataset
problematic because: (1) cost differences algorithms become relatively small
misclassification cost increases, (2) difficult combine results multiple
datasets fair manner (e.g., average), (3) difficult combine average
different misclassification costs. overcome problems, Turney suggests normalizing
average cost classification dividing standard cost. Let C cost
take tests. Let frequency class data. error response
always class therefore (1 ). standard cost defined
C + mini (1 ) maxi,j (Mi,j ) .
standard cost approximation maximal cost given problem.
consists two components: maximal test cost misclassification cost
classifier achieves baseline accuracy (e.g., majority-based classifier error
costs uniform). classifiers may perform even worse baseline
accuracy, standard cost strictly upper bound real cost.
experiments, however, exceeded.
4.2.3 Statistical Significance
problem 105, single 10-fold cross-validation experiment conducted.
partition train-test sets used compared algorithms. determine
statistical significance performance differences ACT, ICET, DTMC
used two tests:
Paired t-test = 5% confidence. problem 105
pair algorithms, 10 pairs results obtained 10-fold cross
validation runs. used paired t-test determine weather difference
two algorithms given problem significant (rejecting null hypothesis
algorithms differ performance). Then, count algorithm
many times significant winner.
Wilcoxon test (Demsar, 2006), compares classifiers multiple datasets
states whether one method significantly better ( = 5%).
4.3 Fixed-time Comparison
105 5 problem instances, ran different algorithms, including ACT
r = 5. chose r = 5 average runtime ACT would shorter ICET
problems. methods much shorter runtime due greedy nature.
Table 1 summarizes results.8 pair numbers represents average normalized
cost associated confidence interval ( = 5%). Figure 10 illustrates average results
plots normalized costs different algorithms misclassification costs.
Statistical significance test results ACT, ICET, DTMC given Table 2.
algorithms compared using t-test Wilcoxon test. table lists
8. full results available http://www.cs.technion.ac.il/esaher/publications/cost.

17

fiEsmeir & Markovitch

Table 1: Average cost classification percentage standard cost classification
different mc values. numbers represent average 105 datasets
associated confidence intervals ( = 5%).
mc
100
500
1000
5000
10000

C4.5
50.6
49.9
50.4
53.3
54.5

LSID3

4.2
4.2
4.6
5.9
6.4

45.3
43.0
42.4
43.6
44.5

IDX

3.7
3.9
4.5
6.1
6.6

34.4
42.4
47.5
58.1
60.8

CSID3

3.6
3.6
4.2
5.9
6.4

41.7
45.2
47.8
54.3
56.2

3.8
3.9
4.4
5.9
6.4

EG2
35.1
42.5
47.3
57.3
59.9

DTMC

3.6
3.6
4.3
5.9
6.4

14.6
37.7
47.1
57.6
59.5

1.8
3.1
3.8
5.2
5.6

ICET
24.3
36.3
40.6
45.7
47.1

3.1
3.1
3.9
5.6
6.0

ACT
15.2
34.5
39.1
41.5
41.4

1.9
3.2
4.2
5.7
6.0

Table 2: DTMC vs. ACT ICET vs. ACT using statistical tests. mc, first
column lists number t-test significant wins second column gives
winner, any, implied Wilcoxon test datasets = 5%.
test WINS
mc
100
500
1000
5000
10000

DTMC vs. ACT
14
9
7
7
6

3
29
45
50
56

W ilcoxon WINNER

ICET vs. ACT
4
5
12
15
7

54
23
24
21
24

DTMC vs. ACT

ICET vs. ACT

DTMC
ACT
ACT
ACT
ACT

ACT
ACT
ACT
ACT
-

number t-test wins algorithm 105 datasets, well winner,
any, Wilcoxon test applied.
misclassification cost relatively small (mc = 100), ACT clearly outperforms
ICET, 54 significant wins opposed ICETs 4 significant wins. significant
difference found remaining runs. setup ACT able produce
small trees, sometimes consisting one node; accuracy learned model ignored
setup. ICET, contrary, produced, datasets, larger
costly trees. DTMC achieved best results, outperformed ACT 14 times.
Wilcoxon test indicates DTMC better ACT ACT better
ICET. investigation showed datasets ACT produced unnecessarily
larger trees. believe better tuning cf would improve ACT scenario
making pruning aggressive.
extreme, misclassification costs dominate (mc = 10000), performance DTMC worse ACT ICET. t-test indicates ACT
significantly better ICET 24 times significantly worse 7 times. According
Wilcoxon test = 5%, difference ACT ICET significant.
Taking > 5.05%, however, would turn result favor ACT. Observe DTMC,
winner mc = 100, becomes worst algorithm mc = 10000. One reason
18

fiAnytime Induction Low-cost, Low-error Classifiers

Average % Standard Cost

60

50

40

30
C4.5
LSID3
EG2
DTMC
ICET
ACT

20

10
100

1000
Misclassification Cost

10000

ACT Cost

Figure 10: Average normalized cost function misclassification cost

100

100

100

80

80

80

60

60

60

40

40

40

20

20

20

0

0
0

20

40
60
ICET Cost

80

100

0
0

20

40
60
ICET Cost

80

100

0

20

40
60
ICET Cost

80

100

Figure 11: Illustration differences performance ACT ICET mc =
100, 1000, 10000 (from left right). point represents dataset. x-axis
represents cost ICET y-axis represents ACT. dashed
line indicates equality. Points ACT performs better
ICET better.

phenomenon DTMC, introduced Ling et al. (2004), perform
post-pruning, although might improve accuracy domains.
two extremes less interesting: first could use algorithm
always outputs tree size 1 second could use cost-insensitive learners.
middle range, mc {500, 1000, 5000}, requires learner carefully balance
two types cost. cases ACT lowest average cost largest
number t-test wins. Moreover, Wilcoxon test indicates superior. ICET
second best method. reported Turney (1995), ICET clearly better
greedy methods EG2, IDX, CSID3.
Note EG2, IDX, CSID3, insensitive misclassification cost, produced trees values mc. trees, however, judged differently
change misclassification cost.
Figure 11 illustrates differences ICET ACT mc = 100, 1000, 10000.
point represents one 105 datasets. x-axis represents cost ICET
y-axis represents ACT. dashed line indicates equality. see,
19

fiEsmeir & Markovitch

100

Average Accuracy

90

80

70
C4.5
LSID3
EG2
DTMC
ICET
ACT

60

50
100

1000
Misclassification Cost

10000

Figure 12: Average accuracy function misclassification cost

majority points equality line, indicating ACT performs better.
mc = 10000 see points located close x-axis large x
value. points represent difficult domains, XOR, ICET could
learn ACT could.
4.4 Comparing Accuracy Learned Models
misclassification costs low, optimal algorithm would produce shallow
tree. misclassification costs dominant, optimal algorithm would produce
highly accurate tree. see, ACTs normalized cost increases increase
misclassification cost. relatively easy produce shallow trees, concepts
easily learnable even cost-insensitive algorithms fail achieve perfect accuracy
them. Hence, importance accuracy increases, normalized cost increases
predictive errors affect dramatically.
learn effect misclassification costs accuracy, compare
accuracy built trees different misclassification costs. Figure 12 shows results.
important property DTMC, ICET, ACT ability compromise accuracy needed. produce inaccurate trees accuracy insignificant much
accurate trees penalty error high. ACTs flexibility, however,
noteworthy: second least accurate method becomes accurate one.
Interestingly, accuracy extremely important, ICET ACT achieve even
better accuracy C4.5. reason non-greedy nature. ICET performs
implicit lookahead reweighting attributes according importance. ACT performs
lookahead sampling space subtrees every split. two, results
indicate ACTs lookahead efficient terms accuracy. DTMC less accurate
C4.5. reason different split selection criterion different pruning
mechanism.
comparison anytime cost insensitive algorithm LSID3, ACT produced less
accurate trees mc relatively low. mc set 5000, however, ACT
achieved comparable accuracy LSID3 slightly outperformed mc = 10000.
Statistical tests found differences accuracy two algorithms
20

fiAnytime Induction Low-cost, Low-error Classifiers

10
EG2
DTMC
ICET
ACT

84
8
Average Cost

Average Cost

82
80
EG2
DTMC
ICET
ACT

78
76

6

4

2

74
72

0
0

0.5

1

1.5

2
2.5
Time [sec]

3

3.5

4

4.5

0

0.2

0.4

0.6

0.8
1
Time [sec]

1.2

1.4

1.6

1.8

80
100
70

90
80
Average Cost

Average Cost

60
50
40
EG2
DTMC
ICET
ACT

30
20

EG2
DTMC
ICET
ACT

70
60
50
40
30

10

20
0
0

1

2
3
Time [sec]

4

5

0

1

2

3
Time [sec]

4

5

6

Figure 13: Average normalized cost function time (from top-left bottom-right)
Breast-cancer-20, Monks1, Multi-XOR, XOR5

case insignificant. ACTs small advantage datasets indicates that,
problems, expected error better heuristic tree size maximizing accuracy.
4.5 Comparison Anytime Behavior
ICET ACT, typical anytime algorithms, perform better increased
resource allocation. ICET expected exploit extra time producing generations hence better tuning parameters final invocation EG2. ACT
use extra time acquire larger samples hence achieve better cost estimations.
examine anytime behavior ICET ACT, ran 4 problems,
namely Breast-cancer-20, Monks-1, Multi-XOR, XOR5, exponentially increasing
time allocation. mc set 5000. ICET run 2, 4, 8, . . . generations ACT
sample size 1, 2, 4, . . .. fixed-time comparison, used 4 instances
problem. Figure 13 plots results averaged 4 instances. included
results greedy methods EG2 DTMC.
results show good anytime behavior ICET ACT: generally worthwhile allocate time. ACT dominates ICET four domains able
produce less costly trees shorter time.
One advantage ACT ICET able consider context
attribute judged. ICET, contrary, reassigns cost attributes globally:
21

fiEsmeir & Markovitch

Average % Standard Cost

60

50

DTMC

ICET

ACT

40

100
500
1000
5000
10000

30

20

10
100

DTMC
ICET
ACT
1000
Misclassification Cost

12.3
31.5
40.4
54.0
57.4

1.8
3.2
3.9
5.2
5.6

18.7
31.8
36.4
43.7
45.6

2.7
3.4
3.9
5.5
5.9

13.0
30.2
33.9
38.5
39.6

2.0
3.3
4.0
5.6
6.1

10000

Figure 14: Average cost test costs assigned randomly

attribute cannot assigned high cost one subtree low cost another. MultiXOR dataset exemplifies concept whose attributes important one sub-concept.
concept composed four sub-concepts, relies different attributes
(see Appendix details). expected, ACT outperforms ICET significantly
latter cannot assign context-based costs. Allowing ICET produce
generations (up 128) result trees comparable obtained ACT.
4.6 Random Costs
costs 100 105 datasets assigned using semi-random mechanism
gives higher costs informative attributes. ensure ACTs success due
particular cost assignment scheme, repeated experiments costs drawn
randomly uniformly given cost range cr, i.e., set 0. Figure 14 shows
results. see, ACT maintains advantage methods: dominates
along scale mc values.
4.7 Nonuniform Misclassification Costs
far, used uniform misclassification cost matrices, i.e., cost
error type identical. explained Section 3, ACT algorithm handle
complex misclassification cost matrices penalty one type error might
higher penalty another type. next experiment examines ACT
nonuniform scenario. Let FP denote penalty false positive FN penalty
false negative. 2 classes, split classes 2 equal groups
according order (or randomly order exists). assign penalty FP
misclassifying instance belongs first group FN one belongs
second group.
obtain wide view, vary ratio FP FN examine different
absolute values. Table 3 Figure 15 give average results. Table 4 lists number
t-test significant wins algorithm achieved. easy see ACT consistently
outperforms methods.
22

fiAnytime Induction Low-cost, Low-error Classifiers


8


4


2




2


4


8


= 500

C4.5
EG2
DTCM
ICET
ACT

29.2
30.1
12.4
23.3
11.9

34.2
33.0
20.3
27.0
18.5

41.3
37.2
29.8
31.5
27.2

49.9
42.5
37.7
36.3
34.5

43.6
39.3
32.5
34.2
29.1

39.0
37.5
22.9
31.8
20.4

36.3
36.8
15.8
29.2
13.8

= 5000

Table 3: Comparison C4.5, EG2, DTMC, ACT, ICET misclassification costs
nonuniform. FP denotes penalty false positive FN penalty
false negative. denotes basic mc unit.

C4.5
EG2
DTCM
ICET
ACT

27.0
30.9
13.8
21.4
12.9

31.3
35.2
23.6
25.6
19.1

39.2
43.1
38.0
32.7
28.8

53.3
57.3
57.6
45.7
41.5

44.0
47.7
42.5
37.4
31.1

39.0
42.4
29.3
32.8
22.5

36.3
39.7
20.1
29.8
14.6

FP
FN

Table 4: Comparing DTMC, ACT, ICET misclassification costs nonuniform.
F P/F N ratio, columns list number t-test significant wins
= 5%. FP denotes penalty false positive FN penalty
false negative. denotes basic mc unit.
= 500
F P/F N
0.125
0.25
0.5
1
2
4
8

DTMC vs. ACT
4
2
10
9
5
3
1

22
31
25
29
35
40
27

= 5000

ICET vs. ACT
11
7
7
5
1
0
4

52
49
42
23
47
72
72

DTMC vs. ACT
5
10
16
7
5
4
0

44
49
52
50
61
58
62

ICET vs. ACT
12
4
10
15
9
0
0

44
36
25
21
31
44
67

Interestingly, graphs slightly asymmetric. reason could
datasets, example medical ones, difficult reduce negative errors positive
ones, vice versa. similar phenomenon reported Turney (1995).
highest cost algorithms observed F P = F N because,
ratio FP FN extremely large extremely small, learner easily
build small tree whose leaves labeled class minimizes costs.
misclassification costs balanced, however, learning process becomes much
complicated.
23

fi50

60

45

55
Average % Standard Cost

Average % Standard Cost

Esmeir & Markovitch

40
35
30
25
C4.5
EG2
DTMC
ICET
ACT

20
15
10
/8

/4

/2
/
2/
Misclassification Cost FP/FN

50
45
40
35
30
25

C4.5
EG2
DTMC
ICET
ACT

20
15

4/

10
/8

8/

/4

/2
/
2/
Misclassification Cost FP/FN

4/

8/

Figure 15: Comparison C4.5, EG2, DTMC, ACT, ICET misclassification costs
nonuniform. misclassification costs represented pair (F P/F N ).
FP denotes penalty false positive FN penalty false
negative. denotes basic mc unit. figures plot average cost
function ratio FP FN, = 500 (left) = 5000
(right).

5. Related Work
addition works referred earlier paper, several related works warrant
discussion here.
Cost-sensitive trees subject many research efforts. Several works proposed learning algorithms consider different misclassification costs (Breiman, Friedman, Olshen, & Stone, 1984; Pazzani, Merz, Murphy, Ali, Hume, & Brunk, 1994; Provost
& Buchanan, 1995; Bradford, Kunz, Kohavi, Brunk, & Brodley, 1998; Domingos, 1999;
Elkan, 2001; Zadrozny, Langford, & Abe, 2003; Lachiche & Flach, 2003; Abe, Zadrozny, &
Langford, 2004; Vadera, 2005; Margineantu, 2005; Zhu, Wu, Khoshgoftaar, & Yong, 2007;
Sheng & Ling, 2007). methods, however, consider test costs hence
appropriate mainly domains test costs constraint.
Davis, Ha, Rossbach, Ramadan, Witchel (2006) presented greedy cost-sensitive
decision tree algorithm forensic classification: problem classifying irreproducible
events. setup, assume tests might used testing must
acquired hence charged classification.
One way exploit additional time searching less costly tree widen
search space. Bayer-Zubek Dietterich (2005) formulated cost-sensitive learning
problem Markov decision process (MDP), used systematic search algorithm
based AO* heuristic search procedure solve MDP. make AO* efficient,
algorithm uses two-step lookahead based heuristic. limited lookahead
informed immediate heuristics still insufficient complex domains might
cause search go astray (Esmeir & Markovitch, 2007a). algorithm shown
output better diagnostic policies several greedy methods using reasonable resources.
optimal solution, however, could always found due time memory limits.
nice property algorithm serve anytime algorithm computing
24

fiAnytime Induction Low-cost, Low-error Classifiers

best complete policy found far. anytime behavior, nevertheless, problematic
policies optimal respect train data tend overfit. result,
performance eventually start degrade.
Arnt Zilberstein (2005) tackled problem time cost sensitive classification
(TCSC). TCSC, utility labeling instance depends correctness
labeling, amount time takes. Therefore total cost function
additional component, reflects time needed measure attribute. Typically,
super-linear form: cost quick result small fairly constant,
waiting time increases, time cost grows increasing rate. problem
complicated sequence time-sensitive classification instances considered,
time spent administering tests one case adversely affect costs future
instances. Arnt Zilberstein suggest solving problems extending decision
theoretic approach introduced Bayer-Zubek Dietterich (2005). work,
assume time takes administer test incorporated cost.
future, intend extend framework support time-sensitive classification,
individual cases sequences.
Fan, Lee, Stolfo, Miller (2000) studied problem cost-sensitive intrusion detection systems (IDS). goal maximize security minimizing costs.
prediction (action) cost. Features categorized three cost levels according
amount information needed compute values. reduce cost IDS, high
cost rules considered predictions low cost rules sufficiently
accurate.
Costs involved learning phase, example acquisition
model learning. problem budgeted learning studied Lizotte, Madani,
Greiner (2003). cost associated obtaining attribute value
training example, task determine attributes test given budget.
related problem active feature-value acquisition. setup one tries reduce
cost improving accuracy identifying highly informative instances. Melville, SaarTsechansky, Provost, Mooney (2004) introduced approach instances
selected acquisition based accuracy current model confidence
prediction.
Greiner, Grove, Roth (2002) pioneers studying classifiers actively decide
tests administer. defined active classifier classifier given
partially specified instance, returns either class label strategy specifies
test performed next. Greiner et al. analyzed theoretical aspects
learning optimal active classifiers using variant probably-approximately-correct
(PAC) model. showed task learning optimal cost-sensitive active classifiers
often intractable. However, task shown achievable active classifier
allowed perform (at most) constant number tests, limit provided
learning. setup proposed taking dynamic programming approach
build trees depth d.
setup assumed charged acquiring feature values test
cases. term test strategy (Sheng, Ling, & Yang, 2005) describes process feature
values acquisition: values query order. Several test strategies
studied, including sequential, single batch multiple batch (Sheng et al., 2006),
25

fiEsmeir & Markovitch

corresponds different diagnosis policy. strategies orthogonal
work assume given decision tree.
Bilgic Getoor (2007) tackled problem feature subset selection costs
involved. objective minimize sum information acquisition cost
misclassification cost. Unlike greedy approaches compute value features one
time, used novel data structure called value information lattice (VOILA),
exploits dependencies missing features makes possible share information value computations different feature subsets possible. VIOLA shown
empirically achieve dramatic cost improvements without prohibitive computational
costs comprehensive search.

6. Conclusions
Machine learning techniques increasingly used produce wide range classifiers complex real-world applications involve nonuniform testing misclassification costs. increasing complexity applications poses real challenge
resource management learning classification. work introduced novel
framework operating complex environments. framework four major
advantages:
uses non-greedy approach build decision tree therefore able overcome
local minima problems.
evaluates entire trees search; thus, adjusted cost scheme
defined trees.
exhibits good anytime behavior allows learning speed traded classification costs. many applications willing allocate time
would allocate greedy methods. proposed framework exploit extra
resources.
sampling process easily parallelized method benefit distributed computer power.
evaluate ACT designed extensive set experiments wide range
costs. Since publicly available cost-oriented datasets, designed
parametric scheme automatically assigns costs existing datasets. experimental
results show ACT superior ICET DTMC, existing cost-sensitive algorithms
attempt minimize test costs misclassification costs simultaneously. Significance
tests found differences statistically strong. ACT exhibited good anytime
behavior: increase time allocation, cost learned models decreased.
ACT contract anytime algorithm requires sample size predetermined.
future intend convert ACT interruptible anytime algorithm adopting
IIDT general framework (Esmeir & Markovitch, 2007a). addition, plan apply
monitoring techniques (Hansen & Zilberstein, 2001) optimal scheduling ACT
examine strategies evaluating subtrees.
26

fiAnytime Induction Low-cost, Low-error Classifiers

Table 5: Characteristics datasets used

Dataset
Breast Cancer
Bupa
Car
Flare
Glass
Heart
Hepatitis
Iris
KRK
Monks-1
Monks-2
Monks-3
Multiplexer-20
Multi-XOR
Multi-AND-OR
Nursery
Pima
TAE
Tic-Tac-Toe
Titanic
Thyroid
Voting
Wine
XOR 3D
XOR-5

Instances
277
345
1728
323
214
296
154
150
28056
124+432
169+432
122+432
615
200
200
8703
768
151
958
2201
3772
232
178
200
200

Attributes
Nominal (binary) Numeric
9 (3)
0 (0)
6 (0)
10 (5)
0 (0)
8(4)
13(13)
0 (0)
6(0)
6 (2)
6 (2)
6 (2)
20 (20)
11 (11)
11 (11)
8(8)
0(0)
4(1)
9 (0)
3(2)
15(15)
16 (16)
0 (0)
0 (0)
10 (10)

0
5
0
0
9
5
6
4
0
0
0
0
0
0
0
0
8
1
0
0
5
0
13
6
0

Max attribute
domain

Classes

13
4
7
4
2
8
4
4
4
2
2
2
5
26
3
4
2
2
2

2
2
4
4
7
2
2
3
17
2
2
2
2
2
2
5
2
3
2
2
3
2
3
2
2

Acknowledgments
work partially supported funding EC-sponsored MUSCLE Network
Excellence (FP6-507752).

Appendix A. Datasets
Table 5 lists characteristics 25 datasets used. give detailed
description non-UCI datasets used experiments:
1. Multiplexer: multiplexer task used several researchers evaluating classifiers (e.g., Quinlan, 1993). instance series bits length + 2a ,
positive integer. first bits represent index remaining bits
label instance value indexed bit. experiments considered
20-Multiplexer (a = 4). dataset contains 500 randomly drawn instances.
2. Boolean XOR: Parity-like functions known problematic many learning
algorithms. However, naturally arise real-world data, Drosophila
survival concept (Page & Ray, 2003). considered XOR five variables five
additional irrelevant attributes.
27

fiEsmeir & Markovitch

3. Numeric XOR: XOR based numeric dataset used evaluate learning
algorithms (e.g., Baram, El-Yaniv, & Luz, 2003). example consists values
x coordinates. example labeled 1 product x positive,
1 otherwise. generalized domain three dimensions added irrelevant
variables make concept harder.
4. Multi-XOR / Multi-AND-OR: concepts defined 11 binary attributes.
cases target concept composed several subconcepts, first
two attributes determines considered. 10 attributes
used form subconcepts. Multi-XOR dataset, subconcept XOR,
Multi-AND-OR dataset, subconcept either OR.

References
Abe, N., Zadrozny, B., & Langford, J. (2004). iterative method multi-class costsensitive learning. Proceedings 10th ACM SIGKDD International Conference
Knowledge Discovery Data Mining (KDD-2004), Seattle, WA, USA.
Arnt, A., & Zilberstein, S. (2005). Learning policies sequential time cost sensitive
classification. Proceedings 1st international workshop Utility-based data
mining (UBDM05) held KDD05, pp. 3945, New York, NY, USA. ACM Press.
Asuncion, A., & Newman, D. (2007).
UCI machine learning repository.
University California, Irvine, School Information Computer Sciences.
http://www.ics.uci.edu/mlearn/MLRepository.html.
Baram, Y., El-Yaniv, R., & Luz, K. (2003). Online choice active learning algorithms.
Proceedings 20 International Conference Machine Learning (ICML-2003),
pp. 1926, Washington, DC, USA.
Bayer-Zubek, V., & Dietterich (2005). Integrating learning examples search
diagnostic policies. Artificial Intelligence, 24, 263303.
Bilgic, M., & Getoor, L. (2007). Voila: Efficient feature-value acquisition classification.
Proceedings 22nd National Conference Artificial Intelligence (AAAI-2007).
Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. K. (1987). Occams Razor.
Information Processing Letters, 24 (6), 377380.
Boddy, M., & Dean, T. L. (1994). Deliberation scheduling problem solving time
constrained environments. Artificial Intelligence, 67 (2), 245285.
Bradford, J., Kunz, C., Kohavi, R., Brunk, C., & Brodley, C. (1998). Pruning decision
trees misclassification costs. Proceedings 9th European Conference
Machine Learning (ECML-1998), pp. 131136.
Breiman, L., Friedman, J., Olshen, R., & Stone, C. (1984). Classification Regression
Trees. Wadsworth Brooks, Monterey, CA.
Chickering, D. M., Meek, C., & Rounthwaite, R. (2001). Efficient determination dynamic
split points decision tree. Proceedings 1st IEEE International Conference
28

fiAnytime Induction Low-cost, Low-error Classifiers

Data Mining (ICDM-2001), pp. 9198, Washington, DC, USA. IEEE Computer
Society.
Davis, J. V., Ha, J., Rossbach, C. J., Ramadan, H. E., & Witchel, E. (2006). Cost-sensitive
decision tree learning forensic classification. Proceedings 17th European
Conference Machine Learning (ECML-2006), pp. 622629, Berlin, Germany.
Demsar, J. (2006). Statistical comparisons classifiers multiple data sets. Journal
Machine Learning Research, 7, 130.
Domingos, P. (1999). Metacost: general method making classifiers cost-sensitive.
Proceedings 5th International Conference Knowledge Discovery Data
Mining (KDD1999), pp. 155164.
Elkan, C. (2001). foundations cost-sensitive learning. Proceedings 17th
International Joint Conference Artificial Intelligence (IJCAI-2001), pp. 973978,
Seattle, Washington, USA.
Esmeir, S., & Markovitch, S. (2006). decision tree learner plenty time.
Proceedings 21st National Conference Artificial Intelligence (AAAI-2006),
Boston, MA, USA.
Esmeir, S., & Markovitch, S. (2007a). Anytime learning decision trees. Journal Machine
Learning Research, 8.
Esmeir, S., & Markovitch, S. (2007b). Occams razor got sharper. Proceedings
20th International Joint Conference Artificial Intelligence (IJCAI-2007),
Hyderabad, India.
Fan, W., Lee, W., Stolfo, S. J., & Miller, M. (2000). multiple model cost-sensitive approach
intrusion detection. Proceedings 11th European Conference Machine
Learning (ECML-2000), pp. 142153, Barcelona, Catalonia, Spain.
Frank, E. (2000). Pruning Decision Trees Lists. Ph.D. thesis, Department Computer
Science, University Waikato.
Good, I. (1965). Estimation Probabilities: Essay Modern Bayesian Methods.
MIT Press, USA.
Greiner, R., Grove, A. J., & Roth, D. (2002). Learning cost-sensitive active classifiers.
Artificial Intelligence, 139 (2), 137174.
Hansen, E. A., & Zilberstein, S. (2001). Monitoring control anytime algorithms:
dynamic programming approach. Artificial Intelligence, 126 (1-2), 139157.
Hastie, T., Tibshirani, R., & Friedman, J. (2001). Elements Statistical Learning:
Data Mining, Inference, Prediction. New York: Springer-Verlag.
Hyafil, L., & Rivest, R. L. (1976). Constructing optimal binary decision trees NPcomplete. Information Processing Letters, 5 (1), 1517.
Lachiche, N., & Flach, P. (2003). Improving accuracy cost two-class multiclass probabilistic classifiers using roc curves. Proceedings 20th International
Conference Machine Learning (ICML-2003).
29

fiEsmeir & Markovitch

Lenert, L., & Soetikno, R. (1997). Automated computer interviews elicit utilities: Potential applications treatment deep venous thrombosis. American Medical
Informatics Association, 4 (1), 4956.
Ling, C. X., Yang, Q., Wang, J., & Zhang, S. (2004). Decision trees minimal costs.
Proceedings 21st International Conference Machine Learning (ICML-2004).
Lizotte, D. J., Madani, O., & Greiner, R. (2003). Budgeted learning naive bayes classifiers.
Proceedings 19th Conference Uncertainty Artificial Intelligence (UAI2003), Acapulco, Mexico.
Margineantu, D. (2005). Active cost-sensitive learning. Proceedings 19th International Joint Conference Artificial Intelligence (IJCAI-2005), Edinburgh, Scotland.
Melville, P., Saar-Tsechansky, M., Provost, F., & Mooney, R. J. (2004). Active feature acquisition classifier induction. Proceedings 4th IEEE International Conference
Data Mining (ICDM-2004), pp. 483486, Brighton, UK.
Norton, S. W. (1989). Generating better decision trees. Sridharan, N. S. (Ed.), Proceedings Eleventh International Joint Conference Artificial Intelligence, pp.
800805, Detroit, Michigan, USA.
Nunez, M. (1991). use background knowledge decision tree induction. Machine
Learning, 6, 231250.
Page, D., & Ray, S. (2003). Skewing: efficient alternative lookahead decision
tree induction. Proceedings 18th International Joint Conference Artificial
Intelligence (IJCAI-2003), Acapulco, Mexico.
Pazzani, M., Merz, C., Murphy, P., Ali, K., Hume, T., & Brunk, C. (1994). Reducing
misclassification costs: knowledge intensive approaches learning noisy data.
Proceedings 11th International Conference Machine Learning (ICML1994).
Provost, F., & Buchanan, B. (1995). Inductive policy: pragmatics bias selection.
Machine Learning, 20 (1-2), 3561.
Qin, Z., Zhang, S., & Zhang, C. (2004). Cost-sensitive decision trees multiple cost
scales. Lecture Notes Computer Scienc, AI 2004: Advances Artificial Intelligence,
Volume 3339/2004, 380390.
Quinlan, J. R. (1986). Induction decision trees. Machine Learning, 1, 81106.
Quinlan, J. R. (1993). C4.5: Programs Machine Learning. Morgan Kaufmann, San
Mateo, CA.
Sheng, S., Ling, C. X., Ni, A., & Zhang, S. (2006). Cost-sensitive test strategies.
Proceedings 21st National Conference Artificial Intelligence (AAAI-2006),
Boston, MA, USA.
Sheng, S., Ling, C. X., & Yang, Q. (2005). Simple test strategies cost-sensitive decision
trees. Proceedings 9th European Conference Machine Learning (ECML2005), pp. 365376, Porto, Portugal.
30

fiAnytime Induction Low-cost, Low-error Classifiers

Sheng, V. S., & Ling, C. X. (2007). Roulette sampling cost-sensitive learning.
Proceedings 18th European Conference Machine Learning (ECML-2007),
pp. 724731, Warsaw, Poland.
Tan, M., & Schlimmer, J. C. (1989). Cost-sensitive concept learning sensor use approach recognition. Proceedings 6th International Workshop Machine
Learning, pp. 392395, Ithaca, NY.
Turney, P. (2000). Types cost inductive concept learning. Proceedings
Workshop Cost-Sensitive Learning held 17th International Conference
Machine Learning (ICML-2000), Stanford, CA.
Turney, P. D. (1995). Cost-sensitive classification: Empirical evaluation hybrid genetic
decision tree induction algorithm. Journal Artificial Intelligence Research, 2, 369
409.
Vadera, S. (2005). Inducing cost-sensitive non-linear decision trees. Technical report 03-052005, School Computing, Science Engineering, University Salford.
Zadrozny, B., Langford, J., & Abe, N. (2003). Cost-sensitive learning cost-proportionate
example weighting. Proceedings 3rd IEEE International Conference Data
Mining (ICDM-2003), Melbourne, Florida, USA.
Zhu, X., Wu, X., Khoshgoftaar, T., & Yong, S. (2007). empirical study noise
impact cost-sensitive learning. Proceedings 20th International Joint Conference Artificial Intelligence (IJCAI-2007), Hyderabad, India.

31


