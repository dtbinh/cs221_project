Journal Artificial Intelligence Research 33 (2008) 551574

Submitted 09/08; published 12/08

Learning Reach Agreement Continuous Ultimatum Game
Steven de Jong
Simon Uyttendaele

STEVEN . DEJONG @ MICC . UNIMAAS . NL

MICC, Maastricht University
P.O. Box 616, 6200 MD Maastricht, Netherlands

Karl Tuyls

K . P. TUYLS @ TUE . NL

Eindhoven University Technology
P.O. Box 513, 5600 MB Eindhoven, Netherlands

Abstract
well-known acting individually rational manner, according principles classical game theory, may lead sub-optimal solutions class problems named social dilemmas.
contrast, humans generally much difficulty social dilemmas, able
balance personal benefit group benefit. agents multi-agent systems regularly confronted social dilemmas, instance tasks resource allocation, agents may
benefit inclusion mechanisms thought facilitate human fairness. Although many
mechanisms already implemented multi-agent systems context, application usually limited rather abstract social dilemmas discrete set available strategies
(usually two). Given many real-world examples social dilemmas actually continuous
nature, extend previous work general dilemmas, agents operate continuous strategy space. social dilemma study well-known Ultimatum Game,
optimal solution achieved agents agree common strategy. investigate whether
scale-free interaction network facilitates agents reach agreement, especially presence
fixed-strategy agents represent desired (e.g. human) outcome. Moreover, study influence rewiring interaction network. agents equipped continuous-action
learning automata play large number random pairwise games order establish common strategy. experiments, may conclude results obtained discrete-strategy
games generalized continuous-strategy games certain extent: scale-free interaction network structure allows agents achieve agreement common strategy, rewiring
interaction network greatly enhances agents ability reach agreement. However,
becomes clear alternative mechanisms, reputation volunteering, many
subtleties involved convincing beneficial effects continuous case.

1. Introduction
Sharing limited resources others common challenge individuals human societies
well agents multi-agent systems (Chevaleyre et al., 2006). Often, conflict
interest personal benefit group (or social) benefit. conflict prominently
present class problems named social dilemmas, individuals need consider
personal benefit, effects choices others, failure may
lead sub-optimal solutions. dilemmas, classical game theory, assumes players
agents completely individually rational strategic circumstances, seems limited value
(Gintis, 2001; Maynard-Smith, 1982), individually rational players socially conditioned.
Humans hand generally show remarkable ability address social dilemmas, due
c
2008
AI Access Foundation. rights reserved.

fiD E J ONG , U YTTENDAELE & UYLS

tendency consider concepts fairness addition personal benefit (see, e.g.
Dannenberg et al., 2007; Fehr & Schmidt, 1999; Gintis, 2001; Oosterbeek et al., 2004).
prime example social dilemma modeled well-known Ultimatum Game (Gueth
et al., 1982).1 game, two agents bargain division amount R, obtained
outsider. first agent proposes offer r2 second agent (e.g. receive $4
$10). second agent accepts, agent gets share (i.e. first agent receives R r2 ,
second receives r2 ); however, second agent rejects, agents left nothing.
individually rational first agent would offer smallest amount possible, knowing second
agent choose obtaining amount accepting, nothing rejecting. Thus,
accepting smallest amount possible individually rational response. contrast, human
players game hardly (if ever) offer less 20%, offer occurs,
likely accepted (Bearden, 2001; Oosterbeek et al., 2004). Thus, individually rational player
plays proposer human player probably gain money.
Researchers proposed various mechanisms may responsible emergence
fair strategies human populations playing social dilemmas, well resistance
strategies invasion individually rational strategies (see, e.g. Fehr & Schmidt, 1999; Gintis, 2001; Nowak et al., 2000; Santos et al., 2006a). Often, mechanisms implemented multi-agent systems validation purposes, i.e. agents shown prefer
fair strategies individually rational ones, makes plausible humans actually
affected underlying mechanisms. However, argue multi-agent systems driven fairness mechanisms may used validate mechanisms, allow agents act
fair way real-world applications. Given agents often face tasks resource sharing
allocation (if explicitly, implicitly, e.g. sharing limited computational resources),
tasks regularly contain elements social dilemmas, important enable agents
act based individual rationality, based fairness (Chevaleyre et al., 2006).
Unfortunately, existing work usually introduces number abstractions allow resulting multi-agent systems applied realistic problems resource allocation.
prominently, work focused social dilemmas discrete strategy sets (usually limited
two).2 abstraction simplifies dilemmas hand reflect potential realworld nature, since many dilemmas, especially related real-world resource allocation,
continuum strategies (i.e. continuous strategy space) rather discrete set
pure strategies. Moreover, social dilemmas continuous strategy spaces, qualifications
cooperation defection, often used discrete social dilemmas, actually relative: certain strategy may seen cooperative (i.e. desirable) certain context, whereas
may either defective simply naive another one. clear dilemma may far
complicated continuous strategy spaces, agents may need use different way
determining whether behavior desirable.
1. analogy Ultimatum Game social dilemmas, Public Goods Game, shown
full mathematical rigor (Sigmund et al., 2001). De Jong & Tuyls (2008) report preliminary results applying
methodology described paper Public Goods Game.
2. Theoretical work field evolutionary game theory occasionally limited discrete strategy sets. Worth
mentioning work Peters (2000), introduces theoretical extension evolutionary stable
strategy concept (ESS) continuous strategy spaces, i.e. extended stability calculus. extension provides
theoretical solution concept clarify egalitarian outcomes. However, concept shed light
learning agents possibly achieve fair outcomes social dilemmas. work therefore complementary
work, aims mechanisms enabling agents find fair outcomes.

552

fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AME

paper, generalize existing work achieving agreement, cooperation fairness
social dilemmas continuous strategy spaces, aim presenting methodology
allows agents reach satisfactory outcomes dilemmas, well real-world problems
containing elements dilemmas. apply proposed methodology Ultimatum
Game, example social dilemma. game, population agents needs reach agreement (i.e. converge common strategy) order obtain satisfactory payoff. agreement
specifies populations cooperative, desirable strategy. precise nature agreement
may vary.3 population agents, strategy agreed upon sufficient number
agents successful, dictate culture, desirable strategy, population.
wish agents learn human strategy, may introduce number simulated human
agents, i.e. agents play according desirable strategy. learning agents
able imitate strategy, even already reached agreement different, possibly
relatively defective, strategy.
remainder paper structured follows. First, 2, give brief overview
related work paper aims continue. Next, 3, discuss methodology, aimed
establishing agreement large populations learning agents. 4, present experiments
results. 5, outline number alternative approaches proposed dilemmas
discrete strategy sets, fail impress dilemma continuous strategy space. discuss
case. Finally, conclude paper 6.

2. Related Work
work basically builds upon two tracks existing work. give overview
tracks indicate related current work. extensive discussion,
refer previous work (De Jong et al., 2008b).
2.1 Learning Fairness Bargaining
De Jong et al. (2008a) investigated behavior agents playing Ultimatum Game
Nash Bargaining Game continuous action learning automata. games, agents
interacting time. Ultimatum Game, required extension two
players agents, one other, demanded portion reward hand. last
player received left. Homo Egualis utility function, developed Fehr & Schmidt
(1999) Gintis (2001), used represent desired outcome, i.e. required minimal amount
every agent wished obtain. 100 agents able successfully find maintain agreements games. addition, observed solutions agreed upon corresponded
solutions agreed upon humans, reported literature. work, similarly use continuous action learning automata learn agreement Ultimatum Game. However, multi-agent
system, organized network structure, efficiently populated much larger number
agents (e.g. thousands). contrast previous work, agents play pairwise games. Moreover,
3. Note analogy humans, cultural background one primary influences constitutes fair,
cooperative, desirable strategy. Although general tendency deviate pure individual rationality
favor socially-aware strategies, exact implications vary greatly (Henrich et al., 2004; Oosterbeek et al.,
2004; Roth et al., 1991). Ultimatum Game instance, actual amounts offered minimally accepted
vary 10% much 70%, depending various factors, amount bargain (Cameron,
1999; De Jong et al., 2008c; Slonim & Roth, 1998; Sonnegard, 1996), culture (Henrich et al., 2004). Cultural
differences may persist groups agents, within groups (Axelrod, 1997).

553

fiD E J ONG , U YTTENDAELE & UYLS

use Homo Egualis utility function. Instead, desired, human-inspired outcome
offered Homo Egualis utility function replaced (potentially) including agents
always play according certain fixed strategy (i.e. simulated human players).
2.2 Network Topology
Santos et al. (2006b) investigated impact scale-free networks resulting strategies social
dilemmas. scale-free network used order randomly determine two agents (neighbors) would play together various social dilemmas, Prisoners Dilemma
Snowdrift Game. agents limited two strategies, i.e., cooperate defect,
initially equally probable. observed that, due scale-free network, defectors could
spread entire network games, network structures. authors
identified topology network contributed observed maintained cooperation.
subsequent research, Santos et al. (2006a) introduced rewiring network played many different social dilemmas, two strategies per agent. concluded ease
(measure individuals inertia readjust ties) rewiring increasing rate
cooperators efficiently wipe defectors. contrast work Santos et al. (2006a,b),
used discrete strategy set, work uses continuous strategy space. requires another view
fairness, cooperation agreement, departing traditional view fairness achieved
driving agents (manually labeled) cooperative strategies. social dilemmas
Ultimatum Game, strategy agents may agree leads satisfactory outcomes.

3. Methodology
discussing methodology detail, first outline basic setting. continue
explaining continuous-action learning automata, central methodology. Next,
discuss structure topology networks interaction use. discuss
agent types initial strategies agents. elaborate provide additional
possibility rewiring connections agents. Finally, explain experimental setup.
3.1 Basic Setting
study large group adaptive agents, driven continuous action learning automata, playing
Ultimatum Game pairwise interactions. Pairs chosen according (scale-free) network
interaction. Every agent randomly assigned role proposer responder Ultimatum
Game. Agents start different strategies. good performance, need converge
agreement playing many pairwise games, i.e. need learn common strategy.
agents may fixed strategies; agents represent external strategy adaptive
agents need converge to, instance preference dictated humans. addition
basic setting, study influence adding option agents rewire network
interaction response agent behaved defecting manner.
3.2 Continuous Action Learning Automata
Continuous Action Learning Automata (CALA; Thathachar & Sastry, 2004) learning automata
developed problems continuous action spaces. CALA essentially function optimizers:
every action continuous, one-dimensional action space A, receive feedback
554

fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AME

(x) goal optimize feedback. CALA proven convergence (local) optima,
given feedback function (x) sufficiently smooth. advantage CALA many
reinforcement learning techniques (see, e.g. Sutton & Barto, 1998), necessary
discretize continuous action spaces, actions simply real numbers.
3.2.1 H OW CALA W ORK
Essentially, CALA maintain Gaussian distribution actions pulled. contrast
standard learning automata, CALA require feedback two actions, action corresponding
mean Gaussian distribution, action corresponding sample x, taken
distribution. actions lead feedback () (x), respectively, turn,
feedback used update probability distributions . precisely, update formula
CALA written as:
x
= + (x)()
()
()


2
(x)()
x
= + ()
1 K ( L )
()

(1)

equation, represents learning rate; K represents large constant driving .
variance kept threshold L keep calculations tractable even case convergence.4
implemented using function:
() = max (, L )

(2)

intuition behind update formula quite straightforward (De Jong et al., 2008a). Using
update formula, CALA rather quickly converge (local) optimum. multiple (e.g. n) learning
automata, every automaton receives feedback respect joint actions, respectively ()
(x), = 1 , . . . , n x = x1 , . . . , xn . case, still convergence
(local) optimum (Thathachar & Sastry, 2004).
3.2.2 ODIFICATIONS CALA P URPOSES
outlined above, use CALA enable agents learn sensible proposer responder strategy Ultimatum Game. playing Ultimatum Game, two agents may agree
one two joint actions (i.e. obtain one high one low feedback),
may even disagree (i.e. obtain two low feedbacks). situations need
additional attention, occurrence prevents CALA converging correct solutions.
address situations, propose two domain-specific modifications update formula
CALA (De Jong et al., 2008a).5
First, case joint actions yield feedback 0, CALA unable draw effective
conclusions, even though may tried ineffective strategy thus actually
4. used following settings initial experiments: = 0.02, K = 0.001 L = 107 .
precise settings L decisive influence outcomes, although values may lead slower
convergence. K chosen large, (rather vaguely) implied Thathachar & Sastry (2004), decreases
fast, i.e. usually CALA stop exploring sufficient solution found.
5. Note modifications uncommon literature; see, e.g. work Selten & Stoecker (1986)
learning direction theory. Grosskopf (2003) successfully applied directional learning setting Ultimatum
Game, focusing responder competition (which addressed paper).

555

fiD E J ONG , U YTTENDAELE & UYLS

learn. order counter problem, introduce driving force, allows agents update
strategy even feedback received 0. driving force defined as:

proposers: () = x
iff () = (x) = 0
(3)
responders: () = x
effect modification, call zero-feedback avoidance (ZFA), agent
playing proposer learn offer more, agent playing responder accept
lower expectation. roles, lead probable agreement.
Second, one joint action yields agreement, feedback 0, CALA may adapt
strategies drastically favor first joint action fact, shifts values greater
109 observed (De Jong & Tuyls, 2008; De Jong et al., 2008a). tackle problem,
restrict difference possible two feedbacks CALA receive every
iteration. precisely, empirically set:


() (x)

fi1
(4)


()
Thus, large difference feedback -action x-action, preserve
direction indicated feedback, prevent automaton jump far direction.
call modification strategy update limitation (SUL).
3.3 Network Interaction
scale-free network (Barabasi & Albert, 1999) network degree distribution follows
power law. precisely, fraction P (k) nodes network k connections
nodes goes large values k P (k) k . value constant typically
range 2 < < 3. Scale-free networks noteworthy many empirically observed networks
appear scale-free, including world wide web, protein networks, citation networks,
social networks. mechanism preferential attachment proposed explain power law
degree distributions networks. Preferential attachment implies nodes prefer attaching
nodes already large number neighbors, nodes small
number neighbors.
Previous research indicated scale-free networks contribute emergence cooperation (Santos et al., 2006b). wish determine whether phenomenon still occurs
continuous strategy spaces therefore use scale-free topology interaction network, using Barabasi-Albert model. precisely, probability pi newly introduced node
connected existing node degree ki equal to:
ki
pi = P
(5)
j kj
construct network, two first nodes linked other,
nodes introduced sequentially connected one existing nodes, using pi .
way, newly introduced node probably connect heavily linked hub one
connections. simulations, connect every new node one, two three
existing ones (uniform probabilities). yields networks interaction realistic
acyclic ones obtained always connecting new nodes one existing node. example,
network modeling friendship network, avoiding cycles means assuming friends
certain person never friends other.
556

fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AME

3.4 Agent Types Strategies
order study agreement concerning common strategy emerges, need make
agents learn reach common strategy, starting situation absent (i.e.
agents different strategies). Moreover, need study whether common strategy
established example agents, whether robust agents use different,
potentially relatively defective strategy.
3.4.1 WO YPES AGENTS
introduce two types agents, i.e. dynamic strategy (DS) agents fixed strategy (FS) agents.
DS agents learning agents. start certain predefined strategy allowed
adapt strategy constantly, according learning mechanism learning automaton.
Basically, agents similar used earlier work (De Jong et al., 2008a). FS agents
(optional) good examples: model example strategy needs learned
(other) agents system, therefore refuse adapt strategy.
3.4.2 NE WO CALA PER AGENT ?
outlined above, agent needs able perform two different roles Ultimatum Game, i.e. playing proposer well playing responder. words,
agent one two distinct states, state requires learn different strategy. CALA
stateless learners, agent therefore would require two CALA. Nonetheless, remainder
paper, equip every DS agent one CALA, representing agents proposer
strategy well responder strategy.
choice one CALA motivated two observations, i.e. (1) human behavior, (2)
initial experiments. First, human strategies often consistent, implying generally
accept offers, reject offers lower (Oosterbeek et al., 2004), even high
amounts stake (De Jong et al., 2008c; Sonnegard, 1996). Second, set initial experiments,
observed agents using two CALA generally converge one single strategy anyway.
illustration, three learning curves obtained fully connected network three agents playing
Ultimatum Game displayed Figure 1. clearly visible agents proposer strategies
(bold lines) strongly attracted agents responder strategies (thin lines), especially
lowest responder strategies. presence FS agent offers 4.5 accepts
least 1, first strategy immediately ignored favor (lower) second one. DS
agents, strategies attracted lowest responder strategy present.6
future work, study observation perspective evolutionary game theory
replicator equations (Gintis, 2001; Maynard-Smith & Price, 1973). current paper,
use observation justify abstraction, i.e. limit complexity agents equipping
one CALA. CALA represents agents proposer strategy well
responder strategy. updated agent plays proposer well plays
responder, according CALA update formula presented 3.2.1 modifications
presented 3.2.2. Thus, agents single CALA receive twice much feedback two separate
CALA would. abstraction therefore increases efficiency learning process.
6. agents quickly adapt strategies downward upward (Figure 1). Therefore, multiple (e.g. 10)
DS agents learning (i.e. without FS agents), strategy usually converges 0. due artifact
learning process; two CALA trying learn others current strategy tend driven downward.

557

fi5

5

4.5

4.5

4

4

3.5

3.5

3

Strategy

Strategy

E J ONG , U YTTENDAELE & UYLS

2.5
2
1.5

2
1.5

1

1

0.5

0.5

0

0
500 1000 1500 2000 2500 3000 3500 4000 4500
Iteration

500 1000 1500 2000 2500 3000 3500 4000 4500
Iteration

10

(top-left) Two DS agents, one starting offering accepting 4.5, one starting offering accepting 0.01, learn
play optimally FS agent offering
4.5 accepting 1. DS agents rather quickly learn
offer accept 1.

9
8
7
Strategy

3
2.5

6

(top-right) Two DS agents, starting offering 4.5
accepting 1, learn play optimally
FS agent offering 4.5 accepting 1. DS
agents learn offer accept 1.

5
4
3

(bottom-left) Three DS agents, starting different initial
strategies (i.e. offering 9, 4.5, 1, accepting 3, 2
1, respectively), quickly learn single, similar strategy.

2
1
0
500 1000 1500 2000 2500 3000 3500 4000 4500
Iteration

Figure 1: Evolving strategies fully connected network three agents. Proposal strategies
indicated bold line, response strategies indicated thin line. Agents
converge situation two initial strategies become similar.

3.4.3 AGENTS TRATEGIES
simulations, use two types DS agents one type FS agents. precisely,
DSr agents learning agents start rational solution offering X N (0.01, 1) (and
accepting amount more). DSh agents start human, fair solution, i.e.
offering X N (4.5, 1) (and accepting amount more). Since FS agents
examples desired solution, equip fair, human-inspired solution see whether
agents able adapt solution. FS agents always offer 4.5, accept
offer 4.5 more. agents limited strategies taken continuous interval
c = [0, 10], 10 chosen upper bound (instead common 1)
common amount money needs shared Ultimatum Game. agents strategy
falls outside interval c, round strategy nearest value within interval.
3.5 Rewiring
Agents play together based connections interaction network. Thus, order avoid
playing certain undesirable neighbor j, agent may decide break connection
558

fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AME

j create new link random neighbor j (Santos et al., 2006a).7 rewiring,
use heuristic proposed Santos et al.: agents want disconnect (relative)
defectors, prefer play relative cooperators. Thus, probability agent unwires
agent j, calculated as:
si sj
pr =
(6)
C
Here, si sj agents current strategies (more precisely, agent responder strategy
agent js proposer strategy), C amount stake Ultimatum Game, i.e. 10. Even
agents determine want unwire probability, may still allowed
to, breaks last link one them. unwiring takes place, agent creates new wire
random neighbor agent j.
3.6 Experimental Setup
Using aforementioned types agents, need determine whether proposed methodology
possesses traits would see. population said established
successful agreement manages reach common strategy incorporates preferences
good examples, time discouraging agents try exploit dominant
strategy. Thus, population consisting DS agents, strategy shared (or
all) agents leads good performance, since agents agree games, yielding average payoff
5 per game per agent architecture able find common strategy.
using DS well FS agents, FS agents dictate strategy DS agents converge
to, regardless whether start DSh DSr agents.
order measure whether agents achieved satisfactory outcome, study four quantities
related learning process final outcome, viz. (1) point convergence, (2)
learned strategy, (3) performance (4) resulting network structure. briefly explain
four quantities below. general, remark every simulation lasts 3, 000 iterations
per agent, i.e. 3, 000n iterations n agents. repeat every simulation 50 times obtain reliable
estimates quantities interest.
3.6.1 P OINT C ONVERGENCE
important quantity concerning agents learning process point convergence,
which, present, tells us many games agents needed play order establish agreement. determine point convergence, calculate save average population strategy
avg(t) pairwise game (i.e. iteration learning process). iterations,
obtain ordered set averages, i.e. {avg(1), . . . , avg(T )}. Initially, average population
strategy changes time, agents learning. certain point time t, agents stop
learning, result, average population strategy avg(t) change much anymore.
estimate point t, i.e. point convergence, find lowest standard
deviation subset {avg(t), . . . , avg(T )} 103 . Subsequently, report number
games per agent played iteration t, i.e. nt . experiments, every simulation repeated
7. Note may choose allow agent create new connection specific agents instead
random neighbors neighbor j. However, especially combination reputation (see 5.1), allows
(relative) defectors quickly identify (relative) cooperators, may connect
attempt exploit. Preliminary experiments shown behavior leads interaction network losing
scale-freeness, may seriously impair emergence agreement.

559

fiD E J ONG , U YTTENDAELE & UYLS

Avg
Std
Conv

0

0

00
45

0
00

00
40

35

00

0

0
30

0

00
25

0
00

00
20

0
15

00

00
50

Iteration

10

Strategy

10
9
8
7
6
5
4
3
2
1
0
0

0
00

0

Avg
Std
Conv

Population strategy

45

0
00

00
40

35

00

0

0
30

0

00
25

0
00

00
20

00

00

0
15

10

50

0

Strategy

Population strategy
10
9
8
7
6
5
4
3
2
1
0

Iteration

Figure 2: Two examples convergence point single run. graphs, display
average strategy population (bold line) well standard deviation
average (thin line). dotted vertical line denotes convergence point, found
analysis detailed text.

50 times, resulting 50 convergence points. use box plot visualize distribution
50 convergence points.8
example, Figure 2 (left), see 17 FS agents, 17 DSh agents 16 DSr agents
converge agreement, using rewiring. first 50, 000 games shown. addition
bold line denoting average population strategy, plot thinner line, denoting standard
deviation average. Using method outlined above, point convergence determined
around 27, 500 games, i.e. approximately 550 games per agent necessary. Figure
2 (right), show similar results 10 FS agents 40 DSr agents, using rewiring.
Here, point convergence around 34, 000 games, i.e. approximately 680 games per agent
necessary, means learning reach agreement difficult.
3.6.2 L EARNED TRATEGY
established iteration agents converged, state average
learned strategy precisely avg(t). repeat every simulation 50 times obtain reliable estimate average. again, results, use box plot visualize distribution
average learned strategy.
3.6.3 P ERFORMANCE
measure performance, first allow agents learn playing 3, 000 Ultimatum Games
each. Then, fix strategies DS agents. let every agent play proposer
neighbors (one one), count number games successful.9 divide
8. box plots, report average instead median, average informative quantity, e.g.
comparing results existing work. may result box plots mid point located outside box.
9. Note CALA update formula prevents agents converging exact strategy, standard deviation
CALAs Gaussian kept artificially strictly positive. Therefore, noise strategies agents
converged to. counter noise measuring performance, set responders strategies 99%
actual strategies. Thus, agent strategy 4 propose 4 accept offer 3.96 more.

560

fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AME

number total number games played (i.e. twice number edges interaction
network). resulting number denotes performance, lies 0 (for utterly catastrophic) 1 (for complete agreement). Human players Ultimatum Game typically achieve
performance 0.80.9 (Fehr & Schmidt, 1999; Oosterbeek et al., 2004). again, 50
repetitions lead 50 measures performance, displayed box plot results.
3.6.4 R ESULTING N ETWORK TRUCTURE
Since network interaction may rewired agents satisfied neighbors, interested network structure resulting agents learning processes.
examine network structure looking degree distribution nodes network (i.e.
number neighbors agents). 50 repeated simulations, may draw single box
plot expressing average degree distribution.

4. Experiments Results
present experiments results two subsections. First, study setup without rewiring
setup rewiring, varying number agents, keeping proportion DSh, DSr
FS agents constant equal (i.e. 33% type agent). Second, study
two setups various population sizes, time varying proportion FS agents,
remainder population half DSh half DSr. general, remark every experiment
reports results averaged 50 simulations. every simulation, allow agents
play 3, 000n random games, n denotes number agents (i.e. population size).
4.1 Varying Population Size
many multi-agent systems, increasing number agents (i.e. population size) causes
difficulties. Many mechanisms work relatively low number agents stop working
well high number agents, instance due computational complexity undesired
emergent properties. According previous research, issue scalability applies
task learning social dilemmas. Indeed, previous research using evolutionary algorithms games
discrete strategy sets mentions number games needed converge agreement
(i.e. cooperation) may prohibitively large (Santos et al., 2006a).10
Since agents learning continuous strategy spaces, may expect scalability issue
well. determine whether proposed methodology issue, vary population
size 10 10, 000 (with steps between), keeping proportion FS,
DSh DSr agents constant one-third each. study setup without rewiring well
setup rewiring, determine (1) point convergence, i.e. number games per agent
needed reach convergence; (2) average learned strategy agents converged to; (3) final
performance system; finally (4) resulting network structure. Especially first
third quantities give indication scalability methodology.
10. order limit time taken learning, Santos et al. (2006a) terminate learning process 108 iterations,
using 103 agents, leading average (more than) 105 games per agent available. Still,
high number games per agent, report agents occasionally converge.

561

fiD E J ONG , U YTTENDAELE & UYLS

W ITH REWIRING
3000

2500

2500
Games per agent

Games per agent

N REWIRING
3000

2000
1500
1000

2000
1500
1000
500

500

0

0
10

50

100 200 500
Number agents

10

1000 10000

50

100 200 500 1000 10000
Number agents

Figure 3: Games per agent convergence; without rewiring (left) rewiring (right).

4.1.1 P OINT C ONVERGENCE
setup without rewiring (Figure 3, left) tends require games per agent total number
agents increases. certain point, i.e. around population size 200 agents, tendency
stops, mainly average number games per agent approaches maximum, i.e. 3, 000
games per agent. setup rewiring (same figure, right) convincingly outperforms one without
rewiring, increasing population size hardly affects number games per agent required
reach convergence. Independent population size, setup requires approximately 500
games per agent converge. Note difference previous research (i.e. Santos et al., 2006a),
reports requiring 105 games per agent (or more).
4.1.2 L EARNED TRATEGY
setup without rewiring (Figure 4, left) average converges strategy offering well
accepting around 3, 4.5 would required, 33% FS agents present population
play strategy (i.e. 66% DS agents average strategy 2). increasing
population size, average strategy affected; however, becomes
certainty established. again, setup rewiring (same figure, right) shows convincingly
better results. Independent population size, learning agents converge desired
strategy, i.e. 4.5.
4.1.3 P ERFORMANCE
setup without rewiring (Figure 5, left), already saw average learned strategy
DS agents good. Performance seriously affected; around 60%, indicates
DS agents ever agree FS agents. However, average performance influenced
population size. learned strategy, performance around 60% becomes
certainly established. expected, setup rewiring (same figure, right) shows much
satisfying results, i.e. generally 80% agreement. results actually positively affected
population size, average performance increases increasing population.
562

fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AME

N REWIRING

W ITH REWIRING
5
Average converged strategy

Average converged strategy

5
4
3
2
1

4
3
2
1
0

0
10

50

100
200
500
Number agents

1000

10

10000

50

100 200 500
Number agents

1000 10000

Figure 4: Average learned strategy; without rewiring (left) rewiring (right).

W ITH REWIRING

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

Performance convergence

Performance convergence

N REWIRING

10

50

100
200
500
Number agents

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
10

1000 10000

50

100 200 500
Number agents

1000 10000

Figure 5: Final performance; without rewiring (left) rewiring (right).

4.1.4 R ESULTING N ETWORK TRUCTURE
look network structure resulting learning reach agreement, determine whether
structure influenced population size. Obviously, setup without rewiring (Figure 6,
left) display influence here, network static. setup rewiring (same
figure, right) shows interesting tendency. average degree resulting network stays low,
maximum degree increases increasing population size. Clearly, population
size increases, hubs scale-free network receive preferential attachment,
correspondingly, less densely connected nodes become even less densely connected.
examine number times agents actually rewire, find number generally lies
1, 000, i.e. low percentage total number games played actually made agents
rewire random neighbor undesired proposer.
563

fiD E J ONG , U YTTENDAELE & UYLS

W ITH REWIRING
150

120

120

Degree distribution

Degree distribution

N REWIRING
150

90
60
30

90
60
30
0

0
10

50

100 200 500
Number agents

10

1000 10000

50

100 200 500
Number agents

1000 10000

Figure 6: Resulting network structure; without rewiring (left) rewiring (right).
4.1.5 N C ONCLUSION
conclusion subsection, may state proposed methodology suffering
severe scalability issues. setup include rewiring clearly outperformed one
include rewiring, neither setup without rewiring, setup rewiring, suffer
severely increasing number agents.
4.2 Varying Proportion Good Examples (FS Agents)
section, investigate behavior proposed methodology proportion
good examples population (i.e. FS agents strategy 4.5) varied. remainder
population consists DSh DSr agents equal proportions. experimented
number population sizes, ranging 50 500.
Since results population size rather similar, restrict graphically
reporting analyzing results experiments 100 agents remainder
section. selection remaining results given Table 1. Specifically, setup without rewiring setup rewiring, report population size (Pop), percentage FS
agents used (%FS), average number games per agent needed converge (Games), average learned strategy (Strat), average performance (Perf), finally, maximum number
connections single agent agents network (Netw). discuss
below, results reported Table 1 population sizes 100 highly similar
population size 100 agents.
4.2.1 P OINT C ONVERGENCE
setup without rewiring (Figure 7, left) requires games per agent converge,
proportion FS agents reaches around 30%. Then, required number games decreases
again, although great deal uncertainty. Introducing rewiring (same figure, right) yields
much better results. number games required per agent hardly exceeds 700, number
decreases steadily increasing proportion population FS agent.
564

fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AME

N REWIRING
Pop

% FS

50

W ITH REWIRING

Games

Strat

Perf

Netw

Pop

% FS

0
30
50
80

663.80
2,588.50
1,800.02
259.86

0.01
2.87
4.12
4.47

0.63
0.59
0.70
0.87

15
15
16
15

50

0
30
50
80

200

0
30
50
80

671.30
2,796.85
1,354.80
288.35

0.01
2.64
4.17
4.47

0.63
0.57
0.70
0.88

18
17
18
18

200

500

0
30
50
80

662.50
2,793.55
1,237.75
264.60

0.01
2.85
4.18
4.47

0.64
0.59
0.69
0.89

20
20
21
21

500

Games

Strat

Perf

Netw

639.38
528.52
485.60
356.34

0.01
4.45
4.47
4.49

0.63
0.81
0.89
0.96

22
38
29
23

0
30
50
80

743.00
540.40
493.40
382.20

0.01
4.45
4.47
4.49

0.62
0.87
0.91
0.97

20
52
28
24

0
30
50
80

650.20
549.95
498.00
380.91

0.01
4.45
4.47
4.49

0.65
0.87
0.92
0.97

60
100
55
35

Table 1: Summary results experiments proportion FS agents varied.
details additional results, see main text.
W ITH REWIRING

3000

3000

2500

2500
Games per agent

Games per agent

N REWIRING

2000
1500
1000

2000
1500
1000

500

500

0

0
0

0 10 20 30 40 50 60 70 80 90 100
Percentage FS agents

10 20 30 40 50 60 70 80 90 100
Percentage FS agents

Figure 7: Games per agent convergence; without rewiring (left) rewiring (right).

4.2.2 L EARNED TRATEGY
Interestingly, population consisting DS agents tends converge offering accepting
lowest amount possible, setup use rewiring (Figure 8, left), well
setup (same figure, right). explained 3, DS agents tend adapt
strategies downward easily upward. Thus, two DS agents approximately
strategy, may slowly pull others strategy downward. many DS agents,
probability happens increases. Adding FS agents population results different
behavior two setups. setup without rewiring difficulties moving away lowest
amount possible; sufficient number FS agents (i.e. 30% population)
average learned strategy reflect DS agents move towards strategy dictated FS
agents. rewiring, results convincingly better; even 10% FS agents, DS agents
average converge towards offering accepting amount dictated agents, i.e. 4.5.
565

fiD E J ONG , U YTTENDAELE & UYLS

N REWIRING

W ITH REWIRING
5
Average converged strategy

Average converged strategy

5
4
3
2
1

4
3
2
1
0

0
0

10

20

30 40 50 60 70
Percentage FS agents

80

0

90 100

10 20 30 40 50 60 70 80 90 100
Percentage FS agents

Figure 8: Average learned strategy; without rewiring (left) rewiring (right).
W ITH REWIRING

Performance convergence

Performance convergence

N REWIRING
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0

10 20 30 40 50 60 70 80 90 100
Percentage FS agents

10 20 30 40 50 60 70 80 90 100
Percentage FS agents

Figure 9: Final performance; without rewiring (left) rewiring (right).
4.2.3 P ERFORMANCE
observations concerning learned strategy, reported above, reflected performance
collective agents. setup without rewiring (Figure 9, left), performance decreases
initially increasing proportion FS agents, DS agents refuse adapt dictated
strategy. proportion FS agents becomes large enough, DS agents start picking
strategy, resulting increasing performance. setup rewiring (same figure, right)
better, performance increases increasing number FS agents. Even though average
learned strategy close 4.5 every proportion FS agents, low proportions FS agents
still display less performance higher proportions. may require additional explanation.
Note box plot Figure 8 shows distribution average strategy 50 repeated
simulations; i.e. show strategy distribution within single simulation.
Thus, even though average strategy single simulation always close 4.5,
still variance. low number FS agents, variance prominently caused inertia,
566

fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AME

W ITH REWIRING

Degree distribution

Degree distribution

N REWIRING
100
90
80
70
60
50
40
30
20
10
0
0

10 20 30 40 50 60 70 80 90 100
Percentage FS agents

100
90
80
70
60
50
40
30
20
10
0
0

10 20 30 40 50 60 70 80 90 100
Percentage FS agents

Figure 10: Resulting network structure; without rewiring (left) rewiring (right).
i.e. DS agents directly connected FS agent, implies need learn
desired strategy neighboring agents learning. Especially rewiring,
may result two agents playing together compatible neighbors,
(yet) other.
4.2.4 R ESULTING N ETWORK TRUCTURE
Clearly, network structure setup without rewiring (Figure 10, left) influenced
varying proportion FS agents. rewiring used (same figure, right), observe
interesting phenomenon, closely related observations 4.1. again, number
times agents actually rewire generally lies 1, 000. Even though low number,
affect network structure useful way. low proportion FS agents, large
tendency increased preferential attachment. 10% FS agents instance, single
agent connects 70 100 agents. increasing proportion FS agents,
maximum degree network decreases, finally, closely resembles original network.
Clearly, presence examples desired strategy, DS agents attempt connect
agents provide examples. interesting useful emergent behavior.
4.2.5 N C ONCLUSION
compare results obtained population 100 agents results
population sizes, reported Table 1, see highly similar. conclusion
subsection, may state setup using rewiring severe difficulties converging
desired example proportion FS agents providing example low. for, e.g. half
population consisting examples, half learn desired behavior. setup
using rewiring absolutely problems converging desired strategy, even
low proportion FS agents. cases, completely omitting examples leads agents
converging individually rational solution. caused artifact learning method
used, i.e. mentioned before, two CALA trying learn others strategy tend driven
downward lowest value allowed.
567

fiD E J ONG , U YTTENDAELE & UYLS

5. Discussion
results presented previous section suggest mechanisms lead cooperative solutions social dilemmas discrete set strategies (e.g. scale-free networks rewiring),
lead agreement social dilemmas continuous strategy space. section, however, show trivial issue. precisely, discuss number mechanisms
enhance agents abilities reach cooperation social dilemmas discrete strategy sets,
directly enhance agents abilities reach agreement continuous strategy spaces.
empirically analyze case.
5.1 Reputation
Reputation one main concepts used behavioral economics explain fairness
emerges (e.g. Bowles et al., 1997; Fehr, 2004). Basically, assumed interactions
people lead expectations concerning future interactions. expectations may positive
negative may kept oneself, actually shared peers.
work closely related work, Nowak et al. (2000) show reputation deters agents
accepting low offers Ultimatum Game, information spread, leading
agents receiving low offers return. Then, agents refuse accept low offers,
provide high offers. Thus, Nowak et al. argue population goes toward providing
accepting high offers. However, note shared strategy (i.e. agreement)
Ultimatum Game yields expected payoff 50% amount stake agents. Thus,
reputation may indeed help agents decide strategy play others, preference
playing cooperatively (i.e. providing high offers) directly result reputation.
5.1.1 PREADING R EPUTATION
study effects reputation optionally adding second network system.
interaction network, consider reputation network scale-free. contrast
interaction network however, reputation network assumed static, agents truthful
concerning reputation, making unnecessary agents consider rewiring. Note two agents
sharing reputation information may may connected well interaction network,
consequence, two agents playing Ultimatum Game may may share reputation
information other. effect, every Ultimatum Game, responding agent may
broadcast reputation information neighbors reputation network. information sent
responder concerns offer done proposer; information
guaranteed correct. Agents receive information probability:
pij = 1


H

(7)

Here, distance sender (potential) receiver j reputation network.
Thus, reputation information may travel H hops, decreasing probability per hop.
simulations, set H = 5. note relatively small networks, implies
reputation information essentially public.
Note reputation information may helpful allow agents something
information. work Nowak et al. (2000), instance, reputation others used
agents determine offer others. Given (1) observation reputation, used
568

fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AME

way, necessarily promote cooperative strategies (see above), (2) fact
already use CALA determine agents offer other, want reputation affect
something else agents strategies. discuss number ways agents may use
reputation, taken literature, i.e. interacting preferred neighbor (below) using
reputation facilitate voluntary participation (5.3).
5.1.2 U SING R EPUTATION
Without reputation, agents play random neighbor interaction network. Reputation
may used make agents prefer interacting specific neighbors Chiang (2008) discusses
strategies fairness could evolve dominant agents allowed choose preferred partners
play against. Chiang allows agents select partners helped agent previously.
determine preferred partner, use heuristic proposed Santos et al. (2006a),
i.e. agent prefers playing (relative) cooperators, help obtaining high payoff
responder. Thus, probability agent plays agent j Ni , Ni set
agent neighbors, is:
sj si
pij = P
(8)
kNi sk
Here, si , sj sk agents current strategies (for agents i, estimates
based reputation previous experience).
two problems approach. First, number times agent receives
information agent j Ni may rather low, especially many agents. Even
50 agents, observe around 25% reputation information received agents actually concerned one neighbors. problem may addressed making reputation
network identical interaction network (as neighbor relations networks identical). However, may seen considerable abstraction. Second, probability agent
information concerning neighbors low, need specify default values s0j .
Clearly, default value often wrong right, unless use centralized mechanism
estimate by, instance, using current average population strategy,
simulations.
mechanism place, perform experiments 4, i.e. vary
population size 10 10, 000 agents, proportion FS agents steps 10%.
statistical analysis reveals significant difference setup uses reputation setup
not. analyze results, see that, expected, agents almost always
need resort default values neighbors strategies. Thus, average, reputation
system often change probabilities certain neighbors selected.
5.2 Reputation Rewiring
seen 4, rewiring works well without reputation (i.e. purely based agents
experience). Adding reputation may beneficial agents, longer need interact
allowed unwire. Thus, agents may increase preference
certain others. Reputation information (i.e. amount offered certain agent) propagates
(static) reputation network, allowing agents receiving information potentially
unwire one neighbors consider neighbors behavior undesirable.
rewiring mechanism used detailed 3 (i.e. Equation 6). allow responder
569

fiD E J ONG , U YTTENDAELE & UYLS

Ultimatum Game broadcast reputation information reputation network,
maximum H = 5 hops.
again, perform experiments 4, again, significant
difference main results. analyze number times agents actually rewired,
find number average increases factor 2 respect setup reputation
used (i.e. reported 4.3). However, increase increase performance. average, agents neighbors; thus, generally receive reputation information concerning
neighbor that, absence reputation, would play soon anyway.
5.3 Volunteering
According existing research human fairness (e.g. Boyd & Mathew, 2007; Hauert et al., 2007;
Sigmund et al., 2001) mechanism volunteering may contribute reaching cooperation
games two strategies. mechanism volunteering consists allowing players
participate certain games, enabling fall back safe side income depend
players strategies. risk-averse optional participation prevent exploiters
gaining upper hand, left empty-handed cooperative players preferring
participate. Clearly, side income must carefully selected, agents encouraged
participate population sufficiently cooperative. Experiments show volunteering indeed
allows collective players spend time happy state (Boyd & Mathew, 2007)
players cooperative.
biggest problem applying volunteering basically introduce yet another
social dilemma. agent may refrain participating make statement
agent, may convince agent become social future, make
statement, agent must refuse expected positive payoff: Ultimatum Game randomly
assigned roles, expected payoff always positive. Nonetheless, study whether volunteering
promotes agreement games continuous strategy spaces. use heuristic
proposed Santos et al. (2006a), already applied various mechanisms
paper: agent thinks agent j (relative) cooperator, agrees playing.
agents agree, game played. prevent agents playing game (after all,
agents see relative cooperator already playing strategy),
introduce 10% probability games played anyway, even one agents want
to. Note reputation may used here, may allow agents estimate whether one
neighbors relative cooperator not, without play neighbor.
Unfortunately, experimental results point agents using volunteering (with without
reputation) severe difficulties establishing common strategy (Uyttendaele, 2008). result, measuring performance, see around 50% games played.
games played, performance similar setup rewiring (e.g. 80%), may
expected, two agents usually agree play strategies similar. reason
agents converge properly quite simple: avoid playing agents
different them. Therefore, learn behave way similar others.
5.4 General Discussion
general, may state mechanism rewiring, clearly find good balance
allowing agents play preferred neighbors one hand, forcing agents
570

fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AME

learn different hand. additions discussed allow
agents selective, i.e. much influence play against.
may interest individual agents, generally leads agents playing others
different them, instead learning others, required obtain convergence
agreement.

6. Conclusion
paper, argue mechanisms thought allow humans find fair, satisfactory solutions
social dilemmas, may useful multi-agent systems, many multi-agent systems need
address tasks contain elements social dilemmas, e.g. resource allocation (Chevaleyre et al.,
2006). Existing work concerning (human-inspired) fairness multi-agent systems generally restricted discrete strategy sets, usually two strategies, one deemed
cooperative (i.e. desirable). However, many real-world applications multi-agent systems pose
social dilemmas require strategy taken continuous strategy space, rather discrete strategy set. observed traditional concept cooperation trivially applicable
continuous strategy spaces, especially since longer feasible manually label certain
strategy cooperative absolute manner. certain strategy may cooperative certain
culture, whereas may defective naive another. Thus, cooperation relative rather
absolute concept continuous strategy spaces.
propose concept agreement (as introduced statistical physics; DallAsta et al.,
2006) may used alternative cooperation. discuss emergence agreement
continuous strategy spaces, using learning agents play pairwise Ultimatum Games, based
scale-free interaction network possibility rewire network. Ultimatum
Game, two agents agree offering agent offers least minimal amount satisfies
responding agent (in case, two agents cooperate). Thus, population agents
agree many random pairwise games, agents converge strategy. Without
external influences, shared strategy sufficient. external influences, e.g. preference
dictated humans, agents adapt dictated strategy, even already agreeing
completely different strategy. propose methodology, based continuous-action learning
automata, interactions scale-free networks, rewiring networks, aimed allowing
agents reach agreement. set experiments investigates usability methodology.
conclusion, give four statements. (1) proposed methodology able establish agreement common strategy, especially agents given option rewire network
interaction. Humans playing Ultimatum Game reach agreement approximately 80-90%
(Oosterbeek et al., 2004). Without rewiring, agents worse (generally, 65% games
successful); rewiring, well humans. Thus, games discrete strategy set,
rewiring greatly enhances agents abilities reach agreement, without compromising scalefree network structure. indicates interactions scale-free networks, well rewiring
networks, plausible mechanisms making agents reach agreement. (2) comparison
methodologies reported related work (e.g. Santos et al., 2006b), methodology facilitates
convergence low number games per agent needed (e.g. 500 instead 10,000).
indicates continuous-action learning automata satisfactory approach aiming
allowing agents learn relatively low number examples. (3) performance
collective seriously influenced size. clearly influenced characteristics
571

fiD E J ONG , U YTTENDAELE & UYLS

scale-free, self-similar network. (4) Concepts reputation volunteering,
reported facilitate cooperative outcomes discrete-strategy games, seem
(additional) benefits continuous strategy spaces.
Although Ultimatum Game one example social dilemma, core difficulty
present social dilemmas: selecting individually rational action, optimize ones
payoff, actually may hurt payoff. Ultimatum Game, problem caused fact
may play (as proposer) someone would rather go home empty-handed
accept deal perceived unfair. Similar fairness-related problems may arise various
interactions, e.g. bargaining division reward, resource allocation (Chevaleyre
et al., 2006; Endriss, 2008). Many multi-agent systems need allocate resources, explicitly
assigned task, implicitly, instance multiple agents share certain, limited
amount computation time. Thus, fair division important area research, recently
receiving increasing attention multi-agent systems community (Endriss, 2008). humans
often display adequate immediate ability come fair division accepted
them, definitely pay allow agents learn imitate human strategies.
paper, examined task may executed.

Acknowledgments
authors wish thank anonymous referees valuable contributions. Steven de Jong
funded Breedtestrategie programme Maastricht University.

References
Axelrod, R. (1997). Dissemination Culture: Model Local Convergence Global
Polarization. Journal Conflict Resolution, 41:203226.
Barabasi, A.-L. Albert, R. (1999). Emergence scaling random networks. Science, 286:509
512.
Bearden, J. N. (2001). Ultimatum Bargaining Experiments: State Art. SSRN eLibrary.
Bowles, S., Boyd, R., Fehr, E., Gintis, H. (1997). Homo reciprocans: Research Initiative
Origins, Dimensions, Policy Implications Reciprocal Fairness. Advances Complex
Systems, 4:130.
Boyd, R. Mathew, S. (2007). Narrow Road Cooperation. Science, 316:18581859.
Cameron, L. (1999). Raising stakes ultimatum game: Evidence Indonesia. Journal
Economic Inquiry, 37:4759.
Chevaleyre, Y., Dunne, P., Endriss, U., Lang, J., Lematre, M., Maudet, N., Padget, J., Phelps, S.,
Rodriguez-Aguilar, J., Sousa, P. (2006). Issues Multiagent Resource Allocation. Informatica, 30:331.
Chiang, Y.-S. (2008). Path Toward Fairness: Preferential Association Evolution Strategies Ultimatum Game. Rationality Society, 20(2):173201.
572

fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AME

DallAsta, L., Baronchelli, A., Barrat, A., Loreto, V. (2006). Agreement dynamics smallworld networks. Europhysics Letters, 73(6):pp. 969975.
Dannenberg, A., Riechmann, T., Sturm, B., Vogt, C. (2007). Inequity Aversion Individual
Behavior Public Good Games: Experimental Investigation. SSRN eLibrary.
de Jong, S. Tuyls, K. (2008). Learning cooperate public-goods interactions. Presented
EUMAS08 Workshop, Bath, UK, December 18-19.
de Jong, S., Tuyls, K., Verbeeck, K. (2008a). Artificial Agents Learning Human Fairness.
Proceedings international joint conference Autonomous Agents Multi-Agent
Systems (AAMAS08), pages 863870.
de Jong, S., Tuyls, K., Verbeeck, K. (2008b). Fairness multi-agent systems. Knowledge
Engineering Review, 23(2):153180.
de Jong, S., Tuyls, K., Verbeeck, K., Roos, N. (2008c). Priority Awareness: Towards Computational Model Human Fairness Multi-agent Systems. Adaptive Agents Multi-Agent
Systems III. Adaptation Multi-Agent Learning, 4865:117128.
Endriss, U. (2008). Fair Division. Tutorial International Conference Autonomous Agents
Multi-Agent Systems (AAMAS).
Fehr, E. (2004). Dont lose reputation. Nature, 432:499500.
Fehr, E. Schmidt, K. (1999). Theory Fairness, Competition Cooperation. Quarterly
Journal Economics, 114:817868.
Gintis, H. (2001). Game Theory Evolving: Problem-Centered Introduction Modeling Strategic
Interaction. Princeton University Press, Princeton, USA.
Grosskopf, B. (2003). Reinforcement Directional Learning Ultimatum Game Responder Competition. Experimental Economics, 6(2):141158.
Gueth, W., Schmittberger, R., Schwarze, B. (1982). Experimental Analysis Ultimatum
Bargaining. Journal Economic Behavior Organization, 3 (4):367388.
Hauert, C., Traulsen, A., Brandt, H., Nowak, M. A., Sigmund, K. (2007). Via freedom
coercion: emergence costly punishment. Science, 316:19051907.
Henrich, J., Boyd, R., Bowles, S., Camerer, C., Fehr, E., Gintis, H. (2004). Foundations
Human Sociality: Economic Experiments Ethnographic Evidence Fifteen Small-Scale
Societies. Oxford University Press, Oxford, UK.
Maynard-Smith, J. (1982). Evolution Theory Games. Cambridge University Press.
Maynard-Smith, J. Price, G. R. (1973). logic animal conflict. Nature, 246:1518.
Nowak, M. A., Page, K. M., Sigmund, K. (2000). Fairness versus reason Ultimatum
Game. Science, 289:17731775.
573

fiD E J ONG , U YTTENDAELE & UYLS

Oosterbeek, H., Sloof, R., van de Kuilen, G. (2004). Cultural Differences Ultimatum Game
Experiments: Evidence Meta-Analysis. Experimental Economics, 7:171188.
Peters, R. (2000). Evolutionary Stability Ultimatum Game. Group Decision Negotiation,
9:315324.
Roth, A. E., Prasnikar, V., Okuno-Fujiwara, M., Zamir, S. (1991). Bargaining Market
Behavior Jerusalem, Ljubljana, Pittsburgh, Tokyo: Experimental Study. American
Economic Review, 81(5):106895.
Santos, F. C., Pacheco, J. M., Lenaerts, T. (2006a). Cooperation Prevails Individuals
Adjust Social Ties. PLoS Comput. Biol., 2(10):12841291.
Santos, F. C., Pacheco, J. M., Lenaerts, T. (2006b). Evolutionary Dynamics Social Dilemmas
Structured Heterogeneous Populations. Proc. Natl. Acad. Sci. USA, 103:34903494.
Selten, R. Stoecker, R. (1986). End behavior sequences finite Prisoners Dilemma supergames : learning theory approach. Journal Economic Behavior & Organization, 7(1):47
70.
Sigmund, K., Hauert, C., Nowak, M. A. (2001). Reward punishment. Proceedings
National Academy Sciences, 98(19):1075710762.
Slonim, R. Roth, A. (1998). Learning high stakes ulitmatum games: experiment
Slovak republic. Econometrica, 66:569596.
Sonnegard, J. (1996). Determination first movers sequential bargaining games: experimental study. Journal Economic Psychology, 17:359386.
Sutton, R. S. Barto, A. G. (1998). Reinforcement Learning: Introduction. MIT Press,
Cambridge, MA. Bradford Book.
Thathachar, M. A. L. Sastry, P. S. (2004). Networks Learning Automata: Techniques
Online Stochastic Optimization. Kluwer Academic Publishers, Dordrecht, Netherlands.
Uyttendaele, S. (2008). Fairness agreement complex networks. Masters thesis, MICC,
Maastricht University.

574


