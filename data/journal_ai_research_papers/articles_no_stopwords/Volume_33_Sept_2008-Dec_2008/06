journal artificial intelligence

submitted published

learning reach agreement continuous ultimatum game
steven de jong
simon uyttendaele

steven dejong micc unimaas nl

micc maastricht university
p box md maastricht netherlands

karl tuyls

k p tuyls tue nl

eindhoven university technology
p box mb eindhoven netherlands

abstract
well known acting individually rational manner according principles classical game theory may lead sub optimal solutions class named social dilemmas
contrast humans generally much difficulty social dilemmas able
balance personal benefit group benefit agents multi agent systems regularly confronted social dilemmas instance tasks resource allocation agents may
benefit inclusion mechanisms thought facilitate human fairness although many
mechanisms already implemented multi agent systems context application usually limited rather abstract social dilemmas discrete set available strategies
usually two given many real world examples social dilemmas actually continuous
nature extend previous work general dilemmas agents operate continuous strategy space social dilemma study well known ultimatum game
optimal solution achieved agents agree common strategy investigate whether
scale free interaction network facilitates agents reach agreement especially presence
fixed strategy agents represent desired e g human outcome moreover study influence rewiring interaction network agents equipped continuous action
learning automata play large number random pairwise games order establish common strategy experiments may conclude obtained discrete strategy
games generalized continuous strategy games certain extent scale free interaction network structure allows agents achieve agreement common strategy rewiring
interaction network greatly enhances agents ability reach agreement however
becomes clear alternative mechanisms reputation volunteering many
subtleties involved convincing beneficial effects continuous case

introduction
sharing limited resources others common challenge individuals human societies
well agents multi agent systems chevaleyre et al often conflict
interest personal benefit group social benefit conflict prominently
present class named social dilemmas individuals need consider
personal benefit effects choices others failure may
lead sub optimal solutions dilemmas classical game theory assumes players
agents completely individually rational strategic circumstances seems limited value
gintis maynard smith individually rational players socially conditioned
humans hand generally remarkable ability address social dilemmas due
c

ai access foundation rights reserved

fid e j ong u yttendaele uyls

tendency consider concepts fairness addition personal benefit see e g
dannenberg et al fehr schmidt gintis oosterbeek et al
prime example social dilemma modeled well known ultimatum game gueth
et al game two agents bargain division amount r obtained
outsider first agent proposes offer r second agent e g receive
second agent accepts agent gets share e first agent receives r r
second receives r however second agent rejects agents left nothing
individually rational first agent would offer smallest amount possible knowing second
agent choose obtaining amount accepting nothing rejecting thus
accepting smallest amount possible individually rational response contrast human
players game hardly ever offer less offer occurs
likely accepted bearden oosterbeek et al thus individually rational player
plays proposer human player probably gain money
researchers proposed mechanisms may responsible emergence
fair strategies human populations playing social dilemmas well resistance
strategies invasion individually rational strategies see e g fehr schmidt gintis nowak et al santos et al often mechanisms implemented multi agent systems validation purposes e agents shown prefer
fair strategies individually rational ones makes plausible humans actually
affected underlying mechanisms however argue multi agent systems driven fairness mechanisms may used validate mechanisms allow agents act
fair way real world applications given agents often face tasks resource sharing
allocation explicitly implicitly e g sharing limited computational resources
tasks regularly contain elements social dilemmas important enable agents
act individual rationality fairness chevaleyre et al
unfortunately existing work usually introduces number abstractions allow resulting multi agent systems applied realistic resource allocation
prominently work focused social dilemmas discrete strategy sets usually limited
two abstraction simplifies dilemmas hand reflect potential realworld nature since many dilemmas especially related real world resource allocation
continuum strategies e continuous strategy space rather discrete set
pure strategies moreover social dilemmas continuous strategy spaces qualifications
cooperation defection often used discrete social dilemmas actually relative certain strategy may seen cooperative e desirable certain context whereas
may defective simply naive another one clear dilemma may far
complicated continuous strategy spaces agents may need use different way
determining whether behavior desirable
analogy ultimatum game social dilemmas public goods game shown
full mathematical rigor sigmund et al de jong tuyls report preliminary applying
methodology described public goods game
theoretical work field evolutionary game theory occasionally limited discrete strategy sets worth
mentioning work peters introduces theoretical extension evolutionary stable
strategy concept ess continuous strategy spaces e extended stability calculus extension provides
theoretical solution concept clarify egalitarian outcomes however concept shed light
learning agents possibly achieve fair outcomes social dilemmas work therefore complementary
work aims mechanisms enabling agents fair outcomes



fil earning r agreement c ontinuous u ltimatum g ame

generalize existing work achieving agreement cooperation fairness
social dilemmas continuous strategy spaces aim presenting methodology
allows agents reach satisfactory outcomes dilemmas well real world
containing elements dilemmas apply proposed methodology ultimatum
game example social dilemma game population agents needs reach agreement e converge common strategy order obtain satisfactory payoff agreement
specifies populations cooperative desirable strategy precise nature agreement
may vary population agents strategy agreed upon sufficient number
agents successful dictate culture desirable strategy population
wish agents learn human strategy may introduce number simulated human
agents e agents play according desirable strategy learning agents
able imitate strategy even already reached agreement different possibly
relatively defective strategy
remainder structured follows first give brief overview
related work aims continue next discuss methodology aimed
establishing agreement large populations learning agents present experiments
outline number alternative approaches proposed dilemmas
discrete strategy sets fail impress dilemma continuous strategy space discuss
case finally conclude

related work
work basically builds upon two tracks existing work give overview
tracks indicate related current work extensive discussion
refer previous work de jong et al b
learning fairness bargaining
de jong et al investigated behavior agents playing ultimatum game
nash bargaining game continuous action learning automata games agents
interacting time ultimatum game required extension two
players agents one demanded portion reward hand last
player received left homo egualis utility function developed fehr schmidt
gintis used represent desired outcome e required minimal amount
every agent wished obtain agents able successfully maintain agreements games addition observed solutions agreed upon corresponded
solutions agreed upon humans reported literature work similarly use continuous action learning automata learn agreement ultimatum game however multi agent
system organized network structure efficiently populated much larger number
agents e g thousands contrast previous work agents play pairwise games moreover
note analogy humans cultural background one primary influences constitutes fair
cooperative desirable strategy although general tendency deviate pure individual rationality
favor socially aware strategies exact implications vary greatly henrich et al oosterbeek et al
roth et al ultimatum game instance actual amounts offered minimally accepted
vary much depending factors amount bargain cameron
de jong et al c slonim roth sonnegard culture henrich et al cultural
differences may persist groups agents within groups axelrod



fid e j ong u yttendaele uyls

use homo egualis utility function instead desired human inspired outcome
offered homo egualis utility function replaced potentially including agents
play according certain fixed strategy e simulated human players
network topology
santos et al b investigated impact scale free networks resulting strategies social
dilemmas scale free network used order randomly determine two agents neighbors would play together social dilemmas prisoners dilemma
snowdrift game agents limited two strategies e cooperate defect
initially equally probable observed due scale free network defectors could
spread entire network games network structures authors
identified topology network contributed observed maintained cooperation
subsequent santos et al introduced rewiring network played many different social dilemmas two strategies per agent concluded ease
measure individuals inertia readjust ties rewiring increasing rate
cooperators efficiently wipe defectors contrast work santos et al b
used discrete strategy set work uses continuous strategy space requires another view
fairness cooperation agreement departing traditional view fairness achieved
driving agents manually labeled cooperative strategies social dilemmas
ultimatum game strategy agents may agree leads satisfactory outcomes

methodology
discussing methodology detail first outline basic setting continue
explaining continuous action learning automata central methodology next
discuss structure topology networks interaction use discuss
agent types initial strategies agents elaborate provide additional
possibility rewiring connections agents finally explain experimental setup
basic setting
study large group adaptive agents driven continuous action learning automata playing
ultimatum game pairwise interactions pairs chosen according scale free network
interaction every agent randomly assigned role proposer responder ultimatum
game agents start different strategies good performance need converge
agreement playing many pairwise games e need learn common strategy
agents may fixed strategies agents represent external strategy adaptive
agents need converge instance preference dictated humans addition
basic setting study influence adding option agents rewire network
interaction response agent behaved defecting manner
continuous action learning automata
continuous action learning automata cala thathachar sastry learning automata
developed continuous action spaces cala essentially function optimizers
every action continuous one dimensional action space receive feedback


fil earning r agreement c ontinuous u ltimatum g ame

x goal optimize feedback cala proven convergence local optima
given feedback function x sufficiently smooth advantage cala many
reinforcement learning techniques see e g sutton barto necessary
discretize continuous action spaces actions simply real numbers
h ow cala w ork
essentially cala maintain gaussian distribution actions pulled contrast
standard learning automata cala require feedback two actions action corresponding
mean gaussian distribution action corresponding sample x taken
distribution actions lead feedback x respectively turn
feedback used update probability distributions precisely update formula
cala written
x
x





x
x

k l




equation represents learning rate k represents large constant driving
variance kept threshold l keep calculations tractable even case convergence
implemented function
max l



intuition behind update formula quite straightforward de jong et al
update formula cala rather quickly converge local optimum multiple e g n learning
automata every automaton receives feedback respect joint actions respectively
x n x x xn case still convergence
local optimum thathachar sastry
odifications cala p urposes
outlined use cala enable agents learn sensible proposer responder strategy ultimatum game playing ultimatum game two agents may agree
one two joint actions e obtain one high one low feedback
may even disagree e obtain two low feedbacks situations need
additional attention occurrence prevents cala converging correct solutions
address situations propose two domain specific modifications update formula
cala de jong et al
first case joint actions yield feedback cala unable draw effective
conclusions even though may tried ineffective strategy thus actually
used following settings initial experiments k l
precise settings l decisive influence outcomes although values may lead slower
convergence k chosen large rather vaguely implied thathachar sastry decreases
fast e usually cala stop exploring sufficient solution found
note modifications uncommon literature see e g work selten stoecker
learning direction theory grosskopf successfully applied directional learning setting ultimatum
game focusing responder competition addressed



fid e j ong u yttendaele uyls

learn order counter introduce driving force allows agents update
strategy even feedback received driving force defined

proposers x
iff x

responders x
effect modification call zero feedback avoidance zfa agent
playing proposer learn offer agent playing responder accept
lower expectation roles lead probable agreement
second one joint action yields agreement feedback cala may adapt
strategies drastically favor first joint action fact shifts values greater
observed de jong tuyls de jong et al tackle
restrict difference possible two feedbacks cala receive every
iteration precisely empirically set


x






thus large difference feedback action x action preserve
direction indicated feedback prevent automaton jump far direction
call modification strategy update limitation sul
network interaction
scale free network barabasi albert network degree distribution follows
power law precisely fraction p k nodes network k connections
nodes goes large values k p k k value constant typically
range scale free networks noteworthy many empirically observed networks
appear scale free including world wide web protein networks citation networks
social networks mechanism preferential attachment proposed explain power law
degree distributions networks preferential attachment implies nodes prefer attaching
nodes already large number neighbors nodes small
number neighbors
previous indicated scale free networks contribute emergence cooperation santos et al b wish determine whether phenomenon still occurs
continuous strategy spaces therefore use scale free topology interaction network barabasi albert model precisely probability pi newly introduced node
connected existing node degree ki equal
ki
pi p

j kj
construct network two first nodes linked
nodes introduced sequentially connected one existing nodes pi
way newly introduced node probably connect heavily linked hub one
connections simulations connect every node one two three
existing ones uniform probabilities yields networks interaction realistic
acyclic ones obtained connecting nodes one existing node example
network modeling friendship network avoiding cycles means assuming friends
certain person never friends


fil earning r agreement c ontinuous u ltimatum g ame

agent types strategies
order study agreement concerning common strategy emerges need make
agents learn reach common strategy starting situation absent e
agents different strategies moreover need study whether common strategy
established example agents whether robust agents use different
potentially relatively defective strategy
wo ypes agents
introduce two types agents e dynamic strategy ds agents fixed strategy fs agents
ds agents learning agents start certain predefined strategy allowed
adapt strategy constantly according learning mechanism learning automaton
basically agents similar used earlier work de jong et al fs agents
optional good examples model example strategy needs learned
agents system therefore refuse adapt strategy
ne wo cala per agent
outlined agent needs able perform two different roles ultimatum game e playing proposer well playing responder words
agent one two distinct states state requires learn different strategy cala
stateless learners agent therefore would require two cala nonetheless remainder
equip every ds agent one cala representing agents proposer
strategy well responder strategy
choice one cala motivated two observations e human behavior
initial experiments first human strategies often consistent implying generally
accept offers reject offers lower oosterbeek et al even high
amounts stake de jong et al c sonnegard second set initial experiments
observed agents two cala generally converge one single strategy anyway
illustration three learning curves obtained fully connected network three agents playing
ultimatum game displayed figure clearly visible agents proposer strategies
bold lines strongly attracted agents responder strategies thin lines especially
lowest responder strategies presence fs agent offers accepts
least first strategy immediately ignored favor lower second one ds
agents strategies attracted lowest responder strategy present
future work study observation perspective evolutionary game theory
replicator equations gintis maynard smith price current
use observation justify abstraction e limit complexity agents equipping
one cala cala represents agents proposer strategy well
responder strategy updated agent plays proposer well plays
responder according cala update formula presented modifications
presented thus agents single cala receive twice much feedback two separate
cala would abstraction therefore increases efficiency learning process
agents quickly adapt strategies downward upward figure therefore multiple e g
ds agents learning e without fs agents strategy usually converges due artifact
learning process two cala trying learn others current strategy tend driven downward





















strategy

strategy

e j ong u yttendaele uyls




















iteration


iteration



top left two ds agents one starting offering accepting one starting offering accepting learn
play optimally fs agent offering
accepting ds agents rather quickly learn
offer accept




strategy






top right two ds agents starting offering
accepting learn play optimally
fs agent offering accepting ds
agents learn offer accept





bottom left three ds agents starting different initial
strategies e offering accepting
respectively quickly learn single similar strategy





iteration

figure evolving strategies fully connected network three agents proposal strategies
indicated bold line response strategies indicated thin line agents
converge situation two initial strategies become similar

agents trategies
simulations use two types ds agents one type fs agents precisely
dsr agents learning agents start rational solution offering x n
accepting amount dsh agents start human fair solution e
offering x n accepting amount since fs agents
examples desired solution equip fair human inspired solution see whether
agents able adapt solution fs agents offer accept
offer agents limited strategies taken continuous interval
c chosen upper bound instead common
common amount money needs shared ultimatum game agents strategy
falls outside interval c round strategy nearest value within interval
rewiring
agents play together connections interaction network thus order avoid
playing certain undesirable neighbor j agent may decide break connection


fil earning r agreement c ontinuous u ltimatum g ame

j create link random neighbor j santos et al rewiring
use heuristic proposed santos et al agents want disconnect relative
defectors prefer play relative cooperators thus probability agent unwires
agent j calculated
si sj
pr

c
si sj agents current strategies precisely agent responder strategy
agent js proposer strategy c amount stake ultimatum game e even
agents determine want unwire probability may still allowed
breaks last link one unwiring takes place agent creates wire
random neighbor agent j
experimental setup
aforementioned types agents need determine whether proposed methodology
possesses traits would see population said established
successful agreement manages reach common strategy incorporates preferences
good examples time discouraging agents try exploit dominant
strategy thus population consisting ds agents strategy shared
agents leads good performance since agents agree games yielding average payoff
per game per agent architecture able common strategy
ds well fs agents fs agents dictate strategy ds agents converge
regardless whether start dsh dsr agents
order measure whether agents achieved satisfactory outcome study four quantities
related learning process final outcome viz point convergence
learned strategy performance resulting network structure briefly explain
four quantities general remark every simulation lasts iterations
per agent e n iterations n agents repeat every simulation times obtain reliable
estimates quantities interest
p oint c onvergence
important quantity concerning agents learning process point convergence
present tells us many games agents needed play order establish agreement determine point convergence calculate save average population strategy
avg pairwise game e iteration learning process iterations
obtain ordered set averages e avg avg initially average population
strategy changes time agents learning certain point time agents stop
learning average population strategy avg change much anymore
estimate point e point convergence lowest standard
deviation subset avg avg subsequently report number
games per agent played iteration e nt experiments every simulation repeated
note may choose allow agent create connection specific agents instead
random neighbors neighbor j however especially combination reputation see allows
relative defectors quickly identify relative cooperators may connect
attempt exploit preliminary experiments shown behavior leads interaction network losing
scale freeness may seriously impair emergence agreement



fid e j ong u yttendaele uyls

avg
std
conv










































iteration



strategy



















avg
std
conv

population strategy










































strategy

population strategy












iteration

figure two examples convergence point single run graphs display
average strategy population bold line well standard deviation
average thin line dotted vertical line denotes convergence point found
analysis detailed text

times resulting convergence points use box plot visualize distribution
convergence points
example figure left see fs agents dsh agents dsr agents
converge agreement rewiring first games shown addition
bold line denoting average population strategy plot thinner line denoting standard
deviation average method outlined point convergence determined
around games e approximately games per agent necessary figure
right similar fs agents dsr agents rewiring
point convergence around games e approximately games per agent
necessary means learning reach agreement difficult
l earned trategy
established iteration agents converged state average
learned strategy precisely avg repeat every simulation times obtain reliable estimate average use box plot visualize distribution
average learned strategy
p erformance
measure performance first allow agents learn playing ultimatum games
fix strategies ds agents let every agent play proposer
neighbors one one count number games successful divide
box plots report average instead median average informative quantity e g
comparing existing work may box plots mid point located outside box
note cala update formula prevents agents converging exact strategy standard deviation
calas gaussian kept artificially strictly positive therefore noise strategies agents
converged counter noise measuring performance set responders strategies
actual strategies thus agent strategy propose accept offer



fil earning r agreement c ontinuous u ltimatum g ame

number total number games played e twice number edges interaction
network resulting number denotes performance lies utterly catastrophic complete agreement human players ultimatum game typically achieve
performance fehr schmidt oosterbeek et al
repetitions lead measures performance displayed box plot
r esulting n etwork tructure
since network interaction may rewired agents satisfied neighbors interested network structure resulting agents learning processes
examine network structure looking degree distribution nodes network e
number neighbors agents repeated simulations may draw single box
plot expressing average degree distribution

experiments
present experiments two subsections first study setup without rewiring
setup rewiring varying number agents keeping proportion dsh dsr
fs agents constant equal e type agent second study
two setups population sizes time varying proportion fs agents
remainder population half dsh half dsr general remark every experiment
reports averaged simulations every simulation allow agents
play n random games n denotes number agents e population size
varying population size
many multi agent systems increasing number agents e population size causes
difficulties many mechanisms work relatively low number agents stop working
well high number agents instance due computational complexity undesired
emergent properties according previous issue scalability applies
task learning social dilemmas indeed previous evolutionary games
discrete strategy sets mentions number games needed converge agreement
e cooperation may prohibitively large santos et al
since agents learning continuous strategy spaces may expect scalability issue
well determine whether proposed methodology issue vary population
size steps keeping proportion fs
dsh dsr agents constant one third study setup without rewiring well
setup rewiring determine point convergence e number games per agent
needed reach convergence average learned strategy agents converged final
performance system finally resulting network structure especially first
third quantities give indication scalability methodology
order limit time taken learning santos et al terminate learning process iterations
agents leading average games per agent available still
high number games per agent report agents occasionally converge



fid e j ong u yttendaele uyls

w ith rewiring





games per agent

games per agent

n rewiring





















number agents








number agents

figure games per agent convergence without rewiring left rewiring right

p oint c onvergence
setup without rewiring figure left tends require games per agent total number
agents increases certain point e around population size agents tendency
stops mainly average number games per agent approaches maximum e
games per agent setup rewiring figure right convincingly outperforms one without
rewiring increasing population size hardly affects number games per agent required
reach convergence independent population size setup requires approximately
games per agent converge note difference previous e santos et al
reports requiring games per agent
l earned trategy
setup without rewiring figure left average converges strategy offering well
accepting around would required fs agents present population
play strategy e ds agents average strategy increasing
population size average strategy affected however becomes
certainty established setup rewiring figure right shows convincingly
better independent population size learning agents converge desired
strategy e
p erformance
setup without rewiring figure left already saw average learned strategy
ds agents good performance seriously affected around indicates
ds agents ever agree fs agents however average performance influenced
population size learned strategy performance around becomes
certainly established expected setup rewiring figure right shows much
satisfying e generally agreement actually positively affected
population size average performance increases increasing population


fil earning r agreement c ontinuous u ltimatum g ame

n rewiring

w ith rewiring

average converged strategy

average converged strategy





















number agents










number agents



figure average learned strategy without rewiring left rewiring right

w ith rewiring













performance convergence

performance convergence

n rewiring








number agents



















number agents



figure final performance without rewiring left rewiring right

r esulting n etwork tructure
look network structure resulting learning reach agreement determine whether
structure influenced population size obviously setup without rewiring figure
left display influence network static setup rewiring
figure right shows interesting tendency average degree resulting network stays low
maximum degree increases increasing population size clearly population
size increases hubs scale free network receive preferential attachment
correspondingly less densely connected nodes become even less densely connected
examine number times agents actually rewire number generally lies
e low percentage total number games played actually made agents
rewire random neighbor undesired proposer


fid e j ong u yttendaele uyls

w ith rewiring






degree distribution

degree distribution

n rewiring

















number agents








number agents



figure resulting network structure without rewiring left rewiring right
n c onclusion
conclusion subsection may state proposed methodology suffering
severe scalability issues setup include rewiring clearly outperformed one
include rewiring neither setup without rewiring setup rewiring suffer
severely increasing number agents
varying proportion good examples fs agents
section investigate behavior proposed methodology proportion
good examples population e fs agents strategy varied remainder
population consists dsh dsr agents equal proportions experimented
number population sizes ranging
since population size rather similar restrict graphically
reporting analyzing experiments agents remainder
section selection remaining given table specifically setup without rewiring setup rewiring report population size pop percentage fs
agents used fs average number games per agent needed converge games average learned strategy strat average performance perf finally maximum number
connections single agent agents network netw discuss
reported table population sizes highly similar
population size agents
p oint c onvergence
setup without rewiring figure left requires games per agent converge
proportion fs agents reaches around required number games decreases
although great deal uncertainty introducing rewiring figure right yields
much better number games required per agent hardly exceeds number
decreases steadily increasing proportion population fs agent


fil earning r agreement c ontinuous u ltimatum g ame

n rewiring
pop

fs



w ith rewiring

games

strat

perf

netw

pop

fs



























































































games

strat

perf

netw







































































table summary experiments proportion fs agents varied
details additional see main text
w ith rewiring








games per agent

games per agent

n rewiring



















percentage fs agents


percentage fs agents

figure games per agent convergence without rewiring left rewiring right

l earned trategy
interestingly population consisting ds agents tends converge offering accepting
lowest amount possible setup use rewiring figure left well
setup figure right explained ds agents tend adapt
strategies downward easily upward thus two ds agents approximately
strategy may slowly pull others strategy downward many ds agents
probability happens increases adding fs agents population different
behavior two setups setup without rewiring difficulties moving away lowest
amount possible sufficient number fs agents e population
average learned strategy reflect ds agents move towards strategy dictated fs
agents rewiring convincingly better even fs agents ds agents
average converge towards offering accepting amount dictated agents e


fid e j ong u yttendaele uyls

n rewiring

w ith rewiring

average converged strategy

average converged strategy





















percentage fs agents








percentage fs agents

figure average learned strategy without rewiring left rewiring right
w ith rewiring

performance convergence

performance convergence

n rewiring



























percentage fs agents


percentage fs agents

figure final performance without rewiring left rewiring right
p erformance
observations concerning learned strategy reported reflected performance
collective agents setup without rewiring figure left performance decreases
initially increasing proportion fs agents ds agents refuse adapt dictated
strategy proportion fs agents becomes large enough ds agents start picking
strategy resulting increasing performance setup rewiring figure right
better performance increases increasing number fs agents even though average
learned strategy close every proportion fs agents low proportions fs agents
still display less performance higher proportions may require additional explanation
note box plot figure shows distribution average strategy repeated
simulations e strategy distribution within single simulation
thus even though average strategy single simulation close
still variance low number fs agents variance prominently caused inertia


fil earning r agreement c ontinuous u ltimatum g ame

w ith rewiring

degree distribution

degree distribution

n rewiring














percentage fs agents















percentage fs agents

figure resulting network structure without rewiring left rewiring right
e ds agents directly connected fs agent implies need learn
desired strategy neighboring agents learning especially rewiring
may two agents playing together compatible neighbors
yet
r esulting n etwork tructure
clearly network structure setup without rewiring figure left influenced
varying proportion fs agents rewiring used figure right observe
interesting phenomenon closely related observations number
times agents actually rewire generally lies even though low number
affect network structure useful way low proportion fs agents large
tendency increased preferential attachment fs agents instance single
agent connects agents increasing proportion fs agents
maximum degree network decreases finally closely resembles original network
clearly presence examples desired strategy ds agents attempt connect
agents provide examples interesting useful emergent behavior
n c onclusion
compare obtained population agents
population sizes reported table see highly similar conclusion
subsection may state setup rewiring severe difficulties converging
desired example proportion fs agents providing example low e g half
population consisting examples half learn desired behavior setup
rewiring absolutely converging desired strategy even
low proportion fs agents cases completely omitting examples leads agents
converging individually rational solution caused artifact learning method
used e mentioned two cala trying learn others strategy tend driven
downward lowest value allowed


fid e j ong u yttendaele uyls

discussion
presented previous section suggest mechanisms lead cooperative solutions social dilemmas discrete set strategies e g scale free networks rewiring
lead agreement social dilemmas continuous strategy space section however trivial issue precisely discuss number mechanisms
enhance agents abilities reach cooperation social dilemmas discrete strategy sets
directly enhance agents abilities reach agreement continuous strategy spaces
empirically analyze case
reputation
reputation one main concepts used behavioral economics explain fairness
emerges e g bowles et al fehr basically assumed interactions
people lead expectations concerning future interactions expectations may positive
negative may kept oneself actually shared peers
work closely related work nowak et al reputation deters agents
accepting low offers ultimatum game information spread leading
agents receiving low offers return agents refuse accept low offers
provide high offers thus nowak et al argue population goes toward providing
accepting high offers however note shared strategy e agreement
ultimatum game yields expected payoff amount stake agents thus
reputation may indeed help agents decide strategy play others preference
playing cooperatively e providing high offers directly reputation
preading r eputation
study effects reputation optionally adding second network system
interaction network consider reputation network scale free contrast
interaction network however reputation network assumed static agents truthful
concerning reputation making unnecessary agents consider rewiring note two agents
sharing reputation information may may connected well interaction network
consequence two agents playing ultimatum game may may share reputation
information effect every ultimatum game responding agent may
broadcast reputation information neighbors reputation network information sent
responder concerns offer done proposer information
guaranteed correct agents receive information probability
pij


h



distance sender potential receiver j reputation network
thus reputation information may travel h hops decreasing probability per hop
simulations set h note relatively small networks implies
reputation information essentially public
note reputation information may helpful allow agents something
information work nowak et al instance reputation others used
agents determine offer others given observation reputation used


fil earning r agreement c ontinuous u ltimatum g ame

way necessarily promote cooperative strategies see fact
already use cala determine agents offer want reputation affect
something else agents strategies discuss number ways agents may use
reputation taken literature e interacting preferred neighbor
reputation facilitate voluntary participation
u sing r eputation
without reputation agents play random neighbor interaction network reputation
may used make agents prefer interacting specific neighbors chiang discusses
strategies fairness could evolve dominant agents allowed choose preferred partners
play chiang allows agents select partners helped agent previously
determine preferred partner use heuristic proposed santos et al
e agent prefers playing relative cooperators help obtaining high payoff
responder thus probability agent plays agent j ni ni set
agent neighbors
sj si
pij p

kni sk
si sj sk agents current strategies agents estimates
reputation previous experience
two first number times agent receives
information agent j ni may rather low especially many agents even
agents observe around reputation information received agents actually concerned one neighbors may addressed making reputation
network identical interaction network neighbor relations networks identical however may seen considerable abstraction second probability agent
information concerning neighbors low need specify default values j
clearly default value often wrong right unless use centralized mechanism
estimate instance current average population strategy
simulations
mechanism place perform experiments e vary
population size agents proportion fs agents steps
statistical analysis reveals significant difference setup uses reputation setup
analyze see expected agents almost
need resort default values neighbors strategies thus average reputation
system often change probabilities certain neighbors selected
reputation rewiring
seen rewiring works well without reputation e purely agents
experience adding reputation may beneficial agents longer need interact
allowed unwire thus agents may increase preference
certain others reputation information e amount offered certain agent propagates
static reputation network allowing agents receiving information potentially
unwire one neighbors consider neighbors behavior undesirable
rewiring mechanism used detailed e equation allow responder


fid e j ong u yttendaele uyls

ultimatum game broadcast reputation information reputation network
maximum h hops
perform experiments significant
difference main analyze number times agents actually rewired
number average increases factor respect setup reputation
used e reported however increase increase performance average agents neighbors thus generally receive reputation information concerning
neighbor absence reputation would play soon anyway
volunteering
according existing human fairness e g boyd mathew hauert et al
sigmund et al mechanism volunteering may contribute reaching cooperation
games two strategies mechanism volunteering consists allowing players
participate certain games enabling fall back safe side income depend
players strategies risk averse optional participation prevent exploiters
gaining upper hand left empty handed cooperative players preferring
participate clearly side income must carefully selected agents encouraged
participate population sufficiently cooperative experiments volunteering indeed
allows collective players spend time happy state boyd mathew
players cooperative
biggest applying volunteering basically introduce yet another
social dilemma agent may refrain participating make statement
agent may convince agent become social future make
statement agent must refuse expected positive payoff ultimatum game randomly
assigned roles expected payoff positive nonetheless study whether volunteering
promotes agreement games continuous strategy spaces use heuristic
proposed santos et al already applied mechanisms
agent thinks agent j relative cooperator agrees playing
agents agree game played prevent agents playing game
agents see relative cooperator already playing strategy
introduce probability games played anyway even one agents want
note reputation may used may allow agents estimate whether one
neighbors relative cooperator without play neighbor
unfortunately experimental point agents volunteering without
reputation severe difficulties establishing common strategy uyttendaele measuring performance see around games played
games played performance similar setup rewiring e g may
expected two agents usually agree play strategies similar reason
agents converge properly quite simple avoid playing agents
different therefore learn behave way similar others
general discussion
general may state mechanism rewiring clearly good balance
allowing agents play preferred neighbors one hand forcing agents


fil earning r agreement c ontinuous u ltimatum g ame

learn different hand additions discussed allow
agents selective e much influence play
may interest individual agents generally leads agents playing others
different instead learning others required obtain convergence
agreement

conclusion
argue mechanisms thought allow humans fair satisfactory solutions
social dilemmas may useful multi agent systems many multi agent systems need
address tasks contain elements social dilemmas e g resource allocation chevaleyre et al
existing work concerning human inspired fairness multi agent systems generally restricted discrete strategy sets usually two strategies one deemed
cooperative e desirable however many real world applications multi agent systems pose
social dilemmas require strategy taken continuous strategy space rather discrete strategy set observed traditional concept cooperation trivially applicable
continuous strategy spaces especially since longer feasible manually label certain
strategy cooperative absolute manner certain strategy may cooperative certain
culture whereas may defective naive another thus cooperation relative rather
absolute concept continuous strategy spaces
propose concept agreement introduced statistical physics dallasta et al
may used alternative cooperation discuss emergence agreement
continuous strategy spaces learning agents play pairwise ultimatum games
scale free interaction network possibility rewire network ultimatum
game two agents agree offering agent offers least minimal amount satisfies
responding agent case two agents cooperate thus population agents
agree many random pairwise games agents converge strategy without
external influences shared strategy sufficient external influences e g preference
dictated humans agents adapt dictated strategy even already agreeing
completely different strategy propose methodology continuous action learning
automata interactions scale free networks rewiring networks aimed allowing
agents reach agreement set experiments investigates usability methodology
conclusion give four statements proposed methodology able establish agreement common strategy especially agents given option rewire network
interaction humans playing ultimatum game reach agreement approximately
oosterbeek et al without rewiring agents worse generally games
successful rewiring well humans thus games discrete strategy set
rewiring greatly enhances agents abilities reach agreement without compromising scalefree network structure indicates interactions scale free networks well rewiring
networks plausible mechanisms making agents reach agreement comparison
methodologies reported related work e g santos et al b methodology facilitates
convergence low number games per agent needed e g instead
indicates continuous action learning automata satisfactory aiming
allowing agents learn relatively low number examples performance
collective seriously influenced size clearly influenced characteristics


fid e j ong u yttendaele uyls

scale free self similar network concepts reputation volunteering
reported facilitate cooperative outcomes discrete strategy games seem
additional benefits continuous strategy spaces
although ultimatum game one example social dilemma core difficulty
present social dilemmas selecting individually rational action optimize ones
payoff actually may hurt payoff ultimatum game caused fact
may play proposer someone would rather go home empty handed
accept deal perceived unfair similar fairness related may arise
interactions e g bargaining division reward resource allocation chevaleyre
et al endriss many multi agent systems need allocate resources explicitly
assigned task implicitly instance multiple agents share certain limited
amount computation time thus fair division important area recently
receiving increasing attention multi agent systems community endriss humans
often display adequate immediate ability come fair division accepted
definitely pay allow agents learn imitate human strategies
examined task may executed

acknowledgments
authors wish thank anonymous referees valuable contributions steven de jong
funded breedtestrategie programme maastricht university

references
axelrod r dissemination culture model local convergence global
polarization journal conflict resolution
barabasi l albert r emergence scaling random networks science

bearden j n ultimatum bargaining experiments state art ssrn elibrary
bowles boyd r fehr e gintis h homo reciprocans initiative
origins dimensions policy implications reciprocal fairness advances complex
systems
boyd r mathew narrow road cooperation science
cameron l raising stakes ultimatum game evidence indonesia journal
economic inquiry
chevaleyre dunne p endriss u lang j lematre maudet n padget j phelps
rodriguez aguilar j sousa p issues multiagent resource allocation informatica
chiang path toward fairness preferential association evolution strategies ultimatum game rationality society


fil earning r agreement c ontinuous u ltimatum g ame

dallasta l baronchelli barrat loreto v agreement dynamics smallworld networks europhysics letters pp
dannenberg riechmann sturm b vogt c inequity aversion individual
behavior public good games experimental investigation ssrn elibrary
de jong tuyls k learning cooperate public goods interactions presented
eumas workshop bath uk december
de jong tuyls k verbeeck k artificial agents learning human fairness
proceedings international joint conference autonomous agents multi agent
systems aamas
de jong tuyls k verbeeck k b fairness multi agent systems knowledge
engineering review
de jong tuyls k verbeeck k roos n c priority awareness towards computational model human fairness multi agent systems adaptive agents multi agent
systems iii adaptation multi agent learning
endriss u fair division tutorial international conference autonomous agents
multi agent systems aamas
fehr e dont lose reputation nature
fehr e schmidt k theory fairness competition cooperation quarterly
journal economics
gintis h game theory evolving centered introduction modeling strategic
interaction princeton university press princeton usa
grosskopf b reinforcement directional learning ultimatum game responder competition experimental economics
gueth w schmittberger r schwarze b experimental analysis ultimatum
bargaining journal economic behavior organization
hauert c traulsen brandt h nowak sigmund k via freedom
coercion emergence costly punishment science
henrich j boyd r bowles camerer c fehr e gintis h foundations
human sociality economic experiments ethnographic evidence fifteen small scale
societies oxford university press oxford uk
maynard smith j evolution theory games cambridge university press
maynard smith j price g r logic animal conflict nature
nowak page k sigmund k fairness versus reason ultimatum
game science


fid e j ong u yttendaele uyls

oosterbeek h sloof r van de kuilen g cultural differences ultimatum game
experiments evidence meta analysis experimental economics
peters r evolutionary stability ultimatum game group decision negotiation

roth e prasnikar v okuno fujiwara zamir bargaining market
behavior jerusalem ljubljana pittsburgh tokyo experimental study american
economic review
santos f c pacheco j lenaerts cooperation prevails individuals
adjust social ties plos comput biol
santos f c pacheco j lenaerts b evolutionary dynamics social dilemmas
structured heterogeneous populations proc natl acad sci usa
selten r stoecker r end behavior sequences finite prisoners dilemma supergames learning theory journal economic behavior organization

sigmund k hauert c nowak reward punishment proceedings
national academy sciences
slonim r roth learning high stakes ulitmatum games experiment
slovak republic econometrica
sonnegard j determination first movers sequential bargaining games experimental study journal economic psychology
sutton r barto g reinforcement learning introduction mit press
cambridge bradford book
thathachar l sastry p networks learning automata techniques
online stochastic optimization kluwer academic publishers dordrecht netherlands
uyttendaele fairness agreement complex networks masters thesis micc
maastricht university




