Journal Artificial Intelligence Research 33 (2008) 79-107

Submitted 07/07; published 09/08

Use Automatically Acquired Examples
All-Nouns Word Sense Disambiguation
David Martinez

davidm@csse.unimelb.edu.au

University Melbourne
3010, Melbourne, Australia

Oier Lopez de Lacalle

oier.lopezdelacalle@ehu.es

University Basque Country
20018, Donostia, Basque Country

Eneko Agirre

e.agirre@ehu.es

University Basque Country
20018, Donostia, Basque Country

Abstract
article focuses Word Sense Disambiguation (WSD), Natural Language Processing task thought important many Language Technology
applications, Information Retrieval, Information Extraction, Machine Translation. One main issues preventing deployment WSD technology lack
training examples Machine Learning systems, known Knowledge Acquisition Bottleneck. method shown work small samples words
automatic acquisition examples. previously shown one
promising example acquisition methods scales produces freely available database
150 million examples Web snippets polysemous nouns WordNet.
paper focuses issues arise using examples, alone addition
manually tagged examples, train supervised WSD system nouns. extensive
evaluation lexical-sample all-words Senseval benchmarks shows
able improve commonly used baselines achieve top-rank performance.
good use prior distributions senses proved crucial factor.

1. Introduction
paper devoted Word Sense Disambiguation (WSD) task Natural Language
Processing (NLP). goal task determine senses words
appear context. instance, given sentence took money bank.,
focus word bank, goal would identify intended sense,
context would financial sense, instead possibilities edge
river sense. senses defined dictionary, knowledge-base ontology.
task defined intermediate step towards natural language understanding.
construction efficient algorithms WSD would benefit many NLP applications
Machine Translation (MT), Information Retrieval (IR) systems (Resnik, 2006).
instance, MT system translate previous example French, would
need choose among possible translations word bank. word
translated banque used financial sense (as example),
rive used edge river sense. See work Vickrey, Biewald,
c
2008
AI Access Foundation. rights reserved.

fiMartinez, Lopez de Lacalle & Agirre

Teyssier, Koller (2005) recent evaluation cross-lingual WSD MT. IR
engines, would useful determine sense word query
order retrieve relevant documents, specially working multilingual documents
Cross-Language Information Retrieval (CLIR), IR scenarios recall key
performance factor, retrieving images captions. evidence favor
using WSD IR gathered lately (Kim, Seo, & Rim, 2004; Liu, Liu, Yu, & Meng,
2004; Stevenson & Clough, 2004; Vossen, Rigau, Alegra, Agirre, Farwell, & Fuentes, 2006).
WSD techniques fill important role context Semantic Web.
Web grown focusing human communication, rather automatic processing.
Semantic Web vision automatic agents working information
Web semantic level, achieving interoperability use common terminologies
ontologies (Daconta, Obrst, & Smith, 2005). Unfortunately information
Web unstructured textual form. task linking terms texts
concepts reference ontology paramount Semantic Web.
Narrower domains Biomedicine calling WSD techniques. Unified
Medical Language System (UMLS) (Humphreys, Lindberg, Schoolman, & Barnett, 1998)
one extensive ontologies field, studies mapping terms medical
documents resource reported high levels ambiguity, calls WSD
technology (Weeber, Mork, & Aronson, 2001).
WSD received attention many groups researchers, general NLP books
dedicating separate chapters WSD (Manning & Schutze, 1999; Jurafsky & Martin, 2000;
Dale, Moisl, & Somers, 2000), special issues WSD NLP journals (Ide & Veronis,
1998; Edmonds & Kilgarriff, 2002), books devoted specifically issue (Ravin &
Leacock, 2001; Stevenson, 2003; Agirre & Edmonds, 2006). interested reader start
dedicated chapter Manning Schutze (1999) WSD book (Agirre
& Edmonds, 2006). widespread interest motivated Senseval initiative1 ,
joined different research groups common WSD evaluation framework since 1998.
goal follow example successful competitive evaluations, DUC
(Document Understanding Conference) TREC (Text Retrieval Conference).
WSD systems classified according knowledge use build models, derived different resources corpora, dictionaries, ontologies.
Another distinction drawn corpus-based systems, distinguishing
rely hand-tagged corpora (supervised systems), require resource (unsupervised systems). distinction important effort required
hand-tag senses high, would costly obtain tagged examples word
senses languages, estimations show (Mihalcea & Chklovski, 2003). spite
drawback (referred knowledge acquisition bottleneck), recent
efforts devoted improvement supervised systems, ones
obtain highest performance, even current low amounts training data.
systems rely sophisticated Machine Learning (ML) algorithms construct
models based features extracted training examples.
Alternatively, Senseval defines two kinds WSD tasks: lexical-sample all-words.
lexical-sample task systems need disambiguate specific occurrences handful
1. http://www.senseval.org

80

fiOn Use Automatically Acquired Examples All-Nouns WSD

words relatively large numbers training examples provided (more
100 examples cases). all-words task, training data provided, testing
done whole documents. Systems need tag content words occurring texts,
even small amounts external training data available.
analysis results English lexical-sample exercise third edition
Senseval (Mihalcea & Edmonds, 2004) suggested plateau performance
reached ML methods. task, systems relatively large amounts
training data, many systems top, performing close other.
systems able significantly improve baselines attained accuracies
70% (Mihalcea, Chklovski, & Killgariff, 2004).
case different all-words task (Snyder & Palmer, 2004), supervised
systems performed best. used training examples Semcor (Miller, Leacock,
Tengi, & Bunker, 1993), sizable all-words sense-tagged corpus time
writing paper. scarcity examples use test documents corpora
unrelated Semcor heavily affected performance, systems scored
baseline method assigning frequent sense Semcor. order useful
NLP applications, WSD systems address knowledge acquisition bottleneck
(or least significant part) word types, evaluated all-words tasks.
Lexical-sample tasks useful evaluating WSD systems ideal conditions (i.e.
regarding availability training data), show systems scalable
words vocabulary. work use lexical-sample task order
adjust parameters system, main evaluation all-words task.
experiments designed accordingly: lexical-sample tests show empirical evidence
specific parameters, all-words evaluation compares systems state
art.
article, explore method alleviate knowledge acquisition bottleneck
large scale. use WordNet (Fellbaum, 1998) automatically acquire examples
Web. seminal work Leacock, Chodorow, Miller (1998) showed approach
promising, good results small sample nouns. works field
automatic acquisition examples focused exploring different approaches
acquisition process (Agirre, Ansa, Martinez, & Hovy, 2000; Mihalcea, 2002; Cuadros, Padro,
& Rigau, 2006), straightforward application WSD. explorations typically
required costly querying Web, thus tried limited number variations
handful words. approach different spirit: want go whole
process nouns, acquisition examples use WSD
thorough evaluation Senseval 2 lexical-sample Senseval 3 all-words datasets.
comes cost exploring different possibilities step,
advantage showing results extensive, limited small set
nouns.
reasons, given prior work acquisition techniques, use
efficient effective example acquisition method according independent experiments
performed Agirre et al. (2000) Cuadros et al. (2006). focus paper thus
issues arise using examples training data supervised ML
system. paper show automatically acquired examples effectively
81

fiMartinez, Lopez de Lacalle & Agirre

used without pre-existing data, deciding amount examples use
sense (the prior distribution) key issue.
objectives paper show existing methods acquire examples
Web scale-up nouns, study issues arise examples
used training data all-nouns WSD system. goal build stateof-the-art WSD system nouns using automatically retrieved examples.
Given cost large-scale example acquisition, decided limit scope
work nouns. think noun disambiguation useful tool
many applications, specially IR tasks mentioned above. method easily
adapted verbs adjectives (Cuadros et al., 2006), plan pursue line
future.
work reported partially published two previous conference papers.
method automatic acquisition examples described Agirre Lopez
de Lacalle (2004). first try application examples Word Sense Disambiguation presented Agirre Martinez (2004b). paper present global
view whole system, together thorough evaluation, shows
automatically acquired examples used build state-of-the-art WSD systems
variety settings.
article structured follows. introduction, related work knowledge acquisition bottleneck WSD described Section 2, focus automatic
example acquisition. Section 3 introduces method automatically build SenseCorpus,
automatically acquired examples WordNet senses. Section 4 describes experimental setting. Section 5 explores factors use SenseCorpus evaluates
lexical-sample task. final systems thoroughly evaluated all-nouns
task Section 6. Finally, Section 7 provides discussion, conclusions
work outlined Section 8.

2. Related Work
construction WSD systems applicable words goal many research initiatives. section describe related work looks ways
alleviate knowledge acquisition bottleneck using following techniques: bootstrapping, active learning, parallel corpora, automatic acquisition examples acquisition
topic signatures. Sections 5 6, evaluate proposed system public datasets,
review best performing systems literature.
Bootstrapping techniques consist algorithms learn instances labeled
data (seeds) big set unlabeled examples. Among approaches, highlight
co-training (Blum & Mitchell, 1998) derivatives (Collins & Singer, 1999; Abney,
2002). techniques appropriate WSD NLP tasks
wide availability untagged data scarcity tagged data. However,
systems shown perform well fine-grained WSD. well-known work,
Yarowsky (1995) applied iterative bootstrapping process induce classifier based
Decision Lists. minimum set seed examples, disambiguation results comparable
supervised methods obtained limited set binary sense distinctions,
success extended fine-grained senses.
82

fiOn Use Automatically Acquired Examples All-Nouns WSD

Recent work bootstrapping applied WSD reported Mihalcea (2004)
Pham, Ng, Lee (2005). former, use unlabeled data significantly
increases performance lexical-sample system. latter, Pham et al. apply
WSD classifier all-words task Senseval-2, targeting words threshold
frequency Semcor WSJ corpora. observe slight increase accuracy
relying unlabeled data.
Active learning used choose informative examples hand-tagging, order
reduce manual cost. one works directly applied WSD, Fujii, Inui, Tokunaga, Tanaka (1998) used selective sampling acquisition examples
disambiguation verb senses, iterative process human taggers. informative
examples chosen following two criteria: maximum number neighbors unsupervised
data, minimum similarity supervised example set. Another active learning
approach Open Mind Word Expert (Mihalcea & Chklovski, 2003), project
collect sense-tagged examples Web users. system selects examples
tagged applying selective sampling method based two different classifiers, choosing
unlabeled examples disagreement. collected data used
Senseval-3 English lexical-sample task.
Parallel corpora another alternative avoid need hand-tagged data. Recently
Chan Ng (2005) built classifier English-Chinese parallel corpora. grouped
senses share Chinese translation, occurrences word
English side parallel corpora considered disambiguated sense
tagged appropriate Chinese translations. system successfully evaluated
all-words task Senseval-2. However, parallel corpora expensive resource
obtain target words. related approach use monolingual corpora second
language use bilingual dictionaries translate training data (Wang & Carroll,
2005). Instead using bilingual dictionaries, Wang Martinez (2006) applied machine
translation text snippets foreign languages back English achieved good results
English lexical-sample WSD.
automatic acquisition training examples, external lexical resource (WordNet,
instance) sense-tagged corpus used obtain new examples large
untagged corpus (e.g. Web). Leacock et al. (1998) present method obtain sensetagged examples using monosemous relatives WordNet. approach based
early work (cf. Section 3). algorithm, Leacock et al. (1998) retrieve number
examples per sense, give preference monosemous relatives consist
multiword containing target word. experiment evaluated 14 nouns
coarse sense-granularity senses. results showed monosemous
corpus provided precision close hand-tagged data.
Another automatic acquisition approach (Mihalcea & Moldovan, 1999) used information
WordNet (e.g. monosemous synonyms glosses) construct queries, later
fed Altavista2 search engine. Four procedures used sequentially, decreasing
order precision, increasing levels coverage. Results evaluated hand,
showing 91% examples correctly retrieved among set 1,080 instances
120 word senses. However, corpus resulting experiment used
2. http://www.altavista.com

83

fiMartinez, Lopez de Lacalle & Agirre

train real WSD system. Agirre Martinez (2000), early precursor work
presented here, tried apply technique train WSD system unsatisfactory
results. authors concluded examples correct,
somehow mislead ML classifier, providing biased features.
related work, Mihalcea (2002) generated sense tagged corpus (GenCor) using
set seeds consisting sense-tagged examples four sources: (i) Semcor, (ii) WordNet,
(iii) examples created using method above, (iv) hand-tagged examples
sources (e.g. Senseval-2 corpus). means iterative process, system obtained
new seeds retrieved examples. total, corpus 160,000 examples
gathered. However, evaluation carried lexical-sample task, showing
method useful subset Senseval-2 testing words (results 5 words
provided), without analysing sources performance gain. Even
work presented uses techniques, work seen extension
limited study, sense evaluate all-words tasks.
previous works focused use two different kinds techniques
automatic acquisition examples, namely, use monosemous relatives alone (Leacock
et al., 1998) use combination monosemous relatives glosses (Mihalcea
& Moldovan, 1999; Mihalcea, 2002). cases examples directly used feed
supervised ML WSD system, limited evaluation indication
methods scale-up. Unfortunately, direct comparison alternative methods
parameters automatically acquire examples WSD exists, see preference
use Web, existing corpora would contain occurrences monosemous
terms gloss fragments.
closely related area automatic acquisition examples WSD
enriching knowledge bases topic signatures. instance, Agirre et al. (2000)
Agirre, Ansa, Martinez, Hovy (2001) used combined monosemous-relatives plus
glosses strategy query Altavista, retrieve original documents build lists related
words word sense (so called topic signatures). topic signatures difficult
evaluate hand, applied context vectors WSD straightforward way.
Note authors train ML algorithm, rather combined examples
one vector per sense. showed using Web compared favorably using
fixed corpus, computationally costly: system first needs query search
engine retrieve original document order get example sense.
alternative, Agirre Lopez de Lacalle (2004) showed possible scale
gather examples nouns WordNet query limited using monosemous
relatives snippets returned Google used instead whole document.
point, Cuadros et al. (2006) set systematic framework evaluation
different parameters affect construction topic signatures, including
methods automatically acquire examples. study explores wide range querying
strategies (monosemous synonyms, monosemous relatives different distances, glosses,
combined using either operators) particular corpus (the British National Corpus) Web. best results obtained using Infomap3 British
National Corpus monosemous relatives method Web (Agirre & Lopez de
3. http://infomap-nlp.sourceforge.net

84

fiOn Use Automatically Acquired Examples All-Nouns WSD

Lacalle, 2004). Contrary method, Infomap returns lists related words,
thus used retrieve training examples. results confirmed
experiments reported Cuadros Rigau (2006).
all, literature shows using monosemous relatives snippets
Web (Agirre & Lopez de Lacalle, 2004) provides method automatically acquire examples
scales nouns WordNet, provides topic signatures better quality
alternative methods. explain examples acquired.

3. Building Sense-Tagged Corpus Nouns Automatically
order build corpus (which refer SenseCorpus) acquired 1,000
Google snippets monosemous noun WordNet 1.6 (including multiwords, e.g.
church building). Then, word sense ambiguous noun, gathered examples monosemous relatives (e.g. sense #2 church, gather examples
relative church building). way collect examples simply querying corpus
word string words (e.g. church building). method inspired
work Leacock et al. (1998) and, already mentioned Section 2, shown
efficient effective experiments topic signature acquisition.
basic assumption method given word sense target word,
monosemous synonym word sense, examples synonym
similar target word sense, could therefore used train
classifier target word sense. idea , lesser extent, applied
monosemous relatives, direct hyponyms, direct hypernyms, siblings, indirect
hyponyms, etc. expected reliability decreases distance hierarchy
monosemous relative target word sense.
actual method build SenseCorpus following. collected examples
Web monosemous relatives. relatives associated number
(type), correlates roughly distance target word, indicates
relevance: higher type, less reliable relative. Synonyms type 0, direct
hyponyms get type 1, distant hyponyms receive type number equal distance
target sense. Direct hypernyms get type 2, general
target sense, thus introduce noise direct hyponyms. decided
include less reliable siblings, type 3. sophisticated schemes could tried,
using WordNet similarity weight distance target relative
word. However, chose approach capture notion distance simplicity,
avoid testing many parameters. sample monosemous relatives different
senses church, together sense inventory WordNet 1.7 shown Figure 1.
following subsections describe step step method construct
corpus. First explain acquisition highest possible amount examples per
sense, explain different ways limit number examples per sense
better performance.
3.1 Collecting Examples
method collect examples previously published (Agirre & Lopez de
Lacalle, 2004), comprises following steps:
85

fiMartinez, Lopez de Lacalle & Agirre

Sense inventory (church)
Sense 1: group Christians; group professing Christian doctrine belief.
Sense 2: place public (especially Christian) worship.
Sense 3: service conducted church.
Monosemous relatives different senses (of church)
Synonyms (Type 0): church building (sense 2), church service (sense 3) ...
Direct hyponyms (Type 1): Protestant Church (sense 1), Coptic Church (sense 1) ...
Direct hypernyms (Type 2): house prayer (sense 2), religious service (sense 3) ...
Distant hyponyms (Type 2,3,4...):
(sense 1)...

Greek Church (sense 1), Western Church

Siblings (Type 3): Hebraism (sense 2), synagogue (sense 2) ...

Figure 1: Sense inventory sample monosemous relatives WordNet 1.7 church.

1: query Google4 monosemous relatives sense, extract
snippets returned search engine. snippets used (up 1,000),
dropped next step.
2: try detect full meaningful sentences snippets contain target
word. first detect sentence boundaries snippet extract sentence
encloses target word. sentences filtered out, according following
criteria: length shorter 6 words, non-alphanumeric characters words
divided two, words uppercase lowercase.
3: automatically acquired examples contain monosemous relative target
word. order use examples train classifiers, monosemous relative (which
multiword term) substituted target word. case monosemous
relative multiword contains target word (e.g. Protestant Church church)
choose substitute, Protestant, instance, useful feature
first sense church. tried alternatives, Section 5 show
obtain slightly better results substitution applied multiwords.
4: given word sense, collect desired number examples (see following
section) order type: first retrieve examples type 0, type 1, etc.
type 3 necessary examples obtained. collect examples type 4
upwards. make distinctions relatives type. Contrary
Leacock et al. (1998) give preference multiword relatives containing
target word.
all, acquired around 150 million examples nouns WordNet using
technique, publicly available5 .
4. use off-line XML interface kindly provided Google research.
5. http://ixa.si.ehu.es/Ixa/resources/sensecorpus.

86

fiOn Use Automatically Acquired Examples All-Nouns WSD

3.2 Number Examples per Sense (Prior)
Previous work (Agirre & Martinez, 2000) reported distribution number
examples per word sense (prior short) strong influence quality
results. is, results degrade significantly whenever training testing samples
different distributions senses. shown type-based approach
predicts majority sense word domain provide good performance
(McCarthy, Koeling, Weeds, & Carroll, 2004).
extracting examples automatically, decide many examples
use sense. order test impact prior, different settings
tried:
prior: take equal amount examples sense.
Web prior: take examples gathered Web.
Automatic ranking: number examples given ranking obtained following
method McCarthy et al. (2004).
Sense-tagged prior: take number examples proportional relative frequency word senses hand-tagged corpus.
first method assumes uniform priors. second assumes number
monosemous relatives occurrences correlated sense importance, is,
frequent senses would occurrences monosemous relatives. fourth
method uses information hand-tagged corpus, typically Semcor. Note
last kind prior requires hand-tagged data, rest (including third method
below) completely unsupervised.
third method sophisticated deserves clarification. McCarthy et al. (2004) present method acquire sense priors automatically domain
corpus. two-step process. first step corpus-based method, given
target word builds list contextually similar words (Lin, 1998) weights.
case, co-occurrence data gathered British National Corpus. instance,
given target word authority, list topmost contextually similar words include government, police, official agency 6 . second step ranks senses
target word, depending scores WordNet-based similarity metric (Patwardhan
& Pedersen, 2003) relative list contextually similar words. Following
example, pairwise WordNet similarity authority government greater
sense 5 authority, evidence sense prominence corpus.
pairwise similarity scores added, yielding ranking 7 senses authority.
Table 2 shows column named Auto.MR normalized scores assigned
senses authority according technique.
Table 1 shows number examples per type (0,1,...) acquired church
following Semcor prior. last column gives number examples Semcor. Note
number examples sometimes smaller 1,000 (maximum number snippets
returned Google one query). due rare monosemous relatives,
6. Actual list words taken demo http://www.cs.ualberta.ca/~lindek/demos/depsim.htm.

87

fiMartinez, Lopez de Lacalle & Agirre

Sense
church#1
church#2
church#3
Overall

0
0
306
147
453

1
476
100
0
576

2
524
561
20
1,105

3
0
0
0
0

Total
1,000
967
167
2,134

Semcor
60
58
10
128

Table 1: Examples per type (0,1,2,3) acquired Web three senses
church following Semcor prior, total number examples Semcor.

Sense
authority#1
authority#2
authority#3
authority#4
authority#5
authority#6
authority#7
Overall

Semcor
#ex
18
5
3
2
1
1
0
30

%
60.0
16.7
10.0
6.7
3.3
3.3
0.0
100.0

Web PR
#ex
%
338
0.5
44932
66.4
10798
16.0
886
1.3
6526
9.6
72
0.1
4106
6.1
67657
100.0

Auto.
#ex
138
75
93
67
205
71
67
716

SenseCorpus
MR
Semcor PR
%
#ex
%
19.3
338
33.7
10.5
277
27.6
13.0
166
16.6
9.4
111
11.1
28.6
55
5.5
9.9
55
5.5
9.4
1
0.1
100.0
1003
100.0

Semcor
#ex
324
90
54
36
18
18
1
541

MR
%
59.9
16.6
10.0
6.7
3.3
3.3
0.2
100.0

Senseval
test
#ex
%
37
37.4
17
17.2
1
1.0
0
0.0
34
34.3
10
10.1
0
0.0
99
100.0

Table 2: Distribution examples senses authority different corpora. PR
(proportional) MR (minimum ratio) columns correspond different ways
apply Semcor prior.

usually caused sentence extraction filtering process, discards around 50%
snippets.
way apply prior straightforward. illustration, focus
Semcor prior. first approach Semcor prior, assigned 1,000 examples
major sense Semcor, gave senses proportion examples. call
method proportional (PR). cases number examples extracted
less expected distribution senses Semcor. result, actual number
examples available would follow desired distribution.
alternative, computed, word, minimum ratio (MR) examples
available given target distribution given number examples extracted
Web. observed last approach would reflect better original prior,
cost less examples.
Table 2 presents different distributions examples authority. see
Senseval-testing Semcor distributions, together total number examples
Web (Web PR); Semcor proportional distribution (Semcor PR) minimum
ratio (Semcor MR); automatic distribution minimum ratio (Auto MR).
Getting maximum one thousand examples per monosemous relative allows get
44,932 examples second sense (Web PR column), 72 sixth sense.
88

fiOn Use Automatically Acquired Examples All-Nouns WSD

Semcor
Word
art
authority
bar
bum
chair
channel
child
church
circuit
day
detention
dyke
facility
fatigue
feeling
Average
Total

Web
prior
15,387
67,657
50,925
17,244
24,625
31,582
47,619
8,704
21,977
84,448
2,650
4,210
11,049
6,237
9,601
24,137
699,086

Automatic
prior
2,610
716
5,329
4,745
2,111
10,015
791
6,355
5,095
3,660
511
843
1,196
5,477
945
3,455
100,215

Semcor
prior
10,656
541
16,627
2,555
8,512
3,235
3,504
5,376
3,588
9,690
1,510
1,367
8,578
3,438
1,160
4,719
136,874

Semcor
Word
grip
hearth
holiday
lady
material
mouth
nation
nature
post
restraint
sense
spade
stress
yew

Web
prior
20,874
6,682
16,714
12,161
100,109
648
608
32,553
34,968
33,055
10,315
5,361
10,356
10,767

Automatic
prior
277
2,730
1,846
884
6,385
464
608
9,813
8,005
2,877
2,176
2,657
3,081
8,013

Semcor
prior
2,209
1,531
1,248
2,959
7,855
287
594
24,746
4,264
2,152
2,059
2,458
2,175
2,000

Table 3: Number examples following different sense distributions Senseval-2
nouns. Minimum ratio applied Semcor automatic priors.

sixth sense single monosemous relative, rare word hits
Google, second sense many frequent monosemous relatives.
Regarding use minimum ratio, table illustrates MR allows better
approximate distribution senses Semcor: first sense7 60% Semcor,
gets 33.7% SenseCorpus proportional Semcor prior
338 examples SenseCorpus first sense. contrast SenseCorpus
minimum ratio using Semcor assign 59.9% examples first sense.
better approximation comes cost getting 541 examples authority, contrast
1,003 PR. Note authority occurs 30 times Semcor.
table shows word distributions senses Semcor
Senseval-test important differences (sense 5 gets 3.3% 34.3% respectively), although frequent sense same. Web automatic distributions,
salient sense different Semcor, Web prior (Web PR column)
assigning 0.5% first sense. Note automatic method able detect
sense 5 salient test corpus, Semcor ranks 5th. general, distribution
discrepancies similar table observed words test
set.
conclude section, Table 3 shows number examples acquired automatically
word Senseval-2 lexical-sample following three approaches: Web prior,
Semcor prior minimum ratio, Automatic prior minimum ratio.
see retrieving examples (Web prior) get 24,137 examples average per
word; respectively 4,700 3,400 apply Semcor prior Automatic prior.
7. senses WordNet numbered according frequency Semcor, first sense
WordNet paramount frequent sense Semcor.

89

fiMartinez, Lopez de Lacalle & Agirre

3.3 Decision Lists
supervised learning method used measure quality corpus Decision Lists
(DL). simple method performs reasonably well comparison supervised
methods Senseval words (as illustrate Table 6.4), preliminary experiments showed perform better automatically retrieved examples
sophisticated methods Support Vector Machines Vector Space Model. well
known learning methods perform differently according several conditions, showed
instance Yarowsky Florian (2003), analyzed depth performance
various learning methods (including DL) WSD tasks.
think main reason DL perform better preliminary experiments
SenseCorpus noisy corpus conflicting features. Decision Lists use
single powerful feature test context make predictions, contrast
ML techniques, could make perform better corpus. Specially
all-words task, hand-tagged examples per word cases, even
sophisticate ML algorithms cannot deal problem themselves.
best systems Senseval-3 lexical-sample rely complex kernel-based methods,
all-words task top systems find external ways deal sparseness
data apply well-known methods, memory based learning decision
trees (Mihalcea & Edmonds, 2004).
DL algorithm described Yarowsky (1994). method, sense sk
highest weighted feature selected, according log-likelihood (see Formula 1).
implementation, applied simple smoothing method: cases
denominator zero, use 0.1 denominator. roughly equivalent assigning
0.1 probability mass rest senses, shown effective enough
compared complex methods (Yarowsky, 1994; Agirre & Martinez, 2004a).
P r(sk |fi )
)
j6=k P r(sj |fi )

weight(sk , ) = log( P

(1)

3.4 Feature Types
feature types extracted context grouped three main sets:
Local collocations: bigrams trigrams formed words around target.
features constituted lemmas, word-forms, PoS tags8 . local features
formed previous/posterior lemma/word-form context.
Syntactic dependencies: syntactic dependencies extracted using heuristic patterns,
regular expressions defined PoS tags around target9 . following relations used: object, subject, noun-modifier, preposition, sibling.
Topical features: extract lemmas content words whole sentence
4-word window around target. obtain salient bigrams context,
methods software described Pedersen (2001).
8. PoS tagging performed fnTBL toolkit (Ngai & Florian, 2001).
9. software kindly provided David Yarowskys group, Johns Hopkins University.

90

fiOn Use Automatically Acquired Examples All-Nouns WSD

complete feature set applied main experiments all-words Senseval3 corpus. However, initial experiments lexical-sample task local features
topical features (without salient bigrams) applied.

4. Experimental Setting
already noted introduction lexical-sample evaluations defined Senseval
realistic: relatively large amounts training examples available, drawn
corpus test examples, train test examples tagged
team. Besides, developing system handful words necessarily
show scalable. contrast, all-words evaluations provide training data.
Supervised WSD systems typically use Semcor (Miller et al., 1993) training.
corpus offers tagged examples open-class words occurring 350.000 word subset
balanced Brown corpus, tagged WordNet 1.6 senses. contrast lexicalsample, polysemous words authority get handful examples (30
case, cf. Table 2). Note test examples (from Senseval) Semcor come
different corpora thus might related different domains, topics genres.
added difficulty posed fact tagged different teams
annotators distinct institutions.
mind, designed two sets experiments: first set performed
sample nouns (lexical-sample), used develop fine-tune method
basic aspects effect kinds features importance prior.
use training examples, except measure impact priors. provide
comparison state-of-the-art systems.
second set experiment used show method scalable, useful
noun, performs state-of-the art WSD realistic setting. thus
selected apply WSD nouns running text (all-nouns). setting apply
best configurations obtained first set experiments, explore use
SenseCorpus alone, combined priors Semcor, training data
Semcor. provide comparison results state-of-the-art systems.
lexical-sample evaluation, test part Senseval-2 English lexical-sample
task chosen, consisted instances 29 nouns, tagged WordNet 1.7 senses.
advantage corpus could focus word-set enough examples
testing. Besides, different corpus, therefore evaluation realistic
made using cross-validation Semcor. order factor pre-processing
focus WSD, test examples whose senses multiwords phrasal verbs
removed. Note problematic since efficiently detected
methods preprocess.
important note training part Senseval-2 lexical-sample used
construction systems, goal test performance could achieve
minimal resources (i.e. available word). relied Senseval-2
training prior preliminary experiments local/topical features, upperbound
compare performance types priors.
all-words evaluation relied Senseval-3 all-words corpus (Snyder &
Palmer, 2004). test data task consisted 5,000 words text. data
91

fiMartinez, Lopez de Lacalle & Agirre

extracted two Wall Street Journal articles one excerpt Brown Corpus.
texts represent three different domains: editorial, news story, fiction. Overall, 2,212
words tagged WordNet 1.7.1. senses (2,081 include multiwords).
these, 695 occurrences correspond polysemous nouns part multiwords,
comprise testing set.
rest Senseval participants, added difficulty WordNet versions
coincide. therefore used one freely available mappings WordNet
versions (Daude, Padro, & Rigau, 2000) convert training material Semcor
(tagged WordNet 1.6 senses) WordNet 1.7 WordNet 1.7.1 versions (depending
target corpus). preferred use mapping rather relying
available mappings converted Semcors. knowledge, comparative evaluation
among mappings performed, Daude et al. show mapping obtained
high scores extensive manual evaluation. Note versions Semcor
available Web (other original one, tagged WordNet 1.6)
obtained using automatic mapping.
lexical-sample all-nouns settings, provide set baselines,
based frequent heuristic. heuristic known hard beat WSD,
specially unsupervised systems access priors, even
supervised systems all-nouns setting.

5. Lexical-Sample Evaluation
performed four sets experiments order study different factors, compare
performance state-of-the-art unsupervised systems Senseval-2 lexical-sample
task. First analyzed results systems using different sets local
topical features, well substituting multiwords. next experiments
devoted measure effect prior performance. that, compared
approach unsupervised systems participated Senseval-2. mentioned
introduction, results obtained lexical-sample evaluations realistic,
cannot expect hand-tagged data words target corpus.
reason report results supervised systems (which use training data).
next section all-nouns evaluation, realistic, compare supervised
systems
5.1 Local vs. Topical Features, Substitution
Previous work automatic acquisition examples (Leacock et al., 1998) reported
lower performance using local collocations formed PoS tags closed-class words.
contrast, Kohomban Lee (2005), related approach, used local features
WSD discriminated better senses. Given fact SenseCorpus
constructed automatically, contradictory results previous
works, performed initial experiment comparing results using local features, topical
features, combination both. case used SenseCorpus Senseval training
prior, distributed according MR approach, always substituting target word.
results (per word overall) given Table 4.
92

fiOn Use Automatically Acquired Examples All-Nouns WSD

Local Feats.
Word
art
authority
bar
bum
chair
channel
child
church
circuit
day
detention
dyke
facility
fatigue
feeling
grip
hearth
holiday
lady
material
mouth
nation
nature
post
restraint
sense
spade
stress
yew
Overall

Coverage
94.4
93.4
98.3
100.0
100.0
73.5
100.0
100.0
88.7
98.6
100.0
100.0
98.2
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
98.3
79.5
93.0
100.0
100.0
100.0
96.7

Precision
57.4
51.2
53.0
81.2
88.7
54.0
56.5
67.7
51.1
60.2
87.5
89.3
29.1
82.5
55.1
19.0
73.4
96.3
80.4
43.2
36.8
80.6
44.4
44.7
37.1
62.5
74.2
53.9
81.5
58.5

Recall
54.2
47.8
52.1
81.2
88.7
39.7
56.5
67.7
45.3
59.4
87.5
89.3
28.6
82.5
55.1
19.0
73.4
96.3
80.4
43.2
36.8
80.6
44.4
43.9
29.5
58.1
74.2
53.9
81.5
56.5

Topical
Feats.
Recall
45.6
43.2
55.9
87.5
88.7
53.7
55.6
51.6
54.2
54.7
87.5
89.3
21.4
82.5
60.2
38.0
75.0
96.3
73.9
44.2
38.6
80.6
39.3
40.5
37.5
37.2
72.6
46.1
81.5
56.0

Combined
Subst.
Recall
47.0
46.2
57.2
85.0
88.7
55.9
56.5
54.8
56.1
56.8
87.5
89.3
21.4
82.5
60.2
39.0
75.0
96.3
73.9
43.8
39.5
80.6
40.7
40.5
37.1
38.4
74.2
48.7
81.5
57.0

Combined
Subst.
Recall
44.9
46.2
57.2
85.0
88.7
57.4
58.9
51.6
58.0
60.4
87.5
89.3
21.4
82.5
60.2
38.0
75.0
96.3
73.9
42.9
39.5
80.6
40.7
40.5
37.1
48.8
74.2
48.7
81.5
57.5

Table 4: Results per feature type (local, topical, combination), using SenseCorpus
Senseval-2 training prior (MR). Coverage precision given local
features (topical combination full coverage). Combination shown
substitution substitution options. best recall per word given
bold.

93

fiMartinez, Lopez de Lacalle & Agirre

experiment, observed local collocations achieved best precision overall, combination features obtained best recall. Local features achieve
58.5% precision 96.7% coverage overall10 , topical combined features
full-coverage. table shows clear differences results per word, fact
known algorithms using real training data (Yarowsky & Florian, 2003).
variability another important factor focus all-words settings, large numbers
different words involved.
show results substituting monosemous relative target
word monosemous relative multiword. see results mixed,
slight overall improvement choose substitute cases.
following experiments, chose work combination features
substitution, achieved best overall recall.
5.2 Impact Prior
order evaluate acquired corpus, first task analyze impact
prior. mentioned Section 3.2, training Decision Lists examples
SenseCorpus, need decide amount examples sense (what seen
estimation prior probabilities senses).
Table 5 shows recall11 attained DL four proposed methods
estimate priors target word, plus use training part Senseval-2 lexical
sample estimate prior. Note last estimation method realistic, one
cannot expect hand-tagged data words given target corpus,
thus taken upperbound. fact presented section completeness,
used comparison systems.
results show constant improvement less informative priors
informed ones. Among three unsupervised prior estimation methods, best results
obtained automatic ranking, worst uniform distribution (no prior
column), distribution examples returned SenseCorpus (Web prior)
middle. Estimating priors hand-tagged data improves results considerably,
even target corpus estimation corpus different (Semcor), best
results overall obtained priors estimated training part Senseval2 lexical-sample dataset. results word word show word behaves differently,
well-known behavior WSD. Note priors except informed
one number words performances 10%, might indicate DL trained
SenseCorpus sensitive badly estimated priors.
Table 6 shows overall results Table 5, together obtained using
prior (prior only). results show improvement attained training
SenseCorpus prominent unsupervised priors (from 6.5 19.7 percentage
points), lower improvements (around 2.0 percentage points) priors estimated
hand-tagged corpora. results show clearly acquired corpus use10. Note due sparse data problem, test examples might feature common
training data. cases DL algorithm return result, thus coverage
lower 100%
11. results following tables given recall, coverage always 100% precision
equals recall case.

94

fiOn Use Automatically Acquired Examples All-Nouns WSD

Unsupervised
Word
art
authority
bar
bum
chair
channel
child
church
circuit
day
detention
dyke
facility
fatigue
feeling
grip
hearth
holiday
lady
material
mouth
nation
nature
post
restraint
sense
spade
stress
yew
Overall


prior
34.0
20.9
24.7
36.7
61.3
42.2
40.3
43.8
44.3
15.3
52.1
92.9
19.6
58.8
27.2
11.3
57.8
70.4
24.3
51.7
39.5
80.6
21.9
36.8
26.3
44.8
74.2
38.6
70.4
38.0

Web
prior
61.1
22.0
52.1
18.8
62.9
28.7
1.6
62.1
52.8
2.2
16.7
89.3
26.8
73.8
51.0
8.0
37.5
7.4
79.3
50.8
39.5
80.6
44.4
47.4
9.1
18.6
66.1
52.6
85.2
39.8

Autom.
ranking
45.6
40.0
26.4
57.5
69.4
30.9
34.7
49.7
49.1
12.5
87.5
80.4
22.0
75.0
42.5
28.2
60.4
72.2
23.9
52.3
46.5
80.6
34.1
47.4
31.4
41.9
85.5
27.6
77.8
43.2

Minimally-Supervised
Semcor
prior
55.6
41.8
51.6
5.0
88.7
16.2
54.0
48.4
41.5
48.0
52.1
92.9
26.8
82.5
60.2
16.0
75.0
96.3
80.4
54.2
54.4
80.6
46.7
34.2
27.3
47.7
67.7
2.6
66.7
49.8

Senseval-2
prior
44.9
46.2
57.2
85.0
88.7
57.4
58.9
51.6
58.0
60.4
87.5
89.3
21.4
82.5
60.2
38.0
75.0
96.3
73.9
42.9
39.5
80.6
40.7
40.5
37.1
48.8
74.2
48.7
81.5
57.5

Table 5: Performance (recall) SenseCorpus 29 nouns Senseval-2 lexical-sample,
using different priors train DL. Best results word bold.

ful information word senses, estimation prior extremely
important.

Prior
prior
Web prior
autom. ranking
Semcor prior
Senseval2 prior

Type
unsupervised
minimallysupervised

prior
18.3
33.3
36.1
47.8
55.6

SenseCorpus
38.0
39.8
43.2
49.8
57.5

Diff.
+19.7
+6.5
+7.1
+2.0
+1.9

Table 6: Performance (recall) nouns Senseval-2 lexical-sample. row, results
given prior own, SenseCorpus using prior, difference
both.

95

fiMartinez, Lopez de Lacalle & Agirre

Method
SenseCorpus (Semcor prior)
UNED
SenseCorpus (Autom. prior)
Kenneth Litkowski-clr-ls
Haynes-IIT2
Haynes-IIT1

Type
minimallysupervised
unsupervised

Recall
49.8
45.1
43.3
35.8
27.9
26.4

Table 7: Results nouns best minimally supervised fully unsupervised systems (in bold) compared unsupervised systems took part Senseval-2
lexical-sample.

5.3 Comparison Systems
point, important compare performance DL-based approach
systems state art. section compare best unsupervised
system (the one using Automatic ranking) minimally unsupervised system (using
Semcor prior) systems participating Senseval-2 deemed unsupervised. order results systems, used resources available
Senseval-2 competition, answers participating systems different
tasks available12 . made possible compare results test data,
set nouns occurrences.
5 systems presented Senseval-2 lexical-sample task unsupervised,
WASP-Bench system relied lexicographers hand-code information semi-automatically
(Tugwell & Kilgarriff, 2001). system use training data, uses
manually coded knowledge think falls supervised category.
results 4 systems shown Table 7. classified
UNED system (Fernandez-Amoros, Gonzalo, & Verdejo, 2001) minimally supervised.
use hand-tagged examples training, heuristics applied
system rely prior information available Semcor. distribution senses
used discard low-frequency senses, choose first sense back-off
strategy. conditions, minimally supervised system attains 49.8% recall,
nearly 5 points better.
rest systems fully unsupervised, perform significantly worse
unsupervised system.

6. Nouns Evaluation
explained introduction, main goal research develop WSD
system able tag nouns context, sample them. previous
section explored different settings system, adjusting according results
handful words lexical-sample task.
12. http://www.senseval.org

96

fiOn Use Automatically Acquired Examples All-Nouns WSD

section test SenseCorpus 695 occurrences polysemous nouns
present Senseval-3 all-words task, compare results performance
systems participated competition. present analysis results
according frequency target nouns.
developed three different systems, based SenseCorpus, different requirements external information. less informed system unsupervised
system (called SenseCorpus-U), use hand-coded corpus prior extracted therein. system relies examples SenseCorpus following Automatic Ranking (McCarthy et al., 2004) train DL (see Section 3.2). following
system minimally-supervised (SenseCorpus-MS), sense uses priors
obtained Semcor define distribution examples SenseCorpus
fed DL. Lastly, informed system trains DL hand-tagged
examples Semcor SenseCorpus (following Semcor prior), known
SenseCorpus-S. three systems follow widely used distinction among unsupervised,
minimally-supervised supervised systems, compare similar
systems participated Senseval-3.
systems respond realistic scenarios. unsupervised system called
case languages all-words hand-tagged corpus exists, cases priors coming Semcor appropriate, domain-specific corpora. minimally
supervised system useful hand-tagged corpora,
indication distribution senses. Lastly, supervised system (SenseCorpus-S)
shows performance SenseCorpus currently available conditions English,
is, all-words corpus limited size available.
order measure real contribution SenseCorpus, compare three systems
following baselines: SenseCorpus-U vs. first sense according
automatically obtained ranking, SenseCorpus-MS vs. frequent sense Semcor,
SenseCorpus-S vs. Decision Lists trained Semcor. order judge
significance improvements, applied one-tail paired t-test.
6.1 Comparison Unsupervised Systems Senseval-3
systems participated all-words task three rely
hand-tagged corpora (not even estimating prior information). compare performance systems unsupervised system SenseCorpus-U Table 8. order
make fair comparison respect participants, removed answers
correctly guess lemma test instance (discarding errors pre-processing
Senseval-3 XML data).
see one participating systems automatic ranking McCarthy
et al. (2004) used baseline. Although able improve system,
results best unsupervised system (IRST-DDD-LSI) (Strapparava, Gliozzo, &
Giuliano, 2004). Surprisingly, unsupervised method able obtain better performance
dataset version relies Semcor frequencies (IRST-DDD-0, see next
subsection), discrepancy explained authors. reasons
remarkable results IRST-DDD-LSI clear, subsequent publications
authors shed light it.
97

fiMartinez, Lopez de Lacalle & Agirre

Code
IRST-DDD-LSI
SenseCorpus-U
AutoPS (Baseline)
DLSI-UA

Method
LSI
Decision Lists
Automatic Rank.
WordNet Domains

Attempt.
570
680
675
648

Prec.
64.6
45.5
44.6
27.8

Rec.
52.9
44.4
43.3
25.9

F
58.2
45.0
43.9
26.8

p-value
0.001

0.001
0.000

Table 8: Performance unsupervised systems participating Senseval-3 all-words
695 polysemous nouns, accompanied p-values one tailed paired t-test
respect unsupervised system (in bold).

Code
SenseCorpus-MS
MFS (Baseline)
IRST-DDD-00
Clr04-aw
KUNLP
IRST-DDD-09

Method
DL
MFS
Domain-driven
Dictionary clues
Similar relative WordNet
Domain-driven

Attempt.
695
695
669
576
628
346

Prec.
63.9
62.7
55.6
58.7
54.2
69.7

Rec.
63.9
62.7
53.5
48.6
49.0
34.7

F
63.9
62.7
54.5
53.2
51.5
46.3

p-value

0.044
0.000
0.000
0.000
0.000

Table 9: Performance minimally supervised systems participating Senseval-3 allwords 695 polysemous nouns, accompanied p-values one tailed
paired t-test respect SenseCorpus-MS (in bold).

improvement baseline lower lexical-sample case,
significant 0.99 level (significance 1p-value). order explore reasons
this, performed experiments separating words different sets according
frequency Semcor, reported Section 6.4.
6.2 Comparison Minimally Supervised Systems Senseval-3
four systems Senseval used Semcor estimate sense distribution,
without using examples word training. show performance
systems, together frequent sense baseline Table 9.
results show SenseCorpus examples able obtain best performance
kind systems, well rest. improvement Semcor MFS baseline
significant 0.96 level.
6.3 Comparison Supervised Systems Senseval-3
systems participated all-words task supervised systems
relied mainly Semcor. Table 10 present results top 10 competing systems
system, trained SenseCorpus Semcor. include DL system
trained Semcor, baseline.
results show using SenseCorpus able obtain significant improvement
2.9% points F-score baseline. score places system second, close
98

fiOn Use Automatically Acquired Examples All-Nouns WSD

Code
SenseLearner
SenseCorpus-S
LCCaw
kuaw.ans
R2D2English
GAMBL-AW
upv-eaw.upv-eaw2
Meaning
upv-eaw.upv-eaw
Prob5
Semcor baseline
UJAEN2

Method
Syntactic Patterns
DL

Ensemble
Optim.,TiMBL
Ensemble

DL

Attempt.
695
695
695
695
695
695
695
695
695
691
695
695

Prec.
65.9
65.3
65.3
64.8
64.5
63.3
63.3
63.2
62.9
62.8
62.4
62.4

Rec.
65.9
65.3
65.3
64.7
64.5
63.3
63.3
63.2
62.9
62.4
62.4
62.4

F
65.9
65.3
65.3
64.7
64.5
63.3
63.3
63.2
62.9
62.6
62.4
62.4

p-value
0.313

0.166
0.115
0.054
0.013
0.014
0.009
0.007
0.007
0.006
0.002

Table 10: Performance top 10 supervised systems participating Senseval-3 allwords 695 polysemous nouns, accompanied p-values one tailed
paired t-test respect SenseCorpus-S (in bold).

best system all-nouns. statistical significance tests score 90%
top 4 systems, 95% rest systems. means system performs
similar top three systems, significantly better rest.
6.4 Analysis Performance Word Frequency
previous sections observed different words achieve different rates accuracy.
instance, lexical-sample experiments showed precision unsupervised
system ranged 12.5% 87.5% (cf. Table 5). Clearly, words
whose performance low using SenseCorpus. section, group
nouns Senseval-3 all-nouns task according frequency see whether
correlation frequency words performance system.
goal identify sets words disambiguated higher accuracy
method. process would allow us previously detect type words system
applied to, thus providing better tool work combination WSD systems
exploit properties language.
study, created separate word sets according frequency occurrence
Semcor. Table 11 shows different word-sets, frequency ranges, number
nouns range, average polysemy. see frequent words
tend polysemous. case supervised systems, polysemy
number training examples tend compensate other, yielding good results
kinds words. is, polysemous words difficult disambiguate,
examples train Semcor (Agirre & Martinez, 2000).
Table 12 shows results different frequency ranges top unsupervised systems Senseval-3, together method. see systems
performance low high-frequency range. best performing system (IRSTDDD-LSI) profits use threshold leaves many instances unanswered.
Regarding improvement SenseCorpus-U Automatic Ranking baseline (Au99

fiMartinez, Lopez de Lacalle & Agirre

Range
010
1120
2140
4160
6180
81100
101
Overall

#Nouns
207
101
89
88
54
31
125
695

Avg. Polysemy
3.6
5.1
6.1
6.6
6.9
9.3
9.6
5.4

Table 11: Number noun occurrences frequency ranges (in Semcor),
average polysemy.

010
1120
2140
4160
6180
81100
101
overall

DLSI-UA
Att.
F-sc.
188
35.98
96
34.50
75
15.82
82
19.97
54
22.20
31
9.70
122
24.30
648
26.83

IRST-DDD-LSI
Att.
F-sc.
195
67.13
91
69.77
81
57.65
75
55.21
50
57.69
19
36.02
59
35.85
570
58.22

SenseCorpus-U
Att.
F-sc.
198
62.77
98
58.90
89
25.50
85
42.84
54
35.80
31
23.70
125
29.60
680
45.00

AutoPS
Att.
F-sc
198
62.68
98
49.25
86
26.24
85
42.75
54
31.50
31
29.00
123
31.44
675
43.95

Table 12: Results unsupervised systems Senseval-3 words, evaluated
nouns Semcor frequency range. Att. stands number words
attempted range. Best F-score per system given bold.

toPS), best results obtained low-frequency range (0-20), baseline
scores 50-60% F-score range. results SenseCorpus lower baseline
words frequency higher 80. suggests system reliable
low-frequency words, simple threshold takes account frequency words
would indicative performance expect. behavior apparent
unsupervised systems, shows weak spot kind
systems. think future research focus high frequency words.

7. Discussion
work implemented evaluated all-words WSD system nouns
able reach state-of-the-art performance three supervised, unsupervised
semi-supervised settings. produced different systems combining SenseCorpus
different priors actual examples Semcor. supervised system, trained
hand-tagged (Semcor) automatically obtained corpora, reaches F-score
65.3%, would rank second Senseval-3 all-nouns test data. semi-supervised
system, using priors Semcor manually-tagged examples, would rank first
100

fiOn Use Automatically Acquired Examples All-Nouns WSD

class, unsupervised system second. cases, SenseCorpus improves
baselines.
results remarkable. compare system came first
unsupervised supervised settings, see uses completely
different strategy. contrary, system, using primarily automatically acquired
examples, able perform top ranks three settings.
case, deep gap exists among following three kinds systems: (i) Supervised
systems specific training (e.g. Senseval lexical-sample systems), (ii) Supervised systems
all-words training (e.g. trained using Semcor), (iii) Unsupervised systems.
algorithm implemented all-words supervised system, unsupervised system. Although implementations obtain state-of-the-art performance
categories, different issues could addressed order close gaps,
make all-words unsupervised performance closer supervised systems.
identified three main sources error: low quality relatives applied
words, different distributions senses training testing data, low
performance high-frequency (and highly polysemous) words. examine
turn.
algorithm suffers noise introduced relatives far target
word, share local context it. Better filtering would required
alleviate problem, one way could retrieve examples
share part local context target word discard examples. Another
interesting aspect problem would identify type words achieve low
performance SenseCorpus. already observed high-frequency words obtain low
performance, another study performance according type relatives would
useful better application algorithm.
order deal words close WordNet relatives, another source
examples would use distributionally similar words. words would obtained
methods one presented Lin (1998), retrieved examples would
linked target senses using WordNet similarity package (Patwardhan & Pedersen,
2003).
second main problem systems rely automatic acquisition fact
sense distributions training test data different, seriously
affects performance. system relies automatically-obtained sense ranking
alleviate problem. However, words still get many examples senses
relevant domain. preliminary experiments, observed benefit using
heuristics filter senses, using number close relatives WordNet,
promising results.
Finally, third problem observed Section 6.4, fact high-frequency
words profit automatically acquired examples. unsupervised methods,
frequent (and polysemous) words get low performances, threshold-based
systems usually discard answering them. straightforward way improve F-score
system would apply threshold discard words apply another method
back-off strategy them.
all, detecting limitations system give us important clues work
towards accurate unsupervised all-words system. literature shows single
101

fiMartinez, Lopez de Lacalle & Agirre

unsupervised system able perform well words. able identify type
words suited different algorithms heuristics, integration
algorithms one single combined system could way go. instance, could
detect cases relatives target word different apply SenseCorpus
approach, cases automatic ranking enough evidence.
observed simple heuristics number close relatives WordNet
successfully applied sets words. Meta-learning techniques (Vilalta & Drissi, 2002)
could useful exploit strengths unsupervised systems.

8. Conclusions Future Work
paper presents evidence showing proper use automatically acquired examples
allows state-of-the-art performance WSD nouns. gathered examples
nouns WordNet 1.6 resource called SenseCorpus, amounting 150 million examples,
made resource publicly available community.
used examples train supervised WSD system, variety settings:
own, combined prior information coming different sources, combined
training examples Semcor. Depending knowledge used, able build,
respectively, unsupervised system seen hand-labeled training data,
semisupervised one sees priors generic hand-labeled corpus (Semcor),
fully-supervised system uses generic hand-labeled corpus (Semcor)
training data.
evaluation lexical-sample all-words settings shown SenseCorpus
improves commonly used baselines combinations, achieves state-of-the-art
performance all-words Senseval-3 evaluation set nouns. Previous work automatic example acquisition evaluated handful words. contrast
shown able scale nouns producing excellent results. way,
learned use prior senses crucial apply acquired examples
effectively.
discussion outlined different ways overcome limitations
system, proposed lines could improve significantly current performance.
Although recent literature shows unsupervised system performs
high precision words, believe different systems complement
other, usually perform well different sets words. meta-learning perspective, could build word-expert system able apply best knowledge
source problem: SenseCorpus, hand-tagged examples, simple heuristics,
unsupervised algorithms incorporated.
future work, aside proposed improvements, think would
interesting apply method testbeds. order applied, monosemous
relative method requires ontology raw corpus. resources found many
specific domains, Biomedicine, fine-grainedness WordNet,
could lead practical applications.
102

fiOn Use Automatically Acquired Examples All-Nouns WSD

Acknowledgments
work partially financed Ministry Education (KNOW project, ICT2007-211423) Basque Government (consolidated research groups grant, IT-397-07).
Oier Lopez de Lacalle supported PhD grant Basque Government. David
Martinez funded Australian Research Council, grant no. DP0663879.

References
Abney, S. (2002). Bootstrapping. Proceedings 40th Annual Meeting Association Computational Linguistics, ACL02, Philadelphia.
Agirre, E., Ansa, O., Martinez, D., & Hovy, E. (2000). Enriching large ontologies using
WWW. Proceedings Ontology Learning Workshop, organized ECAI,
Berlin (Germany).
Agirre, E., Ansa, O., Martinez, D., & Hovy, E. (2001). Enriching WordNet concepts
topic signatures. Procceedings SIGLEX workshop WordNet
Lexical Resources: Applications, Extensions Customizations. conjunction
NAACL.
Agirre, E., & Edmonds, P. (Eds.). (2006). Word Sense Disambiguation: Algorithms
Applications. Springer.
Agirre, E., & Lopez de Lacalle, O. (2004). Publicly available topic signatures WordNet nominal senses. Proceedings 4th International Conference Language
Resources Evaluation (LREC), Lisbon, Portugal.
Agirre, E., & Martinez, D. (2000). Exploring automatic word sense disambiguation Decision Lists Web. Procedings COLING 2000 Workshop Semantic
Annotation Intelligent Content, Luxembourg.
Agirre, E., & Martinez, D. (2004a). Smoothing word sense disambiguation. Proceedings Expaa Natural Language Processing (EsTAL), Alicante, Spain.
Agirre, E., & Martinez, D. (2004b). Unsupervised WSD based automatically retrieved
examples: importance bias. Proceedings Conference Empirical
Methods Natural Language Processing, Barcelona, Spain.
Blum, A., & Mitchell, T. (1998). Combining labeled unlabeled data co-training.
Proceedings 11h Annual Conference Computational Learning Theory, pp.
92100, New York. ACM Press.
Chan, Y., & Ng, H. (2005). Scaling word sense disambiguation via parallel texts.
Proceedings 20th National Conference Artificial Intelligence (AAAI 2005),
Pittsburgh, Pennsylvania, USA.
Collins, M., & Singer, Y. (1999). Unsupervised models named entity classification.
Proceedings Joint SIGDAT Conference Empirical Methods Natural
Language Processing Large Corpora, EMNLP/VLC99, College Park, MD,
USA.
103

fiMartinez, Lopez de Lacalle & Agirre

Cuadros, M., Padro, L., & Rigau, G. (2006). empirical study automatic acquisition
topic signatures. Proceedings Third International WordNet Conference, Jeju
Island (Korea).
Cuadros, M., & Rigau, G. (2006). Quality assessment large scale knowledge resources.
Proceedings 2006 Conference Empirical Methods Natural Language Processing, pp. 534541, Sydney, Australia. Association Computational Linguistics.
Daconta, M., Obrst, L., & Smith, K. (2005). Semantic Web: Guide Future
XML, Web Services, Knowledge Management. John Wiley & Sons.
Dale, R., Moisl, H., & Somers, H. (2000). Handbook Natural Language Processing. Marcel
Dekker Inc.
Daude, J., Padro, L., & Rigau, G. (2000). Mapping WordNets using structural information.
38th Anual Meeting Association Computational Linguistics (ACL2000),
Hong Kong.
Edmonds, P., & Kilgarriff, A. (2002). Natural Language Engineering, Special Issue Word
Sense Disambiguation Systems. No. 8 (4). Cambridge University Press.
Fellbaum, C. (1998). WordNet: Electronic Lexical Database. MIT Press.
Fernandez-Amoros, D., Gonzalo, J., & Verdejo, F. (2001). UNED systems Senseval2. Proceedings SENSEVAL-2 Workshop. conjunction ACL, Toulouse,
France.
Fujii, A., Inui, K., Tokunaga, T., & Tanaka, H. (1998). Selective sampling example-based
word sense disambiguation. Computational Linguistics, No. 24 (4), pp. 573598.
Humphreys, L., Lindberg, D., Schoolman, H., & Barnett, G. (1998). Unified Medical
Language System: informatics research collaboration. Journal American
Medical Informatics Association, 1 (5).
Ide, N., & Veronis, J. (1998). Introduction special issue word sense disambiguation:
state art. Computational Linguistics, 24 (1), 140.
Jurafsky, D., & Martin, J. (2000). Introduction Natural Language Processing, Computational Linguistics, Speech Recognition. Prentice-Hall, Upper Saddle River,
NJ 07458.
Kim, S.-B., Seo, H.-C., & Rim, H.-C. (2004). Information retrieval using word senses: root
sense tagging approach. SIGIR 04: Proceedings 27th annual international
ACM SIGIR conference Research development information retrieval, pp.
258265, New York, NY, USA. ACM.
Kohomban, U., & Lee, W. (2005). Learning semantic classes word sense disambiguation. Proceedings 43rd Annual Meeting Association Computational
Linguistics (ACL05).
Leacock, C., Chodorow, M., & Miller, G. A. (1998). Using corpus statistics WordNet
relations sense identification. Computational Linguistics, Vol. 24, pp. 147165.
Lin, D. (1998). Automatic retrieval clustering similar words. Proceedings
COLING-ACL, Montreal, Canada.
104

fiOn Use Automatically Acquired Examples All-Nouns WSD

Liu, S., Liu, F., Yu, C., & Meng, W. (2004). effective approach document retrieval
via utilizing WordNet recognizing phrases. SIGIR 04: Proceedings
27th annual international ACM SIGIR conference Research development
information retrieval, pp. 266272, New York, NY, USA. ACM.
Manning, C. D., & Schutze, H. (1999). Foundations Statistical Natural Language Processing. MIT Press, Cambridge, Massachusetts.
McCarthy, D., Koeling, R., Weeds, J., & Carroll, J. (2004). Finding predominant word senses
untagged text. Proceedings 42nd Annual Meeting Association
Computational Linguistics (ACL), Barcelona, Spain.
Mihalcea, R. (2002). Bootstrapping large sense tagged corpora. Proceedings
3rd International Conference Language Resources Evaluation (LREC), Las
Palmas, Spain.
Mihalcea, R. (2004). Co-training self-training word sense disambiguation. Proceedings Conference Natural Language Learning (CoNLL 2004), Boston,
USA.
Mihalcea, R., & Chklovski, T. (2003). Open Mind Word Expert: Creating large annotated
data collections Web users help. Proceedings EACL 2003 Workshop
Linguistically Annotated Corpora (LINC 2003), Budapest, Hungary.
Mihalcea, R., Chklovski, T., & Killgariff, A. (2004). Senseval-3 English lexical sample
task. Proceedings 3rd ACL workshop Evaluation Systems
Semantic Analysis Text (SENSEVAL), Barcelona, Spain.
Mihalcea, R., & Edmonds, P. (2004). Senseval-3, Third International Workshop
Evaluation Systems Semantic Analysis Text. Association Computational Linguistics.
Mihalcea, R., & Moldovan, D. (1999). automatic method generating sense tagged
corpora. Proceedings AAAI-99, Orlando, FL.
Miller, G. A., Leacock, C., Tengi, R., & Bunker, R. (1993). semantic concordance.
Proceedings ARPA Human Language Technology Workshop, pp. 303308,
Princeton, NJ. distributed Human Language Technology San Mateo, CA: Morgan
Kaufmann Publishers.
Ngai, G., & Florian, R. (2001). Transformation-Based Learning fast lane. Proceedings Second Conference North American Chapter Association
Computational Linguistics, pp. 4047, Pittsburgh, PA, USA.
Patwardhan, S., & Pedersen, T. (2003). cpan wordnet::similarity package.
http://search.cpan.org/author/SID/WordNet-Similarity-0.03/.



Pedersen, T. (2001). decision tree bigrams accurate predictor word sense.
Proceedings Second Meeting North American Chapter Association
Computational Linguistics (NAACL-01), Pittsburgh, PA.
Pham, T. P., Ng, H. T., & Lee, W. S. (2005). Word sense disambiguation semisupervised learning. Proceedings 20th National Conference Artificial
Intelligence (AAAI 2005), pp. 10931098, Pittsburgh, Pennsylvania, USA.
105

fiMartinez, Lopez de Lacalle & Agirre

Ravin, Y., & Leacock, C. (2001). Polysemy: Theoretical Computational Approaches.
Oxford University Press.
Resnik, P. (2006). Word sense disambiguation natural language processing applications.
Agirre, E., & Edmonds, P. (Eds.), Word Sense Disambiguation, chap. 11, pp. 299
337. Springer.
Snyder, B., & Palmer, M. (2004). English all-words task. Proceedings 3rd
ACL workshop Evaluation Systems Semantic Analysis Text (SENSEVAL), Barcelona, Spain.
Stevenson, M. (2003). Word Sense Disambiguation: Case Combining Knowledge
Sources. CSLI Publications, Stanford, CA.
Stevenson, M., & Clough, P. (2004). Eurowordnet resource cross-language information retrieval. Proceedings Fourth International Conference Language
Resources Evaluation, Lisbon, Portugal.
Strapparava, C., Gliozzo, A., & Giuliano, C. (2004). Pattern abstraction term similarity
word sense disambiguation: IRST Senseval-3. Proceedings 3rd ACL
workshop Evaluation Systems Semantic Analysis Text (SENSEVAL), Barcelona, Spain.
Tugwell, D., & Kilgarriff, A. (2001). WASP-Bench: lexicographic tool supporting word
sense disambiguation. Proceedings SENSEVAL-2 Workshop. conjunction
ACL-2001/EACL-2001, Toulouse, France.
Vickrey, D., Biewald, L., Teyssier, M., & Koller, D. (2005). Word-sense disambiguation
machine translation. Proceedings Human Language Technology Conference
Conference Empirical Methods Natural Language Processing.
Vilalta, R., & Drissi, Y. (2002). perspective view survey meta-learning. Artificial
Intelligence Review, No. 18 (2), pp. 7795.
Vossen, P., Rigau, G., Alegra, I., Agirre, E., Farwell, D., & Fuentes, M. (2006). Meaningful
results information retrieval MEANING project. Proceedings Third
International WordNet Conference, Jeju Island, Korea.
Wang, X., & Carroll, J. (2005). Word sense disambiguation using sense examples automatically acquired second language. Proceedings joint Human Language
Technologies Empirical Methods Natural Language Processing conference, Vancouver, Canada.
Wang, X., & Martinez, D. (2006). Word sense disambiguation using automatically translated sense examples. Proceedings EACL 2006 Workshop Cross Language
Knowledge Induction, Trento, Italy.
Weeber, M., Mork, J., & Aronson, A. (2001). Developing test collection biomedical
word sense disambiguation. Proceedings AMIA Symposium, pp. 746750.
Yarowsky, D. (1994). Decision lists lexical ambiguity resolution: Application accent
restoration Spanish French. Proceedings 32nd Annual Meeting
Association Computational Linguistics, pp. 8895, Las Cruces, NM.
106

fiOn Use Automatically Acquired Examples All-Nouns WSD

Yarowsky, D. (1995). Unsupervised word sense disambiguation rivaling supervised methods. Proceedings 33rd Annual Meeting Association Computational
Linguistics, pp. 189196, Cambridge, MA.
Yarowsky, D., & Florian, R. (2003). Evaluating sense disambiguation across diverse parameter spaces. Natural Language Engineering, 8 (2), 293310.

107


