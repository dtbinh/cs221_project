Journal Artificial Intelligence Research 21 (2004) 429-470

Submitted 07/03; published 04/04

Grounded Semantic Composition Visual Scenes
Peter Gorniak
Deb Roy

pgorniak@media.mit.edu
dkroy@media.mit.edu

MIT Media Laboratory
20 Ames St.,Cambridge, 02139 USA

Abstract
present visually-grounded language understanding model based study
people verbally describe objects scenes. emphasis model combination
individual word meanings produce meanings complex referring expressions.
model implemented, able understand broad range spatial referring
expressions. describe implementation word level visually-grounded semantics
embedding compositional parsing framework. implemented system selects
correct referents response natural language expressions large percentage
test cases. analysis systems successes failures reveal visual context
influences semantics utterances propose future extensions model take
context account.

1. Introduction
introduce visually-grounded language understanding model based study
people describe objects visual scenes kind shown Figure 1. designed study
elicit descriptions would naturally occur joint reference setting easy
produce understand human listener. typical referring expression Figure 1
might be, far back purple cone thats behind row green ones. Speakers construct
expressions guide listeners attention intended objects. referring expressions
succeed communication speakers listeners find similar features visual
scene salient, share understanding language grounded terms
features. work step towards longer term goals develop conversational
robot (Hsiao, Mavridis, & Roy, 2003) fluidly connect language perception
action.
study use descriptive spatial language task similar one robots
perform, collected several hundred referring expressions based scenes similar Figure
1. analysed descriptions cataloguing visual features referred
within scene, range linguistic devices (words grammatical patterns)
used refer features. combination visual feature corresponding
linguistic device referred descriptive strategy. example sentence contains
several descriptive strategies make use colour, spatial relations, spatial grouping.
descriptive strategies used composition speaker make reference
unique object.
propose set computational mechanisms correspond commonly
used descriptive strategies study. resulting model implemented
set visual feature extraction algorithms, lexicon grounded terms visual
c
2004
AI Access Foundation. rights reserved.

fiGorniak & Roy

Figure 1: sample scene used elicit visually-grounded referring expressions (if figure
reproduced black white, light cones green colour,
dark cones purple)

features, robust parser capture syntax spoken utterances, compositional
engine driven parser combines visual groundings lexical units. evaluate
system, collected set spoken utterances three speakers. verbatim transcriptions speech, complete speech repairs various ungrammaticalities
common spoken language, fed model. model able correctly
understand visual referents 59% expressions (chance performance
assuming
P30
random object selected sessions 30 trials 1/30 i=1 1/i = 13%).
system able resolve range linguistic phenomena made use relatively
complex compositions spatial semantics. provide detailed analysis sources
failure evaluation, based propose number improvements
required achieve human level performance. designing framework build
prior work human reference resolution integration semantics parsing.
main contribution work lies using visual features based study human visual
linguistic reference grounded semantic core natural language understanding
system.
previous work visually-grounded language centred machine learning
approaches (Roy, Gorniak, Mukherjee, & Juster, 2002; Roy & Pentland, 2002; Roy, 2002),
chose apply machine learning problem compositional grounded semantics
investigation. Rather, endeavoured provide framework process
types descriptive strategies compositionality found study human
participants. future work, investigate machine learning methods used
acquire parts framework experience, leading robust accurate
performance.
430

fiGrounded Semantic Composition Visual Scenes

1.1 Grounded Semantic Composition
use term grounded semantic composition highlight semantics
individual words word composition process visually-grounded.
model, lexical entrys meaning grounded association visual model.
example, green associated probability distribution function defined
colour space. propose processes combine visual models words, governed
rules syntax.
Given goal understanding modelling grounded semantic composition, several
questions arise:
visual features people use describe objects scenes
Figure 1?
features connect language?
features descriptions combine produce whole utterances
meanings?
word meanings independent visual scene describe?
meaning whole utterance based meanings parts?
composition meanings purely incremental process?
assumed easy answers questions place start modelling
effort. current implementation assumes meaning whole utterance
fully derived meanings parts, performs composition incrementally (without
backtracking), let visual context influence interpretation word
meanings. Despite assumptions, system handles relatively sophisticated semantic
composition. evaluated test data, system correctly understood chose
appropriate referents expressions as, purple one behind two green ones
left green cone front back purple one.
analysing systems performance human participants utterances, found
that:
Word meanings strongly dependent visual scene describe.
instance, found four distinct visual interpretations word middle
linguistically indistinguishable, instead depend different visual contexts
understood.
meaning utterance may sometimes depend meanings
parts. meaning may depend visual context utterance
occurs, modify parts compose. example, objects
referred frontmost left would referred neither left frontmost
isolation, result multiplicative joined estimation two.
Composition meanings task purely incremental process.
cases found necessary backtrack reinterpret parts utterance
431

fiGorniak & Roy

good referents found later processing stage, ambiguities cannot
resolved current interpretation. Due strictly feed forward model
language understanding, current implementation fails cases.
results similar reported prior studies (Brown-Schmidt, Campana,
& Tanenhaus, 2002; Griffin & Bock, 2000; Pechmann, 1989). Although model
currently address issues context-dependent interpretation backtracking,
believe framework approach grounded compositional semantics provide
useful steps towards understanding spatial language. system performs well understanding spatial descriptions, applied various tasks natural language
speech based human-machine interfaces.
paper begins highlighting several strands related previous work. section 2,
introduce visual description task serves basis study model.
Section 3 presents framework grounded compositional semantics. Section 4 describes
resulting computational model. example whole system work given
Section 5. discuss results applying system human data spatial
description task section 6, together analysis systems successes failures.
leads suggestions future work Section 7, followed summary section 8.
1.2 Related Work
Winograds SHRDLU well known system could understand generate natural
language referring objects actions simple blocks world (Winograd, 1970).
system performs semantic interpretation parsing attaching short procedures
lexical units (see Miller & Johnson-Laird, 1976). However, SHRDLU access
clean symbolic representation scene, whereas system discussed works
synthetic vision system reasons geometric visual measures. Furthermore,
intend system robustly understand many ways human participants
verbally describe objects complex visual scenes other, whereas SHRDLU
restricted sentences could parse completely translate correctly formalism.
SHRDLU based formal approach semantics problem meaning
addressed logical set theoretic formalisms. Partee provides overview
approach problems context based meanings meaning compositionality
perspective (Partee, 1995). work reflects many ideas work,
viewing adjectives functions. Pustejovskys theory Generative Lexicon
(GL) particular takes seriously noun phrase semantics semantic compositionality
(Pustejovsky, 1995). approach lexical semantic composition originally inspired
Pustejovskys qualia structures. However, formal approaches operate symbolic
domain leave details non-linguistic influences meaning unspecified, whereas
take computational modelling influences primary concern.
Research concerning human production referring expressions lead studies related one described here, without computational counterparts. Brown-Schmidt
e.a., example, engage participants free-form dialogue (as opposed one-sided
descriptions task) producing referential descriptions solve spatial arrangement
problem (Brown-Schmidt et al., 2002). Due use complicated scenes
complete dialogues, find participants often engage agreement behaviours
432

fiGrounded Semantic Composition Visual Scenes

use discourse visual context disambiguate underspecified referring expressions
often study. Similar tasks used studies dialogue
referring expressions (Pechmann, 1989; Griffin & Bock, 2000). intentionally eliminated dialogue used simpler visual scene task elicit spatial descriptions (as
opposed description object attributes), able computationally model
strategies participants employ. Formal theories vagueness support findings
expressions produced participants (Kyburg & Morreau, 2000; Barker, 2002).
Word meanings approached several researchers problem associating
visual representations, often complex internal structure, word forms. Models
suggested visual representations underlying colour (Lammens, 1994) spatial
relations (Regier, 1996; Regier & Carlson, 2001). Models verbs include grounding
semantics perception actions (Siskind, 2001), grounding terms motor
control programs (Bailey, 1997; Narayanan, 1997). Object shape clearly important
connecting language world, remains challenging problem computational
models language grounding. previous work, used histograms local geometric
features found sufficient grounding names basic objects (dogs, shoes, cars,
etc.) (Roy & Pentland, 2002). representation captures characteristics overall
outline form object invariant in-plane rotations changes scale. Landau
Jackendoff provide detailed analysis additional visual shape features play
role language (Landau & Jackendoff, 1993). example, suggest importance
extracting geometric axes objects order ground words end, end
stick. Shi Malik propose approach performing visual grouping images
(Shi & Malik, 2000). work draws findings Gestalt psychology provide
many insights visual grouping behaviour (Wertheimer, 1999; Desolneux, Moisan, &
Morel, 2003). Engbers e.a. give overview formalization grouping problem
general various approaches solution (Engbers & Smeulders, 2003). parallel
work presented paper, studying visual grouping fold
results systen described (Dhande, 2003).
model incremental semantic interpretation parsing follows tradition
employing constraint satisfaction algorithms incorporate semantic information starting
SHRDLU continued systems (Haddock, 1989). prior systems use
declaratively stated set semantic facts disconnected perception. Closely
related work area Schulers (2003), integrates determination referents
parsing process augmenting grammar logical expressions, much
augment grammar grounded composition rules (see Section 3.4). emphasis,
however, system actively ground word utterance meanings
sensory system. Even though system described senses synthetic scene,
makes continuous measurements parsing process integrating
active vision system (Hsiao et al., 2003). Schulers system requires human-specified
clean logical encoding world state, ignores noisy, complex difficultto-maintain process linking language sensed world. consider process,
call grounding process, one important aspects situated human-like
language understanding.
SAM (Brown, Buntschuh, & Wilpon, 1992) Ubiquitous Talker (Nagao & Rekimoto,
1995) language understanding systems map language objects visual scenes.
433

fiGorniak & Roy

Similar SHDRLU, underlying representation visual scenes symbolic loses
much subtle visual information work, work cited above, focus
on. SAM Ubiquitous Talker incorporate vision system, phrase parser
understanding system. systems translate visually perceived objects symbolic
knowledge base map utterances plans operate knowledge base.
contrast, primarily concerned understanding language referring objects
relations appear visually.
previously proposed methods visually-grounded language learning (Roy &
Pentland, 2002), understanding (Roy et al., 2002), generation (Roy, 2002). However,
treatment semantic composition efforts relatively primitive. phrase,
visual models word phrase individually evaluated multiplied.
method works phrases conjunctive modifiers, even cases,
discuss later, ordering modifiers sometimes needs taken account (i.e., leftmost
front always refer front leftmost does). simple approach
worked constrained domains addressed past, scale
present task. example, Describer system (Roy, 2002) encodes spatial locations
absolute terms within frame reference visual scene. result, Describer
makes mistakes humans would make. grounding word highest,
example, defined probability distribution centred specific height scene,
object closest height best example highest, accounting
fact may objects greater height (depending relative sizes
shapes objects). addition, Describers interpretation phrase highest
green rectangle find object close center probability
distributions highest green, accounting fact human
listener highest green rectangle need high screen (but higher
green rectangles). word highest requires visual binding
includes notion rank ordering. move, however, requires rethinking
achieve semantic composition, addressed approach here.

2. Spatial Description Task
designed task requires people describe objects computer generated scenes
containing 30 objects random positions virtual surface. objects
identical shapes sizes, either green purple colour. object
50% chance green, otherwise purple. refer task Bishop
task, resulting language understanding model implemented system simply
Bishop.
2.1 Motivation Task Design
previous work, investigated speakers describe objects distinctive
attributes colour, shape size constraint speaking task scenes
constant number objects (Roy, 2002). Speakers task rarely compelled
use spatial relations never use groups objects, cases objects
distinguished listing properties. designing Bishop task, goal
naturally lead speakers make reference spatial aspects scene. Therefore,
434

fiGrounded Semantic Composition Visual Scenes

drastically increased number objects scene decreased number
distinctive object attributes. let number objects vary throughout trials
cover scenes cluttered objects scenes objects analysis.
variation task, ran experiments system chose objects
random speaker describer, rather allowing describer make choice.
found made task difficult highly unnatural speaker
often visually salient arrangements randomly chosen objects took part
in. result, listeners make many errors resolving reference variation
task (3.5% error speaker chose object versus 13% system
chose). limits accuracy pure linguistic reference appeared
reaching random selection version task. Speakers seemed much
harder time finding visually salient landmarks, leading long less natural descriptions,
example centre bunch green cones, four them, um, actually
four, but, ah, theres one thats centre pretty much pile
top, ahm, say this... seventh cone right
side (followed listener counting cones pointing screen). avoid collecting
unnatural data, decided use random selection version task.
Another possible variant task would let system choose objects
non-random manner based systems analysis objects would natural describe. However, approach would clearly bias data towards objects
matched preexisting models system.
Since interested people described objects spatially well visual
features found salient, decided let listener pick objects felt
concisely yet trivially describable. acknowledge task design eases
difficulty understanding task; speakers could find interesting object
easy describe ways, resorted simpler choices leftmost
one. Yet, utterances elicited task relatively complex (see Appendix
complete listing) provided serious challenges automatic language
understanding perspective.
Scenes rendered 3D instead using equivalent 2D scene anticipation
transition understanding system camera driven vision system. use
3D rendering introduces occlusion, shadows, sources ambiguity must
eventually addressed transition real vision system. However, note
scene include interesting 3D spatial relations features,
claim description task thus system presented would generalize
directly true 3D setting. Furthermore, use 3D information
visual scene, system interprets spatial relations 2D. errs 2D side
ambiguity inherent word leftmost reference one scenes (the
interpretation differ due perspective effects: leftmost object interpreting
scene 2D might leftmost interpreting 3D. believe
task types visually grounded descriptions produced challenging
computational system understand, hope show remainder paper.
Finally, note goal design task study collaboration,
dialogue agreement, goal experiments analyses (Carletta &
Mellish, 1996; Eugenio, Jordan, Thomason, & Moore, 2000). use speaker/listener dyad
435

fiGorniak & Roy

ensure descriptions produced understandable human listener,
purposefully allow listeners speak. feedback channel speakers
successful unsuccessful selection described object. introduce
minimal form dialogue, low error rate listeners leads us believe negative
reinforcement negligible speakers task viewed
exercise collaboration. cannot rule listeners adopted strategies used
partners turn speak. However, relative similarity strategies
pairs shows phenomenon make data unrepresentative,
even produces types shortenings vagueness would expect see
extended description task speaking machine.
2.2 Data Collection
Participants study ranged age 22 30 years, included native
non-native English speakers. Pairs participants seated backs
other, looking computer screen displayed identical scenes
Figure 1. pair, one participant served describer, listener.
describer wore microphone used record speech. describer used
mouse select object scene, verbally described selected object
listener. listener allowed communicate verbally otherwise all,
except object selections. listeners task select object
computer display based verbal description. selected objects matched,
disappeared scene describer would select describe another object.
match, describer would re-attempt description understood
listener. Using describer-listener dyad ensured speech data resembled natural
communicative dialogue. Participants told free select object
scene describe way thought would clear. told
make task trivial by, example, always selecting leftmost object describing
leftmost. scene contained 30 objects beginning session,
session ended objects remained, point describer listener switched
roles completed second session (some participants fulfilled role multiple times).
found listeners study made extremely mistakes interpreting descriptions,
seemed generally find task easy perform.
Initially, collected 268 spoken object descriptions 6 participants. raw
audio segmented using speech segmentation algorithm based pause structure
(Yoshida, 2002). Along utterances, corresponding scene layout target
object identity recorded together times objects selected.
268 utterance corpus referred development data set. manually transcribed
spoken utterance verbatim, retaining speech errors (false starts various
ungrammaticalities). Rather working grammatically controlled language,
interest model language occurs conversational settings since longer term
goal transplant results work conversational robots language
spoken form. Off-topic speech events (laughter, questions task,
remarks, filled pauses) marked (they appear results
report).
436

fiGrounded Semantic Composition Visual Scenes

developed simple algorithm pair utterances selections based time
stamps. algorithm works backwards time point correct
object removed scene. collects on-topic utterances occurred
removal event previous removal event 4
seconds apart. fuses single utterance, sends scene description,
complete utterance identity removed object understanding system.
utterance fusing necessary participants often paused descriptions.
time, pauses beyond certain length usually indicated utterances
pause contained errors rephrase occurred. pairing algorithm
obviously heuristic nature, mark instances makes mistakes (wrongly
leaving utterances attributing utterances wrong selection event) analysis
data below. report numbers utterances data sets paper,
correspond many utterance-selection pairs pairing algorithm produces.
means due errors algorithm numbers utterances report
divisible 30, actual number objects selected session.
development corpus analysed catalogue range common referring strategies (see Section 2.3). analysis served basis developing visually-grounded
language understanding system designed replace human listener task described
above. implementation yielded acceptable results development corpus,
collected another 179 spoken descriptions three additional participants evaluate generalization coverage approach. used exactly equipment, instructions
collection protocol collecting development data collect test data.
average length utterances development test set 8
9 words. discussion analysis following sections focuses development
set. Section 6 discuss performance test set.
2.3 Descriptive Strategies Achieving Joint Reference
noted earlier, call combination visual feature measured current scene
(or, case anaphora, previous scene) together linguistic realization
descriptive strategy. section, catalogue common strategies describers used communicate listeners. analysis based strictly development
data set. discuss implemented system handles categories Section 4.
distinguish three subsets development data:
set containing utterance/selection pairs contain errors. error
due repair mistake human speakers part, segmentation mistake
speech segmenter, error utterance/selection pairing algorithm.
set contains utterance/selection pairs employ descriptive strategies
cover computational understanding system (we cover
Sections 2.3.1 2.3.5).
set utterance/selection pairs development data member
either subset described above. refer subset clean set.
437

fiGorniak & Roy

Note first two subsets mutually exclusive. following sections,
report two percentages descriptive strategy. first percentage
utterance/selection pairs employ specific descriptive strategy relative utterance/selection pairs development data set. second percentage utterance/selection pairs relative clean set utterance/selection pairs, described above.
examples given paper actual utterances scenes development
test sets.
2.3.1 Colour
Almost every utterance employs colour pick objects. designing task,
intentionally trivialized problem colour reference. Objects come two distinct
colours, green purple. Unsurprisingly, participants used terms green
purple refer colours. previous work addressed problems
learning visually-grounded models colour words (Roy & Pentland, 2002; Roy, 2002).
Here, focus semantic compositionality terms, chose simplify colour
naming problem. Figure 2 shows one instances colour used pick
referent. examples subsequent sections colour composed
descriptive strategies.
Syntactically, colours expressed adjectives (as mentioned: green purple) always directly precede nouns modify. is, nobody ever said
green left one data, rather adjectives would commonly occur order
left green one.
data, green purple sometimes take roles nouns, least
left dangling noun phrase ellipse leftmost purple. Although
form dangling modifier might seem unlikely, occur spoken utterances
task. objects Bishop cones, participants trouble understanding
ellipsis, occur 7% data.
Participants used colour identify one objects 96% data, 95%
clean data.

purple cone
Figure 2: Example utterance using colour

438

fiGrounded Semantic Composition Visual Scenes

2.3.2 Spatial Regions Extrema
second common descriptive strategy refer spatial extremes within groups
objects spatial regions scene. left example Figure 3 uses two spatial
terms pick referent: closest front, leverage spatial
extrema direct listeners attention. example, selection spatial extremum
appears operate relative green objects, i.e. speaker seems first attend
set green cones, choose amongst them. Alternatively, closest
front could pick several objects colour, colour specification could filter
spatial extrema determine final referent. case two interpretations yield
referent, cases corpus second alternative (spatial
selection followed colour selection) yields referents all.

green one thats closest us
front

purple one left side

Figure 3: Example utterances specifying objects referring spatial extrema
right example Figure 3 shows phrases explicitly indicating spatial extrema still sometimes intended interpreted referring extrema. listener
interpret left side referring left side scene, phrase would
ambiguous since four purple cones left side scene.
hand, phrase unambiguous interpreted picking extremum. Figure 4 shows
instance right hand side actually refers region board.
first example figure shows phrase right hand side combined
extremum term, lowest. Note referent right extremum. second
example Figure 4, referent bottommost green object, and, (arguably,
taking scene existing 3D), neither leftmost. Regions board
seem play role cases. Often local context region may play stronger
role global one, referent second example Figure 4 found
attending front left area scene, selecting left bottom example
amongst candidates area. Along lines, words middle largely
used describe region board, position relative cones.
rather ubiquitous data, spatial extrema spatial regions often used
combination descriptive strategies grouping, frequently
combined extrema region specifications. opposed combined
colour adjectives, multiple spatial specifications tend interpreted left right order,
is, selecting group objects matching first term, amongst choosing
objects match second term. examples Figure 4 could understood
439

fiGorniak & Roy

lowest purple right hand side

green cone left bottom

Figure 4: Example utterances specifying regions

purple one front left corner
Figure 5: Extrema sequence
simply ignoring order spatial specifications instead finding conjoined best fit, i.e.
best example bottommost leftmost. However, Figure 5 demonstrates
generally case. scene contains two objects best fits
unordered interpretation front left, yet human participant confidently picks
front object. Possible conclusions extrema need interpreted sequence,
participants demonstrating bias preferring front-back features left-right
ones. implementation, choose sequence spatial extrema order occur
input.
Participants used single spatial extrema identify one objects 72%
data, 78% clean data. used spatial region specifications 20%
data (also 20% clean data), combined multiple extrema regions 28% (30%
clean data).
2.3.3 Grouping
provide landmarks spatial relations specify sets objects select from, participants used language describe groups objects. Figure 6 shows two examples
grouping constructs, first using unnumbered group cones (the green cones),
second using count specify group (three). function group different
two examples: left scene participant specifies group landmark
serve spatial relation (see Section 2.3.4), whereas right scene participant
first specifies group containing target object, utters another description select
within group. Note grouping alone never yields individual reference, participants compose grouping constructs referential tactics (predominantly extrema
spatial relations) cases.
440

fiGrounded Semantic Composition Visual Scenes

purple cone middle left
green cones

theres three left side; one
furthest back

Figure 6: Example utterances using grouping

Participants used grouping identify objects 12% data 10% clean
data. selected objects within described groups 7.5% data (8% clean
data) specified groups number objects (two, three, ...) 8.5% data
(also 8.5% clean data).
2.3.4 Spatial Relations
already mentioned Section 2.3.3, participants sometimes used spatial relations
objects groups objects. Examples relations expressed prepositions
behind well phrases left front of. already
saw example spatial relation involving group objects Figure 6, Figure 7
shows two examples involve spatial relations individual objects.
first example one examples pure spatial relations two individual
objects referenced colour. second example typical one
spatial relation combined another strategy, extremum (as well two speech
errors describer).

green cone green cone

theres purple cone thats
way left hand side
another purple

Figure 7: Example utterances specifying spatial relations

Participants used spatial relations 6% data (7% clean data).
441

fiGorniak & Roy

2.3.5 Anaphora
number cases participants used anaphoric references previous object removed
description task. Figure 8 shows sequence two scenes corresponding
utterances second utterance refers back object selected first.

closest purple one far left
side

green one right behind one

Figure 8: Example sequence anaphoric utterance

Participants employed spatial relations 4% data (3% clean data).
2.3.6
addition phenomena listed preceding sections, participants used small
number description strategies. occurred
yet addressed computational model selection distance (lexicalised
close next to), selection neighbourhood (the green one surrounded purple
ones), selection symmetry (the one opposite one), selection something
akin local connectivity (the lone one). additional types groupings,
example grouping linearity (the row green ones, three purple diagonal)
picking objects within group number (the second one left row
five purple ones) cover here. strategies occurs less often
data anaphora (it occurs 4% utterances, see previous section).
annotated 13% data containing descriptive strategies ones
covered preceding sections. However, devices often combined
phenomena covered here. marked 15% data containing errors. Errors
come form repairs speaker, faulty utterance segmentation speech
segmenter, misaligning utterances scenes system.
instances participants composing semantic phenomena ways
handle. two instances combining spatial relations (the one
right) instances specifying groups spatial extrema
regions (the group purple ones left). count
evaluation, rather counted errors; reported success rate correspondingly lower.
442

fiGrounded Semantic Composition Visual Scenes

2.4 Summary
preceding sections catalogue strategies participants employed describing objects.
computational system understands utterances using strategies must fulfill
following requirements:
system must access visual scene able compute visual
features used human speakers: natural groupings, inter-object distances,
orderings spatial relations
must robust language parsing mechanism discovers grammatical patterns associated descriptive strategies
Feeding parsing mechanism must visually grounded lexicon; entry
lexicon must carry information descriptive strategies takes part
in, descriptive strategies combine others
semantic interpretation composition machinery must embedded
parsing process
system must able interpret results parsing utterance make
best guess object whole utterance describes
go describe systems understanding framework, consisting visual
system, grounded lexical entries parser Section 3. Section 4 discuss
modules implemented understand human descriptive strategies.

3. Understanding Framework
section describe components Bishop understanding system detail,
emphasis fit together work visually grounded understanding
system. cover turn Bishops vision system, parser lexicon give short
overview implementation descriptive strategies fits framework.
3.1 Synthetic Vision
Instead relying information use render scenes Bishop, includes
3D object locations viewing angle, implemented simple synthetic vision algorithm. algorithm produces map attributing pixel rendered image one
objects background. addition, use full colour information
pixel drawn rendered scene. goal loosely simulate view camera
pointed scene real world objects, situation robots find in.
past successfully migrated models synthetic vision (Roy, 2002) computer
vision (Roy et al., 2002) plan similar route deploy Bishop. Obviously, many
hard problems object detection well lighting noise robustness need
solved synthetic case, hope transfer back robots camera
made easier working 2D image. chose work virtual world
project could freely change colour, number, size, shape arrangement
443

fiGorniak & Roy

objects elicit interesting verbal behaviours participants, without running
limitations object detection algorithms field view problems.
Given input image regions corresponding objects segmented,
features produced vision system are:
average RGB colour average red, green blue components pixels
attributed object
centre mass average x pixel positions object
distance euclidean distance pairs objects centres mass
groups groups objects scene determined finding sets objects
contain one object, object less threshold
distance another object group (distances measured centres
mass)
pairs groups, filtered produce groups two objects
triplets groups, filtered produce groups three objects
convex hull set pixels forming smallest convex region enclosing set objects
attentional vector sum (AVS) AVS spatial relation measure objects. extreme parameter settings measures one two angles, formed
centres, formed closest points two objects. use
parameter setting ( = 0.7) two extremes, produces intermediate angle depending objects shape. resulting direction measured relative set reference angles, system four Cartesian vectors
(0, 1), (0, 1), (1, 0), (1, 0) (Regier & Carlson, 2001).
3.2 Knowledge Representation
Objects represented integer IDs system. ID set IDs vision
system compute visual features described Section 3.1 based corresponding
set pixels image. distinguishing ID together visual features represents
systems total knowledge objects present scene. system
instantiate new objects vision system convex hull groups objects.
system remembers ID object removed last, ask vision
system perform feature computation visual scene object
removed.
Groups objects integer IDs treated objects
(all visual features available them). IDs stored together list
constituent objects IDs, groups broken apart necessary.
Finally, visible lexicon file Appendix B, lexical item stored set
associated parameters. parameters specify grammatical type, compositional arity
reference behaviour (what word sense taken referring own:
single object, group objects objects.) Furthermore, lexical item associated
444

fiGrounded Semantic Composition Visual Scenes

semantic composer (see Sections 3.3 4) store sets parameters,
specifying Gaussian together applicable dimensions case
probabilistic composers.
3.3 Lexical Entries Concepts
Conceptually, treat lexical entries classes object oriented programming language. instantiated, maintain internal state simple tag
identifying dimension along perform ordering, complex multidimensional probability distributions. entry function interface specifies
performs semantic composition. Currently, interface definition consists number
arrangement arguments entry willing accept, whereas type mismatches
handled composition rather enforced interface. Finally,
entry contain semantic composer encapsulates actual function combine
entry constituents parse. composers described in-depth
Section 4. lexicon used Bishop contains many lexical entries attaching different
semantic composers word. example, left either spatial relation
extremum. grammatical structure detected parser (see next Section)
determines compositions attempted given utterance.
composition, structures representing objects constituent references
passed lexical entries. refer structures concepts. entry
accepts zero concepts, produces zero concepts result
composition operation. concept lists entities world possible referents
constituent associated with, together real numbers representing ranking
due last composition operation. composer mark concept referring
previous visual scene, allow anaphoric reference (see Section 4.5). contains
flags specifying whether referent group objects single object (cones
vs. cone), whether uniquely pick single object ambiguous
nature (the vs. a). flags used post-processing stage determine
possible ambiguities conflicts.
lexicon, based development corpus, contains 93 words: 33 ADJ (adjectives), 2 CADJ (colour adjectives: green, purple), 38 N (nouns), 2 REL (relative
pronouns: that, which), 1 VPRES (present tense verbs: is), 2 RELVPRES (relative pronoun/present tense verb combinations: thats, its), 1 ART (the), 3 SPEC
(adjective specifiers: right (as right above), just), 7 P (prepositions), 4 specific
prepositions (POF, PAT, two versions PIN). complete lexicon specification
reproduced Appendix B.
3.4 Parsing
previous work used Markov models parse generate utterances
(Roy, 2002), employ context free grammars. grammars naturally let us
specify local compositional constraints iterative structures. Specifically, allow us
naturally perform grounded semantic composition whenever grammar rule syntactically
complete, producing partial understanding fragments every node parse tree.
parse structure utterance thus dictates compositions attempted. use
445

fiGorniak & Roy

bottom-up chart parser guide interpretation phrases (Allen, 1995). parser
advantage employs dynamic programming strategy efficiently reuse
already computed subtrees parse. Furthermore, produces sub-components
parse thus produces useable result without need parse specific symbol.
using dynamic programming approach assuming meanings parts
assembled meanings wholes. strictly committed assumption
future consider backtracking strategies necessary. note due
fact framework often produces functions applied later stages
interpretation (see section 4) avoid possible overcommitting decisions (excluding
correct referent early stage understanding).
Bishop performs partial parse, parse required cover whole
utterance, simply takes longest referring parsed segments best guess.
Unknown words stop parse process. Rather, constituents would otherwise
end unknown word taken include unknown word, essence making
unknown words invisible parser understanding process. way recover
essentially grammatical chunks relations important understanding
restricted task. overview related partial parsing techniques, see work Abney
(1997).
grammar used partial chart parser shown Figure 1. Together
grammar rules table shows argument structures associated rule.
given grammar one argument structure per rule,
number argument structures. design grammar constrained
compositions must occur rule applied. especially seen
prepositional phrases, must occur rule noun phrase modify.
chart parser incrementally builds rule fragments left right fashion parse.
rule syntactically complete, checks whether composers constituents
tail rule accept number arguments specified rule (as shown
last column Table 1). so, calls semantic composer associated
constituent concepts yielded arguments produce concept head
rule. compose operation fails reason (the constituent cannot accept
arguments compose operation yield new concept) rule
succeed produce new constituent. several argument structures
(not case final grammar shown here) compose operation yields several
alternative concepts, several instances head constituent created,
concept.
provide example chart produced grammar Figure 10 Section 5,
part example whole understanding process. composition actions associated
lexical item, thus rule completion using grammar, listed
Appendix B.
3.5 Post-Parse Filtering
parse performed, post-parse filtering algorithm picks best interpretation utterance. First, algorithm extracts longest constituents
chart marked referring objects, assuming parsing utter446

fiGrounded Semantic Composition Visual Scenes

ADJ
NP
NP
NP
NP
NP
NP
NP
NP
NP
NP
NP
NP
NP
NP
NP
P
P
P





















T0
ADJ
ADJ
CADJ
N
ART
NP
NP
NP
NP
NP
NP
NP
NP
NP
NP
NP
SPEC
P
POF

T1
ADJ
NP
N

T2

NP
P
P
RELVPRES
P
REL
REL
REL
RELVPRES
REL
RELVPRES
REL
P
P

NP
ART
P
N
VPRES
P
VPRES
P
VPRES
ADJ
CADJ

T3

T4

T5

T6

N
ART
POF
NP
NP
P
NP
ADJ

POF
N
NP

NP
POF

NP

NP

Arg Structure
T1 (T0 )
T0 (T1 )
T0 (T1 )
T0 ()
T0 (T1 )
T1 (T0 , T2 )
T3 (T0 , T5 )
T3 (T0 , T5 )
T2 (T0 , T4 )
T1 (T0 , T3 )
T2 (T0 , T3 )
T3 (T0 , T4 )
T2 (T0 , T3 )
T3 (T0 )
T2 (T0 )
T2 (T0 )
T0 (T1 )
T1 ()
T0 ()

Table 1: Grammar used Bishop
ance implies better understanding. filtering process checks candidates
consistency along following criteria:
candidates must either refer group single object
candidates marked referring unambiguously specified single object,
must unambiguously pick referent
referent specified single object case must across candidates
candidates marked selecting group, must select group
consistency checks fail, filtering algorithm provide exact information type inconsistency occurred (within-group ambiguity, contradicting
constituents, object fulfilling description), constituents involved
inconsistency objects (if any) referenced candidate constituent.
future, plan use information resolve inconsistencies active dialogue.
Currently, enforce best single object choice post processing stage.
filtering yields single object, nothing needs done. filtering yields group
objects, choose best matching object (note case ignore fact
whether resulting concept marked referring group single object).
several inconsistent groups referents remain filtering, randomly pick one object
groups.
447

fiGorniak & Roy

4. Semantic Composition
section revisit list descriptive strategies Section 2.3 explain
computationally capture strategy composition parts
utterance. composers presented follow composition schema: take
one concepts arguments yield another concept references possibly
different set objects. Concepts reference objects real numbered values indicating
strength reference. Composers may introduce new objects, even ones
exist scene such, may introduce new types objects (e.g. groups
objects referenced one object). perform compositions, concept
provides functionality produce single referent, group referents. single
object produced simply one maximum reference strength, whereas group
produced using reference strength threshold objects considered
possible referents concept. threshold relative minimum maximum
reference strength concept. composers first convert incoming concept
objects references, subsequently perform computations objects.
Furthermore, composers mark concepts referring, referring single
object referring group objects. independent actual number
objects yielded concept, used identify misinterpretations ambiguities. currently use flags delay composition arguments refer
objects. example, constraint prevents left green cause composition green considered adjective. cases, new chaining semantic
composers created delay application whole chain compositions
referring word encountered. chaining composers internally maintain queue
composers. argument composition operation refer object,
composer producing argument composer accepting pushed onto
queue. first referring argument encountered, whole queue composers
executed starting new argument proceeding backwards order
composers encountered.
plan exploit features co-operative setting one
described here, system engage clarifying dialogue user. explain
Section 3.5 converged single object reference task discussion
here, alternatives would be.
4.1 Colour - Probabilistic Attribute Composers
mentioned Section 3.1, chose exploit information used render
scene, therefore must recover colour information final rendered image.
hard problem Bishop presents virtual objects two colours.
renderer produce colour variations objects due different angles distances
light sources camera. colour average 2D projection object
varies due occlusion objects. interest making framework easily
transferable noisier vision system, worked within probabilistic framework.
separately collected set labelled instances green purple cones, estimated
three dimensional Gaussian distribution average red, green blue values
pixel belonging example cones.
448

fiGrounded Semantic Composition Visual Scenes

asked compose given concept, type probabilistic attribute composer assigns object referenced source concept probability density function
evaluated measured average colour object.
4.2 Spatial Extrema Spatial Regions - Ordering Composers
determine spatial regions extrema, ordering composer orders objects along
specified feature dimension (e.g. x coordinate relative group) picks referents
extreme end ordering. so, assigns exponential weight function objects
according
i(1+v)
picking minimal objects, objects position sequence, v value
along feature dimension specified, normalized range 0 1 objects
consideration. maximal case weighted similarly, using reverse ordering
subtracting fraction exponent 2,
(imax i)(2v)
imax number objects considered. reported results = 0.38.
formula lets referent weights fall exponentially position ordering distance extreme object. way extreme objects isolated
except cases many referents cluster around extremum, making picking
single referent difficult. attach type composer words leftmost
top.
ordering composer order objects according absolute position, corresponding closely spatial regions rather spatial extrema relative group.
reference strength formula version


(1+ dmax )
euclidean distance reference point, dmax maximum
distance amongst objects consideration.
version composer attached words middle. effect
reference weights relative absolute position screen. object close centre board achieves greater reference weight word middle, independently
position objects kind. Ordering composers work across number
dimensions simply ordering objects Euclidean distance, using exponential falloff function cases. ordering composer middle,
example, computes distance board centre centres mass objects,
thus prefers centred screen.
4.3 Grouping Composers
non-numbered grouping (e.g., describer says group cones), grouping
composer searches scene groups objects within maximum distance
threshold another group member. threshold currently set hand based
small number random scenes designers identified isolated groups
449

fiGorniak & Roy

adjusted threshold correctly find others. considers objects
referenced concept passed argument. numbered groups
(two, three), composer applies additional constraint groups
contain correct number objects. Reference strengths concept determined
average distance objects within group. acknowledge approach
grouping simplistic currently investigating powerful visual grouping
algorithms take topological features consideration. spite simple approach,
demonstrate instances successfully understanding references groups
Bishop.
output grouping composer may thought group groups. understand motivation this, consider utterance, one left group
purple ones. expression, phrase group purple ones activate grouping
composer find clusters purple cones. cluster, composer computes
convex hull (the minimal elastic band encompasses objects) creates
new composite object convex hull shape. composition
takes place understand entire utterance, composite group serves potential
landmark relative left.
However, concepts marked behaviour changes split apart concepts
refering groups. example, composer attached sets flag concepts
passing it. Note involved composition grammar rules
type NP NP P NP, performing spatial compositions phrases
left of. Therefore, phrase frontmost one three green ones pick
front object within best group three green objects.
4.4 Spatial Relations - Spatial Composers
spatial semantic composer employs version Attentional Vector Sum (AVS)
suggested Regier Carlson (2001). AVS measure spatial relation meant
approximate human judgements corresponding words left
2D scenes objects. computes interpolation angle
centres masses objects angle two closest points objects,
addition value depending height relative top landmark object.
Despite participants talking 2D projections 3D scenes found AVS
distinguishes spatial relations used data rather well simply applied
2D projections. participants often used spatial descriptors below, suggesting
sometimes conceptualized scenes 2D. 3D setting would expect
see consistent use semantic patterns front instead below.
Given two concepts arguments, spatial semantic composer converts sets
objects, treating one set providing possible landmarks, providing possible
targets. composer calculates AVS possible combination landmarks
targets. reference vector used AVS specified lexical entry containing
composer, e.g. (0, 1) behind. Finally, spatial composer divides result
Euclidean distance objects centres mass, account fact
participants exclusively used nearby objects select spatial relations.
450

fiGrounded Semantic Composition Visual Scenes

4.5 Anaphoric Composers
Triggered words (as left one) previous, anaphoric
composer produces concept refers single object, namely last object removed
scene session. object specially marks concept referring
current, previous visual scene, calculations concept
performed visual context.
example, parser calls upon anaphoric composer attached lexical entry provide interpretation one, composer marks
produced concept referring back previous visual scene, sets previously selected object possible referent. consider another composer, say spatial
composer attached left one left one. asks spatial
relation features referents one one, spatial relation
features (see Section 4.4) computed previous visual scene object
removed due previous utterance possible landmark spatial
relation.

5. Example: Understanding Description

example scene

purple one

one left

purple one left

Figure 9: Example: purple one left
illustrate operation overall system, section step
examples Bishop works detail. Consider scene top left Figure 9,
output chart parser utterance, purple one left Figure 10.
Starting top left parse output, parser finds lexicon ART
(article) selecting composer takes one argument. finds two lexical entries
purple, one marked CADJ (colour adjective), one N (noun).
composer, probabilistic attribute composer marked P(),
adjective expects one argument whereas noun expects none. Given noun
expects arguments grammar contains rule form NP N, NP
(noun phrase) instantiated probabilistic composer applied default set
objects yielded N, consists objects visible. composer call marked
451

fiGorniak & Roy

P(N) chart. composition, NP contains subset purple objects
(Figure 9, top right). point parser applies NP ART NP, produces
NP spanning first two words contains purple objects, marked
unambiguously referring object. S(NP) marks application selecting
composer called S.

ART:the

purple

one





left

CADJ:purple
N:purple
NP:P(N)
NP:S(NP)
N:one
NP:one
NP:P(N)
NP:S(NP)
P:on
ART:the
N:left
ADJ:left
N:left
NP:left
NP:left
NP:S(NP)
NP:S(NP)
NP:O.x.min(NP)
NP:O.x.min(NP)
NP:O.x.min(NP)

Figure 10: Sample parse referring noun phrase

parser goes produce similar NP covering first three words combining
purple CADJ one result the. P (preposition) left
dangling moment needs constituent follows it. contains modifying
semantic composer simply bridges P, applying first argument second.
another the, left several lexical entries: ADJ one N forms
contains ordering semantic composer takes single argument, whereas second N
form contains spatial semantic composer takes two arguments determine target
landmark object. point parser combine left two
possible NPs, one containing ordering spatial composer. first
NPs turn fulfills need P second argument according NP
NP P NP, performing ordering compose first one (for one left), selecting
objects left (Figure 9, bottom left). application ordering composer
452

fiGrounded Semantic Composition Visual Scenes

denoted O.x.min(NP) chart, indicating ordering composer ordering
along x axis selecting minimum along axis. combining purple
one, composer selects purple objects left (Figure 9, bottom right).
Finally purple one, produces set objects purple one, marks
concept unambiguously picking single object. Note parser attempts
use second interpretation left (the one containing spatial composer) fails
composer expects two arguments provided grammatical
structure sentence.

6. Results Discussion
section first discuss systems overall performance collected data, followed detailed discussion performance implemented descriptive strategies.
6.1 Overall Performance
evaluation purposes, hand-annotated data, marking descriptive strategies
occurred example. examples use several reference strategies. Table 2
present overall accuracy results, indicating percentage different groups
examples system picked referent person describing object. first
line table shows performance relative total set utterances collected.
second one shows percentage utterances system understood correctly excluding
marked using descriptive strategy listed Section 4, thus
expected understood Bishop. examples given Section 2.3.6.
final line Table 2 shows percentage utterances system picked
correct referent relative clean development testing sets, leaving utterances
marked well marked containing kind error. defined
earlier, could speech error still understood human listener, due
error algorithm pairs utterances selection events. Additionally, relying
automatic speech segmentation sometimes merged utterances one
separate utterances. mistakenly attributes combination two descriptions
one object selection leaves another object selection without corresponding utterance.
Note, however, due loose parsing strategy frequent redundancies
speakers utterances system able handle good number utterances marked
either error.
Utterance Set

except
except Errors (clean)

Accuracy - Development
76.5%
83.2%
86.7%

Accuracy - Testing
58.7%
68.8%
72.5%

Table 2: Overall Results

Using unconstrained speech primarily made writing covering yet precise grammar difficult. difficulty together loose parsing strategy made system occasionally
attempt compositions supported grammatical structure utterance.
453

fiGorniak & Roy

overeager parsing strategy produces number correct guesses would
found tighter grammar, found development tradeoff often
favoured looser parsing terms number correct responses produced. Constructing
grammar obvious area addressed machine learning approach
future. Using speech segmenter together utterance reassembler produced
errors used successful selection event strong guideline deciding
speech segments part description. Errors type occur less 1%
data.
Bishop Performance

100

random guess mean
random guess mean +/ std dev

90
80

Average Accuracy

70
60
50
40
30
20
10
0

1

2

3

4

development

development clean

test

test clean

Figure 11: Results development test corpora

Figure 11 graphs results corpus simulation uniform random
selection strategy. bar shows mean performance data set, error bars delimiting one standard deviation. figure shows results left right complete
development corpus, clean development corpus, complete test corpus clean
test corpus. system understands vast majority targeted utterances performs
significantly better random baseline. Given unconstrained nature input
complexity descriptive strategies described Section 2.3 consider
important achievement.
Table 3 provides detail various descriptive strategies lists percentage correctly identified referents utterances employing spatial extrema regions,
454

fiGrounded Semantic Composition Visual Scenes

combinations one spatial extremum, grouping constructs, spatial relations
anaphora. Note categories mutually exclusive. list
separate results utterances employing colour terms colour terms
source errors (due synthetic nature vision system).
Utterance Set
Spatial Extrema
Combined Spatial Extrema
Grouping
Spatial Relations
Anaphora

Accuracy - Development
86.8% (132/152)
87.5% (49/56)
34.8% (8/23)
64.3% (9/14)
100% (6/6)

Accuracy - Test
77.4% (72/93)
75.0% (27/36)
38.5% (5/13)
40.0% (8/20)
75.0% (3/4)

Table 3: Detailed Results
surprisingly, Bishop makes mistakes errors present strategies
implemented occur. However, Bishop achieves good coverage even
cases. often result overspecification part describer.
tendency towards redundancy shows even simple cases, example use
purple even though purple cones left scene. translates furthermore
specifications relative groups objects simple leftmost would
suffice. Overspecification human referring expressions well-known phenomenon often
attributed incremental nature speech production. Speakers may listing visually
salient characteristics colour determining whether colour distinguishing
feature intended referent (Pechmann, 1989).
worst performance, grouping composers, attributed
fact visual grouping strategy simplistic task hand,
phenomenon often combined rather complex ways strategies.
combinations account number mistakes amongst composer
perform much better combined strategies grouping. cover
shortcomings grouping composers detail Section 6.2.3.
Mistakes amongst descriptive strategies cover several causes:
Overcommitment/undercommitment errors due fact interpretation implemented filtering process without backtracking. semantic
composer must produce set objects attached reference strengths,
next composer works set objects strictly feedforward manner.
composition strategy fails target object left one stage (e.g.
leftmost one front, leftmost selects leftmost objects, including
obvious example front good example leftmost).
fails many target objects included (e.g. poor example leftmost
included set turns ideal example front). Estimating
group membership thresholds data certainly decrease occurrence
errors, real solution lies backtracking strategy combined composers sensitive visual scenery beyond immediate function.
sensitive composers might take account facts isolated nature certain
455

fiGorniak & Roy

candidates well global distribution cones across board. discuss
specific cases global local visual context influence interpretations
words Section 6.2.
Insufficient grammar cases contain many prepositional phrases (e.g.
leftmost one group purple ones right bottom) grammar
specific enough produce unambiguous answers. grammar might attach
right object rather group objects, taking account
biases parsing human listeners showed.
Flawed composers composers implemented sufficient
understand facets corresponding human descriptive strategies.
mention problems following section.
6.2 Performance Composers
go reconsider descriptive strategy discuss successes failures
composers designed deal each.
6.2.1 Colour
Due simple nature colour naming Bishop task, probabilistic composers
responsible selecting objects based colour made errors.
6.2.2 Spatial Regions Extrema
ordering composers correctly identify 100% cases participant uses
colour single spatial extremum description. conclude participants
follow process yields result ordering objects along spatial dimension
picking extreme candidate. Participants favour descriptive strategy, using
colour alone 38% clean data. Figure 3 provides examples type
system handles without problems.
Description spatial region occurs alone 5% clean data, together
strategies 15% clean data. Almost examples strategy
occurring alone use words middle centre. left image Figure 12 exemplifies
use middle ordering semantic composer models. object referred
one closest centre board. model fact human speakers
use version descriptive strategy obvious single candidate object.
right image Figure 12 shows different interpretation middle: object
middle group objects. Note group objects linguistically mentioned.
note within group two candidate centre objects, one
front preferred. composer picks correct object use middle
target object happens one closest centre board.
Figure 13 shows another use word middle. strategy seems related
last one (picking object middle group), however scene happens
divided two groups objects single object them. Even though
object back closest one centre board, due visual
456

fiGrounded Semantic Composition Visual Scenes

green one middle

purple cone middle

Figure 12: Types middles 1
context participants understand object middle. composer fails
case.

purple one middle
Figure 13: Types middles 2
Figure 14 shows sequence two scenes followed data collection
session. first scene utterance clear example extremum combined
region specification, ordering composers easily pick correct object.
next scene, listener identified leftmost object one right middle,
despite scenes similarity right image Figure 12, middle object
middle group. suspect use middle scene
biases understanding middle relative board case, providing
example visual, historical context influence meanings
words. (Note right utterance right middle interpreted
SPEC grammatical type Bishop, spatial role. See grammar
Table 1.)

green one middle front

purple one right middle

Figure 14: Types middles 3
457

fiGorniak & Roy

summary, catalogue number different meanings word middle
data linguistically indistinguishable, depend visual historical
context correctly understood. generally, impossible distinguish regionbased uses various extrema-based uses words based utterance alone
data. made decision treat middle signify regions left, top, etc.
signify extrema, examples middle show selection meaning
words use depends far subtler criteria global local visual context,
existence unambiguous candidate past use descriptive strategies.
Participants composed one spatial region extrema references 30%
clean data. ordering composers correctly interpret 85% cases, example
Figure 4 Section 2.3.2. mistakes composers make usually due
overcommitment faulty ordering. Figure 15 shows example could interpreted
either problem (we indicate correct example object system selects).
note example comes non-native English speaker often
used native speakers would use in. system selects purple object
closest back board instead indicated correct solution. could
interpreted overcommitment, composer back include
target object, leaving composer left wrong set objects choose from.
better explanation perhaps ordering composers reversed
case, composer back take objects selected left input.
However, ordering violates far common left-to-right ordering region
extrema strategies data, selected implement system. question
thus becomes causes difference ordering cases one Figure 15.
again, suspect visual context plays role. Perhaps clear listener
double spatial specification would overspecification object system
selects (it simply purple one back). response, listener may seek
object needs full utterance, true target. However, analysis
hard combine common trend towards overspecification part
speaker, leaving us need run focused study phenomena pin
factors play role interpretation.

purple cone back left side
Figure 15: Misinterpreted utterance using composed extrema

458

fiGrounded Semantic Composition Visual Scenes

6.2.3 Grouping
composers implementing grouping strategies used participants simplistic composers implemented, compared depth actual phenomenon
visual grouping. left scene Figure 16 shows example grouping composer
handles without problem. group two cones isolated cones
example, thus easily found distance thresholding algorithm. contrast,
right scene depicts example would require much greater sophistication find
correct group. target group three cones visually isolated scene,
requiring criteria colinearity even make candidate. Furthermore,
second colinear group three cones would easily best example row
three purple cones absence target group. target groups
alignment vertical axis let stand row make
likely interpretation. algorithm currently fails include grouping hints,
thus fails pick correct answer scene. Note hints always linguistically marked (through row), often colinearity silently
assumed holding groups, making simple grouping operator fail. rich source
models possible human grouping strategies co-linearity comes research
Gestalt Grouping (Wertheimer, 1999).

cone right pair
cones

purple cone front
row three purple cones

Figure 16: Easy hard visual grouping

6.2.4 Spatial Relations

green cone behind purple cone
Figure 17: Successful spatial relation understanding
AVS measure divided distance objects corresponds well human
spatial relation judgements task. errors occur utterances contain
459

fiGorniak & Roy

spatial relations due possible landmarks targets correctly identified
(grouping region composers might fail provide correct referents). spatial
relation composer picks correct referent cases landmarks targets
correct ones, example Figure 17. see next section correct
example spatial relations. Obviously, types spatial relations relations
based purely distance combined relations (to left behind) decided
cover implementation, occur data covered
future efforts.
6.2.5 Anaphora

cone right front

cone behind left

Figure 18: Successful Anaphora Understanding
solution use anaphora Bishop task performs perfectly replicating
reference back single object clean data. reference usually combined
spatial relation data, Figure 18. Due equally good performance
spatial relation composer, cover cases anaphora development data. However,
complex variants anaphora currently cover, example
reference back groups objects sequence Figure 19, follows
right example Figure 16.

next cone row

last cone row

Figure 19: Group Based Anaphora

7. Future Directions
Given analysis Bishops performance, several areas future improvements may explored. descriptive strategies classified
understood computational system:
460

fiGrounded Semantic Composition Visual Scenes

Distance simple implementation understand strategy grammatical behaviour spatial relation composers, uses inverted distance measure
score target objects.
Symmetry Selection symmetry occurred symmetry across horizontal centre
board data. thus propose mirror landmark object across
horizontal centre, scoring possible targets inverted distance mirror
image.
Numbered grouping limited groups two three objects,
algorithm work higher numbers.
In-group numbering descriptive strategy second row understood slight modification ordering composer put peak
exponential distribution ends middle sequences,
rather arbitrary points.
Connectivity simple way understand lonely cone could measure distance
closest objects within group possible referents. better solution might
construct local connectivity graph look topologically isolated objects.
Furthermore, already mentioned several areas possible improvement
existing system due faulty assumptions:
Individual composers Every one semantic composers attempts solve separate
hard problem, (e.g. grouping spatial relations) seen long
lines work dedicated sophisticated solutions ours. individual
problems emphasis paper. believe improvements
implementation improve system whole, much following
possible techniques.
Backtracking lack backtracking Bishop addressed. parse
produce single referent, backtracking would provide opportunity revise
loosen decisions made various stages interpretation referent
produced.
Visual context semantics Backtracking solves problems system knows
either failed obtain answer, knows answer produced
unlikely one. However, numerous examples data one
interpretation utterance produces perfectly likely answer according
measurements, example middle finds object exact centre screen.
many scenes interpretation produces correct answer, measurement
relative objects would produce wrong one. However, observe
participants interpret middle way obvious structure
rest scene. chance scene divided group objects left
group objects right, middle reliably refer isolated object
groups, even another object closer actual centre
screen. future system take account local global visual context
composition account human selection strategies.
461

fiGorniak & Roy

Lexical entries made assumption lexical entries word-like entities
contain encapsulated semantic information. Even relatively constrained
task, somewhat faulty assumption. example, ambiguity word
resolved careful design grammar (see section 4.3), may
useful treat entire phrases left single lexical entries, perhaps
grammar replace left spatial markers (Jackendoff
proposed absorbing rules syntax lexicon, see Jackendoff, 2002).
Dialogue constructing parse charts obtain rich set partial full syntactic
semantic fragments offering explanations parts utterance. present,
largely ignore information-rich resource selecting best referent.
successful approach might entail backtracking revision described above,
engage clarification dialogue human speaker. system could
use fragments knows check validity interpretation (is
group green ones mean?) could simply disambiguate directly (Which
two mean?) followed explanation confusion terms
semantic fragments formed.
Manual construction visually-grounded lexicon presented limited
accuracy due various structural parametric decisions manually approximated. Machine learning algorithms may used learn many parameter settings
set hand work, including on-line learning adapt parameters
verbal interaction. Although thresholds probability distribution functions may
estimated data using relatively straightforward methods, learning problems
far challenging. example, learning new types composers appropriate
corresponding grammatical constructs poses difficult challenge future. Minimally,
plan automate creation new versions old composers (e.g. applied different
dimensions attributes). Moving beyond this, clear how, example, set
handling functionality used determine groups referents expand automatically
useful ways. interesting study people learn understand novel
descriptive strategies.
continuing work applying results grounded language systems
multimodal interface design (Gorniak & Roy, 2003). recently demonstrated application Bishop system described paper problem referent resolution
graphical user interface 3D modelling application, Blender (Blender Foundation ,
2003). Using Bishop/Blender hybrid, users select sets objects correct wrong
mouse selections voice commands select door behind one show
windows.

8. Summary
presented model visually-grounded language understanding. heart
model set lexical items, grounded terms visual features grouping
properties applied objects scene. robust parsing algorithm finds chunks
syntactically coherent words input utterance. determine semantics
phrases, parser activates semantic composers combine words determine
462

fiGrounded Semantic Composition Visual Scenes

joint reference. robust parser able process grammatically ill-formed transcripts
natural spoken utterances. evaluations, system selected correct objects response
utterances 76.5% development set data, 58.7% test set data.
clean data sets various speech processing errors held out, performance higher
yet. suggested several avenues improving performance system including better
methods spatial grouping, semantically guided backtracking sentence processing,
use machine learning replace hand construction models, use interactive
dialogue resolve ambiguities. near future, plan transplant Bishop
interactive conversational robot (Hsiao et al., 2003), vastly improving robots ability
comprehend spatial language situated spoken dialogue.

Acknowledgments
Thanks Ripley, Newt Jones.

Appendix A. Utterances Test Data Set
following 179 utterances collected test data set. presented
correct order seen understanding system. means include
errors due faulty speech segmentation well due algorithm stitches oversegmented utterances back together.
























green cone middle
purple cone behind
purple cone way left
purple cone corner right
green cone front
green cone back next purple cone
purple cone middle front
purple cone middle
frontmost purple cone
green cone corner
obstructed green cone
purple cone hidden back
purple cone right rear
green cone front
solitary green cone
purple cone front row three purple cones
next cone row
last cone row
cone right pair cones
cone
cone closest middle front
cone right set cones furthest left
cone right front
463

fiGorniak & Roy

cone behind left
frontmost left cone
backmost left cone
solitary cone
cone middle right
front cone cone
frontmost green cone
green cone front right purple cone
green cone back row four
cone green cone behind purple cone
purple cone behind row three green cones
frontmost green cone right
green cone corner back
green cone back
purple cone back right
green cone front left
purple cone behind
purple cone behind one
green cone behind purple cone
green cone two purple cone
purple cone front
purple cone touching green cone
green cone front
purple cone left
green cone back left
purple cone middle front two purple cones
purple cone left four green cones
purple cone left leftmost
green cone
frontmost green cone
rear cone
rightmost cone
rearmost cone
left green cone
purple cone green cone
green cone
furthestmost green cone exact middle
frontmost green cone
rightmost green cone clump four green cones right
green cone front two purple cones near left
green cone two purple cones back middle
frontmost purple cone
leftmost two purple cones right mean left,
sorry leftmost two purple cones left side
green cone left halfway back
464

fiGrounded Semantic Composition Visual Scenes

























frontmost green cone front purple cone
middle purple cone
green cone left left
green cone middle front green cone
green cone left
furthestmost purple cone left
furthest green cone
leftmost green cone
leftmost purple cone
middle green cone
green cone two green cones
frontmost purple cone
backmost purple cone
green cone two purple cones nearest front
leftmost purple cone
green cone front
green cone
frontmost two back purple cones
rightmost purple cone
leftmost purple cone
purple cone front
last purple cone
frontmost purple cone clump five purple cones
right
backmost green cone
backmost purple cone
green cone directly front purple cone
purple cone behind green cone left
green cone behind purple cone left
leftmost two left back corner green cones
rightmost purple cone
middle cone behind frontmost purple cone
green cone left front corner
purple cone right back corner
third green cone line green cones near middle
green cone two purple cones near back
green cone back left
green cone back
green cone behind frontmost green cone
frontmost green cone
green cone
last line four purple cones
centre purple cone three cones left
purple cone two purple cones
middle purple cone
465

fiGorniak & Roy












leftmost purple cone
middle purple cone
front left purple cone
front right purple cone
second four purple cones
middle purple cone
purple cone left
last purple cone
green one middle way back
purple one way middle little
left way back
green one middle front thats front another
green one
purple one middle thats behind green one
right purple one front line
purple ones
left green one two purple ones line
left purple one middle row mean middle
line three purple ones
green one left thats hidden purple one
left purple one thats way corner separate
middle towards right theres line purple ones
theres kink line one thats right lines turns
purple one way right front
purple one front middle
green one middle
purple front way right
rightmost green one
leftmost green one
last green one last green one
frontmost purple one right
purple one back towards left thats next two
purple ones
purple one back towards right thats part pair
purple one front group right
purple one middle thats front group one right
purple one left way back
purple one left thats behind another purple one
purple one left thats front purple one
left thats
purple one right thats hidden two purple ones
purple one way back corner right
purple one thats front right last one
last one
purple front right
466

fiGrounded Semantic Composition Visual Scenes








purple one way right
green one right thats middle bunch
green one way left thats almost totally obscured
last purple one left crooked line purple ones
first purple one left thats crooked line
purple one way one way back towards
left thats behind green purple one way
back
purple one towards back thats pretty much back
thats front green purple one
purple one middle back
purple one left thats furthest back
green one middle thats furthest front
purple one towards middle towards left
thats closest
middle purple one stands thats closest
purple ones middle towards right one
corner
purple one thats closest middle
way right green one middle line three
green ones
way right closest green one
way right close green one
green one way right corner back
purple one thats towards back left corner
purple one front left corner
purple one near middle thats another purple one
purple one thats front another purple one
purple one right
purple one left
green one middle thats behind another green one
closest green one middle green one thats closest
middle
green one way back towards right
close green one one left
one left

Appendix B
following specifies complete lexicon used Bishop XML format. initial
comment explains attributes lexical entries.
see online appendix file lexicon.xml.

467

fiGorniak & Roy

References
Abney, S. (1997). Part-of-speech tagging partial parsing. Corpus-Based Methods
Language Speech, chap. 4, pp. 118136. Kluwer Academic Press, Dordrecht.
Allen, J. (1995). Natural Language Understanding, chap. 3. Benjamin/Cummings
Publishing Company, Inc, Redwood City, CA, USA.
Bailey, D. (1997). push comes shove: computational model role motor
control acquisition action verbs. Ph.D. thesis, Computer science division,
EECS Department, University California Berkeley.
Barker, C. (2002). dynamics vagueness. Linguistics Philosophy, 25, 136.
Blender Foundation (2003). Blender 3D graphics creation suite. http://www.blender3d.org.
Brown, M., Buntschuh, B., & Wilpon, J. (1992). SAM: perceptive spoken languageunderstanding robot. IEEE Transactions Systems, Man Cybernetics, 6 (22),
13901402.
Brown-Schmidt, S., Campana, E., & Tanenhaus, M. K. (2002). Reference resolution
wild. Proceedings Cognitive Science Society.
Carletta, J., & Mellish, C. (1996). Risk-taking recovery task-oriented dialogue.
Journal Pragmatics, 26, 71107.
Desolneux, A., Moisan, L., & Morel, J. (2003). grouping principle four applications.
IEEE Transactions Pattern Analysis Machine Intelligence, 255 (4), 508513.
Dhande, S. (2003). computational model connect gestalt perception natural
language. Masters thesis, Massachusetts Institure Technology.
Engbers, E., & Smeulders, A. (2003). Design considerations generic grouping vision.
IEEE Transactions Pattern Analysis Machine Intelligence, 255 (4), 445457.
Eugenio, B. D., Jordan, P. W., Thomason, R. H., & Moore, J. D. (2000). agreement
process: empirical investigation human-human computer-mediated collaborative
dialogues. International Journal Human-Computer Studies, 53 (6), 10171076.
Gorniak, P., & Roy, D. (2003). Augmenting user interfaces adaptive speech commands.
Proceedings International Conference Multimodal Interfaces.
Griffin, Z., & Bock, K. (2000). eyes say speaking. Psychological Science,
11, 274279.
Haddock, N. (1989). Computational models incremental semantic interpretation. Language Cognitive Processes, 4, 337368.
Hsiao, K., Mavridis, N., & Roy, D. (2003). Coupling perception simulation: Steps
towards conversational robotics. Proceedings IEEE/RSJ International Conference Intelligent Robots Systems (IROS).
Jackendoff, R. (2002). Whats lexicon?. Noteboom, S., Weerman, F., & Wijnen
(Eds.), Storage Computation Language Faculty, chap. 2. Kluwer Academic
Press.
468

fiGrounded Semantic Composition Visual Scenes

Kyburg, A., & Morreau, M. (2000). Fitting words: vague words context. Linguistics
Philosophy, 23, 577597.
Lammens, J. M. (1994). computational model color perception color naming. Ph.D.
thesis, State University New York.
Landau, B., & Jackendoff, R. (1993). spatial language spatial
cognition. Behavioural Brain Sciences, 2 (16), 217238.
Miller, G., & Johnson-Laird, P. (1976). Language Perception. Harvard University Press.
Nagao, K., & Rekimoto, J. (1995). Ubiquitous talker: Spoken language interaction
real world objects. Proceeding International Joint Conference Artificial
Intelligence.
Narayanan, S. (1997). KARMA: Knowledge-based Action Representations Metaphor
Aspect. Ph.D. thesis, Univesity California, Berkeley.
Partee, B. H. (1995). Lexical semantics compositionality. Gleitman, L. R., & Liberman, M. (Eds.), Invitation Cognitive Science: Language, Vol. 1, chap. 11, pp.
311360. MIT Press, Cambridge, MA.
Pechmann, T. (1989). Incremental speech production referential overspecification. Linguistics, 27, 89110.
Pustejovsky, J. (1995). Generative Lexicon. MIT Press, Cambridge, MA, USA.
Regier, T. (1996). Human Semantic Potential. MIT Press.
Regier, T., & Carlson, L. (2001). Grounding spatial language perception: empirical computational investigation. Journal Experimental Psychology: General,
130 (2), 273298.
Roy, D. (2002). Learning visually-grounded words syntax scene description task.
Computer Speech Language, 16 (3).
Roy, D., Gorniak, P. J., Mukherjee, N., & Juster, J. (2002). trainable spoken language
understanding system. Proceedings International Conference Spoken Language Processing.
Roy, D., & Pentland, A. (2002). Learning words sights sounds: computational
model. Cognitive Science, 26 (1), 113146.
Schuler, W. (2003). Using model-theoretic semantic interpretation guide statistical parsing word recognition spoken language interface. Proceedings Association Computational Linguistics.
Shi, J., & Malik, J. (2000). Normalized cuts image segmentation. IEEE Transactions
Pattern Analysis Machine Intelligence, 8 (22), 888905.
Siskind, J. M. (2001). Grounding lexical semantics verbs visual perception using
force dynamics event logic. Journal Artificial Intelligence Research, 15, 3190.
Wertheimer, M. (1999). Laws organization perceptual forms. source book
Gestalt psychology, pp. 7188. Routledge, New York.
Winograd, T. (1970). Procedures representation data computer program
understanding natural language. Ph.D. thesis, Massachusetts Institute Technology.
469

fiGorniak & Roy

Yoshida, N. (2002). Utterance segmenation spontaneous speech recognition. Masters
thesis, Massachusetts Institute Technology.

470


