Journal Artificial Intelligence Research 21 (2004) 1-17

Submitted 7/03; published 1/04

Effective Dimensions Hierarchical Latent Class Models
Nevin L. Zhang

lzhang@cs.ust.hk

Department Computer Science
Hong Kong University Science Technology, China

Tomas Kocka

kocka@lisp.vse.cz

Laboratory Intelligent Systems Prague
Prague University Economics, Czech Republic

Abstract
Hierarchical latent class (HLC) models tree-structured Bayesian networks
leaf nodes observed internal nodes latent. theoretically well
justified model selection criteria HLC models particular Bayesian networks
latent nodes general. Nonetheless, empirical studies suggest BIC score
reasonable criterion use practice learning HLC models. Empirical studies
suggest sometimes model selection improved standard model dimension
replaced effective model dimension penalty term BIC score.
Effective dimensions difficult compute. paper, prove theorem
relates effective dimension HLC model effective dimensions number
latent class models. theorem makes computationally feasible compute
effective dimensions large HLC models. theorem used compute
effective dimensions general tree models.

1. Introduction
Hierarchical latent class (HLC) models (Zhang, 2002) tree-structured Bayesian networks
(BNs) leaf nodes observed internal nodes latent. generalize latent
class models (Lazarsfeld Henry, 1968) first identified potentially useful
class Bayesian networks Pearl (1988). concerned learning HLC models
data. fundamental question select among competing models.
BIC score (Schwarz, 1978) popular metric researchers use select among
Bayesian network models. consists loglikelihood term measures fitness
data penalty term depends linearly upon standard model dimension, i.e.
number linearly independent standard model parameters. variables
observed, BIC score asymptotic approximation (the logarithm) marginal
likelihood (Schwarz, 1978). consistent sense that, given sufficient data,
BIC score generative model model data sampled larger
models equivalent generative model.
latent variables present, BIC score longer asymptotic approximation marginal likelihood (Geiger et al., 1996). remedied,
extent, using concept effective model dimension. fact replace standard model
dimension effective model dimension BIC score, resulting scoring function,
called BICe score, asymptotic approximation marginal likelihood almost
everywhere except singular points (Rusakov Geiger, 2002).
c
2004
AI Access Foundation. rights reserved.

fiZhang & Kocka

Neither BIC BICe proved consistent latent variable models.
matter fact, even defined means model selection criterion
consistent latent variable models. Empirical studies suggest BIC score
well-behaved practice task learning HLC models. three related searchbased algorithms learning HLC models, namely double hill-climbing (DHC) (Zhang,
2002), single hill-climbing (SHC) (Zhang et al., 2003), heuristic SHC (HSHC) (Zhang,
2003). absence theoretically well justified model selection criterion, Zhang (2002)
tested DHC four existing scoring functions, namely AIC score (Akaike, 1974),
BIC score, Cheeseman-Stutz (CS) score (Cheeseman Stutz, 1995), holdout
logarithmic score (HLS)(Cowell et al., 1999). real-world synthetic data used.
real-world data, BIC CS enabled DHC find models regarded
best domain experts. synthetic data, BIC CS enabled DHC find
models either identical resemble closely true generative models.
coupled AIC HLS, hand, DHC performed significantly worse. SHC
HSHC tested synthetic data sampled fairly large HLC models (as much
28 nodes). BIC used tests. cases, BIC enabled SHC
HSHC find models either identical resemble closely true generative
models. empirical results indicate algorithms perform well,
suggest BIC reasonable scoring function use learning HLC models.
experiments reveal model selection sometimes improved BICe
score used instead BIC score. explain detail Section 3
order use BICe score practice, need way compute effective dimensions. trivial task. effective dimension HLC model rank
Jacobian matrix mapping parameters model parameters
joint distribution observed variables. number rows Jacobian
matrix increases exponentially number observed variables. construction
Jacobian matrix calculation rank computationally demanding.
Moreover done algebraically high numerical precision avoid
degenerate cases. necessary precision grows size matrix.
Settimi Smith (1998, 1999) studied effective dimensions two classes models:
trees binary variables latent class (LC) models two observed variables.
obtained complete characterization two classes. Geiger et al. (1996) computed effective dimensions number models. conjectured rare
effective standard dimensions LC model differ. matter fact,
found one model. Kocka Zhang (2002) found quite number LC models
whose effective standard dimensions differ. proposed easily computable
formula estimating effective dimensions LC models. estimation formula
empirically shown accurate.
paper, prove theorem relates effective dimension HLC model
effective dimensions two HLC models contain fewer latent variables.
Repeated application theorem allows one reduce task computing effective
dimension HLC model subtasks computing effective dimensions LC models.
makes computationally feasible compute effective dimensions large HLC
models.
2

fiEffective Dimensions HLC Models

start Section 2 formal definition effective dimensions Bayesian networks latent variables. Section 3, provide empirical evidence suggest use
BICe instead BIC sometimes improves model selection. Section 4 presents main
theorem Section 5 devoted proof theorem. Section 6, prove theorem effective dimensions general tree models explain main
theorem allows one compute effective dimension arbitrary tree models. Finally,
concluding remarks provided Section 7.

2. Effective Dimensions Bayesian Networks
paper, use capital letters X denote variables lower case
letters x denote states variables. domain cardinality
variable X denoted X |X| respectively. Bold face capital letters
denote sets variables. denotes Cartesian product domains variables
set Y. Elements denoted bold lower case letters
sometimes referred states Y. consider variables finite
number states.
Consider Bayesian network model possibly contains latent variables.
standard dimension ds(M ) number linearly independent parameters
standard parameterization . parameters denote, variable parent
configuration variable, probability variable state (except one)
given parent configuration. Suppose consist k variables x1 , x2 , . . . , xk . Let ri
qi respectively number states xi number possible combinations
states parents. xi parent, let qi 1. ds(M ) given
ds(M ) =

k
X

qi (ri 1).

i=1

~
notational simplicity, denote standard dimension n. Let =(
1 , 2 , . . . , n )
vector n linearly independent model parameters . let set
observed variables. Suppose m+1 possible states. enumerate first states
y1 , y1 , . . . , ym .
~ mapping
(1im), P (yi ) function parameters .
n

n dimensional parameter space (a subspace R ) R , namely : (1 , 2 , . . . , n )
(P (y1 ), P (y2 ), . . . , P (ym )). Jacobian matrix mapping following mn
matrix:
~ = [Jij ] = [ P (yi ) ]
JM ()
j
], understanding
convenience, often write matrix JM = [ P(Y)
j
elements j-th column obtained allowing run possible states
except one.
~ commonly used parameterizations
i, P (yi ) function .
~ Hence make following
Bayesian networks, actually polynomial function .
assumption:
3

fiZhang & Kocka

Assumption 1 Bayesian network parameterized parameters
joint distribution observed variables polynomial functions parameters
M.
obvious consequence assumption elements JM polynomial
~
functions .
~ JM matrix real numbers. Due Assumption 1, rank
given value ,
matrix constant almost everywhere parameter space (Geiger et al.,
1996. see Section 5.1.). specific, rank everywhere except set
measure zero smaller d. constant called regular rank JM .
regular rank JM called effective dimension Bayesian network
model . Hence denote de(M ). understand term effective dimension,
consider subspace Rm spanned joint probability P (Y) observed variables,
equivalently range mapping . term reflects fact that, almost every
~ small enough open ball around ()
~ resembles Euclidean space dimension
value ,
(Geiger et al., 1996).
multiple ways parameterize given Bayesian network model. However,
choice parameterization affect space spanned joint probability P (Y).
Together interpretation previous paragraph, implies definition
effective dimension depend particular parameterization one uses.

3. Selecting among HLC Models
hierarchical latent class (HLC) model Bayesian network (1) network structure rooted tree (2) variables leaf nodes observed
variables not. observed variables sometimes referred manifest variables
variables latent variables. Figure 1 shows structures two HLC
models. latent class (LC) model HLC model one latent variable.
theme paper computation effective dimensions HLC models.
mentioned introduction, interesting effective dimension, used
BIC score, gives us better approximation marginal likelihood. section,
give example illustrate use effective dimension sometimes leads
better model selection. motivate introduce concept regularity
used subsequent sections.
3.1 Example Model Selection
Consider two HLC models shown Figure 1. one experiment, instantiated
parameters M1 random fashion sampled set D1 10,000 data records
observed variables. ran SHC HSHC data set D1 guidance
BIC score. algorithms produced model M2 . following, explain why,
based D1 , one would prefer M2 M1 BIC used model selection M1
would preferred BICe used instead. argue M1 preferred based
D1 hence BICe better scoring metric case.
4

fiEffective Dimensions HLC Models

X1

X2

X2
Y1

Y2

X3
Y3

Y4

Y1

Y5

Y2

Y6

M1

Y3

X3

Y4

Y5

Y6

M2

Figure 1: Two HLC models. shaded variables latent, variables
observed. cardinality X1 2, cardinalities variables
3.

BIC BICe scores model given data set defined follows:
ds(M )
BIC(M |D) = logP (D|M, ~ )
logN,
2
de(M )
BICe(M |D) = logP (D|M, ~ )
logN
2
~ maximum likelihood estimate parameters based N
sample size.
example, notice M2 includes M1 sense M2 represent
probability distributions observed variables M1 can. fact, make
conditional probability distributions observed variables M2 M1
set PM2 (X2 ) PM2 (X3 |X2 )
PM2 (X2 )PM2 (X3 |X2 ) =

X

PM1 (X1 )PM1 (X2 |X1 )PM1 (X3 |X1 ),

X1

probability distribution observed variables two models identical.
M2 includes M1 , logP (D1 |M1 , ~1 ) logP (D1 |M2 , ~2 ). Together
fact D1 sampled M1 , implies logP (D1 |M1 , ~1 ) logP (D1 |M2 , ~2 )
sufficiently large enough sample size. standard dimension M1 45,
M2 44. Hence
BIC(M1 |D1 ) < BIC(M2 |D1 ).
hand, effective dimensions M1 M2 43 44 respectively. Hence
BICe(M1 |D1 ) > BICe(M2 |D1 ).
Model M2 includes M1 . opposite clearly true effective dimension
M1 smaller M2 . So, M2 reality complex model M1 .
model fit data D1 equally well. Hence simpler one, i.e. M1 , preferred
other. agrees choice BICe score, disagrees choice
BIC score. Hence, BICe appropriate BIC case.
5

fiZhang & Kocka

3.2 Regularity
consider another model M1 M1 except cardinality X1
increased 2 3. easy show M2 includes M1 vice versa. So, two
models equivalent terms capabilities representing probability distributions
observed variables. hence said marginally equivalent. However, M1
standard parameters M2 hence would always prefer M2 M1 .
formalize consideration, introduce concept regularity.
latent variable Z HLC model, enumerate neighbors (parent children)
X1 , X2 , . . . , Xk . HLC model regular latent variable Z,
|Z|

Qk

i=1 |Xi |
,
maxki=1 |Xi |

(1)

strict inequality holds Z two neighbors least one
latent node. Models M1 M2 regular, model M1 not.
irregular model always exists regular model marginally equivalent fewer standard parameters (Zhang, 2003b). regular model
obtained follows: latent node two neighbors
cardinality smaller one neighbors, remove latent node
connect two neighbors. latent node two neighbors
violates (1), reduce cardinality quantity right hand side. Repeat
steps changes made.
interesting note collection regular HLC models given set
observed variables finite (Zhang, 2002). provides finite search space task
learning regular HLC models.1 rest paper, consider regular
HLC models.
ending subsection, point nice property effective model dimension
relation model inclusion. HLC model includes another model, effective
dimension less latter. consequence, two marginally equivalent
models effective dimensions hence BICe score.
true standard model dimension BIC score.
3.3 CS CSe Scores
argued empirical grounds BIC score reasonable scoring function
use learning HLC models BICe score sometimes improve model
selection. two scores free problems. One problem derivation
Laplace approximations marginal likelihood valid boundary
parameter space. CS score way alleviates problem. involves BIC score
based completed data BIC score based original data. words,
involves two Laplace approximations marginal likelihood. lets errors two
approximation cancel other.
Chickering Heckerman (1997) empirically found CS score quite accurate
approximation marginal likelihood robust boundary parameter
1. definition regularity given paper slightly different one given Zhang (2002).
Nonetheless, two conclusions mentioned paragraph remain true.

6

fiEffective Dimensions HLC Models

X

Z

X









Z

Z

X


M2

M1

Figure 2: Problem reduction.
space. realized need effective model dimension CS score, although
actually use it. would made differences experiments
because, models used, standard effective dimensions agree.
use CSe refer scoring function one obtains replacing standard model
dimension CS score effective model dimensions. BICe better
BIC approximations marginal likelihood (Geiger et al., 1996), CSe better
CS. compute CSe, need calculate effective dimensions.

4. Effective Dimensions HLC Models
seen, effective model dimension interesting number reasons.
main result paper theorem effective dimension de(M ) regular
HLC model contains one latent variable. Let X root ,
latent node. least two latent nodes, must exist another latent
node Z child X. following, use terms X-branch Z-branch
respectively refer sets nodes separated Z X X Z.
Let set observed variables Z-branch let set
observed variables. Note X-branch doesnt contain node X. relationship
among X, Z, Y, depicted left-most picture Figure 2.
standard parameterization includes parameters P (X) parameters
P (Z|X). convenience, replace parameters parameters P (X, Z).
mentioned end Section 2, reparameterization affect effective
dimension de(M ). reflect reparameterization, edge X Z
directed Figure 2.
(0)

(0)

(0)

Suppose P (X, Z) k0 parameters 1 , 2 , . . . , k0 . Suppose conditional distri(1)

(1)

(1)

butions variables X-branch consists k1 parameters 1 , 2 , . . . , k1
(2)

(2)

conditional distributions variables Z-branch consists k2 parameters 1 , 2 ,
(2)
. . . , k2 . convenience sometimes refer three groups parameters using
three vectors ~(0) , ~(1) ~(2) respectively.
following, define two HLC models M1 M2 starting
establish relationship effective dimensions effective dimension
. context, , M1 , M2 regarded purely Mathematical objects.
semantics variables concern. particular, variable H latent
7

fiZhang & Kocka

X1
X1 (6)
X2
X2 (3)

Y3 (3)

X4 (5)

X3 (3)

Y3

X3

X2
X4

X5 (5)

X3
X1

X1

X4
Y1 (3)

Y2 (2)

Y4 (2)

X5
X5

Y5 (6)
Y1

Y2

X2

X3

Y4

Y5

Figure 3: picture left shows HLC model five observed five latent
variables, variable annotated name cardinality. picture
right shows components decompose HLC model
applying Theorem 1. Latent variables shaded, observed variables
not.

might designated observed M1 M2 part definition
Mathematical objects.
obtain Bayesian network model B1 deleting Z-branch. Strictly
speaking B1 Bayesian network due parameterization inherits : instead
probability tables P (X) P (Z|X), table P (X, Z). P (X) P (Z|X)
readily obtained P (X, Z). mind, view B1 Bayesian network.
network obviously tree-structured. leaf variables include set
variable Z. define M1 HLC model share structure B1
variable Z variables observed. parameters M1
~(0) ~(1) .
Similarly let B2 Bayesian network model obtained deleting Xbranch. tree-structure leaf variables include variable X.
define M2 HLC model share structure B2
variable X variables observed. parameters M2 ~(0) ~(2) .
Theorem 1 Suppose regular HLC model contains two latent nodes.
two HLC models M1 M2 defined text regular. Moreover,
de(M ) = de(M1 )+de(M2 )[ds(M1 )+ds(M2 )ds(M )].

(2)

words, effective dimension equals sum effective dimensions M1
M2 minus number common parameters M1 M2 share.
appreciate significance theorem, consider task computing effective dimension regular HLC model contains two latent nodes.
8

fiEffective Dimensions HLC Models

repeatedly applying theorem, reduce task subtasks calculating effective dimensions LC models. example, consider HLC model depicted
picture left Figure 3. Theorem 1 allows us to, purpose computing
effective dimension, decompose HLC model five LC models, shown
right Figure 3.
might one compute effective dimension LC model? One way use
algorithm suggested Geiger et al. (1996). algorithm first symbolically computes
Jacobian matrix, possible due Assumption 1. randomly assigns
values parameters, resulting numerical matrix. rank numerical matrix
computed diagonalization. rank Jacobian matrix equals effective
dimension LC model almost everywhere, get regular rank probability
one. algorithm recently implemented Rusakov Geiger (2003). Kocka
Zhang (2002) suggest alternative algorithm computes upper bound.
algorithm fast empirically shown produce extremely tight bounds.
Going back example, effective dimension LC models X1 , X2 , X3 , X4
X5 26, 23, 23, 34 17 respectively. Thus effective dimension HLC model
Figure 3 26+23+34+23+17(531)(361)(631)(351) = 61. contrast,
standard dimension model 5+62+62+62+34+55+5+34+52+5 = 110.

5. Proof Main Result
section devoted proof Theorem 1. begin properties
Jacobian matrices Bayesian network models.
5.1 Properties Jacobian Matrices
Consider Jacobian matrix JM Bayesian network model . matrix parameterized parameters ~ . Let v1 , v2 , . . . , vm column vectors JM .
Lemma 1 number column vectors v1 , v2 , . . . , vm Jacobian matrix JM
either linearly dependent everywhere linearly independent almost everywhere.
linearly dependent everywhere exists least one column vector vj
expressed linear combination column vectors everywhere.
Proof: Consider diagonalizing following transposed matrix:
[v1 , v2 , . . . , vm ]T .
~ Hence would
According Assumption 1, elements matrix polynomials (of ).
multiply rows polynomials fraction polynomials. course, need add
one row another row. end process, get diagonal matrix whose nonzero
elements polynomials fractions polynomials. Suppose k nonzero rows
suppose correspond v1 , v2 , . . . , vk .
elements diagonalized matrix polynomials fractions polynomials,
~
well-defined 2 nonzero almost everywhere (i.e. almost values ).
k=m, vectors linearly independent almost everywhere.
2. fraction well defined denominator zero.

9

fiZhang & Kocka

k<m, exist, j (k<jm), polynomials fractions polynomials ci
(1ik)
vj =

k
X

ci vi .

(3)

i=1

coefficients ci determined tracing diagonalization process. vj
expressed linear combination {vi |i = 1, . . . , k} everywhere 3 . 2
Although might sound trivial, lemma actually quite interesting.
JM parameterized matrix. first part, example, implies exist
two subspaces parameter space nonzero measures
vectors linearly independent one subspace linearly dependent other.
total number column vectors JM , get following lemma:
Lemma 2 Jacobian matrix JM , exists collection column vectors form
basis column space almost everywhere. number vectors collection
equals regular rank matrix. Moreover, collection chosen include
given set column vectors linearly independent almost everywhere.
Proof: first part already proved. second part follows definition
regular rank. last part true could start diagonalization process
transpose vectors set top matrix. 2
5.2 Proof Theorem 1
set prove Theorem 1. straightforward verify HLC models
M1 M2 regular. suffices prove equation (2). rest
section.
set observed variables Y, set observed variables M1
{Z} set observed variables M2 {X}. Hence Jacobian matrices
models , M1 , M2 respectively written follows:
JM

= [

JM1

= [

JM2

= [

P (O, Y)

,...,

P (O, Y) P (O, Y)
P (O, Y) P (O, Y)
P (O, Y)
;
;
]
,...,
,...,
(0)
(1)
(1)
(2)
(2)
k0
1
k1
1
k2

,...,

P (O, Z) P (O, Z)
P (O, Z)
;
]
,...,
(0)
(1)
(1)
k0
1
k1

(0)
1

P (O, Z)
(0)
1

P (X, Y)
(0)
1

,...,

P (X, Y) P (X, Y)
P (X, Y)
;
]
,...,
(0)
(2)
(2)
k0
1
k2

~ ci might undefined
3. subtle point here. fractions polynomials ,
~ equation (3) alone, cannot conclude vj linearly depends {vi |i = 1, . . . , k}
values .
everywhere.
conclusion nonetheless true two reasons. First set ~ values ci
undefined measure zero. Second, vj linearly depend {vi |i = 1, . . . , k} one value
~ would true sufficiently small nonetheless measure-positive ball around
,
value.

10

fiEffective Dimensions HLC Models

clear one-to-one correspondence first k0 +k1 column vectors
JM column vectors JM1 one-to-one correspondence
first k0 last k2 column vectors JM column vectors JM2 .
first show
Claim 1: first k0 vectors JM (JM1 JM2 ) linearly independent
almost everywhere.
Together Lemma 2, Claim 1 implies collection column vectors
JM1 includes first k0 vectors basis column space JM1 almost
everywhere. particular, implies de(M1 )k0 . Suppose de(M1 )=k0 +r. Without
loss generality, suppose basis vectors
P (O, Z)
(0)
1

,...,

P (O, Z) P (O, Z)
P (O, Z)
;
.
,...,
(0)
(1)
(1)
k0
1
r

(4)

symmetry, assume de(M2 )=k0 +s s0 following column
vectors form basis JM2 almost everywhere:
P (X, Y)
(0)
1

,...,

P (X, Y) P (X, Y)
P (X, Y)
.
;
,...,
(0)
(2)
(2)
k0
1


(5)

consider following list vectors JM :
P (O, Y)
(0)
1

,...,

P (O, Y) P (O, Y)
P (O, Y) P (O, Y)
P (O, Y)
;
;
.
,...,
,...,
(0)
(1)
(1)
(2)
(2)
k0
1
r
1


(6)

show
Claim 2: column vectors JM linearly depend vectors listed (6)
everywhere.
Claim 3: vectors listed (6) linearly independent almost everywhere.
two claims imply vectors listed (6) form basis column space
JM almost everywhere. Therefore
de(M ) = k0 +r+s = de(M1 )+de(M2 )k0 .
clear k0 =ds(M1 )+ds(M2 )ds(M ). Therefore Theorem 1 proved. 2
5.3 Proof Claim 1
Lemma 3 Let Z latent node HLC model set observed
nodes subtree rooted Z. regular, set conditional distributions
nodes subtree way encode injective mapping Z
sense P (Y=(z)|Z=z) = 1 z Z .
11

fiZhang & Kocka

Proof: prove lemma induction number latent nodes subtree
rooted Z. First consider case one latent node, namely Z.
case, Z parent nodes Y. Enumerate nodes Y1 , Y2 , . . . , Yk .
Q
regular, |Z| ki=1 |Yi |. Hence define injective mapping
Q
Z = ki=1 Yi . state z Z, (z) written = (y1 , y2 , . . . , yk ),
yi state Yi . set
P (Yi =yi |Z=z) = 1,
P (Y=(z)|Z=z) = 1.
consider case least two hidden nodes subtree rooted
Z. Let W one latent node latent node descendants. Let (1)
set observed nodes subtree rooted W Y(2) =Y\Y(1) . induction
hypothesis, parameterize subtree rooted W way encodes
injective mapping W Y(1) . Moreover, nodes W removed ,
remains regular HLC model. model, parameterize subtree rooted
Z way encodes injective mapping Z (W,Y(2) ) = W Y(2) .
Together, two facts prove lemma. 2
Corollary 1 Let Z latent node HLC model . Suppose Z latent neighbor
X. Let set observed nodes separated X Z. regular,
set probability distributions nodes separated X Z way
encode injective mapping Z sense P (Y=(z)|Z=z) = 1
z Z .
Proof: corollary follows readily Lemma 3 property root-walking
operation (Zhang, 2002). 2
Proof Claim 1: Consider following matrix
[
(0)

(0)

P (X, Z)
(0)
1

...,

P (X, Z)
(0)

k0

]

(7)

(0)

1 , 2 , . . . , k0 parameters joint distribution P (X, Z), matrix
identity matrix rows properly arranged. column vectors linearly
independent almost everywhere.
(0)
(0)
consider first k0 column vectors JM : P (O, Y)/1 , . . . , P (O, Y)/k0 .
must linearly independent almost everywhere. not, one vectors, say
(0)
P (O, Y)/k0 , would linearly depend rest everywhere according Lemma 1.
Observe (1ik0 ),
P (O, Y)
(0)


=

X

P (O|X)P (Y|Z)

X,Z

P (X, Z)
(0)



.
(0)

Choose P (O|X) P (Y|Z) Corollary 1. vector P (O, Y)/i might contain zero elements. remove zero elements, remains vector iden(0)
(0)
tical P (X, Z)/i . conclude P (X, Z)/k0 linearly depends
12

fiEffective Dimensions HLC Models

(0)

(0)

P (X, Z)/1 . . . , P (X, Z)/k0 1 everywhere, contradicts conclusion
previous paragraph. Hence first k0 vectors JM must linearly independent almost
everywhere.
evident that, using similar arguments, show first k0 vectors
JM1 (JM2 ) linearly independent almost everywhere. Claim 1 therefore proved. 2
5.4 Proof Claim 2
Every column vector JM1 linearly depends vectors listed (4) everywhere. Observe

P (O, Y)
(0)


X

P (Y|Z)

Z

P (O, Y)
(1)


=
=

X

P (Y|Z)

Z

P (O, Z)
(0)

, = 1, . . . , k0

(1)

, = 1, . . . , k1 .


P (O, Z)


Therefore every column vector JM corresponds vectors JM1 linearly depends
first k0 +r vectors listed (6) everywhere.
symmetry, every column vector JM corresponds vectors JM2 linearly
depends first k0 last vectors listed (6) everywhere. claim proved.
2
5.5 Proof Claim 3
prove claim contradiction. Assume vectors listed (6) linearly
independent almost everywhere. According Lemma 1, one them, say v, must linearly
depend rest everywhere. Claim 1 Lemma 2, assume v
(2)
among last r+s vectors. Without loss generality, assume v P (O, Y)/s .
~ exist real numbers ci (1ik0 ), c(1) (1ir), c(2)
value ,


(1is1)
P (O, Y)
(2)


=

k0
X
i=1

ci

P (O, Y)
(0)


+

r
X

(1) P (O, Y)
(1)

i=1

ci

+

s1
X

(2) P (O, Y)
.
(2)

i=1

ci

Note last term right hand side, runs 1 s1.
parameter vector ~ consists three subvectors ~(0) , ~(1) ~(2) . Set parameters
(1)
~ (for X-branch) Lemma 3. exists injective mapping X

P (O=(x)|X=x) = 1 x X .

(8)

vectors (6), consider subvector consisting elements
states images states X mapping . subvectors
(0)
(1)
(2)
denoted P (OX , Y)/i , P (OX , Y)/i , P (OX , Y)/i .
values ~(0) ~(2) , still
13

fiZhang & Kocka

P (OX , Y)

=

(2)


k0
X

ci

P (OX , Y)

i=1

(0)


+

r
X

(1) P (OX , Y)
(1)

i=1

ci

+

s1
X

(2) P (OX , Y)
.
(2)

i=1

ci

(9)

Consider first two terms right hand side:
k0
X

ci

r
X

(1) P (OX , Y)
(1)

i=1
k0
r
X
X
P (OX , Z)
P (OX , Z) X
(1) X
c
ci
+
P (Y|Z)
P (Y|Z)

(0)
(1)


i=1
i=1
Z
Z
k0
r
X
X
P (OX , Z) X
(1) P (OX , Z)
ci
+
}
P (Y|Z){ ci
(0)
(1)


i=1
i=1
Z

P (OX , Y)
(0)


i=1

=
=

+

ci

P

(8) fact P (O, Z) =
X P (X, Z)P (O|X), column vector
(0)
(0)
P (OX , Z)/i identical vector P (X, Z)/i . argued proving
(0)
Claim 1, vectors {P (X, Z)/i |i=1, . . . , k0 } constitute basis k0 -dimensional
(1)
Euclidian space. implies that, vectors P (OX , Z)/i represented
(0)
linear combination vectors {P (OX , Z)/i |i = 1, . . . , k0 }. Consequently,
exist ci (1ik0 )
k0
X

ci

P (OX , Z)

i=1

(0)


k0
X

P (OX , Y)

+

r
X

(1) P (OX , Z)
(1)

i=1

ci

=

k0
X

ci

P (OX , Z)

ci

P (OX , Y)

i=1

(0)



Hence

i=1

ci

+

(0)


r
X

(1) P (OX , Y)
(1)

i=1

ci

=

k0
X
i=1

(0)



Combining equation equation (9), get
P (OX , Y)
(2)


=

k0
X

ci

P (OX , Y)

i=1

(0)


+

s1
X

(2) P (OX , Y)
.
(2)

i=1


ci

P

(8) fact fact P (O, Y) = X P (X, Y)P (O|X), column
(1)
(1)
vector P (OX , Y)/i identical vector P (X, Y)/i column vector
(2)
(2)
P (OX , Y)/i identical vector P (X, Y)/i . Hence
P (X, Y)
(2)


=

k0
X
i=1

ci

P (X, Y)
(0)


+

s1
X

(2) P (X, Y)
.
(2)

i=1

ci

contradicts fact vectors equation form basis column space
JM2 almost everywhere (see (5) Section 5.2) Therefore, Claim 3 must true. 2
14

fiEffective Dimensions HLC Models

6. Effective Dimensions Trees
Let us use term tree model refer Markov random fields undirected trees
finite number random variables. root tree model nodes, get
tree-structured Bayesian network model. tree model, define leaf nodes
one neighbor. HLC model tree model leaf nodes observed
others latent.
turns Theorem 1 enables us compute effective dimension tree
model. Consider arbitrary tree model. leaf nodes latent, remove
nodes without affecting effective dimension.
removing latent leaf nodes, leaf nodes observed. non-leaf nodes
observed, decompose model submodels observed non-leaf
node. following theorem tells us model submodels related terms
effective dimensions.
Theorem 2 Suppose observed non-leaf node tree model . decomposes
k submodels M1 , . . . , Mk ,
de(M ) =

k
X

de(Mi ) (k 1)(|Y | 1).

i=1

possible decompositions, final submodels either contain latent nodes
HLC models. Effective dimensions submodels latent variables simply
standard dimensions. HLC submodel irregular, make regular applying
transformation mentioned end Section 3.2. transformation affect
effective dimensions submodels. Finally, effective dimensions regular HLC
submodels computed using Theorem 1.
Proof Theorem 2: possible prove theorem starting Jacobian
matrix. take less formal revealing approach.
suffices consider case k 2. two submodels M1 M2 share one
node, namely . Let O1 O2 respectively sets observed nodes two
submodels excluding . Root .
P (Y, O1 , O2 )P (Y ) = P (O1 , )P (O2 , ).
Let ~0 set parameters distribution P (Y ), ~1 ~2 respectively sets
parameters conditional probability distributions nodes M1 M2 . Consider
fixing ~0 letting ~1 ~2 vary. case, space spanned P (Y ) consists
one vector, namely ~0 itself. Moreover, one-to-one correspondence vectors
space spanned P (Y, O1 , O2 ) vectors Cartesian product spaces
spanned P (O1 , ) P (O2 , ). let ~0 vary. adds |Y |1 dimensions
four spaces spanned P (Y, O1 , O2 ), P (Y ), P (O1 , ), P (O2 , ). Consequently,

de(M ) = de(M1 ) + de(M2 ) (|Y | 1).
theorem proved. 2
15

fiZhang & Kocka

7. Concluding Remarks
paper study effective dimensions HLC models. work motivated
empirical evidence BIC behaves quite well used several hill-climbing
algorithms learning HLC models BICe score sometimes leads better
model selection BIC score. proved theorem relates effective
dimension HLC model effective dimensions two HLC models
contain fewer latent variables. Repeated application theorem allows one reduce
task computing effective dimension HLC model subtasks computing
effective dimensions LC models. makes computationally feasible compute
effective dimensions large HLC models. addition, proved theorem
effective dimensions general tree models. main theorem allows one
compute effective dimension arbitrary tree models.
Acknowledgements
work initiated authors visiting Department Computer Science,
Aalborg University, Denmark. thank Poul S. Eriksen, Finn V. Jensen, Jiri Vomlel,
Marta Vomlelova, Thomas D. Nielsen, Olav Bangso, Jose Pena, Kristian G. Olesen.
grateful annonymous reviewers whose comments helped us greatly
improving paper. Research paper partially supported GA CR grant
201/02/1269 Hong Kong Research Grant Council grant HKUST6088/01E.

References
Akaike, H. (1974). new look statistical model identification. IEEE Trans. Autom.
Contr., 19, 716-723.
Bartholomew, D. J. Knott, M. (1999). Latent variable models factor analysis, 2nd
edition. Kendalls Library Statistics 7. London: Arnold.
Cheeseman, P. Stutz, J. (1995). Bayesian classification (AutoClass): Theory
results. Fayyad, U., Piatesky-Shaoiro, G., Smyth, P., Uthurusamy, R. (eds.),
Advancesin Knowledge Discovery Data Mining, AAAI Press, Menlo Park, CA.
Chickering D. M. Heckerman D. (1997). Efficient Approximations Marginal
Likelihood Bayesian Networks Hidden variables. Machine Learning, 29, 181212.
Cowell, R. G., Dawid, A. P., Lauritzen, S. L., Spiegelhalter, D. J. (1999). Probabilistic
networks expert systems, Springer.
Kocka, T. Zhang, N. L. (2002). Dimension correction hierarchical latent class
models. Proc. 18th Conference Uncertainty Artificial Intelligence
(UAI-02).
Geiger D., Heckerman D. Meek C. (1996). Asymptotic model selection directed
networks hidden variables. Proc. 12th Conference Uncertainty
Artificial Intelligence, 283-290.
16

fiEffective Dimensions HLC Models

Goodman, L. A. (1974). Exploratory latent structure analysis using identifiable
unidentifiable models. Biometrika, 61, 215-231.
Lazarsfeld, P. F., Henry, N.W. (1968).
Mifflin.
Rusakov, D. Geiger, D. (2002).
networks. UAI-02.

Latent structure analysis. Boston: Houghton

Asymptotic model selection Naive Bayesian

Rusakov, D. Geiger, D. (2003). Automated analytic asymptotic evaluation marginal
likelihood latent models. UAI-03.
Schwarz G. (1978). Estimating dimension model. Annals Statistics, 6, 461-464.
Settimi, R. Smith, J.Q. (1998). geometry Bayesian graphical models
hidden variables. Proceedings Fourteenth Conference Uncertainty
Artificial Intelligence, Morgan Kaufmann Publishers, S. Francisco, CA, 472-479.
Settimi, R. Smith, J.Q. (1999). Geometry, moments Bayesian networks hidden
variables. Proceedings Seventh International Workshop Artificial Intelligence Statistics, Fort Lauderdale, Florida (3-6 January 1999), Morgan Kaufmann
Publishers, S. Francisco, CA.
Zhang N. L. (2002). Hierarchical latent class models cluster analysis. AAAI-02, 230-237.
Zhang, N. L., Kocka, T., Karciauskas, G., Jensen, F. V. (2003). Learning hierarchical
latent class models. Technical Report HKUST-CS03-01, Department Computer
Science, Hong Kong University Science Technology.
Zhang, N. L. (2003).
Structural EM Hierarchical Latent Class Models. Technical
Report HKUST-CS03-06, Department Computer Science, Hong Kong University
Science Technology.
Zhang, N. L. (2003b). Hierarchical latent class models cluster analysis. Journal
Machine Learning Research, appear.

17


