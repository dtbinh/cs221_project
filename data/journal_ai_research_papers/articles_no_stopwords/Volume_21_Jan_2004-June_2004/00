journal artificial intelligence

submitted published

effective dimensions hierarchical latent class
nevin l zhang

lzhang cs ust hk

department computer science
hong kong university science technology china

tomas kocka

kocka lisp vse cz

laboratory intelligent systems prague
prague university economics czech republic

abstract
hierarchical latent class hlc tree structured bayesian networks
leaf nodes observed internal nodes latent theoretically well
justified model selection criteria hlc particular bayesian networks
latent nodes general nonetheless empirical studies suggest bic score
reasonable criterion use practice learning hlc empirical studies
suggest sometimes model selection improved standard model dimension
replaced effective model dimension penalty term bic score
effective dimensions difficult compute prove theorem
relates effective dimension hlc model effective dimensions number
latent class theorem makes computationally feasible compute
effective dimensions large hlc theorem used compute
effective dimensions general tree

introduction
hierarchical latent class hlc zhang tree structured bayesian networks
bns leaf nodes observed internal nodes latent generalize latent
class lazarsfeld henry first identified potentially useful
class bayesian networks pearl concerned learning hlc
data fundamental question select among competing
bic score schwarz popular metric researchers use select among
bayesian network consists loglikelihood term measures fitness
data penalty term depends linearly upon standard model dimension e
number linearly independent standard model parameters variables
observed bic score asymptotic approximation logarithm marginal
likelihood schwarz consistent sense given sufficient data
bic score generative model model data sampled larger
equivalent generative model
latent variables present bic score longer asymptotic approximation marginal likelihood geiger et al remedied
extent concept effective model dimension fact replace standard model
dimension effective model dimension bic score resulting scoring function
called bice score asymptotic approximation marginal likelihood almost
everywhere except singular points rusakov geiger
c

ai access foundation rights reserved

fizhang kocka

neither bic bice proved consistent latent variable
matter fact even defined means model selection criterion
consistent latent variable empirical studies suggest bic score
well behaved practice task learning hlc three related searchbased learning hlc namely double hill climbing dhc zhang
single hill climbing shc zhang et al heuristic shc hshc zhang
absence theoretically well justified model selection criterion zhang
tested dhc four existing scoring functions namely aic score akaike
bic score cheeseman stutz cs score cheeseman stutz holdout
logarithmic score hls cowell et al real world synthetic data used
real world data bic cs enabled dhc regarded
best domain experts synthetic data bic cs enabled dhc
identical resemble closely true generative
coupled aic hls hand dhc performed significantly worse shc
hshc tested synthetic data sampled fairly large hlc much
nodes bic used tests cases bic enabled shc
hshc identical resemble closely true generative
empirical indicate perform well
suggest bic reasonable scoring function use learning hlc
experiments reveal model selection sometimes improved bice
score used instead bic score explain detail section
order use bice score practice need way compute effective dimensions trivial task effective dimension hlc model rank
jacobian matrix mapping parameters model parameters
joint distribution observed variables number rows jacobian
matrix increases exponentially number observed variables construction
jacobian matrix calculation rank computationally demanding
moreover done algebraically high numerical precision avoid
degenerate cases necessary precision grows size matrix
settimi smith studied effective dimensions two classes
trees binary variables latent class lc two observed variables
obtained complete characterization two classes geiger et al computed effective dimensions number conjectured rare
effective standard dimensions lc model differ matter fact
found one model kocka zhang found quite number lc
whose effective standard dimensions differ proposed easily computable
formula estimating effective dimensions lc estimation formula
empirically shown accurate
prove theorem relates effective dimension hlc model
effective dimensions two hlc contain fewer latent variables
repeated application theorem allows one reduce task computing effective
dimension hlc model subtasks computing effective dimensions lc
makes computationally feasible compute effective dimensions large hlc



fieffective dimensions hlc

start section formal definition effective dimensions bayesian networks latent variables section provide empirical evidence suggest use
bice instead bic sometimes improves model selection section presents main
theorem section devoted proof theorem section prove theorem effective dimensions general tree explain main
theorem allows one compute effective dimension arbitrary tree finally
concluding remarks provided section

effective dimensions bayesian networks
use capital letters x denote variables lower case
letters x denote states variables domain cardinality
variable x denoted x x respectively bold face capital letters
denote sets variables denotes cartesian product domains variables
set elements denoted bold lower case letters
sometimes referred states consider variables finite
number states
consider bayesian network model possibly contains latent variables
standard dimension ds number linearly independent parameters
standard parameterization parameters denote variable parent
configuration variable probability variable state except one
given parent configuration suppose consist k variables x x xk let ri
qi respectively number states xi number possible combinations
states parents xi parent let qi ds given
ds

k
x

qi ri




notational simplicity denote standard dimension n let
n
vector n linearly independent model parameters let set
observed variables suppose possible states enumerate first states
ym
mapping
im p yi function parameters
n

n dimensional parameter space subspace r r namely n
p p p ym jacobian matrix mapping following mn
matrix
jij p yi
jm
j
understanding
convenience often write matrix jm p
j
elements j th column obtained allowing run possible states
except one
commonly used parameterizations
p yi function
hence make following
bayesian networks actually polynomial function
assumption


fizhang kocka

assumption bayesian network parameterized parameters
joint distribution observed variables polynomial functions parameters

obvious consequence assumption elements jm polynomial

functions
jm matrix real numbers due assumption rank
given value
matrix constant almost everywhere parameter space geiger et al
see section specific rank everywhere except set
measure zero smaller constant called regular rank jm
regular rank jm called effective dimension bayesian network
model hence denote de understand term effective dimension
consider subspace rm spanned joint probability p observed variables
equivalently range mapping term reflects fact almost every
small enough open ball around
resembles euclidean space dimension
value
geiger et al
multiple ways parameterize given bayesian network model however
choice parameterization affect space spanned joint probability p
together interpretation previous paragraph implies definition
effective dimension depend particular parameterization one uses

selecting among hlc
hierarchical latent class hlc model bayesian network network structure rooted tree variables leaf nodes observed
variables observed variables sometimes referred manifest variables
variables latent variables figure shows structures two hlc
latent class lc model hlc model one latent variable
theme computation effective dimensions hlc
mentioned introduction interesting effective dimension used
bic score gives us better approximation marginal likelihood section
give example illustrate use effective dimension sometimes leads
better model selection motivate introduce concept regularity
used subsequent sections
example model selection
consider two hlc shown figure one experiment instantiated
parameters random fashion sampled set data records
observed variables ran shc hshc data set guidance
bic score produced model following explain
one would prefer bic used model selection
would preferred bice used instead argue preferred
hence bice better scoring metric case


fieffective dimensions hlc

x

x

x




x
















x









figure two hlc shaded variables latent variables
observed cardinality x cardinalities variables


bic bice scores model given data set defined follows
ds
bic logp
logn

de
bice logp
logn

maximum likelihood estimate parameters n
sample size
example notice includes sense represent
probability distributions observed variables fact make
conditional probability distributions observed variables
set pm x pm x x
pm x pm x x

x

pm x pm x x pm x x

x

probability distribution observed variables two identical
includes logp logp together
fact sampled implies logp logp
sufficiently large enough sample size standard dimension
hence
bic bic
hand effective dimensions respectively hence
bice bice
model includes opposite clearly true effective dimension
smaller reality complex model
model fit data equally well hence simpler one e preferred
agrees choice bice score disagrees choice
bic score hence bice appropriate bic case


fizhang kocka

regularity
consider another model except cardinality x
increased easy includes vice versa two
equivalent terms capabilities representing probability distributions
observed variables hence said marginally equivalent however
standard parameters hence would prefer
formalize consideration introduce concept regularity
latent variable z hlc model enumerate neighbors parent children
x x xk hlc model regular latent variable z
z

qk

xi

maxki xi



strict inequality holds z two neighbors least one
latent node regular model
irregular model exists regular model marginally equivalent fewer standard parameters zhang b regular model
obtained follows latent node two neighbors
cardinality smaller one neighbors remove latent node
connect two neighbors latent node two neighbors
violates reduce cardinality quantity right hand side repeat
steps changes made
interesting note collection regular hlc given set
observed variables finite zhang provides finite search space task
learning regular hlc rest consider regular
hlc
ending subsection point nice property effective model dimension
relation model inclusion hlc model includes another model effective
dimension less latter consequence two marginally equivalent
effective dimensions hence bice score
true standard model dimension bic score
cs cse scores
argued empirical grounds bic score reasonable scoring function
use learning hlc bice score sometimes improve model
selection two scores free one derivation
laplace approximations marginal likelihood valid boundary
parameter space cs score way alleviates involves bic score
completed data bic score original data words
involves two laplace approximations marginal likelihood lets errors two
approximation cancel
chickering heckerman empirically found cs score quite accurate
approximation marginal likelihood robust boundary parameter
definition regularity given slightly different one given zhang
nonetheless two conclusions mentioned paragraph remain true



fieffective dimensions hlc

x

z

x









z

z

x






figure reduction
space realized need effective model dimension cs score although
actually use would made differences experiments
used standard effective dimensions agree
use cse refer scoring function one obtains replacing standard model
dimension cs score effective model dimensions bice better
bic approximations marginal likelihood geiger et al cse better
cs compute cse need calculate effective dimensions

effective dimensions hlc
seen effective model dimension interesting number reasons
main theorem effective dimension de regular
hlc model contains one latent variable let x root
latent node least two latent nodes must exist another latent
node z child x following use terms x branch z branch
respectively refer sets nodes separated z x x z
let set observed variables z branch let set
observed variables note x branch doesnt contain node x relationship
among x z depicted left picture figure
standard parameterization includes parameters p x parameters
p z x convenience replace parameters parameters p x z
mentioned end section reparameterization affect effective
dimension de reflect reparameterization edge x z
directed figure






suppose p x z k parameters k suppose conditional distri





butions variables x branch consists k parameters k




conditional distributions variables z branch consists k parameters

k convenience sometimes refer three groups parameters
three vectors respectively
following define two hlc starting
establish relationship effective dimensions effective dimension
context regarded purely mathematical objects
semantics variables concern particular variable h latent


fizhang kocka

x
x
x
x



x

x



x

x
x

x

x
x

x

x






x
x






x

x





figure picture left shows hlc model five observed five latent
variables variable annotated name cardinality picture
right shows components decompose hlc model
applying theorem latent variables shaded observed variables


might designated observed part definition
mathematical objects
obtain bayesian network model b deleting z branch strictly
speaking b bayesian network due parameterization inherits instead
probability tables p x p z x table p x z p x p z x
readily obtained p x z mind view b bayesian network
network obviously tree structured leaf variables include set
variable z define hlc model share structure b
variable z variables observed parameters

similarly let b bayesian network model obtained deleting xbranch tree structure leaf variables include variable x
define hlc model share structure b
variable x variables observed parameters
theorem suppose regular hlc model contains two latent nodes
two hlc defined text regular moreover
de de de ds ds ds



words effective dimension equals sum effective dimensions
minus number common parameters share
appreciate significance theorem consider task computing effective dimension regular hlc model contains two latent nodes


fieffective dimensions hlc

repeatedly applying theorem reduce task subtasks calculating effective dimensions lc example consider hlc model depicted
picture left figure theorem allows us purpose computing
effective dimension decompose hlc model five lc shown
right figure
might one compute effective dimension lc model one way use
suggested geiger et al first symbolically computes
jacobian matrix possible due assumption randomly assigns
values parameters resulting numerical matrix rank numerical matrix
computed diagonalization rank jacobian matrix equals effective
dimension lc model almost everywhere get regular rank probability
one recently implemented rusakov geiger kocka
zhang suggest alternative computes upper bound
fast empirically shown produce extremely tight bounds
going back example effective dimension lc x x x x
x respectively thus effective dimension hlc model
figure contrast
standard dimension model

proof main
section devoted proof theorem begin properties
jacobian matrices bayesian network
properties jacobian matrices
consider jacobian matrix jm bayesian network model matrix parameterized parameters let v v vm column vectors jm
lemma number column vectors v v vm jacobian matrix jm
linearly dependent everywhere linearly independent almost everywhere
linearly dependent everywhere exists least one column vector vj
expressed linear combination column vectors everywhere
proof consider diagonalizing following transposed matrix
v v vm
hence would
according assumption elements matrix polynomials
multiply rows polynomials fraction polynomials course need add
one row another row end process get diagonal matrix whose nonzero
elements polynomials fractions polynomials suppose k nonzero rows
suppose correspond v v vk
elements diagonalized matrix polynomials fractions polynomials

well defined nonzero almost everywhere e almost values
k vectors linearly independent almost everywhere
fraction well defined denominator zero



fizhang kocka

k exist j k jm polynomials fractions polynomials ci
ik
vj

k
x

ci vi





coefficients ci determined tracing diagonalization process vj
expressed linear combination vi k everywhere
although might sound trivial lemma actually quite interesting
jm parameterized matrix first part example implies exist
two subspaces parameter space nonzero measures
vectors linearly independent one subspace linearly dependent
total number column vectors jm get following lemma
lemma jacobian matrix jm exists collection column vectors form
basis column space almost everywhere number vectors collection
equals regular rank matrix moreover collection chosen include
given set column vectors linearly independent almost everywhere
proof first part already proved second part follows definition
regular rank last part true could start diagonalization process
transpose vectors set top matrix
proof theorem
set prove theorem straightforward verify hlc
regular suffices prove equation rest
section
set observed variables set observed variables
z set observed variables x hence jacobian matrices
respectively written follows
jm



jm



jm



p



p p
p p
p










k

k

k



p z p z
p z






k

k




p z



p x





p x p x
p x






k

k

ci might undefined
subtle point fractions polynomials
equation alone cannot conclude vj linearly depends vi k
values
everywhere
conclusion nonetheless true two reasons first set values ci
undefined measure zero second vj linearly depend vi k one value
would true sufficiently small nonetheless measure positive ball around

value



fieffective dimensions hlc

clear one one correspondence first k k column vectors
jm column vectors jm one one correspondence
first k last k column vectors jm column vectors jm
first
claim first k vectors jm jm jm linearly independent
almost everywhere
together lemma claim implies collection column vectors
jm includes first k vectors basis column space jm almost
everywhere particular implies de k suppose de k r without
loss generality suppose basis vectors
p z





p z p z
p z






k

r



symmetry assume de k following column
vectors form basis jm almost everywhere
p x





p x p x
p x






k





consider following list vectors jm
p





p p
p p
p










k

r






claim column vectors jm linearly depend vectors listed
everywhere
claim vectors listed linearly independent almost everywhere
two claims imply vectors listed form basis column space
jm almost everywhere therefore
de k r de de k
clear k ds ds ds therefore theorem proved
proof claim
lemma let z latent node hlc model set observed
nodes subtree rooted z regular set conditional distributions
nodes subtree way encode injective mapping z
sense p z z z z z


fizhang kocka

proof prove lemma induction number latent nodes subtree
rooted z first consider case one latent node namely z
case z parent nodes enumerate nodes yk
q
regular z ki yi hence define injective mapping
q
z ki yi state z z z written yk
yi state yi set
p yi yi z z
p z z z
consider case least two hidden nodes subtree rooted
z let w one latent node latent node descendants let
set observed nodes subtree rooted w induction
hypothesis parameterize subtree rooted w way encodes
injective mapping w moreover nodes w removed
remains regular hlc model model parameterize subtree rooted
z way encodes injective mapping z w w
together two facts prove lemma
corollary let z latent node hlc model suppose z latent neighbor
x let set observed nodes separated x z regular
set probability distributions nodes separated x z way
encode injective mapping z sense p z z z
z z
proof corollary follows readily lemma property root walking
operation zhang
proof claim consider following matrix





p x z





p x z


k







k parameters joint distribution p x z matrix
identity matrix rows properly arranged column vectors linearly
independent almost everywhere


consider first k column vectors jm p p k
must linearly independent almost everywhere one vectors say

p k would linearly depend rest everywhere according lemma
observe ik
p





x

p x p z

x z

p x z







choose p x p z corollary vector p might contain zero elements remove zero elements remains vector iden

tical p x z conclude p x z k linearly depends


fieffective dimensions hlc





p x z p x z k everywhere contradicts conclusion
previous paragraph hence first k vectors jm must linearly independent almost
everywhere
evident similar arguments first k vectors
jm jm linearly independent almost everywhere claim therefore proved
proof claim
every column vector jm linearly depends vectors listed everywhere observe

p



x

p z

z

p






x

p z

z

p z


k



k


p z


therefore every column vector jm corresponds vectors jm linearly depends
first k r vectors listed everywhere
symmetry every column vector jm corresponds vectors jm linearly
depends first k last vectors listed everywhere claim proved

proof claim
prove claim contradiction assume vectors listed linearly
independent almost everywhere according lemma one say v must linearly
depend rest everywhere claim lemma assume v

among last r vectors without loss generality assume v p
exist real numbers ci ik c ir c
value



p





k
x


ci

p





r
x

p




ci




x

p





ci

note last term right hand side runs
parameter vector consists three subvectors set parameters

x branch lemma exists injective mapping x

p x x x x x



vectors consider subvector consisting elements
states images states x mapping subvectors



denoted p ox p ox p ox
values still


fizhang kocka

p ox






k
x

ci

p ox








r
x

p ox




ci




x

p ox





ci



consider first two terms right hand side
k
x

ci

r
x

p ox



k
r
x
x
p ox z
p ox z x
x
c
ci

p z
p z







z
z
k
r
x
x
p ox z x
p ox z
ci


p z ci






z

p ox










ci

p

fact p z
x p x z p x column vector


p ox z identical vector p x z argued proving

claim vectors p x z k constitute basis k dimensional

euclidian space implies vectors p ox z represented

linear combination vectors p ox z k consequently
exist ci ik
k
x

ci

p ox z






k
x

p ox



r
x

p ox z




ci



k
x

ci

p ox z

ci

p ox







hence



ci






r
x

p ox




ci



k
x






combining equation equation get
p ox





k
x

ci

p ox









x

p ox






ci

p

fact fact p x p x p x column


vector p ox identical vector p x column vector


p ox identical vector p x hence
p x





k
x


ci

p x






x

p x





ci

contradicts fact vectors equation form basis column space
jm almost everywhere see section therefore claim must true


fieffective dimensions hlc

effective dimensions trees
let us use term tree model refer markov random fields undirected trees
finite number random variables root tree model nodes get
tree structured bayesian network model tree model define leaf nodes
one neighbor hlc model tree model leaf nodes observed
others latent
turns theorem enables us compute effective dimension tree
model consider arbitrary tree model leaf nodes latent remove
nodes without affecting effective dimension
removing latent leaf nodes leaf nodes observed non leaf nodes
observed decompose model submodels observed non leaf
node following theorem tells us model submodels related terms
effective dimensions
theorem suppose observed non leaf node tree model decomposes
k submodels mk
de

k
x

de mi k



possible decompositions final submodels contain latent nodes
hlc effective dimensions submodels latent variables simply
standard dimensions hlc submodel irregular make regular applying
transformation mentioned end section transformation affect
effective dimensions submodels finally effective dimensions regular hlc
submodels computed theorem
proof theorem possible prove theorem starting jacobian
matrix take less formal revealing
suffices consider case k two submodels share one
node namely let respectively sets observed nodes two
submodels excluding root
p p p p
let set parameters distribution p respectively sets
parameters conditional probability distributions nodes consider
fixing letting vary case space spanned p consists
one vector namely moreover one one correspondence vectors
space spanned p vectors cartesian product spaces
spanned p p let vary adds dimensions
four spaces spanned p p p p consequently

de de de
theorem proved


fizhang kocka

concluding remarks
study effective dimensions hlc work motivated
empirical evidence bic behaves quite well used several hill climbing
learning hlc bice score sometimes leads better
model selection bic score proved theorem relates effective
dimension hlc model effective dimensions two hlc
contain fewer latent variables repeated application theorem allows one reduce
task computing effective dimension hlc model subtasks computing
effective dimensions lc makes computationally feasible compute
effective dimensions large hlc addition proved theorem
effective dimensions general tree main theorem allows one
compute effective dimension arbitrary tree
acknowledgements
work initiated authors visiting department computer science
aalborg university denmark thank poul eriksen finn v jensen jiri vomlel
marta vomlelova thomas nielsen olav bangso jose pena kristian g olesen
grateful annonymous reviewers whose comments helped us greatly
improving partially supported ga cr grant
hong kong grant council grant hkust e

references
akaike h look statistical model identification ieee trans autom
contr
bartholomew j knott latent variable factor analysis nd
edition kendalls library statistics london arnold
cheeseman p stutz j bayesian classification autoclass theory
fayyad u piatesky shaoiro g smyth p uthurusamy r eds
advancesin knowledge discovery data mining aaai press menlo park ca
chickering heckerman efficient approximations marginal
likelihood bayesian networks hidden variables machine learning
cowell r g dawid p lauritzen l spiegelhalter j probabilistic
networks expert systems springer
kocka zhang n l dimension correction hierarchical latent class
proc th conference uncertainty artificial intelligence
uai
geiger heckerman meek c asymptotic model selection directed
networks hidden variables proc th conference uncertainty
artificial intelligence


fieffective dimensions hlc

goodman l exploratory latent structure analysis identifiable
unidentifiable biometrika
lazarsfeld p f henry n w
mifflin
rusakov geiger
networks uai

latent structure analysis boston houghton

asymptotic model selection naive bayesian

rusakov geiger automated analytic asymptotic evaluation marginal
likelihood latent uai
schwarz g estimating dimension model annals statistics
settimi r smith j q geometry bayesian graphical
hidden variables proceedings fourteenth conference uncertainty
artificial intelligence morgan kaufmann publishers francisco ca
settimi r smith j q geometry moments bayesian networks hidden
variables proceedings seventh international workshop artificial intelligence statistics fort lauderdale florida january morgan kaufmann
publishers francisco ca
zhang n l hierarchical latent class cluster analysis aaai
zhang n l kocka karciauskas g jensen f v learning hierarchical
latent class technical report hkust cs department computer
science hong kong university science technology
zhang n l
structural em hierarchical latent class technical
report hkust cs department computer science hong kong university
science technology
zhang n l b hierarchical latent class cluster analysis journal
machine learning appear




