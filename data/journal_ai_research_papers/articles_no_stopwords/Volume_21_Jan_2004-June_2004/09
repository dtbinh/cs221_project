Journal Artificial Intelligence Research 21 (2004) 319356

Submitted 11/02; published 03/04

Representation Dependence Probabilistic Inference
Joseph Y. Halpern

halpern@cs.cornell.edu

Cornell University, Computer Science Department
Ithaca, NY 14853
http://www.cs.cornell.edu/home/halpern

Daphne Koller

koller@cs.stanford.edu

Stanford University, Computer Science Department
Stanford, CA 94035
http://www.cs.stanford.edu/ koller

Abstract
Non-deductive reasoning systems often representation dependent: representing
situation two different ways may cause system return two different answers. viewed significant problem. example, principle
maximum entropy subjected much criticism due representation dependence. has, however, almost work investigating representation dependence.
paper, formalize notion show problem specific maximum entropy. fact, show representation-independent probabilistic inference
procedure ignores irrelevant information essentially entailment, precise sense.
Moreover, show representation independence incompatible even weak default assumption independence. show invariance restricted class
representation changes form reasonable compromise representation independence desiderata, provide construction family inference procedures
provides restricted representation independence, using relative entropy.

1. Introduction
well known way problem represented significant impact
ease people solve it, complexity algorithm solving
it. interested arguably even fundamental issue: extent
answers get depend input represented. too,
well known work, particularly Tversky Kahneman (see, example, (Kahneman,
Slovic, & Tversky, 1982)), showing answers given people vary significantly
(and systematic ways) depending question framed. phenomenon often
viewed indicating problem human information processing; implicit assumption
although people make mistakes sort, shouldnt. hand,
competing intuition suggests representation (and ) matter;
representation dependence natural consequence fact.
consider one type reasoning, probabilistic inference, examine extent
answers depend representation. issue representation dependence
particular interest context interest using probability knowledge
representation (e.g., (Pearl, 1988)) probabilistic inference source
c
2004
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiHalpern & Koller

many concerns expressed regarding representation. However, approach
applicable far generally.
begin noting notion probabilistic inference two quite different
interpretations. one interpretation, forms basis Bayesian paradigm,
probabilitic inference consists basically conditioning: start prior distribution event space, condition whatever observations obtained.
interpretation, given set probabilistic assertions, goal
reach conclusions probabilities various events. paper,
focus latter interpretation, although discuss relationship Bayesian
approach Section 7.2.
Suppose procedure making inferences probabilistic knowledge
base. sensitive way knowledge represented? Consider following
examples, use perhaps best-known non-deductive notion probabilistic inference,
maximum entropy (Jaynes, 1978).1
Example 1.1: Suppose information whatsoever regarding whether
object colorful. probability assign proposition colorful ? Symmetry
arguments might suggest 1/2. Since information, seems object
likely colorful non-colorful. conclusion reached
maximum entropy provided language proposition colorful .
suppose know colors red, blue, green, propositions corresponding
colors. Moreover, colorful actually mean red blue green.
case, maximum entropy dictates probability red blue green 7/8. Note that,
cases, conclusion follows constraints trivial one:
probability query somewhere 0 1.
Example 1.2: Suppose told half birds fly. two reasonable ways represent information. One propositions bird fly,
use knowledge base KB fly
1 =def [Pr(fly | bird ) = 1/2]. second might
basic predicates bird flying-bird , use knowledge base KB fly
2 =def [(flying-bird
bird ) Pr(flying-bird | bird ) = 1/2]. Although first representation may appear natural, seems representations intuitively adequate insofar representing
information given. use inference method maximum
entropy, first representation leads us infer Pr(bird ) = 1/2, second leads us
infer Pr(bird ) = 2/3.
Examples basis frequent criticisms maximum entropy
grounds representation dependence. pointing examples,
little work problem. fact, work Salmon (1961,
1963) Paris (1994), seems work formalizing notion representation dependence. One might say consensus was: whatever representation
independence is, property enjoyed maximum entropy.
1. Although much discussion motivated representation-dependence problem encountered
maximum entropy, understanding maximum entropy works essential
understanding discussion.

320

fiRepresentation Dependence

inference procedures it? paper attempt understand notion
representation dependence, study extent achievable.
study representation dependence, must first understand mean
representation. real world complex. reasoning process, must focus
certain details ignore others. semantic level, relevant distinctions captured
using space X possible alternatives states (possible worlds). Example 1.1,
first representation focused single attribute colorful . case, two
states state space, corresponding colorful true false, respectively.
second representation, using red , blue, green, richer state space. Clearly,
distinctions could make.
interpret representation syntactic entity. case, typically capture relevant distinctions using formal language. example, use propositional
logic basic knowledge representation language, choice primitive propositions
characterizes distinctions chosen make. take states
truth assignments propositions. Similarly, use probabilistic representation language belief networks (Pearl, 1988) knowledge representation
language, must choose set relevant random variables. states
possible assignments values variables.
mean shift representation (i.e., state space) X another representation ? Roughly speaking, want capture level state space shift
from, say, feet meters. Thus, X distances might described terms feet
might described terms meters. would expect constraint
relating feet meters. constraint would give extra information X;
would relate worlds X worlds . Thus, first attempt capture representation independence somewhat indirectly, requiring adding constraints relating X
place constraints X result different conclusions X.
resulting notion, called robustness, turns surprisingly strong. show
every robust inference procedure must behave essentially logical entailment.
try define representation independence directly, using mapping f
one representation another. example, f could map world individual
6 feet tall corresponding world individual 1.83 meters tall.
obvious constraints f necessary ensure corresponds intuition
representation shift. define representation-independent inference procedure
one preserves inferences every legitimate mapping f ; i.e., KB ,
KB | iff f (KB ) | f ().
definition turns somewhat reasonable first attempt,
exist nontrivial representation-independent inference procedures. However, still
strong notion. particular, representation-independent inference procedure must
act essentially logical entailment knowledge base objective information
(i.e., essentially non-probabilistic information). Moreover, show representation
independence incompatible even simplest default assumption independence.
Even told nothing propositions p q, representation independence
allow us jump conclusion p q independent.
results suggest want inference procedures capable jumping
nontrivial conclusions, must accept least degree representation de321

fiHalpern & Koller

pendence. add support claim choice language carry great
deal information, complete representation independence much expect.
positive note, show use intuition choice language
carries information get limited forms representation independence. idea
language put constraints counts appropriate representation
shift. example, suppose certain propositions represent colors others represent
birds. may willing transform colorful red blue green, may
willing transform red sparrow . reason demand inference procedure behave way suddenly shift wildly inappropriate representation,
symbols mean something completely different. provide general approach
constructing inference procedures invariant specific class representation
shifts. construction allows us combine degree representation independence
certain non-deductive properties want inference procedure. particular, present inference method supports default assumption independence,
yet invariant natural class representation shifts.
rest paper organized follows. Section 2, define probabilistic inference procedures characterize them. Section 3, define robust inference procedures
show every robust inference procedure essentially entailment. Section 4,
define representation independence, show representation independence
strong requirement. particular, show representation-independent inference
procedure essentially acts logical entailment objective knowledge bases
representation independence incompatible default assumption independence.
Section 5 contains general discussion notion representation independence
reasonable assume choice language affect inference.
may indeed seem reasonable assume choice language affect
inference, point assumption consequences might view
unfortunate. Section 6, discuss limited forms representation independence
achieved. discuss related work Section 7, conclude Section 8.

2. Probabilistic Inference
begin defining probabilistic inference procedures. discussed introduction,
two quite different ways term used. one, given prior
distribution probability space; knowledge typically consists events
space, used condition distribution obtain posterior.
other, focus work, probabilistic inference procedure takes input
probabilistic knowledge base returns probabilistic conclusion.
take knowledge base conclusion assertions probabilities events measurable space (X, FX ), measurable space consists
set X algebra FX subsets X (that is, FX set subsets X closed
union complementation, containing X itself).2 Formally, assertions viewed
statements (or constraints on) probability measures (X, FX ). example,
2. X infinite, may want consider countably-additive probability measures take FX
closed countable unions. issue play significant role paper. simplicity,
restrict finite additivity require FX closed finite unions.

322

fiRepresentation Dependence

FX , statement Pr(S) 2/3 holds distributions probability
least 2/3. Therefore, (X,FX ) set probability measures (X, FX ) (that
is, probability measures domain FX ), view knowledge base set
constraints (X,FX ) . FX clear context, often omit notation,
writing X rather (X,FX ) .
place restrictions language used express constraints.
assume includes assertions form Pr(S) subsets FX rational
[0, 1], closed conjunction negation, KB KB 0
knowledge bases expressing constraints, KB KB 0 KB . (However,
langauge could include many assertions besides obtained starting assertions
form Pr(S) closing conjunction negation.) Since language
puts constraints probability measures, cannot directly say FX must hold.
closest approximation language assertion Pr(S) = 1. Thus, call
constraints objective. knowledge base consisting objective constraints called
objective knowledge base. Since Pr(T1 ) = 1 Pr(T2 ) = 1 equivalent Pr(T1 T2 ) = 1,
without loss generality, objective knowledge base consists single constraint
form Pr(T ) = 1. Given knowledge base KB placing constraints X , write |= KB
measure X satisfies constraints KB . use [[KB ]]X denote
measures satisfying constraints.
practice, knowledge typically represented syntactically, using logical language describe possible states. Typical languages include propositional logic, firstorder logic, language describing values set random variables. general,
base logic L defines set formulas L() given vocabulary . propositional
logic, vocabulary simply set propositional symbols. probability theory,
vocabulary consist set random variables. first-order logic, vocabulary
set constant symbols, function symbols, predicate symbols. facilitate comparison
vocabularies, assume base logic vocabularies finite
subsets one fixed infinite vocabulary .
working language, assume state state space defines
interpretation symbols hence formulas L(). case
propositional logic, thus assume associate state truth assignment
primitive propositions . first-order logic, assume associate
state domain interpretation symbols . probabilistic
setting, assume associate state assignment values
random variables. often convenient assume state space fact
subset W W(), set interpretations (or assignments to) vocabulary .
Note truth formula L() determined state. true
state w, write w |= .
probabilistic extension Lpr () base logic L() simply set probability
formulas L(). Formally, L(), Pr() numeric term. formulas
Lpr () defined Boolean combinations arithmetic expressions involving
numeric terms. example, Pr(fly | bird ) 1/2 formula Lpr ({fly, bird }) (where
interpret conditional probability expression Pr( | ) Pr( )/ Pr()
multiply clear denominator). analogy constraints, formula form
Pr() = 1 called objective formula.
323

fiHalpern & Koller

Given set W W(), assume FW algebra consisting sets
form [[]]W = {w : w |= }, L(). (In case propositional logic,
consists finite set primitive propositions, FW = 2W . case first-order logic,
sets necessarily definable formulas, FW may strict subset 2W .)
Let probability measure (W, FW ). ascribe semantics Lpr ()
probability space (W, FW , ) straightforward way. particular, interpret
numeric term Pr() ({w W : w |= }). Since formula L() describes event
space W , formula Lpr () clearly constraint measures W . write
|= measure W satisfies formula .
syntactic knowledge base KB Lpr () viewed constraint W
obvious way. Formally, KB represents set probability measures [[KB ]] W ,
consists measures W |= KB .
say KB (whether syntactic semantic) consistent [[KB ]]X 6= , i.e.,
constraints satisfiable. Finally, say KB entails (where another set
constraints X ), written KB |=X , [[KB ]]X [[]]X , i.e., every measure satisfies
KB satisfies . write |=X satisfied every measure X . omit
subscript X |= clear context.
Entailment well-known weak method drawing conclusions
knowledge base, particular respect treatment irrelevant information.
Consider knowledge base consisting constraint Pr(fly | bird ) 0.9. Even
though know nothing suggest red relevant, entailment allow us
reach nontrivial conclusion Pr(fly | bird red ).
One way get powerful conclusions consider, measures satisfy
KB , subset them. Intuitively, given knowledge base KB , inference procedure
picks subset measures satisfying KB , infers holds subset. Clearly,
conclusions hold every measure subset hold every measure
entire set.
Definition 2.1 : (X, FX )-inference procedure partial function : 2(X,FX ) 7
2(X,FX ) I(A) (X,FX ) I(A) = iff = 2(X,FX )
domain (i.e., defined). write KB |I I([[KB ]]X ) [[]]X .
FX clear context irrelevant, often speak X-inference procedures.
remark Paris (1994) considers calls inference processes.
inference procedures defined that, given set probability measures,
return unique probability measure (rather arbitrary subset A). Paris
gives number examples inference processes. considers various properties
inference process might have. closely related various properties
representation indepedence consider. discuss Pariss work Section 7.
Entailment X-inference procedure defined sets determined taking
identity. Maximum entropy inference procedure sense.
Definition 2.2: Given probability measure finite space X (where sets
P
measurable), entropy H() defined xX (x) log (x). (The log taken
(A) consist measures
base 2 here.) Given set measures X , let IX
324

fiRepresentation Dependence

highest entropy measures whose entropy least high
measure A; measures, inf (A) undefined.
easy see inf (A) defined closed (in topological sense; i.e.,
n sequence probability measures n converges , A). Thus,
could take domain inf
X consist closed sets measures X .
open sets inf (A) defined, although defined
open sets A. example, suppose X = {x1 , x2 } let = { : (x1 ) < 1/2}. Let 0
0 (x1 ) = 1/2. easy check H(0 ) = 1, H() < 1 A.
However, , H() > 1 . follows
measure whose entropy higher measure A, inf (A)
undefined. hand, A0 = { : (x1 ) < 2/3}, measure whose
entropy maximum open set A0 , namely measure 0 .
are, course, many inference procedures besides entailment maximum entropy defined measurable space. fact, following proposition shows,
binary relation | satisfying certain reasonable properties inference procedure
type.
Proposition 2.3: X-inference procedure following properties hold
every KB , KB 0 , , X KB domain I.
Reflexivity: KB |I KB .
Left Logical Equivalence: KB logically equivalent KB 0 , i.e., |= KB KB 0 ,
every KB |I iff KB 0 |I .
Right Weakening: KB |I |= KB |I .
And: KB |I KB |I , KB |I .
Consistency: KB consistent KB |6I false.
Proof: Straightforward definitions.
Interestingly, properties commonly viewed part core reasonable
properties nonmonotonic inference relation (Kraus, Lehmann, & Magidor, 1990).
would prove converse, showing relation | probabilistic
constraints space X satisfies five properties must form
|IX . quite true, following example shows.
Example 2.4: Fix measurable space (X, FX ). Let language consist (finite)
Boolean combination statements form Pr(S) , FX . fix one
nonempty strict subset S0 X, let n statement Pr(S0 ) 1/n. Define
inference procedure | follows. KB equivalent true (i.e, [[KB ]]X 6= X ),
KB | iff KB |= . hand, true | iff n |= sufficiently large
n. is, true | exists N n N , n |= .
easy check five properties Proposition 2.3 hold |. However, | |I
X-inference procedure I. suppose were. Note n |= n m,
325

fiHalpern & Koller

true | m. Thus, must I(X ) [[n ]]X n. follows
IX (X ) [[Pr(S0 ) = 0]]X , true |I Pr(S0 ) = 0. However, n 6|= Pr(S) = 0 n,
true | Pr(S) = 0. contradicts assumption | = |I .
Essentially need get converse Proposition 2.3 infinitary version
V
Rule, would say KB |I i, KB |I . language
closed infinite conjunctions, rule would fact need.
Since assumed language closed infinite conjunctions, use
variant rule.
Infinitary And: set statements, KB |I |= ,
KB |I .
Proposition 2.5: Let | relation probabilistic constraints X
properties Reflexivity, Left Logical Equivalence, Right Weakening, Infinitary And, Consistency hold KB domain | . (That is, KB domain |
KB | , KB | KB , on.) | |I X-inference
procedure I.
Proof: See Appendix A.1.
typically interested inference procedure defined one space X,
family related inference procedures, defined number spaces. example,
entailment inference procedure defined spaces X; maximum entropy
defined finite measurable spaces (X, 2X ).
Definition 2.6 : X set measurable spaces, X -inference procedure set
{I(X,FX ) : (X, FX ) X }, I(X,FX ) (X, FX )-inference procedure (X, FX ) X .
sometimes talk X -inference procedure I, write KB |I (X, FX )
X clear context. However, stressed that, formally, X -inference procedure really set inference procedures (typically related natural way).
Clearly entailment X -inference procedure X , IX simply identity
function X X . X consists finite measurable spaces sets measurable,
maximum entropy X -inference procedure. typically denote inference
procedure | . Thus, KB | holds probability measures maximum
entropy satisfying KB .
Important assumptions: remainder paper, deal X inference procedures X satisfies two richness assumptions. assumptions
hold standard inference procedures considered.
assume X closed crossproducts, (X, FX ), (Y, FY ) X ,
(X Y, FXY ) X , FXY algebra formed taking finite unions
disjoint sets form , FX FY . easy see
algebra, since = (S ) (S 0 0 ) =
(S 0 ) (T 0 ) (from follows union sets
326

fiRepresentation Dependence

written disjoint union). Note X finite sets, FX = 2X ,
FY = 2Y , FX FY = 2XY . shall see, (X Y, FXY ) X
(X, FX ) (Y, FY ) X allows us relate constraints X constraints
natural way.
assume X contain sets finite cardinalities; precisely, n 2,
exists set (X, FX ) X |X| = n FX = 2X . assumption
actually needed results, since assumption X closed
crossproducts already implies that, finite n, exists measurable
space (X, FX ) X |X| n; already suffices prove results
paper. However, assuming X sets cardinalities make
proofs easier.
want domain satisfy certain assumptions, defer stating
assumptions introduced additional definitions notation.

3. Robustness
order define robustness representation shifts, must first define notion
representation shift. first attempt definition based idea using
constraints specify relationship two vocabularies. example, Example 1.1, might X = {colorful , colorless} = {red , blue, green, colorless}.
specify relationship X via constraint asserts colorful
(red blue green).
course, every constraint legitimate mapping representations.
example, formula asserted colorful obviously legitimate representation
shift. minimum, must assume constraint give additional
information X far logical inference goes. syntactic level, use
following definition. Given knowledge base KB Lpr (), say Lpr ( 0 )
-conservative KB if, formulas Lpr (), KB |= iff KB |= .
Thus, adding knowledge base permit additional logical inferences
vocabulary . inference procedure robust unaffected conservative
extensions; is, KB , Lpr (), KB |I iff KB |I
-conservative KB . Roughly speaking, says getting new information
uninformative far logical inference goes affect default conclusions.
formal definition robustness, uses semantic rather syntactic concepts,
extends intuitions arbitrary constraints measures (not ones
expressed language Lpr ).
Definition 3.1: X1 ...Xn , define Xi Xi taking Xi (A) = (X1
Xi1 Xi+1 Xn ). constraint Xi viewed constraint
X1 ...Xn taking [[]]X1 ...Xn = { X1 ...Xn : Xi |= }. frequently identify
constraints Xi constraints X1 . . . Xn way. B X1 Xn , define
proj Xi (B) = {Xi : B}. constraint X1 Xn said Xi -conservative
constraint KB Xi proj Xi ([[KB ]]X1 Xn ) = [[KB ]]Xi .
327

fiHalpern & Koller

see definition generalizes earlier language-oriented definition, note
KB constraints X constraint XY , KB |= iff
proj 1 ([[KB ]]XY ) [[]]X , KB |= iff [[KB ]]X [[]]X .
Definition 3.2: {IX : X X } robust X -inference procedure spaces X, X ,
constraints KB X , constraints XY X-conservative
KB , KB |IX iff KB |IXY . (Note definition implicitly assumes
X X X, X , assumption made explicit earlier.)
first glance, robustness might seem reasonable desideratum. all,
adding constraint XY places restrictions X change conclusions might reach X? Unfortunately, turns definition
deceptively strong, disallows interesting inference procedures. particular,
one property may hope inference procedure draw nontrivial conclusions
probabilities events, is, conclusions follow entailment.
example, maximum entropy (or inference procedure based symmetry) conclude
Pr(p) = 1/2 empty knowledge base. show inference procedures
robust really allow much way nontrivial conclusions
probabilities events.
Definition 3.3: (X, FX )-inference procedure essentially entailment knowledge base KB X FX , KB |I < Pr(S) < KB |= Pr(S) .
essentially entailment X essentially entailment knowledge bases KB
domain IX .
Thus, entailment lets us conclude Pr(S) [, ], inference procedure
essentially entailment lets us draw slightly stronger conclusion Pr(S) (, ).
prove this, need make three assumptions domain I. (For results,
need assumptions domain I.)
DI1. Pr(S) domain I(X,FX ) FX , , IR.
DI2. KB domain IX , domain IXY (when KB
viewed constraint XY .)
DI3. KB 1 KB 2 domain IX , KB 1 KB 2 .
Note sets form Pr(S) closed sets. certainly seems reasonable
require sets domain inference procedure; correspond
basic observations. DI2 seems quite innocuous; observed earlier, want
able view constraints X constraints XY ,
prevent domain I. DI3 seems reasonable assumption,
since KB 1 KB 2 correspond possible observations, want able draw
conclusions combining observations. DI3 holds domain consists
closed sets. note hold take domain consist
sets measure whose entropy maximum. example, X = {x1 , x2 },
= {0 } { : (x1 ) > 3/4}, B = { : (x1 ) 2/3}, 0 (x0 ) = 1/2,
B measure whose entropy maximum, B measure
whose entropy maximum.
328

fiRepresentation Dependence

Theorem 3.4: {IX : X X } robust X -inference procedure satisfies DI1, DI2,
DI3, IX essentially entailment X X .
Proof: See Appendix A.2.
possible construct robust inference procedures almost quite
entailment, simply strengthening conclusions Pr(S) [, ] Pr(S)
(, ). Clearly, however, robust inference procedure extremely limited ability
jump conclusions. next section, look definition seems closer
intuitive notion representation independence, somewhat reasonable
consequences.

4. Representation Independence
4.1 Representation shifts
X two different representations phenomena then, intuitively,
way relating states X corresponding states . want
correspondence respect logical structure events. Formally, require
homomorphism respect complementation intersection.
Definition 4.1: (X, FX )-(Y, FY ) embedding f function f : FX 7 FY
f (S ) = f (S) f (T ) f (S) = f (S) S, FX .
elsewhere, talk X-Y embeddings rather (X, FX )-(Y, FY ) embeddings
FX FY play significant role.
goal consider effect transformation probabilistic formulas. Hence,
interested sets states probabilities.
Definition 4.2: f X-Y embedding, X , , correspond
f (S) = (f (S)) events FX . define mapping f : 2X 7 2Y
follows. first define f singleton sets (except that, convenience, write f ()
rather f ({}) taking f () = { : (f (S)) = (S) FX }. Thus,
f () consists measures correspond f . arbitrary
subset 2X , define f (D) = f () X .
constraint X expressed language, typically write f () rather
f ([[]]X ). implicitly assume language constraint f ()
expressible. hard see f () constraint results replacing every
set FX appears f (S).
Example 4.3: Example 1.1, might X = {colorful , colorless} = {red , blue,
green, colorless}. case, might f (colorful ) = {red , blue, green} f (colorless) =
{colorless}. Consider measure X (colorful ) = 0.7 (colorless) =
0.3. f () set measures total probability assigned set
states {red , blue, green} 0.7. Note uncountably many measures.
easy check constraint X Pr(colorful ) > 3/4, f ()
Pr({red , blue, green}) > 3/4.
329

fiHalpern & Koller

Embeddings viewed semantic analogue syntactic notion interpretation defined (Enderton, 1972, pp. 157162), used recent
literature abstraction (Giunchiglia & Walsh, 1992; Nayak & Levy, 1995). Essentially,
interpretation maps formulas vocabulary formulas different vocabulary
mapping primitive propositions (e.g., colorful ) formulas (e.g.,
red blue green) extending complex formulas obvious way. representation shift Example 1.2 captured terms interpretation, one
taking flying-bird fly bird .
Definition 4.4: Let two vocabularies. propositional case, interpretation function associates every primitive proposition p
formula i(p) L(). complex definition spirit applies first-order
vocabularies. example, R k-ary predicate, i(R) formula k free
variables.
Given interpretation i, get syntactic translation formulas L() formulas
L() using obvious way; example, i((p q) r) = (i(p) i(q)) i(r)
(see (Enderton, 1972) details). Clearly interpretation induces
embedding f W1 W() W2 W(): map [[]]W1 [[i()]]W2 .
course, embeddings count legitimate representation shifts. example,
consider embedding f defined terms interpretation maps propositions p q proposition r. process changing representations using
f gives us information p q equivalent, information might
originally. Intuitively, f gives us new information telling us certain
situationthat p q holdsis possible.
formally, embedding f
following undesirable property: maps set states satisfying p q
empty set. means state p q holds analogue new
representation. want disallow embeddings.
Definition 4.5: X-Y embedding f faithful if, S, FX , iff
f (S) f (T ).
definition desired consequence disallowing embeddings give new
information far logical consequence goes.
Lemma 4.6: X-Y embedding f faithful constraints KB ,
KB |= iff f (KB ) |= f ().
Proof: See Appendix A.3.
clear embedding Example 4.3 faithful: f (colorful ) = {red , blue, green}
f (colorless) = colorless. following proposition gives insight faithful
embeddings.
Proposition 4.7: Let f faithful X-Y embedding. following statements
equivalent:
(a) correspond f ;
330

fiRepresentation Dependence

(b) formulas , |= iff |= f ().
Proof: See Appendix A.3.
embedding f reasonable representation shift, would inference
procedure return answers shift representations using f .
Definition 4.8: X, X , X -inference procedure {IX : X X } invariant
X-Y embedding f constraints KB X , KB |IX iff
f (KB ) |IY f (). (Note that, particular, means KB domain | IX
iff f (KB ) domain | IY .)
Definition 4.9: X -inference procedure {IX : X X } representation independent
invariant faithful X-Y embeddings X, X .
Since embedding Example 4.3 faithful, representation-independent inference procedure would return answers Pr(colorful ) Pr(red blue green).
issue somewhat subtle Example 1.2. There, would embedding f generated interpretation i(flying-bird ) = fly bird i(bird ) = bird .
faithful embedding, since flying-bird bird valid formula,
i(flying-bird bird ) (fly bird ) bird valid. Looking problem semantically, see state corresponding model flying-bird bird holds
mapped . clearly source problem. According linguistic intuitions domain, legitimate state. Rather considering states
W({flying-bird , bird }), perhaps appropriate consider subset X consisting truth assignments characterized formulas {flying-bird bird , flying-bird
bird , flying-bird bird }. use embed X W({fly, bird }), resulting embedding indeed faithful. So, previous example, invariance
embedding would guarantee get answers representations.
4.2 Representation-independent inference procedures
Although definition representation independence seems natural, definition
robustness. two definitions relate other? First, show representation independence weaker notion robustness. result, need
consider inference procedures satisfy two assumptions.
DI4. f faithful X-Y embedding, KB domain IX iff f (KB )
domain IY .
DI5. KB domain IXY , f faithful X-Y embedding, 1 constraint
X , KB (1 f (1 )) domain IXY .
DI4 natural satisfied standard inference procedures. easy
check KB closed iff f (KB ) closed. DI5 may appear natural,
hold domains consisting closed sets, since hard check f (1 )
closed. DI5 would follow DI3 assumption f (1 ) domain
IXY , actually weaker combination two assumptions.
particular, holds domain consisting sets measure
maximum entropy.
331

fiHalpern & Koller

Theorem 4.10: X -inference procedure robust satisfies DI2, DI4, DI5,
representation independent.
Proof: See Appendix A.3.
already shown robust inference procedure must almost trivial.
interesting representation-independent inference procedures? shall see,
answer mixed. nontrivial representation-independent inference procedures,
interesting.
first result shows representation independence, robustness, trivializes
inference procedure, knowledge bases.
Theorem 4.11: {IX : X X } representation-independent X -inference procedure
then, X X , IX essentially entailment objective knowledge bases
domain.3
Proof: See Appendix A.3.
Corollary 4.12: {IX : X X } representation-independent X -inference procedure,
KB objective, KB |I < Pr(S) < 0 1, = 0
= 1.
result tells us objective knowledge base Pr(T ) = 1, reach
three possible conclusions set S. S, conclude Pr(S) = 1;
S, conclude Pr(S) = 0; otherwise, strongest conclusion
make Pr(S) somewhere 0 1.
construct representation-independent inference procedure entailment precisely behavior restrict attention countable state spaces. Suppose X countable. Given objective knowledge base KB form Pr(T ) = 1,
FX , let KB + consist formulas form 0 < Pr(S) < 1
nonempty strict subsets FX .4 define X-inference procedure
0 follows: KB equivalent objective knowledge base, KB |
IX
I0
KB KB + |= ; KB equivalent objective knowledge base, KB |I 0
0 indeed inference procedure.
KB |= . follows easily Proposition 2.5 IX
Moreover, equivalent standard notion entailment; example,
true |I 0 0 < Pr(p) < 1, 6|=0 < Pr(p) < 1. Nevertheless, prove 0
representation independent.
0 : X X } representationLemma 4.13: Let X consist countable sets. {IX
independent X -inference procedure.

3. earlier version paper (Halpern & Koller, 1995), claimed representationindependent inference procedure satisfied minimal irrelevance property (implied robustness,
equivalent it) essentially entailment knowledge bases. Jaeger (1996) shows,
inference procedure along lines 1 described constructed show result
correct. seem need full strength robustness.
4. requirement X countable necessary here. X uncountable every singleton FX ,
KB + inconsistent uncountable. impossible uncountable
collection points positive measure.

332

fiRepresentation Dependence

Proof: See Appendix A.3.
objective knowledge bases may appear interesting restrict propositional languages, languages include first-order statistical information
become quite interesting. Indeed, shown (Bacchus, 1990; Bacchus, Grove, Halpern, &
Koller, 1996), knowledge bases first-order (objective) statistical information allow
us express great deal information naturally encounter. example,
express fact 90% birds fly objective statement number
flying birds domain relative overall number birds. course, Theorem 4.11
applies immediately knowledge bases.
Theorem 4.11 implies various inference procedures cannot representation
independent. particular, since true | Pr(p) = 1/2 primitive proposition p,
follows maximum entropy essentially entailment. observation provides
another proof maximum entropy representation independent.
consistent Theorem 4.11 representation-independent inference
procedures almost entailment probabilistic knowledge bases. example,
1 defined follows. Given , exists
consider X-inference procedure IX
X
1 (A) = { : (S) 1/3};
FX = { X : (S) 1/4}, IX
X
1 (A) = A. Thus, Pr(S) 1/4 | Pr(S) 1/3. Clearly, 1 essentially
otherwise, IX
X
I1
entailment. Yet, prove following result.

Lemma 4.14: Suppose X consists measure spaces form (X, 2X ),
1 : X X } representation-independent X -inference procedure.
X finite. {IX

Proof: See Appendix A.3.
Note follows Theorem 3.4 1 cannot robust. Thus, shown
representation independence strictly weaker notion robustness.
example might lead us believe representation-independent inference procedures interesting probabilistic knowledge bases. However,
show, representation-independent inference procedure cannot satisfy one key desideratum: ability conclude independence default. example, important feature
maximum-entropy approach nonmononotic reasoning (Goldszmidt, Morris, & Pearl,
1993) ability ignore irrelevant information, implicitly assuming independence. course, maximum entropy satisfy representation independence.
result shows approach probabilistic reasoning simultaneously assure
representation independence default assumption independence.
try give general notion default assumption independence here,
since need result. Rather, give minimal property would
hope inference procedure might have, show property sufficient preclude
representation independence. Syntactically, property want
disjoint vocabularies, KB Lpr (), L(), L(), KB |I Pr( ) =
Pr() Pr().
333

fiHalpern & Koller

Definition 4.15: X -inference procedure {IX : X X } enforces minimal default independence if, whenever X X , KB constraint X domain |IX ,
FX , FY , KB |IXY Pr(S ) = Pr(S) Pr(T ).5
definition clearly generalizes syntactic definition.
Clearly, entailment satisfy minimal default independence. Maximum entropy,
however, does. Indeed, semantic property implies minimal default independence
used Shore Johnson (1980) one axioms axiomatic characterization
maximum-entropy.
Theorem 4.16: {IX : X X } X -inference procedure enforces minimal default
independence satisfies DI1, IX representation independent.
Proof: See Appendix A.3.
result interesting far irrelevance concerned. might hope
learning irrelevant information affect conclusions. attempt
define irrelevance here, certainly would expect KB 0 vocabulary disjoint
KB , then, example, KB |I Pr() = iff KB KB 0 |I Pr() = . KB 0
objective, standard probabilistic approach would identify learning KB 0
conditioning KB 0 . Suppose restrict inference procedures indeed
condition objective information (as case class inference procedures
consider Section 6). KB KB 0 |I Pr() = exactly KB |I Pr( | KB 0 ) = .
Thus, Theorem 4.16 tells us inference procedures condition new (objective)
information cannot representation independent ignore irrelevant information.
Thus, although representation independence, unlike robustness, force us use
entirely trivial inference procedures, prevent us using procedures
certain highly desirable properties.

5. Discussion
results suggest type representation independence hard come by.
raise concern perhaps definitions quite right. provide
seems even support latter point.
Example 5.1: Let Q unary predicate c1 , . . . , c100 , constant symbols. Suppose
two vocabularies = {Q, d} = {Q, c1 , . . . , c100 , d}. Consider
interpretation i(d) = i(Q(x)) = Q(x) Q(c1 ) . . . Q(c100 ).
Now, consider KB = xQ(x). case, i(KB ) = x(Q(x) Q(c1 ) . . . Q(c100 ).
Intuitively, since ci may refer domain element, conclusion
make certainty Q(c1 ) . . . Q(c100 ) exists least one Q
domain, gives us additional information beyond KB . convert example
general argument embedding f corresponding faithful. Intuitively,
5. Since working space X , KB viewed constraint XY here, Pr(S)
understood Pr(S ), Pr(T ) understood Pr(X ). Recall that,
assumption, X X .

334

fiRepresentation Dependence

KB , get conclusion Q(c1 ) . . . Q(c100 ) f (KB ) Q(x) appears
positively KB ; but, case, already know least one Q,
gain new information embedding. seem unreasonable
inference procedure assign different degrees belief Q(d) given KB = xQ(x)
one hand given i(KB ) = x(Q(x) Q(c1 ) . . . Q(c100 )) other,6 particularly
domain small. fact, many reasoning systems explicitly adopt unique names
assumption, would clearly force different conclusions two situations.
example suggests that, least first-order case, even faithful embeddings
always match intuition reasonable representation shift. One might
therefore think perhaps problem definition even propositional
case. Maybe totally different definition representation independence avoids
problems. possible, believe case. techniques
used prove Theorem 4.16 3.4 seem apply reasonable notion
representation independence.7 give flavor type argument used prove
theorems, consider Example 1.1, assume true |I Pr(colorful ) = (0, 1).8
Using embedding g g(colorful ) = red , conclude true |I Pr(red ) = .
Similarly, conclude Pr(blue) = Pr(green) = . order |I
invariant original embedding, must true |I Pr(red blue green) = ,
completely inconsistent previous conclusions. embeddings use
argument natural ones; would want definition representation
independence disallowed them.
results viewed support position representation dependence
justified; choice appropriate representation encodes significant information.
particular, encodes bias knowledge-base designer world. Researchers
machine learning long realized bias inevitable component effective
inductive reasoning (i.e., learning evidence). completely surprised
turns types leaping conclusions (as context) depend
bias.
need little careful here. example, cases identify
vocabulary (and hence, representation) sensors agent
disposal. may seem unreasonable agent temperature sensor
motion sensor might carve world differently agent color sensor
distance sensor. consider two agents different sensors yet
made observations. Suppose talk distance tree.
reasonable two agents reach different conclusions distance
different sensors (and thus use different vocabularies), although
made observations? would follow agents change
conclusions switched sensors, despite made observations.
seem reasonable!
Bias representation independence viewed two extremes spectrum.
accept knowledge base encodes users bias, obligation
6. Actually, i(Q(d)) = Q(d) Q(c1 ) . . . Q(c100 ), latter equivalent Q(d) given KB .
7. certainly applied many definitions tried!
8. fact, suffices assume true |I Pr(colorful ) [, ], long > 0 < 1.

335

fiHalpern & Koller

invariant representation shifts all. hand, assume
representation used carries information, coherence requires inference procedure
give answers equivalent representations. believe right answer lies somewhere between. typically number reasonable ways
represent information, might want inference procedure return
conclusions matter choose. thus makes sense require
inference procedure invariant embeddings take us one reasonable representation another. follow must invariant
embeddings, even embeddings syntactically similar ones wish
allow. may willing refine colorful red blue green define flying-bird
fly bird , transform red sparrow . next section, show
construct inference procedures representation independent limited class
representation shifts.

6. Selective invariance
discussed above, want construct inference procedure invariant
certain embeddings. purposes section, restrict attention finite
spaces, sets measurable. is, focus X -inference procedures
X consists measure spaces form (X, 2X ), X finite.
first step understand conditions X -inference procedure
invariant specific X-Y embedding f . conclude KB X ?
Recall inference procedure IX picks subset DX = IX (KB ), concludes iff
holds every measure DX . Similarly, applied f (KB ) , IY picks subset
DY = IY (f (KB )). invariant f respect KB ,
tight connection DX DY .
understand connection, first consider pair measures X .
Recall Proposition 4.7 correspond f iff, formulas ,
|= iff |= f (). understand correspondence extends sets probability
measures, consider following example:
Example 6.1: Consider embedding f Example 4.3, let DX = {, 0 }
above, 0 (colorful ) = 0.6. guarantee reach corresponding
conclusions DX DY ? Assume, example, DY contains measure
correspond either 0 , e.g., measure assigns probability 1/4
four states. case, conclusion Pr(colorful ) 0.7 holds DX ,
holds measures; corresponding conclusion Pr(red blue green) 0.7
hold DY . Therefore, every probability measure DY must correspond
measure DX . Conversely, every measure DX must correspond measure DY .
suppose measure DY corresponding . get conclusion
Pr(blue red green) 6= 0.7 DY , corresponding conclusion Pr(colorful ) 6= 0.7
follow DX . Note two conditions imply DY must
precisely set measures corresponding measures DX . particular, might
DY containing single measure corresponding (and least one corresponding
0 ), e.g., one (red ) = 0.5, (blue) = 0, (green) = 0.2, (colorless) = 0.3.
336

fiRepresentation Dependence

Based example, use following extension definition correspondence.
Definition 6.2: say DX DY correspond f DY , exists
corresponding DX (so (S) = (f (S)) X), DX ,
exists corresponding DY .
Proposition 6.3: Suppose f faithful X-Y embedding, DX X , DY .
following two conditions equivalent:
(a) DX DY correspond f ;
(b) , DX |= iff DY |= f ().9
Proof: See Appendix A.4.
produce inference procedure invariant X-Y embedding f ,
must ensure every KB , IX (KB ) IY (KB ) correspond. first glance,
seems rather difficult guarantee correspondence every knowledge base. turns
situation bad. remainder section, show how, starting
correspondence knowledge base truethat is, starting correspondence
IX (X ) IY (Y )we bootstrap correspondence KB s, using
standard probabilistic updating procedures.
First consider problem updating objective information. standard way
update via conditioning. measure X event X, define
|S measure assigns probability (w)/(S) every w S, zero
states. set measures DX X , define DX |S {|S : DX }.
following result easy show.
Proposition 6.4: Let X event let f faithful X-Y embedding.
correspond f , |S |f (S) correspond f .
Proof: Almost immediate definitions; left reader. (In case, note
result follows Theorem 6.4 below.)
Clearly, result extends sets measures.
Corollary 6.5: f faithful X-Y embedding, DX DY correspond f ,
DX |S DY |f (S) correspond f .
want update constraint objective? standard extension
conditioning case via relative entropy KL-divergence (Kullback & Leibler,
1951).
9. (a) implies (b) arbitrary spaces, implication (b) (a) depends restriction
finite spaces made section. suppose X natural numbers N , f identity, DX
consists probability measures N , DY consists measures measure 0
0 (n) = 1/2n+1 . language consists finite Boolean combinations assertions form
Pr(S) , N , easy see DX |= iff DY |= formulas , clearly DX
DY correspond identity map.

337

fiHalpern & Koller

Definition 6.6: 0 measures X, relative entropy 0 ,
P
denoted KLX (0 k), defined xX 0 (x) log(0 (x)/(x)). measure X
constraint , let | denote set measures 0 satisfying KLX (0 k)
minimal.
Intuitively, KL-divergence measures distance 0 . measure 0 satisfying KLX (0 k) minimal thought closest measure
satisfies . denotes objective constraint, unique measure satisfying KLX (0 k) minimal conditional measure | (Kullback & Leibler,
1951). (That deliberately used notation conditioning.)
Moreover, easy show KLX (0 k) = 0 iff 0 = . follows ,
| = .
Given set measure DX X constraint X , define DX | DX |.
apply well-known result (see, e.g., (Seidenfeld, 1987)) generalize Proposition 6.4 case relative entropy.
Theorem 6.7: Let arbitrary constraint X . f faithful X-Y embedding
correspond f , | |f () correspond f .
Proof: See Appendix A.4.
Again, result clearly extends sets measures.
Corollary 6.8: f faithful X-Y embedding, DX DY correspond f ,
DX | DY |f () correspond f .
results give us way bootstrap invariance. construct inference procedure uses relative entropy starting set prior probability measures. Intuitively, encode users prior beliefs domain. information comes in,
measures updated using cross-entropy. design priors certain invariances hold, Corollary 6.8 guarantees invariances continue hold throughout
process.
Formally, prior function P X maps X X set P(X) probability measures
P (KB ) = P(X)|KB . Note
X . Define inference procedure P taking IX
P (true) = P(X), constraints all, use P(X) basis
IX
inference. standard inference procedures form P
prior function P. fairly straightforward verify, example, entailment P
P(X) = X . (This because, observed earlier, |KB = KB .) Standard
Bayesian conditioning (defined objective knowledge bases) form, take
P(X) single measure space X. interestingly, well known (Kullback
& Leibler, 1951) maximum entropy Pu Pu (X) singleton set containing
uniform prior X.
say robustness P representation shifts? Using Proposition 6.3 Corollary 6.5, easy show want P invariant
set F embeddings, must ensure prior function right
correspondence property.
Theorem 6.9: f faithful X-Y embedding, P invariant f iff P(X)
P(Y ) correspond f .
338

fiRepresentation Dependence

Proof: See Appendix A.4.
Theorem 6.9 sheds light maximum entropy inference procedure.
mentioned, | precisely inference procedure based prior function Pu .
corollary asserts | invariant f precisely uniform priors X
correspond f . shows maximum entropys lack representation
independence immediate consequence identical problem uniform prior.
class F embeddings maximum entropy invariant? Clearly,
answer yes. easy see embedding takes elements X (disjoint)
sets equal cardinality correspondence property required Theorem 6.9. follows
maximum entropy invariant embeddings. fact, requirement
maximum entropy invariant subset embeddings one axioms
Shore Johnsons (1980) axiomatic characterization maximum-entropy. (We remark
Paris (1994, Theorem 7.10) proves maximum entropy satisfies variant
atomicity principle; invariance result essentially special case Theorem 6.9.)
behavior maximum entropy representation shifts, Theorem 6.9 provides solution: simply start different prior function.
want maintain invariance representation shifts, P(X) must include
non-extreme priors (i.e., measures X (A)
/ {0, 1}

/ {, X}). set priors gives essential entailment inference procedure. If,
however, prior knowledge embeddings encode reasonable representation shifts, often make smaller class priors, resulting inference
procedure prone leap conclusions. Given class reasonable embeddings F, often find prior function P closed f F, i.e.,
measure P(X) X-Y embedding f F make sure
corresponding measure P(Y ), vice versa. Thus, guarantee P
appropriate structure using process closing f F.
course, execute process reverse. Suppose want support
certain reasoning pattern requires leaping conclusions. classical example
reasoning pattern is, course, default assumption independence.
representation independence get without losing reasoning pattern?
show, Theorem 6.9 gives us answer.
begin providing one plausible formulation desired reasoning pattern.
finite space X, say X1 Xn product decomposition X X =
X1 Xn n largest number X written product way.
(It easy see X finite, maximal product decomposition unique.)
measure X product measure X X1 Xn product decomposition
X exist measures Xi = 1, . . . , n = 1 n ,
Q
is, (U1 Un ) = ni=1 (Ui ), Ui Xi , = 1, . . . , n. Let P set product
measures X. P prior relative entropy rule used update prior
given knowledge base, |P satisfies form minimal default independence.
fact, easy show satisfies following stronger property.
339

fiHalpern & Koller

Proposition 6.10: Suppose X1 Xn product decomposition X and,
= 1, . . . , n, KB constraint Xi , Si subset Xi .
n
^

KB |IP Pr(S1 . . . Sn ) =

i=1



n


Pr(Si ).

i=1

Proof: See Appendix A.4.
Theorem 4.16 shows |P cannot invariant embeddings. Theorem 6.9
tells us invariant precisely embeddings P invariant.
embeddings characterized syntactically natural way. Suppose 1 , . . . , n
partition finite set primitive propositions. Note truth assignment
primitive propositions viewed crossproduct truth assignments
primitive propositions 1 , . . . , n . identification, suppose set X truth
assignments decomposed X1 Xn , Xi consists truth assignments
. case, p j q, r k j 6= k, true |P Pr(p q) =
Pr(p) Pr(q), since since q r subset, true |P Pr(r
q) = Pr(r) Pr(q). Hence, P invariant interpretation maps p
r maps q itself. Intuitively, problem crossing subset boundaries;
mapping primitive propositions different subsets subset.
restrict interpretations thatpreserve subset boundaries, avoid problem.
get semantic characterization follows. product decomposition
X X1 Xn product decomposition Y1 Yn , f
X-Y product embedding f X-Y embedding Xi -Yi embeddings ,
= 1, . . . , n, f (hx1 , . . . , xn i) = f1 (x1 ) fn (xn ). Product embeddings capture
intuition preserving subset boundaries; elements given subset Xi remain
subset (Yi ) embedding. However, notion product embedding
somewhat restrictive; requires elements ith subset X map elements
ith component , = 1, . . . , n. still preserve default independence
components product permuted. g permutation embedding exists
permutation {1, . . . , n} g(hx1 , . . . , xn i) = hx(1) , . . . , x(n) i.
Theorem 6.11: inference procedure IP invariant faithful product embeddings
permutation embeddings.
Theorem 6.9 thus provides us basic tools easily define inference procedure
enforces minimal default independence constraints involving disjoint parts
language, time guaranteeing invariance large natural class
embeddings. Given negative result Theorem 4.16, type result best
could possibly hope for. general, Theorem 6.9 provides us principled
framework controlling tradeoff strength conclusions
reached inference procedure invariance representation shifts.

7. Related Work
mentioned earlier, two types probabilistic inference. partition
discussion related work along lines.
340

fiRepresentation Dependence

7.1 Probabilistic Inference Knowledge Base
Given importance representation reasoning, fact one main criticisms maximum entropy sensitivity representation shifts, surprising
little work problem representation dependence. Indeed,
best knowledge, work focused representation independence
logical sense considered prior Salmon Paris.
Salmon (1961) defined criterion linguistic invariance, seems essentially equivalent notion representation independence. tried use criterion defend
one particular method inductive inference but, pointed Barker commentary end (Salmon, 1961), preferred method satisfy criterion either.
Salmon (1963) attempted define modified inductive inference method would
satisfy criterion clear attempt succeeded. case, results
show modified method certainly cannot representation independent sense.
said earlier, Paris (1994) considers inference processes, given constraint
X , choose unique measure satisfying constraint. considers various properties
inference process might have. Several closely related properties
considered here. (In describing notions, made inessential
changes able express notation.)
X -inference process language invariant X, X constraints KB
X , KB |IX iff KB |IXY . Clearly language invariance
special case robustness. Paris shows center mass inference process
(that, given set X , chooses measure center mass A)
language invariant; hand, well known maximum entropy
language invariant.
X -inference process satisfies principle irrelevant information
spaces X, X , constraints KB X , constraints ,
KB |IX iff KB |IXY . Again, special case robustness, since
constraint must X-conservative. Paris shows maximum entropy
satisfies principle. (He restricts domain maximum entropy process
closed convex sets, always unique probability measure maximizes
entropy.)
X -inference process satisfies renaming principle if, whenever X
finite spaces, g : X isomorphism, f : 2X 2Y faithful embedding
based g (in f (S) = {g(s) : S}), constraints KB X ,
KB |IX iff f (KB ) |IY f (). Clearly, renaming principle special
case representation independence. Paris shows number inference processes
(including maximum entropy) satisfy renaming principle.
X -inference process satisfies principle independence if, whenever X, ,
Z X , FX , FY , U FZ , KB constraint Pr(U ) =
Pr(S|U ) = b Pr(T |U ) = c, > 0, KB | Pr(S |U ) = bc. Ignoring conditional probabilities, clearly special case minimal default
independence. Paris Vencovska (1990) show maximum entropy unique
341

fiHalpern & Koller

inference process satisfying number principles, including renaming, irrelevant
information, independence.
X -inference process satisfies atomicity principle if, X, Y1 , . . . , Yn
X , whenever f 0 embedding {0, 1} X, f obvious extension
f 0 embedding {0, 1} Y1 . . . Yn X Y1 . . . Yn ,
constraints KB {0,1}Y1 ...Yn , KB |IX iff f (KB ) |IY f ().
Clearly atomicity special case representation independence. Paris shows
inference process satisfies atomicity. argument similar spirit
used prove Theorems 4.11 4.16, much simpler, since inference
processes return unique probability measure, set them.
recently, Jaeger (1996), building definitions, examined representation
independence general nonmonotonic logics. considers representation independence
respect collection transformations, proves results degree
certain nonmonotonic formalisms, rational closure (Lehmann & Magidor,
1992), satisfy representation independence.
Another line research relevant representation independence work
abstraction (Giunchiglia & Walsh, 1992; Nayak & Levy, 1995). Although goal
work make connections two different ways representing
situation, significant differences focus. work abstraction, two ways
representing situation expected equivalent. Rather, one representation
typically abstracts away irrelevant details present other. hand,
treatment issues terms deductive entailment, terms general
inference procedures. would interesting combine two lines work.
7.2 Bayesian Probabilistic Inference
Bayesian statistics takes different perspective issues discuss paper.
discussed, Bayesian approach generally assumes construct prior, use
standard probabilistic conditioning update prior new information obtained.
approach, representation knowledge obtained effect conclusions.
Two pieces information semantically equivalent (denote event)
precisely effect used condition distribution.
paradigm, analysis directly related step precedes
probabilistic conditioningthe selection prior. specific beliefs
want encode prior distribution (as do, example, constructing
Bayesian network), design prior reflect beliefs terms vocabulary
used. example, particular distribution mind location object,
encode one way representing space terms Cartesian coordinates,
another way using polar coordinates. effect, view representation
transformation embedding f , two priors corresponding f ,
sense Definition 4.2. Thus, design prior already takes representation
account.
hand, trying construct uninformed prior class
problems, issue representation independence becomes directly relevant. Indeed,
342

fiRepresentation Dependence

standard problems maximum entropy arise even simple case
simply Bayesian conditioning starting uniform prior space.
standard approach Bayesian statistics use invariance certain transformations order define appropriate uninformed prior. example, might
want prior images invariant rotation translation. certain cases,
specify transformation want measure invariant,
measure uniquely determined (Jaynes, 1968; Kass & Wasserman, 1993). case,
argument goes, uniquely determined measure perforce right one. idea
picking prior using invariance properties spirit approach take
Section 6. Indeed, approach simply uses standard probabilistic conditioning
objective information (such observations), Bayesian approach uninformed
prior invariant set embeddings is, sense, special case. However, approach
force us choose unique prior. Rather, allow use set prior
distributions, allowing us explore wider spectrum inference procedures.
approach related work Walley (1996), observes representation independence important desideratum certain statistical applications involving
multinomial data. Walley proposes use sets Dirichlet densities encode ignorance
prior, shows approach representation independent domain
application.

8. Conclusions
paper takes first step towards understanding issue representation dependence probabilistic reasoning, defining notions invariance representation independence, showing representation independence incompatible drawing many
standard default conclusions, defining limited notions invariance might
allow compromise desiderata able draw interesting conclusions
(not already entailed evidence) representation independence. focus
inference probabilistic logic, notion representation independence
important many contexts. definitions clearly extended
non-probabilistic logics. mentioned, Jaeger (1996) obtained results representation independence general setting, clearly much
done. generally, would interest understand better tension
representation independence strength conclusions drawn
inference procedure.

Acknowledgments
Thanks Ed Perkins pointing us (Keisler & Tarski, 1964) and, particular,
result countably additive probability measure defined subalgebra algebra
F could necessarily extended countably additive probability measure F.
Thanks reviewers paper perceptive comments pointing
(Horn & Tarski, 1948). Much Halperns work paper done
IBM Almaden Research Center. recent work supported NSF
343

fiHalpern & Koller

grant IRI-96-25901 IIS-0090145 ONR grant N00014-01-1-0795.
Kollers work done U.C. Berkeley. research sponsored part Air
Force Office Scientific Research (AFSC), Contract F49620-91-C-0080,
University California Presidents Postdoctoral Fellowship. Daphne Kollers later work
paper supported generosity Powell foundation, ONR
grant N00014-96-1-0718. preliminary version appears Proceedings IJCAI 95,
pp. 18531860.

Appendix A. Proofs
A.1 Proofs Section 2
Proposition 2.5: Let | relation probabilistic constraints X
properties Reflexivity, Left Logical Equivalence, Right Weakening, Infinitary And, Consistency hold KB domain | . (That is, KB domain | ,
KB | , KB | KB , on.) | |I X-inference
procedure I.
Proof: Define follows. X , KB domain | , = [[KB ]]X
statement KB , domain I(A) = {[[]]X : KB | }.
Note Left Logical Equivalence, well defined, since = [[KB 0 ]]X ,
{[[]]X : KB | } = {[[]]X : KB 0 | }. 6= [[KB ]]X statement KB ,
domain I. remains check X-inference procedure (i.e.,
I(A) I(A) = iff = domain I), | = |I .
check X-inference procedure, suppose domain I. Thus,
= [[KB ]]X Reflexivity, easily follows I([[KB ]]X ) [[KB ]]X . Next suppose
I([[KB ]]X ) = . follows {[[]]X : KB | } = . Thus, { : KB | } |= false.
Infinitary rule, must KB |I false. Consistency Rule, follows
[[KB ]]X = . Thus, indeed X-inference procedure. Finally, note KB |
then, definition I, I([[KB ]]X ) [[]]X , KB |I . opposite inclusion, note
KB |I , { : KB | } |= . Thus, Infinitary rule, follows
KB | .
A.2 Proofs Section 3
prove Theorem 3.4, need following lemma.
Lemma A.1: Given two spaces X0 X1 , measures 0 (X0 ,FX0 ) 1 (X1 ,FX1 ) ,
subsets S0 FX0 S1 FX1 0 (S0 ) = 1 (S1 ), exists measure
2 (X0 X1 ,FX0 X1 ) 2Xi = , = 1, 2, 2 (S0 S1 ) = 1.10
Proof: B FX0 FX1 , define
2 (A B) = (0 (A S0 )1 (B S1 )/1 (S1 )) + (0 (A S0 )1 (B S1 )/1 (S1 )),
take 0 (A S0 )1 (B S1 )/1 (S1 ) = 0 1 (S1 ) = 0 take 0 (A S0 )1 (B
S1 )/1 (S1 ) = 0 1 (S1 ) = 0. Extend disjoint unions sets additivity. Since
10. B sets, use notation B denote set (A B) (A B).

344

fiRepresentation Dependence

sets FX0 X1 written disjoint unions sets form B FX0 FX1 ,
suffices define 2 . see 2 actually measure, note 2 (X ) =
0 (S0 ) + 0 (S0 ) = 1. Additivity clearly enforced definition. Finally, see
2 desired properties, suppose 1 (S1 ) 6= 0 1 (S1 ) 6= 0. (The argument
easier case; leave details reader.)
2X0 (A) = 2 (A ) = 0 (A S0 )1 (S1 )/1 (S1 ) + 0 (A S0 1 (S1 )/1 (S1 )
= 0 (A S0 ) + 0 (A S0 ) = 0 (A).
Since 0 (S0 ) = 1 (S1 ) assumption (and 0 (S0 ) = 1 (S1 )),
2X1 (B) = 2 (X B) = 0 (S0 )1 (B S1 )/1 (S1 ) + 0 (S0 )1 (B S1 )/1 S1 )
= 1 (B S1 ) + 1 (B S1 ) = 1 (B).
completes proof.
Theorem 3.4: {IX : X X } robust X -inference procedure satisfies DI1, DI2,
DI3, IX essentially entailment X X .
Proof: Suppose {IX : X X } robust IX essentially entailment X X .
must constraint KB X set FX KB |I < Pr(S) <
KB 6|= Pr(S) . Thus, must
/ [, ] KB Pr(S) =
consistent. assume without loss generality < (otherwise replace
S).
first construct space Y0 X subsets U1 , . . . , Un following properties:
(a) measure Y0 (Ui ) > , = 1, ..., n.
(b) i, measure 0i Y0 0i (Ui ) = 1 0i (Uj ) >
j 6= i.
proceed follows. Choose n < (d 1)/(n 1) < d/n < .
assumption, exists Y0 X |Y0 | = n!/(n d)!. Without loss generality,
assume Y0 consists tuples form (a1 , . . . , ad ), ai
distinct, 1 n. Let Ui consist tuples Y0 somewhere
subscript; easy see d(n 1)!/(n d)! tuples. Suppose
probability measure Y0 . easy see (U1 ) + + (Un ) = d, since
tuple Y0 exactly Ui gets counted exactly times, sum
probabilities tuples 1. Thus, cannot (Ui ) > d/n (and, fortiori,
cannot (Ui ) > i). takes care first requirement. Next, consider
probability distribution 0i makes tuples making Ui equally probable,
gives tuples probability 0. easy see 0i (Ui ) = 1. Moreover,
since straightforward check exactly d(d 1)(n 2)!/(n d)! tuples
Ui Uj j 6= i, 0i (Uj ) = [d(d 1)(n 2)!/(n d)!]/[d(n 1)!/(n d)!] =
(d 1)/(n 1). takes care second requirement.
assumption, measurable space X |Y | = 2. Suppose
= {y, 0 }. Let Z = X n Y0 n , n n chosen
construction Y0 . Again, assumption, Z X . = 1, . . . , n,
345

fiHalpern & Koller

X, let Ai = X i1 X ni Y0 n Z.
let KB = { Z : Xi KB };
let Yi subset n ith copy replaced {y};
let Vi subset Z form X n Ui Yi (where U1 , . . . , Un subsets
Yi constructed above).
Let following constraint Z :
KB 1 . . . KB n Pr(S1 V1 ) = 1 . . . Pr(Sn Vn ) = 1.
Let Xi denote ith copy X Z. is, ease exposition, view Z
form X1 Xn Y0 , although Xi identical, since
helpful able refer specific Xi . claim Xi -conservative KB ,
= 1, . . . , n. Thus, must show proj Xi ([[KB ]]Z ) = [[KB ]]X . immediate
proj Xi ([[KB ]]Z ) [[KB ]]X . opposite inclusion, suppose [[KB ]]X .
must show exists [[KB ]]Z Xi = . proceed
follows.
Let 00 measure Y0 00 (Ui ) = 1 00 (Uj ) > , j 6= i.
construction Uj s, measure must exist. j {1, . . . , n}, let 0j measure
0i (y) = (S) j 6= i, 0j (y) = /00 (Uj ) (and 0j (y 0 ) = 1 0j (y)).
Let 0 measure Y0 n crossproduct 00 , 01 , . . . , 0n . is,
0 (T0 Tn ) = 00 (T0 ) 0n (Tn ). construction, 0 (Vj ) = j 6=
0 (Vi ) = (S).
assumption, measure 0 X 0 |= KB Pr(S) = .
proceed inductively define measure k X k Y0 n (a) Pr((S1
V1 ) . . . (Sk Vk )) = 1, (b) jY = 0 jXj = j = 1, . . . , k. define 0 = 0 .
inductive step, simply apply Lemma A.1. Finally, take n .
construction guarantees X j = , hence |= KB j . addition, construction
guarantees |= Pr(S1 V1 ) = 1 . . . Pr(Sn Vn ) = 1. Hence |= , desired.
follows DI1, DI2, DI3 domain IZ . Since KB
equivalent , follows KB domain IZ . Now, robustness,
constraint Xi , KB |I iff KB |I . Since KB |I Pr(Si ) >
KB equivalent , follows |I Pr(Si ) > = 1, . . . , n. rule
(Proposition 2.3), follows |I Pr(S1 ) > . . . Pr(Sn ) > . Since |= Pr((S1
V1 ) (Sn Vn )) = 1, easily follows |I Pr(U1 ) > . . . Pr(Un ) > .
construction guarantees Pr(U1 ) > . . .Pr(Un ) > inconsistent. Thus, |I false.
robustness, follows KB |I false. happen KB |= false,
implies KB |= Pr(S) , contradicting original assumption.
A.3 Proofs Section 4
prove Lemma 4.6, useful first prove two additional results:
Lemma A.2: f X-Y embedding, f (X) = f () = .
346

fiRepresentation Dependence

Proof: Suppose f X-Y embedding. first show f () = .
definition embedding, follows f () = f (X ) = f (X) f (). Thus, f () f (X).
definition embedding implies f () = f (X) = f (X). Thus,
f (X) f (X). happen f (X) = f () = f (X) = .

Lemma A.3: f faithful X-Y embedding,
(a) X , measure corresponds ;
(b) , measure X corresponds .
Proof: prove (a), consider algebra subsets form f (S), FX .
Define function 0 algebra via 0 (f (S)) = (S). mapping well defined,
f (S) = f (T ), faithfulness guarantees = . Moreover, 0 probability
measure algebra. see this, note Lemma A.2 0 (Y ) = 0 (f (X)) = (X) = 1.
Moreover, f (S) f (T ) = , (by definition embedding) f (S ) = so, since
f faithful, = (for otherwise f (S ) = f () Lemma A.2, 6= ).
Thus,
0 (f (S) f (T )) = 0 (f (S )) = (S ) = (S) + (T ) = 0 (f (S)) + 0 (f (T )).
shown Horn Tarski (1948), possible extend 0 probability measure
FY .11 construction, corresponds .
prove (b), use similar process. Define function algebra sets
X via (S) = (f (S)). easy see already probability measure X ,
construction corresponds .
prove Lemma 4.6.
Lemma 4.6: X-Y embedding f faithful constraints KB ,
KB |= iff f (KB ) |= f ().
Proof: Suppose f faithful. show KB |= iff f (KB ) |= f (), must
show [[KB ]]X [[]]X iff [[f (KB )]]Y [[f (]]Y . direction immediate
definition f . prove direction, suppose not. must exist
[[KB ]]X [[]]X f () [[f ()]]Y . Let probability measure
corresponds . Since f () f (), must 0 [[]]X
f (0 ). Since 0 6= , must FX 0 (S) 6= (S). Since
f () f (0 ), must (f (S)) = (S) (f (S)) = 0 (S).
contradiction. completes proof direction.
converse, suppose KB |= iff f (KB ) |= f () KB . Given
S, FX , following chain equivalences:
11. critical result working finitely additive measures. may
countably additive measure extending 0 , even 0 countably additive. example, take FY0
Borel sets [0, 1] take FY subsets [0, 1]. Let 0 Lebesgue measure. known
that, continuum hypothesis, countably additive measure extending 0 defined
subsets [0, 1] (Ulam, 1930) (see (Keisler & Tarski, 1964) discussion).

347

fiHalpern & Koller

ST
iff Pr(S) = 1 |= Pr(T ) = 1
iff f (Pr(S) = 1) |= f (Pr(T ) = 1) (by assumption)
iff Pr(f (S)) = 1 |= Pr(f (T )) = 1 (by definition f )
iff f (S) f (T ).
Thus, f faithful.
Proposition 4.7: Let f faithful X-Y embedding. following statements
equivalent:
(a) correspond f ;
(b) formulas , |= iff |= f ().
Proof: first show (a) implies (b). suppose correspond f .
direction (b) trivial: |= f () f (), since f faithful.
direction, proceed much proof Lemma 4.6. Assume |= f ()
6|= . Since f (), definition f must 0 [[]]X
f (0 ). Since 0 |= whereas 6|= , must 6= 0 . Hence, must
FX (S) 6= 0 (S). Since f () f (0 ), follows (f (S)) = (S)
(f (S)) = 0 (S), gives desired contradiction.
show (b) implies (a). Assume contradiction
correspond f . must event FX (S) 6= (f (S)).
Let p = (S) let constraint Pr(S) = p. |= , whereas 6|= f (),
providing desired contradiction.
Theorem 4.10: X -inference procedure robust satisfies DI2, DI4, DI5,
representation independent.
Proof: Suppose {IX : X X } robust X -inference procedure. want show
representation independent. suppose KB , constraints X f
X-Y embedding, X, X . want show KB |IX iff f (KB ) |IY f ().
Let following constraint XY :
( f ()) (KB f (KB )).
claim X-conservative KB -conservative f (KB ). Thus, must
show proj X ([[KB ]]XY ) = [[KB ]]X proj ([[f (KB ) ]]XY ) = [[f (KB )]]Y .
show proj X ([[KB ]]XY ) = [[KB ]]X here; argument proj ([[f (KB )
]]XY ) = [[f (KB )]]Y almost identical.
Clearly [[KB ]]XY X [[KB ]]X , proj X ([[KB ]]XY ) [[KB ]]X .
opposite inclusion, suppose [[KB ]]X . want find measure 0
0 = . Let 00 measure f () let 0
[[KB ]]XY ) X
XY
0 = .
crossproduct 00 ; is, 0 (A B) = (A) 00 (B). Clearly X
see 0 [[KB ]]XY ), clearly suffices show 0 |= . since 00
correspond f , immediate Proposition 4.7 |= KB iff 00 |= f (KB )
|= iff 00 |= f (). Thus, |= , desired.
348

fiRepresentation Dependence

suppose KB |IX . DI2 DI5, KB domain IXY .
robustness, KB |IXY . Thus, I([[KB ]]XY ) [[]]XY . Since I([[KB ]]XY )
[[KB ]]XY [[ f ()]]XY , follows I([[KB ]]XY ) [[f ()]]XY .
Moreover, KB equivalent f (KB ) , I([[f (KB ) ]]XY ) [[f ()]]XY ,
i.e., f (KB ) |IXY f (). DI4, f (KB ) domain IY . Since conservative f (KB ), robustness {IX : X X } implies f (KB ) |IY f ().
opposite implication (if f (KB ) |IY f () KB |IX ) goes way. Thus,
{IX : X X } representation independent.
Next, turn attention Theorems 4.11 4.16. results follow
relatively straightforward way one key proposition. state it, need
definitions.
Definition A.4: say constraint KB X depends S1 , . . . , Sk FX
(the sets S1 , . . . , Sk necessarily disjoint) if, whenever , 0 X agree S1 , . . . , Sk ,
|= KB iff 0 |= KB .
example, KB form Pr(S1 ) > 1/3 Pr(S2 ) 3/4, KB depends
S1 S2 . Similarly, KB form Pr(S1 | S2 ) > 3/4, KB depends
S1 S2 .
Definition A.5: Given S1 , . . . , Sk FX , atom S1 , . . . , Sk set form
T1 . . . Tk , Ti either Si Si .
Proposition A.6: Suppose {IX : X X } X -inference procedure and,
X X , exist S, S1 , . . . , SK FX consistent constraint KB X depends
S1 , . . . , Sk , following two conditions satisfied:
nonempty every nonempty atom S1 , . . . , Sk ,
KB |IX < Pr(S) < , either > 0 < 1.
{IX : X X } representation independent.
Proof: Suppose, way contradiction, {IX : X X } representation-independent
inference procedure nevertheless, X X , exists sets S, S1 , . . . , Sk FX
knowledge base KB satisfies conditions above, , . Assume
> 0 (a similar argument used deal case < 1).
Let T1 , . . . , TM nonempty atoms S1 , . . . , Sk . Choose N 1/N < .
goal find collection f1 , . . . , fN embeddings X X
embeddings effect KB , sets fj (S) disjoint.
Since KB |IX Pr(fj (S)) > j = 1, . . . , N , fj (KB ) = f (KB ) j = 1, . . . , N ,
follow f (KB ) |IY Pr(fj (S)) > j = 1, . . . , N , contradiction. proceed
follows.
assumption, exists set Z X |Z| = N . Let = X Z.
Since X closed crossproducts, X . Suppose Z = {z1 , . . . , zM N }, let
Zi = {zN (i1)+1 , . . . , zN }, = 1, . . . , . Thus, Zi partition Z disjoint sets,
cardinality N . Let Bi = X Zi , let Bij = X {zN (i1)+j }, j = 1, . . . , N .
easy see find faithful X-Y embeddings f1 , . . . , fN
349

fiHalpern & Koller

1. fj (Ti ) = Bi , = 1, . . . , , j = 1, . . . , N ,
2. fj (Ti S) = Bij , = 1, . . . , , j = 1, . . . , N .
Notice need assumption Ti Ti nonempty T1 , . . . , TM
(that is, nonempty atom S1 , . . . , Sk ) guarantee find faithful
embeddings. Ti = , since fj embedding, f (Ti S) = 6= Bi ;
Ti = , fj (Ti S) = fj (Ti ) f (Ti S)) = , means Bi = Bij ,
inconsistent construction.
easy check that, since KB depends S1 , . . . , Sk , fj (KB ) depends
fj (S1 ), . . . , fj (Sk ), j = 1, . . . , N . next show fj (Si ) independent j;
is, fj (Si ) = fj 0 (Si ) 1 j, j 0 N . Notice h = 1, . . . , k,
fj (Sh ) = Ti Sh fj (Ti ) = {i:Ti Sh } Bi . Thus, fj (Sh ) independent j, desired. Since
fj (KB ) depends fj (S1 ), . . . , fj (Sk ), must independent j. Let KB
f1 (KB ) (which, observed, identical f2 (KB ), . . . , fk (KB )).
Since, assumption, {IX : X X } representation independent, KB |IX P r(S) >
, KB |IY Pr(fj (S)) > , j = 1, . . . , N . Thus, KB |IY Pr(f1 (S)) >
. . . Pr(fN (S)) > . note that, construction, fj (S) = {i:Ti S6=} Bij . Thus,
sets fj (S) pairwise disjoint. Since > 1/N , cannot N disjoint sets
probability greater . Thus, KB |IY false. KB consistent, KB = fj (KB )
must well. Thus, IY (KB ) 6= , assumption. contradicts conclusion
KB |IY false. Thus, {IX : X X } cannot representation independent.
use Proposition A.6 help prove Theorem 4.11.
Theorem 4.11: {IX : X X } representation-independent X -inference procedure
then, X X , IX essentially entailment objective knowledge bases
domain.
Proof: Suppose, way contradiction, {IX : X X } representation independent
IX essentially entailment X X objective knowledge base KB .
must set FX KB |IX < Pr(S) < KB 6|=
Pr(S) . Without loss generality, assume KB form Pr(T ) = 1
FX . Moreover, assume 6= , nonempty, measurable
strict subset. (For otherwise, choose = {y, 0 } X consider space X 0 = X .
assumption, X 0 X . Let f X-Y embedding maps U FX U . Since
representation independent, Pr(T ) = 1 |I < Pr(S ) < ,
{y} .)
nonempty, let Z nonempty, measurable strict subset (which exists
assumption); otherwise let Z empty set. Let U set (T S) (T Z). Notice
= U . Moreover, since, set V , Pr(T ) = 1 Pr(V ) = Pr(V ) valid,
follows Reflexivity Right Weakening KB |IX Pr(V ) = Pr(V ). Thus,
KB |IX Pr(S) = Pr(S ) = Pr(U ) = Pr(U ). follows KB |IX < Pr(U ) < .
want apply Proposition A.6. Note KB depends . Thus,
must show U U nonempty, nonempty, U
U well. observed above, U = S. Thus, U = , S,
contradicting assumption KB |I Pr(S) > 0. easy see U = S.
Again, cannot U = , S, contradicting assumption
350

fiRepresentation Dependence

KB |I Pr(S) < 1. construction, U = Z = Z. assumption, 6= ,
Z 6= . Finally, U = Z; again, construction, nonempty set 6= .
follows Proposition A.6 {IX : X X } representation independent.
Corollary 4.12: {IX : X X } representation-independent X -inference procedure,
X X , KB objective knowledge base putting constraints X ,
KB |IX < Pr(S) < 0 1, = 0 = 1.
Proof: Assume hypotheses corollary hold. Since KB objective,
form Pr(T ) = 1 FX . three possibilities. Either (1) S, (2)
S, (3) nonempty. (1) holds, KB |= Pr(S) = 1,
(2) holds, KB |= Pr(S) = 0. Thus, (1) (2) incompatible
KB |IX < Pr(S) < . hand, (3) holds, easy see
, Pr(S) = consistent KB (since probability measure assigns
probability probability 1 S). Since KB |IX < Pr(S) < ,
Theorem 4.11, must KB |= Pr(S) . follows choices
true = 0 = 1.
Theorem 4.16: {IX : X X } X -inference procedure enforces minimal
default independence satisfies DI1, IX representation independent.
Proof: Suppose {IX : X X } X -inference procedure enforces minimal
default independence satisfies DI1. Choose X = {x, x0 } X let KB 1/3
Pr(x) 2/3. assumption, X X X . view KB constraint XX ;
case, interpreted 1/3 Pr({x} X) 2/3. DI1, KB
domain IXX . Note KB equivalent constraint 1/3 Pr(x0 ) 2/3.
minimal default independence, KB |IXX Pr((x, x)) > Pr(x X)/3
KB |IXX Pr((x0 , x0 )) > Pr(x0 X)/3. Applying straightforward probabilistic reasoning,
get KB |IXX Pr({(x, x), (x0 , x0 )}) > 1/3. apply Proposition A.6, taking
{(x, x), (x0 , x0 )} 0 {(x, x), (x, x0 )}. Note KB depends 0 .
almost immediate definition 0 0 , 0 , 0 ,
0 nonempty. Thus, Proposition A.6, {IX : X X } representation
independent.
0 : X X } representationLemma 4.13: Let X consist countable sets. {IX
independent X -inference procedure.

Proof: said main part text, easily follows Proposition 2.5
0 inference procedure X X , since easily seen five propIX
erties described proposition. see 0 representation independent, suppose
f faithful X-Y embedding, X, X . Clearly KB objective
f (KB ) objective. KB objective, easy see KB |I 0 iff
f (KB ) |I 0 f (), since |I 0 reduces entailment case. suppose KB objective form Pr(T ) = 1, FX . KB |I 0 iff KB KB + |= .
Lemma 4.6, holds iff f (KB )f (KB + ) |= f (). hand, f (KB ) |I 0 f ()
iff f (KB ) (f (KB ))+ |= f () Thus, suffices show f (KB ) f (KB + ) |= f ()
iff f (KB ) (f (KB ))+ |= f (). easy show (f (KB ))+ implies f (KB + ),
f (KB ) f (KB + ) |= f () f (KB ) (f (KB ))+ |= f (). necessarily
351

fiHalpern & Koller

case f (KB + ) implies (f (KB ))+ . example, consider embedding described
Example 4.3. case, KB objective knowledge base Pr(colorful ) = 1,
KB + empty, hence f (KB + ), (f (KB ))+ includes constraints
0 < Pr(green) < 1. Nevertheless, suppose f (KB ) (f (KB ))+ |= f () and, way
contradiction, |= f (KB ) f (KB + ) f (). Choose
f (). correspond, |= KB KB + . easy
show exists 0 f () 0 < 0 (S) < 1 nonempty subsets
f (T ). see this, note (x) 6= 0, suffices ensure 0 (f (x)) = (x)
0 (y) 6= 0 f (x). Since countable, straightforward. Since 0
correspond, must 0 |= f () f (KB ). construction, 0 |= (f (KB ))+ .
contradicts assumption f (KB ) (f (KB ))+ |= f ().
Lemma 4.14: Suppose X consists measure spaces form (X, 2X ),
1 : X X } representation-independent X -inference procedure.
X finite. {IX
Proof: Suppose X, X , KB constraints X , f X-Y
embedding. must show KB |I 1 iff f (KB ) |I 1 f (). purposes
X

proof, say subset X interesting exists FX
= { X : (S) 1/4}. easy see KB interesting f (KB )
interesting. converse true, given assumption X consists finite
spaces sets measurable. suppose f (KB ) interesting.
set f (KB ) = { : (T ) 1/4}. Let = {S 0 X : f (S 0 ) }.
Since X finite, A; easily follows = A.12 Clearly (S) 1/4,
f () f (KB ), [[KB ]]X . Thus, [[KB ]]X { X : (S) 1/4}.
hand, KB , f () f (KB ). Thus, f (), since A, must
case (S) = (f (S)) (T ) 1/4. Thus, [[KB ]]X { X : (S) 1/4}.
follows KB equivalent Pr(S) 1/4, must interesting. (We must
= f (S), although needed result.)
KB interesting, KB |I 1 iff KB |= iff f (KB ) |= f () (since entailment
X
representation independent) iff f (KB ) |I 1 . hand, KB interesting,

KB equivalent Pr(S) 1/4 X, f (KB ) equivalent
Pr(f (S)) 1/4. Moreover, KB |I 1 iff Pr(S) 1/3 |= iff Pr(f (S)) 1/3 |= f () iff
X
f (KB ) |I 1 . Thus, get representation independence, desired.


A.4 Proofs Section 6
Proposition 6.3: Suppose f faithful X-Y embedding, DX X , DY .
following two conditions equivalent:
(a) DX DY correspond f ;
(b) , DX |= iff DY |= f ().
12. general true X infinite without additional requirement f (i Ai ) = f (Ai )
arbitrary unions.

352

fiRepresentation Dependence

Proof: prove (a) implies (b), assume way contradiction that, ,
DX |= DY 6|= f (). DY 6|= f (). Let DX
measure corresponding . Then, Proposition 4.7, 6|= , desired
contradiction. proof direction (a) identical.
prove (b) implies (a), first consider measure DX . must find DY
corresponds . Suppose X = {x1 , . . . , xn } (recall restricting
finite spaces Section 6) (xi ) = ai , = 1, . . . , n. Let constraint
ni=1 Pr({xi }) = ai . assumptions language, constraint
language. Clearly [[]]X = {}. Since DX , know DX 6|= . Hence, DY 6|=
f (), exists DY 6 f (). Hence f () = f ({}).
definition f , corresponds .
consider measure DY , let measure X corresponds
. Assume way contradiction 6 DX . Taking above, follows
DX |= and, therefore, assumption, DY |= f (). Thus, |= f (). |= and,
assumption, correspond. contradicts Proposition 4.7.
Theorem 6.7: Let arbitrary constraint X . f faithful X-Y embedding
correspond f , | |f () correspond f .
Proof: Assume correspond f . Recall assuming section
X finite space; let X = {x1 , . . . , xn }. Let Yi = f (xi ). Given distribution
00 , define i00 = 00 |Yi let (f )1 ( 00 ) denote unique 00 X
00 f (00 ).
suppose 0 |. Define 0 measure
0 (y) = 0 (xi ) (y),
index Yi . Since = |Yi , follows (Yi ) = 1. Thus,
0 (Yi ) = (xi ), 0 leaves relative probabilities elements within Yi
. easy verify 0 0 correspond. Hence, Proposition 4.7, 0 |= f ().
claim 0 |f (). show that, need show KLY ( 0 k) minimal
among KLY ( 00 k) 00 |= f (). follows standard properties relative
entropy (Cover & Thomas, 1991, Theorem 2.5.3) 00 ,
KLY ( 00 k) = KLX ((f )1 ( 00 )k(f )1 ()) +

n
X

KLY (i00 ki ).

(1)

i=1

Note = i0 , KLY (i0 ki ) = 0, = 1, . . . , n. Thus, follows (1)
KLY ( 0 k) = KLX (0 k).
Now, let 00 00 |= f () let 00 = (f )1 (00 ). Since 00 00
correspond f , follows Proposition 4.7 00 |= . Using (1) again,

KLY ( 00 k) = KLX (00 k) +

n
X
i=1

KLX (00 k).
353

KLY (i00 ki )

fiHalpern & Koller

since 0 |, know KLX (0 k) KLX (00 k). Hence conclude
KLY ( 00 k) KLY ( 0 k),
0 |f ().
Theorem 6.9: f faithful X-Y embedding, P invariant f iff P(X)
P(Y ) correspond f .
Proof: Suppose f faithful X-Y embedding. definition, P invariant
f iff, KB , ,
KB | P iff f (KB ) | P f ().

(2)

definition P , (2) holds iff
P(X)|KB [[]]X iff P(Y )|f (KB ) [[f ()]]Y KB , .

(3)

Proposition 6.3, (3) holds iff P(X)|KB P(Y )|f (KB ) correspond KB .
Corollary 6.5, P(X) P(Y ) correspond, P(X)|KB P(Y )|f (KB ) correspond
KB . hand, P(X)|KB P(Y )|f (KB ) correspond KB ,
P(X) P(Y ) must correspond: simply take KB = true observe P(X)|KB ) =
P(X) P(Y )|f (KB ) = P(Y ).
Proposition 6.10: Suppose X1 Xn product decomposition X and,
= 1, . . . , n, KB constraint Xi , Si subset Xi .
n
^
i=1

KB |IP Pr(S1 . . . Sn ) =


n


Pr(Si ).

i=1

Proof: KB satisfiable constraint Xi , = 1, . . . , n, exist product
V
measures X satisfying constraints ni=1 KB . product measures precisely
Vn
measures P |( i=1 KB ). Since measures satisfies Pr(S1 . . . Sn ) =
Qn
i=1 Pr(Si ) assumption, conclusion holds case. constraint KB
satisfiable, result trivially holds.
Theorem 6.11: inference procedure IP invariant faithful product embeddings permutation embeddings.
Proof: Suppose f faithful X-Y product embedding, X1 Xn product
decomposition X, Y1 Yn product decomposition . show
P invariant f , suffices show P (X) P (Y ) correspond f .
Supposethat P (Y ). = 1 n , measure Xi , = 1, . . . , n.
Moreover, since f product embedding, exist f1 , . . . , fn f = f1 fn .
Let (i ), = 1, . . . , n. easy check = 1 n f ().
Conversely, suppose P (Y ). = 1 n , Yi
= 1, . . . , n. Define Xi setting (S) = (fi (S)). Since faithful Xi -Yi
354

fiRepresentation Dependence

embedding, easy check Xi (i ). Thus, f ().
completes proof P invariant faithful X-Y product embeddings.
argument P invariant faithful X-X permutation embeddings
similar (and easier). leave details reader.

References
Bacchus, F. (1990). Representing Reasoning Probabilistic Knowledge. MIT Press,
Cambridge, Mass.
Bacchus, F., Grove, A. J., Halpern, J. Y., & Koller, D. (1996). statistical knowledge
bases degrees belief. Artificial Intelligence, 87 (12), 75143.
Cover, T. M., & Thomas, J. A. (1991). Elements Information Theory. Wiley, New York.
Enderton, H. B. (1972). Mathematical Introduction Logic. Academic Press, New York.
Giunchiglia, F., & Walsh, T. (1992). theory abstraction. Artificial Intelligence, 56 (23),
323390.
Goldszmidt, M., Morris, P., & Pearl, J. (1993). maximum entropy approach nonmonotonic reasoning. IEEE Transactions Pattern Analysis Machine Intelligence,
15 (3), 220232.
Halpern, J. Y., & Koller, D. (1995). Representation dependence probabilistic inference.
Proc. Fourteenth International Joint Conference Artificial Intelligence (IJCAI
95), pp. 18531860.
Horn, A., & Tarski, A. (1948). Measures Boolean algebras. Transactions AMS,
64 (1), 467497.
Jaeger, M. (1996). Representation independence nonmonotonic inference relations.
Principles Knowledge Representation Reasoning: Proc. Fifth International
Conference (KR 96), pp. 461472.
Jaynes, E. T. (1968). Prior probabilities. IEEE Transactions Systems Science
Cybernetics, SSC-4, 227241.
Jaynes, E. T. (1978). stand maximum entropy?. Levine, R. D., & Tribus,
M. (Eds.), Maximum Entropy Formalism, pp. 15118. MIT Press, Cambridge,
Mass.
Kahneman, D., Slovic, P., & Tversky, A. (Eds.). (1982). Judgment Uncertainty:
Heuristics Biases. Cambridge University Press, Cambridge/New York.
Kass, R. E., & Wasserman, L. (1993). Formal rules selecting prior distributions: review
annotated bibliography. Tech. rep. Technical Report #583, Dept. Statistics,
Carnegie Mellon University.
355

fiHalpern & Koller

Keisler, J., & Tarski, A. (1964). accessible inaccessible cardinals. Fundamenta
Mathematica, 53, 225308.
Kraus, S., Lehmann, D., & Magidor, M. (1990). Nonmonotonic reasoning, preferential
models cumulative logics. Artificial Intelligence, 44, 167207.
Kullback, S., & Leibler, R. A. (1951). information sufficiency. Annals Mathematical Statistics, 22, 7686.
Lehmann, D., & Magidor, M. (1992). conditional knowledge base entail?.
Artificial Intelligence, 55, 160.
Nayak, P. P., & Levy, A. Y. (1995). semantic theory abstractions. Proc. Fourteenth
International Joint Conference Artificial Intelligence (IJCAI 95), pp. 196203.
Paris, J. B. (1994). Uncertain Reasoners Companion. Cambridge University Press,
Cambridge, U.K.
Paris, J., & Vencovska, A. (1990). note inevitability maximum entropy. International Journal Approximate Reasoning, 4 (3), 183224.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems. Morgan Kaufmann, San
Francisco.
Salmon, W. (1961). Vindication induction. Feigl, H., & Maxwell, G. (Eds.), Current
Issues Philosophy Science, pp. 245264. Holt, Rinehart, Winston, New
York.
Salmon, W. (1963). vindicating induction. Kyburg, H. E., & Nagel, E. (Eds.),
Induction: Current Issues, pp. 2754. Wesleyan University Press, Middletown,
Conn.
Seidenfeld, T. (1987). Entropy uncertainty. MacNeill, I. B., & Umphrey, G. J. (Eds.),
Foundations Statistical Inferences, pp. 259287. Reidel, Dordrecht, Netherlands.
Shore, J. E., & Johnson, R. W. (1980). Axiomatic derivation principle maximum entropy principle minimimum cross-entropy. IEEE Transactions
Information Theory, IT-26 (1), 2637.
Ulam, S. (1930). Zur masstheorie der allgemeinen mengenlehre. Fundamenta Mathematicae, 16, 140150.
Walley, P. (1996). Inferences multinomial data: learning bag marbles.
Journal Royal Statistical Society, Series B, 58 (1), 334. Discussion
paper various commentators appears pp. 3457.

356


