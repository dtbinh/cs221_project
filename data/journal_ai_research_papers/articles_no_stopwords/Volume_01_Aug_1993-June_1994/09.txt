Journal Artificial Intelligence Research 1 (1994) 231-255

Submitted 12/93; published 2/94

Substructure Discovery Using Minimum Description
Length Background Knowledge
Diane J. Cook
Lawrence B. Holder

Department Computer Science Engineering
Box 19015
University Texas Arlington
Arlington, TX 76019 USA

cook@cse.uta.edu
holder@cse.uta.edu

Abstract

ability identify interesting repetitive substructures essential component discovering knowledge structural data. describe new version Subdue substructure discovery system based minimum description length principle.
Subdue system discovers substructures compress original data represent
structural concepts data. replacing previously-discovered substructures
data, multiple passes Subdue produce hierarchical description structural regularities data. Subdue uses computationally-bounded inexact graph match
identifies similar, identical, instances substructure finds approximate
measure closeness two substructures computational constraints. addition minimum description length principle, background knowledge used
Subdue guide search towards appropriate substructures. Experiments
variety domains demonstrate Subdue's ability find substructures capable compressing original data discover structural concepts important domain.

1. Introduction
large amount data collected today quickly overwhelming researchers' abilities
interpret data discover interesting patterns within data. response
problem, number researchers developed techniques discovering concepts
databases. techniques work well data expressed non-structural, attributevalue representation, address issues data relevance, missing data, noise uncertainty, utilization domain knowledge. However, recent data acquisition projects
collecting structural data describing relationships among data objects. Correspondingly, exists need techniques analyze discover concepts structural
databases.
One method discovering knowledge structural data identification common substructures within data. motivation process find substructures
capable compressing data identify conceptually interesting substructures
enhance interpretation data. Substructure discovery process identifying
concepts describing interesting repetitive substructures within structural data.
discovered, substructure concept used simplify data replacing instances
substructure pointer newly discovered concept. discovered substructure concepts allow abstraction detailed structure original data provide
c 1994 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiCook & Holder
new, relevant attributes interpreting data. Iteration substructure discovery
replacement process constructs hierarchical description structural data terms
discovered substructures. hierarchy provides varying levels interpretation
accessed based goals data analysis.
describe system called Subdue (Holder, Cook, & Bunke, 1992; Holder & Cook,
1993) discovers interesting substructures structural data based minimum
description length principle. Subdue system discovers substructures compress
original data represent structural concepts data. replacing previouslydiscovered substructures data, multiple passes Subdue produce hierarchical description structural regularities data. Subdue uses computationally-bounded
inexact graph match identifies similar, identical, instances substructure
finds approximate measure closeness two substructures computational
constraints. addition minimum description length principle, background
knowledge used Subdue guide search towards appropriate substructures.
following sections describe approach detail. Section 2 describes process
substructure discovery introduces needed definitions. Section 3 compares Subdue
discovery system work found literature. Section 4 introduces minimum
description length encoding used approach, Section 5 presents inexact
graph match algorithm employed Subdue. Section 6 describes methods incorporating
background knowledge substructure discovery process. experiments detailed
Section 7 demonstrate Subdue's ability find substructures compress data
re-discover known concepts variety domains. Section 8 details hierarchical
discovery process. conclude observations directions future research.

2. Substructure Discovery
substructure discovery system represents structured data labeled graph. Objects
data map vertices small subgraphs graph, relationships
objects map directed undirected edges graph. substructure connected
subgraph within graphical representation. graphical representation serves input
substructure discovery system. Figure 1 shows geometric example input
graph. objects figure (e.g., T1, S1, R1) become labeled vertices graph,
relationships (e.g., on(T1,S1), shape(C1,circle)) become labeled edges graph.
graphical representation substructure discovered Subdue data
shown Figure 1.
instance substructure input graph set vertices edges
input graph match, graph theoretically, graphical representation
substructure. example, instances substructure Figure 1 shown
Figure 2.
substructure discovery algorithm used Subdue computationally-constrained
beam search. algorithm begins substructure matching single vertex
graph. iteration algorithm selects best substructure expands
instances substructure one neighboring edge possible ways. new unique
generated substructures become candidates expansion. algorithm searches
232

fiSubstructure Discovery

Input Graph

Substructure

T1

pe

S1

triangle


sh

C1
R1

T2

T3

S2

S3



T4

pe

square


sh

S4

Figure 1: Example substructure graph form.
Instance

1

Instance

2

Instance

3

Instance

T1

T2

T3

T4

S1

S2

S3

S4

4

Figure 2: Instances substructure.
best substructure possible substructures considered total
amount computation exceeds given limit. evaluation substructure guided
MDL principle background knowledge provided user.
Typically, description length expanding substructure begins increase,
expansion substructure yield smaller description length.
result, Subdue makes use optional pruning mechanism eliminates substructure
expansions consideration description lengths expansions increases.

3. Related Work
Several approaches substructure discovery developed. Winston's Arch program (Winston, 1975) discovers substructures order deepen hierarchical description
scene group objects general concepts. Arch program searches
two types substructure blocks-world domain. first type involves sequence
objects connected chain similar relations. second type involves set
objects similar relationship \grouping" object. main difference
substructure discovery procedures used Arch program Subdue
Arch program designed specifically blocks-world domain. instance,
sequence discovery method looks supported-by in-front-of relations only.
Subdue's substructure discovery method domain independent, although inclusion
domain-specific knowledge would improve Subdue's performance.
Motivated need construct knowledge base chemical structures, Levinson
(Levinson, 1984) developed system storing labeled graphs individual graphs
233

fiCook & Holder
represented set vertices universal graph. addition, individual graphs
maintained partial ordering defined subgraph-of relation, improves
performance graph comparisons. universal graph representation provides
method compressing set graphs stored knowledge base. Subgraphs
universal graph used several individual graphs suggest common substructure
individual graphs. One difference two approaches Levinson's system
designed incrementally process smaller individual graphs; whereas, Subdue processes
larger graphs once. Also, Levinson's system discovers common substructure
indirect result universal graph construction; whereas, Subdue's main goal
discover output substructure definitions reduce minimum description
length encoding graph. Finally, subgraph-of partial ordering used Levinson's
system included Subdue, maintaining partial ordering would improve
performance graph matching procedure pruning number possible matching
graphs.
Segen (Segen, 1990) describes system storing graphs using probabilistic graph
model represent subsets graph. Alternative models evaluated based minimum description length measure information needed represent stored graphs
using model. addition, Segen's system clusters graphs classes based
minimizing description length graphs according entire clustering. Apart
probabilistic representation, Segen's approach similar Levinson's system
methods take advantage commonalities graphs assist graph storage matching. probabilistic graphs contain information identifying common
substructure exact graphs represent. portion probabilistic graph
high probability defines substructure appears frequently exact graphs.
notion emphasized Segen's work, provides alternative method
substructure discovery clustering subgraphs original input graphs. Levinson's approach, graphs processed incrementally, substructure found across several
graphs, within single graph Subdue.
Labyrinth system (Thompson & Langley, 1991) extends Cobweb incremental
conceptual clustering system (Fisher, 1987) handle structured objects. Labyrinth uses
Cobweb form hierarchical concepts individual objects domain based
primitive attributes. Concepts structured objects formed similar manner
using individual objects attributes. resulting hierarchy represents componential
model structured objects. Cobweb's concepts probabilistic, Labyrinth
produces probabilistic models structured objects, added hierarchical
organization. upper-level components structured-object hierarchy produced
Labyrinth represent substructures common examples. Therefore, although
primary focus, Labyrinth discovering substructure, constrained context
general graph representation used Subdue.
Conklin et al. (Conklin & Glasgow, 1992) developed i-mem system constructing image hierarchy, similar Labyrinth, used discovering common
substructures set images ecient retrieval images similar given image.
Images expressed terms set relations defined user. Specific general
(conceptual) images stored hierarchy based subsumption relation similar
234

fiSubstructure Discovery
Levinson's subgraph-of partial ordering. Image matching utilizes transformational
approach (similar Subdue's inexact graph match) measure image closeness.
approaches Segen Levinson, i-mem designed process individual
images. Therefore, general image concepts appear higher i-mem's hierarchy
represent common substructures across several images. Subdue designed discover
common substructures within single image. Subdue mimic individual approach
systems processing set individual images one disconnected graph.
substructures found common individual images. hierarchy represents
componential view images. view constructed Subdue using
multiple passes graph replacing portions input graph substructures
discovered previous passes. i-mem performed well simple chess domain
molecular chemistry domains (Conklin & Glasgow, 1992). However, i-mem requires
domain-specific relations expressing images order hierarchy find relevant
substructures image matching ecient. Again, maintaining concepts
(images, graphs) partially-ordered hierarchy improves eciency matching
retrieval, suggests possible improvement Subdue.
CLiP system (Yoshida, Motoda, & Indurkhya, 1993) graph-based induction
similar Subdue previous systems. CLiP iteratively discovers patterns
graphs expanding combining patterns discovered previous iterations. Patterns
grouped views based collective ability compress original input
graph. iteration CLiP uses existing views contract input graph
considers adding views new patterns consisting two vertices edge
contracted graph. compression new proposed views estimated,
best views (according given beam width) retained next iteration.
CLiP discovers substructures (patterns) differently Subdue. First, CLiP produces
set substructures collectively compress input graph; whereas, Subdue produces
single substructures evaluated using principled minimum description length.
CLiP ability grow substructures agglomeratively (i.e., merging two substructures
together); whereas, Subdue always produces new substructures using incremental growth
along one new edge. CLiP initially estimates compression value new views based
compression value parent view; whereas, Subdue performs expensive exact
measurement compression new substructure. Finally, CLiP employs ecient
graph match based graph identity, graph isomorphism Subdue. Graph identity
assumes ordering incident edges vertex consider possible
mappings looking occurrences pattern input graph. differences
CLiP suggest possible enhancements Subdue.
Research pattern recognition begun investigate use graphs graph
grammars underlying representation structural problems (Schalkoff, 1992). Many
results grammatical inference applicable constrained classes graphs (e.g., trees)
(Fu, 1982; Miclet, 1986). approach begins set sample graphs produces
generalized graph grammar capable deriving original sample graphs many others.
production rules general grammar capture regularities (substructures)
sample graphs. Jeltsch Kreowski (Jeltsch & Kreowski, 1991) describe approach
begins maximally-specific grammar iteratively identifies common subgraphs
right-hand sides production rules. common subgraphs used form
235

fiCook & Holder
new, general production rules. Although method address underlying
combinatorial nondeterminism, heuristic approaches could provide feasible method
extracting substructures form graph grammars. Furthermore, graph grammar
production-rule may provide suitable representation background knowledge
substructure discovery process.

4. Minimum Description Length Encoding Graphs

minimum description length principle (MDLP) introduced Rissanen (Rissanen,
1989) states best theory describe set data theory minimizes
description length entire data set. MDL principle used decision
tree induction (Quinlan & Rivest, 1989), image processing (Pednault, 1989; Pentland, 1989;
Leclerc, 1989), concept learning relational data (Derthick, 1991), learning models
non-homogeneous engineering domains (Rao & Lu, 1992).
demonstrate minimum description length principle used discover
substructures complex data. particular, substructure evaluated based well
compress entire dataset using minimum description length. define
minimum description length graph number bits necessary completely
describe graph.
According minimum description length (MDL) principle, theory best
accounts collection data one minimizes (S ) + (GjS ),
discovered substructure, G input graph, (S ) number bits required encode
discovered substructure, (GjS ) number bits required encode input
graph G respect .
graph connectivity represented adjacency matrix. Consider graph
n vertices, numbered 0; 1; : : :; n , 1. n n adjacency matrix
formed entry A[i; j ] set 0 1. A[i; j ] = 0, connection
vertex vertex j . A[i; j ] = 1, least one connection vertex
vertex j . Undirected edges recorded one entry matrix. adjacency
matrix graph Figure 3 shown below.
x 20 1 1 0 0 03
triangle 66 0 0 0 0 0 0 77
666 0 0 0 1 1 0 777
square 66 0 0 0 0 0 0 77
r 40 0 0 0 0 15
rectangle 0 0 0 0 0 0
encoding graph consists following steps. assume decoder
table lu unique labels original graph G.
1. Determine number bits vbits needed encode vertex labels graph.
First, need (lg v ) bits encode number vertices v graph. Then,
encoding labels v vertices requires (v lg lu ) bits. assume vertices
specified order appear adjacency matrix. total number
bits encode vertex labels
vbits = lg v + v lg lu
236

fiSubstructure Discovery

triangle

e

p
ha


x



pe

square


sh




pe

rectangle


sh
r

Figure 3: MDL example graph.
example Figure 3, v = 6, assume lu = 8 unique
labels original graph. number bits needed encode vertices
lg 6 + 6 lg 8 = 20:58 bits.
2. Determine number bits rbits needed encode rows adjacency matrix
A. Typically, large graphs, single vertex edges small percentage
vertices entire graph. Therefore, typical row adjacency matrix
much fewer v 1s, v total number vertices graph.
apply variant coding scheme used (Quinlan & Rivest, 1989) encode bit
strings length n consisting k 1s (n , k) 0s, k (n , k).
case, row (1 v ) represented bit string length v containing ki
1s. let b = maxi ki , ith row adjacency matrix encoded
follows.
(a) Encoding value ki requires lg(b + 1) bits.

(b) Given ki 1s occur row bit string length v , kvi strings
0s 1s
Since strings equal probability
possible.

v
occurrence, lg ki bits needed encode positions 1s row i.
value v known vertex encoding.
Finally, need additional lg(b + 1) bits encode number bits needed
specify value ki row. total encoding length bits adjacency
matrix

rbits = lg(b + 1) +

v
X
i=1

= (v + 1) lg(b + 1)
237



lg(b + 1) + lg kvi
v
X
i=1



lg kvi

fiCook & Holder
example Figure 3, b = 2,


number

bits
needed
encode

6
6
6
6
6
6
adjacency matrix (7 lg 3)+lg 2 +lg 0 +lg 2 +lg 0 +lg 1 +lg 0 = 21:49
bits.
3. Determine number bits ebits needed encode edges represented
entries A[i; j ] = 1 adjacency matrix A. number bits needed encode
entry A[i; j ] (lg m) + e(i; j )[1 + lg lu ], e(i; j ) actual number edges
vertex j graph = maxi;j e(i; j ). (lg m) bits needed
encode number edges vertex j , [1 + lg lu ] bits needed
per edge encode edge label whether edge directed undirected.
addition encoding edges, need encode number bits (lg m) needed
specify number edges per entry. total encoding edges

ebits = lg +

v X
v
X
i=1 j =1

lg + e(i; j )[1 + lg lu ]

= lg + e(1 + lg lu ) +

v X
v
X
i=1 j =1

A[i; j ] lg

= e(1 + lg lu ) + (K + 1) lg
e number edges graph, K number 1s adjacency
matrix A. example Figure 3, e = 5, K = 5, = 1, lu = 8, number
bits needed encode edges 5(1 + lg 8) + 6 lg 1 = 20.
total encoding graph takes (vbits + rbits + ebits) bits. example
Figure 3, value 62:07 bits.
input graph discovered substructure encoded using
scheme. substructure discovered, instance substructure input
graph replaced single vertex representing entire substructure. discovered
substructure represented (S ) bits, graph substructure replacement
represented (GjS ) bits. Subdue searches substructure graph G minimizing
(S ) + (GjS ).

5. Inexact Graph Match

Although exact structure match used find many interesting substructures, many
interesting substructures show slightly different form throughout
data. differences may due noise distortion, may illustrate slight
differences instances general class structures. Consider image
shown Figure 9. pencil cube would make ideal substructures picture,
exact match algorithm may consider strong substructures,
rarely occur form level detail throughout picture.
Given input graph set defined substructures, want find subgraphs
input graph closely resemble given substructures. Furthermore, want
associate distance measure pair graphs consisting given substructure
subgraph input graph. adopt approach inexact graph match given
Bunke Allermann (Bunke & Allermann, 1983).
238

fiSubstructure Discovery

g1

g2

b

B




B

1

2



3

4



b

b
5
B

Figure 4: Two similar graphs g1 g2 .
inexact match approach, distortion graph assigned cost. distortion
described terms basic transformations deletion, insertion, substitution
vertices edges. distortion costs determined user bias match
particular types distortions.
inexact graph match two graphs g1 g2 maps g1 g2 g2
interpreted distorted version g1. Formally, inexact graph match mapping
f : N1 ! N2 [ fg, N1 N2 sets vertices g1 g2, respectively.
vertex v 2 N1 mapped (i.e., f (v ) = ) deleted. is, corresponding
vertex g2. Given set particular distortion costs discussed above, define cost
inexact graph match cost(f ), sum cost individual transformations
resulting f , define matchcost(g1 ; g2) value least-cost function
maps graph g1 onto graph g2.
Given g1 , g2, set distortion costs, actual computation matchcost(g1 ; g2)
determined using tree search procedure. state search tree corresponds
partial match maps subset vertices g1 subset vertices g2.
Initially, start empty mapping root search tree. Expanding state
corresponds adding pair vertices, one g1 one g2, partial mapping
constructed far. final state search tree match maps vertices g1
g2 . complete search tree example Figure 4 shown Figure 5.
example assign value 1 distortion cost. numbers circles
figure represent cost state. eventually interested mapping
minimum cost, state search tree gets assigned cost partial mapping
represents. Thus goal state found tree search procedure
final state minimum cost among final states. Figure 5 conclude
minimum cost inexact graph match g1 g2 given mapping f (1) = 4, f (2) = 3.
cost mapping 4.
Given graphs g1 n vertices g2 vertices, n, complexity
full inexact graph match O(nm+1 ). routine used heavily throughout
239

fiCook & Holder

(1, 3) 1

(1, 5) 1

(1, 4) 0

(1,

)1

(2,4) (2,5) (2, ) (2,3) (2,5) (2, ) (2,3) (2,4) (2, ) (2,3) (2,4) (2,5) (2, )
7

6

10

3

6

9

7

7

10

9

10

9

11

Figure 5: Search tree computing matchcost(g1,g2) Figure 4.
discovery evaluation process, complexity algorithm significantly degrade
performance system.
improve performance inexact graph match algorithm, extend Bunke's
approach applying branch-and-bound search tree. cost root
tree given node computed described above. Nodes considered pairings
order heavily connected vertex least connected, constrains
remaining match. branch-and-bound search guarantees optimal solution,
search ends soon first complete mapping found.
addition, user place limit number search nodes considered
branch-and-bound procedure (defined function size input graphs).
number nodes expanded search tree reaches defined limit, search
resorts hill climbing using cost mapping far measure choosing
best node given level. defining limit, significant speedup realized
expense accuracy computed match cost.
Another approach inexact graph match would encode difference
two graphs using MDL principle. Smaller encodings would indicate lower match cost
two graphs. leave future research direction.

6. Guiding Discovery Process Background Knowledge
Although principle minimum description length useful discovering substructures maximize compression data, scientists may realize benefit
discovery substructures exhibit domain-specific domain-independent characteristics.
make Subdue powerful across wide variety domains, added
ability guide discovery process background knowledge. Although minimum
description length principle still drives discovery process, background knowledge
used input bias toward certain types substructures. background knowledge
encoded form rules evaluating substructures, represent domainindependent domain-dependent rules. time substructure evaluated, input
240

fiSubstructure Discovery
rules used determine value substructure consideration.
most-favored substructures kept expanded, rules bias discovery
process system.
background rule assigned positive, zero, negative weight, biases
procedure toward type substructure, eliminates use rule, biases
procedure away type substructure, respectively. value substructure
defined description length (DL) input graph using substructure multiplied weighted value background rule set rules R applied
substructure.

value(s) = DL(G; s)

jRj

r=1

ruler (s)er

(1)

Three domain-independent heuristics incorporated rules Subdue system compactness, connectivity, coverage. definitions rules,

let G represent input graph, represent substructure graph,
represent set instances substructure G. instance weight w
instance 2 substructure defined
(i; s)
(2)
w(i; s) = 1 , matchcost
size(i) ;
size(i) = #vertices(i) + #edges(i). match cost greater size
larger graph, w(i; s) = 0. instance weights used rules compute
weighted average instances substructure. value 1 added formula
exponential weights used control rule's significance.
first rule, compactness, generalization Wertheimer's Factor Closure,
states human attention drawn closed structures (Wertheimer, 1939). closed
substructure least many edges vertices, whereas non-closed substructure
fewer edges vertices (Prather, 1976). Thus, closed substructures higher
compactness value. Compactness defined weighted average ratio
number edges substructure number vertices substructure.

compactness(s) = 1 + j1I j

X

i2I

edges(i)
w(i; s) ##vertices
(i)

(3)

second rule, connectivity, measures amount external connection instances substructure. connectivity rule variant Wertheimer's Factor
Proximity (Wertheimer, 1939), related earlier numerical clustering techniques
(Zahn, 1971). works demonstrate human preference \isolated" substructures,
is, substructures minimally related adjoining structure. Connectivity measures \isolation" substructure computing inverse average number
external connections weighted instances substructure input graph.
external connection defined edge connects vertex substructure
vertex outside substructure. formula determining connectivity
substructure instances input graph G given below.
241

fiCook & Holder
"

connectivity(s) = 1 + 1

X

jI j i2I w(i; s) num external conns(i)

#,1

(4)

third rule, coverage, measures fraction structure input graph described
substructure. coverage rule motivated research inductive learning
provides concept descriptions describing input examples considered better
(Michalski & Stepp, 1983). Although MDL measures amount structure, coverage
rule includes relevance savings respect size entire input graph.
Coverage defined number unique vertices edges instances
substructure divided total number vertices edges input graph.
formula, unique structure(i) instance number vertices edges
already appeared previous instances summation.

coverage(s) = 1 +

P
i2I w(i; s) unique

size(G)

structure(i)

(5)

Domain-dependent rules used guide discovery process domain
scientists contribute expertise. example, CAD circuits generally consist
two types components, active passive components. active components
main driving components. Identifying active components first step understanding main function circuit. add knowledge Subdue include
rule assigns higher values substructures (circuit components) representing active
components lower values substructures representing passive components. Since
active components higher scores, expected selected. system
focus attention active components expanded functional
substructures.
Another method biasing discovery process background knowledge let
background rules affect prior probabilities possible substructures. However, choosing
appropriate prior probabilities express desired properties substructures dicult, indicates future direction inclusion background knowledge
substructure discovery process.

7. Experiments
experiments section evaluate Subdue's substructure discovery capability
several domains, including chemical compound analysis, scene analysis, CAD circuit design
analysis, analysis artificially-generated structural database.
Two goals substructure discovery system find substructures reduce
amount information needed describe data, find substructures
considered interesting given database. result, evaluate Subdue system
section along two criteria. First, measure amount compression
Subdue provides across variety databases. Second, use Subdue system
additional background knowledge rules re-discover substructures identified
interesting experts specific domain. Section 7.1 describes domains used
experiments, Section 7.2 presents experimental results.
242

fiSubstructure Discovery

CH 2OH


CH 3

CH 3



C


OH

Figure 6: Cortisone.
CH 3
C
CH
2

CH 3

H

C

C
CH

2

CH

CH

2
C

CH 3

2

CH 3

H

C

C
CH

CH

2

2

CH

CH

2

2

C

C
CH 3

H

H

CH

2

C
CH

2

C
H

Figure 7: Natural rubber (all-cis polyisoprene).

7.1 Domains
7.1.1 Chemical Compound Analysis

Chemical compounds rich structure. Identification common interesting
substructures benefit scientists identifying recurring components, simplying data
description, focusing substructures stand merit additional attention.
Chemical compounds represented graphically mapping individual atoms,
carbon oxygen, labeled vertices graph, mapping bonds
atoms onto labeled edges graph. Figures 6, 7, 8 show graphs representing
chemical compound databases cortisone, rubber, portion DNA molecule.
7.1.2 Scene Analysis

Images scene descriptions provide rich source structure. Images humans
encounter, natural synthesized, many structured subcomponents draw
attention help us interpret data scene.
Discovering common structures scenes useful computer vision system.
First, automatic substructure discovery help system interpret image. Instead
working low-level vertices edges, Subdue provide abstract structured
components, resulting hierarchical view image machine analyze
many levels detail focus, depending goal analysis. Second, substructure
discovery makes use inexact graph match help identify objects 2D image
3D scene noise orientation differences likely exist. object appears often scene, inexact graph match driving Subdue system may capture
slightly different views object. Although object may dicult identify
243

fiCook & Holder


CH2
N

adenine

N
N

N




N

P




H
N

OH



N

CH 3



H
P

HO


N

H

N

H



N

guanine
N

P





CH2



thymine

CH2



H

N

N





N
H

OH

N

cytosine

CH2
CH 3



H

P

HO








CH2
N

thymine
P

N

H

N



N




N
N

CH 3



H

OH

CH2

adenine

N
H


P

HO




Figure 8: Portion DNA molecule.

Figure 9: Scene analysis example.
244



fiSubstructure Discovery

f



k

x

l



p



Figure 10: Possible vertices labels.

l

l
l

l

l

l

l

l



l

l




l







l

l

l

l

l

l

l




l



l



f
l

l



Figure 11: Portion graph representing scene Figure 4.
one 2D picture, Subdue match instances similar objects, differences instances provide additional information identification. Third,
substructure discovery used compress image. Replacing common interesting
substructures single vertex simplifies image description reduces amount
storage necessary represent image.
apply Subdue image data, extract edge information image
construct graph representing scene. graph representation consists eight types
vertices two types arcs (edge space). vertex labels (f , a, l, t, k, x, p,
m) follow Waltz labelings (Waltz, 1975) junctions edges image represent
types vertices shown Figure 10. edge arc represents edge object
image, space arc links non-connecting objects together. edge arcs represent
edge scene connects two vertices, space arcs connect closest vertices
two disjoint neighboring objects. Distance, curve, angle information
included graph representation, added give additional information
scene. Figure 11 shows graph representation portion scene depicted
Figure 9. figure, edge arcs solid space arcs dashed.
245

fiCook & Holder

VCC

ext_pin

drain
drain
gate
n_mosfet

gate
source

source
connect

drain

drain
gate
connect

gate

n_mosfet

source

ext_pin

GND

Figure 12: Amplifier circuit graph representation.
7.1.3 CAD Circuit Analysis

domain, employ Subdue find circuit components CAD circuit data. Discovery substructures circuit data valuable tool engineer attempting
identify common reusable parts circuit layout. Replacing individual components
circuit description larger substructure descriptions simplify representation
circuit.
data circuit domain obtained National Semiconductor, consists set components making circuit output Cadence Design System.
particular circuit used experiment portion analog-to-digital converter. Figure 12 presents circuit amplifier gives corresponding graph
representation.
7.1.4 Artificial Domain

final domain, artificially generate graphs evaluate Subdue's ability discover
substructures capable compressing graph. Four substructures created varying
sizes randomly-selected vertices edges (see Figure 13). name substructure
ects number vertices edges graph representation. Next, substructures embedded larger graphs whose size 15 times size substructure.
graphs vary across four parameters: number possible vertex edge labels (one
times two times number labels used substructure), connectivity
substructure (1 2 external connections), coverage instances (60% 80%),
246

fiSubstructure Discovery

e1
e2

e3

n4

n1

e1
n3

e2

n2

n4

n3
e3

n2

e3
e6
n7

e3
n5

e1
n2

e4
n5

e6

e3

n1

n6

n3

n1

e9

e3
e5
n3

e2
n3

n1

e2

e3
n1

n7

e8
n2

n4

e6

e1
e7
e8

Figure 13: Four artificial substructures used evaluate Subdue.

amount distortion instances (0, 1 2 distortions). yields total 96
graphs (24 different substructure).

7.2 Experimental Results
7.2.1 Experiment 1: Data compression

first experiment, test Subdue's ability compress structural database. Using
beam width 4 Subdue's pruning mechanism, applied discovery algorithm
databases mentioned above. repeat experiment match thresholds
ranging 0.0 1.0 increments 0.1. Table 1 shows description length (DL)
original graph, description length best substructure discovered Subdue,
graph
value compression. Compression defined DLDLofofcompressed
original graph . Figure 14,
shows actual discovered substructures first four datasets.
seen Table 1, Subdue able reduce database slightly
larger 14 original size best case. average compression value
domains (treating artificial graphs one value) 0.62. results
experiment demonstrate substructure discovered Subdue significantly
reduce amount data needed represent input graph. expect compressing
graph using combinations substructures hierarchies substructures realize
even greater compression databases.
247

fiCook & Holder

Database
DLoriginal Thresholdoptimal DLcompressed Compression
Rubber
371.78
0.1
95.20
0.26
Cortisone
355.03
0.3
173.25
0.49
DNA
2427.93
1.0
2211.87
0.91
Pencils
1592.33
1.0
769.18
0.48
CAD { M1
4095.73
0.7
2148.8
0.52
CAD { S1SegDec
1860.14
0.7
1149.29
0.62
CAD { S1DrvBlk
12715.12
0.7
9070.21
0.71
CAD { BlankSub
8606.69
0.7
6204.74
0.72
CAD { And2
427.73
0.1
324.52
0.76
Artificial (avg. 96 graphs) 1636.25
0.0: : :1.0
1164.02
0.71
Table 1: Graph compression results.

CH 3

H

CH2







C
C

C

C

C

CH

CH
2

2

(a)

C

C

(b)

C
l



(c)



(d)

Figure 14: Best substructure (a) rubber database, (b) cortisone database, (c) DNA
database, (d) image database.

248

fiSubstructure Discovery

Figure 15: Benzene ring discovered Subdue.
7.2.2 Experiment 2: Re-discovery known substructures using background
knowledge

Another way evaluating discovery process evaluate interestingness
discovered substructures. determination value change domain
domain. result, second set experiments test Subdue's ability discover
substructures already labeled important experts domains
consideration.
chemical compound domain, chemists frequently describe compounds terms
building-block components heavily used. example, rubber compound
database shown Figure 7, compound made chain structures
labeled chemists isoprene units. Subdue's ability re-discover structure
exemplified Figure 14a. substructure, discovered using MDL principle
extra background knowledge, represents isoprene unit.
Although Subdue able re-discover isoprene units without extra background
knowledge, substructure affording compression always interesting important substructure database. example, cortisone database
benzene ring consists ring carbons discovered using MDL
principle. However, additional background rules used increase chance
finding interesting substructures domains. case cortisone compound,
know interesting structures exhibit characteristic closure. Therefore,
give strong weight (8.0) compactness background rule use match threshold
0.2 allow deviations benzene ring instances. resulting output, Subdue
finds benzene ring shown Figure 15.
way, use background rules find pencil substructure
image data. image Figure 9 viewed, substructure interest
pencil various forms. However, substructure afforded compression
make entire pencil. know pencils high degree closure
coverage, weights rules set 1.0. weights, Subdue
able find pencil substructure shown Figure 16 tested match thresholds
0.0 1.0.

8. Hierarchical Concept Discovery

substructure discovered, instance substructure input graph
replaced single vertex representing entire substructure. discovery procedure
repeated compressed data set, resulting new interesting substructures.
newly-discovered substructures defined terms existing substructure concepts,
substructure definitions form hierarchy substructure concepts.
249

fiCook & Holder

l

l




l

Figure 16: Pencil substructure discovered Subdue.

Hierarchical concept discovery adds capability improve Subdue's performance. Subdue applied large input graph, complexity algorithm
prevents consideration larger substructures. Using hierarchical concept discovery, Subdue first discover smaller substructures best compress data. Applying
compression reduces graph manageable size, increasing chance
Subdue find larger substructures subsequent passes database.
Subdue selects substructure, vertices comprise exact instances
substructure replaced graph single vertex representing discovered
substructure. Edges connecting vertices outside instance vertices inside instance
connect new vertex. Edges internal instance removed. discovery
process applied compressed data. hierarchical description concepts
particularly desired, heavier weight given substructures utilize previously
discovered substructures. increased weight ects increased attention substructure. Figure 17 illustrates compressed rubber compound graph using substructure
shown Figure 14a.
demonstrate ability Subdue find hierarchy substructures, let system make multiple passes database represents portion DNA molecule.
Figure 8 shows portion two chains double helix, using three pairs bases
held together hydrogen bonds. Figure 18 shows substructures found Subdue
three passes data. Note that, third pass, Subdue linked
together instances substructure second pass find chains double
helix.
Although replacing portions input graph discovered substructures compresses data provides basis discovering hierarchical concepts data,
substructure replacement procedure becomes complicated concepts inexact
instances discovered. inexact instances discovered concept replaced
single vertex data, distortions graph (differences instance
graph substructure definition) must attached annotations vertex label.
250

fiSubstructure Discovery

Highest-valued substructure

CH 3

H

=
1

C

C
CH

CH

2

2

Compressed graph using discovered substructure

G

S1

=

CH

CH

C
2

C
2

CH

CH

2

2
C

CH 3

CH

H

3

C
CH

S1

S1

CH

H

3

=

S1


1

C

C
CH

CH

2

2

CH

CH

2

2

C

C
CH 3

H

H

3

CH

2

C
H

Figure 17: Compressed graph rubber compound data.

251

C
CH

2

fiCook & Holder

Highest-valued substructure
First Pass

CH2

S1 =




Highest-valued substructure
Second Pass

S1

C

CH2



S2 =

=


P
P


CH2

Highest-valued substructure
Third Pass





S2

P

OH

OH


=

S3 =

CH2



S2

OH



S2

OH




P



OH

CH2




P

OH


Figure 18: Hierarchical discovery DNA data.

252

fiSubstructure Discovery

9. Conclusions

Extracting knowledge structural databases requires identification repetitive substructures data. Substructure discovery identifies interesting repetitive structure
structural data. substructures represent concepts found data means
reducing complexity representation abstracting instances substructure. shown minimum description length (MDL) principle used
perform substructure discovery variety domains. substructure discovery process
guided background knowledge. use inexact graph match allows
deviation instances substructure. substructure discovered, instances
substructure replaced concept definition, affording compression
data description providing basis discovering hierarchically-defined structures.
Future work combine structural discovery discovery concepts using linearbased representation AutoClass (Cheeseman, Kelly, Self, Stutz, Taylor, & Freeman,
1988). particular, use Subdue compress data fed AutoClass,
let Subdue evaluate interesting structures classes generated AutoClass.
addition, developing parallel implementation AutoClass / Subdue
system enable application substructure discovery larger structural databases.

Acknowledgements

project supported NASA grant NAS5-32337. authors would thank
Mike Shay National Semiconductor providing circuit data. would
thank Surnjani Djoko Tom Lai help project. Thanks
reviewers numerous insightful comments.

References

Bunke, H., & Allermann, G. (1983). Inexact graph matching structural pattern recognition. Pattern Recognition Letters, 1 (4), 245{253.
Cheeseman, P., Kelly, J., Self, M., Stutz, J., Taylor, W., & Freeman, D. (1988). Autoclass:
bayesian classification system. Proceedings Fifth International Workshop
Machine Learning, pp. 54{64.
Conklin, D., & Glasgow, J. (1992). Spatial analogy subsumption. Proceedings
Ninth International Machine Learning Workshop, pp. 111{116.
Derthick, M. (1991). minimal encoding approach feature discovery. Proceedings
Ninth National Conference Artificial Intelligence, pp. 565{571.
Fisher, D. H. (1987). Knowledge acquisition via incremental conceptual clustering. Machine
Learning, 2 (2), 139{172.
Fu, K. S. (1982). Syntactic Pattern Recognition Applications. Prentice-Hall.
Holder, L. B., Cook, D. J., & Bunke, H. (1992). Fuzzy substructure discovery. Proceedings
Ninth International Machine Learning Conference, pp. 218{223.
253

fiCook & Holder
Holder, L. B., & Cook, D. J. (1993). Discovery inexact concepts structural data.
IEEE Transactions Knowledge Data Engineering, 5 (6), 992{994.
Jeltsch, E., & Kreowski, H. J. (1991). Grammatical inference based hyperedge replacement. Fourth International Workshop Graph Grammars Application
Computer Science, pp. 461{474.
Leclerc, Y. G. (1989). Constructing simple stable descriptions image partitioning. International journal Computer Vision, 3 (1), 73{102.
Levinson, R. (1984). self-organizing retrieval system graphs. Proceedings
Second National Conference Artificial Intelligence, pp. 203{206.
Michalski, R. S., & Stepp, R. E. (1983). Learning observation: Conceptual clustering.
Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), Machine Learning:
Artificial Intelligence Approach, Vol. I, pp. 331{363. Tioga Publishing Company.
Miclet, L. (1986). Structural Methods Pattern Recognition. Chapman Hall.
Pednault, E. P. D. (1989). experiments applying inductive inference principles
surfa ce reconstruction. Proceedings International Joint Conference
Artificial Intelligence, pp. 1603{1609.
Pentland, A. (1989). Part segmentation object recognition. Neural Computation, 1,
82{91.
Prather, R. (1976). Discrete Mathemetical Structures Computer Science. Houghton
Min Company.
Quinlan, J. R., & Rivest, R. L. (1989). Inferring decision trees using minimum description length principle. Information Computation, 80, 227{248.
Rao, R. B., & Lu, S. C. (1992). Learning engineering models minimum description length principle. Proceedings Tenth National Conference Artificial
Intelligence, pp. 717{722.
Rissanen, J. (1989). Stochastic Complexity Statistical Inquiry. World Scientific Publishing
Company.
Schalkoff, R. J. (1992). Pattern Recognition: Statistical, Structural Neural Approaches.
John Wiley & Sons.
Segen, J. (1990). Graph clustering model learning data compression. Proceedings
Seventh International Machine Learning Workshop, pp. 93{101.
Thompson, K., & Langley, P. (1991). Concept formation structured domains. Fisher,
D. H., & Pazzani, M. (Eds.), Concept Formation: Knowledge Experience Unsupervised Learning, chap. 5. Morgan Kaufmann Publishers, Inc.
Waltz, D. (1975). Understanding line drawings scenes shadows. Winston, P. H.
(Ed.), Psychology Computer Vision. McGraw-Hill.
254

fiSubstructure Discovery
Wertheimer, M. (1939). Laws organization perceptual forms. Ellis, W. D. (Ed.),
Sourcebook Gestalt Psychology, pp. 331{363. Harcourt, Brace Company.
Winston, P. H. (1975). Learning structural descriptions examples. Winston, P. H.
(Ed.), Psychology Computer Vision, pp. 157{210. McGraw-Hill.
Yoshida, K., Motoda, H., & Indurkhya, N. (1993). Unifying learning methods colored
digraphs. Proceedings Learning Knowledge Acquisition Workshop
IJCAI-93.
Zahn, C. T. (1971). Graph-theoretical methods detecting describing gestalt clusters.
IEEE Transactions Computers, 20 (1), 68{86.

255


