Journal Artificial Intelligence Research 1 (1993) 25-46

Submitted 7/93; published 8/93

Dynamic Backtracking
Matthew L. Ginsberg

ginsberg@cs.uoregon.edu

CIRL, University Oregon,
Eugene, 97403-1269 USA

Abstract

occasional need return shallow points search tree, existing
backtracking methods sometimes erase meaningful progress toward solving search
problem. paper, present method backtrack points moved
deeper search space, thereby avoiding diculty. technique developed
variant dependency-directed backtracking uses polynomial space still
providing useful control information retaining completeness guarantees provided
earlier approaches.

1. Introduction
Imagine trying solve constraint-satisfaction problem, csp.
interests definiteness, suppose csp question involves coloring map
United States subject restriction adjacent states colored differently.
Imagine begin coloring states along Mississippi, thereby splitting
remaining problem two. begin color states western half
country, coloring perhaps half dozen deciding likely able
color rest. Suppose last state colored Arizona.
point, change focus eastern half country. all, can't
color eastern half coloring choices states along Mississippi,
point wasting time completing coloring western states.
successfully color eastern states return west. Unfortunately,
color New Mexico Utah get stuck, unable color (say) Nevada. What's more,
backtracking doesn't help, least sense changing colors New Mexico
Utah alone allow us proceed farther. Depth-first search would
us backtrack eastern states, trying new color (say) New York vain hope
would solve problems West.
obviously pointless; blockade along Mississippi makes impossible
New York impact attempt color Nevada western states.
What's more, likely examine every possible coloring eastern states
addressing problem actually source diculties.
solutions proposed involve finding ways backtrack directly
state might actually allow us make progress, case Arizona earlier.
Dependency-directed backtracking (Stallman & Sussman, 1977) involves direct backtrack
source diculty; backjumping (Gaschnig, 1979) avoids computational overhead technique using syntactic methods estimate point backtrack
necessary.

c 1993 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiGinsberg

cases, however, note although backtrack source problem,
backtrack successful solution half original problem, discarding
solution problem coloring states East. again, problem
worse { recolor Arizona, danger solving East yet
realizing new choice Arizona needs changed all. won't
examine every possible coloring eastern states, danger rediscovering
successful coloring exponential number times.
hardly seems sensible; human problem solver working problem would
simply ignore East possible, returning directly Arizona proceeding.
states along Mississippi needed new colors would East reconsidered { even
new coloring could found Mississippi consistent
eastern solution.
paper formalize technique, presenting modification conventional
search techniques capable backtracking recently expanded
node, directly node elsewhere search tree. dynamic way
search structured, refer technique dynamic backtracking.
specific outline follows: begin next section introducing
variety notational conventions allow us cast existing work new
ideas uniform computational setting. Section 3 discusses backjumping, intermediate
simple chronological backtracking ideas, presented
Section 4. example dynamic backtracking algorithm use appears Section
5 experimental analysis technique Section 6. summary results
suggestions future work Section 7. proofs deferred appendix
interests continuity exposition.

2. Preliminaries
Definition 2.1 constraint satisfaction problem (I; V; ) mean set vari-

ables; 2 , set Vi possible values variable i. set
constraints, pair (J; P ) J = (j1; . . . ; jk ) ordered subset P
subset Vj1 Vjk .
solution csp set vi values variables vi 2 Vi
every constraint (J; P ) form , (vj1 ; . . . ; vjk ) 2 P .

example introduction, set states Vi set possible
colors state i. constraint, first part constraint pair adjacent
states second part set allowable color combinations states.
basic plan paper present formal versions search algorithms
described introduction, beginning simple depth-first search proceeding
backjumping dynamic backtracking. start, make following definition
partial solution csp:

Definition 2.2 Let (I; V; ) csp. partial solution csp mean ordered
subset J assignment value variable J .
26

fiDynamic Backtracking

denote partial solution tuple ordered pairs, ordered pair

(i; v ) assigns value v variable i. partial solution P , denote P
set variables assigned values P .

Constraint-satisfaction problems solved practice taking partial solutions
extending assigning values new variables. general, course, value
assigned variable inconsistent constraints. therefore
make following definition:

Definition 2.3 Given partial solution P

csp, eliminating explanation
variable pair (v; ) v 2 Vi P . intended meaning
cannot take value v values already assigned P variables .
elimination mechanism csp function accepts arguments partial
solution P , variable 62 P . function returns (possibly empty) set (P; i)
eliminating explanations i.

set E eliminating explanations, denote Eb values
identified eliminated, ignoring reasons given. therefore denote b(P; i) set
values eliminated elements (P; i).
Note definition somewhat exible regard amount work
done elimination mechanism { values violate completed constraints might
eliminated, amount lookahead might done. will, however, make
following assumptions elimination mechanisms:
1. correct. partial solution P , value vi 62 b(P; i), every
constraint (S; ) P [fig satisfied values partial solution
value vi i. constraints complete value vi
assigned i.
2. complete. Suppose P partial solution csp,
solution extends P assigning value v i. P 0 extension P
(v; E ) 2 (P 0 ; i),
E \ (P 0 , P ) 6=
(1)
words, whenever P successfully extended assigning v
P 0 cannot be, least one element P 0 , P identified possible reason
problem.
3. concise. partial solution P , variable eliminated value v ,
single element form (v; E ) 2 (P; i). one reason given
variable cannot value v .

Lemma 2.4 Let complete elimination mechanism csp, let P partial solution csp let 62 P . P successfully extended complete solution
assigning value v , v 62 b(P; i).
apologize swarm definitions, allow us give clean description
depth-first search:
27

fiGinsberg

Algorithm 2.5 (Depth-first search) Given inputs constraint-satisfaction problem

elimination mechanism :
1. Set P = . P partial solution csp. Set Ei = 2 ; Ei
set values eliminated variable i.
2. P = , P assigns value every element , solution
original problem. Return it. Otherwise, select variable 2 , P . Set Ei = b(P; i),
values eliminated possible choices i.
3. Set = Vi , Ei, set remaining possibilities i. nonempty, choose
element v 2 . Add (i; v ) P , thereby setting i's value v , return step 2.
4. empty, let (j; vj ) last entry P ; entry, return failure.
Remove (j; vj ) P , add vj Ej , set = j return step 3.

written algorithm returns single answer csp; modification accumulate answers straightforward.
problem Algorithm 2.5 looks little conventional depth-first
search, since instead recording unexpanded children particular node,
keeping track failed siblings node. following:

Lemma 2.6 point execution Algorithm 2.5, last element partial
solution P assigns value variable i, unexplored siblings current node
assign values Vi , Ei .
Proposition 2.7 Algorithm 2.5 equivalent depth-first search therefore complete.

remarked, basic difference Algorithm 2.5 conventional description depth-first search inclusion elimination sets Ei.
conventional description expects nodes include pointers back parents; siblings given node found examining children node's parent. Since
reorganizing space search, impractical framework.
might seem natural solution diculty would record
values eliminated variable i, remain considered.
technical reason done much easier maintain
elimination information search progresses. understand intuitive level,
note search backtracks, conclusion implicitly drawn
particular node fails expand solution, opposed conclusion
currently unexplored portion search space. little surprise
ecient way manipulate information recording approximately form.

3. Backjumping

describe dependency-directed backtracking backjumping setting?
cases, partial solution forced backtrack;
sophisticated backtracking mechanisms use information reason failure
identify backtrack points might allow problem addressed. start, need
modify Algorithm 2.5 maintain explanations eliminated values:
28

fiDynamic Backtracking

Algorithm 3.1 Given inputs constraint-satisfaction problem elimination mechanism :
1. Set P = Ei = 2 . Ei set eliminating explanations i.
2. P = , return P . Otherwise, select variable 2 , P . Set Ei = (P; i):
3. Set = Vi , Ebi. nonempty, choose element v 2 . Add (i; v ) P
return step 2.
4. empty, let (j; vj ) last entry P ; entry, return failure.
Remove (j; vj ) P . must Ebi = Vi, every value
eliminated; let E set variables appearing explanations
eliminated value. Add (vj ; E , fj g) Ej , set = j return step 3.

Lemma 3.2 Let P partial solution obtained execution Algorithm 3.1,

let 2 P variable assigned value P . P 0 P successfully
extended complete solution assigning value v (v; E ) 2 Ei , must

E \ (P , P 0) 6=

words, assignment value variable P , P 0 correctly identified
source problem.
Note step 4 algorithm, could added (vj ; E \ P ) instead (vj ; E ,
fj g) Ej ; either way, idea remove E variables longer assigned
values P .
backjumping, simply change backtrack method; instead removing
single entry P returning variable assigned value prior problematic
variable i, return variable actually impact i. words,
return variable set E .

Algorithm 3.3 (Backjumping) Given inputs constraint-satisfaction problem
elimination mechanism :
1. Set P = Ei = 2 .

2. P = , return P . Otherwise, select variable 2 , P . Set Ei = (P; i):
3. Set = Vi , Ebi. nonempty, choose element v 2 . Add (i; v ) P
return step 2.
4. empty, must Ebi = Vi . Let E set variables appearing
explanations eliminated value.
5. E = , return failure. Otherwise, let (j; vj ) last entry P j 2 E .
Remove P entry entry following it. Add (vj ; E \ P ) Ej , set = j
return step 3.
29

fiGinsberg

step 5, add (vj ; E \ P ) Ej , removing E variables longer
assigned values P .

Proposition 3.4 Backjumping complete always expands fewer nodes depthfirst search.

Let us look map-coloring example. partial coloring
P looking specific state i, suppose denote C set colors
obviously illegal con ict color already assigned one i's
neighbors.
One possible elimination mechanism returns (P; i) list (c; P ) color
c 2 C used color neighbor i. reproduces depth-first search, since
gradually try possible colors idea went wrong need
backtrack since every colored state included P . far sensible choice would take
(P; i) list (c; fng) n neighbor already colored c. would
ensure backjump neighbor coloring found.
causes us backjump another state j , add i's neighbors eliminating explanation j 's original color, need backtrack still further,
consider neighbors either j . be, since changing color one
i's neighbors might allow us solve coloring problem reverting original
choice color state j .
have:

Proposition 3.5 amount space needed backjumping o(i2v), = jI j

number variables problem v number values variable
largest value set Vi .

result contrasts sharply approach csps relies truth-maintenance
techniques maintain list nogoods (de Kleer, 1986). There, number nogoods
found grow linearly time taken analysis, typically
exponential size problem. Backjumping avoids problem resetting
set Ei eliminating explanations step 2 Algorithm 3.3.
description given quite similar developed (Bruynooghe,
1981). explanations somewhat coarser ours, listing variables
involved eliminating explanation particular variable csp,
idea essentially same. Bruynooghe's eliminating explanations stored
o(i2) space (instead o(i2v )), associated loss information makes technique
less effective practice. earlier work description backjumping only, since
intermediate information erased search proceeds.

4. Dynamic backtracking

finally turn new results. basic problem Algorithm 3.3 backjumps wrong place, needlessly erases great deal work
done thus far. least, retain values selected variables
backjumped over, sense moving backjump variable end partial
30

fiDynamic Backtracking

solution order replace value without modifying values variables
followed it.
additional modification probably clearest return
example introduction. Suppose example, color eastern
states returning western half country. reorder variables order
backtrack Arizona eventually succeed coloring West without disturbing
colors used East.
Unfortunately, return East backtracking required find
needing change coloring eastern states dealt earlier.
ideas presented allow us avoid erasing solution problems
West, search eastern states ecient, need
retain information portion East's search space
eliminated. all, determined New York cannot colored yellow,
changes West reverse conclusion { Mississippi really isolate one
section country other.
machinery needed capture sort reasoning already place.
backjump variable k, retain choice value k, k's
elimination set. do, however, need remove elimination set entry
involves eventual backtrack variable j , since entries longer valid {
depend assumption j takes old value, assumption false.

Algorithm 4.1 (Dynamic backtracking I) Given inputs constraint-satisfaction problem elimination mechanism :
1. Set P = Ei = 2 .
2. P = , return P . Otherwise, select variable 2 , P . Set Ei = Ei [ (P; i).
3. Set = Vi , Ebi. nonempty, choose element v 2 . Add (i; v ) P
return step 2.
4. empty, must Ebi = Vi ; let E set variables appearing
explanations eliminated value.
5. E = , return failure. Otherwise, let (j; vj ) last entry P j 2 E .
Remove (j; vj ) P and, variable k assigned value j , remove
Ek eliminating explanation involves j . Set

Ej = Ej [ (P; j ) [ f(vj ; E \ P )g
(2)
vj eliminated value j values taken variables
E \ P . inclusion term (P; j ) incorporates new information variables
assigned values since original assignment vj j . set = j
return step 3.

Theorem 4.2 Dynamic backtracking always terminates complete. continues

satisfy Proposition 3.5 expected expand fewer nodes backjumping provided
goal nodes distributed randomly search space.
31

fiGinsberg

essential difference dynamic dependency-directed backtracking
structure eliminating explanations means save nogood information
based current values assigned variables; nogood depends outdated information, drop it. this, avoid need retain exponential amount
nogood information. makes technique valuable (as stated theorem)
termination still guaranteed.
one trivial modification make Algorithm 4.1 quite useful
practice. removing current value backtrack variable j , Algorithm 4.1
immediately replaces another. real reason this; could
instead pick value entirely different variable:

Algorithm 4.3 (Dynamic backtracking) Given inputs constraint-satisfaction problem elimination mechanism :
1. Set P = Ei = 2 .
2. P = , return P . Otherwise, select variable 2 , P . Set Ei = Ei [ (P; i).
3. Set = Vi , Ebi. nonempty, choose element v 2 . Add (i; v ) P
return step 2.
4. empty, must Ebi = Vi ; let E set variables appearing
explanations eliminated value.
5. E = , return failure. Otherwise, let (j; vj ) last entry P binds
variable appearing E . Remove (j; vj ) P and, variable k assigned
value j , remove Ek eliminating explanation involves j . Add
(vj ; E \ P ) Ej return step 2.

5. example
order make Algorithm 4.3 bit clearer, suppose consider small mapcoloring problem detail. map shown Figure 1 consists five countries:
Albania, Bulgaria, Czechoslovakia, Denmark England. assume (wrongly!)
countries border shown figure, countries denoted nodes
border one another arc connecting them.
coloring map, use three colors red, yellow blue. typically
abbreviate country names single letters obvious way.
begin search Albania, deciding (say) color red. look
Bulgaria, colors eliminated Albania Bulgaria share border;
decide color Bulgaria yellow. (This mistake.)
go consider Czechoslovakia; since borders Albania, color red
eliminated. decide color Czechoslovakia blue situation this:
32

fiDynamic Backtracking

Denmark




Czechoslovakia

,,@@
,
@@
,
,
@
,
@@
,
Albanias,
@,sBulgaria
@@
,
,
@@
,
,
@
,
@@ ,
@,s
England

Figure 1: small map-coloring problem
country
color red yellow blue
Albania
red
Bulgaria
yellow
Czechoslovakia blue
Denmark
England
country, indicate current color eliminating explanations mean
cannot colored three colors (when explanations exist). look
Denmark.
Denmark cannot colored red border Albania cannot colored
yellow border Bulgaria; must therefore colored blue.
England cannot colored color borders Albania, Bulgaria
Denmark, therefore need backtrack one three countries.
point, elimination lists follows:
country
color red yellow blue
Albania
red
Bulgaria
yellow
Czechoslovakia blue
Denmark
blue
B
England

B

backtrack Denmark recent three possibilities,
begin removing eliminating explanation involving Denmark table
get:
33

fiGinsberg

color red yellow blue
country
Albania
red
Bulgaria
yellow
Czechoslovakia blue
Denmark

B
England

B
Next, add Denmark's elimination list pair
(blue; fA; B g)
indicates correctly current colors Albania Bulgaria, Denmark cannot colored blue (because subsequent dead end England). Since every
color eliminated, must backtrack country set fA; B g. Changing
Czechoslovakia's color won't help must deal Bulgaria instead. elimination
lists now:
country
color red yellow blue
Albania
red
Bulgaria
Czechoslovakia blue
Denmark

B A,B
England

B
remove eliminating explanations involving Bulgaria add Bulgaria's elimination list pair
(yellow; A)
indicating correctly Bulgaria cannot colored yellow current choice
color Albania (red).
situation now:
color red yellow blue
country
Albania
red
Czechoslovakia blue
Bulgaria

Denmark

England

moved Bulgaria past Czechoslovakia ect search reordering algorithm. complete problem coloring Bulgaria red, Denmark either yellow
blue, England color used Denmark.
example almost trivially simple, course; thing note
changed color Bulgaria, retained blue color Czechoslovakia
information indicating none Czechoslovakia, Denmark England could red.
complex examples, information may hard-won retaining may
save us great deal subsequent search effort.
Another feature specific example (and example introduction
well) computational benefits dynamic backtracking consequence
34

fiDynamic Backtracking

automatic realization problem splits disjoint subproblems. authors
discussed idea applying divide-and-conquer techniques csps (Seidel, 1981;
Zabih, 1990), methods suffer disadvantage constrain order
unassigned variables assigned values, perhaps odds common heuristic
assigning values first variables tightly constrained. Dynamic
backtracking expected use situations problem question
split two disjoint subproblems.1

6. Experimentation

Dynamic backtracking incorporated crossword-puzzle generation program
described (Ginsberg, Frank, Halpin, & Torrance, 1990), leads significant performance improvements restricted domain. specifically, method tested
problem generating 19 puzzles sizes ranging 2 2 13 13; puzzle
attempted 100 times using dynamic backtracking simple backjumping.
dictionary shued solution attempts maximum 1000 backtracks
permitted program deemed failed.
cases, algorithms extended include iterative broadening (Ginsberg
& Harvey, 1992), cheapest-first heuristic forward checking. Cheapest-first
called \most constrained first" selects instantiation variable
fewest number remaining possibilities (i.e., variable cheapest
enumerate possible values (Smith & Genesereth, 1985)). Forward checking prunes
set possibilities crossing words whenever new word entered constitutes
experimental choice elimination mechanism: point, words legal
crossing word eliminated. ensures word entered crossword
word potential crossing words point. cheapest-first heuristic
would identify problem next step search, forward checking reduces
number backtracks substantially. \least-constraining" heuristic (Ginsberg et al.,
1990) used; heuristic suggests word slot filled word
minimally constrains subsequent search. heuristic used would
invalidate technique shuing dictionary solution attempts order
gather useful statistics.
table Figure 2 indicates number successful solution attempts (out 100)
two methods 19 crossword frames. Dynamic backtracking
successful six cases less successful none.
regard number nodes expanded two methods, consider data
presented Figure 3, graph average number backtracks needed
two methods.2 Although initially comparable, dynamic backtracking provides increasing
computational savings problems become dicult. somewhat broader set
experiments described (Jonsson & Ginsberg, 1993) leads similar conclusions.
examples (Jonsson & Ginsberg, 1993) dynamic backtracking
leads performance degradation, however; typical case appears Figure 4.3
1. indebted David McAllester observations.
2. 17 points shown point plotted backjumping unable solve problem.
3. worst performance degradation observed factor approximately 4.

35

fiGinsberg

Dynamic
Dynamic
Frame backtracking Backjumping Frame backtracking Backjumping
1
100
100
11
100
98
100
100
12
100
100
2
3
100
100
13
100
100
100
100
14
100
100
4
5
100
100
15
99
14
100
100
16
100
26
6
7
100
100
17
100
30
100
100
18
61
0
8
9
100
100
19
10
0
10
100
100
Figure 2: Number problems solved successfully

400
r
r

dynamic 200
backtracking

r
r

r rr
rrr
rr

r
r

rr

200

400
600
backjumping

Figure 3: Number backtracks needed

36

800

1000

fiDynamic Backtracking

Region
1


,,
,
,
,
,
,
B
sa
s,
@@
aaaa
aaa
aaa @@
aaa @
aa@a
@a@s

Region 2

Figure 4: dicult problem dynamic backtracking
figure, first color A, B , countries region 1, get stuck region
2.
presumably backtrack directly B , leaving coloring region 1 alone.
may well mistake { colors region 1 restrict choices B , perhaps
making subproblem consisting A, B region 2 dicult might be.
region 1 easy color, would better erasing even though didn't
need to.
analysis suggests dependency-directed backtracking fare worse
coloring problems dynamic backtracking trouble, currently
extending experiments (Jonsson & Ginsberg, 1993) confirm this. conjecture
borne out, variety solutions come mind. might, example, record
many backtracks made node B figure, use
determine exibility B important retaining choices made region
1. diculty finding coloring region 1 determined number
backtracks involved search.

7. Summary
7.1 works
two separate ideas exploited development Algorithm 4.3
others leading it. first, easily important, notion
possible modify variable order way allows us retain
results earlier work backtracking variable assigned value early
search.
37

fiGinsberg

reordering confused work authors suggested
dynamic choice among variables remain assigned values (Dechter & Meiri,
1989; Ginsberg et al., 1990; P. Purdom & Robertson, 1981; Zabih & McAllester, 1988);
instead reordering variables assigned values search thus far.
Another way look idea found way \erase" value given
variable directly opposed backtracking it. idea explored
Minton et.al. (Minton, Johnston, Philips, & Laird, 1990) Selman et.al.
(Selman, Levesque, & Mitchell, 1992); authors directly replace values assigned
variables satisfiability problems. Unfortunately, heuristic repair method used
incomplete dependency information retained one state problem
solver next.
third way view well. space examining really
graph, opposed tree; reach point coloring Albania blue
Bulgaria red color opposite order. decide backjump
particular node search space, know need back particular
property node ceases hold { key idea backtracking along
path one node generated, may able backtrack
slightly would otherwise need retreat great deal. observation
interesting may well apply problems csps. Unfortunately,
clear guarantee completeness search discovers node using one path
backtracks using another.
idea less novel. already remarked, use eliminating
explanations quite similar use nogoods atms community; principal
difference attach explanations variables impact drop
cease relevant. (They might become relevant later, course.)
avoids prohibitive space requirements systems permanently cache results
nogood calculations; observation may extensible beyond domain
csps specifically. Again, ways view { Gashnig's notion backmarking
(Gaschnig, 1979) records similar information reason particular portions
search space known contain solutions.

7.2 Future work
variety ways techniques presented extended;
section, sketch obvious ones.
7.2.1 Backtracking older culprits

One extension work involves lifting restriction Algorithm 4.3 variable
erased always recently assigned member set E .
general, cannot retaining completeness search. Consider
following example:
Imagine csp involves three variables, x, z , take value 0
1. Further, suppose csp solutions, pick two values
x , realize suitable choice z .
38

fiDynamic Backtracking

begin taking x = = 0; realize need backtrack, introduce
nogood
x = 0 6= 0
(3)
replace value = 1.
fails, too, suppose decide backtrack x, introducing
new nogood
= 1 x 6= 0
(4)
change x's value 1 erase (3).
fails. decide problem change value 0, introducing
nogood
x = 1 6= 1
erasing (4). fails, danger returning x = = 0,
eliminated beginning example. loop may cause modified version
dynamic backtracking algorithm fail terminate.
terms proof Theorem 4.2, nogoods discovered already include information
assigned variables, difference (7) (8). drop
(3) favor (4), longer position recover (3).
deal placing conditions variables choose
backtrack; conditions need defined proof Theorem 4.2 continues
hold.4 Experimentation indicates loops form described extremely
rare practice; may possible detect directly thereby retain
substantial freedom choice backtrack point.
freedom backtrack raises important question yet addressed
literature: backtracking avoid diculty sort, one
backtrack?
Previous work constrained backtrack recent choice
might impact problem question; decision would incomplete
inecient. Although extension Algorithm 4.3 need operate restriction,
given indication backtrack point selected.
several easily identified factors expected bear choice.
first remains reason expect backtracking chronologically recent
choices effective { choices expected contributed
fewest eliminating explanations, obvious advantage retaining many
eliminating explanations possible one point search next. possible, however, simply identify backtrack point affects fewest number
eliminating explanations use that.
Alternatively, might important backtrack choice point
many new choices possible; extreme example, variable
every value current one already eliminated
reasons, backtracking guaranteed generate another backtrack immediately
probably avoided possible.
4. Another solution appears (McAllester, 1993).

39

fiGinsberg

Finally, measure \directness" variable bears
problem. unable find value particular variable i, probably sensible
backtrack second variable shares constraint itself, opposed
variable affects indirectly.
competing considerations weighed? idea. framework developed interesting allows us work question.
basic terms, \debug" partial solutions csps directly, moving laterally
search space attempt remain close solution possible.
sort lateral movement seems central human solution dicult search problems,
encouraging begin understand formal way.
7.2.2 Dependency pruning

often case one value variable eliminated solving csp,
others eliminated well. example, solving scheduling problem particular
choice time (say = 16) may eliminated task isn't enough
time subsequent task B ; case, later times obviously
eliminated well.
Formalizing subtle; all, later time isn't uniformly worse
earlier time may tasks need precede making later
makes part schedule easier. It's problem B alone forces
earlier; again, analysis depends ability maintain dependency information
search proceeds.
formalize follows. Given csp (I; V; ), suppose value v
assigned 2 . construct new csp (I 0; V 0 ; 0) involving
remaining variables 0 = ,fig, new set V 0 need mention possible values
Vi i, 0 generated modifying constraints indicate
assigned value v . make following definition:

Definition 7.1 Given csp, suppose variable two possible values u

v. say v stricter u every constraint csp induced assigning
u constraint csp induced assigning value v.
point, course, v stricter u is, point trying
solution involving v u eliminated. all, finding solution would
involve satisfying constraints v restriction, superset
u restriction, unable satisfy constraints u restriction originally.
example began section generalizes following:

Proposition 7.2 Suppose csp involves set variables,
partial solution assigns values variables subset P . Suppose
extend partial solution assigning value u variable 62 P ,
extension solution entire csp. consider csp involving
variables , P induced choices values variables P . v stricter
u choice value problem, original csp solution
assigns v extends given partial solution P .
40

fiDynamic Backtracking

proposition isn't quite enough; earlier example, choice = 17
stricter = 16 task needs scheduled is.
need record fact B (which longer assigned value) source
diculty. this, need augment dependency information
working.
precisely, say set variables fxi g eliminates value v variable
x, mean search date allowed us conclude
(v1 = x1) ^ ^ (vk = xk ) v 6= x
vi current choices xi . obviously rewrite
(v1 = x1 ) ^ ^ (vk = xk ) ^ (v = x) F
(5)
F indicates csp question solution.
Let's specific still, indicating (5) exactly csp solution:
(v1 = x1 ) ^ ^ (vk = xk ) ^ (v = x) F (I )
(6)
set variables complete csp.
address example began section; csp
known fail expression (6) entire problem, subset it.
example, considering, subproblem involves two tasks B .
general, augment nogoods include information subproblems
fail, measure strictness respect restricted subproblems
only. example, indeed allow us eliminate = 17 consideration
possible time A.
additional information stored nogoods doubles size (we store
second subset variables csp), variable sets involved manipulated
easily search proceeds. cost involved employing technique therefore
strictness computation. may substantial given data structures currently
used represent csps (which typically support need check constraint
violated little more), seems likely compile-time modifications data
structures used make strictness question easier answer. scheduling
problems, preliminary experimental work shows idea important one; here,
too, much done.
basic lesson dynamic backtracking retaining nogoods
still relevant given partial solution working, storage diculties
encountered full dependency-directed methods alleviated. makes
ideas proposed possible { erasing values, selecting alternate backtrack
points, dependency pruning. surely many effective uses practical
dependency maintenance system well.

Acknowledgements
work supported Air Force Oce Scientific Research grant
number 92-0693 DARPA/Rome Labs grant number F30602-91-C-0036.
41

fiGinsberg

would thank Rina Dechter, Mark Fox, Geddis, Harvey, Vipin Kumar,
Scott Roy Narinder Singh helpful comments ideas. Ari Jonsson
David McAllester provided invaluable assistance experimentation proofs
respectively.

A. Proofs
Lemma 2.4 Let complete elimination mechanism csp, let P partial solution

csp let 62 P . P successfully extended complete solution
assigning value v , v 62 b(P; i).

Proof. Suppose otherwise, (v; E ) 2 (P; i). follows directly completeness


E \ (P , P ) 6=

contradiction.

Lemma 2.6 point execution Algorithm 2.5, last element partial

solution P assigns value variable i, unexplored siblings current node
assign values Vi , Ei .

Proof. first note decide assign value new variable step 2

algorithm, take Ei = b(P; i) Vi , Ei set allowed values
variable. lemma therefore holds case. fact continues hold
repetition loop steps 3 4 simple induction; point,
add Ei node failed possible value assigned i.

Proposition 2.7 Algorithm 2.5 equivalent depth-first search therefore complete.
Proof. easy consequence lemma. Partial solutions correspond nodes

search space.
Lemma 3.2 Let P partial solution obtained execution Algorithm 3.1,
let 2 P variable assigned value P . P 0 P successfully extended
complete solution assigning value v (v; E ) 2 Ei , must

E \ (P , P 0) 6=

Proof. proof Lemma 2.6, show step Algorithm 3.1 cause
Lemma 3.2 become false.
lemma holds step 2, search extended consider new
variable, immediate consequence assumption elimination mechanism
complete.
step 4, add (vj ; E , fj g) set eliminating explanations j ,
simply recording fact search solution j set vj failed
unable extend solution i. consequence inductive hypothesis
long variable E , fj g changes, conclusion remain valid.
Proposition 3.4 Backjumping complete always expands fewer nodes depthfirst search.

42

fiDynamic Backtracking

Proof. fewer nodes examined clear; completeness, follows Lemma
3.2 backtrack element E step 5 always necessary solution
found.
Proposition 3.5 amount space needed backjumping o(i2v), = jI j
number variables problem v number values variable
largest value set Vi .
Proof. amount space needed dominated storage requirements elimination sets Ej ; these. one might refer possible values
particular variable j ; space needed store reason value j eliminated
jI j, since reason simply list variables assigned values.
never two eliminating explanations variable, since concise
never rebind variable value eliminated.
Theorem 4.2 Dynamic backtracking always terminates complete. continues

satisfy Proposition 3.5 expected expand fewer nodes backjumping provided
goal nodes distributed randomly search space.

Proof. four things need show: dynamic backtracking needs o(i2v)

space, complete, expected expand fewer nodes backjumping,
terminates. prove things order.
Space clear; amount space needed continues bounded structure
eliminating explanations.
Completeness clear, since Lemma 3.2, eliminating explanations
retained algorithm obviously still valid. new explanations added (2)
obviously correct, since indicate j cannot take value vj backjumping
j cannot take values eliminated variables backjumped
over.
Eciency see expect expand fewer nodes, suppose subproblem
involving variables jumped solutions total, one given
existing variable assignments. Assuming solutions distributed randomly
search space, least 1=s chance particular solution leads
solution entire csp; so, reordered search { considers solution earlier
{ save expense either assigning new values variables
repeating search led existing choices. reordered search benefit
information nogoods retained variables jumped
over.
Termination dicult part proof.
work algorithm, generating (and discarding) variety
eliminating explanations. Suppose e explanation, saying j cannot
take value vj values currently taken variables set eV .
denote variables eV x1 ; . . . ; xk current values v1; . . . ; vk .
declarative terms, eliminating explanation telling us
(x1 = v1) ^ ^ (xk = vk ) j 6= vj
43

(7)

fiGinsberg

Dependency-directed backtracking would us accumulate nogoods; dynamic
backtracking allows us drop particular instance (7) antecedent
longer valid.
reason dependency-directed backtracking guaranteed terminate
set accumulated nogoods eliminates monotonically increasing amount search
space. nogood eliminates new section search space nature
search process node examined consistent nogoods
accumulated thus far; process monotonic nogoods retained throughout
search. arguments cannot applied dynamic backtracking, since nogoods
forgotten search proceeds. make analogous argument.
this, suppose discover nogood (7), record
variables precede variable j partial order, together values currently
assigned variables. Thus eliminating explanation becomes essentially nogood
n form (7) together set variable/value pairs.
define mapping (n; ) changes antecedent (7) include assumptions variables bound , = fsi ; vi g,

(n; ) = [(s1 = v1) ^ ^ (sl = vl) j 6= vj ]

(8)

point execution algorithm, denote N conjunction
modified nogoods form (8).
make following claims:
1. eliminating explanation (n; ), n j= (n; ) (n; ) valid
problem hand.
2. new eliminating explanation (n; ), (n; ) consequence N .
3. deductive consequences N grow monotonically dynamic backtracking
algorithm proceeds.
theorem follow three observations, since know N valid
set conclusions search problem making monotonic
progress toward eliminating entire search space concluding problem
unsolvable.
(n; ) consequence (n; ) clear, since modification used obtain
(8) (7) involves strengthening antecedent (7). clear (n; )
consequence nogoods already obtained, since added antecedent
conditions hold node search space currently examination.
(n; ) consequence nogoods obtained thus far, node would
considered.
last observation depends following lemma:

Lemma A.1 Suppose x variable assigned value partial solution

x appears antecedent nogood n pair (n; ). 0 set
variables assigned values later x, 0 .
44

fiDynamic Backtracking

Proof. Consider 2 0, suppose . cannot = x, since

would mentioned nogood n therefore . suppose
actually assigned value earlier x is. (n; ) added set
eliminating explanations, must case x assigned value (since
appears antecedent n) not. know
later time assigned value x not, since precedes x current
partial solution. means x must changed value point (n; )
added set eliminating explanations { (n; ) would deleted
happened. contradiction completes proof.
Returning proof Theorem 4.2, suppose eventually drop (n; )
collection nogoods so, new nogood added (n0; 0).
follows lemma 0 . Since xi = vi clause antecedent (n; ),
follows (n0 ; 0) imply negation antecedent (n; ) therefore
imply (n; ) itself. Although drop (n; ) drop nogood (n; ), (n; )
continues entailed modified set N , consequences seen
growing monotonically.

References

Bruynooghe, M. (1981). Solving combinatorial search problems intelligent backtracking.
Information Processing Letters, 12 (1), 36{39.
de Kleer, J. (1986). assumption-based truth maintenance system. Artificial Intelligence,
28, 127{162.
Dechter, R., & Meiri, I. (1989). Experimental evaluation preprocessing techniques
constraint satisfaction problems. Proceedings Eleventh International Joint
Conference Artificial Intelligence, pp. 271{277.
Gaschnig, J. (1979). Performance measurement analysis certain search algorithms.
Tech. rep. CMU-CS-79-124, Carnegie-Mellon University.
Ginsberg, M. L., Frank, M., Halpin, M. P., & Torrance, M. C. (1990). Search lessons learned
crossword puzzles. Proceedings Eighth National Conference Artificial
Intelligence, pp. 210{215.
Ginsberg, M. L., & Harvey, W. D. (1992). Iterative broadening. Artificial Intelligence, 55,
367{383.
Jonsson, A. K., & Ginsberg, M. L. (1993). Experimenting new systematic nonsystematic search techniques. Proceedings AAAI Spring Symposium AI
NP-Hard Problems Stanford, California.
McAllester, D. A. (1993). Partial order backtracking. Journal Artificial Intelligence
Research, 1. Submitted.
Minton, S., Johnston, M. D., Philips, A. B., & Laird, P. (1990). Solving large-scale constraint satisfaction scheduling problems using heuristic repair method. Proceedings Eighth National Conference Artificial Intelligence, pp. 17{24.
45

fiGinsberg

P. Purdom, C. B., & Robertson, E. (1981). Backtracking multi-level dynamic search
rearrangement. Acta Informatica, 15, 99{114.
Seidel, R. (1981). new method solving constraint satisfaction problems. Proceedings
Seventh International Joint Conference Artificial Intelligence, pp. 338{342.
Selman, B., Levesque, H., & Mitchell, D. (1992). new method solving hard satisfiability
problems. Proceedings Tenth National Conference Artificial Intelligence.
Smith, D. E., & Genesereth, M. R. (1985). Ordering conjunctive queries. Artificial Intelligence, 26 (2), 171{215.
Stallman, R. M., & Sussman, G. J. (1977). Forward reasoning dependency-directed
backtracking system computer-aided circuit analysis. Artificial Intelligence,
9 (2), 135{196.
Zabih, R. (1990). applications graph bandwidth constraint satisfaction problems.
Proceedings Eighth National Conference Artificial Intelligence, pp. 46{51.
Zabih, R., & McAllester, D. A. (1988). rearrangement search strategy determining
propositional satisfiability. Proceedings Seventh National Conference
Artificial Intelligence, pp. 155{160.

46


