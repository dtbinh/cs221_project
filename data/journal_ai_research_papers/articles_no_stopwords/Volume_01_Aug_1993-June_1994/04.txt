Journal Artificial Intelligence Research 1 (1993) 91-107

Submitted 8/93; published 11/93

Diculties Learning Logic Programs Cut
Francesco Bergadano

bergadan@di.unito.it

Daniele Gunetti
Umberto Trinchero

gunetti@di.unito.it
trincher@di.unito.it

Universita di Catania, Dipartimento di Matematica,
via Andrea Doria 6, 95100 Catania, Italy
Universita di Torino, Dipartimento di Informatica,
corso Svizzera 185, 10149 Torino, Italy

Abstract

real logic programmers normally use cut (!), effective learning procedure logic
programs able deal it. cut predicate procedural
meaning, clauses containing cut cannot learned using extensional evaluation method,
done learning systems. hand, searching space possible
programs (instead space independent clauses) unfeasible. alternative solution
generate first candidate base program covers positive examples,
make consistent inserting cut appropriate. problem learning programs
cut investigated seems natural reasonable
approach. generalize scheme investigate diculties arise.
major shortcomings actually caused, general, need intensional evaluation.
conclusion, analysis paper suggests, precise technical grounds,
learning cut dicult, current induction techniques probably restricted
purely declarative logic languages.

1. Introduction

Much recent research AI Machine Learning addressing problem learning
relations examples, especially title Inductive Logic Programming (Muggleton, 1991). One goal line research, although certainly one,
inductive synthesis logic programs. generally, interested construction
program development tools based Machine Learning techniques. techniques
include ecient algorithms induction logical descriptions recursive relations.
However, real logic programs contain features purely logical, notably
cut (!) predicate. problem learning programs cut studied
Inductive Logic Programming, paper analyzes diculties involved.

1.1 Learn Programs Cut?

two main motivations learning logic programs cut:
1. ILP provide practical tools developing logic programs, context
general program development methodology (e.g., (Bergadano, 1993b)); real
size logic programs normally contain cut, learning cut important creating
integrated Software Engineering framework.

c 1993 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiBergadano, Gunetti, & Trinchero

2. Extensive use cut make programs sensibly shorter, diculty learning
given logic program much related length.
objectives, need cuts make programs
ecient without changing input-output behavior (\green cuts"), cuts
eliminate possible computed results (\red cuts"). Red cuts sometimes considered
bad programming style, often useful. Moreover, red cuts effective
making programs shorter. Green cuts important, less controversial.
correct program inferred via inductive methods, could made ecient
insertion green cuts, either manually means automated program
transformation techniques (Lau & Clement, 1993).

1.2 Standard Approaches Cannot Used?
Machine Learning algorithms generate rules clauses one time independently
other: rule useful (it covers positive example) correct (it
cover negative example), added description program
generated, positive examples covered. means searching
space possible clauses, without backtracking. obviously great advantage,
programs sets clauses, therefore space possible programs exponentially
larger.
one principle allows simplification problem extensional
evaluation possible clauses, used determine whether clause C covers example
e. fact clause C covers example e used approximation
fact logic program containing C derives e. Consider, instance, clause C =
\p(X,Y) ff", suppose example e p(a,b). order see whether C covers e,
extensionality principle makes us evaluate literal true matches
given positive example. instance, = q(X,Z) ^ p(Z,Y), example p(a,b)
extensionally covered iff ground term c q(a,c) p(c,b) given
positive examples. particular, order obtain truth value p(c,b),
need call clauses learned previously. reason, determining
whether C covers e depends C positive examples. Therefore, learning
system decide whether accept C part final program P independently
clauses P contain.
extensionality principle found Foil (Quinlan, 1990) derivatives,
used bottom-up methods Golem (Muggleton & Feng, 1990). Shapiro's MIS
system (Shapiro, 1983) uses refining clauses, although backtracing
inconsistencies. used extensional evaluation clauses FILP system
(Bergadano & Gunetti, 1993).
learning programs cut, clauses longer independent standalone extensional evaluation meaningless. cut predicate evaluated, possible clauses proving goal ignored. changes meaning
clauses. Even clause extensionally covers example e, may case
final program derive e, derivation paths eliminated
evaluation cut predicate.
92

fiThe Difficulties Learning Logic Programs Cut

However, exhaustive search space programs prohibitive. Learning methods,
even based extensionality, often considered inecient sucient prior information
available; searching sets clauses exponentially worse. would amount
brute-force enumeration possible logic programs containing cut, program
consistent given examples found.

1.3 Alternative Method?

Cut eliminate computed results, i.e., adding cut program,
may case example longer derived. observation suggests general
learning strategy: base program P induced standard techniques, given positive
maybe negative examples, remaining negative examples ruled
inserting cut clause P. Obviously, inserting cut, must make sure
positive examples may still derived.
Given present technology discussion above, seems viable
path possible solution. Using standard techniques, base program P would generated one clause time, positive examples extensionally covered. However,
think view restrictive, programs derive given positive
examples, although cover extensionally (Bergadano, 1993a; DeRaedt,
Lavrac, & Dzeroski, 1993). generally, consider traces positive examples:

Definition 1 Given hypothesis space possible clauses, example e
`

e, set clauses TS used derivation e called trace e.

use candidate base program P subset union
traces positive examples. PS extensionally covers positive examples,
union traces, converse always true. candidate
program generated, attempt made insert cuts negative examples
derived. successful, solution, otherwise, backtrack another
candidate base program. analyze many problems inherent learning cut
class trace-based learning methods, but, discuss later (Section 4),
problems need faced restrictive framework extensional evaluation.
words, even choose learn base program P extensionally,
try make consistent using cut, computational problems would still arise.
main difference standard approaches based extensionality allow
backtracking guarantee correct solution found (Bergadano, 1993a).
far computational complexity concerned, trace-based methods complexity
standing search space independent clauses (for extensional methods)
exhaustive search space possible programs. need following:

Definition 2 Given hypothesis space S, depth example e maximum

number clauses successfully used derivation e.

example, list processing domain, contains recursive calls
type \P([HjT]) :- ..., P(T), ..." depth example P(L) length L.
practical program induction tasks, often case depth example
93

fiBergadano, Gunetti, & Trinchero

related complexity, hypothesis space S. maximum depth
given positive examples, complexity trace-based methods order
jS jmd, extensional methods enumerate possible clauses complexity
linear jS j, enumerating possible programs exponential jS j.

2. Simple Induction Procedure
trace-based induction procedure analyze takes input finite set clauses
set positive negative examples E+ E- tries find subset
derives positive examples none negative examples. every
positive example e+ 2 E+, assume large enough derive it. Moreover,
assume clauses attened1 . case, clauses attened
preprocessing step.
consider one possible proof ` e+, build intermediate program
containing trace derivation. done positive examples,
corresponding traces merged. Every time updated, checked
negative examples. derived T, cut (!) inserted antecedents
clauses T, consistent program found, exists. case,
procedure backtracks different proof ` e+. algorithm informally
described follows:
input: set clauses
set positive examples E+
set negative examples ES := atten(S)
;
positive example e+ 2 E+
find T1 T1 `SLD e+ (backtracking point 1)
[ T1
derives negative example e- trycut(T,e-)
trycut(T,e-) fails backtrack
output clauses listed
trycut(T,e-):
insert ! somewhere (backtracking point 2)
1. previously covered positive examples still derived T,
2. 6`SLD e-

complexity adding cut somewhere trace T, negative example eis longer derived, obviously depends size T. size depends
depth positive examples, size hypothesis space S. Although
1. clause flattened contain functional symbol. Given un attened clause, alway
possible atten (by turning functions new predicates additional argument representing
result function) vice versa (Rouveirol, press).

94

fiThe Difficulties Learning Logic Programs Cut

clever ways devised, based particular example e-, propose
simple enumerative technique implementation described Appendix.

3. Example: Simplifying List

section show example use induction procedure learn logic
program \simplify ". Simplify takes input list whose members may lists,
transforms \ attened" list single members, containing repetitions
lists members. program appears exercise number 25 (Coelho & Cotta, 1988),
composed nine clauses (plus clauses append member); six
recursive, one doubly-recursive cut extensively used. Even simplify
complex logic program, complex usual ILP test cases. instance,
quicksort partition program, often used, composed five
clauses (plus append), three recursive. Moreover, note
conciseness simplify essentially due extensive use cut. Without cut,
program would much longer. general, longer logic program, dicult
learn it.
consequence, start relatively strong bias; suppose following
hypothesis space N=8449 possible clauses defined user:



clause \simplify(L,NL) :- atten(L,L1), remove(L1,NL)."
clauses whose head \ atten(X,L)" whose body composed conjunction
following literals:
head(X,H), tail(X,L1), equal(X,[L1,T]), null(T), null(H), null(L1), equal(X,[L1]),
atten(H,X1), atten(L1,X2),
append(X1,X2,L), assign(X1,L), assign(X2,L), list(X,L).



clauses whose head \remove(IL,OL)" whose body composed conjunction following literals:
cons(X,N,OL), null(IL), assign([],OL),
head(IL,X), tail(IL,L), member(X,L), remove(L,OL), remove(L,N).



correct clauses null, head, tail, equal, assign, member, append given:
null([]).
head([Hj ],H).
tail([ jT],T).
equal(X,X).
assign(X,X).
member(X,[Xj ]).
member(X,[ jT]) :- member(X,T).
95

fiBergadano, Gunetti, & Trinchero

append([],Z,Z).
append([HjX],Y,[HjZ]) :- append(X,Y,Z).
using various kinds constraints, initial number clauses strongly reduced.
Possible constraints following:
output produced must instantiated again. means
variable cannot occur output antecedent once.
Inputs must used: input variables head clause must occur
antecedent.
conjunctions literals ruled never true, e.g.
null(IL)^head(IL,X).
applying various combination constraints possible strongly restrict
initial hypothesis space, given input learning procedure. set
positive negative examples used learning task is:
simplify pos([[[],[b,a,a]],[]],[b,a]). remove pos([a,a],[a]).
(simplify neg([[[],[b,a,a]],[]],X),not equal(X,[b,a])).
simplify neg([[a,b,a],[]],[a,[b,a]]). remove neg([a,a],[a,a]).
Note define negative examples simplify examples
input given positive example different output, instance simplify neg([[[],[b,a,a]],[]],[a,b]). Obviously, possible give negative examples
normal ground literals. learning procedure outputs program simplify reported
below, turns substantially equivalent one described (Coelho &
Cotta, 1988) (we kept clauses un attened).
simplify(L,NL) :- atten(L,L1), remove(L1,NL).
atten(X,L) :- equal(X,[L1,T]), null(T), !, atten(L1,X2), assign(X2,L).
atten(X,L) :- head(X,H), tail(X,L1), null(H), !, atten(L1,X2), assign(X2,L).
atten(X,L) :- equal(X,[L1]), !, atten(L1,X2), assign(X2,L).
atten(X,L) :- head(X,H), tail(X,L1), !,
atten(H,X1), !, atten(L1,X2), append(X1,X2,L).
atten(X,L) :- list(X,L).
remove(IL,OL) :- head(IL,X), tail(IL,L), member(X,L), !, remove(L,OL).
remove(IL,OL) :- head(IL,X), tail(IL,L), remove(L,N), cons(X,N,OL).
remove(IL,OL) :- null(IL), assign([],OL).
learning task takes 44 seconds implementation. However, obtained
special conditions, thoroughly discussed next sections:
constraints listed applied, final hypothesis space
reduced less one hundred clauses.
96

fiThe Difficulties Learning Logic Programs Cut





Clauses hypothesis space generated correct order, must appear
final program. Moreover, literals clause correct position.
important, since logic program cut relative position clauses
literals significant. consequence, learn simplify without test
different clause literal orderings (see subsections 4.2 4.5).
tell learning procedure use two cuts per clause. seems
quite intuitive constraint since, fact, many classical logic programs
one cut per clause (see subsections 4.1 5.4).

4. Problems

Experiments induction procedure shown many problems arise
learning logic programs containing cut. following, analyze problems,
major contribution present paper. cut cannot evaluated extensionally,
analysis general, depend specific induction method adopted.
possible partial solutions discussed Section 5.

4.1 Problem 1: Intensional Evaluation, Backtracking Cut

learning procedure Section 2 simple, inecient. However,
believe common every intensional method, clauses cannot learned
independently one another. consequence, backtracking cannot avoided
impact complexity learning process. Moreover, cut must
added every trace covering negative examples. constraints force,
range one cut whole trace cut two literals clause
trace. Clearly, number possibilities exponential number literals
trace. Fortunately, number usually much smaller size hypothesis
space, depends depth positive examples.
However, backtracking advantages; particular, useful search
alternative solutions. alternative programs confronted basis
required characteristic, simplicity eciency. example, using backtracking
discovered version simplify equivalent one given without cut predicate
two recursive calls fourth clause flatten.

4.2 Problem 2: Ordering Clauses Trace

logic program containing cut, mutual position clauses significant, different ordering lead different (perhaps wrong) behavior program. example,
following program intersection:

c1) int(X,S2,Y) :- null(X), null(Y).
c2) int(X,S2,Y) :- head(X,H), tail(X,Tail), member(H,S2), !, int(Tail,S2,S), cons(H,S,Y).
c3) int(X,S2,Y) :- head(X,H), tail(X,Tail), int(Tail,S2,Y).
behaves correctly c2 comes c3. Suppose hypothesis space given input
induction procedure consists three clauses above, c3
97

fiBergadano, Gunetti, & Trinchero

c2. :int([a],[a],[]) given negative example, learning task fails,
clauses c1 c3 derive example.

words, learning program containing cut means learn set
clauses, specific ordering clauses. terms induction procedure
means every trace covering negative example, must check
every position inserting cuts, every possible clause ordering trace.
\generate test" behavior dicult implement, dramatically decrease
performance learning task. worst case possible permutations must
generated checked, requires time proportional (md)! trace md
clauses2 .
necessity test different permutations clauses trace primary source
ineciency learning programs cut, probably dicult problem
solve.

4.3 Problem 3: Kinds Given Examples

induction procedure able learn programs traces, i.e. every
clause program used derive least one positive example. learning definite
clauses, problem, derivation monotone, every program P,
complete consistent w.r.t. given examples, program P0P
complete consistent trace3. hand, learning clauses containing cut, may happen complete consistent program(s) hypothesis
space neither trace, contains subset. derivation longer
monotone case negative example derived set clauses,
superset them, following simple example:
= fsum(A,B,C) :- A>0, !, A-1, sum(M,B,N), C N+1.
sum(A,B,C) :- C B.g
sum pos(0,2,2), sum neg(2,2,2).
two clauses hypothesis space represent complete consistent program
given examples, procedure unable learn it. Observe negative
example derived second clause, trace positive example,
first second together.
problem avoided require that, every negative example, corresponding positive example input given (in case, example
required sum pos(2,2,4)). way, complete program exists hypothesis
space, trace, learned. made consistent using
cut, order rule derivation negative examples. constraint positive
negative examples seems quite intuitive. fact, writing program,
2. must noted learning programs two different predicates, j k clauses
respectively (that is, md = j +k), consider (j +k)! different programs,
j !+k!. better if, inside program, known non-recursive clauses fixed
position, put recursive clauses.
3. learned program P complete derives given positive examples, consistent
derive given negative examples

98

fiThe Difficulties Learning Logic Programs Cut

programmer usually thinks terms program compute given inputs,
tries avoid wrong computations inputs.

4.4 Problem 4: Ordering Given Examples
learning clauses cut, even order positive examples may significant.
example above, sum pos(2,2,4) comes sum pos(0,2,2) learning task
fails learn correct program sum, cannot find program consistent w.r.t.
first positive example negative one(s).
general, given set positive examples problem remedied
testing different example orderings. Again, worst case k! different orderings set
k positive examples must checked. Moreover, situations favorable ordering
exist. Consider following hypothesis space:

c1) int(X,Y,W) :- head(X,A), tail(X,B), notmember(A,Y), int(B,Y,W).
c2) int(X,Y,W) :- head(X,A), tail(X,B), notmember(A,Y), !, int(B,Y,W).
c3) int(X,Y,Z) :- head(X,A), tail(X,B), int(B,Y,W), cons(A,W,Z).
c4) int(X,Y,Z) :- head(X,A), tail(X,B), !, int(B,Y,W), cons(A,W,Z).
c5) int(X,Y,Z) :- null(Z).
together set examples:

e1 ) int pos([a],[b],[ ]).
e2 ) int pos([a],[a],[a]).
e3 ) int neg([a],[b],[a]).
e4 ) int neg([a],[a],[ ]).
induction procedure able find correct program ordering
two positive examples, even program exist ([c2,c4,c5]). program
union two traces: [c2,c5], covers e1 , [c4,c5], covers e2 . traces
inconsistent, first covers e4 , second covers e3 . problem
remedied positive examples derived check negative
examples done.
However, case loss eciency, inconsistent
traces discarded end. words, would need learn program
covering positive examples, make consistent using cut reordering clauses. Moreover, way make program consistent using cut
reorderings. consequence, time used build program wasted.
example, suppose given following hypothesis space:

c01) int(X,Y,Z) :- head(X,A), tail(X,B), int(B,Y,W), cons(A,W,Z).
c02) int(X,Y,Z) :- null(X), null(Z).
c03) int(X,Y,Z) :- null(Z).
99

fiBergadano, Gunetti, & Trinchero

examples:

e01) int pos([a],[a],[a]).
e02 ) int pos([a,b],[c],[]).
e03 ) int neg([a],[b],[a]).
learn trace [c01,c02] e01 trace [c03] e02 . [c01,c02,c03] covers
e03, way make consistent using cut reordering clauses. fact,
first partial trace responsible inconsistency, hence time used learn
[c03] totally wasted.
possible understand need attened clauses. Consider following program intersection, equivalent [c2,c4,c5], three clauses
un attened:

u2 ) int([AjB],Y,W) :- notmember(A,Y), !, int(B,Y,W).
u4 ) int([AjB],Y,[AjW]) :- !, int(B,Y,W).
u5 ) int( , ,[]).
Now, program covers int neg([a],[a],[]), i.e. [u2 ,u4,u5 ] ` int([a],[a],[]). fact, clause
u2 fails example member [a]. Clause u4 fails empty
list cannot matched [AjW]. clause u5 succeeds arguments match
negative example. consequence, program would rejected
induction procedure.
problem that, use un attened clauses, may happen clause body
evaluated example match head clause. consequence,
possible cuts clause evaluated cannot uence behavior entire
program. example, cut clause u4 effect output argument
int([a],[a],[]) match [AjW], body u4 evaluated all. u5
fired negative example covered. attened version, clause c4 fails
cons(a,[],[]) reached, point cut force clause c5 cannot activated.
Note program [u2 ,u4,u5] behaves correctly query int([a],[a],X), gives X=[a]
output.

4.5 Problem 5: Ordering Literals

Even relative position literals cut clause significant. Consider
correct program intersection ([c2,c4,c5]), c4 modified putting
cons literal front antecedent:

c04) int(X,Y,Z) :- cons(A,W,Z), head(X,A), tail(X,B), int(B,Y,W).
Then, way get correct program intersection using clause. rule
negative example int neg([a],[a],[]) must put cut cons predicate,
order prevent activation c5. But, then, positive examples longer
covered, int pos([a],[],[]). fact, wrong behavior every time clause c04
100

fiThe Difficulties Learning Logic Programs Cut

called fails, since prevents activation c5 . general, problem cannot
avoided even reordering clauses: put c04 c2 c5 , int neg([a],[a],[])
covered. consequence, test every possible permutation literals
every clause candidate program.

5. Situations Learning Cut still Practical
analysis, learning cut appears dicult since, general, learning
procedure able backtrack candidate base programs (e.g., traces),
position cut(s) program, order clauses program,
order literals clauses order given positive examples. However,
spotted general conditions learning cut could still practical. Clearly,
conditions cannot final solution learning cut, but, applicable, alleviate
computational problems task.

5.1 Small Hypothesis Space

First all, restricted hypothesis space necessary. clauses cannot learned independently one another, small hypothesis space would help limit backtracking
required candidate traces (problem 1). Moreover, even number clauses trace
would probably smaller, hence number different permutations
number different positions inserted cuts (problems 2 1). small trace would
slight positive impact need test different literal orderings clauses
(problem 5).
general, many kinds constraints applied keep hypothesis space small,
ij-determinism (Muggleton & Feng, 1990), rule sets schemata (Kietz & Wrobel,
1991; Bergadano & Gunetti, 1993), determinations (Russell, 1988), locality (Cohen, 1993),
etc (in fact, restrictions others, listed Section 3,
available actual implementation procedure - see Appendix4 ). Moreover,
candidate recursive clauses must designed infinite chains recursive calls
take place (Bergadano & Gunetti, 1993) (otherwise learning task could
non-terminating). general, number possible recursive calls must kept small,
order avoid much backtracking searching possible traces. However, general
constraints may sucient. hypothesis space must designed carefully
beginning, dicult. example learning simplify initial
hypothesis space \only" 8449 clauses obtained specifying set required
predicates, even variables occurring every literal.
clauses cannot learned independently, experiments shown us dramatic improvement learning task obtained generating clauses
hypothesis space recursive clauses, general complex clauses, taken
consideration simpler non-recursive ones. Since simpler non recursive
clauses require less time evaluated, small impact learning time.
Moreover, learning simpler clauses (i.e. shorter) alleviates problem 5.
4. found constraints particularly useful. using often able restrict hypothesis
space one order magnitude without ruling possible solution.

101

fiBergadano, Gunetti, & Trinchero

Finally, must noted induction procedure necessarily require
hypothesis space possible clauses represented explicitly. learning task could
start empty set implicit description hypothesis space, example
one given Section 3. positive example cannot derived S, new clause
asked clause generator added S. step repeated example
derivable updated S, learning task proceed normally.

5.2 Simple Examples

Another improvement achieved using examples simple possible.
fact, example may involve recursive call potentially responsible
activation corresponding clauses hypothesis space. complex
example, larger number consecutive recursive activations clauses larger
number traces considered backtracking (problem 1). instance, learn
append relation, may sucient use example append([a],[b],[a,b]) instead
one append([a,b,c,d],[b],[a,b,c,d,b]). Since simple examples would probably require
smaller number different clauses derived, would result smaller traces,
alleviating problem permutation clauses literals trace (problems 2 5)
decreasing number positions cuts (problem 1).

5.3 Small Number Examples

Since candidate program formed taking union partial traces learned single
examples, want small trace (problems 2 5) must use examples
possible, still completely describing required concept. words,
avoid redundant information. example, want learn program append,
normally sucient use one two positive examples append([a],[b],[a,b])
append([c],[d],[c,d]). Obviously may happen different examples derived
set clauses, case final program change.
check possible orderings set positive examples, small number
examples solution problem 4. Fortunately, experiments shown normally
positive examples needed learn program, hence corresponding
number different orderings is, case, small number. Moreover, since
method positive example sucient learn clauses necessary derive it,
time complete program learned using one well chosen example.
example found (as case learning task section 3, one
example simplify one remove given), computational problem testing
different example orderings automatically solved.
However, must noted that, general, small number examples may
sucient, except simple programs. fact, want learn logic programs
member, append, reverse on, example involving recursion
sucient. complex programs choice may trivial. example,
procedure able learn quicksort (plus partition) program one \good"
example. one know quicksort partition work, likely
provide example allowing learn partial description partition.
particularly clear example simplify . used positive example
102

fiThe Difficulties Learning Logic Programs Cut

simplify pos([[[],[b,a,a]]],[b,a]) (which close one effectively used), first clause
flatten would learned. words, give examples must give
good examples, often possible mind (at least partially
informal way) target program. Moreover, complex programs, good examples
mean complex examples, contrast previous requirement.
studies learning good examples refer reader work Ling (1991)
Aha, Ling, Matwin Lapointe (1993).

5.4 Constrained Positions Cut Literals

Experiments shown practical allow learning procedure test
possible positions cut trace, even able keep number clauses
trace small. user must able indicate positions cut allowed
occur, e.g., beginning clause body, recursive call. case, many
alternative programs cut automatically ruled thus tested
negative examples. may useful limit maximum number cuts
per clause per trace. example, time one cut per clause sucient
learn correct program. actual implementation procedure, fact
possible specify exact position cut w.r.t. literal group literals within
clause hypothesis space, information known.
eliminate need test different ordering literals (problem 5), may
impose particular global order, must maintained every clause hypothesis
space. However requires deep knowledge program want, otherwise
(or even all) solutions lost. Moreover, solution contrast use
constrained positions cut, since solution program particular literal ordering
particular positions cuts may exist.

6. Conclusion

induction procedure based intensional evaluation clauses. Since cut
predicate declarative meaning, believe intensional evaluation clauses
cannot abandoned, independently kind learning method adopted.
decrease performance learning task, compared extensional methods,
examine clauses one time without backtracking. However, computational problems
outlined Section 4 remain even choose learn complete program extensionally,
try make consistent inserting cut. difference
backtracking (problem 1), situation probably worse, since extensional
methods fail learn complete program even exists hypothesis space.
(Bergadano, 1993a).
Even ability learn clauses containing procedural predicates cut seems
fundamental learning \real" logic programs, particular short ecient programs,
many problems uencing complexity learning task must faced. include
number relative ordering clauses literals hypothesis space, kind
relative ordering given examples. problems seem related need
intensional evaluation clauses general, particular learning method
adopted. Even alleviate problems, seems necessary know lot
103

fiBergadano, Gunetti, & Trinchero

target program. alternative solution simply ignore problems. is,
avoid testing different clause and/or literal and/or example orderings. Clearly,
way learning process become feasible, fail find solution even
exists. However, many ILP systems (such Foil) adopt \incomplete-but-fast"
approach, guided heuristic information.
consequence, view results presented paper as, least partially, negative. problems raised appear computationally dicult, suggest attention
restricted purely declarative logic languages, are, case, suciently
expressive.

Acknowledgements
work part supported BRA ESPRIT project 6020 Inductive Logic Programming.

Appendix
induction procedure Section 2 written C-prolog (interpreted) runs
SUNsparcstation 1. planning translate QUINTUS prolog. Appendix
contains simplified description implementation. preliminary step, order
record trace clauses deriving positive example e+, every clause hypothesis
space5 must numbered modified adding body two literals. first
one, allowed(n,m) used activate clauses must checked
negative examples. second one, marker(n), used remember clause number n
successfully used deriving e+. Hence, general, clause hypothesis
space takes following form:

P (X1 ,: : : ,Xm) :- allowed(n,m), ,marker(n).
actual body clause, n number clause set
number used deal cuts. every clause n, one without cut augmented
allowed(n,0), containing cut somewhere body augmented
allowed(n,1), allowed(n,2), ..., on. Moreover, every augmented clause above,
fact \alt(n,m)." inserted S, order implement enumeration mechanism.
simplified (but running) version learning algorithm reported below.
algorithm, output, any, variable Trace containing list (numbers the)
clauses representing learned program P. using backtracking mechanism Prolog,
one solution (trace) found. assume two predicates listpositive
listnegative build list given positive negative examples, respectively.
consult(file containing set clauses S).
5. assume clauses hypothesis space attened

104

fiThe Difficulties Learning Logic Programs Cut

allowed(X,0).
marker(X) :- assert(trace(X)).
marker(X) :- retract(trace(X)), !, fail.
main :- listpositive(Posexamplelist), tracer([],Posexamplelist,Trace).
tracer(Covered,[ExamplejCdr],Trace) :- Example, /? backtracking point 1 ?/
setof(L,trace(L),Trace1),
notneg(Trace1,[ExamplejCovered],Cdr),
tracer([ExamplejCovered],Cdr,Trace).
tracer( ,[],Trace) :- setof((I,J),allowed(I,J),Trace), asserta((marker(X) :- true, !)).
assertem([]).
assertem([IjCdr]) :- alt(I,J), backassert(allowed(I,J)), assertem(Cdr).
prep(T) :- retract(allowed(X,0)), assertem(T).
backassert(X) :- assert(X).
backassert(X) :- retract(X), !, fail.
resetallowed([]) :- !.
resetallowed( ) :- abolish(allowed,2), assert(allowed(X,0)), !.
notneg(T,Covered,Remaining) :- listnegative([]).
notneg(T,Covered,Remaining) :- listnegative(Negexamplelist),
asserta((marker(X) :- true,!)),
prep(T), /? backtracking point 2 ?/
trypos(Covered), trynegs(Negexamplelist),
resetallowed(Remaining),
retract((marker(X) :- true,!)).
notneg(T,Covered,Remaining) :- resetallowed(Remaining),
retract((marker(X) :- true,!)), !, fail.
trypos([ExamplejCdr]) :- Example, !, trypos(Cdr).
trypos([]) :- !.
trynegs([ExamplejCdr]) :- Example,!,fail.
trynegs([ExamplejCdr]) :- trynegs(Cdr).
trynegs([]) :- !.
Actually, complete implementation complex, order achieve greater
eciency. behavior learning task quite simple. Initially, set clauses
read Prolog interpreter, together learning algorithm. learning
task started calling predicate main. list positive examples formed
105

fiBergadano, Gunetti, & Trinchero

tracer procedure called list. every positive example, tracer calls
example itself, firing clauses may resolved example.
Observe that, initially, allowed(X,0) predicate asserted database: way
clauses containing cut allowed used (this clauses cut
employed negative example derived). Then, trace, any, (the numbers
associated to) clauses successfully used derivation example built, using
setof predicate.
trace added traces found previous examples, result
checked set negative examples calling notneg procedure. notneg
fail (i.e. negative examples covered trace) new positive
example taken consideration. Otherwise notneg modifies trace cut
tests again. fails, backtracking occurs new trace current example
(and possibly previous ones) searched for.
notneg procedure works follows. First, clauses trace allowed
checked negative examples, retracting allowed(X,0) clause
asserting allowed(n,0) n-th clause (without cut) trace. done
prep assertem predicates. list negative examples formed
check derived clauses trace. least one negative example
covered, (i.e., trynegs fails) backtrack prep procedure (backtracking point
2) clause trace substituted equivalent one cut inserted
somewhere (or different position). correct program found way
trying possible alternatives (i.e. using cut possible ways), notneg fails,
backtracking backtracking point 1 occurs, another trace searched for. Otherwise,
clauses without cut reactivated asserting allowed(X,0), next
positive example considered. Note trypos used notneg verify modified
trace still derives set positive examples derived initially. possibility substitute
clauses current trace others cut inserted somewhere achieved
alt predicate assertem procedure. Finally, note simplified version
learning procedure able generate test different orderings clauses
trace different ordering literals clause, use different orderings
set positive examples.
order derive positive examples check negative ones
(see subsection 4.4), must change first clause tracer procedure into:
tracer([Pos1, ... ,Posn]):-Pos1, ... ,Posn, setof(L,trace(L),T), notneg(T).
actual implementation induction procedure available ftp.
information contact gunetti@di.unito.it.

References

Aha, D., Ling, C., Matwin, S., & Lapointe, S. (1993). Learning Singly Recursive Relations
Small Datasets. Proceedings IJCAI-93 workshop ILP.
Bergadano, F. (1993a). Inductive database relations. IEEE Transactions Data
Knowledge Engineering, 5 (6).
106

fiThe Difficulties Learning Logic Programs Cut

Bergadano, F. (1993b). Test Case Generation Means Learning Techniques. Proceedings ACM SIGSOFT-93.
Bergadano, F., & Gunetti, D. (1993). interactive system learn functional logic programs. Proceedings IJCAI-93.
Coelho, H., & Cotta, J. C. (1988). Prolog Example: learn teach use it. Berlin:
Springer-Verlag.
Cohen, W. (1993). Rapid Prototyping ILP Systems Using Explicit Bias. Proceedings
IJCAI-93 workshop ILP.
DeRaedt, L., Lavrac, N., & Dzeroski, S. (1993). Multiple predicate learning. Proceedings
IJCAI-93.
Kietz, J. U., & Wrobel, S. (1991). Controlling Complexity Learning Logic
Syntactic Task-Oriented Models. Muggleton, S. (Ed.), Inductive Logic Programming. London: Academic Press.
Lau, K. K., & Clement, T. (Eds.). (1993). Logic Program Synthesis Transformation.
Berlin: Springer-Verlag.
Ling, X. C. (1991). Learning Good Examples. Proceedings IJCAI-91.
Muggleton, S. (Ed.). (1991). Inductive Logic Programming. London: Academic Press.
Muggleton, S., & Feng, C. (1990). Ecient Induction Logic Programs. Proceedings
first conference Algorithmic Learning Theory.
Quinlan, R. (1990). Learning Logical Definitions Relations. Machine Learning, 5,
239{266.
Rouveirol, C. (in press). Flattening: representation change generalization. Machine
Learning.
Russell, S. (1988). Tree-structured bias. Proceedings AAAI-88.
Shapiro, E. Y. (1983). Algorithmic Program Debugging. Cambridge, CA: MIT Press.

107


