journal artificial intelligence

submitted published

learning past tense english verbs
symbolic pattern associator vs connectionist
charles x ling

ling csd uwo ca

department computer science
university western ontario
london ontario canada n b

abstract

learning past tense english verbs seemingly minor aspect language acquisition generated heated debates since become landmark task
testing adequacy cognitive modeling several artificial neural networks anns
implemented challenge better symbolic posed
present general purpose symbolic pattern associator spa upon
decision tree learning id conduct extensive head head comparisons
generalization ability ann spa different representations conclude spa generalizes past tense unseen verbs better
ann wide margin offer insights case
discuss default strategy decision tree learning

introduction
learning past tense english verbs seemingly minor aspect language acquisition
generated heated debates since first connectionist implementation rumelhart mcclelland rumelhart mcclelland claimed
use acquisition human knowledge language best formulated ann
artificial neural network without symbol processing postulates existence
explicit symbolic representation rules since learning past tense become landmark task testing adequacy cognitive modeling years
number criticisms connectionist modeling appeared pinker prince lachter
bever prasada pinker ling cherwenka marinov criticisms
centered mainly upon issues high error rates low reliability experimental inappropriateness training testing procedures hidden features
representation network architecture facilitate learning well opaque
knowledge representation networks several subsequent attempts improving
original ann made plunkett marchman cottrell plunkett macwhinney leinbach daugherty seidenberg
notably macwhinney leinbach constructed multilayer neural network
backpropagation bp attempted answer early criticisms hand
supporters symbolic believe symbol structures parse trees
propositions etc rules manipulations critical cognitive level
connectionist may provide account neural structures
traditional symbol processing cognitive architecture implemented fodor
pylyshyn pinker prasada pinker argue proper

c ai access foundation morgan kaufmann publishers rights reserved

filing

accounting regular verbs dependent upon production rules irregular
past tense ections may generalized ann associative memory
proper way debating adequacy symbolic connectionist modeling
contrasting competitive implementations thus symbolic implementation needed
compared ann fact challenge posed macwhinney
leinbach assert symbolic methods would work well
model section titled better symbolic model claim
provided even accurate
characterization learning process might still forced reject
connectionist despite successes proper way debating conceptualizations contrasting competitive implementations
present case would need symbolic implementation could contrasted
current implementation macwhinney leinbach page
present general purpose symbolic pattern associator spa
upon symbolic decision tree learning id quinlan shown
ling marinov spa much psychologically realistic
ann compared human subjects issue predictive accuracy
macwhinney leinbach report important model unseen
regular verbs reply criticism macwhinney implemented ann
model claimed raw generalization power close spa
believed case systems learn data set
good reason equivalent performance two
two computationally powerful systems given
set input data extract every bit data regularity input
without processing much blood squeezed
turnip systems spa ann extracted
could macwhinney page
case obviously reasons one learning
outperforms another otherwise study different learning
occam razor principle preferring simplest hypothesis complex
ones creates preference biases learning preference bias preference
order among competitive hypotheses hypothesis space different learning
however employ different ways measuring simplicity thus concepts bias
different well learning program generalizes depends upon degree
regularity data fits bias study compare raw generalization
ability symbolic ann task learning past tense english
verbs perform extensive head head comparisons ann spa
effects different representations encodings generalization abilities
experimental demonstrate clearly
distributed representation feature connectionists advocating
lead better generalization compared symbolic representation arbitrary error correcting codes proper length


filearning past tense symbolic vs connectionist

anns cannot learn identity mapping preserves verb stem past
tense well spa
representation suggested macwhinney improves predictive accuracy spa ann spa still outperforms ann
sum spa generalizes past tense unseen verbs better ann
wide margin
section discuss reasons spa better learning model
task english past tense acquisition support view many
rule governed cognitive processes better modeled symbolic rather connectionist systems

review previous work
section review brie two main connectionist learning past
tenses english verbs subsequent criticisms

rumelhart mcclelland model

rumelhart mcclelland model simple perceptron pattern associator interfaced input output encoding decoding network allows model
associate verb stems past tenses special wickelphone wickelfeature
phoneme representation format learning classical perceptron convergence procedure training testing sets mutually disjoint experiments
errors made model training process broadly follow u shaped learning curve stages acquisition english past tense exhibited young children
testing sample consists unseen low frequency verbs irregular regular
randomly chosen testing sample error rate
irregulars regulars fare better error rate thus overall error rate
whole testing sample wrong ambiguous past tense forms tested
rumelhart mcclelland claim outcome experiment disconfirms
view exist explicit though inaccessible rules underlie human knowledge
language

macwhinney leinbach model

macwhinney leinbach report connectionist model learning
past tenses english verbs claim simulation far
superior rumelhart mcclelland answer criticisms aimed earlier model major departure rumelhart mcclelland
model wickelphone wickelfeature representational format replaced
unibet macwhinney phoneme representational system allows assignment single alphabetic numerical letter total phonemes macwhinney
leinbach use special templates code phoneme position
word actual input network created coding individual phonemes sets


filing

phonetic features way similar coding wickelphones wickelfeatures cf
section network two layers hidden units fully connected adjacent
layers number arrived trial error addition network
special purpose set connections copy input units directly onto output units
altogether regular irregular english verbs selected experiment
used training regular irregular low
frequency irregular verbs used testing macwhinney leinbach page
training network takes epochs end training still errors
irregular pasts macwhinney leinbach believe allow network
run several additional days give additional hidden unit resources probably
reach complete convergence macwhinney leinbach page
testing error rate reported small biased test sample unseen
irregular verbs predicted incorrectly test model
unseen regular verbs unfortunately test similar set regulars
macwhinney leinbach page

criticism connectionist

previous current criticisms connectionist learning past tenses
english verbs center mainly several issues issues summarized
following subsections

error rates

error rate producing past tenses unseen test verbs high
ann important tests carried macwhinney leinbach
model experimental indicate neither model reaches level
adult competence addition relatively large numbers errors psychologically
realistic since humans rarely make

training testing procedures

rumelhart mcclelland model macwhinney leinbach model
generalization ability measured one training testing sample testing
sets randomly chosen small accuracy testing irregular
verbs vary greatly depending upon particular set testing verbs chosen thus
multiple runs large testing samples necessary assess true generalization
ability learning model therefore previous connectionist
reliable section set reliable testing procedure compare connectionist
symbolic previous connectionist simulations
criticized crude training processes example sudden increase regular
verbs training set create behavior u shaped learning curves

data representation network architecture

past criticisms connectionist aimed datarepresentation formats employed simulations lachter bever pointed


filearning past tense symbolic vs connectionist

achieved rumelhart mcclelland model would impossible without use several trics representations crucially supposes
introduced adoption wickelphone wickelfeature representational format
macwhinney leinbach claim improved upon earlier connectionist
model getting rid wickelphone wickelfeature representation format thus
responded many criticisms format entailed however macwhinney leinbach introduce several trics data representation format
example instead coding predecessor successor phonemes wickelphones introduce special templates code positional information means
network learn associate patterns phoneme positions within predetermined consonant vowel pattern use restrictive templates gets rid many english
verbs fit chosen template may bias model favour shorter
verbs predominantly anglo saxon origin longer verbs predominantly composite latin french origin another trics introduced phonetic feature
encoding distributed representation clear phonetic features front
centre back high etc chosen represent finer grained microfeatures
help capture regularities english past tenses section
straightforward symbolic representation leads better generalization
carefully engineered distributed representation undermines claimed advantages
distributed representation connectionist

knowledge representation integration acquired knowledge
pinker prince lachter bever point rumelhart
mcclelland try model acquisition production past tense isolation
rest english morphological system rumelhart mcclelland well
macwhinney leinbach assume acquisition process establishes direct
mapping phonetic representation stem phonetic representation
past tense form direct mapping collapses well established distinctions
lexical item vs phoneme string morphological category vs morpheme simply
remaining level phonetic patterns impossible express categorical
information first order predicate function variable format one inherent deficits
connectionist implementations thing variable verb
stem hence way model attain knowledge one could
add sux stem get past tense pinker prince page since
acquired knowledge networks large weight matrix usually opaque
human observer unclear phonological levels processing connectionist
carry integrated morphological lexical syntactical level
processing neither rumelhart mcclelland macwhinney leinbach address
issue contrast anns whose internal representations entirely opaque
spa represent acquired knowledge form production rules allow
processing resulting higher level categories verb stem voiced
consonants linguistically realistic production rules categories regular
verbs associative templates irregular verbs ling marinov


filing

symbolic pattern associator

take macwhinney leinbach challenge better symbolic model learning
past tense english verbs present general purpose symbolic pattern associator
spa generalize past tense unseen verbs much accurately
connectionist section model symbolic several reasons first
input output representation learning program set phoneme symbols
basic elements governing past tense ection second learning
program operates phoneme symbols directly acquired knowledge
represented form production rules phoneme symbols well third
production rules phonological level easily generalized firstorder rules use abstract high level symbolic categories morphemes
verb stem ling marinov contrast connectionist operate
distributed representation phonetic feature vectors acquired knowledge
embedded large weight matrix therefore hard see knowledge
generalized abstract representations categories

architecture symbolic pattern associator

spa c quinlan improved implementation id
learning cf quinlan id program inducing classification rules
form decision trees set classified examples uses information gain ratio
criterion selecting attributes roots subtrees divide conquer strategy
recursively applied building subtrees remaining examples training set
belong single concept class leaf labeled concept information
gain guides greedy heuristic search locally relevant discriminating attribute
maximally reduces entropy randomness divided set examples
use heuristic usually building small decision trees instead larger ones
fit training data
task learn classify set different patterns single class several
mutually exclusive categories id shown comparable neural networks
e within range predictive accuracy many real world learning tasks
cf shavlik mooney towell feng king sutherland henery ripley
weiss kulikowski however task classify set input patterns
output patterns many attributes id cannot applied directly reason
id treats different output patterns mutually exclusive classes number
classes would exponentially large importantly generalization individual
output attributes within output patterns would lost
turn id similar n classification system general purpose n
symbolic pattern associators spa applies id output attributes combines
individual decision trees forest set trees similar proposed
dealing distributed binary encoding multiclass learning tasks nettalk
english text speech mapping dietterich hild bakiri tree takes
input set attributes input patterns used determine value
spa programs relevant datasets obtained anonymously ftp csd uwo ca
pub spa



filearning past tense symbolic vs connectionist

one attribute output pattern specifically pair input attributes n
output attributes represented

n

spa build total decision trees one output attribute
taking input attributes n per tree trees built spa
use jointly determine output pattern input pattern
n

important feature spa explicit knowledge representation decision trees
output attributes easily transformed propositional production rules quinlan
since entities rules symbols semantic meanings acquired
knowledge often comprehensible human observer addition processing
integration rules yield high level knowledge e g rules verb stems
ling marinov another feature spa trees different output
attributes contain identical components branches subtrees ling marinov
components similar roles hidden units anns since shared
decision trees one output attribute identical components
viewed high level concepts feature combinations created learning program

default strategies

interesting issue decision tree learning handle default
class default class class assigned leaves training examples
classified call leaves empty leaves happens attributes
many different values training set relatively small cases
tree construction branches explored attributes testing
examples fall empty leaves default strategy needed assign classes
empty leaves
easier understanding use spelling form verbs subsection explain
different default strategies work actual learning experiment verbs
represented phonetic form use consecutive left right alphabetic representation
verb stems past tenses small training set represented follows
f f r
e
l u n c h
l e v e






f f r e
e
l u n c h e
l e f

used filler empty space left hand columns input patterns
stems verbs right hand columns output patterns
corresponding correct past tense forms
discussed decision trees constructed one output attribute
decision tree first output attribute constructed see figure
following examples
f f r
e
l u n c h l



filing

l e v e l

last column classification first output attribute however many
branches c figure explored since training example
attribute value testing pattern first input attribute equal c
class assigned id uses majority default popular
class whole subtree assigned empty leaves example
class l chosen since training examples however
clearly right strategy task since verb create would output
l incorrect unlikely small training set
variations attribute values majority default strategy id appropriate
task




e


l



c

z

passthrough










l

c



z


p

x n indicates n examples
classified leaf labelled x
x boxed indicates empty leaves



figure passthrough default

majority

r



l



k



b default

applications verb past tense learning default heuristic passthrough
may suitable classification empty leaf
attribute value branch example passthrough default strategy
create output c passthrough strategy gives decision trees first order
avor since production rules empty leaves represented attribute x
class x x unused attribute values passthrough domaindependent heuristic strategy class labels may nothing
attribute values applications
applying passthrough strategy alone however adequate every output
attribute endings regular past tenses identical input
patterns irregular verbs may vowel consonant changes middle
verbs cases majority default may suitable passthrough
order choose right default strategy majority passthrough decision
made upon training data corresponding subtree spa first determines
majority class counts number examples subtrees belong
class counts number examples subtrees coincide


filearning past tense symbolic vs connectionist

passthrough strategy two numbers compared default strategy employed
examples chosen instance example see figure
majority class l instances however examples coinciding
passthrough default two l one thus passthrough strategy takes
assigns empty leaves level empty attribute branch c would assigned
class c note default strategy empty leaves attribute x depends upon
training examples falling subtree rooted x localized method ensures
related objects uence calculating default classes spa
adapt default strategy best suited different levels decision trees
example figure b two different default strategies used different levels
tree use spa adaptive default strategy throughout remainder
note default strategy trics data representation
rather represents bias learning program learning default
strategy independent data representation effect different data representations
generalization discussed sections passthrough strategy
imposed anns well adding set copy connections input units
twin output units see section detail

comparisons default strategies id spa ann
default strategy neural networks tend take generalizing default classes
compared id spa conducted several experiments determine neural
networks default strategy assume domain one attribute x
may take values b c class one b c training
examples attribute values b c reserved testing default
class training set contains multiple copies example form certain
majority class table shows two sets training testing examples used test
compare default strategies id spa neural networks
data set
data set
training examples
training examples
values x class copies values x class copies




c

b
b

b
b

c
c

c
c

testing example
testing example






table two data sets testing default strategies methods
classification testing examples id spa quite easy decide since
id takes majority default output class training examples
first data set c training examples second data set spa
number examples passthrough first data set second


filing

data set therefore passthrough strategy wins first case output class
majority strategy wins second case output class c
neural networks coding methods used represent values attribute x dense coding used represent b c
tried standard one per class encoding real number encoding
b c networks trained hidden units
possible case found cases classification testing example stable varies different random seeds initialize networks table
summarises experimental anns classifications obtained
different random seeds listed first ones occurring frequently seems
neural networks consistent default strategy
neither majority default id passthrough default spa may
explain connectionist cannot generalize unseen regular verbs well even
training set contains regular verbs see section networks diculty
underconstrained generalizing identity mapping copies attributes
verb stems past tenses
classification testing example
data set data set
id

c
spa

c
ann dense coding
b c
b
ann one per class
b c
c b
ann real numbers
c
c
table default strategies id spa ann two data sets

head head comparisons symbolic ann

section perform series extensive head head comparisons several
different representations encoding methods demonstrate spa generalizes
past tense unseen verbs better ann wide margin

format data

verb set came macwhinney original list verbs set contains
stem past tense pairs learning upon phonological unibet representation macwhinney different phonemes represented different
alphabetic numerical letters total phonemes source file transferred
standard format pairs input output patterns example verbs
table represented pairs input output patterns verb stem past tense
b n n

b n n
k e l r e k e l r e



filearning past tense symbolic vs connectionist

r z r z
b k b k e

see table original verb set available online appendix keep one
form past tense among multiple past tenses hang hanged hang hung
data set addition homophones exist original data set consequently
noise contradictory data input pattern different output
patterns training testing examples note information whether
verb regular irregular provided training testing processes
base stem
unibet
b base
irregular
spelling form phonetic form past tense regular
abandon
b nd n
b

abandoned
b nd nd


benefit
ben fit
b

benefited
ben fitid


arise
r z
b

arose
roz


become
bik
b

became
bikem



table source file macwhinney leinbach

experiment setup

guarantee unbiased reliable comparison use training testing samples
randomly drawn several independent runs spa ann provided
sets training testing examples run allows us achieve reliable
estimate inductive generalization capabilities model task
neural network program used package called xerion developed
university toronto several sophisticated search mechanisms
standard steepest gradient descent method momentum found training
conjugate gradient method much faster standard backpropagation
conjugate gradient method avoids need search proper
settings parameters learning rate however need determine
proper number hidden units experiments anns first tried
numbers hidden units chose one produced best predictive accuracy
trial run use network number hidden units actual runs
spa hand parameters adjust
one major difference implementation anns spa spa take
symbolic phoneme letters directly anns normally encode phoneme letter
binary bits course spa apply binary representation studied
binary encoding methods compared spa symbolic letter


filing

representation since outputs neural networks real numbers need decode
network outputs back phoneme letters used standard method decoding
phoneme letter minimal real number hamming distance smallest angle
network outputs chosen see binary encoding affects generalization
spa trained binary representation since spa outputs
binary decoding process may tie several phoneme letters case one
chosen randomly ects probability correct decoding level
phoneme letters phoneme letters decoded one letters
incorrect whole pattern counted incorrect word level

templated distributed representation

set experiments conducted distributed representation suggested
macwhinney leinbach according macwhinney leinbach output
left justified template format cccvvcccvvcccvvccc c stands
consonant v vowel space holders input two components left justified
template format input right justified template format
vvccc example verb bet represented unibet coding bet shown
template format follows blank phoneme
input
bet
template
output
bet
template

b e
cccvvcccvvcccvvccc
left justified

e
vvccc
right justified

b e
cccvvcccvvcccvvccc
left justified

specific distributed representation set binary phonetic features used
encode phoneme letters connectionist networks vowel v
templates encoded phonetic features front centre back high low middle round
diphthong consonant c templates phonetic features
voiced labial dental palatal velar nasal liquid trill fricative interdental note
two feature sets vowels consonants identical templates
needed order decode right type phoneme letters outputs
network
experimental comparison decided use right justified template
vvccc since information redundant therefore used left justified
template cccvvcccvvcccvvccc input output whole verb set
templated phoneme representation available online appendix contains
pairs verb stems past tenses fit template ease implementation
added two extra features assigned vowel phonetic feature
set therefore vowels consonants encoded binary bits ann
thus input bits output bits found one layer
hidden units macwhinney model reached highest predictive accuracy
trial run see figure network architecture used


filearning past tense symbolic vs connectionist

output units

c

















c

c

v

v

cccvvcccvv

c

c

c

full connection two layers


hidden units



full connection two layers

c

















c

c

v

v

cccvvcccvv

c

c

c

input units

figure architecture network used experiment
spa trained tested data sets phoneme letters directly
decision trees built phoneme letters output templates
see phonetic feature encoding affects generalization trained spa
distributed representation binary bit patterns input bits
output bits exactly ann simulation addition see
symbolic encoding works ann train another neural network
hidden units one per class encoding phoneme letter total
phoneme letters plus one blank encoded bits one phoneme letter
used verb pairs including regular irregular verbs training
testing sets sampling done randomly without replacement training testing
sets disjoint three runs spa ann conducted spa ann
trained tested data set run training reached accuracy
spa around ann
testing accuracy novel verbs produced interesting ann model
spa distributed representation similar accuracy ann
slightly better may well caused binary outputs spa suppress
fine differences prediction hand spa phoneme letters directly
produces much higher accuracy testing spa outperforms neural networks
distributed one per class representations percentage points testing
ann spa found table findings clearly indicate
spa symbolic representation leads much better generalization ann

learning regular verbs

predicting past tense unseen verb regular irregular
easy task irregular verbs learned rote traditionally thought since


filing

distributed representation
ann correct
spa correct
reg irrg comb reg irrg comb





symbolic representation
ann correct
spa correct
reg irrg comb reg irrg comb





table comparisons testing accuracy spa ann distributed symbolic
representations
children adults occasionally extend irregular ection irregular sounding regular
verbs pseudo verbs cleef cleft prasada pinker similar
novel verb cluster irregular verbs similar phonological patterns
likely prediction irregular past tense form pinker prasada
pinker argue regular past tenses governed rules irregulars may
generated associated memory graded effect irregular past tense
generalization would interesting therefore compare spa ann
past tense generalization regular verbs spa ann use
position specific representation learning regular past tenses would require learning different
suxes different positions learn identity mapping copies verb stem
past tenses verbs different lengths
used templated representation previous section training
testing sets contained regular verbs samples drawn randomly without
replacement maximize size testing sets testing sets simply consisted
regular verbs sampled training sets training testing sets
used following methods compared see effect adaptive
default strategy discussed section generalization spa majority
default adaptive default tested ann similar
used previous section except one layer hidden units turned
best predictive accuracy test run passthrough default strategy
imposed neural networks adding set copy connections connect directly
input units twin output units macwhinney leinbach used
copy connections simulation therefore tested networks copy
connection see generalization would improved well
predictive accuracy spa anns one run
randomly sampled training testing sets summarized table see
spa adaptive default strategy combines majority passthrough
default outperforms spa majority default strategy used id
phonological form three different suxes regular verbs verb stem ends
unibet phonetic representations sux id example extend extended
spelling form verb stem ends unvoiced consonant sux example talk
talked verb stem ends voiced consonant vowel sux example
solve solved



filearning past tense symbolic vs connectionist

anns copy connections generalize better ones without however even
ann copy connections lower predictive accuracy spa majority addition differences predictive accuracy larger smaller sets
training examples smaller training sets make difference testing accuracy
evident training set contains patterns testing accuracy
becomes similar would asymptotically larger training sets
upon examination errors made ann occur identity mapping
e strange phoneme change drop verb stems cannot preserved past
tense phonemes previously seen training examples contradicts
findings prasada pinker native english speakers generate
regular sux adding past tenses equally well unfamiliar sounding verb stems long
verb stems sound close irregular verbs indicates bias
ann learning suitable type task see discussion
section
training
percent correct testing
size
spa adaptive spa majority ann copy con ann normal

























table predictive accuracy learning past tense regular verbs

error correcting codes
dietterich bakiri reported increase predictive accuracy errorcorrecting codes large hamming distances used encode values attributes
codes larger hamming distance allow correcting fewer
bits errors thus learning programs allowed make mistakes bit level
without outputs misinterpreted word level
wanted performances spa anns improved errorcorrecting codes encoding phonemes chose error correcting codes ranging
ones small hamming distance ones large hamming distance
bhc codes see dietterich bakiri number attributes
phoneme large data representation changed slightly experiment
instead phoneme holders templates consecutive left right phoneme holders
used verbs stems past tenses phonemes removed
training testing sets whole verb set representation available online
appendix contains pairs verb stems past tenses whose lengths shorter
spa ann take exactly training testing sets contains
pairs verb stems past tenses error correcting codes encoding phoneme


filing

letter still training networks bit longer error correcting codes takes long
run input attributes output attributes therefore
two runs bit codes conducted consistent dietterich bakiri
findings found testing accuracy generally increases hamming
distance increases however observed testing accuracy decreases
slightly codes become long accuracy bit codes hamming
distance reaches maximum value quite close accuracy
spa direct phoneme letter representation seems trade
tolerance errors large hamming distance diculty learning
longer codes addition found testing accuracy anns lower one
spa bit bit error correcting codes summarized
table
ann
hamming distance correct bit level correct word level
bit codes



bit codes



spa
hamming distance correct bit level correct word level
bit codes



bit codes



bit codes



bit codes



table comparisons testing accuracy spa anns error correcting codes
previous two subsections undermine advantages
distributed representation anns unique feature advocated connectionists
demonstrated task distributed representation actually allow
adequate generalization spa direct symbolic phoneme letters spa
error correcting codes outperform anns distributed representation wide margin
however neither phoneme symbols bits error correcting codes encode implicitly
explicitly micro features distributed representation may
distributed representation used optimally designed nevertheless straightforward
symbolic format requires little representation engineering compared distributed
representation anns

right justified isolated sux representation

macwhinney leinbach report important predictive accuracy model unseen regular verbs reply macwhinney
ling marinov macwhinney implemented ann model
implementation verb stem past tense pairs training set among
regular irregular training took epochs reached
correct regulars irregulars testing set consisted regulars
irregulars percent correct testing epoch regulars
irregulars combined testing set macwhinney claimed raw


filearning past tense symbolic vs connectionist

generalization power ann model close spa believes
case simply systems trained data set
realize via private communication representation used macwhinney
recent implementation plays critical role improved performance macwhinney
representation input verb stems coded right justified template
cccvvcccvvcccvvccc output contains two parts right justified template
one input coda form vvccc rightjustified template output used represent past tense without including
sux regular verbs sux regular past tense stays coda
isolated main right justified templates irregular past tense
coda left empty example input output templated patterns past tense
verbs table represented
input
right justified
cccvvcccvvcccvvccc
b nd n
b e n f
r z
b k

output
right justified
cccvvcccvvcccvvccc
b nd n
b e n f
r z
b k e

suffix
vvccc
abandon abandoned
benefit benefited
arise arose
become became

data representation clearly facilitates learning regular verbs output
patterns identical input patterns addition verb ending phoneme
letters appear fixed positions e right vvccc section
input template due right justified templated representation furthermore sux
occupies coda isolated right justified templates
performed series experiments see much improvement could accomplish representation macwhinney recent ann model
left justified representation discussed section spa averaged predictive
accuracy outperforms macwhinney recent ann implementation predictive accuracy wide margin addition predictive accuracy
improved average left justified representation
right justified isolated sux one see table

general discussion conclusions

two factors contribute generalization ability learning program first
data representation bias learning program arriving
right optimal representation dicult task argued prasada pinker
regular verbs represented coarse grain terms verb stem
suxes irregular verbs finer grain terms phonological properties
admittedly spa works uniformly level phoneme letters anns however
spa produces simple production rules use phoneme letters directly
rules generalized first order rules representations stems
voiced consonants used across board rule learning
modules ling marinov one major advantages ann


filing

predictive accuracy right justified isolated sux representation
spa
macwhinney ann model
training testing training testing
training testing



run


run


run


average


one run
table comparisons testing accuracy spa ann right justified isolated
sux representation
seems quite conceivable children acquire high level concepts stems
voiced consonants learning noun plurals verb past tense verb third person
singular comparative adjectives large weight matrix
learning hard see knowledge generalized ann
shared modules
even exactly data representation exist learning tasks
symbolic methods spa generalize categorically better anns converse true fact ects different inductive biases different learning
occam razor principle preferring simplest hypothesis
complex ones creates preference bias preference choosing certain hypotheses
others hypothesis space however different learning choose different hypotheses use different measurements simplicity example among
possible decision trees fit training examples id spa induce simple decision
trees instead complicated ones simple decision trees converted small sets
production rules well learning generalizes depends upon degree
underlying regularities target concept fit bias words
underlying regularities represented compactly format hypotheses produced
learning data generalized well even small set training
examples otherwise underlying regularities large hypothesis
looking compact ones per occam razor principle hypotheses inferred accurate learning searches hypotheses larger
necessary e use occam razor principle normally underconstrained know training examples many
competitive hypotheses large size inferred
describe bias learning looking training examples
different classes separated n dimensional hyperspace n number
attributes decision node decision tree forms hyperplane described
linear function x hyperplanes perpendicular axis
partial space hyperplanes extend within subregion formed
hyperplanes parents nodes likewise hidden units threshold function
anns viewed forming hyperplanes hyperspace however unlike ones
decision trees need perpendicular axis full space


filearning past tense symbolic vs connectionist

hyperplanes extend whole space id applied concepts fit
ann bias especially hyperplanes perpendicular axis many
zigzag hyperplanes perpendicular axes would needed separate different
classes examples hence large decision tree would needed fit
id bias similarly ann learning applied concepts fit id
bias especially hyperplanes form many separated partial space regions many
hidden units may needed regions
another major difference anns id anns larger variation
weaker bias cf geman bienenstock doursat id many
boolean functions e g linearly separable functions fit small network e g one
hidden units small decision tree sometimes attributed
claimed versatility exibility anns learn necessarily predict reliably well many functions symbolic methods brittle however belief
humans versatile learning large variation
rather set strong biased learning somehow
search bias space add members set learning tasks symbolic learning clear semantic components explicit representation
thus easily construct strong motivated specific
learning tasks adaptive default strategy spa example
hand still largely know effectively strengthen bias anns many
specific tasks identity mapping k term dnf etc techniques
adding copy connections weight decaying exist exact effects biasing
towards classes functions clear
analyses ling marinov underlying regularities governing
ection past tense english verbs form small set production rules
phoneme letters especially regular verbs rules identity
rules sux adding rules example decision trees converted set
precedence ordered production rules complicated rules rules conditions listed first example consecutive left right phonetic representation
typical sux adding rule verb stems phoneme letters talk talked
k
fourth input phoneme k fifth blank e verb
ending fifth output phoneme hand identity mapping rules
one condition typical identity rule looks
l l
fact passthrough default strategy allows identity mapping rules represented simple first order format
x x
x phoneme clearly knowledge forming regular past tenses
thus expressed simple conjunctive rules fit bias spa id
therefore spa much better generalization ability ann
conclude demonstrated via extensive head head comparisons
spa realistic better generalization capacity anns learning
past tense english verbs argued symbolic decision tree production rule
learning outperform anns first domain seems


filing

governed compact set rules thus fits bias symbolic learning
second spa directly manipulates representation better anns e
symbolic phoneme letters vs distributed representation third spa able
derive high level concepts used throughout english morphology support
view many high level rule governed cognitive tasks better modeled
symbolic rather connectionist systems

acknowledgements
gratefully thank steve pinker constant encouragement marin marinov steve
cherwenka huaqing zeng discussions help implementing spa
thank brian macwhinney providing verb data used simulation discussions
tom dietterich dave touretzky brian macwhinney well comments
reviewers helpful conducted support nserc
grant computing facilities department

references

cottrell g plunkett k recurrent net learn past tense
proceedings cognitive science society conference
daugherty k seidenberg beyond rules exceptions connectionist
modeling ectional morphology lima ed reality
linguistic rules john benjamins
dietterich bakiri g error correcting output codes general method
improving multiclass inductive learning programs aaai proceedings ninth
national conference artificial intelligence
dietterich hild h bakiri g comparative study id backpropagation english text speech mapping proceedings th international
conference machine learning morgan kaufmann
feng c king r sutherland henery r comparison symbolic statistical neural network classifiers manuscript department computer science
university ottawa
fodor j pylyshyn z connectionism cognitive architecture critical
analysis pinker mehler j eds connections symbols pp
cambridge mit press
geman bienenstock e doursat r neural networks bias variance
dilemma neural computation
lachter j bever relation linguistic structure associative
theories language learning constructive critique connectionist learning
pinker mehler j eds connections symbols pp
cambridge mit press


filearning past tense symbolic vs connectionist

ling x cherwenka marinov symbolic model learning past
tenses english verbs proceedings ijcai thirteenth international conference artificial intelligence pp morgan kaufmann publishers
ling x marinov answering connectionist challenge symbolic model
learning past tense english verbs cognition
macwhinney b childes project tools analyzing talk hillsdale nj
erlbaum
macwhinney b connections symbols closing gap cognition

macwhinney b leinbach j implementations conceptualizations revising verb model cognition
pinker rules language science
pinker prince language connectionism analysis parallel
distributed processing model language acquisition pinker mehler j
eds connections symbols pp cambridge mit press
plunkett k marchman v u shaped learning frequency effects multilayered perceptron implications child language acquisition cognition

prasada pinker generalization regular irregular morphological
patterns language cognitive processes
quinlan j induction decision trees machine learning
quinlan j c programs machine learning morgan kaufmann san mateo
ca
ripley b statistical aspects neural networks invited lectures semstat
seminaire europeen de statistique sandbjerg denmark april
rumelhart mcclelland j learning past tenses english verbs
rumelhart mcclelland j pdp group eds parallel distributed processing vol pp cambridge mit press
shavlik j mooney r towell g symbolic neural learning
experimental comparison machine learning
weiss kulikowski c computer systems learn classification prediction methods statistics neural networks machine learning expert systems
morgan kaufmann san mateo ca




