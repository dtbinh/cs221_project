Journal Artificial Intelligence Research 1 (1994) 209-229

Submitted 11/93; published 2/94

Learning Past Tense English Verbs:
Symbolic Pattern Associator vs. Connectionist Models
Charles X. Ling

ling@csd.uwo.ca

Department Computer Science
University Western Ontario
London, Ontario, Canada N6A 5B7

Abstract

Learning past tense English verbs | seemingly minor aspect language acquisition | generated heated debates since 1986, become landmark task
testing adequacy cognitive modeling. Several artificial neural networks (ANNs)
implemented, challenge better symbolic models posed.
paper, present general-purpose Symbolic Pattern Associator (SPA) based upon
decision-tree learning algorithm ID3. conduct extensive head-to-head comparisons
generalization ability ANN models SPA different representations. conclude SPA generalizes past tense unseen verbs better
ANN models wide margin, offer insights case.
discuss new default strategy decision-tree learning algorithms.

1. Introduction
Learning past tense English verbs, seemingly minor aspect language acquisition,
generated heated debates since first connectionist implementation 1986 (Rumelhart & McClelland, 1986). Based results, Rumelhart McClelland claimed
use acquisition human knowledge language best formulated ANN
(Artificial Neural Network) models without symbol processing postulates existence
explicit symbolic representation rules. Since then, learning past tense become landmark task testing adequacy cognitive modeling. years
number criticisms connectionist modeling appeared (Pinker & Prince, 1988; Lachter &
Bever, 1988; Prasada & Pinker, 1993; Ling, Cherwenka, & Marinov, 1993). criticisms
centered mainly upon issues high error rates low reliability experimental results, inappropriateness training testing procedures, \hidden" features
representation network architecture facilitate learning, well opaque
knowledge representation networks. Several subsequent attempts improving
original results new ANN models made (Plunkett & Marchman, 1991; Cottrell & Plunkett, 1991; MacWhinney & Leinbach, 1991; Daugherty & Seidenberg, 1993).
notably, MacWhinney Leinbach (1991) constructed multilayer neural network
backpropagation (BP), attempted answer early criticisms. hand,
supporters symbolic approach believe symbol structures parse trees,
propositions, etc., rules manipulations, critical cognitive level,
connectionist approach may provide account neural structures
traditional symbol-processing cognitive architecture implemented (Fodor
& Pylyshyn, 1988). Pinker (1991) Prasada Pinker (1993) argue proper

c 1994 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiLing

accounting regular verbs dependent upon production rules, irregular
past-tense ections may generalized ANN-like associative memory.
proper way debating adequacy symbolic connectionist modeling
contrasting competitive implementations. Thus, symbolic implementation needed
compared ANN models. is, fact, challenge posed MacWhinney
Leinbach (1991), assert symbolic methods would work well
model. section titled \Is better symbolic model?" claim:
approach provided even accurate
characterization learning process, might still forced reject
connectionist approach, despite successes. proper way debating conceptualizations contrasting competitive implementations.
present case, would need symbolic implementation could contrasted
current implementation. (MacWhinney & Leinbach, 1991, page 153)
paper, present general-purpose Symbolic Pattern Associator (SPA) based
upon symbolic decision tree learning algorithm ID3 (Quinlan, 1986). shown
(Ling & Marinov, 1993) SPA's results much psychologically realistic
ANN models compared human subjects. issue predictive accuracy,
MacWhinney Leinbach (1991) report important results model unseen
regular verbs. reply criticism, MacWhinney (1993) re-implemented ANN
model, claimed raw generalization power close SPA.
believed case systems learn data set:
good reason equivalent performance two
models. [...] two computationally powerful systems given
set input data, extract every bit data regularity input.
Without processing, much blood squeezed
turnip, systems [SPA ANN] extracted
could. (MacWhinney, 1993, page 295)
show case; obviously reasons one learning
algorithm outperforms another (otherwise study different learning algorithms?).
Occam's Razor Principle | preferring simplest hypothesis complex
ones | creates preference biases learning algorithms. preference bias preference
order among competitive hypotheses hypothesis space. Different learning algorithms,
however, employ different ways measuring simplicity, thus concepts bias
different. well learning program generalizes depends upon degree
regularity data fits bias. study compare raw generalization
ability symbolic ANN models task learning past tense English
verbs. perform extensive head-to-head comparisons ANN SPA, show
effects different representations encodings generalization abilities.
experimental results demonstrate clearly
1. distributed representation, feature connectionists advocating,
lead better generalization compared symbolic representation, arbitrary error-correcting codes proper length;
210

fiLearning Past Tense: Symbolic vs Connectionist Models

2. ANNs cannot learn identity mapping preserves verb stem past
tense well SPA can;
3. new representation suggested MacWhinney (1993) improves predictive accuracy SPA ANN, SPA still outperforms ANN models;
4. sum, SPA generalizes past tense unseen verbs better ANN models
wide margin.
Section 5 discuss reasons SPA better learning model
task English past-tense acquisition. results support view many
rule-governed cognitive processes better modeled symbolic, rather connectionist, systems.

2. Review Previous Work
section, review brie two main connectionist models learning past
tenses English verbs, subsequent criticisms.

2.1 Rumelhart McClelland's Model

Rumelhart McClelland's model based simple perceptron-based pattern associator interfaced input/output encoding/decoding network allows model
associate verb stems past tenses using special Wickelphone/Wickelfeature
phoneme-representation format. learning algorithm classical perceptron convergence procedure. training testing sets mutually disjoint experiments.
errors made model training process broadly follow U-shaped learning curve stages acquisition English past tense exhibited young children.
testing sample consists 86 \unseen" low frequency verbs (14 irregular 72 regular)
randomly chosen. testing sample results 93% error rate
irregulars. regulars fare better 33.3% error rate. Thus, overall error rate
whole testing sample 43% | 37 wrong ambiguous past tense forms 86 tested.
Rumelhart McClelland (1986) claim outcome experiment disconfirms
view exist explicit (though inaccessible) rules underlie human knowledge
language.

2.2 MacWhinney Leinbach's Model

MacWhinney Leinbach (1991) report new connectionist model learning
past tenses English verbs. claim results new simulation far
superior Rumelhart McClelland's results, answer criticisms aimed earlier model. major departure Rumelhart McClelland's
model Wickelphone/Wickelfeature representational format replaced
UNIBET (MacWhinney, 1990) phoneme representational system allows assignment single alphabetic/numerical letter total 36 phonemes. MacWhinney
Leinbach use special templates code phoneme position
word. actual input network created coding individual phonemes sets
211

fiLing

phonetic features way similar coding Wickelphones Wickelfeatures (cf
Section 4.3). network two layers 200 \hidden" units fully connected adjacent
layers. number arrived trial error. addition, network
special-purpose set connections copy input units directly onto output units.
Altogether, 2062 regular irregular English verbs selected experiment
| 1650 used training (1532 regular 118 irregular), 13 low
frequency irregular verbs used testing (MacWhinney & Leinbach, 1991, page 144).
Training network takes 24,000 epochs. end training still 11 errors
irregular pasts. MacWhinney Leinbach believe allow network
run several additional days give additional hidden unit resources, probably
reach complete convergence (MacWhinney & Leinbach, 1991, page 151).
testing error rate reported based small biased test sample 13 unseen
irregular verbs; 9 13 predicted incorrectly. test model
unseen regular verbs: \Unfortunately, test similar set 13 regulars."
(MacWhinney & Leinbach, 1991, page 151).

2.3 Criticism Connectionist Models

Previous current criticisms connectionist models learning past tenses
English verbs center mainly several issues. issues summarized
following subsections.

2.3.1 Error Rates

error rate producing past tenses \unseen" test verbs high
ANN models, important tests carried MacWhinney Leinbach
(1991) model. experimental results indicate neither model reaches level
adult competence. addition, relatively large numbers errors psychologically
realistic since humans rarely make them.

2.3.2 Training Testing Procedures

Rumelhart McClelland's model, MacWhinney Leinbach's model,
generalization ability measured one training/testing sample. Further, testing
sets randomly chosen, small. accuracy testing irregular
verbs vary greatly depending upon particular set testing verbs chosen, thus
multiple runs large testing samples necessary assess true generalization
ability learning model. Therefore, results previous connectionist models
reliable. Section 4, set reliable testing procedure compare connectionist
models symbolic approach. Previous connectionist simulations
criticized crude training processes (for example, sudden increase regular
verbs training set), create behavior U-shaped learning curves.

2.3.3 Data Representation Network Architecture

past criticisms connectionist models aimed datarepresentation formats employed simulations. Lachter Bever (1988) pointed
212

fiLearning Past Tense: Symbolic vs Connectionist Models

results achieved Rumelhart McClelland's model would impossible without use several TRICS (The Representations Crucially Supposes)
introduced adoption Wickelphone/Wickelfeature representational format.
MacWhinney Leinbach claim improved upon earlier connectionist
model getting rid Wickelphone/Wickelfeature representation format, thus
responded many criticisms format entailed. However, MacWhinney Leinbach introduce several TRICS data-representation format.
example, instead coding predecessor successor phonemes Wickelphones, introduce special templates code positional information. means
network learn associate patterns phoneme/positions within predetermined consonant/vowel pattern. Further, use restrictive templates gets rid many English
verbs fit chosen template. may bias model favour shorter
verbs, predominantly Anglo-Saxon origin, longer verbs, predominantly composite Latin French origin. Another TRICS introduced phonetic feature
encoding (a distributed representation). clear phonetic features front,
centre, back, high, etc. chosen. represent finer grained \microfeatures"
help capture regularities English past tenses? Section 4.5, show
straightforward symbolic representation leads better generalization
carefully engineered distributed representation. undermines claimed advantages
distributed representation connectionist models.

2.3.4 Knowledge Representation Integration Acquired Knowledge
Pinker Prince (1988), Lachter Bever (1988) point Rumelhart
McClelland try model acquisition production past tense isolation
rest English morphological system. Rumelhart McClelland, well
MacWhinney Leinbach, assume acquisition process establishes direct
mapping phonetic representation stem phonetic representation
past tense form. direct mapping collapses well-established distinctions
lexical item vs. phoneme string, morphological category vs. morpheme. Simply
remaining level phonetic patterns, impossible express new categorical
information first-order (predicate/function/variable) format. One inherent deficits
connectionist implementations thing variable verb
stem, hence way model attain knowledge one could
add sux stem get past tense (Pinker & Prince, 1988, page 124). Since
acquired knowledge networks large weight matrix, usually opaque
human observer, unclear phonological levels processing connectionist
models carry integrated morphological, lexical, syntactical level
processing. Neither Rumelhart McClelland MacWhinney Leinbach address
issue. contrast ANNs whose internal representations entirely opaque,
SPA represent acquired knowledge form production rules, allow
processing, resulting higher-level categories verb stem voiced
consonants, linguistically realistic production rules using new categories regular
verbs, associative templates irregular verbs (Ling & Marinov, 1993).
213

fiLing

3. Symbolic Pattern Associator

take MacWhinney Leinbach's challenge better symbolic model learning
past tense English verbs, present general-purpose Symbolic Pattern Associator
(SPA)1 generalize past tense unseen verbs much accurately
connectionist models section. model symbolic several reasons. First,
input/output representation learning program set phoneme symbols,
basic elements governing past-tense ection. Second, learning
program operates phoneme symbols directly, acquired knowledge
represented form production rules using phoneme symbols well. Third,
production rules phonological level easily generalized firstorder rules use abstract, high-level symbolic categories morphemes
verb stem (Ling & Marinov, 1993). contrast, connectionist models operate
distributed representation (phonetic feature vectors), acquired knowledge
embedded large weight matrix; therefore hard see knowledge
generalized abstract representations categories.

3.1 Architecture Symbolic Pattern Associator

SPA based C4.5 (Quinlan, 1993) improved implementation ID3
learning algorithm (cf. (Quinlan, 1986)). ID3 program inducing classification rules
form decision trees set classified examples. uses information gain ratio
criterion selecting attributes roots subtrees. divide-and-conquer strategy
recursively applied building subtrees remaining examples training set
belong single concept (class); leaf labeled concept. information
gain guides greedy heuristic search locally relevant discriminating attribute
maximally reduces entropy (randomness) divided set examples.
use heuristic usually results building small decision trees instead larger ones
fit training data.
task learn classify set different patterns single class several
mutually exclusive categories, ID3 shown comparable neural networks
(i.e., within 5% range predictive accuracy) many real-world learning tasks
(cf. (Shavlik, Mooney, & Towell, 1991; Feng, King, Sutherland, & Henery, 1992; Ripley,
1992; Weiss & Kulikowski, 1991)). However, task classify set (input) patterns
(output) patterns many attributes, ID3 cannot applied directly. reason
ID3 treats different output patterns mutually exclusive classes, number
classes would exponentially large and, importantly, generalization individual
output attributes within output patterns would lost.
turn ID3 similar N-to-1 classification system general purpose N-to-M
symbolic pattern associators, SPA applies ID3 output attributes combines
individual decision trees \forest", set trees. similar approach proposed
dealing distributed (binary) encoding multiclass learning tasks NETtalk
(English text-to-speech mapping) (Dietterich, Hild, & Bakiri, 1990). tree takes
input set attributes input patterns, used determine value
1. SPA programs relevant datasets obtained anonymously ftp.csd.uwo.ca
pub/SPA/ .

214

fiLearning Past Tense: Symbolic vs Connectionist Models

one attribute output pattern. specifically, pair input attributes (1 n )
output attributes (!1 !m ) represented as:

1; :::; n ! !1; :::; !m

SPA build total decision trees, one output attribute !i (1
m) taking input attributes 1; :::; n per tree. trees built, SPA
use jointly determine output pattern !1 ; :::; !m input pattern
1; :::; n.

important feature SPA explicit knowledge representation. Decision trees
output attributes easily transformed propositional production rules (Quinlan,
1993). Since entities rules symbols semantic meanings, acquired
knowledge often comprehensible human observer. addition, processing
integration rules yield high-level knowledge (e.g., rules using verb stems)
(Ling & Marinov, 1993). Another feature SPA trees different output
attributes contain identical components (branches subtrees) (Ling & Marinov, 1993).
components similar roles hidden units ANNs since shared
decision trees one output attribute. identical components
viewed high-level concepts feature combinations created learning program.

3.2 Default Strategies

interesting research issue decision-tree learning algorithms handle default
class. default class class assigned leaves training examples
classified into. call leaves empty leaves. happens attributes
many different values, training set relatively small. cases,
tree construction, branches explored attributes. testing
examples fall empty leaves, default strategy needed assign classes
empty leaves.
easier understanding, use spelling form verbs subsection explain
different default strategies work. (In actual learning experiment verbs
represented phonetic form.) use consecutive left-to-right alphabetic representation,
verb stems past tenses small training set represented follows:
a,f,f,o,r,d,_,_,_,_,_,_,_,_,_
e,a,t,_,_,_,_,_,_,_,_,_,_,_,_
l,a,u,n,c,h,_,_,_,_,_,_,_,_,_
l,e,a,v,e,_,_,_,_,_,_,_,_,_,_

=>
=>
=>
=>

a,f,f,o,r,d,e,d,_,_,_,_,_,_,_
a,t,e,_,_,_,_,_,_,_,_,_,_,_,_
l,a,u,n,c,h,e,d,_,_,_,_,_,_,_
l,e,f,t,_,_,_,_,_,_,_,_,_,_,_

used filler empty space. left-hand 15 columns input patterns
stems verbs; right-hand 15 columns output patterns
corresponding correct past tense forms.
discussed, 15 decision trees constructed, one output attribute.
decision tree first output attribute constructed (see Figure 1 (a))
following 4 examples:
a,f,f,o,r,d,_,_,_,_,_,_,_,_,_ =>
e,a,t,_,_,_,_,_,_,_,_,_,_,_,_ =>
l,a,u,n,c,h,_,_,_,_,_,_,_,_,_ => l

215

fiLing

l,e,a,v,e,_,_,_,_,_,_,_,_,_,_ => l

last column classification first output attribute. However, many
branches (such 1 = c Figure 1 (a)) explored, since training example
attribute value. testing pattern first input attribute equal c,
class assigned to? ID3 uses majority default. is, popular
class whole subtree 1 assigned empty leaves. example above,
either class l chosen since 2 training examples. However,
clearly right strategy task since verb create would output
l...... a......, incorrect. unlikely small training set
variations attribute values, majority default strategy ID3 appropriate
task.

1


e

6
l



c

z

<= Passthrough

__



5
a:1

a:1

l:2

c:0

o:2

z:0

d:20
p

x:n indicates n examples
classified leaf labelled x.
x:0 (boxed) indicates empty leaves.

t:10

Figure 1: (a) Passthrough default

<= Majority

r

d:2

l

d:5

k

t:0

(b) Various default

applications verb past-tense learning, new default heuristic | passthrough
| may suitable. is, classification empty leaf
attribute value branch. example, using passthrough default strategy,
create output c....... passthrough strategy gives decision trees first-order
avor, since production rules empty leaves represented Attribute = X
Class = X X unused attribute values. Passthrough domaindependent heuristic strategy class labels may nothing
attribute values applications.
Applying passthrough strategy alone, however, adequate every output
attribute. endings regular past tenses identical input
patterns, irregular verbs may vowel consonant changes middle
verbs. cases, majority default may suitable passthrough.
order choose right default strategy | majority passthrough | decision
made based upon training data corresponding subtree. SPA first determines
majority class, counts number examples subtrees belong
class. counts number examples subtrees coincide
216

fiLearning Past Tense: Symbolic vs Connectionist Models

passthrough strategy. two numbers compared, default strategy employed
examples chosen. instance, example (see Figure 1 (a)),
majority class l (or a) 2 instances. However, 3 examples coinciding
passthrough default: two l one a. Thus passthrough strategy takes over,
assigns empty leaves level. empty attribute branch c would assigned
class c. Note default strategy empty leaves attribute X depends upon
training examples falling subtree rooted X . localized method ensures
related objects uence calculating default classes. result, SPA
adapt default strategy best suited different levels decision trees.
example, Figure 1 (b), two different default strategies used different levels
tree. use SPA adaptive default strategy throughout remainder
paper. Note new default strategy TRICS data representation;
rather, represents bias learning program. learning algorithm default
strategy independent data representation. effect different data representations
generalization discussed Sections 4.3, 4.5, 4.6. passthrough strategy
imposed ANNs well adding set copy connections input units
twin output units. See Section 4.4 detail.

3.3 Comparisons Default Strategies ID3, SPA, ANN
default strategy neural networks tend take generalizing default classes
compared ID3 SPA? conducted several experiments determine neural
networks' default strategy. assume domain one attribute X
may take values a, b, c, d. class one a, b, c, d. training
examples attribute values a, b, c | reserved testing default
class. training set contains multiple copies example form certain
majority class. Table 1 shows two sets training/testing examples used test
compare default strategies ID3, SPA neural networks.
Data set 1
Data set 2
Training examples
Training examples
Values X Class # copies Values X Class # copies


10

c
10
b
b
2
b
b
6
c
c
3
c
c
7
Testing example
Testing example

?
1

?
1
Table 1: Two data sets testing default strategies various methods.
classification testing examples ID3 SPA quite easy decide. Since
ID3 takes majority default, output class (with 10 training examples)
first data set, c (with 17 training examples) second data set. SPA,
number examples using passthrough 15 first data set, 13 second
217

fiLing

data set. Therefore, passthrough strategy wins first case output class
d, majority strategy wins second case output class c.
neural networks, various coding methods used represent values attribute X . dense coding, used 00 represent a, 01 b, 10 c 11
d. tried standard one-per-class encoding, real number encoding (0.2 a,
0.4 b, 0.6 c 0.8 d). networks trained using hidden units
possible case. found cases classification testing example stable; varies different random seeds initialize networks. Table
2 summarises experimental results. ANNs, various classifications obtained 20
different random seeds listed first ones occurring frequently. seems
neural networks consistent default strategy,
neither majority default ID3 passthrough default SPA. may
explain connectionist models cannot generalize unseen regular verbs well even
training set contains regular verbs (see Section 4.4). networks diculty
(or underconstrained) generalizing identity mapping copies attributes
verb stems past tenses.
classification testing example
Data set 1 Data set 2
ID3

c
SPA

c
ANN, dense coding
b; c
b
ANN, one-per-class
b; c;
c; b
ANN, real numbers
c;
d; c
Table 2: Default strategies ID3, SPA ANN two data sets.

4. Head-to-head Comparisons Symbolic ANN Models

section, perform series extensive head-to-head comparisons using several
different representations encoding methods, demonstrate SPA generalizes
past tense unseen verbs better ANN models wide margin.

4.1 Format data

verb set came MacWhinney's original list verbs. set contains
1400 stem/past tense pairs. Learning based upon phonological UNIBET representation (MacWhinney, 1990), different phonemes represented different
alphabetic/numerical letters. total 36 phonemes. source file transferred
standard format pairs input output patterns. example, verbs
Table 3 represented pairs input output patterns (verb stem => past tense):
6,b,&,n,d,6,n
=>
6,b,&,n,d,6,n,d
I,k,s,E,l,6,r,e,t => I,k,s,E,l,6,r,e,t,I,d

218

fiLearning Past Tense: Symbolic vs Connectionist Models

6,r,3,z => 6,r,o,z
b,I,k,6,m => b,I,k,e,m

See Table 3 (The original verb set available Online Appendix 1). keep one
form past tense among multiple past tenses (such hang-hanged hang-hung)
data set. addition, homophones exist original data set. Consequently,
noise (contradictory data input pattern different output
patterns) training testing examples. Note information whether
verb regular irregular provided training/testing processes.
base (stem)
UNIBET
b=base
1 = irregular
spelling form phonetic form d= past tense 0 = regular
abandon
6b&nd6n
b
0
abandoned
6b&nd6nd

0
benefit
bEn6fIt
b
0
benefited
bEn6fItId

0
arise
6r3z
b
0
arose
6roz

1
become
bIk6m
b
0
became
bIkem

1
......
Table 3: Source file MacWhinney Leinbach.

4.2 Experiment Setup

guarantee unbiased reliable comparison results, use training testing samples
randomly drawn several independent runs. SPA ANN provided
sets training/testing examples run. allows us achieve reliable
estimate inductive generalization capabilities model task.
neural network program used package called Xerion, developed
University Toronto. several sophisticated search mechanisms
standard steepest gradient descent method momentum. found training
conjugate-gradient method much faster standard backpropagation
algorithm. Using conjugate-gradient method avoids need search proper
settings parameters learning rate. However, need determine
proper number hidden units. experiments ANNs, first tried various
numbers hidden units chose one produced best predictive accuracy
trial run, use network number hidden units actual runs.
SPA, hand, parameters adjust.
One major difference implementation ANNs SPA SPA take
(symbolic) phoneme letters directly ANNs normally encode phoneme letter
binary bits. (Of course, SPA apply binary representation). studied
various binary encoding methods compared results SPA using symbolic letter
219

fiLing

representation. Since outputs neural networks real numbers, need decode
network outputs back phoneme letters. used standard method decoding:
phoneme letter minimal real-number Hamming distance (smallest angle)
network outputs chosen. see binary encoding affects generalization,
SPA trained binary representation. Since SPA's outputs
binary, decoding process may tie several phoneme letters. case, one
chosen randomly. ects probability correct decoding level
phoneme letters. phoneme letters decoded, one letters
incorrect, whole pattern counted incorrect word level.

4.3 Templated, Distributed Representation

set experiments conducted using distributed representation suggested
MacWhinney Leinbach (1991). According MacWhinney Leinbach, output
left-justified template format CCCVVCCCVVCCCVVCCC, C stands
consonant V vowel space holders. input two components: left-justified
template format input, right-justified template format
VVCCC. example, verb bet, represented UNIBET coding bEt, shown
template format follows ( blank phoneme):
INPUT
bEt
template:
OUTPUT
bEt
template:

b__E_t____________
CCCVVCCCVVCCCVVCCC
(left-justified)

_E__t
VVCCC
(right-justified)

b__E_t____________
CCCVVCCCVVCCCVVCCC
(left-justified)

specific distributed representation | set (binary) phonetic features | used
encode phoneme letters connectionist networks. vowel (V
templates) encoded 8 phonetic features (front, centre, back, high, low, middle, round,
diphthong) consonant (C templates) 10 phonetic features
(voiced, labial, dental, palatal, velar, nasal, liquid, trill, fricative interdental). Note
two feature sets vowels consonants identical, templates
needed order decode right type phoneme letters outputs
network.
experimental comparison, decided use right-justified template
(VVCCC) since information redundant. Therefore, used left-justified
template (CCCVVCCCVVCCCVVCCC) input output. (The whole verb set
templated phoneme representation available Online Appendix 1. contains
1320 pairs verb stems past tenses fit template). ease implementation,
added two extra features always assigned 0 vowel phonetic feature
set. Therefore, vowels consonants encoded 10 binary bits. ANN
thus 18 10 = 180 input bits 180 output bits, found one layer 200
hidden units (same MacWhinney (1993) model) reached highest predictive accuracy
trial run. See Figure 2 network architecture used.
220

fiLearning Past Tense: Symbolic vs Connectionist Models

(180 output units)

C

...

...

...

...

......

...

...

...

C

C

V

V

CCCVVCCCVV

C

C

C

(full connection two layers)
......

(200 hidden units)

......

(full connection two layers)

C

...

...

...

...

......

...

...

...

C

C

V

V

CCCVVCCCVV

C

C

C

(180 input units)

Figure 2: architecture network used experiment.
SPA trained tested data sets phoneme letters directly;
is, 18 decision trees built phoneme letters output templates.
see phonetic feature encoding affects generalization, trained SPA
distributed representation | binary bit patterns 180 input bits
180 output bits | exactly ANN simulation. addition, see
\symbolic" encoding works ANN, train another neural network (with 120
hidden units) \one-per-class" encoding. is, phoneme letter (total 37;
36 phoneme letters plus one blank) encoded 37 bits, one phoneme letter.
used 500 verb pairs (including regular irregular verbs) training
testing sets. Sampling done randomly without replacement, training testing
sets disjoint. Three runs SPA ANN conducted, SPA ANN
trained tested data set run. Training reached 100% accuracy
SPA around 99% ANN.
Testing accuracy novel verbs produced interesting results. ANN model
SPA using distributed representation similar accuracy, ANN
slightly better. may well caused binary outputs SPA suppress
fine differences prediction. hand, SPA using phoneme letters directly
produces much higher accuracy testing. SPA outperforms neural networks (with
either distributed one-per-class representations) 20 percentage points! testing
results ANN SPA found Table 4. findings clearly indicate
SPA using symbolic representation leads much better generalization ANN models.

4.4 Learning Regular Verbs

Predicting past tense unseen verb, either regular irregular,
easy task. Irregular verbs learned rote traditionally thought since
221

fiLing

Distributed representation
ANN: % Correct
SPA: % Correct
Reg Irrg Comb Reg Irrg Comb
65.3 14.6 60.4 62.2 18.8 58.0
59.7 8.6 53.8 57.9 8.2 52.2
60.0 16.0 55.6 58.0 8.0 53.0
61.7 13.1 56.6 59.4 11.7 54.4

Symbolic representation
ANN: % Correct
SPA: % Correct
Reg Irrg Comb Reg Irrg Comb
63.3 18.8 59.2 83.0 29.2 77.8
58.8 10.3 53.2 83.3 22.4 76.2
58.7 16.0 54.4 80.9 20.0 74.8
60.3 15.0 55.6 82.4 23.9 76.3

Table 4: Comparisons testing accuracy SPA ANN distributed symbolic
representations.
children adults occasionally extend irregular ection irregular-sounding regular
verbs pseudo verbs (such cleef | cleft) (Prasada & Pinker, 1993). similar
novel verb cluster irregular verbs similar phonological patterns,
likely prediction irregular past-tense form. Pinker (1991) Prasada
Pinker (1993) argue regular past tenses governed rules, irregulars may
generated associated memory graded effect irregular past-tense
generalization. would interesting, therefore, compare SPA ANN
past-tense generalization regular verbs only. SPA ANN use same,
position specific, representation, learning regular past tenses would require learning different
suxes2 different positions, learn identity mapping copies verb stem
past tenses verbs different lengths.
used templated representation previous section, training
testing sets contained regular verbs. samples drawn randomly without
replacement. maximize size testing sets, testing sets simply consisted
regular verbs sampled training sets. training testing sets
used following methods compared. see effect adaptive
default strategy (as discussed Section 3.2) generalization, SPA majority
default adaptive default tested. ANN models similar
used previous section (except 160 one-layer hidden units, turned
best predictive accuracy test run). passthrough default strategy
imposed neural networks adding set copy connections connect directly
input units twin output units. MacWhinney Leinbach (1991) used
copy connections simulation. therefore tested networks copy
connection see generalization would improved well.
results predictive accuracy SPA ANNs one run
randomly sampled training testing sets summarized Table 5. see,
SPA adaptive default strategy, combines majority passthrough
default, outperforms SPA majority default strategy used ID3.
2. phonological form three different suxes regular verbs. verb stem ends
(UNIBET phonetic representations), sux Id. example, extend | extended (in
spelling form). verb stem ends unvoiced consonant, sux t. example, talk
| talked. verb stem ends voiced consonant vowel, sux d. example,
solve | solved.

222

fiLearning Past Tense: Symbolic vs Connectionist Models

ANNs copy connections generalize better ones without. However, even
ANN models copy connections lower predictive accuracy SPA (majority). addition, differences predictive accuracy larger smaller sets
training examples. Smaller training sets make difference testing accuracy
evident. training set contains 1000 patterns (out 1184), testing accuracy
becomes similar, would approach asymptotically 100% larger training sets.
Upon examination, errors made ANN models occur identity mapping
(i.e., strange phoneme change drop); verb stems cannot preserved past
tense phonemes previously seen training examples. contradicts
findings Prasada Pinker (1993), show native English speakers generate
regular sux-adding past tenses equally well unfamiliar-sounding verb stems (as long
verb stems sound close irregular verbs). indicates bias
ANN learning algorithms suitable type task. See discussion
Section 5.
Training
Percent correct testing
size
SPA (adaptive) SPA (majority) ANN (copy con.) ANN (normal)
50
55.4
30.0
14.6
7.3
100
72.9
58.6
34.6
24.9
300
87.0
83.7
59.8
58.2
500
92.5
89.0
82.6
67.9
1000
93.5
92.4
92.0
87.3
Table 5: Predictive accuracy learning past tense regular verbs

4.5 Error Correcting Codes
Dietterich Bakiri (1991) reported increase predictive accuracy errorcorrecting codes large Hamming distances used encode values attributes.
codes larger Hamming distance (d) allow correcting fewer d=2
bits errors. Thus, learning programs allowed make mistakes bit level
without outputs misinterpreted word level.
wanted find performances SPA ANNs improved errorcorrecting codes encoding 36 phonemes. chose error-correcting codes ranging
ones small Hamming distance ones large Hamming distance (using
BHC codes, see Dietterich Bakiri (1991)). number attributes
phoneme large, data representation changed slightly experiment.
Instead 18 phoneme holders templates, 8 consecutive, left-to-right phoneme holders
used. Verbs stems past tenses 8 phonemes removed
training/testing sets. (The whole verb set representation available Online
Appendix 1. contains 1225 pairs verb stems past tenses whose lengths shorter
8). SPA ANN take exactly training/testing sets, contains 500
pairs verb stems past tenses, error-correcting codes encoding phoneme
223

fiLing

letter. Still, training networks 92-bit longer error-correcting codes takes long
run (there 8 92 = 736 input attributes 736 output attributes). Therefore,
two runs 23- 46-bit codes conducted. Consistent Dietterich Bakiri
(1991)'s findings, found testing accuracy generally increases Hamming
distance increases. However, observed testing accuracy decreases
slightly codes become long. accuracy using 46-bit codes (with Hamming
distance 20) reaches maximum value (77.2%), quite close accuracy
(78.3%) SPA using direct phoneme letter representation. seems trade-off
tolerance errors large Hamming distance diculty learning
longer codes. addition, found testing accuracy ANNs lower one
SPA 23 bit- 46-bit error-correcting codes. results summarized
Table 6.
ANN
Hamming Distance Correct bit level Correct word level
23-bit codes
10
93.5%
65.6%
46-bit codes
20
94.1%
67.4%
SPA
Hamming Distance Correct bit level Correct word level
23-bit codes
10
96.3%
72.4%
46-bit codes
20
96.3%
77.2%
92-bit codes
40
96.1%
75.6%
127-bit codes
54
96.1%
75.4%
Table 6: Comparisons testing accuracy SPA ANNs error-correcting codes
results previous two subsections undermine advantages
distributed representation ANNs, unique feature advocated connectionists.
demonstrated that, task, distributed representation actually allow
adequate generalization. SPA using direct symbolic phoneme letters SPA
error-correcting codes outperform ANNs distributed representation wide margin.
However, neither phoneme symbols bits error-correcting codes encode, implicitly
explicitly, micro-features distributed representation. may
distributed representation used optimally designed. Nevertheless, straightforward
symbolic format requires little representation engineering compared distributed
representation ANNs.

4.6 Right-justified, Isolated Sux Representation

MacWhinney Leinbach (1991) report important results predictive accuracy model unseen regular verbs. reply (MacWhinney, 1993)
paper (Ling & Marinov, 1993), MacWhinney re-implemented ANN model. new
implementation, 1,200 verb stem past-tense pairs training set, among
1081 regular 119 irregular. Training took 4,200 epochs, reached 100%
correct regulars 80% irregulars. testing set consisted 87 regulars 15
irregulars. percent correct testing epoch 4,200 91% regulars 27%
irregulars, combined 80.0% testing set. MacWhinney claimed raw
224

fiLearning Past Tense: Symbolic vs Connectionist Models

generalization power ANN model close SPA. believes
case simply systems trained data set.
realize (via private communication) new representation used MacWhinney's
recent implementation plays critical role improved performance. MacWhinney's
new representation, input (for verb stems) coded right-justified template
CCCVVCCCVVCCCVVCCC. output contains two parts: right-justified template
one input, coda form VVCCC. rightjustified template output used represent past tense without including
sux regular verbs. sux regular past tense always stays coda,
isolated main, right-justified templates. irregular past tense,
coda left empty. example, input output templated patterns past tense
verbs Table 3 represented as:
INPUT
(right-justified)
CCCVVCCCVVCCCVVCCC
___6_b__&_nd_6_n__
b__E_n__6_f__I_t__
________6_r__3_z__
_____b__I_k__6_m__

OUTPUT
(right-justified)
CCCVVCCCVVCCCVVCCC
___6_b__&_nd_6_n__
b__E_n__6_f__I_t__
________6_r__o_z__
_____b__I_k__e_m__

(suffix only)
VVCCC
__d__ (for abandon-abandoned)
I_d__ (for benefit-benefited)
_____ (for arise-arose)
_____ (for become-became)

data representation clearly facilitates learning. regular verbs, output
patterns always identical input patterns. addition, verb-ending phoneme
letters always appear fixed positions (i.e., right VVCCC section
input template) due right-justified, templated representation. Furthermore, sux
always occupies coda, isolated right-justified templates.
performed series experiments see much improvement could accomplish using new representation MacWhinney's recent ANN model
left-justified representation discussed Section 4.3. SPA (with averaged predictive
accuracy 89.0%) outperforms MacWhinney's recent ANN implementation (with predictive accuracy 80.0%) wide margin. addition, predictive accuracy
improved average 76.3% left-justified representation 82.8%
right-justified, isolated sux one. See results Table 7.

5. General Discussion Conclusions

Two factors contribute generalization ability learning program. first
data representation, bias learning program. Arriving
right, optimal, representation dicult task. argued Prasada Pinker
(1993), regular verbs represented coarse grain terms verb stem
suxes; irregular verbs finer grain terms phonological properties.
Admittedly, SPA works uniformly level phoneme letters, ANNs do. However,
SPA produces simple production rules use phoneme letters directly,
rules generalized first-order rules new representations stems
voiced consonants used across board rule-learning
modules (Ling & Marinov, 1993). one major advantages ANN models.
225

fiLing

Predictive accuracy right-justified, isolated sux representation
SPA
MacWhinney's ANN model
training/testing training/testing
training/testing
500/500
1200/102
1200/102
Run 1
81.3
89.2
Run 2
84.1
90.4
Run 3
83.1
87.4
Average
82.8
89.0
80.0 (one run)
Table 7: Comparisons testing accuracy SPA ANN (with right-justified, isolated
sux representation)
seems quite conceivable children acquire high-level concepts stems
voiced consonants learning noun plurals, verb past tense, verb third-person
singular, comparative adjectives, on. large weight matrix result
learning, hard see knowledge generalized ANN models
shared modules.
Even exactly data representation, exist learning tasks
symbolic methods SPA generalize categorically better ANNs. converse true. fact ects different inductive biases different learning
algorithms. Occam's Razor Principle | preferring simplest hypothesis
complex ones | creates preference bias, preference choosing certain hypotheses
others hypothesis space. However, different learning algorithms choose different hypotheses use different measurements simplicity. example, among
possible decision trees fit training examples, ID3 SPA induce simple decision
trees instead complicated ones. Simple decision trees converted small sets
production rules. well learning algorithm generalizes depends upon degree
underlying regularities target concept fit bias. words,
underlying regularities represented compactly format hypotheses produced
learning algorithm, data generalized well, even small set training
examples. Otherwise, underlying regularities large hypothesis,
algorithm looking compact ones (as per Occam's Razor Principle), hypotheses inferred accurate. learning algorithm searches hypotheses larger
necessary (i.e., use Occam's Razor Principle) normally \underconstrained"; know, based training examples only, many
competitive hypotheses large size inferred.
describe bias learning algorithm looking training examples
different classes separated n-dimensional hyperspace n number
attributes. decision node decision tree forms hyperplane described
linear function X = a. hyperplanes perpendicular axis,
partial-space hyperplanes extend within subregion formed
hyperplanes parents' nodes. Likewise, hidden units threshold function
ANNs viewed forming hyperplanes hyperspace. However, unlike ones
decision trees, need perpendicular axis, full-space
226

fiLearning Past Tense: Symbolic vs Connectionist Models

hyperplanes extend whole space. ID3 applied concepts fit
ANN's bias, especially hyperplanes perpendicular axis, many
zigzag hyperplanes perpendicular axes would needed separate different
classes examples. Hence, large decision tree would needed, fit
ID3's bias. Similarly, ANN learning algorithms applied concepts fit ID3's
bias, especially hyperplanes form many separated, partial-space regions, many
hidden units may needed regions.
Another major difference ANNs ID3 ANNs larger variation
weaker bias (cf. (Geman, Bienenstock, & Doursat, 1992)) ID3. Many
Boolean functions (e.g., linearly separable functions) fit small network (e.g., one
hidden units) small decision tree. sometimes attributed
claimed versatility exibility ANNs; learn (but necessarily predict reliably well) many functions, symbolic methods brittle. However, belief
humans versatile, learning algorithm large variation,
rather set strong-biased learning algorithms, somehow
search bias space add new members set new learning tasks. Symbolic learning algorithms clear semantic components explicit representation,
thus easily construct strong-based algorithms motivated various specific
learning tasks. adaptive default strategy SPA example.
hand, still largely know effectively strengthen bias ANNs many
specific tasks (such identity mapping, k-term DNF, etc.). techniques (such
adding copy connections weight decaying) exist, exact effects biasing
towards classes functions clear.
analyses (Ling & Marinov, 1993), underlying regularities governing
ection past tense English verbs form small set production rules
phoneme letters. especially regular verbs; rules either identity
rules sux-adding rules. example, decision trees converted set
precedence-ordered production rules complicated rules (rules conditions) listed first. example, using consecutive, left-to-right phonetic representation,
typical sux-adding rule verb stems 4 phoneme letters (such talk | talked) is:
4 = k 5 = , !5 =
is, fourth input phoneme k fifth blank (i.e., verb
ending) fifth output phoneme t. hand, identity-mapping rules
one condition. typical identity rule looks like:
3 = l, !3 = l
fact, passthrough default strategy allows identity-mapping rules represented simple first-order format:
3 = X, !3 = X
X phoneme. Clearly, knowledge forming regular past tenses
thus expressed simple, conjunctive rules fit bias SPA (ID3),
therefore, SPA much better generalization ability ANN models.
conclude, demonstrated, via extensive head-to-head comparisons,
SPA realistic better generalization capacity ANNs learning
past tense English verbs. argued symbolic decision-tree/production-rule
learning algorithms outperform ANNs. because, first, domain seems
227

fiLing

governed compact set rules, thus fits bias symbolic learning algorithm;
second, SPA directly manipulates representation better ANNs (i.e.,
symbolic phoneme letters vs. distributed representation); third, SPA able
derive high-level concepts used throughout English morphology. results support
view many high-level, rule-governed cognitive tasks better modeled
symbolic, rather connectionist, systems.

Acknowledgements
gratefully thank Steve Pinker constant encouragement, Marin Marinov, Steve
Cherwenka Huaqing Zeng discussions help implementing SPA.
thank Brian MacWhinney providing verb data used simulation. Discussions
Tom Dietterich, Dave Touretzky Brian MacWhinney, well comments
reviewers, helpful. research conducted support NSERC
Research Grant computing facilities Department.

References

Cottrell, G., & Plunkett, K. (1991). Using recurrent net learn past tense.
Proceedings Cognitive Science Society Conference.
Daugherty, K., & Seidenberg, M. (1993). Beyond rules exceptions: connectionist
modeling approach ectional morphology. Lima, S. (Ed.), Reality
Linguistic Rules. John Benjamins.
Dietterich, T., & Bakiri, G. (1991). Error-correcting output codes: general method
improving multiclass inductive learning programs. AAAI-91 (Proceedings Ninth
National Conference Artificial Intelligence).
Dietterich, T., Hild, H., & Bakiri, G. (1990). comparative study ID3 backpropagation English text-to-speech mapping. Proceedings 7th International
Conference Machine Learning. Morgan Kaufmann.
Feng, C., King, R., Sutherland, A., & Henery, R. (1992). Comparison symbolic, statistical neural network classifiers. Manuscript. Department Computer Science,
University Ottawa.
Fodor, J., & Pylyshyn, Z. (1988). Connectionism cognitive architecture: critical
analysis. Pinker, S., & Mehler, J. (Eds.), Connections Symbols, pp. 3 { 71.
Cambridge, MA: MIT Press.
Geman, S., Bienenstock, E., & Doursat, R. (1992). Neural networks bias/variance
dilemma. Neural Computation, 4, 1 { 58.
Lachter, J., & Bever, T. (1988). relation linguistic structure associative
theories language learning { constructive critique connectionist learning
models. Pinker, S., & Mehler, J. (Eds.), Connections Symbols, pp. 195 { 247.
Cambridge, MA: MIT Press.
228

fiLearning Past Tense: Symbolic vs Connectionist Models

Ling, X., Cherwenka, S., & Marinov, M. (1993). symbolic model learning past
tenses English verbs. Proceedings IJCAI-93 (Thirteenth International Conference Artificial Intelligence), pp. 1143{1149. Morgan Kaufmann Publishers.
Ling, X., & Marinov, M. (1993). Answering connectionist challenge: symbolic model
learning past tense English verbs. Cognition, 49 (3), 235{290.
MacWhinney, B. (1990). CHILDES Project: Tools Analyzing Talk. Hillsdale, NJ:
Erlbaum.
MacWhinney, B. (1993). Connections symbols: closing gap. Cognition, 49 (3),
291{296.
MacWhinney, B., & Leinbach, J. (1991). Implementations conceptualizations: Revising verb model. Cognition, 40, 121 { 157.
Pinker, S. (1991). Rules language. Science, 253, 530 { 535.
Pinker, S., & Prince, A. (1988). language connectionism: Analysis parallel
distributed processing model language acquisition. Pinker, S., & Mehler, J.
(Eds.), Connections Symbols, pp. 73 { 193. Cambridge, MA: MIT Press.
Plunkett, K., & Marchman, V. (1991). U-shaped learning frequency effects multilayered perceptron: Implications child language acquisition. Cognition, 38, 43 {
102.
Prasada, S., & Pinker, S. (1993). Generalization regular irregular morphological
patterns. Language Cognitive Processes, 8 (1), 1 { 56.
Quinlan, J. (1986). Induction decision trees. Machine Learning, 1 (1), 81 { 106.
Quinlan, J. (1993). C4.5 Programs Machine Learning. Morgan Kaufmann: San Mateo,
CA.
Ripley, B. (1992). Statistical aspects neural networks. Invited lectures SemStat
(Seminaire Europeen de Statistique, Sandbjerg, Denmark, 25-30 April 1992).
Rumelhart, D., & McClelland, J. (1986). learning past tenses English verbs.
Rumelhart, D., McClelland, J., & PDP Research Group (Eds.), Parallel Distributed Processing Vol 2, pp. 216 { 271. Cambridge, MA: MIT Press.
Shavlik, J., Mooney, R., & Towell, G. (1991). Symbolic neural learning algorithms:
experimental comparison. Machine Learning, 6 (2), 111 { 144.
Weiss, S., & Kulikowski, C. (1991). Computer Systems Learn: classification prediction methods statistics, neural networks, machine learning, expert systems.
Morgan Kaufmann, San Mateo, CA.

229


