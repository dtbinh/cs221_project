Journal Artificial Intelligence Research 40 (2011) 767-813

Submitted 11/10; published 04/11

Scaling Heuristic Planning Relational Decision Trees
Tomas de la Rosa
Sergio Jimenez
Raquel Fuentetaja
Daniel Borrajo

TROSA @ INF. UC 3 . ES
SJIMENEZ @ INF. UC 3 . ES
RFUENTET @ INF. UC 3 . ES
DBORRAJO @ IA . UC 3 . ES

Departamento de Informatica
Universidad Carlos III de Madrid
Av. Universidad 30, Leganes, Madrid, Spain

Abstract
Current evaluation functions heuristic planning expensive compute. numerous
planning problems functions provide good guidance solution, worth
expense. However, evaluation functions misguiding planning problems large
enough, lots node evaluations must computed, severely limits scalability heuristic planners. paper, present novel solution reducing node evaluations heuristic
planning based machine learning. Particularly, define task learning search control
heuristic planning relational classification task, use off-the-shelf relational classification tool address learning task. relational classification task captures preferred action
select different planning contexts specific planning domain. planning contexts
defined set helpful actions current state, goals remaining achieved,
static predicates planning task. paper shows two methods guiding search
heuristic planner learned classifiers. first one consists using resulting classifier action policy. second one consists applying classifier generate lookahead
states within Best First Search algorithm. Experiments variety domains reveal
heuristic planner using learned classifiers solves larger problems state-of-the-art planners.

1. Introduction
last years, state-space heuristic search planning achieved significant results
become one popular paradigms automated planning. However, heuristic search
planners suffer strong scalability limitations. Even well-studied domains Blocksworld
become challenging planners number blocks relatively large. Usually, statespace heuristic search planners based action grounding, makes state-space
explored large number objects and/or action parameters large enough. Moreover,
domain-independent heuristics expensive compute. domains heuristics
misleading, heuristic planners spend planning time computing useless node
evaluations. Even best current domain-independent heuristic functions literature,
forward chaining heuristic planners currently visit many nodes, takes considerable
time, especially due time required compute heuristic functions.
problems entail strong limitations application heuristic planners real problems. instance, logistics applications need handle hundreds objects together hundreds
vehicles locations (Florez, Garca, Torralba, Linares, Garca-Olaya, & Borrajo, 2010). Current heuristic search planners exhaust computational resources solving problem
real logistics application.
c
2011
AI Access Foundation. rights reserved.

fiD E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

classic approach dealing planning scalability issues assisting search engines
planners Domain-specific Control Knowledge (DCK). Examples planning systems
benefit knowledge TLP LAN (Bacchus & Kabanza, 2000), TALP LANNER (Doherty
& Kvarnstrom, 2001) SHOP2 (Nau, Au, Ilghami, Kuter, Murdock, Wu, & Yaman, 2003).
Nevertheless, hand-coding DCK complex task implies expertise both, planning domain search algorithm planning system. recent years
renewed interest using Machine Learning (ML) automatically extract DCK. Zimmerman
Kambhampati (2003) made comprehensive survey ML defining DCK. shown first
learning planning competition held 2008 (Learning Track), renewed interest specially
targeted heuristic planners.
paper presents approach learning DCK planning building domain-dependent
relational decision trees examples good quality solutions forward-chaining heuristic
planner. decision trees built off-the-shelf relational classification tool capture best action take possible decision planner given domain.
resulting decision trees used either policy solve planning problems directly
generate lookahead states within Best First Search (BFS) algorithm. techniques allow
planner avoid state evaluations, helps objective improving scalability.
approach implemented system called ROLLER. work improvement
previous one (De la Rosa, Jimenez, & Borrajo, 2008). Alternatively, ROLLER version
repairing relaxed plans (De la Rosa, Jimenez, Garca-Duran, Fernandez, Garca-Olaya, & Borrajo,
2009) competed Learning Track 6th International Planning Competition (IPC) held
2008. ROLLER improvements presented article mainly result lessons learned
competition, discussed later.
paper organized follows. Section 2 introduces issues need considered
designing learning system heuristic planning. help us clarify decisions made development approach. Section 3 describes ROLLER system
detail. Section 4 presents experimental results obtained variety benchmarks. Section 5
discusses improvements ROLLER system compared previous version system.
Section 6 revises related work learning DCK heuristic planning. Finally, last section
discusses conclusions future work.

2. Common Issues Learning Domain-specific Control Knowledge
designing ML process automatic acquisition DCK, one must consider
common issues, among others:
1. representation learned knowledge. Predicate logic common language
represent planning DCK planning tasks usually defined language. However, representation languages used aiming make learning DCK
effective. instance, languages describing object classes Concept
Language (Martin & Geffner, 2000) Taxonomic Syntax (Mcallester & Givan, 1989)
shown provide useful learning bias different domains.
Another representation issue selection feature space (i.e., set instance
features used representing learned knowledge training system.). feature
space able capture key knowledge domain. Traditionally, feature
768

fiS CALING H EURISTIC P LANNING R ELATIONAL ECISION REES

space consisted predicates describing current state goals planning
task. feature space enriched extra predicates, called metapredicates,
capture extra useful information planning context applicable operators
pending goals (Veloso, Carbonell, Perez, Borrajo, Fink, & Blythe, 1995). Recently, works
learning DCK heuristic planners define metapredicates capture information
planning context heuristic planner, including example, predicates capture
actions relaxed plan given state (Yoon, Fern, & Givan, 2008).
2. learning algorithms. Inductive Logic Programming (ILP) (Muggleton & De Raedt,
1994) deals development inductive techniques learn given target concept
examples described predicate logic. planning tasks normally represented
predicate logic, ILP algorithms quite suitable DCK learning. Moreover, recent
years, ILP broadened scope cover whole spectrum ML tasks regression, clustering association analysis, extending classical propositional ML algorithms
relational framework. Consequently, ILP algorithms used heuristic planners capture DCK different forms decision rules select actions different
planning context regression rules obtain better node evaluations (Yoon et al., 2008).
3. generation training examples. success ML algorithms depends directly
quality training examples used. learning planning DCK, examples
extracted experience collected solving training problems,
representative different tasks across domain. Therefore, quality training
examples depend variety problems used training quality
solutions problems. Traditionally, training problems obtained random
generators provided parameters tune problems difficulty. way, one
find, domain, kind problems makes learning algorithm generalize
useful DCK.
4. Use learned DCK. Decisions made three issues affect quality
learned DCK. representation schemes may expressive enough capture effective DCK given domain, learning algorithm may able acquire useful
DCK within reasonable time memory requirements, set training problems may
lack significant examples key knowledge. situations, direct use
learned DCK improve scalability planner, could even decrease performance. effective way dealing problem heuristic planners integrating
learned DCK within robust strategies Best-First Search (Yoon et al., 2008)
combining domain-independent heuristic functions (Roger & Helmert, 2010).

3. ROLLER System
section describes general scheme learning DCK instantiated ROLLER
system. First, describes DCK representation followed ROLLER. Second, explains
learning algorithm used ROLLER. Third, depicts ROLLER collects good quality training
examples finally, shows different approaches scaling heuristic planning algorithms
learned DCK.
769

fiD E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

3.1 Representation Learned Knowledge: Helpful Contexts Heuristic Planning
present approach following notation specified Planning Domain Definition Language (PDDL) typed STRIPS tasks. Accordingly, definition planning domain comprises definition of:
hierarchy types.
set typed constants, CD , representing objects present tasks domain.
set empty.
set predicate symbols, P, one corresponding arity type
arguments.
set operators O, whose arguments typed variables.
Variables declared directly defining operator argument, local
operator definition. call Po set atomic formulas generated using
defined predicates P, variables defined arguments operator O, general
constants CD . Then, operator defined three sets: pre(o) Po , operator
preconditions; add(o) Po , positive effects; del(o) Po , negative effects
operator.
planning task domain tuple < C , s0 , G > C set typed
constants representing objects particular task, s0 set ground atomic
formulas describing initial state G set ground atomic formulas describing goals.
Given total set constants C = C CD , task defines finite state space finite set
instantiated operators O. state set ground atomic formulas representing
facts true s. States described following closed world assumption. instantiated
operator action operator variable replaced constant C
type. Thus, set actions generated using set constants C
set operators O. definition, solving planning task implies finding plan
sequence actions (a1 , . . . , ), ai transforms initial state state
goals achieved.
planning contexts defined ROLLER rely concepts relaxed plan heuristic
helpful actions, introduced planner (Hoffmann & Nebel, 2001). relaxed plan
heuristic returns integer evaluated node, number actions solution
relaxed planning task + node. + simplification original task
deletes actions ignored. idea delete-relaxation computing heuristics planning
first introduced McDermott (1996) Bonet, Loerincs Geffner (1997).
relaxed plan extracted relaxed planning graph, sequence facts
actions layers (F0 , A0 , . . . , , Ft ). first fact layer contains facts initial state.
action layer contains set applicable actions given previous fact layer. fact
layer contains set positive effects actions appearing previous layers.
process finishes goals fact layer, two consecutive facts layers
facts. last case, relaxed problems solution relaxed plan heuristic
returns infinity.
770

fiS CALING H EURISTIC P LANNING R ELATIONAL ECISION REES

relaxed planning graph built, solution extracted backwards process.
goal appearing first time fact layer assigned set goals layer, Gi . Then,
last set goals, Gt , second set goals, G1 , goal goals set,
action selected generates goal whose layer index minimal. Afterwards,
precondition action (i.e. subgoal) included goals set corresponding first
layer fact appears. process finished, set selected actions comprises
relaxed plan.
According extraction process, planner marks helpful actions set actions
first layer A0 relaxed planning graph achieve subgoals next
fact layer, i.e. goals set G1 . words, helpful actions applicable actions
generate facts top-level goals problems required action relaxed plan.
Formally, set helpful actions given state defined as:
helpful (s) = {a A0 | add(a) G1 6= }
planner uses helpful actions search pruning technique, considered candidates selected search. Given state generates
particular set helpful actions, claim helpful actions, together remaining goals static literals planning task, encode helpful context related state.
helpful actions remaining target goals relate actions likely applied
goals need achieved. relations arise helpful actions target
goals often share arguments (problem objects). Additionally, static predicates express
facts characterize objects planning task. Identifying objects relevant since
may shared arguments helpful actions and/or target goals.
Definition 1 helpful context state defined
H(s) = {helpful (s), target(s), static(s)}
target(s) G describes set goals achieved state s, target(s) = G
static(s) set literals always hold planning task. defined
initial state present every state given changed action. Thus,
static(s) = {p | @a : p add(a) p del(a)}.
helpful context alternative representation tuple <state, goals, applied action>,
traditionally used learning DCK planning. Helpful contexts present advantages
improving scalability heuristic planners:
domains, set helpful actions contains actions likely applied
focusing reasoning shown good strategy.
set helpful actions normally smaller set non-static literals state
(i.e., static(s)). Thus, process matching learned DCK within search obtains
benefits using compact representation.
number helpful actions normally decreases search fewer goals left.
Therefore, matching process become faster search advancing towards
goals.
771

fiD E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

3.2 Learning Algorithm: Learning Generalized Policies Relational Decision Trees
ROLLER implements two-step learning process building DCK collection examples
different helpful contexts:

1. Learning operator classifier. ROLLER builds classifier choose best operator
different helpful contexts.
2. Learning binding classifiers. operator domain, ROLLER builds classifier
choose best binding (instantiation operator) different helpful contexts.
learning process split two steps build DCK off-the-shelf learning
tools. planning action may different number arguments arguments different types (e.g. actions switch on(instrument,satellite) turn to(satellite,
direction,direction) Satellite domain) hinders definition target
classes. two-step decision process clearer decision-making point view.
helps users understand generated DCK better focusing either decision operator apply bindings use given selected operator. learning algorithm
set learning examples two learning steps. Figure 1 shows overview
learning process ROLLER system.

Roller Learner
Training
Problems

1. operator classifier
Example
Generator

PDDL
Domain

op. examples

Relational
Classification

bind. examples

Tool

2. binding classifiers

...

language bias

Figure 1: Overview ROLLER learning process.

3.2.1 L EARNING R ELATIONAL ECISION RESS
classic approach assist decision making consists gathering significant set previous decisions building decision tree generalizes them. leaves resulting tree contain
classes (decisions make), internal nodes contain conditions lead decisions. common way build trees following Top-Down Induction Decision
Trees (TDIDT) algorithm (Quinlan, 1986). algorithm builds tree repeatedly splitting
set training examples according conditions minimize entropy examples. Traditionally, training examples described attribute-value representation. Therefore,
conditions decision trees represent tests value given attribute examples.
Nevertheless, attribute-value approach suitable representing decisions want
keep predicate logic representation. better approach represent decisions relationally,
instance, given action chosen reach certain goals given context share arguments. Recently, new algorithms building relational decision trees examples described
772

fiS CALING H EURISTIC P LANNING R ELATIONAL ECISION REES

predicate logic facts developed. new relational learning algorithms similar
propositional ones, except (1) condition nodes tree refer attribute values,
logic queries relational facts holding training examples (2), logic queries
share variables condition nodes placed decision tree. learning algorithm
greedy search process. Since space potential relational decision trees usually huge,
search normally biased according specification syntactic restrictions called language bias.
specification contains target concept, predicates appear condition nodes
trees learning-specific knowledge type information, input output
variables predicates.
paper use tool TILDE (Blockeel & De Raedt, 1998) learning operator
binding classifiers. tool implements relational version TDIDT algorithm, although
off-the-shelf tool learning relational classifiers could used, PRO GOL (Muggleton, 1995) RIBL (Emde & Wettschereck, 1996). different learning
algorithms would provide different results, since explore classifiers space differently.
study pros cons different algorithms beyond scope paper.
comprehensive explanation current relational learning approaches please refer work De
Raedt (2008).
3.2.2 L EARNING PERATOR C LASSIFIER
inputs learning operator classifier are:
Training examples. Examples represented Prolog-like syntax consist
operator selected (the class) together helpful context (the background knowledge
terms relational learning) selected. particular, example contains:
Class. use predicate arity 3 selected encode operator chosen
context. predicate target concept learning step. first argument holds
example identifier links rest example predicates. second argument
problem identifier, links static predicates shared examples coming
planning problem. third argument example class, i.e., name
selected operator helpful context.
Helpful predicates. predicates express helpful actions contained
helpful context. predicate symbol predicates helpful ai ai
name instantiated action. arguments example problem
identifier together parameters action ai . instantiated action,
parameters constants.
Target goal predicates. represent predicates appear goals
hold current state. predicates form target goal gi
gi domain predicates. predicate contains example problem
identifiers.
Static predicates. represent static predicates given problem. predicates shared training examples belong planning problem.
form static fact domain predicates
appear effects domain action. arguments problem identifier corresponding arguments domain predicate.
773

fiD E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

Figure 2 shows one learning example id tr01 e1 consisting selection operator switch-on associated helpful context. example used building
operator classifier Satellite domain.
% Example tr01 e1 problem tr01
selected(tr01 e1,tr01,switch on).
helpful turn to(tr01 e1,tr01,satellite0,groundstation1,star0).
helpful turn to(tr01 e1,tr01,satellite0,phenomenon2,star0).
helpful turn to(tr01 e1,tr01,satellite0,phenomenon3,star0).
helpful turn to(tr01 e1,tr01,satellite0,phenomenon4,star0).
helpful switch on(tr01 e1,tr01,instrument0,satellite0).
target goal image(tr01 e1,tr01,phenomenon3,infrared2).
target goal image(tr01 e1,tr01,phenomenon4,infrared2).
target goal image(tr01 e1,tr01,phenomenon2,spectrograph1).
% Static Predicates problem
static fact calibration target(tr01,instrument0,groundstation1).
static fact supports(tr01,instrument0,spectrograph1).
static fact supports(tr01,instrument0,infrared2).
static fact board(tr01,instrument0,satellite0).

Figure 2: Knowledge base corresponding example Satellite domain. example
id tr01 e1, links example predicates. obtained solving
training problem tr01 links rest examples problem.
selected operator helpful context switch on, corresponds one
helpful actions encoded helpful predicates example.

Language bias: bias specifies constraints arguments predicates
training examples. assume domain-specific constraint, given learning
technique domain-independent. So, bias contains restrictions argument types
restrictions ensure identifier variables added new variables
classifier generation. bias automatically extracted PDDL domain definitions
consists declaration predicates used learning example argument
types. Figure 3 shows language bias specified learning operator classifier
Satellite domain.
resulting relational decision tree represents set disjoint rules action selection
used provide advice planner: internal nodes tree contain set conditions related helpful context advice provided. leaf nodes contain
corresponding advice; case, operator select number examples covered
rule. operator select one selected often training
examples covered rule. operator classifiers learned ROLLER advise nonhelpful actions. Given state, non-helpful actions subset applicable actions state
considered helpful actions. Certainly, actions part helpful contexts defined. However, learned operator classifiers indicate name operator select
regardless whether helpful not. Figure 4 shows operator tree learned Satellite
774

fiS CALING H EURISTIC P LANNING R ELATIONAL ECISION REES

% ---- target concept ---predict(selected(+IdExample,+IdProblem,-Operator)).
type(selected(idex,idprob,class)).
classes([turn to,switch on,switch off,calibrate,take image]).
% ---- helpful context ---% predicates helpful actions
rmode(helpful turn to(+IdExample,+IdProblem,+-S1,+-D1,+-D2)).
type(helpful turn to(idex,idprob,satellite,direction,direction)).
rmode(helpful switch on(+IdExample,+IdProblem,+-I1,+-S1)).
type(helpful switch on(idex,idprob,instrument,satellite)).
rmode(helpful switch off(+IdExample,+IdProblem,+-I1,+-S1)).
type(helpful switch off(idex,idprob,instrument,satellite)).
rmode(helpful calibrate(+IdExample,+IdProblem,+-S1,+-I1,+-D1)).
type(helpful calibrate(idex,idprob,satellite,instrument,direction)).
rmode(helpful take image(+IdExample,+IdProblem,+-S1,+-D1,+-I1,+-M1)).
type(helpful take image(idex,idprob,satellite,direction,instrument,mode)).
% predicates target goals
rmode(target goal pointing(+IdExample,+IdProblem,+-S1,+-D1)).
type(target goal pointing(idex,idprob,satellite,direction)).
rmode(target goal image(+IdExample,+IdProblem,+-D1,+-M1)).
type(target goal image(idex,idprob,direction,mode)).
% predicates static facts
rmode(static fact board(+IdProblem,+-I1,+-S1)).
type(static fact board(idprob,instrument,satellite)).
rmode(static fact supports(+IdProblem,+-I1,+-M1)).
type(static fact supports(idprob,instrument,mode)).
rmode(static fact calibration target(+IdProblem,+-I1,+-D1)).
type(static fact calibration target(idprob,instrument,direction)).

Figure 3: Language bias learning operator classifier Satellite domain. automatically generated PDDL definition. rmode predicates indicate
used tree. type predicates indicate types particular rmode.

domain. learned decision trees branch denoted symbols +--:<yes/no>,
yes indicates next node positive answers current question indicates next
node negative answers. figure, first branch states calibrate
action set helpful actions, recommendation (in square brackets) choosing action
(i.e. calibrate). addition, branch indicates recommended action occurred 44
times training examples. Moreover, leaf node information (in double square brack775

fiD E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

ets) number times type action selected training examples covered
rule current branch. Thus, case, action calibrate selected 44
total 44 times, operators never selected. second branch says
calibrate helpful action, take image one, planner selected
take image 110 110 times. helpful calibrate take image actions helpful switch action, switch recommendation,
selected 44 59 times. tree branches interpreted similarly.
selected(-A,-B,-C)
helpful calibrate(A,B,-D,-E,-F) ?
+--yes:[calibrate] 44.0 [[turn to:0.0,switch on:0.0,switch off:0.0,
|
calibrate:44.0,take image:0.0]]
+--no: helpful take image(A,B,-G,-H,-I,-J) ?
+--yes:[take image] 110.0 [[turn to:0.0,switch on:0.0,switch off:0.0,
|
calibrate:0.0,take image:110.0]]
+--no: helpful switch on(A,B,-K,-L) ?
+--yes:[switch on] 59.0 [[turn to:15.0,switch on:44.0,
|
switch off:0.0,calibrate:0.0,
|
take image:0.0]]
+--no: [turn to] 149.0 [[turn to:149.0,switch on:0.0,
switch off:0.0,calibrate:0.0,
take image:0.0]]

Figure 4: Relational decision tree learned operator selection Satellite domain. Internal
nodes (with ? ending) queries helpful contexts. Leaf nodes (in brackets)
class number observed examples operator.

3.2.3 L EARNING B INDING C LASSIFIERS
second learning step, relational decision tree built domain operator O.
trees indicate bindings select different helpful contexts. inputs learning
binding classifier operator are:
Training examples. consist exclusively helpful contexts operator
selected, together applicable instantiations contexts. Note
given helpful context, applicable instantiations may include helpful nonhelpful actions. Helpful contexts coded exactly previous learning step.
applicable instantiations represented selected predicate. predicate target concept second learning step arguments example
problem identifiers, instantiated arguments applicable action example class (selected rejected). purpose predicate distinguish
good bad bindings operator. Figure 5 shows piece knowledge base
building binding tree corresponding action switch Satellite domain. example, id tr07 e63, resulted selection action instantiation
776

fiS CALING H EURISTIC P LANNING R ELATIONAL ECISION REES

switch on(instrument1,satellite0). action switch on(instrument0,
satellite0) applicable rejected planner.
Language bias: bias learning binding trees bias learning
operator tree, except includes definition selected predicate.
previous learning step, language bias learning binding tree automatically extracted PDDL domain definition. Figure 6 shows part language bias specified
learning binding tree action switch Satellite domain.

% Example tr07 e63 problem tr07
selected switch on(tr07 e63,tr07,instrument0,satellite0,rejected).
selected switch on(tr07 e63,tr07,instrument1,satellite0,selected).
helpful switch on(tr07 e63,tr07,instrument0,satellite0).
helpful switch on(tr07 e63,tr07,instrument1,satellite0).
helpful turn to(tr07 e63,tr07,satellite0,star1,star2).
helpful turn to(tr07 e63,tr07,satellite0,star5,star2).
helpful turn to(tr07 e63,tr07,satellite0,phenomenon7,star2).
helpful turn to(tr07 e63,tr07,satellite0,phenomenon8,star2).
target goal image(tr07 e63,tr07,phenomenon8,spectrograph2).
target goal image(tr07 e63,tr07,phenomenon7,spectrograph2).
target goal image(tr07 e63,tr07,star5,image1).
% Static Predicates problem
static fact calibration target(tr07,instrument0,star1).
static fact calibration target(tr07,instrument1,star1).
static fact supports(tr07,instrument0,image1).
static fact supports(tr07,instrument1,spectrograph2).
static fact supports(tr07,instrument1,image1).
static fact supports(tr07,instrument1,image4).
static fact board(tr07,instrument0,satellite0).
static fact board(tr07,instrument1,satellite0).

Figure 5: Knowledge base corresponding example tr07 e63 obtained solving training problem tr07 Satellite domain.
result second learning step relational decision tree uninstantiated
operator O. consists set disjoint rules binding selection o. Figure 7
shows example binding tree tswitch built operator switch Satellite
domain. According tree, first branch states helpful action
switch instrument C satellite D, switch bindings (C, D) selected
planner 213 249 times. Note binding trees learned ROLLER advise
non-helpful actions. Frequently, selected predicate matches tree queries refer
helpful predicates. cases, no-branch query may cover bindings non-helpful
actions operator.
binding trees Satellite domain refer reader Online Appendix
article, include learned DCK domains used experimental section.
777

fiD E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

% ---- target concept ---predict(selected switch on(+IdExample,+IdProblem,+INST0,+SAT1,-Class)).
type(selected switch on(idex,idprob,instrument,satellite,class)).
classes([selected,rejected]).
% ---- helpful context ----, operator classification
...

Figure 6: Part language bias learning binding tree switch action
Satellite domain.

selected switch on(-A,-B,-C,-D,-E)
helpful switch on(A,B,C,D) ?
+--yes: [selected] 249.0 [[selected:213.0,rejected:36.0]]
+--no: [rejected] 63.0 [[selected:2.0,rejected:61.0]]

Figure 7: Relational decision tree learned bindings selection switch action
Satellite domain.

many cases, decision trees somewhat complex one shown Figure 7.
instance, turn binding tree 29 nodes includes several queries target goals (e.g.,
asking pending image new pointed direction) others static facts (e.g.,
asking new pointed direction calibration target).
3.3 Generation Training Examples
ROLLER training examples instances decisions made solving training problems. order
characterize variety good solutions, decisions consider different alternatives
solving individual problem. given search tree node (state), alternatives come
possibility choosing different operators different bindings single operator,
cases assuming alternative lead equally good solutions.

Regarding binding decisions, actions alternative solutions ignored,
tagged rejected consequently introduce noise learning process. instance,
consider problem Figure 8 Satellite domain satellite, calibrated
instrument, must turn directions D1,D2 D3 order take images there. planning context, three turn actions helpful actions regarding one solution makes
learning consider one action selected two actions rejected. However, learned
knowledge always recommend helpful turn action towards direction
satellite (with corresponding calibrated instrument) take image. learn kind
knowledge, ROLLER consider three turn actions selected three actions correspond selectable actions learning correct knowledge particular planning
778

fiS CALING H EURISTIC P LANNING R ELATIONAL ECISION REES

context. actions marked rejected learner consider selecting turn
described context bad choice.
takeimage D2

S3

S4

turnto(D2,D3)

S5

takeimage D3

turnto(D1,D2)
takeimage D1
turnto(D4,D1)

S1

turnto(D4,D2)
S0

S1

turnto(D4,D3)
S1"

...

S2

turnto(D1,D3)
S3

...

...

Figure 8: Solution path alternatives simplified Satellite problem.
Regarding operator decisions, complete training full catalogue different solutions
confuse learning process. instance, consider example problem Figure 9
goal take image direction D2. applying calibrate action s2 , necessary
switch instrument turn satellite D1 (the calibration target direction).
two actions helpful generate two different solution paths. fact, commutative.
Generalizing operator selection kinds helpful contexts difficult training
examples contain examples types (i.e. examples switch-on action situated
turn-to action vice versa). caused fact helpful
context different operators choose equally good choices.
S1
turnto(D3,D1)

switchon

S2

S0

switchon

calibrate

S3

turnto(D1,D2)

S4

takeimage D2

G

turnto(D3,D1)
S1

Figure 9: Solution path alternatives simplified Satellite problem.
ROLLER follows commitment approach generation training examples: (1) Generation solutions. Given training problem, ROLLER performs exhaustive search obtain
multiple best-cost solutions, taking account alternatives different binding choices. (2)
Selection solutions. ROLLER selects subset solutions set best-cost solutions
order reproduce particular preference operator alternatives. (3) Extraction examples solutions. ROLLER encodes selected subset solutions examples required
learning, operator classification binding classification. following sections detail ROLLER
proceeds three steps.

779

G

fiD E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

3.3.1 G ENERATION OLUTIONS
ROLLER solves training problem using Best-First Branch Bound (BFS-BnB) algorithm
extracts multiple good-quality solutions. search space explored exhaustively
within time bound, problem discarded examples generated it. Therefore,
training problems need sufficiently small. addition, training problems need representative enough generalize DCK assists ROLLER solving future problems
domain.
BFS-BnB search completed without pruning repeated states. practice, many repeated
states generated changing order among actions different solution paths. Thus, pruning
repeated states would involve tagging actions leading solutions rejected bindings,
fact true. addition, BFS-BnB algorithm prunes according evaluation
function f (n) = g(n) + h(n), g(n) node cost (in work use plan length
cost function) h(n) heuristic. safe way prune search space using
admissible heuristic. However, existing admissible heuristics allow ROLLER complete
exhaustive search problems reasonable size. practice, using heuristic produces
overestimations introduces negligible noise learning process. end
search, BFS-BnB algorithm returns set solutions best cost. solutions
used tag nodes search tree belong solutions label solution.

3.3.2 ELECTING OLUTIONS
set best-cost solutions found, ROLLER selects subset solutions used
generating training examples. Since difficult develop domain-independent criteria systematically selecting solutions reproduce operator selection particular context,
defined approach which, heuristically, prefers actions others. preferences
are:
Least-commitment preference: Prefer actions generate alternatives different solution paths.
Difficulty preference: Prefer actions reach goals sub-goals difficult
achieve. example Figure 9 instrument switched achievable
one action. hand, pointing direction D1 considered easier since
achieved actions turn to(D2,D1) turn to(D3,D1).
Given 0 = a1 , . . . , , best-cost plan planning task, compute preferences
functions depending action.
commitment (ai ) =| {a0 | a0 successors(ai ) solution(a0 )} |
function successors(ai ) returns applicable actions state si+1 function solution(a)
verifies whether action tagged part best-cost plan.
difficulty (ai ) =

min

1
| supporters(l) |

ladd(ai )

function supporters(l) = {a | l add(a)} returns set actions achieve
literal l.
780

fiS CALING H EURISTIC P LANNING R ELATIONAL ECISION REES

Solutions ranked according preferences. ranking solution 0 =
a1 , . . . , computed weighted sum action preferences, follows:
ranking( 0 , ) =

X
i=0,...,n1

(n i)
(ai+1 )
n

n plan length one commitment difficulty . sum weighted
give importance preferences first actions plan. first action preference
value multiplied 1, second (n 1)/n, on. Otherwise, several alternatives (i.e.,
commutative actions different positions within plan) would lead ranking value.
compute ranking best-cost solutions using commitment . Ties ranking broken
ranking computed difficulty . subset solutions best ranking values
subset solutions selected generating training examples.
3.3.3 E XTRACTING E XAMPLES OLUTIONS
takes subset solutions selected previous step generates training examples. generating examples operator classification, ROLLER takes solution plans 0 =
{a1 , a2 , ..., } correspond sequence state transitions {s0 , s1 , ..., sn } generates
one learning example pair < si , ai+1 > consisting H(si ) class (i.e., operator
name action ai+1 ). See learning example shown Figure 2.
generating examples binding classification operator o, ROLLER considers
pairs < si , ai+1 > ai+1 matches operator o. learning example generated pair
< si , ai+1 > binding selection operator consists H(si ) classes
applicable actions si match o, including ai+1 . Applicable actions solution label
belong selected class applicable actions rejected class. Moreover, actions
belonging solutions top ranking still marked selected even though
nodes example generated. See learning example shown Figure 5.
ROLLER

3.4 Use Learned Knowledge: Planning Relational Decision Trees
section details make heuristic planning benefit DCK, beginning
build action orderings learned DCK. Then, explains two different search strategies
exploit orderings: (1) application DCK generalized action policy (DepthFirst H-Context Policy algorithm) (2) use DCK generate lookahead states within
Best-First Search (BFS) guided heuristic (H-Context Policy Lookahead-BFS algorithm).
3.4.1 RDERING ACTIONS R ELATIONAL ECISION REES
Given state s, expression app(s) denotes set actions applicable s. learned DCK
provides ordering app(s). ordering built matching action app(s) first
operator classifier corresponding binding classifier. Figure 10 shows detail
algorithm ordering applicable actions relational decision trees.
algorithm divides set applicable actions two subsets: helpful actions,
non-helpful actions. Then, matches helpful context state, i.e., H(s), tree
operator classification. matching provides leaf node contains list operators
sorted number examples covered leaf training phase (see operator
781

fiD E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

DT-Filter-Sort (A,H,T):sorted list applicable actions
A: List actions
H: Helpful Context
T: Decision Trees

selected-actions =
HA = helpful-actions(A, H)
NON-HA = \ HA
leaf-node = classify-operators-tree(T, H)
HA
priority(a) = leaf-node-operator-value(leaf-node, a)
priority(a) > 0
(selected(a),rejected(a)) = classify-bindings-tree(T, H, a)
selected(a)
selection ratio(a)= selected(a)+rejected(a)
priority(a) = priority(a) + selection ratio(a)
selected-actions = selected-actions {a}
max-HA-priority = maxaselected-actions priority(a)
NON-HA
priority(a) = leaf-node-operator-value(leaf-node,a)
priority(a) > max-HA-priority
(selected(a),rejected(a)) = classify-bindings-tree(T, H, a)
selected(a)
selection ratio(a)= selected(a)+rejected(a)
priority(a) = priority(a) + selection ratio(a)
selected-actions = selected-actions {a}
return sort(selected-actions, priority)

Figure 10: Algorithm ordering actions using relational decision trees.
classification tree Figure 4). number examples covered gives operator ordering
used prefer actions search. algorithm uses number initialize
priority value helpful action, taking value corresponding operator. algorithm
keeps helpful actions least one matching example. actions,
algorithm matches action corresponding binding classification tree. resulting leaf
binding tree returns two values: number times ground action selected,
number times rejected training phase. define selection ratio ground
action as:
selected(a)
selection ratio(a) =
selected(a) + rejected(a)
ratio represents proportion good bindings covered particular leaf binding
tree. denominator zero, selection ratio assumed zero. priority
action updated adding selection ratio. Thus, final priority action higher
782

fiS CALING H EURISTIC P LANNING R ELATIONAL ECISION REES

actions operators operator classification tree provides higher values, i.e.
selected often training examples. Since selection ratio remains 0
1, adding number considered method breaking ties initial priority
value, using information binding classification tree.
priority non-helpful actions computed similar way except that, case,
algorithm considers actions whose initial priority (the value provided operator classification tree), higher maximum priority helpful actions. manner, capture
useful non-helpful actions. follows heuristic criterion classify action helpful. Although
heuristic shown useful, case may arise useful action
particular moment classified helpful. Decision trees capture information, given
recommend choosing non-helpful action. described method takes advantage
fact defines way using information applying learned knowledge. alternative approach would extend planning context new meta-predicate non-helpful
actions. However, pay variety problems domains means significantly larger contexts, causes expensive matching. Finally, selected actions
sorted order decreasing priority values. sorted list actions output algorithm.
3.4.2 H-C ONTEXT P OLICY LGORITHM
helpful context-action policy algorithm moves forward, applying state best action
according DCK. pseudo-code algorithm shown Figure 11. algorithm
maintains ordered open-list. open-list contains states expanded extracted
order. extracted, state evaluated using heuristic. Thus, evaluate upon
extraction nodes included open-list. evaluation provides heuristic
value state, h, set helpful actions HA, needed generate helpful
context. heuristic value used for: (1) continuing search state recognized
dead-end (h = ), (2) goal checking (h = 0). Then, helpful context generated. Subsequently, algorithm obtains set AA actions applicable state sorts using
decision trees (as shown algorithm Figure 10). result AA0 AA, sorted list
applicable actions. algorithm inserts successors generated actions AA0 beginning open-list preserving ordering (function push-ordered-list-in-open).
Furthermore, make algorithm complete robust, successors generated applicable
actions AA0 included secondary list called delayed-list. delayed list
used open-list empty. case, one node delayed-list moved
open-list then, algorithm continues extracting nodes open-list.
algorithm, node maintains pointer parent order recover solution
found. Also, node maintains g value, i.e. length path
initial state node. function push-ordered-list-in-open inserts
open list candidates that: (1) repeated states, (2) repeated states lower g
value previous one. Otherwise, repeated states pruned. type pruning guarantees
maintain node shortest solution found.
words, proposed search algorithm depth-first search delayed successors.
benefit algorithm exploits best action selection policy per783

fiD E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

Depth-First H-Context Policy (I, G, ): plan
I: Initial state
G: Goals
: Decision Trees

open-list = {I};
delayed-list = ;
open-list 6=
n = pop(open-list)
(h, HA) = evaluate(n, G) /*compute heuristic*/
h = /*recognized dead-end*/
continue
h = 0 /*goal state*/
return path(I, n)
H = helpful-context(HA, G, n)
AA = applicable-actions(n)
AA = DT-Filter-Sort(AA, H, )
candidates = generate-successors(n, AA)
open-list = push-ordered-list-in-open(candidates,open-list)
delayed-candidates = generate-successors(n, AA \ AA)
delayed-list = push-ordered-list(delayed-candidates, delayed-list)
open-list = delayed-list 6=
open-list = { pop(delayed-list) }
return fail

Figure 11: depth-first algorithm sorting strategy given DCK.
fect1 action ordering not. Particularly, perfect DCK directly applied
backtrack-free search inaccurate DCK force search algorithm backtrack.
3.4.3 H-C ONTEXT P OLICY L OOKAHEAD TRATEGY
many domains learned DCK may contain flaws: helpful context may expressive
enough capture good decisions, learning algorithm may able generalize well
training examples may representative enough. cases, direct application
learned DCK (without backtracking) may allow planner reach goals problem.
Poor quality learned DCK balanced guide different nature
domain-independent heuristic. successful example ObtuseWedge system (Yoon et al.,
2008) combined learned generalized policy heuristic. ObtuseWedge exploited
learned policy synthesize lookahead states within lookahead strategy. Lookahead states
1. perfect policy refer policy leads directly goal state. policies guaranteed perfect
given generated inductive learning.

784

fiS CALING H EURISTIC P LANNING R ELATIONAL ECISION REES

first applied heuristic planning YAHSP planner (Vidal, 2004). intermediate
states frequently closer goal state direct descendants current state.
intermediate states added list nodes expanded used within
different search algorithms. learned policy contains flaws, lookahead states synthesized
policy may provide good guidance search. However, lookahead states
included complete search algorithm considers ordinary successors, search
process becomes robust. general, use lookahead states forward state-space
search slightly increases branching factor search process, overall, shown
YAHSP planner IPC-2004 experiments included YAHSP paper (Vidal, 2004),
approach seems improve performance significantly.
Figure 12 shows generic algorithm using lookahead states generated policy
search. algorithm weighted Best-First Search (BFS), modification one lookahead states inserted open list expanding node.
weighted BFS, nodes expanded maintained open list ordered evaluation
function f (n) = h(n) + g(n). Apart usual arguments BFS, algorithm receives
policy (P ) horizon. horizon represents maximum number policy steps
applied generating lookahead states. experiments, use algorithm
heuristic h(n).

H-Context Policy Lookahead BFS (I,G,T ,horizon): plan
I: Initial state
G: Goals
: Decision Trees (policy)
horizon: horizon
open-list =
add-to-open(I)
open-list 6=
n = pop(open-list)
goal-state(n, G)
return path(I, n)
add-to-open-lookahead-successors(n, G, T, horizon)
add-to-open-standard-successors(n)
return fail

Figure 12: Generic Lookahead BFS algorithm.
heuristic evaluation, h(n), g-value, g(n), set helpful actions, saved
node node evaluated. function add-to-open(state) evaluates
state inserts open-list, ordered increasing values evaluation function,
f (n). function prunes repeated states, following strategy described DepthFirst H-Context Policy algorithm: repeated states higher g(n) existent one
785

fiD E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

pruned. function add-to-open-standard-successors(n) calls add-to-open
successor node n. function add-to-open-lookahead-successors explained below.
adapted generic Lookahead BFS algorithm learned DCK. particular
instantiation function add-to-open-lookahead-successors shown Figure 13.
case, lookahead states generated iteratively applying first action action ordering provided DCK. inputs algorithm current state, problem goals,
decision trees horizon. First, algorithm generates helpful context applicable
actions. helpful actions, n.HA, recovered node. Then, algorithm sorts applicable actions using decision trees (as previously shown algorithm Figure 10).
that, successor generated first action inserted open list, recursive
call successor horizon decremented one. function add-to-open returns
true argument added open list false otherwise. fact, returns
false two cases: (1) state repeated state g-value higher g-value
existent state2 (2) state recognized dead-end. ordered list becomes empty,
lookahead state generated initial node returned. occurs
horizon zero. described implementation similar lookahead strategy approach
followed ObtuseWedge, instead perform lookahead generation using helpful contexts
relational decision trees.
hand, described H-Context Policy Lookahead BFS algorithm search
perfomed set applicable actions node. However, many domains use
helpful actions shown good heuristic. One possible way prioritizing helpful
actions non-helpful actions include open list successors given helpful
actions, include remaining successors secondary list. implemented idea
following strategy used Depth-first H-Context Policy algorithm: open list
becomes empty one node passed secondary list open list, search
continues. algorithm still complete given prune successor. helpful
actions good enough, strategy save many heuristic evaluations. experiments
compare strategy previous one. intuition adequacy
strategy depends directly quality helpful actions, quality learned DCK,
accuracy heuristic particular domain.
Another technique prioritizing helpful actions BFS implemented YAHSP (Vidal,
2004) inserts two consecutive instances node open list. nodes
equal f (n) since represent state. first one contains helpful actions,
therefore, expanded, generates successors resulting actions. second
contains non-helpful actions, called rescue actions. way, successors lower
f (n) parent node sub-tree generated helpful actions expanded
successor resulting non-helpful actions.
performed preliminary experiments, obtaining similar results two described methods prioritizing helpful actions BFS: use secondary list non-helpful
actions, use rescue nodes. reason, include results first technique
experimental section. call algorithm H-Context Policy Lookahead BFS-HA.
2. state repeated g-value smaller existent one, add-to-open re-evaluate
instead takes heuristic evaluation existent state.

786

fiS CALING H EURISTIC P LANNING R ELATIONAL ECISION REES

add-to-open-lookahead-successors (n,G,T ,horizon) :state
n: Node (state)
G: Goals
: Decision Trees (policy)
horizon: horizon
horizon = 0
return n
H = helpful-context(n.HA, G, n)
AA = applicable-actions(n)
AA0 = DT-Filter-Sort(AA, H, )
AA0 6=
= pop(AA0 )
n0 = generate-successor(n, a)
added = add-to-open(n0 )
added
goal-state(n0 , G)
return n0
return add-to-open-lookahead-successors(n0 , G, , horizon 1)
return n

Figure 13: Algorithm generating lookahead states decision trees.

4. Experimental Results
section evaluate performance ROLLER system. evaluation carried
variety domains belonging diverse IPCs: Four domains come learning track
IPC-2008 (Gold-miner, Matching Blocksworld, Parking Thoughtful). rest domains
(Blocksworld, Depots, Satellite, Rovers, Storage TPP) selected among domains
sequential tracks IPC 2000 2008 presented different structure
difficulty, available random problem generators, automatically build training sets learning DCK. domain, complete training phase
ROLLER learns corresponding DCK testing phase evaluate scalability
quality solutions found ROLLER learned DCK. Next, detail experimental
results obtained two phases. Moreover, domains give particular
details training test sets, learned DCK observed ROLLER performance.
4.1 Training Phase
domain, built training set thirty randomly generated problems. size
structure problems discussed particular details given domain.
explained section 3.2, ROLLER generates training examples solving problems
787

fiD E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

training set BFS-BnB search. set time-bound 60 seconds solve training
problem, discarding exhaustively explored time-bound. Then, ROLLER
generates training examples solutions found builds corresponding decision
trees TILDE system (Blockeel & De Raedt, 1998).
evaluate efficiency ROLLER training phase computed following metrics:
time needed solving training problems, number training examples generated
process, time spent TILDE learning decision trees number leaves
operator selection tree. last number gives clue size learned DCK. Table 1
shows results obtained domain.
Domain
Blocksworld
Depots
Gold-miner
Matching-BW
Parking
Rovers
Satellite
Storage
Thoughtful
TPP

Training
Time (s)
836.0
456.2
1156.9
865.8
105.8
528.3
19.8
136.3
883.4
995.9

Learning
Examples
2542
493
126
430
442
1011
1702
677
502
560

Learning
Time (s)
13.3
23.1
4.5
12.4
7.0
13.6
13.4
5.1
352.2
23.3

Tree
Leaves
18
13
5
23
12
24
4
6
19
6

Table 1: Experimental results training process. Training learning times shown,
well number training examples, complexity generated trees (number
leaves).

achieves shorter Learning Times, fourth column Table 1, state-of-the-art
systems learning generalized policies (Martin & Geffner, 2004; Yoon et al., 2008). Particularly,
systems implement ad-hoc learning algorithms sometimes require hours order
obtain good policies, approach needs seconds learn DCK given domain.
fact makes approach suitable architectures need on-line planning learning processes. However, learning times constant different domains, depend
number training examples (in work, number given amount different
solutions training problems), size training examples (in work size
given number arity predicates actions planning domain) training
examples structured, i.e., whether examples easily separated learning not.
ROLLER

4.2 Testing Phase
testing phase ROLLER attempts solve, domain, set thirty test problems.
problems taken evaluation set corresponding IPC. evaluation set
contains problems, thirty problems thirty hardest ones. Depots domain
exception twenty-two problems, evaluation set domain IPC-2002
788

fiS CALING H EURISTIC P LANNING R ELATIONAL ECISION REES

contained twenty-two problems. Three experiments made testing phase.
first one evaluates ROLLERs performance DCK learned solutions training
problems ranked solution approach. second one evaluates usefulness
learned DCK third one compares ROLLER state-of-the-art planners. experiment evaluate two different dimensions solutions found ROLLER: scalability
quality. testing experiments done using 2.4 GHz processor time-bound 900
seconds3 6Gb memory-bound.
4.2.1 OLUTION R ANKING E VALUATION
experiment evaluates effect selecting solutions following approach described Section 3.3. ROLLER configurations evaluation are:
Top-Ranked Solutions: Depth-First H-Context Policy algorithm using DCK learned
sub-set top ranked solutions. use search algorithm, since performance depends quality learned DCK algorithms
using DCK.
Solutions: Depth-First H-Context Policy algorithm using DCK learned solutions obtained BFS-BnB algorithm.
Table 2 shows number problems solved configuration, time plan
length average computed problems solved configurations. number brackets
first column number problems solved common. Top-ranked solutions configuration solved thirty problems solutions configuration, mainly due difference
21 problems Matching Blocksworld domain.
Domains
Blocksworld (30)
Depots (18)
Gold-miner (30)
Matching-BW (0)
Parking (30)
Rovers (27)
Satellite (28)
Storage (10)
Thoughtful (12)
TPP (30)
Total

Top-Ranked Solutions
Solved
Time Length
30
0,62
170,0
21
0,94
489,1
30
0,01
65,3
21


30
4,90
148,9
28
1,40
166,0
30
11,21
123,6
15
0,00
9,0
12
1,25
249,7
30
0,97
147,1
247



Solutions
Solved
Time Length
30
2,39
550,7
18
0,97
607,3
30
0,01
65,3
0


30
2,20
56,2
29
31,20
355,8
28
11,47
121,6
10
0,00
9,0
12
1,28
249,2
30
0,90
132,8
217



Table 2: Problems solved time plan length average evaluation ranking solution
heuristic.

effect selecting solutions varies across domains. instance, quite important regarding plan quality Blocksworld, Depots Rovers. Satellite domain top-ranked
solutions allow ROLLER solve two problems maintaining similar time plan length
3. 900 seconds time-bound established learning track IPC-2008.

789

fiD E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

average. Gold Miner domain, selecting solutions irrelevant equally
good solutions per problem (i.e., goal always single fact gold) fairly
top-ranked ones. Parking domain benefit selecting solutions.
Considering overall results, think selecting solutions useful heuristic improving
DCK quality many domains. remaining evaluations refer DCK used
ROLLER decision trees learned top-ranked solutions.
4.2.2 DCK U SEFULNESS E VALUATION
shown IPC Learning Track results, DCK may degrade performance base planner,
DCK incorrect. mind, designed experiment measure performance ROLLER algorithms comparing versions without DCK. made two versions
non-learning algorithms. first one empty configuration decision
tree given algorithm, thus ordering computed helpful actions, second one
systematic configuration, ordering supplied heuristic instead.
ROLLER configurations used comparisons are:
ROLLER: Depth-First H-Context Policy algorithm DCK learned training
phase.
ROLLER-BFS: H-Context Policy Lookahead BFS algorithm DCK learned
training phase. configuration uses horizon h = 100. choose value
basis empirical evaluations.
ROLLER-BFS-HA: modified version ROLLER - BFS helpful actions considered immediate successors. lookahead states generated original version, using horizon.
three algorithms equivalent version empty configuration:
DF-HA (Depth-first Helpful Actions): empty DCK ROLLER corresponds depthfirst algorithm helpful actions. original algorithm, non-helpful actions
placed delayed list.
BFS: empty DCK ROLLER - BFS generate lookahead states (i.e., algorithm add-to-open-lookahead-successors Figure 12). Therefore, algorithm becomes
standard Best-first Search.
BFS-HA: modified version BFS helpful actions considered. Non-helpful
actions placed delayed list.
Previous configurations systematic version. case action ordering computed
heuristic:
GR-HA (Greedy Helpful Actions): algorithm corresponds greedy search
helpful actions. node, helpful immediate successors sorted heuristic.
Non-helpful nodes go delayed list.
LH-BFS (Lookahead-BFS): BFS lookahead states. function DT-Filter-Sort
replaced function computes ordering using heuristic.
790

fiS CALING H EURISTIC P LANNING R ELATIONAL ECISION REES

LH-BFS-HA: modified version LH - BFS helpful actions considered. Nonhelpful actions placed delayed list.
comparison, computed number problems solved scores used
IPC-2008 learning track evaluate planners performance terms CPU time quality (plan
length). time score computed follows: problem planner receives Ti /Ti
points, Ti minimum time participant used solving problem i, Ti
CPU time used planner question. 30 problem test set planner receive 30
points, higher score better. quality score computed way, replacing
L, L measures quality terms plan length. addition compute time
quality averages problems solved configurations. configuration solve
problem, taken account measure. Average measures complement scores
since give direct information commonly solved problems, scores tend benefit
configurations solve problems others not.
Table 3 shows summary results obtained DCK usefulness evaluation.
configuration compute number domains algorithm top performer
evaluated criteria (i.e., numbers solved problems, time quality scores averages). top performer domain algorithm equal better measure
rest algorithms. table, algorithm 10 points, number
evaluated domains. Global section refers overall top performers. Relative section refers
number domains configuration equal better two configurations
algorithm strategy (i.e., depth-first, best-first, best-first helpful actions). averages
commonly solved problems computed configurations solve one problem. Results show ROLLER good number solved problems speed metrics.
Regarding quality score, ROLLER ROLLER - BFS - HA best performers three domains
each. However, BFS BFS - HA obtained better results quality average.
Global
Solved Problems
Time Score
Time Average
Quality Score
Quality Average
Relative
Solved Problems
Time Score
Time Average
Quality Score
Quality Average

DEPTH-FIRST
roller gr-ha df-ha
7
2
2
8
1
0
9
1
1
3
1
0
0
0
0
8
9
9
7
5

3
1
1
3
5

2
0
1
0
0

BEST-FIRST
roller-bfs lh-bfs
1
0
0
0
1
0
1
0
1
2
7
8
9
4
1

3
1
1
4
2

bfs
1
0
0
1
3
2
1
0
2
7

HELPFUL BEST-FIRST
roller-bfs-ha lh-bfs-ha bfs-ha
5
1
1
1
0
0
1
0
0
3
1
2
0
1
5
9
9
7
7
2

3
0
1
1
1

2
1
2
2
7

Table 3: Summary DCK usefulness evaluation. column gives number domains
configuration top performer row item.

Table 4 shows number solved problems DCK usefulness evaluation. Total
row shows ROLLER configuration solved problems empty systematic
versions. Results time quality scores reported Table 9 Table 10 Appendix A.
791

fiD E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

Detailed results averages considered less interesting since many domains
common solved problems, easy problems.
Domains
Blocksworld (30)
Depots (22)
Gold-miner (30)
Matching-BW (30)
Parking (30)
Rovers (30)
Satellite (30)
Storage (30)
Thoughtful(30)
TPP (30)
Total

DEPTH-FIRST
roller gr-ha df-ha
30
1
0
21
18
18
30
0
0
21
0
0
30
25
1
28
30
30
30
23
22
15
9
10
12
15
0
30
30
30
247
151
111

BEST-FIRST
roller-bfs lh-bfs
bfs
8
0
0
20
19
13
17
17
16
14
7
14
30
11
7
26
28
11
25
22
15
19
18
20
20
14
11
16
24
9
195
160 116

HELPFUL BEST-FIRST
roller-bfs-ha lh-bfs-ha bfs-ha
8
0
0
20
20
20
30
0
0
19
10
17
30
11
9
30
30
30
30
23
23
19
10
10
23
16
12
19
26
14
228
146
135

Table 4: Problems solved DCK usefulness evaluation.

4.2.3 IME P ERFORMANCE C OMPARISON
experiment evaluates scalability ROLLER system, compared state-of-the-art planners. comparison, chosen LAMA (Richter & Westphal, 2010), winner
sequential track past IPC, FF, last IPC shown still competitive.
used three ROLLER configurations explained previous evaluation. configuration
planners are:
FF. Running Enforced Hill-Climbing (EHC) algorithm helpful actions together
complete BFS case EHC fails 4 . Though planner dates 2001 include
evaluation because, shown results IPC-2008, still competitive
state-of-the-art planners. Besides, planner extensively used planning
learning systems.
LAMA-first. winner classical track IPC-2008. configuration LAMA
modified stop finds first solution. way, comparison fair
rest configurations implement anytime behavior, i.e., continuous solution
refinement reaching time-bound). anytime behavior LAMA compared later
ROLLER performance next section.
Table 5 shows number problems solved together speed score. results
give overall view performance different planners. ROLLER solves many
problems configuration 6 10 domains achieves top speed score
seven domains. second best score belongs ROLLER - BFS - HA, solves many
problems planners six domains. LAMA-first fairly competitive, since solves seven
problems less ROLLER 13 problems ROLLER - BFS - HA. cases LAMA-first
achieves lower speed score.
4. planner actually Metric-FF running STRIPS domains. consider implementation adequate baseline
comparison ROLLER implemented code rather original order extend
approach planning models.

792

fiS CALING H EURISTIC P LANNING R ELATIONAL ECISION REES

Domain (problems)
Blocksworld (30)
Depots (22)
Gold-miner (30)
Matching-BW (30)
Parking (30)
Rovers (30)
Satellite (30)
Storage (30)
Thoughtful(30)
TPP (30)
Total

ROLLER
solved
score
30
29.87
21
19.86
30
26.00
21
14.84
30
28.57
28
24.82
30
22.60
15
11.02
12
11.99
30
29.50
247
219.07

ROLLER-BFS
solved
score
8
2.47
20
11.01
17
0.03
14
1.32
30
22.72
26
13.41
25
14.61
19
12.31
20
12.38
16
14.83
195 105.09

ROLLER-BFS-HA
solved
score
8
2.40
20
11.46
30
5.35
19
1.71
30
23.60
30
16.24
30
18.33
19
16.17
23
13.09
19
13.97
228
122.32


solved
0
20
27
9
24
29
22
17
14
26
188

score
0.00
8.70
0.22
0.27
0.94
5.51
5.31
10.51
8.16
6.27
45.89

LAMA-first
solved
score
17
0.17
20
3.88
29
12.24
25
20.02
23
1.69
30
18.59
28
15.66
19
9.03
20
11.29
30
9.66
241
102.23

Table 5: Problems solved speed score five configurations.

Table 6 shows average time five configurations addressing subset problems solved configurations. first column shows parenthesis number commonly
solved problems. results closely related shown Table 5. ROLLER achieves
best average time eight ten domains. observe different configurations
good particular domains even particular problems. instance, Thoughtful
domain four problems solved configurations.
Domain (problems)
Blocksworld (7)
Depots (18)
Gold-miner (17)
Matching-BW (6)
Parking (22)
Rovers (25)
Satellite (22)
Storage (14)
Thoughtful(4)
TPP (16)

ROLLER
0.36
0.84
0.00
1.99
1.86
1.37
1.24
11.74
1.49
0.02

ROLLER-BFS
66.31
15.53
49.82
42.25
2.91
24.38
7.08
0.01
10.84
0.02

ROLLER-BFS-r
67.99
2.54
0.02
44.53
2.78
9.83
1.87
0.03
9.52
0.02


4.01
0.28
74.90
74.02
42.82
18.23
0.05
14.52
0.70

LAMA-first
139.69
61.73
0.01
1.96
108.22
1.59
1.33
0.19
3.55
0.10

Table 6: Planning time averages problems solved configurations.

4.2.4 Q UALITY P ERFORMANCE C OMPARISON
experiment compares quality first solutions found solutions found
anytime behavior. anytime configuration, planners exhaust time-bound trying improve
incrementally best solution found. Three ROLLER algorithms modified configuration
best solution found far used upper-bound order prune nodes
exceed plan length. anytime behavior regular configuration LAMA.
anytime behavior, included anytime comparison well base
comparing quality improvements planners.
Table 7 shows quality scores first solution last solution found
anytime configurations. anytime column planner shows score variation reveals
whether planner able make relative improvements first solutions. relative
793

fiD E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

Domain
Blocksworld
Depots
Gold-miner
Matching-BW
Parking
Rovers
Satellite
Storage
Thoughtful
TPP
Total

ROLLER
first anytime
29.83
29.83
8.50
9.26
14.30
18.00
9.43
9.52
19.38
17.04
21.38
21.39
28.65
28.81
13.41
13.46
6.27
6.21
25.38
24.26
176.53
177.35

ROLLER-BFS
first anytime
8.00
8.00
12.39
17.01
11.50
17.00
13.01
12.43
24.24
23.98
21.78
21.59
23.20
23.00
15.69
18.38
15.93
15.12
14.45
15.09
160.19
171.65

ROLLER-BFS-HA
first
anytime
8.00
8.00
12.85
18.95
13.08
15.39
17.67
17.16
24.24
25.51
25.66
26.14
28.18
28.94
15.64
17.26
18.63
18.35
16.80
17.77
180.75
193.53


first
0.00
19.01
27.00
8.23
21.53
28.66
21.55
16.23
13.96
23.42
179.59

relative
0.00
17.96
27.00
7.15
17.79
28.33
21.33
15.80
13.09
21.56
170.06

LAMA
first anytime
7.42
8.29
18.32
19.28
14.04
26.81
23.25
24.72
19.16
22.56
28.26
28.97
27.02
27.42
17.24
18.81
18.84
18.59
29.99
29.82
203.54
219.24

Table 7: Quality scores first solution anytime configuration evaluated planners.

shows score solutions compared solutions given anytime configuration
planners. loses points cases others able improve
solutions. two LAMA configurations obtained top score category. Nevertheless,
planner dominated domains. Furthermore, configurations achieved top quality score
first solution least one domain.
Domain
Blocksworld (7)
Depots (18)
Gold-miner (17)
Matching-BW (6)
Parking (22)
Rovers (25)
Satellite (22)
Storage (14)
Thoughtful(4)
TPP (16)

ROLLER
first
anytime
146.29
146.29
385.78
372.22
55.65
38.18
186.00
170.33
96.91
93.82
150.80
149.96
78.41
77.59
43.07
42.64
292.25
291.75
60.25
57.00

ROLLER-BFS
first
anytime
142.86
142.86
81.78
54.00
30.06
19.65
75.00
70.00
75.32
59.86
115.56
115.56
80.05
80.00
15.21
11.36
168.50
168.25
60.00
52.06

ROLLER-BFS-HA
first
anytime
142.86
142.86
76.83
43.33
47.88
39.35
76.00
69.33
75.32
54.45
114.40
112.12
80.05
77.32
15.64
13.29
168.50
164.50
60.25
49.38


first

46.39
19.65
71.67
60.00
94.20
77.18
12.43
123.25
59.19

relative

46.39
19.65
71.67
60.00
94.20
77.18
12.43
123.25
59.19

LAMA
first
anytime
358.57
318.00
49.28
41.56
43.35
19.65
78.33
62.33
64.14
47.91
101.44
98.36
76.91
75.50
12.71
11.29
140.25
128.50
51.81
47.94

Table 8: Quality averages first solution anytime configuration evaluated planners.

Table 8 shows plan length average problems solved configurations. first
column shows average first solutions anytime column gives average
last solutions anytime configuration. commonly solved problems
reported Table 6. Although planner solved fewer problems, achieves
best average plan length seven domains. Plan length averages reveal ROLLER able
find first solutions good quality domains. ROLLER - BFS ROLLER - BFS - HA find
better quality solutions ROLLER, several domains, averages competitive
LAMA . ROLLER - BFS ROLLER - BFS - HA show better quality performance mainly due
combination learned DCK domain-independent heuristic within BFS algorithm.
following subsections discuss particular details domains. give
brief description domain together information training test sets used
experimental evaluation. domain, analyze learned DCK obtained
794

fiS CALING H EURISTIC P LANNING R ELATIONAL ECISION REES

results order give fine-grained interpretation observed performance. details
domains found IPC web site.5
4.2.5 B LOCKSWORLD ETAILS
Problems domain concerned configuring towers blocks using robotic arm.
training set used experiments consisted of: ten eight-block problems, ten nine-block
problems ten ten-block problems. test set consisted 30 largest typed problems
IPC-2000, 36 50 blocks.
blocksworld domain
100
ROLLER
ROLLER-BFS
ROLLER-BFS-HA
LAMA-first

Percentage solved

80

60

40

20

0
0.1

1

10
CPU Time

100

1000

Figure 14: Percentage solved problems increasing time evaluating scalability performance Blocksworld domain.
Although domain one oldest benchmarks automated planning, still challenging state-of-the-art heuristic planners. Blocksworld presents strong interaction among goals
current heuristics fail capture. particular, achieving goal domain may undo
previously satisfied goals. Therefore, crucial achieve goals specific order. DCK
learned ROLLER gives total order domain actions different contexts capturing key
knowledge, lets ROLLER achieve impressive scalability results producing good quality
solution plans. ROLLER configurations considerably better non-learning configurations.
Particularly, ROLLER solved thirty problems set DF - HA GR - HA solve
problem. ROLLER quite good compared state-of-the-art planners. Figure 14
observe ROLLER performs two orders magnitude faster LAMA. x-axis
figure represents CPU time logarithmic scale y-axis represents percentage
solved problems particular time. Moreover, ROLLER obtained best quality score first
solution anytime evaluations. addition, average plan length common problems fairly
close best average, obtained ROLLER - BFS ROLLER - BFS - HA. BFS algorithms
5. http://idm-lab.org/wiki/icaps/index.php/Main/Competitions

795

fiD E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

scale well domain partially guided heuristic, considerably
underestimates distance goals. Similarly, lookahead states generated policy
discarded fail escape plateaus generated heuristic function.
analyzing learned operator tree found explanations good performance
ROLLER Blocksworld domain: operator tree clearly split two parts. first part contains decisions take arm holding block. situation, tree captures
STACK PUT-DOWN block. second part contains decisions take arm empty.
case tree captures UNSTACK PICK-UP block. second part tree,
current state search matches logical query helpful unstack(Block1,Block2)6
means tower blocks Block1 well arranged, i.e., Block1 least
one block beneath Block1 well placed. Therefore, set helpful actions compactly
encodes useful concept bad tower. kind knowledge manually defined
previous works order learn good policies Blocksworld. One approach consisted including recursive definitions new predicates, support predicates above(X,Y)
inplace(X) (Khardon, 1999). Another alternative involved changing representation language, instance concept language (Martin & Geffner, 2004) taxonomic syntax (Yoon,
Fern, & Givan, 2007). Kleene-star operator taxonomic syntax (i.e., operator defining recursion) discarded subsequent work (Yoon et al., 2008) predicate
used instead. ROLLERs ability recognize bad-towers without extra predicates arises
misplaced block tower makes UNSTACK action top block helpful, since always
part relaxed plan arm empty.
Due extraordinary performance ROLLER domain, built extra test set
clarify whether trend observed ROLLER configuration would hold larger
problems. aim, randomly generated 30 problems distributed sub-sets 50, 60, 70,
80, 90 100 blocks 5 problems sub-set. ROLLER solved 30 problems
extra test set time average 20.1 seconds per problem spending 175.3 solve
problem. Obviously, problems became difficult ROLLER number blocks increase.
4.2.6 EPOTS ETAILS
domain combination transportation domain Blocksworld domain,
crates instead blocks hoists instead robot arm. problems consist trucks
transporting crates around depots distributors. Using hoists, crates stacked onto pallets
top crates final destination. domain, 30 training problems
different combinations 2 3 locations (depots distributors), 1 2 trucks, 1 2 pallets per
location, 1 hoist per location 2 5 crates placed different configurations.
testing phase used 22 problems IPC-2002 set. hardest problem 12
locations (1 2 pallets 1 2 hoists), 6 trucks 20 crates.
ROLLER ROLLER - BFS improve performance non-learning strategies, three
configurations BFS Helpful-Action solved 20 problems. ROLLER able solve 21
problems, achieving best speed score. However, high average plan length indicates
policy producing good quality plans. ROLLER - BFS - HA obtains second best speed score
competitive plan lengths. Figure 15 shows percentage solved problems
6. explained section 3.2 logic queries ROLLER present example problem Ids. case Ids
ignored simplicity given needed matching current helpful context.

796

fiS CALING H EURISTIC P LANNING R ELATIONAL ECISION REES

increasing CPU time (in logarithmic scale). anytime configuration, ROLLER - BFS - HA
able refine solutions, achieving quality average similar LAMA.
depots domain
100
ROLLER
ROLLER-BFS
ROLLER-BFS-HA

LAMA-first

Percentage solved

80

60

40

20

0
0.01

0.1

1

10

100

1000

CPU Time

Figure 15: Percentage solved problem increasing time evaluating scalability performance Depots domain.

DCK learned domain provides inaccurate advice large planning contexts.
instance, ROLLER makes mistakes deciding crate unload several crates
loaded truck. reason inaccurate DCK training problems large
enough gain knowledge. addition, adding crates problems makes unfeasible solved BFS-BnB. Nevertheless, limitation learned DCK
evident. Depots domain undirected (i.e., actions reversible),
dead ends. Therefore, mistakes made DCK fixed additional actions, leads
worse quality plans. Besides, since first solutions rapidly found, ROLLER configurations
spend time refining solutions. reason great improvement plan average
ROLLER - BFS - HA .
4.2.7 G OLD -M INER ETAILS
objective domain navigate grid cells reaching cell containing gold.
cells occupied rocks cleared using bombs laser. domain
training set consists of: 10 problems 3 3 cells, 10 problems 4 4 cells, 10
problems 5 5 cells. domain part learning track IPC-2008 used
test set used competition. set problems ranging 5 5 7 7 cells.
Problems Gold-Miner domain solvable helpful actions alone. explains
difference number solved problems ROLLER, ROLLER - BFS - HA nonlearning counterpart. general terms, domain trivial ROLLER, ROLLER - BFS - HA (they
solved test problems less 10 seconds per problem) LAMA. Nevertheless, scales797

fiD E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

poorly. domain essential actions picking bombs frequently considered
helpful actions, relaxed problem solvable using laser. Consequently, fails
solve problems EHC requires additional BFS search. Figure 16 shows
percentage solved problems increasing CPU time. Regarding anytime evaluation,
tested configurations improved first solution found many problems.
gold-miner domain
100

Percentage solved

80

60

40

20
ROLLER
ROLLER-BFS
ROLLER-BFS-HA

LAMA-first
0
0.01

0.1

1

10

100

1000

CPU Time

Figure 16: Percentage solved problems increasing time evaluating scalability performance Gold-miner domain.
domain, operator tree succeeds capturing key knowledge. initial states,
bombs laser cell, robot needs decide pick-up.
operator tree domain matches logical query candidate pickup laser(Cell)
higher ratio operator PICKUP-BOMB operator PICKUP-LASER. operator
preference allows ROLLER avoid dead ends laser destroys gold. hand,
situations laser required (i.e., destroy hard rocks) reached second choice
policy. fact implies backtracking ROLLER, additional evaluated nodes
significantly affect overall performance. preference PICKUP-BOMB
PICKUP-LASER action example selecting non-helpful actions.
4.2.8 ATCHING B LOCKSWORLD ETAILS
domain version Blocksworld designed analyze limitations relaxed plan heuristic.
version blocks polarized, either positive negative. two polarized robot
arms. Furthermore, block placed (stack put-down actions) arm different
polarity, block becomes damaged block placed top it. However, picking
unstacking block wrong polarity seems harmless. fact makes recognizing
dead ends difficult task heuristic. Particularly, relaxed task blocks never
damaged. Thus, relaxed plan (and consequently set helpful actions) heuristic
estimation wrong. training set used domain consists fifteen 6-blocks problems
798

fiS CALING H EURISTIC P LANNING R ELATIONAL ECISION REES

fifteen 8-blocks problems. used even number blocks keep problems balanced (i.e.,
half blocks polarity). testing phase used test set learning track
IPC-2008. set problems ranging 15 25 blocks.
DF - HA GR - HA solve problem, problems solvable
helpful actions alone. learned DCK recommended useful non-helpful actions, thus
ROLLER able solve 21 problems. Policy configurations perform better systematic strategies, fairly similar using lookahead strategy. fact reveals learned DCK
effective enough pay effort building lookahead states. LAMA planner
solves problems. Figure 17 shows percentage solved problems increasing
CPU time.
matching-bw domain
100
ROLLER
ROLLER-BFS
ROLLER-BFS-HA

LAMA-first

Percentage solved

80

60

40

20

0
0.1

1

10
CPU Time

100

1000

Figure 17: Percentage solved problems increasing time evaluating scalability performance Matching Blocksworld domain.

ROLLER solved problems evaluating considerable number nodes plan length,
means DCK learned domain accurate. analyzing training
examples find many solution plans satisfy key knowledge domain (robot
arms unstack pick-up blocks polarity). Specifically, robot handling
top block, i.e., block blocks goal state, polarity robot
arm becomes meaningless. effect unavoidable shortest plans involve managing
top blocks efficient way ignoring polarities. examples include noise
learning make generalization complex.

4.2.9 PARKING ETAILS
domain involves parking cars street N curb locations cars double
parked, triple parked. goal move one configuration parked cars another
driving cars one curb location another. domain training set consists of: fifteen
799

fiD E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

problems six cars four curbs fifteen problems eight cars five curbs. testing
used test set learning track IPC-2008. hardest problem set 38 cars
20 curbs.
three ROLLER configurations solve problems perform significantly better nonlearning strategies. addition, three ROLLER configurations outperform LAMA
difference one order magnitude. reason LAMA low speed
scores. ROLLER configurations consistently better systematic empty configurations. Figure 18 shows percentage solved problems increasing CPU time.
hand, three ROLLER configurations achieve first solutions suficient quality. However, solutions refined anytime evaluation, especially ROLLER - BFS - HA,
achieves top quality score plan length average fairly similar LAMA.
parking domain
100
ROLLER
ROLLER-BFS
ROLLER-BFS-HA

LAMA-first

Percentage solved

80

60

40

20

0
0.1

1

10
CPU Time

100

1000

Figure 18: Percentage solved problems increasing time evaluating scalability performance Parking domain.

learned DCK domain quite effective (ROLLER rarely backtracked). operator tree perfectly classifies MOVE-CAR-TO-CURB action first tree node, asking
considered helpful action. Besides, binding tree operator selects right car asking
target goal rejecting candidates. two decisions guide planner place
car right position whenever possible. result, large number nodes evaluated,
explains scalability difference LAMA.
4.2.10 ROVERS ETAILS
domain simplification tasks performed autonomous exploration vehicles sent
Mars. tasks consist navigating rovers, collecting soils rocks samples, taking
images different objectives. domain training set consists of: ten problems one
rover, four waypoints, two objectives one camera; ten problems additional camera;
800

fiS CALING H EURISTIC P LANNING R ELATIONAL ECISION REES

ten problems additional rover. Problems test set thirty largest problems
IPC-2006 set (i.e., problems 11 40). largest problem set 14 rovers 100
waypoints.
DCK strategies faster systematic empty strategies, differences significant since configurations solved problems. one hand helpful actions
Rovers domain quite good hand test set problems
big enough generate differences among approaches. Regarding planner comparison, ROLLER
achieves top performance score scales significantly better FF, solves two problems
less LAMA. Figure 19 shows percentage solved problems increasing CPU time.
Regarding anytime evaluation, planners able refine first solutions. LAMA gets
top quality score best plan length refining solutions.
rovers domain
100
ROLLER
ROLLER-BFS
ROLLER-BFS-HA

LAMA-first

Percentage solved

80

60

40

20

0
0.01

0.1

1

10

100

1000

CPU Time

Figure 19: Percentage solved problems increasing time evaluating scalability performance Rovers domain.

domain, ROLLER learned imperfect DCK, manages achieve good scalability
results. DCK imperfect partially actions communicating rock, soil image analysis
applied order among them. Therefore, preferences ranking selecting
solutions fail discriminate among actions confuse learning algorithm. Since
actions could applied order, DCK mistakes seem harmless planning
time.
4.2.11 ATELLITE ETAILS
domain comprises set satellites different instruments, operate different
formats (modes). tasks consist managing instruments taking images certain targets
particular modes. domain training set consist thirty problems one satellite, two
instruments, five modes five observations. Problems test set thirty largest problems
801

fiD E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

IPC-2004 (i.e., problems 7 36). largest problem set 10 satellites, 5 modes
174 observations.
three ROLLER configurations improved number solved problems non-learning
counterpart. addition, ROLLER ROLLER - BFS - HA solved 30 problems set, two
LAMA eight FF. Figure 20 shows percentage solved problems
increasing CPU time. ROLLER ROLLER - BFS - HA achieve good quality solutions
able refine anytime evaluation, achieving plan lengths similar LAMA.
satellite domain
100
ROLLER
ROLLER-BFS
ROLLER-BFS-HA

LAMA-first

Percentage solved

80

60

40

20

0
0.01

0.1

1

10

100

1000

CPU Time

Figure 20: Percentage solved problems increasing time evaluating scalability performance Satellite domain.

learned DCK captures key knowledge Satellite domain. trees shown
Figure 4 Figure 7 part learned DCK fewer training examples. domain
ROLLER ROLLER - BFS - HA perform quite similarly. reason heuristic
quite accurate domain. Thus, deepest lookahead state generated learned policy
frequently selected heuristic BFS search.
4.2.12 TORAGE ETAILS
domain concerned storage set crates taking account spatial configuration depot. domain tasks comprise using hoists move crates containers
particular area depot. training set consists 30 problems 1 depot, 1 container, 1
hoist different combinations 2 3 crates 2 6 areas inside depot.
test set used 30 problems IPC-2006 set. largest problem domain 4
depots 8 areas each, 5 hoist 20 crates.
first 12 problems trivially solved configurations. Then, problem difficulty increases quickly number problem objects increases. BFS solved 20 problems, one
DCK strategy, meaning DCK lookahead strategies pay off. domain
802

fiS CALING H EURISTIC P LANNING R ELATIONAL ECISION REES

hard LAMA. Figure 21 shows percentage solved problems increasing
CPU time.
storage domain
100
ROLLER
ROLLER-BFS
ROLLER-BFS-HA

LAMA-first

Percentage solved

80

60

40

20

0
0.01

0.1

1

10

100

1000

CPU Time

Figure 21: Percentage solved problems increasing time evaluating scalability performance Storage domain.

Although DCK effective, found interesting properties it. learned operator tree
compact succeeds selecting GO-IN action normally marked helpful
action.
4.2.13 HOUGHTFUL ETAILS
domain models version solitaire card game, cards visible one
turn card talon rather 3 cards time. original version, goal
game place cards ascending order corresponding suit stacks (home deck).
available random problem generator domain. Therefore, used bootstrap
problem distribution given learning track IPC-2008. set contains problems
four suits, card seven suit. test phase used 30 problems
test distribution learning IPC-2008. largest problem domain full set
standard card game.
ROLLER solves 12 problems, three fewer GR - HA . However, ROLLER - BFS ROLLER BFS - HA better number solved problems non-learning approaches. domain,
use DCK lookahead construction combined heuristic makes search process
robust policy mistakes. ROLLER - BFS - HA solves 23, three LAMA. Figure 22
shows percentage solved problems increasing CPU time.
BFS-BnB algorithm generating training examples able solve 12 30
problems bootstrap problem distribution. believe different bootstrap distribution smaller problems would generate accurate DCK. Additionally, even though DCK
803

fiD E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

thoughtful domain
100
ROLLER
ROLLER-BFS
ROLLER-BFS-HA

LAMA-first

Percentage solved

80

60

40

20

0
0.1

1

10
CPU Time

100

1000

Figure 22: Percentage solved problems increasing time evaluating scalability performance Thoughtful domain.

lookahead strategies achieve good results, learning accurate decision trees complex
many classes (20 operators particular domain) many arguments
predicates background knowledge (up 6 parameters operator col-to-home 7
parameters operator col-to-home-b).
4.2.14 TPP ETAILS
TPP stands Traveling Purchase Problem, generalization Traveling Salesman
Problem. Tasks domain consist selecting subset markets satisfy demand
set goods. selection markets try optimize routing purchasing
costs goods. STRIPS version, graph connects markets equal costs
arcs. Nevertheless, domain still interesting difficult planners scale
increasing number goods, markets trucks. training set consists thirty problems
number goods, trucks depots varying one three load levels five
six. test set consists thirty problems used planner evaluation IPC-2006.
largest problem set 20 goods, 8 trucks, 8 markets load level six.
ROLLER , GR - HA DF - HA solved 30 problems test set, ROLLER performs faster
two, achieving similar plan lengths. Besides, ROLLER outperforms rest
planners two orders magnitude faster FF. main reason overwhelming
branching factor large problems together fact heuristic falls big plateaus
domain. Greedy (depth-first) approaches perform better avoid effect
plateaus. Additionally, ROLLER achieved competitive quality scores average plan length
first solution anytime evaluation. ROLLER - BFS ROLLER - BFS - HA got bad results

804

fiS CALING H EURISTIC P LANNING R ELATIONAL ECISION REES

domain imprecision heuristic. Figure 23 shows percentage
solved problems increasing CPU time.
tpp domain
100

Percentage solved

80

60

40

20
ROLLER
ROLLER-BFS
ROLLER-BFS-HA

LAMA-first
0
0.01

0.1

1

10

100

1000

CPU Time

Figure 23: Percentage solved problems increasing time evaluating scalability performance TPP domain.
learned DCK compact useful reducing number evaluations, shown
ROLLER performance. instance, DRIVE binding tree recognizes perfectly truck
market need go market B already truck B handling goods
market. situations, state truck B helpful action DRIVE, meaning
truck B something deliver.

5. Lessons Learned IPC
IPC-2008 included specific track planning systems benefit learning. Thirteen systems
took part track including previous version ROLLER (De la Rosa et al., 2009) achieved
7th position. version upgrade original ROLLER system (De la Rosa et al.,
2008). first version proposed EHC-Sorted algorithm alternative H-Context Policy, effective many domains. competing version tried recommend ordering
applying actions relaxed plans. idea, although initially appealing, good
choice usefulness strongly depends fact relaxed plan contains right actions. competition completed analysis ROLLER performance diagnose
strengthen weak points. system resulting improvements ROLLER version
described article. One example ROLLER improvements results obtained
Thoughtful Matching Blocksworld domains. IPC-2008, ROLLER failed solve problems Thoughtful domain solved two problems Matching Blocksworld.
reported section 4, current version ROLLER solves 23 19 problems respectively
domains. addition, current version ROLLER outperforms LAMA Park805

fiD E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

ing domain one order magnitude. improvements ROLLER overcome limitations
version submitted IPC-2008 three aspects:
Robustness wrong DCK. Issues discussed Section 2 decisions introduce biases learning process making learning DCK complex task. fact, competitor
IPC-2008 able learn useful DCK domains. Furthermore, many domains
learned DCK damaged performance baseline planner. case
ROLLER . described paper, strengthened ROLLER wrong DCK
proposing two versions modified BFS algorithm combine learned DCK
numerical heuristic. combination DCK heuristic makes planning process robust imperfect and/or incorrectly learned knowledge. similar approach
followed winner best learner award, BTUSE W EDGE (Yoon et al., 2008).
Efficiency baseline. overall competition winner P BP (Gerevini, Saetti, & Vallati, 2009) portfolio state-of-the-art planners learns planner settings
best ones given planning domain. result, performance competitor
never worse performance state-of-the-art planner. IPC-2008 baseline performance ROLLER far competitive state-of-the art planners
ROLLER algorithms coded LISP. overcome weakness optimized
implementation ROLLER using C code outperformed IPC-2008 results
domains.
Definition significant training sets. Training examples extracted experience
collected solving problems training set. Therefore, quality training
examples depends quality problems used training. IPC-2008 training
problems fixed organizers and, many domains, large
ROLLER system extract useful DCK. paper created training problems using
random generators build useful training sets ROLLER system domain.
Selection training examples. Relational classifiers induce set rules/trees model
regularities training data. case forward state-space search planning
best-cost solutions problem may used training data, leads alternatives confuse learner. avoid this, training data cleaned
used learning algorithm. ranking solution selection proposed article
option give learner training data clearer regularities.
Additionally, ROLLER performed poorly Sokoban N-puzzle domains. Traditionally,
useful DCK domains form numeric functions, Manhattan distance,
provides lower-bound solution length. general, action policies inaccurate
domains, lack knowledge trajectory goals. Currently, still
unable learn useful DCK ROLLER domains. possible future direction introduce
goals subgoals (e.g. landmarks) helpful context aim capturing
knowledge.

6. Related Work
approach strongly inspired way Prodigy (Veloso et al., 1995) models DCK.
Prodigy architecture, action selection two-step process: first, Prodigy selects uninstan806

fiS CALING H EURISTIC P LANNING R ELATIONAL ECISION REES

tiated operator apply, second, selects bindings operator. selections
guided DCK form control rules (Leckie & Zukerman, 1998; Minton, 1990).
returned idea two-step action selection allows us define learning
planning DCK standard classification task therefore solve learning task
off-the-shelf classification technique relational decision trees. Nevertheless, ROLLER
need distinguish among different kinds nodes Prodigy does, ROLLER performs
standard forward heuristic search state space search nodes
type.
Relational decision trees previously used learn action policies context
Relational Reinforcement Learning (RRL) (Dzeroski, De Raedt, & Blockeel, 1998). comparison
DCK learned ROLLER, RRL action policies present two limitations solving planning problems. First, RRL learned knowledge targeted given set goals, therefore
RRL cannot directly generalize learned knowledge different goals within given domain.
Second, since training examples RRL consist explicit representations states, RRL needs
add extra background knowledge learn effective policies domains recursive predicates
Blocksworld.
Previous works learning generalized policies (Martin & Geffner, 2004; Yoon et al., 2008)
succeed addressing two limitations RRL. First, introduce planning goals
training examples. way learned policy applies set goals domain. Second,
change representation language DCK predicate logic concept language.
language makes capturing decisions related recursive concepts easier. Alternatively, ROLLER
captures effective DCK domains Blocksworld without varying representation language.
ROLLER implicitly encodes states terms set helpful actions state. result,
ROLLER benefit directly off-the-shelf relational classifiers work predicate logic.
fact makes learning times shorter resulting policies easier read.
Recently, techniques developed improve performance heuristic
planners:
Learning Macro-actions (Botea, Enzenberger, Muller, & Schaeffer, 2005; Coles & Smith,
2007) combination two operators considered new domain operators order reduce search tree depth. However, benefit decreases number
new macro-actions added enlarge branching factor search tree causing utility problem (Minton, 1990). approaches overcome problem, applying
filters decide applicability macro-actions (Newton, Levine, Fox, & Long,
2007). Two versions work participated learning track IPC-2008, obtaining
third fourth place. One advantage macro-actions learned knowledge
exploited planner. Thus, approaches learn generalized policies could
benefit macro-actions. Nevertheless, far know, combination
tried improving heuristic planners.
Learning domain-specific heuristic functions: approach (Yoon, Fern, & Givan, 2006;
Xu, Fern, & Yoon, 2007), state-generalized heuristic function obtained examples
solution plans. main drawback learning domain-specific heuristic functions
result learning algorithm difficult understand humans makes
verification learned knowledge difficult. hand, learned knowledge
easy combine existing domain-independent heuristics. slightly different approach
807

fiD E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

consists learning ranking function greedy search algorithms (Xu, Fern, & Yoon,
2009, 2010). step greedy search, current node expanded child node
highest rank selected current node. case, ranking function
iteratively estimated attempt cover set solution plans greedy algorithm.
Learning task decomposition: approach learns divide planning tasks given
domain smaller subtasks easier solve. Techniques reachability analysis
landmark extraction (Hoffmann, Porteous, & Sebastia, 2004) able compute intermediate states must reached satisfying goals. However, clear
systematically exploit knowledge build good problem decompositions. Vidal et al. (2010) consider optimization problem use specialized optimization
algorithm discover good decompositions.
general, system learns planning DCK deal ambiguity training
examples, given planning state may present many good actions. Trying learn DCK
selects one action other, inherently equal, complex learning problem. cope
ambiguous training data ROLLER created function ranks solutions aim learning
kind solutions. different approach followed Xu et al. (2010) generate
training examples partially ordered plans.

7. Conclusions Future Work
presented new technique reducing number node evaluations heuristic planning based learning exploiting generalized policies. technique defines process
learning generalized policies two-step classification builds domain-specific relational decision trees capture action selected different planning contexts. work,
planning contexts specified helpful actions state, pending goals static
predicates problem. Finally, explained exploit learned policies solve
classical planning problems, applying directly combining domain independent
heuristic lookahead strategy BFS algorithm. work contributes state-of-the-art
learning-based planning three ways:
1. Representation. propose new encoding generalized policies able capture
efficient DCK using predicate logic. opposed previous works represent generalized
policies predicate logic (Khardon, 1999), representation need extra background knowledge (support predicates) learn efficient policies Blocksworld domain.
Besides, encoding states set helpful actions frequently compact furthermore, set normally decreases search fewer goals left. Thus, process
matching DCK becomes faster search advances towards goals.
2. Learning. defined task learning generalized policy two-step standard
classification task. Thus, learn generalized policy off-the-shelf tool
building relational classifiers. Results paper obtained TILDE system (Blockeel & De Raedt, 1998), tool learning relational classifiers could
used. this, advances relational classification applied straightforward
manner ROLLER learn faster better planning DCK.
808

fiS CALING H EURISTIC P LANNING R ELATIONAL ECISION REES

3. Planning. explained extract action ordering H-Context Policy
shown use ordering reduce node evaluations: (1) algorithm Depth-First H-Context Policy allows direct application H-Context policies;
(2) H-Context Policy Lookahead BFS, combines policy domainindependent heuristic within BFS algorithm. addition, included modified
version algorithm (ROLLER - BFS - HA) considers helpful successors order
reduce number evaluations domains helpful actions good.
Experimental results show approach improved scalability baseline heuristic
planners LAMA (winner IPC-2008) variety IPC domains. effect
evident domains learned DCK presents good quality, e.g. Blocksworld Parking.
domains direct application learned DCK saves large amounts node evaluations
achieving impressive scalability performance. Moreover, using learned DCK combination
domain-independent heuristic BFS algorithm achieves good quality solutions.
quality learned DCK poor, planning direct application policy fails
solve many problems, mainly largest ones difficult solve without reasonable
guide. Unfortunately, current mechanism quantifying quality learned DCK
evaluating set test problems. Therefore, good compromise solution combining
learned DCK domain-independent heuristics.
domains, DCK learned ROLLER presents poor quality helpful context
able represent concepts necessary order discriminate good bad
actions. problem frequently arises arguments good action correspond
problem goals static predicates. plan study refinements definition
helpful context achieve good DCK domains. One possible direction extending
helpful context subgoal information landmarks (Hoffmann et al., 2004)
relaxed plan. Moreover, use decision trees introduces important bias learning step.
Algorithms tree learning insert new query tree produces significant
information gain. However, domains information gain obtained
conjunction two queries. Finally, currently providing learner fixed
distribution training examples. near future, plan explore learner generate
convenient distribution training examples according target planning task proposed
Fuentetaja Borrajo (2006).

Acknowledgments
work partially supported Spanish MICIIN project TIN2008-06701-C03-03
regional CAM-UC3M project CCG08-UC3M/TIC-4141.

809

fiD E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

Appendix A. DCK Usefulness Results

Domains
Blocksworld (30)
Depots (22)
Gold-miner (30)
Matching-BW (30)
Parking (30)
Rovers (30)
Satellite (30)
Storage (30)
Thoughtful(30)
TPP (30)
Total

DEPH-FIRST
roller gr-ha df-ha
29.87
0.03
0.00
19.42
7.70
3.61
28.00
0.00
0.00
20.88
0.00
0.00
28.57
1.23
0.00
25.99 10.09
7.73
27.97
2.93
1.69
11.02
8.03
8.08
11.91 13.15
0.00
29.50 10.86 11.45
233.13 54.02 32.56

BEST-FIRST
roller-bfs lh-bfs
bfs
2.47
0.00
0.00
10.51
5.32
2.45
0.04
0.05
0.00
3.82
0.98
3.90
22.72
0.26
0.01
14.58
5.52
0.14
16.09
3.05
0.08
11.53 10.56
8.97
11.30
8.90
2.31
14.83
8.67
5.00
107.89 43.31 22.86

HELPFUL BEST-FIRST
roller-bfs-ha lh-bfs-ha bfs-ha
2.40
0.00
0.00
10.98
5.27
7.08
7.35
0.00
0.00
3.72
2.20
5.81
23.60
0.26
0.04
18.12
17.17
8.21
21.93
2.68
2.90
16.12
7.00
7.00
11.89
9.05
3.04
13.97
7.54
6.13
130.08
51.17
40.21

Table 9: Problems solved DCK usefulness evaluation.

Domains
Blocksworld (30)
Depots (22)
Gold-miner (30)
Matching-BW (30)
Parking (30)
Rovers (30)
Satellite (30)
Storage (30)
Thoughtful(30)
TPP (30)
Total

DEPH-FIRST
roller
gr-ha df-ha
29.83
0.06
0.00
8.82
8.21
3.02
19.78
0.00
0.00
11.53
0.00
0.00
21.53
8.20
0.01
21.54
26.34 25.51
28.03
17.36
9.58
13.43
8.02
8.00
6.89
12.40
0.00
25.46
27.92 23.75
186.84 108.51 69.87

BEST-FIRST
roller-bfs
lh-bfs
8.00
0.00
12.68
14.84
11.50
17.00
12.76
6.35
26.92
6.33
21.94
24.63
22.60
21.48
15.59
16.87
16.56
13.35
13.94
22.04
162.49 142.89

bfs
0.00
12.37
15.41
13.84
6.14
10.75
14.64
19.23
10.60
8.71
111.69

HELPFUL BEST-FIRST
roller-bfs-ha lh-bfs-ha
bfs-ha
8.00
0.00
0.00
13.20
16.06
19.97
16.67
0.00
0.00
17.21
9.39
16.54
26.92
6.33
8.18
25.71
26.32
29.63
27.60
22.31
22.42
15.66
8.59
9.31
19.48
14.91
11.75
16.20
24.36
13.88
186.65
128.27 132.35

Table 10: Quality scores DCK usefulness evaluation.

References
Bacchus, F., & Kabanza, F. (2000). Using temporal logics express search control knowledge
planning. Artificial Intelligence, 116(1-2), 123191.
Biba, J., Saveant, P., Schoenauer, M., & Vidal, V. (2010). evolutionary metaheuristic based
state decomposition domain-independent satisficing planning. Proceedings 20th
International Conference Automated Planning Scheduling (ICAPS10) Toronto, ON,
Canada. AAAI Press.
Blockeel, H., & De Raedt, L. (1998). Top-down induction first-order logical decision trees.
Artificial Intelligence, 101(1-2), 285297.
810

fiS CALING H EURISTIC P LANNING R ELATIONAL ECISION REES

Bonet, B., Loerincs, G., & Geffner, H. (1997). robust fast action selection mechanism
planning. Proceedings American Association Advancement Artificial
Intelligence Conference (AAAI), pp. 714719. MIT Press.
Botea, A., Enzenberger, M., Muller, M., & Schaeffer, J. (2005). Macro-FF: Improving AI planning
automatically learned macro-operators. Journal Artificial Intelligence Research, 24,
581621.
Coles, A., & Smith, A. (2007). Marvin: heuristic search planner online macro-action learning. Journal Artificial Intelligence Research, 28, 119156.
De la Rosa, T., Jimenez, S., & Borrajo, D. (2008). Learning relational decision trees guiding heuristic planning. International Conference Automated Planning Scheduling
(ICAPS).
De la Rosa, T., Jimenez, S., Garca-Duran, R., Fernandez, F., Garca-Olaya, A., & Borrajo, D.
(2009). Three relational learning approaches lookahead heuristic planning. Working
Notes ICAPS 2009 Workshop Planning Learning, pp. 3744.
De Raedt, L. (2008). Logical Relational Learning. Springer, Berlin Heidelberg.
Doherty, P., & Kvarnstrom, J. (2001). Talplanner: temporal logic based planner. AI Magazine,
22(3), 95102.
Dzeroski, S., De Raedt, L., & Blockeel, H. (1998). Relational reinforcement learning. International Workshop ILP, pp. 1122.
Emde, W., & Wettschereck, D. (1996). Relational instance-based learning. Proceedings
13th Conference Machine Learning, pp. 122130.
Florez, J. E., Garca, J., Torralba, A., Linares, C., Garca-Olaya, A., & Borrajo, D. (2010). Timiplan: application solve multimodal transportation problems. Proceedings SPARK,
Scheduling Planning Applications workshop, ICAPS10.
Fuentetaja, R., & Borrajo, D. (2006). Improving control-knowledge acquisition planning
active learning. ECML, Berlin, Germany, Vol. 4212, pp. 138149.
Gerevini, A., Saetti, A., & Vallati, M. (2009). automatically configurable portfolio-based planner
macro-actions: Pbp. Proceedings 19th International Conference Automated
Planning Scheduling, pp. 191199 Thessaloniki, Greece.
Hoffmann, J., & Nebel, B. (2001). planning system: Fast plan generation heuristic
search. Journal Artificial Intelligence Research, 14, 253302.
Hoffmann, J., Porteous, J., & Sebastia, L. (2004). Ordered landmarks planning. Journal
Artificial Intelligence Research, 22.
Khardon, R. (1999). Learning action strategies planning domains. Artificial Intelligence, 113,
125148.
811

fiD E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

Leckie, C., & Zukerman, I. (1998). Inductive learning search control rules planning. Artificial
Intelligence, 101(12), 6398.
Martin, M., & Geffner, H. (2000). Learning generalized policies planning using concept languages. International Conference Artificial Intelligence Planning Systems, AIPS00.
Martin, M., & Geffner, H. (2004). Learning generalized policies planning examples using
concept languages. Appl. Intell, 20, 919.
Mcallester, D., & Givan, R. (1989). Taxonomic syntax first order inference. Journal ACM,
40, 289300.
McDermott, D. (1996). heuristic estimator means-ends analysis planning. Proceedings
3rd Conference Artificial Intelligence Planning Systems (AIPS), pp. 142149. AAAI
Press.
Minton, S. (1990). Quantitative results concerning utility explanation-based learning. Artif.
Intell., 42(2-3), 363391.
Muggleton, S. (1995). Inverse entailment progol. New Generation Computing, 13, 245286.
Muggleton, S., & De Raedt, L. (1994). Inductive logic programming: Theory methods. Journal
Logic Programming, 19, 629679.
Nau, D., Au, T.-C., Ilghami, O., Kuter, U., Murdock, W., Wu, D., & Yaman, F. (2003). SHOP2:
HTN planning system. Journal Artificial Intelligence Research, 20, 379404.
Newton, M. A. H., Levine, J., Fox, M., & Long, D. (2007). Learning macro-actions arbitrary
planners domains. Proceedings 17th International Conference Automated
Planning Scheduling (ICAPS).
Quinlan, J. (1986). Induction decision trees. Machine Learning, 1, 81106.
Richter, S., & Westphal, M. (2010). LAMA planner: Guiding cost-based anytime planning
landmarks. Journal Artificial Intelligence Research, 39, 127177.
Roger, G., & Helmert, M. (2010). more, merrier: Combining heuristic estimators satisficing planning. Proceedings 20th International Conference Automated Planning
Scheduling (ICAPS), pp. 246249.
Veloso, M., Carbonell, J., Perez, A., Borrajo, D., Fink, E., & Blythe, J. (1995). Integrating planning
learning: PRODIGY architecture. JETAI, 7(1), 81120.
Vidal, V. (2004). lookahead strategy heuristic search planning. Proceedings 14th
International Conference Automated Planning Scheduling (ICAPS 2004), Whistler,
British Columbia, Canada, pp. 150160.
Xu, Y., Fern, A., & Yoon, S. W. (2007). Discriminative learning beam-search heuristics
planning. IJCAI 2007, Proceedings 20th IJCAI, pp. 20412046.
812

fiS CALING H EURISTIC P LANNING R ELATIONAL ECISION REES

Xu, Y., Fern, A., & Yoon, S. (2009). Learning linear ranking functions beam search
application planning. Journal Machine Learning Research, 10, 15711610.
Xu, Y., Fern, A., & Yoon, S. (2010). Iterative learning weighted rule sets greedy search.
Proceedings 20th International Conference Automated Planning Scheduling
(ICAPS) Toronto, Canada.
Yoon, S., Fern, A., & Givan, R. (2006). Learning heuristic functions relaxed plans. Proceedings 16th International Conference Automated Planning Scheduling (ICAPS).
Yoon, S., Fern, A., & Givan, R. (2007). Using learned policies heuristic-search planning.
Proceedings 20th IJCAI.
Yoon, S., Fern, A., & Givan, R. (2008). Learning control knowledge forward search planning.
J. Mach. Learn. Res., 9, 683718.
Zimmerman, T., & Kambhampati, S. (2003). Learning-assisted automated planning: looking back,
taking stock, going forward. AI Magazine, 24, 73 96.

813


