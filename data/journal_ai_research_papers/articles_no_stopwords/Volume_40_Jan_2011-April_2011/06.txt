Journal Articial Intelligence Research 40 (2011) 571-598

Submitted 11/10; published 03/11

Multiagent Learning Large Anonymous Games
Ian A. Kash

kash@seas.harvard.edu

Center Research Computation Society
Harvard University

Eric J. Friedman

ejf27@cornell.edu

Department Operations Research
Information Engineering
Cornell University

Joseph Y. Halpern

halpern@cs.cornell.edu

Department Computer Science
Cornell University

Abstract
large systems, important agents learn act eectively, sophisticated
multi-agent learning algorithms generally scale. alternative approach nd
restricted classes games simple, ecient algorithms converge. shown
stage learning eciently converges Nash equilibria large anonymous games bestreply dynamics converge. Two features identied improve convergence. First,
rather making learning dicult, agents actually benecial many
settings. Second, providing agents statistical information behavior others
signicantly reduce number observations needed.

1. Introduction
Designers distributed systems frequently unable determine agent
system behave, optimal behavior depends users preferences
actions others. natural approach agents use learning algorithm.
Many multiagent learning algorithms proposed including simple strategy update
procedures ctitious play (Fudenberg & Levine, 1998), multiagent versions Qlearning (Watkins & Dayan, 1992), no-regret algorithms (Cesa-Bianchi & Lugosi, 2006).
goal work help designers distributed systems understand
learning practical. discuss Section 2, existing algorithms generally unsuitable
large distributed systems. distributed system, agent limited view
actions agents. Algorithms require knowing, example, strategy chosen
every agent cannot implemented. Furthermore, size distributed systems requires
fast convergence. Users may use system short periods time conditions
system change time, practical algorithm system thousands millions
users needs convergence rate sublinear number agents. Existing
algorithms tend provide performance guarantees polynomial even exponential.
Finally, large number agents system guarantees noise. Agents
make mistakes behave unexpectedly. Even agent changes strategy,
still noise agent payos. example, gossip protocol match dierent

c
2011
AI Access Foundation. rights reserved.

fiKash, Friedman, & Halpern

agents round round; congestion underlying network may eect message delays
agents. learning algorithm needs robust noise.
nding algorithm satises requirements arbitrary games may
dicult, distributed systems characteristics make problem easier. First,
involve large number agents. agents may seem make learning
harderafter all, possible interactions. However, advantage
outcome action typically depends weakly agents do.
makes outcomes robust noise. large number agents make less useful
agent try inuence others; becomes better policy try learn optimal
response. contrast, small number agents, agent attempt guide learning
agents outcome benecial him.
Second, distributed systems often anonymous; matter something, rather many agents it. example, congestion link,
experience single agent depend sending packets,
many sent. Anonymous games long history economics literature
(e.g., Blonski, 2001) subject recent interest computer science
literature (Daskalakis & Papadimitriou, 2007; Gradwohl & Reingold, 2008).
Finally, perhaps importantly, distributed system system designer
controls game agents playing. gives us somewhat dierent perspective
work, takes game given. need solve hard problem
nding ecient algorithm games. Instead, nd algorithms work
eciently interesting classes games, us interesting means type
games system designer might wish agents play. games well behaved,
since would strange design system agents decisions inuence
agents pathological ways.
Section 3, show stage learning (Friedman & Shenker, 1998) robust, implementable minimal information, converges eciently interesting class
games. algorithm, agents divide rounds game series stages.
stage, agent uses xed strategy except occasionally explores. end
stage, agent chooses strategy next stage whatever strategy
highest average reward current stage. prove that, appropriate conditions,
large system stage learners follow (approximate) best-reply dynamics1 despite errors
exploration.
games best-reply dynamics converge, theorem guarantees learners
play approximate Nash equilibrium. contrast previous results, convergence guarantee scales poorly number agents, theorem guarantees convergence nite amount time innite number agents. assumption
best-reply dynamics converge strong one, many interesting games converge best-reply dynamics, including dominance-solvable games, games monotone best
replies, max-solvable games (Nisan, Schapira, & Zohar, 2008). class max-solvable
games particular includes many important games Transmission Control Protocol
(TCP) congestion control, interdomain routing Border Gateway Protocol (BGP),
cost-sharing games, stable-roommates games (Nisan, Schapira, Valiant, & Zohar, 2011).
1. paper, consider best-reply dynamics agents update strategy time.
results best-reply dynamics assume agents update strategy one time.

572

fiMultiagent Learning Large Anonymous Games

Marden, Arslan, Shamma (2007a) observed convergence best-reply dynamics often property games humans design (although observation
slightly dierent notion best-reply dynamics). Moreover, convergence best-reply dynamics weaker assumption common assumption made mechanism design
literature, games interest dominant strategies (each agent strategy
optimal matter agents do).
Simulation results, presented Section 4, show convergence fast practice:
system thousands agents converge thousand rounds. Furthermore,
identify two factors determine rate quality convergence. One number
agents: agents makes noise system consistent agents
learn using fewer observations. giving agents statistical information
behavior agents; speed convergence order magnitude. Indeed,
even noisy statistical information agent behavior, relatively easy
obtain disseminate, signicantly improve performance.
theoretical results limited stage learning, provide intuition
well behaved learning algorithms converge. simulations,
include two learning algorithms, bear out. Furthermore, demonstrate
applicability stage learning realistic settings, simulate results learning
scrip system (Kash, Friedman, & Halpern, 2007). results demonstrate stage
learning robust factors churn (agents joining leaving system)
asynchrony (agents using stages dierent lengths). However, stage learning robust
changes. include simulations games small number agents, games
anonymous, games continuous. games violate
assumptions theoretical results; simulations show that, games, stage
learning converges slowly all.
Finally, participants system necessarily behave expected. learning
useful real system, needs robust behavior. Section 5, show
continuity utility functions key property makes stage learning robust
Byzantine behavior small fraction agents.

2. Related Work
One approach learning play games generalize reinforcement learning algorithms
Q-learning (Watkins & Dayan, 1992). One nice feature approach
handle games state, important distributed systems. Q-learning,
agent associates value state-action pair. chooses action state ,
updates value (, ) based reward received best value
achieve resulting state (max ( , )). generalizing multiple agents,
become vectors state action every agent max replaced
prediction behavior agents. Dierent algorithms use dierent predictions;
example, Nash-Q uses Nash equilibrium calculation (Hu & Wellman, 2003). See
work Shoham, Powers, Grenager (2003) survey.
Unfortunately, algorithms converge slowly large distributed system.
algorithm needs experience possible action prole many times guarantee convergence. So, agents strategies, naive convergence time ( ). Even
573

fiKash, Friedman, & Halpern

better representation anonymous games, convergence time still ( ) (typically
). fundamental problem approach: assumes information agent unlikely have. order know value update,
agent must learn action chosen every agent. practice, agent learn
something actions agents directly interacts, unlikely
gain much information actions agents.
Another approach no-regret learning, agents choose strategy round
guarantees regret choices low. Hart Mas-Colell (2000)
present learning procedure converges correlated equilibrium 2 given knowledge payos every action would round. provide variant algorithm requires information agents actual
payos (Hart & Mas-Colell, 2001). However, guarantee convergence within
correlated equilibrium requires (/2 log ), still slow large systems. Furthermore, convergence guarantee distribution play converges equilibrium;
strategies individual learners converge. Many no-regret algorithms
exist (Blum & Mansour, 2007). Section 4, use Exp3 algorithm (Auer, CesaBianchi, Freund, & Schapire, 2002). achieve even better convergence restricted
settings. example, Blum, Even-Dar, Ligett (2006) showed routing games
continuum no-regret learners approximate Nash equilibrium nite amount
time. Jafari, Greenwald, Gondek, Ercal (2001) showed no-regret learners converge
Nash equilibrium dominance solvable, constant sum, general sum 2 2 games.
Foster Young (2006) use stage-learning procedure converges Nash equilibrium two-player games. Germano Lugosi (2007) showed converges generic
-player games (games best replies unique). Young (2009) uses similar algorithm without explicit stages converges generic -player games. Rather
selecting best replies, algorithms agents choose new actions randomly
equilibrium. Unfortunately, algorithms involve searching whole strategy space,
convergence time exponential. Another algorithm uses stages provide
stable learning environment ESRL algorithm coordinated exploration (Verbeeck,
Nowe, Parent, & Tuyls, 2007).
Marden, Arslan, Shamma (2007b) Marden, Young, Arslan, Shamma (2009)
use algorithm experimentation best replies without explicit stages
converges weakly acyclic games, best-reply dynamics converge agents move
one time, rather moving once, assume here. Convergence based
existence sequence exploration moves lead equilibrium.
agents explore probability , analysis gives convergence time (1/ ).
Furthermore, guarantee requires suciently small agents essentially explore
one time, needs (1/).
Adlakha, Johari, Weintraub, Goldsmith (2010) independently given conditions
existence oblivious equilibrium, mean eld equilibrium, stochastic
games. model require game large, anonymous, continuous. oblivious equilibrium, player reacts average states
2. Correlated equilibrium general solution concept Nash equilibrium (see Osborne & Rubenstein, 1994); every Nash equilibrium correlated equilibrium, may correlated equilibria
Nash equilibria.

574

fiMultiagent Learning Large Anonymous Games

strategies players rather exact values. However, model assumes
players payo depends state players actions. Adlakha
Johari (2010) consider stochastic games strategic complementarities show
mean eld equilibria exist, best-reply dynamics converge, myopic learning dynamics
(which require knowledge aggregate states players) nd them.
long history work examining simple learning procedures ctitious
play (Fudenberg & Levine, 1998), agent makes best response assuming
players strategy characterized empirical frequency observed
moves. contrast algorithms convergence guarantees general games, algorithms fail converge many games. classes games converge,
tend rapidly. However, work area assumes actions
agents observed agents, agents know payo matrix, payos deterministic. recent approach tradition based Win Learn Fast principle,
limited convergence guarantees often performs well practice (Bowling &
Veloso, 2001). Hopkins (1999) showed many procedures converge symmetric
games innite number learners, although results provide guarantees
rate convergence.
body empirical work convergence learning algorithms
multiagent settings. Q-learning empirical success pricing games (Tesauro &
Kephart, 2002), -player cooperative games (Claus & Boutilier, 1998), grid world
games (Bowling, 2000). Greenwald al. (2001) showed number algorithms,
including stage learning, converge variety simple games. Marden et al. (2009) found
algorithm converged must faster congestion game theoretical analysis
would suggest. theorem suggests explanation empirical observations: bestreply dynamics converge games. theorem applies directly
stage learning, provides intuition algorithms learn quickly enough
change behavior slowly enough rapidly converge Nash equilibrium practice.

3. Theoretical Results
section present theoretical analysis model. provide support
simulations following section.
3.1 Large Anonymous Games
interested anonymous games countably many agents. Assuming
countably many agents simplies proofs; straightforward extend results
games large nite number agents. model adapted Blonski (2001). Formally, large anonymous game characterized tuple = (, , , Pr).
countably innite set agents.
nite set actions agent choose (for simplicity, assume
agent choose set actions).
(), set probability distributions , two useful interpretations.
rst set mixed actions. abuse notation denote
575

fiKash, Friedman, & Halpern

mixed action probability 1 . round agent chooses one
mixed actions. second interpretation () fraction
agents choosing action . important notion anonymity,
says agents utility depend many agents choose
action rather chooses it.
= { : ()} set (mixed) action proles (i.e. action
agent chooses). Given mixed action every agent, want know fraction
agents end choosing action . , let ()() denote probability
agent plays according () ().We express fraction
agents choose action lim (1/) =0 ()(), limit exists.
limit exists actions , let () give value limit
. proles use determined simple random process.
proles , strong law large numbers (SLLN) guarantees probability
1 well dened. Thus typically well dened (using similar limits) us
talk fraction agents something.
nite set payos agents receive.
Pr : () ( ) denotes distribution payos results
agent performs action agents follow action prole . use probability
distribution payos rather payo model fact agent payos may
change even agent changes strategy. expected utility agent
performs

mixed action agents follow action distribution (, ) =

() Pr, (). denition Pr terms () rather
ensures game anonymous. require Pr (and thus ) Lipschitz
continuous.3 deniteness, use L1 norm notion distance
specifying continuity (the L1 distance two vectors sum absolute
values dierences component). Note formulation assumes
agents share common utility function. assumption relaxed allow
agents nite number types, show Appendix A.
example large anonymous game one where, round, agent plays
two-player game opponent chosen random. random matching games
common literature (e.g., Hopkins, 1999), meaning opponent chosen
random made formal (Boylan, 1992). game, set actions
two-player game set payos game. every agent chooses
action, distribution opponent actions characterized (). Let ,
denote payo agent plays agent plays . utility
mixed action given distribution

(, ) =
()( ), .
, 2

3. Lipschitz continuity imposes additional constraint constant Pr(, )
Pr(, )/ 1 . Intuitively, ensures distribution outcomes
change fast. standard assumption easily seen hold games
typically considered literature.

576

fiMultiagent Learning Large Anonymous Games

3.2 Best-Reply Dynamics
Given game action distribution , natural goal agent play
action maximizes expected utility respect : argmax (, ). call
action best reply . practical amount time, agent may diculty
determining two actions close expected utilities better, allow
agents choose actions close best replies. best reply ,
-best reply ( , ) + (, ). may one -best reply;
denote set -best replies ABR ().
single agent looking best reply; every agent trying nd one
time. agents start action distribution 0 , nd
best reply new action distribution 1 . assume 0 () = 1/ (agents
choose initial strategy uniformly random), results apply distribution
used determine initial strategy. say sequence (0 , 1 , . . .) -bestreply sequence support +1 subset ABR ( ); +1 gives positive
probability approximate best replies . best-reply sequence converges
exists > , = . Note particularly
strong notion convergence require converge nite time
merely limit. game may innitely many best-reply sequences, say
approximate best-reply dynamics converge exists > 0 every -bestreply sequence converges. limit distribution determines mixed strategy
-Nash equilibrium (i.e. support subset ( )).
main result shows learners successfully learn large anonymous games
approximate best-reply dynamics converge. number stages needed converge
determined number best replies needed sequence converges. possible design games long best-reply sequences, practice games
short sequences. One condition guarantees 0 degenerate action
distributions (i.e., distributions assign probability 1 ) unique
best replies. case, best replies equilibrium reached,
assumed agents utility function. Furthermore,
games distinction -best replies best replies irrelevant; suciently small , -best reply best reply. hard show property
degenerate strategies unique best replies generic; holds almost every game.
3.3 Stage Learners
agent wants nd best reply may know set payos , mapping
actions distributions payos Pr, action distribution (and, indeed,
may changing time), use type learning algorithm learn
it. approach divide play game sequence stages. stage,
agent almost always plays xed action , explores actions.
end stage, chooses new next stage based learned.
important feature approach agents maintain actions entire stage,
stage provides stable environment agents learn. simplify
results, specify way exploring learning within stage (originally described
Friedman & Shenker, 1998), results generalize reasonable learning
577

fiKash, Friedman, & Halpern

algorithm used learn within stage. (We discuss reasonable Section 6.)
section, show that, given suitable parameter, stage agents
learned best reply environment stage.
Given game , round agent needs select mixed action , .
agents use strategies denote , , () = 1 ( = ) =
/( 1). Thus, , agent almost always plays , probability explores
strategies uniformly random. Thus far specied information
agent use choose ,. Dierent games may provide dierent information.
require agent know previous actions previous payos.
precisely, < , knows action () (which determined , )
payos () (which determined Pr(, , ), action distribution
round ; note assume agent knows .) Using information,
express average value action previous = 1/2 rounds (the length
stage).4 Let (, , ) = { < () = }
set recent rounds
played . average value (, , ) = (,,) ()/(, , )
(, , ) > 0 0 otherwise. need value times
multiples , convenience dene arbitrary times .
say agent -stage learner chooses actions follows. = 0,
chosen random { }. nonzero multiple , , = (, )
(, ) = argmax (, , ). Otherwise, , = ,1 . Thus, within stage, mixed
action xed; end stage updates use action highest average
value previous stage.
evolution game played stage learners deterministic; agent chooses
random ,0 sequence () () observes random. However,
countably innite set agents, use SLLN make statements overall
behavior game. Let () = ,. run game consists sequence triples
( , , ). SLLN guarantees probability 1 fraction agents choose
strategy (). Similarly, fraction agents chose receive
payo Pr(, )() probability 1.
make notion stage precise, refer sequence tuples
( , , ) . . . ((+1) 1 , (+1) 1 , (+1) 1 ) stage run. stage
stationary action distribution denote . ,(+1) = ABR ( ),
say agent learned -best reply stage run. following lemma shows, suciently small , agents learn -best reply.
Lemma 3.1. large anonymous games , action proles, approximations > 0,
probabilities error > 0, exists > 0 < ,
agents -stage learners, least 1 fraction agents learn -best reply
stage .
Proof. (Sketch) average, agent using strategy plays action (1 ) times
stage plays actions /( 1) times each. large, realized number
times played close expectation value high probability. Thus,
suciently large, average payo action exponentially close
4. use exponent 2 arbitrary. require expected number times strategy
explored increases decreases.

578

fiMultiagent Learning Large Anonymous Games

true expected value (via standard Hoeding bound sums i.i.d. random variables),
thus learner correctly identify action approximately highest expected
payo probability least 1 . SLLN, least 1 fraction agents
learn -best reply. detailed version proof general setting found
work Friedman Shenker (1998).
3.4 Convergence Theorem
Thus far dened large anonymous games approximate best-reply dynamics
converge. agents game -stage learners, sequence 0 , 1 , . . .
action distributions run game best-reply sequence, close.
action used agents time action used
approximate best reply sequence.
order prove this, need dene close. denition based error rate
exploration rate introduces noise . Intuitively, distribution close
if, changing strategies fraction agents agents explore
fraction time, go action prole corresponding action distribution
one corresponding distribution . Note denition symmetric.
denition, identies (pure) action agent using leads ,
allows fraction agents use action, incorporates fact
agent exploring, strategy (the agent usually plays explores
probability ).
Denition 3.2. Action distribution (, )-close exist , ,
that:
= = ;
() ;
1 2 (this allows fraction agents play dierent strategy
);
, () = () = .
use nal requirement ensures two distributions (, )-close
( , )-close . example asymmetry
denition, (0, ) close , reverse true. (, )-closeness
useful distance measure analysis, unnatural notion distance specifying
continuity , used L1 norm. following simple lemma shows
distinction unimportant; suciently (, )-close close according
L1 measure well.
Lemma 3.3. (, )-close ,
1 2( + ).
Proof. Since (, )-close , exist , , Denition 3.2. Consider
distributions = , , = . view three distributions vectors,
calculate L1 distances. Denition 3.2, 1 2. 1 2
fraction agents explore. Thus triangle inequality, L1 distance
2( + ).
579

fiKash, Friedman, & Halpern

assumed approximate best reply sequences converge,
run game agents actually learning approximate best replies .
following lemma shows distinction matter suciently close.
Lemma 3.4. exists (, )-close , > 0, > 0,
+ < ABR (/2) (
) ABR ().
Proof. Let maximum Lipschitz constants (, ) (one constant
) = /(8). (, )-close , (, ) (, )

1 2/(8) = /4 Lemma 3.3.
Let
/ ABR () argmax ( , ). (, ) + < ( , ). Combining
gives (, ) + /2 < ( , ). Thus
/ ABR/2 (
).
Lemmas 3.1 3.4 give requirements (, ). statement theorem, call
(, ) -acceptable satisfy requirements lemmas /2 -best-reply
sequences converge .
Theorem 3.5. Let large anonymous game approximate best-reply dynamics
converge let (, ) -acceptable . agents -stage learners then,
runs, exists -best-reply sequence 0 , 1 , . . . stage least 1
fraction learn best reply probability 1.
Proof. 0 = 0 (both uniform distribution), 0 (, )-close . Assume
(, )-close . Lemma 3.1 least 1 fraction learn /2-best reply .
Lemma 3.4, -best reply . Thus +1 (, )-close +1 .
Theorem 3.5 guarantees nite number stages, agents close
approximate Nash equilibrium prole. Specically, (, )-close -Nash
equilibrium prole . Note means actually -Nash equilibrium
larger depends ,,, Lipschitz constant .
three requirements practical learning algorithm require minimal
information, converge quickly large system, robust noise. Stage learning
requires agent know payos, rst condition satised. Theorem 3.5 shows satises two requirements. Convergence guaranteed
nite number stages. number stages depends game, Section 3.2
argued many cases quite small. Finally, robustness comes tolerating
fraction errors. proofs assumed errors due learning,
analysis noise sources churn agents
making errors. discuss issue Section 6.

4. Simulation Results
section, discuss experimental results demonstrate practicality learning
large anonymous games. Theorem 3.5 guarantees convergence suciently small
exploration probability , decreasing increases , length stage.
rst set experiments shows necessary values quite reasonable
practice. theorem applies stage learning, analysis provides intuition
580

fiMultiagent Learning Large Anonymous Games

reasonable algorithm changes slowly enough learners chance
learn best replies converge well. demonstrate this, implemented two
learning algorithms, quickly converged.
theoretical results make two signicant predictions factors inuence
rate convergence. Lemma 3.1 tells us length stage determined
number times strategy needs explored get accurate estimate
value. Thus, amount information provided observation large eect
rate convergence. example, random matching game, agents payo
provides information strategy one agent. hand,
receives expected payo matched, single observation provides information
entire distribution strategies. latter case agent learn many
fewer observations. related prediction agents lead faster
convergence, particularly games payos determined average behavior
agents, variance payos due exploration mistakes decreases
number agents increases. experimental results illustrate phenomena.
game used rst set experiments, many simple games used test
learning algorithms, symmetric. Hopkins (1999) showed many learning algorithms
well behaved symmetric games large populations. demonstrate
main results due something symmetry, tested stage learning
asymmetric game, observed convergence even small population.
explore applicability stage learning practical setting violates
number assumptions theorem, implemented variant stage learning
game based scrip system (Kash et al., 2007). demonstrate applicability
approach real systems, included experiments churn (agents leaving
replaced new agents) agents learning dierent rates.
Finally, give examples games large, anonymous, continuous, provide simulations showing stage learners learn far slowly
games satisfy hypotheses Theorem 3.5, learn play
equilibrium all. examples demonstrate assumptions essential
results.
4.1 Contribution Game
rst set experiments, agents play contribution game (also called Diamondtype search model work Milgrom & Roberts, 1990). contribution game,
two agents choose strategies 0 19, indicating much eort contribute
collective enterprise. value agent depends much contributes, well
much agent contributes. contributes contribution
agent , utility 4 ( 5)3 . round game, agent
paired random agent play contribution game. game, best-reply
dynamics converge within 4 stages starting distribution.
implemented three learning algorithms run game. implementation
stage learners described Section 3.3, = 0.05. Rather taking length
stage 1/2 , set = 2500 suciently long stages value , rather
decreasing stages long enough. second algorithm based

581

fiKash, Friedman, & Halpern

6
2 Agents
10 Agents
100 Agents

Distance Equilibrium

5

4

3

2

1

0

0

1

2

3

4

Time

5
4

x 10

Figure 1: Stage learners random matching.
3.5
2 Agents
10 Agents
100 Agents

Distance Equilibrium

3

2.5

2

1.5

1

0.5

0

1

2

3
Time

4

5
4

x 10

Figure 2: Hart Mas-Colell random matching.
Hart Mas-Colell (2001), improvements suggested Greenwald, Friedman,
Shenker (2001). algorithm takes parameters (the exploration probability).
used = 16 = 0.05. nal learning algorithm Exp3 (Auer et al., 2002).
set , exploration probability, 0.05. algorithm requires payos
normalized lie [0, 1]. Since choices strategies lead large negative
payos, naive normalization leads almost every payo close 1. better
performance, normalized payos payos fell range [0, 1]
outside set 0 1 appropriate.
results three algorithms shown Figures 1, 2, 3. curve
shows distance equilibrium function number rounds population
agents given size using given learning algorithm. results averaged
ten runs. Since payos nearby strategies close, want notion distance
take account agents playing 7 closer equilibrium (8) thanthose playing
zero. Therefore, consider expected distance equilibrium:
() 8.
determine , counted number times action taken length
582

fiMultiagent Learning Large Anonymous Games

5
2 Agents
10 Agents
100 Agents

4.5

Distance Equilibrium

4
3.5
3
2.5
2
1.5
1
0.5

0

1

2

3

4

Time

5
4

x 10

Figure 3: Exp3 random matching.
6
2 Agents
10 Agents
100 Agents

Distance Equilibrium

5

4

3

2

1

0

0

1

2

3
Time

4

5
4

x 10

Figure 4: Stage learning average-based payos.
stage, practice distance never zero due mistakes exploration.
ease presentation, graph shows populations size 100; similar results
obtained populations 5000 agents.
stage learning, increasing population size dramatic impact. two
agents, mistakes best replies results mistakes cause behavior quite
chaotic. ten agents, agents successfully learn, although mistakes suboptimal
strategies quite frequent. one hundred agents, agents converge quickly
near equilibrium strategies signicant mistakes rare.
Despite lack theoretical guarantees, two algorithms converge, although
somewhat slowly. long-run performance Exp3 similar stage learning.
Hart Mas-Colells algorithm asymptotic convergence guarantees, tends
converge slowly practice tuned tight convergence. get converge
reasonable amount time tuned parameters accept somewhat weaker convergence
(although particular game shown dierence convergence dramatic).

583

fiKash, Friedman, & Halpern

1.8
2 Agents
10 Agents
100 Agents

1.6

Distance Equilibrium

1.4
1.2
1
0.8
0.6
0.4
0.2
0

0

0.5

1

1.5
Time

2

2.5
4

x 10

Figure 5: Stage learners congestion game.
Convergence stage learning random-matching game takes approximately 10,000
rounds, slow many applications. system design requires type
matching, makes learning problematic. However, results Figure 4 suggest
learning could done much faster system designer could supply agents
information. suggests collecting statistical information behavior
agents may critical feature ensuring fast convergence. model scenario,
consider related game where, rather matched random opponent,
agents contribute project reward based average contribution
agents. results stage learning game shown Figure 4.
much information available agents observation, able cut
length stage factor 10. number stages needed reach equilibrium
remained essentially same. Convergence tighter well; mistakes rare
almost distance equilibrium due exploration.
4.2 Congestion Game
dierent game, tested performance stage learners congestion game.
game models situation two agents share network link. gain utility
proportional transmission rate link, penalized based resulting
congestion experience. game asymmetric two dierent types
agents place dierent values transmission rate. game described detail
Greenwald, Friedman, Shenker (2001), showed no-regret learners able
nd equilibrium game. extension theoretical results games
multiple types presenting Appendix A.
Figure 5 shows stage learners able learn quickly game, using
stages length 250 even though randomly matched player
type. dierent types agents dierent equilibrium strategies,
distance measure use treat observed distribution strategies
equilibrium distribution vectors compute L1 distance.

584

fiMultiagent Learning Large Anonymous Games

1

Fraction Playing Equilibrium Strategy

0.9
0.8
0.7
0.6
0.5
0.4
0.3

Capacity 2
Capacity 4
Capacity 5

0.2
0.1
0

0

5

10
Number Stages

15

20

Figure 6: Stage learners TCP-like game.
4.3 TCP-like Game
previous example, considered random-matching game two agents
dierent types share link. consider game large number agents share
several links (this game variant congestion control game studied Nisan et al.,
2011).
three types agents using network. agent chooses integer rate
transmit 0 10. Links network maximum average rate
agents transmit; exceeded share capacity evenly among agents.
agents utility overall transmission rate network minus penalty
trac dropped due congestion. agent attempts transmit rate
actual rate penalty 0.5( ).5
agents share link average capacity 5. One third agents
constrained sharing link average capacity 2 another third share link
average capacity 4. game unique equilibrium agents rst
third choose rate 2, agents second third choose rate 4, agents
nal third choose rate 9 (so overall average rate 5). results game
best-reply dynamics converge stages uniform starting distribution.
Figure 6 shows results 90 learners (30 type) = 50000 = 0.01,
averaged ten runs. Agents constrained average capacity two quickly learn
equilibrium strategy, followed average capacity four. Agents constrained
average capacity learn equilibrium strategy, sawtooth pattern
small fraction alternately plays 10 rather 9. because, exploration,
actually optimal small number agents play 10. noticeable fraction
so, 9 uniquely optimal. demonstrates that, strictly speaking, game
satisfy continuity requirement. equilibrium, demand bandwidth exactly
equal supply. Thus, small changes demand agents due exploration
large eect amount actually demanded thus payos
5. penalty used work Nisan et al. (2011); using avoids tie-breaking issues
consider.

585

fiKash, Friedman, & Halpern

1

Fraction Playing Equilibrium Strategy

0.9
0.8
0.7
0.6
0.5
0.4
0.3
Type 1
Type 2
Type 3

0.2
0.1
0

0

5

10
Number Stages

15

20

Figure 7: Stage learners random TCP-like games.
various strategies. However, structure game play still tends
remain close equilibrium terms rates agents choose.
addition specic parameters mentioned above, ran 100 simulations
three capacities randomly chosen integer 0 10. Figure 7
shows that, average, results similar. three types agents share common
constraint; type 1 type 2 additional constraint. Unsurprisingly, since
two types symmetric results almost identical. three types demonstrate
sawtooth behavior, type 3 runs due examples Figure 6
fewer constraints gives agents exibility. primarily comes runs
type 1 type 2 constraints larger overall constraint (i.e.
overall constraint matters). Thus three types ability benet resources
demanded agents explore.
4.4 Scrip System Game
motivation work help designers distributed systems understand
learning practical. order demonstrate stage learning could applied
setting, tested variant stage learners model scrip system used
Kash et al. (2007). model, agents pay agents provide service
turn provide service earn money pay future service. Agents may place
dierent values receiving service (), incur dierent costs provide service (), discount
future utility dierent rates (), dierent availabilities provide service ().
used single type agent parameters = 1.0, = 0.05, = 0.9, = 1, average
amount money per agent = 1, stages 200 rounds per agent (only one agent
makes request round).
model large anonymous game whether agent provide
service depends much money currently has. Thus, stage learning specied
work, take account current state (stochastic) game.
Despite this, still implement variant stage learning: x strategy
stage end stage use algorithm designed game determine
586

fiMultiagent Learning Large Anonymous Games

1.6
10 Agents
100 Agents

1.4

Distance Equilibrium

1.2
1
0.8
0.6
0.4
0.2
0
0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

Time

2
4

x 10

Figure 8: Stage learners scrip system.
1.6
10 Agents
100 Agents

1.4

Distance Equilibrium

1.2
1
0.8
0.6
0.4
0.2
0
0.2

0.4

0.6

0.8

1

1.2
Time

1.4

1.6

1.8

2
4

x 10

Figure 9: scrip system churn.
new strategy best reply agent observed. algorithm works
estimating agents probabilities making request chosen volunteer
round, uses probabilities compute optimal policy. Figure 8
shows quite eective. distance measure used based directly measuring
distance agents chosen (threshold) strategy equilibrium strategy, since
unlike previous games impossible directly infer agents strategy round
solely decision whether volunteer. Note number rounds
normalized based number agents Figure 8 later gures; stages actually
lasted ten times long 100 agents.
Real systems static population learning agents. demonstrate
robustness stage learning churn, replaced ten percent agents new agents
randomly chosen initial strategies end period. Figure 9 shows,
essentially eect convergence.

587

fiKash, Friedman, & Halpern

1.6
10 Agents
100 Agents

1.4

Distance Equilibrium

1.2
1
0.8
0.6
0.4
0.2
0
0.2

0.4

0.6

0.8

1

1.2
Time

1.4

1.6

1.8

2
4

x 10

Figure 10: scrip system dierent stage lengths.
Finally, real system often unreasonable expect agents able update
strategies time. Figure 10 shows half agents use stages
222 rounds per agent rather 200 signicant eect convergence.6
4.5 Learning Counterexamples
rst glance, Theorem 3.5 may seem trivial. game best-reply dynamics
guaranteed converge, seems obvious agents attempt nd best replies
successfully nd reach equilibrium. However, show section,
fact alone sucient. particular, three key features games
studythat large, anonymous, continuousare required theorem
hold.
First, game small number agents, mistake made single
agent could quite important, point learning essentially start over.
So, results converted results probability none
nite number agents make mistake given stage, expected time reach
equilibrium following algorithm signicantly longer best-reply dynamics
would suggest. following example game number best replies
needed reach equilibrium approximately number strategies, experimental
results show number stages needed stage learners nd equilibrium
signicantly longer. (We conjecture fact learning time exponentially longer.)
contrast, Theorem 3.5 guarantees that, games satisfying requirements, number
stages needed equal number best replies.
Consider game three agents, , set actions, {0, 1, . . . , }.
utility functions agents symmetric; rst agents utility function given
following table:
6. general expect small variations stage lengths aect convergence; however large
enough dierences result non-Nash convergence. See work Greenwald et al. (2001)
simulations analysis.

588

fiMultiagent Learning Large Anonymous Games

actions
(0, , )
(, , )
(0, 1, 0)
(0, 0, 1)
(1, 1, 0)
(1, 0, 1)
(, , )
(, , )
(, , )
(, , )

payo
1
0
0
0
1
1
1
0
1
0

conditions
= either > 1 > 1
= > 0 either > 1 > 1

= + 1
= + 1 <
=

Agents learning best replies viewed climbing ladder. best reply
(, , ) ( + 1, + 1, + 1) agents reach (, , ), Nash equilibrium.
However, mistake made, agents essentially start over. see works,
suppose agents (3, 3, 3) next stage one makes mistake
select (5, 4, 4). leads best reply sequence (5, 0, 0), (1, 0, 0), (1, 1, 1),
point agents begin climbing again. somewhat complicated structure payos
near 0 ensures agents begin climbing arbitrary patterns mistakes.
typical run, + 2 stages best replies needed reach equilibrium: one stage
initial randomly-chosen strategies, one stage three agents switch strategy 0,
stages climbing. exact number stages vary two agents choose
initial strategy, never greater + 3.
following table gives number rounds (averaged ten runs) stage learners
game rst reach equilibrium. number strategies varies, length
stage = 100( + 1), exploration probability = 0.05.
rounds reach
4
7.0
9
19.3
14
25.8
19
39.5
24
37.3
29
102.7
34
169.4
39
246.6
= 4, stage learners typically require + 2 stages, occasional error raising
average slightly. 9 24, majority runs feature least one agent
making mistake, number stages required closer 2. = 29 up,
many opportunities agents make mistake, number stages required
average range 3 6. Thus learning slower best-reply dynamics,
disparity grows number strategies increases.
small modication example shows problems arise games
anonymous. non-anonymous game large number agents, payos
depend entirely actions small number agents. example, split
set agents three disjoint sets, 0 , 1 , 2 , choose agents 0 0 , 1 1 ,
589

fiKash, Friedman, & Halpern

1
0.9
0.8

Fraction Playing 0

0.7
0.6
0.5
10 agents
100 agents
1000 agents

0.4
0.3
0.2
0.1
0

0

5

10
Number Stages

15

20

Figure 11: Stage learners discontinuous game.
2 2 . Again, agent chooses action {0, . . . , }. payos agents 0, 1,
2 determined above; everyone 0 gets payo 0, everyone 1
gets payo 1, everyone 2 gets payo 2. Again, convergence
equilibrium signicantly slower best-reply dynamics.
Finally, consider following game, large anonymous, satisfy
continuity requirement. set actions = {0, 1}, agent always receives
payo = {0, 1, 10}. agent chooses action 0, payo always 1 (Pr0, (1) = 1).
chooses action 1, payo 10 every agent chooses action 1, 10 every
agent chooses action 0, 0 otherwise (Pr1,(1,0) (10) = 1, Pr1,(0,1) (10) = 1,
Pr1, (0) = 1) {(1, 0), (0, 1)}).
game, suppose approximate best-reply dynamics start (0.5, 0.5) (each action
chosen half agents). coordinated, unique approximate best
reply agents action 0, one best reply, action distribution (1, 0).
Since agents coordinated, another round approximate best replies leads
equilibrium (0, 1). agents stage learners, rst stage learn
approximate best reply (0.5, 0.5) (exploration change action prole
case), adopt mixed action 0 : playing 0 probability 1 1
probability . Thus, even agents make mistake, action distribution next
stage least fraction playing action 1. Thus unique approximate best
reply action 0; stage learners stuck 0, never reach equilibrium
1.
Figure 11 shows fraction times strategy 0 played stage (averaged
ten runs) 10, 100, 1000 agents ( = 100 = 0.05). ten agents,
initial mistakes made, stage 10 strategy 0 played 2.5%
time runs, corresponds fraction time expect see simply
exploration. 100 agents see another sawtooth pattern agents
stuck playing 0, alternating rounds small fraction plays 1. happens
because, rounds playing 0, small fraction lucky explore 1
agents explore. result, adopt strategy 1 next stage. However,
not, following stage agents return playing 0. oscillating
590

fiMultiagent Learning Large Anonymous Games

behavior observed learning contexts, example among competing myopic
pricebots (Kephart, Hanson, & Greenwald, 2000). 1000 agents, lucky agents
quite rare, essentially agents constantly stuck playing 0.

5. Learning Byzantine Agents
practice, learning algorithms need robust presence agents
following algorithm. seen stage learning large anonymous games
robust agents learn instead follow xed strategy stage.
analysis, agents simply treated agents made mistake
previous stage. However, agent need follow xed strategy; agent attempting
interfere learning malicious reasons personal gain likely adapt
strategy time. However, show section, stage learning handle
manipulation large anonymous games.
Gradwohl Reingold (2008) examined several classes games introduced
notion stable equilibrium one change strategy small fraction
agents small eect payo agents. denition
games nite number agents, easily adapted notion large
anonymous game. take notion step characterize game, rather
equilibrium, stable every strategy stable.
Denition 5.1. large anonymous game (, )-stable , ()
1 , (, ) (, ) .
One class games consider -continuous games. -continuity essentially
version Lipschitz continuity nite games, easy show large anonymous
games stable, amount manipulation tolerated depends
Lipschitz constants agents utility functions.
Lemma 5.2. large anonymous games , exists constant
, (, / )-stable
Proof. , (, ) Lipschitz continuous constant (, )
(, )/ 1 . Take = max . 1
/ ,
(, ) (, ) 1
1
(
)

( )

.

Gradwohl Reingold (2008) show stable equilibria several nice properties.
small fraction agents deviate, payos agent decrease much
relative equilibrium. Additionally, following strategies still approximate
591

fiKash, Friedman, & Halpern

equilibrium despite deviation. Finally, means strategies still constitute
approximate equilibrium even asynchronous play causes strategies fraction
agents revealed others.
show that, game stable, learning robust actions small
fraction Byzantine agents. following lemma adapts Lemma 3.1 show that,
stage, agents learn approximate best replies despite actions Byzantine agents.
Thus agents successfully reach equilibrium, shown Theorem 3.5.
state lemma, need dene actions Byzantine agent.

Byzantine agents, stage would stationary strategy

corresponding fraction agents choosing action . fraction Byzantine agents
change actions arbitrarily round, eect
actions agents. Thus, Byzantine agents cause observed fraction
agents choosing strategy round 1 < 2. refer
sequence , . . . , ( +1)1 condition holds consistent
sequence. say agents learn -best reply stage , mean
, actions players
strategy learn approximate best reply
would used Byzantine players, actual action prole,
includes strategies used Byzantine players.
Lemma 5.3. large anonymous games , action distributions , approximations
> 0, probabilities error > 0, fractions agents < /6 , exists > 0
< , , consistent sequences , . . . , ( +1)1 , agents
-stage learners, least 1 fraction agents learn -best reply
stage despite fraction Byzantine agents.
Proof. Consider agent round stage . agents stage learners
action distribution would = . However, Byzantine agents changed
2. Fix action . Lemma 5.2,
(, ) (, ) 2 <

2

=
6
3

means Byzantine agents adjust agents expected estimate value
action /3. Let best reply (the action used stage learners
stage ). round stage ,
( , ) ( , ) <


.
3

action -best reply,
( , ) (, ) = (( , ) (, ) + ((, ) (, )) >


2
=
.
3
3

Thus, regardless actions fraction Byzantine agents, agent expected
estimate value exceeds expected estimate value least /3.
Using Hoeding bounds before, suciently large , estimates exponentially
close expectations, probability least 1 , select best
action -best reply. SLLN, means least 1 fraction
agents learn -best reply.
592

fiMultiagent Learning Large Anonymous Games

Thus, Lemma 5.3 shows, stage learners learn despite agents learning incorrect values, tolerate suciently small number agents behaving
arbitrarily.

6. Discussion
results show natural learning algorithm learn eciently interesting class games, many issues merit exploration.
6.1 Learning Algorithms
theorem assumes agents use simple rule learning within stage:
average value payos received. However, certainly rules estimating value action; used long rule guarantees
errors made arbitrarily rare given sucient time. necessary restrict
agents stage learning. Stage learning guarantees stationary environment period
time, strict behavior may needed practical. approaches,
exponentially discounting weight observations (Greenwald et al., 2001; Marden
et al., 2009) Win Learn Fast (Bowling & Veloso, 2001) allow algorithm focus
learning recent observations provide stable environment agents
learn.
6.2 Update Rules
addition using dierent algorithms estimate values actions, learner could
change way uses values update behavior. example, rather
basing new strategy last stage, could base entire history
stages use rule spirit ctitious play. Since games ctitious
play converges best-reply dynamics not, could extend results another
interesting class games, long errors period accumulate time.
Another possibility update probabilistically use tolerance determine whether
update (see, e.g., Foster & Young, 2006; Hart & Mas-Colell, 2001). could allow
convergence games best-reply dynamics oscillate decrease fraction agents
make mistakes system reaches equilibrium.
6.3 Model Assumptions
model makes several unrealistic assumptions, notably countably
many agents share utility function. Essentially results holds
large, nite number agents, adding error terms. particular, since
always small probability every agent makes mistake time,
prove 1 fraction agents make errors rounds,
agents spend time playing equilibrium strategies.
implicitly assumed set agents xed. Figure 9 shows,
could easily allow churn. natural strategy newly arriving agents pick
random use next stage. agents this, follows convergence
unaected: treat new agents part fraction made mistake
593

fiKash, Friedman, & Halpern

last stage. Furthermore, tells us newly arriving agents catch quickly.
single stage, new agents guaranteed learned best reply probability
least 1 .
Finally, assumed agents utility function. results
easily extended include nite number dierent types agents,
utility function, since SLLN applied type agent. extension
discussed Appendix A. believe results hold even set possible
types innite. happen, example, agents utility depends valuation
drawn interval. However, care needed dene best-reply sequences
case.
6.4 State
One common feature distributed systems addressed theoretical portion
work state. saw scrip system Section 4.4, agents current state
often important factor choosing optimal action.
principle, could extend framework games state: stage
agent chooses policy usually follow explores actions probability .
agent could use o-policy algorithm (one agent learn without
controlling sequence observations; see Kaelbling, Littman, & Moore, 1996 examples) learn optimal policy use next stage. One major problem
approach standard algorithms learn slowly purposes. example, Q-learning (Watkins & Dayan, 1992) typically needs observe state-action pair
hundreds times practice. low exploration probability means expected
/ rounds needed explore pair even large. Ecient learning requires
specialized algorithms make better use structure problem. However, use specialized algorithms makes providing general guarantee convergence
dicult. Another problem that, even agent explores action
possible local states, payo receives depend states agents,
thus actions chose. need property game guarantee
distribution states sense well behaved. Adlakha Joharis (2010) work
mean eld equilibria gives one condition. setting, use publicly available
statistics might provide solution problems.
6.5 Mixed Equilibria
Another restriction results agents learn pure strategies. One way
address discretize mixed strategy space (see, e.g., Foster & Young, 2006).
one resulting strategies suciently close equilibrium strategy bestreply dynamics converge discretized strategies, expect agents converge
near-equilibrium distribution strategies. empirical success using
approach learn play rock-paper-scissors.

594

fiMultiagent Learning Large Anonymous Games

7. Conclusion
Learning distributed systems requires algorithms scalable thousands agents
implemented minimal information actions agents.
general-purpose multiagent learning algorithms fail one requirements.
shown stage learning ecient solution large anonymous games
approximate best-reply dynamics lead approximate pure strategy Nash equilibria.
Many interesting classes games property, frequently found designed
games. contrast previous work, time convergence guaranteed theorem
increase number agents. system designers nd appropriate
game satisfying properties base systems, condent
nodes eciently learn appropriate behavior.
results highlight two factors aid convergence. First, learners often improves performance. learners, noise introduced payos
exploration mistakes becomes consistent. Second, information typically improves performance. Publicly available statistics observed behavior
agents allow agent learn eectively making fewer local observations.
simulations demonstrate eects two factors, well results generalize
situations learning algorithms, churn, asynchrony, Byzantine behavior.
7.1 Acknowledgments
work done IK Cornell University. EF, IK, JH supported
part NSF grant ITR-0325453. JH supported part NSF grant IIS-0812045
AFOSR grants FA9550-08-1-0438 FA9550-05-1-0055. EF supported
part NSF grant CDI-0835706.

Appendix A. Multiple Types
section, extend denition large anonymous game settings
agents may dierent utility functions. so, introduce notion type.
Agents utilities may depend type fraction type taking action.
results rely strong law large numbers, restrict set types
nite. Formally, large anonymous game types characterized tuple =
(, , , , , Pr). dene , , , before. remaining terms:
nite set agent types.
: function mapping agent type.
before, () set probability distributions , viewed
set mixed actions available agent. now, describe fraction
agents type choosing action, must use element () .
Pr : () ( ) determines distribution payos results
agent type performs action agents follow action prole .
expected utility agent type performs mixed action agents

595

fiKash, Friedman, & Halpern



follow action distribution (, , ) = () Pr,, (). before,
require Pr (and thus ) Lipschitz continuous.
revised denitions -best reply, -Nash equilibrium, -best-reply sequence, convergence approximate best-reply dynamics, (, )-close follow naturally
revised denitions . Lemma 3.1 applies type agent separately, shows small fraction type learn approximate best
reply stage. Lemma 3.3 Lemma 3.4 hold given revised denitions
. Thus Theorem 3.5, combines these, still holds.

References
Adlakha, S., & Johari, R. (2010). Mean eld equilibrium dynamic games complementarities. IEEE Conference Decision Control (CDC).
Adlakha, S., Johari, R., Weintraub, G. Y., & Goldsmith, A. (2010). Mean eld analysis
large population stochastic games. IEEE Conference Decision Control
(CDC).
Auer, P., Cesa-Bianchi, N., Freund, Y., & Schapire, R. E. (2002). nonstochastic multiarmed bandit problem. SIAM Journal Computing, 32 (1), 4877.
Blonski, M. (2001). Equilibrium characterization large anonymous games. Tech. rep.,
U. Mannheim.
Blum, A., Even-Dar, E., & Ligett, K. (2006). Routing without regret: convergence
Nash equilibria regret-minimizing algorithms routing games. 25th ACM
Symp. Principles Distributed Computing (PODC), pp. 4552.
Blum, A., & Mansour, Y. (2007). Learning, regret minimization, equilibria. Nisan,
N., Roughgarden, T., Tardos, E., & Vazirani, V. (Eds.), Algorithmic Game Theory,
pp. 79102. Cambridge University Press.
Bowling, M. H. (2000). Convergence problems general-sum multiagent reinforcement
learning. 17th Int. Conf. Machine Learning (ICML 2000), pp. 8994.
Bowling, M. H., & Veloso, M. M. (2001). Rational convergent learning stochastic
games. 17th Int. Joint Conference Articial Intelligence (IJCAI 2001), pp.
10211026.
Boylan, R. T. (1992). Laws large numbers dynamical systems randomly matched
indviduals. Journal Economic Theory, 57, 473504.
Cesa-Bianchi, N., & Lugosi, G. (2006). Prediction, Learning Games. Cambridge University Press.
Claus, C., & Boutilier, C. (1998). dynamics reinforcement learning cooperative
multiagent systems. AAAI-97 Workshop Multiagent Learning, pp. 746752.
Daskalakis, C., & Papadimitriou, C. H. (2007). Computing equilibria anonymous games.
48th Annual IEEE Symposium Foundations Computer Science (FOCS 2007),
pp. 8393.

596

fiMultiagent Learning Large Anonymous Games

Foster, D. P., & Young, P. (2006). Regret testing: Learning play Nash equilibrium without
knowing opponent. Theoretical Economics, 1, 341367.
Friedman, E. J., & Shenker, S. (1998). Learning implementation internet. Tech.
rep., Cornell University.
Fudenberg, D., & Levine, D. (1998). Theory Learning Games. MIT Press.
Germano, F., & Lugosi, G. (2007). Global Nash convergence Foster Youngs regret
testing. Games Economic Behavior, 60 (1), 135154.
Gradwohl, R., & Reingold, O. (2008). Fault tolerance large games. Proc. 9th ACM
Conference Electronic Commerce (EC 2008), pp. 274283.
Greenwald, A., Friedman, E. J., & Shenker, S. (2001). Learning networks contexts:
Experimental results simulations. Games Economic Behavior, 35 (1-2), 80
123.
Hart, S., & Mas-Colell, A. (2000). simple adaptive procedure leading correlated equilibrium. Econometrica, 68 (5), 11271150.
Hart, S., & Mas-Colell, A. (2001). reinforecement learning procedure leading correlated
equilibrium. Debreu, G., Neuefeind, W., & Trockel, W. (Eds.), Economic Essays,
pp. 181200. Springer.
Hopkins, E. (1999). Learning, matching, aggregation. Games Economic Behavior,
26, 79110.
Hu, J., & Wellman, M. P. (2003). Nash Q-learning general-sum stochastic games. Journal
Machine Learning Research, 4, 10391069.
Jafari, A., Greenwald, A. R., Gondek, D., & Ercal, G. (2001). no-regret learning,
ctitious play, nash equilibrium. Proc. Eighteenth International Conference
Machine Learning (ICML), pp. 226233.
Kaelbling, L. P., Littman, M. L., & Moore, A. P. (1996). Reinforcement learning: survey.
J. Artif. Intell. Res. (JAIR), 4, 237285.
Kash, I. A., Friedman, E. J., & Halpern, J. Y. (2007). Optimizing scrip systems: Eciency,
crashes, hoarders altruists. Eighth ACM Conference Electronic Commerce
(EC 2007), pp. 305315.
Kephart, J. O., Hanson, J. E., & Greenwald, A. R. (2000). Dynamic pricing software
agents. Computer Networks, 32 (6), 731752.
Marden, J. R., Arslan, G., & Shamma, J. S. (2007a). Connections cooperative
control potential games. Proc. 2007 European Control Conference (ECC).
Marden, J. R., Arslan, G., & Shamma, J. S. (2007b). Regret based dynamics: convergence
weakly acyclic games. 6th Int. Joint Conf. Autonomous Agents Multiagent
Systems (AAMAS), pp. 4249.
Marden, J. R., Young, H. P., Arslan, G., & Shamma, J. S. (2009). Payo-based dynamics
multi-player weakly acyclic games. SIAM Journal Control Optimization,
48 (1), 373396.

597

fiKash, Friedman, & Halpern

Milgrom, P., & Roberts, J. (1990). Rationalizability, learning, equilibrium games
strategic complement- arities. Econometrica, 58 (6), 12551277.
Nisan, N., Schapira, M., Valiant, G., & Zohar, A. (2011). Best-response mechanisms.
Proc. Second Syposium Innovations Computer Science (ICS). Appear.
Nisan, N., Schapira, M., & Zohar, A. (2008). Asynchronous best-reply dynamics.
Proc. 4th International Workshop Internet Netwrok Economics (WINE), pp.
531538.
Osborne, M., & Rubenstein, A. (1994). Course Game Theory. MIT Press.
Shoham, Y., Powers, R., & Grenager, T. (2003). Multi-agent reinforcement learning:
critical survey. Tech. rep., Stanford.
Tesauro, G., & Kephart, J. O. (2002). Pricing agent economies using multi-agent Qlearning. Autonomous Agents Multi-Agent Systems, 5 (3), 289304.
Verbeeck, K., Nowe, A., Parent, J., & Tuyls, K. (2007). Exploring selsh reinforcement
learning repeated games stochastic rewards. Journal Autonomous Agents
Multi-agent Systems, 14, 239269.
Watkins, C. J., & Dayan, P. (1992). Technical note Q-learning. Machine Learning, 8,
279292.
Young, H. P. (2009). Learning trial error. Games Economic Behavior, 65 (2),
626643.

598


