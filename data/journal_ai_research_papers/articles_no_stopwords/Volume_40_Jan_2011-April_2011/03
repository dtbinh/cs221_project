journal artificial intelligence

submitted published

narrowing modeling gap
cluster ranking coreference resolution
altaf rahman
vincent ng

altaf hlt utdallas edu
vince hlt utdallas edu

human language technology institute
university texas dallas
west campbell road mail station ec
richardson tx u

abstract
traditional learning coreference resolvers operate training mention pair
model determining whether two mentions coreferent though conceptually
simple easy understand mention pair model linguistically rather unappealing
lags far behind heuristic coreference proposed pre statistical
nlp era terms sophistication two independent lines recent attempted improve mention pair model one acquiring mention ranking model
rank preceding mentions given anaphor training entity mention
model determine whether preceding cluster coreferent given mention
propose cluster ranking coreference resolution combines strengths
mention ranking model entity mention model therefore theoretically
appealing addition seek improve cluster rankers
via two extensions lexicalization incorporating knowledge anaphoricity
jointly modeling anaphoricity determination coreference resolution experimental ace data sets demonstrate superior performance cluster rankers
competing approaches well effectiveness two extensions

introduction
noun phrase np coreference resolution task identifying nps mentions
ace terminology text dialogue refer real world entity concept
computational perspective coreference clustering task goal partitioning
set mentions coreference clusters cluster contains
mentions co referring mathematical perspective coreference relation
equivalence relation defined pair mentions satisfies reflexivity symmetry
transitivity following previous work coreference resolution use term
anaphoric describe mention part coreference chain head
chain given anaphoric mention mk antecedent mk mention coreferent
mk precedes associated text set candidate antecedents mk
consists mentions precede mk
precisely mention instance reference entity real world article
treat terms mention noun phrase synonymous use interchangeably
note definitions somewhat overloaded linguistically anaphor noun phrase
depends antecedent semantic interpretation hence barack obama anaphoric
definition formal definition
c

ai access foundation rights reserved

firahman ng

focus computational coreference resolution exhibited gradual shift
heuristic approaches machine learning approaches past decade shift
attributed part advent statistical natural language processing nlp
era part public availability coreference annotated corpora produced
muc muc conferences series ace evaluations one
influential machine learning approaches coreference resolution classificationbased coreference recast binary classification task e g aone
bennett mccarthy lehnert specifically classifier trained
coreference annotated data used determine whether pair mentions co referring
however pairwise classifications produced classifier commonly known mention pair model may satisfy transitivity property inherent
coreference relation since possible model classify b coreferent
b c coreferent c coreferent separate clustering mechanism needed coordinate possibly contradictory pairwise classification decisions
construct partition given mentions
mention pair model significantly influenced learning coreference
past fifteen years fact many recently published coreference papers
still classical learning coreference model e g bengtson roth
stoyanov gilbert cardie riloff despite popularity model least
two major weaknesses first since candidate antecedent mention resolved
henceforth active mention considered independently others model
determines good candidate antecedent relative active mention
good candidate antecedent relative candidates words fails
answer critical question candidate antecedent probable second
limitations expressiveness information extracted two mentions
alone may sufficient making informed coreference decision especially
candidate antecedent pronoun semantically empty mention lacks
descriptive information gender e g clinton
recently coreference researchers investigated alternative coreference
aim address aforementioned weaknesses mention pair model address
first weakness researchers proposed mention ranking model model determines candidate antecedent probable given active mention imposing
ranking candidate antecedents e g denis baldridge b iida inui
matsumoto ranking arguably natural formulation coreference resolution classification ranker allows candidate antecedents considered
simultaneously therefore directly captures competition among another desirable consequence exists natural resolution strategy ranking
mention resolved candidate antecedent highest rank contrasts
classification approaches many clustering employed
co ordinate pairwise coreference decisions unclear one best
address second weakness researchers proposed entity mention coreference
model e g luo ittycheriah jing kambhatla roukos yang su zhou tan
yang su lang tan li unlike mention pair model entity mention
model trained determine whether active mention belongs preceding possibly
partially formed coreference cluster hence employ cluster level features e fea

fia cluster ranking coreference resolution

tures defined subset mentions preceding cluster makes
expressive mention pair model
entity mention model mention ranking model conceptually simple
extensions mention pair model born nearly ten years mention pair
model proposed particular contributions estimated
paved way thinking supervised modeling coreference represents
significant departure mention pair counterpart many years
learning coreference model nlp researchers proposal two
facilitated part advances statistical modeling natural languages statistical
nlp evolved capturing local information global information
employing classification ranking context
coreference resolution entity mention model enables us compute features
variable number mentions mention ranking model enables us rank variable
number candidate antecedents nevertheless neither addresses
weaknesses mention pair model satisfactorily mention ranking model allows
candidate antecedents ranked compared simultaneously enable
use cluster level features hand entity mention model employ
cluster level features allow candidates considered simultaneously
motivated part observation propose learning coreference resolution theoretically appealing mention ranking model
entity mention model cluster ranking specifically recast coreference determining set preceding coreference clusters
best link active mention learned cluster ranking model essence
cluster ranking model combines strengths mention ranking model entitymention model addresses weaknesses associated mention pair model
cluster ranking model appears conceptually simple natural extension entity mention model mention ranking model believe
simplicity stems primarily choice presentation concepts easiest
reader understand particular note mental processes involved
design cluster ranking model means simple way model
presented requires analysis strengths weaknesses existing
approaches learning coreference resolution connection
formulation view entity mention model mention ranking
model addressing two complementary weaknesses mention pair model believe
significance cluster ranking model lies bridging two rather independent
lines learning coreference going past years
one involving entity mention model mention ranking model
addition seek improve cluster ranking model two sources linguistic knowledge first propose exploit knowledge anaphoricity e knowledge
whether mention anaphoric anaphoricity determination means
neither use anaphoricity information improve coreference resolution innovation lies way learn knowledge anaphoricity specifically
previous work typically adopted pipeline coreference architecture
anaphoricity determination performed prior coreference resolution resulting
information used prevent coreference system resolving mentions de

firahman ng

termined non anaphoric overview see work poesio uryupina vieira
alexandrov kabadjov goulart propose model jointly learning anaphoricity determination coreference resolution note major weakness pipeline
architecture lies fact errors anaphoricity determination could propagated
coreference resolver possibly leading deterioration coreference performance
ng cardie joint model potential solution error propagation

second examine kind linguistic features exploited majority
existing supervised coreference resolvers word pairs composed strings
head nouns active mention one preceding mentions intuitively
word pairs contain useful information example may help improve precision
model allowing learner learn moderate probability
anaphoric contrary taken phrase contrary never
anaphoric may help improve recall allowing learner determine
instance airline carrier coreferent hence offer convenient
means attack one major coreference identifying coreferent
common nouns lexically dissimilar semantically related note
extremely easy compute even called cheap features stringmatching grammatical features yang zhou su tan majority
existing supervised coreference systems unlexicalized hence exploiting
somewhat unexpectedly however researchers lexicalize coreference
employing word pairs features e g luo et al daume iii marcu
bengtson roth feature analysis experiments indicate lexical features
best marginally useful instance luo et al daume iii marcu report
leaving lexical features feature ablation experiments causes ace value
drop respectively previous attempts lexicalization merely
append word pairs conventional coreference feature set goal investigate
whether make better use lexical features learning coreference resolution
sum propose cluster ranking coreference resolution joint
model exploiting anaphoricity information investigate role lexicalization
learning coreference resolution besides empirically demonstrating clusterranking model significantly outperforms competing approaches ace coreference
data set two extensions model namely lexicalization joint modeling
effective improving performance believe work makes four contributions
coreference resolution
narrowing modeling gap machine learning approaches coreference resolution received lot attention since mid mention pair model
heavily influenced learning coreference decade yet
model lags far behind heuristic coreference proposed
terms sophistication particular notion ranking traced back
centering information see books mitkov walker joshi
prince idea behind ranking preceding clusters heuristic manner
found lappin leasss influential pronoun resolution
cluster ranking model completely close gap simplicity machine
learning approaches sophistication heuristic approaches coreference resolu

fia cluster ranking coreference resolution

tion believe represents important step towards narrowing gap another
important gap cluster ranking model helps bridge two independent lines
learning coreference going past years one
involving entity mention model mention ranking model
promoting use ranking mention ranking model
empirically shown outperform mention pair model denis baldridge b
former received much attention among coreference researchers
particular mention pair model continues popularly used investigated
past years mention ranking model believe lack excitement
ranking approaches coreference resolution attributed least part
lack theoretical understanding ranking previous work ranking coreference
resolution employed ranking essentially black box without opening
black box could difficult researchers appreciate subtle difference
ranking classification attempt promote use ranking
provide brief history use ranking coreference resolution section
tease apart differences classification ranking showing constrained
optimization support vector machine svm attempts solve classificationbased ranking coreference section
gaining better understanding existing learning coreference
recall lexicalization one two linguistic knowledge sources propose
use improve cluster ranking model note lexicalization applied
cluster ranking model essentially learning coreference however
mentioned vast majority existing coreference resolvers unlexicalized
fact mention ranking model shown improve mention pair model
unlexicalized feature set attempt gain additional insights behavior
different learning coreference compare performance lexicalized
feature set furthermore analyze via experiments involving feature ablation
data source adaptability well report performance resolving different types
anaphoric expressions
providing implementation cluster ranking model stimulate
ranking approaches coreference resolution facilitate use
coreference information high level nlp applications make software implements cluster ranking model publicly available
rest article organized follows section provides overview use
ranking coreference resolution section describes baseline coreference
mention pair model entity mention model mention ranking model
discuss cluster ranking joint model anaphoricity determination
coreference resolution section section provides details lexicalize
coreference present evaluation experimental analyses different
aspects coreference section section respectively finally
conclude section
software available http www hlt utdallas edu altaf cherrypicker



firahman ng

ranking approaches coreference resolution bit history
ranking theoretically empirically better formulation learning coreference resolution classification mention ranking model popularly
used investigated mention pair counterpart since proposed promote
ranking coreference set stage discussion learningbased coreference next section provide section brief history
use ranking heuristic learning coreference resolution
broader sense many heuristic anaphora coreference resolvers rankingbased example antecedent anaphoric pronoun hobbss seminal syntax resolution considers sentences given text reverse
order starting sentence pronoun resides searching potential
antecedents corresponding parse trees left right breadth first manner
obeys binding agreement constraints hence keep searching beginning
text reached e stop even proposes antecedent
obtain ranking candidate antecedents pronoun consideration rank candidate determined order proposed
fact rank antecedent obtained via method commonly
known hobbss distance used linguistic feature statistical
pronoun resolvers e g ge hale charniak charniak elsner general
search resolution hobbss consider candidate antecedents particular order typically propose first candidate satisfies linguistic constraints
antecedent
strictly speaking however may want consider heuristic resolution
ranking considers candidate antecedents simultaneously
example assigning rank score candidate selecting highest ranked
highest scored candidate antecedent even stricter definition
ranking still many heuristic resolvers ranking resolvers
typically assign rank score candidate antecedent number factors
knowledge sources propose one highest rank score
antecedent e g carbonell brown cardie wagstaff factor belongs
one two types constraints preferences mitkov constraints must satisfied
two mentions posited coreferent examples constraints include gender
number agreement binding constraints semantic compatibility preferences indicate likelihood candidate antecedent preference factors measure
compatibility anaphor candidate e g syntactic parallelism favors candidates grammatical role anaphor preference factors
computed candidate typically capturing salience candidate
constraint preference manually assigned weight indicating importance
instance gender disagreement typically assigned weight indicating
candidate anaphor must agree gender whereas preference factors typically
finite weight score candidate obtained summing weights
factors associated candidate
ranking resolution assign score candidate antecedent rather simply impose ranking candidates salience


fia cluster ranking coreference resolution

perhaps representative family employ salience rank candidates centering descriptions specific centering see
work grosz joshi weinstein walker et al mitkov
salience mention typically estimated grammatical role used rank
forward looking centers
work related lappin leass whose goal perform pronoun resolution assigning anaphoric pronoun highest ranked preceding
cluster therefore heuristic cluster ranking model many heuristic
resolvers lappin leasss identifies highest ranked preceding cluster
active mention first applying set linguistic constraints filter candidate antecedents grammatically incompatible active mention ranking
preceding clusters contain mentions survive filtering process
salience factors examples salience factors include sentence recency whether preceding cluster contains mention appears sentence currently processed
subject emphasis whether cluster contains mention subject position existential emphasis whether cluster contains mention predicate nominal
existential construction accusative emphasis whether cluster contains mention
appears verbal complement accusative case salience factor associated
manually assigned weight indicates importance relative factors
score cluster sum weights salience factors applicable
cluster lappin leasss widely read pronoun resolution
cluster ranking aspect rarely emphasized fact
aware recent work learning coreference resolution establishes
connection entity mention model lappin leasss
despite conceptual similarities cluster ranking model lappin leasss
differ several respects first lappin leass tackle pronoun resolution rather full coreference task second apply linguistic constraints
filter incompatible candidate antecedents resolution strategy learned without applying hand coded constraints separate filtering step third attempt
compute salience preceding cluster respect active mention attempt
determine compatibility cluster active mention factors
determine salience lexical grammatical compatibility instance
finally heuristic weights associated salience
factor encoded manually rather learned unlike system
first learning coreference resolution written connolly burger
day published year lappin leasss
contrary common expectation coreference model proposes rankingbased model influential mention pair model main idea behind connolly et
al convert ranking n candidate antecedents set
pairwise ranking involves ranking exactly two candidates
rank two candidates classifier trained training set instance
corresponds active mention well two candidate antecedents possesses
class value indicates two candidates better idea certainly ahead
time embodied many advanced ranking developed
machine learning information retrieval communities past years


firahman ng

later invented almost time independently yang et al
iida inui takamura matsumoto refer twin candidate model
tournament model respectively name twin candidate model motivated
fact model considers two candidates time whereas name tournament
model assigned ranking two candidates viewed tournament
higher ranked candidate winning tournament candidate wins
largest number tournaments chosen antecedent active mention
bit history rarely mentioned literature reveals three somewhat interesting
perhaps surprising facts first ranking first applied train coreference
much earlier people typically think second despite first learning
coreference model connolly et al ranking model theoretically appealing
classification mention pair model later shown yang et al
iida et al empirically better well finally despite theoretical empirical
superiority connolly et al model largely ignored nlp community received
attention invented nearly decade later time period
mention pair counterpart essentially dominated learning coreference
conclude section making important observation distinction classification ranking applies discriminative generative
generative try capture true conditional probability event context coreference resolution probability mention particular
antecedent referring particular entity e preceding cluster since probabilities normalize similar ranking objective system trying raise
probability mention refers correct antecedent entity expense
probabilities refers thus antecedent version generative
coreference model proposed ge et al resembles mention ranking model
entity version proposed haghighi klein similar spirit
cluster ranking model

baseline coreference
section describe three coreference serve baselines
mention pair model entity mention model mention ranking model illustrative purposes use text segment shown figure mention
segment annotated cid
mid mid mention id cid id cluster
belongs see mentions partitioned four sets barack
obama one cluster remaining mentions cluster

may possible perhaps crucial determine mention pair model received
lot attention connolly et al model since days academic papers
could accessed easily electronic form speculate publication venue played role
connolly et al work published methods language processing conference
later book chapter whereas mention pair model introduced aone
bennetts mccarthy lehnerts appeared proceedings
two comparatively higher profile ai conferences acl ijcai



fia cluster ranking coreference resolution

barack obama nominated hillary rodham clinton secretary state monday


figure illustrative example
mention pair model
noted mention pair model classifier decides whether
active mention mk coreferent candidate antecedent mj instance mj mk
represents mj mk implementation instance consists features shown
table features largely employed state art learning
coreference systems e g soon ng lim ng cardie b bengtson roth
computed automatically seen features divided four
blocks first two blocks consist features describe properties mj mk
respectively last two blocks features describe relationship mj
mk classification associated training instance positive negative
depending whether mj mk coreferent
one training instance created pair mentions negative instances
would significantly outnumber positives yielding skewed class distribution
typically adverse effect model training subset mention
pairs generated training following soon et al create positive
instance anaphoric mention mk closest antecedent mj negative
instance mk paired intervening mentions mj mj mk
running example shown figure three training instances generated
monday secretary state first two instances
labeled negative last one labeled positive train mention pair
model use svm learning svmlight package joachims
mentioned introduction previous work learning coreference
resolution typically treats underlying machine learner simply black box tool
choose provide reader overview svms learner employing
work note self contained overview means comprehensive
introduction maximum margin learning goal provide reader
details believe needed understand difference classification
ranking perhaps appreciate importance ranking
begin assume given data set consisting positively labeled
points class value negatively labeled points class
since svmlight assumes real valued features cannot operate features multiple discrete values
directly hence need convert features shown table equivalent set features
used directly svmlight uniformity perform conversion feature
table rather multi valued features follows create one binary valued feature
svmlight feature value pair derived feature set table example
pronoun two values n derive two binary valued features pronoun
pronoun n one value value instance
overview theory maximum margin learning refer reader burgess
tutorial



firahman ng

features describing mj candidate antecedent
pronoun
mj pronoun else n
subject
mj subject else n
nested
mj nested np else n
features describing mk mention resolved
number
singular plural determined lexicon
male female neuter unknown determined list
gender
common first names
mk pronoun else n
pronoun
nested
mk nested np else n
semclass
semantic class mk one person location organization date time money percent object others determined wordnet fellbaum stanford ne recognizer finkel grenager manning
animacy
mk determined human animal wordnet ne
recognizer else n
nominative case mk pronoun else na e g
pro type
feature value
features describing relationship mj candidate antecedent mk
mention resolved
head match
c mentions head noun else
str match
c mentions string else
substr match
c one mention substring else
c mentions pronominal string else
pro str match
pn str match
c mentions proper names string else
nonpro str match c two mentions non pronominal
string else
modifier match
c mentions modifiers na one
dont modifier else
c mentions pronominal pronoun
pro type match
different respect case na least one
pronominal else
number
c mentions agree number disagree na
number one mentions cannot determined
gender
c mentions agree gender disagree na gender
one mentions cannot determined
agreement
c mentions agree gender number disagree
number gender else na
animacy
c mentions match animacy dont na
animacy one mentions cannot determined
c mentions pronouns neither pronouns else na
pronouns
proper nounsc mentions proper nouns neither proper nouns
else na
maximalnp
c two mentions maximial np projection else
span
c neither mention spans else
indefinite
c mk indefinite np appositive relationship
else
appositive
c mentions appositive relationship else
copular
c mentions copular construction else



fia cluster ranking coreference resolution

features describing relationship mj candidate antecedent mk
mention resolved continued previous page
semclass
c mentions semantic class set
semantic classes considered enumerated description
semclass feature dont na semantic class
information one mentions cannot determined
alias
c one mention abbreviation acronym else

distance
binned values sentence distance mentions
additional features describing relationship mj candidate antecedent
mk mention resolved
number
concatenation number feature values mj mk
e g mj clinton mk feature value singularplural since mj singular mk plural
gender
concatenation gender feature values mj mk
pronoun
concatenation pronoun feature values mj mk
nested
concatenation nested feature values mj mk
semclass
concatenation semclass feature values mj mk
animacy
concatenation animacy feature values mj mk
concatenation pro type feature values mj mk
pro type

table feature set coreference resolution non relational features describe mention
cases take value yes relational features describe relationship
two mentions indicate whether compatible incompatible
applicable
value used classification mode svm learner aims learn hyperplane
e linear classifier separates positive points negative points
one hyperplane achieves zero training error learner choose
hyperplane maximizes margin separation e distance
hyperplane training example closest larger margin proven
provide better generalization unseen data vapnik formally maximum
margin hyperplane defined w x b x feature vector representing
arbitrary data point w weight vector b scalar parameters
learned solving following constrained optimization
optimization hard margin svm classification
arg min
subject


kwk


yi w xi b

n

yi class th training point xi note data point
xi exactly one linear constraint optimization ensures xi
correctly classified particular value right side inequality


firahman ng

constraint ensures certain distance e margin xi hyperplane
shown margin inversely proportional length weight vector
hence minimizing length weight vector equivalent maximizing margin
resulting svm classifier known hard margin svm margin hard
data point correct side hyperplane
however cases data set linearly separable hyperplane
perfectly separate positives negatives
constrained optimization solution instead asking svm
learner give return solution solve relaxed version
consider hyperplanes produce non zero training errors potential solutions
words modify linear constraints associated data point
training errors allowed however modify linear constraints
leave objective function learner search maximum margin
hyperplane regardless training error produces since training error correlates
positively generalization error crucial objective function take
consideration training error hyperplane large margin low training
error found however non trivial maximize margin minimize
training error simultaneously since training error typically increases maximize
margin need trade two criteria resulting
objective function linear combination margin size training error
formally optimal hyperplane solving following constrained optimization

optimization soft margin svm classification
arg min

x

kwk c




subject
yi w xi b n
yi class th training point xi c regularization
parameter balances training error margin size finally non negative slack
variable represents degree misclassification xi particular
data point wrong side hyperplane svm allows data points
appear wrong side hyperplane known soft margin svm
given optimization rely training employed svmlight
finding optimal hyperplane
training resulting svm classifier used clustering identify
antecedent mention test text specifically active mention compared
turn preceding mention pair test instance created training
presented svm classifier returns value indicates likelihood
two mentions coreferent mention pairs class values considered
coreferent otherwise pair considered coreferent following soon et al
apply closest first linking regime antecedent selection given active mention mk


fia cluster ranking coreference resolution

select antecedent closest preceding mention classified coreferent
mk mk classified coreferent preceding mention considered
non anaphoric e antecedent selected mk
entity mention model
unlike mention pair model entity mention model classifier decides whether
active mention mk belongs partial coreference cluster cj precedes mk
training instance cj mk represents cj mk features instance
divided two types features describe mk e shown second block
table cluster level features describe relationship cj
mk cluster level feature created feature employed mention pair
model applying logical predicate example given number feature e feature
table determines whether two mentions agree number apply
predicate create cluster level feature value yes mk agrees
number mentions cj otherwise motivated previous work luo
et al culotta wick mccallum yang et al create cluster level
features mention pair features four commonly used logical predicates none
false true specifically feature x shown last two
blocks table first convert x equivalent set binary valued features
multi valued resulting binary valued feature xb create four binaryvalued cluster level features none xb true xb false mk
mention cj false xb true xb true mk less half
least one mentions cj true xb true xb true
mk least half mentions cj xb true xb
true mk mention cj hence xb exactly one four
cluster level features evaluates true
following yang et al create positive instance anaphoric mention
mk preceding cluster cj belongs negative instance mk
paired preceding cluster whose last mention appears mk closest
antecedent e last mention cj consider running example three
training instances generated monday secretary state
barack obama first two instances labeled negative
last one labeled positive mention pair model train
entity mention model svm learner
since entity mention model classifier use svmlight classification
mode resulting constrained optimization essentially optimization except training example xi represents active mention
one preceding clusters rather two mentions
note cluster level feature represented probabilistic feature specifically recall
four logical predicates partitions interval predicate evaluates true given
cluster level feature depends probability obtained computation feature instead
applying logical predicates convert probability one four discrete values
simply use probability value cluster level feature however choose employ
probabilistic representation preliminary experiments indicated probabilistic features
yielded slightly worse logical features



firahman ng

training resulting classifier used identify preceding cluster mention
test text specifically mentions processed left right manner
active mention mk test instance created mk preceding clusters
formed far test instances presented classifier finally adopt
closest first clustering regime linking mk closest preceding cluster classified
coreferent mk mk classified coreferent preceding cluster
considered non anaphoric note partial clusters preceding mk formed
incrementally predictions classifier first k mentions
gold standard coreference information used formation
mention ranking model
noted ranking model imposes ranking candidate antecedents
active mention mk train ranking model use svm ranker learning
joachimss svmlight package
mention pair model training instance mj mk represents mk
preceding mention mj fact features represent instance method
creating training instances identical employed mention pair model
difference lies labeling training instances assuming sk set
training instances created anaphoric mention mk rank value mj mk sk
rank mj among competing candidate antecedents mj closest
antecedent mk otherwise exemplify consider running example
mention pair model three training instances generated monday
secretary state third instance rank value
remaining two rank value
first glance seems training set generated learning mentionranking model identical one learning mention pair model instance
represents two mentions labeled one two possible values since previous work
ranking coreference resolution attempt clarify difference
two believe could difficult reader appreciate idea
ranking coreference resolution
let us first describe difference classification ranking high level
beginning training sets employed mention ranking model mentionpair model difference label associated instance training
mention ranking model rank value whereas label associated instance
training mention pair model class value specifically since ranking svm
learns rank set candidate antecedents relative ranks two candidates
rather absolute rank candidate matter training process
words point view ranking svm training set instance
rank value instance rank value functionally equivalent one
rank value rank value assuming remaining
instances generated anaphor two training sets identical
rank value
larger rank value implies better rank svmlight



fia cluster ranking coreference resolution

next take closer look ranker training process denote training set
created described addition assume instance
denoted xjk yjk xjk feature vector created anaphoric mention
mk candidate antecedent mj yjk rank value training ranker
svm ranker learning derives training set original training set
follows specifically every pair training instances xik yik xjk yjk
yik yjk create training instance xijk yijk xijk xik xjk
yijk xik larger rank value xjk otherwise way
creation resembles connolly et al pairwise ranking saw
section convert ranking pairwise classification
goal ranker learning hyperplane minimizes
number misclassifications note since yijk class value
instance depends relative ranks two candidate antecedents
absolute rank values
given conversion ranking pairwise classification
constrained optimization svm ranker learning attempts
solve described similar optimization
optimization soft margin svm ranking
x

ijk
arg min kwk c

subject
yijk w xik xjk b ijk
ijk non negative slack variable represents degree misclassification
xijk c regularization parameter balances training error margin size
two points deserve mention first optimization equivalent one
classification svm pairwise difference feature vectors xik xjk
training used solve optimization applicable
optimization second number linear inequality constraints
generated document optimization training mention pair
model entity mention model quadratic number mentions
number constraints generated ranking svm cubic number mentions
since instance represents three rather two mentions
training mention ranking model applied rank candidate antecedents
active mention test text follows given active mention mk follow denis
baldridge use independently trained classifier determine whether mk
non anaphoric mk resolved otherwise create test instances mk
pairing preceding mentions test instances presented
ranker computes rank value instance taking dot product
main difference training set employed connolly et al
instance formed taking difference feature vectors two instances whereas
connolly et al training set instance formed concatenating feature vectors two
instances



firahman ng

instance vector weight vector preceding mention assigned largest
value ranker selected antecedent mk ties broken preferring
antecedent closest distance mk
anaphoricity classifier used resolution step trained publicly available
implementation maximum entropy maxent modeling instance corresponds
mention represented features deemed useful distinguishing
anaphoric non anaphoric mentions see table details linguistically
features broadly divided three types string matching grammatical
semantic relational feature compares mention one
preceding mentions non relational feature encodes certain linguistic property
mention whose anaphoricity determined e g np type number definiteness

coreference cluster ranking
section describe cluster ranking np coreference noted
aims combine strengths entity mention model
mention ranking model
training applying cluster ranker
ease exposition describe subsection train apply clusterranking model used pipeline architecture anaphoricity determination
performed prior coreference resolution next subsection
two tasks learned jointly
recall cluster ranking model ranks set preceding clusters active
mention mk since cluster ranking model hybrid mention ranking model
entity mention model way trained applied hybrid
two particular instance representation employed cluster ranking model
identical used entity mention model training instance cj mk
represents preceding cluster cj anaphoric mention mk consists clusterlevel features formed predicates unlike entity mention model however
cluster ranking model training instance created anaphoric mention mk
preceding clusters since training model ranking clusters
assignment rank values training instances similar mention ranking
model specifically rank value training instance cj mk created mk
rank cj among competing clusters mk belongs cj otherwise
train cluster ranking model use svm learner ranking mode resulting
constrained optimization essentially optimization
except training example xijk represents active mention mk two
preceding clusters ci cj rather two preceding mentions
applying learned cluster ranker test text similar applying mentionranking model specifically mentions processed left right manner
active mention mk first apply independently trained classifier determine mk
non anaphoric mk resolved otherwise create test instances mk
see http homepages inf ed ac uk maxent toolkit html



fia cluster ranking coreference resolution

feature type
lexical

feature
str match

head match

grammatical
np type

uppercase
definite
demonstrative
indefinite
quantified
article

grammatical
np
property
relationship

pronoun
proper noun
bare singular
bare plural
embedded
appositive
prednom
number

contains pn
grammatical
syntactic
pattern

n
n
pn
pn n
adj n
num n
ne
sing n

semantic

alias

description
exists mention mj preceding mk
discarding determiners mj mk string else
n
exists mention mj preceding mk mj
mk head else n
mk entirely uppercase else n
mk starts else n
mk starts demonstrative
else n
mk starts else n
mk starts quantifiers every
many much none else n
definite mk definite np quantified mk quantified np else indefinite
mk pronoun else n
mk proper noun else n
mk singular start article else n
mk plural start article else n
mk prenominal modifier else n
mk first two mentions appositive
construction else n
mk first two mentions predicate nominal
construction else n
singular mk singular number plural mk plural
number unknown number information cannot
determined
mk proper noun contains proper noun else
n
mk starts followed exactly one common
noun else n
mk starts followed exactly two common
nouns else n
mk starts followed exactly proper noun
else n
mk starts followed exactly proper noun
common noun else n
mk starts followed exactly adjective
common noun else n
mk starts followed exactly cardinal
common noun else n
mk starts followed exactly named entity
else n
mk starts followed singular np containing proper noun else n
exists mention mj preceding mk mj
mk aliases else n

table feature set anaphoricity determination instance represents single mention
mk characterized features


firahman ng

pairing preceding clusters test instances presented
ranker mk linked cluster assigned highest value ranker ties
broken preferring cluster whose last mention closest distance mk note
partial clusters preceding mk formed incrementally predictions
ranker first k mentions
joint anaphoricity determination coreference resolution
cluster ranker described used determine preceding cluster
anaphoric mention linked cannot used determine whether mention anaphoric reason simple training instances generated
anaphoric mentions hence jointly learn anaphoricity determination coreference
resolution must train ranker instances generated anaphoric
non anaphoric mentions
specifically training ranker provide active mention option
start cluster creating additional instance contains features solely
describe active mention e features shown second block table
highest rank value among competing clusters e non anaphoric
lowest rank value e otherwise main advantage jointly learning two tasks
allows ranking model evaluate possible options active mention
e whether resolve preceding cluster best simultaneously
essentially method applied jointly learn two tasks mentionranking model
training resulting cluster ranker processes mentions test text
left right manner active mention mk create test instances pairing
preceding clusters allow possibility mk non anaphoric
create additional test instance contains features solely describe active
mention similar training step test instances
presented ranker additional test instance assigned highest rank value
ranker mk classified non anaphoric resolved otherwise
mk linked cluster highest rank ties broken preferring
antecedent closest mk partial clusters preceding mk formed
incrementally predictions ranker first k mentions
finally note model jointly learning anaphoricity determination coreference resolution different recent attempts perform joint inference anaphoricity
determination coreference resolution integer linear programming ilp
anaphoricity classifier coreference classifier trained independently
ilp applied postprocessing step jointly infer anaphoricity coreference decisions consistent e g denis baldridge
joint inference different joint learning allows two tasks
learned jointly independently

lexicalization coreference resolution
next investigate role lexicalization e use word pairs linguistic features
learning coreference resolution motivation behind investigation two

fia cluster ranking coreference resolution

fold first lexical features easy compute yet investigated
coreference resolution particular attempts made employ
train mention pair model e g luo et al daume iii marcu
bengtson roth contrast want determine whether improve
performance cluster ranking model second mention pair model
mention ranking model compared respect non lexical feature set
denis baldridge b clear perform relative
trained lexical features desire answer question
allow us gain additional insights strengths weaknesses
learning coreference
recall introduction previous attempts lexicalizing mention pair
model lexical features best marginally useful hence one goals
determine whether make better use lexical features learning
coreference resolver particular unlike aforementioned attempts lexicalization
simply append word pairs conventional coreference feature set consisting
string matching grammatical semantic distance e proximity features e g
feature set shown table investigate model exploits lexical features
combination small subset conventional coreference features
would allow us better understanding significance conventional
features example features encode agreement gender number semantic
class two mentions employed virtually learning coreference resolver
never question whether better alternatives features could
build lexicalized coreference model without commonly used features
observe performance deterioration would imply conventional features
replaceable prototypical way building learning coreference
system
question small subset conventional features use
combination lexical features mentioned since one advantages
lexical features extremely easy compute desire conventional
features easy compute especially require dictionary
compute see choose use two features alias feature
distance feature see features table rely shelf named
entity ne recognizer compute ne types
note however usefulness lexical features could limited part data
sparseness many word pairs appear training data may appear test
data employing conventional features described e g distance
help alleviate seek improve generalizability introducing
two types features semi lexical unseen features henceforth refer
feature set comprises two types features lexical features alias feature
distance feature lexical feature set addition refer feature
set shown table conventional feature set
first describe lexical feature set training mention pair model
mention ranking model section create cluster level
features feature set training entity mention model cluster ranking


firahman ng

model well issues training joint model anaphoricity determination coreference resolution section
lexical feature set
unlike previous work lexicalizing learning coreference lexical feature
set consists four types features lexical features semi lexical features unseen features
well two conventional features namely alias distance
compute features preprocess training text randomly replacing
nominal mentions e common nouns label unseen mention mk
replaced unseen mentions string mk replaced
unseen test text preprocessed differently simply replace mentions whose
strings seen training data unseen hence artificially creating unseen
labels training text allow learner learn handle unseen words
test text potentially improving generalizability
preprocessing compute features instance assuming
training mention pair model mention ranking model instance corresponds
two mentions mj mk mj precedes mk text features
divided four groups unseen lexical semi lexical conventional describing
features two points deserve mention first least one mj mk unseen
lexical semi lexical conventional features created since features
involving unseen mention likely misleading learner sense
may yield incorrect generalizations training set second since use svm
training testing instance contain number features unless otherwise
stated feature value
unseen feature mj mk unseen determine whether
string create unseen feature otherwise create unseendiff feature one unseen feature created
lexical feature create lexical feature mj mk ordered
pair consisting heads mentions pronoun common noun head
assumed last word mention proper noun head taken
entire noun phrase
semi lexical features features aim improve generalizability specifically
exactly one mj mk tagged ne stanford ne recognizer finkel et al
create semi lexical feature identical lexical feature described
except ne replaced ne label e person location organization
mentions nes check whether string create
feature ne ne replaced corresponding ne label otherwise
check whether ne tag word subset match e whether
see evaluation section mention extractor trained extract base nps hence
heuristic extracting head nouns arguably overly simplistic applied
recursive nps e g nps contain prepositional phrases phrases likely
make mistakes however desire better extraction accuracy extract head nouns
syntactic parsers provide head information collinss parser



fia cluster ranking coreference resolution

word tokens one mention appear others list tokens create feature
ne subsame ne replaced ne label otherwise create feature
concatenation ne labels two mentions
conventional features improve generalizability incorporate two easy
compute features conventional feature set alias distance
feature generation
lexical feature set training mention pair model mentionranking model describe two extensions feature set needed
train entity mention model cluster ranking model perform joint
learning anaphoricity determination coreference resolution
first extension concerns generation cluster level features entity mention
model cluster level model recall section create cluster level features given conventional feature set first convert feature employed
mention pair model equivalent set binary valued features create
cluster level feature resulting binary valued features hand
given lexical feature set method producing cluster level features applicable two conventional features e alias distance appear
conventional feature set unseen lexical semi lexical feature create
feature active mention mention preceding cluster described
section value feature number times appears instance encoding feature values frequency rather binary values allows us capture
cluster level information shallow manner
second extension concerns generation features representing additional
instance created training joint version mention ranking model
cluster ranking model recall section conventional feature set
used represented additional instance features computed solely
active mention hand given lexical feature set longer
use method representing additional instance feature
lexical feature set computed solely active mention
represent additional instance one feature null x x head
active mention help learner learn x likely non anaphoric

evaluation
evaluation driven following questions focusing comparison among
different learning coreference effect lexicalization
specifically
learning coreference namely mention pair model
entity mention model mention ranking model cluster ranking model
compare
strictly speaking resulting feature cluster level feature computed active
mention one mentions preceding cluster



firahman ng

joint modeling anaphoricity determination coreference resolution offer
benefits pipeline architecture anaphoricity performed prior
coreference resolution
lexicalized coreference perform better unlexicalized counterparts
rest section first describe experimental setup section
performance four including effect lexicalization
joint modeling whenever applicable three different feature sets section
experimental setup
begin providing details data sets automatic mention extraction
method scoring programs
corpus
use ace coreference corpus released ldc consists
training documents used official ace evaluation ensure diversity corpus
created selecting documents six different sources broadcast news bn broadcast
conversations bc newswire nw webblog wb usenet un conversational
telephone speech cts number documents belonging source shown
table
data set
documents

bn


bc


nw


wl


un


cts


table statistics ace corpus

mention extraction
evaluate coreference model system mentions extract system mentions
test text trained mention extractor training texts following florian
et al recast mention extraction sequence labeling task assign
token test text label indicates whether begins mention inside
mention outside mention hence learn extractor create one training
instance token training text derive class value one b
annotated data instance represents wi token consideration
consists linguistic features many modeled systems bikel
schwartz weischedel florian et al described
lexical

tokens window wi wi

capitalization determine whether wi isallcap isinitcap iscapperiod
isalllower
since participate ace access official test set



fia cluster ranking coreference resolution

morphological wi prefixes suffixes length one two three four
grammatical part speech pos tag wi obtained stanford loglinear pos tagger toutanova klein manning singer
semantic named entity ne tag wi obtained stanford crf
ne recognizer finkel et al
dictionaries employ eight dictionary features indicate presence
absence wi particular dictionary eight dictionaries contain pronouns
entries common words words names k person names k
person titles honorifics vehicle words location names k company
names k nouns extracted wordnet hyponyms person k
employ crf c implementation conditional random fields training
mention detector training set overall detector achieves f measure
recall precision test set extracted mentions used
system mentions coreference experiments
scoring programs
score output coreference model employ two scoring programs b bagga
baldwin ceaf luo address inherent weaknesses
muc scoring program vilain burger aberdeen connolly hirschman
b ceaf score response e system generated partition r key
e gold standard partition k report coreference performance terms recall
precision f measure b first computes recall precision mention mk
follows
recall mk

rmk kmk
rmk kmk
precision mk

kmk
rmk

rmk coreference cluster containing mk r kmk coreference cluster
containing mk k computes overall recall resp precision averaging
per mention recall resp precision scores
hand ceaf first constructs optimal one one mapping
clusters key partition response partition specifically assume
k k k km set clusters key partition r r r rn
set clusters response partition compute recall ceaf first computes
score cluster ki k follows
score ki ki rj
available http crfpp sourceforge net
ceaf two versions ceaf ceaf two versions differ similarity two
aligned clusters computed refer reader luos details ceaf chosen
commonly used version ceaf
briefly muc scoring program suffers two often cited weaknesses first link measure
reward successful identification singleton clusters since mentions clusters
linked mentions second tends penalize partitions overly large clusters
see work bagga baldwin luo recasens hovy details



firahman ng

rj cluster ki mapped optimal one one mapping
constructed efficiently kuhn munkres kuhn note
ki mapped cluster r score ki ceaf computes recall
summing score cluster k dividing sum number mentions
k precision computed manner except reverse roles
k r
complication arises b used score response partition containing system
mentions recall b constructs mapping mentions response
key hence response generated gold standard mentions
every mention response mapped mention key vice versa
words twinless e unmapped mentions stoyanov et al
case system mentions used original description b
specify twinless mentions scored bagga baldwin address
set per mention recall precision twinless mention zero
regardless whether mention appears key response note ceaf
compare partitions twinless mentions without modification since operates
aligning clusters mentions
additionally apply preprocessing step response partition scoring
remove twinless system mentions singletons reason
simple since coreference resolver successfully identified mentions singletons
penalized removing allows us avoid penalty note
remove twinless opposed system mentions singletons allows
us reward resolver successful identification singleton mentions twins
hand retain twinless system mentions non singletons
resolver penalized identifying spurious coreference relations twinless
mentions key partition want ensure resolver makes correct
coreference non coreference decisions

showing learning coreference let us consider head
match baseline commonly used heuristic baseline coreference resolution
posits two mentions coreferent head nouns match head nouns
determined described section head proper noun string entire
mention whereas head pronoun common noun last word mention
since one goals examine effect lexicalization coreference model
head match baseline provide information well one simplest
kinds string matching baseline shown row table expressed
terms recall r precision p f measure f obtained via b ceaf
see table baseline achieves f measure scores according
b ceaf respectively
addition method described number methods proposed address
mapping refer reader work enrique gonzalo artiles verdejo
stoyanov et al cai strube details



fia cluster ranking coreference resolution

next train evaluate learning coreference five fold cross
validation data set si shown table partition documents si
five folds approximately equal size si si train coreference model
four folds use generate coreference chains documents remaining
fold repeating step five times fold used test fold exactly
apply b ceaf entire set automatically coreference annotated
documents obtain scores table discuss learningbased coreference obtained used combination three feature sets
conventional feature set section lexical feature set section
combined feature set composed features conventional lexical
section
conventional features
gauge performance cluster ranking model employ baselines mentionpair model entity mention model mention ranking model
mention pair baseline train first learning baseline mentionpair model svm learning implemented svmlight package
see row table mention pair model achieves f measure scores
b ceaf represent statistically significant improvement
f measure corresponding head match baseline
entity mention baseline next train second learning baseline
entity mention model svm learner see row table
baseline achieves f measure scores b ceaf represent small
statistically significant improvements mention pair model significant performance difference perhaps particularly surprising given improved expressiveness
entity mention model mention pair model
mention ranking baseline third baseline mention ranking model
trained ranker learning svmlight identify non anaphoric mentions employ two methods first method follow denis baldridge
adopt pipeline architecture train maxent classifier anaphoricity determination independently mention ranker training set features
described section apply resulting classifier test text filter nonanaphoric mentions prior coreference resolution pipeline mention ranker
shown row table see ranker achieves f measure scores
b ceaf yielding significant performance deterioration comparison
entity mention baseline
second method perform anaphoricity determination jointly coreference
resolution method described section discussed joint learning
method context cluster ranking easy see method
equally applicable mention ranking model mention ranker
subsequent uses svm learner set parameters default values
particular employ linear kernel obtain article
statistical significance article obtained paired test p



firahman ng

r


b
p


ceaf
p
f




coreference model
head match

f


r









conventional feature set
mention pair model

entity mention model

mention ranking model pipeline

mention ranking model joint

cluster ranking model pipeline

cluster ranking model joint






























lexical feature set
mention pair model

entity mention model

mention ranking model pipeline

mention ranking model joint

cluster ranking model pipeline

cluster ranking model joint






























combined
mention pair model

entity mention model

mention ranking model pipeline

mention ranking model joint

cluster ranking model pipeline

cluster ranking model joint























feature







set







table five fold cross validation coreference obtained b ceaf
best f measure achieved feature set scoring program combination boldfaced

joint architecture shown row table see ranker achieves fmeasure scores b ceaf represent significant improvements
entity mention model pipeline counterpart
demonstrate superiority joint mention ranking model entity mention model
substantiate hypothesis joint modeling offers benefits pipeline modeling
cluster ranking model finally evaluate cluster ranking model
mention ranking baselines employ pipeline architecture joint architecture anaphoricity determination shown rows table
respectively two architectures see pipeline architecture yields fmeasure scores b ceaf represent significant improvement
mention ranker adopting pipeline architecture joint architecture
cluster ranker achieves f measure scores b ceaf rep

fia cluster ranking coreference resolution

resents significant improvement mention ranker adopting joint architecture
best baselines taken together demonstrate superiority
cluster ranker mention ranker finally fact joint cluster ranker performs
significantly better pipeline counterpart provides empirical support
benefits joint modeling pipeline modeling
lexical features
next evaluate learning coreference lexical features
shown rows table comparison obtained conventional features see different trend joint mention ranking model replaces
cluster ranking model best performing model moreover improvement
second best performing model entity mention model according b
pipeline mention ranking model according ceaf statistically significant regardless
scoring program used closer examination reveals employing
lexical rather conventional features substantially improves performance
mention ranking model comparison unlexicalized joint mention ranking model
row f measure scores lexicalized joint mention ranking model row rise
b ceaf increase f measure attributed primarily
substantial rise recall even though large increase ceaf precision
besides joint mention ranking model mention pair model entity mention
model benefit substantially conventional features replaced lexical features see f measure scores increase b ceaf
mention pair model b ceaf entity mention model
gains f measure two attributed large increases
recall precision hand joint cluster ranking model
improve replace conventional features lexical features fact
performance difference cluster ranking model entity mention model
statistically indistinguishable finally see benefits jointly learning anaphoricity
determination coreference resolution joint version mentionranking model used rather pipeline version compare rows
f measure scores rise significantly b ceaf similarly clusterranking model joint version improves pipeline version significantly b
ceaf f measure
overall somewhat unexpected recall lexical features
knowledge lean consisting lexical semi lexical unseen features well
two conventional features particular employ conventional coreference
features encode agreement gender number implies many existing
implementations mention pair model entity mention model mentionranking model unlexicalized rely heavily conventional features
making effective use labeled data perhaps importantly indicate
coreference perform well fact better even without conventional
coreference features since lexical computed extremely easily
readily applied languages another advantage feature set
hand interesting see versions cluster ranking model exhibit


firahman ng

less dramatic changes performance replace conventional features
lexical features
combined features
since conventional features lexical features represent two fairly different sources
knowledge examine whether improve coreference combining
two feature sets coreference combined features
shown rows table exhibit essentially trend
obtained conventional features joint cluster ranking model performing
best mention pair model performing worst fact joint cluster ranking
model yields significantly better performance used combined features
conventional features lexical features alone similarly pipeline
cluster ranking model achieves significantly better performance combined
features conventional lexical features seem suggest
cluster ranking model able exploit potentially different sources information
provided two feature sets improve performance addition demonstrate
benefits joint modeling mention ranking model joint version improves
pipeline version significantly b ceaf f measure
cluster ranking model joint version improves pipeline counterpart significantly
b ceaf f measure
remaining coreference exhibit drop performance combined
features used lieu lexical features seem suggest
cluster ranking model offers robust performance face changes underlying feature set coreference feature selection issue
explored coreference resolution may crucial employ coreference perhaps importantly despite fact conventional features
lexical features represent two fairly different sources information
cluster ranking model unable exploit potentially richer amount information
contained combined feature set hence virtually linguistic features
recently developed supervised coreference resolution evaluated
mention pair model see example work strube rapp muller ji
westbrook grishman ponzetto strube utility features may
better demonstrated cluster ranking model
natural question joint cluster ranking model compare existing
coreference systems since participate ace evaluations
access official test sets compare model ace
participating coreference systems comparison complicated fact
existing coreference systems evaluated different data sets including two
muc data sets muc muc ace data sets e g ace
ace ace ace well different partitions given data set
knowledge coreference model evaluated test
data haghighi kleins unsupervised coreference model model
fact ng cardie b strube muller ponzetto strube
mention pair model improved feature selection



fia cluster ranking coreference resolution

recently shown surpass performance stoyanov et al system
one best existing implementations mention pair model test
data haghighi kleins model achieves b f measure achieves
b f measure provide suggestive evidence cluster ranking
model achieves performance comparable one best existing coreference

nevertheless caution allow one claim anything
fact model compares favorably haghighi kleins model
instance one cannot claim model better achieves level
performance without labeled data reasons mentions
used two coreference process extracted differently
linguistic features employed two way features computed
different since previous work shown linguistic
preprocessing steps considerable impact performance resolver barbu
mitkov stoyanov et al possible one model employed features
mentions model currently would different
hence one fairly compare two coreference evaluated
set mentions rather set documents given access
set knowledge sources essentially way compare
learning coreference article

experimental analyses
attempt gain insights different aspects coreference
conduct additional experiments analyses rather report five fold cross validation
section report one fold e fold designate test
set use remaining four folds solely training
improving classification coreference
given generally poorer performance classification coreference natural question improved answer question investigate whether
improved employing different clustering different
learning reasons decision focus two dimensions
first noted introduction one weaknesses
clear clustering offers best performance given observation
examine whether improve replacing soon et al
closest first linking regime best first linking strategy shown
offer better performance mention pair model muc data sets ng cardie
b second discussed end section may able achieve
advantage ranking classification employing learning
optimizes conditional probabilities instead decisions motivated observation examine whether improve classification training
maxent employs likelihood loss function note maxent one
note haghighi klein report ceaf scores



firahman ng

popular learning training coreference see example
morton kehler appelt taylor simma ponzetto strube denis
baldridge finkel manning ng
evaluate two modifications apply isolation combination
two classification e mention pair model entity mention
model trained three different feature sets e conventional lexical combined train maxent coreference yasmet
follow ng cardies b implementation best first clustering
specifically among candidate antecedents preceding clusters classified
coreferent active mention mk best first clustering links mk likely one
maxent model pair classified coreferent classification value
likely antecedent preceding cluster mk one
highest probability coreference mk svmlight trained model pair
classified coreferent classification value likely
antecedent preceding cluster mk one positive classification
value
table presents b ceaf two classification coreference
trained two learning e svm maxent used
combination two clustering e closest first clustering best first
clustering study choice clustering impacts performance
compare closest first clustering best first clustering table
combination learning feature set coreference model scoring
program instance comparing rows table enables us examine
two clustering better mention pair model trained
conventional feature set two learners overall see fairly consistent
trend best first clustering yields slightly worse obtained
closest first clustering regardless choice clustering learning
feature set scoring program first glance seem
contradictory ng cardie b demonstrate superiority bestfirst clustering closest first clustering coreference resolution speculate
contradictory attributed two reasons first best first clustering
experiments still employed soon et al training instance selection method
created positive training instance anaphoric mention closest
antecedent preceding cluster unlike ng cardie claim proposed bestfirst clustering successful however different method training instance selection
would needed particular propose use confident antecedent
rather closest antecedent generate positive instances anaphoric mention
second ng cardie demonstrate success best first clustering muc data
sets possible success may carry ace data sets additional
experiments needed determine reason however
see http www fjoch com yasmet html reason yasmet chosen provides
capability rank allows us compare maxent trained classification
ranking see work ravichandran hovy och discussion differences
training two types maxent



fia cluster ranking coreference resolution

coreference model

r

svm
p

f

r

maxent
p
f






b conventional feature
mention pair model closest first

mention pair model best first

entity mention model closest first
entity mention model best first


set




















b lexical feature set
mention pair model closest first

mention pair model best first

entity mention model closest first
entity mention model best first

















combined feature set























ceaf conventional feature set
mention pair model closest first


mention pair model best first


entity mention model closest first

entity mention model best first


















ceaf
mention pair model closest first
mention pair model best first
entity mention model closest first
entity mention model best first
















ceaf combined
mention pair model closest first

mention pair model best first

entity mention model closest first
entity mention model best first

















b
mention pair model closest first
mention pair model best first
entity mention model closest first
entity mention model best first

b

lexical feature set







feature





set





b ceaf

table svm vs maxent classification coreference one fold
b ceaf scores obtained training coreference svm maxent
best f measure achieved feature set scoring program combination boldfaced


firahman ng

next examine whether minimizing likelihood loss via maxent training instead
svms classification loss would enable us achieve advantage ranking
hence leads better performance compare two columns table
see conventional feature set used maxent outperforms svm regardless
choice clustering scoring program coreference model
hand lexical features combined features used svm outperforms
maxent consistently overall mixed seem suggest whether maxent
offers better performance svm extent dependent underlying feature
set
performance maximum entropy ranking
prior work suggests maxent ranking may provide better gains svmbased ranking since generate reliable confidence values dynamically adjust
relative ranks according baseline e g ji rudin grishman determine whether case coreference resolution conduct experiments
train ranking coreference ranker learning yasmet
b ceaf mention ranking model cluster ranking model
trained maxent combination three different feature sets e conventional
lexical combined shown maxent column table comparison
corresponding obtained via svm ranking table
see svm column comparing two columns see mixed
experiments involve ranking maxent ranking outperforms svm
ranking six words suggest coreference task
svm ranking generally better maxent ranking
accuracy anaphoricity determination
section saw joint ranking model performs significantly better
pipeline counterpart words joint modeling coreference anaphoricity
improves coreference resolution natural question joint modeling improve
anaphoricity determination
answer question measure accuracy anaphoricity information resulting pipeline modeling joint modeling recall pipeline modeling rely
output anaphoricity classifier trained independently coreference
system uses anaphoricity information see section accuracy classifier test set shown acc column row table addition
table recall r precision p f measure f identifying anaphoric
mentions see classifier achieves accuracy f measure score

hand joint modeling compute accuracy anaphoricity
determination output joint coreference model specifically given output
joint model determine mentions resolved preceding antecedent
assuming mention resolved anaphoric one
resolved non anaphoric compute accuracy anaphoricity determination


fia cluster ranking coreference resolution

coreference model

r

svm
p

f

r

maxent
p
f






b conventional feature
mention ranking model pipeline

mention ranking model joint

cluster ranking model pipeline

cluster ranking model joint


set




















b lexical feature set
mention ranking model pipeline

mention ranking model joint

cluster ranking model pipeline

cluster ranking model joint

















combined feature set


















conventional feature set























b
mention ranking model pipeline
mention ranking model joint
cluster ranking model pipeline
cluster ranking model joint

b






ceaf
mention ranking model pipeline
mention ranking model joint
cluster ranking model pipeline
cluster ranking model joint






ceaf
mention ranking model pipeline
mention ranking model joint
cluster ranking model pipeline
cluster ranking model joint

lexical feature set























ceaf
mention ranking model pipeline
mention ranking model joint
cluster ranking model pipeline
cluster ranking model joint

combined feature set




















b ceaf

table svm vs maxent ranking basd coreference one fold b
ceaf scores obtained training coreference svm maxent best
f measure achieved feature set scoring program combination boldfaced


firahman ng









source anaphoricity information
anaphoricity classifier
mention ranking conventional
cluster ranking conventional
mention ranking lexical
cluster ranking lexical
mention ranking combined
cluster ranking combined

acc








r








p








f








table anaphoricity determination

well precision recall f measure identifying anaphoric mentions since
performance numbers derived output joint model compute
two joint ranking e mention ranking model
cluster ranking model used combination three coreference feature
sets e conventional lexical combined six sets performance
numbers shown rows table see accuracies range
f measure scores range
comparison anaphoricity classifier shown row see
joint modeling improves performance anaphoricity determination except two
cases namely mention ranking conventional mention ranking combined
words two cases joint modeling benefits coreference resolution anaphoricity determination seems counter intuitive one achieve better coreference
performance lower accuracy determining anaphoricity difficult
see reason joint model trained maximize pairwise ranking accuracy
presumably correlates coreference performance whereas anaphoricity classifier trained maximize accuracy determining anaphoricity mention
may correlation coreference performance words
improvements anaphoricity accuracy generally necessarily imply corresponding
improvements clustering level coreference accuracy
finally important bear mind conclusions drawn regarding
pipeline joint modeling anaphoricity classifier trained
features possible different conclusions could drawn trained
anaphoricity classifier different set features therefore interesting future direction
would improve anaphoricity classifier employing additional features
proposed uryupina may able derive sophisticated features
harnessing recent advances lexical semantics specifically methods
phrase clustering e g lin wu lexical chain discovery e g morris hirst
paraphrase discovery see survey papers androutsopoulos malakasiotis
madnani dorr


fia cluster ranking coreference resolution

joint inference versus joint learning mention pair model
mentioned end section joint modeling anaphoricity determination
coreference resolution fundamentally different joint inference two tasks
recall joint inference ilp anaphoricity classifier coreference classifier
trained independently ilp applied postprocessing step
jointly infer anaphoricity coreference decisions consistent
e g denis baldridge subsection investigate joint learning
compares joint inference anaphoricity determination coreference resolution
let us begin overview ilp proposed denis baldridge
joint inference anaphoricity determination coreference resolution
ilp motivated observation output anaphoricity model
coreference model given document satisfy certain constraints
instance coreference model determines mention mk coreferent
mentions associated text anaphoricity model determine
mk non anaphoric practice however since two trained independently
constraints cannot enforced
denis baldridge provide ilp framework jointly determining anaphoricity coreference decisions given set mentions probabilities provided
anaphoricity model pa mention pair coreference model pc
resulting joint decisions satisfy desired constraints respecting much possible probabilistic decisions made independently trained pa pc specifically ilp program composed objective function optimized subject
set linear constraints created test text follows let
set mentions p set mention pairs formed e p
mj mk mj mk j k ilp program set indicator variables
case one binary valued variable anaphoricity decision coreference
decision made ilp solver following denis baldridges notation use
yk denote anaphoricity decision mention mk xhj ki denote coreference
decision involving mentions mj mk addition variable associated
assignment cost specifically let cc
hj ki log pc mj mk cost setting xhj ki
c
chj ki log pc mj mk complementary cost setting xhj ki
similarly define cost associated yk letting ca
k log pa mk


log

p




complementary
cost setting
cost setting yk ca

k
k
yk given costs aim optimize following objective function
min

x

c
cc
hj ki xhj ki chj ki xhj ki

x


ca
k yk ck yk

mk

mj mk p

subject set manually specified linear constraints denis baldridge specify four
types constraints indicator variable take value mj
mk coreferent xhj ki mk anaphoric yk mk anaphoric yk
must coreferent preceding mention mj mk non anaphoric
cannot coreferent mention
two points deserve mention first minimizing objective function since
assignment cost expressed negative logarithm value second since transitivity


firahman ng

guaranteed constraints use closest link clustering
put two mentions posited coreferent cluster note
best link clustering strategy applicable since binary decision assigned
pair mentions ilp solver use lp solve publicly available ilp solver
solve program
b ceaf performing joint inference outputs anaphoricity
model mention pair model ilp shown joint inference column
tables b respectively rows correspond obtained training
coreference different feature sets since one goals compare joint
inference joint learning joint learning column
joint mention ranking model anaphoricity determination coreference resolution
learned joint fashion note reason mention ranking model
rather cluster ranking model joint model want ensure
fair comparison joint learning joint inference much possible chosen
cluster ranking model joint model difference joint learning
joint inference could caused increased expressiveness
cluster ranking model finally better understand whether mention pair model
benefits joint inference ilp inference column relevant
mention pair model table output model postprocessed
inference mechanism
table see joint learning substantially better
joint inference except one case conventional ceaf two achieve
comparable performance previous work roth roth yih
suggested often effective learn simple local use complicated
integration strategies make sure constraints output satisfied learn
satisfy constraints directly imply true
coreference task
comparing joint inference inference table see
mention pair model benefit application ilp fact performance
deteriorates ilp used inconsistent reported denis
baldridge joint inference ilp improve mentionpair model speculate inconsistency accures fact denis
baldridge evaluate ilp true mentions e gold standard mentions
evaluate system mentions additional experiments needed determine
reason however
data source adaptability
one may argue since train test model documents data
source e model trained documents bc tested documents
finkel manning formulate linear constraints ilp solver outputs
coreference decisions satisfy transitivity however since number additional constraints needed
guarantee transitivity grows cubically number mentions previous work shows
additional constraints yield substantial performance improvements applied
system mentions ng decided employ experiments
available http lpsolve sourceforge net



fia cluster ranking coreference resolution





feature set
conventional
lexical
combined

joint learning
r
p
f




joint inference
r
p
f





r




inference
p
f





r




inference
p
f




b





feature set
conventional
lexical
combined

joint learning
r
p
f




joint inference
r
p
f




b ceaf

table joint learning vs joint inference joint modeling obtained
mention ranking model joint inference obtained applying ilp
anaphoricity classifier mention pair model inference produced
mention pair model coreference trained maxent

bc example surprising lexicalization helps since word pairs
training set likely found test set training test texts
data source examine whether employ lexical features
suffer trained tested different data sources perform set data
source adaptability experiments apply coreference model trained
lexical features documents one data source documents data sources
obtained mention ranking model primarily
yielded best performance lexical features among learning coreference
comparison data source adaptability obtained
mention ranking model trained non lexical conventional feature set
b ceaf f measure scores experiments shown tables
b left half right half table contain lexicalized mention ranking
model unlexicalized mention ranking model respectively row
corresponds data source model trained except last two rows
explain shortly column corresponds test set particular data
source
answer question whether performance coreference model employs
lexical features deteriorate trained tested different data sources
look diagonal entries left half tables b contain
obtained lexicalized mention ranking model trained tested
documents source model indeed performs worse trained
tested documents different sources diagonal entry contain
highest score among entries column see left half
two tables large extent correct four six diagonal entries contain
highest scores respective columns according scoring programs provides


firahman ng

lexical features
pp
pp test
train ppp
p
bc
bn
cts
nw
un
wl
maxmin
std dev

conventional features

bc

bn

cts nw un

wl

bc

bn

cts nw un

wl













































































































b

lexical features
pp
pp test
train ppp
p
bc
bn
cts
nw
un
wl
maxmin
std dev

conventional features

bc

bn

cts nw un

wl

bc

bn

cts nw un

wl













































































































b ceaf

table data source adaptability row shows obtained training
mention ranking model data set shown first column row column
corresponds test set particular data source best obtained test set
two coreference boldfaced

suggestive evidence answer question affirmative nevertheless look
right half two tables obtained unlexicalized
mention ranking model see similar perhaps weaker trend according ceaf
four six diagonal entries contain highest scores respective columns
according b two six diagonal entries exhibit trend hence fact
model performs worse trained tested different data sources cannot
attributed solely lexicalization
perhaps informative question lexicalized trained different data
sources exhibit varied performance given test set composed documents
source unlexicalized trained different data sources affirmative
answer question provide empirical support hypothesis lexicalized
model fits data trained unlexicalized counterpart answer
question compute column two difference
highest lowest scores see maxmin row standard
deviation six scores corresponding column see std dev row


fia cluster ranking coreference resolution

compare corresponding columns two coreference see except
bn lexicalized model exhibit varied performance given test set
unlexicalized model according scoring programs regardless whether
measuring variation maxmin standard deviation
feature analysis
subsection analyze effects linguistic features performance
coreference given large number trained three
feature sets feasible us analyze features model feature
set since cluster ranking model used combined feature set yields
best performance analyze features addition since lexical features
yielded good performance mention ranking model would informative see
lexical features greatest contribution performance
perform feature analysis two model feature set combinations
although identified two particular model feature set combinations actually
total model feature set combinations recall except row row
table shows aggregated six data sets trained one model
data set words two combinations selected
six learned reduce number need analyze yet maximize
insights gain choose analyze trained data sets
two fairly different domains newswire nw broadcast news bn
next question analyze features apply backward elimination feature selection see survey blum langley
starts full feature set removes iteration feature whose removal
yields best system performance despite greedy nature runs time
quadratic number features making computationally expensive run
feature sets reduce computational cost divide features feature types
apply backward elimination eliminate one feature type per iteration
features grouped follows lexical feature set divide features
five types unseen features lexical features semi lexical features distance alias words division corresponds roughly one described
section except put two conventional features two different groups
since linguistically one positional feature semantic feature
combined feature set divide features seven groups first four
identical division lexical features remaining features
divide string matching features comprise features table
grammatical features comprise features
semantic features comprise features note alias semantic feature lexical feature set combined semantic features
conventional feature set form semantic feature type
shown tables specifically tables b b
ceaf f measure scores feature analysis experiments involving mention ranking
model lexical feature set nw data set table first row shows
system would perform class features removed remove least



















ee
n
u
ns


lia



ist

ce

l
ex
ic
al
se


le
xi
ca
l

rahman ng




















ee
n
u
ns


lia



ist

ce

le
xi
ca
l

se


l
ex
ic
al

b



b ceaf

table feature analysis terms f measure scores mention ranking
model lexical features nw data set feature types used train
model b ceaf f measure scores respectively

important feature class e feature class whose removal yields best performance
next row shows adjusted system would perform without remaining
class according scoring programs removing unseen features yields least
drop performance note caption full feature set b score
ceaf score fact two scorers agree lexical
semi lexical features important unseen alias distance features
nevertheless suggest five feature types important since best
performance achieved full feature set
tables b b ceaf f measure scores feature analysis
experiments involving cluster ranking model combined feature set nw
data set recall combined feature set seven types features
see two scorers agree completely order features removed
particular important features lexical semi lexical features
whereas least important features present lexical feature
set namely grammatical string matching semantic features suggests
lexical features general important non lexical features
used combination somewhat surprising non lexical features
commonly used features coreference resolution whereas lexical features
comparatively much less investigated coreference researchers nevertheless unlike
saw table feature types appear relevant table see








ica
l



ch

g


tic
se





g
ra









ist

ce

u
ns

ee
n

l
ex
ica
l







st
rin








se


le
xi
ca
l

cluster ranking coreference resolution












ica
l



ch

g


tic
se





g
ra









ist

ce

u
ns

ee
n

l
ex
ica
l







st
rin








se


le
xi
ca
l

b






b ceaf

table feature analysis terms f measure cluster ranking model
combined features nw data set feature types used train
model b ceaf f measure scores respectively

best b f measure score achieved lexical features
represents absolute gain f measure model trained seven
feature types suggesting learning coreference model could improved via feature
selection
next investigate whether similar trends observed trained
different source broadcast news specifically tables b b
ceaf f measure scores feature analysis experiments involving mentionranking model lexical feature set bn data set table
see two scorers agree completely order features
removed fact similar observed table nw data set
scorers determine lexical semi lexical features important
whereas distance alias features least important although five feature
types appear relevant according scorers
finally tables b b ceaf f measure scores feature
analysis experiments involving cluster ranking model combined feature set




















lia


ee
n
u
ns


ist

ce

l
ex
ica
l
se


le
xi
ca
l

rahman ng





















lia


ee
n
u
ns


ist

ce

l
ex
ica
l
se


le
xi
ca
l

b



b ceaf

table feature analysis terms f measure mention ranking model
lexical features bn data set feature types used train
model b ceaf f measure scores respectively

bn data set tables two scorers agree completely order
features removed far feature contribution concerned two
tables resemble tables b cases lexical semi lexical unseen
features important string matching grammatical features
least important semantic distance features middle case
however seven feature types seem relevant best performance achieved
full feature set according scorers perhaps interestingly numbers
column generally increasing move column means
feature type becomes progressively less useful remove feature types
suggests interactions different feature types non trivial
feature type may useful presence another feature type
summary two data sets nw bn two scoring programs demonstrate general feature types crucial overall performance
little investigated lexical features contribute overall performance
commonly used conventional features
resolution performance
gain additional insights analyze behavior coreference
different types anaphoric expressions trained different
feature sets specifically partition mentions different resolution classes


fiat
ica
l





ch




g


ist

ce


tic





g
ra








se


u
ns

ee
n

l
ex
ica
l







st
rin








se


le
xi
ca
l

cluster ranking coreference resolution







ica
l





ch




g


ist

ce


tic





g
ra








se


u
ns

ee
n

l
ex
ica
l







st
rin








se


le
xi
ca
l

b






b ceaf

table feature analysis terms f measure cluster ranking model
combined features bn data set feature types used train
model b ceaf f measure scores respectively

previous work focused mainly three rather coarse grained resolution classes namely
pronouns proper nouns common nouns follow stoyanov et al subdivide
class three fine grained classes worth mentioning none stoyanov et
al classes corresponds non anaphoric expressions since believe non anaphoric
expressions play important role analysis performance coreference
model propose three additional classes correspond non anaphoric pronouns
non anaphoric proper nouns non anaphoric common nouns finally certain
types anaphoric pronouns e g wh pronouns fall stoyanov et
al pronoun categories fill gap create another category serves
default category anaphoric pronouns covered stoyanov et al classes
resolution classes discussed detail
proper nouns four classes defined proper nouns e proper noun assigned
exact string match class preceding mention two
coreferent string p proper noun assigned partial string
match class preceding mention two coreferent


firahman ng

content words common n proper noun assigned string match class
preceding mention two coreferent content
words common na proper noun assigned non anaphor class
coreferent preceding mention
common nouns four analogous resolution classes defined mentions whose head
common noun e p n na
pronouns three pronoun classes st nd person pronouns
g gendered rd person pronouns e g u ungendered rd person pronouns
oa anaphoric pronouns belong na
non anaphoric pronouns
next score resolution class unlike stoyanov et al use modified
version muc scorer employ b reasons muc scorer
reward singleton clusters inflate systems performance clusters
overly large compute score class c process mentions test text
left right manner mention encountered check whether belongs c
use coreference model decide resolve otherwise use oracle
make correct resolution decision end mistakes attributed
incorrect resolution mentions c thus allowing us directly measure
impact overall performance test documents processed compute
b f measure score mentions belong c
performance resolution class aggregated test sets six data
sources way shown table provides nice diagnosis
strengths weaknesses coreference model used combination
feature set table percentage mentions belonging
class name class abbreviate name model follows hm
corresponds head match baseline whereas mp em mr cr denote mentionpair model entity mention model mention ranking model cluster ranking
model respectively ranking model two versions pipeline version denoted
p joint version denoted j
points deserve mention recall table conventional features
used joint mention ranking model performs better mention pair model
entity mention model comparing row rows table
see improvements attributed primarily better handling one proper
oracle determines mention anaphoric antecedents cluster
model previously made mistake employ following heuristic select
antecedent resolve mention try resolve closest preceding antecedent
belong class c antecedent exists resolve closest preceding antecedent
belongs class c reason behind heuristics preference preceding antecedent
belong class c simple since resolving mention oracle want choose
antecedent allows us maximize overall score resolving mention antecedent
belong c likely yield better score resolving antecedent
belongs c since former resolved oracle latter heuristic
applies trying use oracle resolve mention preceding cluster first attempt
resolve closest preceding cluster containing mention belong c
antecedent exists resolve closest preceding cluster containing mention belongs c



fia cluster ranking coreference resolution

proper nouns
p
n
na




e


common nouns
p
n
na




class


e




hm


















mp
em
mr p
mr j
cr p
cr j





































mp
em
mr p
mr j
cr p
cr j






















lexical feature set














mp
em
mr p
mr j
cr p
cr j






























pronouns
u
oa






g












conventional feature set















































































































combined feature set







na


table b f measure scores different resolution classes
noun class e three classes correspond non anaphoric mentions na
indicate important take account non anaphoric mentions
analyzing performance coreference model time see
joint mention ranking model resolve type e common nouns well
mention pair model entity mention model rows indicate
joint cluster ranking model better joint mention ranking model due
better handling type e common nouns non anaphoric common nouns
well anaphoric pronouns
next recall table lexical features used lieu conventional features mention pair model entity mention model joint mentionranking model exhibit significant improvements performance mention pair
model entity mention model improvements stem primarily better handling three proper noun classes e p na two common noun classes e na nonanaphoric pronouns compare rows well rows table joint
mention ranking model hand improvements accrue better handling
two proper noun classes p n two common classes e na anaphoric pronouns


firahman ng

seen rows joint cluster ranking model
overall improvement switch conventional lexical features compare rows
resulting behave differently specifically lexical features
model gets worse handling one proper noun class e one common noun class e
better handling another proper noun class n two common noun classes p n
one anaphoric pronoun class non anaphoric pronouns
finally recall combined features used lieu lexical features
cluster ranking model deterioration performance mention pair
model entity mention model deterioration performance attributed
poorer handling two proper noun classes e na two common noun classes e na
non anaphoric pronouns although better handling one proper noun
class n anaphoric pronouns compare rows well rows
table overall poorer handling anaphoricity appears major factor responsible
performance deterioration joint mention ranking model reasons
performance deterioration slightly different comparing rows see poorer
handling two proper noun classes p n three common noun classes p n na
anaphoric pronouns although better handling non anaphoric proper nouns
pronouns mentioned two versions cluster ranking model improve
trained combined features however improvements stem
improvements classes compare rows well rows
instance replacing lexical features combined features joint
cluster ranking model see improvements two proper noun classes e na one common
noun class e several pronoun classes g u performance drops another
proper noun class p three common noun classes p n na two pronoun classes
oa na
overall provide us additional insights strengths weaknesses learning coreference model well directions future work particular even two yield similar overall performance quite different
resolution class level since single coreference model outperforms
others resolution classes may beneficial apply ensemble
anaphor belonging particular resolution class resolved model offers
best performance class

conclusions
mitkov p puts coreference resolution difficult intractable
researchers making steady progress improving machine learning approaches past fifteen years progress slow however
despite deficiencies mention pair model widely thought learningbased coreference model almost decade entity mention model mentionranking model emerged mention pair model dominated learning
coreference nearly ten years although two conceptually simple represent significant departure mention pair model way
thinking alternative coreference designed cluster ranking
model advances learning coreference theoretically combining


fia cluster ranking coreference resolution

strengths two thereby addressing two commonly cited weaknesses
mention pair model bridges gap two independent lines learningbased coreference one concerning entity mention model
mention ranking model going past years narrows modeling gap sophistication rule coreference
simplicity learning coreference empirically shown
ace coreference data set cluster ranking model acquired jointly learning
anaphoricity determination coreference resolution surpasses performance several
competing approaches including mention pair model entity mention model
mention ranking model perhaps equally importantly cluster ranking model
model considered profitably exploit information provided two
fairly different sources information conventional features lexical features
ranking natural formulation coreference resolution classification
ranking coreference popularly used influential
mention pair model one goals article promote application ranking
techniques coreference resolution specifically attempted clarify difference classification ranking coreference showing constrained
optimization svm learner needs solve type hoping
help reader appreciate importance ranking coreference resolution addition provided ample empirical evidence ranking
superior classification coreference resolution
another contribution work lies empirical demonstration benefits
lexicalizing learning coreference previous work showed lexicalization provides marginal benefits coreference model showed lexicalization significantly improve mention pair model entity mention model
mention ranking model point even surpass performance
cluster ranking model interestingly showed benefit lexicalization conventional coreference features used challenges
common belief prototypical set linguistic features e g gender
number agreement must used constructing learning coreference systems
addition feature analysis experiments indicated conventional features contributed less overall performance rarely studied lexical features joint
cluster ranking coreference model two types features used combination
finally examined performance coreference model resolving mentions
belonging different resolution classes found even two achieve similar
overall performance quite different resolution class level overall
provide us additional insights strengths weaknesses learningbased coreference model well promising directions future

bibliographic note
portions work previously presented conference publication rahman
ng current article extends work several ways notably
overview literature ranking approaches coreference resolution section
detailed explanation difference classification ranking section


firahman ng

investigation issues lexicalizing coreference section depth
analysis different aspects coreference system section

acknowledgments
authors acknowledge support national science foundation nsf grant iis thank three anonymous reviewers insightful comments unanimously recommending article publication jair opinions findings conclusions recommendations expressed article authors
necessarily reflect views official policies expressed implied nsf

references
androutsopoulos malakasiotis p survey paraphrasing textual
entailment methods journal artificial intelligence
aone c bennett w evaluating automated manual acquisition
anaphora resolution strategies proceedings rd annual meeting
association computational linguistics acl pp
bagga baldwin b scoring coreference chains proceedings
linguistic coreference workshop first international conference
language resources evaluation lrec pp
barbu c mitkov r evaluation tool rule anaphora resolution methods proceedings th annual meeting association computational
linguistics acl pp
bengtson e roth understanding values features coreference
resolution proceedings conference empirical methods natural
language processing emnlp pp
berger l della pietra della pietra v j maximum entropy
natural language processing computational linguistics
bikel schwartz r weischedel r learns whats
name machine learning special issue natural language learning

blum langley p selection relevant features examples machine
learning artificial intelligence
burges c j c tutorial support vector machines pattern recognition
data mining knowledge discovery
cai j strube evaluation metrics end end coreference resolution
systems proceedings th annual sigdial meeting discourse dialogue
sigdial pp
carbonell j brown r anaphora resolution multi strategy
proceedings th international conference computational linguistics coling pp


fia cluster ranking coreference resolution

cardie c wagstaff k noun phrase coreference clustering proceedings
joint sigdat conference empirical methods natural language
processing large corpora emnlp vlc pp
charniak e elsner em works pronoun anaphora resolution proceedings th conference european chapter association computational linguistics eacl pp
collins j head driven statistical natural language parsing ph
thesis department computer information science university pennsylvania
philadelphia pa
connolly burger j day machine learning anaphoric
reference proceedings international conference methods language
processing pp
culotta wick mccallum first order probabilistic coreference resolution human language technologies conference north
american chapter association computational linguistics proceedings
main conference naacl hlt pp
daume iii h marcu large scale exploration effective global features
joint entity detection tracking model proceedings human language
technology conference conference empirical methods natural language
processing hlt emnlp pp
denis p baldridge j global joint determination anaphoricity coreference resolution integer programming human language technologies
conference north american chapter association computational
linguistics proceedings main conference naacl hlt pp
denis p baldridge j b ranking pronoun resolution proceedings
twentieth international conference artificial intelligence ijcai pp

denis p baldridge j specialized ranking coreference resolution
proceedings conference empirical methods natural language
processing emnlp pp
enrique gonzalo j artiles j verdejo f comparison extrinsic clustering evaluation metrics formal constraints information retrieval

fellbaum c wordnet electronic lexical database mit press cambridge
finkel j r grenager manning c incorporating non local information
information extraction systems gibbs sampling proceedings rd annual
meeting association computational linguistics acl pp
finkel j r manning c enforcing transitivity coreference resolution
proceedings acl hlt short papers companion pp
florian r hassan h ittycheriah jing h kambhatla n luo x nicolov n
roukos statistical model multilingual entity detection tracking
hlt naacl main proceedings pp


firahman ng

ge n hale j charniak e statistical anaphora resolution
proceedings sixth workshop large corpora wvlc pp
grosz b j joshi k weinstein providing unified account definite noun phrases discourse proceedings th annual meeting
association computational linguistics acl pp
grosz b j joshi k weinstein centering framework modeling
local coherence discourse computational linguistics
haghighi klein coreference resolution modular entity centered
model human language technologies annual conference north
american chapter association computational linguistics naacl hlt
pp
hobbs j resolving pronoun references lingua
iida r inui k matsumoto capturing salience trainable cache
model zero anaphora resolution proceedings joint conference th
annual meeting acl th international joint conference natural
language processing afnlp acl ijcnlp pp
iida r inui k takamura h matsumoto incorporating contextual cues
trainable coreference resolution proceedings eacl workshop
computational treatment anaphora
ji h rudin c grishman r ranking name tagging
proceedings workshop computationally hard joint inference
speech language processing pp
ji h westbrook grishman r semantic relations refine coreference
decisions proceedings human language technology conference conference
empirical methods natural language processing hlt emnlp pp
joachims making large scale svm learning practical scholkopf b burges
c smola eds advances kernel methods support vector learning pp
mit press cambridge
joachims optimizing search engines clickthrough data proceedings
eighth acm sigkdd international conference knowledge discovery
data mining kdd pp
kehler appelt taylor l simma non utility predicateargument frequencies pronoun interpretation proceedings human language technology conference north american chapter association
computational linguistics hlt naacl pp
kuhn h w hungarian method assignment naval
logistics quarterly
lappin leass h pronominal anaphora resolution computational linguistics
lin wu x phrase clustering discriminative learning proceedings
joint conference th annual meeting acl th international


fia cluster ranking coreference resolution

joint conference natural language processing afnlp acl ijcnlp pp

luo x coreference resolution performance metrics proceedings human
language technology conference conference empirical methods natural
language processing hlt emnlp pp
luo x ittycheriah jing h kambhatla n roukos mentionsynchronous coreference resolution bell tree proceedings
nd annual meeting association computational linguistics acl
pp
madnani n dorr b generating phrasal sentential paraphrases survey
data driven methods computational linguistics
mccarthy j lehnert w decision trees coreference resolution proceedings fourteenth international conference artificial intelligence ijcai
pp
mitkov r robust pronoun resolution limited knowledge proceedings
th annual meeting association computational linguistics th
international conference computational linguistics coling acl pp

mitkov r outstanding issues anaphora resolution gelbukh ed
computational linguistics intelligent text processing pp springer
mitkov r anaphora resolution longman
morris j hirst g lexical cohesion computed thesaural relations
indicator struture text computational linguistics
morton coreference nlp applications proceedings th annual
meeting association computational linguistics acl
muc proceedings sixth message understanding conference muc
morgan kaufmann san francisco ca
muc proceedings seventh message understanding conference muc
morgan kaufmann san francisco ca
ng v graph cut anaphoricity determination coreference resolution
proceedings conference north american chapter association
computational linguistics human language technologies naacl hlt pp

ng v cardie c identifying anaphoric non anaphoric noun phrases
improve coreference resolution proceedings th international conference
computational linguistics coling pp
ng v cardie c b improving machine learning approaches coreference resolution proceedings th annual meeting association computational
linguistics acl pp


firahman ng

poesio uryupina vieira r alexandrov kabadjov goulart r
discourse detectors definite description resolution survey preliminary
proposal proeedings acl workshop reference resolution
ponzetto p strube exploiting semantic role labeling wordnet
wikipedia coreference resolution proceedings human language technology conference conference north american chapter association
computational linguistics hlt naacl pp
rahman ng v supervised coreference resolution proceedings conference empirical methods natural language processing
emnlp pp
ravichandran hovy e och f j statistical qa classifier vs ranker
whats difference proceedings acl workshop multilingual
summarization question answering pp
recasens hovy e blanc implementing rand index coreference
resolution natural language engineering appear
roth reasoning classifiers proceedings th european conference
machine learning ecml pp
roth yih w linear programming formulation global inference
natural language tasks proceedings eighth conference computational
natural language learning conll pp
soon w ng h lim c machine learning coreference
resolution noun phrases computational linguistics
stoyanov v gilbert n cardie c riloff e conundrums noun phrase
coreference resolution making sense state art proceedings
joint conference th annual meeting acl th international
joint conference natural language processing afnlp acl ijcnlp pp

strube muller c machine learning pronoun resolution
spoken dialogue proceedings st annual meeting association
computational linguistics acl pp
strube rapp muller c influence minimum edit distance
reference resolution proceedings conference empirical methods
natural language processing emnlp pp
toutanova k klein manning c singer feature rich part speech
tagging cyclic dependency network hlt naacl proceedings
main conference pp
uryupina high precision identification discourse unique noun
phrases proceedings st annual meeting association computational linguistics companion pp
vapnik v n nature statistical learning springer york


fia cluster ranking coreference resolution

vilain burger j aberdeen j connolly hirschman l modeltheoretic coreference scoring scheme proceedings sixth message understanding conference muc pp
walker joshi prince e eds centering theory discourse oxford
university press
yang x su j lang j tan c l li entity mention model
coreference resolution inductive logic programming proceedings th
annual meeting association computational linguistics human language
technologies acl hlt pp
yang x su j zhou g tan c l np cluster coreference
resolution proceedings th international conference computational
linguistics coling
yang x zhou g su j tan c l coreference resolution competitive
learning proceedings st annual meeting association
computational linguistics acl pp




