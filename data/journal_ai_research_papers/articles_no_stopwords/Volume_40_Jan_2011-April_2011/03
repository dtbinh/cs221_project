Journal Artificial Intelligence Research 40 (2011) 469521

Submitted 06/10; published 02/11

Narrowing Modeling Gap:
Cluster-Ranking Approach Coreference Resolution
Altaf Rahman
Vincent Ng

altaf@hlt.utdallas.edu
vince@hlt.utdallas.edu

Human Language Technology Research Institute
University Texas Dallas
800 West Campbell Road; Mail Station EC31
Richardson, TX 75080-3021 U.S.A.

Abstract
Traditional learning-based coreference resolvers operate training mention-pair
model determining whether two mentions coreferent not. Though conceptually
simple easy understand, mention-pair model linguistically rather unappealing
lags far behind heuristic-based coreference models proposed pre-statistical
NLP era terms sophistication. Two independent lines recent research attempted improve mention-pair model, one acquiring mention-ranking model
rank preceding mentions given anaphor, training entity-mention
model determine whether preceding cluster coreferent given mention.
propose cluster-ranking approach coreference resolution, combines strengths
mention-ranking model entity-mention model, therefore theoretically
appealing models. addition, seek improve cluster rankers
via two extensions: (1) lexicalization (2) incorporating knowledge anaphoricity
jointly modeling anaphoricity determination coreference resolution. Experimental results ACE data sets demonstrate superior performance cluster rankers
competing approaches well effectiveness two extensions.

1. Introduction
Noun phrase (NP) coreference resolution task identifying NPs (or mentions
ACE terminology1 ) text dialogue refer real-world entity concept.
computational perspective, coreference clustering task, goal partitioning
set mentions coreference clusters cluster contains
mentions co-referring. mathematical perspective, coreference relation
equivalence relation defined pair mentions, satisfies reflexivity, symmetry,
transitivity. Following previous work coreference resolution, use term
anaphoric describe mention part coreference chain head
chain. Given anaphoric mention mk , antecedent mk mention coreferent
mk precedes associated text, set candidate antecedents mk
consists mentions precede mk .2
1. precisely, mention instance reference entity real world. article,
treat terms mention noun phrase synonymous use interchangeably.
2. Note definitions somewhat overloaded. Linguistically, anaphor noun phrase
depends antecedent semantic interpretation. Hence, Barack Obama anaphoric
definition formal definition.
c
2011
AI Access Foundation. rights reserved.

fiRahman & Ng

research focus computational coreference resolution exhibited gradual shift
heuristic-based approaches machine learning approaches past decade. shift
attributed part advent statistical natural language processing (NLP)
era, part public availability coreference-annotated corpora produced
result MUC-6 MUC-7 conferences series ACE evaluations. One
influential machine learning approaches coreference resolution classificationbased approach, coreference recast binary classification task (e.g., Aone &
Bennett, 1995; McCarthy & Lehnert, 1995). Specifically, classifier trained
coreference-annotated data used determine whether pair mentions co-referring
not. However, pairwise classifications produced classifier (which commonly known mention-pair model) may satisfy transitivity property inherent
coreference relation, since possible model classify (A,B) coreferent,
(B,C) coreferent, (A,C) coreferent. result, separate clustering mechanism needed coordinate possibly contradictory pairwise classification decisions
construct partition given mentions.
mention-pair model significantly influenced learning-based coreference research
past fifteen years. fact, many recently published coreference papers
still based classical learning-based coreference model (e.g., Bengtson & Roth, 2008;
Stoyanov, Gilbert, Cardie, & Riloff, 2009). Despite popularity, model least
two major weaknesses. First, since candidate antecedent mention resolved
(henceforth active mention) considered independently others, model
determines good candidate antecedent relative active mention,
good candidate antecedent relative candidates. words, fails
answer critical question candidate antecedent probable. Second,
limitations expressiveness: information extracted two mentions
alone may sufficient making informed coreference decision, especially
candidate antecedent pronoun (which semantically empty) mention lacks
descriptive information gender (e.g., Clinton).
Recently, coreference researchers investigated alternative models coreference
aim address aforementioned weaknesses mention-pair model. address
first weakness, researchers proposed mention-ranking model. model determines candidate antecedent probable given active mention imposing
ranking candidate antecedents (e.g., Denis & Baldridge, 2007b, 2008; Iida, Inui,
& Matsumoto, 2009). Ranking arguably natural formulation coreference resolution classification, ranker allows candidate antecedents considered
simultaneously therefore directly captures competition among them. Another desirable consequence exists natural resolution strategy ranking approach:
mention resolved candidate antecedent highest rank. contrasts
classification-based approaches, many clustering algorithms employed
co-ordinate pairwise coreference decisions (because unclear one best).
address second weakness, researchers proposed entity-mention coreference
model (e.g., Luo, Ittycheriah, Jing, Kambhatla, & Roukos, 2004; Yang, Su, Zhou, & Tan,
2004; Yang, Su, Lang, Tan, & Li, 2008). Unlike mention-pair model, entity-mention
model trained determine whether active mention belongs preceding, possibly
partially-formed, coreference cluster. Hence, employ cluster-level features (i.e., fea470

fiA Cluster-Ranking Approach Coreference Resolution

tures defined subset mentions preceding cluster), makes
expressive mention-pair model.
entity-mention model mention-ranking model conceptually simple
extensions mention-pair model, born nearly ten years mention-pair
model proposed, particular, contributions under-estimated:
paved new way thinking supervised modeling coreference represents
significant departure mention-pair counterpart, many years
learning-based coreference model NLP researchers. proposal two models
facilitated part advances statistical modeling natural languages: statistical
NLP models evolved capturing local information global information,
employing classification-based models ranking-based models. context
coreference resolution, entity-mention model enables us compute features based
variable number mentions, mention-ranking model enables us rank variable
number candidate antecedents. Nevertheless, neither models addresses
weaknesses mention-pair model satisfactorily: mention-ranking model allows
candidate antecedents ranked compared simultaneously, enable
use cluster-level features; hand, entity-mention model employ
cluster-level features, allow candidates considered simultaneously.
Motivated part observation, propose learning-based approach coreference resolution theoretically appealing mention-ranking model
entity-mention model: cluster-ranking approach. Specifically, recast coreference problem determining set preceding coreference clusters
best link active mention using learned cluster-ranking model. essence,
cluster-ranking model combines strengths mention-ranking model entitymention model, addresses weaknesses associated mention-pair model.
cluster-ranking model appears conceptually simple natural extension entity-mention model mention-ranking model, believe
simplicity stems primarily choice presentation concepts easiest
reader understand. particular, note mental processes involved
design cluster-ranking model means simple way model
presented: requires analysis strengths weaknesses existing
approaches learning-based coreference resolution connection them,
formulation view entity-mention model mention-ranking
model addressing two complementary weaknesses mention-pair model. believe
significance cluster-ranking model lies bridging two rather independent
lines learning-based coreference research going past years,
one involving entity-mention model mention-ranking model.
addition, seek improve cluster-ranking model two sources linguistic knowledge. First, propose exploit knowledge anaphoricity (i.e., knowledge
whether mention anaphoric not). Anaphoricity determination means new
problem, neither use anaphoricity information improve coreference resolution. innovation lies way learn knowledge anaphoricity. Specifically,
previous work typically adopted pipeline coreference architecture,
anaphoricity determination performed prior coreference resolution resulting
information used prevent coreference system resolving mentions de471

fiRahman & Ng

termined non-anaphoric (for overview, see work Poesio, Uryupina, Vieira,
Alexandrov-Kabadjov, & Goulart, 2004), propose model jointly learning anaphoricity determination coreference resolution. Note major weakness pipeline
architecture lies fact errors anaphoricity determination could propagated
coreference resolver, possibly leading deterioration coreference performance
(Ng & Cardie, 2002a). joint model potential solution error-propagation
problem.
Second, examine kind linguistic features exploited majority
existing supervised coreference resolvers: word pairs composed strings (or
head nouns) active mention one preceding mentions. Intuitively,
word pairs contain useful information. example, may help improve precision
model, allowing learner learn moderate probability
anaphoric, contrary taken phrase contrary never
anaphoric. may help improve recall, allowing learner determine,
instance, airline carrier coreferent. Hence, offer convenient
means attack one major problems coreference research: identifying coreferent
common nouns lexically dissimilar semantically related. Note
extremely easy compute, even so-called cheap features stringmatching grammatical features (Yang, Zhou, Su, & Tan, 2003), majority
existing supervised coreference systems unlexicalized hence exploiting
them. Somewhat unexpectedly, however, researchers lexicalize coreference
models employing word pairs features (e.g., Luo et al., 2004; Daume III & Marcu, 2005;
Bengtson & Roth, 2008), feature analysis experiments indicate lexical features
best marginally useful. instance, Luo et al. Daume III Marcu report
leaving lexical features feature ablation experiments causes ACE value
drop 0.8 0.7, respectively. previous attempts lexicalization merely
append word pairs conventional coreference feature set, goal investigate
whether make better use lexical features learning-based coreference resolution.
sum up, propose cluster-ranking approach coreference resolution joint
model exploiting anaphoricity information, investigate role lexicalization
learning-based coreference resolution. Besides empirically demonstrating clusterranking model significantly outperforms competing approaches ACE 2005 coreference
data set, two extensions model, namely lexicalization joint modeling,
effective improving performance, believe work makes four contributions
coreference resolution:
Narrowing modeling gap. machine learning approaches coreference resolution received lot attention since mid-1990s, mention-pair model
heavily influenced learning-based coreference research decade, yet
model lags far behind heuristic-based coreference models proposed 1980s
1990s terms sophistication. particular, notion ranking traced back
centering algorithms (for information, see books Mitkov, 2002; Walker, Joshi,
& Prince, 1998), idea behind ranking preceding clusters (in heuristic manner)
found Lappin Leasss (1994) influential paper pronoun resolution.
cluster-ranking model completely close gap simplicity machine
learning approaches sophistication heuristic approaches coreference resolu472

fiA Cluster-Ranking Approach Coreference Resolution

tion, believe represents important step towards narrowing gap. Another
important gap cluster-ranking model helps bridge two independent lines
learning-based coreference research going past years, one
involving entity-mention model mention-ranking model.
Promoting use ranking models. mention-ranking model
empirically shown outperform mention-pair model (Denis & Baldridge, 2007b, 2008),
former received much attention among coreference researchers should.
particular, mention-pair model continues popularly used investigated
past years mention-ranking model. believe lack excitement
ranking-based approaches coreference resolution attributed least part
lack theoretical understanding ranking, previous work ranking-based coreference
resolution employed ranking algorithms essentially black box. Without opening
black box, could difficult researchers appreciate subtle difference
ranking classification. attempt promote use ranking-based models,
provide brief history use ranking coreference resolution (Section 2),
tease apart differences classification ranking showing constrained
optimization problem support vector machine (SVM) attempts solve classificationbased ranking-based coreference models (Section 3).
Gaining better understanding existing learning-based coreference models.
Recall lexicalization one two linguistic knowledge sources propose
use improve cluster-ranking model. Note lexicalization applied
cluster-ranking model, essentially learning-based coreference models. However,
mentioned before, vast majority existing coreference resolvers unlexicalized.
fact, mention-ranking model shown improve mention-pair model
unlexicalized feature set. attempt gain additional insights behavior
different learning-based coreference models, compare performance lexicalized
feature set. Furthermore, analyze via experiments involving feature ablation
data source adaptability, well report performance resolving different types
anaphoric expressions.
Providing implementation cluster-ranking model. stimulate
research ranking-based approaches coreference resolution, facilitate use
coreference information high-level NLP applications, make software implements cluster-ranking model publicly available.3
rest article organized follows. Section 2 provides overview use
ranking coreference resolution. Section 3 describes baseline coreference models:
mention-pair model, entity-mention model, mention-ranking model.
discuss cluster-ranking approach joint model anaphoricity determination
coreference resolution Section 4. Section 5 provides details lexicalize
coreference models. present evaluation results experimental analyses different
aspects coreference models Section 6 Section 7, respectively. Finally,
conclude Section 8.
3. software available http://www.hlt.utdallas.edu/~ altaf/cherrypicker/.

473

fiRahman & Ng

2. Ranking Approaches Coreference Resolution: Bit History
ranking theoretically empirically better formulation learning-based coreference resolution classification, mention-ranking model popularly
used investigated mention-pair counterpart since proposed. promote
ranking-based coreference models, set stage discussion learningbased coreference models next section, provide section brief history
use ranking heuristic-based learning-based coreference resolution.
broader sense, many heuristic anaphora coreference resolvers rankingbased. example, find antecedent anaphoric pronoun, Hobbss (1978) seminal syntax-based resolution algorithm considers sentences given text reverse
order, starting sentence pronoun resides searching potential
antecedents corresponding parse trees left-to-right, breadth-first manner
obeys binding agreement constraints. Hence, keep searching beginning
text reached (i.e., stop even algorithm proposes antecedent),
obtain ranking candidate antecedents pronoun consideration, rank candidate determined order proposed
algorithm. fact, rank antecedent obtained via method commonly
known Hobbss distance, used linguistic feature statistical
pronoun resolvers (e.g., Ge, Hale, & Charniak, 1998; Charniak & Elsner, 2009). general,
search-based resolution algorithms Hobbss consider candidate antecedents particular order (typically) propose first candidate satisfies linguistic constraints
antecedent.
Strictly speaking, however, may want consider heuristic resolution algorithm
ranking-based algorithm considers candidate antecedents simultaneously,
example assigning rank score candidate selecting highest-ranked
highest-scored candidate antecedent. Even stricter definition
ranking, still many heuristic resolvers ranking-based. resolvers
typically assign rank score candidate antecedent based number factors,
knowledge sources, propose one highest rank score
antecedent (e.g., Carbonell & Brown, 1988; Cardie & Wagstaff, 1999). factor belongs
one two types: constraints preferences (Mitkov, 1998). Constraints must satisfied
two mentions posited coreferent. Examples constraints include gender
number agreement, binding constraints, semantic compatibility. Preferences indicate likelihood candidate antecedent. preference factors measure
compatibility anaphor candidate (e.g., syntactic parallelism favors candidates grammatical role anaphor), preference factors
computed based candidate only, typically capturing salience candidate.
constraint preference manually assigned weight indicating importance.
instance, gender disagreement typically assigned weight , indicating
candidate anaphor must agree gender, whereas preference factors typically
finite weight. score candidate obtained summing weights
factors associated candidate.
ranking-based resolution algorithms assign score candidate antecedent. Rather, simply impose ranking candidates based salience.
474

fiA Cluster-Ranking Approach Coreference Resolution

Perhaps representative family algorithms employ salience rank candidates centering algorithms (for descriptions specific centering algorithms, see
work Grosz, Joshi, & Weinstein, 1983, 1995; Walker et al., 1998; Mitkov, 2002),
salience mention, typically estimated using grammatical role, used rank
forward-looking centers.
work related Lappin Leass (1994), whose goal perform pronoun resolution assigning anaphoric pronoun highest-ranked preceding
cluster, therefore heuristic cluster-ranking model. many heuristic-based
resolvers, Lappin Leasss algorithm identifies highest-ranked preceding cluster
active mention first applying set linguistic constraints filter candidate antecedents grammatically incompatible active mention, ranking
preceding clusters, contain mentions survive filtering process, using
salience factors. Examples salience factors include sentence recency (whether preceding cluster contains mention appears sentence currently processed),
subject emphasis (whether cluster contains mention subject position), existential emphasis (whether cluster contains mention predicate nominal
existential construction), accusative emphasis (whether cluster contains mention
appears verbal complement accusative case). salience factor associated
manually-assigned weight indicates importance relative factors,
score cluster sum weights salience factors applicable
cluster. Lappin Leasss paper widely read paper pronoun resolution,
cluster ranking aspect algorithm rarely emphasized. fact,
aware recent work learning-based coreference resolution establishes
connection entity-mention model Lappin Leasss algorithm.
Despite conceptual similarities, cluster-ranking model Lappin Leasss
(1994) algorithm differ several respects. First, Lappin Leass tackle pronoun resolution rather full coreference task. Second, apply linguistic constraints
filter incompatible candidate antecedents, resolution strategy learned without applying hand-coded constraints separate filtering step. Third, attempt
compute salience preceding cluster respect active mention, attempt
determine compatibility cluster active mention, using factors
determine salience lexical grammatical compatibility, instance.
Finally, algorithm heuristic-based, weights associated salience
factor encoded manually rather learned, unlike system.
first paper learning-based coreference resolution written Connolly, Burger,
Day (1994) published year Lappin Leasss (1994) paper.
Contrary common expectation, coreference model paper proposes rankingbased model, influential mention-pair model. main idea behind Connolly et
al.s approach convert problem ranking N candidate antecedents set
pairwise ranking problems, involves ranking exactly two candidates.
rank two candidates, classifier trained using training set instance
corresponds active mention well two candidate antecedents possesses
class value indicates two candidates better. idea certainly ahead
time, embodied many advanced ranking algorithms developed
machine learning information retrieval communities past years.
475

fiRahman & Ng

later re-invented almost time, independently, Yang et al. (2003)
Iida, Inui, Takamura, Matsumoto (2003), refer twin-candidate model
tournament model, respectively. name twin-candidate model motivated
fact model considers two candidates time, whereas name tournament
model assigned ranking two candidates viewed tournament
(with higher-ranked candidate winning tournament) candidate wins
largest number tournaments chosen antecedent active mention.
bit history rarely mentioned literature, reveals three somewhat interesting
perhaps surprising facts. First, ranking first applied train coreference models
much earlier people typically think. Second, despite first learning-based
coreference model, Connolly et al.s ranking-based model theoretically appealing
classification-based mention-pair model, later shown Yang et al.
Iida et al.. empirically better well. Finally, despite theoretical empirical
superiority, Connolly et al.s model largely ignored NLP community received
attention re-invented nearly decade later, time period
mention-pair counterpart essentially dominated learning-based coreference research.4
conclude section making important observation distinction classification ranking applies discriminative models generative models.
Generative models try capture true conditional probability event. context coreference resolution, probability mention particular
antecedent referring particular entity (i.e., preceding cluster). Since probabilities normalize, similar ranking objective: system trying raise
probability mention refers correct antecedent entity expense
probabilities refers other. Thus, antecedent version generative
coreference model proposed Ge et al. (1998) resembles mention-ranking model,
entity version proposed Haghighi Klein (2010) similar spirit
cluster-ranking model.

3. Baseline Coreference Models
section, describe three coreference models serve baselines:
mention-pair model, entity-mention model, mention-ranking model. illustrative purposes, use text segment shown Figure 1. mention
segment annotated [m]cid
mid , mid mention id cid id cluster
belongs. see, mentions partitioned four sets, Barack
Obama, his, one cluster, remaining mentions cluster.

4. may possible (and perhaps crucial) determine mention-pair model received
lot attention Connolly et al.s model, since days academic papers
could accessed easily electronic form, speculate publication venue played role:
Connolly et al.s work published New Methods Language Processing conference 1994
(and later book chapter 1997), whereas mention-pair model introduced Aone
Bennetts (1995) paper McCarthy Lehnerts (1995) paper, appeared proceedings
two comparatively higher-profile AI conferences: ACL 1995 IJCAI 1995.

476

fiA Cluster-Ranking Approach Coreference Resolution

[Barack Obama]11 nominated [Hillary Rodham Clinton]22 [[his]13 secretary state]34 [Monday]45 .
[He]16 ...

Figure 1: illustrative example
3.1 Mention-Pair Model
noted before, mention-pair model classifier decides whether
active mention mk coreferent candidate antecedent mj . instance i(mj , mk )
represents mj mk . implementation, instance consists 39 features shown
Table 1. features largely employed state-of-the-art learning-based
coreference systems (e.g., Soon, Ng, & Lim, 2001; Ng & Cardie, 2002b; Bengtson & Roth,
2008), computed automatically. seen, features divided four
blocks. first two blocks consist features describe properties mj mk ,
respectively, last two blocks features describe relationship mj
mk . classification associated training instance either positive negative,
depending whether mj mk coreferent.
one training instance created pair mentions, negative instances
would significantly outnumber positives, yielding skewed class distribution
typically adverse effect model training. result, subset mention
pairs generated training. Following Soon et al. (2001), create (1) positive
instance anaphoric mention mk closest antecedent mj ; (2) negative
instance mk paired intervening mentions, mj+1 , mj+2 , . . . , mk1 .
running example shown Figure 1, three training instances generated He:
i(Monday, He), i(secretary state, He), i(his, He). first two instances
labeled negative, last one labeled positive. train mention-pair
model, use SVM learning algorithm SVMlight package (Joachims, 1999).5
mentioned introduction, previous work learning-based coreference
resolution typically treats underlying machine learner simply black-box tool,
choose provide reader overview SVMs, learner employing
work. Note self-contained overview, means comprehensive
introduction maximum-margin learning: goal provide reader
details believe needed understand difference classification
ranking perhaps appreciate importance ranking.6
begin with, assume given data set consisting positively labeled
points, class value +1, negatively labeled points, class
5. Since SVMlight assumes real-valued features, cannot operate features multiple discrete values
directly. Hence, need convert features shown Table 1 equivalent set features
used directly SVMlight . uniformity, perform conversion feature
Table 1 (rather multi-valued features) follows: create one binary-valued feature
SVMlight feature-value pair derived feature set Table 1. example,
pronoun 1 two values, N. derive two binary-valued features, pronoun 1=Y
pronoun 1=N. One value 1 value 0 instance.
6. overview theory maximum-margin learning, refer reader Burgess (1998)
tutorial.

477

fiRahman & Ng

Features describing mj , candidate antecedent
1 pronoun 1
mj pronoun; else N
2 subject 1
mj subject; else N
3 nested 1
mj nested NP; else N
Features describing mk , mention resolved
4 number 2
singular plural, determined using lexicon
male, female, neuter, unknown, determined using list
5 gender 2
common first names
mk pronoun; else N
6 pronoun 2
7 nested 2
mk nested NP; else N
8 semclass 2
semantic class mk ; one person, location, organization, date, time, money, percent, object, others, determined using WordNet (Fellbaum, 1998) Stanford NE recognizer (Finkel, Grenager, & Manning, 2005)
9 animacy 2
mk determined human animal WordNet NE
recognizer; else N
nominative case mk pronoun; else NA. E.g.,
10 pro type 2
feature value
Features describing relationship mj , candidate antecedent mk ,
mention resolved
11 head match
C mentions head noun; else
12 str match
C mentions string; else
13 substr match
C one mention substring other; else
C mentions pronominal string; else
14 pro str match
15 pn str match
C mentions proper names string; else
16 nonpro str match C two mentions non-pronominal
string; else
17 modifier match
C mentions modifiers; NA one
dont modifier; else
C mentions pronominal either pronoun
18 pro type match
different respect case; NA least one
pronominal; else
19 number
C mentions agree number; disagree; NA
number one mentions cannot determined
20 gender
C mentions agree gender; disagree; NA gender
one mentions cannot determined
21 agreement
C mentions agree gender number; disagree
number gender; else NA
22 animacy
C mentions match animacy; dont; NA
animacy one mentions cannot determined
C mentions pronouns; neither pronouns; else NA
23 pronouns
24 proper nounsC mentions proper nouns; neither proper nouns;
else NA
25 maximalnp
C two mentions maximial NP projection; else
26 span
C neither mention spans other; else
27 indefinite
C mk indefinite NP appositive relationship;
else
28 appositive
C mentions appositive relationship; else
29 copular
C mentions copular construction; else

478

fiA Cluster-Ranking Approach Coreference Resolution

Features describing relationship mj , candidate antecedent mk ,
mention resolved (continued previous page)
30 semclass
C mentions semantic class (where set
semantic classes considered enumerated description
semclass 2 feature); dont; NA semantic class
information one mentions cannot determined
31 alias
C one mention abbreviation acronym other; else

32 distance
binned values sentence distance mentions
Additional features describing relationship mj , candidate antecedent
mk , mention resolved
33 number
concatenation number 2 feature values mj mk .
E.g., mj Clinton mk they, feature value singularplural, since mj singular mk plural
34 gender
concatenation gender 2 feature values mj mk
35 pronoun
concatenation pronoun 2 feature values mj mk
36 nested
concatenation nested 2 feature values mj mk
37 semclass
concatenation semclass 2 feature values mj mk
38 animacy
concatenation animacy 2 feature values mj mk
concatenation pro type 2 feature values mj mk
39 pro type

Table 1: Feature set coreference resolution. Non-relational features describe mention
cases take value Yes No. Relational features describe relationship
two mentions indicate whether Compatible, Incompatible
Applicable.
value 1. used classification mode, SVM learner aims learn hyperplane
(i.e., linear classifier) separates positive points negative points.
one hyperplane achieves zero training error, learner choose
hyperplane maximizes margin separation (i.e., distance
hyperplane training example closest it), larger margin proven
provide better generalization unseen data (Vapnik, 1995). formally, maximum
margin hyperplane defined w x b = 0, x feature vector representing
arbitrary data point, w (a weight vector) b (a scalar) parameters
learned solving following constrained optimization problem:
Optimization Problem 1: Hard-Margin SVM Classification
arg min
subject

1
kwk2
2

yi (w xi b) 1,

1 n,

yi {+1, 1} class i-th training point xi . Note data point
xi , exactly one linear constraint optimization problem ensures xi
correctly classified. particular, using value 1 right side inequality
479

fiRahman & Ng

constraint ensures certain distance (i.e., margin) xi hyperplane.
shown margin inversely proportional length weight vector.
Hence, minimizing length weight vector equivalent maximizing margin.
resulting SVM classifier known hard-margin SVM: margin hard
data point correct side hyperplane.
However, cases data set linearly separable, hyperplane
perfectly separate positives negatives, result,
constrained optimization problem solution. Instead asking SVM
learner give return solution, solve relaxed version problem
consider hyperplanes produce non-zero training errors potential solutions.
words, modify linear constraints associated data point
training errors allowed. However, modify linear constraints
leave objective function is, learner search maximum-margin
hyperplane regardless training error produces. Since training error correlates
positively generalization error, crucial objective function take
consideration training error hyperplane large margin low training
error found. However, non-trivial maximize margin minimize
training error simultaneously, since training error typically increases maximize
margin. result, need find trade-off two criteria, resulting
objective function linear combination margin size training error.
formally, find optimal hyperplane solving following constrained optimization
problem:
Optimization Problem 2: Soft-Margin SVM Classification
arg min

X
1
kwk2 + C

2


subject
yi (w xi b) 1 , 1 n.
before, yi {+1, 1} class i-th training point xi . C regularization
parameter balances training error margin size. Finally, non-negative slack
variable represents degree misclassification xi ; particular, > 1,
data point wrong side hyperplane. SVM allows data points
appear wrong side hyperplane, known soft-margin SVM.
Given optimization problem, rely training algorithm employed SVMlight
finding optimal hyperplane.
training, resulting SVM classifier used clustering algorithm identify
antecedent mention test text. Specifically, active mention compared
turn preceding mention. pair, test instance created training
presented SVM classifier, returns value indicates likelihood
two mentions coreferent. Mention pairs class values 0 considered
coreferent; otherwise pair considered coreferent. Following Soon et al. (2001),
apply closest-first linking regime antecedent selection: given active mention mk ,
480

fiA Cluster-Ranking Approach Coreference Resolution

select antecedent closest preceding mention classified coreferent
mk . mk classified coreferent preceding mention, considered
non-anaphoric (i.e., antecedent selected mk ).
3.2 Entity-Mention Model
Unlike mention-pair model, entity-mention model classifier decides whether
active mention mk belongs partial coreference cluster cj precedes mk .
training instance, i(cj , mk ), represents cj mk . features instance
divided two types: (1) features describe mk (i.e, shown second block
Table 1), (2) cluster-level features, describe relationship cj
mk . cluster-level feature created feature employed mention-pair
model applying logical predicate. example, given number feature (i.e., feature
#19 Table 1), determines whether two mentions agree number, apply
predicate create cluster-level feature value yes mk agrees
number mentions cj otherwise. Motivated previous work (Luo
et al., 2004; Culotta, Wick, & McCallum, 2007; Yang et al., 2008), create cluster-level
features mention-pair features using four commonly-used logical predicates: none,
most-false, most-true, all. Specifically, feature x shown last two
blocks Table 1, first convert x equivalent set binary-valued features
multi-valued. Then, resulting binary-valued feature xb , create four binaryvalued cluster-level features: (1) none-xb true xb false mk
mention cj ; (2) most-false-xb true xb true mk less half
(but least one) mentions cj ; (3) most-true-xb true xb true
mk least half (but all) mentions cj ; (4) all-xb true xb
true mk mention cj . Hence, xb , exactly one four
cluster-level features evaluates true.7
Following Yang et al. (2008), create (1) positive instance anaphoric mention
mk preceding cluster cj belongs; (2) negative instance mk
paired preceding cluster whose last mention appears mk closest
antecedent (i.e., last mention cj ). Consider running example. Three
training instances generated He: i({Monday}, He), i({secretary state}, He),
i({Barack Obama, his}, He). first two instances labeled negative,
last one labeled positive. mention-pair model, train
entity-mention model using SVM learner.
Since entity-mention model classifier, use SVMlight classification
mode, resulting constrained optimization problem essentially Optimization Problem 2, except training example xi represents active mention
one preceding clusters rather two mentions.
7. Note cluster-level feature represented probabilistic feature. Specifically, recall
four logical predicates partitions [0,1] interval. predicate evaluates true given
cluster-level feature depends probability obtained computation feature. Instead
applying logical predicates convert probability one four discrete values,
simply use probability value cluster-level feature. However, choose employ
probabilistic representation, preliminary experiments indicated using probabilistic features
yielded slightly worse results using logical features.

481

fiRahman & Ng

training, resulting classifier used identify preceding cluster mention
test text. Specifically, mentions processed left-to-right manner.
active mention mk , test instance created mk preceding clusters
formed far. test instances presented classifier. Finally, adopt
closest-first clustering regime, linking mk closest preceding cluster classified
coreferent mk . mk classified coreferent preceding cluster,
considered non-anaphoric. Note partial clusters preceding mk formed
incrementally based predictions classifier first k 1 mentions;
gold-standard coreference information used formation.
3.3 Mention-Ranking Model
noted before, ranking model imposes ranking candidate antecedents
active mention mk . train ranking-model, use SVM ranker-learning algorithm
Joachimss (2002) SVMlight package.
mention-pair model, training instance i(mj , mk ) represents mk
preceding mention mj . fact, features represent instance method
creating training instances identical employed mention-pair model.
difference lies labeling training instances. Assuming Sk set
training instances created anaphoric mention mk , rank value i(mj , mk ) Sk
rank mj among competing candidate antecedents, 2 mj closest
antecedent mk , 1 otherwise.8 exemplify, consider running example.
mention-pair model, three training instances generated He: i(Monday,
He), i(secretary state, He), i(his, He). third instance rank value 2,
remaining two rank value 1.
first glance, seems training set generated learning mentionranking model, identical one learning mention-pair model, instance
represents two mentions labeled one two possible values. Since previous work
ranking-based coreference resolution attempt clarify difference
two, believe could difficult reader appreciate idea using
ranking coreference resolution.
Let us first describe difference classification ranking high level,
beginning training sets employed mention-ranking model mentionpair model. difference label associated instance training
mention-ranking model rank value, whereas label associated instance
training mention-pair model class value. specifically, since ranking SVM
learns rank set candidate antecedents, relative ranks two candidates,
rather absolute rank candidate, matter training process.
words, point view ranking SVM, training set instance #1
rank value 2 instance #2 rank value 1 functionally equivalent one
#1 rank value 10 #2 rank value 5, assuming remaining
instances generated anaphor two training sets identical
rank value 1 10.
8. larger rank value implies better rank SVMlight .

482

fiA Cluster-Ranking Approach Coreference Resolution

Next, take closer look ranker-training process. denote training set
created described . addition, assume instance
denoted (xjk , yjk ), xjk feature vector created anaphoric mention
mk candidate antecedent mj , yjk rank value. training ranker,
SVM ranker-learning algorithm derives training set original training set
follows. Specifically, every pair training instances (xik , yik ) (xjk , yjk )
yik 6= yjk , create new training instance (xijk , yijk ) , xijk = xik xjk ,
yijk {+1, 1} 1 xik larger rank value xjk (and 1 otherwise). way,
creation resembles Connolly et al.s (1994) pairwise ranking approach saw
Section 2, convert ranking problem pairwise classification problem.9
goal ranker-learning algorithm, then, find hyperplane minimizes
number misclassifications . Note since yijk {+1, 1}, class value
instance depends relative ranks two candidate antecedents,
absolute rank values.
Given conversion ranking problem pairwise classification problem,
constrained optimization problem SVM ranker-learning algorithm attempts
solve, described below, similar Optimization Problem 2:
Optimization Problem 3: Soft-Margin SVM Ranking
X
1
ijk
arg min kwk2 + C
2
subject
yijk (w (xik xjk ) b) 1 ijk ,
ijk non-negative slack variable represents degree misclassification
xijk , C regularization parameter balances training error margin size.
Two points deserve mention. First, optimization problem equivalent one
classification SVM pairwise difference feature vectors xik xjk . result,
training algorithm used solve Optimization Problem 2 applicable
optimization problem. Second, number linear inequality constraints
generated document optimization problems training mention-pair
model entity-mention model quadratic number mentions d,
number constraints generated ranking SVM cubic number mentions,
since instance represents three (rather two) mentions.
training, mention-ranking model applied rank candidate antecedents
active mention test text follows. Given active mention mk , follow Denis
Baldridge (2008) use independently-trained classifier determine whether mk
non-anaphoric. so, mk resolved. Otherwise, create test instances mk
pairing preceding mentions. test instances presented
ranker, computes rank value instance taking dot product
9. main difference training set employed Connolly et al.s approach
, instance formed taking difference feature vectors two instances , whereas
Connolly et al.s training set, instance formed concatenating feature vectors two
instances .

483

fiRahman & Ng

instance vector weight vector. preceding mention assigned largest
value ranker selected antecedent mk . Ties broken preferring
antecedent closest distance mk .
anaphoricity classifier used resolution step trained using publicly-available
implementation10 maximum entropy (MaxEnt) modeling. instance corresponds
mention represented 26 features deemed useful distinguishing
anaphoric non-anaphoric mentions (see Table 2 details). Linguistically,
features broadly divided three types: string-matching, grammatical,
semantic. either relational feature, compares mention one
preceding mentions, non-relational feature, encodes certain linguistic property
mention whose anaphoricity determined (e.g., NP type, number, definiteness).

4. Coreference Cluster Ranking
section, describe cluster-ranking approach NP coreference. noted
before, approach aims combine strengths entity-mention model
mention-ranking model.
4.1 Training Applying Cluster Ranker
ease exposition, describe subsection train apply clusterranking model used pipeline architecture, anaphoricity determination
performed prior coreference resolution. next subsection, show
two tasks learned jointly.
Recall cluster-ranking model ranks set preceding clusters active
mention mk . Since cluster-ranking model hybrid mention-ranking model
entity-mention model, way trained applied hybrid
two. particular, instance representation employed cluster-ranking model
identical used entity-mention model, training instance i(cj , mk )
represents preceding cluster cj anaphoric mention mk consists clusterlevel features formed predicates. Unlike entity-mention model, however,
cluster-ranking model, (1) training instance created anaphoric mention mk
preceding clusters; (2) since training model ranking clusters,
assignment rank values training instances similar mention-ranking
model. Specifically, rank value training instance i(cj , mk ) created mk
rank cj among competing clusters, 2 mk belongs cj , 1 otherwise.
train cluster-ranking model, use SVM learner ranking mode, resulting
constrained optimization problem essentially Optimization Problem
3, except training example xijk represents active mention mk two
preceding clusters, ci cj , rather two preceding mentions.
Applying learned cluster ranker test text similar applying mentionranking model. Specifically, mentions processed left-to-right manner.
active mention mk , first apply independently-trained classifier determine mk
non-anaphoric. so, mk resolved. Otherwise, create test instances mk
10. See http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html.

484

fiA Cluster-Ranking Approach Coreference Resolution

Feature Type
Lexical

Feature
str match

head match

Grammatical
(NP type)

uppercase
definite
demonstrative
indefinite
quantified
article

Grammatical
(NP
property/
relationship

pronoun
proper noun
bare singular
bare plural
embedded
appositive
prednom
number

contains pn
Grammatical
(Syntactic
Pattern)

n
2n
pn
pn n
adj n
num n
ne
sing n

Semantic

alias

Description
exists mention mj preceding mk that,
discarding determiners, mj mk string; else
N.
exists mention mj preceding mk mj
mk head; else N.
mk entirely uppercase; else N.
mk starts the; else N.
mk starts demonstrative this, that,
these, those; else N.
mk starts an; else N.
mk starts quantifiers every, some, all,
most, many, much, few, none; else N.
definite mk definite NP; quantified mk quantified NP; else indefinite.
mk pronoun; else N.
mk proper noun; else N.
mk singular start article; else N.
mk plural start article; else N.
mk prenominal modifier; else N.
mk first two mentions appositive
construction; else N.
mk first two mentions predicate nominal
construction; else N.
singular mk singular number; plural mk plural
number; unknown number information cannot
determined.
mk proper noun contains proper noun; else
N.
mk starts followed exactly one common
noun; else N.
mk starts followed exactly two common
nouns; else N.
mk starts followed exactly proper noun;
else N.
mk starts followed exactly proper noun
common noun; else N.
mk starts followed exactly adjective
common noun; else N.
mk starts followed exactly cardinal
common noun; else N.
mk starts followed exactly named entity;
else N.
mk starts followed singular NP containing proper noun; else N.
exists mention mj preceding mk mj
mk aliases; else N.

Table 2: Feature set anaphoricity determination. instance represents single mention,
mk , characterized 26 features.
485

fiRahman & Ng

pairing preceding clusters. test instances presented
ranker, mk linked cluster assigned highest value ranker. Ties
broken preferring cluster whose last mention closest distance mk . Note
partial clusters preceding mk formed incrementally based predictions
ranker first k 1 mentions.
4.2 Joint Anaphoricity Determination Coreference Resolution
cluster ranker described used determine preceding cluster
anaphoric mention linked to, cannot used determine whether mention anaphoric not. reason simple: training instances generated
anaphoric mentions. Hence, jointly learn anaphoricity determination coreference
resolution, must train ranker using instances generated anaphoric
non-anaphoric mentions.
Specifically, training ranker, provide active mention option
start new cluster creating additional instance (1) contains features solely
describe active mention (i.e., features shown second block Table 1), (2)
highest rank value among competing clusters (i.e., 2) non-anaphoric
lowest rank value (i.e., 1) otherwise. main advantage jointly learning two tasks
allows ranking model evaluate possible options active mention
(i.e., whether resolve it, so, preceding cluster best) simultaneously.
Essentially method applied jointly learn two tasks mentionranking model.
training, resulting cluster ranker processes mentions test text
left-to-right manner. active mention mk , create test instances pairing
preceding clusters. allow possibility mk non-anaphoric,
create additional test instance contains features solely describe active
mention (similar training step above). test instances
presented ranker. additional test instance assigned highest rank value
ranker, mk classified non-anaphoric resolved. Otherwise,
mk linked cluster highest rank, ties broken preferring
antecedent closest mk . before, partial clusters preceding mk formed
incrementally based predictions ranker first k 1 mentions.
Finally, note model jointly learning anaphoricity determination coreference resolution different recent attempts perform joint inference anaphoricity
determination coreference resolution using integer linear programming (ILP),
anaphoricity classifier coreference classifier trained independently other,
ILP applied postprocessing step jointly infer anaphoricity coreference decisions consistent (e.g., Denis & Baldridge, 2007a).
Joint inference different joint-learning approach, allows two tasks
learned jointly independently.

5. Lexicalization Coreference Resolution
Next, investigate role lexicalization (i.e., use word pairs linguistic features)
learning-based coreference resolution. motivation behind investigation two486

fiA Cluster-Ranking Approach Coreference Resolution

fold. First, lexical features easy compute yet under-investigated
coreference resolution. particular, attempts made employ
train mention-pair model (e.g., Luo et al., 2004; Daume III & Marcu, 2005;
Bengtson & Roth, 2008). contrast, want determine whether improve
performance cluster-ranking model. Second, mention-pair model
mention-ranking model compared respect non-lexical feature set
(Denis & Baldridge, 2007b, 2008), clear perform relative
trained lexical features. desire answer question,
allow us gain additional insights strengths weaknesses
learning-based coreference models.
Recall introduction previous attempts lexicalizing mention-pair
model show lexical features best marginally useful. Hence, one goals
determine whether make better use lexical features learning-based
coreference resolver. particular, unlike aforementioned attempts lexicalization,
simply append word pairs conventional coreference feature set consisting
string-matching, grammatical, semantic, distance (i.e., proximity-based) features (e.g.,
feature set shown Table 1), investigate model exploits lexical features
combination small subset conventional coreference features.
would allow us better understanding significance conventional
features. example, features encode agreement gender, number, semantic
class two mentions employed virtually learning-based coreference resolver,
never question whether better alternatives features. could
build lexicalized coreference model without commonly-used features
observe performance deterioration, would imply conventional features
replaceable, prototypical way building learning-based coreference
system.
question is: small subset conventional features use
combination lexical features? mentioned above, since one advantages
lexical features extremely easy compute, desire conventional
features easy compute, especially require dictionary
compute. see, choose use two features, alias feature
distance feature (see features 31 32 Table 1), rely off-the-shelf named
entity (NE) recognizer compute NE types.
Note, however, usefulness lexical features could limited part data
sparseness: many word pairs appear training data may appear test
data. employing conventional features described (e.g., distance)
help alleviate problem, seek improve generalizability introducing
two types features: semi-lexical unseen features. henceforth refer
feature set comprises two types features, lexical features, alias feature,
distance feature Lexical feature set. addition, refer feature
set shown Table 1 Conventional feature set.
first describe Lexical feature set training mention-pair model
mention-ranking model (Section 5.1). that, show create cluster-level
features feature set training entity-mention model cluster-ranking
487

fiRahman & Ng

model, well issues training joint model anaphoricity determination coreference resolution (Section 5.2).
5.1 Lexical Feature Set
Unlike previous work lexicalizing learning-based coreference models, Lexical feature
set consists four types features: lexical features, semi-lexical features, unseen features,
well two conventional features (namely, alias distance).
compute features, preprocess training text randomly replacing 10%
nominal mentions (i.e., common nouns) label unseen. mention mk
replaced unseen, mentions string mk replaced
unseen. test text preprocessed differently: simply replace mentions whose
strings seen training data unseen. Hence, artificially creating unseen
labels training text allow learner learn handle unseen words
test text, potentially improving generalizability.
preprocessing, compute features instance. Assuming
training mention-pair model mention-ranking model, instance corresponds
two mentions, mj mk , mj precedes mk text. features
divided four groups: unseen, lexical, semi-lexical, conventional. describing
features, two points deserve mention. First, least one mj mk unseen,
lexical, semi-lexical, conventional features created them, since features
involving unseen mention likely misleading learner sense
may yield incorrect generalizations training set. Second, since use SVM
training testing, instance contain number features, unless otherwise
stated, feature value 1.
Unseen feature. mj mk unseen, determine whether
string. so, create unseen-same feature; otherwise, create unseendiff feature. one unseen, feature created.
Lexical feature. create lexical feature mj mk , ordered
pair consisting heads mentions. pronoun common noun, head
assumed last word mention11 ; proper noun, head taken
entire noun phrase.
Semi-lexical features. features aim improve generalizability. Specifically,
exactly one mj mk tagged NE Stanford NE recognizer (Finkel et al.,
2005), create semi-lexical feature identical lexical feature described above,
except NE replaced NE label (i.e., person, location, organization).
mentions NEs, check whether string. so, create
feature *ne*-same, *ne* replaced corresponding NE label. Otherwise,
check whether NE tag word-subset match (i.e., whether
11. see evaluation section, mention extractor trained extract base NPs. Hence,
heuristic extracting head nouns arguably overly simplistic, applied
recursive NPs (e.g., NPs contain prepositional phrases), phrases likely
make mistakes. However, desire better extraction accuracy, extract head nouns
syntactic parsers provide head information, Collinss (1999) parser.

488

fiA Cluster-Ranking Approach Coreference Resolution

word tokens one mention appear others list tokens). so, create feature
*ne*-subsame, *ne* replaced NE label. Otherwise, create feature
concatenation NE labels two mentions.
Conventional features. improve generalizability, incorporate two easy-to
compute features Conventional feature set: alias distance.
5.2 Feature Generation
Lexical feature set training mention-pair model mentionranking model, describe two extensions feature set needed
(1) train entity-mention model cluster-ranking model, (2) perform joint
learning anaphoricity determination coreference resolution.
first extension concerns generation cluster-level features entity-mention
model cluster-level model. Recall Section 3.2 create cluster-level features given Conventional feature set, first convert feature employed
mention-pair model equivalent set binary-valued features, create
cluster-level feature resulting binary-valued features. hand,
given Lexical feature set, method producing cluster-level features applicable two conventional features (i.e., alias distance), appear
Conventional feature set. unseen, lexical, semi-lexical feature, create
feature active mention mention preceding cluster, described
Section 5.112 , value feature number times appears instance. Encoding feature values frequency rather binary values allows us capture
cluster-level information shallow manner.
second extension concerns generation features representing additional
instance created training joint version mention-ranking model
cluster-ranking model. Recall Section 4.2 Conventional feature set
used, represented additional instance using features computed solely
active mention. hand, given Lexical feature set, longer
use method representing additional instance, feature
Lexical feature set computed solely active mention. result,
represent additional instance using one feature, null-x, x head
active mention, help learner learn x likely non-anaphoric.

6. Evaluation
evaluation driven following questions, focusing (1) comparison among
different learning-based coreference models, (2) effect lexicalization
models. Specifically:
learning-based coreference models (namely, mention-pair model,
entity-mention model, mention-ranking model, cluster-ranking model)
compare other?
12. Strictly speaking, resulting feature cluster-level feature, computed active
mention one mentions preceding cluster.

489

fiRahman & Ng

joint modeling anaphoricity determination coreference resolution offer
benefits pipeline architecture, anaphoricity performed prior
coreference resolution?
lexicalized coreference models perform better unlexicalized counterparts?
rest section, first describe experimental setup (Section 6.1),
show performance four models, including effect lexicalization
joint modeling whenever applicable, three different feature sets (Section 6.2).
6.1 Experimental Setup
begin providing details data sets, automatic mention extraction
method, scoring programs.
6.1.1 Corpus
use ACE 2005 coreference corpus released LDC, consists 599
training documents used official ACE evaluation.13 ensure diversity, corpus
created selecting documents six different sources: Broadcast News (BN), Broadcast
Conversations (BC), Newswire (NW), Webblog (WB), Usenet (UN), Conversational
Telephone Speech (CTS). number documents belonging source shown
Table 3.
Data set
# documents

BN
226

BC
60

NW
106

WL
119

UN
49

CTS
39

Table 3: Statistics ACE 2005 corpus

6.1.2 Mention Extraction
evaluate coreference model using system mentions. extract system mentions
test text, trained mention extractor training texts. Following Florian
et al. (2004), recast mention extraction sequence labeling task, assign
token test text label indicates whether begins mention, inside
mention, outside mention. Hence, learn extractor, create one training
instance token training text derive class value (one b, i, o)
annotated data. instance represents wi , token consideration,
consists 29 linguistic features, many modeled systems Bikel,
Schwartz, Weischedel (1999) Florian et al. (2004), described below.
Lexical (7):

Tokens window 7: {wi3 , . . . , wi+3 }.

Capitalization (4): Determine whether wi IsAllCap, IsInitCap, IsCapPeriod,
IsAllLower.
13. Since participate ACE 2005, access official test set.

490

fiA Cluster-Ranking Approach Coreference Resolution

Morphological (8): wi prefixes suffixes length one, two, three, four.
Grammatical (1): part-of-speech (POS) tag wi obtained using Stanford loglinear POS tagger (Toutanova, Klein, Manning, & Singer, 2003).
Semantic (1): named entity (NE) tag wi obtained using Stanford CRF-based
NE recognizer (Finkel et al., 2005).
Dictionaries (8): employ eight dictionary-based features indicate presence
absence wi particular dictionary. eight dictionaries contain pronouns (77
entries), common words words names (399.6k), person names (83.6k),
person titles honorifics (761), vehicle words (226), location names (1.8k), company
names (77.6k), nouns extracted WordNet hyponyms person (6.3k).
employ CRF++14 , C++ implementation conditional random fields, training
mention detector training set. Overall, detector achieves F-measure
86.7 (86.1 recall, 87.2 precision) test set. extracted mentions used
system mentions coreference experiments.
6.1.3 Scoring Programs
score output coreference model, employ two scoring programs, B3 (Bagga
& Baldwin, 1998) 3 -CEAF15 (Luo, 2005), address inherent weaknesses
MUC scoring program (Vilain, Burger, Aberdeen, Connolly, & Hirschman, 1995).16
B3 CEAF score response (i.e., system-generated) partition, R, key
(i.e., gold-standard) partition, K, report coreference performance terms recall,
precision, F-measure. B3 first computes recall precision mention, mk ,
follows:
recall(mk ) =

|Rmk Kmk |
|Rmk Kmk |
, precision(mk ) =
,
|Kmk |
|Rmk |

Rmk coreference cluster containing mk R, Kmk coreference cluster
containing mk K. computes overall recall (resp. precision) averaging
per-mention recall (resp. precision) scores.
hand, CEAF first constructs optimal one-to-one mapping
clusters key partition response partition. Specifically, assume
K = {K1 , K2 , . . . , Km } set clusters key partition, R = {R1 , R2 , . . . , Rn }
set clusters response partition. compute recall, CEAF first computes
score cluster, Ki , K follows:
score(Ki ) = |Ki Rj |,
14. Available http://crfpp.sourceforge.net
15. CEAF two versions: 3 -CEAF 4 -CEAF. two versions differ similarity two
aligned clusters computed. refer reader Luos (2005) paper details. 3 -CEAF chosen
commonly-used version CEAF.
16. Briefly, MUC scoring program suffers two often-cited weaknesses. First, link-based measure,
reward successful identification singleton clusters, since mentions clusters
linked mentions. Second, tends under-penalize partitions overly large clusters.
See work Bagga Baldwin (1998), Luo (2005), Recasens Hovy (2011) details.

491

fiRahman & Ng

Rj cluster Ki mapped optimal one-to-one mapping,
constructed efficiently using Kuhn-Munkres algorithm (Kuhn, 1955). Note
Ki mapped cluster R, score(Ki ) = 0. CEAF computes recall
summing score cluster K dividing sum number mentions
K. Precision computed manner, except reverse roles
K R.
complication arises B3 used score response partition containing system
mentions. Recall B3 constructs mapping mentions response
key. Hence, response generated using gold-standard mentions,
every mention response mapped mention key vice versa.
words, twinless (i.e., unmapped) mentions (Stoyanov et al., 2009).
case system mentions used, original description B3
specify twinless mentions scored (Bagga & Baldwin, 1998). address
problem, set per-mention recall precision twinless mention zero,
regardless whether mention appears key response. Note CEAF
compare partitions twinless mentions without modification, since operates
aligning clusters, mentions.
Additionally, apply preprocessing step response partition scoring it:
remove twinless system mentions singletons. reason
simple: since coreference resolver successfully identified mentions singletons,
penalized, removing allows us avoid penalty. Note
remove twinless (as opposed all) system mentions singletons: allows
us reward resolver successful identification singleton mentions twins.
hand, retain (1) twinless system mentions non-singletons (as
resolver penalized identifying spurious coreference relations) (2) twinless
mentions key partition (as want ensure resolver makes correct
coreference non-coreference decisions them).17
6.2 Results
showing results learning-based coreference models, let us consider head
match baseline, commonly-used heuristic baseline coreference resolution.
posits two mentions coreferent head nouns match. Head nouns
determined described Section 5.1: head proper noun string entire
mention, whereas head pronoun common noun last word mention.
Since one goals examine effect lexicalization coreference model,
head match baseline provide information well one simplest
kinds string matching. Results baseline, shown row 1 Table 4, expressed
terms recall (R), precision (P), F-measure (F) obtained via B3 CEAF.
see Table 4, baseline achieves F-measure scores 54.9 49.6 according
B3 CEAF, respectively.
17. addition method described here, number methods proposed address
mapping problem. refer reader work Enrique, Gonzalo, Artiles, Verdejo (2009),
Stoyanov et al. (2009), Cai Strube (2010) details.

492

fiA Cluster-Ranking Approach Coreference Resolution

Next, train evaluate learning-based coreference models using five-fold cross
validation. data set si shown Table 3, partition documents si
five folds approximately equal size, si1 , . . . , si5 . train coreference model
four folds use generate coreference chains documents remaining
fold, repeating step five times fold used test fold exactly once.
that, apply B3 CEAF entire set automatically coreference-annotated
documents obtain scores Table 4. discuss results learningbased coreference models obtained used combination three feature sets:
Conventional feature set (Section 6.2.1), Lexical feature set (Section 6.2.2),
Combined feature set, composed features Conventional Lexical
(Section 6.2.3).
6.2.1 Results Using Conventional Features
gauge performance cluster-ranking model, employ baselines mentionpair model, entity-mention model, mention-ranking model.
mention-pair baseline. train first learning-based baseline, mentionpair model, using SVM learning algorithm implemented SVMlight package.18
see row 2 Table 4, mention-pair model achieves F-measure scores
58.6 (B3 ) 54.4 (CEAF), represent statistically significant improvement 3.7%
4.8% F-measure corresponding results head match baseline.19
entity-mention baseline. Next, train second learning-based baseline,
entity-mention model, using SVM learner. see row 3 Table 4,
baseline achieves F-measure scores 58.9 (B3 ) 54.8 (CEAF), represent small
statistically significant improvements mention-pair model. significant performance difference perhaps particularly surprising given improved expressiveness
entity-mention model mention-pair model.
mention-ranking baseline. third baseline mention-ranking model,
trained using ranker-learning algorithm SVMlight . identify non-anaphoric mentions, employ two methods. first method, follow Denis Baldridge (2008)
adopt pipeline architecture, train MaxEnt classifier anaphoricity determination independently mention ranker training set using 26 features
described Section 3.3. apply resulting classifier test text filter nonanaphoric mentions prior coreference resolution. Results pipeline mention ranker
shown row 4 Table 4. see, ranker achieves F-measure scores 57.7
(B3 ) 53.0 (CEAF), yielding significant performance deterioration comparison
entity-mention baseline.
second method, perform anaphoricity determination jointly coreference
resolution using method described Section 4.2. discussed joint learning
method context cluster ranking, easy see method
equally applicable mention-ranking model. Results mention ranker using
18. subsequent uses SVM learner, set parameters default values.
particular, employ linear kernel obtain results article.
19. statistical significance results article obtained using paired t-test, p < 0.05.

493

fiRahman & Ng

R
44.1

B3
P
72.9

CEAF
P
F
60.8 49.6

1

Coreference Model
Head match

F
54.9

R
41.9

2
3
4
5
6
7

Using Conventional feature set
Mention-pair model
49.7 71.4 58.6
Entity-mention model
49.9 71.7 58.9
Mention-ranking model (Pipeline)
48.1 72.1 57.7
Mention-ranking model (Joint)
49.1 76.1 59.7
Cluster-ranking model (Pipeline)
49.9 71.6 58.8
Cluster-ranking model (Joint)
51.1 73.3 60.2

49.5
51.0
51.7
52.9
53.4
54.1

60.5
59.2
54.4
59.2
54.6
60.2

54.4
54.8
53.0
55.9
54.0
57.0

8
9
10
11
12
13

Using Lexical feature set
Mention-pair model
53.0 75.3 62.2
Entity-mention model
53.1 75.8 62.5
Mention-ranking model (Pipeline)
55.7 67.5 61.0
Mention-ranking model (Joint)
56.6 73.1 63.8
Cluster-ranking model (Pipeline)
51.0 67.1 58.0
Cluster-ranking model (Joint)
51.3 75.7 61.1

55.6
55.7
56.3
58.5
53.1
53.3

62.0
62.2
62.0
65.0
55.3
58.6

58.6
58.8
59.0
61.6
54.1
55.8

14
15
16
17
18
19

Using Combined
Mention-pair model
50.4
Entity-mention model
50.5
Mention-ranking model (Pipeline)
50.6
Mention-ranking model (Joint)
49.9
Cluster-ranking model (Pipeline)
52.9
Cluster-ranking model (Joint)
54.3

53.8
54.1
54.1
54.7
57.5
57.6

61.9
62.3
61.8
61.4
62.1
64.3

57.5
57.9
57.7
57.9
59.7
60.8

feature
73.1
73.4
74.6
79.3
70.9
75.1

set
59.6
59.8
60.3
61.3
60.6
63.0

Table 4: Five-fold cross-validation coreference results obtained using B3 CEAF.
best F-measure achieved feature set/scoring program combination boldfaced.

joint architecture shown row 5 Table 4. see, ranker achieves Fmeasure scores 59.7 (B3 ) 55.9 (CEAF), represent significant improvements
entity-mention model pipeline counterpart. results
demonstrate superiority joint mention-ranking model entity-mention model,
substantiate hypothesis joint modeling offers benefits pipeline modeling.
cluster-ranking model. Finally, evaluate cluster-ranking model.
mention-ranking baselines, employ pipeline architecture joint architecture anaphoricity determination. Results shown rows 6 7 Table 4,
respectively, two architectures. see, pipeline architecture yields Fmeasure scores 58.8 (B3 ) 54.0 (CEAF), represent significant improvement
mention ranker adopting pipeline architecture. joint architecture,
cluster ranker achieves F-measure scores 60.2 (B3 ) 57.0 (CEAF). rep494

fiA Cluster-Ranking Approach Coreference Resolution

resents significant improvement mention ranker adopting joint architecture,
best baselines. Taken together, results demonstrate superiority
cluster ranker mention ranker. Finally, fact joint cluster ranker performs
significantly better pipeline counterpart provides empirical support
benefits joint modeling pipeline modeling.
6.2.2 Results Using Lexical Features
Next, evaluate learning-based coreference models using Lexical features. Results
shown rows 813 Table 4. comparison results obtained using Conventional features, see different trend: joint mention-ranking model replaces
cluster-ranking model best-performing model. Moreover, improvement
second best-performing model, entity-mention model according B3
pipeline mention-ranking model according CEAF, statistically significant regardless
scoring program used. closer examination results reveals employing
Lexical rather Conventional features substantially improves performance
mention-ranking model: comparison unlexicalized joint mention-ranking model
(row 5), F-measure scores lexicalized joint mention-ranking model (row 11) rise
4.1% (B3 ) 5.7% (CEAF). increase F-measure attributed primarily
substantial rise recall, even though large increase CEAF precision.
Besides joint mention-ranking model, mention-pair model entity-mention
model benefit substantially Conventional features replaced Lexical features: see F-measure scores increase 3.6% (B3 ) 4.2% (CEAF)
mention-pair model, 3.6% (B3 ) 4.0% (CEAF) entity-mention model.
gains F-measure two models attributed large increases
recall precision. hand, joint cluster-ranking model always
improve replace Conventional features Lexical features. fact,
performance difference cluster-ranking model entity-mention model
statistically indistinguishable. Finally, see benefits jointly learning anaphoricity
determination coreference resolution again: joint version mentionranking model used rather pipeline version (compare rows 10 11),
F-measure scores rise significantly 2.8% (B3 ) 2.6% (CEAF). Similarly clusterranking model: joint version improves pipeline version significantly 3.1% (B3 )
1.7% (CEAF) F-measure.
Overall, results somewhat unexpected: recall Lexical features
knowledge-lean, consisting lexical, semi-lexical, unseen features, well
two Conventional features. particular, employ conventional coreference
features encode agreement gender number. implies many existing
implementations mention-pair model, entity-mention model, mentionranking model, unlexicalized rely heavily conventional features,
making effective use labeled data. Perhaps importantly, results indicate
coreference models perform well (and fact better) even without conventional
coreference features. Since Lexical computed extremely easily,
readily applied languages, another advantage feature set.
hand, interesting see versions cluster-ranking model exhibit
495

fiRahman & Ng

less dramatic changes performance replace Conventional features
Lexical features.
6.2.3 Results Using Combined Features
Since Conventional features Lexical features represent two fairly different sources
knowledge, examine whether improve coreference models combining
two feature sets. Results coreference models using Combined features
shown rows 1419 Table 4. results exhibit essentially trend
obtained Conventional features, joint cluster-ranking model performing
best mention-pair model performing worst. fact, joint cluster-ranking
model yields significantly better performance used Combined features
Conventional features Lexical features alone. Similarly pipeline
cluster-ranking model, achieves significantly better performance Combined
features Conventional Lexical features. results seem suggest
cluster-ranking model able exploit potentially different sources information
provided two feature sets improve performance. addition, demonstrate
benefits joint modeling: mention-ranking model, joint version improves
pipeline version significantly 1.0% (B3 ) 0.2% (CEAF) F-measure;
cluster-ranking model, joint version improves pipeline counterpart significantly
2.4% (B3 ) 1.1% (CEAF) F-measure.
remaining coreference models exhibit drop performance Combined
features used lieu Lexical features. results seem suggest
cluster-ranking model offers robust performance face changes underlying feature set coreference models, feature selection, issue
under-explored coreference resolution, may crucial employ coreference models.20 Perhaps importantly, despite fact Conventional features
Lexical features represent two fairly different sources information,
cluster-ranking model unable exploit potentially richer amount information
contained Combined feature set. Hence, virtually linguistic features
recently developed supervised coreference resolution evaluated using
mention-pair model (see, example, work Strube, Rapp, & Muller, 2002; Ji,
Westbrook, & Grishman, 2005; Ponzetto & Strube, 2006), utility features may
better demonstrated using cluster-ranking model.
natural question is: joint cluster-ranking model compare existing
coreference systems? Since participate ACE evaluations,
access official test sets compare model ACE
participating coreference systems. comparison complicated fact
existing coreference systems evaluated different data sets, including two
MUC data sets (MUC-6, 1995; MUC-7, 1998) various ACE data sets (e.g., ACE-2,
ACE 2003, ACE 2004, ACE 2005), well different partitions given data set.
knowledge, coreference model evaluated test
data Haghighi Kleins (2010) unsupervised coreference model. model
20. fact, Ng Cardie (2002b), Strube Muller (2003), Ponzetto Strube (2006) show
mention-pair model improved using feature selection.

496

fiA Cluster-Ranking Approach Coreference Resolution

recently shown surpass performance Stoyanov et al.s (2009) system,
one best existing implementations mention-pair model. test
data, Haghighi Kleins model achieves B3 F-measure 62.7, achieves
B3 F-measure 62.8.21 results provide suggestive evidence cluster-ranking
model achieves performance comparable one best existing coreference
models.
Nevertheless, caution results allow one claim anything
fact model compares favorably Haghighi Kleins (2010) model.
instance, one cannot claim model better achieves level
performance without using labeled data. reasons (1) mentions
used two models coreference process extracted differently (2)
linguistic features employed two models way features computed
different other. Since previous work shown linguistic
preprocessing steps considerable impact performance resolver (Barbu
& Mitkov, 2001; Stoyanov et al., 2009), possible one model employed features
mentions model currently using, results would different.
Hence, one fairly compare two coreference models, evaluated
set mentions (rather set documents) given access
set knowledge sources, essentially way compare various
learning-based coreference models article.

7. Experimental Analyses
attempt gain insights different aspects coreference models,
conduct additional experiments analyses. Rather report five-fold cross-validation
results, section report results one fold (i.e., fold designate test
set) use remaining four folds solely training.
7.1 Improving Classification-Based Coreference Models
Given generally poorer performance classification-based coreference models, natural question is: improved? answer question, investigate whether
models improved employing different clustering algorithm different
learning algorithm. reasons decision focus two dimensions.
First, noted introduction, one weaknesses models
clear clustering algorithm offers best performance. Given observation,
examine whether improve models replacing Soon et al.s (2001)
closest-first linking regime best-first linking strategy, shown
offer better performance mention-pair model MUC data sets (Ng & Cardie,
2002b). Second, discussed end Section 2, may able achieve
advantage ranking classification-based models employing learning algorithm
optimizes conditional probabilities instead 0/1 decisions. Motivated observation, examine whether improve classification-based models training
using MaxEnt, employs likelihood-based loss function. Note MaxEnt one
21. Note Haghighi Klein report CEAF scores paper.

497

fiRahman & Ng

popular learning algorithms training coreference models (see, example,
Morton, 2000; Kehler, Appelt, Taylor, & Simma, 2004; Ponzetto & Strube, 2006; Denis &
Baldridge, 2008; Finkel & Manning, 2008; Ng, 2009).
evaluate two modifications, apply isolation combination
two classification-based models (i.e., mention-pair model entity-mention
model) trained using three different feature sets (i.e., Conventional, Lexical, Combined). train MaxEnt-based coreference models using YASMET22 ,
follow Ng Cardies (2002b) implementation best-first clustering algorithm.
Specifically, among candidate antecedents preceding clusters classified
coreferent active mention mk , best-first clustering links mk likely one.
MaxEnt model, pair classified coreferent classification value
0.5, likely antecedent/preceding cluster mk one
highest probability coreference mk . SVMlight -trained model, pair
classified coreferent classification value 0, likely
antecedent/preceding cluster mk one positive classification
value.
Table 5 presents B3 CEAF results two classification-based coreference models
trained using two learning algorithms (i.e., SVM MaxEnt) used
combination two clustering algorithms (i.e., closest-first clustering best-first
clustering). study choice clustering algorithm impacts performance,
compare results closest-first clustering best-first clustering Table 5
combination learning algorithm, feature set, coreference model, scoring
program. instance, comparing rows 1 2 Table 5 enables us examine
two clustering algorithms better mention-pair model trained
Conventional feature set two learners. Overall, see fairly consistent
trend: best-first clustering yields results slightly worse obtained using
closest-first clustering, regardless choice clustering algorithm, learning
algorithm, feature set, scoring program. first glance, results seem
contradictory Ng Cardie (2002b), demonstrate superiority bestfirst clustering closest-first clustering coreference resolution. speculate
contradictory results attributed two reasons. First, best-first clustering
experiments, still employed Soon et al.s (2001) training instance selection method,
created positive training instance anaphoric mention closest
antecedent/preceding cluster, unlike Ng Cardie, claim proposed bestfirst clustering successful, however, different method training instance selection
would needed. particular, propose use confident antecedent,
rather closest antecedent, generate positive instances anaphoric mention.
Second, Ng Cardie demonstrate success best-first clustering MUC data
sets, possible success may carry ACE data sets. Additional
experiments needed determine reason, however.
22. See http://www.fjoch.com/YASMET.html. reason YASMET chosen provides
capability rank, allows us compare results MaxEnt-trained classification models
ranking models. See work Ravichandran, Hovy, Och (2003) discussion differences
training two types MaxEnt models.

498

fiA Cluster-Ranking Approach Coreference Resolution

Coreference Model

R

SVM
P

F

R

MaxEnt
P
F

1
2
3
4

B3 results using Conventional feature
Mention-pair model (Closest first)
46.2 72.0 56.2
Mention-pair model (Best first)
45.7 71.0 55.6
Entity-mention model (Closest first) 46.8 72.5 56.8
Entity-mention model (Best first)
46.3 72.1 56.3

set
59.6
59.2
59.7
59.3

55.3
54.8
55.9
55.1

57.3
56.9
57.7
57.1

5
6
7
8

B3 results using Lexical feature set
Mention-pair model (Closest first)
52.8 73.0 61.2
Mention-pair model (Best first)
52.1 72.1 60.5
Entity-mention model (Closest first) 52.8 73.6 61.2
Entity-mention model (Best first)
52.4 72.2 60.8

52.8
52.1
52.8
52.2

64.6
64.2
64.6
64.3

58.1
57.5
58.2
57.6

Combined feature set
49.1 73.2 58.8
50.3
48.7 72.8 58.3
49.3
49.5 73.2 59.1 50.5
49.1 72.7 58.6
50.1

65.9
65.2
66.1
65.6

57.0
56.1
57.3
56.8

13
14
15
16

CEAF results using Conventional feature set
Mention-pair model (Closest first)
48.5 55.3 51.6
51.4
Mention-pair model (Best first)
48.1 54.9 51.2
51.1
Entity-mention model (Closest first) 49.5 55.8 52.5
51.4
Entity-mention model (Best first)
49.2 55.1 51.9
51.1

56.5
56.1
56.7
56.2

53.8
53.4
53.9
53.5

17
18
19
20

CEAF results using
Mention-pair model (Closest first)
Mention-pair model (Best first)
Entity-mention model (Closest first)
Entity-mention model (Best first)

56.8
56.2
57.4
56.9

55.1
54.6
55.3
54.9

21
22
23
24

CEAF results using Combined
Mention-pair model (Closest first)
53.8 60.0
Mention-pair model (Best first)
53.1 59.7
Entity-mention model (Closest first) 54.1 60.9
Entity-mention model (Best first)
53.7 60.3

56.3
55.8
56.8
56.2

55.5
55.0
55.9
55.3

9
10
11
12

B3 results using
Mention-pair model (Closest first)
Mention-pair model (Best first)
Entity-mention model (Closest first)
Entity-mention model (Best first)

(a) B3 results

Lexical feature set
54.6 61.2 57.7
53.5
54.2 60.7 57.3
53.1
54.9 61.7 58.1 53.5
54.5 61.1 57.6
53.1
feature
56.7
56.2
57.3
56.8

set
54.9
54.3
55.1
54.5

(b) CEAF3 results

Table 5: SVM vs. MaxEnt results classification-based coreference models. one-fold
B3 CEAF scores obtained training coreference models using SVM MaxEnt.
best F-measure achieved feature set/scoring program combination boldfaced.
499

fiRahman & Ng

Next, examine whether minimizing likelihood-based loss via MaxEnt training instead
SVMs classification loss would enable us achieve advantage ranking
(and hence leads better performance), compare two columns Table 5.
see, Conventional feature set used, MaxEnt outperforms SVM, regardless
choice clustering algorithm, scoring program, coreference model.
hand, Lexical features Combined features used, SVM outperforms
MaxEnt consistently. Overall, mixed results seem suggest whether MaxEnt
offers better performance SVM extent dependent underlying feature
set.
7.2 Performance Maximum-Entropy-Based Ranking Models
prior work suggests MaxEnt-based ranking may provide better gains SVMbased ranking, since generate reliable confidence values dynamically adjust
relative ranks according baseline results (e.g., Ji, Rudin, & Grishman, 2006). determine whether case coreference resolution, conduct experiments
train ranking-based coreference models using ranker-learning algorithm YASMET.
B3 CEAF results mention-ranking model cluster-ranking model
trained using MaxEnt combination three different feature sets (i.e., Conventional,
Lexical, Combined) shown MaxEnt column Table 6. comparison,
show corresponding results obtained via SVM-based ranking table
(see SVM column). Comparing two columns, see mixed results: 24
experiments involve ranking models, MaxEnt-based ranking outperforms SVM-based
ranking six them. words, results suggest coreference task,
SVM-based ranking generally better MaxEnt-based ranking.
7.3 Accuracy Anaphoricity Determination
Section 6.2, saw joint ranking model always performs significantly better
pipeline counterpart. words, joint modeling coreference anaphoricity
improves coreference resolution. natural question is: joint modeling improve
anaphoricity determination?
answer question, measure accuracy anaphoricity information resulting pipeline modeling joint modeling. Recall pipeline modeling, rely
output anaphoricity classifier trained independently coreference
system uses anaphoricity information (see Section 3.3). accuracy classifier test set shown Acc column row 1 Table 7. addition,
show table recall (R), precision (P), F-measure (F) identifying anaphoric
mentions. see, classifier achieves accuracy 81.1 F-measure score
83.8.
hand, joint modeling, compute accuracy anaphoricity
determination output joint coreference model. Specifically, given output
joint model, determine mentions resolved preceding antecedent
not. Assuming mention resolved anaphoric one
resolved non-anaphoric, compute accuracy anaphoricity determination
500

fiA Cluster-Ranking Approach Coreference Resolution

Coreference Model

R

SVM
P

F

R

MaxEnt
P
F

1
2
3
4

B3 results using Conventional feature
Mention-ranking model (Pipeline)
46.7 71.5 56.5
Mention-ranking model (Joint)
47.6 74.8 58.2
Cluster-ranking model (Pipeline)
53.6 59.5 56.4
Cluster-ranking model (Joint)
52.2 73.8 61.2

set
58.7
59.1
51.7
52.1

59.1
59.3
69.9
70.6

58.8
59.2
59.3
60.0

5
6
7
8

B3 results using Lexical feature set
Mention-ranking model (Pipeline)
53.8 68.3 60.1
Mention-ranking model (Joint)
54.6 72.8 62.4
Cluster-ranking model (Pipeline)
51.7 68.2 58.8
Cluster-ranking model (Joint)
52.9 73.4 61.5

56.6
56.3
48.4
48.0

61.1
64.4
66.6
72.9

58.8
60.1
56.1
57.8

Combined feature set
49.8 72.6 59.1
51.4
50.5 77.6 61.2
52.5
53.8 71.2 61.3
54.1
54.4 74.8 62.8 54.5

68.3
70.3
67.5
68.3

58.7
60.1
60.1
60.6

Conventional feature set
49.4 55.7 52.4
51.5
50.5 56.3 53.2
51.8
53.6 59.5 56.4
53.1
55.2 61.6 58.2 53.2

56.6
56.9
58.7
59.3

53.9
54.2
55.8
56.1

9
10
11
12

B3 results using
Mention-ranking model (Pipeline)
Mention-ranking model (Joint)
Cluster-ranking model (Pipeline)
Cluster-ranking model (Joint)

(a) B3 results

13
14
15
16

CEAF results using
Mention-ranking model (Pipeline)
Mention-ranking model (Joint)
Cluster-ranking model (Pipeline)
Cluster-ranking model (Joint)

17
18
19
20

CEAF results using
Mention-ranking model (Pipeline)
Mention-ranking model (Joint)
Cluster-ranking model (Pipeline)
Cluster-ranking model (Joint)

Lexical feature set
54.7 59.8 57.1
55.7
56.9 63.3 59.9 55.4
52.7 58.4 55.4
50.9
55.0 61.4 58.1
50.5

56.6
60.7
52.3
56.6

56.1
57.9
51.5
53.3

21
22
23
24

CEAF results
Mention-ranking model (Pipeline)
Mention-ranking model (Joint)
Cluster-ranking model (Pipeline)
Cluster-ranking model (Joint)

Combined feature set
53.7 58.8 56.1
54.9 61.7 58.1
55.1 60.1 57.4
58.4 65.1 61.6

58.7
56.3
60.0
62.5

55.9
55.5
57.9
59.5

53.4
54.9
56.1
56.7

(b) CEAF results

Table 6: SVM vs. MaxEnt results ranking-basd coreference models. one-fold B3
CEAF scores obtained training coreference models using SVM MaxEnt. best
F-measure achieved feature set/scoring program combination boldfaced.
501

fiRahman & Ng

1
2
3
4
5
6
7

Source Anaphoricity Information
Anaphoricity Classifier
Mention-ranking (Conventional)
Cluster-ranking (Conventional)
Mention-ranking (Lexical)
Cluster-ranking (Lexical)
Mention-ranking (Combined)
Cluster-ranking (Combined)

Acc
81.1
78.7
81.9
84.2
83.1
79.1
83.1

R
87.6
83.1
87.8
88.3
87.9
84.3
87.4

P
80.3
79.3
80.4
82.1
81.8
79.1
82.1

F
83.8
81.2
83.9
85.1
84.7
81.6
84.6

Table 7: Anaphoricity determination results.

well precision, recall, F-measure identifying anaphoric mentions. Since
performance numbers derived output joint model, compute
two joint ranking models (i.e., mention-ranking model
cluster-ranking model) used combination three coreference feature
sets (i.e., Conventional, Lexical, Combined). results six sets performance
numbers, shown rows 27 Table 4. see, accuracies range
78.7 84.2, F-measure scores range 81.2 85.1.
comparison results anaphoricity classifier shown row 1, see
joint modeling improves performance anaphoricity determination except two
cases, namely, mention-ranking/Conventional mention-ranking/Combined.
words, two cases, joint modeling benefits coreference resolution anaphoricity determination. seems counter-intuitive one achieve better coreference
performance lower accuracy determining anaphoricity, difficult
see reason: joint model trained maximize pairwise ranking accuracy,
presumably correlates coreference performance, whereas anaphoricity classifier trained maximize accuracy determining anaphoricity mention,
may always correlation coreference performance. words,
improvements anaphoricity accuracy generally necessarily imply corresponding
improvements clustering-level coreference accuracy.
Finally, important bear mind conclusions drawn regarding
pipeline joint modeling based results anaphoricity classifier trained
26 features. possible different conclusions could drawn trained
anaphoricity classifier different set features. Therefore, interesting future direction
would improve anaphoricity classifier employing additional features,
proposed Uryupina (2003). may able derive sophisticated features
harnessing recent advances lexical semantics research, specifically using methods
phrase clustering (e.g., Lin & Wu, 2009), lexical chain discovery (e.g., Morris & Hirst,
1991), paraphrase discovery (see survey papers Androutsopoulos & Malakasiotis,
2010; Madnani & Dorr, 2010).
502

fiA Cluster-Ranking Approach Coreference Resolution

7.4 Joint Inference Versus Joint Learning Mention-Pair Model
mentioned end Section 4.2, joint modeling anaphoricity determination
coreference resolution fundamentally different joint inference two tasks.
Recall joint inference using ILP, anaphoricity classifier coreference classifier
trained independently other, ILP applied postprocessing step
jointly infer anaphoricity coreference decisions consistent
(e.g., Denis & Baldridge, 2007a). subsection, investigate joint learning
compares joint inference anaphoricity determination coreference resolution.
Let us begin overview ILP approach proposed Denis Baldridge
(2007a) joint inference anaphoricity determination coreference resolution.
ILP approach motivated observation output anaphoricity model
coreference model given document satisfy certain constraints.
instance, coreference model determines mention mk coreferent
mentions associated text, anaphoricity model determine
mk non-anaphoric. practice, however, since two models trained independently
other, constraints cannot enforced.
Denis Baldridge (2007a) provide ILP framework jointly determining anaphoricity coreference decisions given set mentions based probabilities provided
anaphoricity model PA mention-pair coreference model PC ,
resulting joint decisions satisfy desired constraints respecting much possible probabilistic decisions made independently-trained PA PC . Specifically, ILP program composed objective function optimized subject
set linear constraints, created test text follows. Let
set mentions D, P set mention pairs formed (i.e., P =
{(mj , mk ) | mj , mk M, j < k}). ILP program set indicator variables.
case, one binary-valued variable anaphoricity decision coreference
decision made ILP solver. Following Denis Baldridges notation, use
yk denote anaphoricity decision mention mk , xhj,ki denote coreference
decision involving mentions mj mk . addition, variable associated
assignment cost. Specifically, let cC
hj,ki = log(PC (mj , mk )) cost setting xhj,ki
C
1, chj,ki = log(1 PC (mj , mk )) complementary cost setting xhj,ki 0.
similarly define cost associated yk , letting cA
k = log(PA (mk ))
=

log(1

P
(m
))


complementary
cost setting
cost setting yk 1, cA

k
k
yk 0. Given costs, aim optimize following objective function:
min

X

C
cC
hj,ki xhj,ki + chj,ki (1 xhj,ki ) +

X


cA
k yk + ck (1 yk )

mk

(mj ,mk )P

subject set manually-specified linear constraints. Denis Baldridge specify four
types constraints: (1) indicator variable take value 0 1; (2) mj
mk coreferent (xhj,ki =1), mk anaphoric (yk =1); (3) mk anaphoric (yk =1),
must coreferent preceding mention mj ; (4) mk non-anaphoric,
cannot coreferent mention.
Two points deserve mention. First, minimizing objective function, since
assignment cost expressed negative logarithm value. Second, since transitivity
503

fiRahman & Ng

guaranteed constraints23 , use closest-link clustering algorithm
put two mentions posited coreferent cluster. Note
best-link clustering strategy applicable here, since binary decision assigned
pair mentions ILP solver. use lp solve24 , publicly-available ILP solver,
solve program.
B3 CEAF results performing joint inference outputs anaphoricity
model mention-pair model using ILP shown Joint Inference column
Tables 8a 8b, respectively, rows correspond results obtained training
coreference models different feature sets. Since one goals compare joint
inference joint learning, show Joint Learning column results
joint mention-ranking model, anaphoricity determination coreference resolution
learned joint fashion. Note reason using mention-ranking model
(rather cluster-ranking model) joint model want ensure
fair comparison joint learning joint inference much possible: chosen
cluster-ranking model joint model, difference joint learning results
joint inference results could caused increased expressiveness
cluster-ranking model. Finally, better understand whether mention-pair model
benefits joint inference using ILP, show Inference column relevant
mention-pair model results Table 4, output model postprocessed
inference mechanism.
Table 8, see joint learning results substantially better
joint inference results, except one case (Conventional/CEAF), two achieve
comparable performance. Previous work Roth (2002) Roth Yih (2004)
suggested often effective learn simple local models use complicated
integration strategies make sure constraints output satisfied learn
models satisfy constraints directly. results imply true
coreference task.
Comparing joint inference Inference results Table 8, see
mention-pair model benefit application ILP. fact, performance
deteriorates ILP used. results inconsistent reported Denis
Baldridge (2007a), show joint inference using ILP improve mentionpair model. speculate inconsistency accures fact Denis
Baldridge evaluate ILP approach true mentions (i.e., gold-standard mentions),
evaluate system mentions. Additional experiments needed determine
reason, however.
7.5 Data Source Adaptability
One may argue since train test model documents data
source (i.e., model trained documents BC tested documents
23. Finkel Manning (2008) show formulate linear constraints ILP solver outputs
coreference decisions satisfy transitivity. However, since number additional constraints needed
guarantee transitivity grows cubically number mentions previous work shows
additional constraints yield substantial performance improvements applied
system mentions (Ng, 2009), decided employ experiments.
24. Available http://lpsolve.sourceforge.net/

504

fiA Cluster-Ranking Approach Coreference Resolution

1
2
3

Feature Set
Conventional
Lexical
Combined

Joint Learning
R
P
F
47.6 74.8 58.2
54.6 72.8 62.4
50.5 77.6 61.2

Joint Inference
R
P
F
58.2 55.9 57.0
49.1 70.1 57.8
53.2 56.9 54.9


R
59.6
52.8
50.3

Inference
P
F
55.3 57.3
64.6 58.1
65.9 57.0


R
51.4
53.5
54.9

Inference
P
F
56.5 53.8
56.8 55.1
56.3 55.5

(a) B3 results

1
2
3

Feature Set
Conventional
Lexical
Combined

Joint Learning
R
P
F
50.5 56.3 53.2
56.9 63.3 59.9
54.9 61.7 58.1

Joint Inference
R
P
F
49.7 57.5 53.3
50.6 58.6 54.3
53.2 56.9 54.9

(b) CEAF results

Table 8: Joint learning vs. joint inference results. joint modeling results obtained
using mention-ranking model. joint inference results obtained applying ILP
anaphoricity classifier mention-pair model. inference results produced
mention-pair model. coreference models trained using MaxEnt.

BC, example), surprising lexicalization helps, since word pairs
training set likely found test set training test texts
data source. examine whether models employ Lexical features
suffer trained tested different data sources, perform set data
source adaptability experiments, apply coreference model trained
Lexical features documents one data source documents data sources.
Here, show results obtained using mention-ranking model, primarily
yielded best performance Lexical features among learning-based coreference
models. comparison, show data source adaptability results obtained using
mention-ranking model trained (non-lexical) Conventional feature set.
B3 CEAF F-measure scores experiments shown Tables 9a
9b, left half right half table contain lexicalized mention-ranking
model results unlexicalized mention-ranking model results, respectively. row
corresponds data source model trained, except last two rows,
explain shortly. column corresponds test set particular data
source.
answer question whether performance coreference model employs
Lexical features deteriorate trained tested different data sources,
look diagonal entries left half Tables 9a 9b, contain
results obtained lexicalized mention-ranking model trained tested
documents source. model indeed performs worse trained
tested documents different sources, diagonal entry contain
highest score among entries column. see left half
two tables, large extent correct: four six diagonal entries contain
highest scores respective columns according scoring programs. provides
505

fiRahman & Ng

Lexical features
PP
PP Test
Train PPP
P
BC
BN
CTS
NW
UN
WL
MaxMin
Std. Dev.

Conventional features

BC

BN

CTS NW UN

WL

BC

BN

CTS NW UN

WL

56.5
57.6
55.5
55.6
56.4
56.4
2.1
0.76

61.0
63.5
61.1
62.1
62.8
62.8
2.5
1.01

58.7
60.7
62.7
56.7
60.0
58.5
6.0
2.07

66.6
67.0
65.9
59.2
67.3
68.7
9.5
3.36

52.1
51.8
51.8
51.5
52.7
51.5
1.2
0.45

55.9
59.7
58.4
55.3
57.3
56.5
4.4
1.64

55.1
58.4
59.5
55.7
59.2
55.1
4.4
2.09

63.8
62.8
64.2
60.3
64.2
64.0
3.9
1.52

64.2
63.5
61.9
65.4
62.9
63.4
3.5
1.18

57.2
55.4
54.9
56.4
56.3
55.9
2.3
0.81

59.1
58.7
59.2
58.5
59.0
59.0
0.7
0.26

52.8
52.5
53.6
52.1
53.2
52.7
1.5
0.53

(a) B3 results

Lexical features
PP
PP Test
Train PPP
P
BC
BN
CTS
NW
UN
WL
MaxMin
Std. Dev.

Conventional features

BC

BN

CTS NW UN

WL

BC

BN

CTS NW UN

WL

52.0
53.8
51.9
50.5
52.5
53.4
3.3
1.18

57.2
61.3
58.6
58.9
60.3
61.1
4.1
1.60

55.5
58.8
62.0
53.3
58.3
55.7
8.7
3.07

65.7
66.0
64.8
54.5
67.0
67.1
12.7
4.82

46.0
45.9
44.7
45.5
46.0
45.7
1.3
0.50

49.3
54.8
52.3
49.2
51.3
50.4
5.6
2.11

52.7
55.2
56.5
52.6
56.2
51.4
5.1
2.14

60.8
60.1
61.0
56.5
60.8
61.1
4.6
1.77

61.5
60.5
58.4
62.7
59.3
59.8
4.3
1.55

56.5
54.1
53.7
54.6
55.7
55.0
2.8
1.04

53.1
52.9
53.8
52.7
52.8
52.9
1.1
0.40

49.1
48.0
50.0
48.4
48.8
50.1
2.1
0.85

(b) CEAF results

Table 9: Results data source adaptability. row shows results obtained training
mention ranking model data set shown first column row, column
corresponds test set particular data source. best result obtained test set
two coreference models boldfaced.

suggestive evidence answer question affirmative. Nevertheless, look
right half two tables, show results obtained using unlexicalized
mention-ranking model, see similar, perhaps weaker, trend: according CEAF,
four six diagonal entries contain highest scores respective columns,
according B3 , two six diagonal entries exhibit trend. Hence, fact
model performs worse trained tested different data sources cannot
attributed solely lexicalization.
Perhaps informative question is: lexicalized models trained different data
sources exhibit varied performance given test set (composed documents
source) unlexicalized models trained different data sources? affirmative
answer question provide empirical support hypothesis lexicalized
model fits data trained unlexicalized counterpart. answer
question, compute column two models (1) difference
highest lowest scores (see MaxMin row), (2) standard
deviation six scores corresponding column (see Std. Dev. row).
506

fiA Cluster-Ranking Approach Coreference Resolution

compare corresponding columns two coreference models, see except
BN, lexicalized model exhibit varied performance given test set
unlexicalized model according scoring programs, regardless whether
measuring variation using MaxMin standard deviation.
7.6 Feature Analysis
subsection, analyze effects linguistic features performance
coreference models. Given large number models trained three
feature sets, feasible us analyze features model feature
set. Since cluster-ranking model, used Combined feature set, yields
best performance, analyze features. addition, since Lexical features
yielded good performance mention-ranking model, would informative see
Lexical features greatest contribution performance. result,
perform feature analysis two model/feature set combinations.
Although identified two particular model/feature set combinations, actually
total 12 model/feature set combinations: recall except row 1, row
Table 4 shows aggregated result six data sets, trained one model
data set. words, two combinations selected above,
six learned models. reduce number models need analyze yet maximize
insights gain, choose analyze models trained data sets
two fairly different domains: Newswire (NW) Broadcast News (BN).
next question is: analyze features? apply backward elimination feature selection algorithm (see survey paper Blum & Langley, 1997),
starts full feature set removes iteration feature whose removal
yields best system performance. Despite greedy nature, algorithm runs time
quadratic number features, making computationally expensive run
feature sets. reduce computational cost, divide features feature types
apply backward elimination eliminate one feature type per iteration.
features grouped follows. Lexical feature set, divide features
five types: (1) unseen features, (2) lexical features, (3) semi-lexical features, (4) distance, (5) alias. words, division corresponds roughly one described
Section 5.1, except put two conventional features two different groups,
since linguistically one positional feature semantic feature.
Combined feature set, divide features seven groups, first four
identical division Lexical features above. remaining features,
divide string-matching features, comprise features 1118 Table 1;
grammatical features, comprise features 17, 910, 1929, 3336, 3839;
semantic features, comprise features 8, 30, 31. Note alias, semantic feature Lexical feature set, combined semantic features
Conventional feature set form semantic feature type.
Results shown Tables 1013. Specifically, Tables 10a 10b show B3
CEAF F-measure scores feature analysis experiments involving mention-ranking
model, using Lexical feature set NW data set. table, first row shows
system would perform class features removed. remove least
507

fi60.6
60.1
59.2
60.7

62.4
53.1
59.7
60.9

63.3
60.1
61.8

64.6
63.5

ee
n
U
ns


lia



ist

ce

i-l
ex
ic
al
Se


Le
xi
ca
l

Rahman & Ng

65.2

58.4
44.7
53.4
54.7

56.0
55.0
53.5
55.6

60.7
55.3
58.7

61.7
59.3

ee
n
U
ns


lia



ist

ce

Le
xi
ca
l

Se


i-l
ex
ic
al

(a) B3 results

62.1

(b) CEAF results

Table 10: Feature analysis results (in terms F-measure scores) mention-ranking
model using Lexical features NW data set. feature types used train
model, B3 CEAF F-measure scores 65.4 62.7, respectively.

important feature class (i.e., feature class whose removal yields best performance),
next row shows adjusted system would perform without remaining
class. According scoring programs, removing unseen features yields least
drop performance (note caption full feature set, B3 score
65.4 CEAF score 62.7). fact, two scorers agree lexical
semi-lexical features important unseen, alias, distance features.
Nevertheless, results suggest five feature types important, since best
performance achieved using full feature set.
Tables 11a 11b show B3 CEAF F-measure scores feature analysis
experiments involving cluster-ranking model, using Combined feature set NW
data set. Recall Combined feature set, seven types features.
see, two scorers agree completely order features removed.
particular, important features lexical semi-lexical features,
whereas least important features present Lexical feature
set, namely, grammatical, string-matching, semantic features. suggests
lexical features general important non-lexical features
used combination. somewhat surprising, non-lexical features
commonly-used features coreference resolution, whereas Lexical features
comparatively much less investigated coreference researchers. Nevertheless, unlike
saw Table 10, feature types appear relevant, Table 11a, see
508

fi57.6
58.6
59.9
63.9


ica
l



ch

g


tic
Se

59.7
60.2
62.4

G
ra


58.5
58.8
58.9
57.5
63.8


ist

ce

U
ns

ee
n

i-l
ex
ica
l
59.7
60.4
61.1
62.3
61.9
67.2

St
rin

54.7
56.6
58.9
58.3
63.2
63.2

Se


Le
xi
ca
l

Cluster-Ranking Approach Coreference Resolution

59.6
61.1

60.4

55.6
56.9
58.8
60.0


ica
l



ch

g


tic
Se

57.7
58.5
60.1

G
ra


56.9
57.6
58.2
56.7
60.0


ist

ce

U
ns

ee
n

i-l
ex
ica
l
53.3
54.3
57.1
57.2
55.3
61.9

St
rin

50.2
50.6
51.4
51.0
57.9
57.9

Se


Le
xi
ca
l

(a) B3 results

58.4
60.6

58.7

(b) CEAF results

Table 11: Feature analysis results (in terms F-measure) cluster-ranking model
using Combined features NW data set. feature types used train
model, B3 CEAF F-measure scores 64.6 62.3, respectively.

best B3 F-measure score 67.2, achieved using lexical features.
represents 2.6% absolute gain F-measure model trained seven
feature types, suggesting learning-based coreference model could improved via feature
selection.
Next, investigate whether similar trends observed models trained
different source: Broadcast News. Specifically, show Tables 12a 12b B3
CEAF F-measure scores feature analysis experiments involving mentionranking model, using Lexical feature set BN data set. Table 11,
see two scorers agree completely order features
removed. fact, similar observed Table 10 (on NW data set),
scorers determine lexical semi-lexical features important,
whereas distance alias features least important, although five feature
types appear relevant according scorers.
Finally, show Tables 13a 13b B3 CEAF F-measure scores feature
analysis experiments involving cluster-ranking model, using Combined feature set
509

fi53.4
53.4
52.3
52.9

62.7
62.3
61.2
61.6

60.9
59.9
61.7

62.9
62.9


lia


ee
n
U
ns


ist

ce

i-l
ex
ica
l
Se


Le
xi
ca
l

Rahman & Ng

63.4

47.1
47.1
44.9
45.5

59.6
59.1
57.4
57.5

58.6
57.8
58.0

59.9
60.0


lia


ee
n
U
ns


ist

ce

i-l
ex
ica
l
Se


Le
xi
ca
l

(a) B3 results

61.2

(b) CEAF results

Table 12: Feature analysis results (in terms F-measure) mention-ranking model
using Lexical features BN data set. feature types used train
model, B3 CEAF F-measure scores 63.5 61.3, respectively.

BN data set. Tables 11 12, two scorers agree completely order
features removed. far feature contribution concerned, two
tables resemble Tables 11a 11b: cases, lexical, semi-lexical, unseen
features important; string-matching grammatical features
least important; semantic distance features middle. case,
however, seven feature types seem relevant, best performance achieved
using full feature set according scorers. Perhaps interestingly, numbers
column generally increasing move column. means
feature type becomes progressively less useful remove feature types.
suggests interactions different feature types non-trivial
feature type may useful presence another feature type.
summary, results two data sets (NW BN) two scoring programs demonstrate (1) general feature types crucial overall performance, (2)
little-investigated Lexical features contribute overall performance
commonly-used Conventional features.
7.7 Resolution Performance
gain additional insights results, analyze behavior coreference
models different types anaphoric expressions trained different
feature sets. Specifically, partition mentions different resolution classes.
510

fiat
ica
l





ch
51.8
53.3
56.6

g


ist

ce


tic
51.2
54.3
60.6
61.3

G
ra


52.3
53.9
56.6
59.5
61.3

Se


U
ns

ee
n

i-l
ex
ica
l
52.7
54.2
56.9
60.4
57.4
60.6

St
rin

54.7
55.2
55.4
55.5
56.9
56.9

Se


Le
xi
ca
l

Cluster-Ranking Approach Coreference Resolution

53.9
55.6

54.8


ica
l





ch
47.1
50.0
58.5

g


ist

ce


tic
44.3
51.0
54.0
57.7

G
ra


45.2
50.7
54.9
57.0
57.1

Se


U
ns

ee
n

i-l
ex
ica
l
45.7
51.3
52.9
56.7
50.5
55.4

St
rin

44.4
44.3
45.5
46.3
48.5
48.6

Se


Le
xi
ca
l

(a) B3 results

46.4
51.8

52.1

(b) CEAF results

Table 13: Feature analysis results (in terms F-measure) cluster-ranking model
using Combined features BN data set. feature types used train
model, B3 CEAF F-measure scores 63.6 61.3, respectively.

previous work focused mainly three rather coarse-grained resolution classes (namely,
pronouns, proper nouns, common nouns), follow Stoyanov et al. (2009) subdivide
class three fine-grained classes. worth mentioning none Stoyanov et
al.s classes corresponds non-anaphoric expressions. Since believe non-anaphoric
expressions play important role analysis performance coreference
model, propose three additional classes correspond non-anaphoric pronouns,
non-anaphoric proper nouns, non-anaphoric common nouns. Finally, certain
types anaphoric pronouns (e.g., wh-pronouns) fall Stoyanov et
al.s pronoun categories. fill gap, create another category serves
default category anaphoric pronouns covered Stoyanov et al.s classes.
results 13 resolution classes, discussed detail.
Proper nouns. Four classes defined proper nouns. (1) e: proper noun assigned
exact string match class preceding mention two
coreferent string; (2) p: proper noun assigned partial string
match class preceding mention two coreferent
511

fiRahman & Ng

content words common; (3) n: proper noun assigned string match class
preceding mention two coreferent content
words common; (4) na: proper noun assigned non-anaphor class
coreferent preceding mention.
Common nouns. Four analogous resolution classes defined mentions whose head
common noun: (5) e; (6) p; (7) n; (8) na.
Pronouns. three pronoun classes. (9) 1+2: 1st 2nd person pronouns; (10)
G3: gendered 3rd person pronouns (e.g., she); (11) U3: ungendered 3rd person pronouns;
(12) oa: anaphoric pronouns belong (9), (10), (11); (13) na:
non-anaphoric pronouns.
Next, score resolution class. Unlike Stoyanov et al. (2009), use modified
version MUC scorer, employ B3 . reasons MUC scorer (1)
reward singleton clusters, (2) inflate systems performance clusters
overly large. compute score class C, process mentions test text
left-to-right manner. mention encountered, check whether belongs C.
so, use coreference model decide resolve it. Otherwise, use oracle
make correct resolution decision25 (so end mistakes attributed
incorrect resolution mentions C, thus allowing us directly measure
impact overall performance). test documents processed, compute
B3 F-measure score mentions belong C.
Performance resolution class, aggregated test sets six data
sources way before, shown Table 14, provides nice diagnosis
strengths weaknesses coreference model used combination
feature set. show table percentage mentions belonging
class name class, abbreviate name model follows: HM
corresponds head match baseline, whereas MP, EM, MR, CR denote mentionpair model, entity-mention model, mention-ranking model, cluster-ranking
model, respectively. ranking model two versions, pipeline version (denoted
P) joint version (denoted J).
points deserve mention. Recall Table 4 Conventional features
used, joint mention-ranking model performs better mention-pair model
entity-mention model. Comparing row 5 rows 2 3 Table 14,
see improvements attributed primarily better handling one proper
25. oracle determines mention anaphoric antecedents cluster
(because model previously made mistake), employ following heuristic select
antecedent resolve mention to: try resolve closest preceding antecedent
belong class C, antecedent exists, resolve closest preceding antecedent
belongs class C. reason behind heuristics preference preceding antecedent
belong class C simple: since resolving mention using oracle, want choose
antecedent allows us maximize overall score; resolving mention antecedent
belong C likely yield better score resolving antecedent
belongs C, since former resolved using oracle latter not. heuristic
applies trying use oracle resolve mention preceding cluster: first attempt
resolve closest preceding cluster containing mention belong C,
antecedent exists, resolve closest preceding cluster containing mention belongs C.

512

fiA Cluster-Ranking Approach Coreference Resolution

Proper nouns
p
n
na
1.6
3.2
13.9

e
6.3

Common nouns
p
n
na
0.3
4.7
19.2

Class
%

e
15.2

1

HM

68.3

33.0

34.4

63.5

48.1

2
3
4
5
6
7

MP
EM
MR-P
MR-J
CR-P
CR-J

69.6
69.9
78.3
79.4
79.9
79.9

35.4
35.9
41.1
42.5
42.5
43.9

35.6
35.9
32.7
33.4
34.2
34.4

Using
65.8
65.8
76.0
76.4
75.9
76.7

8
9
10
11
12
13

MP
EM
MR-P
MR-J
CR-P
CR-J

78.8
79.1
78.3
79.5
75.3
76.4

41.9
41.8
66.4
67.1
65.7
68.4

32.1
32.4
40.7
41.3
40.4
41.1

Using Lexical feature set
78.6 66.5 54.2 24.1 77.6
78.4 66.5 54.7 24.1 77.9
75.8 53.2 60.7 28.3 83.3
76.3 54.4 61.1 28.6 83.5
76.6 50.2 61.4 30.3 81.9
77.2 50.8 63.2 31.1 83.1

14
15
16
17
18
19

MP
EM
MR-P
MR-J
CR-P
CR-J

73.8
73.9
76.4
77.2
78.3
79.9

40.1
40.6
50.9
52.3
61.3
62.0

38.8
39.2
33.3
34.7
41.5
42.4

Using
67.6 55.9
68.2 56.3
80.7 53.4
82.0 54.3
78.3 60.9
79.1 62.8

Pronouns
U3
oa
5.1
4.4

1+2
15.1

G3
4.9

50.7

46.3

41.7

23.2

55.7

Conventional feature set
56.1 54.7 24.0 70.4 53.8
57.3 55.1 24.3 70.9 54.2
48.5 58.3 27.2 78.2 54.1
48.2 59.0 27.6 78.5 54.4
64.1 58.6 27.1 80.8 57.9
65.0 59.2 27.4 82.1 58.6

55.6
56.0
57.1
57.7
61.8
62.5

46.1
46.4
44.9
45.8
49.7
50.7

24.1
24.6
22.7
23.0
25.6
26.3

51.9
51.7
61.6
62.2
58.1
59.7

55.5
55.7
62.1
62.6
60.3
61.9

57.4
57.8
60.6
62.3
61.0
62.6

44.1
44.1
47.3
47.9
50.6
51.8

24.3
24.2
29.1
31.3
36.2
37.6

61.8
62.1
61.8
62.6
66.3
67.0

58.6
58.7
58.3
59.5
62.1
62.7

60.7
61.6
56.1
59.5
65.5
66.2

49.3
49.6
44.3
45.8
51.4
52.8

27.6
26.2
25.8
26.5
34.7
35.5

58.1
58.3
66.4
67.1
62.7
64.4

55.6

24.7

68.1

Combined feature set
54.8 25.0 73.7
55.8 25.0 74.4
55.4 23.2 78.9
56.8 24.7 80.9
55.4 24.4 79.3
56.8 25.5 79.9

na
6.1

Table 14: B3 F-measure scores different resolution classes.
noun class (e) three classes correspond non-anaphoric mentions (na).
results indicate important take account non-anaphoric mentions
analyzing performance coreference model. time, see
joint mention-ranking model resolve type e common nouns well
mention-pair model entity-mention model. Also, results rows 5 7 indicate
joint cluster-ranking model better joint mention-ranking model due
better handling type e common nouns, non-anaphoric common nouns,
well anaphoric pronouns.
Next, recall Table 4 Lexical features used lieu Conventional features, mention-pair model, entity-mention model, joint mentionranking model exhibit significant improvements performance. mention-pair
model entity-mention model, improvements stem primarily better handling three proper noun classes (e,p,na), two common noun classes (e,na), nonanaphoric pronouns (compare rows 2 8 well rows 3 9 Table 14). joint
mention-ranking model, hand, improvements accrue better handling
two proper noun classes (p,n), two common classes (e,na), anaphoric pronouns,
513

fiRahman & Ng

seen rows 5 11. joint cluster-ranking model show
overall improvement switch Conventional Lexical features (compare rows 7
13), resulting models behave differently. Specifically, using Lexical features,
model gets worse handling one proper noun class (e) one common noun class (e),
better handling another proper noun class (n), two common noun classes (p,n),
one anaphoric pronoun class (1+2), non-anaphoric pronouns.
Finally, recall Combined features used lieu Lexical features,
cluster-ranking model show deterioration performance. mention-pair
model entity-mention model, deterioration performance attributed
poorer handling two proper noun classes (e,na), two common noun classes (e,na),
non-anaphoric pronouns, although better handling one proper noun
class (n) anaphoric pronouns (compare rows 8 14 well rows 9 15
Table 14). Overall, poorer handling anaphoricity appears major factor responsible
performance deterioration. joint mention-ranking model, reasons
performance deterioration slightly different: comparing rows 11 17, see poorer
handling two proper noun classes (p,n), three common noun classes (p,n,na),
anaphoric pronouns, although better handling non-anaphoric proper nouns
pronouns. mentioned before, two versions cluster-ranking model improve
trained Combined features. However, improvements stem
improvements classes (compare rows 12 18 well rows 13 19).
instance, replacing Lexical features Combined features joint
cluster-ranking model, see improvements two proper noun classes (e,na), one common
noun class (e), several pronoun classes (1+2,G3,U3), performance drops another
proper noun class (p), three common noun classes (p,n,na), two pronoun classes
(oa,na).
Overall, results provide us additional insights strengths weaknesses learning-based coreference model well directions future work. particular, even two models yield similar overall performance, quite different
resolution class level. Since single coreference model outperforms
others resolution classes, may beneficial apply ensemble approach,
anaphor belonging particular resolution class resolved model offers
best performance class.

8. Conclusions
Mitkov (2001, p. 122) puts it, coreference resolution difficult, intractable
problem, researchers making steady progress improving machine learning approaches problem past fifteen years. progress slow, however.
Despite deficiencies, mention-pair model widely thought learningbased coreference model almost decade. entity-mention model mentionranking model emerged mention-pair model dominated learning-based
coreference research nearly ten years. Although two models conceptually simple, represent significant departure mention-pair model new way
thinking alternative models coreference designed. cluster-ranking
model advances learning-based coreference research theoretically combining
514

fiA Cluster-Ranking Approach Coreference Resolution

strengths two models, thereby addressing two commonly cited weaknesses
mention-pair model. bridges gap two independent lines learningbased coreference research one concerning entity-mention model
mention-ranking model going past years, narrows modeling gap sophistication rule-based coreference models
simplicity learning-based coreference models. Empirically, shown using
ACE 2005 coreference data set cluster-ranking model acquired jointly learning
anaphoricity determination coreference resolution surpasses performance several
competing approaches, including mention-pair model, entity-mention model,
mention-ranking model. Perhaps equally importantly, cluster-ranking model
model considered profitably exploit information provided two
fairly different sources information, Conventional features Lexical features.
ranking natural formulation coreference resolution classification,
ranking-based coreference models popularly used influential
mention-pair model. One goals article promote application ranking
techniques coreference resolution. Specifically, attempted clarify difference classification-based ranking-based coreference models showing constrained
optimization problem SVM learner needs solve type models, hoping
help reader appreciate importance ranking coreference resolution. addition, provided ample empirical evidence ranking-based models
superior classification-based models coreference resolution.
Another contribution work lies empirical demonstration benefits
lexicalizing learning-based coreference models. previous work showed lexicalization provides marginal benefits coreference model, showed lexicalization significantly improve mention-pair model, entity-mention model,
mention-ranking model, point approach even surpass performance
cluster-ranking model. Interestingly, showed models benefit lexicalization conventional coreference features used. challenges
common belief prototypical set linguistic features (e.g., gender
number agreement) must used constructing learning-based coreference systems.
addition, feature analysis experiments indicated conventional features contributed less overall performance rarely studied lexical features joint
cluster-ranking coreference model two types features used combination.
Finally, examined performance coreference model resolving mentions
belonging different resolution classes. found even two models achieve similar
overall performance, quite different resolution class level. Overall,
results provide us additional insights strengths weaknesses learningbased coreference model well promising directions future research.

Bibliographic Note
Portions work previously presented conference publication (Rahman &
Ng, 2009). current article extends work several ways, notably: (1)
overview literature ranking approaches coreference resolution (Section 2); (2)
detailed explanation difference classification ranking (Section 3); (3)
515

fiRahman & Ng

investigation issues lexicalizing coreference models (Section 5); (4) in-depth
analysis different aspects coreference system (Section 7).

Acknowledgments
authors acknowledge support National Science Foundation (NSF) grant IIS0812261. thank three anonymous reviewers insightful comments unanimously recommending article publication JAIR. opinions, findings, conclusions recommendations expressed article authors
necessarily reflect views official policies, either expressed implied, NSF.

References
Androutsopoulos, I., & Malakasiotis, P. (2010). survey paraphrasing textual
entailment methods. Journal Artificial Intelligence Research, 38 , 135187.
Aone, C., & Bennett, S. W. (1995). Evaluating automated manual acquisition
anaphora resolution strategies. Proceedings 33rd Annual Meeting
Association Computational Linguistics (ACL), pp. 122129.
Bagga, A., & Baldwin, B. (1998). Algorithms scoring coreference chains. Proceedings
Linguistic Coreference Workshop First International Conference
Language Resources Evaluation (LREC), pp. 563566.
Barbu, C., & Mitkov, R. (2001). Evaluation tool rule-based anaphora resolution methods. Proceedings 39th Annual Meeting Association Computational
Linguistics (ACL), pp. 3441.
Bengtson, E., & Roth, D. (2008). Understanding values features coreference
resolution. Proceedings 2008 Conference Empirical Methods Natural
Language Processing (EMNLP), pp. 294303.
Berger, A. L., Della Pietra, S. A., & Della Pietra, V. J. (1996). maximum entropy
approach natural language processing. Computational Linguistics, 22 (1), 3971.
Bikel, D. M., Schwartz, R., & Weischedel, R. M. (1999). algorithm learns whats
name. Machine Learning: Special Issue Natural Language Learning, 34 (13),
211231.
Blum, A., & Langley, P. (1997). Selection relevant features examples machine
learning. Artificial Intelligence, 97 (12), 245271.
Burges, C. J. C. (1998). tutorial support vector machines pattern recognition.
Data Mining Knowledge Discovery, 2 (2), 121167.
Cai, J., & Strube, M. (2010). Evaluation metrics end-to-end coreference resolution
systems. Proceedings 11th Annual SIGdial Meeting Discourse Dialogue
(SIGDIAL), pp. 2836.
Carbonell, J., & Brown, R. (1988). Anaphora resolution: multi-strategy approach.
Proceedings 12th International Conference Computational Linguistics (COLING), pp. 96101.
516

fiA Cluster-Ranking Approach Coreference Resolution

Cardie, C., & Wagstaff, K. (1999). Noun phrase coreference clustering. Proceedings
1999 Joint SIGDAT Conference Empirical Methods Natural Language
Processing Large Corpora (EMNLP/VLC), pp. 8289.
Charniak, E., & Elsner, M. (2009). EM works pronoun anaphora resolution. Proceedings 12th Conference European Chapter Association Computational Linguistics (EACL), pp. 148156.
Collins, M. J. (1999). Head-Driven Statistical Models Natural Language Parsing. Ph.D.
thesis, Department Computer Information Science, University Pennsylvania,
Philadelphia, PA.
Connolly, D., Burger, J. D., & Day, D. S. (1994). machine learning approach anaphoric
reference. Proceedings International Conference New Methods Language
Processing, pp. 255261.
Culotta, A., Wick, M., & McCallum, A. (2007). First-order probabilistic models coreference resolution. Human Language Technologies 2007: Conference North
American Chapter Association Computational Linguistics; Proceedings
Main Conference (NAACL HLT), pp. 8188.
Daume III, H., & Marcu, D. (2005). large-scale exploration effective global features
joint entity detection tracking model. Proceedings Human Language
Technology Conference Conference Empirical Methods Natural Language
Processing (HLT/EMNLP), pp. 97104.
Denis, P., & Baldridge, J. (2007a). Global, joint determination anaphoricity coreference resolution using integer programming. Human Language Technologies 2007:
Conference North American Chapter Association Computational
Linguistics; Proceedings Main Conference (NAACL HLT), pp. 236243.
Denis, P., & Baldridge, J. (2007b). ranking approach pronoun resolution. Proceedings
Twentieth International Conference Artificial Intelligence (IJCAI), pp. 1588
1593.
Denis, P., & Baldridge, J. (2008). Specialized models ranking coreference resolution.
Proceedings 2008 Conference Empirical Methods Natural Language
Processing (EMNLP), pp. 660669.
Enrique, A., Gonzalo, J., Artiles, J., & Verdejo, F. (2009). comparison extrinsic clustering evaluation metrics based formal constraints. Information Retrieval, 12 (4),
461486.
Fellbaum, C. (1998). WordNet: electronic lexical database. MIT Press, Cambridge, MA.
Finkel, J. R., Grenager, T., & Manning, C. (2005). Incorporating non-local information
information extraction systems Gibbs sampling. Proceedings 43rd Annual
Meeting Association Computational Linguistics (ACL), pp. 363370.
Finkel, J. R., & Manning, C. (2008). Enforcing transitivity coreference resolution.
Proceedings ACL-08: HLT Short Papers (Companion Volume), pp. 4548.
Florian, R., Hassan, H., Ittycheriah, A., Jing, H., Kambhatla, N., Luo, X., Nicolov, N., &
Roukos, S. (2004). statistical model multilingual entity detection tracking.
HLT-NAACL 2004: Main Proceedings, pp. 18.
517

fiRahman & Ng

Ge, N., Hale, J., & Charniak, E. (1998). statistical approach anaphora resolution.
Proceedings Sixth Workshop Large Corpora (WVLC), pp. 161170.
Grosz, B. J., Joshi, A. K., & Weinstein, S. (1983). Providing unified account definite noun phrases discourse. Proceedings 21th Annual Meeting
Association Computational Linguistics (ACL), pp. 4450.
Grosz, B. J., Joshi, A. K., & Weinstein, S. (1995). Centering: framework modeling
local coherence discourse. Computational Linguistics, 21 (2), 203226.
Haghighi, A., & Klein, D. (2010). Coreference resolution modular, entity-centered
model. Human Language Technologies: 2010 Annual Conference North
American Chapter Association Computational Linguistics (NAACL HLT),
pp. 385393.
Hobbs, J. (1978). Resolving pronoun references. Lingua, 44, 311338.
Iida, R., Inui, K., & Matsumoto, Y. (2009). Capturing salience trainable cache
model zero-anaphora resolution. Proceedings Joint Conference 47th
Annual Meeting ACL 4th International Joint Conference Natural
Language Processing AFNLP (ACL-IJCNLP), pp. 647655.
Iida, R., Inui, K., Takamura, H., & Matsumoto, Y. (2003). Incorporating contextual cues
trainable models coreference resolution. Proceedings EACL Workshop
Computational Treatment Anaphora.
Ji, H., Rudin, C., & Grishman, R. (2006). Re-Ranking algorithms name tagging.
Proceedings Workshop Computationally Hard Problems Joint Inference
Speech Language Processing, pp. 4956.
Ji, H., Westbrook, D., & Grishman, R. (2005). Using semantic relations refine coreference
decisions. Proceedings Human Language Technology Conference Conference
Empirical Methods Natural Language Processing (HLT/EMNLP), pp. 1724.
Joachims, T. (1999). Making large-scale SVM learning practical. Scholkopf, B., Burges,
C., & Smola, A. (Eds.), Advances Kernel Methods Support Vector Learning, pp.
4456. MIT Press, Cambridge, MA.
Joachims, T. (2002). Optimizing search engines using clickthrough data. Proceedings
Eighth ACM SIGKDD International Conference Knowledge Discovery
Data Mining (KDD), pp. 133142.
Kehler, A., Appelt, D., Taylor, L., & Simma, A. (2004). (non)utility predicateargument frequencies pronoun interpretation. Proceedings Human Language Technology Conference North American Chapter Association
Computational Linguistics (HLT/NAACL), pp. 289296.
Kuhn, H. W. (1955). Hungarian method assignment problem. Naval Research
Logistics Quarterly, 2, 8397.
Lappin, S., & Leass, H. (1994). algorithm pronominal anaphora resolution. Computational Linguistics, 20 (4), 535562.
Lin, D., & Wu, X. (2009). Phrase clustering discriminative learning. Proceedings
Joint Conference 47th Annual Meeting ACL 4th International
518

fiA Cluster-Ranking Approach Coreference Resolution

Joint Conference Natural Language Processing AFNLP (ACL-IJCNLP), pp.
10301038.
Luo, X. (2005). coreference resolution performance metrics. Proceedings Human
Language Technology Conference Conference Empirical Methods Natural
Language Processing (HLT/EMNLP), pp. 2532.
Luo, X., Ittycheriah, A., Jing, H., Kambhatla, N., & Roukos, S. (2004). mentionsynchronous coreference resolution algorithm based Bell tree. Proceedings
42nd Annual Meeting Association Computational Linguistics (ACL),
pp. 135142.
Madnani, N., & Dorr, B. (2010). Generating phrasal sentential paraphrases: survey
data-driven methods. Computational Linguistics, 36 (3), 341387.
McCarthy, J., & Lehnert, W. (1995). Using decision trees coreference resolution. Proceedings Fourteenth International Conference Artificial Intelligence (IJCAI),
pp. 10501055.
Mitkov, R. (1998). Robust pronoun resolution limited knowledge. Proceedings
36th Annual Meeting Association Computational Linguistics 17th
International Conference Computational Linguistics (COLING/ACL), pp. 869
875.
Mitkov, R. (2001). Outstanding issues anaphora resolution. Gelbukh, A. (Ed.),
Computational Linguistics Intelligent Text Processing, pp. 110125. Springer.
Mitkov, R. (2002). Anaphora Resolution. Longman.
Morris, J., & Hirst, G. (1991). Lexical cohesion computed thesaural relations
indicator struture text. Computational Linguistics, 17 (1), 2148.
Morton, T. (2000). Coreference NLP applications. Proceedings 38th Annual
Meeting Association Computational Linguistics (ACL).
MUC-6 (1995). Proceedings Sixth Message Understanding Conference (MUC-6).
Morgan Kaufmann, San Francisco, CA.
MUC-7 (1998). Proceedings Seventh Message Understanding Conference (MUC-7).
Morgan Kaufmann, San Francisco, CA.
Ng, V. (2009). Graph-cut-based anaphoricity determination coreference resolution.
Proceedings 2009 Conference North American Chapter Association
Computational Linguistics: Human Language Technologies (NAACL HLT), pp.
575583.
Ng, V., & Cardie, C. (2002a). Identifying anaphoric non-anaphoric noun phrases
improve coreference resolution. Proceedings 19th International Conference
Computational Linguistics (COLING), pp. 730736.
Ng, V., & Cardie, C. (2002b). Improving machine learning approaches coreference resolution. Proceedings 40th Annual Meeting Association Computational
Linguistics (ACL), pp. 104111.
519

fiRahman & Ng

Poesio, M., Uryupina, O., Vieira, R., Alexandrov-Kabadjov, M., & Goulart, R. (2004).
Discourse-new detectors definite description resolution: survey preliminary
proposal. Proeedings ACL Workshop Reference Resolution.
Ponzetto, S. P., & Strube, M. (2006). Exploiting semantic role labeling, WordNet
Wikipedia coreference resolution. Proceedings Human Language Technology Conference Conference North American Chapter Association
Computational Linguistics (HLT/NAACL), pp. 192199.
Rahman, A., & Ng, V. (2009). Supervised models coreference resolution. Proceedings 2009 Conference Empirical Methods Natural Language Processing
(EMNLP), pp. 968977.
Ravichandran, D., Hovy, E., & Och, F. J. (2003). Statistical QA - classifier vs. re-ranker:
Whats difference? Proceedings ACL 2003 Workshop Multilingual
Summarization Question Answering, pp. 6975.
Recasens, M., & Hovy, E. (2011). BLANC: Implementing Rand Index coreference
resolution. Natural Language Engineering (to appear).
Roth, D. (2002). Reasoning classifiers.. Proceedings 13th European Conference
Machine Learning (ECML), pp. 506510.
Roth, D., & Yih, W.-T. (2009). linear programming formulation global inference
natural language tasks.. Proceedings Eighth Conference Computational
Natural Language Learning (CoNLL), pp. 18.
Soon, W. M., Ng, H. T., & Lim, D. C. Y. (2001). machine learning approach coreference
resolution noun phrases. Computational Linguistics, 27 (4), 521544.
Stoyanov, V., Gilbert, N., Cardie, C., & Riloff, E. (2009). Conundrums noun phrase
coreference resolution: Making sense state-of-the-art. Proceedings
Joint Conference 47th Annual Meeting ACL 4th International
Joint Conference Natural Language Processing AFNLP (ACL-IJCNLP), pp.
656664.
Strube, M., & Muller, C. (2003). machine learning approach pronoun resolution
spoken dialogue. Proceedings 41st Annual Meeting Association
Computational Linguistics (ACL), pp. 168175.
Strube, M., Rapp, S., & Muller, C. (2002). influence minimum edit distance
reference resolution. Proceedings 2002 Conference Empirical Methods
Natural Language Processing (EMNLP), pp. 312319.
Toutanova, K., Klein, D., Manning, C. D., & Singer, Y. (2003). Feature-rich part-of-speech
tagging cyclic dependency network. HLT-NAACL 2003: Proceedings
Main Conference, pp. 173180.
Uryupina, O. (2003). High-precision identification discourse new unique noun
phrases. Proceedings 41st Annual Meeting Association Computational Linguistics: Companion Volume, pp. 8086.
Vapnik, V. N. (1995). Nature Statistical Learning. Springer, New York.
520

fiA Cluster-Ranking Approach Coreference Resolution

Vilain, M., Burger, J., Aberdeen, J., Connolly, D., & Hirschman, L. (1995). modeltheoretic coreference scoring scheme. Proceedings Sixth Message Understanding Conference (MUC-6), pp. 4552.
Walker, M., Joshi, A., & Prince, E. (Eds.). (1998). Centering Theory Discourse. Oxford
University Press.
Yang, X., Su, J., Lang, J., Tan, C. L., & Li, S. (2008). entity-mention model
coreference resolution inductive logic programming. Proceedings 46th
Annual Meeting Association Computational Linguistics: Human Language
Technologies (ACL-08: HLT), pp. 843851.
Yang, X., Su, J., Zhou, G., & Tan, C. L. (2004). NP-cluster based approach coreference
resolution. Proceedings 20th International Conference Computational
Linguistics (COLING), pages 226232.
Yang, X., Zhou, G., Su, J., & Tan, C. L. (2003). Coreference resolution using competitive
learning approach. Proceedings 41st Annual Meeting Association
Computational Linguistics (ACL), pp. 176183.

521


