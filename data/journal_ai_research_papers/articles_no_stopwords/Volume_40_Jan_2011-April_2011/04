journal artificial intelligence

submitted published

scaling heuristic relational decision trees
tomas de la rosa
sergio jimenez
raquel fuentetaja
daniel borrajo

trosa inf uc es
sjimenez inf uc es
rfuentet inf uc es
dborrajo ia uc es

departamento de informatica
universidad carlos iii de madrid
av universidad leganes madrid spain

abstract
current evaluation functions heuristic expensive compute numerous
functions provide good guidance solution worth
expense however evaluation functions misguiding large
enough lots node evaluations must computed severely limits scalability heuristic planners present novel solution reducing node evaluations heuristic
machine learning particularly define task learning search control
heuristic relational classification task use shelf relational classification tool address learning task relational classification task captures preferred action
select different contexts specific domain contexts
defined set helpful actions current state goals remaining achieved
static predicates task shows two methods guiding search
heuristic planner learned classifiers first one consists resulting classifier action policy second one consists applying classifier generate lookahead
states within best first search experiments variety domains reveal
heuristic planner learned classifiers solves larger state art planners

introduction
last years state space heuristic search achieved significant
become one popular paradigms automated however heuristic search
planners suffer strong scalability limitations even well studied domains blocksworld
become challenging planners number blocks relatively large usually statespace heuristic search planners action grounding makes state space
explored large number objects action parameters large enough moreover
domain independent heuristics expensive compute domains heuristics
misleading heuristic planners spend time computing useless node
evaluations even best current domain independent heuristic functions literature
forward chaining heuristic planners currently visit many nodes takes considerable
time especially due time required compute heuristic functions
entail strong limitations application heuristic planners real instance logistics applications need handle hundreds objects together hundreds
vehicles locations florez garca torralba linares garca olaya borrajo current heuristic search planners exhaust computational resources solving
real logistics application
c

ai access foundation rights reserved

fid e la rosa j imenez f uentetaja b orrajo

classic dealing scalability issues assisting search engines
planners domain specific control knowledge dck examples systems
benefit knowledge tlp lan bacchus kabanza talp lanner doherty
kvarnstrom shop nau au ilghami kuter murdock wu yaman
nevertheless hand coding dck complex task implies expertise domain search system recent years
renewed interest machine learning ml automatically extract dck zimmerman
kambhampati made comprehensive survey ml defining dck shown first
learning competition held learning track renewed interest specially
targeted heuristic planners
presents learning dck building domain dependent
relational decision trees examples good quality solutions forward chaining heuristic
planner decision trees built shelf relational classification tool capture best action take possible decision planner given domain
resulting decision trees used policy solve directly
generate lookahead states within best first search bfs techniques allow
planner avoid state evaluations helps objective improving scalability
implemented system called roller work improvement
previous one de la rosa jimenez borrajo alternatively roller version
repairing relaxed plans de la rosa jimenez garca duran fernandez garca olaya borrajo
competed learning track th international competition ipc held
roller improvements presented article mainly lessons learned
competition discussed later
organized follows section introduces issues need considered
designing learning system heuristic help us clarify decisions made development section describes roller system
detail section presents experimental obtained variety benchmarks section
discusses improvements roller system compared previous version system
section revises related work learning dck heuristic finally last section
discusses conclusions future work

common issues learning domain specific control knowledge
designing ml process automatic acquisition dck one must consider
common issues among others
representation learned knowledge predicate logic common language
represent dck tasks usually defined language however representation languages used aiming make learning dck
effective instance languages describing object classes concept
language martin geffner taxonomic syntax mcallester givan
shown provide useful learning bias different domains
another representation issue selection feature space e set instance
features used representing learned knowledge training system feature
space able capture key knowledge domain traditionally feature


fis caling h euristic p lanning r elational ecision rees

space consisted predicates describing current state goals
task feature space enriched extra predicates called metapredicates
capture extra useful information context applicable operators
pending goals veloso carbonell perez borrajo fink blythe recently works
learning dck heuristic planners define metapredicates capture information
context heuristic planner including example predicates capture
actions relaxed plan given state yoon fern givan
learning inductive logic programming ilp muggleton de raedt
deals development inductive techniques learn given target concept
examples described predicate logic tasks normally represented
predicate logic ilp quite suitable dck learning moreover recent
years ilp broadened scope cover whole spectrum ml tasks regression clustering association analysis extending classical propositional ml
relational framework consequently ilp used heuristic planners capture dck different forms decision rules select actions different
context regression rules obtain better node evaluations yoon et al
generation training examples success ml depends directly
quality training examples used learning dck examples
extracted experience collected solving training
representative different tasks across domain therefore quality training
examples depend variety used training quality
solutions traditionally training obtained random
generators provided parameters tune difficulty way one
domain kind makes learning generalize
useful dck
use learned dck decisions made three issues affect quality
learned dck representation schemes may expressive enough capture effective dck given domain learning may able acquire useful
dck within reasonable time memory requirements set training may
lack significant examples key knowledge situations direct use
learned dck improve scalability planner could even decrease performance effective way dealing heuristic planners integrating
learned dck within robust strategies best first search yoon et al
combining domain independent heuristic functions roger helmert

roller system
section describes general scheme learning dck instantiated roller
system first describes dck representation followed roller second explains
learning used roller third depicts roller collects good quality training
examples finally shows different approaches scaling heuristic
learned dck


fid e la rosa j imenez f uentetaja b orrajo

representation learned knowledge helpful contexts heuristic
present following notation specified domain definition language pddl typed strips tasks accordingly definition domain comprises definition
hierarchy types
set typed constants cd representing objects present tasks domain
set empty
set predicate symbols p one corresponding arity type
arguments
set operators whose arguments typed variables
variables declared directly defining operator argument local
operator definition call po set atomic formulas generated
defined predicates p variables defined arguments operator general
constants cd operator defined three sets pre po operator
preconditions add po positive effects del po negative effects
operator
task domain tuple c g c set typed
constants representing objects particular task set ground atomic
formulas describing initial state g set ground atomic formulas describing goals
given total set constants c c cd task defines finite state space finite set
instantiated operators state set ground atomic formulas representing
facts true states described following closed world assumption instantiated
operator action operator variable replaced constant c
type thus set actions generated set constants c
set operators definition solving task implies finding plan
sequence actions ai transforms initial state state
goals achieved
contexts defined roller rely concepts relaxed plan heuristic
helpful actions introduced planner hoffmann nebel relaxed plan
heuristic returns integer evaluated node number actions solution
relaxed task node simplification original task
deletes actions ignored idea delete relaxation computing heuristics
first introduced mcdermott bonet loerincs geffner
relaxed plan extracted relaxed graph sequence facts
actions layers f ft first fact layer contains facts initial state
action layer contains set applicable actions given previous fact layer fact
layer contains set positive effects actions appearing previous layers
process finishes goals fact layer two consecutive facts layers
facts last case relaxed solution relaxed plan heuristic
returns infinity


fis caling h euristic p lanning r elational ecision rees

relaxed graph built solution extracted backwards process
goal appearing first time fact layer assigned set goals layer gi
last set goals gt second set goals g goal goals set
action selected generates goal whose layer index minimal afterwards
precondition action e subgoal included goals set corresponding first
layer fact appears process finished set selected actions comprises
relaxed plan
according extraction process planner marks helpful actions set actions
first layer relaxed graph achieve subgoals next
fact layer e goals set g words helpful actions applicable actions
generate facts top level goals required action relaxed plan
formally set helpful actions given state defined
helpful add g
planner uses helpful actions search pruning technique considered candidates selected search given state generates
particular set helpful actions claim helpful actions together remaining goals static literals task encode helpful context related state
helpful actions remaining target goals relate actions likely applied
goals need achieved relations arise helpful actions target
goals often share arguments objects additionally static predicates express
facts characterize objects task identifying objects relevant since
may shared arguments helpful actions target goals
definition helpful context state defined
h helpful target static
target g describes set goals achieved state target g
static set literals hold task defined
initial state present every state given changed action thus
static p p add p del
helpful context alternative representation tuple state goals applied action
traditionally used learning dck helpful contexts present advantages
improving scalability heuristic planners
domains set helpful actions contains actions likely applied
focusing reasoning shown good strategy
set helpful actions normally smaller set non static literals state
e static thus process matching learned dck within search obtains
benefits compact representation
number helpful actions normally decreases search fewer goals left
therefore matching process become faster search advancing towards
goals


fid e la rosa j imenez f uentetaja b orrajo

learning learning generalized policies relational decision trees
roller implements two step learning process building dck collection examples
different helpful contexts

learning operator classifier roller builds classifier choose best operator
different helpful contexts
learning binding classifiers operator domain roller builds classifier
choose best binding instantiation operator different helpful contexts
learning process split two steps build dck shelf learning
tools action may different number arguments arguments different types e g actions switch instrument satellite turn satellite
direction direction satellite domain hinders definition target
classes two step decision process clearer decision making point view
helps users understand generated dck better focusing decision operator apply bindings use given selected operator learning
set learning examples two learning steps figure shows overview
learning process roller system

roller learner
training


operator classifier
example
generator

pddl
domain

op examples

relational
classification

bind examples

tool

binding classifiers



language bias

figure overview roller learning process

l earning r elational ecision ress
classic assist decision making consists gathering significant set previous decisions building decision tree generalizes leaves resulting tree contain
classes decisions make internal nodes contain conditions lead decisions common way build trees following top induction decision
trees tdidt quinlan builds tree repeatedly splitting
set training examples according conditions minimize entropy examples traditionally training examples described attribute value representation therefore
conditions decision trees represent tests value given attribute examples
nevertheless attribute value suitable representing decisions want
keep predicate logic representation better represent decisions relationally
instance given action chosen reach certain goals given context share arguments recently building relational decision trees examples described


fis caling h euristic p lanning r elational ecision rees

predicate logic facts developed relational learning similar
propositional ones except condition nodes tree refer attribute values
logic queries relational facts holding training examples logic queries
share variables condition nodes placed decision tree learning
greedy search process since space potential relational decision trees usually huge
search normally biased according specification syntactic restrictions called language bias
specification contains target concept predicates appear condition nodes
trees learning specific knowledge type information input output
variables predicates
use tool tilde blockeel de raedt learning operator
binding classifiers tool implements relational version tdidt although
shelf tool learning relational classifiers could used pro gol muggleton ribl emde wettschereck different learning
would provide different since explore classifiers space differently
study pros cons different beyond scope
comprehensive explanation current relational learning approaches please refer work de
raedt
l earning perator c lassifier
inputs learning operator classifier
training examples examples represented prolog syntax consist
operator selected class together helpful context background knowledge
terms relational learning selected particular example contains
class use predicate arity selected encode operator chosen
context predicate target concept learning step first argument holds
example identifier links rest example predicates second argument
identifier links static predicates shared examples coming
third argument example class e name
selected operator helpful context
helpful predicates predicates express helpful actions contained
helpful context predicate symbol predicates helpful ai ai
name instantiated action arguments example
identifier together parameters action ai instantiated action
parameters constants
target goal predicates represent predicates appear goals
hold current state predicates form target goal gi
gi domain predicates predicate contains example
identifiers
static predicates represent static predicates given predicates shared training examples belong
form static fact domain predicates
appear effects domain action arguments identifier corresponding arguments domain predicate


fid e la rosa j imenez f uentetaja b orrajo

figure shows one learning example id tr e consisting selection operator switch associated helpful context example used building
operator classifier satellite domain
example tr e tr
selected tr e tr switch
helpful turn tr e tr satellite groundstation star
helpful turn tr e tr satellite phenomenon star
helpful turn tr e tr satellite phenomenon star
helpful turn tr e tr satellite phenomenon star
helpful switch tr e tr instrument satellite
target goal image tr e tr phenomenon infrared
target goal image tr e tr phenomenon infrared
target goal image tr e tr phenomenon spectrograph
static predicates
static fact calibration target tr instrument groundstation
static fact supports tr instrument spectrograph
static fact supports tr instrument infrared
static fact board tr instrument satellite

figure knowledge base corresponding example satellite domain example
id tr e links example predicates obtained solving
training tr links rest examples
selected operator helpful context switch corresponds one
helpful actions encoded helpful predicates example

language bias bias specifies constraints arguments predicates
training examples assume domain specific constraint given learning
technique domain independent bias contains restrictions argument types
restrictions ensure identifier variables added variables
classifier generation bias automatically extracted pddl domain definitions
consists declaration predicates used learning example argument
types figure shows language bias specified learning operator classifier
satellite domain
resulting relational decision tree represents set disjoint rules action selection
used provide advice planner internal nodes tree contain set conditions related helpful context advice provided leaf nodes contain
corresponding advice case operator select number examples covered
rule operator select one selected often training
examples covered rule operator classifiers learned roller advise nonhelpful actions given state non helpful actions subset applicable actions state
considered helpful actions certainly actions part helpful contexts defined however learned operator classifiers indicate name operator select
regardless whether helpful figure shows operator tree learned satellite


fis caling h euristic p lanning r elational ecision rees

target concept predict selected idexample idproblem operator
type selected idex idprob class
classes turn switch switch calibrate take image
helpful context predicates helpful actions
rmode helpful turn idexample idproblem
type helpful turn idex idprob satellite direction direction
rmode helpful switch idexample idproblem
type helpful switch idex idprob instrument satellite
rmode helpful switch idexample idproblem
type helpful switch idex idprob instrument satellite
rmode helpful calibrate idexample idproblem
type helpful calibrate idex idprob satellite instrument direction
rmode helpful take image idexample idproblem
type helpful take image idex idprob satellite direction instrument mode
predicates target goals
rmode target goal pointing idexample idproblem
type target goal pointing idex idprob satellite direction
rmode target goal image idexample idproblem
type target goal image idex idprob direction mode
predicates static facts
rmode static fact board idproblem
type static fact board idprob instrument satellite
rmode static fact supports idproblem
type static fact supports idprob instrument mode
rmode static fact calibration target idproblem
type static fact calibration target idprob instrument direction

figure language bias learning operator classifier satellite domain automatically generated pddl definition rmode predicates indicate
used tree type predicates indicate types particular rmode

domain learned decision trees branch denoted symbols yes
yes indicates next node positive answers current question indicates next
node negative answers figure first branch states calibrate
action set helpful actions recommendation square brackets choosing action
e calibrate addition branch indicates recommended action occurred
times training examples moreover leaf node information double square brack

fid e la rosa j imenez f uentetaja b orrajo

ets number times type action selected training examples covered
rule current branch thus case action calibrate selected
total times operators never selected second branch says
calibrate helpful action take image one planner selected
take image times helpful calibrate take image actions helpful switch action switch recommendation
selected times tree branches interpreted similarly
selected b c
helpful calibrate b e f
yes calibrate turn switch switch

calibrate take image
helpful take image b g h j
yes take image turn switch switch

calibrate take image
helpful switch b k l
yes switch turn switch

switch calibrate

take image
turn turn switch
switch calibrate
take image

figure relational decision tree learned operator selection satellite domain internal
nodes ending queries helpful contexts leaf nodes brackets
class number observed examples operator

l earning b inding c lassifiers
second learning step relational decision tree built domain operator
trees indicate bindings select different helpful contexts inputs learning
binding classifier operator
training examples consist exclusively helpful contexts operator
selected together applicable instantiations contexts note
given helpful context applicable instantiations may include helpful nonhelpful actions helpful contexts coded exactly previous learning step
applicable instantiations represented selected predicate predicate target concept second learning step arguments example
identifiers instantiated arguments applicable action example class selected rejected purpose predicate distinguish
good bad bindings operator figure shows piece knowledge base
building binding tree corresponding action switch satellite domain example id tr e resulted selection action instantiation


fis caling h euristic p lanning r elational ecision rees

switch instrument satellite action switch instrument
satellite applicable rejected planner
language bias bias learning binding trees bias learning
operator tree except includes definition selected predicate
previous learning step language bias learning binding tree automatically extracted pddl domain definition figure shows part language bias specified
learning binding tree action switch satellite domain

example tr e tr
selected switch tr e tr instrument satellite rejected
selected switch tr e tr instrument satellite selected
helpful switch tr e tr instrument satellite
helpful switch tr e tr instrument satellite
helpful turn tr e tr satellite star star
helpful turn tr e tr satellite star star
helpful turn tr e tr satellite phenomenon star
helpful turn tr e tr satellite phenomenon star
target goal image tr e tr phenomenon spectrograph
target goal image tr e tr phenomenon spectrograph
target goal image tr e tr star image
static predicates
static fact calibration target tr instrument star
static fact calibration target tr instrument star
static fact supports tr instrument image
static fact supports tr instrument spectrograph
static fact supports tr instrument image
static fact supports tr instrument image
static fact board tr instrument satellite
static fact board tr instrument satellite

figure knowledge base corresponding example tr e obtained solving training tr satellite domain
second learning step relational decision tree uninstantiated
operator consists set disjoint rules binding selection figure
shows example binding tree tswitch built operator switch satellite
domain according tree first branch states helpful action
switch instrument c satellite switch bindings c selected
planner times note binding trees learned roller advise
non helpful actions frequently selected predicate matches tree queries refer
helpful predicates cases branch query may cover bindings non helpful
actions operator
binding trees satellite domain refer reader online appendix
article include learned dck domains used experimental section


fid e la rosa j imenez f uentetaja b orrajo

target concept predict selected switch idexample idproblem inst sat class
type selected switch idex idprob instrument satellite class
classes selected rejected
helpful context operator classification


figure part language bias learning binding tree switch action
satellite domain

selected switch b c e
helpful switch b c
yes selected selected rejected
rejected selected rejected

figure relational decision tree learned bindings selection switch action
satellite domain

many cases decision trees somewhat complex one shown figure
instance turn binding tree nodes includes several queries target goals e g
asking pending image pointed direction others static facts e g
asking pointed direction calibration target
generation training examples
roller training examples instances decisions made solving training order
characterize variety good solutions decisions consider different alternatives
solving individual given search tree node state alternatives come
possibility choosing different operators different bindings single operator
cases assuming alternative lead equally good solutions

regarding binding decisions actions alternative solutions ignored
tagged rejected consequently introduce noise learning process instance
consider figure satellite domain satellite calibrated
instrument must turn directions order take images context three turn actions helpful actions regarding one solution makes
learning consider one action selected two actions rejected however learned
knowledge recommend helpful turn action towards direction
satellite corresponding calibrated instrument take image learn kind
knowledge roller consider three turn actions selected three actions correspond selectable actions learning correct knowledge particular


fis caling h euristic p lanning r elational ecision rees

context actions marked rejected learner consider selecting turn
described context bad choice
takeimage





turnto



takeimage

turnto
takeimage
turnto



turnto




turnto






turnto






figure solution path alternatives simplified satellite
regarding operator decisions complete training full catalogue different solutions
confuse learning process instance consider example figure
goal take image direction applying calibrate action necessary
switch instrument turn satellite calibration target direction
two actions helpful generate two different solution paths fact commutative
generalizing operator selection kinds helpful contexts difficult training
examples contain examples types e examples switch action situated
turn action vice versa caused fact helpful
context different operators choose equally good choices

turnto

switchon





switchon

calibrate



turnto



takeimage

g

turnto


figure solution path alternatives simplified satellite
roller follows commitment generation training examples generation solutions given training roller performs exhaustive search obtain
multiple best cost solutions taking account alternatives different binding choices
selection solutions roller selects subset solutions set best cost solutions
order reproduce particular preference operator alternatives extraction examples solutions roller encodes selected subset solutions examples required
learning operator classification binding classification following sections detail roller
proceeds three steps



g

fid e la rosa j imenez f uentetaja b orrajo

g eneration olutions
roller solves training best first branch bound bfs bnb
extracts multiple good quality solutions search space explored exhaustively
within time bound discarded examples generated therefore
training need sufficiently small addition training need representative enough generalize dck assists roller solving future
domain
bfs bnb search completed without pruning repeated states practice many repeated
states generated changing order among actions different solution paths thus pruning
repeated states would involve tagging actions leading solutions rejected bindings
fact true addition bfs bnb prunes according evaluation
function f n g n h n g n node cost work use plan length
cost function h n heuristic safe way prune search space
admissible heuristic however existing admissible heuristics allow roller complete
exhaustive search reasonable size practice heuristic produces
overestimations introduces negligible noise learning process end
search bfs bnb returns set solutions best cost solutions
used tag nodes search tree belong solutions label solution

electing olutions
set best cost solutions found roller selects subset solutions used
generating training examples since difficult develop domain independent criteria systematically selecting solutions reproduce operator selection particular context
defined heuristically prefers actions others preferences

least commitment preference prefer actions generate alternatives different solution paths
difficulty preference prefer actions reach goals sub goals difficult
achieve example figure instrument switched achievable
one action hand pointing direction considered easier since
achieved actions turn turn
given best cost plan task compute preferences
functions depending action
commitment ai successors ai solution
function successors ai returns applicable actions state si function solution
verifies whether action tagged part best cost plan
difficulty ai

min


supporters l

ladd ai

function supporters l l add returns set actions achieve
literal l


fis caling h euristic p lanning r elational ecision rees

solutions ranked according preferences ranking solution
computed weighted sum action preferences follows
ranking

x
n

n
ai
n

n plan length one commitment difficulty sum weighted
give importance preferences first actions plan first action preference
value multiplied second n n otherwise several alternatives e
commutative actions different positions within plan would lead ranking value
compute ranking best cost solutions commitment ties ranking broken
ranking computed difficulty subset solutions best ranking values
subset solutions selected generating training examples
e xtracting e xamples olutions
takes subset solutions selected previous step generates training examples generating examples operator classification roller takes solution plans
correspond sequence state transitions sn generates
one learning example pair si ai consisting h si class e operator
name action ai see learning example shown figure
generating examples binding classification operator roller considers
pairs si ai ai matches operator learning example generated pair
si ai binding selection operator consists h si classes
applicable actions si match including ai applicable actions solution label
belong selected class applicable actions rejected class moreover actions
belonging solutions top ranking still marked selected even though
nodes example generated see learning example shown figure
roller

use learned knowledge relational decision trees
section details make heuristic benefit dck beginning
build action orderings learned dck explains two different search strategies
exploit orderings application dck generalized action policy depthfirst h context policy use dck generate lookahead states within
best first search bfs guided heuristic h context policy lookahead bfs
rdering actions r elational ecision rees
given state expression app denotes set actions applicable learned dck
provides ordering app ordering built matching action app first
operator classifier corresponding binding classifier figure shows detail
ordering applicable actions relational decision trees
divides set applicable actions two subsets helpful actions
non helpful actions matches helpful context state e h tree
operator classification matching provides leaf node contains list operators
sorted number examples covered leaf training phase see operator


fid e la rosa j imenez f uentetaja b orrajo

dt filter sort h sorted list applicable actions
list actions
h helpful context
decision trees

selected actions
ha helpful actions h
non ha ha
leaf node classify operators tree h
ha
priority leaf node operator value leaf node
priority
selected rejected classify bindings tree h
selected
selection ratio selected rejected
priority priority selection ratio
selected actions selected actions
max ha priority maxaselected actions priority
non ha
priority leaf node operator value leaf node
priority max ha priority
selected rejected classify bindings tree h
selected
selection ratio selected rejected
priority priority selection ratio
selected actions selected actions
return sort selected actions priority

figure ordering actions relational decision trees
classification tree figure number examples covered gives operator ordering
used prefer actions search uses number initialize
priority value helpful action taking value corresponding operator
keeps helpful actions least one matching example actions
matches action corresponding binding classification tree resulting leaf
binding tree returns two values number times ground action selected
number times rejected training phase define selection ratio ground
action
selected
selection ratio
selected rejected
ratio represents proportion good bindings covered particular leaf binding
tree denominator zero selection ratio assumed zero priority
action updated adding selection ratio thus final priority action higher


fis caling h euristic p lanning r elational ecision rees

actions operators operator classification tree provides higher values e
selected often training examples since selection ratio remains
adding number considered method breaking ties initial priority
value information binding classification tree
priority non helpful actions computed similar way except case
considers actions whose initial priority value provided operator classification tree higher maximum priority helpful actions manner capture
useful non helpful actions follows heuristic criterion classify action helpful although
heuristic shown useful case may arise useful action
particular moment classified helpful decision trees capture information given
recommend choosing non helpful action described method takes advantage
fact defines way information applying learned knowledge alternative would extend context meta predicate non helpful
actions however pay variety domains means significantly larger contexts causes expensive matching finally selected actions
sorted order decreasing priority values sorted list actions output
h c ontext p olicy lgorithm
helpful context action policy moves forward applying state best action
according dck pseudo code shown figure
maintains ordered open list open list contains states expanded extracted
order extracted state evaluated heuristic thus evaluate upon
extraction nodes included open list evaluation provides heuristic
value state h set helpful actions ha needed generate helpful
context heuristic value used continuing search state recognized
dead end h goal checking h helpful context generated subsequently obtains set aa actions applicable state sorts
decision trees shown figure aa aa sorted list
applicable actions inserts successors generated actions aa beginning open list preserving ordering function push ordered list open
furthermore make complete robust successors generated applicable
actions aa included secondary list called delayed list delayed list
used open list empty case one node delayed list moved
open list continues extracting nodes open list
node maintains pointer parent order recover solution
found node maintains g value e length path
initial state node function push ordered list open inserts
open list candidates repeated states repeated states lower g
value previous one otherwise repeated states pruned type pruning guarantees
maintain node shortest solution found
words proposed search depth first search delayed successors
benefit exploits best action selection policy per

fid e la rosa j imenez f uentetaja b orrajo

depth first h context policy g plan
initial state
g goals
decision trees

open list
delayed list
open list
n pop open list
h ha evaluate n g compute heuristic
h recognized dead end
continue
h goal state
return path n
h helpful context ha g n
aa applicable actions n
aa dt filter sort aa h
candidates generate successors n aa
open list push ordered list open candidates open list
delayed candidates generate successors n aa aa
delayed list push ordered list delayed candidates delayed list
open list delayed list
open list pop delayed list
return fail

figure depth first sorting strategy given dck
fect action ordering particularly perfect dck directly applied
backtrack free search inaccurate dck force search backtrack
h c ontext p olicy l ookahead trategy
many domains learned dck may contain flaws helpful context may expressive
enough capture good decisions learning may able generalize well
training examples may representative enough cases direct application
learned dck without backtracking may allow planner reach goals
poor quality learned dck balanced guide different nature
domain independent heuristic successful example obtusewedge system yoon et al
combined learned generalized policy heuristic obtusewedge exploited
learned policy synthesize lookahead states within lookahead strategy lookahead states
perfect policy refer policy leads directly goal state policies guaranteed perfect
given generated inductive learning



fis caling h euristic p lanning r elational ecision rees

first applied heuristic yahsp planner vidal intermediate
states frequently closer goal state direct descendants current state
intermediate states added list nodes expanded used within
different search learned policy contains flaws lookahead states synthesized
policy may provide good guidance search however lookahead states
included complete search considers ordinary successors search
process becomes robust general use lookahead states forward state space
search slightly increases branching factor search process overall shown
yahsp planner ipc experiments included yahsp vidal
seems improve performance significantly
figure shows generic lookahead states generated policy
search weighted best first search bfs modification one lookahead states inserted open list expanding node
weighted bfs nodes expanded maintained open list ordered evaluation
function f n h n g n apart usual arguments bfs receives
policy p horizon horizon represents maximum number policy steps
applied generating lookahead states experiments use
heuristic h n

h context policy lookahead bfs g horizon plan
initial state
g goals
decision trees policy
horizon horizon
open list
add open
open list
n pop open list
goal state n g
return path n
add open lookahead successors n g horizon
add open standard successors n
return fail

figure generic lookahead bfs
heuristic evaluation h n g value g n set helpful actions saved
node node evaluated function add open state evaluates
state inserts open list ordered increasing values evaluation function
f n function prunes repeated states following strategy described depthfirst h context policy repeated states higher g n existent one


fid e la rosa j imenez f uentetaja b orrajo

pruned function add open standard successors n calls add open
successor node n function add open lookahead successors explained
adapted generic lookahead bfs learned dck particular
instantiation function add open lookahead successors shown figure
case lookahead states generated iteratively applying first action action ordering provided dck inputs current state goals
decision trees horizon first generates helpful context applicable
actions helpful actions n ha recovered node sorts applicable actions decision trees previously shown figure
successor generated first action inserted open list recursive
call successor horizon decremented one function add open returns
true argument added open list false otherwise fact returns
false two cases state repeated state g value higher g value
existent state state recognized dead end ordered list becomes empty
lookahead state generated initial node returned occurs
horizon zero described implementation similar lookahead strategy
followed obtusewedge instead perform lookahead generation helpful contexts
relational decision trees
hand described h context policy lookahead bfs search
perfomed set applicable actions node however many domains use
helpful actions shown good heuristic one possible way prioritizing helpful
actions non helpful actions include open list successors given helpful
actions include remaining successors secondary list implemented idea
following strategy used depth first h context policy open list
becomes empty one node passed secondary list open list search
continues still complete given prune successor helpful
actions good enough strategy save many heuristic evaluations experiments
compare strategy previous one intuition adequacy
strategy depends directly quality helpful actions quality learned dck
accuracy heuristic particular domain
another technique prioritizing helpful actions bfs implemented yahsp vidal
inserts two consecutive instances node open list nodes
equal f n since represent state first one contains helpful actions
therefore expanded generates successors resulting actions second
contains non helpful actions called rescue actions way successors lower
f n parent node sub tree generated helpful actions expanded
successor resulting non helpful actions
performed preliminary experiments obtaining similar two described methods prioritizing helpful actions bfs use secondary list non helpful
actions use rescue nodes reason include first technique
experimental section call h context policy lookahead bfs ha
state repeated g value smaller existent one add open evaluate
instead takes heuristic evaluation existent state



fis caling h euristic p lanning r elational ecision rees

add open lookahead successors n g horizon state
n node state
g goals
decision trees policy
horizon horizon
horizon
return n
h helpful context n ha g n
aa applicable actions n
aa dt filter sort aa h
aa
pop aa
n generate successor n
added add open n
added
goal state n g
return n
return add open lookahead successors n g horizon
return n

figure generating lookahead states decision trees

experimental
section evaluate performance roller system evaluation carried
variety domains belonging diverse ipcs four domains come learning track
ipc gold miner matching blocksworld parking thoughtful rest domains
blocksworld depots satellite rovers storage tpp selected among domains
sequential tracks ipc presented different structure
difficulty available random generators automatically build training sets learning dck domain complete training phase
roller learns corresponding dck testing phase evaluate scalability
quality solutions found roller learned dck next detail experimental
obtained two phases moreover domains give particular
details training test sets learned dck observed roller performance
training phase
domain built training set thirty randomly generated size
structure discussed particular details given domain
explained section roller generates training examples solving


fid e la rosa j imenez f uentetaja b orrajo

training set bfs bnb search set time bound seconds solve training
discarding exhaustively explored time bound roller
generates training examples solutions found builds corresponding decision
trees tilde system blockeel de raedt
evaluate efficiency roller training phase computed following metrics
time needed solving training number training examples generated
process time spent tilde learning decision trees number leaves
operator selection tree last number gives clue size learned dck table
shows obtained domain
domain
blocksworld
depots
gold miner
matching bw
parking
rovers
satellite
storage
thoughtful
tpp

training
time











learning
examples











learning
time











tree
leaves











table experimental training process training learning times shown
well number training examples complexity generated trees number
leaves

achieves shorter learning times fourth column table state art
systems learning generalized policies martin geffner yoon et al particularly
systems implement ad hoc learning sometimes require hours order
obtain good policies needs seconds learn dck given domain
fact makes suitable architectures need line learning processes however learning times constant different domains depend
number training examples work number given amount different
solutions training size training examples work size
given number arity predicates actions domain training
examples structured e whether examples easily separated learning
roller

testing phase
testing phase roller attempts solve domain set thirty test
taken evaluation set corresponding ipc evaluation set
contains thirty thirty hardest ones depots domain
exception twenty two evaluation set domain ipc


fis caling h euristic p lanning r elational ecision rees

contained twenty two three experiments made testing phase
first one evaluates rollers performance dck learned solutions training
ranked solution second one evaluates usefulness
learned dck third one compares roller state art planners experiment evaluate two different dimensions solutions found roller scalability
quality testing experiments done ghz processor time bound
seconds gb memory bound
olution r anking e valuation
experiment evaluates effect selecting solutions following described section roller configurations evaluation
top ranked solutions depth first h context policy dck learned
sub set top ranked solutions use search since performance depends quality learned dck
dck
solutions depth first h context policy dck learned solutions obtained bfs bnb
table shows number solved configuration time plan
length average computed solved configurations number brackets
first column number solved common top ranked solutions configuration solved thirty solutions configuration mainly due difference
matching blocksworld domain
domains
blocksworld
depots
gold miner
matching bw
parking
rovers
satellite
storage
thoughtful
tpp
total

top ranked solutions
solved
time length


































solutions
solved
time length


































table solved time plan length average evaluation ranking solution
heuristic

effect selecting solutions varies across domains instance quite important regarding plan quality blocksworld depots rovers satellite domain top ranked
solutions allow roller solve two maintaining similar time plan length
seconds time bound established learning track ipc



fid e la rosa j imenez f uentetaja b orrajo

average gold miner domain selecting solutions irrelevant equally
good solutions per e goal single fact gold fairly
top ranked ones parking domain benefit selecting solutions
considering overall think selecting solutions useful heuristic improving
dck quality many domains remaining evaluations refer dck used
roller decision trees learned top ranked solutions
dck u sefulness e valuation
shown ipc learning track dck may degrade performance base planner
dck incorrect mind designed experiment measure performance roller comparing versions without dck made two versions
non learning first one empty configuration decision
tree given thus ordering computed helpful actions second one
systematic configuration ordering supplied heuristic instead
roller configurations used comparisons
roller depth first h context policy dck learned training
phase
roller bfs h context policy lookahead bfs dck learned
training phase configuration uses horizon h choose value
basis empirical evaluations
roller bfs ha modified version roller bfs helpful actions considered immediate successors lookahead states generated original version horizon
three equivalent version empty configuration
df ha depth first helpful actions empty dck roller corresponds depthfirst helpful actions original non helpful actions
placed delayed list
bfs empty dck roller bfs generate lookahead states e add open lookahead successors figure therefore becomes
standard best first search
bfs ha modified version bfs helpful actions considered non helpful
actions placed delayed list
previous configurations systematic version case action ordering computed
heuristic
gr ha greedy helpful actions corresponds greedy search
helpful actions node helpful immediate successors sorted heuristic
non helpful nodes go delayed list
lh bfs lookahead bfs bfs lookahead states function dt filter sort
replaced function computes ordering heuristic


fis caling h euristic p lanning r elational ecision rees

lh bfs ha modified version lh bfs helpful actions considered nonhelpful actions placed delayed list
comparison computed number solved scores used
ipc learning track evaluate planners performance terms cpu time quality plan
length time score computed follows planner receives ti ti
points ti minimum time participant used solving ti
cpu time used planner question test set planner receive
points higher score better quality score computed way replacing
l l measures quality terms plan length addition compute time
quality averages solved configurations configuration solve
taken account measure average measures complement scores
since give direct information commonly solved scores tend benefit
configurations solve others
table shows summary obtained dck usefulness evaluation
configuration compute number domains top performer
evaluated criteria e numbers solved time quality scores averages top performer domain equal better measure
rest table points number
evaluated domains global section refers overall top performers relative section refers
number domains configuration equal better two configurations
strategy e depth first best first best first helpful actions averages
commonly solved computed configurations solve one roller good number solved speed metrics
regarding quality score roller roller bfs ha best performers three domains
however bfs bfs ha obtained better quality average
global
solved
time score
time average
quality score
quality average
relative
solved
time score
time average
quality score
quality average

depth first
roller gr ha df ha

































best first
roller bfs lh bfs






















bfs











helpful best first
roller bfs ha lh bfs ha bfs ha

































table summary dck usefulness evaluation column gives number domains
configuration top performer row item

table shows number solved dck usefulness evaluation total
row shows roller configuration solved empty systematic
versions time quality scores reported table table appendix


fid e la rosa j imenez f uentetaja b orrajo

detailed averages considered less interesting since many domains
common solved easy
domains
blocksworld
depots
gold miner
matching bw
parking
rovers
satellite
storage
thoughtful
tpp
total

depth first
roller gr ha df ha


































best first
roller bfs lh bfs
bfs

































helpful best first
roller bfs ha lh bfs ha bfs ha


































table solved dck usefulness evaluation

ime p erformance c omparison
experiment evaluates scalability roller system compared state art planners comparison chosen lama richter westphal winner
sequential track past ipc last ipc shown still competitive
used three roller configurations explained previous evaluation configuration
planners
running enforced hill climbing ehc helpful actions together
complete bfs case ehc fails though planner dates include
evaluation shown ipc still competitive
state art planners besides planner extensively used
learning systems
lama first winner classical track ipc configuration lama
modified stop finds first solution way comparison fair
rest configurations implement anytime behavior e continuous solution
refinement reaching time bound anytime behavior lama compared later
roller performance next section
table shows number solved together speed score
give overall view performance different planners roller solves many
configuration domains achieves top speed score
seven domains second best score belongs roller bfs ha solves many
planners six domains lama first fairly competitive since solves seven
less roller roller bfs ha cases lama first
achieves lower speed score
planner actually metric running strips domains consider implementation adequate baseline
comparison roller implemented code rather original order extend




fis caling h euristic p lanning r elational ecision rees

domain
blocksworld
depots
gold miner
matching bw
parking
rovers
satellite
storage
thoughtful
tpp
total

roller
solved
score























roller bfs
solved
score






















roller bfs ha
solved
score
























solved












score












lama first
solved
score























table solved speed score five configurations

table shows average time five configurations addressing subset solved configurations first column shows parenthesis number commonly
solved closely related shown table roller achieves
best average time eight ten domains observe different configurations
good particular domains even particular instance thoughtful
domain four solved configurations
domain
blocksworld
depots
gold miner
matching bw
parking
rovers
satellite
storage
thoughtful
tpp

roller











roller bfs











roller bfs r






















lama first











table time averages solved configurations

q uality p erformance c omparison
experiment compares quality first solutions found solutions found
anytime behavior anytime configuration planners exhaust time bound trying improve
incrementally best solution found three roller modified configuration
best solution found far used upper bound order prune nodes
exceed plan length anytime behavior regular configuration lama
anytime behavior included anytime comparison well base
comparing quality improvements planners
table shows quality scores first solution last solution found
anytime configurations anytime column planner shows score variation reveals
whether planner able make relative improvements first solutions relative


fid e la rosa j imenez f uentetaja b orrajo

domain
blocksworld
depots
gold miner
matching bw
parking
rovers
satellite
storage
thoughtful
tpp
total

roller
first anytime























roller bfs
first anytime























roller bfs ha
first
anytime
























first












relative












lama
first anytime























table quality scores first solution anytime configuration evaluated planners

shows score solutions compared solutions given anytime configuration
planners loses points cases others able improve
solutions two lama configurations obtained top score category nevertheless
planner dominated domains furthermore configurations achieved top quality score
first solution least one domain
domain
blocksworld
depots
gold miner
matching bw
parking
rovers
satellite
storage
thoughtful
tpp

roller
first
anytime





















roller bfs
first
anytime





















roller bfs ha
first
anytime






















first











relative











lama
first
anytime





















table quality averages first solution anytime configuration evaluated planners

table shows plan length average solved configurations first
column shows average first solutions anytime column gives average
last solutions anytime configuration commonly solved
reported table although planner solved fewer achieves
best average plan length seven domains plan length averages reveal roller able
first solutions good quality domains roller bfs roller bfs ha
better quality solutions roller several domains averages competitive
lama roller bfs roller bfs ha better quality performance mainly due
combination learned dck domain independent heuristic within bfs
following subsections discuss particular details domains give
brief description domain together information training test sets used
experimental evaluation domain analyze learned dck obtained


fis caling h euristic p lanning r elational ecision rees

order give fine grained interpretation observed performance details
domains found ipc web site
b locksworld etails
domain concerned configuring towers blocks robotic arm
training set used experiments consisted ten eight block ten nine block
ten ten block test set consisted largest typed
ipc blocks
blocksworld domain

roller
roller bfs
roller bfs ha
lama first

percentage solved















cpu time





figure percentage solved increasing time evaluating scalability performance blocksworld domain
although domain one oldest benchmarks automated still challenging state art heuristic planners blocksworld presents strong interaction among goals
current heuristics fail capture particular achieving goal domain may undo
previously satisfied goals therefore crucial achieve goals specific order dck
learned roller gives total order domain actions different contexts capturing key
knowledge lets roller achieve impressive scalability producing good quality
solution plans roller configurations considerably better non learning configurations
particularly roller solved thirty set df ha gr ha solve
roller quite good compared state art planners figure
observe roller performs two orders magnitude faster lama x axis
figure represents cpu time logarithmic scale axis represents percentage
solved particular time moreover roller obtained best quality score first
solution anytime evaluations addition average plan length common fairly
close best average obtained roller bfs roller bfs ha bfs
http idm lab org wiki icaps index php main competitions



fid e la rosa j imenez f uentetaja b orrajo

scale well domain partially guided heuristic considerably
underestimates distance goals similarly lookahead states generated policy
discarded fail escape plateaus generated heuristic function
analyzing learned operator tree found explanations good performance
roller blocksworld domain operator tree clearly split two parts first part contains decisions take arm holding block situation tree captures
stack put block second part contains decisions take arm empty
case tree captures unstack pick block second part tree
current state search matches logical query helpful unstack block block
means tower blocks block well arranged e block least
one block beneath block well placed therefore set helpful actions compactly
encodes useful concept bad tower kind knowledge manually defined
previous works order learn good policies blocksworld one consisted including recursive definitions predicates support predicates x
inplace x khardon another alternative involved changing representation language instance concept language martin geffner taxonomic syntax yoon
fern givan kleene star operator taxonomic syntax e operator defining recursion discarded subsequent work yoon et al predicate
used instead rollers ability recognize bad towers without extra predicates arises
misplaced block tower makes unstack action top block helpful since
part relaxed plan arm empty
due extraordinary performance roller domain built extra test set
clarify whether trend observed roller configuration would hold larger
aim randomly generated distributed sub sets
blocks sub set roller solved
extra test set time average seconds per spending solve
obviously became difficult roller number blocks increase
epots etails
domain combination transportation domain blocksworld domain
crates instead blocks hoists instead robot arm consist trucks
transporting crates around depots distributors hoists crates stacked onto pallets
top crates final destination domain training
different combinations locations depots distributors trucks pallets per
location hoist per location crates placed different configurations
testing phase used ipc set hardest
locations pallets hoists trucks crates
roller roller bfs improve performance non learning strategies three
configurations bfs helpful action solved roller able solve
achieving best speed score however high average plan length indicates
policy producing good quality plans roller bfs ha obtains second best speed score
competitive plan lengths figure shows percentage solved
explained section logic queries roller present example ids case ids
ignored simplicity given needed matching current helpful context



fis caling h euristic p lanning r elational ecision rees

increasing cpu time logarithmic scale anytime configuration roller bfs ha
able refine solutions achieving quality average similar lama
depots domain

roller
roller bfs
roller bfs ha

lama first

percentage solved






















cpu time

figure percentage solved increasing time evaluating scalability performance depots domain

dck learned domain provides inaccurate advice large contexts
instance roller makes mistakes deciding crate unload several crates
loaded truck reason inaccurate dck training large
enough gain knowledge addition adding crates makes unfeasible solved bfs bnb nevertheless limitation learned dck
evident depots domain undirected e actions reversible
dead ends therefore mistakes made dck fixed additional actions leads
worse quality plans besides since first solutions rapidly found roller configurations
spend time refining solutions reason great improvement plan average
roller bfs ha
g old iner etails
objective domain navigate grid cells reaching cell containing gold
cells occupied rocks cleared bombs laser domain
training set consists cells cells
cells domain part learning track ipc used
test set used competition set ranging cells
gold miner domain solvable helpful actions alone explains
difference number solved roller roller bfs ha nonlearning counterpart general terms domain trivial roller roller bfs ha
solved test less seconds per lama nevertheless scales

fid e la rosa j imenez f uentetaja b orrajo

poorly domain essential actions picking bombs frequently considered
helpful actions relaxed solvable laser consequently fails
solve ehc requires additional bfs search figure shows
percentage solved increasing cpu time regarding anytime evaluation
tested configurations improved first solution found many
gold miner domain


percentage solved








roller
roller bfs
roller bfs ha

lama first













cpu time

figure percentage solved increasing time evaluating scalability performance gold miner domain
domain operator tree succeeds capturing key knowledge initial states
bombs laser cell robot needs decide pick
operator tree domain matches logical query candidate pickup laser cell
higher ratio operator pickup bomb operator pickup laser operator
preference allows roller avoid dead ends laser destroys gold hand
situations laser required e destroy hard rocks reached second choice
policy fact implies backtracking roller additional evaluated nodes
significantly affect overall performance preference pickup bomb
pickup laser action example selecting non helpful actions
atching b locksworld etails
domain version blocksworld designed analyze limitations relaxed plan heuristic
version blocks polarized positive negative two polarized robot
arms furthermore block placed stack put actions arm different
polarity block becomes damaged block placed top however picking
unstacking block wrong polarity seems harmless fact makes recognizing
dead ends difficult task heuristic particularly relaxed task blocks never
damaged thus relaxed plan consequently set helpful actions heuristic
estimation wrong training set used domain consists fifteen blocks


fis caling h euristic p lanning r elational ecision rees

fifteen blocks used even number blocks keep balanced e
half blocks polarity testing phase used test set learning track
ipc set ranging blocks
df ha gr ha solve solvable
helpful actions alone learned dck recommended useful non helpful actions thus
roller able solve policy configurations perform better systematic strategies fairly similar lookahead strategy fact reveals learned dck
effective enough pay effort building lookahead states lama planner
solves figure shows percentage solved increasing
cpu time
matching bw domain

roller
roller bfs
roller bfs ha

lama first

percentage solved















cpu time





figure percentage solved increasing time evaluating scalability performance matching blocksworld domain

roller solved evaluating considerable number nodes plan length
means dck learned domain accurate analyzing training
examples many solution plans satisfy key knowledge domain robot
arms unstack pick blocks polarity specifically robot handling
top block e block blocks goal state polarity robot
arm becomes meaningless effect unavoidable shortest plans involve managing
top blocks efficient way ignoring polarities examples include noise
learning make generalization complex

parking etails
domain involves parking cars street n curb locations cars double
parked triple parked goal move one configuration parked cars another
driving cars one curb location another domain training set consists fifteen


fid e la rosa j imenez f uentetaja b orrajo

six cars four curbs fifteen eight cars five curbs testing
used test set learning track ipc hardest set cars
curbs
three roller configurations solve perform significantly better nonlearning strategies addition three roller configurations outperform lama
difference one order magnitude reason lama low speed
scores roller configurations consistently better systematic empty configurations figure shows percentage solved increasing cpu time
hand three roller configurations achieve first solutions suficient quality however solutions refined anytime evaluation especially roller bfs ha
achieves top quality score plan length average fairly similar lama
parking domain

roller
roller bfs
roller bfs ha

lama first

percentage solved















cpu time





figure percentage solved increasing time evaluating scalability performance parking domain

learned dck domain quite effective roller rarely backtracked operator tree perfectly classifies move car curb action first tree node asking
considered helpful action besides binding tree operator selects right car asking
target goal rejecting candidates two decisions guide planner place
car right position whenever possible large number nodes evaluated
explains scalability difference lama
rovers etails
domain simplification tasks performed autonomous exploration vehicles sent
mars tasks consist navigating rovers collecting soils rocks samples taking
images different objectives domain training set consists ten one
rover four waypoints two objectives one camera ten additional camera


fis caling h euristic p lanning r elational ecision rees

ten additional rover test set thirty largest
ipc set e largest set rovers
waypoints
dck strategies faster systematic empty strategies differences significant since configurations solved one hand helpful actions
rovers domain quite good hand test set
big enough generate differences among approaches regarding planner comparison roller
achieves top performance score scales significantly better solves two
less lama figure shows percentage solved increasing cpu time
regarding anytime evaluation planners able refine first solutions lama gets
top quality score best plan length refining solutions
rovers domain

roller
roller bfs
roller bfs ha

lama first

percentage solved






















cpu time

figure percentage solved increasing time evaluating scalability performance rovers domain

domain roller learned imperfect dck manages achieve good scalability
dck imperfect partially actions communicating rock soil image analysis
applied order among therefore preferences ranking selecting
solutions fail discriminate among actions confuse learning since
actions could applied order dck mistakes seem harmless
time
atellite etails
domain comprises set satellites different instruments operate different
formats modes tasks consist managing instruments taking images certain targets
particular modes domain training set consist thirty one satellite two
instruments five modes five observations test set thirty largest


fid e la rosa j imenez f uentetaja b orrajo

ipc e largest set satellites modes
observations
three roller configurations improved number solved non learning
counterpart addition roller roller bfs ha solved set two
lama eight figure shows percentage solved
increasing cpu time roller roller bfs ha achieve good quality solutions
able refine anytime evaluation achieving plan lengths similar lama
satellite domain

roller
roller bfs
roller bfs ha

lama first

percentage solved






















cpu time

figure percentage solved increasing time evaluating scalability performance satellite domain

learned dck captures key knowledge satellite domain trees shown
figure figure part learned dck fewer training examples domain
roller roller bfs ha perform quite similarly reason heuristic
quite accurate domain thus deepest lookahead state generated learned policy
frequently selected heuristic bfs search
torage etails
domain concerned storage set crates taking account spatial configuration depot domain tasks comprise hoists move crates containers
particular area depot training set consists depot container
hoist different combinations crates areas inside depot
test set used ipc set largest domain
depots areas hoist crates
first trivially solved configurations difficulty increases quickly number objects increases bfs solved one
dck strategy meaning dck lookahead strategies pay domain


fis caling h euristic p lanning r elational ecision rees

hard lama figure shows percentage solved increasing
cpu time
storage domain

roller
roller bfs
roller bfs ha

lama first

percentage solved






















cpu time

figure percentage solved increasing time evaluating scalability performance storage domain

although dck effective found interesting properties learned operator tree
compact succeeds selecting go action normally marked helpful
action
houghtful etails
domain version solitaire card game cards visible one
turn card talon rather cards time original version goal
game place cards ascending order corresponding suit stacks home deck
available random generator domain therefore used bootstrap
distribution given learning track ipc set contains
four suits card seven suit test phase used
test distribution learning ipc largest domain full set
standard card game
roller solves three fewer gr ha however roller bfs roller bfs ha better number solved non learning approaches domain
use dck lookahead construction combined heuristic makes search process
robust policy mistakes roller bfs ha solves three lama figure
shows percentage solved increasing cpu time
bfs bnb generating training examples able solve
bootstrap distribution believe different bootstrap distribution smaller would generate accurate dck additionally even though dck


fid e la rosa j imenez f uentetaja b orrajo

thoughtful domain

roller
roller bfs
roller bfs ha

lama first

percentage solved















cpu time





figure percentage solved increasing time evaluating scalability performance thoughtful domain

lookahead strategies achieve good learning accurate decision trees complex
many classes operators particular domain many arguments
predicates background knowledge parameters operator col home
parameters operator col home b
tpp etails
tpp stands traveling purchase generalization traveling salesman
tasks domain consist selecting subset markets satisfy demand
set goods selection markets try optimize routing purchasing
costs goods strips version graph connects markets equal costs
arcs nevertheless domain still interesting difficult planners scale
increasing number goods markets trucks training set consists thirty
number goods trucks depots varying one three load levels five
six test set consists thirty used planner evaluation ipc
largest set goods trucks markets load level six
roller gr ha df ha solved test set roller performs faster
two achieving similar plan lengths besides roller outperforms rest
planners two orders magnitude faster main reason overwhelming
branching factor large together fact heuristic falls big plateaus
domain greedy depth first approaches perform better avoid effect
plateaus additionally roller achieved competitive quality scores average plan length
first solution anytime evaluation roller bfs roller bfs ha got bad



fis caling h euristic p lanning r elational ecision rees

domain imprecision heuristic figure shows percentage
solved increasing cpu time
tpp domain


percentage solved








roller
roller bfs
roller bfs ha

lama first













cpu time

figure percentage solved increasing time evaluating scalability performance tpp domain
learned dck compact useful reducing number evaluations shown
roller performance instance drive binding tree recognizes perfectly truck
market need go market b already truck b handling goods
market situations state truck b helpful action drive meaning
truck b something deliver

lessons learned ipc
ipc included specific track systems benefit learning thirteen systems
took part track including previous version roller de la rosa et al achieved
th position version upgrade original roller system de la rosa et al
first version proposed ehc sorted alternative h context policy effective many domains competing version tried recommend ordering
applying actions relaxed plans idea although initially appealing good
choice usefulness strongly depends fact relaxed plan contains right actions competition completed analysis roller performance diagnose
strengthen weak points system resulting improvements roller version
described article one example roller improvements obtained
thoughtful matching blocksworld domains ipc roller failed solve thoughtful domain solved two matching blocksworld
reported section current version roller solves respectively
domains addition current version roller outperforms lama park

fid e la rosa j imenez f uentetaja b orrajo

ing domain one order magnitude improvements roller overcome limitations
version submitted ipc three aspects
robustness wrong dck issues discussed section decisions introduce biases learning process making learning dck complex task fact competitor
ipc able learn useful dck domains furthermore many domains
learned dck damaged performance baseline planner case
roller described strengthened roller wrong dck
proposing two versions modified bfs combine learned dck
numerical heuristic combination dck heuristic makes process robust imperfect incorrectly learned knowledge similar
followed winner best learner award btuse w edge yoon et al
efficiency baseline overall competition winner p bp gerevini saetti vallati portfolio state art planners learns planner settings
best ones given domain performance competitor
never worse performance state art planner ipc baseline performance roller far competitive state art planners
roller coded lisp overcome weakness optimized
implementation roller c code outperformed ipc
domains
definition significant training sets training examples extracted experience
collected solving training set therefore quality training
examples depends quality used training ipc training
fixed organizers many domains large
roller system extract useful dck created training
random generators build useful training sets roller system domain
selection training examples relational classifiers induce set rules trees model
regularities training data case forward state space search
best cost solutions may used training data leads alternatives confuse learner avoid training data cleaned
used learning ranking solution selection proposed article
option give learner training data clearer regularities
additionally roller performed poorly sokoban n puzzle domains traditionally
useful dck domains form numeric functions manhattan distance
provides lower bound solution length general action policies inaccurate
domains lack knowledge trajectory goals currently still
unable learn useful dck roller domains possible future direction introduce
goals subgoals e g landmarks helpful context aim capturing
knowledge

related work
strongly inspired way prodigy veloso et al dck
prodigy architecture action selection two step process first prodigy selects uninstan

fis caling h euristic p lanning r elational ecision rees

tiated operator apply second selects bindings operator selections
guided dck form control rules leckie zukerman minton
returned idea two step action selection allows us define learning
dck standard classification task therefore solve learning task
shelf classification technique relational decision trees nevertheless roller
need distinguish among different kinds nodes prodigy roller performs
standard forward heuristic search state space search nodes
type
relational decision trees previously used learn action policies context
relational reinforcement learning rrl dzeroski de raedt blockeel comparison
dck learned roller rrl action policies present two limitations solving first rrl learned knowledge targeted given set goals therefore
rrl cannot directly generalize learned knowledge different goals within given domain
second since training examples rrl consist explicit representations states rrl needs
add extra background knowledge learn effective policies domains recursive predicates
blocksworld
previous works learning generalized policies martin geffner yoon et al
succeed addressing two limitations rrl first introduce goals
training examples way learned policy applies set goals domain second
change representation language dck predicate logic concept language
language makes capturing decisions related recursive concepts easier alternatively roller
captures effective dck domains blocksworld without varying representation language
roller implicitly encodes states terms set helpful actions state
roller benefit directly shelf relational classifiers work predicate logic
fact makes learning times shorter resulting policies easier read
recently techniques developed improve performance heuristic
planners
learning macro actions botea enzenberger muller schaeffer coles smith
combination two operators considered domain operators order reduce search tree depth however benefit decreases number
macro actions added enlarge branching factor search tree causing utility minton approaches overcome applying
filters decide applicability macro actions newton levine fox long
two versions work participated learning track ipc obtaining
third fourth place one advantage macro actions learned knowledge
exploited planner thus approaches learn generalized policies could
benefit macro actions nevertheless far know combination
tried improving heuristic planners
learning domain specific heuristic functions yoon fern givan
xu fern yoon state generalized heuristic function obtained examples
solution plans main drawback learning domain specific heuristic functions
learning difficult understand humans makes
verification learned knowledge difficult hand learned knowledge
easy combine existing domain independent heuristics slightly different


fid e la rosa j imenez f uentetaja b orrajo

consists learning ranking function greedy search xu fern yoon
step greedy search current node expanded child node
highest rank selected current node case ranking function
iteratively estimated attempt cover set solution plans greedy
learning task decomposition learns divide tasks given
domain smaller subtasks easier solve techniques reachability analysis
landmark extraction hoffmann porteous sebastia able compute intermediate states must reached satisfying goals however clear
systematically exploit knowledge build good decompositions vidal et al consider optimization use specialized optimization
discover good decompositions
general system learns dck deal ambiguity training
examples given state may present many good actions trying learn dck
selects one action inherently equal complex learning cope
ambiguous training data roller created function ranks solutions aim learning
kind solutions different followed xu et al generate
training examples partially ordered plans

conclusions future work
presented technique reducing number node evaluations heuristic learning exploiting generalized policies technique defines process
learning generalized policies two step classification builds domain specific relational decision trees capture action selected different contexts work
contexts specified helpful actions state pending goals static
predicates finally explained exploit learned policies solve
classical applying directly combining domain independent
heuristic lookahead strategy bfs work contributes state art
learning three ways
representation propose encoding generalized policies able capture
efficient dck predicate logic opposed previous works represent generalized
policies predicate logic khardon representation need extra background knowledge support predicates learn efficient policies blocksworld domain
besides encoding states set helpful actions frequently compact furthermore set normally decreases search fewer goals left thus process
matching dck becomes faster search advances towards goals
learning defined task learning generalized policy two step standard
classification task thus learn generalized policy shelf tool
building relational classifiers obtained tilde system blockeel de raedt tool learning relational classifiers could
used advances relational classification applied straightforward
manner roller learn faster better dck


fis caling h euristic p lanning r elational ecision rees

explained extract action ordering h context policy
shown use ordering reduce node evaluations depth first h context policy allows direct application h context policies
h context policy lookahead bfs combines policy domainindependent heuristic within bfs addition included modified
version roller bfs ha considers helpful successors order
reduce number evaluations domains helpful actions good
experimental improved scalability baseline heuristic
planners lama winner ipc variety ipc domains effect
evident domains learned dck presents good quality e g blocksworld parking
domains direct application learned dck saves large amounts node evaluations
achieving impressive scalability performance moreover learned dck combination
domain independent heuristic bfs achieves good quality solutions
quality learned dck poor direct application policy fails
solve many mainly largest ones difficult solve without reasonable
guide unfortunately current mechanism quantifying quality learned dck
evaluating set test therefore good compromise solution combining
learned dck domain independent heuristics
domains dck learned roller presents poor quality helpful context
able represent concepts necessary order discriminate good bad
actions frequently arises arguments good action correspond
goals static predicates plan study refinements definition
helpful context achieve good dck domains one possible direction extending
helpful context subgoal information landmarks hoffmann et al
relaxed plan moreover use decision trees introduces important bias learning step
tree learning insert query tree produces significant
information gain however domains information gain obtained
conjunction two queries finally currently providing learner fixed
distribution training examples near future plan explore learner generate
convenient distribution training examples according target task proposed
fuentetaja borrajo

acknowledgments
work partially supported spanish miciin project tin c
regional cam uc project ccg uc tic



fid e la rosa j imenez f uentetaja b orrajo

appendix dck usefulness

domains
blocksworld
depots
gold miner
matching bw
parking
rovers
satellite
storage
thoughtful
tpp
total

deph first
roller gr ha df ha




























best first
roller bfs lh bfs
bfs































helpful best first
roller bfs ha lh bfs ha bfs ha


































table solved dck usefulness evaluation

domains
blocksworld
depots
gold miner
matching bw
parking
rovers
satellite
storage
thoughtful
tpp
total

deph first
roller
gr ha df ha






























best first
roller bfs
lh bfs






















bfs












helpful best first
roller bfs ha lh bfs ha
bfs ha

































table quality scores dck usefulness evaluation

references
bacchus f kabanza f temporal logics express search control knowledge
artificial intelligence
biba j saveant p schoenauer vidal v evolutionary metaheuristic
state decomposition domain independent satisficing proceedings th
international conference automated scheduling icaps toronto
canada aaai press
blockeel h de raedt l top induction first order logical decision trees
artificial intelligence


fis caling h euristic p lanning r elational ecision rees

bonet b loerincs g geffner h robust fast action selection mechanism
proceedings american association advancement artificial
intelligence conference aaai pp mit press
botea enzenberger muller schaeffer j macro improving ai
automatically learned macro operators journal artificial intelligence

coles smith marvin heuristic search planner online macro action learning journal artificial intelligence
de la rosa jimenez borrajo learning relational decision trees guiding heuristic international conference automated scheduling
icaps
de la rosa jimenez garca duran r fernandez f garca olaya borrajo
three relational learning approaches lookahead heuristic working
notes icaps workshop learning pp
de raedt l logical relational learning springer berlin heidelberg
doherty p kvarnstrom j talplanner temporal logic planner ai magazine

dzeroski de raedt l blockeel h relational reinforcement learning international workshop ilp pp
emde w wettschereck relational instance learning proceedings
th conference machine learning pp
florez j e garca j torralba linares c garca olaya borrajo timiplan application solve multimodal transportation proceedings spark
scheduling applications workshop icaps
fuentetaja r borrajo improving control knowledge acquisition
active learning ecml berlin germany vol pp
gerevini saetti vallati automatically configurable portfolio planner
macro actions pbp proceedings th international conference automated
scheduling pp thessaloniki greece
hoffmann j nebel b system fast plan generation heuristic
search journal artificial intelligence
hoffmann j porteous j sebastia l ordered landmarks journal
artificial intelligence
khardon r learning action strategies domains artificial intelligence



fid e la rosa j imenez f uentetaja b orrajo

leckie c zukerman inductive learning search control rules artificial
intelligence
martin geffner h learning generalized policies concept languages international conference artificial intelligence systems aips
martin geffner h learning generalized policies examples
concept languages appl intell
mcallester givan r taxonomic syntax first order inference journal acm

mcdermott heuristic estimator means ends analysis proceedings
rd conference artificial intelligence systems aips pp aaai
press
minton quantitative concerning utility explanation learning artif
intell
muggleton inverse entailment progol generation computing
muggleton de raedt l inductive logic programming theory methods journal
logic programming
nau au c ilghami kuter u murdock w wu yaman f shop
htn system journal artificial intelligence
newton h levine j fox long learning macro actions arbitrary
planners domains proceedings th international conference automated
scheduling icaps
quinlan j induction decision trees machine learning
richter westphal lama planner guiding cost anytime
landmarks journal artificial intelligence
roger g helmert merrier combining heuristic estimators satisficing proceedings th international conference automated
scheduling icaps pp
veloso carbonell j perez borrajo fink e blythe j integrating
learning prodigy architecture jetai
vidal v lookahead strategy heuristic search proceedings th
international conference automated scheduling icaps whistler
british columbia canada pp
xu fern yoon w discriminative learning beam search heuristics
ijcai proceedings th ijcai pp


fis caling h euristic p lanning r elational ecision rees

xu fern yoon learning linear ranking functions beam search
application journal machine learning
xu fern yoon iterative learning weighted rule sets greedy search
proceedings th international conference automated scheduling
icaps toronto canada
yoon fern givan r learning heuristic functions relaxed plans proceedings th international conference automated scheduling icaps
yoon fern givan r learned policies heuristic search
proceedings th ijcai
yoon fern givan r learning control knowledge forward search
j mach learn res
zimmerman kambhampati learning assisted automated looking back
taking stock going forward ai magazine




