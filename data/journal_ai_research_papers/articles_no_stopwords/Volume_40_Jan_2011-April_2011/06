journal articial intelligence

submitted published

multiagent learning large anonymous games
ian kash

kash seas harvard edu

center computation society
harvard university

eric j friedman

ejf cornell edu

department operations
information engineering
cornell university

joseph halpern

halpern cs cornell edu

department computer science
cornell university

abstract
large systems important agents learn act eectively sophisticated
multi agent learning generally scale alternative nd
restricted classes games simple ecient converge shown
stage learning eciently converges nash equilibria large anonymous games bestreply dynamics converge two features identied improve convergence first
rather making learning dicult agents actually benecial many
settings second providing agents statistical information behavior others
signicantly reduce number observations needed

introduction
designers distributed systems frequently unable determine agent
system behave optimal behavior depends users preferences
actions others natural agents use learning
many multiagent learning proposed including simple strategy update
procedures ctitious play fudenberg levine multiagent versions qlearning watkins dayan regret cesa bianchi lugosi
goal work help designers distributed systems understand
learning practical discuss section existing generally unsuitable
large distributed systems distributed system agent limited view
actions agents require knowing example strategy chosen
every agent cannot implemented furthermore size distributed systems requires
fast convergence users may use system short periods time conditions
system change time practical system thousands millions
users needs convergence rate sublinear number agents existing
tend provide performance guarantees polynomial even exponential
finally large number agents system guarantees noise agents
make mistakes behave unexpectedly even agent changes strategy
still noise agent payos example gossip protocol match dierent

c

ai access foundation rights reserved

fikash friedman halpern

agents round round congestion underlying network may eect message delays
agents learning needs robust noise
nding satises requirements arbitrary games may
dicult distributed systems characteristics make easier first
involve large number agents agents may seem make learning
harderafter possible interactions however advantage
outcome action typically depends weakly agents
makes outcomes robust noise large number agents make less useful
agent try inuence others becomes better policy try learn optimal
response contrast small number agents agent attempt guide learning
agents outcome benecial
second distributed systems often anonymous matter something rather many agents example congestion link
experience single agent depend sending packets
many sent anonymous games long history economics literature
e g blonski subject recent interest computer science
literature daskalakis papadimitriou gradwohl reingold
finally perhaps importantly distributed system system designer
controls game agents playing gives us somewhat dierent perspective
work takes game given need solve hard
nding ecient games instead nd work
eciently interesting classes games us interesting means type
games system designer might wish agents play games well behaved
since would strange design system agents decisions inuence
agents pathological ways
section stage learning friedman shenker robust implementable minimal information converges eciently interesting class
games agents divide rounds game series stages
stage agent uses xed strategy except occasionally explores end
stage agent chooses strategy next stage whatever strategy
highest average reward current stage prove appropriate conditions
large system stage learners follow approximate best reply dynamics despite errors
exploration
games best reply dynamics converge theorem guarantees learners
play approximate nash equilibrium contrast previous convergence guarantee scales poorly number agents theorem guarantees convergence nite amount time innite number agents assumption
best reply dynamics converge strong one many interesting games converge best reply dynamics including dominance solvable games games monotone best
replies max solvable games nisan schapira zohar class max solvable
games particular includes many important games transmission control protocol
tcp congestion control interdomain routing border gateway protocol bgp
cost sharing games stable roommates games nisan schapira valiant zohar
consider best reply dynamics agents update strategy time
best reply dynamics assume agents update strategy one time



fimultiagent learning large anonymous games

marden arslan shamma observed convergence best reply dynamics often property games humans design although observation
slightly dierent notion best reply dynamics moreover convergence best reply dynamics weaker assumption common assumption made mechanism design
literature games interest dominant strategies agent strategy
optimal matter agents
simulation presented section convergence fast practice
system thousands agents converge thousand rounds furthermore
identify two factors determine rate quality convergence one number
agents agents makes noise system consistent agents
learn fewer observations giving agents statistical information
behavior agents speed convergence order magnitude indeed
even noisy statistical information agent behavior relatively easy
obtain disseminate signicantly improve performance
theoretical limited stage learning provide intuition
well behaved learning converge simulations
include two learning bear furthermore demonstrate
applicability stage learning realistic settings simulate learning
scrip system kash friedman halpern demonstrate stage
learning robust factors churn agents joining leaving system
asynchrony agents stages dierent lengths however stage learning robust
changes include simulations games small number agents games
anonymous games continuous games violate
assumptions theoretical simulations games stage
learning converges slowly
finally participants system necessarily behave expected learning
useful real system needs robust behavior section
continuity utility functions key property makes stage learning robust
byzantine behavior small fraction agents

related work
one learning play games generalize reinforcement learning
q learning watkins dayan one nice feature
handle games state important distributed systems q learning
agent associates value state action pair chooses action state
updates value reward received best value
achieve resulting state max generalizing multiple agents
become vectors state action every agent max replaced
prediction behavior agents dierent use dierent predictions
example nash q uses nash equilibrium calculation hu wellman see
work shoham powers grenager survey
unfortunately converge slowly large distributed system
needs experience possible action prole many times guarantee convergence agents strategies naive convergence time even


fikash friedman halpern

better representation anonymous games convergence time still typically
fundamental assumes information agent unlikely order know value update
agent must learn action chosen every agent practice agent learn
something actions agents directly interacts unlikely
gain much information actions agents
another regret learning agents choose strategy round
guarantees regret choices low hart mas colell
present learning procedure converges correlated equilibrium given knowledge payos every action would round provide variant requires information agents actual
payos hart mas colell however guarantee convergence within
correlated equilibrium requires log still slow large systems furthermore convergence guarantee distribution play converges equilibrium
strategies individual learners converge many regret
exist blum mansour section use exp auer cesabianchi freund schapire achieve even better convergence restricted
settings example blum even dar ligett showed routing games
continuum regret learners approximate nash equilibrium nite amount
time jafari greenwald gondek ercal showed regret learners converge
nash equilibrium dominance solvable constant sum general sum games
foster young use stage learning procedure converges nash equilibrium two player games germano lugosi showed converges generic
player games games best replies unique young uses similar without explicit stages converges generic player games rather
selecting best replies agents choose actions randomly
equilibrium unfortunately involve searching whole strategy space
convergence time exponential another uses stages provide
stable learning environment esrl coordinated exploration verbeeck
nowe parent tuyls
marden arslan shamma b marden young arslan shamma
use experimentation best replies without explicit stages
converges weakly acyclic games best reply dynamics converge agents move
one time rather moving assume convergence
existence sequence exploration moves lead equilibrium
agents explore probability analysis gives convergence time
furthermore guarantee requires suciently small agents essentially explore
one time needs
adlakha johari weintraub goldsmith independently given conditions
existence oblivious equilibrium mean eld equilibrium stochastic
games model require game large anonymous continuous oblivious equilibrium player reacts average states
correlated equilibrium general solution concept nash equilibrium see osborne rubenstein every nash equilibrium correlated equilibrium may correlated equilibria
nash equilibria



fimultiagent learning large anonymous games

strategies players rather exact values however model assumes
players payo depends state players actions adlakha
johari consider stochastic games strategic complementarities
mean eld equilibria exist best reply dynamics converge myopic learning dynamics
require knowledge aggregate states players nd
long history work examining simple learning procedures ctitious
play fudenberg levine agent makes best response assuming
players strategy characterized empirical frequency observed
moves contrast convergence guarantees general games fail converge many games classes games converge
tend rapidly however work area assumes actions
agents observed agents agents know payo matrix payos deterministic recent tradition win learn fast principle
limited convergence guarantees often performs well practice bowling
veloso hopkins showed many procedures converge symmetric
games innite number learners although provide guarantees
rate convergence
body empirical work convergence learning
multiagent settings q learning empirical success pricing games tesauro
kephart player cooperative games claus boutilier grid world
games bowling greenwald al showed number
including stage learning converge variety simple games marden et al found
converged must faster congestion game theoretical analysis
would suggest theorem suggests explanation empirical observations bestreply dynamics converge games theorem applies directly
stage learning provides intuition learn quickly enough
change behavior slowly enough rapidly converge nash equilibrium practice

theoretical
section present theoretical analysis model provide support
simulations following section
large anonymous games
interested anonymous games countably many agents assuming
countably many agents simplies proofs straightforward extend
games large nite number agents model adapted blonski formally large anonymous game characterized tuple pr
countably innite set agents
nite set actions agent choose simplicity assume
agent choose set actions
set probability distributions two useful interpretations
rst set mixed actions abuse notation denote


fikash friedman halpern

mixed action probability round agent chooses one
mixed actions second interpretation fraction
agents choosing action important notion anonymity
says agents utility depend many agents choose
action rather chooses
set mixed action proles e action
agent chooses given mixed action every agent want know fraction
agents end choosing action let denote probability
agent plays according express fraction
agents choose action lim limit exists
limit exists actions let give value limit
proles use determined simple random process
proles strong law large numbers slln guarantees probability
well dened thus typically well dened similar limits us
talk fraction agents something
nite set payos agents receive
pr denotes distribution payos
agent performs action agents follow action prole use probability
distribution payos rather payo model fact agent payos may
change even agent changes strategy expected utility agent
performs

mixed action agents follow action distribution

pr denition pr terms rather
ensures game anonymous require pr thus lipschitz
continuous deniteness use l norm notion distance
specifying continuity l distance two vectors sum absolute
values dierences component note formulation assumes
agents share common utility function assumption relaxed allow
agents nite number types appendix
example large anonymous game one round agent plays
two player game opponent chosen random random matching games
common literature e g hopkins meaning opponent chosen
random made formal boylan game set actions
two player game set payos game every agent chooses
action distribution opponent actions characterized let
denote payo agent plays agent plays utility
mixed action given distribution





lipschitz continuity imposes additional constraint constant pr
pr intuitively ensures distribution outcomes
change fast standard assumption easily seen hold games
typically considered literature



fimultiagent learning large anonymous games

best reply dynamics
given game action distribution natural goal agent play
action maximizes expected utility respect argmax call
action best reply practical amount time agent may diculty
determining two actions close expected utilities better allow
agents choose actions close best replies best reply
best reply may one best reply
denote set best replies abr
single agent looking best reply every agent trying nd one
time agents start action distribution nd
best reply action distribution assume agents
choose initial strategy uniformly random apply distribution
used determine initial strategy say sequence bestreply sequence support subset abr gives positive
probability approximate best replies best reply sequence converges
exists note particularly
strong notion convergence require converge nite time
merely limit game may innitely many best reply sequences say
approximate best reply dynamics converge exists every bestreply sequence converges limit distribution determines mixed strategy
nash equilibrium e support subset
main shows learners successfully learn large anonymous games
approximate best reply dynamics converge number stages needed converge
determined number best replies needed sequence converges possible design games long best reply sequences practice games
short sequences one condition guarantees degenerate action
distributions e distributions assign probability unique
best replies case best replies equilibrium reached
assumed agents utility function furthermore
games distinction best replies best replies irrelevant suciently small best reply best reply hard property
degenerate strategies unique best replies generic holds almost every game
stage learners
agent wants nd best reply may know set payos mapping
actions distributions payos pr action distribution indeed
may changing time use type learning learn
divide play game sequence stages stage
agent almost plays xed action explores actions
end stage chooses next stage learned
important feature agents maintain actions entire stage
stage provides stable environment agents learn simplify
specify way exploring learning within stage originally described
friedman shenker generalize reasonable learning


fikash friedman halpern

used learn within stage discuss reasonable section
section given suitable parameter stage agents
learned best reply environment stage
given game round agent needs select mixed action
agents use strategies denote
thus agent almost plays probability explores
strategies uniformly random thus far specied information
agent use choose dierent games may provide dierent information
require agent know previous actions previous payos
precisely knows action determined
payos determined pr action distribution
round note assume agent knows information
express average value action previous rounds length
stage let
set recent rounds
played average value
otherwise need value times
multiples convenience dene arbitrary times
say agent stage learner chooses actions follows
chosen random nonzero multiple
argmax otherwise thus within stage mixed
action xed end stage updates use action highest average
value previous stage
evolution game played stage learners deterministic agent chooses
random sequence observes random however
countably innite set agents use slln make statements overall
behavior game let run game consists sequence triples
slln guarantees probability fraction agents choose
strategy similarly fraction agents chose receive
payo pr probability
make notion stage precise refer sequence tuples
stage run stage
stationary action distribution denote abr
say agent learned best reply stage run following lemma shows suciently small agents learn best reply
lemma large anonymous games action proles approximations
probabilities error exists
agents stage learners least fraction agents learn best reply
stage
proof sketch average agent strategy plays action times
stage plays actions times large realized number
times played close expectation value high probability thus
suciently large average payo action exponentially close
use exponent arbitrary require expected number times strategy
explored increases decreases



fimultiagent learning large anonymous games

true expected value via standard hoeding bound sums random variables
thus learner correctly identify action approximately highest expected
payo probability least slln least fraction agents
learn best reply detailed version proof general setting found
work friedman shenker
convergence theorem
thus far dened large anonymous games approximate best reply dynamics
converge agents game stage learners sequence
action distributions run game best reply sequence close
action used agents time action used
approximate best reply sequence
order prove need dene close denition error rate
exploration rate introduces noise intuitively distribution close
changing strategies fraction agents agents explore
fraction time go action prole corresponding action distribution
one corresponding distribution note denition symmetric
denition identies pure action agent leads
allows fraction agents use action incorporates fact
agent exploring strategy agent usually plays explores
probability
denition action distribution close exist



allows fraction agents play dierent strategy


use nal requirement ensures two distributions close
close example asymmetry
denition close reverse true closeness
useful distance measure analysis unnatural notion distance specifying
continuity used l norm following simple lemma shows
distinction unimportant suciently close close according
l measure well
lemma close

proof since close exist denition consider
distributions view three distributions vectors
calculate l distances denition
fraction agents explore thus triangle inequality l distance



fikash friedman halpern

assumed approximate best reply sequences converge
run game agents actually learning approximate best replies
following lemma shows distinction matter suciently close
lemma exists close
abr
abr
proof let maximum lipschitz constants one constant
close

lemma
let
abr argmax combining
gives thus
abr

lemmas give requirements statement theorem call
acceptable satisfy requirements lemmas best reply
sequences converge
theorem let large anonymous game approximate best reply dynamics
converge let acceptable agents stage learners
runs exists best reply sequence stage least
fraction learn best reply probability
proof uniform distribution close assume
close lemma least fraction learn best reply
lemma best reply thus close
theorem guarantees nite number stages agents close
approximate nash equilibrium prole specically close nash
equilibrium prole note means actually nash equilibrium
larger depends lipschitz constant
three requirements practical learning require minimal
information converge quickly large system robust noise stage learning
requires agent know payos rst condition satised theorem shows satises two requirements convergence guaranteed
nite number stages number stages depends game section
argued many cases quite small finally robustness comes tolerating
fraction errors proofs assumed errors due learning
analysis noise sources churn agents
making errors discuss issue section

simulation
section discuss experimental demonstrate practicality learning
large anonymous games theorem guarantees convergence suciently small
exploration probability decreasing increases length stage
rst set experiments shows necessary values quite reasonable
practice theorem applies stage learning analysis provides intuition


fimultiagent learning large anonymous games

reasonable changes slowly enough learners chance
learn best replies converge well demonstrate implemented two
learning quickly converged
theoretical make two signicant predictions factors inuence
rate convergence lemma tells us length stage determined
number times strategy needs explored get accurate estimate
value thus amount information provided observation large eect
rate convergence example random matching game agents payo
provides information strategy one agent hand
receives expected payo matched single observation provides information
entire distribution strategies latter case agent learn many
fewer observations related prediction agents lead faster
convergence particularly games payos determined average behavior
agents variance payos due exploration mistakes decreases
number agents increases experimental illustrate phenomena
game used rst set experiments many simple games used test
learning symmetric hopkins showed many learning
well behaved symmetric games large populations demonstrate
main due something symmetry tested stage learning
asymmetric game observed convergence even small population
explore applicability stage learning practical setting violates
number assumptions theorem implemented variant stage learning
game scrip system kash et al demonstrate applicability
real systems included experiments churn agents leaving
replaced agents agents learning dierent rates
finally give examples games large anonymous continuous provide simulations showing stage learners learn far slowly
games satisfy hypotheses theorem learn play
equilibrium examples demonstrate assumptions essential

contribution game
rst set experiments agents play contribution game called diamondtype search model work milgrom roberts contribution game
two agents choose strategies indicating much eort contribute
collective enterprise value agent depends much contributes well
much agent contributes contributes contribution
agent utility round game agent
paired random agent play contribution game game best reply
dynamics converge within stages starting distribution
implemented three learning run game implementation
stage learners described section rather taking length
stage set suciently long stages value rather
decreasing stages long enough second



fikash friedman halpern


agents
agents
agents

distance equilibrium























time




x

figure stage learners random matching

agents
agents
agents

distance equilibrium




















time






x

figure hart mas colell random matching
hart mas colell improvements suggested greenwald friedman
shenker takes parameters exploration probability
used nal learning exp auer et al
set exploration probability requires payos
normalized lie since choices strategies lead large negative
payos naive normalization leads almost every payo close better
performance normalized payos payos fell range
outside set appropriate
three shown figures curve
shows distance equilibrium function number rounds population
agents given size given learning averaged
ten runs since payos nearby strategies close want notion distance
take account agents playing closer equilibrium thanthose playing
zero therefore consider expected distance equilibrium

determine counted number times action taken length


fimultiagent learning large anonymous games


agents
agents
agents



distance equilibrium




















time




x

figure exp random matching

agents
agents
agents

distance equilibrium




















time






x

figure stage learning average payos
stage practice distance never zero due mistakes exploration
ease presentation graph shows populations size similar
obtained populations agents
stage learning increasing population size dramatic impact two
agents mistakes best replies mistakes cause behavior quite
chaotic ten agents agents successfully learn although mistakes suboptimal
strategies quite frequent one hundred agents agents converge quickly
near equilibrium strategies signicant mistakes rare
despite lack theoretical guarantees two converge although
somewhat slowly long run performance exp similar stage learning
hart mas colells asymptotic convergence guarantees tends
converge slowly practice tuned tight convergence get converge
reasonable amount time tuned parameters accept somewhat weaker convergence
although particular game shown dierence convergence dramatic



fikash friedman halpern


agents
agents
agents



distance equilibrium

















time






x

figure stage learners congestion game
convergence stage learning random matching game takes approximately
rounds slow many applications system design requires type
matching makes learning problematic however figure suggest
learning could done much faster system designer could supply agents
information suggests collecting statistical information behavior
agents may critical feature ensuring fast convergence model scenario
consider related game rather matched random opponent
agents contribute project reward average contribution
agents stage learning game shown figure
much information available agents observation able cut
length stage factor number stages needed reach equilibrium
remained essentially convergence tighter well mistakes rare
almost distance equilibrium due exploration
congestion game
dierent game tested performance stage learners congestion game
game situation two agents share network link gain utility
proportional transmission rate link penalized resulting
congestion experience game asymmetric two dierent types
agents place dierent values transmission rate game described detail
greenwald friedman shenker showed regret learners able
nd equilibrium game extension theoretical games
multiple types presenting appendix
figure shows stage learners able learn quickly game
stages length even though randomly matched player
type dierent types agents dierent equilibrium strategies
distance measure use treat observed distribution strategies
equilibrium distribution vectors compute l distance



fimultiagent learning large anonymous games



fraction playing equilibrium strategy









capacity
capacity
capacity










number stages





figure stage learners tcp game
tcp game
previous example considered random matching game two agents
dierent types share link consider game large number agents share
several links game variant congestion control game studied nisan et al

three types agents network agent chooses integer rate
transmit links network maximum average rate
agents transmit exceeded share capacity evenly among agents
agents utility overall transmission rate network minus penalty
trac dropped due congestion agent attempts transmit rate
actual rate penalty
agents share link average capacity one third agents
constrained sharing link average capacity another third share link
average capacity game unique equilibrium agents rst
third choose rate agents second third choose rate agents
nal third choose rate overall average rate game
best reply dynamics converge stages uniform starting distribution
figure shows learners type
averaged ten runs agents constrained average capacity two quickly learn
equilibrium strategy followed average capacity four agents constrained
average capacity learn equilibrium strategy sawtooth pattern
small fraction alternately plays rather exploration
actually optimal small number agents play noticeable fraction
uniquely optimal demonstrates strictly speaking game
satisfy continuity requirement equilibrium demand bandwidth exactly
equal supply thus small changes demand agents due exploration
large eect amount actually demanded thus payos
penalty used work nisan et al avoids tie breaking issues
consider



fikash friedman halpern



fraction playing equilibrium strategy








type
type
type










number stages





figure stage learners random tcp games
strategies however structure game play still tends
remain close equilibrium terms rates agents choose
addition specic parameters mentioned ran simulations
three capacities randomly chosen integer figure
shows average similar three types agents share common
constraint type type additional constraint unsurprisingly since
two types symmetric almost identical three types demonstrate
sawtooth behavior type runs due examples figure
fewer constraints gives agents exibility primarily comes runs
type type constraints larger overall constraint e
overall constraint matters thus three types ability benet resources
demanded agents explore
scrip system game
motivation work help designers distributed systems understand
learning practical order demonstrate stage learning could applied
setting tested variant stage learners model scrip system used
kash et al model agents pay agents provide service
turn provide service earn money pay future service agents may place
dierent values receiving service incur dierent costs provide service discount
future utility dierent rates dierent availabilities provide service
used single type agent parameters average
amount money per agent stages rounds per agent one agent
makes request round
model large anonymous game whether agent provide
service depends much money currently thus stage learning specied
work take account current state stochastic game
despite still implement variant stage learning x strategy
stage end stage use designed game determine


fimultiagent learning large anonymous games


agents
agents



distance equilibrium


























time




x

figure stage learners scrip system

agents
agents



distance equilibrium



















time










x

figure scrip system churn
strategy best reply agent observed works
estimating agents probabilities making request chosen volunteer
round uses probabilities compute optimal policy figure
shows quite eective distance measure used directly measuring
distance agents chosen threshold strategy equilibrium strategy since
unlike previous games impossible directly infer agents strategy round
solely decision whether volunteer note number rounds
normalized number agents figure later gures stages actually
lasted ten times long agents
real systems static population learning agents demonstrate
robustness stage learning churn replaced ten percent agents agents
randomly chosen initial strategies end period figure shows
essentially eect convergence



fikash friedman halpern


agents
agents



distance equilibrium



















time










x

figure scrip system dierent stage lengths
finally real system often unreasonable expect agents able update
strategies time figure shows half agents use stages
rounds per agent rather signicant eect convergence
learning counterexamples
rst glance theorem may seem trivial game best reply dynamics
guaranteed converge seems obvious agents attempt nd best replies
successfully nd reach equilibrium however section
fact alone sucient particular three key features games
studythat large anonymous continuousare required theorem
hold
first game small number agents mistake made single
agent could quite important point learning essentially start
converted probability none
nite number agents make mistake given stage expected time reach
equilibrium following signicantly longer best reply dynamics
would suggest following example game number best replies
needed reach equilibrium approximately number strategies experimental
number stages needed stage learners nd equilibrium
signicantly longer conjecture fact learning time exponentially longer
contrast theorem guarantees games satisfying requirements number
stages needed equal number best replies
consider game three agents set actions
utility functions agents symmetric rst agents utility function given
following table
general expect small variations stage lengths aect convergence however large
enough dierences non nash convergence see work greenwald et al
simulations analysis



fimultiagent learning large anonymous games

actions











payo











conditions







agents learning best replies viewed climbing ladder best reply
agents reach nash equilibrium
however mistake made agents essentially start see works
suppose agents next stage one makes mistake
select leads best reply sequence
point agents begin climbing somewhat complicated structure payos
near ensures agents begin climbing arbitrary patterns mistakes
typical run stages best replies needed reach equilibrium one stage
initial randomly chosen strategies one stage three agents switch strategy
stages climbing exact number stages vary two agents choose
initial strategy never greater
following table gives number rounds averaged ten runs stage learners
game rst reach equilibrium number strategies varies length
stage exploration probability
rounds reach
















stage learners typically require stages occasional error raising
average slightly majority runs feature least one agent
making mistake number stages required closer
many opportunities agents make mistake number stages required
average range thus learning slower best reply dynamics
disparity grows number strategies increases
small modication example shows arise games
anonymous non anonymous game large number agents payos
depend entirely actions small number agents example split
set agents three disjoint sets choose agents


fikash friedman halpern





fraction playing




agents
agents
agents












number stages





figure stage learners discontinuous game
agent chooses action payos agents
determined everyone gets payo everyone
gets payo everyone gets payo convergence
equilibrium signicantly slower best reply dynamics
finally consider following game large anonymous satisfy
continuity requirement set actions agent receives
payo agent chooses action payo pr
chooses action payo every agent chooses action every
agent chooses action otherwise pr pr
pr
game suppose approximate best reply dynamics start action
chosen half agents coordinated unique approximate best
reply agents action one best reply action distribution
since agents coordinated another round approximate best replies leads
equilibrium agents stage learners rst stage learn
approximate best reply exploration change action prole
case adopt mixed action playing probability
probability thus even agents make mistake action distribution next
stage least fraction playing action thus unique approximate best
reply action stage learners stuck never reach equilibrium

figure shows fraction times strategy played stage averaged
ten runs agents ten agents
initial mistakes made stage strategy played
time runs corresponds fraction time expect see simply
exploration agents see another sawtooth pattern agents
stuck playing alternating rounds small fraction plays happens
rounds playing small fraction lucky explore
agents explore adopt strategy next stage however
following stage agents return playing oscillating


fimultiagent learning large anonymous games

behavior observed learning contexts example among competing myopic
pricebots kephart hanson greenwald agents lucky agents
quite rare essentially agents constantly stuck playing

learning byzantine agents
practice learning need robust presence agents
following seen stage learning large anonymous games
robust agents learn instead follow xed strategy stage
analysis agents simply treated agents made mistake
previous stage however agent need follow xed strategy agent attempting
interfere learning malicious reasons personal gain likely adapt
strategy time however section stage learning handle
manipulation large anonymous games
gradwohl reingold examined several classes games introduced
notion stable equilibrium one change strategy small fraction
agents small eect payo agents denition
games nite number agents easily adapted notion large
anonymous game take notion step characterize game rather
equilibrium stable every strategy stable
denition large anonymous game stable

one class games consider continuous games continuity essentially
version lipschitz continuity nite games easy large anonymous
games stable amount manipulation tolerated depends
lipschitz constants agents utility functions
lemma large anonymous games exists constant
stable
proof lipschitz continuous constant
take max










gradwohl reingold stable equilibria several nice properties
small fraction agents deviate payos agent decrease much
relative equilibrium additionally following strategies still approximate


fikash friedman halpern

equilibrium despite deviation finally means strategies still constitute
approximate equilibrium even asynchronous play causes strategies fraction
agents revealed others
game stable learning robust actions small
fraction byzantine agents following lemma adapts lemma
stage agents learn approximate best replies despite actions byzantine agents
thus agents successfully reach equilibrium shown theorem
state lemma need dene actions byzantine agent

byzantine agents stage would stationary strategy

corresponding fraction agents choosing action fraction byzantine agents
change actions arbitrarily round eect
actions agents thus byzantine agents cause observed fraction
agents choosing strategy round refer
sequence condition holds consistent
sequence say agents learn best reply stage mean
actions players
strategy learn approximate best reply
would used byzantine players actual action prole
includes strategies used byzantine players
lemma large anonymous games action distributions approximations
probabilities error fractions agents exists
consistent sequences agents
stage learners least fraction agents learn best reply
stage despite fraction byzantine agents
proof consider agent round stage agents stage learners
action distribution would however byzantine agents changed
fix action lemma








means byzantine agents adjust agents expected estimate value
action let best reply action used stage learners
stage round stage






action best reply









thus regardless actions fraction byzantine agents agent expected
estimate value exceeds expected estimate value least
hoeding bounds suciently large estimates exponentially
close expectations probability least select best
action best reply slln means least fraction
agents learn best reply


fimultiagent learning large anonymous games

thus lemma shows stage learners learn despite agents learning incorrect values tolerate suciently small number agents behaving
arbitrarily

discussion
natural learning learn eciently interesting class games many issues merit exploration
learning
theorem assumes agents use simple rule learning within stage
average value payos received however certainly rules estimating value action used long rule guarantees
errors made arbitrarily rare given sucient time necessary restrict
agents stage learning stage learning guarantees stationary environment period
time strict behavior may needed practical approaches
exponentially discounting weight observations greenwald et al marden
et al win learn fast bowling veloso allow focus
learning recent observations provide stable environment agents
learn
update rules
addition dierent estimate values actions learner could
change way uses values update behavior example rather
basing strategy last stage could base entire history
stages use rule spirit ctitious play since games ctitious
play converges best reply dynamics could extend another
interesting class games long errors period accumulate time
another possibility update probabilistically use tolerance determine whether
update see e g foster young hart mas colell could allow
convergence games best reply dynamics oscillate decrease fraction agents
make mistakes system reaches equilibrium
model assumptions
model makes several unrealistic assumptions notably countably
many agents share utility function essentially holds
large nite number agents adding error terms particular since
small probability every agent makes mistake time
prove fraction agents make errors rounds
agents spend time playing equilibrium strategies
implicitly assumed set agents xed figure shows
could easily allow churn natural strategy newly arriving agents pick
random use next stage agents follows convergence
unaected treat agents part fraction made mistake


fikash friedman halpern

last stage furthermore tells us newly arriving agents catch quickly
single stage agents guaranteed learned best reply probability
least
finally assumed agents utility function
easily extended include nite number dierent types agents
utility function since slln applied type agent extension
discussed appendix believe hold even set possible
types innite happen example agents utility depends valuation
drawn interval however care needed dene best reply sequences
case
state
one common feature distributed systems addressed theoretical portion
work state saw scrip system section agents current state
often important factor choosing optimal action
principle could extend framework games state stage
agent chooses policy usually follow explores actions probability
agent could use policy one agent learn without
controlling sequence observations see kaelbling littman moore examples learn optimal policy use next stage one major
standard learn slowly purposes example q learning watkins dayan typically needs observe state action pair
hundreds times practice low exploration probability means expected
rounds needed explore pair even large ecient learning requires
specialized make better use structure however use specialized makes providing general guarantee convergence
dicult another even agent explores action
possible local states payo receives depend states agents
thus actions chose need property game guarantee
distribution states sense well behaved adlakha joharis work
mean eld equilibria gives one condition setting use publicly available
statistics might provide solution
mixed equilibria
another restriction agents learn pure strategies one way
address discretize mixed strategy space see e g foster young
one resulting strategies suciently close equilibrium strategy bestreply dynamics converge discretized strategies expect agents converge
near equilibrium distribution strategies empirical success
learn play rock scissors



fimultiagent learning large anonymous games

conclusion
learning distributed systems requires scalable thousands agents
implemented minimal information actions agents
general purpose multiagent learning fail one requirements
shown stage learning ecient solution large anonymous games
approximate best reply dynamics lead approximate pure strategy nash equilibria
many interesting classes games property frequently found designed
games contrast previous work time convergence guaranteed theorem
increase number agents system designers nd appropriate
game satisfying properties base systems condent
nodes eciently learn appropriate behavior
highlight two factors aid convergence first learners often improves performance learners noise introduced payos
exploration mistakes becomes consistent second information typically improves performance publicly available statistics observed behavior
agents allow agent learn eectively making fewer local observations
simulations demonstrate eects two factors well generalize
situations learning churn asynchrony byzantine behavior
acknowledgments
work done ik cornell university ef ik jh supported
part nsf grant itr jh supported part nsf grant iis
afosr grants fa fa ef supported
part nsf grant cdi

appendix multiple types
section extend denition large anonymous game settings
agents may dierent utility functions introduce notion type
agents utilities may depend type fraction type taking action
rely strong law large numbers restrict set types
nite formally large anonymous game types characterized tuple
pr dene remaining terms
nite set agent types
function mapping agent type
set probability distributions viewed
set mixed actions available agent describe fraction
agents type choosing action must use element
pr determines distribution payos
agent type performs action agents follow action prole
expected utility agent type performs mixed action agents



fikash friedman halpern



follow action distribution pr
require pr thus lipschitz continuous
revised denitions best reply nash equilibrium best reply sequence convergence approximate best reply dynamics close follow naturally
revised denitions lemma applies type agent separately shows small fraction type learn approximate best
reply stage lemma lemma hold given revised denitions
thus theorem combines still holds

references
adlakha johari r mean eld equilibrium dynamic games complementarities ieee conference decision control cdc
adlakha johari r weintraub g goldsmith mean eld analysis
large population stochastic games ieee conference decision control
cdc
auer p cesa bianchi n freund schapire r e nonstochastic multiarmed bandit siam journal computing
blonski equilibrium characterization large anonymous games tech rep
u mannheim
blum even dar e ligett k routing without regret convergence
nash equilibria regret minimizing routing games th acm
symp principles distributed computing podc pp
blum mansour learning regret minimization equilibria nisan
n roughgarden tardos e vazirani v eds algorithmic game theory
pp cambridge university press
bowling h convergence general sum multiagent reinforcement
learning th int conf machine learning icml pp
bowling h veloso rational convergent learning stochastic
games th int joint conference articial intelligence ijcai pp

boylan r laws large numbers dynamical systems randomly matched
indviduals journal economic theory
cesa bianchi n lugosi g prediction learning games cambridge university press
claus c boutilier c dynamics reinforcement learning cooperative
multiagent systems aaai workshop multiagent learning pp
daskalakis c papadimitriou c h computing equilibria anonymous games
th annual ieee symposium foundations computer science focs
pp



fimultiagent learning large anonymous games

foster p young p regret testing learning play nash equilibrium without
knowing opponent theoretical economics
friedman e j shenker learning implementation internet tech
rep cornell university
fudenberg levine theory learning games mit press
germano f lugosi g global nash convergence foster youngs regret
testing games economic behavior
gradwohl r reingold fault tolerance large games proc th acm
conference electronic commerce ec pp
greenwald friedman e j shenker learning networks contexts
experimental simulations games economic behavior

hart mas colell simple adaptive procedure leading correlated equilibrium econometrica
hart mas colell reinforecement learning procedure leading correlated
equilibrium debreu g neuefeind w trockel w eds economic essays
pp springer
hopkins e learning matching aggregation games economic behavior

hu j wellman p nash q learning general sum stochastic games journal
machine learning
jafari greenwald r gondek ercal g regret learning
ctitious play nash equilibrium proc eighteenth international conference
machine learning icml pp
kaelbling l p littman l moore p reinforcement learning survey
j artif intell res jair
kash friedman e j halpern j optimizing scrip systems eciency
crashes hoarders altruists eighth acm conference electronic commerce
ec pp
kephart j hanson j e greenwald r dynamic pricing software
agents computer networks
marden j r arslan g shamma j connections cooperative
control potential games proc european control conference ecc
marden j r arslan g shamma j b regret dynamics convergence
weakly acyclic games th int joint conf autonomous agents multiagent
systems aamas pp
marden j r young h p arslan g shamma j payo dynamics
multi player weakly acyclic games siam journal control optimization




fikash friedman halpern

milgrom p roberts j rationalizability learning equilibrium games
strategic complement arities econometrica
nisan n schapira valiant g zohar best response mechanisms
proc second syposium innovations computer science ics appear
nisan n schapira zohar asynchronous best reply dynamics
proc th international workshop internet netwrok economics wine pp

osborne rubenstein course game theory mit press
shoham powers r grenager multi agent reinforcement learning
critical survey tech rep stanford
tesauro g kephart j pricing agent economies multi agent qlearning autonomous agents multi agent systems
verbeeck k nowe parent j tuyls k exploring selsh reinforcement
learning repeated games stochastic rewards journal autonomous agents
multi agent systems
watkins c j dayan p technical note q learning machine learning

young h p learning trial error games economic behavior





