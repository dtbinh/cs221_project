Journal Artificial Intelligence Research 40 (2011) 1-24

Submitted 09/10; published 01/11

Non-Deterministic Policies
Markovian Decision Processes
Mahdi Milani Fard
Joelle Pineau

mmilan1@cs.mcgill.ca
jpineau@cs.mcgill.ca

Reasoning Learning Laboratory
School Computer Science, McGill University
Montreal, QC, Canada

Abstract
Markovian processes long used model stochastic environments. Reinforcement learning emerged framework solve sequential planning decision-making
problems environments. recent years, attempts made apply methods
reinforcement learning construct decision support systems action selection
Markovian environments. Although conventional methods reinforcement learning
proved useful problems concerning sequential decision-making, cannot applied current form decision support systems, medical domains,
suggest policies often highly prescriptive leave little room users
input. Without ability provide flexible guidelines, unlikely methods
gain ground users systems.
paper introduces new concept non-deterministic policies allow flexibility users decision-making process, constraining decisions remain near
optimal solutions. provide two algorithms compute non-deterministic policies
discrete domains. study output running time method set
synthetic real-world problems. experiment human subjects, show
humans assisted hints based non-deterministic policies outperform human-only
computer-only agents web navigation task.

1. Introduction
Planning decision-making well studied AI community. Intelligent
agents designed developed act in, interact with, variety environments. usually involves sensing environment, making decision using
intelligent inference mechanism, performing action environment (Russell & Norvig, 2003). Often times, process involves level learning, along
decision-making process, make agent efficient performing intended
goal.
Reinforcement Learning (RL) branch AI tries develop computational
approach solving problem learning interaction. RL process
learning dohow map situations actionsso maximize numerical
reward signal (Sutton & Barto, 1998). Many methods developed solve
RL problem different types environments different types agents. However,
work RL focused autonomous agents robots software
agents. RL controllers thus designed issue single action time-step
c
2011
AI Access Foundation. rights reserved.

fiMilani Fard & Pineau

executed acting agent. past years, methods developed
RL community started used sequential decision support systems (Murphy,
2005; Pineau, Bellemare, Rush, Ghizaru, & Murphy, 2007; Thapa, Jung, & Wang, 2005;
Hauskrecht & Fraser, 2000). many systems, human makes final
decision. Usability acceptance issues thus become important cases.
RL methods therefore require level adaptation used decision support
systems. adaptations main contribution paper.
Medical domains among cases RL needs adaptation. Although
RL framework correctly models sequential decision-making complex medical scenarios, including long-term treatment design, standard RL methods cannot applied
medical settings current form lack flexibility suggestions.
requirements are, course, specific medical domains and, instance, might
needed aircraft controller provides suggestions pilot.
important difference decision support system classical RL problem
stems fact decision support system, acting agent often human
being, course his/her decision process. Therefore, assumption
controller send one clear commanding signal acting agent
appropriate. accurate assume aspect decision-making process
influenced user system.
view decision process particularly relevant two different situations.
First, many practical cases, exact model system. Instead,
may noisy model built finite number interactions environment.
leads type uncertainty usually referred extrinsic uncertainty.
RL algorithms ignore uncertainty assume model perfect. However
look closely, performance optimal action based imperfect model might
statistically different next best action. Bayesian approaches looked
problem providing confidence measure agents performance (Mannor, Simester,
Sun, & Tsitsiklis, 2007). cases acting agent human being, use
confidence measures provide user complete set actions, might
optimal enough evidence differentiate. user
use his/her expertise make final decision. methods guarantee
suggestions provided system statistically meaningful plausible.
hand, even complete knowledge system
identify optimal action, might still actions roughly equal
performance. point, decision near-optimal options could left
acting agentnamely human using decision support system.
could many advantages, ranging better user experience, increased robustness
flexibly. Among near-optimal solutions, user select based domain
knowledge, preferences, captured system. instance,
medical diagnosis system suggests treatments, providing physician several
options might useful final decision could made based knowledge
patients medical status, preferences regarding side effects.
Throughout paper address latter issue combination theoretical
empirical investigations. introduce new concept non-deterministic policies
capture decision-making process intended decision support systems. policies
2

fiNon-Deterministic Policies Markovian Decision Processes

involve suggesting set actions, non-deterministic choice made
user. apply formulation solve problem finding near-optimal policies
provide flexible suggestions user.
particular, investigate suggest several actions acting agent,
providing performance guarantees worst-case analysis. Section 2 introduces
necessary technical background material. Section 3 defines concept non-deterministic
policies related concepts. Section 4 addresses problem providing choice
acting agent keeping near-optimality guarantees performance worst-case
scenario. propose two algorithms solve problems provide approximation
techniques speed computation larger domains.
Methods introduced paper general enough apply decision support system observable Markovian environment. empirical investigations focus
primarily sequential decision-making problems clinical domains, system
provide suggestions best treatment options patients. decisions
provided sequence treatment phases. systems specifically interesting
often times, different treatment options seem provide slightly different results. Therefore, providing physician several suggestions would beneficial
improving usability system performance final decision.

2. Definitions Notations
section introduces main notions behind sequential decision-making mathematical formulations used RL.
2.1 Markov Decision Processes
Markov Decision Process (MDP) model system dynamics sequential decision
problems involves probabilistic uncertainty future states system (Bellman,
1957). MDPs used model interactions agent observable
Markovian environment. system assumed state given time.
agent observes state performs action accordingly. system makes
transition next state agent receives reward.
Formally, MDP defined 5-tuple (S, A, T, R, ):
States: set states. state usually captures complete configuration
system. state system known, future system
independent previous system transitions. means state
system sufficient statistic history system.
Actions: : 2A set actions allowed state set
actions. A(s) set actions agent choose from, interacting
system state s.
Transition Probabilities: : [0, 1] defines transition probabilities
system. function specifies likely end state, given
current state specific action performed agent. Transition probabilities
3

fiMilani Fard & Pineau

specified based Markovian assumption. is, state system
time denoted st action time , have:
Pr(st+1 |at , st , at1 , at1 , . . . , a0 , s0 ) = P r(st+1 |at , st ).

(1)

focus homogeneous processes system dynamics independent
time. Thus transition function stationary respect time:
def

(s, a, s0 ) = P r(st+1 = s0 |at = a, st = s).

(2)

Rewards: R : R [0, 1] probabilistic reward model. Depending
current state system action taken, agent receive reward
drawn model. focus homogeneous processes which, again,
reward distribution change time. reward time denoted
rt , have:
rt R(st , ).

(3)

Depending domain, reward could deterministic stochastic. use
general stochastic model throughout paper. mean distribution
denoted R(s, a).
Discount Factor: [0, 1) discount rate used calculate long-term
return.
agent starts initial state s0 S. time step t, action A(st )
taken agent. system makes transition st+1 (st , ) agent
receives immediate reward rt R(st , ).
goal agent maximize discounted sum rewards planning
horizon h (could infinite). usually referred return (denoted D):
D=

h
X

rt .

(4)

t=0

finite horizon case, sum taken horizon limit discount factor
set 1. However, infinite horizon case discount factor less
1 return finite value. return process depends
stochastic transitions rewards, well actions taken agent.
Often times transition structure MDP contains loop non-zero probability. transition graph modeled directed acyclic graph (DAG).
class MDPs interesting includes multi-step decision-making finite horizons,
found medical domains.
2.2 Policy Value Function
policy way defining agents action selection respect changes
environment. (probabilistic) policy MDP mapping state space
distribution action space:
: [0, 1].
4

(5)

fiNon-Deterministic Policies Markovian Decision Processes

deterministic policy policy defines single action per state. is, (s)
A(s). later introduce notion non-deterministic policies MDPs deal
sets actions.
agent interacts environment takes actions according policy.
value function policy defined expectation return given
agent acts according policy:
"
#
X
def
V (s) = E[D (s)] = E
rt |s0 = s, = (st ) .
(6)
t=0

Using linearity expectation, write expression recursive
form, known Bellman equation (Bellman, 1957):
"
#
X
X

0
0
V (s) =
(s, a) R(s, a) +
(s, a, )V (s ) .
(7)
s0

aA

value function used primary measure performance much
RL literature. are, however, ideas take risk variance
return account measure optimality (Heger, 1994; Sato & Kobayashi, 2000).
common criteria, though, assume agent trying find policy
maximizes value function. policy referred optimal policy.
define value function state-action pairs. usually referred
Q-function, Q-value, pair. definition:
"
#
X
def
(8)
Q (s, a) = E[D (s, a)] = E
rt |s0 = s, a0 = a, 1 : = (st ) .
t=0

is, Q-value expectation return, given agent starts state
s, takes action a, follows policy . Q-function satisfies Bellman
equation:
X
X
Q (s, a) = R(s, a) +
(s, a, s0 )
(s0 , a0 )Q (s0 , a0 ),
(9)
s0

a0

rewritten as:
Q (s, a) = R(s, a) +

X

(s, a, s0 )V (s0 ).

(10)

s0

Q-function often used compare optimality different actions given fixed
subsequent policy.
2.3 Planning Algorithms Optimality
optimal policy, denoted , defined policy maximizes value
function initial state:
= argmax V (s0 ).


5

(11)

fiMilani Fard & Pineau

shown (Bellman, 1957) MDP, exists optimal deterministic policy worse policy MDP. value optimal
policy V satisfies Bellman optimality equation:
"
#
X
V (s) = max R(s, a) +
(s, a, s0 )V (s0 ) .
(12)
aA

s0

deterministic optimal policy follows this:
"
#
X

0
0
(s) = argmax R(s, a) +
(s, a, )V (s ) .
aA

(13)

s0

Alternatively write equations Q-function:
X
Q (s, a) = R(s, a) +
(s, a, s0 )V (s0 ).

(14)

s0

Thus V (s) = maxa Q (s, a) (s) = argmaxa Q (s, a).
Much literature RL focused finding optimal policy. many
methods developed policy optimization. One way find optimal policy solve
Bellman optimality equation use Eqn 13 choose actions. Bellman
optimality equation formulated simple linear program (Bertsekas, 1995):
minV V, subject
P
V (s) R(s, a) + s0 (s, a, s0 )V (s0 ) s, a,

(15)

represents initial distribution states. solution problem optimal value function. Notice V represented matrix form
equation. known linear programs solved polynomial time (Karmarkar,
1984). However, solving might become impractical large (or infinite) state spaces.
Therefore often times methods based dynamic programming preferred linear
programming solution.

3. Non-Deterministic Policies: Definition Motivation
begin section considering problem decision-making sequential decision
support systems. Recently, MDPs emerged useful frameworks optimizing action
choices context medical decision support systems (Schaefer, Bailey, Shechter, &
Roberts, 2004; Hauskrecht & Fraser, 2000; Magni, Quaglini, Marchetti, & Barosi, 2000;
Ernst, Stan, Concalves, & Wehenkel, 2006). Given adequate MDP model (or data
source), many methods used find good action-selection policy. policy
usually deterministic stochastic function. policies types face substantial
barrier terms gaining acceptance medical community, highly
prescriptive leave little room doctors input. problems are, course,
specific medical domain present application actions
executed human. cases, may preferable provide several equivalently
6

fiNon-Deterministic Policies Markovian Decision Processes

good action choices, agent pick among according
heuristics preferences.
address problem, work introduces notion non-deterministic policy,
function mapping state set actions, acting agent
choose.
Definition 1. non-deterministic policy MDP (S, A, T, R, ) function
maps state non-empty set actions denoted (s) A(s).
agent choose action (s) whenever MDP state s.
Definition 2. size non-deterministic
policy , denoted ||, sum
P
cardinality action sets : || = |(s)|.
following sections discuss two scenarios non-deterministic policies
useful. show used implement robust decision support
systems statistical guarantees performance.
3.1 Providing Choice Acting Agent
Even cases complete knowledge dynamics planning problem
hand, accurately calculate actions utilities, might desirable
provide user optimal choice action time step. domains,
difference utility top actions may substantial. medical
decision-making, instance, difference may medically significant based
given state variables.
cases, seems natural let user decide top actions,
using his/her expertise domain. results injection domain
knowledge decision-making process, thus making robust practical.
decisions based facts known user incorporated automated
planning system. based preferences might change case case.
instance, doctor get several recommendations treat patient
maximize chance remission, decide medication apply considering
patients medical record, preferences regarding side effects, medical expenses.
idea providing choice user accompanied reasonable guarantees performance final decision, regardless choice made user.
notion near-optimality enforced make sure actions never far
best possible option. guarantees enforced providing worst-case analysis
decision process.
3.2 Handling Model Uncertainty
many practical cases complete knowledge system hand. Instead,
may get set trajectories collected system according specific policy.
cases, may given chance choose policy (in on-line active RL),
cases may access data fixed policy. medical
trials, particular, data usually collected according randomized policy, fixed ahead
time consultation clinical researchers.
7

fiMilani Fard & Pineau

Given set sample trajectories, either build model domain (in modelbased approaches) directly estimate utility different actions (with model-free approaches). However models estimates always accurate
observe finite amount data. many cases, data may sparse incomplete
uniquely identify best option. is, difference performance measure
different actions statistically significant.
cases might useful let user decide final choice
actions enough evidence differentiate.
comes assumption user identify best choice among
recommended. task therefore provide user small set actions
almost surely include optimal one.
paper focus problem providing flexible policies nearoptimal performance. Using non-deterministic policies handling model uncertainty remains interesting future work.

4. Near-Optimal Non-Deterministic Policies
Often times, beneficial provide user decision support system set
near-optimal solutions. MDPs, would suggest set near-optimal actions
user let user make decision among proposed actions. notion
near-optimality therefore set possible policies consistent
proposed actions. is, matter action chosen among proposed
options state, final performance close optimal policy.
constraint suggests worst-case analysis decision-making process. Therefore,
opt guarantee performance action selection consistent non-deterministic
policy putting near-optimality constraint worst-case selection actions
user.
Definition 3. (worst-case) value state-action pair (s, a) according nondeterministic policy MDP = (S, A, T, R, ) given recursive definition:

X

0

0 0
QM (s, a) = R(s, a) +
(s, a, ) min QM (s , ) ,
(16)
a0 (s0 )

s0

worst-case expected return allowed set actions.
Definition 4. define (worst-case) value state according non (s), be:
deterministic policy , denoted VM
min Q
(s, a).

(17)

a(s)

calculate value non-deterministic policy, construct evaluation MDP,
0 = (S, A0 , R0 , T, ), A0 = R0 = R.
Theorem 1. negated value non-deterministic policy equal
optimal policy evaluation MDP:

Q
(s, a) = QM 0 (s, a).

8

(18)

fiNon-Deterministic Policies Markovian Decision Processes

Proof. show QM 0 satisfies Bellman optimality equation 0 ,
negated values satisfy Eqn 16 :
X
QM 0 (s, a) = R0 (s, a) +
(s, a, s0 ) max
QM 0 (s0 , a0 )
(19)
0
0


s0

QM 0 (s, a) = R0 (s, a)

X

(s, a, s0 ) max
QM 0 (s0 , a0 )
0
0

(20)

(s, a, s0 ) min QM 0 (s0 , a0 ),

(21)



s0

QM 0 (s, a) = R(s, a) +

X
s0

a0 (s0 )


equivalent Eqn 16 Q
(s, a) = QM 0 (s, a).

means policy evaluation non-deterministic policy achieved
method finds optimal policy MDP.
Definition 5. non-deterministic policy said augmented state-action pair
(s, a), denoted 0 = + (s, a), satisfies:
(
(s0 ),
s0 6=
0 (s0 ) =
(22)
(s0 ) {a}, s0 = s.
policy achieved number augmentations policy 0 , say
includes 0 .
Definition 6. non-deterministic policy said non-augmentable according
constraint satisfies , state-action pair (s, a), + (s, a)
satisfy .
paper working constraints particular property:
policy satisfy , policy includes satisfy .
refer constraints monotonic. One constraint -optimality,
discussed next section.
4.1 -Optimal Non-Deterministic Policies
Definition 7. non-deterministic policy MDP said -optimal,
[0, 1], have1 :


VM
(s) (1 )VM
(s), S.

(23)

thought constraint space non-deterministic policies, set
ensure worst-case expected return within range optimal value.
Theorem 2. -optimality constraint monotonic.

1. MDP literature, -optimality defined additive constraint (Q
QM ) (Kearns
& Singh, 2002). derivations analogous case. chose multiplicative constraint
cleaner derivations.

9

fiMilani Fard & Pineau

Proof. Suppose -optimal. augmentation 0 = + (s, a), have:

X
0
0
0 0 0
Q
(s,
a)
=
R(s,
a)
+


(s,
a,

)
min
Q
(s
,

)


s0

a0 0 (s0 )


X
0
0 0 0
(s, a, ) min QM (s , )
R(s, a) +
s0

a0 (s0 )

Q
(s, a),
implies:
0



VM
(s) VM
(s).

-optimal, means 0 -optimal either value function
decrease policy augmentation.
intuitively, follows fact adding options cannot increase
minimum utility former worst case choice still available augmentation.
Definition 8. conservative -optimal non-deterministic policy MDP
policy non-augmentable according following constraint:
X



R(s, a) +
(s, a, s0 )(1 )VM
(s0 ) (1 )VM
(s), (s).
(24)
s0

constraint indicates add actions policy whose reward plus
(1 ) future optimal return within sub-optimal margin. ensures
non-deterministic policy -optimal using inequality:
X


Q
(s, a, s0 )(1 )VM
(s0 ) ,
(25)
(s, a) R(s, a) +
s0

instead solving Eqn 16 using inequality constraint Eqn 23. Applying Eqn 24
guarantees non-deterministic policy -optimal may still augmentable
according Eqn 23, hence name conservative.
shown conservative policy unique.
two different conservative policies, union would conservative,
violates assumption non-augmentable according Eqn 24.
Definition 9. non-augmentable -optimal non-deterministic policy MDP
policy non-augmentable according constraint Eqn 23.
non-deterministic policy adding actions violates nearoptimality constraint worst-case performance. search -optimal policies,
non-augmentable one locally maximal size. means although policy
might largest among -optimal policies, cannot add actions
without removing actions, hence locally maximal reference.
non-augmentable -optimal policy includes conservative policy.
always add conservative policy policy remain within bound.
10

fiNon-Deterministic Policies Markovian Decision Processes

However, non-augmentable -optimal policies necessarily unique,
locally maximal size.
remainder section, focus problem searching space
non-augmentable -optimal policies, maximize criteria. Specifically,
aim find non-deterministic policies give acting agent options staying
within acceptable sub-optimal margin.
present example clarifies concepts introduced far. simplify
presentation example, assume deterministic transitions. However, concepts
apply well probabilistic MDP. Figure 1 shows example MDP. labels
arcs show action names corresponding rewards shown parentheses.
assume ' 1 = 0.05. Figure 2 shows optimal policy MDP.
conservative -optimal non-deterministic policy MDP shown Figure 3.

Figure 1: Example MDP

Figure 2: Optimal policy

Figure 3: Conservative -optimal policy

Figure 4: Two non-augmentable -optimal policies

Figure 4 includes two possible non-augmentable -optimal policies. Although policies Figure 4 -optimal, union -optimal. due fact
adding option one states removes possibility adding options
11

fiMilani Fard & Pineau

states, illustrates local changes policy always appropriate
searching space -optimal policies.
4.2 Optimization Criteria
formalize problem finding -optimal non-deterministic policy terms
optimization problem. several optimization criteria formulated,
still complying -optimality constraint.
Maximizing size policy: According criterion, seek nonaugmentable -optimal policies biggest overall size (Def 2). provides
options agent still keeping -optimal guarantees. algorithms
proposed later sections use optimization criterion. Notice solution
optimization problem non-augmentable according -optimal constraint,
maximizes overall size policy.
variant this, try maximize sum log size action
sets:
X
log |(s)|.
(26)
sS

enforces even distribution choice action set. However,
using basic case maximizing overall size easier optimization
problem.
Maximizing margin: aim maximize margin non-deterministic
policy :
max (),

(27)



where:

() = min
sS

min



Q(s, a) Q(s, ) .
0

a(s),a0 (s)
/

(28)

optimization criterion useful one wants find clear separation
good bad actions state.
Minimizing uncertainty: learn models data
uncertainty optimal action state. use variance estimation value function (Mannor, Simester, Sun, & Tsitsiklis, 2004) along
Z-Test get confidence level comparisons find probability
wrong order comparing actions according values. Let Q
value true model Q empirical estimate based dataset
D. aim minimize uncertainty non-deterministic policy :
min (),


(29)

where:

() = max
sS

max

a(s),a0 (s)
/

12




P r Q(s, a) < Q(s, a0 )|D
.

(30)

fiNon-Deterministic Policies Markovian Decision Processes

Notice last two criteria defined space -optimal policies,
non-augmentable ones.
following sections provide algorithms solve first optimization problem
mentioned above, aims maximize size policy. focus criterion
seems appropriate medical decision support systems, desirable
acceptability system find policies provide much choice possible
acting agent. Developing algorithms address two optimization criteria
remains interesting open problem.
4.3 Maximal -Optimal Policy
exact computational complexity finding maximal -optimal policy yet known.
problem certainly NP, one find value non-deterministic policy
polynomial time solving evaluation MDP linear program. suspect
problem NP-complete, yet find reduction known NP-complete
problem.
order find largest -optimal policy, present two algorithms. first present
Mixed Integer Program (MIP) formulation problem, present search algorithm uses monotonic property -optimal constraint. MIP method
useful general theoretical formulation problem, search algorithm
potential extensions heuristics.
4.3.1 Mixed Integer Programming Solution
Recall formulate problem finding optimal deterministic policy
MDP simple linear program (Bertsekas, 1995):
minV V, subject
P
V (s) R(s, a) + s0 (s, a, s0 )V (s0 ) s, a,

(31)

thought initial distribution states. solution
problem optimal value function (V ). Similarly, computed V using
Eqn 31, problem searching optimal non-deterministic policy according
size criterion rewritten Mixed Integer Program:2
maxV, (T V + (Vmax Vmin )eTs ea ), subject
V (s) (1 )V (s)

P
(s, a) > 0

P
0
0
V (s) R(s, a) + s0 (s, a, )V (s ) + Vmax (1 (s, a)) s, a.

(32)

overloading notation define binary matrix representing policy,
(s, a) 1 (s), 0 otherwise. define Vmax = Rmax /(1 )
Vmin = Rmin /(1 ). es column vectors 1 appropriate dimensions.
first set constraints makes sure stay within optimal return.
2. Note MIP, unlike standard LP MDPs, choice affect solution cases
tie size .

13

fiMilani Fard & Pineau

second set constraints ensures least one action selected per state. third
set ensures state-action pairs chosen policy, Bellman
constraint holds, otherwise, constant Vmax makes constraint trivial. Notice
solution problem maximizes || result non-augmentable.
Theorem 3. solution mixed integer program Eqn 32 non-augmentable
according -optimality constraint.
Proof. First, notice solution -optimal, due first set constraints
(worst-case) value function. show non-augmentable, counter argument,
suppose could add state-action pair solution , still staying suboptimal margin. adding pair, objective function increased (Vmax Vmin ),
bigger possible decrease V term, thus objective
improved, conflicts solution.
use MIP solver solve problem. Note however
make use monotonic nature constraints. general purpose MIP solver could
end searching space possible non-deterministic policies, would
require running time exponential number state-action pairs (O(2|S||A|+ )).
4.3.2 Heuristic Search
Alternatively, develop heuristic search algorithm find maximal -optimal policy.
make use monotonic property -optimal policies narrow
search. start computing conservative policy. augment arrive
non-augmentable policy. make use fact policy -optimal,
neither policy includes it, thus cut search tree
point.
Table 1: Heuristic search algorithm find -optimal policies maximum size
Function getOptimal(, startIndex, )

startIndex |S||A|
(s, a) pi

/ (s) & V ( + (s, a)) (1 )V
0 getOptimal ( + (s, a), + 1, )
g(0 ) > g(o )
0
end
end
end
return
algorithm presented Table 1 one-sided recursive depth-first-search algorithm
searches space plausible non-deterministic policies maximize function
g(). assume ordering set state-action pairs {pi } =
14

fiNon-Deterministic Policies Markovian Decision Processes

{(sj , ak )}. ordering chosen according heuristic along mechanism
cut parts search space. V optimal value function function
V returns value non-deterministic policy calculated solving
corresponding evaluation MDP.
make call function passing conservative policy
starting first state-action pair: getOptimal(m , 0, ).
asymptotic running time algorithm O((|S||A|)d (tm + tg )),
maximum size -optimal policy minus size conservative policy, tm
time solve original MDP (polynomial relevant parameters), tg time
calculate function g. Although worst-case running time still exponential
number state-action pairs, run-time much less search space sufficiently
small. |A| term due fact check possible augmentations
state. Note algorithm searches space -optimal policies rather
non-augmentable ones. set function g() = ||, algorithm
return biggest non-augmentable -optimal policy.
search improved using heuristics order state-action pairs
prune search. One start search policy rather
conservative policy. potentially useful constraints
problem.
4.3.3 Directed Acyclic Transition Graphs
One way narrow search add action maximum value
state s, ignore rest actions adding top action result values
-optimality bound:
!
0 = +

s, argmax Q (s, a) .
a(s)
/

modified algorithm follows:
Table 2: Modified heuristic search algorithm augmentation rule Eqn 33.
Function getOptimal(, )

(s) 6= A(s)
argmaxa(s)
Q (s, a)
/
V ( + (s, a)) (1 )V
0 getOptimal ( + (s, a), )
g(0 ) > g(o )
0
end
end
end
return

15

(33)

fiMilani Fard & Pineau

algorithm Table 2 leads running time O(|S|d (tm + tg )). However
guarantee see non-augmentable policies. due fact
adding action, order values might change. transition structure MDP
contains loop non-zero probability (transition graph directed acyclic, i.e. DAG),
heuristic produce optimal result cutting search time.
Theorem 4. MDPs DAG transition structure, algorithm Table 2 generate non-augmentable -optimal policies would generated full search.
Proof. prove this, first notice sort DAG topological sort. Therefore, arrange states levels, state make transitions states
future level. easy see adding actions state non-deterministic
policy change worst-case value past levels. effect
Q-values current level future level.
given non-augmentable -optimal policy generated full search,
sequence augmentations generated policy. permutation sequence
would create policy intermediate polices -optimal. rearrange sequence add actions reverse order level.
point mentioned above, Q-value actions point added
change target policy realized. Therefore actions Q-values
minimum value must policy, otherwise add them, conflicts
target policy non-augmentable. Since actions certain Q-value
must added, add order. Therefore target policy realized
rule Eqn 33.
transition structure DAG, one might partial evaluation
augmented policy approximate value adding actions, possibly
backups rather using original Q-values. offers possibility trading-off
computation time better solutions.

5. Empirical Results
evaluate framework proposed algorithms, first test MIP search
formulations MDPs created randomly, test search algorithm real-world
treatment design scenario. Finally, conduct experiment computer-aided web
navigation task human subjects assess usefulness non-deterministic policies
assisting human decision-making.
5.1 Random MDPs
first experiment, aim study non-deterministic policies change
value two algorithms compare terms running time. begin,
generated random MDPs 5 states 4 actions. transitions deterministic
(chosen uniformly random) rewards random values 0 1, except
one states reward 10 one actions; set 0.95. MIP
method implemented MATLAB CPLEX.
16

fiNon-Deterministic Policies Markovian Decision Processes

=0

= 0.01

= 0.02

= 0.03

Figure 5: MIP solution different values {0, 0.01, 0.02, 0.03}. labels
edges action indices, followed corresponding immediate rewards.
Figure 5 shows solution MIP defined Eqn 32 particular randomly
generated MDP. see size non-deterministic policy increases performance threshold relaxed. see even small values several
actions included policy state. course result Q-values
close other. property typical many medical scenarios different
treatments provide slightly different results.
compare running time MIP solver search algorithm, constructed
random MDPs described state-action pairs. Figure 6 shows running
time averaged 20 different random MDPs 5 states, assuming = 0.01 (which
allows several solutions). expected, algorithms running time exponential
number state-action pairs (note exponential scale time axis).
running time search algorithm bigger constant factor (possibly due naive
implementation), smaller exponent base, results faster asymptotic
running time. Even exponential running time, one still use search algorithm
solve problems hundred state-action pairs. sufficient
many practical domains, including real-world medical decision scenarios shown
next section.
observe effect choice running time algorithms, fix
size random MDPs 7 states 5 actions state, change
17

fiMilani Fard & Pineau

Figure 6: Running time MIP search algorithm function number
state-action pairs = 0.01.
value measure running time algorithms 100 trials. Figure 7 shows
average running time algorithms different values . expected,
search algorithm go deeper search tree optimality threshold relaxed
running time thus increase. running time MIP method,
hand, remains relativity constant exhaustively searches space possible
non-deterministic policies. results representative relative behaviour
two approaches range problems.

Figure 7: Running time MIP search algorithm function , 7 states
5 actions. Many actions included policy = 0.02.

5.2 Medical Decision-making
demonstrate non-deterministic policies used presented medical
domain, tested full search algorithm MDP constructed medical decisionmaking task involving real patient data. data collected part large (4000+
patients) multi-step randomized clinical trial, designed investigate comparative effectiveness different treatments provided sequentially patients suffering depression
(Fava et al., 2003). goal find treatment plan maximizes chance
18

fiNon-Deterministic Policies Markovian Decision Processes

remission. dataset includes large number measured outcomes. current
experiment, focus numerical score called Quick Inventory Depressive Symptomatology (QIDS), used study assess levels depression (including
patients achieved remission). purposes experiment, discretize
QIDS scores (which range 5 27) uniformly quartiles, assume this,
along treatment step (up 4 steps allowed), completely describe patients state. Note underlying transition graph treated DAG,
study limited four steps treatment action choices change steps.
19 actions (treatments) total. reward 1 given patient achieves
remission (at step) reward 0 given otherwise. transition reward
models estimated empirically medical database using frequentist approach.
Table 3: Policy running time full search algorithm medical problem.
= 0.02

= 0.015

= 0.01

=0

118.7

12.3

3.5

1.4

CT
SER
BUP
CIT+BUS

CT
SER

CT

CT

9 QIDS < 12

CIT+BUP
CIT+CT

CIT+BUP
CIT+CT

CIT+BUP

CIT+BUP

VEN
CIT+BUS
CT

VEN
CIT+BUS

VEN

VEN

12 QIDS < 16

16 QIDS 27

CT
CIT+CT

CT
CIT+CT

CT
CIT+CT

CT

Time (seconds)
5 < QIDS < 9

Table 3 shows non-deterministic policy obtained state second
step trial (each acronym refers specific treatment). computed using
search algorithm, assuming different values . Although problem tractable
MIP formulation (304 state-action pairs), full search space -optimal policies
still possible. Table 3 shows running time algorithm, expected,
increases relax threshold . Here, use heuristics. However,
underlying transition graph DAG, could use heuristic discussed previous
section (Eqn 33) get policies even faster.
interesting question set priori. practice, doctor may use
full table guideline, using smaller values he/she wants rely
decision support system, larger values relying his/her assessments.
believe particular presentation non-deterministic policies could used
accepted clinicians, excessively prescriptive keeps physician
patient decision cycle. contrast traditional notion policies
reinforcement learning, often leaves place physicians intervention.
19

fiMilani Fard & Pineau

5.3 Human Subject Interaction
Finally, conduct experiment assess usefulness non-deterministic policies
human subjects. Ideally, would conduct experiments medical settings
physicians, studies costly difficult conduct given require
participation many medical professionals. therefore study non-deterministic policies
easier domain constructing web-based game played computer
human (either jointly separately).
game defined follows. user given target word asked navigate
around pages Wikipedia visit pages contain target word. user
click word page. system uses Google search Wiki website
clicked word keyword current page (the choice keyword
discussed later). randomly chooses one top eight search results moves
page. process mimics hyperlink structure web (extending
hyperlink structure Wiki make target words easily reachable).
user given ten attempts asked reach many pages target word
possible. similar game used another work infer semantic distances
concepts (West, Pineau, & Precup, 2009). game, however, designed way
computer model provide results similar human player thus enable us
assess effectiveness computer-aided decisions non-deterministic policies.
construct task CD version Wikipedia (Schools-Wikipedia, 2009),
structured manageable version Wikipedia intended use schools. test
approach need build MDP model task. done using empirical data
follows. First, use Latent Dirichlet Allocation (LDA) using Gibbs sampling (Griffiths
& Steyvers, 2004) divide pages Wikipedia 20 topics. topic corresponds
state MDP. LDA algorithm identifies topic set keywords
occur often pages topic. define sets keywords
action (20 actions totals, corresponding 20 keywords). randomly navigate
around Wiki using protocol described (with computer player
clicks LDA keywords) collect 200,000 transitions. use observed data build
transition reward model MDP (the reward 1 hit 0 otherwise).
specific choices LDA parameter number states actions
MDP made way best policy provided model comparable
performance human player.
Using Amazon Mechanical Turk (MTurk, 2010), consider three experimental conditions task. one experiment, given target, computer chooses word
(uniformly random) set keywords (the action) comes optimal
policy MDP model. another experiment, human subjects choose click
word without help. Finally, test domain human users
computer highlights, hints, words come non-deterministic policy
= 0.1. record time taken process number times target word
observed (number hits). Table 4 summarizes average outcomes experiment
four target words (we used seven target words, could collect enough data
them). include p-value t-test comparing results human
agents without hints. computer score averaged 1000 runs.
20

fiNon-Deterministic Policies Markovian Decision Processes

Table 4: Comparison different agents web navigation task. t-test
number hits human player uses hints one not.
Target

Computer

Human

Human hint

t-Test

Marriage

1.88 hits

1.94 hits
103 seconds
(86 subjects)

2.63 hits
93 seconds
(86 subjects)

0.012

4.86 hits
91 seconds
(67 subjects)

5.61 hits
84 seconds
(97 subjects)

0.049

3.67 hits
85 seconds
(98 subjects)

4.39 hits
89 seconds
(83 subjects)

0.014

3.18 hits
96 seconds
(92 subjects)

3.42 hits
85 seconds
(123 subjects)

0.46

(1000 runs)
Military

4.72 hits
(1000 runs)

Book

3.77 hits
(1000 runs)

Animal

2.50 hits
(1000 runs)

first three target words, performance computer agent close
human user, observe providing hints user results statistically significant
increase number hits. fact see computer-aided human outperforms
computer human agents. shows non-deterministic policies
provide means inject human domain knowledge computer models way
final outcome superior decision-making solely performed one party.
last word, computer model working poorly, judging low hit rate. Thus,
surprising see hints provide much help human agent
case (as seen non-significant p-value). observe general speedup
(for three targets) time taken agent choose click words,
shows usefulness non-deterministic policies accelerating human
subjects decision-making process.

6. Discussion
paper introduces new concept non-deterministic policies potential use
decision support systems based Markovian processes. context, investigate
assumption decision-making system return single optimal action
relaxed, instead return set near-optimal actions.
Non-deterministic policies inherently different stochastic policies. Stochastic
policies assume randomized action selection strategy specific probabilities,
whereas non-deterministic policies impose constraint. thus use bestcase worst-case analysis non-deterministic policies highlight different scenarios
human user.
21

fiMilani Fard & Pineau

benefits non-deterministic policies sequential decision-making two-fold.
First, several actions difference performance negligible,
report actions near-optimal options. instance, medical setting,
difference outcome two treatment options might medically
significant. case, may beneficial provide near-optimal options.
makes system robust user-friendly. medical decision-making
process, instance, physician make final decision among near-optimal
options based side effects burden, patients preferences, expense, criteria
captured model used decision support system. key constraint,
however, make sure regardless final choice actions, performance
executed policy always bounded near optimal. framework, property
maintained -optimality guarantee worst-case scenario.
Another potential use non-deterministic action sets Markovian decision processes capture uncertainties optimality actions. Often times, amount
data models constructed sufficient clearly identify single optimal
action. forced chose one action optimal one, might high
chance making wrong decision. However, given chance provide set
possibly-optimal actions, ensure include promising options
cutting obviously bad ones. setting, task trim action set much
possible providing guarantee optimal action still among top
possible options.
solve first problem, paper introduces two algorithms find flexible nearoptimal policies. First derive exact solution MIP formulation find maximal
-optimal policy. MIP solution is, however, computationally expensive
scale large domains. describe search algorithm solve problem
less computational cost. algorithm fast enough applied real world medical
domains. show use heuristics search algorithm find solution
DAG structures even faster. heuristic search provide approximate solutions
general case.
Another way scale problem larger domains approximate solution
MIP program relaxing constraints. One relax constraints
allow non-integral solutions penalize objective values away 0 1.
study approximation methods remains interesting direction future work.
idea non-deterministic policies introduces wide range new problems
research topics. Section 4, discuss idea near optimal non-deterministic policies
address problem finding one largest action set. mentioned,
optimization criteria might useful decision support systems.
include maximizing decision margin (the margin worst selected action
best one selected), alternatively minimizing uncertainty wrong selection.
Formalizing problems MIP formulation, incorporating heuristic
search, might prove useful.
evidenced human interaction experiments, non-deterministic policies substantially improve outcome planning decision-making tasks human
user assisted robust computer-generated plan. Allowing several suggestions
step provides effective way incorporating domain knowledge human side
22

fiNon-Deterministic Policies Markovian Decision Processes

decision-making process. medical domains physicians domain knowledge
often hard capture computer model, collaborative model decision-making
non-deterministic policies could offer powerful framework selecting effective,
clinically acceptable, treatment strategies.

Acknowledgments
authors wish thank A. John Rush (Duke-NUS Graduate Medical School), Susan
A. Murphy (University Michigan), Doina Precup (McGill University) helpful
discussions regarding work. Funding provided National Institutes Health
(grant R21 DA019800) NSERC Discovery Grant program.

References
Bellman, R. (1957). Dynamic Programming. Princeton University Press.
Bertsekas, D. (1995). Dynamic Programming Optimal Control, Vol 2. Athena Scientific.
Ernst, D., Stan, G. B., Concalves, J., & Wehenkel, L. (2006). Clinical data based optimal
STI strategies HIV: reinforcement learning approach. Proceedings
Fifteenth Machine Learning conference Belgium Netherlands (Benelearn),
pp. 6572.
Fava, M., Rush, A., Trivedi, M., Nierenberg, A., Thase, M., Sackeim, H., Quitkin, F., Wisniewski, S., Lavori, P., Rosenbaum, J., & Kupfer, D. (2003). Background rationale
sequenced treatment alternatives relieve depression (STAR* D) study. Psychiatric Clinics North America, 26 (2), 457494.
Griffiths, T. L., & Steyvers, M. (2004). Finding scientific topics. Proceedings National
Academy Sciences, 101 (Suppl. 1), 52285235.
Hauskrecht, M., & Fraser, H. (2000). Planning treatment ischemic heart disease
partially observable Markov decision processes. Artificial Intelligence Medicine,
18 (3), 221244.
Heger, M. (1994). Consideration risk reinforcement learning. Proceedings
Eleventh International Conference Machine Learning (ICML), pp. 105111.
Karmarkar, N. (1984). new polynomial-time algorithm linear programming. Combinatorica, 4 (4), 373395.
Kearns, M., & Singh, S. (2002). Near-optimal reinforcement learning polynomial time.
Machine Learning, 49.
Magni, P., Quaglini, S., Marchetti, M., & Barosi, G. (2000). Deciding intervene:
Markov decision process approach. International Journal Medical Informatics,
60 (3), 237253.
Mannor, S., Simester, D., Sun, P., & Tsitsiklis, J. N. (2004). Bias variance value
function estimation. Proceedings Twenty-First International Conference
Machine Learning (ICML), pp. 308322.
23

fiMilani Fard & Pineau

Mannor, S., Simester, D., Sun, P., & Tsitsiklis, J. N. (2007). Bias variance approximation value function estimates. Management Science, 53 (2), 308322.
MTurk (2010). Amazon mechanical turk. http://www.mturk.com/.
Murphy, S. A. (2005). experimental design development adaptive treatment
strategies. Statistics Medicine, 24 (10), 14551481.
Pineau, J., Bellemare, M. G., Rush, A. J., Ghizaru, A., & Murphy, S. A. (2007). Constructing evidence-based treatment strategies using methods computer science. Drug
Alcohol Dependence, 88 (Supplement 2), S52 S60.
Russell, S. J., & Norvig, P. (2003). Artificial Intelligence: Modern Approach (Second
Edition). Prentice Hall.
Sato, M., & Kobayashi, S. (2000). Variance-penalized reinforcement learning risk-averse
asset allocation. Proceedings Second International Conference Intelligent
Data Engineering Automated Learning, Data Mining, Financial Engineering,
Intelligent Agents, pp. 244249. Springer-Verlag.
Schaefer, A., Bailey, M., Shechter, S., & Roberts, M. (2004). Handbook Operations
Research / Management Science Applications Health Care, chap. Medical decisions
using Markov decision processes. Kluwer Academic Publishers.
Schools-Wikipedia (2009).
wikipedia.org/.

2008/9 wikipedia selection schools.

http://schools-

Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction (Adaptive
Computation Machine Learning). MIT Press.
Thapa, D., Jung, I., & Wang, G. (2005). Agent based decision support system using reinforcement learning emergency circumstances. Lecture Notes Computer
Science, 3610, 888.
West, R., Pineau, J., & Precup, D. (2009). Wikispeedia: online game inferring
semantic distances concepts. Proceedings Twenty-First International
Jont Conference Artifical Intelligence (IJCAI), pp. 15981603, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.

24


