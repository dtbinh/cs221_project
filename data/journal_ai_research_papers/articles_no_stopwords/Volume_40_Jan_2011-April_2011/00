journal artificial intelligence

submitted published

non deterministic policies
markovian decision processes
mahdi milani fard
joelle pineau

mmilan cs mcgill ca
jpineau cs mcgill ca

reasoning learning laboratory
school computer science mcgill university
montreal qc canada

abstract
markovian processes long used model stochastic environments reinforcement learning emerged framework solve sequential decision making
environments recent years attempts made apply methods
reinforcement learning construct decision support systems action selection
markovian environments although conventional methods reinforcement learning
proved useful concerning sequential decision making cannot applied current form decision support systems medical domains
suggest policies often highly prescriptive leave little room users
input without ability provide flexible guidelines unlikely methods
gain ground users systems
introduces concept non deterministic policies allow flexibility users decision making process constraining decisions remain near
optimal solutions provide two compute non deterministic policies
discrete domains study output running time method set
synthetic real world experiment human subjects
humans assisted hints non deterministic policies outperform human
computer agents web navigation task

introduction
decision making well studied ai community intelligent
agents designed developed act interact variety environments usually involves sensing environment making decision
intelligent inference mechanism performing action environment russell norvig often times process involves level learning along
decision making process make agent efficient performing intended
goal
reinforcement learning rl branch ai tries develop computational
solving learning interaction rl process
learning dohow map situations actionsso maximize numerical
reward signal sutton barto many methods developed solve
rl different types environments different types agents however
work rl focused autonomous agents robots software
agents rl controllers thus designed issue single action time step
c

ai access foundation rights reserved

fimilani fard pineau

executed acting agent past years methods developed
rl community started used sequential decision support systems murphy
pineau bellemare rush ghizaru murphy thapa jung wang
hauskrecht fraser many systems human makes final
decision usability acceptance issues thus become important cases
rl methods therefore require level adaptation used decision support
systems adaptations main contribution
medical domains among cases rl needs adaptation although
rl framework correctly sequential decision making complex medical scenarios including long term treatment design standard rl methods cannot applied
medical settings current form lack flexibility suggestions
requirements course specific medical domains instance might
needed aircraft controller provides suggestions pilot
important difference decision support system classical rl
stems fact decision support system acting agent often human
course decision process therefore assumption
controller send one clear commanding signal acting agent
appropriate accurate assume aspect decision making process
influenced user system
view decision process particularly relevant two different situations
first many practical cases exact model system instead
may noisy model built finite number interactions environment
leads type uncertainty usually referred extrinsic uncertainty
rl ignore uncertainty assume model perfect however
look closely performance optimal action imperfect model might
statistically different next best action bayesian approaches looked
providing confidence measure agents performance mannor simester
sun tsitsiklis cases acting agent human use
confidence measures provide user complete set actions might
optimal enough evidence differentiate user
use expertise make final decision methods guarantee
suggestions provided system statistically meaningful plausible
hand even complete knowledge system
identify optimal action might still actions roughly equal
performance point decision near optimal options could left
acting agentnamely human decision support system
could many advantages ranging better user experience increased robustness
flexibly among near optimal solutions user select domain
knowledge preferences captured system instance
medical diagnosis system suggests treatments providing physician several
options might useful final decision could made knowledge
patients medical status preferences regarding side effects
throughout address latter issue combination theoretical
empirical investigations introduce concept non deterministic policies
capture decision making process intended decision support systems policies


finon deterministic policies markovian decision processes

involve suggesting set actions non deterministic choice made
user apply formulation solve finding near optimal policies
provide flexible suggestions user
particular investigate suggest several actions acting agent
providing performance guarantees worst case analysis section introduces
necessary technical background material section defines concept non deterministic
policies related concepts section addresses providing choice
acting agent keeping near optimality guarantees performance worst case
scenario propose two solve provide approximation
techniques speed computation larger domains
methods introduced general enough apply decision support system observable markovian environment empirical investigations focus
primarily sequential decision making clinical domains system
provide suggestions best treatment options patients decisions
provided sequence treatment phases systems specifically interesting
often times different treatment options seem provide slightly different therefore providing physician several suggestions would beneficial
improving usability system performance final decision

definitions notations
section introduces main notions behind sequential decision making mathematical formulations used rl
markov decision processes
markov decision process mdp model system dynamics sequential decision
involves probabilistic uncertainty future states system bellman
mdps used model interactions agent observable
markovian environment system assumed state given time
agent observes state performs action accordingly system makes
transition next state agent receives reward
formally mdp defined tuple r
states set states state usually captures complete configuration
system state system known future system
independent previous system transitions means state
system sufficient statistic history system
actions set actions allowed state set
actions set actions agent choose interacting
system state
transition probabilities defines transition probabilities
system function specifies likely end state given
current state specific action performed agent transition probabilities


fimilani fard pineau

specified markovian assumption state system
time denoted st action time
pr st st p r st st



focus homogeneous processes system dynamics independent
time thus transition function stationary respect time
def

p r st st



rewards r r probabilistic reward model depending
current state system action taken agent receive reward
drawn model focus homogeneous processes
reward distribution change time reward time denoted
rt
rt r st



depending domain reward could deterministic stochastic use
general stochastic model throughout mean distribution
denoted r
discount factor discount rate used calculate long term
return
agent starts initial state time step action st
taken agent system makes transition st st agent
receives immediate reward rt r st
goal agent maximize discounted sum rewards
horizon h could infinite usually referred return denoted


h
x

rt





finite horizon case sum taken horizon limit discount factor
set however infinite horizon case discount factor less
return finite value return process depends
stochastic transitions rewards well actions taken agent
often times transition structure mdp contains loop non zero probability transition graph modeled directed acyclic graph dag
class mdps interesting includes multi step decision making finite horizons
found medical domains
policy value function
policy way defining agents action selection respect changes
environment probabilistic policy mdp mapping state space
distribution action space





finon deterministic policies markovian decision processes

deterministic policy policy defines single action per state
later introduce notion non deterministic policies mdps deal
sets actions
agent interacts environment takes actions according policy
value function policy defined expectation return given
agent acts according policy


x
def
v e e
rt st



linearity expectation write expression recursive
form known bellman equation bellman


x
x



v
r
v



aa

value function used primary measure performance much
rl literature however ideas take risk variance
return account measure optimality heger sato kobayashi
common criteria though assume agent trying policy
maximizes value function policy referred optimal policy
define value function state action pairs usually referred
q function q value pair definition


x
def

q e e
rt st


q value expectation return given agent starts state
takes action follows policy q function satisfies bellman
equation
x
x
q r

q





rewritten
q r

x

v





q function often used compare optimality different actions given fixed
subsequent policy
optimality
optimal policy denoted defined policy maximizes value
function initial state
argmax v






fimilani fard pineau

shown bellman mdp exists optimal deterministic policy worse policy mdp value optimal
policy v satisfies bellman optimality equation


x
v max r
v

aa



deterministic optimal policy follows


x



argmax r
v
aa





alternatively write equations q function
x
q r
v





thus v maxa q argmaxa q
much literature rl focused finding optimal policy many
methods developed policy optimization one way optimal policy solve
bellman optimality equation use eqn choose actions bellman
optimality equation formulated simple linear program bertsekas
minv v subject
p
v r v



represents initial distribution states solution optimal value function notice v represented matrix form
equation known linear programs solved polynomial time karmarkar
however solving might become impractical large infinite state spaces
therefore often times methods dynamic programming preferred linear
programming solution

non deterministic policies definition motivation
begin section considering decision making sequential decision
support systems recently mdps emerged useful frameworks optimizing action
choices context medical decision support systems schaefer bailey shechter
roberts hauskrecht fraser magni quaglini marchetti barosi
ernst stan concalves wehenkel given adequate mdp model data
source many methods used good action selection policy policy
usually deterministic stochastic function policies types face substantial
barrier terms gaining acceptance medical community highly
prescriptive leave little room doctors input course
specific medical domain present application actions
executed human cases may preferable provide several equivalently


finon deterministic policies markovian decision processes

good action choices agent pick among according
heuristics preferences
address work introduces notion non deterministic policy
function mapping state set actions acting agent
choose
definition non deterministic policy mdp r function
maps state non empty set actions denoted
agent choose action whenever mdp state
definition size non deterministic
policy denoted sum
p
cardinality action sets
following sections discuss two scenarios non deterministic policies
useful used implement robust decision support
systems statistical guarantees performance
providing choice acting agent
even cases complete knowledge dynamics
hand accurately calculate actions utilities might desirable
provide user optimal choice action time step domains
difference utility top actions may substantial medical
decision making instance difference may medically significant
given state variables
cases seems natural let user decide top actions
expertise domain injection domain
knowledge decision making process thus making robust practical
decisions facts known user incorporated automated
system preferences might change case case
instance doctor get several recommendations treat patient
maximize chance remission decide medication apply considering
patients medical record preferences regarding side effects medical expenses
idea providing choice user accompanied reasonable guarantees performance final decision regardless choice made user
notion near optimality enforced make sure actions never far
best possible option guarantees enforced providing worst case analysis
decision process
handling model uncertainty
many practical cases complete knowledge system hand instead
may get set trajectories collected system according specific policy
cases may given chance choose policy line active rl
cases may access data fixed policy medical
trials particular data usually collected according randomized policy fixed ahead
time consultation clinical researchers


fimilani fard pineau

given set sample trajectories build model domain modelbased approaches directly estimate utility different actions model free approaches however estimates accurate
observe finite amount data many cases data may sparse incomplete
uniquely identify best option difference performance measure
different actions statistically significant
cases might useful let user decide final choice
actions enough evidence differentiate
comes assumption user identify best choice among
recommended task therefore provide user small set actions
almost surely include optimal one
focus providing flexible policies nearoptimal performance non deterministic policies handling model uncertainty remains interesting future work

near optimal non deterministic policies
often times beneficial provide user decision support system set
near optimal solutions mdps would suggest set near optimal actions
user let user make decision among proposed actions notion
near optimality therefore set possible policies consistent
proposed actions matter action chosen among proposed
options state final performance close optimal policy
constraint suggests worst case analysis decision making process therefore
opt guarantee performance action selection consistent non deterministic
policy putting near optimality constraint worst case selection actions
user
definition worst case value state action pair according nondeterministic policy mdp r given recursive definition

x




qm r
min qm





worst case expected return allowed set actions
definition define worst case value state according non
deterministic policy denoted vm
min q






calculate value non deterministic policy construct evaluation mdp
r r r
theorem negated value non deterministic policy equal
optimal policy evaluation mdp

q
qm





finon deterministic policies markovian decision processes

proof qm satisfies bellman optimality equation
negated values satisfy eqn
x
qm r
max
qm







qm r

x

max
qm





min qm







qm r

x





equivalent eqn q
qm

means policy evaluation non deterministic policy achieved
method finds optimal policy mdp
definition non deterministic policy said augmented state action pair
denoted satisfies






policy achieved number augmentations policy say
includes
definition non deterministic policy said non augmentable according
constraint satisfies state action pair
satisfy
working constraints particular property
policy satisfy policy includes satisfy
refer constraints monotonic one constraint optimality
discussed next section
optimal non deterministic policies
definition non deterministic policy mdp said optimal



vm
vm




thought constraint space non deterministic policies set
ensure worst case expected return within range optimal value
theorem optimality constraint monotonic

mdp literature optimality defined additive constraint q
qm kearns
singh derivations analogous case chose multiplicative constraint
cleaner derivations



fimilani fard pineau

proof suppose optimal augmentation

x



q



r








min
q











x


min qm
r




q

implies




vm
vm


optimal means optimal value function
decrease policy augmentation
intuitively follows fact adding options cannot increase
minimum utility former worst case choice still available augmentation
definition conservative optimal non deterministic policy mdp
policy non augmentable according following constraint
x



r
vm
vm




constraint indicates add actions policy whose reward plus
future optimal return within sub optimal margin ensures
non deterministic policy optimal inequality
x


q
vm


r


instead solving eqn inequality constraint eqn applying eqn
guarantees non deterministic policy optimal may still augmentable
according eqn hence name conservative
shown conservative policy unique
two different conservative policies union would conservative
violates assumption non augmentable according eqn
definition non augmentable optimal non deterministic policy mdp
policy non augmentable according constraint eqn
non deterministic policy adding actions violates nearoptimality constraint worst case performance search optimal policies
non augmentable one locally maximal size means although policy
might largest among optimal policies cannot add actions
without removing actions hence locally maximal reference
non augmentable optimal policy includes conservative policy
add conservative policy policy remain within bound


finon deterministic policies markovian decision processes

however non augmentable optimal policies necessarily unique
locally maximal size
remainder section focus searching space
non augmentable optimal policies maximize criteria specifically
aim non deterministic policies give acting agent options staying
within acceptable sub optimal margin
present example clarifies concepts introduced far simplify
presentation example assume deterministic transitions however concepts
apply well probabilistic mdp figure shows example mdp labels
arcs action names corresponding rewards shown parentheses
assume figure shows optimal policy mdp
conservative optimal non deterministic policy mdp shown figure

figure example mdp

figure optimal policy

figure conservative optimal policy

figure two non augmentable optimal policies

figure includes two possible non augmentable optimal policies although policies figure optimal union optimal due fact
adding option one states removes possibility adding options


fimilani fard pineau

states illustrates local changes policy appropriate
searching space optimal policies
optimization criteria
formalize finding optimal non deterministic policy terms
optimization several optimization criteria formulated
still complying optimality constraint
maximizing size policy according criterion seek nonaugmentable optimal policies biggest overall size def provides
options agent still keeping optimal guarantees
proposed later sections use optimization criterion notice solution
optimization non augmentable according optimal constraint
maximizes overall size policy
variant try maximize sum log size action
sets
x
log

ss

enforces even distribution choice action set however
basic case maximizing overall size easier optimization

maximizing margin aim maximize margin non deterministic
policy
max







min
ss

min



q q







optimization criterion useful one wants clear separation
good bad actions state
minimizing uncertainty learn data
uncertainty optimal action state use variance estimation value function mannor simester sun tsitsiklis along
z test get confidence level comparisons probability
wrong order comparing actions according values let q
value true model q empirical estimate dataset
aim minimize uncertainty non deterministic policy
min






max
ss

max









p r q q




finon deterministic policies markovian decision processes

notice last two criteria defined space optimal policies
non augmentable ones
following sections provide solve first optimization
mentioned aims maximize size policy focus criterion
seems appropriate medical decision support systems desirable
acceptability system policies provide much choice possible
acting agent developing address two optimization criteria
remains interesting open
maximal optimal policy
exact computational complexity finding maximal optimal policy yet known
certainly np one value non deterministic policy
polynomial time solving evaluation mdp linear program suspect
np complete yet reduction known np complete

order largest optimal policy present two first present
mixed integer program mip formulation present search uses monotonic property optimal constraint mip method
useful general theoretical formulation search
potential extensions heuristics
mixed integer programming solution
recall formulate finding optimal deterministic policy
mdp simple linear program bertsekas
minv v subject
p
v r v



thought initial distribution states solution
optimal value function v similarly computed v
eqn searching optimal non deterministic policy according
size criterion rewritten mixed integer program
maxv v vmax vmin ets ea subject
v v

p


p


v r v vmax



overloading notation define binary matrix representing policy
otherwise define vmax rmax
vmin rmin es column vectors appropriate dimensions
first set constraints makes sure stay within optimal return
note mip unlike standard lp mdps choice affect solution cases
tie size



fimilani fard pineau

second set constraints ensures least one action selected per state third
set ensures state action pairs chosen policy bellman
constraint holds otherwise constant vmax makes constraint trivial notice
solution maximizes non augmentable
theorem solution mixed integer program eqn non augmentable
according optimality constraint
proof first notice solution optimal due first set constraints
worst case value function non augmentable counter argument
suppose could add state action pair solution still staying suboptimal margin adding pair objective function increased vmax vmin
bigger possible decrease v term thus objective
improved conflicts solution
use mip solver solve note however
make use monotonic nature constraints general purpose mip solver could
end searching space possible non deterministic policies would
require running time exponential number state action pairs
heuristic search
alternatively develop heuristic search maximal optimal policy
make use monotonic property optimal policies narrow
search start computing conservative policy augment arrive
non augmentable policy make use fact policy optimal
neither policy includes thus cut search tree
point
table heuristic search optimal policies maximum size
function getoptimal startindex

startindex
pi

v v
getoptimal
g g

end
end
end
return
presented table one sided recursive depth first search
searches space plausible non deterministic policies maximize function
g assume ordering set state action pairs pi


finon deterministic policies markovian decision processes

sj ak ordering chosen according heuristic along mechanism
cut parts search space v optimal value function function
v returns value non deterministic policy calculated solving
corresponding evaluation mdp
make call function passing conservative policy
starting first state action pair getoptimal
asymptotic running time tm tg
maximum size optimal policy minus size conservative policy tm
time solve original mdp polynomial relevant parameters tg time
calculate function g although worst case running time still exponential
number state action pairs run time much less search space sufficiently
small term due fact check possible augmentations
state note searches space optimal policies rather
non augmentable ones set function g
return biggest non augmentable optimal policy
search improved heuristics order state action pairs
prune search one start search policy rather
conservative policy potentially useful constraints

directed acyclic transition graphs
one way narrow search add action maximum value
state ignore rest actions adding top action values
optimality bound



argmax q



modified follows
table modified heuristic search augmentation rule eqn
function getoptimal


argmaxa
q

v v
getoptimal
g g

end
end
end
return





fimilani fard pineau

table leads running time tm tg however
guarantee see non augmentable policies due fact
adding action order values might change transition structure mdp
contains loop non zero probability transition graph directed acyclic e dag
heuristic produce optimal cutting search time
theorem mdps dag transition structure table generate non augmentable optimal policies would generated full search
proof prove first notice sort dag topological sort therefore arrange states levels state make transitions states
future level easy see adding actions state non deterministic
policy change worst case value past levels effect
q values current level future level
given non augmentable optimal policy generated full search
sequence augmentations generated policy permutation sequence
would create policy intermediate polices optimal rearrange sequence add actions reverse order level
point mentioned q value actions point added
change target policy realized therefore actions q values
minimum value must policy otherwise add conflicts
target policy non augmentable since actions certain q value
must added add order therefore target policy realized
rule eqn
transition structure dag one might partial evaluation
augmented policy approximate value adding actions possibly
backups rather original q values offers possibility trading
computation time better solutions

empirical
evaluate framework proposed first test mip search
formulations mdps created randomly test search real world
treatment design scenario finally conduct experiment computer aided web
navigation task human subjects assess usefulness non deterministic policies
assisting human decision making
random mdps
first experiment aim study non deterministic policies change
value two compare terms running time begin
generated random mdps states actions transitions deterministic
chosen uniformly random rewards random values except
one states reward one actions set mip
method implemented matlab cplex


finon deterministic policies markovian decision processes









figure mip solution different values labels
edges action indices followed corresponding immediate rewards
figure shows solution mip defined eqn particular randomly
generated mdp see size non deterministic policy increases performance threshold relaxed see even small values several
actions included policy state course q values
close property typical many medical scenarios different
treatments provide slightly different
compare running time mip solver search constructed
random mdps described state action pairs figure shows running
time averaged different random mdps states assuming
allows several solutions expected running time exponential
number state action pairs note exponential scale time axis
running time search bigger constant factor possibly due naive
implementation smaller exponent base faster asymptotic
running time even exponential running time one still use search
solve hundred state action pairs sufficient
many practical domains including real world medical decision scenarios shown
next section
observe effect choice running time fix
size random mdps states actions state change


fimilani fard pineau

figure running time mip search function number
state action pairs
value measure running time trials figure shows
average running time different values expected
search go deeper search tree optimality threshold relaxed
running time thus increase running time mip method
hand remains relativity constant exhaustively searches space possible
non deterministic policies representative relative behaviour
two approaches range

figure running time mip search function states
actions many actions included policy

medical decision making
demonstrate non deterministic policies used presented medical
domain tested full search mdp constructed medical decisionmaking task involving real patient data data collected part large
patients multi step randomized clinical trial designed investigate comparative effectiveness different treatments provided sequentially patients suffering depression
fava et al goal treatment plan maximizes chance


finon deterministic policies markovian decision processes

remission dataset includes large number measured outcomes current
experiment focus numerical score called quick inventory depressive symptomatology qids used study assess levels depression including
patients achieved remission purposes experiment discretize
qids scores range uniformly quartiles assume
along treatment step steps allowed completely describe patients state note underlying transition graph treated dag
study limited four steps treatment action choices change steps
actions treatments total reward given patient achieves
remission step reward given otherwise transition reward
estimated empirically medical database frequentist
table policy running time full search medical
















ct
ser
bup
cit bus

ct
ser

ct

ct

qids

cit bup
cit ct

cit bup
cit ct

cit bup

cit bup

ven
cit bus
ct

ven
cit bus

ven

ven

qids

qids

ct
cit ct

ct
cit ct

ct
cit ct

ct

time seconds
qids

table shows non deterministic policy obtained state second
step trial acronym refers specific treatment computed
search assuming different values although tractable
mip formulation state action pairs full search space optimal policies
still possible table shows running time expected
increases relax threshold use heuristics however
underlying transition graph dag could use heuristic discussed previous
section eqn get policies even faster
interesting question set priori practice doctor may use
full table guideline smaller values wants rely
decision support system larger values relying assessments
believe particular presentation non deterministic policies could used
accepted clinicians excessively prescriptive keeps physician
patient decision cycle contrast traditional notion policies
reinforcement learning often leaves place physicians intervention


fimilani fard pineau

human subject interaction
finally conduct experiment assess usefulness non deterministic policies
human subjects ideally would conduct experiments medical settings
physicians studies costly difficult conduct given require
participation many medical professionals therefore study non deterministic policies
easier domain constructing web game played computer
human jointly separately
game defined follows user given target word asked navigate
around wikipedia visit contain target word user
click word page system uses google search wiki website
clicked word keyword current page choice keyword
discussed later randomly chooses one top eight search moves
page process mimics hyperlink structure web extending
hyperlink structure wiki make target words easily reachable
user given ten attempts asked reach many target word
possible similar game used another work infer semantic distances
concepts west pineau precup game however designed way
computer model provide similar human player thus enable us
assess effectiveness computer aided decisions non deterministic policies
construct task cd version wikipedia schools wikipedia
structured manageable version wikipedia intended use schools test
need build mdp model task done empirical data
follows first use latent dirichlet allocation lda gibbs sampling griffiths
steyvers divide wikipedia topics topic corresponds
state mdp lda identifies topic set keywords
occur often topic define sets keywords
action actions totals corresponding keywords randomly navigate
around wiki protocol described computer player
clicks lda keywords collect transitions use observed data build
transition reward model mdp reward hit otherwise
specific choices lda parameter number states actions
mdp made way best policy provided model comparable
performance human player
amazon mechanical turk mturk consider three experimental conditions task one experiment given target computer chooses word
uniformly random set keywords action comes optimal
policy mdp model another experiment human subjects choose click
word without help finally test domain human users
computer highlights hints words come non deterministic policy
record time taken process number times target word
observed number hits table summarizes average outcomes experiment
four target words used seven target words could collect enough data
include p value test comparing human
agents without hints computer score averaged runs


finon deterministic policies markovian decision processes

table comparison different agents web navigation task test
number hits human player uses hints one
target

computer

human

human hint

test

marriage

hits

hits
seconds
subjects

hits
seconds
subjects



hits
seconds
subjects

hits
seconds
subjects



hits
seconds
subjects

hits
seconds
subjects



hits
seconds
subjects

hits
seconds
subjects



runs
military

hits
runs

book

hits
runs

animal

hits
runs

first three target words performance computer agent close
human user observe providing hints user statistically significant
increase number hits fact see computer aided human outperforms
computer human agents shows non deterministic policies
provide means inject human domain knowledge computer way
final outcome superior decision making solely performed one party
last word computer model working poorly judging low hit rate thus
surprising see hints provide much help human agent
case seen non significant p value observe general speedup
three targets time taken agent choose click words
shows usefulness non deterministic policies accelerating human
subjects decision making process

discussion
introduces concept non deterministic policies potential use
decision support systems markovian processes context investigate
assumption decision making system return single optimal action
relaxed instead return set near optimal actions
non deterministic policies inherently different stochastic policies stochastic
policies assume randomized action selection strategy specific probabilities
whereas non deterministic policies impose constraint thus use bestcase worst case analysis non deterministic policies highlight different scenarios
human user


fimilani fard pineau

benefits non deterministic policies sequential decision making two fold
first several actions difference performance negligible
report actions near optimal options instance medical setting
difference outcome two treatment options might medically
significant case may beneficial provide near optimal options
makes system robust user friendly medical decision making
process instance physician make final decision among near optimal
options side effects burden patients preferences expense criteria
captured model used decision support system key constraint
however make sure regardless final choice actions performance
executed policy bounded near optimal framework property
maintained optimality guarantee worst case scenario
another potential use non deterministic action sets markovian decision processes capture uncertainties optimality actions often times amount
data constructed sufficient clearly identify single optimal
action forced chose one action optimal one might high
chance making wrong decision however given chance provide set
possibly optimal actions ensure include promising options
cutting obviously bad ones setting task trim action set much
possible providing guarantee optimal action still among top
possible options
solve first introduces two flexible nearoptimal policies first derive exact solution mip formulation maximal
optimal policy mip solution however computationally expensive
scale large domains describe search solve
less computational cost fast enough applied real world medical
domains use heuristics search solution
dag structures even faster heuristic search provide approximate solutions
general case
another way scale larger domains approximate solution
mip program relaxing constraints one relax constraints
allow non integral solutions penalize objective values away
study approximation methods remains interesting direction future work
idea non deterministic policies introduces wide range
topics section discuss idea near optimal non deterministic policies
address finding one largest action set mentioned
optimization criteria might useful decision support systems
include maximizing decision margin margin worst selected action
best one selected alternatively minimizing uncertainty wrong selection
formalizing mip formulation incorporating heuristic
search might prove useful
evidenced human interaction experiments non deterministic policies substantially improve outcome decision making tasks human
user assisted robust computer generated plan allowing several suggestions
step provides effective way incorporating domain knowledge human side


finon deterministic policies markovian decision processes

decision making process medical domains physicians domain knowledge
often hard capture computer model collaborative model decision making
non deterministic policies could offer powerful framework selecting effective
clinically acceptable treatment strategies

acknowledgments
authors wish thank john rush duke nus graduate medical school susan
murphy university michigan doina precup mcgill university helpful
discussions regarding work funding provided national institutes health
grant r da nserc discovery grant program

references
bellman r dynamic programming princeton university press
bertsekas dynamic programming optimal control vol athena scientific
ernst stan g b concalves j wehenkel l clinical data optimal
sti strategies hiv reinforcement learning proceedings
fifteenth machine learning conference belgium netherlands benelearn
pp
fava rush trivedi nierenberg thase sackeim h quitkin f wisniewski lavori p rosenbaum j kupfer background rationale
sequenced treatment alternatives relieve depression star study psychiatric clinics north america
griffiths l steyvers finding scientific topics proceedings national
academy sciences suppl
hauskrecht fraser h treatment ischemic heart disease
partially observable markov decision processes artificial intelligence medicine

heger consideration risk reinforcement learning proceedings
eleventh international conference machine learning icml pp
karmarkar n polynomial time linear programming combinatorica
kearns singh near optimal reinforcement learning polynomial time
machine learning
magni p quaglini marchetti barosi g deciding intervene
markov decision process international journal medical informatics

mannor simester sun p tsitsiklis j n bias variance value
function estimation proceedings twenty first international conference
machine learning icml pp


fimilani fard pineau

mannor simester sun p tsitsiklis j n bias variance approximation value function estimates management science
mturk amazon mechanical turk http www mturk com
murphy experimental design development adaptive treatment
strategies statistics medicine
pineau j bellemare g rush j ghizaru murphy constructing evidence treatment strategies methods computer science drug
alcohol dependence supplement
russell j norvig p artificial intelligence modern second
edition prentice hall
sato kobayashi variance penalized reinforcement learning risk averse
asset allocation proceedings second international conference intelligent
data engineering automated learning data mining financial engineering
intelligent agents pp springer verlag
schaefer bailey shechter roberts handbook operations
management science applications health care chap medical decisions
markov decision processes kluwer academic publishers
schools wikipedia
wikipedia org

wikipedia selection schools

http schools

sutton r barto g reinforcement learning introduction adaptive
computation machine learning mit press
thapa jung wang g agent decision support system reinforcement learning emergency circumstances lecture notes computer
science
west r pineau j precup wikispeedia online game inferring
semantic distances concepts proceedings twenty first international
jont conference artifical intelligence ijcai pp san francisco ca
usa morgan kaufmann publishers inc




