journal artificial intelligence

submitted published

word vs class word sense disambiguation
ruben izquierdo

ruben izquierdobevia vu nl

vu university amsterdam
amsterdam netherlands

armando suarez

armando dlsi ua es

university alicante
alicante spain

german rigau

german rigau ehu es

university basque country
san sebastian spain

abstract
empirically demonstrated word sense disambiguation wsd tasks last senseval semeval exercises assigning appropriate meaning words context resisted
attempts successfully addressed many authors argue one possible reason could
use inappropriate sets word meanings particular wordnet used de facto
standard repository word meanings tasks thus instead word
senses defined wordnet approaches derived semantic classes representing groups
word senses however meanings represented wordnet used wsd
fine grained sense level coarse grained semantic class level called supersenses suspect appropriate level abstraction could levels
contributions manifold first propose simple method automatically
derive semantic classes intermediate levels abstraction covering nominal verbal wordnet meanings second empirically demonstrate automatically derived semantic classes
outperform classical approaches word senses coarse grained sense groupings
third demonstrate supervised wsd system benefits semantic classes additional semantic features reducing amount training examples
finally demonstrate robustness supervised semantic class wsd system
tested domain corpus

introduction
word sense disambiguation wsd intermediate natural language processing nlp task
consists assigning correct lexical interpretation ambiguous words depending surrounding context agirre edmonds navigli one successful approaches
last years supervised learning examples machine learning classification
induced semantically annotated corpora marquez escudero martnez rigau
quite often machine learning systems obtained better knowledge
ones shown experimental work international evaluation exercises senseval semeval nevertheless lately weakly supervised knowledgebased approaches reaching
performance close supervised techniques specific tasks tasks
information competitions found http www senseval org
c

ai access foundation rights reserved

fii zquierdo u arez r igau

corpora usually manually annotated experts word senses taken particular lexical
semantic resource commonly wordnet fellbaum
however wordnet widely criticized sense repository often provides
finegrained sense distinctions higher level applications machine translation mt
question answering aq fact wsd low level semantic granularity resisted
attempts inferring robust broad coverage seems many wordsense distinctions
subtle captured automatic systems current small volumes wordsense
annotated examples wordnet sense repository organizers english words
task senseval reported inter annotation agreement snyder palmer interestingly difficult outperform state art sense wsd systems
moreover supervised sensebased approaches biased towards frequent sense
predominant sense training data therefore performance supervised sensebased
systems strongly punished applied domain specific texts sense distribution differs considerably respect sense distribution training corpora escudero marquez
rigau
try overcome facing task wsd semantic
class point view instead traditional word sense semantic class
seen abstract concept groups subconcepts word senses sharing semantic properties features examples semantic classes vehicle food animal hypothesis
appropriate set semantic classes instead word senses could help wsd several
aspects
higher level abstraction could ease integration wsd systems higher
level nlp applications machine translation question answering
grouping together semantically coherent sets training examples could increase
robustness supervised wsd systems
socalled bottleneck acquisition could alleviated
points explained along following hypothesis propose
create classifiers semantic classes instead word sense experts one semantic classifier
trained semantic class final system assign proper semantic class
ambiguous word instead sense traditional approaches example
automatically derived semantic classes introduced later three senses church
wordnet subsumed semantic classes r eligious rganization b uilding
r eligious c eremony note semantic classes still discriminate among three
different senses word church instance assign semantic class b uilding
occurrence church context still know refers second sense additionally
semantic class b uilding covers six times training examples
covered second sense church
example text senseval automatically annotated semantic classes seen
figure shows automatic annotations classbased classifiers different semantic classes blc stands basic level concepts izquierdo suarez rigau ss
use following format throughout refer particular sense wordnum
pos pos
part speech n nouns v verbs adjectives r adverbs num stands sense number



fiw ord vs c lass w ord ense isambiguation

supersenses ciaramita johnson wnd wordnet domains magnini cavaglia
l bentivogli pianta sumo suggested upper merged ontology niles
pease incorrect assignments marked italics correct tags included
brackets next automatic ones obviously semantic resources relate senses different
level abstraction diverse semantic criteria properties could interest subsequent semantic processing moreover combination could improve overall since
offer different semantic perspectives text
id








word

ancient
stone
church
amid

fields

blc

ss

wnd

sumo

artifact n
building n

noun artifact
noun artifact

building
building

mineral
building

geographic area n
physical object n

noun location
noun object

factotum geography

landarea







sound

property n

noun attribute

factotum acoustics

radiatingsound
soundattribute





bells

device n

noun artifact

musicalinstrument







cascading


tower
calling

move v

verb motion

factotum acoustics
factotum

construction n
designate v
request v

noun artifact
factotum
verb stative
factotum
verb communication

building
communication
requesting





faithful

sogroup n
cial group n

noun group

person religion

group





evensong

time day n
writing n

noun communication

religion

timeinterval
text

motion

table example automatic annotation text several semantic class labels
main goal investigate performance alternative semantic classes
derived wordnet supervised wsd first propose system automatically extract sets
semantically coherent groupings nominal verbal senses wordnet system
allows generate arbitrary sets semantic classes distinct levels abstraction second
analyze impact respect alternative semantic classes performing classbased
wsd empirical automatically generated classes performs better
created manually wndomains sumo supersenses etc capturing precise
information third demonstrate supervised wsd system benefits
semantic classes additional semantic features reducing amount training


fii zquierdo u arez r igau

examples finally supervised class system adapted particular
domain traditional word sense systems included comparison purposes
summarizing empirically investigates
performance alternative semantic groupings used supervised class
wsd system
impact class semantic features supervised wsd framework
required amount training examples needed class wsd order obtain
competitive
relative performance class wsd systems respect wsd word
experts
robustness class wsd system specific domains
moreover tested domain dataset supervised class wsd system obtains slightly better state art word sense wsd system itmakessense
system presented zhong ng
introduction present work directly related supervised
wsd semantic classes section presents sense groupings semantic classes
used study section explains method automatically derive semantic classes
wordnet different levels abstraction moreover analysis different semantic groupings
included section presents system developed perform supervised class
wsd performance system shown section system tested several
wsd datasets provided international evaluations comparison participants
competitions introduced sections experiments system applied
specific domain analyzed section finally conclusions future work presented
section

related work
field wsd broad large amount publications wsd
last years section revises relevant wsd approaches dealing appropriate
sets meanings word
focused deriving different word sense groupings overcome
finegrained distinctions wordnet hearst schutze peters peters vossen
mihalcea moldovan agirre de lacalle navigli snow
provide methods grouping senses word thus producing coarser word
sense groupings example word church three senses wordnet sense
grouping presented snow et al produces unique grouping according
church monosemous
ontonotes project hovy marcus palmer ramshaw weischedel different
meanings word considered kind tree ranging coarse concepts root
finegrained meanings leaves merging increased fine coarse grained
obtaining inter annotator agreement around coarse grained repository used


fiw ord vs c lass w ord ense isambiguation

wsd lexical sample task semeval pradhan dligach palmer
systems scored fscore note merging created word following
manual costly process
similarly previous another task organized within semeval
consisted traditional wsd word task another coarsegrained sense repository derived
wordnet navigli litkowski hargraves case wordnet synsets
automatically linked oxford dictionary english ode graph
meanings word linked ode entry merged coarse sense systems
achieving top scores followed supervised approaches taking advantage different corpora
training reaching top fscore
previous cases aimed solving granularity word sense
definitions wordnet however approaches still word experts one classifier trained
word obviously decreasing average polysemy word coarsersenses
makes easier classification choice performance systems increase
cost reducing discriminative power
conversely instead word experts creates semantic class experts
semantic classifiers exploit diverse information extracted meanings different
words belong class
wikipedia wikipedia recently used overcome supervised learning methods excessively finegrained definition meanings lack annotated data
strong domain dependence existing annotated corpora way wikipedia provides
source annotated data large constantly expansion mihalcea gangemi
nuzzolese presutti draicchio musetti ciancarini
contrast focused predefined sets sense groupings
learning classbased classifiers wsd segond schiller greffenstette chanod ciaramita johnson villarejo marquez rigau curran ciaramita altun
izquierdo suarez rigau grouping senses different words
explicit comprehensive semantic class work presented mihalcea csomai
ciaramita makes use three different sets semantic classes wordnet classes two
named entity annotated corpora train sequential classifiers classifiers trained
basic features collocations semantic features reach performance around
th position semeval allwords task
semantic classes wordnet called supersenses widely used different
works instance paa reichartz apply conditional random fields model
sequential context words relation supersenses extend model include
potential supersenses word training data f score reported
nouns verbs potential labels used training data worse
training data right labels although interesting evaluate
system applying fold cross validation semcor

semantic classes levels abstraction
meanings represented wordnet used wsd fine grained sense
level coarse grained semantic class level called supersenses suspect
appropriate level abstraction could found levels section propose


fii zquierdo u arez r igau

simple method automatically derive semantic classes intermediate levels abstraction covering nominal verbal wordnet meanings first introduce wordnet semantic resource
sense repository used wsd systems note semantic classes used
work linked wordnet
wordnet fellbaum online lexical database english contains concepts
represented synsets sets synonyms content words nouns verbs adjectives
adverbs one synset groups together several senses different words synonyms
wordnet different types lexical semantic relations interlink different synsets creating
way large structured lexical semantic network important relation
encoded wordnet subclass relation nouns called hyponymy relation verbs
troponymy relation table shows basic figures different wordnet versions including
total number words polysemous words synsets senses possible senses words
average polysemy
version
wn
wn
wn
wn
wn
wn

words







polysemous







synsets







senses







avg polysemy







table statistics wordnet versions

supersenses
supersenses name wordnet lexicographer files within framework wsd
detail wordnet synsets organized forty five supersenses syntactic categories
nouns verbs adjectives adverbs logical groupings person phenomenon
feeling location etc basic categories nouns verbs adjectives
adverbs cases different senses word grouped high level
supersense reducing polysemy word often case similar
senses word classes adjectives adverbs supersense taggers
usually developed nouns verbs tsvetkov schneider hovy bhatia faruqui
dyer presents interesting study tagging adjectives supersenses acquired
germanet hamp feldweg et al
wordnet domains
wordnet domains wnd magnini cavaglia l bentivogli pianta hierarchy domains used label semi automatically wordnet synsets
set labels organized taxonomy following dewey decimal classification system
information supersenses found http wordnet princeton edu wordnet
man lexnames wn html
http wndomains itc
http www oclc org dewey



fiw ord vs c lass w ord ense isambiguation

building wnd many labels assigned high levels wordnet hierarchy
automatically inherited across hypernym troponym hierarchy thus semi automatic
method used develop resource free errors inconsistencies castillo real
rigau gonzalez rigau castillo
information brought domain labels complementary already wordnet wnd
present characteristics interesting wsd first domain label may contain
senses different wordnet subhierarchies derived different supersenses instance
domain religion contains senses priest deriving noun person church
deriving noun artifact second domain label may include synsets different
syntactic categories instance domain religion contains verb pray adjective
holy
furthermore single wnd label subsume different senses word reducing
way polysemy instance first third senses church wordnet
domain label religion
sumo concepts
sumo niles pease created part ieee standard upper ontology working
group goal develop standard upper ontology promote data interoperability information search retrieval automated inference natural language processing umo consists
set concepts relations axioms formalize upper ontology experiments
used complete wordnet mapping umo labels niles pease
case three noun senses church wordnet classified r eligious rganization
b uilding r eligious c eremony according sumo ontology
example semantic classes
example table presents three senses glosses word church wordnet
sense





wordnet
gloss

christian churchn group christians group professing
christian doctrine belief church biblical term assembly
church n church building n
public especially christian worship
church empty
church service n church n
service conducted church dont late
church
word senses
church n
christianity n

table glosses examples senses churchn
table classes assigned sense according semantic resources introduced previously instance considering wordnet domains observed senses
number group christians service conducted church belong domain
several cycles manual checking automatically labeled data
http www ontologyportal org



fii zquierdo u arez r igau

religion contrary supersenses sumo represent three senses church
different semantic classes note resulting assignment semantic classes identifies
word sense individually

sense




supersense
noun group
noun artifact
noun act

semantic class
wnd
sumo
r eligion r eligious rganization
b uildings
b uilding
r eligion
r eligious c eremony

table semantic classes noun churchn
levels abstraction
basic level concepts rosch hereinafter blc compromise two
conflicting principles characterization general vs specific
represent many concepts possible
represent many features possible
conflicting characterization blc typically occur middle levels
semantic hierarchies
notion base concepts hereinafter bc introduced eurowordnet vossen
bc supposed important concepts several language specific wordnets
importance measured terms two main criteria
high position semantic hierarchy
many relations concepts
eurowordnet set concepts selected called common base concepts
common bc concepts act bc least two languages local wordnets english
dutch spanish used select set common bc later initiatives similar sets
derived harmonize construction multilingual wordnets
considering definitions next section present method automatically generate
different sets basic level concepts wordnet different levels abstraction

automatic selection basic level concepts
several approaches developed trying alleviate fine granularity wordnet
senses obtaining word sense groupings hearst schutze peters et al mihalcea
moldovan agirre de lacalle navigli snow et al bhagwani satapathy karnick cases consists grouping different senses
word resulting decrease polysemy obviously polysemy reduced
wsd task classification becomes easier system coarse senses
obtain better systems word senses works used predefined sets
semantic classes mainly supersenses segond et al ciaramita johnson curran


fiw ord vs c lass w ord ense isambiguation

villarejo et al ciaramita altun picca gliozzo ciaramita paa
reichartz b tsvetkov et al
section describe simple method automatically create different sets basic level
concepts wordnet method exploits nominal verbal structure wordnet
basic idea synsets wordnet high number relations important could
candidates blc capture relevance synset wordnet considered two
options
total number relations encoded wordnet synset
hypo total number hyponymy relations synset
method follows bottomup exploiting hypernymy chains wordnet
synset process starts visiting synsets hyperonymy chain selecting
stopping walk synset blc ancestor first local maximum considering
total number relations hypo synsets one hyperonym
method chooses one higher number relations continue process process
ends preliminary set candidate synsets selected potential blc
additionally synset selected potential blc candidate must subsume represent
least certain number descendant synsets thus minimum number synsets blc must
subsume another parameter represented symbol candidate
blcs reach threshold discarded subsumed synsets reassigned
blc candidate appearing higher levels abstraction
presents pseudocode parameters
wordnet resource type relations considered hypo minimum number
concepts must subsumed blc two phases first
one selects candidate blc following bottomup second phase discards
candidate blc satisfy threshold
figure shows schema illustrate selection process node represents synset
edges represent hyperonymy relations instance hyperonym
hyperonym f number synset indicates number hyponymy relations
schema illustrates selection process blc candidates synset j criterion hypo
process starts checking hyperonym j f f two hyperonyms b
next synset visited hyperonymy chain j since higher number hyponymy
relations three compares number relations hyperonym synset
three relations previous synset f two number increasing
process continues next node visit number relations two
number three process stops synset selected blc candidate j
table shows real example selection process noun church wordnet
hyperonym chain number relations encoded wordnet criterion shown
synset local maximum chain marked bold
starts checking first hyperonym synset synset



fii zquierdo u arez r igau

figure example blc selection
rel






rel







rel







synset
group grouping
social group
organisation organization
establishment institution
faith religion
christianity church christian church
synset
entity something
object physical object
artifact artefact
construction structure
building edifice
place worship
church church building
synset
act human action human activity
activity
ceremony
religious ceremony religious ritual
service religious service divine service
church church service

table blc selection noun church wordnet



fiw ord vs c lass w ord ense isambiguation

blc extraction
require wordnet wn typeofrelation threshold
blccandidates
synset w n
cur
obtaining hypernym chains current synset cur
h hypernyms w n cur
synsetw ithm orerelations w n h
iterating number relations increased
n umof rels w n cur n umof rels w n
cur
h hypernyms w n cur
synsetw ithm orerelations w n h
end store cur candidate blc
blccandidates blccandidates cur
end
filtering blc candidates
blcf inal
blc blccandidates
n umberof descendants w n blc
blcf inal blcf inal blc
end
end
return blcf inal

figure example blc selection sense church


fii zquierdo u arez r igau

figure see diagram showing partial view selection process candidate
blc sense number noun church synset dotted synset
processed church n synsets bold visited one
gray building n one selected blc church n process stops checking synset
structure n number relations lower number relations
previous synset relations edifice n
obviously combining different values threshold example
criterion considered hypo process ends different sets blc
extracted automatically wordnet version
furthermore instead number relations consider frequency synsets
corpus measure importance synset frequency calculated sum
frequencies word senses contained synset obtained semcor miller
leacock tengi bunker wordnet
sum two main parameters parameter representing minimum number synsets blc must represent criterion used characterizing
relevance synsets values parameters
parameter integer value greater equal
synset relevance parameter value considered measure importance synset
four possibilities
number relations synset
relations encoded synset
hypo hyponymy relations
frequency synset
freqwn frequency obtained wordnet
freqsc frequency obtained semcor
implementation different sets blc used several
wordnet versions freely available
analysis basic level concepts
selected wordnet generate several sets blc combining four types synset
relevance criteria values values selected since
represent different levels abstraction ranging filtering blc
must subsume least synsets table shows combinations synset relevance
parameters number concepts set blc contains average depth
wordnet hierarchy group gray highlight two sets blc blc blc
relations parameter use experiments described
expected increasing threshold direct effect number blc
average depth wordnet hierarchy particular values decreased indicating
threshold increased concepts selected abstract general instance
http adimen si ehu es web blc



fiw ord vs c lass w ord ense isambiguation

threshold

synset relevance










hypo
freqsc
freqwn

hypo
freqsc
freqwn

hypo
freqsc
freqwn

hypo
freqsc
freqwn

blc
nouns verbs





























depth
nouns verbs

































table automatic base level concepts wn

nominal part wordnet number concepts selected range
filtering however average depth reduction acute since
varies fact shows robustness method selecting synsets
intermediate level abstraction
expected verbal part wordnet behave differently case since verbal
hierarchies less deep average depth synsets selected ranges
relations hypo relations
general frequency criteria observe similar behavior
relation criteria however effect threshold dramatic specially
nouns expected verbs behave differently nouns number blc
semcor wordnet frequencies reaches plateau around fact number
close verbal top beginners wordnet
summing devised simple automatic procedure deriving different sets blc
representing different level abstraction whole set nominal verbal synsets wordnet following section explain supervised framework developed wsd
order exploit semantic classes described section previous one

supervised class wsd
follow supervised machine learning develop set semantic class wsd
classifiers systems use implementation support vector machine train
classifiers one per semantic class semantic annotated corpora acquiring positive
negative examples class classifiers built basis set features defined
representing examples class training data must collected treated
pretty different way usual word


fii zquierdo u arez r igau

first word class approaches selects training examples differently
word instances word used training examples figure
shows distribution training examples used generate word sense classifier noun
house following binary definition svm one classifier generated word sense
classifiers occurrences word sense associated classifier
used positive examples rest word sense occurrences used negative examples
classifier house

classifier
sense

house n

classifier
sense

house n

house n

classifier
sense

house n

house n

figure distribution examples word
class use examples words belong
particular semantic class figure shows distribution examples class
case one classifier created semantic class occurrences words belonging
semantic class associated classifier used positive examples rest
occurrences word senses associated different semantic class selected negative
examples
obviously class number examples training increased table
shows example sense church n following word examples
found semcor church n conversely positive training examples used
building classifier semantic class building edifice
think several advantages first semantic classes reduce average
polysemy degree words word senses might grouped together within semantic
class moreover acquisition bottleneck supervised machine learning
attenuated increase number training examples however mixing
one classifier examples different words instance building class
grouping together examples hotel hospital church could introduce noise
learning process grouping unrelated word senses
learning svm
support vector machines svm proven robust competitive many nlp
tasks wsd particular marquez et al experiments used svm light


fiw ord vs c lass w ord ense isambiguation

classifier animal

classifier building

hospital
building

house
building

dog
animal

cat
animal

star
person

figure distribution examples class

church n

classifier
word

building edifice
class

examples
church n
church n
building n
hotel n
hospital n
barn n


positive examples







examples

table number examples semcor word vs class approaches



fii zquierdo u arez r igau

implementation joachims svm used induce hyperplane separates positive
negative examples maximum margin means hyperplane located
intermediate position positive negative examples trying keep maximum distance
closest positive example closest negative example cases possible
get hyperplane divides space linearly better allow errors obtain
efficient hyperplane known soft margin svm requires estimation parameter
c represents trade allowed training errors margin set
value demonstrated good value svm wsd tasks
classifying example obtain value output function svm classifier
corresponding semantic class word example system simply selects class
greatest value
corpora
three semantic annotated corpora used training testing semcor training
senseval senseval english words tasks testing
semcor miller et al subset brown corpus plus novel red badge
courage developed group created wordnet contains
texts around running words lemmatized sensetagged according princeton wordnet sense annotations semcor
automatically ported wordnet versions
senseval english words corpus hereinafter se palmer fellbaum cotton delfs
dang consists words text three wall street journal wsj articles representing different domains penn treebank ii sense inventory used tagging
wordnet
senseval english words corpus hereinafter se snyder palmer made
words extracted two wsj articles one excerpt brown corpus sense
repository wordnet used tag words proper senses
considered alternative evaluation datasets instance semeval coarse
grained task corpus however dataset discarded corpus annotated
particular set word sense clusters additionally provide clear simple way
compare orthogonal sets clusterings although recent senseval semeval
tasks wsd think purpose evaluation different level abstraction
wsd senseval senseval still datasets best fit purposes recent
semeval competitions designed address specific topics multilinguality joint
wsd named entity recognition however make additional experiments
domain adaptation dataset provided semeval task words word sense
disambiguation specific domain wsd domain agirre lopez de lacalle fellbaum
hsieh tesconi monachini vossen segers






http web eecs umich edu mihalcea downloads html semcor
http www sle sharp co uk senseval
http www senseval org senseval
indeed participated task preliminary version system
http semeval fbk eu semeval php location tasks



fiw ord vs c lass w ord ense isambiguation

feature types
following previous contributions supervised wsd selected set basic features
represent training testing examples include additional features semantic
classes
basic features
word forms lemmas window words around target word
pos concatenation preceding following three five pos tags
bigrams trigrams formed lemmas word forms window words
around target word use tokens regardless pos build bi trigrams
replace target word character x features increase generalization
semantic features
frequent semantic class target word calculated semcor
monosemous semantic class monosemous words window size five words
around target word
basic features widely used literature work presented yarowsky
features pieces information occur context target word local features
including bigrams trigrams including target word lemmas word forms partof
speech labels pos addition wordforms lemmas larger window around target
word considered features representing topic discourse
set features extended semantic information several types semantic classes
considered create features particular two different sets blc blc
blc supersenses wordnet domains wnd sumo
order increase generalization capabilities class classifiers filter
irrelevant features measure relevance feature f class c terms frequency
f class c feature f class calculate frequency feature
within class number times occurs examples class obtain
total frequency feature classes get relative frequency dividing
values classfreq totalfreq lower certain threshold feature
removed feature list class c way make sure features selected
class frequently related class others set threshold
obtained empirically preliminary versions classifiers applying crossvalidation setting semcor
selected set since represent different levels abstraction said section refer
threshold minimum number synsets possible blc must subsume considered proper blc
sets blc built criterion
value feature example feature type word form feature type
houses
depending experiment around original features removed filter



fii zquierdo u arez r igau

semantic classbased wsd experiments
section present performance semantic class wsd system
words wsd senseval se senseval se datasets want analyze behavior
class wsd system working different levels abstraction said
level abstraction defined semantic class used build classifiers
experiment defined two different parameters one involving particular set
semantic classes
target class semantic classes used train classifiers determining abstraction
level system case tested word sense blc blc wordnet domains wnd sumo supersenses ss
semantic features class semantic classes used building semantic features
case tested blc blc wnd sumo supersenses ss
target class type classes classifier assigns given ambiguous word
instance target class traditional word expert classifiers word senses semantic
feature class one used building semantic features independent target
class instance use wordnet domains extract monosemous words context
target word use wnd labels words semantic features building
classifier
combining different semantic classes target features generated set experiments described next sections way evaluate independently impact
selecting one semantic class another target class semantic feature class
test
se
se

pos
n
v
n
v

sense





blc





blc





sumo





ss





wnd





table average polysemy se se
table shows average polysemy ap measured se se respect different semantic classes used evaluation target classes expected every corpus behaves
differently average polysemy verbs higher nouns could assume
advance relevant reductions polysemy degree obtained increasing level
abstraction fact acute verbs note large reduction polysemy verbs
supersenses wnd note priori se seems difficult
disambiguate se independently abstraction level
baselines
baselines evaluations define frequent classes mfc word calculated
semcor ties classes specific word solved obtaining global frequency
included word evaluation comparison purposes since current system designed
class evaluation



fiw ord vs c lass w ord ense isambiguation

semcor tied classes selecting frequent class whole training
corpus semcor occurrences particular word able calculate
frequent class word compute global frequency possible
semantic classes obtained wordnet semcor select frequent one table
shows baseline semantic class testing corpora
class
sense
blc
blc
sumo
supersense
wnd

pos
n
v
n
v
n
v
n
v
n
v
n
v

se
mfc ap













se
mfc
ap
























table frequent class baselines average polysemy ap se se
expected performances mfc baselines high particular corresponding nouns ranging nominal baselines seem perform similarly
se se verbal baselines appear consistently much lower se se
se verbal baselines range se verbal baselines range
wnd high due low polysemy degree nouns verbs
obviously increasing level abstraction senses wnd increase
basic system
section present performance supervised semantic classbased wsd system
table shows system trained varying target classes
basic feature set values correspond f measures harmonic mean recall
precision training systems semcor testing se se test sets
improve baselines shown italics additionally showing statistically
significant positive difference compared corresponding baseline mcnemars
test marked bold
interestingly basic system word sense level outperforms baselines se
se nouns verbs addition systems obtain cases significantly better
verbs interesting verbs word sense level baselines
different class level differences datasets much smaller
expected systems increase augmenting level abstraction
senses wnd cases baseline reached outperformed even
relevant consider baseline already quite high however high
level abstraction supersenses wnd basic systems seem unable outperform
baselines


fii zquierdo u arez r igau

class
sense
blc
blc
sumo
supersense
wnd

pos
n
v
n
v
n
v
n
v
n
v
n
v

se













se













table basic system trained semcor basic set features evaluated
se se

general obtained blc different blc instance
consider number classes within blc classes blc classes supersense classes blc classifiers obtain high performance rates maintaining much higher
expressive power supersenses able classify among much larger number classes
fact supersenses classes nouns verbs obtain accurate semantic tagger performance close even interesting could use blc tagging
nouns semantic classes f around supersenses verbs semantic classes
f around
exploiting semantic features
one main goals prove simple semantic features added training process
capable producing significant improvements basic systems experiments considering different types semantic features presented tables
respectively nouns verbs
tables column labeled class refers called target class
column labeled sf indicates type semantic features included represent examples
within machine learning
values tables correspond f measures harmonic mean recall
precision training systems semcor testing se se test sets
improving baselines appear italics additionally showing statistically significant positive difference compared corresponding baseline mcnemars test
marked bold
regarding nouns see table different behavior observed se se adding
semantic features mainly improves se se none systems present
significant improvement baselines se improvement obtained
several types semantic features particular wnd features se use
semantic class features seems improve systems target classes intermediate
levels abstraction specially blc blc interestingly se blc blc


fiw ord vs c lass w ord ense isambiguation

class

sf
baseline
basicfeat
blc
blc
sumo
ss
wnd
baseline
basicfeat
blc
blc
sumo
ss
wnd
baseline
basicfeat
blc
blc
sumo
ss
wnd

sense

blc

blc

se






















se






















class

sumo

ss

wnd

sf
baseline
basicfeat
blc
blc
sumo
ss
wnd
baseline
basicfeat
blc
blc
sumo
ss
wnd
baseline
basicfeat
blc
blc
sumo
ss
wnd

se






















se






















table nouns extended system
seem provide improvements baselines target classes instance
blc blc ss although significant
regarding verbs see table different behavior observed se se
case observe almost opposite effect nouns se semantic
class features improve obtained baselines se systems
present significant improvement baselines se improvement obtained
several types semantic features however case obtain significantly better
several semantic features se use semantic class features seems
benefit lower levels abstraction specially word sense blc blc sumo
general semantic features addition basic features helps
reach better performance class wsd systems additionally seems
semantic features able obtain competitive classifiers sense level
learning curves
investigate behavior class wsd system respect number training
examples although experiments carried nouns verbs
include nouns since cases trend similar
experiment semcor files randomly selected added training
corpus order generate subsets etc training corpus train
portion contains files previous portion example files portion
contained portion



fii zquierdo u arez r igau

class

sense

blc

blc

sf
baseline
basicfeat
blc
blc
sumo
ss
wnd
baseline
basicfeat
blc
blc
sumo
ss
wnd
baseline
basicfeat
blc
blc
sumo
ss
wnd

se






















se






















class

sumo

ss

wnd

sf
baseline
basicfeat
blc
blc
sumo
ss
wnd
baseline
basicfeat
blc
blc
sumo
ss
wnd
baseline
basicfeat
blc
blc
sumo
ss
wnd

se






















se






















table verbs extended system
system training portions test system se se finally
compare resulting system baseline computed training portion
figures present learning curves se se respectively case
selected blc class wsd system wordnet domains semantic features
surprisingly se system improves f measure around increasing
training corpus semcor se system improves f
measure around increasing training corpus semcor
knowledge required class wsd system seems already present
small part semcor
figures present learning curves se se respectively class
wsd system supersenses semantic features built wordnet domains
se system improves f measure around increasing training corpus
semcor se system improves f measure around
increasing training corpus semcor
whole corpus class wsd system reaches f close performance corpus
se ans se blc figures supersenses figures
semantic classes wsd behavior system similar mfc baseline
interesting since mfc obtains high due way defined mfc
total corpus assigned occurrences word training corpus without
definition would large number words test set occurrences
shown previous experiments combination obtains good performance



fiw ord vs c lass w ord ense isambiguation



system sv
mfc sv








f















































corpus

figure learning curve blc classifier se



system sv
mfc sv







f





























corpus















figure learning curve blc classifier se





fii zquierdo u arez r igau



system sv
mfc sv







f
















































corpus

figure learning curve supersense classifier se



system sv
mfc sv





f



























corpus

















figure learning curve supersense classifier se



fiw ord vs c lass w ord ense isambiguation

small training portions cases recall baselines turn f would much
lower
evaluation seems indicate class wsd reduces considerably
required amount training examples

comparison senseval systems sense level
main goal experiments included section verify whether abstraction level
class systems maintains discriminative power evaluated sense level additionally compare top participant systems se se
provided best senselevel outputs thus class systems adapted following simple protocol output semantic classes converted sense identifiers
instead semantic class produced systems particular instance select first
sense word according wordnet sense ranking belonging predicted semantic class
first obtain semantic class means classifiers obtain restricted set
senses word match semantic class obtained choose frequent
sense restricted subset
first experiment se data shown table systems
prefix svm suffix denotes type semantic class used generate classifier
cases experiments wnd selected target semantic class generate
semantic features two baselines marked italics included first sense wordnet base wordnet frequent sense semcor base semcor fact developers
wordnet ranked word senses semcor sense annotated corpora thus
frequencies ranks appearing semcor wordnet similar equal
include system working word level svm sense
cases nouns verbs systems outperform frequent baselines
frequent sense word according wordnet sense ranking competitive
wsd tasks extremely hard improve upon even slightly mccarthy koeling weeds
carroll expected behavior different semantic features produces slightly
different however independently semantic features used se sense level
class systems rank third position
table shows experiment se dataset case class systems
clearly outperform baselines achieving best nouns second place verbs
interestingly nouns best system se achieve semcor baseline recall
se seems difficult se
worth mention class systems use features nouns
verbs instance take profit complex feature sets encoding syntactic information
seems important verbs
experiments class classifiers seem quite competitive evaluated word sense level perform frequent sense according wordnet
semcor achieve higher position nouns second verbs se third
position nouns verbs se obviously indicates class wsd maintains
high discriminative power word sense level
instance svm blc stands experiment creates classifier considering blc semantic classes



fii zquierdo u arez r igau

class sense se
nouns
verbs
system
f
system
smuam
smuaw
ave antwerp
ave antwerp
svm semblc svm semsumo
svm semblc svm sense
svm semsumo svm semwnd
svm semwnd
svm semblc
svm sense
svm semss
svm semss
svm semblc
base wordnet
lia sinequa
base semcor
base semcor
lia sinequa
base wordnet

f












table class sense se class word sense transformation

class sense se
nouns
system
svm semwnd
svm semblc
svm semsumo
svm sense
svm semblc
svm semss
base semcor
gambl aw
base wordnet
kuaw
untaw
meaning allwords
lccaw

f














verbs
system
gambl aw
svm semsumo
svm semwnd
svm semss
svm sense
svm semblc
svm semblc
untaw
meaning allwords
kuaw
r
base semcor
base wordnet

f














table class sense se class word sense transformation



fiw ord vs c lass w ord ense isambiguation

comparison senseval systems class level
experiments presented section explore performance word classifiers
participating se se evaluated class level perform kind evaluation
word sense output participant systems mapped corresponding semantic
classes class systems modified obviously expect different performances
systems depending semantic class level considering presented tables
order perform comparison selected experiments use wnd
build semantic features thus system different target semantic classes
represented svm semwnd
table presents ordered f measure best performing systems se data
evaluated different levels abstraction previously italics include
frequent senses according wordnet base wordnet semcor base semcor
se independently abstraction level pos system svm semwnd scores
first positions ranking one case system reaches best position twice
second one baselines outperformed experiments except nouns wnd
basesemcor high
table presents ordered f measure best performing systems se data
evaluated different levels abstraction italics include frequent senses
according wordnet base wordnet semcor base semcor systems represented
svm semwnd
se see system performs better baselines cases except
semcorbased baseline nouns obtains high particular system
obtains good verbs reaching first second best positions cases
outperforming baselines cases
sum classbased outperforms senseval participants se
se sense level semantic class level suggests good performance
semantic classifiers due polysemy reduction actually confirms
classbased semantic classifiers learning semantic class training examples different
abstraction levels

domain evaluation
section describe system semeval allwords word sense disambiguation
specific domain task izquierdo suarez rigau aim evaluation
robust semantic class tested specific domain different
domain training material
traditionally senseval competitions focused general domain texts thus domain
specific texts present fresh challenges wsd example specific domains reduce possible meaning word given context moreover distribution word senses data
examples changes compared general domains affect supervised
knowledgebased systems fact supervised word wsd systems sensitive
corpora used training testing system escudero et al
remind semantic features frequent class target word semantic class monosemous words context around target word



fii zquierdo u arez r igau

nouns

verbs
f
system
sense blc
smuaw
smuaw
svm semwnd svm semwnd
ave antwerp
lia sinequa
base semcor
ave antwerp
base wordnet
base semcor
lia sinequa
base wordnet
sense blc
smuaw
smuaw
svm semwnd svm semwnd
ave antwerp
lia sinequa
base semcor
ave antwerp
base wordnet
base semcor
lia sinequa
base wordnet
sense sumo
smuaw
smuaw
svm semwnd lia sinequa
base semcor
ave antwerp
ave antwerp
svm semwnd
lia sinequa
base semcor
base wordnet
base wordnet
sense supersense
svm semwnd smuaw
smuaw
lia sinequa
ave antwerp
svm semwnd
base semcor
ave antwerp
lia sinequa
base wordnet
base wordnet

base semcor
sense wnd
smuaw
smuaw
base semcor
svm semwnd
svm semwnd base semcor
ave antwerp
lia sinequa
base wordnet
base wordnet
lia sinequa
ave antwerp
system

f































table sense blc blc sumo supersense wnd semantic classes
se



fiw ord vs c lass w ord ense isambiguation

nouns

verbs
f
system
sense blc
base semcor
gambl aw
gambl aw
svm semwnd
kuaw
kuaw
lccaw
r
untaw
untaw
svm semwnd
meaning allwords
base wordnet

base semcor
meaning allwords base wordnet
sense blc
base semcor
gambl aw
gambl aw
svm semwnd
kuaw
kuaw
svm semwnd
r
lccaw
untaw
untaw
meaning allwords
base wordnet
base semcor
r
base wordnet
sense sumo
base semcor
gambl aw
kuaw
svm semwnd
lccaw
untaw
svm semwnd
kuaw
untaw
meaning allwords
gambl aw
upv eaw
base wordnet

base semcor
meaning allwords base wordnet
sense supersense
base semcor
svm semwnd
kuaw
gambl aw
svm semwnd
base semcor
untaw
base wordnet
gambl aw
meaning allwords
upv eaw
meaning simple
upv eaw
kuaw
base wordnet
upv eaw
sense wnd
base semcor
svm semwnd
svm semwnd
base semcor
untaw
untaw
kuaw
gambl aw
gambl aw
base wordnet
base wordnet

r
lccaw
meaning simple
meaning allwords kuaw
system

f









































table sense blc blc sumo supersense wnd semantic classes
se



fii zquierdo u arez r igau

therefore main challenge develop specific domain wsd systems adapt
general system particular domain following line task proposed within
semeval competition allwords word sense disambiguation specific domain agirre
et al restricted domain selected task environmental domain test
corpora consist three texts compiled european center nature conservation ecnc
world wildlife forum wwf task proposed several languages chinese dutch
english italian although participation limited english detail
total noun tokens verb tokens tagged moreover set background
documents related environmental domain provided texts sense tagged
plain text provided ecnc wwf could used
systems help adaptation specific domain english total
background documents containing words
apply kind specific domain adaptation technique supervised classbased
system order adapt supervised system environmental domain increase automatically training data training examples domain acquire examples
use background documents environmental domain provided organizers
use treetagger schmid preprocess documents performing postagging lemmatization since background documents semantically annotated supervised system
needs labeled data selected monosemous instances occurring documents
according blc semantic classes note exploited classbased wsd systems way obtained automatically large set examples annotated
blc semantic class selected provided good previous
experiments order analyze system would work level
abstraction performed evaluation posteriori blc wordnet domains
supersenses besides blc official participation semeval nevertheless
section focused blc
regarding blc table presents total number training examples extracted semcor sc background documents bg expected method large number
monosemous examples obtained nouns verbs although verbs much less productive nouns however background examples correspond reduced set
monosemous words
sc
bg
total

nouns




verbs




n v




table number training examples blc
table lists ten frequent monosemous nouns verbs occurring background
documents remember examples monosemous according blc semantic
classes
http www ecnc org
http wwf org
blc see section stands basic level concepts obtained relations criterion minimum threshold
subconcepts subsumed equal



fiw ord vs c lass w ord ense isambiguation












nouns
lemma
biodiversity
habitat
specie
climate
european
ecosystem
river
grassland
datum
directive

ex











verbs
lemma ex
monitor

achieve

target

select

enable

seem

pine

evaluate
explore

believe


table frequent monosemous words background documents
sc
bg
total

nouns




verbs




n v




table number training examples word senses
applies semantic class architecture shown previous sections
examples extracted background documents case semantic class used
extract examples generate classifiers blc select simple feature set widely
used many wsd systems particular use window five tokens around target word
extract word forms lemmas bigrams trigrams word forms lemmas trigrams pos
tags frequent blc semantic class target word training corpus
analyze contribution monosemous examples performance system three
experiments defined
blc sc training examples extracted semcor
blc bg monosemous examples extracted background data
blc scbg training examples extracted semcor monosemous background data
first run blc sc aims behavior supervised system trained general
corpus tested specific domain second one blc bg analyzes contribution
monosemous examples extracted background data finally third run blc scbg studies robustness combining training examples semcor
automatic ones obtained background documents
table summarizes ordered recall official participants english
wsd domain specific task semeval table type refers followed
corresponding system weakly supervised ws supervised kb knowledge
unsupervised participate system blc semantic class blc
sc bg scbg runs wordbased classifiers labeled sensebg sense sc sensescbg
case use set blcs wordnet version wn one used
annotation



fii zquierdo u arez r igau

included evaluation campaign finally mentioned introduction
included performance itmakessense system one best performing wsd systems task comparison purposes row table called
itmakessense italics
rank
















system id
cfilt
cfilt
iiith ppr
iiith ppr
blc scbg
itmakessense
blc sc
frequent sense
cfilt
treematch
treematch
sensescbg
sensesc

blc bg

random baseline
sensebg

type
ws
ws
ws
ws



kb
kb
kb







p



















r



















table precision recall semeval participants itmakessense included
comparison purpose
general reported semeval task quite low best system
achieved precision frequent baseline reached precision
fact shows domain adaptation wsd systems difficult task
analyzing three runs semeval worst obtained system
monosemous background examples blc bg system ranks rd
precision recall nouns verbs system semcor
blc sc ranks th precision recall nouns verbs
performance first sense baseline expected best three
runs obtained combining examples semcor background blc scbg
supervised system obtains th position precision recall
nouns verbs slightly baseline actually version system
obtains slightly better best performing supervised system itmakessense note
could include automatically monosemous examples background test thanks
class nature wsd system
moreover system one completely supervised participating task organizers calculated recall confidence interval bootstrap sampling procedure
noreen method estimation might strict pairwise methods
reveals differences four first systems system blc scbg
table appears th position due included wordbased classifier



fiw ord vs c lass w ord ense isambiguation

statistically significant seen figure overlapping recall confidence interval four first systems system ranking th proves
differences statistically significant

figure recall confidence intervals
possibly reason low performance bcl bg system high correlation features target word semantic class case features correspond
monosemous word later evaluated polysemous words kind features however seems class systems robust enough incorporate large sets
monosemous examples domain text fact knowledge first time
supervised wsd successfully adapted specific domain furthermore
system trained semcor achieves good performance reaching frequent
baseline showing robustness class wsd approaches domain variations
comparing wordbased classifiers seems blc classes contribute two main
aspects first set features classbased classifiers obtain better
wordbased ones classifiers built blc robust domain adaptable
wordbased approaches second experiment uses examples extracted background data considering word senses sense bg obtain accuracy close zero
experiment blc semantic classes blc bg reaches accuracy
fact indicates blcs useful extract good training examples unlabeled data
mentioned previously order obtain better insight evaluation campaign performed
evaluation system semantic classes represent different levels
abstractions blc wordnet domains supersenses table shows precision p
recall r evaluation considering different training datasets semcor background
documents semcor background documents sc bg sc bg respectively
different semantic classes
seen table blc leads better performance three different
corpora training bg sc scbg training monosemous examples extracted
background documents blc obtains best may indicate level
abstraction adequate including wnd ss sets much smaller
much lower polysemy effect drawn training
semcor monosemous examples background scbg best
obtained blc together supersenses two semantic classes seem
figure taken directly overview task
figures obtained official scorer script official gold key without modification



fii zquierdo u arez r igau

system id
blc scbg
itmakessense
blc sc
frequent sense
wndsc
sensescbg
sensesc
ss scbg
blc scbg
blc sc
sssc
wnscbg
blc bg
wndbg
ssbg
blc bg
random baseline

type


















p


















r


















table experiments according different semantic classes
benefit background monosemous examples seem confirm potential
capabilities blc provide adequate level abstraction perform class wsd
finally proved system performs level one state art sys
tem itmakessense system zhong ng considering set features
system quite simple apply machine learning optimization feature
engineering use semantic classes provides robust behavior
specific domains reaching state art

concluding remarks
word sense disambiguation difficult task empirically demonstrated senseval semeval exercises one reason difficulties could use inappropriate sets
word meanings wordnet de facto standard repository meanings several attempts
made grouping senses order achieve higher levels accuracy moreover
tries ease hard task creating large enough sets annotated data per domain
language train supervised systems possible solution would use manual annotation semantic class labels instead fine grained word senses schneider mohit oflazer smith
schneider mohit dyer oflazer smith
several attempts made obtain word sense groupings alleviate
fine granularity word senses widely wordnet senses cases
consists grouping different senses word resulting decrease polysemy
reducing discriminative capacity works use predefined sets semantic classes
integrated directly wsd system mainly supersenses
tested offline itmakessense system participate task downloaded last
version software http www comp nus edu sg nlp software html



fiw ord vs c lass w ord ense isambiguation

work describe simple method automatically select basic level concepts
wordnet simple structural properties wordnet method automatically selects
different sets blc representing different levels abstraction
aim work explore several allwords wsd tasks performance different
levels abstraction provided basic level concepts wordnet domains sumo supersense
labels furthermore study empirically demonstrates
word sense groupings cluster senses coherent level abstraction order
perform supervised classbased wsd harming performance
b semantic classes successfully used semantic features boost performance
classifiers
c classbased wsd reduces dramatically required amount training examples obtain competitive classifiers
classbased obtains competitive performances compared word systems
e classbased outperforms wordbased systems evaluated class level
f robustness class wsd system performing domain evaluation
g system reaches comparable state art system itmakessense
tested specific domain
general classbased disambiguation nouns verbs achieves better
wordbased systems presented senseval senseval showed classbased reduces considerably required amount training examples order prove
type disambiguation possible accurate ranked class systems
together senseval senseval official order establish fair comparison
mapped necessary word senses semantic classes viceversa
experiments designed use classbased classifiers perform wordsense
disambiguation shown simple selecting first sense wordnet corresponds class selected classifiers performs well top systems
senseval senseval
additional experiments carried compare wordbased systems perform
classbased disambiguation case translated official system outputs corresponding semantic classes
different experiments performed different levels abstraction ranging
supersenses small set sumo labels linked wordnet senses
wordnet domains labels basic level concepts arbitrary number classes
depending abstraction level selected
expected differences senseval senseval class
systems outperform baselines nouns verbs specially nouns class
systems outperforms senseval senseval systems general obtained
svm semblc different svm semblc thus select


fii zquierdo u arez r igau

medium level abstraction without significant decrease performance considering number classes blc classifiers obtain high performance rates maintaining much
higher expressiveness supersenses however supersenses classes obtain
accurate semantic tagger performances around even better use blc
tagging nouns semantic classes f supersenses verbs semantic
classes f around
systems semeval words word sense disambiguation specific domain task
proved simple features exploiting blc perform well sophisticated methods
comparing wordbased classifiers see blc classes contribute two main
aspects classbased classifiers obtain better wordbased ones semantic classes
contribute effectively fact indicates particular blc useful
extract monosemous training examples unlabeled domain data
next goal exploit inconsistencies different labeling provided different
class classifiers order obtain robust accurate class wsd system
main idea study several classifiers one different degree abstraction e g
blc blc wordnet domains etc label concrete context example incompatible
tags manner would able predict apply best classifier depending
context

acknowledgements
work partially supported newsreader project ict spanish project skater tin c

references
agirre e de lacalle l clustering wordnet word senses proceedings
ranlp borovets bulgaria
agirre e edmonds p word sense disambiguation applications
springer
agirre e lopez de lacalle fellbaum c hsieh k tesconi monachini vossen
p segers r semeval task words word sense disambiguation
specific domain proceedings th international workshop semantic evaluation
pp uppsala sweden association computational linguistics
bhagwani satapathy karnick h merging word senses proceedings workshop graph methods natural language processing textgraphs pp
castillo real f rigau g automatic assignment domain labels wordnet
proceeding nd international wordnet conference pp
ciaramita altun broad coverage sense disambiguation information extraction supersense sequence tagger proceedings conference empirical methods natural language processing emnlp pp sydney australia acl
http www newsreader project eu
http nlp lsi upc edu skater



fiw ord vs c lass w ord ense isambiguation

ciaramita johnson supersense tagging unknown nouns wordnet
proceedings conference empirical methods natural language processing
emnlp pp acl
curran j supersense tagging unknown nouns semantic similarity proceedings
rd annual meeting association computational linguistics acl pp
acl
escudero g marquez l rigau g empirical study domain dependence
supervised word sense disambiguation systems proceedings joint sigdat
conference empirical methods natural language processing large corpora
emnlp vlc hong kong china
fellbaum c ed wordnet electronic lexical database mit press
gangemi nuzzolese g presutti v draicchio f musetti ciancarini p
automatic typing dbpedia entities proceedings th international conference
semantic web part iswc pp berlin heidelberg springer verlag
gonzalez rigau g castillo graph method improve wordnet domains
computational linguistics intelligent text processing pp springer
hamp b feldweg h et al germanet lexical semantic net german proceedings
acl workshop automatic information extraction building lexical semantic resources
nlp applications pp citeseer
hearst schutze h customizing lexicon better suit computational task
proceedingns acl siglex workshop lexical acquisition stuttgart germany
hovy e marcus palmer ramshaw l weischedel r ontonotes
proceedings human language technology conference naacl companion
short papers naacl short pp stroudsburg pa usa association
computational linguistics
izquierdo r suarez rigau g exploring automatic selection basic level concepts et al g ed international conference recent advances natural language
processing pp borovets bulgaria
izquierdo r suarez rigau g empirical study class word sense disambiguation proceedings th conference european chapter association
computational linguistics eacl pp stroudsburg pa usa association
computational linguistics
izquierdo r suarez rigau g gplsi ixa semantic classes acquire monosemous training examples domain texts proceedings th international workshop
semantic evaluation pp association computational linguistics
joachims text categorization support vector machines learning many relevant
features nedellec c rouveirol c eds proceedings ecml th european
conference machine learning pp chemnitz de springer verlag
heidelberg de
l bentivogli p forner b pianta e revising wordnet domains hierarchy semantics coverage balancing coling workshop multilingual linguistic
resources geneva switzerland


fii zquierdo u arez r igau

magnini b cavaglia g integrating subject field codes wordnet proceedings
lrec athens greece
marquez l escudero g martnez rigau g supervised corpus methods
wsd e agirre p edmonds eds word sense disambiguation
applications vol text speech language technology springer
mccarthy koeling r weeds j carroll j finding predominant word senses
untagged text nd annual meeting association computational linguistics
barcelona spain
mihalcea r wikipedia automatic word sense disambiguation proceedings
naacl hlt
mihalcea r csomai ciaramita unt yahoo supersenselearner combining
senselearner supersense coarse semantic features proceedings th
international workshop semantic evaluations semeval pp stroudsburg
pa usa association computational linguistics
mihalcea r moldovan automatic generation coarse grained wordnet proceding naacl workshop wordnet lexical resources applications extensions customizations pittsburg usa
miller g leacock c tengi r bunker r semantic concordance proceedings
arpa workshop human language technology
navigli r meaningful clustering senses helps boost word sense disambiguation performance acl proceedings st international conference computational
linguistics th annual meeting association computational linguistics
pp morristown nj usa association computational linguistics
navigli r word sense disambiguation survey acm computing surveys
navigli r litkowski k hargraves semeval task coarse grained english
words task proceedings fourth international workshop semantic evaluations semeval pp prague czech republic association computational
linguistics
niles pease towards standard upper ontology proceedings nd
international conference formal ontology information systems fois pp
chris welty barry smith eds
niles pease linking lexicons ontologies mapping wordnet suggested
upper merged ontology arabnia h r ed proc ieee int conf inf
knowledge engin ike vol pp csrea press
noreen e computer intensive methods testing hypotheses introduction wiley
interscience publication wiley
paa g reichartz f exploiting semantic constraints estimating supersenses
crfs sdm pp siam
paa g reichartz f b exploiting semantic constraints estimating supersenses
crfs sdm pp siam


fiw ord vs c lass w ord ense isambiguation

palmer fellbaum c cotton delfs l dang h english tasks words
verb lexical sample proceedings senseval workshop conjunction
acl eacl toulouse france
peters w peters vossen p automatic sense clustering eurowordnet first international conference language resources evaluation lrec granada spain
picca gliozzo ciaramita supersense tagger italian lrec citeseer
pradhan dligach e l palmer semeval task english lexical sample
srl words semeval proceedings th international workshop semantic
evaluations pp morristown nj usa association computational linguistics
rosch e human categorisation studies cross cultural psychology
schmid h probabilistic part speech tagging decision trees proceedings
international conference methods language processing pp
schneider n mohit b dyer c oflazer k smith n supersense tagging
arabic mt middle attack hlt naacl pp citeseer
schneider n mohit b oflazer k smith n coarse lexical semantic annotation
supersenses arabic case study proceedings th annual meeting
association computational linguistics short papers pp association
computational linguistics
segond f schiller greffenstette g chanod j experiment semantic tagging
hidden markov model tagging acl workshop automatic information extraction
building lexical semantic resources nlp applications pp acl
brunswick jersey
snow r p j n learning merge word senses proceedings joint
conference empirical methods natural language processing computational natural language learning emnlp conll pp
snyder b palmer english words task mihalcea r edmonds p
eds senseval third international workshop evaluation systems semantic analysis text pp barcelona spain association computational linguistics
tsvetkov schneider n hovy bhatia faruqui dyer c augmenting
english adjective senses supersenses proc lrec pp
villarejo l marquez l rigau g exploring construction semantic class classifiers wsd proceedings th annual meeting sociedad espaola para el
procesamiento del lenguaje natural sepln pp granada spain issn
vossen p ed eurowordnet multilingual database lexical semantic networks
kluwer academic publishers
wikipedia wikipedia free encyclopedia https en wikipedia org online
accessed august
yarowsky decision lists lexical ambiguity resolution application accent restoration spanish french proceedings nd annual meeting association
computational linguistics acl


fii zquierdo u arez r igau

zhong z ng h makes sense wide coverage word sense disambiguation system
free text proceedings acl system demonstrations acldemos pp
stroudsburg pa usa association computational linguistics




