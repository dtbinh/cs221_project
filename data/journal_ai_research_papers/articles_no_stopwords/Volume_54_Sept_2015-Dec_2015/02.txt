Journal Artificial Intelligence Research 54 (2015) 83-122

Submitted 02/15; published 09/15

Word vs. Class-Based Word Sense Disambiguation
Ruben Izquierdo

RUBEN . IZQUIERDOBEVIA @ VU . NL

VU University Amsterdam
Amsterdam. Netherlands

Armando Suarez

ARMANDO @ DLSI . UA . ES

University Alicante
Alicante. Spain

German Rigau

GERMAN . RIGAU @ EHU . ES

University Basque Country
San Sebastian. Spain

Abstract
empirically demonstrated Word Sense Disambiguation (WSD) tasks last SensEval/SemEval exercises, assigning appropriate meaning words context resisted
attempts successfully addressed. Many authors argue one possible reason could
use inappropriate sets word meanings. particular, WordNet used de-facto
standard repository word meanings tasks. Thus, instead using word
senses defined WordNet, approaches derived semantic classes representing groups
word senses. However, meanings represented WordNet used WSD
fine-grained sense level coarse-grained semantic class level (also called SuperSenses). suspect appropriate level abstraction could levels.
contributions paper manifold. First, propose simple method automatically
derive semantic classes intermediate levels abstraction covering nominal verbal WordNet meanings. Second, empirically demonstrate automatically derived semantic classes
outperform classical approaches based word senses coarse-grained sense groupings.
Third, demonstrate supervised WSD system benefits using new semantic classes additional semantic features reducing amount training examples.
Finally, demonstrate robustness supervised semantic class-based WSD system
tested domain corpus.

1. Introduction
Word Sense Disambiguation (WSD) intermediate Natural Language Processing (NLP) task
consists assigning correct lexical interpretation ambiguous words depending surrounding context (Agirre & Edmonds, 2007; Navigli, 2009). One successful approaches
last years supervised learning examples, Machine Learning classification
models induced semantically annotated corpora (Marquez, Escudero, Martnez, & Rigau,
2006). Quite often, machine learning systems obtained better results knowledge-based
ones, shown experimental work international evaluation exercises Senseval SemEval1 . Nevertheless, lately weakly supervised knowledgebased approaches reaching
performance close supervised techniques specific tasks. tasks,
1. information competitions found http://www.senseval.org.
c
2015
AI Access Foundation. rights reserved.

fiI ZQUIERDO , U AREZ & R IGAU

corpora usually manually annotated experts word senses taken particular lexical
semantic resource, commonly WordNet (Fellbaum, 1998).
However, WordNet widely criticized sense repository often provides
finegrained sense distinctions higher level applications Machine Translation (MT)
Question & Answering (AQ). fact, WSD low level semantic granularity resisted
attempts inferring robust broad-coverage models. seems many wordsense distinctions
subtle captured automatic systems current small volumes wordsense
annotated examples. Using WordNet sense repository, organizers English all-words
task SensEval-3 reported inter-annotation agreement 72.5% (Snyder & Palmer, 2004). Interestingly, result difficult outperform state-of-the-art sense-based WSD systems.
Moreover, supervised sensebased approaches biased towards frequent sense
predominant sense training data. Therefore, performance supervised sensebased
systems strongly punished applied domain specific texts sense distribution differs considerably respect sense distribution training corpora (Escudero, Marquez,
& Rigau., 2000).
paper try overcome problems facing task WSD Semantic
Class point view instead traditional word sense based approach. semantic class
seen abstract concept groups subconcepts word senses sharing semantic properties features. Examples semantic classes VEHICLE, FOOD ANIMAL. hypothesis
using appropriate set semantic classes instead word-senses could help WSD several
aspects:
higher level abstraction could ease integration WSD systems higher
level NLP applications Machine Translation Question & Answering
Grouping together semantically coherent sets training examples could increase
robustness supervised WSD systems
socalled bottleneck acquisition problem could alleviated
points explained along paper. Following hypothesis propose
create classifiers based semantic classes instead word sense experts. One semantic classifier
trained semantic class final system assign proper semantic class
ambiguous word (instead sense traditional approaches). example, using
automatically derived semantic classes (that introduced later), three senses church
WordNet 1.6 subsumed semantic classes R ELIGIOUS RGANIZATION, B UILDING
R ELIGIOUS C EREMONY. note semantic classes still discriminate among three
different senses word church. instance, assign semantic class B UILDING
occurrence church context, still know refers second sense. Additionally,
semantic class B UILDING covers six times training examples
covered second sense church.
example text senseval2 automatically annotated semantic classes seen
Figure 1. shows automatic annotations classbased classifiers different semantic classes. BLC stands Basic Level Concepts2 (Izquierdo, Suarez, & Rigau, 2007), SS
2. use following format throughout paper refer particular sense: wordnum
pos , pos
part-of-speech: n nouns, v verbs, adjectives r adverbs, num stands sense number.

84

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

SuperSenses (Ciaramita & Johnson, 2003), WND WordNet Domains (Magnini & Cavaglia,
2000; L. Bentivogli & Pianta, 2004) SUMO Suggested Upper Merged Ontology (Niles &
Pease, 2001). Incorrect assignments marked italics. correct tags included
brackets next automatic ones. Obviously, semantic resources relate senses different
level abstraction using diverse semantic criteria properties could interest subsequent semantic processing. Moreover, combination could improve overall results since
offer different semantic perspectives text.
Id
1
2
3
4
6
7
8

Word

ancient
stone
church
amid

fields

BLC

SS

WND

SUMO

artifact1n
building1n

noun.artifact
noun.artifact

building
building

Mineral
Building

geographic area1n
[physical object1n ]

noun.location
[noun.object]

factotum [geography]

LandArea

9
10
11

,

sound

property2n

noun.attribute

factotum [acoustics]

RadiatingSound
[SoundAttribute]

12
13


bells

device1n

noun.artifact

MusicalInstrument

14
15
16
17
18

cascading


tower
calling

move2v

verb.motion

factotum [acoustics]
factotum

construction3n
designate2v
[request2v ]

noun.artifact
factotum
verb.stative
factotum
[verb.communication]

Building
Communication
[Requesting]

19
20


faithful

[sogroup1n
cial group1n ]

noun.group

person [religion]

Group

21
22


evensong

time day1n
[writing2n ]

noun.communication

religion

TimeInterval
[Text]

Motion

Table 1: Example automatic annotation text several semantic class labels
main goal research investigate performance alternative Semantic Classes
derived WordNet supervised WSD. First, propose system automatically extract sets
semantically coherent groupings nominal verbal senses WordNet. system
allows generate arbitrary sets semantic classes distinct levels abstraction. Second,
analyze impact respect alternative Semantic Classes performing classbased
WSD. empirical results show automatically generated classes performs better
created manually (WNDomains, SUMO, SuperSenses, etc.) capturing precise
information. Third, demonstrate supervised WSD system benefits using
new semantic classes additional semantic features reducing amount training
85

fiI ZQUIERDO , U AREZ & R IGAU

examples. Finally, show supervised class-based system adapted particular
domain. Traditional word sense based systems included comparison purposes.
Summarizing, research empirically investigates:
performance alternative semantic groupings used supervised class-based
WSD system
impact class-based semantic features supervised WSD framework
required amount training examples needed class-based WSD order obtain
competitive results
relative performance class-based WSD systems respect WSD based word
experts
robustness class-based WSD system specific domains
Moreover, tested domain dataset, supervised class-based WSD system obtains slightly better results state-of-the-art word sense based WSD system, ItMakesSense
system presented Zhong Ng (2010).
introduction, present work directly related research supervised
WSD based semantic classes. Then, Section 3 presents sense-groupings semantic classes
used study. Section 4 explains method automatically derive semantic classes
WordNet different levels abstraction. Moreover analysis different semantic groupings
included. Section 5, presents system developed perform supervised class-based
WSD. performance system shown Section 6, system tested several
WSD datasets provided international evaluations. comparison participants
competitions introduced sections 7 8. experiments system applied
specific domain analyzed Section 9. Finally, conclusions future work presented
section 10.

2. Related Work
field WSD broad. large amount publications WSD
last 50 years. section revises relevant WSD approaches dealing appropriate
sets meanings word have.
research focused deriving different word-sense groupings overcome
finegrained distinctions WordNet (Hearst & Schutze, 1993; Peters, Peters, & Vossen, 1998;
Mihalcea & Moldovan, 2001; Agirre & de Lacalle, 2003; Navigli, 2006; Snow, S., D., & A., 2007).
is, provide methods grouping senses word, thus producing coarser word
sense groupings. example, word church three senses WordNet 1.6, sense
grouping presented Snow et al. (2007) produces unique grouping. is, according
approach church monosemous.
OntoNotes project (Hovy, Marcus, Palmer, Ramshaw, & Weischedel, 2006), different
meanings word considered kind tree, ranging coarse concepts root
finegrained meanings leaves. merging increased fine coarse grained
obtaining inter annotator agreement around 90%. coarse-grained repository used
86

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

WSD lexical sample task SemEval-2007 (Pradhan, Dligach, & Palmer, 2007),
systems scored 88.7% Fscore. Note merging created word following
manual costly process.
Similarly previous approach, another task organized within SemEval-2007
consisted traditional WSD word task using another coarsegrained sense repository derived
WordNet (Navigli, Litkowski, & Hargraves, 2007). case WordNet synsets
automatically linked Oxford Dictionary English (ODE) using graph algorithm.
meanings word linked ODE entry merged coarse sense. systems
achieving top scores followed supervised approaches taking advantage different corpora
training, reaching top Fscore 82.50%.
previous cases aimed solving granularity problem word sense
definitions WordNet. However, approaches still word experts (one classifier trained
word). Obviously, decreasing average polysemy word using coarsersenses
makes easier classification choice. result, performance systems increase
cost reducing discriminative power.
Conversely, instead word experts, approach creates semantic class experts.
semantic classifiers exploit diverse information extracted meanings different
words belong class.
Wikipedia (Wikipedia, 2015) recently used overcome problems supervised learning methods: excessively finegrained definition meanings, lack annotated data
strong domain dependence existing annotated corpora. way, Wikipedia provides
new source annotated data, large constantly expansion (Mihalcea, 2007; Gangemi,
Nuzzolese, Presutti, Draicchio, Musetti, & Ciancarini, 2012).
contrast, research focused using predefined sets sense-groupings
learning classbased classifiers WSD (Segond, Schiller, Greffenstette, & Chanod, 1997; Ciaramita & Johnson, 2003; Villarejo, Marquez, & Rigau, 2005; Curran, 2005; Ciaramita & Altun,
2006; Izquierdo, Suarez, & Rigau, 2009). is, grouping senses different words
explicit comprehensive semantic class. work presented Mihalcea, Csomai,
Ciaramita (2007) makes use three different sets semantic classes (WordNet classes two
Named Entity annotated corpora) train sequential classifiers. classifiers trained using
basic features, collocations semantic features, reach performance around 60%
14th position SemEval-2007 allwords task.
semantic classes WordNet (also called SuperSenses) widely used different
works. instance, Paa Reichartz (2009a) apply Conditional Random Fields model
sequential context words relation SuperSenses. extend model include
potential SuperSenses word training data. F1 score 82.8% reported (both
nouns verbs) potential labels used (no training data all) 1% worse
using training data right labels. Although interesting, evaluate
system applying 5-fold cross validation SemCor.

3. Semantic Classes Levels Abstraction
meanings represented WordNet used WSD fine-grained sense
level coarse-grained semantic class level (also called SuperSenses). suspect
appropriate level abstraction could found levels. section propose
87

fiI ZQUIERDO , U AREZ & R IGAU

simple method automatically derive semantic classes intermediate levels abstraction covering nominal verbal WordNet meanings. First, introduce WordNet, semantic resource
sense repository used WSD systems. note semantic classes used
work linked WordNet.
WordNet (Fellbaum, 1998) online lexical database English contains concepts
represented synsets, sets synonyms content words (nouns, verbs, adjectives
adverbs). One synset groups together several senses different words synonyms.
WordNet different types lexical semantic relations interlink different synsets, creating
way large structured lexical semantic network. important relation
encoded WordNet subclass relation (for nouns called hyponymy relation verbs
troponymy relation). Table 2 shows basic figures different WordNet versions including
total number words, polysemous words, synsets, senses (all possible senses words)
average polysemy.
Version
WN 1.6
WN 1.7
WN 1.7.1
WN 2.0
WN 2.1
WN 3.0

Words
121,962
144,684
146,350
152,059
155,327
155,287

Polysemous
23,255
24,735
25,944
26,275
27,006
26,896

Synsets
99,642
109,377
111,223
115,424
117,597
120,982

Senses
173,941
192,460
195,817
203,145
207,016
206,941

Avg. Polysemy
2.91
2.93
2.86
2.94
2.89
2.89

Table 2: Statistics WordNet versions.

3.1 SuperSenses
SuperSenses name WordNet Lexicographer Files within framework WSD3 .
detail, WordNet synsets organized forty five SuperSenses, based syntactic categories
(nouns, verbs, adjectives adverbs) logical groupings PERSON, PHENOMENON,
FEELING , LOCATION , etc. 26 basic categories nouns, 15 verbs, 3 adjectives
1 adverbs. cases, different senses word grouped high level
SuperSense, reducing polysemy word. often case similar
senses word. classes adjectives adverbs, SuperSense taggers
usually developed nouns verbs. (Tsvetkov, Schneider, Hovy, Bhatia, Faruqui, &
Dyer, 2014) presents interesting study tagging adjectives SuperSenses acquired
GermaNet (Hamp, Feldweg, et al., 1997).
3.2 WordNet Domains
WordNet Domains4 (WND) (Magnini & Cavaglia, 2000; L. Bentivogli & Pianta, 2004) hierarchy 165 domains used label semi-automatically WordNet synsets.
set labels organized taxonomy following Dewey Decimal Classification System5 .
3. information SuperSenses found http://wordnet.princeton.edu/wordnet/
man/lexnames.5WN.html.
4. http://wndomains.itc.it
5. http://www.oclc.org/dewey

88

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

building WND, many labels assigned high levels WordNet hierarchy
automatically inherited across hypernym troponym hierarchy. Thus, semi-automatic
method6 used develop resource free errors inconsistencies (Castillo, Real, &
Rigau, 2004; Gonzalez, Rigau, & Castillo, 2012).
Information brought domain labels complementary already WordNet. WND
present characteristics interesting WSD. First all, domain label may contain
senses different WordNet subhierarchies (derived different SuperSenses). instance,
domain RELIGION contains senses priest, deriving NOUN . PERSON church,
deriving NOUN . ARTIFACT. Second, domain label may include synsets different
syntactic categories. instance, domain RELIGION contains verb pray adjective
holy.
Furthermore, single WND label subsume different senses word, reducing
way polysemy. instance, first third senses church WordNet 1.6
domain label RELIGION.
3.3 SUMO Concepts
SUMO7 (Niles & Pease, 2001) created part IEEE Standard Upper Ontology Working
Group. goal develop standard upper ontology promote data interoperability, information search retrieval, automated inference, natural language processing. UMO consists
set concepts, relations, axioms formalize upper ontology. experiments,
used complete WordNet 1.6 mapping 1,019 UMO labels (Niles & Pease, 2003).
case, three noun senses church WordNet 1.6 classified R ELIGIOUS RGANIZATION,
B UILDING R ELIGIOUS C EREMONY according SUMO ontology.
3.4 Example Semantic Classes
example, table 3 presents three senses glosses word church WordNet 1.6.
Sense
1

2
3

WordNet 1.6
gloss
1
Christian churchn group Christians; group professing
Christian doctrine belief: church biblical term assembly
church2n church building1n
public (especially Christian) worship:
church empty
church service1n church3n
service conducted church: dont late
church
word senses
church1n
Christianity2n

Table 3: Glosses examples senses churchn
Table 4 show classes assigned sense according semantic resources introduced previously. instance, considering WordNet Domains, observed senses
number 1 (group Christians) 3 (service conducted church) belong domain
6. based several cycles manual checking automatically labeled data.
7. http://www.ontologyportal.org

89

fiI ZQUIERDO , U AREZ & R IGAU

RELIGION . contrary, SuperSenses SUMO represent three senses church using
different semantic classes. note resulting assignment semantic classes identifies
word sense individually.

Sense
1
2
3

SuperSense
NOUN . GROUP
NOUN . ARTIFACT
NOUN . ACT

Semantic Class
WND
SUMO
R ELIGION R ELIGIOUS RGANIZATION
B UILDINGS
B UILDING
R ELIGION
R ELIGIOUS C EREMONY

Table 4: Semantic Classes noun churchn
3.5 Levels Abstraction
Basic Level Concepts (Rosch, 1977) (hereinafter BLC) result compromise two
conflicting principles characterization (general vs. specific):
Represent many concepts possible
Represent many features possible
result conflicting characterization, BLC typically occur middle levels
semantic hierarchies.
notion Base Concepts (hereinafter BC) introduced EuroWordNet (Vossen, 1998).
BC supposed important concepts several language specific wordnets.
importance measured terms two main criteria:
high position semantic hierarchy
many relations concepts
EuroWordNet set 1,024 concepts selected called Common Base Concepts.
Common BC concepts act BC least two languages. local wordnets English,
Dutch Spanish used select set Common BC. later initiatives, similar sets
derived harmonize construction multilingual wordnets.
Considering definitions, next section present method automatically generate
different sets Basic Level Concepts WordNet different levels abstraction.

4. Automatic Selection Basic Level Concepts
Several approaches developed trying alleviate fine granularity problem WordNet
senses obtaining word sense groupings (Hearst & Schutze, 1993; Peters et al., 1998; Mihalcea
& Moldovan, 2001; Agirre & de Lacalle, 2003; Navigli, 2006; Snow et al., 2007; Bhagwani, Satapathy, & Karnick, 2013). cases approach consists grouping different senses
word, resulting decrease polysemy. Obviously, polysemy reduced
WSD task classification problem becomes easier, system using coarse senses
obtain better results systems using word senses. works used predefined sets
semantic classes, mainly SuperSenses (Segond et al., 1997; Ciaramita & Johnson, 2003; Curran,
90

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

2005; Villarejo et al., 2005; Ciaramita & Altun, 2006; Picca, Gliozzo, & Ciaramita, 2008; Paa &
Reichartz, 2009b; Tsvetkov et al., 2014).
section, describe simple method automatically create different sets Basic Level
Concepts WordNet. method exploits nominal verbal structure WordNet.
basic idea synsets WordNet high number relations important, could
candidates BLC. capture relevance synset WordNet considered two
options:
1. All: total number relations encoded WordNet synset
2. Hypo: total number hyponymy relations synset
method follows bottomup approach exploiting hypernymy chains WordNet.
synset, process starts visiting synsets hyperonymy chain selecting (and
stopping walk synset) BLC ancestor first local maximum considering
total number relations (either Hypo)8 . synsets one hyperonym,
method chooses one higher number relations continue process. process
ends preliminary set candidate synsets selected potential BLC.
Additionally, synset selected potential BLC candidate must subsume (or represent)
least certain number descendant synsets. Thus, minimum number synsets BLC must
subsume another parameter algorithm, represented symbol . Candidate
BLCs reach threshold discarded, subsumed synsets reassigned
BLC candidate appearing higher levels abstraction.
Algorithm 1 presents pseudocode algorithm. parameters algorithm are:
WordNet resource, type relations considered (All Hypo), minimum number
concepts must subsumed BLC (). algorithm two phases. first
one selects candidate BLC, following bottomup approach. second phase discards
candidate BLC satisfy threshold.
Figure 1 shows schema illustrate selection process. node represents synset,
edges represent hyperonymy relations (for instance, hyperonym D,
hyperonym F). number synset indicates number hyponymy relations.
schema illustrates selection process BLC candidates synset J using criterion Hypo.
process starts checking hyperonym J, F. F two hyperonyms, B D.
next synset visited hyperonymy chain J since higher number hyponymy
relations (three). algorithm compares number relations hyperonym synset (D
three relations), previous synset (F two). number increasing
process continues. Now, next node visit A. number relations two
number three, process stops synset selected BLC candidate J D.
Table 5 shows real example selection process noun church WordNet 1.6.
hyperonym chain number relations encoded WordNet (All criterion) shown
synset. local maximum chain marked bold.
8. algorithm starts checking first hyperonym synset, synset itself.

91

fiI ZQUIERDO , U AREZ & R IGAU

Figure 1: Example BLC selection
#rel.
18
19
37
10
12
5
#rel.
14
29
39
63
79
11
19
#rel.
20
69
5
11
7
1

synset
group 1,grouping 1
social group 1
organisation 2,organization 1
establishment 2,institution 1
faith 3,religion 2
Christianity 2,church 1,Christian church 1
synset
entity 1,something 1
object 1,physical object 1
artifact 1,artefact 1
construction 3,structure 1
building 1,edifice 1
place worship 1, ...
church 2,church building 1
synset
act 2,human action 1,human activity 1
activity 1
ceremony 3
religious ceremony 1,religious ritual 1
service 3,religious service 1,divine service 1
church 3,church service 1

Table 5: BLC selection noun church WordNet 1.6

92

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

Algorithm 1 BLC Extraction
Require: WordNet (WN) , typeOfRelation (T), threshold ()
BlcCandidates =
{synset W N }
cur :=
{Obtaining hypernym chains current synset cur}
H := Hypernyms(W N, cur)
new := SynsetW ithM oreRelations(W N, H, )
{Iterating number relations increased}
N umOf Rels(W N, T, cur) < N umOf Rels(W N, T, new)
cur := new
H := Hypernyms(W N, cur)
new := SynsetW ithM oreRelations(W N, H, )
end while{Store cur candidate BLC}
BlcCandidates := BlcCandidates {cur}
end
{Filtering BLC candidates}
BlcF inal =
{blc BlcCandidates}
< N umberOf Descendants(W N, blc)
BlcF inal := BlcF inal {blc}
end
end
return BlcF inal

Figure 2: Example BLC selection sense 2 church
93

fiI ZQUIERDO , U AREZ & R IGAU

figure 2 see diagram showing partial view selection process candidate
BLC sense number 2 noun church. synset dotted synset
processed (church2n ). synsets bold visited algorithm, one
gray (building1n ) one selected BLC church2n . process stops checking synset
structure1n number relations 63, lower number relations
previous synset (79 relations edifice1n ).
Obviously, combining different values threshold (for example 0, 10, 20 50)
criterion considered algorithm (All Hypo), process ends different sets BLC
extracted automatically WordNet version.
Furthermore, instead number relations consider frequency synsets
corpus measure importance. Synset frequency calculated sum
frequencies word senses contained synset, obtained SemCor (Miller,
Leacock, Tengi, & Bunker, 1993), WordNet.
sum up, algorithm two main parameters, parameter, representing minimum number synsets BLC must represent, criterion used characterizing
relevance synsets. values parameters be:
parameter: integer value greater equal 0
Synset relevance parameter: value considered measure importance synset.
Four possibilities:
Number relations synset
All: relations encoded synset
Hypo: hyponymy relations
Frequency synset
FreqWN: frequency obtained using WordNet
FreqSC: frequency obtained using SemCor
implementation algorithm different sets BLC used paper several
WordNet versions freely available9 .
4.1 Analysis Basic Level Concepts
selected WordNet 1.6 generate several sets BLC, combining four types synset
relevance criteria values 0, 10, 20 50 . values selected since
represent different levels abstraction, ranging = 0 (no filtering) = 50 (each BLC
must subsume least 50 synsets). Table 6 shows, combinations synset relevance
parameters, number concepts set BLC contains, average depth
WordNet hierarchy group. gray highlight two sets BLC (BLC-20 BLC-50
relations parameter) use experiments described paper.
expected, increasing threshold direct effect number BLC
average depth WordNet hierarchy. particular, values decreased, indicating
threshold increased, concepts selected abstract general. instance,
9. http://adimen.si.ehu.es/web/BLC

94

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

Threshold

Synset Relevance

0

10

20

50


Hypo
FreqSC
FreqWN

Hypo
FreqSC
FreqWN

Hypo
FreqSC
FreqWN

Hypo
FreqSC
FreqWN

# BLC
Nouns Verbs
3,094 1,256
2,490 1,041
34,865 3,070
34,183 2,615
971
719
993
718
690
731
691
738
558
673
558
672
339
659
340
667
253
633
248
633
94
630
99
631

Depth
Nouns Verbs
7.09
3.32
7.09
3.31
7.44
3.41
7.44
3.30
6.20
1.39
6.23
1.36
5.74
1.38
5.77
1.40
5.81
1.25
5.80
1.21
5.43
1.22
5.47
1.23
5.21
1.13
5.21
1.10
4.35
1.12
4.41
1.12

Table 6: Automatic Base Level Concepts WN1.6

using (All) nominal part WordNet, number concepts selected range 3,094
filtering ( = 0) 253 ( = 50). However, average, depth reduction acute since
varies 7.09 5.21. fact shows robustness method selecting synsets
intermediate level abstraction.
expected, verbal part WordNet behave differently. case, since verbal
hierarchies less deep, average depth synsets selected ranges 3.32 1.13
using relations, 3.31 1.10 using Hypo relations.
general, using frequency criteria, observe similar behavior
using relation criteria. However, effect threshold dramatic, specially
nouns. Again, expected, verbs behave differently nouns. number BLC (for
SemCor WordNet frequencies) reaches plateau around 600. fact, number
close verbal top beginners WordNet.
Summing up, devised simple automatic procedure deriving different sets BLC
representing different level abstraction whole set nominal verbal synsets WordNet. following section show explain supervised framework developed WSD
order exploit semantic classes described section previous one.

5. Supervised Class-Based WSD
follow supervised machine learning approach develop set semantic class based WSD
classifiers. systems use implementation Support Vector Machine algorithm train
classifiers, one per semantic class, semantic annotated corpora acquiring positive
negative examples class. classifiers built basis set features defined
representing examples. class-based, training data must collected treated
pretty different way usual word-based approach.
95

fiI ZQUIERDO , U AREZ & R IGAU

First, word-based class-based approaches selects training examples differently.
word-based approach, instances word used training examples. Figure
3 shows distribution training examples used generate word sense classifier noun
house. Following binary definition SVM, one classifier generated word sense.
classifiers, occurrences word sense associated classifier
used positive examples, rest word sense occurrences used negative examples.
Classifier HOUSE

Classifier
sense#1

... house.n#1 ...

Classifier
sense#2

... house.n#2...

... house.n#1 ...

Classifier
sense#3

... house.n#2 ...

... house.n#3 ...

Figure 3: Distribution examples using word-based approach
class-based approach, use examples words belong
particular semantic class. Figure 4 shows distribution examples class-based approach.
case, one classifier created semantic class. occurrences words belonging
semantic class associated classifier used positive examples, rest
occurrences word senses associated different semantic class selected negative
examples.
Obviously, class-based approach number examples training increased. Table
7 shows example sense church2n . Following word-based approach 58 examples
found Semcor church2n . Conversely, 371 positive training examples used
building classifier semantic class building, edifice.
think approach several advantages. First, semantic classes reduce average
polysemy degree words (some word senses might grouped together within semantic
class). Moreover, acquisition bottleneck problem supervised machine learning algorithms
attenuated increase number training examples. However, mixing
one classifier examples different words. instance, building class
grouping together examples hotel, hospital church, could introduce noise
learning process grouping unrelated word senses.
5.1 Learning Algorithm: SVM
Support Vector Machines (SVM) proven robust competitive many NLP
tasks, WSD particular (Marquez et al., 2006). experiments, used SVM-Light
96

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

Classifier ANIMAL

Classifier BUILDING

...hospital..
(BUILDING)

...house..
(BUILDING)

...dog...
(ANIMAL)

...cat...
(ANIMAL)

...star..
(PERSON)

Figure 4: Distribution examples using class-based approach

church2n

Classifier
(word-based approach)

building, edifice
(class approach)

Examples
church2n
church2n
building1n
hotel1n
hospital1n
barn1n
.......

# positive examples
58
58
48
39
20
17
......
371 examples

Table 7: Number examples Semcor: word vs. class-based approaches

97

fiI ZQUIERDO , U AREZ & R IGAU

implementation (Joachims, 1998). SVM used induce hyperplane separates positive
negative examples maximum margin. means hyperplane located
intermediate position positive negative examples, trying keep maximum distance
closest positive example, closest negative example. cases, possible
get hyperplane divides space linearly, better allow errors obtain
efficient hyperplane. known soft-margin SVM, requires estimation parameter
(C), represents trade-off allowed training errors margin. set
value 0.01, demonstrated good value SVM WSD tasks.
classifying example, obtain value output function SVM classifier
corresponding semantic class word example, system simply selects class
greatest value.
5.2 Corpora
Three semantic annotated corpora used training testing. Semcor training,
SensEval-2 SensEval-3 English all-words tasks, testing.
SemCor (Miller et al., 1993) subset Brown Corpus plus novel Red Badge
Courage, developed group created WordNet. contains 253
texts around 700,000 running words, 200,000 lemmatized sensetagged according Princeton WordNet 1.6. sense annotations SemCor
automatically ported WordNet versions10 .
SensEval-211 English all-words corpus (hereinafter SE2) (Palmer, Fellbaum, Cotton, Delfs, &
Dang, 2001) consists 5,000 words text three Wall Street Journal (WSJ) articles representing different domains Penn TreeBank II. sense inventory used tagging
WordNet 1.7.
SensEval-312 English all-words corpus (hereinafter SE3) (Snyder & Palmer, 2004), made
5,000 words, extracted two WSJ articles one excerpt Brown Corpus. Sense
repository WordNet 1.7.1 used tag 2,041 words proper senses.
considered alternative evaluation datasets. instance, SemEval-2007 coarse
grained task corpus13 . However, dataset discarded corpus annotated
particular set word sense clusters. Additionally, provide clear simple way
compare orthogonal sets clusterings. Although recent SensEval/SemEval
tasks WSD, think purpose evaluation (different level abstraction
WSD), SensEval-2 SensEval-3 still datasets best fit purposes. recent
SemEval competitions designed address specific topics, multilinguality joint
WSD Named Entity Recognition. However, make additional experiments
domain adaptation dataset provided SemEval-10 task 17 All-words Word Sense
Disambiguation Specific Domain (WSD-domain)14 (Agirre, Lopez de Lacalle, Fellbaum,
Hsieh, Tesconi, Monachini, Vossen, & Segers, 2010).
10.
11.
12.
13.
14.

http://web.eecs.umich.edu/mihalcea/downloads.html#semcor
http://www.sle.sharp.co.uk/senseval2
http://www.senseval.org/senseval3
Indeed participated task preliminary version system
http://semeval2.fbk.eu/semeval2.php?location=tasks#T25

98

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

5.3 Feature Types
Following previous contributions supervised WSD, selected set basic features
represent training testing examples. include additional features based semantic
classes.
Basic features
Word-forms lemmas window 10 words around target word.
PoS, concatenation preceding/following three five PoS tags.
Bigrams trigrams formed lemmas word-forms window 5 words
around target word; use tokens regardless PoS build bi/trigrams.
replace target word character X features increase generalization.
Semantic features
frequent semantic class target word, calculated SemCor.
Monosemous semantic class monosemous words window size five words
around target word.
Basic features widely used literature, work presented Yarowsky (1994).
features pieces information occur context target word: local features
including bigrams trigrams (including target word) lemmas, word-forms partof
speech labels (PoS). addition, wordforms lemmas larger window around target
word considered features representing topic discourse.
set features extended semantic information. Several types semantic classes
considered create features. particular, two different sets BLC (BLC20
BLC5015 ), SuperSenses, WordNet Domains (WND) SUMO.
order increase generalization capabilities class-based classifiers filter
irrelevant features. measure relevance feature16 f class c terms frequency
f. class c, feature f class, calculate frequency feature
within class (the number times occurs examples class), obtain
total frequency feature classes. get relative frequency dividing
values (classFreq / totalFreq) result lower certain threshold t, feature
removed feature list class c17 . way, make sure features selected
class frequently related class others. set threshold
0.25, obtained empirically preliminary versions classifiers applying crossvalidation setting SemCor.
15. selected set since represent different levels abstraction. said section 4, 20 50 refer
threshold minimum number synsets possible BLC must subsume considered proper BLC.
sets BLC built using criterion.
16. is, value feature, example feature type word-form, feature type
houses.
17. Depending experiment, around 30% original features removed filter.

99

fiI ZQUIERDO , U AREZ & R IGAU

6. Semantic ClassBased WSD Experiments
section present performance semantic class-based WSD system
words WSD SensEval-2 (SE2) SensEval3 (SE3) datasets. want analyze behavior
class-based WSD system working different levels abstraction. said
before, level abstraction defined semantic class used build classifiers.
experiment defined two different parameters one involving particular set
semantic classes.
1. Target class: semantic classes used train classifiers (determining abstraction
level system). case, tested: word-sense18 , BLC20, BLC50, WordNet Domains (WND), SUMO SuperSenses (SS).
2. Semantic features class: semantic classes used building semantic features.
case, tested: BLC20, BLC50, WND, SUMO SuperSenses (SS).
target class type classes classifier assigns given ambiguous word.
instance, target class traditional word expert classifiers word senses. Semantic
feature class one used building semantic features, independent target
class. instance, use WordNet Domains extract monosemous words context
target word use WND labels words semantic features building
classifier.
Combining different semantic classes target features, generated set experiments described next sections. way, evaluate independently impact
selecting one semantic class another target class semantic feature class.
Test
SE2
SE3

PoS
N
V
N
V

Sense
4.02
9.82
4.93
10.95

BLC20
3.45
7.11
4.08
8.64

BLC50
3.34
6.94
3.92
8.46

SUMO
3.33
5.94
3.94
7.60

SS
2.73
4.06
3.06
4.08

WND
2.66
2.69
3.05
2.49

Table 8: Average polysemy SE2 SE3
Table 8 shows average polysemy (AP) measured SE2 SE3 respect different semantic classes used evaluation target classes. expected, every corpus behaves
differently average polysemy verbs higher nouns. could assume
advance, relevant reductions polysemy degree obtained increasing level
abstraction. fact acute verbs. Note large reduction polysemy verbs
using SuperSenses WND. note priori SE3 seems difficult
disambiguate SE2, independently abstraction level.
6.1 Baselines
baselines evaluations define frequent classes (MFC) word calculated
SemCor. Ties classes specific word solved obtaining global frequency
18. included word-based evaluation comparison purposes since current system designed
class-based evaluation.

100

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

SemCor tied classes, selecting frequent class whole training
corpus. Semcor occurrences particular word (that is, able calculate
frequent class word), compute global frequency possible
semantic classes (obtained WordNet) SemCor, select frequent one. Table
9 shows baseline semantic class testing corpora.
Class
Sense
BLC20
BLC50
SUMO
SuperSense
WND

Pos
N
V
N
V
N
V
N
V
N
V
N
V

SE2
MFC AP
70.02 4.02
44.75 9.82
75.71 3.45
55.13 7.11
76.65 3.34
54.93 6.94
76.09 3.33
60.35 5.94
80.41 2.73
68.47 4.06
86.11 2.66
90.33 2.69

SE3
MFC
AP
72.30
4.93
52.88 10.95
76.29
4.08
58.82
8.64
76.64
3.92
60.05
8.46
79.55
3.94
64.71
7.60
81.50
3.06
79.07
4.08
83.82
3.05
92.20
2.49

Table 9: Frequent Class baselines average polysemy (AP) SE2 SE3
expected, performances MFC baselines high. particular, corresponding nouns (ranging 70% 80%). nominal baselines seem perform similarly
SE2 SE3, verbal baselines appear consistently much lower SE2 SE3.
SE2, verbal baselines range 44% 68% SE3 verbal baselines range 52%
79%. results WND high due low polysemy degree nouns verbs.
Obviously, increasing level abstraction (from senses WND) results increase.
6.2 Results Basic System
section present performance supervised semantic classbased WSD system.
Table 10 shows results system trained varying target classes using
basic feature set. values correspond F1 measures (harmonic mean recall
precision) training systems SemCor testing SE2 SE3 test sets. results
improve baselines shown italics. Additionally, results showing statistically
significant positive difference compared corresponding baseline using McNemars
test marked bold.
Interestingly, basic system word-sense level outperforms baselines SE2
SE3 nouns verbs. addition, systems obtain cases significantly better
results verbs. interesting verbs word-sense level baselines results
different, class-level differences datasets much smaller.
expected, results systems increase augmenting level abstraction (from
senses WND), cases, baseline results reached outperformed. even
relevant consider baseline results already quite high. However, high
level abstraction (SuperSenses WND) basic systems seem unable outperform
baselines.
101

fiI ZQUIERDO , U AREZ & R IGAU

Class
Sense
BLC20
BLC50
SUMO
SuperSense
WND

Pos
N
V
N
V
N
V
N
V
N
V
N
V

SE2
71.20
45.53
75.52
57.06
74.57
58.03
77.60
62.09
79.94
71.95
80.81
90.14

SE3
73.15
57.02
73.82
61.10
75.84
61.97
76.74
66.21
79.48
78.39
77.64
88.92

Table 10: Results basic system trained SemCor basic set features evaluated
SE2 SE3

general, results obtained BLC20 different BLC50. instance,
consider number classes within BLC20 (558 classes), BLC50 (253 classes) SuperSense (24 classes), BLC classifiers obtain high performance rates maintaining much higher
expressive power SuperSenses (they able classify among much larger number classes).
fact, using SuperSenses (40 classes nouns verbs) obtain accurate semantic tagger performance close 80%. Even interesting, could use BLC20 tagging
nouns (558 semantic classes F1 around 75%) SuperSenses verbs (14 semantic classes
F1 around 75%).
6.3 Results Exploiting Semantic Features
One main goals prove simple semantic features added training process
capable producing significant improvements basic systems. results experiments considering different types semantic features presented Tables 11 12,
respectively nouns verbs.
tables, column labeled Class refers called target class,
column labeled SF indicates type semantic features included represent examples
within machine learning approach.
Again, values tables correspond F1 measures (harmonic mean recall
precision) training systems SemCor testing SE2 SE3 test sets. results
improving baselines appear italics. Additionally, results showing statistically significant positive difference compared corresponding baseline using McNemars test
marked bold.
Regarding nouns (see Table 11), different behavior observed SE2 SE3. Adding
semantic features mainly improves results SE2. SE3 none systems present
significant improvement baselines, SE2 improvement obtained using
several types semantic features (in particular, using WND features SE2). use
semantic class-based features seems improve systems using target classes intermediate
levels abstraction (specially BLC20 BLC50). Interestingly, SE3 BLC20 BLC50
102

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

Class

SF
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND

Sense

BLC20

BLC50

SE2
70.02
71.20
71.79
71.69
71.59
71.10
71.20
75.75
75.52
77.69
77.79
77.60
75.14
77.88
76.65
74.57
78.45
76.65
79.58
75.52
78.92

SE3
72.30
73.15
73.15
73.04
73.15
72.70
73.15
76.29
73.82
76.52
75.73
73.71
73.82
74.24
76.74
75.84
76.85
76.74
75.51
74.61
74.83

Class

SUMO

SS

WND

SF
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND

SE2
76.09
77.60
75.52
75.52
77.88
77.50
77.88
80.41
79.94
81.07
80.22
80.51
80.32
82.47
86.11
80.81
81.85
82.33
83.55
83.08
86.01

SE3
79.55
76.74
76.74
77.19
78.76
76.97
77.42
81.50
79.48
81.39
81.73
81.05
76.46
79.82
83.82
77.64
80.79
80.11
81.24
78.31
83.71

Table 11: Results nouns using extended system
seem provide improvements baselines target classes (for instance,
BLC20, BLC50 SS), although significant.
Regarding verbs (see Table 12), different behavior observed SE2 SE3.
case, observe almost opposite effect nouns. SE3 semantic
class features improve results obtained baselines. SE2 systems
present significant improvement baselines, SE3 improvement obtained
using several types semantic features. However, case obtain significantly better
results several semantic features SE2. use semantic class-based features seems
benefit lower levels abstraction (specially word-sense, BLC20, BLC50 SUMO).
general, results show using semantic features addition basic features helps
reach better performance class-based WSD systems. Additionally, seems using
semantic features able obtain competitive classifiers sense level.
6.4 Learning Curves
investigate behavior class-based WSD system respect number training
examples. Although experiments carried nouns verbs,
include results nouns since cases, trend similar.
experiment, Semcor files randomly selected added training
corpus order generate subsets 5%, 10%, 15%, etc. training corpus19 . Then, train
19. portion contains files previous portion. example, files 25% portion
contained 30% portion.

103

fiI ZQUIERDO , U AREZ & R IGAU

Class

Sense

BLC20

BLC50

SF
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND

SE2
44.75
45.53
45.14
45.53
45.73
45.34
45.53
55.13
57.06
56.87
55.90
57.06
56.29
58.61
54.93
58.03
57.45
56.67
57.06
57.45
59.77

SE3
52.88
57.02
56.61
56.47
57.02
56.75
56.75
58.82
61.10
59.92
60.60
61.15
61.29
60.88
60.05
61.97
61.29
61.01
61.83
61.83
62.38

Class

SUMO

SS

WND

SF
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND

SE2
60.35
62.09
61.12
62.09
60.74
59.96
61.51
68.47
71.95
69.25
69.25
70.21
69.25
71.76
90.33
90.14
90.14
90.14
90.52
89.75
90.52

SE3
64.71
66.21
66.07
66.48
64.98
64.71
66.35
79.07
78.39
77.70
77.70
77.70
77.84
79.75
92.20
88.92
90.42
90.15
89.88
88.78
92.20

Table 12: Results verbs using extended system
system training portions test system SE2 SE3. Finally,
compare resulting system baseline computed training portion.
Figures 5 6 present learning curves SE2 SE3, respectively. case,
selected BLC20 class-based WSD system using WordNet Domains semantic features20 .
Surprisingly, SE2 system improves F1 measure around 2% increasing
training corpus 25% 100% SemCor. SE3, system improves F1
measure around 3% increasing training corpus 30% 100% SemCor. is,
knowledge required class-based WSD system seems already present
small part SemCor.
Figures 7 8 present learning curves SE2 SE3, respectively, class-based
WSD system based SuperSenses using semantic features built WordNet Domains.
SE2 system improves F1 measure around 2% increasing training corpus
25% 100% SemCor. SE3, system improves F1 measure around 2%
increasing training corpus 30% 100% SemCor. is, 25%
whole corpus, class-based WSD system reaches F1 close performance using corpus.
SE2 ans SE3, using BLC20 (Figures 5 6) SuperSenses (Figures 7 8)
semantic classes WSD, behavior system similar MFC baseline.
interesting since MFC obtains high results due way defined: MFC
total corpus assigned occurrences word training corpus. Without
definition, would large number words test set occurrences using
20. shown previous experiments, combination obtains good performance.

104

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

80

System SV2
MFC SV2

78

76

74

72
F1
70

68

66

64

62
5

10

15

20

25

30

35

40

45

50

55

60

65

70

75

80

85

90

95 100

% corpus

Figure 5: Learning curve BLC20 classifier SE2

78

System SV3
MFC SV3

76

74

72

F1

70

68

66

64

62
5

10

15

20

25

30

35

40

45

50 55
% corpus

60

65

70

75

80

85

90

Figure 6: Learning curve BLC20 classifier SE3

105

95 100

fiI ZQUIERDO , U AREZ & R IGAU

84

System SV2
MFC SV2

82

80

78

F1

76

74

72

70

68
5

10

15

20

25

30

35

40

45

50

55

60

65

70

75

80

85

90

95 100

% corpus

Figure 7: Learning curve SuperSense classifier SE2

82

System SV3
MFC SV3

80

78

F1

76

74

72

70
5

10

15

20

25

30

35

40

45

50 55
% corpus

60

65

70

75

80

85

90

95 100

Figure 8: Learning curve SuperSense classifier SE3

106

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

small training portions. cases, recall baselines (and turn F1) would much
lower.
evaluation seems indicate class-based approach WSD reduces considerably
required amount training examples.

7. Comparison SensEval Systems: Sense Level
main goal experiments included section verify whether abstraction level
class-based systems maintains discriminative power evaluated sense level. Additionally, compare results results top participant systems SE2 SE3
provided best senselevel outputs. Thus, class-based systems adapted following simple protocol. output based semantic classes converted sense identifiers:
instead semantic class produced systems particular instance, select first
sense word according WordNet sense ranking belonging predicted semantic class.
So, first obtain semantic class means classifiers, obtain restricted set
senses word match semantic class obtained, choose frequent
sense restricted subset.
results first experiment SE2 data shown Table 13. systems
prefix SVM- suffix denotes type semantic class used generate classifier21 .
cases experiments, WND selected target semantic class generate
semantic features. Two baselines marked Italics included. first sense WordNet (base-WordNet) frequent sense SemCor (base-SemCor). fact, developers
WordNet ranked word senses using SemCor sense-annotated corpora. Thus,
frequencies ranks appearing SemCor WordNet similar, equal.
include results system working word level (SVM-sense).
cases, nouns verbs, systems outperform frequent baselines.
frequent sense word, according WordNet sense ranking competitive
WSD tasks, extremely hard improve upon even slightly (McCarthy, Koeling, Weeds,
& Carroll, 2004). expected, behavior different semantic features produces slightly
different results. However, independently semantic features used, SE2 sense level,
class-based systems rank third position.
Table 14 shows experiment using SE3 dataset. case, class-based systems
clearly outperform baselines, achieving best results nouns second place verbs.
Interestingly, nouns, best system SE3 achieve SemCor baseline. recall
SE3 seems difficult SE2.
worth mention class-based systems use features nouns
verbs. instance, take profit complex feature sets encoding syntactic information
seems important verbs.
experiments show class-based classifiers seem quite competitive evaluated word sense level. perform frequent sense according WordNet
SemCor, achieve higher position nouns second verbs SE3, third
position nouns verbs SE2. Obviously, indicates class-based WSD maintains
high discriminative power word sense level.
21. instance, SVM-BLC20 stands experiment creates classifier considering BLC20 semantic classes.

107

fiI ZQUIERDO , U AREZ & R IGAU

Class Sense SE2
Nouns
Verbs
System
F1
System
SMUam
73.80 SMUaw
AVe-Antwerp
74.40 AVe-antwerp
SVM-semBLC20 71.80 SVM-semSUMO
SVM-semBLC50 71.70 SVM-sense
SVM-semSUMO 71.60 SVM-semWND
SVM-semWND
71.20 SVM-semBLC50
SVM-sense
71.20 SVM-semSS
SVM-semSS
71.10 SVM-semBLC20
base-WordNet
70.10 LIA-Sinequa
base-SemCor
70.00 base-SemCor
LIA-Sinequa
70.00 base-WordNet

F1
52.70
47.90
45.70
45.53
45.50
45.50
45.30
45.10
44.80
44.80
43.80

Table 13: Class Sense results SE2. Class word sense transformation.

Class Sense SE3
Nouns
System
SVM-semWND
SVM-semBLC20
SVM-semSUMO
SVM.sense
SVM-semBLC50
SVM-semSS
base-SemCor
GAMBL-AW
base-WordNet
kuaw
UNTaw
Meaning-allwords
LCCaw

F1
73.20
73.20
73.20
73.15
73.00
72.70
72.30
70.80
70.70
70.60
69.60
69.40
69.30

Verbs
System
GAMBL-AW
SVM-semSUMO
SVM-semWND
SVM-semSS
SVM-sense
SVM-semBLC20
SVM-semBLC50
UNTaw
Meaning-allwords
kuaw
R2D2
base-SemCor
base-WordNet

F1
59.30
57.00
56.80
56.80
56.75
56.60
56.50
56.40
55.20
54.50
54.40
52.90
52.80

Table 14: Class Sense results SE3. Class word sense transformation.

108

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

8. Comparison SensEval Systems: Class Level
experiments presented section explore performance word-based classifiers
participating SE2 SE3 evaluated class level. perform kind evaluation,
word sense output participant systems mapped corresponding semantic
classes. class-based systems modified. Obviously, expect different performances
systems depending semantic class level. Considering results presented tables
11 12, order perform comparison, selected experiments use WND
build semantic features22 . Thus, system results using different target semantic classes
represented SVM-semWND.
Table 15 presents ordered F1-measure results best performing systems SE2 data
evaluated different levels abstraction. previously, italics include
frequent senses according WordNet base-WordNet SemCor base-SemCor.
SE2, independently abstraction level PoS, system (SVM-semWND) scores
first positions ranking. one case system reaches best position, twice
second one. baselines outperformed experiments, except nouns using WND,
baseSemCor high.
Table 16 presents ordered F1-measure results best performing systems SE3 data
evaluated different levels abstraction. italics include frequent senses
according WordNet base-WordNet SemCor base-SemCor. systems represented
SVM-semWND.
SE3, see system performs better baselines cases, except
SemCorbased baseline nouns, obtains high result. particular, system
obtains good results verbs, reaching first second best positions cases,
outperforming baselines cases.
sum up, classbased approach outperforms SensEval participants (both SE2
SE3), sense level semantic class level. suggests good performance
semantic classifiers due polysemy reduction. Actually, confirms
classbased semantic classifiers learning semantic class training examples different
abstraction levels.

9. Domain Evaluation
section describe system SemEval-2 Allwords Word Sense Disambiguation
Specific Domain task (Izquierdo, Suarez, & Rigau, 2010). aim evaluation
show robust semantic class approach tested specific domain, different
domain training material.
Traditionally, SensEval competitions focused general domain texts. Thus, domain
specific texts present fresh challenges WSD. example, specific domains reduce possible meaning word given context. Moreover, distribution word senses data
examples changes compared general domains. problems affect supervised
knowledgebased systems. fact, supervised word-based WSD systems sensitive
corpora used training testing system (Escudero et al., 2000).
22. Remind semantic features frequent class target word, semantic class monosemous words context around target word.

109

fiI ZQUIERDO , U AREZ & R IGAU

Nouns

Verbs
F1
System
Sense BLC20
SMUaw
78.72 SMUaw
SVM-semWND 77.88 SVM-semWND
AVe-antwerp
76.71 LIA-Sinequa
base-SemCor
75.71 AVe-antwerp
base-WordNet
74.29 base-SemCor
LIA-Sinequa
73.39 base-WordNet
Sense BLC50
SMUaw
79.01 SMUaw
SVM-semWND 78.92 SVM-semWND
AVe-antwerp
77.57 LIA-Sinequa
base-SemCor
76.65 AVe-Antwerp
base-WordNet
75.24 base-SemCor
LIA-Sinequa
74.53 base-WordNet
Sense SUMO
SMUaw
79.30 SMUaw
SVM-semWND 77.88 LIA-Sinequa
base-SemCor
76.09 AVe-Antwerp
AVe-Antwerp
75.94 SVM-semWND
LIA-Sinequa
74.92 base-SemCor
base-WordNet
71.74 base-WordNet
Sense SuperSense
SVM-semWND 82.47 SMUaw
SMUaw
81.21 LIA-Sinequa
AVe-Antwerp
80.75 SVM-semWND
base-SemCor
80.41 AVe-Antwerp
LIA-Sinequa
79.58 base-WordNet
base-WordNet
78.16
base-SemCor
Sense WND
SMUaw
88.80 SMUaw
base-SemCor
86.11 SVM-semWND
SVM-semWND 86.01 base-SemCor
AVe-Antwerp
87.30 LIA-Sinequa
base-WordNet
85.82 base-WordNet
LIA-Sinequa
84.85 AVe-Antwerp
System

F1
61.22
58.61
57.42
57.28
55.13
54.16
61.61
59.77
57.81
57.67
54.93
54.55
68.22
64.79
62.56
61.51
61.33
60.35
73.47
72.74
71.76
69.31
69.05
68.47
91.16
90.52
90.33
89.82
89.75
89.74

Table 15: Results sense BLC20, BLC50, SUMO, SuperSense WND semantic classes
SE2

110

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

Nouns

Verbs
F1
System
Sense BLC20
base-SemCor
76.29 GAMBL-AW
GAMBL-AW
74.77 SVM-semWND
kuaw
74.69 kuaw
LCCaw
74.44 R2D2
UNTaw
74.40 UNTaw
SVM-semWND
74.24 Meaning-allwords
base-WordNet
74.16
base-SemCor
Meaning-allwords 73.11 base-WordNet
Sense BLC50
base-SemCor
76.74 GAMBL-AW
GAMBL-AW
75.56 SVM-semWND
kuaw
75.25 kuaw
SVM-semWND
74.83 R2D2
LCCaw
74.78 UNTaw
UNTaw
74.73 Meaning-allwords
base-WordNet
74.49 base-SemCor
R2D2
73.93 base-WordNet
Sense SUMO
base-SemCor
79.55 GAMBL-AW
kuaw
78.18 SVM-semWND
LCCaw
77.54 UNTaw
SVM-semWND
77.42 kuaw
UNTaw
77.32 Meaning-allwords
GAMBL-AW
77.14 upv-eaw2
base-WordNet
76.97
base-SemCor
Meaning-allwords 76.75 base-WordNet
Sense SuperSense
base-SemCor
81.50 SVM-semWND
kuaw
79.89 GAMBL-AW
SVM-semWND
79.82 base-SemCor
UNTaw
79.71 base-WordNet
GAMBL-AW
79.62 Meaning-allwords
upv-eaw2
79.27 Meaning-simple
upv-eaw
78.42 kuaw
base-WordNet
78.25 upv-eaw2
Sense WND
base-SemCor
83.80 SVM-semWND
SVM-semWND
83.71 base-SemCor
UNTaw
83.62 UNTaw
kuaw
81.78 GAMBL-AW
GAMBL-AW
81.53 base-WordNet
base-WordNet
81.46
R2D2
LCCaw
80.64 Meaning-simple
Meaning-allwords 80.50 kuaw
System

F1
63.56
60.88
60.66
59.79
59.73
59.37
58.82
58.28
64.38
62.38
61.22
60.35
60.27
60.19
60.06
58.82
68.77
66.35
66.03
65.93
65.43
64.92
64.71
64.02
79.75
79.40
79.07
78.25
78.14
77.72
77.53
77.21
92.20
92.20
91.37
91.01
90.83
90.52
90.50
90.44

Table 16: Results sense BLC20, BLC50, SUMO, SuperSense WND semantic classes
SE3

111

fiI ZQUIERDO , U AREZ & R IGAU

Therefore, main challenge develop specific domain WSD systems adapt
general system particular domain. Following research line, task proposed within
SemEval2 competition: Allwords Word Sense Disambiguation Specific Domain (Agirre
et al., 2010). restricted domain selected task environmental domain. test
corpora consist three texts compiled European Center Nature Conservation23 (ECNC)
World Wildlife Forum24 (WWF). task proposed several languages: Chinese, Dutch,
English Italian, although participation limited English. detail,
total 1,032 noun tokens 366 verb tokens tagged. Moreover, set background
documents related environmental domain provided. texts sense tagged,
plain text, provided ECNC WWF. could used
systems help adaptation specific domain. English, total 113
background documents, containing 2,737,202 words.
apply kind specific domain adaptation technique supervised classbased
system. order adapt supervised system environmental domain increase automatically training data new training examples domain. acquire examples,
use 113 background documents environmental domain provided organizers.
use TreeTagger (Schmid, 1994) preprocess documents, performing PoStagging lemmatization. Since background documents semantically annotated, supervised system
needs labeled data, selected monosemous instances occurring documents
according BLC20 semantic classes25 . Note approach exploited classbased WSD systems. way, obtained automatically large set examples annotated
BLC20. semantic class selected provided good results previous
experiments. order analyze approach system would work level
abstraction, performed evaluation posteriori using BLC50, WordNet Domains
SuperSenses besides BLC20, official participation SemEval-2. Nevertheless,
section focused BLC20.
Regarding BLC20, Table 17 presents total number training examples extracted SemCor (SC) background documents (BG). expected, method large number
monosemous examples obtained nouns verbs, although, verbs much less productive nouns. However, background examples correspond reduced set 7,646
monosemous words.
SC
BG
Total

Nouns
87,978
193,536
281,514

Verbs
48,267
10,821
59,088

N+V
136,245
204,357
340,602

Table 17: Number training examples BLC20
Table 18 lists ten frequent monosemous nouns verbs occurring background
documents. Remember examples monosemous according BLC20 semantic
classes.
23. http://www.ecnc.org
24. http://wwf.org
25. BLC20 (see section 4) stands Basic Level Concepts obtained relations criterion minimum threshold
subconcepts subsumed equal 20.

112

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

1
2
3
4
5
6
7
8
9
10

Nouns
Lemma
biodiversity
habitat
specie
climate
european
ecosystem
river
grassland
datum
directive

# ex.
7,476
7,206
7,067
3,539
2,818
2,669
2,420
2,303
2,276
2,197

Verbs
Lemma # ex.
monitor
788
achieve
784
target
484
select
345
enable
334
seem
287
pine
281
evaluate 246
explore
200
believe
172

Table 18: frequent monosemous words background documents
SC
BG
Total

Nouns
87,978
116,912
204,890

Verbs
48,267
7,019
55,286

N+V
136,245
123,931
260,176

Table 19: Number training examples word senses
approach applies semantic class architecture shown previous sections,
using examples extracted background documents. case, semantic class used
extract examples generate classifiers BLC2026 . select simple feature set widely
used many WSD systems. particular, use window five tokens around target word
extract word forms, lemmas; bigrams trigrams word forms lemmas; trigrams PoS
tags, frequent BLC20 semantic class target word training corpus.
analyze contribution monosemous examples performance system three
experiments defined:
BLC20SC: training examples extracted SemCor
BLC20BG: monosemous examples extracted background data
BLC20SCBG: training examples extracted SemCor monosemous background data
first run (BLC20SC) aims show behavior supervised system trained general
corpus, tested specific domain. second one (BLC20BG) analyzes contribution
monosemous examples extracted background data. Finally, third run (BLC20SCBG) studies robustness approach combining training examples SemCor
automatic ones obtained background documents.
Table 20 summarizes ordered recall official results participants English
WSD domain specific task SemEval2. table, Type refers approach followed
corresponding system: Weakly Supervised (WS), Supervised (S) KB (Knowledge Based,
unsupervised). participate system using BLC20 semantic class (the BLC20
SC/BG/SCBG runs). wordbased classifiers (labeled SenseBG, Sense-SC SenseSCBG)
26. case use set BLCs WordNet3.0, version WN one used
annotation.

113

fiI ZQUIERDO , U AREZ & R IGAU

included evaluation campaign. Finally, mentioned introduction,
included performance ItMakesSense system, one best performing WSD systems, task comparison purposes (it row table called
ItMakesSense Italics).
Rank
1
2
3
4
5
6
7
8
9
10
11
...
25
...
32

System ID
CFILT2
CFILT1
IIITH1-d.1.ppr.05
IIITH2-d.2.ppr.05
BLC20SCBG
ItMakesSense
BLC20SC
Frequent Sense
CFILT3
Treematch
Treematch2
SenseSCBG
SenseSC
...
BLC20BG
...
Random baseline
SenseBG

Type
WS
WS
WS
WS



KB
KB
KB


...

...


P
0.570
0.554
0.534
0.522
0.513
0.510
0.505
0.505
0.512
0.506
0.504
0.498
0.498
...
0.380
...
0.232
0.045

R
0.555
0.540
0.528
0.516
0.513
0.510
0.505
0.505
0.495
0.493
0.491
0.484
0.484
...
0.380
...
0.232
0.001

Table 20: Precision Recall SemEval2 participants. ItMakesSense results included
comparison purpose
general, results reported SemEval task quite low. best system
achieved precision 0.570, frequent baseline reached precision 0.505.
fact shows domain adaptation WSD systems difficult task.
Analyzing results three runs SemEval, worst result obtained system
using monosemous background examples (BLC20BG). system ranks 23rd27
Precision Recall 0.380 (0.385 nouns 0.366 verbs). system using SemCor
(BLC20SC) ranks 6th Precision Recall 0.505 (0.527 nouns 0.443 verbs).
performance first sense baseline. expected, best result three
runs obtained combining examples SemCor background (BLC20SCBG).
supervised system obtains 5th position Precision Recall 0.513 (0.534
nouns, 0.454 verbs) slightly baseline. Actually, version system
obtains slightly better results best performing supervised system (ItMakesSense). note
could include automatically monosemous examples background test thanks
class-based nature WSD system.
Moreover, system one completely supervised participating task. organizers calculated recall confidence interval 95% using bootstrap re-sampling procedure
(Noreen, 1989). method estimation might strict pairwise methods.
reveals differences four first systems system (BLC20SCBG)
27. table appears 25th position due included wordbased classifier results.

114

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

statistically significant. seen Figure 9, overlapping recall confidence interval four first systems system (ranking 5th), proves
differences statistically significant28 .

Figure 9: Recall confidence intervals.
Possibly, reason low performance BCL20BG system high correlation features target word semantic class. case, features correspond
monosemous word later evaluated polysemous words, kind features. However, seems class-based systems robust enough incorporate large sets
monosemous examples domain text. fact, knowledge, first time
supervised WSD algorithm successfully adapted specific domain. Furthermore,
system trained SemCor achieves good performance, reaching frequent
baseline, showing robustness class-based WSD approaches domain variations.
Comparing wordbased classifiers, seems BLC20 classes contribute two main
aspects. First, using set features, classbased classifiers obtain better results
wordbased ones. classifiers built BLC20 robust domain adaptable
wordbased approaches. Second, experiment uses examples extracted background data considering word senses (Sense-BG) obtain accuracy close zero,
experiment using BLC20 semantic classes (BLC20BG) reaches accuracy 0.380.
fact indicates BLCs useful extract good training examples unlabeled data.
mentioned previously, order obtain better insight, evaluation campaign performed
evaluation system using semantic classes represent different levels
abstractions: BLC50, WordNet Domains SuperSenses. Table 21 shows precision (P)
recall (R)29 evaluation considering different training datasets (SemCor only, Background
documents SemCor Background documents: SC, BG SC+BG respectively)
different semantic classes.
seen Table 21, BLC20 leads better performance using three different
corpora training (BG, SC SCBG). training monosemous examples extracted
background documents, BLC20 obtains best result, may indicate level
abstraction adequate other, including WND SS, sets much smaller
much lower polysemy. effect drawn results training
SemCor monosemous examples background (SCBG). best results
obtained BLC20, together SuperSenses two semantic classes seem
28. figure taken directly overview paper task.
29. figures obtained using official scorer script official gold key, without modification.

115

fiI ZQUIERDO , U AREZ & R IGAU

System ID
BLC20SCBG
ItMakesSense
BLC20SC
Frequent Sense
WNDSC
SenseSCBG
SenseSC
SS-SCBG
BLC50SCBG
BLC50SC
SSSC
WNSCBG
BLC20BG
WNDBG
SSBG
BLC50BG
Random baseline

Type
















-

P
0.513
0.510
0.505
0.505
0.495
0.498
0.498
0.484
0.481
0.481
0.472
0.471
0.380
0.362
0.348
0.277
0.232

R
0.513
0.510
0.505
0.505
0.495
0.484
0.484
0.484
0.481
0.481
0.457
0.471
0.380
0.362
0.348
0.277
0.232

Table 21: Results experiments according different semantic classes
benefit background monosemous examples. results seem confirm potential
capabilities BLC20 provide adequate level abstraction perform class-based WSD.
Finally, proved system performs level one state-of-the-art sys30
tem , ItMakesSense system (Zhong & Ng, 2010). Considering set features
system quite simple, apply machine learning optimization feature
engineering, results show use Semantic Classes provides robust behavior
specific domains, reaching state-of-the-art results.

10. Concluding Remarks
Word sense disambiguation difficult task empirically demonstrated SensEval/SemEval exercises. One reason difficulties could use inappropriate sets
word meanings. WordNet de-facto standard repository meanings, several attempts
made grouping senses order achieve higher levels accuracy. Moreover,
approach tries ease hard task creating large enough sets annotated data per domain
language train supervised systems. possible solution would use manual annotation semantic class labels instead fine-grained word senses (Schneider, Mohit, Oflazer, & Smith, 2012;
Schneider, Mohit, Dyer, Oflazer, & Smith, 2013).
Several attempts made obtain word sense groupings alleviate problem
fine granularity word senses, widely using WordNet senses. cases approach
consists grouping different senses word, resulting decrease polysemy,
reducing discriminative capacity. works use predefined sets semantic classes
integrated directly WSD system, mainly SuperSenses.
30. tested offline, ItMakesSense system participate task. downloaded last
version software http://www.comp.nus.edu.sg/nlp/software.html.

116

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

work describe simple method automatically select Basic Level Concepts
WordNet. Based simple structural properties WordNet, method automatically selects
different sets BLC representing different levels abstraction.
aim work explore several allwords WSD tasks performance different
levels abstraction provided Basic Level Concepts, WordNet Domains, SUMO SuperSense
labels. Furthermore, study empirically demonstrates that:
a) word sense groupings cluster senses coherent level abstraction order
perform supervised classbased WSD harming performance,
b) semantic classes successfully used semantic features boost performance
classifiers,
c) classbased approach WSD reduces dramatically required amount training examples obtain competitive classifiers,
d) classbased approach obtains competitive performances compared word-based systems,
e) classbased approach outperforms wordbased systems evaluated class level,
f) robustness class-based WSD system performing domain evaluation,
g) system reaches results comparable state-of-the-art system (ItMakesSense)
tested specific domain.
general, classbased disambiguation nouns verbs achieves better results
wordbased systems presented SensEval2 SensEval3. showed classbased approach reduces considerably required amount training examples. order prove
type disambiguation possible accurate ranked class-based systems
together SensEval2 Senseval3 official results. order establish fair comparison
mapped necessary word senses semantic classes viceversa.
experiments designed use classbased classifiers perform wordsense
disambiguation. shown simple approach selecting first sense WordNet corresponds class selected classifiers performs well top systems
SensEval2 SensEval3.
Additional experiments carried compare wordbased systems perform
classbased disambiguation. case translated official system outputs corresponding semantic classes.
Different experiments performed using different levels abstraction, ranging
SuperSenses (a small set) SUMO (which 1,000 labels linked WordNet1.6 senses),
WordNet Domains (with 163 labels), Basic Level Concepts (with arbitrary number classes
depending abstraction level selected).
expected differences SensEval2 SensEval3 results, class
based systems outperform baselines nouns verbs. Specially nouns, class-based
systems outperforms SensEval2 SensEval3 systems. general, results obtained
SVM-semBLC20 different results SVM-semBLC50. Thus, select
117

fiI ZQUIERDO , U AREZ & R IGAU

medium level abstraction, without significant decrease performance. Considering number classes, BLC classifiers obtain high performance rates maintaining much
higher expressiveness SuperSenses. However, using SuperSenses (40 classes) obtain
accurate semantic tagger performances around 80%. Even better, use BLC20
tagging nouns (558 semantic classes F1 75%) SuperSenses verbs (14 semantic
classes F1 around 75%).
systems SemEval2 All-words Word Sense Disambiguation Specific Domain task
proved simple features exploiting BLC perform well sophisticated methods.
Comparing wordbased classifiers, see BLC20 classes contribute two main
aspects: classbased classifiers obtain better results wordbased ones semantic classes
contribute effectively results. fact indicates that, particular, BLC20 useful
extract monosemous training examples unlabeled domain data.
next goal exploit inconsistencies different labeling provided different
class-based classifiers order obtain robust accurate class-based WSD system.
main idea study several classifiers, one based different degree abstraction (e.g.
BLC20, BLC50, WordNet Domains, etc.) label concrete context example incompatible
tags. manner, would able predict apply best classifier depending
context.

Acknowledgements
work partially supported NewsReader project31 (ICT-2011-316404), Spanish project SKaTer32 (TIN2012-38584-C06-02).

References
Agirre, E., & de Lacalle, O. L. (2003). Clustering wordnet word senses. Proceedings
RANLP03, Borovets, Bulgaria.
Agirre, E., & Edmonds, P. (2007). Word Sense Disambiguation: Algorithms Applications.
Springer.
Agirre, E., Lopez de Lacalle, O., Fellbaum, C., Hsieh, S.-K., Tesconi, M., Monachini, M., Vossen,
P., & Segers, R. (2010). Semeval-2010 task 17: All-words word sense disambiguation
specific domain. Proceedings 5th International Workshop Semantic Evaluation,
pp. 7580, Uppsala, Sweden. Association Computational Linguistics.
Bhagwani, S., Satapathy, S., & Karnick, H. (2013). Merging word senses. Proceedings Workshop Graph-based Methods Natural Language Processing (TextGraphs-8), pp. 1119.
Castillo, M., Real, F., & Rigau, G. (2004). Automatic assignment domain labels wordnet.
Proceeding 2nd International WordNet Conference, pp. 7582.
Ciaramita, M., & Altun, Y. (2006). Broad-coverage sense disambiguation information extraction supersense sequence tagger. Proceedings Conference Empirical Methods Natural Language Processing (EMNLP06), pp. 594602, Sydney, Australia. ACL.
31. http://www.newsreader-project.eu
32. http://nlp.lsi.upc.edu/skater

118

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

Ciaramita, M., & Johnson, M. (2003). Supersense tagging unknown nouns wordnet.
Proceedings Conference Empirical methods natural language processing
(EMNLP03), pp. 168175. ACL.
Curran, J. (2005). Supersense tagging unknown nouns using semantic similarity. Proceedings
43rd Annual Meeting Association Computational Linguistics (ACL05), pp. 26
33. ACL.
Escudero, G., Marquez, L., & Rigau., G. (2000). Empirical Study Domain Dependence
Supervised Word Sense Disambiguation Systems. Proceedings joint SIGDAT
Conference Empirical Methods Natural Language Processing Large Corpora,
EMNLP/VLC, Hong Kong, China.
Fellbaum, C. (Ed.). (1998). WordNet. Electronic Lexical Database. MIT Press.
Gangemi, A., Nuzzolese, A. G., Presutti, V., Draicchio, F., Musetti, A., & Ciancarini, P. (2012).
Automatic typing dbpedia entities. Proceedings 11th International Conference
Semantic Web - Volume Part I, ISWC12, pp. 6581, Berlin, Heidelberg. Springer-Verlag.
Gonzalez, A., Rigau, G., & Castillo, M. (2012). graph-based method improve wordnet domains.
Computational Linguistics Intelligent Text Processing, pp. 1728. Springer.
Hamp, B., Feldweg, H., et al. (1997). Germanet-a lexical-semantic net german. Proceedings
ACL workshop Automatic Information Extraction Building Lexical Semantic Resources
NLP Applications, pp. 915. Citeseer.
Hearst, M., & Schutze, H. (1993). Customizing lexicon better suit computational task.
Proceedingns ACL SIGLEX Workshop Lexical Acquisition, Stuttgart, Germany.
Hovy, E., Marcus, M., Palmer, M., Ramshaw, L., & Weischedel, R. (2006). Ontonotes: 90
Proceedings Human Language Technology Conference NAACL, Companion
Volume: Short Papers, NAACL-Short 06, pp. 5760, Stroudsburg, PA, USA. Association
Computational Linguistics.
Izquierdo, R., Suarez, A., & Rigau, G. (2007). Exploring automatic selection basic level concepts. et al., G. A. (Ed.), International Conference Recent Advances Natural Language
Processing, pp. 298302, Borovets, Bulgaria.
Izquierdo, R., Suarez, A., & Rigau, G. (2009). empirical study class-based word sense disambiguation. Proceedings 12th Conference European Chapter Association
Computational Linguistics, EACL 09, pp. 389397, Stroudsburg, PA, USA. Association
Computational Linguistics.
Izquierdo, R., Suarez, A., & Rigau, G. (2010). Gplsi-ixa: Using semantic classes acquire monosemous training examples domain texts. Proceedings 5th International Workshop
Semantic Evaluation, pp. 402406. Association Computational Linguistics.
Joachims, T. (1998). Text categorization support vector machines: learning many relevant
features. Nedellec, C., & Rouveirol, C. (Eds.), Proceedings ECML-98, 10th European
Conference Machine Learning, No. 1398, pp. 137142, Chemnitz, DE. Springer Verlag,
Heidelberg, DE.
L. Bentivogli, P. Forner, B. M., & Pianta, E. (2004). Revising wordnet domains hierarchy: Semantics, coverage, balancing. COLING 2004 Workshop Multilingual Linguistic
Resources, Geneva, Switzerland.
119

fiI ZQUIERDO , U AREZ & R IGAU

Magnini, B., & Cavaglia, G. (2000). Integrating subject field codes wordnet. Proceedings
LREC, Athens. Greece.
Marquez, L., Escudero, G., Martnez, D., & Rigau, G. (2006). Supervised corpus-based methods
wsd. E. Agirre P. Edmonds (Eds.) Word Sense Disambiguation: Algorithms
applications., Vol. 33 Text, Speech Language Technology. Springer.
McCarthy, D., Koeling, R., Weeds, J., & Carroll, J. (2004). Finding predominant word senses
untagged text. 42nd Annual Meeting Association Computational Linguistics,
Barcelona, Spain.
Mihalcea, R. (2007). Using wikipedia automatic word sense disambiguation. Proceedings
NAACL HLT 2007.
Mihalcea, R., Csomai, A., & Ciaramita, M. (2007). Unt-yahoo: Supersenselearner: Combining
senselearner supersense coarse semantic features. Proceedings 4th
International Workshop Semantic Evaluations, SemEval 07, pp. 406409, Stroudsburg,
PA, USA. Association Computational Linguistics.
Mihalcea, R., & Moldovan, D. (2001). Automatic generation coarse grained wordnet. Proceding NAACL workshop WordNet Lexical Resources: Applications, Extensions Customizations, Pittsburg, USA.
Miller, G., Leacock, C., Tengi, R., & Bunker, R. (1993). Semantic Concordance. Proceedings
ARPA Workshop Human Language Technology.
Navigli, R. (2006). Meaningful clustering senses helps boost word sense disambiguation performance. ACL-44: Proceedings 21st International Conference Computational
Linguistics 44th annual meeting Association Computational Linguistics,
pp. 105112, Morristown, NJ, USA. Association Computational Linguistics.
Navigli, R. (2009). Word Sense Disambiguation: survey. ACM Computing Surveys, 41(2), 169.
Navigli, R., Litkowski, K., & Hargraves, O. (2007). Semeval-2007 task 07: Coarse-grained english
all-words task. Proceedings Fourth International Workshop Semantic Evaluations (SemEval-2007), pp. 3035, Prague, Czech Republic. Association Computational
Linguistics.
Niles, I., & Pease, A. (2001). Towards standard upper ontology. Proceedings 2nd
International Conference Formal Ontology Information Systems (FOIS-2001), pp. 17
19. Chris Welty Barry Smith, eds.
Niles, I., & Pease, A. (2003). Linking lexicons ontologies: Mapping WordNet Suggested
Upper Merged Ontology. Arabnia, H. R. (Ed.), Proc. IEEE Int. Conf. Inf.
Knowledge Engin. (IKE 2003), Vol. 2, pp. 412416. CSREA Press.
Noreen, E. (1989). Computer-intensive methods testing hypotheses: introduction. Wiley
Interscience publication. Wiley.
Paa, G., & Reichartz, F. (2009a). Exploiting semantic constraints estimating supersenses
crfs.. SDM, pp. 485496. SIAM.
Paa, G., & Reichartz, F. (2009b). Exploiting semantic constraints estimating supersenses
crfs.. SDM, pp. 485496. SIAM.
120

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

Palmer, M., Fellbaum, C., Cotton, S., Delfs, L., & Dang, H. T. (2001). English tasks: All-words
verb lexical sample. Proceedings SENSEVAL-2 Workshop. conjunction
ACL2001/EACL2001, Toulouse, France.
Peters, W., Peters, I., & Vossen, P. (1998). Automatic sense clustering eurowordnet. First International Conference Language Resources Evaluation (LREC98), Granada, Spain.
Picca, D., Gliozzo, A. M., & Ciaramita, M. (2008). Supersense tagger italian.. LREC. Citeseer.
Pradhan, S., Dligach, E. L. D., & Palmer, M. (2007). Semeval-2007 task 17: English lexical sample,
srl words. SemEval 07: Proceedings 4th International Workshop Semantic
Evaluations, pp. 8792, Morristown, NJ, USA. Association Computational Linguistics.
Rosch, E. (1977). Human categorisation. Studies Cross-Cultural Psychology, I(1), 149.
Schmid, H. (1994). Probabilistic Part-of-Speech Tagging Using Decision Trees. Proceedings
International Conference New Methods Language Processing, pp. 4449.
Schneider, N., Mohit, B., Dyer, C., Oflazer, K., & Smith, N. A. (2013). Supersense tagging
arabic: mt-in-the-middle attack.. HLT-NAACL, pp. 661667. Citeseer.
Schneider, N., Mohit, B., Oflazer, K., & Smith, N. A. (2012). Coarse lexical semantic annotation
supersenses: arabic case study. Proceedings 50th Annual Meeting
Association Computational Linguistics: Short Papers-Volume 2, pp. 253258. Association
Computational Linguistics.
Segond, F., Schiller, A., Greffenstette, G., & Chanod, J. (1997). experiment semantic tagging
using hidden markov model tagging. ACL Workshop Automatic Information Extraction
Building Lexical Semantic Resources NLP Applications, pp. 7881. ACL, New
Brunswick, New Jersey.
Snow, R., S., P., D., J., & A., N. (2007). Learning merge word senses. Proceedings Joint
Conference Empirical Methods Natural Language Processing Computational Natural Language Learning (EMNLP-CoNLL), pp. 10051014.
Snyder, B., & Palmer, M. (2004). english all-words task. Mihalcea, R., & Edmonds, P.
(Eds.), Senseval-3: Third International Workshop Evaluation Systems Semantic Analysis Text, pp. 4143, Barcelona, Spain. Association Computational Linguistics.
Tsvetkov, Y., Schneider, N., Hovy, D., Bhatia, A., Faruqui, M., & Dyer, C. (2014). Augmenting
english adjective senses supersenses. Proc. LREC, pp. 43594365.
Villarejo, L., Marquez, L., & Rigau, G. (2005). Exploring construction semantic class classifiers wsd. Proceedings 21th Annual Meeting Sociedad Espaola para el
Procesamiento del Lenguaje Natural SEPLN05, pp. 195202, Granada, Spain. ISSN 11365948.
Vossen, P. (Ed.). (1998). EuroWordNet: Multilingual Database Lexical Semantic Networks
. Kluwer Academic Publishers .
Wikipedia (2015). Wikipedia, free encyclopedia. https://en.wikipedia.org.. [Online;
accessed 21-August-2015].
Yarowsky, D. (1994). Decision lists lexical ambiguity resolution: Application accent restoration spanish french. Proceedings 32nd Annual Meeting Association
Computational Linguistics (ACL94).
121

fiI ZQUIERDO , U AREZ & R IGAU

Zhong, Z., & Ng, H. T. (2010). makes sense: wide-coverage word sense disambiguation system
free text. Proceedings ACL 2010 System Demonstrations, ACLDemos 10, pp.
7883, Stroudsburg, PA, USA. Association Computational Linguistics.

122


