journal artificial intelligence

submitted published

knowledge textual inference via
parse tree transformations
roy bar haim

barhair gmail com

ido dagan

dagan cs biu ac il

computer science department bar ilan university
ramat gan israel

jonathan berant

yonatan cs stanford edu

computer science department stanford university

abstract
textual inference important component many applications understanding
natural language classical approaches textual inference rely logical representations
meaning may regarded external natural language however
practical applications usually adopt shallower lexical lexical syntactic representations
correspond closely language structure many cases approaches lack principled meaning representation inference framework describe inference formalism
operates directly language structures particularly syntactic parse trees
trees generated applying inference rules provide unified representation
varying types inferences use manual automatic methods generate rules
cover generic linguistic structures well specific lexical inferences
present novel packed data structure corresponding inference allows
efficient implementation formalism proved correctness
established efficiency analytically empirically utility
illustrated two tasks unsupervised relation extraction large corpus
recognizing textual entailment rte benchmarks

introduction
textual inference natural language processing nlp concerned deriving target
meanings texts textual entailment framework dagan roth sammons
zanzotto reduced inferring textual statement hypothesis h
source text traditional approaches formal semantics perform inferences
logical forms derived text contrast practical nlp applications avoid
complexities logical interpretation instead operate shallower representations
parse trees possibly supplemented limited semantic information named
entities semantic roles forth clearly demonstrated recent pascal
recognizing textual entailment rte challenges dagan glickman magnini b
bar haim dagan dolan ferro giampiccolo magnini szpektor giampiccolo
magnini dagan dolan giampiccolo trang dang magnini dagan dolan
bentivogli dagan dang giampiccolo magnini bentivogli clark dagan
c

ai access foundation rights reserved

fibar haim dagan berant

dang giampiccolo popular framework evaluating application independent
semantic inference
inference representations commonly made applying transformations
substitutions tree graph representing text transformations
available knowledge paraphrases lexical relations synonyms hyponyms
syntactic variations de salvo braz girju punyakanok roth sammons
haghighi ng manning kouylekov magnini harmeling
transformations may generally viewed inference rules available semantic knowledge bases composed manually experts example wordnet
fellbaum large community contributors wikipedia
dbpedia resource lehmann et al knowledge bases learned automatically distributional pattern methods aligned monolingual
bilingual parallel texts lin pantel shinyama sekine sudo grishman
szpektor tanev dagan coppola chklovski pantel bhagat
ravichandran ganitkevitch van durme callison burch overall applied
knowledge inference prominent line gained much interest recent examples include series workshops knowledge reasoning answering
questions saint dizier mehta melkar evaluation knowledge resources
recent recognizing textual entailment challenges bentivogli et al
many applied systems use semantic knowledge inference rules
use typically limited application specific somewhat heuristic formalizing
practices important textual inference analogous role well formalized
parsing machine translation take step direction introducing
generic inference formalism parse trees formalism uses inference rules capture
wide variety inference knowledge simple uniform manner specifies small
set operations suffice broadly utilize knowledge
formalism applying inference rule clear intuitive interpretation generating sentence parse consequent semantically entailed source sentence
inferred consequent may subject rule applications rule applications may independent modifying disjoint parts source tree
may specify mutually exclusive alternatives e g different synonyms source
word deriving hypothesis text analogous proof search logic
propositions parse trees deduction steps correspond rule applications
nave implementation formalism would generate consequent explicitly
separate tree however discuss section implementation raises
severe efficiency issues since number consequents may grow exponentially
number possible rule applications previous work proposed partial solutions
cf section work present novel data structure termed compact
forest packed representation entailed consequents corresponding inference
prove valid implementation formalism
establish efficiency analytically showing typical exponential linear reduction
empirically showing improvement orders magnitude together formalism
see instance listing techniques per submission provided organizers first
three challenges dagan et al b bar haim et al giampiccolo et al



fiknowledge textual inference via parse tree transformations

novel efficient inference open way large scale rule application within
well formalized framework
formalism inference built inference engine
incorporates variety semantic syntactic knowledge bases cf section
evaluated inference engine following tasks
unsupervised relation extraction large corpus setting allows evaluation
knowledge inferences real world distribution texts
recognizing textual entailment rte cope complex rte examples complemented knowledge inference engine machine learningbased entailment classifier provides necessary approximate matching capabilities
inference engine shown substantial contribution tasks illustrating
utility
bar haim dagan greental shnarch bar haim berant dagan
described earlier versions inference framework efficient implementation respectively current article includes major enhancements
contributions formalism presented detail including
examples pseudo code present several extensions formalism including treatment co reference traces long range dependencies enhanced
modeling polarity efficient inference presented detail
including pseudo code addition provide complete proofs theorems
establish correctness finally article contains extended analysis
inference component rte system terms applicability coverage
correctness rule applications

background
section provide background textual entailment survey
approaches applied task recognizing textual entailment rte particular
focus use semantic knowledge within current rte systems
textual entailment
many semantic applications need identify meaning expressed
inferred language expressions example question answering systems
need verify retrieved passage text entails selected answer given question
john lennons widow text yoko ono unveiled bronze statue late
husband john lennon complete official renaming englands liverpool airport
liverpool john lennon airport entails expected answer yoko ono john lennons
widow similarly information extraction systems need validate given text
indeed entails semantic relation expected hold extracted slot fillers
e g x works information retrieval queries alzheimers drug treatment
example taken rte dataset bar haim et al
one topics trec ir benchmark voorhees harman



fibar haim dagan berant

rephrased propositions e g alzheimers disease treated drugs
expected entailed relevant documents selecting sentences
included summary multi document summarization systems verify
meaning candidate sentence entailed sentences already summary
avoid redundancy
observation led dagan glickman propose unifying framework modeling
language variability termed textual entailment te dagan glickman dagan
et al b define te follows
say entails h typically human reading would infer h
likely true somewhat informal definition assumes common human understanding language well common background knowledge
dagan et al discuss te definition relation classical semantic
entailment linguistics literature recognizing textual entailment challenges rte
held annually since dagan et al b bar haim et al
giampiccolo et al bentivogli et al formed growing
community around task
holy grail te development entailment engines used
generic modules within different semantic applications similar current use
syntactic parsers morphological analyzers since textual entailment defined
relation surface texts bound particular semantic representation
allows black box view entailment engine input output interface
independent internal implementation may employ different types
semantic representations inference methods
determining entailment
consider following h pair

h

oddest thing uae million
people living country uae citizens
population united arab emirates million

understanding h involves several inference steps first infer
reduced relative clause million people living country proposition
million people live country
next observe country refers uae rewrite
million people live uae
knowing uae acronym united arab emirates obtain
million people live united arab emirates
taken rte test set dagan et al b



fiknowledge textual inference via parse tree transformations

finally paraphrase obtain h
population united arab emirates million
general textual inference involves diverse linguistic world knowledge including
knowledge relevant syntactic phenomena e g relative clauses paraphrasing x people
live population x lexical knowledge uae united arab emirates
may require co reference resolution example substituting country uae may think types knowledge representing inference
rules define derivation entailed propositions consequents work
introduce formal inference framework inference rule application current
discussion however informal notion inference rules would suffice
example illustrates derivation h sequence inference
rule applications procedure generally known forward chaining finding sequence
rule applications would get us h close possible thus search
defined space possible rule application chains
ideally would base entailment engine solely trusted knowledge
inferences practice however available knowledge incomplete full derivation h
often feasible therefore requiring strict knowledge proofs likely
yield limited recall alternatively may back heuristic approximate
entailment classification
next two sections survey two complementary inference types knowledgebased inference focus approximate entailment matching
classification
knowledge inference
section describe common resources inference rules
use textual entailment systems
semantic knowledge resources
lexical knowledge lexical semantic relations words phrases play important role textual inference prominent lexical resource wordnet fellbaum
manually composed wide coverage lexical semantic database following wordnet relations typically used inference synonyms buy purchase antonyms win
lose hypernyms hyponyms relations violin musical instrument meronyms
part relations provence france derivations meeting meet
many researchers aimed deriving lexical relations automatically diverse methods sources much automatically extracted knowledge complementary
wordnet however typically less accurate snow jurafsky ng presented
method automatically expanding wordnet synsets achieving high precision
lins thesaurus lin distributional similarity recently several works
aimed extract lexical semantic knowledge wikipedia metadata well
textual definitions kazama torisawa ponzetto strube shnarch barak
dagan lehmann et al others recent empirical study


fibar haim dagan berant

inferential utility common lexical resources see work mirkin dagan shnarch

paraphrases lexical syntactic inference rules rules typically represent
entailment equivalence predicates including correct mapping
arguments e g acquisition x x purchase much work dedicated
unsupervised learning relations comparable corpora barzilay mckeown barzilay lee pang knight marcu querying web
ravichandran hovy szpektor et al local corpus lin pantel
glickman dagan bhagat ravichandran szpektor dagan
yates etzioni particular textual entailment systems widely used
dirt resource lin pantel common idea underlying
predicates sharing argument instantiations likely semantically related
nomlex plus meyers reeves macleod szekeley zielinska young lexicon containing mostly nominalizations verbs allowed argument structures e g
xs acquisition ys acquisition x etc argument mapped wordnet amwn
szpektor dagan resource inference rules verbal nominal predicates including argument mapping wordnet nomlex plus
verified statistically intersection unary dirt szpektor
dagan
syntactic transformations textual entailment often involves inference generic
syntactic phenomena passive active transformations appositions conjunctions etc
illustrated following examples
john smiled laughed john laughed conjunction
neighbor john came john neighbor apposition
im reading interesting im reading relative clause
syntactic transformations addressed extent de salvo braz et al
romano kouylekov szpektor dagan lavelli describe novel
syntactic rule base entailment survey relevant linguistic literature well
extensive data analysis sections
use semantic knowledge textual entailment systems
following description common knowledge sources textual inference discuss
use knowledge textual entailment systems
textual entailment systems usually represent h trees graphs
syntactic parse predicate argument structure semantic relations entailment
determined measuring well h matched embedded estimating
distance h commonly defined cost transforming h
next section briefly cover methods proposed approximate
matching heuristic transformations graphs trees role semantic knowledge
general scheme bridge gaps h stem language
variability example applying lexical semantic rule purchase buy allows
matching word buy appearing h word purchase appearing


fiknowledge textual inference via parse tree transformations

rte systems restrict type allowed inference rules search space
systems lexical word phrase matching h haghighi et al
maccartney galley manning heuristic transformation h
kouylekov magnini harmeling typically apply lexical rules without
variables sides rule matched directly h
hickl derived given h pair small set consequents terms
discourse commitments commitments generated several different tools
techniques syntax conjunctions appositions relative clauses etc co reference
predicate argument structure extraction certain relations paraphrase acquisition
web pairs commitments derived h fed next stages
rte system lexical alignment entailment classification prior commitment
generation several linguistic preprocessing modules applied text including
syntactic dependency parsing semantic dependency parsing named entity recognition
co reference resolution hickl employed probabilistic finite state transducer fst
extraction framework commitment generation extraction rules modeled
series weighted regular expressions commitments textual form fed
back system additional commitments generated
de salvo braz et al first incorporate syntactic semantic inference
rules comprehensive entailment system system inference rules applied
hybrid syntactic semantic structures called concept graphs left hand side lhs
rule matched concept graph graph augmented instantiation
right hand side rhs rule several iterations rule application
system attempts embed hypothesis augmented graph types semantic
knowledge verb normalization lexical substitutions applied
rule application preprocessing time rule application part hypothesis
subsumption embedding
several entailment systems logical inference bos markert
represented h drs structures used discourse representation theory kamp
reyle translated first order logic background knowledge
bk encoded axioms comprised lexical relations wordnet geographical
knowledge small set manually composed axioms encoding generic knowledge
bos markert used logic theorem prover proof entails h alone
together background knowledge bk h inconsistent
implying non entailment background knowledge logic prover
complemented model builder aimed counter examples e g model
h holds logical inference system suffered low coverage due limited
background knowledge available able proofs small fraction
rte dataset therefore rte system bos markert combined logical inference
shallow approximate matching method mainly word overlap
lccs logic entailment system tatu moldovan one top performers rte rte tatu iles slavick novischi moldovan tatu
moldovan proprietary tools deriving rich semantic representations extensive knowledge engineering syntactic parses h
transformed logic forms moldovan rus representation enriched
variety relations extracted semantic parser well named entities


fibar haim dagan berant

temporal relations inference knowledge included demand axioms extended
wordnet lexical chains wordnet glosses nlp rewrite rules additional knowledge
types included several hundreds world knowledge axioms temporal axioms semantic composition axioms e g encoding transitivity kinship relation
rich semantic representation extensive set axioms theorem prover aimed
prove refutation entails h proof failed h repeatedly simplified
proof found reducing proof score simplification
approximate entailment classification
semantic knowledge incomplete therefore cases knowledge inference must complemented approximate heuristic methods determining entailment rte systems employ limited amount semantic knowledge
focus methods approximate entailment classification common architecture
rte systems hickl bensley williams roberts rink shi snow vanderwende
menezes b maccartney grenager de marneffe cer manning comprises
following stages
linguistic processing includes syntactic possibly semantic parsing namedentity recognition co reference resolution etc often h represented trees
graphs nodes correspond words edges represent relations
words
alignment best mapping h nodes nodes taking account
node edge matching
entailment classification alignment found set features extracted
passed classifier determining entailment features measure
alignment quality try detect cues false entailment example
node h negated aligned node negated may indicate false
entailment
alternative aims transform text hypothesis rather
aligning kouylekov magnini applied tree edit distance
textual entailment edit operation node insertion deletion substitution assigned
cost aims minimum cost sequence operations transform
h mehdad magnini b proposed method estimating cost
edit operation particle swarm optimization wang manning
presented probabilistic tree edit edit operations structured
latent variables tree edits represented state transitions finite state machine
fsm model parameterized conditional random field crf harmeling
developed probabilistic transformation defined fixed set
operations including syntactic transformations wordnet substitutions
heuristic transformations adding removing verb noun probability
transformation estimated development set similarly heilman smith
classify entailment sequence edits transforming h employ
generic edit operations greedy search heuristic guided cost function
measures remaining distance h tree kernel


fiknowledge textual inference via parse tree transformations

zanzotto pennacchiotti moschitti aimed classify given h pair
analogy similar pairs training set method finding intra pair
alignment e h capturing transformation h interpair alignment capturing analogy pair h previously seen
pair h cross pair similarity kernel computed tree kernel similarity
applied aligned texts aligned hypotheses another cross pair similarity kernel
proposed wang neumann extracted tree skeletons h
consisting left right spines defined unlexicalized paths starting root
found sections h spines differ compared sections across pairs
subsequence kernel

goal
goal textual entailment develop entailment engines used
generic inference components within text understanding applications logic
entailment systems provide formalized expressive framework textual inference
however deriving logic representations text complex task available tools
match accuracy robustness current syntactic parsers often basis
semantic parsing furthermore interpretation logic forms often unnecessary
many common inferences modeled shallower representations
follows textual entailment systems text understanding applications
general operate lexical syntactic representations possibly supplemented
partial semantic annotation however unlike logic approaches systems
lack clear unified formalism knowledge representation inference instead
employ multiple representations inference mechanisms notable exception
natural logic framework maccartney manning rather different
focus current work discuss section
work develop well formalized entailment lexical syntactic
level formalism wide variety inference rules composition
unified representation small set inference operations moreover present
efficient implementation formalism novel data structure
allow compact representation proof search space
see contribution work practical theoretical practical
engineering perspective formalism may simplify development entailment
systems number representations inference mechanisms need dealt
minimal furthermore efficient implementation may allow entailment engines
explore much larger search spaces theoretical perspective concise formal modeling
leads better insight phenomenon investigation particular
formal model entailment engine makes possible apply formal methods investigating properties enabled us prove correctness efficient implementation
formalism cf appendix next present inference formalism


fibar haim dagan berant

rule
type
syntactic

sources

examples

manually composed

lexical

learned unsupervised dirt tease
derived automatically integrating information wordnet
nomlex verified corpus
statistics amwn
wordnet wikipedia

passive active apposition relative
clause conjunctions
xs wife x married

syntactic

lexical

x bought sold x

x maker x produces
steal take albanianalbania
janis joplinsinger
amazonsouth america

table representing diverse knowledge types inference rules

inference formalism parse trees
previous sections highlighted need principled well formalized
textual inference lexical syntactic level section propose step towards
filling gap defining formalism textual inference parse representations semantic knowledge required inference represented inference rules
encode parse tree transformations rule application generates consequent sentence represented parse tree source tree figure b shows sample inference
rule representing passive active transformation
knowledge representation usage perspective inference rules provide simple
unifying formalism representing applying broad range inference knowledge
examples breadth illustrated table knowledge acquisition
perspective representing inference rules lexical syntactic level allows easy incorporation rules learned unsupervised methods important scaling inference
systems interpretation stipulated semantic representations often difficult
inherently supervised semantic task learning circumvented altogether
historical machine translation perspective similar transfer translation contrasted semantic interpretation interlingua overall goal
explore reach inference identify scope
semantic interpretation may needed
given syntactically parsed source text set inference rules formalism
defines set consequents derivable text rules consequent
obtained sequence rule applications generating intermediate parse
tree similar proof process logic addition consequents may inferred
co reference relations identified traces formalism includes annotation rules
add features existing trees according formalism text entails hypothesis
h h consequent
rest section define illustrate formalism components
sentence representation section inference rules application sections
inference co reference relations traces section annotation


fiknowledge textual inference via parse tree transformations

input source tree rule e l r
output set derived trees
set matches l

f
l subtree matched l according match f
r instantiation
r copy r
variable v r
instantiate v f v
aligned pair nodes ul l ur r
daughter ul
l
copy subtree rooted ur r dependency relation
derived tree generation
substitution rule
copy l descendants nodes replaced r
else introduction rule
dr
add

applying rule tree
rules section components form inference process specifies set
inferable consequents given text set rules section section extends
hypothesis definition allowing h template rather proposition finally
section discusses limitations possible extensions formalism
sentence representation
assume sentences represented form parse trees work focus
dependency tree representation often preferred directly capture predicateargument relations two dependency trees shown figure nodes represent words
hold set features values features include word lemma
part speech additional features may added inference process
edges annotated dependency relations
inference rules
entailment inference rule l r primarily composed two templates lefthand side lhs l right hand side rhs r templates dependency subtrees
may contain pos tagged variables matching lemma figure shows passiveto active transformation rule illustrates application
rule application procedure given rule application generates set
derived trees consequents source tree steps described


fibar haim dagan berant

root




rain verb

expletive

r

wha





adj


r

mary noun
mod

see verb

obj

q


mod

bysubj



verb

prep



yesterday noun

pcompn


little adj

john noun

source rained little mary seen john yesterday

root




rain verb

r

expletive

wha





adj



subj

r

john noun

see verb
obj

mod



mary noun yesterday noun
mod



little adj
derived rained john saw little mary yesterday

passive active tree transformation


v verb
obj

l

u

n noun

v verb

bysubj


subj

obj





u



verb

prep

n noun

n noun

pcompn

r



n noun
b passive active substitution rule
figure application inference rule pos relation labels minipar
lin n n v variables whose instances l r implicitly aligned
subj dependency relation indicates passive sentence



fiknowledge textual inference via parse tree transformations

root

root









v verb v verb
l



wha

r

adj




v verb
figure temporal clausal modifier extraction introduction rule

l matching
first matches l source tree sought l matched exists
one one node mapping function f l
node u l f u features feature values u variables
match lemma value f u
edge u v l edge f u f v dependency
relation
matching fails rule applicable example variable v matched
verb see n matched mary n matched john matching succeeds
following performed match found
r instantiation
copy r generated variables instantiated according matching node
l addition rule may specify alignments defined partial function l nodes
r nodes alignment indicates modifier source node
part rule structure subtree rooted copied modifier
target node addition explicitly defining alignments variable l implicitly
aligned counterpart r example alignment v nodes implies
yesterday modifying see copied generated sentence similarly
little modifying mary copied n
derived tree generation
let r instantiated r along descendants copied l alignment
l subtree matched l formalism two methods generating
derived tree substitution introduction specified rule type substitution
rules specify modification subtree leaving rest unchanged thus
formed copying replacing l descendants ls nodes r
case passive rule well lexical rules buy purchase
contrast introduction rules used make inferences subtree
parts ignored affect typical example inferring proposition
embedded relative clause case derived tree simply taken


fibar haim dagan berant

root


root




buy verb
subj



purchase verb

obj

subj

obj

v



v



john noun

books noun

john noun

books noun

john bought books

l

buy verb

john purchased books



purchase verb

r

figure application lexical substitution rule dotted arc represents explicit
alignment

r figure presents rule enables deriving propositions embedded
within temporal modifiers note derived tree depend main clause
applying rule right part figure yields proposition john saw little
mary yesterday
examples rule application
section illustrate rule representation application additional
examples
lexical substitution rule explicit alignment
figure shows derivation consequent john purchased books sentence
john bought books lexical substitution rule buy purchase example
illustrates role explicit alignment since buy purchase variables
implicitly aligned however need aligned explicitly otherwise daughters
buy would copied purchase
lexical syntactic introduction rule
figure illustrates application lexical syntactic rule derives sentence
husband died knew late husband defined introduction rule since
resulting tree derived solely phrase late husband ignoring
rest source tree example illustrates leaf variable l variable
leaf node may become non leaf r vice versa alignment
instances variable n matched husband allows copying modifier recall
alignments defined implicitly formalism note correctness
rule application may depend context applied instance
rule example correct late meaning longer alive given
context discuss context sensitivity rule application section


fiknowledge textual inference via parse tree transformations

root

root







know verb
subj

die verb

obj

subj

v



noun

husband noun
gen




husband noun

mod

v



noun

late adj

gen



noun

knew late husband

husband died

root


l



n noun

die verb



subj

mod

late adj



r

n noun

figure application lexical syntactic introduction rule

co reference trace inference
aside primary inference mechanism rule application formalism allows
inference co reference relations long distance dependencies view coreference equivalence relation complete subtrees within tree
different trees linked co reference chain practice relations
obtained external co reference resolution tool part text pre processing
co reference substitution operation similar application substitution rule
given pair co referring subtrees derived tree generated copying
tree containing replacing operation symmetrically
applicable example given sentences brother musician plays
drums infer brother plays drums
long distance dependencies another type useful relation inference illustrated following examples
relative clause boyi saw ti went home
saw boy
control verbs johni managed ti open door
john opened door
view co referring expressions substitutional found seminal van
deemter kibble noun phrases shown non substitutable evidence
co referring



fibar haim dagan berant

verbal conjunction johni sang ti danced
john danced
parsers including minipar use current work recognize annotate
long distance dependencies instance minipar generates node representing
trace ti examples holds pointer antecedent e g johni
shown examples inference sentences may involve resolving long distance
dependencies traces substituted antecedent thus generalize
co reference substitution operate trace antecedent pairs well mechanism
works together inference rule application instance substituting trace
antecedent obtain john managed john opened door
apply introduction rule n managed extract embedded clause john
opened door
polarity annotation rules
addition inference rules formalism implementation includes mechanism
adding semantic features parse tree nodes however many cases natural
way define semantic features classes hence often difficult agree right
set semantic annotations common example definition word senses
aim keep semantic annotation minimum sticking lexicalsyntactic representation widely agreed schemes exist
consequently semantic annotation employ predicate polarity feature
marks truth predicate may take one following values positive
negative unknown examples polarity annotation shown
john called mary
john hasnt called mary yet
john forgot call mary
john might called mary
john wanted call mary
sentences entail john didnt call mary hence negative annotation
call contrast truth john called mary cannot determined
therefore predicate call marked unknown general polarity predicates
may affected existence modals negation conditionals certain verbs etc
technically annotation rules right hand side r rather node l
may contain annotation features l matched tree annotations contains
copied matched nodes figure shows example annotation rule application
predicates assumed positive polarity default polarity rules used
mark negative unknown polarity one rule applies predicate
sentence john forgot call mary may applied order
following simple calculus employed combine current polarity polarity


fiknowledge textual inference via parse tree transformations

root


v
l





listen
subj

verb

verb





v



verb

john noun

verb

neg

neg



adj



adj
john listening

annotation rule

b annotated sentence

figure application annotation rule marking predicate listen negative
polarity b

current polarity





polarity











annotation rules used detecting polarity mismatches text hypothesis incompatible polarity would block hypothesis matched text
case approximate entailment classification polarity mismatches detected
annotation rules used features classifier discuss section
addition existence polarity annotation features may prevent inappropriate inference
rule applications blocking l matching discuss section
inference process
let set dependency trees representing text along co reference
trace information let h dependency tree representing hypothesis let r
collection inference rules including inference polarity rules
previously defined components inference framework next give procedural
definition set trees inferable r denoted r inference
process comprises following steps
initialize r
apply matching polarity rules r trees r cf section
replace trace nodes copy antecedent subtree cf section
add r trees derivable co reference substitution cf section


fibar haim dagan berant

apply matching inference rules r trees r cf section
add derived trees r repeat step iteratively newly added
trees trees added
steps performed h well h inferable r h r
since r may infinite large practical implementation process must
limit search space example restricting number iterations applied
rules iteration
inference rule applied polarity annotation propagated source
tree derived tree follows first nodes copied retain original
polarity second node gets polarity aligned node
template hypotheses
many applications useful allow hypothesis h template rather
proposition contain variables variables case existentially quantified entails h exists proposition h obtained h variable instantiation
entails h variable x instantiated replaced subtree sx x
modifiers h e x leaf become modifiers sx root obtained
variable instantiations may stand answers sought questions slots filled relation extraction example applying framework question answering setting
question killed kennedy may transformed hypothesis x killed kennedy
successful proof h sentence assassination kennedy oswald shook
nation would instantiate x oswald providing sought answer
limitations possible extensions
conclude section discussing limitations presented inference formalism
well possible extensions address limitations first inference rules match
single subtree therefore less expressive logic axioms used bos
markert tatu moldovan may combine several predicates
originating text representation well background knowledge
allows logic systems make inferences combine multiple pieces information
instance text says person x lives city background knowledge
tells us city country z infer x lives country z
rule person x location location z live x z live x z
schoenmackers etzioni weld davis describe system acquires rules
first order horn clauses web text allowing rules match multiple subtrees
well information background knowledge seems plausible future extension
formalism
another limitation formalism lack context disambiguation word sense
mismatch potential cause incorrect rule applications example rule hit
score applied correctly
step applied h since hypothesis typically short simple sentence usually
include co referring nps moreover presented formalism h single tree applying co referencebased inference would resulted additional trees inferred h thus would required
extending formalism accordingly



fiknowledge textual inference via parse tree transformations

team hit home run team scored home run
car hit tree car scored tree
several works past years addressed context dependent rule application dagan glickman gliozzo marmorshtein strapparava pantel bhagat
coppola chklovski hovy connor roth szpektor dagan bar haim
goldberger dinu lapata ritter mausam etzioni berant dagan
goldberger melamud berant dagan goldberger szpektor szpektor
et al proposed comprehensive framework modeling context matching termed
contextual preferences cp given text hypothesis h possibly template hypothesis inference rule r bridging h objects annotated
two context components global topical context b preferences constraints instantiation objects variables r template h cp requires
h r matched h matched r context component
matched counterpart szpektor et al proposed concrete implementations
components example could model global context
r sets content words compute semantic relatedness
two sets methods latent semantic analysis lsa deerwester dumais
furnas landauer harshman explicit semantic analysis esa gabrilovich
markovitch would expect semantic relatedness score
team home run much higher score car tree would
permit inference
rte systems including system rte experiments described
section lexicalized rules bridge h directly rules lhs
rhs matched h respectively since rte benchmarks h tend
semantic context setting alleviates context matching
extent however analysis presented later work subsection shows
context matching remains issue even setting expected become even
important chaining lexicalized rules attempted adding contextual preferences
formalism important direction future work
validity rule application depends monotonicity properties application site instance hypernym rule poodle dog applicable upward
monotone contexts monotonicity may affected presence quantifiers negation certain verbs implicatives counterfactives nairn condoravdi
karttunen common textual entailment systems assume upward monotonicity anywhere assumption usually holds true cases may lead
incorrect inferences following examples correct applications rule
upward monotone contexts incorrect applications downward monotone
contexts
bought poodle bought dog
didnt buy poodle didnt buy dog
poodles smart dogs smart
context matching textual entailment directional relation



fibar haim dagan berant

failed avoid buying poodle failed avoid buying dog
fail avoid buying poodle fail avoid buying dog
maccartney manning address monotonicity well semantic relations
exclusion natural logic framework syntactic representation
discuss work detail section
finally since polarity annotation rules applied locally may fail complex
cases computing polarity buying sentences polarity
information need propagated along syntactic structure sentence
truthteller system lotan stern dagan computes predicate polarity truth
value combination annotation rules global polarity propagation
extending previous work nairn et al maccartney manning
summary
section presented well formalized textual inference parsebased representations core framework semantic knowledge
represented uniformly inference rules specifying tree transformations provided
detailed definitions representation rules well inference mechanisms
apply formalism inferences co reference relations
traces addition includes annotation rules used detect contexts affecting
polarity predicates next section present efficient implementation
formalism

compact forest scalable inference
according formalism rule application generates sentence parse consequent semantically entailed source sentence inferred consequent may
subject rule applications straightforward implementation
formalism would generate consequent separate tree unfortunately nave
raises severe efficiency issues since number consequents may grow exponentially number rule applications consider example sentence children
fond candies following rules childrenkids candiessweets x
fond yx likes number derivable sentences including source sentence
would power set size rule applied independently
found exponential explosion leads poor scalability nave implementation
practice
intuitively would rule application add entailed part rule
e g kids packed sentence representation yet still want resulting structure
represent set entailed sentences rather mixture sentence fragments
unclear semantics discussed section previous work proposed partial solutions

section introduce novel data structure termed compact forest corresponding inference efficiently generate represent consequents
preserving identity individual one data structure allows compact representation large set inferred trees rule application generates explicitly


fiknowledge textual inference via parse tree transformations

nodes rules right hand side rest consequent tree shared source
sentence reduces number redundant rule applications explained later
section representation primarily disjunction edges
extension dependency edges specify set alternative edges multiple trees
since follow well defined inference formalism able prove inference
operations formalism equivalently applied compact forest compare
inference cost compact forests explicit consequent generation theoretically
illustrating exponential linear complexity ratio empirically showing improvement
orders magnitude empirical reported section
compact forest data structure
compact forest f represents set dependency trees figure shows example
compact forest containing trees sentences little mary seen john yesterday
john saw little mary yesterday first define general data structure
directed graphs narrow definition case trees
compact directed graph cdg pair g v e v set nodes e
set disjunction edges edges let set dependency relations edge
triple sd reld td sd td disjoint sets source nodes target
nodes reld sd function specifying dependency relation corresponds
source node graphically edges shown point nodes incoming edges
source nodes outgoing edges target nodes instance let bottommost
edge figure sd td candy sweet rel pcomp n
rel obj
edge represents si sd set alternative directed edges si tj tj
td labeled relation given reld si edges
termed embedded edge e edge would correspond different graph represented g
obj

obj

pcompn

previous example e edges likecandy likesweet ofcandy
pcompn
ofsweet definition implies source nodes sd set
alternative target nodes td edge called outgoing edge node v v sd
incoming edge v v td compact directed acyclic graph cdag
cdg contains cycles e edges
dag g rooted node v v cdag g embedded g derived
follows initialize g v alone expand v choosing exactly one target
node td outgoing edge v adding corresponding e edge
v g expansion process repeated recursively node added g
set choices different dag v root figure
may choose connect root left see resulting source passive
sentence right see resulting derived active sentence
compact forest f cdag single root r e r incoming edges
embedded dags rooted r trees set trees termed embedded
trees denoted f comprise set trees represented f
figure shows another example compact forest efficiently representing sentences resulting three independently applied rules presented beginning
section


fibar haim dagan berant

root

root





see

v

subj obj





mary

pcomp n

john

see

mod



subj obj

yesterday



mod

pcomp n

little

mod



yesterday

little

b variable instantiation

root

root





see
obj



mod

john

right hand side generation

subj

mary

see

see

see



subj

mod mod

see
mod

mod

obj

obj

subj


pcomp n

john

mary



yesterday



mod



yesterday
pcomp n

little

john

c alignment sharing

mary
mod

little

dual leaf variable sharing

figure step step construction compact forest containing source sentence little mary seen john yesterday sentence john saw little mary
yesterday derived via application passive rule figure b parts
speech omitted

inference process
next describe implementing inference process described section
compact forest henceforth compact inference illustrated figures b
passive active rule


fiknowledge textual inference via parse tree transformations

root



pred

fond



mod subj

subj
obj



child

kid
pcomp n

candy

sweet

figure compact forest representing sentences derivable sentence children fond candies following three rules childrenkids candiessweets
x fond yx likes

forest initialization
f initialized set dependency trees representing text sentences
roots connected forest root target nodes single edge dependency
edges transformed trivially edges single source target annotation
rules applied stage initial f figure without node labeled v
incoming edge corresponds initial forest containing single sentence
example
inference rule application
inference rule application comprises steps described summarized

l matching first matches rules lhs l forest f line
sake brevity omitted technical details l matching implementation
pseudocode following high level description matching
procedure focusing key algorithmic points
l matched f exists embedded tree f l matched
section denote l subtree l matched line


fibar haim dagan berant

input compact forest f inference rule e l r
output modified f denoted f f f set trees derived
applying e subset ls matches trees f
set matches l f
match f

l subtree f l matched according f












right hand side generation
sr copy r excluding dual leaf variable nodes
add sr f
sl l excluding dual leaf variable nodes
rr root sr
rl root l
e substitution rule
incoming edge rl set sr alternative sl
else introduction rule
outgoing edge root f set sr alternative trees f
add rr td







variable instantiation
variable x held node xr sr rs variables excluding dual leaves
x leaf l
xl f x node sl matched x
xr lemma xr polarity xl lemma xl polarity









else x leaf l matched whole target node set
xr lemma xr polarity n lemma n polarity node n f x
n f x n n
generate substitution rule n n n n aligned apply xr
x r instantiation n
u sl u aligned xr
add alignment u x r








alignment sharing
aligned pair nodes nl sl nr sr
nr polarity nl polarity
outgoing edge nl whose e edges part sl
add nr sd
reld nr reld nl






dual leaf variable sharing
dual leaf variable x matched node v l
incoming edge v
p parent node x sr







go p alternatives p generated variable instantiation
p set target nodes ps incoming edge
p p
add p sd
reld p relation x p

applying inference rule compact forest



fiknowledge textual inference via parse tree transformations

subtree may shared multiple trees represented f case rule
applied simultaneously trees section match example
v n n see mary john definition allow l scattered
multiple embedded trees matches constructed incrementally aiming add ls nodes
one one partial matches constructed far verifying candidate node
f node content corresponding edge labels match verified
match contain one e edge edge nodes f
indexed hash table enable fast lookup
target nodes edge specify alternatives position tree
parts speech expected substitutable assume target nodes
edge part speech polarity consequently variables
leaves l may match certain target node edge mapped whole
set target nodes td rather single node yields compact representation
multiple matches prevents redundant rule applications instance given compact
representation children kids fond candies sweets cf figure rule x
fond yx likes matched applied rather four times
combination matching x
right hand side generation given inference rule l r define dual leaf
variable variable leaf l r example n n
dual leaf variables passive active rule figure b variables
node r hence root leaf variables additional
alignments implicit alignment occurrences l r
considered dual leaves explained instantiations dual leaf variables
shared source target trees
right hand side generation step template sr line consisting r
excluding dual leaf variables generated inserted f line example
sr includes node v passive rules rhs similarly define sl l
excluding dual leaf variables line
case substitution rule example sr set alternative sl
adding sr root td incoming edge sl root line case
introduction rule set alternative trees forest adding
sr root target node set forest roots outgoing edge line figure
illustrates step example sr gray node labeled
variable v becomes additional target node edge entering original
left see
variable instantiation variable sr e non dual leaf instantiated lines
according match l section example v instantiated
see figure b lines specified variable sr leaf l
case example matched set nodes
instantiated sr lines decomposed sequence simpler
operations first sr instantiated representative set line
apply ad hoc lexical substitution rules creating node additional node
case current implementation coarse tag set minipar



fibar haim dagan berant

set line nodes addition usual alignment source nodes
sl lines share daughters sr due alignment n
n defined line
alignment sharing modifiers aligned nodes shared rather copied follows
given node nl sl aligned node nr sr outgoing edge nl
part l share nl nr adding nr sd setting
reld nr reld nl lines example figure c aligned nodes nl
nr left right see nodes respectively shared modifier yesterday
dependency relation mod copied right see node copy polarity annotation
nl nr line
note point instantiation variables dual leaves cannot
shared typically different modifiers two sides rule yet
modifiers part rule shared alignment operation
recall common variables considered aligned dual leaf variables
hand might shared described next since rule doesnt specify modifiers

dual leaf variable sharing final step lines performed similarly
alignment sharing suppose dual leaf variable x matched node v l whose
incoming edge simply add parent p x sr sd set reld p
relation p x r since v shared modifiers become shared
well implicitly implementing alignment operation subtrees little mary john
shared way variables n n figure ad hoc substitution rules
applied p variable instantiation phase generated nodes serve alternative
parents x thus sharing procedure applied p repeated
applying rule example added single node linked four edges
compared duplicating whole tree explicit inference
co reference substitution
section defined co reference substitution inference operation allows replacing subtree co referring subtree operation implemented generating
fly substitution rule applying implementation
initial compact forest annotated co reference relations obtained external
co reference resolution tool substitutions performed prior rule applications
substitutions pronoun ignored usually useful
correctness
section present two theorems proving inference process presented
valid implementation inference formalism provide full proofs appendix
theorem argue applying rule compact forest compact
forest since begin valid compact forest created initialization step follows
induction sequence rule applications inference process
compact forest fact embedded dags generated inference
process indeed trees trivial since nodes generally many incoming e edges


fiknowledge textual inference via parse tree transformations

many nodes however pair parent nodes cannot part
embedded dag example figure node candy incoming
e edge node node however nodes
part embedded dag edge emanating root
forces us choose node node thus see reason
correctness local two incoming e edges leaf node candies cannot
embedded dag rule applied root tree turn
theorem proof scheme
theorem applying rule compact forest compact forest
proof scheme prove applying rule compact forest creates cycle
embedded dag tree cycle non tree dag already existed
prior rule application contradicts assumption original structure
compact forest crucial observation proof directed path
node u node v passes sr u v outside sr
analogous path u v passes sl instead
next theorem main argue inference process compact
forest complete sound generates exactly set consequents derivable
text according inference formalism
theorem given rule base r set initial trees tree represented
compact forest derivable inference process consequent according
inference formalism
proof scheme first completeness induction number explicit rule
applications let tn tree derived tree tn rule rn according
inference formalism inductive assumption determines tn embedded
derivable compact forest f easy verify applying rn f yield compact
forest f tn embedded
next soundness induction number rule applications
compact forest let tn tree represented derived compact forest fn tn
f n fn derived compact forest fn rule rn inductive
assertion states trees f n consequents according formalism
hence tn already f n consequent otherwise shown
exists tree tn f n applying rn tn yield tn according
formalism tn consequent according inductive assertion therefore
tn consequent well
two theorems guarantee compact inference process valid
yields compact forest represents exactly set consequents derivable given
text given rule set


fibar haim dagan berant

complexity
section explain compact inference exponentially reduces time space
complexity typical scenarios
consider set rule matches tree independent matched left handsides excluding dual leaf variables overlap application
chained order example three rule matches presented figure
independent
let us consider explicit inference first assume start single tree k
independent rules matched applying k rules yield k trees since subset
rules might applied therefore time space complexity applying k
independent rule matches k applying rules newly derived consequents
behaves similar manner
next examine compact inference applying rule compact inference adds
right hand side rule shares existing edges since size
right hand side number outgoing edges per node practically bounded
low constants applying k rules tree yields linear increase size forest
thus resulting size k see figure
time complexity rule application composed matching rule forest
applying matched rule applying matched rule linear size matching
rule size r forest f takes f r time even performing exhaustive
search matches forest since r tends quite small bounded
low constant already gives polynomial time complexity furthermore matches
constructed incrementally step aim extend partial matches found
due typical low connectivity forest well constraints imposed
rule lemma pos dependency relation number candidates extending
matches step f candidates retrieved efficiently
proper indexing thus matching procedure fast practice illustrated
empirical evaluation described section
related work packed representations
packed representations nlp tasks share common principles underlie
compact forest factoring common substructures representing choice local
disjunctions applying general scheme individual typically requires specific representations depending type alternatives
represented specified operations creating create alternatives rule
application newly derived subtree set alternative existing subtrees
alternatives specified locally edges
packed chart representations parse forests introduced classical parsing cyk earley jurafsky martin extended later
work purposes maxwell iii kaplan kay alternatives
parse chart stem syntactic ambiguities specified locally possible decompositions phrase sub phrases
rte system average rule lhs size found nodes maximal size
nodes experimental setting described section applied rte test set



fiknowledge textual inference via parse tree transformations

packed representations utilized transfer machine translation
emele dorna translated packed source language representation packed target
language representation avoiding unnecessary unpacking transfer unlike
rule application work transfer rules preserve ambiguity stemming source
language rather generating alternatives mi et al applied statistical
machine translation source language parse forest rather best parse
transfer rules tree string contrary tree tree rules chaining
attempted rules applied single top pass source forest thus
representation quite different

incorporated knowledge bases
section describe knowledge bases used inference engine
first describe novel rule base addressing generic linguistic structures rule base
composed manually formalism includes inference rules section
polarity annotation rules section addition derived inference rules
several large scale semantic resources section overall variety illustrates
suitability formalism representing diverse types inference knowledge
inference rules generic linguistic phenomena
rules capture inferences associated common syntactic structures
summarized table rules three major functions
simplification canonization source tree categories table
extracting embedded propositions categories
inferring propositions non propositional subtrees source tree category
inference rules merely extract subtree source tree without changing
structure relative clause rule useful exact inference aims generate
hypothesis used evaluation inferences cf section however currently implemented approximate classification features focused matching
substructures hypothesis forest described section hence
take advantage extractions therefore rules excluded rest
experiments reported sections
rules categories depend solely syntactic structure closed class words
referred generic rules contrast verb complement extraction rules category
considered lexicalized rules since specific certain verbs replace forced
advised example entailment would hold extracted parc
polarity lexicon nairn et al list verbs allow inference appearing
positive polarity contexts generated inference rules verbs list
complemented reporting verbs say announce since information
news domain rules applied experiments cf section
often given reported speech speaker usually considered reliable
sidestep issue polarity propagation applying rules main
clause implemented including tree root node rule lhs


fibar haim dagan berant




category
conjunctions



clausal extraction
connectives
relative
clauses





appositives



determiner
canonization



passive



genitive
modifier



verb complement clause
extraction

example source
helenas experienced
played long time
tour
celebrations muted
many iranians observed
shiite mourning month
assailants fired six bullets car carried
vladimir skobtsov
frank robinson onetime manager indians distinction
nl
plaintiffs filed lawsuit last year u district
court miami
approached
investment banker
malaysias crude palm oil
output estimated
risen six percent
yadav forced resign

example derived
helena played long
time tour
many iranians observed
shiite mourning month
car carried vladimir
skobtsov
frank robinson onetime manager indians

plaintiffs filed lawsuit last year u district
court miami
investment banker approached us
crude palm oil output malaysia estimated
risen six percent
yadav resigned

table inference rules generic linguistic structures

embedded clause extracted becomes main clause derived tree rules
extract embedded clauses polarity verb detected applying
annotation rules described next verb annotated negative unknown
polarity matching complement extraction rules fails example last sentence
table yadav forced resign forced would annotated negative
polarity consequently matching corresponding complement extraction rule
would fail yadav resigned would entailed hence annotation rules may block
erroneous inference rule applications polarity important correct application
rules case rule types passive active transformation
therefore checked polarity matching rule application exact inference
experiment section verb complement extraction rules used leave
analysis polarity dependence rules future work
polarity annotation rules
use annotation rules mark negative unknown polarity predicates cf section table summarizes polarity inducing contexts address inference rules annotation rules comprise generic rules categories lexicalized


fiknowledge textual inference via parse tree transformations




category
explicit negation





implied negation
modal auxiliaries
overt conditionals





verb complements
adjectives
adverbs

example
weve never seen actual costs come

one stayed last lecture
could eat whale
venus wins game meet sarena
finals
pretend know calculus
impossible survived fall
probably danced night

table polarity annotation rules

rules categories verb complement embedded clause negative unknown
polarity extracted however polarity annotated category compare
category table list verbs imply negative unknown polarity
clausal complements taken parc lexicon well verbnet kipper

lexical lexical syntactic rules
addition manually composed generic rules system integrates inference knowledge variety large scale semantic resources introduced section information derived resources represented uniformly inference rules
formalism examples rules shown table following resources
used
wordnet extracted wordnet fellbaum lexical rules synonym hyponym word entailed hyponym e g dog animal instance
hyponym derivation relations
wikipedia used lexical rulebase shnarch et al extracted rules
janis joplin singer wikipedia metadata e g
links redirects text definitions patterns x
dirt dirt lin pantel learns corpus inference rules
binary predicates example x fond yx likes used
version learns canonical rule forms szpektor dagan
argument mapped wordnet amwn resource inference rules predicates covering verbal nominal forms szpektor dagan includ according wordnet glossary instance proper noun refers particular unique
referent distinguished nouns refer classes specific form hyponym
example ganges instance river
addition extraction methods described shnarch et al employed two additional
methods first extraction entailments among terms redirected page second
generalization rules rhs common lhs head different modifiers instance
rules ferrari f car ferrari ascari car generalized ferrari car



fibar haim dagan berant

ing argument mapping wordnet nomlex plus meyers
et al verified statistically intersection unary dirt szpektor dagan amwn rules defined unary templates
example kill xx die
automatically extracted inference rules lack two attributes defined formalism rule type substitution introduction explicit alignments beyond alignments
rs variables l counterparts defined default attributes added automatically following heuristics
roots l r part speech substitution rule
e g x buy sold x otherwise e g ys acquisition x
sold x introduction rule
roots l r assumed aligned
note application rules e g wordnet derivations
rules learned dirt valid parse tree rules
used aiming exact derivation h however may useful
inference engine used together approximate matching component
rte system approximate matcher described section employs features
coverage words subtrees h f therefore benefit
inferences rules preferably applied last step inference
process avoid cascading errors

evaluation
section present empirical evaluation entailment system whole
well evaluation individual components evaluate quality systems
output terms accuracy precision recall computational efficiency terms
running time space application settings
first evaluate knowledge inference engine section describe
experiment engine aims prove simple template hypotheses representing
binary predicates texts sampled large corpus next section evaluate
efficiency engine implementation compact forest data structure
evaluate complete entailment system including approximate entailment classifier
section finally sections provide depth analysis performance
inference component rte data
proof system evaluation
experiment evaluate inference engine finding strict proofs
inference process must derive precisely target hypothesis instantiation
case template hypotheses contain variables defined section
thus evaluate precision text hypothesis pairs complete proof
chain found available rules note pascal rte datasets
suitable purpose rather small datasets include many text hypothesis pairs


fiknowledge textual inference via parse tree transformations

available inference rules would suffice deriving complete proofs furthermore
since focus applied textual inference inference engine
evaluated nlp application setting texts represent realistic distribution
linguistic phenomena manually composed benchmarks fracas test suite
cooper et al contains synthetic examples specific semantic phenomena
clearly suitable evaluation
alternative chose relation extraction setting complete
proofs achieved large number corpus sentences setting system
needs identify pairs arguments sentences target semantic relation e g x buy

system configuration
experiment first reported bar haim et al used earlier
version engine rule bases engine experiment make use
compact forest rather generates consequent explicitly polarity annotations
propagated source derived trees instead polarity annotation rules
applied original text inferred consequent prior application
inference rule following rule bases used experiment
generic linguistic rules used generic rule base presented section including inference polarity annotation rules early version include
lexicalized polarity rules derived verbnet parc lexicon category
table
lexical syntactic rules nominalization rules inference rules xs acquisition
x acquired capture relations verbs nominalizations
rules derived automatically ron nomlex hand coded database
english nominalizations macleod grishman meyers barrett reeves
wordnet
automatically learned rules used dirt paraphrase collection well
output tease szpektor et al another unsupervised learning
lexical syntactic rules tease acquires entailment relations web given
input template identifying characteristic variable instantiations shared
templates provide ranked list output templates given input
template learned rules linguistic paraphrases e g x confirm x
approve others capture world knowledge e g x buy x
learn entailment direction rule reduces accuracy
applied given direction system considered top bi directional
rules learned template
generic default rules rules used define default behavior situations
case case rules available used one default rule allows removal
modifiers nodes ideally rule would replaced future work
specific rules removing modifiers


fibar haim dagan berant

evaluation process
use sample test template hypotheses correspond typical relations
x approve identify large test corpus sentences instantiation
test hypothesis proved example sentence budget approved
parliament found prove instantiated hypothesis parliament approve budget
via passive active inference rule finally sample candidate sentenceshypothesis pairs judged manually true entailment repeated process compare
different system configurations
since publicly available sample output tease much smaller
resources randomly selected resource transitive verbs may correspond
typical predicates formed test templates adding subject object varisubj

able nodes example verb accuse constructed template xnoun
obj

accuse verb ynoun
test template h identify sentences corpus template
proved system efficiently proof chains generate h corpus
sentences combine forward backward breadth first searches available
rules first use backward search lexical syntactic rules starting rules
whose right hand side identical test template process backward chaining
dirt tease nominalization rules generates set templates ti
proving deriving h example hypothesis x approve may generate
template x confirm backward application dirt tease rule
generate template confirmation x nominalization rule
since templates ti generated lexical syntactic rules modify open class
lexical items may considered lexical expansions h
next specific ti generate search engine query composed open class
words ti query fetches candidate sentences corpus ti might
proven generic linguistic rules recall rules modify openclass words end use forward search applies generic rules starting
candidate sentence trying derive ti sequence rule applications
successful variables ti instantiated cf section consequently know
variable instantiations h proven since derives ti turn
derives h
performed search sentences prove test template
reuters rcv corpus cd applying minipar parsing random sampling
obtained sentences prove according tested system configuration
test templates yielding total pairs sentence instantiated hypothesis four tested configurations described pairs overall
pairs split entailment judgment two human annotators graduate students
bar ilan nlp group annotators achieved sample shared exam output tease dirt well many knowledge resources available rte
knowledge resources page
http aclweb org aclwiki index php title rte knowledge resources
verbs approve consult lead observe play seek sign strike



fiknowledge textual inference via parse tree transformations







configuration
baseline embed h anywhere
proof embed h root
proof generic
proof generic lexical syntactic

precision





yield





table proof system evaluation

ples agreement level kappa value corresponding substantial
agreement

tested four configurations proof system
baseline baseline configuration follows prominent graph
entailment systems system tries embed given hypothesis anywhere
candidate sentence tree negative unknown polarity detected
annotation rules may block embedding
proof configuration h strictly generated candidate sentence inference rule available default rule removing modifiers
polarity annotation rules active baseline configuration equivalent
embedding h root h matched root since modifiers
part match removed default rule however
h embedded elsewhere extracted opposed baseline
configuration
proof generic proof plus generic linguistic rules
proof generic lexical syntactic previous configuration plus
lexical syntactic rules
system configuration measure precision percentage examples judged
correct entailing average extrapolated yield expected number
truly entailing sentences corpus would proven system
extrapolated yield specific template calculated number sample sentences
judged entailing multiplied sampling proportion average calculated
test templates note similar ir evaluations possible compute
true recall setting since total number entailing sentences corpus
known recall equal yield divided total however straightforward
measure relative recall differences among different configurations yield thus
two measures estimated large corpus possible conduct robust
comparison different configurations reliably estimate impact different
rule types analysis possible rte datasets rather small
hand picked examples represent actual distribution linguistic phenomena


fibar haim dagan berant

reported table first comparing proof
baseline observe requirement matching h root e
main clause rather allowing matched anywhere improves
precision considerably baseline reducing yield nearly
proof configuration avoids errors resulting improper extraction embedded
clauses
remarkably generic inference rules system able gain back lost
yield proof surpass yield baseline configuration addition
obtain higher precision baseline difference statistically
significant p level z test proportions demonstrates
principled proof appears superior heuristic baseline embedding
exemplifies contribution generic rule base overall generic rules
used proofs
adding lexical syntactic rules increased yield factor six shows
importance acquiring lexical syntactic variability patterns however precision
dirt tease currently quite low causing overall low precision manual filtering
rules learned systems currently required obtain reasonable precision
error analysis revealed third configuration proof generic rules
significant errors due parsing errors notably incorrect dependency
relation assignment incorrect pos assignment incorrect argument selection incorrect analysis complex verbs e g play text vs play hypothesis ungrammatical sentence fragments another errors represent conditionals negation
modality phenomena could handled additional rules making use elaborate syntactic information verb tense remaining
rather small errors represent truly ambiguous sentences would require
considerable world knowledge successful analysis
compact forest efficiency evaluation
next evaluate efficiency compact inference cf section setting recognizing textual entailment rte rte datasets giampiccolo et al
datasets consist text hypothesis pairs need classified
entailing non entailing first experiment generic inference rule set shows
compact inference outperforms explicit inference efficiency wise orders magnitude section second experiment shows compact inference scales well
full blown rte setting several large scale rule bases hundreds rules
applied per text section
compact vs explicit inference
compare explicit compact inference randomly sampled pairs rte
development set parsed text pair minipar lin avoid
memory overflow explicit inference applied sentences subset
generic inference rules described section fair comparison aimed make
explicit inference implementation reasonably efficient example preventing multiple
generations tree different permutations rule applications


fiknowledge textual inference via parse tree transformations

time msec
rule applications
node count
edge endpoints

compact





explicit





ratio





table compact vs explicit inference generic rules averaged per
text hypothesis pair

configurations perform rule application iteratively matches found
iteration first rule matches apply matching rules compare run
time number rule applications overall generated size nodes edges
edge size represented sum endpoints regular edge sd td
edge
summarized table expected compact
inference orders magnitude efficient explicit inference avoid memory
overflow inference terminated reaching nodes three test
texts reached limit explicit inference maximal node count compact
inference number rule applications reduced due sharing
common subtrees compact forest single rule application operates
simultaneously large number embedded trees suggest scaling
larger rule bases longer inference chains would feasible compact inference
prohibitive explicit inference
application rte system
goal second experiment test compact inference scales well broad
inference rule bases experiment used bar ilan rte system bar haim et al
system operates two primary stages
inference inference rules first applied initial compact forest f aiming bring
closer hypothesis h experiment use knowledge bases
described section overall rule bases contain millions rules
current system implemented simple search strategy spirit
de salvo braz et al first applied three exhaustive iterations generic
rules since rules low fan possible right hand sides given
left hand side affordable apply chain freely iteration
first rule matches apply matched rules avoid repeated
identical rule applications mark newly added nodes iteration
next iteration consider matches containing nodes perform single
iteration lexical lexical syntactic rules applying l
part matched f r part matched h investigation
effective search heuristics representation left future
classification following inference set features extracted resulting f
h fed svm classifier determines entailment describe


fibar haim dagan berant

rule applications
node count
edge endpoints

rte dev
avg max






rte
avg max






table application compact inference rte dev rte datasets
rule types

classification stage detail next section discusses performance
rte system
table provides statistics rule applications rule bases rte
development set rte dataset overall primary compact
forest indeed accommodates well extensive rule applications large scale rule bases
resulting forest size kept small even maximal cases causing memory
overflow explicit inference
complete rte system evaluation
previous sections evaluated knowledge inference engine proof system respect quality output precision recall well computational
efficiency time space evaluate complete rte system combines
inference engine approximate classification module
classification setting features quite typical rte literature features broadly categorized two subsets lexical features solely depend
lexical items f h b lexical syntactic features take account
syntactic structures dependency relations f h brief description
features complete description appears rte system report bar haim et al

lexical features coverage features check words h present covered f
assume high degree lexical coverage correlates entailment
features measure proportion uncovered content words verbs nouns adjectives
adverbs named entities numbers polarity mismatch features detect cases
nouns verbs h matched f incompatible polarity
features assumed indicate non entailment
edge coverage features say edge h matched f edge
f matching relation source node target node say edge h
loosely matched path f matching source node matching
target node definitions extract two features proportion h
edges matched loosely matched f
running time included since dedicated rule fetching rather slow
available implementation resources elapsed time seconds per h pair
look subset edges labeled relevant dependency relations



fiknowledge textual inference via parse tree transformations

predicate argument features f entails h predicates h matched
f along arguments predicates include verbs except verb
subject complements copular sentences example smart joseph smart
arguments daughters predicate node h four features computed
f h pair categorize every predicate h match f one
four possible categories
complete match matching predicate exists f matching arguments
dependency relations
partial match matching predicate exists f matching arguments
dependency relations
opposite match matching predicate exists f matching arguments
incorrect dependency relations
match matching predicate f matching arguments
predicate categorized complete match category
finally compute four features f h pair proportion predicates
h complete match f three binary features checking
predicate h categorized partial match opposite match match since
subject object arguments crucial textual entailment compute four
similar features subset predicates arguments ignoring
arguments
global lexical syntactic feature feature measures well subtrees h
covered f weighted according proximity root h feature
somewhat similar dependency tree kernel collins duffy
measures similarity two dependency trees counting common
subtrees however measure several distinct properties makes suitable
needs directional measure estimating coverage h f
vice versa b operates compact forest tree rather pair
trees c takes account distance root h assuming nodes
closer root important
system trained rte development set tested rte
rte test sets development set released rte co reference substitution
disabled due insufficient accuracy co reference resolution tool used
first report overall performance provide analysis inference module
focus work
accuracies obtained experiment shown table inference
column rte quite competitive compared teams
participated rte scored higher three systems
scored rte rank teams
scoring higher overall system well situated
state art rte task
table provides detailed view systems performance precision recall
f given entailing non entailing pairs well overall accuracy
dependent preposition clause take complement preposition head
clause respectively dependent



fibar haim dagan berant

table shows per task ie ir qa sum overall system tends
predict entailment often non entailment recall entailing pairs much
higher recall non entailing pairs precision non entailing pairs
much higher entailing pairs performance varies considerably among different tasks
rte accuracy qa ir considerably higher average
achieved rte submissions reported organizers giampiccolo et al
respectively ie sum bit average
rte better ir sum seem easier
tasks rte giampiccolo et al
usage contribution knowledge bases
evaluate accuracy gain knowledge inference ran system
inference module disabled entailment classification applied directly initial
parse tree text shown inference column table
comparing full system accuracy inference see applying
inference module resulted higher accuracy test sets contribution
prominent rte dataset illustrate typical contribution current
knowledge sources current rte systems contribution likely increase
current near future topics extending improving knowledge
resources applying semantically suitable contexts improved classification
features broader search strategies
tables illustrate usage contribution individual rule bases table
shows distribution rule applications rule bases table presents
ablation study showing marginal accuracy gain rule base
rule bases applicable large portion pairs contributes
overall accuracy note highly dependent search
strategy instance chaining lexical rules expected increase number lexical
rule applications reduce accuracy provide detailed analysis rule
applications system next section
manual analysis
conclude evaluation two manual analyses inference component within
rte system first analysis subsection assesses applicability inference
framework rte task well actual coverage current system
categorizes cases formalism falls short subsection assess
correctness applied rules analyze causes incorrect applications
analyses done one authors randomly sampled subsets rte
test set

according rte organizers ie task appeared difficult task sum
ir seemed easier tasks however report average accuracy per task



fiknowledge textual inference via parse tree transformations

test set
rte
rte

accuracy
inference inference









lexical
overlap



best rte




table inference contribution rte performance system trained rte development set indicates statistically significant difference level p
mcnemars test best achieved rte rte challenges hickl
bensley bensley hickl well lexical overlap baseline mehdad
magnini given reference mehdad magnini tested eight
configurations lexical overlap baselines chose one performs best average
rte test sets

rte

rte

task
ie
ir
qa
sum

ie
ir
qa
sum


non entailing pairs
precision recall
f





















entailing pairs
precision recall
f





















accuracy











table rte breakdown task pair type

rule base
wordnet
amwn
wikipedia
dirt
generic
polarity

rte dev
rules app












rte
rules app












table average number rule applications per h pair rule base app counts
rule application rules ignores multiple matches rule
iteration

applicability coverage
analysis assesses ability inference framework derive complete proofs
rte h pairs idealized setting perfect knowledge bases co reference
resolution available provides upper bound coverage inference


fibar haim dagan berant

rule base
wordnet
amwn
wikipedia
dirt
generic
polarity

accuracy rte







table contribution rule bases accuracy loss rte obtained
removing rule base ablation tests

engine similar analysis previously done bar haim szpektor glickman
subset rte dataset however go assess
actual coverage required inferences implemented rte system b present
classification uncovered cases different categories
carried analysis follows positive entailing pairs randomly
sampled rte test set pair aimed manually derive proof
comprising inference steps expressible formalism similar example
section complete proof could derived pair classified inferable
otherwise classified one following categories
discourse references complete proof requires incorporating pieces information
discourse including event co reference bridging mirkin et al nominal co reference substitution included covered formalism
instance text titanics sinking hitting iceberg april
year explicitly specified time titanics sinking
relation derived discourse order infer hypothesis
titanic sank
non decomposable inference cannot reasonably decomposed sequence
local rewrites case example text black plague lasted
four years killed one third population europe approximately
million people hypothesis black plague swept europe
cases fall categories
distribution categories shown table found
pairs could proven formalism given appropriate inference rules co reference
information demonstrates utility somewhat
higher reported bar haim et al may attributed
fact rte considered difficult dataset entailment systems consistently
perform better rte
remaining pairs analysis highlights significance discourse
references occur pairs previous analysis discourse references
textual entailment applied rte search task text sentences
interpreted context full discourse mirkin et al analysis shows


fiknowledge textual inference via parse tree transformations

category
inferable
non decomposable
discourse references


count











table applicability inference framework rte task randomly selected
entailing pairs rte test set analyzed

significance discourse references even short self contained texts rte composed mirkin et al framework similar methods
tree transformations extended utilize discourse references several works
last years targeted implied predicate argument relationships notable
semeval task linking events participants discourse
ruppenhofer sporleder morante baker palmer particular stern dagan
recently showed identifying relations improves performance
rte system finally entailment pairs could established
sequence local rewrites thus cases likely require deeper methods semantic
analysis inference
manually derived proofs inferable pairs included total rule applications average rule applications per pair maximal number rules per
pair rules applied system proofs
inferable pairs fully derived rte system partial proofs derived
additional pairs remaining pairs system apply
rules manual proof demonstrate utility inference
mechanisms rule bases system hand suggest still
much room improvement coverage existing rule bases
correctness applied rules
next assess correctness rules applied inference engine focus
four lexical lexical syntactic rule bases described section wordnet wikipedia
dirt argument mapped wordnet amwn except wordnet rule bases
generated automatically therefore accuracy issue accuracy
manually composed generic inference rules polarity annotation rules furthermore lexicalized rules often context sensitive additional potential source
incorrect rule applications
evaluation randomly sampled pairs rte test set analyzed
lexical lexical syntactic rule applications performed system pairs
total rule applications define two levels rule application correctness

previously mentioned rte system apply rules merely extract subtree
given source tree accordingly rules ignored analysis well



fibar haim dagan berant

propositional derived tree resulting rule application grammatical
entailed source tree level correctness assumed
formalism
referential case propositional correctness hold turn weaker criterion referential correctness following notion lexical reference glickman
shnarch dagan shnarch et al extend case
template rules variables let rule e l r inference rule matched
source tree let l r instantiations l r respectively according
variable matching l say referential correctness holds l generates reference possible meaning r examples rules found
analyzed sample popepapal turkishturkey fishermenfishing
rule applications valid entailed tree still useful
context rte system applies approximate matching previously
discussed end section
incorrect rule applications classified one following categories
bad rule rule priori incorrect e g walesyear
bad context rule incorrect context source sentence example
wordnet rule strikecreate corresponds rare sense strike defined
produce ignition blow strike fire flint stone
bad match rule applied due incorrect matching left hand side
resulting incorrect parse source tree
summarized table overall rule applications correct
interestingly referential propositional rule applications unsurprisingly accurate knowledge resource manually composed
wordnet correct applications followed amwn wikipedia
rule bases derived automatically human generated resources
least accurate resource dirt makes use human knowledge engineering rather learned automatically corpus statistics accuracy dirt
considerably lower accuracy resources substantially decreasing
overall accuracy well errors dirt wikipedia due bad rules
overall dominant cause incorrect applications wordnet
amwn priori rule quality high errors due bad context wikipedia rules suffer bad context explained fact
left hand side often unambiguous named entity madrid antelope valley
freeway microsoft office analysis highlights need improving accuracy
automatically generated rule bases whose quality still far human generated resources analysis shows context sensitivity lexicalized rules still issue
even rules applied conservatively experiment chaining l
r matched f h addressed future


fiknowledge textual inference via parse tree transformations

rule applications
propositional
referential
correct
bad rule
bad context
bad matching
incorrect

dirt









amwn









wikipedia









wordnet



















table analysis lexical lexical syntactic rule applications

discussion comparison related approaches
section compare work several closely related inference methods
described section
discourse commitments derived hickl quite similar kind consequents generate applying syntactic lexical syntactic co reference substitution rules however work differs hickls several respects first foremost
hickls work fully describe knowledge representation inference framework
main focus work hickl briefly mentions commitments
generated probabilistic fst extraction framework explanations examples given second framework allows unified modeling
variety inference types addressed tools components hickls
system fst relation extraction paraphrase acquisition etc addition system
operates lexical syntactic representations rely semantic parsing finally consequents generated formalism packed efficient data structure
whereas hickls commitments generated explicitly discuss commitment
generation efficiency noted however explicit generation commitments restricts search space may simplify approximate matching e g finding
alignment h given consequent vs aligning h whole compact forest
de salvo braz et al presented semantic inference framework augments
text representation right hand side applied rule respect
similar however work rule application semantics
resulting augmented structure fully specified particular distinction
individual consequents lost augmented graph contrast compact
inference fully formalized proved equivalent expressive well defined
formalism operating individual trees inferred consequent recovered
compact forest
maccartney manning proposed model natural language inference
similar framework operates directly parse representations work extends previous work natural logic valencia focused semantic containment monotonicity incorporating semantic exclusion implicativity model
inference h sequence atomic edits thought generating
intermediate premise calculus computes semantic relation source


fibar haim dagan berant

derived premise propagating semantic relation local edit upward
parse tree according properties intermediate nodes example
correctly infer first year students arrived students arrived
every first year student arrived every student arrived composition semantic relations along inference chain yields semantic relation holding
h contribution complementary approaches inference
h modeled sequence atomic steps rule applications edits focus
framework representation application diverse types transformations
needed textual inference well efficient representation possible inference chains
application inference rule assumed generate entailed consequent
polarity rules may used detect situations assumption hold
block rule application comparison formalism maccartney manning assumes
rather simple edit operations focused precise predication semantic relation
h given sequence edits transform h thus combining
two complementary approaches natural direction future

conclusion
subject work representation use semantic knowledge textual
inference lexical syntactic level defined novel inference framework parse
trees represents diverse semantic knowledge inference rules proof process
aims transform source text target hypothesis sequence rule
applications generating intermediate parse tree complementary contribution
work novel data structure associated rule application
proved valid implementation inference formalism illustrated inference
efficiency analytically empirically
several advantageous properties first ability represent
apply wide variety inferences combine rule chaining makes framework expressive previous rte architectures second expressive
power obtained well formalized compact framework unified knowledge
representation inference mechanisms finally shown rte experiments
compact forest data structure allows scale well practical settings
involve large rule bases hundreds rule applications per text hypothesis pair
demonstrated utility two different semantic tasks experiments unsupervised relation extraction showed exact proofs outperform
heuristic common practice hypothesis embedding achieved competitive
rte benchmarks adding simple approximate matching module
inference engine contribution semantic knowledge illustrated tasks
limitations possible extensions formalism discussed section
manual analysis inference engines performance relation extraction rte
tasks suggested promising directions future discussed subsections
two additional major areas approximate matching
heuristics proof search strategy stern dagan stern stern dagan
felner extended work address two aspects respectively


fiknowledge textual inference via parse tree transformations

acknowledgments
article doctoral dissertation first author completed
guidance second author bar ilan university bar haim
work partially supported israel science foundation grants
ist programme european community pascal network excellence ist pascal network excellence european community
fp ict israel internet association isoc il grant
fbk irst bar ilan university collaboration third author grateful azrieli
foundation award azrieli fellowship authors wish thank cleo condoravdi making polarity lexicon developed parc available
grateful eyal shnarch help implementing experimental setup described
section thank iddo greental collaboration developing generic rule
base finally would thank dan roth idan szpektor yonatan aumann marco
pennacchiotti marc dymetman anonymous reviewers valuable feedback
work

appendix compact forest complete proofs
section provide complete proofs correctness compact inference
presented section start definitions
definition let l r rule matched applied compact forest f section let l subtree represented tree f l matched recall
sl defined l excluding nodes matched dual leaf variables similarly sr
defined copy r without dual leaf variables generated inserted
f part rule application roots sl sr denoted rl rr respectively
say node sr tied node sl set source node one
outgoing edges due alignment sharing dual leaf variable sharing
graph operations performed applying rule l r compact forest f
summarized follows
adding subtree sr f
setting rr target node edge f
setting nodes sr tied nodes sl source nodes edges f
according rules variable sharing dual leaf variable sharing recall
edges part sl
first simple property cdgs generated inference process
lemma every node cdg generated inference process one incoming edge


fibar haim dagan berant

proof construction initial forest node one incoming edge
rule application adds subtree sr whose nodes one incoming edge
last root rr initially incoming edges set target single
edge rule application incoming edge rl therefore lemma follows
induction number rule applications
following theorem inference process generates compact
forest
theorem applying rule compact forest compact forest
proof let f cdg generated applying rule l r compact forest f
f compact forest cdag single root r
embedded dags rooted r trees first f cdag e
contain cycle e edges
assume contradiction f contains simple cycle e edges c applying
rule l r add e edges nodes f therefore c must pass
rr root sr contain e edge p rr since sr tree c must leave sr
e edge u v u sr v
sr cycle written p rr
u v p notice path v p fully contained f since cycle
c simple entering sr possible rr
l r must substitution rule otherwise p would root f
impossible since root incoming edges therefore rr rl
single incoming edge e edge p rl exists f addition u added
source node edge f since tied u sl source node
therefore path rl u v exists f finally know path v
p fully contained f therefore construct cycle p rl u v p
f contradiction assumption f compact forest
shown f cdag next define generalization embedded dags
help us embedded dags f rooted r trees
definition embedded partial dag g v e cdag g rooted node v v
similar embedded dag generated following process
initialize g v alone
repeat number iterations
choose node v
b choose outgoing edge already chosen previous
iteration edges chosen halt
c choose target node td add e edge g
embedded partial dags f rooted node trees since
embedded dag embedded partial dag proves embedded dags
f rooted r trees assume contradiction applying l r


fiknowledge textual inference via parse tree transformations

embedded partial dag rooted node n tree assume n
sr otherwise extend adding path p rr n p
node outside sr source node incoming edge rr
since tree two simple paths p p n reach
node z two different e edges z cannot sr since two paths meet
subtree sr must first meet root rr entering incoming edge however could
construct f two paths selecting rl instead rr contradiction
assumption f compact forest clearly p p must pass
subtree sr otherwise two paths already existed f
first handle case without loss generality p passes sr
p p passes sr contains e edge p rr since z
sr
contains e edge u v u sr v
sr p written
n p rr u v z paths n p v z
f way enter sr rr p simple
incrementally construct f following embedded partial dag first construct
p section p n p next expand p e edge p rl
instead p rr would expand rl reach z possible
previously explained u tied node u sl therefore e edge u v exists
f therefore path p sl rl u v z however
guaranteed whole p added try expand incrementally
p step adding next e edge path succeed embedded
graph f two paths z contradiction fail due e edge
z p cannot add thus z must already p node
two distinct paths embedded graph contradiction path constructed
indeed different p since contains e edge p rl cannot part p since
p contains disjoint edge p rr
remaining case p p pass sr reach node z
sr p
written n u v z p n u v z
u u sr v v
sr assume first e edges u v u v
originate edge u u otherwise u v u v could
embedded partial dag u u tied nodes u u sl
u u assume contradiction u u u u tied u
u due alignment sharing dual leaf variable sharing u cannot tied u
u due alignment sharing since alignment function nodes sl nodes
sr cannot tied due dual leaf variable sharing since variable appears
r finally u tied u without loss generality due dual leaf
variable sharing edge part l therefore u include
aligned modifier thus u tied u due alignment
construct embedded graph rooted rl f sl part
match l f construct embedded graph rooted rl path
node sl particular paths u u since u u u
u source nodes part sl expand two paths
e edges u v u v get embedded graph gn tree
contradiction


fibar haim dagan berant

suppose e edges u v u v originate different edges
respectively u u tied u u therefore v v construct
following embedded graph rooted rl previous case expand
paths sl rl u u next add e edges u v u v
recall sl therefore used expansion try
expand embedded graph include paths v v z succeed
two paths leading z fail two paths tn meeting
node z explained last v v v v node f two
incoming edges contradicting lemma
case introduction rule quite similar simpler p passes sr
p n must root compact forest node path
rr however case n single outgoing edge therefore outgoing
e edges disjoint e cannot part embedded dag thus p must
pass rr contradiction p p pass sr proof identical
case substitution rule
shown f cdag whose embedded dags rooted r trees f
single root nodes added applying l r incoming
edge hence f compact forest
corollary inference process generates compact forest
proof easy verify initialization generates compact forest since applying
rule compact forest compact forest inference process generates
compact forest induction number rule applications
theorem given rule base r set initial trees tree represented
compact forest derivable inference process consequent according
inference formalism
proof first completeness induction number rule applications n
n one initial trees represented initial compact forest
let tn tree derived formalism applying sequence n rules
tn represented derivable compact forest tn derived applying
rule l r tree tn according inductive assumption tn represented
compact forest f derivable inference process therefore rule l r
matched applied f assume l r substitution rule since case
introduction rule similar tn almost identical tn except contains subtree
r instead l instantiated variables aligned modifiers easy verify
application l r f resulting f f contain embedded tree
almost identical tn except root sr rr chosen instead root
sl rl rest sr chosen appropriate instantiated variables
modifiers therefore tn contained f required guaranteed
tree according corollary
next prove soundness induction number rule applications
forest initialization initial trees consequents let fn compact
forest derived n rule applications corollary guarantees fn indeed


fiknowledge textual inference via parse tree transformations

compact forest given tree tn represented fn tn consequent
formalism
tn already represented compact forest n rule applications
according assumption induction consequent formalism
tn embedded tree created application rule l r therefore
tn contains entire subtree sr incrementally construct embedded tree tn
represented fn tn applying l r tn
substitution rule first construct part tn include
subtree rooted rr introduction rule take path forests root
rl next construct sl rl instead sr rr possible since
according corollary embedded graphs trees therefore nodes sl
already tn look set e edges tn sr
sr
let z edge originating edge sz subtree rooted z
tn notice sz already part fn tied sl therefore source
node expand tn include edge z sz already used
edge tn guaranteed part sl edges
part sl shared finally complete construction tn arbitrarily
expanding unused outgoing edge tn nodes obtain complete embedded
tree
constructed embedded tree tn fn therefore according inductive
assumption tn consequent formalism tn contains sl instantiation
dual leaf variables therefore matched l rule l r applied
easy verify application rule tn yield tn required thus tn
consequent formalism

sake simplicity proofs ignored case one leaf
variables l match multiple target nodes l appear r non leaves described
section case matched target nodes inserted sr alternatives
proper sharing modifiers consequently sr becomes compact forest containing
multiple trees similarly sl compact forest whose represented trees correspond
possible choices matching leaf variables mapping nodes matched
leaf variables sl nodes generated sr defines one one
mapping trees sl sr
proofs easily adapted handle case follows first proof
lemma need change theorem proof rule application create
cycles still holds underlying graph sr dag rather tree prove
embedded partial dag tree observe exactly one trees embedded
sr part thus consider tree sr corresponding tree
sl ignoring rest sr sl proceed original proof similarly
prove completeness theorem refer tree represented sl part
tn corresponding tree sr prove soundness consider subtrees
sr corresponding tree sl


fibar haim dagan berant

references
bar haim r semantic inference lexical syntactic level ph thesis
department computer science bar ilan university ramat gan israel
bar haim r berant j dagan compact forest scalable inference
entailment paraphrase rules proceedings emnlp
bar haim r berant j dagan greental mirkin shnarch e szpektor
efficient semantic deduction approximate matching compact parse
forests proceedings tac workshop
bar haim r dagan dolan b ferro l giampiccolo magnini b szpektor
second pascal recognising textual entailment challenge
second pascal challenges workshop recognizing textual entailment
bar haim r dagan greental shnarch e semantic inference
lexical syntactic level proceedings aaai
bar haim r szpektor glickman definition analysis intermediate
entailment levels proceedings acl workshop empirical modeling
semantic equivalence entailment
barzilay r lee l learning paraphrase unsupervised
multiple sequence alignment proceedings hlt naacl
barzilay r mckeown k r extracting paraphrases parallel corpus
proceedings acl
bensley j hickl workshop application lccs groundhog system
rte proceedings tac workshop
bentivogli l clark p dagan dang h giampiccolo sixth
pascal recognizing textual entailment challenge proceedings tac
workshop
bentivogli l dagan dang h giampiccolo magnini b fifth
pascal recognizing textual entailment challenge proceedings tac
workshop
berant j dagan goldberger j global learning typed entailment rules
proceedings acl
bhagat r ravichandran large scale acquisition paraphrases learning
surface patterns proceedings acl hlt
bos j markert k recognising textual entailment logical inference techniques proceedings emnlp
bos j markert k logical inference helps determining textual entailment
doesnt proceedings second pascal recognising textual
entailment challenge
chklovski pantel p verbocean mining web fine grained semantic
verb relations proceedings emnlp


fiknowledge textual inference via parse tree transformations

collins duffy n convolution kernels natural language advances
neural information processing systems
connor roth context sensitive paraphrasing single unsupervised
classifier ecml
cooper r crouch r van eijck j fox c van genabith j jaspars j kamp h
pinkal milward poesio pulman briscoe maier h konrad k
framework tech rep fracas framework computational
semantics
dagan glickman probabilistic textual entailment generic applied modeling language variability pascal workshop text understanding mining
dagan glickman gliozzo marmorshtein e strapparava c direct
word sense matching lexical substitution proceedings coling acl
dagan glickman magnini b b pascal recognising textual entailment challenge quinonero candela j dagan magnini b dalche buc f
eds machine learning challenges lecture notes computer science vol
pp springer
dagan roth sammons zanzotto f recognizing textual entailment applications synthesis lectures human language technologies
morgan claypool publishers
de salvo braz r girju r punyakanok v roth sammons inference
model semantic entailment natural language proceedings aaai
deerwester dumais furnas g w landauer k harshman r
indexing latent semantic analysis journal american society information
science
dinu g lapata topic meaning similarity context proceedings coling posters
emele c dorna ambiguity preserving machine translation packed
representations proceedings coling acl
fellbaum c ed wordnet electronic lexical database language speech
communication mit press
gabrilovich e markovitch computing semantic relatedness wikipediabased explicit semantic analysis proceedings ijcai
ganitkevitch j van durme b callison burch c ppdb paraphrase
database proceedings hlt naacl
giampiccolo magnini b dagan dolan b third pascal recognizing textual entailment challenge proceedings acl pascal workshop
textual entailment paraphrasing
giampiccolo trang dang h magnini b dagan dolan b fourth
pascal recognizing textual entailment challenge proceedings tac
workshop


fibar haim dagan berant

glickman dagan identifying lexical paraphrases single corpus
case study verbs proceedings ranlp
glickman shnarch e dagan lexical reference semantic matching
subtask proceedings emnlp
haghighi ng manning c robust textual inference via graph
matching proceedings emnlp
harmeling inferring textual entailment probabilistically sound calculus
natural language engineering
heilman smith n tree edit recognizing textual entailments
paraphrases answers questions proceedings hlt naacl
hickl discourse commitments recognize textual entailment proceedings coling
hickl bensley j discourse commitment framework recognizing textual entailment proceedings acl pascal workshop textual
entailment paraphrasing
hickl bensley j williams j roberts k rink b shi recognizing textual entailment lccs groundhog system second pascal
challenges workshop recognizing textual entailment
jurafsky martin j h speech language processing introduction
natural language processing computational linguistics speech recognition
second edition prentice hall
kamp h reyle u discourse logic introduction modeltheoretic
semantics natural language formal logic discourse representation theory
kluwer academic publishers dordrecht
kay chart generation proceedings acl
kazama j torisawa k exploiting wikipedia external knowledge named
entity recognition proceedings emnlp conll
kipper k verbnet broad coverage comprehensive verb lexicon ph thesis
university pennsylvania
kouylekov magnini b tree edit distance textual entailment proceedings ranlp
lehmann j bizer c kobilarov g auer becker c cyganiak r hellmann
dbpedia crystallization point web data journal web
semantics
lin dependency evaluation minipar proceedings workshop
evaluation parsing systems lrec
lin pantel p discovery inference rules question answering natural
language engineering
lotan stern dagan truthteller annotating predicate truth
proceedings hlt naacl


fiknowledge textual inference via parse tree transformations

maccartney b galley manning c phrase alignment model
natural language inference proceedings emnlp
maccartney b grenager de marneffe c cer manning c
learning recognize features valid textual entailments proceedings hltnaacl
maccartney b manning c extended model natural logic proceedings iwcs
macleod c grishman r meyers barrett l reeves r nomlex lexicon
nominalizations proceedings euralex
maxwell iii j kaplan r method disjunctive constraint satisfaction tomita ed current issues parsing technology kluwer academic
publishers
mehdad magnini b word overlap baseline recognizing textual
entailment task unpublished manuscript
mehdad magnini b b optimizing textual entailment recognition particle swarm optimization proceedings workshop applied textual
inference
melamud berant j dagan goldberger j szpektor two level
model context sensitive inference rules proceedings acl
meyers reeves r macleod c szekeley r zielinska v young b
cross breeding dictionaries proceedings lrec
mi h huang l liu q forest translation proceedings acl
hlt
mirkin dagan pado assessing role discourse references
entailment inference proceedings acl
mirkin dagan shnarch e evaluating inferential utility lexicalsemantic resources proceedings eacl
moldovan rus v logic form transformation wordnet applicability question answering proceedings acl
nairn r condoravdi c karttunen l computing relative polarity textual
inference proceedings international workshop inference computational
semantics icos
pang b knight k marcu syntax alignment multiple translations
extracting paraphrases generating sentences proceedings hlt naacl
pantel p bhagat r coppola b chklovski hovy e isp learning
inferential selectional preferences proceedings hlt naacl
ponzetto p strube deriving large scale taxonomy wikipedia
proceedings aaai
ravichandran hovy e learning surface text patterns question answering system proceedings acl


fibar haim dagan berant

ritter mausam etzioni latent dirichlet allocation method selectional
preferences proceedings acl
romano l kouylekov szpektor dagan lavelli investigating
generic paraphrase relation extraction proceedings eacl
ron generating entailment rules online lexical resources masters
thesis computer science department bar ilan university
ruppenhofer j sporleder c morante r baker c palmer semeval task linking events participants discourse proceedings
workshop semantic evaluations recent achievements future directions
sew
saint dizier p mehta melkar r eds proceedings joint workshop fam lbr kraq learning reading applications intelligent
question answering
schoenmackers etzioni weld davis j learning first order horn
clauses web text proceedings emnlp
shinyama sekine sudo k grishman r automatic paraphrase acquisition news articles proceedings hlt
shnarch e barak l dagan extracting lexical reference rules
wikipedia proceedings acl ijcnlp
snow r jurafsky ng semantic taxonomy induction heterogenous evidence proceedings coling acl
snow r vanderwende l menezes b effectively syntax recognizing
false entailment proceedings hlt naacl
stern dagan confidence model syntactically motivated entailment
proofs proceedings ranlp
stern dagan recognizing implied predicate argument relationships
textual inference proceedings acl
stern stern r dagan felner efficient search transformation
inference proceedings acl
szpektor dagan learning canonical forms entailment rules proceedings
ranlp
szpektor dagan learning entailment rules unary templates proceedings coling
szpektor dagan augmenting wordnet inference argument
mapping proceedings acl ijcnlp workshop applied textual inference
textinfer
szpektor dagan bar haim r goldberger j contextual preferences
proceedings acl hlt
szpektor tanev h dagan coppola b scaling web acquisition
entailment patterns proceedings emnlp


fiknowledge textual inference via parse tree transformations

tatu iles b slavick j novischi moldovan cogex second recognizing textual entailment challenge second pascal challenges
workshop recognizing textual entailment
tatu moldovan logic semantic recognizing textual
entailment proceedings coling acl
tatu moldovan cogex rte proceedings acl pascal
workshop textual entailment paraphrasing
valencia v studies natural logic categorial grammar ph thesis
university amsterdam
van deemter k kibble r coreferring coreference muc related
annotation schemes computational linguistics
voorhees e harman overview sixth text retrieval conference
trec proceedings trec
wang manning c probabilistic tree edit structured latent
variables textual entailment question answering proceedings coling
wang r neumann g recognizing textual entailment subsequence
kernel method proceedings aaai
yates etzioni unsupervised methods determining object relation
synonyms web journal artificial intelligence jair
zanzotto f pennacchiotti moschitti machine learning
textual entailment recognition natural language engineering




