Journal Artificial Intelligence Research 54 (2015) 471492

Submitted 05/15; published 11/15

Weighted Regret-Based Likelihood: New Approach
Describing Uncertainty
Joseph Y. Halpern

halpern@cs.cornell.edu

Computer Science Department
Cornell University
Ithaca, NY 14853, USA

Abstract
Recently, Halpern Leung suggested representing uncertainty set weighted
probability measures, suggested way making decisions based representation
uncertainty: maximizing weighted regret. paper answer apparently
simpler question: means, according representation uncertainty,
event E likely event E 0 . paper, notion comparative
likelihood uncertainty represented set weighted probability measures
defined. generalizes ordering defined probability (and lower probability)
natural way; generalization upper probability defined. complete
axiomatic characterization notion regret-based likelihood given.

1. Introduction
Recently, Samantha Leung (Halpern & Leung, 2012) suggested representing uncertainty set weighted probability measures, suggested way making decisions
based representation uncertainty: maximizing weighted regret. However,
answer apparently simpler question: given representation uncertainty,
mean event E likely event E 0 ?
paper. explain issues, start reviewing Halpern-Leung approach.
frequently observed many situations agents uncertainty adequately described single probability measure. Specifically, single
measure may adequate representing agents ignorance. example,
seems big difference coin known fair coin whose bias agent
know, yet agent use single measure represent uncertainty,
cases would seem measure assigns heads probability 1/2
would used.
One approach suggested representing ignorance use set P
probability measures. idea old one, apparently going back work Boole
(1854, ch. 1621) Ostrogradsky (1838); authors (e.g., Campos & Moral, 1995;
Couso, Moral, & Walley, 1999; Gilboa & Schmeidler, 1993; Levi, 1985; Walley, 1991)
additionally required set P convex (so 1 2 P,
a1 + b2 , a, b [0, 1] + b = 1). approach benefit representing
uncertainty general, single number, range numbers. allows us
distinguish certainty coin fair (in case uncertainty heads
represented single number, 1/2) knowing probability heads could
anywhere between, say, 1/3 2/3.
c
2015
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiHalpern

approach problems. example, consider agent believes
coin may slight bias. Thus, although unlikely completely fair,
close fair. represent set probability measures? Suppose
agent quite sure bias 1/3 2/3. could, course, take
P consist measures give heads probability 1/3 2/3.
agent know possible biases exactly 1/3 2/3.
consider 2/3 + possible small ? even confident bias
1/3 2/3, representation cannot take account possibility
views biases closer 1/2 likely biases 1/2.
second well-known concern: learning. Suppose agent initially
considers possible measures gives heads probability 1/3 2/3.
starts tossing coin, sees that, first 20 tosses, 12 heads. seems
agent consider bias greater 1/2 likely bias less
1/2. use standard approach updating sets probability measures
(Halpern, 2003), condition measures observation, since coin
tosses viewed independent, agent continue believe probability
next coin toss 1/3 2/3. observation impact far learning
predict better. set P stays same, matter observation made.
well-known solution problems: putting measure uncertainty
probability measures P. idea long history. One special case put
second-order probability probability measures; see (Good, 1980) discussion
approach references. example, agent express fact
bias coin likely close 1/2 far 1/2. addition,
problem learning dealt straightforward conditioning. approach
leads problems. Essentially, seems ambiguity agent might feel
outcome coin toss seems disappeared. example, suppose
agent idea bias is. obvious second-order probability use
uniform probability possible biases. cannot talk probability
coin heads (there set probabilities, all, single probability), expected
probability heads 1/2. agent idea bias coin
know believe expected probability heads 1/2? course, one use
single probability measure describe uncertainty, symmetry considerations dictate
one ascribes equal likelihood heads tails; similarly, one
put single second-order probability set possible biases, uniform probability
seems obvious choice. Moreover, interest making decisions,
maximizing expected utility using expected probability take
agents ignorance account. Kyburg (1988) Pearl (1987) even argued
need second-order probability probabilities; whatever done
second-order probability already done basic probability.
Nevertheless, comes decision-making, seem useful use approach
represents ambiguity, still maintaining features secondorder probability probabilities. idea goes back least Gardenfors Sahlin
(1982, 1983). Walley (1997) suggested putting possibility measure (Dubois & Prade, 1998;
Zadeh, 1978) probability measures; essentially done Cattaneo (2007),
Chateauneuf Faro (2009), de Cooman (2005). authors others,
472

fiWeighted Regret-Based Likelihood

Klibanoff et al. (2005), Maccheroni et al. (2006), Nau (1992), proposed approaches
decision making using representations uncertainty.
Leung similarly suggested putting weights probability measure P. Since
assumed weights normalized supremum weights 1,
weights viewed possibility measure. set P finite,
normalize view weights second-order probabilities. secondorder probabilities, weights vary time, information acquired.
example, start state complete ignorance (modeled assuming
probability measures weight 1), update weights making observation
ob, take weight measure Pr relative likelihood ob Pr
true measure. (See Section 2 details.) approach, called likelihood updating
Halpern Leung (2012), true underlying measure generating data,
time, weight true measure approaches 1, weight measures
approaches 0. Thus, approach allows learning natural way. If, example,
actual bias coin 5/8 example above, matter initial weights,
long 5/8 positive weight, weight would almost surely converge 1
observations made, weight measures would approach 0. This,
course, exactly would happen second-order probability P.
weights used represent fact probabilities set P
likely others.
essentially others considered representation uncertainty based set
probability weights, Leung suggested way using representation
make decisions. However, approach different suggested earlier.
based approach regret, standard approach decision-making introduced
(independently) Niehans (1948) Savage (1951). uncertainty represented
set P probability measures, regret works follows: act
measure Pr P, compute expected regret respect Pr;
difference expected utility expected utility act
gives highest expected utility respect Pr. associate act
worst-case expected regret a, measures Pr P, compare acts
respect worst-case expected regret. weights picture, modify
procedure multiplying expected regret associated measure Pr weight
Pr, compare acts according worst-case weighted expected regret. approach
making decisions different others mentioned incorporate
likelihood probabilities. Moreover, using weights way means cannot
simply replace set weighted probability measures single probability measure;
objections Kyburg (1988) Pearl (1987) apply.
Leung (Halpern & Leung, 2012) show approach seems reasonable
things number examples interest, provide axiomatization decision-making
approach. Since sets weighted probabilities certainly intended way
representing uncertainty, seems natural ask whether used represent
relative likelihood direct way. Surprisingly, something largely considered
earlier papers using sets weighted probabilities, since focus decision-making
(although work Nau discussed Section 3 exception).
473

fiHalpern

Representing relative likelihood straightforward uncertainty represented
single probability measure: E likely E 0 exactly probability E greater
probability E 0 . using sets probability measures, various approaches
considered literature. common takes E likely
E 0 lower probability E greater lower probability E 0 , lower
probability E worst-case probability, taken measures P (see Section 3).
could compare E E 0 respect upper probabilities (the best-case
probability respect measures P). Another possibility take E
likely E 0 Pr(E) Pr(E 0 ) measures Pr P; gives partial order
likelihood.1 uncertainty represented set weighted
probability measures?
paper, define notion relative likelihood uncertainty represented
set weighted probability measures generalizes ordering defined lower
probability natural way; define generalization upper probability.
associate event E two numbers analogues lower upper probability.
uncertainty represented single measure, two numbers coincide; general,
not. interval thought representing degree ambiguity
likelihood E. Indeed, special case weights 1, numbers
essentially lower upper probability (technically, 1 minus lower
upper probability, respectively). Interestingly, approach assigning likelihood
based approach decision-making. Essentially, analogue
defining probability terms expected utility, rather way around.
approach viewed generalizing probability lower probability,
time allowing natural approach updating.
interested representation? ever probability use make decisions, arguably wouldnt much interest;
work Leung already shows sets weighted probabilities used decisionmaking. results paper add nothing question. However, often
talk likelihood events quite independent use decision-making.
clearly many examples physics. issue arises AI applications well: typical
explanation rather B thought event E
likely F . computations expectation, clearly involve representation
uncertainty, arise many AI applications. Thus, analogue probability seems
important useful right.
rest paper organized follows. reviewing relevant material
(Halpern & Leung, 2012) Section 2, define regret-based likelihood Section 3,
compare lower probability. provide axiomatic characterization regret-based
likelihood Section 4, show relates axiomatic characterization lower
probability. conclude Section 5.

1. long tradition considering partially ordered notions likelihood; see (Halpern, 1997)
references therein, work Walley (1991).

474

fiWeighted Regret-Based Likelihood

2. Weighted Expected Regret: Review
Consider standard setup decision theory. state space outcome
space O. act function O; describes outcome state. Suppose
utility function u outcomes set P + weighted probability measures.
is, P + consists pairs (Pr, Pr ), Pr weight [0, 1] Pr probability
S. Let P = {Pr : ((Pr, ) P + )}. Pr P assumed
exactly one , denoted Pr , (Pr, ) P + . assumed weights
normalized least one measure Pr P Pr = 1.
Finally, P + assumed weakly closed, (Prn , n ) Pr+ n = 1, 2, 3, . . .,
(Prn , n ) (Pr, Pr ), Pr > 0, (Pr, Pr ) P + . (I discuss require
P + weakly closed, rather closed.)
assumption least one probability measure weight 1 convenient
comparison approaches; see below. However, making assumption
impact results paper; long restrict sets weight bounded,
results hold without change. assumption is, course, incompatible
weights probabilities. Note assumption weights probabilities
runs difficulties infinite number measures P; example, P
includes measures heads 1/3 2/3, discussed Introduction, using
uniform probability, would forced assign individual probability measure
weight 0, would work well later definitions.
weights P + coming from? general, viewed subjective,
probability measures. However, Leung (Halpern & Leung, 2012)
observed, important special case weights given natural
interpretation. Suppose that, case biased coin Introduction, make
observations situation probability making given observation determined
objective source. start giving probability measures weight 1.
Given observation ob (e.g., sequence coin tosses example Introduction),
compute Pr(ob) measure Pr P; update weight Pr
Pr(ob)/ supPr0 P Pr0 (ob). Thus, likely observation according Pr,
higher updated weight Pr relative probability measures P.2 (The
denominator normalization ensure measure weight 1.)
approach updating, true underlying measure generating data,
agent makes observations, almost surely, weight true measure approaches
1, weight measures approaches 0.3 addition, approach gives
agent natural way determining weights probability measure P. While,
general, means agent may need carry around lot information (not
2. idea putting possibility probabilities P determined likelihood appears
work Moral (1992), although consider general approach dealing sets weighted
probability measures.
3. almost surely due fact that, probability approaching 0, observations made, possible agent make misleading observations representative
true measure. depends set possible observations rich enough allow
agent ultimately discover true measure generating observations; example, agent
never learn distributions outcomes die never gets observe die lands 5 6.
Since learning focus paper, make notion rich enough precise here.

475

fiHalpern

possibly infinite set probabilities, weight associated one),
set P reasonable parametric representation, weight often evaluated
terms parameters, admit compact representation (see Example 3.2).
weight associated probability Pr viewed upper bound
agents confidence Pr actually describes situation. agent
idea going modeled starting placing weight 1 probability
measures. believe weights allow agents express nuances
consider important, weights hard elicit. Whether
case really empirical question, one believe deserves exploration,
beyond scope paper.
review definition weighted regret, introduce notion absolute
(weighted) regret. start regret. regret act state
difference utility best act state utility s. Typically,
act compared acts, acts set , called menu. Thus,
regret state relative menu , denoted reg (a, s), supa0 u(a0 (s)) u(a(s)).4
typically constraints put ensure supa0 u(a0 (s)) finitethis
certainly case finite, convex closure finite set acts,
best possible outcome outcome space O. latter assumption holds paper,
assume throughout supa0 u(a0 (s)) finite.
simplicity, assume state space finite. Given probability measure
Pr S, expected regret act respect Pr relative menu
P

reg
sS reg (a, s) Pr(s). (expected) regret respect P menu
Pr (a) =
worst-case regret, is,

reg
P (a) = sup reg Pr (a).
PrP

Similarly, weighted (expected) regret respect P + menu
worst-case weighted regret, is,

wr
P + (a) = sup Pr reg Pr (a).
PrP

Thus, regret special case weighted regret, weights 1.
Note that, far weighted regret goes, hurt augment set P + weighted
probability measures adding pairs form (Pr, 0) Pr
/ P. start set
+
P unweighted probability measures, set P = {(Pr, 1) : Pr P}{(Pr, 0) : Pr
/ P}
closed general, although weakly closed. may well sequence Prn Pr,
Prn
/ P n, Pr P. would (Prn , 0) P + converging
+
(Pr, 0)
/ P . exactly required weak closedness. Note future reference
that, since P + assumed weakly closed, wr
P + (a) > 0, element

+

(Pr, Pr ) P wr P + (a) = Pr reg Pr (a).
Weighted regret induces obvious preference order acts: act least good
0
0


a0 respect P + , written reg
P + ,M , wr P + (a) wr P + (a ). usual,
4. Recall X set real numbers, sup X, supremum X, smallest real numbers
greater equal elements X. X finite, sup max.
X is, say, interval (0, 1), sup X = 1. Similarly, inf X largest real number
less equal elements X.

476

fiWeighted Regret-Based Likelihood

reg
reg
0
0
0
write reg
P + ,M P + ,M case P + ,M a. standard notion
regret special case weighted regret weights 1. sometimes write
0
+
reg
P,M denote unweighted case (i.e., weights P 1).
setting, using weighted regret gives approach allows agent transition
smoothly regret expected utility. well known regret generalizes expected
0
utility sense P singleton {Pr}, wr
P (a) wr P (a ) iff EUPr (a)
0
EUPr (a ) (where EUPr (a) denotes expected utility act respect probability
Pr); follows observation that, given menu , constant cM
that, acts , wr
{Pr} (a) = cM EUPr (a). (In particular, means P
singleton, regret menu independent.) start weights 1, then,
observed above, weighted regret standard notion regret. agent
makes observations, measure Pr generating uncertainty, weights
get closer closer situation Pr gets weight 1, weights
measures dropping quickly 0, ordering acts converge ordering
given expected utility respect Pr.
another approach similar properties, starts uncertainty represented set P (unweighted) probability measures. Define wc P (a) =
inf PrP EUPr (a). Thus wc P (a) worst-case expected utility a, taken Pr P.
define mm
a0 wc P (a) wc P (a0 ). maxmin expected utility rule, quite
P
often used economics (Gilboa & Schmeidler, 1989). difficulties getting
weighted version maxmin expected utility (Halpern & Leung, 2012) (discussed
Section 3); however, Epstein Schneider (2007) propose another approach
combined maxmin expected utility. fix parameter (0, 1), update P
observation ob retaining measures Pr Pr(ob) .
choice < 1, end converging almost surely single measure,
approach converges almost surely expected utility.
conclude section discussion menu dependence. Maxmin expected utility
menu dependent; preference ordering acts induced regret be,
following example illustrates.

Example 2.1: Take outcome space {0, 1}, utility function
identity, u(1) = 1 u(0) = 0. usual, E S, 1E denotes indicator
function E, where, state S, 1E (s) = 1 E, 1E (s) = 0

/ E. Let = {s1 , s2 , s3 , s4 }, E1 = {s1 }, E2 = {s2 }, E3 = {s2 , s3 }, M1 = {1E1 , 1E2 },
M2 = {1E1 , 1E2 , 1E3 }, P = {Pr1 , Pr2 }, Pr1 (s1 ) = Pr1 (s3 ) = Pr1 (s4 ) = 1/3,
1
Pr2 (s2 ) = 1/4, Pr2 (s3 ) = 3/4. straightforward calculation shows reg
Pr1 (1E1 ) =
M1
M1
M1
M2
M2
0, reg Pr1 (1E2 ) = 1/3, reg Pr2 (1E1 ) = 1/4, reg Pr2 (1E2 ) = 0, reg Pr1 (1E1 ) = 1/3, reg Pr1 (1E2 ) =
M1
M1
M2
2
2/3, reg
Pr2 (1E1 ) = 1, reg Pr2 (1E2 ) = 3/4. Thus, 1/4 = reg P (1E1 ) < reg P (1E2 ) = 1/3,
M2
2
1 = reg
P (1E1 ) > reg P (1E2 ) = 3/4. preference 1E1 1E2 depends
whether consider menu M1 menu M2 .
Suppose outcome gives maximum utility; is,
u(o) O. constant act gives outcomes states,
clearly best act states. best act, absolute,
menu-independent notion weighted expected regret defined always comparing

u(o )

477

fiHalpern

. is, define
reg(s, a) = u(o ) u(a(s));
P
reg Pr (a) = sS (u(o ) u(a(s)) Pr(s) = u(o ) EUPr (a);
P
reg P (a) = supPrP sS (u(o ) u(a(s)) Pr(s) = u(o ) inf PrP (EUPr (a);
P
wr P + (a) = supPrP Pr sS (u(o ) u(a(s)) Pr(s) = supPrP Pr (u(o ) EUPr (a)).
best act, write P + a0 wr P + (a) wr P + (a0 ); similarly
unweighted case, write P a0 wr P (a) wr P (a0 ).
Conceptually, think agent always aware best outcome ,
comparing actual utility u(o ). Equivalently, absolute notion regret
equivalent menu-based notion respect menu includes (since
menu includes , best act every state). shall see, setting,
always reduce menu-dependent regret absolute, menu-independent notion, since
fact best act: 1S .

3. Relative Ordering Events Using Weighted Regret
section, consider notion comparative likelihood defined using sets
weighted probability measures.
Example 2.1, take outcome space {0, 1}, utility function
identity, consider indicator functions. easy see EUPr (1E ) = Pr(E),
setup, recover probability expected utility. Thus, uncertainty
represented single probability measure Pr make decisions preferring
acts maximize expected utility, 1E 1E 0 iff Pr(E) Pr(E 0 ).
Consider happens apply approach maxmin expected utility.
1E mm
1E 0 iff inf PrP Pr(E) inf PrP Pr(E 0 ). literature, inf PrP Pr(E),
P
denoted P (E), called lower probability E, standard approach describing likelihood. dual upper probability, supPrP Pr(E), denoted P (E). easy
calculation shows
P (E) = 1 P (E),
where, usual, E denotes complement E. interval [P (E), P (E)]
thought describing uncertainty E; larger interval, greater ambiguity.
happens apply approach regret? First consider unweighted regret.
restrict acts form 1E , best act clearly 1S ,
constant function 1. Thus, (and do) use absolute notion regret here,
remainder paper. get 1E reg
P 1E 0 iff supPrP (1 Pr(E))
0
0
supPrP (1 Pr(E )) iff supPrP Pr(E) supPrP Pr(E ); is,
0



1E reg
P 1E 0 iff P (E) P (E ).

478

fiWeighted Regret-Based Likelihood

Moreover, easy manipulation shows supPrP (1 Pr(E)) = 1 inf PrP Pr(E) = 1
P (E). follows
1E reg
P 1E 0
iff (1 P (E)) (1 P (E 0 ))
iff P (E) P (E 0 )
iff 1E mm
1E 0 .
P
is, regret maxmin expected utility put ordering events.
+ (E), (weighted) regret-based
extension weighted regret immediate. Let Preg
likelihood E, defined taking
+
Preg
(E) = sup Pr Pr(E).
PrP

P + unweighted, weights 1, write Preg (E) denote supPrP Pr(E).
Note Preg (E) = 1 P (E),
Preg (E) Preg (E 0 ) iff P (E) P (E 0 ).
is, ordering induced Preg opposite induced P . So, example,
Preg () = 1 Preg (S) = 0; smaller sets larger regret-based likelihood. However, since
act smaller regret viewed better, ordering acts form 1E induced
regret induced maxmin expected utility.
Regret-based likelihood provides way associating number event,
probability lower probability do. Moreover, lower probability gives lower
+ (E) giving upper bound uncertainty.
bound uncertainty, think Preg
(It upper bound rather lower bound larger regret means less likely,
smaller lower probability does.) naive corresponding lower bound given
inf PrP Pr Pr(E). lower bound terribly interesting; probability
measures Pr0 P Pr0 close 0, lower bound close 0,
independent agents actual feeling likelihood E. reasonable
+
lower bound given expression P +
reg (E) = 1 Preg (E) (recall analogous
expression relates upper probability lower probability). intuition choice
following. nature conspiring us, would try prove us wrong
making Pr Pr(E) large possiblethat is, make weighted probability
wrong large possible. hand, nature conspiring us, would
try make Pr Pr(E) large possible, or, equivalently, make 1 Pr Pr(E) small
possible. Note different making Pr Pr(E) large possible, unless
Pr = 1 Pr P. easy calculation shows
+ (E) = 1 sup
1 Preg
PrP Pr Pr(E)
= inf PrP (1 Pr Pr(E)).

motivates definition P +
reg .
following lemma clarifies relationship expressions, shows
+
[P +
reg (E), Preg (E)] really give interval ambiguity.
+ (E) P + (E).
Lemma 3.1: inf PrP Pr Pr(E) 1 Preg
reg

479

fiHalpern

Proof: Clearly
inf Pr Pr(E) = inf Pr (1 Pr(E)).

PrP

PrP

Since, observed above,
+
1 Preg
(E) = inf (1 Pr Pr(E)),
PrP

Pr P,
1 Pr Pr(E) Pr (1 Pr(E)),
+ (E).
follows inf PrP Pr Pr(E) 1 Preg
Since, assumption, probability measure Pr0 P Pr0 = 1,
follows
+ (E) = 1 sup
1 Preg
PrP Pr Pr(E)
1 Pr0 (E)
= Pr0 (E)
supPrP Pr Pr(E)
+ (E).
Preg

general, equality hold Lemma 3.1, shown following example.
example illustrates ambiguity interval decrease weighted regret,
weights updated Leung (Halpern & Leung, 2012) suggested.
Example 3.2: Suppose state space consists {h, t} (for heads tails); let Pr
measure puts probability h. Let P0+ = {(Pr , 1) : 1/3 2/3}. is,
initially consider measures put probability 1/3 2/3 heads.
toss coin observe lands heads. Intuitively, consider likely
probability heads greater 1/2. Indeed, applying likelihood updating,
get set P1+ = {(Pr , 3/2) : 1/3 2/3}; probability measures give h higher
probability get higher weight. particular, weight Pr2/3 still 1, weight
Pr1/3 1/2. (The weight Pr likelihood observing heads according Pr ,
, normalized likelihood observing heads according measure
gives heads highest probability, namely 2/3.) coin tossed
time tails observed, update get P2+ = {(Pr , 4(1 )) : 1/3 2/3}.
going on, worth noting simple parametric form P0+ leads
simple parametric forms P1+ P2+ .
+
+
+
easy calculation shows [P +
0,reg (h), P0,reg (h)] = [1/3, 2/3], [P 1,regret (h), P1,reg (h)] =
+
+
[1/3, 3/8], [P 2,reg (h), P2,reg (h)] = [11/27, 16/27]. detail, since Pr (h) =
Pr (t) = 1 , following:
0
P0,reg
(h) = sup[1/3,2/3] (1 ) = 2/3.

P 00,reg (h) = inf [1/3, 2/3](1 ) = 1/3.
0
P1,reg
(h) = sup[1/3,2/3] (3/2)(1). Taking derivative shows (3/2)(1)
0
maximized = 1/2, P1,reg
(h) = 3/8.

480

fiWeighted Regret-Based Likelihood

P 01,reg (h) = inf [1/3,2/3] (1 (3/2)). 1 (3/2) minimized, (3/2)
maximized; [1/3, 2/3], happens = 2/3, P 01,reg (h) = 1/3.
0
P2,reg
(h) = sup[1/3,2/3] 4(1)(1). Taking derivative shows 4(1)2
maximized = 1/3, case 16/27.

P 02,reg (h) = inf [1/3,2/3] (1 4(1 )). 1 4 2 (1 ) minimized
4 2 (1) maximized; [1/3, 2/3], happens = 2/3, P 01,reg (h) =
11/27.
easy see inf Pr 4(1 ) Pr (t) = inf [1/3,2/3] 4(1 )2 = 8/27,
+
+
inf 4(1 )Pr (t) < 1 P2,reg
(t) < P2,reg
(h).

PrP2

Thus, P2+ , get strict inequalities expressions Lemma 3.1.
+
width interval [P +
reg (E), Preg (E)] viewed measure ambiguity
agent feels E, interval [P (E), P (E)]. Indeed, weights 1,
+ (E) P (E) = 1 P + (E)
two intervals width, since P (E) = 1 Preg
reg
case.
However, weighted regret significant advantage upper lower probability.
true bias coin is, say 5/8, set Pk+ represents uncertainty
+
k steps, k increases, almost surely, [P +
k,reg (h), Pk,reg (h)] smaller smaller
interval containing 1 5/8 = 3/8. generally, using likelihood updated combined
weighted regret provides natural way model reduction ambiguity via learning.
worth point comparing approach representing likelihood taken
work Nau (1992). Nau starts preference order lotteries (functions
finite state space reals) satisfying certain axioms, derives
calls confidence-weighted (lower upper) probabilities. Roughly speaking, rather
associating event lower upper probability, Nau associate
probabilities
event E, confidence c [0, 1], probability p [0, 1] set Pc,p
give event E lower probability p confidence least c. c0 c, Pc0 ,p Pc,p
(every probability measures gives E lower probability p higher confidence
c0 give lower probability p confidence c, converse may hold).
Similarly, consider probability measures give E upper probability p
confidence c. set P unweighted probabilities, agents uncertainty regarding
event E characterized single interval [P (E), P (E)]. Naus framework,
agents uncertainty regarding E characterized family intervals [Pc (E), P c (E)],
indexed confidence c, Pc (E) largest p E lower probability
confidence c, P c (E) defined similarly. Clearly intervals nested;
0
c0 > c, [Pc0 (E), P c (E)] contains [Pc (E), P c (E)]. Thus, Naus approach provides
fine-grained representation uncertainty single intervals [P (E), P (E)]
+
[P +
reg (E), Preg (E)]. extent, distinction due fact Naus preference
order lotteries partial order; preference order induced max=min expected
+ , P + put
utility regret total. However, note even though P , P , Preg
reg

+ , P + together,
total order events, considering P P Preg
reg

481

fiHalpern

obtain partial order events; particular, approaches express
ambiguity.
One benefit regret-based approach provides natural way updating.
Nau consider updating; would interesting see analogue likelihood
updating could defined axiomatically Naus framework, perhaps spirit
characterization Leung (Halpern & Leung, 2012) gave likelihood updating
context regret.
One concern use regret dependence regret menu;
Naus approach, approaches decision-making based regret,
require menu. evidence psychology literature suggesting
people quite sensitive menus, worth noting dealing likelihood,
sense work absolute notion weighted regret without
loss generality: restrict indicator functions, preference relative menu
always reduced absolute preference. Given menu consisting indicator
functions, let EM = {E : 1E }; is, EM union events
corresponding indicator function . following property shows that, restrict
indicator functions, regret satisfies satisfies axiom similar spirit Naus (1992)
cancellation axiom.
Proposition 3.3: menu consisting indicator functions, 1E1 , 1E2 ,
reg
1E1 reg
P + ,M 1E2 iff 1E1 + 1E P + 1E2 + 1E .
Proof: Let 0 menu consisting indicator functions includes 1E1 + 1E ,
reg
1E2 + 1E , 1S . Recall 1E1 + 1E reg
P + 1E2 + 1E iff 1E1 + 1E 0 ,P + 1E2 + 1E ;
absolute notion regret equivalent menu-based notion, long menu
includes best act, case 1S . clearly suffices show that, states
acts 1E ,
0

reg (1E , s) = reg (1E + 1E , s).
straightforward. two cases, depending whether EM .
EM , then, definition, act 1E 0 E 0 ,
supaM u(a(s)) = u(1). Clearly supaM 0 u(a(s)) = u(1), since 1S 0 . Moreover,
1E (s) = 0, (1E + 1E )(s) = 1E (s). Thus, EM ,
reg (1E , s) = supaM u(a(s)) u(1E (s))
= supaM 0 u(a(s)) u((1E + 1E )(s))
0
= reg (1E + 1E , s).

/ E , a(s) = 0 1E (s) = 0, supaM u(a(s)) u(1E (s)) =
0. hand, supaM 0 u(a(s)) = u(1), u((1E + 1E )(s)) = u(1),
0
supaM 0 u(a(s)) u((1E + 1E )(s)) = 0. Thus, reg (1E , s) = reg (1E +
1E , s).

482

fiWeighted Regret-Based Likelihood

4. Characterizing Weighted Regret-Based Likelihood
goal section characterize weighted regret-based likelihood axiomatically.
order so, helpful review characterizations probability lower
probability. ease exposition discussion, assume sample space
finite sets measurable.
probability measure finite set maps subsets [0, 1] way satisfies
following three properties:
Pr1. Pr(S) = 1.
Pr2. Pr() = 0.5
Pr3. Pr(E E 0 ) = Pr(E) + Pr(E 0 ) E E 0 = .
three properties characterize probability sense function f : 2S [0, 1]
satisfies properties probability measure.
Lower probabilities satisfy analogues properties:
LP1. P (S) = 1.
LP2. P () = 0.
LP30 . P (E E 0 ) P (E) + P (E 0 ) E E 0 = .
However, properties characterize lower probability. functions
satisfy LP1, LP2, LP30 lower probability corresponding set
probability measures. (See (Halpern & Pucella, 2002, Proposition 2.2) example
showing analogous properties characterize P ; example shows
characterize P .)
Various characterizations P (and P ) proposed literature (Anger &
Lembcke, 1985; Giles, 1982; Huber, 1976, 1981; Lorentz, 1952; Williams, 1976; Wolf, 1977),
similar spirit. discuss one due Anger Lembcke (1985) here, since makes
contrast lower probability regret particularly clear. characterization
based notion set cover: set E said covered n times multiset
every element E appears least n times . important note
multiset, set; elements necessarily distinct. (Of course, set
special case multiset.) Let denote multiset union; thus, M1 M2 multisets,
M1 M2 consists elements M1 M2 , appear multiplicity
sum multiplicities M1 M2 . example, using {{. . .}} notation
denote multiset, {{1, 1, 2}} {{1, 2, 3}} = {{1, 1, 1, 2, 2, 3}}.
E S, (n, k)-cover (E, S) multiset covers k times
covers E n + k times. Multiset n-cover E covers E n times. example,
= {1, 2, 3}, {{1, 1, 1, 2, 2, 3}} (2, 1)-cover ({1}, S), (1, 1)-cover ({1, 2}, S),
3-cover {1}.
interested whether multiset form E 1 . . . E (n, k)-cover
(E, S). perhaps best thought terms indicator functions. E 1 . . . E
5. property actually follows two, using observation Pr(S ) = Pr(S) + Pr();
include ease comparison approaches.

483

fiHalpern

(n, k)-cover (E, S) 1E1 + + 1Em n1E + k1S . use equalities inequalities involving sums indicator functions axiomatic characterizations
uncertainty long history; example, used Scott (1964) characterize
qualitative probability. Set covers special case inequalities. Typically,
axioms make possible apply results linear programming prove characterization
results. shall see, case too.
Consider following property:
LP3. integers m, n, k subsets E1 , . . . , Em S, E1 . . . Em (n, k)P
6
cover (E, S), k + nP (E)
i=1 P (Ei ).
analogous property upper probability, replaced . easy
see LP3 implies LP30 (since E E 0 (1, 0) cover (E E 0 , S)). follows
straightforward induction LP30 E1 , . . . , Em pairwise disjoint,
P (E1 . . . Em ) P (E1 ) + + P (E1 ). LP3 generalizes property allow sets
necessarily disjoint. soundness LP3 lower probability follows using
techniques given soundness property REG3. Anger
Lembcke (1985) show, LP3 property needed characterize lower
probability.
Theorem 4.1: (Anger & Lembcke, 1985) f : 2S [0, 1], exists set P
probability measures f = P f satisfies LP1, LP2, LP3.
Moving regret-based likelihood, clearly
+ (S) = 0.
REG1. Preg
+ () = 1.
REG2. Preg

whole space least regret; empty set greatest regret. Again, see
regret-based likelihood inverts standard ordering probability; larger regret-based
likelihood corresponds probability.
unweighted case, since Preg (E) = P (E), REG1, REG2, following analogue LP3 (appropriately modified P ) clearly characterize Preg :
REG30 . integers m, n, k subsets E1 , . . . , Em S, E 1 . . . E
P
(n, k)-cover (E, S), k + nPreg (E)
i=1 Preg (Ei ).
Note complements sets (E 1 , . . . , E , E) used here, since regret minimized
probability complement maximized. need work complement
makes statement properties (and proofs theorems) slightly less elegant,
seems necessary.
hard see REG30 hold weighted regret-based likelihood.
example, suppose = {a, b, c} P + = ((Pr1 , 2/3), (Pr2 , 2/3), (Pr3 , 1)), where,
identifying probability Pr tuple (Pr(a), Pr(b), Pr(c)),
Pr1 = (2/3, 0, 1/3);
6. Note LP3 implies LP2, using fact (1,0)-cover (, S).

484

fiWeighted Regret-Based Likelihood

Pr2 = (1/3, 0, 2/3);
Pr3 = (1/3, 1/3, 1/3).
+ ({a, b}) = P + ({b, c}) = 4/9, P + ({b}) = 2/3. Since {a, b} {b, c}
Preg
reg
reg
(1,1)-cover ({b}, {a, b, c}), REG30 would require
+
+
+
Preg
({a, b}) + Preg
({b, c}) 1 + Preg
({b}),

clearly case.
must thus weaken REG30 capture weighted regret-based likelihood. turns
appropriate weakening following:
REG3. integers m, n subsets E1 , . . . , Em S, E 1 . . . E n-cover
+ (E) Pm P + (E ).
E, nPreg

i=1 reg
Although REG3 weaker REG30 , still nontrivial consequences.
+ anti-monotonic. E E 0 , E 1-cover
example, follows REG3 Preg
0
+ (E) P + (E 0 ). Since E E 0 trivially 1-cover
E , REG3, must Preg
reg
+ (E) + P + (E 0 ) P + (E E 0 ). REG3 implies REG1,
E E 0 , follows Preg
reg
reg
since (= S) n-cover n.
state representation theorem. says representation uncertainty
satisfies REG1, REG2, REG3 iff weighted regret-based likelihood determined
set P + . set P + unique, taken maximal,
sense weighted regret-based likelihood respect set (P 0 )+ gives
representation, pairs (Pr, 0 ) (P 0 )+ , exists 0
(Pr, ) P + . (unique) maximal set P + viewed canonical representation
uncertainty.
Theorem 4.2: f : 2S [0, 1], exists weakly closed set P + weighted
+ f satisfies REG1, REG2, REG3;
probability measures f = Preg
moreover, P + taken maximal.
Proof: Clearly, given weakly closed set P + weighted probability measures, function
+ satisfies REG1 REG2. see satisfies REG3, suppose E . . . E
Preg
1

+ (E) = 0, REG3 trivially holds. P + (E) > 0, since P +
n-cover E. Preg
reg
+ (E) = Pr(E).
weakly closed, must probability Pr P Preg
Pr
Since E 1 t. . .tE n-cover E, easy see Pr(E 1 )+ +Pr(E ) = n Pr(E),
+ (E), construction,
Pr Pr(E 1 ) + + Pr Pr(E ) = nPr Pr(E). Pr Pr(E) = Preg
P

+ (E ), = 1, . . . , n. Thus, nP + (E)
+
Pr Pr(E ) Preg

reg
i=1 Preg (Ei ).

opposite direction, suppose f : 2 [0, 1] satisfies REG1, REG2,
REG3. Let P = (S), set probability measures S, Pr P, define
Pr = sup{ : Pr(E) f (E) E S}.
Note that, Pr P, 0 Pr(E) f (E) E S, since f (E) [0, 1],
1 Pr() = f () = 1. follows Pr [0, 1] Pr P. Let P + = {(Pr, Pr ) :
485

fiHalpern

Pr (S)}. easy see P + weakly closed. Moreover, show P +
+ ), immediate P + maximal among sets weighted
represents f (i.e., f = Preg
probability measures represent f . Thus, suffices show exists Pr (S)
(1) Pr = 1 (since one conditions sets weighted measures)
+ (E) E S.
(2) f (E) = Preg
proof result makes critical use following variant Farkas Lemma
(Farkas, 1902) (see Schrijver, 1986, pg. 89) linear programming,
matrix, b column vector, x column vector distinct variables:
Lemma 4.3: Ax b unsatisfiable, exists row vector
1. 0
2. = 0
3. b > 0.
Intuitively, witness fact Ax b unsatisfiable.
vector x satisfying Ax b, 0 = (A)x = (Ax) b > 0, contradiction.
prove first claim, suppose = {s1 , . . . , sN }. construct set linear
equations variables x1 , . . . , xN solution equations guarantees
existence probability measure Pr (S) Pr = 1. Intuitively, want
xi Pr(si ). Since must Pr(E) f (E) E S,7 E S,
P
inequality {i:si E}
xi f (E). Note since f () = 1, equation
/
E = x1 + + xN 1. addition, require xi 0 = 1, . . . , N ,
x1 + +xN = 1. suffices require x1 + +xn 1, since, observed earlier,
equation corresponding E = already says x1 + + xn 1. apply Farkas Lemma
inequalities need involve , collection inequalities must rewritten as:
{i:si E}
xi f (E), E
/
xi 0, = 1, . . . , N
x1 + + xN 1.
P

system inequalities expressed form Ax b. Note matrix
whose entries either 1, 0, 1, and, first 2N 1 rows (the lines corresponding
equations E S), entries either 0 1, final N + 1
rows, entries either 0 1.
solution system inequalities provides desired Pr. systems
solution, Farkas Lemma, exists nonnegative vector = 0
b > 0. Since entries either 1, 0, 1, follows standard
observations (cf., Fagin, Halpern, & Megiddo, 1990, Lemma 2.7) take vector
whose entries rational.8 Since multiply term product
7. use denote strict subset.
8. slight subtlety since satisfy b > 0, b may involve irrational numbers
(since f (E) may irrational sets E). However, nonnegative satisfies = 0
b > 0, nonnegative satisfies = 0 b0 > 0, b0 consists
rational entries b0 b. Thus, vector rational entries = 0 b0 > 0,
b > 0.

486

fiWeighted Regret-Based Likelihood

denominators entries , assume without loss generality
entries natural numbers.
Since 2N + N rows, vector form (1 , . . . , 2N +N ). Let A1 , . . . , A2N +N
rows A; vector length N . Since = 0, means
1 A1 + + 2N +N A2N +N = 0. Suppose 2N , . . . , 2N +N 1 (the coefficients
rows corresponding inequalities xi 0 = 1, . . . , N ) 0; show
below, assumption made without loss generality.
assumption, rewrite equations 1 A1 +. . . 2N 1 A2N 1 = 2N +N A2N +N .
E1 , . . . , E2N 1 subsets correspond equations A1 , . . . , A2N 1 ,
respectively, equation says 1 copies E 1 , 2 copies E 2 , . . . , 2N 1 copies
E 2N 1 form 2N +N -cover S. (Recall A2N +N row 1s, A2N +N corresponds S.) Thus, REG3, 1 f (E1 ) + + 2N 1 f (E2N 1 ) 2N +N f () = 2N +N .
Farkas Lemma requires b > 0, where, construction, bi = f (Ei ) =
1, . . . , 2N 1, bi = 0 = 2N , . . . , 2N + N 1, b2N +N = 1. Thus, must
(1 f (E1 )+ +2N 1 f (E2N 1 )) > 2N +N . Clearly, gives contradiction. Thus,
conclude, desired, equations solvable, exists probability
measure Pr Pr = 1.
N
N
remains show assume without loss generality 2 , . . . , 2 +N 1
0. Note since 0, must nonnegative. prove induction
2N + + 2N +N 1 vector 0 = 0 b > 0,
vector 2N + + 2N +N 1 = 0.
suppose solution 2N + + 2N +N 1 > 0. Suppose without
loss generality 2N > 0. Recall A2N corresponds inequality x1 0.
Choose j {0, . . . , 2N 1} j > 0 s1
/ Ej . must j,
otherwise would = 0. Let j 0 Ej 0 = Ej {s1 }. Define vector
/ {j, j 0 , 2N }.
0 20 N = 2N 1, j0 = j 1, j0 0 = j + 1, i0 =
0
0
0
easy check = 0 2N + + 2N +N 1 < 2N + + 2N +N 1 .
remains show 0 b > 0. Since Ej Ej 0 , must f (Ej ) f (Ej 0 ),
0 b = b + f (Ej ) f (Ej 0 ) b > 0. completes inductive step argument.
+ (E)
must show second required property holds, namely, f (E) = Preg
E S. construction, Pr Pr(E) f (E) E S, suffices show
Pr P Pr Pr(E) = f (E). this, suffices show exists
measure Pr Pr(E) = 1, E 0 S, f (E) Pr(E 0 ) f (E 0 ), since
Pr = f (E), Pr Pr(E) = f (E), desired.
show measure exists, construct set linear inequalities
much above, apply Farkas Lemma. Using notation above, suppose
simplicity E = {s1 , . . . , sM }, N . required inequalities involve
variables x1 , . . . , xM :
0

{i:s EE 0 } xi f (E 0 )/f (E), E 0 E E 6=

xi 0, = 1, . . . ,
x1 + + xM 1.
P

Again, requirement x1 + + xM 1 follows equation E.
system inequalities satisfiable, required probability measure,
suppose satisfiable. Again, writing system equations Ax b,
487

fiHalpern

Farkas Lemma, exists nonnegative vector = 0 b > 0.
proceed much before. Again, assume vector natural numbers.


assume 2 , . . . , 2 +M 1 (the coefficients rows corresponding
inequalities xi 0 = 1, . . . , N ) 0, fact = 0 means
2M +M cover E. get contradiction REG3 almost identical way
above. completes argument.
said earlier, set P + guaranteed exist Theorem 4.2 unique, although
canonical, sense unique maximal set weighted probability measures
represents f . might wonder actually get uniqueness imposing
extra requirements, particularly since Leung able representation
theorem. answer seems no. explain why, helpful review material
(Halpern & Leung, 2012).
Define sub-probability measure p probability measure (i.e., function
mapping measurable subsets [0, 1] p(T 0 ) = p(T ) + p(T 0 ) disjoint
sets 0 ), without requirement p(S) = 1. identify weighted
probability distribution (Pr, ) sub-probability measure Pr. Conversely, given
sub-probability measure p, unique pair (, Pr) P = Pr: simply
take = p(S) Pr = p/. Thus, sequel, identify set sub-probability
measures set weighted probability measures.
set B sub-probability measures downward-closed if, whenever p B q p,
q B.
One advantage considering sub-probability measures clear
would mean set weighted probabilities convex (indeed, obvious
count convex combination (Pr, ) (Pr0 , 0 )), quite clear counts
convex combination sub-probability measures. Moreover, convex combination
sub-probability measures sub-probability measure.
Call set subprobability measures regular convex, downward-closed, closed,
contains least one proper probability measure. (The latter requirement corresponds
Pr = 1 Pr P + .) Leung provide set axioms preference
orders, show family preference orders indexed menus satisfies
axioms iff unique regular set weighted probability measures P + that,

b iff wr
P + (a) wr P + (b). Thus, might hope get uniqueness
imposing regularity requirement. easy see canonical maximal set P +
constructed proof Theorem 4.2 regular, lends credence hope.
Unfortunately, following example shows, regularity suffice uniqueness.
Example 4.4: Let = {s1 , s2 }, let f defined 2S taking f ({s1 }) = 1/4
f ({s2 }) = 1 (and f (S) = 0 f () = 1). sub-probability measure p
identified pair (p(s1 ), p(s2 )), makes easy think sub-probability
measures geometrically. set sub-probability measures region IR2
contained triangle bounded lines x = 0, = 0, = 1 x. set P +
subprobability measures downward closed if, whenever contains point (x, y),
contains (x0 , 0 ) rectangle defined points (0, 0), (x, 0), (0, y), (x, y).
intuition, let P0+ set subprobabilities quadrilateral bounded
x = 0, = 0, = 1 x, = 1/4 (the region marked vertical lines Figure 1).
488

fiWeighted Regret-Based Likelihood

hard show P0+ maximal set weighted probabilities representing f .
+
clearly regular. Since contains subprobability (1, 0), follows P0,reg
({s2 }) = 1.
+
+
easy see that, since (0, 1/4) P0 p(s2 ) 1/4 p P0 ,
+
P0,reg
({s1 }) = 1/4.
let P1+ consist sub-probabilities triangle bounded x = 0, = 0,
+
= 1x
4 (the region marked horizontal lines Figure 1). Clearly P1 strict
+
subset P0 , clear figure regular. Moreover, since
contains points ( 41 , 0) (0, 1), represents f . Indeed, easily follows
geometry situation uncountably many regular sets weighted
probabilities representing f ; z [0, 34 ], regular set bounded lines x = 0,
= 0, = 14 , line (z, 41 ) (1, 0).

1

( 34 , 14 )

1
4

0

3
4

1

x

Figure 1: Regular sets weighted probability measures represent f .

Intuitively, problem function contain enough information
uniquely determine regular set weighted probability measures. clear whether
natural conditions imposed lead uniqueness.
seems closest come uniqueness consider maximal set.

5. Conclusion
defined approach associating event E numerical representation
likelihood uncertainty represented set weighted probability measures.
representation consists pair numbers, thought upper
lower bounds uncertainty. difference numbers viewed
measure ambiguity. two numbers coincide uncertainty represented
single probability. Moreover, probability measure gets weight 1,
two numbers essentially viewed lower upper probabilities E (more
precisely, 1 P (E) 1 P (E)). Thus, approach viewed generalization
lower upper probability case weighted probability measures, regretbased likelihood corresponding upper probability. definitions show
489

fiHalpern

interesting connection regret-based approaches minimization/maximization
approaches comes defining likelihood; connection breaks comes
general utility calculations (Halpern & Leung, 2012).
main technical result paper complete characterization likelihood
case state space finite. notion likelihood easily extended
case infinite state space (of course, integral used instead sum
calculate expected utility). conjecture characterization theorem still hold
essentially change, although checked details carefully.
course, would useful get better understanding numerical representation, see really captures agents feelings ambiguity risk
associated event, understand technical properties. leave future
work.

Acknowledgments
thank Samantha Leung, reviewers ECSQARU, JAIR referees many useful
comments paper. work supported part NSF grants IIS-0812045, IIS0911036, CCF-1214844, AFOSR grants FA9550-08-1-0438, FA9550-09-1-0266,
FA9550-12-1-0040, ARO grant W911NF-09-1-0281.

References
Anger, B., & Lembcke, J. (1985). Infinitely subadditive capacities upper envelopes
measures. Zeitschrift fur Wahrscheinlichkeitstheorie und Verwandte Gebiete, 68, 403
414.
Boole, G. (1854). Investigation Laws Thought Founded
Mathematical Theories Logic Probabilities. Macmillan, London.
Campos, L. M. d., & Moral, S. (1995). Independence concepts sets probabilities.
Proc. Eleventh Conference Uncertainty Artificial Intelligence (UAI 95), pp.
108115.
Cattaneo, M. E. G. V. (2007). Statistical decisions based directly likeihood function.
Ph.D. thesis, ETH.
Chateauneuf, A., & Faro, J. (2009). Ambiguity confidence functions. Journal
Mathematical Economics, 45, 535 558.
Couso, I., Moral, S., & Walley, P. (1999). Examples independence imprecise probabilities. Proc. First International Symposium Imprecise Probabilities
Applications (ISIPTA 99).
de Cooman, G. (2005). behavioral model vague probability assessments. Fuzzy Sets
Systems, 154 (3), 305358.
Dubois, D., & Prade, H. (1998). Possibility measures: qualitative quantitative aspects.
Gabbay, D. M., & Smets, P. (Eds.), Quantified Representation Uncertainty
490

fiWeighted Regret-Based Likelihood

Imprecision, Vol. 1 Handbook Defeasible Reasoning Uncertainty Management
Systems, pp. 169226. Kluwer, Dordrecht, Netherlands.
Epstein, L., & Schneider, M. (2007). Learning ambiguity. Review Economic
Studies, 74 (4), 12751303.
Fagin, R., Halpern, J. Y., & Megiddo, N. (1990). logic reasoning probabilities.
Information Computation, 87 (1/2), 78128.
Farkas, J. (1902). Theorie der enfachen ungleichungen. J. Reine und Angewandte Math.,
124, 127.
Gardenfors, P., & Sahlin, N. (1982). Unreliable probabilities, risk taking, decision
making. Synthese, 53, 361386.
Gardenfors, P., & Sahlin, N. (1983). Decision making unreliable probabilities. British
Journal Mathematical Statistical Psychology, 36, 240251.
Gilboa, I., & Schmeidler, D. (1989). Maxmin expected utility non-unique prior.
Journal Mathematical Economics, 18, 141153.
Gilboa, I., & Schmeidler, D. (1993). Updating ambiguous beliefs. Journal Economic
Theory, 59, 3349.
Giles, R. (1982). Foundations theory possibility. Gupta, M. M., & Sanchez, E.
(Eds.), Fuzzy Information Decision Processes, pp. 183195. North-Holland.
Good, I. J. (1980). history hierarchical Bayesian methodology. Bernardo,
J. M., DeGroot, M. H., Lindley, D., & Smith, A. (Eds.), Bayesian Statistic I, pp.
489504. University Press: Valencia.
Halpern, J. Y. (1997). Defining relative likelihood partially-ordered preferential structures. Journal A.I. Research, 7, 124.
Halpern, J. Y. (2003). Reasoning Uncertainty. MIT Press, Cambridge, Mass.
Halpern, J. Y., & Leung, S. (2012). Weighted sets probabilities minimax weighted
expected regret: new approaches representing uncertainty making decisions.
Proc. Twenty-Ninth Conference Uncertainty Artificial Intelligence (UAI 2012),
pp. 336345. appear, Theory Decision.
Halpern, J. Y., & Pucella, R. (2002). logic reasoning upper probabilities.
Journal A.I. Research, 17, 5781.
Huber, P. J. (1976). Kapazitaten statt Wahrscheinlichkeiten? Gedanken zur Grundlegung
der Statistik. Jahresbericht der Deutschen Mathematiker-Vereinigung, 78, 8192.
Huber, P. J. (1981). Robust Statistics. Wiley, New York.
Klibanoff, P., Marinacci, M., & Mukerji, S. (2005). smooth model decision making
ambiguity. Econometrica, 73 (6), 18491892.
491

fiHalpern

Kyburg, Jr., H. E. (1988). Higher order probabilities intervals. International Journal
Approximate Reasoning, 2, 195209.
Levi, I. (1985). Imprecision uncertainty probability judgment. Philosophy Science,
52, 390406.
Lorentz, G. G. (1952). Multiply subadditive functions. Canadian Journal Mathematics,
4 (4), 455462.
Maccheroni, F., Marinacci, M., & Rustichini, A. (2006). Ambiguity aversion, robustness,
variational representation preferences. Econometrica, 74 (6), 14471498.
Moral, S. (1992). Calculating uncertainty intervals conditional convex sets probabilities. Proc. Eighth Conference Uncertainty Artificial Intelligence (UAI
95), pp. 199206.
Nau, R. F. (1992). Indeterminate probabilities finite sets. Annals Statistics, 40 (4),
17371767.
Niehans, J. (1948). Zur preisbildung bei ungewissen erwartungen. Schweizerische Zeitschrift
fur Volkswirtschaft und Statistik, 84 (5), 433456.
Ostrogradsky, M. V. (1838). Extrait dun memoire sur la probabilite des erreurs des tribuneaux. Memoires dAcademie St. Petersbourg, Series 6, 3, xixxxv.
Pearl, J. (1987). need higher-order probabilities and, so, mean?.
Proc. Third Workshop Uncertainty Artificial Intelligence (UAI 87), pp. 4760.
Savage, L. J. (1951). theory statistical decision. Journal American Statistical
Association, 46, 5567.
Schrijver, A. (1986). Theory Linear Integer Programming. Wiley, New York.
Scott, D. (1964). Measurement structures linear inequalities. Journal Mathematical
Psychology, 1, 233247.
Walley, P. (1991). Statistical Reasoning Imprecise Probabilities, Vol. 42 Monographs
Statistics Applied Probability. Chapman Hall, London.
Walley, P. (1997). Statistical inferences based second-order possibility distribution.
International Journal General Systems, 26 (4), 337383.
Williams, P. M. (1976). Indeterminate probabilities. Przelecki, M., Szaniawski, K., &
Wojcicki, R. (Eds.), Formal Methods Methodology Empirical Sciences, pp.
229246. Reidel, Dordrecht, Netherlands.
Wolf, G. (1977). Obere und untere Wahrscheinlichkeiten. Ph.D. thesis, ETH, Zurich.
Zadeh, L. A. (1978). Fuzzy sets basis theory possibility. Fuzzy Sets Systems,
1, 328.

492


