Journal Artificial Intelligence Research 7 (1997) 161-198

Submitted 5/97; published 11/97

Storing Indexing Plan Derivations
Explanation-based Analysis Retrieval Failures
Laurie H. Ihrig
Subbarao Kambhampati

Department Computer Science Engineering
Arizona State University
Tempe, AZ 85287-5406

ihrig@asu.edu
rao@asu.edu

Abstract

Case-Based Planning (CBP) provides way scaling domain-independent planning
solve large problems complex domains. replaces detailed lengthy search
solution retrieval adaptation previous planning experiences. general, CBP demonstrated improve performance generative (from-scratch)
planning. However, performance improvements provides dependent adequate
judgements problem similarity. particular, although CBP may substantially reduce planning effort overall, subject mis-retrieval problem. success CBP
depends retrieval errors relatively rare. paper describes design
implementation replay framework case-based planner dersnlp+ebl. dersnlp+ebl extends current CBP methodology incorporating explanation-based learning
techniques allow explain learn retrieval failures encounters.
techniques used refine judgements case similarity response feedback
wrong decision made. failure analysis used building case
library, addition repairing cases. Large problems split stored
single goal subproblems. Multi-goal problems stored smaller cases fail
merged full solution. empirical evaluation approach demonstrates
advantage learning experienced retrieval failure.

1. Introduction
Case-Based Planning improves eciency plan generation taking advantage previous problem-solving experiences. shown effective method scaling
domain-independent planning solve large problems complex domains (Kambhampati & Hendler, 1992; Veloso, 1994). CBP involves storing information particular
planning episodes problems successfully solved. information may include
goals achieved, world state conditions found relevant
achievement, final plan decisions taken arriving
plan. Whenever new problem encountered, judgment made similarity
previous experiences. Similar cases reused extended search
solution new problem. example, previous plan may transformed
skeletal plan refined new solution (Friedland & Iwasaki, 1985;
Kambhampati & Hendler, 1992; Hanks & Weld, 1995). Multiple cases, corresponding
small subproblem, may combined extended solving single larger problem
(Redmond, 1990; Ram & Francis, 1996). Alternatively, plan derivations may replayed
c 1997 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiIhrig & Kambhampati

provide guidance new search process (Veloso, 1994; Ihrig & Kambhampati, 1994a).
CBP improves problem-solving problems solved less time comparison
generative planning.
One challenging tasks CBP determining cases store
match cases new problem-solving context. complex domain, unlikely
problem seen once. Moreover, every problem solved
stored, library large cost associated retrieval may overshadow
gains provides (Koehler, 1994; Francis & Ram, 1995). Ultimately, would
retain library minimum number cases new problems
solved ecient retrieval adaptation cases stored (Smyth &
Keane, 1995). However, complex domains, planner's theory problem similarity
incomplete (Barletta & Mark, 1988). information relevant
features new situation determine stored case applicable. Sometimes
new problem contain extra goals and/or changed initial state conditions.
changes may mean solution cannot found consistent earlier
planning decisions made stored episode. planner cannot predict ahead time
previous choices wrong current situation, experience retrieval
error.
paper, introduce dersnlp+ebl (DERivational Systematic NonLinear Planner
+ Explanation-Based Learning), CBP system priar (Kambhampati & Hendler,
1992) spa (Hanks & Weld, 1995) based sound complete domain-independent
planner. dersnlp+ebl deals mis-retrieval problem allowing planner
learn planning failures may anticipate future errors. Failure explanations
automatically generated search process used extending case
new problem-solving situation. used building case library
addition repairing cases.
Although earlier systems chef (Hammond, 1990) exploited EBL techniques, use restricted reasoning correctness plans generated
case-based planner. contrast, dersnlp+ebl starts sound complete plan
synthesis strategy. emphasis improving performance base-level
planner guidance retrieved cases. guidance considered succeed
leads planner search path leading solution new problem.
retrieval error occurs planner directed wrong path search
solution, is, path lead solution. dersnlp+ebl extends current
CBP methodology EBL techniques employed automatic generation reasons retrieval failure. Analytical failures occur leaf nodes
search tree explained terms subsets con icting plan constraints. leaf
node failure explanations regressed failing search paths form reason
retrieval failure.
dersnlp+ebl builds indexes case library based failure analysis.
failure reason used construction new repairing case. example, retrieved
case fails due presence extra interacting goal covered retrieved
episodes, explanation failure formed identifies subset new input
goals negatively interacting. failure reason used construct new case
solves goals alone. failure analysis employed refining
162

fiStoring Indexing Plan Derivations

Retriever

Library

Cases Problem

Planning
Problem

Storer

Plan Derivation
Case Failure Reason

Case-Based Planner
Replay/Extension[Recovery]

Problem
Solution

Domain Operators

Figure 1: schematic diagram illustrating approach dersnlp+ebl.
indexing case library censor retrieval failing case whenever
interacting goals present again, direct retriever new repairing case
avoids failure.
dersnlp+ebl's failure-based storage strategy limits size case library. Library
size reduced splitting problems single goal subproblems, storing separately. Large problems solved retrieval adaptation multiple
instances smaller cases. Multi-goal problems stored retrieved
cases fail merged extended full solution. describe empirical studies
demonstrate substantial improvements performance novel approach
multi-case adaptation.
remainder paper organized follows: Section 2 describes dersnlp+ebl learns case failure improve case retrieval. reports
preliminary experiments testing learning component. Section 3 provides ecient techniques used store, retrieve adapt multiple cases. describes experiments test
dersnlp+ebl's method plan merging. Section 4 describes evaluation full dersnlp+ebl system solving large problems drawn complex domain. Section 5
relates work previous case-based planners, including chef prodigy/analogy.
Section 6 provides summary.

2. Learning Case Failure
stated earlier, dersnlp+ebl based complete correct domain-independent
planning strategy. priar (Kambhampati & Hendler, 1992) spa (Hanks & Weld,
1995), implemented partial-order planner. aspect differs statespace systems prodigy/analogy (Veloso & Carbonell, 1993a; Veloso, 1994)
paris (Bergmann & Wilke, 1995). prodigy/analogy, employs case adaptation
strategy, derivational replay stores planning experience form successful plan
derivations. Previous decisions made earlier planning episodes become instructions
guide search process solving new problem. Derivational replay includes
following elements, illustrated Figure 1 (Veloso, 1994; Veloso & Carbonell, 1993a):
facility within underlying planner generate trace derivation plan,
163

fiIhrig & Kambhampati

null
plan

skeletal
plan
e1

final
plan

X
e2 X e3 X
- - - - - - - - - - - - - - - - - - - -depth limit
X

Figure 2: Multiple derivation traces (each sequence decisions shown figure
rectangles) used guide new search process. figure, solution
could reached backtracking skeletal plan, lies
outside new plan derivation (shown filled circles).
indexing storage derivation trace library previous cases, retrieval
multiple cases preparation solving new problem, finally, replay mechanism
planner employs retrieved plan derivations sequence instructions
guide new search process.
dersnlp+ebl's methodology depends aspects common molgen
(Friedland & Iwasaki, 1985) priar (Kambhampati & Hendler, 1992). requires
eager case adaptation strategy, skeletal plan constructed contains
constraints added advice retrieved cases, constraints.
separate failure resulting previous guidance subsequent
planning effort. eager case adaptation, planning decisions encapsulated
retrieved cases greedily adopted decisions extended solve
extra goals covered. Multiple retrieved plan derivations replayed sequence
produce skeletal plan contains recommended plan constraints.
planner returns from-scratch planning previous decisions
retrieved cases visited. skeletal plan refined achieve
goals left open. Previous work demonstrated effectiveness approach
plan-space replay well advantage state-space replay (Ihrig & Kambhampati,
1994a, 1994b).
Eager case adaptation described extension-first. skeletal plan
first extended search solution, and, extension fails, plan
backtracked over, discarding plan constraints added advice previous
episodes. general approach case adaptation therefore involves three distinct phases:
case replay, case extension, and, extension fails, recovery. search process
employed extending skeletal plan, planner constructs explanation
plan's failure becomes reason case retrieval failure. Explanations formed
analytical failures occur leaf nodes directly skeletal plan (See
Figure 2). analytical failure explained set inconsistent plan constraints.
failure explanations immediately regressed search paths encountered.
regressed explanations collected root tree form reason
164

fiStoring Indexing Plan Derivations

l1

l1

OB1

ld

ld

lp

lp

l2

OB2

l2

(b) New Problem Extra
Goal

(a) Previous Plan

Figure 3: (a) plan accomplish transport single package, ob1, destination
airport ld . (b) new problem contains extra goal involves additional
transport ld second package, ob2.
retrieval error. dersnlp+ebl detects retrieval error occurred ways
refining skeletal plan tried, planner forced backtrack
plan. point failure reason fully constructed. Performing skeletal plan
extension separate process prior recovery allows planner identify retrieval
error terms failure skeletal plan, construct reason failure.
reason communicated Storer used augmenting library
new repairing case.
Consider simple example illustrated Figure 3 taken Logistics
Transportation domain shown Figure 4. goal package ob1 located
destination location ld. package initially location l1 . plane located
lp used transport package. Figure 3a illustrates previous plan
contains steps determine plane's route destination airport well steps
accomplish loading package right place along route. Eagerly
replaying earlier step addition decisions new problem extra
package transport produces skeletal plan may readily extended include
loading unloading extra package long package lies along
route. However, new package old route, planner may able
solve extra goal without backtracking previous step addition decisions.
(See Figure 3b).
case failure reason shown Figure 5. gives conditions future
replay case result failure. conditions refer presence
new problem set, C , negatively interacting goals, well initial state
conditions, contained E . summary information content failure reason is:
extra package transport destination location, package
destination location, inside plane, located plane's
route.
165

fiIhrig & Kambhampati

action
precond
add
delete

(LOAD-TRUCK ?O ?T ?L)
(AT-OB ?O ?L)
(AT-TR ?T ?L)
(INSIDE-TR ?O ?T)
(AT-OB ?O ?L)

action
precond
add
delete

(LOAD-PLANE ?O ?P ?L)
(AT-OB ?O ?L)
(AT-PL ?P ?L)
(INSIDE-PL ?O ?P)
(AT-OB ?O ?L)

action
precond
add
delete

(UNLOAD-TRUCK ?O ?T ?L)
(INSIDE-TR ?O ?T)
(AT-TR ?T ?L)
(AT-OB ?O ?L)
(INSIDE-TR ?O ?T)

action
precond
add
delete

(UNLOAD-PLANE ?O ?P ?Li)
(INSIDE-PL ?O ?A)
(AT-PL ?P ?Li)
(AT-OB ?O ?Li)
(INSIDE-PL ?O ?A)

action
precond
add
delete
equals

(DRIVE-TRUCK ?T ?Li ?Lg)
(AT-TR ?T ?Li)
(SAME-CITY ?Li ?Lg)
(AT-TR ?T ?Lg)
(AT-TR ?T ?Li)
(NOT (?Li ?Lg))

action
precond
add
delete
equals

(FLY-PLANE ?P ?Li ?Lg)
(IS-A AIRPORT ?Lg)
(AT-PL ?P ?Li))
(AT-PL ?P ?Lg)
(AT-PL ?P ?Li)
(NOT (?Li ?Lg))

Figure 4: specification Logistics Transportation Domain adapted experiments
Subsequent backtracking skeletal plan, planner continues search,
go find solution full problem one exists. new solution achieves
negatively interacting goals identified failure reason. Moreover, since
goals represent subset problem goals, new derivation may used construct
case covering goals alone. dersnlp+ebl stores new case directly beneath
failing case censor retrieval. ensure whenever failure reason
holds (for example, whenever extra package plane's route),
retriever directed away failing case toward case repairs failure.
position describe detail dersnlp+ebl's eager derivation
replay strategy, well learns reasons underlying case failure.

2.1 Eager Derivation Replay

derivation trace contains sequence instructions representing choices lie
along derivation path leading root search tree final plan leaf
node. trace fitted context new search process validating choice
new context, replaying decision valid. order understand validation
process, must first describe decision steps planner takes arriving
solution planning problem. planning problem 3-tuple hI; G; Ai,
complete description initial state, G description goal state,
set operators strips representation (Fikes & Nilsson, 1971). ground operator
sequence said solution planning problem executed initial
state, resulting state world satisfies goal.
dersnlp+ebl refinement planner solves planning problem navigating
space potential solutions, represented partly constructed plan1 . Syntactically,
1. formal development refinement search semantics partial plans, refer reader
work Kambhampati, Knoblock, Yang (1995).

166

fiStoring Indexing Plan Derivations

Case Failure Explanation:
C = f h(AT-OB OB1 ld); tG i; h(AT-OB OB2 ld); tG g
E = f htI ; (:AT-OB OB2 ld)i; htI ; (:INSIDE-PL OB2 ?PL )i;
htI ; (:AT-OB OB2 l1)i; htI ; (:AT-OB OB2 lp)i g

Figure 5: Example case failure reason
plan space P seen set constraints (see below). Semantically, partial
plan shorthand notation set ground operator sequences consistent
constraints. latter called candidate set partial plan, denoted
hhPii. particular, partial plan represented 6-tuple, hS ; O; B; L; E ; Ci,
1. set actions (step names) plan, mapped onto
operator domain theory. contains two dummy steps: tI whose effects
initial state conditions, tG whose preconditions input goals, G.
2. B set codesignation (binding) non-codesignation (prohibited binding) constraints variables appearing preconditions post-conditions
operators represented plan steps, .
3. partial ordering relation , representing ordering constraints
steps .
4. L set causal links form hs; p; s0 s; s0 2 .
5. E contains step effects, represented hs; ei, 2 .
6. C set open conditions partial plan, tuple hp; si
p precondition step link supporting p L.
Planning consists starting null plan (denoted P;) , whose candidate set
corresponds possible ground operator sequences, successively refining plan
adding constraints solution reached. planning decision represents choice
resolve existing aw plan, either open condition (unachieved
goal) threat causal link. understand choices validated
replay process useful think planning decision operator acting
partly-constructed plan. possible choices available dersnlp+ebl shown
Figure 6.
Planning decisions preconditions based existence aw
current active plan effects alter constraints eliminate
aw. example, precondition establishment choice specified terms
existence unachieved subgoal. effect addition causal link achieves
open condition. precondition resolution decision threat one step
clobbering existing causal link. threat resolved adding step ordering
either promotes demotes clobberer relative causal link.
167

fiIhrig & Kambhampati

Type : ESTABLISHMENT
Kind : NEW STEP
Preconditions
:
hp0 ; s0 2 C
Effects
:
00 = [ fsg 0
O0 = [ fs g 0
B 0 = B [ unify(p;0 p )
L = L [ fhs; p;s ig
E 0= E [ effects
(s)
C = C , fhp0 ; s0 ig
[ preconditions(s)

Type : ESTABLISHMENT
Kind : NEW LINK
Preconditions
:
hp0 ; s0 2 C
Effects
:
O00 = [ fs s0 g 0
B 0 = B [ unify(p;0 p )
L0 = L [ fhs;p;
ig
C = C , fhp0 ; s0 ig

Type : RESOLUTION
Kind : PROMOTION
Preconditions
:
0
0

hs; p ; 2 L
ht; :p 2 E
ft sg; fs tg 62
Effects :
= [ ft sg
0

0

0

Type : RESOLUTION
Kind : DEMOTION
Preconditions
:
0
0

hs; p ; 2 L
ht; :p 2 E
ft sg; fs tg 62
Effects :
= [ fs tg
0

0

0

0

Figure 6: Planning decisions based active plan hS ; O; B; L; E ; Ci effects alter constraints produce new current active plan
hS 0 ; O0 ; B0 ; L0 ; E 0 ; C 0 i.
decision replayed, first compared current active plan determine whether precondition holds new context. Invalid decisions, whose
preconditions don't match, skipped. Establishment decisions ignored goals
achieve present open conditions current active plan. Threat resolutions skipped threat present. Previous choices justified
current situation used guidance direct new search process. Replaying valid
decision involves selecting match decision children current active
plan, making child next plan refinement.
dersnlp+ebl's eager derivation replay strategy replays applicable decisions
trace sequence. replay strategy contrasted
prodigy/analogy (Veloso, 1994) replay alternated from-scratch planning
extra goals covered case. eager derivation replay previous decision
eagerly adopted justified current context. Since invalid instructions
skipped, skeletal plan end result replay comparable product
fitting phase plan reuse (Kambhampati & Hendler, 1992; Hanks & Weld, 1995).
contrast plan reuse, derivation replay alter underlying planning strategy.
Replay merely provides search control, directing search node visit next.
means dersnlp+ebl inherits properties snlp, including soundness,
completeness, systematicity.
sample trace snlp's decision process shown Figure 7. trace corresponds
simple problem logistics transportation domain (Veloso, 1994) adapted
snlp Figure 4. problem contains goal getting single package, ob1,
designated airport, ld . derivation trace contains choices made along
path root search tree final plan leaf node. Instructions contain
description decision taken basis justification new context.

2.2 Eager Case Extension Recovery
decisions trace skipped replay known
priori unjustified. guarantee skeletal plan left
168

fiStoring Indexing Plan Derivations

Goal : (AT-OB OB1 ld )
Initial : ((IS-A AIRPORT ld ) (IS-A AIRPORT li ))
(IS-A AIRPORT lp ) (AT-PL PL1 lp )
(AT-OB OB1 li ) ...
Name : G1
Name : G7
Type : START-NODE
Type : ESTABLISHMENT
Name : G2
Kind : NEW LINK
Type : ESTABLISHMENT
New Link: (0 (IS-A AIRPORT ld ) 2)
Kind : NEW STEP
Open Cond: ((IS-A AIRPORT ld ) 2)
New Step: (UNLOAD-PL OB1 ?P1 ld )
Name : G8
New Link: (1 (AT-OB OB1 ld ) GOAL)
Type : ESTABLISHMENT
Open Cond: ((AT-OB OB1 ld ) GOAL)
Kind : NEW STEP
Name : G3
New Step: (LOAD-PL OB1 PL1 ?A4)
Type : ESTABLISHMENT
New Link: (4 (INSIDE-PL OB1 PL1) 1)
Kind : NEW STEP
Open Cond: ((INSIDE-PL OB1 PL1) 1)
New Step: (FLY-PL ?P1 ?A2 ld )
Name : G9
New Link: (2 (AT-PL ?P1 ld ) 1)
Type : ESTABLISHMENT
Open Cond: ((AT-PL ?P1 ld ) 1)
Kind : NEW LINK
Name : G4
New Link: (3 (AT-PL PL1 li ) 4)
Type : ESTABLISHMENT
Open Cond: ((AT-PL PL1 ?A4) 4)
Kind : NEW STEP
Name : G10
New Step: (FLY-PL ?P1 ?A3 ?A2)
Type : RESOLUTION
New Link: (3 (AT-PL ?P1 ?A2) 2)
Kind : PROMOTION
Open Cond: ((AT-PL ?P1 ?A2) 2)
Unsafe-link : ((3 (AT-PL PL1 li ) 4)
Name : G5
Effect : 2 :(AT-PL PL1 li ))
Type : ESTABLISHMENT
Name : G11
Kind : NEW LINK
Type : ESTABLISHMENT
New Link: (0 (AT-PL PL1 lp ) 3)
Kind : NEW LINK
Open Cond: ((AT-PL ?P1 ?A3) 3)
New Link: (0 (AT-OB OB1 li ) 4)
Name : G6
Open Cond: ((AT-OB OB1 li ) 4)
Type : ESTABLISHMENT
Key Abbreviations:
Kind : NEW LINK
PL = PLANE
New Link: (0 (IS-A AIRPORT li ) 3)
OB = OBJECT
Open Cond: ((IS-A AIRPORT ?A2) 3)
Final Plan: (FLY-PL PL1 lp li ) Created 3
(LOAD-PL OB1 PL1 li ) Created 4
(FLY-PL PL1 li ld ) Created 2
(UNLOAD-PL OB1 PL1 ld ) Created 1
Ordering Steps: ((4 < 2) (3 < 4) (4 < 1) (3 < 2) (2 < 1))

Figure 7: Example solution trace dersnlp+ebl

169

fiIhrig & Kambhampati

2 DmS 1 :
(Affi precond : fIi; Pff g add : fgig delete : fIj jj < ig)
(Afii precond : fIi Pfi g add : fgig delete : fIj jj < ig)
(Aff precond : fg add : fgff g delete : fPfi g [ fgij8ig)

Figure 8: specification Barrett Weld's Transformed Dm 1Domain
ultimately refined solution current problem. Without actually completing
search way predicting whether constraints left skeletal
plan consistent complete solution. Whenever skeletal plan complete
(whenever extra goals unsatisfied initial state conditions) planner must
undergo planning effort extend plan possibility effort
may fail, necessitating recovery phase.
dersnlp+ebl, skeletal plan extended first, prior recovery. plan
backtracked search process fails refine full solution new
problem. strategy requires depth limit placed search tree2 . Otherwise
skeletal plan extension may continue indefinitely, planning algorithm becomes incomplete. eager extension strategy not, however, linked particular search method.
example, may used best-first, depth-first iterative deepening search.
different search methods used exploration subtree skeletal plan,
prior backtracking plan. skeletal plan found fail, recovery
phase initiated merely involves exploring siblings replayed path.
extension, recovery linked particular search strategy.

2.3 Analyzing Failure Case Extension

order skeletal plan successfully extended achieve conditions left open,
sequence decisions adopted guidance previous trace must
concatenated choices arrive solution. occur, replayed
path must decision-sequencable respect new problem, defined
follows:

Definition 1 (Decision-Sequencable Search Path) search path contains sequence decisions decision-sequencable respect new problem, hI 0 ; G0 ; Ai ,
exist two decision sequences E E 0 E E 0 (where \"
decision sequencing operator) produce plan correct hI 0 ; G0 ; Ai.
One primary reasons replayed path may decision sequencable goal
interactions occur input goals new problem. particular, extra
goals achieved case may interact covered, making retrieved
case inapplicable. long recognized relative diculty problem-solving
linked level interaction various input goals problem (Korf,
1987; Joslin & Roach, 1990; Barrett & Weld, 1994; Veloso & Blythe, 1994; Kambhampati,
2. practice, limit actually bound placed number steps contained plan.

170

fiStoring Indexing Plan Derivations

Ihrig, & Srivastava, 1996a). Goal interaction formalized Korf (1987) terms
problem search space. Barrett Weld (1994) extend Korf's analysis plan
space. plan-space planner, order goals achieved crucial.
Goals laboriously serializable state-space planner (in exist goal
orderings goals may solved sequence) may trivially serializable
plan-space planner (meaning goals solved order).
However, goals always trivially serializable plan-space planner (Veloso &
Blythe, 1994). example, consider 2Dm 1 domain (Barrett & Weld, 1994) shown
Figure 8. Notice gff one set problem goals, true initially,
goal, gi , present set must achieved operator Affi,
Afii . means time case replayed previously solved goal, gi ,
action Afii , gff extra goal covered case, replay fail.
CBP, however, much concerned general properties
domain, properties particular search paths stored
case library. required input goals every problem trivially serializable
CBP beneficial planning performance. were, would
domains CBP effective. Trivial serializability requirement since
necessary every plan every subset input goals consistent
solution full problem. particular plans retrieved
library concerned with.
Even goals problem trivially serializable, replay may decision
sequencable, depending cases actually retrieved library.
2 Dm 1 domain, single-goal cases retrieved solve gi action Afii ,
decision-sequencable new multi-goal problem contains
goal gff . However stored cases solved Affi , replay cases
sequencable. fact, aim dersnlp+ebl's learning component achieve
indexing within case library new problems encountered
planner may solved sequenced replay cases retrieved library.
next section describes dersnlp+ebl able work towards objective
learning component learns replay failures.

2.4 Constructing Reasons Retrieval Failure
dersnlp+ebl constructs explanations retrieval failures use explanationbased learning techniques allow planner explain failures individual plans
planner's search space. leaf node plan represents analytical failure
contains set inconsistent constraints prevent plan refined
solution. analytical failure explained terms constraints (Kambhampati, Katukam, & Qu, 1996b). Leaf node failure explanations identify minimal set
constraints plan together inconsistent. dersnlp+ebl forms explanations
analytical failures occur subtree directly skeletal plan.
regressed failing search paths collected root tree
form reason retrieval failure (See Figure 9a). regressed explanation terms
new problem specification. contains subset interacting goals, well initial
state conditions relevant goals.

171

fiIhrig & Kambhampati

null
plan

e11

< (AT-OB OB2 ld), tG >
<tI , (AT-OB OB2 ld ) >

e1f-1
df-1
e1f
df
e1

null
plan

< (AT-OB OB2 ld), tG >
<tI , (AT-OB OB2 ld) >

skeletal
plan

df-1
df

skeletal
plan

X

X
e2

X

e3

X

< tI , (AT-OB OB2 ld ), tG >
< tI , (AT-OB OB2 ld) >

(a) Regression Process

(b) Detailed Example

Figure 9: path failure explanation root tree computed e11 = d,1 1 (d,2 1
(d,f 1(e1 )) ).
Since plan failure explained subset constraints, failure explanations
represented manner plan itself. Recall dersnlp+ebl represents
plans 6-tuple, hS ; O; B; L; E ; Ci (See Section 2). explanation failure
occurring leaf node contains constraints contribute inconsistency.
inconsistencies appear new constraints added con ict existing
constraints. discussed Section 2, dersnlp+ebl makes two types decisions, establishment resolution. type decision may result plan failure. establishment
decision represents choice method achieving open condition, either
new/existing step, adding causal link initial state. attempt
made achieve condition linking initial state effect, condition
satisfied initial state, plan contains contradiction. explanation
failure constructed identifies two con icting constraints:

h;; ;; ;; fhtI ; p; sig; fhtI ; :pig; ;i

precondition resolution decision threat causal link. dersnlp+ebl
uses two methods resolving threat, promotion demotion, adds step
ordering plan. either decision adds ordering con icts existing
ordering, explanation failure identifies con ict:

h;; fs s0 ; s0 sg; ;; ;; ;; ;i

con icting constraints failure explanation regressed final
decision, results sorted according type form new regressed explanation.
process illustrated graphically Figure 9b. example, new link
initial state results failure. explanation, e1 is:
h;; ;; ;; fhtI ; (AT,OB OB 2 ld ); tG ig; fhtI ; :(AT,OB OB 2 ld)ig; ;i
172

fiStoring Indexing Plan Derivations

e1 regressed final decision, df , obtain new explanation, initial
state effect regresses itself. However, since link explanation added
decision, df , link regresses open condition precondition adding
link. new explanation, ef1 , therefore

h;; ;; ;; ;; fhtI ; :(AT,OB OB 2 ld)ig; fh(AT,OB OB 2 ld ); tG igi

regression process continues failing path reaches root search
tree. paths subtree underneath skeletal plan failed,
failure reason root tree provides reason failure retrieved
cases. represents combined explanation path failures. case failure
reason contains aspects new problem responsible failure.
may contain subset problem goals. Also, initial state effects
present leaf node explanation, present reason case failure3 .

2.5 Empirical Evaluation Utility Case Failure Analysis

preliminary study conducted aim demonstrating advantage storing
retrieving cases basis experienced retrieval failure. Domains chosen
randomly generated problems contained negatively interacting goals, planning
performance tested dersnlp+ebl solving multi-goal problems scratch
replay single cases covering smaller subset goals. Replay performance
tested without case failure information.
2.5.1 Domains

Experiments run problems drawn two domains. first artificial
domain, 2 DmS 1, originally described (Barrett & Weld, 1994) shown Figure 8.
Testing done problems randomly generated domain
restriction always contain goal gff . Logistics Transportation domain
(Veloso, 1994) adopted second set experiments. Eight packages one
airplane randomly distributed four cities. Problem goals represented task
getting one packages single destination airport4. fly operator augmented delete condition prevented planes visiting airport
once. meant replay failed extra package transported
previous route taken plane.
2.5.2 Retrieval Strategy

Cases initially retrieved basis static similarity metric takes
account goals covered case well relevant initial state
conditions (Kambhampati, 1994; Veloso, 1994). Prior studies show reasonably
3. dersnlp+ebl's EBL component explains analytical failures. Depth limit failures ignored.
means failure explanations formed sound case depth limit failure,
retriever may reject case applicable. Rejecting applicable case may lead
storage duplicate cases larger library size. However, empirical work shown
practical importance reasons outlined Section 3.2.2.
4. comprehensive evaluation unbiased problem set see Section 4.

173

fiIhrig & Kambhampati

effective metric. learning mode, cases retrieved basis. However,
mode, failure reasons attached case used censor retrieval.
time case retrieved learning mode, failure conditions
tested. failure reason satisfied new problem specification, retrieval
mechanism returned case replay. If, hand, failure reason found
true new problem context, case repaired failure retrieved.
Following retrieval, problem solved replay retrieved case well
planning scratch.
2.5.3 Experimental Setup

experiment consisted three phases, phase corresponding increase
problem size. Goals randomly selected problem, and, case
logistics domain, initial state randomly varied problems. initial
training session took place start phase n, 30 n-goal problems solved
scratch, derivation trace stored library. Following training,
testing session consisted generating problems manner additional
goal. time new (n +1) goal problem tried, attempt made retrieve
similar n-goal problem library. testing session, case
similar new problem found previously failed, problem
solved learning, static from-scratch modes, became part 30-problem
set. method, able evaluate improvements provided failurebased retrieval retrieval static metric alone ineffective, failure
conditions available.
2.5.4 Experimental Results

results experiments shown Tables 1 2. table entry represents
cumulative results obtained sequence 30 problems corresponding one phase
experiment. first row Table 1 shows percentage problems correctly solved
within time limit (550 seconds). average solution length shown parentheses
logistics domain (solution length omitted 2 DmS 1 since problems
generated within phase solution length). second third rows
Table 1 contain respectively total number search nodes visited 30 test
problems, total CPU time (including case retrieval time).
results summarized Figure 10. dersnlp+ebl learning mode
able solve many multi-goal problems two modes
substantially less time. Case retrieval based case failure resulted performance
improvements increased problem size. Comparable improvements
found retrieval based static similarity metric alone.
surprising since cases retrieved experienced least one earlier failure.
meant testing done cases likelihood failing retrieval
based static metric.
Table 2 records three different measures ect effectiveness replay. first
percentage sequenced replay. Recall replay trace considered
sequenced skeletal plan refined reach solution new problem.
174

fiStoring Indexing Plan Derivations

2 DmS 1

Static

Scratch

Learning

Logistics
Static

Scratch

100%
90
1

100%
240
4

100%
300
2

100% (6.0)
1773
30

100% (6.0)
1773
34

100% (6.0)
2735
56

% Solved
nodes
time(sec)

100%
120
2

100%
810
15

100%
990
8

100% (8.2)
6924
146

100% (8.2)
13842
290

100% (8.2)
20677
402

% Solved
nodes
time(sec)

100%
150
3

100%
2340
41

100%
2533
21

100% (10.3)
290
32

100% (10.3)
38456
916

100% (10.3)
127237
2967

Phase

Learning

%Solved
nodes
time(sec)

(1) Two Goal

(2) Three Goal

(3) Four Goal

Table 1: Performance statistics 2 DmS 1 Logistics Transportation Domain (Average
solution length shown parentheses next %Solved logistics domain
only)

(a) 2Dm 1

(b) Logistics

Figure 10: Replay performance 2 DmS 1and Logistics Transportation domain.
175

fiIhrig & Kambhampati

2 DmS 1

Logistics
Learning
Static

Phase

Learning

Static

% Seq
% Der
% Rep

100%
60%
100%

0%
0%
0%

53%
48%
85%

53%
48%
85%

% Seq
% Der
% Rep

100%
70%
100%

0%
0%
0%

80%
63%
89%

47%
50%
72%

% Seq
% Der
% Rep

100%
94%
100%

0%
0%
0%

100%
79%
100%

70%
62%
81%

Two Goal

Three Goal

Four Goal

Table 2: Measures effectiveness replay.
results point greater eciency replay learning mode. 2 Dm 1 domain,
replay entirely sequenced mode. transportation domain, retrieval based
failure always result sequenced replay, often static
mode.
greater effectiveness replay learning mode indicated two
measures contained subsequent two rows Table 2. respectively, percentage plan refinements final derivation path formed guidance
replay (% Der), percentage total number plans created replay remain final derivation path (% Rep). case-based planner learning
mode showed much greater improvements according measures, demonstrating relative effectiveness guiding retrieval learning component based
replay failures. results indicate dersnlp+ebl's integration CBP EBL
promising approach extra interacting goals hinder success replay.
Section 4 report thorough evaluation dersnlp+ebl's learning component. conducted purpose investigating learning case failure
benefit planner solving random problems complex domain. evaluation implemented full case-based planning system along novel case storage
adaptation strategies. next section, describe storage strategy
developed evaluation.

3. Improving Case Storage Adaptation
aim case-based planning eciently solve large problems complex domains.
complex domain means great variety problems encountered. problem size
(measured terms number goals, n) large, unlikely n-goal
problem seen before. therefore advantage able store cases
covering smaller subsets goals, retrieve adapt multiple cases solving single
large problem.
176

fiStoring Indexing Plan Derivations

implementing strategy, decisions made goal combinations store. previous work within state-space planning Veloso (1994) developed
approach reducing size library first transforming totally ordered plan
partially ordered graph, separating connected components graph, storing
subplans individually. Goals interact respective plans must
interleaved order form complete solution stored together single case.
replay based plan-space planner snlp, component subplan may
subdivided, since planner ability first piece plans together, later add step
orderings interleave subplans (Kambhampati & Chen, 1993; Ihrig & Kambhampati,
1994a). Replay smaller cases sequenced long individual subplans
may interleaved addition step orderings form full solution. plan-space
planner therefore greater capability reducing size problems stored
library, and, consequence, number cases stored.
dersnlp+ebl's storage strategy makes use plan-space planners' ability piece
small plans together, add step orderings interleave plans. earlier approaches, priar (Kambhampati & Hendler, 1992), prodigy/analogy (Veloso,
1994) caplan (Munoz-Avilla & Weberskirch, 1996), cases stored cover
smaller subsets original set input goals achieved successful problem-solving
episode. dersnlp+ebl differs earlier approaches division goal
subsets based structure final plan alone, sequence events
making problem-solving episode. new repairing case stored cases
retrieved library solving new problem fail extended new
solution. storer constructs new case based failure explanation obtained extension phase well new successful plan derivation obtained
recovery.
failure explanation identifies set negatively interacting goals responsible
failure. goals form subset input goals achieved new
solution. repairing case stored, new plan derivation stripped
decisions irrelevant achievement interacting goals. new case
covers negatively interacting goals.
Note define negative interaction based failure skeletal plan.
interaction occurs set input goals cannot solved refining skeletal plan,
causing planner backtrack plan. Moreover, cannot determine
whether two goals negatively interacting merely analyzing final solution.
include information planning failures encountered generating
solution. particular, final solution tell us whether additional goal
achieved extending replayed path, backtracking path. Approaches
case storage determine goal interaction final plan alone (Veloso, 1994;
Munoz-Avilla & Weberskirch, 1996) therefore ignore retrieval failures
encountered planning episode.
Retrieval failures provide important guidance library may improved
avoid similar failures. dersnlp+ebl, used dynamically improve storage
library addition new goal combinations. Multi-goal problems
stored retrieved cases corresponding single-goal subproblems fail merged
177

fiIhrig & Kambhampati

Case Failure Explanation:

C = fhgff; tG i; hg8 ; tG ig
E = fhtI ; i8 i; htI ; Pfi ig
Figure 11: Example case failure reason
extended new solution. Repairing cases constructed achieve negatively
interacting goals responsible identified failure explanation.

3.1 Example Negative Interaction
Figure 11 provides example explanation failure encountered solving
problem Barrett Weld's 2Dm 1 domain shown figure 8. problem contains
three goals, gff , g6 g8 , attempted replay case solves
two goals, gff g6 , second case, achieves g8 . latter,
goal achieved action Afi8 represents incorrect operator choice
input goals problem include goal gff .
failure explanation shown Figure 11 identifies subset interacting goals, made
g8 gff . Note interaction evident final plan shown Figure 12.
plan, three input goals problem achieved connected
component. base storage solely plan graph represented successful plan,
three input goals stored single case. Moreover, new problem
representing novel combination goals stored library, causing library
size increase exponentially problem size. example, suppose domain includes
goals, fgi j1 < < ng gff . number problems size three
number 3-goal subsets n + 1 goals. dersnlp+ebl's strategy storing cases
based explanations retrieval failure result maximum 2n + 1 cases stored.
goal fgi j1 < < ng appears two cases, one representing single-goal problem
one representing two goal problem achieves gff .
Storing negatively interacting goals multi-goal problems may therefore result
substantial reduction size case library. represents tradeoff,
replayed cases must extended from-scratch planning solve con icts
individual plans recommended separate cases. Moreover, complex domains,
may goals interact positively may solved common
steps (Ihrig & Kambhampati, 1996; Munoz-Avilla & Weberskirch, 1997). goals
stored separate cases, replay may result unnecessary redundancy plan.
dersnlp+ebl, positive interactions handled replay process itself,
merges subplans provided multiple cases. Section 3.3 describe
merging accomplished. next section provides detail case storage
strategy implemented empirical study.
178

fiStoring Indexing Plan Derivations

tI

1:

2:

3:

tG

Figure 12: Solution example problem.

3.2 Building Case Library
following deliberative strategy adopted building case library. new
problem contains n goals, first goal attempted, and, solved, case covering
goal alone stored library. Problem-solving continues increasing problem size
one goal time. example, problem attempted contained goal set,
G = hg1 ; g2 ; :::; gi solved decision sequence Di second decision
sequence, Di+1 , stored whenever Di cannot replayed extended achieve next
goal gi+1 . Whenever replayed derivation path fails, recovery phase successful
producing new solution, explanation case retrieval failure used identify
subset negatively interacting input goals, N = hgj :::gj +m i, responsible
failure. replayed path fails extended, backtracked reach solution
new problem, new successful derivation passed storer along
failure explanation. explanation used delete derivation decisions
relevant set negatively interacting goals, N . reduced derivation
stored library repairing case. Alternatively, whenever next goal
set solved simple extension previous decision sequence, case
stored includes goal.
storage strategy entails two important properties. (1) new case corresponds
either new single-goal problem multi-goal problem containing negatively interacting
goals. (2) plan derivations arising single problem-solving episode
different decision sequence stored library prefix another stored case.
case added library new problem solved extending
retrieved case. New cases stored previous decisions need
backtracked search new solution.
dersnlp+ebl's strategy restricting multi-goal cases goals
negatively interacting serves ameliorate mis-retrieval problem. experience
planner problem-solving, interactions discovered,
less likely planner backtrack replayed paths. aim
eventually library minimal number cases problems
encountered may achieved successfully merging multiple instances stored cases.
approach therefore retain cases based competence well performance
(Smyth & Keane, 1995).
3.2.1 Example dersnlp+ebl's Storage Strategy

example multi-goal problem stored, consider problem contained
Figure 13 three packages, ob1, ob2 ob3, transported
destination location, ld . Initially goal set contains goal transporting ob1 alone,
represented (at-ob ob1 ld ), successful derivation stored Case A.
second goal added set. Since problem attempted achieves first
179

fiIhrig & Kambhampati

OB1



OB3

B
B

ld

B
lp

OB2

l2

Figure 13: logistics transportation example illustrating multi-case storage. figure
shows two plans produced two stored derivations. Case achieves goal
single packages, ob1, transported destination airport, ld . Case
B achieves goal ob1 ob2 located airport.
goal decision sequence backtracked order solve
additional goal, second derivation, Case B , stored. new derivation solves
mutually interacting goals, (at-ob ob1 ld ) (at-ob ob2 ld ). Problem-solving
continues addition third goal. goal solved simple extension
previous decision sequence. case stored includes goal. means
two cases stored library: Case corresponding single-goal problem
Case B corresponding multi-goal problem containing two negatively interacting goals.
Multi-goal problems stored problem goals mutually interacting,
is, individual derivations cannot sequenced extended solve
full problem.
dersnlp+ebl's storage strategy, size library limited amount
interaction domain. example, negative interaction, single
goal cases stored. logistics transportation domain, potential
problem goals interact negatively. However, since significant percentage
non-interacting goals, strategy reduces size library comparison one
multi-goal problems successfully solved stored. storage
strategy represents tradeoff since effort must expended merging retrieved
cases full solution (See Section 3.3).
3.2.2 Indexing Basis Replay Failure

Multi-goal cases stored library censor retrieval corresponding
single-goal subproblems. library organization differs earlier work stores
cases common fashion single level, first indexing case goals,
success conditions relevant goals (Veloso, 1994; Munoz-Avilla &
Weberskirch, 1996). contrast, dersnlp+ebl indexes cases discrimination
net similar one depicted Figure 14. figure shows one fragment case
library includes cases solve single input goal. Individual planning
episodes achieve goal represented one level lower net. labeled
180

fiStoring Indexing Plan Derivations

G0
input goals:

initial conditions:

(AT-OB OB1 ld )
(AT-PL PL1 lp)

(AT-OB OB1 l1)

G1

G2

derivation 1
r1

failure reasons:

(AT-PL PL1 lq)

derivation 2
r2

G3

G4

derivation 3

derivation 4

Figure 14: Library fragment indexing stored cases solve single input goal, (at-ob
ob1 ld ).
relevant initial state conditions, otherwise known footprinted initial state (Veloso,
1994). Together, goal initial state conditions make static success conditions
cases first retrieved. one cases retrieved replay replay
fails, derivation corresponding extra interacting goals added library
indexed directly failing case. future retrievals case, failure
conditions checked see whether extra goals responsible failure present
conditions. so, retrieval process returns repairing case
achieves con icting goals. case failure reason thus used direct retrieval away
case repeat known failure, towards case avoids it.
One might question hierarchical organization instances failures due
interacting goals alone. store cases single level first indexing
case goals, conditions relevant goals? answer
lies need censor cases failure conditions satisfied. type error
found retrieving multiple cases. example, consider new problem
contains three goals, g1 , g2 g3 . Suppose goal g2 negatively interacts
g1 g3 . case retrieved library achieves g1 g2 ,
one goal, g3 , left open. However, case retrieved solves g3 alone,
fail presence g2 . type retrieval error handled prioritizing
cases. repairing case stored subclass case failed. Failing cases
annotated failure reason directs retriever case avoids
failure.
Prioritizing cases basis negatively interacting goals alone sucient
capture retrieval failures may encountered. cases retrieved
basis partial match relevant initial state conditions, retrieval errors may
occur unmatched conditions (Veloso, 1994). example, failure might
occur logistics transportation example extra package plane's
route, similar failure occur package moved plane's route. strategy
adopted deal types failure information annotate case
181

fiIhrig & Kambhampati

OB1

OB3



l1
ld

B B
B

B

B
lp

OB2

B

l2

Figure 15: logistics transportation example illustrating multi-case retrieval.
failure reason (whether extra goal unmatched initial state condition)
use failure reasons prioritize cases. EBL techniques employed
construction failure explanations may used types failures.
dersnlp+ebl's method storing multi-goal cases goals negatively interacting limits size case library. aspects dersnlp+ebl's storage strategy
serve lower library size. planner always uses current library solving new
problems. New derivations stored applicable case,
retrieved cases fail. strategy avoids storage duplicate cases, may
entirely effective since soundness failure explanations guaranteed. failure
explanations sound, pointers repairing cases may eventually lead duplicate
case, causing library continue grow indefinitely. However, easily checked
putting depth limit number repairing cases discrimination net. Also,
failures due interacting goals result unchecked growth library
since number interacting goals limited maximum problem size.
3.2.3 Detailed Example Case Retrieval

example case retrieval illustrated Figure 15. figure contains three subplans
corresponding two separate cases stored library. Case achieves goal
single package, ob1, located destination ld . Case B achieves goal
ob1 ob2 located ld.
Assume new problem attempted replay contains three
goals, (at-ob ob1 ld), (at-ob ob2 ld ), (at-ob ob3 ld ). second goal negatively
interacts goals. retriever first attempt find case
solves first goal alone. Case solves goal. However, case annotated
failure reason satisfied new problem situation, therefore censored
favor repairing case, Case B . retriever returns Case B ,
one open goal covered, is, (at-ob ob3 ld ). seek case solves
goal alone, find Case A. However, A's failure reason satisfied
new problem state rejected favor second copy B (which
call Case B 0), solves problem transporting ob3 ob2.
two instances Case B retrieved solve three goal problem,
Case B Case B 0 . Together cover new problem goals. dersnlp+ebl replays
182

fiStoring Indexing Plan Derivations

Figure 16: New linking opportunities indicated increase number siblings
step addition decision.
copies B sequence obtain solution full problem, thereby merging
respective subplans. Notice, however, union plans contain redundant
steps. example, plans plane location l1 . Section 3.3 describes
dersnlp+ebl deals positive goal interactions.

3.3 Multi-case Merging

say two plans mergeable respect problem, hI 0; G0 ; Ai, exists
solution problem contains combined constraints.

Definition 2 (Mergeability) plan P1 achieving goal g1 mergeable plan P2
goal g2 respect problem, hI 0 ; G0 ; Ai , plan P 0 correct
hI 0; G0 ; Ai hhP 0ii hhP1ii\hhP2 ii. (Thus syntactically, P 0 contains constraints
P1 P2 ).

Multi-case replay accomplishes plan merging, may result lower quality plans
care taken avoid redundant step additions (Ihrig & Kambhampati, 1996; MunozAvilla & Weberskirch, 1997). occur goals covered separate cases positively
interact may solved common steps. Replaying case sequence
results unneeded steps plan5 .
multi-case replay, open condition justification adding new step,
steps may added already exist plan due earlier replay another
case. first retrieved derivation replayed, none replayed step additions
result redundancy. However, subsequent goals solved replay
additional cases, step additions may unnecessary opportunities
linking open conditions achieve earlier established steps. planner
way determining priori steps may represented single step
plan6 .
dersnlp+ebl's replay framework handles redundant step additions skipping
step addition establishments whenever open condition may achieved new link.
thus strengthens increases justification replaying step addition decisions
open condition longer basis validating decision. justification replay strengthened add condition new linking opportunities
5. analogous decrease plan quality occurs state-space plan reuse, sequencing macro-operators
results state loops (Minton, 1990a).
6. Consider, example, domain plane may transport two packages one trip, not,
depending capacity.

183

fiIhrig & Kambhampati

dersnlp+ebl

dersnlp+ebl-ij

87%(6) 67%(5)
2465
5796

87%(7) 67% (5)
2198
5810

replay
%Solved
time(sec)

scratch

replay

scratch

Table 3: Percentage problems solved, total CPU time seconds 30 problems
problems Logistics Transportation Domain. Average solution length
shown parentheses next %Solved.
present. may detected increase number siblings prescribed
step addition choice (See Figure 16). siblings stored step addition decision
recorded annotations derivation trace. new links available
contained within siblings, step addition decision skipped. replay,
alternative new links explored normal course plan refinement.
means step may eventually added new links fail.
Increasing justification step addition decisions improves quality plans
terms number steps contain. example, Case B B 0 would normally
produce subplans shown Figure 15. cases replayed sequence
solving single problem, plans merged plane moves city
once. Plan merging increasing justification replay accomplishes
retracting redundant action sequences, may cause planning failure. thus
deals action-merging interactions defined (Yang, Nau, & Hendler, 1992).
next section describe empirical study testing effectiveness merging
strategy.
3.3.1 Empirical Test dersnlp+ebl's Plan Merging Strategy

preliminary study conducted test effectiveness dersnlp+ebl's method
plan merging replay. experiment compared dersnlp+ebl
without increasing justification replay. experimental setup consisted training
dersnlp+ebl set 20 randomly generated 4-goal training problems, testing
different set 30 4-goal test problems. initial state problem contained 12
locations (6 post oces 6 airports) 12 transport devices (6 planes 6 trucks).
training phase, planner solved problems stored successful plan derivations
case library. testing phase, planner retrieved multiple stored plan
derivations used guidance solving test problems. dersnlp+ebl
tested 30 problems replay from-scratch modes. Replay either
(dersnlp+ebl) without (dersnlp+ebl-ij) increased justification. results
shown Table 3.
Although overall performance poorer, quality plans terms number
steps improved dersnlp+ebl's strategy increasing justification step addition. result suggests dersnlp+ebl's method plan merging serves reduce
184

fiStoring Indexing Plan Derivations

redundancy plans produced multi-case replay. Recently, Munoz-Avilla
Weberskirch (1997) tested non-redundant merging strategy process planning domain found similar improvements plan size. next section describes
evaluation full dersnlp+ebl system.

4. Experimental Evaluation Complete System

experiments reported section tested full dersnlp+ebl system using
dynamic multi-case storage retrieval strategy described Section 3. aim
evaluate replay system complex domain. hypothesis performance
would improve problem solving experience negative interactions discovered
stored. addition, predicted dersnlp+ebl's method storage would result
low library size low retrieval costs.
Logistics Transportation domain (Veloso, 1994) become somewhat benchmark CBP literature. scaled version therefore chosen purpose.
tested large multi-goal problems drawn domain shown Figure 4 scaled
first 6 15 cities. size domain unusual current literature.

4.1 Experimental Setup

experiment run phases, phase corresponding increase problem
size. Thirty test problems size randomly generated. Since possible
obtain truly random distribution within nonartificial domain, following strategy
adopted problem generation. First, initial state constructed fixing
number objects type contained domain description. example, first
experiment, six cities (12 locations within cities), six planes, six trucks.
initial state problem constructed first including filter conditions (nonachievable conditions). defined layout cities. example, condition (is-a
airport ap1) identified ap1 airport. condition (same-city ap1 po1) indicated
ap1 po1 located city. Second, achievable (non-filter) conditions present add clauses domain operators varied
problem choosing object constants randomly available restriction
two initial state conditions inconsistent. example, plane package
assigned single randomly-chosen location. Goals chosen among
achievable conditions manner. Although attempt made create interacting goals, goal interaction common multi-goal problems.
limit imposed number steps plan. meant multi-goal problems often could solved concatenating subplans individual subgoals.
instances, planner could take advantage linking opportunities achieve multiple
goals common steps. meant often planner backtrack
derivation one goal order solve additional goal.
first experiment used 6-city domain run 6 phases. size
test problems (which ranged 1 6 goals) increased phase. Prior
phase n experiment, case library emptied planner retrained
randomly generated problems size n. Training problems solved attempting
single-goal subproblems scratch, storing trace derivation solution
185

fiIhrig & Kambhampati

Phase

0

20

%Solved
time(sec)

100%(3)
15

% Solved
time(sec)

Logistics (Best-first

CPU limit: 500sec)
80
100

40

60

100%(3)
14(.1)

100%(3)
13(.1)

100%(3)
4(.0)

100%(3)
5(.10)

100%(3)
3(.13)

100%(3)
3(.13)

90%(4)
1548

93%(4)
1069(.2)

100%(5)
22(1.0)

100%(5)
23(.2)

100%(5)
25(.28)

100%(5)
15(.28)

100%(5)
11(.26)

% Solved
time(sec)

53%(5)
7038

87%(7)
93%(7)
93%(7)
93%(7) 100%(8)
2214(.55) 1209(.49) 1203(.54) 1222(.52) 250(.54)

100%(8)
134(.58)

% Solved
time(sec)

43%(5)
8525

100%(8)
563(.99)

100%(8)
395(.79)

100%(8)
452(.91)

100%(9)
24(.97)

100%(9)
22(.89)

100%(9)
22(.88)

% Solved
time(sec)

0%
15000

70%(11)
5269(2)

90%(11)
2450(1)

93%(11)
1425(2)

93%(11)
1479(1)

93%(11) 100%(12)
1501(1) 375(1)

% Solved
time(sec)

0%
15000

50%(12)
7748(3)

70%(13)
4578(5)

87%(14)
2191(5)

93%(14)
1299(3)

93%(14)
1319(3)

One Goal

Two Goal

Three Goal

Four Goal

Five Goal

Six Goal

120

93%(14)
1244(3)

Table 4: Performance statistics Logistics Transportation Domain. Average solution
length shown parentheses next %Solved. Case retrieval time shown
parentheses next CPU time.
problem one already present library, successively adding extra
goal. Multi-goal problems stored retrieved cases used solving problem
failed. Whenever problem could solved sequenced replay previous cases,
negatively interacting goals contained failure reason identified new
case achieving goals alone stored library. phase experiment,
planner tested 30 randomly generated test problems varying
amounts training. problems solved from-scratch mode replay
multiple cases retrieved library constructed training.
second experiment tested planner complex 15 city domain employed stable case library formed dersnlp+ebl trained 120 (6 city, 6 goal)
logistics transportation problems. library smaller problems used
planner tested larger (15 city) problems ranging 6 10 goals.

4.2 Experimental Results

first experiment 6 city domain dersnlp+ebl showed substantial improvements multi-case replay evident results Table 4. Moreover, replay
performance improved problem-solving experience. plans produced
showed slight increase number steps solutions obtained
from-scratch mode. results plotted Figure 17 graphs cumulative
CPU time test problems six experiments. figure illustrates CPU
time decreased number training problems solved. insert shows total CPU
186

fiStoring Indexing Plan Derivations

Figure 17: Replay performance Logistics Transportation Domain increasing
amounts training. Thirty problems tested problem size (1
6 goals). amount time needed solve test problems size
(including case retrieval time) shown problems solved scratch
(level 0) replay increasing levels training (after solving 20 ...
120 randomly generated problems). insert shows amount time taken
solve test problems increasing amounts training. time limit
500 seconds placed problem solving.

187

fiIhrig & Kambhampati

Figure 18: Replay performance Logistics Transportation Domain scaled 15
cities. case library formed 120 training problems (6 cities, 6 goals)
solved. library used solving test sets containing larger
problems (15 cities, 6 10 goals). None problems solved within
time limit (500 sec) from-scratch mode. replay mode, average solution
length shown parentheses next problem size.

Figure 19: Replay performance logistics transportation. percentage test problems solved within time limit (500 sec) plotted number training
problems solved. Percentage solved shown problems increasing size (1,
3, 5 goals).

188

fiStoring Indexing Plan Derivations

Figure 20: Figure shows size case library increased number training
problems solved. Library size increases training problem size (1, 3, 5
goals). 5' shows number single-goal subproblems contained 5-goal
training problems.
time (including case retrieval time) test problems six experiments.
evident insert, planning performance improves increased experience random
problems. However, relatively little experience (20 problems solved) enough show
significant performance improvements.
Replay raised problem-solving horizon, illustrated Figure 19. effective
larger problem size, from-scratch planning tends exceed time limit imposed
problem-solving. Figure 20 shows increase size library increasing
amounts training. figure indicates library size determined
amount interaction domain, opposed number training problems solved.
rate case library grows tapers higher planner trained
larger problems7 .
second experiment, library formed course training 6-goal problems
used solve larger problems (6 10 goals) complex domain (15 cities) (See
Figure 18). None larger problems solved from-scratch mode within time
limit 500 sec 8 . planner continued maximum time problems, indicated
figure linear increase CPU time. performance substantially better
replay, however. Since library size relatively small, improvements planning
performance offset cost retrieving adapting previous cases. finding
suggests replay strategy employed experiments represents effective
method improving planning performance complex domains.
7. opportunity interaction larger problems. example, 6-goal problem could
contain 6 goals mutually interact, whereas 5-goal problem maximum 5 interacting goals.
8. dersnlp+ebl from-scratch mode used best-first strategy. replay, best-first strategy biased
subtree replayed path explored first, siblings path.

189

fiIhrig & Kambhampati

action (Put-On ?X ?Y ?Z)
precond (On ?X ?Z)
(Clear ?X)
(Clear ?Y)
add
(On ?X ?Y)
(Clear ?Z)
delete (On ?X ?Z)
(Clear ?Y)

action (New-Tower ?X ?Z)
precond (On ?X ?Z)
(Clear ?X)
add
delete

(On ?X Table)
(Clear ?Z)
(on ?X ?Z)

Figure 21: specification Blocks World Domain adapted experiments.

4.3 Empirical Comparison dersnlp+ebl Rule-Based EBL
Case-based planning explanation-based learning offer two differing approaches improving performance planner. Prior research (Kambhampati, 1992) analyzed
tradeoffs. hybrid learning approach dersnlp+ebl designed alleviate
drawbacks associated pure case-based planning, rule-based EBL. Prior
work, EBL used construct generalized search control rules may
applied new problem-solving situation. rules matched choice
point search process (DeJong & Mooney, 1986; Minton, 1990b; Mostow & Bhatnagar,
1987; Kambhampati et al., 1996b). approach known exhibit utility problem since
rule base grows rapidly increasing problem-solving experience even small
number rules may result high total match cost (Minton, 1990b; Tambe, Newell, &
Rosenbloom, 1990; Kambhampati, 1992; Francis & Ram, 1995). contrast, empirical
results discussed (see Table 4) indicate dersnlp+ebl low case retrieval
match cost.
demonstrate dersnlp+ebl reduces match cost, conducted empirical study
compared performance ucpop+ebl, rule-based search control learning
framework (Kambhampati et al., 1996b). framework constructs reasons planning
failures manner similar dersnlp+ebl. However, approach similar
Minton (1990b) employs explanations construction search control
rules matched node search tree. planners tested
set problems ranging 2 6 goals randomly generated blocks
domain shown Figure 21. Testing performed set thirty problems
increasing amounts training.
illustrated Figure 22, dersnlp+ebl improved performance 10 training problems solved. ucpop+ebl failed improve significantly. reason evident
ucpop+ebl's match time (ucpop-match) graphed Figure 22. ucpop+ebl,
time spent matching rules increases training, wiping improvements
may gained use rules. rules matched
choice point search tree, small number rules sucient substantially increase
total match cost.
190

fiStoring Indexing Plan Derivations

Figure 22: Total CPU Time 30 blocks world problems increased amounts training.
possible improve performance rule-based EBL reducing number
rules use utility monitoring strategies (Gratch & DeJong, 1992),
using sophisticated match algorithm (Doorenbos, 1995). example, Doorenbos
(1995) employs improved rule matcher based Rete algorithm. dersnlp+ebl,
hand, aims alleviating utility problem reducing number times rules
matched. Similar rule-based EBL, learning component employed generate
rules. However, rules generated govern retrieval cases stored
library. compiled indexing structure. dersnlp+ebl exhibits low
match cost applying retrieval rules one point search process. Specifically,
retrieves cases start problem-solving. case represents sequence
choices (a derivation path) thus providing global control opposed local. results
shown Table 4 indicate cost retrieving cases significantly lower comparison
time spent problem-solving.

5. Related Work Discussion
dersnlp+ebl's storage strategy relies capability case-based planner replay
multiple cases, covering small subset goals, add step orderings interleave respective plans. strategy differs earlier approaches priar
(Kambhampati & Hendler, 1992), prodigy/analogy (Veloso, 1994), paris (Bergmann
& Wilke, 1995), caplan (Munoz-Avilla & Weberskirch, 1996), division
goal subsets based structure final plan alone, sequence
events making problem-solving episode. Retrieval failures treated opportunity planner stores new repairing case. aspect similar
Hammond's chef (Hammond, 1990) learns improve retrieval strategy
based failures. Despite surface similarity, important differences

191

fiIhrig & Kambhampati

Transformational
PRIAR
SPA
MPA
Plan-Space

State-Space
DERSNLP

PRODIGY/ANALOGY
PARIS

Derivational

Figure 23: different approaches case-based planning case adaptation accomplished underlying generative planner.
approach. dersnlp+ebl learns case extension failures, whereas chef concentrates
learning execution failures. Specifically, chef assumes incomplete domain model,
consisting stored cases, domain-specific modification theory patches. Given
new problem, chef retrieves previous case, modifies retrieved plan using domain
specific modification rules generate candidate solution current problem.
correctness solution tested respect external causal simulator
domain. solution found incorrect, explanation incorrectness (supplied
simulator) used modify case-library censor retrieval case
similar situations future. effect improves correctness chef's domain
theory. contrast, dersnlp+ebl assumes complete knowledge domain, form
domain operators. access sound complete plan synthesis strategy.
aim case-based reasoning dersnlp+ebl improve performance
base-level planner. end, dersnlp+ebl analyzes case extension failures predict
case cannot extended solve new problem.
Fox Leake (1995) taken approach similar chef, use introspective reasoning explain failures find repairing cases. Similar chef, introspective
reasoning used revise indexing case library (Fox & Leake, 1995; Ram & Cox,
1994). approaches employ domain-specific techniques improve storage retrieval case library (Munoz-Avilla & Weberskirch, 1996; Smyth & Keane, 1995).
dersnlp+ebl differs automatically generates new indices well defined
domain-independent methodology (Kambhampati et al., 1996b) incorporated
underlying planning strategy.
Since EBL employed explaining case failure well success, dersnlp+ebl complements extends earlier approaches case retrieval (Barletta & Mark, 1988; Kambhampati & Hendler, 1992; Hendler, Stoffel, & Mulvehill, 1996; Veloso, 1994; Bergmann
& Wilke, 1995; Munoz-Avilla & Weberskirch, 1996; Ram & Francis, 1996). Although
192

fiStoring Indexing Plan Derivations

exhibits low retrieval match cost, CBP system, eciency may degrade larger domain size. dersnlp+ebl's approach compatible others aimed
improving match cost (Doorenbos, 1995; Ram & Francis, 1996; Hendler et al., 1996).
example, mpa (Ram & Francis, 1996) built around retrieval engine performs
asynchronous memory retrieval. caper (Hendler et al., 1996) uses structure matching
algorithm parallelizes process plan's success conditions represented
retrieval probe matched large knowledge base world facts. process
expands binary predicates match success conditions larger structure containing implicitly specified relations knowledge base. structure acts filter,
eliminating matches fail line probe.
dersnlp+ebl similar case-based systems employ complete correct
domain-independent planner generate cases stored (Hanks & Weld, 1995; Kambhampati & Hendler, 1992; Koehler, 1994; Veloso, 1994; Ram & Francis, 1996). surveying
literature, possible distinguish approaches two orthogonal scales
shown Figure 23. horizontal direction, CBP frameworks ranked
underlying planning strategy falls continuum whose end extremes represent
state-space vs plan-space dichotomy. Towards state-space end spectrum
prodigy/analogy, employs means-ends analysis (MEA) planner, nolimit,
extend previous case. nolimit classed state-space planner since applies
actions plan based current world state thereby advances world state.
priar framework (Kambhampati & Hendler, 1992; Kambhampati, 1994) based
within nonlin (Tate, 1977). nonlin creates plans hierarchical task reduction.
partial-order (plan-space) planner constructs plans protecting underlying causal structure. dersnlp+ebl, extends case normal course
plan refinement defined underlying plan-space strategy. However, dersnlp+ebl
implemented within partial-order, causal-link planner, snlp (McAllester & Rosenblitt,
1991; Barrett & Weld, 1994). aspect similar spa system developed
Hanks Weld (1995).
different CBP systems may distinguished according case adaptation strategy. roughly categorized either transformational derivational
(Carbonell, 1983; Veloso & Carbonell, 1993b), according whether transform previous plan replay previous plan derivation. transformational strategies priar
spa, final plan product planning episode stored case
library. case retrieved plan fitted adapt new problem-solving situation retracting irrelevant redundant subparts. Early CBP systems (Carbonell,
1983; Hammond, 1990) employ transformational techniques adapt previous solution. Causal-link planners snlp ready-made plan reuse since causal
structure employed plan adaptation part plan itself. priar spa
use plan's causal structure fitting plan new problem context,
extending fitted plan solve new problem. priar differs spa
employs extension-first strategy. skeletal plan first refined addition
plan constraints undertaking retraction constraints. spa,
hand, alternates retraction plan constraints addition
new constraints. mpa (Ram & Francis, 1996) extends spa's transformational strategy
accomplish multi-case retrieval adaptation.
193

fiIhrig & Kambhampati

mentioned earlier, derivational analogy case-based planning technique
introduced Carbonell (Veloso & Carbonell, 1993b). model developed
Veloso prodigy/analogy (Veloso, 1994), employed case fitting strategy called
derivational replay. Case fitting based replay similar fitting plan reuse,
based plan's underlying causal structure. justification planning
decision stored derivation trace ects causal dependencies plan
steps. justified choices replayed solving new problem. Replay thus serves
purpose retraction plan reuse. Replay may advantage multi-case
reuse since allows planner readily merge small subplans solve large problems.
dersnlp contrasted prodigy/analogy employs case fitting
methodology called eager derivation replay (Ihrig & Kambhampati, 1994a, 1996).
replay strategy, applicable cases replayed sequence returning fromscratch planning. Eager replay simplifies replay process avoiding decision
alternate replay multiple cases. effectiveness approach dependent
underlying plan-space planning strategy (Ihrig & Kambhampati, 1994a). dersnlp's
eager case adaptation strategy allows case failure defined terms failure
single node search tree. particular, case failure defined failure
skeletal plan, contains constraints adopted advice
previous cases. Eager case adaptation means explanations case failure may
constructed use EBL techniques developed explain
analytical failures occurring planner's search space.

6. Summary Conclusion
paper described design implementation case-based planner,
dersnlp+ebl. dersnlp+ebl framework represents integration eager case adaptation failure-based EBL. EBL techniques employed building case library
basis experienced retrieval failures. approach improves earlier treatments case retrieval (Barletta & Mark, 1988; Kambhampati & Hendler, 1992; Ihrig &
Kambhampati, 1994a; Veloso & Carbonell, 1993a). partial-order case-based planner,
dersnlp ability solve large problems retrieving multiple instances smaller
subproblems merging cases sequenced replay (Ihrig & Kambhampati,
1994a). dersnlp+ebl framework extends approach use new EBL
techniques employed construction case library. techniques
used explain plan merging failure identify set negatively interacting goals.
library augmented new repairing case covering interacting goals.
dersnlp+ebl's method storing multi-goal cases goals negatively interacting results small library size low retrieval costs. However, multi-case adaptation
involves tradeoff since effort expended merging multiple instances stored cases.
dersnlp+ebl accomplishes merging increasing justification replay step addition decisions. strategy avoids addition redundant steps goals positively
interact. dersnlp+ebl therefore aimed domains Logistics Transportation
domain significant amount positive interaction. aimed domains
negative interaction. course futile spend effort explaining case
failure none encountered.
194

fiStoring Indexing Plan Derivations

Section 4 describes evaluation overall eciency storage retrieval
strategy solving large problems complex domain. dersnlp+ebl shows improvement planning performance offsets added cost entailed
retrieving failure conditions. amount improvement provided replay shown
experiments seen lower bound since random problem distribution
may mean less problem similarity found real world problems.
conclusion, paper described novel approach integrating explanationbased learning techniques case-based planning. approach aimed issues
associated pure case-based planning, rule-based EBL. particular,
addresses mis-retrieval problem CBP, well utility problem. results
demonstrate eager case adaptation combined dersnlp+ebl's dynamic case
retrieval effective method improving planning performance.

Acknowledgements
authors wish thank Amol D. Mali, Eric Lambrecht, Eric Parker, anonymous
reviewers helpful comments earlier versions paper. Thanks due
Terry Zimmerman providing insight ucpop+ebl. research supported
part NSF Research Initiation Award IRI-9210997, NSF Young Investigator award
IRI-9457634, ARPA Planning Initiative grants F30602093-C-0039 (phase II)
F30602-95-C-0247 (phase III).

References

Barletta, R., & Mark, W. (1988). Explanation-based indexing cases. Proceedings
AAAI-88.
Barrett, A., & Weld, D. (1994). Partial order planning: evaluating possible eciency gains.
Artificial Intelligence, 67, 71{112.
Bergmann, R., & Wilke, W. (1995). Building refining abstract planning cases change
representation language.. Journal Artificial Intelligence Research, 3, 53{118.
Carbonell, J. (1983). Learning analogy: Formulating generalizing plans past
experience. Michalski, R., Carbonell, J., & Mitchell, T. (Eds.), Machine Learning:
Artificial Intelligence approach, Vol. 1. Palo Alto, CA: Tioga Press.
DeJong, G., & Mooney, R. (1986). Explanation-based learning: alternative view. Machine Learning, 1 (2), 145{176.
Doorenbos, R. (1995). Production Matching Large Learning Systems. Ph.D. thesis,
Computer Science Department, Carnegie Mellon University.
Fikes, R., & Nilsson, N. (1971). new approach application theorem proving
problem solving. Artificial Intelligence, 2, 189{208.
Fox, S., & Leake, D. (1995). Using introspective reasoning refine indexing. Proceedings
IJCAI-95.
195

fiIhrig & Kambhampati

Francis, A., & Ram, S. (1995). comparative utility analysis case-based reasoning
control-rule learning systems. Proceedings 8th European Conference
Machine Learning, ECML-95.
Friedland, P., & Iwasaki, Y. (1985). concept implementation skeletal plans.
Journal Automated Reasoning, 1 (2), 161{208.
Gratch, J., & DeJong, G. (1992). Composer: probabilistic solution utility problem
speed-up learning. Proceedings AAAI-92.
Hammond, K. (1990). Explaining repairing plans fail. Artificial Intelligence, 45,
173{228.
Hanks, S., & Weld, D. (1995). domain-independent algorithm plan adaptation. Journal
Artificial Intelligence Research, 2, 319{360.
Hendler, J., Stoffel, K., & Mulvehill, A. (1996). High performance support case-based
planning applications. Technological Achievements Arpa/Rome Laboratory
Planning Initiative: Advanced Planning Technology. AAAI Press.
Ihrig, L., & Kambhampati, S. (1994a). Derivation replay partial-order planning.
Proceedings AAAI-94.
Ihrig, L., & Kambhampati, S. (1994b). Plan-space vs state-space planning reuse
replay. Tech. rep. 94-006, Department Computer Science Engineering. Arizona
State University. available http://rakaposhi.eas.asu.edu/yochan.html.
Ihrig, L., & Kambhampati, S. (1996). Design implementation replay framework
based partial order planner. Proceedings AAAI-96.
Joslin, D., & Roach, J. (1990). theoretical analysis conjunctive goal problems. Artificial
Intelligence, 41, 97{106.
Kambhampati, S. (1992). Utility tradeoffs incremental modification reuse plans.
Proceedings AAAI Spring Symposium Computational Considerations Supporting
Incremental Modification Reuse.
Kambhampati, S. (1994). Exploiting causal structure control retrieval refitting
plan reuse. Computational Intelligence, 10.
Kambhampati, S., & Chen, J. (1993). Relative utility ebg based plan reuse partial ordering vs total ordering planning. Proceedings AAAI-93, pp. 514{519. Washington,
D.C.
Kambhampati, S., & Hendler, J. A. (1992). validation structure based theory plan
modification reuse. Artificial Intelligence, 55, 193{258.
Kambhampati, S., Ihrig, L., & Srivastava, B. (1996a). candidate set based analysis
subgoal interactions conjunctive goal planning. Proceedings 3rd Intl. Conf.
AI Planning Systems.
196

fiStoring Indexing Plan Derivations

Kambhampati, S., Katukam, S., & Qu, Y. (1996b). Failure driven dynamic search control
partial order planners: explanation-based approach. Artificial Intelligence, 88,
253{315.
Kambhampati, S., Knoblock, C., & Yang, Q. (1995). Planning refinement search:
unified framework evaluating design tradeoffs partial-order planning. Artificial
Intelligence, 76, 167{238.
Koehler, J. (1994). Avoiding pitfalls case-based planning. Proceedings 2nd Intl.
Conf. AI Planning Systems.
Korf, R. (1987). Planning search: qualitative approach. Artificial Intelligence, 33,
65{68.
McAllester, D., & Rosenblitt, D. (1991). Systematic nonlinear planning. Proceedings
AAAI-91.
Minton, S. (1990a). Issues design operator composition systems. Proceedings
International conference Machine Learning.
Minton, S. (1990b). Quantitative results concerning utility explanation-based learning. Artificial Intelligence, 42, 363{392.
Mostow, J., & Bhatnagar, N. (1987). Failsafe: oor planner uses ebg learn
failures. Proceedings IJCAI-87.
Munoz-Avilla, H., & Weberskirch, F. (1996). Planning manufacturing workpieces
storing, indexing replaying planning decisions. Proceedings 3rd Intl.
Conf. AI Planning Systems. AAAI-Press.
Munoz-Avilla, H., & Weberskirch, F. (1997). case study mergeability cases
partial-order planner. Proceedings 4th European Conf. Planning.
Ram, A., & Cox, M. (1994). Introspecive reasoning using meta-explanations multistrategy learning. Michalski, R., & Tecuci, G. (Eds.), Machine Learning: multistrategy
approach Vol. IV. Morgan Kaufmann.
Ram, S., & Francis, A. (1996). Multi-plan retrieval adaptation experience-based
agent. Leake, D. B. (Ed.), Case-Based Reasoning: experiences, lessons, future
directions. AAAI Press/The MIT Press.
Redmond, M. (1990). Distributed cases case-based reasoning:facilitating use multiple
cases. Proceedings AAAI-90.
Smyth, B., & Keane, M. (1995). Remembering forget: competence-preserving deletion
policy cbr. Proceedings IJCAI-95.
Tambe, N., Newell, A., & Rosenbloom, P. (1990). problem expensive chunks
solution restricting expressiveness. Machine Learning, 5, 299{349.
Tate, A. (1977). Generating project networks. Proceedings IJCAI-77.
197

fiIhrig & Kambhampati

Veloso, M. (1994). Planning learning analogical reasoning. Springer Verlag. Number
886 Lecture Notes Artificial Intelligence.
Veloso, M., & Blythe, J. (1994). Linkability: Examining causal link commitments partialorder planning. Proceedings 2nd Intl. Conf. AI Planning Systems.
Veloso, M., & Carbonell, J. (1993a). Derivational analogy prodigy: Automating case
acquisition, storage utilization. Machine Learning, 10, 249{278.
Veloso, M., & Carbonell, J. (1993b). Toward scaling machine learning: case study
derivational analogy prodigy. Minton, S. (Ed.), Machine Learning methods
planning. Morgan Kaufmann.
Yang, Q., Nau, D., & Hendler, J. (1992). Merging separately generated plans restricted
interactions. Computational Intelligence, 8 (2), 648{676.

198


