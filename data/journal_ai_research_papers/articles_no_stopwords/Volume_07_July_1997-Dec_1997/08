journal artificial intelligence

submitted published

dynamic non bayesian decision making
dov monderer
moshe tennenholtz

dov ie technion ac il
moshet ie technion ac il

industrial engineering management
technion israel institute technology
haifa israel

abstract

model non bayesian agent faces repeated game incomplete information nature appropriate tool modeling general agent environment
interactions model environment state controlled nature may change arbitrarily feedback reward function initially unknown agent bayesian
form prior probability neither state selection strategy nature
reward function policy agent function assigns action
every history observations actions two basic feedback structures considered
one perfect monitoring case agent able observe previous
environment state part feedback imperfect monitoring
case available agent reward obtained settings
refer partially observable processes current environment state unknown
main refers competitive ratio criterion perfect monitoring case
prove existence ecient stochastic policy ensures competitive
ratio obtained almost stages arbitrarily high probability eciency
measured terms rate convergence shown optimal
policy exist imperfect monitoring case moreover proved
perfect monitoring case exist deterministic policy satisfies long
run optimality criterion addition discuss maxmin criterion prove
deterministic ecient optimal strategy exist imperfect monitoring case
criterion finally long run optimality viewed
qualitative distinguishes previous work area

introduction
decision making central task artificial agents russell norvig wellman
wellman doyle point time agent needs select among
several actions may simple decision takes place
complicated decision series simple decisions made question
right actions basic issue discussed settings
fundamental importance design artificial agents
static decision making context artificial agent consists set actions agent may perform set possible environment states utility reward
function determines feedback agent performs particular action
particular state best represented matrix columns indexed
states rows indexed actions rewards entries reward
function known agent say agent payoff uncertainty
c ai access foundation morgan kaufmann publishers rights reserved

fimonderer tennenholtz

refer incomplete information fudenberg tirole
modeling incomplete information one must describe underlying assumptions knowledge agent reward function example
agent may know bounds rewards may know partially know underlying
probabilistic structure dynamic multistage decision making setup agent faces
static decision stages stage agent selects action performed environment selects state history actions states determines
immediate reward well next one shot decision history actions
states determines next selected state work reinforcement learning artificial intelligence kaelbling littman moore adopted view agent
operating probabilistic bayesian setting agent last action last state
determine next environment state given probability distribution naturally
learner may priori familiar probability distribution existence
underlying probabilistic model key issue system modeling however
assumption ultimate one particular much work areas ai
economics dealt non probabilistic settings environment changes
unpredictable manner agent know uence choices
selection next state e certain environment strategy
say agent strategic uncertainty
use general model representation agent environment interactions agent payoff strategic uncertainty deal
non bayesian agent faces repeated game incomplete information nature
repeated game nature agent faces static decision
stage environment state taken action chosen opponents
decision called game stress fact agent action
state independently chosen fact game repeated refers fact
set actions set possible states one shot utility function
vary time said consider agent payoff uncertainty
strategic uncertainty priori ignorant utility function e
game incomplete information well state selection strategy nature
agent non bayesian sense assume probabilistic model
concerning nature strategy sense assume probabilistic
model concerning reward function though may assume lower upper bounds
consider two examples illustrate mentioned notions model consider
example agent may know probability distribution set reward functions may assume
probability exists without assumption structure may partial information
distribution ignorant parameters e g may believe reward
function drawn according normal distribution unknown covariance matrix
many intermediate cases assumed changes probabilistic nonmarkovian structure
general setup sets may vary time useful analysis done model
changes completely arbitrary
repeated games complete information generally multistage games stochastic games
extensively studied game theory economics partial list includes shapley
blackwell luce raiffa recently fudenberg tirole mertens
sorin zamir evolving literature learning e g fudenberg levine
incomplete information setup player ignorant game played inspired



fidynamic non bayesian decision making

investor investing daily certain index stock market daily profits
depends action selling buying certain amount environment state
percentage change price index investor complete information
reward function knows reward realized particular
investment particular change strategic uncertainty changes
index price playing repeated game complete information
nature strategic uncertainty
consider another investor invests particular mutual fund fund invests
stock market strategy known investor assume
state represents vector percentage changes stocks investor
know reward function example cannot say advance would profit
would buy one unit fund stock prices increase percent thus
plays repeated game incomplete information addition attempt
construct probabilistic model concerning reward function market behavior
non bayesian analysis may apply another example assume bob
decide evening whether prepare tea coffee wife gets
home wife wishes drink tea coffee wishes ready
reaction bob wife tea coffee may depend state day
predicted history actions states previous days bob
got married cannot tell reward get wife happy makes
cup tea course may eventually know decisions learning
period precisely subject
example generality mentioned setting consider model
markov decision processes complete incomplete information markov decision
process agent action given state determines probabilistic fashion next
state obtained agent structural assumption state selection
strategy repeated game nature without added assumptions captures fact
transition state state may depend history arbitrary way
agent performs action state st part feedback would u st
u reward function distinguish two basic feedback structures
one perfect monitoring case agent able observe previous
environment state part feedback imperfect monitoring
case available agent reward obtained notice
feedback structures current state observed agent called
select action investors face repeated game perfect monitoring
percentage changes become public knowledge iteration
example bob make decision situation imperfect
monitoring bob would able observe reward behavior e g whether
harsanyi see aumann maschler comprehensive survey
literature deals partially bayesian agents rare exceptions cited section
notice former assumption popular related game theory literature aumann
maschler many intermediate monitoring structures may interesting well
case evolving literature controlling partially observable markov
decision processes lovejoy cassandra kaelbling littman monahan contrast
q learning theory watkins watkins dayan sutton assume knowledge
current state



fimonderer tennenholtz

says thanks terrible exactly wanted etc perfect
monitoring case bob able observe wife state e g whether came home
happy sad nervous etc addition reward cases bob investors
able observe wife state making decision particular day
consider case one stage game nature utility function
known agent cannot observe current environment state selecting
action agent choose action work decision making uncertainty
suggested several approaches savage milnor luce raiffa kreps
one approaches maxmin safety level according
agent would choose action maximizes worst case payoff another
competitive ratio additive variant termed minmax
regret decision criterion milnor according agent would choose
action minimizes worst case ratio payoff could obtained
known environment state payoff would actually obtain returning
back example bob would known actual state wife could choose
action maximizes payoff since hint state go ahead
choose action minimizes competitive ratio example action leads
competitive ratio two bob guarantee payoff would get
half payoff could gotten known actual state wife
given repeated game incomplete information nature agent would
able choose one stage optimal action respect competitive ratio
maxmin value criteria stage since utility function initially unknown
bob initially know reward would receive actions function
wife state able simply choose action guarantees best
competitive ratio calls precise definition long run optimality criterion
mainly concerned policies strategies guaranteeing optimal
competitive ratio maxmin value obtained stages interested
particular ecient policies eciency measured terms rate convergence
hence bob case interested policy adopted bob would guarantee
almost day high probability least payoff guaranteed action
leading competitive ratio moreover bob wait much
start getting type satisfactory behavior
section define mentioned setting introduce preliminaries
sections discuss long run competitive ratio criterion section
even perfect monitoring case deterministic optimal policy exist
however exists ecient stochastic policy ensures
long run competitive ratio criterion holds high probability section
stochastic policies exist imperfect monitoring case section
prove perfect imperfect monitoring cases exists deterministic
ecient optimal policy long run maxmin criterion section compare
notions long run optimality criteria appearing related literature
particular long run optimality interpreted
competitive ratio decision criterion found useful settings line
e g papadimitriou yanakakis



fidynamic non bayesian decision making

qualitative distinguishes previous work area discuss
connections work work reinforcement learning

preliminaries
one shot decision payoff certainty strategic uncertainty tuple
u finite sets u real valued function defined
u every elements called actions
called states u called utility function interpretation numerical
values u context dependent let na denote number actions let ns
denote number states let n max na ns
mentioned setting classical static setting decision making
uncertainty actual state nature luce raiffa
deal dynamic setup agent faces decision without
knowing utility function u infinite number stages
explained introduction setting enables us capture general dynamic nonbayesian decision making contexts environment may change behavior
arbitrary unpredictable fashion mentioned introduction best captured
means repeated game nature state environment point
plays role action taken nature corresponding game agent knows
sets know payoff function u dynamic decision
payoff uncertainty strategic uncertainty therefore represented agent
pair dd finite sets stage nature chooses state st
agent know chosen state chooses receives u st
distinguish two informational structures perfect monitoring case state
st revealed agent alongside payoff u st imperfect monitoring case
states revealed agent generic history available agent stage
denoted ht perfect monitoring case ht htp r r denotes
set positive real numbers imperfect monitoring case ht htimp r
particular case assume h p h imp feg singleton containing
p
imp h imp symbol h
empty history e let h p
ht let h

used h p h imp strategy agent dynamic decision
function f h denotes
p set probability measures
every ht h f ht f ht words
agent observes history ht chooses randomizing amongst actions
probability f ht assigned action strategy f called pure f ht
probability measure concentrated singleton every
sections strategy recommended agent chosen according longrun decision criterion induced competitive ratio one stage decision criterion
see discussion section
remain unchanged agent initially know set rather
upper bound ns
notice need include explicit transition function representation due
fact non bayesian setup every transition possible
strategy decision theoretic concept coincides term policy used control theory
literature term protocol used distributed systems literature



fimonderer tennenholtz

competitive ratio decision criterion described may used agent
faces decision knows payoff function u well
sets reasonable decision criteria could used instead
one maxmin decision criterion discussed section another
minmax regret decision criterion luce raiffa milnor latter
simple variant competitive ratio treated similarly therefore
treated explicitly
every let maximal payoff agent get state

max
u

every define

c um ss

denote c maxs c let





cr min
c min
max c


cr called competitive ratio u action cr c
called competitive ratio action short cr action agent chooses
fraction could gotten
cr action guarantees receiving least cr

known state u cr every agent cannot guarantee
bigger fraction
long run decision non bayesian agent form prior probability
way nature choosing states nature may choose fixed sequence states
generally use probabilistic strategy g g q q
qt
nature viewed second player knows reward function







strategy may course refer whole history actions states given point
may depend payoff function
payoff function u pair probabilistic strategies f g g depend u
generate probability measure f g u set infinite histories q
endowed natural measurable structure event b q denote
probability b according b prob b precisely probability
measure uniquely defined values finite cylinder sets let q
st q coordinate random variables contain values actions
states selected agent environment stage respectively
h st h st every h q every
every st qt

prob st st








f g st

empty histories
st



fidynamic non bayesian decision making

definition depends monitoring structure perfect monitoring
case
u st u st
imperfect monitoring case
u u st
define auxiliary additional random variables q p
let xt c st cr xt otherwise let nt tt xt let
strategy f optimal exists integer k every payoff function u
every nature strategy g

prob nt every k

f g u strategy f optimal optimal
roughly speaking nt measures number stages competitive ratio
better value obtained first iterations optimal strategy
exists number k system runs k iterations get high
probability nt close e almost iterations good ones optimal

strategy guarantee get close wish situation iterations
good ones probability high wish notice require
mentioned useful property hold every payoff function every strategy
nature strong requirement consequence non bayesian setup since
clue reward function strategy selected nature
strategy may yield arbitrary sequences states reached best policy would
insist good behavior behavior adopted nature notice however
two relaxations introduced require successful behavior stages
whole process would successful high probability
major objective policy enable hold every dynamic
decision every nature strategy moreover wish hold small enough
k k small agent benefit obtaining desired behavior already
early stage subject following section complete section
useful technical observation strategy f optimal satisfies
optimality criterion every reward function every stationary strategy
nature stationary strategy defined sequence states z st

strategy nature chooses st stage independent history indeed assume f
strategy holds every reward function every stationary strategy
nature f optimal
given payoff function u strategy g optimality respect stationary
strategies implies f g u
prob nt every k js
note function c depends payoff function u therefore random variables
xt nt
interested reader may wish think long run optimality criteria view original work
pac learning valiant setting pac learning wish obtain desired behavior
situations high probability relatively fast



fimonderer tennenholtz

probability one therefore

prob nt every k
roughly speaking captures fact non bayesian setting
need present strategy good sequence states chosen nature
regardless way chosen

perfect monitoring

section present main refers case perfect monitoring shows existence optimal strategy case guarantees
desired behavior obtained polynomially many stages constructive first present rough idea strategy employed proof utility
function known agent could use competitive ratio action since
utility function initially unknown agent use greedy strategy
selects action optimal far competitive ratio concerned according
agent knowledge given point however agent try time time
sample random action strategy chooses right tradeoff exploration
exploitation phases order yield desired careful analysis needed
order prove optimality related strategy fact yields desired
polynomially many stages
introduce main theorem

theorem let dd dynamic decision perfect monitoring

every exists optimal strategy moreover optimal strategy
chosen ecient sense k taken polynomial
max n

proof recall na ns denote number actions states respectively
n max na ns proof assume simplicity n na ns
slight modifications required removing assumption without loss generality
define strategy f follows let



stage construct matrices utf ctf subset actions wt
following way define u f stage
performed stage st observed update u replacing
st entry u st stage utf define
f b
ctf utf
ctf maxfb utf b g uuttf
finally wt set
minb maxs ctf b obtained refer elements wt
temporarily good actions stage let zt sequence f g random
use uniform probability distribution select among actions exploration phase
obtained different exploration techniques well



fidynamic non bayesian decision making

variables prob zt sequence generated part strategy
independent observed history stage choosing action
agent ips coin independently past observations stage agent observes
zt zt agent chooses action wt randomizing equal probabilities
zt agent randomizes equal probabilities amongst actions
complete description strategy f let u given payoff function let st

given sequence states proceed holds k upper
integer value max




ln

n n ln







n



p

recall xt c st cr xt otherwise nt tt xt
slight change notation denote p prob probability measure induced
f u sequence states f g f g corresponds zt
values
let define

bk


x







zt k


roughly speaking bk captures cases temporarily good actions selected
stages
chernoff see alon spencer erdos every

p


x





zt e



recall given set denotes complement
hence





x
x
x

zt
e
p bk p
therefore
since k
define

k

p bk


z

k



e dt e


p bk

k

k






lk fnt every k g
roughly speaking lk captures cases competitive ratio actions better

actions regard selected stages
order prove f optimal e satisfied prove

p lk




fimonderer tennenholtz

suces prove


p lk jbk
define every six auxiliary random variables yt rt yts rst yts rs

let yt whenever zt xt yt otherwise let

rt


x


yt

every let yts whenever yt st yts otherwise let

x
yts

yts

rst

every every let
whenever yts

yt otherwise let

x
rs

yts



let g integer value k

p lk jbk p k rt gjbk



order prove

lk bk f k rt g g bk
indeed w path bk every k rt g w every
k

x
x
nt
xt vt yt
tt zt



vt denotes number stages zt since w bk
n r g








every k since g k nt every k hence
w lk
implies suces prove

p k rt gjbk
therefore suces prove every


p k rst ng jbk n
hence suces prove every every




fidynamic non bayesian decision making



p k

g
rs
n jbk



n



g
order prove note inequality rs
n satisfied gw
c cr nevertheless considered good action least n stages
b cr b
w l g assume ng integer let b satisfy uu

ever played stage st wt therefore




p k b played first ng stages st sjbk
hence
x x e x x

ut





nm

g
n



g
e n nm n

theorem shows ecient dynamic non bayesian decisions may obtained
appropriate stochastic policy moreover shows optimality obtained
time low degree polynomial max n interesting question whether
similar obtained pure deterministic strategy following example
shows deterministic strategies suce job
give example agent optimal pure strategy

example let fa g fs g assume negation agent
optimal pure strategy f

consider following two decision whose rows indexed actions
whose columns indexed states











corresponding ratio matrices

c




c












fimonderer tennenholtz

assume addition cases nature uses strategy g defined follows
g ht si f ht ai every zt zt
zt denote action state selected stage respectively let
let nti denote nt decision since f optimal exists k
every k nt nt note sequence
zt generated cases nk k implies used
half stages k hand nk k implies
used half stages k contradiction
ut
analytical completeness end section proving existence optimal
strategy merely optimal strategy optimal strategy obtained
utilizing optimal strategies whose existence proved theorem intervals
stages sizes converge infinity

corollary every dynamic decision perfect monitoring exists
optimal strategy

proof let fm

sequence limm let
every

optimal strategy decreasing


km


increasing
sequence
integers







prob nt every km



pm
k
km j j

let f strategy utilizes fm stages k km
k km km k easily verified f optimal

ut

imperfect monitoring

proceed give example imperfect monitoring case suciently
small agent optimal strategy

example non existence optimal strategies imperfect monitoring case

let fa g fs g let defined end
proof assume negation exists optimal strategy f consider
following two decision whose rows indexed actions whose columns
indexed states




b

c
b c


fidynamic non bayesian decision making



b c



b c

b c positive numbers satisfying b c let
ci ci ratio matrices

c






ac
b
b c

c



note unique cr action unique cr action
assume nature uses strategy g randomizes stage equal probabilities
amongst states given strategy nature agent cannot distinguish
two decision even knows nature strategy told
one chosen implies probability measures induced
f g decision respectively every
f g distribution stochastic process nti
respect j j f g
depend j every








prob nti mt prob nti mt f g
every sequence mt tt mt f tg
give complete proof rather illustrate proving representing
case reader easily derive complete proof

prob n prob n



indeed j

probj n


x

k

f e f uj sk



let f g f g defined

u sk u k
every k therefore implies
f optimal exists integer k probability least
respect hence respect nt every k
implies probability least played least
stages time k particular k choose integer k
suciently large according law large numbers nature chooses


fimonderer tennenholtz

least stages stage k probability least let cr
c denote cr ct decision respectively
cr max b

c
b c
therefore c st cr st
hence

probability least stages c cr
therefore f cannot optimal choose

ut



safety level

sake comparison discuss section safety level known maxmin
criterion let u decision denote

v max
min
u
v called safety level agent maxmin value every action
u v every called safety level action consider imperfect
monitoring model dynamic decision every sequence states
z st
st every every pure strategy f agent induce
z f
sequence actions
corresponding sequence payoffs ut
z f
uz f
u st every let nt denote number stages stage
agent payoff exceeds safety level v
ntz f f uz f
vg



say f safety level optimal every decision every sequence
states
lim n z f


convergence holds uniformly w r payoff functions u sequences states
every exists k k ntz f every
k every decision u every sequence states z
proposition every dynamic decision possesses safety level optimal strategy
imperfect monitoring case consequently perfect monitoring case moreover
optimal strategy chosen strongly ecient sense every
sequence states exists na stages agent receives payoff
smaller safety level na denotes number actions
proof let n na define strategy f follows play actions
first n stages every n every history h ht


fidynamic non bayesian decision making

u u ut define f h follows let vth
min ut min ranges stages define f h
maximizes vth obvious every sequence states

z st
n stages u st v
z f
sequence actions generated f sequence states hence nt n
ntz f defined thus k n ntz f every k

ut

discussion

note notations established section proposition well remain
unchanged assume utility function u takes values totally pre ordered
set without group structure decision making qualitative
ordinal distinguishes work previous work non bayesian repeated
games used probabilistic safety level criterion basic solution concept
one shot game works including blackwell hannan banos
megiddo recently auer cesa bianchi freund schapire hart
mas colell used several versions long run solution concepts
optimization average utility values time p
papers
goal strategies guarantee high probability tt u st
close vp
work best knowledge first introduce ecient dynamic
optimal policy basic competitive ratio context study sections
easily adapted case qualitative competitive ratio well
utility function takes values totally pre ordered set g addition assume
regret function maps g g pre ordered set h g g g g g
level regret agent receives utility level g rather g given
action state regret function determine maximal regret c h
agent action performed state

c max u u b
b ranges actions
qualitative regret action maximal regret action states
optimal qualitative competitive ratio obtained action
qualitative regret minimal notice arithmetic calculations needed make
sense qualitative version adapted case qualitative
competitive ratio ease exposition however used quantitative version
model numerical utility function represents regret function
probabilistic safety value vp agent decision u maxmin
value max ranges mixed actions

vp maxq mins

x



u q

set probability distributions q



fimonderer tennenholtz

work relevant reinforcement learning ai work area
however dealt mostly bayesian makes work complementary
work wish brie discuss connections differences
work existing work reinforcement learning
usual underlying structure reinforcement learning literature environment changes agent action particular probabilistic
function agent reward may probabilistic well notation markov
decision process mdp repeated game nature complete information
strategic certainty nature strategy depends probabilistically last action
state chosen standard partially observable mdp pomdp described similarly introducing level monitoring perfect imperfect monitoring
addition bandit basically modeled repeated games nature
probabilistic structural assumption nature strategy strategic uncertainty values transition probabilities example nature action
play role state slot machine basic bandit main difference classical discussed setting
state slot machine may change setting totally unpredictable manner e g
seed machine manually changed iteration say
solving learning solved reinforcement learning
mdp pomdp bandit later settings optimal strategy behave
poorly relative strategies obtained theory reinforcement learning take
particular structure account
non bayesian qualitative setup call optimality criteria differ
ones used current work reinforcement learning work reinforcement learning
discusses learning mechanisms optimize expected payoff long run
qualitative setting described long run expected payoffs may make much
sense optimality criteria expresses need obtain desired behavior
stages one easily construct examples one approaches favorite
one emphasis obtaining desired behavior relatively short run
though analytical reinforcement learning concerned
eventual convergence desired behavior policies shown quite
ecient practice
addition previously mentioned differences work work reinforcement learning wish emphasize much work pomdp uses information
structures different discussed work pomdp usually
assumes observations current state may available following presentation smallwood sondik although observations previous state
discussed well boutilier poole recall case perfect monitoring
previous environment state revealed immediate reward revealed
prefect imperfect monitoring may useful consider situations
presented extended case randomness
reward obtained agents well
likewise stochastic games shapley considered repeated games nature
partial information nature strategy matter one redefine concept state
games state pair state system action opponent



fidynamic non bayesian decision making

partial observations previous state current state revealed time
time may used setting completely clear may serve
subject future

references

alon n spencer j erdos p probabilistic method wiley interscience
auer p cesa bianchi n freund schapire r gambling rigged
casino adversial multi armed bandit proceedings th annual
symposium foundations computer science pp
aumann r maschler repeated games incomplete information
mit press
banos pseudo games annals mathematical statistics
blackwell analog minimax theorem vector payoffs pacific journal
mathematic
boutilier c poole computing optimal policies partially observable
decision processes compact representations proceedings th national
conference artificial intelligence pp
cassandra kaelbling l littman acting optimally partially observable stochastic domain proceedings th national conference artificial
intelligence pp
chernoff h measure asymptotic eciency tests hypothesis
sum observations annals mathematical statistics
fudenberg levine theory learning games miemo
fudenberg tirole j game theory mit press
hannan j approximation bayes risk repeated play dresher tucker
wolfe p eds contributions theory games vol iii annals
mathematics studies pp princeton university press
harsanyi j games incomplete information played bayesian players parts
ii iii management science
hart mas colell simple adaptive procedure leading correlated
equilibrium discussion center rationality interactive decision
theory hebrew university
kaelbling l littman moore reinforcement learning survey journal
artificial intelligence
kreps notes theory choice westview press


fimonderer tennenholtz

lovejoy w survey algorithmic methods partially observed markov decision
processes annals operations
luce r raiffa h games decisions introduction critical survey
john wiley sons
megiddo n repeated games incomplete information played nonbayesian players international journal game theory
mertens j f sorin zamir repeated games part core dp
milnor j games nature thrall r coombs c davis r
eds decision processes john wiley sons
monahan g survey partially observable markov decision processes theory
management science
papadimitriou c yannakakis shortest paths without map automata
languages programming th international colloquium proceedings pp

russell norvig p artificial intelligence modern prentice hall
savage l foundations statistics dover publications york
shapley l stochastic games proceeding national academic sciences
usa
smallwood r sondik e optimal control partially observable markov
processes finite horizon operations
sutton r special issue reinforcement learning machine learning
valiant l g theory learnable comm acm
watkins c dayan p technical note q learning machine learning

watkins c learning delayed rewards ph thesis cambridge university
wellman doyle j modular utility representation decision theoretic
proceedings first international conference ai systems
morgan kaufmann
wellman reasoning preference tech rep mit lcs tr
laboratory computer science mit




