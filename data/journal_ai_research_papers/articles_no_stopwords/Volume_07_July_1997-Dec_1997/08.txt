Journal Artificial Intelligence Research 7 (1997) 231-248

Submitted 7/97; published 11/97

Dynamic Non-Bayesian Decision Making
Dov Monderer
Moshe Tennenholtz

dov@ie.technion.ac.il
moshet@ie.technion.ac.il

Industrial Engineering Management
Technion { Israel Institute Technology
Haifa 32000, Israel

Abstract

model non-Bayesian agent faces repeated game incomplete information Nature appropriate tool modeling general agent-environment
interactions. model environment state (controlled Nature) may change arbitrarily, feedback/reward function initially unknown. agent Bayesian,
form prior probability neither state selection strategy Nature,
reward function. policy agent function assigns action
every history observations actions. Two basic feedback structures considered.
one { perfect monitoring case { agent able observe previous
environment state part feedback, { imperfect monitoring
case { available agent reward obtained. settings
refer partially observable processes, current environment state unknown.
main result refers competitive ratio criterion perfect monitoring case.
prove existence ecient stochastic policy ensures competitive
ratio obtained almost stages arbitrarily high probability, eciency
measured terms rate convergence. shown optimal
policy exist imperfect monitoring case. Moreover, proved
perfect monitoring case exist deterministic policy satisfies long
run optimality criterion. addition, discuss maxmin criterion prove
deterministic ecient optimal strategy exist imperfect monitoring case
criterion. Finally show approach long-run optimality viewed
qualitative, distinguishes previous work area.

1. Introduction
Decision making central task artificial agents (Russell & Norvig, 1995; Wellman,
1985; Wellman & Doyle, 1992). point time, agent needs select among
several actions. may simple decision, takes place once,
complicated decision series simple decisions made. question
\what right actions be" basic issue discussed settings,
fundamental importance design artificial agents.
static decision-making context (problem) artificial agent consists set actions agent may perform, set possible environment states, utility/reward
function determines feedback agent performs particular action
particular state. problem best represented matrix columns indexed
states, rows indexed actions rewards entries. reward
function known agent say agent payoff uncertainty
c 1997 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiMonderer Tennenholtz

refer problem problem incomplete information(Fudenberg & Tirole, 1991).
modeling problem incomplete information one must describe underlying assumptions knowledge agent reward function. example,
agent may know bounds rewards, may know (or partially know) underlying
probabilistic structure1 . dynamic (multistage) decision-making setup agent faces
static decision problems stages. stage agent selects action performed environment selects state. history actions states determines
immediate reward well next one-shot decision problem. history actions
states determines next selected state. Work reinforcement learning artificial intelligence (Kaelbling, Littman, & Moore, 1996) adopted view agent
operating probabilistic Bayesian setting, agent's last action last state
determine next environment state based given probability distribution. Naturally,
learner may a-priori familiar probability distribution, existence
underlying probabilistic model key issue system's modeling. However,
assumption ultimate one. particular, much work areas AI
economics dealt non-probabilistic settings environment changes
unpredictable manner2 . agent know uence choices
selection next state (i.e., certain environment strategy),
say agent strategic uncertainty.
paper use general model representation agent-environment interactions agent payoff strategic uncertainty. deal
non-Bayesian agent faces repeated game incomplete information Nature.
repeated game Nature agent faces static decision problem
stage environment state taken action chosen opponents.
decision problem called game stress fact agent's action
state independently chosen. fact game repeated refers fact
set actions, set possible states, one shot utility function
vary time3 . said, consider agent payoff uncertainty
strategic uncertainty. is, a-priori ignorant utility function (i.e.,
game incomplete information) well state selection strategy Nature.
agent non-Bayesian sense assume probabilistic model
concerning nature's strategy sense assume probabilistic
model concerning reward function, though may assume lower upper bounds4 .
consider two examples illustrate above-mentioned notions model. Consider
1. example, agent may know probability distribution set reward functions, may assume
probability exists without assumption structure, may partial information
distribution ignorant parameters (e.g., may believe reward
function drawn according normal distribution unknown covariance matrix).
2. many intermediate cases assumed changes probabilistic nonMarkovian structure.
3. general setup, sets may vary time. useful analysis done model
changes completely arbitrary.
4. Repeated games complete information, generally, multistage games stochastic games
extensively studied game theory economics. partial list includes: (Shapley,
1953; Blackwell, 1956; Luce & Raiffa, 1957), recently (Fudenberg & Tirole, 1991; Mertens,
Sorin, & Zamir, 1995), evolving literature learning (e.g., Fudenberg & Levine 1997).
incomplete information setup player ignorant game played inspired

232

fiDynamic Non-Bayesian Decision Making

investor, , investing daily certain index stock market. daily profits
depends action (selling buying certain amount) environment state
{ percentage change price index. investor complete information
reward function knows reward realized particular
investment particular change, strategic uncertainty changes
index price. So, playing repeated game complete information
Nature strategic uncertainty.
Consider another investor, 1, invests particular mutual fund. fund invests
stock market strategy known investor. Assume
state represents vector percentage changes stocks, investor
know reward function. example, cannot say advance would profit
would buy one unit fund stock prices increase 1 percent. Thus, 1
plays repeated game incomplete information. addition 1 attempt
construct probabilistic model concerning reward function market behavior,
non-Bayesian analysis may apply him. another example, assume Bob
decide evening whether prepare tea coffee wife gets
home. wife wishes drink either tea coffee wishes ready her.
reaction Bob's wife tea coffee may depend state day,
predicted based history actions states previous days. Bob
got married cannot tell reward get wife happy makes
cup tea. course may eventually know it, decisions learning
period precisely subject paper.
example generality above-mentioned setting, consider model
Markov decision processes complete incomplete information. Markov decision
process agent's action given state determines (in probabilistic fashion) next
state obtained. is, agent structural assumption state selection
strategy. repeated game Nature without added assumptions captures fact
transition state state may depend history arbitrary way.
agent performs action state st , part feedback would u(at; st ),
u reward function. distinguish two basic feedback structures.
one { perfect monitoring case { agent able observe previous
environment state part feedback, { imperfect monitoring
case { available agent reward obtained5 . Notice
feedback structures, current state observed agent called
select action6 . investors 1 face repeated game perfect monitoring
percentage changes become public knowledge iteration.
example, Bob make decision, situation imperfect
monitoring, Bob would able observe reward behavior (e.g., whether
(Harsanyi, 1967). See Aumann Maschler (1995) comprehensive survey.
literature deals (partially) Bayesian agents. rare exceptions cited Section 6.
5. Notice former assumption popular related game theory literature (Aumann &
Maschler, 1995). Many intermediate monitoring structures may interesting well.
6. case evolving literature problem controlling partially observable Markov
decision processes (Lovejoy, 1991; Cassandra, Kaelbling, & Littman, 1994; Monahan, 1982). contrast,
Q-learning theory (Watkins, 1989; Watkins & Dayan, 1992; Sutton, 1992) assume knowledge
current state.

233

fiMonderer Tennenholtz

says \thanks", \that's terrible", \this exactly wanted", etc.). perfect
monitoring case, Bob able observe wife's state (e.g., whether came home
happy, sad, nervous, etc.) addition reward. cases Bob (like investors)
able observe wife's state making decision particular day.
Consider case one stage game Nature, utility function
known, agent cannot observe current environment state selecting
action. agent choose action? Work decision making uncertainty
suggested several approaches (Savage, 1972; Milnor, 1954; Luce & Raiffa, 1957; Kreps,
1988). One approaches maxmin (safety-level) approach. According
approach agent would choose action maximizes worst case payoff. Another
approach competitive ratio approach (or additive variant, termed minmax
regret decision criterion (Milnor, 1954). According approach agent would choose
action minimizes worst case ratio payoff could obtained
known environment state payoff would actually obtain.7 Returning
back example, Bob would known actual state wife, could choose
action maximizes payoff. Since hint state, go ahead
choose action minimizes competitive ratio. example, action leads
competitive ratio two, Bob guarantee payoff would get
half payoff could gotten known actual state wife.
Given repeated game incomplete information Nature, agent would
able choose one stage optimal action (with respect competitive ratio
maxmin value criteria) stage, since utility function initially unknown. So,
Bob initially know reward would receive actions function
wife's state, able simply choose action guarantees best
competitive ratio. calls precise definition long-run optimality criterion.
paper mainly concerned policies (strategies) guaranteeing optimal
competitive ratio (or maxmin value) obtained stages. interested
particular ecient policies, eciency measured terms rate convergence.
Hence Bob's case, interested policy adopted Bob would guarantee
almost day, high probability, least payoff guaranteed action
leading competitive ratio. Moreover, Bob wait much
start getting type satisfactory behavior.
Section 2 define mentioned setting introduce preliminaries.
Sections 3 4 discuss long-run competitive ratio criterion: Section 3 show
even perfect monitoring case, deterministic optimal policy exist.
However, show exists ecient stochastic policy ensures
long-run competitive ratio criterion holds high probability. Section 4 show
stochastic policies exist imperfect monitoring case. Section 5
prove perfect imperfect monitoring cases exists deterministic
ecient optimal policy long-run maxmin criterion. Section 6 compare
notions long-run optimality criteria appearing related literature.
particular, show approach long-run optimality interpreted
7. competitive ratio decision criterion found useful settings on-line
algorithms (e.g., Papadimitriou & Yanakakis, 1989).

234

fiDynamic Non-Bayesian Decision Making

qualitative, distinguishes previous work area. discuss
connections work work reinforcement learning.

2. Preliminaries
(one-shot) decision problem (with payoff certainty strategic uncertainty) 3-tuple
=< A; S; u >, finite sets u real-valued function defined
u(a; s) > 0 every (a; s) 2 . Elements called actions
called states. u called utility function. interpretation numerical
values u(a; s) context-dependent8 . Let nA denote number actions A, let nS
denote number states let n = max(nA ; nS ).
above-mentioned setting classical static setting decision making,
uncertainty actual state nature (Luce & Raiffa, 1957). paper
deal dynamic setup, agent faces decision problem D, without
knowing utility function u, infinite number stages, = 1; 2; : : :.
explained introduction, setting enables us capture general dynamic nonBayesian decision-making contexts, environment may change behavior
arbitrary unpredictable fashion. mentioned introduction, best captured
means repeated game Nature. state environment point
plays role action taken Nature corresponding game. agent knows
sets , know payoff function u.9 dynamic decision problem
(with payoff uncertainty strategic uncertainty) therefore represented agent
pair DD =< A; > finite sets10. stage t, Nature chooses state st 2 .
agent, know chosen state, chooses 2 A, receives u(at; st).
distinguish two informational structures. perfect monitoring case, state
st revealed agent alongside payoff u(at; st). imperfect monitoring case,
states revealed agent. generic history available agent stage +1
denoted ht . perfect monitoring case, ht 2 Htp = (A R+ )t , R+ denotes
set positive real numbers. imperfect monitoring case, ht 2 Htimp = (A R+ )t.
particular case = 0 assume H0p = H0imp = feg singleton containing
p
imp = [1 H imp . symbol H
empty history e. Let H p = [1
t=0 Ht let H
t=0
used H p H imp . strategy11 agent dynamic decision problem
function F : H ! (A) , (A) denotes
P set probability measures A.
is, every ht 2 H , F (ht ) : ! [0; 1] a2A F (ht )(a) = 1. words,
agent observes history ht chooses at+1 randomizing amongst actions,
probability F (ht )(a) assigned action a. strategy F called pure F (ht )
probability measure concentrated singleton every 0.
Sections 2{4 strategy recommended agent chosen according \longrun" decision criterion induced competitive ratio one-stage decision criterion.
8. See discussion Section 6.
9. results paper remain unchanged agent initially know set , rather
upper bound nS .
10. Notice need include explicit transition function representation. due
fact non-Bayesian setup every transition possible.
11. Strategy decision theoretic concept. coincides term policy used control theory
literature, term protocol used distributed systems literature.

235

fiMonderer Tennenholtz

competitive ratio decision criterion, described below, may used agent
faces decision problem once, knows payoff function u well
sets . \reasonable" decision criteria could used instead.
One maxmin decision criterion discussed Section 5, another
minmax regret decision criterion (Luce & Raiffa, 1957; Milnor, 1954). latter
simple variant competitive ratio (and treated similarly), therefore
treated explicitly paper.
every 2 let (s) maximal payoff agent get state s.

(s) = max
u(a; s):
a2A
every 2 2 define

c(a; s) = uM(a;(ss)) :

Denote c(a) = maxs2S c(a; s), let





CR = min
c(a) = min
max c(a; s) :
a2A
a2A s2S
CR called competitive ratio =< A; S; u >. action CR = c(a)
called competitive ratio action, short CR action. agent chooses
1 fraction could gotten,
CR action guarantees receiving least CR
1
known state s. is, u(a; s) CR (s) every 2 . agent cannot guarantee
bigger fraction.
long-run decision problem, non-Bayesian agent form prior probability
way Nature choosing states. Nature may choose fixed sequence states or,
generally, use probabilistic strategy G, G : Q ! (S ), Q = [1
t=0 Qt =
t. Nature viewed second player knows reward function.
[1
(



)
t=0
strategy may course refer whole history actions states given point
may depend payoff function.
payoff function u pair probabilistic strategies F; G, G depend u,
generate probability measure = F;G;u set infinite histories Q1 = (A )1
endowed natural measurable structure. event B Q1 denote
probability B according (B ) Prob (B ). precisely, probability
measure uniquely defined values finite cylinder sets: Let : Q1 !
St : Q1 ! coordinate random variables contain values actions
states selected agent environment stage (respectively). is,
At(h) = St(h) = st every h = ((a1; s1); (a2; s2); : : :) Q1 . every 1
every ((a1; s1); : : :; (aT ; sT )) 2 QT ,

Prob ((At; St) = (at; st) 1 ) =


0



t=1

F ('t,1)(at )G( t,1)(st );

'0 empty histories, 2
t,1 = ((a1; s1 ); : : :; (at,1; st,1)) ;

236

fiDynamic Non-Bayesian Decision Making

definition 't,1 depends monitoring structure. perfect monitoring
case,
't,1 = ((a1 ; s1; u(a1; s1)); : : :; (at,1; st,1; u(at,1; st,1))) ;
imperfect monitoring case
't,1 = ((a1; u(a1; s1)); : : :; (at,1; u(at,1; st,1))) :
define auxiliary additional random variables Q1 . P
Let Xt = 1 c(At ; St) CR Xt = 0 otherwise, let NT = Tt=1 Xt .12 Let > 0.
strategy F -optimal exists integer K every payoff function u
every Nature's strategy G

Prob (NT (1 , )T every K ) 1 ,
(1)
= F;G;u . strategy F optimal -optimal > 0.
Roughly speaking, NT measures number stages competitive ratio (or
better value) obtained first iterations. -optimal strategy
exists number K , system runs K iterations get high
probability NT close 1 (i.e., almost iterations good ones). optimal

strategy guarantee get close wish situation iterations
good ones, probability high wish. Notice require
above-mentioned useful property hold every payoff function every strategy
Nature. strong requirement consequence non-Bayesian setup; since
clue reward function strategy selected Nature (and
strategy may yield arbitrary sequences states reached), best policy would
insist good behavior behavior adopted Nature. Notice however
two relaxations introduced here; require successful behavior stages,
whole process would successful (very high) probability.
major objective find policy enable (1) hold every dynamic
decision problem every Nature strategy. Moreover, wish (1) hold small enough
K . K small agent benefit obtaining desired behavior already
early stage. 13 subject following section. complete section
useful technical observation. show strategy F -optimal satisfies
optimality criterion (1) every reward function every stationary strategy
nature, stationary strategy defined sequence states z = (st )1
t=1 .
strategy Nature chooses st stage t, independent history. Indeed, assume F
strategy (1) holds every reward function every stationary strategy
Nature, show F -optimal.
Given payoff function u strategy G, -optimality respect stationary
strategies implies = F;G;u ,
Prob (NT (1 , )T every K )jS1; S2; : : :) 1 , ;
12. Note function c(a; s) depends payoff function u therefore random variables
Xt Nt .
13. interested reader may wish think long-run optimality criteria view original work
PAC learning (Valiant, 1984). setting, PAC learning, wish obtain desired behavior,
situations, high probability, relatively fast.

237

fiMonderer Tennenholtz

probability one. Therefore

Prob(NT (1 , )T every K ) 1 , :
Roughly speaking, captures fact non-Bayesian setting
need present strategy good sequence states chosen Nature,
regardless way chosen.

3. Perfect Monitoring

section present main result. result refers case perfect monitoring, shows existence -optimal strategy case. guarantees
desired behavior obtained polynomially many stages. result constructive. first present rough idea strategy employed proof. utility
function known agent could use competitive ratio action. Since
utility function initially unknown agent use greedy strategy,
selects action optimal far competitive ratio concerned, according
agent's knowledge given point. However, agent try time time
sample random action.14 strategy chooses right tradeoff exploration
exploitation phases order yield desired result. careful analysis needed
order prove optimality related strategy, fact yields desired
result polynomially many stages.
introduce main theorem.

Theorem 3.1: Let DD =< A; > dynamic decision problem perfect monitoring.

every > 0 exists -optimal strategy. Moreover, -optimal strategy
chosen ecient sense K (in (1)) taken polynomial
max(n; 1 ).

Proof: Recall nA nS denote number actions states respectively,
n = max(nA ; nS ). proof assume simplicity n = nA = nS .
slight modifications required removing assumption. Without loss generality,
< 1. define strategy F follows: Let = 8 . is,

1
= 8:
stage 1 construct matrices UTF ; CTF subset actions WT
following way: Define U1F (a; s) = a; s. stage > 1, ,1
performed stage , 1, sT ,1 observed, update U replacing
(aT ,1; sT ,1) entry u(aT ,1; sT ,1). stage 1, UTF (a; s) = , define
F b;s)
CTF (a; s) = 1. UTF (a; s) =
6 , CTF (a; s) = maxfb:UTF (b;s)6=g UUTTF ((a;s
) . Finally WT set
2 minb2A maxs2S CTF (b; s) obtained. refer elements WT
temporarily good actions stage . Let (Zt)t1 sequence i.i.d. f0; 1g random
14. use uniform probability distribution select among actions exploration phase. result
obtained different exploration techniques well.

238

fiDynamic Non-Bayesian Decision Making

variables Prob(Zt = 1) = 1 , M1 . sequence generated part strategy,
independent observed history. stage, choosing action,
agent ips coin, independently past observations. stage agent observes
Zt. Zt = 1, agent chooses action Wt randomizing equal probabilities.
Zt = 0 agent randomizes equal probabilities amongst actions A.
complete description strategy F . Let u given payoff function, let (st)1
t=1
given sequence states. proceed show (1) holds K upper
integer value = max(ff1 + 2; ff2 + 2),


256
ff1 = 128
2 ln 3

n2 (n 8 + 1) ln
ff2 =
3
4



2
2n + 1


:
P

Recall Xt = 1 c(At; st ) CR Xt = 0 otherwise, NT = Tt=1 Xt .
slight change notation, denote P = Prob probability measure induced
F , u sequence states (A f0; 1g)1 (where f0; 1g corresponds Zt
values).
Let " = 8 . Define

BK =

(T
X

t=1

)


1
Zt 1 , , " K :


Roughly speaking, BK captures cases temporarily good actions selected
stages.
(Chernoff, 1952) (see (Alon, Spencer, & Erdos, 1992)), every ,

P


X
t=1

!

2
Zt < (1 , 1 , ")T e, " 2T :



Recall given set , denotes complement .
Hence,
!
1

1
2
X
X
X
1
Zt < (1 , , ")T
e, " 2T :
P (BK ) P
Therefore
Since K > ff1 ,
Define:

=k

P (BK )

t=1
Z1

k ,1



e, " 2T dT = "22 e,
2

P (BK ) < 2

=K

"2 (K ,1)

2

:
(2)

LK = fNT (1 , )T every K g:
Roughly speaking, LK captures cases competitive ratio actions (or better

actions regard) selected stages.
order prove F -optimal (i.e., (1) satisfied), prove

P (LK ) <
239

(3)

fiMonderer Tennenholtz

(2) suces prove

(4)
P (LK jBK ) 2
define every 1, 2 2 six auxiliary random variables, Yt ; Rt; Yts ; Rst; Yts;a ; Rs;a
.
Let Yt = 1 whenever Zt = 1 Xt = 0, Yt = 0 otherwise. Let

RT =


X
t=1

Yt :

every 2 let Yts = 1 whenever Yt = 1 st = s, Yts = 0 otherwise. Let

X
Yts :
t=1
Yts;a = 1

RsT =

every 2 every 2 A, let
whenever Yts = 1 = a,
s;a
Yt = 0 otherwise. Let

X
Rs;a
=
Yts;a :

t=1

Let g integer value 34 K . show

P (LK jBK ) P(9T K ; RT gjBK )

(5)

order prove (5) show

LK \ BK f9T K ; RT g g \ BK :
Indeed, w path BK every K RT < g , then, w, every
K,

X
X
NT
Xt VT , Yt;
1tT;Zt=1

t=1

VT denotes number stages 1 Zt = 1. Since w 2 BK ,
N (1 , 1 , ")T , R > (1 , 1 , ")T , g








every K . Since M1 = " = 8 g 34 K , NT (1 , )T every K . Hence,
w 2 LK .
(5) implies suces prove

P(9T K ; RT gjBK ) 2
Therefore suces prove every 2 ,


P 9T K; RsT ng jBK 2n :
Hence suces prove every 2 every 2 A,
240

(6)

fiDynamic Non-Bayesian Decision Making



= P 9T K;

g
Rs;a
n2 jBK



2n 2

(7)

g
order prove (7) , note inequality Rs;a
n2 satisfied gw,
c(a; s) > CR nevertheless considered good action least n2 stages
b;s) > CR. b
1 (w.l.o.g. assume ng2 integer). Let b 2 satisfy uu((a;s
)
ever played stage st = s, 62 Wt t. Therefore




P 9T K; b played first ng2 stages st = sjBK :
Hence
(1 , x1 )x+1 e,x x 1,

ut



1, 1

nM

g2
n

:

g
e, n2 (nM +1) < 2n 2 :

Theorem 3.1 shows ecient dynamic non-Bayesian decisions may obtained
appropriate stochastic policy. Moreover, shows -optimality obtained
time (low degree) polynomial max(n; 1 ). interesting question whether
similar results obtained pure/deterministic strategy. following example
shows, deterministic strategies suce job.
give example agent optimal pure strategy.

Example 1: Let = fa1; a2g = fs1; s2g. Assume negation agent
optimal pure strategy f .

Consider following two decision problems whose rows indexed actions
whose columns indexed states:

D1 =

1 10
30 2

D2 =

1 30
10 2

corresponding ratio matrices:

C1 =

30 1
1 5

C2 =

10 1
1 15
241

!

!

!
!

fiMonderer Tennenholtz

Assume addition cases Nature uses strategy g , defined follows:
g (ht) = si f (ht) = ai , = 1; 2. is, every t, (at; zt ) = (a1; s1) (at; zt) = (a2; s2),
zt denote action state selected stage t, respectively. Let < 0:25.
Let NTi denote NT decision problem i. Since f -optimal, exists K
every K , NT1 (1 , )T NT2 (1 , )T . Note sequence
((at; zt))t1 generated cases. NK1 (1 , )K implies (a2 ; s2) used
half stages 1; 2; : : :; K . hand, NK2 (1 , )K implies (a1; s1)
used half stages 1; 2; : : :; K . contradiction.
ut
analytical completeness, end section proving existence optimal
strategy (and merely -optimal strategy). optimal strategy obtained
utilizing -optimal strategies (whose existence proved Theorem 3.1) intervals
stages sizes converge infinity, ! 0.

Corollary 3.2: every dynamic decision problem perfect monitoring exists
optimal strategy.

Proof: 1, let Fm

sequence limm!1 = 0. Let
every 1

-optimal strategy, (m )1 decreasing
m=1
2
(Km )1


increasing
sequence
integers
m=1






Prob NT (1 , 2 )T every Km 1 , 2m ;



Pm
K
Km+1 2 j=1 j :

Let F strategy 1 utilizes Fm stages K0 + : : : + Km,1 + 1
K0 + : : : + Km,1 + Km, K0 = 0. easily verified F optimal.

ut

4. Imperfect Monitoring

proceed give example imperfect monitoring case, suciently
small > 0, agent -optimal strategy.

Example 2 (Non-existence -optimal strategies imperfect monitoring case)

Let = fa1; a2g, = fs1 ; s2; s3g. Let < 0 , 0 defined end
proof. Assume negation exists -optimal strategy F . Consider
following two decision problems whose rows indexed actions whose columns
indexed states:
!
2

2
b
2
c
D1 = b c
242

fiDynamic Non-Bayesian Decision Making

!

2a 2b 2c

D2 =

b c

a; b c positive numbers satisfying: > 4b > 16c. = 1; 2, let
Ci = (ci (a; s))a2A;s2S ratio matrices. is:

C1 =

1 1 1
2 2 2

!

1 1 2ac
2a 2b 1
b c

C2 =

!

Note a1 unique CR action D1 a2 unique CR action D2.
Assume Nature uses strategy G randomizes stage equal probabilities
(of 13 ) amongst 3 states. Given strategy Nature, agent cannot distinguish
two decision problems, even knows Nature's strategy told
one chosen. implies 1 2 probability measures induced
F G (A )1 decision problems D1 D2 respectively, every
2 f1; 2g, distribution stochastic process (NTi )1
=1 respect j , j 2 f1; 2g,
depend j . is, every 1








Prob1 Nti 2 Mt = Prob2 Nti 2 Mt ; 2 f1; 2g (8)
every sequence (Mt )Tt=1 Mt f0; 1; : : :; tg 1 .
give complete proof (8), rather illustrate proving representing
case. reader easily derive complete proof. show

Prob1 (N21 = 0) = Prob2 (N21 = 0)

(9)

Indeed, j = 1; 2,

Probj (N21 = 0) = 31

3
X

k=1

F (e)(a2)F (a2 ; uj (a2 ; sk ))(a2)

(10)

Let : f1; 2; 3g ! f1; 2; 3g defined (1) = 3, (2) = 1, (3) = 2.

u1(a2 ; sk ) = u2 (a2; s(k) )
every 1 k 3. Therefore (10) implies (9).
F -optimal, exists integer K probability least
1 , respect 1 , hence respect 2 , NT1 (1 , )T every K .
implies probability least 1 , , a1 played least 1 ,
stages time , K , particular = K . choose integer K
suciently large according Law Large Numbers, Nature chooses
243

fiMonderer Tennenholtz

s3 least 31 , stages stage K probability least 1 , . Let CR2
c2t denote CR ct decision problem 2, respectively.
> CR = max( 2a ; 2b ):
2
2c
b c
Therefore, = a1, C2(At ; St) CR2 St =
6 s3. Hence,
2
probability least 1 , 2 , + (1 , )( 3 + ) stages c2t CR2.
Therefore F cannot -optimal, choose 0 20 < 1 , 0

ut

0 + (1 , 0)( 23 + 0 ) < 1 , 0 :

5. Safety Level

sake comparison discuss section safety level (known maxmin)
criterion. Let =< A; S; u > decision problem. Denote

v = max
min
u(a; s)
v called safety level agent (or maxmin value). Every action
u(a; s) v every called safety level action. consider imperfect
monitoring model dynamic decision problem < A; >. Every sequence states
z = (st)1
t=1 st 2 every 1 every pure strategy f agent induce
z;f 1
sequence actions (at)1
t=1 corresponding sequence payoffs (ut )t=1 ,
z;f
uz;f
= u(at ; st) every 1. Let NT denote number stages stage
agent's payoff exceeds safety level v . is,
NTz;f = #f1 : uz;f
vg

(11)

say f safety level optimal every decision problem every sequence
states,
lim 1 N z;f = 1;
!1

convergence holds uniformly w.r.t. payoff functions u sequences states
. is, every > 0 exists K = K ( ) NTz;f (1 , )T every
K every decision problem < A; S; u > every sequence states z .
Proposition 5.1: Every dynamic decision problem possesses safety level optimal strategy
imperfect monitoring case, consequently perfect monitoring case. Moreover,
optimal strategy chosen strongly ecient sense every
sequence states exists nA , 1 stages agent receives payoff
smaller safety level, nA denotes number actions.
Proof: Let n = nA . Define strategy f follows: Play actions
first n stages. every n + 1, every history h = hT ,1 =
244

fiDynamic Non-Bayesian Decision Making

((a1; u1); (a2; u2; : : :; (aT ,1; uT ,1)) define f (h) 2 follows: 2 A, let vTh (a) =
min ut , min ranges stages , 1 = a. Define f (h) = ,
maximizes vTh (a) 2 A. obvious every sequence states
1
z = (st )1
t=1 n , 1 stages u(at ; st) < v , (at )t=1
z;f
sequence actions generated f sequence states. Hence NT , n,
NTz;f defined (11). Thus K () = n , T1 NTz;f 1 , every K ( ).

ut

6. Discussion

Note notations established Section 5, Proposition 5.1 well, remain
unchanged assume utility function u takes values totally pre-ordered
set without group structure. is, approach decision making qualitative
(or ordinal). distinguishes work previous work non-Bayesian repeated
games, used probabilistic safety level criterion basic solution concept
one shot game15. works, including (Blackwell, 1956; Hannan, 1957; Banos, 1968;
Megiddo, 1980), recently (Auer, Cesa-Bianchi, Freund, & Schapire, 1995; Hart
& Mas-Colell, 1997), used several versions long-run solution concepts, based
optimization average utility values time. is, P
papers
goal find strategies guarantee high probability T1 Tt=1 u(At; St)
close vp .
work is, best knowledge, first introduce ecient dynamic
optimal policy basic competitive ratio context. study results sections 2-4
easily adapted case qualitative competitive ratio well. approach,
utility function takes values totally pre-ordered set G addition assume
regret function, maps G G pre-ordered set H . g1 ; g2 2 G, (g1; g2)
level regret agent receives utility level g1 rather g2. Given
action state s, regret function determine maximal regret, c(a; s) 2 H
agent action performed state s. is,

c(a; s) = max (u(a; s); u(b; s));
b ranges actions.
qualitative regret action maximal regret action states.
optimal qualitative competitive ratio obtained using action
qualitative regret minimal. Notice arithmetic calculations needed (or make
sense) qualitative version. results adapted case qualitative
competitive ratio. ease exposition, however, used quantitative version
model, numerical utility function represents regret function.
15. probabilistic safety value, vp , agent decision problem =< A; S; u > maxmin
value max ranges mixed actions.

vp = maxq2(A) mins2S

X

a2A

u(a; s)q(a);

(A) set probability distributions q A.

245

fiMonderer Tennenholtz

work relevant research reinforcement learning AI. Work area,
however, dealt mostly Bayesian models. makes work complementary
work. wish brie discuss connections differences
work existing work reinforcement learning.
usual underlying structure reinforcement learning literature environment changes result agent's action based particular probabilistic
function. agent's reward may probabilistic well.16 notation, Markov
decision process (MDP) repeated game Nature complete information
strategic certainty, Nature's strategy depends probabilistically last action
state chosen17 . Standard partially observable MDP (POMDP) described similarly introducing level monitoring perfect imperfect monitoring.
addition, bandit problems basically modeled repeated games Nature
probabilistic structural assumption Nature's strategy , strategic uncertainty values transition probabilities. example, Nature's action
play role state slot machine basic bandit problem. main difference classical problem problem discussed setting
state slot machine may change setting totally unpredictable manner (e.g.,
seed machine manually changed iteration). say
solving learning problem solved problem reinforcement learning
MDP, POMDP, bandit problems. later settings, optimal strategy behave
poorly relative strategies obtained theory reinforcement learning, take
particular structure account.
non-Bayesian qualitative setup call optimality criteria differ
ones used current work reinforcement learning. Work reinforcement learning
discusses learning mechanisms optimize expected payoff long run.
qualitative setting (as described above) long-run expected payoffs may make much
sense. optimality criteria expresses need obtain desired behavior
stages. One easily construct examples one approaches favorite
one. emphasis obtaining desired behavior relatively short run.
Though, analytical results reinforcement learning concerned
eventual convergence desired behavior, policies shown quite
ecient practice.
addition previously mentioned differences work work reinforcement learning, wish emphasize much work POMDP uses information
structures different discussed paper. Work POMDP usually
assumes observations current state may available (following presentation Smallwood & Sondik, 1973), although observations previous state
discussed well (Boutilier & Poole, 1996). Recall case perfect monitoring
previous environment state revealed, immediate reward revealed
prefect imperfect monitoring. may useful consider situations
16. results presented paper extended case randomness
reward obtained agents well.
17. Likewise, stochastic games (Shapley, 1953) considered repeated games Nature
partial information Nature's strategy. matter one redefine concept state
games. state pair (s; a), state system action opponent.

246

fiDynamic Non-Bayesian Decision Making

(partial) observations previous state current state revealed time
time. may used setting completely clear, may serve
subject future research.

References

Alon, N., Spencer, J., & Erdos, P. (1992). Probabilistic Method. Wiley-Interscience.
Auer, P., Cesa-Bianchi, N., Freund, Y., & Schapire, R. (1995). Gambling rigged
casino: adversial multi-armed bandit problem. Proceedings 36th Annual
Symposium Foundations Computer Science, pp. 322{331.
Aumann, R., & Maschler, M. (1995). Repeated Games Incomplete Information.
MIT Press.
Banos, A. (1968). pseudo games. Annals Mathematical Statistics, 39, 1932{1945.
Blackwell, D. (1956). analog minimax theorem vector payoffs. Pacific Journal
Mathematic, 6, 1{8.
Boutilier, C., & Poole, D. (1996). Computing optimal policies partially observable
decision processes using compact representations. Proceedings 13th National
Conference Artificial Intelligence, pp. 1168{1175.
Cassandra, A., Kaelbling, L., & Littman, M. (1994). Acting optimally partially observable stochastic domain. Proceedings 12th National Conference Artificial
Intelligence, pp. 1023{1028.
Chernoff, H. (1952). measure asymptotic eciency tests hypothesis based
sum observations. Annals Mathematical Statistics, 23, 493{509.
Fudenberg, D., & Levine, D. (1997). Theory learning games. miemo.
Fudenberg, D., & Tirole, J. (1991). Game Theory. MIT Press.
Hannan, J. (1957). Approximation bayes risk repeated play. Dresher, M., Tucker,
A., & Wolfe, P. (Eds.), Contributions Theory Games, vol. III (Annals
Mathematics Studies 39), pp. 97{139. Princeton University Press.
Harsanyi, J. (1967). Games incomplete information played bayesian players, parts
i, ii, iii. Management Science, 14, 159{182.
Hart, S., & Mas-Colell, A. (1997). simple adaptive procedure leading correlated
equilibrium. Discussion paper 126, Center Rationality Interactive Decision
Theory, Hebrew University.
Kaelbling, L., Littman, M., & Moore, A. (1996). Reinforcement learning: survey. Journal
Artificial Intelligence Research, 4, 237{258.
Kreps, D. (1988). Notes Theory Choice. Westview press.
247

fiMonderer Tennenholtz

Lovejoy, W. (1991). survey algorithmic methods partially observed markov decision
processes. Annals Operations Research, 28 (1{4), 47{66.
Luce, R. D., & Raiffa, H. (1957). Games Decisions- Introduction Critical Survey.
John Wiley Sons.
Megiddo, N. (1980). repeated games incomplete information played nonbayesian players. International Journal Game Theory, 9, 157{167.
Mertens, J.-F., Sorin, S., & Zamir, S. (1995). Repeated games, part a. CORE, DP-9420.
Milnor, J. (1954). Games Nature. Thrall, R. M., Coombs, C., & Davis, R.
(Eds.), Decision Processes. John Wiley & Sons.
Monahan, G. (1982). survey partially observable markov decision processes: Theory,
models algorithms. Management Science, 28, 1{16.
Papadimitriou, C., & Yannakakis, M. (1989). Shortest Paths Without Map. Automata,
Languages Programming. 16th International Colloquium Proceedings, pp. 610{
620.
Russell, S., & Norvig, P. (1995). Artificial Intelligence: Modern Approach. Prentice Hall.
Savage, L. (1972). Foundations Statistics. Dover Publications, New York.
Shapley, L. (1953). Stochastic games. Proceeding National Academic Sciences
(USA), 39, 1095{1100.
Smallwood, R., & Sondik, E. (1973). optimal control partially observable markov
processes finite horizon. Operations Research, 21, 1071{1088.
Sutton, R. (1992). Special issue reinforcement learning. Machine Learning, 8 (3{4).
Valiant, L. G. (1984). theory learnable. Comm. ACM, 27 (11), 1134{1142.
Watkins, C., & Dayan, P. (1992). Technical note: Q-learning. Machine Learning, 8 (3{4),
279{292.
Watkins, C. (1989). Learning Delayed Rewards. Ph.D. thesis, Cambridge University.
Wellman, M., & Doyle, J. (1992). Modular utility representation decision-theoretic
planning. Proceedings first international conference AI planning systems.
Morgan Kaufmann.
Wellman, M. (1985). Reasoning preference models. Tech. rep. MIT/LCS/TR-340,
Laboratory Computer Science, MIT.

248


