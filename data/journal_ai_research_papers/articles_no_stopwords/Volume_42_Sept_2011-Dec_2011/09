Journal Artificial Intelligence Research 42 (2011) 353-392

Submitted 5/11; published 11/11

Learning Make Predictions Partially Observable
Environments Without Generative Model
Erik Talvitie

erik.talvitie@fandm.edu

Mathematics Computer Science
Franklin Marshall College
Lancaster, PA 17604-3003, USA

Satinder Singh

baveja@umich.edu

Computer Science Engineering
University Michigan
Ann Arbor, MI 48109-2121, USA

Abstract
faced problem learning model high-dimensional environment,
common approach limit model make restricted set predictions, thereby
simplifying learning problem. partial models may directly useful making
decisions may combined together form complete, structured model. However, partially observable (non-Markov) environments, standard model-learning methods
learn generative models, i.e. models provide probability distribution possible futures (such POMDPs). straightforward restrict models make
certain predictions, always simplify learning problem.
paper present prediction profile models: non-generative partial models partially
observable systems make given set predictions, therefore far simpler
generative models cases. formalize problem learning prediction
profile model transformation original model-learning problem, show empirically one learn prediction profile models make small set important
predictions even systems complex standard generative models.

1. Introduction
Learning model dynamics environment experience critical capability artificial agent. Agents learn make predictions future events
anticipate consequences actions use predictions plan
make better decisions. agents environment complex, however, learning problem pose serious challenges. One common approach dealing complex
environments learn partial models, focusing model-learning problem making
restricted set particularly important predictions. Often predictions
need made, much complexity dynamics modeled safely ignored. Sometimes partial model directly useful making decisions, instance
model makes predictions agents future rewards (e.g., see McCallum, 1995;
Mahmud, 2010). cases, many partial models making restricted predictions
combined form complete model in, instance, factored MDPs (Boutilier,
Dean, & Hanks, 1999), factored PSRs (Wolfe, James, & Singh, 2008), collections
local models (Talvitie & Singh, 2009b).
c
2011
AI Access Foundation. rights reserved.

fiTalvitie & Singh

common approach learning partial model apply abstraction
(whether learned supplied domain expert) filters detail training data irrelevant making important predictions. Model-learning methods
applied abstract data, typically learning problem
tractable result. However, especially case partially observable systems, abstraction alone may sufficiently simplify learning problem, even (as see
subsequent examples) model asked make intuitively simple predictions.
counter-intuitive complexity learning partial model partially observable case
direct result fact standard model-learning approaches partially observable systems learn generative models attempt make every possible prediction
future cannot straightforwardly restricted making particularly
important predictions.
paper present alternative approach learns non-generative models
make specified predictions, conditioned history. following illustrative
example, see sometimes small set predictions necessary
good control performance learning make predictions high-dimensional
environment using standard generative models pose serious challenges. contrast
see exists simple, non-generative model make maintain
predictions form learning target method.
1.1 Example
Consider simple game Three Card Monte. dealer, perhaps crowded street,
three cards, one ace. dealer shows location ace, flips
cards, mixes swapping two cards every time step. player
game must keep track location ace. Eventually dealer stops mixing
cards asks guess. player correctly guesses ace is, win
money. guess wrong, lose money.
Consider artificial agent attempting learn model dynamics game
experience. takes sequence actions perceives sequence observations.
raw data received agent includes rich, high-dimensional scene including
activities crowd, movement cars, weather, well game (the
dealer swapping cards). Clearly, learning model encompasses complex
phenomena infeasible unnecessary. order win game, agent needs
focus making predictions cards, need anticipate future behavior city scene around it. particular, agent need make three predictions:
flip card 1, ace? corresponding predictions cards 2
3. One safely ignore much detail agents experience still make
important predictions accurately. one filters irrelevant detail, agents
experience might look this:
bet pos2 watch swap1, 2 watch swap2, 3 . . . ,
agent takes bet action, starting game, observes dealer showing
card position 2. agent takes watch action, observes dealer swapping
cards 1 2, takes watch action again, observes dealer swapping cards 2 3,
354

fiLearning Make Predictions Without Generative Model

dealer prompts agent guess (note uncontrolled
system; watch indeed action agent must select over, say, reaching
flipping cards itself, real game Three Card Monte would certainly result
negative utility!) data reflects movement cards. One could learn
model using new data set learning problem would far simpler
since complex irrelevant phenomena crowd weather ignored.
Markov case, agent directly observes entire state environment
therefore learn make predictions direct function state. Abstraction simplifies
representation state thereby simplifies learning problem. Note, however,
Three Card Monte problem partially observable (non-Markov). agent cannot
directly observe state environment (the location ace state
dealers mind hidden agent). partially observable case, agent
must learn maintain compact representation state well learn dynamics
state. common methods achieve this, expectation-maximization
(EM) learning POMDPs (Baum, Petrie, Soules, & Weiss, 1970), learn generative models
provide probability distribution possible futures.
Three Card Monte, even irrelevant details ignored data
contains information cards movement, generative model still intractably complex! generative model makes predictions future events.
includes predictions model meant make (such whether flipping card 1
next time-step reveal ace) many irrelevant predictions. generative
model, predict, instance, whether flipping card 1 10 time-steps
reveal ace whether cards 1 2 swapped next time-step. make
predictions, model must capture dynamics cards
dealers decision-making process. dealer decides cards swap using
complex process (as human dealer might) problem learning generative model
abstract system correspondingly complex.
course, Three Card Monte, predicting dealers future behavior entirely
unnecessary win. required maintain aces current location time.
such, learning model devotes complexity anticipating dealers
decisions counter-intuitive best. far reasonable model seen Figure 1.
states model labeled predictions aces location.
transitions labeled observations dealers behavior. agent plays
game, could use model maintain predictions location ace
time, taking dealers behavior account, predicting dealers future
behavior. Note non-generative model. provide distribution
possible futures cannot used simulate world
predict dealers next move. provides limited set conditional predictions
future, given history past actions observations. hand,
far simpler generative model would be. model dealers
decision-making process, model 3 states, regardless underlying process
used dealer.
model Figure 1 example term prediction profile model.
paper formalize prediction profile models present algorithm learning
data, assumptions (to specified established necessary
355

fiTalvitie & Singh

Figure 1: Maintaining predictions location ace Three Card Monte. Transitions labeled dealers swaps. States labeled predicted
position special card.

terminology). empirically demonstrate partially observable systems
prove complex standard generative model-learning methods, possible
learn prediction profile model makes small set important predictions allow
agent make good decisions. next sections formally describe setting
establish notation terminology formalize general learning problem
addressed. Subsequent sections formally present prediction profile models
algorithm learning them, well several relevant theoretical empirical results.
1.2 Discrete Dynamical Systems
focus discrete dynamical systems. agent finite set actions
take environment finite set observations produce. every
time step i, agent chooses action ai environment stochastically emits
observation oi O.
Definition 1. time step i, sequence past actions observations since
beginning time hi = a1 o1 a2 o2 . . . ai oi called history time i.
history time zero, agent taken actions seen observations
h0 , called null history.
1.2.1 Predictions
agent uses model make conditional predictions future events, given history actions observations given future behavior. environment
assumed stochastic, predictions probabilities future events. primitive
building block used describe future events called test (after Rivest & Schapire, 1994;
Littman, Sutton, & Singh, 2002). test simply sequence actions observations
356

fiLearning Make Predictions Without Generative Model

could possibly occur, = a1 o1 . . . ak ok . agent actually takes action sequence
observes observation sequence t, say test succeeded. prediction
p(t | h) probability test succeeds history h, assuming agent takes
actions test. Essentially, prediction test answer question
take particular sequence actions, probability would see
particular sequence observations, given history far? Formally,
def

p(t | h) = Pr(o1 | h, a1 )Pr(o2 | ha1 o1 , a2 ) . . . Pr(ok | ha1 o1 a2 o2 . . . ak1 ok1 , ak ).

(1)

Let set tests (that is, set possible action-observation sequences
lengths). set possible histories H set action-observation
sequences could possibly occur starting null history, null history itself:
def
H = {t | p(t | h0 ) > 0} {h0 }.
model make prediction p(t | h) h H make
conditional prediction future (Littman et al., 2002). represents
probability distribution futures, model used sample
distribution order simulate world, sample possible future trajectories.
such, call model makes predictions generative model.
Note use word generative closely related broader sense
general density estimation. one attempting represent conditional probability
distribution Pr(A | B), generative approach would represent full joint distribution Pr(A, B) conditional probabilities computed Pr(A,B)
Pr(B) .
say, generative model sense makes predictions even variables wish
condition on. non-generative or, settings, discriminitive approach would
instead directly represent conditional distribution, taking value B un-modeled
input. non-generative approach sometimes result significant savings Pr(B)
difficult represent/learn, Pr(A | B) relatively simple (so long one truly
disinterested modeling joint distribution).
particular setting, generative model one provides probability distribution futures (given agents actions). such, one would use generative model
0)
. fact, Equation 1 one
compute p(t | h) particular h p(ht|h
p(h|h0 )
see prediction multi-step test computed predictions
one-step tests:
p(a1 o1 a2 o2 . . . ak ok | h) = p(a1 o1 | h)p(a2 o2 | ha1 o1 ) . . . p(ak ok | ha1 o1 a2 o2 . . . ak ok ).
leads simple definition generative model:
Definition 2. model provide predictions p(ao | h) actions A,
observations histories h H generative model.
non-generative model, then, would make one-step predictions histories
and, consequently, would directly represent prediction p(t | h) history h
un-modeled input. would condition given history, necessarily capable
computing probability history sequence. saw Three Card Monte
example, beneficial making maintaining predictions substantially
simpler making predictions every possible action-observation sequence.
357

fiTalvitie & Singh

Note test describes specific future event (a sequence specific actions
observations). many cases one might wish make predictions abstract
events. achieved composing predictions many tests. instance set
tests (Wingate, Soni, Wolfe, & Singh, 2007) sequence actions set observation
sequences. set test succeeds agent takes specified action sequence sees
observation sequence contained within set occur. traditional tests allow
agent, instance express question go outside, probability see
exact sequence images? set test express far useful, abstract question
go outside, probability sunny? grouping together
observations sunny day. Even generally, option tests (Wolfe & Singh, 2006; Soni
& Singh, 2007) express future events agents behavior described abstractly
well resulting observations. types abstract predictions computed
linear combination set concrete predictions.
1.2.2 System Dynamics Matrix Linear Dimension
sometimes useful describe dynamical system using conceptual object called
system dynamics matrix (Singh, James, & Rudary, 2004). system dynamics matrix
contains values possible predictions, therefore fully encodes dynamics
system. Specifically,
Definition 3. system dynamics matrix dynamical system infinity-by-infinity
matrix. column corresponding every test . row corresponding
every history h H. ijth entry system dynamics matrix prediction
p(tj | hi ) test corresponding column j history corresponding row
entry every history-test pair.
Though system dynamics matrix infinitely many entries, many cases
finite rank. rank system dynamics matrix thought measure
complexity system (Singh et al., 2004).
Definition 4. linear dimension dynamical system rank corresponding
system dynamics matrix.
popular modeling representations, linear dimension major factor
complexity representing learning generative model system. instance,
POMDPs, number hidden states required represent system lower-bounded
linear dimension. work adopt linear dimension measure
complexity dynamical system. say system simpler another,
mean lower linear dimension.
1.2.3 Markov Property
dynamical system Markov one needs know history order make
predictions future events recent observation.
Definition 5. system Markov two histories h h (that may null
history), two actions , observation o, test t, p(t | hao) = p(t | h o).
358

fiLearning Make Predictions Without Generative Model

Markov case use notational shorthand p(t | o) indicate prediction
history ends observation o. Markov case, observations
contain information needed make prediction future, often
called state (because describe state world). system Markov,
partially observable. partially observable systems predictions depend arbitrarily
entire history. focus partially observable case.

2. Learning Make Predictions
work assume that, Three Card Monte, though agent may live
complex environment, small set important predictions make.
predictions could identified important designer, learning
process. address problem identifying predictions made,
rather focus problem learning make predictions, identified.
general, imagine given finite set = {t1 , t2 , . . . , tm } tests
interest would model make accurate predictions. term
test construed broadly, possibly including abstract tests addition raw
sequences actions observations. tests interest future events model
predict. instance, Three Card Monte problem, order perform well
agent must predict whether see ace flips card.
three one-step tests interest: f lip1 ace, f lip2 ace, f lip3 ace (representing
future events agent flips card 1, 2, 3, respectively, sees ace).
agent learn maintain probability events time, win
game.
such, general problem learn function : H [0, 1]m
def

(h) = hp(t1 | h), p(t2 | h), . . . , p(tm | h)i,

(2)

is, function histories predictions test interest (which refer
predictions interest) history. Note output necessarily
probability distribution. tests interest may selected arbitrarily therefore
need represent mutually exclusive exhaustive events. call particular vector
predictions tests interest prediction profile.
Definition 6. call (h) prediction profile history h.
describe two existing general approaches learning : learning direct function history predictions (most common Markov case), learning fully
generative model maintains finite-dimensional summary history (common
partially observable case). strengths weaknesses approaches learning
. Section 2.3 contrast approach, combines strengths
approaches.
2.1 Direct Function Approximation
system Markov, learning conceptually straightforward; essentially
problem learning function observation (state) predictions. Rather
359

fiTalvitie & Singh

learning takes histories input, one instead learn function arkov :
[0, 1]m , maps observation predictions tests interest resulting
histories end observation. Note that, immediate consequence,
discrete Markov systems finite number distinct prediction profiles. fact,
distinct prediction profiles observations.
number observations number tests interest small enough,
arkov represented |O| |T | look-up table, entries estimated using
sample averages1 :
p(ti | o) =

# times succeeds histories ending
.
# times acts(t) taken histories ending

(3)

main challenge learning Markov models arises number observations
large. becomes necessary generalize across observations, using data gathered
one observation learn many others. Specifically, one may able exploit
fact observations associated similar (or identical) prediction
profiles (that is, predictions tests interest) share data amongst them.
Restricting models attention predictions afford generalization,
learning partial model beneficial Markov setting.
Even system partially observable, one still attempt learn directly,
typically performing sort regression set features entire histories.
instance, U-Tree (McCallum, 1995) takes set history features learns decision tree
attempts distinguish histories result different expected asymptotic return
optimal behavior. Wolfe Barto (2006) apply U-Tree-like algorithm rather
restricting model predicting future rewards, learn make predictions
pre-selected set features next observation (a special case
general concept tests interest). Dinculescu Precup (2010) learn expected value
given feature future direct function given real-valued feature history
clustering futures histories similar associated values.
directly approximate types models make predictions
therefore non-generative (and therefore able, instance, avoid falling
trap predicting dealers decisions Three Card Monte). Though approach
demonstrated promise, faces clear pragmatic challenge, especially partially
observable setting: feature selection. function history, ever-expanding
sequence actions observations, finding reasonable set compactly represented features collectively capture history information needed make predictions
interest significant challenge. sense, even partially observable setting,
type approach takes small step away Markov case. still requires
good idea priori information extracted history (in form
features) order make predictions interest.
1. Bowling, McCracken, James, Neufeld, Wilkinson (2006) showed estimator unbiased
case data collected using blind policy, action selection depend
history observations provided alternative estimator unbiased policies.
simplicitys sake, however, assume throughout data gathering policy blind.

360

fiLearning Make Predictions Without Generative Model

2.2 Generative Models
one good idea priori features extracted history
make accurate predictions, one faces additional challenge learning summarize
relevant information history compact sufficient statistic.
exist methods learn training data maintain finite-dimensional
statistic history prediction computed. analogy Markov
case, statistic called state vector. Clearly model maintain state
used compute (since make predictions). briefly mention two
examples approach particularly relevant development analysis
method.
POMDPs far popular representation models partially observable
systems partially observable Markov decision process (POMDP) (Monahan, 1982).
POMDP posits underlying MDP (Puterman, 1994) set hidden states
agent never observes. given time-step i, system particular hidden
state si1 (unknown agent). agent takes action ai system
transitions next state si according transition probability Pr(si | si1 , ai ).
observation oi emitted according probability distribution general
may depend upon si1 , ai , si : Pr(oi | si1 , ai , si ).
agent observe hidden states, cannot know hidden
state system given moment. agent however maintain probability
distribution represents agents current beliefs hidden state. probability distribution called belief state. belief state associated history h
known, straightforward compute prediction test t:
X
p(t | h) =
Pr(s | h)Pr(t | s),
sS

Pr(t | s) computed using transition observation emission probabilities.
belief state finite summary history prediction
future computed. So, belief state state vector POMDP. Given
transition probabilities observation emission probabilities, possible maintain
belief state time using Bayes rule. current history h one knows Pr(s | h)
hidden states agent takes action observes observation o, one
compute probability hidden state new history:
P



Pr(s | h)Pr(s | , ai )Pr(oi | , ai , s)
P
Pr(s | hao) = P
.
(4)






Pr(s | h)Pr(s | , ai )Pr(oi | , ai , )
parameters POMDP must learned order able maintain
state transition probabilities observation emission probabilities. Given
parameters, belief state corresponding given history recursively computed
model thereby make prediction history. POMDP parameters
typically learned using Expectation Maximization (EM) algorithm (Baum et al., 1970).
Given training data number actions, observations, hidden states
input, EM essentially performs gradient ascent find transition emission distributions
(locally) maximize likelihood provided data.
361

fiTalvitie & Singh

PSRs Another recently introduced modeling representation predictive state
representation (PSR) (Littman et al., 2002). Instead hidden states, PSRs defined
directly terms system dynamics matrix (described Section 1.2.2). Specifically,
PSRs find set core tests Q whose corresponding columns system dynamics matrix
form basis. Recall system dynamics matrix often finite rank (for instance,
matrix associated POMDP finite hidden states finite linear dimension)
thus Q finite many systems interest. Since predictions Q basis,
prediction test history computed linear combination
predictions Q history.
vector predictions Q called predictive state. belief state
state vector POMDPs, predictive state state vector PSRs.
maintained application Bayes rule. Specifically, history h, p(q | h)
known core tests q agent takes action observes
observation O, one compute prediction core test q new
history:
P


p(aoq | h)
q Q p(q | h)maoq (q )
P
p(q | hao) =
=
,
(5)


p(ao | h)
q Q p(q | h)mao (q )
maoq (q ) coefficient p(q | h) linear combination computes
prediction p(aoq | h).
So, given set core tests, parameters PSR must learned order
maintain state coefficients mao every action observation
coefficients maoq every action a, observation o, core tests q. Given parameters
predictive state given history recursively computed used make
prediction future. PSRs learned directly estimating system dynamics matrix (James & Singh, 2004; Wolfe, James, & Singh, 2005) or, recently,
sub-matrix derived matrix thereof (Boots, Siddiqi, & Gordon, 2010, 2011) using sample
averages training data. estimated matrix used find set core tests
parameters estimated using linear regression.
Note types models inherently generative. rely
upon maintenance state vector order make predictions and,
seen Equations 4 5, state update equations models rely upon access
one-step predictions perform Bayesian update. such, unlike direct function
approximation approach, one cannot simply choose set predictions model
make. models necessity make predictions.
many reasons desire complete, generative model. makes
possible predictions, model used sample possible future trajectories
useful capability planning. generative model also, definition, flexible
predictions used make. hand, many cases complete,
generative model may difficult obtain. PSR POMDP training methods scale
poorly linear dimension system learned. linear dimension
lower-bounds number hidden states needed represent system POMDP
precisely number core tests needed represent PSR. learning methods
POMDPs PSRs rarely successfully applied systems linear dimension
362

fiLearning Make Predictions Without Generative Model

Figure 2: Size 10 1D Ball Bounce
hundred (though work Boots et al. pushing limits further).
systems interest several orders magnitude higher linear dimension.
Furthermore, complete, generative model overkill problem hand. Recall
seek make predictions; focused making particularly
important predictions . Even problems learning make predictions might
intractable, still possible make simple important predictions.
2.2.1 Abstract Generative Models
discussed earlier, restricted set tests interest, learning problem
often simplified ignoring irrelevant details abstraction. course,
abstraction solve problem partial observability. typically done
apply abstraction training data, discarding irrelevant details (as
Three Card Monte example) apply model learning methods
ones described abstract data set. Markov setting, cases
observation abstraction greatly simplify learning problem (certainly learning
cards Three Card Monte easier learning cards crowd
weather on).
Ignoring details irrelevant making predictions interest intuitive
significantly simplify learning problem. hand, generative
models, abstract POMDP PSR still make abstract predictions. typically
includes predictions directly interest. extra predictions
require complex model, even abstract generative model intractible learn.
true Three Card Monte example (where generative model ends modeling
dealer well cards). following another simple example phenomenon.
Example. Consider uncontrolled system pictured Figure 2, called 1D Ball
Bounce system. agent observes strip pixels black white.
black pixel represents position ball moves around strip. ball
current direction every time-step moves one pixel direction. Whenever
reaches edge pixel, current direction changes move away edge. Figure
3(a) complete POMDP model 10 pixel version system pictured.
k pixels, POMDP 2k 2 hidden states (because ball one 2
possible directions one k possible positions, except two ends,
one possible direction).
say agent wishes predict whether ball position marked
x next time step. Clearly prediction made paying
attention immediate neighborhood x. details happens
ball far away matter making predictions. So, one could apply
363

fiTalvitie & Singh

(a)

(b)

Figure 3: POMDP model size 10 1D Ball Bounce system (a) abstracted
1D Ball Bounce system (b).

abstraction lumps together observations neighborhood x
looks same. problem abstract generative model system makes
predictions x, pixels surrounding x. Specifically,
model still makes predictions whether ball enter neighborhood near
future. course depends long since ball left neighborhood.
So, POMDP model abstract system (pictured Figure 3(b)) exactly
state diagram original system, though observations changed reflect
abstraction. abstract system primitive system linear dimension.
order make predictions x, one must condition information
pixels surrounding x. Consequently, generative model makes predictions
pixels. Counterintuitively, abstract models complexity mainly devoted making
predictions predictions interest. general, learning abstract
model drastically simplify learning problem ignoring irrelevant details, abstract generative model still learns make predictions details relevant,
even directly interest.

2.3 Prediction Profile Models
contribution paper, prediction profile models, seek combine main strengths
two model-learning approaches discussed above. direct approximation
, prediction profile model make predictions interest, others.
such, far simpler generative model, typically make many
extraneous predictions. However, learning method prediction profile models
require set history features given priori. leveraging existing generative
model learning methods, prediction profile models learn maintain state information
necessary making predictions interest.
364

fiLearning Make Predictions Without Generative Model

Figure 4: Prediction profile model 1D Ball Bounce system
typical model learns make predictions future observations emitted
system. main idea behind prediction profile models instead model values
predictions change time, conditioned actions chosen
agent observations emitted system.
already seen example Three Card Monte. prediction profile
model (shown Figure 1) takes observations dealers behavior input outputs
predictions tests interest. predict dealers behavior, takes
account updating predictions interest. Recall that, though Three Card
Monte system arbitrarily complicated (depending dealer), prediction
profile system three states, regardless dealers decision making process.
Another example shown Figure 4. prediction profile system 1D
Ball Bounce system (Figure 2), model must predict whether ball enter
position x next time-step. state prediction profile model labeled
prediction pixel x (white black). transitions labeled observations
3-pixel neighborhood centered position x. case transitions capture ball
entering neighborhood, moving position x, leaving neighborhood, staying away
undetermined amount time, returning again. Recall POMDP model
system 2k 2 hidden states, k number pixels, even ignoring
pixels irrelevant making predictions pixel x. contrast, prediction profile
model always three states, regardless number pixels.
next section formally describe prediction profile models models dynamical system results transformation original system. Subsequent sections
discuss learn prediction profile models data (by converting data
original system data transformed system learning model converted data set) present results help characterize conditions
prediction profile models best applied.

3. Prediction Profile System
formally describe theoretical dynamical system, defined terms
dynamics original system given tests interest. call constructed
system prediction profile system. prediction profile model, goal
365

fiTalvitie & Singh

construct, model prediction profile system (that is, system ideal,
theoretical construct, model may imperfect, approximate, etc.). such, analysis
problem learning prediction profile model depend great deal understanding
properties prediction profile system.
paper make restrictive assumption that, Markov case,
finite number distinct prediction profiles (that is, predictions interest take
finite number distinct values). certainly true partially observable
systems sets tests interest, though true many interesting examples.
Formally, assumption requires map histories finite set prediction profiles:
Assumption 7. Assume exists finite set prediction profiles P = {1 , 2 , . . . , k }
[0, 1]m every history h, (h) P .
assumption allows definition prediction profile system (or P P short)
discrete dynamical system captures sequence prediction profiles time,
given action observation sequence. prediction profile systems actions, observations,
dynamics defined terms quantities associated original system:
Definition 8. prediction profile system defined set observations, set
actions, rule governing dynamics.
1. Observations: set prediction profile observations, OP P , defined set
def
distinct prediction profiles. is, OP P = P = {1 , . . . , k }.
2. Actions: set prediction profile actions, AP P , defined set actiondef
observation pairs original system. is, AP P = O.
3. Dynamics: dynamics prediction profile system deterministically governed . prediction profile history, ha1 , o1 i1 ha2 , o2 i2 . . . haj , oj ij ,
next P P -action, haj+1 , oj+1 i, prediction profile system deterministically
emits P P -observation (a1 o1 a2 o2 . . . aj oj aj+1 oj+1 ).
present key facts prediction profile system. Specifically,
noted prediction profile system always deterministic. Also, though
prediction profile system may Markov (as Three Card Monte example),
general partially observable.
Proposition 9. Even original system stochastic, prediction profile system
always deterministic.
Proof. follows immediately definition: every history corresponds exactly
one prediction profile. P P -history (action-observation-profile sequence) P P action (action-observation pair) fully determine next P P -observation (prediction profile). stochastic observations original system folded unmodeled actions prediction profile system.
Proposition 10. original system Markov, prediction profile system Markov.
366

fiLearning Make Predictions Without Generative Model

Proof. definition, original system Markov prediction profile time
step depends recent observation. So, time step t, current profile
, agent takes action at+1 observes observation ot+1 , next profile simply
t+1 = arkov (ot+1 ). So, fact, original system Markov, prediction profile
system satisfies even stronger condition: next P P -observation fully determined
P P -action dependence history whatsoever (including recent
P P -observation).
Proposition 11. Even original system partially observable, prediction profile
system may Markov.
Proof. Consider Three Card Monte example. original system clearly non-Markov
(the recent observation, dealers recent swap, tells one little
location ace). However, prediction profile system tests interest
regarding location special card (pictured Figure 1) Markov. next profile
fully determined current profile P P -action.
general, however, P P system may partially observable. Though Three
Card Monte example current prediction profile next action-observation pair
together fully determine next prediction profile, general next prediction profile
determined history action-observation pairs (and prediction profiles).
Proposition 12. prediction profile system may partially observable.
Proof. Recall 1D Ball Bounce example. corresponding prediction profile system
shown Figure 4. Note two distinct states update graph associated
prediction profile (pixel x white). Given current prediction profile
(pixel x white) P P -action (observe ball neighboring pixel left
right), one cannot determine whether ball entering leaving neighborhood,
thus cannot uniquely determine next profile. prediction profile system
partially observable.
So, general, prediction profile system deterministic, partially-observable dynamical system. model prediction profile system used make
predictions interest. such, one wishes use prediction profile model generative
model, one must select tests interest carefully. instance:
Proposition 13. tests interest include set one-step primitive tests,
{ao | A, O} , model prediction profile system used
generative model original system.
Proof. follows immediately definition generative model.
special case prediction profile model complete, generative
model system, shown Section 5 one desires generative model,
essentially never preferable learn prediction profile model traditional
representation. prediction profile model best applied relatively simple make
maintain predictions interest comparison making predictions. general,
367

fiTalvitie & Singh

Figure 5: Flow algorithm.
prediction profile model conditions observations, necessarily predict
next observation. such, model prediction profile system cannot typically
used purposes model-based planning/control generative model could.
experiments Section 6 demonstrate output prediction profile models can,
however, useful model-free control methods.

4. Learning Prediction Profile Model
definition prediction profile system straightforwardly suggests method
learning prediction profile models (estimate prediction profiles, learn model
dynamics using standard model-learning technique). section present
learning algorithm, discussing main practical challenges arise.
Let training data set trajectories experience original system (actionobservation sequences) let = {t1 , t2 , . . . , tk } set tests interest.
algorithm presented section learn model prediction profile system
data S. algorithm three main steps (pictured Figure 5). First training
data used estimate prediction profiles (both number unique profiles
values). Next, learned set prediction profiles used translate training data
trajectories experience prediction profile system. Finally, applicable model
learning method trained transformed data learn model prediction
profile system. Ultimately, experiments, learned prediction profile models
evaluated useful predictions features control.
4.1 Estimating Prediction Profiles
Given , first step learning prediction profile model determine
many distinct prediction profiles are, well values. estimated prediction
test interest history h is:
p(t | h) =

# times succeeds h
.
# times acts(t) taken h
def

(6)

One could, point, directly estimate letting (h) = hp(t1 | h), p(t2 | h), . . . , p(tk |
h)i. course, due sampling error, unlikely estimated profiles
exactly same, even true underlying prediction profiles identical. So,
368

fiLearning Make Predictions Without Generative Model

estimate number distinct underlying profiles, statistical tests used find
histories significantly different prediction profiles.
compare profiles two histories, likelihood-ratio test homogeneity performed counts test interest two histories. statistical test
associated test interest rejects null hypothesis prediction
histories, two histories different prediction profiles.
order find set distinct prediction profiles, greedily cluster estimated
prediction profiles. Specifically, initially empty set exemplar histories maintained.
algorithm searches histories agents experience, comparing historys
estimated profile exemplar histories estimated profiles. candidate historys
profile significantly different profiles exemplar histories, candidate
added new exemplar. end, estimated profiles corresponding exemplar
histories used set prediction profiles. order obtain best estimates
possible, search ordered prioritize histories lots associated data.
prediction profile estimation procedure two main sources complexity.
first sample complexity estimating prediction profiles. take great
deal exploration see history enough times obtain good statistics, especially
number actions observations large. issue could addressed adding
generalization estimation procedure, data one sample trajectory could
improve estimates many similar histories. one experiments Section 6,
observation abstraction employed simple form generalization. second
bottleneck computational complexity searching prediction profiles, involves exhaustively enumerating histories agents experience. would valuable
develop heuristics identify histories likely provide new profiles, order
avoid searching histories. experiments Section 6, simple heuristic
limiting search short histories employed. Long histories tend less
associated data, therefore less likely provide distinguishably new profiles.
4.2 Generating Prediction Profile Trajectories
generated finite set distinct prediction profiles, next step translate
agents experience sequences action-observation pairs prediction profiles.
trajectories used train model prediction profile system.
process translating action-observation sequence prediction profile trajectory straightforward and, apart practical concerns, follows directly
Definition 8. Recall that, action-observation sequence = a1 o1 a2 o2 . . . ak ok , corresponding P P -action sequence ha1 , o1 iha2 , o2 . . . hak , ok i. corresponding sequence
profiles (a1 o1 )(a1 o1 a2 o2 ) . . . (a1 o1 . . . ak ok ). Thus, principle, every primitive actionobservation sequence translated action-observation-profile sequence.
course available generate sequence prediction profiles. So,
necessary use approximation , generated training data. Specifically,
estimated predictions tests interest history h (computed using Equation
6) compared, using statistical tests, set distinct estimated prediction profiles
Section 4.1. one estimated profile statistically significantly
different estimated predictions h, let (h) = .
369

fiTalvitie & Singh

Given sufficient data, statistical tests uniquely identify correct match
high probability. practice, however, histories much associated
data. possible case test homogeneity fail reject null
hypothesis two profiles. indicates enough data distinguish multiple possible matches. experiments Section 6, two different
heuristic strategies handling situation employed. first strategy lets (h)
matching profile smallest empirical KL-Divergence estimated
predictions (summed tests interest). heuristic choice may lead
noise prediction profile labeling, could turn affect accuracy learned
model. second strategy simply cut trajectory point multiple
matches occur, rather risk assigning incorrect labeling. ensures labels
appear prediction profile trajectories reasonable level confidence
correctness. However, wasteful throw training data way.
4.3 Learning Prediction Profile Model
translation step produces set trajectories interaction prediction
profile system. Recall prediction profile system deterministic, partially observable, discrete dynamical system trajectories used train model
prediction profile system using, principle, applicable model-learning method.
issue faced models prediction profile system present
usual discrete dynamical systems modeling setting. prediction profile labels
present training data, actually using model available. Say
current history h, action a1 taken observation o1 emitted. Together,
action-observation pair constitutes P P -action. model prediction profile
system, prediction profile model identify next profile, . profile used
compute predictions p(t | ha1 o1 ) tests interest history ha1 o1 .
another action a2 observation o2 occur. necessary update PP-models
state order obtain next prediction profile. typical dynamical systems model
makes predictions next observation, able update state
actual observation occurred. prediction profile models observations prediction
profiles themselves, observable interacting world. such,
prediction profile model update state prediction profile predicted
(). updated, prediction profile model obtain profile follows ha2 , o2
gives predictions tests interest new history ha1 o1 a2 o2 .
prediction profile model perfect model prediction profile system,
poses problems. prediction profile system deterministic, need
observe true prediction profile label; fully determined history. practice,
course, model imperfect different modeling representations require
different considerations performing two functions providing predictions
tests interest, providing profile sake updating model.
4.3.1 PP-POMDPs
Since prediction profile system partially observable natural model using POMDP. Unfortunately, even training data deterministic sys370

fiLearning Make Predictions Without Generative Model

tem, POMDP training using EM algorithm generally provide deterministic
POMDP. Thus, given history, learned POMDP model prediction profile
system (PP-POMDP) provide distribution prediction profiles instead deterministically providing one profile associated history. implementation
used Section 6 simply takes likely profile distribution profile
associated history uses make predictions tests interest, well
update POMDP model.
4.3.2 PP-LPSTs
Another natural choice representation prediction profile model looping predictive
suffix tree (LPST) (Holmes & Isbell, 2006). LPSTs specialized deterministic, partially
observable systems. such, could used model original system (which
assumed stochastic general), apply prediction profile system
(and determinized POMDP).
Briefly, LPST captures parts recent history relevant predicting next
observation. Every node tree corresponds action-observation pair. node
may leaf, may children, may loop one ancestors. Every leaf
tree corresponds history suffix deterministic prediction observation
every action. order predict next observation particular history, one
reads history reverse order, following corresponding links tree leaf
reached, gives prediction. Holmes Isbell provide learning algorithm that,
certain conditions training data, guaranteed produce optimal tree.
reader referred work Holmes Isbell (2006) details.
One weakness LPSTs, however, fail make prediction next
observation current history lead leaf node tree (or leaf
node reached prediction action queried). typically occurs
history suffixes occur training data occur using
model. PP-LPST, mean histories model cannot uniquely
determine corresponding prediction profile. happens implementation
used Section 6 simply finds longest suffix current history occur
data. suffix associated multiple prediction profiles (otherwise LPST
would provided prediction). make predictions tests interest, model
provides average prediction set profiles. profile used update
model picked set uniformly randomly.
4.3.3 PP-PSRs
Applying PSR learning algorithms prediction profile data poses practical concern.
Specifically, methods attempt estimate system dynamics matrix (James & Singh,
2004; Wolfe et al., 2005) implicitly presume every action sequence could principle
taken every history. action sequences taken histories
others, matrix undefined entries. poses challenges rank
estimation (and, indeed, definition model representation). Unfortunately,
case prediction profile system since P P -actions (action-observation
pairs) completely agents control; partly selected environ371

fiTalvitie & Singh

ment itself. recent spectral learning algorithms presented Boots et al. (2010) may
able side-step issue, flexibility selecting predictions
estimated use model-learning process, though investigated
possibility work.
Note that, though method learning prediction profile model involves standard
model-learning methods partially observable environments, result generative
model original system. prediction profile model generative model
prediction profile system and, such, cannot used make predictions
original system, predictions interest.

5. Complexity Prediction Profile System
learning algorithm presented evaluated empirically Section 6. First,
however, analyze complexity prediction profile system relation complexity original system. give indication difficult learn
prediction profile model provide insight appropriate learn prediction
profile model typical generative model approach.
many factors affect complexity learning model. section
largely focus linear dimension measure complexity, taking view that,
generally speaking, systems lower linear dimension easier learn systems
larger linear dimension. discussed Section 1.2.2, generally true POMDPs,
linear dimension lower-bounds number hidden states. comparing
linear dimension prediction profile system original system give
idea whether would easier learn PP-POMDP learn standard
POMDP original system. course, model-learning methods
complexity measures would appropriate (for instance known
precisely LPSTs interact linear dimension). Extending results
measures complexity may interesting topic future investigation.
5.1 Linear Dimension Comparison
section discuss linear dimension prediction profile system relates
original system. first result proof concept simply states
exist problems prediction profile system vastly simple
original system. fact, problem already presented.
Proposition 14. prediction profile system linear dimension arbitrarily
lower original system.
Proof. Recall Three Card Monte example. Thus far domain described
without describing dealers behavior. However, note prediction profile system
tests interest relating location special card (pictured Figure 1)
linear dimension 3, regardless dealers swaps chosen. complex
dealer chosen, original system high linear dimension, prediction
profile systems linear dimension remain constant. instance, experiments
Section 6, dealer chooses cards swap stochastically, likely choose
372

fiLearning Make Predictions Without Generative Model

swap selected least often far. Thus, order predict dealers
next decision, one must count many times swap chosen history
result system effectively infinite linear dimension.
hand, prediction profile models panacea. following results
indicate problems learning prediction profile model would
advisable learning standard generative model, linear dimension
prediction profile system far greater original system. Later
section special cases characterized prediction profile models likely
useful. next result shows linear dimension prediction profile model
infinite original system finite linear dimension, via lower bound
linear dimension true deterministic dynamical systems.
Proposition 15. deterministic dynamical system actions A, observations
O, linear dimension, n log(|A|1)+log(|O|+1)
.
log |A|
Proof. See Appendix A.1.
Proposition 15 applies deterministic dynamical systems, certainly applies prediction profile system. Though loose bound, basic implication
number prediction profiles (the observations P P ) increases comparison number action-observation pairs (the actions P P ), linear dimension
prediction profile system necessarily increases. bound clearly illustrates
importance assumption finite number distinct prediction profiles.
Corollary 16. infinitely many distinct prediction profiles, prediction profile
system infinite linear dimension.
Proof. Clearly |AP P | = |A O| finite long finitely many actions
observations. So, last result follows immediately number distinct
prediction profiles |OP P | approaches infinity, must linear dimension
prediction profile system.
Hence, long prediction profile models represented using methods rely
finite linear dimension, critical finitely many prediction profiles.
Note fundamental barrier, side effect representational choice.
Model learning methods sensitive linear dimension (such designed
model continuous dynamical systems) may able effectively capture systems
infinitely many prediction profiles.
One conclusion drawn last results knowing linear dimension
original system not, itself, necessarily say much complexity
prediction profile system. prediction profile system may far simpler far
complex original system. Thus may informative turn factors
trying characterize complexity prediction profile system.
373

fiTalvitie & Singh

5.2 Bounding Complexity Prediction Profile System
results previous section take account obviously important aspect
prediction profile system: predictions asked make. predictions
interest made simply keeping track little information.
predictions rely great deal history information therefore require
complex model. next result identifies worst case set tests interest
system: tests interest whose corresponding prediction profile model highest
linear dimension. Ultimately section present (non-exhaustive) conditions
prediction profile system likely simpler original system.
Proposition 17. given system set tests interest, linear dimension
corresponding prediction profile system greater prediction profile
system associated set core tests system (as described Section 2.2).
Proof. See Appendix A.2.
worst case identified, one immediately obtain bounds complex
prediction profile system possibly be.
Corollary 18. system set tests interest, corresponding prediction
profile system linear dimension greater number distinct predictive states
original system.
Proof. prediction profile system set core tests Q deterministic MDP
observations prediction profiles Q (that is, predictive states). is, state
associated unique prediction profile. linear dimension MDP never
greater number observations (Singh et al., 2004). Therefore, previous
result prediction profile system set tests interest linear dimension
greater number predictive states.
Corollary 19. original system POMDP, prediction profile system set
tests interest linear dimension greater number distinct belief states.
Proof. follows immediately previous result fact number
distinct predictive states greater number distinct belief states (Littman
et al., 2002).
bounds presented far help explain prediction profile system
complex original system. However, focused worst possible
choice tests interest, little illuminate opposite true. prediction
profile model complex asked perform task generative
model: keep track much information history necessary make possible
predictions (or equivalently, predictive state belief state). results indicate
that, generally speaking, one desires generative model, standard approaches would
preferable learning prediction profile model.
hand, stated goal learn generative model, instead
focus particular predictions hopefully far simpler make
predictions. examples seen make clear cases, predictions
374

fiLearning Make Predictions Without Generative Model

made prediction profile model far simpler generative model original
system. general one might expect prediction profile model simple
predictions interest rely small amount state information required
maintain generative model. next bound aligns intuitive reasoning.
Essentially result points often much hidden state information
POMDP irrelevant predictions interest. linear dimension
prediction profile system bounded number distinct beliefs relevant
parts hidden state, rather number distinct beliefs states overall. idea
result one impose abstraction hidden states POMDP
(not observations) still allows predictions interest made accurately
allows abstract belief states computed accurately, prediction profile
systems linear dimension bounded number abstract belief states.
Proposition 20. Consider POMDP hidden states S, actions A, observations
O. Let set tests interest. Let ai action taken time-step i, si
hidden state reached taking action ai , oi observation emitted si . Now,
consider surjection : mapping hidden states set abstract states
following properties:
1. pair primitive states s1 , s2 S, (s1 ) = (s2 ), time-step
test interest , p(t | si = s1 ) = p(t | si = s2 ).
2. pair primitive states s1 , s2 S, (s1 ) = (s2 ), time-step i,
abstract state , observation O, action A,
Pr((si+1 ) = | si = s1 ,ai+1 = a, oi+1 = o) =
Pr((si+1 ) = | si = s2 , ai+1 = a, oi+1 = o).
, prediction profile system linear dimension greater
number distinct beliefs abstract states, .
Proof. See Appendix A.3
things note result. First, surjection always exists
def
properties 1 2. One always define : (s) = s. degenerate
case trivially satisfies requirements Proposition 20 recovers bound given
Corollary 19. However, Proposition 20 applies surjections satisfy conditions.
must surjection satisfies conditions results smallest number
beliefs abstract states. Essentially, one ignores much state information
possible still allowing predictions interest made accurately
surjection tightly bounds complexity prediction profile system (even
known).
course, may still large even infinite number distinct beliefs, even
abstract states, factors must come play ensure simple prediction profile
system. Furthermore, result characterize settings prediction
profile system simple. said, result support intuition
375

fiTalvitie & Singh

prediction profile system tend simple predictions asked make
depend small amounts state information.
order build intuition result relates earlier examples, recall
Three Card Monte problem. Three Card Monte two sources hidden state:
aces unobserved position whatever hidden mechanism dealer uses make
decisions. Clearly agents predictions interest depend first part
hidden state. So, case one satisfy Property 1 surjection maps
two hidden states abstract state ace position, regardless
dealers state. 3 abstract states (one possible
position), even though might infinitely many true hidden states. Now, different
states corresponding ace position different distributions aces
next position; distribution does, all, depend upon dealers state. However,
Property 2 statement distribution next abstract state given
observation emitted entering abstract state. one knows current
abstract state observes dealer does, next abstract state fully determined.
Property 2 holds well. fact, since aces position known beginning
game, means current abstract state always known absolute certainty, even
though beliefs dealers state general uncertain. Hence, 3
distinct beliefs abstract states (one state). such, prediction profile
models linear dimension upper-bounded 3, regardless dealers complexity (and
case bound met).
5.3 Bounding Number Prediction Profiles
previous section describes conditions prediction profile system
may lower linear dimension original system. concern number
prediction profiles, whether number finite. section briefly discuss
(non-exhaustive) cases number prediction profiles bounded.
One case already discussed original system Markov.
case number prediction profiles bounded number observations (states).
course, original system Markov, little need use prediction profile
models. Another, similar case system partially observable, completely
deterministic (that is, next observation completely determined history
selected action). system deterministic POMDP given history
current hidden state known. such, number belief states bounded
number hidden states. Since cannot prediction profiles belief states,
number prediction profiles bounded well.
One move away determinism different ways. First, note key
property deterministic POMDP hidden state fully determined history.
possible satisfy property even stochastic systems, long one uniquely
determine hidden state, given observation emitted arriving there.
case, observations emitted stochastically, number belief states (and
number prediction profiles) still bounded number hidden states.
Another step away determinism class systems, introduced Littman (1996),
called Det-POMDPs. Det-POMDP POMDP transition function
376

fiLearning Make Predictions Without Generative Model

observation function deterministic, initial state distribution may
stochastic. Det-POMDP deterministic dynamical system, uncertainty
hidden state. uncertainty, system appears emit observations
stochastically. underlying dynamics deterministic. Littman showed
Det-POMDP n hidden states initial state distribution states
support (n + 1)m 1 distinct belief states. So, bounds number
prediction profiles well.
Finally, importantly, hidden state abstracted Proposition 20,
properties really need hold abstract beliefs. is, environment
may complex stochastic arbitrary ways, abstract hidden state
described Proposition 20 fully determined history, number prediction
profiles bounded number abstract states (as case Three Card Monte).
Similarly, Det-POMDP-like properties imagined abstract hidden states well.
cases means cover situations number prediction profiles
bounded, seem indicate class problems number
prediction profiles finite quite broad, may contain many interesting examples.

6. Experiments
section empirically evaluate prediction profile model learning procedure developed Section 4. experiment agent faces environment generative
model would challenge learn due high linear dimension. However,
problem agent could make good decisions could predictions small
number important tests. prediction profile model learned important tests
accuracy learned predictions evaluated.
experiments demonstrate one possible use prediction profile models (and
partial models general) control. generative, prediction profile
models cannot typically used directly offline, model-based planning methods. However, output may useful model-free methods control. Specifically,
experiments, predictions made learned prediction profile models provided
features policy gradient algorithm.
6.1 Predictive Features Policy Gradient
Policy gradient methods (e.g., Williams, 1992; Baxter & Bartlett, 2000; Peters & Schaal,
2008) successful viable options model-free control partially observable domains. Though differences various algorithms, common
thread assume parametric form agents policy attempt
alter parameters direction gradient respect expected average reward. experiments make use Online GPOMDP Average Reward Baseline
(Weaver & Tao, 2001), OLGARB (readers referred original paper details).
OLGARB assumes set features history, agents policy takes
parametric form:
Pr(a | h; w)
~ =P

e

P



377

wi,a (h)
P
wi,a (h)



e

fiTalvitie & Singh

(h) ith feature parameter wi,a weight specific feature
action considered.
Typically features used policy gradient features directly read
history (e.g., features recent observations presence/absence
event history). difficult know priori historical features
important making good control decisions. contrast, idea experiments
provide values predictions features. predictive features direct
consequences control, provide information effects possible behaviors
agent might engage in. such, may easier select set predictive features
likely informative optimal action take (e.g., agent
reach goal state takes action? taking action damage
agent?). Furthermore, information may expressed compactly terms prediction
would complex specify purely terms past observations. seen
discussion PSRs Section 2.2, arbitrary-length history fully captured
finite set short-term predictions. reasons seems reasonable speculate
predictive features, maintained prediction profile model, may particularly
valuable model-free control methods policy gradient.
6.2 Experimental Setup
learning algorithm applied two example problems. problem prediction
profile models learned various amounts training data (using LPSTs
POMDPs representation using strategies dealing multiple matches,
described Section 4.3). prediction accuracy models evaluated, well
useful predictions features control. training data generated
executing uniform random policy environment.
free parameter learning algorithm significance value statistical
tests, . Given large number contingency tests performed
data set, compound probability false negative, set fairly
low. experiments use = 0.00001, though several reasonable values tried
similar results. discussed Section 4, maximum length
histories consider search prediction profiles. cutoff allows search
avoid considering long histories, many long histories search
unlikely provide new prediction profiles.
prediction profile model learned, predictions evaluated features
policy gradient algorithm OLGARB. Specifically, test interest unit
interval split 10 equally-sized bins b binary feature ft,b provided
1 prediction lies bin b, 0 otherwise. provided binary features fo ,
possible observation o. feature fo = 1 recent observation
0, otherwise. parameters OLGARB, learning rate discount factor, set
0.01 0.95, respectively experiments.
evaluate prediction profile model OLGARB run 1,000,000 steps. average
reward obtained root mean squared error (RMSE) predictions tests
interest accrued model along way reported. Prediction performance
compared obtained learning POMDP training data using
378

fiLearning Make Predictions Without Generative Model

Prediction Performance

Control Performance
0.1

Avg. Reward (20 trials)

Avg. RMSE (20 Trials)

1
0.8
0.6
Flat POMDP

0.4
PPLPST(KLD)
PPLPST(cut)

0.2
0
0

PPPOMDP(KLD)
PPPOMDP(cut)

2

4

# Training Trajectories

True

0.06
0.04

PPPOMDP(KLD)
PPPOMDP(cut)

0.02

PPLPST(KLD)
PPLPST(cut)

0
Flat POMDP

0.02
0.04
0.06
0

6

Expert

0.08

SOM

2

4

6

# Training Trajectories x 104

4

x 10

Figure 6: Results Three Card Monte domain.
make predictions interest. problems complex feasibly train
POMDP correct number underlying states, 30-state POMDPs used
(stopping EM maximum 50 iterations)2 . Control performance compared
obtained OLGARB using predictions provided learned POMDP model
features, well OLGARB using true predictions features (the best prediction
profile model could hope do), OLGARB using second-order Markov features (the two
recent observations, well action them) predictive features
all, hand-coded expert policy.
6.3 Three Card Monte
first domain Three Card Monte example. agent presented three
cards. Initially, card middle (card 2) ace. agent four actions
available it: watch, f lip1, f lip2, f lip3. agent chooses flip action, observes
whether card flipped special card. agent chooses watch action,
dealer swap positions two cards, case agent observes two
cards swapped, dealer ask guess. dealer asked
guess, watch results 0 reward flip action results -1 reward. dealer
asks guess agent flips special card, agent gets reward 1.
agent flips one two cards, doesnt flip card (by selecting watch),
gets reward -1. agent three tests interest, take form f lipX ace,
card X (that is, flip card X, see ace?).
discussed previously, complexity system directly related complexity dealers decision-making process. experiment, agent chooses
watch dealer swaps pair cards swapped least far probability
0.5; probability 0.4 chooses uniformly amongst pairs cards; otherwise
asks guess. Since dealer keeping count many times swap
made, process governing dynamics effectively infinite linear dimension.
2. Similar results obtained 5, 10, 15, 20, 25 states.

379

fiTalvitie & Singh

prediction profile system, hand, 3 states, regardless dealers
complexity (see Figure 1).
Training trajectories length 10. Figure 6 shows results various amounts
training data, averaged 20 trials. PP-POMDPs PP-LPSTs learned make
accurate predictions tests interest, eventually achieving zero prediction error.
case, PP-POMDPs using less data. likely POMDP model
readily able take advantage fact prediction profile system Three
Card Monte Markov. expected, standard POMDP model unable accurately
predict tests interest.
compared two different strategies dealing multiple matches discussed Section 4.3. Recall first one (marked KLD graph) picks
matching profile smallest empirical KL-Divergence estimated predictions.
second (marked cut graph) simply cuts trajectory point
multiple match avoid incorrect labels. problem two strategies result almost exactly performance. likely profiles Three
Card Monte deterministic, therefore quite easy distinguish (making multiple
matches unlikely). next experiment stochastic profiles.
predictive features provided prediction profile models clearly useful
control, control performance OLGARB using predictions approaches,
eventually exactly matches OLGARB using true predictions (marked True).
inaccurate predictions provided POMDP useful control; OLGARB using POMDP provided predictions even break even, meaning loses
game often wins. POMDP features did, however, seem contain
useful information beyond provided second-order Markov features (marked
SOM) which, one might expect, performed poorly.
6.4 Shooting Gallery
second example called Shooting Gallery, pictured Figure 7(a). agent
gun aimed fixed position 88 grid (marked X) . target moves
diagonally, bouncing boundaries image 22 obstacles (an example
trajectory pictured). agents task shoot target. agent two actions:
watch shoot. agent chooses watch, gets 0 reward. agent chooses
shoot target crosshairs step agent shoots, agent gets
reward 10, otherwise gets reward -5. Whenever agent hits target,
shooting range resets: agent receives special reset observation, 2 2 square
range made obstacle probability 0.1, target placed random
position. 0.01 probability range reset every time step.
difficulty target sticky. Every time step probability 0.7 moves
current direction, probability 0.3 sticks place. Thus, looking recent
history, agent may able determine targets current direction. agent
needs know probability target sights next step, clearly
single test interest is: watch target (that choose watch action,
target enter crosshairs?). target far crosshairs, prediction
test 0. target crosshairs, 0.3. target
380

fiLearning Make Predictions Without Generative Model

(a)

(b)

Figure 7: Shooting Gallery domain. (a) possible arrangement obstacles trajectory target (lighter back time). case target
definitely enter agents crosshairs, since bounce obstacle.
(b) abstraction applied recent observation.

near crosshairs, model must determine whether prediction 0.7 0, based
targets previous behavior (its direction) configuration nearby obstacles.
problem stochastic prediction profiles, expected data
required differentiate them. Also, due number possible configurations
obstacles positions target, system roughly 4,000,000 observations
even latent states. results large number possible histories,
small probability occurring. discussed Section 4, lead large sample
complexity obtaining good estimates prediction profiles. addressed
simple form generalization: observation abstraction. Two observations treated
target position configuration obstacles
immediate vicinity target same. words, abstract observation
contains information targets position obstacles surrounding
target, placement obstacles far away target (see Figure 7(b))
example. abstraction, abstract observations still provide enough detail
make accurate predictions. is, two histories indeed prediction profile
action sequence observation sequences correspond
sequence aggregate observations. enables one sample trajectory improve
estimates several histories, though, even abstraction, still
2000 action-observation pairs. observation abstraction applied training
POMDP model.
Training trajectories length 4 search profiles restricted length
3 histories. Results shown Figure 8. Perhaps eye-catching feature
results upward trending curve prediction error graph, corresponding
PP-POMDP KL-Divergence based matching (labeled PP-POMDP(KLD)).
Recall danger KL-divergence based matching strategy may produce
incorrect labels training data. Apparently errors severe enough
problem drastically mislead POMDP model. small amount data obtained
381

fiTalvitie & Singh

Prediction Performance

Control Performance
0.025

Avg. Reward (20 Trials)

Avg. RMSE (20 Trials)

0.25
0.2
PPPOMDP(cut)

0.15
Flat POMDP

0.1

PPPOMDP(KLD)

0.05

PPLPST(cut)
PPLPST(KLD)

0
0

2

4

6

8

# Training Trajectories

0.02
0.015
0.01

PPLPST(KLD)
PPPOMDP(KLD)

PPLPST(cut)

0.005
PPPOMDP(cut)

0
0.005
0

10
5
x 10

Expert
True

SOM
Flat POMDP

2

4

6

8

10

# Training Trajectories x 105

Figure 8: Results Shooting Gallery domain.
good prediction error, data came misleading labelings,
performance suffered. PP-POMDP trained matching method (PPPOMDP(cut)) displays typical learning curve (more data results better error),
though takes great deal data begins make reasonable predictions.
cutting trajectories multiple matches throws away data might
informative model. PP-LPSTs generally outperform PP-POMDPs
problem. trajectory cutting method, PP-LPST (PP-LPST(cut))
quickly outperforms flat POMDP and, enough data, outperforms versions
PP-POMDP. PP-LPST KL-divergence based matching (PP-LPST(KLD))
far best performer, quickly achieving small prediction error. Clearly incorrect
labels training data dramatic effect LPST learning, possibly
because, suffix tree, LPST mostly makes predictions based recent history,
limiting effects labeling errors time-steps.
Control performance essentially mirrors prediction performance, interesting
exceptions. Note even though PP-POMDP(KLD) obtains roughly prediction
error flat POMDP 1,000,000 training trajectories, predictive features provides
still result substantially better control performance. indicates that, even though
PP-POMDP making errors exact values predictions, still captured
important dynamics predictions flat POMDP has. flat POMDP
provides features roughly useful second-order Markov features,
result good performance. Again, OLGARB using features break
even, meaning wasting bullets target likely enter crosshairs.
best-performing prediction profile model, PP-LPST(KLD) approaches performance
OLGARB using true predictions sufficient data.

7. Related Work
idea modeling aspects observations dynamical system
certainly raised before. instance, recent example Rudary (2008) learned linear382

fiLearning Make Predictions Without Generative Model

Gaussian models continuous partially observable environments dimensions
observation treated unmodeled exogenous input. inputs assumed
linear effect state transition. Along somewhat similar lines, context
model minimization (taking given, complete model deriving simpler, abstract model
preserves value function) Wolfe (2010) constructed abstract model
shadow model predicts observation details ignored abstraction.
shadow model takes abstract observations abstract model unmodeled input.
Splitting observation modeled un-modeled components learning
generative model certainly related approach. case, model would make
conditional predictions modeled portion observation, given exogenous
inputs (as well actual actions history). Prediction profile models take
extreme, treating entire observation input. Instead predicting future sequences
piece next observation conditioned another piece, prediction profile models
predict values arbitrary set predictions interest next time step, given
entire action observation. allows significantly freedom choosing
predictions model make (and, importantly, make).
One modeling method closely related prediction profiles Causal State Splitting
Reconstruction (CSSR) (Shalizi & Klinker, 2004). CSSR algorithm learning generative models discrete, partially observable, uncontrolled dynamical systems. basic
idea define equivalence relation histories two histories considered
equivalent associated identical distributions possible futures.
equivalence classes relation called causal states. CSSR algorithm learns
number causal states, distribution next observations associated
causal state, transitions one causal state next, given observation.
straightforward see one-to-one correspondance causal states
predictive states PSR. such, causal state model precisely prediction
profile model set tests interest Q, set core tests. correspondance hand, results Section 5.2 show many cases number causal
states greatly exceed linear dimension original system therefore
CSSR may inadvisable many problems, comparison standard modeling
approaches. possible CSSR algorithm could adapted general
setting arbitrary sets tests interest, however algorithm rely heavily
fact prediction profile model Q tests interest Markov,
generally case sets tests interest.
mentioned Section 2, McCallum (1995) presented UTree, suffix-tree-based algorithm learning value functions partially observable environments. UTree
learns value function (a prediction future rewards), make
predictions observations, UTree learn non-generative partial model. Wolfe
Barto (2006) extend UTree make one-step predictions particular observation
features rather limiting predictions value function. learns suffix
tree, UTree able operate non-episodic domains (whereas method requires seeing
histories multiple times) required explicitly search distinct prediction profiles. UTree directly incorporates abstraction learning, learning simultaneously
observation features important, history suffix attend them.
said, main drawback suffix tree approach tree takes account
383

fiTalvitie & Singh

information relatively recent history (a suffix history). cannot remember
important information arbitrary number steps recurrent state-based model
can. Three Card Monte example, instance, access depth-limited suffix
history would little help. order track ace, one must take account
every move dealer made since beginning game. UTree would essentially
forget card games length surpassed depth memory.
McCallum (1993) Mahmud (2010) provide methods learning state machines
predict immediate reward resulting given action-observation pair partially observable control tasks (and thus suffer issue finite-depth memory
suffix trees do). Thus, learning problem special case ours,
restrict models make one-step predictions immediate reward.
cases, simple model incrementally greedily elaborated proposing states split
evaluating results (via statistical tests case McCallum via likelihood
hill-climbing case Mahmud). McCallum expressed concern approach
difficulty extracting long-range dependencies (for instance, learning attend event
appear affect distribution rewards many steps later);
clear extent Mahmuds approach addresses issue. methods
advantages UTree, notably applied non-episodic
domains. said, approach advantages well. re-casting problem
learning non-generative model standard generative model-learning problem,
able gain deeper understanding complexity applicability prediction
profile models compared standard generative models. Furthermore, allowed us incorporate standard, well-studied generative model-learning methods
learning algorithm, thereby leveraging strengths non-generative setting.
specifically, resulting principled (albeit heuristic) learning algorithm,
rely guess-and-check stochastic local search.
prediction profile system similar spirit finite state controllers
POMDPs. Sondik (1978) noted cases, possible represent optimal policy POMDP finite state machine. finite state controllers
much prediction profile models take action-observation pairs inputs,
instead outputting predictions associated current history, output
optimal action take. Multiple authors (e.g., Hansen, 1998; Poupart & Boutilier, 2003)
provide techniques learning finite state controllers. However, algorithms typically
require access complete POMDP model world begin which, setting,
assumed impractical.

8. Conclusions Future Directions
standard methods learning models partially observable environments learn
generative models. one small set predictions interest make (and
therefore require full power generative model), one ignore irrelevant
detail via abstraction simplify learning problem. Even so, generative model
necessarily make predictions relevant details, even directly
interest. seen example resulting model counter-intuitively
complex, even predictions model asked make quite simple.
384

fiLearning Make Predictions Without Generative Model

presented prediction profile models, non-generative models partially
observable systems make predictions interest others. main idea
prediction profile models learn model dynamics predictions
change time, rather model dynamics system. learning
method prediction profile models learns transformation training data
applies standard methods transformed data (assuming predictions interest
take finite number distinct values). result, retains advantages methods
EM POMDPs learn information history must maintained order
make predictions (rather requiring set history features priori). showed
prediction profile model far simpler generative model, though
far complex, depending predictions asked make. However,
predictions interest depend relatively little state information, prediction profile
models provide substantial savings standard modeling methods POMDPs.
experiments Section 6 demonstrate possible learn prediction
profile models contrived systems complex POMDPs, specific learning algorithm presented likely scale natural domains without modification.
critical scaling issues prediction profile models sample complexity
estimating prediction profiles, computational complexity searching prediction profiles translating data. cases, critical source complexity
essentially many distinct histories training data (more distinct histories
means data spread thin amongst estimated profiles search
through). such, generalization prediction estimates across many histories would
key step toward applying ideas realistic domains. currently developing learning algorithms combine ideas behind prediction profile models
methods learning abstractions allow many essentially equivalent histories
lumped together purposes estimating predictions interest.
Another limitation prediction profile model learning method presented
reliance assumption finite number prediction profiles. assumption
hold many cases, ideal method would able deal gracefully large
infinite number prediction profiles. One possibility simply cluster predictions
ways. instance, one may desire certain level prediction accuracy
may therefore willing lump distinct prediction profiles together exchange
simpler prediction profile system. Another idea would learn prediction profile model
using continuous-valued representations Kalman filters (Kalman, 1960) PLGs
(Rudary, Singh, & Wingate, 2005) (or nonlinear variants, e.g., Julier & Uhlmann,
1997; Wingate, 2008). representations learning algorithms explicitly deal
systems infinite number observations (prediction profiles case). Even
finitely many prediction profiles, methods learning non-linear continuous
models may still able (approximately) capture discrete dynamics.
Additionally, though results focused discrete systems, main motivation
behind prediction profile models purchase continuous setting. Typical methods learning models partially observable systems continuous systems, much
discrete valued counterparts, learn generative models. such, non-generative
approach prediction profile models may provide similar benefits continuous setting
predictions need made. setting, prediction profiles might represented
385

fiTalvitie & Singh

parametric form (for instance, mean variance Gaussian). main idea
prediction profile models (though specific method presented here) could still
applied: learn model dynamics distribution parameters, rather
dynamics system itself.
Finally, discussed work tests interest determined, predict selected. Automatically selecting interesting/important predictive features targets partial models would certainly
interesting research challenge. course, would depend predictions
used for. predictions used features control, done
experiments, would certainly seem intuitive start predictive features
regarding reward signal, perhaps observation features strongly correlate
reward (as intuitively done hand experiments). may useful
consider making predictions predictions style TD Networks (Sutton
& Tanner, 2005). instance, one could imagine learning models make predictions
profile another model emit. way models could chained together
make predictions extant rewards, rather focusing solely predicting
immediate reward signal (which always particularly good feature temporal
decision problems). Another common use partial models decompose large modeling
problem many small ones, in, instance, factored MDPs (Boutilier et al., 1999),
factored PSRs (Wolfe et al., 2008), collections local models (Talvitie & Singh, 2009b).
setting, choosing tests interest would example structure learning
problem: decomposing one-step predictions relatively independent components
assigning different models.

Acknowledgments
Erik Talvitie supported NSF GRFP. Satinder Singh supported NSF
grant IIS-0905146. opinions, findings, conclusions recommendations expressed
material authors necessarily reflect views NSF.
work presented paper extension work presented IJCAI (Talvitie &
Singh, 2009a). grateful anonymous reviewers whose helpful comments
improved presentation work.

Appendix A.
A.1 Proof Proposition 15
result follow straightforwardly general fact dynamical systems. Let
h[i...j] sequence actions observations h starting ith time-step
sequence ending jth time-step sequence. conveniences sake,
> j let h[i...j] = h0 , null sequence. following two results show
test ever positive probability, must positive probability history
length less linear dimension system.

386

fiLearning Make Predictions Without Generative Model

Figure 9: matrix constructed Lemma 21 full rank (a contradiction).
Lemma 21. linear dimension dynamical system n, test
history h length(h) = k n p(t | h) > 0, i, j 0 < j 1 k
p(t | h[1...i] h[j...k] ) > 0.
Proof. Note p(t | h) > 0, p(h[(i+1)...k] | h[1...i] ) = p(t | h)p(h[(i+1)...k] | h[1...i] ) >
0 0 k. assume i, j 0 < j 1 k p(h[j...k] | h[1...i] ) =
p(t | h[1...i] h[j...k] )p(h[j...k] | h[1...i] ) = 0 seek contradiction. Consider submatrix
system dynamics matrix. rows submatrix correspond prefixes h: h[1...i]
0 k. columns correspond suffixes h pre-pended test t: h[j...k]
1 j k + 1. k + 1 k + 1 matrix. assumption,
matrix triangular positive entries along diagonal (Figure 9 shows matrix
k = 4). such, matrix full rank (rank k + 1). contradiction since
k n submatrix never higher rank matrix contains it.
next result follows immediately Lemma 21.
Corollary 22. system linear dimension n test history h
p(t | h) > 0, exists (possibly non-consecutive) subsequence h h
length(h ) < n p(t | h ) > 0.
Proof. Lemma 21, every history h length k n p(t | h) > 0 must
subsequence h1 length k1 < k p(t | h) > 0. k1 n, h1 must
subsequence h2 length k2 < k1 . argument repeated subsequence
length less n.
consequence Corollary 22 every test ever positive probability,
must positive probability following history length less n. fact
hand, Proposition 15 proven.
Proposition 15. deterministic dynamical system actions A, observa.
tions O, linear dimension, n log(|A|1)+log(|O|+1)
log |A|
387

fiTalvitie & Singh

Proof. Since system deterministic, history action correspond exactly one
resulting observation. history sequence actions observations. However, since
sequence observations fully determined sequence actions deterministic
system, number distinct histories length k simply |A|k . history
|A| action choices could result different observation. So, number
observations could possibly occur histories length k simply |A|k+1 .
Corollary 22, linear dimension n, observations must occur history h
length(h) n 1. Thus, number observations possibly follow histories
length less n is:
|O|

n1
X

|A|i+1 =

i=0

|A|n+1 1
1.
|A| 1

Solving n yields bound linear dimension terms number actions
number observations.
A.2 Proof Proposition 17
Proposition 17. given system set tests interest, linear dimension
corresponding prediction profile system greater prediction profile
system associated set core tests system (as described Section 2.2).
Proof. Recall discussion PSRs Section 2.2 set core tests, Q,
set tests whose corresponding columns system dynamics matrix constitute
basis. predictions core tests given history form predictive state
history. So, predictive state precisely prediction profile core tests Q.
prediction test computed linear function prediction profile
Q. Note prediction profile system Q MDP. shown
Section 2.2 compute next predictive state given current predictive state
action-observation pair.
consider set tests interest . predictions Q
used compute prediction test, must
function maps prediction profiles Q prediction profiles .
general, multiple predictive states may map prediction profile
surjection. easy see prediction profile system result
applying observation abstraction prediction profile system Q. Performing
observation abstraction MDP generally produces POMDP, never increases
linear dimension (Talvitie, 2010). Hence, prediction profile system set tests
interest linear dimension greater prediction profile system
set core tests, Q.
A.3 Proof Proposition 20
Proposition 20. Consider POMDP hidden states S, actions A, observations
O. Let set tests interest. Let ai action taken time-step i, si
hidden state reached taking action ai , oi observation emitted si . Now,
388

fiLearning Make Predictions Without Generative Model

consider surjection : mapping hidden states set abstract states
following properties:
1. pair primitive states s1 , s2 S, (s1 ) = (s2 ), time-step
test interest , p(t | si = s1 ) = p(t | si = s2 ).
2. pair primitive states s1 , s2 S, (s1 ) = (s2 ), time-step i,
abstract state , observation O, action A,
Pr((si+1 ) = | si = s1 ,ai+1 = a, oi+1 = o) =
Pr((si+1 ) = | si = s2 , ai+1 = a, oi+1 = o).
exists, prediction profile system linear dimension greater
number distinct beliefs abstract states, .
Proof. proof follows similar reasoning proof Proposition 17. Note that,
Property 1 belief abstract states given history sufficient compute
prediction profile. history h test interest :
X
X X
p(t | h) =
Pr(s | h)p(t | s) =
Pr(s | h)p(t | s)
sS

=

X

SS

p(t | S)

X

SS sS

Pr(s | h) =

X

p(t | S)Pr(S | h),

SS

sS

third equality follows property 1: , hidden states
associated probabilities tests interest.
Now, consider dynamical system beliefs abstract states observations
action-observation pairs actions. Call abstract belief system.
predictive state, possible compute prediction profile
abstract beliefs, prediction profile model seen result
observation aggregation abstract belief system. result, prediction profile
system linear dimension greater abstract belief system.
rest proof shows that, Property 2, abstract belief system
MDP, therefore linear dimension greater number distinct beliefs
abstract states.
Given probability distribution abstract states given history h, agent
takes action observes observation o, possible compute probability
abstract state new history:
X
X X
Pr(S | hao) =
Pr(s | h)Pr(S | s, a, o) =
Pr(s | h)Pr(S | s, a, o)
sS

=

X



Pr(S | , a, o)

X

sS

Pr(s | h) =

X

Pr(S | , a, o)Pr(S | h),



sS

third equality follows Property 2: , hidden states
associated conditional distribution next abstract states, given action
observation.
389

fiTalvitie & Singh

So, one compute next abstract beliefs previous abstract beliefs,
abstract belief system MDP, therefore linear dimension greater
number observations (the number distinct abstract beliefs). one
compute prediction profile abstract beliefs, prediction profile system
constructed applying observation abstraction abstract belief system. Thus,
prediction profile system linear dimension greater number distinct
abstract beliefs.

References
Baum, L. E., Petrie, T., Soules, G., & Weiss, N. (1970). maximization technique occuring
statistical analysis probabilistic functions markov chains. Annals
Mathematical Statistics, 41 (1), 164171.
Baxter, J., & Bartlett, P. L. (2000). Reinforcement learning POMDPs via direct gradient ascent. Proceedings Eighteenth International Conference Machine
Learning (ICML), pp. 4148.
Boots, B., Siddiqi, S., & Gordon, G. (2010). Closing learning-planning loop predictive state representations. Proceedings Robotics: Science Systems, Zaragoza,
Spain.
Boots, B., Siddiqi, S., & Gordon, G. (2011). online spectral learning algorithm
partially observable nonlinear dynamical systems. Proceedings Twenty-Fifth
National Conference Artificial Intelligence (AAAI).
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions computational leverage. Journal Artificial Intelligence Research,
11, 194.
Bowling, M., McCracken, P., James, M., Neufeld, J., & Wilkinson, D. (2006). Learning
predictive state representations using non-blind policies. Proceedings TwentyThird International Conference Machine Learning (ICML), pp. 129136.
Dinculescu, M., & Precup, D. (2010). Approximate predictive representations partially
observable systems. Proceedings Twenty-Seventh International Conference
Machine Learning (ICML), pp. 895902.
Hansen, E. (1998). Finite-Memory Control Partially Observable Systems. Ph.D. thesis,
University Massachussetts, Amherst, MA.
Holmes, M., & Isbell, C. (2006). Looping suffix tree-based inference partially observable hidden state. Proceedings Twenty-Third International Conference
Machine Learning (ICML), pp. 409416.
James, M., & Singh, S. (2004). Learning discovery predictive state representations
dynamical systems reset. Proceedings Twenty-First International
Conference Machine Learning (ICML), pp. 417424.
Julier, S. J., & Uhlmann, J. K. (1997). new extension kalman filter nonlinear
systems. Proceedings AeroSense: Eleventh International Symposium
Aerospace/Defense Sensing, Simulation Controls, pp. 182193.
390

fiLearning Make Predictions Without Generative Model

Kalman, R. E. (1960). new approach linear filtering prediction problems. Transactions ASME Journal Basic Engineering, 82, 3545.
Littman, M., Sutton, R., & Singh, S. (2002). Predictive representations state. Advances
Neural Information Processing Systems 14 (NIPS), pp. 15551561.
Littman, M. L. (1996). Algorithms Sequential Decision Making. Ph.D. thesis, Brown
University, Providence, RI.
Mahmud, M. M. H. (2010). Constructing states reinforcement learning. Proceedings
Twenty-Seventh International Conference Machine Learning (ICML), pp.
727734.
McCallum, A. K. (1995). Reinforcement Learning Selective Perception Hidden
State. Ph.D. thesis, Rutgers University.
McCallum, R. A. (1993). Overcoming incomplete perception utile distinction memory.
Proceedings Tenth International Conference Machine Learning (ICML),
pp. 190196.
Monahan, G. E. (1982). survey partially observable markov decisions processes: Theory,
models, algorithms. Management Science, 28 (1), 116.
Peters, J., & Schaal, S. (2008). Natural actor-critic. Neurocomputing, 71, 11801190.
Poupart, P., & Boutilier, C. (2003). Bounded finite state controllers. Advances Neural
Information Processing Systems 16 (NIPS).
Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley Sons, New York, NY.
Rivest, R. L., & Schapire, R. E. (1994). Diversity-based inference finite automata. Journal
Association Computing Machinery, 41 (3), 555589.
Rudary, M. (2008). Predictive Linear Gaussian Models. Ph.D. thesis, University
Michigan.
Rudary, M., Singh, S., & Wingate, D. (2005). Predictive linear-gaussian models stochastic dynamical systems. Uncertainty Artificial Intelligence: Proceedings
Twenty-First Conference (UAI), pp. 501508.
Shalizi, C. R., & Klinker, K. L. (2004). Blind construction optimal nonlinear recursive
predictors discrete sequences. Proceedings Twentieth Conference
Uncertainty Artificial Intelligence (UAI), pp. 504511.
Singh, S., James, M. R., & Rudary, M. R. (2004). Predictive state representations:
new theory modeling dynamical systems. Uncertainty Artificial Intelligence:
Proceedings Twentieth Conference (UAI), pp. 512519.
Sondik, E. J. (1978). optimal control partially observable markov processes
infinite horizon: Discounted costs. Operations Research, 26, 282304.
Soni, V., & Singh, S. (2007). Abstraction predictive state representations. Proceedings
Twenty-Second National Conference Artificial Intelligence (AAAI), pp. 639
644.
391

fiTalvitie & Singh

Sutton, R. S., & Tanner, B. (2005). Temporal-difference networks. Advances Neural
Information Processing Systems 17 (NIPS), pp. 13771384.
Talvitie, E. (2010). Simple Partial Models Complex Dynamical Systems. Ph.D. thesis,
University Michigan, Ann Arbor, MI.
Talvitie, E., & Singh, S. (2009a). Maintaining predictions time without model.
Proceedings Twenty-First International Joint Conference Artificial Intelligence (IJCAI), pp. 12491254.
Talvitie, E., & Singh, S. (2009b). Simple local models complex dynamical systems.
Advances Neural Information Processing Systems 21 (NIPS), pp. 16171624.
Weaver, L., & Tao, N. (2001). optimal reward baseline gradient-based reinforcement learning. Uncertainty Artificial Intelligence: Proceedings Seventeenth
Conference (UAI), pp. 538545.
Williams, R. (1992). Simple statistical gradient-following algorithms connectionist reinforcement learning. Machine Learning, 8, 229256.
Wingate, D. (2008). Exponential Family Predictive Representations State. Ph.D. thesis,
University Michigan.
Wingate, D., Soni, V., Wolfe, B., & Singh, S. (2007). Relational knowledge predictive
state representations. Proceedings Twentieth International Joint Conference
Artificial Intelligence (IJCAI), pp. 20352040.
Wolfe, A. P. (2010). Paying Attention Matters: Observation Abstraction Partially
Observable Environments. Ph.D. thesis, University Massachussetts, Amherst, MA.
Wolfe, A. P., & Barto, A. G. (2006). Decision tree methods finding reusable MDP
homomorphisms. Proceedings Twenty-First National Conference Artificial
Intelligence (AAAI).
Wolfe, B., James, M., & Singh, S. (2008). Approximate predictive state representations.
Proceedings Seventh Conference Autonomous Agents Multiagent Systems
(AAMAS).
Wolfe, B., James, M. R., & Singh, S. (2005). Learning predictive state representations
dynamical systems without reset. Proceedings Twenty-Second International
Conference Machine Learning (ICML), pp. 985992.
Wolfe, B., & Singh, S. (2006). Predictive state representations options. Proceedings Twenty-Third International Conference Machine Learning (ICML), pp.
10251032.

392


