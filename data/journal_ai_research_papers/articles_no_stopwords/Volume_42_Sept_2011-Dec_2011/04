Journal Artificial Intelligence Research 42 (2011) 815-850

Submitted 07/11; published 12/11

Stochastic Enforced Hill-Climbing
Jia-Hong Wu

JW @ ALUMNI . PURDUE . EDU

Institute Statistical Science,
Academia Sinica, Taipei 115, Taiwan ROC

Rajesh Kalyanam
Robert Givan

RKALYANA @ PURDUE . EDU
GIVAN @ PURDUE . EDU

Electrical Computer Engineering,
Purdue University, W. Lafayette, 47907, USA

Abstract
Enforced hill-climbing effective deterministic hill-climbing technique deals local optima using breadth-first search (a process called basin flooding). propose evaluate
stochastic generalization enforced hill-climbing online use goal-oriented probabilistic planning problems. assume provided heuristic function estimating expected cost
goal flaws local optima plateaus thwart straightforward greedy action choice.
breadth-first search effective exploring basins around local optima deterministic problems, stochastic problems dynamically build solve heuristic-based Markov decision
process (MDP) model basin order find good escape policy exiting local optimum.
note building model involves integrating heuristic MDP problem
local goal improve heuristic.
evaluate proposal twenty-four recent probabilistic planning-competition benchmark
domains twelve probabilistically interesting problems recent literature. evaluation,
show stochastic enforced hill-climbing (SEH) produces better policies greedy heuristic
following value/cost functions derived two different ways: one type derived using
deterministic heuristics deterministic relaxation second type derived automatic learning Bellman-error features domain-specific experience. Using first type heuristic,
SEH shown generally outperform planners first three international probabilistic
planning competitions.

1. Introduction
Heuristic estimates distance-to-the-goal long used deterministic search deterministic planning. estimates typically flaws local extrema plateaus limit
utility. Methods simulated annealing (Kirkpatrick, Gelatt, & Vecchi, 1983; Cerny,
1985) A* (Nilsson, 1980) search developed handling flaws heuristics.
recently, excellent practical results obtained flooding local optima using breadth-first
search. method called enforced hill-climbing (Hoffmann & Nebel, 2001).
Deterministic enforced hill-climbing (DEH) proposed work Hoffmann Nebel
(2001) core element successful deterministic planner Fast-Forward (FF). DEH
extension basic hill-climbing approach simply selecting actions greedily looking
ahead one action step, terminating reaching local optimum. DEH extends basic hillclimbing replacing termination local optima breadth-first search find successor state
strictly better heuristic value. planner moves descendant repeats
c
2011
AI Access Foundation. rights reserved.

fiW U , K ALYANAM , & G IVAN

process. DEH guaranteed find path goal problem deadend-free (so
every state path). relatively weak guarantee applies independent quality
heuristic function, intent DEH remediate flaws generally accurate heuristic
order leverage heuristic finding short paths goal. domains basin
size (search depth needed escape optimum) bounded, DEH provide polynomial-time
solution method (Hoffmann, 2005).
Enforced hill-climbing defined probabilistic problems, due stochastic outcomes
actions. presence stochastic outcomes, finding descendants better values longer
implies existence policy reaches descendants high probability. One may argue FF-Replan (Yoon, Fern, & Givan, 2007)a top performer recent probabilistic planning
benchmarksuses enforced hill-climbing call FF. However, enforced hill-climbing
process used determinized problem, FF-Replan use form hill climbing
directly stochastic problem. fact, FF-Replan consider outcome probabilities
all.
One problem consider generalizing enforced hill-climbing stochastic domains
solution deterministic problem typically concise, sequential plan. contrast, solution
stochastic problem policy (action choice) possibly reached states. essential
motivation hill-climbing avoid storing exponential information search, even
explicit solution stochastic problem cannot directly stored respecting motivation.
reason, limit consideration online setting, solution problem
local policy around current state. local policy committed executed
local region exited, planner new online problem solve (possibly retaining
information previous solution). approach generalizes directly construction
offline policies situations space store policies available. Note that, contrast,
deterministic enforced hill-climbing easily implemented offline solution technique.
propose novel tool stochastic planning generalizing enforced hill-climbing goalbased stochastic domains. Rather seeking sequence actions deterministically leading
better state, method uses finite-horizon MDP analysis around current state seek policy
expects improve heuristic value current state. Critical process direct
incorporation probabilistic model heuristic function finding desired policy.
Therefore, finite-horizon analysis, heuristic function integrated MDP problem
order represent temporary, greedy goal improving current heuristic value.
integration done building novel heuristic-based MDP state new exit
action available terminates execution cost equal heuristic estimate state,
action costs removed1 . heuristic-based MDP, finite-horizon policies restricted
requirement horizon one, exit action must selected (but selected
horizons). heuristic-based MDP, cost policy state expected
value heuristic upon exit (or horizon) executed s.
Thus, find desired local policy using value iteration heuristic-based MDP around
current state, deepening horizon, policy found cost improving heuristic
estimate current state. restriction selecting exit action horizon one corresponds
initializing value iteration provided heuristic function. policy found,
1. motivation removal action costs heuristic-based MDP discussed Section 3.2.

816

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

method executes policy exiting action indicated (or horizon used computing
policy).
resulting method, stochastic enforced hill-climbing (SEH), simply generalizes depth-k
breadth-first search state improved heuristic value (from DEH) k-horizon value iteration computation seeking policy expects improvement heuristic value. Note although
stochastic enforced hill-climbing explicit statespace technique, suitable use astronomically large statespaces heuristic used informative enough limit effective size
horizon k needed find expected heuristic improvement. empirical results work
demonstrate behavior successfully.
1.1 Applicability Limitations
Stochastic enforced hill-climbing (SEH) applied heuristic function. However,
applicability (and likewise limitations) SEH greatly depends characteristics
heuristic function. SEH appropriate goal-oriented problem given strong enough heuristic
function, demonstrate empirically SEH generally outperforms greedy following
heuristic variety heuristics variety domains, even presence probabilistically interesting features (Little & Thiebaux, 2007) deadends. SEH rely upon heuristic
function identification dead-ends appropriate handling probabilistically interesting features require non-local analysisSEH simply provides local search often correct
flaws heuristic function. SEH thus intended possible improvement stochastic
solution methods construct cost-to-go (cost) function follow greedily using
constructed cost function search heuristic. Many methods constructing value/cost functions
proposed evaluated literature, potentially improved
goal-based domains using SEH place greedy following (Sutton, 1988; Fahlman & Lebiere,
1990; Bertsekas, 1995; Gordon, 1995; Mahadevan & Maggioni, 2007; Sanner & Boutilier, 2009)2 .
prove correctness SEH Section 3.4 showing deadend-free domains, SEH
finds goal probability one (i.e. SEH get stuck local optima).
SEH search technique leverages heuristic estimate distance go, must
emphasized that, unlike many search techniques, SEH makes promises
optimality solution path found. SEH greedy, local technique promise
repeatedly find policy reduces heuristic value, possible. such,
SEH inappropriate technique use optimal solutions required.
Stochastic enforced hill-climbing ineffective presence huge plateaus valleys
heuristic functions, due extreme resource consumption finding desired local policies.
Heuristic functions huge plateaus result methods failed find useful information problem state regions. SEH inappropriate tool solving
stochastic planning problemother tools needed construct useful heuristic function
manages deadends avoids huge plateaus. weakness mirrors weakness enforced hillclimbing deterministic domains. SEH fail find goals avoidable dead-ends
present recognized early enough heuristic. fact, effective dead-end detection
central goal heuristic design greedy technique applied heuristic.
2. applicability SEH, cost function must non-negative must identify goals assigning zero state
goal state; however, general value/cost functions normalized satisfy requirements.

817

fiW U , K ALYANAM , & G IVAN

insight usefulness SEH gained comparison recent determinizing replanners. mentioned above, one way exploit deterministic planning techniques
DEH stochastic problems determinize planning problem use deterministic planner select action sequence. Executing action sequence problem guaranteed
reach goal due determinization approximation, replanning needed augment
technique. paper, call stochastic planners use technique determinizing replanners. Determinizing replanners using determinization (called outcomes) retains
possible state transitions shown reach goal probability one absence
dead-end states.
contrast determinizing replanners, SEH point relies determinization
problem, instead analyzes increasing-size local probabilistic approximations problem.
SEH conducts full probabilistic analysis within horizon, seeking objective reducing
provided heuristic, using value iteration. way, SEH leverages probabilistic parameters
ignored determinizing replanners, well provided heuristic function,
based upon substantial probabilistic analysis. result, SEH successfully handles probabilistic
problem aspects cause major problems determinizing replanners. However, point,
theoretical results characterizing gains determinizing replanners. Instead,
extensive empirical evaluation showing advantages FF-Replan (Yoon et al., 2007)
RFF (Teichteil-Konigsbuch, Kuter, & Infantes, 2010) (two determinizing replanners), well
substantial gains compared greedy following heuristic (which uses transition
probability parameters).
1.2 Evaluation
test SEH broad range domains first three international probabilistic planning
competitions (as well probabilistically interesting domains Little & Thiebaux, 2007),
using two different methods generate heuristic functions. First, test SEH heuristic function based ideas successful re-planner FF-Replan (Yoon et al., 2007).
new controlled-randomness (CR-FF) heuristic deterministic heuristic (Hoffmann
& Nebel, 2001) computed simple determinization probabilistic problem makes
available deterministic transition wherever probabilistic transition possible. note
FF-Replan use (or any) heuristic function stochastic problem. Instead, FFReplan relies construct plan deterministic problem, calls turn use
deterministic enforced hill-climbing exactly heuristic. Here, consider performance
heuristic directly stochastic problem, comparing greedy heuristic-following SEHbased search around heuristic. latter method using SEH constitutes novel method
combining determinization (that removes probabilistic parameters) probabilistic reasoning.
experiments show new method substantially outperforms FF-Replan across broad
evaluation.
performed second evaluation technique heuristic functions learned
domain-specific experience relational feature-learning method presented work
Wu Givan (2007, 2010). heuristic functions already shown give good
performance used construct simple greedy policy, improved SEH.
SEH technique seen perform well domain-by-domain analysis across
broad set competition planning domains, full domain-by-domain results available
818

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

online appendix. However, compress summarize extensive per-problem results,
divided evaluation domains experimenter-defined categories aggregated performance measurement within problem category. categories single domains,
generally, multiple closely related domains may aggregated within single category.
example, multiple domains competitions variants blocks world,
problems domains aggregated B LOCKSWORLD category.
order fairly compare SEH FF-based planners (such RFF, described TeichteilKonigsbuch et al., 2010, FF-Replan) exploit blocksworld-targeted planning heuristics
added goal deletion goal agenda, provided heuristics extensions SEH.
resulting planner called SEH+ , described detail Section 3.6. results show SEH+
performs nearly identically SEH non-blocksworld categories using CR-FF heuristic.
employ extensions comparing SEH CR-FF heuristic planners.
Using experimenter-defined categories, able show SEH exploits heuristic
functions effectively greedy following heuristic. SEH statistically significantly
outperforms greedy following thirteen seventeen categories using CR-FF heuristics
losing one category. SEH outperforms greedy following six seven categories using learned heuristics. (In cases, categories showed similar performance
compared planners.)
show SEH+ , using CR-FF heuristics, outperforms FF-Replan ten
fifteen categories, similar performance two categories, losing three categories.
aggregate results show SEH+ (using CR-FF heuristics) particularly strong performance advantage FF-Replan probabilistically interesting categories (Little & Thiebaux,
2007).
Finally, compare performance SEH+ RFF-BG (Teichteil-Konigsbuch
et al., 2010), one winner fully-observable track third international probabilistic planning competition. SEH+ outperforms RFF-BG twelve fifteen categories, similar
performance one category, losing two categories.
summary, empirical work demonstrates SEH provides novel automatic technique
improving heuristic function using limited searches, simply applying SEH
reasonable heuristic functions produces state-of-the-art planner.

2. Technical Background: Markov Decision Processes
give brief review Markov decision processes (MDPs) specialized goal-region objectives.
detail MDPs, see work Bertsekas (1995), Puterman (2005), Sutton Barto
(1998).
2.1 Goal-Oriented Markov Decision Processes
Markov decision process (MDP) tuple (S, A, C, T, sinit ). Here, finite state space
containing initial state sinit , selects non-empty finite available action set A(s) state
S. action-cost function C assigns non-negative real action-cost state-action-state
triple (s, a, ) action enabled state s, i.e., A(s). transition probability
function maps state-action pairs (s, a) probability distributions S, P(S),
A(s).
819

fiW U , K ALYANAM , & G IVAN

represent goal, include zero-cost absorbing state , i.e., C(, a, s) =
0 (, a, ) = 1 A(). Goal-oriented MDPs MDPs
subset G statespace, containing , that: (1) C(g, a, ) zero whenever g G
one otherwise, (2) (g, a, ) one g G A(g). set G thus
taken define action-cost function C, well constrain transition probabilities .
(stochastic) policy MDP : N P(A) specifies distribution actions
state finite horizon. cost-to-go function J (s, k) gives expected cumulative
cost k steps execution starting state selecting actions according () state
encountered. horizon k, least one (deterministic) optimal policy (, k)

J (s, k), abbreviated J (s, k), greater J (s, k) every state s,
policy . following Q function evaluates action using provided cost-to-go function
J estimate value action applied,
X
Q(s, a, J) =
(s, a, )[C(s, a, ) + J(s )].


Recursive Bellman equations use Q() describe J J follows:
J (s, k) = E [Q(s, (s, k), J (, k 1))]
J (s, k) = min Q(s, a, J (, k 1)),
aA(s)

taking expectation random choice made possibly stochastic policy (s, k).
cases, zero step cost-to-go function zero everywhere, J (s, 0) = J (s, 0) = 0
s. Value iteration computes J (s, k) k increasing order starting zero. Note
policy cost function depend k, may drop k argument list.
using Q(), select action greedily relative cost function. policy
Greedy(J) selects, state horizon k, uniformly randomly selected action
argminaA(s) Q(s, a, J(, k 1)).
goal-based MDP problems directly specified above, may specified
exponentially compactly using planning languages PPDDL (Younes, Littman, Weissman, & Asmuth, 2005), used experiments. technique avoids converting
entire PPDDL problem explicitly form, resource reasons, instead constructs
sequence smaller problems explicit MDP form modeling heuristic flaws.
dead-end state state every policy zero probability reaching goal
horizon. say policy reaches region states probability one following policy
horizon k probability entering region point converges one k goes
infinity. say dead-ends unavoidable problem whenever policy sinit
reaches goal region probability one. (We say domain unavoidable dead-ends
problem domain unavoidable dead-ends.) note greedy techniques
hill-climbing expected perform poorly domains dead-end states attractive
heuristic values. Application SEH thus leaves responsibility detecting avoiding deadend states design heuristic function.
heuristic h : R may provided, intended estimate cost function J large
horizons, h(s) = 0 G, h(s) > 0 otherwise. heuristic may indicate dead-end
states returning large positive value V assume selected experimenter
exceed expected steps goal state reach goal. experiments,
820

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

add trivial, incomplete dead-end detection (described Section 5.2) heuristic function
evaluate.
note domains evaluated paper contain unavoidable deadends,
may policy success ratio one. choice large value used recognized
dead-end states effects trade-off optimizing success ratio optimizing expected cost
incurred goal successful.
2.2 Determinizing Stochastic Planning Problems
stochastic planners heuristic computation techniques, including used experiments, rely computing deterministic approximations stochastic problems. One planner,
all-outcomes FF-Replan (Yoon et al., 2007), determinizes stochastic planning problem
invokes deterministic planner (Hoffmann & Nebel, 2001) determinized problem.
determinization used FF-Replan constructed creating new deterministic action
possible outcome stochastic action ignoring probability outcome happening. effectively allows planner control randomness executing actions, making
determinization kind relaxation problem. Section 5.2, define domainindependent heuristic function, controlled-randomness heuristic (CR-FF), deterministic heuristic (Hoffmann & Nebel, 2001) computed all-outcomes FF-Replan determinization probabilistic problem3 . variety relaxations previously combined
variety deterministic heuristics order apply deterministic planning techniques stochastic problems (Bonet & Geffner, 2005). generally, deterministic relaxations provide general
technique transferring techniques deterministic planning use solution stochastic
problems.

3. Stochastic Enforced Hill-Climbing
Deterministic enforced hill-climbing (DEH) (Hoffmann & Nebel, 2001) searches successor
state strictly better heuristic value returns path current state successor.
path action sequence guarantees reaching desired successor. illustrate
behavior DEH compared greedy policy using example Figure 1. stochastic
environment, may single better descendant reached probability one,
since actions may multiple stochastic outcomes. simply use breadth-first search
DEH find single better descendant ignore possible outcomes, might end
selecting action low probability actually leading state better heuristic value,
illustrated Figure 2. shown figure, algorithm, stochastic enforced hill-climbing
(SEH), accurately analyzes probabilistic dynamics problem improving heuristic
value.
section, give details SEH. note DEH, local breadth-first search
gives local policy state region surrounding current state deterministic environment.
value following policy heuristic value improved descendant found
breadth-first search. SEH, implement ideas stochastic setting.
3. deterministic heuristic, described work Hoffmann Nebel (2001), planner version 2.3
available http://www.loria.fr/hoffmanj/ff.html, efficiently computes greedy plan length problem relaxation
state facts never deleted. plan found relaxed problem referred relaxed plan
problem.

821

fiW U , K ALYANAM , & G IVAN

h=7

h=7

h=5

h=5

h=7
h=8

h=0

h=7

h=8

h=8

h=0

h=6
h=8

h=6

h = 10

h = 10

(a) Behavior greedy policy.

(b) Behavior DEH.

Figure 1: Comparison behavior DEH greedy policy local optimum
encountered. solid black circle represents current state, shaded circle represents
goal state (with heuristic value zero). (a) greedy policy keeps selecting actions indicated
wide arrow cannot reach goal state. hand, DEH uses breadth-first search
finds goal state two steps away current state, shown (b).

h=7
h=5
h=7
h=8

h=7

p =0.2
p =0.8

h=6

h=5

h=2

h = 10

p =0.2

h=7

h=0

p =0.8
h=8

(a) Behavior DEH stochastic environments.

h=6

h=2

h=0

h = 10

(b) Behavior SEH stochastic environments.

Figure 2: Comparison behavior SEH DEH stochastic example. assume
DEH first determinizes problem, creating one deterministic action possible stochastic
outcome. solid black circle represents current state, shaded circle represents
goal state (with heuristic value zero). (a) DEH looks one step ahead selects action drawn
double lines, one outcomes leads state h = 2, better current
state. However, action choice higher probability going state h = 10
one h = 2. (b) SEH first decides policy better value 5
horizon MDP includes states reachable current state one step. SEH
extends horizon two states considered. selects actions indicated
wide arrows lead goal state.

822

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

Online Planning using Local Planner
1. Repeat
2.
current state
3.
local Find-Local-Policy(s,h)
4.
Follow local selected
5. goal reached
Table 1: Pseudo-code online planning framework. policy local may non-stationary,
case local planner returns initial horizon execution policy termination line 4 happen reaching specified horizon.

present SEH two steps. First, present simple general framework online planning repeatedly calls local planner selects policy around current state. Second,
present local planner based enforced hill-climbing idea. online planning
framework instantiated local planner, resulting algorithm SEH. combination
two steps constitute central algorithmic contribution paper. Finally, present
analytical properties algorithm.
3.1 Simple Online Planning Framework
familiar direct approach online planning call planner current state
planner select action. action executed environment, resulting new current
state. process repeated.
Here, present simple generalization approach allows planner select
one action call, action executed. idea planner makes
plan local context surrounding current state, plan executed local
context exited. local context exited, new current state process
repeated.
formally, augment action space new terminate action (called ), indicating planned-for local context exited. define local policy around state
partial mapping states augmented action space defined every
state reachable policy4 . online planner built repeatedly seeking
executing local policy around current state using planning subroutine. local policy
executed terminate action called (which effect state), point
new local policy must sought. ideas reflected pseudo-code shown Table 1.
note notion local context discussion informal precise
notion given use terminate action. local policy executed selects
terminate action. Find-Local-Policy routine free use method decide state
assigned terminate action. Previously published envelope methods (Dean, Kaelbling,
Kirman, & Nicholson, 1995) provide one way address issue, termination
assigned every state outside envelope states. However, framework general
envelope methods, allows local policies selected based upon pre-existing
4. local policy returned non-stationary finite horizon, must select terminate action
final stage, reachable states.

823

fiW U , K ALYANAM , & G IVAN

envelopes states (though always, post-planning, interpret set reachable states
envelope). general intuition selecting action current states may involve
analysis sufficient select actions many surrounding states, framework allows
Find-Local-Policy routine return policy specifying action selections.
Also, note online planning framework includes recent re-planners FFReplan (Yoon et al., 2007) RFF (Teichteil-Konigsbuch et al., 2010). However, replanning
current plan failed (e.g. determinization used generate naive)
quite different character SEH, constructs plans improve heuristic value,
replans time plan terminates. Thus, SEH uses heuristic function define subgoals
plan original goal incrementally.
remains present local planner combine online planning framework
define stochastic enforced hill climbing. local planner analyzes MDP problem around
current state, heuristic function integrated problem embody subgoal
improving heuristic value current state. describe simple integration
heuristic function problem next, discuss local planner based integration.
3.2 Heuristic-Based Markov Decision Processes
method relies finite horizon analyses transformed MDP problem, increasing horizons. transform MDP problem novel heuristic achievement transform analysis
order represent goal finding executing policy expects improve initial
(current) states heuristic value.
heuristic achievement transform straightforward, applies goal-oriented
MDP problem. First, action costs removed problem. Second, terminate action
assigned action cost h(s) transitions deterministically absorbing state .
policy executed, selection action state result replanning, discussed
online planning framework presented. actions thought heuristic
achievement actions, allowing immediate achievement value promised heuristic
function.
Analyzing MDP transformed heuristic achievement transform finite horizon
around s0 represents problem finding policy improving heuristic value s0
without regard cost achieving improvement heuristic. Allowing heuristic
achievement action selected point state reflects greedy nature goal:
planner forced look improvement found, long policy
initial state expects see improvement.
Formally, given MDP = (S, A, C, T, s0 ) non-negative heuristic function h : R,
heuristic achievement transform h, written Mh , given (S, , C , , s0 ),
, C , follows. Let s, s1 , s2 arbitrary states S. define (s)
A(s) {a }, take C (s1 , a, s2 ) = 0 (s1 , a, s2 ) = (s1 , a, s2 ) A(s1 ).
Finally, define (s, , ) = 1 C (s, , ) = h(s).
transformed MDP zero-cost policies states, immediate use. However, policies required select final action (at horizon one)
represent policies seeking get regions low heuristic value, whatever cost.
increasing-horizon search policies corresponds roughly breadth-first search
improved heuristic value deterministic enforced hill-climbing. Formally, define class
824

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

heuristic-achievement policies H class policies (s, k) satisfy (s, 1) = s.
define Jh (s, k) value minH J (s, k) heuristic transform MDP h h
policy achieves value. note that, due zeroing non-terminal action costs,
Jh (s, k) represents expected heuristic value achieved next execution ,
required horizon k before. Formally, define random variable state
h first executes trajectory s, Jh (s, k) = E[h(s )].
rough motivation setting action costs zero analysis heuristic-based
MDP actions considered method remediate flawed heuristic.
cumulative action cost required reach state improved heuristic value measure
magnitude flaw heuristic. Here, remove cost analysis order
directly express subgoal reaching state lower heuristic value. Including action costs
might, example, lead preferring cheap paths higher heuristic values (i.e., states worse
s0 ) expensive paths lower heuristic values found. basic motivation
enforced hill climbing strongly seek improved heuristic values. Instead diluting
subgoal adding action costs, methods seek shortest path heuristic improvement
analyzing heuristic-based MDP iteratively deepened finite horizon, discussed
next subsection. approach reasonable settings action
cost, finite-horizon value iteration stochastic-setting analogue uniform cost search.
settings varying action cost, future work needed adapt SEH usefully consider
cost without excessively diluting focus improving heuristic.
3.2.1 H EURISTIC ACHIEVEMENT VALUE TERATION
Following formalism value iteration Section 2.1, compute Jh (s, k) heuristic
achievement value iteration follows:
Jh (s, 1) = h(s),
Jh (s, k) = min Q(s, a, Jh (, k 1)) k 2.
aA (s)

non-stationary policy achieving cost-to-go given Jh (, k) computed using
following definition:
h (s, 1) = ,
h (s, k) = argminaA (s) Q(s, a, Jh (, k 1)) k 2.
Note Q() computed heuristic-achievement transformed MDP Mh equations.
technical reasons arise zero-cost loops present, require tie breaking
argmin h (s, k) favors action selected h (s, k 1) whenever one options.
prevent selection looping actions shorter, direct routes value.
3.3 Local Planner
consider method stochastic enforced hill-climbing uses online planning framework, presented Table 1, together local policy selection method solves
heuristic-achievement MDP (exactly, approximately, heuristically). Here, describe one
straightforward method local policy selection defining SEH-find-local-policy using finitehorizon value iteration. method generalizes breadth-first search used deterministic
825

fiW U , K ALYANAM , & G IVAN

enforced hill-climbing, seeks expected heuristic improvement rather deterministic
path improved heuristic value. sophisticated heuristic methods finite-horizon
value iteration considered implementation presented finds local MDP problems intractable. analytical results Section 3.4 apply method exactly solves
heuristic-achievement MDP, method presented Table 2; experimental results
conducted using implementation Table 2 well.
present pseudo-code SEH-Find-Local-Policy Table 2. heuristic function h respects
goals h(s) = 0 iff G. algorithm assumes non-negative heuristic function h : R
respects goals, input. SEH-Find-Local-Policy(s0 ,h) returns policy h horizon k.
policy computed states horizons needed order execute h s0 using
horizon k policy terminates.
Thus, lines 5 11 Table 2 , heuristic-achievement value iteration conducted increasing horizons around s0 , seeking policy improving h(s0 ). Note given horizon k + 1,
states reachable within k steps need included value iteration.
3.3.1 E ARLY ERMINATION
primary termination condition repeated local policy construction discovery policy
improving heuristic estimate initial state. discussed Proposition 1,
domains without deadends, SEH-Find-Local-Policy always find policy improving h(s0 ),
given sufficient resources.
However, badly flawed heuristic functions large enough horizons analyzed
SEH-Find-Local-Policy may unacceptably large given resource constraints. Moreover, domains unavoidable deadends, may horizon, however large, policy improving
heuristic initial state. reasons, practice, algorithm stops enlarging
horizon heuristic-based MDP analysis user-specified resource limits exceeded.
horizon-limited analysis heuristic-transform MDP construction yield
desired results inexpensively, biased random walk used seek new initial state. example, consider problem provided heuristic labels states reachable k steps
cost-to-go estimates similar, forming large plateau. analysis large
plateau exceeds resources available, biased random walk indicated, lack useful heuristic
guidance.
So, horizon k found Jh (s0 , k) < h(s0 ), system executes h s0
horizon k terminate action selected. resource limit exceeded without
finding horizon, system executes biased random walk length , terminating
action imposed states reachable biased random walk h(s) < h(s0 ).
additional biased random walk allows method retain beneficial properties
random exploration domains heuristic flaws large MDP analysis. resource
consumption threshold random walk triggered viewed parameter controlling
blend random walk MDP-based search used overcoming heuristic flaws.
currently principled way analyzing tradeoff resource consumption
cost switching biased random walk, determining switching. Instead,
use domain-independent resource limits described Section 5.1, determined
experimentation.
826

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

SEH-Find-Local-Policy(s0 ,h)
//
s0 current state.
//
h : {} R heuristic function, extended h() = 0.
//
assume global variable Mh heuristic-achievement
transform original problem h.
//
seek policy problem Mh achieving cost less h(s0 ).
1. k = 1
2. Repeat
3.
k =k+1
4.
// Compute Jh (s0 , k) Mh using value iteration
5.
Jh (, 1) = h(), h (, 1) = , n = 1
6.
Repeat
7.
n=n+1
8.
reachable s0 Mh using k n steps
9.
Jh (s, n) = minaA (s) Q(s, a, Jh (, n 1))
10.
h (s, n) = argminaA (s) Q(s, a, Jh (, n 1))
11.
n = k
12. Jh (s0 , k) < h(s0 ) resource consumption exceeds user-set limits
13. Jh (s0 , k) < h(s0 )
14.

15.
Return h horizon k
16.
else
17.
// Return -step biased random walk policy
18.
// Note: implementations compute lazily online
19.
n = 1
20.
state
21.
h(s) < h(s0 )
22.

23.
(s, n) selects probability one
24.
else
25.
(s, n) selects action A(s) probability
26.

P

eQ(s,a,h)
ai A(s) (e

Q(s,ai ,h) )

Return horizon

Table 2: Pseudo-code local planner used implement stochastic enforced hill-climbing.

827

fiW U , K ALYANAM , & G IVAN

Horizon-limited analysis heuristic-transform MDP may terminate without finding
horizon k Jh (s0 , k) < h(s0 ) entire reachable statespace explored,
presence deadends. may happen without exceeding available resources,
case fall back fixed number iterations standard VI original MDP model
(including action costs without heuristic transform) reachable states.
3.4 Analytical Properties Stochastic Enforced Hill-Climbing
deterministic settings, given sufficient resources dead-ends, enforced hill-climbing
guarantee finding deterministic path improved heuristic value (if nothing else, goal state
suffice). Given finite state space, guarantee implies guarantee repeated enforced
hill-climbing find goal.
situation subtle stochastic settings. problem dead-ends, every
state optimal policy reaches goal probability one. follows problems,
h assigning zero every goal state, every state real value > 0, horizon
k Jh (s, k) < . (Recall Jh analyzes heuristic transform MDP wherein action costs
dropped except h() must realized horizon one.) SEH-Find-Local-Policy(s,h)
considers k turn Jh (s, k) < h(s) have:

Proposition 1. Given non-goal state s, dead-ends, non-negative heuristic function
h : R respecting goals, sufficient resources, routine SEH-Find-Localpolicy(s,h) returns policy h horizon k expected return Jh (s, k) strictly
less h(s).
However, unlike deterministic setting, policy found routine SEH-Find-Local-Policy
expects improvement heuristic value. particular executions policy
current state may result degraded heuristic value.
Here, prove even stochastic settings, spite possibility poor results
one iteration, SEH reach goal region probability one, absence dead-end states
sufficient resources. practice, provision sufficient resources serious hurdle,
must addressed providing base heuristic modest-sized flaws.

Theorem 1. dead-end free domains, unbounded memory resources, SEH reaches goal region probability one.
Proof. Let x0 , x1 , x2 , . . . , xm , . . . random variables representing sequence
states assigned line 2 Table 1 execute SEH planning problem,
x0 initial state sinit . algorithm achieves x G ,
thus terminates, take xj+1 = xj j . (Note result xj G implies
xj+1 G, G goal region states.)
show arbitrary > 0 probability xm goal region
h(sinit )
least 1
, real value > 0 defined below. expression goes one
828

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

goes infinity, conclude SEH reaches goal region
probability one.
Proposition 1, non-goal state s, absent dead-ends sufficient
resources, one iteration SEH guaranteed return policy finite horizon
ks value Jh (s, ks ) improving h(s). Let = h(s) Jh (s, ks ) > 0 value
improvement horizon ks . finitely many non-goal
states, exists = minsSG > 0 improvement h(s) Jh (s, ks )
least . Consider arbitrary xi
/ G. Noting Jh (xi , kxi ) =
E[h(xi+1 )] due zero action costs Mh , follows immediately E[h(xi )
h(xi+1 )|xi
/ G] , G goal region states. Using xi G implies
xi+1 G h(xi ) = h(xi+1 ) = 0, write
E[h(xi ) h(xi+1 )]
=E[h(xi ) h(xi+1 )|xi
/ G]Qi
+ E[h(xi ) h(xi+1 )|xi G](1 Qi )

(1)

Qi , > 0,
defining Qj probability xj
/ G.
Now, lower-bound expected heuristic improvement E[h(x0 ) h(xm )] calls SEH-Find-Local-Policy, > 0. decompose expected
improvement calls SEH-Find-Local-Policy sum expected improvements individual calls. Then, lower-bounding sum using smallest
term, get
E[h(x0 ) h(xm )]
=


m1
X

i=0
m1
X

E[h(xi ) h(xi+1 )]
(2)
Qi (from Inequality 1)

i=0

mQm ,
Qm non-increasing, since xm1 G implies xm G.
Next, combine lower bound natural upper bound h(sinit ), since h
assumed non-negative (so E[h(x0 ) h(xm )] h(sinit )) x0 = sinit . Thus,
h(sinit ) Qm m.
h(s

)

init , converging zero
Therefore probability Qm xm
/ G
large SEH reaches goal probability one.

theorem assumes absence dead-ends, problems dead-ends covered theorem well dead-ends avoidable identified heuristic.
Specifically, may require heuristic function assigns state
policy reach goal state probability one. case, problem
converted form required theorem simply removing states assigned
consideration (either pre-processing local MDP construction).
829

fiW U , K ALYANAM , & G IVAN

3.5 Variants Extensions SEH
SEH based finite-horizon analysis MDP transformed heuristic-achievement transform around current state s0 . particular heuristic-achievement transform describe
course option incorporating heuristic local search around s0 .
already considered number related alternatives arriving choice describe,
options considered future research. One notable restriction transform
removal action costs, discussed Section 3.2. important method
retain actual heuristic value analysis trade large, small, positive
negative changes heuristic value according probabilities arising. reason,
heuristic transform abstract away value simply assign rewards 1
0 according whether state improves h(s0 ). choice remove action costs local
expansion lead poor performance domains flawed heuristics interacting badly
high variations action costs. subject future research method.
Also, MDP models describe paper limited obvious ways.
limitations include state space discrete finite, problem setting lacks discounting,
objective goal-oriented. yet implement extension relax limitations,
leave consideration issues arise future work. note would appear
method fundamentally goal-oriented, given goal repeatedly reducing heuristic value
current state. However, possible contemplate infinite-horizon discounted non-goaloriented variants seek policies maintain current heuristic estimate.
3.6 Incorporating Goal-Ordering Techniques SEH
planner contains heuristic elements inspired ordering issues arise blocksworld
problems (Hoffmann & Nebel, 2001). heuristic elements improve performance blocksworld problems significantly. assist fair comparison SEH FF-Replan,
implemented two heuristic elements, namely goal agenda added goal deletion, variant
SEH call SEH+ .
implementation SEH+ follows. stochastic planning problem first determinized using outcomes determinization described Section 2.2. goal-agenda technique invoked determinized problem extract sequence temporary goals
G1 , . . . , Gm , Gi set goal facts Gm original problem goal. SEH
stochastic version added goal deletion, described next subsection, invoked repeatedly compute sequence states s0 , . . . , sm , s0 initial state > 0 si
defined state reached invoking SEH state si1 goal Gi (thus satisfying Gi ).
Added goal deletion idea pruning state search space avoiding repetitive addition
deletion goal fact along searched paths. FF, search state s, goal fact
achieved action arriving s, deleted action relaxed plan found s,
expanded (Hoffmann & Nebel, 2001).
stochastic adaptation added goal deletion, define set facts added
state transition (s, a, ) facts true represent set difference
s. Then, set added goal facts transition added facts true
current temporary goal Gi , i.e., (s s) Gi . prune state transition (s, a, ) whenever
relaxed plan computed current temporary goal Gi contains action deletes
added goal facts. transition (s, a, ) pruned modifying Bellman update
830

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

state contributes dead-end state value (V ) Q-value s, weighted
transition probability (instead contributing cost-to-go ). formally, define
modified Q-function using added goal deletion, Qagd (s, a, J) follows:
(
1, f (s s) Gi deleted action relaxed plan(s ,Gi )5

I(s ) =
0, otherwise
X
Qagd (s, a, J) =
(s, a, )[I(s )V + (1 I(s ))J(s )]


Qagd () replaces Q() definition cost-to-go function Jh () Section 3.2. Also,
reachability line 8 Table 2 use pruned transitions.
problems, subsequent deletion newly added goals unavoidable valid plan.
Added goal deletion prunes routes leading goal region problems even though
actual deadend present. Hence, incomplete technique discussed work
Hoffmann Nebel (2001). falls back best-first search DEH able find valid
plan due pruning. Similarly, unable find improved policy, SEH falls back either
value iteration biased random walk described Section 3.3.
Preliminary exploration incorporating stochastic variants FFs helpful action pruning
(Hoffmann & Nebel, 2001) SEH improve performance, much effect added
goal deletion domains except blocksworld6 . result, report helpfulaction-pruning methods here.

4. Related Work
section discuss planning techniques close work one dimensions.
4.1 Fast-Foward (FF) Planner Deterministic Enforced Hill-Climbing
introduction deterministic enforced hill-climbing (DEH) relation technique,
please see Section 3. Here, additionally note several lines work directly
extend planner allow planning numeric state-variables (Hoffmann, 2003) planning uncertainty (Hoffmann & Brafman, 2006, 2005; Domshlak & Hoffmann, 2007). Although
techniques involve significant changes computation relaxed-plan heuristic
possible addition use belief states handle uncertainty, enforced hill-climbing still
primary search technique used lines work. note although work Domshlak Hoffmann actions probabilistic outcome handled, planner (Probabilistic-FF)
designed probabilistic planning observability, whereas planner designed
probabilistic planning full observability.
4.2 Envelope-Based Planning Techniques
Stochastic enforced hill-climbing dynamically constructs local MDPs find local policy leading
heuristically better state regions. concept forming local MDPs, envelopes, using
5. relaxed plan(s ,Gi ) computes relaxed plan states Gi defined work Hoffmann
Nebel (2001) using all-outcomes problem determinization defined Section 2.2.
6. explored ideas based defining helpfulness action expectation helpfulness
deterministic outcomes.

831

fiW U , K ALYANAM , & G IVAN

facilitate probabilistic planning used previous research work Bonet
Geffner (2006), Dean et al. (1995), briefly review here.
envelope-based methods work Dean et al. (1995) Gardiol Kaelbling (2003)
start partial policy restricted area problem (the envelope), iteratively improves solution quality extending envelope recomputing partial policy.
typical assumption implementing method planner initial trajectory
starting state goal, generated stochastic planner, use initial envelope.
Another line work, including RTDP (Barto, Bradtke, & Singh, 1995), LAO* (Hansen &
Zilberstein, 2001), LDFS (Bonet & Geffner, 2006), starts envelope containing
initial state, iteratively expands envelope expanding states. States expanded
according state values dynamic programming methods used backup state values
newly added states, convergence criterion reached. Stochastic enforced hill-climbing
viewed repeatedly deploying envelope method goal, time, improving
heuristic estimate distance-to-go. good h function, invocations result trivial
one-step envelopes. However, local optima plateaus encountered, envelope may
need grow locate stochastically reachable set exits.
referenced previous search methods constructed envelopes seeking high
quality policy goal rather far limited relatively inexpensive goal basin
escape. results derive online greedy exploitation heuristic rather
expensive offline computation converged values proving overall (near) optimality. LDFS,
example, compute/check values least states reachable optimal policy (even
given J input) possibly vastly many others well computation.
previous methods able exploit properties (such admissibility)
heuristic function guarantee avoiding state expansions regions state space. Clearly,
SEH exploits heuristic function way avoid expanding regions statespace.
However, point conducted theoretical analysis regions guaranteed unexpanded particular kinds heuristic, analyses may quite difficult.
4.3 Policy Rollout
technique policy rollout (Tesauro & Galperin, 1996; Bertsekas & Tsitsiklis, 1996) uses
provided base policy make online decisions. technique follows policy Greedy(Vf ),
Vf computed online sampling simulations policy .
computation optimal heuristic-transform policy h SEH similarities policy
rollout: case, online decisions made local probabilistic analysis leverages provided information manage longer-range aspects local choice. SEH, heuristic function
provided while, policy rollout, base policy provided. view, policy rollout local
analysis assumption non-local execution use base policy , whereas SEH
local analysis assumption non-local execution achieve base heuristic
cost estimate h.
fact, goal-oriented setting, provided heuristic function h stochastic (a simple generalization describe paper), equal sampled-simulation evaluation
V policy , SEH executes policy policy rollout, assuming uniform
action costs sufficient sampling correctly order action choices. claim follows h = V always action yield expected improvement h one step,
832

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

goal-oriented setting. need uniform action costs claim may relaxed
variant SEH developed retains action costs heuristic transform.
policy rollout, horizon-one greedy use sampled heuristic needed, main
substance SEH enable repair use heuristic functions flaws cannot
repaired horizon one. Thus central differences techniques reflected
ability SEH leverage arbitrary heuristic functions repair flaws functions larger
horizons.
Policy rollout provides elegant guarantee online policy selected improves base
policy, given sufficient sampling. result follows intuitively computed policy
policy-iteration improvement base policy. Unfortunately, similar guarantee known
apply SEH arbitrary heuristic function. However, policy rollout cannot used improve
arbitrary heuristic function either.
4.4 Local Search Optimization
Stochastic enforced hill-climbing regarded one many local-search techniques designed
improve greedy one-step lookahead, naive form local search optimization.
briefly discuss connections method simulated annealing, one large family related
local search techniques. detail, please see work Aarts Lenstra (1997).
Simulated annealing (Kirkpatrick et al., 1983; Cerny, 1985) allows selection actions
inferior expected outcome probability monotone action q-value. probability
inferior action selected often starts high decreases time according cooling schedule. ability select inferior actions leads non-zero probability escaping local
optima. However, method systematically search policy so. contrast,
stochastic enforced hill-climbing analyzes heuristic-based MDP increasing horizons systematically search policies give improved expected value (hence leaving local extrema).
substantial preliminary experiments, could find successful parameter settings control
simulated annealing effective application online action selection goal-directed stochastic
planning. knowledge, simulated annealing otherwise tested direct forwardsearch action selection planning, although variants applied success
planning-as-search settings (Selman, Kautz, & Cohen, 1993; Kautz & Selman, 1992; Gerevini &
Serina, 2003) planning via Boolean satisfiability search.

5. Setup Empirical Evaluation
Here, describe parameters used evaluating method, heuristics test
method on, problem categories tests conducted, random variables
aggregated evaluation, issues arising interpreting results statistical significance.
run experiments Intel Xeon 2.8GHz machines 533 MHz bus speed 512KB
cache.
5.1 Implementation Details
horizon increase new states reachable, implementation SEH simply switches
explicit statespace method solve MDP formed reachable states. specifically,
833

fiW U , K ALYANAM , & G IVAN

increase k line 3 Table 2 lead new reachable states line 8, trigger
value iteration states reachable s0 .
Throughout experiments, thresholds used terminate local planning line 12 Table 2
set 1.5 105 states one minute. set biased random walk length ten. work
makes assumption heuristic functions used assign large values easily recognized deadends, hill-climbing works poorly presence dead-end attractor states. enforce
requirement simple dead-end detection front-end heuristic function
(described next Section 5.2 heuristic) assigning value 1.0 105 recognized
dead-end states.
denote implementation running heuristic h SEH(h).
5.2 Heuristics Evaluated
describe two different types heuristic functions used evaluation associated
dead-end detection mechanisms.
5.2.1 C ONTROLLED -R ANDOMNESS H EURISTIC
use evaluations, define domain-independent heuristic function, controlledrandomness heuristic (CR-FF). define CR-FF state distance-to-goal
estimate (Hoffmann & Nebel, 2001) computed all-outcomes determinization described
Section 2.2. denote resulting heuristic function F . computing CR-FF heuristic,
use reachability analysis built planner detection deadends.
5.2.2 L EARNED H EURISTICS
test stochastic enforced hill-climbing automatically generated heuristic functions
work Wu Givan (2010), perform state-of-the-art used
construct greedy policy. shift heuristic functions fit non-negative range requirement h discussed previously. learned heuristic functions currently available
seven test categories, tested categories.
note heuristics learned discounted setting without action costs
direct fit distance-to-go formalization adopted here. still able get
significant improvements applying technique. denote heuristics L. states
valid action choice available labeled deadends applying SEH learned
heuristics.
5.3 Goals Evaluation
primary empirical goal show stochastic enforced hill-climbing generally improves
significantly upon greedy following heuristic (using policy Greedy(h) described
technical background above). show true heuristics defined
Section 5.2. show empirically applicability limitation SEH discussed Section 1.1,
different types problems including probabilistically interesting ones (Little & Thiebaux, 2007).
secondary goal evaluation show base heuristics resulting performance strong comparison deterministic replanners FF-Replan (Yoon et al., 2007)
RFF (Teichteil-Konigsbuch et al., 2010). FF-Replan RFF use Fast-Forward (FF)
834

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

base planner, RFF uses most-probable-outcome determinization contrast all-outcomes
determinization used FF-Replan. primary difference RFF FF-Replan
executing plan, RFF grows policy trees minimize probability
replan, FF-Replan not.
5.4 Adapting IPPC Domains Experiments
conduct empirical evaluation using problems first three international probabilistic planning competitions (IPPCs) well twelve probabilistically interesting problems
work Little Thiebaux (2007). omit particular problems domains
particular comparisons several practical reasons, detailed online appendix.
enforced hill-climbing nature goal-oriented technique SEH formulated
goal-oriented setting, ignore reward structure (including action goal rewards)
evaluated problems assume uniform action cost one problems,
transforming reward-oriented problem description goal-oriented one.
provide detailed per-problem results online appendix planner evaluated
work. However, support main conclusions, limit presentation aggregations
comparing pairs planners sets related problems. purpose, define seventeen
problem categories aggregate within problem category. categories single
domains, generally, multiple closely related domains may aggregated within single category. example, blocksworld category aggregates blocksworld problems three
competitions, even though action definitions exactly every problem.
paired comparisons, aggregated results problems labeled constructed
probabilistically interesting IPPC3 organizers work Little Thiebaux
(2007) combined category PI PROBLEMS.
Table 3, list evaluated categories (including combined category PI PROBLEMS),
well planning competitions literature problems category from.
evaluated problems category identified online appendix.
reward-oriented YSADMIN domain IPPC3 stochastic longest-path problem
best performance required avoiding goal continue accumulating reward long
possible (Bryce & Buffet, 2008). (Note contrary organizers report, domains goal
condition servers rather servers down.) goal-oriented adaptation removes
longest-path aspect domain, converting domain goal get
servers up.
B LOCKSWORLD problems IPPC2 contain flawed definitions may lead block
stacking top itself. Nevertheless, goal problems well defined achievable
using valid actions, hence problems included B LOCKSWORLD category.
discovered five rectangle-tireworld problems (p11 p15 IPPC3 2T IREWORLD) apparent bugno requirement remain alive included goal condition. domain design provides powerful teleport action non-alive agents intended
increase branching factor (Buffet, 2011). However, lacking requirement alive goal,
domain easily solved deliberately becoming non-alive teleporting goal.
modified problems require predicate alive goal region. merged
modified rectangle-tireworld problems triangle-tireworld problems IPPC3
835

fiW U , K ALYANAM , & G IVAN

Category

Problem Source(s)

B LOCKSWORLD

IPPC1, IPPC2, IPPC3

B OXWORLD

IPPC1, IPPC3

B USFARE

Little Thiebaux (2007)

RIVE

IPPC2

E LEVATOR

IPPC2

E XPLODING B LOCKSWORLD

IPPC1, IPPC2, IPPC3

F ILEWORLD

IPPC1

P ITCHCATCH

IPPC2

R ANDOM

IPPC2

R IVER

Little Thiebaux (2007)

CHEDULE

IPPC2, IPPC3

EARCH R ESCUE

IPPC3

YSADMIN

IPPC3

YSTEMATIC - TIRE

Triangle-tireworld (IPPC3 2-Tireworld P1 P10, Little Thiebaux (2007)),
Rectangle-tireworld (IPPC3 2-Tireworld P11 P15) bug fixed

IREWORLD

IPPC1, IPPC2

OWERS H ANOI

IPPC1

Z ENOTRAVEL

IPPC1, IPPC2

PI PROBLEMS

B USFARE, RIVE, E XPLODING B LOCKSWORLD
P ITCHCATCH, R IVER, CHEDULE, YSTEMATIC - TIRE, IREWORLD

Table 3: List categories planning competitions literature problems
category taken.

836

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

work Little Thiebaux (2007) category YSTEMATIC - TIRE, problems
systematically constructed emphasize PI features.
5.5 Aggregating Performance Measurements
experiments, designed repeatable aggregate measurements sample
many times order evaluate statistical significance. define random variables representing aggregate measurements describe sampling process, well method
evaluating statistical significance.
5.5.1 EFINING AMPLING AGGREGATE -M EASUREMENT R ANDOM VARIABLES
pair compared planners, define four random variables representing aggregate performance comparisons problems category. random variable based upon
sampling process runs planner five times problems category, aggregates
per-problem result computing mean. use five-trial runs reduce incidence lowsuccess planners failing generate plan length comparison. mean value five-trial run
sample value respective random variable.
First, per-problem success ratio (SR) fraction five runs succeed
problem. success ratio random variable category planner mean SR
across problems category.
Second, per-problem successful plan length (SLen) mean plan length successful
runs among five runs. order compare two planners plan length, define perproblem ratio jointly successful plan lengths (JSLEN-RATIO) two compared planners
follows. planners positive SR among five trials problem, JSLEN-RATIO
ratio SLen values two planners; otherwise, JSLEN-RATIO undefined
problem. use ratio lengths emphasize small plan length differences short solutions
long solutions, decrease sensitivity granularity action definitions.
mean JSLEN-RATIO random variable category pair planners
geometric mean JSLEN-RATIO across problems category JSLEN-RATIO
well defined. manner ensure two planners compared exactly set
problems. Note that, unlike SR, JSLEN-RATIO depends pair compared planners,
rather measurement single planner; ratio successful plan length
jointly solved problems two planners.
Similarly, per-problem ratio jointly successful runtimes (JSTIME-RATIO) defined
manner used comparing plan lengths. mean JSTIME-RATIO computed
geometric mean well-defined per-problem JSTIME-RATIO values.
JSLEN-RATIO JSTIME-RATIO ratios two measurements, use geometric mean aggregate per-problem results generate sample value, whereas use arithmetic
mean SR variables. Note geometric mean desired property planners tied overall (so geometric mean one), mean insensitive planner
given denominator ratio.
Thus, draw single sample four aggregate random variables (SR planner,
JSLEN-RATIO, JSTIME-RATIO) comparing two planners, run two planners
problem five times, computing per-problem values four variables, take (arith837

fiW U , K ALYANAM , & G IVAN

metic geometric) means per-problem variables get one sample aggregate variable. process used repeatedly draw many samples needed get significant results.
use plan-length cutoff 2000 attempt. attempt given time limit 30
minutes.
5.5.2 IGNIFICANCE P ERFORMANCE IFFERENCES B ETWEEN P LANNERS
general goal order pairs planners overall performance category problem.
this, must trade success rate plan length. take position significant
advantage success rate primary goal, plan length used determine preference
among planners success rate differences found significant.
determine significance three performance measurements (SR, JSLENRATIO, JSTIME-RATIO) using t-tests, ascribing significance results p-value
less 0.05. exact hypothesis tested form t-test used depends performance
measurement, follows:
1. SR use paired one-sided t-test hypothesis difference true means
larger 0.02.
2. JSLEN-RATIO use one-sample one-sided t-test hypothesis true geometric mean JSLEN-RATIO exceeds 1.05 (log true mean JSLEN-RATIO exceeds
log(1.05)).
3. JSTIME-RATIO use one-sample one-sided t-test hypothesis true
geometric mean JSTIME-RATIO exceeds 1.05 (log true mean JSTIME-RATIO
exceeds log(1.05)).
stop sampling performance variables achieved one following criteria, representing SR winner determined SR appears tied:
1. Thirty samples drawn p-value SR difference 0.05 0.5.
2. Sixty samples drawn p-value SR difference 0.05 0.1.
3. One hundred fifty samples drawn.
experiments present next, stopping rule leads 30 samples drawn
unless otherwise mentioned. Upon stopping, conclude ranking planners (naming
winner) either SR difference JSLEN-RATIO p-value 0.05, significant
SR differences used first determine winner. neither measure significant upon
stopping, deem experiment inconclusive.
Combining categories evaluations, aggregate results across multiple categories problem, e.g., combined category PI PROBLEMS. cases, effectively
defined one larger category, techniques defining performance measurements determining statistical significance Section 5.5. However, actually re-run
planners combined-category measurements. Instead, re-use planner runs used
single-category experiments. Rather use stopping rule described, compute
maximum number runs available combined categories use many samples
838

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

combined-category performance measurements. avoid double counting problem results,
treat combined categories separately analyzing results counting wins losses.

6. Empirical Results
present performance evaluation stochastic enforced hill-climbing (SEH) section.
experiments underlying results presented involve 169,850 planner runs 17 categories.
6.1 Summary Comparison
results Table 4 show that, CR-FF heuristic, SEH goal-ordering addedgoal-deletion enhancements (SEH+ (F )) improves significantly baseline SEH technique
(SEH(F )) category B LOCKSWORLD, show significant changes aggregated
performance non-blocksworld problems7 . remainder experiments involving CRFF, evaluate SEH+ (F ), noting comparison planners (FF-Replan RFF)
benefit goal-ordering added-goal-deletion enhancements base planner, FFplan.
results present next SEH+ (F ) show:
SEH+ (F ) significantly outperforms Greedy(F ) 13 categories, outperformed
Greedy(F ) CHEDULE. three categories comparison inconclusive (B USFARE, R IVER IREWORLD). See Table 5 details.
FF-Replan inapplicable two categories (IPPC3 EARCH - -R ESCUE IPPC3
YSADMIN). SEH+ (F ) significantly outperforms FF-Replan 10 categories, outperformed FF-Replan three categories (E XPLODING B LOCKSWORLD, P ITCHCATCH,
Z ENOTRAVEL). two categories comparison inconclusive (F ILE WORLD R IVER ). SEH+ (F ) significantly outperforms FF-Replan combined
category PI PROBLEMS, although winner varied aggregated categories. See
Table 6 details.
RFF-BG inapplicable two categories (B USFARE IPPC1 F ILEWORLD). SEH+ (F )
significantly outperforms RFF-BG 12 categories, outperformed RFF-BG two
categories (E XPLODING B LOCKSWORLD YSTEMATIC - TIRE). one category
comparison inconclusive (S YSADMIN). SEH+ (F ) significantly outperforms RFF-BG combined category PI PROBLEMS, although winner varied
aggregated categories. See Table 7 details.
learned heuristic work Wu Givan (2010) computed
subset domains, hence seven categories applicable evaluation using
learned heuristic (see online appendix details). results present next SEH
learned heuristic, SEH(L), show:
SEH(L) significantly outperforms Greedy(L) six categories. one category
(T IREWORLD) comparison inconclusive. See Table 8 details.
7. show p-values rounded two decimal places. example, show p=0.00 value p rounded two
decimal places 0.

839

fiW U , K ALYANAM , & G IVAN

SR
SR
SEH+ (F ) SEH(F )

Category

JSLENRATIO
(SEH/
SEH+ )

JSTIMERATIO
(SEH/
SEH+ )

SR
Difference
Significant?
(p-value)

JSLENRATIO
Significant?
(p-value)

Winner

B LOCKSWORLD

0.93

0.72

1.58

2.85

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

N - BLOCKSWORLD

0.69

0.69

1.01

0.97



(p=1.00)



(p=1.00)

Inconclusive

Table 4: Aggregated comparison SEH+ (F ) SEH(F ).

SEH(L) significantly outperforms FF-Replan five categories, outperformed FFReplan two categories (E XPLODING B LOCKSWORLD Z ENOTRAVEL). See Table 9
details.
6.2 Discussion
discuss results comparisons pairs planners, including SEH versus greedy
heuristic-following, SEH versus FF-Replan, SEH versus RFF-BG.
6.2.1 SEH/SEH+ V ERSUS G REEDY
primary evaluation goal show stochastic enforced hill-climbing generally improves
significantly upon greedy following heuristic (using policy Greedy(h) described
technical background above). demonstrated evaluating SEH two different
heuristics Tables 5 8, SEH(h) significantly outperforms Greedy(h) nineteen
twenty-four heuristic/category pairs, losing CHEDULE SEH+ (F ) Greedy(F ).
discuss category Greedy outperforms SEH techniques significantly.
CHEDULE, multiple classes network packets different arrival rates. Packets deadlines, packet served deadline, agent encounters classdependent risk death well delay packet cleaned up. reach goal
serving packet every class, agent must minimize dropping-related risk dying
waiting arrival low-arrival-rate class. all-outcomes determinization underlying
CR-FF heuristic gives deterministic domain definition dying optional (never chosen)
unlikely packet arrivals happen choice, leading optimistic heuristic value.
using optimistic heuristic value, basic local goal SEH, improve
current state heuristic, leads building large local MDPs analysis. presence
dead-ends (death, above), even arbitrarily large local MDPs may able achieve local
improvement, CHEDULE, SEH+ typically hit resource limit MDP size
every action step.
contrast, greedy local decision making well suited packet scheduling. Many well known
packet scheduling policies (e.g. earliest deadline first static priority work Liu &
Layland, 1973) make greedy local decisions practically quite effective. experiments,
Greedy policy applied CR-FF benefits locally seeking avoid incidental delays
dropped-packet cleanup: even though heuristic sees risk-of-dying cost dropping, still
recognizes delay cleaning lost dropped packets. Thus, Greedy(F ) class-insensitive
840

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

JSLENSR
RATIO
SR
SEH+ (F ) Greedy(F ) (Greedy/
SEH+ )

Category

JSTIMERATIO
(Greedy/
SEH+ )

SR
Difference
Significant?
(p-value)

JSLENRATIO
Significant?
(p-value)

Winner

B LOCKSWORLD

0.93

0.35

1.40

0.63

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

B OXWORLD

0.99

0.05

1.18

1.12

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

B USFARE

1.00

0.99

0.85

0.86



(p=0.97)



(p=0.21)

Inconclusive

RIVE

0.69

0.35

1.60

1.41

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

E LEVATOR

1.00

0.40

1.82

1.81

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

E XPLODING
B LOCKSWORLD

0.44

0.18

1.01

0.63

YES

(p=0.00)



(p=0.93)

SEH+ (F )

F ILEWORLD

1.00

0.21

1.03

0.24

YES

(p=0.00)



(p=1.00)

SEH+ (F )

P ITCHCATCH

0.45

0.00





YES

(p=0.00)

R ANDOM

0.99

0.94

1.76

0.59

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

R IVER

0.66

0.67

0.97

0.98



(p=0.60)



(p=0.75)

Inconclusive

CHEDULE

0.54

0.60

1.18

0.32

YES

(p=0.00)

YES

(p=0.01)

Greedy(F )

EARCH
R ESCUE

1.00

1.00

1.23

1.08



(p=1.00)

YES

(p=0.00)

SEH+ (F )

YSADMIN

0.27

0.27

1.21

1.23



(p=1.00)

YES

(p=0.00)

SEH+ (F )

YSTEMATIC
- TIRE

0.29

0.21

1.03

0.72

YES

(p=0.00)



(p=0.86)

SEH+ (F )

IREWORLD

0.91

0.90

0.96

0.79



(p=0.93)



(p=0.74)

Inconclusive

OWERS
H ANOI

0.53

0.00





YES

(p=0.00)

0.90

0.20

1.31

0.74

YES

(p=0.00)



Z ENOTRAVEL




YES

(p=0.00)

SEH+ (F )

SEH+ (F )
SEH+ (F )

Table 5: Aggregated comparison SEH+ (F ) Greedy(F ). R IVER domain evaluation required extending sampling 60 samples per experimental protocol described Section 5.5.2. values p-values JSLEN-RATIO JSTIME-RATIO P ITCHCATCH
OWERS H ANOI available due zero success ratio Greedy(F ) categories.

841

fiW U , K ALYANAM , & G IVAN

JSLENSR
SR
RATIO
SEH+ (F ) FF-Replan (FFR/
SEH+ (F ))

Category

JSTIMERATIO
(FFR/
SEH+ (F ))

SR
Difference
Significant?
(p-value)

JSLENRATIO
Significant?
(p-value)

Winner

B LOCKSWORLD

0.93

0.87

1.33

1.17

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

B OXWORLD

0.99

0.88

3.93

1.57

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

B USFARE

1.00

0.01

0.00

0.00

YES

(p=0.00)

RIVE

0.69

0.54

1.26

2.42

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

E LEVATOR

1.00

0.93

0.95

0.93

YES

(p=0.00)



(p=0.36)

SEH+ (F )

E XPLODING
B LOCKSWORLD

0.44

0.44

0.85

0.56



(p=0.96)

YES

(p=0.00)

FF-Replan

F ILEWORLD

1.00

1.00

0.97

0.57



(p=1.00)



(p=1.00)

Inconclusive

P ITCHCATCH

0.45

0.51

2.78

0.21

YES

(p=0.00)

YES

(p=0.00)

FF-Replan

R ANDOM

0.99

0.96

1.37

0.19

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

R IVER

0.66

0.65

0.94

0.93



(p=0.60)



(p=0.33)

Inconclusive

CHEDULE

0.54

0.48

1.04

0.10

YES

(p=0.00)



(p=0.59)

SEH+ (F )

YSTEMATIC
- TIRE

0.29

0.07

0.36

0.38

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

IREWORLD

0.91

0.69

0.69

0.57

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

OWERS
H ANOI

0.59

0.50

0.64

0.06

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

Z ENOTRAVEL

0.90

1.00

0.70

0.10

YES

(p=0.00)

YES

(p=0.00)

FF-Replan

PI
P ROBLEMS

0.55

0.45

1.02

0.54

YES

(p=0.00)



(p=1.00)

SEH+ (F )





SEH+ (F )

Table 6: Aggregated comparison SEH+ (F ) FF-Replan (FFR). R ANDOM R IVER
domains required extending sampling 60 samples OWERS H ANOI domain required
extending sampling 150 samples per experimental protocol described Section 5.5.2.
p-value JSLEN-RATIO B USFARE available extremely low success rate
FFR leads one sample JSLEN gathered 30 attempts, yielding estimated
variance.

842

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

SR
SR
SEH+ (F ) RFF-BG

Category

JSLENRATIO
(RFF-BG/
SEH+ (F ))

JSTIMERATIO
(RFF-BG/
SEH+ (F ))

SR
Difference
Significant?
(p-value)

JSLENRATIO
Significant?
(p-value)

Winner

B LOCKSWORLD

0.93

0.77

0.79

0.22

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

B OXWORLD

0.99

0.89

1.03

3.70

YES

(p=0.00)



(p=1.00)

SEH+ (F )

RIVE

0.69

0.61

1.07

1.24

YES

(p=0.00)



(p=0.08)

SEH+ (F )

E LEVATOR

1.00

1.00

1.27

0.15



(p=1.00)

YES

(p=0.00)

SEH+ (F )

E XPLODING
B LOCKSWORLD

0.44

0.43

0.84

0.56



(p=0.92)

YES

(p=0.00)

RFF-BG

P ITCHCATCH

0.45

0.00





YES

(p=0.00)

R ANDOM

0.99

0.74

1.26

0.56

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

R IVER

0.66

0.51

0.77

0.21

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

CHEDULE

0.54

0.43

1.06

0.08

YES

(p=0.00)



(p=0.40)

SEH+ (F )

EARCH
R ESCUE

1.00

0.01

2.99

0.86

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

YSADMIN

0.27

0.27

1.10

9.31



(p=1.00)



(p=0.05)

Inconclusive

YSTEMATIC
- TIRE

0.29

0.81

1.22

4.49

YES

(p=0.00)

YES

(p=0.00)

RFF-BG

IREWORLD

0.91

0.71

0.68

0.21

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

OWERS
H ANOI

0.58

0.48

0.64

0.01

YES

(p=0.03)

YES

(p=0.00)

SEH+ (F )

Z ENOTRAVEL

0.90

0.02

1.20

0.04

YES

(p=0.00)



(p=0.27)

SEH+ (F )

PI
P ROBLEMS

0.55

0.51

0.91

0.50

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )





SEH+ (F )

Table 7: Aggregated comparison SEH+ (F ) RFF-BG. R IVER OWERS
H ANOI domains required extending sampling 60 samples per experimental protocol described Section 5.5.2. values p-values JSLEN-RATIO JSTIME-RATIO P ITCH CATCH available due zero success ratio RFF-BG category.

843

fiW U , K ALYANAM , & G IVAN

SR
SEH(L)

Category

JSLENSR
RATIO
Greedy(L) (Greedy/
SEH)

JSTIMERATIO
(Greedy/
SEH)

SR
Difference
Significant?
(p-value)

JSLENRATIO
Significant?
(p-value)

Winner

B LOCKSWORLD

1.00

1.00

7.00

3.69



(p=1.00)

YES

(p=0.00)

SEH(L)

B OXWORLD

0.89

0.89

5.00

0.55



(p=1.00)

YES

(p=0.00)

SEH(L)

E XPLODING
B LOCKSWORLD

0.10

0.02

1.09

1.00

YES

(p=0.00)



(p=0.31)

SEH(L)

YSTEMATIC
- TIRE

0.34

0.14

0.75

0.39

YES

(p=0.00)

YES

(p=0.00)

SEH(L)

IREWORLD

0.90

0.89

1.05

1.05



(p=0.92)



(p=0.60)

Inconclusive

OWERS
H ANOI

0.60

0.00





YES

(p=0.00)

0.58

0.03

13.25

5.66

YES

(p=0.00)



Z ENOTRAVEL


YES

(p=0.00)

SEH(L)
SEH(L)

Table 8: Aggregated comparison SEH(L) Greedy(L). values JSLEN-RATIO
JSTIME-RATIO p-value JSLEN-RATIO OWERS H ANOI available due
zero success ratio Greedy(L) category.

SR
SEH(L)

Category

JSLENSR
RATIO
FF-Replan (FFR/
SEH(L))

JSTIMERATIO
(FFR/
SEH(L))

SR
Difference
Significant?
(p-value)

JSLENRATIO
Significant?
(p-value)

Winner

B LOCKSWORLD

1.00

0.83

0.99

2.06

YES

(p=0.00)



(p=1.00)

SEH(L)

B OXWORLD

0.89

0.88

3.61

0.54



(p=0.97)

YES

(p=0.00)

SEH(L)

E XPLODING
B LOCKSWORLD

0.10

0.46

0.71

0.73

YES

(p=0.00)

YES

(p=0.00)

FF-Replan

YSTEMATIC
- TIRE

0.34

0.10

0.28

0.18

YES

(p=0.00)

YES

(p=0.00)

SEH(L)

IREWORLD

0.90

0.70

0.66

0.51

YES

(p=0.00)

YES

(p=0.00)

SEH(L)

OWERS
H ANOI

0.60

0.42

0.64

4.76

YES

(p=0.00)

YES

(p=0.00)

SEH(L)

0.58

1.00

0.58

0.03

YES

(p=0.00)

YES

(p=0.00)

FF-Replan



Z ENOTRAVEL

Table 9: Aggregated comparison SEH(L) FF-Replan (FFR).

844

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

policy greedily seeks avoid dropping, similar earliest deadline first. problems
SEH encounters evaluation CHEDULE suggest future work automatically recognizing
domains large MDP construction proving futile automatically reducing MDP size
limits adapt performance towards behavior greedy policy. note across tested
benchmark domains heuristics, one domain/heuristic combination
phenomenon arose practice.
6.2.2 SEH/SEH+ V ERSUS FF-R EPLAN



RFF-BG

demonstrated performance improvement SEH+ (F ) best performing planners first three international probabilistic planning competitions, outperforming FF-Replan
ten fifteen categories losing three (E XPLODING B LOCKSWORLD, P ITCHCATCH,
Z ENOTRAVEL), outperforming RFF-BG 12 15 categories losing E XPLODING
B LOCKSWORLD YSTEMATIC - TIRE. Additionally, SEH(L) outperforms FF-Replan five
seven categories losing E XPLODING B LOCKSWORLD Z ENOTRAVEL. section
discuss categories SEH+ (F ) SEH(L) lose FF-Replan RFF-BG.
Z ENOTRAVEL logistics domain people transported cities via airplanes
load/unload/fly action non-zero probability effect. result, takes
uncertain number attempts complete task. domains probabilistic effect choice change change, all-outcome determinization leads
safe determinized plan FF-Replanone replanning needed reach goal.
domains, including Z ENOTRAVEL, all-outcomes determinization provide effective
way employ deterministic enforced hill-climbing problem. note though though,
determinization still ignores probabilities action outcomes, lead bad
choices domains (not Z ENOTRAVEL). deterministic stochastic enforced
hill-climbing must climb large basins Z ENOTRAVEL, substantial overhead stochastic backup computations basin expansion leads least constant factor advantage deterministic expansion. extension SEH might address problem successfully future
research would detect domains stochastic choice change non-change,
handle domains emphasis determinization.
E XPLODING B LOCKSWORLD variant blocks world two new predicates detonated destroyed. block detonate once, put-down, probability,
destroying object placed upon. state resulting action depicted Figure 3 delete-relaxed path goal, actual path, state dead-end attractor
delete-relaxation heuristics CR-FF. FF-Replan RFF-BG never select action
path goal including action. SEH+ (F ) weak dead-end detection used experiments select dead action shown, resulting poor performance
situation arises. would possible use all-outcomes determinization improved
dead-end detector conjunction SEH+ (F ) order avoid selecting actions.
dead-end detection would carefully implemented managed control run-time
costs incurred SEH relies critically able expand sufficiently large local MDP regions
online action selection.
P ITCHCATCH, unavoidable dead-end states (used domain designers simulate cost penalties). However, CR-FF heuristic, based all-outcomes determinization, assigns optimistic values correspond assumed avoidance dead-end states.
845

fiW U , K ALYANAM , & G IVAN

Current
State

b3

b5

b4

b1

b2

b5

b3

b2

b4

Goal State

Path
Destroyed Table

Pick table b3

b3

b5

b4

b1

b2

Destroyed Table

Figure 3: illustration critical action choice SEH+ (F ) E XPLODING B LOCKSWORLD
problem (IPPC2 P1). middle state actual path goal delete-relaxed path
goal. Due table exploded, block placed onto table, resulting
middle state dead-end state. middle state dead-end attractive heuristic
value without regard whether blocks shown remaining explosive charge not,
state feature shown.
result, local search SEH+ (F ) unable find expected improvement CR-FF values,
falls back biased random walk domain. domain suggests, domains SEH+ (F ) performs weakly, work needed managing domains
unavoidable deadend states.
two categories SEH(L) loses FF-Replan (E XPLODING B LOCKSWORLD
Z ENOTRAVEL) categories SEH+ (F ) loses FF-Replan. Greedily following
learned heuristics two categories leads lower success ratio greedily following CRFF, suggesting significant flaws learned heuristics CR-FF. Although SEH able
give least five-fold improvement greedy following, success ratio two categories, improvement large enough SEH(L) match performance SEH+ (F )
FF-Replan, based relaxed-plan heuristic FF.
SEH+ loses RFF YSTEMATIC - TIRE due weak performance Triangle Tireworld problems. Triangle Tireworld provides map connected locations arranged single
safe path source destination, exponentially many shorter unsafe paths8 .
Determinizing heuristics detect risk unsafe paths greedy following
heuristics lead planners (such SEH+ ) take unsafe paths, lowering success rate.
results show SEH+ often repair flawed heuristic, Triangle Tireworld domain heuristic attracts SEH+ apparent improvements actually dead-ends.
contrast, RFF designed increase robustness determinized plans high probability failure. RFF continue planning avoid failure rather relying replanning
failure. initial determinized plan high probability failure (relative RFFs
8. safe path drawn following two sides triangular map, many unsafe paths interior
triangle. Safety domain represented presence spare tires repair flat tire 50%
chance occurring every step.

846

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

JSLENRATIO
(FFR/
SEH+ )

JSTIMERATIO
(FFR/
SEH+ )

SR
Difference
Significant?
(p-value)

JSLENRATIO
Significant?
(p-value)

SR
SEH+ (F )

SR
FFReplan

B LOCKSWORLD

0.70

0.37

0.72

0.88

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

B OXWORLD

0.67

0.34

5.02

0.98

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

Category

Winner

Table 10: Aggregated comparison SEH+ (F ) FF-Replan scaled-up problems.

Category

SR
SEH+ (F )

SR
RFFBG

JSLENRATIO
(RFFBG/
SEH+ )

JSTIMERATIO
(RFFBG/
SEH+ )

SR
Difference
Significant?
(p-value)

B LOCKSWORLD

0.70

0.33

0.46

0.14

YES

(p=0.00)

B OXWORLD

0.67

0.00

0.88

10.81

YES

(p=0.00)

JSLENRATIO
Significant?
(p-value)

YES

(p=0.00)


Winner

SEH+ (F )
SEH+ (F )

Table 11: Aggregated comparison SEH+ (F ) RFF-BG scaled-up problems.

threshold), RFF extends plan execution often detect need use longer,
safe route.
6.2.3 P ERFORMANCE L ARGE P ROBLEMS
order demonstrate advantages SEH emphasized problem size grows,
present aggregated performance SEH+ (F ) additional large-sized problems generated using generators provided first IPPC. scaling experiments computationally
expensive, run two domains widely evaluated planning literature: B LOCKSWORLD B OXWORLD (which stochastic version logistics).
B LOCKSWORLD, generated 15 problems 25- 30-block problems. B OXWORLD,
generated 15 problems size 20 cities 20 boxes. (Only one problem across three
competitions reached size B OXWORLD, problem unsolved competition
winner, RFF.) aggregated results FF-Replan RFF-BG presented Tables 10
11. experiments scaled-up problems consumed 3,265 hours CPU time
show SEH+ (F ) successfully completed majority attempts FF-Replan RFF
succeeded substantially less often9 .
Note although heuristic good B OXWORLD logistics domains,
failure all-outcomes determinization take account probabilities action outcomes
quite damaging FFR B OXWORLD, leading planner often select action hoping
9. statistical protocol requires 30 samples random variable averaging performance 5 solution attempts,
planner problem. 45 problems 3 planners, yields 30*5*45*3=20,250 solution attempts,
taking approximately 10 CPU minutes large problems.

847

fiW U , K ALYANAM , & G IVAN

low-probability error outcome. note RFF uses most-probable-outcome determinization
suffer issues FFR boxworld. Given high accuracy
heuristic boxworld, believe ideas RFF likely re-implemented and/or
tuned achieve better scalability boxworld problems. leave possibility direction
future work understanding scalability RFF.

7. Summary
proposed evaluated stochastic enforced hill-climbing, novel generalization
deterministic enforced hill-climbing method used planner (Hoffmann & Nebel, 2001).
Generalizing deterministic search descendant strictly better current state
heuristic value, analyze heuristic-based MDP around local optimum plateau reached
increasing horizons seek policy expects exit MDP better valued state.
demonstrated approach provides substantial improvement greedy hill-climbing
heuristics created using two different styles heuristic definition. demonstrated
one resulting planner substantial improvement FF-Replan (Yoon et al., 2007)
RFF (Teichteil-Konigsbuch et al., 2010) experiments.
find runtime stochastic enforced hill-climbing concern domains.
One reason long runtime number size local optima basins plateaus may
large. Currently, long runtime managed primarily reducing biased random walk
resource consumption exceeds user-set thresholds. possible future research direction regarding
issue prune search space automatically state action pruning.

Acknowledgments
material based upon work supported part National Science Foundation, United
States Grant No. 0905372 National Science Council, Republic China (98-2811M-001-149 99-2811-M-001-067).

References
Aarts, E., & Lenstra, J. (Eds.). (1997). Local Search Combinatorial Optimization. John Wiley &
Sons, Inc.
Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning act using real-time dynamic programming. Artificial Intelligence, 72, 81138.
Bertsekas, D. P. (1995). Dynamic Programming Optimal Control. Athena Scientific.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific.
Bonet, B., & Geffner, H. (2005). mGPT: probabilistic planner based heuristic search. Journal
Artificial Intelligence Research, 24, 933944.
Bonet, B., & Geffner, H. (2006). Learning depth-first search: unified approach heuristic search
deterministic non-deterministic settings, application MDPs. Proceedings
Sixteenth International Conference Automated Planning Scheduling, pp. 142
151.
848

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

Bryce, D., & Buffet, O. (2008). International planning competition uncertainty part: Benchmarks
results.. http://ippc-2008.loria.fr/wiki/images/0/03/Results.pdf.
Buffet, O. (2011) Personal communication.
Cerny, V. (1985). Thermodynamical approach traveling salesman problem: efficient simulation algorithm. J. Optim. Theory Appl., 45, 4151.
Dean, T., Kaelbling, L. P., Kirman, J., & Nicholson, A. (1995). Planning time constraints
stochastic domains. Artificial Intelligence, 76, 3574.
Domshlak, C., & Hoffmann, J. (2007). Probabilistic planning via heuristic forward search
weighted model counting. Journal Artificial Intelligence Research, 30, 565620.
Fahlman, S., & Lebiere, C. (1990). cascade-correlation learning architecture. Advances
Neural Information Processing Systems 2, pp. 524 532.
Gardiol, N. H., & Kaelbling, L. P. (2003). Envelope-based planning relational MDPs. Proceedings Seventeenth Annual Conference Advances Neural Information Processing
Systems.
Gerevini, A., & Serina, I. (2003). Planning propositional CSP: Walksat local search
techniques action graphs. Constraints, 8(4), 389413.
Gordon, G. (1995). Stable function approximation dynamic programming. Proceedings
Twelfth International Conference Machine Learning, pp. 261268.
Hansen, E., & Zilberstein, S. (2001). LAO*: heuristic search algorithm finds solutions
loops. Artificial Intelligence, 129, 3562.
Hoffmann, J. (2003). Metric-FF planning system: Translating ignoring delete lists numeric
state variables. Journal Artificial Intelligence Research, 20, 291341.
Hoffmann, J., & Brafman, R. (2005). Contingent planning via heuristic forward search implicit
belief states. Proceedings 15th International Conference Automated Planning
Scheduling.
Hoffmann, J., & Brafman, R. (2006). Conformant planning via heuristic forward search: new
approach. Artificial Intelligence, 170(6-7), 507 541.
Hoffmann, J. (2005). ignoring delete lists works: Local search topology planning benchmarks. Journal Artificial Intelligence Research, 24, 685758.
Hoffmann, J., & Nebel, B. (2001). planning system: Fast plan generation heuristic
search. Journal Artificial Intelligence Research, 14, 253302.
Kautz, H., & Selman, B. (1992). Planning satisfiability. Proceedings Tenth European
Conference Artificial Intelligence (ECAI92).
Kirkpatrick, S., Gelatt, Jr, C., & Vecchi, M. (1983). Optimization simulated annealing. Science,
220, 671680.
Little, I., & Thiebaux, S. (2007). Probabilistic planning vs replanning. Workshop International
Planning Competition: Past, Present Future (ICAPS).
Liu, C., & Layland, J. (1973). Scheduling algorithms multiprogramming hard-real-time
environment. Journal Association Computing Machinery, 20, 4661.
849

fiW U , K ALYANAM , & G IVAN

Mahadevan, S., & Maggioni, M. (2007). Proto-value functions: Laplacian framework learning representation control Markov decision processes. Journal Machine Learning
Research, 8, 21692231.
Nilsson, N. (1980). Principles Artificial Intelligence. Tioga Publishing, Palo Alto, CA.
Puterman, M. L. (2005). Markov Decision Processes: Discrete Stochastic Dynamic Programming.
John Wiley & Sons, Inc.
Sanner, S., & Boutilier, C. (2009). Practical solution techniques first-order MDPs. Artificial
Intelligence, 173(5-6), 748788.
Selman, B., Kautz, H., & Cohen, B. (1993). Local search strategies satisfiability testing.
DIMACS Series Discrete Mathematics Theoretical Computer Science, pp. 521532.
Sutton, R. S. (1988). Learning predict methods temporal differences. Machine Learning,
3, 944.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. MIT Press.
Teichteil-Konigsbuch, F., Kuter, U., & Infantes, G. (2010). Incremental plan aggregation generating policies MDPs. Proceedings Ninth International Conference Autonomous
Agents Multiagent Systems (AAMAS 2010), pp. 12311238.
Tesauro, G., & Galperin, G. (1996). On-line policy improvement using Monte-Carlo search.
NIPS.
Wu, J., & Givan, R. (2007). Discovering relational domain features probabilistic planning.
Proceedings Seventeenth International Conference Automated Planning
Scheduling, pp. 344351.
Wu, J., & Givan, R. (2010). Automatic induction Bellman-Error features probabilistic planning. Journal Artificial Intelligence Research, 38, 687755.
Yoon, S., Fern, A., & Givan, R. (2007). FF-Replan: baseline probabilistic planning. Proceedings Seventeenth International Conference Automated Planning Scheduling, pp. 352358.
Younes, H., Littman, M., Weissman, D., & Asmuth, J. (2005). first probabilistic track
international planning competition. Journal Artificial Intelligence Research, 24, 851887.

850


