journal artificial intelligence

submitted published

stochastic enforced hill climbing
jia hong wu

jw alumni purdue edu

institute statistical science
academia sinica taipei taiwan roc

rajesh kalyanam
robert givan

rkalyana purdue edu
givan purdue edu

electrical computer engineering
purdue university w lafayette usa

abstract
enforced hill climbing effective deterministic hill climbing technique deals local optima breadth first search process called basin flooding propose evaluate
stochastic generalization enforced hill climbing online use goal oriented probabilistic assume provided heuristic function estimating expected cost
goal flaws local optima plateaus thwart straightforward greedy action choice
breadth first search effective exploring basins around local optima deterministic stochastic dynamically build solve heuristic markov decision
process mdp model basin order good escape policy exiting local optimum
note building model involves integrating heuristic mdp
local goal improve heuristic
evaluate proposal twenty four recent probabilistic competition benchmark
domains twelve probabilistically interesting recent literature evaluation
stochastic enforced hill climbing seh produces better policies greedy heuristic
following value cost functions derived two different ways one type derived
deterministic heuristics deterministic relaxation second type derived automatic learning bellman error features domain specific experience first type heuristic
seh shown generally outperform planners first three international probabilistic
competitions

introduction
heuristic estimates distance goal long used deterministic search deterministic estimates typically flaws local extrema plateaus limit
utility methods simulated annealing kirkpatrick gelatt vecchi cerny
nilsson search developed handling flaws heuristics
recently excellent practical obtained flooding local optima breadth first
search method called enforced hill climbing hoffmann nebel
deterministic enforced hill climbing deh proposed work hoffmann nebel
core element successful deterministic planner fast forward deh
extension basic hill climbing simply selecting actions greedily looking
ahead one action step terminating reaching local optimum deh extends basic hillclimbing replacing termination local optima breadth first search successor state
strictly better heuristic value planner moves descendant repeats
c

ai access foundation rights reserved

fiw u k alyanam g ivan

process deh guaranteed path goal deadend free
every state path relatively weak guarantee applies independent quality
heuristic function intent deh remediate flaws generally accurate heuristic
order leverage heuristic finding short paths goal domains basin
size search depth needed escape optimum bounded deh provide polynomial time
solution method hoffmann
enforced hill climbing defined probabilistic due stochastic outcomes
actions presence stochastic outcomes finding descendants better values longer
implies existence policy reaches descendants high probability one may argue replan yoon fern givan top performer recent probabilistic
benchmarksuses enforced hill climbing call however enforced hill climbing
process used determinized replan use form hill climbing
directly stochastic fact replan consider outcome probabilities

one consider generalizing enforced hill climbing stochastic domains
solution deterministic typically concise sequential plan contrast solution
stochastic policy action choice possibly reached states essential
motivation hill climbing avoid storing exponential information search even
explicit solution stochastic cannot directly stored respecting motivation
reason limit consideration online setting solution
local policy around current state local policy committed executed
local region exited planner online solve possibly retaining
information previous solution generalizes directly construction
offline policies situations space store policies available note contrast
deterministic enforced hill climbing easily implemented offline solution technique
propose novel tool stochastic generalizing enforced hill climbing goalbased stochastic domains rather seeking sequence actions deterministically leading
better state method uses finite horizon mdp analysis around current state seek policy
expects improve heuristic value current state critical process direct
incorporation probabilistic model heuristic function finding desired policy
therefore finite horizon analysis heuristic function integrated mdp
order represent temporary greedy goal improving current heuristic value
integration done building novel heuristic mdp state exit
action available terminates execution cost equal heuristic estimate state
action costs removed heuristic mdp finite horizon policies restricted
requirement horizon one exit action must selected selected
horizons heuristic mdp cost policy state expected
value heuristic upon exit horizon executed
thus desired local policy value iteration heuristic mdp around
current state deepening horizon policy found cost improving heuristic
estimate current state restriction selecting exit action horizon one corresponds
initializing value iteration provided heuristic function policy found
motivation removal action costs heuristic mdp discussed section



fis tochastic e nforced h ill c limbing

method executes policy exiting action indicated horizon used computing
policy
resulting method stochastic enforced hill climbing seh simply generalizes depth k
breadth first search state improved heuristic value deh k horizon value iteration computation seeking policy expects improvement heuristic value note although
stochastic enforced hill climbing explicit statespace technique suitable use astronomically large statespaces heuristic used informative enough limit effective size
horizon k needed expected heuristic improvement empirical work
demonstrate behavior successfully
applicability limitations
stochastic enforced hill climbing seh applied heuristic function however
applicability likewise limitations seh greatly depends characteristics
heuristic function seh appropriate goal oriented given strong enough heuristic
function demonstrate empirically seh generally outperforms greedy following
heuristic variety heuristics variety domains even presence probabilistically interesting features little thiebaux deadends seh rely upon heuristic
function identification dead ends appropriate handling probabilistically interesting features require non local analysisseh simply provides local search often correct
flaws heuristic function seh thus intended possible improvement stochastic
solution methods construct cost go cost function follow greedily
constructed cost function search heuristic many methods constructing value cost functions
proposed evaluated literature potentially improved
goal domains seh place greedy following sutton fahlman lebiere
bertsekas gordon mahadevan maggioni sanner boutilier
prove correctness seh section showing deadend free domains seh
finds goal probability one e seh get stuck local optima
seh search technique leverages heuristic estimate distance go must
emphasized unlike many search techniques seh makes promises
optimality solution path found seh greedy local technique promise
repeatedly policy reduces heuristic value possible
seh inappropriate technique use optimal solutions required
stochastic enforced hill climbing ineffective presence huge plateaus valleys
heuristic functions due extreme resource consumption finding desired local policies
heuristic functions huge plateaus methods failed useful information state regions seh inappropriate tool solving
stochastic problemother tools needed construct useful heuristic function
manages deadends avoids huge plateaus weakness mirrors weakness enforced hillclimbing deterministic domains seh fail goals avoidable dead ends
present recognized early enough heuristic fact effective dead end detection
central goal heuristic design greedy technique applied heuristic
applicability seh cost function must non negative must identify goals assigning zero state
goal state however general value cost functions normalized satisfy requirements



fiw u k alyanam g ivan

insight usefulness seh gained comparison recent determinizing replanners mentioned one way exploit deterministic techniques
deh stochastic determinize use deterministic planner select action sequence executing action sequence guaranteed
reach goal due determinization approximation replanning needed augment
technique call stochastic planners use technique determinizing replanners determinizing replanners determinization called outcomes retains
possible state transitions shown reach goal probability one absence
dead end states
contrast determinizing replanners seh point relies determinization
instead analyzes increasing size local probabilistic approximations
seh conducts full probabilistic analysis within horizon seeking objective reducing
provided heuristic value iteration way seh leverages probabilistic parameters
ignored determinizing replanners well provided heuristic function
upon substantial probabilistic analysis seh successfully handles probabilistic
aspects cause major determinizing replanners however point
theoretical characterizing gains determinizing replanners instead
extensive empirical evaluation showing advantages replan yoon et al
rff teichteil konigsbuch kuter infantes two determinizing replanners well
substantial gains compared greedy following heuristic uses transition
probability parameters
evaluation
test seh broad range domains first three international probabilistic
competitions well probabilistically interesting domains little thiebaux
two different methods generate heuristic functions first test seh heuristic function ideas successful planner replan yoon et al
controlled randomness cr heuristic deterministic heuristic hoffmann
nebel computed simple determinization probabilistic makes
available deterministic transition wherever probabilistic transition possible note
replan use heuristic function stochastic instead ffreplan relies construct plan deterministic calls turn use
deterministic enforced hill climbing exactly heuristic consider performance
heuristic directly stochastic comparing greedy heuristic following sehbased search around heuristic latter method seh constitutes novel method
combining determinization removes probabilistic parameters probabilistic reasoning
experiments method substantially outperforms replan across broad
evaluation
performed second evaluation technique heuristic functions learned
domain specific experience relational feature learning method presented work
wu givan heuristic functions already shown give good
performance used construct simple greedy policy improved seh
seh technique seen perform well domain domain analysis across
broad set competition domains full domain domain available


fis tochastic e nforced h ill c limbing

online appendix however compress summarize extensive per
divided evaluation domains experimenter defined categories aggregated performance measurement within category categories single domains
generally multiple closely related domains may aggregated within single category
example multiple domains competitions variants blocks world
domains aggregated b locksworld category
order fairly compare seh planners rff described teichteilkonigsbuch et al replan exploit blocksworld targeted heuristics
added goal deletion goal agenda provided heuristics extensions seh
resulting planner called seh described detail section seh
performs nearly identically seh non blocksworld categories cr heuristic
employ extensions comparing seh cr heuristic planners
experimenter defined categories able seh exploits heuristic
functions effectively greedy following heuristic seh statistically significantly
outperforms greedy following thirteen seventeen categories cr heuristics
losing one category seh outperforms greedy following six seven categories learned heuristics cases categories showed similar performance
compared planners
seh cr heuristics outperforms replan ten
fifteen categories similar performance two categories losing three categories
aggregate seh cr heuristics particularly strong performance advantage replan probabilistically interesting categories little thiebaux

finally compare performance seh rff bg teichteil konigsbuch
et al one winner fully observable track third international probabilistic competition seh outperforms rff bg twelve fifteen categories similar
performance one category losing two categories
summary empirical work demonstrates seh provides novel automatic technique
improving heuristic function limited searches simply applying seh
reasonable heuristic functions produces state art planner

technical background markov decision processes
give brief review markov decision processes mdps specialized goal region objectives
detail mdps see work bertsekas puterman sutton barto

goal oriented markov decision processes
markov decision process mdp tuple c sinit finite state space
containing initial state sinit selects non empty finite available action set state
action cost function c assigns non negative real action cost state action state
triple action enabled state e transition probability
function maps state action pairs probability distributions p



fiw u k alyanam g ivan

represent goal include zero cost absorbing state e c
goal oriented mdps mdps
subset g statespace containing c g zero whenever g g
one otherwise g one g g g set g thus
taken define action cost function c well constrain transition probabilities
stochastic policy mdp n p specifies distribution actions
state finite horizon cost go function j k gives expected cumulative
cost k steps execution starting state selecting actions according state
encountered horizon k least one deterministic optimal policy k

j k abbreviated j k greater j k every state
policy following q function evaluates action provided cost go function
j estimate value action applied
x
q j
c j


recursive bellman equations use q describe j j follows
j k e q k j k
j k min q j k
aa

taking expectation random choice made possibly stochastic policy k
cases zero step cost go function zero everywhere j j
value iteration computes j k k increasing order starting zero note
policy cost function depend k may drop k argument list
q select action greedily relative cost function policy
greedy j selects state horizon k uniformly randomly selected action
argminaa q j k
goal mdp directly specified may specified
exponentially compactly languages ppddl younes littman weissman asmuth used experiments technique avoids converting
entire ppddl explicitly form resource reasons instead constructs
sequence smaller explicit mdp form modeling heuristic flaws
dead end state state every policy zero probability reaching goal
horizon say policy reaches region states probability one following policy
horizon k probability entering region point converges one k goes
infinity say dead ends unavoidable whenever policy sinit
reaches goal region probability one say domain unavoidable dead ends
domain unavoidable dead ends note greedy techniques
hill climbing expected perform poorly domains dead end states attractive
heuristic values application seh thus leaves responsibility detecting avoiding deadend states design heuristic function
heuristic h r may provided intended estimate cost function j large
horizons h g h otherwise heuristic may indicate dead end
states returning large positive value v assume selected experimenter
exceed expected steps goal state reach goal experiments


fis tochastic e nforced h ill c limbing

add trivial incomplete dead end detection described section heuristic function
evaluate
note domains evaluated contain unavoidable deadends
may policy success ratio one choice large value used recognized
dead end states effects trade optimizing success ratio optimizing expected cost
incurred goal successful
determinizing stochastic
stochastic planners heuristic computation techniques including used experiments rely computing deterministic approximations stochastic one planner
outcomes replan yoon et al determinizes stochastic
invokes deterministic planner hoffmann nebel determinized
determinization used replan constructed creating deterministic action
possible outcome stochastic action ignoring probability outcome happening effectively allows planner control randomness executing actions making
determinization kind relaxation section define domainindependent heuristic function controlled randomness heuristic cr deterministic heuristic hoffmann nebel computed outcomes replan determinization probabilistic variety relaxations previously combined
variety deterministic heuristics order apply deterministic techniques stochastic bonet geffner generally deterministic relaxations provide general
technique transferring techniques deterministic use solution stochastic


stochastic enforced hill climbing
deterministic enforced hill climbing deh hoffmann nebel searches successor
state strictly better heuristic value returns path current state successor
path action sequence guarantees reaching desired successor illustrate
behavior deh compared greedy policy example figure stochastic
environment may single better descendant reached probability one
since actions may multiple stochastic outcomes simply use breadth first search
deh single better descendant ignore possible outcomes might end
selecting action low probability actually leading state better heuristic value
illustrated figure shown figure stochastic enforced hill climbing
seh accurately analyzes probabilistic dynamics improving heuristic
value
section give details seh note deh local breadth first search
gives local policy state region surrounding current state deterministic environment
value following policy heuristic value improved descendant found
breadth first search seh implement ideas stochastic setting
deterministic heuristic described work hoffmann nebel planner version
available http www loria fr hoffmanj html efficiently computes greedy plan length relaxation
state facts never deleted plan found relaxed referred relaxed plan




fiw u k alyanam g ivan

h

h

h

h

h
h

h

h

h

h

h

h
h

h

h

h

behavior greedy policy

b behavior deh

figure comparison behavior deh greedy policy local optimum
encountered solid black circle represents current state shaded circle represents
goal state heuristic value zero greedy policy keeps selecting actions indicated
wide arrow cannot reach goal state hand deh uses breadth first search
finds goal state two steps away current state shown b

h
h
h
h

h

p
p

h

h

h

h

p

h

h

p
h

behavior deh stochastic environments

h

h

h

h

b behavior seh stochastic environments

figure comparison behavior seh deh stochastic example assume
deh first determinizes creating one deterministic action possible stochastic
outcome solid black circle represents current state shaded circle represents
goal state heuristic value zero deh looks one step ahead selects action drawn
double lines one outcomes leads state h better current
state however action choice higher probability going state h
one h b seh first decides policy better value
horizon mdp includes states reachable current state one step seh
extends horizon two states considered selects actions indicated
wide arrows lead goal state



fis tochastic e nforced h ill c limbing

online local planner
repeat

current state

local local policy h

follow local selected
goal reached
table pseudo code online framework policy local may non stationary
case local planner returns initial horizon execution policy termination line happen reaching specified horizon

present seh two steps first present simple general framework online repeatedly calls local planner selects policy around current state second
present local planner enforced hill climbing idea online
framework instantiated local planner resulting seh combination
two steps constitute central algorithmic contribution finally present
analytical properties
simple online framework
familiar direct online call planner current state
planner select action action executed environment resulting current
state process repeated
present simple generalization allows planner select
one action call action executed idea planner makes
plan local context surrounding current state plan executed local
context exited local context exited current state process
repeated
formally augment action space terminate action called indicating planned local context exited define local policy around state
partial mapping states augmented action space defined every
state reachable policy online planner built repeatedly seeking
executing local policy around current state subroutine local policy
executed terminate action called effect state point
local policy must sought ideas reflected pseudo code shown table
note notion local context discussion informal precise
notion given use terminate action local policy executed selects
terminate action local policy routine free use method decide state
assigned terminate action previously published envelope methods dean kaelbling
kirman nicholson provide one way address issue termination
assigned every state outside envelope states however framework general
envelope methods allows local policies selected upon pre existing
local policy returned non stationary finite horizon must select terminate action
final stage reachable states



fiw u k alyanam g ivan

envelopes states though post interpret set reachable states
envelope general intuition selecting action current states may involve
analysis sufficient select actions many surrounding states framework allows
local policy routine return policy specifying action selections
note online framework includes recent planners ffreplan yoon et al rff teichteil konigsbuch et al however replanning
current plan failed e g determinization used generate naive
quite different character seh constructs plans improve heuristic value
replans time plan terminates thus seh uses heuristic function define subgoals
plan original goal incrementally
remains present local planner combine online framework
define stochastic enforced hill climbing local planner analyzes mdp around
current state heuristic function integrated embody subgoal
improving heuristic value current state describe simple integration
heuristic function next discuss local planner integration
heuristic markov decision processes
method relies finite horizon analyses transformed mdp increasing horizons transform mdp novel heuristic achievement transform analysis
order represent goal finding executing policy expects improve initial
current states heuristic value
heuristic achievement transform straightforward applies goal oriented
mdp first action costs removed second terminate action
assigned action cost h transitions deterministically absorbing state
policy executed selection action state replanning discussed
online framework presented actions thought heuristic
achievement actions allowing immediate achievement value promised heuristic
function
analyzing mdp transformed heuristic achievement transform finite horizon
around represents finding policy improving heuristic value
without regard cost achieving improvement heuristic allowing heuristic
achievement action selected point state reflects greedy nature goal
planner forced look improvement found long policy
initial state expects see improvement
formally given mdp c non negative heuristic function h r
heuristic achievement transform h written mh given c
c follows let arbitrary states define
take c
finally define c h
transformed mdp zero cost policies states immediate use however policies required select final action horizon one
represent policies seeking get regions low heuristic value whatever cost
increasing horizon search policies corresponds roughly breadth first search
improved heuristic value deterministic enforced hill climbing formally define class


fis tochastic e nforced h ill c limbing

heuristic achievement policies h class policies k satisfy
define jh k value minh j k heuristic transform mdp h h
policy achieves value note due zeroing non terminal action costs
jh k represents expected heuristic value achieved next execution
required horizon k formally define random variable state
h first executes trajectory jh k e h
rough motivation setting action costs zero analysis heuristic
mdp actions considered method remediate flawed heuristic
cumulative action cost required reach state improved heuristic value measure
magnitude flaw heuristic remove cost analysis order
directly express subgoal reaching state lower heuristic value including action costs
might example lead preferring cheap paths higher heuristic values e states worse
expensive paths lower heuristic values found basic motivation
enforced hill climbing strongly seek improved heuristic values instead diluting
subgoal adding action costs methods seek shortest path heuristic improvement
analyzing heuristic mdp iteratively deepened finite horizon discussed
next subsection reasonable settings action
cost finite horizon value iteration stochastic setting analogue uniform cost search
settings varying action cost future work needed adapt seh usefully consider
cost without excessively diluting focus improving heuristic
h euristic achievement value teration
following formalism value iteration section compute jh k heuristic
achievement value iteration follows
jh h
jh k min q jh k k
aa

non stationary policy achieving cost go given jh k computed
following definition
h
h k argminaa q jh k k
note q computed heuristic achievement transformed mdp mh equations
technical reasons arise zero cost loops present require tie breaking
argmin h k favors action selected h k whenever one options
prevent selection looping actions shorter direct routes value
local planner
consider method stochastic enforced hill climbing uses online framework presented table together local policy selection method solves
heuristic achievement mdp exactly approximately heuristically describe one
straightforward method local policy selection defining seh local policy finitehorizon value iteration method generalizes breadth first search used deterministic


fiw u k alyanam g ivan

enforced hill climbing seeks expected heuristic improvement rather deterministic
path improved heuristic value sophisticated heuristic methods finite horizon
value iteration considered implementation presented finds local mdp intractable analytical section apply method exactly solves
heuristic achievement mdp method presented table experimental
conducted implementation table well
present pseudo code seh local policy table heuristic function h respects
goals h iff g assumes non negative heuristic function h r
respects goals input seh local policy h returns policy h horizon k
policy computed states horizons needed order execute h
horizon k policy terminates
thus lines table heuristic achievement value iteration conducted increasing horizons around seeking policy improving h note given horizon k
states reachable within k steps need included value iteration
e arly ermination
primary termination condition repeated local policy construction discovery policy
improving heuristic estimate initial state discussed proposition
domains without deadends seh local policy policy improving h
given sufficient resources
however badly flawed heuristic functions large enough horizons analyzed
seh local policy may unacceptably large given resource constraints moreover domains unavoidable deadends may horizon however large policy improving
heuristic initial state reasons practice stops enlarging
horizon heuristic mdp analysis user specified resource limits exceeded
horizon limited analysis heuristic transform mdp construction yield
desired inexpensively biased random walk used seek initial state example consider provided heuristic labels states reachable k steps
cost go estimates similar forming large plateau analysis large
plateau exceeds resources available biased random walk indicated lack useful heuristic
guidance
horizon k found jh k h system executes h
horizon k terminate action selected resource limit exceeded without
finding horizon system executes biased random walk length terminating
action imposed states reachable biased random walk h h
additional biased random walk allows method retain beneficial properties
random exploration domains heuristic flaws large mdp analysis resource
consumption threshold random walk triggered viewed parameter controlling
blend random walk mdp search used overcoming heuristic flaws
currently principled way analyzing tradeoff resource consumption
cost switching biased random walk determining switching instead
use domain independent resource limits described section determined
experimentation


fis tochastic e nforced h ill c limbing

seh local policy h

current state

h r heuristic function extended h

assume global variable mh heuristic achievement
transform original h

seek policy mh achieving cost less h
k
repeat

k k

compute jh k mh value iteration

jh h h n

repeat

n n

reachable mh k n steps

jh n minaa q jh n

h n argminaa q jh n

n k
jh k h resource consumption exceeds user set limits
jh k h



return h horizon k

else

return step biased random walk policy

note implementations compute lazily online

n

state

h h



n selects probability one

else

n selects action probability


p

eq h
ai e

q ai h

return horizon

table pseudo code local planner used implement stochastic enforced hill climbing



fiw u k alyanam g ivan

horizon limited analysis heuristic transform mdp may terminate without finding
horizon k jh k h entire reachable statespace explored
presence deadends may happen without exceeding available resources
case fall back fixed number iterations standard vi original mdp model
including action costs without heuristic transform reachable states
analytical properties stochastic enforced hill climbing
deterministic settings given sufficient resources dead ends enforced hill climbing
guarantee finding deterministic path improved heuristic value nothing else goal state
suffice given finite state space guarantee implies guarantee repeated enforced
hill climbing goal
situation subtle stochastic settings dead ends every
state optimal policy reaches goal probability one follows
h assigning zero every goal state every state real value horizon
k jh k recall jh analyzes heuristic transform mdp wherein action costs
dropped except h must realized horizon one seh local policy h
considers k turn jh k h

proposition given non goal state dead ends non negative heuristic function
h r respecting goals sufficient resources routine seh localpolicy h returns policy h horizon k expected return jh k strictly
less h
however unlike deterministic setting policy found routine seh local policy
expects improvement heuristic value particular executions policy
current state may degraded heuristic value
prove even stochastic settings spite possibility poor
one iteration seh reach goal region probability one absence dead end states
sufficient resources practice provision sufficient resources serious hurdle
must addressed providing base heuristic modest sized flaws

theorem dead end free domains unbounded memory resources seh reaches goal region probability one
proof let x x x xm random variables representing sequence
states assigned line table execute seh
x initial state sinit achieves x g
thus terminates take xj xj j note xj g implies
xj g g goal region states
arbitrary probability xm goal region
h sinit
least
real value defined expression goes one


fis tochastic e nforced h ill c limbing

goes infinity conclude seh reaches goal region
probability one
proposition non goal state absent dead ends sufficient
resources one iteration seh guaranteed return policy finite horizon
ks value jh ks improving h let h jh ks value
improvement horizon ks finitely many non goal
states exists minssg improvement h jh ks
least consider arbitrary xi
g noting jh xi kxi
e h xi due zero action costs mh follows immediately e h xi
h xi xi
g g goal region states xi g implies
xi g h xi h xi write
e h xi h xi
e h xi h xi xi
g qi
e h xi h xi xi g qi



qi
defining qj probability xj
g
lower bound expected heuristic improvement e h x h xm calls seh local policy decompose expected
improvement calls seh local policy sum expected improvements individual calls lower bounding sum smallest
term get
e h x h xm




x



x

e h xi h xi

qi inequality



mqm
qm non increasing since xm g implies xm g
next combine lower bound natural upper bound h sinit since h
assumed non negative e h x h xm h sinit x sinit thus
h sinit qm
h



init converging zero
therefore probability qm xm
g
large seh reaches goal probability one

theorem assumes absence dead ends dead ends covered theorem well dead ends avoidable identified heuristic
specifically may require heuristic function assigns state
policy reach goal state probability one case
converted form required theorem simply removing states assigned
consideration pre processing local mdp construction


fiw u k alyanam g ivan

variants extensions seh
seh finite horizon analysis mdp transformed heuristic achievement transform around current state particular heuristic achievement transform describe
course option incorporating heuristic local search around
already considered number related alternatives arriving choice describe
options considered future one notable restriction transform
removal action costs discussed section important method
retain actual heuristic value analysis trade large small positive
negative changes heuristic value according probabilities arising reason
heuristic transform abstract away value simply assign rewards
according whether state improves h choice remove action costs local
expansion lead poor performance domains flawed heuristics interacting badly
high variations action costs subject future method
mdp describe limited obvious ways
limitations include state space discrete finite setting lacks discounting
objective goal oriented yet implement extension relax limitations
leave consideration issues arise future work note would appear
method fundamentally goal oriented given goal repeatedly reducing heuristic value
current state however possible contemplate infinite horizon discounted non goaloriented variants seek policies maintain current heuristic estimate
incorporating goal ordering techniques seh
planner contains heuristic elements inspired ordering issues arise blocksworld
hoffmann nebel heuristic elements improve performance blocksworld significantly assist fair comparison seh replan
implemented two heuristic elements namely goal agenda added goal deletion variant
seh call seh
implementation seh follows stochastic first determinized outcomes determinization described section goal agenda technique invoked determinized extract sequence temporary goals
g gm gi set goal facts gm original goal seh
stochastic version added goal deletion described next subsection invoked repeatedly compute sequence states sm initial state si
defined state reached invoking seh state si goal gi thus satisfying gi
added goal deletion idea pruning state search space avoiding repetitive addition
deletion goal fact along searched paths search state goal fact
achieved action arriving deleted action relaxed plan found
expanded hoffmann nebel
stochastic adaptation added goal deletion define set facts added
state transition facts true represent set difference
set added goal facts transition added facts true
current temporary goal gi e gi prune state transition whenever
relaxed plan computed current temporary goal gi contains action deletes
added goal facts transition pruned modifying bellman update


fis tochastic e nforced h ill c limbing

state contributes dead end state value v q value weighted
transition probability instead contributing cost go formally define
modified q function added goal deletion qagd j follows

f gi deleted action relaxed plan gi


otherwise
x
qagd j
v j


qagd replaces q definition cost go function jh section
reachability line table use pruned transitions
subsequent deletion newly added goals unavoidable valid plan
added goal deletion prunes routes leading goal region even though
actual deadend present hence incomplete technique discussed work
hoffmann nebel falls back best first search deh able valid
plan due pruning similarly unable improved policy seh falls back
value iteration biased random walk described section
preliminary exploration incorporating stochastic variants ffs helpful action pruning
hoffmann nebel seh improve performance much effect added
goal deletion domains except blocksworld report helpfulaction pruning methods

related work
section discuss techniques close work one dimensions
fast foward planner deterministic enforced hill climbing
introduction deterministic enforced hill climbing deh relation technique
please see section additionally note several lines work directly
extend planner allow numeric state variables hoffmann uncertainty hoffmann brafman domshlak hoffmann although
techniques involve significant changes computation relaxed plan heuristic
possible addition use belief states handle uncertainty enforced hill climbing still
primary search technique used lines work note although work domshlak hoffmann actions probabilistic outcome handled planner probabilistic
designed probabilistic observability whereas planner designed
probabilistic full observability
envelope techniques
stochastic enforced hill climbing dynamically constructs local mdps local policy leading
heuristically better state regions concept forming local mdps envelopes
relaxed plan gi computes relaxed plan states gi defined work hoffmann
nebel outcomes determinization defined section
explored ideas defining helpfulness action expectation helpfulness
deterministic outcomes



fiw u k alyanam g ivan

facilitate probabilistic used previous work bonet
geffner dean et al briefly review
envelope methods work dean et al gardiol kaelbling
start partial policy restricted area envelope iteratively improves solution quality extending envelope recomputing partial policy
typical assumption implementing method planner initial trajectory
starting state goal generated stochastic planner use initial envelope
another line work including rtdp barto bradtke singh lao hansen
zilberstein ldfs bonet geffner starts envelope containing
initial state iteratively expands envelope expanding states states expanded
according state values dynamic programming methods used backup state values
newly added states convergence criterion reached stochastic enforced hill climbing
viewed repeatedly deploying envelope method goal time improving
heuristic estimate distance go good h function invocations trivial
one step envelopes however local optima plateaus encountered envelope may
need grow locate stochastically reachable set exits
referenced previous search methods constructed envelopes seeking high
quality policy goal rather far limited relatively inexpensive goal basin
escape derive online greedy exploitation heuristic rather
expensive offline computation converged values proving overall near optimality ldfs
example compute check values least states reachable optimal policy even
given j input possibly vastly many others well computation
previous methods able exploit properties admissibility
heuristic function guarantee avoiding state expansions regions state space clearly
seh exploits heuristic function way avoid expanding regions statespace
however point conducted theoretical analysis regions guaranteed unexpanded particular kinds heuristic analyses may quite difficult
policy rollout
technique policy rollout tesauro galperin bertsekas tsitsiklis uses
provided base policy make online decisions technique follows policy greedy vf
vf computed online sampling simulations policy
computation optimal heuristic transform policy h seh similarities policy
rollout case online decisions made local probabilistic analysis leverages provided information manage longer range aspects local choice seh heuristic function
provided policy rollout base policy provided view policy rollout local
analysis assumption non local execution use base policy whereas seh
local analysis assumption non local execution achieve base heuristic
cost estimate h
fact goal oriented setting provided heuristic function h stochastic simple generalization describe equal sampled simulation evaluation
v policy seh executes policy policy rollout assuming uniform
action costs sufficient sampling correctly order action choices claim follows h v action yield expected improvement h one step


fis tochastic e nforced h ill c limbing

goal oriented setting need uniform action costs claim may relaxed
variant seh developed retains action costs heuristic transform
policy rollout horizon one greedy use sampled heuristic needed main
substance seh enable repair use heuristic functions flaws cannot
repaired horizon one thus central differences techniques reflected
ability seh leverage arbitrary heuristic functions repair flaws functions larger
horizons
policy rollout provides elegant guarantee online policy selected improves base
policy given sufficient sampling follows intuitively computed policy
policy iteration improvement base policy unfortunately similar guarantee known
apply seh arbitrary heuristic function however policy rollout cannot used improve
arbitrary heuristic function
local search optimization
stochastic enforced hill climbing regarded one many local search techniques designed
improve greedy one step lookahead naive form local search optimization
briefly discuss connections method simulated annealing one large family related
local search techniques detail please see work aarts lenstra
simulated annealing kirkpatrick et al cerny allows selection actions
inferior expected outcome probability monotone action q value probability
inferior action selected often starts high decreases time according cooling schedule ability select inferior actions leads non zero probability escaping local
optima however method systematically search policy contrast
stochastic enforced hill climbing analyzes heuristic mdp increasing horizons systematically search policies give improved expected value hence leaving local extrema
substantial preliminary experiments could successful parameter settings control
simulated annealing effective application online action selection goal directed stochastic
knowledge simulated annealing otherwise tested direct forwardsearch action selection although variants applied success
search settings selman kautz cohen kautz selman gerevini
serina via boolean satisfiability search

setup empirical evaluation
describe parameters used evaluating method heuristics test
method categories tests conducted random variables
aggregated evaluation issues arising interpreting statistical significance
run experiments intel xeon ghz machines mhz bus speed kb
cache
implementation details
horizon increase states reachable implementation seh simply switches
explicit statespace method solve mdp formed reachable states specifically


fiw u k alyanam g ivan

increase k line table lead reachable states line trigger
value iteration states reachable
throughout experiments thresholds used terminate local line table
set states one minute set biased random walk length ten work
makes assumption heuristic functions used assign large values easily recognized deadends hill climbing works poorly presence dead end attractor states enforce
requirement simple dead end detection front end heuristic function
described next section heuristic assigning value recognized
dead end states
denote implementation running heuristic h seh h
heuristics evaluated
describe two different types heuristic functions used evaluation associated
dead end detection mechanisms
c ontrolled r andomness h euristic
use evaluations define domain independent heuristic function controlledrandomness heuristic cr define cr state distance goal
estimate hoffmann nebel computed outcomes determinization described
section denote resulting heuristic function f computing cr heuristic
use reachability analysis built planner detection deadends
l earned h euristics
test stochastic enforced hill climbing automatically generated heuristic functions
work wu givan perform state art used
construct greedy policy shift heuristic functions fit non negative range requirement h discussed previously learned heuristic functions currently available
seven test categories tested categories
note heuristics learned discounted setting without action costs
direct fit distance go formalization adopted still able get
significant improvements applying technique denote heuristics l states
valid action choice available labeled deadends applying seh learned
heuristics
goals evaluation
primary empirical goal stochastic enforced hill climbing generally improves
significantly upon greedy following heuristic policy greedy h described
technical background true heuristics defined
section empirically applicability limitation seh discussed section
different types including probabilistically interesting ones little thiebaux
secondary goal evaluation base heuristics resulting performance strong comparison deterministic replanners replan yoon et al
rff teichteil konigsbuch et al replan rff use fast forward


fis tochastic e nforced h ill c limbing

base planner rff uses probable outcome determinization contrast outcomes
determinization used replan primary difference rff replan
executing plan rff grows policy trees minimize probability
replan replan
adapting ippc domains experiments
conduct empirical evaluation first three international probabilistic competitions ippcs well twelve probabilistically interesting
work little thiebaux omit particular domains
particular comparisons several practical reasons detailed online appendix
enforced hill climbing nature goal oriented technique seh formulated
goal oriented setting ignore reward structure including action goal rewards
evaluated assume uniform action cost one
transforming reward oriented description goal oriented one
provide detailed per online appendix planner evaluated
work however support main conclusions limit presentation aggregations
comparing pairs planners sets related purpose define seventeen
categories aggregate within category categories single
domains generally multiple closely related domains may aggregated within single category example blocksworld category aggregates blocksworld three
competitions even though action definitions exactly every
paired comparisons aggregated labeled constructed
probabilistically interesting ippc organizers work little thiebaux
combined category pi
table list evaluated categories including combined category pi
well competitions literature category
evaluated category identified online appendix
reward oriented ysadmin domain ippc stochastic longest path
best performance required avoiding goal continue accumulating reward long
possible bryce buffet note contrary organizers report domains goal
condition servers rather servers goal oriented adaptation removes
longest path aspect domain converting domain goal get
servers
b locksworld ippc contain flawed definitions may lead block
stacking top nevertheless goal well defined achievable
valid actions hence included b locksworld category
discovered five rectangle tireworld p p ippc ireworld apparent bugno requirement remain alive included goal condition domain design provides powerful teleport action non alive agents intended
increase branching factor buffet however lacking requirement alive goal
domain easily solved deliberately becoming non alive teleporting goal
modified require predicate alive goal region merged
modified rectangle tireworld triangle tireworld ippc


fiw u k alyanam g ivan

category

source

b locksworld

ippc ippc ippc

b oxworld

ippc ippc

b usfare

little thiebaux

rive

ippc

e levator

ippc

e xploding b locksworld

ippc ippc ippc

f ileworld

ippc

p itchcatch

ippc

r andom

ippc

r iver

little thiebaux

chedule

ippc ippc

earch r escue

ippc

ysadmin

ippc

ystematic tire

triangle tireworld ippc tireworld p p little thiebaux
rectangle tireworld ippc tireworld p p bug fixed

ireworld

ippc ippc

owers h anoi

ippc

z enotravel

ippc ippc

pi

b usfare rive e xploding b locksworld
p itchcatch r iver chedule ystematic tire ireworld

table list categories competitions literature
category taken



fis tochastic e nforced h ill c limbing

work little thiebaux category ystematic tire
systematically constructed emphasize pi features
aggregating performance measurements
experiments designed repeatable aggregate measurements sample
many times order evaluate statistical significance define random variables representing aggregate measurements describe sampling process well method
evaluating statistical significance
efining ampling aggregate easurement r andom variables
pair compared planners define four random variables representing aggregate performance comparisons category random variable upon
sampling process runs planner five times category aggregates
per computing mean use five trial runs reduce incidence lowsuccess planners failing generate plan length comparison mean value five trial run
sample value respective random variable
first per success ratio sr fraction five runs succeed
success ratio random variable category planner mean sr
across category
second per successful plan length slen mean plan length successful
runs among five runs order compare two planners plan length define perproblem ratio jointly successful plan lengths jslen ratio two compared planners
follows planners positive sr among five trials jslen ratio
ratio slen values two planners otherwise jslen ratio undefined
use ratio lengths emphasize small plan length differences short solutions
long solutions decrease sensitivity granularity action definitions
mean jslen ratio random variable category pair planners
geometric mean jslen ratio across category jslen ratio
well defined manner ensure two planners compared exactly set
note unlike sr jslen ratio depends pair compared planners
rather measurement single planner ratio successful plan length
jointly solved two planners
similarly per ratio jointly successful runtimes jstime ratio defined
manner used comparing plan lengths mean jstime ratio computed
geometric mean well defined per jstime ratio values
jslen ratio jstime ratio ratios two measurements use geometric mean aggregate per generate sample value whereas use arithmetic
mean sr variables note geometric mean desired property planners tied overall geometric mean one mean insensitive planner
given denominator ratio
thus draw single sample four aggregate random variables sr planner
jslen ratio jstime ratio comparing two planners run two planners
five times computing per values four variables take arith

fiw u k alyanam g ivan

metic geometric means per variables get one sample aggregate variable process used repeatedly draw many samples needed get significant
use plan length cutoff attempt attempt given time limit
minutes
ignificance p erformance ifferences b etween p lanners
general goal order pairs planners overall performance category
must trade success rate plan length take position significant
advantage success rate primary goal plan length used determine preference
among planners success rate differences found significant
determine significance three performance measurements sr jslenratio jstime ratio tests ascribing significance p value
less exact hypothesis tested form test used depends performance
measurement follows
sr use paired one sided test hypothesis difference true means
larger
jslen ratio use one sample one sided test hypothesis true geometric mean jslen ratio exceeds log true mean jslen ratio exceeds
log
jstime ratio use one sample one sided test hypothesis true
geometric mean jstime ratio exceeds log true mean jstime ratio
exceeds log
stop sampling performance variables achieved one following criteria representing sr winner determined sr appears tied
thirty samples drawn p value sr difference
sixty samples drawn p value sr difference
one hundred fifty samples drawn
experiments present next stopping rule leads samples drawn
unless otherwise mentioned upon stopping conclude ranking planners naming
winner sr difference jslen ratio p value significant
sr differences used first determine winner neither measure significant upon
stopping deem experiment inconclusive
combining categories evaluations aggregate across multiple categories e g combined category pi cases effectively
defined one larger category techniques defining performance measurements determining statistical significance section however actually run
planners combined category measurements instead use planner runs used
single category experiments rather use stopping rule described compute
maximum number runs available combined categories use many samples


fis tochastic e nforced h ill c limbing

combined category performance measurements avoid double counting
treat combined categories separately analyzing counting wins losses

empirical
present performance evaluation stochastic enforced hill climbing seh section
experiments underlying presented involve planner runs categories
summary comparison
table cr heuristic seh goal ordering addedgoal deletion enhancements seh f improves significantly baseline seh technique
seh f category b locksworld significant changes aggregated
performance non blocksworld remainder experiments involving crff evaluate seh f noting comparison planners replan rff
benefit goal ordering added goal deletion enhancements base planner ffplan
present next seh f
seh f significantly outperforms greedy f categories outperformed
greedy f chedule three categories comparison inconclusive b usfare r iver ireworld see table details
replan inapplicable two categories ippc earch r escue ippc
ysadmin seh f significantly outperforms replan categories outperformed replan three categories e xploding b locksworld p itchcatch
z enotravel two categories comparison inconclusive f ile world r iver seh f significantly outperforms replan combined
category pi although winner varied aggregated categories see
table details
rff bg inapplicable two categories b usfare ippc f ileworld seh f
significantly outperforms rff bg categories outperformed rff bg two
categories e xploding b locksworld ystematic tire one category
comparison inconclusive ysadmin seh f significantly outperforms rff bg combined category pi although winner varied
aggregated categories see table details
learned heuristic work wu givan computed
subset domains hence seven categories applicable evaluation
learned heuristic see online appendix details present next seh
learned heuristic seh l
seh l significantly outperforms greedy l six categories one category
ireworld comparison inconclusive see table details
p values rounded two decimal places example p value p rounded two
decimal places



fiw u k alyanam g ivan

sr
sr
seh f seh f

category

jslenratio
seh
seh

jstimeratio
seh
seh

sr
difference
significant
p value

jslenratio
significant
p value

winner

b locksworld









yes

p

yes

p

seh f

n blocksworld











p



p

inconclusive

table aggregated comparison seh f seh f

seh l significantly outperforms replan five categories outperformed ffreplan two categories e xploding b locksworld z enotravel see table
details
discussion
discuss comparisons pairs planners including seh versus greedy
heuristic following seh versus replan seh versus rff bg
seh seh v ersus g reedy
primary evaluation goal stochastic enforced hill climbing generally improves
significantly upon greedy following heuristic policy greedy h described
technical background demonstrated evaluating seh two different
heuristics tables seh h significantly outperforms greedy h nineteen
twenty four heuristic category pairs losing chedule seh f greedy f
discuss category greedy outperforms seh techniques significantly
chedule multiple classes network packets different arrival rates packets deadlines packet served deadline agent encounters classdependent risk death well delay packet cleaned reach goal
serving packet every class agent must minimize dropping related risk dying
waiting arrival low arrival rate class outcomes determinization underlying
cr heuristic gives deterministic domain definition dying optional never chosen
unlikely packet arrivals happen choice leading optimistic heuristic value
optimistic heuristic value basic local goal seh improve
current state heuristic leads building large local mdps analysis presence
dead ends death even arbitrarily large local mdps may able achieve local
improvement chedule seh typically hit resource limit mdp size
every action step
contrast greedy local decision making well suited packet scheduling many well known
packet scheduling policies e g earliest deadline first static priority work liu
layland make greedy local decisions practically quite effective experiments
greedy policy applied cr benefits locally seeking avoid incidental delays
dropped packet cleanup even though heuristic sees risk dying cost dropping still
recognizes delay cleaning lost dropped packets thus greedy f class insensitive


fis tochastic e nforced h ill c limbing

jslensr
ratio
sr
seh f greedy f greedy
seh

category

jstimeratio
greedy
seh

sr
difference
significant
p value

jslenratio
significant
p value

winner

b locksworld









yes

p

yes

p

seh f

b oxworld









yes

p

yes

p

seh f

b usfare











p



p

inconclusive

rive









yes

p

yes

p

seh f

e levator









yes

p

yes

p

seh f

e xploding
b locksworld









yes

p



p

seh f

f ileworld









yes

p



p

seh f

p itchcatch









yes

p

r andom









yes

p

yes

p

seh f

r iver











p



p

inconclusive

chedule









yes

p

yes

p

greedy f

earch
r escue











p

yes

p

seh f

ysadmin











p

yes

p

seh f

ystematic
tire









yes

p



p

seh f

ireworld











p



p

inconclusive

owers
h anoi









yes

p









yes

p



z enotravel




yes

p

seh f

seh f
seh f

table aggregated comparison seh f greedy f r iver domain evaluation required extending sampling samples per experimental protocol described section values p values jslen ratio jstime ratio p itchcatch
owers h anoi available due zero success ratio greedy f categories



fiw u k alyanam g ivan

jslensr
sr
ratio
seh f replan ffr
seh f

category

jstimeratio
ffr
seh f

sr
difference
significant
p value

jslenratio
significant
p value

winner

b locksworld









yes

p

yes

p

seh f

b oxworld









yes

p

yes

p

seh f

b usfare









yes

p

rive









yes

p

yes

p

seh f

e levator









yes

p



p

seh f

e xploding
b locksworld











p

yes

p

replan

f ileworld











p



p

inconclusive

p itchcatch









yes

p

yes

p

replan

r andom









yes

p

yes

p

seh f

r iver











p



p

inconclusive

chedule









yes

p



p

seh f

ystematic
tire









yes

p

yes

p

seh f

ireworld









yes

p

yes

p

seh f

owers
h anoi









yes

p

yes

p

seh f

z enotravel









yes

p

yes

p

replan

pi
p roblems









yes

p



p

seh f





seh f

table aggregated comparison seh f replan ffr r andom r iver
domains required extending sampling samples owers h anoi domain required
extending sampling samples per experimental protocol described section
p value jslen ratio b usfare available extremely low success rate
ffr leads one sample jslen gathered attempts yielding estimated
variance



fis tochastic e nforced h ill c limbing

sr
sr
seh f rff bg

category

jslenratio
rff bg
seh f

jstimeratio
rff bg
seh f

sr
difference
significant
p value

jslenratio
significant
p value

winner

b locksworld









yes

p

yes

p

seh f

b oxworld









yes

p



p

seh f

rive









yes

p



p

seh f

e levator











p

yes

p

seh f

e xploding
b locksworld











p

yes

p

rff bg

p itchcatch









yes

p

r andom









yes

p

yes

p

seh f

r iver









yes

p

yes

p

seh f

chedule









yes

p



p

seh f

earch
r escue









yes

p

yes

p

seh f

ysadmin











p



p

inconclusive

ystematic
tire









yes

p

yes

p

rff bg

ireworld









yes

p

yes

p

seh f

owers
h anoi









yes

p

yes

p

seh f

z enotravel









yes

p



p

seh f

pi
p roblems









yes

p

yes

p

seh f





seh f

table aggregated comparison seh f rff bg r iver owers
h anoi domains required extending sampling samples per experimental protocol described section values p values jslen ratio jstime ratio p itch catch available due zero success ratio rff bg category



fiw u k alyanam g ivan

sr
seh l

category

jslensr
ratio
greedy l greedy
seh

jstimeratio
greedy
seh

sr
difference
significant
p value

jslenratio
significant
p value

winner

b locksworld











p

yes

p

seh l

b oxworld











p

yes

p

seh l

e xploding
b locksworld









yes

p



p

seh l

ystematic
tire









yes

p

yes

p

seh l

ireworld











p



p

inconclusive

owers
h anoi









yes

p









yes

p



z enotravel


yes

p

seh l
seh l

table aggregated comparison seh l greedy l values jslen ratio
jstime ratio p value jslen ratio owers h anoi available due
zero success ratio greedy l category

sr
seh l

category

jslensr
ratio
replan ffr
seh l

jstimeratio
ffr
seh l

sr
difference
significant
p value

jslenratio
significant
p value

winner

b locksworld









yes

p



p

seh l

b oxworld











p

yes

p

seh l

e xploding
b locksworld









yes

p

yes

p

replan

ystematic
tire









yes

p

yes

p

seh l

ireworld









yes

p

yes

p

seh l

owers
h anoi









yes

p

yes

p

seh l









yes

p

yes

p

replan



z enotravel

table aggregated comparison seh l replan ffr



fis tochastic e nforced h ill c limbing

policy greedily seeks avoid dropping similar earliest deadline first
seh encounters evaluation chedule suggest future work automatically recognizing
domains large mdp construction proving futile automatically reducing mdp size
limits adapt performance towards behavior greedy policy note across tested
benchmark domains heuristics one domain heuristic combination
phenomenon arose practice
seh seh v ersus r eplan



rff bg

demonstrated performance improvement seh f best performing planners first three international probabilistic competitions outperforming replan
ten fifteen categories losing three e xploding b locksworld p itchcatch
z enotravel outperforming rff bg categories losing e xploding
b locksworld ystematic tire additionally seh l outperforms replan five
seven categories losing e xploding b locksworld z enotravel section
discuss categories seh f seh l lose replan rff bg
z enotravel logistics domain people transported cities via airplanes
load unload fly action non zero probability effect takes
uncertain number attempts complete task domains probabilistic effect choice change change outcome determinization leads
safe determinized plan replanone replanning needed reach goal
domains including z enotravel outcomes determinization provide effective
way employ deterministic enforced hill climbing note though though
determinization still ignores probabilities action outcomes lead bad
choices domains z enotravel deterministic stochastic enforced
hill climbing must climb large basins z enotravel substantial overhead stochastic backup computations basin expansion leads least constant factor advantage deterministic expansion extension seh might address successfully future
would detect domains stochastic choice change non change
handle domains emphasis determinization
e xploding b locksworld variant blocks world two predicates detonated destroyed block detonate put probability
destroying object placed upon state resulting action depicted figure delete relaxed path goal actual path state dead end attractor
delete relaxation heuristics cr replan rff bg never select action
path goal including action seh f weak dead end detection used experiments select dead action shown resulting poor performance
situation arises would possible use outcomes determinization improved
dead end detector conjunction seh f order avoid selecting actions
dead end detection would carefully implemented managed control run time
costs incurred seh relies critically able expand sufficiently large local mdp regions
online action selection
p itchcatch unavoidable dead end states used domain designers simulate cost penalties however cr heuristic outcomes determinization assigns optimistic values correspond assumed avoidance dead end states


fiw u k alyanam g ivan

current
state

b

b

b

b

b

b

b

b

b

goal state

path
destroyed table

pick table b

b

b

b

b

b

destroyed table

figure illustration critical action choice seh f e xploding b locksworld
ippc p middle state actual path goal delete relaxed path
goal due table exploded block placed onto table resulting
middle state dead end state middle state dead end attractive heuristic
value without regard whether blocks shown remaining explosive charge
state feature shown
local search seh f unable expected improvement cr values
falls back biased random walk domain domain suggests domains seh f performs weakly work needed managing domains
unavoidable deadend states
two categories seh l loses replan e xploding b locksworld
z enotravel categories seh f loses replan greedily following
learned heuristics two categories leads lower success ratio greedily following crff suggesting significant flaws learned heuristics cr although seh able
give least five fold improvement greedy following success ratio two categories improvement large enough seh l match performance seh f
replan relaxed plan heuristic
seh loses rff ystematic tire due weak performance triangle tireworld triangle tireworld provides map connected locations arranged single
safe path source destination exponentially many shorter unsafe paths
determinizing heuristics detect risk unsafe paths greedy following
heuristics lead planners seh take unsafe paths lowering success rate
seh often repair flawed heuristic triangle tireworld domain heuristic attracts seh apparent improvements actually dead ends
contrast rff designed increase robustness determinized plans high probability failure rff continue avoid failure rather relying replanning
failure initial determinized plan high probability failure relative rffs
safe path drawn following two sides triangular map many unsafe paths interior
triangle safety domain represented presence spare tires repair flat tire
chance occurring every step



fis tochastic e nforced h ill c limbing

jslenratio
ffr
seh

jstimeratio
ffr
seh

sr
difference
significant
p value

jslenratio
significant
p value

sr
seh f

sr
ffreplan

b locksworld









yes

p

yes

p

seh f

b oxworld









yes

p

yes

p

seh f

category

winner

table aggregated comparison seh f replan scaled

category

sr
seh f

sr
rffbg

jslenratio
rffbg
seh

jstimeratio
rffbg
seh

sr
difference
significant
p value

b locksworld









yes

p

b oxworld









yes

p

jslenratio
significant
p value

yes

p


winner

seh f
seh f

table aggregated comparison seh f rff bg scaled

threshold rff extends plan execution often detect need use longer
safe route
p erformance l arge p roblems
order demonstrate advantages seh emphasized size grows
present aggregated performance seh f additional large sized generated generators provided first ippc scaling experiments computationally
expensive run two domains widely evaluated literature b locksworld b oxworld stochastic version logistics
b locksworld generated block b oxworld
generated size cities boxes one across three
competitions reached size b oxworld unsolved competition
winner rff aggregated replan rff bg presented tables
experiments scaled consumed hours cpu time
seh f successfully completed majority attempts replan rff
succeeded substantially less often
note although heuristic good b oxworld logistics domains
failure outcomes determinization take account probabilities action outcomes
quite damaging ffr b oxworld leading planner often select action hoping
statistical protocol requires samples random variable averaging performance solution attempts
planner planners yields solution attempts
taking approximately cpu minutes large



fiw u k alyanam g ivan

low probability error outcome note rff uses probable outcome determinization
suffer issues ffr boxworld given high accuracy
heuristic boxworld believe ideas rff likely implemented
tuned achieve better scalability boxworld leave possibility direction
future work understanding scalability rff

summary
proposed evaluated stochastic enforced hill climbing novel generalization
deterministic enforced hill climbing method used planner hoffmann nebel
generalizing deterministic search descendant strictly better current state
heuristic value analyze heuristic mdp around local optimum plateau reached
increasing horizons seek policy expects exit mdp better valued state
demonstrated provides substantial improvement greedy hill climbing
heuristics created two different styles heuristic definition demonstrated
one resulting planner substantial improvement replan yoon et al
rff teichteil konigsbuch et al experiments
runtime stochastic enforced hill climbing concern domains
one reason long runtime number size local optima basins plateaus may
large currently long runtime managed primarily reducing biased random walk
resource consumption exceeds user set thresholds possible future direction regarding
issue prune search space automatically state action pruning

acknowledgments
material upon work supported part national science foundation united
states grant national science council republic china

references
aarts e lenstra j eds local search combinatorial optimization john wiley
sons inc
barto g bradtke j singh p learning act real time dynamic programming artificial intelligence
bertsekas p dynamic programming optimal control athena scientific
bertsekas p tsitsiklis j n neuro dynamic programming athena scientific
bonet b geffner h mgpt probabilistic planner heuristic search journal
artificial intelligence
bonet b geffner h learning depth first search unified heuristic search
deterministic non deterministic settings application mdps proceedings
sixteenth international conference automated scheduling pp



fis tochastic e nforced h ill c limbing

bryce buffet international competition uncertainty part benchmarks
http ippc loria fr wiki images pdf
buffet personal communication
cerny v thermodynamical traveling salesman efficient simulation j optim theory appl
dean kaelbling l p kirman j nicholson time constraints
stochastic domains artificial intelligence
domshlak c hoffmann j probabilistic via heuristic forward search
weighted model counting journal artificial intelligence
fahlman lebiere c cascade correlation learning architecture advances
neural information processing systems pp
gardiol n h kaelbling l p envelope relational mdps proceedings seventeenth annual conference advances neural information processing
systems
gerevini serina propositional csp walksat local search
techniques action graphs constraints
gordon g stable function approximation dynamic programming proceedings
twelfth international conference machine learning pp
hansen e zilberstein lao heuristic search finds solutions
loops artificial intelligence
hoffmann j metric system translating ignoring delete lists numeric
state variables journal artificial intelligence
hoffmann j brafman r contingent via heuristic forward search implicit
belief states proceedings th international conference automated
scheduling
hoffmann j brafman r conformant via heuristic forward search
artificial intelligence
hoffmann j ignoring delete lists works local search topology benchmarks journal artificial intelligence
hoffmann j nebel b system fast plan generation heuristic
search journal artificial intelligence
kautz h selman b satisfiability proceedings tenth european
conference artificial intelligence ecai
kirkpatrick gelatt jr c vecchi optimization simulated annealing science

little thiebaux probabilistic vs replanning workshop international
competition past present future icaps
liu c layland j scheduling multiprogramming hard real time
environment journal association computing machinery


fiw u k alyanam g ivan

mahadevan maggioni proto value functions laplacian framework learning representation control markov decision processes journal machine learning

nilsson n principles artificial intelligence tioga publishing palo alto ca
puterman l markov decision processes discrete stochastic dynamic programming
john wiley sons inc
sanner boutilier c practical solution techniques first order mdps artificial
intelligence
selman b kautz h cohen b local search strategies satisfiability testing
dimacs series discrete mathematics theoretical computer science pp
sutton r learning predict methods temporal differences machine learning

sutton r barto g reinforcement learning introduction mit press
teichteil konigsbuch f kuter u infantes g incremental plan aggregation generating policies mdps proceedings ninth international conference autonomous
agents multiagent systems aamas pp
tesauro g galperin g line policy improvement monte carlo search
nips
wu j givan r discovering relational domain features probabilistic
proceedings seventeenth international conference automated
scheduling pp
wu j givan r automatic induction bellman error features probabilistic journal artificial intelligence
yoon fern givan r replan baseline probabilistic proceedings seventeenth international conference automated scheduling pp
younes h littman weissman asmuth j first probabilistic track
international competition journal artificial intelligence




