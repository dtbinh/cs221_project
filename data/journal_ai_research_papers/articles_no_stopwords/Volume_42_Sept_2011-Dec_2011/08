journal artificial intelligence

submitted published

combining evaluation metrics via unanimous
improvement ratio application clustering tasks
enrique amigo
julio gonzalo

enrique lsi uned es
julio lsi uned es

uned nlp ir group juan del rosal
madrid spain

javier artiles

javier artiles qc cuny edu

blender lab queens college cuny
kissena blvd ny usa

felisa verdejo

felisa lsi uned es

uned nlp ir group juan del rosal
madrid spain

abstract
many artificial intelligence tasks cannot evaluated single quality criterion
sort weighted combination needed provide system rankings
weighted combination measures slight changes relative weights may produce
substantial changes system rankings introduces unanimous improvement ratio uir measure complements standard metric combination criteria
van rijsbergens f measure indicates robust measured differences
changes relative weights individual metrics uir meant elucidate
whether perceived difference two systems artifact individual metrics
weighted
besides discussing theoretical foundations uir presents empirical
confirm validity usefulness metric text clustering tradeoff precision recall metrics
particularly sensitive weighting scheme used combine remarkably
experiments uir used predictor well differences
systems measured given test bed hold different test bed

introduction
many artificial intelligence tasks cannot evaluated single quality criterion
sort weighted combination needed provide system rankings many
instance require considering precision p recall r compare systems
performance perhaps common combining function f measure van rijsbergen includes parameter sets relative weight metrics
metrics relative weight f computes harmonic mean
weighted combination measures relative weights established
intuitively given task time slight change relative weights may
produce substantial changes system rankings reason behavior
overall improvement f often derives improvement one individual
c

ai access foundation rights reserved

fiamigo gonzalo artiles verdejo

metrics expense decrement instance system improves
system b precision loss recall f may say better b viceversa
depending relative weight precision recall e value
situation common one might expect table shows evaluation
different tasks extracted acl su et al conference proceedings
p r combined f measure considered
three evaluation one maximizes f presented best
baseline alternative method considered note
cases top ranked system improves baseline according f measure
cost decreasing one metrics instance case word
alignment average r grows p decreases
sentiment analysis p increases four points r decreases five points
reasonable assume contrastive system indeed improving baseline
evaluation alternative controversial cases
alternative improves best system according one metric improved according therefore depending relative metric weighting
alternative could considered better worse best scored system
conclusion parameter crucial comparing real systems
practice however authors set equal weights precision recall
standard agnostic choice requires justification thus without notion
much perceived difference systems depends relative weights
metrics interpretation f combination scheme
misleading
goal therefore way estimating extent perceived difference
metric combination scheme f robust changes relative
weights assigned individual metric
propose novel measure unanimity improvement ratio uir
relies simple observation system improves system b according
individual metrics improvement unanimous better b weighting
scheme given test collection n test cases test cases improvements
unanimous robust perceived difference average difference f
combination scheme
words well statistical significance tests provide information
robustness evaluation across test cases perceived difference two systems
artifact set test cases used test collection uir meant provide
information robustness evaluation across variations relative metric
weightings perceived difference two systems artifact relative metric
weighting chosen evaluation metric
experiments clustering test collections uir contributes analysis
evaluation two ways
allows detect system improvements biased metric weighting
scheme cases experimenters carefully justify particular choice
relative weights check whether swapped vicinity


ficombining evaluation metrics via unanimous improvement ratio

systems
precision recall
f
task word alignment huang
baseline
bm


max f
link select


alternative



task opinion question answering li et al
baseline
system



max f
ophit



alternative
oppagerank



task sentiment analysis kim et al
baseline
baseline



max f
vs lsa dtp



alternative
vs pmi



task lexical reference rule extraction shnarch et al
baseline
expansion



max f
wordnet wiki



alternative rules dice filter



table three way system comparisons taken acl conference proceedings su et al

increases substantially consistency evaluation across datasets
supported high unanimous improvement ratio much likely hold
different test collection perhaps relevant practical application
uir predictor much replicable across test collections
although work presented applies areas
focus clustering task one relevant examples clustering
tasks specially sensitive metric relative weightings goals
investigate empirically whether clustering evaluation biased precision
recall relative weights f use one recent test collections
focused text clustering artiles gonzalo sekine
introduce measure quantifies robustness evaluation across
metric combining criteria leads us propose uir measure derived
conjoint measurement theory luce tukey
analyze empirically uir f measure complement
illustrate application uir comparing systems context shared
task measure uir serves predictor consistency evaluation
across different test collections



fiamigo gonzalo artiles verdejo

figure evaluation semantic labeling conll

combining functions evaluation metrics
section briefly review different metrics combination criteria present
rationale behind metric weighting well effects systems ranking
f measure
frequent way combining two evaluation metrics f measure van rijsbergen originally proposed evaluation information retrieval systems
ir use expanded many tasks given two metrics p r e g
precision recall purity inverse purity etc van rijsbergens f measure combines
single measure efficiency follows
f r p

p


r

f assumes value set particular evaluation scenario parameter
represents relative weight metrics cases value crucial
particular metrics correlated instance figure shows precision
recall levels obtained conll shared task evaluating semantic role labeling
systems carreras marquez except one system every substantial improvement
precision involves increase recall case relative metric weighting
substantially modify system ranking
cases metrics completely correlated decreasing marginal effectiveness property van rijsbergen ensures certain robustness across values f
satisfies property states large decrease one metric cannot compensated large increase metric therefore systems low precision
recall obtain low f values value discussed detail section however section cases decreasing marginal


ficombining evaluation metrics via unanimous improvement ratio

effectiveness property prevent f measure overly sensitive small
changes value

figure example two systems evaluated break even point

precision recall break even point
another way combining metrics consists evaluating system point one
metric equals tao li zhu method applicable system
represented trade metrics instance precision recall curve
method relies idea increasing metrics implies necessarily overall
quality increase instance assumes obtaining precision recall
point better obtaining precision recall point
actually break even point assumes relevance metrics considers
precision recall point system distributes efforts equitably
metrics indeed could change relative relevance metrics computing
break even point
figure illustrates idea continuous curve represents trade
precision recall system straight diagonal represents points
metrics return score quality system corresponds therefore
intersection diagonal precision recall curve hand
discontinuous curve represents another system achieves increase precision
low recall levels cost decreasing precision high recall levels according
break even points second system superior first one
however could give relevance recall identifying point recall doubles
precision case would obtain intersection points q q shown
figure reverses quality order systems conclusion break even
point assumes arbitrary relative relevance combined metrics
area precision recall curve
approaches average scores every potential parameterization metric combining function instance mean average precision map oriented ir systems


fiamigo gonzalo artiles verdejo

computes average precision across number recall levels another example
receiver operating characteristic roc function used evaluate binary classifiers
cormack lynam roc computes probability positive sample receives
confidence score higher negative sample independently threshold used
classify samples functions related area auc exists
precision recall curve cormack lynam
map roc low high recall regions relative relevance
computing area could change measures order assign different
weights high low recall levels indeed weng poon weighted area
curve proposed something similar would happen average f across different
values
note measures applied certain kinds binary
classification document retrieval system output seen ranking
different cutoff points ranking give different precision recall values
directly applicable particular clustering focus work


combining metrics clustering tasks
section present metric combination experiments specific clustering task
corroborate importance quantifying robustness systems across different
weighting schemes
clustering task
clustering grouping similar items applications wide range artificial intelligence
particular context textual information access clustering
employed information retrieval clustering text documents according content
similarity document summarization grouping pieces text order detect redundant
information topic tracking opinion mining e g grouping opinions specific topic
etc
scenarios clustering distributions produced systems usually evaluated
according similarity manually produced gold standard extrinsic evaluation
wide set metrics measure similarity amigo gonzalo artiles
verdejo rely two quality dimensions extent items
cluster belong group gold standard ii
extent items different clusters belong different groups gold standard
wide set extrinsic metrics proposed entropy class entropy steinbach
karypis kumar ghosh purity inverse purity zhao karypis
precision recall bcubed metrics bagga baldwin metrics counting
pairs halkidi batistakis vazirgiannis meila etc

see work amigo et al detailed overview



ficombining evaluation metrics via unanimous improvement ratio

dataset
weps web people search campaigns focused task disambiguating person
names web search input systems ranked list web retrieved
web search engine person name query e g john smith
challenge correctly estimate number different people sharing name
search group documents referring individual every person
name weps datasets provide around web top search
quoted person name query order provide different ambiguity scenarios person
names sampled us census wikipedia listings program committee
members computer science conferences
systems evaluated comparing output gold standard manual grouping
documents produced two human judges two rounds first annotated corpus
independently discussed disagreements together note single
document assigned one cluster amazon search list
instance may refer books written different authors name weps
task therefore overlapping clustering general case clustering
items restricted belong one single cluster weps datasets
official evaluation metrics reflect fact
experiments focused evaluation obtained weps
artiles gonzalo sekine weps artiles et al evaluation campaigns
weps corpus includes data web test bed mann
used trial purposes follows similar annotation guidelines although number
document per ambiguous name variable refer corpora weps
trial weps b weps
thresholds stopping criteria
clustering task involves three main aspects determine systems output quality
first one method used measuring similarity documents second
clustering k neighbors hierarchical agglomerative clustering etc
third aspect considered usually consists couple related variables fixed
similarity threshold two considered related stopping
criterion determines clustering process stops consequently number
clusters produced system
figure shows purity inverse purity values change different clustering
stopping points one systems evaluated weps b corpus purity focuses
frequency common category cluster amigo et al
c set clusters evaluated l set categories reference distribution
weps datasets selected experiments address relevant well defined
clustering task ii use widespread weps datasets used hundreds experiments
since first weps evaluation iii runs submitted participants weps weps
available us essential experiment different evaluation measures weps datasets
freely available http nlp uned es weps
system bag words tf idf word weighting stopword removal cosine distance
hierarchical agglomerative clustering



fiamigo gonzalo artiles verdejo

n number clustered items purity computed taking weighted average
maximal precision values
purity

x ci


n

maxj precision ci lj

precision cluster ci given category lj defined
ci lj
precision ci lj
ci


purity penalizes noise cluster reward grouping items
category together simply make one cluster per item reach trivially
maximum purity value inverse purity focuses cluster maximum recall
category inverse purity defined
inverse purity

x li


n

maxj precision li cj

inverse purity rewards grouping items together penalize mixing items
different categories reach maximum value inverse purity making single
cluster items
change stopping point implies increase purity cost decrease
inverse purity viceversa therefore possible value f rewards different stopping
points phenomenon produces high dependency clustering evaluation
metric combining function

figure example trade purity inverse purity optimizing
grouping threshold



ficombining evaluation metrics via unanimous improvement ratio

robustness across values
determining appropriate value given scenario trivial instance
users point view weps task easier discard irrelevant documents
good cluster precision perfect high recall
check additional relevant documents clusters precision high
recall therefore seems inverse purity priority purity
e value point view company providing
web people search service however situation quite different priority
high precision mixing profiles say criminal doctor may
company sued perspective receive high value
weps campaign decided agnostic set neutral value
table shows resulting system ranking weps b according f set
ranking includes two baseline systems b consists grouping document
separate cluster b consists grouping documents one single cluster
b maximizes purity b maximizes inverse purity
b b may obtain high low f measure depending value
table shows b outperforms b considerable number systems
reason weps b test set many singleton clusters
people referred one web page means default strategy
making one cluster per document achieve maximal purity
acceptable inverse purity however fixed b goes bottom
ranking outperformed systems including baseline b
note outperforming trivial baseline system b crucial optimize
systems given optimization cycle could otherwise lead baseline
b drawback b informative output depend
input crucially sensitive variations words performance
robust changes metric combination criterion remarkably top scoring
system best values primary motivation article
quantify robustness across values order complement information given
traditional system ranking
robustness across test beds
average size clusters gold standard may change one test bed
another affects purity inverse purity trade clustering system
may obtain different balance metrics different corpora may
produce contradictory evaluation comparing systems across different corpora
even value
instance weps b test bed artiles et al b substantially outperforms
b vs f weps data set artiles et al however
b outperforms b versus reason singletons less common
weps words comparison b b depends value
particular distribution reference cluster sizes test bed
point system improvements robust across values
case b b affected phenomenon therefore estimating


fiamigo gonzalo artiles verdejo

f
ranked systems f














b

















b




f
ranked systems















b

b

f



















table weps b system ranking according f vs f purity inverse
purity

robustness system improvements changes prevent reaching contradictory
different test beds indeed evidence presented section

proposal
primary motivation article quantify robustness across values
order complement information given traditional system rankings end
introduce section unanimous improvement ratio
unanimous improvements
combining evaluation metrics closely related theory conjoint
measurement see section detailed discussion van rijsbergen argued
possible determine empirically metric combining function
adequate context information retrieval evaluation however starting
measurement theory principles van rijsbergen described set properties metric
combining function satisfy set includes independence axiom called
single cancellation monotonicity property derives monotonicity
property states quality system surpasses equals another one according
metrics necessarily equal better words one system


ficombining evaluation metrics via unanimous improvement ratio

better dependence whatsoever relative importance
metric set
define combination procedure metrics unanimous improvement
property
qx qx b x x qx qx b
qx quality according set metrics x
relationship dependence metrics scaled weighted
degree correlation metric set equality derived directly
unanimous equality implies systems obtain score metrics
qx qx b qx qx b qx b qx

strict unanimous improvement implies one system improves strictly
least one metric improved according metric
qx qx b qx qx b qx qx b
qx qx b qx b qx

non comparability k derived occurs metrics favor one
system metrics favor refer cases metric biased
improvements
qx k qx b qx qx b qx b qx

theoretical properties unanimous improvement described depth
section important property unanimous improvement
relational structure depend relative metric weightings satisfying
independence monotonicity axiom words claim system improvement according metric combining function depend whatsoever metric
weightings quality decrease according individual metric
theoretical justification assertion developed section
unanimous improvement ratio
according unanimous improvement unique observable test case
three valued function unanimous improvement equality biased improvement need
however way quantitatively comparing systems
given two systems b unanimous improvement relationship set
test cases samples improves b qx qx b samples b improves qx b qx samples biased improvements qx k qx b
refer sets ta b tb tak b respectively unanimous improvement ratio uir defined according three formal restrictions


fiamigo gonzalo artiles verdejo

test cases











precision
system system b





















recall
system system b





















b
yes
yes
yes
yes
yes
yes





b
yes
yes




yes
yes



table example experiment input compute uir
uir b decrease number biased improvements tak b
boundary condition samples biased improvements tak b
uir b
improves b much b improves ta b tb uir b
given fixed number biased improvements tak b uir b proportional
ta b inversely proportional tb
given restrictions propose following uir definition
uirx b

ta b tb



qx qx b qx b qx

alternatively formulated
uirx b p b p b
probabilities estimated frequentist manner
uir range symmetric uirx b uirx b
illustration uir computed consider experiment outcome table systems
b compared terms precision recall test cases test case
instance unanimous improvement b better terms precision
recall table uir value
uirx b

ta b tb


uirx b



uir two formal limitations first transitive see section therefore
possible define linear system ranking uir however


ficombining evaluation metrics via unanimous improvement ratio

necessary uir meant provide ranking complement ranking provided
f measure metric combining function indicating robust
changes section illustrates uir integrated insights provided
system ranking
second limitation uir consider improvement ranges therefore
less sensitive f measure empirical however uir
sensitive enough discriminate robust improvements versus metric biased improvements
section make empirical comparison non parametric definition uir
parametric version make non parametric definition preferable

theoretical foundations
section discuss theoretical foundations unanimous improvement ratio
framework conjoint measurement theory proceed describe
formal properties uir implications point view evaluation
methodology readers interested solely practical implications uir may
proceed directly section
conjoint measurement theory
combining evaluation metrics closely related conjoint measurement theory independently discovered economist debreu
mathematical psychologist r duncan luce statistician john tukey luce tukey
theory measurement defines necessary conditions state homomorphism empirical relational structure e g john bigger bill
numeric relational structure johns height meters bills height meters
case conjoint measurement theory relational structure factored
two ordered substructures e g height weight
context numerical structures given evaluation metric scores e g
purity inverse purity however empirical quality ordering
clustering systems different human assessors could assign relevance purity
inverse purity viceversa nevertheless conjoint measurement theory provide
mechanisms state kind numerical structures produce homomorphism
assuming empirical structure satisfies certain axioms van rijsbergen used
idea analyze combining evaluation metrics axioms shape
additive conjoint structure r p quality system according two evaluation
metrics r p axioms
connectedness systems comparable formally r p
r p r p r p
transitivity r p r p r p r p implies r p r p
axioms transitivity connectedness shape weak order
thomsen condition r p r p r p r p imply r p
r p indicates equal effectiveness


fiamigo gonzalo artiles verdejo

independence two components contribute effects independently effectiveness formally r p r p implies r p r p p
r p r p implies r p r p r property implies
monotonicity narens luce states improvement metrics necessarily produces improvement according metric combining function
restricted solvability property concerned continuity
component makes precise intuitively would expect considering
existence intermediate levels formally whenever r p r p r p
exists r r p r p
essential components variation one leaving constant gives variation effectiveness exists r r p case
r p r p exists p p r case
r p r p
archimedean property merely ensures intervals component
comparable
f measure proposed van rijsbergen arithmetic mean p r
satisfy axioms according restrictions indeed unlimited set acceptable combining functions evaluation metrics defined f relational structure
however satisfies another property satisfied functions
arithmetic mean property decreasing marginal effectiveness basic idea
increasing one unit one metric decreasing one unit metric
improve overall quality e first metric weight combining function imply great loss one metric compensated great
increase defined
r p n p n r n r p
according high values metrics required obtain high overall
improvement makes measures observing property f robust
arbitrary metric weightings
formal properties unanimous improvement
unanimous improvement x trivially satisfies desirable properties proposed van rijsbergen metric combining functions transitivity independence
thomsen condition restricted solvability essential components decreasing marginal
effectiveness exception connectedness property given non comparability k biased improvements see section derived unanimous improvement possible system pairs neither qx qx b qx b qx
hold therefore connectedness satisfied
formally limitation unanimous improvement represent
weak order cannot satisfy transitivity connectedness simultaneously let
us elaborate issue
sake simplicity consider combination two metrics r p



ficombining evaluation metrics via unanimous improvement ratio

systems

b
c

metric x




metric x




table counter sample transitivity unanimous improvement
could satisfy connectedness considering biased improvements represent
equivalent system pairs case transitivity would satisfied see
instance table according table
qx b k qx qx c k qx b
therefore considering k represents equivalence
qx b qx qx c qx b

qx c qx
summary choose satisfy transitivity connectedness
unanimous improvement derive weak order
uniqueness unanimous improvement
unanimous improvement interesting property contradict
evaluation given f measure regardless value used f
qx qx b f f b
due fact f measure value satisfies monotonicity
axiom unanimous improvement grounded property essential
purpose checking robustness system improvements across values
crucially unanimous improvement function satisfies property
precisely unanimous improvement relational structure satisfying
monotonicity contradict additive conjoint structure see section
order prove assertion need define concept compatibility
additive conjoint structure let add additive conjoint structure let r
relational structure say r compatible conjoint structure

ha b add qx r qx b qx add qx b
words r holds additive conjoint holds want prove
unanimous improvement relation satisfies property therefore
prove r monotonic compatible relational structure
necessarily matches unanimous improvement definition


fiamigo gonzalo artiles verdejo

r monotonic compatible qx r qx b xi xi b xi x
split
r monotonic compatible qx r qx b xi xi b xi x
r monotonic compatible qx r qx b xi xi b xi x
proving immediate since rightmost component corresponds monotonicity property definition let us prove reductio ad absurdum assuming
exists relational structure
monotonic compatible qx qx b xi x xi xi b
case could define additive conjoint structure combined measure
q x x xi n xn big enough q x q x b
q additive conjoint structure would contradict therefore would compatible
contradiction conclusion predicate true unanimous improvement x
monotonic compatible relational structure
interesting corollary derived analysis unanimous improvement compatible relational structure formally conclude
measurement system improvements without dependence metric weighting schemes
derive weak order e one satisfies transitivity connectedness
corollary practical implications possible establish system ranking
independent metric weighting schemes
natural way proceed therefore use unanimous improvement addition
standard f measure suitable value provides additional information
robustness system improvements across values

f versus uir empirical study
section perform number empirical studies weps corpora order
uir behaves practice first focus number empirical
uir rewards robustness across values information complementary information provided f second examine extent f
uir correlated
uir rewarding robustness
figure shows three examples system comparisons weps b corpus metrics
purity inverse purity curve represents f value obtained one system
according different values system black curves compared
grey curves three graphs cases similar quality increase
according f uir however ranges depending robust
difference changes highest difference uir system
pair rightmost graph systems swap f values value


ficombining evaluation metrics via unanimous improvement ratio

f
uir

improvements

system pairs



cases
system pairs



table uir f increase f increases values

figure f measure vs uir rewarding robustness
smallest uir value better values
worse larger comparison illustrates uir captures similar
increments f ones less dependent relative weighting scheme
precision recall
let us consider two system combinations weps b corpus dividing
two sets system pairs f increases values e purity
inverse purity increases ii pairs relative systems performance swaps
value e f increases values decreases rest
one would expect average increase f larger system pairs
one beats every value surprisingly true table shows
average increments uir f sets uir behaves expected average
value substantially larger set different lead contradictory
vs average relative increase f however similar
sets vs
conclusion certain f improvement range say anything
whether purity inverse purity simultaneously improved
words matter large measured improvement f still extremely
dependent weighting individual metrics measurement
conclusion corroborated considering independently metrics purity inverse purity according statistical significance improvements
independent metrics distinguish three cases
opposite significant improvements one metrics purity inverse purity
increases decreases changes statistically significant


fiamigo gonzalo artiles verdejo

f
uir

significant
concordant
improvements
pairs



significant
opposite
improvements
pairs



non
significant
improvements
pairs



table uir f increases vs statistical significance tests
concordant significant improvements metrics improve significantly least
one improves significantly decrease significantly
non significant improvements statistically significant differences
systems metric
use wilcoxon test p detect statistical significance table
shows average uir f values three cases remarkably
f average increase even larger opposite improvements set
concordant improvements set according would seem
f rewards individual metric improvements obtained cost smaller
decreases metric uir hand sharply different behavior
strongly rewarding concordant improvements set versus
confirm uir provides essential information experimental
outcome two system comparisons provided main evaluation metric
f
correlation f uir
fact uir f offer different information outcome experiment
imply uir f orthogonal fact correlation
values
figure represents f differences uir values possible system pair
weps test bed general trends high uir values imply positive difference
f ii high f values imply anything uir values iii low uir seem
imply anything f values overall figure suggest triangle relationship
gives pearson correlation
reflecting improvement ranges
consistent difference two systems values uir rewards
larger improvement ranges let us illustrate behavior considering three sample system
pairs taken weps test bed
figure represents f values three system pairs cases one system
improves values however uir assigns higher values larger improvements f larger distance black grey curves reason


ficombining evaluation metrics via unanimous improvement ratio

figure f vs uir

figure f vs uir reflecting improvement ranges
larger average improvement test cases makes less likely cases individual test
cases ones uir considers contradict average
another interesting finding metrics improved metric
weakest improvement determines behavior uir figure illustrates
relationship ten system pairs largest improvement pearson correlation
graph words individual metrics improve uir sensitive
weakest improvement
analysis boundary cases
order better understanding relationship uir f
examine detail two cases system improvements uir f produce drastically
different two cases marked b figure
point marked case figure corresponds comparison systems
exists substantial statistically significant difference
systems according f however uir low value e improvement
robust changes according uir


fiamigo gonzalo artiles verdejo

figure correlation uir weakest single metric improvement

figure purity inverse purity per test case systems
visual explanation seen figure shows purity
inverse purity systems every test case test cases
important advantage purity cost slight consistent loss inverse
purity given f compares purity inverse purity ranges states
exists important statistically significant improvement however
slight consistent decrease inverse purity affects uir decreases
test cases improvements f metric biased k notation
case b see figure opposite example small difference systems
according f differences purity inverse purity
small however gives small consistent improvements purity inverse
purity test cases right vertical line figure unanimous
improvements therefore uir considers exists robust overall improvement
case


ficombining evaluation metrics via unanimous improvement ratio

figure purity inverse purity per test case systems
cases uir gives additional valuable information comparative behavior systems
significance threshold uir
mentioned earlier uir parallelism statistical significance tests
typically used information retrieval estimate probability p observed
difference two systems obtained chance e difference artifact
test collection rather true difference systems computing
statistical significance useful establish threshold allows binary decision
instance often said statistically significant p significant
otherwise choosing level significance arbitrary nevertheless helps reporting
summarizing significance tests stricter thresholds increase confidence test
run increased risk failing detect significant
situation applies uir would establish uir threshold
decides whether observed difference reasonably robust changes set
threshold could restrictive decide instance improvement
significantly robust uir condition however hard would
never satisfied practice therefore uir test would informative
hand set permissive threshold satisfied system
pairs informative question whether exists
threshold uir values obtaining uir threshold guarantees
improvement robust time strong satisfied practice
given set two system combinations uir surpasses certain candidate
threshold think desirable features
must able differentiate two types improvements robust vs nonrobust words one two types usually empty almost empty
threshold informative


fiamigo gonzalo artiles verdejo

robust set contain high ratio two system combinations
average f increases values f f b
robust set contain high ratio significant concordant improvements
low ratio significant opposite improvements see section
robust set contain low ratio cases f contradicts uir dots
figure region f

figure improvement detected across uir thresholds
figure shows conditions met every threshold range
uir threshold accepts around system pairs low ratio
significant opposite improvements high ratio significant concordant improvements threshold half robust cases f increases values
cases f increases seems therefore uir reasonable
threshold least clustering task note however rough rule thumb
revised adjusted dealing clustering tasks weps
uir system rankings
presented far focused pairwise system comparisons according
nature uir turn question use uir component
analysis evaluation campaign
order answer question applied uir weps
evaluation campaign artiles et al campaign best runs system
ranked according bcubed precision recall metrics combined f
addition participant systems three baseline approaches included ranking


ficombining evaluation metrics via unanimous improvement ratio

documents one cluster b document one cluster b union
bcomb
table shows applying uir weps participant systems robust
improvements represented third column improved systems every system
displays set systems improves uir fourth column
reference system defined follows given system reference system
one improves maximal uir
sref argmaxs uir
words sref represents system replaced order
robustly improve across different values finally last column uir
reference system displays uir system reference uir sref si
note uir adds insights evaluation process let us highlight two
interesting facts
although three top scoring systems similar performance
terms f consistently best system according uir
reference systems
baseline b contrast reference reference
therefore f uir together strongly point towards best
system f alone able discern set three top scoring systems
although non informative baseline b documents one cluster better
five systems according f improvement robust according uir
note uir signal near baseline behaviors participant systems low
value receive large f depending nature test collection
average cluster large small systems tend cluster everything
nothing artificially rewarded opinion substantial improvement
f alone

uir predictor stability across test collections
common issue evaluating systems deal natural language
different test collections often contradictory particular case text clustering
factor contributes average size clusters vary across
different test beds variability modifies optimal balance precision
recall system tends favor precision creating small clusters may good
dataset small average cluster size worse test collection
larger average cluster size
therefore apply f combine single metrics reach contradictory
different test beds uir depend metric weighting criteria
hypothesis high uir value ensures robustness evaluation across test beds
see work artiles et al extended explanation



fiamigo gonzalo artiles verdejo

system

f













b























bcomb



b


improved systems
uir
b
b
b


b






bcomb





reference
system








b






uir
reference system















table weps bcubed precision recall f uir measures
words given particular test bed high uir value good predictor
observed difference two systems still hold test beds
following experiment designed verify hypothesis implemented
four different systems weps agglomerative clustering hac used best systems weps system employs
certain cluster linkage technique complete link single link certain feature extraction criterion word bigrams unigrams system experimented
stopping criteria therefore used x system variants overall evaluated
systems weps weps b weps corpora
first observation given system pairs f gives consistent
three test beds cases system pairs best system
different depending test collection robust evaluation criterion predict
given single test collection whether still hold collections
consider two alternative ways predicting observed difference system
better system b one test bed still hold three test beds
first f f b larger value reference test bed
likely f f b still positive different test collection
weps originally used training first weps campaign weps b used testing



ficombining evaluation metrics via unanimous improvement ratio

second u ir b instead f larger uir likely
f f b positive different test bed
summary want compare f uir predictors robust
change test collection tested
select reference corpus weps weps b weps test beds
cref weps weps b weps
system pair reference corpus compute improvement one
system respect according f uir take system pairs
one improves certain threshold uirc
uir systems test bed c fc f
system test bed c
su ir c uirc
sf c fc fc
every threshold su ir sf represent set robust improvements
predicted uir f respectively
consider system pairs one improves according f
three test collections simultaneously
fc fc c
gold standard compared predictions su ir sf
every threshold compute precision recall uir f predictions
su ir c sf c versus actual set robust across collections

p recision su ir c

su ir c
su ir

p recision sf c

sf c
sf c

recall su ir c

recall sf c

su ir c


sf c


trace precision recall curve predictors f uir
compare figures precision recall values f triangles
uir rhombi figure displays one reference test beds weps weps b weps
altogether figures uir much effective f predictor
note f suffers sudden drop performance low recall levels suggests
curve parametric uir refers alternative definition uir explained section



fiamigo gonzalo artiles verdejo

figure predictive power uir f weps

figure predictive power uir f weps b



ficombining evaluation metrics via unanimous improvement ratio

figure predictive power uir f weps
big improvements f tend due peculiarities test collection rather
real superiority one system versus
opinion remarkable differences uir better indicators
reliability measured difference f amount measured difference
therefore uir useful know stable changes
changes test collection e indicator reliable perceived difference

note explicitly tested dependency reliability uir
number test cases reference collection however working
collection less test cases unlikely practical terms usability uir
granted test collections least respect number test cases

parametric versus non parametric uir
according analysis see section given two measures p r relational
structure pairs hpi ri depend weighting criteria unanimous
improvement
b pa pb ra rb
comparing systems uir measure counts unanimous improvement
across test cases
uirx b

ta b tb


alternatively formulation expressed terms probabilities


fiamigo gonzalo artiles verdejo

uirx b prob b prob b
probabilities estimated frequentist manner
said main drawback unanimous improvement threevalued function consider metric ranges uir inherits drawback
consequence uir less sensitive combining schemes f measure
order solve drawback could estimate uir parametrically however
section seem indicate best option
one way estimating p rob b p rob b consists assuming
metric differences p r two systems across test cases follow normal bivariate
distribution estimate distribution case samples provided
test bed estimating density function p rob p r estimate p rob
b
p rob b p rob p r

z p r

p rob p r dp dr
p r

expression used compute uirx b prob b prob b
leads parametric version uir
order compare effectiveness parametric uir versus original uir
repeated experiment described section adding uirparam precision recall
curves figures squares figures represent
parametric version uir note behavior lies somewhere f nonparametric uir low levels recall behaves original uir intermediate
levels general worse original definition better f
recall high end overlaps f probably due fact
parametric uir estimation considers ranges becomes sensitive unreliability
high improvements f

conclusions
work addressed practical strong dependency usually
degree arbitrariness relative weights assigned metrics applying metric
combination criteria f
theory measurement established relevant theoretical fundamental one monotonic relational structure
contradict additive conjoint structure unique relationship
transitive implies possible establish ranking complete ordering systems without assuming arbitrary relative metric weighting transitive
relationship however necessary ensure robustness specific pairwise system
comparisons
theoretical analysis introduced unanimous improvement ratio uir estimates robustness measured system improvements across potential
metric combining schemes uir measure complementary metric combination
computation employed matlab tool



ficombining evaluation metrics via unanimous improvement ratio

scheme works similarly statistical relevance test indicating perceived difference two systems reliable biased particular weighting scheme used
evaluate overall performance systems
empirical text clustering task particularly sensitive
confirm uir indeed useful analysis tool pairwise system comparisons similar increments f uir captures ones less dependent
relative weighting scheme precision recall ii unlike f uir rewards system
improvements corroborated statistical significance tests single measure iii practice high uir tends imply large f increase large increase
f imply high uir words large increase f completely
biased weighting scheme therefore uir essential information add f
looking evaluation campaign uir proved useful discern
best system among set systems similar performance according f
ii penalize trivial baseline strategies systems baseline behavior
perhaps relevant side effect proposed measure defined
uir good estimator robust changes test collection
words given measured increase f test collection high uir value makes
likely increase observed test collections remarkably uir
estimates cross collection robustness f increases much better absolute value
f increase
limitation present study tested uir text clustering
usefulness clustering already makes uir useful analysis
tool potential goes well beyond particular natural language general many artificial intelligence evaluated terms
many individual measures trivial combine uir powerful tool
many scenarios
uir evaluation package available download http nlp uned es

acknowledgments
partially supported spanish government grant holopedia
tin c regional government madrid network
vicmr tic

references
amigo e gonzalo j artiles j verdejo f comparison extrinsic clustering
evaluation metrics formal constraints information retrieval
artiles j gonzalo j sekine weps evaluation campaign overview
web people search clustering task proceedings nd web people search
evaluation workshop weps
artiles j gonzalo j sekine semeval weps evaluation establishing benchmark web people search task proceedings th
international workshop semantic evaluations semeval pp stroudsburg pa usa association computational linguistics


fiamigo gonzalo artiles verdejo

bagga baldwin b entity cross document coreferencing
vector space model proceedings th annual meeting association
computational linguistics th international conference computational
linguistics coling acl pp
carreras x marquez l introduction conll shared task semantic
role labeling ng h riloff e eds hlt naacl workshop eighth
conference computational natural language learning conll pp
boston massachusetts usa association computational linguistics
cormack g v lynam r trec spam track overview proceedings
fourteenth text retrieval conference trec
debreu g topological methods cardinal utility theory mathematical methods
social sciences stanford university press
ghosh j scalable clustering methods data mining ye n ed handbook
data mining lawrence erlbaum
halkidi batistakis vazirgiannis clustering validation techniques
journal intelligent information systems
luce r tukey j simultaneous conjoint measurement scale type
fundamental measurement journal mathematical psychology
mann g multi document statistical fact extraction fusion ph thesis
johns hopkins university
meila comparing clusterings proceedings colt
narens l luce r measurement theory numerical assignments psychological bulletin
steinbach karypis g kumar v comparison document clustering
techniques kdd workshop text mining
su k su j wiebe j li h eds proceedings joint conference
th annual meeting acl th international joint conference
natural language processing afnlp association computational linguistics suntec singapore
tao li c z zhu empirical studies multilabel classification proceedings th ieee international conference tools artificial intelligence
ictai
van rijsbergen c j foundation evaluation journal documentation

weng c g poon j evaluation measure imbalanced datasets
roddick j f li j christen p kennedy p j eds seventh australasian
data mining conference ausdm vol crpit pp glenelg south
australia acs
zhao karypis g criterion functions document clustering experiments
analysis technical report tr department computer science university minnesota minneapolis mn



