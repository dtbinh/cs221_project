Journal Artificial Intelligence Research 42 (2011) 689-718

Submitted 06/11; published 12/11

Combining Evaluation Metrics via Unanimous
Improvement Ratio Application Clustering Tasks
Enrique Amigo
Julio Gonzalo

enrique@lsi.uned.es
julio@lsi.uned.es

UNED NLP & IR Group, Juan del Rosal 16
Madrid 28040, Spain

Javier Artiles

javier.artiles@qc.cuny.edu

Blender Lab, Queens College (CUNY),
65-30 Kissena Blvd, NY 11367, USA

Felisa Verdejo

felisa@lsi.uned.es

UNED NLP & IR Group, Juan del Rosal 16
Madrid 28040, Spain

Abstract
Many Artificial Intelligence tasks cannot evaluated single quality criterion
sort weighted combination needed provide system rankings. problem
weighted combination measures slight changes relative weights may produce
substantial changes system rankings. paper introduces Unanimous Improvement Ratio (UIR), measure complements standard metric combination criteria
(such van Rijsbergens F-measure) indicates robust measured differences
changes relative weights individual metrics. UIR meant elucidate
whether perceived difference two systems artifact individual metrics
weighted.
Besides discussing theoretical foundations UIR, paper presents empirical
results confirm validity usefulness metric Text Clustering problem, tradeoff precision recall based metrics results
particularly sensitive weighting scheme used combine them. Remarkably,
experiments show UIR used predictor well differences
systems measured given test bed hold different test bed.

1. Introduction
Many Artificial Intelligence tasks cannot evaluated single quality criterion,
sort weighted combination needed provide system rankings. Many problems,
instance, require considering Precision (P) Recall (R) compare systems
performance. Perhaps common combining function F-measure (van Rijsbergen, 1974), includes parameter sets relative weight metrics;
= 0.5, metrics relative weight F computes harmonic mean.
problem weighted combination measures relative weights established
intuitively given task, time slight change relative weights may
produce substantial changes system rankings. reason behavior
overall improvement F often derives improvement one individual
c
2011
AI Access Foundation. rights reserved.

fiAmigo, Gonzalo, Artiles & Verdejo

metrics expense decrement other. instance, system improves
system B precision loss recall, F may say better B viceversa,
depending relative weight precision recall (i.e. value).
situation common one might expect. Table 1 shows evaluation results
different tasks extracted ACL 2009 (Su et al., 2009) conference proceedings,
P R combined using F-measure. paper considered
three evaluation results: one maximizes F, presented best result
paper, baseline, alternative method considered. Note
cases, top ranked system improves baseline according F-measure,
cost decreasing one metrics. instance, case paper Word
Alignment, average R grows 54.82 72.49, P decreases 72.76 69.19.
paper Sentiment Analysis, P increases four points R decreases five points.
reasonable assume contrastive system indeed improving baseline?
evaluation results alternative approach controversial: cases,
alternative approach improves best system according one metric, improved according other. Therefore, depending relative metric weighting,
alternative approach could considered better worse best scored system.
conclusion parameter crucial comparing real systems.
practice, however, authors set = 0.5 (equal weights precision recall)
standard, agnostic choice requires justification. Thus, without notion
much perceived difference systems depends relative weights
metrics, interpretation results F combination scheme
misleading.
goal is, therefore, find way estimating extent perceived difference
using metric combination scheme F robust changes relative
weights assigned individual metric.
paper propose novel measure, Unanimity Improvement Ratio (UIR),
relies simple observation: system improves system B according
individual metrics (the improvement unanimous), better B weighting
scheme. Given test collection n test cases, test cases improvements
unanimous, robust perceived difference (average difference F
combination scheme) be.
words, well statistical significance tests provide information
robustness evaluation across test cases (Is perceived difference two systems
artifact set test cases used test collection? ), UIR meant provide
information robustness evaluation across variations relative metric
weightings (Is perceived difference two systems artifact relative metric
weighting chosen evaluation metric? ).
experiments clustering test collections show UIR contributes analysis
evaluation results two ways:
allows detect system improvements biased metric weighting
scheme. cases, experimenters carefully justify particular choice
relative weights check whether results swapped vicinity.
690

fiCombining Evaluation Metrics via Unanimous Improvement Ratio

Systems
Precision Recall
F
Task: Word alignment (Huang 2009)
Baseline
BM
72.76
54.82 62.53
Max. F
Link-Select
69.19
72.49 70.81
Alternative

72.66
66.17 69.26
Task: Opinion Question Answering (Li et al. 2009)
Baseline
System 3
10.9
21.6
17.2
Max. F
OpHit
10.2
25.6
20.5
Alternative
OpPageRank
10.9
24.2
20
Task: Sentiment Analysis (Kim et al 2009)
Baseline
BASELINE
30.5
86.6
45.1
Max. F
VS-LSA-DTP
34.9
71.9
47
Alternative
VS PMI
31.1
83.3
45.3
Task: Lexical Reference Rule Extraction (Shnarch et al 2009)
Baseline
expansion
54
19
28
Max. F
Wordnet+Wiki
47
35
40
Alternative rules + Dice filter
49
31
38
Table 1: three-way system comparisons taken ACL 2009 Conference Proceedings (Su et al., 2009)

increases substantially consistency evaluation results across datasets: result
supported high Unanimous Improvement Ratio much likely hold
different test collection. is, perhaps, relevant practical application
UIR: predictor much result replicable across test collections.
Although work presented paper applies research areas,
focus clustering task one relevant examples clustering
tasks specially sensitive metric relative weightings. research goals are:
1. investigate empirically whether clustering evaluation biased precision
recall relative weights F. use one recent test collections
focused text clustering problem (Artiles, Gonzalo, & Sekine, 2009).
2. introduce measure quantifies robustness evaluation results across
metric combining criteria, leads us propose UIR measure, derived
Conjoint Measurement Theory (Luce & Tukey, 1964).
3. analyze empirically UIR F-measure complement other.
4. illustrate application UIR comparing systems context shared
task, measure UIR serves predictor consistency evaluation
results across different test collections.

691

fiAmigo, Gonzalo, Artiles & Verdejo

Figure 1: Evaluation results semantic labeling CoNLL 2004

2. Combining Functions Evaluation Metrics
section briefly review different metrics combination criteria. present
rationale behind metric weighting approach well effects systems ranking.
2.1 F-measure
frequent way combining two evaluation metrics F-measure (van Rijsbergen, 1974). originally proposed evaluation Information Retrieval systems
(IR), use expanded many tasks. Given two metrics P R (e.g.
precision recall, Purity Inverse Purity, etc.), van Rijsbergens F-measure combines
single measure efficiency follows:
F (R, P ) =

( P1 )

1
+ (1 )( R1 )

F assumes value set particular evaluation scenario. parameter
represents relative weight metrics. cases value crucial;
particular, metrics correlated. instance, Figure 1 shows precision
recall levels obtained CoNLL-2004 shared task evaluating Semantic Role Labeling
systems (Carreras & Marquez, 2004). Except one system, every substantial improvement
precision involves increase recall. case, relative metric weighting
substantially modify system ranking.
cases metrics completely correlated, Decreasing Marginal Effectiveness property (van Rijsbergen, 1974) ensures certain robustness across values. F
satisfies property, states large decrease one metric cannot compensated large increase metric. Therefore, systems low precision
recall obtain low F-values value. discussed detail Section 5.1. However, show Section 3.4, cases Decreasing Marginal
692

fiCombining Evaluation Metrics via Unanimous Improvement Ratio

Effectiveness property prevent F-measure overly sensitive small
changes value.

Figure 2: example two systems evaluated break-even point

2.2 Precision Recall Break-even Point
Another way combining metrics consists evaluating system point one
metric equals (Tao Li & Zhu, 2006). method applicable system
represented trade-off metrics, instance, precision/recall curve.
method relies idea increasing metrics implies necessarily overall
quality increase. instance, assumes obtaining 0.4 precision recall
point 0.4 better obtaining 0.3 precision recall point 0.3.
Actually, break-even point assumes relevance metrics. considers
precision/recall point system distributes efforts equitably
metrics. Indeed, could change relative relevance metrics computing
break-even point.
Figure 2 illustrates idea. continuous curve represents trade-off
precision recall system S1. straight diagonal represents points
metrics return score. quality system corresponds therefore
intersection diagonal precision/recall curve. hand,
discontinuous curve represents another system S2 achieves increase precision
low recall levels cost decreasing precision high recall levels. According
break-even points, second system superior first one.
However, could give relevance recall identifying point recall doubles
precision. case, would obtain intersection points Q01 Q02 shown
figure, reverses quality order systems. conclusion, break-even
point assumes arbitrary relative relevance combined metrics.
2.3 Area Precision/Recall Curve
approaches average scores every potential parameterization metric combining function. instance, Mean Average Precision (MAP) oriented IR systems,
693

fiAmigo, Gonzalo, Artiles & Verdejo

computes average precision across number recall levels. Another example
Receiver Operating Characteristic (ROC) function used evaluate binary classifiers
(Cormack & Lynam, 2005). ROC computes probability positive sample receives
confidence score higher negative sample, independently threshold used
classify samples. functions related area AUC exists
precision/recall curve (Cormack & Lynam, 2005).
MAP ROC low high recall regions relative relevance
computing area. Again, could change measures order assign different
weights high low recall levels. Indeed (Weng & Poon, 2008) weighted Area
Curve proposed. Something similar would happen average F across different
values.
Note measures applied certain kinds problem, binary
classification document retrieval, system output seen ranking,
different cutoff points ranking give different precision/recall values.
directly applicable, particular, clustering problem focus work
here.

3. Combining Metrics Clustering Tasks
section present metric combination experiments specific clustering task.
results corroborate importance quantifying robustness systems across different
weighting schemes.
3.1 Clustering Task
Clustering (grouping similar items) applications wide range Artificial Intelligence
problems. particular, context textual information access, clustering algorithms
employed Information Retrieval (clustering text documents according content
similarity), document summarization (grouping pieces text order detect redundant
information), topic tracking, opinion mining (e.g. grouping opinions specific topic),
etc.
scenarios, clustering distributions produced systems usually evaluated
according similarity manually produced gold standard (extrinsic evaluation).
wide set metrics measure similarity (Amigo, Gonzalo, Artiles, &
Verdejo, 2008), rely two quality dimensions: (i) extent items
cluster belong group gold standard; (ii)
extent items different clusters belong different groups gold standard.
wide set extrinsic metrics proposed: Entropy Class Entropy (Steinbach,
Karypis, & Kumar, 2000; Ghosh, 2003), Purity Inverse Purity (Zhao & Karypis, 2001),
precision recall Bcubed metrics (Bagga & Baldwin, 1998), metrics based counting
pairs (Halkidi, Batistakis, & Vazirgiannis, 2001; Meila, 2003), etc.1

1. See work Amigo et al. (2008) detailed overview.

694

fiCombining Evaluation Metrics via Unanimous Improvement Ratio

3.2 Dataset
WePS (Web People Search) campaigns focused task disambiguating person
names Web search results. input systems ranked list web pages retrieved
Web search engine using person name query (e.g. John Smith).
challenge correctly estimate number different people sharing name
search results group documents referring individual. every person
name, WePS datasets provide around 100 web pages top search results, using
quoted person name query. order provide different ambiguity scenarios, person
names sampled US Census, Wikipedia, listings Program Committee
members Computer Science Conferences.
Systems evaluated comparing output gold standard: manual grouping
documents produced two human judges two rounds (first annotated corpus
independently discussed disagreements together). Note single
document assigned one cluster: Amazon search results list,
instance, may refer books written different authors name. WePS
task is, therefore, overlapping clustering problem, general case clustering
items restricted belong one single cluster. WePS datasets
official evaluation metrics reflect fact.
experiments focused evaluation results obtained WePS-1
(Artiles, Gonzalo, & Sekine, 2007) WePS-2 (Artiles et al., 2009) evaluation campaigns.
WePS-1 corpus includes data Web03 test bed (Mann, 2006),
used trial purposes follows similar annotation guidelines, although number
document per ambiguous name variable. refer corpora WePS-1a
(trial), WePS-1b WePS-2 2 .
3.3 Thresholds Stopping Criteria
clustering task involves three main aspects determine systems output quality.
first one method used measuring similarity documents; second
clustering algorithm (k-neighbors, Hierarchical Agglomerative Clustering, etc.);
third aspect considered usually consists couple related variables fixed:
similarity threshold two pages considered related stopping
criterion determines clustering process stops and, consequently, number
clusters produced system.
Figure 3 shows Purity Inverse Purity values change different clustering
stopping points, one systems evaluated WePS-1b corpus 3 . Purity focuses
frequency common category cluster (Amigo et al., 2008).
C set clusters evaluated, L set categories (reference distribution)
2. WePS datasets selected experiments (i) address relevant well-defined
clustering task; (ii) use widespread: WePS datasets used hundreds experiments
since first WePS evaluation 2007; (iii) runs submitted participants WePS-1 WePS-2
available us, essential experiment different evaluation measures. WePS datasets
freely available http://nlp.uned.es/weps.
3. system based bag words, TF/IDF word weighting, stopword removal, cosine distance
Hierarchical Agglomerative Clustering algorithm.

695

fiAmigo, Gonzalo, Artiles & Verdejo

N number clustered items, Purity computed taking weighted average
maximal precision values:
Purity =

X |Ci |


N

maxj Precision(Ci , Lj )

precision cluster Ci given category Lj defined as:
|Ci Lj |
Precision(Ci , Lj ) =
|Ci |


Purity penalizes noise cluster, reward grouping items
category together; simply make one cluster per item, reach trivially
maximum purity value. Inverse Purity focuses cluster maximum recall
category. Inverse Purity defined as:
Inverse Purity =

X |Li |


N

maxj Precision(Li , Cj )

Inverse Purity rewards grouping items together, penalize mixing items
different categories; reach maximum value Inverse purity making single
cluster items.
change stopping point implies increase Purity cost decrease
Inverse Purity, viceversa. Therefore, possible value F rewards different stopping
points. phenomenon produces high dependency clustering evaluation results
metric combining function.

Figure 3: example trade-off Purity Inverse Purity optimizing
grouping threshold

696

fiCombining Evaluation Metrics via Unanimous Improvement Ratio

3.4 Robustness Across Values
Determining appropriate value given scenario trivial. instance,
users point view WePS task, easier discard irrelevant documents
good cluster (because precision perfect high recall)
check additional relevant documents clusters (because precision high
recall not). Therefore, seems Inverse Purity priority Purity,
i.e., value 0.5. point view company providing
web people search service, however, situation quite different: priority
high precision, mixing profiles of, say, criminal doctor may result
company sued. perspective, receive high value.
WePS campaign decided agnostic set neutral = 0.5 value.
Table 2 shows resulting system ranking WePS-1b according F set 0.5
0.2. ranking includes two baseline systems: B1 consists grouping document
separate cluster, B100 consists grouping documents one single cluster.
B1 maximizes Purity, B100 maximizes Inverse Purity.
B1 B100 may obtain high low F-measure depending value.
table shows, = 0.5 B1 outperforms B100 considerable number systems.
reason result that, WePS-1b test set, many singleton clusters
(people referred one web page). means default strategy
making one cluster per document achieve maximal Purity,
acceptable Inverse Purity (0.45). However, fixed 0.2, B1 goes bottom
ranking outperformed systems, including baseline B100 .
Note outperforming trivial baseline system B1 crucial optimize
systems, given optimization cycle could otherwise lead baseline approach
B1 . drawback B1 informative (the output depend
input) and, crucially, sensitive variations . words, performance
robust changes metric combination criterion. Remarkably, top scoring
system, S1 , best values. primary motivation article
quantify robustness across values order complement information given
traditional system ranking.
3.5 Robustness Across Test Beds
average size clusters gold standard may change one test bed
another. affects Purity Inverse Purity trade-off, clustering system
may obtain different balance metrics different corpora; may
produce contradictory evaluation results comparing systems across different corpora,
even value.
instance, WePS-1b test bed (Artiles et al., 2007), B1 substantially outperforms
B100 (0.58 vs. 0.49 using F=0.5 ). WePS-2 data set (Artiles et al., 2009), however,
B100 outperforms B1 (0.53 versus 0.34). reason singletons less common
WePS-2. words, comparison B100 B1 depends value
particular distribution reference cluster sizes test bed.
point system improvements robust across values (which
case B1 B100 ) affected phenomenon. Therefore, estimating
697

fiAmigo, Gonzalo, Artiles & Verdejo

F=0.5
Ranked systems F result
S1
0.78
S2
0.75
S3
0.75
S4
0.67
S5
0.66
S6
0.65
S7
0.62
B1
0.61
S8
0.61
S9
0.58
S10
0.58
S11
0.57
S12
0.53
S13
0.49
S14
0.49
S15
0.48
B100
0.4
S16
0.4

F0.2
Ranked systems
S1
S3
S2
S6
S5
S8
S11
S7
S14
S15
S12
S9
S13
S4
S10
B100
S16
B1

F result
0.83
0.78
0.77
0.76
0.73
0.73
0.71
0.67
0.66
0.66
0.65
0.64
0.63
0.62
0.6
0.58
0.56
0.49

Table 2: WePS-1b system ranking according F=0.5 vs F=0.2 using Purity Inverse
Purity

robustness system improvements changes prevent reaching contradictory
results different test beds. Indeed, evidence presented Section 7.

4. Proposal
primary motivation article quantify robustness across values
order complement information given traditional system rankings. end
introduce section Unanimous Improvement Ratio.
4.1 Unanimous Improvements
problem combining evaluation metrics closely related theory conjoint
measurement (see Section 5.1 detailed discussion). Van Rijsbergen (1974) argued
possible determine empirically metric combining function
adequate context Information Retrieval evaluation. However, starting
measurement theory principles, van Rijsbergen described set properties metric
combining function satisfy. set includes Independence axiom (also called
Single Cancellation), Monotonicity property derives. Monotonicity
property states quality system surpasses equals another one according
metrics necessarily equal better other. words, one system
698

fiCombining Evaluation Metrics via Unanimous Improvement Ratio

better dependence whatsoever relative importance
metric set.
define combination procedure metrics, Unanimous Improvement,
based property:
QX (a) QX (b) x X.Qx (a) Qx (b)
QX (a) quality according set metrics X.
relationship dependence metrics scaled weighted,
degree correlation metric set. Equality (= ) derived directly :
unanimous equality implies systems obtain score metrics:
QX (a) = QX (b) (QX (a) QX (b)) (QX (b) QX (a))

strict unanimous improvement implies one system improves strictly
least one metric, improved according metric:
QX (a) > QX (b) (QX (a) QX (b)) (QX (a) = QX (b))
(QX (a) QX (b)) (QX (b) QX (a))

Non comparability k derived here: occurs metrics favor one
system metrics favor other. refer cases metric-biased
improvements.
QX (a)k QX (b) (QX (a) QX (b)) (QX (b) QX (a))

theoretical properties Unanimous Improvement described depth
Section 5.2. important property Unanimous Improvement
relational structure depend relative metric weightings, satisfying
Independence (Monotonicity) axiom. words, claim that: system improvement according metric combining function depend whatsoever metric
weightings quality decrease according individual metric.
theoretical justification assertion developed Section 5.2.1.
4.2 Unanimous Improvement Ratio
According Unanimous Improvement, unique observable test case
three-valued function (unanimous improvement, equality biased improvement). need,
however, way quantitatively comparing systems.
Given two systems, b, Unanimous Improvement relationship set
test cases , samples improves b (QX (a) QX (b)), samples b improves (QX (b) QX (a)) samples biased improvements (QX (a)k QX (b)).
refer sets Ta b , Tb Tak b , respectively. Unanimous Improvement Ratio (UIR) defined according three formal restrictions:
699

fiAmigo, Gonzalo, Artiles & Verdejo

Test cases
1
2
3
4
5
6
7
8
9
10

Precision
System System B
0.5
0.5
0.5
0.5
0.5
0.4
0.6
0.6
0.7
0.6
0.3
0.1
0.4
0.5
0.4
0.6
0.3
0.1
0.2
0.4

Recall
System System B
0.5
0.5
0.2
0.2
0.2
0.2
0.4
0.3
0.5
0.4
0.5
0.4
0.5
0.6
0.5
0.6
0.5
0.6
0.5
0.3

B
YES
YES
YES
YES
YES
YES





B
YES
YES




YES
YES



Table 3: Example experiment input compute UIR
1. UIR(a, b) decrease number biased improvements (Tak b ).
boundary condition samples biased improvements (Tak b = ),
UIR(a, b) 0.
2. improves b much b improves (Ta b = Tb ) UIR(a, b) = 0.
3. Given fixed number biased improvements (Tak b ), UIR(a, b) proportional
Ta b inversely proportional Tb .
Given restrictions, propose following UIR definition:
UIRX,T (a, b) =

|Ta b | |Tb |
=
|T |

|t /QX (a) QX (b)| |t /QX (b) QX (a)|
|T |
alternatively formulated as:
UIRX,T (a, b) = P(a b) P(b a)
probabilities estimated frequentist manner.
UIR range [1, 1] symmetric: UIRX,T (a, b) = UIRX,T (b, a).
illustration UIR computed, consider experiment outcome Table 3. Systems
B compared terms precision recall 10 test cases. test case 5,
instance, unanimous improvement B: better terms precision
(0.7 > 0.6) recall (0.5 > 0.4). table, UIR value is:
UIRX,T (A, B) =

|TA B | |TB |
64
=
= 0.2 = UIRX,T (B, A)
|T |
10

UIR two formal limitations. First, transitive (see Section 5.2). Therefore,
possible define linear system ranking based UIR. is, however,
700

fiCombining Evaluation Metrics via Unanimous Improvement Ratio

necessary: UIR meant provide ranking, complement ranking provided
F-measure (or metric combining function), indicating robust results
changes . Section 6.4 illustrates UIR integrated insights provided
system ranking.
second limitation UIR consider improvement ranges; therefore,
less sensitive F-measure. empirical results, however, show UIR
sensitive enough discriminate robust improvements versus metric-biased improvements;
Section 8 make empirical comparison non-parametric definition UIR
parametric version, results make non-parametric definition preferable.

5. Theoretical Foundations
section discuss theoretical foundations Unanimous Improvement Ratio
framework Conjoint Measurement Theory. proceed describe
formal properties UIR implications point view evaluation
methodology. Readers interested solely practical implications using UIR may
proceed directly Section 6.
5.1 Conjoint Measurement Theory
problem combining evaluation metrics closely related Conjoint Measurement Theory, independently discovered economist Debreu (1959)
mathematical psychologist R. Duncan Luce statistician John Tukey (Luce & Tukey,
1964). Theory Measurement defines necessary conditions state homomorphism empirical relational structure (e.g. John bigger Bill)
numeric relational structure (Johns height 1.79 meters Bills height 1.56 meters).
case Conjoint Measurement Theory, relational structure factored
two (or more) ordered substructures (e.g. height weight).
context, numerical structures given evaluation metric scores (e.g.
Purity Inverse Purity). However, empirical quality ordering
clustering systems. Different human assessors could assign relevance Purity
Inverse Purity viceversa. Nevertheless, Conjoint Measurement Theory provide
mechanisms state kind numerical structures produce homomorphism
assuming empirical structure satisfies certain axioms. Van Rijsbergen (1974) used
idea analyze problem combining evaluation metrics. axioms shape
additive conjoint structure. (R, P ) quality system according two evaluation
metrics R P , axioms are:
Connectedness: systems comparable other. Formally: (R, P )
(R0 , P 0 ) (R0 , P 0 ) (R, P ).
Transitivity: (R, P ) (R0 , P 0 ) (R0 , P 0 ) (R00 , P 00 ) implies (R, P ) (R00 , P 00 ).
axioms Transitivity Connectedness shape weak order.
Thomsen condition: (R1 , P3 ) (R3 , P2 ) (R3 , P1 ) (R2 , P3 ) imply (R1 , P1 )
(R2 , P2 ) (where indicates equal effectiveness).
701

fiAmigo, Gonzalo, Artiles & Verdejo

Independence: two components contribute effects independently effectiveness. Formally, (R1 , P ) (R2 , P ) implies (R1 , P 0 ) (R2 , P 0 ) P 0 ,
(R, P1 ) (R, P2 ) implies (R0 , P2 ) (R0 , P2 ) R0 . property implies
Monotonicity (Narens & Luce, 1986) states improvement metrics necessarily produces improvement according metric combining function.
Restricted Solvability: property ... concerned continuity
component. makes precise intuitively would expect considering
existence intermediate levels. Formally: whenever (R1 , P 0 ) (R, P ) (R2 , P 0 )
exists R (R0 , P 0 ) = (R, P ).
Essential Components: Variation one leaving constant gives variation effectiveness. exists R, R0 P case
(R, P ) = (R0 , P ); exists P , P 0 R case
(R, P ) = (R, P 0 ).
Archimedean Property: merely ensures intervals component
comparable.
F-measure proposed van Rijsbergen (1974) arithmetic mean P,R
satisfy axioms. According restrictions, indeed, unlimited set acceptable combining functions evaluation metrics defined. F relational structure,
however, satisfies another property satisfied functions
arithmetic mean. property Decreasing Marginal Effectiveness. basic idea
increasing one unit one metric decreasing one unit metric
improve overall quality (i.e. first metric weight combining function), imply great loss one metric compensated great
increase other. defined as:
R, P > 0, n > 0 ((P + n, R n) < (R, P ))
According this, high values metrics required obtain high overall
improvement. makes measures observing property - F - robust
arbitrary metric weightings.
5.2 Formal Properties Unanimous Improvement
Unanimous Improvement x trivially satisfies desirable properties proposed van Rijsbergen (1974) metric combining functions: transitivity, independence,
Thomsen condition, Restricted Solvability, Essential Components Decreasing Marginal
Effectiveness; exception connectedness property4 . Given non comparability k (biased improvements, see Section 4.1) derived Unanimous Improvement, possible find system pairs neither QX (a) QX (b) QX (b) QX (a)
hold. Therefore, Connectedness satisfied.
Formally, limitation Unanimous Improvement represent
weak order, cannot satisfy Transitivity Connectedness simultaneously. Let
us elaborate issue.
4. sake simplicity, consider combination two metrics (R, P ).

702

fiCombining Evaluation Metrics via Unanimous Improvement Ratio

Systems

B
C

Metric x1
0.5
0.6
0.45

Metric x2
0.5
0.4
0.45

Table 4: counter sample Transitivity Unanimous Improvement
could satisfy Connectedness considering biased improvements represent
equivalent system pairs (= ). case, Transitivity would satisfied. See,
instance, Table 4. According table:
QX (B)k QX (A) QX (C)k QX (B)
Therefore, considering k represents equivalence, have:
QX (B) QX (A) QX (C) QX (B)

QX (C) QX (A)
summary, choose satisfy transitivity connectedness, both:
Unanimous Improvement derive weak order.
5.2.1 Uniqueness Unanimous Improvement
Unanimous Improvement interesting property contradict
evaluation result given F-measure, regardless value used F:
QX (a) QX (b) F (a) F (b)
due fact F-measure (for value) satisfies monotonicity
axiom, Unanimous Improvement grounded. property essential
purpose checking robustness system improvements across values.
crucially, Unanimous Improvement function satisfies property.
precisely, Unanimous Improvement relational structure that, satisfying
monotonicity, contradict Additive Conjoint Structure (see Section 5.1).
order prove assertion, need define concept compatibility
additive conjoint structure. Let add additive conjoint structure let R
relational structure. say R compatible conjoint structure
if:
ha, b, add i.(QX (a) R QX (b)) (QX (a) add QX (b))
words: R holds, additive conjoint holds. want prove
unanimous improvement relation satisfies property; therefore,
prove R monotonic compatible relational structure,
necessarily matches unanimous improvement definition:
703

fiAmigo, Gonzalo, Artiles & Verdejo

R monotonic compatible = (QX (a) R QX (b) xi (a) xi (b)xi X)
split in:
(1) R monotonic compatible = (QX (a) R QX (b) xi (a) xi (b)xi X)
(2) R monotonic compatible = (QX (a) R QX (b) xi (a) xi (b)xi X)
Proving (1) immediate, since rightmost component corresponds monotonicity property definition. Let us prove (2) reductio ad absurdum, assuming
exists relational structure that:
(o monotonic compatible) (QX (a) QX (b)) (xi X.xi (a) < xi (b))
case, could define additive conjoint structure combined measure
Q0X (a) = 1 x1 (a)+..i xi (a)..+n xn (a) big enough Q0X (a) < Q0X (b).
Q0 additive conjoint structure would contradict . Therefore, would compatible
(contradiction). conclusion, predicate (2) true Unanimous Improvement X
monotonic compatible relational structure.
interesting corollary derived analysis. Unanimous Improvement compatible relational structure, formally conclude
measurement system improvements without dependence metric weighting schemes
derive weak order (i.e. one satisfies transitivity connectedness).
corollary practical implications: possible establish system ranking
independent metric weighting schemes.
natural way proceed is, therefore, use unanimous improvement addition
standard F-measure (for suitable value) provides additional information
robustness system improvements across values.

6. F versus UIR: Empirical Study
Section perform number empirical studies WePS corpora order
find UIR behaves practice. First, focus number empirical results
show UIR rewards robustness across values, information complementary information provided F. Second, examine extent F
UIR correlated.
6.1 UIR: Rewarding Robustness
Figure 4 shows three examples system comparisons WePS-1b corpus using metrics
Purity Inverse Purity. curve represents F value obtained one system
according different values. System S6 (black curves) compared S10, S9
S11 (grey curves) three graphs. cases similar quality increase
according F=0.5 ; UIR, however, ranges 0.32 0.42, depending robust
difference changes . highest difference UIR (S6,S11) system
pair (rightmost graph), systems swap F values value.
704

fiCombining Evaluation Metrics via Unanimous Improvement Ratio

| 4 F=0.5 |
|UIR|

Improvements

(28 system pairs)
0.12
0.53

cases
(125 system pairs)
0.13
0.14

Table 5: UIR F=0.5 increase F increases values

Figure 4: F-measure vs. UIR: rewarding robustness
smallest UIR value (S6,S10), S6 better S10 values
0.8, worse larger. comparison illustrates UIR captures, similar
increments F, ones less dependent relative weighting scheme
precision recall.
Let us consider two-system combinations WePS-1b corpus, dividing
two sets: (i) system pairs F increases values (i.e. Purity
Inverse Purity increases), (ii) pairs relative systems performance swaps
value; i.e. F increases values decreases rest.
One would expect average increase F larger system pairs
one beats every value. Surprisingly, true: Table 5 shows
average increments UIR F=0.5 sets. UIR behaves expected: average
value substantially larger set different lead contradictory results
(0.53 vs. 0.14). average relative increase F=0.5 , however, similar
sets (0.12 vs. 0.13).
conclusion certain F=0.5 improvement range say anything
whether Purity Inverse Purity simultaneously improved not.
words: matter large measured improvement F is, still extremely
dependent weighting individual metrics measurement.
conclusion corroborated considering independently metrics (Purity Inverse Purity). According statistical significance improvements
independent metrics, distinguish three cases:
1. Opposite significant improvements: One metrics (Purity Inverse Purity)
increases decreases, changes statistically significant.
705

fiAmigo, Gonzalo, Artiles & Verdejo

| 4 F=0.5 |
|UIR|

Significant
concordant
improvements
53 pairs
0.11
0.42

Significant
opposite
improvements
89 pairs
0.15
0.08

Non
significant
improvements
11 pairs
0.05
0.027

Table 6: UIR F=0.5 increases vs. statistical significance tests
2. Concordant significant improvements: metrics improve significantly least
one improves significantly decrease significantly.
3. Non-significant improvements: statistically significant differences
systems metric.
use Wilcoxon test p < 0.05 detect statistical significance. Table 6
shows average UIR | 4 F=0.5 | values three cases. Remarkably,
F=0.5 average increase even larger opposite improvements set (0.15)
concordant improvements set (0.11). According results, would seem
F=0.5 rewards individual metric improvements obtained cost (smaller)
decreases metric. UIR, hand, sharply different behavior,
strongly rewarding concordant improvements set (0.42 versus 0.08).
results confirm UIR provides essential information experimental
outcome two-system comparisons, provided main evaluation metric
F .
6.2 Correlation F UIR
fact UIR F offer different information outcome experiment
imply UIR F orthogonal; fact, correlation
values.
Figure 5 represents F=0.5 differences UIR values possible system pair
WePS-1 test bed. general trends (i) high UIR values imply positive difference
F (ii) high |4F0,.5 | values imply anything UIR values; (iii) low UIR seem
imply anything |4F0,.5 | values. Overall, figure suggest triangle relationship,
gives Pearson correlation 0.58.
6.2.1 Reflecting improvement ranges
consistent difference two systems values, UIR rewards
larger improvement ranges. Let us illustrate behavior considering three sample system
pairs taken WePS-1 test bed.
Figure 6 represents F[0,1] values three system pairs. cases, one system
improves values. However, UIR assigns higher values larger improvements F (larger distance black grey curves). reason
706

fiCombining Evaluation Metrics via Unanimous Improvement Ratio

Figure 5: |4F0,.5 | vs UIR

Figure 6: F vs. UIR: reflecting improvement ranges
larger average improvement test cases makes less likely cases individual test
cases (which ones UIR considers) contradict average result.
Another interesting finding that, metrics improved, metric
weakest improvement determines behavior UIR. Figure 7 illustrates
relationship ten system pairs largest improvement; Pearson correlation
graph 0.94. words, individual metrics improve, UIR sensitive
weakest improvement.
6.2.2 Analysis boundary cases
order better understanding relationship UIR F,
examine detail two cases system improvements UIR F produce drastically
different results. two cases marked B Figure 5.
point marked case Figure corresponds comparison systems
S1 S15 . exists substantial (and statistically significant) difference
systems according F=0.5 . However, UIR low value, i.e., improvement
robust changes according UIR.
707

fiAmigo, Gonzalo, Artiles & Verdejo

Figure 7: Correlation UIR weakest single metric improvement.

Figure 8: Purity Inverse Purity per test case, systems S1 S1 5
visual explanation results seen Figure 8. shows Purity
Inverse Purity results systems S1 , S15 every test case. test cases, S1
important advantage Purity cost slight consistent loss Inverse
Purity. Given F=0.5 compares Purity Inverse Purity ranges, states
exists important statistically significant improvement S15 S1 . However,
slight consistent decrease Inverse Purity affects UIR, decreases
test cases improvements F metric biased (k notation).
Case B (see Figure 9) opposite example: small difference systems
S8 S12 according F=0.5 , differences Purity Inverse Purity
small. S8, however, gives small consistent improvements Purity Inverse
Purity (all test cases right vertical line figure); unanimous
improvements. Therefore, UIR considers exists robust overall improvement
case.
708

fiCombining Evaluation Metrics via Unanimous Improvement Ratio

Figure 9: Purity Inverse Purity per test case, systems S1 2 S8
Again, cases show UIR gives additional valuable information comparative behavior systems.
6.3 Significance Threshold UIR
mentioned earlier UIR parallelism statistical significance tests,
typically used Information Retrieval estimate probability p observed
difference two systems obtained chance, i.e., difference artifact
test collection rather true difference systems. computing
statistical significance, useful establish threshold allows binary decision;
instance, result often said statistically significant p < 0.05, significant
otherwise. Choosing level significance arbitrary, nevertheless helps reporting
summarizing significance tests. Stricter thresholds increase confidence test,
run increased risk failing detect significant result.
situation applies UIR: would establish UIR threshold
decides whether observed difference reasonably robust changes . set
threshold? could restrictive decide, instance, improvement
significantly robust UIR 0.75. condition, however, hard would
never satisfied practice, therefore UIR test would informative.
hand, set permissive threshold satisfied system
pairs and, again, informative. question whether exists
threshold UIR values obtaining UIR threshold guarantees
improvement robust, and, time, strong satisfied practice.
Given set two-system combinations UIR surpasses certain candidate
threshold, think desirable features:
1. must able differentiate two types improvements (robust vs. nonrobust); words, one two types usually empty almost empty,
threshold informative.
709

fiAmigo, Gonzalo, Artiles & Verdejo

2. robust set contain high ratio two-system combinations
average F increases values (F (a) > F (b)).
3. robust set contain high ratio significant concordant improvements
low ratio significant opposite improvements (see Section 6.1).
4. robust set contain low ratio cases F contradicts UIR (the dots
Figure 5 region |4F0,.5 | < 0).

Figure 10: Improvement detected across UIR thresholds
Figure 10 shows conditions met every threshold range [0, 0.8].
UIR threshold 0.25 accepts around 30% system pairs, low (4%) ratio
significant opposite improvements high (80%) ratio significant concordant improvements. threshold, half robust cases F increases values,
cases (94%) F=0.5 increases. seems, therefore, UIR 0.25 reasonable
threshold, least clustering task. Note, however, rough rule thumb
revised/adjusted dealing clustering tasks WePS.
6.4 UIR System Rankings
results presented far focused pairwise system comparisons, according
nature UIR. turn question use UIR component
analysis results evaluation campaign.
order answer question applied UIR results WePS-2
evaluation campaign (Artiles et al., 2009). campaign, best runs system
ranked according Bcubed precision recall metrics, combined F=0.5 .
addition participant systems, three baseline approaches included ranking:
710

fiCombining Evaluation Metrics via Unanimous Improvement Ratio

documents one cluster (B100 ), document one cluster (B1 ) union
(BCOMB )5 .
Table 7 shows results applying UIR WePS-2 participant systems. -robust
improvements represented third column (improved systems): every system,
displays set systems improves UIR 0.25. fourth column
reference system, defined follows: given system a, reference system
one improves maximal UIR:
Sref (a) = ArgmaxS (UIR(S, a))
words, Sref (a) represents system replaced order
robustly improve results across different values. Finally, last column (UIR
reference system) displays UIR system reference (UIR(Sref , Si )).
Note UIR adds new insights evaluation process. Let us highlight two
interesting facts:
Although three top-scoring systems (S1, S2, S3) similar performance
terms F (0.82, 0.81 0.81), S1 consistently best system according UIR,
reference 10 systems (S2, S4, S6, S8, S12, S13, S14, S15,
S16 baseline B1 ). contrast, S2 reference S7 only, S3 reference
S11 only. Therefore, F UIR together strongly point towards S1 best
system, F alone able discern set three top-scoring systems.
Although non-informative baseline B100 (all documents one cluster) better
five systems according F, improvement robust according UIR.
Note UIR signal near-baseline behaviors participant systems low
value, receive large F depending nature test collection:
average cluster large small, systems tend cluster everything
nothing artificially rewarded. is, opinion, substantial improvement
using F alone.

7. UIR Predictor Stability Results across Test Collections
common issue evaluating systems deal Natural Language results
different test collections often contradictory. particular case Text Clustering,
factor contributes problem average size clusters vary across
different test beds, variability modifies optimal balance precision
recall. system tends favor precision, creating small clusters, may good
results dataset small average cluster size worse results test collection
larger average cluster size.
Therefore, apply F combine single metrics, reach contradictory
results different test beds. UIR depend metric weighting criteria,
hypothesis high UIR value ensures robustness evaluation results across test beds.
5. See work Artiles et al. (2009) extended explanation.

711

fiAmigo, Gonzalo, Artiles & Verdejo

System

F0.5

S1
S2
S3
S4
S5
S6
S7
S8
S9
S10
S11
S12
B100
S13

0,82
0,81
0,81
0,72
0,71
0,71
0,70
0,70
0,63
0,63
0,57
0,53
0,53
0,52
0,52
0,42
0,41
0,39
0,34
0,33

BCOMB
S14
S15
S16
B1
S17

Improved systems
(UIR > 0.25)
S2 S4 S6 S7 S8 S11..S17 B1
S4 S6 S7 S8 S11..S17 B1
S2 S4 S7 S8 S11..S17 B1
S11 S13..S17
S12..S16
S4 S7 S11 S13..S17 B1
S11 S13..S17
S11..S17
S4 S12 S14 S16
S12..S16
S14..S17
S14 S16
BCOMB
S15 S16
S16
S17
-

Reference
system
S1
S1
S1
S2
S1
S3
S1
S1
B100
S1
S1
S1
S1
S6

UIR
reference system
0,26
0,58
0,35
0,65
0,74
0,68
0,71
0,9
0,65
0,9
0,97
1,00
0,29
0,84

Table 7: WePS-2 results Bcubed precision recall, F UIR measures
words: given particular test bed, high UIR value good predictor
observed difference two systems still hold test beds.
following experiment designed verify hypothesis. implemented
four different systems WePS problem, based agglomerative clustering algorithm (HAC) used best systems WePS-2. system employs
certain cluster linkage technique (complete link single link) certain feature extraction criterion (word bigrams unigrams). system experimented 20
stopping criteria. Therefore, used 20x4 system variants overall. evaluated
systems WePS-1a, WePS-1b WePS-2 corpora6 .
first observation that, given system pairs, F=0.5 gives consistent results
three test beds 18% cases. system pairs, best system
different depending test collection. robust evaluation criterion predict,
given single test collection, whether results still hold collections.
consider two alternative ways predicting observed difference (system
better system B) one test-bed still hold three test beds:
first using F (A) F (B): larger value reference test bed,
likely F (A) F (B) still positive different test collection.
6. WEPS-1a originally used training first WePS campaign, WePS-1b used testing.

712

fiCombining Evaluation Metrics via Unanimous Improvement Ratio

second using U IR(A, B) instead F: larger UIR is, likely
F (A) F (B) positive different test bed.
summary, want compare F UIR predictors robust result
change test collection. tested it:
1. select reference corpus WePS-1a, WePS-1b WePS-2 test beds.
Cref {WePS-1a,WePS-1b,WePS-2}
2. system pair reference corpus, compute improvement one
system respect according F UIR. take system pairs
one improves certain threshold t. UIRC (s1 , s2 )
UIR results systems s1 s2 test-bed C, FC (s) results F
system test-bed C:
SU IR,t (C) = {(s1 , s2 )|UIRC (s1 , s2 ) > t}
SF,t (C) = {s1 , s2 |(FC (s1 ) FC (s2 )) > t)}
every threshold t, SU IR,t SF,t represent set robust improvements
predicted UIR F, respectively.
3. Then, consider system pairs one improves according F
three test collections simultaneously.
= {s1 , s2 |FC (s1 ) > FC (s2 )C}
gold standard compared predictions SU IR,t SF,t .
4. every threshold t, compute precision recall UIR F predictions
(SU IR,t (C) SF,t (C)) versus actual set robust results across collections
(T ).
P recision(SU IR,t (C)) =

|SU IR,t (C) |
|SU IR,t |

P recision(SF,t (C)) =

|SF,t (C) |
|SF,t (C)|

Recall(SU IR,t (C)) =

Recall(SF,t (C)) =

|SU IR,t (C) |
|T |

|SF,t (C) |
|T |

trace precision/recall curve predictors F, UIR
compare results. Figures 11, 12 13, show precision/recall values F (triangles)
UIR (rhombi); figure displays results one reference test-beds: WEPS1a,WEPS-1b WePS-27 .
Altogether, figures show UIR much effective F predictor.
Note F suffers sudden drop performance low recall levels, suggests
7. curve parametric UIR refers alternative definition UIR explained Section 8

713

fiAmigo, Gonzalo, Artiles & Verdejo

Figure 11: Predictive power UIR F WePS-1a

Figure 12: Predictive power UIR F WePS-1b

714

fiCombining Evaluation Metrics via Unanimous Improvement Ratio

Figure 13: Predictive power UIR F WePS-2
big improvements F tend due peculiarities test collection rather
real superiority one system versus other.
is, opinion, remarkable result: differences UIR better indicators
reliability measured difference F amount measured difference.
Therefore, UIR useful know stable results changes ,
changes test collection, i.e., indicator reliable perceived difference
is.
Note explicitly tested dependency (and reliability) UIR results
number test cases reference collection. However, working
collection less 30 test cases unlikely, practical terms usability UIR
granted test collections, least respect number test cases.

8. Parametric versus Non-Parametric UIR
According analysis (see Section 5.2), given two measures P R, relational
structure pairs hPi , Ri depend weighting criteria unanimous
improvement:
b Pa Pb Ra Rb
comparing systems, UIR measure counts unanimous improvement results
across test cases:
UIRX,T (a, b) =

|Ta b | |Tb |
|T |

Alternatively, formulation expressed terms probabilities:
715

fiAmigo, Gonzalo, Artiles & Verdejo

UIRX,T (a, b) = Prob(a b) Prob(b a)
probabilities estimated frequentist manner.
said, main drawback unanimous improvement threevalued function consider metric ranges; UIR inherits drawback.
consequence, UIR less sensitive combining schemes F measure.
order solve drawback, could estimate UIR parametrically. However, results
section seem indicate best option.
One way estimating P rob(a b) P rob(b a) consists assuming
metric differences (P, R) two systems across test cases follow normal bivariate
distribution. estimate distribution case samples provided
test bed. estimating density function P rob(P, R), estimate P rob(a
b) as8 :
P rob(a b) = P rob(P 0 R 0) =

Z P =1,R=1

P rob(P, R) dP dR
P =0,R=0

expression used compute UIRX,T (a, b) = Prob(a b) Prob(b a),
leads parametric version UIR.
order compare effectiveness parametric UIR versus original UIR,
repeated experiment described Section 7, adding UIRparam precision/recall
curves Figures 11, 12 13. squares figures represent results
parametric version UIR. Note behavior lies somewhere F nonparametric UIR: low levels recall, behaves original UIR; intermediate
levels, general worse original definition better F;
recall high-end, overlaps results F. probably due fact
parametric UIR estimation considers ranges, becomes sensitive unreliability
high improvements F.

9. Conclusions
work addressed practical problem strong dependency (and usually
degree arbitrariness) relative weights assigned metrics applying metric
combination criteria, F .
Based theory measurement, established relevant theoretical results: fundamental one monotonic relational structure
contradict Additive Conjoint Structure, unique relationship
transitive. implies possible establish ranking (a complete ordering) systems without assuming arbitrary relative metric weighting. transitive
relationship, however, necessary ensure robustness specific pairwise system
comparisons.
Based theoretical analysis, introduced Unanimous Improvement Ratio (UIR), estimates robustness measured system improvements across potential
metric combining schemes. UIR measure complementary metric combination
8. computation employed Matlab tool

716

fiCombining Evaluation Metrics via Unanimous Improvement Ratio

scheme works similarly statistical relevance test, indicating perceived difference two systems reliable biased particular weighting scheme used
evaluate overall performance systems.
empirical results text clustering task, particularly sensitive
problem, confirm UIR indeed useful analysis tool pairwise system comparisons: (i) similar increments F, UIR captures ones less dependent
relative weighting scheme precision recall; (ii) unlike F, UIR rewards system
improvements corroborated statistical significance tests single measure; (iii) practice, high UIR tends imply large F increase, large increase
F imply high UIR; words, large increase F completely
biased weighting scheme, therefore UIR essential information add F.
looking results evaluation campaign, UIR proved useful (i) discern
best system among set systems similar performance according F ;
(ii) penalize trivial baseline strategies systems baseline-like behavior.
Perhaps relevant result side effect proposed measure defined:
UIR good estimator robust result changes test collection.
words, given measured increase F test collection, high UIR value makes
likely increase observed test collections. Remarkably, UIR
estimates cross-collection robustness F increases much better absolute value
F increase.
limitation present study tested UIR text clustering
problem. usefulness clustering problems already makes UIR useful analysis
tool, potential goes well beyond particular problem. Natural Language problems and, general, many problems Artificial Intelligence evaluated terms
many individual measures trivial combine. UIR powerful tool
many scenarios.
UIR evaluation package available download http://nlp.uned.es.

Acknowledgments
research partially supported Spanish Government (grant Holopedia,
TIN2010-21128-C02) Regional Government Madrid Research Network
MA2VICMR (S2009/TIC-1542).

References
Amigo, E., Gonzalo, J., Artiles, J., & Verdejo, F. (2008). comparison extrinsic clustering
evaluation metrics based formal constraints. Information Retrieval, 12 (4), 461486.
Artiles, J., Gonzalo, J., & Sekine, S. (2009). WePS-2 Evaluation Campaign: Overview
Web People Search Clustering Task. Proceedings 2nd Web People Search
Evaluation Workshop (WePS 2009).
Artiles, J., Gonzalo, J., & Sekine, S. (2007). SemEval-2007 WePS evaluation: Establishing Benchmark Web People Search Task. Proceedings 4th
International Workshop Semantic Evaluations, SemEval 07, pp. 6469 Stroudsburg, PA, USA. Association Computational Linguistics.
717

fiAmigo, Gonzalo, Artiles & Verdejo

Bagga, A., & Baldwin, B. (1998). Entity-Based Cross-Document Coreferencing Using
Vector Space Model. Proceedings 36th Annual Meeting Association
Computational Linguistics 17th International Conference Computational
Linguistics (COLING-ACL98), pp. 7985.
Carreras, X., & Marquez, L. (2004). Introduction CoNLL-2004 Shared Task: Semantic
Role Labeling. Ng, H. T., & Riloff, E. (Eds.), HLT-NAACL 2004 Workshop: Eighth
Conference Computational Natural Language Learning (CoNLL-2004), pp. 8997
Boston, Massachusetts, USA. Association Computational Linguistics.
Cormack, G. V., & Lynam, T. R. (2005). TREC 2005 Spam Track Overview. Proceedings
fourteenth Text REtrieval Conference (TREC-2005).
Debreu, G. (1959). Topological methods cardinal utility theory. Mathematical Methods
Social Sciences, Stanford University Press, 1 (76), 1626.
Ghosh, J. (2003). Scalable clustering methods data mining. Ye, N. (Ed.), Handbook
Data Mining. Lawrence Erlbaum.
Halkidi, M., Batistakis, Y., & Vazirgiannis, M. (2001). Clustering Validation Techniques.
Journal Intelligent Information Systems, 17 (2-3), 107145.
Luce, R., & Tukey, J. (1964). Simultaneous conjoint measurement: new scale type
fundamental measurement. Journal Mathematical Psychology, 1 (1).
Mann, G. S. (2006). Multi-Document Statistical Fact Extraction Fusion. Ph.D. thesis,
Johns Hopkins University.
Meila, M. (2003). Comparing clusterings. Proceedings COLT 03.
Narens, L., & Luce, R. D. (1986). Measurement: theory numerical assignments. Psychological Bulletin, 99.
Steinbach, M., Karypis, G., & Kumar, V. (2000). comparison document clustering
techniques. KDD Workshop Text Mining,2000.
Su, K.-Y., Su, J., Wiebe, J., & Li, H. (Eds.). (2009). Proceedings Joint Conference
47th Annual Meeting ACL 4th International Joint Conference
Natural Language Processing AFNLP. Association Computational Linguistics, Suntec, Singapore.
Tao Li, C. Z., & Zhu, S. (2006). Empirical Studies Multilabel Classification. Proceedings 18th IEEE International Conference Tools Artificial Intelligence
(ICTAI 2006).
van Rijsbergen, C. J. (1974). Foundation evaluation. Journal Documentation, 30 (4),
365373.
Weng, C. G., & Poon, J. (2008). New Evaluation Measure Imbalanced Datasets.
Roddick, J. F., Li, J., Christen, P., & Kennedy, P. J. (Eds.), Seventh Australasian
Data Mining Conference (AusDM 2008), Vol. 87 CRPIT, pp. 2732 Glenelg, South
Australia. ACS.
Zhao, Y., & Karypis, G. (2001). Criterion functions document clustering: Experiments
analysis. Technical Report TR 0140, Department Computer Science, University Minnesota, Minneapolis, MN.
718


