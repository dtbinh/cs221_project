Journal Artificial Intelligence Research 42 (2011) 661-687

Submitted 05/11; published 12/11

Finding Consensus Bayesian Network Structures
Jose M. Pena

jose.m.pena@liu.se

ADIT
Department Computer Information Science
Linkoping University
SE-58183 Linkoping
Sweden

Abstract
Suppose multiple experts (or learning algorithms) provide us alternative
Bayesian network (BN) structures domain, interested combining
single consensus BN structure. Specifically, interested
consensus BN structure represents independences given BN structures agree
upon parameters associated possible. paper, prove
may exist several non-equivalent consensus BN structures finding one
NP-hard. Thus, decide resort heuristics find approximated
consensus BN structure. paper, consider heuristic proposed Matzkevich
Abramson, builds upon two algorithms, called Methods B, efficiently
deriving minimal directed independence map BN structure relative given node
ordering. Methods B claimed correct although proof provided (a
proof sketched). paper, show Methods B correct
propose correction them.

1. Introduction
Bayesian networks (BNs) popular graphical formalism representing probability distributions. BN consists structure parameters. structure, directed acyclic
graph (DAG), induces set independencies represented probability distribution
satisfies. parameters specify conditional probability distribution node given
parents structure. BN represents probability distribution results
product conditional probability distributions. Typically, single expert
(or learning algorithm) consulted construct BN domain hand. Therefore,
risk so-constructed BN accurate could if, instance,
expert bias overlooks certain details. One way minimize risk consists
obtaining multiple BNs domain multiple experts and, then, combining
single consensus BN. approach received significant attention literature (Matzkevich & Abramson, 1992, 1993b; Maynard-Reid II & Chajewska, 2001; Nielsen
& Parsons, 2007; Pennock & Wellman, 1999; Richardson & Domingos, 2003; del Sagrado
& Moral, 2003). relevant references probably work Pennock
Wellman (1999), shows even experts agree BN structure,
method combining experts BNs produces consensus BN respects
reasonable assumptions whose structure agreed BN structure. Unfortunately,
problem often overlooked. avoid it, propose combine experts BNs
c
2011
AI Access Foundation. rights reserved.

fiPena

two steps. First, finding consensus BN structure and, then, finding consensus
parameters consensus BN structure. paper focuses first step (we
briefly discuss second step Section 8). Specifically, assume multiple experts
provide us alternative DAG models domain, interested combining
single consensus DAG. Specifically, interested consensus
DAG represents independences given DAGs agree upon many
possible. words, consensus DAG DAG represents independences among minimal directed independence (MDI) maps intersection
independence models induced given DAGs.1 knowledge, whether
consensus DAG cannot found efficiently still open problem. See work
Matzkevich Abramson (1992, 1993b) information. paper, redefine
consensus DAG DAG fewest parameters associated among
MDI maps intersection independence models induced given DAGs.
definition line finding DAG represent probability distribution p.
desired DAG typically defined MDI map p fewest parameters
associated rather MDI map p represents independences. See,
instance, work Chickering et al. (2004). number parameters associated
DAG measure complexity DAG, since number parameters
required specify probability distributions represented DAG.
paper, prove may exist several non-equivalent consensus DAGs
finding one NP-hard. Thus, decide resort heuristics find
approximated consensus DAG. paper, consider following heuristic due
Matzkevich Abramson (1992, 1993b). See work Matzkevich Abramson
(1993a) related information. First, let denote ordering nodes given
DAGs, denote G1 , . . . , Gm . Then, find MDI map Gi Gi relative
. Finally, let approximated consensus DAG DAG whose arcs exactly
union arcs G1 , . . . , Gm
. mentioned formulation
heuristic differs Matzkevich Abramson (1992, 1993b) following two
points. First, heuristic introduced original definition consensus DAG.
justify later heuristic makes sense definition consensus DAG.
Second, originally required consistent one given DAGs. remove
requirement. all, key step heuristic finding MDI map Gi
Gi . Since task trivial, Matzkevich Abramson (1993b) present two algorithms,
called Methods B, efficiently deriving Gi Gi . Methods B claimed
correct although proof provided (a proof sketched). paper,
show Methods B correct propose correction them.
said, first study problem finding consensus DAG. addition works discussed Matzkevich Abramson (1992, 1993b) Pennock
Wellman (1999), works devoted problem Maynard-Reid
II Chajewska (2001); Nielsen Parsons (2007); Richardson Domingos (2003);
1. worth mentioning term consensus DAG different meaning computational biology
(Jackson et al., 2005). There, consensus DAG given set DAGs G1 , . . . , Gm defined
DAG contains arcs G1 , . . . , Gm . Therefore, difficulty lies keeping many
arcs possible without creating cycles. Note that, unlike present work, DAG interpreted
inducing independence model Jackson et al.

662

fiFinding Consensus Bayesian Network Structures

del Sagrado Moral (2003). elaborate differences works
ours. Maynard-Reid II Chajewska (2001) propose adapt existing score-based algorithms learning DAGs data case learning data replaced
BNs provided experts. approach suffers problem pointed Pennock
Wellman (1999), consists essentially learning consensus DAG
combination given BNs. somehow related approach proposed Richardson
Domingos (2003). Specifically, propose Bayesian approach learning DAGs
data, prior probability distribution DAGs constructed DAGs
provided experts. Since approach requires data combine
given DAGs single DAG, addresses problem rather different one
paper. Moreover, construction prior probability distribution DAGs ignores
fact given DAGs may different equivalent. is, unlike
present work, DAG interpreted inducing independence model. work
relatively close del Sagrado Moral (2003). Specifically, show
construct MDI map intersection union independence models
induced DAGs provided experts. However, three main differences
work ours. First, unlike us, assume given DAGs
defined set nodes. Second, unlike us, assume exists
node ordering consistent given DAGs. Third, goal find MDI
map whereas find MDI map fewest parameters associated among
MDI maps, i.e. consensus DAG. Finally, Nielsen Parsons (2007) develop
general framework construct consensus DAG gradually. framework general
sense tailored particular definition consensus DAG. Instead,
relies upon score defined user expert use score different
extensions current partial consensus DAG. individual scores combined
choose extension perform. Unfortunately, see framework could
applied definition consensus DAG.
worth recalling paper deals combination probability distributions expressed BNs. readers interested combination probability distributions expressed non-graphical numerical forms referred to, instance, work
Genest Zidek (1986). Note interested combination
data observed. readers interested combination data
observed expert updated beliefs accordingly referred to, instance,
work Ng Abramson (1994). Finally, note aim combining given
DAGs DAG, consensus DAG. readers interested finding DAG
graphical features (e.g. arcs paths) significant number experts agree upon may
want consult works Friedman Koller (2003); Hartemink et al. (2002); Pena et
al. (2004), since works deal similar problem.
rest paper organized follows. start reviewing preliminary
concepts Section 2. analyze complexity finding consensus DAG Section
3. discuss heuristic finding approximated consensus DAG detail
Section 4. introduce Methods B Section 5 show correct.
correct Section 6. analyze complexity corrected Methods
B Section 7 show efficient approach think
solve problem. close discussion Section 8.
663

fiPena

2. Preliminaries
section, review concepts used paper. DAGs, probability
distributions independence models paper defined V, unless otherwise
stated. B DAG G, say B adjacent G. Moreover,
say parent B B child G. denote parents B G
P aG (B). node called sink node G children G. route
two nodes B G sequence nodes starting ending B
every two consecutive nodes sequence adjacent G. Note nodes
route necessarily distinct. length route number (not necessarily
distinct) arcs route. treat nodes G routes length zero. route
B called descending B arcs route directed
towards B. descending route B, B called descendant A.
Note descendant itself, since allow routes length zero. Given subset
X V, node X called maximal G descendant node X \ {A}
G. Given route B G route 0 B C G, 0
denotes route C G resulting appending 0 .
P
Q
number parameters associated DAG G BV [ AP aG (B) rA ](rB 1),
rA rB numbers states random variables corresponding
node B. arc B G said covered P aG (A) = P aG (B) \ {A}.
covering arc B G mean adding G smallest set arcs B
becomes covered. say node C collider route DAG exist two
nodes B C B subroute route. Note B may
coincide. Let X, Z denote three disjoint subsets V. route DAG said
Z-active (i) every collider node route Z, (ii) every non-collider node
route outside Z. route DAG G node X
node Z-active, say X separated given Z G denote
X G Y|Z. denote X 6 G Y|Z X G Y|Z hold. definition
separation equivalent common definitions (Studeny, 1998, Section 5.1).
Let X, Y, Z W denote four disjoint subsets V. Let us abbreviate X
XY. independence model set statements form X Y|Z, meaning
X independent given Z. Given subset U V, denote [M ]U
statements X, Y, Z U. Given two independence models N ,
denote N X Y|Z X N Y|Z. say graphoid
satisfies following properties: symmetry X Y|Z X|Z, decomposition
X YW|Z X Y|Z, weak union X YW|Z X Y|ZW, contraction
X Y|ZW X W|Z X YW|Z, intersection X Y|ZW X
W|ZY X YW|Z. independence model induced probability distribution p,
denoted I(p), set probabilistic independences p. independence model
induced DAG G, denoted I(G), set separation statements X G Y|Z.
known I(G) graphoid (Studeny & Bouckaert, 1998, Lemma 3.1). Moreover, I(G)
satisfies composition property X G Y|Z X G W|Z X G YW|Z (Chickering &
Meek, 2002, Proposition 1). Two DAGs G H called equivalent I(G) = I(H).
DAG G directed independence map independence model I(G) .
Moreover, G minimal directed independence (MDI) map removing arc
664

fiFinding Consensus Bayesian Network Structures

G makes cease directed independence map . say G
ordering nodes consistent when, every arc B G, precedes B
node ordering. say DAG G MDI map independence model
relative node ordering G MDI map G consistent .
graphoid, G unique (Pearl, 1988, Thms. 4 9). Specifically, node
A, P aG (A) smallest subset X predecessors , P (A),
P (A) \ X|X.

3. Finding Consensus DAG NP-Hard
Recall defined consensus DAG given set DAGs G1 , . . . , Gm

DAG fewest parameters associated among MDI maps
i=1 I(G ).
sensible way start quest consensus DAG investigating whether
exist several non-equivalent consensus DAGs. following theorem answers question.
Theorem 1. exists set DAGs two non-equivalent consensus DAGs.
Proof. Consider following two DAGs four random variables number
states each:


K



J







L

K



J

L

following two non-equivalent DAGs consensus DAG two DAGs
above:


K


&




K

J

L


%


J

L

natural follow-up question investigate whether consensus DAG found
efficiently. Unfortunately, finding consensus DAG NP-hard, prove below. Specifically, prove following decision problem NP-hard:
CONSENSUS
INSTANCE: set DAGs G1 , . . . , Gm V, positive integer d.

QUESTION: exist DAG G V I(G)
i=1 I(G )
number parameters associated G greater ?

Proving CONSENSUS NP-hard implies finding consensus DAG
NP-hard, existed efficient algorithm finding consensus DAG,
could use solve CONSENSUS efficiently. proof makes use following two
665

fiPena

decision problems:
FEEDBACK ARC SET
INSTANCE: directed graph G = (V, A) positive integer k.
QUESTION: exist subset B |B| k B least
one arc every directed cycle G ?
LEARN
INSTANCE: probability distribution p V, positive integer d.
QUESTION: exist DAG G V I(G) I(p) number
parameters associated G greater ?
FEEDBACK ARC SET NP-complete (Garey & Johnson, 1979). FEEDBACK ARC
SET remains NP-complete directed graphs total degree vertex
three (Gavril, 1977). degree-bounded FEEDBACK ARC SET problem used
Chickering et al. (2004) prove LEARN NP-hard. proof, Chickering
et al. (2004) use following polynomial reduction instance degree-bounded
FEEDBACK ARC SET instance LEARN:
Let instance degree-bounded FEEDBACK ARC SET consist directed
graph F = (VF , AF ) positive integer k.
Let L denote DAG whose nodes arcs determined F follows.
every arc ViF VjF AF , create following nodes arcs L:

ViF(9)



Aij (9)

Bij (2)

Cij (3)

Hij

(2)

.

&

Dij (9)

Eij (2)

Fij (2)



Gij



VjF(9)

(9)

number parenthesis besides node number states corresponding random variable. Let HL denote nodes Hij L, let VL denote
rest nodes L.
Specify (join) probability distribution p(HL , VL ) I(p(HL , VL )) = I(L).
Let instance LEARN consist (marginal) probability distribution p(VL )
positive integer d, computed F k shown work
Chickering et al. (2004, Equation 2).
describe instance LEARN resulting reduction
reduced instance CONSENSUS polynomial time:
Let C 1 denote DAG VL arcs L whose
endpoints VL .
666

fiFinding Consensus Bayesian Network Structures

Let C 2 denote DAG VL arcs Bij Cij Fij
j.
Let C 3 denote DAG VL arcs Cij Fij Eij
j.
Let instance CONSENSUS consist DAGs C 1 , C 2 C 3 , positive
integer d.
Theorem 2. CONSENSUS NP-hard.
Proof. start proving polynomial reduction instance F
degree-bounded FEEDBACK ARC SET instance C CONSENSUS. First, reduce
F instance L LEARN shown work Chickering et al. (2004) and, then,
reduce L C shown above.
prove solution F iff solution C. Chickering et
al. (2004, Thms. 8 9) prove solution F iff solution L.
Therefore, remains prove solution L iff solution
C (note parameter L parameter C same). Let L
p(HL , VL ) denote DAG probability distribution constructed reduction
F L. Recall I(p(HL , VL )) = I(L). Moreover:
Let L1 denote DAG (HL , VL ) arcs L whose
endpoints VL .
Let L2 denote DAG (HL , VL ) arcs Bij Cij Hij Fij
j.
Let L3 denote DAG (HL , VL ) arcs Cij Hij Fij Eij
j.
Note separation statement holds L holds L1 , L2 L3 . Then,
I(p(HL , VL )) = I(L) 3i=1 I(Li ) and, thus, I(p(VL )) [3i=1 I(Li )]VL = 3i=1 [I(Li )]VL .
Let C 1 , C 2 C 3 denote DAGs constructed reduction L C. Note
[I(Li )]VL = I(C ) i. Then, I(p(VL )) 3i=1 I(C ) and, thus, solution
L solution C. prove opposite. proof essentially
work Chickering et al. (2004, Thm. 9). Let us define (Vi , Vj )
edge component DAG G VL subgraph G arcs
G whose endpoints {Vi , Aij , Bij , Cij , Dij , Eij , Fij , Gij , Vj }. Given solution
C C, create another solution C 0 C follows:
Initialize C 0 C 1 .
every (Vi , Vj ) edge component C, directed path C Vi
Vj , add C 0 arcs Eij Cij Fij .
every (Vi , Vj ) edge component C, directed path C Vi Vj ,
add C 0 arcs Bij Fij Cij .
667

fiPena

Note C 0 acyclic C acyclic. Moreover, I(C 0 ) 3i=1 I(C )
I(C 0 ) I(C ) i. order able conclude C 0 solution C,
remains prove number parameters associated C 0 greater
d. Specifically, prove C 0 parameters associated C,
less parameters associated solution C.
seen before, I(C 0 ) I(C 1 ). Likewise, I(C) I(C 1 ) C solution C.
Thus, exists sequence (resp. 0 ) covered arc reversals arc additions
transforms C 1 C (resp. C 0 ) (Chickering, 2002, Thm. 4). Note covered arc
reversal modify number parameters associated DAG, whereas arc
addition increases (Chickering, 1995, Thm. 3). Thus, 0 monotonically increase
number parameters associated C 1 transform it. Recall C 1 consists
series edge components form

ViF(9)



Aij (9)

Bij (2)

Cij (3)

Dij (9)

Eij (2)

Fij (2)



Gij



VjF(9)

(9)

number parenthesis besides node number states corresponding
random variable. Let us study sequences 0 modify edge component
C 1 . 0 simply adds arcs Bij Fij Cij arcs Eij Cij Fij . Note
adding first pair arcs results increase 10 parameters, whereas adding
second pair arcs results increase 12 parameters. Unlike 0 , may reverse
arc edge component. case, must cover arc first, implies
increase least 16 parameters (covering Fij Vj adding Eij Vj implies
increase exactly 16 parameters, whereas arc covering implies larger increase).
Then, implies larger increase number parameters 0 . hand,
reverse arc edge component, simply adds arcs
C C 1 . Note either Cij Fij Cij Fij C, otherwise
Cij C Fij |Z Z VL contradicts fact C solution C since
Cij 6 C 2 Fij |Z. Cij Fij C, either Bij Fij Bij Fij C
otherwise Bij C Fij |Z Z VL Cij Z, contradicts fact
C solution C since Bij 6 C 2 Fij |Z. Bij Fij would create cycle C, Bij Fij
C. Therefore, adds arcs Bij Fij Cij and, construction C 0 , 0
adds them. Thus, implies increase least many parameters 0 .
hand, Cij Fij C, either Cij Eij Cij Eij C otherwise
Cij C Eij |Z Z VL Fij Z, contradicts fact C
solution C since Cij 6 C 3 Eij |Z. Cij Eij would create cycle C, Cij Eij
C. Therefore, adds arcs Eij Cij Fij and, construction C 0 , 0 adds either
arcs Eij Cij Fij arcs Bij Fij Cij . case, implies increase
least many parameters 0 . Consequently, C 0 parameters
associated C.
Finally, note I(p(VL )) I(C 0 ) Chickering et al. (2004, Lemma 7). Thus,
solution C solution L.
668

fiFinding Consensus Bayesian Network Structures

worth noting proof contains two restrictions. First, number
DAGs consensuate three. Second, number states random variable
VL arbitrary prescribed. first restriction easy relax: proof
extended consensuate three DAGs simply letting C DAG VL
arcs > 3. However, open question whether CONSENSUS remains
NP-hard number DAGs consensuate two and/or number states
random variable VL arbitrary.
following theorem strengthens previous one.
Theorem 3. CONSENSUS NP-complete.
Proof. Theorem 2, remains prove CONSENSUS NP, i.e.
verify polynomial time given DAG G solution given instance
CONSENSUS.
Let denote node ordering consistent G. causal list G relative
set separation statements G P (A) \ P aG (A)|P aG (A) node A.
known I(G) coincides closure respect graphoid properties

causal list G relative (Pearl, 1988, Corollary 7). Therefore, I(G)
i=1 I(G ) iff

Gi P (A) \ P aG (A)|P aG (A) 1 m,
i=1 I(G ) graphoid (del
Sagrado & Moral, 2003, Corollary 1). Let n, ai denote, respectively, number
nodes G, number arcs G, number arcs Gi . Let b = max1im ai .
Checking separation statement Gi takes O(ai ) time (Geiger et al., 1990, p. 530). Then,

checking whether I(G)
i=1 I(G ) takes O(mnb) time. Finally, note computing
number parameters associated G takes O(a).

4. Finding Approximated Consensus DAG
Since finding consensus DAG given DAGs NP-hard, decide resort
heuristics find approximated consensus DAG. mean discard
existence fast super-polynomial algorithms. simply means pursue
possibility paper. Specifically, paper consider following heuristic
due Matzkevich Abramson (1992, 1993b). See work Matzkevich
Abramson (1993a) related information. First, let denote ordering nodes
given DAGs, denote G1 , . . . , Gm . Then, find MDI map Gi
Gi relative . Finally, let approximated consensus DAG DAG whose
arcs exactly union arcs G1 , . . . , Gm
. following theorem justifies taking
union arcs. Specifically, proves DAG returned heuristic
consensus DAG required consistent .
Theorem 4. DAG H returned heuristic DAG fewest

parameters associated among MDI maps
i=1 I(G ) relative .

Proof. start proving H MDI map
i=1 I(G ). First, show




I(H)
i=1 I(G ). suffices note I(H) i=1 I(G ) G subm





graph H, i=1 I(G ) i=1 I(G ) I(G ) I(G ) i. Now,

669

fiPena

assume contrary DAG H 0 resulting removing arc B H


satisfies I(H 0 )
i=1 I(G ). construction H, B G i, say
= j. Note B H 0 P (B) \ P aH 0 (B)|P aH 0 (B), implies B Gj P (B) \


((m
i=1 P aGi (B)) \ {A})|(i=1 P aGi (B)) \ {A} P aH 0 (B) = (i=1 P aGi (B)) \ {A}

I(H 0 )
i=1 I(G ). Note B Gj P (B) \ P aGj (B)|P aGj (B), implies B Gj P (B) \ P aGj (B)|P aGj (B) I(Gj ) I(Gj ). Therefore, B Gj


P (B) \ (P aGj (B) \ {A})|P aGj (B) \ {A} intersection. However, contradicts



fact Gj MDI map Gj relative . Then, H MDI map
i=1 I(G )
relative .

Finally, note
i=1 I(G ) graphoid (del Sagrado & Moral, 2003, Corollary 1).

Consequently, H MDI map
i=1 I(G ) relative .

key step heuristic is, course, choosing good node ordering . Unfortunately, fact CONSENSUS NP-hard implies NP-hard find
best node ordering , i.e. node ordering makes heuristic return MDI

map
i=1 I(G ) fewest parameters associated. see it, note
existed efficient algorithm finding best node ordering, Theorem 4 would
imply could solve CONSENSUS efficiently running heuristic best
node ordering.
last sentence, implicitly assumed heuristic efficient,
implies implicitly assumed efficiently find MDI map Gi
Gi . rest paper shows assumption correct.

5. Methods B Correct
Matzkevich Abramson (1993b) propose heuristic discussed previous section, present two algorithms, called Methods B, efficiently
deriving MDI map G DAG G relative node ordering . algorithms work
iteratively covering reversing arc G resulting DAG consistent
. obvious way working produces directed independence map G.
However, order arrive G , arc cover reverse iteration must
carefully chosen. pseudocode Methods B seen Figure 1. Method
starts calling Construct derive node ordering consistent G
close possible (line 6). close possible, mean
number arcs Methods B later cover reverse kept minimum,
Methods B use choose arc cover reverse iteration.
particular, Method finds leftmost node interchanged left
neighbor (line 2) repeatedly interchanges node left neighbor (lines 3-4
6-7). interchanges preceded covering reversing corresponding arc G (line 5). Method B essentially identical Method A. differences
word right replaced word left vice versa
lines 2-4, arcs point opposite directions line 5. Note Methods
B reverse arc once.
670

fiFinding Consensus Bayesian Network Structures

Construct (G, )
/* Given DAG G node ordering , algorithm returns node ordering
consistent G close possible */
1
2
3
/* 3
4
5
6
7
8
9
10
11

=
G0 = G
Let denote sink node G0
Let denote rightmost node sink node G0 */
Add leftmost node
Let B denote right neighbor
B 6=
/ P aG (B) right B
Interchange B
Go line 5
Remove incoming arcs G0
G0 6= go line 3
Return
Method A(G, )
/* Given DAG G node ordering , algorithm returns G */

1
2
3
4
5
6
7
8
9

=Construct (G, )
Let denote leftmost node whose left neighbor right
Let Z denote left neighbor
Z right
Z G cover reverse Z G
Interchange Z
Go line 3
6= go line 2
Return G
Method B(G, )
/* Given DAG G node ordering , algorithm returns G */

1
2
3
4
5
6
7
8
9

=Construct (G, )
Let denote leftmost node whose right neighbor left
Let Z denote right neighbor
Z left
Z G cover reverse Z G
Interchange Z
Go line 3
6= go line 2
Return G

Figure 1: Construct , Methods B. correction Construct consists (i)
replacing line 3 line comments it, (ii) removing lines 5-8.
671

fiPena

Figure 2: counterexample correctness Methods B.
Methods B claimed correct work Matzkevich Abramson
(1993b, Thm. 4 Corollary 2) although proof provided (a proof sketched).
following counterexample shows Methods B actually correct. Let G
DAG left-hand side Figure 2. Let = (M, I, K, J, L). Then, make
use characterization introduced Section 2 see G DAG center
Figure 2. However, Methods B return DAG right-hand side Figure
2. see it, follow execution Methods B step step. First, Methods
B construct calling Construct , runs follows:
1. Initially, = G0 = G.
2. Select sink node G0 . Then, = (M ). Remove incoming arcs
G0 .
3. Select sink node L G0 . Then, = (L, ). interchange performed
L P aG (M ). Remove L incoming arcs G0 .
4. Select sink node K G0 . Then, = (K, L, ). interchange performed
K left L . Remove K incoming arcs G0 .
5. Select sink node J G0 . Then, = (J, K, L, ). interchange performed
J P aG (K).
6. Select sink node G0 . Then, = (I, J, K, L, ). interchange
performed left J .
Construct ends, Methods B continue follows:
672

fiFinding Consensus Bayesian Network Structures

7. Initially, = (I, J, K, L, ).
8. Add arc J reverse arc J K G. Interchange J K .
Then, = (I, K, J, L, ).
9. Add arc J reverse arc L G. Interchange L .
Then, = (I, K, J, M, L).
10. Add arcs K , reverse arc J G. Interchange J
. Then, = (I, K, M, J, L).
11. Reverse arc K G. Interchange K . Then, = (I, M, K, J, L).
12. Reverse arc G. Interchange . Then, = (M, I, K, J, L) =
.
matter fact, one see early step 8 Methods B
fail: One see separated DAG resulting step 8,
implies separated DAG returned Methods B,
covering reversing arcs never introduces new separation statements. However,
separated G .
Note constructed selecting first , L, K, J, finally I.
However, could selected first K, I, , L, finally J, would
resulted = (J, L, M, I, K). , Methods B return G . Therefore,
makes difference sink node selected line 3 Construct . However, Construct
overlooks detail. propose correcting Construct (i) replacing line 3 Let
denote rightmost node sink node G0 , (ii) removing lines 5-8 since
never executed. Hereinafter, assume call Construct call
corrected version thereof. rest paper devoted prove Methods
B return G .

6. Corrected Methods B Correct
proving Methods B correct, introduce auxiliary lemmas.
proof found Appendix A. Let us call percolating right-to-left
iterating lines 3-7 Method possible. Let us modify Method replacing
line 2 Let denote leftmost node considered
adding check Z 6= line 4. pseudocode resulting algorithm,
call Method A2, seen Figure 3. Method A2 percolates right-to-left one
one nodes order appear .
Lemma 1. Method A(G, ) Method A2(G, ) return DAG.
Lemma 2. Method A2(G, ) Method B(G, ) return DAG.
Let us call percolating left-to-right iterating lines 3-7 Method B
possible. Let us modify Method B replacing line 2 Let denote rightmost
node considered adding check Z 6= line 4.
pseudocode resulting algorithm, call Method B2, seen Figure
673

fiPena

Method A2(G, )
/* Given DAG G node ordering , algorithm returns G */
1
2
3
4
5
6
7
8
9

=Construct (G, )
Let denote leftmost node considered
Let Z denote left neighbor
Z 6= Z right
Z G cover reverse Z G
Interchange Z
Go line 3
6= go line 2
Return G
Method B2(G, )
/* Given DAG G node ordering , algorithm returns G */

1
2
3
4
5
6
7
8
9

=Construct (G, )
Let denote rightmost node considered
Let Z denote right neighbor
Z 6= Z left
Z G cover reverse Z G
Interchange Z
Go line 3
6= go line 2
Return G

Figure 3: Methods A2 B2.
3. Method B2 percolates left-to-right one one nodes reverse order
appear .
Lemma 3. Method B(G, ) Method B2(G, ) return DAG.
ready prove main result paper.
Theorem 5. Let G denote MDI map DAG G relative node ordering . Then,
Method A(G, ) Method B(G, ) return G .
Proof. Lemmas 1-3, suffices prove Method B2(G, ) returns G . evident
Method B2 transforms and, thus, halts point. Therefore,
Method B2 performs finite sequence n modifications (arc additions covered arc
reversals) G. Let Gi denote DAG resulting first modifications G,
let G0 = G. Specifically, Method B2 constructs Gi+1 Gi either (i) reversing
covered arc Z, (ii) adding arc X Z X P aGi (Y ) \ P aGi (Z),
(iii) adding arc X X P aGi (Z) \ P aGi (Y ). Note I(Gi+1 ) I(Gi )
0 < n and, thus, I(Gn ) I(G0 ).
674

fiFinding Consensus Bayesian Network Structures

start proving Gi DAG consistent 0 n. Since
true G0 due line 1, suffices prove Gi DAG consistent
Gi+1 0 < n. consider following four cases.
Case 1 Method B2 constructs Gi+1 Gi reversing covered arc Z. Then,
Gi+1 DAG reversing covered arc create cycle (Chickering,
1995, Lemma 1). Moreover, note Z interchanged immediately
covered arc reversal. Thus, Gi+1 consistent .
Case 2 Method B2 constructs Gi+1 Gi adding arc X Z X
P aGi (Y ) \ P aGi (Z). Note X left left Z ,
Gi consistent . Then, X left Z and, thus, Gi+1
DAG consistent .
Case 3 Method B2 constructs Gi+1 Gi adding arc X X
P aGi (Z) \ P aGi (Y ). Note X left Z Gi consistent
, left neighbor Z (recall line 3). Then, X left
and, thus, Gi+1 DAG consistent .
Case 4 Note may get modified Method B2 constructs Gi+1 Gi . Specifically, happens Method B2 executes lines 5-6 arc
Z Gi . However, fact Gi consistent Z
interchanged fact Z neighbors (recall line 3) imply
Gi consistent Z interchanged.
Since Method B2 transforms , follows result proven Gn
DAG consistent . order prove theorem, i.e. Gn = G ,
remains prove I(G ) I(Gn ). see it, note Gn = G follows
I(G ) I(Gn ), I(Gn ) I(G0 ), fact Gn DAG consistent ,
fact G unique MDI map G0 relative . Recall G guaranteed
unique I(G0 ) graphoid.
rest proof devoted prove I(G ) I(Gn ). Specifically, prove
I(G ) I(Gi ) I(G ) I(Gi+1 ) 0 < n. Note implies
I(G ) I(Gn ) I(G ) I(G0 ) definition MDI map. First, prove
Method B2 constructs Gi+1 Gi reversing covered arc Z.
arc reversed covered implies I(Gi+1 ) = I(Gi ) (Chickering, 1995, Lemma 1). Thus,
I(G ) I(Gi+1 ) I(G ) I(Gi ).
Now, prove I(G ) I(Gi ) I(G ) I(Gi+1 ) 0 < n
Method B2 constructs Gi+1 Gi adding arc. Specifically, prove
S-active route (S V) AB
i+1 two nodes B Gi+1 ,
S-active route B G . prove result induction number
occurrences added arc AB
i+1 . assume without loss generality added
AB
arc occurs i+1 fewer times S-active route B
2
Gi+1 . call minimality property AB
i+1 . number occurrences
2. difficult show number occurrences added arc AB
i+1 two
(see Case 2.1 intuition). However, proof theorem simpler ignore fact.

675

fiPena

Figure 4: Different cases proof Theorem 5. relevant subgraphs Gi+1
G depicted. undirected edge two nodes denotes nodes
adjacent. curved edge two nodes denotes S-active route
two nodes. curved edge directed, route descending.
grey node denotes node S.

AB
added arc AB
i+1 zero, i+1 S-active route B Gi and,
thus, S-active route B G since I(G ) I(Gi ). Assume
induction hypothesis result holds k occurrences added arc AB
i+1 .
prove k + 1 occurrences. consider following two cases. case
illustrated Figure 4.

Case 1 Method B2 constructs Gi+1 Gi adding arc X Z X
3
AB
AX
ZB
P aGi (Y )\P aGi (Z). Note X Z occurs AB
i+1 . Let i+1 = i+1 X Zi+1 .
AX
AB
Note X
/ i+1 S-active Gi+1 because, otherwise, i+1 would
3. Note maybe = X and/or B = Z.

676

fiFinding Consensus Bayesian Network Structures

S-active Gi+1 . Then, S-active route AX
X G

ZB
induction hypothesis. Moreover, because, otherwise, AX
i+1 X Z i+1
would S-active route B Gi+1 would violate minimality
property AB
i+1 . Note Z G (i) Z adjacent
G since I(G ) I(Gi ), (ii) Z left (recall line 4). Note
X G . see it, note X adjacent G since
I(G ) I(Gi ). Recall Method B2 percolates left-to-right one one
nodes reverse order appear . Method B2 currently
percolating and, thus, nodes right right
too. X G X would right and, thus, X would
right . However, would contradict fact X
left , follows fact Gi consistent . Thus, X
G . consider two cases.
Case 1.1 Assume Z
/ S. Then, ZB
i+1 S-active Gi+1 because, otherwise,
AB
i+1 would S-active Gi+1 . Then, S-active route ZB

Z B G induction hypothesis. Then, AX

X



Z ZB


S-active route B G .
WB 4
Case 1.2 Assume Z S. Then, ZB
/
i+1 = Z W i+1 . Note W
W
B
AB
i+1 S-active Gi+1 because, otherwise, i+1 would S-active Gi+1 .
B W B G induction
Then, S-active route W


hypothesis. Note W Z adjacent G since I(G ) I(Gi ).
fact proven Z G imply W adjacent
G because, otherwise, 6 Gi W |U G W |U U V
Z U, would contradict I(G ) I(Gi ). fact, W
G . see it, recall nodes right right
too. W G W would right and, thus,
W would right too. However, would contradict fact
W left , follows fact W left
Z Gi consistent , fact left neighbor
WB
Z (recall line 3). Thus, W G . Then, AX
X W
S-active route B G .

Case 2 Method B2 constructs Gi+1 Gi adding arc X X
5
AB
AX
YB
P aGi (Z)\P aGi (Y ). Note X occurs AB
i+1 . Let i+1 = i+1 X i+1 .
AX
AB
Note X
/ i+1 S-active Gi+1 because, otherwise, i+1 would
S-active Gi+1 . Then, S-active route AX
X G

induction hypothesis. Note Z G (i) Z adjacent
G since I(G ) I(Gi ), (ii) Z left (recall line 4). Note
X Z adjacent G since I(G ) I(Gi ). fact Z
G imply X adjacent G because, otherwise, X 6 Gi |U
X G |U U V Z U, would contradict
WB
4. Note maybe W = B. Note W 6= X because, otherwise, AX
i+1 X X i+1 would
S-active route B Gi+1 would violate minimality property AB
i+1 .
5. Note maybe = X and/or B = .

677

fiPena

I(G ) I(Gi ). fact, X G . see it, recall Method B2 percolates
left-to-right one one nodes reverse order appear
. Method B2 currently percolating and, thus, nodes right
right too. X G X would
right and, thus, X would right too. However, would
contradict fact X left , follows fact
X left Z Gi consistent , fact
left neighbor Z (recall line 3). Thus, X G . consider three
cases.
B = X XB . Note XB S-active
Case 2.1 Assume Yi+1
i+1
i+1
AB
Gi+1 because, otherwise, i+1 would S-active Gi+1 . Then,
S-active route XB
X B G induction hypothesis.

AX
Then, X X XB
S-active route B G .

B = W W B .6 Note W
Case 2.2 Assume Yi+1
/
i+1
W
B
i+1 S-active Gi+1 because, otherwise, AB
would


S-active
Gi+1 .
i+1
B W B G induction
Then, S-active route W


hypothesis. Note W G . see it, note W
adjacent G since I(G ) I(Gi ). Recall nodes right
right too. W G W would
right and, thus, W would right too.
However, would contradict fact W left ,
follows fact Gi consistent . Thus, W G . Then,
W B S-active route B G .
AX

X W

Case 2.3 Assume
/ S. proof case based step 8
work Chickering (2002, Lemma 30). Let denote node maximal
G set descendants Gi . Note guaranteed
unique Chickering (2002, Lemma 29), I(G ) I(Gi ). Note
6= , Z descendant Gi and, shown above, Z
G . show descendant Z Gi . consider three cases.
Case 2.3.1 Assume = Z. Then, descendant Z Gi .
Case 2.3.2 Assume 6= Z descendant Z G0 . Recall
Method B2 percolates left-to-right one one nodes
reverse order appear . Method B2 currently percolating
and, thus, yet percolated Z Z left
(recall line 4). Therefore, none descendants Z G0 (among
D) left Z . fact consistent Gi
imply Z node maximal Gi set descendants
Z G0 . Actually, Z node Chickering (2002, Lemma 29),
I(Gi ) I(G0 ). Then, descendants Z G0 descendants
Z Gi too. Thus, descendant Z Gi .
6. Note maybe W = B. Note W 6= X, case W = X covered Case
2.1.

678

fiFinding Consensus Bayesian Network Structures

Case 2.3.3 Assume 6= Z descendant Z G0 .
shown Case 2.3.2, descendants Z G0 descendants Z
Gi too. Therefore, none descendants Z G0 left
because, otherwise, descendant Z thus Gi would
left , would contradict definition D.
fact descendant Z G0 imply still
G0 Z became sink node G0 Construct (recall Figure 1).
Therefore, Construct added added Z (recall lines 3-4),
left Z definition D.7 reason,
Method B2 interchanged Z (recall line 4). Thus,
currently still left Z , implies left
, left neighbor Z (recall line 3). However,
contradicts fact Gi consistent , descendant
Gi . Thus, case never occurs.
B
continue proof Case 2.3. Note
/ implies Yi+1
S-active Gi+1 because, otherwise, AB
i+1 would S-active Gi+1 . Note
descendant Z Gi because, otherwise, would
XY B
S-active route XY
X Gi and, thus, AX

i+1
i+1
would S-active route B Gi+1 would violate
minimality property AB
/ because, shown above,
i+1 . implies
descendant Z Gi . implies S-active descending
ZD S-active route
route ZD
Z Gi . Then, AX

i+1 X Z
ZD S-active route
Gi+1 . Likewise,
i+1 Z

B Gi+1 , i+1 denotes route resulting reversing
B . Therefore, S-active routes AD BD
Yi+1


B G induction hypothesis.
Consider subroute AB
i+1 starts arc X continues
direction arc reaches node E E = B E S.
Note E descendant Gi and, thus, E descendant G
definition D. Let DE
denote descending route E G .

Assume without loss generality G descending route
B node shorter DE
. implies E = B
DE
DE
S-active G because, shown above,
/ S. Thus, AD

S-active route B G . hand, E E 6=
DE ED DB S-active route

/ S. Thus, AD



ED
B G , DB
denote

routes resulting reversing

BD .
DE





Finally, show correctness Method B2 leads alternative proof
so-called Meeks conjecture (1997). Given two DAGs G H I(H) I(G),
Meeks conjecture states transform G H sequence arc additions
covered arc reversals operation sequence G DAG
7. Note statement true thanks correction Construct .

679

fiPena

Method G2H(G, H)
/* Given two DAGs G H I(H) I(G), algorithm transforms
G H sequence arc additions covered arc reversals
operation sequence G DAG I(H) I(G) */
1
2
3

Let denote node ordering consistent H
G=Method B2(G, )
Add G arcs H G

Figure 5: Method G2H.
I(H) I(G). importance Meeks conjecture lies allows develop efficient
asymptotically correct algorithms learning BNs data mild assumptions
(Chickering, 2002; Chickering & Meek, 2002; Meek, 1997; Nielsen et al., 2003). Meeks
conjecture proven true work Chickering (2002, Thm. 4) developing
algorithm constructs valid sequence arc additions covered arc reversals.
propose alternative algorithm construct sequence. pseudocode
algorithm, called Method G2H, seen Figure 5. following corollary proves
Method G2H correct.
Corollary 1. Given two DAGs G H I(H) I(G), Method G2H(G, H)
transforms G H sequence arc additions covered arc reversals
operation sequence G DAG I(H) I(G).
Proof. Note Method G2Hs line 1 denotes node ordering consistent
H. Let G denote MDI map G relative . Recall G guaranteed
unique I(G) graphoid. Note I(H) I(G) implies G subgraph
H. see it, note I(H) I(G) implies obtain MDI map G relative
removing arcs H. However, G MDI map G relative .
Then, follows proof Theorem 5 Method G2Hs line 2 transforms
G G sequence arc additions covered arc reversals,
operation sequence G DAG I(G ) I(G). Thus, operation
sequence I(H) I(G) I(H) I(G ) since, shown above, G subgraph
H. Moreover, Method G2Hs line 3 transforms G G H sequence arc
additions. course, arc addition G DAG I(H) I(G) G
subgraph H.

7. Corrected Methods B Efficient
section, show Methods B efficient solution
problem think of. Let n denote, respectively, number
nodes arcs G. Moreover, let us assume hereinafter DAG implemented
680

fiFinding Consensus Bayesian Network Structures

adjacency matrix, whereas node ordering implemented array entry per
node indicating position node ordering. Since I(G) graphoid, first
solution think consists applying following characterization G :
node A, P aG (A) smallest subset X P (A) G P (A) \ X|X.
solution implies evaluating node O(2n ) subsets P (A). Evaluating
subset implies checking separation statement G, takes O(a) time (Geiger et al.,
1990, p. 530). Therefore, overall runtime solution O(an2n ).
Since I(G) satisfies composition property addition graphoid properties,
efficient solution consists running incremental association Markov boundary
(IAMB) algorithm (Pena et al., 2007, Thm. 8) node find P aG (A). IAMB
algorithm first sets P aG (A) = and, then, proceeds following two steps.
first step consists iterating following line P aG (A) change:
Take node B P (A) \ P aG (A) 6 G B|P aG (A) add P aG (A).
second step consists iterating following line P aG (A)
change: Take node B P aG (A) considered
G B|P aG (A)\{B}, remove P aG (A). first step IAMB algorithm
add O(n) nodes P aG (A). addition implies evaluating O(n) candidates
addition, since P (A) O(n) nodes. Evaluating candidate implies checking
separation statement G, takes O(a) time (Geiger et al., 1990, p. 530). Then,
first step IAMB algorithm runs O(an2 ) time. Similarly, second step
IAMB algorithm runs O(an) time. Therefore, IAMB algorithm runs O(an2 ) time.
Since IAMB algorithm run n nodes, overall runtime
solution O(an3 ).
analyze efficiency Methods B. exact, analyze
Methods A2 B2 (recall Figure 3) rather original Methods B (recall
Figure 1), former efficient latter. Methods A2 B2 run
O(n3 ) time. First, note Construct runs O(n3 ) time. algorithm iterates n
times lines 3-10 and, iterations, iterates O(n) times lines
5-8. Moreover, line 3 takes O(n2 ) time, line 6 takes O(1) time, line 9 takes O(n) time.
Now, note Methods A2 B2 iterate n times lines 2-8 and,
iterations, iterate O(n) times lines 3-7. Moreover, line 4 takes O(1) time,
line 5 takes O(n) time covering arc implies updating adjacency matrix
accordingly. Consequently, Methods B efficient solution
problem think of.
Finally, analyze complexity Method G2H. Method G2H runs O(n3 ) time:
constructed O(n3 ) time calling Construct (H, ) node
ordering, running Method B2 takes O(n3 ) time, adding G arcs H
G done O(n2 ) time. Recall Method G2H alternative
algorithm work Chickering (2002). Unfortunately, implementation details
provided work Chickering and, thus, comparison runtime
algorithm possible. However, believe algorithm efficient.
681

fiPena

8. Discussion
paper, studied problem combining several given DAGs consensus
DAG represents independences given DAGs agree upon
parameters associated possible. Although definition consensus DAG reasonable,
would leave number parameters associated focus solely
independencies represented consensus DAG. words, would define
consensus DAG DAG represents independences given DAGs
agree upon many possible. currently investigating whether
definitions equivalent. paper, proven may exist several nonequivalent consensus DAGs. principle, equally good. able
conclude one represents independencies rest, would prefer
one. paper, proven finding consensus DAG NP-hard.
made us resort heuristics find approximated consensus DAG. mean
discard existence fast super-polynomial algorithms general case,
polynomial algorithms constrained cases given DAGs bounded
in-degree. question currently investigating. paper,
considered heuristic originally proposed Matzkevich Abramson (1992, 1993b).
heuristic takes input node ordering, shown finding best
node ordering heuristic NP-hard. currently investigating application
meta-heuristics space node orderings find good node ordering
heuristic. preliminary experiments indicate approach highly beneficial,
best node ordering almost never coincides node orderings
consistent given DAGs.
said Section 1, aim combining BNs provided multiple experts (or
learning algorithms) single consensus BN robust individual
BNs. paper, proposed combine experts BNs two steps avoid
problems discussed Pennock Wellman (1999). First, finding consensus BN
structure and, then, finding consensus parameters consensus BN structure.
paper focused first step. currently working second
step along following lines. Let (G1 , 1 ), . . . , (Gm , ) denote BNs provided
experts. first element pair denotes BN structure whereas second denotes
BN parameters. Let p1 , . . . , pm denote probability distributions represented
BNs provided experts. Then, call p0 = f (p1 , . . . , pm ) consensus probability
distribution, f combination function, e.g. weighted arithmetic geometric
mean. Let G denote consensus BN structure obtained G1 , . . . , Gm described
paper. propose obtain consensus BN parameterizing G
p (A|P aG (A)) = p0 (A|P aG (A)) V, p probability distribution
represented consensus BN. motivation parameterization minimizes
Kullback-Leibler divergence p p0 (Koller & Friedman, 2009, Thm. 8.7).
hints speed computation parameterization performing
inference experts BNs found work Pennock Wellman (1999,
Properties 3 4, Section 5). Alternatively, one could first sample p0 and, then,
parameterize G p (A|P aG (A)) = p0 (A|P aG (A)) V, p0
empirical probability distribution obtained sample. Again, motivation
682

fiFinding Consensus Bayesian Network Structures

parameterization minimizes Kullback-Leibler divergence p p0
(Koller & Friedman, 2009, Thm. 17.1) and, course, p0 p0 sample sufficiently
large. Note use p0 parameterize G construct G which, discussed
Section 1, allows us avoid problems discussed Pennock Wellman (1999).
Finally, note present work combines DAGs G1 , . . . , Gm although
guarantee Gi MDI map I(pi ), i.e. Gi may superfluous arcs.
Therefore, one may want check Gi contains superfluous arcs remove
combination takes place. general, several MDI maps I(pi ) may exist,
may differ number parameters associated them. would interesting
study number parameters associated MDI map I(pi ) chosen affects
number parameters associated consensus DAG obtained method
proposed paper.

Acknowledgments
thank anonymous referees editor thorough review manuscript.
thank Dr. Jens D. Nielsen Dag Sonntag proof-reading manuscript.
work funded Center Industrial Information Technology (CENIIT) socalled career contract Linkoping University.

Appendix A. Proofs Lemmas 1-3
Lemma 1. Method A(G, ) Method A2(G, ) return DAG.

Proof. evident Methods A2 transform and, thus, halt
point. prove return DAG. prove result
induction number times Method executes line 6 halting.
evident result holds number executions one, Methods A2
share line 1. Assume induction hypothesis result holds k 1 executions.
prove k executions. Let Z denote nodes involved first
k executions. Since induction hypothesis applies remaining k 1 executions,
run Method summarized

Z G cover reverse Z G
Interchange Z
= 1 n
Percolate right-to-left leftmost node percolated

n number nodes G. Now, assume percolated = j. Note
first j 1 percolations involve nodes left . Thus, run
equivalent
683

fiPena

= 1 j 1
Percolate right-to-left leftmost node percolated
Z G cover reverse Z G
Interchange Z
Percolate right-to-left
Percolate Z right-to-left
= j + 2 n
Percolate right-to-left leftmost node percolated before.
Now, let W denote nodes left Z first k executions
line 6. Note fact Z nodes involved first execution implies
nodes W left Z . Note that, Z percolated
latter run above, nodes left Z exactly W {Y }. Since
nodes W {Y } left Z , percolation Z latter run
perform arc covering reversal node interchange. Thus, latter run
equivalent
= 1 j 1
Percolate right-to-left leftmost node percolated
Percolate Z right-to-left
Percolate right-to-left
= j + 2 n
Percolate right-to-left leftmost node percolated
exactly run Method A2. Consequently, Methods A2 return
DAG.

Lemma 2. Method A2(G, ) Method B(G, ) return DAG.
Proof. prove lemma much way Lemma 1. simply need
replace Z vice versa proof Lemma 1.

Lemma 3. Method B(G, ) Method B2(G, ) return DAG.
Proof. evident Methods B B2 transform and, thus, halt
point. prove return DAG. prove result
induction number times Method B executes line 6 halting.
evident result holds number executions one, Methods B B2
share line 1. Assume induction hypothesis result holds k 1 executions.
prove k executions. Let Z denote nodes involved first
k executions. Since induction hypothesis applies remaining k 1 executions,
run Method B summarized
684

fiFinding Consensus Bayesian Network Structures

Z G cover reverse Z G
Interchange Z
= 1 n
Percolate left-to-right rightmost node percolated
n number nodes G. Now, assume j-th rightmost node
. Note that, 1 < j, i-th rightmost node Wi right
Wi percolated run above. see it, assume contrary Wi
left . implies Wi left Z , Z
neighbors . However, contradiction Wi would selected
line 2 instead first execution line 6. Thus, first j 1 percolations
run involve nodes right Z . Then, run equivalent
= 1 j 1
Percolate left-to-right rightmost node percolated
Z G cover reverse Z G
Interchange Z
= j n
Percolate left-to-right rightmost node percolated
exactly run Method B2.

References
Chickering, D. M. Transformational Characterization Equivalent Bayesian Network
Structures. Proceedings Eleventh Conference Uncertainty Artificial Intelligence, 87-98, 1995.
Chickering, D. M. Optimal Structure Identification Greedy Search. Journal Machine
Learning Research, 3:507-554, 2002.
Chickering, D. M. & Meek, C. Finding Optimal Bayesian Networks. Proceedings
Eighteenth Conference Uncertainty Artificial Intelligence, 94-102, 2002.
Chickering, D. M., Heckerman, D. & Meek, C. Large-Sample Learning Bayesian Networks
NP-Hard. Journal Machine Learning Research, 5:1287-1330, 2004.
Friedman, N. & Koller, D. Bayesian Network Structure. Bayesian Approach
Structure Discovery Bayesian Networks. Machine Learning, 50:95-12, 2003.
Gavril, F. NP-Complete Problems Graphs. Proceedings Eleventh Conference Information Sciences Systems, 91-95, 1977.
Garey, M. & Johnson, D. Computers Intractability: Guide Theory NPCompleteness. W. H. Freeman, 1979.
685

fiPena

Geiger, D., Verma, T. & Pearl, J. Identifying Independence Bayesian Networks. Networks,
20:507-534, 1990.
Genest, C. & Zidek, J. V. Combining Probability Distributions: Critique Annotated Bibliography. Statistical Science, 1:114-148, 1986.
Hartemink, A. J., Gifford, D. K., Jaakkola, T. S. & Young, R. A. Combining Location
Expression Data Principled Discovery Genetic Regulatory Network Models.
Pacific Symposium Biocomputing 7, 437-449, 2002.
Jackson, B. N., Aluru, S. & Schnable, P. S. Consensus Genetic Maps: Graph Theoretic Approach. Proceedings 2005 IEEE Computational Systems Bioinformatics
Conference, 35-43, 2005.
Koller, D. & Friedman, N. Probabilistic Graphical Models: Principles Techniques. MIT
Press, 2009.
Matzkevich, I. & Abramson, B. Topological Fusion Bayes Nets. Proceedings
Eight Conference Conference Uncertainty Artificial Intelligence, 191-198, 1992.
Matzkevich, I. & Abramson, B. Complexity Considerations Combination
Belief Networks. Proceedings Ninth Conference Conference Uncertainty
Artificial Intelligence, 152-158, 1993a.
Matzkevich, I. & Abramson, B. Deriving Minimal I-Map Belief Network Relative
Target Ordering Nodes. Proceedings Ninth Conference Conference
Uncertainty Artificial Intelligence, 159-165, 1993b.
Maynard-Reid II, P. & Chajewska, U. Agregating Learned Probabilistic Beliefs. Proceedings Seventeenth Conference Uncertainty Artificial Intelligence, 354-361,
2001.
Meek, C. Graphical Models: Selecting Causal Statistical Models. PhD thesis, Carnegie
Mellon Unversity, 1997.
Ng, K.-C. & Abramson, B. Probabilistic Multi-Knowledge-Base Systems. Journal Applied
Intelligence, 4:219-236, 1994.
Nielsen, J. D., Kocka, T. & Pena, J. M. Local Optima Learning Bayesian Networks.
Proceedings Nineteenth Conference Uncertainty Artificial Intelligence,
435-442, 2003.
Nielsen, S. H. & Parsons, S. Application Formal Argumentation: Fusing Bayesian
Networks Multi-Agent Systems. Artificial Intelligence 171:754-775, 2007.
Pearl, J. Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference.
Morgan Kaufmann, 1988.
Pennock, D. M. & Wellman, M. P. Graphical Representations Consensus Belief. Proceedings Fifteenth Conference Uncertainty Artificial Intelligence, 531-540,
1999.
686

fiFinding Consensus Bayesian Network Structures

Pena, J. M., Nilsson, R., Bjorkegren, J. & Tegner, J. Towards Scalable Data Efficient
Learning Markov Boundaries. International Journal Approximate Reasoning, 45:211232, 2007.
Pena, J. M., Kocka, T. & Nielsen, J. D. Featuring Multiple Local Optima Assist User
Interpretation Induced Bayesian Network Models. Proceedings Tenth
International Conference Information Processing Management Uncertainty
Knowledge-Based Systems, 1683-1690, 2004.
Richardson, M. & Domingos, P. Learning Knowledge Multiple Experts. Proceedings Twentieth International Conference Machine Learning, 624-631, 2003.
del Sagrado, J. & Moral, S. Qualitative Combination Bayesian Networks. International
Journal Intelligent Systems, 18:237-249, 2003.
Studeny, M. Bayesian Networks Point View Chain Graphs. Proceedings
Fourteenth Conference Conference Uncertainty Artificial Intelligence, 496-503,
1998.
Studeny, M. & Bouckaert, R. R. Chain Graph Models Description Conditional
Independence Structures. Annals Statistics, 26:1434-1495, 1998.

687


