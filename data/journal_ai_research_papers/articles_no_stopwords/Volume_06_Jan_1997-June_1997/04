Journal Artificial Intelligence Research 6 (1997) 147-176

Submitted 6/96; published 5/97

Query DAGs: Practical Paradigm Implementing
Belief-Network Inference
Adnan Darwiche

darwiche@aub.edu.lb

Department Mathematics
American University Beirut
PO Box 11 - 236, Beirut, Lebanon

Gregory Provan

provan@risc.rockwell.com

Rockwell Science Center
1049 Camino Dos Rios
Thousand Oaks, CA 91360

Abstract

describe new paradigm implementing inference belief networks, consists two steps: (1) compiling belief network arithmetic expression called
Query DAG (Q-DAG); (2) answering queries using simple evaluation algorithm.
node Q-DAG represents numeric operation, number, symbol evidence. leaf node Q-DAG represents answer network query, is,
probability event interest. appears Q-DAGs generated using standard algorithms exact inference belief networks | show
generated using clustering conditioning algorithms. time space
complexity Q-DAG generation algorithm worse time complexity
inference algorithm based. complexity Q-DAG evaluation algorithm
linear size Q-DAG, inference amounts standard evaluation
arithmetic expression represents. intended value Q-DAGs reducing
software hardware resources required utilize belief networks on-line, real-world
applications. proposed framework facilitates development on-line inference
different software hardware platforms due simplicity Q-DAG evaluation
algorithm. Interestingly enough, Q-DAGs found serve purposes: simple techniques reducing Q-DAGs tend subsume relatively complex optimization techniques
belief-network inference, network-pruning computation-caching.

1. Introduction
Consider designing car self-diagnostic system alert driver range
problems. Figure 1 shows simplistic belief network could provide ranked set
diagnoses car troubleshooting, given input sensors hooked battery,
alternator, fuel-tank oil-system.
standard approach building diagnostic system put belief network,
along inference code, onto car's computer; see Figure 2. encountered
number diculties using approach embody belief network technology industrial applications. First, asked provide technology multiple platforms.
applications, technology implemented ADA pass certain certification procedures. others, implemented domain-specific hardware
supports primitive programming languages. Second, memory limited keep

c 1997 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiDarwiche & Provan

fuel sensor
fuel
battery

oil-pressure
battery sensor

fault

oil-pressure
sensor
alternator
alternator
sensor

Figure 1: simple belief network car diagnosis.
cost unit certain threshold maintain product profitability. dilemma
following: belief network algorithms trivial implement, especially optimization crucial, porting algorithms multiple platforms languages would
prohibitively expensive, time-consuming demanding qualified manpower.
overcome diculties, devised exible approach implementing
belief network systems, based following observation. Almost work
performed standard algorithms belief networks independent specific evidence
gathered variables. example, run algorithm battery-sensor set
low run later variable set dead, find almost algorithmic
difference two runs. is, algorithm branch differently
key decisions makes, difference two runs specific
arguments invoked numeric operations. Therefore, one apply standard inference
algorithm network evidence parameter instead specific value.
result returned algorithm arithmetic expression parameters
depend specific evidence. parameterized expression call Query
DAG, example shown Figure 4.1
approach proposing consists two steps. First, given belief network, set
variables evidence may collected (evidence variables), set variables need compute probability distributions (query variables), Q-DAG
compiled off-line, shown Figure 3. compilation typically done sophisticated software/hardware platform, using traditional belief network inference algorithm
conjunction Q-DAG compilation method. part process far away
costly computationally. Second, on-line system composed generated
Q-DAG evaluator specific given platform used evaluate Q-DAG. Given
evidence, parameterized arithmetic expression evaluated straightforward manner
using simple arithmetic operations rather complicated belief network inference.
1. sharing subexpressions makes Directed Acyclic Graph instead tree.

148

fiA Practical Paradigm Implementing Belief-Network Inference

Traditional Approach

Compiled Approach

Fault
Variables
Causal
Network


N
L

N
E

Sensor
Values

Causal Network
Inference
Software

Sensor
Variables

Q-DAG
Compiler

Query
DAG

Q-DAG
Evaluator


F
F
L

N
E

0
N
L

N
E

Fault
Probabilities

Figure 2: figure compares traditional approach exact belief-network inference
(shown left) new compiled approach (shown right)
context diagnostic reasoning. traditional approach, belief network
sensor values used on-line compute probability distributions
fault variables; compiled approach, belief network, fault variables
sensor variables compiled off-line produce Q-DAG, evaluated
on-line using sensor values compute required distributions.
computational work needed perform on-line evaluation straightforward
lends easy implementations different software hardware platforms.
approach shares commonality methods symbolically manipulate probability expressions, SPI (Li & D'Ambrosio, 1994; Shachter, D'Ambrosio, &
del Favero, 1990); differs SPI objective manipulations and, hence,
results obtained. SPI explicates notion arithmetic expression state
belief-network inference viewed expression-factoring operation. allows
results optimization theory utilized belief-network inference.
hand, define arithmetic expression explicate formalize boundaries
on-line off-line inference, goal identifying minimal piece software
required on-line. results therefore oriented towards purpose include:
(a) formal definition Q-DAG evaluator; (b) method generating Q-DAGs
using standard inference algorithms | algorithm need subscribe inference-as149

fiDarwiche & Provan

Query Variables

Evidence Variables

Causal Network

Q-DAG Compiler
Off-line

On-line
Query DAG

Evidence

Q-DAG Evaluator

Figure 3: proposed framework implementing belief-network inference.

(a)

(b)

Pr(A=ON) = .3

Pr(B=OFF, c)

Pr(B=ON, c)



+

+

.56

.075

+

Pr(B=ON|a)

.14

.225

B

C



*

*

*

*



*

Pr(C=ON|a)



.25



.9



.8



.5

.9

*
.1

(C,ON)

+
*

*
.5

(C,OFF)

Figure 4: belief network (a); corresponding Query-DAG (b). Here, C evidence
variable, interested probability variable B .
factoring view used Q-DAG generation; (c) computational guarantees
size Q-DAGs terms computational guarantees inference algorithm used
generate them. Although SPI framework positioned formulate related results,
pursued direction.
important stress following properties proposed approach. First, declaring evidence variable compilation process mean evidence must
collected variable on-line|this important evidence values, e.g.,
sensors, may lost practice|it means evidence may collected. Therefore, one declare variables evidence one wishes. Second, variable
declared evidence query. allows one perform value-of-information
150

fiA Practical Paradigm Implementing Belief-Network Inference

computations decide whether worth collecting evidence specific variable.
Third, space complexity Q-DAG terms number evidence variables
worse time complexity underlying inference algorithm; therefore,
simple enumerate-all-possible-cases approach. Finally, time space complexity
generating Q-DAG worse time complexity standard belief-network
algorithm used generation. Therefore, network solved using standard
inference algorithm, time complexity algorithm worse space
complexity,2 construct Q-DAG network.
following section explains concept Q-DAG concrete example
provides formal definitions. Section 3 dedicated generation Q-DAGs
computational complexity, showing standard belief-network inference algorithm
used compile Q-DAG long meets general conditions. Section 4
discusses reduction Q-DAG generated, showing reduction
subsumes key optimizations typically implemented belief network algorithms.
Section 5 contains detailed example application framework diagnostic
reasoning. Finally, Section 6 closes concluding remarks.

2. Query DAGs

section starts treatment Q-DAGs concrete example. consider
particular belief network, define set queries interest, show Q-DAG
used answer queries. discuss Q-DAG generated;
used. allow concrete introduction Q-DAGs help us
ground formal definitions follow.
belief network consider one Figure 4(a). class queries
interested Pr (B j C ), is, probability variable B takes value
given known (or unknown) value C . Figure 4(b) depicts Q-DAG answering
queries, essentially parameterized arithmetic expression values
parameters depend evidence obtained. Q-DAG actually answer queries
form Pr (B; C ), use normalization compute Pr (B j C ).
First, number observations Q-DAG Figure 4(b):
Q-DAG two leaf nodes labeled Pr (B=ON ; c) Pr (B=OFF ; c).
called query nodes values represent answers queries Pr (B=ON ; c)
Pr (B=OFF ; c).
Q-DAG two root nodes labeled (C; ) (C; ). called
Evidence Specific Nodes (ESNs) since values depend evidence collected
variable C on-line.
According semantics Q-DAGs, value node (V; v ) 1 variable V
observed v unknown, 0 otherwise. values ESNs determined,
evaluate remaining nodes Q-DAG using numeric multiplication addition.
numbers get assigned query nodes result evaluation answers
queries represented nodes.
2. Algorithms based join trees property.

151

fiDarwiche & Provan

.2725

.2875

.0925

Pr(B=OFF, c)

Pr(B=ON, c)

Pr(B=OFF, c)

.3475

Pr(B=ON, c)

+

+
.28

.2025

.07

*

*

*

*

.9

.0075

.28

*

*

.5

.1

+

+

.5

+

.9

0

.5

0

0

.1

*

*

*

*

*

*

(C,ON)

.5

.9

.1

0
(C,OFF)

0

(b)

.14

.225

+

.1

*

*

.56

.075

.07

.0225

.9

1

(a)

.14

.225

.56

.075

+

+

.0675

(C,ON)

0

.5

*

*
.5
1
(C,OFF)

Figure 5: Evaluating Q-DAG Figure 4 respect two pieces evidence: (a)
C=ON (b) C=OFF .
example, suppose evidence C = . ESN (C; )
evaluated 1 ESN (C; ) evaluated 0. Q-DAG Figure 4(b)
evaluated given Figure 5(a), thus leading
Pr (B=ON ; C=ON ) = :3475;



Pr (B=OFF ; C=ON ) = :2725;
conclude Pr (C = ) = :62. compute conditional
probabilities Pr (B=ON j C=ON ) Pr (B=OFF j C=ON ) using:
Pr (B=ON j C=ON ) = Pr (B=ON ; C=ON )=Pr (C=ON );
Pr (B=OFF j C=ON ) = Pr (B=OFF ; C=ON )=Pr (C=ON ):
evidence C=OFF , however, (C; ) evaluates 0 (C; )
evaluates 1. Q-DAG Figure 4(b) evaluated given Figure 5(b),
thus leading
Pr (B=ON ; C=OFF ) = :2875;

Pr (B=OFF ; C=OFF ) = :0925:
use following notation denoting variables values. Variables
denoted using uppercase letters, A; B; C , variable values denoted
lowercase letters, a; b; c. Sets variables denoted boldface uppercase letters,
A; B; C, instantiations denoted boldface lowercase letters,
a; b; c. use E denote set variables evidence. Therefore,
152

fiA Practical Paradigm Implementing Belief-Network Inference

use e denote instantiation variables represents evidence. Finally,
family variable set containing variable parents directed acyclic
graph.
Following formal definition Q-DAG.

Definition 1 Q-DAG tuple (V ; ; ; D; Z )
1. V distinguished set symbols (called evidence variables)
2. symbol (called unknown value)
3. maps variable V set symbols (called variable values) different
.
4. directed acyclic graph
- non-root node labeled either +
- root node labeled either
- number [0; 1]
- pair (V; v ) V evidence variable v value

5. Z distinguished set nodes (called query nodes)

Evidence variables V correspond network variables expect collect
evidence on-line. example, Figure 5, C evidence variable. one
variables set possible values captured function . example,
Figure 5, evidence variable C values . special value used
value variable known. example, may sensor variable
values \low," \medium," \high," lose sensor value on-line reasoning.
case, set sensor value .3 Query nodes representing answers
user queries. example, Figure 5, B query variable, leads query nodes
Pr(B=ON ; c) Pr(B=OFF ; c).
important notion evidence:

Definition 2 given Q-DAG (V ; ; ; D; Z ), evidence defined function E
maps variable V V set values (V ) [ fg.
variable V mapped v 2 (V ), evidence tells us V instantiated
value v . V mapped , evidence tell us anything value
V .
state formally evaluate Q-DAG given evidence. first
need notation:
1. Numeric-Node: n(p) denotes node labeled number p 2 [0; 1];
2. ESN: n(V; v ) denotes node labeled (V; v );
3. useful cases variable measured value information justifies
that.

153

fiDarwiche & Provan

3. Operation-Node: n1
: : :
ni denotes node labeled parents
n1 ; : : :; ni ;
4. Operation-Node: n1 : : : ni denotes node labeled + parents
n1 ; : : :; ni .
following definition tells us evaluate Q-DAG evaluating nodes.
recursive definition according value assigned node function
values assigned parents. first two cases boundary conditions, assigning
values root nodes. last two cases recursive ones.
Definition 3 Q-DAG (V ; ; ; D; Z ) evidence E , node evaluator defined
function maps node number [0; 1] that:
1. [n(p)] = p
(The value node labeled number number itself.)
(
E (V ) = v E (V ) = ;
2. [n(V; v )] = 10;; ifotherwise
(The value evidence-specific node depends available evidence: 1 v
consistent evidence 0 otherwise.)
3. [n1
: : :
ni ] = (n1) : : : (ni )
(The value node labeled product values parent nodes.)
4. [n1 : : : ni ] = (n1) + : : : + (ni )
(The value node labeled + sum values parent nodes.)
One typically interested values nodes Q-DAG since
nodes represent intermediate results interest user. query nodes
Q-DAG represent answers user queries values nodes one
seeks constructing Q-DAG. values queries captured notion
Q-DAG output.
Definition 4 node evaluator extended Q-DAGs follows:
((V ; ; ; D; Z )) = f(n; (n)) j n 2 Zg:
set ((V ; ; ; D; Z )) called Q-DAG output.
output one seeks Q-DAG. element output represents
probabilistic query answer.
Let us consider evaluations Q-DAG shown Figure 4, shown
Figure 5. Given evidence E (C )= , assuming Qnode(B = ) Qnode(B =
) stand Q-DAG nodes labeled Pr (B=ON ; c) Pr (B=OFF ; c), respectively,

[n(C; )] = 1;
[n(C; )] = 0;
[Qnode(B=ON )] = :075 (:9 1 + :1 0) + :56 (1 :5 + :5 0) = :3475;
[Qnode(B=OFF )] = (:9 1 + :1 0) :225 + (1 :5 + :5 0) :14 = :2725;
154

fiA Practical Paradigm Implementing Belief-Network Inference

meaning Pr (B=ON ; C=ON ) = :3475 Pr (B=OFF ; C=ON ) = :2725. instead
evidence E (C )=OFF , set analogous computations done.
possible evidence tells us nothing value variable C , is,
E (C ) = . case, would

[n(C; )] = 1;
[n(C; )] = 1;
[Qnode(B=ON )] = :075 (:9 1 + :1 1) + :56 (1 :5 + :5 1) = :635;
[Qnode(B=OFF )] = (:9 1 + :1 1) :225 + (1 :5 + :5 1) :14 = :365;
meaning Pr (B=ON ) = :635 Pr (B=OFF ) = :365.

2.1 Implementing Q-DAG Evaluator

Q-DAG evaluator implemented using event-driven, forward propagation scheme.
Whenever value Q-DAG node changes, one updates value children,
on, possible update values possible. Another way implement evaluator
using backward propagation scheme one starts query node updates
value updating values parent nodes. specifics application
typically determine method (or combination) appropriate.
important stress level refinement enjoyed Q-DAG propagation scheme implications eciency query updates. Propagation
Q-DAGs done arithmetic-operation level, contrasted propagation
message-operation level (used many standard algorithms). propagation schemes
typically optimized keeping validity ags messages invalid messages
recomputed new evidence arrives. clearly avoid unnecessary computations never avoid unnecessary computations message typically
coarse purpose. example, one entry message invalid,
whole message considered invalid. Recomputing message lead many unnecessary computations. problem avoided Q-DAG propagation since validity
ags attributed arithmetic operations, building blocks message operations. Therefore, necessary arithmetic operations recomputed Q-DAG
propagation scheme, leading detailed level optimization.
stress process evaluating updating Q-DAG done outside
probability theory belief network inference. makes development ecient online inference software accessible larger group people may lack strong backgrounds
areas.4

2.2 Availability Evidence

construction Q-DAG requires identification query evidence variables.
may give incorrect impression must know front variables observed
not. could problematic (1) applications one may lose sensor
reading, thus changing status variable observed unobserved;
4. fact, appears background compiler theory may relevant generating ecient
evaluator background belief network theory.

155

fiDarwiche & Provan



.3

.3

Pr(B=true|a)
.1
.8

true
false

Pr(B=true,b)

+

*

Pr(a)



B

Pr(A=true,b)


true

Pr(B=false,b)

+

Pr(A=false,b)

+

+

*

.1

*

.9

(B,true)

(B,false)

.8

*

.7

.2

Figure 6: belief network corresponding Q-DAG variable B declared
query evidence.
(2) applications variable may expensive observe, leading on-line
decision whether observe (using value-of-information computation).
situations dealt Q-DAG framework. First, mentioned
earlier, Q-DAGs allow us handle missing evidence use notation
denotes unknown value variable. Therefore, Q-DAGs handle missing sensor
readings. Second, variable declared query evidence. means
incorporate evidence variable available, compute
probability distribution variable case evidence available. Figure 6 depicts
Q-DAG variable declared query variable, variable B declared
evidence query variable (both variables true false values).
case, two ESNs variable B two query nodes (see Figure 6).
Q-DAG used two ways:
1. compute probability distributions variables B evidence
available B . situation, values n(B; true ) n(B; false )
set 1,
Pr (A = true ) = :3 :1 + :3 :9 = :3
Pr (A = false ) = :8 :7 + :7 :2 = :7
Pr (B = true ) = :3 :1 + :8 :7 = :59
156

fiA Practical Paradigm Implementing Belief-Network Inference

Pr (B = false ) = :3 :9 + :7 :2 = :41

2. compute probability variable evidence available B .
example, suppose observe B false . value n(B; true )
set 0 value n(B; false ) set 1,
Pr (A = true ; B = false ) = :3 :9 = :27
Pr (A = false ; B = false ) = :7 :2 = :14

ability declare variable evidence query variable seems
essential applications (1) decision may need made whether collect
evidence variable B ; (2) making decision requires knowing probability
distribution variable B . example, suppose using following formula
(Pearl, 1988, Page 313) compute utility observing variable B :
X
Utility Observing (B ) = Pr(B = bje) U (B = b);
b

U (B = b) utility decision maker finding variable B value b.
Suppose U (B = true ) = $2:5 U (B = false ) = ,$3. use Q-DAG
compute probability distribution B use evaluate Utility Observing (B ):
Utility Observing (B ) = ($2:5 :59) + (,$3 :41) = $0:24;

leads us observe variable B . Observing B , find value false .
accommodate evidence Q-DAG continue analysis.

3. Generating Query DAGs

section shows Q-DAGs generated using traditional algorithms exact
belief-network inference. particular, show Q-DAGs generated using
clustering (join tree, Jensen, LS) algorithm (Jensen, Lauritzen, & Olesen, 1990; Shachter,
Andersen, & Szolovits, 1994; Shenoy & Shafer, 1986), polytree algorithm, cutset
conditioning (Pearl, 1988; Peot & Shachter, 1991). outline properties must
satisfied belief network algorithms order adapt generating Q-DAGs
propose.

3.1 Clustering Algorithm

provide sketch clustering algorithm section. Readers interested
details referred (Shachter et al., 1994; Jensen et al., 1990; Shenoy & Shafer, 1986).
According clustering method, start by:
1. constructing join tree given belief network;5
5. join tree tree clusters satisfies following property: intersection two clusters
belongs clusters path connecting them.

157

fiDarwiche & Provan

2. assigning matrix variable belief network cluster contains
variable's family.
join tree secondary structure inference algorithm operates. need
following notation state algorithm:
- S1; : : :; Sn clusters, cluster corresponds set variables
original belief network.
- potential function cluster Si , mapping instantiations
variables Si real numbers.
- Pi posterior probability distribution cluster Si , mapping
instantiations variables Si real numbers.
- Mij message sent cluster Si cluster Sj , mapping instantiations variables Si \ Sj real numbers.
- e given evidence, is, instantiation evidence variables E.
assume standard multiplication marginalization operations potentials.
goal compute potential Pr (X; e) maps instantiation x
variable X belief network probability Pr (x; e). Given notation,
state algorithm follows:
Potential functions initialized using

= Pr X X ;
X



{ X variable whose matrix assigned cluster Si;
{ Pr X matrix variable X : mapping instantiations family
X conditional probabilities;
{ X likelihood vector variable X : X (x) 1 x consistent given

evidence e 0 otherwise.
Posterior distributions computed using

Pi =


k

Mki ;

Sk clusters adjacent cluster Si .
Messages computed using
X
Mij =
Mki ;
Si nSj

k6=j

Sk clusters adjacent cluster Si .
158

fiA Practical Paradigm Implementing Belief-Network Inference

potential Pr (X; e) computed using
Pr (X; e) =

X
Si nfX g

Pi ;

Si cluster X belongs.
equations used follows. compute probability variable, must
compute posterior distribution cluster containing variable. compute
posterior distribution cluster, collect messages neighboring clusters. message
cluster Si Sj computed collecting messages clusters adjacent Si
except Sj .
statement join tree algorithm appropriate situations evidence
changing frequently since involves computing initial potentials time evidence
changes. necessary general one provide optimized versions
algorithm. issue, however, irrelevant context generating Q-DAGs
updating probabilities face evidence changes take place Q-DAG level,
includes optimization technique discuss later.

3.2 Generating Q-DAGs

generate Q-DAGs using clustering method, go two steps. First,
modify initialization potential functions join tree quantified
using Q-DAG nodes instead numeric probabilities. Second, replace numeric
addition multiplication algorithm analogous functions operate Q-DAG
nodes. particular:
1. Numeric multiplication replaced operation
takes Q-DAG nodes
n1; : : :; ni arguments, constructs returns new node n label parents
n1; : : :; ni .
2. Numeric addition + replaced operation takes Q-DAG nodes n1 ; : : :; ni
arguments, constructs returns new node n label + parents n1 ; : : :; ni.
Therefore, instead numeric operations, Q-DAG-node constructors. instead
returning number computation result, return Q-DAG node.
state Q-DAG clustering algorithm, realize evidence
e, instead set evidence variables E collect evidence.
Therefore, Q-DAG algorithm compute answer query Pr (x; e), instead
compute Q-DAG node evaluates Pr (x; e) instantiation e variables
E.
following equations, potentials mappings variable instantiations QDAG nodes (instead numbers). example, matrix variable X map
instantiation X 's family Q-DAG node n(p) instead mapping number
p. Q-DAG operations
extended operate new potentials
way + extended clustering algorithm.
new set equations is:
159

fiDarwiche & Provan

Potential functions initialized using


= n(Pr X )
n(E );
X

E



{ X variable whose matrix assigned cluster Si;
{ n(Pr X ) Q-DAG matrix X : mapping instantiations X 's family

Q-DAG nodes representing conditional probabilities;
{ E evidence variable whose matrix assigned cluster Si;
{ n(E ) Q-DAG likelihood vector variable E : n(E )(e) = n(E; e),
means node n(E )(e) evaluates 1 e consistent given evidence
0 otherwise.
Posterior distributions computed using

Pi = Mki ;
k

Sk clusters adjacent cluster Si .
Messages computed using

Mij =
Mki ;
Si nSj

k6=j

Sk clusters adjacent cluster Si .
Q-DAG nodes answering queries form Pr (x; e) computed using

Qnode(X ) =
Pi ;
Si nfX g

Si cluster X belongs.
Qnode(X ) potential maps instantiation x variable X Q-DAG
node Qnode(X )(x) evaluates Pr (x; e) given instantiation e variables E.
Hence, modifications made clustering algorithm (a) changing
initialization potential functions (b) replacing multiplication addition
Q-DAG constructors multiplication addition nodes.

3.3 Example

show proposed Q-DAG algorithm used generate Q-DAG
belief network Figure 4(a).
one evidence variable example, C . interested generating Q-DAG answering queries variable B , is, queries form Pr (b; e).
Figure 7(a) shows join tree belief network Figure 4(a), tables contain
potential functions needed probabilistic clustering algorithm. Figure 7(b) shows
160

fiA Practical Paradigm Implementing Belief-Network Inference

S1

AC




C=ON
.9
.5

2

(a)

AB


C=OFF
.1

S1

S2

AC


1

.5




B=ON
.25 * .3
.8 * .7

B=OFF
.75 * .3
.2 * .7

AB


C=OFF

C=ON



n(.9)

n(C,ON)

n(.1)

n(C,OFF)



n(.5)

n(C,ON)

n(.5)

n(C,OFF)


2

(b)

B=ON

S2

1

B=OFF



n(.075)

n(.225)



n(.56)

n(.14)

Figure 7: join tree quantified numbers (a), Q-DAG nodes (b).
join tree again, tables contain potential functions needed Q-DAG
clustering algorithm. Note tables filled Q-DAGs instead numbers.
apply Q-DAG algorithm. compute Q-DAG nodes evaluate
Pr (b; e), must compute posterior distribution P2 cluster S2 since
cluster variable B belongs. sum distribution variable
obtain want. compute distribution P2 must first compute message
M12 cluster S1 cluster S2 .
message M12 computed summing
potential function 1 cluster S1
possible values variable C , i.e., M12 = 1 ; leads to:
C

M12 (A=ON ) = [n(:9)
n(C; )] [n(:1)
n(C; )];
M12(A=OFF ) = [n(:5)
n(C; )] [n(:5)
n(C; )]:
posterior distribution cluster S2 , P2 , computed using P2 = 2
M12 ;

leads

P2 (A=ON ; B=ON ) = n(:075)
[[n(:9)
n(C; )] [n(:1)
n(C; )]]
P2(A=ON ; B=OFF ) = n(:225)
[[n(:9)
n(C; )] [n(:1)
n(C; )]]
P2(A=OFF ; B=ON ) = n(:56)
[[n(:5)
n(C; )] [n(:5)
n(C; )]]
P2(A=OFF ; B=OFF ) = n(:14)
[[n(:5)
n(C; )] [n(:5)
n(C; )]]:
Q-DAG node Qnode(b) answering queries
form Pr (b; e) computed
summing posterior P2 variable A, Qnode =
P2 ; leading
nfB g
Qnode(B=ON ) = [n(:075)
[[n(:9)
n(C; )] [n(:1)
n(C; )]]]
[n(:56)
[[n(:5)
n(C; )] [n(:5)
n(C; )]]]
Qnode(B=OFF ) = [n(:225)
[[n(:9)
n(C; )] [n(:1)
n(C; )]]]
[n(:14)
[[n(:5)
n(C; )] [n(:5)
n(C; )]]];
2

161

fiDarwiche & Provan

Q-DAG depicted Figure 4(b). Therefore, result applying algorithm
two Q-DAG nodes, one evaluate Pr (B = ; e) evaluate
Pr (B=OFF ; e) instantiation e evidence variables E.

3.4 Computational Complexity Q-DAG Generation

computational complexity algorithm generating Q-DAGs determined
computational complexity clustering algorithm. particular, proposed algorithm applies -operation precisely clustering algorithm applies additionoperation. Similarly, applies
-operation precisely clustering algorithm applies
multiplication-operation. Therefore, assume
take constant time,
algorithms time complexity.
application
ends adding new node Q-DAG.
way new node added Q-DAG. Moreover, number parents
added node equal number arguments corresponding arithmetic operation
invoked clustering algorithm. Therefore, space complexity Q-DAG
time complexity clustering algorithm.
particular, means space complexity Q-DAGs terms number
evidence variables time complexity clustering algorithm
terms. Moreover, evidence variable E add evidence-specific nodes
Q-DAG, number values variable E take. important
stress without complexity guarantee may hard distinguish
proposed approach brute-force approach builds big table containing possible
instantiations evidence variables together corresponding distributions query
variables.

3.5 Generation Algorithms

polytree algorithm special case clustering algorithm shown (Shachter
et al., 1994). Therefore, polytree algorithm modified suggested
compute Q-DAGs. means cutset conditioning easily modified
compute Q-DAGs: instantiation c cutset C, compute Q-DAG node
Pr (x; c; e) using polytree algorithm take -sum resulting nodes.
algorithms exact inference belief networks adapted generate QDAGs. general, algorithm must satisfy key condition adaptable computing
Q-DAGs suggested above. condition behavior algorithm
never depend specific evidence obtained, depend variables
evidence collected. is, whether variable E instantiated value v1
value v2 affect complexity algorithm. whether variable E
instantiated matter.
belief networks algorithms aware satisfy property. reason
seems notion probabilistic independence algorithms
based. Specifically, read topology belief network relation
(X; Z; Y), stating variables X independent given variables Z. is,
Pr (x; j z) = Pr (x j z)Pr (y j z)
162

fiA Practical Paradigm Implementing Belief-Network Inference

instantiations x; y; z variables. possible, however, hold
instantiations z specific ones. standard algorithms aware
take advantage instantiation{specific notion independence.6 Therefore,
cannot attach computational significance specific value variable
instantiated. property existing algorithms makes easily adaptable
generation Q-DAGs.

3.6 Soundness Q-DAG Clustering Algorithm

soundness proposed algorithm stated below. proof given Appendix A.

Theorem 1 Suppose Qnode(X ) Q-DAG potential generated Q-DAG clustering algorithm query variable X evidence variables E. Let e0 instantiation
variables E, let Q-DAG evidence E defined follows:
(
evidence e0 sets variable E value e;
E (E ) = e;; otherwise.


(Qnode(X )(x)) = Pr (x; e0):

is, theorem guarantees Q-DAG nodes generated algorithm
always evaluate corresponding probabilities partial full instantiation
evidence variables.

4. Reducing Query DAGs

section focused reducing Q-DAGs generated. main
motivation behind reduction twofold: faster evaluation Q-DAGs less space
store them. Interestingly enough, observed few, simple reduction techniques
tend certain cases subsume optimization techniques uential practical implementations belief-network inference. Therefore, reducing Q-DAGs
important practically.
section structured follows. First, start discussing four simple reduction
operations form rewrite rules. show examples reductions subsume two key optimization techniques known network-pruning computation-caching.

4.1 Reductions

goal Q-DAG reduction reduce size Q-DAG maintaining
arithmetic expression represents. describing equivalence arithmetic expressions,
define notion Q-DAG equivalence:
Definition 5 Two Q-DAGs equivalent iff set evidence-specific
nodes output possible Q-DAG evidence.
6. algorithms two{level binary networks (BN20 networks), versions SPI algorithm
take advantage independences.

163

fiDarwiche & Provan

Q



.

.

.
p

.

q
Q2

Q

p

.

+

Q1

*
Q1

Q3

*
Q2 Q 1

*

.

q

+

Q2

Q3

b) numeric
reduction

c) associative
merging

Q1

Q1
Q2

(a) Identity
elimination

Q3

Q3

d) commutative
merging

Figure 8: four main methods Q-DAG reduction.
Figure 8 shows four basic reduction operations experimented with:
1. Identity elimination: eliminates numeric node identity element child
operation node.
2. Numeric reduction: replaces operation node numeric node parents
numeric nodes.
3. Associative merging: eliminates operation node using operation associativity.
4. Commutative merging: eliminates operation node using operation commutativity.
rules applied successively different order applications
possible.
proven operations sound (Darwiche & Provan, 1995). Based
analysis network structure preliminary empirical results, observed
many factors govern effectives operations. degree reduction
operations, numeric reduction particular, reduce size Q-DAG depends
topology given belief network set evidence query variables.
example, root nodes evidence variables belief network, leaf nodes
query variables, numeric reduction lead little Q-DAG reduction.
focus numeric reduction, showing sometimes subsumes two optimization techniques uential belief network algorithms. optimizations, show examples unoptimized algorithm employs numeric reduction
yields Q-DAG optimized algorithm. major implication optimizations done uniformly Q-DAG level, freeing underlying belief network
algorithms implementational complications.
following examples assume applying polytree algorithm singlyconnected networks.
164

fiA Practical Paradigm Implementing Belief-Network Inference





P(a)



.6




B

.9
.5

.6



B



.8
.3

P(B=ON|a)
.9
.5




P(C=ON|b)

b

(a)

P(a)

P(B=ON|a)




C




(b)

Figure 9: simple belief network pruning (a) pruning (b). light-shaded
node, A, query node, dark-shaded node, B , evidence node.

P(A=ON,B=b)

P(A=ON,B=b)

*

*

+

(B,ON)

+

.8

.2

.1

+

(B,OFF)

.3

.6

*

*

*

*
.9

+

.6

.9

(B,ON)

(B,OFF)

.1

.7

(a) Original Q-DAG

(b) Reduced Q-DAG

Figure 10: Q-DAG (a) reduction (b).

4.2 Network Pruning
Pruning process deleting irrelevant parts belief network invoking inference. Consider network Figure 9(a) example, B evidence variable
query variable. One prune node C network, leading network
Figure 9(b). query form Pr (a j b) value respect either
network. clear working smaller network preferred. general,
pruning lead dramatic savings since reduce multiply-connected network
singly-connected one.
165

fiDarwiche & Provan

generate Q-DAG network Figure 9(a) using polytree algorithm,
obtain one Figure 10(a). Q-DAG corresponds following expression,
X
X
Pr (A=ON ; e) = Pr (A=ON ) B (b)Pr (b j A=ON ) Pr (c j b):
c

b

generate Q-DAG network Figure 9(b), however, obtain one
Figure 10(b) corresponds following expression,
X
Pr (A=ON ; e) = Pr (A=ON ) B (b)Pr (b j A=ON ):
b

expected, Q-DAG smaller Q-DAG Figure 10(a), contains subset
nodes Figure 10(a).
key observation, however, optimized Q-DAG Figure 10(b)
obtained unoptimized one Figure 10(a) using Q-DAG reduction. particular,
nodes enclosed dotted lines collapsed using numeric reduction single
node value 1. Identity elimination remove resulting node, leading
optimized Q-DAG Figure 10(b).
general observation, however, prunable nodes contribute identity elements computing answers queries. contributions appear Q-DAG nodes
evaluate identity elements instantiations evidence. nodes
easily detected collapsed identity elements using numeric reduction. Identity
elimination remove Q-DAG, leading effect network
pruning.7 Whether Q-DAG reduction replace possible pruning operations open
question outside scope paper.

4.3 Computation Caching

Caching computations another uential technique optimizing inference belief networks. consider example, suppose applying polytree algorithm
compute Pr (c; b) network Figure 11. Given evidence, say B =ON , algorithm
compute Pr (c; B = ) passing messages shown Figure 12. evidence
changes B=OFF , however, algorithm employing caching recompute message B (a) (which represents causal support B (Pearl, 1988)) since value
message depend evidence B .8 kind optimization typically
7. Note, however, Q-DAG reduction reduce computational complexity generating QDAG, although network pruning may. example, multiply{connected network may become singlyconnected pruning, thereby, reducing complexity generating Q-DAG. using Q-DAG
reduction, still generate Q-DAG working multiply-connected network.
8. seen considering following expression, evaluated incrementally polytree
algorithm message passes:
Pr (c; e) =

X
b

Pr (c j b) B (b)

|

X


Pr (b j a) Pr (a) :

{z

C (b)

| {z }
B
}
( )

clear subexpression corresponding message B (a) B independent
evidence B .

166

fiA Practical Paradigm Implementing Belief-Network Inference


B
C



Pr(a)



.6



Pr(B=ON|a)
.9
.5




Pr(C=ON|b)

b

.8
.3




Figure 11: simple belief network demonstrating relationship Q-DAG reduction computation caching. light-shaded node, C , query node,
dark-shaded node, B , evidence node.



(a)
B
B

(b)
C
C

Figure 12: Message passing C queried B observed.
implemented caching values messages keeping track messages
affected evidence.
Now, consider Q-DAG corresponding problem shown Figure 13(a).
nodes enclosed dotted lines correspond message B .9 nodes
evidence-specific nodes ancestor set and, therefore, never change values
due evidence changes. fact, numeric reduction replace one nodes
ancestors single node shown Figure 13(b).
general, numeric reduction applied Q-DAG, one guaranteed following:
(a) Q-DAG node represents message depend evidence, node
re-evaluated given evidence changes; (b) numeric reduction guarantee
P Pr (b a)Pr (a).
9. precisely, correspond expression


167

j

fiDarwiche & Provan

P(C=ON,B=b)

P(C=ON,B=b)

+

+

*

.8

*
+

*

*

(B,ON)

*

.3

*

.74
+

(B,OFF)

*

(B,OFF)

(B,ON)

.3

*

.8

.26

*

*

*

*

cached value
.9

.6

.5

.4
.1

.6 .5

.4

(a) Original Q-DAG

(b) Reduced Q-DAG

Figure 13: Q-DAG (a) reduction (b).
Q-DAG evaluation method since replace node ancestor set
single root node.10

4.4 Optimization Belief-Network Inference

Network pruning computation caching proven uential practical
implementations belief-network inference. fact, experience shown
optimizations typically make difference usable non-usable beliefnetwork system.
One problem optimizations, however, algorithm-specific implementations although based general principles (e.g., taking advantage network
topology). Another problem make elegant algorithms complicated hard
understand. Moreover, optimizations often hard define succinctly, hence
well documented within community.
contrast, belief{network inference optimized generating Q-DAGs using unoptimized inference algorithms, optimizing generated Q-DAG reduction techniques. shown examples earlier respect pruning
caching optimizations. However, whether alternate approach optimization always
feasible yet known. positive answer clearly provide algorithm{independent
10. Note Q-DAGs lead refined caching mechanism Q-DAG evaluator (1) caches value
Q-DAG node (2) updates cached values need (that is,
value parent node changes). refined mechanism allows caching values messages
depend evidence well.

168

fiA Practical Paradigm Implementing Belief-Network Inference

fuel sensor
fuel
battery

oil-pressure
battery sensor

fault

oil-pressure
sensor
alternator
alternator
sensor

Figure 14: simple belief network car diagnosis.
approach optimizing belief{network inference, practically important least
two reasons. First, Q-DAG reduction techniques seem much simpler understand
implement since deal graphically represented arithmetic expressions, without
invoke probability belief network theory. Second, reduction operations applicable Q-DAGs generated belief{network algorithm. Therefore, optimization
approach based Q-DAG reduction would systematic accessible bigger
class developers.

5. Diagnosis Example
section contains comprehensive example illustrating application Q-DAG
framework diagnostic reasoning.
Consider car troubleshooting example depicted Figure 14. simple case
want determine probability distribution fault node, given evidence four
sensors: battery-, alternator-, fuel- oil-sensors. sensor provides information
corresponding system. fault node defines five possible faults: normal, cloggedfuel-injector, dead-battery, short-circuit, broken-fuel-pump.
denote fault variable F , sensor variables E, want build
system compute probability Pr (f; e); fault f evidence e.
probabilities represent unnormalized probability distribution fault variable
given sensor readings. Q-DAG framework, realizing diagnostic system involves three
steps: Q-DAG generation, reduction, evaluation. first two steps accomplished
off-line, final step performed on-line. discuss one steps
detail.

5.1 Q-DAG Generation
first step generate Q-DAG. accomplished applying Q-DAG
clustering algorithm fault query variable sensors evidence vari169

fiDarwiche & Provan

P(F=normal,e)

P(F=normal)
.90

P(F=pump)
.05

fuel
(normal)

P(F=pump,e)

fuel
subtree

(pump)

battery
(normal)

battery
(pump)

alternator
(normal)

oil
alternator
(pump)

(normal)

oil
(pump)

structure-sharing

Figure 15: partial Q-DAG car example, displaying two five query nodes,
broken fuel pump normal. shaded regions portions Q-DAG
shared multiple query nodes; values nodes relevant
value one query node.
ables. resulting Q-DAG five query nodes, Qnode(F = normal ; e), Qnode(F =
clogged fuel injector ; e), Qnode(F = dead battery ; e), Qnode(F = short circuit ; e),
Qnode(F = broken fuel pump; e). node evaluates probability corresponding fault instantiation evidence. probabilities constitute differential
diagnosis tells us fault probable given certain sensor values.
Figure 15 shows stylized description Q-DAG restricted two five query
nodes, corresponding Pr (F = broken fuel pump ; e) Pr (F = normal ; e). Q-DAG
structure symmetric fault value sensor.
Given Q-DAG symmetric possible faults, clarity exposition
look subset needed evaluate node Pr (F = broken fuel pump ; e). Figure 16
shows stylized version Q-DAG produced node. Following observations Q-DAG. First, evidence-specific node every instantiation
sensor variables, corresponding forms sensor measurements possible. Second,
roots Q-DAG probabilities. Third, one five parents query node
Pr(F = broken fuel pump ; e) prior F = broken fuel pump , four
contributions four sensors. example, Figure 16 highlights (in dots)
part Q-DAG computing contribution battery sensor.

5.2 Q-DAG Reduction

generating Q-DAG, one proceeds reducing using graph rewrite rules. Figure 16
shows example reduction Q-DAG restricted one query node
simplicity. give idea kind reduction applied, consider
partial Q-DAG enclosed dots figure. Figure 17 compares reduced Q-DAG
unreduced one generated. Given goal generating Q-DAGs
(a) evaluated eciently possible (b) require minimal space store,
170

fiA Practical Paradigm Implementing Belief-Network Inference

KEY
F-S fuel-sensor
B-S battery-sensor
A-S alternator-sensor
O-S oil-sensor

P(F=pump,e)

*

+

P(F=pump)
.05

+

*

*

*

*

full)

+

*

*

*

*

.36 ESN(B-S, .64 ESN(A-S, .45 ESN(A-S, .55 ESN(O-S, .27 ESN(O-S,

.4 ESN(F-S, .6 ESN(B-S,

ESN(F-S,
empty)

+

dead)

charged)

not-OK)

OK)

low)

.73

normal)

Figure 16: partial Q-DAG car example.
+
*

*

(a) Reduced Q-DAG
ESN(B-S,

.36 ESN(B-S, .64

dead)

charged)

+

(b) Original Q-DAG

+

+

*
ESN(B-S,

ESN(B-S,

*

charged)

dead)

P(B-S=charged| P(B=charged|
F=pump,B=charged) F=pump)
.8

*

*

.6

ESN(B-S,

*

charged)

P(B-S=dead|
P(B-S=charged|
P(B=charged|
F=pump,B=dead)
F=pump,B=charged) F=pump)
.2

.6

.4

*
*
P(B=dead|
F=pump)
.4

ESN(B-S,
dead)
P(B-S=dead|
F=pump,B=dead)
.6

*
P(B=dead|
F=pump)
.4

Figure 17: Reduced unreduced Q-DAGs car diagnosis example.
important see, even simple example, Q-DAG reduction make big difference
size.
171

fiDarwiche & Provan

5.3 Q-DAG Evaluation

reduced Q-DAG, use compute answers diagnostic queries.
section presents examples evaluation respect generated Q-DAG.
Suppose obtain readings dead, normal, ok full battery, oil,
alternator fuel sensors, respectively. let us compute probability distribution
fault variable. obtained evidence formalized follows:
- E (battery sensor ) = dead ,
- E (oil sensor ) = normal ,
- E (alternator sensor ) = ok ,
- E (fuel sensor ) = full .
Evidence-specific nodes evaluated according Definition 3. example,

[n(battery sensor ; charged)] = 0;

[n(battery sensor ; dead )] = 1:
evaluation evidence-specific nodes shown pictorially Figure 18(a). Definition 3
used evaluate remaining nodes: values node's parents
known, value node determined. Figure 18(b) depicts results
evaluating nodes. result interest probability 0.00434 assigned
query node Pr (fault = broken fuel pump ; e).
Suppose evidence changed value fuel sensor empty instead
full. update probability assigned node Pr (fault = broken fuel pump ; e), brute
force method re-evaluate whole Q-DAG. However, forward propagation scheme
used implement node evaluator, four nodes need re-evaluated
Figure 18(b) (those enclosed circles) instead thirteen (the total number nodes).
stress point refined updating scheme, easy implement
framework, much harder achieve one attempts embed standard beliefnetwork algorithms based message passing.

6. Concluding Remarks

introduced new paradigm implementing belief-network inference oriented towards real-world, on-line applications. proposed framework utilizes knowledge
query evidence variables application compile belief network arithmetic expression called Query DAG (Q-DAG). node Q-DAG represents numeric
operation, number, symbol depends available evidence. leaf node
Q-DAG represents answer network query, is, probability event
interest. Inference Q-DAGs linear size amounts standard evaluation
arithmetic expressions represent.
important point stress work reported proposing
new algorithm belief-network inference. proposing paradigm
172

fiA Practical Paradigm Implementing Belief-Network Inference

Pr(F=pump,e)
(a) Evaluating ESNs

*

+

.05
Pr(F=pump)

+

*
0

*
1

.4

ESN(F-S,
empty)

+

*

*

1

.6

*

.36 0

ESN(B-S,
dead)

ESN(F-S,
full)

+

.64

ESN(B-S,
charged)

0

*

*

.45 1

ESN(A-S,
not-OK)

.55

ESN(A-S,
OK)

*
.27

0

ESN(O-S,
low)

1

ESN(O-S,
normal)

.73

ESN values

.0043362

Pr(F=pump,e)
(b) Propagating probabilities

*

0 *
0
ESN(F-S,
empty)

+ .36

+ .6

.05
Pr(F=pump)

* .36

.6 *
.4

1
ESN(F-S,
full)

.6

1
ESN(B-S,
dead)

+ .55

0*

.36 0
ESN(B-S,
charged)

*0
.64

0
ESN(A-S,
not-OK)

+ .73

.55*
.45 1
ESN(A-S,
OK)

* 0
.55

0

ESN(O-S,
low)

.73 *

.27

1

ESN(O-S,
normal)

.73

ESN values

Figure 18: Evaluating Q-DAG car diagnosis example given evidence sensors.
bar (a) indicates instantiation ESNs. shaded numbers
(b) indicate probability values computed node evaluator.
circled operations left-hand-side (b) ones need
updated evidence fuel-system sensor altered, denoted circled
ESNs.

173

fiDarwiche & Provan

implementing belief-network inference orthogonal standard inference algorithms
engineered meet demands real-world, on-line applications. class
applications typically demanding following reasons:
1. typically requires short response time, i.e., milliseconds.
2. requires software written specialized languages, ADA, C++,
assembly pass certification procedures.
3. imposes severe restrictions available software hardware resources order
keep cost \unit" (such electromechanical device) low possible.
address real-world constraints, proposing one compile belief network
Q-DAG shown Figure 3 use Q-DAG evaluator on-line reasoning.
brings required memory needed storing Q-DAG evaluator.
brings required software needed implementing Q-DAG evaluator,
simple seen earlier.
proposed approach still requires belief-network algorithm generate Q-DAG,
makes eciency algorithm less critical factor.11 example,
show standard optimizations belief-network inference, pruning
caching, become less critical Q-DAG framework since optimizations tend
subsumed simple Q-DAG reduction techniques, numeric reduction.
work reported paper extended least two ways. First, QDAG reduction techniques could explored, oriented towards reducing evaluation
time Q-DAGs, others towards minimizing memory needed store them. Second,
shown optimization techniques dramatically improve belief-network
algorithms may become irrelevant size Q-DAGs Q-DAG reduction employed.
investigation needed prove formal results guarantees effectiveness
Q-DAG reduction.
close section noting framework proposed applicable
order-of-magnitude (OMP) belief networks, multiplication addition get replaced
addition minimization, respectively (Goldszmidt, 1992; Darwiche & Goldszmidt,
1994). OMP Q-DAG evaluator, however, much ecient probabilistic
counterpart since one may evaluate minimization node without evaluate
parents many cases. make considerable difference performance Q-DAG
evaluator.

Acknowledgements
work paper carried first author Rockwell Science
Center. Special thanks Jack Breese, Bruce D'Ambrosio anonymous reviewers
useful comments earlier drafts paper.
11. shown clustering conditioning algorithms used Q-DAG generation,
algorithms SPI (Li & D'Ambrosio, 1994; Shachter et al., 1990) used well.

174

fiA Practical Paradigm Implementing Belief-Network Inference

Appendix A. Proof Theorem 1

Without loss generality, assume proof variables declared evidence
variables. prove soundness theorem, need show Q-DAG potential evaluate corresponding probabilistic potential possible evidence.
Formally, cluster variables X , matrices assigned ,
need show


( n(Pr X )
n(X )) = Pr X X
(1)
X

X

given evidence E . establish this, guaranteed Qnode(X )(x)
evaluate probability Pr (x; e) application
Q-DAG algorithm isomorphic application + probabilistic algorithm, respectively.
prove Equation 1, extend Q-DAG node evaluator mappings
standard way. is, f mapping instantiations Q-DAG nodes, (f )
defined follows:
(f )(x) =def (f (x)):
is, simply apply Q-DAG node evaluator range mapping f .
Note (f
g ) equal (f )ME (g ). Therefore,

( n(Pr X )
n(X ))
XY
=
(n(Pr X ))ME (n(X ))
X

=
Pr X (n(X )) definition n(Pr X ):
X

Note definition n(X ), n(X )(x) equals n(X; x). Therefore,
(n(X ))(x) = (n(X )(x))
=
( E (n(X; x))
1; E (X ) = x E (X ) =
=
0; otherwise
= X (x):
Therefore,


( n(Pr X )
n(X )) = Pr X X :
X

X

References

Darwiche, A., & Goldszmidt, M. (1994). relation kappa calculus probabilistic reasoning. Proceedings Tenth Conference Uncertainty Artificial
Intelligence (UAI), pp. 145{153.
Darwiche, A., & Provan, G. (1995). Query DAGs: practical paradigm implementing
on-line causal-network inference. Tech. rep. 95-86, Rockwell Science Center, Thousand
Oaks, CA.
175

fiDarwiche & Provan

Goldszmidt, M. (1992). Qualitative probabilities: normative framework commonsense
reasoning. Tech. rep. R-190, University California Los Angeles, Ph.D. thesis.
Jensen, F. V., Lauritzen, S., & Olesen, K. (1990). Bayesian updating recursive graphical
models local computation. Computational Statistics Quarterly, 4, 269{282.
Li, Z., & D'Ambrosio, B. (1994). Ecient Inference Bayes Networks Combinatorial
Optimization Problem. International Journal Approximate Reasoning, 11, 55{81.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference. Morgan Kaufmann Publishers, Inc., San Mateo, California.
Peot, M. A., & Shachter, R. D. (1991). Fusion propagation multiple observations
belief networks. Artificial Intelligence, 48 (3), 299{318.
Shachter, R., Andersen, S., & Szolovits, P. (1994). Global Conditioning Probabilistic
Inference Belief Networks. Proc. Tenth Conference Uncertainty AI, pp.
514{522 Seattle WA.
Shachter, R., D'Ambrosio, B., & del Favero, B. (1990). Symbolic Probabilistic Inference
Belief Networks. Proc. Conf. Uncertainty AI, pp. 126{131.
Shenoy, P. P., & Shafer, G. (1986). Propagating belief functions local computations.
IEEE Expert, 1 (3), 43{52.

176


