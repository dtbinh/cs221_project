journal artificial intelligence

submitted published

screen learning flat syntactic semantic spoken
language analysis artificial neural networks
stefan wermter
volker weber

wermter informatik uni hamburg de
weber informatik uni hamburg de

department computer science
university hamburg
hamburg germany

abstract
previous approaches analyzing spontaneously spoken language often
encoding syntactic semantic knowledge manually symbolically
progress statistical connectionist language many current
spoken language systems still use relatively brittle hand coded symbolic grammar
symbolic semantic component
contrast describe called screening learning robust processing
spontaneously spoken language screening analysis uses shallow sequences category representations analyzing utterance syntactic
semantic dialog levels rather deeply structured symbolic analysis
use connectionist analysis screening aims supporting speech
language processing data driven learning robustness connectionist
networks order test developed screen system
robust learned analysis
focus detailed description screen architecture
syntactic semantic analysis interaction speech recognizer detailed
evaluation analysis robustness uence noisy incomplete input
main representations allow robust processing
spontaneous spoken language deeply structured representations particular
fault tolerance learning capability connectionist networks support
analysis providing robust spoken language processing within overall
hybrid symbolic connectionist framework

introduction
recently fields speech processing well language processing seen
efforts examine possibility integrating speech language processing von hahn
pyka jurafsky et al b waibel et al ward menzel geutner
et al wermter et al large speech language corpora
developed rapidly techniques examined particularly support
properties speech language processing although quite
approaches spoken language analysis mellish young et al hauenstein
weber ward emphasized learning syntactic semantic
analysis spoken language hybrid connectionist architecture topic
c ai access foundation morgan kaufmann publishers rights reserved

fiwermter weber

goal screen however learning important reduction
knowledge acquisition automatic system adaptation increasing system
portability domains different previous approaches
demonstrate hybrid connectionist learning techniques used providing
robust analysis faulty spoken language
processing spoken language different processing written language successful techniques text processing may useful spoken language processing
processing spoken language less constrained contains errors less strict regularities written language errors occur levels spoken language processing
instance acoustic errors repetitions false starts repairs prominent spontaneously spoken language furthermore incorrectly analyzed words unforeseen grammatical semantic constructions occur often spoken language order deal
important real world language analysis robust processing necessary
therefore cannot expect existing techniques context free tree representations
proven work written language simply transferred spoken
language
instance consider speech recognizer produced correct german sentence hypothesis ich meine naturlich marz english translation mean course
march standard techniques text processing chart parsers context free
grammars may able produce deeply structured tree representations many correct
sentences shown figure
sentence
verb phrase

noun group

pronoun

verb group

verb

noun group

adverb

ich meine mean natrlich course

noun

mrz march

figure tree representation correctly recognized sentence
however currently speech recognizers still far perfect produce many word
errors possible rely perfect sentence hypothesis therefore incorrect
sometimes connectionist networks called artificial neural networks use
term connectionist networks term hybrid connectionist architecture refer
architecture emphasizes use connectionist networks rule use
symbolic representations higher levels might needed
symbolic connectionist robust enterprise natural language



fiscreen flat syntactic semantic spoken language analysis

variations ich meine ich marz mean march ich hatte ich marz
march ich ich meine marz mean march analyzed however
context free grammars single syntactic semantic category error may prevent
complete tree built standard top chart parsers may fail completely however suboptimal sentence hypotheses analyzed since sometimes sentence
hypotheses best possible output produced speech recognizer furthermore
lot content extracted even partially incorrect sentence hypotheses
instance march plausible agent said something
time march therefore robust analysis able analyze sentence
hypotheses ideally break input

screening flat representations support robustness
examples incorrect variations sentence hypotheses depth structured
syntactic semantic representation advantageous since arbitrary word order spontaneous errors make often impossible determine desired deep highly
structured representation furthermore deep highly structured representation may
many restrictions appropriate spontaneously spoken language however
maybe even important certain tasks necessary perform depth
analysis instance inferences story understanding require depth
understanding dyer tasks information extraction spoken language
need much depth analysis instance output parser used
translating speech recognizer sentence hypothesis eh ich meine eh ich marz eh
mean eh march may sucient extract agent uttered mean
time march contrast deeply structured representation screening
aims reaching robust representation spoken language screening
shallow analysis category sequences called representations
syntactic semantic levels
representation structures utterance u words w wn according
syntactic semantic properties words contexts e g according sequence
basic abstract syntactic categories instance phrase meeting london
described representation determiner noun preposition noun basic
syntactic level representation noun group noun group prepositional group
prepositional group abstract syntactic level similar representations used
semantic categories dialog act categories etc
kase
rubbish
noun

noun group
negation

ich

pronoun
animate
noun group
agent

meine
mean
verb
utter
verb group
action

naturlich
course
adverb
nil
special group
miscellaneous

marz
march
noun
time
noun group
time

figure utterance representation


fiwermter weber

figure gives example representation correct sentence hypothesis
kase ich meine naturlich marz rubbish mean course march first line shows
sentence second literal translation third line describes basic syntactic
category word fourth line shows basic semantic category last two lines
illustrate syntactic semantic categories phrase level
kase
rubbish
noun

noun group
negation

ich

pronoun
animate
noun group
agent

hatte

verb

verb group
action

ich

pronoun
animate
noun group
agent

marz
march
noun
time
noun group
time

figure utterance representation
figure gives example representation incorrect sentence hypothesis
kase ich hatte ich marz rubbish march parser spoken language
able process sentence hypotheses far possible use
representations support necessary robustness example analysis
least provide animate agent noun group made statement
specific time noun group march flat representations potential support
robustness better since minimal sequential structure even error
occurs whole representation still built contrast standard tree structured
representations many decisions made construct deeply structured
representation therefore possibilities make incorrect decisions
particular noisy spontaneously spoken language chose representations
rather highly structured representations desired robustness
mistakes speech language systems

flat representations learned hybrid connectionist framework

robust spoken language analysis representations could pursued different
approaches therefore want motivate use hybrid connectionist
uses connectionist networks far possible rule use symbolic
knowledge use connectionist networks
important due distributed fault tolerance connectionist networks support
robustness rumelhart et al sun connectionist networks number properties relevant spoken language analysis instance
connectionist networks well known learning generalization capabilities
learning capabilities allow induce regularities directly examples training
examples representative task noisy robust processing supported
inductive connectionist learning
furthermore hybrid connectionist architecture property different knowledge sources take advantage learning generalization capabilities connectionist networks hand knowledge task control knowledge


fiscreen flat syntactic semantic spoken language analysis

rules known represented directly symbolic representations since humans apparently symbolic inferencing real neural networks abstract
symbolic representations connectionist networks additional potential
shed light human language processing capabilities respect
differs candidates robust processing statistical taggers statistical
n grams statistical techniques used robust analysis charniak
statistical techniques n grams relate human cognitive language capabilities simple recurrent connectionist networks relationships human
cognitive language capabilities elman
screen hybrid connectionist system developed examination
syntactic semantic analysis spoken language earlier work explored
scanning understanding written texts wermter wermter lochel
wermter peters experience started completely project
screen explore learned fault tolerant analysis spontaneously spoken language
processing preliminary successful case studies transcripts developed
screen system knowledge generated speech recognizer previous work
gave brief summary screen specific focus segmentation parsing dialog
act processing wermter weber focus detailed description
screen architecture syntactic semantic analysis interaction
speech recognizer detailed evaluation analysis robustness uence
noisy incomplete input

organization claim
structured follows section provide detailed description
examples noise spoken language noise introduced human speaker
speech recognizer noise spoken language analysis motivates representations whose categories described section basic abstract categories
syntactic semantic level explained section section motivate
explain design screen architecture brief functional overview
overall architecture explain details individual modules connectionist
network level order demonstrate behavior analysis spoken language
provide detailed examples section several representative sentences
walk reader detailed step step analysis behavior system explained provide overall analysis screen system section
evaluate system individual networks compare performance simple recurrent networks statistical n gram techniques simple recurrent networks
performed better grams syntactic semantic prediction furthermore
provide overall system evaluation examine overall performance uence
additional noise supply transfer different second domain finally
compare approaches conclude representations
connectionist networks provide robust learned spoken language analysis
want point make argument deeply structured symbolic representations language processing general usually deeply
structured representation built course due additional knowledge con

fiwermter weber

tains potential powerful relationships interpretations greater
representation instance depth analysis required tasks making
detailed inferences reading text stories however screening
motivated noisy spoken language analysis noisy spoken language analysis
representations support robustness connectionist networks effective providing robustness due learned fault tolerance main contribution
demonstrate building evaluating computational hybrid
connectionist architecture screen robust learned processing

processing spoken language

goal learn process spontaneously spoken language syntactic semantic
level fault tolerant manner section give motivating examples spoken
language

noise spoken language

domain arrangement meetings business partners
currently use spoken dialog turns utterances domain one
turn consists one subsequent utterances speaker
utterances thousands utterance hypotheses generated processed
underlying speech recognizer german utterance examples domain
shown together literal english translation important note
english translations word word translations
kase ich meine naturlich marz
rubbish mean course march
der vierzehnte ist ein mittwoch richtig
fourteenth wednesday right
hm sechsten april bin ich leider auer hause
eh sixth april unfortunately home
ich dachte noch der nachsten woche auf jeden fall noch im april
thought still next week case still april
gut prima vielen dank dann ist das ja kein
good great many thanks yeah
oh das ist schlecht da habe ich um vierzehn uhr dreiig einen termin beim zahnarzt
oh bad fourteen clock thirty date dentist
ja genau allerdings habe ich da von neun bis vier uhr schon einen arzttermin
yes exactly however nine four clock already doctorappointment
see spoken language contains many performance phenomena among
exclamations rubbish see example interjections eh oh see examples


fiscreen flat syntactic semantic spoken language analysis

starts see example furthermore syntactic
semantic constraints spoken language less strict written text instance
word order spontaneously spoken language often different written language
therefore spoken language noisier written language even transcribed
sentences well known parsing strategies text processing rely
wellformedness criteria directly applicable analyzing spoken language

noise speech recognizer

want analyze spoken language computational model
noise introduced humans speaking noise introduced limitations speech recognizers typical speech recognizers produce many separated word
hypotheses different plausibilities time given speech signal word
hypotheses connected word hypothesis sequence evaluated
providing basis analysis typically word hypothesis consists four parts
pause


hm eh

ich



e



wie
e

htte





e

april april
e



ich




ich

leider unfortunately
e



e

sechsten sixth

e



e



bin
e





e

e

wenn
e

ich
e



e

leider unfortunately


e

auer


hause home

e



e



pause


e

recognized
e

figure simple word graph spoken utterance ahm sechsten april bin ich
leider auer hause eh sixth april unfortunately home
node represents word hypothesis arrow represents possible
subsequent word hypotheses word hypothesis shown word
string start time end time interval acoustic plausibility


fiwermter weber

start time seconds end time seconds word string hypothesis
plausibility hypothesis confidence speech recognizer simple word graph practice word graphs spontaneous speech
much longer leading comprehensive word hypothesis sequences however illustrating
properties speech input focus relatively short simple word graph
figure
word hypotheses overlap time constitute directed graph called word
graph node word graph represents one word hypothesis two hypotheses
graph generated word hypotheses connected end time first word
hypothesis directly start time second word hypothesis instance
word hypothesis ending hypothesis sechsten sixth
starting connected word hypothesis sequence
hm
eh
hm
eh
sec

ich





sechsten
sixth

april
april

bin


leider
unfort

auer


hause
home




sechsten
sixth

april
april

wenn ich ich leider
unfort

auer


hause
home

sec

ich


sec

figure two examples word hypothesis sequences word graph
example word graph simple however shown figure possible
word hypothesis sequence desired hm sechsten april bin ich leider
auer hause eh sixth april unfortunately home sequence
hm ich sechsten april wenn ich ich leider auer hause eh sixth april
unfortunately home consequently deal incorrectly recognized
words extraordinary order therefore syntactic semantic analysis
fault tolerant order process noisy word hypothesis sequences

flat category representation intermediate connecting
representation
section describe category representations first
categories syntactic analysis depict categories semantic
analysis
speech input form test word graphs taken called blaubeuren meeting
corpus particular word graphs used provided project partners general test
purposes verbmobil project particularly generated testing parsing strategies
therefore speech recognizer fine tuned produce relatively small word graphs relatively
high word accuracy vocabulary size hmm recognizer average number
hypotheses per word dialogs



fiscreen flat syntactic semantic spoken language analysis

categories flat syntactic analysis

flat syntactic analysis assignment syntactic categories sequence words e g
word hypothesis sequence generated speech recognizer flat representations
phrase group level support local structural decisions local structural decisions deal
phrase group abstract syntactic category word belongs
case local directly preceding words phrase group uence
current decision instance determiner could part prepositional group
mine part starting noun group old mine local structural
decisions depending local context made analysis
syntactic analysis developed level basic syntactic categories
abstract syntactic categories syntactic categories may vary depending language degree detail intended structural representation however
general rather independent specifically used categories fact
used syntactic categories two different domains railway counter interactions
business meeting arrangements basic syntactic categories used noun
verb preposition pronoun numeral past participle pause adjective adverb conjunction
determiner interjection shown abbreviations table
category
noun n
verb v
preposition r
pronoun u
numeral
participle p
pause

examples
date april
meet choose


fourteenth
taken
pause

category
adjective j
adverb
conjunction c
determiner
interjection


examples
late
often


eh oh
particles

table basic syntactic categories
abstract syntactic categories used verb group noun group adverbial group
prepositional group conjunction group modus group special group interjection group
abstract syntactic categories shown table
category
verb group vg
noun group ng
adverbial group ag
prepositional group pg
conjunction group cg
modus group mg
special group sg
interjection group ig

examples
mean would propose
date next possible slot
later early possible
dining hall

interrogatives confirmations long yes
additives politeness please
interjections pauses eh oh

table abstract syntactic categories


fiwermter weber

categories express main syntactic properties phrases
basic abstract syntactic categories widely used different parsers however
representations crucially rely specific set basic
abstract syntactic categories goal train learn generalize syntactic
analysis abstract syntactic categories basic syntactic categories local syntactic decisions made far possible local syntactic ambiguities
phrase group level abstract syntactic categories dealt global ambiguities prepositional phrase attachment dealt since need
additional knowledge e g semantics module complete syntax trees
certain preference might turn wrong semantic knowledge
syntactic representation goes far possible local syntactic knowledge
disambiguation

categories flat semantic analysis

since semantic analysis domain dependent semantic categories differ different
domains worked particularly two domains railway counter interactions called
regensburg train corpus business meeting arrangements called blaubeuren meeting
corpus overlap semantic categories train corpus
category
select sel
suggest sug
meet meet
utter utter


move move
aux aux
question quest
physical phys
animate anim
abstract abs

source src
destination dest
location loc
time time
negative evaluation
positive evaluation yes
nil nil

examples
select choose
propose suggest
meet join
say think


come go
would could
question words
physical objects building oce
animate objects
abstract objects date
time location state words prepositions
time location source words prepositions
time location destination words prepositions
hamburg pittsburgh
tomorrow clock april
bad
yes good
words without specific semantics e g determiner

table basic semantic categories
meeting corpus wermter weber b differences occurred mainly verbs
e g need events frequent railway counter interactions suggestevents frequent business meeting interactions semantic categories


fiscreen flat syntactic semantic spoken language analysis

category
action act
aux action aux
agent agent
object obj
recipient recip
instrument instr
manner manner
time tm
time tm frm
time tm
loc lc
loc lc frm
loc lc
confirmation conf
negation neg
question quest
misc misc

examples
action full verb events meet select
auxiliary action auxiliary events would
agent action
object action date
recipient action
instrument action elevator
achieve action without changing rooms
time morning
start time
end time pm
location frankfurt york
start location boston dortmund
end location hamburg
confirmation phrase ok great yes wonderful
negation phrase stop
question phrases time
miscellaneous words e g politeness please eh

table abstract semantic categories
railway counter interactions described previous work weber wermter
primarily focus semantic categories meeting corpus basic
semantic categories word shown table higher level abstraction
word belong abstract semantic category possible abstract semantic categories
shown table summary categories provide basis analysis
word represented syntactically semantically context four categories two
basic two abstract levels

architecture screen system

section want describe constraints principles important
system design outlined motivated introduction screening
robust learned analysis spoken language category sequences called
representations syntactic semantic levels order test screening
designed implemented hybrid connectionist screen system
processes spontaneously spoken language learned connectionist representations
summarize main requirements order motivate specific system design
explained subsequent subsections

general motivation architecture

consider learning extremely important spoken language analysis several
reasons learning reduces knowledge acquisition increases portability particularly
spoken language analysis underlying rules regularities dicult
formulate often reliable furthermore cases inductive learning may detect


fiwermter weber

unknown implicit regularities want use connectionist learning simple recurrent
networks rather forms learning e g decision trees primarily
inherent fault tolerance connectionist networks knowledge
sequence words categories learned simple recurrent networks
fault tolerance often occurring language errors ected system
design commonly occurring errors interjections pauses word repairs
phrase repairs however fault tolerance cannot go far try model class
occurring errors number potentially occurring errors unpredictable constructions far large screen want incorporate explicit fault tolerance
specific modules correction well implicit fault tolerance connectionist network techniques inherently fault tolerant due support similarity
processing fact even word completely unknown recurrent networks use
empty input may even assign correct category sucient previous context
flat representations motivated sections may support robust spokenlanguage analysis however connectionist representations provide full recursive power arbitrary syntactic semantic symbolic knowledge structures contrast
context free parsers representations provide better basis robust processing
automatic knowledge acquisition inductive learning however argued use potentially unrestricted recursion well known context free grammar
parsers provides computational model recursive power humans
order understand language order better support robustness want use
representations spontaneous language analysis
incremental processing speech syntax semantics dialog processing parallel
allows us start language analysis parallel speech recognizer finished
analysis incremental processing advantage providing analysis
early stage example syntactic semantic processing occur parallel
slightly behind speech processing analyzing spoken language speech
recognizer output want consider many competing paths word hypothesis sequences
parallel
respect hybrid representations examine hybrid connectionist architecture
connectionist networks useful want use symbolic processing wherever necessary symbolic processing useful complex control
large system hand learning robust analysis use feedforward
simple recurrent networks many modules try use rather homogeneous supervised
networks

overview architecture

screen parallel integrated hybrid architecture wermter

main properties

outside module difference communication symbolic
connectionist module previous hybrid architectures emphasized different
symbolic connectionist representations different representations screen
benefit common module interface outside connectionist symbolic


fiscreen flat syntactic semantic spoken language analysis

module communication identically realized symbolic lists contain values
connectionist units
previous hybrid symbolic connectionist architectures usually within
symbolic connectionist module hendler faisal kwasny
medsker screen global state described collection individual
symbolic connectionist modules processing parallel long one module
need input second module
communication among symbolic connectionist modules organized via
messages hybrid architectures often used activation
values symbolic structures used messages consisting lists symbols
associated activation plausibility values provide communication medium
supports connectionist processing well symbolic processing
give overview parts screen see figure
important output consists syntactic semantic category representations
input incrementally recognized parallel word hypotheses speech recognizer
generates many incorrect word hypotheses time even correctly recognized speech
contain many errors introduced humans representation used since
fault tolerant robust instance context free tree representation since
tree representation requires many decisions representation
module system instance disambiguation abstract syntactic categories contains connectionist network symbolic program integration symbolic
connectionist representations occurs encapsulation symbolic connectionist
processes module level connectionist networks embedded symbolic modules
communicate via messages
however essential parts needed purposes learning spokenlanguage analysis starting output individual word hypotheses
speech recognizer first need component receives incremental stream
individual parallel word hypotheses produces incremental stream word hypothesis sequences see figure call part speech sequence construction part
needed transforming parallel overlapping individual word hypotheses word hypothesis sequences word hypothesis sequences different quality goal
work best word hypothesis sequences therefore need speech
evaluation part combine speech related plausibilities syntactic semantic
plausibilities order restrict attention best found word hypothesis sequences
furthermore need part analyzes best found word hypothesis sequences
according syntactic semantic representation category part receives
stream current word hypothesis sequences two word hypothesis sequences
shown figure part provides interpretation word hypothesis sequence
basic syntactic categories abstract syntactic categories basic semantic categories
abstract semantic categories word hypothesis sequence assigned four
graded preferences four word categories
human speech analyzed speech recognizer may contain many errors question
arises extent want consider errors analysis several hundred


fiwermter weber

two word hypotheses sequences


output analysis


kse

ich

meine

natrlich

rubbish



mean

course march

n

ng

u

ng



neg

anim

agent utter act

v

vg



sg

n

ng

nill

misc

time

tmat

kse

ich

htte

ich

rubbish







n

ng

u

ng



neg

anim

agent

v

mrz

mrz
march

vg

u

act

anim agent time

ng

n

ng
tmat


syntactic semantic hypotheses

case frame part

dialog part
learned
flat
syntactic
semantic
analysis

correction part
speech evaluation part

category part
constructed word hypotheses sequences
kse ich meine natrlich mrz
rubbish mean course march

speech sequence construction part

kse ich htte ich mrz
march

rubbish



word hypotheses
word hypotheses generated speech recognizer
















input speech recognizer
current word hypothesis

















pause
kse
ich
ich
meine
meine
htte
etliche
ich
mrz
da
natrlich
mrz
aus
pause

figure overview screen


rubbish


mean
mean

several

march

course
march


e
e
e
e
e
e
e
e
e
e
e
e
e
e
e

fiscreen flat syntactic semantic spoken language analysis

transcripts speech recognizer outputs revealed errors occur
often regularly interjections pauses word repairs phrase repairs
therefore designed correction part receives hypotheses words deals
frequently occurring errors spoken language explicitly
parts outlined far build center integration speech related
language related knowledge fault tolerant learning architecture therefore
focus parts however want process complete dialog turns
contain several individual utterances need know certain utterance
starts constituents belong utterance task performed case
frame part fills frame incrementally segments speaker turn utterances
long term perspective screen provide analysis tasks spoken
utterance translation information extraction besides syntactic semantic analysis
utterance intended dialog acts convey important additional knowledge therefore
dialog part needed assigning dialog acts utterances instance utterance
request suggestion fact already fully implemented case frame part
dialog part utterances however describe details two
parts since described elsewhere wermter lochel
learning screen concepts supervised learning instance feedforward networks rumelhart et al simple recurrent networks elman
general recurrent plausibility networks wermter general recurrent plausibility networks allow arbitrary number context hidden layers considering long
distance dependencies however many network modules screen attempted
keep individual networks simple homogeneous therefore first version
described used variations feedforward networks rumelhart et al
simple recurrent networks elman due greater potential sequential
context representations recurrent plausibility networks might provide improvements
optimizations simple recurrent networks however primarily interested
overall real world hybrid connectionist architecture screen rather optimization single networks following description give detailed examples
individual networks

detailed view

motivated parts screen give detailed description
architecture screen respect modules syntactic semantic
analysis word hypothesis sequences therefore focus speech related parts
categorization part correction part figure shows detailed overview
parts basic data ow shown arrows many modules generate hypotheses
used subsequent modules higher level hypotheses illustrated
rising arrows modules output contains local predictive hypotheses sometimes
called local top hypotheses used modules lower level
hypotheses illustrated falling arrows local predictive hypotheses used
correction part eliminate repaired utterance parts speech evaluation part
eliminate syntactically semantically implausible word hypothesis sequences
means repaired utterance parts actually marked deleted



fiwermter weber

segment parser

dia act

frame

slots
dialog act
type
verb form



case frame part





reject
utter
meinen
mean





dialog part
phrase error









bas syn eq

bas sem eq









correction part

lex start eq

lex word eq





word error


pause interjection
hesitation unresolved
phonetic material

interjection





pause error

pause

dialog lexicon



abs syn eq












sem speech error



phrase start



bas syn pre




separates
syn speech error

abs sem eq



abs syn cat

abs sem cat













bas sem pre









bas syn dis





bas sem dis












verb pronoun
utter nil

speech evaluation part

syntactic lexicon
semantic lexicon

category part


con sequ hyps
kse ich meine

rubbish mean
constructed word hypotheses sequences
kse ich
kse ich meine

rubbish
rubbish mean

speech sequence construction part

figure detailed overview screen abbreviations functionality
modules described text


fiscreen flat syntactic semantic spoken language analysis

cases arrows would complex used numbers illustrate
data ow individual modules
speech sequence construction part

speech sequence construction part receives stream parallel word hypotheses
generates stream word hypothesis sequences within module con sequ hyps
bottom figure current word hypotheses many word hypothesis sequences
may possible cases reduce number current word hypotheses e g
know time passed far specific word hypothesis sequence cannot
extended anymore time current word hypothesis case
eliminate sequence since word hypothesis sequences could reach end
sentence candidates successful speech interpretation
furthermore use speech plausibility values individual word hypothesis
determine speech plausibility word hypothesis sequence
best word hypothesis sequences reduce large space possible sequences
generated stream word hypothesis sequences similar set partial n best
representations generated pruned incrementally speech analysis rather
end speech analysis process
speech evaluation part

speech evaluation part computes plausibilities syntactic semantic knowledge order evaluate word hypothesis sequences part contains modules
detection speech related errors currently performance speech recognizers
spontaneously spoken speaker independent speech general still far perfect
typically many word hypotheses generated certain signal therefore many hypothesized words produced speech recognizer incorrect speech confidence
value word hypothesis alone provide enough evidence finding desired
string signal therefore goal speech evaluation part provide preference
filtering unlikely word hypothesis sequences syn speech error sem speecherror two modules decide current word hypothesis syntactically
semantically plausible extension current word hypothesis sequence syntactic
semantic plausibility basic syntactic semantic category disambiguation
prediction
summary word hypothesis sequence acoustic confidence
speech recognizer syntactic confidence syn speech error semantic
confidence sem speech error three values integrated weighted
equally determine best word hypothesis sequences way two modules
hmm speech recognizer used generating word hypotheses domain word accuracy
best match word graph desired transcript utterance
recognizer particularly optimized task domain order able examine
robustness language level unoptimized version task domain currently
word accuracy
integration speech syntax semantics confidence values provided better
one two three knowledge sources



fiwermter weber

act evaluator speech recognizer well filter language processing
part
statistical speech recognition bigram trigram used language filtering best possible hypotheses used simple recurrent
networks since networks performed slightly better bigram trigram model
implemented comparison sauerland later section
detailed comparison simple recurrent networks n gram n
reason better performance internal representation simple
recurrent network restrict covered context fixed number two
three words potential learn required context needed
output layer
units
n

j

v



r

c

u







p





connections

context layer

hidden layer

conn

input layer

units

copy

connections

units
n

j

v



r

c

u







p





disambiguated representation ich bas syn dis

figure network architecture syntactic prediction speech evaluation part
bas syn pre abbreviations explained table
knowledge syntactic semantic plausibility provided prediction
networks bas syn pre bas sem pre speech evaluation part disambiguation networks bas syn dis bas sem dis categorization part example network bas syn pre figure previous basic syntactic
category currently considered word hypothesis sequence input network
example ich word hypothesis sequence kase ich meine rubbish
mean found pronoun u therefore syntactic category representation
ich contains pronoun u category categories receive
input network consists units categories output
network size unit vector represents plausibility
predicted basic syntax category last word current word hypothesis sequence
plausibility unit representing desired basic syntactic category found
bas syn dis taken syntactic plausibility currently considered word hypothesis
sequence syn speech error example meine mean found verb


fiscreen flat syntactic semantic spoken language analysis

v therefore plausibility verb v taken syntax plausibility selection
marked box output layer bas syn pre figure
summary syntactic semantic plausibility word hypothesis sequence evaluated degree agreement disambiguated syntactic semantic category
current word predicted syntactic semantic category previous word
since decisions current state whole sequence made preceding
context represented copying hidden layer current word context layer
next word srn network structure elman connections
network n connections except connections hidden layer
context layer simply used copy store internal preceding state
context layer later processing next word comes general speech
evaluation part provides ranking current word hypothesis sequences equally
weighted combination acoustic syntactic semantic plausibility
category part

module bas syn dis performs basic syntactic disambiguation see figure input
module sequence potentially ambiguous syntactic word representations one
word utterance time module disambiguates syntactic category
representation according syntactic possibilities previous context output
preference disambiguated syntactic category syntactic disambiguation task
learned simple recurrent network input output network ambiguous
disambiguated syntactic category representations figure example
input representation meine mean verb pronoun
however sequence ich meine mean meine verb therefore
network receives disambiguated verb category representation alone
module bas sem dis similar module bas syn dis instead receiving
potentially ambiguous syntactic category input producing disambiguated syntactic
category output module bas sem dis receives semantic category representation
lexicon provides disambiguated semantic category representation output
semantic disambiguation learned simple recurrent network provides mapping ambiguous semantic word representation disambiguated semantic word
representation modules bas syn dis bas sem dis provide disambiguation
subsequent tasks association abstract categories test category
equality word error detection possible
module abs syn cat supplies mapping disambiguated basic syntactic
category representations abstract syntactic category representations see figure
module provides abstract syntactic categorization realized simple
recurrent network module important providing abstract interpretation
utterance preparing input detection phrase errors figure shows
disambiguated basic syntactic representation meine mean verb
small preference pronoun mapped verb group category higher
abstract syntactic category representation number basic abstract
syntactic categories input units basic syntactic categories output
units abstract syntactic categories


fiwermter weber

output layer
units
n

j

v



r

c

u







p





connections

hidden layer

context layer

conn

input layer

units

copy

connections

units
n

j

v



r

c

u





p





ambiguous representation
meine verb pronoun

syntactic lexicon
meine

kse
rubbish



verb
pronoun

ich meine
mean

current word hypotheses sequence

figure network architecture basic syntactic disambiguation bas syn dis
abbreviations explained table
module abs sem cat parallel module abs syn cat uses basic semantic
category representations input abstract semantic category representations output
similar previous modules used simple recurrent network learn
mapping represent sequential context input network basic
semantic category representation word output abstract category
preference
described four networks provide basis fault tolerant analysis
detection errors furthermore module phrase start distinguishing
abstract categories task module indicate boundaries subsequent
abstract categories delimiter use boundaries determine abstract
syntactic abstract semantic category phrase earlier experiments provided
support take abstract syntactic category first word phrase final
abstract syntactic category phrase since phrase starts e g prepositions good
figure uence phrase start delimiter abstract syntactic semantic
categorization dotted lines



fiscreen flat syntactic semantic spoken language analysis

output layer
units
ng

vg

pg

cg

ag mg

sg

ig

connections

hidden layer

context layer

conn

input layer

connections

units

copy

units
n
j
v

r
c
u

disambiguated representation meine mean





p





figure network architecture abstract syntactic categorization abs syn cat
abbreviations explained table
indicators abstract syntactic categories wermter lochel hand
earlier experiments supported take abstract semantic category last word
phrase final abstract semantic category phrase since phrase ends e g nouns
good indicators abstract semantic categories wermter peters furthermore phrase start gives us opportunity distinguish two equal subsequent abstract
categories two phrases instance phrase hamburg monday
know border exists first second prepositional
phrase
correction part

correction part contains modules detecting pauses interjections well repetitions repairs words phrases see figure modules detecting pause
errors pause error pause interjection modules pause interjection receive currently processed word detect potential occurrence pause
interjection respectively output modules input module pauseerror soon pause interjection detected word marked deleted
therefore virtually eliminated input stream elimination interjections
pauses desired instance speech translation task order provide inter pauses interjections sometimes provide clues repairs nakatani hirschberg although
currently use clues repair detection compared lexical syntactic semantic
equality constituents interjections pauses provide relatively weak indicators repairs since
occur relatively often places sentence however since mark interjections
pauses deleted could make use knowledge future necessary



fiwermter weber

pretation errors possible since three modules basically occurrence
tests realized symbolic representations
second main cluster modules correction part modules
responsible detection word related errors word repairs sechsten
april bin ich ich sixth april wir haben ein termin treffen
date meeting dealt certain preferences finding repetitions
repairs word level among preferences lexical equality two
subsequent words symbolic module lex word eq equality two basic syntactic
category representations connectionist module bas syn eq equality basic
semantic categories two words connectionist module bas sem eq example
three modules test syntactic equality bas syn eq figure
output layer
units
equal equal
connections

hidden layer
units
input layer

connections

n j v r c u p

n j v r c u p


units

disambiguated representation second ich

disambiguated repr first ich

output layer
units
equal equal
connections

hidden layer
units
input layer

connections

n j v r c u p
disambiguated repr termin date

n j v r c u p


units

disambiguated representation treffen meeting

figure network architecture equality basic syntactic category representation bas syn eq abbreviations explained table


fiscreen flat syntactic semantic spoken language analysis

two output units plausible implausible outcome used since network two output units gave consistently better compared network
one output unit plausible implausible reason network two output units performed better separation weights plausible
implausible hidden output layer order receive single value two output values integrated according formula unit unit output
three equality modules value represents equality
represents inequality although single preference may sucient common
uence provides reasonable basis detecting word repairs word repetitions
module word error word repairs repetitions eliminated original
utterance since modules word related errors two representations two
subsequent input words since context play minor role use feedforward
networks modules hand simple test lexical equality
two words lex word eq represented effectively symbolic representation
third main cluster correction part consists modules detection
correction phrase errors example phrase error wir brauchen
den fruheren termin den spateren termin need earlier date later date
preferences phrase errors lexical start two subsequent phrases
equal abstract syntactic categories equal abstract semantic categories
equal three preferences modules lex start eq abs syn eq
abs sem eq modules receive two input representations two corresponding
words two phrases lex start eq receives two lexical words abs syn eq two abstract
syntactic category representations abs sem eq two abstract semantic category representations output three modules value toward equality toward
otherwise values input module phrase error finally decides
whether phrase replaced another phrase lexical equality two words
discrete test implemented lex start eq symbolically preferences
phrase error implemented feedforward networks

detailed analysis examples

section detailed look processing output speech recognizer
producing syntactic semantic interpretation concurrent word hypothesis
sequences called sentence hypothesis

overall environment

overall processing incremental left right time multiple sentence
hypotheses processed parallel figure shows snapshot screen
utterance time snapshot shows first three sentence hypotheses
german words together literal english translations rubbish mean rubbish rubbish screen environment allows user view inspect
incremental generation word hypothesis sequences partial sentence hypotheses
preferred syntactic semantic categories basic abstract level
sentence hypothesis illustrated horizontally certain time many sentence hypotheses
active parallel ranked according descending plausibility


fiwermter weber

screen symbolic connectionist robust enterprise natural language
quit

line

stop

go

single step



sentencehypotheses time system display

n



ng

sug

neg conf

kse rubbish

n





ng

sug

neg conf

kse rubbish

n



ng

sug

neg conf

kse rubbish

u

ng

sug

anim agent conf

ich

v

nil

sug

utter nil

conf

meine mean

u

nil

sug

anim

nil

conf

ng

sug

ich

u

anim agent conf

ich

v

nil

sug



nil

conf

htte




n
j
v

r
c
u



p



figure first snapshot sentence kase ich meine naturlich marz rubbish mean
course march abbreviations explained table
second pop window illustrates full preferences word meine
mean basic syntactic categories
sentence hypotheses snapshot figure currently three sentence
hypotheses preferred current sentence hypothesis consists rubbish mean
sentence hypotheses syntactically semantically plausible starts
underlying variations introduced speech recognizer produced different word
hypotheses slightly overlapping signal parts sentence besides speech plausibility syntax semantics help choosing better sentence hypotheses currently
combine speech recognition plausibility syntactic plausibility semantic
plausibility compute plausibility sentence hypotheses multiplication
respective normalized plausibility values since speech recognizer
contain syntactic semantic knowledge sequence hypothesis rated plausible
speech knowledge alone may neglect potential syntactic semantic regularity


fiscreen flat syntactic semantic spoken language analysis

corresponding syntactic semantic plausibility values sentence hypothesis
integrate acoustic syntactic semantic knowledge
word hypothesis shown preferred basic syntactic hypothesis upper left
square word hypothesis preferred abstract syntactic hypothesis upper middle
square preferred basic semantic hypothesis lower left square preferred abstract
semantic hypothesis lower middle square preferred dialog act upper right square
integrated acoustic syntactic semantic confidence partial sentence hypothesis point lower right square size square illustrates strength
hypothesis full black square means preferred hypothesis close one
instance word hypothesis ich first sentence hypothesis
hypothesis pronoun u basic syntactic category noun group ng
abstract syntactic category animate object anim basic semantic category
agent abstract semantic category suggestion sug dialog act furthermore length vertical bar word hypotheses indicate plausibility
phrase start
another example see representation example word meine could
verb mean pronoun german used throughout
network descriptions see figure network correct preference meine
verb v figure shows preference well zoomed illustration
less favored preferences second pop window see ambiguous pronoun preference u received second strongest activation
preferences close shown activation preferences output values
corresponding network basic syntactic categorization shown activation value
snapshots shows preferred hypothesis hypotheses
shown request
within display scroll descending ascending sentence
hypotheses furthermore scroll left right analyzing specific longer word
hypothesis sequences step mode allows screen system wait
interactive mouse click process next incoming word hypothesis
detailed analysis step mode adapted different number steps word
hypotheses switched completely one decides analyze sentence
hypotheses later end word hypotheses preferred possible
syntactic semantic hypotheses shown therefore many different hypotheses appear
size however clicking one squares less confident
hypotheses displayed well
dialog acts use accept acc query query reject rej request suggest
request state state state suggest sug miscellaneous misc since focuses
syntactic semantic aspects screen elaborate implemented dialog
part details dialog act processing described previously wermter lochel

snapshots figure abstract syntactic semantic categories yet computed
therefore represented nil next processing step computation performed
seen next figure



fiwermter weber

analyzing final snapshot short sentence hypotheses

figure illustrate final state utterance eight possible sentence hypotheses remained see first four figure starting
fourth sentence hypothesis kase ich hatte ich marz rubbish march
see lower rated sentence hypothesis desired sentence lower ranked
hypotheses good examples current state art speech recognizers alone
able produce reliable sentence hypotheses since analyzing spontaneous
speaker independent speech complex therefore syntactic semantic components spontaneous language take account highly irregular
sequences shown however interesting observe underlying connectionist networks produce preference syntactic semantic interpretation
abstract basic level fact although lower ranked sentence hypotheses
constitute desired sentence assigned syntactic semantic categories
correct individual word hypotheses course may cases network
could make wrong decision uncertain word hypotheses however syntactic
semantic processing never break possible sentence hypothesis
respect different well known methods symbolic context free chart parsers
look top ranked sentence hypothesis kase ich meine naturlich marz rubbish mean course march desired sentence plausible
screen symbolic connectionist robust enterprise natural language
quit

go

line

stop

single step



sentencehypotheses time system display

n

ng dsug



neg conf

kse



ng dsug

anim agent conf

ich

n

ng dsug



neg conf

kse

u

ng dsug

ng dsug

anim agent conf



neg conf

kse

u

ng dsug

ng dsug

anim agent conf



neg conf

u

utter act

conf

v

vg dsug

utter act

v



ng dsug

v



htte



sg dsug

nill

misc conf

natrlich

conf



sg dsug

nill

misc conf

natrlich

vg dsug

act

conf

htte

anim agent conf

ich

vg dsug

meine

ich

n

v

meine

ich

n

kse

u

u

ng dsug

anim agent conf

ich

vg dsug

act

conf

u

n

ng dsug

time tmat conf

mrz

n

ng dsug

time tmat conf

mrz

n

ng dsug

time tmat conf

mrz

ng dsug

anim agent conf

ich

n

ng dsug

time tmat conf

mrz


figure final snapshot sentence kase ich meine naturlich marz rubbish mean
course march


fiscreen flat syntactic semantic spoken language analysis

sentence speech language plausibility furthermore see assigned categories correct german word kase rubbish found noun
part noun group expresses negation ich starts phrase
pronoun noun group represents animate agent following
german word meine particularly interesting since used verb sense
mean pronoun sense therefore connectionist network
basic syntactic classification disambiguate two possibilities
preceding context network learned take consideration preceding
context able choose correct basic syntactic category verb v rather
pronoun u word meine mean time phrase start
found well following word naturlich course highest preference
adverb special group finally word marz march assigned highest
plausibility noun noun group well time something happens

phrase starts phrase groups longer sentence hypotheses

focus detailed analysis second example hm ja genau allerdings
habe ich da von neun bis vier uhr schon einen arzttermin literally translated
screen symbolic connectionist robust enterprise natural language
quit

go

line

stop

single step



sentencehypotheses time system display



yes

mg dacc

conf conf

ja

yes

mg dacc

conf conf

ja

drej

conf conf

j

yes

yes

mg dacc

conf conf

ja



nill

mg

drej

conf conf

yes

mg dacc

conf conf



nill



sg

drej

neg conf





sg

drej

neg conf

allerdings

mg dmisc

misc conf

dennoch





allerdings

genau



ja

yes

mg

genau





j

mg dmisc

misc conf

dennoch





sg dmisc

neg conf

allerdings





sg dmisc

neg conf

allerdings

v

vg

drej



act

conf

habe

vg

drej



act

conf

habe

u

nill

act

conf



ng

drej

misc conf

act

conf

anim agent conf

u

nill

es

drej

misc conf

nill

sg

drej

misc conf

nill

sg dmisc

misc conf

da

misc conf

drej

tmat conf

r

pg

drej

tmat conf

r



nil dmisc

nil

conf

von



nill

da

pg

von



ng dmisc

r

von



ng dmisc

ich

habe

nill

sg

da

u

vg dmisc



da

es

habe

v

drej

anim agent conf

vg dmisc



ng

ich

v

v

u

sg dmisc

misc conf

r



nil dacc

nil

conf

von



figure first part snapshot sentence hm ja genau allerdings habe ich
da von neun bis vier uhr schon einen arzttermin literal translation yes
exactly however nine four clock already doctorappointment improved translation eh yes exactly however
doctor appointment nine four clock


fiwermter weber

screen symbolic connectionist robust enterprise natural language
quit

line

stop

go

single step



sentencehypotheses time system display



pg

misc

time tmat conf

neun nine



pg

misc

neun nine



pg

misc

time tmat conf

pg

misc conf

r

nil

r

nil

acc

r

nil

bis



pg

misc

time tmat conf

vier four

pg

misc

misc conf



pg

pg

misc

misc

time tmat conf

misc conf



pg

misc

misc

time tmat conf

misc conf



pg

misc

time tmat conf

n

pg

misc

time tmat conf

n

pg

misc

time tmat conf

uhr oclock

misc

time tmat conf

zehn ten

pg

uhr oclock

zehn ten

pg

n

uhr oclock

vier four

bis

time tmat conf

neun nine

misc

bis

neun nine



nil

pg

bis

time tmat conf



r

n

pg

acc

time tmat conf

uhr oclock



nil

sg

misc

misc conf

schon already



nil

sg

misc

misc conf

schon already



nil

sg

misc

misc conf

schon already



nil

sg

acc

misc conf

schon already



nil

ng

res

misc conf

einen



nil

ng

res

misc conf

nil

ng

res

misc conf

nil

ng

abs

obj

conf

n

ng

res

abs

obj

conf

n

ng

res

abs

obj

conf

arzttermin doc app

res

misc conf

einen

res

arzttermin doc app

einen



ng

arzttermin doc app

einen



n

n

ng

res

abs

obj

conf

arzttermin doc app



figure second part snapshot sentence hm ja genau allerdings habe ich
da von neun bis vier uhr schon einen arzttermin yes exactly however
nine four clock already doctor appointment
sentence analyzed eh yes exactly however nine four clock
already doctor appointment better non literal translation would eh yes
exactly however doctor appointment nine four clock
analysis first sentence hypotheses interjection ahm eh detected
corresponding module correction part eliminated respective
sentence hypotheses
figure figure best found four sentence hypotheses
categories sentence hypotheses look similar keep separate
hypotheses since differ time stamps speech confidence values
two snapshots longer example illustrate uence
phrase starts sequences von neun nine bis vier uhr four
clock constitute two phrase groups clearly separated black bar
prepositions von bis words neun nine
vier four uhr clock start another phrase group since underlying connectionist network learning phrase boundaries simple recurrent network
example demonstrates network learned preceding context without
learned preposition von bis noun


fiscreen flat syntactic semantic spoken language analysis

uhr clock within prepositional phrase group could
part noun phrase another context vier uhr pat gut four clock fits
well

dealing noise repairs
finally focus example simple word graph shown beginning
page hm sechsten april bin ich leider auer hause literal
translation eh th april unfortunately home sentence
give example interjection simple word repair dealing hesitations
repairs large area spontaneous language processing main topic
detailed discussion repairs screen found previous work
weber wermter nevertheless sake illustration completeness
ability screen deal interjections word repairs first snapshot
figure shows start example sentence leading interjection
eh eliminated already
furthermore see second word hypothesis sequence shows two subsequent
word hypotheses ich possible since two word hypotheses
screen symbolic connectionist robust enterprise natural language
quit

go

line

stop

single step



sentencehypotheses time system display

r

pg

sug

tmat conf



r

pg

sug



r

pg

sug

tmat conf

time tmat conf



pg

sug

time tmat conf



pg

sug

time tmat conf

sechsten th

pg

sug

tmat conf



sug

sechsten th



r

pg

sechsten th

tmat conf







pg

sug

time tmat conf

sechsten th

n

pg

sug

time tmat conf

april april

n

pg

sug

time tmat conf

pg

sug

time tmat conf

pg



act

conf

v

vg

sug



act

conf

u

pg

sug

u

ng

state

anim agent conf

ich

u

nil

state

anim

nil

conf

u

nil

state

anim

nil

conf

ich

u

nil

sug

anim

nil

conf

u

nil

sug

anim

nil

conf

ng

state

ich

sug

anim recip conf

ich

time tmat conf

april april

state

htte

april april

n

vg

bin

april april

n

v

ich

v

vg

state



act

conf

bin

u

anim agent conf

ich

ich



figure first snapshot sentence hm sechsten april bin ich leider auer
hause eh th april unfortunately home


fiwermter weber

generated speech recognizer could connected case
four word hypotheses shown
start time





end time





word hypothesis
ich
ich
ich
ich

speech plausibility
e
e
e
e

speech knowledge word hypotheses possible connect
second hypothesis runs fourth hypothesis runs
example noise generated speech recognizer since
desired sentence contains one word ich sentence hypothesis
point contains two repetition treated eliminated way actual
word repairs language reasons occurrence repairs different
effect repeated word therefore case repeated ich
eliminated sentence sequence figure final snapshot
sentence see word repairs occur top ranked sentence hypothesis
desired sentence
screen symbolic connectionist robust enterprise natural language
quit

line

stop

go

single step



sentencehypotheses time system display



pg

sug

time tmat conf

sechsten th



pg

sug

time tmat conf



sechsten th



pg

sug

time tmat conf

sechsten th



pg

sug

time tmat conf

sechsten th

n

pg

sug

time tmat conf

april april

n

pg

sug

time tmat conf

pg

sug

time tmat conf

pg



act

conf

u

pg

sug

u

ng

state

anim agent conf

ich

sug

anim recip conf

u

v

vg

state

ng

sug

anim recip conf



act

conf

vg

state

ng

state

anim agent conf



act

conf

u



sg

rej

neg conf





sg

rej

neg conf

leider unfort

ich

v

bin

u



leider unfort

ich

bin

time tmat conf

april april

state

ich

april april

n

vg

bin

april april

n

v





sg

rej

neg conf

leider unfort

ng

state

anim agent conf

ich





sg

rej

neg conf

leider unfort

r

pg

rej

lcat conf

auer

r

pg

rej

lcat conf

auer

r

pg

rej

lcat conf

auer

r

pg

rej

lcat conf

auer

n

pg

rej

phys lcat conf

hause home

n

pg

rej

phys lcat conf

hause home

n

pg

rej

phys lcat conf

hause home

n

pg

rej

phys lcat conf

hause home



figure final snapshot sentence hm sechsten april bin ich leider auer
hause eh th april unfortunately home


fiscreen flat syntactic semantic spoken language analysis

general language repairs screen deal elimination interjections
pauses repair word repetitions word corrections words may
different categories well simple forms phrase repairs
phrase repeated replaced another phrase

design analysis screen
section describe design choices screen particular focus
issues use connectionist networks reach high accuracy little training
screen compared systems design principles

use connectionist networks screen
past n gram techniques used successfully tasks syntactic
category prediction part speech tagging therefore possible ask developed simple recurrent networks screen subsection provide detailed
comparison simple recurrent networks n gram techniques prediction basic
syntactic categories chose task detailed comparison since currently
dicult task simple recurrent network screen purposefully
choose subtask simple recurrent network high accuracy
prediction task since dicult predict category compared disambiguating among categories instance chose dicult prediction relatively
low network performance order extremely fair comparison n gram
techniques
primarily interested generalization behavior unknown input
therefore figure shows accuracy syntactic prediction unknown test
set word several different syntactic categories follow syntactic
categories excluded instance determiner adjective noun
follow short appointment determiner preposition
implausible occur probably excluded therefore important
know many categories ruled figure shows relationship
prediction accuracy number excluded categories n grams simple
recurrent network described figure
expect techniques n grams recurrent networks prediction
accuracy higher categories excluded performance lower
many categories excluded however interestingly see simple
recurrent networks performed better grams grams grams grams grams
furthermore interesting note higher n grams necessarily lead better
performance instance grams grams perform worse grams since
would probably need much larger training sets
comparison n grams simple recurrent networks
semantic prediction received simple recurrent networks performed
better n grams performance best n gram often slightly worse
performance simple recurrent network indicates n grams
reasonably useful technique however comparisons simple recurrent networks per

fiwermter weber

testset


correct prediction







srn
gram
gram
gram
gram
gram


















number excluded categories

figure comparison simple recurrent network n grams
formed least slightly better best n grams therefore used simple recurrent
networks primary technique connectionist sequence learning screen
explain n grams grams still perform reasonably well
task simple recurrent networks closest performance however
simple recurrent networks perform slightly better since contain fixed
limited context many sequences simple recurrent network may primarily use
directly preceding word representation make prediction however exceptions
context required recurrent network memory internal reduced
representation preceding context therefore potential exible
respect context size
n grams may perform optimally extremely fast question arises
much time necessary compute category input current
context network general networks differ slightly size typically
contain several hundred weights typical representative simple recurrent network
input units hidden units output units context units weights
takes sparc ultra compute category within whole forward sweep


fiscreen flat syntactic semantic spoken language analysis

since techniques smoothed n grams basically rely ecient table look
precomputed values course typical n gram techniques still faster however due
fixed size context may perform well simple recurrent networks furthermore computing next possible categories fast enough current version
screen sake explanation one could argue screen contains
networks modules typical utterance contains words single utterance hypothesis could performed however different text tagging single
sentences process word graphs depending specific utterance word
hypothesis sequences could generated processed furthermore
book keeping required keeping best word hypotheses loading appropriate networks appropriate word hypotheses etc potentially large number
word hypotheses additional book keeping performance number individual
modules syntax semantics dialog processing explain total analysis time
whole unoptimized screen system order seconds although single recurrent
network performs order

improvement hypothesis space
subsection analyze extent syntactic semantic prediction
knowledge used improve best found sentence hypotheses illustrate
pruning performance hypothesis space integrating acoustic syntactic semantic knowledge speech recognizer alone provides acoustic confidence
values screen adds syntactic semantic knowledge knowledge sources
weighted equally order compute single plausibility value current word hypothesis sequence plausibility value used speech construction part prune
hypothesis space select currently best word hypothesis sequences several
word hypothesis sequences processed incremental parallel given time

n best incremental word hypothesis sequences kept
syntactic semantic plausibility values basic syntactic semantic prediction bas syn pre bas sem pre next possible categories
word selection preference determined basic syntactic respectively semantic category bas syn dis bas sem dis performance disambiguation
modules test set prediction modules performance
semantic syntactic test set respectively want exclude least
possible categories performance allows us computation syntactic
semantic plausibility syn speech error sem speech error
combined acoustic syntactic semantic knowledge first tests turns
accuracy constructed sentence hypotheses screen could increased
acoustic syntactic plausibilities acoustic
syntactic semantic plausibilities wermter weber
experiments low values n provided best overall performance
explained detail section



fiwermter weber

screen network performance networks yield high
accuracy little training

evaluating performance screen categorization part meeting corpus
first percentages correctly classified words important networks
categorization bas syn dis bas sem dis abs syn cat abs sem cat phrase start
turns corpus utterances words
words turns used training testing usually data used
training testing preliminary earlier experiments used training
testing however performance unknown test set similar
training set test set therefore used testing training data since
interested generalization performance unknown instances test
set compared training performance known instances
first sight might seem relatively little data training statistical techniques
information retrieval techniques often work large texts individual lexical word
items need much less material get reasonable performance since work
syntactic semantic representations rather words would stress
use syntactic semantic category representations words training
testing rather lexical words therefore category representation
requires much less training data lexical word representation would required
side effect training time reduced training set keeping
performance test set training used category representations
dialog turns testing generalization category representations remaining
dialog turns
table shows test individual networks unknown test set
networks trained epochs learning rate hidden units
configuration provided best performance network architectures
general tested network architectures hidden units learning parameters
learning rule used generalized delta rule rumelhart et al
assigned output category representation word counted correct
category maximum activation desired category
module

accuracy test set
bas syn dis

bas sem dis

abs syn cat

abs sem cat

phrase start

word error

phrase error


table performance individual networks test set meeting corpus
performance basic syntactic disambiguation unknown test
set current syntactic text taggers reach accuracy texts however


fiscreen flat syntactic semantic spoken language analysis

big difference text speech parsing due spontaneous noise
spoken language interjections pauses repetitions repairs starts
ungrammatical syntactic varieties spoken language domain reasons
typical accuracy syntactic text taggers reached
hand see accuracy basic semantic disambiguation
relatively high semantics evidence noisy ungrammatical
variety spoken language hurts syntax less semantics due domain dependence
semantic classifications dicult compare explain semantic performance
however different study within domain railway interactions could reach
similar performance details see section experiments syntactic
better semantic indicating syntactic classification easier
learn generalize furthermore syntactic close noisy
spoken language consider good comparison regular
text language
performance abstract categories somewhat lower basic categories since evaluation word introduces unavoidable errors instance
network cannot yet know time location follow make
early decision already general networks perform relatively well dicult
real world corpus given eliminate sentence reason took
spontaneous sentences spoken
furthermore use transcripts spontaneous language training domain
meeting arrangements utterances questions answers dates
locations restricts potential syntactic semantic constructions certainly
benefit restricted domain furthermore mappings ambiguous
learning e g noun part noun group prepositional group mappings
relatively unambiguous e g verb part verb group would expect
performance mixed arbitrary domains random spoken sentences
topics passers city however performance somewhat
restricted domains learned promising manner transfer different
domain see section evidence simple recurrent networks
provide good performance small training data restricted domain

screen overall output performance
described individual network performance focus
performance running system performance running screen system
different performance individual networks number reasons first
individual networks trained separately order support modular architecture
running screen system however connectionist networks receive input
underlying networks therefore actual input connectionist network
running screen system may differ original training test sets second
spoken sentences may contain errors interjections word repairs part
individual network training running screen system able detect
correct certain interjections word corrections phrase corrections therefore system
network performance differ dis uencies third want evaluate


fiwermter weber

performance abstract semantic categorization abstract syntactic categorization
particularly interested certain sentence parts abstract syntactic categorization
e g detection prepositional phrase consider beginning
phrase significant function word e g preposition important
location syntactic categorization contrast abstract semantic categorization
content word end phrase group directly next phrase start
important
correct syntactic output representation
correct semantic output representation

table overall syntactic semantic accuracy running screen system
unknown test set meeting corpus
expect explanation previous paragraph overall accuracy output complete running system lower performance
individual modules fact true table shows overall syntactic
semantic phrase accuracy running screen system assigned syntactic
phrase representations unknown test set correct assigned semantic
phrase representations slight performance drop partially explained
uncertain input underlying networks uenced
networks hand cases decisions different modules e g
three modules lexical syntactic semantic category equality two words
combined order clean errors e g wrong decision one single module
general given dialog turns test set completely unrestricted unknown real world spontaneous language turns believe overall performance
quite promising

screen overall performance incomplete lexicon

one important property screen robustness therefore interesting question
screen would behave could receive incomplete input lexicon
situations realistic since speakers could use words speech recognizer
seen furthermore test robustness techniques standard
context free parsers usually cannot provide analysis words missing lexicon
screen would break missing input representations although course
expect overall classification performance must drop less reliable input provided
order test situation controlled uence removing items
lexicon first tested scenario randomly eliminated syntactic
semantic lexicon representations word unknown screen used single syntactic
single semantic average default vector instead average default vector contained
normalized frequency syntactic respectively semantic category across lexicon
even without lexicon entries utterances could still analyzed screen
break missing word representations attempts provide analysis good


fiscreen flat syntactic semantic spoken language analysis

correct syntactic output representation
correct semantic output representation

table overall syntactic semantic accuracy running screen system
meeting corpus unknown test set lexicon entries eliminated
possible expected table shows performance drop overall syntactic
semantic accuracy however compared performance complete
lexicon see table still syntactic output representations
semantic output representations correct eliminating lexicon entries
correct syntactic output representation
correct semantic output representation

table overall syntactic semantic accuracy running screen system
meeting corpus unknown test set lexicon entries
eliminated
another experiment eliminated syntactic semantic lexicon entries
case syntactic accuracy still semantic accuracy
eliminating lexicon led syntactic accuracy reduction
versus semantic accuracy reduction versus general
see experiments percentage accuracy reduction much less
percentage eliminated lexicon entries demonstrating screen robustness working
incomplete lexicon

comparison different domain

order compare performance techniques
experiments different spoken regensburg train corpus intention cannot
describe experiments domain level detail done
blaubeuren meeting corpus however provide summary order
provide point reference comparison experiments meeting corpus
comparison serves another additional possibility judge meeting
corpus
different domain chose dialog turns railway counter people ask
questions receive answers train connections typical utterance yes need
eh sleeping car pause pause regensburg hamburg used exactly
screen communication architecture process spoken utterances domain
architecture used dialog turns used training


fiwermter weber

testing unseen unknown utterances syntactic processing even used exactly
network structure since expect much syntactic differences
two domains semantic processing retrained semantic networks different
categories used semantic classification particular actions actions
meetings e g visit meet predominant meeting corpus actions
selecting connections e g choose select important train corpus wermter
weber b give reader impression portability screen
would estimate original human effort system architecture networks could
used domain remaining needed necessary
semantic tagging training domain
module

accuracy test set
bas syn dis

bas sem dis

abs syn cat

abs sem cat

phrase start

word error

phrase error


table performance individual networks test set train corpus
table shows performance test set train corpus compare
meeting corpus table train corpus see
particular abstract syntactic processing almost meeting corpus
table compared table abstract semantic processing better
meeting corpus table compared table modules dealing
explicit robustness repairs phrase start word repair errors phrase repair errors
almost performance vs vs vs
correct syntactic output representation
correct semantic output representation

table overall syntactic semantic accuracy running screen system
unknown test set different train corpus
comparison summarize overall performance different train
domain table shows screen syntactic performance two
domains compare table different domain essentially confirm
previous syntactic processing performance vs however semantic
processing appears harder train domain since performance lower
meeting domain however semantic processing semantic tagging
semantic classification often found much harder syntactic processing general


fiscreen flat syntactic semantic spoken language analysis

difference still within range usual performance differences syntax
semantics since semantic categories agents locations time expressions
two domains dicult action categorization mainly responsible
difference semantic performance two domains
general transfer one domain another requires limited amount
hand modeling course syntactic semantic categories specified
lexicon transcripts syntactically semantically tagged transcript sentences
direct basis generating training sets networks generating
trainings sets main manual effort transferring system domain
generation training sets performed training networks
proceed automatically training typical single recurrent network takes
order hours much less manual work required transferring standard
symbolic parser domain generating syntactic semantic grammar

illustrative comparison argument symbolic parser
made point screen learned representations robust
hand coded deeply structured representations would elaborate point
compelling illustrative argument consider different variations sentence hypotheses
speech recognizer figure correct sentence hypothesis sechsten
april bin ich auer hause th april home partially incorrect
input sechsten april bin ich auer hause
th april home
output

pp


vp

np

np

ng

ng

r

pp

v

np

adjg
n

u

adj


sechsten th april april

r

ng
n

bin

input sechsten april ich ich auer haus
th april home
output nil analysis possible

ich

auer hause home



figure two sentence hypotheses speech recognizer first hypothesis
analyzed second partially incorrect hypothesis cannot analyzed
anymore symbolic parser


fiwermter weber

sentence hypothesis sechsten april ich ich auer hause th april
home focusing syntactic analysis used existing chart parser existing
grammar used extensively real world parsing sentence
level wermter necessary significant adaptation addition rule
n g u pronouns part original grammar rule states
pronoun u e g noun group ng
run first sentence hypothesis symbolic context free parser receive desired syntactic analysis shown figure run second slightly
incorrect sentence hypothesis parser receive analysis syntactic category abbreviations figure used manner throughout
see table furthermore usual stands sentence adjg adjective group np complex nominal phrase vp verb phrase literal english
translations shown brackets
reason second sentence hypothesis could parsed context free
chart parser speech recognizer generated incorrect output verb
second sentence hypothesis additional pronoun mistakes
occur rather frequently imperfectness current speech recognition technology
course one could argue grammar relaxed made exible
deal mistakes however rules fault detection integrated
grammar parser complicated grammar parser even
important impossible predict possible mistakes integrate symbolic
context free grammar finally relaxing grammar dealing mistakes
explicit specific rules might lead additional mistakes grammar
extremely underspecified
shown instance figure screen dealing
speech recognizer variations mistakes main difference standard
context free symbolic chart parser analysis screen analysis screen learned
provide analysis noisy conditions context free parser handcoded provide structural analysis emphasized
make argument structural representations per se general
structure provided better particularly tasks require structured
world knowledge however robustness major concern lower syntactic
semantic spoken language analysis learned analysis provides robustness

comparisons related hybrid systems
recently connectionist networks received lot attention computational learning
mechanisms written language processing reilly sharkey miikkulainen
feldman barnden holyoak wermter however
focused examination hybrid connectionist techniques spoken language processing previous approaches speech language processing processing often
sequential one module speech recognizer syntactic analyzer completed work next module semantic analyzer started work contrast
screen works incrementally allows system modules running par

fiscreen flat syntactic semantic spoken language analysis

allel integrate knowledge sources early compute analysis
similar humans since humans start process sentences may completed
compare related work systems head head comparison different system dicult different computer environments
whether systems accessed adapted easily input furthermore
different systems typically used different purposes different language corpora
grammars rules etc however made extensive effort fair conceptual
comparison
parsec jain hybrid connectionist system embedded larger
speech translation effort janus waibel et al input parsec sentences
output case role representations system consists several connectionist modules
associated symbolic transformation rules providing transformations suggested
connectionist networks parsec philosophy use connectionist networks
triggering symbolic transformations screen uses connectionist networks transformations screen philosophy use connectionist networks wherever
possible symbolic rules necessary
found symbolic processing particularly useful simple known tests lexical
equality complex control tasks whole system module communicate module much actual transformational work done
trained connectionist networks contrast design philosophy parsec
connectionist modules provide control knowledge transformation
performed selected transformation actually performed symbolic procedure screen uses connectionist modules transformations symbolic control
parsec uses connectionist modules control symbolic procedures transformations
different screen parsec receives sentence hypotheses sentence transcripts n best hypotheses janus system receives incremental
word hypotheses used speech construction part build sentence hypotheses part used prune hypothesis space determine best sentence
hypotheses analysis screen semantic syntactic plausibilities
partial sentence hypothesis still uence partial sentence hypotheses
processed
parsec screen modular architecture tested advantage
connectionist module learn relatively easy subtask contrast
development parsec experience modularity requires less training time
furthermore modules screen able work independently
parallel addition syntactic semantic knowledge parsec make use
prosodic knowledge screen currently use prosodic hints hand
screen contains modules learning dialog act assignment modules
currently part parsec learning dialog act processing important determining
intended meaning utterance wermter lochel
recent extensions parsec provide structure use annotated
linguistic features bu et al authors state implemented
parsec connectionist system approximate shift reduce parser
connectionist shift reduce parser substantially differs original parsec architecture


fiwermter weber

refer parsec extension parsec extension labels complete
sentence first level categories first level categories input
network order provide second level categories complete sentence
highest level sentence symbol added
recursion step parsec extension provide deeper structural
interpretations screen currently however recursion step construction structure price first labels np noun phrase
defined lexical items lexicon second important complete utterance
labeled n th level categories processing n th level categories
starts therefore several parses e g utterance big brother loved
utterance necessary means recent parsec extension
powerful screen original parsec system jain respect opportunity provide deeper structural interpretations however time
parsec extension looses possibility process utterances incremental manner
however incrementality important property spoken language processing
screen besides fact humans process language incremental left right
manner allows screen prune search space incoming word hypotheses
early
comparing parsec screen parsec aims supporting symbolic rules symbolic transformations triggered connectionist networks integrating linguistic features currently linguistic features recent parsec extension bu et al
provide structural morphological knowledge screen therefore
currently appears easier integrate parsec extension larger systems
high level linguistic processing fact parsec used context janus
framework hand screen aims robust incremental processing
word hypothesis space specific repair modules representations
particular screen emphasizes robustness spoken language processing since
contains explicit repair mechanisms implicit robustness explicit robustness covers
often occurring errors interjections pauses word phrase repairs explicit modules
less predictable types errors supported implicit similaritybased robustness connectionist networks general representations generated extension parsec provide better support deeper structures
screen screen provides better support incremental robust processing
recent extension parsec called feaspar overall parsing performance
syntactic semantic feature accuracy although additional improvements
shown subsequent search techniques parsing consider
subsequent search techniques better parses since would violate incremental
processing bu without subsequent search techniques screen reaches
overall semantic syntactic accuracy shown table however
pointed screen feaspar use different input sentences features
architectures
besides parsec berp trains systems focus hybrid spoken language processing berp berkeley restaurant project current project employs multiple
different representations speech language analysis wooters jurafsky et al
b task berp act knowledge consultant giving advice choos

fiscreen flat syntactic semantic spoken language analysis

ing restaurants different components berp feature extractor receives
digitized acoustic data extracts features features used connectionist phonetic probability estimation output connectionist feedforward network
used viterbi decoder uses multiple pronunciation lexicon different language e g bigram hand coded grammar rules output decoder word
strings transformed database queries stochastic chart parser finally
dialog manager controls dialog user ask questions
berp screen common ability deal errors humans
speech recognizer well relatively analysis however reaching robustness berp probabilistic chart parser used compute possible fragments first
additional fragment combination used combining fragments
cover greatest number input words different sequential process
first computing fragments utterance combining fragments screen
uses incremental processing desirably provides best possible interpretation
sense screen language analysis weaker general screen analysis never
break produce best possible interpretation noisy utterances strategy
may particularly useful incremental translation hand berp language
analysis stronger restricted berp analysis may stop fragment level
contradictory fragments strategy may particularly useful question
answering additional world knowledge necessary available
trains related spoken language project building assistant
reason time actions events allen allen et al
goal building general framework natural language processing
train scheduling trains needs lot commonsense knowledge scenario person
interacts system order solutions train scheduling cooperative
manner person assumed know goals scheduling
system supposed details domain utterance person
parsed syntactic semantic parser linguistic reasoning completed
modules scoping reference resolution linguistic reasoning conversation
acts determined system dialog manager responses generated
template driven natural language generator performance phenomena spoken language
repairs false starts currently dealt already heeman allen b
compared screen trains project focuses processing spoken
language depth level screen uses primarily connectionist
language analysis trains uses chart parser generalized phrase structure grammar

discussion
first focus learned processing spoken language processing
started screen project predetermined whether deep analysis
screening analysis would particularly appropriate robust analysis spoken
sentences deep analysis highly structured representations less appropriate since
unpredictable faulty variations spoken language limit usefulness deep structured knowledge representations much case written language deep
interpretations structured representations instance possible hpsg


fiwermter weber

grammars text processing make great deal assumptions predictions
hold faulty spoken language furthermore learned generating
semantic syntactic representation even need use deep interpretation
certain tasks instance translating two languages necessary
disambiguate prepositional phrase attachment ambiguities since process
translation disambiguations may get ambiguous target language
however use structure level words phrases syntax semantics respectively learned single semantics level rather four
syntax semantics levels sucient since syntax necessary detecting phrase
boundaries one could argue one syntactic abstract phrase representation one
abstract semantic phrase representation may enough however found basic
syntactic semantic representations word level make task easier subsequent abstract analysis phrase level furthermore basic syntactic semantic
representations necessary tasks well instance judgment
plausibility sequence syntactic semantic categories plausibility used
filter finding good word hypothesis sequences therefore argue processing
faulty spoken language task sentence translation question answering
need much less structured representations typically used well known parsers
need structured representations single level tagger
previous work made early experiences related connectionist
networks analyzing text phrases moving analyzing text phrases analyzing unrestricted spoken utterances tremendous differences two tasks found
phrase oriented analysis used scan wermter advantageous principle
spoken language analysis phrase oriented analysis common learning text
speech processing however learned spoken language analysis needs much
sophisticated architecture particular since spoken language contains many unpredictable errors variations fault tolerance robustness much important
connectionist networks inherent implicit robustness similarity
processing gradual numerical representations addition found classes
relatively often occurring mistakes explicit robustness provided
machinery interjections word phrase repairs furthermore architecture
consider processing potentially large number competing word hypothesis
sequences rather single sentence phrase text processing
focus learned connectionist hybrid architectures beginning predetermine whether connectionist methods would
particularly useful control individual modules however development screen system became clear general task
spoken language understanding individual subtasks syntactic analysis
fault tolerant noise spoken language due humans speechrecognizers well especially unforeseeable variations often occur spontaneously spoken
language cannot predefined well advance symbolic rules general manner
fault tolerance task level could supported particularly well inherent
fault tolerance connectionist networks individual tasks support inductive
learning learned robust understanding spoken language
connectionist networks particularly effective within individual subtasks


fiscreen flat syntactic semantic spoken language analysis

quite lot work control connectionist networks however
many cases approaches concentrated control single networks
recently work control modular architectures sumida
jacobs et al b jain jordan jacobs miikkulainen instance
jacobs jordan jacobs et al b jordan jacobs task
knowledge control knowledge learned task knowledge learned individual
task networks higher control networks responsible learning single task
network responsible producing output originally open question whether
connectionist control would possible processing spoken language automatic
modular task decomposition jacobs et al done simple forms function
approximation complex understanding spoken language real world
environments still need designer modular task decomposition necessary tasks
learned connectionist control architecture lot modules
subtasks currently seems beyond capabilities current connectionist networks
shown connectionist control possible limited number connectionist modules miikkulainen jain instance miikkulainen shows
connectionist segmenter connectionist stack control parser analyze embedded clauses however communication paths still restricted within
three modules especially real world system spoken language understanding speech syntax semantics dialog processing translation extremely
dicult learn coordinate different activities especially large parallel stream
word hypothesis sequences believe may possible future however
currently connectionist control screen restricted detection certain hesitations
phenomena corrections
considering screening analysis spoken language hybrid connectionist techniques together developed followed general guideline design philosophy
little knowledge necessary getting far possible connectionist
networks wherever possible symbolic representations necessary guideline
led us robust representation spoken language analysis
use hybrid connectionist techniques support task choice possibly
appropriate knowledge structure many hybrid systems contain small portion
connectionist representations addition many modules e g berp wooters
jurafsky et al b janus waibel et al trains allen allen
et al contrast important subtasks screen performed directly
many connectionist networks
furthermore learned syntactic semantic representations could give
surprisingly good training test trained tested medium corpus
words dialog turns good mostly due
learned internal weight representation local context adds sequentiality
category assignments without internal weight representation preceding context
syntactic semantic categorization perform equally well choice
recurrent networks crucial many sequential category assignments therefore
networks techniques hold potential especially medium size domains
restricted amount training material available statistical techniques often


fiwermter weber

used large data sets work well medium data sets connectionist
techniques used work well medium size domains
used techniques ported different domains used different purposes even different sets categories would used learning networks
able extract syntactic regularities automatically besides domain arranging
business meetings ported screen domain interactions railway
counter comparable syntactic semantic two domains differed primarily semantic categories syntactic categories networks screen
could used directly
screen potential scaling fact imperfect output
speech recognizer several thousand sentence hypotheses already processed
words processed syntactic semantic basic categories simply
entered lexicon structure individual networks change units
added therefore networks retrained
amount hand coding restricted primarily symbolic control module
interaction labeling training material individual networks
changed domain railway counter interactions could use identical control
well syntactic networks semantic networks retrained due
different domain
far focused supervised learning simple recurrent networks feedforward networks supervised learning still requires training set manual labeling
work still done although especially medium size corpora labeling examples
easier instance designing complete rule bases would nice automate
knowledge acquisition even currently plan build sophisticated lexicon component provide support automatic lexicon design riloff
dynamic lexicon entry determination local context miikkulainen
furthermore screen could expanded speech construction evaluation
part syntactic semantic hypotheses could used interaction
speech recognizer currently syntactic semantic hypotheses speech evaluation
part used exclude unlikely word hypothesis sequences language modules
however hypotheses connectionist networks syntax semantics
particular modules basic syntactic semantic category prediction could
used directly process recognition future order provide
syntactic semantic feedback speech recognizer early stage besides syntax
semantics cue phrases stress intonation could provide additional knowledge
speech language processing hirschberg gupta touretzky issues
additional major efforts future

conclusions
described underlying principles implemented architecture evaluation screening learning analysis spoken language work
makes number original contributions fields artificial intelligence advances
state art several perspectives perspective symbolic connectionist design argue hybrid solution connectionist networks used


fiscreen flat syntactic semantic spoken language analysis

wherever useful symbolic processing used control higher level analysis furthermore shown recurrent networks provided better syntactic
semantic prediction grams perspective connectionist networks
alone demonstrated connectionist networks fact used real world
spoken language analysis perspective natural language processing argue
hybrid system design advantageous integrating speech language since lower
speech related processing supported fault tolerant learning connectionist networks
higher processing control supported symbolic knowledge structures general properties support parallel rather sequential learned rather coded
fault tolerant rather strict processing spoken language
main learned representations support robust processing spoken language better depth structured representations connectionist networks provide fault tolerance reach robustness due noise
spontaneous language interjections pauses repairs repetitions false starts ungrammaticalities additional false word hypotheses speech recognizer complex structured possibly recursive representations often cannot computed standard symbolic
representations context free parsers hand tasks information
extraction spoken language may need depth structured representation believe hybrid connectionist techniques considerable potential
tasks instance information extraction restricted noisy spoken language domains depth understanding inferencing story interpretation needs
complex structured representations shallow understanding instance information
extraction noisy speech language environments benefit robust learned
representations

acknowledgements
funded german federal ministry technology
bmbf grant iv german association dfg
grant dfg ha grant dfg would thank haack
lochel meurer u sauerland schrattenholzer work screen
well david bean alexandra klein steven minton johanna moore ellen riloff five
anonymous referees comments earlier versions

references
allen j f trains parsing system user manual tech rep trains
tn university rochester computer science department
allen j f schubert l k ferguson g heeman p hwang c h kato light
martin n g miller b w poesio traum r trains
project case study building conversational agent journal experimental theoretical ai


fiwermter weber

barnden j holyoak k j eds advances connectionist neural
computation theory vol ablex publishing corporation
bu f feaspar feature structure parser learning parse spontaneous
speech ph thesis university karlsruhe karlsruhe frg
bu f polzin waibel learning complex output representations
connectionist parsing spoken language proceedings international conference acoustics speech signal processing vol pp adelaide
australia
charniak e statistical language learning mit press cambridge
dyer g depth understanding computer model integrated processing
narrative comprehension mit press cambridge
elman j l finding structure time cognitive science
faisal k kwasny c design hybrid deterministic parser proceedings th international conference computational linguistics pp
helsinki finnland
feldman j structured connectionist language learning artificial
intelligence review
geutner p suhm b bu f kemp mayfield l mcnair e rogina
schultz sloboda ward w woszczyna waibel integrating
different learning approaches multilingual spoken language translation system
wermter riloff e scheler g eds connectionist statistical symbolic approaches learning natural language processing pp springer
heidelberg
gupta p touretzky connectionist linguistic theory investigations stress systems language cognitive science
hauenstein weber h h investigation tightly coupled time synchronous
speech language interfaces unification grammar proceedings th
national conference artificial intelligence workshop integration natural
language speech processing pp seattle washington
heeman p allen j detecting correcting speech repairs proceedings
nd annual meeting association computational linguistics pp
las cruces nm
heeman p allen j b tagging speech repairs proceedings human
language technology workshop pp plainsboro nj
hendler j marker passing microfeatures towards hybrid symbolic connectionist model cognitive science


fiscreen flat syntactic semantic spoken language analysis

hirschberg j pitch accent context predicting intonational prominence
text artificial intelligence
jacobs r jordan barto g task decomposition competition modular connectionist architecture vision tasks
cognitive science
jacobs r jordan nowlan j hinton g e b adaptive mixtures
local experts neural computation
jain n parsing complex sentences structured connectionist networks
neural computation
jordan jacobs r hierarchies adaptive experts moody j e
hanson j lippmann r r eds advances neural information processing
systems pp morgan kaufmann san mateo ca
jurafsky wooters c tajchman g segal j stolcke fosler e morgan
n berkeley restaurant project proceedings international
conference speech language processing pp yokohama japan
jurafsky wooters c tajchman g segal j stolcke morgan n b integrating experimental syntax phonology accent dialect speech recognizer investigation tightly coupled time synchronous speech proceedings
th national conference artificial intelligence workshop integration
natural language speech processing pp seattle washington
medsker l r hybrid neural network expert systems kluwer academic
publishers boston
mellish c chart techniques parsing ill formed input proceedings th annual meeting association computational linguistics pp
vancouver canada
menzel w parsing spoken language time constraints cohn g
ed proceedings th european conference artificial intelligence pp
amsterdam
miikkulainen r subsymbolic natural language processing integrated model
scripts lexicon memory mit press cambridge
miikkulainen r subsymbolic case role analysis sentences embedded clauses
cognitive science
nakatani c hirschberg j speech first model repair detection correction proceedings st annual meeting association computational
linguistics pp columbus ohio
reilly r g sharkey n e eds connectionist approaches natural language processing lawrence erlbaum associates hillsdale nj


fiwermter weber

riloff e automatically constructing dictionary information extraction tasks
proceedings th national conference artificial intelligence pp
washington dc
rumelhart e hinton g e williams r j learning internal representations error propagation rumelhart e mcclelland j l pdp
group eds parallel distributed processing vol pp mit press
cambridge
sauerland u konzeption und implementierung einer speech language
schnittstelle master thesis university hamburg computer science department
hamburg frg
sumida r dynamic inferencing parallel distributed semantic networks
proceedings th annual meeting cognitive science society pp
boston chicago
sun r integrating rules connectionism robust common sense reasoning
wiley sons york
von hahn w pyka c system architectures speech understanding
language processing heyer g haugeneder h eds applied linguistics
wiesbaden
waibel jain n mcnair tebelskis j osterholtz l saito h schmidbauer
sloboda woszczyna janus speech speech translation
connectionist non connectionist techniques moody j e hanson j
lippmann r r eds advances neural information processing systems pp
morgan kaufmann san mateo ca
ward n tightly coupled syntactic semantic processing speech
understanding proceedings th national conference artificial intelligence workshop integration natural language speech processing pp
seattle washington
weber v wermter towards learning semantics spontaneous dialog utterances hybrid framework hallam j ed hybrid hybrid solutions
proceedings th biennial conference ai cognitive science pp
sheeld uk
weber v wermter artificial neural networks repairing language
proceedings th international conference neural networks applications pp marseille fra
wermter weber v interactive spoken language processing hybrid
connectionist system ieee computer theme issue interactive natural language
processing july


fiscreen flat syntactic semantic spoken language analysis

wermter hybride symbolische und subsymbolische verarbeitung beispiel
der sprachverarbeitung duwe kurfe f paa g vogel eds
herbstschule konnektionismus und neuronale netze gesellschaft fur mathematik und
datenverarbeitung gmd sankt augustin frg
wermter hybrid connectionist natural language processing chapman
hall thompson international london uk
wermter lochel connectionist learning syntactic analysis
speech language systems marinaro morasso p g eds proceedings
international conference artificial neural networks vol pp
sorrento italy
wermter lochel learning dialog act processing proceedings
th international conference computational linguistics pp kopenhagen denmark
wermter peters u learning incremental case assignment modular
connectionist knowledge sources werbos p szu h widrow b eds proceedings world congress neural networks vol pp san diego
ca
wermter riloff e scheler g eds connectionist statistical symbolic
approaches learning natural language processing springer berlin
wermter weber v b artificial neural networks automatic knowledge acquisition multiple real world language domains proceedings th international conference neural networks applications pp marseille
fra
wooters c c lexical modeling speaker independent speech understanding
system tech rep tr international computer science institute berkeley
young r hauptmann g ward w h smith e werner p high
level knowledge sources usable speech recognition systems communications
acm




