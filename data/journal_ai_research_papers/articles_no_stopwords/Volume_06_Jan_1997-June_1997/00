Journal Artificial Intelligence Research 6 (1997) 1-34

Submitted 5/96; published 1/97

Improved Heterogeneous Distance Functions
D. Randall Wilson
Tony R. Martinez
Computer Science Department
Brigham Young University
Provo, UT 84602, USA

RANDY @AXON.CS.BYU.EDU
MARTINEZ @CS.BYU.EDU

Abstract
Instance-based learning techniques typically handle continuous linear input values well,
often handle nominal input attributes appropriately. Value Difference Metric
(VDM) designed find reasonable distance values nominal attribute values,
largely ignores continuous attributes, requiring discretization map continuous values
nominal values. paper proposes three new heterogeneous distance functions, called
Heterogeneous Value Difference Metric (HVDM), Interpolated Value Difference Metric
(IVDM), Windowed Value Difference Metric (WVDM). new distance functions
designed handle applications nominal attributes, continuous attributes, both.
experiments 48 applications new distance metrics achieve higher classification accuracy
average three previous distance functions datasets nominal
continuous attributes.

1. Introduction
Instance-Based Learning (IBL) (Aha, Kibler & Albert, 1991; Aha, 1992; Wilson & Martinez,
1993; Wettschereck, Aha & Mohri, 1995; Domingos, 1995) paradigm learning
algorithms typically store n available training examples (instances)
training set, T, learning. instance input vector x, output class c.
generalization, systems use distance function determine close new
input vector stored instance, use nearest instance instances predict
output class (i.e., classify y). instance-based learning algorithms referred
nearest neighbor techniques (Cover & Hart, 1967; Hart, 1968; Dasarathy, 1991), memorybased reasoning methods (Stanfill & Waltz, 1986; Cost & Salzberg, 1993; Rachlin et al., 1994)
overlap significantly instance-based paradigm well. algorithms much
success wide variety applications (real-world classification tasks).
Many neural network models make use distance functions, including radial basis
function networks (Broomhead & Lowe, 1988; Renals & Rohwer, 1989; Wasserman, 1993),
counterpropagation networks (Hecht-Nielsen, 1987), ART (Carpenter & Grossberg, 1987), selforganizing maps (Kohonen, 1990) competitive learning (Rumelhart & McClelland, 1986).
Distance functions used many fields besides machine learning neural networks,
including statistics (Atkeson, Moore & Schaal, 1996), pattern recognition (Diday, 1974;
Michalski, Stepp & Diday, 1981), cognitive psychology (Tversky, 1977; Nosofsky, 1986).
1997 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiWILSON & MARTINEZ

many distance functions proposed decide instance
closest given input vector (Michalski, Stepp & Diday, 1981; Diday, 1974). Many
metrics work well numerical attributes appropriately handle nominal (i.e.,
discrete, perhaps unordered) attributes.
Value Difference Metric (VDM) (Stanfill & Waltz, 1986) introduced define
appropriate distance function nominal (also called symbolic) attributes. Modified Value
Difference Metric (MVDM) uses different weighting scheme VDM used
PEBLS system (Cost & Salzberg, 1993; Rachlin et al., 1994). distance metrics work well
many nominal domains, handle continuous attributes directly. Instead,
rely upon discretization (Lebowitz, 1985; Schlimmer, 1987), degrade generalization
accuracy (Ventura & Martinez, 1995).
Many real-world applications nominal linear attributes, including,
example, half datasets UCI Machine Learning Database Repository (Merz &
Murphy, 1996). paper introduces three new distance functions appropriate
previous functions applications nominal continuous attributes.
new distance functions incorporated many learning systems areas
study, augmented weighting schemes (Wettschereck, Aha & Mohri, 1995;
Atkeson, Moore & Schaal, 1996) enhancements system provides.
choice distance function influences bias learning algorithm. bias rule
method causes algorithm choose one generalized output another (Mitchell,
1980). learning algorithm must bias order generalize, shown
learning algorithm generalize accurately summed
possible problems (Schaffer, 1994) (unless information problem
training data available). follows distance function strictly better
terms generalization ability, considering possible problems equal
probability.
However, higher probability one class problems occurring another,
learning algorithms generalize accurately others (Wolpert, 1993).
better summed problems, problems
perform well likely occur. sense, one algorithm distance function
improvement another higher probability good generalization
another, better matched kinds problems likely occur.
Many learning algorithms use bias simplicity (Mitchell, 1980; Wolpert, 1993)
generalize, bias appropriatemeaning leads good generalization
accuracyfor wide variety real-world applications, though meaning simplicity varies
depending upon representational language learning algorithm. biases,
decisions made basis additional domain knowledge particular problem (Mitchell,
1980), improve generalization.
light, distance functions presented paper appropriate
used comparison average yield improved generalization accuracy
collection 48 applications. results theoretically limited set datasets,
hope datasets representative problems interest (and occur
frequently) real world, distance functions presented useful
cases, especially involving continuous nominal input attributes.
Section 2 provides background information distance functions used previously. Section 3
2

fiIMPROVED HETEROGENEOUS DISTANCE FUNCTIONS

introduces distance function combines Euclidean distance VDM handle
continuous nominal attributes. Sections 4 5 present two extensions Value
Difference Metric allow direct use continuous attributes. Section 4 introduces
Interpolated Value Difference Metric (IVDM), uses interpolation probabilities avoid
problems related discretization. Section 5 presents Windowed Value Difference Metric
(WVDM), uses detailed probability density function similar interpolation
process.
Section 6 presents empirical results comparing three commonly-used distance functions
three new functions presented paper. results obtained using
distance functions instance-based learning system 48 datasets. results indicate
new heterogeneous distance functions appropriate previously used functions
datasets nominal linear attributes, achieve higher average
generalization accuracy datasets. Section 7 discusses related work, Section 8
provides conclusions future research directions.

2. Previous Distance Functions
mentioned introduction, many learning systems depend upon good
distance function successful. variety distance functions available uses,
including Minkowsky (Batchelor, 1978), Mahalanobis (Nadler & Smith, 1993), Camberra,
Chebychev, Quadratic, Correlation, Chi-square distance metrics (Michalski, Stepp &
Diday, 1981; Diday, 1974); Context-Similarity measure (Biberman, 1994); Contrast
Model (Tversky, 1977); hyperrectangle distance functions (Salzberg, 1991; Domingos, 1995)
others. Several functions defined Figure 1.
Although many distance functions proposed, far commonly
used Euclidean Distance function, defined as:


E(x, y) =

(xa ya )2

(1)

a=1

x two input vectors (one typically stored instance,
input vector classified) number input variables (attributes)
application. square root often computed practice, closest instance(s)
still closest, regardless whether square root taken.
alternative function, city-block Manhattan distance function, requires less
computation defined as:
M(x, y) =



xa ya

(2)

a=1

Euclidean Manhattan distance functions equivalent Minkowskian rdistance function (Batchelor, 1978) r = 2 1, respectively.

3

fiWILSON & MARTINEZ

Minkowsky:

Euclidean:


r
D(x, y) = xi yi
i=1

Camberra:

1

r

Manhattan / city-block:



2
( xi yi )

D(x, y) =

i=1

x

D(x, y) =
x
+


i=1

Chebychev:



D(x, y) = xi yi
i=1



D(x, y) = max xi yi
i=1



D(x, y) = (x y)T Q(x y) = (xi yi )q ji (x j j )
Quadratic:

Q problem-specific positive
j=1 i=1
definite weight matrix
V covariance matrix 1..Am,
Mahalanobis:
Aj vector values
D(x, y) = [det V]1/ (x y)T V 1 (x y)
attribute j occuring training set
instances 1..n.

Correlation:
(xi xi )(yi yi )
xi = yi average value
i=1
D(x, y) =
attribute
occuring training set.


2
2
(xi xi ) (yi yi )
i=1

i=1


1 xi
Chi-square: D(x, y) =


sumi sizex size
i=1


Kendalls Rank Correlation:
sign(x)=-1, 0 1 x < 0,
x = 0, x > 0, respectively.

D(x, y) = 1

2

sumi sum values attribute
occuring training set, sizex
sum values vector x.

i1
2

sign(xi x j )sign(yi j )
n(n 1) i=1 j=1

Figure 1. Equations selected distance functions.
(x vectors attribute values).
2.1. Normalization
One weakness basic Euclidean distance function one input attributes
relatively large range, overpower attributes. example, application
two attributes, B, values 1 1000, B values
1 10, Bs influence distance function usually overpowered
influence. Therefore, distances often normalized dividing distance attribute
range (i.e., maximum-minimum) attribute, distance attribute
approximate range 0..1. order avoid outliers, common divide
standard deviation instead range, trim range removing highest lowest
percent (e.g., 5%) data consideration defining range. possible
map value outside range minimum maximum value avoid normalized
values outside range 0..1. Domain knowledge often used decide method
appropriate.
Related idea normalization using attribute weights weighting
4

fiIMPROVED HETEROGENEOUS DISTANCE FUNCTIONS

schemes. Many learning systems use distance functions incorporate various weighting
schemes distance calculations (Wettschereck, Aha & Mohri, 1995; Atkeson, Moore &
Schaal, 1996). improvements presented paper independent schemes,
various weighting schemes (as well enhancements instance pruning
techniques) used conjunction new distance functions presented here.
2.2. Attribute Types
None distance functions shown Figure 1, including Euclidean distance, appropriately
handle non-continuous input attributes.
attribute linear nominal, linear attribute continuous discrete.
continuous (or continuously-valued) attribute uses real values, mass planet
velocity object. linear discrete (or integer) attribute discrete set
linear values, number children.
argued value stored computer discrete level. reason
continuous attributes treated differently many different values
value may appear rarely (perhaps particular application). causes
problems algorithms VDM (described Section 2.4) depend testing two
values equality, two continuous values rarely equal, though may
quite close other.
nominal (or symbolic) attribute discrete attribute whose values necessarily
linear order. example, variable representing color might values red,
green, blue, brown, black white, could represented integers 1 6,
respectively. Using linear distance measurement (1) (2) values makes little
sense case.
2.3. Heterogeneous Euclidean-Overlap Metric (HEOM)
One way handle applications continuous nominal attributes use
heterogeneous distance function uses different attribute distance functions different
kinds attributes. One approach used use overlap metric nominal
attributes normalized Euclidean distance linear attributes.
purposes comparison testing, define heterogeneous distance function
similar used IB1, IB2 IB3 (Aha, Kibler & Albert, 1991; Aha, 1992) well
used Giraud-Carrier & Martinez (1995). function defines distance
two values x given attribute as:
x unknown, else
1,

da (x, y) = overlap(x, y), nominal, else
rn_ diff (x, y)



(3)

Unknown attribute values handled returning attribute distance 1 (i.e., maximal
distance) either attribute values unknown. function overlap rangenormalized difference rn_diff defined as:
0, x =
overlap(x, y) =
1, otherwise

5

(4)

fiWILSON & MARTINEZ

rn_ diff (x, y) =

| x y|
rangea

(5)

value rangea used normalize attributes, defined as:
rangea= maxa- mina

(6)

max mina maximum minimum values, respectively, observed
training set attribute a. means possible new input vector value
outside range produce difference value greater one. However, cases
rare, occur, large difference may acceptable anyway. normalization
serves scale attribute point differences almost always less one.
definition da returns value (typically) range 0..1, whether
attribute nominal linear. overall distance two (possibly heterogeneous) input
vectors x given Heterogeneous Euclidean-Overlap Metric function HEOM(x,y):


da (xa , ya )2

HEOM(x, y) =

(7)

a=1

distance function removes effects arbitrary ordering nominal values,
overly simplistic approach handling nominal attributes fails make use additional
information provided nominal attribute values aid generalization.
2.4. Value Difference Metric (VDM)
Value Difference Metric (VDM) introduced Stanfill Waltz (1986) provide
appropriate distance function nominal attributes. simplified version VDM (without
weighting schemes) defines distance two values x attribute as:
C Na,x,c

Na,y,c
vdma (x, y) =

N
Na,y
a,x
c=1

q

C

= Pa,x,c Pa,y,c

q

(8)

c=1


Na,x number instances training set value x attribute a;
Na,x,c number instances value x attribute output class c;
C number output classes problem domain;
q constant, usually 1 2;
P a,x,c conditional probability output class c given attribute
value x, i.e., P(c | xa). seen (8), Pa,x,c defined as:
Pa,x,c =

Na,x,c
Na,x

(9)

Na,x sum Na,x,c classes, i.e.,
C

Na,x = Na,x,c
c=1

6

(10)

fiIMPROVED HETEROGENEOUS DISTANCE FUNCTIONS

sum Pa,x,c C classes 1 fixed value x.
Using distance measure vdma(x,y), two values considered closer
similar classifications (i.e., similar correlations output classes), regardless
order values may given in. fact, linear discrete attributes values
remapped randomly without changing resultant distance measurements.
example, attribute color three values red, green blue, application
identify whether object apple, red green would considered closer
red blue former two similar correlations output class apple.
original VDM algorithm (Stanfill & Waltz, 1986) makes use feature weights
included equations, variants VDM (Cost & Salzberg, 1993;
Rachlin et al., 1994; Domingos, 1995) used alternate weighting schemes. discussed
earlier, new distance functions presented paper independent schemes
cases make use similar enhancements.
One problem formulas presented define
done value appears new input vector never appeared training set.
attribute never value x instance training set, Na,x,c c 0,
N a,x (which sum Na,x,c classes) 0. cases P a,x,c = 0/0,
undefined. nominal attributes, way know probability
value, since inherent ordering values. paper assign
P a,x,c default value 0 cases (though possible let Pa,x,c = 1/C, C
number output classes, since sum Pa,x,c c = 1..C always 1.0).
distance function used directly continuous attributes, values
potentially unique, case Na,x 1 every value x, Na,x,c 1 one value c
0 others given value x. addition, new vectors likely unique
values, resulting division zero problem above. Even value 0 substituted
0/0, resulting distance measurement nearly useless.
Even values unique, often enough different values continuous
attribute statistical sample unreliably small value, distance measure
still untrustworthy. problems, inappropriate use VDM directly
continuous attributes.
2.5. Discretization
One approach problem using VDM continuous attributes discretization
(Lebowitz, 1985; Schlimmer, 1987; Ventura, 1995). models used VDM
variants (Cost & Salzberg, 1993; Rachlin et al., 1994; Mohri & Tanaka, 1994)
discretized continuous attributes somewhat arbitrary number discrete ranges,
treated values nominal (discrete unordered) values. method advantage
generating large enough statistical sample nominal value P values
significance. However, discretization lose much important information available
continuous values. example, two values discretized range considered
equal even opposite ends range. effects reduce generalization
accuracy (Ventura & Martinez, 1995).
paper propose three new alternatives, presented following three
sections. Section 3 presents heterogeneous distance function uses Euclidean distance
linear attributes VDM nominal attributes. method requires careful attention
7

fiWILSON & MARTINEZ

problem normalization neither nominal linear attributes regularly given
much weight.
Sections 4 5 present two distance functions, Interpolated Value Difference
Metric (IVDM) Windowed Value Difference Metric (WVDM), use discretization
collect statistics determine values Pa,x,c continuous values occurring training
set instances, retain continuous values later use. generalization,
value Pa,y,c continuous value interpolated two values P, namely,
P a,x1,c Pa,x2,c, x 1 x2. IVDM WVDM essentially different techniques
nonparametric probability density estimation (Tapia & Thompson, 1978)
determine values P class. generic version VDM algorithm, called
discretized value difference metric (DVDM) used comparisons two new
algorithms.

3. Heterogeneous Value Difference Metric (HVDM)
discussed previous section, Euclidean distance function inappropriate
nominal attributes, VDM inappropriate continuous attributes, neither sufficient
use heterogeneous application, i.e., one nominal continuous
attributes.
section, define heterogeneous distance function HVDM returns distance
two input vectors x y. defined follows:
HVDM(x, y) =



da2 (xa , ya )

(11)

a=1

number attributes. function da(x,y) returns distance two
values x attribute defined as:
x unknown; otherwise...
1,

da (x, y) = normalized_ vdma (x, y), nominal
normalized_ diff (x, y), linear



(12)

function da(x,y) uses one two functions (defined Section 3.1), depending
whether attribute nominal linear. Note practice square root (11)
typically performed distance always positive, nearest neighbor(s) still
nearest whether distance squared. However, models (e.g.,
distance-weighted k-nearest neighbor, Dudani, 1976) require square root
evaluated.
Many applications contain unknown input values must handled appropriately
practical system (Quinlan, 1989). function da(x,y) therefore returns distance 1 either
x unknown, done Aha, Kibler & Albert (1991) Giraud-Carrier & Martinez
(1995). complicated methods tried (Wilson & Martinez, 1993),
little effect accuracy.
function HVDM similar function HOEM given Section 2.3, except
8

fiIMPROVED HETEROGENEOUS DISTANCE FUNCTIONS

uses VDM instead overlap metric nominal values normalizes differently.
similar distance function used RISE 2.0 (Domingos, 1995),
important differences noted Section 3.2.
Section 3.1 presents three alternatives normalizing nominal linear attributes.
Section 3.2 presents experimental results show one schemes provides better
normalization two set several datasets. Section 3.3 gives empirical results
comparing HVDM two commonly-used distance functions.
3.1. Normalization
discussed Section 2.1, distances often normalized dividing distance
variable range attribute, distance input variable range
0..1. policy used HEOM Section 2.3. However, dividing range allows
outliers (extreme values) profound effect contribution attribute.
example, variable values range 0..10 almost every case one
exceptional (and possibly erroneous) value 50, dividing range would almost
always result value less 0.2. robust alternative presence outliers
divide values standard deviation reduce effect extreme values typical
cases.
new heterogeneous distance metric HVDM, situation complicated
nominal numeric distance values come different types measurements:
numeric distances computed difference two linear values, normalized
standard deviation, nominal attributes computed sum C differences
probability values (where C number output classes). therefore necessary find
way scale two different kinds measurements approximately range
give variable similar influence overall distance measurement.
Since 95% values normal distribution fall within two standard deviations
mean, difference numeric values divided 4 standard deviations scale
value range usually width 1. function normalized_diff therefore defined
shown Equation 13:
normalized_ diff (x, y) =

xy
4

(13)

standard deviation numeric values attribute a.
Three alternatives function normalized_vdm considered use
heterogeneous distance function. labeled N1, N2 N3, definitions
given below:
N1: normalized_ vdm1a (x, y) =

C N
a,x,c



c=1

N2: normalized_ vdm2 (x, y) =

C N
a,x,c



c=1

9

Na,x

Na,x



Na,y,c



Na,y,c

(14)

Na,y

Na,y

2

(15)

fiWILSON & MARTINEZ

C N
a,x,c

N3: normalized_ vdm3a (x, y) = C *

c=1

Na,x



Na,y,c

2

(16)

Na,y

function N1 Equation (8) q=1. similar formula used PEBLS
(Rachlin et al., 1994) RISE (Domingos, 1995) nominal attributes.
N2 uses q=2, thus squaring individual differences. analogous using Euclidean
distance instead Manhattan distance. Though slightly expensive computationally,
formula hypothesized robust N1 favors class
correlations fairly similar rather close different. N1
would able distinguish two. practice square root taken,
individual attribute distances squared HVDM function.
N3 function used Heterogeneous Radial Basis Function Networks (Wilson &
Martinez, 1996), HVDM first introduced.
3.2. Normalization Experiments
order determine whether normalization scheme N1, N2 N3 gave unfair weight
either nominal linear attributes, experiments run 15 databases machine
learning database repository University California, Irvine (Merz & Murphy, 1996).
datasets experiment least nominal linear attributes,
thus require heterogeneous distance function.
experiment, five-fold cross validation used. five trials,
distance instance test set instance training set
computed. computing distance attribute, normalized_diff function
used linear attributes, normalized_vdm function N1, N2, N3 used (in
three respective experiments) nominal attributes.
average distance (i.e., sum distances divided number comparisons)
computed attribute. average linear attributes database
computed averages listed heading avgLin Table 1.

Database
Anneal
Australian
Bridges
Crx
Echocardiogram
Flag
Heart
Heart.Cleveland
Heart.Hungarian
Heart.Long-Beach-VA
Heart.More
Heart.Swiss
Hepatitis
Horse-Colic
Soybean-Large
Average

avgLin
0.427
0.215
0.328
0.141
0.113
0.188
0.268
0.271
0.382
0.507
0.360
0.263
0.271
0.444
0.309
0.299

N1
avgNom
0.849
0.266
0.579
0.268
0.487
0.372
0.323
0.345
0.417
0.386
0.440
0.390
0.205
0.407
0.601
0.422

N2
avgNom
0.841
0.188
0.324
0.193
0.344
0.195
0.228
0.195
0.347
0.324
0.340
0.329
0.158
0.386
0.301
0.313

N3
avgNom
0.859
0.266
0.808
0.268
0.487
0.552
0.323
0.434
0.557
0.417
0.503
0.421
0.205
0.407
0.872
0.492

#Nom.
29
8
7
9
2
18
6
6
6
6
6
6
13
16
29
11

#Lin.
9
6
4
6
7
10
7
7
7
7
7
7
6
7
6
7

Table 1. Average attribute distance linear nominal attributes.
10

#C
6
2
7
2
2
8
2
5
5
5
5
5
2
2
19
5

fiIMPROVED HETEROGENEOUS DISTANCE FUNCTIONS

Average distance

Average distance

Average distance

averages nominal attributes three normalization schemes
listed headings avgNom Table 1 well. average distance linear
variables exactly regardless whether N1, N2 N3 used, average
given once. Table 1 lists number nominal (#Nom.) number linear
(#Lin.) attributes database, along number output classes (#C).
seen overall averages first four columns last row
Table 1, N2 closer N1 N3. However, important understand reasons behind
difference order know normalization scheme N2 robust general.
Figures 2-4 graphically display averages shown Table 1 headings N1, N2
N3, respectively, ordered left right number output classes.
hypothesized number output classes grows, normalization would get worse
N3 indeed appropriate add scaling factor C sum. length
line indicates much difference
average distance nominal attributes
Nominal
1
linear attributes. ideal normalization scheme
Linear
would difference zero, longer lines
.8
indicate worse normalization.
.6
number output classes grows,
.4
difference N3 linear distances
.2
nominal distances grows wider cases.
N2, hand, seems remain quite close
0
2 2 2 2 2 2 5 5 5 5 5 6 7 8 19 Avg
independent number output classes.
Number output classes
Interestingly, N1 almost poorly N3, even
Figure 2. Average distances N1.
though use scaling factor C.
Apparently squaring factor provides
Nominal
1
well-rounded distance metric nominal attributes
Linear
similar provided using Euclidean distance
.8
instead Manhattan distance linear attributes.
.6
underlying hypothesis behind performing
.4
normalization proper normalization
.2
typically improve generalization accuracy.
nearest neighbor classifier (with k =1)
0
2 2 2 2 2 2 5 5 5 5 5 6 7 8 19 Avg
implemented using HVDM distance metric.
Number output classes
system tested heterogeneous
Figure 3. Average distances N2.
datasets appearing Table 1 using three
different normalization schemes discussed above,
1
Nominal
using ten-fold cross-validation (Schaffer, 1993),
Linear
results summarized Table 2.
.8
normalization schemes used training sets
.6
test sets trial. Bold entries indicate
.4
scheme highest accuracy.
.2
asterisk indicates difference greater
1% next highest scheme.
0
2 2 2 2 2 2 5 5 5 5 5 6 7 8 19 Avg
seen table, normalization
Number output classes
scheme N2 highest accuracy, N1
Figure 4. Average distances N3.
11

fiWILSON & MARTINEZ

substantially lower two. N2
Database
N1
N2
N3
N3 highest accuracy
Anneal
93.98
94.61 94.99
8 domains. significantly, N2
Australian
71.30
81.45 81.59
Bridges
43.36
59.64 59.55
1% higher 5 times compared N1
Crx
70.29
80.87 81.01
1% higher one dataset.
Echocardiogram
70.36
94.82 94.82
N3 higher two
Flag
28.95
55.82* 51.50
Heart.Cleveland
73.88
76.56* 71.61
one dataset, lower average
Heart.Hungarian
70.75
76.85* 75.82
accuracy N2.
Heart.Long-Beach-Va
65.50
65.50 70.00*
results support hypothesis
Heart.More
60.03
72.09 72.48
Heart
88.46
89.49 89.49
normalization scheme N2
Heart.Swiss
74.81
78.52* 75.19
achieves higher generalization accuracy
Hepatitis
73.50
76.67 77.33
N1 N3 (on datasets) due
Horse-Colic
64.75* 60.53 60.53
Soybean-Large
41.45
90.88* 87.89
robust normalization though
Average
66.09
76.95 76.25
accuracy N3 almost good N2.
Note proper normalization
Table 2. Generalization accuracy
using N1, N2 N3.
always necessarily improve generalization
accuracy. one attribute
important others classification, giving higher weight may improve
classification. Therefore, important attribute given higher weight accidentally
poor normalization, may actually improve generalization accuracy. However,
random improvement typically case. Proper normalization improve
generalization cases used typical applications.
consequence results, N2 used normalization scheme HVDM,
function normalized_vdm defined (15).
3.3. Empirical Results HVDM vs. Euclidean HOEM
nearest neighbor classifier (with k=1) using three distance functions listed Table 3
tested 48 datasets UCI machine learning database repository. 48 datasets,
results obtained 35 datasets least nominal attributes shown
Table 3.
results approximately equivalent datasets linear attributes, results
remaining datasets shown here, found Section 6. 10-fold crossvalidation used, three distance metrics used training sets test sets
trial.
results experiments shown Table 3. first column lists name
database (.test means database originally meant used test set,
instead used entirety separate database). second column shows results
obtained using Euclidean distance function normalized standard deviation
attributes, including nominal attributes. next column shows generalization accuracy
obtained using HOEM metric, uses range-normalized Euclidean distance linear
attributes overlap metric nominal attributes. final column shows accuracy
obtained using HVDM distance function uses standard-deviation-normalized
Euclidean distance (i.e., normalized_diff defined Equation 13) linear attributes
normalized_vdm function N2 nominal attributes.
highest accuracy obtained database shown bold. Entries Euclid.
12

fiIMPROVED HETEROGENEOUS DISTANCE FUNCTIONS

HOEM columns significantly
higher HVDM (at 90% higher
confidence level, using two-tailed
paired test) marked
asterisk (*).
Entries
significantly lower HVDM
marked less-than sign (<).
seen Table 3,
HVDM distance functions overall
average accuracy higher
two metrics 3%.
HVDM achieved high higher
generalization accuracy
two distance functions 21 35
datasets. Euclidean distance
function highest 18 datasets,
HOEM highest 12
datasets.
HVDM significantly higher
Euclidean distance function
10 datasets, significantly lower
3. Similarly, HVDM higher
HOEM 6 datasets,
significantly lower 4.
results support hypothesis
HVDM handles nominal attributes
appropriately Euclidean
distance heterogeneous
Euclidean-overlap metric, thus
tends achieve higher generalization
accuracy typical applications.

Database
Euclid.
Anneal
94.99
Audiology
60.50 <
Audiology.Test
41.67 <
Australian
80.58
Bridges
58.64
Crx
78.99
Echocardiogram
94.82
Flag
48.95 <
Heart.Cleveland
73.94
Heart.Hungarian
73.45 <
Heart.Long-Beach-Va 71.50
Heart.More
72.09
Heart.Swiss
93.53 *
Hepatitis
77.50
Horse-Colic
65.77
House-Votes-84
93.12 <
Image.Segmentation
92.86
Led+17
42.90 <
Led-Creator
57.20 *
Monks-1.Test
77.08
Monks-2.Test
59.04 <
Monks-3.Test
87.26 <
Mushroom
100.00
Promoters
73.73 <
Soybean-Large
87.26 <
Soybean-Small
100.00
Thyroid.Allbp
94.89
Thyroid.Allhyper
97.00
Thyroid.Allhypo
90.39
Thyroid.Allrep
96.14
Thyroid.Dis
98.21
Thyroid.Hypothyroid 93.42
Thyroid.Sick-Euthyroid 68.23
Thyroid.Sick
86.93 *
Zoo
97.78
Average:
79.44

HOEM
94.61
72.00 <
75.00
81.16
53.73
81.01
94.82
48.84
74.96
74.47
71.00 *
71.90
91.86
77.50
60.82
93.12 <
93.57
42.90 <
57.20 *
69.43
54.65 <
78.49 <
100.00
82.09 <
89.20
100.00
94.89
97.00
90.39 *
96.14
98.21
93.42
68.23
86.89 *
94.44
80.11

HVDM
94.61
77.50
78.33
81.45
59.64
80.87
94.82
55.82
76.56
76.85
65.50
72.09
89.49
76.67
60.53
95.17
92.86
60.70
56.40
68.09
97.50
100.00
100.00
92.36
90.88
100.00
95.00
96.86
90.29
96.11
98.21
93.36
68.23
86.61
98.89
83.38

Table 3. Generalization accuracy
Euclidean, HOEM, HVDM distance functions.

4. Interpolated Value Difference Metric (IVDM)
section Section 5 introduce distance functions allow VDM applied
directly continuous attributes. alleviates need normalization attributes.
cases provides better measure distance continuous attributes linear
distance.
example, consider application input attribute height output class
indicates whether person good candidate fighter pilot particular airplane.
individuals heights significantly preferred height might
considered poor candidates, thus could beneficial consider heights
similar preferred height, even though farther apart
linear sense.

13

fiWILSON & MARTINEZ

hand, linear attributes linearly distant values tend indicate different
classifications handled appropriately. Interpolated Value Difference Metric
(IVDM) handles situations, handles heterogeneous applications robustly.
generic version VDM distance function, called discretized value difference
metric (DVDM) used comparisons extensions VDM presented paper.
4.1. IVDM Learning Algorithm
original value difference metric (VDM) uses statistics derived training set
instances determine probability Pa,x,c output class c given input value x
attribute a.
using IVDM, continuous values discretized equal-width intervals (though
continuous values retained later use), integer supplied user.
Unfortunately, currently little guidance value use. value
large reduce statistical strength values P, value small allow
discrimination among classes. purposes paper, use heuristic
determine automatically: let 5 C, whichever greatest, C number
output classes problem domain. Current research examining sophisticated
techniques determining good values s, cross-validation, statistical
methods (e.g., Tapia & Thompson, 1978, p. 67). (Early experimental results indicate
value may critical long C n, n number instances
training set.)
width wa discretized interval attribute given by:
wa =

maxa mina


(17)

max mina maximum minimum value, respectively, occurring
training set attribute a.
example, consider Iris database UCI machine learning databases.
Iris database four continuous input attributes, first sepal length. Let
training set consisting 90% 150 available training instances, test set
consisting remaining 10%.
one division training set, values sepal length attribute ranged
4.3 7.9. three output classes database, let s=5, resulting
width |7.9 - 4.3| / 5 = 0.72. Note since discretization part learning process,
would unfair use instances test set help determine discretize
values. discretized value v continuous value x attribute integer 1 s,
given by:
x, discrete, else

v = discretizea (x) = s, x = max , else
(x min ) / w + 1




(18)

deciding upon finding w a, discretized values continuous attributes
14

fiIMPROVED HETEROGENEOUS DISTANCE FUNCTIONS

used discrete values nominal attributes finding Pa,x,c. Figure 5 lists pseudo-code
done.
LearnP(training set T)
attribute
instance
Let x input value attribute instance i.
v = discretizea(x) [which x discrete]
Let c output class instance i.
Increment Na,v,c 1.
Increment Na,v 1.
discrete value v (of attribute a)
class c
Na,v=0
Pa,v,c=0
Else Pa,v,c = Na,v,c / Na,v
Return 3-D array Pa,v,c.

Figure 5. Pseudo code finding Pa,x,c.

Probability

first attribute Iris database, values Pa,x,c displayed Figure 6.
five discretized ranges x, probability three corresponding
output classes shown bar heights. Note heights three bars sum 1.0
discretized range. bold integers indicate discretized value range.
example, sepal length greater equal 5.74 less 6.46 would discretized
value 3.
1.0

1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

.867

Output Class:
1. Iris
Setosa

.609
.485
.455

.100
.033
4.3

1

5.02

.474 .500

3. Iris
Viginica

.061 .026

2

2. Iris
Versicolor

.391

0.0

3

5.74

6.46

4

Sepal Length (in cm)

0.0 0.0
7.18 5

7.9

Bold =
discretized
range number.

Figure 6. Pa,x,c a=1, x=1..5, c=1..3, first attribute Iris database.
4.2. IVDM DVDM Generalization
Thus far DVDM IVDM algorithms learn identically. However, point DVDM
algorithm need retain original continuous values use discretized
values generalization. hand, IVDM use continuous values.
generalization, algorithm nearest neighbor classifier use distance
function DVDM, defined follows:
DVDM(x, y) =



vdma (discretizea (xa ), discretizea (ya ))

a=1

15

2

(19)

fiWILSON & MARTINEZ

discretizea defined Equation (18) vdma defined Equation (8),
q=2. repeat convenience:
vdma (x, y) =

C

Pa,x,c Pa,y,c

2

(20)

c=1

Unknown input values (Quinlan, 1989) treated simply another discrete value, done
(Domingos, 1995).

A:
B:

1
5.0
5.7

y:

5.1

Input Attributes
2
3
3.6
1.4
2.8
4.5
3.8

1.9

4
0.2
1.3

->
->

Output Class
1 (Iris Setosa)
2 (Iris Versicolor)

0.4

Table 4. Example Iris database.
example, consider two training instances B shown Table 4, new
input vector classified. attribute a=1, discretized values A, B, 1, 2,
2, respectively. Using values Figure 6, distance attribute 1 is:
|.867-.485|2 + |.1-.455|2 + |.033-.061|2 = .273

Probability Class 2

distance B 0, since discretized value.
Note B values different ends range 2, actually nearly
close are. spite fact, discretized distance function says B
equal happen fall discretized range.
IVDM uses interpolation alleviate problems. IVDM assumes Pa,x,c values
hold true midpoint range, interpolates midpoints find P
attribute values.
Figure 7 shows P values second output class (Iris Versicolor) function
first attribute value (sepal length). dashed line indicates P value used DVDM,
solid line shows IVDM uses.
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

Center
points
DVDM
IVDM

4

4.3

1

5.02 2

5.74 3 6.46 4
7.18 5
Sepal Length (in cm)

7.9

Bold =
discretized
range number.

Figure 7. P1,x,2 values DVDM IVDM attribute 1, class 2 Iris database.
16

fiIMPROVED HETEROGENEOUS DISTANCE FUNCTIONS

distance function Interpolated Value Difference Metric defined as:
IVDM(x, y) =



ivdma (xa , ya )2

(21)

a=1

ivdma defined as:
discrete
vdma (x, y),
C
2
ivdma (x, y) =
p (x) pa,c (y) , otherwise
a,c
c=1

(22)

formula determining interpolated probability value pa,c(x) continuous value x
attribute class c is:


x mida,u
pa,c (x) = Pa,u,c +
* (Pa,u+1,c Pa,u,c )
mida,u+1 mida,u

(23)

equation, mida,u mida,u+1 midpoints two consecutive discretized ranges
mida,u x < mida,u+1. Pa,u,c probability value discretized range u,
taken probability value midpoint range u (and similarly Pa,u+1,c).
value u found first setting u = discretizea(x), subtracting 1 u x < mida,u.
value mida,u found follows:
mida,u = mina + widtha * (u+.5)

(24)

Probability Class

Figure 8 shows values pa,c(x) attribute a=1 Iris database three output
classes (i.e. c=1, 2, 3). Since data points outside range mina..maxa,
probability value Pa,u,c taken 0 u < 1 u > s, seen visually
diagonal lines sloping toward zero outer edges graph. Note sum
probabilities three output classes sum 1.0 every point midpoint range 1
midpoint range 5.
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

Output Class:
1. Iris Setosa
2. Iris Versicolor
3. Iris Viginica

4

4.3

1

5.02 2

5.74 3 6.46 4
7.18 5
Sepal Length (in cm)

7.9

Bold =
discretized
range number.

Figure 8. Interpolated probability values attribute 1 Iris database.
17

fiWILSON & MARTINEZ


B

value
5.0
5.7

p1,1 (v)
.687
.281

p1,2 (v)
.268
.463

p1,3 (v)
.046
.256



5.1

.634

.317

.050

ivdm1(v,y)
.005
.188

vdm1(v,y)
.273
0

Table 5. Example ivdm vs. vdm.
Using IVDM example instances Table 4, values first attribute
discretized DVDM, used find interpolated probability values.
example, value 5.1, p1,c(x) interpolates midpoints 1 2, returning
values shown Table 5 three classes. Instance value 5.0,
falls midpoints 1 2, instance B value 5.7, falls
midpoints 2 3.
seen Table 5, IVDM (using single-attribute distance function ivdm)
returns distance indicates closer B (for first attribute),
certainly case here. DVDM (using
discretized vdm), hand, returns
Database
DVDM
IVDM
Annealing
94.99
96.11 *
distance indicates value
Australian
83.04 *
80.58
equal B, quite far A,
Bridges
56.73
60.55
illustrating problems involved
Credit Screening
80.14
80.14
Echocardiogram
100.00
100.00
using discretization.
Flag
58.76
57.66
IVDM DVDM algorithms
Glass
56.06
70.54 *
implemented tested 48 datasets
Heart Disease
80.37
81.85
Heart (Cleveland)
79.86
78.90
UCI machine learning databases.
Heart
(Hungarian)
81.30
80.98
results 34 datasets contain
Heart (Long-Beach-Va) 71.00
66.00
least continuous attributes
Heart (More)
72.29
73.33
shown Table 6. (Since IVDM
Heart (Swiss)
88.59
87.88
Hepatitis
80.58
82.58
DVDM equivalent domains
Horse-Colic
76.75
76.78
discrete attributes, results
Image Segmentation
92.38
92.86
remaining datasets deferred Section
Ionosphere
92.60
91.17
Iris
92.00
94.67
6.) 10-fold cross-validation
Liver Disorders
55.04
58.23
used, average accuracy
Pima Indians Diabetes
71.89
69.28
database 10 trials shown
Satellite Image
87.06
89.79 *
Shuttle
96.17
99.77 *
Table 6. Bold values indicate value
Sonar
78.45
84.17
highest dataset. Asterisks (*)
Thyroid (Allbp)
94.86
95.32
indicates difference statistically
Thyroid (Allhyper)
96.93
97.86 *
Thyroid (Allhypo)
89.36
96.07 *
significant 90% confidence level
Thyroid (Allrep)
96.86
98.43 *
higher, using two-tailed paired t-test.
Thyroid (Dis)
98.29
98.04
set datasets, IVDM
Thyroid (Hypothyroid)
93.01
98.07 *
higher average generalization accuracy
Thyroid (Sick)
88.24
95.07 *
Thyroid (Sick-Euthyroid) 88.82
96.86 *
overall discretized algorithm.
Vehicle
63.72
69.27 *
IVDM obtained higher generalization
Vowel
91.47
97.53 *
accuracy DVDM 23 34
Wine
94.38
97.78 *
Average:
83.08
85.22
cases, 13 significant
90% level above. DVDM higher
Table 6. Generalization DVDM vs. IVDM.
18

fiIMPROVED HETEROGENEOUS DISTANCE FUNCTIONS

accuracy 9 cases, one difference statistically significant.
results indicate interpolated distance function typically appropriate
discretized value difference metric applications one continuous
attributes. Section 6 contains comparisons IVDM distance functions.

5. Windowed Value Difference Metric (WVDM)
IVDM algorithm thought sampling value Pa,u,c midpoint mida,u
discretized range u. P sampled first finding instances value
attribute range mida,u w / 2. Na,u incremented instance,
N a,u,c incremented instance whose output class c,
P a,u,c = Na,u,c / Na,u computed. IVDM interpolates sampled points
provide continuous rough approximation function pa,c(x). possible sample P
points thus provide closer approximation function pa,c(x), may
turn provide accurate distance measurements values.
Figure 9 shows pseudo-code Windowed Value Difference Metric (WVDM).
WVDM samples value P a,x,c value x occurring training set
Define:
instance[a][1..n] list n instances sorted ascending order attribute a.
instance[a][i].val[a] value attribute instance[a][i].
x
center value current window, i.e., x=instance[a][i].val[a].
p[a][i][c]
probability Pa,x,c output class c given input value x
attribute a. Note index, value itself.
N[c]
number Na,x,c instances current window output class c.
N
total number Na,x instances current window.
instance[a][in] first instance window.
instance[a][out] first instance outside window. (i.e., window contains
instances instance[a][in..out-1]).
w[a]
window width attribute a.
LearnWVDM(training set T)
continuous attribute
Sort instance[a][1..n] ascending order attribute a, using quicksort.
Initialize N N[c] 0, 1 (i.e., start empty window).
i=1..n
Let x=instance[a][i].val[a].
// Expand window include instances range
(out < n) (instance[a][out].val[a] < (x + w[a]/2))
Increment N[c], c=the class instance[a][out].
Increment N.
Increment out.
// Shrink window exclude instances longer range
(in < out) (instance[a][in].val[a] < (x - w[a]/2))
Decrement N[c], c=the class instance[a][in].
Decrement N.
Increment in.
// Compute probability value class current window
class c=1..C
p[a][i][c] = N[c] / N. (i.e., Pa,x,c = Na,x,c / Na,x).
Return 3-D array p[a][i][c].

Figure 9. Pseudo code WVDM learning algorithm.
19

fiWILSON & MARTINEZ

attribute a, instead midpoints range. fact, discretized ranges
even used WVDM continuous attributes, except determine appropriate window
width, wa, range width used DVDM IVDM. pseudo-code
learning algorithm used determine Pa,x,c attribute value x given Figure 9.
value x occurring training set attribute a, P sampled finding
instances value attribute range x w / 2, computing Na,x,
Na,x,c, Pa,x,c = Na,x,c / Na,x before. Thus, instead fixed number sampling
points, window instances, centered training instance, used determining
probability given point. technique similar concept shifted histogram estimators
(Rosenblatt, 1956) Parzen window techniques (Parzen, 1962).
attribute values sorted (using O(nlogn) sorting algorithm) allow
sliding window used thus collect needed statistics O(n) time attribute.
sorted order retained attribute binary search performed O(log
n) time generalization.
Values occurring sampled points interpolated IVDM, except
many points available, new value interpolated two
closer, precise values IVDM.
WVDM_Find_P(attribute a,continuous value x)
// Find Pa,x,c c=1..C, given value x attribute a.
Find instance[a][i].val[a] x instance[a][i+1].val[a] (binary search).
x1 = instance[a][i].val[a]
(unless i<1, case x1=min[a] - (w[a] / 2))
x2 = instance[a][i+1].val[a] (unless i>n, case x2=max[a] + (w[a] / 2))
class c=1..C
p1=p[a][i][c]
(unless i<1, case p1=0)
p2=p[a][i+1][c] (unless i>n, case p2=0)
Pa,x,c = p1 + ((x-x1)/(x2-x1)) * (p2 - p1)
Return array Pa,x,1..C.

Figure 10. Pseudo-code WVDM probability interpolation (see Figure 9 definitions).
pseudo-code interpolation algorithm given Figure 10. algorithm takes
value x attribute returns vector C probability values Pa,x,c c=1..C. first
binary search find two consecutive instances sorted list instances
attribute surround x. probability class interpolated
stored two surrounding instances. (The exceptions noted parenthesis handle
outlying values interpolating towards 0 done IVDM.)
probability values input vectors attribute values computed,
used vdm function discrete probability values are.
WVDM distance function defined as:
WVDM(x, y) =



wvdma (xa , ya )2

(25)

a=1

wvdma defined as:
discrete
vdma (x, y),
C
2
wvdma (x, y) =
P
Pa,y,c , otherwise
a,x,c
c=1

20

(26)

fiProbability Class

IMPROVED HETEROGENEOUS DISTANCE FUNCTIONS

1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

Output Class:
1. Iris Setosa
2. Iris Versicolor
3. Iris Viginica

4

5

6

7

8

Sepal Length (in cm)

Figure 11. Example WVDM probability landscape.
Pa,x,c interpolated probability value continuous value x computed
Figure 10. Note typically finding distance new input vector
instance training set. Since
instances training set used
Database
DVDM WVDM
define probability attribute
Annealing
94.99
95.87
Australian
83.04
82.46
values, binary search interpolation
Bridges
56.73
56.64
unnecessary training instances
Credit Screening
80.14
81.45
immediately recall stored
Echocardiogram
100.00
98.57
probability values, unless pruning techniques
Flag
58.76
58.74
Glass
56.06
71.49 *
used.
Heart Disease
80.37
82.96
One drawback approach
Heart (Cleveland)
79.86
80.23
increased storage needed retain C
Heart (Hungarian)
81.30
79.26
Heart (Long-Beach-Va) 71.00
68.00
probability values attribute value
Heart (More)
72.29
73.33
training set. Execution time
Heart (Swiss)
88.59
88.72
significantly increased IVDM
Hepatitis
80.58
79.88
Horse-Colic
76.75
74.77
DVDM. (See Section 6.2 discussion
Image
Segmentation
92.38
93.33
efficiency considerations).
Ionosphere
92.60
91.44
Figure 11 shows probability values
Iris
92.00
96.00
Liver Disorders
55.04
57.09
three classes first attribute
Pima Indians Diabetes
71.89
70.32
Iris database again, time using
Satellite Image
87.06
89.33 *
windowed sampling technique. Comparing
Shuttle
96.17
99.61 *
Figure 11 Figure 8 reveals
Sonar
78.45
84.19
Thyroid (Allbp)
94.86
95.29
attribute IVDM provides approximately
Thyroid (Allhyper)
96.93
97.50
overall shape, misses much
Thyroid (Allhypo)
89.36
90.18
detail. example, peak occurring
Thyroid (Allrep)
96.86
97.07
Thyroid (Dis)
98.29
98.00
output class 2 approximately sepal
Thyroid (Hypothyroid)
93.01
96.96 *
length=5.75. Figure 8 flat line
Thyroid (Sick)
88.24
97.11 *
misses peak entirely, due mostly
Thyroid (Sick-Euthyroid) 88.82
94.40 *
Vehicle
63.72
65.37 *
somewhat arbitrary position
Vowel
91.47
96.21 *
midpoints probability values
Wine
94.38
97.22 *
sampled.
Average:
83.08
84.68
Table 7 summarizes results testing
Table 7. Generalization WVDM vs. DVDM.
21

fiWILSON & MARTINEZ

WVDM algorithm datasets DVDM IVDM. bold entry indicates
highest two accuracy measurements, asterisk (*) indicates difference
statistically significant 90% confidence level, using two-tailed paired t-test.
set databases, WVDM average 1.6% accurate DVDM
overall. WVDM higher average accuracy DVDM 23 34 databases,
significantly higher 9, DVDM higher 11 databases, none
differences statistically significant.
Section 6 provides comparisons WVDM distance functions, including
IVDM.

6. Empirical Comparisons Analysis Distance Functions
section compares distance functions discussed paper. nearest neighbor
classifier implemented using six different distance functions: Euclidean
(normalized standard deviation) HOEM discussed Section 2; HVDM discussed
Section 3; DVDM IVDM discussed Section 4; WVDM discussed Section
5. Figure 12 summarizes definition distance function.
functions use
overall distance function:

D(x, y) =



da (xa , ya )2

a=1

Distance
Function
Euclidean

Definition da(xa,ya) attribute type:
Linear
Continuous
Discrete Nominal
xa ya
xa ya



HOEM

xa ya
rangea

0 xa = ya
1 xa ya

HVDM

xa ya
4

vdma (xa , ya )

DVDM

vdma(disca(xa),disca(ya))

vdma (xa , ya )

IVDM

ivdma(xa,ya)
Interpolate probabilities
range midpoints.

vdma (xa , ya )

WVDM

wvdma(xa,ya)
Interpolate probabilities
adjacent values.

vdma (xa , ya )

rangea = maxa mina , vdma (x, y) =

C

Pa,x,c Pa,y,c

2

c=1

Figure 12. Summary distance function definitions.
distance function tested 48 datasets UCI machine learning databases,
22

fiIMPROVED HETEROGENEOUS DISTANCE FUNCTIONS

using 10-fold cross-validation. average accuracy 10 trials reported
test Table 8. highest accuracy achieved dataset shown bold.
names three new distance functions presented paper (HVDM, IVDM WVDM)
shown bold identify them.
Table 8 lists number instances database (#Inst.), number
continuous (Con), integer (Int, i.e., linear discrete), nominal (Nom) input attributes.

Database
Euclid HOEM
Annealing
94.99
94.61
Audiology
60.50
72.00
Audiology (test)
41.67
75.00
Australian
80.58
81.16
Breast Cancer
94.99
95.28
Bridges
58.64
53.73
Credit Screening
78.99
81.01
Echocardiogram
94.82
94.82
Flag
48.95
48.84
Glass
72.36
70.52
Heart Disease
72.22
75.56
Heart (Cleveland)
73.94
74.96
Heart (Hungarian)
73.45
74.47
Heart (Long-Beach-Va)
71.50
71.00
Heart (More)
72.09
71.90
Heart (Swiss)
93.53
91.86
Hepatitis
77.50
77.50
Horse-Colic
65.77
60.82
House-Votes-84
93.12
93.12
Image Segmentation
92.86
93.57
Ionosphere
86.32
86.33
Iris
94.67
95.33
LED+17 noise
42.90
42.90
LED
57.20
57.20
Liver Disorders
62.92
63.47
Monks-1
77.08
69.43
Monks-2
59.04
54.65
Monks-3
87.26
78.49
Mushroom
100.00 100.00
Pima Indians Diabetes
71.09
70.31
Promoters
73.73
82.09
Satellite Image
90.21
90.24
Shuttle
99.78
99.78
Sonar
87.02
86.60
Soybean (Large)
87.26
89.20
Soybean (Small)
100.00 100.00
Thyroid (Allbp)
94.89
94.89
Thyroid (Allhyper)
97.00
97.00
Thyroid (Allhypo)
90.39
90.39
Thyroid (Allrep)
96.14
96.14
Thyroid (Dis)
98.21
98.21
Thyroid (Hypothyroid)
93.42
93.42
Thyroid (Sick-Euthyroid) 68.23
68.23
Thyroid (Sick)
86.93
86.89
Vehicle
70.93
70.22
Vowel
99.24
98.86
Wine
95.46
95.46
Zoo
97.78
94.44
Average:
80.78
81.29

n c e
HVDM
94.61
77.50
78.33
81.45
94.99
59.64
80.87
94.82
55.82
72.36
78.52
76.56
76.85
65.50
72.09
89.49
76.67
60.53
95.17
92.86
86.32
94.67
60.70
56.40
62.92
68.09
97.50
100.00
100.00
71.09
92.36
90.21
99.78
87.02
90.88
100.00
95.00
96.86
90.29
96.11
98.21
93.36
68.23
86.61
70.93
99.24
95.46
98.89
83.79

F u n
DVDM
94.99
77.50
78.33
83.04
95.57
56.73
80.14
100.00
58.76
56.06
80.37
79.86
81.30
71.00
72.29
88.59
80.58
76.75
95.17
92.38
92.60
92.00
60.70
56.40
55.04
68.09
97.50
100.00
100.00
71.89
92.36
87.06
96.17
78.45
92.18
100.00
94.86
96.93
89.36
96.86
98.29
93.01
88.24
88.82
63.72
91.47
94.38
98.89
84.06

c n
# inputs
IVDM WVDM #Inst. Con Int Nom
96.11
95.87
798
6 3
29
77.50
77.50
200
0 0
69
78.33
78.33
26
0 0
69
80.58
82.46
690
6 0
8
95.57
95.57
699
0 9
0
60.55
56.64
108
1 3
7
80.14
81.45
690
6 0
9
100.00
98.57
132
7 0
2
57.66
58.74
194
3 7
18
70.54
71.49
214
9 0
0
81.85
82.96
270
5 2
6
78.90
80.23
303
5 2
6
80.98
79.26
294
5 2
6
66.00
68.00
200
5 2
6
73.33
73.33 1541
5 2
6
87.88
88.72
123
5 2
6
82.58
79.88
155
6 0
13
76.78
74.77
301
7 0
16
95.17
95.17
435
0 0
16
92.86
93.33
420 18 0
1
91.17
91.44
351 34 0
0
94.67
96.00
150
4 0
0
60.70
60.70 10000
0 0
24
56.40
56.40 1000
0 0
7
58.23
57.09
345
6 0
0
68.09
68.09
432
0 0
6
97.50
97.50
432
0 0
6
100.00 100.00
432
0 0
6
100.00 100.00 8124
0 1
21
69.28
70.32
768
8 0
0
92.36
92.36
106
0 0
57
89.79
89.33 4435 36 0
0
99.77
99.61 9253
9 0
0
84.17
84.19
208 60 0
0
92.18
92.18
307
0 6
29
100.00 100.00
47
0 6
29
95.32
95.29 2800
6 0
22
97.86
97.50 2800
6 0
22
96.07
90.18 2800
6 0
22
98.43
97.07 2800
6 0
22
98.04
98.00 2800
6 0
22
98.07
96.96 3163
7 0
18
95.07
94.40 3163
7 0
18
96.86
97.11 2800
6 0
22
69.27
65.37
846 18 0
0
97.53
96.21
528 10 0
0
97.78
97.22
178 13 0
0
98.89
98.89
90
0 0
16
85.56
85.24

Table 8. Summary Generalization Accuracy

23

fiWILSON & MARTINEZ

set 48 datasets, three new distance functions (HVDM, IVDM WVDM)
substantially better Euclidean distance HOEM. IVDM highest average accuracy
(85.56%) almost 5% higher average Euclidean distance (80.78%), indicating
robust distance function datasets, especially nominal
attributes. WVDM slightly lower IVDM 85.24% accuracy. Somewhat
surprisingly, DVDM slightly higher HVDM datasets, even though uses
discretization instead linear distance continuous attributes. four VDM-based
distance functions outperformed Euclidean distance HOEM.
48 datasets, Euclidean distance highest accuracy 11 times; HOEM
highest 7 times; HVDM, 14; DVDM, 19; IVDM, 25 WVDM, 18.
datasets continuous attributes, four VDM-based distance functions
(HVDM, DVDM, IVDM WVDM) equivalent. datasets, VDM-based
distance functions achieve average accuracy 86.6% compared 78.8% HOEM
76.6% Euclidean, indicating substantial superiority problems.
datasets nominal attributes, Euclidean HVDM equivalent,
distance functions perform average except DVDM, averages
4% less others, indicating detrimental effects discretization. Euclidean
HOEM similar definitions applications without nominal attributes, except
Euclidean normalized standard deviation HOEM normalized range
attribute. interesting average accuracy datasets slightly higher
Euclidean HOEM, indicating standard deviation may provide better normalization
datasets. However, difference small (less 1%), datasets
contain many outliers, difference probably negligible case.
One disadvantage scaling attributes standard deviation attributes
almost always value (e.g., boolean attribute almost always 0)
given large weightnot due scale, relative frequencies attribute
values. related problem occur HVDM. skewed class distribution
(i.e., many instances classes others), P values quite
small classes quite large others, either case difference |Pa,x,c - Pa,y,c|
correspondingly small, thus nominal attributes get little weight
compared linear attributes. phenomenon noted Ting (1994, 1996),
recognized problems hypothyroid dataset. Future research address
normalization problems look automated solutions. Fortunately, DVDM, IVDM
WVDM suffer either problem, attributes scaled amount
cases, may part account success HVDM
experiments.
datasets nominal continuous attributes, HVDM slightly higher
Euclidean distance datasets, turn slightly higher HOEM, indicating
overlap metric may much improvement heterogeneous databases.
DVDM, IVDM WVDM higher Euclidean distance datasets,
IVDM lead.
6.1. Effects Sparse Data
Distance functions use VDM require statistics determine distance. therefore
hypothesized generalization accuracy might lower VDM-based distance functions
24

fiIMPROVED HETEROGENEOUS DISTANCE FUNCTIONS

Euclidean distance HOEM little data available, VDMbased functions would increase accuracy slowly others instances
made available, sufficient number instances allowed reasonable sample size
determine good probability values.

85.00

%Average Generalization Accuracy

80.00

75.00

Euclidean
70.00
HOEM
HVDM

65.00

DVDM
IVDM

60.00

WVDM
55.00
0

20

40
60
%Instances Used

80

100

Figure 13. Average accuracy amount data increases.
test hypothesis, experiments used obtain results shown Table 8
repeated using part available training data. Figure 13 shows generalization
accuracy test set improves percentage available training instances used
learning generalization increased 1% 100%. generalization accuracy values
shown averages 48 datasets Table 8.
Surprisingly, VDM-based distance functions increased accuracy fast faster
Euclidean HOEM even little data available. may
little data available, random positioning sample data input space
greater detrimental affect accuracy error statistical sampling VDM-based
functions.
interesting note Figure 13 six distance functions seem pair
three distinct pairs. interpolated VDM-based distance functions (IVDM WVDM)
maintain highest accuracy, two VDM-based functions next, functions
based linear overlap distance remain lowest early graph.
25

fiWILSON & MARTINEZ

6.2. Efficiency Considerations
section considers storage requirements, learning speed, generalization speed
algorithms presented paper.
6.2.1. STORAGE
distance functions must store entire training set, requiring O(nm) storage,
n number instances training set number input attributes
application, unless instance pruning technique used. Euclidean HOEM
functions, necessary, even amount storage restrictive n
grows large.
HVDM, DVDM, IVDM, probabilities Pa,x,c attributes (only discrete
attributes HVDM) must stored, requiring O(mvC) storage, v average number
attribute values discrete (or discretized) attributes C number output
classes application. possible instead store array Da,x,y = vdma(x,y) HVDM
DVDM, storage would O(mv2), savings C < v.
WVDM, C probability values must stored continuous attribute value,
resulting O(nmC) storage typically much larger O(mvC) n usually
much larger v (and cannot less). necessary store list (pointers to)
instances attribute, requiring additional O(mn) storage. Thus total storage
WVDM O((C+2)nm) = O(Cnm).
Distance Function
Euclidean
HOEM
HVDM
DVDM
IVDM
WVDM

Storage
O(mn)
O(mn)
O(mn+mvC)
O(mn+mvC)
O(mn+mvC)
O(Cmn)

Learning Time
O(mn)
O(mn)
O(mn+mvC)
O(mn+mvC)
O(mn+mvC)
O(mnlogn+mvC)

Generalization Time
O(mn)
O(mn)
O(mnC) O(mn)
O(mnC) O(mn)
O(mnC) O(mn)
O(mnC)

Table 9. Summary efficiency six distance metrics.
Table 9 summarizes storage requirements system. WVDM one
distance functions requires significantly storage others.
applications, n critical factor, distance functions could used
conjunction instance pruning techniques reduce storage requirements. See Section 7
list several techniques reduce number instances retained training set
subsequent generalization.
6.2.2. L EARNING SPEED
takes nm time read training set. takes additional 2nm time find standard
deviation attributes Euclidean distance, nm time find ranges HOEM.
Computing VDM statistics HVDM, DVDM IVDM takes mn+mvC time,
approximately O(mn). Computing WVDM statistics takes mnlogn+mnC time,
approximately O(mnlogn).
general, learning time quite acceptable distance functions.
26

fiIMPROVED HETEROGENEOUS DISTANCE FUNCTIONS

6.2.3. GENERALIZATION SPEED
Assuming distance function must compare new input vector training instances,
Euclidean HOEM take O(mn) time. HVDM, IVDM DVDM take O(mnC) (unless
Da,x,y stored instead Pa,x,c HVDM, case search done O(mn)
time). WVDM takes O(logn+mnC) = O(mnC) time.
Though C typically fairly small, generalization process require
significant amount time and/or computational resources n grows large. Techniques
k-d trees (Deng & Moore, 1995; Wess, Althoff & Derwand, 1993; Sproull, 1991)
projection (Papadimitriou & Bentley, 1980) reduce time required locate nearest
neighbors training set, though algorithms may require modification handle
continuous nominal attributes. Pruning techniques used reduce storage (as Section
6.2.1) reduce number instances must searched generalization.

7. Related Work
Distance functions used variety fields, including instance-based learning, neural
networks, statistics, pattern recognition, cognitive psychology (see Section 1
references). Section 2 lists several commonly-used distance functions involving numeric
attributes.
Normalization often desirable using linear distance function Euclidean
distance attributes arbitrarily get weight others. Dividing
range standard deviation normalize numerical attributes common practice. Turney
(1993; Turney & Halasz, 1993) investigated contextual normalization, standard
deviation mean used normalization continuous attributes depend context
input vector obtained. paper attempt use contextual
normalization, instead use simpler methods normalizing continuous attributes,
focus normalize appropriately continuous nominal attributes.
Value Distance Metric (VDM) introduced Stanfill & Waltz (1986). uses
attribute weights used functions presented paper. Modified Value
Difference Metric (MVDM) (Cost & Salzberg, 1993; Rachlin et al., 1994) use attribute
weights instead uses instance weights. assumed systems use discretization
(Lebowitz, 1985; Schlimmer, 1987) handle continuous attributes.
Ventura (1995; Ventura & Martinez, 1995) explored variety discretization methods
use systems use discrete input attributes. found using discretization
preprocess data often degraded accuracy, recommended machine learning algorithms
designed handle continuous attributes directly.
Ting (1994, 1996) used several different discretization techniques conjunction
MVDM IB1 (Aha, Kibler & Albert, 1991). results showed improved generalization
accuracy using discretization. Discretization allowed algorithm use MVDM
attributes instead using linear distance continuous attributes, thus avoided
normalization problems discussed Sections 3.1 3.2. paper, similar
results seen slightly higher results DVDM (which discretizes continuous
attributes uses VDM) compared HVDM (which uses linear distance
continuous attributes). paper, DVDM uses equal-width intervals discretization,
27

fiWILSON & MARTINEZ

Tings algorithms make use advanced discretization techniques.
Domingos (1995) uses heterogeneous distance function similar HVDM RISE
system, hybrid rule instance-based learning system. However, RISE uses normalization
scheme similar N1 Sections 3.1 3.2, square individual attribute
distances.
Mohri & Tanaka (1994) use statistical technique called Quantification Method II (QM2)
derive attribute weights, present distance functions handle nominal
continuous attributes. transform nominal attributes values boolean
attributes, one time, weights attribute actually
correspond individual attribute values original data.
Turney (1994) addresses cross-validation error voting (i.e. using values k > 1)
instance-based learning systems, explores issues related selecting parameter k (i.e.,
number neighbors used decide classification). paper use k = 1 order
focus attention distance functions themselves, accuracy would improved
applications using k > 1.
IVDM WVDM use nonparametric density estimation techniques (Tapia & Thompson,
1978) determining values P use computing distances. Parzen windows (Parzen,
1962) shifting histograms (Rosenblatt, 1956) similar concept techniques,
especially WVDM. techniques often use gaussian kernels advanced
techniques instead fixed-sized sliding window. experimented gaussianweighted kernels well results slightly worse either WVDM IVDM, perhaps
increased overfitting.
paper applies distance function problem classification, input
vector mapped discrete output class. distance functions could used
systems perform regression (Atkeson, Moore & Schaal, 1996; Atkeson, 1989; Cleveland &
Loader, 1994), output real value, often interpolated nearby points,
kernel regression (Deng & Moore, 1995).
mentioned Section 6.2 elsewhere, pruning techniques used reduce
storage requirements instance-based systems improve classification speed. Several
techniques introduced, including IB3 (Aha, Kibler & Albert, 1991; Aha, 1992),
condensed nearest neighbor rule (Hart, 1968), reduced nearest neighbor rule (Gates, 1972),
selective nearest neighbor rule (Rittler et al., 1975), typical instance based learning
algorithm (Zhang, 1992), prototype methods (Chang, 1974), hyperrectangle techniques
(Salzberg, 1991; Wettschereck & Dietterich, 1995), rule-based techniques (Domingos, 1995),
random mutation hill climbing (Skalak, 1994; Cameron-Jones, 1995) others (Kibler & Aha,
1987; Tomek, 1976; Wilson, 1972).

8. Conclusions & Future Research Areas
many learning systems depend reliable distance function achieve accurate
generalization. Euclidean distance function many distance functions
inappropriate nominal attributes, HOEM function throws away information
achieve much better accuracy Euclidean function itself.
Value Difference Metric (VDM) designed provide appropriate measure

28

fiIMPROVED HETEROGENEOUS DISTANCE FUNCTIONS

distance two nominal attribute values. However, current systems use VDM
often discretize continuous data discrete ranges, causes loss information
often corresponding loss generalization accuracy.
paper introduced three new distance functions. Heterogeneous Value Difference
Function (HVDM) uses Euclidean distance linear attributes VDM nominal attributes,
uses appropriate normalization. Interpolated Value Difference Metric (IVDM)
Windowed Value Difference Metric (WVDM) handle continuous attributes within
paradigm VDM. IVDM WVDM provide classification accuracy higher
average discretized version algorithm (DVDM) datasets continuous
attributes examined, equivalent DVDM applications without
continuous attributes.
experiments 48 datasets, IVDM WVDM achieved higher average accuracy
HVDM, better DVDM, HOEM Euclidean distance. IVDM
slightly accurate WVDM requires less time storage, thus would seem
desirable distance function heterogeneous applications similar used
paper. Properly normalized Euclidean distance achieves comparable generalization
accuracy nominal attributes, situations still appropriate
distance function.
learning system used obtain generalization accuracy results paper nearest
neighbor classifier, HVDM, IVDM WVDM distance functions used knearest neighbor classifier k > 1 incorporated wide variety systems
allow handle continuous values including instance-based learning algorithms (such
PEBLS), radial basis function networks, distance-based neural networks. new
distance metrics used areas statistics, cognitive psychology, pattern
recognition areas distance heterogeneous input vectors
interest. distance functions used conjunction weighting schemes
improvements system provides.
new distance functions presented show improved average generalization 48
datasets used experimentation. hoped datasets representative kinds
applications face real world, new distance functions
continue provide improved generalization accuracy cases.
Future research look determining conditions distance function
appropriate particular application. look closely problem selecting
window width, look possibility smoothing WVDMs probability landscape
avoid overfitting. new distance functions used conjunction variety
weighting schemes provide robust generalization presence noise
irrelevant attributes, well increase generalization accuracy wide variety
applications.

References
Aha, David W., (1992). Tolerating noisy, irrelevant novel attributes instance-based
learning algorithms. International Journal Man-Machine Studies, Vol. 36, pp. 267-287.
Aha, David W., Dennis Kibler, Marc K. Albert, (1991). Instance-Based Learning
Algorithms. Machine Learning, Vol. 6, pp. 37-66.
29

fiWILSON & MARTINEZ

Atkeson, Chris, (1989). Using local models control movement. D. S. Touretzky (Ed.),
Advances Neural Information Processing Systems 2. San Mateo, CA: Morgan Kaufmann.
Atkeson, Chris, Andrew Moore, Stefan Schaal, (1996). Locally weighted learning.
appear Artificial Intelligence Review.
Batchelor, Bruce G., (1978). Pattern Recognition: Ideas Practice. New York: Plenum Press,
pp. 71-72.
Biberman, Yoram, (1994). Context Similarity Measure. Proceedings European
Conference Machine Learning (ECML-94). Catalina, Italy: Springer Verlag, pp. 49-63.
Broomhead, D. S., D. Lowe (1988). Multi-variable functional interpolation adaptive
networks. Complex Systems, Vol. 2, pp. 321-355.
Cameron-Jones, R. M., (1995). Instance Selection Encoding Length Heuristic Random
Mutation Hill Climbing. Proceedings Eighth Australian Joint Conference
Artificial Intelligence, pp. 99-106.
Carpenter, Gail A., Stephen Grossberg, (1987). Massively Parallel Architecture
Self-Organizing Neural Pattern Recognition Machine. Computer Vision, Graphics,
Image Processing, Vol. 37, pp. 54-115.
Chang, Chin-Liang, (1974). Finding Prototypes Nearest Neighbor Classifiers. IEEE
Transactions Computers, Vol. 23, No. 11, pp. 1179-1184.
Cleveland, W. S., C. Loader, (1994). Computational Methods Local Regression.
Technical Report 11, Murray Hill, NJ: AT&T Bell Laboratories, Statistics Department.
Cost, Scott, Steven Salzberg, (1993). Weighted Nearest Neighbor Algorithm
Learning Symbolic Features. Machine Learning, Vol. 10, pp. 57-78.
Cover, T. M., P. E. Hart, (1967). Nearest Neighbor Pattern Classification. Institute
Electrical Electronics Engineers Transactions Information Theory, Vol. 13, No. 1,
pp. 21-27.
Dasarathy, Belur V., (1991). Nearest Neighbor (NN) Norms: NN Pattern Classification
Techniques. Los Alamitos, CA: IEEE Computer Society Press.
Deng, Kan, Andrew W. Moore, (1995). Multiresolution Instance-Based Learning.
appear Proceedings International Joint Conference Artificial Intelligence
(IJCAI95).
Diday, Edwin, (1974). Recent Progress Distance Similarity Measures Pattern
Recognition. Second International Joint Conference Pattern Recognition, pp. 534-539.
Domingos, Pedro, (1995). Rule Induction Instance-Based Learning: Unified Approach.
appear 1995 International Joint Conference Artificial Intelligence (IJCAI-95).
Dudani, Sahibsingh A., (1976). Distance-Weighted k-Nearest-Neighbor Rule. IEEE
Transactions Systems, Man Cybernetics, Vol. 6, No. 4, April 1976, pp. 325-327.
30

fiIMPROVED HETEROGENEOUS DISTANCE FUNCTIONS

Gates, G. W., (1972). Reduced Nearest Neighbor Rule. IEEE Transactions Information
Theory, Vol. IT-18, No. 3, pp. 431-433.
Giraud-Carrier, Christophe, Tony Martinez, (1995). Efficient Metric Heterogeneous
Inductive Learning Applications Attribute-Value Language. Intelligent Systems, pp.
341-350.
Hart, P. E., (1968). Condensed Nearest Neighbor Rule. Institute Electrical
Electronics Engineers Transactions Information Theory, Vol. 14, pp. 515-516.
Hecht-Nielsen, R., (1987). Counterpropagation Networks. Applied Optics, Vol. 26, No. 23, pp.
4979-4984.
Kibler, D., David W. Aha, (1987). Learning representative exemplars concepts:
initial case study. Proceedings Fourth International Workshop Machine
Learning. Irvine, CA: Morgan Kaufmann, pp. 24-30.
Kohonen, Teuvo, (1990). Self-Organizing Map. Proceedings IEEE, Vol. 78, No.
9, pp. 1464-1480.
Lebowitz, Michael, (1985). Categorizing Numeric Information Generalization. Cognitive
Science, Vol. 9, pp. 285-308.
Merz, C. J., P. M. Murphy, (1996). UCI Repository Machine Learning Databases.
Irvine, CA: University California Irvine, Department Information Computer
Science. Internet: http://www.ics.uci.edu/~mlearn/MLRepository.html.
Michalski, Ryszard S., Robert E. Stepp, Edwin Diday, (1981). Recent Advance Data
Analysis: Clustering Objects Classes Characterized Conjunctive Concepts.
Progress Pattern Recognition, Vol. 1, Laveen N. Kanal Azriel Rosenfeld (Eds.).
New York: North-Holland, pp. 33-56.
Mitchell, Tom M., (1980). Need Biases Learning Generalizations. J. W. Shavlik
& T. G. Dietterich (Eds.), Readings Machine Learning. San Mateo, CA: Morgan
Kaufmann, 1990, pp. 184-191.
Mohri, Takao, Hidehiko Tanaka, (1994). Optimal Weighting Criterion Case
Indexing Numeric Symbolic Attributes. D. W. Aha (Ed.), Case-Based
Reasoning: Papers 1994 Workshop, Technical Report WS-94-01. Menlo Park,
CA: AIII Press, pp. 123-127.
Nadler, Morton, Eric P. Smith, (1993). Pattern Recognition Engineering. New York:
Wiley, pp. 293-294.
Nosofsky, Robert M., (1986). Attention, Similarity, Identification-Categorization
Relationship. Journal Experimental Psychology: General, Vol. 115, No. 1, pp. 39-57.
Papadimitriou, Christos H., Jon Louis Bentley, (1980). Worst-Case Analysis Nearest
Neighbor Searching Projection. Lecture Notes Computer Science, Vol. 85,
Automata Languages Programming, pp. 470-482.
31

fiWILSON & MARTINEZ

Parzen, Emanuel, (1962). estimation probability density function mode. Annals
Mathematical Statistics. Vol. 33, pp. 1065-1076.
Quinlan, J. R., (1989). Unknown Attribute Values Induction. Proceedings 6th
International Workshop Machine Learning. San Mateo, CA: Morgan Kaufmann, pp.
164-168.
Rachlin, John, Simon Kasif, Steven Salzberg, David W. Aha, (1994). Towards Better
Understanding Memory-Based Bayesian Classifiers. Proceedings
Eleventh International Machine Learning Conference. New Brunswick, NJ: Morgan
Kaufmann, pp. 242-250.
Renals, Steve, Richard Rohwer, (1989). Phoneme Classification Experiments Using Radial
Basis Functions. Proceedings IEEE International Joint Conference Neural
Networks (IJCNN89), Vol. 1, pp. 461-467.
Rittler, G. L., H. B. Woodruff, S. R. Lowry, T. L. Isenhour, (1975). Algorithm
Selective Nearest Neighbor Decision Rule. IEEE Transactions Information Theory,
Vol. 21, No. 6, pp. 665-669.
Rosenblatt, Murray, (1956). Remarks Nonparametric Estimates Density Function.
Annals Mathematical Statistics. Vol. 27, pp. 832-835.
Rumelhart, D. E., J. L. McClelland, (1986). Parallel Distributed Processing, MIT Press,
Ch. 8, pp. 318-362.
Salzberg, Steven, (1991). Nearest Hyperrectangle Learning Method. Machine Learning,
Vol. 6, pp. 277-309.
Schaffer, Cullen, (1993). Selecting Classification Method Cross-Validation. Machine
Learning, Vol. 13, No. 1.
Schaffer, Cullen, (1994). Conservation Law Generalization Performance. Proceedings
Eleventh International Conference Machine Learning (ML94), Morgan
Kaufmann, 1994.
Schlimmer, Jeffrey C., (1987). Learning Representation Change. Proceedings
Sixth National Conference Artificial Intelligence (AAAI87), Vol. 2, pp. 511-535.
Skalak, D. B., (1994). Prototype Feature Selection Sampling Random Mutation Hill
Climbing Algorithsm. Proceedings Eleventh International Conference
Machine Learning (ML94). Morgan Kaufman, pp. 293-301.
Sproull, Robert F., (1991). Refinements Nearest-Neighbor Searching k-Dimensional
Trees. Algorithmica, Vol. 6, pp. 579-589.
Stanfill, C., D. Waltz, (1986). Toward memory-based reasoning. Communications
ACM, Vol. 29, December 1986, pp. 1213-1228.

32

fiIMPROVED HETEROGENEOUS DISTANCE FUNCTIONS

Tapia, Richard A., James R. Thompson, (1978). Nonparametric Probability Density
Estimation. Baltimore, MD: Johns Hopkins University Press.
Ting, Kai Ming, (1994). Discretization Continuous-Valued Attributes Instance-Based
Learning. Technical Report No. 491, Basser Department Computer Science, University
Sydney, Australia.
Ting, Kai Ming, (1996). Discretisation Lazy Learning. appear special issue
Lazy Learning Artificial Intelligence Review.
Tomek, Ivan, (1976). Experiment Edited Nearest-Neighbor Rule. IEEE
Transactions Systems, Man, Cybernetics, Vol. 6, No. 6, June 1976, pp. 448-452.
Turney, Peter, (1994). Theoretical Analyses Cross-Validation Error Voting InstanceBased Learning. Journal Experimental Theoretical Artificial Intelligence (JETAI),
pp. 331-360.
Turney, Peter, (1993). Exploiting context learning classify. Proceedings
European Conference Machine Learning. Vienna, Austria: Springer-Verlag, pp. 402407.
Turney, Peter, Michael Halasz, (1993). Contextual Normalization Applied Aircraft Gas
Turbine Engine Diagnosis. Journal Applied Intelligence, Vol. 3, pp. 109-129.
Tversky, Amos, (1977). Features Similarity. Psychological Review, Vol. 84, No. 4, pp. 327352.
Ventura, Dan, (1995). Discretization Preprocessing Step Supervised Learning
Models, Masters Thesis, Department Computer Science, Brigham Young University.
Ventura, Dan, Tony R. Martinez (1995). Empirical Comparison Discretization
Methods. Proceedings Tenth International Symposium Computer
Information Sciences, pp. 443-450.
Wasserman, Philip D., (1993). Advanced Methods Neural Computing. New York, NY: Van
Nostrand Reinhold, pp. 147-176.
Wess, Stefan, Klaus-Dieter Althoff Guido Derwand, (1994). Using k-d Trees Improve
Retrieval Step Case-Based Reasoning. Stefan Wess, Klaus-Dieter Althoff, & M. M.
Richter (Eds.), Topics Case-Based Reasoning. Berlin: Springer-Verlag, pp. 167-181.
Wettschereck, Dietrich, Thomas G. Dietterich, (1995). Experimental Comparison
Nearest-Neighbor Nearest-Hyperrectangle Algorithms. Machine Learning, Vol. 19,
No. 1, pp. 5-28.
Wettschereck, Dietrich, David W. Aha, Takao Mohri, (1995). Review Comparative
Evaluation Feature Weighting Methods Lazy Learning Algorithms. Technical
Report AIC-95-012. Washington, D.C.: Naval Research Laboratory, Navy Center
Applied Research Artificial Intelligence.

33

fiWILSON & MARTINEZ

Wilson, D. Randall, Tony R. Martinez, (1993). Potential Prototype Styles
Generalization. Proceedings Sixth Australian Joint Conference Artifical
Intelligence (AI93), pp. 356-361.
Wilson, D. Randall, Tony R. Martinez, (1996). Heterogeneous Radial Basis Functions.
Proceedings International Conference Neural Networks (ICNN96), Vol. 2, pp.
1263-1267.
Wilson, Dennis L., (1972). Asymptotic Properties Nearest Neighbor Rules Using Edited
Data. IEEE Transactions Systems, Man, Cybernetics, Vol. 2, No. 3, pp. 408-421.
Wolpert, David H., (1993). Overfitting Avoidance Bias. Technical Report SFI TR 9203-5001. Santa Fe, NM: Santa Fe Institute.
Zhang, Jianping, (1992). Selecting Typical Instances Instance-Based Learning. Proceedings
Ninth International Conference Machine Learning.

34


