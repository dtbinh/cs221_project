journal artificial intelligence

submitted published

improved heterogeneous distance functions
randall wilson
tony r martinez
computer science department
brigham young university
provo ut usa

randy axon cs byu edu
martinez cs byu edu

abstract
instance learning techniques typically handle continuous linear input values well
often handle nominal input attributes appropriately value difference metric
vdm designed reasonable distance values nominal attribute values
largely ignores continuous attributes requiring discretization map continuous values
nominal values proposes three heterogeneous distance functions called
heterogeneous value difference metric hvdm interpolated value difference metric
ivdm windowed value difference metric wvdm distance functions
designed handle applications nominal attributes continuous attributes
experiments applications distance metrics achieve higher classification accuracy
average three previous distance functions datasets nominal
continuous attributes

introduction
instance learning ibl aha kibler albert aha wilson martinez
wettschereck aha mohri domingos paradigm learning
typically store n available training examples instances
training set learning instance input vector x output class c
generalization systems use distance function determine close
input vector stored instance use nearest instance instances predict
output class e classify instance learning referred
nearest neighbor techniques cover hart hart dasarathy memorybased reasoning methods stanfill waltz cost salzberg rachlin et al
overlap significantly instance paradigm well much
success wide variety applications real world classification tasks
many neural network make use distance functions including radial basis
function networks broomhead lowe renals rohwer wasserman
counterpropagation networks hecht nielsen art carpenter grossberg selforganizing maps kohonen competitive learning rumelhart mcclelland
distance functions used many fields besides machine learning neural networks
including statistics atkeson moore schaal pattern recognition diday
michalski stepp diday cognitive psychology tversky nosofsky
ai access foundation morgan kaufmann publishers rights reserved

fiwilson martinez

many distance functions proposed decide instance
closest given input vector michalski stepp diday diday many
metrics work well numerical attributes appropriately handle nominal e
discrete perhaps unordered attributes
value difference metric vdm stanfill waltz introduced define
appropriate distance function nominal called symbolic attributes modified value
difference metric mvdm uses different weighting scheme vdm used
pebls system cost salzberg rachlin et al distance metrics work well
many nominal domains handle continuous attributes directly instead
rely upon discretization lebowitz schlimmer degrade generalization
accuracy ventura martinez
many real world applications nominal linear attributes including
example half datasets uci machine learning database repository merz
murphy introduces three distance functions appropriate
previous functions applications nominal continuous attributes
distance functions incorporated many learning systems areas
study augmented weighting schemes wettschereck aha mohri
atkeson moore schaal enhancements system provides
choice distance function influences bias learning bias rule
method causes choose one generalized output another mitchell
learning must bias order generalize shown
learning generalize accurately summed
possible schaffer unless information
training data available follows distance function strictly better
terms generalization ability considering possible equal
probability
however higher probability one class occurring another
learning generalize accurately others wolpert
better summed
perform well likely occur sense one distance function
improvement another higher probability good generalization
another better matched kinds likely occur
many learning use bias simplicity mitchell wolpert
generalize bias appropriatemeaning leads good generalization
accuracyfor wide variety real world applications though meaning simplicity varies
depending upon representational language learning biases
decisions made basis additional domain knowledge particular mitchell
improve generalization
light distance functions presented appropriate
used comparison average yield improved generalization accuracy
collection applications theoretically limited set datasets
hope datasets representative interest occur
frequently real world distance functions presented useful
cases especially involving continuous nominal input attributes
section provides background information distance functions used previously section


fiimproved heterogeneous distance functions

introduces distance function combines euclidean distance vdm handle
continuous nominal attributes sections present two extensions value
difference metric allow direct use continuous attributes section introduces
interpolated value difference metric ivdm uses interpolation probabilities avoid
related discretization section presents windowed value difference metric
wvdm uses detailed probability density function similar interpolation
process
section presents empirical comparing three commonly used distance functions
three functions presented obtained
distance functions instance learning system datasets indicate
heterogeneous distance functions appropriate previously used functions
datasets nominal linear attributes achieve higher average
generalization accuracy datasets section discusses related work section
provides conclusions future directions

previous distance functions
mentioned introduction many learning systems depend upon good
distance function successful variety distance functions available uses
including minkowsky batchelor mahalanobis nadler smith camberra
chebychev quadratic correlation chi square distance metrics michalski stepp
diday diday context similarity measure biberman contrast
model tversky hyperrectangle distance functions salzberg domingos
others several functions defined figure
although many distance functions proposed far commonly
used euclidean distance function defined


e x

xa ya





x two input vectors one typically stored instance
input vector classified number input variables attributes
application square root often computed practice closest instance
still closest regardless whether square root taken
alternative function city block manhattan distance function requires less
computation defined
x



xa ya





euclidean manhattan distance functions equivalent minkowskian rdistance function batchelor r respectively



fiwilson martinez

minkowsky

euclidean


r
x xi yi


camberra



r

manhattan city block




xi yi

x



x

x
x





chebychev



x xi yi




x max xi yi




x x q x xi yi q ji x j j
quadratic

q specific positive
j
definite weight matrix
v covariance matrix
mahalanobis
aj vector values
x det v x v x
attribute j occuring training set
instances n

correlation
xi xi yi yi
xi yi average value

x
attribute
occuring training set




xi xi yi yi





xi
chi square x


sumi sizex size



kendalls rank correlation
sign x x
x x respectively

x



sumi sum values attribute
occuring training set sizex
sum values vector x




sign xi x j sign yi j
n n j

figure equations selected distance functions
x vectors attribute values
normalization
one weakness basic euclidean distance function one input attributes
relatively large range overpower attributes example application
two attributes b values b values
bs influence distance function usually overpowered
influence therefore distances often normalized dividing distance attribute
range e maximum minimum attribute distance attribute
approximate range order avoid outliers common divide
standard deviation instead range trim range removing highest lowest
percent e g data consideration defining range possible
map value outside range minimum maximum value avoid normalized
values outside range domain knowledge often used decide method
appropriate
related idea normalization attribute weights weighting


fiimproved heterogeneous distance functions

schemes many learning systems use distance functions incorporate weighting
schemes distance calculations wettschereck aha mohri atkeson moore
schaal improvements presented independent schemes
weighting schemes well enhancements instance pruning
techniques used conjunction distance functions presented
attribute types
none distance functions shown figure including euclidean distance appropriately
handle non continuous input attributes
attribute linear nominal linear attribute continuous discrete
continuous continuously valued attribute uses real values mass planet
velocity object linear discrete integer attribute discrete set
linear values number children
argued value stored computer discrete level reason
continuous attributes treated differently many different values
value may appear rarely perhaps particular application causes
vdm described section depend testing two
values equality two continuous values rarely equal though may
quite close
nominal symbolic attribute discrete attribute whose values necessarily
linear order example variable representing color might values red
green blue brown black white could represented integers
respectively linear distance measurement values makes little
sense case
heterogeneous euclidean overlap metric heom
one way handle applications continuous nominal attributes use
heterogeneous distance function uses different attribute distance functions different
kinds attributes one used use overlap metric nominal
attributes normalized euclidean distance linear attributes
purposes comparison testing define heterogeneous distance function
similar used ib ib ib aha kibler albert aha well
used giraud carrier martinez function defines distance
two values x given attribute
x unknown else


da x overlap x nominal else
rn diff x





unknown attribute values handled returning attribute distance e maximal
distance attribute values unknown function overlap rangenormalized difference rn diff defined
x
overlap x
otherwise





fiwilson martinez

rn diff x

x
rangea



value rangea used normalize attributes defined
rangea maxa mina



max mina maximum minimum values respectively observed
training set attribute means possible input vector value
outside range produce difference value greater one however cases
rare occur large difference may acceptable anyway normalization
serves scale attribute point differences almost less one
definition da returns value typically range whether
attribute nominal linear overall distance two possibly heterogeneous input
vectors x given heterogeneous euclidean overlap metric function heom x


da xa ya

heom x





distance function removes effects arbitrary ordering nominal values
overly simplistic handling nominal attributes fails make use additional
information provided nominal attribute values aid generalization
value difference metric vdm
value difference metric vdm introduced stanfill waltz provide
appropriate distance function nominal attributes simplified version vdm without
weighting schemes defines distance two values x attribute
c na x c

na c
vdma x

n
na
x
c

q

c

pa x c pa c

q



c


na x number instances training set value x attribute
na x c number instances value x attribute output class c
c number output classes domain
q constant usually
p x c conditional probability output class c given attribute
value x e p c xa seen pa x c defined
pa x c

na x c
na x



na x sum na x c classes e
c

na x na x c
c





fiimproved heterogeneous distance functions

sum pa x c c classes fixed value x
distance measure vdma x two values considered closer
similar classifications e similar correlations output classes regardless
order values may given fact linear discrete attributes values
remapped randomly without changing resultant distance measurements
example attribute color three values red green blue application
identify whether object apple red green would considered closer
red blue former two similar correlations output class apple
original vdm stanfill waltz makes use feature weights
included equations variants vdm cost salzberg
rachlin et al domingos used alternate weighting schemes discussed
earlier distance functions presented independent schemes
cases make use similar enhancements
one formulas presented define
done value appears input vector never appeared training set
attribute never value x instance training set na x c c
n x sum na x c classes cases p x c
undefined nominal attributes way know probability
value since inherent ordering values assign
p x c default value cases though possible let pa x c c c
number output classes since sum pa x c c c
distance function used directly continuous attributes values
potentially unique case na x every value x na x c one value c
others given value x addition vectors likely unique
values resulting division zero even value substituted
resulting distance measurement nearly useless
even values unique often enough different values continuous
attribute statistical sample unreliably small value distance measure
still untrustworthy inappropriate use vdm directly
continuous attributes
discretization
one vdm continuous attributes discretization
lebowitz schlimmer ventura used vdm
variants cost salzberg rachlin et al mohri tanaka
discretized continuous attributes somewhat arbitrary number discrete ranges
treated values nominal discrete unordered values method advantage
generating large enough statistical sample nominal value p values
significance however discretization lose much important information available
continuous values example two values discretized range considered
equal even opposite ends range effects reduce generalization
accuracy ventura martinez
propose three alternatives presented following three
sections section presents heterogeneous distance function uses euclidean distance
linear attributes vdm nominal attributes method requires careful attention


fiwilson martinez

normalization neither nominal linear attributes regularly given
much weight
sections present two distance functions interpolated value difference
metric ivdm windowed value difference metric wvdm use discretization
collect statistics determine values pa x c continuous values occurring training
set instances retain continuous values later use generalization
value pa c continuous value interpolated two values p namely
p x c pa x c x x ivdm wvdm essentially different techniques
nonparametric probability density estimation tapia thompson
determine values p class generic version vdm called
discretized value difference metric dvdm used comparisons two


heterogeneous value difference metric hvdm
discussed previous section euclidean distance function inappropriate
nominal attributes vdm inappropriate continuous attributes neither sufficient
use heterogeneous application e one nominal continuous
attributes
section define heterogeneous distance function hvdm returns distance
two input vectors x defined follows
hvdm x



da xa ya





number attributes function da x returns distance two
values x attribute defined
x unknown otherwise


da x normalized vdma x nominal
normalized diff x linear





function da x uses one two functions defined section depending
whether attribute nominal linear note practice square root
typically performed distance positive nearest neighbor still
nearest whether distance squared however e g
distance weighted k nearest neighbor dudani require square root
evaluated
many applications contain unknown input values must handled appropriately
practical system quinlan function da x therefore returns distance
x unknown done aha kibler albert giraud carrier martinez
complicated methods tried wilson martinez
little effect accuracy
function hvdm similar function hoem given section except


fiimproved heterogeneous distance functions

uses vdm instead overlap metric nominal values normalizes differently
similar distance function used rise domingos
important differences noted section
section presents three alternatives normalizing nominal linear attributes
section presents experimental one schemes provides better
normalization two set several datasets section gives empirical
comparing hvdm two commonly used distance functions
normalization
discussed section distances often normalized dividing distance
variable range attribute distance input variable range
policy used heom section however dividing range allows
outliers extreme values profound effect contribution attribute
example variable values range almost every case one
exceptional possibly erroneous value dividing range would almost
value less robust alternative presence outliers
divide values standard deviation reduce effect extreme values typical
cases
heterogeneous distance metric hvdm situation complicated
nominal numeric distance values come different types measurements
numeric distances computed difference two linear values normalized
standard deviation nominal attributes computed sum c differences
probability values c number output classes therefore necessary
way scale two different kinds measurements approximately range
give variable similar influence overall distance measurement
since values normal distribution fall within two standard deviations
mean difference numeric values divided standard deviations scale
value range usually width function normalized diff therefore defined
shown equation
normalized diff x

xy




standard deviation numeric values attribute
three alternatives function normalized vdm considered use
heterogeneous distance function labeled n n n definitions
given
n normalized vdm x

c n
x c



c

n normalized vdm x

c n
x c



c



na x

na x



na c



na c



na

na





fiwilson martinez

c n
x c

n normalized vdm x c

c

na x



na c





na

function n equation q similar formula used pebls
rachlin et al rise domingos nominal attributes
n uses q thus squaring individual differences analogous euclidean
distance instead manhattan distance though slightly expensive computationally
formula hypothesized robust n favors class
correlations fairly similar rather close different n
would able distinguish two practice square root taken
individual attribute distances squared hvdm function
n function used heterogeneous radial basis function networks wilson
martinez hvdm first introduced
normalization experiments
order determine whether normalization scheme n n n gave unfair weight
nominal linear attributes experiments run databases machine
learning database repository university california irvine merz murphy
datasets experiment least nominal linear attributes
thus require heterogeneous distance function
experiment five fold cross validation used five trials
distance instance test set instance training set
computed computing distance attribute normalized diff function
used linear attributes normalized vdm function n n n used
three respective experiments nominal attributes
average distance e sum distances divided number comparisons
computed attribute average linear attributes database
computed averages listed heading avglin table

database
anneal
australian
bridges
crx
echocardiogram
flag
heart
heart cleveland
heart hungarian
heart long beach va
heart
heart swiss
hepatitis
horse colic
soybean large
average

avglin

















n
avgnom

















n
avgnom

















n
avgnom

















nom

















lin

















table average attribute distance linear nominal attributes


c

















fiimproved heterogeneous distance functions

average distance

average distance

average distance

averages nominal attributes three normalization schemes
listed headings avgnom table well average distance linear
variables exactly regardless whether n n n used average
given table lists number nominal nom number linear
lin attributes database along number output classes c
seen overall averages first four columns last row
table n closer n n however important understand reasons behind
difference order know normalization scheme n robust general
figures graphically display averages shown table headings n n
n respectively ordered left right number output classes
hypothesized number output classes grows normalization would get worse
n indeed appropriate add scaling factor c sum length
line indicates much difference
average distance nominal attributes
nominal

linear attributes ideal normalization scheme
linear
would difference zero longer lines

indicate worse normalization

number output classes grows

difference n linear distances

nominal distances grows wider cases
n hand seems remain quite close

avg
independent number output classes
number output classes
interestingly n almost poorly n even
figure average distances n
though use scaling factor c
apparently squaring factor provides
nominal

well rounded distance metric nominal attributes
linear
similar provided euclidean distance

instead manhattan distance linear attributes

underlying hypothesis behind performing

normalization proper normalization

typically improve generalization accuracy
nearest neighbor classifier k

avg
implemented hvdm distance metric
number output classes
system tested heterogeneous
figure average distances n
datasets appearing table three
different normalization schemes discussed

nominal
ten fold cross validation schaffer
linear
summarized table

normalization schemes used training sets

test sets trial bold entries indicate

scheme highest accuracy

asterisk indicates difference greater
next highest scheme

avg
seen table normalization
number output classes
scheme n highest accuracy n
figure average distances n


fiwilson martinez

substantially lower two n
database
n
n
n
n highest accuracy
anneal


domains significantly n
australian


bridges


higher times compared n
crx


higher one dataset
echocardiogram


n higher two
flag


heart cleveland


one dataset lower average
heart hungarian


accuracy n
heart long beach va


support hypothesis
heart


heart


normalization scheme n
heart swiss


achieves higher generalization accuracy
hepatitis


n n datasets due
horse colic

soybean large


robust normalization though
average


accuracy n almost good n
note proper normalization
table generalization accuracy
n n n
necessarily improve generalization
accuracy one attribute
important others classification giving higher weight may improve
classification therefore important attribute given higher weight accidentally
poor normalization may actually improve generalization accuracy however
random improvement typically case proper normalization improve
generalization cases used typical applications
consequence n used normalization scheme hvdm
function normalized vdm defined
empirical hvdm vs euclidean hoem
nearest neighbor classifier k three distance functions listed table
tested datasets uci machine learning database repository datasets
obtained datasets least nominal attributes shown
table
approximately equivalent datasets linear attributes
remaining datasets shown found section fold crossvalidation used three distance metrics used training sets test sets
trial
experiments shown table first column lists name
database test means database originally meant used test set
instead used entirety separate database second column shows
obtained euclidean distance function normalized standard deviation
attributes including nominal attributes next column shows generalization accuracy
obtained hoem metric uses range normalized euclidean distance linear
attributes overlap metric nominal attributes final column shows accuracy
obtained hvdm distance function uses standard deviation normalized
euclidean distance e normalized diff defined equation linear attributes
normalized vdm function n nominal attributes
highest accuracy obtained database shown bold entries euclid


fiimproved heterogeneous distance functions

hoem columns significantly
higher hvdm higher
confidence level two tailed
paired test marked
asterisk
entries
significantly lower hvdm
marked less sign
seen table
hvdm distance functions overall
average accuracy higher
two metrics
hvdm achieved high higher
generalization accuracy
two distance functions
datasets euclidean distance
function highest datasets
hoem highest
datasets
hvdm significantly higher
euclidean distance function
datasets significantly lower
similarly hvdm higher
hoem datasets
significantly lower
support hypothesis
hvdm handles nominal attributes
appropriately euclidean
distance heterogeneous
euclidean overlap metric thus
tends achieve higher generalization
accuracy typical applications

database
euclid
anneal

audiology

audiology test

australian

bridges

crx

echocardiogram

flag

heart cleveland

heart hungarian

heart long beach va
heart

heart swiss

hepatitis

horse colic

house votes

image segmentation

led

led creator

monks test

monks test

monks test

mushroom

promoters

soybean large

soybean small

thyroid allbp

thyroid allhyper

thyroid allhypo

thyroid allrep

thyroid dis

thyroid hypothyroid
thyroid sick euthyroid
thyroid sick

zoo

average


hoem





































hvdm





































table generalization accuracy
euclidean hoem hvdm distance functions

interpolated value difference metric ivdm
section section introduce distance functions allow vdm applied
directly continuous attributes alleviates need normalization attributes
cases provides better measure distance continuous attributes linear
distance
example consider application input attribute height output class
indicates whether person good candidate fighter pilot particular airplane
individuals heights significantly preferred height might
considered poor candidates thus could beneficial consider heights
similar preferred height even though farther apart
linear sense



fiwilson martinez

hand linear attributes linearly distant values tend indicate different
classifications handled appropriately interpolated value difference metric
ivdm handles situations handles heterogeneous applications robustly
generic version vdm distance function called discretized value difference
metric dvdm used comparisons extensions vdm presented
ivdm learning
original value difference metric vdm uses statistics derived training set
instances determine probability pa x c output class c given input value x
attribute
ivdm continuous values discretized equal width intervals though
continuous values retained later use integer supplied user
unfortunately currently little guidance value use value
large reduce statistical strength values p value small allow
discrimination among classes purposes use heuristic
determine automatically let c whichever greatest c number
output classes domain current examining sophisticated
techniques determining good values cross validation statistical
methods e g tapia thompson p early experimental indicate
value may critical long c n n number instances
training set
width wa discretized interval attribute given
wa

maxa mina




max mina maximum minimum value respectively occurring
training set attribute
example consider iris database uci machine learning databases
iris database four continuous input attributes first sepal length let
training set consisting available training instances test set
consisting remaining
one division training set values sepal length attribute ranged
three output classes database let resulting
width note since discretization part learning process
would unfair use instances test set help determine discretize
values discretized value v continuous value x attribute integer
given
x discrete else

v discretizea x x max else
x min w






deciding upon finding w discretized values continuous attributes


fiimproved heterogeneous distance functions

used discrete values nominal attributes finding pa x c figure lists pseudo code
done
learnp training set
attribute
instance
let x input value attribute instance
v discretizea x x discrete
let c output class instance
increment na v c
increment na v
discrete value v attribute
class c
na v
pa v c
else pa v c na v c na v
return array pa v c

figure pseudo code finding pa x c

probability

first attribute iris database values pa x c displayed figure
five discretized ranges x probability three corresponding
output classes shown bar heights note heights three bars sum
discretized range bold integers indicate discretized value range
example sepal length greater equal less would discretized
value
















output class
iris
setosa















iris
viginica





iris
versicolor













sepal length cm






bold
discretized
range number

figure pa x c x c first attribute iris database
ivdm dvdm generalization
thus far dvdm ivdm learn identically however point dvdm
need retain original continuous values use discretized
values generalization hand ivdm use continuous values
generalization nearest neighbor classifier use distance
function dvdm defined follows
dvdm x



vdma discretizea xa discretizea ya









fiwilson martinez

discretizea defined equation vdma defined equation
q repeat convenience
vdma x

c

pa x c pa c





c

unknown input values quinlan treated simply another discrete value done
domingos


b









input attributes

















output class
iris setosa
iris versicolor



table example iris database
example consider two training instances b shown table
input vector classified attribute discretized values b
respectively values figure distance attribute


probability class

distance b since discretized value
note b values different ends range actually nearly
close spite fact discretized distance function says b
equal happen fall discretized range
ivdm uses interpolation alleviate ivdm assumes pa x c values
hold true midpoint range interpolates midpoints p
attribute values
figure shows p values second output class iris versicolor function
first attribute value sepal length dashed line indicates p value used dvdm
solid line shows ivdm uses












center
points
dvdm
ivdm











sepal length cm



bold
discretized
range number

figure p x values dvdm ivdm attribute class iris database


fiimproved heterogeneous distance functions

distance function interpolated value difference metric defined
ivdm x



ivdma xa ya





ivdma defined
discrete
vdma x
c

ivdma x
p x pa c otherwise
c
c



formula determining interpolated probability value pa c x continuous value x
attribute class c


x mida u
pa c x pa u c
pa u c pa u c
mida u mida u



equation mida u mida u midpoints two consecutive discretized ranges
mida u x mida u pa u c probability value discretized range u
taken probability value midpoint range u similarly pa u c
value u found first setting u discretizea x subtracting u x mida u
value mida u found follows
mida u mina widtha u



probability class

figure shows values pa c x attribute iris database three output
classes e c since data points outside range mina maxa
probability value pa u c taken u u seen visually
diagonal lines sloping toward zero outer edges graph note sum
probabilities three output classes sum every point midpoint range
midpoint range












output class
iris setosa
iris versicolor
iris viginica











sepal length cm



bold
discretized
range number

figure interpolated probability values attribute iris database


fiwilson martinez


b

value



p v



p v



p v













ivdm v



vdm v



table example ivdm vs vdm
ivdm example instances table values first attribute
discretized dvdm used interpolated probability values
example value p c x interpolates midpoints returning
values shown table three classes instance value
falls midpoints instance b value falls
midpoints
seen table ivdm single attribute distance function ivdm
returns distance indicates closer b first attribute
certainly case dvdm
discretized vdm hand returns
database
dvdm
ivdm
annealing


distance indicates value
australian


equal b quite far
bridges


illustrating involved
credit screening


echocardiogram


discretization
flag


ivdm dvdm
glass


implemented tested datasets
heart disease


heart cleveland


uci machine learning databases
heart
hungarian


datasets contain
heart long beach va

least continuous attributes
heart


shown table since ivdm
heart swiss


hepatitis


dvdm equivalent domains
horse colic


discrete attributes
image segmentation


remaining datasets deferred section
ionosphere


iris


fold cross validation
liver disorders


used average accuracy
pima indians diabetes


database trials shown
satellite image


shuttle


table bold values indicate value
sonar


highest dataset asterisks
thyroid allbp


indicates difference statistically
thyroid allhyper


thyroid allhypo


significant confidence level
thyroid allrep


higher two tailed paired test
thyroid dis


set datasets ivdm
thyroid hypothyroid


higher average generalization accuracy
thyroid sick


thyroid sick euthyroid

overall discretized
vehicle


ivdm obtained higher generalization
vowel


accuracy dvdm
wine


average


cases significant
level dvdm higher
table generalization dvdm vs ivdm


fiimproved heterogeneous distance functions

accuracy cases one difference statistically significant
indicate interpolated distance function typically appropriate
discretized value difference metric applications one continuous
attributes section contains comparisons ivdm distance functions

windowed value difference metric wvdm
ivdm thought sampling value pa u c midpoint mida u
discretized range u p sampled first finding instances value
attribute range mida u w na u incremented instance
n u c incremented instance whose output class c
p u c na u c na u computed ivdm interpolates sampled points
provide continuous rough approximation function pa c x possible sample p
points thus provide closer approximation function pa c x may
turn provide accurate distance measurements values
figure shows pseudo code windowed value difference metric wvdm
wvdm samples value p x c value x occurring training set
define
instance n list n instances sorted ascending order attribute
instance val value attribute instance
x
center value current window e x instance val
p c
probability pa x c output class c given input value x
attribute note index value
n c
number na x c instances current window output class c
n
total number na x instances current window
instance first instance window
instance first instance outside window e window contains
instances instance
w
window width attribute
learnwvdm training set
continuous attribute
sort instance n ascending order attribute quicksort
initialize n n c e start empty window
n
let x instance val
expand window include instances range
n instance val x w
increment n c c class instance
increment n
increment
shrink window exclude instances longer range
instance val x w
decrement n c c class instance
decrement n
increment
compute probability value class current window
class c c
p c n c n e pa x c na x c na x
return array p c

figure pseudo code wvdm learning


fiwilson martinez

attribute instead midpoints range fact discretized ranges
even used wvdm continuous attributes except determine appropriate window
width wa range width used dvdm ivdm pseudo code
learning used determine pa x c attribute value x given figure
value x occurring training set attribute p sampled finding
instances value attribute range x w computing na x
na x c pa x c na x c na x thus instead fixed number sampling
points window instances centered training instance used determining
probability given point technique similar concept shifted histogram estimators
rosenblatt parzen window techniques parzen
attribute values sorted nlogn sorting allow
sliding window used thus collect needed statistics n time attribute
sorted order retained attribute binary search performed log
n time generalization
values occurring sampled points interpolated ivdm except
many points available value interpolated two
closer precise values ivdm
wvdm p attribute continuous value x
pa x c c c given value x attribute
instance val x instance val binary search
x instance val
unless case x min w
x instance val unless n case x max w
class c c
p p c
unless case p
p p c unless n case p
pa x c p x x x x p p
return array pa x c

figure pseudo code wvdm probability interpolation see figure definitions
pseudo code interpolation given figure takes
value x attribute returns vector c probability values pa x c c c first
binary search two consecutive instances sorted list instances
attribute surround x probability class interpolated
stored two surrounding instances exceptions noted parenthesis handle
outlying values interpolating towards done ivdm
probability values input vectors attribute values computed
used vdm function discrete probability values
wvdm distance function defined
wvdm x



wvdma xa ya





wvdma defined
discrete
vdma x
c

wvdma x
p
pa c otherwise
x c
c





fiprobability class

improved heterogeneous distance functions













output class
iris setosa
iris versicolor
iris viginica











sepal length cm

figure example wvdm probability landscape
pa x c interpolated probability value continuous value x computed
figure note typically finding distance input vector
instance training set since
instances training set used
database
dvdm wvdm
define probability attribute
annealing


australian


values binary search interpolation
bridges


unnecessary training instances
credit screening


immediately recall stored
echocardiogram


probability values unless pruning techniques
flag


glass


used
heart disease


one drawback
heart cleveland


increased storage needed retain c
heart hungarian


heart long beach va

probability values attribute value
heart


training set execution time
heart swiss


significantly increased ivdm
hepatitis


horse colic


dvdm see section discussion
image
segmentation


efficiency considerations
ionosphere


figure shows probability values
iris


liver disorders


three classes first attribute
pima indians diabetes


iris database time
satellite image


windowed sampling technique comparing
shuttle


figure figure reveals
sonar


thyroid allbp


attribute ivdm provides approximately
thyroid allhyper


overall shape misses much
thyroid allhypo


detail example peak occurring
thyroid allrep


thyroid dis


output class approximately sepal
thyroid hypothyroid


length figure flat line
thyroid sick


misses peak entirely due mostly
thyroid sick euthyroid

vehicle


somewhat arbitrary position
vowel


midpoints probability values
wine


sampled
average


table summarizes testing
table generalization wvdm vs dvdm


fiwilson martinez

wvdm datasets dvdm ivdm bold entry indicates
highest two accuracy measurements asterisk indicates difference
statistically significant confidence level two tailed paired test
set databases wvdm average accurate dvdm
overall wvdm higher average accuracy dvdm databases
significantly higher dvdm higher databases none
differences statistically significant
section provides comparisons wvdm distance functions including
ivdm

empirical comparisons analysis distance functions
section compares distance functions discussed nearest neighbor
classifier implemented six different distance functions euclidean
normalized standard deviation hoem discussed section hvdm discussed
section dvdm ivdm discussed section wvdm discussed section
figure summarizes definition distance function
functions use
overall distance function

x



da xa ya



distance
function
euclidean

definition da xa ya attribute type
linear
continuous
discrete nominal
xa ya
xa ya



hoem

xa ya
rangea

xa ya
xa ya

hvdm

xa ya


vdma xa ya

dvdm

vdma disca xa disca ya

vdma xa ya

ivdm

ivdma xa ya
interpolate probabilities
range midpoints

vdma xa ya

wvdm

wvdma xa ya
interpolate probabilities
adjacent values

vdma xa ya

rangea maxa mina vdma x

c

pa x c pa c



c

figure summary distance function definitions
distance function tested datasets uci machine learning databases


fiimproved heterogeneous distance functions

fold cross validation average accuracy trials reported
test table highest accuracy achieved dataset shown bold
names three distance functions presented hvdm ivdm wvdm
shown bold identify
table lists number instances database inst number
continuous con integer int e linear discrete nominal nom input attributes

database
euclid hoem
annealing


audiology


audiology test


australian


breast cancer


bridges


credit screening


echocardiogram


flag


glass


heart disease


heart cleveland


heart hungarian


heart long beach va


heart


heart swiss


hepatitis


horse colic


house votes


image segmentation


ionosphere


iris


led noise


led


liver disorders


monks


monks


monks


mushroom

pima indians diabetes


promoters


satellite image


shuttle


sonar


soybean large


soybean small

thyroid allbp


thyroid allhyper


thyroid allhypo


thyroid allrep


thyroid dis


thyroid hypothyroid


thyroid sick euthyroid

thyroid sick


vehicle


vowel


wine


zoo


average



n c e
hvdm


















































f u n
dvdm


















































c n
inputs
ivdm wvdm inst con int nom



























































































































































































































table summary generalization accuracy



fiwilson martinez

set datasets three distance functions hvdm ivdm wvdm
substantially better euclidean distance hoem ivdm highest average accuracy
almost higher average euclidean distance indicating
robust distance function datasets especially nominal
attributes wvdm slightly lower ivdm accuracy somewhat
surprisingly dvdm slightly higher hvdm datasets even though uses
discretization instead linear distance continuous attributes four vdm
distance functions outperformed euclidean distance hoem
datasets euclidean distance highest accuracy times hoem
highest times hvdm dvdm ivdm wvdm
datasets continuous attributes four vdm distance functions
hvdm dvdm ivdm wvdm equivalent datasets vdm
distance functions achieve average accuracy compared hoem
euclidean indicating substantial superiority
datasets nominal attributes euclidean hvdm equivalent
distance functions perform average except dvdm averages
less others indicating detrimental effects discretization euclidean
hoem similar definitions applications without nominal attributes except
euclidean normalized standard deviation hoem normalized range
attribute interesting average accuracy datasets slightly higher
euclidean hoem indicating standard deviation may provide better normalization
datasets however difference small less datasets
contain many outliers difference probably negligible case
one disadvantage scaling attributes standard deviation attributes
almost value e g boolean attribute almost
given large weightnot due scale relative frequencies attribute
values related occur hvdm skewed class distribution
e many instances classes others p values quite
small classes quite large others case difference pa x c pa c
correspondingly small thus nominal attributes get little weight
compared linear attributes phenomenon noted ting
recognized hypothyroid dataset future address
normalization look automated solutions fortunately dvdm ivdm
wvdm suffer attributes scaled amount
cases may part account success hvdm
experiments
datasets nominal continuous attributes hvdm slightly higher
euclidean distance datasets turn slightly higher hoem indicating
overlap metric may much improvement heterogeneous databases
dvdm ivdm wvdm higher euclidean distance datasets
ivdm lead
effects sparse data
distance functions use vdm require statistics determine distance therefore
hypothesized generalization accuracy might lower vdm distance functions


fiimproved heterogeneous distance functions

euclidean distance hoem little data available vdmbased functions would increase accuracy slowly others instances
made available sufficient number instances allowed reasonable sample size
determine good probability values



average generalization accuracy





euclidean

hoem
hvdm



dvdm
ivdm



wvdm







instances used





figure average accuracy amount data increases
test hypothesis experiments used obtain shown table
repeated part available training data figure shows generalization
accuracy test set improves percentage available training instances used
learning generalization increased generalization accuracy values
shown averages datasets table
surprisingly vdm distance functions increased accuracy fast faster
euclidean hoem even little data available may
little data available random positioning sample data input space
greater detrimental affect accuracy error statistical sampling vdm
functions
interesting note figure six distance functions seem pair
three distinct pairs interpolated vdm distance functions ivdm wvdm
maintain highest accuracy two vdm functions next functions
linear overlap distance remain lowest early graph


fiwilson martinez

efficiency considerations
section considers storage requirements learning speed generalization speed
presented
storage
distance functions must store entire training set requiring nm storage
n number instances training set number input attributes
application unless instance pruning technique used euclidean hoem
functions necessary even amount storage restrictive n
grows large
hvdm dvdm ivdm probabilities pa x c attributes discrete
attributes hvdm must stored requiring mvc storage v average number
attribute values discrete discretized attributes c number output
classes application possible instead store array da x vdma x hvdm
dvdm storage would mv savings c v
wvdm c probability values must stored continuous attribute value
resulting nmc storage typically much larger mvc n usually
much larger v cannot less necessary store list pointers
instances attribute requiring additional mn storage thus total storage
wvdm c nm cnm
distance function
euclidean
hoem
hvdm
dvdm
ivdm
wvdm

storage
mn
mn
mn mvc
mn mvc
mn mvc
cmn

learning time
mn
mn
mn mvc
mn mvc
mn mvc
mnlogn mvc

generalization time
mn
mn
mnc mn
mnc mn
mnc mn
mnc

table summary efficiency six distance metrics
table summarizes storage requirements system wvdm one
distance functions requires significantly storage others
applications n critical factor distance functions could used
conjunction instance pruning techniques reduce storage requirements see section
list several techniques reduce number instances retained training set
subsequent generalization
l earning speed
takes nm time read training set takes additional nm time standard
deviation attributes euclidean distance nm time ranges hoem
computing vdm statistics hvdm dvdm ivdm takes mn mvc time
approximately mn computing wvdm statistics takes mnlogn mnc time
approximately mnlogn
general learning time quite acceptable distance functions


fiimproved heterogeneous distance functions

generalization speed
assuming distance function must compare input vector training instances
euclidean hoem take mn time hvdm ivdm dvdm take mnc unless
da x stored instead pa x c hvdm case search done mn
time wvdm takes logn mnc mnc time
though c typically fairly small generalization process require
significant amount time computational resources n grows large techniques
k trees deng moore wess althoff derwand sproull
projection papadimitriou bentley reduce time required locate nearest
neighbors training set though may require modification handle
continuous nominal attributes pruning techniques used reduce storage section
reduce number instances must searched generalization

related work
distance functions used variety fields including instance learning neural
networks statistics pattern recognition cognitive psychology see section
references section lists several commonly used distance functions involving numeric
attributes
normalization often desirable linear distance function euclidean
distance attributes arbitrarily get weight others dividing
range standard deviation normalize numerical attributes common practice turney
turney halasz investigated contextual normalization standard
deviation mean used normalization continuous attributes depend context
input vector obtained attempt use contextual
normalization instead use simpler methods normalizing continuous attributes
focus normalize appropriately continuous nominal attributes
value distance metric vdm introduced stanfill waltz uses
attribute weights used functions presented modified value
difference metric mvdm cost salzberg rachlin et al use attribute
weights instead uses instance weights assumed systems use discretization
lebowitz schlimmer handle continuous attributes
ventura ventura martinez explored variety discretization methods
use systems use discrete input attributes found discretization
preprocess data often degraded accuracy recommended machine learning
designed handle continuous attributes directly
ting used several different discretization techniques conjunction
mvdm ib aha kibler albert showed improved generalization
accuracy discretization discretization allowed use mvdm
attributes instead linear distance continuous attributes thus avoided
normalization discussed sections similar
seen slightly higher dvdm discretizes continuous
attributes uses vdm compared hvdm uses linear distance
continuous attributes dvdm uses equal width intervals discretization


fiwilson martinez

tings make use advanced discretization techniques
domingos uses heterogeneous distance function similar hvdm rise
system hybrid rule instance learning system however rise uses normalization
scheme similar n sections square individual attribute
distances
mohri tanaka use statistical technique called quantification method ii qm
derive attribute weights present distance functions handle nominal
continuous attributes transform nominal attributes values boolean
attributes one time weights attribute actually
correspond individual attribute values original data
turney addresses cross validation error voting e values k
instance learning systems explores issues related selecting parameter k e
number neighbors used decide classification use k order
focus attention distance functions accuracy would improved
applications k
ivdm wvdm use nonparametric density estimation techniques tapia thompson
determining values p use computing distances parzen windows parzen
shifting histograms rosenblatt similar concept techniques
especially wvdm techniques often use gaussian kernels advanced
techniques instead fixed sized sliding window experimented gaussianweighted kernels well slightly worse wvdm ivdm perhaps
increased overfitting
applies distance function classification input
vector mapped discrete output class distance functions could used
systems perform regression atkeson moore schaal atkeson cleveland
loader output real value often interpolated nearby points
kernel regression deng moore
mentioned section elsewhere pruning techniques used reduce
storage requirements instance systems improve classification speed several
techniques introduced including ib aha kibler albert aha
condensed nearest neighbor rule hart reduced nearest neighbor rule gates
selective nearest neighbor rule rittler et al typical instance learning
zhang prototype methods chang hyperrectangle techniques
salzberg wettschereck dietterich rule techniques domingos
random mutation hill climbing skalak cameron jones others kibler aha
tomek wilson

conclusions future areas
many learning systems depend reliable distance function achieve accurate
generalization euclidean distance function many distance functions
inappropriate nominal attributes hoem function throws away information
achieve much better accuracy euclidean function
value difference metric vdm designed provide appropriate measure



fiimproved heterogeneous distance functions

distance two nominal attribute values however current systems use vdm
often discretize continuous data discrete ranges causes loss information
often corresponding loss generalization accuracy
introduced three distance functions heterogeneous value difference
function hvdm uses euclidean distance linear attributes vdm nominal attributes
uses appropriate normalization interpolated value difference metric ivdm
windowed value difference metric wvdm handle continuous attributes within
paradigm vdm ivdm wvdm provide classification accuracy higher
average discretized version dvdm datasets continuous
attributes examined equivalent dvdm applications without
continuous attributes
experiments datasets ivdm wvdm achieved higher average accuracy
hvdm better dvdm hoem euclidean distance ivdm
slightly accurate wvdm requires less time storage thus would seem
desirable distance function heterogeneous applications similar used
properly normalized euclidean distance achieves comparable generalization
accuracy nominal attributes situations still appropriate
distance function
learning system used obtain generalization accuracy nearest
neighbor classifier hvdm ivdm wvdm distance functions used knearest neighbor classifier k incorporated wide variety systems
allow handle continuous values including instance learning
pebls radial basis function networks distance neural networks
distance metrics used areas statistics cognitive psychology pattern
recognition areas distance heterogeneous input vectors
interest distance functions used conjunction weighting schemes
improvements system provides
distance functions presented improved average generalization
datasets used experimentation hoped datasets representative kinds
applications face real world distance functions
continue provide improved generalization accuracy cases
future look determining conditions distance function
appropriate particular application look closely selecting
window width look possibility smoothing wvdms probability landscape
avoid overfitting distance functions used conjunction variety
weighting schemes provide robust generalization presence noise
irrelevant attributes well increase generalization accuracy wide variety
applications

references
aha david w tolerating noisy irrelevant novel attributes instance
learning international journal man machine studies vol pp
aha david w dennis kibler marc k albert instance learning
machine learning vol pp


fiwilson martinez

atkeson chris local control movement touretzky ed
advances neural information processing systems san mateo ca morgan kaufmann
atkeson chris andrew moore stefan schaal locally weighted learning
appear artificial intelligence review
batchelor bruce g pattern recognition ideas practice york plenum press
pp
biberman yoram context similarity measure proceedings european
conference machine learning ecml catalina italy springer verlag pp
broomhead lowe multi variable functional interpolation adaptive
networks complex systems vol pp
cameron jones r instance selection encoding length heuristic random
mutation hill climbing proceedings eighth australian joint conference
artificial intelligence pp
carpenter gail stephen grossberg massively parallel architecture
self organizing neural pattern recognition machine computer vision graphics
image processing vol pp
chang chin liang finding prototypes nearest neighbor classifiers ieee
transactions computers vol pp
cleveland w c loader computational methods local regression
technical report murray hill nj bell laboratories statistics department
cost scott steven salzberg weighted nearest neighbor
learning symbolic features machine learning vol pp
cover p e hart nearest neighbor pattern classification institute
electrical electronics engineers transactions information theory vol
pp
dasarathy belur v nearest neighbor nn norms nn pattern classification
techniques los alamitos ca ieee computer society press
deng kan andrew w moore multiresolution instance learning
appear proceedings international joint conference artificial intelligence
ijcai
diday edwin recent progress distance similarity measures pattern
recognition second international joint conference pattern recognition pp
domingos pedro rule induction instance learning unified
appear international joint conference artificial intelligence ijcai
dudani sahibsingh distance weighted k nearest neighbor rule ieee
transactions systems man cybernetics vol april pp


fiimproved heterogeneous distance functions

gates g w reduced nearest neighbor rule ieee transactions information
theory vol pp
giraud carrier christophe tony martinez efficient metric heterogeneous
inductive learning applications attribute value language intelligent systems pp

hart p e condensed nearest neighbor rule institute electrical
electronics engineers transactions information theory vol pp
hecht nielsen r counterpropagation networks applied optics vol pp

kibler david w aha learning representative exemplars concepts
initial case study proceedings fourth international workshop machine
learning irvine ca morgan kaufmann pp
kohonen teuvo self organizing map proceedings ieee vol
pp
lebowitz michael categorizing numeric information generalization cognitive
science vol pp
merz c j p murphy uci repository machine learning databases
irvine ca university california irvine department information computer
science internet http www ics uci edu mlearn mlrepository html
michalski ryszard robert e stepp edwin diday recent advance data
analysis clustering objects classes characterized conjunctive concepts
progress pattern recognition vol laveen n kanal azriel rosenfeld eds
york north holland pp
mitchell tom need biases learning generalizations j w shavlik
g dietterich eds readings machine learning san mateo ca morgan
kaufmann pp
mohri takao hidehiko tanaka optimal weighting criterion case
indexing numeric symbolic attributes w aha ed case
reasoning papers workshop technical report ws menlo park
ca aiii press pp
nadler morton eric p smith pattern recognition engineering york
wiley pp
nosofsky robert attention similarity identification categorization
relationship journal experimental psychology general vol pp
papadimitriou christos h jon louis bentley worst case analysis nearest
neighbor searching projection lecture notes computer science vol
automata languages programming pp


fiwilson martinez

parzen emanuel estimation probability density function mode annals
mathematical statistics vol pp
quinlan j r unknown attribute values induction proceedings th
international workshop machine learning san mateo ca morgan kaufmann pp

rachlin john simon kasif steven salzberg david w aha towards better
understanding memory bayesian classifiers proceedings
eleventh international machine learning conference brunswick nj morgan
kaufmann pp
renals steve richard rohwer phoneme classification experiments radial
basis functions proceedings ieee international joint conference neural
networks ijcnn vol pp
rittler g l h b woodruff r lowry l isenhour
selective nearest neighbor decision rule ieee transactions information theory
vol pp
rosenblatt murray remarks nonparametric estimates density function
annals mathematical statistics vol pp
rumelhart e j l mcclelland parallel distributed processing mit press
ch pp
salzberg steven nearest hyperrectangle learning method machine learning
vol pp
schaffer cullen selecting classification method cross validation machine
learning vol
schaffer cullen conservation law generalization performance proceedings
eleventh international conference machine learning ml morgan
kaufmann
schlimmer jeffrey c learning representation change proceedings
sixth national conference artificial intelligence aaai vol pp
skalak b prototype feature selection sampling random mutation hill
climbing algorithsm proceedings eleventh international conference
machine learning ml morgan kaufman pp
sproull robert f refinements nearest neighbor searching k dimensional
trees algorithmica vol pp
stanfill c waltz toward memory reasoning communications
acm vol december pp



fiimproved heterogeneous distance functions

tapia richard james r thompson nonparametric probability density
estimation baltimore md johns hopkins university press
ting kai ming discretization continuous valued attributes instance
learning technical report basser department computer science university
sydney australia
ting kai ming discretisation lazy learning appear special issue
lazy learning artificial intelligence review
tomek ivan experiment edited nearest neighbor rule ieee
transactions systems man cybernetics vol june pp
turney peter theoretical analyses cross validation error voting instancebased learning journal experimental theoretical artificial intelligence jetai
pp
turney peter exploiting context learning classify proceedings
european conference machine learning vienna austria springer verlag pp
turney peter michael halasz contextual normalization applied aircraft gas
turbine engine diagnosis journal applied intelligence vol pp
tversky amos features similarity psychological review vol pp
ventura dan discretization preprocessing step supervised learning
masters thesis department computer science brigham young university
ventura dan tony r martinez empirical comparison discretization
methods proceedings tenth international symposium computer
information sciences pp
wasserman philip advanced methods neural computing york ny van
nostrand reinhold pp
wess stefan klaus dieter althoff guido derwand k trees improve
retrieval step case reasoning stefan wess klaus dieter althoff
richter eds topics case reasoning berlin springer verlag pp
wettschereck dietrich thomas g dietterich experimental comparison
nearest neighbor nearest hyperrectangle machine learning vol
pp
wettschereck dietrich david w aha takao mohri review comparative
evaluation feature weighting methods lazy learning technical
report aic washington c naval laboratory navy center
applied artificial intelligence



fiwilson martinez

wilson randall tony r martinez potential prototype styles
generalization proceedings sixth australian joint conference artifical
intelligence ai pp
wilson randall tony r martinez heterogeneous radial basis functions
proceedings international conference neural networks icnn vol pp

wilson dennis l asymptotic properties nearest neighbor rules edited
data ieee transactions systems man cybernetics vol pp
wolpert david h overfitting avoidance bias technical report sfi tr santa fe nm santa fe institute
zhang jianping selecting typical instances instance learning proceedings
ninth international conference machine learning




