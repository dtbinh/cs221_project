Journal Artificial Intelligence Research 6 (1997) 177-209

Submitted 10/96; published 5/97

Connectionist Theory Refinement:
Genetically Searching Space Network Topologies
David W. Opitz

opitz@cs.umt.edu

Jude W. Shavlik

shavlik@cs.wisc.edu

Department Computer Science
University Montana
Missoula, MT 59812 USA

Computer Sciences Department
University Wisconsin
1210 W. Dayton St.
Madison, WI 53706 USA

Abstract

algorithm learns set examples ideally able exploit
available resources (a) abundant computing power (b) domain-specific knowledge
improve ability generalize. Connectionist theory-refinement systems, use background knowledge select neural network's topology initial weights, proven
effective exploiting domain-specific knowledge; however, exploit available computing power. weakness occurs lack ability refine
topology neural networks produce, thereby limiting generalization, especially
given impoverished domain theories. present Regent algorithm uses
(a) domain-specific knowledge help create initial population knowledge-based neural networks (b) genetic operators crossover mutation (specifically designed
knowledge-based networks) continually search better network topologies. Experiments three real-world domains indicate new algorithm able significantly
increase generalization compared standard connectionist theory-refinement system,
well previous algorithm growing knowledge-based networks.

1. Introduction
Many scientific industrial problems better understood learning samples
task. reason, machine learning statistics communities devote considerable research effort inductive-learning algorithms. Often, however, learning
algorithms fail capitalize number potentially available resources, domainspecific knowledge computing power, improve ability generalize. Using
domain-specific knowledge desirable inductive learners start approximately correct theory achieve improved \generalization" (accuracy examples
seen training) significantly fewer training examples (Ginsberg, 1990; Ourston
& Mooney, 1994; Pazzani & Kibler, 1992; Towell & Shavlik, 1994). Making effective use
available computing power desirable because, many applications, important
obtain concepts generalize well induce concepts quickly. article, present algorithm, called Regent (REfining, Genetic Evolution, Network
Topologies), utilizes available computer time extensively search neural-network

c 1997 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiOpitz & Shavlik

topology best explains training data minimizing changes domain-specific
theory.
Inductive learning systems utilize set approximately correct, domain-specific
inference rules (called domain theory) describe currently known
domain, called theory-refinement systems. Making use knowledge
shown important since rules may contain insight easily obtainable
current set training examples (Ourston & Mooney, 1994; Pazzani & Kibler, 1992;
Towell & Shavlik, 1994). domains, expert created theory willing
wait weeks, even months, learning system produce improved theory.
Thus, given rapid growth computing power, believe important learning
techniques able trade expense large numbers computing cycles gains
predictive accuracy. Analogous anytime planning techniques (Dean & Boddy, 1988),
believe machine learning researchers create better anytime learning algorithms.
learning algorithms produce good concept quickly, continue search
concept space, reporting new \best" concept whenever one found.
concentrate connectionist theory-refinement systems, since shown
frequently generalize better many inductive-learning theory-refinement
systems (Fu, 1989; Lacher, Hruska, & Kuncicky, 1992; Towell, 1991). Kbann (Towell &
Shavlik, 1994) example connectionist system; translates provided
domain theory neural network, thereby determining network's topology,
refines reformulated rules using backpropagation (Rumelhart, Hinton, & Williams,
1986). However, Kbann, connectionist theory-refinement systems
alter network topologies, suffer given impoverished domain theories { ones
missing rules needed adequately learn true concept (Opitz & Shavlik, 1995;
Towell & Shavlik, 1994). TopGen (Opitz & Shavlik, 1995) improvement
systems; heuristically searches space possible network topologies adding
hidden nodes neural representation domain theory. TopGen showed statistically
significant improvements Kbann several real-world domains (Opitz & Shavlik, 1995);
however, paper empirically show TopGen nevertheless suffers
considers simple expansions Kbann network.
address limitation, broaden types topologies TopGen considers
using genetic algorithms (GAs). choose GAs two reasons. First, GAs
shown effective optimization techniques ecient use global
information (Goldberg, 1989; Holland, 1975; Mitchell, 1996). Second, GAs inherent
quality makes suitable anytime learning. \off-line" application mode
(DeJong, 1975), GAs simulate many alternatives output best alternative seen
far.
new algorithm, Regent, proceeds first trying generate, domain
theory, diversified initial population. produces new candidate networks via
genetic operators crossover mutation, networks trained using
backpropagation. Regent's crossover operator tries maintain rule structure
network, mutation operator adds nodes network using TopGen algorithm. Hence, genetic operators specialized connectionist theory refinement.
Experiments reported herein show Regent better able search network topologies TopGen.
178

fiConnectionist Theory Refinement

rest paper organized follows. next section, brie argue
importance effectively exploiting data, theory, available computer time
learning process. review Kbann TopGen algorithms. present
details Regent algorithm Section 4. followed empirical results
three Human Genome Project domains. Section 6, discuss results, well
future work. review related work concluding.

2. Using Data, Prior Knowledge, Available CPU Cycles
system learns set labeled examples called inductive learner (alternately, supervised, empirical, similarity-based learner). output example
provided teacher, set labeled examples given learner called
training set. task inductive learning generate training set concept
description correctly predicts output future examples,
training set. Many inductive-learning algorithms previously studied (e.g.,
Michalski, 1983; Quinlan, 1986; Rumelhart et al., 1986). algorithms differ
concept-representation language, method (or bias) constructing
concept within language. differences important since determine
concepts classifier induce.
alternative inductive learning paradigm build concept description
set examples, querying experts field directly assembling set
rules describe concept (i.e., build expert system; Waterman, 1986). problem
building expert systems theories derived interviewing experts tend
approximately correct. Thus, expert-provided domain theory usually
good first approximation concept learned, inaccuracies frequently exposed
empirical testing.
Theory-refinement systems (Ginsberg, 1990; Ourston & Mooney, 1994; Pazzani & Kibler,
1992; Towell & Shavlik, 1994) systems revise theory basis collection
examples. systems try improve theory making minimal repairs theory
make consistent training data. Changes initial domain theory
kept minimum theory presumably contains useful information, even
completely correct. hybrid learning systems designed learn
theory data, empirical tests shown achieve high generalization
significantly fewer examples purely inductive-learning techniques (Ourston & Mooney,
1994; Pazzani & Kibler, 1992; Towell & Shavlik, 1994). Thus, ideal inductive-learning
system able incorporate background knowledge available
form domain theory improve ability generalize.
indicated earlier, available computer time important resource since (a) computing power rapidly increasing, (b) problems expert willing wait
lengthy period improved concept. reasons, one develop \anytime"
learning algorithms continually improve quality answer time. Dean
Boddy (1988) defined criteria anytime algorithm be: (a) algorithm
suspended resumed minimal overhead, (b) algorithm stopped
time return answer, (c) algorithm must return answers improve
179

fiOpitz & Shavlik

x
x

x

x

Output

x x

x

x

x

Input

Figure 1: classical regression example smooth function (the solid curve)
fit noisy data points (the x's) probably better predictor
high-degree polynomial (the dashed curve).

time. criteria created planning scheduling algorithms,
apply inductive learning algorithms well.1
standard inductive learners, backpropagation (Rumelhart et al., 1986)
ID3 (Quinlan, 1986), unable continually improve answers (at least
receive additional training examples). fact, run long, algorithms tend
\overfit" training set (Holder, 1991). Overfitting occurs learning algorithm
produces concept captures much information training examples,
enough general characteristics domain whole. concepts
great job classifying training instances, poor job generalizing
new examples { ultimate measure success. help illustrate point, consider
typical regression case shown Figure 1. Here, fitting noisy data high-degree
polynomial likely lead poor generalization.
general framework use encouraging algorithm improve answer
time quite simple. spend computer time considering many different possible
concept descriptions, scoring possibility, always keeping description scores
best. framework anytime respect scoring function. scoring
function approximate measure generalization obviously still prone
problems overfitting; thus guarantee generalization monotonically
decrease time. Nevertheless, assuming accurate scoring function, long
considering wide range good possibilities, quality best concept likely
improve longer period time.
1. use term anytime learning differs Grefenstette Ramsey (1992); use
mean continuous learning changing environment.

180

fiConnectionist Theory Refinement

3. Review KBANN TopGen
goal research exploit prior knowledge available computing cycles
search neural network likely generalize best. proceed
choosing, initial guess, network defined Kbann algorithm.
continually refine topology find best network concept. presenting
new algorithm (Regent), give overview Kbann algorithm well
initial approach refining Kbann-created network's topology (TopGen).

3.1 KBANN Algorithm
Kbann (Towell & Shavlik, 1994) works translating domain theory consisting set

propositional rules directly neural network (see Figure 2). Figure 2a shows Prologlike rule set defines membership category a. Figure 2b represents hierarchical
structure rules, solid lines representing necessary dependencies dotted lines
representing prohibitory dependencies. Figure 2c represents network Kbann creates
translation. sets biases nodes representing disjuncts output
near 1 least one high-weighted antecedents satisfied, nodes
representing conjuncts must high-weighted antecedents satisfied (i.e., near
1 positive links near 0 negative links). Otherwise activations near 0. Kbann
creates nodes b1 b2 Figure 2c handle two rules disjunctively defining b.
thin lines Figure 2c represent low-weighted links Kbann adds allow rules
add new antecedents backpropagation training. Following network initialization,
Kbann uses available training instances refine network links. Refer Towell
(1991) Towell Shavlik (1994) details.
Kbann successfully applied several real-world problems, control
chemical plant (Scott, Shavlik, & Ray, 1992), protein folding (Maclin & Shavlik, 1993),


: b, c.
b : d, f, g.

b



b1

b : d, f, i.
c : h, j, k.

(a)

b

c

e f g h j k
(b)

e

b2

f

g h
(c)

c

j k

Figure 2: KBANN's translation knowledge base neural network. Panel (a) shows
sample propositional rule set Prolog (Clocksin & Mellish, 1987) notation,
panel (b) illustrates rule set's corresponding and/or dependency tree,
panel (c) shows resulting network created Kbann's translation.
181

fiOpitz & Shavlik

finding genes sequence DNA (Opitz & Shavlik, 1995; Towell & Shavlik, 1994),
ECG patient monitoring (Watrous, Towell, & Glassman, 1993). case, Kbann
shown produce improvements generalization standard neural networks small
numbers training examples. fact, Towell (1991) favorably compared Kbann
wide variety algorithms, including purely symbolic theory-refinement systems,
version promoter splice-junction tasks include testbeds Section 5.
training Kbann-created network alters antecedents existing rules,
capability inducing new rules add additional
hidden nodes training. instance, Kbann unable add third rule
inferring b Figure 2's example. help illustrate point, consider following
example. Assume Figure 2's target concept consists Figure 2a's domain theory plus
rule:
b :- d, e, g.
Although trained Kbann network shown Figure 2c possible examples
target concept, unable completely learn conditions true.
topology Kbann network must modified order learn new rule.
Studies show (Opitz & Shavlik, 1995; Towell, 1991) Kbann effective
removing extraneous rules antecedents expert-provided domain theory, generalization ability suffers given \impoverished" domain theories { theories
missing rules antecedents needed adequately learn true concept. ideal connectionist theory-refinement algorithm, therefore, able dynamically expand
topology network training.

3.2 TopGen Algorithm

TopGen (Opitz & Shavlik, 1995) addresses Kbann's limitation heuristically searching
space possible expansions knowledge-based neural network { network
whose topology determined direct mapping dependencies domain theory
(e.g., Kbann network). TopGen proceeds first training Kbann network,
placing search queue. cycle, TopGen takes best network search
queue, estimates errors occur network, adds new nodes response
estimates, trains new networks, places back queue. TopGen judges
errors occur network using training examples increment two counters
node, one false negatives one false positives.
Figure 3 illustrates possible ways TopGen add nodes one networks.
symbolic rule base uses negation-by-failure, one decrease false negatives either
dropping antecedents existing rules adding new rules rule base. Kbann
effective removing antecedents existing rules, unable add new rules;
therefore, TopGen adds nodes, intended decreasing false negatives, fashion
analogous adding new rule rule base. existing node node, TopGen
adds new node child (see Figure 3a), fully connects new node input
nodes. existing node node, TopGen creates new node
parent original node another new node TopGen fully connects
inputs (see Figure 3c); TopGen moves outgoing links original node (A
Figure 3c) become outgoing links new node.
182

fiConnectionist Theory Refinement

Existing Node

Decrease False Negatives

Decrease False Positives
...

...

...

New
Node





New
Node


B

B

C

Node

C

New
Node

B

C

(a)

(b)
...



C

Node



New
Node


B

...

New
Node

...

B
B

C

C

(c)

New
Node

(d)

Figure 3: Possible ways add new nodes knowledge-based neural network (arcs indicate nodes). decrease false negatives, wish broaden applicability node. Conversely, decrease false positives, wish
constrain node.
symbolic rule base, one decrease false positives either adding antecedents
existing rules removing rules rule base. Kbann effectively remove
rules, less effective adding antecedents rules unable invent (i.e.,
constructively induce; Michalski, 1983) new terms antecedents. Thus TopGen adds new
nodes, intended decrease false positives, fashion analogous adding new
constructively induced antecedents network. Figures 3b 3d illustrates
done (analogous Figures 3a 3c explained above). Refer Opitz Shavlik (1993;
1995) details.
TopGen showed statistically significant improvements Kbann several real-world
domains, comparative experiments simpler approach adding nodes verified
new nodes must added intelligent manner (Opitz & Shavlik, 1995).
article, however, increase number networks TopGen considers search
show increase generalization primarily limited first networks
considered. Therefore, TopGen much \anytime" algorithm, rather first
step towards one. mostly due fact TopGen considers larger networks contain original Kbann network subgraphs; however, one increases
number networks considered, one increase variety networks considered
183

fiOpitz & Shavlik

search. Broadening range networks considered search
topology space major focus paper.

4. REGENT Algorithm

new algorithm, Regent, tries broaden types networks TopGen considers
use GAs. view Regent two phases: (a) genetically searching
topology space, (b) training network using backpropagation's gradient
descent method. Regent uses domain theory aid phases. uses theory
help guide search topology space give good starting point weight
space.
Table 1 summarizes Regent algorithm. Regent first sets aside validation set
(from part training instances) use scoring different networks. perturbs Kbann-produced network create initial set candidate networks. Next,
Regent trains networks using backpropagation places population. cycle, Regent creates new networks crossing mutating networks
current population randomly picked proportional fitness (i.e.,
validation-set correctness). trains new networks places
population. searches, Regent keeps network lowest validation-set
error best concept seen far, breaking ties choosing smaller network
application Occam's Razor. parallel version Regent trains many candidate networks time using Condor system (Litzkow, Livny, & Mutka, 1988),
runs jobs idle workstations.
diverse initial population broaden types networks Regent considers
search; however, since domain theory may provide useful information may
present training set, still desirable use theory generating initial
population. Regent creates diversity around domain theory randomly perturbing
Kbann network various nodes. Regent perturbs node either deleting it,
adding new nodes manner analogous one TopGen's four methods adding

GOAL: Search best network topology describing domain theory data.

1. Set aside validation set training instances.
2. Perturb Kbann-produced network multiple ways create initial networks, train
networks using backpropagation place population.
3. Loop forever:
(a) Create new networks using crossover mutation operators.
(b) Train networks backpropagation, score validation set, place
population.
(c) new network network lowest validation-set error seen far (breaking
ties preferring smallest network), report current best concept.

Table 1: REGENT algorithm.
184

fiConnectionist Theory Refinement

Crossover Two Networks:

GOAL: Crossover two networks generate two new network topologies.
1. Divide network's hidden nodes sets B using DivideNodes.

2. Set forms one network, set B forms another. new network created follows:
(a) network inherits weight w parent nodes j either inherited
input output nodes.
(b) Link unconnected nodes levels near-zero weights.
(c) Adjust node biases keep original function node (see text explanation).
ji

DivideNodes:
GOAL:

Divide hidden nodes sets B, probabilistically maintaining
network's rule structure.
hidden node assigned set B:
(i) Collect unassigned hidden nodes whose output linked either previouslyassigned nodes output nodes.
(ii) set set B empty:
node collected part (i), randomly assign set set B.

Else

Probabilistically add nodes collected part (i) set set B. Equation 1
shows probability assigned set A. probability assigned
set B one minus value.

Table 2: REGENT's method crossing networks.
nodes. (Should happen multiple theories domain,
used seed population.)

4.1 REGENT's Crossover Operator

Regent crosses two networks first dividing nodes parent network

two sets, B, combining nodes set form two new networks (i.e.,
nodes two sets form one network, nodes two B sets form another).
Table 2 summarizes Regent's method crossover Figure 4 illustrates
example. Regent divides nodes, one level2 time, starting level nearest
output nodes. considering level, either set set B empty, cycles
node level randomly assigns either set. neither set empty, nodes
probabilistically placed set. following equation calculates probability
2. Although one define level several different ways, define node's level longest path
output node.

185

fiOpitz & Shavlik

Original
Networks
Crossed


Output

Output

Input

Input

Output

Output

Input

Input

Resulting
Networks
Figure 4: REGENT's method crossing two networks. hidden nodes
original network divided sets B; nodes two sets
form one new network, nodes two B sets form another. Grey lines
represent low-weighted links added fully connect neighboring levels.
given node assigned set A:

Pj2A jwjij
Prob(node 2 setA) = P jw j + P jw j ;
j 2A ji
j 2B ji

(1)

j 2 means node j member set wji weight value node
node j . probability belonging set B one minus probability.
probabilities, Regent tends assign set nodes heavily linked
together. helps minimize destruction rule structure crossed-over
networks, since nodes belonging syntactic rule connected heavily linked
weights. Thus, Regent's crossover operator produces new networks crossing-over rules,
rather simply crossing-over nodes.
Regent must next decide connect nodes newly created networks.
First, new network inherits weight values parents links (a) connect
two nodes inherited new network, (b) connect inherited hidden
node input output node, (c) directly connect input node output node.
adds randomly set, low-weighted links unconnected nodes consecutive
levels.
Finally, adjusts bias nodes help maintain original function.
instance, Regent removes positively weighted incoming link node,
decrements node's bias subtracting product link's magnitude
186

fiConnectionist Theory Refinement

average activation (over set training examples) entering link.
bias node needs slightly less sum positive weights
incoming links (see Towell Shavlik, 1994 details). Regent increments
bias node analogous amount removes negatively weighted incoming
links (since bias node slightly greater sum negative
weights incoming links node inactive incoming negatively
weighted linked nodes active positively weighted linked nodes inactive).

4.2 REGENT's Mutation Operator

Regent mutates networks applying variant TopGen. Regent uses TopGen's
method incrementing false-negatives false-positives counters node. Regent adds nodes, based values counters, way TopGen does.

Since neural learning effective removing unwanted antecedents rules KNNs
(see Section 3.1), Regent considers adding nodes, deleting them, mutation. Thus, mutation operator adds diversity population, still maintaining
directed, heuristic-search technique choosing add nodes; directedness
necessary currently unable evaluate thousand possible
networks per day.

4.3 Additional Details
Regent adds newly trained networks population validation-set correctness better equal existing member population. Regent

replaces member, replaces member lowest correctness (ties broken
choosing oldest member). techniques (Goldberg, 1989), replacing
member nearest new candidate network, promote diverse populations; however,
want promote diversity expense decreased generalization. future
research topic, plan investigate incorporating diversity-promoting techniques
able consider tens thousands networks.
Regent considered Lamarckian3 , genetic-hillclimbing algorithm (Ackley, 1987),
since performs local optimizations individuals, passes successful optimizations
offspring. ability individuals learn smooth fitness landscape
facilitate subsequent learning. Thus, Lamarckian learning lead large increase
learning speed solution quality (Ackley & Littman, 1994; Farmer & Belin, 1992).

5. Experimental Results
section, test Regent three real-world Human Genome Project problems
aid locating genes DNA sequences (recognizing promoters, splice-junctions,
ribosome-binding sites). domains, input short segment DNA nucleotides
(about 100 elements long) task learn predict DNA subsequence contains
biologically important site. domain accompanied domain theory generated
DNA expert (M. Noordewier).
3. Lamarckian evolution theory based inheritance characteristics acquired lifetime.

187

fiOpitz & Shavlik

promoter domain contains 234 positive examples, 702 negative examples, 31
rules. splice-junction domain contains 1,200 examples distributed equally among three
classes, 23 rules. Finally, ribosome binding sites (RBS) domain, contains 366
positive examples, 1,098 negative examples, 17 rules. (Note promoter data set
domain theory later version one appears Towell, 1994.) domains
available University Wisconsin Machine Learning (UW-ML) site via World
Wide Web (ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/datasets/)
anonymous ftp (ftp.cs.wisc.edu, machine-learning/shavlik-group/datasets).
first directly compare Regent TopGen Kbann. perform
lesion study4 Regent. particular, investigate value adding randomly
created networks Regent's initial population examine utility Regent's
genetic operators.

5.1 Experimental Methodology

results article ten-fold cross validation runs. ten-fold cross
validation data set first partitioned ten equal-sized sets, set turn
used test set classifier trains nine sets. fold, Regent
run population size 20. network trained using backpropagation. Parameter
settings neural networks include learning rate 0.10, momentum term 0.9,
number training epochs 20; first two standard settings 20
epochs may fewer typically found neural network literature, set 20
help avoid overfitting. set aside validation set consisting 10% training
examples Regent use scoring function.

5.2 Generalization Ability REGENT

section's experiments compare test-set accuracy (i.e., generalization) Regent
TopGen's. Figure 5 shows test-set error Kbann, TopGen, Regent
search space network topologies. horizontal line graph results
Kbann algorithm. drew horizontal line sake visual comparison;
recall Kbann considers single network. first point graph,
one network considered, nearly three algorithms, since start
Kbann network; however, TopGen Regent differ slightly Kbann since
must set aside part training set score candidate networks. Notice
TopGen stops improving considering 10 30 networks generalization
ability Regent better TopGen's point. reason occasional
upward movements Figure 5 due fact validation set (or scoring
function) inexact estimate true generalization error (as results
ten-fold cross validation).
Figure 6 presents test-set error TopGen Regent consider
500 candidate topologies. standard neural network results fully connected,
single-layer, feed-forward neural network; fold, trained 20 networks containing
100 hidden nodes used validation set choose best network. results
4. lesion study one components algorithm individually disabled ascertain
contribution full algorithm's performance (Kibler & Langley, 1988).

188

fiConnectionist Theory Refinement

10 %

8%
Ribosome Binding Sites

6%
KBANN
TopGen

4%

REGENT

TestSet Error

2%

6%

4%

Splice Junctions

2%

6%

4%
Promoters
2%

0

100

200

300

400

Networks Considered
Figure 5: Error rates three Human Genome problems.

189

500

fiOpitz & Shavlik

show Kbann generalizes much better best standard networks, thus
confirming Kbann's effectiveness generating good network topologies. TopGen
able improve Kbann network, Regent able significantly decrease error
rate Kbann TopGen. (For benchmark purposes, Regent error rate
3.9% ten-fold cross validation full Splice Junction dataset 3190 examples
commonly used machine learning researchers.)
Table 3 contains number hidden nodes final networks produced Kbann,
TopGen, Regent. results demonstrate Regent produces networks
larger Kbann's TopGen's networks (even though TopGen adds nodes
search). Regent's networks larger, necessarily mean
\complex." inspected sample networks found large
portions network either used (e.g., weights insignificantly small)
functional duplications groups hidden nodes.
One could prune weights nodes Regent's search; however, pruning
prematurely reduce variety structures available recombination crossover
(Koza, 1992). Real-life organisms, instance, super uous DNA believed
enhance rate evolution (Watson, Hopkins, Roberts, Argetsinger-Steitz, & Weiner,
1987). However, pruning network size genetic search may unwise, one
could prune Regent's final network using, say, Hassibi Stork's (1992) Optimal Brain
Surgeon algorithm. post-pruning process may increase future classification speed
network, well increase comprehensibility possibly accuracy.

5.3 Lesion Study REGENT
section, describe lesion study performed Regent. Since single run
Regent takes four CPU days consider 500 networks, single ten-fold cross
10.70
10 %

9.40

9.15

TestSet Error

8.23
8%

7.83
6.62

6%

5.25 4.92

Key

6.26

Standard NN

5.25

4.08

4.17

KBANN
TopGen

4%

REGENT
2%

RBS

Splice Junctions

Promoters

Figure 6: Test-set error rates TopGen REGENT consider 500 networks. Pairwise, one-tailed t-tests indicate Regent differs Standard NN, Kbann,
TopGen 95% confidence level three problems.
190

fiConnectionist Theory Refinement

Domain
Kbann TopGen
Regent
RBS
18
42.1 (9.3) 70.1 (25.1)
Splice Junction
21
28.4 (4.1) 32.4 (12.2)
Promoters
31
40.2 (3.3) 74.9 (38.9)
Table 3: Number hidden nodes networks produced KBANN, TopGen,
REGENT. columns show mean number hidden nodes found within
networks. Standard deviations contained within parentheses;
report standard deviations Kbann since uses one network.

validation takes (a minimum of) 40 CPU days. Therefore, given inherent similarity
investigating various aspects Regent multiple datasets, feasible
run experiments section 95% confidence level reached cases
(assuming level actually exists). Nonetheless, results convey important
information various components Regent, and, shown previous section,
complete Regent algorithm generate statistically significant improvements
existing algorithms.
5.3.1 Including Non-KNNs REGENT's Population

correct theory may quite different initial domain theory. Thus,
section investigate whether one include, initial population networks,
variety networks obtained directly domain theory. Currently, Regent
creates initial population always perturbing Kbann network. include networks
obtained domain theory, first randomly pick number hidden
nodes include network, randomly create hidden nodes network.
adding new nodes randomly selected output hidden node using one
TopGen's four methods adding new nodes (refer Figure 3). Adding nodes
manner creates random networks whose node structure analogous dependencies found
symbolic rule bases, thus creating networks suitable Regent's crossover mutation
operators.
Table 4 shows test-set error Regent various percentages knowledge-based
neural networks (KNNs) present initial population. first row contains results
initializing Regent purely random initial population (i.e., population contains
KNNs). second row lists results Regent creates half population
domain theory, half randomly. Finally, last row contains results
seeding entire population domain theory.
results suggest including, initial population, networks
created domain theory increases Regent's test-set error three domains.
occurs randomly generated networks correct KNNs,
191

fiOpitz & Shavlik

0% KNN
50% KNN
100% KNN

RBS
9.7%
8.6%
8.2%

Splice Junction
6.3%
4.3%
4.1%

Promoters
5.1%
4.6%
4.2%

Table 4: Test-set error considering 500 networks. row gives pergentage
KNNs present initial population. Pairwise, one-tailed t-tests indicate
initializing Regent 100% KNNs differs 0% KNNs 95% confidence
level three domains; however, difference runs 50%
100% KNNs significant level.

thus offspring original KNN quickly replace random networks. Hence, diversity population suffers compared methods start whole population
KNNs. Assuming domain theory \malicious," therefore better seed
entire population Kbann network. domain theory indeed malicious
contain information promotes spurious correlations data, would
reasonable randomly create \whole" population. Running Regent
without domain theory allows one investigate utility theory.
results interesting GA point view. Forrest Mitchell (1993)
showed GAs perform poorly complex problems basic building blocks either
(a) non-trivial find (b) get split crossover. Seeding initial population
domain theory (as Regent does) help define basic building blocks
problems.
5.3.2 Value REGENT's Mutation

Typically GAs, mutation secondary operation sparingly used (Goldberg, 1989); however, Regent's mutation directed approach heuristically adds
nodes KNNs provenly effective manner (i.e., uses TopGen). therefore reasonable hypothesize one apply mutation operator frequently
traditionally done GAs. results section test hypothesis.
Figure 7 presents test-set error Regent varying percentages mutation
(versus crossover) creating new networks step 3a Table 1. graph plots four
curves: (a) 0% mutation (i.e., Regent uses crossover), (b) 10% mutation, (c) 50%
mutation, (d) 100% mutation. Performing mutations tests value solely using
crossover, 100% mutation tests ecacy mutation operator itself. Note
100% mutation TopGen different search strategy; instead keeping
OPEN list heuristic search, population KNNs generated members
population improved proportional fitness. two curves (10%
50% mutation) test synergy two operators. Performing 10% mutation
192

fiConnectionist Theory Refinement

10 %

8%

Ribosome Binding Sites
6%
0% Mutation
10% Mutation

4%

50% Mutation
100% Mutation

TestSet Error

2%

6%

4%

Splice Junctions
2%

6%

4%

Promoters
2%

0

100

200

300

400

500

Networks Considered
Figure 7: Error rates REGENT different fractions mutation versus crossover
considering 500 networks. Arguably due inherent similarity
algorithms, limited number runs due computational complexity,
results significant 95% confidence level.

193

fiOpitz & Shavlik

closer traditional GA viewpoint mutation secondary operation, 50%
mutation means operations equally valuable. (Previous experiments
section used 50% mutation 50% crossover.)
differences statistically significant, results nevertheless suggest
synergy exists two operations. Except middle portion
promoter domain, results show that, qualitatively, using operations
time better using either operation alone. fact, equally mixing mutation
crossover operator better three curves three domains Regent
considered 500 networks. result particularly pronounced splice-junction
domain.
5.3.3 Value REGENT's Crossover
Regent tries cross rules networks, rather blindly crossing

nodes. probabilistically dividing nodes network two sets
nodes belonging rule tend belong set. section,
test ecacy Regent's crossover comparing variant
randomly assigns nodes two sets (rather using DivideNodes Table 2).
Table 5 contains results test 250 networks considered.
first row, Regent-random-crossover, Regent randomly breaks hidden nodes
two sets, second row, Regent assigns nodes two sets according Table
2. cases, Regent creates half networks mutation operator,
half crossover operator. Although differences statistically significant,
results suggest keeping rule structure networks intact crossover
important; otherwise, basic building blocks networks (i.e., rules) get split
crossover, studies shown importance keeping intact basic building
blocks crossover (Forrest & Mitchell, 1993; Goldberg, 1989).

Regent-random-crossover
Regent

Promoters Splice Junction RBS
4.6%
4.7%
9.1%
4.4%
4.1%
8.8%

Table 5: Test-set error two runs REGENT: (a) randomly crossing \nodes"
networks, (b) one crossing \rules" network (defined
Equation 1). runs considered 250 networks used half crossover, half
mutation. results significant 95% confidence level;
slight difference learning algorithms long run-times limited
runs ten-fold cross validation.

6. Discussion Future Work

Towell (1991) showed Kbann generalized better many machine learning algorithms promoter splice-junction domains (the RBS dataset exist then).
194

fiConnectionist Theory Refinement

Despite success, Regent able effectively use available computer cycles significantly improve generalization Kbann previous improvement Kbann,
TopGen algorithm. Regent reduces Kbann's test-set error 12% RBS domain, 22% splice-junction domain, 33% promoter domain; reduces
TopGen's test-set error 10% RBS domain, 17% splice-junction domain,
21% promoter domain. Also, Regent's ability use available computing time
aided inherently parallel, since train many networks simultaneously.
results show Regent's two genetic operators complement other.
crossover operator considers large variety network topologies probabilistically combining rules contained within two \successful" KNNs. Mutation, hand, makes
smaller, directed improvements members population, time adding
diversity population adding new rules population. Equal use operators, therefore, allows wide variety topologies considered well allowing
incremental improvements members population.
Since Regent searches many candidate networks, important
able recognize networks likely generalize best. mind,
first planned extension Regent develop test different network-evaluation functions. currently use validation set; however, validation sets several drawbacks.
First, keeping aside validation set decreases number training instances available
network. Second, performance validation set noisy approximator
true error (MacKay, 1992; Weigend, Huberman, & Rumelhart, 1990). Finally,
increase number networks searched, Regent may start selecting networks
overfit validation set. fact, explains occasional upward trend test-set error,
TopGen Regent, Figure 5.
avoid problem overfitting data, common regression trick cost
function includes \smoothness" term along error term. best function,
then, smoothest function fits data well. neural networks, one
add estimated error smoothness component measure complexity
network. complexity network cannot simply estimated counting
number possible parameters, since tends significant duplication
function weight network, especially early training process (Weigend,
1993). Two techniques try take account effective size network
Generalized Prediction Error (Moody, 1991) Bayesian methods (MacKay, 1992).
Quinlan Cameron-Jones (1995) propose adding additional term accuracy
smoothness term takes account length time spent searching. coin
term \oversearching" describe phenomenon extensive searching causes
lower predictive accuracy. claim oversearching orthogonal overfitting,
thus complexity-based methods alone cannot prevent oversearching. increase
number networks consider search, may start oversearching,
thus plan investigate adding oversearching penalty term well.
indicated earlier, Regent Lamarckian passes local optimizations individuals (i.e., trained weights network) offspring. viable alternative, called
Baldwin effect (Ackley & Littman, 1992; Baldwin, 1896; Belew & Mitchell, 1996; Hinton &
Nowlan, 1987), local search still change fitness individual (backpropagation learning case), pass changes offspring (this form
195

fiOpitz & Shavlik

evolution Darwinian nature). Even though learned explicitly coded
genetic material, individuals best able learn offspring;
thus learning still impacts evolution. fact form evolution sometimes outperform forms Lamarckian evolution employ local search strategy (Whitley,
Gordon, & Mathias, 1994). Future work investigate utility Baldwin effect
Regent. case would cross trained networks, instead cross
initial weight settings backpropagation learning took place.
Finally, often times multiple, even con icting, theories domain. Future work, then, investigate ways using domain theories seed
initial population. Although results Section 5.3.1 show including randomly generated networks degrades generalization performance, seeding population multiple
approximately correct theories degrade generalization, assuming networks
initial correctness. Thus Regent able naturally
combine good parts multiple theories. Also, given domain theory, many
different equivalent ways represent theory using set propositional rules.
representation leads different network topology, even though network
starts theory, topologies may conducive neural refinement.

7. Related Work

Regent mainly differs previous work an\anytime" theory-refinement sys-

tem continually searches, non-hillclimbing manner, improvements domain
theory. summary, work unique provides connectionist approach
attempts effectively utilize available background knowledge available computer cycles
generate best concept possible. broken rest section four parts:
(a) connectionist theory-refinement algorithms, (b) purely symbolic theory-refinement algorithms, (c) algorithms find appropriate domain-specific neural-network topology,
(d) optimization algorithms wrapped around induction algorithms.

7.1 Connectionist Theory-Refinement Techniques

begin discussion connectionist theory-refinement systems. systems
developed refine many types rule bases. instance, number systems
proposed revising certainty-factor rule bases (Fu, 1989; Lacher et al., 1992;
Mahoney & Mooney, 1993), finite-state automata (Maclin & Shavlik, 1993; Omlin & Giles,
1992), push-down automata (Das, Giles, & Sun, 1992), fuzzy-logic rules (Berenji, 1991;
Masuoka, Watanabe, Kawamura, Owada, & Asakawa, 1990), mathematical equations
(Roscheisen, Hofmann, & Tresp, 1991; Scott et al., 1992). systems work
Kbann first translating domain knowledge neural network, modifying
weights resulting network. attempts (which describe next)
made dynamically adjust resulting network's topology training (as Regent
does).
TopGen Regent, Fletcher Obradovic (1993) present approach
adds nodes Kbann network. system constructs single layer nodes, fully
connected input output nodes, \off side" Kbann network.
generate new hidden nodes using variant Baum Lang's (1991) constructive
196

fiConnectionist Theory Refinement

algorithm. Baum Lang's algorithm first divides feature space hyperplanes.
find hyperplane randomly selecting two points different classes,
localizing suitable split points. Baum Lang repeat process
generate fixed number hyperplanes. Fletcher Obradovic map
Baum Lang's hyperplanes one new hidden node, thus defining weights
input layer hidden node. Fletcher Obradovic's algorithm change
weights Kbann portion network, modifications initial rule base
solely left constructed hidden nodes. Thus, system take advantage
Kbann's strength removing unwanted antecedents rules original rule
base. fact, TopGen compared favorably similar technique added nodes
side Kbann (Opitz & Shavlik, 1993) Regent outperformed TopGen
article's experiments.
Rapture (Mahoney & Mooney, 1994) designed domain theories containing probabilistic rules. connectionist theory-refinement systems, Rapture first translates
domain theory neural network, refines weights network
modified backpropagation algorithm. Regent, Rapture able dynamically
refine topology network. using Upstart algorithm (Frean,
1990) add new nodes network. Aside designed probabilistic rules,
Rapture differs Regent adds nodes intention completely
learning training set, generalizing well. Thus, Rapture hillclimbs
training set learned, Regent continually searches topology space looking network
minimizes scoring function's error. Also, Rapture initially creates links
specified domain theory, explicitly adds links ID3's (Quinlan,
1986) information-gain metric. Regent, hand, fully connect consecutive layers
networks, allowing rule possibility adding antecedents training.
Daid algorithm (Towell & Shavlik, 1992) extension Kbann uses
domain theory help train Kbann network. Since Kbann effective dropping antecedents adding them, Daid tries find potentially useful inputs features
mentioned domain theory. backing-up errors lowest level
domain theory, computing correlations features. Daid increases
weight links potentially useful input features based correlations.
Daid mainly differs Regent refine topology Kbann network. Thus, Daid addresses Kbann's limitation effectively adding antecedents,
still unable introduce new rules constructively induce new antecedents. Daid
therefore suffer impoverished domain theories. notice since Daid improvement training KNNs, Regent use Daid train network considers
search (however, done so).
Opitz Shavlik (1996) used variant Regent learning algorithm
generating neural network \ensemble." neural-network ensemble successful
technique outputs set separately trained neural networks combined
form one unified prediction (Drucker, Cortes, Jackel, LeCun, & Vapnik, 1994; Hansen
& Salamon, 1990; Perrone, 1993). Since Regent considers many networks, select
subset final population networks ensemble minimal extra cost. Previous
work, though, shown ideal ensemble one networks accurate
make errors different parts input space (Hansen & Salamon, 1990;
197

fiOpitz & Shavlik

Krogh & Vedelsby, 1995). result, Opitz Shavlik (1996) changed scoring
function Regent \fit" network one accurate
disagreed members population much possible. addition,
algorithm (Addemup) actively tries generate good candidates emphasizing
current population's erroneous examples backpropagation training. result
alterations, Addemup able create enough diversity among population
networks able effectively exploit knowledge domain theory. Opitz
Shavlik (1996) show Addemup able generate significantly better ensemble
using domain theory either running Addemup without benefit theory
simply combining Regent's final population networks. Actively searching highly
diverse population, however, aid searching single best network. fact,
single best network produced Addemup significantly worse Regent's single
best network three domains.

7.2 Purely Symbolic Theory-Refinement Techniques
Additional work related Regent includes purely symbolic theory-refinement systems
modify domain theory directly initial form. Systems Focl (Pazzani
& Kibler, 1992) Forte (Richards & Mooney, 1995) first-order, theory-refinement
systems revise predicate-logic theories. One drawback systems
currently generalize well connectionist approaches many real-world problems,
DNA promoter task (Cohen, 1992).
several genetic-based, first-order logic, multimodal concept learners
(Greene & Smith, 1993; Janikow, 1993). Giordana Saitta (1993) showed integrate one system, Regal (Giordana, Saitta, & Zini, 1994; Neri & Saitta, 1996),
deductive engine ML-SMART (Bergadano, Giordana, & Ponsero, 1989) help
refine incomplete inconsistent domain theory. version works first using automated theorem prover recognize unresolved literals proof, uses GA-based
Regal induce corrections literals. Regent, hand, use genetic
algorithms (along neural learning) refine whole domain theory time.
Dogma (Hekanaho, 1996) recently proposed GA-based learner use background knowledge learn description language Regal. Current restrictions,
however, force representation language domain theory propositional rules.
Dogma converts \ at" set background rules (i.e., handle intermediate
conclusions) individual bitstrings used building blocks higher-level
concept. Dogma focus theory refinement, rather builds completely new
theory using substructures background knowledge. term approach
theory-suggested theory-guided (Hekanaho, 1996).
Several systems, including ours, proposed refining propositional rule bases.
Early approaches could handle improvements overly specific theories (Danyluk,
1989) specializations overly general theories (Flann & Dietterich, 1989). Later systems
Rtls (Ginsberg, 1990), Either (Ourston & Mooney, 1994), Ptr (Koppel, Feldman,
& Segre, 1994), Tgci (Donoho & Rendell, 1995) later able handle types
refinements. discuss Either system representative propositional
systems.
198

fiConnectionist Theory Refinement

Either four theory-revision operators: (a) removing antecedents rule, (b)
adding antecedents rule, (c) removing rules rule base, (d) inventing new
rules. Either uses operators make revisions domain theory correctly
classify previously misclassified training examples without undefining
correctly classified examples. Either uses inductive learning algorithms invent new
rules; currently uses ID3 (Quinlan, 1986) induction component.
Even though Regent's mutation operator add nodes manner analogous
symbolic system adds antecedents rules, underlying learning algorithm \connectionist." Towell (1991) showed Kbann outperformed Either promoter
task, Regent outperformed Kbann article. Kbann's power domain
largely attributed ability make \fine-grain" refinements domain theory
(Towell, 1991). Either's diculty domain, Baffes Mooney (1993)
presented extension called Neither-MofN able learn -of-N rules {
rules true N antecedents true. improvement generated
concept closely matches Kbann's generalization performance.
want minimize changes theory, want expense accuracy; however, Donoho Rendell (1995) demonstrate existing
theory-refinement systems, Either, suffer able make small,
local changes domain theory. Thus, accurate theory significantly far
structure initial theory, systems forced either become trapped
local maximum similar initial theory, forced drop entire rules replace
new rules inductively created purely scratch. Regent
suffer translates theory less restricting representation
neural networks (Donoho & Rendell, 1995). Also, Regent able reconfigure
structure domain genetic algorithms.
Many authors reported results using varying subsets splice junction domain
(e.g., Donoho Rendell 1995; Mahoney 1996; Neri Saitta 1996, Towell Shavlik 1994). authors used different training set sizes, nevertheless worthwhile
qualitatively discuss conclusions here. Towell Shavlik (1994) compared
Kbann numerous machine learning algorithms learning algorithm
given training set 1000 examples; Kbann's generalization ability compared favorably
algorithms splice domain Regent, turn, compared favorably
Kbann article. Donoho Rendell (1995) showed purely symbolic approach
converged performance Kbann around 200 examples. Mahoney (1996) showed,
using training set sizes 400 examples, Rapture algorithm generalized
better Kbann domain; results look similar Regent. Finally,
Neri Saitta (1996) showed generalization ability GA-based Regal compares favorably purely symbolic, non-GA based techniques; used slightly
different training set sizes article, Regent compares well results
reported paper.

7.3 Finding Appropriate Network Topologies
third area related work covers techniques attempt find good domaindependent topology dynamically refining network's topology training. Many
199

fiOpitz & Shavlik

studies shown generalization ability neural network depends topology network (Baum & Haussler, 1989; Tishby, Levin, & Solla, 1989). trying
find appropriate topology, one approach construct modify topology
incremental fashion. Network-shrinking algorithms start many parameters,
remove nodes weights training (Hassibi & Stork, 1992; Le Cun, Denker, &
Solla, 1989; Mozer & Smolensky, 1989). Network-growing algorithms, hand,
start parameters, add nodes weights training (Blanziere
& Katenkamp, 1996; Fahlman & Lebiere, 1989; Frean, 1990). obvious difference Regent algorithms Regent uses domain knowledge
symbolic rule-refinement techniques help determine network's topology. Also,
algorithms restructure network based solely training-set error, Regent
minimized validation-set error.
Instead incrementally finding appropriate topology, one mount \richer"
search hillclimbing space topologies. One common approach
combine genetic algorithms neural networks (as Regent does). Genetic algorithms
applied neural networks two different ways: (a) optimize connection
weights fixed topology, (b) optimize topology network. Techniques
solely use genetic algorithms optimize weights (Montana & Davis, 1989; Whitley
& Hanson, 1989) performed competitively gradient-based training algorithms;
however, one problem genetic algorithms ineciency fine-tuned local search,
thus scalability methods question (Yao, 1993). Kitano (1990b) presents
method combines genetic algorithms backpropagation. using
genetic algorithm determine starting weights network,
refined backpropagation. Regent differs Kitano's method use domain
theory help determine network's starting weights genetically search, instead,
appropriate network topologies.
methods use genetic algorithms optimize network topology similar
Regent use backpropagation train network's weights.
methods, many directly encode link network (Miller, Todd, & Hegde,
1989; Oliker, Furst, & Maimon, 1992; Schiffmann, Joost, & Werner, 1992). methods
relatively straightforward implement, good fine tuning small networks
(Miller et al., 1989); however, scale well since require large matrices
represent links large networks (Yao, 1993). techniques (Dodd, 1990;
Harp, Samad, & Guha, 1989; Kitano, 1990a) encode important features
network, number hidden layers, number hidden nodes
layer, etc. indirect encoding schemes evolve different sets parameters along
network's topology shown good scalability (Yao, 1993).
techniques (Koza & Rice, 1991; Oliker et al., 1992) evolve architecture
connection weights time; however, combination two levels evolution
greatly increases search space.
Regent mainly differs genetic-algorithm-based training methods designed knowledge-based neural networks. Thus Regent uses domain-specific knowledge
symbolic rule-refinement techniques aid determining network's topology
initial weight setting. Regent differs explicitly encode networks;
rather, spirit Lamarkian evolution, passes trained network weights off200

fiConnectionist Theory Refinement

spring. final difference algorithms restructure network
based solely training-set error, Regent minimizes validation-set error.

7.4 Wrapping Optimization Around Learning

end related work discussion brief overview methods combine global
local optimization strategies. Local search algorithms iteratively improve estimate
minimum searching local neighborhood current solution; local minima
guaranteed global minima. (Many inductive learning methods often
equated local optimization techniques; Rumelhart et al., 1986.) Global optimization
methods (such GAs), hand, perform sophisticated search across
multiple local minima good finding regions search space nearoptimal solutions found; however, usually good refining solution
(once close near-optimal solution) local optimization strategies (Hart, 1994).
Recent research shown desirable emply global local search
strategy (Hart, 1994).
Hybrid GAs (such Regent) combine local search traditional GA.
focus hybrid-GA algorithms section, two-tiered search strategy
employed researchers well (Kohavi & John, 1997; Provost & Buchanan, 1995;
Schaffer, 1993). GAs combined many local search methods (Bala, Huang,
Vafaie, DeJong, & Wechsler, 1995; Belew, 1990; Hinton & Nowlan, 1987; Turney, 1995).
Neural networks common choice local search strategy hybrid GA
systems discussed GA/neural-network hybrids Section 7.3. two
common forms hybrid GAs: Lamarckian-based evolution Darwinian-based evolution (the Baldwin effect). Lamarckian evolution encodes local improvements directly
genetic material, Darwinian evolution leaves genetic material unchanged
learning. discussed Section 6, authors use Lamarckian local search techniques many shown numerous cases Lamarckian evolution outperforms
non-Lamarckian local search (Belew, McInerney, & Schraudolph, 1992; Hart, 1994; Judson,
Colvin, Meza, Huffa, & Gutierrez, 1992).

8. Conclusion

ideal inductive-learning algorithm able exploit available resources
extensive computing power domain-specific knowledge improve ability generalize. Kbann (Towell & Shavlik, 1994) shown effective translating
domain theory neural network; however, Kbann suffers alter
topology. TopGen (Opitz & Shavlik, 1995) improved Kbann algorithm using
available computer power search effective places add nodes Kbann network;
however, show empirically TopGen suffers restricting search expansions
Kbann network, unable improve performance searching beyond
topologies. Therefore TopGen unable exploit available computing power
increase correctness induced concept.
present new algorithm, Regent, uses specialized genetic algorithm
broaden types topologies considered TopGen's search. Experiments indicate
Regent able significantly increase generalization TopGen; hence, new
201

fiOpitz & Shavlik

algorithm successful overcoming TopGen's limitation searching small portion
space possible network topologies. so, Regent able generate
good solution quickly, using Kbann, able continually improve solution
searches concept space. Therefore, Regent takes step toward true anytime theory
refinement system able make effective use problem-specific knowledge
available computing cycles.

Acknowledgements
work supported Oce Naval Research grant N00014-93-1-0998 National
Science Foundation grant IRI 95-02990. Thanks Richard Maclin, Richard Sutton,
three anonymous reviewers helpful comments. extended version
paper published Machine Learning: Proceedings Eleventh International Conference,
pp. 208-216, New Brunswick, NJ, Morgan Kaufmann. David Opitz completed portion
work graduate student University Wisconsin professor
University Minnesota, Duluth.

References
Ackley, D. (1987). Connectionist Machine Genetic Hillclimbing. Kluwer, Norwell,
MA.
Ackley, D., & Littman, M. (1992). Interactions learning evolution. Langton,
C., Taylor, C., Farmer, C., & Rasmussen, S. (Eds.), Artificial Life II, pp. 487{509,
Redwood City, CA. Addison-Wesley.
Ackley, D., & Littman, M. (1994). case Lamarckian evolution. Langton, C. (Ed.),
Artificial Life III, pp. 3{10, Redwood City, CA. Addison-Wesley.
Baffes, P., & Mooney, R. (1993). Symbolic revision theories M-of-N rules.
Proceedings Thirteenth International Joint Conference Artificial Intelligence,
pp. 1135{1140, Chambery, France. Morgan Kaufmann.
Bala, J., Huang, J., Vafaie, H., DeJong, K., & Wechsler, H. (1995). Hybrid learning using
genetic algorithms decision trees pattern classification. Proceedings
Fourteenth International Joint Conference Artificial Intelligence, pp. 719{724,
Montreal, Canada. Morgan Kaufmann.
Baldwin, J. (1896). Physical social heredity. American Naturalist, 30, 441{451.
Baum, E., & Haussler, D. (1989). size net gives valid generalization? Neural Computation, 1, 151{160.
Baum, E., & Lang, K. (1991). Constructing hidden units using examples queries.
Lippmann, R., Moody, J., & Touretzky, D. (Eds.), Advances Neural Information
Processing Systems, Vol. 3, pp. 904{910, San Mateo, CA. Morgan Kaufmann.
202

fiConnectionist Theory Refinement

Belew, R. (1990). Evolution, learning culture: Computational metaphors adaptive
search. Complex Systems, 4, 11{49.
Belew, R., McInerney, J., & Schraudolph, N. (1992). Evolving networks: Using genetic
algorithm connectionist learning. Langton, C., Taylor, C., Farmer, C., &
Rasmussen, S. (Eds.), Artificial Life II, pp. 511{547, Redwood City, CA. AddisonWesley.
Belew, R., & Mitchell, M. (1996). Adaptive Individuals Evolving Populations: Models
Algorithms. Addison-Wesley, Massachusetts.
Berenji, H. (1991). Refinement approximate reasoning-based controllers reinforcement
learning. Proceedings Eighth International Machine Learning Workshop, pp.
475{479, Evanston, IL. Morgan Kaufmann.
Bergadano, F., Giordana, A., & Ponsero, S. (1989). Deduction top-down inductive
learning. Proceedings Sixth International Workshop Machine Learning,
pp. 23{25, Ithaca, NY. Morgan Kaufmann.
Blanziere, E., & Katenkamp, P. (1996). Learning radial basis function networks on-line.
Proceedings Thirteenth International Conference Machine Learning, pp.
37{45, Bari, Italy. Morgan Kaufmann.
Clocksin, W., & Mellish, C. (1987). Programming Prolog. Springer-Verlag, New York.
Cohen, W. (1992). Compiling prior knowledge explicit bias. Proceedings
Ninth International Conference Machine Learning, pp. 102{110, Aberdeen,
Scotland. Morgan Kaufmann.
Danyluk, A. (1989). Finding new rules incomplete theories: Explicit biases induction
contextual information. Proceedings Sixth International Workshop
Machine Learning, pp. 34{36, Ithaca, NY. Morgan Kaufmann.
Das, A., Giles, C., & Sun, G. (1992). Using prior knowledge NNPDA learn
context-free languages. Hanson, S., Cowan, J., & Giles, C. (Eds.), Advances
Neural Information Processing Systems, Vol. 5, pp. 65{72, San Mateo, CA. Morgan
Kaufmann.
Dean, T., & Boddy, M. (1988). analysis time-dependent planning. Proceedings
Seventh National Conference Artificial Intelligence, pp. 49{54, St. Paul, MN.
Morgan Kaufmann.
DeJong, K. (1975). Analysis Behavior class Genetic Adaptive Systems.
Ph.D. thesis, University Michigan, Ann Arbor, MI.
Dodd, N. (1990). Optimization network structure using genetic techniques. Proceedings
IEEE International Joint Conference Neural Networks, Vol. III, pp. 965{970,
Paris. IEEE Press.
203

fiOpitz & Shavlik

Donoho, S., & Rendell, L. (1995). Rerepresenting restructuring domain theories:
constructive induction approach. Journal Artificial Intelligence Research, 2, 411{
446.
Drucker, H., Cortes, C., Jackel, L., LeCun, Y., & Vapnik, V. (1994). Boosting
machine learning algorithms. Proceedings Eleventh International Conference
Machine Learning, pp. 53{61, New Brunswick, NJ. Morgan Kaufmann.
Fahlman, S., & Lebiere, C. (1989). cascade-correlation learning architecture. Touretzky, D. (Ed.), Advances Neural Information Processing Systems, Vol. 2, pp. 524{
532, San Mateo, CA. Morgan Kaufmann.
Farmer, J., & Belin, A. (1992). Artificial life: coming evolution. Langton, C., Taylor,
C., Farmer, J. D., & Rasmussen, S. (Eds.), Artificial Life II, pp. 815{840, Redwood
City, CA. Addison-Wesley.
Flann, N., & Dietterich, T. (1989). study explanation-based methods inductive
learning. Machine Learning, 4, 187{226.
Fletcher, J., & Obradovic, Z. (1993). Combining prior symbolic knowledge constructive
neural network learning. Connection Science, 5, 365{375.
Forrest, S., & Mitchell, M. (1993). makes problem hard genetic algorithm?
anomalous results explanation. Machine Learning, 13, 285{319.
Frean, M. (1990). upstart algorithm: method constructing training feedforward neural networks. Neural Computation, 2, 198{209.
Fu, L. (1989). Integration neural heuristics knowledge-based inference. Connection
Science, 1, 325{340.
Ginsberg, A. (1990). Theory reduction, theory revision, retranslation. Proceedings
Eighth National Conference Artificial Intelligence, pp. 777{782, Boston, MA.
AAAI/MIT Press.
Giordana, A., & Saitta, L. (1993). REGAL: integrated system relations using genetic
algorithms. Proceedings Second International Workshop Multistrategy
Learning, pp. 234{249, Harpers Ferry, WV.
Giordana, A., Saitta, L., & Zini, F. (1994). Learning disjunctive concepts means genetic algorithms. Proceedings Eleventh International Conference Machine
Learning, pp. 96{104, New Brunswick, NJ. Morgan Kaufmann.
Goldberg, D. (1989). Genetic Algorithms Search, Optimization, Machine Learning.
Addison-Wesley, Reading, MA.
Greene, D., & Smith, S. (1993). Competition-based induction decision models
examples. Machine Learning, 13, 229{258.
204

fiConnectionist Theory Refinement

Grefenstette, J., & Ramsey, C. (1992). approach anytime learning. Proceedings
Ninth International Conference Machine Learning, pp. 189{195, Aberdeen,
Scotland. Morgan Kaufmann.
Hansen, L., & Salamon, P. (1990). Neural network ensembles. IEEE Transactions
Pattern Analysis Machine Intelligence, 12, 993{1001.
Harp, S., Samad, T., & Guha, A. (1989). Designing application-specific neural networks
using genetic algorithm. Touretzky, D. (Ed.), Advances Neural Information
Processing Systems, Vol. 2, pp. 447{454, San Mateo, CA. Morgan Kaufmann.
Hart, W. (1994). Adaptive Global Optimization Local Search. Ph.D. thesis, University
California, San Diego.
Hassibi, B., & Stork, D. (1992). Second order derivatives network pruning: Optimal brain
surgeon. Hanson, S., Cowan, J., & Giles, C. (Eds.), Advances Neural Information
Processing Systems, Vol. 5, pp. 164{171, San Mateo, CA. Morgan Kaufmann.
Hekanaho, J. (1996). Background knowledge GA-based concept learning. Proceedings
Thirteenth International Conference Machine Learning, pp. 234{242, Bari,
Italy. Morgan Kaufmann.
Hinton, G., & Nowlan, S. (1987). learning guide evolution. Complex Systems, 1,
495{502.
Holder, L. (1991). Maintaining Utility Learned Knowledge Using Model-Based Control. Ph.D. thesis, Computer Science Department, University Illinois UrbanaChampaign.
Holland, J. (1975). Adaptation Natural Artificial Systems. University Michigan
Press, Ann Arbor, MI.
Janikow, C. (1993). knowledge intensive GA supervised learning. Machine Learning,
13, 198{228.
Judson, R., Colvin, M., Meza, J., Huffa, A., & Gutierrez, D. (1992). intelligent configuration search techniques outperform random search large molecules? International
Journal Quantum Chemistry, 277{290.
Kibler, D., & Langley, P. (1988). Machine learning experimental science. Proceedings Third European Working Session Learning, pp. 1{12, Edinburgh,
UK.
Kitano, H. (1990a). Designing neural networks using genetic algorithms graph generation system. Complex Systems, 4, 461{476.
Kitano, H. (1990b). Empirical studies speed convergence neural network training using genetic algorithms. Proceedings Eighth National Conference
Artificial Intelligence, pp. 789{795, Boston, MA. AAAI/MIT Press.
205

fiOpitz & Shavlik

Kohavi, R., & John, G. (1997). Wrappers feature subset selection. Artificial Intelligence.
Koppel, M., Feldman, R., & Segre, A. (1994). Bias-driven revision logical domain theories.
Journal Artificial Intelligence Research, 1, 159{208.
Koza, J. (1992). Genetic Programming. MIT Press, Cambridge, MA.
Koza, J., & Rice, J. (1991). Genetic generation weights architectures
neural network. International Joint Conference Neural Networks, Vol. 2, pp.
397{404, Seattle, WA. IEEE Press.
Krogh, A., & Vedelsby, J. (1995). Neural network ensembles, cross validation, active
learning. Tesauro, G., Touretzky, D., & Leen, T. (Eds.), Advances Neural
Information Processing Systems, Vol. 7, pp. 231{238, Cambridge, MA. MIT Press.
Lacher, R., Hruska, S., & Kuncicky, D. (1992). Back-propagation learning expert networks. IEEE Transactions Neural Networks, 3, 62{72.
Le Cun, Y., Denker, J., & Solla, S. (1989). Optimal brain damage. Touretzky, D. (Ed.),
Advances Neural Information Processing Systems, Vol. 2, pp. 598{605, San Mateo,
CA. Morgan Kaufmann.
Litzkow, M., Livny, M., & Mutka, M. (1988). Condor | hunter idle workstations.
Proceedings Eighth International Conference Distributed Computing Systems,
pp. 104{111, San Jose, CA. Computer Society Press.
MacKay, D. (1992). practical Bayesian framework backpropagation networks. Neural
Computation, 4, 448{472.
Maclin, R., & Shavlik, J. (1993). Using knowledge-based neural networks improve algorithms: Refining Chou-Fasman algorithm protein folding. Machine Learning,
11, 195{215.
Mahoney, J. (1996). Combining Symbolic Connectionist Learning Methods Refine
Certainty-Factor Rule-Bases. Ph.D. thesis, University Texas, Austin, TX.
Mahoney, J., & Mooney, R. (1993). Combining connectionist symbolic learning refine
certainty-factor rule-bases. Connection Science, 5, 339{364.
Mahoney, J., & Mooney, R. (1994). Comparing methods refining certainty-factor rulebases. Proceedings Eleventh International Conference Machine Learning,
pp. 173{180, New Brunswick, NJ. Morgan Kaufmann.
Masuoka, R., Watanabe, N., Kawamura, A., Owada, Y., & Asakawa, K. (1990). Neurofuzzy
system | fuzzy inference using structured neural network. Proceedings
International Conference Fuzzy Logic & Neural Networks, pp. 173{177, Iizuka,
Japan.
Michalski, R. (1983). theory methodology inductive learning. Artificial Intelligence,
20, 111{161.
206

fiConnectionist Theory Refinement

Miller, G., Todd, P., & Hegde, S. (1989). Designing neural networks using genetic algorithms. Proceedings Third International Conference Genetic Algorithms,
pp. 379{384, Arlington, VA. Morgan Kaufmann.
Mitchell, M. (1996). Introduction Genetic Algorithms. MIT Press, Cambridge, MA.
Mitchell, T. (1982). Generalization search. Artificial Intelligence, 18, 203{226.
Montana, D., & Davis, L. (1989). Training feedforward networks using genetic algorithms.
Proceedings Eleventh International Joint Conference Artificial Intelligence,
pp. 762{767, Detroit, MI. Morgan Kaufmann.
Moody, J. (1991). effective number parameters: analysis generalization
regularization nonlinear learning systems. Moody, J., Hanson, S., & Lippmann,
R. (Eds.), Advances Neural Information Processing Systems, Vol. 4, pp. 847{854,
San Mateo, CA. Morgan Kaufmann.
Mozer, M. C., & Smolensky, P. (1989). Using relevance reduce network size automatically.
Connection Science, 1, 3{16.
Neri, F., & Saitta, L. (1996). Exploring power genetic search learning symbolic
classifiers. IEEE Transactions Pattern Analisys Machine Intelligence.
Oliker, S., Furst, M., & Maimon, O. (1992). distributed genetic algorithm neural
network design training. Complex Systems, 6, 459{477.
Omlin, C., & Giles, C. (1992). Training second-order recurrent neural networks using hints.
Proceedings Ninth International Conference Machine Learning, pp. 361{
366, Aberdeen, Scotland. Morgan Kaufmann.
Opitz, D., & Shavlik, J. (1993). Heuristically expanding knowledge-based neural networks.
Proceedings Thirteenth International Joint Conference Artificial Intelligence, pp. 1360{1365, Chambery, France. Morgan Kaufmann.
Opitz, D., & Shavlik, J. (1995). Dynamically adding symbolically meaningful nodes
knowledge-based neural networks. Knowledge-Based Systems, 8, 301{311.
Opitz, D., & Shavlik, J. (1996). Actively searching effective neural-network ensemble.
Connection Science, 8, 337{353.
Ourston, D., & Mooney, R. (1994). Theory refinement combining analytical empirical
methods. Artificial Intelligence, 66, 273{309.
Pazzani, M., & Kibler, D. (1992). utility knowledge inductive learning. Machine
Learning, 9, 57{94.
Perrone, M. (1993). Improving Regression Estimation: Averaging Methods Variance
Reduction Extension General Convex Measure Optimization. Ph.D. thesis,
Brown University, Providence, RI.
207

fiOpitz & Shavlik

Provost, F., & Buchanan, B. (1995). Inductive policy: pragmatics bias selection.
Machine Learning, 20, 35{61.
Quinlan, J. (1986). Induction decision trees. Machine Learning, 1, 81{106.
Quinlan, J., & Cameron-Jones, R. (1995). Lookahead pathology decision tree induction. Proceedings Fourteenth International Joint Conference Artificial
Intelligence, pp. 1019{1024, Montreal, Canada. Morgan Kaufmann.
Richards, B., & Mooney, R. (1995). Automated refinement first-order Horn-clause domain
theories. Machine Learning, 19, 95{131.
Roscheisen, M., Hofmann, R., & Tresp, V. (1991). Neural control rolling mills: Incorporating domain theories overcome data deficiency. Moody, J., Hanson, S., &
Lippmann, R. (Eds.), Advances Neural Information Processing Systems, Vol. 4, pp.
659{666, San Mateo, CA. Morgan Kaufmann.
Rumelhart, D., Hinton, G., & Williams, R. (1986). Learning internal representations
error propagation. Rumelhart, D., & McClelland, J. (Eds.), Parallel Distributed
Processing: Explorations microstructure cognition. Volume 1: Foundations,
pp. 318{363. MIT Press, Cambridge, MA.
Schaffer, C. (1993). Selecting classification method cross-validation. Machine Learning,
13, 135{143.
Schiffmann, W., Joost, M., & Werner, R. (1992). Synthesis performance analysis
multilayer neural network architectures. Tech. rep. 16, University Koblenz, Institute
Physics.
Scott, G., Shavlik, J., & Ray, W. (1992). Refining PID controllers using neural networks.
Neural Computation, 5, 746{757.
Tishby, N., Levin, E., & Solla, S. (1989). Consistent inference probabilities layered
networks, predictions generalization. International Joint Conference Neural
Networks, pp. 403{410, Washington, D.C. IEEE Press.
Towell, G. (1991). Symbolic Knowledge Neural Networks: Insertion, Refinement,
Extraction. Ph.D. thesis, Computer Sciences Department, University Wisconsin,
Madison, WI.
Towell, G., & Shavlik, J. (1992). Using symbolic learning improve knowledge-based neural
networks. Proceedings Tenth National Conference Artificial Intelligence,
pp. 177{182, San Jose, CA. AAAI/MIT Press.
Towell, G., & Shavlik, J. (1994). Knowledge-based artificial neural networks. Artificial
Intelligence, 70, 119{165.
Turney, P. (1995). Cost-sensitive classification: Empirical evaluation hybrid genetic
decision tree induction algorithm. Journal Artificial Intelligence Research, 2, 369{
409.
208

fiConnectionist Theory Refinement

Waterman, D. (1986). Guide Expert Systems. Addison Wesley, Reading, MA.
Watrous, R., Towell, G., & Glassman, M. (1993). Synthesize, optimize, analyze, repeat
(SOAR): Application neural network tools ECG patient monitoring. Proceedings Symposium Nonlinear Theory Applications, pp. 565{570,
Honolulu, Hawaii.
Watson, J. D., Hopkins, N. H., Roberts, J. W., Argetsinger-Steitz, J., & Weiner, A. M.
(1987). Molecular Biology Gene (Fourth edition). Benjamin/Cummings, Menlo
Park, CA.
Weigend, A. (1993). overfitting effective number hidden units. Proceedings 1993 Connectionist Models Summer School, pp. 335{342, Boulder, CO.
Lawrence Erlbaum Associates.
Weigend, A., Huberman, B., & Rumelhart, D. (1990). Predicting future: connectionist
approach. International Journal Neural Systems, I, 193{209.
Whitley, D., Gordon, S., & Mathias, K. (1994). Lamarckian evolution, Baldwin effect
function optimization. Davidor, Y., Schwefel, H., & Manner, R. (Eds.), Parallel
Problem Solving Nature - PPSN III, pp. 6{15. Springer-Verlag.
Whitley, D., & Hanson, T. (1989). Optimizing neural networks using faster, accurate genetic search. Proceedings Third International Conference Genetic
Algorithms, pp. 391{396, Arlington, VA. Morgan Kaufmann.
Yao, X. (1993). Evolutionary artificial neural networks. International Journal Neural
Systems, 4, 203{221.

209


