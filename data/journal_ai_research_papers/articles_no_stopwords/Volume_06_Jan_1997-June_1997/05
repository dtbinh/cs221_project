journal artificial intelligence

submitted published

connectionist theory refinement
genetically searching space network topologies
david w opitz

opitz cs umt edu

jude w shavlik

shavlik cs wisc edu

department computer science
university montana
missoula mt usa

computer sciences department
university wisconsin
w dayton st
madison wi usa

abstract

learns set examples ideally able exploit
available resources abundant computing power b domain specific knowledge
improve ability generalize connectionist theory refinement systems use background knowledge select neural network topology initial weights proven
effective exploiting domain specific knowledge however exploit available computing power weakness occurs lack ability refine
topology neural networks produce thereby limiting generalization especially
given impoverished domain theories present regent uses
domain specific knowledge help create initial population knowledge neural networks b genetic operators crossover mutation specifically designed
knowledge networks continually search better network topologies experiments three real world domains indicate able significantly
increase generalization compared standard connectionist theory refinement system
well previous growing knowledge networks

introduction
many scientific industrial better understood learning samples
task reason machine learning statistics communities devote considerable effort inductive learning often however learning
fail capitalize number potentially available resources domainspecific knowledge computing power improve ability generalize
domain specific knowledge desirable inductive learners start approximately correct theory achieve improved generalization accuracy examples
seen training significantly fewer training examples ginsberg ourston
mooney pazzani kibler towell shavlik making effective use
available computing power desirable many applications important
obtain concepts generalize well induce concepts quickly article present called regent refining genetic evolution network
topologies utilizes available computer time extensively search neural network

c ai access foundation morgan kaufmann publishers rights reserved

fiopitz shavlik

topology best explains training data minimizing changes domain specific
theory
inductive learning systems utilize set approximately correct domain specific
inference rules called domain theory describe currently known
domain called theory refinement systems making use knowledge
shown important since rules may contain insight easily obtainable
current set training examples ourston mooney pazzani kibler
towell shavlik domains expert created theory willing
wait weeks even months learning system produce improved theory
thus given rapid growth computing power believe important learning
techniques able trade expense large numbers computing cycles gains
predictive accuracy analogous anytime techniques dean boddy
believe machine learning researchers create better anytime learning
learning produce good concept quickly continue search
concept space reporting best concept whenever one found
concentrate connectionist theory refinement systems since shown
frequently generalize better many inductive learning theory refinement
systems fu lacher hruska kuncicky towell kbann towell
shavlik example connectionist system translates provided
domain theory neural network thereby determining network topology
refines reformulated rules backpropagation rumelhart hinton williams
however kbann connectionist theory refinement systems
alter network topologies suffer given impoverished domain theories ones
missing rules needed adequately learn true concept opitz shavlik
towell shavlik topgen opitz shavlik improvement
systems heuristically searches space possible network topologies adding
hidden nodes neural representation domain theory topgen showed statistically
significant improvements kbann several real world domains opitz shavlik
however empirically topgen nevertheless suffers
considers simple expansions kbann network
address limitation broaden types topologies topgen considers
genetic gas choose gas two reasons first gas
shown effective optimization techniques ecient use global
information goldberg holland mitchell second gas inherent
quality makes suitable anytime learning line application mode
dejong gas simulate many alternatives output best alternative seen
far
regent proceeds first trying generate domain
theory diversified initial population produces candidate networks via
genetic operators crossover mutation networks trained
backpropagation regent crossover operator tries maintain rule structure
network mutation operator adds nodes network topgen hence genetic operators specialized connectionist theory refinement
experiments reported herein regent better able search network topologies topgen


ficonnectionist theory refinement

rest organized follows next section brie argue
importance effectively exploiting data theory available computer time
learning process review kbann topgen present
details regent section followed empirical
three human genome project domains section discuss well
future work review related work concluding

data prior knowledge available cpu cycles
system learns set labeled examples called inductive learner alternately supervised empirical similarity learner output example
provided teacher set labeled examples given learner called
training set task inductive learning generate training set concept
description correctly predicts output future examples
training set many inductive learning previously studied e g
michalski quinlan rumelhart et al differ
concept representation language method bias constructing
concept within language differences important since determine
concepts classifier induce
alternative inductive learning paradigm build concept description
set examples querying experts field directly assembling set
rules describe concept e build expert system waterman
building expert systems theories derived interviewing experts tend
approximately correct thus expert provided domain theory usually
good first approximation concept learned inaccuracies frequently exposed
empirical testing
theory refinement systems ginsberg ourston mooney pazzani kibler
towell shavlik systems revise theory basis collection
examples systems try improve theory making minimal repairs theory
make consistent training data changes initial domain theory
kept minimum theory presumably contains useful information even
completely correct hybrid learning systems designed learn
theory data empirical tests shown achieve high generalization
significantly fewer examples purely inductive learning techniques ourston mooney
pazzani kibler towell shavlik thus ideal inductive learning
system able incorporate background knowledge available
form domain theory improve ability generalize
indicated earlier available computer time important resource since computing power rapidly increasing b expert willing wait
lengthy period improved concept reasons one develop anytime
learning continually improve quality answer time dean
boddy defined criteria anytime
suspended resumed minimal overhead b stopped
time return answer c must return answers improve


fiopitz shavlik

x
x

x

x

output

x x

x

x

x

input

figure classical regression example smooth function solid curve
fit noisy data points x probably better predictor
high degree polynomial dashed curve

time criteria created scheduling
apply inductive learning well
standard inductive learners backpropagation rumelhart et al
id quinlan unable continually improve answers least
receive additional training examples fact run long tend
overfit training set holder overfitting occurs learning
produces concept captures much information training examples
enough general characteristics domain whole concepts
great job classifying training instances poor job generalizing
examples ultimate measure success help illustrate point consider
typical regression case shown figure fitting noisy data high degree
polynomial likely lead poor generalization
general framework use encouraging improve answer
time quite simple spend computer time considering many different possible
concept descriptions scoring possibility keeping description scores
best framework anytime respect scoring function scoring
function approximate measure generalization obviously still prone
overfitting thus guarantee generalization monotonically
decrease time nevertheless assuming accurate scoring function long
considering wide range good possibilities quality best concept likely
improve longer period time
use term anytime learning differs grefenstette ramsey use
mean continuous learning changing environment



ficonnectionist theory refinement

review kbann topgen
goal exploit prior knowledge available computing cycles
search neural network likely generalize best proceed
choosing initial guess network defined kbann
continually refine topology best network concept presenting
regent give overview kbann well
initial refining kbann created network topology topgen

kbann
kbann towell shavlik works translating domain theory consisting set

propositional rules directly neural network see figure figure shows prologlike rule set defines membership category figure b represents hierarchical
structure rules solid lines representing necessary dependencies dotted lines
representing prohibitory dependencies figure c represents network kbann creates
translation sets biases nodes representing disjuncts output
near least one high weighted antecedents satisfied nodes
representing conjuncts must high weighted antecedents satisfied e near
positive links near negative links otherwise activations near kbann
creates nodes b b figure c handle two rules disjunctively defining b
thin lines figure c represent low weighted links kbann adds allow rules
add antecedents backpropagation training following network initialization
kbann uses available training instances refine network links refer towell
towell shavlik details
kbann successfully applied several real world control
chemical plant scott shavlik ray protein folding maclin shavlik


b c
b f g

b



b

b f
c h j k



b

c

e f g h j k
b

e

b

f

g h
c

c

j k

figure kbann translation knowledge base neural network panel shows
sample propositional rule set prolog clocksin mellish notation
panel b illustrates rule set corresponding dependency tree
panel c shows resulting network created kbann translation


fiopitz shavlik

finding genes sequence dna opitz shavlik towell shavlik
ecg patient monitoring watrous towell glassman case kbann
shown produce improvements generalization standard neural networks small
numbers training examples fact towell favorably compared kbann
wide variety including purely symbolic theory refinement systems
version promoter splice junction tasks include testbeds section
training kbann created network alters antecedents existing rules
capability inducing rules add additional
hidden nodes training instance kbann unable add third rule
inferring b figure example help illustrate point consider following
example assume figure target concept consists figure domain theory plus
rule
b e g
although trained kbann network shown figure c possible examples
target concept unable completely learn conditions true
topology kbann network must modified order learn rule
studies opitz shavlik towell kbann effective
removing extraneous rules antecedents expert provided domain theory generalization ability suffers given impoverished domain theories theories
missing rules antecedents needed adequately learn true concept ideal connectionist theory refinement therefore able dynamically expand
topology network training

topgen

topgen opitz shavlik addresses kbann limitation heuristically searching
space possible expansions knowledge neural network network
whose topology determined direct mapping dependencies domain theory
e g kbann network topgen proceeds first training kbann network
placing search queue cycle topgen takes best network search
queue estimates errors occur network adds nodes response
estimates trains networks places back queue topgen judges
errors occur network training examples increment two counters
node one false negatives one false positives
figure illustrates possible ways topgen add nodes one networks
symbolic rule base uses negation failure one decrease false negatives
dropping antecedents existing rules adding rules rule base kbann
effective removing antecedents existing rules unable add rules
therefore topgen adds nodes intended decreasing false negatives fashion
analogous adding rule rule base existing node node topgen
adds node child see figure fully connects node input
nodes existing node node topgen creates node
parent original node another node topgen fully connects
inputs see figure c topgen moves outgoing links original node
figure c become outgoing links node


ficonnectionist theory refinement

existing node

decrease false negatives

decrease false positives







node






node


b

b

c

node

c


node

b

c



b




c

node




node


b




node



b
b

c

c

c


node



figure possible ways add nodes knowledge neural network arcs indicate nodes decrease false negatives wish broaden applicability node conversely decrease false positives wish
constrain node
symbolic rule base one decrease false positives adding antecedents
existing rules removing rules rule base kbann effectively remove
rules less effective adding antecedents rules unable invent e
constructively induce michalski terms antecedents thus topgen adds
nodes intended decrease false positives fashion analogous adding
constructively induced antecedents network figures b illustrates
done analogous figures c explained refer opitz shavlik
details
topgen showed statistically significant improvements kbann several real world
domains comparative experiments simpler adding nodes verified
nodes must added intelligent manner opitz shavlik
article however increase number networks topgen considers search
increase generalization primarily limited first networks
considered therefore topgen much anytime rather first
step towards one mostly due fact topgen considers larger networks contain original kbann network subgraphs however one increases
number networks considered one increase variety networks considered


fiopitz shavlik

search broadening range networks considered search
topology space major focus

regent

regent tries broaden types networks topgen considers
use gas view regent two phases genetically searching
topology space b training network backpropagation gradient
descent method regent uses domain theory aid phases uses theory
help guide search topology space give good starting point weight
space
table summarizes regent regent first sets aside validation set
part training instances use scoring different networks perturbs kbann produced network create initial set candidate networks next
regent trains networks backpropagation places population cycle regent creates networks crossing mutating networks
current population randomly picked proportional fitness e
validation set correctness trains networks places
population searches regent keeps network lowest validation set
error best concept seen far breaking ties choosing smaller network
application occam razor parallel version regent trains many candidate networks time condor system litzkow livny mutka
runs jobs idle workstations
diverse initial population broaden types networks regent considers
search however since domain theory may provide useful information may
present training set still desirable use theory generating initial
population regent creates diversity around domain theory randomly perturbing
kbann network nodes regent perturbs node deleting
adding nodes manner analogous one topgen four methods adding

goal search best network topology describing domain theory data

set aside validation set training instances
perturb kbann produced network multiple ways create initial networks train
networks backpropagation place population
loop forever
create networks crossover mutation operators
b train networks backpropagation score validation set place
population
c network network lowest validation set error seen far breaking
ties preferring smallest network report current best concept

table regent


ficonnectionist theory refinement

crossover two networks

goal crossover two networks generate two network topologies
divide network hidden nodes sets b dividenodes

set forms one network set b forms another network created follows
network inherits weight w parent nodes j inherited
input output nodes
b link unconnected nodes levels near zero weights
c adjust node biases keep original function node see text explanation
ji

dividenodes
goal

divide hidden nodes sets b probabilistically maintaining
network rule structure
hidden node assigned set b
collect unassigned hidden nodes whose output linked previouslyassigned nodes output nodes
ii set set b empty
node collected part randomly assign set set b

else

probabilistically add nodes collected part set set b equation
shows probability assigned set probability assigned
set b one minus value

table regent method crossing networks
nodes happen multiple theories domain
used seed population

regent crossover operator

regent crosses two networks first dividing nodes parent network

two sets b combining nodes set form two networks e
nodes two sets form one network nodes two b sets form another
table summarizes regent method crossover figure illustrates
example regent divides nodes one level time starting level nearest
output nodes considering level set set b empty cycles
node level randomly assigns set neither set empty nodes
probabilistically placed set following equation calculates probability
although one define level several different ways define node level longest path
output node



fiopitz shavlik

original
networks
crossed


output

output

input

input

output

output

input

input

resulting
networks
figure regent method crossing two networks hidden nodes
original network divided sets b nodes two sets
form one network nodes two b sets form another grey lines
represent low weighted links added fully connect neighboring levels
given node assigned set

pj jwjij
prob node seta p jw j p jw j
j ji
j b ji



j means node j member set wji weight value node
node j probability belonging set b one minus probability
probabilities regent tends assign set nodes heavily linked
together helps minimize destruction rule structure crossed
networks since nodes belonging syntactic rule connected heavily linked
weights thus regent crossover operator produces networks crossing rules
rather simply crossing nodes
regent must next decide connect nodes newly created networks
first network inherits weight values parents links connect
two nodes inherited network b connect inherited hidden
node input output node c directly connect input node output node
adds randomly set low weighted links unconnected nodes consecutive
levels
finally adjusts bias nodes help maintain original function
instance regent removes positively weighted incoming link node
decrements node bias subtracting product link magnitude


ficonnectionist theory refinement

average activation set training examples entering link
bias node needs slightly less sum positive weights
incoming links see towell shavlik details regent increments
bias node analogous amount removes negatively weighted incoming
links since bias node slightly greater sum negative
weights incoming links node inactive incoming negatively
weighted linked nodes active positively weighted linked nodes inactive

regent mutation operator

regent mutates networks applying variant topgen regent uses topgen
method incrementing false negatives false positives counters node regent adds nodes values counters way topgen

since neural learning effective removing unwanted antecedents rules knns
see section regent considers adding nodes deleting mutation thus mutation operator adds diversity population still maintaining
directed heuristic search technique choosing add nodes directedness
necessary currently unable evaluate thousand possible
networks per day

additional details
regent adds newly trained networks population validation set correctness better equal existing member population regent

replaces member replaces member lowest correctness ties broken
choosing oldest member techniques goldberg replacing
member nearest candidate network promote diverse populations however
want promote diversity expense decreased generalization future
topic plan investigate incorporating diversity promoting techniques
able consider tens thousands networks
regent considered lamarckian genetic hillclimbing ackley
since performs local optimizations individuals passes successful optimizations
offspring ability individuals learn smooth fitness landscape
facilitate subsequent learning thus lamarckian learning lead large increase
learning speed solution quality ackley littman farmer belin

experimental
section test regent three real world human genome project
aid locating genes dna sequences recognizing promoters splice junctions
ribosome binding sites domains input short segment dna nucleotides
elements long task learn predict dna subsequence contains
biologically important site domain accompanied domain theory generated
dna expert noordewier
lamarckian evolution theory inheritance characteristics acquired lifetime



fiopitz shavlik

promoter domain contains positive examples negative examples
rules splice junction domain contains examples distributed equally among three
classes rules finally ribosome binding sites rbs domain contains
positive examples negative examples rules note promoter data set
domain theory later version one appears towell domains
available university wisconsin machine learning uw ml site via world
wide web ftp ftp cs wisc edu machine learning shavlik group datasets
anonymous ftp ftp cs wisc edu machine learning shavlik group datasets
first directly compare regent topgen kbann perform
lesion study regent particular investigate value adding randomly
created networks regent initial population examine utility regent
genetic operators

experimental methodology

article ten fold cross validation runs ten fold cross
validation data set first partitioned ten equal sized sets set turn
used test set classifier trains nine sets fold regent
run population size network trained backpropagation parameter
settings neural networks include learning rate momentum term
number training epochs first two standard settings
epochs may fewer typically found neural network literature set
help avoid overfitting set aside validation set consisting training
examples regent use scoring function

generalization ability regent

section experiments compare test set accuracy e generalization regent
topgen figure shows test set error kbann topgen regent
search space network topologies horizontal line graph
kbann drew horizontal line sake visual comparison
recall kbann considers single network first point graph
one network considered nearly three since start
kbann network however topgen regent differ slightly kbann since
must set aside part training set score candidate networks notice
topgen stops improving considering networks generalization
ability regent better topgen point reason occasional
upward movements figure due fact validation set scoring
function inexact estimate true generalization error
ten fold cross validation
figure presents test set error topgen regent consider
candidate topologies standard neural network fully connected
single layer feed forward neural network fold trained networks containing
hidden nodes used validation set choose best network
lesion study one components individually disabled ascertain
contribution full performance kibler langley



ficonnectionist theory refinement




ribosome binding sites


kbann
topgen



regent

testset error







splice junctions






promoters












networks considered
figure error rates three human genome





fiopitz shavlik

kbann generalizes much better best standard networks thus
confirming kbann effectiveness generating good network topologies topgen
able improve kbann network regent able significantly decrease error
rate kbann topgen benchmark purposes regent error rate
ten fold cross validation full splice junction dataset examples
commonly used machine learning researchers
table contains number hidden nodes final networks produced kbann
topgen regent demonstrate regent produces networks
larger kbann topgen networks even though topgen adds nodes
search regent networks larger necessarily mean
complex inspected sample networks found large
portions network used e g weights insignificantly small
functional duplications groups hidden nodes
one could prune weights nodes regent search however pruning
prematurely reduce variety structures available recombination crossover
koza real life organisms instance super uous dna believed
enhance rate evolution watson hopkins roberts argetsinger steitz weiner
however pruning network size genetic search may unwise one
could prune regent final network say hassibi stork optimal brain
surgeon post pruning process may increase future classification speed
network well increase comprehensibility possibly accuracy

lesion study regent
section describe lesion study performed regent since single run
regent takes four cpu days consider networks single ten fold cross







testset error











key



standard nn







kbann
topgen



regent


rbs

splice junctions

promoters

figure test set error rates topgen regent consider networks pairwise one tailed tests indicate regent differs standard nn kbann
topgen confidence level three


ficonnectionist theory refinement

domain
kbann topgen
regent
rbs


splice junction


promoters


table number hidden nodes networks produced kbann topgen
regent columns mean number hidden nodes found within
networks standard deviations contained within parentheses
report standard deviations kbann since uses one network

validation takes minimum cpu days therefore given inherent similarity
investigating aspects regent multiple datasets feasible
run experiments section confidence level reached cases
assuming level actually exists nonetheless convey important
information components regent shown previous section
complete regent generate statistically significant improvements
existing
including non knns regent population

correct theory may quite different initial domain theory thus
section investigate whether one include initial population networks
variety networks obtained directly domain theory currently regent
creates initial population perturbing kbann network include networks
obtained domain theory first randomly pick number hidden
nodes include network randomly create hidden nodes network
adding nodes randomly selected output hidden node one
topgen four methods adding nodes refer figure adding nodes
manner creates random networks whose node structure analogous dependencies found
symbolic rule bases thus creating networks suitable regent crossover mutation
operators
table shows test set error regent percentages knowledge
neural networks knns present initial population first row contains
initializing regent purely random initial population e population contains
knns second row lists regent creates half population
domain theory half randomly finally last row contains
seeding entire population domain theory
suggest including initial population networks
created domain theory increases regent test set error three domains
occurs randomly generated networks correct knns


fiopitz shavlik

knn
knn
knn

rbs




splice junction




promoters




table test set error considering networks row gives pergentage
knns present initial population pairwise one tailed tests indicate
initializing regent knns differs knns confidence
level three domains however difference runs
knns significant level

thus offspring original knn quickly replace random networks hence diversity population suffers compared methods start whole population
knns assuming domain theory malicious therefore better seed
entire population kbann network domain theory indeed malicious
contain information promotes spurious correlations data would
reasonable randomly create whole population running regent
without domain theory allows one investigate utility theory
interesting ga point view forrest mitchell
showed gas perform poorly complex basic building blocks
non trivial b get split crossover seeding initial population
domain theory regent help define basic building blocks

value regent mutation

typically gas mutation secondary operation sparingly used goldberg however regent mutation directed heuristically adds
nodes knns provenly effective manner e uses topgen therefore reasonable hypothesize one apply mutation operator frequently
traditionally done gas section test hypothesis
figure presents test set error regent varying percentages mutation
versus crossover creating networks step table graph plots four
curves mutation e regent uses crossover b mutation c
mutation mutation performing mutations tests value solely
crossover mutation tests ecacy mutation operator note
mutation topgen different search strategy instead keeping
open list heuristic search population knns generated members
population improved proportional fitness two curves
mutation test synergy two operators performing mutation


ficonnectionist theory refinement





ribosome binding sites

mutation
mutation



mutation
mutation

testset error







splice junctions






promoters














networks considered
figure error rates regent different fractions mutation versus crossover
considering networks arguably due inherent similarity
limited number runs due computational complexity
significant confidence level



fiopitz shavlik

closer traditional ga viewpoint mutation secondary operation
mutation means operations equally valuable previous experiments
section used mutation crossover
differences statistically significant nevertheless suggest
synergy exists two operations except middle portion
promoter domain qualitatively operations
time better operation alone fact equally mixing mutation
crossover operator better three curves three domains regent
considered networks particularly pronounced splice junction
domain
value regent crossover
regent tries cross rules networks rather blindly crossing

nodes probabilistically dividing nodes network two sets
nodes belonging rule tend belong set section
test ecacy regent crossover comparing variant
randomly assigns nodes two sets rather dividenodes table
table contains test networks considered
first row regent random crossover regent randomly breaks hidden nodes
two sets second row regent assigns nodes two sets according table
cases regent creates half networks mutation operator
half crossover operator although differences statistically significant
suggest keeping rule structure networks intact crossover
important otherwise basic building blocks networks e rules get split
crossover studies shown importance keeping intact basic building
blocks crossover forrest mitchell goldberg

regent random crossover
regent

promoters splice junction rbs







table test set error two runs regent randomly crossing nodes
networks b one crossing rules network defined
equation runs considered networks used half crossover half
mutation significant confidence level
slight difference learning long run times limited
runs ten fold cross validation

discussion future work

towell showed kbann generalized better many machine learning promoter splice junction domains rbs dataset exist


ficonnectionist theory refinement

despite success regent able effectively use available computer cycles significantly improve generalization kbann previous improvement kbann
topgen regent reduces kbann test set error rbs domain splice junction domain promoter domain reduces
topgen test set error rbs domain splice junction domain
promoter domain regent ability use available computing time
aided inherently parallel since train many networks simultaneously
regent two genetic operators complement
crossover operator considers large variety network topologies probabilistically combining rules contained within two successful knns mutation hand makes
smaller directed improvements members population time adding
diversity population adding rules population equal use operators therefore allows wide variety topologies considered well allowing
incremental improvements members population
since regent searches many candidate networks important
able recognize networks likely generalize best mind
first planned extension regent develop test different network evaluation functions currently use validation set however validation sets several drawbacks
first keeping aside validation set decreases number training instances available
network second performance validation set noisy approximator
true error mackay weigend huberman rumelhart finally
increase number networks searched regent may start selecting networks
overfit validation set fact explains occasional upward trend test set error
topgen regent figure
avoid overfitting data common regression trick cost
function includes smoothness term along error term best function
smoothest function fits data well neural networks one
add estimated error smoothness component measure complexity
network complexity network cannot simply estimated counting
number possible parameters since tends significant duplication
function weight network especially early training process weigend
two techniques try take account effective size network
generalized prediction error moody bayesian methods mackay
quinlan cameron jones propose adding additional term accuracy
smoothness term takes account length time spent searching coin
term oversearching describe phenomenon extensive searching causes
lower predictive accuracy claim oversearching orthogonal overfitting
thus complexity methods alone cannot prevent oversearching increase
number networks consider search may start oversearching
thus plan investigate adding oversearching penalty term well
indicated earlier regent lamarckian passes local optimizations individuals e trained weights network offspring viable alternative called
baldwin effect ackley littman baldwin belew mitchell hinton
nowlan local search still change fitness individual backpropagation learning case pass changes offspring form


fiopitz shavlik

evolution darwinian nature even though learned explicitly coded
genetic material individuals best able learn offspring
thus learning still impacts evolution fact form evolution sometimes outperform forms lamarckian evolution employ local search strategy whitley
gordon mathias future work investigate utility baldwin effect
regent case would cross trained networks instead cross
initial weight settings backpropagation learning took place
finally often times multiple even con icting theories domain future work investigate ways domain theories seed
initial population although section including randomly generated networks degrades generalization performance seeding population multiple
approximately correct theories degrade generalization assuming networks
initial correctness thus regent able naturally
combine good parts multiple theories given domain theory many
different equivalent ways represent theory set propositional rules
representation leads different network topology even though network
starts theory topologies may conducive neural refinement

related work

regent mainly differs previous work anytime theory refinement sys

tem continually searches non hillclimbing manner improvements domain
theory summary work unique provides connectionist
attempts effectively utilize available background knowledge available computer cycles
generate best concept possible broken rest section four parts
connectionist theory refinement b purely symbolic theory refinement c appropriate domain specific neural network topology
optimization wrapped around induction

connectionist theory refinement techniques

begin discussion connectionist theory refinement systems systems
developed refine many types rule bases instance number systems
proposed revising certainty factor rule bases fu lacher et al
mahoney mooney finite state automata maclin shavlik omlin giles
push automata das giles sun fuzzy logic rules berenji
masuoka watanabe kawamura owada asakawa mathematical equations
roscheisen hofmann tresp scott et al systems work
kbann first translating domain knowledge neural network modifying
weights resulting network attempts describe next
made dynamically adjust resulting network topology training regent

topgen regent fletcher obradovic present
adds nodes kbann network system constructs single layer nodes fully
connected input output nodes side kbann network
generate hidden nodes variant baum lang constructive


ficonnectionist theory refinement

baum lang first divides feature space hyperplanes
hyperplane randomly selecting two points different classes
localizing suitable split points baum lang repeat process
generate fixed number hyperplanes fletcher obradovic map
baum lang hyperplanes one hidden node thus defining weights
input layer hidden node fletcher obradovic change
weights kbann portion network modifications initial rule base
solely left constructed hidden nodes thus system take advantage
kbann strength removing unwanted antecedents rules original rule
base fact topgen compared favorably similar technique added nodes
side kbann opitz shavlik regent outperformed topgen
article experiments
rapture mahoney mooney designed domain theories containing probabilistic rules connectionist theory refinement systems rapture first translates
domain theory neural network refines weights network
modified backpropagation regent rapture able dynamically
refine topology network upstart frean
add nodes network aside designed probabilistic rules
rapture differs regent adds nodes intention completely
learning training set generalizing well thus rapture hillclimbs
training set learned regent continually searches topology space looking network
minimizes scoring function error rapture initially creates links
specified domain theory explicitly adds links id quinlan
information gain metric regent hand fully connect consecutive layers
networks allowing rule possibility adding antecedents training
daid towell shavlik extension kbann uses
domain theory help train kbann network since kbann effective dropping antecedents adding daid tries potentially useful inputs features
mentioned domain theory backing errors lowest level
domain theory computing correlations features daid increases
weight links potentially useful input features correlations
daid mainly differs regent refine topology kbann network thus daid addresses kbann limitation effectively adding antecedents
still unable introduce rules constructively induce antecedents daid
therefore suffer impoverished domain theories notice since daid improvement training knns regent use daid train network considers
search however done
opitz shavlik used variant regent learning
generating neural network ensemble neural network ensemble successful
technique outputs set separately trained neural networks combined
form one unified prediction drucker cortes jackel lecun vapnik hansen
salamon perrone since regent considers many networks select
subset final population networks ensemble minimal extra cost previous
work though shown ideal ensemble one networks accurate
make errors different parts input space hansen salamon


fiopitz shavlik

krogh vedelsby opitz shavlik changed scoring
function regent fit network one accurate
disagreed members population much possible addition
addemup actively tries generate good candidates emphasizing
current population erroneous examples backpropagation training
alterations addemup able create enough diversity among population
networks able effectively exploit knowledge domain theory opitz
shavlik addemup able generate significantly better ensemble
domain theory running addemup without benefit theory
simply combining regent final population networks actively searching highly
diverse population however aid searching single best network fact
single best network produced addemup significantly worse regent single
best network three domains

purely symbolic theory refinement techniques
additional work related regent includes purely symbolic theory refinement systems
modify domain theory directly initial form systems focl pazzani
kibler forte richards mooney first order theory refinement
systems revise predicate logic theories one drawback systems
currently generalize well connectionist approaches many real world
dna promoter task cohen
several genetic first order logic multimodal concept learners
greene smith janikow giordana saitta showed integrate one system regal giordana saitta zini neri saitta
deductive engine ml smart bergadano giordana ponsero help
refine incomplete inconsistent domain theory version works first automated theorem prover recognize unresolved literals proof uses ga
regal induce corrections literals regent hand use genetic
along neural learning refine whole domain theory time
dogma hekanaho recently proposed ga learner use background knowledge learn description language regal current restrictions
however force representation language domain theory propositional rules
dogma converts set background rules e handle intermediate
conclusions individual bitstrings used building blocks higher level
concept dogma focus theory refinement rather builds completely
theory substructures background knowledge term
theory suggested theory guided hekanaho
several systems including proposed refining propositional rule bases
early approaches could handle improvements overly specific theories danyluk
specializations overly general theories flann dietterich later systems
rtls ginsberg ourston mooney ptr koppel feldman
segre tgci donoho rendell later able handle types
refinements discuss system representative propositional
systems


ficonnectionist theory refinement

four theory revision operators removing antecedents rule b
adding antecedents rule c removing rules rule base inventing
rules uses operators make revisions domain theory correctly
classify previously misclassified training examples without undefining
correctly classified examples uses inductive learning invent
rules currently uses id quinlan induction component
even though regent mutation operator add nodes manner analogous
symbolic system adds antecedents rules underlying learning connectionist towell showed kbann outperformed promoter
task regent outperformed kbann article kbann power domain
largely attributed ability make fine grain refinements domain theory
towell diculty domain baffes mooney
presented extension called neither mofn able learn n rules
rules true n antecedents true improvement generated
concept closely matches kbann generalization performance
want minimize changes theory want expense accuracy however donoho rendell demonstrate existing
theory refinement systems suffer able make small
local changes domain theory thus accurate theory significantly far
structure initial theory systems forced become trapped
local maximum similar initial theory forced drop entire rules replace
rules inductively created purely scratch regent
suffer translates theory less restricting representation
neural networks donoho rendell regent able reconfigure
structure domain genetic
many authors reported varying subsets splice junction domain
e g donoho rendell mahoney neri saitta towell shavlik authors used different training set sizes nevertheless worthwhile
qualitatively discuss conclusions towell shavlik compared
kbann numerous machine learning learning
given training set examples kbann generalization ability compared favorably
splice domain regent turn compared favorably
kbann article donoho rendell showed purely symbolic
converged performance kbann around examples mahoney showed
training set sizes examples rapture generalized
better kbann domain look similar regent finally
neri saitta showed generalization ability ga regal compares favorably purely symbolic non ga techniques used slightly
different training set sizes article regent compares well
reported

finding appropriate network topologies
third area related work covers techniques attempt good domaindependent topology dynamically refining network topology training many


fiopitz shavlik

studies shown generalization ability neural network depends topology network baum haussler tishby levin solla trying
appropriate topology one construct modify topology
incremental fashion network shrinking start many parameters
remove nodes weights training hassibi stork le cun denker
solla mozer smolensky network growing hand
start parameters add nodes weights training blanziere
katenkamp fahlman lebiere frean obvious difference regent regent uses domain knowledge
symbolic rule refinement techniques help determine network topology
restructure network solely training set error regent
minimized validation set error
instead incrementally finding appropriate topology one mount richer
search hillclimbing space topologies one common
combine genetic neural networks regent genetic
applied neural networks two different ways optimize connection
weights fixed topology b optimize topology network techniques
solely use genetic optimize weights montana davis whitley
hanson performed competitively gradient training
however one genetic ineciency fine tuned local search
thus scalability methods question yao kitano b presents
method combines genetic backpropagation
genetic determine starting weights network
refined backpropagation regent differs kitano method use domain
theory help determine network starting weights genetically search instead
appropriate network topologies
methods use genetic optimize network topology similar
regent use backpropagation train network weights
methods many directly encode link network miller todd hegde
oliker furst maimon schiffmann joost werner methods
relatively straightforward implement good fine tuning small networks
miller et al however scale well since require large matrices
represent links large networks yao techniques dodd
harp samad guha kitano encode important features
network number hidden layers number hidden nodes
layer etc indirect encoding schemes evolve different sets parameters along
network topology shown good scalability yao
techniques koza rice oliker et al evolve architecture
connection weights time however combination two levels evolution
greatly increases search space
regent mainly differs genetic training methods designed knowledge neural networks thus regent uses domain specific knowledge
symbolic rule refinement techniques aid determining network topology
initial weight setting regent differs explicitly encode networks
rather spirit lamarkian evolution passes trained network weights

ficonnectionist theory refinement

spring final difference restructure network
solely training set error regent minimizes validation set error

wrapping optimization around learning

end related work discussion brief overview methods combine global
local optimization strategies local search iteratively improve estimate
minimum searching local neighborhood current solution local minima
guaranteed global minima many inductive learning methods often
equated local optimization techniques rumelhart et al global optimization
methods gas hand perform sophisticated search across
multiple local minima good finding regions search space nearoptimal solutions found however usually good refining solution
close near optimal solution local optimization strategies hart
recent shown desirable emply global local search
strategy hart
hybrid gas regent combine local search traditional ga
focus hybrid ga section two tiered search strategy
employed researchers well kohavi john provost buchanan
schaffer gas combined many local search methods bala huang
vafaie dejong wechsler belew hinton nowlan turney
neural networks common choice local search strategy hybrid ga
systems discussed ga neural network hybrids section two
common forms hybrid gas lamarckian evolution darwinian evolution baldwin effect lamarckian evolution encodes local improvements directly
genetic material darwinian evolution leaves genetic material unchanged
learning discussed section authors use lamarckian local search techniques many shown numerous cases lamarckian evolution outperforms
non lamarckian local search belew mcinerney schraudolph hart judson
colvin meza huffa gutierrez

conclusion

ideal inductive learning able exploit available resources
extensive computing power domain specific knowledge improve ability generalize kbann towell shavlik shown effective translating
domain theory neural network however kbann suffers alter
topology topgen opitz shavlik improved kbann
available computer power search effective places add nodes kbann network
however empirically topgen suffers restricting search expansions
kbann network unable improve performance searching beyond
topologies therefore topgen unable exploit available computing power
increase correctness induced concept
present regent uses specialized genetic
broaden types topologies considered topgen search experiments indicate
regent able significantly increase generalization topgen hence


fiopitz shavlik

successful overcoming topgen limitation searching small portion
space possible network topologies regent able generate
good solution quickly kbann able continually improve solution
searches concept space therefore regent takes step toward true anytime theory
refinement system able make effective use specific knowledge
available computing cycles

acknowledgements
work supported oce naval grant n national
science foundation grant iri thanks richard maclin richard sutton
three anonymous reviewers helpful comments extended version
published machine learning proceedings eleventh international conference
pp brunswick nj morgan kaufmann david opitz completed portion
work graduate student university wisconsin professor
university minnesota duluth

references
ackley connectionist machine genetic hillclimbing kluwer norwell

ackley littman interactions learning evolution langton
c taylor c farmer c rasmussen eds artificial life ii pp
redwood city ca addison wesley
ackley littman case lamarckian evolution langton c ed
artificial life iii pp redwood city ca addison wesley
baffes p mooney r symbolic revision theories n rules
proceedings thirteenth international joint conference artificial intelligence
pp chambery france morgan kaufmann
bala j huang j vafaie h dejong k wechsler h hybrid learning
genetic decision trees pattern classification proceedings
fourteenth international joint conference artificial intelligence pp
montreal canada morgan kaufmann
baldwin j physical social heredity american naturalist
baum e haussler size net gives valid generalization neural computation
baum e lang k constructing hidden units examples queries
lippmann r moody j touretzky eds advances neural information
processing systems vol pp san mateo ca morgan kaufmann


ficonnectionist theory refinement

belew r evolution learning culture computational metaphors adaptive
search complex systems
belew r mcinerney j schraudolph n evolving networks genetic
connectionist learning langton c taylor c farmer c
rasmussen eds artificial life ii pp redwood city ca addisonwesley
belew r mitchell adaptive individuals evolving populations
addison wesley massachusetts
berenji h refinement approximate reasoning controllers reinforcement
learning proceedings eighth international machine learning workshop pp
evanston il morgan kaufmann
bergadano f giordana ponsero deduction top inductive
learning proceedings sixth international workshop machine learning
pp ithaca ny morgan kaufmann
blanziere e katenkamp p learning radial basis function networks line
proceedings thirteenth international conference machine learning pp
bari italy morgan kaufmann
clocksin w mellish c programming prolog springer verlag york
cohen w compiling prior knowledge explicit bias proceedings
ninth international conference machine learning pp aberdeen
scotland morgan kaufmann
danyluk finding rules incomplete theories explicit biases induction
contextual information proceedings sixth international workshop
machine learning pp ithaca ny morgan kaufmann
das giles c sun g prior knowledge nnpda learn
context free languages hanson cowan j giles c eds advances
neural information processing systems vol pp san mateo ca morgan
kaufmann
dean boddy analysis time dependent proceedings
seventh national conference artificial intelligence pp st paul mn
morgan kaufmann
dejong k analysis behavior class genetic adaptive systems
ph thesis university michigan ann arbor mi
dodd n optimization network structure genetic techniques proceedings
ieee international joint conference neural networks vol iii pp
paris ieee press


fiopitz shavlik

donoho rendell l rerepresenting restructuring domain theories
constructive induction journal artificial intelligence

drucker h cortes c jackel l lecun vapnik v boosting
machine learning proceedings eleventh international conference
machine learning pp brunswick nj morgan kaufmann
fahlman lebiere c cascade correlation learning architecture touretzky ed advances neural information processing systems vol pp
san mateo ca morgan kaufmann
farmer j belin artificial life coming evolution langton c taylor
c farmer j rasmussen eds artificial life ii pp redwood
city ca addison wesley
flann n dietterich study explanation methods inductive
learning machine learning
fletcher j obradovic z combining prior symbolic knowledge constructive
neural network learning connection science
forrest mitchell makes hard genetic
anomalous explanation machine learning
frean upstart method constructing training feedforward neural networks neural computation
fu l integration neural heuristics knowledge inference connection
science
ginsberg theory reduction theory revision retranslation proceedings
eighth national conference artificial intelligence pp boston
aaai mit press
giordana saitta l regal integrated system relations genetic
proceedings second international workshop multistrategy
learning pp harpers ferry wv
giordana saitta l zini f learning disjunctive concepts means genetic proceedings eleventh international conference machine
learning pp brunswick nj morgan kaufmann
goldberg genetic search optimization machine learning
addison wesley reading
greene smith competition induction decision
examples machine learning


ficonnectionist theory refinement

grefenstette j ramsey c anytime learning proceedings
ninth international conference machine learning pp aberdeen
scotland morgan kaufmann
hansen l salamon p neural network ensembles ieee transactions
pattern analysis machine intelligence
harp samad guha designing application specific neural networks
genetic touretzky ed advances neural information
processing systems vol pp san mateo ca morgan kaufmann
hart w adaptive global optimization local search ph thesis university
california san diego
hassibi b stork second order derivatives network pruning optimal brain
surgeon hanson cowan j giles c eds advances neural information
processing systems vol pp san mateo ca morgan kaufmann
hekanaho j background knowledge ga concept learning proceedings
thirteenth international conference machine learning pp bari
italy morgan kaufmann
hinton g nowlan learning guide evolution complex systems

holder l maintaining utility learned knowledge model control ph thesis computer science department university illinois urbanachampaign
holland j adaptation natural artificial systems university michigan
press ann arbor mi
janikow c knowledge intensive ga supervised learning machine learning

judson r colvin meza j huffa gutierrez intelligent configuration search techniques outperform random search large molecules international
journal quantum chemistry
kibler langley p machine learning experimental science proceedings third european working session learning pp edinburgh
uk
kitano h designing neural networks genetic graph generation system complex systems
kitano h b empirical studies speed convergence neural network training genetic proceedings eighth national conference
artificial intelligence pp boston aaai mit press


fiopitz shavlik

kohavi r john g wrappers feature subset selection artificial intelligence
koppel feldman r segre bias driven revision logical domain theories
journal artificial intelligence
koza j genetic programming mit press cambridge
koza j rice j genetic generation weights architectures
neural network international joint conference neural networks vol pp
seattle wa ieee press
krogh vedelsby j neural network ensembles cross validation active
learning tesauro g touretzky leen eds advances neural
information processing systems vol pp cambridge mit press
lacher r hruska kuncicky back propagation learning expert networks ieee transactions neural networks
le cun denker j solla optimal brain damage touretzky ed
advances neural information processing systems vol pp san mateo
ca morgan kaufmann
litzkow livny mutka condor hunter idle workstations
proceedings eighth international conference distributed computing systems
pp san jose ca computer society press
mackay practical bayesian framework backpropagation networks neural
computation
maclin r shavlik j knowledge neural networks improve refining chou fasman protein folding machine learning

mahoney j combining symbolic connectionist learning methods refine
certainty factor rule bases ph thesis university texas austin tx
mahoney j mooney r combining connectionist symbolic learning refine
certainty factor rule bases connection science
mahoney j mooney r comparing methods refining certainty factor rulebases proceedings eleventh international conference machine learning
pp brunswick nj morgan kaufmann
masuoka r watanabe n kawamura owada asakawa k neurofuzzy
system fuzzy inference structured neural network proceedings
international conference fuzzy logic neural networks pp iizuka
japan
michalski r theory methodology inductive learning artificial intelligence



ficonnectionist theory refinement

miller g todd p hegde designing neural networks genetic proceedings third international conference genetic
pp arlington va morgan kaufmann
mitchell introduction genetic mit press cambridge
mitchell generalization search artificial intelligence
montana davis l training feedforward networks genetic
proceedings eleventh international joint conference artificial intelligence
pp detroit mi morgan kaufmann
moody j effective number parameters analysis generalization
regularization nonlinear learning systems moody j hanson lippmann
r eds advances neural information processing systems vol pp
san mateo ca morgan kaufmann
mozer c smolensky p relevance reduce network size automatically
connection science
neri f saitta l exploring power genetic search learning symbolic
classifiers ieee transactions pattern analisys machine intelligence
oliker furst maimon distributed genetic neural
network design training complex systems
omlin c giles c training second order recurrent neural networks hints
proceedings ninth international conference machine learning pp
aberdeen scotland morgan kaufmann
opitz shavlik j heuristically expanding knowledge neural networks
proceedings thirteenth international joint conference artificial intelligence pp chambery france morgan kaufmann
opitz shavlik j dynamically adding symbolically meaningful nodes
knowledge neural networks knowledge systems
opitz shavlik j actively searching effective neural network ensemble
connection science
ourston mooney r theory refinement combining analytical empirical
methods artificial intelligence
pazzani kibler utility knowledge inductive learning machine
learning
perrone improving regression estimation averaging methods variance
reduction extension general convex measure optimization ph thesis
brown university providence ri


fiopitz shavlik

provost f buchanan b inductive policy pragmatics bias selection
machine learning
quinlan j induction decision trees machine learning
quinlan j cameron jones r lookahead pathology decision tree induction proceedings fourteenth international joint conference artificial
intelligence pp montreal canada morgan kaufmann
richards b mooney r automated refinement first order horn clause domain
theories machine learning
roscheisen hofmann r tresp v neural control rolling mills incorporating domain theories overcome data deficiency moody j hanson
lippmann r eds advances neural information processing systems vol pp
san mateo ca morgan kaufmann
rumelhart hinton g williams r learning internal representations
error propagation rumelhart mcclelland j eds parallel distributed
processing explorations microstructure cognition foundations
pp mit press cambridge
schaffer c selecting classification method cross validation machine learning

schiffmann w joost werner r synthesis performance analysis
multilayer neural network architectures tech rep university koblenz institute
physics
scott g shavlik j ray w refining pid controllers neural networks
neural computation
tishby n levin e solla consistent inference probabilities layered
networks predictions generalization international joint conference neural
networks pp washington c ieee press
towell g symbolic knowledge neural networks insertion refinement
extraction ph thesis computer sciences department university wisconsin
madison wi
towell g shavlik j symbolic learning improve knowledge neural
networks proceedings tenth national conference artificial intelligence
pp san jose ca aaai mit press
towell g shavlik j knowledge artificial neural networks artificial
intelligence
turney p cost sensitive classification empirical evaluation hybrid genetic
decision tree induction journal artificial intelligence



ficonnectionist theory refinement

waterman guide expert systems addison wesley reading
watrous r towell g glassman synthesize optimize analyze repeat
soar application neural network tools ecg patient monitoring proceedings symposium nonlinear theory applications pp
honolulu hawaii
watson j hopkins n h roberts j w argetsinger steitz j weiner
molecular biology gene fourth edition benjamin cummings menlo
park ca
weigend overfitting effective number hidden units proceedings connectionist summer school pp boulder co
lawrence erlbaum associates
weigend huberman b rumelhart predicting future connectionist
international journal neural systems
whitley gordon mathias k lamarckian evolution baldwin effect
function optimization davidor schwefel h manner r eds parallel
solving nature ppsn iii pp springer verlag
whitley hanson optimizing neural networks faster accurate genetic search proceedings third international conference genetic
pp arlington va morgan kaufmann
yao x evolutionary artificial neural networks international journal neural
systems




