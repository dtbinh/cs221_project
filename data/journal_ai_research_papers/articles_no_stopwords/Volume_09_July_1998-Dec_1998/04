journal artificial intelligence

submitted published

probabilistic inference arbitrary uncertainty
mixtures factorized generalized gaussians
alberto ruiz
pedro e lpez de teruel
carmen garrido
universidad de murcia facultad de informtica
campus de espinardo murcia spain

aruiz dif um es
pedroe ditec um es
mgarrido dif um es

abstract
presents general efficient framework probabilistic inference learning
arbitrary uncertain information exploits calculation properties finite mixture conjugate families factorization joint probability density variables likelihood
function objective subjective observation approximated special mixture model
way desired conditional distribution directly obtained without numerical integration developed extended version expectation maximization em
estimate parameters mixture uncertain training examples indirect observations
consequence piece exact uncertain information input output values consistently handled inference learning stages ability extremely useful certain situations found alternative methods proposed framework formally justified
standard probabilistic principles illustrative examples provided fields nonparametric
pattern classification nonlinear regression pattern completion finally experiments real application comparative standard databases provide empirical evidence utility
method wide range applications

introduction
estimation unknown magnitudes available information form sensor measurements subjective judgments central many fields science engineering
solve task domain must accurately described model able support desired
range inferences satisfactory cannot derived first principles approximations must obtained empirical data learning stage
consider domain z composed collection objects z z z zn represented
vectors n attributes given partial knowledge expressed general form explained
later certain object z interested computing good estimate z close
true z allow heterogeneous descriptions attribute zi may continuous discrete symbolic valued including mixed types specific subset unknown uncertain attributes
estimated attribute vector partitioned z x z denotes target
output attributes target attributes different different objects z scenario includes several usual inference paradigms instance specific target symbolic
attribute task called pattern recognition classification target attribute continuous inference task called regression function approximation general interested general framework pattern completion partially known objects
example illustrate setting assume preprocessor hypothetical computer
vision system obtains features segmented object instances domain described

ai access foundation morgan kaufmann publishers rights reserved

firuiz lpez de teruel garrido

following n attributes area z color z white black red distance
z shape z circular rectangular triangular texture z soft rough
objecttype z door window angle z typical instance may z
blue triangular soft window object partially occluded dimensional
attributes missing uncertain instance available information z
could expressed blue black triangular window door
z z z uncertain z z exact z z missing case could interested estimates z z z even improving knowledge z z

non deterministic nature many real world domains suggests probabilistic
attributes considered random variables objects assumed drawn independently identically distributed p z p z zn p x multivariate joint probability
density function attributes completely characterizes n dimensional random variable z simplify notation use function symbol p denote different p f
identified without risk confusion
according statistical decision theory berger optimum estimators desired attributes obtained minimization suitable expected loss function

opt argmin e l


l loss incurred true estimated estimators features conditional posterior distribution p target variables given available
information instance minimum squared error mse estimator posterior mean
minimum linear loss estimator posterior median minimum error probability ep
loss estimator posterior mode
example typical prediction unknown attribute observed attributes x case available information written x continuous
reasonable use mse estimator mse e x general regression function
symbolic loss associated errors ep estimator adequate
ep argmaxy p x argmaxy p x p corresponds maximum posteriori rule
bayes test widely used statistical pattern recognition

joint density p z p x plays essential role inference process implicitly
includes complete information attribute dependences principle desired conditional
distribution estimator computed joint density adequate integration probabilistic inference process computing desired conditional probabilities possibly
implicit joint distribution p z prior model domain comprising implications
known event somewhat related certain z could obtain posterior p z
desired target marginal p probabilistic consequent
example observe exact value xo attribute x e x xo

p p xo





p xo
p xo dy

know instance z certain region r attribute space e z r
compute marginal density joint p z p x restricted region r fig



fiprobabilistic inference uncertain data mixtures

p



x

p x x r dx

p x dx
p x dxdy
r

r

general types uncertain information z discussed later


p r
r

p x
x

figure conditional probability density assuming z x r

summary joint density p z multivariate random variable subset variables z may principle estimated given available information whole z x
practical situations two steps required solve inference first good model
true joint density p z must obtained second available information must efficiently processed improve knowledge future partially specified instances z two
complementary aspects learning inference approached many scientific fields providing different methodologies solve practical applications
point view computer science essential goal inductive inference
approximate intensional definition properties unknown concept subset domain incomplete extensional definition finite sample machine learning techniques
michalski carbonell mitchell hutchinson provide practical solutions e g
automatic construction decision trees solve many situations explicit programming
must avoided computational learning theory valiant wolpert vapnik
studies feasibility induction terms generalization ability resource requirements
different learning paradigms
general setting statistical decision theory modeling techniques operational aspects inference numerical integration monte carlo simulation analytic approximations etc extensively studied bayesian perspective berger bernardo
smith specific field statistical pattern recognition duda hart
fukunaga standard parametric nonparametric density approximation techniques izenman
used learn training data class conditional p f required optimum
decision rule instance class conditional densities p x gaussian required parameters mean vector covariance matrix feature vector class decision regions x quadratic boundaries among nonparametric classification techniques parzen method k n nearest neighbors rule must mentioned analogously
target attribute continuous statistical dependence input output variables
p x properly modeled joint normality get multivariate linear regression mse x
x b required parameters mean values covariance matrix attrib



firuiz lpez de teruel garrido

utes nonlinear regression curves derived nonparametric approximation techniques nonparametric methods present slower convergence rates requiring significantly larger
sample sizes obtain satisfactory approximations strongly affected dimensionality data selection smoothing parameter crucial step contrast
require kind smoothness assumption target density
neural networks hertz et al computational trainable empirical data
proposed solve complex situations intrinsic parallel architecture
especially efficient inference stage one widely used neural multilayer perceptron universal function approximator hornik et al breaks limitations
linear decision functions backpropagation learning rumelhart et al
principle adjust network weights implement arbitrary mappings network outputs
desirable probabilistic properties wan rojas unsupervised networks probability density function approximation kohonen however neural
usually contain large number adjustable parameters convenient generalization
frequently long times required training relatively easy tasks input output role
attributes cannot changed runtime missing uncertain values poorly supported
bayesian networks concept conditional independence among
relevant probabilistic inference technologies pearl heckerman wellman joint
density variables modeled directed graph explicitly represents dependence
statements wide range inferences performed framework chang fung
lauritzen spiegelhalter significant inductive learning
network structures bouckaert cooper herskovits valiveti oomen
adequate large number variables showing explicit dependences
simple cause effect relations nevertheless solving arbitrary queries np complete automatic
learning time consuming allowed dependences variables relatively simple
attempt mitigate drawbacks developed general efficient inference learning framework following considerations well known
titterington et al mclachlan basford dalal hall bernardo smith
xu jordan reasonable probability density function p z approximated
desired degree accuracy finite mixture simple components ci l

p z p ci p z ci





superposition simple densities extensively used approximate arbitrary data dependences fig maximum likelihood estimators mixture parameters efficiently obtained samples expectation maximization em dempster laird rubin
redner walker see section



fiprobabilistic inference uncertain data mixtures


b
c
figure illustrative example density approximation mixture model samples p f p x showing nonlinear dependence b mixture model p x
gaussian components obtained standard em c location
components

decomposition probability distributions mixtures frequently applied
unsupervised learning tasks especially cluster analysis mclachlan basford duda
hart fukunaga posteriori probabilities postulated category computed
examples labeled according probable source density however
mixture specially useful nonparametric supervised learning situations instance
class conditional densities required statistical pattern recognition individually approximated priebe marchette traven finite mixtures hierarchical mixtures
linear proposed jordan jacobs peng et al mixtures factor analyzers developed ghahramani hinton hinton dayan revow
mixture useful feature selection pudil et al mixture modeling
growing semiparametric probabilistic learning methodology applications many
areas weiss adelson fan et al moghaddam pentland
introduces framework probabilistic inference learning arbitrary uncertain data piece exact uncertain information input output values consistently handled inference learning stages approximate joint density p z
model domain relative likelihood function p z describing available information specific mixture model factorized conjugate components way numerical integration avoided computation desired estimator marginal conditional
density
advantages modeling arbitrary densities mixtures natural conjugate components already shown dalal hall recently inference procedures
similar idea proposed ghahramani jordan cohn et al peng et al
palm however method efficiently handles uncertain data explicit likelihood
functions extensively used machine learning pattern recognition
related areas follow standard probabilistic principles providing natural statistical validation procedures
organization follows section reviews elementary
concepts used proposed framework section addresses inference stage section
concerned learning extending em manage uncertain information section
discusses method relation alternative techniques presents experimental evaluation
last section summarizes conclusions future directions work



firuiz lpez de teruel garrido

preliminaries
calculus generalized normals
many applications instances domain represented simultaneously continuous
symbolic discrete variables wilson martinez simplify notation
denote probability impulses gaussian densities means common formalism
generalized normal
x denotes probability density function following properties



x



x

exp



x x x x



tzero x
gaussian density mean standard deviation dispersion
reduces
diracs delta function located cases proper p f
x
x dx
x

product generalized normals elegantly expressed papoulis pp berger



x x x



mean dispersion normal given










relation useful computing integral product two generalized normals




x

x x dx



consistency define





x

x x dx

predicate predicate true zero otherwise virtually reasonable univariate
probability distribution likelihood function accurately modeled appropriate mixture
generalized normals particular p f symbolic variables mixtures impulses
without loss generality symbols may arbitrarily mapped specific numbers represented
numeric axes integrals discrete domains become sums
example let us approximate p f p x mixed continuous symbolic valued random variable x mixture generalized normals assume x takes probability
exact value special meaning probability random value continuously distributed following triangular shape shown fig density p x accurately approximated see section generalized normals

x x x x

p x



fiprobabilistic inference uncertain data mixtures

figure p f mixed random variable approximated mixture generalized normals

modeling uncertainty likelihood principle
assume value random variable z must inferred certain observation subjective information z drawn p z measurement judgment process
characterized conditional p z knowledge z updated according p z p z p z
p p z p z p z dz see fig
likelihood function fs z p z probability density ascribed possible
z arbitrary nonnegative function z interpreted two alternative ways
objective conditional distribution p z physical measurement process e g
model sensor noise specifying bias variance observable every possible true
value z known error model subjective judgment chance
different z values e g intervals likely regions etc vague difficult formalize
information dispersion fs z directly related uncertainty associated measurement process following likelihood principle berger explicitly assume
experimental information required perform probabilistic inference contained likelihood
function fs z

p z
z

z

z

z

p z

p z

p z




prior
model
measurement

p

fso z p z

z

observable

likelihood
observation

p z
posterior
z
figure illustration elementary bayesian univariate inference process



firuiz lpez de teruel garrido

inference mixtures conjugate densities
computation p z may hard unless p z p z belong special conjugate families berger bernardo smith case posterior density analytically
obtained parameters prior likelihood avoiding numeric integration
prior likelihood posterior mathematical family belief structure
closed inference process



example univariate case assume z known normally distributed around r
dispersion r e p z z r r assume measurement device gaussian noise observed values distributed according p z
z therefore
observe certain value property product generalized normals eq
posterior knowledge z becomes another normal
z expected location z expressed weighted average r r uncertainty reduced coefficient r r quantifies relative im





portance experiment respect prior

computational advantage extended general case mixtures conjugate families dalal hall approximate desired joint probability distribution
likelihood function
example domain likelihood modeled respectively

p z




pi

z


p z





r

r

z
r

r

r r r depend explicitly observed posterior
written following mixture



p z

r

properties parameters

r

r

z

r



r r weights r given

r r

r

r

r

r

r
r

pi r r r

p
k

l

k l k l

k l

role factorization
given multivariate observation z partitioned two subvectors z x assume
interested inferring value unknown attributes observed attributes x note
x statistically independent joint density factorizable p z p x p x
p therefore posterior p x equals prior marginal p observed x carries
predictive information optimum estimators depend x instance
mse x e x e ep x argmax p simplest estimation task
runtime computations required optimum solution may precalculated



fiprobabilistic inference uncertain data mixtures

realistic situations variables statistically dependent general joint density cannot factorized required marginal densities may hard compute however interesting
consequences arise joint density expressed finite mixture factorized independent variables components c c cl

p z p z z n p ci p z ci p ci p z j ci






j

structure convenient inference purposes particular terms desired partition z x

p z p x

p c p x c p c








marginal densities mixtures marginal components

p x p x dy p ci p x ci




p p ci p ci


desired conditional densities mixtures marginal components

p x x p ci





weights x probabilities observed x generated component ci

x

p ci p x ci

p c p x c
j

p ci x

j

j

p f approximation capabilities mixture factorized components remain
unchanged cost possibly higher number components obtain desired degree
accuracy avoiding artifacts see fig section discusses implications factorization
relation alternative model structures


b
figure density approximation data fig mixture factorized components b location components note arbitrary dependence
represented mixture components independent variables observe
somewhat smoother solution could obtained increasing number components



firuiz lpez de teruel garrido

mfgn framework
previous concepts integrated general probabilistic inference framework call
mfgn mixtures factorized generalized normals fig shows abstract dependence relations among attributes generic domain upper section figure attributes
observed information lower section mfgn framework relations modeled
finite mixtures products generalized normals key idea factorization cope
multivariate domains heterogeneous attribute vectors conjugate densities efficiently perform inferences given arbitrary uncertain information section derive
main inference expressions learning stage described section

p z
z

zn
z

model
domain
p z

zj
model
measurement
p z



figure generic dependences inference process

modeling attribute dependences domain
mfgn framework attribute dependencies domain modeled joint density
form finite mixture factored components expression component
marginals p z j ci z j ij ij generalized normals

p z pi


z

j

ij ij

l j n



j

desired terms associated pure symbolic attributes z j ij
collected way component marginals expressed mixtures impulses

p z j ci ti j z j





ti j p z j ci probability z j takes th value component ci
manipulation reduces number l global components mixture adjustable parameters
model proportions pi p ci mean value ij dispersion ij j th
j
attribute th component symbolic attributes probabilities ti

structure explicitly used symbolic attributes applications illustrative examples mathematical derivations made concise expression



fiprobabilistic inference uncertain data mixtures

variables continuous mfgn architecture reduces mixture gaussians
diagonal covariance matrices proposed factorized structure extends properties
diagonal covariance matrices heterogeneous attribute vectors interested joint
support inferences partial information subset variables note
easy way define measure statistical depencence symbolic continuous
attributes used parameter probability density function required hetereogeneous dependence model conveniently captured superposition simple factorized
independent variables densities
example figure shows illustrative attribute data set x continuous z
symbolic components mfgn approximation obtained em
see section joint density parameters mixture shown table
note overlapped structure data components assigned values symbolic attribute z


b
figure simple data set two continuous one symbolic attribute
b location mixture components



pi

ix

ix

iy

iy

tiz white


















































table parameters mixture model data set fig

tiz black








modeling arbitrary information instances
available information particular instance z denoted following likelihood
principle concerned true nature whether kind physical meas

reason pattern classification tasks separate typically built class conditional density



firuiz lpez de teruel garrido

urement subjective judgment location z attribute space need
update knowledge z form posterior p z relative likelihood function
p z observed general p z nonnegative multivariable function fs z
domain objective case statistical studies measurement process used
determine likelihood function subjective case may obtained standard distribution elicitation techniques berger case mfgn framework likelihood function available information used inference process approximated desired degree accuracy sum products generalized normals

p z p r p r z p r p srj z j
r

r



z
r

j

j

srj rj



j

r

without loss generality available knowledge structured weighted disjunction


r r conjunctions sr sr sr srn elementary uncertain
observations form generalized normal likelihoods z j srj rj centered srj
uncertainty rj measurement process interpreted r objective subjective sensors sr providing conditionally independent information p srj z j attributes
srj depends z j relative strength r note complex uncertain information instance z expressed nested combination elementary uncertain beliefs srj
z j probabilistic connectives ultimately expressed structure translates addition translates product product two generalized normals
attribute becomes single weighted normal
example consider hypothetical computer vision domain example assume
information object z following area around distance around b
likely shape surely triangular else circular area around c angle
around equal e structured piece information formalized

z z b
z triang z circ z c z z e

p z









b







c





expanded becomes mixture factorized components operationally represented
parameters shown table
simpler situation available information z could conjunction uncertain attributes similar color red green area shape rectangular
circular triangular likelihood shape values obtained output
simple pattern classifier e g k n nearest neighbors moment invariants attributes
color area directly extracted image case could interested
distribution values attributes texture objecttype alternatively
could start objecttype door window texture rough order determine probabilities color angle values selecting promising search region



fiprobabilistic inference uncertain data mixtures

r

sr r

sr r

sr r

sr r

sr r

sr r

sr r




b b




triang
c c





triang
c c





circ
c c





circ


c c




table parameters uncertain information model example



e

e

joint model observation density
generic dependence structure fig implemented mfgn framework shown
fig upper section figure model nature obtained previous learning stage
used inference without changes dependences among attributes conducted
intermediary hidden latent component ci lower section represents available
uncertain information measurement model query structure associated particular inference operation

ci

p z ci


domain

p z p ci p z j ci


z

z







zn



n

j



p z






n





r

sr



srn

sr


p

measurement
p z p r p srj z j



r

j

figure structure mfgn model attributes conditionally independent
measurement process modeled collection independent virtual sensors
p srj z j

joint density relevant variables becomes

p ci z sr p r p r z p z ci p ci

p

p ci p r

j
r

z j p z j ci

j

pi r

z

j

srj rj z j ij ij

j





firuiz lpez de teruel garrido

derive alternative expression eq convenient computing marginal densities desired variable following relation

p srj z j p z j ci p z j srj ci p z j srj ci p srj ci
properties define dual densities model

ij r p srj ci



zj

p srj z j p z j ci dz j srj ij ij r



p srj z j
p z j ci z j ij r ji r
p srj ci



ij r z j p z j srj ci

parameters ij r ij r ji r given

ij r ij rj
ij srj rj ij

ij r
j
r



j
r

ij rj
j
r

ij r likelihood r th elementary observation srj j th attribute z j
component ci ij r z j effect r th elementary information srj j th attribute z j marginal component p z j ci component ci notation mfgn model structure conveniently written

p ci z sr pi r ij r ij r z j



j

posterior density
inference process available information combined model domain
update knowledge particular object given piece information must compute posterior distribution p desired target attributes z estimators
obtained p minimize suitable average loss function
efficiently supported mfgn framework regardless complexity domain p z
structure available information r sr
attributes partitioned two subvectors z x z desired
target attributes x z rest attributes accordingly component sr
available information partitioned r rx ry information target attributes
r th observation independent model p z denoted sry often missing
pieces information srx represents information rest
attributes x convention write



fiprobabilistic inference uncertain data mixtures

p z p x pi r r r x r
r

r likelihood r th conjunction sr component ci

r ij r



j

terms ij r z j grouped according partition z x

r x io r z

r id r z





desired posterior p p p computed joint p z marginalization along x obtain p along z obtain p note univariate marginalization p z along attribute z j eliminates terms ij r z j sum

p p x dx pi r r r
x

r

p p z dz pi r r
z

r

therefore posterior density compactly written

p r r



r

r probability object z generated component ci elementary information sr true given total information

r p ci sr

pi r r

p

k

l k l



k l

r p sry ci marginal density p ci desired attributes th
component modified contribution associated sry since p sry ci
p sr ci expression follows expansion

p p sr ci p ci sr
r

summary joint density likelihood function approximated mixture
proposed structure computation conditional densities given events arbitrary geometry notably simplified factorized components reduce multidimensional integration
simple combination univariate integrals conjugate families avoid numeric integration
property illustrated fig



firuiz lpez de teruel garrido





p c



e

c

p x x



c

p c
p x c

p
p x c




p x c



x

p x c








x





figure graphical illustration essential property mfgn framework consider mse estimate conditioned event x x cylindrical
region required multidimensional integrations computed analytically terms
marginal likelihoods ji r associated attribute pair components
ci sr p x x respectively case r p ci
information supplied

example fig shows joint density two continuous variables x modeled
mixture factorized generalized normals fig b shows likelihood function
event x x fig c shows posterior joint density
p x fig shows likelihood function event x x
fig e shows posterior joint density p x fig f g respectively
posterior marginal density p x p complex inferences analytically computed mfgn framework without numeric integration



fiprobabilistic inference uncertain data mixtures



b

c


e
figure illustrative examples probabilistic inference arbitrary uncertain information mfgn framework see example



firuiz lpez de teruel garrido

p















p x





x















f
figure cont



g

expressions estimators
approximations optimum estimators easily obtained taking advantage
mathematically convenient structure posterior density mfgn framework
conditional expected value function g becomes linear combination constants

e g g p dy





r



r

g r dy



r

e r g



r

e r g e g ry ci expected value g th component modified r th observation sry

e r g





g z id r di r dy


analytically compute desired optimum estimators instance mse estimator single continuous attribute z requires mean values e r z id r

mse e r id r
r

explicit expression p compute conditional cost





e mse e mse e mse



r

r






r





r









r id r
r




note computing conditional expected value arbitrary function g several variables may difficult
general g expanded power series obtain e g terms moments p

sx information target attributes constants ei r g precomputed
model nature p z learning stage



fiprobabilistic inference uncertain data mixtures

therefore given tchevichev inequality answer mse e mse
confidence level shape p complex must reported explicitly point estimator mse makes sense p unimodal
example nonlinear regression fig shows mixture components regression
lines confidence band two standard deviations obtained simple example
nonlinear dependence two variables case joint density adequately
approximated components mse component mse comp
mse comp mse comp


b
figure nonlinear regression example components b components

target symbolic must compute posterior probability value
case di r id r id possible values taken z collecting together id r eq written

p r


r

r coefficients impulses located posterior probability
value

q p r
r

instance minimum error probability estimator ep

ep argmax



q

desired rejection threshold easily established reject decision entropy posterior h q log q estimated error probability e max q
high
example nonparametric pattern recognition fig shows bivariate data set elements two different categories represented value additional symbolic attribute joint density satisfactorily approximated component mixture fig
decision regions rejection threshold set shown fig b



firuiz lpez de teruel garrido

note statistical pattern classification usually start implicit explicit approximation class conditional densities contrast start joint density
class conditional densities easily derived fig c


b
c
figure simple nonparametric feature pattern recognition task attribute
joint mixture model feature space mixture components b decision boundary
c one class conditional densities

computation optimum estimators loss functions straightforward observe estimators combination different rules weighted degree
applicability typical structure used many decision methods case since
components joint density independent variables rules reduce constants
simplest type rule
examples elementary pieces information
important types elementary observations srj z j shown including corresponding likelihoods ij r modified marginals ij r z j j required expression
exact information srj z j observation modeled impulse

p srj z j

srj z j srj z j therefore
ij r srj ij ij
ij r z j z j srj
contribution ij r exact information input attribute z j standard likelihood p z j ci observed value z j component hand acquire
exact information target attribute z j one r elementary observation
j z j inference process trivially required p z j z j j
gaussian noise bias rj standard deviation rj observation modeled component mixture p srj z j srj z j rj rj expressed confidence interval z j srj rj rj property



fiprobabilistic inference uncertain data mixtures

ij r srj ij rj ij rj
effect noisy input z j srj rj equivalent effect exact input z j srj
mixture components larger variance ij ij rj uncertainty spreads
effect observation increasing contribution distant components
example fig shows simple two attribute domain approximated component
mixture interested marginal density attribute x given different degrees uncertainty input attribute modeled

p

sharpest density fig b providing x obtain density
b x finally obtain density c x obviously uncertainty increases uncertainty x expected value x moves towards
distant components become likely probability distribution expands situation interesting effect appears mode marginal density
change rate mean uncertainty skews p x effect suggests
optimum estimators different loss functions equally robust uncertainty


b
c


b
figure effect amount uncertainty see text data set component
model b p x uncertain ys around

j
j
output role r z becomes original marginal modified location disper


sion towards srj according factor ij ij rj quantifies relative
importance observation

ij r z j z j srj rj ij rj
missing data information j th attribute srj z j observation modeled p srj z j constant equivalently p srj z j srj b
arbitrary b components contribute weight

ij r p z j anything ci constant
target missing ij r z j reduce original marginal components



firuiz lpez de teruel garrido

ij r z j z j ij ij p z j ci
arbitrary uncertainty general unidimensional relative likelihood function approximated mixture generalized normals shown example ij r ij r z j
given respectively eqs
intervals useful functions cannot accurately approximated small number normal
components typical example indicator function interval used model uncertain
observation values equally likely srj z j b z j considered
input use shortcut ij r j b j j z j cumulative distribution normal marginal component p z j ci unfortunately expression ij r z j
required z j considered output may useful computing certain optimum estimaj
j
j
tors r z restriction p z j ci interval b normalized r
disjunction conjunction events finally standard probability rules used build
structured information simple observations subjective judgments objective evidence ascribe relative degrees credibility rj several observations srj z j overall
j
j j
likelihood becomes r r r particular j z j z j two

possibilities equiprobable ij p ci p ci analogously conjunctions
events translate multiplication likelihood functions
summary inference procedure
domain p z adequately modeled learning process explained section
system enters inference stage partially specified objects parameters
domain p z pi ij ij parameters model observation p z
r srj rj must obtain parameters ij r id r di r desired marginal posterior densities estimators inference procedure comprises following steps


compute elementary likelihoods ij r eq



obtain product r conjunction sr component ci eq



normalize pi r r obtain coefficients r posterior eq



choose desired target attributes z compute parameters id r

di r modified component marginal densities id r z eq


report joint posterior density graphs posterior marginal densities
desired attributes z eq provide optimum point interval etc estimators eq

example iris data inference procedure illustrated well known iris benchmark objects represented four numeric features x z w one symbolic category u u setosa u versicolor u virginica whole data set divided
two disjoints subsets training validation joint density satisfactorily approxi



fiprobabilistic inference uncertain data mixtures

mated see section component mixture error rate classifying u validation
set without rejection fig shows two projections examples location mixture components learned training subset parameters mixture shown table


b
figure two views iris examples components joint density mixture model u white u black u gray attributes x b attributes z w



pi















ix

ix

iy

iy

iz

iz







table parameters iris data joint density model

iw

iw















p u ci p u ci p u ci















table shows inference process following illustrative situations
case attribute z known z
case attributes x u known x u u
case attribute x uncertain x
case attributes x w uncertain x w note uncertainty decreases information supplied compare case
case structured query expressed terms logical connectives uncertain elementary
events z z u u u u










firuiz lpez de teruel garrido

case






input
output
input
output
input
output
input
output
input


output

x


















z






w































approx
unimodal

unimodal

bimodal



u


u u
u
u

u u

u u
u u
u u
u u

bimodal

table inference iris domain

consistency visually checked fig finally table shows
elementary likelihoods









ix

iy

j r case illustrating essence method

iz

iw

ui



ix







































table elementary likelihoods case table

iy







iz

iw

ui






e
e























independent measurements
one key features mfgn framework ability infer arbitrary relational
knowledge attributes form likelihood function adequately approximated
mixture model structure eq instance could answer questions happens z z tends less z j e p z high region z z j
however situations observations single attribute z j statistically
independent information attributes e g z around z j around b
attribute relations pay attention particular case illustrates role
main mfgn framework elements furthermore many practical applications satisfactorily solved assumption independent measurements judgments case
likelihood available information expressed conjunction n marginal observations j z j

p z p j z j
j





fiprobabilistic inference uncertain data mixtures

means sum products equation complete e includes elements
n fold cartesian product attributes

p z rj z j srj rj
j

r

j rj r factored likelihood function considered component
mixture r j j marginal observation allowed
mixtures generalized normals p j z j r rj z j srj rj case even think


function valued attributes z f z f n z n f j z j p j z j

range relative likelihood values z j loosely speaking attributes concentrated
f j z j may considered inputs attributes high dispersion play role outputs
since conditionally independent x given ci posterior obtained expansion

p p ci p ci p ci p ci






interpretation straightforward effect sx zd must computed
x zo components ci simple bayesian update p x
prior made see fig

ci
z



p z j ci

zd
id z








sd

j


zj
p j z j

sj



figure structure mfgn inference process independent pieces information case likelihood function factorizable data flow inference process shown dotted arrows



firuiz lpez de teruel garrido

learning uncertain information
previous section described inference process uncertain information
mfgn framework develop learning model domain
training examples uncertain specifically must parameters pi

ij ij ti j mixture structure approximate true joint density p z
training random sample z k k partially known associated likelihood
functions k structure
overview em
maximum likelihood estimates parameters mixture usually computed
well known expectation maximization em dempster laird rubin redner
walker tanner following idea principle maximization
training sample likelihood j k p z k mathematically complex task due product
sums structure however note j could conveniently expressed maximization components generated example known called complete data em terminology underlying credit assignment disappears estimation task reduces several uncoupled simple maximizations key idea em following instead maximizing
complete data likelihood unknown iteratively maximize expected value
given training sample current mixture parameters shown process
eventually achieves local maximum j
instead rigorous derivation em found references see especially mclachlan krishnan present heuristic justification
provides insight generalizing em accept uncertain examples review
first simplest case missing uncertain values allowed training set
parameters mixture conditional expectations

e z ci g z ci g z p z ci dz
z





particular ij e z j ci ij e z j ij ci ti j e z j ci

mixture proportions pi e p ci z
rewrite conditional expectation bayes theorem form unconditional expectation

e z ci g z ci g z p ci z p z p ci dz



e z g z p ci z pi



z

em interpreted method iteratively update mixture parameters
expression form empirical average training data starting
tentative randomly chosen set parameters following e steps repeated


expression used iterative approximation explicit functions indirectly known
sampling e g subjective likelihood functions sketched human user example case p z set
target function p ci z computed current mixture model



fiprobabilistic inference uncertain data mixtures

total likelihood j longer improves notation expression k means expression computed parameters example z k


e expectation step compute probabilities qi k p ci z k k th example
generated th component mixture

qi k p z k ci p ci p z k
maximization step update parameters component examples weighted
probabilities qi k first priori probabilities component

pi


q k
k

continuous variables mean values standard deviations component


mpi

ij
ij


mpi

q

z j k



k

q

z j k ij





k

symbolic variables probabilities value

ti j


mpi

q

z j k



k

extension uncertain values
general mfgn framework know true values z j attributes
training examples required compute g z p ci z empirical expectation instead
start uncertain observations k true training examples z
likelihood functions expressed mixtures generalized normals

k

form

p k z k p k sr k p sr k z k
r

therefore must express expectation p z unconditional expectation
p distribution generates available information training set
easily done expanding p z ci terms

e z ci g z ci g z p z ci dz
z

p z c p c ds dz
g z p z c dz p c p ds p c




z



z

g z











define







firuiz lpez de teruel garrido

e z ci g z ci g z p z ci p z dz p ci
z

parameters p z finally written unconditional expectation observable p form similar eq

e z ci g z ci e p ci pi



expression justifies extended form em iteratively update parameters p z averaging p ci available training information k
drawn p considered numerical statistical method solving p z
integral equation



z

p z p z dz p

note cannot approximate p fixed mixture terms p ci computing back corresponding p z ci general p z different different training examples reason elementary deconvolution methods directly
applicable
kind addressed vapnik perform inference
indirect measurements ill posed requiring regularization techniques
proposed extended em considered method empirical regularization
solution restricted family mixtures generalized gaussians em
proposed kaveh regularization context image restoration
interpretation straightforward since know exact z required
approximate parameters p z empirically averaging g z p ci z obtain
averaging corresponding p ci domain plays
role g z z uncertain g z replaced expected value component
given information particular exact knowledge training set
tributes k z k e r marginal likelihoods impulses reduces
fig illustrates approximation process performed extended version em
simple univariate situation
convenient develop version proposed extended em uncertain
training sets structured tables sub cases uncertainly valued variables see fig
first let us write eq expanding terms components sr

p z ci p ci p z ci


p z c p p z c p c p


r

r

r



r



r

r

r

therefore

p ci r r p ci r
r

obtained relation ez w z es ez w z w z g z p ci z bayes theorem





fiprobabilistic inference uncertain data mixtures



b


c

figure extended em iteratively reduces large difference
true density p z b mixture model p z indirectly small
discrepancies c true observation density p modeled observation density p real cases p must estimated finite sample
k






















r k

r

sr r k

srj rj k







figure structure uncertain training information extended em coefficients

r k normalized easy detection rows included

uncertain example z



k



k

k

uncertain

reduces single row


j

notation introduced

r r e z sr ci g z r ci



z

g z p z r ci dz



z

p ci r p ci r p r r
write



g z ij r z j dz
j

firuiz lpez de teruel garrido



e z ci g z ci e r g z ij r z j dz pi
z
j
r

mfgn framework contributions r r p ci r empirical expected
values required extended em obtained without numeric integration
need consider case g z z j compute means ij probabilities ti j
g z z j deviations ij already know explicit expression parameters ij r z j z j ij r ji r hence



z

z j r z dz

z
z

j



z

z j ij r z j dz j ij r

r z dz ij r ji r

conclusion steps extended em follows
e expectation step compute elementary likelihoods training set



ij r k srj ij ij rj



k





obtain likelihood conjunction sr k example k component ci

kr ij r k
j


obtain total likelihood example k

k p k pi r k kr


r




compute probabilities qi kr p ci r k k r th component k th exam

ple generated th component mixture

qi kr kr pi r k kr k
maximization step update parameters component ci components

r k examples weighted probabilities qi kr first prior probabilities
component

pi




q
k

k
r

r

mean value standard deviation component

ij


mpi

q
k

r



r

ij r



k

fiprobabilistic inference uncertain data mixtures

ij


mpi

q
k

r



ij r ij r

r

k

ij



symbolic variables representation may use

ij r k p srj k ti j





ti j


mpi

q
k

r

p srj j ij r

r



k



consider particular case attributes training examples contaminated
unbiased gaussian noise likelihood uncertain observations modeled





component mixtures p k z k j z j k j k j k j k variance
measurement process z j k obtains observed value j k ex


pressed confidence interval z j k j k j k case basic em

easily modified take account effect uncertainties j k e step com
pute qi k following deviations

ij ij j k
step apply substitution

z j k j k ij



z j k j k ij




j k



ij
j
j k
measures relative importance observed j k computing ij ij
previous situation illustrates missing values must processed learning stage

j k
z
exact j k original changed

extreme z j k missing modeled j k get therefore

observation j k contribute parameters correct procedure deal
missing values mfgn framework simply omitting empirical averages
note fact arises factorized structure mixture components providing conditionally independent attributes alternative learning methods require careful management
missing data avoid biased estimators ghahramani jordan


evaluation extended em
studied improvement parameter estimations uncertainty observations modeled likelihood functions explicitly taken account proposed extended



firuiz lpez de teruel garrido

em compared em raw observations basic em ignores
likelihood function typically uses average value e g given x basic em uses
x considered synthetic attribute domain following joint density
p x w x w white
x w black

tt

tt

tt

different learning experiments performed varying degrees uncertainty
cases training sample size trained structure true density components since goal experiment measure quality estimation
respect amount uncertainty without regard sources variability
local minima alternative solutions etc empirically studied section table shows
mixture parameters obtained learning fig graphically shows difference extended basic em illustrative cases
case exact data fig
cases extended em learning rate missing
values training data
case basic em attribute biased units probability case extended em
case see fig b observed value sy samples
sy rest samples basic em uses observed value sy extended em uses
explicit likelihood function f ysy sy
case basic em attributes x gaussian noise w changed
probability case extended em case
case basic em x gaussian noise w changed probability
case extended em case see fig c
case basic em x gaussian noise w changed probability
case extended em case see fig



case extended em values missing censoring case extended em case
missing values assumed distributed
providing additional information data generation mechanism
table fig confirm small amounts deterioration relation sample
size estimates computed basic em raw observed data similar
obtained extended em e g cases however data sets
moderately deteriorated true joint density correctly recovered extended em
likelihood functions attributes instead raw observed data e g cases fig
c finally large amount uncertainty respect training sample
size true joint density cannot adequately recovered e g cases fig



fiprobabilistic inference uncertain data mixtures



b

c

figure illustration advantages extended em see text
case exact data b cases biased data c cases moderated noise
cases large noise figures true mixture components gray ellipses available raw observations black white squares components estimated basic em raw observations dotted ellipses components estimated extended em taking account likelihood functions uncertain
values black ellipses

note ability learn uncertain information suggests method manage non
random missing attributes e g censoring ghahramani jordan complex
mechanisms uncertain data generation illustrated case missing data generation
mechanism depends value hidden attribute correct assign equal likelihood
components principle statistical studies kind knowledge may help ascertain
likelihood true values function available observations instance case
replaced missing attributes case normal likelihoods e high improving estimates mixture parameters



firuiz lpez de teruel garrido

case

p

x



x



wwhite

x



true











































table parameter estimates uncertain information see text

x



wblack




















































example learning examples missing attributes performed iris
domain illustrate behavior mfgn framework whole data set randomly
divided two subsets equal size training testing component mixture
obtained evaluated combining missing data proportions error
prediction attribute u plant class following

missing attributes

training set





test set





prediction error





relatively simple iris domain performance degradation due missing attributes much greater inference learning stage extended em able
correctly recover overall structure domain available information

comments
convergence em fast requiring adjustable parameters learning
rates robust respect random initial mixture parameters bad local
maxima frequent alternative solutions usually equally acceptable examples
contribute components never wasted unfortunate initialization fixed
number components progressively increases likelihood j training data
maximum reached number components incremented maximum j
increases limit value obtained cannot improved extra components fukunaga simple heuristics incorporated standard expectation maximization
scheme control value certain parameters e g lower bounds established variances quality model e g mixture components eliminated proportions
small
case factorized components specially convenient matrix inversions
required important uncertain missing values correctly handled



fiprobabilistic inference uncertain data mixtures

simple unified way heterogeneous attribute sets necessary provide
uncertain attribute correlations since covariance parameters must estimated finally
training sample size must large enough relation degree uncertainty examples complexity joint density model order obtain satisfactory approximations
hand number mixture components required satisfactory approximation joint density must specified pragmatic option minimization experimental estimation cost main inference task exists instance regression
could increase number components acceptable estimation error obtained
independent data set cross validation idea applies pattern classification use
number components minimizes error rate independent test set however one
main advantages proposed method independence learning stage
inference stage freely choose dynamically modify input output role
attributes therefore global validity criterion desirable typical validation methods
mixture reviewed mclachlan basford standard
likelihood ratio tests number components unfortunately method validate
mixture selects best number components desoete
since mfgn framework provides explicit expression model p z apply
statistical tests hypothesis independent sample taken true density e g subset examples reserved testing obtained approximation compatible
test data hypothesis h comes p z rejected learning process must
continue possibly increasing number components difficult build statistical
tests e g moments p z sample means variances directly obtained
however data sets usually include symbolic numeric variables developed
test expected likelihood test sample measures well p z covers examples mean variance p z easily obtained properties generalized
normals experiments simple univariate continuous densities test
powerful small sample sizes e incompatibility detected standard tests significantly evidence rejection nevertheless clearly inaccurate approximations
detected improve sample size increases test valid data sets uncertain values
minimum description length li vitnyi principle invoked select
optimum number components trading complexity model accuracy
description data



firuiz lpez de teruel garrido

discussion experimental
advantages joint
inductive inference methods compute direct approximation conditional densities
interest even obtain empirical decision rules without explicit underlying conditional densities cases model learning stage depend selected input
output role variables contrast presented inference learning method
approximation joint probability density function attributes convenient
parametric family special mixture model mfgn framework works pattern completion
machine operating possibly uncertain information example given pattern classification
learning stage suffices predicting class labels feature vectors
estimating value missing features observed information incomplete patterns
joint density finds regions occupied training examples whole attribute
space attribute dependences captured higher abstraction level one provided
strictly empirical rules pre established target variables property extremely useful
many situations shown following examples
example hints provided inference multivalued relations given data
set model example assume interested value x
obtain bimodal marginal density shown fig corresponding estimator x
sense meaningless however specify branch interest
model inferring x e x small obtain unimodal
marginal density fig b reasonable estimator x


b
figure desired branch multivalued relations selected providing
information output values bimodal posterior density inferred
b unimodal posterior density inferred hint x small

example image processing advantages joint model supporting inferences
partial information inputs outputs illustrated following application
natural data see fig image fig characterized attribute density
x r g b describing position color pixels random sample pixels
used build component mixture model interested location certain ob



fiprobabilistic inference uncertain data mixtures

jects image figs b f posterior density desired attributes given following queries


something light green fig b two groups easily identified posterior
density corresponding upper faces green objects c x unknown
r g b



something light green dark red fig c groups
additional scattered group corresponding red object greater dispersion
arises larger size red object fact r component
dark red disperse g component light green two equiprobable components c c x unknown r g b



something light green right fig provide partial information
output c x unknown r g b



something white fig e c x unknown r g b



something white lower left region main diagonal x fig f
provide relational information attributes modeled equiprobable components note case posterior distribution contains components still computationally manageable

x r g b
x r g b
x r g b
x r g b
x r g b
x r g b
cases posterior density consistent structure original image time
required compute posterior distribution lower one second learning time
order hours pentium system simpler component obtained
random pixels produced acceptable much lower learning time furthermore em efficiently parallelized

hand large number irrelevant attributes joint model strategy wastes resources capture proper probability density function along unnecessary dimensions arise specification likelihood function since relevant attributes explicitly appear model joint modeling appropriate domains
moderated number meaningful variables without fixed input output roles



note sharp peak component small dispersion obtained learning process transmits
posterior density kind artifacts inocuous easily removed post processing



firuiz lpez de teruel garrido



b

c



e
f
figure inference image domain example source image
b posterior density attributes x given something light green c
something light green dark red something light green right
e something white f something white lower left region
main diagonal image x

advantages factorization
proposed methodology supported general density approximation property mixture
use components independent variables order make computations feasible



fiprobabilistic inference uncertain data mixtures

inference learning stage factorized components imposed mixture model without loss generality statistical dependence variables still captured cost
possibly larger number components mixture achieve required accuracy
approximation
simplicity building block structure entirely compensated important saving computation time high dimensional integrals analytically computed univariate
integrals matrix inversions avoided learning stage additionally high dimensional
domains easily modeled small number parameters mixture component
viewpoint computational learning theory vapnik small number adjustable parameters actually low expressive power favorable consequences
generalization
mixtures factorized components used latent class analysis desoete
well known unsupervised classification technique assumed statistical dependences
attributes fully explained hidden variable specifying latent class
example method similar gaussian decomposition clustering mentioned
section constrained component conditional attribute independence however goal
unsupervised classification obtaining accurate mathematically convenient expression
joint density variables required derive desired estimators meaning
components irrelevant long whole mixture good approximation joint density
expressive architectures combine mixture local dimensionality reduction considered mixtures linear experts jordan jacobs mixtures
principal component analyzers sung poggio mixtures factor analyzers ghahramani hinton hinton dayan revow unfortunately general kind inference learning uncertain data considered work cannot directly incorporated
architectures computational advantages demonstrated mfgn model
restriction factorized components may produce undesirable artifacts approximations certain domains learned small training samples nevertheless
occurs approximator structure building block match shape
target function case many terms components units etc required good
approximation associated parameters correctly adjusted large training
sample however note complexity model measured uniquely terms
number mixture components number adjustable parameters probably better
measure complexity instance full covariance quadratic growth number free parameters respect dimension attribute vector factorized components growth linear amount training data need unreasonably high even
number mixture components large
real applications nature target function unknown little said priori
best building block structure used universal approximator chosen
simple component structure make inference learning feasible uncertain information section provides experimental evidence realistic proposed model
inferior popular approaches
qualitative comparison alternative approaches
instead proposed methodology mixture em alternative nonparametric density approximation methods could used joint density
specific conditional densities instance nearest neighbor rule locally approximates
target density certain number training samples near point interest symbolic



firuiz lpez de teruel garrido

attributes directly estimated voting scheme continuous attributes estimated
averaging observed values training instances near subspace observed
attributes point interest however small sample sizes estimators
smooth strong sensitivity random fluctuations training set penalizes
estimation cost large sample size time required nearest neighbors becomes
long example consider regression example section fig b shows
mfgn solution components mse fig shows regression line obtained nearest neighbors average higher mse
parzen windows similar kernel approximation methods used smooth
simple nearest neighbors rule duda hart izenman actually mixtures
simple conventional densities located training samples principle properties
mfgn framework could adapted kind approximation ruiz et al learning
becomes trivial strong run time computation effort required since concise model
domain extracted training set kind rote learning negative consequences generalization according occam razor principle li vitnyi adequately cross validated mixture model small number components relation training sample size reasonably guarantees probably true attribute dependencies correctly
captured

c

c
c

c


b
c
figure alternative solutions regression classification see text details

nature solutions obtained backpropagation multilayer perceptrons rumelhart et
al pattern classification illustrative general decision region geometrically expressed union intersections several halfspaces defined units
first hidden layer however backprop networks often require long learning times many adjustable parameters worse apparently simple distributions patterns hard learn
instance solution circle ring classification fig b obtained network hidden units requires hundreds standard backprop epochs decision regions
satisfactory even though network extra flexibility task hidden units
suffice separate training examples better solutions exist resources network architecture backprop learning contrast solution obtained
mfgn components fig c requires learning time orders magnitude
shorter backprop optimization components mixture contribute synthesize
reliable decision regions acceptable solutions obtained smaller number
components



fiprobabilistic inference uncertain data mixtures

proposed closely related well known family approximation techniques
essentially distribute kind clustering self organizing detectors relevant regions input space combine responses computing
desired outputs case radial basis functions rbf hertz et al classification regression trees proposed breiman et al topological maps used cherkassky najafi locate knots required piecewise linear regression
relevant methodology proposed jordan jacobs peng et al
em used learn hierarchical mixtures experts form linear rules
way desired posterior densities explicitly obtained properties em satisfactorily used ghahramani jordan obtain unbiased approximations missing data mixture framework similar framework extends
successful exploiting conjugate properties chosen universal approximation
model uncertain information arbitrary complexity efficiently processed inference
learning stages
mfgn framework appropriate moderated number variables showing relatively
complex dependencies contrast bayesian networks satisfactorily addresses case large
number variables clear conditional independence relations situations
certain subset variables bayesian network shows explicit causal structure subdomain could empirically modeled mixture model order considered later composite node embedded whole network subdomain conditionally isolated
rest variables set communication nodes mfgn framework used
perform required inferences
finally mixture typically used unsupervised classification examples
labeled index component highest posterior probability fact mfgn
framework explicitly finds clusters training set furthermore continuous symbolic attributes allowed joint density examples clustered implicit probabilistic metric automatically weighs heterogeneous attributes even missing
uncertain values however method effective groups interest
structure component densities order simplify inference mixture components
selected constraints gaussian independent variables necessarily verified
natural groups found real applications
tentative possibility inspired common heuristic clustering technique consists joining overlapping components e g according battachariya distance well known bound
bayes error used statistical pattern recognition fukunaga unfortunately experiments indicate overlapping threshold free parameter strongly determines
quality universal threshold independent application seem exist
principle clusters arbitrary geometry may discovered cannot easily automated
therefore nonparametric cluster analysis methods e g density valley seeking suggested
labeling complex groups
experimental evaluation
mfgn method evaluated standard benchmarks machine learning database repository university california irvine merz murphy contains inductive learning representative real world situations experimented
following databases ionosphere pima indians monk horse colic
illustrate different properties proposed methodology cases mfgn compared alternative learning methods respect inference task considered interest



firuiz lpez de teruel garrido

typically prediction specific attribute given rest usually give
error rate training test set indicate amount overfitting obtained
learning

ionosphere
b pima indians
figure discriminant projections two representative databases

ionosphere database
two classes radar returns ionosphere must discriminated vectors continuous attributes examples randomly partitioned two disjoint subsets approximately equal size training testing prevalence minoritary class random
prediction rate figure table typical statistical pattern recognition easily solvable standard methods suggest bayes optimum
error probability around
error rate
pe
training set
method
test set
linear mse pseudoinverse
nearest neighbor
nearest neighbor
parzen model
backprop multilayer perceptron hidden units
support vector machine rbf kernel width v
support vector machine rbf kernel width v
support vector machine polinomial kernel order v
support vector machine polinomial kernel order v
support vector machine polinomial kernel order v
full covariance gaussian mixture component class
full covariance gaussian mixture component class
full covariance gaussian mixture component class
mfgn components average
mfgn components average
mfgn components average
mfgn best cross validation components
table ionosphere database
















originally database contains attributes two meaningless ill behaved eliminated





















fiprobabilistic inference uncertain data mixtures

plain mfgn method without special heuristics learning stage
comparable average alternative methods best solution training set crossvalidation entirely satisfactory
ionosphere database present exhaustive study performance given varying
proportions missing values training testing examples value x means
training test examples value attribute deleted probability x basic experiment consists learning mfgn model prescribed number components
computing error rate training test sets table shows mean value standard deviations error rates obtained repetitions basic experiment configuration column contains error rate configuration training set training test partition kept fixed analyze variability solutions due random initialization
em
learning




inference





comp
comp
comp





















comp
comp
comp





















comp
comp
comp





















comp

comp

comp

table evaluation mfgn ionosphere database given
different proportions missing data training testing subsets

expected mfgn model robust respect large proportions missing values
test patterns moderated proportions missing data training set compared behavior standard decision tree construction inspired
quinlan able support missing values table shows error rates
decision trees experimental setting table kind decision tree obtains
error rates better averages obtained mfgn however mfgn best solutions
selected cross validation better ones obtained decision tree furthermore
decision tree performance degrades faster mfgn especially respect proportion
missing values inference stage



essentially missing values handled follows learning stage attribute selected examples
missing values sent partitions appropriate weights inference stage node asks missing
value follows branches appropriate weights finally outputs combined



firuiz lpez de teruel garrido

learning




inference





























table evaluation basic decision tree ionosphere database given different proportions missing data training testing subsets

pima indians database
must discriminate two possible diabetes test given pima
indians continuous attributes examples randomly partitioned two disjoint subsets equal size training testing prevalence minority class
attribute vector normalized table presents comparative
error rate
pe
training set
method
test set
linear mse pseudoinverse
oblique decision tree decision nodes
nearest neighbor
nearest neighbor
full covariance gaussian mixture component class
full covariance gaussian mixture component class
full covariance gaussian mixture component class
full covariance gaussian mixture component class
backprop multilayer perceptron hidden units
backprop multilayer perceptron hidden units
backprop multilayer perceptron hidden units
support vector machine rbf kernel width v
support vector machine rbf kernel width v
support vector machine polynomial kernel order v
support vector machine polynomial kernel order v
mfgn components
mfgn components
mfgn components
table pima indians database



































despite low dimensionality large number examples classification
hard see figure b even sophisticated learners backpropagation networks decision
trees support vector machines able store reasonable proportion training set
achieve significant generalization mfgn shows similar behavior although slightly
less prone overfitting error rate training set misleading
horse colic database
database contains classification task heterogeneous attribute vector including symbolic discrete continuous variables missing values illustrates



fiprobabilistic inference uncertain data mixtures

feature selection context joint modeling mentioned section table shows
error rates obtained mfgn different attribute subsets take advantage general
inference properties mfgn model must applied attribute subset interest
inference task fixed number attributes large alternative methods
used
method

pe

pe

distribution initializations

best

selected attributes
mfgn components
mfgn components
mfgn components
mfgn components
mfgn components
mfgn components
mfgn components
mfgn components
mfgn components













































selected attributes
mfgn components
mfgn components
mfgn components
mfgn components
mfgn components
mfgn components

selected attributes
mfgn components
mfgn components
mfgn components
mfgn components
table horse colic database random rate

monk
monk three concept learning tasks symbolic attributes widely used
benchmarks inductive learning thrun et al seen table mfgn fails
monk acceptable generalization obtained monk training
examples cannot even stored contrast mfgn correctly solves monk behavior
related fact monk deterministic abstract concepts
may lack kind geometric regularities attribute space required probabilistic


features individually selected simple discrimination index related kolmogorov smirnov statistic
ruiz

typical example parity acceptable training set generalization cannot achieved inductive
bias learning machine biased towards smooth solutions



firuiz lpez de teruel garrido

fig shows discriminant projections datasets illustrates fact
monk cannot easily captured statistic techniques benchmark mfgn performance
similar popular probabilistic methods thrun et al

monk
b monk
figure discriminant projections monk datasets

c monk

error rate
training set

method

pe
test set

monk random rate
linear mse pseudoinverse
nearest neighbor
support vector machine rbf kernel width v
cascade correlation
mfgn components
mfgn components













monk random rate
linear mse pseudoinverse
nearest neighbor
support vector machine rbf kernel width v
cascade correlation
mfgn components
mfgn components
mfgn components















monk random rate
linear mse pseudoinverse
nearest neighbor
support vector machine rbf kernel width v
cascade correlation
mfgn components
mfgn components
mfgn components
table monk

















fiprobabilistic inference uncertain data mixtures

comments
experiments demonstrate mfgn model able obtain acceptable
many real world applications particular error rates obtained standard classification tasks
comparable obtained popular learners additionally mfgn able perform
inferences attribute given uncertain partial information possible
alternative methods property makes mfgn attractive alternative many
inference one illustrated example experiments contributed characterize kind mfgn model best suited essentially
relationship among attributes must true probabilistic nature attribute vector must
moderated size containing relevant variables previous feature selection accommodation
stage recommended certain applications

conclusions
developed efficient methodology probabilistic inference learning uncertain information proposed mfgn framework joint probability density function
attributes likelihood function available information approximated mixtures factorized generalized normals mathematical structure allows efficient computation
without numerical integration posterior densities expectations desired variables given
events arbitrary geometry extended version em learning developed estimate parameters required mixture uncertain training examples
different paradigms pattern recognition regression pattern completion subsumed
common framework
comprehensive collection examples illustrates methodology critically
compared alternative techniques extended em able learn satisfactory
domain reasonable number examples uncertain values taking account
explicit likelihood functions available information satisfactory whenever
sample size large relation amount known degradation training set experiments characterized kind situations model manages better domains described moderate number heterogeneous attributes complex probabilistic dependences
output variables necessarily known learning stage e pattern
completion finally explicit management uncertainty needed learning inference stage even mfgn framework obtained
favorable trade useful features model complexity solutions different applications benchmarks
future developments work include improving learning stage heuristic
steps combined standard e steps control adequacy acquired
additional studies required validation tests generalization scalability robustness
data preprocessing essential idea working explicit likelihood functions
incorporated parzen approximation scheme interested expressive
model structures mixtures factor analyzers principal component analyzers linear experts finally methodology developed pure bayesian framework subsumed
dempster shafer evidence theory



firuiz lpez de teruel garrido

acknowledgments
authors would thank anonymous reviewers careful reading helpful suggestions work supported spanish cicyt grants tic tic c tic c

references
berger j statistical decision theory bayesian analysis springer verlag
bernardo j smith f bayesian theory wiley
bouckaert r r properties bayesian belief network learning proceedings uncertainty ai pp
breiman l friedman j h olshen r stone c j classification regression
trees wadsworth international group belmont ca
chang k fung r symbolic probabilistic inference discrete continuous variables ieee tran systems man cybernetics vol june pp
cherkassky v lari najafi h nonparametric regression analyisis selforganizing topological maps h wechsler ed neural networks perception vol
computation learning architectures san diego academic press
cohn ghahramani z jordan active learning statistical
journal artificial intelligence pp
dalal r hall w j approximating priors mixtures natural conjugate priors
j r statist soc b vol pp
de soete g latent class analysis categorization v mechelen
j hampton r michalski p theuns eds categories concepts theoretical views
inductive data analysis san diego academic press
dempster p laird n rubin b maximum likelihood estimation incomplete data via em journal royal statistical society series b vol
pp
duda r hart p e pattern classification scene analysis john wiley sons
fan c namazi n penafiel p b image motion estimation
em technique ieee transactions pattern analisys machine intelligence vol march pp
fukunaga k introduction statistical pattern recognition academic press
ghahramani z jordan supervised learning incomplete data via em
cowan j tesauro g alspector j eds advances neural information processing systems morgan kauffman
ghahramani z hinton g e em mixtures factor analyzers
tech rep univ toronto crg tr



fiprobabilistic inference uncertain data mixtures

heckerman wellman p bayesian networks communications acm vol
pp march
hertz j krogh palmer r g introduction theory neural computation
addison wesley
hinton g e dayan p revow modeling manifold images handwritten
digits ieee neural networks pp
hornik k stinchcombe white h multilayer feedforward networks universal
approximators neural networks
hutchinson algorithmic learning york oxford univ press
izenman j recent developments nonparametric density estimation j amer statist assoc vol pp
jordan jacobs r hierarchical mixtures experts em
neural computation pp
kohonen self organization associative memory springer verlag
lauritzen l spiegelhalter j local computations probabilities graphical
structures application expert systems j r statist soc b pp
li vitnyi p introduction kolmogorov complexity applications
york springer verlag
mclachlan g j basford k e mixture york marcel dekker
mclachlan g j krishnan em extensions john wiley
sons
michalski r carbonell j mitchell eds machine learning artificial
intelligence palo alto ca tioga press reprinted morgan kaufmann
los altos ca
michalski r carbonell j mitchell eds machine learning artificial
intelligence vol ii los altos ca morgan kaufmann
mohgaddam b pentland probabilistic visual learning object representation ieee pami vol pp
merz c j murphy p uci repository machine learning databases
http www ics uci edu mlearn mlrepository html irvine ca university california
department information computer science
palm h c method generating statistical classifiers assuming linear mixtures
gaussian densities proceedings th iapr international conference pattern recognition jerusalem october vol ieee piscataway nj usa
papoulis probability random variables stochastic processes mcgraw hill
pearl j probabilistic reasoning intelligent systems networks plausible inference
morgan kaufmann



firuiz lpez de teruel garrido

peng f jacobs r tanner bayesian inference mixtures experts hierchical mixtures experts application speech recognition accepted
journal american statistical association
priebe c e marchette j adaptive mixtures recursive nonparametric pattern recognition pattern recognition v n pp
pudil p novovicova j choakjarernwanit n kittler j feature selection
approximation class densities finite mixtures special type pattern recognition
vol pp
quinlan j r c programs machine learning san mateo ca morgan kaufmann
redner r walker h f mixture densities maximum likelihood estimation
em siam review vol pp
rojas r short proof posterior probability property classifier neural networks neural computation vol issue january
rumelhart e hinton g e williams r learning internal representations
error propagation rumelhart mcclelland pdp group pp
rumelhart e mcclelland j l pdp group parallel distributed processing explorations microstructure cognition vol foundations cambrigde
bradford books mit press
ruiz nonparametric bound bayes error pattern recognition vol
pp
ruiz lpez de teruel p e garrido c kernel density estimation indirect observations preparation
sung k k poggio example learning view human face detection ieee trans pattern analyisis machine intelligence vol n january pp

tanner tools statistical inference rd ed springer
thrun et al monk performance comparison different learning
technical report cmu cs
titterington f smith u e makov statistical analysis finite mixture
distributions wiley york
traven h g c neural network statistical pattern classification
semiparametric estimation prob den func ieee neural networks v n
valiant l g view computational learning theory meyrowitz chipman
eds foundations knowledge acquisition machine learning kluwer acad pub
valiveti r oommen b j chi squared metric determining stochastic
dependence pattern recognition v n pp
vapnik v n learning dependencies empirical data springer york
vapnik v n nature statistical learning theory springer york



fiprobabilistic inference uncertain data mixtures

wan e neural networks classification bayesian interpretation ieee trans
neural networks v n
weiss adelson e h perceptually organized em framework motion segmentation combines information form motion tr mit mlpcs tr
wilson r martinez r improved heterogeneous distance functions jair v
pp
wolpert h ed mathematics generalization proc sfi clns workshop
formal approaches supervised learning
xu l jordan convergence properties em gaussian
mixtures neural computation vol issue january
l kaveh regularization joint blur identification image
restoration ieee image processing vol pp




