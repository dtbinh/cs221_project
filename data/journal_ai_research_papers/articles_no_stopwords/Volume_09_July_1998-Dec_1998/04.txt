Journal Artificial Intelligence Research 9 (1998) 167-217

Submitted 6/98; published 10/98

Probabilistic Inference Arbitrary Uncertainty
using Mixtures Factorized Generalized Gaussians
Alberto Ruiz
Pedro E. Lpez-de-Teruel
M. Carmen Garrido
Universidad de Murcia, Facultad de Informtica,
Campus de Espinardo, 30100, Murcia, Spain

ARUIZ@DIF.UM.ES
PEDROE@DITEC.UM.ES
MGARRIDO@DIF.UM.ES

Abstract
paper presents general efficient framework probabilistic inference learning
arbitrary uncertain information. exploits calculation properties finite mixture models, conjugate families factorization. joint probability density variables likelihood
function (objective subjective) observation approximated special mixture model,
way desired conditional distribution directly obtained without numerical integration. developed extended version expectation maximization (EM) algorithm
estimate parameters mixture models uncertain training examples (indirect observations).
consequence, piece exact uncertain information input output values consistently handled inference learning stages. ability, extremely useful certain situations, found alternative methods. proposed framework formally justified
standard probabilistic principles illustrative examples provided fields nonparametric
pattern classification, nonlinear regression pattern completion. Finally, experiments real application comparative results standard databases provide empirical evidence utility
method wide range applications.

1. Introduction
estimation unknown magnitudes available information, form sensor measurements subjective judgments, central problem many fields science engineering.
solve task, domain must accurately described model able support desired
range inferences. satisfactory models cannot derived first principles, approximations must obtained empirical data learning stage.
Consider domain Z composed collection objects z =(z1, z2, ..., zn), represented
vectors n attributes. Given partial knowledge (expressed general form explained
later) certain object z, interested computing good estimate z ( ) , close
true z. allow heterogeneous descriptions; attribute zi may continuous, discrete, symbolic valued, including mixed types. specific subset unknown uncertain attributes
estimated, attribute vector partitioned z = (x, y), z denotes target
output attributes. target attributes different different objects z. scenario includes several usual inference paradigms. instance, specific target symbolic
attribute, task called pattern recognition classification; target attribute continuous, inference task called regression function approximation. general, interested general framework pattern completion partially known objects.
Example 1: illustrate setting, assume preprocessor hypothetical computer
vision system obtains features segmented object. instances domain described

1998 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

following n=7 attributes: AREA: z1 , COLOR: z2 {white, black, red, ...}, DISTANCE:
z3 , SHAPE: z4 {circular, rectangular, triangular, ...}, TEXTURE: z5 {soft, rough, ...},
OBJECTTYPE: z6 {door, window, ...} ANGLE: z7 . typical instance may z = (78,
blue, 3.4, triangular, soft, window, 45). object partially occluded 3-dimensional,
attributes missing uncertain. instance, available information z
could expressed (748, blue black, 3.4, triangular, ?, window 70% door 30%, ?),
z1, z2, z6 uncertain, z3, z4 exact z5, z7 missing. case could interested estimates = {z5, z6, z7} even improving knowledge z1 z2.

non-deterministic nature many real world domains suggests probabilistic approach,
attributes considered random variables. Objects assumed drawn independently identically distributed p(z) = p(z1, ..., zn) = p(x, y), multivariate joint probability
density function attributes, completely characterizes n-dimensional random variable z. simplify notation, use function symbol p() denote different p.d.f.s
identified without risk confusion.
According Statistical Decision Theory (Berger 1985), optimum estimators desired attributes obtained minimization suitable expected loss function:

OPT ( ) = argmin E{ L( , )| S}


L(y, ) loss incurred true estimated . Estimators always features conditional posterior distribution p(y|S) target variables given available
information. instance, minimum squared error (MSE) estimator posterior mean,
minimum linear loss estimator posterior median minimum error probability (EP, 0-1
loss) estimator posterior mode.
Example 2: typical problem prediction unknown attribute observed attributes x. case available information written = (x, ?). continuous,
reasonable use MSE estimator: MSE ( ) = E{ | x} , general regression function.
symbolic loss associated errors, EP estimator adequate:
EP ( ) = argmaxy p(y|x) = argmaxy p(x|y)p(y). corresponds Maximum Posteriori rule
Bayes Test, widely used Statistical Pattern Recognition.

joint density p(z) = p(x, y) plays essential role inference process. implicitly
includes complete information attribute dependences. principle, desired conditional
distribution estimator computed joint density adequate integration. Probabilistic Inference process computing desired conditional probabilities (possibly
implicit) joint distribution. p(z) (the prior, model domain, comprising implications)
(a known event, somewhat related certain z), could obtain posterior p(z|S)
desired target marginal p(y|S) (the probabilistic consequent).
Example 3: observe exact value xo attribute x, i.e. = { x = xo}, have:

p( y| ) p(y| xo) =





p ( xo , )
p( xo , )dy

know instance z certain region R attribute space, i.e. = {z R},
compute marginal density joint p(z) = p(x, y) restricted region R (Fig. 1):

168

fiPROBABILISTIC INFERENCE UNCERTAIN DATA USING MIXTURES

p( y| ) =



X

p( x, y|{( x , ) R}) dx =

p( x, y) dx
p( x, y) dxdy
R

R

general types uncertain information z discussed later.


p(y|R)
R

p(x,y)
x

Figure 1. conditional probability density y, assuming z = (x,y) R.

summary, joint density p(z) multivariate random variable, subset variables z may be, principle, estimated given available information whole z = (x,
y). practical situations, two steps required solve inference problem. First, good model
true joint density p(z) must obtained. Second, available information must efficiently processed improve knowledge future, partially specified instances z. two
complementary aspects, learning inference, approached many scientific fields, providing different methodologies solve practical applications.
point view Computer Science, essential goal Inductive Inference
find approximate intensional definition (properties) unknown concept (subset domain) incomplete extensional definition (finite sample). Machine Learning techniques
(Michalski, Carbonell & Mitchell 1977, 1983, Hutchinson 1994) provide practical solutions (e.g.
automatic construction decision trees) solve many situations explicit programming
must avoided. Computational Learning Theory (Valiant 1993, Wolpert 1994, Vapnik 1995)
studies feasibility induction terms generalization ability resource requirements
different learning paradigms.
general setting Statistical Decision Theory, modeling techniques operational aspects inference (based numerical integration, Monte Carlo simulation, analytic approximations, etc.) extensively studied Bayesian perspective (Berger 1985, Bernardo
& Smith 1994). specific field Statistical Pattern Recognition (Duda & Hart 1973,
Fukunaga 1990), standard parametric nonparametric density approximation techniques (Izenman
1991) used learn training data class-conditional p.d.f.s required optimum
decision rule. instance, class-conditional densities p(x|y) Gaussian, required parameters mean vector covariance matrix feature vector class decision regions x quadratic boundaries. Among nonparametric classification techniques, Parzen method K-N Nearest Neighbors rule must mentioned. Analogously,
target attribute continuous statistical dependence input output variables
p(x,y) properly modeled joint normality, get multivariate linear regression: MSE(x) =
x + B, required parameters mean values covariance matrix attrib-

169

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

utes. Nonlinear regression curves derived nonparametric approximation techniques. Nonparametric methods present slower convergence rates, requiring significantly larger
sample sizes obtain satisfactory approximations; strongly affected dimensionality data selection smoothing parameter crucial step. contrast,
require kind smoothness assumption target density.
Neural Networks (Hertz et. al 1991) computational models trainable empirical data
proposed solve complex situations. intrinsic parallel architecture
especially efficient inference stage. One widely used neural models Multilayer Perceptron, universal function approximator (Hornik et al. 1989) breaks limitations
linear decision functions. Backpropagation learning algorithm (Rumelhart et al. 1986) can,
principle, adjust network weights implement arbitrary mappings, network outputs
show desirable probabilistic properties (Wan 1990, Rojas 1996). unsupervised networks probability density function approximation (Kohonen 1989). However, neural models
usually contain large number adjustable parameters, convenient generalization
and, frequently, long times required training relatively easy tasks. input / output role
attributes cannot changed runtime missing uncertain values poorly supported.
Bayesian Networks, based concept conditional independence, among
relevant probabilistic inference technologies (Pearl 1988, Heckerman & Wellman 1995). joint
density variables modeled directed graph explicitly represents dependence
statements. wide range inferences performed framework (Chang & Fung
1995, Lauritzen & Spiegelhalter 1988) significant results inductive learning
network structures (Bouckaert 1994, Cooper & Herskovits 1992, Valiveti & Oomen 1992).
approach adequate large number variables showing explicit dependences
simple cause-effect relations. Nevertheless, solving arbitrary queries NP-Complete, automatic
learning algorithms time consuming allowed dependences variables relatively simple.
attempt mitigate drawbacks, developed general efficient inference learning framework based following considerations. well known
(Titterington et al. 1985, McLachlan & Basford 1988, Dalal & Hall 1983, Bernardo & Smith 1994,
Xu & Jordan 1996) reasonable probability density function p(z) approximated
desired degree accuracy finite mixture simple components Ci, = 1..l:

p( z ) P{Ci } p( z| Ci )

(1)



superposition simple densities extensively used approximate arbitrary data dependences (Fig. 2). Maximum Likelihood estimators mixture parameters efficiently obtained samples Expectation Maximization (EM) algorithm (Dempster, Laird & Rubin
1977, Redner & Walker 1984) (see Section 4).

170

fiPROBABILISTIC INFERENCE UNCERTAIN DATA USING MIXTURES

(a)
(b)
(c)
Figure 2. Illustrative example density approximation using mixture model. (a) Samples p.d.f. p(x,y) showing nonlinear dependence. (b) Mixture model p(x,y)
6 gaussian components obtained standard EM algorithm. (c) Location
components.

decomposition probability distributions using mixtures frequently applied
unsupervised learning tasks, especially Cluster Analysis (McLachlan & Basford 1988, Duda &
Hart 1973, Fukunaga 1990): posteriori probabilities postulated category computed
examples, labeled according probable source density. However,
mixture models specially useful nonparametric supervised learning situations. instance,
class conditional densities required Statistical Pattern Recognition individually approximated (Priebe & Marchette 1991, Traven 1991) finite mixtures; hierarchical mixtures
linear models proposed (Jordan & Jacobs 1994, Peng et. al 1995); mixtures factor analyzers developed (Ghahramani & Hinton 1996, Hinton, Dayan, & Revow 1997)
mixture models useful feature selection (Pudil et al. 1995). Mixture modeling
growing semiparametric probabilistic learning methodology applications many research
areas (Weiss & Adelson 1995, Fan et al. 1996, Moghaddam & Pentland 1997).
paper introduces framework probabilistic inference learning arbitrary uncertain data: piece exact uncertain information input output values consistently handled inference learning stages. approximate joint density p(z)
(model domain) relative likelihood function p(S|z) (describing available information) specific mixture model factorized conjugate components, way numerical integration avoided computation desired estimator, marginal conditional
density.
advantages modeling arbitrary densities using mixtures natural conjugate components already shown (Dalal & Hall 1983), and, recently, inference procedures based
similar idea proposed (Ghahramani & Jordan 1994, Cohn et al. 1996, Peng et al. 1995,
Palm 1994). However, method efficiently handles uncertain data using explicit likelihood
functions, extensively used Machine Learning, Pattern Recognition
related areas. follow standard probabilistic principles, providing natural statistical validation procedures.
organization paper follows. Section 2 reviews elementary results
concepts used proposed framework. Section 3 addresses inference stage. Section 4
concerned learning, extending EM algorithm manage uncertain information. Section 5
discusses method relation alternative techniques presents experimental evaluation.
last section summarizes conclusions future directions work.

171

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

2. Preliminaries
2.1 Calculus Generalized Normals
many applications, instances domain represented simultaneously continuous
symbolic discrete variables (as Wilson & Martinez 1997). simplify notation,
denote probability impulses Gaussian densities means common formalism.
generalized normal
(x,,) denotes probability density function following properties:



T(x,,)

> 0,

( x ) 2
1
exp

2 22

T(x,,) = T(x,,0) T(x,) (x)

= 0,

Tzero,(x,,)
Gaussian density mean standard deviation 0. dispersion
reduces
Diracs delta function located . cases proper p.d.f.:
T(x,,) > 0
T(x,,) dx = 1
X

product generalized normals elegantly expressed (Papoulis 1991 pp. 258, Berger
1985) by:
1+2 >0:

T(x,1,1) T(x,2,2) = T(x,,) T(1,2, 12 + 22 )

(2)

mean dispersion new normal given by:

122 + 22 1
=
12 + 22

1222
= 2
1 + 22
2

relation useful computing integral product two generalized normals:
1+2 >0:



X

T(x,1,1) T(x,2,2) dx = T(1, 2, 12 + 22 )

(3)

And, consistency, define



1 = 2 = 0:

X

T(x,1) T(x,2) dx = T(1,2) I{1=2}

I{predicate} = 1 predicate true zero otherwise. Virtually reasonable univariate
probability distribution likelihood function accurately modeled appropriate mixture
generalized normals. particular, p.d.f.s symbolic variables mixtures impulses.
Without loss generality, symbols may arbitrarily mapped specific numbers represented
numeric axes. Integrals discrete domains become sums.
Example 4: Let us approximate p.d.f. p(x) mixed continuous symbolic valued random variable x mixture generalized normals. Assume x takes probability 0.4
exact value 10 (with special meaning), probability 0.6 random value continuously distributed following triangular shape shown Fig. 3. density p(x) accurately approximated (see Section 4) using 4 generalized normals:

T(x,10) + .21T(x,.04,.23) + .28T(x,.45,.28) + .11T(x,.99,.21)

p(x) .40

172

fiPROBABILISTIC INFERENCE UNCERTAIN DATA USING MIXTURES

Figure 3. p.d.f. mixed random variable approximated mixture generalized normals.

2.2 Modeling Uncertainty: Likelihood Principle
Assume value random variable z must inferred certain observation subjective information S. z drawn p(z) measurement judgment process
characterized conditional p(S|z), knowledge z updated according p(z|S)=p(z) p(S|z)
/ p(S), p(S) = Z p(S|z) p(z) dz (see Fig. 4).
likelihood function fS(z) p(S|z) probability density ascribed possible
z. arbitrary nonnegative function z interpreted two alternative ways.
objective conditional distribution p(S|z) physical measurement process (e.g.
model sensor noise, specifying bias variance observable every possible true
value z), known error model. subjective judgment chance
different z values (e.g. intervals, likely regions, etc.), based vague difficult formalize
information. dispersion fS(z) directly related uncertainty associated measurement process. Following likelihood principle (Berger 1985), explicitly assume
experimental information required perform probabilistic inference contained likelihood
function fS(z).

p(z)
z1

z2

z3

z

p(s|z2)

p(s|z1)

p(s|z3)




prior
model
measurement

p(s)

fSo(z)= p(so|z)

z

observable

likelihood
observation

p(z|so)
posterior
z
Figure 4. Illustration elementary Bayesian univariate inference process.

173

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

2.3 Inference Using Mixtures Conjugate Densities
computation p(z|S) may hard, unless p(z) p(S|z) belong special (conjugate) families (Berger 1985, Bernardo & Smith 1994). case posterior density analytically
obtained parameters prior likelihood, avoiding numeric integration.
prior, likelihood posterior mathematical family. belief structure
closed inference process.



Example 5: univariate case, assume z known normally distributed around r
dispersion r, i.e. p(z) = (z, r, r). Assume measurement device Gaussian noise, observed values distributed according p(s|z) =
(s, z, s). Therefore,
observe certain value so, property product generalized normals eq.
(2), posterior knowledge z becomes another normal
(z, , ). new expected location z expressed weighted average r so: = + (1-)r uncertainty reduced 2 = S2 . coefficient = 2r / ( 2r + S2 ) quantifies relative im-





portance experiment respect prior.

computational advantage extended general case using mixtures conjugate families (Dalal & Hall 1983) approximate desired joint probability distribution
likelihood function.
Example 6: domain likelihood modeled respectively

p(z)




Pi

T(z, , )


p(so|z)





r

r

T(z, , )
r

r

(where r , r r depend explicitly observed so), posterior
written following mixture:



p(z|so)

i, r

properties (2) (3), parameters

, r

,r

T(z,

, ,r )

(4)

,r ,r weights ,r given by:

i2 r + 2r
=
i2 + r2

,r

,r

,r =

r
i2 + r2

Pi r T( , r , i2 + r2 )

P
k

l

T( k , l , 2k + 2l )

k ,l

2.4 Role Factorization
Given multivariate observation z partitioned two subvectors, z = (x, y), assume
interested inferring value unknown attributes observed attributes x. Note
x statistically independent, joint density factorizable: p(z) = p(x, y) = p(x)
p(y) and, therefore, posterior p(y|x) equals prior marginal p(y). observed x carries
predictive information optimum estimators depend x. instance,
MSE ( x ) = E{y|x} = E{y} EP ( x ) = argmax p( ) . simplest estimation task.
runtime computations required optimum solution, may precalculated.

174

fiPROBABILISTIC INFERENCE UNCERTAIN DATA USING MIXTURES

realistic situations variables statistically dependent. general, joint density cannot factorized required marginal densities may hard compute. However, interesting
consequences arise joint density expressed finite mixture factorized (with independent variables) components C1, C2, ..., Cl :

p(z) = p( z 1 ,..., z n ) = P{Ci } p( z| Ci ) = P{Ci } p( z j | Ci )




(5)

j

structure convenient inference purposes. particular, terms desired partition z = (x, y):

p(z) = p(x, y) =

P{C } p( x| C ) p( y| C )








marginal densities mixtures marginal components:

p( x ) = p( x , )dy = P{Ci } p( x| Ci )




p( ) = P{Ci } p( y| Ci )


desired conditional densities mixtures marginal components:

p( y| x ) = ( x ) p( y| Ci )

(6)



weights (x) probabilities observed x generated component Ci :

( x) =

P{Ci } p( x|Ci )

P{C } p( x|C )
j

= P{Ci | x}

j

j

p.d.f. approximation capabilities mixture models factorized components remain
unchanged, cost possibly higher number components obtain desired degree
accuracy, avoiding artifacts (see Fig. 5). Section 5.2 discusses implications factorization
relation alternative model structures.

(a)
(b)
Figure 5. (a) Density approximation data Fig. 2, using mixture 8 factorized components. (b) Location components. Note arbitrary dependence
represented mixture components independent variables (observe
somewhat smoother solution could obtained increasing number components).

175

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

3. MFGN Framework
previous concepts integrated general probabilistic inference framework call
MFGN (Mixtures Factorized Generalized Normals). Fig. 6 shows abstract dependence relations among attributes generic domain (upper section figure) attributes
observed information (lower section). MFGN framework, relations modeled
finite mixtures products generalized normals. key idea using factorization cope
multivariate domains heterogeneous attribute vectors, conjugate densities efficiently perform inferences given arbitrary uncertain information. section, derive
main inference expressions. learning stage described Section 4.

p(z)
z1

zn
z2

model
domain
p(z)

zj
model
measurement
p(S|z)



Figure 6. Generic dependences inference process.

3.1 Modeling Attribute Dependences Domain
MFGN framework attribute dependencies domain modeled joint density
form finite mixture factored components, expression (5), component
marginals p( z j | Ci ) T( z j , ij , ij ) generalized normals:

p( z ) = Pi


T( z

j

, ij , ij )

i=1..l, j=1..n,

(7)

j

desired, terms associated pure symbolic attributes z j (with ij = 0)
collected way component marginals expressed mixtures impulses:

p( z j | Ci ) ti j, T( z j , )

(8)



ti j, P{z j = | Ci } probability z j takes -th value component Ci .
manipulation reduces number l global components mixture. adjustable parameters
model proportions Pi = P{Ci } mean value ij dispersion ij j-th
j
attribute i-th component (or, symbolic attributes, probabilities ti , ).

structure (8) explicitly used symbolic attributes applications illustrative examples, mathematical derivations made concise expression (7).

176

fiPROBABILISTIC INFERENCE UNCERTAIN DATA USING MIXTURES

variables continuous, MFGN architecture reduces mixture gaussians
diagonal covariance matrices. proposed factorized structure extends properties
diagonal covariance matrices heterogeneous attribute vectors. interested joint models,
support inferences partial information subset variables. Note
easy way define measure statistical depencence symbolic continuous
attributes, used parameter probability density function1. required "hetereogeneous" dependence model conveniently captured superposition simple factorized
(with independent variables) densities.
Example 7: Figure 7 shows illustrative 3-attribute data set (x continuous z
symbolic) components MFGN approximation obtained EM algorithm
(see Section 4) joint density. parameters mixture shown Table 1.
Note that, overlapped structure data, components (5 6) assigned values symbolic attribute z.

(a)
(b)
Figure 7. (a) Simple data set two continuous one symbolic attribute.
(b) Location mixture components.



Pi

ix

ix

iy

iy

tiz,white

1
.14
-.40
.24
-.27
.20
0
2
.09
-.76
.19
-.68
.18
0
3
.20
.55
.23
.66
.24
0
4
.17
-.71
.27
.76
.22
1
5
.13
.21
.17
-.14
.19
.74
6
.18
-.14
.18
.26
.17
.55
7
.09
.65
.16
-.64
.19
1
Table 1. Parameters Mixture Model Data Set Fig. 7.

tiz,black
1
1
1
0
.26
.45
0

3.2 Modeling Arbitrary Information Instances
available information particular instance z denoted S. Following likelihood
principle, concerned true nature S, whether kind physical meas1

reason, pattern classification tasks separate models typically built class-conditional density.

177

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

urement subjective judgment location z attribute space. need
update knowledge z, form posterior p(z|S), relative likelihood function
p(S|z) observed S. general, p(S|z) nonnegative multivariable function fS(z)
domain. objective case, statistical studies measurement process used
determine likelihood function. subjective case, may obtained standard distribution elicitation techniques (Berger 1985). either case, MFGN framework, likelihood function available information used inference process approximated, desired degree accuracy, sum products generalized normals:

p( S| z ) = P{S | r } p( r | z ) = P{S | r } p( srj | z j )
r

r

=

T( z
r

j

j

, srj , rj )

(9)

j

r

Without loss generality, available knowledge structured weighted disjunction =
1
2
{1s1 2s2 ... R R } conjunctions sr = { sr sr ... srn ) elementary uncertain
observations form generalized normal likelihoods T( z j , srj , rj ) centered srj
uncertainty rj . measurement process interpreted result R (objective subjective) sensors sr , providing conditionally independent information p( srj | z j ) attributes
(each srj depends z j ) relative strength r . Note complex uncertain information instance z, expressed nested combination elementary uncertain beliefs srj
z j using probabilistic connectives, ultimately expressed structure (9) (OR translates addition, translates product product two generalized normals
attribute becomes single, weighted normal).
Example 8: Consider hypothetical computer vision domain Example 1. Assume
information object z following: AREA around DISTANCE around b or,
likely, SHAPE surely triangular else circular AREA around c ANGLE
around equal e. structured piece information formalized as:

T(z , a, ) T(z , b, )]
+ .7 [ (.9T(z ,triang)+.1T(z ,circ)) T(z , c, ) (T(z , d, )+T(z ,e)) ]

p(S|z) = .3 [

1

3



4

b

4

1

7

c

7



which, expanded, becomes mixture 5 factorized components operationally represented
parameters shown Table 2.
simpler situation, available information z could conjunction uncertain attributes similar {Color = red 0.8 green 0.2} {Area = 3 .5} {Shape = rectangular 0.6
circular 0.3 triangular 0.1}. likelihood Shape values obtained output
simple pattern classifier (e.g. K-N-nearest neighbors) moment invariants, attributes
Color Area directly extracted image. case could interested
distribution values attributes Texture ObjectType. Alternatively,
could start {ObjectType = door 0.6 window 0.4} {Texture = rough} order determine probabilities Color Angle values selecting promising search region.

178

fiPROBABILISTIC INFERENCE UNCERTAIN DATA USING MIXTURES

r

sr1 , 1r

sr3 , r3

sr2 , 2r

sr4 , 4r

sr5 , 5r

sr6 , 6r

sr7 , 7r

.30
a,
-,
b, b
-,
-,
-,
.63
triang, 0
c, c
-,
-,
-,
-,
.63
triang, 0
c, c
-,
-,
-,
-,
.07
circ, 0
c, c
-,
-,
-,
-,
.07
circ
,
0
c, c
-,
-,
-,
-,
Table 2. Parameters Uncertain Information Model Example 8.

-,
d,d
e, 0
d,d
e, 0

3.3 Joint Model-Observation Density
generic dependence structure Fig. 6 implemented MFGN framework shown
Fig. 8. upper section figure model nature, obtained previous learning stage
used inference without changes. Dependences among attributes conducted
intermediary hidden latent component Ci. lower section represents available
uncertain information, measurement model query structure associated particular inference operation.

Ci

p( z 1 | Ci )
1

Domain:

p( z) P{Ci } p( z j | Ci )
2

z

z

s21

s22

...

zn

...

s2n

j



p( s11 | z1 )
s11

s12

...

s1n

s1

...

s1R

sR2

...

sRn

sR

s2
P{S|s1}

Measurement:
p( S| z ) P{S | r } p( srj | z j )



r

j

Figure 8. Structure MFGN model. attributes conditionally independent.
measurement process modeled collection independent virtual sensors
p( srj | z j ) .

joint density relevant variables becomes:

p(Ci , z , sr , ) = P{S | r } p( r | z ) p( z| Ci ) P{Ci }

p(

= P{Ci } P{S | r }

j
r

| z j ) p( z j | Ci )

j

= Pi r

T( z

j

, srj , rj ) T( z j , ij , ij )

j

179

(10)

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

derive alternative expression eq. (10) convenient computing marginal densities desired variable. Using following relation:

p( srj | z j ) p( z j | Ci ) = p( z j , srj | Ci ) = p( z j | srj , Ci ) p( srj | Ci )
properties (2) (3), define dual densities model:

ij,r p( srj | Ci ) =



Zj

p( srj | z j ) p( z j | Ci ) dz j =T( srj , ij , ij,r )

(11)

p( srj | z j )
p( z j | Ci ) = T( z j , ij,r , ji ,r )
p( srj | Ci )

(12)

ij,r ( z j ) p( z j | srj , Ci ) =

parameters ij,r , ij,r ji ,r given by:

ij,r ( ij ) 2 + ( rj ) 2
( ij ) 2 srj + ( rj ) 2 ij

(ij,r ) 2
j
,r



j
,r

ij rj
j
,r

ij,r likelihood r-th elementary observation srj j-th attribute z j
component Ci ij,r ( z j ) effect r-th elementary information srj j-th attribute z j marginal component p( z j | Ci ) component Ci . Using notation, MFGN model structure conveniently written as:

p(Ci , z , sr , ) = Pi r ij,r ij,r ( z j )

(13)

j

3.4 Posterior Density
inference process available information combined model domain
update knowledge particular object. Given new piece information must compute posterior distribution p( y| ) desired target attributes z. Then, estimators
( ) obtained p( y| ) minimize suitable average loss function.
efficiently supported MFGN framework regardless complexity domain p(z)
structure available information = { r sr } .
attributes partitioned two subvectors z = (x, y), = { z } desired
target attributes x = { z } rest attributes. Accordingly, component sr
available information partitioned r = ( rx , ry ) . information target attributes
r-th observation, independent model p(z), denoted sry (often missing
pieces information) srx represents information rest
attributes x. Using convention write:

180

fiPROBABILISTIC INFERENCE UNCERTAIN DATA USING MIXTURES

p( z , ) = p( x , , ) = Pi r ,r ,r ( x ) ,r ( )
,r

,r likelihood r-th conjunction sr component Ci :

,r ij,r

(14)

j

terms ij,r ( z j ) grouped according partition z = (x, y):

,r ( x ) io,r ( z )

,r ( ) id,r ( z )





desired posterior p(y|S) = p(y,S) / p(S) computed joint p(z,S) marginalization: along x obtain p(y,S) along z obtain p(S). Note univariate marginalization p(z,S) along attribute z j eliminates terms ij,r ( z j ) sum (13):

p( , ) = p( x , , ) dx = Pi r ,r ,r ( )
X

,r

p( ) = p( z , ) dz = Pi r ,r
Z

,r

Therefore, posterior density compactly written as:

p( y| ) = ,r ,r ( )

(15)

,r

,r probability object z generated component Ci elementary information sr true, given total information S:

,r P{Ci , sr | S} =

Pi r ,r

P

k

l k ,l

(16)

k ,l

,r ( ) = p( y| sry , Ci ) marginal density p( y| Ci ) desired attributes i-th
component, modified contribution associated sry . Since p( y| sry , Ci ) =
p( y| sr , Ci ) , expression (16) follows expansion:

p( y| ) = p( y| sr , Ci ) P{Ci , sr | S}
,r

summary, joint density likelihood function approximated mixture
models proposed structure, computation conditional densities given events arbitrary geometry notably simplified. Factorized components reduce multidimensional integration
simple combination univariate integrals conjugate families avoid numeric integration.
property illustrated Fig. 9.

181

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO





p(y|C2)

1y

E{y|S}

C2

p(y,x1,x2)

2y

C1

p(y|C1)
p(x2|C2)

p(y|S)
p(x2|C1)

22,1 22,2
1

p(x |C2)

21,1

x2

p(x1|C1)

11,2

11,1

s1
s2

x

1



Figure 9. Graphical illustration essential property MFGN framework. Consider MSE estimate y, conditioned event (y, x1, x2) cylindrical
region S. required multidimensional integrations computed analytically terms
marginal likelihoods ji,r associated attribute pair components
Ci sr models p(y, x1, x2) S, respectively. case i,r(y)=p(y|Ci)
information supplied S.

Example 9: Fig. 10.a shows joint density two continuous variables x y. modeled
mixture 30 factorized generalized normals. Fig. 10.b shows likelihood function
event S1 = {(x x -y) y>0}. Fig. 10.c shows posterior joint density
p(x,y|S1). Fig. 10.d shows likelihood function event S2 = {(x,y) (0,0) x3}.
Fig. 10.e shows posterior joint density p(x,y|S2). Fig. 10.f 10.g show respectively
posterior marginal density p(x|S2) p(y|S2). complex inferences analytically computed MFGN framework, without numeric integration.

182

fiPROBABILISTIC INFERENCE UNCERTAIN DATA USING MIXTURES

(a)

(b)

(c)

(d)
(e)
Figure 10. Illustrative examples probabilistic inference arbitrary uncertain information MFGN framework (see Example 9).

183

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

p(y|S)

0.4

0.8

0.3

0.6

0.2

0.4

0.1

p(x|S)

0.2



x

-4

-2

2

4

-4

-2

2

(f)
Figure 10. (cont.).

4

(g)

3.5 Expressions Estimators
Approximations optimum estimators easily obtained taking advantage
mathematically convenient structure posterior density. MFGN framework,
conditional expected value function g(y) becomes linear combination constants:

E{g ( )| S} = g ( ) p( y| ) dy =


=


,r



,r

g ( ) ,r ( ) dy =



,r

E ,r {g ( )}

(17)

,r

E ,r {g ( )} E{g ( )| ry , Ci } expected value g(y) i-th component2 modified3 r-th observation sry :

E ,r {g ( )}





g ( ) T( z , id,r , di ,r ) dy


analytically compute desired optimum estimators. instance, MSE estimator single continuous attribute = z requires mean values E ,r {z } = id,r :

MSE ( ) = E{ y| S} = ,r id,r
,r

explicit expression p(y|S) compute conditional cost:

{

}

e 2MSE ( ) = E ( MSE ( ) ) | = E{y 2 | S} 2MSE ( ) =
=


,r

,r

2

[(


,r

) + ( )
2


,r

2

2

]



,r id,r
,r


2

Note computing conditional expected value arbitrary function g(y) several variables may difficult.
general g(y) expanded power series obtain E{g(y)|S} terms moments p(y|S).
3
sx (there information target attributes) constants Ei,r{g(y)} precomputed
model nature p(z) learning stage.

184

fiPROBABILISTIC INFERENCE UNCERTAIN DATA USING MIXTURES

Therefore, given S, Tchevichev inequality answer MSE ( ) 2e MSE ( )
confidence level 75%. shape p(y|S) complex must reported explicitly (the point estimator MSE ( ) makes sense p(y|S) unimodal).
Example 10: Nonlinear regression. Fig. 11 shows mixture components regression
lines (with confidence band two standard deviations) obtained simple example
nonlinear dependence two variables. case joint density adequately
approximated 3 4 components: MSE (1 component) = 0.532, MSE (2 comp.) = 0.449,
MSE (3 comp.) = 0.382, MSE (4 comp.) = 0.381.

(a)
(b)
Figure 11. Nonlinear regression example: (a) 2 components, (b) 4 components.

target symbolic must compute posterior probability value.
case di ,r = 0 id,r = id possible values taken = z . Collecting together id,r = , (8), eq. (15) written as:

p ( y| ) = ,r , ( , )


,r

,r , coefficients impulses located . posterior probability
value is:

q P{ = | S} = ,r ,
,r

instance, minimum error probability estimator (EP) is:

EP ( ) = argmax



q

desired rejection threshold easily established. reject decision entropy posterior, H = q log q, estimated error probability, E = 1- max q,
high.
Example 11: Nonparametric Pattern Recognition. Fig. 12 shows bivariate data set elements two different categories, represented value additional symbolic attribute. joint density satisfactorily approximated 6-component mixture (Fig.
12.a). decision regions rejection threshold set 0.9 shown Fig. 12.b.

185

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

Note Statistical Pattern Classification usually start (implicit explicit) approximation class-conditional densities. contrast, start joint density,
class-conditional densities easily derived (Fig. 12.c).

(a)
(b)
(c)
Figure 12. Simple nonparametric 2feature pattern recognition task 3attribute
joint mixture model: (a) Feature space mixture components. (b) Decision boundary.
(c) One class-conditional densities.

computation optimum estimators loss functions straightforward. Observe estimators based combination different rules, weighted degree
applicability. typical structure used many decision methods. case, since
components joint density independent variables rules reduce constants,
simplest type rule.
3.6 Examples Elementary Pieces Information
important types elementary observations srj z j shown, including corresponding likelihoods ij,r modified marginals ij,r ( z j ) (j=d) required expression (15).
Exact information: srj = z j . observation modeled impulse:

p( srj | z j ) =

T( srj , z j ) = ( srj z j ) . Therefore:
ij,r = T( srj , ij , ij )
ij,r ( z j ) = T( z j , srj )
contribution ij,r exact information input attribute z j standard likelihood p( z j | Ci ) observed value z j component. hand, acquire
exact information target attribute z j (when one (R=1) elementary observation
j = z j ) inference process trivially required: p( z j | ) = ( z j j ) .
Gaussian noise bias rj standard deviation rj : observation modeled 1component mixture: p( srj | z j ) = T( srj , z j + rj , rj ) , expressed 95% confidence interval z j srj + rj 2 rj . property (2-2):

186

fiPROBABILISTIC INFERENCE UNCERTAIN DATA USING MIXTURES

ij,r = T( srj , ij rj , ( ij ) 2 + ( rj ) 2 )
effect noisy input z j srj 2 rj equivalent effect exact input z j = srj
mixture components larger variance: ij ( ij ) 2 + ( rj ) 2 . Uncertainty spreads
effect observation, increasing contribution distant components.
Example 12: Fig. 13.a shows simple two-attribute domain approximated 3-component
mixture. interested marginal density attribute x given different degrees uncertainty input attribute .4 2, modeled

p( | ) = T( ,.4, ) . = 0

sharpest density (A) Fig. 13.b, providing x.4.5. = .25 obtain density
(B) x.3.7. Finally, = .5 obtain density (C) x.2.8. Obviously, uncertainty increases, uncertainty x. expected value x moves towards
distant components, become likely probability distribution expands. situation interesting effect appears: mode marginal density
change rate mean. Uncertainty skews p(x). effect suggests
optimum estimators different loss functions equally robust uncertainty.


B
C

(a)
(b)
Figure 13. Effect amount uncertainty (see text). (a) Data set 3-component
model. (b) p(x | uncertain ys around 0.4).

j
j
output role, ,r ( z ) becomes original marginal, modified location disper2
2
2
sion towards srj according factor = ( ij ) / [( ij ) + ( rj ) ] , quantifies relative
importance observation:

ij,r ( z j ) = T(z j , ( srj rj ) + (1 ) ij , 1/ 2 rj )
Missing data. information j-th attribute, srj = {z j = ?} , observation modeled p( srj | z j ) = constant or, equivalently, p( srj | z j ) = T( srj , , b)
arbitrary b . components contribute weight:

ij,r = p( z j = anything| Ci ) constant 1
target missing ij,r ( z j ) reduce original marginal components:

187

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

ij,r ( z j ) = T( z j , ij , ij ) = p( z j | Ci )
Arbitrary uncertainty. general, unidimensional relative likelihood function approximated mixture generalized normals, shown Example 6, ij,r ij,r ( z j )
given respectively eqs. (11) (12).
Intervals. useful functions cannot accurately approximated small number normal
components. typical example indicator function interval, used model uncertain
observation values equally likely: srj = {z j (a, b)} . z j considered
input, use shortcut ij,r = j (b) j (a ) , j ( z j ) cumulative distribution normal marginal component p( z j | Ci ) . Unfortunately, expression ij,r ( z j ) ,
required z j considered output, may useful computing certain optimum estimaj
j
j
tors. ,r ( z ) restriction p(z j|Ci) interval (a,b) normalized ,r .
Disjunction conjunction events. Finally, standard probability rules used build
structured information simple observations: subjective judgments objective evidence ascribe relative degrees credibility rj several observations srj z j , overall
j
j j
likelihood becomes = r r ,r . particular, j = {z j = 1 z j = 2 } two

possibilities equiprobable ij = p( 1 | Ci ) + p( 2 | Ci ) . Analogously, conjunctions
events translate multiplication likelihood functions.
3.7 Summary Inference Procedure
domain p(z) adequately modeled learning process (as explained Section
4), system enters inference stage new, partially specified objects. parameters
domain p(z) ( Pi , ij ij ) parameters model observation p(S|z)
( r , srj rj ), must obtain parameters ij,r , id,r di ,r desired marginal posterior densities estimators. inference procedure comprises following steps:


Compute elementary likelihoods ij,r , using eq. (11).



Obtain product ,r conjunction sr component Ci , using eq. (14).



Normalize Pi r ,r obtain coefficients ,r posterior, using eq. (16).



Choose desired target attributes = { z } compute parameters id,r ,

di ,r modified component marginal densities id,r ( z ) using eq. (12).


Report joint posterior density y. Show graphs posterior marginal densities
desired attributes z using eq. (15). Provide optimum (point, interval, etc.) estimators using eq. (17).

Example 13: Iris Data. inference procedure illustrated well known Iris benchmark: 150 objects represented four numeric features (x, y, z w) one symbolic category U {U1 (setosa), U2 (versicolor), U3 (virginica)}. whole data set divided
two disjoints subsets training validation. joint density satisfactorily approxi-

188

fiPROBABILISTIC INFERENCE UNCERTAIN DATA USING MIXTURES

mated (see Section 4) 6component mixture (the error rate classifying U validation
set without rejection 2.67%). Fig. 14 shows two projections 150 examples location mixture components learned training subset. parameters mixture shown Table 3.

(a)
(b)
Figure 14. Two views Iris examples components joint density mixture model. U1: white, U2: black, U3: gray. (a) Attributes x, (b) Attributes z, w.



Pi

1
2
3
4
5
6

0.15
0.13
0.21
0.18
0.15
0.17

ix

ix

iy

iy

iz

iz

7.13 0.48 3.12 0.34 6.17 0.45
5.48 0.41 2.50 0.28 3.87 0.32
6.29 0.39 2.93 0.27 4.59 0.20
4.75 0.23 3.25 0.23 1.42 0.21
5.36 0.26 3.76 0.29 1.51 0.16
6.16 0.42 2.77 0.28 5.22 0.30
Table 3. Parameters Iris Data Joint Density Model

iw

iw

2.18
1.20
1.45
0.19
0.32
1.94

0.20
0.21
0.14
0.05
0.10
0.23

P{U1|Ci} P{U2|Ci} P{U3|Ci}

0
0
0
1
1
0

0
0.93
1
0
0
0.00

Table 4 shows results inference process following illustrative situations:
Case 1: Attribute z known: = {z = 5}.
Case 2: Attributes x U known: = {(x = 5.5) (U=U2)}.
Case 3: Attribute x uncertain: = {x 71}.
Case 4: Attributes x w uncertain: = {(x 71) (w 10.5)}. Note uncertainty decreases information supplied (compare Case 3).
Case 5: Structured query expressed terms logical connectives uncertain elementary
events: = {[(z 13) (z 73)] [(U = U1) (U = U2)]}.

189

1
0.07
0.00
0
0
1

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

CASE
1
2
3
4
5

INPUT
OUTPUT
INPUT
OUTPUT
INPUT
OUTPUT
INPUT
OUTPUT
INPUT


OUTPUT

X
?
6.20.9
5.5
5.5
71
6.70.9
71
6.50.7
?
?


?
2.80.6
?
2.60.6
?

z
5.0
5.0
?
4.00.8
?

w
?
1.80.6
?
1.30.4
?

3.00.7
?

5.31.8
?

2.90.6
?
?

1.80.8
10.5
1.30.3
?
?

5.31.2

3.30.9

4.50.8
13
73
23

(approx.
unimodal)

(unimodal)

(bimodal)

0.51

U
?

U2: 22% U3: 78%
U2: 100%
U2: 100%
?
U2: 36% U3: 63%
?
U2: 95% U3: 5%
U1: 50% U2: 50%
U1: 50% U2: 50%
U1: 75% U2: 25%

(bimodal)

Table 4. Inference Results IRIS Domain

consistency results visually checked Fig. 14. Finally, Table 5 shows
elementary likelihoods


1
2
3
4
5
6

ix,1

iy,1

i,j r Case 5, illustrating essence method.

iz,1

iw,1

Ui ,1

,1

ix,2

1
1
1
0
1
.001
0
1
1
1
1
.045
.47 .02
1
1
1
1
.016
.50 .01
1
1
1
1
.254
.50 .13
1
1
1
1
.250
.50 .13
1
1
1
0
1
.006
0
Table 5. Elementary likelihoods Case 5 Table 4.

iy,2
1
1
1
1
1
1

iz,2

iw,2

Ui ,2

,2

.221
.032
.074
3E-4
4E-4
.132

1
1
1
1
1
1

0
.47
.50
.50
.50
0

0
.02
.04
.00
.00
0

3.8 Independent Measurements
One key features MFGN framework ability infer arbitrary relational
knowledge attributes, form likelihood function adequately approximated
mixture model structure eq. (9). instance, could answer questions as: happens z z tends less z j ? (i.e., p(S|z) high region z z j < 0 ).
However, situations observations single attribute z j statistically
independent: information attributes (e.g. z around z j around b)
attribute relations. pay attention particular case illustrates role
main MFGN framework elements. Furthermore, many practical applications satisfactorily solved assumption independent measurements judgments. case,
likelihood available information expressed conjunction n marginal observations j z j :

p ( | z ) = p( j | z j )
j

190

(18)

fiPROBABILISTIC INFERENCE UNCERTAIN DATA USING MIXTURES

means sum products equation (9) complete, i.e., includes elements
N-fold cartesian product attributes:

p( | z ) = rj T( z j , srj , rj )
j

r

j rj = r . factored likelihood function considered 1-component
mixture (with R=1 (9) j s1j ) marginal observation models allowed
mixtures generalized normals: p( j | z j ) = r ' rj' T( z j , srj' , rj ' ) . case even think
1
1
function valued attributes z [ f ( z ),..., f n ( z n )] , f j ( z j ) p( j | z j ) models

range relative likelihood values z j . Loosely speaking, attributes concentrated
f j ( z j ) may considered inputs, attributes high dispersion play role outputs.
Since conditionally independent x given Ci , posterior obtained expansion:

p( y| ) = p( y| , Ci ) P{Ci | S} = p( y| Ci , ) P{Ci | S}


(19)



interpretation (19) straightforward. effect sx = {zd} must computed
x = {zo} components Ci . Then, simple Bayesian update p( y| x ) new
prior made using (see Fig. 15).

Ci
z1



p( z j | Ci )

zd
id ( z )

s1

...


...

sd

j


zj
p( j | z j )

sj

...

Figure 15. Structure MFGN inference process independent pieces information. case, likelihood function factorizable. data flow inference process shown dotted arrows.

191

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

4. Learning Uncertain Information
previous section, described inference process uncertain information
MFGN framework. develop learning algorithm model domain,
training examples uncertain. Specifically, must find parameters Pi ,

ij , ij (or ti j, ) mixture structure (7) approximate true joint density p(z)
training i.i.d. random sample {z(k)}, k=1..M, partially known associated likelihood
functions {S(k)} structure (9).
4.1 Overview EM Algorithm
Maximum Likelihood estimates parameters mixture models usually computed
well-known Expectation-Maximization (EM) algorithm (Dempster, Laird Rubin 1997, Redner
Walker 1984, Tanner 1996), based following idea. principle, maximization
training sample likelihood J = k p( z ( k ) ) mathematically complex task due product
sums structure. However, note J could conveniently expressed maximization components generated example known (this called complete data EM terminology). underlying credit assignment problem disappears estimation task reduces several uncoupled simple maximizations. key idea EM following: instead maximizing
complete data likelihood (which unknown), iteratively maximize expected value
given training sample current mixture parameters. shown process
eventually achieves local maximum J.
Instead rigorous derivation EM algorithm, found references (see especially McLachlan Krishnan, 1997), present heuristic justification
provides insight generalizing EM algorithm accept uncertain examples. review
first simplest case, missing uncertain values allowed training set.
parameters mixture conditional expectations:

E z |Ci {g ( z )| Ci )} = g ( z ) p( z| Ci ) dz
Z

(20)

2
2
particular, ij = E{z j | Ci } , ( ij ) = E{( z j ij ) | Ci } ti j, = E{I {z j = }| Ci } .

mixture proportions Pi = E{ P{Ci | z} } .
rewrite conditional expectation (20) using Bayes Theorem form unconditional expectation:

E z |Ci {g ( z )| Ci )} = g ( z ) P{Ci | z} p( z ) / P{Ci } dz =

(21)

= E z {g ( z ) P{Ci | z}} / Pi

(22)

Z

EM algorithm interpreted method iteratively update mixture parameters
using expression (22) form empirical average training data4. Starting
tentative, randomly chosen set parameters, following E steps repeated
4

Expression (21) used iterative approximation explicit functions indirectly known
i.i.d. sampling (e.g., subjective likelihood functions sketched human user, Example 4). case p(z) set
target function P{Ci| z} computed current mixture model.

192

fiPROBABILISTIC INFERENCE UNCERTAIN DATA USING MIXTURES

total likelihood J longer improves (the notation (expression)(k) means (expression) computed parameters example z(k)):
( )
( )
(E) Expectation step. Compute probabilities qi k P{Ci | z k } k-th example
generated i-th component mixture:

qi( k ) p( z ( k ) | Ci ) P{Ci } / p( z ( k ) )
(M) Maximization step. Update parameters component using examples, weighted
probabilities qi( k ) . First, priori probabilities component:

Pi

1
q (k )
k

Then, continuous variables, mean values standard deviations component:

1
MPi

ij
( ij ) 2

1
MPi

[q

z j ]( k )



k

[q

( z j ) 2 ]( k ) ( ij ) 2



(23)

k

symbolic variables, probabilities value:

ti j,

1
MPi

[q

{z j = }]( k )



k

4.2 Extension Uncertain Values
general, MFGN framework know true values z j attributes
training examples, required compute g ( z ) P{Ci | z} (empirical) expectation (22). Instead,
start uncertain observations ( k ) true training examples z
likelihood functions expressed mixtures generalized normals:

(k )

, form

p( ( k ) z ( k ) ) = P{S ( k ) sr( k ) } p( sr( k ) z ( k ) )
r

Therefore, must express expectation (22) p(z) unconditional expectation
p(S), distribution generates available information training set.
easily done expanding p( z| Ci ) terms S:

E z |Ci {g ( z )| Ci )} = g ( z ) p( z| Ci ) dz
Z

[ p ( z | , C ) p ( | C ) dS ] dz
= [ g ( z ) p( z| , C ) dz ] P{C | S} p( ) dS / P{C }
=



Z



Z

g ( z)











define

193



(24)

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

( ) E z |S ,Ci {g ( z )| , Ci )} = g ( z ) p( z| Ci ) p( | z ) dz / p( | Ci )
Z

parameters p(z) finally written5 unconditional expectation observable p(S) form similar eq. (22):

E z |Ci {g ( z )| Ci )} = E {i ( ) P{Ci | S}} / Pi

(25)

expression justifies extended form EM algorithm iteratively update parameters p(z) averaging ( ) P{Ci | S} available training information { ( k ) }
drawn p(S). considered numerical/statistical method solving p(z)
integral equation:



Z

p ( | z ) p ( z ) dz = p ( )

Note cannot approximate p(S) fixed mixture terms p( | Ci ) computing back corresponding p( z| Ci ) because, general, p( | z ) different different training examples. reason, elementary deconvolution methods directly
applicable.
kind problem addressed Vapnik (1982, 1995) perform inference result
indirect measurements. ill-posed problem, requiring regularization techniques.
proposed extended EM algorithm considered method empirical regularization,
solution restricted family mixtures (generalized) gaussians. EM
proposed Kaveh (1996) regularization context image restoration.
interpretation (25) straightforward. Since know exact z required
approximate parameters p(z) empirically averaging g ( z ) P{Ci | z} , obtain
result averaging corresponding ( ) P{Ci | S} domain, ( ) plays
role g(z) (22). z uncertain, g(z) replaced expected value component
given information S. particular, exact knowledge training set at( )
tributes ( ( k ) = z k , i.e., R = 1 marginal likelihoods impulses) (25) reduces
(22). Fig. 16. illustrates approximation process performed extended version EM
algorithm simple univariate situation.
convenient develop version proposed Extended EM algorithm uncertain
training sets, structured tables (sub)cases (uncertainly valued) variables (see Fig. 17).
First, let us write eq. (24) expanding terms components sr:

p( z| Ci , ) P{Ci | ) = p( z , Ci | )
=

p( z, C | ) P{s | S} = p( z| C , ) P{C | } P{s | S}


r

r

r



r



r

r

r

Therefore

( ) P{Ci | S} = ,r ( r ) P{Ci , r | S}
r

result obtained relation Ez{w(z)} = ES{ Ez|S{w(z)| S} } w(z) g(z) P{Ci|z} Bayes Theorem.

5

194

fiPROBABILISTIC INFERENCE UNCERTAIN DATA USING MIXTURES

(a)

(b)
(d)

(c)

Figure 16. extended EM algorithm iteratively reduces (large) difference
(a) true density p(z), (b) mixture model p ( z ) , indirectly (small)
discrepancies (c) true observation density p(S) (d) modeled observation density p ( ) . real cases p(S) must estimated finite i.i.d. sample
{S(k)}.

S(1)

s1(1)
s2(1)
s(2)
s1(3)
s2(3)
s3(3)
...

.4
.6
1
.2
.5
.3
...

S(2)
S(3)

(r k )

S(r)

( sr1 , 1r ) ( k )

( srj , rj ) ( k )

...

...

...
...
Figure 17. Structure uncertain training information Extended EM Algorithm. coefficients

(r k ) normalized easy detection rows included

uncertain example. z



(k )

= 1

(k)

(k)

uncertain,

reduces single row

= 0.
j

Using notation introduced (12),

,r ( r ) E z |sr ,Ci {g ( z )| r , Ci } =



Z

g ( z ) p( z| r , Ci ) dz =



Z

P{Ci , r | S} = P{Ci | r } P{s r | S} = ,r
write (25) as:

195

g ( z ) ij,r ( z j ) dz
j

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO



E z|Ci {g (z ) | Ci } = E ,r g (z ) ij,r ( z j )dz / Pi
Z
j
r

MFGN framework contributions ,r ( r ) P{Ci , r | S} empirical expected
values required Extended EM algorithm obtained without numeric integration.
need consider case g(z) = z j compute means ij probabilities ti j, ,
g(z) = ( z j )2 deviations ij . (12) already know explicit expression parameters ij,r ( z j ) = T( z j , ij,r , ji ,r ) . Hence:



Z

z j ,r ( z ) dz =

(z
Z

j



Z

z j ij,r ( z j )dz j = ij,r

) 2 ,r ( z ) dz = ( ij,r ) 2 + ( ji ,r ) 2

conclusion, steps Extended EM algorithm follows:
(E) Expectation step. Compute elementary likelihoods training set:

(

ij,r( k ) = srj , ij , ( ij ) 2 + ( rj ) 2

)

(k )

(26)

( )
( )
Obtain likelihood conjunction sr k example k component Ci:

i(,kr) = ij,r( k )
j

( )
Obtain total likelihood example k :

( k ) p( ( k ) ) = Pi r( k ) i(,kr )


r

( )
( )
( )
Compute probabilities qi ,kr P{Ci , r k | k } r-th component k-th exam-

ple generated i-th component mixture:

qi(,kr ) i(,kr) = Pi r ( k ) i(,kr) / ( k )
(M) Maximization step. Update parameters component Ci using components

r( k ) examples weighted probabilities qi(,kr ) . First, prior probabilities
component:

Pi

1


q
k

(k )
,r

r

mean value standard deviation component:

ij

1
MPi

[q
k

r

196

,r

ij,r

]

(k )

fiPROBABILISTIC INFERENCE UNCERTAIN DATA USING MIXTURES

( ij ) 2

1
MPi

[q
k

,r

]

[( ij,r ) 2 + ( ij ,r ) 2 ]

r

(k )

( ij ) 2

(27)

symbolic variables representation (8) may use:

ij,r( k ) = P{srj = }( k ) ti j,

(26)



ti j,

1
MPi

[q
k

,r

P{srj = } j, / ij,r

r

]

(k )

(27)

Consider particular case attributes training examples contaminated
unbiased Gaussian noise. likelihood uncertain observations modeled 1( )
( )
( )
( )
( )
( ) 2
component mixtures: p( k | z k ) = j T( z j k , j k , j k ) , ( j k ) variance
measurement process z j ( k ) obtains observed value j ( k ) . ex( )
( )
( )
pressed confidence interval z j k j k 2 j k . case, basic EM algorithm (23)
( )
easily modified take account effect uncertainties j k . E step, com( )
pute qi k using following deviations:

ij ( ij ) 2 + ( j ( k ) ) 2
and, step, apply substitution:

z j ( k ) j ( k ) + (1 ) ij

[

( z j ( k ) ) 2 j ( k ) + (1 ) ij

] + [ ]
2

j( k ) 2



( ij ) 2
= j 2
( ) + ( j ( k ) ) 2
measures relative importance observed j k computing new ij ij .
previous situation illustrates missing values must processed learning stage.
( )
j (k )
z
exact j k = 0 = 1, original algorithm (23) changed.
( )
extreme, z j ( k ) missing, modeled j k , get = 0 therefore
( )
observation j k contribute new parameters all. correct procedure deal
missing values MFGN framework simply omitting empirical averages.
Note fact arises factorized structure mixture components, providing conditionally independent attributes. Alternative learning methods require careful management
missing data avoid biased estimators (Ghahramani & Jordan 1994).
( )

4.3 Evaluation Extended EM Algorithm
studied improvement parameter estimations uncertainty observations, modeled likelihood functions, explicitly taken account. proposed Extended

197

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

EM compared EM algorithm "raw" observations (Basic EM), ignores
likelihood function typically uses average value (e.g., given x82, Basic EM uses
x=8). considered synthetic 3-attribute domain following joint density:
p(x,y,w) = 0.5 (x,0,2) (y,0,1) (w,white)
+ 0.5 (x,2,1) (y,2,2) (w,black)

TT

TT

TT

Different learning experiments performed varying degrees uncertainty.
cases training sample size 250. trained models structure true density (2 components), since goal experiment measure quality estimation
respect amount uncertainty, without regard sources variability
local minima, alternative solutions, etc., empirically studied Section 5. Table 6 shows
mixture parameters obtained learning algorithms. Fig. 18 graphically shows difference Extended Basic EM illustrative cases.
Case 0: Exact Data (Fig. 18.a).
Cases %: Results Extended EM learning algorithm % rate missing
values training data.
Case 1: Basic EM attribute biased +3 units probability 0.7. Case 2: Extended EM
algorithm Case 1 (see Fig. 18.b). Here, observed value sy=y+3 70% samples
sy=y rest. samples, Basic EM uses observed value sy Extended EM uses
explicit likelihood function f(y) = 0.3 (ysy) + 0.7 (y(sy3)).
Case 3: Basic EM attributes x Gaussian noise = 0.5 w changed
probability 0.1. Case 4: Extended EM algorithm Case 3.
Case 5: Basic EM x Gaussian noise = 1 w changed probability
0.2. Case 6: Extended EM algorithm Case 5 (see Fig. 18.c).
Case 7: Basic EM x Gaussian noise = 2 w changed probability
0.3. Case 8: Extended EM algorithm Case 7 (see Fig. 18.d).



Case 9: Extended EM values y>3 missing (censoring). Case 10: Extended EM Case
9 missing values assumed distributed
(y, 4, 1), providing additional information data generation mechanism.
Table 6 Fig. 18 confirm small amounts deterioration relation sample
size, estimates computed basic EM Algorithm raw observed data similar
obtained Extended EM algorithm (e.g., Cases 3 4). However, data sets
moderately deteriorated true joint density correctly recovered Extended EM using
likelihood functions attributes instead raw observed data (e.g., Cases 5 6, Fig.
18.c). Finally, large amount uncertainty respect training sample
size true joint density cannot adequately recovered (e.g., Cases 7 8, Fig. 18.d).

198

fiPROBABILISTIC INFERENCE UNCERTAIN DATA USING MIXTURES

(a)

(b)

(c)
(d)
Figure 18. Illustration advantages Extended EM algorithm (see text). (a)
Case 0 (exact data). (b) Cases 1 2 (biased data). (c) Cases 5 6 (moderated noise).
(d) Cases 7 8 (large noise). figures show true mixture components (gray ellipses), available raw observations (black white squares), components estimated Basic EM raw observations (dotted ellipses) components estimated Extended EM taking account likelihood functions uncertain
values (black ellipses).

Note ability learn uncertain information suggests method manage non
random missing attributes (e.g., censoring) (Ghahramani & Jordan 1994) complex
mechanisms uncertain data generation. illustrated Case 9, missing data generation
mechanism depends value hidden attribute, correct assign equal likelihood
components. principle, statistical studies kind knowledge may help ascertain
likelihood true values function available observations. instance, Case 10
replaced missing attributes Case 9 normal likelihoods 42 (i.e, high), improving estimates mixture parameters.

199

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

Case

P1

1x

1y

1x

1y

t1,wwhite

2x

2y

true
.5
0
0
2
1
1
2
2
0
.48 -.04 .03 2.11 1.00 1.00 2.09 1.83
20%
.48 .16 -.03 1.91 1.01 0.96 2.31 2.09
40%
.49 .14 -.16 1.78 .99 0.95 2.39 2.29
60%
.45 .02 -.29 2.50 .78 1.00 1.86 2.01
80%
.49 -.11 1.69 2.21 1.73 .50 1.91 0.31
1
.48 .06
.90 1.88 1.73 1.00 1.88 2.98
2
.48 -.04 .05 1.90 .92 1.00 1.98 1.97
3
.47 .02
.09 1.60 1.02 .87 2.00 1.78
4
.49 .27 -.08 1.97 .90
.70 2.10 2.06
5
.43 -.04 -.18 2.40 1.48 .82 1.85 1.52
6
.54 -.07 -.02 1.97 1.09 .56 1.93 2.11
7
.46 .15 -.16 2.73 2.53 .31 1.94 1.62
8
.79 .87
.08 2.09 1.52 .51 1.96 3.61
9
.48 .32 -.02 1.77 1.10 1.00 1.92 0.67
10
.45 .00
.03 2.20 1.01 1.00 2.13 1.55
Table 6. Parameter Estimates Uncertain Information (see text)

2x

2y

t2,wblack

1
1.00
.88
.94
1.03
1.14
.96
1.01
1.14
1.01
1.52
.85
2.47
0.87
0.94
1.04

2
2.08
2.10
2.14
1.77
0.68
2.40
1.90
2.07
1.94
2.33
1.69
2.90
1.21
1.22
1.77

1
1.00
1.00
1.00
1.00
0.47
1.00
1.00
0.85
0.71
.80
.62
.29
.54
1.00
1.00

Example 14: Learning examples missing attributes performed IRIS
domain illustrate behavior MFGN framework. whole data set randomly
divided two subsets equal size training testing. 5-component mixture models
obtained evaluated, combining missing data proportions 0% 50%. error
prediction attribute U (plant class) following:

missing attributes

training set
0%
0%
50%
50%

test set
0%
50%
0%
50%

prediction error
2.7%
12.0%
4.0%
18.7%

relatively simple IRIS domain, performance degradation due 50% missing attributes much greater inference learning stage. Extended EM algorithm able
correctly recover overall structure domain available information.

4.4 Comments
Convergence EM Algorithm fast, requiring adjustable parameters learning
rates. algorithm robust respect random initial mixture parameters: bad local
maxima frequent alternative solutions usually equally acceptable. examples
contribute components, never wasted unfortunate initialization. fixed
number components, algorithm progressively increases likelihood J training data
maximum reached. number components incremented maximum J
increases, limit value obtained cannot improved using extra components (Fukunaga 1990). simple heuristics incorporated standard Expectation-Maximization
scheme control value certain parameters (e.g., lower bounds established variances) quality model (e.g., mixture components eliminated proportions
small).
case, factorized components specially convenient matrix inversions
required and, important, uncertain missing values correctly handled

200

fiPROBABILISTIC INFERENCE UNCERTAIN DATA USING MIXTURES

simple unified way, heterogeneous attribute sets. necessary provide models
uncertain attribute correlations since covariance parameters must estimated. Finally,
training sample size must large enough relation degree uncertainty examples complexity joint density model order obtain satisfactory approximations.
hand, number mixture components required satisfactory approximation joint density must specified. pragmatic option minimization experimental estimation cost main inference task, exists. instance, regression
could increase number components acceptable estimation error obtained
independent data set (cross-validation). idea applies pattern classification: use
number components minimizes error rate independent test set. However, one
main advantages proposed method independence learning stage
inference stage, freely choose dynamically modify input output role
attributes. Therefore, global validity criterion desirable. typical validation methods
mixture models reviewed McLachlan & Basford (1988); standard approach based
likelihood ratio tests number components. Unfortunately, method validate
mixture itself, selects best number components (DeSoete 1993).
Since MFGN framework provides explicit expression model p(z), apply
statistical tests hypothesis independent sample taken true density (e.g. subset examples reserved testing) find obtained approximation compatible
test data. hypothesis H = {T comes p(z)} rejected, learning process must
continue, possibly increasing number components. difficult build statistical
tests, e.g. moments p(z), sample means variances directly obtained.
However, data sets usually include symbolic numeric variables, developed
test expected likelihood test sample, measures well p(z) covers examples. mean variance p(z) easily obtained using properties generalized
normals. experiments simple univariate continuous densities show test
powerful small sample sizes, i.e. incompatibility always detected, standard tests significantly evidence rejection. Nevertheless, clearly inaccurate approximations
detected, results improve sample size increases test valid data sets uncertain values.
Minimum Description Length (Li & Vitnyi 1993) principle invoked select
optimum number components trading-off complexity model accuracy
description data.

201

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

5. Discussion Experimental Results
5.1 Advantages Joint Models
inductive inference methods compute direct approximation conditional densities
interest, even obtain empirical decision rules without explicit models underlying conditional densities. cases, model learning stage depend selected input /
output role variables. contrast, presented inference learning method based
approximation joint probability density function attributes convenient
parametric family (a special mixture model). MFGN framework works pattern completion
machine operating possibly uncertain information. example, given pattern classification
problem, learning stage suffices predicting class labels feature vectors
estimating value missing features observed information incomplete patterns.
joint density approach finds regions occupied training examples whole attribute
space. attribute dependences captured higher abstraction level one provided
strictly empirical rules pre-established target variables. property extremely useful
many situations, shown following examples.
Example 15: Hints provided inference multivalued relations. Given data
set model Example 10, assume interested value x = 0.
obtain bimodal marginal density shown Fig. 19.a corresponding estimator x
0.2 1.4 is, sense, meaningless. However, specify branch interest
model, inferring = 0 x -11 (i.e., x small), obtain unimodal
marginal density Fig. 19.b reasonable estimator x 0.80.5.

(a)
(b)
Figure 19. desired branch multivalued relations selected providing
information output values. (a) Bimodal posterior density inferred
y=0. (b) Unimodal posterior density inferred = 0 hint x small.

Example 16: Image Processing. advantages joint model supporting inferences
partial information inputs outputs illustrated following application
natural data (see Fig. 20). image Fig. 20.a characterized 5-attribute density
(x, y, R, G, B) describing position color pixels. random sample 5000 pixels
used build 100-component mixture model. interested location certain ob-

202

fiPROBABILISTIC INFERENCE UNCERTAIN DATA USING MIXTURES

jects image. Figs. 20.b-f show posterior density desired attributes given following queries:


"Something light green". Fig. 20.b. Two groups easily identified posterior
density, corresponding upper faces green objects6. = C1={x, unknown;
R=11050, G=24510, B=16050}.



"Something light green dark red". Fig. 20.c. find groups
additional, scattered group, corresponding red object. greater dispersion
arises larger size red object fact R component
dark red disperse G component light green. =Two equiprobable components C1 C2={x, unknown; R=11010, G=B=3050}.



"Something light green right". Fig. 20.d. provide partial information
output: = C3={x=24030; unknown; R=11050, G=24510, B=16050}



"Something white". Fig. 20.e. = C4={x,y unknown; R=24510, G=24510, B=24510}



"Something white, lower-left region, main diagonal (y<240-x)". Fig. 20.f.
provide relational information attributes modeled = 6 equiprobable components (note case posterior distribution contains 600 components, still computationally manageable) =

{x=6030, y=18030, R=24510, G=24510, B=24510}+
{x=6030, y=12030, R=24510, G=24510, B=24510}+
{x=6030, y=6030, R=24510, G=24510, B=24510}+
{x=12030, y=12030, R=24510, G=24510, B=24510}+
{x=12030, y=6030, R=24510, G=24510, B=24510}+
{x=18030, y=6030, R=24510, G=24510, B=24510}
cases, posterior density consistent structure original image. time
required compute posterior distribution always lower one second. Learning time
order hours Pentium 100 system. Simpler models (25-component, obtained
1000 random pixels) produced acceptable results much lower learning time. Furthermore, EM algorithm efficiently parallelized.

hand, large number irrelevant attributes, joint model strategy wastes resources capture proper probability density function along unnecessary dimensions. (This problem arise specification likelihood function, since relevant attributes explicitly appear model.) Joint modeling appropriate domains
moderated number "meaningful" variables without fixed input / output roles.

6

Note sharp peak (a component small dispersion) obtained learning process, "transmits"
posterior density. kind artifacts inocuous easily removed post processing.

203

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

(a)

(b)

(c)

(d)

(e)
(f)
Figure 20. Inference results image domain Example 16. (a) source image.
(b) posterior density attributes x-y given "Something light green". (c)
"Something light green dark red". (d) "Something light green right".
(e) "Something white". (f) "Something white, lower-left region,
main diagonal image (Y<240-X)"

5.2 Advantages Factorization
proposed methodology supported general density approximation property mixture
models. use components independent variables order make computations feasible

204

fiPROBABILISTIC INFERENCE UNCERTAIN DATA USING MIXTURES

inference learning stage. Factorized components imposed mixture model without loss generality. statistical dependence variables still captured, cost
possibly larger number components mixture achieve required accuracy
approximation.
simplicity building block structure entirely compensated important saving computation time. High-dimensional integrals analytically computed univariate
integrals matrix inversions avoided learning stage. Additionally, high-dimensional
domains easily modeled using small number parameters mixture component.
viewpoint Computational Learning Theory (Vapnik 1995), models small number adjustable parameters (actually, low expressive power) favorable consequences
generalization.
Mixtures factorized components used Latent Class Analysis (DeSoete 1993),
well-known unsupervised classification technique. assumed statistical dependences
attributes fully explained hidden variable specifying latent class
example. method similar Gaussian decomposition clustering algorithm mentioned
Section 1, constrained component-conditional attribute independence. However, goal
unsupervised classification obtaining accurate mathematically convenient expression
joint density variables, required derive desired estimators. meaning
components irrelevant, long whole mixture good approximation joint density.
expressive architectures, combine mixture models local dimensionality reduction, considered: Mixtures Linear Experts (Jordan & Jacobs 1994), Mixtures
Principal Component Analyzers (Sung & Poggio 1998) Mixtures Factor Analyzers (Ghahramani & Hinton 1996, Hinton, Dayan, & Revow 1997). Unfortunately, general kind inference learning uncertain data considered work cannot directly incorporated
architectures computational advantages demonstrated MFGN model.
restriction factorized components may produce undesirable artifacts approximations certain domains learned small training samples. Nevertheless, problem always
occurs approximator structure building block match shape
target function. case, many terms (or components, units, etc.) required good
approximation associated parameters correctly adjusted large training
sample. However, note complexity model measured uniquely terms
number mixture components. number adjustable parameters probably better
measure complexity. instance, full covariance models show quadratic growth number free parameters respect dimension attribute vector. factorized components growth linear, amount training data need unreasonably high even
number mixture components large.
real applications, nature target function unknown, little said priori
best building block structure used universal approximator. chosen
simple component structure make inference learning feasible uncertain information. Section 5.4 provides experimental evidence realistic problems proposed model
inferior popular approaches.
5.3 Qualitative Comparison Alternative Approaches
Instead proposed methodology, based mixture models EM algorithm, alternative nonparametric density approximation methods could used (either joint density
specific conditional densities). instance, nearest neighbor rule locally approximates
target density using certain number training samples near point interest. Symbolic

205

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

attributes directly estimated voting scheme continuous attributes estimated
averaging observed values training instances near, subspace observed
attributes, point interest. However, small sample sizes, estimators
smooth show strong sensitivity random fluctuations training set, penalizes
estimation cost. large sample size, time required find nearest neighbors becomes
long. example, consider regression problem Example 10, Section 3.5. Fig. 11.b shows
MFGN solution 4 components MSE=0.381. Fig. 21.a shows regression line obtained 5-nearest-neighbors average, higher MSE=0.522.
Parzen windows similar kernel approximation methods used smooth results
simple nearest neighbors rule (Duda & Hart 1973, Izenman 1991). actually mixtures
simple conventional densities located training samples. principle, properties
MFGN framework could adapted kind approximation (Ruiz et al. 1998). Learning
becomes trivial, strong run time computation effort required since concise model
domain extracted training set. kind rote learning negative consequences generalization according Occam Razor Principle (Li & Vitnyi 1993). adequately cross-validated mixture model small number components relation training sample size reasonably guarantees probably true attribute dependencies correctly
captured.

C2

C2
C1

C1

(a)
(b)
(c)
Figure 21. Alternative solutions regression classification (see text details).

nature solutions obtained Backpropagation Multilayer Perceptrons (Rumelhart et
al. 1986) pattern classification illustrative. general, decision region geometrically expressed union intersections several halfspaces defined units
first hidden layer. However, backprop networks often require long learning times, many adjustable parameters and, worse, apparently simple distributions patterns hard learn.
instance, solution circle-ring classification problem Fig. 21.b, obtained network 6 hidden units requires hundreds standard backprop epochs. decision regions
satisfactory, even though network extra flexibility task (3 hidden units
suffice separate training examples). Better solutions exist using resources network architecture, backprop learning find them. contrast, solution obtained
MFGN approach using 7 components (Fig. 21.c) requires learning time orders magnitude
shorter backprop optimization. components mixture contribute synthesize
reliable decision regions acceptable solutions obtained smaller number
components.

206

fiPROBABILISTIC INFERENCE UNCERTAIN DATA USING MIXTURES

proposed approach closely related well-known family approximation techniques
which, essentially, distribute (using kind clustering self-organizing algorithm) detectors relevant regions input space combine responses computing
desired outputs. case Radial Basis Functions (RBF) (Hertz et al.), classification regression trees proposed (Breiman et al. 1984) topological maps used (Cherkassky & Najafi 1992) locate knots required piecewise linear regression.
relevant methodology proposed (Jordan & Jacobs 1994, Peng et al. 1995),
EM algorithm used learn hierarchical mixtures experts form linear rules
way desired posterior densities explicitly obtained. properties EM algorithm satisfactorily used (Ghahramani & Jordan 1994) obtain unbiased approximations missing data mixture-based framework similar ours. framework extends
successful approach exploiting conjugate properties chosen universal approximation
model: uncertain information arbitrary complexity efficiently processed inference
learning stages.
MFGN framework appropriate moderated number variables showing relatively
complex dependencies. contrast, Bayesian Networks satisfactorily addresses case large
number variables clear conditional independence relations. situations
certain subset variables Bayesian Network shows explicit causal structure. subdomain could empirically modeled mixture model order considered later composite node embedded whole network. subdomain conditionally isolated
rest variables set communication nodes, MFGN framework used
perform required inferences.
Finally, mixture models typically used unsupervised classification: examples
labeled index component highest posterior probability. fact, MFGN
framework explicitly finds clusters training set. Furthermore, continuous symbolic attributes allowed joint density, examples clustered using implicit probabilistic metric automatically weighs (heterogeneous) attributes, even missing
uncertain values. However, method effective groups interest
structure component densities. order simplify inference mixture components
selected constraints (Gaussian, independent variables) necessarily verified
natural groups found real applications.
tentative possibility (inspired common heuristic clustering technique) consists joining overlapping components (e.g., according Battachariya distance, well-known bound
Bayes error used Statistical Pattern Recognition (Fukunaga 1990)). Unfortunately, experiments indicate overlapping threshold free parameter strongly determines
quality results. universal threshold, independent application, seem exist.
principle, clusters arbitrary geometry may discovered, cannot easily automated.
Therefore, nonparametric cluster analysis methods (e.g. density valley seeking) suggested
labeling complex groups.
5.4 Experimental Evaluation
MFGN method evaluated standard benchmarks Machine Learning database repository University California, Irvine (Merz Murphy 1996). contains inductive learning problems representative real world situations. experimented
following databases: Ionosphere, Pima Indians, Monk's Problems, Horse Colic,
illustrate different properties proposed methodology. cases MFGN compared alternative learning methods respect inference task considered interest

207

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

problem (typically, prediction specific attribute given rest them). usually give
error rate training test set indicate amount overfitting obtained
learning algorithms.

(a) Ionosphere
(b) Pima Indians
Figure 22. discriminant 2D projections two representative databases.

5.4.1 IONOSPHERE DATABASE
Two classes radar returns ionosphere must discriminated vectors 32 continuous attributes7. 351 examples, randomly partitioned two disjoint subsets approximately equal size training testing. prevalence minoritary class (random
prediction rate) 36%. Figure 22.a Table 7 show typical statistical pattern recognition problem, easily solvable standard methods. results suggest Bayes (optimum)
error probability around 5%.
error rate
PE
(training set)
METHOD
(test set)
Linear MSE (pseudoinverse)
1-1 Nearest Neighbor
2-3 Nearest Neighbor
Parzen Model
Backprop Multilayer Perceptron 2 hidden units
Support Vector Machine, RBF kernel, width 1, (105 s.v.)
Support Vector Machine, RBF kernel, width 3, (35 s.v.)
Support Vector Machine, polinomial kernel, order 2, (41 s.v.)
Support Vector Machine, polinomial kernel, order 3, (45 s.v.)
Support Vector Machine, polinomial kernel, order 4, (42 s.v.)
Full covariance gaussian mixture, 1 component/class
Full covariance gaussian mixture, 2 component/class
Full covariance gaussian mixture, 3 component/class
MFGN 4 components (average)
MFGN 8 components (average)
MFGN 15 components (average)
MFGN, best result cross-validation (8 components)
Table 7. Ionosphere Database Results

7

.11

.05
.00

.03
.01
.005
.22.15
.11.06
.10.05
.07

Originally database contains 34 attributes. Two them, meaningless ill behaved, eliminated.

208

.14
.13
.18
.08
.08
.05
.09
.13
.17
.20
.11
.19
.26
.21.08
.13.06
.13.06
.06

fiPROBABILISTIC INFERENCE UNCERTAIN DATA USING MIXTURES

problem, plain MFGN method, without special heuristics learning stage,
comparable average alternative methods. best solution training set (crossvalidation) entirely satisfactory.
Ionosphere database present exhaustive study performance given varying
proportions missing values training testing examples. value x % means
training test examples value attribute deleted probability x. basic experiment consists learning MFGN model prescribed number components (4, 8 15)
computing error rate training test sets. Table 8 shows mean value 2 standard deviations error rates obtained 10 repetitions basic experiment configuration. Column contains error rate configuration training set. training/test partition kept fixed analyze variability solutions due random initialization
EM.
LEARNING


0%

INFERENCE
10%
25%

50%

4 COMP. - 0%
8 COMP. - 0%
15 COMP. - 0%

22 15
11 6
10 5

21 8
13 6
13 6

21 8
12 6
13 6

22 8
13 5
13 5

22 9
12 6
13 4

4 COMP. - 10%
8 COMP. - 10%
15 COMP. - 10%

21 14
11 3
10 3

23 11
13 5
12 6

23 11
12 5
12 7

23 11
13 5
12 6

23 11
13 4
12 3

4 COMP. - 25%
8 COMP. - 25%
15 COMP. - 25%

18 7
12 7
95

19 5
14 10
12 9

19 5
14 9
13 11

18 6
15 8
13 9

18 6
14 7
13 7

4 COMP. - 50%
27 18 26 15 27 15 27 14 26 13
8 COMP. - 50%
16 12 21 15 21 15 21 13 20 11
15 COMP. - 50% 13 6
26 17 25 15 25 14 23 13
Table 8. Evaluation MFGN Ionosphere Database given
different proportions missing data training testing subsets.

expected, MFGN model robust respect large proportions missing values
test patterns, moderated proportions missing data training set. compared behavior standard algorithm Decision Tree construction inspired
(Quinlan 1993), able support missing values8. Table 9 shows error rates
decision trees experimental setting Table 8. kind Decision Tree obtains
error rates better averages obtained MFGN. However, MFGN's best solutions
(selected cross-validation) better ones obtained Decision Tree. Furthermore,
Decision Tree performance degrades faster MFGN, especially respect proportion
missing values inference stage.

8

Essentially, missing values handled follows. learning stage, attribute selected, examples
missing values sent partitions appropriate weights. inference stage, node asks missing
value, follows branches appropriate weights finally outputs combined.

209

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

LEARNING


0%

INFERENCE
10 %
25 %

50 %

1%
0%
9%
10 %
11 %
12 %
5%
10 %
14 %
15 %
19 %
18 %
6%
25 %
15 %
17 %
17 %
18 %
8%
50 %
17 %
18 %
18 %
19 %
Table 9. Evaluation basic Decision Tree Ionosphere Database given different proportions missing data training testing subsets.

5.4.2 PIMA INDIANS DATABASE
problem must discriminate two possible results diabetes test given Pima
Indians. 8 continuous attributes, 768 examples, randomly partitioned two disjoint subsets equal size training testing. prevalence minority class 35%.
attribute vector normalized. Table 10 presents comparative results.
error rate
PE
(training set)
METHOD
(test set)
Linear MSE (pseudoinverse)
Oblique Decision Tree 8 decision nodes
1-1 Nearest Neighbor
2-3 Nearest Neighbor
Full covariance gaussian mixture, 1 component/class
Full covariance gaussian mixture, 2 component/class
Full covariance gaussian mixture, 3 component/class
Full covariance gaussian mixture, 4 component/class
Backprop Multilayer Perceptron 2 hidden units
Backprop Multilayer Perceptron 4 hidden units
Backprop Multilayer Perceptron 8 hidden units
Support Vector Machine, RBF kernel, width 1 (297 s.v.)
Support Vector Machine, RBF kernel, width 3 (176 s.v.)
Support Vector Machine, polynomial kernel, order 4 (138 s.v.)
Support Vector Machine, polynomial kernel, order 5 (131 s.v.)
MFGN 4 components
MFGN 6 components
MFGN 8 components
Table 10. Pima Indians Database Results

.22
.18

.24
.19
.17
.17
.17
.14
.05

.28
.25
.29

.23
.24
.30
.25
.26
.29
.30
.31
.25
.24
.29
.30
.35
.36
.34
.35
.32
.35

Despite low dimensionality large number examples, classification problem
hard (see Figure 22.b). Even sophisticated learners backpropagation networks, decision
trees support vector machines, able store reasonable proportion training set,
achieve significant generalization. MFGN shows similar behavior, although slightly
less prone overfitting (the error rate training set misleading).
5.4.3 HORSE COLIC DATABASE
database contains classification task heterogeneous attribute vector including symbolic, discrete continuous variables, 30% missing values. illustrates problem

210

fiPROBABILISTIC INFERENCE UNCERTAIN DATA USING MIXTURES

feature selection context joint modeling, mentioned Section 5.1. Table 11 shows
error rates obtained MFGN using different attribute subsets9. take advantage general
inference properties, MFGN model must applied attribute subset interest.
inference task fixed number attributes large, alternative methods
used.
METHOD

PE

PE

(distribution, 10 initializations)

(best)

6 Selected attributes
MFGN 2 components
MFGN 3 components
MFGN 4 components
MFGN 5 components
MFGN 6 components
MFGN 7 components
MFGN 10 components
MFGN 12 components
MFGN 15 components

.32.00
.20.05
.19.02
.20.02
.19.03
.20.04
.22.04
.19.03
.19.04

.32
.18
.18
.18
.16
.18
.18
.16
.15

.22.01
.21.02
.21.03
.23.02
.21.02
.21.02

.21
.19
.18
.18
.18
.18

.28.02
.29.03
.34.08
.34.06

.25
.25
.25
.28

8 Selected attributes
MFGN 4 components
MFGN 6 components
MFGN 8 components
MFGN 10 components
MFGN 12 components
MFGN 15 components

23 Selected attributes
MFGN 6 components
MFGN 8 components
MFGN 10 components
MFGN 15 components
Table 11. Horse Colic Database Results (random rate = .5)

5.4.4 MONK'S PROBLEMS
Monk's problems three concept learning tasks 6 symbolic attributes, widely used
benchmarks inductive learning algorithms (Thrun et al. 1991). seen Table 12, MFGN fails
MONK1 (where acceptable generalization obtained) MONK2 (where training
examples cannot even stored). contrast, MFGN correctly solves MONK3. behavior
related fact MONK's problems based deterministic abstract concepts
may lack kind geometric regularities attribute space required probabilistic models10.
9

Features individually selected using simple discrimination index related Kolmogorov-Smirnov statistic
(Ruiz 1995).
10
typical example parity problem: acceptable off-training-set generalization cannot achieved inductive
bias learning machine biased towards "smooth" solutions.

211

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

Fig. 23 shows discriminant 2D projections datasets illustrates fact
MONK2 cannot easily captured statistic techniques. benchmark, MFGN performance
similar popular probabilistic methods (Thrun et al. 1991).

(a) MONK1
(b) MONK2
Figure 23. Discriminant 2D Projections Monk's Datasets.

(c) MONK3

error rate
(training set)

METHOD

PE
(test set)

MONK1 (random rate = .5)
Linear MSE (pseudoinverse)
1-1 Nearest Neighbor
Support Vector Machine, RBF kernel, width 1 (78 s.v.)
Cascade Correlation
MFGN 4 components
MFGN 8 components

.29

.06
.00

.34
.17
.08
0
.40
.33

MONK2 (random rate .4)
Linear MSE (pseudoinverse)
1-1 Nearest Neighbor
Support Vector Machine, RBF kernel, width 1 (117 s.v.)
Cascade Correlation
MFGN 4 components
MFGN 8 components
MFGN 15 components

.40

.31
.26
.14

.37
.19
.20
0
.38
.44
.50

MONK3 (random rate .5)
Linear MSE (pseudoinverse)
1-1 Nearest Neighbor
Support Vector Machine, RBF kernel, width 1 (69 s.v.)
Cascade Correlation
MFGN 2 components
MFGN 4 components
MFGN 8 components
Table 12. Monk's Problems Results

212

.19

.07
.04
.03

.19
.18
.08
.03
.03
.03
.08

fiPROBABILISTIC INFERENCE UNCERTAIN DATA USING MIXTURES

5.4.5 COMMENTS
experiments demonstrate MFGN model able obtain acceptable results
many real world applications. particular, error rates obtained standard classification tasks
comparable obtained popular learners. Additionally, MFGN able perform
inferences attribute given uncertain partial information, possible
alternative methods. property makes MFGN attractive alternative many
inference problems one illustrated Example 16. experiments contributed characterize kind problems MFGN model best suited. Essentially,
relationship among attributes must true probabilistic nature, attribute vector must
moderated size containing "relevant" variables. previous feature selection / accommodation
stage recommended certain applications.

6. Conclusions
developed efficient methodology probabilistic inference learning uncertain information. proposed MFGN framework, joint probability density function
attributes likelihood function available information approximated Mixtures Factorized Generalized Normals. mathematical structure allows efficient computation,
without numerical integration, posterior densities expectations desired variables given
events arbitrary geometry. extended version EM learning algorithm developed estimate parameters required mixture models uncertain training examples.
Different paradigms pattern recognition, regression pattern completion subsumed
common framework.
comprehensive collection examples illustrates methodology, critically
compared alternative techniques. Extended EM algorithm able learn satisfactory
domain models reasonable number examples uncertain values, taking account
explicit likelihood functions available information. Results satisfactory whenever
sample size large relation amount (known) degradation training set. experiments characterized kind situations model manages better: Domains described moderate number heterogeneous attributes complex probabilistic dependences,
problems output variables necessarily known learning stage (i.e. pattern
completion), and, finally, problems explicit management uncertainty needed, either learning inference stage (or even both). MFGN framework obtained
favorable trade-off useful features model complexity solutions different applications benchmarks.
Future developments work include improving learning stage heuristic
steps combined standard E steps control adequacy acquired
models. Additional studies required validation tests, generalization, scalability, robustness
data preprocessing. essential idea working explicit likelihood functions
incorporated Parzen approximation scheme interested expressive
model structures mixtures factor analyzers, principal component analyzers linear experts. Finally, methodology developed pure Bayesian framework subsumed
Dempster-Shafer Evidence Theory.

213

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

Acknowledgments
authors would thank anonymous reviewers careful reading helpful suggestions. work supported Spanish CICYT grants TIC95-1019, TIC97-1343C02-02 TIC97-0897-C04-03.

References
Berger, J., (1985). Statistical Decision Theory Bayesian Analysis. Springer-Verlag.
Bernardo, J.M., Smith, A.F.M. (1994). Bayesian Theory. Wiley.
Bouckaert, R.R. (1994). Properties Bayesian Belief Network Learning Algorithms. Proceedings Uncertainty AI, pp. 102-109.
Breiman, L., Friedman, J.H., Olshen, R.A., Stone, C.J. (1984). Classification Regression
Trees. Wadsworth International Group, Belmont, CA.
Chang, K. & Fung, R. (1995). Symbolic Probabilistic Inference Discrete Continuous Variables. IEEE Tran. Systems, Man, Cybernetics, Vol. 25, No. 6, june, pp. 910916.
Cherkassky, V. Lari-Najafi, H. (1992). Nonparametric Regression Analyisis Using SelfOrganizing Topological Maps H. Wechsler (ed.), Neural Networks Perception. Vol.2,
Computation, Learning Architectures, San Diego: Academic Press.
Cohn, D.A., Ghahramani, Z. & Jordan, M.I. (1996). Active Learning Statistical Models.
Journal Artificial Intelligence Research 4, pp. 129-145.
Dalal, S.R. & Hall, W.J. (1983). Approximating Priors Mixtures Natural Conjugate Priors.
J. R. Statist. Soc. B, Vol. 45, No. 2, pp. 278-286.
De Soete, G. (1993). Using Latent Class Analysis Categorization Research I. V. Mechelen,
J. Hampton, R.S. Michalski, P. Theuns (eds.), Categories Concepts: Theoretical Views
Inductive Data Analysis, San Diego: Academic Press.
Dempster, A.P., Laird, N.M., Rubin, D.B., (1977). Maximum Likelihood Estimation Incomplete Data via EM Algorithm. Journal Royal Statistical Society, Series B, Vol. 39:
pp. 1-38.
Duda, R.O. Hart, P.E. (1973). Pattern Classification Scene Analysis. John Wiley & Sons.
Fan, C.M., Namazi, N.M. Penafiel, P.B. (1996). New Image Motion Estimation Algorithm
Based EM Technique. IEEE Transactions Pattern Analisys Machine Intelligence, Vol.18, No.3, March, pp. 348-352.
Fukunaga, K. (1990). Introduction Statistical Pattern Recognition. Academic Press.
Ghahramani, Z. Jordan, M.I. (1994) Supervised learning Incomplete data via EM
approach Cowan, J.D., Tesauro, G., Alspector, J. (eds.). Advances Neural Information Processing Systems 6. Morgan Kauffman
Ghahramani, Z. Hinton, G.E. (1996) EM algorithm mixtures factor analyzers.
Tech. Rep. Univ. Toronto. CRG-TR-96-1.

214

fiPROBABILISTIC INFERENCE UNCERTAIN DATA USING MIXTURES

Heckerman, D. & Wellman, M.P. (1995). Bayesian Networks. Communications ACM, Vol.
28, No.3, pp. 27-30, March.
Hertz, J., Krogh, A., Palmer, R.G., (1991). Introduction Theory Neural Computation.
Addison Wesley.
Hinton, G.E., Dayan, P. Revow, M. (1997). Modeling manifold images handwritten
digits. IEEE T. Neural Networks 8, pp. 65-74.
Hornik, K., Stinchcombe, M., White, H., (1989). Multilayer FeedForward Networks Universal
Approximators. Neural Networks, No.2.
Hutchinson, A. (1994). Algorithmic Learning. New York: Oxford Univ. Press.
Izenman, A.J. (1991). Recent Developments Nonparametric Density Estimation. J. Amer. Statist. Assoc. Vol. 86, No. 413, pp. 205-224.
Jordan, M.I., Jacobs, R.A., (1994). Hierarchical Mixtures Experts EM Algorithm.
Neural Computation, 6, pp. 181-214.
Kohonen, T., (1989). Self-Organization Associative Memory. Springer-Verlag.
Lauritzen, S.L. & Spiegelhalter, D. J. (1988). Local Computations Probabilities Graphical
Structures Application Expert Systems. J. R. Statist. Soc. B. 50, No. 2, pp. 157224.
Li, M. Vitnyi, P. (1993). Introduction Kolmogorov Complexity Applications.
New York: Springer-Verlag.
McLachlan, G.J., Basford, K.E., (1988). Mixture Models. New York: Marcel Dekker.
McLachlan, G.J. Krishnan, T. (1997). EM Algorithm Extensions. John Wiley
Sons.
Michalski, R.S., Carbonell, J. Mitchell, T.M., eds. (1983). Machine Learning: Artificial
Intelligence approach. Palo Alto, CA: Tioga Press. reprinted Morgan Kaufmann
(Los Altos, CA).
Michalski, R.S., Carbonell, J. Mitchell, T.M., eds. (1986). Machine Learning: Artificial
Intelligence approach, Vol. II. Los Altos, CA: Morgan Kaufmann.
Mohgaddam, B. Pentland, A. (1997). Probabilistic Visual Learning Object Representation. IEEE PAMI, Vol. 19, No.7, 710. pp. 696-.
Merz, C.J. Murphy, P.M. (1996). UCI Repository machine learning databases.
[http://www.ics.uci.edu/~mlearn/MLRepository.html]. Irvine, CA: University California,
Department Information Computer Science.
Palm, H.C. (1994). New method generating statistical classifiers assuming linear mixtures
Gaussian densities, Proceedings 12th IAPR International Conference Pattern Recognition (Jerusalem, October 9-13, 1994), vol.2, IEEE, Piscataway, NJ, USA,
Papoulis, A., (1991). Probability, Random Variables Stochastic Processes. MCGraw-Hill.
Pearl, J., (1988). Probabilistic reasoning intelligent systems: Networks plausible inference.
Morgan Kaufmann.

215

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

Peng, F., Jacobs, R.A., Tanner, M.A. (1995). Bayesian Inference Mixtures-of-Experts Hierchical Mixtures-of-Experts Models Application Speech Recognition. Accepted
Journal American Statistical Association.
Priebe, C.E., Marchette, D.J., (1991). Adaptive mixtures: recursive nonparametric pattern recognition. Pattern Recognition, V24 N12, pp. 1197-1209.
Pudil, P., Novovicova, J., Choakjarernwanit, N., Kittler, J. (1995). Feature Selection Based
Approximation Class Densities Finite Mixtures Special Type. Pattern Recognition
Vol.28 No.9 pp. 1389-1398.
Quinlan, J.R., (1993). C4.5: Programs Machine Learning. San Mateo, CA: Morgan Kaufmann.
Redner, R.A., Walker, H.F., (1984). Mixture densities, maximum likelihood estimation
EM algorithm. SIAM Review, Vol. 26, pp. 195-239.
Rojas, R. (1996). Short Proof Posterior Probability Property Classifier Neural Networks. Neural Computation Vol.8 Issue 1, January.
Rumelhart, D.E., Hinton, G. E. Williams, R. (1986). Learning Internal Representations
Error Propagation Rumelhart, McClelland & PDP Group (1986), pp. 319-362.
Rumelhart, D.E., McClelland, J.L. & PDP Research Group. (1986). Parallel Distributed Processing: Explorations Microstructure Cognition, vol. 1, Foundations. Cambrigde
MA, Bradford Books/MIT Press.
Ruiz, A. (1995). nonparametric bound Bayes Error. Pattern Recognition, Vol. 28, No.
6, pp. 921-930.
Ruiz, A., Lpez-de-Teruel, P.E. Garrido, M.C. (1998). Kernel Density Estimation Indirect Observations. preparation.
Sung, K.-K. Poggio, T. (1998),. "Example Based Learning View-Based Human Face Detection". IEEE Trans. Pattern Analyisis Machine Intelligence. Vol.20, N.1. January, pp.
39-51.
Tanner, M.A. (1996). Tools statistical inference. (3rd ed.). Springer.
Thrun, S. et al (1991). "The MONK's Problems. performance Comparison Different Learning
Algorithms". Technical Report CMU-CS-91-197.
Titterington, D.M., A.F.M. Smith U.E. Makov (1985). Statistical Analysis Finite Mixture
Distributions, Wiley, New York.
Traven, H.G.C., (1991). Neural Network Approach Statistical Pattern Classification
"Semiparametric" Estimation Prob. Den. Func.. IEEE Neural Networks, V2 N3.
Valiant, L.G. (1993). View Computational Learning Theory Meyrowitz Chipman,
eds. (1993). Foundations Knowledge Acquisition: Machine Learning. Kluwer Acad. Pub.
Valiveti, R.S., Oommen, B.J., (1992). using chi-squared metric determining stochastic
dependence. Pattern Recognition, V25 N11 pp. 1389-1400.
Vapnik, V.N. (1982). Learning Dependencies based Empirical Data. Springer, New York.
Vapnik, V.N. (1995). Nature Statistical Learning Theory. Springer, New York.

216

fiPROBABILISTIC INFERENCE UNCERTAIN DATA USING MIXTURES

Wan, E.A., (1990). Neural Networks Classification: Bayesian Interpretation. IEEE Trans.
Neural Networks, V1 N4.
Weiss, Y. Adelson E.H. (1995). Perceptually organized EM: framework motion segmentation combines information form motion. TR MIT MLPCS TR 315.
Wilson, D.R. Martinez, T.R. (1997). Improved Heterogeneous Distance Functions. JAIR, V6,
pp. 1-34.
Wolpert, D.H., ed. (1994a). Mathematics Generalization. Proc. SFI/CLNS workshop
Formal Approaches Supervised Learning.
Xu, L. & Jordan, M.I. (1996). Convergence Properties EM Algorithm Gaussian
Mixtures. Neural Computation Vol.8 Issue 1, January.
You, Y.L. Kaveh, M. (1996). regularization approach joint blur identification image
restoration. IEEE Image Processing, vol 5.no.3, pp. 416-428.

217


