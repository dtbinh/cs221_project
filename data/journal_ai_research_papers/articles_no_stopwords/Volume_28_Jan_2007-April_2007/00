journal artificial intelligence

submitted published

cutset sampling bayesian networks
bozhena bidyuk
rina dechter

bbidyuk ics uci edu
dechter ics uci edu

school information computer science
university california irvine
irvine ca

abstract
presents sampling methodology bayesian networks samples
subset variables applies exact inference rest cutset sampling
network structure exploiting application rao blackwellisation principle sampling
bayesian networks improves convergence exploiting memory inference viewed anytime approximation exact cutset conditioning
developed pearl cutset sampling implemented efficiently
sampled variables constitute loop cutset bayesian network generally
induced width networks graph conditioned observed sampled variables bounded constant w demonstrate empirically benefit scheme
range benchmarks

introduction
sampling common method approximate inference bayesian networks
exact impractical due prohibitive time memory demands often
feasible offers performance guarantees given bayesian network
variables x x xn evidence e set samples x p x e
estimate f x expected value function f x obtained generated
samples via ergodic average
x
f x

e f x e f x

number samples f x shown converge exact value
increases central query interest bayesian networks computing posterior
marginals p xi e value xi variable xi called belief updating
query f x equals function equation reduces counting fraction
occurrences xi xi samples
p xi e



x
xi x






xi x iff xi xi xi x otherwise alternatively mixture estimator used

x

p xi xi

p xi e



c

ai access foundation rights reserved

fibidyuk dechter



xi x xi
significant limitation sampling however statistical variance increases
number variables network grows therefore number samples
necessary accurate estimation increases present sampling scheme
bayesian networks discrete variables reduces sampling variance sampling
subset variables technique known collapsing rao blackwellisation
fundamentals rao blackwellised sampling developed casella robert
liu wong kong gibbs sampling maceachern clyde
liu doucet gordon krishnamurthy importance sampling
doucet de freitas murphy russell extended rao blackwellisation particle
filtering dynamic bayesian networks
basic rao blackwellisation scheme described follows suppose partition space variables x two subsets c z subsequently write
function f x f c z generate samples distribution p c e compute e f c z c e perform sampling subset c generating samples
c c c approximating quantity interest
e f c z e ec ez f c z c e f x


x
ez f c z c e






posterior marginals estimates cutset variables obtained expression similar eq
x
ci c

p ci e


mixture estimator similar eq
p ci e

x

p ci ci e




xi x c e e p xi e ec p xi c e eq becomes
p xi e

x
p xi c e




since convergence rate gibbs sampler tied maximum correlation
two samples liu expect improvement convergence rate
sampling lower dimensional space since highly correlated variables may
marginalized dependencies variables inside smaller set likely
weaker variables farther apart sampling distributions
smoothed additionally estimates obtained sampling lower dimensional
space expected lower sampling variance therefore require fewer samples
achieve accuracy estimates hand cost generating
sample may increase indeed principles rao blackwellised sampling
applied classes probabilistic specialized structure kong liu
wong escobar maceachern liu doucet andrieu
andrieu de freitas doucet rosti gales


ficutset sampling bayesian networks

contribution presenting general structure scheme
applies rao blackwellisation principle bayesian networks idea exploit
property conditioning subset variables simplifies networks structure allowing efficient query processing exact general exact inference variable
elimination dechter join tree lauritzen spiegelhalter
jensen lauritzen olesen time space exponential induced width w
network however subset variables assigned e conditioned upon
induced width conditioned network may reduced
idea cutset sampling choose subset variables c conditioning
c yields sparse enough bayesian network small induced width allow exact
inference since sample assignment cutset variables efficiently generate
sample cutset variables conditioned network computation
p c e p xi c e bounded particular sampling set c cuts cycles
network e loop cutset inference conditioned network becomes
linear general c w cutset namely subset nodes assigned
induced width conditioned network w time space complexity computing
next sample c n dw maximum domain size n x

idea exploiting properties conditioning subset variables first proposed exact belief updating context cutset conditioning pearl
scheme requires enumerating instantiations cutset variables since number
instances exponential size cutset c sampling cutset space may
right compromise size cutset big thus sampling cutset
viewed anytime approximation cutset conditioning
although rao blackwellisation general cutset sampling particular applied context sampling introduce principle context gibbs sampling geman geman gilks richardson spiegelhalter
mackay markov chain monte carlo sampling method bayesian networks
extension sampling graphical markov
networks straight forward recently demonstrated idea incorporated importance sampling bidyuk dechter
defines analyzes cutset sampling scheme investigates empirically
trade offs sampling exact computation variety randomly generated
networks grid structure networks well known real life benchmarks cpcs
networks coding networks cutset sampling converges faster pure
sampling terms number samples dictated theory almost
time wise cost effective benchmarks tried demonstrate applicability
scheme deterministic networks hailfinder network coding networks
markov chain non ergodic gibbs sampling converge
section provides background information specifically section introduces bayesian
networks section reviews exact inference bayesian networks section provides background gibbs sampling contribution presenting
cutset sampling starts section section presents empirical evaluation cutset
sampling present empirical evaluation sampling variance resulting standard error method batch means details see geyer


fibidyuk dechter

section review previous application rao blackwellisation section provides
summary conclusions

background
section define essential terminology provide background information
bayesian networks
preliminaries
use upper case letters without subscripts x denote sets variables
lower case letters without subscripts denote instantiation group variables e g
x indicates variable set x assigned value use upper case letter
subscript xi denote single variable lower case letter
subscript xi denote instantiated variable e g xi denotes arbitrary value
domain xi means xi xi xi denotes domain variable xi
superscript subscripted lower case letter would used distinguish different specific
values variable e xi x x use x denote instantiation
set variables x x xi xi xi xn xi x xi denote x element
xi removed namely xi x x xi xi xn
definition graph concepts directed graph pair v e v
x xn set nodes e xi xj xi xj v set edges given
xi xj e xi called parent xj xj called child xi set
xi parents denoted pa xi pai set xi children denoted ch xi
chi family xi includes xi parents moral graph directed graph
undirected graph obtained connecting parents nodes
removing arrows cycle cutset undirected graph subset nodes
removed yields graph without cycles loop directed graph subgraph
whose underlying graph cycle directed graph acyclic directed loops
directed graph singly connected called poly tree underlying undirected
graph cycles otherwise called multiply connected
definition loop cutset vertex v sink respect loop l two
edges adjacent v l directed v vertex sink respect
loop l called allowed vertex respect l loop cutset directed graph
set vertices contains least one allowed vertex respect loop
definition belief networks let x x xn set random variables
multi valued domains x xn belief network bn pearl
pair g p g directed acyclic graph whose nodes variables x
p p xi pai n set conditional probability tables cpts associated
xi bn represents joint probability distribution product form
p x xn

n



p xi pa xi

evidence e instantiated subset variables e x


ficutset sampling bayesian networks

structure directed acyclic graph g reflects dependencies
variables separation criterion parents variable xi together children
parents children form markov blanket denoted markovi node xi
use xmarkovi denote x restricted variables markovi know node
xi independent rest variables conditioned markov blanket namely
p xi xi p xi xmarkovi
common query belief networks belief updating task
computing posterior distribution p xi e given evidence e query variable xi
x reasoning bayesian networks np hard cooper finding approximate
posterior marginals fixed accuracy np hard dagum luby abdelbar
hedetniemi network poly tree belief updating inference
tasks accomplished time linear size input general exact inference
exponential induced width networks moral graph
definition induced width width node ordered undirected graph
number nodes neighbors precede ordering width
ordering denoted w maximum width nodes induced width
ordered graph w width ordered graph obtained processing
nodes last first follows node x processed preceding neighbors
connected resulting graph called induced graph triangulated graph
task finding minimal induced width graph possible orderings
np complete arnborg
reasoning bayesian networks
belief propagation introduce section performs belief
updating singly connected bayesian networks time linear size input
pearl loopy networks two main approaches belief updating cutset
conditioning tree clustering often referred inference
briefly describe idea clustering section
conditioning method section
variable elimination join tree clustering jtc
join tree clustering jtc refers family including jointree propagation lauritzen spiegelhalter jensen et al bucket tree
elimination dechter idea first obtain tree decomposition
network clusters functions connected tree propagate messages
clusters tree tree decomposition singly connected undirected
graph whose nodes called clusters contain subsets variables input functions
defined variables tree decomposition must contain function
satisfy running intersection property maier unifying perspective treedecomposition schemes see zhang poole dechter b kask dechter larrosa
dechter
given tree decomposition network message propagation tree
synchronized select one cluster root tree propagate messages


fibidyuk dechter

tree message cluster vi neighbor vj function
separator set vi vj marginalization product functions vi
messages vi received neighbors besides vj assuming maximum
number variables cluster w maximum domain size time
space required process one cluster w since maximum number clusters
bounded x n complexity variable elimination cluster tree
propagation schemes n w parameter w maximum cluster size minus
called tree width tree decomposition minimal tree width identical
minimal induced width graph
iterative belief propagation ibp
belief propagation bp iterative message passing performs exact inference singly connected bayesian networks pearl iteration every node xi
sends j xi message child j receives j xi message child
message passing order organized converges two iterations essence
join tree clustering applied directly poly tree
applied bayesian networks loops usually iterates longer may
converge hence known iterative belief propagation ibp loopy belief propagation ibp provides guarantees convergence quality approximate posterior
marginals shown perform well practice rish kask dechter murphy
weiss jordan considered best inference coding networks
frey mackay kschischang frey finding probable variable
values equals decoding process mceliece mackay cheng ibp
requires linear space usually converges fast converges benchmarks ibp
converged within iterations less see section
cutset conditioning
tree width w bayesian network large requirements inference
schemes bucket elimination join tree clustering jtc exceed available memory
switch alternative cutset conditioning schemes pearl peot shachter
shachter andersen solovitz idea cutset conditioning select
subset variables c x e cutset obtain posterior marginals node
xi x c e
x
p xi e
p xi c e p c e

cd c

eq implies enumerate instantiations c perform exact inference cutset instantiation c obtain p xi c e p c e sum
total computation time exponential size cutset
enumerate instantiations cutset variables
c loop cutset nodes c assigned bayesian network
transformed equivalent poly tree p xi c e p c e computed via
bp time space linear size network example subset
loop cutset belief network shown figure left evidence e e right


ficutset sampling bayesian networks


b

c



f

e

g







b

c



f

e

g



figure nodes loopy bayesian network left instantiated
network transformed equivalent singly connected network right
transformation process replica observed node created
child node

figure shows equivalent singly connected network resulting assigning values

well known minimum induced width w network less
size smallest loop cutset bertele brioschi dechter namely
w c c thus inference approaches e g bucket elimination never
worse often better cutset conditioning time wise however w
large must resort cutset conditioning search order trade space time
considerations yield hybrid search inference since observed variables
break dependencies network network observed subset variables
c often transformed equivalent network smaller induced width wc
term adjusted induced width hence subset variables c x
observed complexity bounded exponentially adjusted induced width
graph wc
definition adjusted induced width given graph g x e adjusted
induced width g relative c denoted wc induced width c removed
moral graph
definition w cutset given graph g x e subset nodes c x
w cutset g adjusted induced width equals w
c w cutset quantities p xi c e p c e computed time
space exponential w much smaller tree width unconditioned
network resulting scheme requires memory exponential w time c n w
n size network maximum domain size thus performance
tuned available system memory resource via bounding parameter w
given constant w finding minimal w cutset minimize cutset conditioning
time hard several greedy heuristic approaches proposed
geiger fishelson bidyuk dechter elaborate
section


fibidyuk dechter

gibbs sampling
since complexity inference memory exponential networks induced
width tree width since resorting cutset conditioning scheme may take
much time w cutset size large must often resort approximation
methods sampling methods bayesian networks commonly used approximation
techniques section provides background gibbs sampling markov chain monte
carlo method one popular sampling schemes focus
although method may applied networks continuous distributions
limit attention discrete random variables finite domains
gibbs sampling bayesian networks
ordered gibbs sampler
input belief network b x x xn evidence e xi ei xi e x
output set samples x

initialize assign random value xi variable xi x e xi assign
evidence variables observed values
generate samples
generate sample x

n compute value xi variable xi



compute distribution p xi xmarkovi sample xi p xi xmarkovi


set xi xi
end
end

figure gibbs sampling
given bayesian network variables x x xn evidence e gibbs
sampling geman geman gilks et al mackay generates set


samples x sample x x xn instantiation variables

superscript denotes sample index xi value xi sample first

sample initialized random generating sample sample xi

value variable xi sampled probability distribution p xi xi recall




p xi xi p xi x











xi xi xn denote xi p xi xi


next sample xi
generated previous sample xi following one two
schemes
random scan gibbs sampling given sample x iteration pick variable

xi random sample value xi conditional distribution xi p xi xi
leaving variables unchanged
systematic scan ordered gibbs sampling given sample x sample
value variable order




x p x x x x
n


ficutset sampling bayesian networks



x p x x



x x
n





xi p xi x





xi xi x
n





xn p xn x



x



xn


bayesian networks conditional distribution p xi xi dependent




assignment markov blanket variable xi thus p xi xi p xi xmarkovi


xmarkovi restriction x markovi given markov blanket xi sampling
probability distribution given explicitly pearl



p xj x

p xi xmarkovi p xi x

paj
pai
j xj chj

thus generating complete sample done n r multiplication steps
r maximum family size n number variables
sequence samples x x viewed sequence states markov



chain transition probability state x
xi xi xi xn state












x
xi xi
xi xn defined sampling distribution p xi xi
construction markov chain induced gibbs sampling invariant distribution
p x e however since values assigned gibbs sampler variables sample
x depend assignment values previous sample x follows
sample x n depends initial state x convergence markov chain
defined rate distance distribution p x n x e
stationary distribution p x e e variational distance l distance converges
function n intuitively reflects quickly inital state x forgotten
convergence guaranteed markov chain ergodic pearl gelfand
smith mackay markov chain finite number states ergodic
aperiodic irreducible liu markov chain aperiodic
regular loops markov chain irreducible get state si state
sj including si non zero probability finite number steps irreducibility
guarantees able visit number samples increases statistically
important regions state space bayesian networks conditions almost
satisfied long conditional probabilities positive tierney
ensure collected samples drawn distribution close p x e
burn time may allocated namely assuming takes k samples
markov chain get close stationary distribution first k samples may included computation posterior marginals however determining k hard jones
hobert general burn optional sense convergence
estimates correct posterior marginals depend completeness
sake given figure
p
convergence conditions satisfied ergodic average ft x f xt
function f x guaranteed converge expected value e f x increases


fibidyuk dechter

words ft x e f x finite state markov chain
irreducible aperiodic following applies see liu theorem

ft x e f x n f

initial assignment x variance term f defined follows
f f
var f x h integrated autocorrelation time
focus computing posterior marginals p xi e xi x e
posterior marginals estimated histogram estimator
p xi xi e


x
xi x


p xi xi e


x

p xi xi


mixture estimator









histogram estimator corresponds counting samples xi xi namely xi x

xi xi equals otherwise gelfand smith pointed since
mixture estimator estimating conditional expectation sampling variance
smaller due rao blackwell theorem thus mixture estimator preferred


since p xi xi p xi xmarkovi mixture estimator simply average conditional
probabilities

x

p xi e
p xi xmarkovi




mentioned markov chain ergodic p xi e converge exact
posterior marginal p xi e number samples increases shown roberts
sahu random scan gibbs sampler expected converge faster
systematic scan gibbs sampler ultimately convergence rate gibbs sampler depends
correlation two consecutive samples liu schervish carlin
liu et al review subject next section
variance reduction schemes
convergence rate gibbs sampler depends strength correlations
samples states markov chain term correlation
used mean samples dependent mentioned earlier case
finite state irreducible aperiodic markov chain convergence rate expressed
maximal correlation states x x n see liu ch practice
convergence rate analyzed covariance cov f x f x f
function called auto covariance
convergence estimates exact values depends convergence
rate markov chain stationary distribution variance estimator


ficutset sampling bayesian networks

factors contribute value term f eq two main
approaches allow reduce correlation samples reduce sampling variance
estimates blocking grouping variables together sampling simultaneously
collapsing integrating random variables sampling subset
known rao blackwellisation
given joint probability distribution three random variables x z
depict essence three sampling schemes follows
standard gibbs
x p x z




z





p x



p z x





z











collapsed variable z integrated
x p x






p x






blocking grouping x together
x p x z
z





p z x










blocking reduces correlation samples grouping highly correlated
variables blocks collapsing highly correlated variables marginalized
smoothing sampling distributions remaining variables
p x smoother p x z approaches lead reduction sampling
variance estimates speeding convergence exact values
generally blocking gibbs sampling expected converge faster standard gibbs
sampler liu et al roberts sahu variations scheme
investigated jensen et al kjaerulff given number samples
estimate resulting collapsed gibbs sampler expected lower variance
converge faster estimate obtained blocking gibbs sampler liu et al
thus collapsing preferred blocking analysis collapsed gibbs sampler
found escobar maceachern liu
caveat utilization collapsed gibbs sampler computation
probabilities p x p x must efficient time wise case bayesian networks
task integrating variables equivalent posterior belief updating
evidence variables sampling variables observed time complexity therefore
exponential adjusted induced width namely effective width network
dependencies broken instantiated variables evidence sampled


fibidyuk dechter

importance sampling
since sampling target distribution hard different sampling methods explore
different trade offs generating samples obtaining estimates already discussed
gibbs sampling generates dependent samples guarantees convergence sampling
distribution target distribution alternative called importance sampling
generate samples sampling distribution q x different p x e
include weight w p x e q x sample x computation
estimates follows

x
f xt w
ft x






convergence ft x e f x guaranteed long condition p x e
q x holds convergence speed depends distance q x
p x e
one simplest forms importance sampling bayesian networks likelihood
weighting fung chang shachter peot processes variables topological order sampling root variables priors remaining variables
conditional distribution p xi pai defined conditional probability table evidence variables assigned observed values sampling distribution close
prior usually converges slowly evidence concentrated around
leaf nodes nodes without children probability evidence small adaptive called dynamic importance sampling method attempts speed
convergence updating sampling distribution weight previously generated samples adaptive importance sampling methods include self importance sampling
heuristic importance sampling shachter peot recently ais bn cheng
druzdzel epis bn yuan druzdzel empirical section
compare performance proposed cutset sampling ais bn
considered state art importance sampling date although epis bn
shown perform better networks hence describe ais bn
detail
ais bn observation could sample node
topological order distribution p xi pai e resulting sample would drawn
target distribution p x e since distribution unknown variable
observed descendants ais bn initializes sampling distributions p xi pai e
equal p xi pai uniform distribution updates distribution
p k xi pai e every l samples next sampling distribution p k xi pai e
closer p xi pai e p k xi pai e follows
p k xi pai e p k xi pai e k p xi pai e p k xi pai e
k positive function determines learning rate p xi pai e
estimate p xi pai e last l samples


ficutset sampling bayesian networks

cutset sampling
section presents cutset sampling scheme discussed sampling
cutset guaranteed statistically efficient cutset sampling scheme computationally efficient way sampling collapsed variable subset c x tying
complexity sample generation structure bayesian network
cutset sampling
cutset sampling scheme partitions variable set x two subsets c x c
objective generate samples space c c c cm sample c
instantiation variables c following gibbs sampling principles

wish generate sample c sampling value ci probability distribution




p ci ci p ci c
c
ci ci cm use left arrow denote


value ci drawn distribution p ci ci



ci p ci ci e




compute probability distribution p ci ci e efficiently sampling
variable ci c generate samples efficiently relevant conditional distributions computed exact inference whose complexity tied network structure denote jt c b xi e generic class variable elimination
join tree clustering given belief network b evidence e outputs
posterior probabilities p xi e variable xi x lauritzen spiegelhalter
jensen et al dechter networks identity clear use
notation jt c xi e
cutset sampling
input belief network b cutset c c cm evidence e
output set samples ct
initialize assign random value c ci c assign e
generate samples
generate sample c follows

compute value ci variable ci follows

compute jt c ci ci e




b compute p ci ci e p ci ci e
c sample


ci
p ci ci e
end
end



figure w cutset sampling
therefore sampling variable ci value ci ci compute




p ci ci e via jt c ci ci e obtain p ci ci e via normalization p ci ci e


p ci ci e



fibidyuk dechter

cutset sampling uses systematic scan gibbs sampler given figure
clearly adapted used random scan gibbs sampler well steps
c generate sample sample every variable ci c sequence

main computation step distribution p ci ci e ci generated
requires executing jt c every value ci ci separately step b
conditional distribution derived normalization finally step c samples value

obtained distribution note use p ci ci e short hand notation








p ci c
ci ci ck e namely sample value variable
ci values variables c ci already updated
next demonstrate process special case loop cutset see definition
example consider belief network previously shown figure observed node
e e loop cutset begin sampling process initializing sampling variables
next compute sample values follows
p e



pjt c c e







p e




p e
pjt c e








p e



process corresponds two iterations inner loop figure eq
sample value variable correspond steps c first iteration second
iteration eq sample value variable since conditioned network
poly tree figure right computing probabilities pjt c e pjt c e via jt c
reduces pearls belief propagation distributions computed linear time

estimating posterior marginals
set samples subset variables c generated estimate posterior
marginals variable network mixture estimator sampling variables
estimator takes form similar eq

x

p ci e
p ci ci e






variables x c e posterior marginal estimator

x
p xi e
p xi c e






use jt c xi c e obtain distribution p xi c e input bayesian
network conditioned c e shown

maintain running sum computed distributions p ci ci e p xi c e
sample generation sums right hand side eq readily
available noted estimators p ci e p xi e guaranteed converge corresponding exact posterior marginals increases long markov


ficutset sampling bayesian networks

chain cutset c ergodic cutset variables estimator simple
ergodic average xi x c e convergence derived directly first
principles
theorem given bayesian network b x evidence variables e x cutset
c x e given set samples c c c obtained via gibbs sampling
p c e assuming markov chain corresponding sampling c ergodic
xi x c e assuming p xi e defined eq p xi e p xi e

proof definition

x
p xi c e
p xi e






instead summing samples rewrite expression sum
possible tuples c c group together samples corresponding tuple
instancep
c let q c denote number times tuple c c occurs set samples
cd c q c easy see
p xi e

fraction

q c


x

cd c

p xi c e

q c




histogram estimator posterior marginal p c e thus get
x
p xi e
p xi c e p c e

cd c

since markov chain formed samples c ergodic p c e p c e
therefore
x
p xi e
p xi c e p c e p xi e
cd c



complexity
time space complexity generating samples estimating posterior marginals
via cutset sampling dominated complexity jt c line
figure linear amount additional memory required maintain running

sums p ci ci e p xi c e used posterior marginal estimators
sample generation complexity
clearly jt c applied network b conditioned cutset variables c
evidence variables e complexity time space exponential induced width
w conditioned network n w c w cutset see definition


fibidyuk dechter

notion w cutset balance sampling exact inference one end
spectrum plain gibbs sampling sample generation fast requiring
linear space may high variance end exact
requiring time space exponential induced width moral graph
two extremes control time space complexity w follows
theorem complexity sample generation given network b x evidence e w cutset c complexity generating sample time space
c n w bounds variables domain size n x
proof c w cutset maximum domain size complexity

computing joint probability p ci ci e conditioned network n w
since operation must repeated ci ci complexity processing one

variable computing distribution p ci ci e n w n w finally
since ordered gibbs sampling requires sampling variable cutset generating one
sample c n w

complexity estimator computation
posterior marginals cutset variable ci c easily obtained end
sampling process without incurring additional computation overhead mentioned earlier

need maintain running sum probabilities p ci ci e ci ci
estimating p xi e xi x c e eq requires computing p xi c e
sample c generated summary
theorem computing marginals given w cutset c complexity computing posteriors variables xi x e samples cutset variables
c n w
proof showed theorem complexity generating one sample c
n w sample c generated computation posterior marginals
remaining variables requires computing p xi c e via jt c xi c e
n w combined computation time one sample c n w
n w c n w repeating computation samples yields
c n w
note space complexity w cutset sampling bounded n w

complexity loop cutset
cutset c loop cutset jt c reduces belief propagation pearl

computes joint distribution p ci ci e linear time refer
special case loop cutset sampling general w cutset sampling
loop cutset w cutset w equals maximum number unobserved
parents upper bounded maximum indegree node however since processing
poly trees linear even large w induced width capture complexity


ficutset sampling bayesian networks

properly notion loop cutset could better captured via hyperwidth
network gottlob leone scarello kask et al hyperwidth polytree therefore loop cutset defined hypercutset alternatively
express complexity via networks input size captures total size
conditional probability tables processed follows
theorem complexity loop cutset sample generation c loop cutset
complexity generating sample c size input
network
proof loop cutset network instantiated belief propagation bp

compute joint probability p ci ci e linear time pearl yielding total
time space c sample

optimizing cutset sampling performance
analysis complexity generating samples theorem overly pessimistic
assuming computation sampling distribution variable cutset
independent variables may change value moving one sample
next change occurs one variable time sequence much
computation retained moving one variable next
sampling cutset variables done efficiently
reducing factor n c theorem n c bounds number
clusters tree decomposition used jt c contains node ci c assume
control order cutset variables sampled

x
x x





yn

yn

x

x

xn

xn

x x

x x

xn xnyn

figure bayesian network top corresponding cluster tree bottom
consider simple network variables x x xn yn cpts
p xi xi yi p yi xi defined every shown figure top join tree
network chain cliques size given figure bottom since loopcutset sample variables lets assume use ordering yn
generate sample given current sample ready generate next sample
applying jt c bucket elimination network whose cutset variables assigned


fibidyuk dechter

makes network effectively singly connected leaves actual variables
cluster sends message cluster containing xn towards
cluster containing x cluster x x gets relevant message cluster
x x sample accomplished linear computations clique
x x yi yi yielding desired distribution p multiply
functions incoming messages cluster sum x x normalize
cutset w cutset computation single clique w
p sampled assigned value cluster x x
sends message cluster x x information necessary
compute p w p available value sampled
cluster computes sends message cluster x x
end obtain full sample via two message passes conditioned network
computation complexity n w example generalized follows
theorem given bayesian network n variables w cutset c tree decomposition
tr given sample c c c sample generated n c w
maximum number clusters containing variable ci c
proof given w cutset c definition exists tree decomposition tr network
includes cutset variables cutset variables c removed
number variables remaining cluster tr bounded w lets impose
directionality tr starting arbitrary cluster call r shown figure let
tci denote connected subtree tr whose clusters include ci figure clarity
collapse subtree ci single node assume cutset nodes
sampled depth first traversal order dictated cluster tree rooted r

tc r
tc
tck
tc

tc

tc

tc
figure cluster tree rooted cluster r subtree cutset node ci
collapsed single node marked tci



ficutset sampling bayesian networks

given sample c jt c send messages leaves tr towards root cluster
assume without loss generality r contains cutset node c first
sampled c jtc pass messages root clusters restricted

tc note r tc messages p c c c computed
w repeat computation value c involving
clusters tc obtain distribution p c w sample value
c thus c appears clusters number message passing computations
initial n pass generate first distribution p c
w
next node depth first traversal order tc thus second variable
sampled c distance variables c c denoted dist shortest
path along tr cluster contains c cluster contains c apply jtcs
mesage passing along path take dist w
obtain conditional distribution p c recompute messages subtree
tc value c c w continue computation similar
manner cutset nodes
jt c traverses tree depth first order needs pass messages along
p c
edge twice see figure thus sum distances traveled disti
n may repeated computation value sampled variable
however accomplished via message passing restricted individual variables
subtrees bounded conclude full sample generated
n c w
worthwhile noting complexity generating sample reduced
factor amounts factor noticing whenever



move variable ci ci joint probability p c
ci
ci ck
already available previous round recomputed need




compute p c
ci
ci ck ci ci buffering last computed
joint probability need apply jt c times therefore total
complexity generating sample n c w
example figure demonstrates application enhancements discussed
depicts moral graph already triangulated corresponding join tree b
bayesian network figure evidence variable e removed variables b form
cutset join tree network cutset evidence variables removed shown
figure c since removing e cluster df e leaves one variable f
combine clusters bdf df e one cluster f g assuming cutset variables
domains size initialize b b
selecting cluster ac root tree jt c first propagates messages leaves
root shown figure c computes p b e cluster ac next
set b b updating functions containing variable b propagating messages
subtree b consisting clusters ac cf figure obtain p b e
normalizing two joint probabilities obtain distribution p b e sample
value b assume sampled value b


fibidyuk dechter

abc
p b p c
p



ac
p b p c
p

ac
p b p c
p

ac
p b p c
p

ac
p b p c
p

cf
p f b c p b

cf
p f b c p b

cf
p f b c p b

cf
p f b c p b

fg
p e f p g f

fg
p e f p g f

fg
p e f p g f

fg
p e f p g f

b
c

bcf
p f b c

f
g

dfg
p b p g f



e

dfe
p e f

b b e e


b

c

b b







e

f

figure join tree width b moral graph transformed join tree
width c evidence variable e cutset variables b instantiated process clusters bdf bcf merged cluster cf
clusters contain variables functions original network cutset
nodes domains size b b b starting
sample b messages propagated c e first sample value
variable b variable e messages propagated
tree compute posterior marginals p b e rest variables f

next need compute p b e sample value variable joint
probability p b e readily available since computed sampling value
b thus set compute second probability p b e updating functions
clusters cf f g sending updated message cf f g figure e
obtain distribution p b e normalizing joint probabilities sample
value since value changed latest computation update
functions clusters cf f g propagate updated messages subtree cd
send message cf f g
order obtain distributions p b e remaining variables c f
g need send updated messages join tree f g cf
cf ac shown figure f last step serves initialization
step next sample generation
example performance cutset sampling significantly better worst
case sent total messages generate sample worst case
suggests least n c messages n equals number clusters


ficutset sampling bayesian networks

finding w cutset
clearly w cutset sampling effective w cutset small calls
task finding minimum size w cutset np hard yet several heuristic
proposed next briefly survey proposals
larossa dechter obtain w cutset processing variables elimination
order next node eliminated selected triangulation heuristics added
cutset current induced width degree greater w geiger fishelson
agument idea heuristics
bidyuk dechter select variables included cutset greedy
heuristics nodes basic graph properties degree node one
scheme starts empty w cutset heuristically adds nodes cutset
tree decomposition width w obtained scheme starts set
c x e containing nodes network cutset removes nodes
set order stops removing next node would tree
decomposition width w
alternatively bidyuk dechter proposed first obtain tree decomposition
network minimal w cutset tree decomposition nphard via well known greedy used set cover
shown yield smaller cutset previously proposed heuristics used finding
w cutset experiments section modification tree decomposition
computed time node removed tree added w cutset

experiments
section present empirical studies cutset sampling several classes
use mean square error posterior marginals estimates
measure accuracy compare traditional gibbs sampling likelihood weighting
fung chang shachter peot state art ais bn adaptive
importance sampling cheng druzdzel implemented ais bn
parameters specified cheng druzdzel implementation
made sure sampling used data access routines
error measures providing uniform framework comparing performance
reference report performance iterative belief propagation ibp
methodology
section detail describe methodology used implementation decisions made
apply collection empirical
sampling methodology
sampling restarted markov chain every samples samples
chain batch averaged separately
pm xi e


x
p xi c e





fibidyuk dechter

final estimate obtained sample average chains

x
pm xi e
p xi e



restarting markov chain known improve sampling convergence rate single
chain become stuck generating samples single high probability region without
ever exploring large number high probability tuples restarting markov chain
different random point sampling achieve better coverage
sampling space experiments observe significant difference
estimates obtained single chain size chains size therefore
choose report multiple markov chains however rely
independence random values pm xi e estimate confidence interval p xi e
implementation gibbs sampling schemes use zero burn time see
section mentioned earlier idea burn time throw away
first k samples ensure remaining samples drawn distribution close
target distribution p x e conservative methods estimating k drift
minorization conditions proposed rosenthal roberts tweedie
required analysis beyond scope consider
comparison gibbs sampling cutset sampling objective fair
sense schemes use k experimental showed positive
indication burn time would beneficial practice burn pre processing
time used high probability regions distribution p c e
case initially spends disproportionally large period time low probability regions
discarding large number low probability tuples obtained initially frequency
remaining high probability tuples automatically adjusted better reflect weight
cpcs b n e w

cpcs b n e lc w

lcs


e



unique samples

e

e

mse

e
e
e
e

lcs








e



e

























samples

samples

figure comparing loop cutset sampling mse vs number samples left number unique samples vs number samples right cpcs b
averaged instances different observations

benchmarks observed full gibbs sampling cutset sampling
able high probability tuples fast relative number samples generated
example one benchmarks cpcs b rate generating unique samples


ficutset sampling bayesian networks

namely ratio cutset instances seen number samples
decreases time specifically loop cutset sampling generates unique tuples
first samples additional unique tuples generating next samples
rate generating unique tuples slows per samples range
samples shown figure right means first
hundred samples spends time revisiting high probability
tuples benchmarks number unique tuple instances generated increases
linearly cpcs thus tuples appear distributed nearly uniformly
case need burn strongly expressed heavy weight
tuples instead burn times sample initial variable values posterior
marginal estimates generated ibp experiments sampling time includes
pre processing time ibp
experiments performed ghz cpu
measures performance
instance defined bayesian network b variables x x xn
evidence e x derived exact posterior marginals p xi e bucket tree
elimination dechter computed mean square error mse
approximate posterior marginals p xi e mse defined
x x

p xi e p xi e
se p
x


xi x e
xi x e xi

mean square error primary accuracy measure consistent
across well known measures average absolute error kl distance squared
hellingers distance loop cutset sampling absolute error
averaged values unobserved variables
x x

p
p xi e p xi e
xi x e xi
xi x e xi

kl distance dk distribution p xi e estimator p xi e defined
follows
x
p xi e
dk p xi e p xi e
p xi e lg
p xi e
x


benchmark instance compute kl distance variable xi x e
average
x

dk p xi e p xi e
dk p p
x e
xi x e

squared hellingers distance dh distribution p xi e estimator
p xi e obtained
q
x p
dh p xi e p xi e
p xi e p xi e
xi



fibidyuk dechter

average squared hellingers distance benchmark instance average
distances posterior distributions one variable
dh p p


x e

x

xi x e

dh p xi e p xi e

average errors different network instances averaged instances
given network typically instances
report confidence interval estimate p xi e similar
well known batch means method billingsley geyer steiger wilson
since chains restarted independently estimates pm xi e independent
thus confidence interval obtained measuring variance estimators
p xi e report section
benchmarks
experimented four classes networks
cpcs considered four cpcs networks derived computer patient
case simulation system parker miller pradhan provan middleton henrion
cpcs network representation internist miller pople myers
quick medical reference qmr miller masarie myers expert systems nodes cpcs networks correspond diseases findings conditional
probabilities describe correlations cpcs network consists n nodes
relatively large loop cutset size lc nodes induced width
cpcs network consists n nodes induced width w
small loop cutset size lc relatively large corresponding adjusted induced
width wlc cpcs b larger cpcs network nodes adjusted induced
width loop cutset lc exact inference cpcs b averaged minutes
largest network cpcs b consisted nodes induced width w
loop cutset size exact inference time cpcs b minutes
hailfinder network small network nodes exact inference
hailfinder network easy since loop cutset size yet network
zero probabilities therefore good benchmark demonstrating convergence
cutset sampling contrast gibbs sampling
random networks experimented several classes random networks random networks layer networks grid networks random networks generated
n binary nodes domains size first nodes x x
designated root nodes non root node xi assigned parents selected
randomly list predecessors x xi refer class random
networks multi partite random networks distinguish bi partite layer random
networks random layer networks generated root nodes first layer
leaf nodes second layer yielding total nodes sample layer random
network shown figure left non root node second layer assigned
parents selected random among root nodes nodes assigned domain size
xi x x


ficutset sampling bayesian networks

figure sample random networks layer left grid center coding right

layer multi partite random networks root nodes assigned uniform priors conditional probabilities chosen randomly namely value
p x pai drawn uniform distribution interval used compute
complementary probability value p x pai p x pai
directed grid networks opposed grid shaped undirected markov random
fields size x nodes constructed uniform priors single
root node random conditional probability tables described sample grid
network shown figure center networks average induced width
size exact inference bucket elimination required minutes
regular structure largest loop cutset containing nearly half
unobserved nodes
coding networks experimented coding networks code bits
parity check bits parity check matrix randomized parity check bit three
parents sample coding network code bits parity checking bits total
transmitted bits shown figure center total number variables network
experiments code bits parity check bits transmitted bit
code parity check bit average loop cutset size induced width
markov chain produced gibbs sampling whole coding network
ergodic due deterministic parity check function gibbs sampling
converge however markov chain corresponding sampling subspace coding
bits ergodic thus cutset sampling schemes converged
next two sections
networks except coding grid networks evidence nodes selected random
among leaf nodes nodes without children since grid network one leaf
node evidence grid networks selected random among nodes
benchmark report chart title number nodes network n average
number evidence nodes e size loop cutset lc average induced width
input instance denoted w distinguish induced width w network adjusted
w cutset


fibidyuk dechter

loop cutset sampling
section compare loop cutset sampling pure gibbs sampling likelihood
weighting ais bn ibp benchmarks cutset selected evidence
sampling nodes together constitute loop cutset network
proposed becker et al accuracy gibbs loop cutset sampling
function number samples time
cpcs networks summarized figures loop cutset curve
chart denoted lcs loop cutset sampling induced width network
wlc loop cutset nodes observed specified caption identical
largest family size poly tree generated cutset variables removed plot
time x axis accuracy mse axis cpcs networks ibp
converged converged fast within seconds consequently ibp curve
straight horizontal line change convergence achieved
curves corresponding gibbs sampling loop cutset sampling likelihood weighting
ais bn demonstrate convergence sampling schemes time three
cpcs networks loop cutset sampling converges much faster gibbs sampling
exception cpcs b figure right induced width conditioned singlyconnected network remains high wlc due large family sizes thus loop cutset
sampling generates samples slowly samples second compared gibbs sampling
samples second since computing sampling distribution exponential w sampling
single variable variables domains size although loopcutset sampling shows significant reduction mse function number samples
figure left enough compensate two orders magnitude difference
loop cutset rate sample generation cpcs figure cpcs figure
cpcs b figure loop cutset sampling achieves greater accuracy ibp within
seconds less
comparison importance sampling schemes observe ais bn consistently outperforms likelihood weighting ais bn slightly better loopcutset sampling cpcs probability evidence p e relatively high
cpcs probability evidence p e e smaller lcs outperforms ais bn
gibbs sampling curves falls ais bn likelihood weighting gibbs
sampling loop cutset sampling outperform ais bn cpcs b cpcs b
probability evidence small cpcs b average p e e cpcs b probability evidence varies e e note likelihood weighting ais bn
performed considerably worse gibbs sampling loop cutset sampling
benchmarks function number samples consequently left
charts showing convergence gibbs loop cutset sampling function
number samples order zoom two focus
empirical studies
coding networks coding networks shown figure
computed error measures coding bits averaged instances instances
different observed values networks different coding matrices
noted earlier markov chains corresponding gibbs sampling coding networks
ergodic gibbs sampling converge however markov


ficutset sampling bayesian networks

gibbs

cpcs n lc w e

lw

cpcs n lc w e

lcs

e

ais bn

e

gibbs

ibp

e

e

lcs

e

ibp

e

mse

mse

e
e
e

e
e

e

e

e

e

e






















samples
lw

cpcs n lc w e







hellinger distance

lcs

e

ais bn

e

gibbs

e

lw

cpcs n lc w e

ais bn

e

kl distance



time sec

ibp

e
e
e
e

gibbs

e

lcs

e

ibp

e
e
e
e
e

e






























time sec

time sec
lw

cpcs n lc w e

ais bn

e

absolute error



gibbs

e

lcs

e

ibp

e
e
e
e
e
















time sec

figure comparing loop cutset sampling lcs wlc gibbs sampling hereby referred
gibbs likelihood weighting lw ais bn ibp cpcs network
averaged instances showing mse function number samples
top left time top right kl distance middle left squared hellingers
distance middle right average error bottom function time



fibidyuk dechter

cpcs n lc w e

lw

cpcs n lc w e

lw

e

ais bn

e

ais bn

gibbs

gibbs
lcs

ibp

e

ibp

e

mse

absolute error

lcs

e

e

e

e






























cpcs n lc w e

lw

cpcs n lc w e

lw

e

ais bn

ais bn

e

gibbs

e

hellinger distance

gibbs

kl distance



time sec

time sec

lcs
ibp

e
e
e
e

lcs

e

ibp

e

e

e
























time sec









time sec
lw

cpcs n lc w e

ais bn

e

gibbs
absolute error

lcs
ibp

e

e

e
















time sec

figure comparing loop cutset sampling lcs wlc gibbs sampling likelihood
weighting lw ais bn ibp cpcs network averaged instances showing mse function number samples top left time
top right kl distance middle left squared hellingers distance middle
right average error bottom function time



ficutset sampling bayesian networks

cpcs b n lc w e

cpcs b n lc w e
e

gibbs

e

lw
ais bn

lcs

e

gibbs

ibp

e

lcs

mse

mse

e

e

ibp

e

e

e

e




















samples

cpcs b n lc w e







cpcs b n lc w e
lw

e

ais bn

hellinger distance

gibbs

e

lcs
ibp

e

lw

e

ais bn

kl distance



time sec

e
e

gibbs

e

lcs

e

ibp

e
e
e

























time sec









time sec

cpcs b n lc w e
e

lw

absolute error

ais bn
gibbs

e

lcs
ibp

e

e














time sec

figure comparing loop cutset sampling lcs wlc gibbs sampling likelihood
weighting lw ais bn ibp cpcs b network averaged instances showing mse function number samples top left time
top right kl distance middle left squared hellingers distance middle
right average error bottom function time



fibidyuk dechter

cpcs b n lc w e

gibbs

cpcs b n lc w e

lcs

e

lw
ais bn

e

gibbs

ibp

e

lcs

e

mse

e

mse

ibp

e

e
e

e

e
e
e

e




















lw

cpcs b n lc w e







lw

cpcs b n lc w e

ais bn

e

ais bn

gibbs

e

lcs

e

ibp

gibbs

e

hellinger distance

kl distance



time sec

samples

e
e
e
e

lcs
e

ibp

e
e
e
e





















time sec









time sec
cpcs b n lc w e

lw
ais bn

e

gibbs

absolute error

lcs
ibp

e

e

e














time sec

figure comparing loop cutset sampling lcs wlc gibbs sampling likelihood
weighting lw ais bn sampling ibp cpcs b network averaged
instances showing mse function number samples top
left time top right kl distance middle left squared hellingers
distance middle right average error bottom function time



ficutset sampling bayesian networks

e

e

e

e
e

e

e

e


















time sec







time sec
lw
ais bn
gibbs
lcs
ibp

e

coding n p lc w

lw
ais bn
gibbs
lcs
ibp

e

hellinger distance

coding n p lc w
e

kl distance

lw
ais bn
gibbs
lcs
ibp

e

mse

absolute error

coding n p lc w

lw
ais bn
gibbs
lcs
ibp

coding n p lc w
e

e
e
e
e
e

e
e
e
e
e















time sec











time sec

figure comparing loop cutset sampling lcs wlc gibbs sampling likelihood
weighting lw ais bn ibp coding networks averaged
instances coding networks instances total graphs average absolute error top left mse top right kl distance bottom left
squared hellingers distance bottom right function time

chain corresponding sampling subspace code bits ergodic therefore
loop cutset sampling samples subset coding bits converges even achieves
higher accuracy ibp time reality ibp certainly preferable coding
networks since size loop cutset grows linearly number code bits
random networks random multi part networks figure top random
layer networks figure middle loop cutset sampling converged faster
gibbs sampling averaged instances network type
cases loop cutset sampling achieved accuracy ibp seconds less layer
networks iterative belief propagation performed particularly poorly gibbs sampling
loop cutset sampling obtained accurate less second
hailfinder network used network addition coding networks compare behavior cutset sampling gibbs sampling deterministic networks since
hailfinder network contains many deterministic probabilities markov chain corresponding gibbs sampling variables non ergodic expected gibbs sampling fails
loop cutset sampling computes accurate marginals figure


fibidyuk dechter

random n e c w

layer r p n e lc w

gibbs

e

lcs

e

ibp

e

gibbs
lcs

e

ibp

mse

mse

e
e
e

e
e

e
e

e




















time sec









time sec

figure comparing loop cutset sampling lcs gibbs sampling ibp random
networks left layer random networks right wlc classes
networks averaged instances mse function time

hailfinder n lc w e

gibbs

e

lcs
ibp

mse

e

e

e
















time sec

figure comparing loop cutset sampling lcs wlc gibbs sampling ibp
hailfinder network instances mse function time

summary empirical demonstrate loop cutset sampling cost effective

time wise superior gibbs sampling measured ratio r mgc number
samples mg generated gibbs number samples mc generated loop cutset
sampling time period relatively constant given network
changes slightly instances differ observations cpcs
cpcs cpcs b cpcs b ratios correspondingly
see table section obtained r random networks r
random layer networks ratio values indicate gibbs sampler generates


ficutset sampling bayesian networks

samples faster loop cutset sampling usually case instances
variance reduction compensated increased computation time fewer samples
needed converge resulting overall better performance loop cutset sampling
compared gibbs sampling cases however reduction sample size
compensates overhead computation sampling one variable value
cases loop cutset sampling generated samples faster gibbs yielding ratio r
improvement accuracy due larger number samples
faster convergence
w cutset sampling
section compare general w cutset scheme different values w
gibbs sampling main goal study performance w cutset sampling
varies w completeness sake include loop cutset sampling shown
section
empirical study used greedy set cover mentioned
section finding minimal w cutset apply manner
w cutset proper subset w cutset thus expected
lower variance converge faster sampling w cutset terms number samples
required following rao blackwellisation theory focus empirical study
trade offs cutset size reduction associated increase sample generation
time gradually increase bound w
used benchmarks included grid networks sampling
given fixed time bound sampling small networks cpcs
w cpcs w exact inference easy sampling
allocated seconds seconds respectively larger networks allocated
seconds depending complexity network fraction exact
computation time
table reports size sampling set used column
reports size corresponding w cutset example cpcs b average
size gibbs sample nodes except evidence loop cutset size size
cutset table shows rate sample generation different
per second observed previously case loop cutset sampling
special cases cutset sampling generated samples faster gibbs sampler
example cpcs b loop cutset sampling cutset sampling generated samples
per second gibbs sampler able generate samples attribute
size cutset sample nodes less reported table compared
size gibbs sample nodes
cpcs networks present two charts one chart demonstrates convergence
time several values w second chart depicts change quality
approximation mse function w two time points half total
sampling time end total sampling time performance gibbs sampling
cutset sampling cpcs shown figure averaged
instances evidence variables graph left figure shows mean
square error estimated posterior marginals function time gibbs sampling


fibidyuk dechter

cpcs
cpcs
cpcs b
cpcs b
grid x
random
layer
coding

gibbs









lc









w









sampling set size
w w w w
































w






w



table markov chain sampling set size function w

cpcs
cpcs
cpcs b
cpcs b
grid x
random
layer
coding

gibbs









lc
w
w
w
w
w
w
w
w

samples
w w w w
































w








w






w



table average number samples generated per second function w
loop cutset sampling w cutset sampling w second chart shows
accuracy function w first point corresponds gibbs sampling points
correspond loop cutset sampling w cutset sampling w ranging
loop cutset embedded w cutset values w explained section
loop cutset corresponds w cutset w maximum number parents
network initially best obtained cutset sampling followed
loop cutset sampling time cutset sampling become best
cpcs reported figure charts loop cutset
sampling w cutset sampling w range superior gibbs sampling
chart left shows best cutset sampling schemes lowest
mse curves cutset sampling loop cutset curve falls
cutset first outperformed cutset seconds loop cutset
sampling cutset sampling outperform gibbs sampling nearly two orders
magnitude mse falls e gibbs mse remains order e cutset sampling fall achieving mse e
curves corresponding loop cutset sampling cutset sampling fall
ibp line means four outperform ibp first seconds
execution ibp converges less second cutset outperforms ibp
seconds figure right see accuracy sampling


ficutset sampling bayesian networks

gibbs

e

mse

e
e

ibp

cpcs n lc w e

ibp
lcs w

e

c w
c w

e

c w
c w

e

e

mse

cpcs n lc w e
e

e

cutset sec
cutset sec

e
e

e

e
















gibbs

w

w

w

time sec

lcs
w

w

w

figure mse function time left w right cpcs instances time
bound seconds

mse

e

e

ibp

cpcs n lc w e

cutset sec

e

cutset sec
e

mse

gibbs
ibp
lcs w
c w
c w
c w
c w

cpcs n lc w e
e

e
e
e





time sec

lc

w




w




w




w




w




g



ib
bs

e

figure mse function time left w right cpcs instances time
bound seconds scale exponential due large variation performance
gibbs cutset sampling

seconds seconds agreement convergence curves
left
cpcs b figure loop cutset sampling cutset sampling similar
performance accuracy estimates slowly degrades w increases loop cutset
sampling w cutset sampling substantially outperform gibbs sampling values w
exceed accuracy ibp within minute
example cpcs b demonstrate significance adjusted induced
width conditioned network performance cutset sampling reported
section loop cutset relatively small lc wlc thus sampling
one loop cutset variable value exponential big adjusted induced width
loop cutset sampling computes samples per second cutset slightly larger nodes respectively see
table compute samples rates samples per second see table


fibidyuk dechter

cpcs b n lc w e

cpcs b n e lc w

gibbs

e

e

c w
c w

e

c w
c w

e

ibp
cutset sec
cutset sec

mse

mse

e

ibp
lcs w

e

e

e

g


w

w



w

time sec





w







w







lc
w







ib
b



w


e

e

figure mse function time left w right cpcs b instances time
bound seconds scale exponential due large variation performance
gibbs cutset sampling

gibbs
ibp
lcs w
c w
c w
c w
c w

e

mse

e
e

cpcs b n lc w e
e

ibp
cutset sec
cutset sec

e

mse

cpcs b n lc w e
e

e

e

e

e

e
e
















e

gibbs

time sec

w

w

w

w

w

w

figure mse function time left w right cpcs b instances time
bound seconds scale exponential due large variation performance
gibbs cutset sampling

cutset closest loop cutset size c computes samples per
second order magnitude loop cutset sampling
cpcs b shown figure loop cutset sampling excluded due
poor performance chart right figure shows w cutset performed well
range w far superior gibbs sampling allowed enough time
w cutset sampling outperformed ibp well ibp converged seconds
cutset improved ibp within seconds cutset seconds
random networks instances random multi partite instances layer networks shown figure see w cutset sampling
substantially improves gibbs sampling ibp reaching optimal performance
w classes networks range performance similar
loop cutset sampling case layer networks accuracy gibbs sampling


ficutset sampling bayesian networks

random r n p lc w

random r n p lc w

e
e

ibp
cutset sec
cutset sec

e

e

e

e
e

g

layer r n p lc w

e

w


ibp

e

cutset sec
cutset sec
e

mse

mse

e

w


layer r n p lc w

gibbs
ibp
lc w
c w
c w
c w
c w

e

w


time sec

w








w



lc



ib
bs



w


e


w


mse

e

e

mse

gibbs
ibp
lc w
c w
c w
c w
c w

e

e

e

e












time sec

e
gibbs

w

lc w

w

w

w

w

figure random multi partite networks top layer networks bottom nodes
instances mse function number samples left w right



fibidyuk dechter

ibp order magnitude less compared cutset sampling figure bottom right
poor convergence accuracy ibp layer networks observed previously
murphy et al
grid x e lc w mse
e

e

ibp
lc w

e

e

c w





cutset sec

time sec

w




w




w




ibb




cutset sec

e
g



ibp

e

w


e

e

w


c w

w


c w

e

e

w

lc
w



c w

e

mse

mse

grid x e lc w

gibbs

figure random networks nodes instances mse function number
samples left w right

grid networks grid networks nodes x class benchmarks full gibbs sampling able produce estimates comparable cutsetsampling figure respect accuracy gibbs sampler loop cutset sampling
cutset sampling best performers achieved similar loop cutset
sampling fastest accurate among cutset sampling schemes still generated samples times slowly compared gibbs sampling table since
loop cutset relatively large accuracy loop cutset sampling closely followed
cutset sampling slowly degrading w increased grid networks
example benchmarks regular graph structure cutset sampling cannot exploit
advantage small cpts two dimensional grid network node
parents gibbs sampling strong
coding x n p lc w

coding x n p lc w

ibp
e

cutset sec

c w

e

cutset sec
e

c w

e

c w

mse

mse

ibp

lc w

e

e
e

e
e

e

e
e












e
w

time sec

w

lc w

w

figure coding networks code bits parity check bits instances time
bound minutes



ficutset sampling bayesian networks

cpcs
cpcs
cpcs b
cpcs b
grid x
random
layer
coding

time
sec
sec
sec
sec
sec
sec
sec
sec

gibbs









markov chain length
lc w w w



























w









w







table individual markov chain length function w length chain
adjusted sampling scheme benchmark total processing
time across sampling

coding networks cutset sampling coding networks shown
figure induced width varied allowing exact inference
however additionally tested observed complexity network grows
exponentially number coding bits even small increase number
coding bits yielding total nodes corresponding adjustments number
parity checking bits transmitted code size induced width exceeds
time sample generation scales linearly collected networks
different parity check matrices different evidence instantiations total
instances decoding bit error rate ber standard error measure however
computed mse unobserved nodes evaluate quality approximate
precisely expected gibbs sampling converge markov chain
non ergodic left charts charts figure loop cutset
optimal choice coding networks whose performance closely followed cutset
sampling saw earlier cutset sampling outperforms ibp
computing error bound
second issue convergence sampling scheme predicting
quality estimates deciding stop sampling section compare
empirically error intervals gibbs cutset sampling estimates
gibbs sampling cutset sampling guaranteed converge correct posterior
distribution ergodic networks however hard estimate many samples
needed achieve certain degree convergence possible derive bounds
absolute error sample variance sampling method samples independent gibbs mcmc methods samples dependent cannot apply
confidence interval estimate directly case gibbs sampling apply batch
means method special case standardized time series method used
bugs software package billingsley geyer steiger wilson


fibidyuk dechter

main idea split markov chain length chains length
let pm xi e estimate derived single chain length
meaning containing samples defined equations estimates pm x e
assumed approximately independent large enough assuming convergence
conditions satisfied central limit theorem holds pm x e distributed
according n e p xi e posterior marginal p xi e obtained
average obtained chain namely
p x e


x
pm x e






sampling variance computed usually



x

pm x e p x e



equivalent expression sampling variance
pm
p x e p x e






easy compute incrementally storing running sums pm x e
x e therefore compute confidence interval percentile
pm
used random variables normal distribution small sampling set sizes namely

r



p p x e p x e

table value distribution degrees freedom
used batch means estimate confidence interval posterior
marginals one modification since working relatively small sample sets
thousand samples notion large enough well defined
restarted chain every samples guarantee estimates pm x e
truly independent method batch means provides meaningful error estimates
assuming samples drawn stationary distribution assume
chains mix fast enough samples drawn target
distribution
applied estimate error bound gibbs sampler
cutset sampler computed confidence interval estimated posterior
marginal p xi e sampling variance pm xi e markov chains
described computed sampling variance eq confidence
interval xi eq averaged nodes
x x

p
xi
n xi


xi xi

estimated confidence interval large practical thus compared
empirical average absolute error


ficutset sampling bayesian networks

cpcs
cpcs
cpcs b
cpcs b
random
layer
coding
grid x


















average error
lc
w

















gibbs















confidence interval
w
w
w































w













table average absolute error measured estimated confidence interval
function w markov chains



n

x

xi

p



x

xi xi

p xi e p xi e

objective study observe whether computed confidence interval
estimated absolute error accurately reflects true absolute error namely verify
investigate empirically whether confidence interval cutsetsampling estimates smaller compared gibbs sampling would expect due
variance reduction
table presents average confidence interval average absolute error
benchmarks benchmark first row row reports average
absolute error second row row reports confidence interval
column table corresponds sampling scheme first column reports
gibbs sampling second column reports loop cutset sampling
remaining columns report w cutset sampling w range loop cutset
sampling cpcs b included due statistically insignificant number
samples generated loop cutset sampling gibbs sampling coding networks
left network ergodic mentioned earlier gibbs sampling
converge
see networks validates method measuring
confidence interval cases estimated confidence interval
times size average error relatively small case cutset sampling
largest confidence interval max reported grid networks loop cutset


fibidyuk dechter

sampling thus confidence interval estimate could used criteria reflecting
quality posterior marginal estimate sampling practice subsequently comparing gibbs sampling cutset sampling observe
significant reduction average absolute error similar reduction
estimated confidence interval across benchmarks estimated confidence interval
gibbs sampler remains e time cutset sampling obtain
e classes networks excluded cpcs grid layer
networks
discussion
empirical evaluation performance cutset sampling demonstrates except
grid networks sampling cutset usually outperforms gibbs sampling
convergence cutset sampling terms number samples dramatically improves
predicted theoretically
experiments clearly exists range w values w cutset
sampling outperforms gibbs sampler performance w cutset sampling deteriorates
increase w yields small reduction cutset size example cpcs b
network starting w increasing w reducing sampling
set node shown table
observe loop cutset good choice cutset sampling long
induced width network wlc conditioned loop cutset reasonably small wlc
large cpcs b loop cutset sampling computationally less efficient w cutset
sampling w wlc
showed section gibbs sampling loop cutset sampling
outperform state art ais bn adaptive importance sampling method
probability evidence small consequently w cutset sampling schemes section outperformed gibbs sampler cpcs b cpcs b would outperfrom
ais bn

related work
mention related work idea marginalising variables improve
efficiency gibbs sampling first proposed liu et al successfully
applied several special classes bayesian kong et al applied collapsing
bivariate gaussian missing data liu defined collapsed gibbs
sampling finding repetitive motifs biological sequences applies integrating
two parameters model similarly gibbs sampling set collapsed escobar
maceachern liu learning nonparametric bayes
instances special relationships variables
exploited integrate several variables resulting collapsed gibbs sampling
compared previous work contribution defining generic scheme
collapsing gibbs sampling bayesian networks takes advantage networks
graph properties depend specific form relationships
variables


ficutset sampling bayesian networks

jensen et al combined sampling exact inference blocking gibbs sampling
scheme groups variables sampled simultaneously exact inference compute
needed conditional distributions empirical demonstrate significant improvement convergence gibbs sampler time yet proposed blocking
gibbs sampling sample contains variables network contrast cutset sampling reduces set variables sampled noted previously collapsing produces
lower variance estimates blocking therefore cutset sampling require fewer
samples converge
different combination sampling exact inference join trees described
koller et al kjaerulff oller et al kjaerulff proposed sample
probability distribution cluster computing outgoing messages kjaerulff
used gibbs sampling large clusters estimate joint probability distribution
p vi vi x cluster estimated p vi recorded instead true joint
distribution conserve memory motivation high probability tuples
recorded remaining low probability tuples assumed probability
small clusters exact joint distribution p vi computed recorded however
analyze introduced errors compare performance scheme
standard gibbs sampler exact analysis error given
comparison approaches
koller et al used sampling used compute messages sent cluster
cluster j posterior joint distributions cluster tree contains discrete
continuous variables subsumes cluster sampling proposed
kjaerulff includes rigorous analysis error estimated posterior
distributions method difficulties propagation evidence empirical
evaluation limited two hybrid network instances compares quality
estimates likelihood weighting instance importance sampling
perform well presence low probability evidence
effectiveness collapsing sampling set demonstrated previously
context particle filtering method dynamic bayesian networks doucet andrieu
godsill doucet defreitas gordon doucet de freitas murphy russell
b shown sampling subspace combined exact inference raoblackwellised particle filtering yields better approximation particle filtering
full set variables however objective study limited observation
effect special cases variables integrated easily
cutset sampling scheme offers generic collapsing gibbs sampler
bayesian network

conclusion
presents w cutset sampling scheme general scheme collapsing gibbs
sampler bayesian networks showed theoretically empirically cutset sampling improves convergence rate allows sampling non ergodic network
ergodic subspace collapsing sampling set reduce dependence
samples marginalising highly correlated variables smoothing
sampling distributions remaining variables estimators obtained sampling


fibidyuk dechter

lower dimensional space lower sampling variance induced
width w controlling parameter w cutset sampling provides mechanism balancing
sampling exact inference
studied power cutset sampling sampling set loop cutset
generally sampling set w cutset network defined subset
variables instantiated induced width network w
rao blackwell theorem cutset sampling requires fewer samples regular sampling
convergence experiments showed reduction number samples
time wise cost effective confirmed range randomly generated real
benchmarks demonstrated cutset sampling superior state art
ais bn importance sampling probability evidence small
since size cutset correlations variables two main
factors contributing speed convergence w cutset sampling may optimized advancement methods finding minimal w cutset another promising
direction future incorporate heuristics avoiding selecting stronglycorrelated variables cutset since correlations driving factors speed
convergence gibbs sampling alternatively could combine sample collapsing
blocking
summary w cutset sampling scheme simple yet powerful extension sampling
bayesian networks likely dominate regular sampling sampling method
focused gibbs sampling better convergence characteristics sampling
schemes implemented cutset sampling principle particular
adapted use likelihood weighting bidyuk dechter

references
abdelbar hedetniemi approximating maps belief networks
np hard theorems artificial intelligence
andrieu c de freitas n doucet rao blackwellised particle filtering via
data augmentation advances neural information processing systems mit
press
arnborg efficient combinatorial graphs
bounded decomposability survey bit
becker bar yehuda r geiger random loop cutset
journal artificial intelligence
bertele u brioschi f nonserial dynamic programming academic press
bidyuk b dechter r empirical study w cutset sampling bayesian networks proceedings th conference uncertainty artificial intelligence
uai pp morgan kaufmann
bidyuk b dechter r finding minimal w cutset proceedings
th conference uncertainty artificial intelligence uai pp
morgan kaufmann


ficutset sampling bayesian networks

bidyuk b dechter r cutset sampling likelihood weighting proceedings nd conference uncertainty artificial intelligence uai pp
morgan kaufmann
billingsley p convergence probability measures john wiley sons york
casella g robert c p rao blackwellisation sampling schemes biometrika

cheng j druzdzel j ais bn adaptive importance sampling
evidenctial reasoning large baysian networks journal aritificial intelligence

cooper g computational complexity probabilistic inferences artificial
intelligence
dagum p luby approximating probabilistic inference bayesian belief
networks np hard artificial intelligence
dechter r bucket elimination unifying framework reasoning artificial
intelligence
dechter r b bucket elimination unifying framework reasoning artificial
intelligence
dechter r constraint processing morgan kaufmann
doucet andrieu c iterative state estimation jump markov
linear systems ieee trans signal processing
doucet andrieu c godsill sequential monte carlo sampling methods bayesian filtering statistics computing
doucet de freitas n murphy k russell b rao blackwellised particle
filtering dynamic bayesian networks proceedings th conference
uncertainty artificial intelligence uai pp
doucet defreitas n gordon n sequential monte carlo methods practice
springer verlag york inc
doucet gordon n krishnamurthy v particle filters state estimation jump markov linear systems tech rep cambridge university engineering
department
escobar estimating normal means iwth dirichlet process prior journal
american statistical aasociation
frey b j mackay j c revolution belief propagation graphs
cycles neural information processing systems vol
fung r chang k c weighing integrating evidence stochastic simulation bayesian networks proceedings th conference uncertainty
artificial intelligence uai pp morgan kaufmann
geiger fishelson optimizing exact genetic linkage computations proceedings th annual international conf computational molecular biology
pp morgan kaufmann


fibidyuk dechter

gelfand smith sampling approaches calculating marginal densities journal american statistical association
geman geman stochastic relaxations gibbs distributions
bayesian restoration images ieee transaction pattern analysis machine
intelligence
geyer c j practical markov chain monte carlo statistical science
gilks w richardson spiegelhalter markov chain monte carlo practice
chapman hall
gottlob g leone n scarello f comparison structural csp decomposition
methods proceedings th international joint conference artificial
intelligence ijcai pp morgan kaufmann
jensen c kong kjrulff u blocking gibbs sampling large probabilistic expert systems int j human computer studies special issue realworld applications uncertain reasoning
jensen f v lauritzen l olesen k g bayesian updating causal
probabilistic networks local computation computational statistics quarterly

jones g hobert j p honest exploration intractable probability distributions
via markov chain monte carlo statist sci
kask k dechter r larrosa j dechter unifying cluster tree decompositions reasoning graphical artificial intelligence
kjrulff u hugs combining exact inference gibbs sampling junction
trees proceedings th conference uncertainty artificial intelligence
uai pp morgan kaufmann
koller lerner u angelov general approximate inference
application hybrid bayes nets proceedings th conference
uncertainty artificial intelligence uai pp
kong liu j wong w sequential imputations bayesian missing
data j american statistical association
kschischang f r frey b j iterative decoding compound codes probability propagation graphical ieee journal selected areas communications
larrosa j dechter r boosting search variable elimination constraint
optimization constraint satisfaction constraints
lauritzen spiegelhalter local computation probabilities graphical
structures application expert systems journal royal statistical
society series b
liu j correlation structure convergence rate gibbs sampler ph
thesis university chicago


ficutset sampling bayesian networks

liu j collapsed gibbs sampler bayesian computations applications
gene regulation journal american statistical association

liu j wong w kong covariance structure gibbs sampler
applications comparison estimators augmentation schemes biometrika

liu j nonparametric hierarchical bayes via sequential imputations annals
statistics
liu j monte carlo strategies scientific computing springer verlag
york inc
maceachern clyde liu j sequential importance sampling nonparametric bayes next generation canadian journal statistics

maceachern n estimating normal means conjugate style dirichlet process
prior communications statistics simulation computation
mackay introduction monte carlo methods proceedings nato advanced study institute learning graphical sept oct pp
maier theory relational databases computer science press rockville
md
mceliece r mackay cheng j f turbo decoding instance pearls
belief propagation ieee j selected areas communication
miller r masarie f myers j quick medical reference qmr diagnostic
assistance medical computing
miller r pople h myers j internist experimental computerbased
diagnostic consultant general internal medicine english journal medicine

murphy k p weiss jordan loopy belief propagation approximate
inference empirical study proceedings th conference uncertainty
artificial intelligence uai pp morgan kaufmann
parker r miller r causal knowledge create simulated patient cases
cpcs project extension internist proceedings th symp
comp appl medical care pp
pearl j probabilistic reasoning intelligent systems morgan kaufmann
peot shachter r fusion propagation multiple observations
belief networks artificial intelligence
pradhan provan g middleton b henrion knowledge engineering
large belief networks proceedings th conference uncertainty artificial
intelligence seattle wa pp
rish kask k dechter r empirical evaluation approximation
probabilistic decoding proceedings th conference uncertainty
artificial intelligence uai pp morgan kaufmann


fibidyuk dechter

roberts g sahu k updating schemes correlation structure blocking
parameterization gibbs sampler journal royal statistical society
series b
roberts g tweedie r l bounds regeneration times convergence
rates markov chains stochastic processes applications
roberts g tweedie r l corregendum bounds regeneration times
convergence rates markov chains stochastic processes applications

rosenthal j convergence rates markov chains siam review

rosti v gales rao blackwellised gibbs sampling switching linear
dynamical systems ieee international conference acoustics speech
signal processing icassp pp
schervish carlin b convergence successive substitution sampling
journal computational graphical statistics
shachter r andersen k solovitz p global conditioning probabilistic
inference belief networks proceedings th conference uncertainty
artificial intelligence uai pp
shachter r peot simulation approaches general probabilistic
inference belief networks proceedings th conference uncertainty
artificial intelligence uai pp
steiger n wilson j r convergence properties batch means method
simulation output analysis informs journal computing
tierney l markov chains exploring posterior distributions annals statistics

yuan c druzdzel importance sampling evidence
pre propagation proceedings th conference uncertainty artificial
intelligence uai pp
zhang n poole simple bayesian network computations
proceedings th canadian conference artificial intelligence pp




