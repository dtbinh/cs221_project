Journal Artificial Intelligence Research 28 (2007) 1-48

Submitted 4/06; published 1/07

Cutset Sampling Bayesian Networks
Bozhena Bidyuk
Rina Dechter

bbidyuk@ics.uci.edu
dechter@ics.uci.edu

School Information Computer Science
University California Irvine
Irvine, CA 92697-3425

Abstract
paper presents new sampling methodology Bayesian networks samples
subset variables applies exact inference rest. Cutset sampling
network structure-exploiting application Rao-Blackwellisation principle sampling
Bayesian networks. improves convergence exploiting memory-based inference algorithms. viewed anytime approximation exact cutset-conditioning
algorithm developed Pearl. Cutset sampling implemented efficiently
sampled variables constitute loop-cutset Bayesian network and, generally,
induced width networks graph conditioned observed sampled variables bounded constant w. demonstrate empirically benefit scheme
range benchmarks.

1. Introduction
Sampling common method approximate inference Bayesian networks.
exact algorithms impractical due prohibitive time memory demands, often
feasible approach offers performance guarantees. Given Bayesian network
variables X = {X1 , ..., Xn }, evidence e, set samples {x(t) } P (X|e),
estimate f(X) expected value function f (X) obtained generated
samples via ergodic average:
1X
f (x(t) ) ,
(1)
E[f (X)|e] f(X) =

number samples. f(X) shown converge exact value
increases. central query interest Bayesian networks computing posterior
marginals P (xi |e) value xi variable Xi , called belief updating.
query, f (X) equals -function, equation reduces counting fraction
occurrences Xi = xi samples,
P (xi |e) =
(t)


1X
(xi |x(t) ) ,


(2)

t=1

(xi |x(t) )=1 iff xi = xi (xi |x(t) )=0 otherwise. Alternatively, mixture estimator used,

1X
(t)
P (xi |xi ) ,
(3)
P (xi |e)] =

t=1

c
2007
AI Access Foundation. rights reserved.

fiBidyuk & Dechter

(t)

xi = x(t) \xi .
significant limitation sampling, however, statistical variance increases
number variables network grows therefore number samples
necessary accurate estimation increases. paper, present sampling scheme
Bayesian networks discrete variables reduces sampling variance sampling
subset variables, technique known collapsing Rao-Blackwellisation.
fundamentals Rao-Blackwellised sampling developed Casella Robert
(1996) Liu, Wong, Kong (1994) Gibbs sampling MacEachern, Clyde,
Liu (1998) Doucet, Gordon, Krishnamurthy (1999) importance sampling.
Doucet, de Freitas, Murphy, Russell (2000) extended Rao-Blackwellisation Particle
Filtering Dynamic Bayesian networks.
basic Rao-Blackwellisation scheme described follows. Suppose partition space variables X two subsets C Z. Subsequently, re-write
function f (X) f (C, Z). generate samples distribution P (C|e) compute E[f (C, Z)|c, e], perform sampling subset C only, generating samples
c(1) , c(2) , ..., c(T ) approximating quantity interest
E[f (C, Z)|e] = EC [EZ [f (C, Z)|c, e]] f(X) =


1X
EZ [f (C, Z)|c(t) , e] .


(4)

t=1

posterior marginals estimates cutset variables obtained using expression similar Eq.(2),
1X
(ci |c(t) ) ,
(5)
P (ci |e) =


using mixture estimator similar Eq.(3),
P (ci |e) =

1X
(t)
P (ci |ci , e) .


(6)

Xi X\C, E, E[P (Xi |e)] = EC [P (Xi |c, e)] Eq.(4) becomes
P (Xi |e) =

1X
P (Xi |c(t) , e) .


(7)

Since convergence rate Gibbs sampler tied maximum correlation
two samples (Liu, 2001), expect improvement convergence rate
sampling lower dimensional space since 1) highly-correlated variables may
marginalized 2) dependencies variables inside smaller set likely
weaker variables farther apart sampling distributions
smoothed out. Additionally, estimates obtained sampling lower dimensional
space expected lower sampling variance therefore require fewer samples
achieve accuracy estimates. hand, cost generating
sample may increase. Indeed, principles Rao-Blackwellised sampling
applied classes probabilistic models specialized structure (Kong, Liu,
& Wong, 1994; Escobar, 1994; MacEachern, 1994; Liu, 1996; Doucet & Andrieu, 2001;
Andrieu, de Freitas, & Doucet, 2002; Rosti & Gales, 2004).
2

fiCutset Sampling Bayesian Networks

contribution paper presenting general, structure-based scheme
applies Rao-Blackwellisation principle Bayesian networks. idea exploit
property conditioning subset variables simplifies networks structure, allowing efficient query processing exact algorithms. general, exact inference variable
elimination (Dechter, 1999a, 2003) join-tree algorithms (Lauritzen & Spiegelhalter, 1988;
Jensen, Lauritzen, & Olesen, 1990) time space exponential induced-width w
network. However, subset variables assigned (i.e., conditioned upon)
induced-width conditioned network may reduced.
idea cutset sampling choose subset variables C conditioning
C yields sparse enough Bayesian network small induced width allow exact
inference. Since sample assignment cutset variables, efficiently generate
new sample cutset variables conditioned network computation
P (c|e) P (Xi |c, e) bounded. particular, sampling set C cuts cycles
network (i.e., loop-cutset), inference conditioned network becomes
linear. general, C w-cutset, namely subset nodes assigned,
induced-width conditioned network w, time space complexity computing
next sample O(|C| N dw+2 ) maximum domain size N = |X|.

idea exploiting properties conditioning subset variables first proposed exact belief updating context cutset-conditioning (Pearl, 1988).
scheme requires enumerating instantiations cutset variables. Since number
instances exponential size cutset |C|, sampling cutset space may
right compromise size cutset big. Thus, sampling cutset
viewed anytime approximation cutset-conditioning approach.
Although Rao-Blackwellisation general cutset sampling particular applied context sampling algorithm, introduce principle context Gibbs sampling (Geman & Geman, 1984; Gilks, Richardson, & Spiegelhalter, 1996;
MacKay, 1996), Markov Chain Monte Carlo sampling method Bayesian networks.
Extension sampling approach graphical models, Markov
networks, straight forward. recently demonstrated idea incorporated importance sampling (Bidyuk & Dechter, 2006).
paper defines analyzes cutset sampling scheme investigates empirically
trade-offs sampling exact computation variety randomly generated
networks grid structure networks well known real-life benchmarks CPCS
networks coding networks. show cutset sampling converges faster pure
sampling terms number samples, dictated theory, almost always
time-wise cost effective benchmarks tried. demonstrate applicability
scheme deterministic networks, Hailfinder network coding networks,
Markov Chain non-ergodic Gibbs sampling converge.
Section 2 provides background information. Specifically, section 2.1 introduces Bayesian
networks, section 2.2 reviews exact inference algorithms Bayesian networks, section 2.3 provides background Gibbs sampling. contribution paper presenting
cutset sampling starts section 3. Section 4 presents empirical evaluation cutset
sampling. present empirical evaluation sampling variance resulting standard error based method batch means (for details, see Geyer, 1992).
3

fiBidyuk & Dechter

section 5, review previous application Rao-Blackwellisation section 6 provides
summary conclusions.

2. Background
section, define essential terminology provide background information
Bayesian networks.
2.1 Preliminaries
use upper case letters without subscripts, X, denote sets variables
lower case letters without subscripts denote instantiation group variables (e.g.,
x indicates variable set X assigned value). use upper case letter
subscript, Xi , denote single variable lower case letter
subscript, xi , denote instantiated variable (e.g., xi denotes arbitrary value
domain Xi means Xi = xi ). D(Xi ) denotes domain variable Xi .
superscript subscripted lower case letter would used distinguish different specific
values variable, i.e., D(Xi ) = {x1i , x2i , ...}. use x denote instantiation
set variables x = {x1 , ..., xi1 , xi , xi+1 , ..., xn } xi = x\xi denote x element
xi removed. Namely, xi = {x1 , x2 , ..., xi1 , xi+1 , ..., xn }.
Definition 2.1 (graph concepts) directed graph pair D=<V ,E>, V =
{X1 , ..., Xn } set nodes E = {(Xi , Xj )|Xi , Xj V } set edges. Given
(Xi , Xj ) E, Xi called parent Xj , Xj called child Xi . set
Xi parents denoted pa(Xi ), pai , set Xi children denoted ch(Xi ),
chi . family Xi includes Xi parents. moral graph directed graph
undirected graph obtained connecting parents nodes
removing arrows. cycle-cutset undirected graph subset nodes that,
removed, yields graph without cycles. loop directed graph subgraph
whose underlying graph cycle. directed graph acyclic directed loops.
directed graph singly-connected (also called poly-tree), underlying undirected
graph cycles. Otherwise, called multiply-connected.
Definition 2.2 (loop-cutset) vertex v sink respect loop L two
edges adjacent v L directed v. vertex sink respect
loop L called allowed vertex respect L. loop-cutset directed graph
set vertices contains least one allowed vertex respect loop D.
Definition 2.3 (Belief Networks) Let X = {X1 , ..., Xn } set random variables
multi-valued domains D(X1 ), ..., D(Xn ). belief network (BN) (Pearl, 1988)
pair <G, P > G directed acyclic graph whose nodes variables X
P = {P (Xi |pai )|i = 1, ..., n} set conditional probability tables (CPTs) associated
Xi . BN represents joint probability distribution product form:
P (x1 , ...., xn ) =

n

i=1

P (xi |pa(Xi ))

evidence e instantiated subset variables E X.
4

fiCutset Sampling Bayesian Networks

structure directed acyclic graph G reflects dependencies
variables using d-separation criterion. parents variable Xi together children
parents children form Markov blanket, denoted markovi , node Xi .
use xmarkovi denote x restricted variables markovi . know node
Xi independent rest variables conditioned Markov blanket. Namely,
P (xi |xi ) = P (xi |xmarkovi ).
common query belief networks belief updating task
computing posterior distribution P (Xi |e) given evidence e query variable Xi
X. Reasoning Bayesian networks NP-hard (Cooper, 1990). Finding approximate
posterior marginals fixed accuracy NP-hard (Dagum & Luby, 1993; Abdelbar
& Hedetniemi, 1998). network poly-tree, belief updating inference
tasks accomplished time linear size input. general, exact inference
exponential induced width networks moral graph.
Definition 2.4 (induced-width) width node ordered undirected graph
number nodes neighbors precede ordering. width
ordering d, denoted w(d), maximum width nodes. induced width
ordered graph, w (d), width ordered graph obtained processing
nodes last first follows: node X processed, preceding neighbors
connected. resulting graph called induced graph triangulated graph.
task finding minimal induced width graph (over possible orderings)
NP-complete (Arnborg, 1985).
2.2 Reasoning Bayesian Networks
Belief propagation algorithm, introduce Section 2.2.2 below, performs belief
updating singly-connected Bayesian networks time linear size input
(Pearl, 1988). loopy networks, two main approaches belief updating cutset
conditioning tree clustering. algorithms often referred inference
algorithms. briefly describe idea clustering algorithms Section 2.2.1
conditioning method Section 2.2.3.
2.2.1 Variable Elimination Join-Tree Clustering (JTC)
join-tree clustering approach (JTC) refers family algorithms including jointree propagation (Lauritzen & Spiegelhalter, 1988; Jensen et al., 1990) bucket-tree
elimination (Dechter, 2003, 1999a). idea first obtain tree-decomposition
network clusters functions connected tree propagate messages
clusters tree. tree-decomposition singly-connected undirected
graph whose nodes, called clusters, contain subsets variables input functions
defined variables. tree-decomposition must contain function
satisfy running intersection property (Maier, 1983). unifying perspective treedecomposition schemes see (Zhang & Poole, 1994; Dechter, 1999b; Kask, Dechter, Larrosa,
& Dechter, 2005).
Given tree-decomposition network, message propagation tree
synchronized. select one cluster root tree propagate messages
5

fiBidyuk & Dechter

tree. message cluster Vi neighbor Vj function
separator set Vi Vj marginalization product functions Vi
messages Vi received neighbors besides Vj . Assuming maximum
number variables cluster w + 1 maximum domain size d, time
space required process one cluster O(d(w+1) ). Since maximum number clusters
bounded |X| = N , complexity variable-elimination algorithms cluster-tree
propagation schemes O(N d(w+1) ). parameter w, maximum cluster size minus
1, called tree-width tree decomposition. minimal tree-width identical
minimal induced width graph.
2.2.2 Iterative Belief Propagation (IBP)
Belief propagation (BP) iterative message-passing algorithm performs exact inference singly-connected Bayesian networks (Pearl, 1988). iteration, every node Xi
sends j (Xi ) message child j receives j (Xi ) message child.
message-passing order organized converges two iterations. essence
algorithm join-tree clustering approach applied directly poly-tree.
Applied Bayesian networks loops, algorithm usually iterates longer (until may
converge) hence, known Iterative Belief Propagation (IBP) loopy belief propagation. IBP provides guarantees convergence quality approximate posterior
marginals shown perform well practice (Rish, Kask, & Dechter, 1998; Murphy,
Weiss, & Jordan, 1999). considered best algorithm inference coding networks
(Frey & MacKay, 1997; Kschischang & Frey, 1998) finding probable variable
values equals decoding process (McEliece, MacKay, & Cheng, 1997). Algorithm IBP
requires linear space usually converges fast converges. benchmarks, IBP
converged within 25 iterations less (see Section 4).
2.2.3 Cutset Conditioning
tree-width w Bayesian network large requirements inference
schemes bucket elimination join-tree clustering (JTC) exceed available memory,
switch alternative cutset conditioning schemes (Pearl, 1988; Peot & Shachter,
1992; Shachter, Andersen, & Solovitz, 1994). idea cutset conditioning select
subset variables C X\E, cutset, obtain posterior marginals node
Xi X\C, E using:
X
P (xi |e) =
P (xi |c, e)P (c|e)
(8)
cD(C)

Eq.(8) implies enumerate instantiations C, perform exact inference cutset instantiation c obtain P (xi |c, e) P (c|e) sum
results. total computation time exponential size cutset
enumerate instantiations cutset variables.
C loop-cutset, then, nodes C assigned, Bayesian network
transformed equivalent poly-tree P (xi |c, e) P (c|e) computed via
BP time space linear size network. example, subset {A, D}
loop-cutset belief network shown Figure 1, left, evidence E = e. right,
6

fiCutset Sampling Bayesian Networks


B

C



F

E

G







B

C



F

E

G



Figure 1: nodes loopy Bayesian network (left) instantiated,
network transformed equivalent singly-connected network (right).
transformation process, replica observed node created
child node.

Figure 1 shows equivalent singly-connected network resulting assigning values
D.
well-known minimum induced width w network always less
size smallest loop-cutset (Bertele & Brioschi, 1972; Dechter, 2003). Namely,
w + 1 |C| C. Thus, inference approaches (e.g., bucket elimination) never
worse often better cutset conditioning time-wise. However, w
large must resort cutset conditioning search order trade space time.
considerations yield hybrid search inference approach. Since observed variables
break dependencies network, network observed subset variables
C often transformed equivalent network smaller induced width, wC ,
term adjusted induced width. Hence, subset variables C X
observed, complexity bounded exponentially adjusted induced width
graph wC .
Definition 2.5 (adjusted induced width) Given graph G=<X,E>, adjusted
induced width G relative C, denoted wC , induced width C removed
moral graph.
Definition 2.6 (w-cutset) Given graph G=<X,E>, subset nodes C X
w-cutset G adjusted induced width equals w.
C w-cutset, quantities P (xi |c, e) P (c|e) computed time
space exponential w, much smaller tree-width unconditioned
network. resulting scheme requires memory exponential w time O(d|C| N d(w+1) )
N size network maximum domain size. Thus, performance
tuned available system memory resource via bounding parameter w.
Given constant w, finding minimal w-cutset (to minimize cutset conditioning
time) hard problem. Several greedy heuristic approaches proposed
Geiger Fishelson (2003) Bidyuk Dechter (2003, 2004). elaborate
Section 3.5.
7

fiBidyuk & Dechter

2.3 Gibbs Sampling
Since complexity inference algorithms memory exponential networks induced
width (or tree-width) since resorting cutset-conditioning scheme may take
much time w-cutset size large, must often resort approximation
methods. Sampling methods Bayesian networks commonly used approximation
techniques. section provides background Gibbs sampling, Markov Chain Monte
Carlo method, one popular sampling schemes focus
paper. Although method may applied networks continuous distributions,
limit attention paper discrete random variables finite domains.
2.3.1 Gibbs Sampling Bayesian Networks
Ordered Gibbs Sampler
Input: belief network B X={X1 , ..., Xn } evidence e={(Xi = ei )|Xi E X}.
Output: set samples {x(t) }, = 1...T .
(0)
1. Initialize: Assign random value xi variable Xi X\E D(Xi ). Assign
evidence variables observed values.
2. Generate samples:
= 1 T, generate new sample x(t) :
(t)
= 1 N, compute new value xi variable Xi :
(t)
(t)
(t)
Compute distribution P (Xi |xmarkovi ) sample xi P (Xi |xmarkovi ).
(t)

Set Xi = xi .
End
End

Figure 2: Gibbs sampling Algorithm
Given Bayesian network variables X = {X1 , ..., Xn }, evidence e, Gibbs
sampling (Geman & Geman, 1984; Gilks et al., 1996; MacKay, 1996) generates set
(t)
(t)
samples {x(t) } sample x(t) = {x1 , ..., xn } instantiation variables.
(t)
superscript denotes sample index xi value Xi sample t. first
(t)
sample initialized random. generating new sample sample xi ,
(t)
new value variable Xi sampled probability distribution P (Xi |xi ) (recall
(t)

(t+1)

P (Xi |xi ) = P (Xi |x1

(t+1)

(t+1)

(t)

(t)

(t)

, ..., xi1 , xi+1 , ..., xn )) denote xi P (Xi |xi ).
(t)

next sample xi
generated previous sample xi following one two
schemes.
Random Scan Gibbs Sampling. Given sample x(t) iteration t, pick variable
(t)
Xi random sample new value xi conditional distribution xi P (Xi |xi )
leaving variables unchanged.
Systematic Scan (Ordered) Gibbs Sampling. Given sample x(t) , sample new
value variable order:
(t)

(t)

x1 P (X1 |x2 , x3 , ..., x(t)
n )
8

fiCutset Sampling Bayesian Networks

(t+1)

x2 P (X2 |x1

(t)

, x3 , ..., x(t)
n )

...

(t+1)

xi P (Xi |x1

(t+1)

(t)

, ..., xi1 , xi+1 , ..., x(t)
n )

...

(t+1)

xn P (Xn |x1

(t+1)

, x2

(t+1)

, ..., xn1 )
(t)

Bayesian networks, conditional distribution P (Xi |xi ) dependent
(t)

(t)

assignment Markov blanket variable Xi . Thus, P (Xi |xi )=P (Xi |xmarkovi )
(t)

xmarkovi restriction x(t) markovi . Given Markov blanket Xi , sampling
probability distribution given explicitly Pearl (1988):

(t)
(t)
P (xj |x(t)
(9)
P (xi |xmarkovi ) = P (xi |x(t)
)
paj )
pai
{j|Xj chj }

Thus, generating complete new sample done O(n r) multiplication steps
r maximum family size n number variables.
sequence samples x(1) , x(2) , ... viewed sequence states Markov
(t+1)
(t+1) (t) (t)
(t)
chain. transition probability state {x1
, ..., xi1 , xi , xi+1 , ..., xn } state
(t+1)

(t+1)

(t+1)

(t)

(t)

(t)

{x1
, ..., xi1 , xi
, xi+1 , ..., xn } defined sampling distribution P (Xi |xi ).
construction, Markov chain induced Gibbs sampling invariant distribution
P (X|e). However, since values assigned Gibbs sampler variables sample
x(t+1) depend assignment values previous sample x(t) , follows
sample x(n) depends initial state x(0) . convergence Markov chain
defined rate distance distribution P (x(n) |x(0) , e)
stationary distribution P (X|e) (i.e., variational distance, L1 -distance, 2 ) converges 0
function n. Intuitively, reflects quickly inital state x(0) forgotten.
convergence guaranteed Markov chain ergodic (Pearl, 1988; Gelfand
& Smith, 1990; MacKay, 1996). Markov chain finite number states ergodic
aperiodic irreducible (Liu, 2001). Markov chain aperiodic
regular loops. Markov chain irreducible get state Si state
Sj (including Si ) non-zero probability finite number steps. irreducibility
guarantees able visit (as number samples increases) statistically
important regions state space. Bayesian networks, conditions almost always
satisfied long conditional probabilities positive (Tierney, 1994).
ensure collected samples drawn distribution close P (X|e),
burn-in time may allocated. Namely, assuming takes K samples
Markov chain get close stationary distribution, first K samples may included computation posterior marginals. However, determining K hard (Jones
& Hobert, 2001). general, burn-in optional sense convergence
estimates correct posterior marginals depend it. completeness
sake, algorithm given Figure 2.
P
convergence conditions satisfied, ergodic average fT (X) = T1 f (xt )
function f (X) guaranteed converge expected value E[f (X)] increases.
9

fiBidyuk & Dechter

words, |fT (X) E[f (X)]| 0 . finite-state Markov chain
irreducible aperiodic, following result applies (see Liu, 2001, Theorem 12.7.2):

|fT (X) E[f (X)]| N (0, (f )2 )
(10)
initial assignment x(0) . variance term (f )2 defined follows:
(f )2 = 2 (f ) 2
2 = var[f (X)] (h) integrated autocorrelation time.
focus computing posterior marginals P (Xi |e) Xi X\E.
posterior marginals estimated using either histogram estimator:
P (Xi = xi |e) =


1X
(xi |x(t) )


P (Xi = xi |e) =


1X
(t)
P (xi |xi )


mixture estimator:

(11)

t=1

(12)

t=1

histogram estimator corresponds counting samples Xi = xi , namely (xi |x(t) ) =
(t)
1 xi = xi equals 0 otherwise. Gelfand Smith (1990) pointed since
mixture estimator based estimating conditional expectation, sampling variance
smaller due Rao-Blackwell theorem. Thus, mixture estimator preferred.
(t)
(t)
Since P (xi |xi ) = P (xi |xmarkovi ), mixture estimator simply average conditional
probabilities:

1X
(t)
P (xi |e) =
P (xi |xmarkovi )
(13)

t=1

mentioned above, Markov chain ergodic, P (Xi |e) converge exact
posterior marginal P (Xi |e) number samples increases. shown Roberts
Sahu (1997) random scan Gibbs sampler expected converge faster
systematic scan Gibbs sampler. Ultimately, convergence rate Gibbs sampler depends
correlation two consecutive samples (Liu, 1991; Schervish & Carlin, 1992;
Liu et al., 1994). review subject next section.
2.4 Variance Reduction Schemes
convergence rate Gibbs sampler depends strength correlations
samples (which states Markov chain). term correlation
used mean samples dependent, mentioned earlier. case
finite-state irreducible aperiodic Markov chain, convergence rate expressed
maximal correlation states x(0) x(n) (see Liu, 2001, ch. 12). practice,
convergence rate analyzed covariance cov[f (x(t) ), f (x(t+1) )], f
function, called auto-covariance.
convergence estimates exact values depends convergence
rate Markov chain stationary distribution variance estimator.
10

fiCutset Sampling Bayesian Networks

factors contribute value term (f )2 Eq.(10). two main
approaches allow reduce correlation samples reduce sampling variance
estimates blocking (grouping variables together sampling simultaneously)
collapsing (integrating random variables sampling subset),
known Rao-Blackwellisation.
Given joint probability distribution three random variables X, , Z,
depict essence three sampling schemes follows:
1. Standard Gibbs:
x(t+1) P (X|y (t) , z (t) )


(t+1)

z

(t+1)

(t+1)

P (Y |x

(t+1)

P (Z|x

(14)

(t)

,z )

,y

(15)

(t+1)

)

(16)

2. Collapsed (variable Z integrated out):
x(t+1) P (X|y (t) )


(t+1)

(t+1)

P (Y |x

(17)
)

(18)

3. Blocking grouping X together:
(x(t+1) , (t+1) ) P (X, |z (t) )
z

(t+1)

(t+1)

P (Z|x

,y

(19)
(t+1)

)

(20)

blocking reduces correlation samples grouping highly correlated
variables blocks. collapsing, highly correlated variables marginalized out,
results smoothing sampling distributions remaining variables
(P (Y |x) smoother P (Y |x, z)). approaches lead reduction sampling
variance estimates, speeding convergence exact values.
Generally, blocking Gibbs sampling expected converge faster standard Gibbs
sampler (Liu et al., 1994; Roberts & Sahu, 1997). Variations scheme
investigated Jensen et al. (1995) Kjaerulff (1995). Given number samples,
estimate resulting collapsed Gibbs sampler expected lower variance
(converge faster) estimate obtained blocking Gibbs sampler (Liu et al., 1994).
Thus, collapsing preferred blocking. analysis collapsed Gibbs sampler
found Escobar (1994), MacEachern (1994), Liu (1994, 1996).
caveat utilization collapsed Gibbs sampler computation
probabilities P (X|y) P (Y |x) must efficient time-wise. case Bayesian networks,
task integrating variables equivalent posterior belief updating
evidence variables sampling variables observed. time complexity therefore
exponential adjusted induced width, namely, effective width network
dependencies broken instantiated variables (evidence sampled).
11

fiBidyuk & Dechter

2.5 Importance Sampling
Since sampling target distribution hard, different sampling methods explore
different trade-offs generating samples obtaining estimates. already discussed,
Gibbs sampling generates dependent samples guarantees convergence sampling
distribution target distribution. Alternative approach, called importance sampling,
generate samples sampling distribution Q(X) different P (X|e)
include weight w(t) = P (x(t) |e)/Q(x(t) ) sample x(t) computation
estimates follows:

1X
f (xt )w(t)
fT (X) =


(21)

t=1

convergence fT (X) E[f (X)] guaranteed long condition P (x|e) 6=
0 Q(x) 6= 0 holds. convergence speed depends distance Q(X)
P (X|e).
One simplest forms importance sampling Bayesian networks likelihood
weighting (Fung & Chang, 1989; Shachter & Peot, 1989) processes variables topological order, sampling root variables priors remaining variables
conditional distribution P (Xi |pai ) defined conditional probability table (the evidence variables assigned observed values). sampling distribution close
prior and, result, usually converges slowly evidence concentrated around
leaf nodes (nodes without children) probability evidence small. Adaptive (also called dynamic) importance sampling method attempts speed
convergence updating sampling distribution based weight previously generated samples. Adaptive importance sampling methods include self-importance sampling,
heuristic importance sampling (Shachter & Peot, 1989), and, recently, AIS-BN (Cheng
& Druzdzel, 2000) EPIS-BN (Yuan & Druzdzel, 2003). empirical section,
compare performance proposed cutset sampling algorithm AIS-BN
considered state-of-the-art importance sampling algorithm date (although EPIS-BN
shown perform better networks) and, hence, describe AIS-BN
detail.
AIS-BN algorithm based observation could sample node
topological order distribution P (Xi |pai , e), resulting sample would drawn
target distribution P (X|e). Since distribution unknown variable
observed descendants, AIS-BN initializes sampling distributions P 0 (Xi |pai , e)
equal either P (Xi |pai ) uniform distribution updates distribution
P k (Xi |pai , e) every l samples next sampling distribution P k+1 (Xi |pai , e)
closer P (Xi |pai , e) P k (Xi |pai , e) follows:
P k+1 (xi |pai , e) = P k (xi |pai , e) + (k) (P (xi |pai , e) P k (xi |pai , e))
(k) positive function determines learning rate P (xi |pai , e)
estimate P (xi |pai , e) based last l samples.
12

fiCutset Sampling Bayesian Networks

3. Cutset Sampling
section presents cutset sampling scheme. discussed above, sampling
cutset guaranteed statistically efficient. Cutset sampling scheme computationally efficient way sampling collapsed variable subset C X, tying
complexity sample generation structure Bayesian network.
3.1 Cutset Sampling Algorithm
cutset sampling scheme partitions variable set X two subsets C X\C.
objective generate samples space C={C1 , C2 , ..., Cm } sample c(t)
instantiation variables C. Following Gibbs sampling principles,
(t)
wish generate new sample c(t) sampling value ci probability distribution
(t)
(t+1) (t+1)
(t+1) (t)
(t)
P (Ci |ci ) = P (Ci |c1
, c2
, ..., ci1 , ci+1 , ..., cm ). use left arrow denote
(t)

value ci drawn distribution P (Ci |ci ):

(t)

ci P (Ci |ci , e)

(22)
(t)

compute probability distribution P (Ci |ci , e) efficiently sampling
variable Ci C, generate samples efficiently. relevant conditional distributions computed exact inference whose complexity tied network structure. denote JT C(B, Xi , e) generic algorithm class variable-elimination
join-tree clustering algorithms which, given belief network B evidence e, outputs
posterior probabilities P (Xi |e) variable Xi X (Lauritzen & Spiegelhalter, 1988;
Jensen et al., 1990; Dechter, 1999a). networks identity clear, use
notation JT C(Xi , e).
Cutset Sampling
Input: belief network B, cutset C = {C1 , ..., Cm }, evidence e.
Output: set samples ct , = 1...T .
1. Initialize: Assign random value c0i Ci C assign e.
2. Generate samples:
= 0 T-1, generate new sample c(t+1) follows:
(t)
= 1 m, compute new value ci variable Ci follows:
(t)
a. Compute JT C(Ci , ci , e).
(t)

(t)

b. Compute P (Ci |ci , e) = P (Ci , ci , e).
c. Sample:
(t+1)
(t)
ci
P (Ci |ci , e)
End
End

(23)

Figure 3: w-Cutset sampling Algorithm
Therefore, sampling variable Ci value ci D(Ci ), compute
(t)
(t)
(t)
(t)
P (Ci , ci , e) via JT C(Ci , ci , e) obtain P (Ci |ci , e) via normalization: P (Ci |ci , e) =
(t)

P (Ci , ci , e).

13

fiBidyuk & Dechter

Cutset sampling algorithm uses systematic scan Gibbs sampler given Figure 3.
Clearly, adapted used random scan Gibbs sampler well. Steps
(a)-(c) generate sample (t + 1) sample (t). every variable Ci C sequence,
(t)
main computation step (a), distribution P (Ci , ci , e) Ci generated.
requires executing JT C every value ci D(Ci ), separately. step (b),
conditional distribution derived normalization. Finally, step (c) samples new value
(t)
obtained distribution. Note use P (Ci |ci , e) short-hand notation
(t+1)

(t+1)

(t)

(t)

P (Ci |c1
, ..., ci1 , ci+1 , ..., ck , e). Namely, sample new value variable
Ci , values variables C1 Ci1 already updated.
next demonstrate process using special case loop-cutset (see Definition 2.1).
Example 3.1 Consider belief network previously shown Figure 1 observed node
E = e loop-cutset {A, D}. begin sampling process initializing sampling variables
a(0) d(0) . Next, compute new sample values a(1) , d(1) follows:
P (A|d(0) , e)

=

PJT C (A, c(0) , e)
(0)

(24)

(1)


P (D|a(1) , e)


=

P (A|d , e)
PJT C (D, a(1) , e)

(25)
(26)

d(1)



P (D|a(1) , e)

(27)

process corresponds two iterations inner loop Figure 3. Eq. (24)-(25),
sample new value variable A, correspond steps (a)-(c) first iteration. second
iteration, Eq.(26)-(27), sample new value variable D. Since conditioned network
poly-tree (Figure 1, right), computing probabilities PJT C (A|d(t) , e) PJT C (D|a(t+1) , e) via JT C
reduces Pearls belief propagation algorithm distributions computed linear time.

3.2 Estimating Posterior Marginals
set samples subset variables C generated, estimate posterior
marginals variable network using mixture estimator. sampling variables,
estimator takes form similar Eq.(12):

1X
(t)
P (Ci |e) =
P (Ci |ci , e)


(28)

t=1

variables X\C, E, posterior marginal estimator is:

1X
P (Xi |e) =
P (Xi |c(t) , e)


(29)

t=1

use JT C(Xi , c(t) , e) obtain distribution P (Xi |c(t) , e) input Bayesian
network conditioned c(t) e shown before.
(t)
maintain running sum computed distributions P (Ci |ci , e) P (Xi |c(t) , e)
sample generation, sums right hand side Eq.(28)-(29) readily
available. noted before, estimators P (Ci |e) P (Xi |e) guaranteed converge corresponding exact posterior marginals increases long Markov
14

fiCutset Sampling Bayesian Networks

chain cutset C ergodic. cutset variables estimator simple
ergodic average, Xi X\C, E convergence derived directly first
principles:
Theorem 3.2 Given Bayesian network B X, evidence variables E X, cutset
C X\E, given set samples c(1) , c(2) , ..., c(T ) obtained via Gibbs sampling
P (C|e), assuming Markov chain corresponding sampling C ergodic,
Xi X\C, E assuming P (Xi |E) defined Eq.(29), P (Xi |e) P (Xi |e)
.
Proof. definition:

1X
P (Xi |c(t) , e)
P (Xi |e) =


(30)

t=1

Instead summing samples, rewrite expression sum
possible tuples c D(C) group together samples corresponding tuple
instanceP
c. Let q(c) denote number times tuple C = c occurs set samples
cD(C) q(c) = . easy see that:
P (Xi |e) =

fraction

q(c)


X

cD(C)

P (Xi |c, e)

q(c)


(31)

histogram estimator posterior marginal P (c|e). Thus, get:
X
P (Xi |e) =
P (Xi |c, e)P (c|e)
(32)
cD(C)

Since Markov chain formed samples C ergodic, P (c|e) P (c|e)
therefore:
X
P (Xi |e)
P (Xi |c, e)P (c|e) = P (Xi |e)
cD(C)



3.3 Complexity
time space complexity generating samples estimating posterior marginals
via cutset sampling dominated complexity JT C line (a) algorithm
(Figure 3). linear amount additional memory required maintain running
(t)
sums P (Ci |ci , e) P (Xi |c(t) , e) used posterior marginal estimators.
3.3.1 Sample Generation Complexity
Clearly, JT C applied network B conditioned cutset variables C
evidence variables E, complexity time space exponential induced width
w conditioned network. O(N d(w+1) ) C w-cutset (see Definition 2.6).
15

fiBidyuk & Dechter

Using notion w-cutset, balance sampling exact inference. one end
spectrum plain Gibbs sampling sample generation fast, requiring
linear space, may high variance. end, exact algorithm
requiring time space exponential induced width moral graph.
two extremes, control time space complexity using w follows.
Theorem 3.3 (Complexity sample generation) Given network B X, evidence E, w-cutset C, complexity generating new sample time space
O(|C| N d(w+2) ) bounds variables domain size N = |X|.
Proof. C w-cutset maximum domain size, complexity
(t)
computing joint probability P (ci , ci , e) conditioned network O(N d(w+1) ).
Since operation must repeated ci D(Ci ), complexity processing one
(t)
variable (computing distribution P (Ci |ci , e)) O(N d(w+1) ) = O(N d(w+2) ). Finally,
since ordered Gibbs sampling requires sampling variable cutset, generating one
sample O(|C| N d(w+2) ).

3.3.2 Complexity Estimator Computation
posterior marginals cutset variable Ci C easily obtained end
sampling process without incurring additional computation overhead. mentioned earlier,
(t)
need maintain running sum probabilities P (ci |ci , e) ci D(Ci ).
Estimating P (Xi |e), Xi X\C, E, using Eq.(29) requires computing P (Xi |c(t) , e)
sample c(t) generated. summary:
Theorem 3.4 (Computing Marginals) Given w-cutset C, complexity computing posteriors variables Xi X\E using samples cutset variables
O(T [|C| + d] N d(w+1) ).
Proof. showed Theorem 3.3, complexity generating one sample O(|C|
N d(w+2) ). sample c(t) generated, computation posterior marginals
remaining variables requires computing P (Xi |c(t) , e) via JT C(Xi , c(t) , e)
O(N d(w+1) ). combined computation time one sample O(|C| N d(w+2) +
N d(w+1) ) = O([|C| + d] N d(w+1) ). Repeating computation samples, yields
O(T [|C| + d] N d(w+1) ).
Note space complexity w-cutset sampling bounded O(N d(w+1) ).

3.3.3 Complexity Loop-Cutset
cutset C loop-cutset, algorithm JT C reduces belief propagation (Pearl,
(t)
1988) computes joint distribution P (Ci , ci , e) linear time. refer
special case loop-cutset sampling general w-cutset sampling.
loop-cutset w-cutset w equals maximum number unobserved
parents (upper bounded maximum indegree node). However, since processing
poly-trees linear even large w, induced width capture complexity
16

fiCutset Sampling Bayesian Networks

properly. notion loop-cutset could better captured via hyperwidth
network (Gottlob, Leone, & Scarello, 1999; Kask et al., 2005). hyperwidth polytree 1 therefore, loop-cutset defined 1-hypercutset. Alternatively,
express complexity via networks input size captures total size
conditional probability tables processed follows:
Theorem 3.5 (Complexity loop-cutset sample generation) C loop-cutset,
complexity generating sample O(|C| ) size input
network.
Proof. loop-cutset network instantiated, belief propagation (BP)
(t)
compute joint probability P (ci , ci , e) linear time O(M ) (Pearl, 1988) yielding total
time space O(|C| ) sample.

3.4 Optimizing Cutset Sampling Performance
analysis complexity generating samples (Theorem 3.3) overly pessimistic
assuming computation sampling distribution variable cutset
independent. variables may change value moving one sample
next, change occurs one variable time sequence much
computation retained moving one variable next .
show sampling cutset variables done efficiently
reducing factor N |C| Theorem 3.3 (N + |C| ) bounds number
clusters tree decomposition used JT C contains node Ci C. assume
control order cutset variables sampled.

X1
X1X2Y1

Y1

Y2

Yn-2

Yn-1

X2

X3

Xn-1

Xn

X2X3Y2

X3X4Y3

Xn-1XnYn-1

Figure 4: Bayesian network (top) corresponding cluster-tree (bottom).
Consider simple network variables X={X1 , ....Xn }, ={Y1 , ..., Yn1 } CPTs
P (Xi+1 |Xi , Yi ) P (Yi+1 |Xi ) defined every shown Figure 4, top. join-tree
network chain cliques size 3 given Figure 4, bottom. Since loopcutset, sample variables . Lets assume use ordering Y1 , Y2 , ...Yn1
generate sample. Given current sample, ready generate next sample
applying JT C (or bucket-elimination) network whose cutset variables assigned.
17

fiBidyuk & Dechter

makes network effectively singly-connected leaves 2 actual variables
cluster. algorithm sends message cluster containing Xn towards
cluster containing X1 . cluster (X1 , X2 , Y1 ) gets relevant message cluster
(X2 , X3 , Y2 ) sample Y1 . accomplished linear computations clique
(X1 , X2 , Y1 ) yi D(Yi ) yielding desired distribution P (Y1 |.) (we multiply
functions incoming messages cluster, sum X1 X2 normalize).
cutset w-cutset, computation single clique O(d(w+1) ).
P (Y1 |), Y1 sampled assigned new value, y1 . Cluster (X1 , X2 , Y1 =
y1 ) sends message cluster (X2 , X3 , Y2 ) information necessary
compute P (Y2 |.) O(d(w+2) ). P (Y2 |.) available, new value Y2 = y2 sampled.
cluster computes sends message cluster (X3 , X4 , Y3 ), on.
end, obtain full sample via two message passes conditioned network
computation complexity O(N d(w+2) ). example generalized follows.
Theorem 3.6 Given Bayesian network N variables, w-cutset C, tree-decomposition
Tr , given sample c1 , ..., c|C| , new sample generated O((N + |C| ) d(w+2) )
maximum number clusters containing variable Ci C.
Proof. Given w-cutset C, definition, exists tree-decomposition Tr network
(that includes cutset variables) cutset variables C removed,
number variables remaining cluster Tr bounded w + 1. Lets impose
directionality Tr starting arbitrary cluster call R shown Figure 5. Let
TCi denote connected subtree Tr whose clusters include Ci . Figure 5, clarity,
collapse subtree Ci single node. assume cutset nodes
sampled depth-first traversal order dictated cluster tree rooted R.

TC1 R
TC2
TCk
TC3

TC4

TC6

TC5
Figure 5: cluster-tree rooted cluster R subtree cutset node Ci
collapsed single node marked TCi .

18

fiCutset Sampling Bayesian Networks

Given sample c(t) , JT C send messages leaves Tr towards root cluster.
assume without loss generality R contains cutset node C1 first
sampled c(t+1) . JTC pass messages root clusters restricted
(t)
TC1 (note R TC1 ). Based messages P (C1 = c1 , c1 ) computed
O(d(w+1) ). repeat computation value C1 involving
clusters TC1 obtain distribution P (C1 |) O(d(w+2) ) sample new value
C1 . Thus, C1 appears clusters, number message passing computations
(after initial O(N ) pass) O() generate first distribution P (C1 |)
O( d(w+2) ).
next node depth-first traversal order TC2 thus, second variable
sampled C2 . distance variables C1 C2 , denoted dist1,2 , shortest
path along Tr cluster contains C1 cluster contains C2 . apply JTCs
mesage-passing along path take O(dist1,2 d(w+1) ). Then,
obtain conditional distribution P (C2 |), recompute messages subtree
TC2 value c2 D(C2 ) O( d(w+2) ). continue computation similar
manner cutset nodes.
JT C traverses tree depth-first order, needs pass messages along
P|C|
edge twice (see Figure 5). Thus, sum distances traveled i=2 disti,i1 =
O(N ). may repeated computation value sampled variable.
This, however, accomplished via message-passing restricted individual variables
subtrees bounded . conclude new full sample generated
O((N + |C| ) d(w+2) ).
worthwhile noting complexity generating sample reduced
factor d/(d1) (which amounts factor 2 = 2) noticing whenever
(t+1)
(t+1) (t)
(t)
move variable Ci Ci+1 , joint probability P (c1
, ..., ci
, ci+1 , ..., ck )
already available previous round recomputed. need
(t+1)
(t+1)
(t)
(t)
compute P (c1
, ..., ci
, ci+1 , ..., ck ) ci+1 6= ci+1 . Buffering last computed
joint probability, need apply JT C algorithm 1 times. Therefore, total
complexity generating new sample O((N + |C| ) (d 1) d(w+1) ).
Example 3.7 Figure 6 demonstrates application enhancements discussed.
depicts moral graph (a), already triangulated, corresponding join-tree (b)
Bayesian network Figure 1. evidence variable E removed, variables B form
1-cutset. join-tree network cutset evidence variables removed shown
Figure 6 (c). Since removing E cluster DF E leaves one variable, F ,
combine clusters BDF DF E one cluster, F G. Assuming cutset variables
domains size 2, initialize B = b0 = d0 .
Selecting cluster AC root tree, JT C first propagates messages leaves
root shown Figure 6 (c) computes P (b0 , d0 , e) cluster AC. Next,
set B = b1 ; updating functions containing variable B, propagating messages
subtree B consisting clusters AC CF (Figure 6 (d)), obtain P (b1 , d0 , e).
Normalizing two joint probabilities, obtain distribution P (B|d0 , e) sample new
value B. Assume sampled value b1 .
19

fiBidyuk & Dechter

ABC
P(B|A),P(C|A),
P(A)



AC
P(b0|A),P(C|A),
P(A)

AC
P(b1|A),P(C|A),
P(A)

AC
P(b1|A),P(C|A),
P(A)

AC
P(b1|A),P(C|A),
P(A)

CF
P(F|b0,C),P(d0|b0)

CF
P(F|b1,C),P(d0|b1)

CF
P(F|b1,C),P(d1|b1)

CF
P(F|b1,C),P(d0|b1)

FG
P(e|d0,F),P(G|d0,F)

FG
P(e|d0,F),P(G|d0,F)

FG
P(e|d1,F),P(G|d1,F)

FG
P(e|d0,F),P(G|d0,F)

B
C

BCF
P(F|B,C)

F
G

DFG
P(D|B), P(G|D,F)



E

DFE
P(E|D,F)

B=b0, D=d0, E=e
(a)

(b)

(c)

B=b1

D=d1

D=d0

(d)

(e)

(f)

Figure 6: join-tree width 2 (b) moral graph (a) transformed join-tree
width 1 (c) evidence variable E cutset variables B instantiated (in process, clusters BDF BCF merged cluster CF ).
clusters contain variables functions original network. cutset
nodes domains size 2, D(B) = {b0 , b1 }, D(D) = {d0 , d1 }. Starting
sample {b0 , d0 }, messages propagated (c)-(e) first, sample new value
variable B (d) variable (e). messages propagated
tree compute posterior marginals P (|b1 , d0 , e) rest variables (f).

Next, need compute P (D|b1 , e) sample new value variable D. joint
probability P (d0 , b1 , e) readily available since computed sampling new value
B. Thus, set = d1 compute second probability P (d1 , b1 , e) updating functions
clusters CF F G sending updated message CF F G (Figure 6 (e)).
obtain distribution P (D|b1 , e) normalizing joint probabilities sample new
value d0 D. Since value changed latest computation, update
functions clusters CF F G propagate updated messages subtree CD
(send message CF F G).
order obtain distributions P (|b1 , d0 , e) remaining variables A, C, F ,
G, need send updated messages join-tree, F G CF
CF AC shown Figure 6 (f ). last step serves initialization
step next sample generation.
example performance cutset sampling significantly better worst
case. sent total 5 messages generate new sample worst case
suggests least N |C| = 3 2 2 = 12 messages (here, N equals number clusters).
20

fiCutset Sampling Bayesian Networks

3.5 finding w-Cutset
Clearly, w-cutset sampling effective w-cutset small. calls
task finding minimum size w-cutset. problem NP-hard; yet, several heuristic
algorithms proposed. next briefly survey proposals.
Larossa Dechter (2003) obtain w-cutset processing variables elimination
order. next node eliminated (selected using triangulation heuristics) added
cutset current induced width (or degree) greater w. Geiger Fishelson
(2003) agument idea various heuristics.
Bidyuk Dechter (2003) select variables included cutset using greedy
heuristics based nodes basic graph properties (such degree node). One
scheme starts empty w-cutset heuristically adds nodes cutset
tree-decomposition width w obtained. scheme starts set
C = X\E containing nodes network cutset removes nodes
set order. algorithm stops removing next node would result tree
decomposition width > w.
Alternatively, Bidyuk Dechter (2004) proposed first obtain tree-decomposition
network find minimal w-cutset tree-decomposition (also NPhard problem) via well-known greedy algorithm used set cover problem. approach
shown yield smaller cutset previously proposed heuristics used finding
w-cutset experiments (section 4.4) modification tree-decomposition
re-computed time node removed tree added w-cutset.

4. Experiments
section, present empirical studies cutset sampling algorithms several classes
problems. use mean square error posterior marginals estimates
measure accuracy. compare traditional Gibbs sampling, likelihood weighting
(Fung & Chang, 1989; Shachter & Peot, 1989), state art AIS-BN adaptive
importance sampling algorithm (Cheng & Druzdzel, 2000). implemented AIS-BN using
parameters specified Cheng Druzdzel (2000). using implementation,
made sure sampling algorithms used data access routines
error measures providing uniform framework comparing performance.
reference report performance Iterative Belief Propagation (IBP) algorithm.
4.1 Methodology
section detail describe methodology used implementation decisions made
apply collection empirical results.
4.1.1 Sampling Methodology
sampling algorithms restarted Markov chain every samples. samples
chain (batch) averaged separately:
Pm (xi |e) =


1X
P (xi |c(t) , e)

t=1

21

fiBidyuk & Dechter

final estimate obtained sample average chains:

1 X
Pm (xi |e)
P (xi |e) =

m=1

Restarting Markov chain known improve sampling convergence rate. single
chain become stuck generating samples single high-probability region without
ever exploring large number high-probability tuples. restarting Markov chain
different random point, sampling algorithm achieve better coverage
sampling space. experiments, observe significant difference
estimates obtained single chain size chains size therefore,
choose report results multiple Markov chains. However, rely
independence random values Pm (xi |e) estimate 90% confidence interval P (xi |e).
implementation Gibbs sampling schemes, use zero burn-in time (see
section 2.3.1). mentioned earlier, idea burn-in time throw away
first K samples ensure remaining samples drawn distribution close
target distribution P (X|e). conservative methods estimating K drift
minorization conditions proposed Rosenthal (1995) Roberts Tweedie
(1999, 2001), required analysis beyond scope paper. consider
comparison Gibbs sampling cutset sampling, objective, fair
sense schemes use K=0. Further, experimental results showed positive
indication burn-in time would beneficial. practice, burn-in pre-processing
time used algorithm find high-probability regions distribution P (C|e)
case initially spends disproportionally large period time low probability regions.
Discarding large number low-probability tuples obtained initially, frequency
remaining high-probability tuples automatically adjusted better reflect weight.
cpcs360b, N=360, |E|=32, w*=21

cpcs360b, N=360, |E|=32, |LC|=26, w*=21

LCS
800

1.40E-04

700

# unique samples

1.60E-04

1.20E-04

MSE

1.00E-04
8.00E-05
6.00E-05
4.00E-05

LCS

600
500
400
300
200
100

2.00E-05

0

0.00E+00

0

2000

4000

6000

8000

10000

0

2000

4000

6000

8000

10000

# samples

# samples

Figure 7: Comparing loop-cutset sampling MSE vs. number samples (left) number unique samples vs. number samples (right) cpcs360b. Results
averaged 10 instances different observations.

benchmarks, observed full Gibbs sampling cutset sampling
able find high probability tuples fast relative number samples generated.
example, one benchmarks, cpcs360b, rate generating unique samples,
22

fiCutset Sampling Bayesian Networks

namely, ratio cutset instances seen number samples,
decreases time. Specifically, loop-cutset sampling generates 200 unique tuples
first 1000 samples, additional 100 unique tuples generating next 1000 samples,
rate generating unique tuples slows 50 per 1000 samples range
2000 10000 samples shown Figure 7, right. means first
hundred samples, algorithm spends time revisiting high-probability
tuples. benchmarks, number unique tuple instances generated increases
linearly (as cpcs54) and, thus, tuples appear distributed nearly uniformly.
case, need burn-in strongly-expressed heavy-weight
tuples. Instead using burn-in times, sample initial variable values posterior
marginal estimates generated IBP experiments. sampling time includes
pre-processing time IBP.
experiments performed 1.8 GHz CPU.
4.1.2 Measures Performance
problem instance defined Bayesian network B variables X = {X1 , ..., Xn }
evidence E X, derived exact posterior marginals P (Xi |e) using bucket-tree
elimination (Dechter, 2003, 1999a) computed mean square error (MSE)
approximate posterior marginals P (Xi |e) algorithm MSE defined by:
X X
1
[P (xi |e) P (xi |e)]2
SE = P
|D(X
)|

Xi X\E
Xi X\E D(Xi )

mean square error primary accuracy measure, results consistent
across well-known measures average absolute error, KL-distance, squared
Hellingers distance show loop-cutset sampling. absolute error
averaged values unobserved variables:
X X
1
= P
|P (xi |e) P (xi |e)|
Xi X\E |D(Xi )|
Xi X\E D(Xi )

KL-distance DK distribution P (Xi |e) estimator P (Xi |e) defined
follows:
X
P (xi |e)
DK (P (Xi |e), P (Xi |e)) =
P (xi |e) lg
P (xi |e)
D(X )


benchmark instance, compute KL-distance variable Xi X\E
average results:
X
1
DK (P (Xi |e), P (Xi |e))
DK (P, P ) =
|X\E|
Xi X\E

squared Hellingers distance DH distribution P (Xi |e) estimator
P (Xi |e) obtained as:
q
X p
DH (P (Xi |e), P (Xi |e)) =
[ P (xi |e) P (xi |e)]2
D(Xi )

23

fiBidyuk & Dechter

average squared Hellingers distance benchmark instance average
distances posterior distributions one variable:
DH (P, P ) =

1
|X\E|

X

Xi X\E

DH (P (Xi |e), P (Xi |e))

average errors different network instances averaged instances
given network (typically, 20 instances).
report confidence interval estimate P (xi |e) using approach similar
well-known batch means method (Billingsley, 1968; Geyer, 1992; Steiger & Wilson,
2001). Since chains restarted independently, estimates Pm (xi |e) independent.
Thus, confidence interval obtained measuring variance estimators
P (Xi |e). report results Section 4.5.
4.2 Benchmarks
experimented four classes networks:
CPCS. considered four CPCS networks derived Computer-based Patient
Case Simulation system (Parker & Miller, 1987; Pradhan, Provan, Middleton, & Henrion,
1994). CPCS network representation based INTERNIST 1 (Miller, Pople, & Myers,
1982) Quick Medical Reference (QMR) (Miller, Masarie, & Myers, 1986) expert systems. nodes CPCS networks correspond diseases findings conditional
probabilities describe correlations. cpcs54 network consists N =54 nodes
relatively large loop-cutset size |LC|=16 (> 25% nodes). induced width
15. cpcs179 network consists N =179 nodes. induced width w =8.
small loop-cutset size |LC|=8 relatively large corresponding adjusted induced
width wLC =7. cpcs360b larger CPCS network 360 nodes, adjusted induced
width 21, loop-cutset |LC|=26. Exact inference cpcs360b averaged 30 minutes.
largest network, cpcs422b, consisted 422 nodes induced width w =22
loop-cutset size 47. exact inference time cpcs422b 50 minutes.
Hailfinder network. small network 56 nodes. exact inference
Hailfinder network easy since loop-cutset size 5. Yet, network
zero probabilities and, therefore, good benchmark demonstrating convergence
cutset sampling contrast Gibbs sampling.
Random networks. experimented several classes random networks: random networks, 2-layer networks, grid networks. random networks generated
N =200 binary nodes (domains size 2). first 100 nodes, {X1 , ..., X100 },
designated root nodes. non-root node Xi , > 100, assigned 3 parents selected
randomly list predecessors {X1 , ..., Xi1 }. refer class random
networks multi-partite random networks distinguish bi-partite (2-layer) random
networks. random 2-layer networks generated 50 root nodes (first layer)
150 leaf nodes (second layer), yielding total 200 nodes. sample 2-layer random
network shown Figure 8, left. non-root node (second layer) assigned 1-3
parents selected random among root nodes. nodes assigned domain size
2, D(Xi ) = {x0i , x1i }.
24

fiCutset Sampling Bayesian Networks

Figure 8: Sample random networks: 2-layer (left), grid (center), coding (right).

2-layer multi-partite random networks, root nodes assigned uniform priors conditional probabilities chosen randomly. Namely, value
P (x0i |pai ) drawn uniform distribution interval (0, 1) used compute
complementary probability value P (x1i |pai ) = 1 P (x0i |pai ).
directed grid networks (as opposed grid-shaped undirected Markov Random
Fields) size 15x30 450 nodes constructed uniform priors (on single
root node) random conditional probability tables (as described above). sample grid
network shown Figure 8, center. networks average induced width
size 20 (exact inference using bucket elimination required 30 minutes).
regular structure largest loop-cutset containing nearly half
unobserved nodes.
Coding networks. experimented coding networks 50 code bits 50
parity check bits. parity check matrix randomized; parity check bit three
parents. sample coding network 4 code bits, 4 parity checking bits, total 8
transmitted bits shown Figure 8, center. total number variables network
experiments 200 (50 code bits, 50 parity check bits, 1 transmitted bit
code parity check bit). average loop-cutset size 26 induced width
21. Markov chain produced Gibbs sampling whole coding network
ergodic due deterministic parity check function. result, Gibbs sampling
converge. However, Markov chain corresponding sampling subspace coding
bits ergodic and, thus, cutset sampling schemes converged
show next two sections.
networks, except coding grid networks, evidence nodes selected random
among leaf nodes (nodes without children). Since grid network one leaf
node, evidence grid networks selected random among nodes.
benchmark, report chart title number nodes network N , average
number evidence nodes |E|, size loop-cutset |LC|, average induced width
input instance denoted w distinguish induced width w network adjusted
w-cutset.
25

fiBidyuk & Dechter

4.3 Results Loop-Cutset Sampling
section compare loop-cutset sampling pure Gibbs sampling, likelihood
weighting, AIS-BN, IBP. benchmarks, cutset selected evidence
sampling nodes together constitute loop-cutset network using algorithm
proposed Becker et al. (2000). show accuracy Gibbs loop-cutset sampling
function number samples time.
CPCS networks. results summarized Figures 9-12. loop-cutset curve
chart denoted LCS (for Loop Cutset Sampling). induced width network
wLC loop-cutset nodes observed specified caption. identical
largest family size poly-tree generated cutset variables removed. plot
time x-axis accuracy (MSE) y-axis. CPCS networks, IBP
always converged converged fast (within seconds). Consequently, IBP curve always
straight horizontal line results change convergence achieved.
curves corresponding Gibbs sampling, loop-cutset sampling, likelihood weighting,
AIS-BN demonstrate convergence sampling schemes time. three
CPCS networks loop-cutset sampling converges much faster Gibbs sampling.
exception cpcs422b (Figure 12, right) induced width conditioned singlyconnected network remains high (wLC = 14) due large family sizes thus, loop-cutset
sampling generates samples slowly (4 samples/second) compared Gibbs sampling
(300 samples/second). Since computing sampling distribution exponential w, sampling
single variable O(214 ) (all variables domains size 2). result, although loopcutset sampling shows significant reduction MSE function number samples
(Figure 12, left), enough compensate two orders magnitude difference
loop-cutset rate sample generation. cpcs54 (Figure 9), cpcs179 (Figure 10),
cpcs360b (Figure 11) loop-cutset sampling achieves greater accuracy IBP within
10 seconds less.
comparison importance sampling schemes, observe AIS-BN algorithm consistently outperforms likelihood weighting AIS-BN slightly better loopcutset sampling cpcs54, probability evidence P (e)=0.0928 relatively high.
cpcs179, probability evidence P (e)=4E-05 smaller, LCS outperforms AIS-BN
Gibbs sampling curves falls AIS-BN likelihood weighting. Gibbs
sampling loop-cutset sampling outperform AIS-BN cpcs360b cpcs422b
probability evidence small. cpcs360b average P (e)=5E-8 cpcs422b probability evidence varies 4E-17 8E-47. Note likelihood weighting AIS-BN
performed considerably worse either Gibbs sampling loop-cutset sampling
benchmarks function number samples. Consequently, left
charts showing convergence Gibbs loop-cutset sampling function
number samples order zoom two algorithms focus
empirical studies.
Coding Networks. results coding networks shown Figure 13.
computed error measures coding bits averaged 100 instances (10 instances,
different observed values, 10 networks different coding matrices).
noted earlier, Markov chains corresponding Gibbs sampling coding networks
ergodic and, result, Gibbs sampling converge. However, Markov
26

fiCutset Sampling Bayesian Networks

Gibbs

cpcs54, N=54, |LC|=16, w*=15, |E|=8

LW

cpcs54, N=54, |LC|=16, w*=15, |E|=8

LCS

4.0E-04

AIS-BN

3.0E-04

Gibbs

IBP

3.5E-04

2.5E-04

LCS

2.0E-04

IBP

3.0E-04

MSE

MSE

2.5E-04
2.0E-04
1.5E-04

1.5E-04
1.0E-04

1.0E-04

5.0E-05

5.0E-05

0.0E+00

0.0E+00
0

5000

10000

15000

20000

25000

0

30000

2

4

6

# samples
LW

cpcs54, N=54, |LC|=16, w*=15, |E|=8

10

12

14

Hellinger-distance

LCS

1.0E-05

AIS-BN

7.0E-06

Gibbs

1.2E-05

LW

cpcs54, N=54, |LC|=16, w*=15, |E|=8

AIS-BN

1.4E-05

KL-distance

8

Time (sec)

IBP

8.0E-06
6.0E-06
4.0E-06
2.0E-06

Gibbs

6.0E-06

LCS

5.0E-06

IBP

4.0E-06
3.0E-06
2.0E-06
1.0E-06
0.0E+00

0.0E+00
0

2

4

6

8

10

12

0

14

2

4

6

10

12

14

Time (sec)

Time (sec)
LW

cpcs54, N=54, |LC|=16, w*=15, |E|=8

AIS-BN

2.1E-03

Absolute Error

8

Gibbs

1.8E-03

LCS

1.5E-03

IBP

1.2E-03
9.0E-04
6.0E-04
3.0E-04
0.0E+00
0

2

4

6

8

10

12

14

Time (sec)

Figure 9: Comparing loop-cutset sampling (LCS), wLC =5, Gibbs sampling (hereby referred
Gibbs), likelihood weighting (LW), AIS-BN, IBP cpcs54 network,
averaged 20 instances, showing MSE function number samples
(top left) time (top right) KL-distance (middle left), squared Hellingers
distance (middle right), average error (bottom) function time.

27

fiBidyuk & Dechter

cpcs179, N=179, |LC|=8, w*=8, |E|=17

LW

cpcs179, N=179, |LC|=8, w*=8, |E|=17

LW

1.0E-02

AIS-BN

1.0E-01

AIS-BN

Gibbs

Gibbs
LCS

IBP

1.0E-02

IBP

1.0E-03

MSE

Absolute Error

LCS

1.0E-04

1.0E-03

1.0E-05

1.0E-04
0

2

4

6

8

10

12

0

14

2

4

6

8

10

12

cpcs179, N=179, |LC|=8, w*=8, |E|=17

LW

cpcs179, N=179, |LC|=8, w*=8, |E|=17

LW

1.0E+00

AIS-BN

AIS-BN

1.0E-01

Gibbs

1.0E-01

Hellinger-distance

Gibbs

KL-distance

14

Time (sec)

Time (sec)

LCS
IBP

1.0E-02
1.0E-03
1.0E-04
1.0E-05

LCS

1.0E-02

IBP

1.0E-03

1.0E-04

1.0E-05
0

2

4

6

8

10

12

14

0

2

4

6

Time (sec)

8

10

12

14

Time (sec)
LW

cpcs179, N=179, |LC|=8, w*=8, |E|=17

AIS-BN

1.0E-01

Gibbs
Absolute Error

LCS
IBP

1.0E-02

1.0E-03

1.0E-04
0

2

4

6

8

10

12

14

Time (sec)

Figure 10: Comparing loop-cutset sampling (LCS), wLC =7, Gibbs sampling, likelihood
weighting (LW), AIS-BN, IBP cpcs179 network, averaged 20 instances, showing MSE function number samples (top left) time
(top right) KL-distance (middle left), squared Hellingers distance (middle
right), average error (bottom) function time.

28

fiCutset Sampling Bayesian Networks

cpcs360b, N=360, |LC|=26, w*=21, |E|=15

cpcs360b, N=360, |LC|=26, w*=21, |E|=15
1.E-02

Gibbs

2.5E-04

LW
AIS-BN

LCS

2.0E-04

Gibbs

IBP

1.5E-04

LCS

MSE

MSE

1.E-03

1.0E-04

IBP

1.E-04

5.0E-05

1.E-05

0.0E+00
0

5000

10000

15000

20000

0

25000

2

4

6

# samples

cpcs360b, N=360, |LC|=26, w*=21, |E|=15

10

12

14

cpcs360b, N=360, |LC|=26, w*=21, |E|=15
LW

1.E-02

AIS-BN

Hellinger-distance

Gibbs

1.E-03

LCS
IBP

1.E-04

LW

1.E-02

AIS-BN

KL-distance

8

Time (sec)

1.E-05
1.E-06

Gibbs

1.E-03

LCS

1.E-04

IBP

1.E-05
1.E-06
1.E-07

0

2

4

6

8

10

12

14

0

2

4

6

Time (sec)

8

10

12

14

Time (sec)

cpcs360b, N=360, |LC|=26, w*=21, |E|=15
1.E-01

LW

Absolute Error

AIS-BN
Gibbs

1.E-02

LCS
IBP

1.E-03

1.E-04
0

5

10

15

20

25

30

Time (sec)

Figure 11: Comparing loop-cutset sampling (LCS), wLC =3, Gibbs sampling, likelihood
weighting (LW), AIS-BN, IBP cpcs360b network, averaged 20 instances, showing MSE function number samples (top left) time
(top right) KL-distance (middle left), squared Hellingers distance (middle
right), average error (bottom) function time.

29

fiBidyuk & Dechter

cpcs422b, N=422, |LC|=47, w*=22, |E|=28

Gibbs

cpcs422b, N=422, |LC|=47, w*=22, |E|=28

LCS

4.0E-04

LW
AIS-BN

1.0E-02

Gibbs

IBP

3.5E-04

LCS

3.0E-04

MSE

2.5E-04

MSE

IBP

1.0E-03

2.0E-04
1.5E-04

1.0E-04

1.0E-04
5.0E-05
1.0E-05

0.0E+00
0

1000

2000

3000

4000

5000

0

6000

10

20

LW

cpcs422b, N=422, |LC|=47, w*=22, |E|=28

40

50

60

LW

cpcs422b, N=422, |LC|=47, w*=22, |E|=28

AIS-BN

1.0E+00

AIS-BN

Gibbs

1.0E-01

LCS

1.0E-02

IBP

Gibbs

1.0E-01

Hellinger-distance

KL-distance

30

Time (sec)

# samples

1.0E-03
1.0E-04
1.0E-05
1.0E-06

LCS
1.0E-02

IBP

1.0E-03
1.0E-04
1.0E-05
1.0E-06

0

10

20

30

40

50

60

0

10

20

Time (sec)

30

40

50

60

Time (sec)
cpcs422b, N=422, |LC|=47, w*=22, |E|=28

LW
AIS-BN

1.0E-01

Gibbs

Absolute Error

LCS
IBP

1.0E-02

1.0E-03

1.0E-04
0

10

20

30

40

50

60

Time (sec)

Figure 12: Comparing loop-cutset sampling (LCS), wLC =14, Gibbs sampling, likelihood
weighting (LW), AIS-BN sampling, IBP cpcs422b network, averaged
10 instances, showing MSE function number samples (top
left) time (top right) KL-distance (middle left), squared Hellingers
distance (middle right), average error (bottom) function time.

30

fiCutset Sampling Bayesian Networks

1.0E-01

1.0E-02

1.0E-02

1.0E-03
1.0E-04

1.0E-03

1.0E-05

1.0E-04
0

2

4

6

8

0

10

2

4

Time (sec)

6

8

10

Time (sec)
LW
AIS-BN
Gibbs
LCS
IBP

1.0E+00

coding, N=200, P=3, |LC|=26, w*=21

LW
AIS-BN
Gibbs
LCS
IBP

1.0E+00

Hellinger-distance

coding, N=200, P=3, |LC|=26, w*=21
1.0E+01

KL-distance

LW
AIS-BN
Gibbs
LCS
IBP

1.0E-01

MSE

Absolute Error

coding, N=200, P=3, |LC|=26, w*=21

LW
AIS-BN
Gibbs
LCS
IBP

coding, N=200, P=3, |LC|=26, w*=21
1.0E+00

1.0E-01
1.0E-02
1.0E-03
1.0E-04
1.0E-05

1.0E-01
1.0E-02
1.0E-03
1.0E-04
1.0E-05

0

2

4

6

8

0

10

Time (sec)

2

4

6

8

10

Time (sec)

Figure 13: Comparing loop-cutset sampling (LCS), wLC =3, Gibbs sampling, likelihood
weighting (LW), AIS-BN, IBP coding networks, =0.4, averaged
10 instances 10 coding networks (100 instances total). graphs show average absolute error ( top left), MSE (top right), KL-distance (bottom left),
squared Hellingers distance (bottom right) function time.

chain corresponding sampling subspace code bits ergodic therefore,
loop-cutset sampling, samples subset coding bits, converges even achieves
higher accuracy IBP time. reality, IBP certainly preferable coding
networks since size loop-cutset grows linearly number code bits.
Random networks. random multi-part networks (Figure 14, top) random
2-layer networks (Figure 14, middle), loop-cutset sampling always converged faster
Gibbs sampling. results averaged 10 instances network type.
cases, loop-cutset sampling achieved accuracy IBP 2 seconds less. 2-layer
networks, Iterative Belief Propagation performed particularly poorly. Gibbs sampling
loop-cutset sampling obtained accurate results less second.
Hailfinder network. used network (in addition coding networks) compare behavior cutset sampling Gibbs sampling deterministic networks. Since
Hailfinder network contains many deterministic probabilities, Markov chain corresponding Gibbs sampling variables non-ergodic. expected, Gibbs sampling fails
loop-cutset sampling computes accurate marginals (Figure 15).
31

fiBidyuk & Dechter

random, N=200, |E|=20, |C|=30, w*=22

2-layer, R=50, P=3, N=200, |E|=16, |LC|=17, w*=16

Gibbs

1.8E-04

LCS

1.5E-04

IBP

1.0E-01

Gibbs
LCS

1.0E-02

IBP

MSE

MSE

1.2E-04
9.0E-05
6.0E-05

1.0E-03
1.0E-04

3.0E-05
1.0E-05

0.0E+00
0

5

10

15

20

25

0

30

2

4

Time (sec)

6

8

10

12

Time (sec)

Figure 14: Comparing loop-cutset sampling (LCS), Gibbs sampling, IBP random
networks (left) 2-layer random networks (right), wLC =3 classes
networks, averaged 10 instances each. MSE function time.

Hailfinder, N=56, |LC|=5, w*=5, |E|=4

Gibbs

1.0E-01

LCS
IBP

MSE

1.0E-02

1.0E-03

1.0E-04
0

1

2

3

4

5

6

7

Time (sec)

Figure 15: Comparing loop-cutset sampling (LCS), wLC =7, Gibbs sampling, IBP
Hailfinder network, 10 instances. MSE function time.

summary, empirical results demonstrate loop-cutset sampling cost-effective

time-wise superior Gibbs sampling. measured ratio R = Mgc number
samples Mg generated Gibbs number samples Mc generated loop-cutset
sampling time period (it relatively constant given network
changes slightly problem instances differ observations). cpcs54,
cpcs179, cpcs360b, cpcs422b ratios correspondingly 2.5, 3.75, 0.7, 75
(see Table 2 section 4.4). obtained R=2.0 random networks R=0.3
random 2-layer networks. ratio values > 1 indicate Gibbs sampler generates
32

fiCutset Sampling Bayesian Networks

samples faster loop-cutset sampling usually case. instances,
variance reduction compensated increased computation time fewer samples
needed converge resulting overall better performance loop-cutset sampling
compared Gibbs sampling. cases, however, reduction sample size
compensates overhead computation sampling one variable value.
cases, loop-cutset sampling generated samples faster Gibbs yielding ratio R < 1.
Then, improvement accuracy due larger number samples
faster convergence.
4.4 w-Cutset Sampling
section, compare general w-cutset scheme different values w
Gibbs sampling. main goal study performance w-cutset sampling
varies w. completeness sake, include results loop-cutset sampling shown
section 4.3.
empirical study, used greedy algorithm set cover problem, mentioned
section 3.5, finding minimal w-cutset. apply algorithm manner
(w + 1)-cutset proper subset w-cutset and, thus, expected
lower variance converge faster sampling w-cutset terms number samples
required (following Rao-Blackwellisation theory). focus empirical study
trade-offs cutset size reduction associated increase sample generation
time gradually increase bound w.
used benchmarks included grid networks. sampling
algorithms given fixed time bound. sampling small networks, cpcs54
(w =15) cpcs179 (w =8), exact inference easy, sampling algorithms
allocated 10 seconds 20 seconds respectively. larger networks allocated 100-200
seconds depending complexity network fraction exact
computation time.
Table 1 reports size sampling set used algorithm column
reports size corresponding w-cutset. example, cpcs360b, average
size Gibbs sample (all nodes except evidence) 345, loop-cutset size 26, size
2-cutset 22, on. Table 2 shows rate sample generation different
algorithms per second. observed previously case loop-cutset sampling,
special cases cutset sampling generated samples faster Gibbs sampler.
example, cpcs360b, loop-cutset sampling 2-cutset sampling generated 600 samples
per second Gibbs sampler able generate 400 samples. attribute
size cutset sample (26 nodes less reported Table 1) compared
size Gibbs sample (over 300 nodes).
CPCS networks. present two charts. One chart demonstrates convergence
time several values w. second chart depicts change quality
approximation (MSE) function w two time points, half total
sampling time end total sampling time. performance Gibbs sampling
cutset sampling cpcs54 shown Figure 16. results averaged 20
instances 5-10 evidence variables. graph left Figure 16 shows mean
square error estimated posterior marginals function time Gibbs sampling,
33

fiBidyuk & Dechter

cpcs54
cpcs179
cpcs360b
cpcs422b
grid15x30
random
2layer
coding

Gibbs
51
162
345
392
410
190
185
100

LC
16
8
26
47
169
30
17
26

w=2
17
11
22
65
163
61
22
38

Sampling Set Size
w=3 w=4 w=5 w=6
15
11
9
8
9
7
5
19
16
15
14
57
50
45
40
119
95
75
60
26
25
24
18
15
13
13
11
23
18
18
-

w=7
13
35
50
17
-

w=8
13
-

Table 1: Markov chain sampling set size function w.

cpcs54
cpcs179
cpcs360b
cpcs422b
grid15x30
random
2layer
coding

Gibbs
5000
1500
400
300
2000
2000
200
2400

LC
2000, w= 5
400, w= 7
600, w= 3
4, w=14
500, w= 2
1000, w= 3
700, w= 3
1000, w= 3

No. Samples
w=2 w=3 w=4 w=5
3000 2400
800
500
400
150
40
10
600
400
160
100
200
150
90
50
300
260
150
105
1400
700
450
300
900
320
150
75
1000
400
200
120

w=6
300
40
30
60
140
40
100

w=7
20
15
35
75
-

w=8
20
-

Table 2: Average number samples generated per second function w.
loop-cutset sampling, w-cutset sampling w=2, 3, 4, 5. second chart shows
accuracy function w. first point corresponds Gibbs sampling; points
correspond loop-cutset sampling w-cutset sampling w ranging 2 6.
loop-cutset result embedded w-cutset values w=5. explained section 3.3,
loop-cutset corresponds w-cutset w maximum number parents
network. Initially, best results obtained 2- 3-cutset sampling followed
loop-cutset sampling. time, 2- 5-cutset sampling become best.
results cpcs179 reported Figure 17. charts show loop-cutset
sampling w-cutset sampling w range 2 5 superior Gibbs sampling.
chart left shows best cutset sampling schemes, lowest
MSE curves, 2- 3-cutset sampling. loop-cutset curve falls 2-
3-cutset first outperformed 2- 3-cutset 12 seconds. Loop-cutset
sampling 2- 3-cutset sampling outperform Gibbs sampling nearly two orders
magnitude MSE falls 1E-04 Gibbs MSE remains order 1E02. 4- 5-cutset sampling results fall between, achieving MSE 1E-03.
curves corresponding loop-cutset sampling 2-, 3- 4-cutset sampling fall
IBP line means four algorithms outperform IBP first seconds
execution (IBP converges less second). 5-cutset outperforms IBP 8
seconds. Figure 17 right, see accuracy results sampling algorithms
34

fiCutset Sampling Bayesian Networks

Gibbs

2.5E-04

MSE

2.0E-04
1.5E-04

IBP

cpcs54, N=54, |LC|=16, w*=15, |E|=8

IBP
LCS,w=5

2.5E-04

|C|=16,w=2
|C|=15,w=3

2.0E-04

|C|=11,w=4
|C|=9,w=5

1.5E-04

1.0E-04

MSE

cpcs54, N=54, |LC|=16, w*=15, |E|=8
3.0E-04

5.0E-05

Cutset, 5 sec
Cutset, 10 sec

1.0E-04
5.0E-05

0.0E+00

0.0E+00
0

2

4

6

8

10

12

14

Gibbs

w=2

w=3

w=4

Time (sec)

LCS,
w=5

w=5

w=6

Figure 16: MSE function time (left) w (right) cpcs54, 20 instances, time
bound=12 seconds.

MSE

1.0E-03

1.0E-04

IBP

cpcs179, N=179, |LC|=8, w*=8, |E|=17

Cutset, 10 sec

1.0E-01

Cutset, 20 sec
1.0E-02

MSE

Gibbs
IBP
LCS,w=7
|C|=11,w=2
|C|=9,w=3
|C|=7,w=4
|C|=5,w=5

cpcs179, N=179, |LC|=8, w*=8, |E|=17
1.0E-02

1.0E-03
1.0E-04
1.0E-05

12

14

Time (sec)

LC
S,
w=
7

10

w=
5

8

w=
4

6

w=
3

4

w=
2

2

G

0

ib
bs

1.0E-05

Figure 17: MSE function time (left) w (right) cpcs179, 20 instances, time
bound=12 seconds. Y-scale exponential due large variation performance
Gibbs cutset sampling.

10 seconds 20 seconds. agreement convergence curves
left.
cpcs360b (Figure 18), loop-cutset sampling 2- 3-cutset sampling similar
performance. accuracy estimates slowly degrades w increases. Loop-cutset
sampling w-cutset sampling substantially outperform Gibbs sampling values w
exceed accuracy IBP within 1 minute.
example cpcs422b, demonstrate significance adjusted induced
width conditioned network performance cutset sampling. reported
section 4.3, loop-cutset relatively small |LC|=47 wLC =14 thus, sampling
one new loop-cutset variable value exponential big adjusted induced width.
result, loop-cutset sampling computes 4 samples per second 2-, 3and 4-cutset, slightly larger 65, 57, 50 nodes respectively (see
Table 1), compute samples rates 200, 150, 90 samples per second (see Table 2).
35

fiBidyuk & Dechter

cpcs360b, N=360, |LC|=26, w*=21, |E|=15

cpcs360b, N=360, |E|=18, |LC|=26, w*=15

Gibbs

1.E-03

1.E-04

|C|=23,w=2
|C|=19,w=3

8.E-05

|C|=16,w=4
|C|=15,w=5

6.E-05

IBP
cutset,t=30sec
cutset,t=60sec

MSE

MSE

1.E-04

IBP
LCS,w=3

4.E-05

1.E-05

2.E-05

G

=7
w

w=
6

=5
w

Time (sec)

=4

70

w

60

=3

50

w

40

=3

30

LC
,w

20



10

ib
b

0

w=
2

0.E+00

1.E-06

Figure 18: MSE function time (left) w (right) cpcs360b, 20 instances, time
bound=60 seconds. Y-scale exponential due large variation performance
Gibbs cutset sampling.

Gibbs
IBP
LCS,w=2
|C|=65,w=2
|C|=57,w=3
|C|=50,w=4
|C|=45,w=5

1.8E-04

MSE

1.5E-04
1.2E-04

cpcs422b, N=422, |LC|=47, w*=22, |E|=28
1.0E-01

IBP
Cutset, 100 sec
Cutset, 200 sec

1.0E-02

MSE

cpcs422b, N=422, |LC|=47, w*=22, |E|=28
2.1E-04

9.0E-05

1.0E-03

6.0E-05

1.0E-04

3.0E-05
0.0E+00
0

20

40

60

80

100

120

140

1.0E-05

Gibbs

Time (sec)

w=2

w=3

w=4

w=5

w=6

w=7

Figure 19: MSE function time (left) w (right) cpcs422b, 10 instances, time
bound=200 seconds. Y-scale exponential due large variation performance
Gibbs cutset sampling.

5-cutset closest loop-cutset size, |C5 | = 45, computes 50 samples per
second order magnitude loop-cutset sampling. results
cpcs422b shown Figure 19. loop-cutset sampling results excluded due
poor performance. chart right Figure 19 shows w-cutset performed well
range w = 2 7 far superior Gibbs sampling. allowed enough time,
w-cutset sampling outperformed IBP well. IBP converged 5 seconds. 2-, 3-,
4-cutset improved IBP within 30 seconds, 5-cutset 50 seconds.
Random networks. Results 10 instances random multi-partite 10 instances 2-layer networks shown Figure 20. see, w-cutset sampling
substantially improves Gibbs sampling IBP reaching optimal performance
w = 2 3 classes networks. range, performance similar
loop-cutset sampling. case 2-layer networks, accuracy Gibbs sampling
36

fiCutset Sampling Bayesian Networks

random, R=50, N=200, P=3, |LC|=30, w*=22

random, R=50, N=200, P=3, |LC|=30, w*=22

1.5E-04
1.0E-04

IBP
cutset,t=30sec
cutset,t=60sec

1.5E-04

1.0E-04

5.0E-05

5.0E-05
0.0E+00

G

2layer, R=50, N=200, P=3, |LC|=17, w*=16

1.0E-04

w=
7

IBP

1.0E-01

cutset,t=10sec
cutset,t=20sec
1.0E-02

MSE

MSE

1.0E-03

w=
6

2layer, R=50, N=200, P=3, |LC|=17, w*=16

Gibbs
IBP
|LC|=17,w*=3
|C|=22,w*=2
|C|=15,w*=3
|C|=13,w*=4
|C|=12,w*=5

1.0E-02

w=
5

Time (sec)

w
=4

50

=3

40

,w

30

LC

20

ib
bs

10

w=
3

0.0E+00
0

w=
2

MSE

2.0E-04

2.0E-04

MSE

Gibbs
IBP
|LC|=30,w*=3
|C|=61,w*=2
|C|=26,w*=3
|C|=25,w*=4
|C|=24,w*=5

2.5E-04

1.0E-03

1.0E-04

1.0E-05
0

5

10

15

20

25

Time (sec)

1.0E-05
Gibbs

w =2

LC,w =3

w =3

w =4

w =5

w =6

Figure 20: Random multi-partite networks (top) 2-layer networks (bottom), 200 nodes,
10 instances. MSE function number samples (left) w (right).

37

fiBidyuk & Dechter

IBP order-of-magnitude less compared cutset sampling (Figure 20, bottom right).
poor convergence accuracy IBP 2-layer networks observed previously
(Murphy et al., 1999).
grid, 15x30, |E|=60, |LC|=169, w*=15, MSE
2.5E-04

1.5E-04

IBP
|LC|=169,w*=2

2.0E-04

1.2E-04

|C|=163,w*=2

100

120

cutset,t=100sec

Time (sec)

w*
=8

80

w*
=7

60

w*
=6

40

ibb


20

cutset,t=50sec

0.0E+00
G

0

IBP

3.0E-05

w*
=5

5.0E-05

6.0E-05

w*
=4

|C|=75,w*=5

w*
=3

|C|=95,w*=4

1.0E-04

9.0E-05

w*
=2
LC
,w
*=
2

|C|=119,w*=3

1.5E-04

MSE

MSE

grid, 15x30, |E|=40, |LC|=169, w*=20

Gibbs

Figure 21: Random networks, 450 nodes, 10 instances. MSE function number
samples (left) w (right).

Grid networks. Grid networks 450 nodes (15x30) class benchmarks full Gibbs sampling able produce estimates comparable cutsetsampling (Figure 21). respect accuracy, Gibbs sampler, loop-cutset sampling,
3-cutset sampling best performers achieved similar results. Loop-cutset
sampling fastest accurate among cutset sampling schemes. Still, generated samples 4 times slowly compared Gibbs sampling (Table 2) since
loop-cutset relatively large. accuracy loop-cutset sampling closely followed
2-, 3- 4-cutset sampling slowly degrading w increased. Grid networks
example benchmarks regular graph structure (that cutset sampling cannot exploit
advantage) small CPTs (in two-dimensional grid network node
2 parents) Gibbs sampling strong.
coding 50x50, N=200, P=3, |LC|=26, w*=19

coding, 50x50, N=200, P=3, |LC|=26, w*=19

IBP
2.5E-04

cutset,t=5sec

|C|=38,w*=2

2.5E-04

cutset,t=10sec
2.0E-04

|C|=21,w*=3

2.0E-04

|C|=18,w*=4

MSE

MSE

IBP

|LC|=26,w*=3

3.0E-04

1.5E-04
1.0E-04

1.5E-04
1.0E-04

5.0E-05

5.0E-05
0.0E+00
0

2

4

6

8

10

0.0E+00
w=2

Time (sec)

w=3

LC,w=3

w=4

Figure 22: Coding networks, 50 code bits, 50 parity check bits, =0.4, 100 instances, time
bound=6 minutes.

38

fiCutset Sampling Bayesian Networks

cpcs54
cpcs179
cpcs360b
cpcs422b
grid15x30
random
2layer
coding

Time
20 sec
40 sec
100 sec
200 sec
100 sec
50 sec
20 sec
20 sec

Gibbs
4500
1500
2000
3000
2000
2000
200
650

Markov Chain Length
LC w=2 w=3 w=4
2200 4000 2400
800
400
400
150
40
3000 3000 2000
800
20 2000 1500
900
500
300
260
150
1000 1400
700
450
700
900
320
150
450
800
600
250


w=5
500
10
500
500
105
300
75
150

w=6
200
250
60
140
40
100

Table 3: Individual Markov chain length function w. length chain
adjusted sampling scheme benchmark total processing
time across sampling algorithms same.

Coding Networks. cutset sampling results coding networks shown
Figure 22. Here, induced width varied 18 22 allowing exact inference.
However, additionally tested observed complexity network grows
exponentially number coding bits (even small increase number
coding bits 60 yielding total 240 nodes corresponding adjustments number
parity-checking bits transmitted code size, induced width exceeds 24)
time sample generation scales linearly. collected results 10 networks
(10 different parity check matrices) 10 different evidence instantiations (total 100
instances). decoding, Bit Error Rate (BER) standard error measure. However,
computed MSE unobserved nodes evaluate quality approximate results
precisely. expected, Gibbs sampling converge (because Markov chain
non-ergodic) left charts. charts Figure 22 show loop-cutset
optimal choice coding networks whose performance closely followed 2-cutset
sampling. saw earlier, cutset sampling outperforms IBP.
4.5 Computing Error Bound
Second issue convergence sampling scheme always problem predicting
quality estimates deciding stop sampling. section, compare
empirically error intervals Gibbs cutset sampling estimates.
Gibbs sampling cutset sampling guaranteed converge correct posterior
distribution ergodic networks. However, hard estimate many samples
needed achieve certain degree convergence. possible derive bounds
absolute error based sample variance sampling method samples independent. Gibbs MCMC methods, samples dependent cannot apply
confidence interval estimate directly. case Gibbs sampling, apply batch
means method special case standardized time series method used
BUGS software package (Billingsley, 1968; Geyer, 1992; Steiger & Wilson, 2001).
39

fiBidyuk & Dechter

main idea split Markov chain length chains length
. Let Pm (xi |e) estimate derived single chain [1, ..., ] length
(meaning, containing samples) defined equations (28)-(29). estimates Pm (x|e)
assumed approximately independent large enough . Assuming convergence
conditions satisfied central limit theorem holds, Pm (x|e) distributed
according N (E[P (xi |e)], 2 ) posterior marginal P (Xi |e) obtained
average results obtained chain, namely:
P (x|e) =


1 X
Pm (x|e)


(33)

m=1

sampling variance computed usually:
2 =


X
1
(Pm (x|e) P (x|e))2
1
m=1

equivalent expression sampling variance is:
PM
P 2 (x|e) P 2 (x|e)
2
= m=1
1

(34)

2 easy compute incrementally storing running sums Pm (x|e)
2 (x|e). Therefore, compute confidence interval 100(1 ) percentile
Pm
used random variables normal distribution small sampling set sizes. Namely:
"
r #
2
=1
(35)
P P (x|e) [P (x|e) 2 ,(M 1)

2 ,(M 1) table value distribution (M 1) degrees freedom.
used batch means approach estimate confidence interval posterior
marginals one modification. Since working relatively small sample sets
(a thousand samples) notion large enough well defined,
restarted chain every samples guarantee estimates Pm (x|e)
truly independent. method batch means provides meaningful error estimates
assuming samples drawn stationary distribution. assume
problems chains mix fast enough samples drawn target
distribution.
applied approach estimate error bound Gibbs sampler
cutset sampler. computed 90% confidence interval estimated posterior
marginal P (xi |e) based sampling variance Pm (xi |e) 20 Markov chains
described above. computed sampling variance 2 Eq.(34) 90% confidence
interval 0.9 (xi ) Eq.(35) averaged nodes:
X X
1
0.9 = P
0.9 (xi )
N |D(Xi )|


xi D(Xi )

estimated confidence interval large practical. Thus, compared 0.9
empirical average absolute error :
40

fiCutset Sampling Bayesian Networks

cpcs54
cpcs179
cpcs360b
cpcs422b
random
2layer
coding
grid15x30


0.9

0.9

0.9

0.9

0.9

0.9

0.9

0.9

Average Error
LC
w=2
0.00036 0.00030
0.00076 0.00064
0.00086 0.00074
0.00148 0.00111
0.00011 0.00010
0.00022 0.00023
- 0.00018
- 0.00033
0.00039 0.00119
0.00080 0.00247
0.00066 0.00063
0.00145 0.00144
0.00014 0.00019
0.00030 0.00035
0.00099 0.00119
0.00214 0.00247

Gibbs
0.00056
0.00119
0.01577
0.02138
0.00051
0.00113
0.00055
0.00119
0.00091
0.00199
0.00436
0.00944
0.00108
0.00248

Confidence Interval
w=3
w=4
w=5
0.00030
0.00040 0.00036
0.00063
0.00098 0.00112
0.00066
0.00113 0.00178
0.00164
0.00235 0.00392
0.00008
0.00014 0.00012
0.00021
0.00030 0.00028
0.00020
0.00018 0.00027
0.00035
0.00043 0.00060
0.00091
0.00099 0.00109
0.00205
0.00225 0.00222
0.00082
0.00117 0.00134
0.00185
0.00235 0.00302
0.00019 0.000174
0.00034 0.000356
0.00091
0.00099 0.00109
0.00205
0.00225 0.00222

w=6
0.00067
0.00116
0.00022
0.00046
0.00037
0.00074
0.00113
0.00239
0.00197
0.00341
0.00113
0.00239

Table 4: Average absolute error (measured) estimated confidence interval 0.9
function w 20 Markov Chains.

=

N

X
1
|D(Xi )|

P



X

xi D(Xi )

|P (xi |e) P (xi |e))

objective study observe whether computed confidence interval 0.9
(estimated absolute error) accurately reflects true absolute error , namely, verify
< 0.9 , so, investigate empirically whether confidence interval cutsetsampling estimates smaller compared Gibbs sampling would expect due
variance reduction.
Table 4 presents average confidence interval average absolute error
benchmarks. benchmark, first row results (row ) reports average
absolute error second row results (row 0.9 ) reports 90% confidence interval.
column Table 4 corresponds sampling scheme. first column reports results
Gibbs sampling. second column reports results loop-cutset sampling.
remaining columns report results w-cutset sampling w range 26. loop-cutset
sampling results cpcs422b included due statistically insignificant number
samples generated loop-cutset sampling. Gibbs sampling results coding networks
left network ergodic (as mentioned earlier) Gibbs sampling
converge.
see networks < 0.9 validates method measuring
confidence interval. cases estimated confidence interval 0.9
2-3 times size average error relatively small. case cutset sampling,
largest confidence interval max 0.9 = 0.00247 reported grid networks loop-cutset
41

fiBidyuk & Dechter

sampling. Thus, confidence interval estimate could used criteria reflecting
quality posterior marginal estimate sampling algorithm practice. Subsequently, comparing results Gibbs sampling cutset sampling, observe
significant reduction average absolute error, similar reduction
estimated confidence interval. Across benchmarks, estimated confidence interval
Gibbs sampler remains 0.9 > 1E-3. time, cutset sampling obtain
0.9 < 1E-3 5 8 classes networks (excluded cpcs179, grid, 2-layer
networks).
4.6 Discussion
empirical evaluation performance cutset sampling demonstrates that, except
grid networks, sampling cutset usually outperforms Gibbs sampling. show
convergence cutset sampling terms number samples dramatically improves
predicted theoretically.
experiments clearly show exists range w-values w-cutset
sampling outperforms Gibbs sampler. performance w-cutset sampling deteriorates
increase w yields small reduction cutset size. example cpcs360b
network starting w=4, increasing w 1 results reducing sampling
set 1 node (shown Table 1).
observe loop-cutset good choice cutset sampling long
induced width network wLC conditioned loop-cutset reasonably small. wLC
large (as cpcs422b), loop-cutset sampling computationally less efficient w-cutset
sampling w < wLC .
showed Section 4.3 Gibbs sampling loop-cutset sampling
outperform state-of-the-art AIS-BN adaptive importance sampling method
probability evidence small. Consequently, w-cutset sampling schemes Section 4.4 outperformed Gibbs sampler cpcs360b cpcs422b would outperfrom
AIS-BN.

5. Related Work
mention related work. idea marginalising variables improve
efficiency Gibbs sampling first proposed Liu et al. (1994). successfully
applied several special classes Bayesian models. Kong et al. (1994) applied collapsing
bivariate Gaussian problem missing data. Liu (1994) defined collapsed Gibbs
sampling algorithm finding repetitive motifs biological sequences applies integrating
two parameters model. Similarly, Gibbs sampling set collapsed Escobar
(1994), MacEachern (1994), Liu (1996) learning nonparametric Bayes problem.
instances above, special relationships problem variables
exploited integrate several variables resulting collapsed Gibbs sampling approach.
Compared previous research work, contribution defining generic scheme
collapsing Gibbs sampling Bayesian networks takes advantage networks
graph properties depend specific form relationships
variables.
42

fiCutset Sampling Bayesian Networks

Jensen et al. (1995) combined sampling exact inference blocking Gibbs sampling
scheme. Groups variables sampled simultaneously using exact inference compute
needed conditional distributions. empirical results demonstrate significant improvement convergence Gibbs sampler time. Yet, proposed blocking
Gibbs sampling, sample contains variables network. contrast, cutset sampling reduces set variables sampled. noted previously, collapsing produces
lower variance estimates blocking and, therefore, cutset sampling require fewer
samples converge.
different combination sampling exact inference join-trees described
Koller et al. (1998) Kjaerulff (1995). oller et al. Kjaerulff proposed sample
probability distribution cluster computing outgoing messages. Kjaerulff
used Gibbs sampling large clusters estimate joint probability distribution
P (Vi ), Vi X cluster i. estimated P (Vi ) recorded instead true joint
distribution conserve memory. motivation high-probability tuples
recorded remaining low-probability tuples assumed probability 0.
small clusters, exact joint distribution P (Vi ) computed recorded. However,
paper analyze introduced errors compare performance scheme
standard Gibbs sampler exact algorithm. analysis error given
comparison approaches.
Koller et al. (1998) used sampling used compute messages sent cluster
cluster j posterior joint distributions cluster-tree contains discrete
continuous variables. approach subsumes cluster-based sampling proposed
Kjaerulff (1995) includes rigorous analysis error estimated posterior
distributions. method difficulties propagation evidence. empirical
evaluation limited two hybrid network instances compares quality
estimates likelihood weighting, instance importance sampling
perform well presence low-probability evidence.
effectiveness collapsing sampling set demonstrated previously
context Particle Filtering method Dynamic Bayesian networks (Doucet, Andrieu, &
Godsill, 2000a; Doucet, deFreitas, & Gordon, 2001; Doucet, de Freitas, Murphy, & Russell,
2000b). shown sampling subspace combined exact inference (RaoBlackwellised Particle Filtering) yields better approximation Particle Filtering
full set variables. However, objective study limited observation
effect special cases variables integrated easily.
cutset sampling scheme offers generic approach collapsing Gibbs sampler
Bayesian network.

6. Conclusion
paper presents w-cutset sampling scheme, general scheme collapsing Gibbs
sampler Bayesian networks. showed theoretically empirically cutset sampling improves convergence rate allows sampling non-ergodic network
ergodic subspace. collapsing sampling set, reduce dependence
samples marginalising highly correlated variables smoothing
sampling distributions remaining variables. estimators obtained sampling
43

fiBidyuk & Dechter

lower-dimensional space lower sampling variance. Using induced
width w controlling parameter, w-cutset sampling provides mechanism balancing
sampling exact inference.
studied power cutset sampling sampling set loop-cutset and,
generally, sampling set w-cutset network (defined subset
variables that, instantiated, induced width network w). Based
Rao-Blackwell theorem, cutset sampling requires fewer samples regular sampling
convergence. experiments showed reduction number samples
time-wise cost-effective. confirmed range randomly generated real
benchmarks. demonstrated cutset sampling superior state art
AIS-BN importance sampling algorithm probability evidence small.
Since size cutset correlations variables two main
factors contributing speed convergence, w-cutset sampling may optimized advancement methods finding minimal w-cutset. Another promising
direction future research incorporate heuristics avoiding selecting stronglycorrelated variables cutset since correlations driving factors speed
convergence Gibbs sampling. Alternatively, could combine sample collapsing
blocking.
summary, w-cutset sampling scheme simple yet powerful extension sampling
Bayesian networks likely dominate regular sampling sampling method.
focused Gibbs sampling better convergence characteristics, sampling
schemes implemented cutset sampling principle. particular,
adapted use likelihood weighting (Bidyuk & Dechter, 2006).

References
Abdelbar, A. M., & Hedetniemi, S. M. (1998). Approximating maps belief networks
NP-hard theorems. Artificial Intelligence, 102, 2138.
Andrieu, C., de Freitas, N., & Doucet, A. (2002). Rao-Blackwellised particle filtering via
data augmentation. Advances Neural Information Processing Systems. MIT
Press.
Arnborg, S. A. (1985). Efficient algorithms combinatorial problems graphs
bounded decomposability - survey. BIT, 25, 223.
Becker, A., Bar-Yehuda, R., & Geiger, D. (2000). Random algorithms loop cutset
problem. Journal Artificial Intelligence Research, 12, 219234.
Bertele, U., & Brioschi, F. (1972). Nonserial Dynamic Programming. Academic Press.
Bidyuk, B., & Dechter, R. (2003). Empirical study w-cutset sampling Bayesian networks. Proceedings 19th Conference Uncertainty Artificial Intelligence
(UAI), pp. 3746. Morgan Kaufmann.
Bidyuk, B., & Dechter, R. (2004). finding minimal w-cutset problem. Proceedings
20th Conference Uncertainty Artificial Intelligence (UAI), pp. 4350.
Morgan Kaufmann.
44

fiCutset Sampling Bayesian Networks

Bidyuk, B., & Dechter, R. (2006). Cutset Sampling Likelihood Weighting. Proceedings 22nd Conference Uncertainty Artificial Intelligence (UAI), pp.
3946. Morgan Kaufmann.
Billingsley, P. (1968). Convergence Probability Measures. John Wiley & Sons, New York.
Casella, G., & Robert, C. P. (1996). Rao-Blackwellisation sampling schemes. Biometrika,
83 (1), 8194.
Cheng, J., & Druzdzel, M. J. (2000). AIS-BN: adaptive importance sampling algorithm
evidenctial reasoning large baysian networks. Journal Aritificial Intelligence
Research, 13, 155188.
Cooper, G. (1990). computational complexity probabilistic inferences. Artificial
Intelligence, 42, 393405.
Dagum, P., & Luby, M. (1993). Approximating probabilistic inference Bayesian belief
networks NP-hard. Artificial Intelligence, 60 (1), 141153.
Dechter, R. (1999a). Bucket elimination: unifying framework reasoning. Artificial
Intelligence, 113, 4185.
Dechter, R. (1999b). Bucket elimination: unifying framework reasoning. Artificial
Intelligence, 113 (12), 4185.
Dechter, R. (2003). Constraint Processing. Morgan Kaufmann.
Doucet, A., & Andrieu, C. (2001). Iterative algorithms state estimation jump Markov
linear systems. IEEE Trans. Signal Processing, 49 (6), 12161227.
Doucet, A., Andrieu, C., & Godsill, S. (2000a). sequential Monte Carlo sampling methods Bayesian filtering. Statistics Computing, 10 (3), 197208.
Doucet, A., de Freitas, N., Murphy, K., & Russell, S. (2000b). Rao-Blackwellised particle
filtering dynamic Bayesian networks. Proceedings 16th Conference
Uncertainty Artificial Intelligence (UAI), pp. 176183.
Doucet, A., deFreitas, N., & Gordon, N. (2001). Sequential Monte Carlo Methods Practice.
Springer-Verlag, New York, Inc.
Doucet, A., Gordon, N., & Krishnamurthy, V. (1999). Particle filters state estimation jump markov linear systems. Tech. rep., Cambridge University Engineering
Department.
Escobar, M. D. (1994). Estimating normal means iwth dirichlet process prior. Journal
American Statistical Aasociation, 89, 268277.
Frey, B. J., & MacKay, D. J. C. (1997). revolution: Belief propagation graphs
cycles. Neural Information Processing Systems, Vol. 10.
Fung, R., & Chang, K.-C. (1989). Weighing integrating evidence stochastic simulation Bayesian networks. Proceedings 5th Conference Uncertainty
Artificial Intelligence (UAI), pp. 209219. Morgan Kaufmann.
Geiger, D., & Fishelson, M. (2003). Optimizing exact genetic linkage computations. Proceedings 7th Annual International Conf. Computational Molecular Biology,
pp. 114121. Morgan Kaufmann.
45

fiBidyuk & Dechter

Gelfand, A., & Smith, A. (1990). Sampling-based approaches calculating marginal densities. Journal American Statistical Association, 85, 398409.
Geman, S., & Geman, D. (1984). Stochastic relaxations, Gibbs distributions
Bayesian restoration images. IEEE Transaction Pattern analysis Machine
Intelligence, 6, 721742.
Geyer, C. J. (1992). Practical Markov Chain Monte Carlo. Statistical Science, 7, 473483.
Gilks, W., Richardson, S., & Spiegelhalter, D. (1996). Markov chain Monte Carlo practice.
Chapman Hall.
Gottlob, G., Leone, N., & Scarello, F. (1999). comparison structural CSP decomposition
methods. Proceedings 16th International Joint Conference Artificial
Intelligence (IJCAI), pp. 394399. Morgan Kaufmann.
Jensen, C., Kong, A., & Kjrulff, U. (1995). Blocking Gibbs sampling large probabilistic expert systems. Int. J. Human Computer Studies. Special Issue RealWorld Applications Uncertain Reasoning, 42 (6), 647666.
Jensen, F. V., Lauritzen, S. L., & Olesen, K. G. (1990). Bayesian updating causal
probabilistic networks local computation. Computational Statistics Quarterly, 4,
269282.
Jones, G., & Hobert, J. P. (2001). Honest exploration intractable probability distributions
via Markov Chain Monte Carlo. Statist. Sci., 16 (4), 312334.
Kask, K., Dechter, R., Larrosa, J., & Dechter, A. (2005). Unifying cluster-tree decompositions reasoning graphical models. Artificial Intelligence, 166, 165193.
Kjrulff, U. (1995). HUGS: Combining exact inference Gibbs sampling junction
trees. Proceedings 11th Conference Uncertainty Artificial Intelligence
(UAI), pp. 368375. Morgan Kaufmann.
Koller, D., Lerner, U., & Angelov, D. (1998). general algorithm approximate inference
application hybrid Bayes nets. Proceedings 14th Conference
Uncertainty Artificial Intelligence (UAI), pp. 324333.
Kong, A., Liu, J. S., & Wong, W. (1994). Sequential imputations Bayesian missing
data problems. J. American Statistical Association, 89 (425), 278288.
Kschischang, F. R., & Frey, B. J. (1998). Iterative decoding compound codes probability propagation graphical models. IEEE Journal Selected Areas Communications, 16, 219230.
Larrosa, J., & Dechter, R. (2003). Boosting search variable elimination constraint
optimization constraint satisfaction problems. Constraints, 8 (3), 303326.
Lauritzen, S., & Spiegelhalter, D. (1988). Local computation probabilities graphical
structures application expert systems. Journal Royal Statistical
Society, Series B, 50(2), 157224.
Liu, J. (1991). Correlation Structure Convergence Rate Gibbs Sampler, Ph.D.
Thesis. University Chicago.
46

fiCutset Sampling Bayesian Networks

Liu, J. (1994). collapsed Gibbs sampler Bayesian computations applications
gene regulation problem. Journal American Statistical Association, 89 (427),
958966.
Liu, J., Wong, W., & Kong, A. (1994). Covariance structure Gibbs sampler
applications comparison estimators augmentation schemes. Biometrika,
81 (1), 2740.
Liu, J. S. (1996). Nonparametric hierarchical bayes via sequential imputations. Annals
Statistics, 24 (3), 911930.
Liu, J. S. (2001). Monte Carlo Strategies Scientific Computing. Springer-Verlag, New
York, Inc.
MacEachern, S., Clyde, M., & Liu, J. (1998). Sequential importance sampling nonparametric bayes models: next generation. Canadian Journal Statistics, 27,
251267.
MacEachern, S. N. (1994). Estimating normal means conjugate style dirichlet process
prior. Communications Statistics-Simulation Computation, 23 (3), 727741.
MacKay, D. (1996). Introduction Monte Carlo methods. Proceedings NATO Advanced Study Institute Learning Graphical Models. Sept 27-Oct 7, pp. 175204.
Maier, D. (1983). theory relational databases. Computer Science Press, Rockville,
MD.
McEliece, R., MacKay, D., & Cheng, J.-F. (1997). Turbo decoding instance Pearls
belief propagation algorithm. IEEE J. Selected Areas Communication, 16, 140152.
Miller, R., Masarie, F., & Myers, J. (1986). Quick medical reference (QMR) diagnostic
assistance. Medical Computing, 3 (5), 3438.
Miller, R., Pople, H., & Myers, J. (1982). Internist-1: experimental computerbased
diagnostic consultant general internal medicine. New English Journal Medicine,
307 (8), 468476.
Murphy, K. P., Weiss, Y., & Jordan, M. I. (1999). Loopy belief propagation approximate
inference: empirical study. Proceedings 15th Conference Uncertainty
Artificial Intelligence (UAI), pp. 467475. Morgan Kaufmann.
Parker, R., & Miller, R. (1987). Using causal knowledge create simulated patient cases:
CPCS project extension INTERNIST-1. Proceedings 11th Symp.
Comp. Appl. Medical Care, pp. 473480.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems. Morgan Kaufmann.
Peot, M. A., & Shachter, R. D. (1992). Fusion propagation multiple observations
belief networks. Artificial Intelligence, 48, 299318.
Pradhan, M., Provan, G., Middleton, B., & Henrion, M. (1994). Knowledge engineering
large belief networks. Proceedings 10th Conference Uncertainty Artificial
Intelligence, Seattle, WA, pp. 484490.
Rish, I., Kask, K., & Dechter, R. (1998). Empirical evaluation approximation algorithms
probabilistic decoding. Proceedings 14th Conference Uncertainty
Artificial Intelligence (UAI), pp. 455463. Morgan Kaufmann.
47

fiBidyuk & Dechter

Roberts, G. O., & Sahu, S. K. (1997). Updating schemes; correlation structure; blocking
parameterization Gibbs sampler. Journal Royal Statistical Society,
Series B, 59 (2), 291317.
Roberts, G. O., & Tweedie, R. L. (1999). Bounds regeneration times convergence
rates Markov chains. Stochastic Processes Applications, 80, 211229.
Roberts, G. O., & Tweedie, R. L. (2001). Corregendum bounds regeneration times
convergence rates Markov chains. Stochastic Processes Applications, 91,
337338.
Rosenthal, J. S. (1995). Convergence rates Markov Chains. SIAM Review, 37 (3), 387
405.
Rosti, A.-V., & Gales, M. (2004). Rao-Blackwellised Gibbs sampling switching linear
dynamical systems. IEEE International Conference Acoustics, Speech,
Signal Processing (ICASSP 2004), pp. 809812.
Schervish, M., & Carlin, B. (1992). convergence successive substitution sampling.
Journal Computational Graphical Statistics, 1, 111127.
Shachter, R. D., Andersen, S. K., & Solovitz, P. (1994). Global conditioning probabilistic
inference belief networks. Proceedings 10th Conference Uncertainty
Artificial Intelligence (UAI), pp. 514522.
Shachter, R. D., & Peot, M. A. (1989). Simulation approaches general probabilistic
inference belief networks. Proceedings 5th Conference Uncertainty
Artificial Intelligence (UAI), pp. 221231.
Steiger, N. M., & Wilson, J. R. (2001). Convergence properties batch means method
simulation output analysis. INFORMS Journal Computing, 13 (4), 277293.
Tierney, L. (1994). Markov chains exploring posterior distributions. Annals Statistics,
22 (4), 17011728.
Yuan, C., & Druzdzel, M. (2003). importance sampling algorithm based evidence
pre-propagation. Proceedings 19th Conference Uncertainty Artificial
Intelligence (UAI), pp. 624631.
Zhang, N., & Poole, D. (1994). simple algorithm Bayesian network computations.
Proceedings 10th Canadian Conference Artificial Intelligence, pp. 171178.

48


