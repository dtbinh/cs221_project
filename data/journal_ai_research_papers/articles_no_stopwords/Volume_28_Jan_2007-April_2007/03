Journal Artificial Intelligence Research 28 (2007) 453-515

Submitted 08/06; published 4/07

Abstract Reasoning Planning Coordination
Bradley J. Clement

BRAD . CLEMENT @ JPL . NASA . GOV

Jet Propulsion Laboratory, Mail Stop: 126-347,
Pasadena, CA 91109 USA

Edmund H. Durfee

DURFEE @ UMICH . EDU

University Michigan, EECS Department,Ann Arbor, MI 48109 USA

Anthony C. Barrett

TONY. BARRETT @ JPL . NASA . GOV

Jet Propulsion Laboratory, Mail Stop: 126-347,
Pasadena, CA 91109 USA

Abstract
judicious use abstraction help planning agents identify key interactions
actions, resolve them, without getting bogged details. However, ignoring wrong
details lead agents building plans work, costly backtracking replanning overlooked interdependencies come light. claim associating systematicallygenerated summary information plans abstract operators ensure plan correctness, even
asynchronously-executed plans must coordinated across multiple agents, still achieving valuable efficiency gains. paper, formally characterize hierarchical plans whose
actions temporal extent, describe principled method deriving summarized state
metric resource information actions. provide sound complete algorithms, along
heuristics, exploit summary information hierarchical refinement planning plan
coordination. analyses experiments show that, clearcut reasonable conditions,
using summary information speed planning much doubly exponentially even plans
involving interacting subproblems.

1. Introduction
Abstraction powerful tool solving large-scale planning scheduling problems. abstracting away less critical details looking large problem, agent find overall solution problem easily. Then, skeleton overall solution place, agent
work additional details solution (Sacerdoti, 1974; Tsuneto, Hendler, & Nau, 1998).
Further, interdependencies fully resolved abstract levels, one agents
flesh sub-pieces abstract solution full details independently (even parallel)
divide-and-conquer approach (Korf, 1987; Lansky, 1990; Knoblock, 1991).
Unfortunately, always obvious best abstract large, complex problems achieve
efficiency improvements. agent solving complicated, many-step planning problem,
example, might able identify details earlier parts critical later
ones tried generate plans schedules seen interdependencies end
arising. Even worse, multiple agents trying plan schedule activities shared
environment, unless lot prior knowledge other, extremely
difficult one agent anticipate aspects planned activities likely affect,
affected by, agents.

c
2007
AI Access Foundation. rights reserved.

fiC LEMENT, URFEE , & BARRETT

paper, describe strategy balances benefits risks abstraction largescale single-agent multi-agent planning problems. approach avoids danger ignoring
important details lead incorrect plans (whose execution fail due overlooked interdependencies) substantial backtracking abstract decisions cannot consistently refined.
Meanwhile, approach still achieves many computational benefits abstraction long
one number reasonable conditions (listed later) holds.
key idea behind strategy annotate abstract operator plan hierarchy
summary information potential needs effects potential refinements. might sound contrary purpose abstraction reducing number
details, fact show strikes good balance. Specifically, possibly
relevant conditions effects modeled, agent agents reasoning abstract
operators absolutely sure important details cannot overlooked. However,
summary information abstracts away details refinement choices conditions
effects manifested, information relative timing conditions needed effects achieved, still often results exponential reduction information
compared flat representation.
Based concept summary information, paper extends prior work summarized
Section 8 make following contributions:
formal model hierarchical plans temporal extent, execution.
many planning systems sophisticated temporal models (e.g., Laborie & Ghallab, 1995; Muscettola, 1994) additionally use hierarchical representations alternative courses action
(Allen, Kautz, Pelavin, & Tenenberg, 1991; Currie & Tate, 1991; Chien, Knight, Stechert, Sherwood, & Rabideau, 2000a; Castillo, Fdez-Olivares, Garca-Perez, & Palao, 2006), know
work extends hierarchical task network (HTN) formalization (Erol, Hendler, & Nau,
1994a; Erol, Nau, & Hendler, 1994b) include temporal extent. need formalism order
clarify semantics summary information concurrently executing agents.
Algorithms deriving summary information propositional metric resource conditions effects, using information determine potential definite interactions abstract tasks. prove summarization techniques guaranteed
correctly capture conditions effects associated abstract operator appropriately, augmented modal information whether conditions must may hold whether
hold entire operation time. summary information
captures conditions effects, algorithms reason operators different levels
abstraction predict often resolve operator interactions without fully detailing task hierarchies,
even operators executing asynchronously different agents.
Sound complete algorithms hierarchical refinement planning centralized plan coordination actions temporal extent, supporting flexible plan execution systems.
agent reduce backtracking planning selectively interleaving refinement plan
predicting resolving potential interdependencies evolving plan plans
asynchronously executed agents. research found benefit
guiding refinement conditions specified higher levels plan hierarchy guide refinement (Sacerdoti, 1974; Young, Pollack, & Moore, 1994; Tsuneto et al., 1998). show
algorithms improve capabilities exploiting hierarchical structure using summary

454

fiA BSTRACT R EASONING P LANNING C OORDINATION

information efficiently converge coordinated plans, refined
individually parallel participating agents.
ability coordinate abstract levels rather detailed plans allows
agents retain local flexibility refine operators best suits current expected
circumstances without jeopardizing coordination triggering new rounds renegotiation.
way, summary information supports robust execution systems PRS (Georgeff & Lansky,
1986), UMPRS (Lee, Huber, Durfee, & Kenny, 1994), RAPS (Firby, 1989), JAM (Huber, 1999), etc.
interleave refinement abstract plan operators execution.
approach extends plan coordination (plan merging) techniques (Georgeff, 1983; Lansky, 1990; Ephrati & Rosenschein, 1994) utilizing plan hierarchies expressive temporal model. Prior techniques assume actions atomic, meaning action either executes
before, after, exactly time another. contrast, use interval point algebra (Vilain & Kautz, 1986) represent possibility several actions one agent executing
execution one action another agent. algorithms choose alternative
refinements HTN dynamically midst plan coordination, support interleaved local
planning, multiagent coordination, concurrent execution.
Search techniques heuristics, including choose-fewest-threats-first (CFTF) expandmost-threats-first (EMTF), take advantage summary information prune search
space. interdependencies run deeply agents plans, resolving abstract levels, possible all, lead unacceptable losses parallel activity. Fortunately, even
agents need delve details plans tease interdependencies, summary information still enable exponential speedups guiding decomposition pruning refinement
choices. search efficiency using summary information comes ignoring irrelevant information, distributed planning system reduces communication overhead exponentially.
Complexity analyses experiments showing potential doubly-exponential speedups refinement local search planning/scheduling using summary information. algorithms
demonstrate exploiting summary information guide hierarchical planning scheduling
achieve exponential speedups, resolving interdependencies abstract levels improve
performance plan coordination algorithms doubly exponentially. others shown
abstraction exponentially reduce search space size (Korf, 1987; Knoblock, 1991) subproblem independence properties hold, show techniques lead exponential improvements
broader conditions hold problem:
solutions found abstract levels;
amount summary information less higher levels lower levels;
choices decompositions lead varying numbers plan threats.
none conditions hold, show generating using summary information
provides benefit increase computation communication overhead. Thus, care must
taken deciding use summary information, though proven extremely worthwhile
types problem domains examined, example next describe.

455

fiC LEMENT, URFEE , & BARRETT

M1

M2

E

transport1

transport2



B

C

tool

bin1

bin2

bin3

bin4

dock

Figure 1: simple example manufacturing domain
produce H
produce G

produce H G

produce G
M2

produce G
M1

move A&B
M2
move M2

move G
M2
build H

move H
bin1

build G

move B M2

Figure 2: production managers hierarchical plan
1.1 Manufacturing Example
running example motivate work, consider manufacturing plant production
manager, facilities manager, inventory manager goals separately
constructed hierarchical plans achieve them. However, still need coordinate use
equipment, availability parts used manufacturing parts, storage parts,
use transports moving parts around. state factory shown Figure 1.
domain, agents produce parts using machines M1 M2, service machines tool,
move parts shipping dock storage bins shop floor using transports.
Initially, machines M1 M2 free use, transports (transport1 transport2),
tool, parts (A E) shown storage locations available.
production manager responsible creating part H using machines M1 M2. Either M1 M2 consume parts B produce G, M2 produce H G.
production managers hierarchical plan manufacturing H involves using transports move
needed parts storage input trays machines, manufacturing G H, transporting H back storage. plan shown Figure 2. Arcs subplan branches mean
subplans must executed. Branches without arcs denote alternative choices achieving
parents goal. decomposition produce G M1 similar produce G M2.
facilities manager services machine equipping tool calibrating it.
machines unavailable production serviced. facilities managers hierarchical plan branches choices servicing machines different orders uses transports

456

fiA BSTRACT R EASONING P LANNING C OORDINATION

maintenance
service M1 M2

service M1

service M2

service M2 M1

move tool
dock

move tool equip M1 tool calibrate M1
M1

Figure 3: facilities managers hierarchical plan
move_parts
move C dock

move D&E

move bin3

move E bin4

Figure 4: inventory managers hierarchical plan
getting tool storage machines (Figure 3). decomposition service M2M1
similar service M1M2.
parts must available space-limited shop floor order agent use them.
Whenever agent moves uses part, becomes unavailable. inventory managers goal
move part C dock move E bins shop floor (shown Figure 4).
accelerate coordination plans, factory manager analyze hierarchical
plan derive summary information abstract plan operator affect world.
information includes summary pre-, post-, in-conditions intuitively correspond
externally required preconditions, externally effective postconditions, internally required
conditions, respectively, plan based potential refinements. Summary conditions augment state conditions modal information whether conditions must may hold
effect. Examples given end Section 3.2.
summary information computed, production inventory managers could
send information top-level plan facilities manager. facilities manager could
reason top-level summary information plans determine
facilities manager serviced machines production manager started producing
parts, production manager finished inventory manager began moving parts
dock, plans executed (refined) way, CanAnyWay.
facilities manager could instruct others add communication actions plans
synchronize actions appropriately.
top-level solution maximizes robustness choices production facilities managers plans preserved, solution inefficient concurrent
activityonly one manager executing plan time. production manager might
want wait facilities manager finish maintenance could negotiate solution
concurrency. case, facilities manager could determine could overlap

457

fiC LEMENT, URFEE , & BARRETT

plans way without risking conflict (CanAnyWay). However, summary information
could tell might way overlap plans (MightSomeWay), suggesting
search solution concurrency (at cost perhaps committing specific
refinement choices) hope success. case, facilities manager could request production manager summary information produce Hs subplans, reason
interactions lower level actions way, find way synchronize subplans
fine-grained solution plans executed concurrently. give algorithm
finding solutions Section 5.
1.2 Overview
first formally define model concurrent hierarchical plan, execution, interactions
(Section 2). Next, describe summary information propositional states metric resources,
mechanisms determining whether particular interactions must may hold based information, algorithms deriving information (Section 3). Built upon algorithms others
using summary information determine whether set CHiPs must might execute successfully set ordering constraints (Section 4). turn used within sound
complete multilevel planning/coordination algorithm employs search techniques heuristics
efficiently navigate prune search space refinement (Section 5). show
planning, scheduling, coordinating abstract levels exponentially improve performance
search execution (Section 6). provide experimental results demonstrating search
techniques greatly reduce search optimal solutions (Section 7). Finally, Section 8
differentiate approach related work mention elsewhere conclude.

2. Model Hierarchical Plans Concurrent Execution
representation temporal extent HTN important modeling concurrently
executing agents performing abstract reasoning summary information. agent
scheduling abstract actions sequentially order them, severely restricted
kinds solutions find. example, agent may prefer solutions shorter
makespans, seek plans subthreads carried concurrently.
section define concurrent hierarchical plans (CHiPs), state changes time
based executions, concepts success failure executions possible world,
history. later define summary information abstract plan interactions terms
definitions semantics given section, treatment fairly detailed (though
even comprehensive treatment, see Clement, 2002). However, begin summarizing
main concepts notation introduced, give reader basic gist.
2.1 Overview
CHiP (or plan p) mainly differentiated HTN including definition inconditions,
in(p), (sometimes called conditions) affect (or assert condition on) state
start time p (ts (p)) must hold throughout duration p. Preconditions (pre(p)) must
hold start, postconditions (post(p)) asserted finish time p (t f (p)). Metric
resource (res) consumption (usage(p, res)) instantaneous start time and, resource
defined non-consumable, instantaneously restored end. decompositions p (d(p))

458

fiA BSTRACT R EASONING P LANNING C OORDINATION

style and/or tree, either partial ordering (order(p)) choice child tasks
conditions.
execution e p instantiation start time, end time, decomposition. is,
execution nails exactly done when. order reason plan interactions,
quantify possible histories, history corresponds combination possible
executions concurrently-executing CHiPs partial ordering activities
context initial state. run (r(h,t)) specifies state time history h.
Achieve, clobber, undo interactions defined terms executions
plans assert positive literal ` negative literal ` relative ` required another plans
execution history. looking literals achieved, clobbered, undone set
executions history, identify conditions must hold prior executions
history external preconditions must hold executions history
external postconditions.
value metric resource time (r(res, h,t)) calculated subtracting prior
state value usage plans start executing (if non-consumable) adding back usages
end t. execution e p fails condition required asserted time
state r(h,t) t, value resource (r(res, h,t)) used plan
limits execution.
remainder section, give careful, detailed descriptions concepts
above, ground definitions firm semantics; casual reader skim
details desired. important note that, rather starting scratch, formalization
weaves together, necessary augments, appropriate aspects theories, including
Allens temporal plans (1983), Georgeffs theory multiagent plans (1984), Fagin et al.s
theory multiagent reasoning knowledge (1995).
2.2

CH P

concurrent hierarchical plan p tuple hpre, in, post, usage, type, subplans, orderi. pre(p),
in(p), post(p) sets literals (v v propositional variable v) representing
preconditions, inconditions, postconditions defined plan p.1
borrow existing model metric resources (Chien, Rabideu, Knight, Sherwood, Engelhardt, Mutz, Estlin, Smith, Fisher, Barrett, Stebbins, & Tran, 2000b; Laborie & Ghallab, 1995).
plans usage function mapping resource variables amount used. write
usage(p, res) indicate amount p uses resource res sometimes treat usage(p) set
pairs (res, amount). metric resource res tuple hmin value, max value, typei. min
max values integer real values representing bounds capacity amount available. type resource either consumable non-consumable. example, fuel
battery energy consumable resources because, use, depleted amount.
non-consumable resource available use (e.g. vehicles, computers, power).
Domain modelers typically specify state conditions resource usage primitive actions hierarchy. Thus, conditions usage CHiP used derive summary conditions,
describe Section 3.4, algorithms reason action hierarchy.
order reason plan hierarchies and/or trees actions, type plan p, type(p),
1. Functions pre(p) used referential convenience throughout paper. Here, pre pre(p)
same, pre(p) read preconditions p.

459

fiC LEMENT, URFEE , & BARRETT

given value either primitive, and, or. plan non-primitive plan accomplished carrying subplans. plan non-primitive plan accomplished
carrying exactly one subplans. So, subplans set plans, primitive plans
subplans empty set. order(p) defined plan p consistent set
temporal relations (Allen, 1983) pairs subplans. Plans left unordered respect
interpreted potentially execute concurrently.
decomposition CHiP style HTN described Erol et al.
(1994a). plan task network, plan extra construct representing set
methods accomplish goal compound task. network tasks corresponds
subplans plan.
example Figure 2, production managers highest level plan produce H (Figure 2)
tuple
h{}, {}, {}, {}, and, {produce G, produce H f rom G}, {be f ore(0, 1)}i.
f ore(0,1), 0 1 indices subplans decomposition referring produce G
produce H f rom G respectively. conditions defined produce H rely
conditions defined primitive plans refinement. plan moving part
bin1 first input tray M1 using transport1 tuple
h{}, {}, {}, {}, and, {start move, f inish move}, {meets(0, 1)}i.
plan decomposes two half moves help capture important intermediate effects.
parent orders children meets relation bind together single move.
start move plan
h{at(A, bin1), available(A), f ree(transport1), f ull(M1 tray1)},
{at(A, bin1), available(A), f ull(bin1), f ull(M1 tray1), f ree(transport1)},
{at(A, bin1), available(A), f ree(transport1), f ull(bin1), f ull(M1 tray1)},
{}, primitive, {}, {}i.
f inish move plan
h{at(A, bin1), available(A), f ree(transport1), f ull(bin1), f ull(M1 tray1)},
{at(A, bin1), available(A), f ree(transport1), f ull(bin1), f ull(M1 tray1)},
{at(A, bin1), at(A, M1 tray1), available(A), f ree(transport1), f ull(bin1), f ull(M1 tray1)},
{}, primitive, {}, {}i.
split move plan two parts order ensure action executes
concurrently one use transport1, part A, input tray M1. would incorrect
instead specify f ree(transport1) incondition single plan another agent could,
instance, use transport1 time f ree(transport1) incondition would agree
f ree(transport1) incondition move action. However, specification still
insufficient since two pairs (start move, f inish move) actions could start end
time without conflict. get around allowing planner reason
move plan parent plans, effect, hiding transition start finish actions.
So, representing transition f ree f ree without knowing transition
460

fiA BSTRACT R EASONING P LANNING C OORDINATION

take place modeler ensures another move plan tries use transport1 concurrently
one cause conflict.2
postcondition required incondition specify whether incondition changes.
clarifies semantics inconditions conditions hold plan execution
whether caused action necessary conditions successful execution.
2.3 Executions
Informally, execution CHiP recursively defined instance decomposition
ordering subplans executions. Intuitively, executing plan, agent chooses plans
start time refined, determining points time conditions must hold,
witnesses finish time. formalism helps us reason outcomes different ways
execute group plans, describe state transitions, define summary information.
execution e CHiP p tuple hd,ts ,t f i. ts (e) f (e) positive, non-zero real numbers
representing start finish times execution e, ts < f . Thus, instantaneous actions
explicitly represented. d(e) set subplan executions representing decomposition plan p
execution e. Specifically, p plan, contains exactly one execution
subplans; plan, contains one execution one subplans;
empty primitive. addition, subplan executions, e0 d, ts (e0 ) f (e0 ) must
consistent relations specified order(p). Also, first subplan(s) start must start
time p, ts (e0 ) = ts (e), last subplan(s) finish must finish time
p, f (e0 ) = f (e). possible executions plan p set E (p) includes possible
instantiations execution p, meaning possible values tuple hd,ts ,t f i, obeying
rules stated.
example Section 1.1, execution production managers top-level plan
produce H would e E (produce H). e might h{e1 , e2 }, 2.0, 9.0 e1
E (produce G), e2 E (produce H f rom G). means execution produce H
begins time 2.0 ends time 9.0.
convenience, subexecutions execution e, subex(e), defined recursively
set subplan executions es decomposition unioned subexecutions.
2.4 Histories Runs
agent reasoning summary information make planning decisions abstract levels needs
first able reason CHiPs. section complete semantics CHiPs
describing affect state time. agent execute plan many different
ways different contexts, need able quantify possible worlds (or histories)
agents fulfill plans different ways. defining history, define run
transformation state time result history executions. formalization
histories runs follows closely Fagin et al. (1995) describing multiagent execution.
state world, s, truth assignment set propositions, representing aspect
environment. refer state set true propositional variables. history,
2. Using universal quantification (Weld, 1994) single plan could agent, agent 6= productionManager
using(transport1, agent) condition would exclude concurrent access transport. could
simply specified transport1 non-consumable resource maximum capacity one.

461

fiC LEMENT, URFEE , & BARRETT

h, tuple hE, sI i. E set plan executions agents occurring h, sI
initial state h plan begins executing. So, history h hypothetical world begins
sI initial state executions E(h) occur. particular, history
manufacturing domain might initial state shown Figure 1 parts machines
available, transports free. set executions E would contain execution
produce H, maintenance, move parts, subexecutions.
run, r, function mapping history time point states. gives complete description
state world evolves time, time ranges positive real numbers.
Axiom 1
r(h, 0) = sI
Axiom 2
v r(h,t > 0) (v r(h,t )
p, e p E(h), (v in(p) ts (e p ) = ) (v post(p) f (e p ) = t))
(6 p0 , e p0 E(h), (v in(p0 ) ts (e p0 ) = ) (v post(p0 ) f (e p0 ) = t))
Axiom 1 states world initial state time zero. Axiom 2 states predicate
v true time already true beforehand, plan asserts v incondition
postcondition t, (in either case) plan asserts v t. plan starts t, inconditions
asserted right start, +, small positive real number. Axiom 2 indicates
inconditions postconditions effects.
state resource level value (integer real). consumable resource usage, task
depletes resource modeled instantaneously deplete resource (subtract usage
current state) start task full amount. non-consumable resource usage, task
depletes usage amount start task, usage restored (added back
resource state) end execution. task replenish resource negative usage.
refer level resource res time history h r(res, h,t). Axioms 3 4
describe calculations consumable non-consumable resources, respectively.
Axiom 3
r(consumable res, h,t) = r(consumable res, h,t ) e p E(h),ts (e p )=t usage(p, consumable res)
Axiom 4
r(nonconsumable res, h,t) =r(nonconsumable res, h,t )
e p E(h),ts (e p )=t usage(p, nonconsumable res)+
e p E(h),t f (e p )=t usage(p, nonconsumable res)
described CHiPs change state, specify conditions
execution succeeds fails. stated formally Definition 1, execution succeeds if:
plans preconditions met start; postconditions met end; inconditions
met throughout duration (not including start end); used resources stay within
value limits throughout duration; executions decomposition succeed. Otherwise,
execution fails.
462

fiA BSTRACT R EASONING P LANNING C OORDINATION

Definition 1
succeeds(e p , h) pre(p) r(h,ts (e p ))
post(p) r(h,t f (e p ))
t, res,ts (e p ) < < f (e p ) usage(p, res) 6= 0
in(p) r(h,t)
min value(res) <= r(res, h,t) <= max value(res)
e d(e p ), succeeds(e, h)
2.5 Asserting, Clobbering, Achieving, Undoing
Conventional planning literature often speaks clobbering achieving preconditions plans
(Weld, 1994). CHiPs, notions slightly different since inconditions clobber
clobbered, seen previous section. Formalizing concepts another, undoing
postconditions, helps us define summary conditions (in Section 3.2). However, convenient
define first means assert condition. Figure 5 gives examples executions involved
interactions, define terms follows:
Definition 2
asserts(e p , `,t, h) (e p E(h))
(` in(p) = ts (e p ) +
` post(p) = f (e p ))
(r(t, h) ` `)
Definition 2 states execution e p history h asserts literal time literal
effect p holds state t. Note point on, beginning Definition 3,
use brackets [ ] shorthand defining similar terms procedures. example, saying [a,
b] implies [c, d] means implies c, b implies d. shorthand help us avoid repetition,
cost slightly difficult parsing.
Definition 3
[achieves, clobbers] precondition(e p , `, e p0 ,t, h)
e p , e p0 E(h)
asserts(e p , [`, `],t, h) ` pre(p0 ) < ts (e p0 )
6 e p00 ,t 00 , (asserts(e p00 , `,t 00 , h) asserts(e p00 , `,t 00 , h)) < 00 ts (e p0 )
Definition 4
clobbers [in, post]condition(e p , `, e p0 ,t, h)
e p , e p0 E(h)
asserts(e p , `,t, h) ` [in(p0 ), post(p0 )] [ts (e p0 ) < < ts (e p0 ),t = f (e p0 )]
Definition 5
undoes(e p , `, e p0 ,t, h)
e p , e p0 E(h)
asserts(e p , `,t, h) ` post(p0 ) f (e p0 ) >
6 e p00 ,t 00 , (asserts(e p00 , `,t 00 , h) asserts(e p00 , `,t 00 , h)) f (e p0 ) 00 <
463

fiC LEMENT, URFEE , & BARRETT

Figure 5: Interval interactions plan steps
So, execution achieves clobbers precondition last (or one last) assert
condition negation (respectively) required. Likewise, execution undoes
postcondition first (or one first) assert negation condition
condition asserted. execution e clobbers incondition postcondition e0 e asserts
negation condition end (respectively) e0 . Achieving effects (inconditions
postconditions) make sense formalism, defined. Figure 5 shows
different ways execution e achieves, clobbers, undoes execution e0 . ` ` point
asserted required met.
2.6 External Conditions
recognized Tsuneto et al. (1998), external conditions important reasoning potential refinements abstract plans. Although basic idea same, define little
differently call external preconditions differentiate conditions
call external postconditions. Intuitively, external precondition group partially ordered
plans precondition one plans achieved another group must
met external group. External postconditions, similarly, undone
plans group net effects group. Definition 6 states ` external [pre,
post]condition execution e p ` [pre, post]condition subplan
[achieved, undone] subplan.
Definition 6
external [pre, post]condition(`, e p )
h, E(h) = {e p } subex(e p )
(e p0 E(h), ` [pre(p0 ), post(p0 )]
6 e p00 E(h),t,[achieves pre, undoes post]condition(e p00 , `, e p0 ,t, h))

464

fiA BSTRACT R EASONING P LANNING C OORDINATION

example Figure 2, available(G) external precondition because, although G
must exist produce H, G supplied execution produce G plan. Thus, available(G)
met internally, making available(G) internal condition. available(M1) external precondition, internal condition, external postcondition needed externally
internally; effect produce G M1 releases M1 finished;
plan decomposition undoes effect.

3. Plan Summary Information
Summary information used find abstract solutions guaranteed succeed matter
refined information describes potential conditions underlying
decomposition. Thus, commitments particular plan choices, whether single agent
agents, made based summary information without worrying deeper details
lurk beneath doom commitments. HTN planners used abstract conditions
guide search (e.g., Sacerdoti, 1974; Tsuneto et al., 1998), rely user-defined subset
constraints help detect potential conflicts. contrast, summary information
used identify potential conflicts.
formalisms previous section, define summary information
describe method computing non-primitive plans (in Section 3.4).
many detailed definitions algorithms section, follow structure
previous section, first give informal overview key concepts notation,
subsequently delve systematically.
3.1 Overview
summary information plan p consists summary pre-, in-, postconditions (presum (p),
insum (p), postsum (p)), summary resource usage (usagesum (p, res)) resource res, whether
plan executed way successfully (consistent).
summary condition (whether pre, post, in) specifies positive negated literal,
additional modal information. summary condition associated existence, whose
value either must may depending whether must hold possible decompositions
abstract operator may hold depending decomposition chosen. timing
summary condition either f irst, last, always, sometimes, specifying condition must
hold plans interval execution. plan p1 must [achieve, clobber] summary precondition
c2 p2 execution p1 (or plan summary information) would
[achieve, clobber] condition summarized c2 (or plan summary information
p2 ).
algorithm deriving summary conditions plan p takes input summary conditions immediate subplans p conditions defined CHiP p. pre-, in-,
postconditions p become must first, must always, must last summary conditions, respectively. algorithm retains existence timing subplan summary conditions parent
depending whether conditions achieved, clobbered, undone siblings, whether
decomposition or, whether subplan ordered first last, whether subplans
share condition. Subplan first, always, last conditions become sometimes conditions parent. parent computed consistent long subplans consistent,

465

fiC LEMENT, URFEE , & BARRETT

subplan may clobber summary condition another, summarized resources violate
limits.
represent summary resource usage three value ranges, hlocal min, local max, persisti,
resources local usage occurs within tasks execution, persistent usage represents usage lasts task terminates depletable resources. summarization
algorithm abstract task takes summary resource usages subtasks, considers legal orderings subtasks, possible usages subintervals within interval
abstract task, build multiple usage profiles. profiles combined algorithms
computing parallel, sequential, disjunctive usages give summary usage parent task.
3.2 Summary Conditions
summary information plan p, psum , tuple hpresum , insum , postsum , usagesum , consistenti,
whose members sets summary conditions, summarized resource usage, consistent flag
indicating whether plan execute consistently internally. presum (p) postsum (p) summary pre- postconditions, external pre- postconditions p, respectively.
summary inconditions p, insum (p), contain conditions must hold within execution
p successful. condition c one sets tuple h`, existence,timingi. `(c)
literal c. existence c must may. existence(c) = must, c called
must condition ` must hold every successful plan execution. convenience usually
write must(c). c may condition (may(c) true) `(c) must hold successful execution.
timing summary condition c either always, sometimes, f irst, last. timing(c)
always c insum `(c) incondition must hold throughout potential executions p
(` holds always); otherwise, timing(c) = sometimes meaning `(c) holds one point, least, within
execution p. So, always condition must, define may always inconditions
whether may existence timing, significantly different may
sometimes planner reasons it. Whether condition may always (however defined)
may sometimes, another plan may clobber relationship condition (as
defined Section 3.3). Note incondition CHiP restricted meaning
must always summary incondition. timing f irst c presum `(c) holds beginning
execution p; otherwise, timing = sometimes. Similarly, timing last c postsum `(c)
asserted end successful execution p; otherwise, sometimes. Although existence
timing syntactically take one value, semantically must(c) may(c), always(c)
sometimes(c).
considered using modal logic operators describe concepts. mix existing
temporal logic dynamic logic (Pratt, 1976) notation could forced work, found
using terminology made definitions much simpler. discuss end
Section 8.
Definitions 7, 8, 9 give formal semantics existence timing representative
condition types. Summary conditions plan defined recursively depend
summary conditions plans immediate subplans instead complete decomposition. single description summary information could represent many different plan hierarchies,
quantify plans p0 , whose subplans summary information
plan p summarized. could defined existence timing properties conditions
based entire hierarchy, so, deriving summary conditions would expensive

466

fiA BSTRACT R EASONING P LANNING C OORDINATION

solving planning problem, one main purposes summary information reduce
computation planning problem. reason would expensive
worst case legal orderings plan steps must explored determine whether condition
must may. discuss example end subsection.
Definition 7
[must, may] f irst precondition(`, p)
p0 = hpre(p), in(p), post(p), {},type(p), subplans(p0 ), order(p)i
summary f ormation f subplans(p0 ) = summary f ormation f subplans(p)
h,e p0 , E(h) = {e p0 } subex(e p0 ) [true, external precondition(`, e p0 )]
e p00 E(h),ts (e p00 ) = ts (e p0 ) ` pre(p00 )

Definition 8
must always incondition(`, p)
p0 = hpre(p), in(p), post(p), {},type(p), subplans(p0 ), order(p)i
summary f ormation f subplans(p0 ) = summary f ormation f subplans(p)
h, e p0 ,E(h) = {e p0 } subex(e p0 ),t,ts (e p0 ) < < f (e p0 )
e p00 E(h),ts (e p00 ) < < f (e p00 ) ` in(p00 )

Definition 9
[must, may] sometimes incondition(`, p)
[, ]p0 = hpre(p), in(p), post(p), {},type(p), subplans(p0 ), order(p)i
summary f ormation f subplans(p0 ) = summary f ormation f subplans(p) [, ]
[, ]h, e p0 ,E(h) = {e p0 } subex(e p0 ), t,ts (e p0 ) < < f (e p0 )[, ]
e p00 E(h), = ts (e p00 ) ` pre(p00 )
ts (e p00 ) < < f (e p00 ) ` in(p00 )
= f (e p00 ) ` post(p00 )
Definition 7 states f irst precondition p external precondition always required beginning execution p0 conditions p
summary information ordering subplans p. last postcondition always asserted
end execution (substitute pre post ts f last two lines Definition 7). [must,may] sometimes precondition [must,may] external precondition
f irst precondition. sometimes postcondition defined similarly. Definition 8 states literal
` must, always incondition plan p time isolated execution p0
summary information p, executing plan p00 incondition `. Definition 9
states [must, may] sometimes incondition plan p condition required [any,
some] execution [any, some] plan p0 summary information ordering
subplans p.
consistent flag boolean indicating whether plan (or plan summary information ordering subplans) would execute successfully matter decomposed matter subplans executed. Definition 10 says possible
467

fiC LEMENT, URFEE , & BARRETT

executions succeed consistent plan. similar CanAnyWay relation
defined Section 4. include whether plan definitely succeed
summary information requires exponential computation see whether conflicts
subplans resolved. computation wait done planning summary
information fully derived.
Definition 10
consistent(p)
p0 = hpre(p), in(p), post(p), usage(p),type(p), subplans(p0 ), order(p)i
summary f ormation f subplans(p0 ) = summary f ormation f subplans(p)
h, e p0 E (p0 ), e p0 succeeds
show subset summary conditions production managers top-level plan (of
Figure 2) below. Following literal modal tags existence timing information. Mu
must; may; F f irst; L last; sometimes; always.
Production managers produce H plan:
Summary preconditions:
available(A)MuF, available(M1)MaS, available(M2)MaS
Summary inconditions:
available(A)MuS, available(M1)MaS, available(M2)MuS, available(G)MuS,
available(A)MuS, available(M1)MaS, available(M2)MuS, available(G)MuS,
available(H)MuS, available(H)MuS
Summary postconditions:
available(A)MuS, available(M1)MaS, available(M2)MuS, available(G)MuS,
available(H)MuL

available(M1) summary precondition may condition production manager
may end using M1 chooses use M2 instead produce G. available(A) f irst summary precondition part must used beginning execution transported
one machines. machines needed sometime parts transported,
sometimes (and first) conditions: needed point time beginning execution.
production manager may use M1 produce G, available(M1) summary
incondition produce H. available(M1) available(M1) inconditions
consistent sometimes conditions, implying hold different times
plans execution. contrast, conditions would conflict must always
(meaning must always hold throughout every possible execution plan).
summary condition available(A) must postcondition top-level plan
definitely consumed make G produced plan decomposition
produce H f rom G. Even though available(G) effect produce G, external
postcondition produce H undone produce H f rom G, consumes G
make H. available(H) last summary postcondition production manager releases
H end execution. available(M2) last manager finishes using M2
moving H storage.
Notice available(M2) may summary precondition. However, matter hierarchy decomposed, M2 must used produce H, available(M2) must established
468

fiA BSTRACT R EASONING P LANNING C OORDINATION

externally production managers plan. summary information defined terms
summary information immediate subplans, subplans produce H, see
produce G available(M2)MaS precondition available(M2)MaS postcondition
would achieve available(M2)MuF precondition produce H f rom G. summary
information tell us precondition produce G exists postcondition
exists, necessary condition determine derived precondition produce H must
condition. Thus, may. augmented summary information subsets conditions
existed together, hunting combinations temporal orderings condition subsets among
subplans derive summary conditions would basically adaptation HTN planning algorithm, summary information intended improve. Instead, derive summary information
polynomial time use improve HTN planning exponentially explain Section 6. tradeoff made beginning section defining summary conditions
terms immediate subplans instead entire hierarchy. Abstraction involves loss
information, loss enables computational gains.
3.3 Summary condition relationships algorithms
order derive summary conditions according definitions, need able recognize
achieve, clobber, undo relationships based summary conditions basic CHiP
conditions. give definitions algorithms these, build constructs algorithms
reasoning temporal relationships, described Appendix A.
Achieving clobbering similar, define together. Definition 11 states
plan p1 must [achieve, clobber] summary precondition c2 p2 executions
two plans, p01 p02 , summary information ordering constraints p1
p2 , execution p01 one subexecutions would [achieve, clobber] external precondition
`(c2 ) p02 .
Definition 11
must [achieve, clobber] precondition(p1 , c2 , p2 , Psum , order)
h H(Psum , order), p01 , p02 , e p01 , e p02 ,
(p01 p02 summary ordering f ormation p1 p2 )
t,e p001 subex(e p01 ), e p002 subex(e p02 ),
[achieve, clobber] precondition(e p001 , `(c2 ), e p002 ,t, h)
external precondition(`(c2 ), e p02 )
Achieving clobbering in- postconditions defined Definition 11 substituting post pre removing last line inconditions. Additionally substituting gives definitions may achieve clobber. Furthermore, definitions
must/may-undo obtained substituting post pre undo achieve Definition 11. Note that, mentioned Section 2.5, achieving inconditions postconditions
make sense formalism.
Algorithms interactions given Figure 6 Figure 7. algorithms build
others (detailed Appendix B) use interval point algebra determine whether plan must
may assert summary condition before, at, time another plan requires summary
condition hold. Similar Definition 3 must-achieve CHiP conditions, Figure 6 says p0
469

fiC LEMENT, URFEE , & BARRETT

Algorithm: Must-[achieve, clobber]
Input: plan p0 , summary condition c plan p, Psum , order
Output: true f alse, whether p0 must-[achieve, clobber] c
begin function
c0 in(p0 ) post(p0 )
`(c0 ) [`(c), `(c)] must(c0 )
c insum (p) p0 must-assert c0 c return [unde f ined, true]
c postsum (p) p0 must-assert c0 c return [unde f ined, true]
c presum (p) p0 must-assert c0 c
set assertion inbetween = f alse
c00 in(p00 ) post(p00 ), p00 Psum assertion inbetween = f alse
(p0 may-assert c0 c00
p00 may-assert c00 c
`(c00 ) [`(c), `(c)])
(p0 must-assert c0 c00
p00 must-assert c00 c
`(c00 ) [`(c), `(c)] must(c00 ))
set assertion inbetween = true
assertion inbetween return true
return f alse
end function

Figure 6: Algorithm whether plan must achieve clobber summary condition
achieves summary condition c must asserts condition must hold,
plans may assert condition negative between. algorithm may-achieve
(in Figure 7) mainly differs p0 may assert condition beforehand, plan
must assert between. undo algorithms achieve swapping c
c0 must/may-assert lines.
complexity determining must/may-clobber inconditions postconditions simply
O(c) check c conditions p0 . conditions hashed, algorithm constant time.
rest algorithm cases, complexity walking summary conditions
checking p00 c00 O(nc) maximum c summary conditions n plans
represented Psum . worst case, summary conditions summarize propositional
variable, O(nc) conditions must visited.
Lets look examples relationships. Figure 8a, p0 = equip M2 tool mayclobber c = available(M2)MaS summary preconditions p = produce G
history equip M2 tool ends produce G starts, calibrate M2 starts
produce G starts. Figure 8b, p0 = build H must-achieve c = available(H)MuF summary preconditions p = move H. Here, c0 available(H)MuL summary postconditions
build H. histories, build H attempts assert c0 move H requires c
met, plan execution attempts assert condition availability
H. equip M2 tool may-clobber c = available(M2)MuF summary preconditions
build H even though equip M2 tool asserts c0 = available(M2)MuL c required
met. calibrate M2 must assert available(M2)MuA time
equip M2 tool asserts c0 c required. Thus, calibrate M2 must-undo equip M2 tool

470

fiA BSTRACT R EASONING P LANNING C OORDINATION

Algorithm: May-[achieve, clobber]
Input: plan p0 , summary condition c plan p
Output: true f alse, whether p0 may-[achieve, clobber] c
begin function
c0 in(p0 ) post(p0 )
`(c0 ) [`(c), `(c)]
c insum (p) p0 may-assert c0 c return [unde f ined, true]
c postsum (p) p0 may-assert c0 c return [unde f ined, true]
c presum (p) p0 may-assert c0 c
set assertion inbetween = f alse
c00 in(p00 ) post(p00 ), p00 Psum assertion inbetween = f alse
p0 must-assert c0 c00
p00 must-assert c00 c
`(c00 ) `(c) `(c) must(c00 ))
set assertion inbetween = true
assertion inbetween return true
return f alse
end function

Figure 7: Algorithm whether plan may achieve clobber summary condition
a)

produce H
produce G

produce H G
move G

build H

move H

service M2
move

equip M2

tool

calibrate M2

tool

b)

produce H
produce H G

produce G

move G
=
move
tool

build H

move H

service M2

equip M2

calibrate M2

tool

Figure 8: production facilities managers plans partially expanded. a) managers plans
unordered respect other. b) equip M2 tool must clobber available(M2)MaL
produce G, calibrate M2 must clobber available(M2)MuF build H.

summary postcondition. calibrate M2 cannot assert postcondition available(M2)MuL
build H requires available(M2)MuF, calibrate M2 must-clobber summary precondition.

471

fiC LEMENT, URFEE , & BARRETT

3.4 Deriving Summary Conditions
algorithms determine interactions abstract plans based summary
conditions, create algorithm derives summary conditions according definitions Section 3.2. Figure 9 shows pseudocode algorithm. method deriving
summary conditions plan p recursive. First, summary information derived ps
subplans. conditions added based ps conditions. rest algorithm
derives summary conditions ps subplans. Whether p consistent depends
consistency subplans whether summary conditions resource usages
conflict. braces { } used slightly different semantics used
brackets. expression {x,y} interpreted simply (x y, respectively).
Definitions algorithms temporal relationships always- f irst covers
Appendix A. algorithm adds copies condition set, one condition exist
literal, conditions information may overwritten literal. cases,
must overwrites may; f irst, last, always overwrite sometimes; but, vice-versa. Further,
uses recursion, procedure assumed work plans whose expansion finite.
3.5 Summary Resource Usage
section, define representation capturing ranges usage local task interval depleted usage lasting end interval. Based introduce
summarization algorithm captures ranges uncertainty represented decomposition choices plans partial temporal orderings plan subtasks. representation
allows coordinator planner reason potential conflicts set tasks.
discuss reasoning later Section 4.2. Although referred resources, variables could
durations additive costs rewards.
3.5.1 R EPRESENTATION
start new example simplicity motivates choice representation. Consider
task coordinating collection rovers explore environment around lander
Mars. exploration takes form visiting different locations making observations.
traversal locations follows established paths minimize effort risk. paths combine form network one mapped Figure 10, vertices denote distinguished
locations, edges denote allowed paths. Thinner edges harder traverse, labeled points
associated observation goals. paths hard ground, others loose
sand traversal harder since rover slip.
Figure 11 gives example abstract task. Imagine rover wants make early
morning trip point point B example map. trip sun slowly rises
horizon giving rover ability progressively use soak rays tasks provide
solar power (a non-consumable resource3 ) motors wheels. addition collecting photons,
morning traverse moves rover, resultant go tasks require path dependent amounts
power. rover traveling point point B take number paths, shortest
three involve following one, two, three steps.
3. important confuse power battery energy. power source (e.g. battery, solar panels) makes fixed
amount power Watts available point time. batterys energy (in Watt-hours) reduced integral
total use power time.

472

fiA BSTRACT R EASONING P LANNING C OORDINATION

Algorithm: Derive summary information
Input: plan p
Output: psum
begin function
derive summary information
p0 d(p)
V
set consistent(p) = p0 d(p) consistent(p0 )
` pre(p) add h`, must, f irsti presum (p)
` in(p) add h`, must, alwaysi insum (p)
` post(p) add h`, must, lasti postsum (p)
summary condition c0 p0 d(p)
set c = c0
c0 {presum (p0 ),postsum (p0 )}
c0 must-{achieved,undone} must-clobbered within d(p),
type(p) = (p0 always { f irst,last}
temporally ordered subplan according order(p)
sometimes- { f irst,last} subplan p0
{ f irst, last} `(c0 ) condition {presum (p0 ),postsum (p0 )}),
set timing(c) = sometimes
c0 may-{achieved,undone} may-clobbered P d(p)
p00 P must `(c0 ) condition {presum (p00 ),postsum (p00 )},
set existence(c) = may
copy c {presum (p),postsum (p)}
c0 insum (p0 ) p0 not-always { f irst,last} according order(p),
must(c0 ) c0 always-not- { f irst,last} according order(p),
set existence(c) = must
set P = 0/
set allAlways = true
p00 d(p), c00 insum (p00 )
`(c00 ) `(c)
always(c00 ) add p00 P
else set allAlways = f alse
else allAlways = f alse
always(c) ((type(p) = P covers p according order(p))
(type(p) = allAlways)),
set timing(c) = always
add c insum (p)
c0 may-clobbered, set consistent = f alse
usagesum (p) = SummarizeResourceUsage(p) (in Section 3.5.2)
consistent(usagesum (p)) = f alse, set consistent(p) = f alse
end function

Figure 9: Algorithm deriving summary information
summarized resource usage consists ranges potential resource usage amounts
performing abstract task, represent summary information plan p
resource res using structure
usagesum (p, res) = hlocal min(p, res), local max(p, res), persist(p, res)i,

473

fiC LEMENT, URFEE , & BARRETT




B
C
F
E

Figure 10: Example map established paths points rover domain
morning activities
move(A,B)
soak rays soak rays soak rays
use -4w use -5w use -6w
20 min
20 min
20 min
go(A,1)
use 3w
10 min

take low path

go(1,2)
use 3w
10 min

high path
middle path
go(A,B)
go(2,B) use 4w go(A,3) go(3,B)
use 6w 50 min use 4w use 6w
20 min
15 min 25 min

Figure 11: and/or tree defining rovers tasks resource usages
resources local usage occurs within ps execution, persistent usage represents
usage lasts execution terminates consumable resources.
Definition 12
usagesum (p, res)
h[minhH,e p E(h) (mints (e p )<t<t f (e p ) (r(res, h,t))), maxhH,e p E(h) (mints (e p )<t<t f (e p ) (r(res, h,t)))]
[minhH,e p E(h) (maxts (e p )<t<t f (e p ) (r(res, h,t))), maxhH,e p E(h) (maxts (e p )<t<t f (e p ) (r(res, h,t)))]
[minhH,e p E(h) (r(res, h,t f (e p ))),
maxhH,e p E(h) (r(res, h,t f (e p )))]

context Definition 12 set histories H value res 0 initial
state, E(h) contains execution p subexecutions. Thus, r(res, h,t) term
combined usage res time executions hierarchy defined Section 2.4. So,
maximum local min highest among histories lowest point usage
ps execution. usage ranges capture multiple possible usage profiles task multiple
decomposition choices timing choices among loosely constrained subtasks. example,
high path task h[4,4],[6,6],[0,0]i summary power use 40 minute interval. case
ranges single points due uncertainty task simply uses 4 watts 15 minutes
followed 6 watts 25 minutes. move(A,B) task provides slightly complex example
due decompositional uncertainty. task h[0,4],[4,6],[0,0]i summary power use
50 minute interval. cases persist [0,0] solar power non-consumable
resource.
example reasoning resource usage summaries, suppose 3 watts power
available move(A,B) task. Given [4,6] local max, know
enough power matter task decomposed. Raising available power 4 watts makes
task executable depending gets decomposed scheduled, raising 6
watts makes task executable possible decompositions.
474

fiA BSTRACT R EASONING P LANNING C OORDINATION

representation abstract (or uncertain) metric resource usage seen extension
tracking optimistic pessimistic resource levels (Drabble & Tate, 1994). Computing
upper lower bounds resource usage abstract plan gives information
whether lower upper bound constraints resource may, must, must violated,
complete. representing upper lower bounds ranges bounds
potential histories, certainly know whether bounds may, must, must violated
histories. example above, tracked one range local usage, [0,6],
would know definitely conflict 3 watts available. Knowing
extra information avoid exploration infeasible search space.
3.5.2 R ESOURCE UMMARIZATION LGORITHM
state summarization algorithm Section 3.4 recursively propagates summary conditions upwards and/or hierarchys leaves, algorithm resource summarization takes
approach. Starting leaves, algorithm finds primitive tasks use constant amounts
resource. resource summary task using x units resource h[x,x],[x,x],[0,0]i
h[x,x],[x,x],[x,x]i tasks duration non-consumable consumable resources respectively.
Moving and/or tree, summarization algorithm either comes branch.
branch combined summary usage comes computation
h [mincchildren (lb(local min(c))), maxcchildren (ub(local min(c)))],
[mincchildren (lb(local max(c))), maxcchildren (ub(local max(c)))],
[mincchildren (lb(persist(c))),
maxcchildren (ub(persist(c)))]
i,

lb() ub() extract lower bound upper bound range respectively. children
denote branchs children durations extended length longest child.
duration extension alters childs resource summary information childs usage profile
zero resource usage extension. instance, determining resource usage
move(A,B), algorithm combines two 40 minute tasks 50 minute task. resulting
summary information describes 50 minute abstract task whose profile might zero watt
power usage 10 minutes. extension move(A,B) [0,4] local min instead
[3,4]. Planners reason variable durations could use [3,4] duration ranging
40 50.
Computing branchs summary information bit complicated due timing
choices among loosely constrained subtasks. take x path examples illustrate simplest subcase, subtasks tightly constrained execute serially. profiles appended together,
resulting summary usage information comes SERIAL-AND computation
h [mincchildren (lb(local min(c)) + lb (c)), mincchildren (ub(local min(c)) + ub (c))],
pre
pre
[maxcchildren (lb(local max(c)) + lb (c)), maxcchildren (ub(local max(c)) + ub (c))],
[cchildren (lb(persist(c))),
cchildren (ub(persist(c)))]
i,
pre

pre

pre
pre
lb
(c) ub
(c) respective lower upper bounds cumulative persistent usages children execute c. computations form
computations final persist.
case subtasks execute parallel identical durations slightly simpler.
usage profiles add together, branchs resultant summary usage comes

475

fiC LEMENT, URFEE , & BARRETT

move(A,B)
soak rays
<[-4,-4],[-4,-4],[0,0]>

<[0,4],[4,6],[0,0]>

soak rays
<[-5,-5],[-5,-5],[0,0]>

soak rays
<[-6,-6],[-6,-6],[0,0]>

Figure 12: Possible task ordering rovers morning activities, resulting subintervals.
PARALLEL-AND computation
h [cchildren (lb(local min(c))),
maxcchildren (ub(local min(c)) + non
ub (c))],
non
[mincchildren (lb(local max(c)) + lb (c)), cchildren (ub(local max(c)))],
[cchildren (lb(persist(c))),
cchildren (ub(persist(c)))]
i,
non
non
ub (c) lb (c) respective sums local max upper bounds local min
lower bounds children except c.
handle tasks loose temporal constraints, consider legal orderings child
task endpoints. example, rovers early morning tasks, three serial solar energy collection subtasks running parallel subtask drive location B. Figure 12 shows
one possible ordering subtask endpoints, breaks move(A,B) three pieces,
two soak rays children half. Given ordering, summarization algorithm (1)
use endpoints children determine subintervals, (2) compute summary information
child task/subinterval combination, (3) combine parallel subinterval summaries using
PARALLEL-AND computation, (4) chain subintervals together using SERIALAND computation. Finally, tasks summary computed combining summaries
possible orderings using computation.
describe step (2) generates different summary resource usages subintervals
child task. child task summary resource usage h[a,b],[c,d],[e, f ]i contributes one two
summary resource usages intersecting subinterval4 :

h[a, b], [c, d], [0, 0]i, h[a, d], [a, d], [0, 0]i.

first usage tighter [a,b],[c,d] local ranges, second looser [a,d],[a,d] local
ranges. Since b c bounds apply subintervals containing subtasks minimum
maximum usages, tighter ranges apply one subtasks intersecting subintervals.
minimum maximum usages may occur subinterval, symmetry arguments
let us connect computation. Thus one subinterval tighter local ranges
intersecting subintervals get looser local ranges, extra complexity comes
investigate subtask/subinterval assignment options. instance, three subintervals
intersecting move(A,B) Figure 12, three different assignments summary resource usages
subintervals: placing [0,4],[4,6] one subinterval [0,6],[0,6] two.
placement options result subtask n subintervals n possible subinterval assignments.
child tasks n alternate assignments, nm combinations
potential subtask/subinterval summary resource usage assignments. Thus propagating summary
information branch exponential number subtasks multiple internal
4. summary resource usages last interval intersecting child task, replace [0, 0] [e, f ] persist.

476

fiA BSTRACT R EASONING P LANNING C OORDINATION

subintervals. However since number subtasks controlled domain modeler
usually bounded constant, computation tractable. addition, summary information
often derived offline domain. propagation algorithm takes form:
consistent ordering endpoints:
consistent subtask/subinterval summary usage assignment:
Use PARALLEL-AND computations combine subtask/subinterval summary
usages subinterval.
Use SERIAL-AND computation subintervals combined summary usages
get consistent summary usage.
Use computation combine consistent summary usages get tasks summary
usage.
described derive summary information, discuss use it.

4. Identifying Abstract Solutions
point, detailed algorithms deriving summary conditions reasoning
potential (may) definite (must) interactions tasks based summary information. addition, outlined algorithms deriving summarized resource usage
yet discussed identify solutions abstract levels. section, show
interactions summary conditions summarized metric resource usages identify potentially
resolvable threats unresolvable conflicts among plans group agents.
4.1 Threats Summary Conditions
Agents attempt resolve conflicts among plans considering commitments particular
decompositions ordering constraints. order this, agents must able identify
remaining conflicts (threats) among plans. present simple algorithms reasoning
threats abstract plans required conditions.
Formally, set CHiPs P ordering constraints order, threat abstract plan
p P summary condition c0 another plan p0 P exists iff p may-clobber c0 . say
threat unresolvable p must-clobber c0 must(c0 ) decomposition choices
ordering constraints could added resolve threat.
So, simple algorithm identifying threats check see O(nc) summary
conditions n plans Psum must- may-clobbered plan. Since complexity
checking see particular condition must- may-clobbered O(nc), algorithms
complexity O(n2 c2 ).
many coordination tasks, agents could determine certain temporal constraints
plans decomposed way (CanAnyWay) constraints
way successfully decomposed (MightSomeWay), make coordination
decisions abstract levels without entering potentially costly search valid plan merges lower
levels. formal definitions CanAnyWay MightSomeWay:

477

fiC LEMENT, URFEE , & BARRETT

a)

produce H

b)

maintenance

produce H
maintenance
move_parts

move_parts
c)

produce H
produce G

produce H G
maintenance
service M1 M2
service

service

M1

M2

move
tool

move_parts

Figure 13: top-level plans managers manufacturing domain
Definition 13
[CanAnyWay, MightSomeWay](order, Psum )
[, ]h, P summary f ormation = Psum h H(P, order)
[, ]e E(h), succeeds(e, h)
Definition 13 states plans summary information Psum ordering constraints
execute way sets plans P summary information Psum
execute successfully history. MightSomeWay true set plans
could possibly execute successfully. could describe CanSomeWay(order,Psum )
MightAnyWay(rel,Psum ) fashion, obvious addition could
influence search. Exploring relations may interesting topic future research.
Figure 13a, three top-level plans managers unordered respect other.
leaf plans partially expanded hierarchies comprise Psum . Arrows represent constraints
order. CanAnyWay({},{produce G, maintenance, move parts}) false several conflicts use machines transports could occur certain executions
plans described Section 3.3 Figure 8. However, MightSomeWay({}, {produce G,
maintenance, move parts}) true plans might way execute successfully
shown Figure 13b. ordering constraints Figure 13b, CanAnyWay({before(1,0),
before(0,2)},{produce G, maintenance, move parts}) true plans execute
way consistent ordering constraints without conflict. Figure 8b example
MightSomeWay false calibrate M2 must-clobber available(M2)MuF summary precondition build H.
shown Figure 14, algorithm determining CanAnyWay summary conditions
simple needs check threats. MightSomeWay complicated
checking unresolvable threat enough. shown Figure 15, case
plan p must clobber p0 p00 could come achieve precondition ` p0 .
Thus, p may-clobbers ` p p00 . However, obviously p clobber one other,
478

fiA BSTRACT R EASONING P LANNING C OORDINATION

Algorithm: [CanAnyWay, MightSomeWay]
Input: order, Psum
Output: true f alse
begin function
psum Psum
[consistent(psum ), f alse] return f alse
p0sum Psum
summary condition c psum
p0 [may-clobber, must-clobber] c,
c [may must, must],
return f alse
resource res
[CanAnyWay, MightSomeWay](order, Psum , res) (see Section 4.2)
return false
return true
end function

Figure 14: Algorithm determining whether plans given summary information CanAnyWay
MightSomeWay execute successfully.
p
l

p
-l

l

l

p
l

l

l

Figure 15: MightSomeWay false even though must-clobber relationship.
MightSomeWay false. order determine MightSomeWay f alse, agent must exhaustively
search exponential number schedules see conflicts resolved. Instead
performing exponential search determine MightSomeWay, use simple algorithm
Figure 14 checks must-clobber relationships. Section 5.1 describe flexible
search find conflict-free abstract plans scheduling abstract level.
Thus, CanAnyWay algorithm sound complete, MightSomeWay algorithm
complete sound. means determining MightSomeWay sound
complete. still make use algorithms sound complete planning/coordination algorithm Section 5.1. complexity algorithms O(n2 c2 ) since
O(nc) procedures determining must/may-clobber must run nc conditions (c
summary conditions n plans represented Psum ).
4.2 Summary Resource Usage Threats
Planners detect threats resource constraints different ways. planner reasons partially ordered actions, must consider combinations actions overlap together
exceed (or fall below) resources maximum value (or minimum value). polynomial algorithm

479

fiC LEMENT, URFEE , & BARRETT

IxTeT planner (Laborie & Ghallab, 1995). planners consider total order plans simply project levels resource initial state plan,
summing overlapping usages, see conflicts (e.g., Chien et al., 2000b).
Finding conflicts involving summarized resource usages work way.
partial order planner, resultant usage clusters actions tested using PARALLELAND algorithm Section 3.5. total order planner, level resource represented
summarized usage, initially h[x, x], [x, x], [x, x]i consumable resource initial level
x h[x, x], [x, x], [0, 0]i non-consumable resource. Then, subinterval
start end times schedule tasks, summary usage computed using
PARALLEL-AND algorithm. level resource computed subinterval
propagating persistent usages using SERIAL-AND algorithm.
decide CanAnyWay MightSomeWay defined Section 4.1, terms summary usage values resulting invocations PARALLEL-AND SERIAL-AND propagation algorithm end Section 3.5.2. CanAnyWay(order, Psum , res) true
potential threats. algorithms discover threat ever compute interval

lb(local min(i)) < min value(res) lb(persist(i)) < min value(res)
ub(local max(i)) > max value(res) ub(persist(i)) > max value(res).

MightSomeWay(order, Psum , res) true possible run potentially
threats. SERIAL-AND discovers run returns summary usage
ub(local min(i)) min value(res) lb(persist(i)) min value(res)
lb(local max(i)) max value(res) ub(persist(i)) max value(res).

mechanisms deriving summary information evaluating plans based
summarizations, discuss exploit planning/coordination algorithm.

5. Hierarchical Planning Coordination Algorithm
earlier defined algorithms reasoning group agents plans multiple levels
abstraction, describe agents efficiently plan coordinate based summary
information. describe coordination algorithm searches ways restrict decomposition ordering collective actions agent(s) order resolve conflicts
maximizing utilities individual agents global utility group.
approach starts making planning decisions abstract level and, needed,
decomposes agents plans top-down fashion. idea introduce information needed. Introducing irrelevant details complicates search increases communication.
describing top-down planning/coordination algorithm, describe search techniques
heuristics algorithm use exploit summary information.
5.1 Top-Down Hierarchical Planning Coordination
formalism summary conditions culminated Section 4 algorithms determine set
plans (abstract primitive) partial set ordering constraints definitely conflict-free
(CanAnyWay) unresolvable conflicts (MightSomeWay). integrate algorithms
one searches consistent plan one agents. particular algorithm
describe shown sound complete (Clement, 2002). search starts
top-level plans agent. solution one possible conflicts among
480

fiA BSTRACT R EASONING P LANNING C OORDINATION

agents plans. algorithm tries find solution top level expands hierarchies
deeper deeper optimal solution found search space exhausted.
pseudocode description algorithm given Figure 16.
state search partially elaborated plan represent set plans (one
agent), set temporal constraints, set blocked plans. subplans plans
leaves partially expanded hierarchies agents. set temporal constraints
includes synchronization constraints added search addition dictated
agents individual hierarchical plans. Blocked subplans keep track pruned subplans.
Decisions made search decentralized fashion. agents negotiate
ordering constraints adopt, choices subplans accomplish higher level plans,
decompositions explore first. algorithm described specify (or
commit to) negotiation technique, provide mechanisms identifying choices
agents negotiate. Although agents make search decisions decentralized fashion, describe algorithm given centralized process requests summary
information agents coordinated.
pseudocode Figure 16, coordinating agent collects summary information
agents plans decomposes them. queue keeps track expanded search states.
CanAnyWay relation holds search state, Dominates function determines current
solutions better every agent solution represented current search state
keeps solution dominated. MightSomeWay false, search space rooted
current search state pruned; otherwise, coordinator applies operators generate new
search states.
operators generating successor search states expanding non-primitive plans, blocking subplans, adding temporal constraints pairs plans. agent expands one
plans, plans summary conditions replaced original conditions
parent plan. subplans summary information ordering constraints added
search state. subplan plan added (or selected) subplans
blocked. ApplyOperator called select block operators, search states
generated selectable blockable subplan, respectively. Blocking subplan
effective resolving constraint subplans involved. example,
inventory manager plans use transport2, production manager could block subplans
using transport2, leaving subplans using transport1 conflict inventory managers
plan. lead least commitment abstract solutions leave agents flexibility selecting among multiple applicable remaining subplans. agents take another approach
selecting subplan (effectively blocking others) investigate preferred choice one
likely avoids conflicts.
operator add temporal constraint, new search state created alternative temporal constraint could added. successor states enqueued
backtracking needed, alternative tried. Adding temporal constraints generate new search states ordering consistent global local constraints.
implementation, add constraints help resolve threats determined
must/may achieves clobbers algorithms. plan expanded selected, ordering
constraints must updated subplans added.
soundness completeness coordination algorithm depends soundness
completeness identifying solutions complete exploration search space. Soundness
481

fiC LEMENT, URFEE , & BARRETT

Concurrent Hierarchical Coordination Algorithm
Input: set top-level plans, initial state
Output: set solutions, pair order constraints blocked plan choices
begin function
summarized plans = 0/
plan p plans
p0 = get summary information plan p
summarized plans = summarized plans { p0 }
end
threats = { (p, p0 ) | p, p0 summarized plans, MayClobber(p, p0 ) }
/ 0,
/ threats) }
queue = { (0,
solutions = 0/
loop
queue == 0/
return solutions
(order, blocked, threats) = Pop(queue)
CanAnyWay(initial state, summarized plans, order, blocked)
solution = (order, blocked)
solutions = solutions {solution}
sol1 sol2 solutions
Dominates(sol1 , sol2 )
solutions = solutions - { sol2 }
MightSomeWay(initial state, summarized plans, order, blocked)
operator = Choose({expand, select, block, constrain})
queue = queue ApplyOperator(operator, summarized plans, order, blocked)
return solutions
end function

Figure 16: concurrent hierarchical coordination algorithm.
completeness defined respect achieving particular goal predicates resolving
conflicts plan hierarchies. domain modeler may represent goals abstract CHiPs
decompose possible plans accomplish series actions agent execute
successfully.
Consider algorithm would find coordinated plans manufacturing agents.
beginning search, coordinating agent gathers summary information top-level
plans three agents plans. first, ordering constraints, order empty
first search state (shown Figure 13a) popped queue. CanAnyWay false,
MightSomeWay true state described earlier section, coordinator chooses
operator apply search state. could choose constrain order maintenance
plan produce H resolve conflicts two plans. order updated
new constraint, new search state inserted queue according ranking
function. next iteration loop, search state queue inserted
popped. coordinator finds CanAnyWay false, MightSomeWay true since
move parts may still conflict plans use transports. choose constrain
produce H move parts resolve remaining conflicts. detected next cycle
search loop CanAnyWay found true search state (shown Figure 13b).

482

fiA BSTRACT R EASONING P LANNING C OORDINATION

plans, two constraints order, empty set blocked plans added solution
since previously found solution Dominates it. Dominates function uses domain
specific criteria determining solution value alternative kept
inferior compared another dropped. manufacturing domain, one solution
dominates another finish time least one agent earlier finish times later
agents. search continues find alternative superior solutions, although agents
may decide terminate search interest time.
5.2 Search Techniques Heuristics
Although summary information valuable finding conflict free coordinated plans abstract
levels, information valuable directing search avoid branches search
space lead inconsistent suboptimal coordinated plans. coordinator prune away
inconsistent coordinated plans abstract level quick check see MightSomeWay
false. example, search somehow reached state shown Figure 8b, coordinator
could backtrack expanding hierarchies avoid reasoning details
plans must fail.
Another strategy first expand plans involved threats. sake completeness, order plan expansions matter long expanded point
search trail cannot pruned. But, employing expand threats first (EMTF)
heuristic aims driving search hierarchy find subplan(s) causing conflicts others resolved quickly. similar most-constrained
variable heuristic often employed constraint satisfaction problems. example, facilities
inventory managers wished execute plans concurrently shown Figure 17a,
abstract level, coordinator would find conflicts use transports
moving parts. Instead decomposing produce H reasoning plan details
conflicts, EMTF heuristic would choose decompose either maintenance move parts
conflicts. decomposing maintenance agents resolve remaining
conflicts still execute concurrently.
Another heuristic coordinator use parallel EMTF choose fewest threats
first (CFTF). search orders states search queue ascending numbers threats
left resolve. effect, least-constraining value heuristic used constraint satisfaction
approaches. mentioned Section 4.1, threats identified CanAnyWay algorithm.
trying resolve threats coordinated plan search states fewer conflicts, hoped
solutions found quickly. So, EMTF heuristic ordering subplans expand,
CFTF, effect, orders subplan choices. example, production manager chooses
use machine M1 instead M2 produce G, coordinator likely closer solution
fewer conflicts resolve. heuristic applied selecting subplan
choices choosing temporal constraints variable bindings search operator
entire set operators.
addition, trying find optimal solutions style branch-and-bound search,
coordinator use cost abstract solutions prune away branches search space whose
minimum cost greater maximum cost current best solution. role
Dominates function description coordination algorithm Section 5.1. usually

483

fiC LEMENT, URFEE , & BARRETT

a)

maintenance
produce H
move_parts

b)

maintenance
service M1 M2
service

service

M1

M2

move
tool

produce H

move_parts

Figure 17:

EMTF

heuristic resolving conflicts decomposing maintenance plan

assumes cost/utility information decomposable hierarchy actions, cost
abstract action function decompositions.

6. Complexity Analyses
Even though planner coordinator use search techniques described Section 5.2
prune search space, able find solutions multiple levels abstraction reduce
computation much doubly exponentially. section, give example
analyze complexity planning scheduling characterize cost reduction
conditions occurs.
agent interleaves execution planning/coordination often must limit total computation execution cost required achieve goals. planning algorithm described Section
5.1 able search solutions different levels abstraction. manufacturing example,
implementation centralized coordinator uses algorithm find 1.9 CPU seconds solution top level agents plans shown Figure 13b. define cost execution
makespan (completion time) coordinated plan, cost solution 210
makespan production managers plan 90, facilities managers 90, inventory
managers 30. solution Figure 13c, coordinator required 667 CPU seconds,
makespan coordinated plan 170. Another solution found intermediate level
abstraction, taking 69 CPU seconds makespan 180. So, little effort,
algorithm expanded hierarchy intermediate level cost solution
reduced 30. Thus, overall cost reduced coordinating intermediate levels.
problem, coordinating higher levels abstraction less costly
fewer plan steps. But, even though fewer plans higher levels, plans may
greater numbers summary conditions reason collected much
greater set plans below. argue even worst case number summary
conditions per plan increases exponentially hierarchy, finding solutions abstract levels
expected exponentially cheaper lower levels. first analyze complexity
484

fiA BSTRACT R EASONING P LANNING C OORDINATION

summarization algorithm help reader understand summary conditions collect
greater sets higher levels.
6.1 Complexity Summarization
Consider hierarchy n total plans, b subplans non-primitive plan, depth d, starting
zero root, shown Figure 18. procedure deriving summary conditions works
basically propagating conditions primitives hierarchy abstract
plans. conditions non-primitive plan depend immediate subplans, deriving summary conditions done quickly number subplans large.
derivation algorithm mainly involves checking achieve, clobber, undo interactions among
subplans possible total orderings subplans (as described Section 3.4). Checking
one relations one summary condition one subplan O(bs) b subplans,
summary conditions (as discussed Section 3.3). Since O(bs) conditions must
checked set subplans, deriving summary conditions one plan subplans
O(b2 s2 ).
However, maximum number summary conditions subplan grows exponentially
hierarchy since, worst case, summary conditions merge summarization.
happens conditions subplan completely different propositions/variables
sibling subplan. case, separate summary condition generated
summary condition subplan. children share conditions variable,
information collapsed single summary condition parent plan.
shown third column table Figure 18, plan lowest level = c
summary conditions derived c pre-, in-, postconditions. plan level 1 derives c
summary conditions conditions c b subplans giving c + bc summary conditions, = O(bc). So, worst case = O(bdi c) plan level hierarchy
plan c (non-summary) conditions. Thus, complexity summarizing plan
level (with subplans level + 1) O(b2 b2(d(i+1)) c2 ) = O(b2(di) c2 ). bi plans
level (second column figure), complexity summarizing set plans level
O(bi b2(di) c2 ) = O(b2di c2 ) shown fourth column figure. Thus, complexity
2(di) c2 ). summation = 0
summarizing entire hierarchy plans would O(d1
i=0 b b
2d
2
dominates, complexity simplified O(b c ). n = O(bd ) plans
hierarchy, write simply O(n2 c2 ), square size hierarchy.
best case conditions variable, plan c summary
2 2
conditions. Thus, complexity summarizing hierarchy O(d1
i=0 b b c ),
simplifies O(bd+1 c2 ) = O(nbc2 ). case, summarization conditions tractable,
discussed Section 3.5.2, summarization resources tractable.
6.2 Complexity Finding Abstract Solutions
order resolve conflicts (and potentially arrive solution) particular level expansion
hierarchy, coordination algorithm checks threats plans particular
ordering constraints level. Checking threats involves finding clobber relations among
plans summary conditions. complexity finding threats among n plans
summary conditions O(n2 s2 ) shown Section 4.1 MightSomeWay algorithm.
hierarchy expanded level i, n = O(bi ) plans frontier expansion, plan
485

fiC LEMENT, URFEE , & BARRETT

level #plans #conds / #operations #test operations / solution
plan derive summ. info. solution candidate space

1

2

...

b

......
.................................
..........
............
1

2

...

b

1

2

...

b

.......

0

1

O(bdc)

O(b2(bd-1c)2)
= O(b2dc2)

O(1)

1

1

b

O(bd-1c)

O(bb2(bd-2c)2)
= O(b2d-1c2)

O(b2(b(d-1)c)2)
= O(b2dc2)

O(kb)

2

b2

O(bd-2c) O(b2b2(bd-3c)2)
= O(b2d-2c2)

O(b4(b(d-2)c)2)
= O(b2dc2)

O(kb )

d-2

bd-2

O(b2c)

O(bd-2b2(bc)2)
= O(bd+2c2)

O(b2(d-2)(b2c)2) O(kb )
= O(b2dc2)

d-1

bd-1

3c+b3c
= O(bc)

O(bd-1b2c2)
= O(bd+1c2)

O(b2(d-1)(bc)2) O(kb )
= O(b2dc2)



bd

3c

O(1)

O(b2dc2)

O(kb )



bi

O(bd-ic)

O(b2d-ic2)

O(b2dc2)

O(kbi)

2

d-2

d-1



Figure 18: Complexity threat identification resolution abstract levels
= O(bdi c) summary conditions. So, shown fifth column table Figure 18,
worst case complexity checking threats one synchronization set plans level
O(b2i (bdi c)2 ) = O(b2d c2 ). Notice drops formula, meaning complexity
checking candidate solution independent depth level. best case summary
conditions fully merge, plan = c summary conditions, complexity checking
candidate solution O(b2i c2 ), factor O(b2(di) )faster worst case.
However, algorithm may check many synchronizations particular level finding
solution exhausting search space. fact search complexity grows exponentially
number plans.5 Thus, shown last column table Figure 18, search space

O(kb ) bi plans level constant k.6 Thus, search space grows doubly exponentially
hierarchy based number plan steps.
refinement coordination planning algorithm, conflict detection basic operation
done resolving conflicts. So, include effect size conditions (in
addition plan steps) complexity planning/coordination algorithm, must multiply

complexity check threats. Thus, complexity O(kb b2d c2 ) summary information

merge O(kb b2i c2 ) summary information fully merges. complexity

resolving conflicts primitive level O(kb b2d c2 ), resolving conflicts abstract


level speeds search doubly exponentially, factor O(kb b ) even summary information
merge summarization. Now, completely merges, speedup factor


O(kb b b2(di) ).
5. fact, NP-complete (Clement, 2002).
6. Georgeff chose cluster multiple operators critical regions synchronize (fewer) regions
since would many fewer interleavings check (1983). exploiting hierarchical structure plans,
use clusters predefined hierarchy kind advantage without needing cluster bottom
up.

486

fiA BSTRACT R EASONING P LANNING C OORDINATION

level
0
1


branching
factor b

...
1

2

n
c constraints
per hierarchy

v
variables

Figure 19: Schedule n task hierarchies c constraints v variables
plans analysis. case plans, able prune
branches higher levels based summary information greatly improve search despite
overhead deriving using summary conditions. Pruning effectively reduces branching
factor since branch eliminated investigating details. Thus, complexity based

number plan steps becomes O(k(bp) ) fraction p/b branches pruned. Thus,
pruning create exponential reduction search.
6.3 Scheduling Complexity
local search planner (e.g. ASPEN, Chien et al., 2000b) backtrack, problem
solved same, one might expect complexity advantages refinement planner. However, search operations local search planner different.
previous study technique called aggregation eliminates search inefficiencies lower levels
detail task hierarchies operating hierarchies single tasks (Knight, Rabideau, & Chien,
2000). Thus, immediately clear additional improvements scheduler could obtained
using summary information. show improvements significant, first must
provide background aggregation.
Moving tasks central scheduling operation iterative repair planners. planner
effectively schedule tasks moving related groups tasks preserve constraints among them.
Hierarchical task representations common way representing groups constraints. Aggregation involves moving fully detailed abstract task hierarchy preserving
temporal ordering constraints among subtasks. Moving individual tasks independently
parent, siblings, subtasks shown much less efficient (Knight et al., 2000). Valid placements task hierarchy schedule computed state resource usage profiles
hierarchy tasks context movement. hierarchys profile represents one instantiation decomposition temporal ordering abstract task
hierarchy.
Consider schedule n task hierarchies maximum branching factor b expanded
maximum depth shown Figure 19. Suppose hierarchy c constraints v
variables (states metric resources). move hierarchy tasks using aggregation, scheduler

487

fiC LEMENT, URFEE , & BARRETT

must compute valid intervals resource variable affected hierarchy.7 scheduler
intersects intervals get valid placements abstract tasks children.
complexity computing set valid intervals resource O(cC) c number
constraints (usages) abstract task children variable, C number
constraints tasks schedule variable (Knight et al., 2000). n similar
task hierarchies entire schedule, C = (n 1)c, complexity computing valid
intervals O(nc2 ). computation done v resource variables (often constant
domain), moving task complexity O(vnc2 ). intersection valid intervals
across variables increase complexity. complexity O(tnr)
nr valid intervals timeline; intersecting intervals pair timelines linear
number intervals; 1 pairs timelines need intersected get intersection
set.
summary information abstract task represents constraints children,
children share constraints resource, information collapsed single
summary resource usage abstract task. Therefore, moving abstract task, number
different constraints involved may far fewer depending domain. scheduler
trying place summarized abstract task among summarized tasks, computation valid
placement intervals greatly reduced c O(vnc2 ) smaller. consider
two extreme cases constraints fully collapsed cannot collapsed
all.
case tasks hierarchy constraints variable, number
constraints hierarchy O(bd ) hierarchy depth branching factor (number child
tasks per parent) b. aggregation, hierarchies fully detailed first, means
complexity moving task O(vnb2d ) c = O(bd ). consider using aggregation
moving partially expanded hierarchy leaves summarized abstract tasks.
hierarchies schedule decomposed level i, O(bi ) tasks hierarchy,
one summarized constraint representing yet undetailed subtasks beneath
constraint variable. c = O(bi ), complexity moving task O(vnb2i ). Thus,
moving abstract task using summary information factor O(b2(di) ) times faster
aggregation. worst case number conflicts increases number plan
steps (just refinement planner), worst case complexity resolving conflicts based

number plan steps level O(kb ). Thus (as refinement planning) using summary
di
information make speedups O(kb b2(di) ) summary information fully collapses.
extreme tasks place constraints different variables. case,
c = 1 hierarchy one constraint per variable. Fully detailed hierarchies
contain v = O(bd ) different variables, complexity moving task case O(nbd ).
moving summarized abstract task tasks schedule decomposed level i, v
abstract task summarizes constraints subtask hierarchy
beneath it, constraints different variables constraints combine
summarized. Thus, complexity moving partially expanded hierarchy
fully expanded one. case, number conflicts change depth
hierarchy conflicts always pairs n hierarchies. So,
7. analysis applies state constraints, restrict discussion resource usage constraints simplicity.

488

fiA BSTRACT R EASONING P LANNING C OORDINATION

extreme case, summary information reduce complexity scheduling would
incur unnecessary overhead.
complexity analyses shown different forms hierarchical problem solving,
need backtrack lower higher levels interacting subproblems, reduce size search space exponential factor (Korf, 1987; Knoblock, 1991).
planner scheduler using summary information witness exponential improvements without
assumption. Backtracking across abstraction levels occurs within planner/coordinator described Section 5.1 current search state MightSomeWay another subplan
higher level selected. demonstrated search space grows doubly
exponentially hierarchy number plans grows exponentially, resolving
conflicts grows exponentially number plans. Thus, long planner coordinator fully expand abstract plans primitive level summary information


merges higher levels, search complexity reduced least factor kb b
level search completed, depth hierarchy. Yang (1997) suggests
ways exponential speedups obtained subplans interact based hierarchy structure.
speedups complementary summary information limits decomposition
task hierarchies compresses information manipulated planner scheduler.

7. Experiments
experimentally evaluate use summary information planning coordination
three different domains: evacuation domain, manufacturing domain described Section 1.1,
multi-rover domain. domains, define performance different ways show
range benefits abstract reasoning offers.
evaluate algorithm described Section 5.1. implementation orders search states
queue generated synchronization operators precede generated
expansion selection operators. Thus, going deeper part hierarchy, implementation algorithm explores orderings agents plans digging deeper
hierarchy. Investigating heuristics choosing synchronization decomposition
operators topic future research.
next section report experiments evacuation domain show abstract
reasoning using summary information find optimal coordination solutions quickly
conventional search strategies. Optimal solutions evacuation domain minimal global execution times evacuees must transported safety quickly possible. Section 7.2,
show summary information improves local search performance significantly tasks
within hierarchy constraints resource, solutions found
level abstraction. evaluate benefits using CFTF EMTF heuristics
iterative repair show summary information slow search.
domains, computation time may insignificant communication costs. costs
could terms privacy self-interested agents, security sensitive information could
obtained malicious agents, simply communication delay. Section 7.3, show multilevel coordination fails reduce communication delay manufacturing domain example but,
domains, expected reduce communication overhead exponentially.

489

fiC LEMENT, URFEE , & BARRETT

1
s0

2

3

0

s3
t2

t1
4

5

Figure 20: Evacuation problem
7.1 Coordinated Planning Experiments
section, describe experiments evaluate use summary information coordinating group evacuation transports must together retrieve evacuees number locations
constraints routes. comparing EMTF CFTF search techniques described Section 5.2 conventional HTN approaches, experiments show reasoning summary
information finds optimally coordinated plans much quickly prior HTN techniques.
compare different techniques ordering expansion subplans
plans direct decomposition plan hierarchies search optimal solutions.
expansion techniques expand (for subplans) select (for subplans) operators
algorithm described Section 5.1.
compare EMTFs expansion plans ExCon heuristic random selection
heuristic. ExCon heuristic (Tsuneto et al., 1998) first selects plans achieve external
precondition, plans, selects one threatens external precondition.
case neither achieving threatening plans, chooses randomly. Note
EMTF additionally choose expand plans threatened external preconditions
preference whether plan achieves, threatens, threatened. expansion
plans, compare CFTF depth-first (DFS) random heuristic.
compare combination CFTF EMTF FAF (fewest alternatives first)
heuristic combination DFS ExCon. FAF heuristic employ summary
information rather chooses expand select plans fewest subplans
(Currie & Tate, 1991; Tsuneto, Hendler, & Nau, 1997). Since summary information used,
threats resolved primitive levels. shown FAF heuristic
effectively used HTN planner (Tsuneto et al., 1997), combination DFS ExCon
shown make great improvements FAF domain task interactions
(Tsuneto et al., 1998). show one domain CFTF EMTF heuristics together
outperform combinations FAF, DFS, ExCon.
problems generated evacuation domain transports responsible
visiting certain locations along restricted routes pick evacuees bring back safety
points. Transports allowed location time, coordinator must
ensure transports avoid collisions along single lane routes. addition, order avoid
risk oncoming danger (from typhoon enemy attack), transports must accomplish
goals quickly possible.
Suppose two transports, t1 t2, located safety points s0 s3 respectively,
must visit locations 0, 1, 2 2, 3, 4 respectively bring evacuees back safe
490

fiA BSTRACT R EASONING P LANNING C OORDINATION

evacuate
move s0-0

make rounds

one switch

switch
clockwise

first route

cw0-1

second route

cw0-2

move 0-1

counterclockwise

ccw1-2

ccw2-0

go back

ccw2-0

goto safe loc

move

move 0-s0

move 3-s3

move 1-2

Figure 21: plan hierarchy transport t1
locations shown Figure 20. overlap locations must visit, coordinator
must synchronize actions order avoid collision. coordinators goal network includes
two unordered tasks, one transport evacuate locations responsible.
shown Figure 21, high-level task t1 (evacuate) decomposes primitive action
moving location 0 ring abstract plan traverse ring (make rounds). t1
travel one direction around ring without switching directions, switch directions once.
t1 either go clockwise counterclockwise and, switching, switch directions
location ( f irst route) travel farthest location needs visit switched
(second route). visited locations, continues around reaches first
safety point path (go back goto sa f e loc). move plan case t1
already location 0. task t2 refined similarly.
Suppose coordinator gathers summary information plan hierarchy attempts
resolve conflicts. Looking summary information one level top, coordinator
determine t1 finishes evacuating t2 even begins, conflicts
since external conditions t1s evacuate plan none routes traversed.
solution makespan (total completion time) 16 steps. optimal solution plan
duration seven t1 moves clockwise reaches location s3, t2 starts clockwise,
switches directions location 4, winds s0. solution t1 waits location 2
one time step avoid collision route location 2 location 3.
generated problems four, six, eight, twelve locations; two, three four
transports; no, some, complete overlap locations transports visit. Performance measured number search states expanded find optimal solution (if
compared heuristics find optimal solution) number states expanded
find solutions highest common quality within memory time bounds. chose instead CPU time measure performance order avoid fairness issues respect
implementation details various approaches.

491

fiC LEMENT, URFEE , & BARRETT

Search States Expanded
100000

CFTF-RAND

10000
1000
100
10
1
1

10

100

1000 10000 1E+05

CFTF-EMTF

Figure 22: Comparing EMTF random expansion searching optimal solutions

Figure 23: Comparing EMTF ExCon searching optimal solutions
scatter plot Figure 22 shows relative performance combination CFTF
combination CFTF random expansion (CFTF-Rand).
chose scatterplots compare results capture results simply trying
plot three dimensions problem size/complexity. Note scatter plots, axes
scaled logarithmically. Points diagonal line mean EMTF (x-axis) performing better
Rand (y-axis) fewer search states required find optimal solution.
performance similar problems, cases CFTF-EMTF outperformed
CFTF -Rand order magnitude more. Figure 23 exhibits similar effect CFTF - EMTF
CFTF-ExCon. Note runs terminated expansion 3,500 search states. Data
points 3,500 (the ones forming horizontal line top) indicate solution found
within memory time constraints. performance similar problems, four
points along top CFTF-ExCon finds solution. Thus, although EMTF greatly
EMTF ( CFTF - EMTF )

492

fiA BSTRACT R EASONING P LANNING C OORDINATION

Figure 24: Comparing CFTF DFS searching optimal solutions
improve performance many problems, rarely performs much worse, almost always avoids
getting stuck fruitless areas search space compared ExCon random heuristic.
expected since EMTF focuses resolving conflicts among problematic plans
first avoids spending lot time reasoning details less problematic plans.
combination CFTF EMTF, pruning inconsistent abstract plan spaces, branchand-bound pruning costly abstract plan spaces (all described Section 5.2) much
dramatically outperforms techniques reason abstract levels. Figure 24 shows DFSRand expanding one three orders magnitude states CFTF-Rand. Runs
terminated expansion 25,000 search states. Data points 25,000 (forming
horizontal line top) indicate solution found within memory time constraints.
avoiding search spaces greater numbers conflicts, CFTF finds optimal near-optimal solutions much quickly. Figures 25 26, CFTF-EMTF outperforms FAF-FAF (FAF
selecting plans) DFS-ExCon one two orders magnitude problems.
last two comparisons especially emphasize importance abstract reasoning finding
optimal solutions. Within maximum 3,500 expanded search states (the lowest cutoff point
experiments), CFTF-EMTF CFTF-Rand found optimal solutions 13 24 problems.
CFTF -ExCon FAF- FAF found 12; DFS -ExCon DFS -Rand found three.
surprising result FAF-FAF performs much better DFS-ExCon evacuation
problems contrary results given Tsuneto et al. (1998) show DFS-ExCon dominating
problems goal interactions. believe result reproduced
experiments involved hierarchies plans. experiments show
selection subplans greatly affects performance order subplans expand.
So, believe DFS-ExCon performed worse FAF-FAF FAF better choosing
subplans ExCon FAF stronger selecting subplans DFS.
However, main point section heuristic combinations use summary information find solutions prune search space abstract levels (CFTF-EMTF, CFTFExCon, CFTF-Rand) greatly outperform (FAF-FAF, DFS-ExCon,
DFS -Rand) searching optimal solutions.

493

fiC LEMENT, URFEE , & BARRETT

Figure 25: Comparing use summary information FAF heuristic

Figure 26: Comparing use summary information algorithm using external conditions
7.2 Scheduling Experiments
experiments describe show summary information improves performance significantly tasks within hierarchy constraints resource, solutions
found level abstraction. time, cases abstract reasoning
incurs significant overhead solutions found deeper levels. However, domains
decomposition choices critical, show overhead insignificant
CFTF heuristic chooses decompositions quickly lead solutions deeper levels.
experiments show EMTF heuristic outperforms simpler heuristic depending
decomposition rate, raising new research questions. use ASPEN Planning System (Chien
et al., 2000b) coordinate rover team problem described next.

494

fiA BSTRACT R EASONING P LANNING C OORDINATION

Figure 27: Randomly generated rectangular field triangulated waypoints

Figure 28: Randomly generated waypoints along corridors
7.2.1 P ROBLEM OMAINS
domain involves team rovers must resolve conflicts shared resources. generate
two classes maps within rovers move. one, randomly generate map triangulated waypoints (Figure 27). other, generate corridor paths circle locations
three paths center points circle represent narrow paths around obstacles
(Figure 28). corridor map used evaluate CFTF heuristic. select subset
points science locations (where rovers study rocks/soil) use simple multiple traveling salesman algorithm assign routes rovers traverse perform experiments.
idea map area around lander constructed image taken upon landing
Mars.
Paths waypoints assigned random capacities either one, two, three
rovers traverse path simultaneously. one rover waypoint, rovers may
traverse paths opposite directions time. constraints modeled metric
resources. State variables used ensure rovers locations
leave. addition, rovers must communicate lander telemetry using shared channel
fixed bandwidth (metric resource). Depending terrain waypoints, required
bandwidth varies. 80 problems generated two five rovers, three six science locations
per rover, 9 105 waypoints. general, problems contain fewer waypoints
science goals difficult interactions among rovers.
Schedules consist abstract task rover decomposition tasks
visiting assigned science location. tasks decomposition three shortest
paths waypoints target science location. paths decomposition
movements waypoints. Additional levels hierarchy introduced longer paths
order keep offline resource summarization tractable. Schedules ranged 180 1300
tasks.

495

fiC LEMENT, URFEE , & BARRETT

7.2.2 E MPIRICAL R ESULTS ARS ROVERS
compare ASPEN using aggregation without summarization three variations
rectangular field domain. using summary information, ASPEN uses EMTF CFTF
decomposition heuristics. One domain excludes communications channel resource (no channel);
one excludes path capacity restrictions (channel only); excludes neither (mixed).
Since movement tasks reserve channel resource, greater improvement performance
expected using summary information according complexity analyses Section 6.3.
constraints channel resource collapse summary information derived
higher levels task hierarchy one constraint resource.
ASPEN use summary information, hierarchies must fully expanded, number
constraints channel resource equivalent number leaf movement tasks.
However, tasks within rovers hierarchy rarely place constraints path variables
once, channel domain corresponds worst case summarization
collapses constraints. complexity moving abstract task without summary information fully expanded hierarchy summary information partially
expanded hierarchy.
Figure 29 (top) exhibits two distributions problems channel domain.
cases (points x=y diagonal), ASPEN summary information finds solution quickly
level abstraction. However, many cases, summary information performs notably worse
(points x=y diagonal). discovered problems finding solution requires
planner dig deeply rovers hierarchies, decomposes hierarchies
level solution, difference additional time find solution two
approaches negligible (unless use summary information found solution slightly higher
level abstraction quickly). Thus, time spent reasoning summary information
higher levels incurred unnecessary overhead.
worst case analysis Section 6.3 showed summary information advantage even found abstract solutions. So, summary information perform
better abstract solutions found? CFTF heuristic since branch
choices result small differences numbers conflicts. actually results stochastic nature ASPENs iterative repair. Although moving abstract tasks using aggregation without
summary information would enabled ASPEN find solutions quickly fully expanded
hierarchies, ASPEN must sometimes move lower level tasks independently parents siblings order resolve conflicts lower levels. problem ASPEN heuristic tell
level needs move activities, sometimes chooses move activities detailed
levels unnecessarily. search lower levels search space explodes. Using summary
information search higher levels lower levels abstraction better protects ASPEN
unnecessary search.
Figure 29 (middle) shows significant improvement summary information mixed domain compared channel domain. Adding channel resource rarely affects use
summary information collapse summary constraints incurs insignificant additional
complexity. However, channel resource makes scheduling task noticeably difficult
ASPEN using summary information. channel domain (Figure 29 bottom), summary information finds solutions abstract level almost immediately, problems still
complicated ASPEN use summary information. results support complexity

496

fiA BSTRACT R EASONING P LANNING C OORDINATION

a)

b)

c)
Figure 29: Plots a) channel, b) mixed, c) channel domains
analysis Section 6.3 argues summary information exponentially improves performance
tasks within hierarchy constraints resource solutions
found level abstraction.
summary information generated offline, domain modeler knows front whether
constraints significantly collapsed. Thus, obvious approach avoiding cases
reasoning summary information causes unnecessary overhead fully expand start
scheduling hierarchies tasks summary information collapse.
complexity moving task hierarchy case whether fully expanded not, ASPEN
waste time duplicating efforts level expansion reaching level
finds solution. Evaluating approach subject future work.
Earlier mentioned CFTF heuristic effective rectangular field problems.
choice among different paths science location usually make

497

fiC LEMENT, URFEE , & BARRETT

Figure 30: Performance using CFTF heuristic

significant difference number conflicts encounteredif rovers cross paths, path
choices usually still lead conflict. set corridor problems, path choices always lead
different corridor get target location, usually path avoids conflict
path causes one depending path choices rovers. ASPEN uses
CFTF heuristic, performance dominates chooses decompositions randomly
two problems (Figure 30). reflects experiments coordination algorithm Section 7
show CFTF crucial reducing search time required find solutions.
order evaluate EMTF heuristic iterative repair planning, compared simple
alternative. alternative strategy (that refer level decomposition) interleave repair
decomposition separate steps. Step 1) planner repairs current schedule
number conflicts cannot reduced. Step 2) decomposes abstract tasks one level
returns Step 1. spending enough time particular level expansion appears
effective, planner attempts find highest decomposition level solutions exist without
wasting time level. time spent searching solution level expansion
controlled rate abstract tasks decomposed. EMTF heuristic implemented
repair method give priority detailing plans involved conflicts.
Figure 31 shows performance EMTF vs. level decomposition different rates decomposition three problems set varied performance. plotted points averages
ten runs problem. Depending choice rate decomposition (the probability
task decompose conflict encountered), performance varies significantly. However, best decomposition rate vary problem problem making potentially difficult
domain expert choose. example, problem figure, tested decomposition
rates EMTF outperformed use level decomposition. time, problem C using
either decomposition technique make significant difference problem B choosing
rate EMTF made big difference whether use EMTF level decomposition. Although
examples show varied performance, results problems showed decompo-

498

fiA BSTRACT R EASONING P LANNING C OORDINATION

1200


level-decomp
1000

B
B level decomp
C

CPU seconds

800

C level decomp
600

400

200

0
0

5

10

15

20

25

30

35

EMTF Decomposition Rate

Figure 31: Performance EMTF vs. level-decomposition heuristics
sition rate around 15% successful. suggests domain modeler may able
choose generally successful decomposition rate running performance experiments set
example problems.8
demonstrated many results complexity analyses Section 6. Scheduling
summary information gains speedups (over aggregation) resolving conflicts appropriate levels abstraction. summary information collapses, scheduler gains exponential
speedups. addition, CFTF heuristic enables exponential speedups decomposition
choices varying numbers conflicts.
7.3 Communication Overhead
show that, depending bandwidth, latency, summary information communicated among agents, delays due communication overhead vary. communication costs
concern, one extreme message delay dominates cost, sending plan hierarchy
without summary information makes sense. extreme bandwidth costs
dominate, makes sense send summary information task separate message
requested. Still, cases sending summary information tasks groups
makes sense. section explain system designer choose much
summary information send time order reduce communication overhead exponentially.
Consider simple protocol agents request coordination central coordinating agent.
search feasible solution, whenever decomposes task, coordinator requests
summary information subtasks yet received. manufacturing domain,
coordinator may already summary information task move part, encounters
different instantiation task schema, still must request parameters new task.
coordinator needs subplans plan, client agent sends required information
subplans, specifying preferences each. coordinator chooses preferred
8. experiments, used decomposition rate 20% since seemed work well.

499

fiC LEMENT, URFEE , & BARRETT

a)

b)

Figure 32: Delay communicating different granularities summary information varying a)
latency b) bandwidth

subplan, case must backtrack, chooses next preferred subplan.
coordinator finds feasible solution, sends modifications agent specifying subplans blocked agent must send wait synchronization messages. agent
choose send summary information number levels expansion requested
tasks hierarchy.
manufacturing problem described Section 1.1, communication data terms numbers messages size collected point coordinator found
solution Figure 13c. data collected cases agents sent summary information
tasks hierarchies, one time, two levels time, once. two levels
include requested task immediate subplans. following table summarizes
numbers total sizes messages sent granularity level information:

one task time
two levels time


number messages
9
4
3

total size (bytes)
8708
10525
16268

Assuming coordinator must wait requested information continuing search
request one task one agent time, coordination delayed amount
time depending bandwidth latency message passing. total delay calculated (n 2)` + s/b, n number messages sent; ` latency seconds;
total size messages; b bandwidth bytes per second. use n 2 instead n
assume agents transmit first top-level summary information message
time, three messages actually incur delay ` instead 3`.
Figure 32a shows communication delay varies three granularities information
fixed bandwidth 100 bytes/second. (We address lack realism example
shortly.) latency less 3 seconds, sending summary information task
separate messages results smallest communication overhead. latencies greater 58
seconds, sending entire hierarchy best; sending summary information two
levels time best. latency fixed 100 seconds, communication delay varies
500

fiA BSTRACT R EASONING P LANNING C OORDINATION

a)

b)

Figure 33: Delay varying a) latency b) bandwidth hypothetical example
bandwidth shown Figure 32b. bandwidth less 3 bytes/second, sending
one time best; sending best bandwidths greater 60 bytes/second;
sending two levels time best bandwidths between.
Admittedly, values unrealistic manufacturing domain. manufacturing problem simple provided mainly interesting domain coordination. realistic problems involving manufacturing domain could much larger hierarchies require
much larger scales data sent. case realistic bandwidth latency values would
exhibit similar tradeoffs.
see this, suppose manufacturing managers hierarchies common branching
factor b depth d. tasks generally reservations similar resources throughout hierarchies, amount total summary information particular level would grow exponentially
hierarchy would number tasks. agents agreed feasible solution
depth level hierarchy, table messages size would appear follows:

one task time
two levels time


number messages
O(bi )
3i/2
3

total size
O(bi )
O(bi )
O(bd )

suppose branching factor b 3; depth 10; solution found level
= 5; summary information task 1 Kbyte. table would look this:

one task time
two levels time


number messages
363
9
3

total size (Kbytes)
1089
3276
246033

Now, fixed bandwidth 100 Kbyte/second varied latency, realistic
tradeoffs seen Figure 33a. Here, see unless latency small, sending summary
information two levels time best. shown Figure 33b, fix latency one second
vary bandwidth, realistic bandwidths sending summary information two levels
time best.

501

fiC LEMENT, URFEE , & BARRETT

simple protocol illustrates communication minimized sending summary
information particular granularity. agents chose send summary information
unsummarized hierarchies instead, would need send entire hierarchies.
experiment shows, hierarchies grow large, sending entire hierarchy (all once) would take
long time, even high bandwidth. Thus, using summary information (as opposed using
it) reduce communication exponentially solutions found abstract levels.
extreme, agents sent summary information one task time, latency
sending many messages grow large larger task hierarchies. solutions could
found primitive levels, sending summary information one task time would cause
exponential latency overhead compared sending entire hierarchy once. But, solutions
found intermediate levels, able send summary information intermediate
granularity minimize total delay.
However, argument assumes summary information collapses higher levels hierarchy. Otherwise, sending summary information intermediate level could almost
expensive sending entire hierarchy cause unnecessary overhead. actual manufacturing domain, tasks agents hierarchies mostly constraints different resources,
summarization able reduce summary information significantly constraints
collapse. result better, case, send entire hierarchy minimize
delay (unless unusual bandwidth latency constraints, shown experiment).
Even so, coordination agent still summarize hierarchies take advantage
computational advantages abstract reasoning.
section showed domain modeler minimize communication overhead communicating summary information proper level granularity. bandwidth, latency,
common depth coordination solutions known, domain modeler perform hypothetical
experiment one varying granularities summary information determine
granularity optimal. summary information collapses hierarchy, solutions
found intermediate levels, communication exponentially reduced manner.

8. Related Work
approach taken abstract reasoning originally inspired earlier work involving
hierarchical behavior-space search agents represent planned behaviors multiple
levels abstraction (Durfee & Montgomery, 1991). Distributed protocols used decide
level abstraction coordination needed resolve conflicts there. approach capitalizes
domains resources abstracted naturally. earlier work viewed
limited, special case work presented here. justified intuitively limited
experiments analyses.
Corkill studied interleaved planning merging distributed version NOAH planner
(1979). recognized that, conditions affected abstract plan operator
might unknown refinement, deal overall effects preconditions
hold matter operator refined captured used identify resolve
conflicts. recognized choices refinement synchronization choices
abstract levels could lead unresolvable conflicts deeper levels, backtracking could
necessary. work directed toward avoiding backtracking using summary information
guide search.

502

fiA BSTRACT R EASONING P LANNING C OORDINATION

closer relation approach, Pappachan shows interleave hierarchical plan coordination plan execution cooperative agents using online iterative constraint relaxation
(OICR) algorithm (Pappachan, 2001). approach, coordination achieved higher
levels abstraction flexible execution, agents decompose tasks lower
levels tighter coordination improve plan quality. OICR approach tailored toward
interleaving coordination flexible execution price completeness coordination
algorithm presented aimed complete interleaved coordination planning price
potentially delaying execution due backtracking.
planning research, hierarchical plans often represented Hierarchical Task Networks (HTNs, Erol et al., 1994a), planners NOAH (Sacerdoti, 1977), NonLin (Tate,
1977), SIPE-2 (Wilkins, 1990), O-Plan (Currie & Tate, 1991), UMCP (Erol et al., 1994b), SHOP 2
(Nau, Au, Ilghami, Kuter, Murdock, Wu, & Yaman, 2003) use search combinations
alternative courses action achieve goals within particular context. actions may partially ordered, giving timing flexibility execution (Wilkins, 1990; Currie & Tate, 1991).
CH P representation extends HTN include temporal extent partial orderings expressed
constraints starting ending timepoints action.
Yang presented method (similar summarization) preprocessing plan hierarchy
order able detect unresolvable conflicts abstract level planner could backtrack inconsistent search spaces (Yang, 1990). corresponds use MightSomeWay
Section 5.2. However, approach requires decomposition hierarchy modeled
abstract operator unique main subaction preconditions effects
parent. avoid restriction analyzing subplans conditions ordering constraints
automatically compute parents summary conditions.
approach focused resolving conflicts among agents, Cox Durfee (2003)
used summary information exploit synergistic interactions. idea using summary information identify overlapping effects help agents skip actions whose effects
achieved others. Thangarajah, Padgham, Winikoff (2003) used summary information
rescheduling execution. representations actually subsumed ours,
work significantly postdates first reporting work paper (Clement & Durfee, 1999).
DSIPE (desJardins & Wolverton, 1999) distributed version SIPE -2 (Wilkins, 1990)
hierarchical planning system. way agents use summary information reduce
communication states common constraints, DSIPE filters conditions
communicated among planners using irrelevance reasoning (Wolverton & desJardins, 1998).
DPOCL (Decompositional Partial-Order Causal-Link) planner (Young et al., 1994) adds
action decomposition SNLP (McAllester & Rosenblitt, 1991). HTN planners, preconditions high level effects added abstract tasks order help planner resolve
conflicts decomposition. addition, causal links specified decomposition schemas
isolate external preconditions DPOCL must satisfy. However, conditions
causal links necessarily capture external conditions abstract tasks, planner
find solutions abstract levels requires tasks completely decomposed. addition, DPOCL cannot determine abstract plan unresolvable conflicts (MightSomeWay)
may effects hidden decompositions yet undetailed tasks could achieve
open preconditions. deriving summary conditions automatically using algorithms determining causal link information (e.g. must-achieve), planning/coordination algorithm find

503

fiC LEMENT, URFEE , & BARRETT

reject abstract plans search without adding burden domain expert specify redundant conditions causal links abstract tasks.
DPOCL, TMS (a framework Task Analysis, Environment Modeling, Simulation)
allows domain modeler specify wide range task relationships (Decker, 1995).
work offers quantitative methods analyzing simulating agents well interactions.
interactions represented discovered using summary conditions,
discover information analysis rather depending model developer
predefine interactions.
Groszs shared plans model collaboration (1996) presents theory modeling multiagent
belief intention. shared plans work directed toward cooperative agents, represents
action hierarchies provides mental models higher level represented article.
However, use analysis summary information complements Groszs work providing
way automatically represent efficiently reason intentions agents multiple levels
abstraction. Future work needed understand summary information bridged
mental states agents exploit techniques employed shared plans work based
BDI (belief-desire-intention) models agents (Rao & Georgeff, 1995).
analysis hierarchical planning (Yang, 1997) explains that, case interacting subgoals, certain structures hierarchy minimize interactions reduce worst case
planning complexity exponentially. However, complexity analyses Section 6 explain using summary information achieve exponential performance gains addition achieved
restructuring plan hierarchies according Yangs analysis limiting decomposition task
hierarchies compressing information manipulated coordinator, planner, scheduler.
SHOP 2 (Nau et al., 2003) HTN planner uses domain translation technique reason
durative action. however express temporal extent way planner
given here. model differs supports ordering relationships endpoints well
conditions effects actions execution. may domain translation
could achieve expression similar constraints solutions systems,
formal model expressions HTN planning.
SIADEX (Castillo et al., 2006) another HTN planner handles temporal extent use
expressive simple temporal networks (Dechter, Meiri, & Pearl, 1991). performance
improvement techniques reported SIADEX temporal reasoning specific HTNs.
Thus, work complementary ours. However, work needed understand
summary information exploited conjunction forward expansion approach
SHOP 2 SIADEX use perform competitively planning competition problems.
Another class hierarchical planners based ABSTRIPS (Sacerdoti, 1974) introduces conditions different levels abstraction critical conflicts handled higher levels
abstraction less important (or easier) conflicts resolved later lower levels.
approach similarly resolves conflicts abstract levels, planning decisions may consistent
conditions lower levels resulting backtracking. Summary information provides means
make sound complete decisions abstract levels without need decompose check
consistency lower levels. However, resolving conflicts based criticality still improve
performance complement approach.
Allens temporal planner (1991) uses hierarchical representations tasks could applied
reasoning concurrent actions multiple agents. However, exploit hierarchy
reasoning abstraction levels separately generates plan proving consistency
504

fiA BSTRACT R EASONING P LANNING C OORDINATION

collective constraints. Allens model temporal plans (1983) subsequent work interval
point algebra (Vilain & Kautz, 1986) strongly influenced hierarchical task representation
algorithms reason them.
many, many models theories concurrency. older examples include
automata representations, Petri nets Hoares theory communicating sequential processes
(Glabbeek, 1997). many temporal logics computational tree logic (CTL,
Emerson & Halpern, 1985) allow modal expressions proposition holding
possible worlds time, time, next state, eventually,
proposition holds. Another language specifying manufacturing processes process
standardized 10 years (Bock, 1996; Schlenoff, Knutilla, & Ray, 2006). Many
logics could used define summary conditions relations MightSomeWay. However, found logics awkward representing inconditions defining summary
conditions terminology used article simplifies definitions.
Model checking uses temporal logics verify different properties system models, software,
hardware (such correctness, deadlock-free, convergence). fact, model checking
planning algorithms used interchangeably problems (e.g., Giunchiglia &
Traverso, 1999). context model checking, summary information set properties
(akin specifiable CTL) system model (as planning domain) summarize system
variable requirements (conditions) assignments (effects). Thus, model checking algorithm
could use summary information efficiently identify resolve potential requirement violations/bugs (condition conflicts) deadlock (resource conflicts) system model operation
(planning/scheduling problem instantiations).

9. Conclusion
article provides formalization Hierarchical Task Network planning that, unlike UMCP
formalism (Erol et al., 1994b), includes actions temporal extent. introduce sound
complete algorithm used generate plan, coordinate group agents hierarchical plans, interleave planning coordination.
algorithms summarizing propositional state metric resource conditions effects
abstract levels mechanisms reason summary information facilitate
construction planning coordination systems reason plans multiple levels
abstraction. mechanisms reasoning summary information determine whether
task (at level abstraction) must may achieve, clobber, undo condition another task
partial order constraints endpoints tasks. Built mechanisms, mechanisms
determine whether group agents decompose execute set partially ordered abstract
tasks way (CanAnyWay), might decompose execute way (MightSomeWay),
cannot execute consistently way (MightSomeWay).
algorithms enable planning system find solutions multiple levels abstraction
without needing fully detail task hierarchy. abstract solutions support flexible execution
remaining uncommitted alternative methods selected runtime, based
circumstances, achieve plan subgoals.
complexity analyses experiments different problem domains quantified benefits using summary information refinement planning local search scheduling algorithm.


potential doubly exponential speedup O(kb b b2(di) ) k ways resolve conflict,

505

fiC LEMENT, URFEE , & BARRETT

hierarchy branching factor b, depth hierarchy d, abstract solution depth i. exponential speedup obtained abstract solutions found, fewer summary conditions
abstract levels, alternative decomposition choices lead varying numbers threats.
conditions exponential improvement significant relaxation compared prior work,
performance improvement greater.
domain modeler run summarization algorithms offline library plan hierarchies
summary information available coordination planning set goal tasks
supported library. Using algorithms reasoning summary information, agents
discover coordinate states resources must coordinate/negotiate. Communicating summary information different levels abstraction reduces
communication costs exponentially conditions similar reducing computation time.
use summary information local search planner (like ASPEN, Section 6.3) another
contribution work. strength local search algorithms ability efficiently reason
large numbers tasks constraints metric resources, state variables, complex
resource classes. integrating algorithms reasoning summarized propositional state
metric resource constraints heuristic local search planner/scheduler, enable scalable
planning systems scale even larger problem domains. use summary information
different style planner demonstrates applicability abstract reasoning improving
performance different kinds planning (and plan coordination) systems.
Future work needed evaluate use summary information planning
scheduling systems wider classes problems requiring expressive representations
resources temporal constraints. Already, approach exploiting cooperative action
among agents based summary information developed (Cox & Durfee, 2003).
promising approaches include abstracting plan information, probabilistic conditions
effects classes resources states (e.g. location regions sub-regions). work
needed understand communicate summary information distributed
planning system.

Acknowledgments
authors wish thank Pradeep Pappachan, Gregg Rabideau, Russell Knight help
implementation. thank anonymous reviewers many valuable suggestions.
work performed Jet Propulsion Laboratory, California Institute Technology,
contract National Aeronautics Space Administration, University Michigan
supported part DARPA (F30602-98-2-0142).

Appendix A: Algorithms Computing Interval Relations
algorithms determining whether defined relations hold summary conditions
plans P use point algebra constraint table (Vilain & Kautz, 1986). point algebra
table constructed interval endpoints corresponding executions plans P;
row column p ts (e) (start endpoint execution e p) p+ f (e) (finish
endpoint) added plan p P. cell table gives time point constraint
row column <, , =, , >, 6=, <=>, empty. <=> means

506

fiA BSTRACT R EASONING P LANNING C OORDINATION

p
p+
p0
p0 +

p
=
>
>
>

p+
<
=
<
<

p0
<
>
=
>

p0 +
<
>
<
=

Table 1: Point algebra table p contains p0
p
p+
p0
p0 +

p
=
>

>

p+
<
=
<=>
<=>

p0

<=>
=
>

p0 +
<
<=>
<
=

Table 2: Point algebra table p p0
points unconstrained. cell empty, allowed temporal relations, indicating
inconsistency. Table 1 shows point algebra table plans p p0 constrained
ps execution contains p0 . Table 2 shows table start p constrained
earlier start p0 . transitive closures constraint relations. Table 1
computed Table 2 constraining p+ < p0 + (by putting < cell row p+
column p0 + ) computing transitive closure, O(n2 ) algorithm n points (Vilain &
Kautz, 1986). transitive closure computed, constraints point
point looked constant time.
Similarly, constraints order P added table, transitive closure
computed get constraints entailed order. needs done
P order determine achieve clobber relationships defined next section.
determine plan q ps subplans temporally ordered always-[ f irst,last]
[q , q+ ] constrained [before, after] equal points point algebra table
ps subplans. done looking entry row [q , q+ ] checking see
constraint [<, >], =, [, ]. case, q not-always-[ f irst,last].
q always-not-[ f irst,last] row [q , q+ ] entry [>, <]
constraint; otherwise, sometimes-[ f irst,last].
interval i0 covered set intervals = {i1 , i2 , . . . , ik } interval
found intersects i0 intersects nothing I. particular covering problem describes
intervals terms partial order endpoints, represent intervals point algebra
table. algorithm covering problem check see i0 covered looking
pairs intervals see overlap. i0 covered (1) either intervals meet either
+

0 i0 , (2) intervals endpoint contained i0
meet opposite endpoint another interval endpoint i0 , (3) intervals
overlapping i0 . Otherwise, i0 covered. Examples given Figure 34.

507

fiC LEMENT, URFEE , & BARRETT

a)
=

=

B

=
C

c)

E

b)



F
=
G



H



Figure 34: a) Interval covered B, C, D. b) E covered F, G, H. c)
covered.

Appendix B: Algorithms Must/May Asserting Summary Conditions
describe algorithms determining temporal plan relationships based summary information. used build algorithms determine whether plan must may achieve,
clobber, undo condition another particular ordering constraints.
definitions algorithms throughout section given within context set
plans P corresponding set summary information Psum , set ordering constraints order,
set histories H including histories E(h) includes execution e
plan P es subexecutions, E(h) satisfies constraints order. concerned
ordering plan execution intervals timing conditions. themselves,
anything whether conditions may need met must met plan
execution.
First, order determine whether abstract plan executions achieve, clobber, undo
conditions others, agent needs able reason summary conditions asserted
required met. Ultimately, agent needs able determine whether partial ordering
abstract plans succeed, may case agents action fails assert summary
condition required action another agent. Therefore, formalize means
action attempt assert summary condition require summary condition
met. definitions rely linking summary condition plan CHiP conditions
summarizes subplans plans decompositions. Thus, first define means
summary condition summarize conditions.
Definition 14
summary condition c summarizes condition ` condition set conds
plan p iff c added procedure deriving summary information
summary condition set p0 ; ` = `(c); either c added ` condition
set conds p = p0 , c added summary condition subplan p0
summarizes ` conds p.
example, at(bin1, A) precondition start move plan moving part bin1
machine M1 (as given Section 2.2). deriving summary conditions start move,

508

fiA BSTRACT R EASONING P LANNING C OORDINATION

at(bin1, A) added summary preconditions. Thus, summary precondition at(bin1,
A)MuF summarizes at(bin1, A) preconditions start move.
Definition 15
execution e p requires summary condition c met iff c
summary condition ps summary information; condition ` condition
set conds p0 summarized c; f irst(c), = ts (e); last(c), = f (e);
always(c), within (ts (e),t f (e)); sometimes(c), execution
subplan p d(e) requires summary condition c0 met t, c0
summarizes ` conds p0 .
So, basically, execution requires summary condition met whenever conditions
summarizes required. execution build G summary precondition at(A,M1 tray1).
execution requires summary condition met ts (build G) at(A, M1 tray1)
precondition build Gs first subplan summarized build Gs summary precondition.
Definition 16
execution e p attempts assert summary condition c iff c
summary condition ps summary information; condition ` condition
set conds p0 summarized c; f irst(c); always(c), smallest
interval ts (e) start end execution follows ts (e);
last(c), = f (e); sometimes(c), execution subplan p d(e)
attempts assert summary condition c0 t; c0 summarizes ` conds p0 .
say execution attempts assert summary condition asserting condition
fail due simultaneous assertion negation condition. example
requiring summary condition, executions build G, produce G M1, produce H
assert summary postconditions M1 becomes available f (build G).
order agents determine potential interactions among abstract plans (such clobbering achieving), need reason summary condition asserted one plan
relation asserted required another. Based interval point algebra constraints
set abstract plans, agent specifically would need able determine whether plan
would assert summary condition time another plan requires asserts summary
condition state variable. addition, reason clobbering inconditions, agent
would need determine summary condition would asserted time summary incondition c required (asserted c). Agents need detect summary postcondition
would asserted time another summary postcondition c (asserted c).
consider cases executions attempt assert summary in- postcondition
time incondition asserted cases, clobber relations already
detected executions always require summary inconditions attempt assert.
example, equip M1 attempted assert incondition M1 unavailable
time build G attempted assert postcondition M1 available, incondition would
clobbered postcondition.
case ordering constraints allow alternative synchronizations abstract
plans, assertions summary conditions may come different orders. Therefore, formalize
must-assert may-assert determine relationships must may occur respectively.
mentioned beginning Section 9, use must may based disjunctive orderings existence summary conditions different decompositions.
509

fiC LEMENT, URFEE , & BARRETT

1
2
3
4

5
6
7

8
9

10
11

12
13
14
15

16
17
18
19

c0 post(p0 )
last


F
?
c0 in(p0 )
always


F
c0 post(p0 )
last

F
c0 in(p0 )
always

F
c0 post(p0 )
last


F
F
c0 in(p0 )
always


F
F

p0 must-assert c0 c
order must impose
constraints

p0 must-assert c0 c
order must impose
constraints

p0 + p
p0 + p
p0 + p
p0 + p

p0 + < p
p0 + < p
p0 + < p
p0 + < p


F
?
c in(p)
always
?
?

p0 < p
p0 p
f alse

p0 < p
p0 p
f alse

p0 + p
p0 + p

p0 + p
p0 + p

?
?
c post(p)
last

F

F

p0 p
f alse

p0 < p
f alse

p0 + p+
p0 + p
p0 + p+
p0 + p

p0 + < p+
p0 + p
p0 + p+
p0 + p


F

F

p0 < p+
p0 p
f alse
f alse

p0 < p+
p0 p
f alse
f alse

c pre(p)
f irst

F
?
?

Table 3: Table must-assert by/before algorithm
following definitions algorithms must- may-assert, assume c c0 summary
conditions plans P.
Definition 17
p0 P must-assert c0 [by, before] c iff histories h H
e top-level execution E(h) p P requires c met t,
e0 top-level execution p0 E(h), 0 e0 attempts assert c0
0 , [t 0 t, 0 < t].
must-assert algorithm described Table 3. p0 must-assert c0 c iff order entails
relationship given row corresponding type timing two conditions. Rows
table indicate timing summary conditions constraints order must dictate
must-assert true. F table indicate whether timing column true
false condition. ? means timing doesnt matter condition case.
example, row 9 says case c0 sometimes (last) postcondition p0 , c
incondition p timing, order must require end p0 start
p order p0 must-assert c0 time c asserted required.

510

fiA BSTRACT R EASONING P LANNING C OORDINATION

1
2
3
4

5
6

7
8
9
10

11
12

13
14
15
16

17
18

c0 post(p0 )
last


F
F
c0 in(p0 )
always
?
?
c0 post(p0 )
last


F
F
c0 in(p0 )
always
?
?
c0 post(p0 )
last


F
F
c0 in(p0 )
always
?
?

p0 may-assert c0 c
order cannot impose
constraints

p0 may-assert c0 c
order cannot impose
constraints

p0 + > p
p0 + p+
p0 p
p0 p+

p0 + p
p0 + p+
p0 p
p0 p+


F
c in(p)
always

F

F

p0 p
p0 p+

p0 p
p0 p+

p0 + > p
p0 + p+
p0 p
p0 p+

p0 + > p
p0 + p+
p0 p
p0 p+


F
c post(p)
last

F

F

p0 > p
p0 p+

p0 p
p0 p+

p0 + > p+
p0 + p+
p0 p+
p0 p+

p0 + p+
p0 + p+
p0 p+
p0 p+


F

p0 p+
p0 p+

p0 p+
p0 p+

c pre(p)
f irst

F

F

Table 4: Table may-assert by/before algorithm

1
2
3
4

5
6
7
8

c0 post(p0 )
last


F
F
c0 in(p0 )
always


F
F

p0 must-assert c0 c
order must impose
constraints

c in(p)
always

F

F

p0 + > p p0 + < p+
f alse
p0 p p0 + p+
f alse


F

F

p0 p p0 < p+
f alse
f alse
f alse

c0 post(p0 )
last


F
F
c0 in(p0 )
always


F
F

c in(p)
always

F

F

p0 + p
p0 + p
p0 + p
p0 + p






p0 + p+
p0 + p+
p0 p+
p0 p+


F

F

p0 + p
p0 + p
p0 + p
p0 + p






p0 p+
p0 p+
p0 p+
p0 p+

Table 5: Table must/may-assert algorithm

511

p0 may-assert c0 c
order cannot impose
constraints

fiC LEMENT, URFEE , & BARRETT

1
2
3
4

c0 post(p0 )
last


F
F

c post(p)
last

F

F

p0 must-assert c0 c
order must impose
constraints
p0 + = p+
f alse
f alse
f alse

c0 post(p0 )
last


F
F

c post(p)
last

F

F

p0 may-assert c0 c
order cannot impose
constraints
p0 + 6= p+
p p0 + p+
p0 + p+ p0 p+
p0 + p p0 p+
p0 +

Table 6: Table must/may-assert algorithm
definitions algorithms assert relationships similar. Tables 4-6 describe
logic algorithms. may relationships, algorithm returns true iff none
corresponding ordering constraints table imposed (can deduced from) order.
illustrate relationships example Figure 8. Figure 8a agents plans
unordered respect other. Part G produced either machine M1 M2 depending potential decompositions produce G plan. produce G must-assert c0 = must,
last available(G) c = must, f irst available(G) summary preconditions move G
matter plans decomposed (for executions histories plans
ordering constraints figure), execution produce G attempts assert c0 execution move G requires c met. algorithm verifies finding
end produce G ordered start move G (row 1 Table 3). case
equip M2 tool may-assert c0 = must, last available(M2) c = may, sometimes available(M2)
summary preconditions produce G two plans unordered respect
other, history equip M2 tool precede produce G. algorithm finds
true since equip M2 constrained start start produce G (row 2 Table 4).
Figure 8b, move tool may-assert c0 = must, last f ree(transport1) c = may, sometimes
f ree(transport1) produce Gs summary inconditions history move tool attempts assert c0 time produce G using transport1 move part machine
M2. addition, equip M2 tool must-assert c0 = must, last available(M2) c = may, last
available(M2) produce Gs summary postconditions equip M2 tool attempts assert
c0 time produce G requires c met. end Section 3.3 gives examples.

References
Allen, J., Kautz, H., Pelavin, R., & Tenenberg, J. (1991). Reasoning plans. Morgan Kaufmann.
Allen, J. F. (1983). Maintaining knowledge temporal intervals. Communications ACM,
26(11), 832843.
Allen, J. F., & Koomen, J. A. (1983). Planning using temporal world model. Proceedings
International Joint Conference Artificial Intelligence, pp. 741747.
Bock, C. (1996). Unified process specification language: Requirements modeling process. Tech.
rep. NISTIR 5910, National Institute Standards Technology.
Castillo, L., Fdez-Olivares, J., Garca-Perez, O., & Palao, F. (2006). Efficiently handling temporal
knowledge HTN planner. 16th International Conference Automated Planning
512

fiA BSTRACT R EASONING P LANNING C OORDINATION

Scheduling (ICAPS-06), pp. 6372. AAAI.
Chien, S., Knight, R., Stechert, A., Sherwood, R., & Rabideau, G. (2000a). Using iterative repair
improve responsiveness planning scheduling. Proceedings International
Conference AI Planning Scheduling, pp. 300307.
Chien, S., Rabideu, G., Knight, R., Sherwood, R., Engelhardt, B., Mutz, D., Estlin, T., Smith, B.,
Fisher, F., Barrett, T., Stebbins, G., & Tran, D. (2000b). Automating space mission operations
using automated planning scheduling. Proc. SpaceOps.
Clement, B. (2002). Abstract Reasoning Multiagent Coordination Planning. Ph.D. thesis,
University Michigan, Ann Arbor.
Clement, B., & Durfee, E. (1999). Top-down search coordinating hierarchical plans
multiple agents. Proceedings International Conference Autonomous Agents.
Corkill, D. (1979). Hierarchical planning distributed environment. Proceedings
International Joint Conference Artificial Intelligence, pp. 168175.
Cox, J. S., & Durfee, E. H. (2003). Discovering exploiting synergy hierarchical planning agents. Proceedings International Joint Conference Autonomous Agents
MultiAgent Systems, pp. 281288.
Currie, K., & Tate, A. (1991). O-Plan: open planning architecture. Artificial Intelligence, 52,
4986.
Dechter, R., Meiri, I., & Pearl, J. (1991). Temporal constraint networks. Artificial Intelligence, 49,
6195.
Decker, K. (1995). Environment centered analysis design coordination mechanisms. Ph.D.
thesis, University Massachusetts.
desJardins, M., & Wolverton, M. (1999). Coordinating distributed planning system. AI Magazine,
20(4), 4553.
Drabble, B., & Tate, A. (1994). use optimistic pessimistic resource profiles inform
search activity based planner. Artificial Intelligence Planning Systems, pp. 243248.
Durfee, E. H., & Montgomery, T. A. (1991). Coordination distributed search hierarchical
behavior space. IEEE Transactions Systems, Man Cybernetics, 21(6), 13631378.
Emerson, E., & Halpern, J. Y. (1985). Decision procedures expressiveness temporal logic
branching time. Journal Computer System Sciences, 30(1), 124.
Ephrati, E., & Rosenschein, J. (1994). Divide conquer multi-agent planning. Proceedings
National Conference Artificial Intelligence, pp. 375380.
Erol, K., Hendler, J., & Nau, D. (1994a). Semantics hierarchical task-network planning. Tech.
rep. CS-TR-3239, University Maryland.
Erol, K., Nau, D., & Hendler, J. (1994b). UMCP: sound complete planning procedure
hierarchical task-network planning.. Proceedings International Conference AI
Planning Scheduling.
Fagin, R., Halpern, J., Moses, Y., & Vardi, M. (1995). Reasoning knowledge. MIT Press.
Firby, J. (1989). Adaptive Execution Complex Dynamic Domains. Ph.D. thesis, Yale University.
513

fiC LEMENT, URFEE , & BARRETT

Georgeff, M. P. (1983). Communication interaction multiagent planning. Proceedings
National Conference Artificial Intelligence, pp. 125129.
Georgeff, M. P. (1984). theory action multiagent planning. Proceedings National
Conference Artificial Intelligence, pp. 121125.
Georgeff, M. P., & Lansky, A. (1986). Procedural knowledge. Proceedings IEEE, 74(10), 1383
1398.
Giunchiglia, F., & Traverso, P. (1999). Planning model checking. Proceedings 5th
European Conference Planning, pp. 120, London, UK. Springer-Verlag.
Glabbeek, R. v. (1997). Notes methodology CCS CSP. Theoretical Computer Science,
177(2), 329349. Originally appeared Report CS-R8624, CWI, Amsterdam, 1986.
Grosz, B., & Kraus, S. (1996). Collaborative plans complex group action. Artificial Intelligence,
86, 269358.
Huber, M. (1999). JAM: BDI-theoretic mobile agent architecture. Proceedings International Conference Autonomous Agents, pp. 236243.
Knight, R., Rabideau, G., & Chien, S. (2000). Computing valid intervals collections activities shared states resources. Proceedings International Conference AI
Planning Scheduling, pp. 600610.
Knoblock, C. (1991). Search reduction hierarchical problem solving. Proceedings
National Conference Artificial Intelligence, pp. 686691.
Korf, R. (1987). Planning search: quantitative approach. Artificial Intelligence, 33, 6588.
Laborie, P., & Ghallab, M. (1995). Planning sharable resource constraints. Proceedings
International Joint Conference Artificial Intelligence, pp. 16431649.
Lansky, A. (1990). Localized search controlling automated reasoning. Proceedings
DARPA Workshop Innovative Approaches Planning, Scheduling Control, pp. 115
125.
Lee, J., Huber, M. J., Durfee, E. H., & Kenny, P. G. (1994). UMPRS: implementation
procedural reasoning system multirobot applications. Proceedings AIAA/NASA
Conference Intelligent Robotics Field, Factory, Service, Space, pp. 842849.
McAllester, D., & Rosenblitt, D. (1991). Systematic nonlinear planning. Proceedings
National Conference Artificial Intelligence, pp. 634639.
Muscettola, N. (1994). HSTS: Integrating planning scheduling. Intelligent Scheduling, 169212.
Nau, D., Au, T., Ilghami, O., Kuter, U., Murdock, J., Wu, D., & Yaman, F. (2003). SHOP2:
HTN planning system. Journal Artificial Intelligence Research, 20, 379404.
Pappachan, P. (2001). Coordinating Plan Execution Dynamic Multiagent Environments. Ph.D.
thesis, University Michigan, Ann Arbor.
Pratt, V. R. (1976). Semantical considerations floyd-hoare logic. 17th Annual IEEE Symposium Foundations Computer Science, pp. 109121.
Rao, A. S., & Georgeff, M. P. (1995). BDI-agents: theory practice. Proceedings
International Conference Multi-Agent Systems, San Francisco.
514

fiA BSTRACT R EASONING P LANNING C OORDINATION

Sacerdoti, E. (1974). Planning hierarchy abstraction spaces. Artificial Intelligence, 5(2),
115135.
Sacerdoti, E. D. (1977). Structure Plans Behavior. Elsevier-North Holland.
Schlenoff, C., Knutilla, A., & Ray, S. (2006). Interprocess communication process specification language. Tech. rep. NISTIR 7348, National Institute Standards Technology.
Tate, A. (1977). Generating project networks. Proceedings International Joint Conference
Artificial Intelligence, pp. 888893.
Thangarajah, J., Padgham, L., & Winikoff, M. (2003). Detecting & avoiding interference
goals intelligent agents. Proceedings International Joint Conference Artificial
Intelligence, pp. 721726.
Tsuneto, R., Hendler, J., & Nau, D. (1997). Space-size minimization refinement planning.
Proceedings European Conference Planning.
Tsuneto, R., Hendler, J., & Nau, D. (1998). Analyzing external conditions improve efficiency
HTN planning. Proceedings National Conference Artificial Intelligence, pp.
913920.
Vilain, & Kautz, H. (1986). Constraint propagation algorithms temporal reasoning. Proceedings National Conference Artificial Intelligence, pp. 377382.
Weld, D. (1994). introduction least commitment planning. AI Magazine, 15(4), 2761.
Wilkins, D. E. (1990). AI planners solve practical problems?. Computational Intelligence,
6(4), 232246.
Wolverton, M., & desJardins, M. (1998). Controlling communication distributed planning using
irrelevance reasoning. Proceedings National Conference Artificial Intelligence,
pp. 868874.
Yang, Q. (1990). Formalizing planning knowledge hierarchical planning. Computational Intelligence, 6(1), 1224.
Yang, Q. (Ed.). (1997). Intelligent Planning: Decomposition Abstraction Based Approach.
Springer.
Young, M., Pollack, M., & Moore, J. (1994). Decomposition causality partial-order planning.
Proceedings International Conference AI Planning Scheduling, pp. 188193.

515


