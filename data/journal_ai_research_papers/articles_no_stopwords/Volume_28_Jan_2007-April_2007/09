journal artificial intelligence

submitted published

closed loop learning visual control policies
sebastien jodogne
justus h piater

jodogne montefiore ulg ac
justus piater ulg ac

montefiore institute b
university liege b liege belgium

abstract
present general flexible framework learning mappings images actions interacting environment basic idea introduce
feature image classifier front reinforcement learning classifier
partitions visual space according presence absence highly informative local descriptors incrementally selected sequence attempts remove
perceptual aliasing address fighting overfitting greedy
finally high level visual features generated
power local descriptors insufficient completely disambiguating aliased states
done building hierarchy composite features consist recursive spatial
combinations visual features demonstrate efficacy solving
three visual navigation tasks visual version classical car hill control


introduction
designing robotic controllers quickly becomes challenging indeed controllers face huge number possible inputs noisy must select actions among
continuous set able automatically adapt evolving stochastic environmental conditions although real world robotic task often solved
directly connecting perceptual space action space given computational
mechanism mappings usually hard derive hand especially perceptual space contains images evidently automatic methods generating mappings
highly desirable many robots nowadays equipped ccd sensors
interested reactive systems learn couple visual perceptions
actions inside dynamic world act reasonably coupling known
visual control policy wide category called vision action tasks
simply visual tasks despite fifty years years active artificial
intelligence robotic agents still largely unable solve many real world visuomotor
tasks easily performed humans even animals vision action
tasks notably include grasping vision guided navigation manipulation objects
achieve goal article introduces general framework suitable building
image action mappings fully automatic flexible learning protocol
vision action reinforcement learning
strong neuropsychological evidence suggests human beings learn extract useful information visual data interactive fashion without external supervisor gibson
c

ai access foundation rights reserved

fijodogne piater

spelke evaluating consequence actions environment
learn pay attention visual cues behaviorally important solving task
way interact outside world gain expertise
tasks tarr cheng obviously process task driven since different tasks
necessarily need make distinctions schyns rodet
breakthrough modern artificial intelligence would design artificial system
would acquire object scene recognition skills experience
surrounding environment state general terms important
direction would design robotic agent could autonomously acquire visual skills
interactions uncommitted environment order achieve set
goals learning visual skills dynamic task driven fashion complete
priori unknown visual task known purposive vision paradigm aloimonos
one plausible framework learn image action mappings according purposive vision reinforcement learning rl bertsekas tsitsiklis kaelbling littman
moore sutton barto reinforcement learning biologically inspired
computational framework generate nearly optimal control policies automatic
way interacting environment rl founded analysis called
reinforcement signal whenever agent takes decision receives feedback real
number evaluates relevance decision biological perspective
signal becomes positive agent experiences pleasure talk reward
conversely negative reinforcement implies sensation pain corresponds
punishment reinforcement signal arbitrarily delayed actions
responsible rl able map every possible perception
action maximizes reinforcement signal time framework agent
never told optimal action facing given percept whether one
decisions optimal rather agent discover promising
actions constituting representative database interactions understanding influence decisions future reinforcements schematically rl lies
supervised learning external teacher gives correct action agent
unsupervised learning clue goodness action given
rl successful applications example turning computer excellent
backgammon player tesauro solving acrobot control yoshimoto
ishii sato making quadruped robot learn progressively walk without
human intervention huber grupen kimura yamashita kobayashi kohl
stone riding bicycle randlv alstrm lagoudakis parr
controlling helicopter bagnell schneider ng coates diel ganapathi schulte
tse berger liang major advantages rl protocol fully
automatic imposes weak constraints environment
unfortunately standard rl highly sensitive number distinct
percepts well noise sensing process general
often referred bellman curse dimensionality bellman thus high
dimensionality noise inherent images forbid use basic rl
direct closed loop learning image action mappings according purposive vision


ficlosed loop learning visual control policies

achieving purposive vision reinforcement learning
exists variety work rl specific robotic involving perceptual
space contains images instance schaal uses visual feedback solve
pole balancing task rl used control vision guided underwater robotic vehicle wettergreen gaskett zelinsky recently kwok fox
demonstrated applicability rl learning sensing strategies aibo robots
reinforcement learning used learn strategies view selection paletta
pinz sequential attention paletta fritz seifert let us
mention use reinforcement learning vision guided tasks ball kicking asada noda tawaratsumida hosoda ball acquisition takahashi takeda
asada visual servoing gaskett fletcher zelinsky robot docking weber wermter zochios martnez marn duckett obstacle avoidance michels saxena ng interestingly rl used way tuning high level parameters image processing applications example peng
bhanu introduce rl image segmentation whereas yin proposes multilevel image thresholding uses entropy reinforcement
signal
applications preprocess images extract high level information
observed scene directly relevant task solved feeds
rl requires prior assumptions images perceived sensors
agent physical structure task preprocessing step
task specific coded hand contrasts objectives consist
introducing able learn directly connect visual space action
space without manually written code without relying prior knowledge
task solved aim develop general applicable
visual task formulated rl framework
noticeable exception work iida et al apply rl seek reach
targets push boxes shibata iida real robots work raw visual
signals directly feed neural network trained actor critic architecture
examples visual signal downscaled averaged monochrome e two color
image pixels output four infrared sensors added
perceptual input effective specific tasks process
used highly controlled environment real world images much richer
could undergo strong reduction size
local appearance paradigm
propose rely extraction visual features
way achieve compact state spaces used input traditional rl
indeed buried noise confusion visual cues images contain
hints regularity regularities captured important notion visual features
loosely speaking visual feature representation aspect local appearance
e g corner formed two intensity edges spatially localized texture signature
color therefore analyze images often sufficient computer program extract
useful information visual signal focusing attention robust highly


fijodogne piater

percepts

image classifier

reinforcements

detected visual class
informative visual features

reinforcement learning

actions

figure structure reinforcement learning visual classes

informative patterns percepts program thereafter seek characteristic
appearance observed scenes objects
actually basic postulate behind local appearance methods
much success computer vision applications image matching image retrieval
object recognition schmid mohr lowe rely detection
discontinuities visual signal thanks interest point detectors schmid mohr
bauckhage similarities images thereafter identified local description
neighborhood around interest points mikolajczyk schmid two images
share sufficient number matching local descriptors considered belong
visual class
local appearance techniques time powerful flexible
robust partial occlusions require segmentation scenes
seems therefore promising introduce front rl feature
image classifier partitions visual space finite set distinct visual classes
according local appearance paradigm focusing attention agent
highly distinctive local descriptors located interest points visual stimuli
symbol corresponding detected visual class could given input
classical embedded rl shown figure
preprocessing step intended reduce size input domain thus enhancing rate convergence generalization capabilities well robustness
rl noise visual domains importantly family visual features
applied wide variety visual tasks thus preprocessing step essentially general
task independent central difficulty dynamic selection discriminative
visual features selection process group images share similar task specific
properties together visual class
contributions
key technical contribution consists introduction reinforcement
learning used perceptual space contains images
developed rely task specific pre treatment consequence
used vision action task formalized markov decision
review three major contributions discussed


ficlosed loop learning visual control policies

adaptive discretization visual space
first contribution propose called reinforcement learning visual
classes rlvc combines aforementioned ideas rlvc iterative
suitable learning direct image action mappings taking advantage
local appearance paradigm consists two simultaneous interleaved learning processes
reinforcement learning mapping visual classes actions incremental building
feature image classifier
initially image classifier contains one single visual class images
mapped class course introduces kind perceptual aliasing hidden
state whitehead ballard optimal decisions cannot made since
percepts requiring different reactions associated class agent
isolates aliased classes since external supervisor agent rely
statistical analysis earned reinforcements detected aliased class agent
dynamically selects visual feature distinctive e best disambiguates
aliased percepts extracted local descriptor used refine classifier way
stage number visual classes classifier grows
visual features learned perceptual aliasing vanishes resulting image classifier
finally used control system
primarily motivated strong positive mccallums u tree
mccallum essence rlvc adaptation u tree visual spaces
though internals different originality rlvc lies
exploitation successful local appearance features rlvc selects subset highly
relevant features fully closed loop purposive learning process
practical interest successfully applied several simulated
visual navigation tasks
compacting visual policies
greedy nature rlvc prone overfitting splitting one visual class
potentially improve control policy visual classes therefore splitting
strategy get stuck local minima split made subsequently proves
useless cannot undone original description rlvc second contribution
provide rlvc possibility aggregating visual classes share similar
properties least three potential benefits
useless features discarded enhances generalization capabilities
rlvc reset search good features
number samples embedded rl disposal
visual class increased better visual control policies
experiments indeed improvement generalization abilities well reduction number visual classes selected features


fijodogne piater

spatial combinations visual features
finally efficacy rlvc clearly depends discriminative power visual
features power insufficient able completely remove
aliasing produce sub optimal control policies practical experiments
simulated visual navigation tasks exhibit deficiency soon number detected
visual features reduced features made similar less sensitive
metric objects encountered world composed number distinct
constituent parts e g face contains nose two eyes phone possesses keypad
parts recursively composed sub parts e g eye contains
iris eyelashes keypad composed buttons hierarchical physical structure
certainly imposes strong constraints spatial disposition visual features
third contribution highly informative spatial combinations visual
features iteratively constructed framework rlvc promising
permits construction features increasingly higher levels discriminative
power enabling us tackle visual tasks unsolvable individual point features
alone best knowledge extension rlvc appears
first attempt build visual feature hierarchies closed loop interactive purposive
learning process

overview reinforcement learning
framework relies theory rl introduced section rl
environment traditionally modeled markov decision process mdp mdp
tuple hs ri finite set states finite set actions
probabilistic transition function r reinforcement function
r mdp obeys following discrete time dynamics time agent
takes action environment lies state st agent perceives numerical
reinforcement rt r st reaches state st probability st st
thus point view agent interaction environment defined
quadruple hst rt st note definition markov decision processes
assumes full observability state space means agent able
distinguish states environment sensors allows us
talk indifferently states percepts visual tasks set images
percept action mapping fixed probabilistic function states
actions percept action mapping tells agent probability
choose action faced percept rl terminology mapping called
stationary markovian control policy infinite sequence interactions starting
state st discounted return
rt


x

rt





discount factor gives current value future reinforcements markov decision given mdp optimal percept action
mapping maximizes expected discounted return whatever starting state


ficlosed loop learning visual control policies

possible prove well defined optimal percept action
mapping exists bellman
markov decision solved dynamic programming dp
howard derman let percept action mapping let us call
state action value function q function giving state
action expected discounted return obtained starting state taking action
thereafter following mapping
q e rt st



e denotes expected value agent follows mapping let us define
h transform q functions q functions
x
hq r
max
q






note h transform equally referred bellman
backup operator state action value functions optimal mappings given mdp
share q function denoted q called optimal state action value function
exists satisfies bellmans called optimality equation bellman
hq q



optimal state action value function q known optimal deterministic perceptto action mapping easily derived choosing
argmax q



aa

another useful concept dp theory optimal value
function v state v corresponds expected discounted return
agent chooses optimal action encountered state e
v max q
aa



dynamic programming includes well known value iteration bellman policy iteration howard modified policy iteration puterman shin value iteration learns optimal state action value function q whereas policy
iteration modified policy iteration directly learn optimal percept action mapping
rl set algorithmic methods solving markov decision
underlying mdp known bertsekas tsitsiklis kaelbling et al sutton
barto precisely rl assume knowledge r
input rl basically sequence interactions hst rt st agent
environment rl techniques often divided two categories
model methods first build estimate underlying mdp e g
computing relative frequencies appear sequence interactions
use classical dp value policy iteration
model free methods sarsa rummery niranjan barto
sutton anderson sutton popular q learning watkins
compute estimate


fijodogne piater

reinforcement learning visual classes
discussed introduction propose insert image classifier rl
classifier maps visual stimuli set visual classes according
local appearance paradigm focusing attention agent highly distinctive
local descriptors detected interest points images
incremental discretization visual space
formally let us call infinite set local descriptors spanned
chosen local description method elements equivalently referred visual
features usually corresponds rn n assume existence visual
feature detector boolean function b testing whether given image
exhibits given local descriptor one interest points schmid et al
suitable metric used test similarity two visual features e g mahalanobis
euclidean distance
image classifier iteratively refined incremental process natural
way implement image classifiers use binary decision trees internal
nodes labeled visual feature presence tested node
leaves trees define set visual classes hopefully much smaller
original visual space upon possible apply directly usual rl
classify image system starts root node progresses
tree according feature detector visual feature found
descent reaching leaf
summarize rlvc builds sequence c c c growing decision trees
sequence attempts remove perceptual aliasing initial classifier c maps
input images single visual class v stage k classifier ck partitions
visual perceptual space finite number mk visual classes vk vk mk
learning architecture
resulting learning architecture called reinforcement learning visual classes
rlvc jodogne piater basic idea behind namely
iterative learning decision tree primarily motivated adaptive resolution techniques
previously introduced reinforcement learning notably mccallums
u tree mccallum section idea showed extremely
fruitful suitably adapted visual spaces links rlvc adaptiveresolution techniques thoroughly discussed section
components rlvc depicted figure depth discussion
components given next sections time review

rl classifier sequence arbitrary standard rl
applied provides information optimal state action function
optimal value function optimal policy induced current classifier
ck purpose computations interactions acquired


ficlosed loop learning visual control policies

figure different components rlvc
database previously collected interactions exploited component
covered sections
aliasing detector agent learned visual classes required complete
task viewpoint embedded rl input space
partially observable aliasing detector extracts classes perceptual
aliasing occurs analysis bellman residuals indeed explained
section exist tight relations perceptual aliasing bellman
residuals aliased class detected rlvc stops
feature generator applied rl database interactions
hst rt st available feature generator component produces set f
candidate visual features aliased class vk features used
refine classifier chosen among set candidates step
exposed sections
feature selector set candidate features f built aliased visual
class vk component selects visual feature f f best reduces
perceptual aliasing candidate feature discriminant component returns
conventional bottom symbol feature selector rlvc described
section
classifier refinement leaves correspond aliased classes featurebased image classifier replaced internal node testing presence absence
selected visual features
post processing optional component invoked every refinement corresponds techniques fighting overfitting details given section
general outline rlvc described note experiments contained model rl applied static


fijodogne piater

general structure rlvc
k
mk
ck binary decision tree one leaf
repeat

collect n interactions hst rt st

apply arbitrary rl sequence mapped ck

ck ck

mk

aliased vk

f generator st ck st vk

f selector vk f

f

ck refine vk testing f

mk mk

end

end

end

k k

post process ck
ck ck
databases interactions fifth step databases collected
fully randomized exploration policy choice guided ease
implementation presentation way collecting experience could used
well example sampling database interactions iteration rlvc
crucial point rlvc generates representation visual control policies
set collected visuomotor experience makes rlvc interactive following sections describe remaining namely aliased generator selector
post process
detection aliased visual classes
discuss aliasing detected classifier ck
projection mdp image classifier
formally image classifier ck converts sequence n interactions
hst rt st
mapped sequence n quadruples
hck st rt ck st
upon embedded rl applied let us define mapped mdp mk
mdp
hsk tk rk


ficlosed loop learning visual control policies

obtained mapped sequence sk set visual classes
known ck tk rk computed relative frequencies
mapped sequence follows
consider two visual classes v v vk vk mk one action define
following functions
v equals ck st v otherwise
v v equals ck st v ck st v otherwise
v number ts v
notation write
sk vk vk mk
p

tk v v n
v v v
p
rk v n
rt v v
optimal q function mapped mdp
mapped mdp mk induces optimal q function domain sk

denoted q
k computing qk difficult general may exist mdp defined
state space sk action space generate given mapped sequence
since latter necessarily markovian anymore thus rl run
mapped sequence might converge toward q
k even converge
however applied mapped sequence model rl method cf section
used compute q
k mk used underlying model conditions
q learning converges optimal q function mapped mdp singh jaakkola
jordan
turn function q
k induces another q function initial domain
relation
qk q

k ck
absence aliasing agent would perform optimally qk would correspond
q according bellman theorem states uniqueness optimal q function cf
section equation function
bk hqk qk



therefore measure aliasing induced image classifier ck rl terminology
bk bellman residual function qk sutton basic idea behind rlvc
refine states non zero bellman residual
measuring aliasing
consider time stamp database interactions hst rt st according
equation bellman residual corresponds state action pair st equals
x
bk st r st
st max
qk qk st








fijodogne piater

aliasing criterion
aliased vk



ck st vk



return true

end

end

return false
unfortunately rl agent access transition probabilities
reinforcement function r mdp modeling environment therefore equation cannot directly evaluated similar arises q learning watkins
fitted q iteration ernst geurts wehenkel
solve considering stochastic version time difference
described equation value
x
st max
qk






indeed estimated
max
qk





successor chosen probability st following transitions
environment ensures making transition st st probability st st
thus
rt max
qk st qk st




rt max
q
k ck st qk ck st






unbiased estimate bellman residual state action pair st jaakkola
jordan singh importantly system deterministic absence
perceptual aliasing estimates equal zero therefore nonzero potentially
indicates presence perceptual aliasing visual class vt ck st respect
action criterion detecting aliased classes consists computing q
k
function sweeping interactions hst rt st identify nonzero
practice assert presence aliasing variance exceeds given
threshold summarized denotes variance set
samples
generation selection distinctive visual features
aliasing detected visual class vk sk respect action
need select local descriptor best explains variations set values
worth noticing corresponds updates would applied q learning
known learning rate time



ficlosed loop learning visual control policies

canonical feature generator
generator sn

f

n

x x interest point si

f f symbol descriptor si x

end

end

return f
corresponding vk local descriptor chosen among set candidate
visual features f
extraction candidate features
informally canonical way building f visual class vk consists
identifying collected visual percepts st ck st vk
locating interest points selected images st
adding f local descriptor interest points
corresponding feature generator detailed latter
descriptor x returns local description point location x image
symbol returns symbol corresponds local descriptor f according
used metric however complex strategies generating visual features
used strategy builds spatial combinations individual point features
presented section
selection candidate features
choosing candidate feature reduces variations set
real valued bellman residuals regression suggest adaptation
popular splitting rule used cart building regression trees breiman
friedman stone
cart variance used impurity indicator split selected refine
particular node one leads greatest reduction sum squared
differences response values learning samples corresponding node
mean formally let hxi yi set learning samples xi rn
input vectors real numbers yi r real valued outputs cart selects
following candidate feature


v
v
f argmin pv
pv




vf

note previous work used splitting rule borrowed building classification
trees quinlan jodogne piater



fijodogne piater

feature selection
selector vk f

f
best feature found far

r
variance reduction induced f



ck st vk

visual feature f f

st exhibits f

st exhibit f





r

r r distributions significantly different

f f

r

end

end

end

return f

pv resp pv proportion samples exhibit resp exhibit
v resp v set samples exhibit resp exhibit
feature v

feature v idea directly transferred framework set xi
corresponds set interactions hst rt st set yi corresponds
set written explicitly
exploit stochastic version bellman residuals course real environments general non deterministic generates variations bellman residuals
consequence perceptual aliasing rlvc made somewhat robust
variability introducing statistical hypothesis test candidate feature
students ttest used decide whether two sub distributions feature induces
significantly different used u tree mccallum
illustration simple navigation task
evaluated system abstract task closely parallels real world scenario
avoiding unnecessary complexity consequence sensor model use may
seem unrealistic better visual sensor model exploited section
rlvc succeeded solving continuous noisy visual navigation task depicted
figure goal agent reach fast possible one two exits
maze set possible locations continuous location agent four
possible actions go right left every move altered gaussian noise
standard deviation size maze glass walls present
maze whenever move would take agent wall outside maze location
changed


ficlosed loop learning visual control policies

figure continuous noisy navigation task exits maze indicated boxes
cross walls glass identified solid lines agent depicted
center figure one four possible moves represented
arrow length corresponds resulting move sensors return
picture corresponds dashed portion image

agent earns reward exit reached move including
forbidden ones generates zero reinforcement agent succeeds escaping
maze arrives terminal state every move gives rise zero reinforcement
task set note agent faced delayed reward
must take distance two exits consideration choosing
attractive one
maze ground carpeted color image pixels
montage pictures coil database nene nayar murase
agent direct access x position maze rather sensors
take picture surrounding portion ground portion larger
blank areas makes input space fully observable importantly glass walls
transparent sensors return portions tapestry behind
therefore way agent directly locate walls obliged
identify regions maze action change location


fijodogne piater

figure deterministic image action mapping rlvc sampled
regularly spaced points manages choose correct action location

experiment used color differential invariants visual features gouet
boujemaa entire tapestry includes different visual features rlvc
selected features corresponding ratio entire set possible features
computation stopped generation image classifiers e k reached
took minutes ghz pentium iv databases interactions
visual classes identified small number compared number
perceptual classes would generated discretization maze agent
knows x position example reasonably sized grid leads perceptual
classes
figure shows optimal deterministic image action mapping
last obtained image classifier ck
argmax qk q
k ck
aa





ficlosed loop learning visual control policies



b

figure optimal value function agent direct access x
position maze set possible locations discretized
grid brighter location greater value b final value
function obtained rlvc

figure compares optimal value function discretized one obtained rlvc similarity two pictures indicates soundness
importantly rlvc operates neither pretreatment human intervention agent initially aware visual features important task
moreover interest selecting descriptors clear application direct tabular
representation q function considering boolean combinations features would
cells
behavior rlvc real word images investigated navigation
rules kept identical tapestry replaced panoramic photograph
pixels subway station depicted figure rlvc took iterations
compute mapping right figure computation time minutes
ghz pentium iv databases interactions distinct visual features
selected among set possible ones generating set visual classes
resulting classifier fine enough obtain nearly optimal image action mapping
task

related work
rlvc thought performing adaptive discretization visual space
basis presence visual features previous reinforcement learning
exploit presence perceptual features contexts discussed


fijodogne piater



b

figure navigation task real world image conventions
figure b deterministic image action mapping computed rlvc



ficlosed loop learning visual control policies

perceptual aliasing
explained incremental selection set informative visual features necessarily leads temporary perceptual aliasing rlvc tries remove generally
perceptual aliasing occurs whenever agent cannot take right basis
percepts
early work reinforcement learning tackled general two distinct
ways agent identifies avoids states perceptual aliasing occurs
lion see whitehead ballard tries build short term
memory allow remove ambiguities percepts predictive
distinctions see chrisman sketchily two detect
presence perceptual aliasing analysis sign q learning updates
possibility managing short term memory led development partially
observable markov decision processes pomdp theory kaelbling littman cassandra
current state random variable percepts
although approaches closely related perceptual aliasing rlvc temporarily introduces consider exploitation perceptual features indeed
tackle structural given control task assume
perceptual aliasing cannot removed consequence approaches orthogonal
interest since ambiguities rlvc generates removed
refining image classifier fact techniques tackle lack information inherent used sensors whereas goal handle surplus information related
high redundancy visual representations
adaptive resolution finite perceptual spaces
rlvc performs adaptive discretization perceptual space autonomous
task driven purposive selection visual features work rl incrementally partitions
large discrete continuous perceptual space piecewise constant value
function usually referred adaptive resolution techniques ideally regions
perceptual space high granularity present needed
lower resolution used elsewhere rlvc adaptive resolution
review several adaptive resolution methods previously
proposed finite perceptual spaces
idea adaptive resolution techniques reinforcement learning goes back
g chapman kaelbling inspired approaches
discussed g considers perceptual spaces made
fixed length binary numbers learns decision tree tests presence informative
bits percepts uses students test determine bit
b percepts mapped given leaf state action utilities states
b set significantly different state action utilities states
b unset bit found corresponding leaf split process repeated
leaf method able learn compact representations even though
large number irrelevant bits percepts unfortunately region split
information associated region lost makes slow learning


fijodogne piater

concretely g solve task whose perceptual space contains distinct
percepts corresponds set binary numbers length bits
mccallums u tree builds upon idea combining selective attention
mechanism inspired g short term memory enables agent
deal partially observable environments mccallum therefore mccallums
keystone reinforcement learning unify g chapman kaelbling chrismans predictive distinctions chrisman
u tree incrementally grows decision tree kolmogorov smirnov tests
succeeded learning behaviors driving simulator simulator percept consists
set discrete variables whose variation domains contain values
leading perceptual space possible percepts thus size perceptual
space much smaller visual space however task difficult
physical state space partially observable perceptual space driving
task contains physical states means several physical states requiring
different reactions mapped percept sensors agent
u tree resolves ambiguities percepts testing presence perceptual
features percepts encountered previously history system
end u tree manages short term memory partially observable
environments considered challenge rather deal huge visual spaces
without hand tuned pre processing difficult novel direction
adaptive resolution continuous perceptual spaces
important notice methods adaptive resolution large scale finite
perceptual spaces use fixed set perceptual features hard wired
distinguished rlvc samples visual features possibly infinite visual feature
space e g set visual features infinite makes prior assumptions
maximum number useful features point view rlvc closer
adaptive resolution techniques continuous perceptual spaces indeed techniques
dynamically select relevant features whole continuum
first adaptive resolution continuous perceptual spaces darling salganicoff current
continuous adaptive resolution splits perceptual space thresholds
purpose darling builds hybrid decision tree assigns label point
perceptual space darling fully line incremental equipped
forgetting mechanism deletes outdated interactions however limited
binary reinforcement signals takes immediate reinforcements account
darling much closer supervised learning reinforcement learning
parti game moore atkeson produces goal directed behaviors
continuous perceptual spaces parti game splits regions deems important
game theoretic moore atkeson parti game learn
competent behavior variety continuous domains unfortunately
currently limited deterministic domains agent greedy controller
goal state known moreover searches solution
given task try optimal one


ficlosed loop learning visual control policies

continuous u tree extension u tree adapted continuous perceptual spaces uther veloso darling continuous u tree
incrementally builds decision tree splits perceptual space finite set hypercubes testing thresholds kolmogorov smirnov sum squared errors used
determine split node decision tree pyeatt howe analyze
performance several splitting criteria variation continuous u tree
conclude students test leads best performance motivates use
statistical test rlvc cf section
munos moore proposed variable resolution grids
assumes perceptual space compact subset euclidean space begins
coarse grid discretization state space contrast abstract
section value function policy vary linearly within region
munos moore use kuhn triangulation efficient way interpolate value function within regions refines approximation refining cells according
splitting criterion munos moore explore several local heuristic measures importance splitting cell including average corner value differences variance
corner value differences policy disagreement explore global heuristic measures involving influence variance approximated system variable resolution
grids probably advanced adaptive resolution available far
discussion
summarize several similar spirit rlvc proposed
years nevertheless work appears first learn direct image toaction mappings reinforcement learning indeed none reinforcement learning
methods combines following desirable properties rlvc set relevant perceptual features chosen priori hand selection process fully
automatic require human intervention visual perceptual spaces explicitly considered appearance visual features highly informative
perceptual features drawn possibly infinite set
advantages rlvc essentially due fact candidate visual
features selected informative ranked according
information theoretic measure inspired decision tree induction breiman et al
ranking required vision action tasks induce large number visual
features typical image contains thousand kind criterion
ranks features though already considered variable resolution grids munos moore
seems discrete perceptual spaces
rlvc defined independently fixed rl similar spirit
continuous u tree uther veloso major exception rlvc deals
boolean features whereas continuous u tree works continuous input space furthermore version rlvc presented uses variance reduction criterion
ranking visual features criterion though already considered variable resolution
grids seems discrete perceptual spaces


fijodogne piater

compacting visual policies
written section original version rlvc subject overfitting jodogne
piater b simple heuristic avoid creation many visual classes simply
bound number visual classes refined stage
since splitting one visual class potentially impact bellman residuals
visual classes practice first try split classes samples
considering others since evidence variance reduction first
tests systematically apply heuristics however often insufficient taken
alone
equivalence relations markov decision processes
since apply embedded rl stage k rlvc properties
optimal value function vk optimal state action value function qk optimal
control policy k known mapped mdp mk properties easy
define whole range equivalence relations visual classes instance
given threshold r list hereunder three possible equivalence relations pair
visual classes v v
optimal value equivalence
vk v vk v
optimal policy equivalence
vk v qk v k v
vk v qk v k v
optimal state action value equivalence
qk v qk v
therefore propose modify rlvc periodically visual classes
equivalent respect one criteria merged together experimentally
observed conjunction first two criteria tends lead best performance
way rlvc alternatively splits merges visual classes compaction phase
done often order allow exploration best knowledge
possibility investigated yet framework adaptive resolution methods
reinforcement learning
original version rlvc visual classes correspond leaves decision
tree decision trees aggregation visual classes achieved
starting bottom tree recursively collapsing leaves dissimilar leaves
found operation close post pruning framework decision trees
machine learning breiman et al practice means classes
similar properties reached one another making number
hops upwards downwards extremely unlikely matched greatly reduces
interest exploiting equivalence relations
drawback due rather limited expressiveness decision trees decision
tree visual class corresponds conjunction visual feature literals defines


ficlosed loop learning visual control policies

path root decision tree one leaf take full advantage equivalence
relations necessary associate visual class arbitrary union conjunctions
visual features indeed exploiting equivalence relations visual classes
sequence conjunctions splitting disjunctions aggregation thus
expressive data structure would able represent general arbitrary boolean
combinations visual features required data structure introduced next
section
binary decision diagrams
representing general boolean functions extensively studied
field computer aided verification since abstract behavior logical electronic
devices fact whole range methods representing state space richer
richer domains developed last years binary decision diagram
bdd bryant number queue decision diagrams boigelot upward
closed sets delzanno raskin real vector automata boigelot jodogne
wolper
framework bdd particularly well suited tool acyclic graph
symbolic representation encoding arbitrary boolean functions much success field computer aided verification bryant bdd unique
ordering variables fixed different variable orderings lead different sizes
bdd since variables discarded reordering process although
finding optimal variable ordering conp complete bryant automatic heuristics practice orderings close optimal interesting
case since reducing size bdd potentially discards irrelevant variables
correspond removing useless visual features
modifications rlvc
summarize extension rlvc use decision trees anymore assigns
one bdd visual class two modifications applied
operation refining visual feature f visual class v labeled
bdd b v consists replacing v two visual classes v v
b v b v f b v b v f
given equivalence relation post process ck operation consists merging
equivalent visual classes merge pair visual classes v v v v
deleted visual class v b v b v b v added every
time merging operation takes place advised carry variable reordering
minimize memory requirements
experiments
applied modified version rlvc another simulated navigation task
task agent moves spots campus university liege cf
figure every time agent one locations body aim four possible


fijodogne piater

n

c google map

figure montefiore campus liege red spots corresponds places
agent moves agent follow links different
spots goal enter montefiore institute labeled red cross
gets reward

orientations north south west east state space therefore size
agent three possible actions turn left turn right go forward goal enter
specific building obtain reward turning left right induces
penalty moving forward penalty discount factor set
optimal control policy unique one depicted figure
agent direct access position orientation rather
perceives picture area front cf figure thus agent
connect input image appropriate reaction without explicitly knowing
geographical localization possible location possible viewing direction
database images size significant viewpoint changes
collected databases randomly divided learning set images
test set images experimental setup versions rlvc learn
image action mapping interactions contain images learning set
images test set used assess accuracy learned visual control policies
sift keypoints used visual features lowe thresholding
mahalanobis distance gave rise set distinct features versions rlvc
applied static database interactions collected
fully randomized exploration policy database used throughout entire
database contains images belong learning set
basic version rlvc version extended bdds
reported figures original version rlvc identified visual
classes selecting sift features error rate computed visual policy e
proportion sub optimal decisions agent presented possible stimuli


ficlosed loop learning visual control policies

c google map

figure one optimal deterministic control policies montefiore navigation
task state indicated optimal action letter f stands
move forward r turn right l turn left policy
obtained applying standard rl scenario
agent direct access p information

learning set images test set used respect
optimal policy agent direct access position viewing direction
modified version rlvc applied one compacting stage every steps
clearly superior error learning set anymore
error rate test set number selected features reduced
furthermore resulting number visual classes becomes instead thus
large improvement generalization abilities well reduction number
visual classes selected features interestingly enough number visual classes
close number physical states tends indicate
starts learn physical interpretation percepts
summarize compacting visual policies probably required step deal realistic visual tasks iterative splitting process applied price pay course


fijodogne piater

c google map

figure percepts agent four possible different percepts shown correspond location viewing direction marked yellow top image



ficlosed loop learning visual control policies




rlvc
rlvc bdd

















iterations k







error rate

























iterations k







error rate

rlvc
rlvc bdd




figure comparison error rates basic extended versions rlvc
error computed policy function step counter k images
learning set resp test set reported left figure resp
right

higher computational cost future work focus theoretical justification used
equivalence relations implies bridging gap theory mdp minimization givan dean greig

learning spatial relationships
motivated introduction section propose extend rlvc constructing hierarchy spatial arrangements individual point features jodogne scalzo
piater idea learning spatial combinations features takes
roots seminal fischler elschlager pictorial structures
collections rigid parts arranged deformable configurations idea
become increasingly popular computer vision community led
large literature modeling detection objects amit kong burl
perona forsyth haddon ioffe crandall huttenlocher provide pointers recent resources among recent techniques scalzo piater
propose build probabilistic hierarchy visual features represented
acyclic graph detect presence model nonparametric belief
propagation sudderth ihler freeman willsky graphical
proposed representing articulated structures pictorial structures felzenszwalb huttenlocher kumar torr zisserman similarly constellation
model represents objects parts modeled terms shape appearance
gaussian probability density functions perona fergus zisserman
work contrasts approaches generation called composite
features driven task solved distinguished techniques
unsupervised learning composite features since additional information


fijodogne piater




rlvc
rlvc bdd

















iterations k







number classes

























iterations k







number selected features

rlvc
rlvc bdd




figure comparison number generated classes selected visual features basic extended versions rlvc number visual classes
resp selected features function step counter k plotted left
figure resp right

embedded inside reinforcement signal drives generation composite features
focusing exploration task relevant spatial arrangements
extension rlvc hierarchy visual features built simultaneously
image classifier soon sufficiently informative visual feature extracted
tries combine two visual features order construct higher level
abstraction hopefully distinctive robust noise extension
rlvc assumes co existence two different kinds visual features
primitive features correspond individual point features e localappearance descriptors cf section
composite features consist spatial combinations lower level visual features
priori bound maximal height hierarchy therefore
composite feature potentially combined primitive feature
composite feature
detection visual features
natural way represent hierarchy use directed acyclic graph g v e
vertex v v corresponds visual feature edge v v e
fact v part composite feature v thus g must binary
e vertex child exactly two children set vp leaves
g corresponds set primitive features set vc internal vertexes
represents set composite features
leaf vertex vp vp annotated local descriptor vp similarly
internal vertex vc vc annotated constraints relative position
parts work consider constraints distances constituent


ficlosed loop learning visual control policies

visual features composite features assume distributed
according gaussian law g mean standard deviation evidently richer
constraints could used taking relative orientation scaling factor constituent features consideration would require use multivariate gaussians
precisely let vc composite feature parts v v order
trigger detection vc image occurrence v
occurrence v relative euclidean distance sufficient likelihood
generated gaussian mean vc standard deviation vc ensure
symmetry location composite feature taken midpoint
locations v v
occurrences visual feature v percept found recursive
course steps test st exhibit v
rewritten function checking occurrences v st
generation composite features
cornerstone extension rlvc way generating composite features
general idea behind accumulate statistical evidence relative
positions detected visual features order conspicuous coincidences visual
features done providing evolved implementation generator sn
one
identifying spatial relations
first extract set f primitive composite features occur within
set provided images sn
f v v si exhibits v



identify pairs visual features occurrences highly correlated within
set provided images sn simply amounts counting number
co occurrences pair features f keeping pairs corresponding
count exceeds fixed threshold
let v v two features highly correlated search meaningful
spatial relationship v v carried images sn
contain occurrences v v co occurrence accumulate set
distances corresponding occurrences v v finally clustering
applied distribution order detect typical distances v
v purpose experiments used hierarchical clustering jain
murty flynn cluster gaussian fitted estimating mean value
standard deviation finally composite feature vc introduced
feature hierarchy v v parts vc vc
summary replace call call


fijodogne piater

detecting composite features
occurrences v

v primitive

return x x interest point local descriptor corresponds v

else



occurrences subfeature v

occurrences subfeature v

p
x x

x x

g v v

x x

end

end

return

end

generation composite features
generator sn

f v v si exhibits v

f

v v f f

enough co occurrences v v sn



n

occurrences x v si

occurrences
x v si
p

x x

end

end

end

apply clustering

cluster c dm

mean c

stddev c

add f composite feature vc composed v v annotated
mean standard deviation

end

end

end

return f



ficlosed loop learning visual control policies

h p


n

u


mg








p



figure car hill control

feature validation
generate several composite features given visual class vk however
end one generated composite feature kept
important notice performance clustering method critical
purpose indeed irrelevant spatial combinations automatically discarded thanks
variance reduction criterion feature selection component fact reinforcement
signal helps direct search good feature advantage unsupervised
methods building feature hierarchies
experiments
demonstrate efficacy version classical car hill
control moore atkeson position velocity information
presented agent visually
episodic task car modeled mass point riding without friction
hill shape defined function

h p

p p
p
p

p p p

goal agent reach fast possible top hill e location
p top hill agent obtains reward car thrust left
right acceleration newtons however gravity acceleration
insufficient agent reach top hill thrusting toward right
rather agent go left hence acquiring potential energy going
left side hill thrusting rightward two constraints agent
allowed reach locations p velocity greater absolute
value leads destruction car


fijodogne piater

formal definition task
formally set possible actions state space p
p system following continuous time dynamics
p





p

h p



gh p

h p

thrust acceleration h p first derivative h p
mass car g acceleration due gravity continuous time
dynamics approximated following discrete time state update rule
st st hpt h st
st pt hst
h integration time step reinforcement signal defined
expression

st st
r st st

otherwise
setup discount factor set
definition actually mix two coexistent formulations car hill
task ernst geurts wehenkel moore atkeson major differences
initial formulation moore atkeson set
possible actions discrete goal top hill rather given
area hill definition ernst et al ensure existence
interesting solution velocity required remain less instead
integration time step set h instead
inputs agent
previous work moore atkeson ernst et al agent assumed
direct access numerical measure position velocity exception
gordons work visual low resolution representation global scene given
agent gordon experimental setup agent provided two
cameras one looking ground underneath second velocity gauge way
agent cannot directly know current position velocity suitably interpret
visual inputs derive
examples pictures sensors return presented figure
ground carpeted color image pixels montage pictures
coil database nene et al important notice
individual point features insufficient solving task since set features
pictures velocity gauge know velocity agent
generate composite features sensitive distance primitive features cursor
respect primitive features digits


ficlosed loop learning visual control policies



b
figure visual percepts corresponding pictures velocity gauge
b visual percepts returned position sensor
region framed white rectangle corresponds portion ground
returned sensor p portion slides back forth
agent moves


experimental setup used color differential invariants gouet boujemaa
primitive features among possible visual inputs position
velocity sensors different primitive features entire image ground
includes interest points whereas images velocity gauge include
interest points
output rlvc decision tree defines visual classes internal
node tree tests presence one visual feature taken set distinct
highly discriminant features selected rlvc among selected visual features
primitive composite features two examples composites features
selected rlvc depicted figure computation stopped k
refinement steps
efficacy method compare performance scenario
agent direct perception current p state latter scenario
state space discretized grid cells number chosen since
approximately corresponds square root number visual classes
produced rlvc way rl provided equivalent number perceptual classes
two scenarios figure compares optimal value function direct perception


fijodogne piater

velocity










position





velocity










position



b

figure optimal value function agent direct access current
p state input space discretized grid
brighter location greater value b value function obtained
rlvc



ficlosed loop learning visual control policies

figure two composite features generated yellow primitive features
composed marked yellow first feature triggers
velocities around whereas second triggers around

one obtained rlvc two pictures similar
indicates soundness
evaluated performance optimal image action mapping
argmax q p



aa

obtained rlvc purpose agent placed randomly hill
initial velocity used mapping choose action reached
final state set trials carried step k
figure compares proportion trials missed goal leaving
hill left acquiring high velocity rlvc directperception k became greater proportion missed trials
smaller rlvc direct perception advantage favor
rlvc due adaptive nature discretization figure compares mean
lengths successful trials mean length rlvc trials clearly converges
direct perception trials staying slightly larger
conclude rlvc achieves performance close direct perception scenario however mapping built rlvc directly links visual percepts appropriate actions
without considering explicitly physical variables

summary
introduces reinforcement learning visual classes rlvc rlvc designed
learn mappings directly connect visual stimuli output actions optimal
surrounding environment framework rlvc general sense
applied formulated markov decision
learning process behind closed loop flexible agent
takes lessons interactions environment according purposive vision
paradigm rlvc focuses attention embedded reinforcement learning
highly informative robust parts inputs testing presence absence
local descriptors interest points input images relevant visual features


fijodogne piater


rlvc
direct perception x grid

missed goal





























iterations k

figure evolution number times goal missed iterations
rlvc



rlvc
direct perception x grid



mean length interaction































iterations k

figure evolution mean lengths successful trials iterations rlvc



ficlosed loop learning visual control policies

incrementally selected sequence attempts remove perceptual aliasing
discretization process targets zero bellman residuals inspired supervised learning building decision trees defined independently
interest point detector schmid et al local description technique mikolajczyk schmid user may choose two components sees fit
techniques fighting overfitting rlvc proposed idea
aggregate visual classes share similar properties respect theory dynamic
programming interestingly process enhances generalization abilities learned
image action mapping reduces number visual classes built
finally extension rlvc introduced allows closed loop interactive
purposive learning hierarchy geometrical combinations visual features
contrast prior work topic uses supervised
unsupervised framework piater fergus perona zisserman bouchard
triggs scalzo piater besides novelty shown
practical value visual control tasks information provided individual
point features alone insufficient solving task indeed spatial combinations visual
features informative robust noise

future work
area applications rlvc wide since nowadays robotic agents often equipped
ccd sensors future includes demonstration applicability
reactive robotic application grasping objects combining visual
haptic feedback coelho piater grupen necessitates extension
techniques continuous action spaces fully satisfactory solutions exist
date rlvc could potentially applied human computer interaction
actions need physical actions
closed loop learning hierarchy visual feature raises interesting
directions example combination rlvc techniques disambiguating
aliased percepts short term memory mccallum could solve visual
tasks percepts agent alone provide enough information solving
task likewise unsupervised learning kinds geometrical felzenszwalb huttenlocher could potentially embedded rlvc hand
spatial relationships currently take consideration relative angles
parts composite feature would increase discriminative power
composite features requires non trivial techniques clustering circular domains

acknowledgments
authors thank associate editor thorsten joachims three anonymous reviewers many suggestions improving quality manuscript sebastien
jodogne gratefully acknowledge financial support belgian national fund
scientific fnrs


fijodogne piater

references
aloimonos purposive qualitative active vision proc th international conference pattern recognition pp
amit kong graphical templates model registration ieee transactions pattern analysis machine intelligence
asada noda tawaratsumida hosoda k vision behavior acquisition shooting robot reinforcement learning proc iapr ieee
workshop visual behaviors pp
bagnell j schneider j autonomous helicopter control reinforcement
learning policy search methods proc international conference robotics
automation ieee
barto sutton r anderson c neuronlike adaptive elements
solve difficult learning control ieee transactions systems man
cybernetics
bellman r dynamic programming princeton university press
bertsekas tsitsiklis j neuro dynamic programming athena scientific
boigelot b symbolic methods exploring infinite state spaces ph thesis
university liege liege belgium
boigelot b jodogne wolper p effective decision procedure linear
arithmetic integer real variables acm transactions computational logic
tocl
bouchard g triggs b hierarchical part visual object categorization
ieee conference computer vision pattern recognition vol pp
san diego ca usa
breiman l friedman j stone c
wadsworth international group

classification regression trees

bryant r graph boolean function manipulation ieee transactions computers
bryant r symbolic boolean manipulation ordered binary decision diagrams
acm computing surveys
burl perona p recognition planar object classes proc ieee
conference computer vision pattern recognition pp san francisco
ca usa
chapman kaelbling l input generalization delayed reinforcement learning performance comparisons proc th international
joint conference artificial intelligence ijcai pp sydney
chrisman l reinforcement learning perceptual aliasing perceptual
distinctions national conference artificial intelligence pp


ficlosed loop learning visual control policies

coelho j piater j grupen r developing haptic visual perceptual categories reaching grasping humanoid robot robotics autonomous
systems special issue humanoid robots
crandall huttenlocher weakly supervised learning part spatial
visual object recognition proc th european conference
computer vision
delzanno g raskin j f symbolic representation upward closed sets
tools construction analysis systems lecture notes
computer science pp berlin germany
derman c finite state markovian decision processes academic press york
ernst geurts p wehenkel l iteratively extending time horizon reinforcement learning proc th european conference machine learning pp
dubrovnik croatia
ernst geurts p wehenkel l tree batch mode reinforcement learning
journal machine learning
felzenszwalb p huttenlocher pictorial structures object recognition
international journal computer vision
fergus r perona p zisserman object class recognition unsupervised
scale invariant learning ieee conference computer vision pattern recognition vol pp madison wi usa
fischler elschlager r representation matching pictorial structures ieee transactions computers
forsyth haddon j ioffe finding objects grouping primitives shape
contour grouping computer vision pp london uk springerverlag
gaskett c fletcher l zelinsky reinforcement learning visual servoing
mobile robot proc australian conference robotics automation
melbourne australia
gibson e spelke e development perception flavell j h markman e eds handbook child psychology vol iii cognitive development
th edition chap pp wiley
givan r dean greig equivalence notions model minimization
markov decision processes artificial intelligence
gordon g stable function approximation dynamic programming proc
international conference machine learning pp
gouet v boujemaa n object queries color points interest
ieee workshop content access image video libraries pp
kauai hi usa
howard r dynamic programming markov processes technology press
wiley cambridge york


fijodogne piater

huber grupen r control structure learning locomotion gaits th
int symposium robotics applications anchorage ak usa tsi press
iida sugisaka shibata k direct vision reinforcement learning
real mobile robot proc international conference neural information
processing systems vol pp
jaakkola jordan singh convergence stochastic iterative dynamic
programming cowan j tesauro g alspector j eds advances neural information processing systems vol pp morgan kaufmann publishers
jain k murty n flynn p j data clustering review acm computing
surveys
jodogne piater j interactive learning mappings visual percepts
actions de raedt l wrobel eds proc nd international
conference machine learning icml pp bonn germany acm
jodogne piater j b learning compacting visual policies extended abstract proc th european workshop reinforcement learning ewrl
pp napoli italy
jodogne scalzo f piater j task driven learning spatial combinations
visual features proc ieee workshop learning computer vision
pattern recognition san diego ca usa ieee
kaelbling l littman cassandra acting partially
observable stochastic domains artificial intelligence
kaelbling l littman moore reinforcement learning survey journal
artificial intelligence
kimura h yamashita kobayashi reinforcement learning walking
behavior four legged robot proc th ieee conference decision
control orlando fl usa
kohl n stone p policy gradient reinforcement learning fast quadrupedal
locomotion proc ieee international conference robotics automation pp orleans
kumar torr p zisserman extending pictorial structures object
recognition proc british machine vision conference
kwok c fox reinforcement learning sensing strategies proc
ieee international conference intelligent robots systems
lagoudakis parr r least squares policy iteration journal machine
learning
lowe distinctive image features scale invariant keypoints international
journal computer vision
martnez marn duckett fast reinforcement learning vision guided
mobile robots proc ieee international conference robotics automation pp barcelona spain


ficlosed loop learning visual control policies

mccallum r reinforcement learning selective perception hidden state
ph thesis university rochester york
michels j saxena ng high speed obstacle avoidance monocular
vision reinforcement learning proc nd international conference
machine learning pp bonn germany
mikolajczyk k schmid c performance evaluation local descriptors
proc ieee conference computer vision pattern recognition vol
pp madison wi usa
moore atkeson c parti game variable resolution reinforcement learning multidimensional state spaces machine learning
munos r moore variable resolution discretization optimal control machine learning
nene nayar murase h columbia object image library coil tech
rep cucs columbia university york
ng coates diel ganapathi v schulte j tse b berger b liang e
inverted autonomous helicopter flight via reinforcement learning proc
international symposium experimental robotics
paletta l fritz g seifert c q learning sequential attention visual object
recognition informative local descriptors proc nd international
conference machine learning icml pp bonn germany
paletta l pinz active object recognition view integration reinforcement learning robotics autonomous systems
peng j bhanu b closed loop object recognition reinforcement learning
ieee transactions pattern analysis machine intelligence
perona p fergus r zisserman object class recognition unsupervised
scale invariant learning conference computer vision pattern recognition
vol p
piater j visual feature learning ph thesis university massachusetts
computer science department amherst usa
puterman shin modified policy iteration discounted
markov decision management science
pyeatt l howe decision tree function approximation reinforcement
learning proc third international symposium adaptive systems pp
havana cuba
quinlan j c programs machine learning morgan kaufmann publishers
inc san francisco ca usa
randlv j alstrm p learning drive bicycle reinforcement learning
shaping proc th international conference machine learning pp
madison wi usa morgan kaufmann


fijodogne piater

rummery g niranjan line q learning connectionist sytems tech
rep cued f infeng tr cambridge university
salganicoff density adaptive learning forgetting proc th
international conference machine learning pp amherst usa
morgan kaufmann publishers
scalzo f piater j unsupervised learning dense hierarchical appearance
representations proc th international conference pattern recognition
hong kong
schaal learning demonstration mozer c jordan petsche
eds advances neural information processing systems vol pp
cambridge mit press
schmid c mohr r local greyvalue invariants image retrieval ieee
transactions pattern analysis machine intelligence
schmid c mohr r bauckhage c evaluation interest point detectors
international journal computer vision
schyns p rodet l categorization creates functional features journal
experimental psychology learning memory cognition
shibata k iida acquisition box pushing direct vision reinforcement learning proc society instrument control engineers annual
conference p
singh jaakkola jordan reinforcement learning soft state aggregation advances neural information processing systems vol pp
mit press
sudderth e ihler freeman w willsky nonparametric belief propagation proc ieee conference computer vision pattern recognition
pp
sutton r learning predict methods temporal differences machine
learning
sutton r barto reinforcement learning introduction mit press
takahashi takeda asada continuous valued q learning visionguided behavior acquisition proc international conference multisensor
fusion integration intelligent systems pp
tarr cheng learning see faces objects trends cognitive
sciences
tesauro g temporal difference learning td gammon communications
acm
uther w veloso tree discretization continuous state space reinforcement learning proc th national conference artificial intelligence
aaai pp madison wi usa


ficlosed loop learning visual control policies

watkins c learning delayed rewards ph thesis kings college cambridge uk
weber c wermter zochios robot docking neural vision
reinforcement knowledge systems
wettergreen gaskett c zelinsky autonomous guidance control
underwater robotic vehicle proc international conference field
service robotics pittsburgh usa
whitehead ballard learning perceive act trial error
machine learning
yin p maximum entropy optimal threshold selection deterministic
reinforcement learning controlled randomization signal processing

yoshimoto j ishii sato application reinforcement learning balancing acrobot proc ieee international conference systems
man cybernetics pp




