Journal Artificial Intelligence Research 28 (2007) 349391

Submitted 06/06; published 03/07

Closed-Loop Learning Visual Control Policies
Sebastien Jodogne
Justus H. Piater

Jodogne@Montefiore.ULg.ac.be
Justus.Piater@ULg.ac.be

Montefiore Institute (B28)
University Liege, B-4000 Liege, Belgium

Abstract
paper present general, flexible framework learning mappings images actions interacting environment. basic idea introduce
feature-based image classifier front reinforcement learning algorithm. classifier
partitions visual space according presence absence highly informative local descriptors incrementally selected sequence attempts remove
perceptual aliasing. address problem fighting overfitting greedy
algorithm. Finally, show high-level visual features generated
power local descriptors insufficient completely disambiguating aliased states.
done building hierarchy composite features consist recursive spatial
combinations visual features. demonstrate efficacy algorithms solving
three visual navigation tasks visual version classical Car Hill control
problem.

1. Introduction
Designing robotic controllers quickly becomes challenging problem. Indeed, controllers face huge number possible inputs noisy, must select actions among
continuous set, able automatically adapt evolving stochastic environmental conditions. Although real-world robotic task often solved
directly connecting perceptual space action space given computational
mechanism, mappings usually hard derive hand, especially perceptual space contains images. Evidently, automatic methods generating mappings
highly desirable, many robots nowadays equipped CCD sensors.
paper, interested reactive systems learn couple visual perceptions
actions inside dynamic world act reasonably. coupling known
visual (control) policy. wide category problems called vision-for-action tasks
(or simply visual tasks). Despite fifty years years active research artificial
intelligence, robotic agents still largely unable solve many real-world visuomotor
tasks easily performed humans even animals. vision-for-action
tasks notably include grasping, vision-guided navigation manipulation objects
achieve goal. article introduces general framework suitable building
image-to-action mappings using fully automatic flexible learning protocol.
1.1 Vision-for-Action Reinforcement Learning
Strong neuropsychological evidence suggests human beings learn extract useful information visual data interactive fashion, without external supervisor (Gibson
c
2007
AI Access Foundation. rights reserved.

fiJodogne & Piater

& Spelke, 1983). evaluating consequence actions environment,
learn pay attention visual cues behaviorally important solving task.
way, interact outside world, gain expertise
tasks (Tarr & Cheng, 2003). Obviously, process task driven, since different tasks
necessarily need make distinctions (Schyns & Rodet, 1997).
breakthrough modern artificial intelligence would design artificial system
would acquire object scene recognition skills based experience
surrounding environment. state general terms, important research
direction would design robotic agent could autonomously acquire visual skills
interactions uncommitted environment order achieve set
goals. Learning new visual skills dynamic, task-driven fashion complete
priori unknown visual task known purposive vision paradigm (Aloimonos, 1990).
One plausible framework learn image-to-action mappings according purposive vision Reinforcement Learning (RL) (Bertsekas & Tsitsiklis, 1996; Kaelbling, Littman,
& Moore, 1996; Sutton & Barto, 1998). Reinforcement learning biologically-inspired
computational framework generate nearly optimal control policies automatic
way, interacting environment. RL founded analysis so-called
reinforcement signal . Whenever agent takes decision, receives feedback real
number evaluates relevance decision. biological perspective,
signal becomes positive, agent experiences pleasure, talk reward .
Conversely, negative reinforcement implies sensation pain, corresponds
punishment. reinforcement signal arbitrarily delayed actions
responsible it. Now, RL algorithms able map every possible perception
action maximizes reinforcement signal time. framework, agent
never told optimal action facing given percept, whether one
decisions optimal. Rather, agent discover promising
actions constituting representative database interactions, understanding influence decisions future reinforcements. Schematically, RL lies
supervised learning (where external teacher gives correct action agent)
unsupervised learning (in clue goodness action given).
RL successful applications, example turning computer excellent
Backgammon player (Tesauro, 1995), solving Acrobot control problem (Yoshimoto,
Ishii, & Sato, 1999), making quadruped robot learn progressively walk without
human intervention (Huber & Grupen, 1998; Kimura, Yamashita, & Kobayashi, 2001; Kohl
& Stone, 2004), riding bicycle (Randlv & Alstrm, 1998; Lagoudakis & Parr, 2003)
controlling helicopter (Bagnell & Schneider, 2001; Ng, Coates, Diel, Ganapathi, Schulte,
Tse, Berger, & Liang, 2004). major advantages RL protocol fully
automatic, imposes weak constraints environment.
Unfortunately, standard RL algorithms highly sensitive number distinct
percepts well noise results sensing process. general problem
often referred Bellman curse dimensionality (Bellman, 1957). Thus, high
dimensionality noise inherent images forbid use basic RL algorithms
direct closed-loop learning image-to-action mappings according purposive vision.
350

fiClosed-Loop Learning Visual Control Policies

1.2 Achieving Purposive Vision Reinforcement Learning
exists variety work RL specific robotic problems involving perceptual
space contains images. instance, Schaal (1997) uses visual feedback solve
pole-balancing task. RL used control vision-guided underwater robotic vehicle (Wettergreen, Gaskett, & Zelinsky, 1999). recently, Kwok Fox (2004)
demonstrated applicability RL learning sensing strategies using Aibo robots.
Reinforcement learning used learn strategies view selection (Paletta &
Pinz, 2000) sequential attention models (Paletta, Fritz, & Seifert, 2005). Let us
mention use reinforcement learning vision-guided tasks ball kicking (Asada, Noda, Tawaratsumida, & Hosoda, 1994), ball acquisition (Takahashi, Takeda,
& Asada, 1999), visual servoing (Gaskett, Fletcher, & Zelinsky, 2000), robot docking (Weber, Wermter, & Zochios, 2004; Martnez-Marn & Duckett, 2005) obstacle avoidance (Michels, Saxena, & Ng, 2005). Interestingly, RL used way tuning high-level parameters image-processing applications. example, Peng
Bhanu (1998) introduce RL algorithms image segmentation, whereas Yin (2002) proposes algorithms multilevel image thresholding, uses entropy reinforcement
signal.
applications preprocess images extract high-level information
observed scene directly relevant task solved feeds
RL algorithm. requires prior assumptions images perceived sensors
agent, physical structure task itself. preprocessing step
task specific coded hand. contrasts objectives, consist
introducing algorithms able learn directly connect visual space action
space, without using manually written code without relying prior knowledge
task solved. aim develop general algorithms applicable
visual task formulated RL framework.
noticeable exception work Iida et al. (2002) apply RL seek reach
targets, push boxes (Shibata & Iida, 2003) real robots. work, raw visual
signals directly feed neural network trained actor-critic architecture.
examples, visual signal downscaled averaged monochrome (i.e. two-color)
image 64 24 = 1536 pixels. output four infrared sensors added
perceptual input. approach effective specific tasks, process
used highly controlled environment. Real-world images much richer
could undergo strong reduction size.
1.3 Local-Appearance Paradigm
paper, propose algorithms rely extraction visual features
way achieve compact state spaces used input traditional RL
algorithms. Indeed, buried noise confusion visual cues, images contain
hints regularity. regularities captured important notion visual features.
Loosely speaking, visual feature representation aspect local appearance,
e.g. corner formed two intensity edges, spatially localized texture signature,
color. Therefore, analyze images, often sufficient computer program extract
useful information visual signal, focusing attention robust highly
351

fiJodogne & Piater

percepts

Image Classifier

reinforcements

detected visual class
informative visual features

Reinforcement Learning

actions

Figure 1: structure Reinforcement Learning Visual Classes.

informative patterns percepts. program thereafter seek characteristic
appearance observed scenes objects.
actually basic postulate behind local-appearance methods
much success computer vision applications image matching, image retrieval
object recognition (Schmid & Mohr, 1997; Lowe, 2004). rely detection
discontinuities visual signal thanks interest point detectors (Schmid, Mohr, &
Bauckhage, 2000). Similarities images thereafter identified using local description
neighborhood around interest points (Mikolajczyk & Schmid, 2003): two images
share sufficient number matching local descriptors, considered belong
visual class.
Local-appearance techniques time powerful flexible,
robust partial occlusions, require segmentation 3D models scenes.
seems therefore promising introduce, front RL algorithm, feature-based
image classifier partitions visual space finite set distinct visual classes
according local-appearance paradigm, focusing attention agent
highly distinctive local descriptors located interest points visual stimuli.
symbol corresponding detected visual class could given input
classical, embedded RL algorithm, shown Figure 1.
preprocessing step intended reduce size input domain, thus enhancing rate convergence, generalization capabilities well robustness
RL noise visual domains. Importantly, family visual features
applied wide variety visual tasks, thus preprocessing step essentially general
task-independent. central difficulty dynamic selection discriminative
visual features. selection process group images share similar, task-specific
properties together visual class.
1.4 Contributions
key technical contribution paper consists introduction reinforcement
learning algorithms used perceptual space contains images.
developed algorithms rely task-specific pre-treatment. consequence,
used vision-for-action task formalized Markov Decision
Problem. review three major contributions discussed paper.
352

fiClosed-Loop Learning Visual Control Policies

1.4.1 Adaptive Discretization Visual Space
first contribution propose new algorithm called Reinforcement Learning Visual
Classes (RLVC) combines aforementioned ideas. RLVC iterative algorithm
suitable learning direct image-to-action mappings taking advantage
local-appearance paradigm. consists two simultaneous, interleaved learning processes:
Reinforcement learning mapping visual classes actions, incremental building
feature-based image classifier.
Initially, image classifier contains one single visual class, images
mapped class. course, introduces kind perceptual aliasing (or hidden
state) (Whitehead & Ballard, 1991): optimal decisions cannot always made, since
percepts requiring different reactions associated class. agent
isolates aliased classes. Since external supervisor, agent rely
statistical analysis earned reinforcements. detected aliased class, agent
dynamically selects new visual feature distinctive, i.e. best disambiguates
aliased percepts. extracted local descriptor used refine classifier. way,
stage algorithm, number visual classes classifier grows. New
visual features learned perceptual aliasing vanishes. resulting image classifier
finally used control system.
approach primarily motivated strong positive results McCallums U-Tree
algorithm (McCallum, 1996). essence, RLVC adaptation U-Tree visual spaces,
though internals algorithms different. originality RLVC lies
exploitation successful local-appearance features. RLVC selects subset highly
relevant features fully closed-loop, purposive learning process. show
algorithm practical interest, successfully applied several simulated
visual navigation tasks.
1.4.2 Compacting Visual Policies
greedy nature, RLVC prone overfitting. Splitting one visual class
potentially improve control policy visual classes. Therefore, splitting
strategy get stuck local minima: split made subsequently proves
useless, cannot undone original description RLVC. second contribution
provide RLVC possibility aggregating visual classes share similar
properties. least three potential benefits:
1. Useless features discarded, enhances generalization capabilities;
2. RLVC reset search good features;
3. number samples embedded RL algorithm disposal
visual class increased, results better visual control policies.
Experiments indeed show improvement generalization abilities, well reduction number visual classes selected features.
353

fiJodogne & Piater

1.4.3 Spatial Combinations Visual Features
Finally, efficacy RLVC clearly depends discriminative power visual
features. power insufficient, algorithm able completely remove
aliasing, produce sub-optimal control policies. Practical experiments
simulated visual navigation tasks exhibit deficiency, soon number detected
visual features reduced features made similar using less sensitive
metric. Now, objects encountered world composed number distinct
constituent parts (e.g. face contains nose two eyes, phone possesses keypad).
parts recursively composed sub-parts (e.g. eye contains
iris eyelashes, keypad composed buttons). hierarchical physical structure
certainly imposes strong constraints spatial disposition visual features.
third contribution show highly informative spatial combinations visual
features iteratively constructed framework RLVC. result promising
permits construction features increasingly higher levels discriminative
power, enabling us tackle visual tasks unsolvable using individual point features
alone. best knowledge, extension RLVC appears
first attempt build visual feature hierarchies closed-loop, interactive purposive
learning process.

2. Overview Reinforcement Learning
framework relies theory RL, introduced section. RL,
environment traditionally modeled Markov Decision Process (MDP). MDP
tuple hS, A, , Ri, finite set states, finite set actions,
probabilistic transition function S, R reinforcement function
R. MDP obeys following discrete-time dynamics: time t, agent
takes action environment lies state st , agent perceives numerical
reinforcement rt+1 = R(st , ), reaches state st+1 probability (st , , st+1 ).
Thus, point view agent, interaction environment defined
quadruple hst , , rt+1 , st+1 i. Note definition Markov decision processes
assumes full observability state space, means agent able
distinguish states environment using sensors. allows us
talk indifferently states percepts. visual tasks, set images.
percept-to-action mapping fixed probabilistic function : 7 states
actions. percept-to-action mapping tells agent probability
choose action faced percept. RL terminology, mapping called
stationary Markovian control policy. infinite sequence interactions starting
state st , discounted return
Rt =


X

rt+i+1 ,

(1)

i=0

[0, 1[ discount factor gives current value future reinforcements. Markov decision problem given MDP find optimal percept-to-action
mapping maximizes expected discounted return, whatever starting state is.
354

fiClosed-Loop Learning Visual Control Policies

possible prove problem well-defined, optimal percept-to-action
mapping always exists (Bellman, 1957).
Markov decision problems solved using Dynamic Programming (DP) algorithms
(Howard, 1960; Derman, 1970). Let percept-to-action mapping. Let us call
state-action value function Q (s, a) , function giving state
action expected discounted return obtained starting state s, taking action
a, thereafter following mapping :
Q (s, a) = E {Rt | st = s, = a} ,

(2)

E denotes expected value agent follows mapping . Let us define
H transform Q functions Q functions
X
(HQ)(s, a) = R(s, a) +
(s, a, s0 ) max
Q(s0 , a0 ),
(3)
0
s0



A. Note H transform equally referred Bellman
backup operator state-action value functions. optimal mappings given MDP
share Q function, denoted Q called optimal state-action value function,
always exists satisfies Bellmans so-called optimality equation (Bellman, 1957)
HQ = Q .

(4)

optimal state-action value function Q known, optimal deterministic perceptto-action mapping easily derived choosing
(s) = argmax Q (s, a),

(5)

aA

S. Another useful concept DP theory optimal value
function V . state S, V (s) corresponds expected discounted return
agent always chooses optimal action encountered state, i.e.
V (s) = max Q (s, a).
aA

(6)

Dynamic Programming includes well-known Value Iteration (Bellman, 1957), Policy Iteration (Howard, 1960) Modified Policy Iteration (Puterman & Shin, 1978) algorithms. Value Iteration learns optimal state-action value function Q , whereas Policy
Iteration Modified Policy Iteration directly learn optimal percept-to-action mapping.
RL set algorithmic methods solving Markov decision problems
underlying MDP known (Bertsekas & Tsitsiklis, 1996; Kaelbling et al., 1996; Sutton
& Barto, 1998). Precisely, RL algorithms assume knowledge R.
input RL algorithms basically sequence interactions hst , , rt+1 , st+1 agent
environment. RL techniques often divided two categories:
1. Model-based methods first build estimate underlying MDP (e.g.
computing relative frequencies appear sequence interactions),
use classical DP algorithms Value Policy Iteration;
2. Model-free methods SARSA (Rummery & Niranjan, 1994), D() (Barto,
Sutton, & Anderson, 1983; Sutton, 1988), popular Q-learning (Watkins,
1989), compute estimate.
355

fiJodogne & Piater

3. Reinforcement Learning Visual Classes
discussed Introduction, propose insert image classifier RL
algorithm. classifier maps visual stimuli set visual classes according
local-appearance paradigm, focusing attention agent highly distinctive
local descriptors detected interest points images.
3.1 Incremental Discretization Visual Space
Formally, let us call D, infinite set local descriptors spanned
chosen local description method. elements equivalently referred visual
features. Usually, corresponds Rn n 1. assume existence visual
feature detector , Boolean function : 7 B testing whether given image
exhibits given local descriptor one interest points (Schmid et al., 2000).
suitable metric used test similarity two visual features, e.g. Mahalanobis
Euclidean distance.
image classifier iteratively refined. incremental process, natural
way implement image classifiers use binary decision trees. internal
nodes labeled visual feature, presence tested node.
leaves trees define set visual classes, hopefully much smaller
original visual space, upon possible apply directly usual RL
algorithm. classify image, system starts root node, progresses
tree according result feature detector visual feature found
descent, reaching leaf.
summarize, RLVC builds sequence C0 , C1 , C2 , . . . growing decision trees,
sequence attempts remove perceptual aliasing. initial classifier C0 maps
input images single visual class V0,1 . stage k, classifier Ck partitions
visual perceptual space finite number mk visual classes {Vk,1 , . . . , Vk,mk }.
3.2 Learning Architecture
resulting learning architecture called Reinforcement Learning Visual Classes
(RLVC) (Jodogne & Piater, 2005a). basic idea behind algorithms, namely
iterative learning decision tree, primarily motivated adaptive-resolution techniques
previously introduced reinforcement learning, notably McCallums
U-Tree algorithm (McCallum, 1996). section, idea showed extremely
fruitful suitably adapted visual spaces. links RLVC adaptiveresolution techniques thoroughly discussed Section 3.6.
components RLVC depicted Figure 2. in-depth discussion
components given next sections. time being, review
them:
RL algorithm: classifier sequence, arbitrary, standard RL algorithm
applied. provides information optimal state-action function,
optimal value function optimal policy induced current classifier
Ck . purpose computations, either new interactions acquired,
356

fiClosed-Loop Learning Visual Control Policies

Figure 2: different components RLVC algorithm.
database previously collected interactions exploited. component
covered Sections 3.3.1 3.3.2.
Aliasing detector: agent learned visual classes required complete
task, viewpoint embedded RL algorithm, input space
partially observable. aliasing detector extracts classes perceptual
aliasing occurs, analysis Bellman residuals. Indeed, explained
Section 3.3.3, exist tight relations perceptual aliasing Bellman
residuals. aliased class detected, RLVC stops.
Feature generator: applied RL algorithm, database interactions
hst , , rt+1 , st+1 available. feature generator component produces set F
candidate visual features aliased class Vk,i . features used
refine classifier chosen among set candidates. step
exposed Sections 3.4 5.
Feature selector: set candidate features F built aliased visual
class Vk,i , component selects visual feature f F best reduces
perceptual aliasing. candidate feature discriminant, component returns
conventional bottom symbol . feature selector RLVC described
Section 3.4.
Classifier refinement: leaves correspond aliased classes featurebased image classifier replaced internal node testing presence absence
selected visual features.
Post-processing: optional component invoked every refinement, corresponds techniques fighting overfitting. Details given Section 4.
general outline RLVC described Algorithm 1. Note experiments contained paper, model-based RL algorithms applied static
357

fiJodogne & Piater

Algorithm 1 General structure RLVC
1: k 0
2: mk 1
3: Ck binary decision tree one leaf
4: repeat
5:
Collect N interactions hst , , rt+1 , st+1
6:
Apply arbitrary RL algorithm sequence mapped Ck
7:
Ck+1 Ck
8:
{1, . . . , mk }
9:
aliased(Vk,i )
10:
F generator ({st | Ck (st ) = Vk,i })
11:
f selector (Vk,i , F )
12:
f 6=
13:
Ck+1 , refine Vk,i testing f
14:
mk+1 mk+1 + 1
15:
end
16:
end
17:
end
18:
k k+1
19:
post-process(Ck )
20: Ck = Ck1
databases interactions fifth step Algorithm 1. databases collected
using fully randomized exploration policy. choice guided ease
implementation presentation; way collecting experience could used
well, example re-sampling new database interactions iteration RLVC.
crucial point RLVC generates representation visual control policies
set collected visuomotor experience, makes RLVC interactive. following sections describe remaining algorithms, namely aliased, generator, selector
post-process.
3.3 Detection Aliased Visual Classes
discuss aliasing detected classifier Ck .
3.3.1 Projection MDP Image Classifier
Formally, image classifier Ck converts sequence N interactions
hst , , rt+1 , st+1 i,
mapped sequence N quadruples
hCk (st ), , rt+1 , Ck (st+1 )i,
upon embedded RL algorithm applied. Let us define mapped MDP Mk
MDP
hSk , A, Tk , Rk i,
358

fiClosed-Loop Learning Visual Control Policies

obtained mapped sequence, Sk set visual classes
known Ck , Tk Rk computed using relative frequencies
mapped sequence, follows.
Consider two visual classes V, V 0 {Vk,1 , . . . , Vk,mk } one action A. define
following functions:
(V, a) equals 1 Ck (st ) = V = a, 0 otherwise;
(V, a, V 0 ) equals 1 Ck (st ) = V , Ck (st+1 ) = V 0 = a, 0 otherwise;
(V, a) number ts (V, a) = 1.
Using notation, write:
Sk = {Vk,1 , . . . , Vk,mk };
P
0
Tk (V, a, V 0 ) = N
t=1 (V, a, V ) /(V, a);
P
Rk (V, a) = N
t=1 rt (V, a)/(V, a).
3.3.2 Optimal Q Function Mapped MDP
mapped MDP Mk induces optimal Q function domain Sk
0
denoted Q0
k . Computing Qk difficult: general, may exist MDP defined
state space Sk action space generate given mapped sequence,
since latter necessarily Markovian anymore. Thus, RL algorithm run
mapped sequence, might converge toward Q0
k , even converge all.
However, applied mapped sequence, model-based RL method (cf. Section 2)
used compute Q0
k Mk used underlying model. conditions,
Q-learning converges optimal Q function mapped MDP (Singh, Jaakkola,
& Jordan, 1995).
turn, function Q0
k induces another Q function initial domain
relation:
Qk (s, a) = Q0
(7)
k (Ck (s), a) ,
absence aliasing, agent would perform optimally, Qk would correspond
Q , according Bellman theorem states uniqueness optimal Q function (cf.
Section 2). Equation 4, function
Bk (s, a) = (HQk )(s, a) Qk (s, a)

(8)

therefore measure aliasing induced image classifier Ck . RL terminology,
Bk Bellman residual function Qk (Sutton, 1988). basic idea behind RLVC
refine states non-zero Bellman residual.
3.3.3 Measuring Aliasing
Consider time stamp database interactions hst , , rt+1 , st+1 i. According
Equation 8, Bellman residual corresponds state-action pair (st , ) equals
X
Bk (st , ) = R(st , ) +
(st , , s0 ) max
Qk (s0 , a0 ) Qk (st , ).
(9)
0


s0

359

fiJodogne & Piater

Algorithm 2 Aliasing Criterion
1: aliased(Vk,i ) :
2:

3:
{t | Ck (st ) = Vk,i = a}
4:
2 () >
5:
return true
6:
end
7:
end
8:
return false
Unfortunately, RL agent access transition probabilities
reinforcement function R MDP modeling environment. Therefore, Equation 9 cannot directly evaluated. similar problem arises Q-learning (Watkins,
1989) Fitted Q Iteration (Ernst, Geurts, & Wehenkel, 2005) algorithms.
algorithms solve problem considering stochastic version time difference
described Equation 9: value
X
(st , , s0 ) max
Qk (s0 , a0 )
(10)
0


s0

indeed estimated
max
Qk (s0 , a0 ),
0


(11)

successor s0 chosen probability (st , , s0 ). following transitions
environment ensures making transition st st+1 probability (st , , st+1 ).
Thus
= rt+1 + max
Qk (st+1 , a0 ) Qk (st , )
a0

0
0
= rt+1 + max
Q0
k Ck (st+1 ) , Qk (Ck (st ), a)
0


(12)
(13)

unbiased estimate Bellman residual state-action pair (st , ) (Jaakkola,
Jordan, & Singh, 1994).1 importantly, system deterministic absence
perceptual aliasing, estimates equal zero. Therefore, nonzero potentially
indicates presence perceptual aliasing visual class Vt = Ck (st ) respect
action . criterion detecting aliased classes consists computing Q0
k
function, sweeping interactions hst , , rt+1 , st+1 identify nonzero
. practice, assert presence aliasing variance exceeds given
threshold . summarized Algorithm 2, 2 () denotes variance set
samples.
3.4 Generation Selection Distinctive Visual Features
aliasing detected visual class Vk,i Sk respect action a,
need select local descriptor best explains variations set values
1. worth noticing corresponds updates would applied Q-learning,
known learning rate time t.

360

fiClosed-Loop Learning Visual Control Policies

Algorithm 3 Canonical feature generator
1: generator({s1 , . . . , sn }) :
2:
F {}
3:
{1, . . . , n}
4:
(x, y) (x, y) interest point si
5:
F F {symbol(descriptor(si , x, y))}
6:
end
7:
end
8:
return F
corresponding Vk,i a. local descriptor chosen among set candidate
visual features F .
3.4.1 Extraction Candidate Features
Informally, canonical way building F visual class Vk,i consists in:
1. identifying collected visual percepts st Ck (st ) = Vk,i ,
2. locating interest points selected images st ,
3. adding F local descriptor interest points.
corresponding feature generator detailed Algorithm 3. latter algorithm,
descriptor(s, x, y) returns local description point location (x, y) image s,
symbol(d) returns symbol corresponds local descriptor F according
used metric. However, complex strategies generating visual features
used. strategy builds spatial combinations individual point features
presented Section 5.
3.4.2 Selection Candidate Features
problem choosing candidate feature reduces variations set
real-valued Bellman residuals regression problem, suggest adaptation
popular splitting rule used CART algorithm building regression trees (Breiman,
Friedman, & Stone, 1984).2
CART, variance used impurity indicator: split selected refine
particular node one leads greatest reduction sum squared
differences response values learning samples corresponding node
mean. formally, let = {hxi , yi i} set learning samples, xi Rn
input vectors real numbers, yi R real-valued outputs. CART selects
following candidate feature:


v
v
f = argmin pv 2
+ pv 2
,

(14)

vF

2. Note previous work, used splitting rule borrowed building classification
trees (Quinlan, 1993; Jodogne & Piater, 2005a).

361

fiJodogne & Piater

Algorithm 4 Feature Selection
1: selector(Vk,i , F ) :
2:
f
{Best feature found far}
3:
r +
{Variance reduction induced f }
4:

5:
{t | Ck (st ) = Vk,i = a}
6:
visual feature f F
7:
{t | st exhibits f }
8:
{t | st exhibit f }
9:
|S |/|T |
10:
|S |/|T |
11:
r 2 (S ) + 2 (S )
12:
r < r distributions (S , ) significantly different
13:
f f
14:
r
15:
end
16:
end
17:
end
18:
return f

pv (resp. pv ) proportion samples exhibit (resp. exhibit)
v (resp. v ) set samples exhibit (resp. exhibit)
feature v,

feature v. idea directly transferred framework, set xi
corresponds set interactions hst , , rt+1 , st+1 i, set yi corresponds
set . written explicitly Algorithm 4.
algorithms exploit stochastic version Bellman residuals. course, real environments general non-deterministic, generates variations Bellman residuals
consequence perceptual aliasing. RLVC made somewhat robust
variability introducing statistical hypothesis test: candidate feature,
Students ttest used decide whether two sub-distributions feature induces
significantly different. approach used U-Tree (McCallum, 1996).
3.5 Illustration Simple Navigation Task
evaluated system abstract task closely parallels real-world scenario
avoiding unnecessary complexity. consequence, sensor model use may
seem unrealistic; better visual sensor model exploited Section 4.4.
RLVC succeeded solving continuous, noisy visual navigation task depicted
Figure 3. goal agent reach fast possible one two exits
maze. set possible locations continuous. location, agent four
possible actions: Go up, right, down, left. Every move altered Gaussian noise,
standard deviation 2% size maze. Glass walls present
maze. Whenever move would take agent wall outside maze, location
changed.
362

fiClosed-Loop Learning Visual Control Policies

Figure 3: continuous, noisy navigation task. exits maze indicated boxes
cross. Walls glass identified solid lines. agent depicted
center figure. one four possible moves represented
arrow, length corresponds resulting move. sensors return
picture corresponds dashed portion image.

agent earns reward 100 exit reached. move, including
forbidden ones, generates zero reinforcement. agent succeeds escaping
maze, arrives terminal state every move gives rise zero reinforcement.
task, set 0.9. Note agent faced delayed reward problem,
must take distance two exits consideration choosing
attractive one.
maze ground carpeted color image 1280 1280 pixels
montage pictures COIL-100 database (Nene, Nayar, & Murase, 1996).
agent direct access (x, y) position maze. Rather, sensors
take picture surrounding portion ground. portion larger
blank areas, makes input space fully observable. Importantly, glass walls
transparent, sensors return portions tapestry behind
them. Therefore, way agent directly locate walls. obliged
identify regions maze action change location.
363

fiJodogne & Piater

Figure 4: deterministic image-to-action mapping results RLVC, sampled
regularly-spaced points. manages choose correct action location.

experiment, used color differential invariants visual features (Gouet
& Boujemaa, 2001). entire tapestry includes 2298 different visual features. RLVC
selected 200 features, corresponding ratio 9% entire set possible features.
computation stopped generation 84 image classifiers (i.e. k reached
84), took 35 minutes 2.4GHz Pentium IV using databases 10,000 interactions.
205 visual classes identified. small number, compared number
perceptual classes would generated discretization maze agent
knows (x, y) position. example, reasonably sized 2020 grid leads 400 perceptual
classes.
Figure 4 shows optimal, deterministic image-to-action mapping results
last obtained image classifier Ck :
(s) = argmax Qk (s, a) = Q0
k (Ck (s), a) .
aA

364

(15)

fiClosed-Loop Learning Visual Control Policies

(a)

(b)

Figure 5: (a) optimal value function, agent direct access (x, y)
position maze set possible locations discretized
50 50 grid. brighter location, greater value. (b) final value
function obtained RLVC.

Figure 5 compares optimal value function discretized problem one obtained RLVC. similarity two pictures indicates soundness
approach. Importantly, RLVC operates neither pretreatment, human intervention. agent initially aware visual features important task.
Moreover, interest selecting descriptors clear application: direct, tabular
representation Q function considering Boolean combinations features would
22298 4 cells.
behavior RLVC real-word images investigated. navigation
rules kept identical, tapestry replaced panoramic photograph
3041 384 pixels subway station, depicted Figure 6. RLVC took 101 iterations
compute mapping right Figure 6. computation time 159 minutes
2.4GHz Pentium IV using databases 10,000 interactions. 144 distinct visual features
selected among set 3739 possible ones, generating set 149 visual classes. again,
resulting classifier fine enough obtain nearly optimal image-to-action mapping
task.

3.6 Related Work
RLVC thought performing adaptive discretization visual space
basis presence visual features. Previous reinforcement learning algorithms
exploit presence perceptual features various contexts discussed.
365

fiJodogne & Piater

(a)

(b)

Figure 6: (a) navigation task real-world image, using conventions
Figure 3. (b) deterministic image-to-action mapping computed RLVC.

366

fiClosed-Loop Learning Visual Control Policies

3.6.1 Perceptual Aliasing
explained above, incremental selection set informative visual features necessarily leads temporary perceptual aliasing, RLVC tries remove. generally,
perceptual aliasing occurs whenever agent cannot always take right basis
percepts.
Early work reinforcement learning tackled general problem two distinct
ways: Either agent identifies avoids states perceptual aliasing occurs
(as Lion algorithm, see Whitehead & Ballard, 1991), tries build short-term
memory allow remove ambiguities percepts (as predictive
distinctions approach, see Chrisman, 1992). sketchily, two algorithms detect
presence perceptual aliasing analysis sign Q-learning updates.
possibility managing short-term memory led development Partially
Observable Markov Decision Processes (POMDP) theory (Kaelbling, Littman, & Cassandra,
1998), current state random variable percepts.
Although approaches closely related perceptual aliasing RLVC temporarily introduces, consider exploitation perceptual features. Indeed,
tackle structural problem given control task, and, such, assume
perceptual aliasing cannot removed. consequence, approaches orthogonal
research interest, since ambiguities RLVC generates removed
refining image classifier. fact, techniques tackle lack information inherent used sensors, whereas goal handle surplus information related
high redundancy visual representations.
3.6.2 Adaptive Resolution Finite Perceptual Spaces
RLVC performs adaptive discretization perceptual space autonomous,
task-driven, purposive selection visual features. Work RL incrementally partitions
large (either discrete continuous) perceptual space piecewise constant value
function usually referred adaptive-resolution techniques. Ideally, regions
perceptual space high granularity present needed,
lower resolution used elsewhere. RLVC adaptive-resolution
algorithm. review several adaptive-resolution methods previously
proposed finite perceptual spaces.
idea adaptive-resolution techniques reinforcement learning goes back
G Algorithm (Chapman & Kaelbling, 1991), inspired approaches
discussed below. G Algorithm considers perceptual spaces made
fixed-length binary numbers. learns decision tree tests presence informative
bits percepts. algorithm uses Students t-test determine bit
b percepts mapped given leaf, state-action utilities states
b set significantly different state-action utilities states
b unset. bit found, corresponding leaf split. process repeated
leaf. method able learn compact representations, even though
large number irrelevant bits percepts. Unfortunately, region split,
information associated region lost, makes slow learning.
367

fiJodogne & Piater

Concretely, G Algorithm solve task whose perceptual space contains 2100 distinct
percepts, corresponds set binary numbers length 100 bits.
McCallums U-Tree algorithm builds upon idea combining selective attention
mechanism inspired G Algorithm short-term memory enables agent
deal partially observable environments (McCallum, 1996). Therefore, McCallums
algorithms keystone reinforcement learning, unify G Algorithm (Chapman & Kaelbling, 1991) Chrismans predictive distinctions (Chrisman, 1992).
U-Tree incrementally grows decision tree Kolmogorov-Smirnov tests.
succeeded learning behaviors driving simulator. simulator, percept consists
set 8 discrete variables whose variation domains contain 2 6 values,
leading perceptual space 2, 592 possible percepts. Thus, size perceptual
space much smaller visual space. However, task difficult
physical state space partially observable perceptual space: driving
task contains 21, 216 physical states, means several physical states requiring
different reactions mapped percept sensors agent.
U-Tree resolves ambiguities percepts testing presence perceptual
features percepts encountered previously history system.
end, U-Tree manages short-term memory. paper, partially observable
environments considered. challenge rather deal huge visual spaces,
without hand-tuned pre-processing, difficult, novel research direction.
3.6.3 Adaptive Resolution Continuous Perceptual Spaces
important notice methods adaptive resolution large-scale, finite
perceptual spaces use fixed set perceptual features hard-wired.
distinguished RLVC samples visual features possibly infinite visual feature
space (e.g. set visual features infinite), makes prior assumptions
maximum number useful features. point view, RLVC closer
adaptive-resolution techniques continuous perceptual spaces. Indeed, techniques
dynamically select new relevant features whole continuum.
first adaptive-resolution algorithm continuous perceptual spaces Darling algorithm (Salganicoff, 1993). algorithm, current algorithms
continuous adaptive resolution, splits perceptual space using thresholds.
purpose, Darling builds hybrid decision tree assigns label point
perceptual space. Darling fully on-line incremental algorithm equipped
forgetting mechanism deletes outdated interactions. however limited
binary reinforcement signals, takes immediate reinforcements account,
Darling much closer supervised learning reinforcement learning.
Parti-Game algorithm (Moore & Atkeson, 1995) produces goal-directed behaviors
continuous perceptual spaces. Parti-Game splits regions deems important,
using game-theoretic approach. Moore Atkeson show Parti-Game learn
competent behavior variety continuous domains. Unfortunately, approach
currently limited deterministic domains agent greedy controller
goal state known. Moreover, algorithm searches solution
given task, try find optimal one.
368

fiClosed-Loop Learning Visual Control Policies

Continuous U-Tree algorithm extension U-Tree adapted continuous perceptual spaces (Uther & Veloso, 1998). Darling, Continuous U-Tree
incrementally builds decision tree splits perceptual space finite set hypercubes, testing thresholds. Kolmogorov-Smirnov sum-of-squared-errors used
determine split node decision tree. Pyeatt Howe (2001) analyze
performance several splitting criteria variation Continuous U-Tree.
conclude Students t-test leads best performance, motivates use
statistical test RLVC (cf. Section 3.4).
Munos Moore (2002) proposed Variable Resolution Grids. algorithm
assumes perceptual space compact subset Euclidean space, begins
coarse, grid-based discretization state space. contrast abstract
algorithms section, value function policy vary linearly within region.
Munos Moore use Kuhn triangulation efficient way interpolate value function within regions. algorithm refines approximation refining cells according
splitting criterion. Munos Moore explore several local heuristic measures importance splitting cell including average corner-value differences, variance
corner-value differences, policy disagreement. explore global heuristic measures involving influence variance approximated system. Variable Resolution
Grids probably advanced adaptive-resolution algorithm available far.
3.6.4 Discussion
summarize, several algorithms similar spirit RLVC proposed
years. Nevertheless, work appears first learn direct image-toaction mappings reinforcement learning. Indeed, none reinforcement learning
methods combines following desirable properties RLVC: (1) set relevant perceptual features chosen priori hand, selection process fully
automatic require human intervention; (2) visual perceptual spaces explicitly considered appearance-based visual features; (3) highly informative
perceptual features drawn possibly infinite set.
advantages RLVC essentially due fact candidate visual
features selected informative: ranked according
information-theoretic measure inspired decision tree induction (Breiman et al.,
1984). ranking required, vision-for-action tasks induce large number visual
features (a typical image contains thousand them). kind criterion
ranks features, though already considered Variable Resolution Grids (Munos & Moore,
2002), seems new discrete perceptual spaces.
RLVC defined independently fixed RL algorithm, similar spirit
Continuous U-Tree (Uther & Veloso, 1998), major exception RLVC deals
Boolean features, whereas Continuous U-Tree works continuous input space. Furthermore, version RLVC presented paper uses variance-reduction criterion
ranking visual features. criterion, though already considered Variable Resolution
Grids, seems new discrete perceptual spaces.
369

fiJodogne & Piater

4. Compacting Visual Policies
written Section 1.4.2, original version RLVC subject overfitting (Jodogne &
Piater, 2005b). simple heuristic avoid creation many visual classes simply
bound number visual classes refined stage algorithm,
since splitting one visual class potentially impact Bellman residuals
visual classes. practice, first try split classes samples
considering others, since evidence variance reduction first.
tests, systematically apply heuristics. However, often insufficient taken
alone.
4.1 Equivalence Relations Markov Decision Processes
Since apply embedded RL algorithm stage k RLVC, properties
optimal value function Vk (), optimal state-action value function Qk (, ) optimal
control policy k () known mapped MDP Mk . Using properties, easy
define whole range equivalence relations visual classes. instance,
given threshold R+ , list hereunder three possible equivalence relations pair
visual classes (V, V 0 ):
Optimal Value Equivalence:
|Vk (V ) Vk (V 0 )| .
Optimal Policy Equivalence:
|Vk (V ) Qk (V 0 , k (V ))|
|Vk (V 0 ) Qk (V, k (V 0 ))| .
Optimal State-Action Value Equivalence:
(a A) |Qk (V, a) Qk (V 0 , a)| .
therefore propose modify RLVC that, periodically, visual classes
equivalent respect one criteria merged together. experimentally
observed conjunction first two criteria tends lead best performance.
way, RLVC alternatively splits merges visual classes. compaction phase
done often, order allow exploration. best knowledge,
possibility investigated yet framework adaptive-resolution methods
reinforcement learning.
original version RLVC, visual classes correspond leaves decision
tree. using decision trees, aggregation visual classes achieved
starting bottom tree recursively collapsing leaves, dissimilar leaves
found. operation close post-pruning framework decision trees
machine learning (Breiman et al., 1984). practice, means classes
similar properties, reached one another making number
hops upwards downwards, extremely unlikely matched. greatly reduces
interest exploiting equivalence relations.
drawback due rather limited expressiveness decision trees. decision
tree, visual class corresponds conjunction visual feature literals, defines
370

fiClosed-Loop Learning Visual Control Policies

path root decision tree one leaf. take full advantage equivalence
relations, necessary associate, visual class, arbitrary union conjunctions
visual features. Indeed, exploiting equivalence relations, visual classes
result sequence conjunctions (splitting) disjunctions (aggregation). Thus,
expressive data structure would able represent general, arbitrary Boolean
combinations visual features required. data structure introduced next
section.
4.2 Using Binary Decision Diagrams
problem representing general Boolean functions extensively studied
field computer-aided verification, since abstract behavior logical electronic
devices. fact, whole range methods representing state space richer
richer domains developed last years, Binary Decision Diagram
(BDD) (Bryant, 1992), Number Queue Decision Diagrams (Boigelot, 1999), Upward
Closed Sets (Delzanno & Raskin, 2000) Real Vector Automata (Boigelot, Jodogne, &
Wolper, 2005).
framework, BDD particularly well-suited tool. acyclic graph-based
symbolic representation encoding arbitrary Boolean functions, much success field computer-aided verification (Bryant, 1992). BDD unique
ordering variables fixed, different variable orderings lead different sizes
BDD, since variables discarded reordering process. Although
problem finding optimal variable ordering coNP-complete (Bryant, 1986), automatic heuristics practice find orderings close optimal. interesting
case, since reducing size BDD potentially discards irrelevant variables,
correspond removing useless visual features.
4.3 Modifications RLVC
summarize, extension RLVC use decision trees anymore, assigns
one BDD visual class. Two modifications applied Algorithm 1:
1. operation refining, visual feature f , visual class V labeled
BDD B(V ), consists replacing V two new visual classes V1 V2
B(V1 ) = B(V ) f B(V2 ) = B(V ) f .
2. Given equivalence relation, post-process(Ck ) operation consists merging
equivalent visual classes. merge pair visual classes (V1 , V2 ), V1 V2
deleted, new visual class V B(V ) = B(V1 ) B(V2 ) added. Every
time merging operation takes place, advised carry variable reordering,
minimize memory requirements.
4.4 Experiments
applied modified version RLVC another simulated navigation task.
task, agent moves 11 spots campus University Liege (cf.
Figure 7). Every time agent one 11 locations, body aim four possible
371

fiJodogne & Piater

N

(c) Google Map

Figure 7: Montefiore campus Liege. Red spots corresponds places
agent moves. agent follow links different
spots. goal enter Montefiore Institute, labeled red cross,
gets reward +100.

orientations: North, South, West, East. state space therefore size 11 4 = 44.
agent three possible actions: Turn left, turn right, go forward. goal enter
specific building, obtain reward +100. Turning left right induces
penalty 5, moving forward, penalty 10. discount factor set 0.8.
optimal control policy unique: One depicted Figure 8.
agent direct access position orientation. Rather,
perceives picture area front (cf. Figure 9). Thus, agent
connect input image appropriate reaction without explicitly knowing
geographical localization. possible location possible viewing direction,
database 24 images size 1024 768 significant viewpoint changes
collected. 44 databases randomly divided learning set 18 images
test set 6 images. experimental setup, versions RLVC learn
image-to-action mapping using interactions contain images learning set.
Images test set used assess accuracy learned visual control policies.
SIFT keypoints used visual features (Lowe, 2004). Thresholding
Mahalanobis distance gave rise set 13,367 distinct features. versions RLVC
applied static database 10,000 interactions collected using
fully randomized exploration policy. database used throughout entire
algorithm, database contains images belong learning set.
results basic version RLVC version extended BDDs
reported Figures 10 11. original version RLVC identified 281 visual
classes selecting 264 SIFT features. error rate computed visual policy (i.e.
proportion sub-optimal decisions agent presented possible stimuli)
372

fiClosed-Loop Learning Visual Control Policies

(c) Google Map

Figure 8: One optimal, deterministic control policies Montefiore navigation
task. state, indicated optimal action (the letter F stands
move forward, R turn right L turn left). policy
obtained applying standard RL algorithm scenario
agent direct access (p, d) information.

0.1% learning set 8% images test set used, respect
optimal policy agent direct access position viewing direction.
modified version RLVC applied, one compacting stage every 10 steps.
results clearly superior. error learning set anymore,
error rate test set 4.5%. number selected features reduced 171.
Furthermore, resulting number visual classes becomes 59, instead 281. Thus,
large improvement generalization abilities, well reduction number
visual classes selected features. Interestingly enough, number visual classes
(59) close number physical states (44), tends indicate
algorithm starts learn physical interpretation percepts.
summarize, compacting visual policies probably required step deal realistic visual tasks, iterative splitting process applied. price pay course
373

fiJodogne & Piater

(c) Google Map

Figure 9: percepts agent. Four possible different percepts shown, correspond location viewing direction marked yellow top image.

374

fiClosed-Loop Learning Visual Control Policies

60

60
RLVC
RLVC + BDD
50

40

40

30

0

20

40

60

80
Iterations (k)

100

120

140

Error rate (%)

50

30

20

20

10

10

0
160

0

20

40

60

80
Iterations (k)

100

120

140

Error rate (%)

RLVC
RLVC + BDD

0
160

Figure 10: Comparison error rates basic extended versions RLVC.
error computed policy function step counter k images
learning set (resp. test set) reported left figure (resp.
right).

higher computational cost. Future work focus theoretical justification used
equivalence relations. implies bridging gap theory MDP minimization (Givan, Dean, & Greig, 2003).

5. Learning Spatial Relationships
motivated Introduction (Section 1.4.3), propose extend RLVC constructing hierarchy spatial arrangements individual point features (Jodogne, Scalzo, &
Piater, 2005). idea learning models spatial combinations features takes
roots seminal paper Fischler Elschlager (1973) pictorial structures,
collections rigid parts arranged deformable configurations. idea
become increasingly popular computer vision community 90s, led
large literature modeling detection objects (Amit & Kong, 1996; Burl
& Perona, 1996; Forsyth, Haddon, & Ioffe, 1999). Crandall Huttenlocher (2006) provide pointers recent resources. Among recent techniques, Scalzo Piater (2006)
propose build probabilistic hierarchy visual features represented
acyclic graph. detect presence model Nonparametric Belief
Propagation (Sudderth, Ihler, Freeman, & Willsky, 2003). graphical models
proposed representing articulated structures, pictorial structures (Felzenszwalb & Huttenlocher, 2005; Kumar, Torr, & Zisserman, 2004). Similarly, constellation
model represents objects parts, modeled terms shape appearance
Gaussian probability density functions (Perona, Fergus, & Zisserman, 2003).
work contrasts approaches generation so-called composite
features driven task solved. distinguished techniques
unsupervised learning composite features, since additional information
375

fiJodogne & Piater

300

300
RLVC
RLVC + BDD
250

200

200

150

0

20

40

60

80
Iterations (k)

100

120

140

Number classes

250

150

100

100

50

50

0
160

0

20

40

60

80
Iterations (k)

100

120

140

Number selected features

RLVC
RLVC + BDD

0
160

Figure 11: Comparison number generated classes selected visual features basic extended versions RLVC. number visual classes
(resp. selected features) function step counter k plotted left
figure (resp. right).

embedded inside reinforcement signal drives generation composite features
focusing exploration task-relevant spatial arrangements.
extension RLVC, hierarchy visual features built simultaneously
image classifier. soon sufficiently informative visual feature extracted,
algorithm tries combine two visual features order construct higher level
abstraction, hopefully distinctive robust noise. extension
RLVC assumes co-existence two different kinds visual features:
Primitive Features: correspond individual point features, i.e. localappearance descriptors (cf. Section 1.3).
Composite Features: consist spatial combinations lower-level visual features.
priori bound maximal height hierarchy. Therefore,
composite feature potentially combined primitive feature,
composite feature.
5.1 Detection Visual Features
natural way represent hierarchy use directed acyclic graph G = (V, E),
vertex v V corresponds visual feature, edge (v, v 0 ) E
models fact v 0 part composite feature v. Thus, G must binary,
i.e. vertex either child, exactly two children. set VP leaves
G corresponds set primitive features, set VC internal vertexes
represents set composite features.
leaf vertex vP VP annotated local descriptor D(vP ). Similarly,
internal vertex vC VC annotated constraints relative position
parts. work, consider constraints distances constituent
376

fiClosed-Loop Learning Visual Control Policies

visual features composite features, assume distributed
according Gaussian law G(, ) mean standard deviation . Evidently, richer
constraints could used, taking relative orientation scaling factor constituent features consideration, would require use multivariate Gaussians.
precisely, let vC composite feature, parts v1 v2 . order
trigger detection vC image s, occurrence v1
occurrence v2 relative Euclidean distance sufficient likelihood
generated Gaussian mean (vC ) standard deviation (vC ). ensure
symmetry, location composite feature taken midpoint
locations v1 v2 .
occurrences visual feature v percept found using recursive
Algorithm 5. course, steps 6 7 Algorithm 4, test st exhibit v?
rewritten function Algorithm 5, checking occurrences(v, st ) 6= .
5.2 Generation Composite Features
cornerstone extension RLVC way generating composite features.
general idea behind algorithm accumulate statistical evidence relative
positions detected visual features order find conspicuous coincidences visual
features. done providing evolved implementation generator(s1 , . . . , sn )
one Algorithm 3.
5.2.1 Identifying Spatial Relations
first extract set F (primitive composite) features occur within
set provided images {s1 , . . . , sn }:
F = {v V | (i) si exhibits v} .

(16)

identify pairs visual features occurrences highly correlated within
set provided images {s1 , . . . , sn }. simply amounts counting number
co-occurrences pair features F , keeping pairs corresponding
count exceeds fixed threshold.
Let v1 v2 two features highly correlated. search meaningful
spatial relationship v1 v2 carried images {s1 , . . . , sn }
contain occurrences v1 v2 . co-occurrence, accumulate set
distances corresponding occurrences v1 v2 . Finally, clustering
algorithm applied distribution order detect typical distances v1
v2 . purpose experiments, used hierarchical clustering (Jain,
Murty, & Flynn, 1999). cluster, Gaussian fitted estimating mean value
standard deviation . Finally, new composite feature vC introduced
feature hierarchy, v1 v2 parts (vC ) = (vC ) = .
summary, Algorithm 1, replace call Algorithm 3 call Algorithm 6.
377

fiJodogne & Piater

Algorithm 5 Detecting Composite Features
1: occurrences(v, s) :
2:
v primitive
3:
return {(x, y) | (x, y) interest point s, local descriptor corresponds D(v)}
4:
else
5:
{}
6:
O1 occurrences(subfeature1 (v), s)
7:
O2 occurrences(subfeature2 (v), s)
8:
p
(x1 , y1 ) O1 (x2 , y2 ) O2
9:
(x2 x1 )2 + (y2 y1 )2
10:
G(d (v), (v))
11:
{((x1 + x2 )/2, (y1 + y2 )/2)}
12:
end
13:
end
14:
return
15:
end

Algorithm 6 Generation Composite Features
1: generator({s1 , . . . , sn }) :
2:
F = {v V | (i) si exhibits v}
3:
F 0 = {}
4:
(v1 , v2 ) F F
5:
enough co-occurrences v1 v2 {s1 , . . . , sn }
6:
{}
7:
{1, . . . , n}
8:
occurrences (x1 , y1 ) v1 si
9:
occurrences
(x2 , y2 ) v2 si
p
10:
{ (x2 x1 )2 + (y2 y1 )2 }
11:
end
12:
end
13:
end
14:
Apply clustering algorithm
15:
cluster C = {d1 , . . . dm }
16:
= mean(C)
17:
= stddev(C)
18:
Add F 0 composite feature vC composed v1 v2 , annotated
mean standard deviation
19:
end
20:
end
21:
end
22:
return F 0

378

fiClosed-Loop Learning Visual Control Policies

H(p)
0.4

N

u
0.2

mg
1

.5

.5

1

p

0.2

Figure 12: Car Hill control problem.

5.2.2 Feature Validation
Algorithm 6 generate several composite features given visual class Vk,i . However,
end Algorithm 4, one generated composite feature kept.
important notice performance clustering method critical
purpose. Indeed, irrelevant spatial combinations automatically discarded, thanks
variance-reduction criterion feature selection component. fact, reinforcement
signal helps direct search good feature, advantage unsupervised
methods building feature hierarchies.
5.3 Experiments
demonstrate efficacy algorithms version classical Car Hill
control problem (Moore & Atkeson, 1995), position velocity information
presented agent visually.
episodic task, car (modeled mass point) riding without friction
hill, shape defined function:

H(p) =

p2 p
+p
p < 0,
2
p/ 1 + 5p p 0.

goal agent reach fast possible top hill, i.e. location
p 1. top hill, agent obtains reward 100. car thrust left
right acceleration 4 Newtons. However, gravity, acceleration
insufficient agent reach top hill always thrusting toward right.
Rather, agent go left while, hence acquiring potential energy going
left side hill, thrusting rightward. two constraints: agent
allowed reach locations p < 1, velocity greater 3 absolute
value leads destruction car.
379

fiJodogne & Piater

5.3.1 Formal Definition Task
Formally, set possible actions = {4, 4}, state space = {(p, s) |
|p| 1 |s| 3}. system following continuous-time dynamics:
p =
=




p

1 + H 0 (p)2



gH 0 (p)
,
1 + H 0 (p)2

thrust acceleration, H 0 (p) first derivative H(p), = 1
mass car, g = 9.81 acceleration due gravity. continuous-time
dynamics approximated following discrete-time state update rule:
st+1 = st + hpt + h2 st /2
st+1 = pt + hst ,
h = 0.1 integration time step. reinforcement signal defined
expression:

100 st+1 1 |st+1 | 3,
R((st , st ), a) =
0
otherwise.
setup, discount factor set 0.75.
definition actually mix two coexistent formulations Car Hill
task (Ernst, Geurts, & Wehenkel, 2003; Moore & Atkeson, 1995). major differences
initial formulation problem (Moore & Atkeson, 1995) set
possible actions discrete, goal top hill (rather given
area hill), definition Ernst et al. (2003). ensure existence
interesting solution, velocity required remain less 3 (instead 2),
integration time step set h = 0.1 (instead 0.01).
5.3.2 Inputs Agent
previous work (Moore & Atkeson, 1995; Ernst et al., 2003), agent always assumed
direct access numerical measure position velocity. exception
Gordons work visual, low-resolution representation global scene given
agent (Gordon, 1995). experimental setup, agent provided two
cameras, one looking ground underneath, second velocity gauge. way,
agent cannot directly know current position velocity, suitably interpret
visual inputs derive them.
examples pictures sensors return presented Figure 13.
ground carpeted color image 1280 128 pixels montage pictures
COIL-100 database (Nene et al., 1996). important notice using
individual point features insufficient solving task, since set features
pictures velocity gauge always same. know velocity, agent
generate composite features sensitive distance primitive features cursor
respect primitive features digits.
380

fiClosed-Loop Learning Visual Control Policies

(a)

(b)
Figure 13: (a) Visual percepts corresponding pictures velocity gauge = 3,
= 0.5 = 1.5. (b) Visual percepts returned position sensor.
region framed white rectangle corresponds portion ground
returned sensor p = 0.1. portion slides back forth
agent moves.

5.3.3 Results
experimental setup, used color differential invariants (Gouet & Boujemaa, 2001)
primitive features. Among possible visual inputs (both position
velocity sensors), 88 different primitive features. entire image ground
includes 142 interest points, whereas images velocity gauge include 20
interest points.
output RLVC decision tree defines 157 visual classes. internal
node tree tests presence one visual feature, taken set 91 distinct,
highly discriminant features selected RLVC. Among 91 selected visual features,
56 primitive 26 composite features. Two examples composites features
selected RLVC depicted Figure 15. computation stopped k = 38
refinement steps Algorithm 1.
show efficacy method, compare performance scenario
agent direct perception current (p, s) state. latter scenario,
state space discretized grid 13 13 cells. number 13 chosen since
approximately corresponds square root 157, number visual classes
produced RLVC. way, RL provided equivalent number perceptual classes
two scenarios. Figure 14 compares optimal value function direct-perception
381

fiJodogne & Piater

Velocity

3

0

3
1

0

Position

1

(a)

Velocity

3

0

3
1

0

Position

1

(b)

Figure 14: (a) optimal value function, agent direct access current
(p, s) state, input space discretized 13 13 grid.
brighter location, greater value. (b) value function obtained
RLVC.

382

fiClosed-Loop Learning Visual Control Policies

Figure 15: Two composite features generated, yellow. primitive features
composed marked yellow. first feature triggers
velocities around 0, whereas second triggers around 2.

problem one obtained RLVC. also, two pictures similar,
indicates soundness approach.
evaluated performance optimal image-to-action mapping
= argmax Q ((p, s), a)

(17)

aA

obtained RLVC. purpose, agent placed randomly hill,
initial velocity 0. Then, used mapping choose action, reached
final state. set 10,000 trials carried step k Algorithm 1.
Figure 16 compares proportion trials missed goal (either leaving
hill left, acquiring high velocity) RLVC directperception problem. k became greater 27, proportion missed trials
always smaller RLVC direct-perception problem. advantage favor
RLVC due adaptive nature discretization. Figure 17 compares mean
lengths successful trials. mean length RLVC trials clearly converges
direct-perception trials, staying slightly larger.
conclude, RLVC achieves performance close direct-perception scenario. However, mapping built RLVC directly links visual percepts appropriate actions,
without considering explicitly physical variables.

6. Summary
paper introduces Reinforcement Learning Visual Classes (RLVC). RLVC designed
learn mappings directly connect visual stimuli output actions optimal
surrounding environment. framework RLVC general, sense
applied problem formulated Markov decision problem.
learning process behind algorithms closed-loop flexible. agent
takes lessons interactions environment, according purposive vision
paradigm. RLVC focuses attention embedded reinforcement learning algorithm
highly informative robust parts inputs testing presence absence
local descriptors interest points input images. relevant visual features
383

fiJodogne & Piater

25
RLVC
Direct perception (13x13 grid)

Missed goal (%)

20

15

10

5

0

0

5

10

15

20

25

30

35

40

Iterations (k)

Figure 16: Evolution number times goal missed iterations
RLVC.

19

RLVC
Direct perception (13x13 grid)

18

Mean length interaction

17

16

15

14

13

12

0

5

10

15

20

25

30

35

40

Iterations (k)

Figure 17: Evolution mean lengths successful trials iterations RLVC.

384

fiClosed-Loop Learning Visual Control Policies

incrementally selected sequence attempts remove perceptual aliasing:
discretization process targets zero Bellman residuals inspired supervised learning algorithms building decision trees. algorithms defined independently
interest point detector (Schmid et al., 2000) local description technique (Mikolajczyk & Schmid, 2003). user may choose two components sees fit.
Techniques fighting overfitting RLVC proposed. idea
aggregate visual classes share similar properties respect theory Dynamic
Programming. Interestingly, process enhances generalization abilities learned
image-to-action mapping, reduces number visual classes built.
Finally, extension RLVC introduced allows closed-loop, interactive
purposive learning hierarchy geometrical combinations visual features.
contrast prior work topic, uses either supervised
unsupervised framework (Piater, 2001; Fergus, Perona, & Zisserman, 2003; Bouchard &
Triggs, 2005; Scalzo & Piater, 2006). Besides novelty approach, shown
practical value visual control tasks information provided individual
point features alone insufficient solving task. Indeed, spatial combinations visual
features informative robust noise.

7. Future Work
area applications RLVC wide, since nowadays robotic agents often equipped
CCD sensors. Future research includes demonstration applicability
algorithms reactive robotic application, grasping objects combining visual
haptic feedback (Coelho, Piater, & Grupen, 2001). necessitates extension
techniques continuous action spaces, fully satisfactory solutions exist
date. RLVC could potentially applied Human-Computer Interaction,
actions need physical actions.
closed-loop learning hierarchy visual feature raises interesting research
directions. example, combination RLVC techniques disambiguating
aliased percepts using short-term memory (McCallum, 1996) could solve visual
tasks percepts agent alone provide enough information solving
task. Likewise, unsupervised learning kinds geometrical models (Felzenszwalb & Huttenlocher, 2005) could potentially embedded RLVC. hand,
spatial relationships currently take consideration relative angles
parts composite feature. would increase discriminative power
composite features, requires non-trivial techniques clustering circular domains.

Acknowledgments
authors thank associate editor Thorsten Joachims three anonymous reviewers many suggestions improving quality manuscript. Sebastien
Jodogne gratefully acknowledge financial support Belgian National Fund
Scientific Research (FNRS).
385

fiJodogne & Piater

References
Aloimonos, Y. (1990). Purposive qualitative active vision. Proc. 10th International Conference Pattern Recognition, pp. 436460.
Amit, Y., & Kong, A. (1996). Graphical templates model registration. IEEE Transactions Pattern Analysis Machine Intelligence, 18 (3), 225236.
Asada, M., Noda, S., Tawaratsumida, S., & Hosoda, K. (1994). Vision-based behavior acquisition shooting robot using reinforcement learning. Proc. IAPR/IEEE
Workshop Visual Behaviors, pp. 112118.
Bagnell, J., & Schneider, J. (2001). Autonomous helicopter control using reinforcement
learning policy search methods. Proc. International Conference Robotics
Automation. IEEE.
Barto, A., Sutton, R., & Anderson, C. (1983). Neuronlike adaptive elements
solve difficult learning control problems. IEEE Transactions Systems, Man
Cybernetics, 13 (5), 835846.
Bellman, R. (1957). Dynamic Programming. Princeton University Press.
Bertsekas, D., & Tsitsiklis, J. (1996). Neuro-Dynamic Programming. Athena Scientific.
Boigelot, B. (1999). Symbolic Methods Exploring Infinite State Spaces. Ph.D. thesis,
University Liege, Liege (Belgium).
Boigelot, B., Jodogne, S., & Wolper, P. (2005). effective decision procedure linear
arithmetic integer real variables. ACM Transactions Computational Logic
(TOCL), 6 (3), 614633.
Bouchard, G., & Triggs, B. (2005). Hierarchical part-based visual object categorization.
IEEE Conference Computer Vision Pattern Recognition, Vol. 1, pp. 710715,
San Diego (CA, USA).
Breiman, L., Friedman, J., & Stone, C. (1984).
Wadsworth International Group.

Classification Regression Trees.

Bryant, R. (1986). Graph-based algorithms boolean function manipulation. IEEE Transactions Computers, 8 (35), 677691.
Bryant, R. (1992). Symbolic boolean manipulation ordered binary decision diagrams.
ACM Computing Surveys, 24 (3), 293318.
Burl, M., & Perona, P. (1996). Recognition planar object classes. Proc. IEEE
Conference Computer Vision Pattern Recognition, pp. 223230, San Francisco
(CA, USA).
Chapman, D., & Kaelbling, L. (1991). Input generalization delayed reinforcement learning: algorithm performance comparisons. Proc. 12th International
Joint Conference Artificial Intelligence (IJCAI), pp. 726731, Sydney.
Chrisman, L. (1992). Reinforcement learning perceptual aliasing: perceptual
distinctions approach. National Conference Artificial Intelligence, pp. 183188.
386

fiClosed-Loop Learning Visual Control Policies

Coelho, J., Piater, J., & Grupen, R. (2001). Developing haptic visual perceptual categories reaching grasping humanoid robot. Robotics Autonomous
Systems, special issue Humanoid Robots, 37 (23), 195218.
Crandall, D., & Huttenlocher, D. (2006). Weakly supervised learning part-based spatial
models visual object recognition. Proc. 9th European Conference
Computer Vision.
Delzanno, G., & Raskin, J.-F. (2000). Symbolic representation upward closed sets.
Tools Algorithms Construction Analysis Systems, Lecture Notes
Computer Science, pp. 426440, Berlin (Germany).
Derman, C. (1970). Finite State Markovian Decision Processes. Academic Press, New York.
Ernst, D., Geurts, P., & Wehenkel, L. (2003). Iteratively extending time horizon reinforcement learning. Proc. 14th European Conference Machine Learning, pp.
96107, Dubrovnik (Croatia).
Ernst, D., Geurts, P., & Wehenkel, L. (2005). Tree-based batch mode reinforcement learning.
Journal Machine Learning Research, 6, 503556.
Felzenszwalb, P., & Huttenlocher, D. (2005). Pictorial structures object recognition.
International Journal Computer Vision, 61 (1), 5579.
Fergus, R., Perona, P., & Zisserman, A. (2003). Object class recognition unsupervised
scale-invariant learning. IEEE Conference Computer Vision Pattern Recognition, Vol. 2, pp. 264271, Madison (WI, USA).
Fischler, M., & Elschlager, R. (1973). representation matching pictorial structures. IEEE Transactions Computers, 22 (1), 6792.
Forsyth, D., Haddon, J., & Ioffe, S. (1999). Finding objects grouping primitives. Shape,
Contour Grouping Computer Vision, pp. 302318, London (UK). SpringerVerlag.
Gaskett, C., Fletcher, L., & Zelinsky, A. (2000). Reinforcement learning visual servoing
mobile robot. Proc. Australian Conference Robotics Automation,
Melbourne (Australia).
Gibson, E., & Spelke, E. (1983). development perception. Flavell, J. H., & Markman, E. M. (Eds.), Handbook Child Psychology Vol. III: Cognitive Development
(4th edition)., chap. 1, pp. 276. Wiley.
Givan, R., Dean, T., & Greig, M. (2003). Equivalence notions model minimization
markov decision processes. Artificial Intelligence, 147 (12), 163223.
Gordon, G. (1995). Stable function approximation dynamic programming. Proc.
International Conference Machine Learning, pp. 261268.
Gouet, V., & Boujemaa, N. (2001). Object-based queries using color points interest.
IEEE Workshop Content-Based Access Image Video Libraries, pp. 3036,
Kauai (HI, USA).
Howard, R. (1960). Dynamic Programming Markov Processes. Technology Press
Wiley, Cambridge (MA) New York.
387

fiJodogne & Piater

Huber, M., & Grupen, R. (1998). control structure learning locomotion gaits. 7th
Int. Symposium Robotics Applications, Anchorage (AK, USA). TSI Press.
Iida, M., Sugisaka, M., & Shibata, K. (2002). Direct-vision-based reinforcement learning
real mobile robot. Proc. International Conference Neural Information
Processing Systems, Vol. 5, pp. 25562560.
Jaakkola, T., Jordan, M., & Singh, S. (1994). Convergence stochastic iterative dynamic
programming algorithms. Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.), Advances Neural Information Processing Systems, Vol. 6, pp. 703710. Morgan Kaufmann Publishers.
Jain, A. K., Murty, M. N., & Flynn, P. J. (1999). Data clustering: review. ACM Computing
Surveys, 31 (3), 264323.
Jodogne, S., & Piater, J. (2005a). Interactive learning mappings visual percepts
actions. De Raedt, L., & Wrobel, S. (Eds.), Proc. 22nd International
Conference Machine Learning (ICML), pp. 393400, Bonn (Germany). ACM.
Jodogne, S., & Piater, J. (2005b). Learning, compacting visual policies (extended abstract). Proc. 7th European Workshop Reinforcement Learning (EWRL),
pp. 810, Napoli (Italy).
Jodogne, S., Scalzo, F., & Piater, J. (2005). Task-driven learning spatial combinations
visual features. Proc. IEEE Workshop Learning Computer Vision
Pattern Recognition, San Diego (CA, USA). IEEE.
Kaelbling, L., Littman, M., & Cassandra, A. (1998). Planning acting partially
observable stochastic domains. Artificial Intelligence, 101 (1-2), 99134.
Kaelbling, L., Littman, M., & Moore, A. (1996). Reinforcement learning: survey. Journal
Artificial Intelligence Research, 4, 237285.
Kimura, H., Yamashita, T., & Kobayashi, S. (2001). Reinforcement learning walking
behavior four-legged robot. Proc. 40th IEEE Conference Decision
Control, Orlando (FL, USA).
Kohl, N., & Stone, P. (2004). Policy gradient reinforcement learning fast quadrupedal
locomotion. Proc. IEEE International Conference Robotics Automation, pp. 26192624, New Orleans.
Kumar, M., Torr, P., & Zisserman, A. (2004). Extending pictorial structures object
recognition. Proc. British Machine Vision Conference.
Kwok, C., & Fox, D. (2004). Reinforcement learning sensing strategies. Proc.
IEEE International Conference Intelligent Robots Systems.
Lagoudakis, M., & Parr, R. (2003). Least-squares policy iteration. Journal Machine
Learning Research, 4, 11071149.
Lowe, D. (2004). Distinctive image features scale-invariant keypoints. International
Journal Computer Vision, 60 (2), 91110.
Martnez-Marn, T., & Duckett, T. (2005). Fast reinforcement learning vision-guided
mobile robots. Proc. IEEE International Conference Robotics Automation, pp. 1822, Barcelona (Spain).
388

fiClosed-Loop Learning Visual Control Policies

McCallum, R. (1996). Reinforcement Learning Selective Perception Hidden State.
Ph.D. thesis, University Rochester, New York.
Michels, J., Saxena, A., & Ng, A. (2005). High speed obstacle avoidance using monocular
vision reinforcement learning. Proc. 22nd International Conference
Machine Learning, pp. 593600, Bonn (Germany).
Mikolajczyk, K., & Schmid, C. (2003). performance evaluation local descriptors.
Proc. IEEE Conference Computer Vision Pattern Recognition, Vol. 2,
pp. 257263, Madison (WI, USA).
Moore, A., & Atkeson, C. (1995). parti-game algorithm variable resolution reinforcement learning multidimensional state-spaces. Machine Learning, 21.
Munos, R., & Moore, A. (2002). Variable resolution discretization optimal control. Machine Learning, 49, 291323.
Nene, S., Nayar, S., & Murase, H. (1996). Columbia object image library (COIL-100). Tech.
rep. CUCS-006-96, Columbia University, New York.
Ng, A., Coates, A., Diel, M., Ganapathi, V., Schulte, J., Tse, B., Berger, B., & Liang, E.
(2004). Inverted autonomous helicopter flight via reinforcement learning. Proc.
International Symposium Experimental Robotics.
Paletta, L., Fritz, G., & Seifert, C. (2005). Q-learning sequential attention visual object
recognition informative local descriptors.. Proc. 22nd International
Conference Machine Learning (ICML), pp. 649656, Bonn (Germany).
Paletta, L., & Pinz, A. (2000). Active object recognition view integration reinforcement learning. Robotics Autonomous Systems, 31 (12), 7186.
Peng, J., & Bhanu, B. (1998). Closed-loop object recognition using reinforcement learning.
IEEE Transactions Pattern Analysis Machine Intelligence, 20 (2), 139154.
Perona, P., Fergus, R., & Zisserman, A. (2003). Object class recognition unsupervised
scale-invariant learning. Conference Computer Vision Pattern Recognition,
Vol. 2, p. 264.
Piater, J. (2001). Visual Feature Learning. Ph.D. thesis, University Massachusetts,
Computer Science Department, Amherst (MA, USA).
Puterman, M., & Shin, M. (1978). Modified policy iteration algorithms discounted
Markov decision problems. Management Science, 24, 11271137.
Pyeatt, L., & Howe, A. (2001). Decision tree function approximation reinforcement
learning. Proc. Third International Symposium Adaptive Systems, pp.
7077, Havana, Cuba.
Quinlan, J. (1993). C4.5: Programs Machine Learning. Morgan Kaufmann Publishers
Inc., San Francisco (CA, USA).
Randlv, J., & Alstrm, P. (1998). Learning drive bicycle using reinforcement learning
shaping. Proc. 15th International Conference Machine Learning, pp.
463471, Madison (WI, USA). Morgan Kaufmann.
389

fiJodogne & Piater

Rummery, G., & Niranjan, M. (1994). On-line Q-learning using connectionist sytems. Tech.
rep. CUED/F-INFENG-TR 166, Cambridge University.
Salganicoff, M. (1993). Density-adaptive learning forgetting. Proc. 10th
International Conference Machine Learning, pp. 276283, Amherst (MA, USA).
Morgan Kaufmann Publishers.
Scalzo, F., & Piater, J. (2006). Unsupervised learning dense hierarchical appearance
representations. Proc. 18th International Conference Pattern Recognition,
Hong-Kong.
Schaal, S. (1997). Learning demonstration. Mozer, M. C., Jordan, M., & Petsche,
T. (Eds.), Advances Neural Information Processing Systems, Vol. 9, pp. 10401046.
Cambridge, MA, MIT Press.
Schmid, C., & Mohr, R. (1997). Local greyvalue invariants image retrieval. IEEE
Transactions Pattern Analysis Machine Intelligence, 19 (5), 530535.
Schmid, C., Mohr, R., & Bauckhage, C. (2000). Evaluation interest point detectors.
International Journal Computer Vision, 37 (2), 151172.
Schyns, P., & Rodet, L. (1997). Categorization creates functional features. Journal
Experimental Psychology: Learning, Memory Cognition, 23 (3), 681696.
Shibata, K., & Iida, M. (2003). Acquisition box pushing direct-vision-based reinforcement learning. Proc. Society Instrument Control Engineers Annual
Conference, p. 6.
Singh, S., Jaakkola, T., & Jordan, M. (1995). Reinforcement learning soft state aggregation. Advances Neural Information Processing Systems, Vol. 7, pp. 361368.
MIT Press.
Sudderth, E., Ihler, A., Freeman, W., & Willsky, A. (2003). Nonparametric belief propagation. Proc. IEEE Conference Computer Vision Pattern Recognition,
pp. 605612.
Sutton, R. (1988). Learning predict methods temporal differences. Machine
Learning, 3 (1), 944.
Sutton, R., & Barto, A. (1998). Reinforcement Learning, Introduction. MIT Press.
Takahashi, Y., Takeda, M., & Asada, M. (1999). Continuous valued Q-learning visionguided behavior acquisition. Proc. International Conference Multisensor
Fusion Integration Intelligent Systems, pp. 255260.
Tarr, M., & Cheng, Y. (2003). Learning see faces objects. Trends Cognitive
Sciences, 7 (1), 2330.
Tesauro, G. (1995). Temporal difference learning TD-Gammon. Communications
ACM, 38 (3), 5868.
Uther, W., & Veloso, M. (1998). Tree based discretization continuous state space reinforcement learning. Proc. 15th National Conference Artificial Intelligence
(AAAI), pp. 769774, Madison (WI, USA).
390

fiClosed-Loop Learning Visual Control Policies

Watkins, C. (1989). Learning Delayed Rewards. Ph.D. thesis, Kings College, Cambridge (UK).
Weber, C., Wermter, S., & Zochios, A. (2004). Robot docking neural vision
reinforcement. Knowledge-Based Systems, 17 (24), 165172.
Wettergreen, D., Gaskett, C., & Zelinsky, A. (1999). Autonomous guidance control
underwater robotic vehicle. Proc. International Conference Field
Service Robotics, Pittsburgh (USA).
Whitehead, S., & Ballard, D. (1991). Learning perceive act trial error.
Machine Learning, 7, 4583.
Yin, P.-Y. (2002). Maximum entropy-based optimal threshold selection using deterministic
reinforcement learning controlled randomization. Signal Processing, 82, 993
1006.
Yoshimoto, J., Ishii, S., & Sato, M. (1999). Application reinforcement learning balancing acrobot. Proc. 1999 IEEE International Conference Systems,
Man Cybernetics, pp. 516521.

391


