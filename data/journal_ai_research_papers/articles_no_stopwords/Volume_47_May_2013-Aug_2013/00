Journal Artificial Intelligence Research 47 (2013) 1-34

Submitted 10/2012; published 05/2013

Feature Subset Selection Algorithm Automatic
Recommendation Method
Guangtao Wang
Qinbao Song
Heli Sun
Xueying Zhang

gt.wang@stu.xjtu.edu.cn
qbsong@mail.xjtu.edu.cn
hlsun@mail.xjtu.edu.cn
zhangxueying.725@stu.xjtu.edu.cn

Department Computer Science & Technology,
Xian Jiaotong University, 710049, China

Baowen Xu
Yuming Zhou

bwxu@nju.edu.cn
zhouyuming@nju.edu.cn

Department Computer Science & Technology,
Nanjing University, China

Abstract
Many feature subset selection (FSS) algorithms proposed,
appropriate given feature selection problem. time, far
rarely good way choose appropriate FSS algorithms problem hand. Thus,
FSS algorithm automatic recommendation important practically useful.
paper, meta learning based FSS algorithm automatic recommendation method
presented. proposed method first identifies data sets similar
one hand k -nearest neighbor classification algorithm, distances among
data sets calculated based commonly-used data set characteristics. Then,
ranks candidate FSS algorithms according performance similar
data sets, chooses algorithms best performance appropriate ones.
performance candidate FSS algorithms evaluated multi-criteria metric
takes account classification accuracy selected features,
runtime feature selection number selected features. proposed
recommendation method extensively tested 115 real world data sets 22 wellknown frequently-used different FSS algorithms five representative classifiers.
results show effectiveness proposed FSS algorithm recommendation method.

1. Introduction
Feature subset selection (FSS) plays important role fields data mining
machine learning. good FSS algorithm effectively remove irrelevant redundant
features take account feature interaction. leads insight
understanding data, improves performance learner enhancing
generalization capacity interpretability learning model (Pudil, Novovicova,
Somol, & Vrnata, 1998a; Pudil, Novovicova, Somol, & Vrnata, 1998b; Molina, Belanche,
& Nebot, 2002; Guyon & Elisseeff, 2003; Saeys, Inza, & Larranaga, 2007; Liu & Yu, 2005;
Liu, Motoda, Setiono, & Zhao, 2010).
c
2013
AI Access Foundation. rights reserved.

fiWang, Song, Sun, Zhang, Xu & Zhou

Although large number FSS algorithms proposed, single
algorithm performs uniformly well feature selection problems. Experiments
(Hall, 1999; Zhao & Liu, 2007) confirmed could exist significant differences
performance (e.g., classification accuracy) among different FSS algorithms given
data set. means given data set, FSS algorithms outperform others.
raises practical important question: FSS algorithms
picked given data set? common solution apply candidate FSS algorithms given data set, choose one best performance crossvalidation strategy. However, solution quite time-consuming especially highdimensional data (Brodley, 1993).
purpose addressing problem efficient way, paper, FSS
algorithm automatic recommendation method proposed. assumption underlying
proposed method performance FSS algorithm data set related
characteristics data set. rationality assumption demonstrated
follows:
1) Generally, new FSS algorithm proposed, performance needs extensively evaluated least real world data sets. However, published FSS algorithms
rarely tested identical group data sets (Hall, 1999; Zhao & Liu, 2007; Yu &
Liu, 2003; Dash & Liu, 2003; Kononenko, 1994). is, two algorithms,
usually tested different data. implies performance FSS
algorithm biases data sets.
2) time, famous NFL (No Free Lunch) (Wolpert, 2001) theory tells us
that, particular data set, different algorithms different data-conditioned performance, performance differences vary data sets.
evidences imply relationship performance
FSS algorithm characteristics data sets. paper, intend explore
relationship utilize automatically recommend appropriate FSS algorithm(s)
given data set. recommendation process viewed specific application
meta-learning (Vilalta & Drissi, 2002; Brazdil, Carrier, Soares, & Vilalta, 2008)
used recommend algorithms classification problems (Ali & Smith, 2006; King,
Feng, & Sutherland, 1995; Brazdil, Soares, & Da Costa, 2003; Kalousis, Gama, & Hilario,
2004; Smith-Miles, 2008; Song, Wang, & Wang, 2012).
model relationship, three fundamental issues considered: i)
features (often referred meta-features) used characterize data set; ii)
evaluate performance FSS algorithm identify applicable one(s)
given data set; iii) recommend FSS algorithm(s) new data set.
paper, meta-features, frequently used meta-learning (Vilalta &
Drissi, 2002; Ali & Smith, 2006; King et al., 1995; Brazdil et al., 2003; Castiello, Castellano,
& Fanelli, 2005), employed characterize data sets. time, multi-criteria
metric, takes account classification accuracy classifier
FSS algorithm runtime feature selection number selected features,
used evaluate performance FSS algorithm. Meanwhile, k -NN (k -Nearest
Neighbor) based method proposed recommend FSS algorithm(s) new data set.
2

fiSubset Selection Algorithm Automatic Recommendation

proposed FSS algorithm recommendation method extensively tested
115 real world data sets 22 well-known frequently-used different FSS algorithms
five representative classifiers. results show effectiveness proposed recommendation method.
rest paper organized follows. Section 2 introduces preliminaries.
Section 3 describes proposed FSS algorithm recommendation method. Section 4 provides experimental results. Section 5 conducts sensitivity analysis number
nearest data sets recommendation results. Finally, section 6 summarizes
work draws conclusions.

2. Preliminaries
section, first describe meta-features used characterize data sets. Then,
introduce multi-criteria evaluation metric used measure performance FSS
algorithms.
2.1 Meta-features Data Sets
proposed FSS algorithm recommendation method based relationship
performance FSS algorithms meta-features data sets.
recommendation viewed data mining problem, performance
FSS algorithms meta-features target function input variables,
respectively. Due ubiquity Garbage In, Garbage (Lee, Lu, Ling, & Ko, 1999)
field data mining, selection meta-features crucial proposed
FSS recommendation method.
meta-features measures extracted data sets used
uniformly characterize different data sets, underlying properties reflected.
meta-features conveniently efficiently calculated, related
performance machine learning algorithms (Castiello et al., 2005).
15 years research study improve meta-features proposed
StatLog project (Michie, Spiegelhalter, & Taylor, 1994). number meta-features
employed characterize data sets (Brazdil et al., 2003; Castiello et al., 2005;
Michie et al., 1994; Engels & Theusinger, 1998; Gama & Brazdil, 1995; Lindner & Studer,
1999; Sohn, 1999), demonstrated working well modeling relationship
characteristics data sets performance (e.g., classification accuracy)
learning algorithms (Ali & Smith, 2006; King et al., 1995; Brazdil et al., 2003; Kalousis et al.,
2004; Smith-Miles, 2008). meta-features characterize data sets themselves,
connection learning algorithms types, use model
relationship data sets FSS algorithms.
commonly used meta-features established focusing following three
aspects data set: i) general properties, ii) statistic-based properties, iii) informationtheoretic based properties (Castiello et al., 2005). Table 11 shows details.
1. order compute information-theoretic features, data sets continuous-valued features,
needed, well-known MDL (Minimum Description Length) method Fayyad & Irani criterion
used discretize continuous values.

3

fiWang, Song, Sun, Zhang, Xu & Zhou

Category
General properties

Statistical based properties

Information-theoretic properties

Notation

F


(X, )
Skew(X)
Kurt(X)
H(C)norm
H(X)norm
I(C, X)
I(C, X)max
ENattr
N Sratio

Measure description
Number instances
Number features
Number target concept values
Data set dimensionality, = I/F
Mean absolute linear correlation coefficient possible pairs features
Mean skewness
Mean kurtosis
Normalized class entropy
Mean normalized feature entropy
Mean mutual information class attribute
Maximum mutual information class attribute
Equivalent number features, ENattr = H(C)/M I(C, X)
Noise-signal ratio, N Sratio = (H(X) I(C, X))/M I(C, X)

Table 1: Meta-features used characterize data set
2.2 Multi-criteria Metric FSS Algorithm Evaluation
section, first, classical metrics evaluating performance FSS algorithm
introduced. Then, analyzing user requirements practice application, based
metrics, new user-oriented multi-criteria metric proposed FSS algorithm
evaluation combining metrics together.
2.2.1 Classical Performance Metrics
evaluating performance FSS algorithm, following three metrics
extensively used feature selection literature: i) classification accuracy , ii) runtime
feature selection, iii) number selected features.
1) classification accuracy (acc) classifier selected features used
measure well selected features describe classification problem.
given data set, different feature subsets generally result different classification
accuracies. Thus, reasonable feature subset higher classification accuracy stronger capability depicting classification problem. classification
accuracy reflects ability FSS algorithm identifying salient features
learning.
2) runtime (t) feature selection measures efficiency FSS algorithm
picking useful features. viewed metric measure cost feature
selection. longer runtime, higher expenditure feature selection.
3) number selected features (n) measures simplicity feature selection
results. classification accuracies two FSS algorithms similar, usually
favor algorithm fewer features.
Feature subset selection aims improve performance learning algorithms
usually measured classification accuracy. FSS algorithms higher classification accuracy favor. However, mean runtime
number selected features could ignored. explained following two
considerations:
1) Suppose two different FSS algorithms Ai Aj , given data set D.
classification accuracy Ai slightly greater Aj ,
4

fiSubset Selection Algorithm Automatic Recommendation

runtime Ai number features selected Ai much greater
Aj , Aj often chosen.
2) Usually, prefer use algorithms higher accuracy longer runtime,
lower accuracy shorter runtime. Therefore, need tradeoff classification accuracy runtime feature selection/the number selected
features. example, real-time systems, impossible choose algorithm
high time-consumption even classification accuracy high.
Therefore, necessary allow users making user-oriented performance evaluation
different FSS algorithms. purpose, needed address problem
integrate classification accuracy runtime feature selection number
selected features obtain unified metric. paper, resort multi-criteria
metric explore problem. underlying reason lies multi-criteria metric
successfully used evaluate data mining algorithms considering positive
properties (e.g. classification accuracy) negative ones (e.g. runtime number
selected features) simultaneously (Nakhaeizadeh & Schnabl, 1997, 1998).
comparing two algorithms, besides metrics used evaluate performance,
ratio metric values used. example, suppose A1 A2 two
different FSS algorithms, A1 better A2 terms classification accuracy, i.e.,
acc1 > acc2 2 , ratio acc1 /acc2 > 1 used show A1 better A2 well.
contrary, negative metrics runtime feature selection number
selected features, corresponding ratio < 1 means better algorithm.
Actually, multi-criteria metric adjusted ratio ratios (ARR) (Brazdil et al., 2003),
combines classification accuracy runtime together unified metric,
proposed evaluate performance learning algorithm. extend ARR integrating runtime feature selection number selected features, new
multi-criteria metric EARR (extened ARR) proposed. following discussion,
show new metric EARR inclusive, flexible, easy understand.
2.2.2 Multi-Criteria Metric EARR
Let DSet = {D1 , D2 , , DN } set N data sets, ASet = {A1 , A2 , , }
set FSS algorithms. Suppose accji classification accuracy classifier FSS
algorithm Ai data set Dj (1 , 1 j N ), tji nji denote runtime
number selected features FSS algorithm Ai data set Dj , respectively.
EARR Ai Aj Dk defined
k
EARRD
Ai ,Aj =

accki /acckj
1 + log (tki /tkj ) + log (nki /nkj )

(1 6= j M, 1 k N ),

(1)

user-predefined parameters denote relative importance
runtime feature selection number selected features, respectively.
computation metric EARR based ratios classical FSS algorithm performance metrics, classification accuracy, runtime feature selection
2. acc1 acc2 corresponding classification accuracies algorithms A1 A2 , respectively.

5

fiWang, Song, Sun, Zhang, Xu & Zhou

number selected features. definition know EARR evaluates
FSS algorithm comparing another algorithm. reasonable since
objective assert algorithm good comparing another one instead
focusing performance. example, suppose classifier 70%
classification accuracy data set, get confused whether classifier good
not. However, compare another classifier obtain 90% classification
accuracy data set, definitely say first classifier good
compared second one.
Noted that, practice, runtime difference two different FSS algorithms
usually quite great. Meanwhile, high-dimensional data sets, difference
number selected features two different FSS algorithms great well. Thus,
ratio runtime ratio number selected features usually much
wide ranges classification accuracy. simple ratio runtime
simple ratio number selected features employed, would dominate
value EARR, ratio classification accuracy drowned. order
avoid situation, common logarithm (i.e., logarithm base 10) ratio
runtime common logarithm ratio number selected features
employed.
parameters represent amount classification accuracy user
willing trade 10 times speedup/reduction runtime feature selection/the
number selected features, respectively. allows users choose algorithms
shorter runtime less features acceptable accuracy. illustrated
following example. Suppose accki = (1 + + ) acckj , runtime algorithm Ai
given data set 10 times Aj (i.e., tki = 10 tkj ), number selected
features algorithm Ai 10 times Aj (i.e., nki = 10 nkj ). Then, according
Dk
1
3
k
Eq. (1), EARR
Ai ,Aj = 1, EARR Aj ,Ai = 1(+)2 > 1. case, Aj outperforms
Ai . user prefers fast algorithms less features, Aj his/her choice.
k
value EARR varies around 1. value EARR
Ai ,Aj greater (or equal
k
to, smaller than) EARR
Aj ,Ai indicates Ai better (or equal to,
worse than) Aj .
Eq. (1) directly used evaluate performance two different FSS algorithms.
comparing multiple FSS algorithms, performance algorithm Ai ASet
given data set evaluated metric EARR
Ai defined follows:

EARRD
Ai =

1
1


X

EARRD
Ai ,Aj .

(2)

j=1j6=i

equation shows EARR FSS algorithm Ai arithmetic
mean EARRD
Ai ,Aj Ai algorithm Aj D. is, performance
FSS algorithm Ai ASet evaluated based comparisons algorithms
{ASet {Ai }}. larger value EARR, better corresponding FSS algorithm
given data set D.
3. Since log (x/y) = log (y/x) ( + )2 > 0

6

fiSubset Selection Algorithm Automatic Recommendation

3. FSS Algorithm Recommendation Method
section, first give framework FSS algorithm recommendation. Then,
introduce nearest neighbor based recommendation method detail.
3.1 Framework
Based assumption relationship performance FSS
algorithm given data set data set characteristics (aka meta-features),
proposed recommendation method firstly constructs meta-knowledge database consisting
data set meta-features FSS algorithm performance. that, help
meta-knowledge database, k-NN based method used model relationship
recommend appropriate FSS algorithms new data set.
Therefore, proposed recommendation method consists two parts: Meta-knowledge
Database Construction FSS Algorithm Recommendation. Fig. 1 shows details.
Meta-knowledge database construction
Feature selection
algorithms

Historical
data sets

Performance metric
aquirement
Meta-features
extraction

Performance metrics

Meta-features

FSS algorithm recommendation

New data set
Recommended
FSS algorithms

Metaknowledge
database
Performance
metrics

Meta-features

Meta-features
extraction

Meta-features

Nearest data sets
identification

Top r algorithms
recommendation

Ranks

FSS algorithms
ranking

Nearest
data sets

Metric
collection

Performance
metrics

Figure 1: Framework feature subset selection algorithm recommendation
1) Meta-Knowledge Database Construction
mentioned previously, meta-knowledge database consists meta-features
set historical data sets performance group FSS algorithms them.
foundation proposed recommendation method, effectiveness
recommendations depends heavily database.
meta-knowledge database constructed following three steps. Firstly,
meta-features Table 1 extracted historical data set module Metafeatures extraction. Then, candidate FSS algorithm applied historical data
set. classification accuracy, runtime feature selection number selected
features recorded, corresponding value performance metric EARR
calculated. accomplished module Performance metric calculation. Finally,
data set, tuple, composed meta-features values
performance metric EARR candidate FSS algorithms, obtained added
knowledge database.
2) FSS Algorithm Recommendation
7

fiWang, Song, Sun, Zhang, Xu & Zhou

Based introduction first part Meta-knowledge Database Construction
presented above, learning target meta-knowledge data set EARR values
instead appropriate FSS algorithm. case, demonstrated
researchers usually resort instance-based k -NN (nearest neighbors) methods
variations (Brazdil et al., 2003, 2008) algorithm recommendation. Thus, k -NN
based FSS algorithm recommendation procedure proposed.
recommending FSS algorithms new data set, firstly, meta-features
data set extracted. Then, distance new data set historical
data set calculated according meta-features. that, k nearest data sets
identified, EARR values candidate FSS algorithms k data sets
retrieved meta-knowledge database. Finally, candidate FSS algorithms
ranked according EARR values, algorithm highest EARR
achieves top rank, one second highest EARR gets second rank,
forth, top r algorithms recommended.
3.2 Recommendation Method
recommend appropriate FSS algorithms new data set Dnew based k nearest
data sets, two foundational issues solved: i) identify k nearest
data sets, ii) recommend appropriate FSS algorithms based k data
sets.
1) k nearest data sets identification
k nearest data sets Dnew identified calculating distance Dnew
historical data set based meta-features. smaller distance,
similar corresponding data set Dnew .
order effectively calculate distance two data sets, L1 norm distance
(Atkeson, Moore, & Schaal, 1997) adopted since easy understand calculate,
ability measuring similarity two data sets demonstrated
Brazdil et al. (2003).
Let = <fi,1 , fi,2 , , fi,h > meta-features data set Di , fi,p
value pth feature h length meta-features. L1 norm distance
data sets Di Dj formulated
dist(Di , Dj ) = kFi Fj k1 =

h
X

|fi,p fj,p |.

(3)

p=1

worth noting ranges different meta-features quite different. example,
meta-features introduced Table 1, value normalized class entropy varies
0 1, number instances millions. Thus, meta-features
different ranges directly used calculate distance two data sets, metafeatures large range would dominate distance, meta-features small
range ignored. order avoid problem, 0-1 standardized method (Eq.
(4)) employed make meta-features range [0, 1].
fi,p min (f,p )
,
max (f,p ) min (f,p )
8

(4)

fiSubset Selection Algorithm Automatic Recommendation

fi,p (1 p h) value pth meta-feature data set Di , min (f,p )
max (f,p ) denote minimum maximum values pth meta-feature historical
data sets, respectively.
2) FSS algorithm recommendation
getting k nearest data sets Dnew , performance candidate FSS
algorithms Dnew evaluated according k nearest data sets. Then,
algorithms best performance recommended.

Let Dknn = {D1 , D2 , , Dk } k nearest data sets Dnew EARR Aij
performance metric FSS algorithm Ai data set Dj Dknn (1 j k).
performance Ai new data set Dnew evaluated
knn
EARRD
Ai

=

k
X

j


EARRAij ,

j = dj

1

k
X
dt 1 , dj = dist(Dnew , Dj ).
/

(5)

t=1

j=1

Eq. (5) indicates performance FSS algorithm Ai Dnew evaluated
performance Dknn Dnew . data set Dj Dknn , smaller distance
dj Dnew , similar two data sets. means two data
sets Dp Dq , dp < dq data set Dp similar Dnew , EARR
Ai Dp important evaluating performance Ai Dnew . Thus,
weighted average, takes account relative importance data set Dknn
rather treating data set equally, employed. Moreover, domain machine
learning, reciprocal distance usually used measure similarity.
k
P
j = dj 1 / dt 1 used weight EARR Ai Dj Dknn .
t=1

According EARR candidate FSS algorithm ASet Dnew , rank
candidate FSS algorithms obtained. greater EARR, higher
rank. Then, top r (e.g., r = 3 paper) FSS algorithms picked
appropriate ones new data set Dnew .
Procedure FSSAlgorithmRecommendation shows pseudo-code recommendation.
Time complexity. recommendation procedure consists two parts. first part
(lines 1-3), k nearest data sets given new data set identified. Firstly,
meta-features F extracted function MetaFeatureExtraction(). Then,
k-nearest historical data sets identified function NeighborRecognition() based
distance F meta-features historical data set Di . Suppose
P number instances Q number features given data set
D, time complexity function MetaFeatureExtraction() O(P + Q). function
NeighborRecognition(), time complexity O(n) depends number
historical data sets n. Consequently, time complexity first part O(P +Q)+O(n).
second part (lines 4-8), r FSS algorithms recommended data set
D. Since weights EARRs k nearest data sets obtained directly,
time complexity two steps O(1). time complexity estimating ranking
EARRs algorithms ASet O(k m) + O(m log(m)), k preassigned
number nearest data sets number candidate FSS algorithms.
9

fiWang, Song, Sun, Zhang, Xu & Zhou

Procedure FSSAlgorithmRecommendation
Inputs :
- new given data set;
DSet - {D1 , D2 , , Dn }, historical data sets;
ASet - {A1 , A2 , , }, candidate FSS algorithms;
MetaDataBase - {<Fi , EARRsi >|1 n} EARRs
meta-features EARRs ASet Di , respectively;
k - predefined number nearest neighbors;
r - number recommended FSS algorithms.
Output: RecAlgs - Recommended FSS algorithms
//Part 1: Recognition k nearest data sets
1 F = MetaFeatureExtraction (D);//Extract meta-features
2 MetaFeatureSet = {F1 , F2 , , Fn };//Meta-feature data set DSet
3 Neighbors = NeighborRecognition (k, F, MetaFeatureSet);
//Part 2: Appropriate FSS algorithm recommendation
4 WeightSet = calculate weight data set Neighbors //See Eq. (5)
5 EARRSet = corresponding EARRs data set Neighbors MetaDataBase;
6 Estimate EARR FSS algorithm ASet according WeightSet EARRSet

Eq. (5) rank algorithms ASet based EARRs;
7 RecAlgs = top r FSS algorithms;
8 return RecAlgs;

sum up, time complexity recommendation procedure O(P + Q) + O(n) +
O(km)+O(mlog(m)). practice, data set needs conduct feature selection,
number instances P and/or number features Q much greater
number nearest data sets k number candidate FSS algorithms
m, major time consumption recommendation procedure determined
first part.

4. Experimental Results Analysis
section, experimentally evaluate proposed feature subset selection (FSS)
algorithm recommendation method recommending algorithms benchmark data
sets.
4.1 Benchmark Data Sets
115 extensively-used real world data sets, come different areas Computer,
Image, Life, Biology, Physical Text 4 , employed experiment. sizes
data sets vary 10 24863 instances, numbers features
5 27680.
statistical summary data sets shown Table 2 terms number
instances (denoted I), number features (denoted F) number target
concepts (denoted T).
4. data sets available http://archive.ics.uci.edu/ml/datasets.html, http://
featureselection.asu.edu/datasets.php, http://sci2s.ugr.es/keel/datasets.php, http://www.
upo.es/eps/bigs/datasets.html, http://tunedit.org/repo/Data, respectively.

10

fiSubset Selection Algorithm Automatic Recommendation

Data ID
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58

Data Name
ada agnostic
ada prior
anneal
anneal ORIG
AR10P 130 674
arrhythmia
audiology
autos
balance-scale
breast-cancer
breast-w
bridges version1
bridges version2
car
CLL-SUB-111 111 2856
cmc
colic
colic.ORIG
colon
credit-a
credit-g
cylinder-bands
dermatology
diabetes
ECML90x27679
ecoli
Embryonaldataset C
eucalyptus
flags
GCM Test
gina agnostic
gina prior
gina prior2
glass
grub-damage
heart-c
heart-h
heart-statlog
hepatitis
hypothyroid
ionosphere
iris
kdd ipums la 97-small
kdd ipums la 98-small
kdd ipums la 99-small
kdd JapaneseVowels test
kdd JapaneseVowels train
kdd synthetic control
kr-vs-kp
labor
Leukemia
Leukemia 3c
leukemia test 34x7129
leukemia train 38x7129
lung-cancer
lymph
Lymphoma45x4026+2classes
Lymphoma96x4026+10classes


4562
4562
898
898
130
452
226
205
625
286
699
107
107
1728
111
1473
368
368
62
690
1000
540
366
768
90
336
60
24863
194
46
3468
3468
3468
214
155
303
294
270
155
3772
351
150
7019
7485
8844
5687
4274
600
3196
57
72
72
34
38
32
148
45
96

F
49
15
39
39
675
280
70
26
5
10
10
13
13
7
2857
10
23
28
2001
16
21
40
35
9
27680
8
7130
249
30
16064
971
785
785
10
9
14
14
14
20
30
35
5
61
61
61
15
15
62
37
17
7130
7130
7130
7130
57
19
4027
4027


2
2
6
6
10
16
24
7
3
2
2
6
6
4
3
3
2
2
2
2
2
2
6
2
43
8
2
12
8
14
2
2
10
7
4
5
5
2
2
4
2
3
9
10
9
9
9
6
2
2
2
3
2
2
3
4
2
11

Data ID
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115

Data Name
Lymphoma96x4026+9classes
mfeat-fourier
mfeat-morphological
mfeat-pixel
mfeat-zernike
molecular-biology promoters
monks-problems-1 test
monks-problems-1 train
monks-problems-2 test
monks-problems-2 train
monks-problems-3 test
monks-problems-3 train
mushroom
oh0.wc
oh10.wc
oh15.wc
oh5.wc
pasture
pendigits
PIE10P 210 1520
postoperative-patient-data
primary-tumor
segment
shuttle-landing-control
sick
SMK-CAN-187 187 1815
solar-flare 1
solar-flare 2
sonar
soybean
spectf test
spectf train
spectrometer
spect test
spect train
splice
sponge
squash-stored
squash-unstored
sylva agnostic
sylva prior
TOX-171 171 1538
tr11.wc
tr12.wc
tr23.wc
tr31.wc
tr41.wc
tr45.wc
trains
vehicle
vote
vowel
wap.wc
waveform-5000
white-clover
wine
zoo


96
2000
2000
2000
2000
106
432
124
432
169
432
122
8124
1003
1050
913
918
36
10992
210
90
339
2310
15
3772
187
323
1066
208
683
269
80
531
187
80
3190
76
52
52
14395
14395
171
414
313
204
927
878
690
10
846
435
990
1560
5000
63
178
101

F
4027
77
7
241
48
59
7
7
7
7
7
7
23
3183
3239
3101
3013
23
17
1521
9
18
20
7
30
1816
13
13
61
36
45
45
103
23
23
62
46
25
24
217
109
1538
6430
5805
5833
10129
7455
8262
33
19
17
14
8461
41
32
14
18


9
10
10
10
10
2
2
2
2
2
2
2
2
10
10
10
10
3
10
10
3
22
7
2
2
2
2
3
2
19
2
2
48
2
2
3
3
3
3
2
2
4
9
8
6
7
10
10
2
4
2
11
20
3
4
3
7

Table 2: Statistical summary 115 data sets

4.2 Experimental Setup
order evaluate performance proposed FSS algorithm recommendation
method, verify whether proposed method potentially useful practice,
confirm reproducibility experiments, set experimental study follows.
4.2.1 FSS Algorithms
FSS algorithms grouped two broad categories: Wrapper Filter (Molina et al.,
2002; Kohavi & John, 1997). Wrapper method uses error rate classification
algorithm evaluation function measure feature subset, evaluation
function Filter method independent classification algorithm. accuracy
Wrapper method usually high; however, generality result limited,
computational complexity high. comparison, Filter method generality,
computational complexity low. Due fact Wrapper method
computationally expensive (Dash & Liu, 1997), Filter method usually good choice
11

fiWang, Song, Sun, Zhang, Xu & Zhou

number features large. Thus, focus Filter method
experiment.
number Filter based FSS algorithms proposed handle feature selection
problems. algorithms significantly distinguished i) search method used
generate feature subset evaluated, ii) evaluation measures used assess
feature subset (Liu & Yu, 2005; de Souza, 2004; Dash & Liu, 1997; Pudil, Novovicova,
& Kittler, 1994).
order guarantee generality experimental results, twelve well-known
latest search methods four representative evaluation measures employed.
brief introduction search methods evaluation measures follows.
1) Search methods
i) Sequential forward search (SFS): Starting empty set, sequentially add
feature results highest value objective function current
feature subset.
ii) Sequential backward search (SBS): Starting full set, sequentially eliminate
feature results smallest decrease value objective function
current feature subset.
iii) Bi-direction search (BiS): parallel implementation SFS SBS. searches
feature subset space two directions.
iv) Genetic search (GS): randomized search method performs using simple
genetic algorithm (Goldberg, 1989). genetic algorithm finds feature subset
maximize special output function using techniques inspired natural evolution.
v) Linear search (LS): extension BestFirst search (Gutlein, Frank, Hall, & Karwath, 2009) searches space feature subsets greedy hill-climbing
augmented backtracking facility.
vi) Rank search (RS) (Battiti, 1994): uses feature evaluator (such gain ratio)
rank features. feature evaluator specified, forward selection
search used generate ranking list.
vii) Scatter search (SS) (Garcia Lopez, Garcia Torres, Melian Batista, Moreno Perez,
& Moreno-Vega, 2006): method performs scatter search feature
subset space. starts population many significant diverse feature
subsets, stops assessment criteria higher given threshold
improvement longer.
viii) Stepwise search (SWS) (Kohavi & John, 1997): variation forward search.
step search process, new feature added, test performed
check features eliminated without significant reduction
output function.
ix) Tabu search (TS) (Hedar, Wang, & Fukushima, 2008): proposed combinatorial optimization problems. adaptive memory responsive exploration
combining local search process anti-cycling memory-based rules avoid
trapping local optimal solutions.
12

fiSubset Selection Algorithm Automatic Recommendation

x) Interactive search (Zhao & Liu, 2007): traverses feature subset space
maximizing target function taking consideration interaction among
features.
xi) FCBF search (Yu & Liu, 2003): evaluates features via relevance redundancy analysis, uses analysis results guideline choose features.
xii) Ranker (Kononenko, 1994; Kira & Rendell, 1992; Liu & Setiono, 1995): evaluates
feature individually ranks features values evaluation
metrics.
2) Evaluation measures
i) Consistency (Liu & Setiono, 1996; Zhao & Liu, 2007): kind measure evaluates worth feature subset level consistency target concept
instances projected onto feature subset. consistency
feature subset never lower full feature set.
ii) Dependency (Hall, 1999; Yu & Liu, 2003): kind measure evaluates worth
subset features considering individual predictive ability feature
along degree redundancy among features. FSS methods based
kind measure assume good feature subsets contain features closely
correlated target concept, uncorrelated other.
iii) Distance (Kira & Rendell, 1992; Kononenko, 1994): kind measure proposed based assumption distance instances different target
concepts greater target concepts.
iv) Probabilistic significance (Zhou & Dillon, 1988; Liu & Setiono, 1995): measure
evaluates worth feature calculating probabilistic significance
two-way function, i.e., association feature target concept.
good feature significant association target concept.
pay attention that, besides four evaluation measures,
another basic kind measure: information-based measure (Liu & Yu, 2005; de Souza, 2004;
Dash & Liu, 1997), contemplated experiments. reason demonstrated follow. information-based measure usually conjunction ranker
search method. Thus, FSS algorithms based kind measure usually provide
rank list features instead telling us features relevant learning target. case, preassign particular thresholds FSS algorithms pick
relevant features. However, effective method set thresholds
acknowledged default threshold FSS algorithms. Moreover, unfair
conclude information measure based FSS algorithms assigned threshold
appropriate comparing FSS algorithms. Therefore, kind
FSS algorithm employed experiments.
Based search methods evaluation measures introduced above, 22 different
FSS algorithms obtained. Table 3 shows brief introduction FSS algorithms.
algorithms available data mining toolkit Weka5 (Hall, Frank,
5. http://www.cs.waikato.ac.nz/ml/weka/

13

fiWang, Song, Sun, Zhang, Xu & Zhou

Holmes, Pfahringer, Reutemann, & Witten, 2009), search method INTERACT
implemented based Weka source codes available online6 .
ID
1
2
3
4
5
6
7
8
9
10
11

Search Method
BestFirst + Sequential Forward Search
BestFirst + Sequential Backward Search
BestFirst + Bi-direction Search
Genetic Search
Linear Search
Rank Search
Scatter Search
Stepwise Search
Tabu Search
Interactive Search
BestFirst + Sequential Forward Search

Evaluation Measure
Dependency
Dependency
Dependency
Dependency
Dependency
Dependency
Dependency
Dependency
Dependency
Dependency
Consistency

Notation
CFS-SFS
CFS-SBS
CFS-BiS
CFS-GS
CFS-LS
CFS-RS
CFS-SS
CFS-SWS
CFS-TS
INTERACT-D
Cons-SFS

ID
12
13
14
15
16
17
18
19
20
21
22

Search Method
BestFirst + Sequential Backward Search
BestFirst + Bi-direction Search
Genetic Search
Linear Search
Rank Search
Scatter Search
Stepwise Search
Interactive Search
FCBFsearch
Ranker
Ranker

Evaluation Measure
Consistency
Consistency
Consistency
Consistency
Consistency
Consistency
Consistency
Consistency
Dependency
Distance
Probabilistic Significance

Notation
Cons-SBS
Cons-BiS
Cons-GS
Cons-LS
Cons-RS
Cons-SS
Cons-SWS
INTERACT-C
FCBF
Relief-F
Signific

Table 3: Introduction 22 FSS algorithms
noted algorithms require particular settings certain parameters. purpose allowing researchers confirm results, introduce
parameter settings FSS algorithms. as, FSS algorithms INTERACT-D
INTERACT-C, parameter, c-contribution threshold, used identify
irrelevant features. set threshold 0.0001 suggested Zhao Liu (2007).
FSS algorithm FCBF, set relevance threshold SU (Symmetric Uncertainty) value bN/ log N cth ranked feature suggested Yu Liu (2003). FSS
algorithm Relief-F, set significance threshold 0.01 used Robnik-Sikonja
Kononenko (2003). FSS algorithm Signific, threshold, statistical significance level , used identify irrelevant features. set commonly-used value
0.01 experiment. FSS algorithms conducted Weka environment
default setting(s).
4.2.2 Classification Algorithms
Since actual relevant features real world data sets usually known advance,
impracticable directly evaluate FSS algorithm selected features. Classification
accuracy extensively used metric evaluating performance FSS algorithms,
plays important role proposed performance metric EARR assessing
different FSS algorithms.
However, different classification algorithms different biases. FSS algorithm may
suitable classification algorithms others (de Souza, 2004). fact
affects performance evaluation FSS algorithms.
mind, order demonstrate proposed FSS algorithm recommendation method limited particular classification algorithm, five representative
classification algorithms based different hypotheses employed. bayes-based
Naive Bayes (John & Langley, 1995) Bayes Network (Friedman, Geiger, & Goldszmidt,
1997), information gain-based C4.5 (Quinlan, 1993), rule-based PART (Frank & Witten,
1998), instance-based IB1 (Aha, Kibler, & Albert, 1991), respectively.
Although Naive Bayes Bayes Net bayes-based classification algorithms,
quite different since Naive Bayes proposed based hypoth6. http://www.public.asu.edu/huanliu/INTERACT/INTERACTsoftware.html

14

fiSubset Selection Algorithm Automatic Recommendation

esis features conditional independent (John & Langley, 1995), Bayes Net
takes account feature interaction (Friedman et al., 1997).
4.2.3 Measures Evaluate Recommendation Method
FSS algorithm recommendation application meta-learning. far know,
unified measures evaluate performance meta-learning methods.
order assess proposed FSS algorithm recommendation method, two measures,
recommendation hit ratio recommendation performance ratio, defined.
Let given data set Arec FSS algorithm recommended recommendation method D, two measures introduced follows.
1) Recommendation hit ratio
intuitive evaluation criterion whether recommended FSS algorithm Arec meets
users requirements. is, whether Arec optimal FSS algorithm D, performance Arec significant difference optimal FSS algorithm.
Suppose Aopt represents optimal FSS algorithm D, ASetopt denotes FSS
algorithm set algorithm significant difference Aopt (of course
includes Aopt well). Then, measure named recommendation hit defined assess
whether recommended algorithm Arec effective D.
Definition 1 (Recommendation hit). FSS algorithm Arec recommended data
set D, recommendation hit Hit(Arec , D) defined
(
1, Arec ASetopt
Hit(Arec , D) =
.
(6)
0, otherwise
Hit(Arec , D) = 1 means recommendation effective since recommended
FSS algorithm Arec one algorithms ASetopt D, hit(Arec , D) = 0 indicates
recommended FSS algorithm Arec member ASetopt , i.e., Arec significantly
worse optimal FSS algorithm Aopt D, thus recommendation bad.
Definition 1 know recommendation hit Hit(Arec , D) used evaluate
recommendation method single data set. Thus, extended recommendation
hit ratio evaluate recommendation set data sets, defined follows.
Definition 2 Recommendation hit ratio
G

1 X
Hit Ratio(Arec ) =
Hit(Arec , Di ).
G

(7)

i=1

G number historical data sets, e.g., G = 115 experiment.
Definition 2 represents percentage data sets appropriate FSS algorithms effectively recommended recommendation method. larger value,
better recommendation method.
2) Recommendation performance ratio
recommendation hit ratio reveals whether appropriate FSS algorithm
recommended given data set, cannot tell us margin recommended
algorithm best one. Thus, new measure, recommendation performance ratio
recommendation, defined.
15

fiWang, Song, Sun, Zhang, Xu & Zhou

Definition 3 (Recommendation performance ratio). Let EARRrec EARRopt
performance recommended FSS algorithm Arec optimal FSS algorithm D,
respectively. Then, recommendation performance ratio (RPR) Arec defined
RPR(Arec , D) = EARRrec /EAARopt .

(8)

definition, best performance EARR opt employed benchmark. Without
benchmark, hard determine recommended algorithms good not.
example, suppose EARR Arec 0.59. EARR opt = 0.61,
recommendation effective since EARR Arec close EARR opt . However,
recommendation poor EARR opt = 0.91.
RPR ratio EARR recommended FSS algorithm optimal one.
measures close recommended FSS algorithm optimal one, reveals
relative performance recommended FSS algorithm. value varies 0 1,
larger value RPR, closer performance recommended FSS algorithm
optimal one. recommended algorithm optimal one
RPR = 1.
4.2.4 Values Parameters
paper, multi-criteria metric EARR proposed evaluate performance
FSS algorithm. proposed metric EARR, two parameters established
users express requirements algorithm performance.
experiment, presenting results, two representative value pairs parameters used follows:
1) = 0 = 0. setting represents situation classification
accuracy important. higher classification accuracy selected features,
better corresponding FSS algorithms.
2) 6= 0 6= 0. setting represents situation user tolerate
accuracy attenuation favor FSS algorithms shorter runtime fewer
selected features. experiment, set 10% quite different
= = 0. allows us explore impact two parameters
recommendation results.
Moreover, order explore parameters affect recommended FSS
algorithms terms classification accuracy, runtime number selected features,
different parameters settings provided. Specifically, values vary 0
10% increase 1%.
4.3 Experimental Process
order make sure soundness experimental conclusion guarantee
experiments reported reproducible, part, introduce four crucial processes
used experiments. i) meta-knowledge database construction, ii) optimal
FSS algorithm set identification given data set, iii) Recommendation method validation
iv) sensitivity analysis number nearest data sets recommendations.
1) Meta-knowledge database construction
16

fiSubset Selection Algorithm Automatic Recommendation

Procedure PerformanceEvaluation
Inputs : data = given data set, i.e, one 115 data sets;
learner = given classification algorithm, i.e., one {Naive Bayes, C4.5, PART,
IB1 Bayes Network};
FSSAlgSet = {FSSAlg 1 , FSSAlg 2 , , FSSAlg 22 }, set 22 FSS
algorithms;
Output: EARRset = {EARR 1 , EARR 2 , , EARR 22 }, EARRs 22 FSS
algorithms data;
1 = 5; FOLDS = 10;
2 = 1 22
3
EARR = 0;
4 = 1
5
randomized order data;
6
generate FOLDS bins data;
7
j = 1 FOLDS
8
TestData = bin[j ];
9
TrainData = data- TestData;
10
numberList = Null , runtimeList = Null , accuracyList = Null ;
11
k = 1 22
12
(Subset, runtime) = apply FSSAlg k TrainData;
13
number = |Subset |;
14
RedTestData = reduce TestData according selected Subset;
15
RedTrainData = reduce TrainData according selected Subset;
16
classifier = learner (RedTrainData);
17
accuracy = apply classifier RedTestData;
18
numberList [k ] = number , runtimeList [k ] = runtime, accuracyList [k ] =

accuracy;
19
20
21

k = 1 22
EARR = EARRCompution(accuracyList, runtimeList, numberList, k );
//Compute EARR FSSAlg k jth bin pass according Eqs. (1) (2)
EARR k = EARR k + EARR;

22 1 22
23
EARR = EARR /(M FOLDS );
24 return EARRset;

data set Di (1 115), i) extract meta-features ; ii) calculate
EARRs 22 candidate FSS algorithms stratified 510-fold cross-validation
strategy (Kohavi, 1995), iii) combine meta-features EARR FSS
algorithm together form tuple, finally added meta-knowledge database.
Since extraction meta-features combination meta-features
EARRs straightforward, present calculation EARRs, procedure PerformanceEvaluation shows details.
2) Optimal FSS algorithm set identification
optimal FSS algorithm set given data set Di consists optimal FSS algorithm data set algorithms significant performance difference
optimal one Di .
optimal FSS algorithm set given data set Di obtained via non-parametric
Friedman test (1937) followed Holm procedure test (1988) performance,
17

fiWang, Song, Sun, Zhang, Xu & Zhou

estimated 510 cross validation strategy, 22 FSS algorithms. result
Friedman test shows significant performance difference among
22 FSS algorithms, 22 FSS algorithms added optimal FSS algorithm set.
Otherwise, FSS algorithm highest performance viewed optimal one
added optimal FSS algorithm set. Then, Holm procedure test performed
identify algorithms rest 21 FSS algorithms. algorithms
significant performance differences optimal one added optimal FSS
algorithm set.
reason non-parametric test employed lies difficult
performance values follow normal distribution satisfy variance homogeneous
condition.
Note optimal FSS algorithm sets different settings parameters
different, since values two parameters directly affect required performance
values.
3) Recommendation method validation
leave-one-out strategy used empirically evaluate proposed FSS algorithm recommendation method follows: data set Di (1 115)
viewed test data, i) identify k nearest data sets training data =
{D1 , , Di1 , Di+1 , , D115 } excluding Di ; ii) calculate performance 22 candidate FSS algorithms according Eq. (5) based k nearest data sets value
k determined standard cross-validation strategy, recommend top three
Di ; iii) evaluate recommendations measures introduced section 4.2.3.
4) Sensitivity analysis number nearest data sets recommendations
order explore effect number nearest data sets recommendations provide users empirical method choose value, data set,
possible numbers nearest data sets tested. is, identifying k
nearest data sets given data set, k set 1 number historical data
sets minus 1 (e.g., 114 experiment).
4.4 Results Analysis
section, present recommendation results terms recommended FSS algorithms, hit ratio performance ratio , respectively. Due space limit
paper, list recommendations, instead present results two
significantly different pairs , i.e., ( = 0, = 0) ( = 10%, = 10%).
Afterward, provide experimental results influence user-oriented
parameters recommendations terms classification accuracy, runtime,
number selected features, respectively.
4.4.1 Recommended Algorithms Hit Ratio
Figs. 2, 3, 4, 5 6 show first recommended FSS algorithms 115 data sets
classification algorithms Naive Bayes, C4.5, PART, IB1 Bayes Network
used, respectively.
18

fiSubset Selection Algorithm Automatic Recommendation

figure, two sub-figures corresponding recommendation results
( = 0, = 0) ( = 10%, = 10%), respectively. sub-figure,
denote correctly incorrectly recommended algorithms, respectively.

FSS algorithm ID

Correctly recommended algorithm

Incorrectly recommended algorithm

22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

96

Data set ID

FSS algorithm ID

(a) = 0, = 0
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

96

Data set ID

(b) = 10%, = 10%

Figure 2: FSS algorithms recommended 115 data sets Naive Bayes used

FSS algorithm ID

Correctly recommended algorithm

Incorrectly recommended algorithm

22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

96

Data set ID

FSS algorithm ID

(a) = 0, = 0
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

96

Data set ID

(b) = 10%, = 10%

Figure 3: FSS algorithms recommended 115 data sets C4.5 used
figures, observe that:
1) five classifiers, proposed method effectively recommend appropriate
FSS algorithms 115 data sets.
case ( = 0, = 0), number data sets, whose appropriate FSS
algorithms correctly recommended, 109 115 Naive Bayes, 111 115
C4.5, 109 115 PART, 108 115 IB1, 109 115 Bayes
19

fiWang, Song, Sun, Zhang, Xu & Zhou

FSS algorithm ID

Correctly recommended algorithm

Incorrectly recommended algorithm

22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

96

Data set ID

FSS algorithm ID

(a) = 0, = 0
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

96

Data set ID

(b) = 10%, = 10%

Figure 4: FSS algorithms recommended 115 data sets PART used

FSS algorithm ID

Correctly recommended algorithm

Incorrectly recommended algorithm

22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

96

Data set ID

FSS algorithm ID

(a) = 0, = 0
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

96

Data set ID

(b) = 10%, = 10%

Figure 5: FSS algorithms recommended 115 data sets IB1 used

Network, respectively. states recommendation method effective
classification accuracy important.
case ( = 10%, = 10%), number data sets, whose appropriate FSS
algorithms correctly recommended, 104 115 Naive Bayes, 109 115
C4.5, 110 115 PART, 106 115 IB1, 104 115 Bayes
Network, respectively. indicates recommendation method works well
tradeoff required among classification accuracy, runtime, number
selected features.
2) distribution recommended FSS algorithms 115 data sets different
different parameters settings. distribution relatively uniform ( = 0, = 0),
20

fiSubset Selection Algorithm Automatic Recommendation

FSS algorithm ID

Correctly recommended algorithm

Incorrectly recommended algorithm

22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

96

Data set ID

FSS algorithm ID

(a) = 0, = 0
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

96

Data set ID

(b) = 10%, = 10%

Figure 6: FSS algorithms recommended 115 data sets Bayes Network used

seriously biased algorithm (e.g., 22th FSS algorithm) ( =
10%, = 10%).
phenomenon similar five classifiers. explained follows.
FSS algorithms best classification accuracy distribute 115 data sets
uniformly. Thus, case ( = 0, = 0) users favor accurate classifiers,
distribution recommended FSS algorithms relatively uniform well. However,
exist FSS algorithms run faster (e.g., 22th algorithm Signific)
select fewer features (e.g., 8th algorithm CFS-SWS, 18th algorithm ConsSWS, 22th algorithm Signific) 115 data sets. reason,
case ( = 10%, = 10%) users prefer FSS algorithms less runtime
fewer features, distribution FSS algorithms best performance
115 data sets biased algorithms, recommended FSS algorithms.
3) 22th FSS algorithm performs well 85 115 data sets classifiers
( = 10%, = 10%). seems FSS algorithm generally wellperformed FSS algorithm adopted FSS tasks need
FSS algorithm recommendation. Unfortunately, case. 22th FSS
algorithm still failing perform well quarter 115 data sets.
Yet, recommendation method distinguish data sets effectively
recommend appropriate FSS algorithms them. indicates recommendation
method still necessary case.
Compared ( = 0, = 0), know case due larger
values explained follows. 22 FSS algorithms, although
classification accuracies classifier features selected different,
differences usually bounded. Meanwhile, Eq. (1) know /
set greater bound value, value EARR dominated
runtime/the number selected features. means set
relatively large value, algorithm lower time complexity algorithm
chooses smaller number features recommended, classification
21

fiWang, Song, Sun, Zhang, Xu & Zhou

accuracy selected features ignored. However, know, one
important targets feature selection improve performance learning
algorithms. unreasonable ignore classification accuracy focus
speed simplicity FSS algorithm.
Thus, real applications, values set limit classification accuracies. Generally, / bounded (accmax accmin )/accmin ,
accmax accmin denote maximum minimum classification accuracies, respectively.
Parameter setting
= 0, = 0

= 10%, = 10%


Recommendation
Alg1st
Alg2nd
Alg3rd
Top 3
Alg1st
Alg2nd
Alg3rd
Top 3

Naive Bayes
94.78
83.48
74.78
99.13
90.43
71.30
38.26
99.13

C4.5
96.52
79.13
80.87
98.26
94.78
69.57
45.22
100.0

PART
94.78
92.17
84.35
99.13
94.78
70.43
42.61
100.0

IB1
93.91
77.39
75.65
99.13
92.17
64.35
43.48
100.0

Bayes Network
94.78
83.48
73.91
98.26
90.43
71.30
36.52
99.13

Algx denotes x -th algorithm ranking list recommended Top 3 means top three
algorithms recommended.

Table 4: Hit ratio (%) recommendations five classifiers different settings
(, )
Table 4 shows hit ratio recommended FSS algorithms five classifiers.
observe that:
1) single FSS algorithm recommended, hit ratio first recommended algorithm Alg1st highest, value 96.52% least 90.43%
five classifiers. Thus, Alg1st first choice.
2) top three algorithms recommended, hit ratio 100% least
98.62%. indicates confidence top three algorithms including
appropriate one high. reason top three algorithms
recommended. Moreover, proposed recommendation method reduced
number candidate algorithms three, users pick one fits
his/her specific requirement them.
4.4.2 Recommendation Performance Ratio
Figs. 7 8 show recommendation performance ratio RPR first recommended
FSS algorithm five classifiers ( = 0, = 0) ( = 10%, = 10%),
respectively. two figures observe that, data sets two
settings , RPRs recommended FSS algorithms greater 95%
100% matter classifier used. indicates
FSS algorithms recommended proposed method close optimal one.
Table 5 shows average RPRs 115 data sets five classifiers
different settings (, ). table, classifier, columns Rec Def
shows RPR value corresponding recommended FSS algorithms default FSS
algorithms, respectively. default FSS algorithm frequent best one
115 data sets classifier.
22

fiSubset Selection Algorithm Automatic Recommendation

RPR (%)

Naive Bayes
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

RPR (%)

C4.5
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

RPR (%)

PART
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

RPR (%)

IB1
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

RPR (%)

Bayes Network
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

Figure 7: RPR 1 st recommended FSS algorithm ( = 0, = 0) five
classifiers
RPR (%)

Naive Bayes
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

RPR (%)

C4.5
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

RPR (%)

PART
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

RPR (%)

IB1
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

RPR (%)

Bayes Network
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

Figure 8: RPR 1 st recommended FSS algorithm ( = 10%, = 10%)
five classifiers

23

fiWang, Song, Sun, Zhang, Xu & Zhou

observe average RPRs range 97.32% 98.8% ( = 0,
= 0), 97.82% 98.99% ( = 10%, = 10%), respectively. Moreover,
average RPR recommended FSS algorithms surpasses default FSS
algorithms five different classifiers. means proposed recommendation
method works well greatly fits users performance requirement.
Parameter setting
= 0, = 0
= 10%, = 10%

Naive bayes
Rec
Def
98.24 96.42
98.69 91.95

C4.5
Rec
Def
98.80
98.18
97.82
92.40

PART
Rec
Def
97.61 94.79
97.89 92.40

IB1
Rec
Def
97.32
96.43
98.11
92.35

Bayes Network
Rec
Def
98.37
96.63
98.99
92.43

Table 5: Average RPR (%) 115 data sets five classifiers

Data ID
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58


NB
0.0443
0.0227
0.0118
0.0079
0.0244
0.019
0.0091
0.0111
0.011
0.0091
0.0086
0.0062
0.0068
0.0077
0.0616
0.0099
0.0074
0.0083
0.0102
0.007
0.008
0.0103
0.008
0.0066
0.0088
0.0061
0.0097
0.0083
0.007
0.0273
0.2236
0.2602
0.3691
0.008
0.0084
0.0103
0.0065
0.0084
0.0098
0.0278
0.007
0.0138
0.1219
0.1453
0.1937
0.0225
0.0149
0.0101
0.0228
0.0069
0.0195
0.0202
0.0128
0.0128
0.0075
0.0084
0.0146
0.0432

C4.5
0.0425
0.0131
0.0124
0.0092
0.0253
0.019
0.0093
0.0062
0.0076
0.0087
0.007
0.0068
0.0085
0.0087
0.0582
0.0083
0.0126
0.0077
0.0078
0.0071
0.01
0.0079
0.0083
0.0064
0.0158
0.0068
0.0078
0.0069
0.0132
0.0272
0.2231
0.2616
0.3722
0.0103
0.0068
0.0068
0.0088
0.007
0.0352
0.0243
0.0099
0.006
0.1228
0.1427
0.1955
0.0232
0.0142
0.0125
0.0245
0.0075
0.0201
0.0207
0.0135
0.013
0.0073
0.0073
0.008
0.0464

PART
0.0499
0.0147
0.0123
0.0082
0.0257
0.0221
0.0093
0.0076
0.0072
0.0084
0.0072
0.0065
0.0064
0.0086
0.0575
0.0081
0.0076
0.0128
0.0105
0.007
0.009
0.011
0.0079
0.0067
0.0101
0.0075
0.0113
0.008
0.0093
0.0291
0.2236
0.2605
0.3689
0.0083
0.0065
0.0066
0.0086
0.0084
0.0069
0.0245
0.011
0.0106
0.1214
0.144
0.1972
0.0242
0.0125
0.0101
0.0191
0.0084
0.0196
0.0165
0.0116
0.0197
0.0069
0.007
0.0095
0.0449

IB1
0.0423
0.0137
0.0117
0.0114
0.0246
0.0192
0.0114
0.0066
0.0074
0.0138
0.0075
0.0063
0.0093
0.0084
0.058
0.0082
0.0084
0.0117
0.0067
0.0069
0.0075
0.0135
0.0093
0.0069
0.0082
0.0083
0.0117
0.0082
0.0072
0.0273
0.2239
0.262
0.3689
0.0113
0.0064
0.0066
0.0059
0.0093
0.0067
0.0246
0.007
0.0116
0.1231
0.145
0.194
0.0219
0.0139
0.0092
0.0198
0.0068
0.0209
0.0192
0.0156
0.0138
0.0065
0.006
0.0082
0.0436

BNet
0.0464
0.0129
0.0116
0.0096
0.0241
0.0208
0.0113
0.0088
0.0076
0.0073
0.0083
0.011
0.0096
0.0077
0.0586
0.009
0.0085
0.007
0.0091
0.0081
0.0079
0.0088
0.0072
0.0093
0.0094
0.0077
0.0123
0.0074
0.007
0.0272
0.2255
0.2596
0.3714
0.007
0.009
0.0069
0.0061
0.0085
0.0077
0.0249
0.0086
0.0063
0.1241
0.1434
0.1935
0.0228
0.015
0.0104
0.024
0.009
0.0197
0.0165
0.0158
0.0134
0.0068
0.0069
0.0103
0.0445

Data ID
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
Average

NB
0.0446
0.0361
0.0087
0.0845
0.0225
0.0108
0.0117
0.0089
0.0092
0.0082
0.0061
0.0069
0.0611
0.203
0.1854
0.1195
0.1246
0.0147
0.0576
0.0685
0.0081
0.0086
0.0203
0.0095
0.0244
0.0683
0.0074
0.0084
0.0066
0.0096
0.0095
0.006
0.0158
0.0068
0.0097
0.0365
0.0108
0.0058
0.0082
0.4402
0.4591
0.0504
0.1012
0.04
0.0633
0.9103
0.6484
0.5864
0.0067
0.0091
0.0082
0.0088
0.7746
0.0267
0.0082
0.0106
0.0086
0.0658

C4.5
0.0439
0.0351
0.0096
0.0843
0.0177
0.0077
0.0105
0.0075
0.0067
0.0063
0.0058
0.0077
0.0496
0.1993
0.1689
0.1112
0.1208
0.0064
0.0526
0.0704
0.006
0.0085
0.0144
0.0109
0.0244
0.0671
0.0069
0.0077
0.0101
0.0091
0.0095
0.0069
0.0152
0.0073
0.0076
0.0396
0.0081
0.0062
0.0078
0.4429
0.4532
0.0496
0.095
0.0393
0.0618
0.9096
0.6448
0.5884
0.0056
0.0075
0.0074
0.0131
0.7759
0.0255
0.0075
0.0095
0.0079
0.0651

PART
0.0443
0.0407
0.0073
0.0835
0.0211
0.0065
0.0074
0.0058
0.0079
0.0071
0.006
0.0079
0.051
0.1976
0.1631
0.1105
0.1217
0.0068
0.0509
0.0641
0.0082
0.0087
0.0141
0.0054
0.0276
0.0674
0.0086
0.0411
0.0071
0.0124
0.0078
0.0085
0.0164
0.0066
0.008
0.0378
0.0064
0.0062
0.0064
0.444
0.4557
0.0502
0.0954
0.0424
0.0644
0.9091
0.6443
0.5861
0.0065
0.0102
0.0087
0.0131
0.7736
0.025
0.0066
0.0095
0.0073
0.0652

IB1
0.0477
0.034
0.0078
0.0853
0.02
0.0076
0.0068
0.008
0.0065
0.0085
0.0067
0.0097
0.0485
0.1994
0.1652
0.1103
0.1209
0.0067
0.0519
0.066
0.008
0.0072
0.0118
0.0091
0.0257
0.0692
0.0086
0.0074
0.0069
0.014
0.0124
0.0066
0.0183
0.0081
0.0065
0.0374
0.0094
0.0061
0.0068
0.4401
0.4551
0.0539
0.0966
0.0416
0.0635
0.9142
0.6464
0.5851
0.0075
0.0131
0.0064
0.0093
0.7739
0.0266
0.0063
0.0055
0.0062
0.0649

BNet
0.0427
0.0354
0.0085
0.0859
0.0194
0.0098
0.0071
0.0079
0.0064
0.0116
0.0067
0.007
0.0514
0.1983
0.1638
0.1096
0.1226
0.006
0.0533
0.0646
0.013
0.0089
0.0184
0.0082
0.0248
0.0664
0.0071
0.0132
0.0077
0.0113
0.0092
0.0079
0.0165
0.0061
0.008
0.0375
0.0077
0.0064
0.0083
0.4413
0.4545
0.0526
0.0968
0.0394
0.0625
0.9122
0.6461
0.5854
0.0055
0.0103
0.0095
0.0078
0.7746
0.0282
0.0135
0.0073
0.0064
0.0650

NB BNet denote Naive Bayes Bayes Network, respectively.

Table 6: Recommendation time 115 data sets five classifiers (in second )

24

fiSubset Selection Algorithm Automatic Recommendation

4.4.3 Recommendation Time
recommending FSS algorithms feature selection problem, recommendation
time contributed meta-features extraction, k nearest data sets identification,
candidate algorithm ranking according performance k data sets.
three recommendation time contributors, candidate algorithm ranking
related parameters performance metric EARR.
However, computation performance EARR whatever values
are. means recommendation time independent specific settings
. Thus, present recommendation time ( = 0, = 0), Table 6
shows details.
Table 6 observe given data set, recommendation time differences
five classifiers small. reason recommendation time mainly
contributed extraction meta-features, relation classifiers.
consistent time complexity analysis Section 3.2. observe
data sets, recommendation time less 0.1 second, average value
115 data sets around 0.65 second five classifiers. much faster
conventional cross validation method.
4.4.4 Impact Parameters
Figs. 9, 10, 11, 12 13 show impact settings classification
accuracy, runtime feature selection, number selected features, Hit Ratio
RPR value, respectively.
NaiveBayes

C4.5

PART

Average accuracy

0.845

Average accuracy

IB1

Bayes Network

0.845

0.84
0.835
0.83
0.825
0.82
0.815
0.81

0.84
0.835
0.83
0.825
0.82
0.815
0.81
0.805

0.805
0

0

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

!

Figure 9: Classification accuracies five classifiers recommended FSS algorithms different values
Fig. 9 shows classification accuracies five classifiers different values
. observe that, increase either , classification
accuracies five classifiers recommended FSS algorithms decrease.
increase indicates users much prefer faster FSS algorithms
FSS algorithms get less features. Thus, proportion classification accuracy
performance decreased. means ranks FSS algorithms run faster
and/or get less features improved corresponding FSS algorithms finally
selected.
25

fiWang, Song, Sun, Zhang, Xu & Zhou

C4.5

PART

IB1

Bayes Network

2400

Average runtime (ms)

Average runtime (ms)

NaiveBayes
2400
2200
2000
1800
1600
1400
1200
1000
800
600
400
200

2200
2000
1800
1600
1400
1200
1000
800
600
400

0

0

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

!

Figure 10: Runtime FSS algorithms recommended five classifiers different
values
Fig. 10 shows runtime FSS algorithms recommended five classifiers
different values five classifiers. observe that:
1) increase , average runtime recommended FSS algorithms
classifier decreases. Note larger value means users favor faster FSS algorithms. Thus, indicates users performance requirement met since faster FSS
algorithms recommended.
2) increase , average runtime recommended FSS algorithms increases
well. proposed recommendation method, appropriate
FSS algorithms given data set recommended based nearest data sets.
Moreover, experiment, half (i.e., 69) 115 data sets,
negative correlation number selected features runtime
22 FSS algorithms. Thus, data sets kind negative correlation,
possible nearest neighbors given data set negative correlation.
Therefore, larger means longer runtime. Another possible reason larger
value means users favor FSS algorithms choose fewer features, order
get fewer features, FSS algorithms need consume relatively time.
C4.5

PART

Average number features

Average number features

NaiveBayes

100
90
80
70
60
50
40
0

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

IB1

Bayes Network

120
100
80
60
40
20
0

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

!

Figure 11: Number features selected FSS algorithms recommended
five classifiers different values
Fig. 11 shows number features selected FSS algorithms recommended
five classifiers different values . observe that:
26

fiSubset Selection Algorithm Automatic Recommendation

1) increase , average number selected features increases well.
proposed recommendation method, appropriate FSS algorithms
given data set recommended based nearest data sets. Moreover,
experiment, half (i.e., 69) 115 data sets, negative correlation number selected features runtime 22 FSS algorithms.
Thus, data sets kind negative correlation, possible
nearest neighbors given data set negative correlation. Therefore, larger
means features. Another possible reason larger value means users
favor faster FSS algorithms. possible shorter computation time obtained
via filter less features features remained.
Note exception. is, average number selected features
C4.5 decreases value small. However, decrement comes quite
small range (i.e., < 0.005).
2) increase , average number features selected recommended
FSS algorithm decreases. Note larger value means users favor FSS algorithms
get fewer features. Thus, indicates users requirement met since
FSS algorithms get fewer features recommended.
NaiveBayes

C4.5

PART

Average Hit Ratio (%)

Average Hit Ratio (%)

IB1

Bayes Network

100

100
99
98
97
96
95
94
93
92

99
98
97
96
95
94
93
92

91
0

0

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

!

Figure 12: Average Hit Ratio FSS algorithms recommended five classifiers
different values
C4.5

PART
100

99.5

99.5

Average RPR (%)

Average RPR (%)

NaiveBayes
100

99
98.5
98
97.5

IB1

Bayes Network

99
98.5
98
97.5
97

97
0

0

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

!

Figure 13: Average RPR FSS algorithms recommended five classifiers
different values
Figs. 12 13 show average hit ratio RPR recommended FSS algorithms
different values five classifiers.
27

fiWang, Song, Sun, Zhang, Xu & Zhou

observe that, average hit ratio falls intervals [91.74%, 100%]
[92.56%, 99.13%]) . average RPR varies intervals [97.69%,
98.82%] [97.68%, 98.73%] . change , hit
ratio RPR recommended FSS algorithms vary well. However, change
intervals fall relative small interval lower bound stands fairly high level.
minimum average hit ratio 91.74% minimum average RPR
97.68%. indicates proposed FSS algorithm recommendation method
general application works well different settings .

5. Sensitivity Analysis Number Nearest Data Sets
Recommendation Results
section, analyze number nearest data sets affects recommendation performance. Based experimental results, provide guidelines
selecting appropriate number nearest data sets practice.
5.1 Experimental Method
Generally, different numbers nearest data sets (i.e., k) result different recommendations. Thus, recommending FSS algorithms feature selection problem,
appropriate k value important.
k value results higher recommendation performance preferred. However,
recommendation performance difference two different k values sometimes might
random significant. Thus, order identify appropriate k value
alternatives, first determine whether performance differences among
statistically significant. Non-parametric statistical test, Friedman test followed
Holm procedure test suggested Demsar (2006), used purpose.
experiment, conducted FSS algorithm recommendation possible k
values (i.e., 1 114) 115 data sets. identifying appropriate k
values, non-parametric statistical test conducted follows.
Firstly, Friedman test performed 114 recommendation performance
significance level 0.05. null hypothesis 114 k values perform equivalently
well proposed recommendation method 115 data sets.
Friedman test rejects null hypothesis, is, exists significant difference
among 114 k values, choose one recommendation best
performance reference. that, Holm procedure test performed find
k values recommendation performance significant difference
reference. identified k values including reference appropriate
numbers nearest data sets.
5.2 Results Analysis
Fig. 14 shows number nearest data sets (i.e., k) affects performance
recommendation method different settings , denotes
k recommendation performance significantly worse others
significance level 0.05. observe that:
28

fiSubset Selection Algorithm Automatic Recommendation

Naive Bayes

C4.5

PART

IB1

Bayes Network

Inappropriate number neighbors

1
0.995

RPR

0.99
0.985
0.98
0.975
0.97
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

97
96

99 101 103 105 107 109 111 113
98 100 102 104 106 108 110 112 114

Number nearest data sets

(a) = 0, = 0
Naive Bayes

C4.5

PART

IB1

Bayes Network

Inappropriate number neighbors

1

RPR

0.995
0.99
0.985
0.98
0.975
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

97
96

99 101 103 105 107 109 111 113
98 100 102 104 106 108 110 112 114

Number nearest data sets

(b) = 10%, = 10%

Figure 14: Number nearest data sets vs. RPR
1) = = 0 (Fig. 14(a)), five classifiers, RPR varies
different k values. Specifically, RPR fluctuant k smaller 20,
relatively flat middle part, decreases k larger 79 except
C4.5. However, increment C4.5 small (< 0.002). might due
C4.5 picks useful features build tree itself, impact feature
selection methods less. Moreover, difference among accuracies C4.5
data sets relatively small, performance metric EARR used evaluate
different FSS algorithms depends classification accuracy = = 0. Thus,
RPR C4.5 relatively stable different values k.
2) case = = 10% (Fig. 14(b)), variation RPR different
= = 0. five classifiers, RPR first decreases fluctuations,
increases, finally decreases slowly steadily. could due that,
parameters set relatively large value (such 10% experiment),
runtime ( number features selected by) FSS algorithm play
important role evaluating performance FSS algorithm. Thus,
given data set, FSS algorithms lower time complexity (or smaller number
selected features) possibly higher ranked larger RPR. Therefore,
increasing k, algorithms possibly recommended. Meanwhile,
data sets, algorithms either real appropriate algorithms
larger RPR, RPR averaged data sets relatively stable increasing
k.
29

fiWang, Song, Sun, Zhang, Xu & Zhou

3) Comparing cases = = 0 = = 10%, found appears
k < 21 former k < 29 latter, emerges k > 76
former. means cannot choose k values falling ranges.
time, found peak values RPR = = 10% appear
range [32, 54], one peak value ranges = = 0 except C4.5.
means set k 28% 47% number data sets, better recommendation
performance obtained.

6. Conclusion
paper, presented FSS algorithm recommendation method aim
support automatic selection appropriate FSS algorithms new feature selection
problem number candidates.
proposed recommendation method consists meta-knowledge database construction algorithm recommendation. former obtains meta-features performance candidate FSS algorithms, latter models relationship
meta-features FSS algorithm performance based k -NN method recommends appropriate algorithms feature selection problem built model.
thoroughly tested recommendation method 115 real world data sets, 22
different FSS algorithms, five representative classification algorithms two typical
users performance requirements. experimental results show recommendation
method effective.
conducted sensitivity analysis explore number nearest
data sets (k) impacts FSS algorithm recommendation, suggest set k 28%
47% number historical data sets.
paper, utilized well-known commonly-used meta-features
characterize different data sets. meta-features informative?
informative meta-features? still open questions. knowledge,
still exist effective method answer questions. Thus, future
work, plan explore measure information meta-features
whether informative meta-features lead improvements FSS algorithm recommendation.

Acknowledgements
work supported National Natural Science Foundation China grant
61070006.

References
Aha, D. W., Kibler, D., & Albert, M. K. (1991). Instance-based learning algorithms. Machine learning, 6 (1), 3766.
Ali, S., & Smith, K. A. (2006). learning algorithm selection classification. Applied
Soft Computing, 6 (2), 119138.
30

fiSubset Selection Algorithm Automatic Recommendation

Atkeson, C. G., Moore, A. W., & Schaal, S. (1997). Locally weighted learning. Artificial
intelligence review, 11 (1), 1173.
Battiti, R. (1994). Using mutual information selecting features supervised neural net
learning. IEEE Transactions Neural Networks, 5 (4), 537550.
Brazdil, P., Carrier, C., Soares, C., & Vilalta, R. (2008). Metalearning: Applications data
mining. Springer.
Brazdil, P. B., Soares, C., & Da Costa, J. P. (2003). Ranking learning algorithms: Using IBL
meta-learning accuracy time results. Machine Learning, 50 (3), 251277.
Brodley, C. E. (1993). Addressing selective superiority problem: Automatic algorithm/model class selection. Proceedings Tenth International Conference
Machine Learning, pp. 1724. Citeseer.
Castiello, C., Castellano, G., & Fanelli, A. (2005). Meta-data: Characterization input
features meta-learning. Modeling Decisions Artificial Intelligence, 457468.
Dash, M., & Liu, H. (1997). Feature selection classification. Intelligent data analysis,
1 (3), 131156.
Dash, M., & Liu, H. (2003). Consistency-based search feature selection. Artificial Intelligence, 151 (1-2), 155176.
de Souza, J. T. (2004). Feature selection general hybrid algorithm. Ph.D. thesis,
University Ottawa.
Demsar, J. (2006). Statistical comparisons classifiers multiple data sets. Journal
Machine Learning Research, 7, 130.
Engels, R., & Theusinger, C. (1998). Using data metric preprocessing advice data
mining applications..
Frank, E., & Witten, I. H. (1998). Generating accurate rule sets without global optimization.
Proceedings 25th international conference Machine learning, pp. 144151.
Morgan Kaufmann, San Francisco, CA.
Friedman, M. (1937). use ranks avoid assumption normality implicit
analysis variance. Journal American Statistical Association, 32 (200),
675701.
Friedman, N., Geiger, D., & Goldszmidt, M. (1997). Bayesian network classifiers. Machine
learning, 29 (2), 131163.
Gama, J., & Brazdil, P. (1995). Characterization classification algorithms. Progress
Artificial Intelligence, 189200.
Garcia Lopez, F., Garcia Torres, M., Melian Batista, B., Moreno Perez, J. A., & MorenoVega, J. M. (2006). Solving feature subset selection problem parallel scatter
search. European Journal Operational Research, 169 (2), 477489.
Goldberg, D. E. (1989). Genetic algorithms search, optimization, machine learning.
Addison-Wesley Professional.
31

fiWang, Song, Sun, Zhang, Xu & Zhou

Gutlein, M., Frank, E., Hall, M., & Karwath, A. (2009). Large-scale attribute selection
using wrappers. Proceedings IEEE Symposium Computational Intelligence
Data Mining, pp. 332339. IEEE.
Guyon, I., & Elisseeff, A. (2003). introduction variable feature selection.
Journal Machine Learning Research, 3, 11571182.
Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., & Witten, I. (2009).
weka data mining software: update. ACM SIGKDD Explorations Newsletter, 11 (1),
1018.
Hall, M. A. (1999). Correlation-based Feature Selection Machine Learning. Ph.D. thesis,
University Waikato.
Hedar, A. R., Wang, J., & Fukushima, M. (2008). Tabu search attribute reduction
rough set theory. Soft Computing-A Fusion Foundations, Methodologies
Applications, 12 (9), 909918.
Hommel, G. (1988). stagewise rejective multiple test procedure based modified
bonferroni test. Biometrika, 75 (2), 383386.
John, G. H., & Langley, P. (1995). Estimating continuous distributions Bayesian classifiers. Proceedings eleventh conference uncertainty artificial intelligence,
Vol. 1, pp. 338345. Citeseer.
Kalousis, A., Gama, J., & Hilario, M. (2004). data algorithms: Understanding
inductive performance. Machine Learning, 54 (3), 275312.
King, R. D., Feng, C., & Sutherland, A. (1995). Statlog: comparison classification algorithms large real-world problems. Applied Artificial Intelligence, 9 (3), 289333.
Kira, K., & Rendell, L. (1992). practical approach feature selection. Proceedings ninth international workshop Machine learning, pp. 249256. Morgan
Kaufmann Publishers Inc.
Kohavi, R. (1995). study cross-validation bootstrap accuracy estimation
model selection. International joint Conference artificial intelligence, Vol. 14,
pp. 11371145. Citeseer.
Kohavi, R., & John, G. (1997). Wrappers feature subset selection. Artificial intelligence,
97 (1), 273324.
Kononenko, I. (1994). Estimating attributes: analysis extensions RELIEF. Proceedings European conference machine learning Machine Learning, pp.
171182. Springer-Verlag New York.
Lee, M., Lu, H., Ling, T., & Ko, Y. (1999). Cleansing data mining warehousing.
Proceedings 10th International Conference Database Expert Systems
Applications, pp. 751760. Springer.
Lindner, G., & Studer, R. (1999). AST: Support algorithm selection CBR approach. Principles Data Mining Knowledge Discovery, 418423.
Liu, H., Motoda, H., Setiono, R., & Zhao, Z. (2010). Feature Selection: Ever Evolving
Frontier Data Mining. Fourth Workshop Feature Selection Data
Mining, pp. 314. Citeseer.
32

fiSubset Selection Algorithm Automatic Recommendation

Liu, H., & Setiono, R. (1995). Chi2: Feature selection discretization numeric attributes. Proceedings Seventh International Conference Tools Artificial Intelligence, pp. 388391. IEEE.
Liu, H., & Setiono, R. (1996). probabilistic approach feature selection-a filter solution..
pp. 319327. Citeseer.
Liu, H., & Yu, L. (2005). Toward integrating feature selection algorithms classification
clustering. IEEE Transactions Knowledge Data Engineering, 17 (4), 491
502.
Michie, D., Spiegelhalter, D. J., & Taylor, C. C. (1994).
statistical classification..

Machine learning, neural

Molina, L. C., Belanche, L., & Nebot, A. (2002). Feature selection algorithms: survey
experimental evaluation. Proceedings IEEE International Conference Data
Mining, pp. 306313. IEEE.
Nakhaeizadeh, G., & Schnabl, A. (1997). Development multi-criteria metrics evaluation data mining algorithms. Proceedings 3rd International Conference
Knowledge Discovery Data mining, pp. 3742.
Nakhaeizadeh, G., & Schnabl, A. (1998). Towards personalization algorithms evaluation data mining. Proceedings 4th International Conference Knowledge
Discovery Data mining, pp. 289293.
Pudil, P., Novovicova, J., & Kittler, J. (1994). Floating search methods feature selection.
Pattern recognition letters, 15 (11), 11191125.
Pudil, P., Novovicova, J., Somol, P., & Vrnata, R. (1998a). Conceptual base feature
selection consulting system. Kybernetika, 34 (4), 451460.
Pudil, P., Novovicova, J., Somol, P., & Vrnata, R. (1998b). Feature selection expertuser
oriented approach. Advances Pattern Recognition, 573582.
Quinlan, J. R. (1993). C4.5: programs machine learning. Morgan Kaufmann.
Robnik-Sikonja, M., & Kononenko, I. (2003). Theoretical empirical analysis relieff
rrelieff. Machine learning, 53 (1), 2369.
Saeys, Y., Inza, I., & Larranaga, P. (2007). review feature selection techniques
bioinformatics. Bioinformatics, 23 (19), 25072517.
Smith-Miles, K. A. (2008). Cross-disciplinary perspectives meta-learning algorithm
selection. ACM Computing Surveys, 41 (1), 125.
Sohn, S. Y. (1999). Meta analysis classification algorithms pattern recognition. IEEE
Transactions Pattern Analysis Machine Intelligence, 21 (11), 11371144.
Song, Q. B., Wang, G. T., & Wang, C. (2012). Automatic recommendation classification
algorithms based data set characteristics. Pattern Recognition, 45 (7), 26722689.
Vilalta, R., & Drissi, Y. (2002). perspective view survey meta-learning. Artificial
Intelligence Review, 18 (2), 7795.
33

fiWang, Song, Sun, Zhang, Xu & Zhou

Wolpert, D. H. (2001). supervised learning no-free-lunch theorems. Proceedings
6th Online World Conference Soft Computing Industrial Applications, pp.
2542. Citeseer.
Yu, L., & Liu, H. (2003). Feature selection high-dimensional data: fast correlationbased filter solution. Proceedings Twentieth International Conference
Machine Leaning, Vol. 20, pp. 856863.
Zhao, Z., & Liu, H. (2007). Searching interacting features. Proceedings 20th
International Joint Conference Artifical Intelligence, pp. 11561161. Morgan Kaufmann Publishers Inc.
Zhou, X., & Dillon, T. (1988). heuristic-statistical feature selection criterion inductive machine learning real world. Proceedings IEEE International
Conference Systems, Man, Cybernetics, Vol. 1, pp. 548552. IEEE.

34


