Journal Artificial Intelligence Research 47 (2013) 853-899

Submitted 03/13; published 08/13

Framing Image Description Ranking Task:
Data, Models Evaluation Metrics
Micah Hodosh
Peter Young
Julia Hockenmaier

mhodosh2@illinois.edu
pyoung2@illinois.edu
juliahmr@illinois.edu

Department Computer Science
University Illinois
Urbana, IL 61801, USA

Abstract
ability associate images natural language sentences describe
depicted hallmark image understanding, prerequisite applications sentence-based image search. analogy image search, propose
frame sentence-based image annotation task ranking given pool captions.
introduce new benchmark collection sentence-based image description search,
consisting 8,000 images paired five different captions provide
clear descriptions salient entities events. introduce number systems
perform quite well task, even though based features
obtained minimal supervision. results clearly indicate importance
training multiple captions per image, capturing syntactic (word order-based)
semantic features captions. perform in-depth comparison human
automatic evaluation metrics task, propose strategies collecting human
judgments cheaply large scale, allowing us augment collection
additional relevance judgments captions describe image. analysis shows
metrics consider ranked list results query image sentence
significantly robust metrics based single response per query. Moreover, study suggests evaluation ranking-based image description systems
may fully automated.

1. Introduction
ability automatically describe entities, events scenes depicted image
possibly ambitious test image understanding. advances task
significant practical implications, since billions images web
personal photo collections. ability efficiently access wealth information
contain hampered limitations standard image search engines, must rely
text appears near image (Datta, Joshi, Li, & Wang, 2008; Popescu, Tsikrika, &
Kludas, 2010). lot work multi-label classification problem
associating images individual words tags (see, e.g., Blei & Jordan, 2003; Barnard,
Duygulu, Forsyth, Freitas, Blei, & Jordan, 2003; Feng & Lapata, 2008; Deschacht & Moens,
2007; Lavrenko, Manmatha, & Jeon, 2004; Makadia, Pavlovic, & Kumar, 2010; Weston,
Bengio, & Usunier, 2010), much harder problem automatically associating images
complete sentences describe recently begun attract attention.

2013 AI Access Foundation. rights reserved.

fiHodosh, Young & Hockenmaier

1.1 Related Work
Although approaches framed sentence-based image description task mapping images sentences written people (Farhadi, Hejrati, Sadeghi, Young, Rashtchian,
Hockenmaier, & Forsyth, 2010; Ordonez, Kulkarni, & Berg, 2011), research
area focused task automatically generating novel captions (Kulkarni, Premraj,
Dhar, Li, Choi, Berg, & Berg, 2011; Yang, Teo, Daume III, & Aloimonos, 2011; Li, Kulkarni, Berg, Berg, & Choi, 2011; Mitchell, Dodge, Goyal, Yamaguchi, Stratos, Han, Mensch,
Berg, Berg, & Daume III, 2012; Kuznetsova, Ordonez, Berg, Berg, & Choi, 2012; Gupta,
Verma, & Jawahar, 2012). argue paper framing image description natural language generation problem introduces number linguistic difficulties detract
attention underlying image understanding problem wish address. Since
sentence-based image description retrieval system requires ability associate images
captions describe depicted them, argue important evaluate
mapping images sentences independently generation aspect. Research caption generation ignored image search task, arguably
much greater practical importance.
systems cited either evaluated data set group released
earlier work (Rashtchian, Young, Hodosh, & Hockenmaier, 2010), SBU Captioned Photo Dataset (Ordonez et al., 2011). data set consists 1,000 images
PASCAL VOC-2008 object recognition challenge annotated five descriptive captions purposely collected task. SBU data set consists one
million images captions harvested Flickr. Gupta et al. (2012) system
use Grubinger, Clough, Mller, Deselaerss (2006) IAPR TC-12 data set, consists
20,000 images paired longer descriptions.
Although details differ, models rely existing detectors define map images
explicit meaning representation language consisting fixed number scenes, objects
(or stuff), attributes spatial relations (Farhadi et al., 2010; Kulkarni et al., 2011;
Li et al., 2011; Yang et al., 2011; Ordonez et al., 2011; Mitchell et al., 2012).
unclear well detector-based approaches generalize: models evaluated
PASCAL VOC-2008 data set (Farhadi et al., 2010; Kulkarni et al., 2011; Li et al., 2011;
Yang et al., 2011; Mitchell et al., 2012) rely detectors may trained
images contained corpus, Kuznetsova et al. (2012) select test set 1,000 images
SBU data set detectors work well. Moreover, among systems
evaluated PASCAL VOC-2008 data set, Kulkarni et al. (2011), Li et al. (2011),
Li et al. (2011) Mitchell et al.s (2012) results may directly comparable, since different
research groups report different evaluation metrics use different parts data set
test training data. evaluation generation systems generally well known
difficult (see, e.g., Dale & White, 2007; Reiter & Belz, 2009), typically requires expensive
human judgments consider quality content selection (what
described) surface realization (the fluency generated text). syntactic
pragmatic issues confound purely semantic question whether image correctly
described caption.

854

fiFraming Image Description Ranking Task

1.2 Approach
paper, focus task associating images sentences drawn large,
predefined pool image descriptions. descriptions generated automatically
harvested web (Feng & Lapata, 2008; Ordonez et al., 2011), written
people asked describe them. argue evaluating ability select
rank, rather generate, appropriate captions image direct test
fundamental semantic question well associate images sentences
describe well. Framing image description ranking task number
additional advantages. First, allows us handle sentence-based image annotation
search unified framework, allowing us evaluate whether advances one task
carry other. Second, framing image description ranking problem greatly
simplifies evaluation. establishing parallel description retrieval,
use metrics evaluate tasks. Moreover, show rank
original caption, easily determined automatically, leads metrics correlate
highly systems rankings obtained human judgments, even underestimate
actual performance. show standard automatic metrics Bleu (Papineni,
Roukos, Ward, & Zhu, 2002) Rouge (Lin, 2004) used evaluate
caption generation systems show poor correlation human judgments, leading us
believe evaluation caption generation system automated.
perform large-scale human evaluation, since sentences data set image
descriptions written people, need collect purely semantic judgments whether
describe images system associated with. since judgments
independent task, use evaluate image description retrieval
systems. Since collect judgments image-caption pairs publicly available
data set, establish common benchmark enables direct comparison different
systems. believe another advantage caption generation task. Since
many possible ways describe image, generation systems liberty
less specific describe image. makes direct comparison
independently obtained judgments quality two different systems difficult,
since one system may aiming solve much harder task other, implies
unless system outputs common benchmark collection images made publicly
available, cannot shared, objective evaluation would allow community
measure progress difficult problem. since caption generation systems
need able determine well caption describes image, data set could
potentially used evaluate semantic component.
1.3 Contributions Outline Paper
Section 2, discuss need new data set image description introduce
new, high quality, data set image description enable community compare
different systems benchmark. PASCAL VOC-2008 data set 1,000
images (Rashtchian et al., 2010) used number image description systems
(Farhadi et al., 2010; Kulkarni et al., 2011; Li et al., 2011; Yang et al., 2011; Mitchell et al.,
2012; Gupta et al., 2012), number shortcomings limit usefulness. First,
domain relatively limited, captions relatively simple. Second, since
855

fiHodosh, Young & Hockenmaier

images drawn data used PASCAL VOC-2008 object classes challenge,
difficult guarantee fair evaluation description systems rely off-the-shelf
object detectors (e.g., Felzenszwalb, McAllester, & Ramanan, 2008) data set, since
may possible identify images detectors trained on.
experiments paper therefore based larger, diverse, data set 8,000
images. Unlike data sets pair images sentences merely related
image (Feng & Lapata, 2008; Ordonez et al., 2011), image data sets
paired five different captions purposely written describe image.
Section 3, describe image description systems. image description
novel task, remains largely unknown kind model, kind
visual linguistic features requires. Instead unidirectional mapping images
sentences common current caption generation systems, map images
sentences space. allows us apply system image search
retrieving images closest query sentence, image description
annotating images sentences closest it. technique use,
Kernel Canonical Correlation Analysis (KCCA; Bach & Jordan, 2002), already
successfully used associate images (Hardoon, Szedmak, & Shawe-Taylor, 2004; Hwang
& Grauman, 2012; Hardoon, Saunders, Szedmak, & Shawe-Taylor, 2006) image regions
(Socher & Li, 2010) individual words sets tags, Canonical Correlation
Analysis (Hotelling, 1936) used associate images related Wikipedia
articles ten different categories (Rasiwasia, Pereira, Coviello, Doyle, Lanckriet, Levy,
& Vasconcelos, 2010). However, performance techniques much
stringent task associating images sentences describe depicted
evaluated. compare number text kernels capture different linguistic
features. experimental results (discussed Section 4) demonstrate importance
robust textual representations consider semantic similarity words, hence take
linguistic diversity different captions associated image account.
visual features relatively simple. number image description systems (Farhadi et al.,
2010; Kulkarni et al., 2011; Li et al., 2011; Yang et al., 2011; Kuznetsova et al., 2012) largely
rely trained detectors, e.g. obtain explicit intermediate meaning representation
depicted objects, scenes events. approach would ultimately require separate
detectors, hence labeled training data, term phrase chosen meaning
representation language. show image features capture low-level
perceptual properties fact work surprisingly well larger data set
in-domain detectors available.
Section 4, consider question evaluation, use number different metrics
compare systems. Since focus problem learning appropriate mapping
images captions, follow standard machine learning practice evaluate
ability function generalize unseen examples. Hence, separate pool
captions images used testing used train systems. first consider
metrics quality single image-caption pair, compare automatically computed
scores detailed human judgments. examine metrics evaluate ranked
lists returned systems. analysis reveals that, current level performance,
differences models may become apparent single caption per image
considered, commonly done caption generation systems. even two models
856

fiFraming Image Description Ranking Task

equally likely fail return suitable caption first result, still prefer
one likely rank good captions higher other, since arguably
provides better approximation semantic space images near captions
describe well. Since test pool contains single gold item query,
first consider metrics based rank recall gold item. show
simpler, binary judgments image descriptions good approximations
fine-grained human judgments collected large scale via crowdsourcing.
augment test pool data set relevance judgments, hope
add usefulness community resource benchmark. judgments
show actual performance systems higher recall gold item
indicates. However, comparison system rankings obtained via different metrics
suggests differences rank recall gold item correlate highly
difference performance according binary relevance judgments.

2. New Data Set Image Description
used crowdsourcing collect descriptive captions large number images
people animals (mostly dogs). describing data set annotation methodology, discuss kind captions useful image description, motivate
need create new data sets task.
2.1 Mean Image Description?
Since automatic image description relatively novel task, worth reflecting
means describe images, wish say image. fact
substantial body work image description related image libraries (Jaimes, Jaimes,
& Chang, 2000; Shatford, 1986) useful revisit purpose. argue
three different kinds image descriptions commonly distinguished, one
type, so-called conceptual descriptions, relevance image understanding aim achieve automatic captioning. Conceptual image descriptions identify
depicted image, may abstract (e.g., concerning mood
picture may convey), image understanding mostly interested concrete descriptions
depicted scene entities, attributes relations, well events
participate in. focus actually image, conceptual descriptions
differ so-called non-visual descriptions, provide additional background information cannot obtained image alone, e.g. situation, time location
image taken. Perceptual descriptions capture low-level visual properties
images (e.g., whether photograph drawing, colors shapes dominate) little interest us, unless link properties explicitly depicted
entities. Among concrete conceptual descriptions, distinction drawn specific descriptions, may identify people locations names,
generic descriptions (which may, e.g., describe person woman skateboarder,
scene city street room). exception iconic entities
recognized (e.g., well-known public figures landmark locations Eiffel
Tower) argue image understanding focus information captured

857

fiHodosh, Young & Hockenmaier

BBC captions
(Feng Lapata 2010)

Consumption
soared
real price
drink fallen

AMD destroys
central vision

SBU Captioned Photo Dataset (Flickr)
(Ordonez et al. 2011)

Downers Grove
don't chew couch
train station (our condo pee kitchen
building
mama!
background),
way AG store
Chicago.

IAPR-TC12 data set
(Grubinger et al. 2006)

blue white airplane standing grey airport;
man red cones standing front two
red-dressed hostesses two passengers directly
stairs front airplane; brown landscape
high dark brown mountains snow-covered
summits light grey sky background;

Figure 1: data sets images captions
generic descriptions. leaves question obtain data set images paired
suitable descriptions train automatic description systems on.
2.2 Need New Data Sets
dearth images associated text available online, argue
text suitable task. work, notably natural language
processing community, focused images news articles (Feng & Lapata, 2008, 2010).
However, images often used illustrate stories, little direct connection
text (Figure 1, left). Furthermore, even captions describe depicted event,
tend focus information cannot obtained image itself. Similarly,
people provide captions images upload websites Flickr (Figure 1,
center), often describe situation images taken in, rather
actually depicted image. is, captions often provide non-visual overly
specific information (e.g., naming people appearing image location
image taken). simple reason people typically provide kinds
generic conceptual descriptions use purposes: Gricean maxims
relevance quantity (Grice, 1975) entail image captions written people
usually provide precisely kind information could obtained image
itself, thus tend bear tenuous relation actually depicted. Or,
state succinctly, captions usually written seen along images
accompany, users may wish bore readers obvious.
Ordonez et al. (2011) harvested images captions Flickr create
SBU Captioned Photo Dataset, discard vast majority images
captions actually descriptive. analysis random sample 100
images final data set revealed majority (67/100) captions describe
information cannot obtained image (e.g., naming people
locations appearing image), substantial fraction (23/100) describe small
detail image otherwise commentary image. Examples
issues shown Figure 1 (center). makes data set less useful kind
image understanding interested in: unless refer specific entities one may
actually wish identify (e.g., celebrities famous landmarks appear image),
proper nouns little help learning visual properties entity types unless one
858

fiFraming Image Description Ranking Task

data set 8,000 Flickr images 5 crowd-sourced captions
man tricks bicycle ramps front crowd.
man bike executes jump part competition crowd watches.
man rides yellow bike ramp others watch.
Bike rider jumping obstacles.
Bmx biker jumps ramp.
group people sit table front large building.
People drinking walking front brick building.
People enjoying drinks table outside large brick building.
Two people seated table drinks.
Two people sitting outdoor cafe front old building.

Figure 2: data set images paired generic conceptual descriptions
infer kind entity refer to.1 IAPR TC-12 data set (Grubinger et al.,
2006), consists 20,000 photographs potentially useful purposes, since
contains descriptions recognized image without prior information
extra knowledge. However, descriptions, consist often multiple sentences
sentence fragments, tendency lengthy (average length: 23.1 words)
overly detailed, instead focusing salient aspects photograph. example,
photo airplane Figure 1 (right), two hostesses barely visible
nevertheless described detail.
2.3 Data Sets
Since kinds captions normally provided images describe images
themselves, collected data sets images captions. captions
obtained using crowdsourcing service provided Amazon Mechanical Turk
annotate image five descriptive captions. asking people describe
people, objects, scenes activities shown picture without giving
information context picture taken, able
obtain conceptual descriptions focus information obtained
image alone. annotation process quality control described detail
Rashtchian et al. (2010)s paper. annotated two different data sets manner:
2.3.1 PASCAL VOC-2008 Data Set
first data set produced relatively small, consists 1,000 images randomly selected training validation set PASCAL 2008 object recognition
challenge (Everingham, Gool, Williams, Winn, & Zisserman, 2008). used
large number image description systems (Farhadi et al., 2010; Kulkarni et al., 2011; Li
et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Gupta et al., 2012), since almost
systems (the exception Gupta et al., 2012) rely detectors trained
1. data set Ordonez et al. (2011) differs significantly content ours: collection
focuses images eventualities, i.e. people animals something, majority Ordonez et
al.s images (60/100) depict people animals (e.g., still lifes, landscape shots).

859

fiHodosh, Young & Hockenmaier

images data set (Felzenszwalb et al., 2008), unclear well
approaches would generalize domains labeled data train detectors
available. captions PASCAL data set relatively simple. example,
since data set contains many pictures depict focus people something, 25% captions contain verb, additional 15% captions
contain common static verbs sit, stand, wear, look.
2.3.2 Flickr 8K Data Set
work reported paper therefore collected larger, diverse data set
consisting 8,092 images Flickr.com website. Unlike static PASCAL
images, images data set focus people animals (mainly dogs) performing
action. Examples data set shown Figure 2. images chosen
six different Flickr groups,2 tend contain well-known people locations,
manually selected depict variety scenes situations. order avoid
ungrammatical captions, allowed workers United States passed
brief spelling grammar test devised annotate images.
interested conceptual descriptions, annotators asked write sentences describe
depicted scenes, situations, events entities (people, animals, objects).
collected multiple captions image considerable degree variance
way many images described. consequence, captions
images often direct paraphrases other: entity event situation
described multiple ways (man vs. bike rider, tricks vs. jumping),
everybody mentions bike rider, everybody mentions crowd ramp.
dynamic nature images reflected described:
Captions data set average length 11.8 words, compared 10.8 words
PASCAL data set, 40% PASCAL captions contain verb
sit, stand, wear, look, 11% captions Flickr 8K set contain
verb, additional 10% contain common verbs. data sets, Flickr
training/test/development splits human relevance judgments used evaluation
test items (Section 4) publicly available.3 online appendix paper contains
instructions workers, including qualification test pass
allowed complete tasks.

3. Systems Sentence-Based Image Description
Since image description requires ability associate images sentences, image
description systems viewed terms affinity function f (i, s) measures
degree association images sentences. evaluate ability compute
affinity functions measuring performance two tasks depend directly them.
Given candidate pool sentences Scand candidate pool images Icand , sentencebased image retrieval aims find image Icand maximizes f (i, sq ) query
sentence sq Scand . Conversely, image annotation aims find sentence Scand
2. groups called strangers!, Wild-Child (Kids Action), Dogs Action (Read Rules),
Outdoor Activities, Action Photography, Flickr-Social (two people photo)
3. http://nlp.cs.illinois.edu/HockenmaierGroup/data.html

860

fiFraming Image Description Ranking Task

maximizes f (iq , s) query image iq Icand . cases, f (i, s) course
maximized image-sentence pairs sentence describes image well:
Image search:
Image annotation:

= arg maxiIcand f (i, sq )


(1)

= arg maxsScand f (iq , s)

formulation completely general: although will, evaluation purposes, define
Scand set captions originally written images Icand ,
case, Scand could also, example, defined implicitly via caption generation
system. order evaluate well f generalizes unseen examples, evaluate
system test pools Itest Stest drawn domain disjoint
training data Dtrain = (Itrain , Strain ) development data Ddev = (Idev , Sdev ).
challenge defining f lies fact images sentences drawn two
different spaces, S. paper, present two different kinds image description
systems. One based nearest-neighbor search (NN), uses technique called
Kernel Canonical Correlation Analysis (KCCA; Bach & Jordan, 2002; Hardoon et al., 2004).
rely set known image-sentence pairs Dtrain = {hi, si}.
3.1 Nearest-Neighbor Search Image Description
Nearest-neighbor based systems use unimodal text image similarity functions directly
first find image-sentence pair training corpus Dtrain contains closest
item query, score items space similarity
item pair:
Image retrieval: fNN (i, sq ) = (iNN , i)

hiNN , sNN = arg max fS (sq , st ) (2)
hit ,st iDtrain

Image annotation: fNN (iq , s) = fS (sNN , s) hiNN , sNN = arg max (iq , )
hit ,st iDtrain

Despite simplicity, nearest-neighbor systems non-trivial baselines:
task annotating images tags keywords, methods annotate unseen images
tags nearest neighbors among training images known achieve competitive performance (Makadia et al., 2010), similar methods recently proposed
image description (Ordonez et al., 2011). Since task address allow
us return items training data, requires us rerank pool unseen captions
images, nearest-neighbor search requires two similarity functions. nearestneighbor systems use image representation KCCA-based systems, described
Section 3.3. main nearest-neighbor system, NN (NN5idf
F1 ), treats five captions
associated training image single document. reweights token
inverse document frequency (IDF) w , defines similarity two sentences
F1-measure (harmonic mean precision recall) computed IDF-reweighted
bag-of-words representation. Dtrain (w) subset training images whose captions
word w appears least once, inverse document frequency (IDF) w defined
|Dtrain |
w = log |Dtrain
(w)|+1 . IDF-reweighting potentially helpful task, since words
describe fewer images may particularly discriminative captions.
861

fiHodosh, Young & Hockenmaier

appendix, provide results NN systems use text representation
two KCCA systems.
3.2 Kernel Canonical Correlation Analysis Image Description
systems present based technique called Kernel Canonical Correlation
Analysis (Bach & Jordan, 2002; Hardoon et al., 2004). first provide brief introduction,
explain apply task.
3.2.1 Kernel Canonical Correlation Analysis (KCCA)
KCCA extension Canonical Correlation Analysis (Hotelling, 1936), takes
training data consisting pairs corresponding items hxi , yi drawn two different
feature spaces (xi X , yi Y), finds maximally correlated linear projections x
sets items newly induced common space Z. Since linear projections
raw features may capture patterns necessary explain pairing
data, KCCA implicitly maps original items higher-order spaces X 0 0 via
kernel functions KX = hX (xi ) X (xj )i, compute dot product two data points
xi xj higher-dimensional space X 0 without requiring explicit computation
mapping X . KCCA operates two resulting kernel matrices KX [i, j] =
hX (xi ) X (xj )i KY [i, j] = hY (yi ) (yj )i evaluate kernel functions
pairwise combinations items training data. returns two sets projection weights,
, maximize correlation two (projected) kernel matrices:
( , ) = arg max q
,

0 KX KY
(0 K2X + 0 KX )( 0 K2Y + 0 KY )

(3)

cast generalized eigenproblem (KX +I)1 KY (KY +I)1 KX = 2 ,
solved partial Gram-Schmidt orthogonalization (Hardoon et al., 2004; Socher & Li,
2010). regularization parameter penalizes size possible solutions, used
avoid overfitting, arises matrices invertible.
One disadvantage KCCA requires two kernel matrices training
data kept memory training. becomes prohibitive large data
sets, cause problems here, since training data consists 6,000
items (see Section 4.1).
3.2.2 Using KCCA Associate Images Sentences
KCCA successfully used associate images (Hardoon et al., 2004; Hwang &
Grauman, 2012; Hardoon et al., 2006) image regions (Socher & Li, 2010) individual
words sets tags. case, two original spaces X = = correspond
images sentences describe them. Images first mapped vectors KI (i)
whose elements KI (i)(t) = KI (it , i) evaluate image kernel function KI t-th
image Dtrain . Similarly, sentences mapped vectors KS (s) evaluate
sentence kernel function KS sentences Dtrain . learned projection weights
( , ) map KI (i) KS (s) induced space Z, expect images
appear near sentences describe well. KCCA-based image annotation
862

fiFraming Image Description Ranking Task

search system, therefore define f cosine similarity (sim) points new space:
fKCCA (i, s) = sim(KI (i), KS (s))

(4)

describe image text kernels used KCCA systems.
3.3 Image Kernels
contrast much work done image description, assumes existence
large number preexisting detectors, image representations used paper
basic, rely three different kinds low-level pixel-based perceptual
features capture color, texture (Varma & Zisserman, 2005) shape information
form SIFT descriptors (Lowe, 2004; Vedaldi & Fulkerson, 2008). believe
establishes important baseline, leave question complex image
representations affect performance future work. use two different kinds kernels:
histogram kernel K Histo , represents image single histogram feature
values computes similarity two images intersection histograms,
pyramid kernel K Py (Lazebnik, Schmid, & Ponce, 2009), represents image
pyramid nested regions, computes similarity two images terms
intersection histograms corresponding regions. cases, compute separate
kernel three types image features average result.
3.3.1 Histogram Kernel (K Histo )
image xi represented histogram Hi discrete-valued features, Hi (v)
fraction pixels xi value v. similarity two images xi xj defined
intersection histograms, i.e. percentage pixels mapped
onto pixel feature value image:
K(xi , xj ) =

V
X

min(Hi (v), Hj (v))

(5)

v=1

combine three kernels based different kinds visual features: KC captures color,
represented three CIELAB coordinates. KT captures texture, represented descriptors capture edge information different orientations centered pixel (Varma
& Zisserman, 2005). KS based SIFT descriptors, capture edge shape information manner invariant changes rotation illumination,
shown distinct across possible objects image Lowe, 2004; Vedaldi & Fulkerson,
2008. use 128 color words, 256 texture words 256 SIFT words, obtained unsupervised fashion K-means clustering 1,000 points 200 images PASCAL
2008 data set (Everingham et al., 2008). final histogram kernel K Histo average
responses three kernels KCHisto , KTHisto , KSHisto , taken pth power:

p
1 X
K Histo (xi , xj ) =
KFHisto (xi , xj )
(6)
3
F {C,S,T}

863

fiHodosh, Young & Hockenmaier

3.3.2 Pyramid Kernel K Py
spatial pyramid kernel (Lazebnik et al., 2009) generalization histogram kernel
captures similarities global, local level. image xi
represented multiple levels scale l (l {0, 1, 2}) level partitions
image smaller smaller grid Cl = 2l 2l cells (C0 = 1, C1 = 4, C2 = 16),
cell c represented histogram Hic . similarity images xi xj level l,
Iijl , turn defined sum histogram similarities corresponding cells
0l , ..., Cl level:
Iijl

=

Cl X
V
X

min(Hic (v), Hjc (v))

(7)

c=0l v=1

Although similarities level l subsume fine-grained level l + 1 (Iijl
Iijl+1 ), similarities hold fine-grained level deemed important, since
indicate greater local similarity. pyramid kernel therefore proceeds
fine-grained (l = L) coarsest (whole-image) scale (l = 0), weights
1
similarities first encountered level l (Iijl Iijl+1 ) 2Ll
:

K

Py

(xi , xj ) =

IijL

+

L1
X
l=0

1
(I l Iijl+1 )
2Ll ij

(8)

L

=

1 0 X 1
+
Il
2L ij
2Ll+1 ij
l=1

compute three separate pyramid kernels KCPy , KTPy , KSPy based
color, texture SIFT features described above, combine single pyramid
kernel K Py , equation 6.
3.4 Basic Text Kernels
examine three different basic text kernels: bag words (BoW) kernel, Hwang
Graumans (2012) TagRank kernel, truncated string kernel (Tri).
3.4.1 Bag Words Kernel (BoW)
Since bag-of-words representations successfully used tasks involving text
images (e.g., Grangier & Bengio, 2008; Hardoon et al., 2006), include basic bag
words kernel, ignores word order represents caption simply vector
word frequencies. BoW kernel function defined cosine similarity
corresponding bag words vectors. either merge five captions training item
single document (BoW5), reduce training item single, arbitrarily chosen,
caption (BoW1). words frequency reweighted IDF-score.
|Dtrain |
nearest neighbor approach, IDF-weight word w defined w = log |Dtrain
(w)|+1 ,
Dtrain (w) subset training images whose captions word w appears least

864

fiFraming Image Description Ranking Task

once. found square root w (BoW5
IDF-score w (BoW5idf ).



idf )

give better results standard

3.4.2 Tag Rank Kernel (TagRank)
Hwang Grauman (2012) apply KCCA keyword-based image annotation retrieval.
focus data set image paired list tags ranked importance, propose new kernel kind data. so-called tag rank kernel
(TagRank) variant bag words kernel aims capture relative importance tags reweighting according position list. Although Hwang
Grauman evaluate ability system associate images entire
sentences, consider another data set lists tags correspond
words descriptive captions, argue linear order words captions
reflects relative importance corresponding objects image, words
appear beginning sentence describe salient aspects image.
TagRank kernel, sentence represented two vectors, ~a ~r. ~a,
weight word based absolute position, first words sentence
always assigned high weight. absolute tag rank representation, caption
mapped vector ~a = [~a(1) . . . ~a(|V |)], |V | size vocabulary. ~a(i)
depends absolute position pi wi (if wi occurs multiple times s, pi averaged
positions). wi occur s, ~a(i) = 0. Otherwise,
~a(i) =

1
log2 (1 + pi )

(9)

~r, weight word depends current position compares distribution positions occupies training data. intuition behind relative rank
representation words higher weight occur earlier sentence usual. Here, caption mapped vector ~r = [~r(1) . . . ~r(V )] relative
tag ranks. Again, wi appear s, ~r(i) = 0. Otherwise wi relative tag rank
~r(i) indicates percent occurrences training data appear position pi .
Defining
P nik number times word wi appears position k training data,
ni = k nik total frequency wi training data:
Ppi
~r(i) = 1

k=1 nik

(10)

ni

final kernel KT given average two 2 kernels computed ~r ~a (
0 normalization terms):
"



#
V
V
1
1 X (~ri (k) ~rj (k))2
1 X (~ai (k) ~aj (k))2
KT (xi , xj ) =
exp
+ exp
(11)
2
2
~ri (k) + ~rj (k)
20
~ai (k) + ~aj (k)
k=1

k=1

Since image training data associated multiple, independently generated captions, evaluate kernel separately sentence pair average
response, instead treating multiple sentences single document.
865

fiHodosh, Young & Hockenmaier

TagRank kernel relatively sensitive overall sentence length, especially cases
subject preceded multiple adjectives modifiers (a large brown
dog vs. dog ). English, absolute tag rank generally assign high weights
subjects sentences, lower weight verbs, even lower weight objects scene
descriptions, tend follow main verb. relative tag rank may downweight
verbs, objects scene descriptions much (as long always used similar
positions sentence).
3.4.3 Trigram Kernel (Tri)
Since bag-of-words representations ignore words appear close
sentence, lose important information: image small child red hair playing
large brown dog white carpet looks quite different one small white dog
playing large red ball brown grass, although descriptions share majority
words. capture information, define trigram kernel truncated variant
string kernels (Shawe-Taylor & Cristianini, 2004) considers many single
words two captions share, many short sequences (pairs triples) words
occur both.
word sequence w = w1 ...wk ordered list words. sentence = s1 ...sn contains
w (w s) long words w appear order specified w. is,
sentence large white dog runs catches red ball beach (when lemmatized)
contains subject-verb-object triple dog catch ball subject-verb-location
triple dog run beach. Formally, every substring (i, j) = si ...sj starts si = w1 ,
ends sj = wk , contains w considered match w. Ms,w set
substrings match sequence w:
Ms,w = {(i, j) | w = w1 ...wk si ...sj , w1 = si , wk = sj }

(12)

w restricted individual words (k = 1), string kernels identical standard
BoW kernel.
match strings s0 pair substrings (i, j) (i0 , j 0 ) s0
match word sequence w. Standard string kernels K(s, s0 ) weight matches
0
0
factor (ji+1)+(j +1) depends adjustable parameter respective
length matching substrings:
K(s, s0 ) =

X

X

X

0

0

(ji+1)+(j +1)

(13)

w (i,j)Ms,w (i0 ,j 0 )Ms0 ,w

order distinguish length matching subsequence, l(w),
length gaps (i, j) (i0 , j 0 ), replace two parameters , g , reformulate
as:
K(s, s0 ) =

X

X

X

0

0

+1)2l(w)
2l(w)
(ji+1)+(j

g

(14)

w (i,j)Ms,w (i0 ,j 0 )Ms0 ,w

found gap score g = 1, means gaps penalized,
match score = 0.5 perform best task.
866

fiFraming Image Description Ranking Task

Although string kernels generally defined sequences arbitrary length (k ),
found allowing longer sequences seem impact performance task
incurred significant computational cost. Intuitively, word pairs triplets represent
linguistic information need capture beyond BoW representation, since
include head-modifier dependencies large-dog vs. small-dog subject-verb-object
dependencies child-play-dog vs. dog-play-ball. therefore consider sequences
length k 3. w restricted sequences length k 3 ms,w = |Ms,w |,
yields following trigram kernel (Tri):
KTri (s, s0 ) =

X

ms,w ms0 ,w 2l(w)


(15)

w:k3

deal differences sentence length, normalize kernel response
two examples geometric mean two example responses themselves.
Since trigram kernel captures sequences merely coincidental,
large white red, may seem advantageous use richer syntactic representations
dependency tree kernels (Moschitti, Pighin, & Basili, 2008), consider word
tuples correspond syntactic dependencies. However, kernels significantly
expensive compute, initial experiments indicated may perform
well trigram kernel. believe due fact image captions
contain little syntactic variation, hence surface word order may sufficient
differentiate e.g. agent action (whose mention subject
sentence) participants entities (whose mentions appear verb).
hand, many image captions contain lot syntactic ambiguity (e.g.
multiple prepositional phrases), vocabulary distinct standard
parsers trained on. may able benefit using richer
representation simply able recover sufficient accuracy.
order capture
relative importance words, reweight sequences

IDF (or idf) weight theirQwords.
w defined before, IDF-weight
j
sequence w = wi ...wj w =
idf-weighted trigram kernel KTriidf
k=i wk .
(Tri5



idf )

therefore

KTriidf (s, s0 ) =

X

w ms,w ms0 ,w 2l(w)


(16)

w:k3

3.5 Extending Trigram Kernel Lexical Similarities
One obvious shortcoming basic text kernels require exact matches
words, cannot account fact situation, event, entity
described variety ways (see Figure 2 examples). One way capturing
linguistic diversity lexical similarities allow us define partial matches
words based semantic relatedness. Lexical similarity found success
tasks, e.g. semantic role labeling (Croce, Moschitti, & Basili, 2011),
fully exploited image description. Ordonez et al. (2011) define explicit equivalence classes
synonyms hyponyms increase natural language vocabulary corresponding
object detectors (e.g. word Dalmatian may trigger dog detector),
867

fiHodosh, Young & Hockenmaier

change underlying, pre-trained detectors themselves, ignoring potential
variation appearance between, e.g., different breeds dog. Similarly, Yang et al.s (2011)
generative model produce variety words type detected object scene,
given object scene label, word choice independent visual features.
therefore investigate effect incorporating different kinds lexical similarities
trigram kernel allow us capture partial matches words.
explore effect incorporating lexical similarities tag-rank kernel, since
unclear affect computation ranks within sentence.
3.5.1 String Kernels Lexical Similarities
Since standard lexical similarities simS (w, wi ) necessarily yield valid kernel functions,
follow Bloehdorn, Basili, Cammisa, Moschitti (2006) use similarities
map word w vectors w
~ N -dimensional space, defined fixed vocabulary
size N . vector component w
~ (i) corresponds similarity w wi defined
:
(17)

w
~ (i) = simS (w, wi )

define corresponding word kernel function (w, w0 ), captures partial
match words w w0 according , cosine angle w
~ w
~ S0 :
(w, w0 ) = cos(~
wS , w
~ S0 )

(18)

may defined subset vocabulary. similarity words outside
vocabulary defined identify function, standard string kernel.
similarity sequences w w0 length l defined product word
kernels corresponding pairs sequence elements wi , wi0 :
(w, w0 ) =

l


(wi , wi0 )

(19)

i=1

(w) = {w0 |S (w0 , w) > 0, l(w0 ) = l(w)} set sequences non-zero
match w, string kernel KS similarity is:
KS (s, s0 ) =

X

X

ms,w ms0 ,w0 2l(w)
(w0 , w)


(20)

w w0 (w)


idf
0
obtain
IDF-weighted version kernel, KS (s, ), inner term multiplied w w0 :
X X p
KS (s, s0 ) =
w w0 ms,w ms0 ,w 2l(w)
(w0 , w)
(21)

w w0 (w)

experiments, use trigram variants kernels, restrict w
sequences length k 3.
consider three different kinds lexical similarities: WordNet-based Lin similarity
(Lin, 1998) (Lin ), distributional similarity metric (D ), novel alignment-based
868

fiFraming Image Description Ranking Task

similarity metric (A ), takes advantage fact image associated
five independently generated captions. metrics computed training corpus.
Distributional similarity computed British National Corpus (BNC Consortium,
2007). corpora lemmatized, stop words removed similarities
computed. Since almost pair words non-zero similarity, word kernel
matrices dense, since similarities close zero,
little effect resulting kernel. therefore zero entries smaller 0.05
alignment-based kernel less 0.01 distributional kernel DC .
3.5.2 Lin Similarity Kernel (Lin )
Lins (1998) similarity relies hypernym/hyponym relations WordNet (Fellbaum,
1998) well corpus statistics. WordNet directed graph nodes (synsets)
represent word senses edges indicate is-a relations: parent sense (e.g., dog1 )
hypernym children (e.g., poodle1 dachshund1 ). Kernels based Lins similarity
found perform well tasks text categorization (Bloehdorn et al., 2006).
exception Farhadi et al. (2010), incorporate Lins similarity
model, evaluate benefit obtain it, WordNets hypernym-hyponym
relations used superficially associating images text (Weston et al.,
2010; Ordonez et al., 2011; Gupta et al., 2012). Lin similarity two word senses si , sj
defined
2 log P (LCS(si , sj ))
simLin (si , sj ) =
(22)
log P (si ) + log P (sj )
LCS(s1 , s2 ) refers lowest common subsumer s1 s2 WordNet, i.e.
specific synset ancestor (hypernym) s1 s2 . P (s) probability
randomly drawn word instance synset descendants (hyponyms).
use training data estimate P (s), follow Bloehdorn et al. (2006) assigning
word w frequent (first) noun sense sw WordNet 3.0. Hence, represent
word w WordNet sense vector w
~ Lin Lin similarities hypernyms H(sw ):
2log(f (si ))

log(f (s))+log(f (si ))
w
~ Lin (i) =
1


0

si H(s)
sw = si
otherwise

(23)

3.5.3 Distributional Similarity (DC )
Distributional similarity metrics based observation words similar
tend appear similar contexts (Jurafsky & Martin, 2008). components
w
~ DC non-negative pointwise mutual information scores (PMI) w wi , computed
corpus C:


PC (w, wi )
(24)
w
~ DC (i) = max 0, log2
PC (w)PC (wi )
PC (w) probability random sentence C contains w, PC (w, wi )
probability random sentence C contains w wi . compute two variants
869

fiHodosh, Young & Hockenmaier

metric: Dic computed image captions training corpus,
defined cooccurrences 1,928 words appear least 5 times corpus,
DBNC uses British National Corpus (BNC Consortium, 2007), defined
1,874 words appear least 5 times corpora, considers PMI scores
141,656 words appear least 5 times BNC.
3.5.4 Alignment-Based Similarity (A )
propose novel, alignment-based, similarity metric (A ), takes advantage
fact image associated five independently generated captions,
specifically designed capture likely two words describe event
entity data set. borrow concept alignment machine translation
(Brown, Pietra, Pietra, & Mercer, 1993), instead aligning words sentences two
different languages, align pairs captions describe image. results
similarity metric better coverage data set WordNet based metrics,
much specific distributional similarities capture broad topical relatedness
rather semantic equivalence. Instead aligning complete captions, found
beneficial align nouns verbs independently other, ignore parts
speech. create two versions training corpus, one consisting nouns
caption, another one consisting verbs caption.
use Giza++ (Och & Ney, 2003) train IBM alignment models 12 (Brown et al., 1993)
pairs noun verb captions image obtain two sets translation
probabilities, one nouns (Pn (|w)) one verbs (Pv (|w)). Finally, combine
noun verb translation probabilities sum weighted relative frequency
word w tagged noun (Pn (w)) verb (Pv (w)) training corpus.
ith entry wA therefore:
w
~ (i) = Pn (wi |w)Pn (w) + Pv (wi |w)Pv (w)

(25)

define noun verb vocabulary follows: words appear least 5 times
noun, tagged noun least 50% occurrences, considered
nouns. since verbs polysemous nouns (leading broader translation
probabilities) often mistagged nouns domain, include words
verbs tagged verbs least 25 times, least 25% occurrences.
results 1180 noun 143 verb lemmas, including 11 nouns verbs.
use OpenNLP POS tagger lemmatization.
3.5.5 Comparing Similarity Metrics (Figure 3)
Figure 3 illustrates different similarity metrics, using words rider swim
examples. distributional similarities high words topically related
(e.g., swim pool ), alignment similarity tends high words used
describe entity (usually synonyms hyper/hyponyms) activity swim
paddle. Distributional similarities obtained image captions
specific domain. BNC similarities much broader help overcome data
sparsity, although BNC relatively low coverage kinds sports occur
data set. Lin similarity associates swim hypernyms sport activity,
870

fiFraming Image Description Ranking Task

Comparing similarity metrics: five words similar rider swim
Alignment
Strain
wi


wi

rider

biker
bicyclist
cyclist
bmx
bicycler

0.86
0.82
0.79
0.75
0.73

bike
dirt
motocross
motorcycle
ride

0.41
0.35
0.33
0.33
0.33

ride
horse
race
bike
jockey

swim

retrieve
paddle
dive
come
wade

0.56
0.54
0.52
0.38
0.31

pool
trunk
water
dive
goggles

0.53
0.35
0.34
0.30
0.29

fish
water
sea
pool
beach

Corpus
w

Distributional
Strain
BNC
Dic wi
DBNC

Lin
Strain
wi

Lin

0.21
0.20
0.19
0.17
0.16

traveler
cyclist
bicyclist
horseman
jockey

0.94
0.89
0.89
0.84
0.84

0.21
0.18
0.18
0.18
0.17

bathe
sport
football
activity
soccer

0.85
0.85
0.77
0.75
0.73

Figure 3: comparison lexical similarities noun rider verb swim
kinds sport football soccer. makes least suitable similarity
task (see Section 4.3.4 experimental results), since terms
considered similar purposes identifying different ways visually similar
events entities described.
3.5.6 Combining Different Similarities
Combining different distributional alignment-based similarities allows us
capture different strengths method. define averaged similarity
captures aspects distributional similarities computed corpora:
DBNC (w, w0 ) + Dic (w, w0 )
(26)
2
every distributional kernel (w, w0 ), define variant D+A (w, w0 )
incorporates alignment-based similarities taking maximum either kernel:4
DBNC,ic (w, w0 ) =

D+A (w, w0 ) = max(A (w, w0 ), (w, w0 ))

(27)

4. Evaluation Procedures Metrics Image Description
order evaluate scoring functions f (i, s) image-caption pairs, need evaluate
ability associate previously unseen images captions other. analogy
caption generation systems, first examine metrics aim measure quality
single image-description pair (Section 4.2). Here, focus image annotation task,
restrict attention first caption returned test item, subset
systems. collect graded human judgments small number native speakers
American English, investigate whether expert judgments approximated
4. operation may preserve positive definiteness matrix required valid kernel,
simply means effectively use (plain) CCA representation.

871

fiHodosh, Young & Hockenmaier

automatically computed Bleu (Papineni et al., 2002) Rouge (Lin & Hovy, 2003)
scores, simpler crowdsourced human judgments collected much
larger scale. Section 4.3, consider approaches evaluation aim measure
quality ranked list image-caption pairs returned system, allow us
evaluate large number systems. reasons space, focus discussion
subset systems, refer interested reader Appendix B
complete results. Since candidate pool contains one sentence image originally
associated query image sentence, first compare systems rank recall
original item. metrics computed automatically,
considered lower bounds actual performance, since image may associated
number captions describe well perhaps minor errors. show
crowdsourced human judgments mapped binary relevance judgments
correlate well fine-grained expert judgments, consider metrics based
relevance judgments.
4.1 Experimental Setup
describe data, tasks, systems evaluate experiments.
4.1.1 Data
Since PASCAL 2008 data set contains total 1,000 images, perform
experiments exclusively Flickr 8K set. split 8,000 images corpus (see
Section 2.3) three disjoint sets. training data Dtrain = hItrain , Strain consists
6,000 images, associated five captions, whereas test development data,
Dtest Ddev , consist 1,000 images associated one, arbitrarily chosen, caption.
captions preprocessed spellchecking Linux spell, normalizing compound words
(e.g., t-shirt, shirt, tee-shirt t-shirt), stop word removal, lemmatization.
4.1.2 Tasks
evaluate systems two tasks, sentence-based image annotation (or description)
sentence-based image search. image search, task return ranked list
1,000 images Itest captions (queries) Stest . Image annotation defined
analogously retrieval problem: task return ranked list 1,000 captions
Stest 1,000 test (query) images Itest . cases, ranked lists
produced independently 1,000 possible queries.
4.1.3 Systems
total 30 different systems, uses either nearest-neighbor approach
KCCA, paired different combination image text representations.
purposes discussing different evaluation metrics, focus small number
systems: best-performing nearest-neighbor-based system, NN (NN5idf
F1 ),
small number KCCA-based systems different text kernels: BoW1 BoW5
use simple bag-of-words kernel. TagRank uses
Hwang Graumans (2012)

idf
kernel, Tri5 uses trigram kernel, Tri5Sem (Tri5A,DBNC+ic Appendix B) uses
872

fiFraming Image Description Ranking Task



idf-reweighted trigram kernel distributional alignment-based similarities.
exception BoW1, arbitrarily selected single caption
training image, models use five captions training images. BoW5,
merge single document. cases, follow Moschitti (2009)
sum kernel responses cross product sentences normalization.
systems (including NN) use pyramid kernel image representation.
large-scale evaluations Section 4.3, scores models given Appendix B.
systems use Hardoon et al.s (2004) KCCA implementation, allows us
vary regularization parameter . vary n, number dimensions (largest
eigenvalues) learned projection allowable values parameters based
early exploratory experiments. experiments reported paper, sampled
4 possible values (0.1, 0.5, 1, 5), n chosen 46 possible values range
(10, 6000). two additional parameters fixed advance text
image kernel pair: image kernels either squared cubed, text kernels
regularized multiplying values diagonal factor range (1, 15).
kernel two tasks (image annotation search), use
development set pick five settings n maximize recall original
item first result, five settings maximize recall among first five results,
five settings maximize recall among first ten results, yielding total 15
different models pair kernels task. query image (annotation)
caption (search) test set, 15 models returns ranking 1,000 test
items (sentences images). combine 15 rankings, use Borda counts (van Erp &
Schomaker, 2000), simple, deterministic method rank aggregation: N items
ranked, system assigns score N r item ranks position r = 0...N 1,
final rank item determined sum scores across systems.
break ties items median ranks across models.
4.2 Metrics Quality Individual Image-Caption Pairs
consider metrics consider quality ranked list results (Section 4.3),
first examine metrics measure quality individual image-caption pairs.
4.2.1 Human Evaluation Graded Expert Judgments
Expert scores decision well caption describes image ultimately requires
human judgment. caption generation task, number different evaluation schemes
proposed image description: Ordonez et al. (2011) presented judges
caption produced model asked make forced choice random
image image caption produced for, Kuznetsova et al. (2012) asked judges
choose captions two models given test image. forced
choice tasks may give clear ranking models, cannot compared across different
experiments unless output system made publicly available. One advantage
framing image description ranking task different systems compared
directly test pool. Forced choice evaluations directly measure
quality captions. Following common practice natural language generation, Yang
et al. (2011) Kulkarni et al. (2011) evaluated captions graded scale relevance
873

fiHodosh, Young & Hockenmaier

... describes image
without errors
(score = 4)

selected caption ...
... describes image
minor errors
(score = 3)

... somewhat
related image
(score = 2)

... unrelated
image
(score = 1)

girl wearing
yellow shirt
sunglasses smiles.

man climbs
sheer wall
ice.

Miami basketball
player dribbles
Arizona State player.

group people
walking city street
warm weather.

boy jumps
blue pool
water.

dog grassy field,
looking up.

Basketball players
action.

man riding motor
bike kicks dirt.

Dogs pulling
sled
sled race.

Two little girls
practice martial
arts.

snowboarder
air snowy
mountain.

child jumping
tennis court.

boy blue life
jacket jumps
water.

black dog
purple collar
running.

Figure 4: 14 rating scale fine-grained expert judgments, actual examples
returned best model (Tri5Sem)
readability, Li et al. (2011) added creativity score, Mitchell et al. (2012)
compared systems based whether captions describe main aspects images,
introduce objects appropriate order, semantically correct, seemed
written human.
Since captions test pool produced people, need evaluate
linguistic quality, focus semantic correctness. order obtain
fine-grained assessment description quality, asked three different judges score imagecaption pairs returned systems graded scale 1 4. judges 21
adult native speakers American English, mostly recruited among local graduate
student population. contrast anonymous crowdsourcing-based evaluation described
Section 4.3.2, refer experts. rating scale illustrated Figure 4
actual examples returned models. score 4 means caption describes
image perfectly (without mistakes), score 3 caption almost describes
image (minor mistakes allowed, e.g. number entities), whereas score
2 indicates caption describes aspects image, could
used description, score 1 indicates caption bears relation
image. online appendix paper contains annotation guidelines. Annotators
took average ten minutes per 50 image-caption pairs, image-caption pairs
judged independently three different annotators. Inter-annotator agreement, measured
Krippendorffs (2004) , high ( = 0.81) (Artstein & Poesio, 2008). final score
image-caption pair obtained averaging three individual scores. Since
time-consuming evaluation, judged highest-ranked caption
test image annotation task, focused subset models described
above. gauge difficulty task data set, include random
baseline. Since evaluate single caption image, interested
percentage images suitable caption returned. therefore show
models cumulative distribution test items scores thresholds ranging
874

fiFraming Image Description Ranking Task

Quality first caption (image annotation)
Cumulative distribution expert scores (% X)
= 4.0 3.66 3.33 3.0 2.66 2.33 2.0
0.5 0.6
3.4 4.1
BoW1
6.6 8.1
BoW5
9.7
11.8
TagRank 9.6
12.3

0.7
5.2
9.9
13.6
14.2

1.1
8.5
18.3
19.8
21.1

1.5
11.4
22.9
24.7
25.8

2.9
16.3
29.7
33.0
32.9

7.8
27.1
44.2
46.9
46.2

Tri5Sem 11.0

15.7

23.0

28.1

36.9

53.0

Random
NN

13.3

Table 1: Cumulative distribution expert judgments 14 scale (Figure 4), indicating
percentage image-caption pairs judged given score. Scores
averaged three judges. Superscripts indicate statistically significant difference
Tri5Sem ( : p 0.1, : p 0.05, : p 0.01).
4.0 2.0. threshold interpreted less strict mapping
fine-grained scores binary relevance judgments. order assess whether difference
models given threshold reaches statistical significance, use McNemars
significance test, paired, non-parametric test advocated evaluation
binary classifiers (Dietterich, 1998). Given output models B
set items, McNemars test considers items Bs output differ (the
discordant pairs output) test null hypothesis outputs drawn
underlying population. Among discordant pairs, compares proportion
items model successful model B proportion items
Model B successful model not. results tables, superscripts indicate
whether difference model Tri5Sem statistically significant ( : p 0.1,
: p 0.05, : p 0.01) .
Expert results (Table 1) first interpret expert scores binary relevance
judgments, therefore show cumulative distribution different thresholds in.
see clear differences random baseline, NN, KCCA models
thresholds. differences NN random model, well
KCCA model NN highly significant (p < 0.001) threshold. random
baseline returns perfect caption 0.5% images, good caption (assuming
threshold 2.66) 1.5% images, best KCCA model, Tri5Sem, returns
perfect caption 11.0% good caption 28.1% images. However,
differences among KCCA models subtle, may become apparent
lower thresholds. significant difference BoW5 TagRank
threshold, significantly better BoW1 (p < 0.001) thresholds
3.33 above. Tri5Sem outperforms models, differences BoW5
TagRank reach statistical significance threshold considered
suitable caption lowered either 3.33 (p = 0.06) 3.0 (p = 0.01), 2.66 (p = 0.08)
2.33 (p = 0.005). lack statistical significance partially explained
fact McNemars test relatively low power percentage items
two models successful low, case higher thresholds here.

875

fiHodosh, Young & Hockenmaier

show Sections 4.3.1 4.3.2 significant difference
Tri5Sem two models image annotation extend analysis
beyond highest-ranked caption. shows evaluations based
single caption returned per image may fail uncover significant differences models
become apparent multiple results considered. may important
consider performance annotation retrieval. image retrieval task,
see Tri5Sem significantly outperforms models even first
result considered. Table 1 reveals another artefact McNemars test: since
based absolute differences performance number discordant pairs,
difference BoW1 Tri5Sem thresholds 2.66 2.0 considered less
significant BoW5 Tri5Sem thresholds, even though
BoW1s scores lower BoW5s. Table 2, present systems average expert
scores, use Fishers Randomization Test determine statistical significance. According
evaluation, Tri5Sem significantly better models (p 0.0001
cases), since average score Tri5Sem 2.08, difference reflected
higher thresholds cumulative distribution shown Table 1.
4.2.2 Automatic Evaluation Bleu Rouge
Since human judgments expensive time-consuming collect, examine
well approximated Bleu (Papineni et al., 2002) Rouge (Lin, 2004),
two standard metrics machine translation summarization.
Bleu Rouge scores Bleu Rouge scores computed automatically
number reference captions, used evaluate number caption
generation systems (Kulkarni et al., 2011; Ordonez et al., 2011; Li et al., 2011; Kuznetsova
et al., 2012; Yang et al., 2011; Gupta et al., 2012), although unclear well
correlate human judgments task.
Given caption image associated set reference captions Ri ,
Bleu score proposed image-caption pair (i, s) based n-gram precision
Ri , Rouge based corresponding n-gram recall. common
image description, consider unigram-based scores (only 3.5% possible imagecaption pairs test non-zero bigram-based Bleu-2 score, 39.4% set
non-zero Bleu-1 score). ignore Bleus brevity penalty, since data set
relatively little variation sentence length, would avoid penalizing short,
generic captions include details otherwise correct. Hence, cs (w)
number times word w occurs s:
P

Bleu(i, s) =
Rouge(i, s)

=

min(cs (w),maxrRi cr (w))
P
ws cs (w)
P
P
min(cs (w),cr (w))
rRi
P wrP
rR
wr cr (w)
ws

(28)



reference candidate captions preprocessed. first tokenize sentences
OpenNLP5 tools. break hyphenated words, stripping non-alphanumeric
5. http://opennlp.apache.org

876

fiFraming Image Description Ranking Task

Avg. score first caption
(image annotation)
Expert

Bleu


Rouge

BoW1
BoW5
TagRank

1.22
1.57
1.90
1.98
1.99

0.31
0.35
0.43
0.46
0.46

0.04
0.11
0.14
0.15
0.15

Tri5Sem

2.08

0.48

0.17

Random
NN



Table 2: Comparison averaged scores according 4-point expert evaluation (Figure 4), Bleu Rouge, using five test captions reference. Superscripts indicate
statistically significant difference Tri5Sem ( : p 0.1, : p 0.05, : p 0.01).
hyphen characters, converting words lower case. Following work Lin
(2004), use stemmer (Porter, 1980) remove stopwords compute Rouge
scores. compute Bleu Rouge score system average Bleu
Rouge scores items test set.6
use Fishers Randomization Test (Fisher, 1935; Smucker, Allan, & Carterette, 2007)
assess statistical significance difference models. paired,
sampling-based test evaluates null hypothesis results models
B produced underlying distribution. sample, scores
B assign test item randomly reassigned two models, p-values
obtained comparing actual difference Bs performance
fraction samples equal greater difference models. sample 100,000
reassignments entire test set.
Bleu Rouge results (Table 2) Table 2 shows average Bleu Rouge scores
highest ranked caption pairs returned image annotation systems, computed
reference pool consisting five original captions test image (including
caption randomly selected part candidate pool). scores
lead broad conclusions average expert scores: metrics find clear
differences (p < 0.0001) random baseline models,
well NN KCCA models, none find significant difference
BoW5 TagRank. Tri5Sem outperforms KCCA models according
metrics, expert evaluation Rouge find much larger difference
BoW5 (Experts: p 0.0001, Rouge: p < 0.001) TagRank (Experts: p = 0.001,
Rouge: p = 0.005). Bleu finds significant difference TagRank (p < 0.05),
BoW5 (p < 0.05), indicates Bleu may less well suited identify
subtle differences systems.
Agreement Bleu Rouge expert scores Since difficult measure
directly well Bleu Rouge scores agree expert judgments, consider
6. systems Bleu score usually computed corpus level, since dealing
unigram scores evaluate systems sentences corpus, averaged sentence-level
Bleu scores systems report almost identical (r > 0.997) corpus-level Bleu scores.

877

fiHodosh, Young & Hockenmaier

number different relevance thresholds type score (B , R , E ), turn
binary relevance judgments. allows us use Cohens (1960) measure
agreement corresponding binarized scores. Since Bleu Rouge
require set reference captions test image, compare four different ways
defining set reference captions (for detailed scores, see Tables 8 9 appendix).
Since data set contains multiple descriptions image, first use five
captions reference. setting, Bleu reaches best agreement ( = 0.72)
E = 4.0 B = 1.0 E 3.6 B 0.8. However, high Bleu
scores generally obtained system proposes original caption. Rouge
much lower agreement ( = 0.54) expert scores, obtained R 0.4 vs.
E 4.0 E 3.6, R 0.3 E 3.0. Since data sets may
one caption per image, evaluate reference corpus consists
single caption test pool. case, metrics reach highest
agreement expert threshold E = 4.0 (Bleu: = 0.71, Rouge: = 0.69),
thresholds B 0.8, R 0.9. conclude neither Bleu Rouge
useful scenario, since require high thresholds capture
often system returned reference caption.
Bleu Rouge used evaluate caption generation systems, cannot
assume generated caption identical one reference captions. therefore
examine extent Bleu Rouge scores agree human judgments
candidate pool contains human generated captions, disjoint reference captions. first use reference corpus four captions per image, excluding caption
use candidate pool. case, three metrics show significantly lower agreement
human judgments candidate pool contains reference caption. Bleu
reaches = 0.52 (with B 0.7 E 3.3) Rouge reaches = 0.51
(with R 0.2 E 2.6). simulate case single caption per
image available, evaluate reference corpus consisting one
four captions. case, agreement human judgments even lower: Bleu reaches
= 0.36, Rouge reaches = 0.42. results suggest Bleu Rouge
appropriate metrics pool candidate captions contain reference
captions, lead us question usefulness evaluation caption generation
systems. consistent findings Reiter Belz (2009), studied
Bleu Rouge scores evaluate natural language generation systems, concluded
may useful metrics fluency, poor measures content quality.
4.3 Metrics Large-Scale Evaluation Image Description Systems
Metrics consider first caption returned image cannot capture fact
better model score good captions higher captions, even fails
consider best possible caption. Since systems return ranked list results
item, examine metrics allow us evaluate quality list.
contrast human evaluations described Section 4.2 above, evaluate
image retrieval systems. first consider metrics computed automatically:
recall median rank item (image sentence) originally associated
query sentence image (Section 4.3.1). show use crowdsourcing

878

fiFraming Image Description Ranking Task

Performance: Rank original item
R@k: percentage queries original item among top X responses.
Median r: median rank original item
R@1

Image annotation
R@5 R@10 Median r

BoW1
BoW5
TagRank
Tri5

2.5
4.8
6.2
6.0
7.1

7.6
13.5
17.1
17.0
17.2

9.7
19.7
24.3
23.8
23.7

Tri5Sem

8.3

21.6

30.3

NN

251.0
64.0
58.0
56.0
53.0
34.0

R@1

Image retrieval
R@5 R@10 Median r

2.5
4.5
5.8
5.4
6.0

4.7
14.3
16.7
17.4
17.8

7.2
20.8
23.6
24.3
26.2

7.6

20.7

30.1

272.0
67.0
60.0
52.5
55.0
38.0

Table 3: Model performance measured rank original image caption (=
correct response). R@k: percentage queries correct response among
first X results. Median r: Median position correct response ranked list
results. Superscripts indicate statistically significant difference Tri5Sem ( : p 0.05,
: p 0.01).
collect large number human judgments (Section 4.3.2), use relevance
judgments define two additional metrics: rate success, akin recall,
R-precision, established information retrieval metric (Section 4.3.3). Although
metrics allow us evaluate systems, focus discussion small set
systems considered far, refer interested reader Section B appendix
scores systems.
4.3.1 Recall Median Rank Original Item
One advantage ranking framework position original caption image
among complete list 1,000 test items determined automatically. Since better
system should, average, assign higher rank original items worse system,
use ranks define number different evaluation metrics.
Recall (R@k) median rank scores Since query associated
single gold result, need concerned precision. However, recall position k
(R@k), i.e. percentage test queries model returns original item among
top k results, useful indicator performance, especially context search,
user may satisfied first k results contain single relevant item. focus
k = 1, 5, 10 (R@1, R@5, R@10). Since binary metric (for query, gold item
either found among top k results not), use McNemars test identify
statistically significant differences models. Conversely, median rank indicates
k system recall 50% (i.e. number results one would
consider order find original item half queries). Here, use Fishers
randomization identify significant differences models.
Recall (R@k) median rank results (Table 3) results Table 3 confirm
earlier observation NN baseline clearly beaten KCCA models (p < 0.001
metrics models, except R@1 search, difference BoW1
879

fiHodosh, Young & Hockenmaier

p-value p < 0.01). Since R@1 annotation scores based image-caption
pairs expert scores Table 1, compare directly. difference
R@1 expert scores, even strictest threshold 4.0 experts, indicates
measures capture often original caption returned viewed
lower bound actual performance: Tri5Sem returns original caption first
8.3% images, human judges found captions describe 11.0%
images without errors. discrepancy even larger BoW5 (6.2% vs. 9.7%)
TagRank (6.0% vs. 9.6%). consequence, automatically computed R@1 scores
indicate erroneously statistically significant difference quality
first captions returned Tri5Sem returned BoW5 TagRank, even
though differences significant according human evaluation. However,
metrics based first caption may fail identify differences
models become apparent metrics. example, R@1 reveals
significant difference Tri5 Tri5Sem annotation task, although
difference highly significant according metrics. Section 4.3.3, present
results large-scale human evaluation confirm actual differences
Tri5Sem Tri5 annotation identified first caption
taken account.
Table 11 Section B provides recall median rank scores models.
4.3.2 Collecting Binary Relevance Judgments Large Scale
order perform human evaluation system goes beyond measuring quality
highest ranked result, would obtain relevance judgments imagecaption pairs among top k results query. Since two tasks,
total 30 different systems, set consists 113,006 distinct image-caption pairs
k = 10, rendering exhaustive evaluation four-point scale described Section 4.2.1
infeasible. therefore needed reduce total number judgments needed,
define simpler annotation task could completed less time. Crowdsourcing
platforms Amazon Mechanical Turk offer new possibilities evaluation
enable us collect large number human judgments rapidly inexpensively,
number researchers evaluated caption generation systems Mechanical Turk
(Ordonez et al., 2011; Yang et al., 2011; Kuznetsova et al., 2012; Kulkarni et al., 2011; Li
et al., 2011). experiments performed scale analysis,
evaluated well crowdsourced judgments task approximate
obtained smaller pool judges given detailed instructions.
examine whether crowdsourcing allows us collect reliable relevance judgments
large scale evaluation image description systems.
crowdsourcing task presented workers images paired ten
different captions, asked indicate (via checkboxes) captions describe
image. adapted guidelines developed fine-grained annotation
caption describes image minor errors (corresponding score 3
4-point scale) would still permitted receive positive score. guidelines
found online appendix paper. individual task consisted six different
images, paired ten captions, included copy guidelines. accessed
880

fiFraming Image Description Ranking Task

Amazon Mechanical Turk service provided Crowdflower.com, makes
easy include control items quality control. One six images task
control item, generated taking random images development
set, using one three original captions correct responses, adding
another nine seven randomly selected captions (which verified manually
describe image) incorrect responses. used workers judged 70%
control items correctly. image-caption pair annotated three different
annotators (at total cost 0.9), final score image-caption pair
computed average number positive judgments received.
Filtering unlikely image-caption pairs order reduce number annotations
needed, devised filter based Bleu scores (Papineni et al., 2002) filter imagecaption pairs whose caption dissimilar five captions originally written
image highly unlikely describes image. found filter based
unigram Bleu-1 scores combination stemming stop word removal
standardly done Lins (2004) Rouge script (Bleupre ) proved particularly effective:
threshold Bleupre 0.25 filters 86.0% possible (1,0001,000) image-caption
pairs test set, eliminates 6.7% pairs expert score 2 32
greater, 3.5% pairs expert score 3 greater. slightly higher cutoff
Bleupre 0.26 would filter 90.4% image caption pairs, discard 12.3%
image-caption pairs expert score 2 23 7.5% image-caption pairs
expert score 3. Among 113,006 image-caption pairs actually wished
obtain judgments for, 0.25 filter eliminates 72.8%, reducing number pairs
needed annotate 30,781. Since setup required us pair image number
captions multiple 10, annotated additional 10,374 image caption
pairs filtered out, allowing us evaluate performance filter.
98.3% filtered pairs, Mechanical Turk judges decided caption
describe image, 99.8% them, majority annotators thought so.
found standard Bleu-1 without preprocessing effective filter: threshold
Bleu 0.330 misses 6.9% good captions (with expert score 2 23 ),
filtering 55% entire data set, whereas threshold Bleu 0.333 filters
65% entire data set, misses 11.9% good captions.
Agreement crowdsourced expert judgments use Cohens
measure agreement crowdsourced expert judgments (Table 10
appendix). best agreement obtained crowdsourced scores threshold
0.66 (i.e. least two three judges think caption describes image)
expert scores threshold 3.33 (one expert thinks caption describes
image perfectly two agree think describes image minor
errors, two experts think describes image perfectly one thinks
least related). = 0.79, significantly better approximation expert
scores possible either Bleu Rouge. examine precision, recall
f-scores approximate relevance judgments achieve compared
relevance judgments obtained binarizing expert judgments (Table 10). 98.6%
items perfect expert score (and 95.0% items almost perfect expert
score 3.7) identified, least 94.7% items pass threshold
881

fiHodosh, Young & Hockenmaier

expert score 2.7 greater (i.e. majority experts agreed caption describes
image perfectly minor errors). Using threshold 0.66 adds 2,031 suitable
image-caption pairs 1,000 test images paired original caption. Among
1,000 test captions, 446 still describe single image, 202 describe two test images, 100
three, 252 describe four images. Among 1,000 test images, 331
single (i.e. original) caption, 202 two possible captions, 100 three possible
captions, 317 four captions.
4.3.3 Large-Scale Evaluation Relevance Judgments
crowdsourced relevance judgments allow us define two new metrics, rate
success (S@k) R-precision. believe R-precision reliable indicator
overall performance, since summarizes human judgments single number
depend arbitrary cutoff. therefore use Section 4.3.4 in-depth
analysis impact different linguistic features models incorporate. S@k
rate success scores motivated fact search engines commonly return multiple
results once. Since users may satisfied long results contain least one
relevant item, S@k scores provide direct measure utility hypothetical users.
Rate success (S@k) scores rate success metric (S@k) analogous
recall-based R@k-scores used Table 3, intended measure utility
system hypothetical user. indicates percentage test items least
one relevant result found among highest ranked k results. Following analysis
Section 4.3.2, image-caption pair considered relevant majority judges say
caption describes image.
Rate success results (Table 4) Table 4 confirms NN performs clearly
worse KCCA models. differences Tri5Sem
models shown Table 4 highly statistically significant (p < 0.001) metrics except
S@1 annotation scores, where, agreement expert scores Table 1,
differences NN BoW1 significant. unclear quality first
caption Tri5Sem returns annotation significantly better returned
models, since outperforms metrics. S@k scores
Table 4 indicate Tri5Sem returns relevant caption among top 10 responses
49.1% images, relevant image 48.5% captions. comparison
expert scores Table 1 shows S@1 annotation scores lie expert scores
threshold 3.66 3.0, comparison R@k results Table 3 shows
S@1 scores least twice high corresponding R@1 scores. is,
highest ranked response often relevant item originally associated
query original gold item itself.
R-precision scores Given crowdsourced relevance judgments, test image may
associated multiple relevant captions, test caption may
deemed relevant multiple images besides one originally written for.
queries variable number relevant answers, performance retrieval systems
commonly measured terms R-precision (Manning, Raghavan, & Schtze, 2008). Unlike
S@k scores, metric depend arbitrary cutoff, summarizes
882

fiFraming Image Description Ranking Task

Rate success (S@k)
(Percentage items relevant response among top X results)
Image annotation
S@1
S@5
S@10

Image retrieval
S@1
S@5
S@10

BoW1
BoW5
TagRank
Tri5

5.8
12.2
15.0
16.2
16.4

15.4
30.3
34.1
34.2
32.9

20.2
39.7
42.7
42.9
43.4

5.0
11.4
12.1
12.4
13.1

13.3
30.5
31.5
31.5
33.1

18.4
40.2
40.8
41.6
43.8

Tri5Sem

16.6

37.7

49.1

15.7

36.9

48.5

NN

Table 4: rate success (S@k) indicates percentage test items top X
results contain least one relevant response. Superscripts indicate statistically significant
difference Tri5Sem ( : p 0.1, : p 0.05, : p 0.01)
R-precision
Annotation
NN



Search


Total

BoW1
BoW5
TagRank
Tri5

5.2
10.7
11.1
11.7
11.6

3.8
9.6
10.5
10.5
11.0

4.5
10.1
10.8
11.1
11.3

Tri5Sem

13.7

13.4

13.5

Table 5: Model performance measured R-precision, statistically significant differences Tri5Sem ( : p 0.1, : p 0.05, : p 0.1)
performance system single number, allowing us rank models according
overall performance (see Section 4.3.4 below). S@k scores measure
whether least one relevant items ranked highly, R-precision requires relevant
items ranked highly. therefore better indicator quality mapping
images sentences, since better mapping prefer relevant captions
images irrelevant caption image.
R-precision system query qi ri known relevant items test data
defined precision rank ri (i.e. percentage relevant items among top ri
responses returned s). R-precision obtained averaging test queries.
use Fishers randomization test assess whether differences models
reaches statistical significance.
R-precision results (Table 5) Table 5 gives R-precision model types
used collecting expert judgments (Section 4.2.1). see nearest neighbor
baseline clearly KCCA models (p < 0.001). R-precision indicates
little difference BoW1, BoW5 TagRank terms overall performance. Although TagRank Tri5 outperform BoW1 slightly search (p = 0.062),
statistically significant difference among three models BoW1
Tri5 search (p = 0.01). contrast human evaluation considered
883

fiHodosh, Young & Hockenmaier

R-precision

Tri5
Ann. Search

+IDF
Ann. Search

Ann.

+Align
Search

+Align&IDF
Ann. Search

Tri5

11.6

11.0

12.5ii

11.3

13.4aaa

12.3aa

13.4a

13.2aaa,ii

+DBNC
+Dic
+DBNC+ic

12.7dd
12.7dd
12.5dd

12.1dd
12.8ddd
12.7ddd

12.9
12.8
13.3d

12.2ddd
13.1ddd
13.0ddd

13.2
13.0
13.4aa

12.8a
12.8
13.2dd

12.9
13.3
13.7

12.9a
13.4
13.4

Table 6: effect adding IDF weighting (i), alignment-based similarities (a) distributional similarities (d) Tri5 model. bolded scores indicate Tri5 (top left)
Tri5Sem (Tri5 +Align&IDF+DBNC+ic ; bottom right). Superscripts indicate statistically
significant differences result addition corresponding feature (x : p 0.1,
xx : 0.05, xxx : p 0.01). Dc = distributional similarities computed corpus c (the
BNC, training corpus image captions (ic), both)
first result (Table 1), Tri5Sem clearly outperforms models annotation
retrieval (for differences p 0.0001). Table 12 Appendix B shows scores
models.
4.3.4 Measuring Impact Linguistic Features (Table 6)
results presented far indicate clearly Tri5Sem outperforms simpler Tri5
model, considered impact individual text features distinguish
two models. Since R-precision summarizes performance system single
number, allows us easily perform analysis.
Using R-precision model comparison Table 6 shows results ablation
study compares R-precision Tri5 Tri5Sem trigrambased KCCA models use subset Tri5Sems additional features. basic Tri5
model yields bolded scores shown top left corner. Tri5Sems scores given
bottom right corner. top row contains models capture distributional
similarities, bottom three rows corresponds addition one kind
distributional similarity (computed BNC, image captions training
corpus, corpora) corresponding model top column. first column
contains models capture IDF reweighting alignment-based similarities.
second column corresponds addition IDF reweighting models first
column, third column adds alignment-based similarities models first
column. last column adds IDF-reweighting alignment-based similarities,
scores compared second third column. Superscripts indicate addition particular feature leads statistically significant improvement
model include feature otherwise identical. is,
superscripts show addition distributional similarity metric leads significant improvement model top cell column. superscripts
indicate addition IDF reweighting leads significant improvement
corresponding model without IDF reweighting immediately preceding cell

884

fiFraming Image Description Ranking Task

row. superscripts third column show addition alignment-based
similarity leads significant improvement model without IDF reweighting shown
first column row, superscripts fifth column show
addition alignment-based similarity model IDF reweighting shown
second column row leads significant improvement.
impact IDF weighting, distributional alignment-based similarities
IDF weighting almost always beneficial, improvements obtained adding
IDF weighting given text kernel reach statistical significance (indicated superscripts Table 6) two cases: performance basic Tri5 model image
annotation, performance alignment-based Tri5 model image search.
contrast, adding lexical similarities leads almost always significant highly significant
improvement. Distributional similarities (d superscripts) beneficial basic
Tri5 model tasks, help IDF weighted Tri5 model image search. Distributional similarities computed corpora significantly improve performance
alignment-based Tri5 model incorporate IDF weighting. Adding
alignment-based Tri5 model without IDF weighting leads improvement
search (while helping slightly decreasing performance annotation, albeit
significantly so). improvements search reach statistical significance
similarities computed corpora added. Conversely, adding alignment-based
similarities non-IDF weighted Tri5 model distributional similarities
corpora leads significant improvement annotation. Finally, top cell last
column shows adding alignment-based similarities IDF-weighted Tri5 model
leads significant improvement tasks, although impact search even
greater. Comparing models performance alignment-based Tri5 model without
IDF weighting shows case, IDF weighting helps search. bottom
cells column show adding alignment-based similarities models already
use IDF weighting distributional similarities, adding IDF weighting models
distributional alignment-based similarities generally lead minor improvements.
Table 6 shows whether difference performance obtained addition
one kind feature reaches statistical significance, worth noting model
captures lexical similarities kind significantly better basic Tri5
model tasks (p 0.02 search; p < 0.0001 annotation), IDF-reweighting
leads significant improvement annotation task (p < 0.03). Moreover,
difference Tri5Sem (13.7 search; 13.4 annotation) basic Tri5 kernel
IDF-reweighting (12.5 search; 11.3 annotation) highly significant (p < 0.03 search;
p < 0.0001 annotation).
impact Lins similarity shown Table 6 performance Tri5Lin ,
model augments trigram kernel Lins (1998) WordNet-based similarity.
Tri5Sem include Lins similarity, since found development Tri5Lin
performed similarly worse basic Tri5 model automatic R@k median
rank scores. reflected Tri5Lin R-precision scores 11.7 annotation (Tri5:
11.6) 10.7 search (Tri5: 11.0). Lins similarity may simply coarse
purposes. shown Table 3, hypernym relations WordNet lead associate terms
swimming football other. even though semantically
885

fiHodosh, Young & Hockenmaier

Correlation system rankings
S@k R@k
Annotation


@1
@5
@10

0.86
0.92
0.96

0.69
0.76
0.82

Correlation system rankings
R-precision

Search


0.97
0.97
0.97

(a) S@k vs. R@k

0.87
0.88
0.87

Annotation


R@1
R@5
R@10
Median rank

0.85
0.93
0.94
-0.92

0.68
0.78
0.79
-0.79

Search


0.94
0.96
0.97
-0.97

0.83
0.87
0.89
-0.89

(b) R-precision vs. R@k median rank

Table 7: Correlation (Spearmans Kendalls ) system rankings obtained
human metrics (S@k R-precision) automated scores (R@k median rank)
related fact different kinds sports activities, visually
dissimilar, considered related systems.
4.3.5 Human Evaluations Approximated Automatic Techniques?
R-precision S@k scores require human judgements, therefore cannot applied
datasets judgements yet collected whose scale may prohibit
ever creating definitive set judgements. However, evaluation intended measure
relative progress image description rather absolute performance, automatic
metrics may sufficient approximation, since yield similar ranking systems
R-precision S@k scores. Table 7(a) shows correlations rankings
NN KCCA systems (n = 30) obtained S@k scores obtained
corresponding R@k scores. Table 7(b) shows correlations R-precision
automatic metrics. report two rank correlation coefficients, Spearmans
Kendalls . first observe system rankings obtained via R@1 correlate highly
either R-precision S@1 based rankings. hand, observe
R@5, R@10, median rank scores correlate well R-precision R@5
R@10 correlate well corresponding S@k metrics. suggests rankingbased metrics significantly robust metrics consider quality
first result. Moreover, results indicate framework, systems
expected rank pool images sentences written people, may enable large-scale,
fully automated, evaluation image description systems require equally
large-scale effort collect human judgments.

5. Summary Contributions Conclusions
paper, proposed frame image description task selecting ranking
descriptions among large pool descriptions provided people, framework
provides direct test purely semantic aspects image description need
concerned difficulties involved automatic generation syntactically
correct pragmatically appropriate sentences. introduced new data set
images paired multiple captions, used data set evaluate number
nearest-neighbor KCCA-based models sentence-based image annotation well
886

fiFraming Image Description Ranking Task

converse task sentence-based image search. experiments indicate importance
capturing lexical similarities. Finally, performed in-depth analysis different
evaluation metrics image description.
5.1 Advantages Framing Image Description Ranking Task
One main motivations framing image description ranking rather
generation problem question objective, comparable evaluation ability
understand depicted images. order make progress challenging
task, important define tasks evaluation metrics allow objective
comparison different approaches. argued task ranking pool
captions written people attractive number reasons: first, results obtained
data set compared directly; second, human evaluation easier
generated captions since needs focus factual correctness description rather
grammaticality, fluency, creativity; third, statistically significant differences
systems may become apparent single caption per image considered;
finally, ranking makes possible automate evaluation, e.g. considering position
original caption. Moreover, framing image description ranking task establishes
clear parallels image retrieval, allowing metrics used tasks.
5.2 Data Set
Flickr 8k data set 8,000 images, paired five crowdsourced captions,
unique resource image description. Although much smaller SBU corpus
(Ordonez et al., 2011), believe generic conceptual descriptions corpus
useful image understanding original Flickr captions SBU data set.
data set perhaps similar IAPR data set (Grubinger et al.,
2006), captions corpus shorter, focus salient aspects
image. focus images people animals, IAPR data set covers
slightly different domain, including city pictures landscape shots typically
depict focus people. distinct advantage corpus pairs image
multiple, independently written captions. results indicate using
single caption training time leads significant increase performance.
shown use multiple captions define alignment-based lexical similarity
may useful image description standard distributional WordNet-based
similarities.
5.3 Models
paper first apply Kernel Canonical Correlation analysis (KCCA) sentencebased image description. results show KCCA significantly outperforms nearest
neighbor-based approaches data set 6,000 training images 1,000 test images
(although may scale better large data sets Ordonez et al.s (2011)
SBU corpus, memory requirements train KCCA may prohibitive). One
advantage KCCA-based approaches image description systems geared
specifically towards caption generation applied image de-

887

fiHodosh, Young & Hockenmaier

scription, image retrieval, results indicate performance
tasks fairly similar.
important difference approach taken paper image
description systems features used models presented computed minimal supervision. feature relies supervised classifier
alignment-based similarity, uses POS-tagger identify nouns verbs. Despite
simplicity underlying features, models achieve relatively high performance,
considering difficulty task: Although 1.5% chance randomly
chosen test caption describe test image well, fine-grained human judgments reveal
image annotation, first caption returned best KCCA system good
description 28% test images. Furthermore, large-scale evaluation shows
best system, almost 50% chance suitable image caption
returned among first ten results. results indicate two main reasons
high performance: availability multiple captions image training
time, use robust text representations capture lexical similarities rather
requiring strict equality words. However, clear task remains
far solved, leave question KCCA may benefit models
rely richer visual linguistic features detector responses rich syntactic
analyses future work.
5.4 Evaluating Ranking-Based Image Description Systems
main advantage framing image description ranking problem allows
direct comparison different approaches, since evaluated data set.
makes possible borrow established evaluation metrics information retrieval,
use metrics data sets sentence-based image annotation image
search.
one hand, shown crowdsourcing used collect large number
binary judgments image-caption pairs relatively low price, crowdsourced judgments correlate well fine-grained judgments. able collect
human judgments large scale particularly important retrieval-based approaches
image description, since number relevance judgments need collected
test collection may significantly larger number judgments commonly
used evaluate single caption generation system. However, experiments image
annotation provided example human judgments first caption returned
test image reveal differences systems become apparent
results taken account. fine-grained evaluation indicates evaluations
based single result may require potentially much larger number test items
order reveal robust statistically significant differences. Among human evaluation
metrics compared, believe R-precision computed crowdsourced
relevance judgments robust. R-precision standard metric evaluating
ranked retrieval results items varying number relevant responses, since
yields single score, makes particularly easy compare systems. However,
S@k scores, measure percentage items top k responses contain
relevant result, perhaps direct measure useful system may prac-

888

fiFraming Image Description Ranking Task

tice. release crowdsourced relevance judgments collected order
enable others evaluate image description system data. hope
establish benchmark used direct fair large-scale comparison
arbitrary number image description systems.
hand, shown framework systems evaluated ability rank pool images sentences may make possible perform
fully automated evaluation. Contrary current practice, analysis indicates clearly
standard metrics Bleu Rouge reliable indicators
well captions describe images, even Bleu Rouge-style preprocessing used
effective filter implausible image-caption pairs. Although consider humangenerated captions, stipulate similar observations may hold automatically generated captions, since similar criticisms Bleus appropriateness generation
machine translation evaluation well known (Reiter & Belz, 2009; Callison-Burch, Osborne, & Koehn, 2006). However, ranking-based framework test query associated
gold response originally associated with, results indicate
metrics based rank gold item lead similar conclusions human judgments. suggest evaluation ranking-based image description task
automated, performed potentially much larger scale examined here.
5.5 Implications Evaluation Caption Generation Systems
Image description can, should, treated problem natural language
generation community. automatically generating captions indistinguishable
captions written people (an evaluation criterion used Mitchell et al. (2012)
comparison caption generation systems) requires much ability
provide factually correct information image. believe linguistic
issues need solved generation setting need evaluated separately
ability decide whether given caption describes image. unclear kinds
evaluations performed e.g. Mitchell et al. could ever automated, since question
natural automatically produced caption seems may always require human judgment.
human experiments expensive, since system generates captions,
judgments collected anew system experiment. Since
consensus constitutes good image description, independently obtained human
assessments different caption generation systems compared directly.
means direct comparison systems, e.g. performed Mitchell et al., typically
possible within one research group, since common data set different
system outputs publicly available. Although automatic scores Bleu Rouge
may still useful caption generation measures fluency (Reiter & Belz, 2009),
shown reliable metrics well caption describes image,
especially candidate pool disjoint reference captions. suggests
evaluation syntactic pragmatic aspects caption generation task
automated, may rely human judgments. However, may
possible use framework proposed paper evaluate semantic affinity
functions f (i, s) implicitly used caption generations systems.

889

fiHodosh, Young & Hockenmaier

Acknowledgments
gratefully acknowledge support project National Science Foundation
IIS Medium grant 0803603, CAREER award 1053856, CNS-1205627 CI-P.

Appendix A. Agreement Approximate Metrics Expert
Human Judgments
Tables 8 9 use Cohens Kappa () measure agreement Bleu Rouge
scores expert judgments. selected thresholds yield optimal results.
Table 10 (a) shows agreement crowdsourced judgments expert
judgments. Since best agreement expert scores obtained crowdsourced
judgments using threshold 0.6, Table 10 (b) measures precision recall resulting binary relevance judgments binarized expert judgments obtained varying
thresholds.

Appendix B. Performance Systems
following tables give results models. Section 4 body paper, NN
idf
corresponds NN5idf
F1 , Tri5Sem corresponds Tri5A,DBNC+ic .
R@k median rank scores Table 11 gives recall median rank original
item (Section 4.3.1) models.
Agreement expert Bleu/Rouge scores (Cohens )
Case 1: Scand Sref
5 reference captions/test image (Scand Sref ; R5 )
Expert
E

0.9

Bleu B
0.8
0.7

0.4

Rouge R
0.3
0.2

=4.0
3.6
3.3
3.0
2.6

0.72
0.71
0.64
0.45
0.35

0.70
0.72
0.67
0.54
0.45

0.54
0.54
0.50
0.45
0.38

0.47
0.50
0.50
0.54
0.51

0.59
0.61
0.63
0.57
0.51

0.29
0.33
0.37
0.49
0.53

1 reference caption/test image (Scand = Sref ; R1 (gold))
Expert
E

0.8

Bleu
0.6 0.5

0.9

=4.0
3.6
3.3
3.0
2.6

0.71
0.68
0.60
0.41
0.32

0.70
0.68
0.59
0.42
0.32

0.69
0.65
0.57
0.39
0.30

0.52
0.56
0.56
0.48
0.42

Rouge
0.7 0.3
0.67
0.64
0.56
0.40
0.32

0.35
0.39
0.40
0.45
0.43

Table 8: Agreement (Cohens ) binarized expert Bleu/Rouge scores
pool candidate captions contains test images reference caption(s).

890

fiFraming Image Description Ranking Task

Agreement expert Bleu/Rouge scores (Cohens )
Case 2: Scand 6 Sref
4 reference captions/ test image (R4 )
Expert
E

0.7

Bleu
0.6 0.5

0.4

Rouge
0.3 0.2

=4.0
3.6
3.3
3.0
2.6

0.50
0.51
0.52
0.47
0.41

0.40
0.43
0.46
0.48
0.47

0.44
0.43
0.40
0.39
0.33

0.40
0.43
0.44
0.50
0.48

0.23
0.28
0.32
0.41
0.44

0.26
0.30
0.34
0.47
0.51

1 reference caption/test image (R1 (other))
Expert
E

0.5

Bleu
0.4 0.3

0.4

Rouge
0.3 0.2

=4.0
3.6
3.3
3.0
2.6

0.33
0.34
0.34
0.32
0.30

0.27
0.29
0.32
0.36
0.35

0.33
0.34
0.35
0.39
0.37

0.30
0.32
0.35
0.42
0.41

0.16
0.19
0.22
0.29
0.31

0.18
0.21
0.24
0.34
0.38

Table 9: Agreement (Cohens ) binarized expert Bleu/Rouge scores
pool candidate captions may contain test images reference caption(s).

Agreement expert
lay scores (Cohens )
Expert
E
=4.0
3.6
3.3
3.0
2.6

=1.0
0.75
0.78
0.74
0.56
0.45

Lay vs. expert
relevance judgments (L = 0.66)

Lay L
0.6 0.3
0.69
0.76
0.79
0.71
0.62

0.49
0.57
0.65
0.74
0.73

(a) Agreement (Cohens ) relevance judgments obtained expert
scores (relevance = score E ) lay
scores (relevance = score L )

E

Precision

Recall

F1

=4.0
3.6
3.3
3.0
2.6
2.3

55.9
65.4
75.2
90.0
94.7
98.2

98.6
95.0
88.0
64.7
53.4
40.1

71.4
77.5
81.1
75.3
68.3
57.0

(b) Precision, recall, F1 scores binarized lay scores (L = 0.66)
binarized expert scores varying
thresholds E .

Table 10: Comparing relevance judgments obtained lay scores
obtained expert scores

891

fiHodosh, Young & Hockenmaier

S@k R-precision scores Table 12 gives S@k success rate (Section 4.3.3)
R-precision scores (Section 4.3.3) models, based crowdsourced human
judgments (Section 4.3.2).

892

fiFraming Image Description Ranking Task

Performance models (automatic evaluation)
(R@k: percentage queries original item top X results
Median r: median rank original item)
R@1

Image annotation
R@5 R@10 Median r

R@1

Image search
R@5 R@10 Median r

NN5F1
NN5idf
F1
NN5BoW5
NN5Tri(best)

1.9
2.5
2.1
2.1

5.9
7.6
5.9
5.9

8.7
9.7
9.6
9.4

251.0
251.0
258.5
248.0

2.1
2.5
2.8
2.3

5.2
4.7
6.4
6.1

7.1
7.2
9.1
9.0

278.0
272.0
266.0
240.0

BoW1
Tri1

4.8
4.6

13.5
14.4

19.7
21.0

64.0
68.0

4.5
4.5

14.3
14.0

20.8
22.5

67.0
71.0

BoW5Histo
BoW5
BoW5idf

BoW5 idf
TagRank

5.9
6.2
6.1
6.1
6.0

14.9
17.1
17.0
17.3
17.0

21.2
24.3
23.2
23.9
23.8

69.0
58.0
60.5
56.0
56.0

4.8
5.8
6.4
6.1
5.4

14.2
16.7
16.5
16.9
17.4

20.8
23.6
24.5
24.5
24.3

74.0
60.0
59.5
60.5
52.5

Tri5Histo
Tri5

6.0
7.1

15.0
17.2

21.7
23.7

63.5
53.0

5.7
6.0

14.5
17.8

22.1
26.2

67.0
55.0

Tri5Lin
Tri5DBNC
Tri5Dic
Tri5DBNC+ic

6.2
7.5
7.0
7.3

16.7
19.8
19.5
20.0

23.7
26.1
27.1
27.0

53.5
40.0
36.0
36.0

6.0
7.2
7.0
6.9

16.7
18.4
19.3
19.2

24.4
27.4
27.5
28.0

61.0
44.5
41.0
42.0

Tri5A
Tri5A,DBNC
Tri5A,Dic
Tri5A,DBNC+ic

7.2
7.9
6.9
7.6

20.2
20.3
20.2
20.7

28.0
28.4
29.5
30.0

41.0
39.0
35.0
35.0

6.8
7.8
7.3
7.4

18.5
19.0
19.9
19.4

27.7
27.4
28.7
29.2

41.5
39.0
39.5
38.0

7.6

18.8

25.1

46.0

6.2

18.0

26.5

52.0

6.8
7.3
6.7

18.7
20.4
20.0

28.9
27.5
28.9

40.0
38.0
35.0

6.7
8.1
7.0

18.2
19.1
19.7

27.6
28.4
28.8

45.0
40.5
39.0

7.3
7.5
7.2

21.1
21.3
20.8

28.3
30.0
29.6

37.0
38.0
34.0

7.5
7.8
7.4

19.2
18.9
21.4

28.7
29.0
30.1

38.0
41.5
37.5

idf,Histo
Tri5A,D
BNC+ic

6.5

18.0

26.7

45.0

6.0

18.0

24.2

48.5

idf
A,DBNC+ic

8.3

21.6

30.3

34.0

7.6

20.7

30.1

38.0



Tri5

idf



idf
Tri5
DBNC
idf
Tri5
Dic
idf
Tri5DBNC+ic


idf
Tri5A

idf
Tri5
A,DBNC
idf
Tri5A,D
ic



Tri5

Table 11: Performance models, measured percentage test items
original item returned among top 1, 5 10 results,
well median rank

idf
idf
original item. Section 4, NN5F1 = NN, Tri5A,DBNC+ic = Tri5Sem.

893

fiHodosh, Young & Hockenmaier

Performance models (human evaluation)
S@k: Percentage items relevant response among top X results
R-prec: R-precision computed relevant responses
S@1
NN5F1
NN5idf
F1
NN5BoW5
NN5Tri(best)

Image annotation
S@5 S@10 R-prec.

S@1

Image search
S@5 S@10 R-prec.

4.9
5.8
6.4
7.2

13.3
15.4
14.8
17.4

19.1
20.2
20.6
23.1

4.2
5.2
5.4
6.2

4.9
5.0
5.7
4.4

13.2
13.3
13.4
13.5

17.8
18.4
18.4
19.8

3.8
3.8
4.6
4.3

BoW1
Tri1

12.2
12.8

30.3
32.2

39.7
40.2

10.7
10.5

11.4
12.2

30.5
30.6

40.2
41.5

9.6
9.9

BoW5Histo
BoW5
BoW5idf

BoW5 idf
TagRank

13.9
15.0
13.9
15.0
16.2

29.8
34.1
32.7
34.0
34.2

39.6
42.7
42.4
42.7
42.9

9.9
11.1
11.0
11.3
11.7

11.5
12.1
13.3
12.9
12.4

28.0
31.5
30.8
31.3
31.5

38.1
40.8
41.8
41.0
41.6

9.3
10.5
10.6
10.7
10.5

Tri5Histo
Tri5

15.0
16.4

29.0
32.9

38.9
43.4

9.9
11.6

12.9
13.1

28.9
33.1

39.9
43.8

10.5
11.0

Tri5Lin
Tri5DBNC
Tri5Dic
Tri5DBNC+ic
Tri5A
Tri5A,DBNC
Tri5A,Dic
Tri5A,DBNC+ic

15.5
16.8
15.8
16.4
17.3
16.6
15.8
16.4

34.1
37.4
36.7
37.2
36.9
36.5
37.0
37.4

43.8
45.5
47.0
47.1
47.4
47.4
48.2
48.3

11.7
12.7
12.7
12.5
13.4
13.2
13.0
13.4

12.7
14.5
14.5
14.8
14.3
15.3
15.2
15.4

32.5
35.3
36.6
36.3
35.4
35.0
37.4
37.0

41.7
44.9
46.1
45.8
46.6
45.8
47.6
46.8

10.7
12.1
12.8
12.7
12.3
12.8
12.8
13.2

16.9

35.4

44.2

12.5

13.0

33.4

43.9

11.3

16.1
15.9
16.2

36.0
36.9
37.4

47.5
46.8
47.5

12.9
12.8
13.3

15.0
16.0
15.3

34.0
35.8
34.8

44.6
47.5
46.7

12.2
13.1
13.0

17.4
15.7
15.7

37.6
36.9
37.3

46.3
48.1
47.3

13.4
12.9
13.3

15.8
15.8
15.5

36.3
35.3
38.3

47.3
47.2
47.8

13.2
12.9
13.4

13.6

30.6

41.9

11.1

14.4

33.8

42.7

12.2

16.6

37.7

49.1

13.7

15.7

36.9

48.5

13.4

Tri5
Tri5
Tri5
Tri5
Tri5
Tri5
Tri5
Tri5
Tri5


idf

idf

BNC
idf

ic
idf
DBNC+ic

idf


idf
A,D
BNC
idf
A,Dic

idf,Histo
A,DBNC+ic

idf
A,DBNC+ic

Table 12: Performance models, measured percentage test items
return item deemed relevant according crowdsourced judgments
among top 1, 5 10 results,
R-precision computed judgments.

idf
idf
Section 4, NN5F1 = NN, Tri5A,DBNC+ic = Tri5Sem.

894

fiFraming Image Description Ranking Task

References
Artstein, R., & Poesio, M. (2008). Inter-coder agreement computational linguistics.
Computational Linguistics, 34 (4), 555596.
Bach, F. R., & Jordan, M. I. (2002). Kernel independent component analysis. Journal
Machine Learning Research, 3, 148.
Barnard, K., Duygulu, P., Forsyth, D., Freitas, N. D., Blei, D. M., & Jordan, M. I. (2003).
Matching words pictures. Journal Machine Learning Research, 3, 11071135.
Blei, D. M., & Jordan, M. I. (2003). Modeling annotated data. SIGIR 2003: Proceedings
26th Annual International ACM SIGIR Conference Research Development
Information Retrieval, pp. 127134, Toronto, Ontario, Canada.
Bloehdorn, S., Basili, R., Cammisa, M., & Moschitti, A. (2006). Semantic kernels text
classification based topological measures feature similarity. Proceedings
6th IEEE International Conference Data Mining (ICDM 2006), pp. 808812, Hong
Kong, China.
BNC Consortium (2007). British National Corpus, version 3 (BNC XML edition).
http://www.natcorp.ox.ac.uk.
Brown, P. F., Pietra, V. J. D., Pietra, S. A. D., & Mercer, R. L. (1993). mathematics
statistical machine translation: parameter estimation. Computational Linguistics,
19 (2), 263311.
Callison-Burch, C., Osborne, M., & Koehn, P. (2006). Re-evaluation role bleu
machine translation research. Proceedings 11th Conference European Chapter Association Computational Linguistics (EACL), pp. 249256,
Trento, Italy.
Cohen, J. (1960). coefficient agreement nominal scales. Educational Psychological Measurement, 20 (1), 3746.
Croce, D., Moschitti, A., & Basili, R. (2011). Structured lexical similarity via convolution
kernels dependency trees. Proceedings 2011 Conference Empirical
Methods Natural Language Processing (EMNLP), pp. 10341046, Edinburgh, UK.
Dale, R., & White, M. (Eds.). (2007). Workshop Shared Tasks Comparative Evaluation Natural Language Generation: Position Papers, Arlington, VA, USA.
Datta, R., Joshi, D., Li, J., & Wang, J. Z. (2008). Image retrieval: Ideas, influences,
trends new age. ACM Computing Surveys, 40 (2), 5:15:60.
Deschacht, K., & Moens, M.-F. (2007). Text analysis automatic image annotation.
Proceedings 45th Annual Meeting Association Computational Linguistics (ACL), pp. 10001007, Prague, Czech Republic.
Dietterich, T. G. (1998). Approximate statistical tests comparing supervised classification
learning algorithms. Neural Computation, 10 (7), 18951923.
Everingham, M., Gool, L. V., Williams, C., Winn, J., & Zisserman, A. (2008).
PASCAL Visual Object Classes Challenge 2008 (VOC2008) Results. http://www.
pascal-network.org/challenges/VOC/voc2008/workshop/.
895

fiHodosh, Young & Hockenmaier

Farhadi, A., Hejrati, M., Sadeghi, M. A., Young, P., Rashtchian, C., Hockenmaier, J., &
Forsyth, D. (2010). Every picture tells story: Generating sentences images.
Proceedings European Conference Computer Vision (ECCV), Part IV, pp.
1529, Heraklion, Greece.
Fellbaum, C. (1998). WordNet: Electronic Lexical Database. Bradford Books.
Felzenszwalb, P., McAllester, D., & Ramanan, D. (2008). discriminatively trained, multiscale, deformable part model. Proceedings 2008 IEEE Conference Computer Vision Pattern Recognition (CVPR), pp. 18, Anchorage, AK, USA.
Feng, Y., & Lapata, M. (2008). Automatic image annotation using auxiliary text information. Proceedings 46th Annual Meeting Association Computational
Linguistics: Human Language Technologies (ACL-08: HLT), pp. 272280, Columbus,
OH, USA.
Feng, Y., & Lapata, M. (2010). many words picture worth? automatic caption generation news images. Proceedings 48th Annual Meeting Association
Computational Linguistics (ACL), pp. 12391249, Uppsala, Sweden.
Fisher, R. A. (1935). Design Experiments. Olyver Boyd, Edinburgh, UK.
Grangier, D., & Bengio, S. (2008). discriminative kernel-based approach rank images
text queries. IEEE Transactions Pattern Analysis Machine Intelligence,
30, 13711384.
Grice, H. P. (1975). Logic conversation. Davidson, D., & Harman, G. H. (Eds.),
Logic Grammar, pp. 6475. Dickenson Publishing Co., Encino, CA, USA.
Grubinger, M., Clough, P., Mller, H., & Deselaers, T. (2006). IAPR benchmark: new
evaluation resource visual information systems. OntoImage 2006, Workshop
Language Resources Content-based Image Retrieval LREC 2006, pp. 1323,
Genoa, Italy.
Gupta, A., Verma, Y., & Jawahar, C. (2012). Choosing linguistics vision describe
images. Proceedings Twenty-Sixth AAAI Conference Artificial Intelligence,
Toronto, Ontario, Canada.
Hardoon, D. R., Saunders, C., Szedmak, S., & Shawe-Taylor, J. (2006). correlation approach automatic image annotation. Li, X., Zaane, O. R., & Li, Z.-H. (Eds.),
Advanced Data Mining Applications, Vol. 4093 Lecture Notes Computer Science, pp. 681692. Springer Berlin Heidelberg.
Hardoon, D. R., Szedmak, S. R., & Shawe-Taylor, J. R. (2004). Canonical correlation
analysis: overview application learning methods. Neural Computation, 16,
26392664.
Hotelling, H. (1936). Relations two sets variates. Biometrika, 28 (3/4), 321377.
Hwang, S., & Grauman, K. (2012). Learning relative importance objects tagged
images retrieval cross-modal search. International Journal Computer Vision,
100 (2), 134153.

896

fiFraming Image Description Ranking Task

Jaimes, A., Jaimes, R., & Chang, S.-F. (2000). conceptual framework indexing visual
information multiple levels. Internet Imaging 2000, Vol. 3964 Proceedings
SPIE, pp. 215, San Jose, CA, USA.
Jurafsky, D., & Martin, J. H. (2008). Speech Language Processing (2nd edition). Prentice
Hall.
Krippendorff, K. (2004). Content analysis: introduction methodology. Sage.
Kulkarni, G., Premraj, V., Dhar, S., Li, S., Choi, Y., Berg, A. C., & Berg, T. L. (2011).
Baby talk: Understanding generating simple image descriptions. Proceedings
2011 IEEE Conference Computer Vision Pattern Recognition (CVPR),
pp. 16011608.
Kuznetsova, P., Ordonez, V., Berg, A., Berg, T., & Choi, Y. (2012). Collective generation
natural image descriptions. Proceedings 50th Annual Meeting Association Computational Linguistics (Volume 1: Long Papers), pp. 359368, Jeju
Island, Korea.
Lavrenko, V., Manmatha, R., & Jeon, J. (2004). model learning semantics pictures. Thrun, S., Saul, L., & Schlkopf, B. (Eds.), Advances Neural Information
Processing Systems 16, Cambridge, MA, USA.
Lazebnik, S., Schmid, C., & Ponce, J. (2009). Spatial pyramid matching. S. Dickinson,
A. Leonardis, B. S., & Tarr, M. (Eds.), Object Categorization: Computer Human
Vision Perspectives, chap. 21, pp. 401415. Cambridge University Press.
Li, S., Kulkarni, G., Berg, T. L., Berg, A. C., & Choi, Y. (2011). Composing simple image descriptions using web-scale n-grams. Proceedings Fifteenth Conference
Computational Natural Language Learning (CoNLL), pp. 220228, Portland, OR,
USA.
Lin, C.-Y. (2004). Rouge: package automatic evaluation summaries. MarieFrancine Moens, S. S. (Ed.), Text Summarization Branches Out: Proceedings
ACL-04 Workshop, pp. 7481, Barcelona, Spain.
Lin, C.-Y., & Hovy, E. H. (2003). Automatic evaluation summaries using n-gram cooccurrence statistics. Proceedings 2003 Human Language Technology Conference North American Chapter Association Computational Linguistics
(HLT-NAACL), pp. 7178, Edmonton, AB, Canada.
Lin, D. (1998). information-theoretic definition similarity. Proceedings Fifteenth International Conference Machine Learning (ICML), pp. 296304, Madison,
WI, USA.
Lowe, D. G. (2004). Distinctive image features scale-invariant keypoints. Internationa
Journal Computer Vision, 60 (2), 91110.
Makadia, A., Pavlovic, V., & Kumar, S. (2010). Baselines image annotation. International
Journal Computer Vision, 90 (1), 88105.
Manning, C. D., Raghavan, P., & Schtze, H. (2008). Introduction Information Retrieval.
Cambridge University Press.

897

fiHodosh, Young & Hockenmaier

Mitchell, M., Dodge, J., Goyal, A., Yamaguchi, K., Stratos, K., Han, X., Mensch, A., Berg,
A., Berg, T., & Daume III, H. (2012). Midge: Generating image descriptions
computer vision detections. Proceedings 13th Conference European
Chapter Association Computational Linguistics (EACL), pp. 747756, Avignon, France.
Moschitti, A. (2009). Syntactic semantic kernels short text pair categorization.
Proceedings 12th Conference European Chapter Association
Computational Linguistics (EACL), pp. 576584, Athens, Greece.
Moschitti, A., Pighin, D., & Basili, R. (2008). Tree kernels semantic role labeling.
Computational Linguistics, 34 (2), 193224.
Och, F. J., & Ney, H. (2003). systematic comparison various statistical alignment
models. Computational Linguistics, 29 (1), 1951.
Ordonez, V., Kulkarni, G., & Berg, T. L. (2011). Im2text: Describing images using 1 million
captioned photographs. Advances Neural Information Processing Systems 24,
pp. 11431151.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). Bleu: method automatic
evaluation machine translation. Proceedings 40th Annual Meeting Association Computational Linguistics (ACL), pp. 311318, Philadelphia, PA, USA.
Popescu, A., Tsikrika, T., & Kludas, J. (2010). Overview Wikipedia retrieval task
ImageCLEF 2010. CLEF (Notebook Papers/LABs/Workshops), Padua, Italy.
Porter, M. F. (1980). algorithm suffix stripping. Program, 14 (3), 130137.
Rashtchian, C., Young, P., Hodosh, M., & Hockenmaier, J. (2010). Collecting image annotations using Amazons Mechanical Turk. NAACL Workshop Creating Speech
Language Data Amazons Mechanical Turk, pp. 139147, Los Angeles, CA,
USA.
Rasiwasia, N., Pereira, J. C., Coviello, E., Doyle, G., Lanckriet, G. R., Levy, R., & Vasconcelos, N. (2010). new approach cross-modal multimedia retrieval. Proceedings
International Conference Multimedia (MM), pp. 251260, New York, NY,
USA.
Reiter, E., & Belz, A. (2009). investigation validity metrics automatically evaluating natural language generation systems. Computational Linguistics,
35 (4), 529558.
Shatford, S. (1986). Analyzing subject picture: theoretical approach. Cataloging
& Classification Quarterly, 6, 3962.
Shawe-Taylor, J., & Cristianini, N. (2004). Kernel Methods Pattern Analysis. Cambridge
University Press.
Smucker, M. D., Allan, J., & Carterette, B. (2007). comparison statistical significance
tests information retrieval evaluation. Proceedings Sixteenth ACM Conference Information Knowledge Management (CIKM), pp. 623632, Lisbon,
Portugal.

898

fiFraming Image Description Ranking Task

Socher, R., & Li, F.-F. (2010). Connecting modalities: Semi-supervised segmentation
annotation images using unaligned text corpora. Proceedings 2010 IEEE
Conference Computer Vision Pattern Recognition (CVPR), pp. 966973, San
Francisco, CA, USA.
van Erp, M., & Schomaker, L. (2000). Variants Borda count method combining
ranked classifier hypotheses. Proceedings Seventh International Workshop
Frontiers Handwriting Recognition (IWFHR), pp. 443452, Nijmegen, Netherlands.
Varma, M., & Zisserman, A. (2005). statistical approach texture classification
single images. International Journal Computer Vision, 62, 6181.
Vedaldi, A., & Fulkerson, B. (2008). VLFeat: open portable library computer
vision algorithms. http://www.vlfeat.org/.
Weston, J., Bengio, S., & Usunier, N. (2010). Large scale image annotation: learning rank
joint word-image embeddings. Machine Learning, 81 (1), 2135.
Yang, Y., Teo, C., Daume III, H., & Aloimonos, Y. (2011). Corpus-guided sentence generation natural images. Proceedings 2011 Conference Empirical Methods
Natural Language Processing (EMNLP), pp. 444454, Edinburgh, UK.

899


