Journal Artificial Intelligence Research 47 (2013) 441-473

Submitted 12/12; published 07/13

Decentralized Anti-coordination
Multi-agent Learning
Ludek Cigler
Boi Faltings

ludek.cigler@epfl.ch
boi.faltings@epfl.ch

Artificial Intelligence Laboratory
Ecole Polytechnique Federale de Lausanne
CH-1015 Lausanne, Switzerland

Abstract
achieve optimal outcome many situations, agents need choose distinct
actions one another. case notably many resource allocation problems,
single resource used one agent time. shall designer
multi-agent system program identical agents behave different way?
game theoretic perspective, situations lead undesirable Nash equilibria.
example consider resource allocation game two players compete exclusive
access single resource. three Nash equilibria. two pure-strategy NE
efficient, fair. one mixed-strategy NE fair, efficient. Aumanns
notion correlated equilibrium fixes problem: assumes correlation device
suggests agent action take.
However, smart coordination device might available. propose using
randomly chosen, stupid integer coordination signal. Smart agents learn action
use value coordination signal.
present multi-agent learning algorithm converges polynomial number
steps correlated equilibrium channel allocation game, variant resource
allocation game. show agents learn play coordination signal value
randomly chosen pure-strategy Nash equilibrium game. Therefore, outcome
efficient correlated equilibrium. CE becomes fair number
available coordination signal values increases.

1. Introduction
many situations, agents coordinate actions order use limited
resource: communication networks, channel might used one agent time.
driving car, agent prefers choose road less traffic, i.e. one chosen
smaller number agents. bidding one item several simultaneous
auctions, agent prefers auction less participants, usually lead
lower price. situations require agents take different decision. However,
agents identical, problem one face same.
learn behave differently everyone else?
Second problem arises agents common preferences action
want take: communication networks problem, every agent prefers transmit
quiet. traffic situation, agents might prefer shorter path. order
achieve efficient allocation, necessary precisely agents stay quiet,
take longer path. achieve agents exploited?
c
2013
AI Access Foundation. rights reserved.

fiCigler & Faltings

agents learn alternate, taking longer road one day, taking shorter
road next day?
central coordinator possesses complete information agents preferences
available resources easily recommend agent action take.
However, omniscient central coordinator always available. Therefore,
would able use distributed scheme. consider scenarios
agents try use set resources repeatedly, use history past
interactions learn coordinate access resources future.
particular, consider problem radio channel access. problem, N agents
try transmit C non-overlapping channels. Fully decentralized schemes,
ALOHA (Abramson, 1970), achieve throughput 1e 37%, i.e. transmission succeeds probability 1e 0.37. complex schemes, based
distributed constraint optimization (Cheng, Raja, Xie, & Howitt, 2009), reach
throughput close 100%. throughput, mean probability successful transmission given channel. However, messages necessary implement schemes
create overhead eliminates part benefits.
paper propose instead use simple signal agents observe
ergodically fluctuates. signal could common clock, radio broadcasting
specified frequency, decimal part price certain stock given time, etc.
Depending stupid signal, smart agents learn take different action
value. say signal stupid, doesnt anything
game agents give meaning acting differently
value coordination signal.
Lets look simplest example channel access problem: one 2 agents try
transmit one shared channel. model situation game normal
form. Agents choose two actions: stay quiet (Q) transmit (T ).
one agent may transmit successfully time. agent transmits alone, receives
positive payoff. agent transmit channel, payoff 0. agents
try transmit channel time, transmissions fail incur
cost c.
payoff matrix game looks follows:

Q


Q
0, 0
1, 0


0, 1
c, c

game two pure-strategy Nash equilibria (NE), one player stays quiet
one transmits. one mixed-strategy NE, player stays
1
quiet probability c+1
. two pure-strategy NE efficient, maximize
social welfare, fair: one player gets full payoff, even though
game symmetric. mixed-strategy NE fair, efficient: expected
payoff players 0.
such, Nash equilibria game rather undesirable: either efficient
fair, time. seminal paper, Aumann (1974) proposed
notion correlated equilibrium fixes problem. correlated equilibrium (CE)
probability distribution joint strategy profiles game. correlation device
442

fiDecentralized Anti-coordination Multi-agent Learning

samples distribution recommends action agent play. probability
distribution CE agents incentive deviate recommended
action.
simple game described above, exists CE fair socially
efficient: correlation device samples two pure-strategy NE probability 21 recommends players NE play. corresponds
authority tells player whether stay quiet transmit.
Correlated equilibria several nice properties: easier find succinct
representation game, polynomial time, see (Papadimitriou & Roughgarden, 2008).
Also, every Nash equilibrium correlated equilibrium. Also, convex combination
two correlated equilibria correlated equilibrium. However, smart correlation device
randomizes joint strategy profiles might always available.
possible achieve correlated equilibrium without actual correlation device.
Assume game played repeatedly, agents observe history
actions taken opponents. learn predict future action (or distribution future actions) opponents. predictions need calibrated, is,
predicted probability agent play certain action aj converge
actual frequency agent plays action aj . Agents always play action
best response predictions opponents actions. Foster Vohra (1997)
showed case, play converges set correlated equilibria.
However, paper, Foster Vohra provide specific learning rule
achieve certain CE. Furthermore, approach requires every agent able
observe actions every opponent. requirement met, convergence
correlated equilibrium guaranteed anymore.
paper, focus generalization simple channel allocation problem
described above. N agents always data transmit,
C channels transmit. assume N C. Access channel
slotted, is, agents synchronized start transmissions
time. Also, transmissions must length. one agent attempts
transmit single channel, collision occurs none transmissions
successful. unsuccessful transmission cost agent, since consume
(possibly constrained) power benefit. transmitting cost
anything.
assume agents receive binary feedback. transmitted data,
find whether transmission successful. transmit,
choose channel observe. receive information whether observed channel
free not.
described normal-form game, problem several efficient (but unfair)
pure-strategy Nash equilibria, group C agents gets assigned channels.
remaining N C agents get stranded. fair inefficient mixed-strategy NE,
agents choose transmission channels random. example resource
allocation game above, exists correlated equilibrium efficient fair.
stupid coordination signal introduced paper helps agents learn
play (potentially) different efficient outcome value. way,
reach efficient allocation still preserving level fairness.
443

fiCigler & Faltings

main contributions work following:
propose learning strategy agents channel allocation game that, using
minimal information, converges polynomial time randomly chosen efficient
pure-strategy Nash equilibrium game.
show agents observe common discrete correlation signal,
learn play efficient pure-strategy NE signal value. result
correlated equilibrium increasingly fair number available signals K
increases.
experimentally evaluate sensitive algorithm player population
dynamic, i.e. players leave enter system. evaluate
algorithms resistance noise, feedback players receive
coordination signal observe.
channel allocation algorithm proposed paper implemented Wang,
Wu, Hamdi, Ni (2011) real-world wireless network setting. showed
wireless devices use use actual data transmitted coordination
signal. way, able achieve 2-3 throughput gain compared random
access protocols ALOHA.
worth noting work, focus reaching correlated fair
outcome, provided agents willing cooperate. situations using
resources costs nothing, self-interested agent could stubbornly keep using it. Everyone
else better trying access resource. sometimes called
watch crazy bully strategy (Littman & Stone, 2002).
order prevent kind behavior, would need make sure order
use resource, agent pay cost. cost may already implicit
problem, fact wireless transmission costs energy, may imposed
external payments. recent work (Cigler & Faltings, 2012), show leads
equilibria rational agents indifferent accessing resource yielding,
equilibria implement allocation policy rational agents. consider
issue beyond scope paper refer reader work
deeper analysis.
rest paper organized follows: Section 2, give basic definitions
game theory theory Markov chains use throughout paper.
Section 3, present algorithm agents use learn action possible
correlation signal value. Section 4 prove algorithm converges
efficient correlated equilibrium polynomial time number agents channels.
show fairness resulting equilibria increases number signals K
increases Section 5. Section 6 highlights experiments show actual convergence
rate fairness. show algorithm performs case population
changing dynamically. Section 7 present related work game theory
cognitive radio literature, Section 8 concludes.
444

fiDecentralized Anti-coordination Multi-agent Learning

2. Preliminaries
section, introduce basic concepts game theory theory
Markov chains going use throughout paper.
2.1 Game Theory
Game theory study interactions among independent, self-interested agents.
agent participates game called player. player utility function
associated state world. Self-interested players take actions achieve
state world maximizes utility. Game theory studies attempts
predict behaviour, well final outcome interactions. Leyton-Brown
Shoham (2008) give complete introduction game theory.
basic way represent strategic interaction (game) using so-called normal
form.
Definition 1. finite, N -person normal-form game tuple (N, A, u),
N set N players;
= A1 A2 . . . , Ai set actions available player i. vector
= (a1 , a2 , . . . , ) called action profile;
u = (u1 , u2 , . . . , uN ), ui : R utility function player assigns
action vector certain utility (payoff).
playing game, players select strategy. pure strategy
player selects one action ai Ai . vector pure strategies player =
(1 , 2 , . . . , N ) called pure strategy profile. mixed strategy selects probability
distribution entire action space, i.e. (Ai ). mixed strategy profile
vector mixed strategies player.
Definition 2. say mixed strategy player best response strategy
profile opponents strategy i0 ,
ui (i , ) ui (i0 , )
One basic goals game theory predict outcome strategic interaction.
outcome stable therefore, usually called equilibrium. One requirement outcome equilibrium none players incentive
change strategy, i.e. players play best-response strategies others.
defines perhaps important equilibrium concept, Nash equilibrium:
Definition 3. strategy profile = (1 , 2 , . . . , N ) Nash equilibrium (NE)
every player i, strategy best response strategies others .
essential Nash equilibria are, several disadvantages. First, may
hard find: Chen Deng (2006) show finding NE PPAD-complete. Second,
might multiple Nash equilibria, shown example Section 1. Third,
efficient NE may fair one, even symmetric game. give
formal definition correlated equilibrium fixes issues:
445

fiCigler & Faltings

Definition 4. Given N -player game (N, A, u), correlated equilibrium tuple (v, , ),
v tuple random variables v = (v1 , v2 , . . . , vN ) whose domains =
(D1 , D2 , . . . , DN ), joint probability distribution v, = (1 , 2 , . . . , N )
vector mappings : Di 7 Ai , player every mapping 0i : Di 7 Ai
case
X
X

(d)ui (1 (d1 ), 2 (d2 ), . . . , N (dN ))
(d)ui 01 (d1 ), 02 (d2 ), . . . , 0N (dN ) .
dD

dD

2.2 Markov Chains
learning algorithm propose analyze paper described randomized algorithm. randomized algorithm, steps depend value
random variable. One useful technique analyze randomized algorithms describe
execution Markov chain.
Markov chain random process Markov property. random process
collection random variables; usually describes evolution random value
time. process Markov property state (or value) next time step depends
exclusively value previous step, values past.
say process memoryless. imagine execution randomized
algorithm finite-state automaton non-deterministic steps, easy see
execution maps Markov chain.
formal definition Markov chain follows:
Definition 5. (Norris, 1998) Let countable set. called state
called state space. say P= (i : I) measure 0 <
I. addition total mass iI equals 1, call distribution.
work throughout probability space (, F, P). Recall random variable X
values function X : I. Suppose set
= Pr(X = i) = Pr ({ : X() = i}) .
defines distribution, distribution X. think X modelling random
state takes value probability .
say matrix P = (pij : i, j I) stochastic every row (pij : j I)
distribution.
say (Xt )t0 Markov chain initial distribution transition matrix
P
1. X0 distribution ;
2. 0, conditional Xt = i, Xt+1 distribution (pij : j I) independent
X0 , X1 , . . . , Xt1 .
explicitly, conditions state that, 0 i0 , . . . , it+1 I,
1. Pr(X0 = i0 ) = i0 ;
2. Pr(Xt+1 = it+1 |X0 = i0 , . . . , Xt = ) = pit it+1 .
446

fiDecentralized Anti-coordination Multi-agent Learning

Theorem 1. Let set states. vector hitting probabilities hA = (hA
:
{0, 1, . . . , N }) minimal non-negative solution system linear equations

1


hi = P

p
h
/A
j{0,1,...,N } ij j
Intuitively, hitting probability hA
probability Markov chain
starts state i, ever reach states A.
One property randomized algorithms particularly interested
convergence. set states algorithm converged, define
time takes reach state set state corresponding
Markov chain hitting time:
Definition 6. (Norris, 1998) Let (Xt )t0 Markov chain state space I. hitting
time subset random variable H : {0, 1, . . .} {} given
H () = inf{t 0 : Xt () A}
Specifically, interested expected hitting time set states A, given
Markov chain starts initial state X0 = i. denote quantity
kiA = Ei (H ).
general, expected hitting time set states found solving
system linear equations.
Theorem 2. vector expected hitting times k = E(H ) = (kiA : I)
minimal non-negative solution system linear equations

ki = 0 P

(1)

kiA = 1 + j
p
k
/A
ij
j
/
Convergence absorbing state may guaranteed general Markov chain.
calculate probability reaching absorbing state, use following theorem
(Norris, 1998):
Theorem 3. Let set states. vector hitting probabilities hA = (hA
:
{0, 1, . . . , N }) minimal non-negative solution system linear equations

1


hi = P

p
h
/A
ij
j
j{0,1,...,N }
Solving systems linear equations Theorems 2 3 analytically might
difficult many Markov chains though. Fortunately, Markov chain
one absorbing state = 0, move state j j, use
following theorem derive upper bound expected hitting time, proved
Rego (1992):
Theorem 4. Let = {0}.
1 : E(Xt+1 |Xt = i) <
> 1,


kiA < log +

447


1




fiCigler & Faltings

3. Learning Algorithm
section, describe algorithm agents use learn correlated equilibrium channel allocation game.
Let us denote space available correlation signals K := {0, 1, . . . , K 1},
space available channels C := {1, 2, . . . , C}. Assume C N ,
agents channels (the opposite case easier). agent strategy : K {0}C
uses decide channel access time receives correlation
signal kt . (kt ) = 0, agent transmit signal kt . agent stores
strategy simply table.
agent adapts strategy follows:
1. beginning, k0 K, (k0 ) initialized uniformly random C.
is, every agent picks random channel transmit on, agent monitor
channels.
2. time t:
(kt ) > 0, agent tries transmit channel (kt ).
otherwise (kt ) = 0, agent chooses random channel mi (t) C
monitor activity.
3. Subsequently, agent observes outcome choice: agent transmitted
channel, observes whether transmission successful. was,
agent keep strategy unchanged. collision occurred, agent sets
(kt ) := 0 probability p. probability 1 p, strategy remains same.
4. agent transmit, observes whether channel mi (t) monitored
free. channel free, agent sets (kt ) := mi (t) probability 1.
channel free, strategy remains same.

4. Convergence
important property learning algorithm if, fast converge
pure-strategy Nash equilibrium channel allocation game every signal value.
algorithm randomized. Therefore, instead analyzing worst-case behavior (that may
arbitrarily bad), analyze expected number steps convergence.
4.1 Convergence C = 1, K = 1
single channel single coordination signal, prove following theorem:
Theorem 5. N agents C = 1, K = 1, 0 < p < 1, expected number steps
allocation algorithm
converges
pure-strategy Nash equilibrium channel


1
allocation game p(1p) log N .
prove convergence algorithm, useful describe execution
Markov chain.
448

fiDecentralized Anti-coordination Multi-agent Learning

N agents compete single signal value, state Markov chain
vector {0, 1}N denotes agents attempting transmit. purpose
convergence proof, important many agents trying transmit,
agents. probability agents back-off
everyone. Therefore, describe algorithm execution using following chain:
Definition 7. Markov chain describing execution allocation algorithm
C = 1, K = 1, 0 < p < 1 chain whose state time Xt {0, 1, . . . , N },
Xt = j means j agents trying transmit time t.
transition probabilities chain look follows:
Pr (Xt+1 = N |Xt = 0) = 1
Pr (Xt+1 = 1|Xt = 1) = 1

ij
Pr (Xt+1 = j|Xt = i) =
p (1 p)j
j

(restart)
(absorbing)
> 1, j

transition probabilities 0. agents
transmitting channel, agent attempt access it.
probability Pr (Xt+1 = N |Xt = 0) equal 1 channel becomes
free (Xt = 0), agents spot time + 1, everyone transmit (therefore
chain state Xt+1 = N ). probability Pr (Xt+1 = 1|Xt = 1) equal one
single agent successfully transmits channel, keep transmitting
forever after, agent attempt transmit there. Finally, probability
Pr (Xt+1 = j|Xt = i) expresses fact Xt = (i agents transmit time t),
probability agent transmitted time keep transmitting time + 1
probability 1 p.
interested number steps take Markov chain first arrive
state Xt = 1 given started state X0 = N . would mean agents
converged setting one transmitting, others not.
Definition 6 defined hitting time describes quantity.
show expected value E[h1 ] hitting time state Xt = 1 (and
corollary, prove Theorem 5) following steps:
1. show expected hitting time set states = {0, 1} (Lemma 6)
2. show probability Markov chain enters state 1 entering state
0, starts state > 1 (Lemma 7)
3. Finally, using law iterated expectations, combine two lemmas show
expected hitting time state 1.
Lemma 6. Let = {0, 1}. expected
hitting

time set states Markov
1
chain described Definition 7 p log N .
Proof. first prove
hitting time set A0 = {0} slightly
expected

modified Markov chain p1 log N .
449

fiCigler & Faltings

Let us define new Markov chain (Yt )t0 following transition probabilities:
Pr (Yt+1 = 0|Yt = 0) = 1

ij
Pr (Yt+1 = j|Yt = i) =
p (1 p)j
j

(absorbing)
j 0, 1

Note transition probabilities chain (Xt )t0 , except
states 0 1. state 1 positive probability going state 0, state 0
absorbing. Clearly, expected hitting time set A0 = {0} new chain
upper bound expected hitting time set = {0, 1} old chain.
path leads state 0 new chain either go state 1
(so happened probability old chain), goes state 1,
old chain would stop state 1 (but would one step shorter).
chain state Yt = i, next state Yt+1 drawn binomial distribution
parameters (i, 1 p). expected next state therefore
E(Yt+1 |Yt = i) = i(1 p)
1
therefore use Theorem 4 := 1p
derive A0 = {0},
hitting time is:


l
1
1
A0
ki < log 1 +
log
1p
p
p

upper bound kiA = {0, 1} old chain.
Lemma 7. probability hi Markov chain defined Definition 7 enters state 1
entering state 0, started state > 1, greater 1 p.
Proof. Calculating probability chain X enters state 1 state 0 equal
calculating hitting probability, i.e. probability chain ever enters
given state, modified Markov chain probability staying state 0
Pr (Xt+1 = 0|Xt = 0) = 1. set states A, let us denote hA
probability
Markov chain starting state ever enters state A. calculate probability,
use Theorem 3. modified Markov chain cannot leave neither state 0
state 1, computing hA
= 1 easy, since matrix system linear equations
lower triangular.
Well show hi q = 1 p > 1 using induction. first step calculating hi
{0, 1, 2}.
h0 = 0
h1 = 1
h2 = (1 p)2 h2 + 2p(1 p)h1 + p2 h0
2p(1 p)
2(1 p)
=
=
1 p.
1 (1 p)2
2p
Now, induction step, derive bound hi assuming hj q = 1 p
j < i, j 2.
450

fiDecentralized Anti-coordination Multi-agent Learning


X
ij
hi =
p (1 p)j hj
j
j=0




X
ij
p (1 p)j q ipi1 (1 p)(q h1 ) pi h0
j
j=0

= q ipi1 (1 p)(q 1) q = 1 p.
means matter state 2 Markov chain starts in, enter
state 1 earlier state 0 probability least 1 p.
finish proof bound expected hitting time state 1.
use law iterated expectations:
Theorem 8. (Billingsley, 2012) Let X random variable satisfying E(|X|) <
another random variable probability space.
E[X] = E [E[X|Y ]] ,
i.e., expected value X equal conditional expected value X given .
Let h1 random variable corresponding hitting time state 1. Define
random variable Z denoting number passes state 0 Markov chain makes
reaches state 1. Theorem 8, get:
E[h1 ] = E[E[h1 |Z]].
Lemma 6, shown expected number steps hA Markov chain
reaches set states = {0, 1}. write
E[E[h1 |Z]] = E

" Z
X

#
hA = E[Z hA ] = hA E[Z].

i=1

Lemma 7 know probability chain passes state 1
passing 0 greater 1 p. Therefore, say E[Z] E[Z 0 ]
Z 0 random variable distributed according geometric distribution success
probability 1 p.


1
hA
0
=O
log N .
E[h1 ] = hA E[Z] hA E[Z ] =
1p
p(1 p)
concludes proof Theorem 5.
shown expected time convergence algorithm finite,
polynomial. probability algorithm converge finite number
steps absorbing state? following theorem shows since expected hitting
time absorbing state finite, probability 1.
451

fiCigler & Faltings

Theorem 9. Let h1 hitting time state Xt = 1 Markov chain
Definition 7.
Pr(h1 finite) = 1.
Proof. Markov inequality, know since h1 0,
Pr(h1 )

E[h1 ]
.


Therefore,
Pr(h1 finite) = 1 lim Pr(h1 ) 1 lim




E[h1 ]
= 1.


means algorithm converges almost surely Nash equilibrium
channel allocation game.
4.2 Convergence C 1, K = 1
Theorem 10. N agents C 1, K = 1, expected number steps
learning algorithm

h converges toia pure-strategy Nash equilibrium channel allocation
1
game C 1p p1 log N + C .
Proof. beginning, least one channel,
N agents want
1
transmit. take average p log N steps get state either 1 0
agents transmit (Lemma 6). call period round.
agents backed off, take average C steps
find empty channel. call period break.
channels might oscillate round break periods parallel,
worst case, whole system oscillate
two periods.


1
single channel, takes average 1p oscillations two periods


one agent transmits channel. C 1, takes
1
average C 1p steps round break channels one

h

1
1
agent transmitting. Therefore, take average C 1p
log
N
+
C
steps
p
system converges.
4.3 Convergence C 1, K 1
show convergence time K > 1, use general problem.
Imagine K identical instances Markov chain. know
original Markov chain converges initial state absorbing state expected
time . imagine complex Markov chain: every step, selects uniformly
random one K instances original Markov chain, executes one step
instance. time Tall K instances converge absorbing states?
extension well-known Coupon collectors problem (Feller, 1968).
following theorem (Gast, 2011, Thm. 4) shows upper bound expected number
steps K instances original Markov chain converge:
452

fiDecentralized Anti-coordination Multi-agent Learning

Theorem 11. (Gast, 2011) Let K instances Markov chain
known converge absorbing state expectation steps. select randomly
one Markov chain instance time allow perform one step chain,
take average E[Tall ] K log K + 2T K + 1 steps K instances converge
absorbing states.
arbitrary C 1, K 1, following theorem follows Theorems 10 11:
Theorem 12. N agents C 1, K 1, 0 < p < 1, 0 < q < 1, expected number
steps learning algorithm converges pure-strategy Nash equilibrium
channel allocation game every k K




1
1
(K log K + 2K)C
C + log N + 1 .
1p
p
Aumann (1974) shows Nash equilibrium correlated equilibrium,
convex combination correlated equilibria correlated equilibrium. know
pure-strategy Nash equilibria algorithm converges efficient:
collisions, every channel every signal value, agent transmits. Therefore,
conclude following:
Theorem 13. learning algorithm defined Section 3 converges expected polynomial
1
time (with respect K, C, p1 , 1p
log N ) efficient correlated equilibrium
channel allocation game.

5. Fairness
Agents decide strategy independently value coordination signal. Therefore, every agent equal chance game converges equilibrium
favorable her. agent transmit resulting equilibrium given signal
value, say agent wins slot. C available channels N agents, agent
C
wins given slot probability N
(since agent transmit two channels
time).
analyse fairness algorithm converged correlated equilibrium. algorithms converge absorbing state (such ALOHA),
need analysed intermediate states execution, believe approach justified fact algorithm converges relatively quickly, polynomial
time.
describe number signals agent wins channel
random variable Xi . variable distributed according binomial distribution
C
parameters K, N
.
measure fairness, use Jain index (Jain, Chiu, & Hawe, 1984).
advantage Jain index continuous, resource allocation strictly
fair higher Jain index (unlike measures assign binary values,
whether least half agents access resource). Also, Jain index independent
population size, unlike measures standard deviation agent allocation.
453

fiCigler & Faltings

random variable X, Jain index following:
J(X) =

(E[X])2
E[X 2 ]

C
),
X distributed according binomial distribution parameters (K, N
first second moments

C
E[X] = K
N


2
C 2
C N C
E X = K
+K

,
N
N
N
Jain index
J(X) =

C K
.
C K + (N C)

(2)

Jain index holds 0 < J(X) 1. allocation considered fair
J(X) = 1.

N
Theorem 14. C, K = N
C , limit limN CK = 0,
lim J(X) = 1,

N

allocation becomes fair N goes .
Proof. theorem follows fact
lim J(X) = lim

N

N

C K
C K + (N C)

limit equal 1, need
N C
=0
N C K
lim

holds exactly K =
assume C N ).

N
C



(that K grows asymptotically faster

N
C;

note

practical purposes, may need know big shall choose K given C
N . following theorem shows that:
Theorem 15. Let > 0.
1
K>





N
1 ,
C

J(X) > 1 .
Proof. theorem follows straightforwardly Equation 2.
454

fiDecentralized Anti-coordination Multi-agent Learning

5

Convergence steps

10

4

10

3

10

0

10

20

30

40

50

60

70

C

Figure 1: Average number steps convergence N = 64, K = N C
{1, 2, . . . , N }.

6. Experimental Results
experiments, report average values 128 runs experiment.
Error bars graphs denote interval contains true expected value
probability 95%, provided samples follow normal distribution. error bars
missing either graph reports values obtained theoretically (Jain index
constant back-off scheme) confidence interval small scale graph.
6.1 Static Player Population
first analyze case population players remains
time.
6.1.1 Convergence
First, interested convergence allocation algorithm. Section 4
know polynomial. many steps algorithm need converge
practice?
Figure 1 presents average number convergence steps N = 64, K = N
increasing number available channels C {1, 2, . . . , N }. Interestingly, convergence
takes longest time C = N . lowest convergence time C = N2 ,
C = 1 increases again.
happens change size signal space K? Figure 2 shows
average number steps convergence fixed N , C varying K. Theoretically,
455

fiCigler & Faltings

1400

Convergence steps

1200
1000
800
600
400
200
0
0

10

20

30

40

50

60

70

K

1

1

0.9

0.9

0.8

0.8

0.7

0.7

Jain index

Jain index

Figure 2: Average number steps convergence N = 64, C =

0.6
0.5
K=N
K = Nlog N

0.4

0.2
0

20

40

60

80

100

120

0.5
K=2
K = 2log N
2

0.3

K = N2

0.2
0

140

N

K {2, . . . , N }.

0.6

0.4

2

0.3

N
2

K = 2N
20

40

60

80

100

120

140

N

(a) C = 1

(b) C =

N
2

Figure 3: Jain fairness index different settings C K, increasing N .

shown number convergence steps O(K log K) Theorem 12. However,
practice convergence resembles linear dependency K. algorithm
needs converge coordination signals.
6.1.2 Fairness

Section 5, know K = N
C , Jain fairness index converges 1
N goes infinity. fast convergence? big need choose K,
depending N C, achieve reasonable bound fairness?
456

fiDecentralized Anti-coordination Multi-agent Learning

Figure 3 shows Jain index N increases, C = 1 C = N2 respectively,
various settings K. Even though every time K = N
C (that is, K grows faster
N
C ) Jain index increases (as shown Theorem 14), marked difference
various settings K. K = N
C , Jain index (from Equation 2):
J(X) =

N
.
2N C

(3)

Therefore, C = 1, Jain index converges 0.5, C =
equal 23 N > 0, Figure 3 shows.

N
2,

Jain index

6.1.3 Optimizing Fairness
saw fair outcome allocation algorithm agents consider game
signal value independently. However, best do?
improve fairness, agent correlates decisions different signal values?
perfectly fair solution, every agent wins (and consequently transmit)
number signal values. However, assume agents know many
agents system. Therefore, agents know fair
share signal values transmit for. Nevertheless, still use information
many slots already transmitted decide whether back-off stop
transmitting collision occurs.
Definition 8. strategy fit agent round t, define cardinality
number signals strategy tells agent access:


|fit | = k K|fit (k) > 0
Intuitively, agents whose strategies higher cardinality back-off often
strategy low cardinality.
compare following variations channel allocation scheme, differ
original one probability agents back collisions:
Constant scheme described Section 3; Every agent backs constant
probability p.
Linear back-off probability p =

|fit |
K .

Exponential back-off probability p =



|f |
1 Ki

parameter 0 < < 1.

Worst-agent-last case collision, agent lowest |fit | back
off. others collided, back off. greedy algorithm requires
information assume agents have.
compare fairness allocations experiments, need define Jain
index actual allocation. resource allocation vector X = (X1 , X2 , . . . , XN ),
Xi cardinality strategy used agent i. allocation X, Jain index is:
P
2
N
i=1 Xi
J(X) =
P
2
N N
i=1 Xi
457

fiCigler & Faltings

C = N/2, K = 2log2N
1
0.98

Jain index

0.96
0.94
0.92
0.9
Constant
Linear
Exponential
Worstagent

0.88
0.86
0

20

40

60

80

100

120

140

N

Figure 4: Jain fairness index channel allocation scheme various back-off probabilities, C = N2 , K = 2 log2 N

Figure 4 shows average Jain fairness index allocation back-off probability
variations. fairness approaching 1 worst-agent-last algorithm.
worst everyone using back-off probability. ratio back-off
probability lowest-cardinality agent highest-cardinality agent decreases,
fairness increases.
shows improve fairness using different back-off probabilities. Nevertheless, shape fairness curve them. Furthermore,
exponential back probabilities lead much longer convergence, shown Figure 5.
C = N2 , convergence time linear constant back-off schemes similar.
unrealistic worst-agent-last scheme obviously fastest, since resolves collisions
1 step, unlike back-off schemes.
6.2 Dynamic Player Population
take look performance algorithm population players
changing time (either new players join old players get replaced new ones).
analyze case errors players observe either
coordination signal channel feedback noisy.
6.2.1 Joining Players
section, present results experiments group players joins
system later. corresponds new nodes joining wireless network. precisely,
458

fiDecentralized Anti-coordination Multi-agent Learning

C = N/2, K = 2log2N
Constant
Linear
Exponential
Worstagent
Convergence steps

3

10

2

10

1

10

1

2

10

10
N

Figure 5: Convergence steps various back-off probabilities.
25% players join network beginning. remaining 75% players
join network later, one one. new player joins network previous players
converged perfect channel allocation.
experiments two ways initializing strategy new player.
Greedy Either, joining players cannot observe many players already system. Therefore, initial strategy tries transmit possible
slots.
Polite Or, players observe N (t), number players already
system time t, new player joins system. Therefore, initial
strategy tries transmit slot probability N1(t) .
Figure 6 shows Jain index final allocation 75% players join later,
C = 1. players join greedy, aggressive. start
transmitting slots. hand, polite, aggressive
enough: new player starts strategy aggressive strategies
players already system. difference new player experience
collision every slot transmits in. old players experience collision
1
N (t) slots. Therefore, back less slots.
Therefore, especially constant scheme, resulting allocation unfair:
either better new players (when greedy) older players (when
players polite).
phenomenon illustrated Figure 7. compares measure called group fairness:
average throughput last 25% players joined network end (new
459

fiCigler & Faltings

C = 1, K = Nlog N, join delay = converge init population = 0.25, KPS
2

1

0.9

0.9

0.8

0.8

0.7

0.7

0.6

0.6

Jain index

Jain index

C = 1, K = Nlog2N, join delay = converge init population = 0.25
1

0.5
0.4
0.3

0.5
0.4
0.3

0.2

0.2

Constant
Linear
Worstplayer

0.1
0
5

10

15

20

25

30

35

40

45

Constant
Linear
Worstplayer

0.1
0
5

50

10

15

20

25

N

30

35

40

45

50

N

(a) Greedy

(b) Polite

Figure 6: Joining players, Jain index. C = 1 K = N log2 N . two graphs show
results two ways initializing strategy new player.

C = 1, K = Nlog2N, join delay = converge init population = 0.25

C = 1, K = Nlog2N, join delay = converge init population = 0.25, KPS

5

0.8

Constant
Linear
Worstplayer

4.5

Constant
Linear
Worstplayer

0.7
0.6

3.5
Group fairness

Group fairness

4

3
2.5
2

0.5
0.4
0.3

1.5
0.2

1
0.5
0

10

20

30

40

50

N

0.1
0

10

20

30

40

50

N

(a) Greedy

(b) Polite

Figure 7: Joining players, group fairness. C = 1 K = N log2 N . two graphs show
results two ways initializing strategy new player.

460

fiDecentralized Anti-coordination Multi-agent Learning

C = N/2, K = 2log2N, join delay = converge init population = 0.25

C = N/2, K = 2log2N, join delay = converge init population = 0.25, KPS

1

1

0.9

0.9

0.8

0.8
0.7

0.6

Jain index

Jain index

0.7

0.5
0.4
0.3

0.5
0.4
0.3

0.2

0.2

Constant
Linear
Worstplayer

0.1
0
5

0.6

10

15

20

25

30

35

40

45

Constant
Linear
Worstplayer

0.1

50

0
5

10

15

N

20

25

30

35

40

45

50

N

(a) Greedy

(b) Polite

Figure 8: Joining players, Jain index. C = N2 K = 2 log2 N . two graphs show
results two ways initializing strategy new player.

players) divided average throughput first 25% players join network
beginning (old players).
Lets look first case players greedy. constant scheme,
ratio around 4.5. linear scheme, ratio lower, although increasing N (the
total number players) grows. worst-player-last scheme, ratio stays constant
interestingly, lower 1, means old players better
new players.
players polite, situation opposite. Old players way better
new players. constant scheme, throughput ratio 0.2.
Figures 8 9 show graphs C = N2 . Here, newly joining players
worse even start transmitting every slot.
experience collision every time (because channels slots occupied), old
players experience collision probability N1 . hand, overall
2

fairness whole population better, channels share
agent use one channel.
difference old new players even pronounced new
players polite.
6.2.2 Restarting Players
Another scenario looked happens one old players switches
replaced new player randomly initialized strategy. say
player got restarted. wireless network, corresponds situation user
restarts router. Note number players network stays same,
players forget learned start scratch.
Specifically, every round, every player probability pR
restarted. restart, start strategy initialized two ways:
461

fiCigler & Faltings

C = N/2, K = 2log N, join delay = converge init population = 0.25

C = N/2, K = 2log2N, join delay = converge init population = 0.25, KPS

2

0.95

1

Constant
Linear
Worstplayer

0.9

Constant
Linear
Worstplayer

0.9

0.85
Group fairness

Group fairness

0.8

0.8
0.75
0.7

0.6
0.5

0.65

0.4

0.6
0.55
0

0.7

10

20

30

40

50

N

0

10

20

30

40

50

N

(a) Greedy

(b) Polite

Figure 9: Joining players, group fairness. C = N2 K = 2 log2 N . two graphs show
results two ways initializing strategy new player.

Greedy Assume player know N , number players system.
signal value k K chooses randomly (k) C. means
attempts transmit every slot randomly chosen channel.
Polite Assume player know N . k K, chooses (k) C probability
C
N , (k) := 0 otherwise.
Figure 10 shows average overall throughput N = 32, C = 1, K = N log2 N
K = N two initialization schemes. dotted line four graphs shows
overall performance players attempt transmit randomly chosen channel
C
probability N
. baseline solution reaches 1e 37% average throughput.
probability restart increases, average throughput decreases. players
get restarted greedy, attempt transmit every slot.
one channel available, means restarted player causes collision every slot.
Therefore, surprising restart probability pR = 101 N = 32,
throughput virtually 0: every step, expectation least one player get restarted,
collision almost always.
interesting phase transition occurs pR 104 K = N log2 N ,
pR 103 K = N . There, performance
baseline random access scenario (that requires players know N though). Similar
phase transition occurs players polite, even though resulting throughput
higher, since restarted players less aggressive.
Yet another interesting, surprising, phenomenon worstplayer-last scheme still achieves highest throughput, constant back scheme
better linear back-off scheme. average overall throughput,
matters fast players able reach perfect allocation disruption.
worst-player-last scheme fastest, since resolves collision 1 step. con462

fiDecentralized Anti-coordination Multi-agent Learning

N = 32, C = 1, K = Nlog2N, KPS
1

0.9

0.9

0.8

0.8

0.7

0.7

System throughput

System throughput

N = 32, C = 1, K = Nlog2N
1

0.6
0.5
0.4
0.3
0.2
0.1
0

6

0.5
0.4
0.3
0.2

Constant
Linear
Worstplayer

0.1
4

10

0.6

10
Restart probability

0

2

10

Constant
Linear
Worstplayer
6

(a) Greedy, K = N log2 N

2

10

N = 32, C = 1, K = N, KPS
1

0.9

0.9

0.8

0.8

0.7

0.7

System throughput

System throughput

N = 32, C = 1, K = N

0.6
0.5
0.4
0.3
0.2

0

10
Restart probability

(b) Polite, K = N log2 N

1

0.1

4

10

0.6
0.5
0.4
0.3
0.2

Constant
Linear
Worstplayer
6

10

0.1
4

10
Restart probability

0

2

10

Constant
Linear
Worstplayer
6

10

(c) Greedy, K = N

4

10
Restart probability

(d) Polite, K = N

Figure 10: Restarting players, throughput, N = 32, C = 1

463

2

10

fiCigler & Faltings

N = 32, C = N/2, K = Nlog2N, KPS
1

0.9

0.9

0.8

0.8

0.7

0.7

System throughput

System throughput

N = 32, C = N/2, K = Nlog2N
1

0.6
0.5
0.4
0.3
0.2
0.1
0

6

0.5
0.4
0.3
0.2

Constant
Linear
Worstplayer

0.1
4

10

0.6

10
Restart probability

0

2

10

Constant
Linear
Worstplayer
6

(a) Greedy, K = 2 log2 N

10

N = 32, C = N/2, K = 2, KPS
1

0.9

0.9

0.8

0.8

0.7

0.7

System throughput

System throughput

N = 32, C = N/2, K = 2

0.6
0.5
0.4
0.3
0.2

0

2

10
Restart probability

(b) Polite, K = 2 log2 N

1

0.1

4

10

0.6
0.5
0.4
0.3
0.2

Constant
Linear
Worstplayer
6

10

0.1
4

10
Restart probability

0

2

10

Constant
Linear
Worstplayer
6

10

(c) Greedy, K = 2

4

2

10
Restart probability

10

(d) Polite, K = 2

Figure 11: Restarting players, throughput, N = 32, C =

N
2

stant scheme back-off probability p = 21 worse (see Theorem 12). linear scheme
slowest.
Figure 11 shows average overall throughput C = N2 , K = log2 N K = 2.
substantial difference players greedy polite. Since
many channels available, restarted player cause small number collisions
(in one channel N2 every slot), throughput decrease much.
Also, convergence time linear constant scheme
C = N2 , adapt disruption equally well.
6.2.3 Noisy Feedback
far assumed players receive perfect feedback whether transmissions
successful not. could observe activity given channel perfectly.
going loosen assumption now.
464

fiDecentralized Anti-coordination Multi-agent Learning

N = 32, C = N/2, K = Nlog2N
1

0.9

0.9

0.8

0.8

0.7

0.7

System throughput

System throughput

N = 32, C = 1, K = Nlog2N
1

0.6
0.5
0.4
0.3
0.2
0.1
0

6

0.5
0.4
0.3
0.2

Constant
Linear
Worstplayer
10

0.6

0.1
4

10
Noisy feedback probability

0

2

10

Constant
Linear
Worstplayer
6

10

(a) C = 1, K = N log2 N

4

10
Noisy feedback probability

(b) C =

N
2

2

10

, K = 2 log2 N

Figure 12: Noisy feedback, throughput, N = 32

N = 32, C = N/2, K = Nlog2N
1

0.98

0.98

0.96

0.96

0.94

0.94

0.92

0.92

Jain index

Jain index

N = 32, C = 1, K = Nlog2N
1

0.9
0.88
0.86

0.88
0.86

0.84

0.84
Constant
Linear
Worstplayer

0.82
0.8

0.9

6

10

4

10
Noisy feedback probability

0.8

2

10

Constant
Linear
Worstplayer

0.82
6

10

(a) C = 1, K = N log2 N

4

10
Noisy feedback probability

(b) C =

N
2

2

10

, K = 2 log2 N

Figure 13: Noisy feedback, Jain index, N = 32
Suppose every step, every player probability pF feedback
receives wrong. is, player transmitted, learn transmission
successful not, vice versa. player observed channel,
learn channel free fact (and vice versa). context
wireless networks, corresponds interference wireless channel.
affect learning?
Figure 12 show average overall throughput C = 1 C = N2 respectively. one channel, constant scheme better linear scheme,
adapts faster disruptions. C = N2 , schemes equivalent,
equally fast adapt. phase transition occurs noisy feedback probability
pF = 102 .
465

fiCigler & Faltings

N = 32, C = N/2, K = Nlog2N

1

0.9

0.9

0.8

0.8

0.7

0.7

System throughput

System throughput

N = 32, C = 1, K = Nlog2N

1

0.6
0.5
0.4
0.3
0.2
0.1
0

0.6
0.5
0.4
0.3
0.2

Constant
Linear
Worstplayer
6

10

0.1
4

10
Noisy signal probability

0

2

10

Constant
Linear
Worstplayer
6

10

(a) C = 1, K = N log2 N

4

10
Noisy signal probability

(b) C =

N
2

2

10

, K = 2 log2 N

Figure 14: Noisy coordination signal, throughput, N = 32
Figure 13 shows Jain index allocation players receive noisy feedback.
usual, linear scheme better constant, even though throuput lower (as
shown above). overall throughput drops close 0, schemes
obviously almost fairness.
6.2.4 Noisy Coordination Signal
algorithm assumes players observe coordination signal every
step. signal come from? may random noise given
frequency, FM radio transmission etc. However, coordination signal might noisy,
different players observe different value. means learning would
sync. wireless networks, corresponds clock drift.
see happens case, use following experiment. every step,
every player observes correct signal (i.e. one observed everyone else)
probability 1 pS . probability pS observes false signal (that still
taken uniformly random set {0, ..., K 1}).
overall throughput shown Figure 14. see system able
cope fairly high level noise signal, drop throughput occurs
pS = 101 . case experiments noisy feedback, constant back-off
scheme able achieve higher throughput thanks faster convergence.
Jain index allocation (Figure 15) stays almost constant,
throughput drops Jain index increases. allocation random,
fair.
6.3 Generic Multi-agent Learning Algorithms
Several algorithms proved converge correlated equilibrium proposed multi-agent learning literature. Introduction, mentioned three
learning algorithms (Foster & Vohra, 1997; Hart & Mas-Colell, 2000; Blum & Man466

fiDecentralized Anti-coordination Multi-agent Learning

N = 32, C = N/2, K = Nlog2N
1

0.98

0.98

0.96

0.96

0.94

0.94

0.92

0.92

Jain index

Jain index

N = 32, C = 1, K = Nlog2N
1

0.9
0.88
0.86

0.88
0.86

0.84

0.84
Constant
Linear
Worstplayer

0.82
0.8

0.9

6

10

4

10
Noisy signal probability

0.8

2

10

Constant
Linear
Worstplayer

0.82
6

10

(a) C = 1, K = N log2 N

4

10
Noisy signal probability

(b) C =

N
2

2

10

, K = 2 log2 N

Figure 15: Noisy coordination signal, Jain index, N = 32
sour, 2007). However, analysis Foster Vohra applicable games two
players. section, briefly recall two multi-agent learning algorithms
(Hart & Mas-Colell, 2000; Blum & Mansour, 2007), compare performance
algorithm presented Section 3.
two algorithms compare algorithm based notion minimizing regret agents experience adopting certain strategy. Intuitively,
describe concept regret follows: Imagine agent uses strategy couple
rounds game, accumulates certain payoff. would know
payoff compare payoff acquired simple alternative strategy . difference payoff strategy regret agent perceives (ex-post)
choosing strategy strategy .
mean simple strategy? One class simple strategies strategies
always select action. external regret compares performance
strategy performance best single action ex-post.
Another class alternative strategies strategies modify strategy slightly.
Every time strategy proposes play action a, alternative strategy proposes
action a0 6= instead. internal regret defined regret strategy compared
best alternative strategy. agents adopt strategy low internal
regret, converge strategy profile close correlated equilibrium (also
shown Blum & Mansour, 2007).
Hart Mas-Colell (2000) present simple multi-agent learning algorithm
guaranteed converge correlated equilibrium. assume players
observe actions opponents every round game. Players start
choosing actions randomly. update strategy follows: Let ai
action player played round t1. action aj Ai , aj 6= ai , player calculates
difference average payoff would received played action aj
instead ai past, average payoff received far playing action ai .
mentioned above, call difference internal regret playing action ai
467

fiCigler & Faltings

C = N/2

C=1
3

10

Constant backoff
HartMasColell
BlumMansour

Constant backoff
HartMasColell
BlumMansour
3

Convergence steps

Convergence steps

10
2

10

2

10

1

10

1

10

5

10

15
N

20

25

(a) C = 1

5

10

15
N

(b) C =

20

25

N
2

Figure 16: General multi-agent learning algorithms, convergence rate.

instead action aj . player chooses action play round probability
proportional internal regret compared previous action ai . Actions negative
regret never played. previous action ai played positive probability
way, strategy certain inertia.
Hart Mas-Colell (2000) prove agents adopt adaptive procedure described above, empirical distribution play (the relative frequency playing
certain pure strategy profile) converges almost surely set correlated equilibria.
Blum Mansour (2007) present general technique convert learning algorithm
low external regret algorithm low internal regret. idea run
multiple copies external regret algorithm. step, copy returns probability
vector playing action. probability vectors combined one joint
probability vector. player observes payoff playing action, updates
payoff beliefs external regret algorithms proportionally weight
joint probability vector. authors show players use learning
algorithm low internal regret, empirical distribution game converges close
correlated equilibrium.
One low-external-regret algorithms Blum Mansour (2007) present
Polynomial Weights (PW) algorithm. There, player keeps weight
actions. every round game, updates weight proportionally loss
(negative payoff) action incurred round. Actions higher weight get
chosen higher probability.
implemented two generic multi-agent learning algorithms: internalregret-based algorithm Hart Mas-Colell (2000), PW algorithm Blum
Mansour (2007). experiments, algorithms always converge pure-strategy
Nash equilibrium channel allocation game, therefore efficient allocation.
However, resulting allocation fair, subset agents size C ever
access channels.
468

fiDecentralized Anti-coordination Multi-agent Learning

C = N/2
1

0.9

0.9

0.8

0.8

0.7

0.7

0.6

0.6

Jain index

Jain index

C=1
1

0.5
0.4
0.3

0.4
0.3

0.2

0.2
Constant backoff
HartMasColell
BlumMansour

0.1
0
5

0.5

10

15
N

20

Constant backoff
HartMasColell
BlumMansour

0.1

25

(a) C = 1

0
5

10

15
N

(b) C =

20

25

N
2

Figure 17: General multi-agent learning algorithms, Jain index.

Figure 16 shows average number rounds algorithms take converge
stable outcome. compare performance learning algorithm Section 3.
learning algorithm, set K = 1, converges pure-strategy
Nash equilibrium game. performed 128 runs algorithm scenario.
error-bars Figure 16 show 95% confidence interval average, assuming
convergence times distributed according normal distribution.
surprisingly, generic algorithms Hart Mas-Colell (2000) Blum
Mansour (2007) cannot match convergence speed algorithm, designed specifically
problem channel allocation. generic algorithms converge pure-strategy
NE, outcome unfair, Jain index low, evidenced Figure 17.
dont report confidence bounds Jain index, experiments
resulting Jain index same.

7. Related Work
Broadly speaking, paper interested games payoff agent receives
certain action inversely proportional number agents chose
action. achieve efficient fair outcome games? Variants
problem studied several previous works.
simplest variant Minority game (Challet, Marsili, & Zhang, 2005).
game, N agents simultaneously choose two actions. Agents chose
action chosen minority agents receive payoff 1, whereas agents whose
action choice majority receive
payoff 0. game many pure-strategy Nash
equilibria, group N 21 agents chooses one action rest choose
action. equilibria efficient, since largest possible number agents achieve
maximum payoff. However, fair: payoff losing group agents
always 0. game one mixed-strategy NE fair: every agent chooses
469

fiCigler & Faltings

action randomly. equilibrium,
hand, efficient: expected size
minority group lower N 21 due variance action selection.
Savit, Manuca, Riolo (1999) show agents receive feedback action
minority, learn coordinate better achieve efficient outcome
repeated minority game. basing agents decisions history
past iterations. Cavagna (1999) shows result achieved agents
base decisions value random coordination signal instead using
history. direct inspiration idea global coordination signal presented
paper.
ideas literature Minority games recently found way
cognitive radio literature. Mahonen Petrova (2008) present channel allocation
problem much ours. agents learn channel use using strategy
similar strategies minority games. difference instead preferring
action chosen minority, channel allocation problem, agent prefers channels
chosen anyone else. Using approach, Mahonen Petrova able
achieve stable throughput 50% even number agents try
transmit channel increases. However, agent essentially choosing one
fixed set strategies, cannot adapt. Therefore, difficult achieve
perfectly efficient channel allocation.
Wang et al. (2011) implemented algorithm work actual wireless
network. setting, wireless devices able monitor activity channels.
coordination signal, used actual data packets agents send.
authors shown practice, learning algorithm (which call attachment
learning) improves throughput 2-3 random access slotted ALOHA protocol.
Another, general variant problem, called dispersion game described
Grenager, Powers, Shoham (2002). dispersion game, agents choose several
actions, prefer one chosen smallest number agents.
authors define maximal dispersion outcome outcome agent move
action fewer agents. set maximal dispersion outcomes corresponds set
pure-strategy Nash equilibria game. propose various strategies converge
maximal dispersion outcome, different assumptions information available
agents. contrary work, individual agents dispersion games
particular preference actions chosen equilibria
achieved. Therefore, issues achieving fair outcome.
Verbeeck, Nowe, Parent, Tuyls (2007) use reinforcement learning, namely linear
reward-inaction automata, learn Nash equilibria common conflicting interest
games. class conflicting interest games (to channel allocation game
belongs), propose algorithm allows agents circulate various
pure-strategy Nash equilibria, outcome game fair. contrast
work, solution requires communication agents, requires
agents know strategies converged. addition, linear reward-inaction automata
guaranteed converge pure-strategy NE conflicting interest games;
may converge pure strategies.
games discussed above, including channel allocation game, form part
family potential games introduced Monderer Shapley (1996). game called
470

fiDecentralized Anti-coordination Multi-agent Learning

potential game admits potential function. potential function defined every
strategy profile, quantifies difference payoffs agent unilaterally deviates
given strategy profile. different kinds potential functions: exact (where
difference payoffs deviating agent corresponds directly difference
potential function), ordinal (where sign potential difference
sign payoff difference) etc.
Potential games several nice properties. important purestrategy Nash equilibrium local maximum potential function. finite
potential games, players reach equilibria unilaterally playing best-response,
matter initial strategy profile start from.
existence natural learning algorithm reach Nash equilibria makes potential
games interesting candidate future research. would see kind
correlated equilibria agents converge there, use simple correlation
signal coordinate.

8. Conclusions
paper, proposed new approach reach efficient fair solutions multi-agent
resource allocation problems. Instead using centralized, smart coordination device
compute allocation, use stupid coordination signal, general random integer
k {0, 1, . . . , K 1}, priori relation problem. Agents smart:
learn, value coordination signal, action take.
game-theoretic perspective, ideal outcome game correlated equilibrium. results show using global coordination signal, agents learn play
convex combination pure-strategy Nash equilibria, correlated equilibrium.
showed learning strategy that, variant channel allocation game, converges expected polynomial number steps efficient correlated equilibrium.
proved equilibrium becomes increasingly fair K, number available
synchronization signals, increases.
confirmed fast convergence well increasing fairness increasing K
experimentally. investigated performance learning strategy case
agent population dynamic. new agents join population, learning strategy
still able learn efficient allocation. However, fairness allocation depend
greedy initial strategies new agents are. agents restart random
intervals, becomes important fast strategy converges. simple strategy
everyone backs transmitting constant probability able achieve higher
throughput sophisticated strategy back-off probability depends
many slots agent already transmitting. showed experimentally
learning strategy robust noise coordination signal, well
feedback agents receive channel use. noisy scenarios, faster convergence constant back-off scheme helped achieve higher throughput
fair linear back-off scheme. Finally, compared performance learning strategy generic multi-agent learning algorithms based regret-minimization (Hart &
Mas-Colell, 2000; Blum & Mansour, 2007). generic algorithms theoretically
proven converge distribution play close correlated equilibrium,
471

fiCigler & Faltings

guaranteed converge specific CE. Indeed, experiments, algorithms
Hart Mas-Colell Blum Mansour always converged efficient unfair
pure-strategy Nash equilibrium channel allocation game.
learning algorithm presented paper implemented real wireless
network Wang et al. (2011), shown achieves 2-3 higher throughput
random access protocols ALOHA.
paper, address issue whether non-cooperative rational
agents would follow protocol outlined. work (Cigler & Faltings, 2012),
address issue show certain conditions, protocol implemented
Nash equilibrium strategies infinitely repeated resource allocation game.

References
Abramson, N. (1970). ALOHA system: another alternative computer communications. Proceedings November 17-19, 1970, fall joint computer conference,
AFIPS 70 (Fall), pp. 281285, New York, NY, USA. ACM.
Aumann, R. (1974). Subjectivity correlation randomized strategies. Journal
Mathematical Economics, 1 (1), 6796.
Billingsley, P. (2012). Probability Measure (Wiley Series Probability Statistics)
(Anniversary Edition edition). Wiley.
Blum, A., & Mansour, Y. (2007). Algorithmic game theory. Nisan, N., Roughgarden,
T., Tardos, E., & Vazirani, V. (Eds.), Algorithmic Game Theory, chap. 4. Cambridge
University Press.
Cavagna, A. (1999). Irrelevance memory minority game. Physical Review E, 59 (4),
R3783R3786.
Challet, D., Marsili, M., & Zhang, Y.-C. (2005). Minority Games: Interacting Agents
Financial Markets (Oxford Finance). Oxford University Press, New York, NY, USA.
Chen, X., & Deng, X. (2006). Settling complexity Two-Player nash equilibrium.
2006 47th Annual IEEE Symposium Foundations Computer Science (FOCS06),
pp. 261272. IEEE.
Cheng, S., Raja, A., Xie, L., & Howitt, I. (2009). distributed constraint optimization algorithm dynamic load balancing wlans. IJCAI-09 Workshop Distributed
Constraint Reasoning (DCR).
Cigler, L., & Faltings, B. (2012). Symmetric subgame perfect equilibria resource allocation. (to appear) Proceedings 26th national conference Artificial
intelligence (AAAI-12), Menlo Park, CA, USA. American Association Artificial
Intelligence.
Feller, W. (1968). Introduction Probability Theory Applications, Vol. 1, 3rd
Edition (3 edition). Wiley.
Foster, D. P., & Vohra, R. V. (1997). Calibrated learning correlated equilibrium. Games
Economic Behavior, 21 (1-2), 4055.
472

fiDecentralized Anti-coordination Multi-agent Learning

Gast, N. (2011). Computing hitting times via fluid approximation: application coupon
collector problem. ArXiv e-prints.
Grenager, T., Powers, R., & Shoham, Y. (2002). Dispersion games: general definitions
specific learning results. Proceedings Eighteenth national conference
Artificial intelligence (AAAI-02), pp. 398403, Menlo Park, CA, USA. American
Association Artificial Intelligence.
Hart, S., & Mas-Colell, A. (2000). simple adaptive procedure leading correlated equilibrium. Econometrica, 68 (5), 11271150.
Jain, R. K., Chiu, D.-M. W., & Hawe, W. R. (1984). quantitative measure fairness
discrimination resource allocation shared computer systems. Tech. rep., Digital
Equipment Corporation.
Leyton-Brown, K., & Shoham, Y. (2008). Essentials Game Theory: Concise, Multidisciplinary Introduction. Morgan & Claypool, San Rafael, CA.
Littman, M., & Stone, P. (2002). Implicit negotiation repeated games intelligent agents
VIII. Meyer, J.-J., & Tambe, M. (Eds.), Intelligent Agents VIII, Vol. 2333 Lecture
Notes Computer Science, chap. 29, pp. 393404. Springer Berlin / Heidelberg,
Berlin, Heidelberg.
Mahonen, P., & Petrova, M. (2008). Minority game cognitive radios: Cooperating without cooperation. Physical Communication, 1 (2), 94102.
Monderer, D., & Shapley, L. S. (1996). Potential games. Games Economic Behavior,
14 (1), 124 143.
Norris, J. R. (1998). Markov Chains (Cambridge Series Statistical Probabilistic
Mathematics). Cambridge University Press.
Papadimitriou, C. H., & Roughgarden, T. (2008). Computing correlated equilibria multiplayer games. Journal ACM, 55 (3), 129.
Rego, V. (1992). Naive asymptotics hitting time bounds markov chains. Acta Informatica, 29 (6), 579594.
Savit, R., Manuca, R., & Riolo, R. (1999). Adaptive competition, market efficiency,
phase transitions. Physical Review Letters, 82 (10), 22032206.
Verbeeck, K., Nowe, A., Parent, J., & Tuyls, K. (2007). Exploring selfish reinforcement
learning repeated games stochastic rewards. Autonomous Agents MultiAgent Systems, 14 (3), 239269.
Wang, L., Wu, K., Hamdi, M., & Ni, L. M. (2011). Attachment learning multi-channel
allocation distributed OFDMA networks. Parallel Distributed Systems, International Conference on, 0, 520527.

473


