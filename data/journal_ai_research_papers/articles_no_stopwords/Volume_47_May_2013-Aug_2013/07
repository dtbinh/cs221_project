journal artificial intelligence

submitted published

decentralized anti coordination
multi agent learning
ludek cigler
boi faltings

ludek cigler epfl ch
boi faltings epfl ch

artificial intelligence laboratory
ecole polytechnique federale de lausanne
ch lausanne switzerland

abstract
achieve optimal outcome many situations agents need choose distinct
actions one another case notably many resource allocation
single resource used one agent time shall designer
multi agent system program identical agents behave different way
game theoretic perspective situations lead undesirable nash equilibria
example consider resource allocation game two players compete exclusive
access single resource three nash equilibria two pure strategy ne
efficient fair one mixed strategy ne fair efficient aumanns
notion correlated equilibrium fixes assumes correlation device
suggests agent action take
however smart coordination device might available propose
randomly chosen stupid integer coordination signal smart agents learn action
use value coordination signal
present multi agent learning converges polynomial number
steps correlated equilibrium channel allocation game variant resource
allocation game agents learn play coordination signal value
randomly chosen pure strategy nash equilibrium game therefore outcome
efficient correlated equilibrium ce becomes fair number
available coordination signal values increases

introduction
many situations agents coordinate actions order use limited
resource communication networks channel might used one agent time
driving car agent prefers choose road less traffic e one chosen
smaller number agents bidding one item several simultaneous
auctions agent prefers auction less participants usually lead
lower price situations require agents take different decision however
agents identical one face
learn behave differently everyone else
second arises agents common preferences action
want take communication networks every agent prefers transmit
quiet traffic situation agents might prefer shorter path order
achieve efficient allocation necessary precisely agents stay quiet
take longer path achieve agents exploited
c

ai access foundation rights reserved

ficigler faltings

agents learn alternate taking longer road one day taking shorter
road next day
central coordinator possesses complete information agents preferences
available resources easily recommend agent action take
however omniscient central coordinator available therefore
would able use distributed scheme consider scenarios
agents try use set resources repeatedly use history past
interactions learn coordinate access resources future
particular consider radio channel access n agents
try transmit c non overlapping channels fully decentralized schemes
aloha abramson achieve throughput e e transmission succeeds probability e complex schemes
distributed constraint optimization cheng raja xie howitt reach
throughput close throughput mean probability successful transmission given channel however messages necessary implement schemes
create overhead eliminates part benefits
propose instead use simple signal agents observe
ergodically fluctuates signal could common clock radio broadcasting
specified frequency decimal part price certain stock given time etc
depending stupid signal smart agents learn take different action
value say signal stupid doesnt anything
game agents give meaning acting differently
value coordination signal
lets look simplest example channel access one agents try
transmit one shared channel model situation game normal
form agents choose two actions stay quiet q transmit
one agent may transmit successfully time agent transmits alone receives
positive payoff agent transmit channel payoff agents
try transmit channel time transmissions fail incur
cost c
payoff matrix game looks follows

q


q





c c

game two pure strategy nash equilibria ne one player stays quiet
one transmits one mixed strategy ne player stays

quiet probability c
two pure strategy ne efficient maximize
social welfare fair one player gets full payoff even though
game symmetric mixed strategy ne fair efficient expected
payoff players
nash equilibria game rather undesirable efficient
fair time seminal aumann proposed
notion correlated equilibrium fixes correlated equilibrium ce
probability distribution joint strategy profiles game correlation device


fidecentralized anti coordination multi agent learning

samples distribution recommends action agent play probability
distribution ce agents incentive deviate recommended
action
simple game described exists ce fair socially
efficient correlation device samples two pure strategy ne probability recommends players ne play corresponds
authority tells player whether stay quiet transmit
correlated equilibria several nice properties easier succinct
representation game polynomial time see papadimitriou roughgarden
every nash equilibrium correlated equilibrium convex combination
two correlated equilibria correlated equilibrium however smart correlation device
randomizes joint strategy profiles might available
possible achieve correlated equilibrium without actual correlation device
assume game played repeatedly agents observe history
actions taken opponents learn predict future action distribution future actions opponents predictions need calibrated
predicted probability agent play certain action aj converge
actual frequency agent plays action aj agents play action
best response predictions opponents actions foster vohra
showed case play converges set correlated equilibria
however foster vohra provide specific learning rule
achieve certain ce furthermore requires every agent able
observe actions every opponent requirement met convergence
correlated equilibrium guaranteed anymore
focus generalization simple channel allocation
described n agents data transmit
c channels transmit assume n c access channel
slotted agents synchronized start transmissions
time transmissions must length one agent attempts
transmit single channel collision occurs none transmissions
successful unsuccessful transmission cost agent since consume
possibly constrained power benefit transmitting cost
anything
assume agents receive binary feedback transmitted data
whether transmission successful transmit
choose channel observe receive information whether observed channel
free
described normal form game several efficient unfair
pure strategy nash equilibria group c agents gets assigned channels
remaining n c agents get stranded fair inefficient mixed strategy ne
agents choose transmission channels random example resource
allocation game exists correlated equilibrium efficient fair
stupid coordination signal introduced helps agents learn
play potentially different efficient outcome value way
reach efficient allocation still preserving level fairness


ficigler faltings

main contributions work following
propose learning strategy agents channel allocation game
minimal information converges polynomial time randomly chosen efficient
pure strategy nash equilibrium game
agents observe common discrete correlation signal
learn play efficient pure strategy ne signal value
correlated equilibrium increasingly fair number available signals k
increases
experimentally evaluate sensitive player population
dynamic e players leave enter system evaluate
resistance noise feedback players receive
coordination signal observe
channel allocation proposed implemented wang
wu hamdi ni real world wireless network setting showed
wireless devices use use actual data transmitted coordination
signal way able achieve throughput gain compared random
access protocols aloha
worth noting work focus reaching correlated fair
outcome provided agents willing cooperate situations
resources costs nothing self interested agent could stubbornly keep everyone
else better trying access resource sometimes called
watch crazy bully strategy littman stone
order prevent kind behavior would need make sure order
use resource agent pay cost cost may already implicit
fact wireless transmission costs energy may imposed
external payments recent work cigler faltings leads
equilibria rational agents indifferent accessing resource yielding
equilibria implement allocation policy rational agents consider
issue beyond scope refer reader work
deeper analysis
rest organized follows section give basic definitions
game theory theory markov chains use throughout
section present agents use learn action possible
correlation signal value section prove converges
efficient correlated equilibrium polynomial time number agents channels
fairness resulting equilibria increases number signals k
increases section section highlights experiments actual convergence
rate fairness performs case population
changing dynamically section present related work game theory
cognitive radio literature section concludes


fidecentralized anti coordination multi agent learning

preliminaries
section introduce basic concepts game theory theory
markov chains going use throughout
game theory
game theory study interactions among independent self interested agents
agent participates game called player player utility function
associated state world self interested players take actions achieve
state world maximizes utility game theory studies attempts
predict behaviour well final outcome interactions leyton brown
shoham give complete introduction game theory
basic way represent strategic interaction game called normal
form
definition finite n person normal form game tuple n u
n set n players
ai set actions available player vector
called action profile
u u u un ui r utility function player assigns
action vector certain utility payoff
playing game players select strategy pure strategy
player selects one action ai ai vector pure strategies player
n called pure strategy profile mixed strategy selects probability
distribution entire action space e ai mixed strategy profile
vector mixed strategies player
definition say mixed strategy player best response strategy
profile opponents strategy
ui ui
one basic goals game theory predict outcome strategic interaction
outcome stable therefore usually called equilibrium one requirement outcome equilibrium none players incentive
change strategy e players play best response strategies others
defines perhaps important equilibrium concept nash equilibrium
definition strategy profile n nash equilibrium ne
every player strategy best response strategies others
essential nash equilibria several disadvantages first may
hard chen deng finding ne ppad complete second
might multiple nash equilibria shown example section third
efficient ne may fair one even symmetric game give
formal definition correlated equilibrium fixes issues


ficigler faltings

definition given n player game n u correlated equilibrium tuple v
v tuple random variables v v v vn whose domains
dn joint probability distribution v n
vector mappings di ai player every mapping di ai
case
x
x

ui n dn
ui n dn
dd

dd

markov chains
learning propose analyze described randomized randomized steps depend value
random variable one useful technique analyze randomized describe
execution markov chain
markov chain random process markov property random process
collection random variables usually describes evolution random value
time process markov property state value next time step depends
exclusively value previous step values past
say process memoryless imagine execution randomized
finite state automaton non deterministic steps easy see
execution maps markov chain
formal definition markov chain follows
definition norris let countable set called state
called state space say p measure
addition total mass ii equals call distribution
work throughout probability space f p recall random variable x
values function x suppose set
pr x pr x
defines distribution distribution x think x modelling random
state takes value probability
say matrix p pij j stochastic every row pij j
distribution
say xt markov chain initial distribution transition matrix
p
x distribution
conditional xt xt distribution pij j independent
x x xt
explicitly conditions state
pr x
pr xt x xt pit


fidecentralized anti coordination multi agent learning

theorem let set states vector hitting probabilities ha ha

n minimal non negative solution system linear equations




hi p

p
h

j n ij j
intuitively hitting probability ha
probability markov chain
starts state ever reach states
one property randomized particularly interested
convergence set states converged define
time takes reach state set state corresponding
markov chain hitting time
definition norris let xt markov chain state space hitting
time subset random variable h given
h inf xt
specifically interested expected hitting time set states given
markov chain starts initial state x denote quantity
kia ei h
general expected hitting time set states found solving
system linear equations
theorem vector expected hitting times k e h kia
minimal non negative solution system linear equations

ki p



kia j
p
k

ij
j

convergence absorbing state may guaranteed general markov chain
calculate probability reaching absorbing state use following theorem
norris
theorem let set states vector hitting probabilities ha ha

n minimal non negative solution system linear equations




hi p

p
h

ij
j
j n
solving systems linear equations theorems analytically might
difficult many markov chains though fortunately markov chain
one absorbing state move state j j use
following theorem derive upper bound expected hitting time proved
rego
theorem let
e xt xt



kia log









ficigler faltings

learning
section describe agents use learn correlated equilibrium channel allocation game
let us denote space available correlation signals k k
space available channels c c assume c n
agents channels opposite case easier agent strategy k c
uses decide channel access time receives correlation
signal kt kt agent transmit signal kt agent stores
strategy simply table
agent adapts strategy follows
beginning k k k initialized uniformly random c
every agent picks random channel transmit agent monitor
channels
time
kt agent tries transmit channel kt
otherwise kt agent chooses random channel mi c
monitor activity
subsequently agent observes outcome choice agent transmitted
channel observes whether transmission successful
agent keep strategy unchanged collision occurred agent sets
kt probability p probability p strategy remains
agent transmit observes whether channel mi monitored
free channel free agent sets kt mi probability
channel free strategy remains

convergence
important property learning fast converge
pure strategy nash equilibrium channel allocation game every signal value
randomized therefore instead analyzing worst case behavior may
arbitrarily bad analyze expected number steps convergence
convergence c k
single channel single coordination signal prove following theorem
theorem n agents c k p expected number steps
allocation
converges
pure strategy nash equilibrium channel



allocation game p p log n
prove convergence useful describe execution
markov chain


fidecentralized anti coordination multi agent learning

n agents compete single signal value state markov chain
vector n denotes agents attempting transmit purpose
convergence proof important many agents trying transmit
agents probability agents back
everyone therefore describe execution following chain
definition markov chain describing execution allocation
c k p chain whose state time xt n
xt j means j agents trying transmit time
transition probabilities chain look follows
pr xt n xt
pr xt xt

ij
pr xt j xt
p p j
j

restart
absorbing
j

transition probabilities agents
transmitting channel agent attempt access
probability pr xt n xt equal channel becomes
free xt agents spot time everyone transmit therefore
chain state xt n probability pr xt xt equal one
single agent successfully transmits channel keep transmitting
forever agent attempt transmit finally probability
pr xt j xt expresses fact xt agents transmit time
probability agent transmitted time keep transmitting time
probability p
interested number steps take markov chain first arrive
state xt given started state x n would mean agents
converged setting one transmitting others
definition defined hitting time describes quantity
expected value e h hitting time state xt
corollary prove theorem following steps
expected hitting time set states lemma
probability markov chain enters state entering state
starts state lemma
finally law iterated expectations combine two lemmas
expected hitting time state
lemma let expected
hitting

time set states markov

chain described definition p log n
proof first prove
hitting time set slightly
expected

modified markov chain p log n


ficigler faltings

let us define markov chain yt following transition probabilities
pr yt yt

ij
pr yt j yt
p p j
j

absorbing
j

note transition probabilities chain xt except
states state positive probability going state state
absorbing clearly expected hitting time set chain
upper bound expected hitting time set old chain
path leads state chain go state
happened probability old chain goes state
old chain would stop state would one step shorter
chain state yt next state yt drawn binomial distribution
parameters p expected next state therefore
e yt yt p

therefore use theorem p
derive
hitting time


l



ki log
log
p
p
p

upper bound kia old chain
lemma probability hi markov chain defined definition enters state
entering state started state greater p
proof calculating probability chain x enters state state equal
calculating hitting probability e probability chain ever enters
given state modified markov chain probability staying state
pr xt xt set states let us denote ha
probability
markov chain starting state ever enters state calculate probability
use theorem modified markov chain cannot leave neither state
state computing ha
easy since matrix system linear equations
lower triangular
well hi q p induction first step calculating hi

h
h
h p h p p h p h
p p
p


p
p
p
induction step derive bound hi assuming hj q p
j j


fidecentralized anti coordination multi agent learning


x
ij
hi
p p j hj
j
j




x
ij
p p j q ipi p q h pi h
j
j

q ipi p q q p
means matter state markov chain starts enter
state earlier state probability least p
finish proof bound expected hitting time state
use law iterated expectations
theorem billingsley let x random variable satisfying e x
another random variable probability space
e x e e x
e expected value x equal conditional expected value x given
let h random variable corresponding hitting time state define
random variable z denoting number passes state markov chain makes
reaches state theorem get
e h e e h z
lemma shown expected number steps ha markov chain
reaches set states write
e e h z e

z
x


ha e z ha ha e z



lemma know probability chain passes state
passing greater p therefore say e z e z
z random variable distributed according geometric distribution success
probability p



ha


log n
e h ha e z ha e z
p
p p
concludes proof theorem
shown expected time convergence finite
polynomial probability converge finite number
steps absorbing state following theorem shows since expected hitting
time absorbing state finite probability


ficigler faltings

theorem let h hitting time state xt markov chain
definition
pr h finite
proof markov inequality know since h
pr h

e h



therefore
pr h finite lim pr h lim




e h



means converges almost surely nash equilibrium
channel allocation game
convergence c k
theorem n agents c k expected number steps
learning

h converges toia pure strategy nash equilibrium channel allocation

game c p p log n c
proof beginning least one channel
n agents want

transmit take average p log n steps get state
agents transmit lemma call period round
agents backed take average c steps
empty channel call period break
channels might oscillate round break periods parallel
worst case whole system oscillate
two periods



single channel takes average p oscillations two periods


one agent transmits channel c takes

average c p steps round break channels one

h



agent transmitting therefore take average c p
log
n

c
steps
p
system converges
convergence c k
convergence time k use general
imagine k identical instances markov chain know
original markov chain converges initial state absorbing state expected
time imagine complex markov chain every step selects uniformly
random one k instances original markov chain executes one step
instance time tall k instances converge absorbing states
extension well known coupon collectors feller
following theorem gast thm shows upper bound expected number
steps k instances original markov chain converge


fidecentralized anti coordination multi agent learning

theorem gast let k instances markov chain
known converge absorbing state expectation steps select randomly
one markov chain instance time allow perform one step chain
take average e tall k log k k steps k instances converge
absorbing states
arbitrary c k following theorem follows theorems
theorem n agents c k p q expected number
steps learning converges pure strategy nash equilibrium
channel allocation game every k k






k log k k c
c log n
p
p
aumann shows nash equilibrium correlated equilibrium
convex combination correlated equilibria correlated equilibrium know
pure strategy nash equilibria converges efficient
collisions every channel every signal value agent transmits therefore
conclude following
theorem learning defined section converges expected polynomial

time respect k c p p
log n efficient correlated equilibrium
channel allocation game

fairness
agents decide strategy independently value coordination signal therefore every agent equal chance game converges equilibrium
favorable agent transmit resulting equilibrium given signal
value say agent wins slot c available channels n agents agent
c
wins given slot probability n
since agent transmit two channels
time
analyse fairness converged correlated equilibrium converge absorbing state aloha
need analysed intermediate states execution believe justified fact converges relatively quickly polynomial
time
describe number signals agent wins channel
random variable xi variable distributed according binomial distribution
c
parameters k n

measure fairness use jain index jain chiu hawe
advantage jain index continuous resource allocation strictly
fair higher jain index unlike measures assign binary values
whether least half agents access resource jain index independent
population size unlike measures standard deviation agent allocation


ficigler faltings

random variable x jain index following
j x

e x
e x

c

x distributed according binomial distribution parameters k n
first second moments

c
e x k
n



c
c n c
e x k
k


n
n
n
jain index
j x

c k

c k n c



jain index holds j x allocation considered fair
j x

n
theorem c k n
c limit limn ck
lim j x

n

allocation becomes fair n goes
proof theorem follows fact
lim j x lim

n

n

c k
c k n c

limit equal need
n c

n c k
lim

holds exactly k
assume c n

n
c



k grows asymptotically faster

n
c

note

practical purposes may need know big shall choose k given c
n following theorem shows
theorem let

k





n

c

j x
proof theorem follows straightforwardly equation


fidecentralized anti coordination multi agent learning



convergence steps



























c

figure average number steps convergence n k n c
n

experimental
experiments report average values runs experiment
error bars graphs denote interval contains true expected value
probability provided samples follow normal distribution error bars
missing graph reports values obtained theoretically jain index
constant back scheme confidence interval small scale graph
static player population
first analyze case population players remains
time
convergence
first interested convergence allocation section
know polynomial many steps need converge
practice
figure presents average number convergence steps n k n
increasing number available channels c n interestingly convergence
takes longest time c n lowest convergence time c n
c increases
happens change size signal space k figure shows
average number steps convergence fixed n c varying k theoretically


ficigler faltings



convergence steps
























k

















jain index

jain index

figure average number steps convergence n c



k n
k nlog n



















k
k log n




k n






n

k n









n


k n














n

c

b c

n


figure jain fairness index different settings c k increasing n

shown number convergence steps k log k theorem however
practice convergence resembles linear dependency k
needs converge coordination signals
fairness

section know k n
c jain fairness index converges
n goes infinity fast convergence big need choose k
depending n c achieve reasonable bound fairness


fidecentralized anti coordination multi agent learning

figure shows jain index n increases c c n respectively
settings k even though every time k n
c k grows faster
n
c jain index increases shown theorem marked difference
settings k k n
c jain index equation
j x

n

n c



therefore c jain index converges c
equal n figure shows

n


jain index

optimizing fairness
saw fair outcome allocation agents consider game
signal value independently however best
improve fairness agent correlates decisions different signal values
perfectly fair solution every agent wins consequently transmit
number signal values however assume agents know many
agents system therefore agents know fair
share signal values transmit nevertheless still use information
many slots already transmitted decide whether back stop
transmitting collision occurs
definition strategy fit agent round define cardinality
number signals strategy tells agent access


fit k k fit k
intuitively agents whose strategies higher cardinality back often
strategy low cardinality
compare following variations channel allocation scheme differ
original one probability agents back collisions
constant scheme described section every agent backs constant
probability p
linear back probability p

fit
k

exponential back probability p



f
ki

parameter

worst agent last case collision agent lowest fit back
others collided back greedy requires
information assume agents
compare fairness allocations experiments need define jain
index actual allocation resource allocation vector x x x xn
xi cardinality strategy used agent allocation x jain index
p

n
xi
j x
p

n n
xi


ficigler faltings

c n k log n



jain index





constant
linear
exponential
worstagent



















n

figure jain fairness index channel allocation scheme back probabilities c n k log n

figure shows average jain fairness index allocation back probability
variations fairness approaching worst agent last
worst everyone back probability ratio back
probability lowest cardinality agent highest cardinality agent decreases
fairness increases
shows improve fairness different back probabilities nevertheless shape fairness curve furthermore
exponential back probabilities lead much longer convergence shown figure
c n convergence time linear constant back schemes similar
unrealistic worst agent last scheme obviously fastest since resolves collisions
step unlike back schemes
dynamic player population
take look performance population players
changing time players join old players get replaced ones
analyze case errors players observe
coordination signal channel feedback noisy
joining players
section present experiments group players joins
system later corresponds nodes joining wireless network precisely


fidecentralized anti coordination multi agent learning

c n k log n
constant
linear
exponential
worstagent
convergence steps




















n

figure convergence steps back probabilities
players join network beginning remaining players
join network later one one player joins network previous players
converged perfect channel allocation
experiments two ways initializing strategy player
greedy joining players cannot observe many players already system therefore initial strategy tries transmit possible
slots
polite players observe n number players already
system time player joins system therefore initial
strategy tries transmit slot probability n
figure shows jain index final allocation players join later
c players join greedy aggressive start
transmitting slots hand polite aggressive
enough player starts strategy aggressive strategies
players already system difference player experience
collision every slot transmits old players experience collision

n slots therefore back less slots
therefore especially constant scheme resulting allocation unfair
better players greedy older players
players polite
phenomenon illustrated figure compares measure called group fairness
average throughput last players joined network end


ficigler faltings

c k nlog n join delay converge init population kps




















jain index

jain index

c k nlog n join delay converge init population














constant
linear
worstplayer





















constant
linear
worstplayer















n











n

greedy

b polite

figure joining players jain index c k n log n two graphs
two ways initializing strategy player

c k nlog n join delay converge init population

c k nlog n join delay converge init population kps





constant
linear
worstplayer



constant
linear
worstplayer





group fairness

group fairness




























n














n

greedy

b polite

figure joining players group fairness c k n log n two graphs
two ways initializing strategy player



fidecentralized anti coordination multi agent learning

c n k log n join delay converge init population

c n k log n join delay converge init population kps
















jain index

jain index















constant
linear
worstplayer























constant
linear
worstplayer












n















n

greedy

b polite

figure joining players jain index c n k log n two graphs
two ways initializing strategy player

players divided average throughput first players join network
beginning old players
lets look first case players greedy constant scheme
ratio around linear scheme ratio lower although increasing n
total number players grows worst player last scheme ratio stays constant
interestingly lower means old players better
players
players polite situation opposite old players way better
players constant scheme throughput ratio
figures graphs c n newly joining players
worse even start transmitting every slot
experience collision every time channels slots occupied old
players experience collision probability n hand overall


fairness whole population better channels share
agent use one channel
difference old players even pronounced
players polite
restarting players
another scenario looked happens one old players switches
replaced player randomly initialized strategy say
player got restarted wireless network corresponds situation user
restarts router note number players network stays
players forget learned start scratch
specifically every round every player probability pr
restarted restart start strategy initialized two ways


ficigler faltings

c n k log n join delay converge init population

c n k log n join delay converge init population kps







constant
linear
worstplayer



constant
linear
worstplayer




group fairness

group fairness






























n













n

greedy

b polite

figure joining players group fairness c n k log n two graphs
two ways initializing strategy player

greedy assume player know n number players system
signal value k k chooses randomly k c means
attempts transmit every slot randomly chosen channel
polite assume player know n k k chooses k c probability
c
n k otherwise
figure shows average overall throughput n c k n log n
k n two initialization schemes dotted line four graphs shows
overall performance players attempt transmit randomly chosen channel
c
probability n
baseline solution reaches e average throughput
probability restart increases average throughput decreases players
get restarted greedy attempt transmit every slot
one channel available means restarted player causes collision every slot
therefore surprising restart probability pr n
throughput virtually every step expectation least one player get restarted
collision almost
interesting phase transition occurs pr k n log n
pr k n performance
baseline random access scenario requires players know n though similar
phase transition occurs players polite even though resulting throughput
higher since restarted players less aggressive
yet another interesting surprising phenomenon worstplayer last scheme still achieves highest throughput constant back scheme
better linear back scheme average overall throughput
matters fast players able reach perfect allocation disruption
worst player last scheme fastest since resolves collision step con

fidecentralized anti coordination multi agent learning

n c k nlog n kps














system throughput

system throughput

n c k nlog n

















constant
linear
worstplayer









restart probability







constant
linear
worstplayer


greedy k n log n





n c k n kps














system throughput

system throughput

n c k n










restart probability

b polite k n log n















constant
linear
worstplayer








restart probability







constant
linear
worstplayer




c greedy k n




restart probability

polite k n

figure restarting players throughput n c







ficigler faltings

n c n k nlog n kps














system throughput

system throughput

n c n k nlog n

















constant
linear
worstplayer









restart probability







constant
linear
worstplayer


greedy k log n



n c n k kps














system throughput

system throughput

n c n k












restart probability

b polite k log n















constant
linear
worstplayer








restart probability







constant
linear
worstplayer




c greedy k






restart probability



polite k

figure restarting players throughput n c

n


stant scheme back probability p worse see theorem linear scheme
slowest
figure shows average overall throughput c n k log n k
substantial difference players greedy polite since
many channels available restarted player cause small number collisions
one channel n every slot throughput decrease much
convergence time linear constant scheme
c n adapt disruption equally well
noisy feedback
far assumed players receive perfect feedback whether transmissions
successful could observe activity given channel perfectly
going loosen assumption


fidecentralized anti coordination multi agent learning

n c n k nlog n














system throughput

system throughput

n c k nlog n

















constant
linear
worstplayer








noisy feedback probability







constant
linear
worstplayer




c k n log n




noisy feedback probability

b c

n






k log n

figure noisy feedback throughput n

n c n k nlog n


















jain index

jain index

n c k nlog n












constant
linear
worstplayer













noisy feedback probability







constant
linear
worstplayer






c k n log n




noisy feedback probability

b c

n






k log n

figure noisy feedback jain index n
suppose every step every player probability pf feedback
receives wrong player transmitted learn transmission
successful vice versa player observed channel
learn channel free fact vice versa context
wireless networks corresponds interference wireless channel
affect learning
figure average overall throughput c c n respectively one channel constant scheme better linear scheme
adapts faster disruptions c n schemes equivalent
equally fast adapt phase transition occurs noisy feedback probability
pf


ficigler faltings

n c n k nlog n















system throughput

system throughput

n c k nlog n

















constant
linear
worstplayer








noisy signal probability







constant
linear
worstplayer




c k n log n




noisy signal probability

b c

n






k log n

figure noisy coordination signal throughput n
figure shows jain index allocation players receive noisy feedback
usual linear scheme better constant even though throuput lower
shown overall throughput drops close schemes
obviously almost fairness
noisy coordination signal
assumes players observe coordination signal every
step signal come may random noise given
frequency fm radio transmission etc however coordination signal might noisy
different players observe different value means learning would
sync wireless networks corresponds clock drift
see happens case use following experiment every step
every player observes correct signal e one observed everyone else
probability ps probability ps observes false signal still
taken uniformly random set k
overall throughput shown figure see system able
cope fairly high level noise signal drop throughput occurs
ps case experiments noisy feedback constant back
scheme able achieve higher throughput thanks faster convergence
jain index allocation figure stays almost constant
throughput drops jain index increases allocation random
fair
generic multi agent learning
several proved converge correlated equilibrium proposed multi agent learning literature introduction mentioned three
learning foster vohra hart mas colell blum man

fidecentralized anti coordination multi agent learning

n c n k nlog n


















jain index

jain index

n c k nlog n












constant
linear
worstplayer













noisy signal probability







constant
linear
worstplayer






c k n log n




noisy signal probability

b c

n






k log n

figure noisy coordination signal jain index n
sour however analysis foster vohra applicable games two
players section briefly recall two multi agent learning
hart mas colell blum mansour compare performance
presented section
two compare notion minimizing regret agents experience adopting certain strategy intuitively
describe concept regret follows imagine agent uses strategy couple
rounds game accumulates certain payoff would know
payoff compare payoff acquired simple alternative strategy difference payoff strategy regret agent perceives ex post
choosing strategy strategy
mean simple strategy one class simple strategies strategies
select action external regret compares performance
strategy performance best single action ex post
another class alternative strategies strategies modify strategy slightly
every time strategy proposes play action alternative strategy proposes
action instead internal regret defined regret strategy compared
best alternative strategy agents adopt strategy low internal
regret converge strategy profile close correlated equilibrium
shown blum mansour
hart mas colell present simple multi agent learning
guaranteed converge correlated equilibrium assume players
observe actions opponents every round game players start
choosing actions randomly update strategy follows let ai
action player played round action aj ai aj ai player calculates
difference average payoff would received played action aj
instead ai past average payoff received far playing action ai
mentioned call difference internal regret playing action ai


ficigler faltings

c n

c




constant backoff
hartmascolell
blummansour

constant backoff
hartmascolell
blummansour


convergence steps

convergence steps























n





c






n

b c





n


figure general multi agent learning convergence rate

instead action aj player chooses action play round probability
proportional internal regret compared previous action ai actions negative
regret never played previous action ai played positive probability
way strategy certain inertia
hart mas colell prove agents adopt adaptive procedure described empirical distribution play relative frequency playing
certain pure strategy profile converges almost surely set correlated equilibria
blum mansour present general technique convert learning
low external regret low internal regret idea run
multiple copies external regret step copy returns probability
vector playing action probability vectors combined one joint
probability vector player observes payoff playing action updates
payoff beliefs external regret proportionally weight
joint probability vector authors players use learning
low internal regret empirical distribution game converges close
correlated equilibrium
one low external regret blum mansour present
polynomial weights pw player keeps weight
actions every round game updates weight proportionally loss
negative payoff action incurred round actions higher weight get
chosen higher probability
implemented two generic multi agent learning internalregret hart mas colell pw blum
mansour experiments converge pure strategy
nash equilibrium channel allocation game therefore efficient allocation
however resulting allocation fair subset agents size c ever
access channels


fidecentralized anti coordination multi agent learning

c n


















jain index

jain index

c












constant backoff
hartmascolell
blummansour










n



constant backoff
hartmascolell
blummansour





c







n

b c





n


figure general multi agent learning jain index

figure shows average number rounds take converge
stable outcome compare performance learning section
learning set k converges pure strategy
nash equilibrium game performed runs scenario
error bars figure confidence interval average assuming
convergence times distributed according normal distribution
surprisingly generic hart mas colell blum
mansour cannot match convergence speed designed specifically
channel allocation generic converge pure strategy
ne outcome unfair jain index low evidenced figure
dont report confidence bounds jain index experiments
resulting jain index

related work
broadly speaking interested games payoff agent receives
certain action inversely proportional number agents chose
action achieve efficient fair outcome games variants
studied several previous works
simplest variant minority game challet marsili zhang
game n agents simultaneously choose two actions agents chose
action chosen minority agents receive payoff whereas agents whose
action choice majority receive
payoff game many pure strategy nash
equilibria group n agents chooses one action rest choose
action equilibria efficient since largest possible number agents achieve
maximum payoff however fair payoff losing group agents
game one mixed strategy ne fair every agent chooses


ficigler faltings

action randomly equilibrium
hand efficient expected size
minority group lower n due variance action selection
savit manuca riolo agents receive feedback action
minority learn coordinate better achieve efficient outcome
repeated minority game basing agents decisions history
past iterations cavagna shows achieved agents
base decisions value random coordination signal instead
history direct inspiration idea global coordination signal presented

ideas literature minority games recently found way
cognitive radio literature mahonen petrova present channel allocation
much agents learn channel use strategy
similar strategies minority games difference instead preferring
action chosen minority channel allocation agent prefers channels
chosen anyone else mahonen petrova able
achieve stable throughput even number agents try
transmit channel increases however agent essentially choosing one
fixed set strategies cannot adapt therefore difficult achieve
perfectly efficient channel allocation
wang et al implemented work actual wireless
network setting wireless devices able monitor activity channels
coordination signal used actual data packets agents send
authors shown practice learning call attachment
learning improves throughput random access slotted aloha protocol
another general variant called dispersion game described
grenager powers shoham dispersion game agents choose several
actions prefer one chosen smallest number agents
authors define maximal dispersion outcome outcome agent move
action fewer agents set maximal dispersion outcomes corresponds set
pure strategy nash equilibria game propose strategies converge
maximal dispersion outcome different assumptions information available
agents contrary work individual agents dispersion games
particular preference actions chosen equilibria
achieved therefore issues achieving fair outcome
verbeeck nowe parent tuyls use reinforcement learning namely linear
reward inaction automata learn nash equilibria common conflicting interest
games class conflicting interest games channel allocation game
belongs propose allows agents circulate
pure strategy nash equilibria outcome game fair contrast
work solution requires communication agents requires
agents know strategies converged addition linear reward inaction automata
guaranteed converge pure strategy ne conflicting interest games
may converge pure strategies
games discussed including channel allocation game form part
family potential games introduced monderer shapley game called


fidecentralized anti coordination multi agent learning

potential game admits potential function potential function defined every
strategy profile quantifies difference payoffs agent unilaterally deviates
given strategy profile different kinds potential functions exact
difference payoffs deviating agent corresponds directly difference
potential function ordinal sign potential difference
sign payoff difference etc
potential games several nice properties important purestrategy nash equilibrium local maximum potential function finite
potential games players reach equilibria unilaterally playing best response
matter initial strategy profile start
existence natural learning reach nash equilibria makes potential
games interesting candidate future would see kind
correlated equilibria agents converge use simple correlation
signal coordinate

conclusions
proposed reach efficient fair solutions multi agent
resource allocation instead centralized smart coordination device
compute allocation use stupid coordination signal general random integer
k k priori relation agents smart
learn value coordination signal action take
game theoretic perspective ideal outcome game correlated equilibrium global coordination signal agents learn play
convex combination pure strategy nash equilibria correlated equilibrium
showed learning strategy variant channel allocation game converges expected polynomial number steps efficient correlated equilibrium
proved equilibrium becomes increasingly fair k number available
synchronization signals increases
confirmed fast convergence well increasing fairness increasing k
experimentally investigated performance learning strategy case
agent population dynamic agents join population learning strategy
still able learn efficient allocation however fairness allocation depend
greedy initial strategies agents agents restart random
intervals becomes important fast strategy converges simple strategy
everyone backs transmitting constant probability able achieve higher
throughput sophisticated strategy back probability depends
many slots agent already transmitting showed experimentally
learning strategy robust noise coordination signal well
feedback agents receive channel use noisy scenarios faster convergence constant back scheme helped achieve higher throughput
fair linear back scheme finally compared performance learning strategy generic multi agent learning regret minimization hart
mas colell blum mansour generic theoretically
proven converge distribution play close correlated equilibrium


ficigler faltings

guaranteed converge specific ce indeed experiments
hart mas colell blum mansour converged efficient unfair
pure strategy nash equilibrium channel allocation game
learning presented implemented real wireless
network wang et al shown achieves higher throughput
random access protocols aloha
address issue whether non cooperative rational
agents would follow protocol outlined work cigler faltings
address issue certain conditions protocol implemented
nash equilibrium strategies infinitely repeated resource allocation game

references
abramson n aloha system another alternative computer communications proceedings november fall joint computer conference
afips fall pp york ny usa acm
aumann r subjectivity correlation randomized strategies journal
mathematical economics
billingsley p probability measure wiley series probability statistics
anniversary edition edition wiley
blum mansour algorithmic game theory nisan n roughgarden
tardos e vazirani v eds algorithmic game theory chap cambridge
university press
cavagna irrelevance memory minority game physical review e
r r
challet marsili zhang c minority games interacting agents
financial markets oxford finance oxford university press york ny usa
chen x deng x settling complexity two player nash equilibrium
th annual ieee symposium foundations computer science focs
pp ieee
cheng raja xie l howitt distributed constraint optimization dynamic load balancing wlans ijcai workshop distributed
constraint reasoning dcr
cigler l faltings b symmetric subgame perfect equilibria resource allocation appear proceedings th national conference artificial
intelligence aaai menlo park ca usa american association artificial
intelligence
feller w introduction probability theory applications vol rd
edition edition wiley
foster p vohra r v calibrated learning correlated equilibrium games
economic behavior


fidecentralized anti coordination multi agent learning

gast n computing hitting times via fluid approximation application coupon
collector arxiv e prints
grenager powers r shoham dispersion games general definitions
specific learning proceedings eighteenth national conference
artificial intelligence aaai pp menlo park ca usa american
association artificial intelligence
hart mas colell simple adaptive procedure leading correlated equilibrium econometrica
jain r k chiu w hawe w r quantitative measure fairness
discrimination resource allocation shared computer systems tech rep digital
equipment corporation
leyton brown k shoham essentials game theory concise multidisciplinary introduction morgan claypool san rafael ca
littman stone p implicit negotiation repeated games intelligent agents
viii meyer j j tambe eds intelligent agents viii vol lecture
notes computer science chap pp springer berlin heidelberg
berlin heidelberg
mahonen p petrova minority game cognitive radios cooperating without cooperation physical communication
monderer shapley l potential games games economic behavior

norris j r markov chains cambridge series statistical probabilistic
mathematics cambridge university press
papadimitriou c h roughgarden computing correlated equilibria multiplayer games journal acm
rego v naive asymptotics hitting time bounds markov chains acta informatica
savit r manuca r riolo r adaptive competition market efficiency
phase transitions physical review letters
verbeeck k nowe parent j tuyls k exploring selfish reinforcement
learning repeated games stochastic rewards autonomous agents multiagent systems
wang l wu k hamdi ni l attachment learning multi channel
allocation distributed ofdma networks parallel distributed systems international conference




