Journal Artificial Intelligence Research 36 (2009) 307-340

Submitted 06/09; published 11/09

Cross-lingual Annotation Projection Semantic Roles
Sebastian Pado

pado@ims.uni-stuttgart.de

Institut fur maschinelle Sprachverarbeitung
Universitat Stuttgart, 70174 Stuttgart, Germany

Mirella Lapata

mlap@inf.ed.ac.uk

School Informatics, University Edinburgh
10 Crichton Street, Edinburgh EH8 10 AB, UK

Abstract
article considers task automatically inducing role-semantic annotations
FrameNet paradigm new languages. propose general framework based
annotation projection, phrased graph optimization problem. relatively inexpensive potential reduce human effort involved creating role-semantic
resources. Within framework, present projection models exploit lexical
syntactic information. provide experimental evaluation English-German parallel corpus demonstrates feasibility inducing high-precision German semantic
role annotation manually automatically annotated English data.

1. Introduction
Semantic roles play prominent role linguistic theory (Fillmore, 1968; Jackendoff, 1990;
Dowty, 1991). describe relations hold predicate arguments,
abstracting surface syntactic configurations. example, consider sentences (1a)
(1b) butter uniformly assigned semantic role Undergoer (since undergoes physical change) even though syntactically realized object verb
melt (1a) subject (1b):
(1)

a.
b.

[Bob]Agent melted [the butter]Undergoer .
[The butter]Undergoer melted.

intermediate representation seems promising first step towards text understanding,
ultimately benefit many natural language processing tasks require broad coverage semantic processing.
Methods automatic identification labeling semantic roles, often referred
shallow semantic parsing (Gildea & Jurafsky, 2002), important prerequisite
widespread use semantic role information large-scale applications. development
shallow semantic parsers1 greatly facilitated availability resources
FrameNet (Fillmore, Johnson, & Petruck, 2003) PropBank (Palmer, Gildea, & Kingsbury, 2005), document possible surface realization semantic roles. Indeed, semantic
1. Approaches building shallow semantic parsers numerous list. refer interested reader
proceedings 2005 CoNLL shared task (Carreras & Marquez, 2005) 2008 Computational Linguistics Special Issue Semantic Role Labeling (Marquez, Carreras, Litkowski, & Stevenson,
2008) overview state-of-the-art.
c
2009
AI Access Foundation. rights reserved.

fiPado & Lapata

Departing
object (the Theme) moves away Source.
Theme

officer left house.
plane leaves seven.
departure delayed.

Source

departed New York.
retreated opponent.
woman left house.

FEEs

abandon.v, desert.v, depart.v, departure.n,
emerge.v, emigrate.v, emigration.n, escape.v,
escape.n, leave.v, quit.v, retreat.v, retreat.n,
split.v, withdraw.v, withdrawal.n

Table 1: Abbreviated FrameNet entry Departing frame
roles recently found use applications ranging information extraction (Surdeanu,
Harabagiu, Williams, & Aarseth, 2003) modeling textual entailment relations (Tatu
& Moldovan, 2005; Burchardt & Frank, 2006), text categorization (Moschitti, 2008), question answering (Narayanan & Harabagiu, 2004; Frank, Krieger, Xu, Uszkoreit, Crysmann,
Jorg, & Schafer, 2007; Moschitti, Quarteroni, Basili, & Manandhar, 2007; Shen & Lapata,
2007), machine translation (Wu & Fung, 2009a, 2009b) evaluation (Gimenez &
Marquez, 2007).
FrameNet paradigm, meaning predicate (usually verb, noun, adjective) represented reference frame, prototypical representation situation
predicate describes (Fillmore, 1982). semantic roles, called frame elements, correspond entities present situation, therefore frame-specific.
frame, English FrameNet database2 lists predicates evoke (called
frame-evoking elements FEEs), gives possible syntactic realizations semantic
roles, provides annotated examples British National Corpus (Burnard, 2000).
abbreviated example definition Departing frame shown Table 1. semantic roles illustrated example sentences FEEs shown bottom
table (e.g., abandon, desert, depart). PropBank corpus, second major semantic role resource English, provides role realization information verbs similar
manner Wall Street Journal portion Penn Treebank. uses index-based role
names (Arg0Argn), Arg0 Arg1 correspond Dowtys (1991) proto-agent
proto-patient. Higher indices defined verb-by-verb basis.
Unfortunately, resources FrameNet PropBank largely absent almost
languages except English, main reason role-semantic annotation
expensive time-consuming process. current English FrameNet (Version 1.3)
developed past twelve years. contains roughly 800 frames covering
2. Available http://framenet.icsi.berkeley.edu.

308

fiCross-lingual Annotation Projection Semantic Roles

around 150,000 annotated tokens 7,000 frame-evoking elements. Although FrameNets
constructed German, Spanish, Japanese, resources considerably
smaller. true PropBank-style resources, developed
Korean3 , Chinese (Xue & Palmer, 2009), Spanish Catalan (Taule, Mart, & Recasens,
2008). Compared English PropBank, covers 113,000 predicate-argument structures, resources languages two three times smaller (e.g., Korean
PropBank provides 33,000 annotations).
Given data requirements supervised learning algorithms (Fleischman & Hovy,
2003) current paucity data, unsupervised methods could potentially enable
creation annotated data new languages reduce human effort involved.
However, unsupervised approaches shallow semantic parsing still early stage,
mostly applicable resources FrameNet (Swier & Stevenson, 2004, 2005;
Grenager & Manning, 2006). article, propose method employs parallel
corpora acquiring frame elements syntactic realizations new languages (see
upper half Table 1). approach leverages existing English FrameNet overcome resource shortage languages exploiting translational equivalences
present aligned data. Specifically, uses annotation projection (Yarowsky, Ngai, & Wicentowski, 2001; Yarowsky & Ngai, 2001; Hwa, Resnik, Weinberg, & Kolak, 2002; Hi &
Hwa, 2005) transfer semantic roles English less resource-rich languages. key
idea projection summarized follows: (1) given pair sentences E (English)
L (new language) translations other, annotate E semantic roles;
(2) project roles onto L using word alignment information. manner,
induce semantic structure L side parallel text, serve
data training shallow semantic parser L independent parallel corpus.
annotation projection paradigm faces least two challenges considering semantic roles. Firstly, semantic structure projected must shared two
sentences. Clearly, role-semantic analysis source sentence E inappropriate
target sentence L, simple projection produce valid semantic role annotations.
Secondly, even two sentences demonstrate semantic parallelism, semantic role annotations
pertain potentially arbitrarily long word spans rather individual words. Recovering word span semantic roles target language challenging given
automatic alignment methods often produce noisy incomplete alignments.
address first challenge showing that, two languages exhibit substantial
degree semantic correspondence, annotation projection feasible. Using EnglishGerman parallel corpus test bed, assess whether English semantic role annotations
transferred successfully onto German. find two languages exhibit
degree semantic correspondence substantial enough warrant projection. tackle
second challenge presenting framework projection semantic role annotations
goes beyond single word alignments. Specifically, construct semantic alignments
constituents source target sentences formalize search best
semantic alignment optimization problem bipartite graph. argue bipartite
graphs offer flexible intuitive framework modeling semantic alignments able
deal noise represent translational divergences. present different classes
3. Korean PropBank available LDC (http://www.ldc.upenn.edu/).

309

fiPado & Lapata

models varying assumptions regarding admissible correspondences source
target constituents. Experimental results demonstrate constituent-based models
outperform word-based alternatives large margin.4
remainder article organized follows. Section 2 discusses annotation
projection general presents annotation study examining degree semantic
parallelism English-German corpus. Section 3, formalize semantic alignments
present modeling framework. experiments detailed Section 4. review
related work Section 5 conclude article discussion future work (Section 6).

2. Annotation Projection Semantic Correspondence
recent years, interest grown parallel corpora multilingual cross-lingual
natural language processing. Beyond machine translation, parallel corpora exploited
relieve effort involved creating annotations new languages. One important
paradigm, annotation projection, creates new monolingual resources transferring annotations English (or resource-rich languages) onto resource-scarce languages
use word alignments. resulting (noisy) annotations used
conjunction robust learning algorithms obtain NLP tools taggers
chunkers relatively cheaply. projection approach successfully used transfer wide range linguistic annotations languages. Examples include parts
speech (Yarowsky et al., 2001; Hi & Hwa, 2005), chunks (Yarowsky et al., 2001), dependencies (Hwa et al., 2002), word senses (Diab & Resnik, 2002; Bentivogli & Pianta, 2005),
information extraction markup (Riloff, Schafer, & Yarowsky, 2002), coreference chains (Postolache, Cristea, & Orasan, 2006), temporal information (Spreyer & Frank, 2008), LFG
f-structures (Tokarczyk & Frank, 2009).
important assumption underlying annotation projection linguistic analyses
one language valid another language. however unrealistic expect
two languages, even family, perfect correspondence.
many well-studied systematic differences across languages often referred translational
divergences (van Leuven-Zwart, 1989; Dorr, 1995). structural,
semantic content source target language realized using different
structures, semantic, content undergoes change translation. Translational divergences (in conjunction poor alignments) major stumbling block
towards achieving accurate projections. Yarowsky Ngai (2001) find parts speech
transferred directly English onto French contain considerable noise, even
cases inaccurate automatic alignments manually corrected (accuracies vary
69% 78% depending tagset granularity). syntax, Hwa, Resnik, Weinberg, Cabezas, Kolak (2005) find 37% English dependency relations
direct counterparts Chinese, 38% Spanish. problem commonly addressed
filtering mechanisms, act post-processing step projection output.
example, Yarowsky Ngai (2001) exclude infrequent projections poor alignments
4. preliminary version work published proceedings EMNLP 2005 COLING/ACL
2006. current article contains detailed description approach, presents several novel
experiments, comprehensive error analysis.

310

fiCross-lingual Annotation Projection Semantic Roles

Hwa et al. (2005) apply transformation rules encode linguistic knowledge
target language.
case semantic frames reason optimism. definition, frames
based conceptual structure (Fillmore, 1982). latter constitute generalizations
surface structure therefore ought less prone syntactic variation. Indeed, efforts
develop FrameNets manually German, Japanese, Spanish reveal large
number English frames re-used directly describe predicates arguments
languages (Ohara, Fujii, Saito, Ishizaki, Ohori, & Suzuki, 2003; Subirats & Petruck,
2003; Burchardt, Erk, Frank, Kowalski, Pado, & Pinkal, 2009). Boas (2005) even suggests
frame semantics interlingual meaning representation.
Computational studies projection parallel corpora obtained good results
semantic annotation. Fung Chen (2004) induce FrameNet-style annotations Chinese mapping English FrameNet entries directly onto concepts listed HowNet5 ,
on-line ontology Chinese, without using parallel texts. experiment, transfer
semantic roles English Chinese accuracy 68%. Basili, Cao, Croce, Coppola, Moschitti (2009) use gold standard annotations transfer semantic roles
English Italian 73% accuracy. Bentivogli Pianta (2005) project EuroWordNet
sense tags, represent fine-grained semantic information FrameNet,
English Italian. obtain precision 88% recall 71%, without applying filtering. Fung, Wu, Yang, Wu (2006, 2007) analyse automatically annotated
EnglishChinese parallel corpus find high cross-lingual agreement PropBank roles
(in range 75%95%, depending role).
provide sound empirical justification projection-based approach, conducted manual annotation study parallel English-German corpus. identified
semantic role information bi-sentences assessed degree frames semantic roles agree diverge English German. degree divergence provides
natural upper bound accuracy attainable annotation projection.
2.1 Sample Selection
English-German bi-sentences drawn second release Europarl (Koehn, 2005),
corpus professionally translated proceedings European Parliament. Europarl
aligned document sentence level available 11 languages. English
German section contains 25 million words sides. Even though restricted
genre (transcriptions spoken text), Europarl fairly open-domain, covering wide range
topics foreign politics, cultural economic affairs, procedural matters.
naive sampling strategy would involve randomly selecting bi-sentences Europarl
contain FrameNet predicate English side aligned word
German side. two caveats here. First, alignment two predicates
may wrong, leading us assign wrong frame German predicate. Secondly, even
alignment accurate, possible randomly chosen English predicate evokes
frame yet covered FrameNet. example, FrameNet 1.3 documents
receive sense verb accept (as sentence Mary accepted gift),
entry admit sense predicate (e.g., accept problem
5. See http://www.keenage.com/zhiwang/e_zhiwang.html.

311

fiPado & Lapata

Measure
Frame Match
Role Match
Span Match

English
89.7
94.9
84.4

German
86.7
95.2
83.0


88.2
95.0
83.7

Table 2: Monolingual inter-annotator agreement calibration set

EU ) relatively frequent Europarl. Indeed, pilot study, inspected
small random sample consisting 100 bi-sentences, using publicly available GIZA++
software (Och & Ney, 2003) induce English-German word alignments. found 25%
English predicates readings documented FrameNet, additional
9% predicate pairs instances wrong alignments. order obtain cleaner
sample, final sampling procedure informed English FrameNet SALSA,
FrameNet-compatible database, German (Erk, Kowalski, Pado, & Pinkal, 2003).
gathered GermanEnglish sentences corpus least one pair
GIZA++-aligned predicates (we , wg ), listed FrameNet wg SALSA,
intersection two frame lists wg non-empty. corpus
contains 83 frame types, 696 lemma pairs, 265 unique English 178 unique German
lemmas. Sentence pairs grouped three bands according frame frequency
(High, Medium, Low). randomly selected 380 pairs band annotation.
total sample consisted 1,140 bi-sentences. semantic annotation took place,
constituency parses corpus obtained Collins (1997) parser English
Dubeys (2005) German. automatic parses corrected manually, following
annotation guidelines Penn Treebank (English) TIGER corpus (German).
2.2 Annotation
syntactic correction, two annotators native-level proficiency German English annotated bi-sentence frames evoked wg semantic
roles (i.e., one frame per monolingual sentence). every predicate, task involved two
steps: (a) selecting appropriate frame (b) assigning instantiated semantic roles
sentence constituents. Annotators provided detailed guidelines explained
task multiple examples.
annotation took place gold standard parsed corpus proceeded three
phases: training phase (40 bi-sentences), calibration phase (100 bi-sentences),
production mode phase (1000 bi-sentences). training, annotators acquainted
annotation style. calibration phase, bi-sentence doubly annotated
assess inter-annotator agreement. Finally, production mode, 1000 bi-sentences
main dataset split half randomly assigned one coders
single annotation. thus ensured annotator saw parts bi-sentence
avoid language bias role assignment (annotators may prone label
English sentence similar German translation vice versa). coder annotated
approximately amount data English German access
FrameNet SALSA resources.
312

fiCross-lingual Annotation Projection Semantic Roles

Measure
Frame Match
Role Match

Precision
71.6
90.5

Recall
71.6
92.3

F1-Score
71.6
91.4

Table 3: Semantic parallelism English German
results inter-annotator agreement study given Table 2. widely used
Kappa statistic directly applicable task requires fixed set items
classified fixed set categories. case, however, fixed items, since
span frame elements length. addition, categories (i.e., frames
roles) predicate-specific, vary item item (for discussion issue, see
work Miltsakaki et al., 2004). Instead, compute three different agreement measures
defined as: ratio common frames two sentences (Frame Match), ratio
common roles (Role Match), ratio roles identical spans (Span Match).
shown Table 2, annotators tend agree frame assignment; disagreements mainly
due fuzzy distinctions closely related frames (e.g., Awareness
Certainty). Annotators agree roles assign identifying role spans.
Overall, obtain high agreement aspects annotation, indicates
task well-defined. aware published agreement figures English
FrameNet annotations, results comparable numbers reported Burchardt,
Erk, Frank, Kowalski, Pado, Pinkal (2006) German, viz. 85% agreement frame
assignment (Frame Match) 86% agreement role annotation.6
2.3 Evaluation
Recall main dataset consists 1,000 English-German bi-sentences annotated
FrameNet semantic roles. Since annotations language created independently, used provide estimate degree semantic parallelism
two languages. measured parallelism using precision recall, treating
German annotations gold standard. evaluation scheme directly gauges usability English source language annotation projection. Less 100% recall
means target language frames roles present English
cannot retrieved annotation projection. Conversely, imperfect precision indicates
English frames roles whose projection yields erroneous annotations target language. Frames roles counted matching occur halves
bi-sentence, regardless role spans, comparable across languages.
shown Table 3, 72% time English German evoke
frame (Frame Match). result encouraging, especially considering frame
disagreements arise within single language demonstrated inter-annotator
study calibration set (see row Frame Match Table 2). However, indicates
non-negligible number cases translational divergence frame level.
often cases one language chooses single predicate express situation
whereas one uses complex predication. following example, English
transitive predicate increase evokes frame Cause change scalar position (An
6. parallel corpus created available http://nlpado.de/~sebastian/srl_data.html.

313

fiPado & Lapata

agent cause increases position variable scale). German translation
fuhrt zu hoheren (leads higher) combines Causation frame evoked fuhren
inchoative Change scalar position frame introduced hoher :
(2)

increase level employment.
Dies wird zu einer hoheren Erwebsquote
fuhren.

higher
level employment lead

level semantic roles, agreement (Role Match) reaches F1-Score 91%.
means frames correspond across languages, roles agree large extent.
Role mismatches frequently cases passivization infinitival constructions leading
role elision. example below, remembered denkt evoke Memory
frame. English uses passive construction leaves deep subject position unfilled.
contrast, German uses active construction deep subject position filled
semantically light pronoun, man (one).
(3)

ask [Ireland]Content remembered.
Ich mochte
darum bitten, dass [man]Cognizer [an Irland]Content denkt.
would
ask one
Ireland
thinks

sum, find substantial cross-lingual semantic correspondence
English German provided predicates evoke frame. enlisted
help SALSA database meet requirement. Alternatively, could used
existing bilingual dictionary (Fung & Chen, 2004), aligned frames automatically using
vector-based representation (Basili et al., 2009) inferred FrameNet-style predicate labels
German following approach proposed Pado Lapata (2005).

3. Modeling Semantic Role Projection Semantic Alignments
previous work projection relies word alignments transfer annotations
languages. surprising, since annotations interest often defined
word level (e.g., parts speech, word senses, dependencies) rarely span
one token. contrast, semantic roles cover sentential constituents arbitrary length,
simply using word alignments projection likely result wrong role spans.
example, consider bi-sentence Figure 1.7 Assume (English) source annotated semantic roles wish project onto
(German) target. Although alignments (indicated dotted lines sentence) accurate (e.g., promised versprach, zu), others noisy incomplete
(e.g., time punktlich instead time punktlich). Based alignments,
Message role would projected German onto (incorrect) word span punktlich zu
instead punktlich zu kommen, since kommen aligned English word.
course possible devise heuristics amending alignment errors. However,
process scale well: different heuristics need created different errors,
7. literal gloss German sentence Kim promises timely come.

314

fiCross-lingual Annotation Projection Semantic Roles



NP SPEAKER


VP

NP SPEAKER

VP
MESSAGE

MESSAGE
Kim promised time

Kim versprach, pnktlich zu kommen

Figure 1: Bilingual projection semantic role information semantic alignments constituents.

process repeated new language pair. Instead, projection model
alleviates problem principled manner taking constituency information
account. Specifically, induce semantic alignments source target sentences
relying syntactic constituents introduce bias towards linguistically meaningful
spans. constituent aligned correctly, sufficient subset yield
correctly word-aligned. So, Figure 1, align time punktlich zu
kommen project role Message accurately, despite fact kommen
aligned other. following, describe detail semantic alignments
computed subsequently guide projection onto target language.
3.1 Framework Formalization
bi-sentence represented set linguistic units. distinguished
source (us Us ) target (ut Ut ) units words, chunks, constituents,
groupings. semantic roles source sentence modeled labeling function
: R 2Us maps roles sets source units. view projection construction
similar role labeling function target sentence, : R 2Ut . Without loss
generality, limit one frame per sentence, FrameNet.8
semantic alignment Us Ut subset Cartesian product
source target units:
Us Ut
(4)
alignment link us Us ut Ut implies us ut semantically
equivalent. Provided role assignment function source sentence, ,
projection consists simply transferring source labels r onto union target
units semantically aligned source units bearing label r:
(r) = {ut k us (r) : (us , ut ) A}

(5)

8. entails cannot take advantage potentially beneficial dependencies arguments different predicates within one sentence, shown improve semantic role
labeling (Carreras, Marquez, & Chrupala, 2004).

315

fiPado & Lapata

phrase search semantic alignment optimization problem. Specifically,
seek alignment maximizes product bilingual similarities sim
source target units:

sim(us , ut )
(6)
= argmax
AA

(us ,ut )A

several well-established methods literature computing semantic similarity within one language (see work Weeds, 2003, Budanitsky & Hirst, 2006,
overviews). Measuring semantic similarity across languages well studied
less consensus appropriate methods are. article, employ
simple method, using automatic word alignments proxy semantic equivalence;
however, similarity measures used (see discussion Section 6). Following
general convention, assume sim function ranging 0 (minimal similarity)
1 (maximal similarity).
wealth optimization methods used solve (6). article, treat
constituent alignment bipartite graph optimization problem. Bipartite graphs provide
simple intuitive modeling framework alignment problems optimization algorithms well-understood computationally moderate. importantly,
imposing constraints bipartite graph, bias model linguistically
implausible alignments, example alignments map multiple English roles onto single German constituent. Different graph topologies correspond different constraints
set admissible alignments A. instance, may want ensure source
target units aligned, restrict alignment one-to-one matches (see Section 3.3
details).
formally, weighted bipartite graph graph G = (V, E) whose node set V
partitioned two nonempty sets V1 V2 way every edge E joins
node V1 node V2 labeled weight. projection application,
two partitions sets linguistic units Us Ut , source target sentence,
respectively. assume G complete, is, source node connected
target nodes vice versa.9 Edge weights model (dis-)similarity pair
source target units.
optimization problem Equation (6) identifies alignment maximizes
product link similarities equivalent edges bipartite graph. Finding
optimal alignment amounts identifying minimum-weight subgraph (Cormen, Leiserson,
& Rivest, 1990) subgraph G0 G satisfies certain structural constraints (see
discussion below) incurring minimal edge cost:
= argmin
AA

X

weight(us , ut )

(7)

(us ,ut )A

minimization problem Equation (7) equivalent maximization problem (6)
setting weight(us , ut ) to:
weight(us , ut ) = log sim(us , ut )
9. Unwanted alignments excluded explicitly setting similarity zero.

316

(8)

fiCross-lingual Annotation Projection Semantic Roles

S4

S'4
VP2

NP3

NP'3

S1

Kim promised time

VP'2
S'1

Kim versprach, pnktlich zu kommen

(a) Bi-sentence word alignments

S1
VP2
NP3
S4

S01
0.58
0.45
0
0.37

VP02
0.45
0.68
0
0.55

NP03
0
0
1
0.18

S04
0.37
0.55
0.18
0.73

S1
VP2
NP3
S4

(b) Constituent Similarities

S01
0.54
0.80

1.00

VP02
0.80
0.39

0.60

NP03


0
1.70

S04
1.00
0.60
1.70
0.31

(c) Edge weights

Figure 2: Example bi-sentence represented edge weight matrix
example, consider Figure 2. shows bi-sentence Figure 1 representation edge weight matrix corresponding complete bipartite graph. nodes
graph (S1 S4 source side S01 S04 target side) model sentential constituents.
numbers Figure 2b similarity scores, corresponding edge weights shown
Figure 2c. High similarity scores correspond low edge weights. Edges zero similarity
set infinity (in practice, large number). Finally, notice alignments
high similarity scores (low edge weights) occur diagonal matrix.
order obtain complete projection models must (a) specify linguistic units
alignment takes place; (b) define appropriate similarity function; (c)
formulate alignment constraints. following, describe two models, one
uses words linguistic units one uses constituents. present appropriate
similarity functions models detail alignment constraints.
3.2 Word-based Projection
first model linguistic units word tokens. Source target sentences represented sets words, Us = {ws1 , ws2 , . . . } Ut = {wt1 , wt2 , . . . }, respectively. Semantic
alignments links individual words. thus conveniently interpret
off-the-shelf word alignments semantic alignments. Formally, achieved
following binary similarity function, trivially turns word alignment optimal
semantic alignment.
(
1 ws wt word-aligned
sim(ws , wt ) =
(9)
0 else
Constraints admissible alignments often encoded word alignment models either
heuristically (e.g., enforcing one-to-one alignments Melamed, 2000) virtue
317

fiPado & Lapata

translation model used computation. example, IBM models introduced
seminal work Brown, Pietra, Pietra, Mercer (1993) require target word
aligned exactly one source word (which may empty word), therefore allow
one-to-many alignments one direction. experiments use alignments induced
publicly available GIZA++ software (Och & Ney, 2003). GIZA++ yields alignments
interfacing IBM models 14 (Brown et al., 1993) HMM extensions models 1
2 (Vogel, Ney, & Tillmann, 1996). particular configuration shown
outperform several heuristic statistical alignment models (Och & Ney, 2003). thus
take advantage alignment constraints already encoded GIZA++ assume
optimal semantic alignment given set GIZA++ links. resulting target
language labeling function is:
aw
(r) = {wt k ws (r) : ws wt GIZA++ word-aligned}

(10)

labeling function corresponds (implicit) labeling functions employed
word-based annotation projection models. models easily derived different
language pairs without recourse corpus-external resources. Unfortunately, discussed Section 2, automatically induced alignments often noisy, thus leading
projection errors. Cases point function words (e.g., prepositions) multi-word
expressions, systematically misaligned due high degree cross-lingual
variation.
3.3 Constituent-based Projection
second model linguistic units constituents. Source target sentences
thus represented constituent sets (Us = {c1s , c2s , . . . }) (Ut = {c1t , c2t , . . . }).
constituent-based similarity function capture extent cs projection ct express semantic content. approximate measuring word
alignment-based word overlap cs ct Jaccards coefficient.
Let yield(c) denote set words yield constituent c, al(c) set
words target language aligned yield c. word overlap
source constituent cs target constituent ct defined as:
o(cs , ct ) =

|al(cs ) yield(ct )|
|al(cs ) yield(ct )|

(11)

Jaccards coefficient asymmetric: consider well projection source
constituent al(cs ) matches target constituent ct , vice versa. order take
target-source source-target correspondences account, measure word overlap
directions use mean measure similarity:
sim(cs , ct ) = (o(cs , ct ) + o(ct , cs ))/2

(12)

addition similarity measure, constituent-based projection model must specify
constraints characterize set admissible alignments A. paper,
consider three types alignment constraints affect number alignment links per
constituent (in graph-theoretic terms, degree nodes Us ). focus motivated
318

fiCross-lingual Annotation Projection Semantic Roles

Us

Ut

1

1

2

2

r2

3

3

r2

4

4

5

e

5

5

6

e

6

6

r1

(a) Perfect matching

Us

Ut

1

1

2

2

r2

3

3

r2

4

4

r1
r1
r2

(b) Edge cover

Us

Ut

1

1

2

2

r2

3

3

r2

4

4

r1
r1
r2

r1
r2

(c) Total alignment

Figure 3: Constituent alignments role projections resulting three families alignment constraints (Us , Ut : source target constituents; r1 , r2 : semantic roles).

patterns observe gold standard corpus (cf. Section 2). English
German constituent, determined whether corresponded none, exactly one,
several constituents language, according gold standard word alignment.
majority constituents correspond exactly one constituent (67%), followed
substantial number one-to-many/many-to-one correspondences (32%), cases
constituents translated (i.e., corresponding node side
bi-sentence) rare (1%).
analysis indicates perfect data, expect best performance
model allows one-to-many alignments. However, common finding machine
learning restrictive models, even though appropriate data hand,
yield better results limiting hypothesis space. spirit, compare three families
admissible alignments range restrictive permissive,
evaluate other.
3.3.1 Alignments Perfect Matchings
first consider restrictive case, constituent exactly one adjacent
alignment edge. Semantic alignments property thought bijective
functions: source constituent mapped one target constituent, vice versa.
resulting bipartite graphs perfect matchings. example perfect bipartite matching
given Figure 3a. Note target side contains two nodes labeled (e), shorthand
empty node. Since bi-sentences often differ size, resulting graph partitions
different sizes well. cases, introduce empty nodes smaller
partition enable perfect matching. Empty nodes assigned similarity zero
nodes. Alignments empty nodes (such source nodes (3) (6))
ignored purposes projection; gives model possibility abstaining
aligning node good correspondence found.
Perfect matchings assume strong equivalence constituent structures
two languages; neither alignments Figure 3(b) 3(c) perfect matching.
319

fiPado & Lapata

Perfect matchings cannot model one-to-many matches, i.e., cases semantic material
expressed one constituent one language split two constituents
language. means perfect matchings appropriate source
target role annotations span exactly one constituent. always case,
perfect matchings advantage expressive models: allowing
node one adjacent edge, introduce strong competition edges. result,
errors word alignment corrected extent.
Perfect matchings computed efficiently using algorithms network optimization (Fredman & Tarjan, 1987) time approximately cubic total number constituents bi-sentence (O(|Us |2 log |Us | + |Us |2 |Ut |)). Furthermore, perfect matchings
equivalent well-known linear assignment problem, many solution algorithms
developed (e.g., Jonker Volgenant 1987, time O(max(|Us |, |Ut |)3 )).
3.3.2 Alignments Edge Covers
next consider edge covers, generalization perfect matchings. Edge covers bipartite graphs source target constituent adjacent least one edge.
illustrated Figure 3b, source target nodes one adjacent edge (i.e., alignment link), nodes one (see source node (2) target node (4)). Edge
covers impose weaker correspondence assumptions perfect matchings, since allow one-to-many alignments constituents either direction.10 So, theory, edge
covers higher chance delivering correct role projection perfect matchings
syntactic structures source target sentences different.
deal better situations semantic roles assigned one constituent
one languages (cf. source nodes (3) (4), labeled role r2 , example
graph). Notice perfect matchings shown Figure 3a edge covers, whereas
graph Figure 3c not, target-side nodes (1) (3) adjacent edges.
Eiter Mannila (1997) develop algorithm computing optimal edge covers.
show minimum-weight edge covers reduced minimum weight perfect
matchings (see above) auxiliary bipartite graph two partitions size |Us | + |Ut |.
allows computation minimum weight edge covers time O((|Us | + |Ut |)3 ) =
O(max(|Us |, |Ut |)3 ), cubic number constituents bi-sentence.
3.3.3 Total Alignments
last family admissible alignments consider total alignments. Here, source
constituent linked target constituent (i.e.,the alignment forms total function
source nodes). Total alignments impose constraints target nodes,
therefore linked arbitrary number source nodes, including none. Total
alignments permissive alignment class. contrast perfect matchings
edge covers, constituents target sentence left unaligned. Total alignments
10. general definition edge covers allows many-to-many alignments. However, optimal edge covers
according Equation (7) cannot many-to-many, since weight edge covers many-to-many
alignments never minimal: many-to-many edge cover, one edge removed,
resulting edge cover lower weight.

320

fiCross-lingual Annotation Projection Semantic Roles

computed linking source node maximally similar target node:
= {(cs , ct ) | cs Us ct = argmax sim(cs , c0t )}

(13)

c0t Ut

Due independence source nodes, local optimization results globally optimal
alignment. time complexity procedure quadratic number constituents,
O(|Us ||Ut |).
example shown Figure 3c, source constituents (1) (2) correspond
target constituent (2), source constituents (3)(6) correspond (4). Target constituents (1) (3) aligned. quality total alignments relies heavily
underlying word alignment. Since little competition edges,
tendency form alignments mostly high (similarity) scoring target constituents.
practice, means potentially important, idiosyncratic, target constituents
low similarity scores left unaligned.
3.4 Noise Reduction
discussed Section 2, noise elimination techniques integral part projection
architectures. Although constituent-based model overly sensitive noise
expect syntactic information compensate alignment errors word-based
model error-prone since relies solely automatically obtained alignments
transferring semantic roles. introduce three filtering techniques either
correct discard erroneous projections.
3.4.1 Filling-the-gaps
According definition projection Equation (5), span projected role r
corresponds union target units aligned source units labeled r.
definition sufficient constituent-based projection models, roles rarely span
one constituent yield many wrong alignments word-based models
roles typically span several source units (i.e., words). Due errors gaps
word alignment, target span role often non-contiguous set words.
repair non-contiguous projections first last word projected
correctly applying simple heuristic fills gaps target role span. Let pos
index word token given sentence, extension set indices
set words. target span role r without gaps defined as:
acc
(r) = {u | min(pos(at (r))) pos(u) max(pos(at (r))}

(14)

apply heuristic word-based models article.
3.4.2 Word Filtering
technique removes words form bi-sentence prior projection according certain
criteria. apply two intuitive instantiations word filtering experiments.
first removes non-content words, i.e., words adjectives, adverbs, verbs,
nouns, source target sentences, since alignments non-content words
321

fiPado & Lapata



NP

VP


Kim versprach, pnktlich zu kommen

Figure 4: Argument filtering (predicate boldface, potential arguments boxes).
notoriously unreliable may negatively impact similarity computations.
second filter removes words remain unaligned output automatic word
alignment. filters aim distinguishing genuine word alignments noisy ones
speed computation semantic alignments.
3.4.3 Argument Filtering
last filter applies constituent-based models defined full parse trees. Previous
work shallow semantic parsing demonstrated nodes tree equally
probable semantic roles given predicate (Xue & Palmer, 2004). fact, assuming
perfect parse, set likely arguments, almost semantic roles
assigned. set likely arguments consists constituents child
ancestor predicate, provided (a) dominate predicate
(b) sentence boundary constituent predicate.
definition covers long-distance dependencies control constructions verbs,
support constructions nouns, extended accommodate coordination.
apply filter reduce size target tree. example Figure 4, tree
nodes removed except NP Kim punktlich zu kommen.
3.5 Discussion
presented framework bilingual projection semantic roles based
notion semantic alignment. discussed two broad instantiations
framework, namely word-based constituent-based models. latter case, operationalize search optimal semantic alignment graph optimization problem.
Specifically, bi-sentence conceptualized bipartite graph. nodes correspond
syntactic constituents bi-sentence, weighted edges cross-lingual
pairwise similarity constituents. Assumptions semantic correspondence
languages formalized constraints graph structure.
discussed three families constraints. Perfect matching forces correspondences English German constituents bijective. contrast, total alignments assume looser correspondence leaving constituents target side unaligned.
Edge covers occupy middle ground, assuming constituents must aligned without strictly enforcing one-to-one alignments. perfect matching linguistically implausible, assuming structural divergence languages, overcome word
alignment errors. Total alignments model structural changes therefore linguis322

fiCross-lingual Annotation Projection Semantic Roles

LingUnit
words
constituents
constituents
constituents

Similarity
binary
overlap
overlap
overlap

Correspondence
one-to-one
one-to-one
one-to-at-least-one
one-to-many

BiGraph
n/a
perfect matching
edge cover
total

Complexity
linear
cubic
cubic
quadratic

Table 4: Framework instantiations
tically appropriate, time sensitive alignment errors. Finding
optimal alignment corresponds finding optimal subgraph consistent constraints. Efficient algorithms exist problem. Finally, introduced small
number filtering techniques reduce impact alignment errors.
models properties summarized Table 4. vary along following dimensions: linguistic units employed (LingUnit), similarity measure (Similarity), assumptions semantic correspondence (Correspondence) structure
bipartite graph entails (BiGraph). mention complexity computation (Complexity). empirically assess performance following sections.

4. Experiments
describe evaluation framework developed Section 3. present two
experiments, consider projection semantic roles English sentences
onto German translations, evaluate German gold standard role annotation. Experiment 1 uses gold standard data syntactic semantic annotation.
oracle setting assesses potential role projection own, separating errors due translational divergence modeling assumptions incurred
preprocessing (e.g., parsing automatic alignment). Experiment 2 investigates
practical setting employs automatic tools syntactic semantic parsing, thus
approximating conditions large-scale role projection parallel corpora.
4.1 Setup
4.1.1 Data
models evaluated parallel corpus described Section 2. corpus
randomly shuffled split development test set (each 50% data).
Table 5 reports number tokens, sentences, frames, arguments development
test set English German.
Word alignments computed GIZA++ toolkit (Och & Ney, 2003).
used entire English-German Europarl bitext training data induce alignments
directions (source-target, target-source), default GIZA++ settings. Following
common practice Machine Translation, alignments symmetrized using intersection heuristic (Koehn, Och, & Marcu, 2003), known lead high-precision
alignments. produced manual word alignments sentences corpus,
using GIZA++ alignments starting point following Blinker annotation
guidelines (Melamed, 1998).
323

fiPado & Lapata

Language
Dev-EN
Test-EN
Dev-DE
Test-DE

Tokens
11,585
12,019
11,229
11,548

Sentences
491
496
491
496

Frames
491
496
491
496

Roles
2,423
2,465
2,576
2,747

Table 5: Statistics gold standard parallel corpus broken development (Dev)
test (Test) set.

4.1.2 Method
implemented four models shown Table 4 filtering
techniques introduced Section 3.4. resulted total sixteen models,
evaluated development set. Results best-performing models next
validated test set. found practical runtime experiment dominated
input/output XML processing rather optimization problem itself.
experiments, constituent-based models compared word-based
model, treat baseline. latter model relatively simple: projection relies
exclusively word alignments, require syntactic analysis, linear time
complexity. thus represents good point comparison models take linguistic
knowledge account.
4.1.3 Evaluation Measure
measure model performance using labeled Precision, Recall, F1 Exact
Match condition, i.e., label span projected English role
match German gold standard role count true positive. assess whether differences performance statistically significant using stratified shuffling (Noreen, 1989),
instance assumption-free approximate randomization testing (see Yeh, 2000, discussion).11 Whenever discuss changes F1, refer absolute (rather relative)
differences.
4.1.4 Upper Bound
annotation study (see Table 2, Section 2.2) obtained inter-annotator agreement
0.84 Span Match condition (annotation roles span).
number seen reasonable upper bound performance automatic
semantic role labeling system within language. difficult determine ceiling
projection task, since addition inter-annotator agreement, take
account effect bilingual divergence. annotation study provide estimate
former, latter. default method measuring bilingual agreement
spans, adopt monolingual Span Match agreement upper bound
projection experiments. Note, however, upper bound strict system
oracle able outperform it.
11. software available http://www.nlpado.de/~sebastian/sigf.html.

324

fiCross-lingual Annotation Projection Semantic Roles


Model
WordBL
PerfMatch
EdgeCover
Total
UpperBnd

Filter
Prec
52.0
75.8
71.7
68.9
85.0

Rec
46.2
57.1
61.8
61.3
83.0

F1
48.9
65.1
66.4
64.9
84.0

NA Filter
Model
Prec Rec
WordBL
52.0 46.2
PerfMatch 81.4 69.4
EdgeCover
77.9 69.3
Total
78.8 70.0
UpperBnd
85.0 83.0

F1
48.9
74.9
73.3
74.1
84.0

NC
Model
WordBL
PerfMatch
EdgeCover
Total
UpperBnd

Filter
Prec
37.1
79.4
75.0
69.7
85.0

Rec
32.0
62.2
63.0
60.1
83.0

F1
34.4
69.8
68.5
64.5
84.0

Arg Filter
Model
Prec Rec
WordBL
52.0 46.2
PerfMatch
88.8 56.2
EdgeCover 81.4 69.7
Total
81.2 69.6
UpperBnd
85.0 83.0

F1
48.9
68.8
75.1
75.0
84.0

Table 6: Model comparison development set using gold standard parses semantic
roles four filtering techniques: filtering (No Filter), removal non-aligned
words (NA Filter), removal non-content words (NC Filter), removal nonarguments (Arg Filter). Best performing models indicated boldface.

4.2 Experiment 1: Projection Gold Standard Data
Experiment 1, use manually annotated semantic roles hand-corrected syntactic
analyses constituent-based projection models. explained Section 4.1, first
discuss results development set. best model instantiations next evaluated
test set.
4.2.1 Development Set
Table 6 shows performance models (No Filter) combination
filtering techniques. Filter condition, word-based model (WordBL) yields
modest F1 48.9% application filling-the-gaps heuristic12 (see Section 3.4
details). condition, constituent-based models deliver F1 increase approximately 20% (all differences WordBL constituent-based models significant,
p < 0.01). EdgeCover model performs significantly better total alignments (Total,
p < 0.05) comparably perfect matchings (PerfMatch).
Filtering schemes generally improve resulting projections constituent-based
models. non-aligned words removed (NA Filter), F1 increases 9.8% PerfMatch, 6.9% EdgeCover, 9.2% Total. PerfMatch Total best performing models NA Filter. significantly outperform EdgeCover (p < 0.05)
condition constituent-based models Filter condition (p < 0.01).
word-based models performance remains constant. definition WordBL considers
aligned words only; thus, NA Filter impact performance.
12. Without filling-the-gaps, F1 drops 40.8%.

325

fiPado & Lapata

Moderate improvements observed constituent-based models non-content
words removed (NC filter). PerfMatch performs best condition. significantly
better PerfMatch, EdgeCover Total Filter condition (p < 0.01), significantly worse constituent-based models NA filter condition (p < 0.01).
NC filter yields significantly worse results WordBL (p < 0.01). surprising,
since word-based model cannot recover words deleted filter,
role-initial prepositions subordinating conjunctions.
Note combinations different filtering techniques applied
constituent- word-based models. example, create constituent-based model
non-aligned content words removed well non-arguments. sake
brevity, present results filter combinations generally
improve results further. find combining filters tends remove large number
words, result, good alignments removed.
Overall, obtain best performing models non-argument words removed
(Arg Filter). Arg Filter aggressive filtering technique, since removes constituents
likely occupy argument positions. EdgeCover Total significantly better
PerfMatch Arg Filter condition (p < 0.01), perform comparably PerfMatch NA Filter applied. Moreover, EdgeCover Total construct almost
identical alignments. indicates two latter models behave similarly
alignment space reduced removing many possible bad alignments, despite imposing
different constraints structure bipartite graph. Interestingly, strict correspondence constraints imposed PerfMatch result substantially different alignments.
Recall PerfMatch attempts construct bilingual one-to-one mapping arguments. direct correspondence identified source nodes, abstains
projecting. result, alignment produced PerfMatch shows highest precision
among models (88.8%), offset lowest recall (56.2%). results tie
earlier analysis constituent alignments (Section 3.3), found
one-third corpus correspondences one-to-many type. Consider
following example:
(15)

Charter means [NP opportunity bring EU closer people].
Die Charta bedeutet [NP eine Chance], [S die EU den Burgern naherzubringen].
Charter means [NP chance], [S EU citizens bring closer].

Ideally, English NP aligned German NP S. EdgeCover,
model one-to-many relationships, acts confidently aligns NP German
maximize overlap similarity, incurring precision recall error. PerfMatch,
hand, cannot handle one-to-many alignments acts cautiously makes
recall error aligning English NP empty node. Thus, according
evaluation criteria, analysis EdgeCover deemed worse PerfMatch, even
though former partly correct.
sum, filtering improves resulting projections making syntactic analyses
source target sentences similar other. Best results observed
NA Filter (PerfMatch) Arg Filter conditions (Total EdgeCover). Finally, note
best models obtain precision figures close upper bound.
326

fiCross-lingual Annotation Projection Semantic Roles

Intersective word
Model
Prec
WordBL
52.9
EdgeCover 86.6
PerfMatch
85.1
UpperBnd
85.0

alignment
Rec
F1
47.4 50.0
75.2 80.5
73.3 78.8
83.0 84.0

Manual word alignment
Model
Prec Rec
F1
WordBL
76.1 73.9 75.0
EdgeCover 86.0 81.8 83.8
PerfMatch
82.8 76.3 79.4
UpperBnd
85.0 83.0 84.0

Table 7: Model performance test set intersective manual alignments. EdgeCover uses Arg Filter PerfMatch uses NA Filter. Best performing models
indicated boldface.

best recall values around 70%, significantly upper bound 83%. Aside
wrongly assigned roles, recall errors due short semantic roles (e.g., pronouns),
intersective word alignment often contain alignment links,
projection cannot proceed.
4.2.2 Test Set
experiments test set focus models obtained best results
development set using specific filtering technique. particular, report performance
EdgeCover PerfMatch Arg Filter NA Filter conditions, respectively.
addition, assess effect automatic word alignment models using
intersective manual word alignments.
results summarized Table 7. intersective alignments used (lefthand side), EdgeCover performs numerically better PerfMatch, difference
statistically significant. corresponds findings development set.13
right-hand side shows results manually annotated word alignments used.
seen, performance WordBL increases sharply 50.0% 75.0% (F1).
underlines reliance word-based model clean word alignments. Despite
performance improvement, WordBL still lags behind best constituent-based model
approximately 9% F1. means errors made word-based model
corrected constituent-based models, mostly cases translation introduced
material target sentence cannot word-aligned expressions source
sentence recovered filling-the-gaps heuristic. example shown below,
translation clarification detailed explanation leads introduction two
German words, die naheren. words unaligned word level thus
form part role word-based projection used.
(16)

[Commissioner Barniers clarification]Role
[die naheren
Erlauterungen von Kommissar
Barnier]Role
[the detailed explanations Commissioner Barnier]Role

13. results test set slightly higher comparison development set. fluctuation
reflects natural randomness partitioning corpus.

327

fiPado & Lapata

Evaluation condition
predicates
Verbs

Prec
81.3
81.3

Rec
58.6
63.8

F1
68.1
71.5

Table 8: Evaluation Giuglea Moschittis (2004) shallow semantic parser English side parallel corpus (test set)

Constituent-based models generally profit less cleaner word alignments. performance increases 1%3% F1. improvement due higher recall (approximately
5% case EdgeCover) precision. words, main effect manually corrected word alignment make possible projection previously unavailable
roles. EdgeCover performs close human upper bound gold standard alignments
used. noise-free conditions able account one-to-many constituent alignments, thus models corpus better.
Aside alignment noise, errors observe output models
due translational divergences, problematic monolingual role assignments,
pronominal adverbs German. Many German verbs glauben (believe) exhibit
diathesis alternation: subcategorize either prepositional phrase (X glaubt Y,
X believes Y), embedded clause must preceded pronominal
adverb daran (X glaubt daran, dass Y, X believes Y). Even though pronominal
adverb forms part complement clause (and therefore role assigned it),
English counterpart. contrast example (16) above, incomplete span dass
forms complete constituent. Unless removed Arg Filter prior alignment,
error cannot corrected use constituents.
4.3 Experiment 2: Projection Automatic Roles
experiment, evaluate projection models realistic setting, using
automatic tools syntactic semantic parsing.
4.3.1 Preprocessing
experiment, use uncorrected syntactic analyses bilingual corpus
provided Collins (1997) Dubeys (2005) parsers (cf. Section 2.1). automatically
assigned semantic roles using state-of-the-art semantic parser developed Giuglea
Moschitti (2004). trained parser FrameNet corpus (release 1.2) using
standard features extended set based PropBank would
required PropBank analysis entire FrameNet corpus.
applied shallow semantic parser English side parallel corpus
obtain semantic roles, treating frames given.14 task involves locating frame
elements sentence finding correct label particular frame element. Table 8
shows evaluation parsers output test set English gold standard
annotation. Giuglea Moschitti report accuracy 85.2% role classification
14. decomposition frame-semantic parsing general practice recent role labeling tasks,
e.g. Senseval-3 (Mihalcea & Edmonds, 2004).

328

fiCross-lingual Annotation Projection Semantic Roles

Model
WordBL
PerfMatch
EdgeCover
UpperBnd

Filter
NA Filter
Arg Filter

Prec
52.5
73.0
70.0
81.3

Rec
34.5
45.4
45.1
58.6

F1
41.6
56.0
54.9
68.1

Table 9: Performance best constituent-based model test set (automatic syntactic
semantic analysis, intersective word alignment)

task, using standard feature set.15 results strictly comparable theirs,
since identify role-bearing constituents addition assigning label.
performance thus expected worse, since inherit errors frame element
identification stage. Secondly, test set differs training data vocabulary
(affecting lexical features) suffers parsing errors. Since Giuglea Moschittis
(2004) implementation handle verbs, assessed performance subset
verbal predicates (87.5% test tokens). difference complete verbs-only
data sets amounts 3.4% F1, represents unassigned roles nouns.
4.3.2 Setup
report results word-based baseline model, best projection models
Experiment 1, namely PerfMatch (NA filter) EdgeCover (Arg Filter). use
complete test set (including nouns adjectives) order make evaluation comparable Experiment 1.
4.3.3 Results
results summarized Table 9. PerfMatch (NA Filter) EdgeCover
(Arg Filter) perform comparably 5556% F1. approximately 25 points F1 worse
results obtained manual annotation (compare Table 9 Table 7). WordBLs
performance (now 41.6% F1) degrades less (around 8% F1), since affected
semantic role assignment errors. However, consistently Experiment 1, constituentbased models sill outperform WordBL 10% F1 (p < 0.01). results
underscore ability bracketing information, albeit noisy, correct extend word
alignment. Although Arg Filter performed well Experiment 1, observe less
effect here. Recall filter uses bracketing, dominance information,
therefore particularly vulnerable misparses. Experiment 1, find
models yield overall high precision low recall. Precision drops 15% F1
automatic annotations used, whereas recall drops 30%; however, note drop
includes 5% nominal roles fall outside scope shallow semantic parser.
analysis showed parsing errors form large source problems Experiment 2. German verb phrases particularly problematic. Here, relatively free
word order combines morphological ambiguity produce ambiguous structures, since
15. See work Giuglea Moschitti (2006) updated version shallow semantic parser.

329

fiPado & Lapata

Band
Error 0
Error 1
Error 2+

Prec
85.1
75.9
40.7

Rec
74.1
34.6
18.5

F1
79.2
47.6
25.4

Table 10: PerfMatchs performance relation error rate automatic semantic role
labeling (Error 0: labeling errors, Error 1: one labeling error, Error 2+: two
labeling errors).

third person plural verb forms (FIN) identical infinitival forms (INF). Consider
following English sentence, (17a), two syntactic analyses German translation,
(17b)/(17c):
(17)

a.
b.
c.

recognize [that work issue]
wenn wir erkennenFIN , [dass wir [daran arbeitenINF ] mussenFIN ]
recognize [that [work it]]
wenn wir [erkennenINF , [dass wir daran arbeitenFIN ]] mussenFIN
[recognize [we work it]]

(17b) gives correct syntactic analysis, parser used produced highly
implausible (17c). result, English sentential complement work
issue cannot aligned single German constituent, combination them.
situation, PerfMatch generally align constituent thus sacrifice
recall. EdgeCover (and Total) produce (wrong) alignment sacrifice precision.
Finally, evaluated impact semantic role labeling errors projection. split
semantic parsers output three bands: (a) sentences role labeling errors
(Error 0, 35% test set), (b) sentences one labeling error (Error 1, 33%
test set), (c) sentences two labeling errors (Error 2+, 31% test set).
Table 10 shows performance best model, PerfMatch (NA filter),
bands. seen, output automatic labeler error-free, projection
attains F1 79.2%, comparable results obtained Experiment 1.16 Even though
projection clearly degrades quality semantic role input, PerfMatch still
delivers projections high precision Error 1 band. discussed above, low
recall values bands Error 1 2+ result labelers low recall.

5. Related Work
Previous work annotation projection primarily focused annotations spanning
short linguistic units. range POS tags (Yarowsky & Ngai, 2001), NP chunks
(Yarowsky & Ngai, 2001), dependencies (Hwa et al., 2002), word senses (Bentivogli &
Pianta, 2005). different strategy cross-lingual induction frame-semantic information presented Fung Chen (2004), require parallel corpus. Instead,
16. reasonable assume sentences, least relevant part syntactic analysis
correct.

330

fiCross-lingual Annotation Projection Semantic Roles

use bilingual dictionary construct mapping FrameNet entries concepts HowNet, on-line ontology Chinese.17 second step, use HowNet
knowledge identify monolingual Chinese sentences predicates instantiate
concepts, label arguments FrameNet roles. Fung Chen report high accuracy values, method relies existence resources presumably
unavailable languages (in particular, rich ontology). Recently, Basili et al. (2009)
propose approach semantic role projection word-based employ syntactic information. Using phrase-based SMT system, heuristically assemble
target role spans phrase translations, defining phrase similarity terms translation probability. method occupies middle ground word-based projection
constituent-based projection.
work described article relies parallel corpus harnessing information
semantic correspondences. Projection works creating semantic alignments
constituents. latter correspond nodes bipartite graph, search best
alignment cast optimization problem. view computing optimal alignments
graph matching relatively widespread machine translation literature (Melamed,
2000; Matusov, Zens, & Ney, 2004; Tiedemann, 2003; Taskar, Lacoste-Julien, & Klein, 2005).
Despite individual differences, approaches formalize word alignment minimumweight matching problem, pair words bi-sentence associated
score representing desirability pair. alignment bi-sentence
highest scoring matching constraints, example matchings must oneto-one. work applies graph matching level constituents compares larger
class constraints (see Section 3.3) previous approaches. example, Taskar et al.
(2005) examine solely perfect matchings Matusov et al. (2004) edge covers.
number studies addressed constituent alignment problem context
extracting translation patterns (Kaji, Kida, & Morimoto, 1992; Imamura, 2001). However,
approaches search pairs constituents perfectly word aligned,
infeasible strategy alignments obtained automatically. work focuses
constituent alignment problem, uses greedy search techniques guaranteed
find optimal solution (Matsumoto, Ishimoto, & Utsuro, 1993; Yamamoto & Matsumoto,
2000). Meyers, Yangarber, Grishman (1996) propose algorithm aligning parse
trees applicable isomorphic structures. Unfortunately, restriction limits
application structurally similar languages high-quality parse trees.
Although evaluate models semantic role projection task, believe
show promise context SMT, especially systems use syntactic
information enhance translation quality. example, Xia McCord (2004) exploit
constituent alignment rearranging sentences source language make
word order similar target language. learn tree reordering rules aligning
constituents heuristically, using optimization procedure analogous total alignment
model presented article. similar approach described paper Collins, Koehn,
Kucerova (2005); however, rules manually specified constituent alignment step reduces inspection source-target sentence pairs. different alignment
17. information HowNet, see http://www.keenage.com/zhiwang/e_zhiwang.html.

331

fiPado & Lapata

models presented article could easily employed reordering task common
approaches.

6. Conclusions
article, argued parallel corpora show promise relieving lexical acquisition bottleneck new languages. proposed annotation projection means
obtaining FrameNet annotations automatically, using resources available English
exploiting parallel corpora. presented general framework projecting semantic
roles capitalizes use constituent information projection, modelled
computation constituent alignment search optimal subgraph
bipartite graph. formalization allows us solve search problem efficiently using
well-known graph optimization methods. experiments focused three modeling
aspects: level noise linguistic annotation, constraints alignments, noise
reduction techniques.
found constituent information yields substantial improvements word
alignments. Word-based models offer starting point low-density languages
parsers available. However, word alignments noisy fragmentary deliver
accurate projections annotations long spans semantic roles. experiments compared contrasted three constituent-based models differ
assumptions regarding cross-lingual correspondence (total alignments, edge covers, perfect matchings). Perfect matchings, restrictive alignment model enforces one-to-one
alignments, performed reliably across experimental conditions. particular,
precision surpassed models. indicates strong semantic correspondence
assumed modelling strategy, least English German parsing
tools available languages. side effect, performance constituent-based
models increases slightly manual word alignments used, means
near-optimal results obtained using automatic alignments.
far alignment noise reduction techniques concerned, find removing
non-aligned words (NA Filter) non-arguments (Arg Filter) yields best results.
filters independent language pair make weak assumptions
underlying linguistic representations question. choice best filter depends
goals projection. Removing non-aligned words relatively conservative tends
balance precision recall. contrast, aggressive filtering non-arguments
yields projections high precision low recall. Arguably, training shallow semantic
parsers target language (Johansson & Nugues, 2006), desirable
access high-quality projections. However, number options increasing
precision subsequent projection explored article. One fully automatic possibility generalization multiple occurrences predicate detect
remove suspicious projection instances, e.g. following work Dickinson Lee
(2008). Another direction postprocessing annotators, e.g., adopting annotate
automatically, correct manually methodology used provide high volume annotation
Penn Treebank project. models could used semi-supervised setting,
e.g., provide training data unknown predicates.
332

fiCross-lingual Annotation Projection Semantic Roles

extensions improvements framework presented many varied.
Firstly, believe models developed article useful semantic
role paradigms besides FrameNet, indeed types semantic annotation. Potential applications include projection PropBank roles18 , discourse structure,
named entities. mentioned earlier, models relevant machine translation
could used reordering constituents. results indicate syntactic
knowledge target side plays important role projecting annotations longer
spans. Unfortunately, many languages broad-coverage parsers available.
However, may necessary obtain complete parses semantic role projection
task. Two types syntactic information especially valuable here: bracketing information (which guides projection towards linguistically plausible role spans) knowledge
arguments sentence predicates. Bracketing information acquired
unsupervised fashion (Geertzen, 2003). Argument structure information could obtained
dependency parsers (e.g., McDonald, 2006) partial parsers able identify
predicate-argument relations (e.g., Hacioglu, 2004). Another interesting direction concerns
combination longer phrases, provided phrase-based SMT systems,
constituent information obtained output parser chunker.
experiments presented article made use simple semantic similarity
measure based word alignment. sophisticated approach could combined
alignment scores information provided bilingual dictionary. Inspired crosslingual information retrieval, Widdows, Dorow, Chan (2002) propose bilingual vector
model. underlying assumption words similar co-occurrences
parallel corpus semantically similar. Source target words represented
n-dimensional vectors whose components correspond frequent content words
source language. framework, similarity source-target word pair
computed using geometric measure cosine Euclidean distance. recent
polylingual topic models (Mimno, Wallach, Naradowsky, Smith, & McCallum, 2009) offer
probabilistic interpretation similar idea.
article, limited parallel sentences frame preserved. allows us transfer roles directly source onto target language
without acquire knowledge possible translational divergences first. generalization framework presented could adopt strategy form
mapping applied projection, akin transfer rules used machine translation.
Thus far, explored models applying identity mapping. Knowledge
possible mappings acquired manually annotated parallel corpora (Pado
& Erk, 2005). interesting avenue future work identify semantic role mappings
fully automatic fashion.
Acknowledgments
grateful three anonymous referees whose feedback helped improve
present article. Special thanks due Chris Callison-Burch linearb word alignment user interface, Ana-Maria Giuglea Alessandro Moschitti providing us
18. Note, however, cross-lingual projection PropBank roles raises question interpretation; see discussion Fung et al. (2007).

333

fiPado & Lapata

shallow semantic parser, annotators Beata Kouchnir Paloma Kreischer. acknowledge financial support DFG (Pado; grant Pi-154/9-2) EPSRC
(Lapata; grant GR/T04540/01).

References
Basili, R., Cao, D., Croce, D., Coppola, B., & Moschitti, A. (2009). Cross-language frame
semantics transfer bilingual corpora. Proceedings 10th International Conference Computational Linguistics Intelligent Text Processing, pp. 332345,
Mexico City, Mexico.
Bentivogli, L., & Pianta, E. (2005). Exploiting parallel texts creation multilingual semantically annotated resources: MultiSemCor corpus. Natural Language
Engineering, 11 (3), 247261.
Boas, H. C. (2005). Semantic frames interlingual representations multilingual lexical
databases. International Journal Lexicography, 18 (4), 445478.
Brown, P. F., Pietra, S. A. D., Pietra, V. J. D., & Mercer, R. L. (1993). Mathematics
statistical machine translation: Parameter estimation. Computational Linguistics,
19 (2), 263311.
Budanitsky, A., & Hirst, G. (2006). Evaluating WordNet-based measures lexical semantic
relatedness. Computational Linguistics, 32 (1), 1347.
Burchardt, A., Erk, K., Frank, A., Kowalski, A., Pado, S., & Pinkal, M. (2006). Consistency coverage: Challenges exhaustive semantic annotation. 28. Jahrestagung,
Deutsche Gesellschaft fur Sprachwissenschaft. Bielefeld, Germany.
Burchardt, A., Erk, K., Frank, A., Kowalski, A., Pado, S., & Pinkal, M. (2009). Framenet
semantic analysis German: Annotation, representation automation.
Boas, H. (Ed.), Multilingual FrameNet. Mouton de Gruyter. appear.
Burchardt, A., & Frank, A. (2006). Approaching textual entailment LFG
FrameNet frames. Proceedings RTE-2 Workshop, Venice, Italy.
Burnard, L. (2000). Users Reference Guide British National Corpus (World
Edition). British National Corpus Consortium, Oxford University Computing Service.
Carreras, X., & Marquez, L. (2005). Introduction CoNLL-2005 shared task: Semantic
role labeling. Proceedings 9th Conference Computational Natural Language
Learning, pp. 152164, Ann Arbor, MI.
Carreras, X., Marquez, L., & Chrupala, G. (2004). Hierarchical recognition propositional
arguments perceptrons. Proceedings Eighth Conference Computational Natural Language Learning, pp. 106109, Boston, MA.
Collins, M. (1997). Three generative, lexicalised models statistical parsing. Proceedings
35th Annual Meeting Association Computational Linguistics, pp. 16
23, Madrid, Spain.
Collins, M., Koehn, P., & Kucerova, I. (2005). Clause restructuring statistical machine translation. Proceedings 43rd Annual Meeting Association
Computational Linguistics, pp. 531540, Ann Arbor, MI.
334

fiCross-lingual Annotation Projection Semantic Roles

Cormen, T., Leiserson, C., & Rivest, R. (1990). Introduction Algorithms. MIT Press.
Diab, M., & Resnik, P. (2002). unsupervised method word sense tagging using
parallel corpora. Proceedings 40th Annual Meeting Association
Computational Linguistics, pp. 255262, Philadelphia, PA.
Dickinson, M., & Lee, C. M. (2008). Detecting errors semantic annotation. Proceedings 6th International Conference Language Resources Evaluation,
Marrakech, Morocco.
Dorr, B. (1995). Machine translation divergences: formal description proposed solution. Computational Linguistics, 20 (4), 597633.
Dowty, D. (1991). Thematic proto-roles argument selection. Language, 67, 547619.
Dubey, A. (2005). lexicalization fails: parsing German suffix analysis
smoothing. Proceedings 43rd Annual Meeting Association
Computational Linguistics, pp. 314321, Ann Arbor, MI.
Eiter, T., & Mannila, H. (1997). Distance measures point sets computation..
Acta Informatica, 34 (2), 109133.
Erk, K., Kowalski, A., Pado, S., & Pinkal, M. (2003). Towards resource lexical semantics: large German corpus extensive semantic annotation. Proceedings
41st Annual Meeting Association Computational Linguistics, pp. 537544,
Sapporo, Japan.
Fillmore, C. J. (1968). case case. Bach, & Harms (Eds.), Universals Linguistic
Theory, pp. 188. Holt, Rinehart, Winston, New York.
Fillmore, C. J. (1982). Frame semantics. Linguistics Morning Calm, pp. 111137.
Hanshin, Seoul, Korea.
Fillmore, C. J., Johnson, C. R., & Petruck, M. R. (2003). Background FrameNet. International Journal Lexicography, 16, 235250.
Fleischman, M., & Hovy, E. (2003). Maximum entropy models FrameNet classification. Proceedings 8th Conference Empirical Methods Natural Language
Processing, pp. 4956, Sapporo, Japan.
Frank, A., Krieger, H.-U., Xu, F., Uszkoreit, H., Crysmann, B., Jorg, B., & Schafer, U.
(2007). Question answering structured knowledge sources. Journal Applied
Logic, 5 (1), 2048.
Fredman, M. L., & Tarjan, R. E. (1987). Fibonacci heaps uses improved network
optimization algorithms. Journal ACM, 34 (3), 596615.
Fung, P., & Chen, B. (2004). BiFrameNet: Bilingual frame semantics resources construction
cross-lingual induction. Proceedings 20th International Conference
Computational Linguistics, pp. 931935, Geneva, Switzerland.
Fung, P., Wu, Z., Yang, Y., & Wu, D. (2006). Automatic learning Chinese-English
semantic structure mapping. Proceedings IEEE/ACL Workshop Spoken
Language Technology, Aruba.
335

fiPado & Lapata

Fung, P., Wu, Z., Yang, Y., & Wu, D. (2007). Learning bilingual semantic frames: Shallow
semantic parsing vs. semantic role projection. Proceedings 11th Conference
Theoretical Methodological Issues Machine Translation, pp. 7584, Skovde,
Sweden.
Geertzen, J. (2003). String alignment grammatical inference: suffix trees do.
Masters thesis, ILK, Tilburg University, Tilburg, Netherlands.
Gildea, D., & Jurafsky, D. (2002). Automatic labeling semantic roles. Computational
Linguistics, 28 (3), 245288.
Gimenez, J., & Marquez, L. (2007). Linguistic features automatic evaluation heterogenous MT systems. Proceedings Second Workshop Statistical Machine
Translation, pp. 256264, Prague, Czech Republic.
Giuglea, A.-M., & Moschitti, A. (2004). Knowledge discovery using FrameNet, VerbNet
PropBank. Proceedings Workshop Ontology Knowledge Discovering
15th European Conference Machine Learning, Pisa, Italy.
Giuglea, A.-M., & Moschitti, A. (2006). Semantic role labeling via FrameNet, VerbNet
PropBank. Proceedings 44th Annual Meeting Association
Computational Linguistics, pp. 929936, Sydney, Australia.
Grenager, T., & Manning, C. (2006). Unsupervised discovery statistical verb lexicon.
Proceedings 11th Conference Empirical Methods Natural Language
Processing, pp. 18, Sydney, Australia.
Hacioglu, K. (2004). lightweight semantic chunker based tagging. Proceedings
joint Human Language Technology Conference Annual Meeting North
American Chapter Association Computational Linguistics, pp. 145148,
Boston, MA.
Hi, C., & Hwa, R. (2005). backoff model bootstrapping resources non-english
languages. Proceedings joint Human Language Technology Conference
Conference Empirical Methods Natural Language Processing, pp. 851858, Vancouver, BC.
Hwa, R., Resnik, P., Weinberg, A., & Kolak, O. (2002). Evaluation translational correspondance using annotation projection. Proceedings 40th Annual Meeting
Association Computational Linguistics, pp. 392399, Philadelphia, PA.
Hwa, R., Resnik, P., Weinberg, A., Cabezas, C., & Kolak, O. (2005). Bootstrapping parsers
via syntactic projection across parallel texts. Journal Natural Language Engineering, 11 (3), 311325.
Imamura, K. (2001). Hierarchical phrase alignment harmonized parsing.. Proceedings 6th Natural Language Processing Pacific Rim Symposium, pp. 377384,
Tokyo, Japan.
Jackendoff, R. S. (1990). Semantic Structures. MIT Press, Cambridge, MA.
Johansson, R., & Nugues, P. (2006). FrameNet-Based Semantic Role Labeler Swedish.
Proceedings 44th Annual Meeting Association Computational Linguistics, pp. 436443, Sydney, Australia.
336

fiCross-lingual Annotation Projection Semantic Roles

Jonker, R., & Volgenant, A. (1987). shortest augmenting path algorithm dense
sparse linear assignment problems. Computing, 38, 325340.
Kaji, H., Kida, Y., & Morimoto, Y. (1992). Learning translation templates bilingual
text. Proceedings 14th International Conference Computational Linguistics, pp. 672678, Nantes, France.
Koehn, P., Och, F. J., & Marcu, D. (2003). Statistical phrase-based translation. Proceedings joint Human Language Technology Conference Annual Meeting
North American Chapter Association Computational Linguistics, pp.
4854, Edmonton, AL.
Koehn, P. (2005). Europarl: parallel corpus statistical machine translation. Proceedings MT Summit X, Phuket, Thailand.
Marquez, L., Carreras, X., Litkowski, K. C., & Stevenson, S. (2008). Semantic role labeling:
introduction special issue. Computational Linguistics, 34 (2), 145159.
Matsumoto, Y., Ishimoto, H., & Utsuro, T. (1993). Structural matching parallel texts.
Proceedings ACL 31st Annual Meeting Association Computational
Linguistics, pp. 2330, Columbus, OH.
Matusov, E., Zens, R., & Ney, H. (2004). Symmetric word alignments statistical maching
translation. Proceedings 20th International Conference Computational
Linguistics, pp. 219225, Geneva, Switzerland.
McDonald, R. (2006). Discriminative Training Spanning Tree Algorithms Dependency Parsing. Ph.D. thesis, University Pennsylvania.
Melamed, I. D. (1998). Manual annotation translational equivalence: Blinker project.
Tech. rep. IRCS TR #98-07, IRCS, University Pennsylvania.
Melamed, I. D. (2000). Models translational equivalence among words. Computational
Linguistics, 2 (23), 221249.
Meyers, A., Yangarber, R., & Grishman, R. (1996). Alignment shared forests bilingual corpora. Proceedings 16th International Conference Computational
Linguistics, pp. 460465, Copenhagen, Denmark.
Mihalcea, R., & Edmonds, P. (Eds.). (2004). Proceedings Senseval-3: 3rd International Workshop Evaluation Systems Semantic Analysis Text,
Barcelona, Spain.
Miltsakaki, E., Prasad, R., Joshi, A., & Webber, B. (2004). Annotating discourse connectives
arguments. Proceedings NAACL/HLT Workshop Frontiers
Corpus Annotation, Boston, MA.
Mimno, D., Wallach, H. M., Naradowsky, J., Smith, D. A., & McCallum, A. (2009). Polylingual topic models. Proceedings 14th Conference Empirical Methods
Natural Language Processing, pp. 880889, Singapore.
Moschitti, A. (2008). Kernel methods, syntax semantics relational text categorization. Proceedings 17th ACM Conference Information Knowledge
Management, pp. 253262, Napa Valley, CA.
337

fiPado & Lapata

Moschitti, A., Quarteroni, S., Basili, R., & Manandhar, S. (2007). Exploiting syntactic
shallow semantic kernels question answer classification. Proceedings
45th Annual Meeting Association Computational Linguistics, pp. 776783,
Prague, Czech Republic.
Narayanan, S., & Harabagiu, S. (2004). Question answering based semantic structures.
Proceedings 20th International Conference Computational Linguistics, pp.
693701, Geneva, Switzerland.
Noreen, E. (1989). Computer-intensive Methods Testing Hypotheses: Introduction.
John Wiley Sons Inc.
Och, F. J., & Ney, H. (2003). systematic comparison various statistical alignment
models. Computational Linguistics, 29 (1), 1952.
Ohara, K. H., Fujii, S., Saito, H., Ishizaki, S., Ohori, T., & Suzuki, R. (2003). Japanese
FrameNet project: preliminary report. Proceedings 6th Meeting
Pacific Association Computational Linguistics, pp. 249254, Halifax, Nova Scotia.
Pado, S., & Erk, K. (2005). cause cause: Cross-lingual semantic matching
paraphrase modelling. Proceedings EUROLAN Workshop Cross-Linguistic
Knowledge Induction, Cluj-Napoca, Romania.
Pado, S., & Lapata, M. (2005). Cross-lingual bootstrapping semantic lexicons.
Proceedings 22nd National Conference Artificial Intelligence, pp. 10871092,
Pittsburgh, PA.
Palmer, M., Gildea, D., & Kingsbury, P. (2005). Proposition Bank: annotated
corpus semantic roles. Computational Linguistics, 31 (1), 71106.
Postolache, O., Cristea, D., & Orasan, C. (2006). Tranferring coreference chains
word alignment. Proceedings 5th International Conference Language
Resources Evaluation, Genoa, Italy.
Riloff, E., Schafer, C., & Yarowsky, D. (2002). Inducing information extraction systems
new languages via cross-language projection. Proceedings 19th International
Conference Computational Linguistics, pp. 828834, Taipei, Taiwan.
Shen, D., & Lapata, M. (2007). Using semantic roles improve question answering.
Proceedings 12th Conference Empirical Methods Natural Language Processing, pp. 1221, Prague, Czech Republic.
Spreyer, K., & Frank, A. (2008). Projection-based acquisition temporal labeller.
Proceedings 3rd International Joint Conference Natural Language Processing,
pp. 489496, Hyderabad, India.
Subirats, C., & Petruck, M. (2003). Surprise: Spanish FrameNet. Proceedings
Workshop Frame Semantics, XVII. International Congress Linguists, Prague,
Czech Republic.
Surdeanu, M., Harabagiu, S., Williams, J., & Aarseth, P. (2003). Using predicate-argument
structures information extraction. Proceedings 41st Annual Meeting
Association Computational Linguistics, pp. 815, Sapporo, Japan.
338

fiCross-lingual Annotation Projection Semantic Roles

Swier, R. S., & Stevenson, S. (2004). Unsupervised semantic role labelling. Proceedings
Conference Empirical Methods Natural Language Processing, pp. 95102.
Bacelona, Spain.
Swier, R. S., & Stevenson, S. (2005). Exploiting verb lexicon automatic semantic
role labelling. Proceedings Joint Human Language Technology Conference
Conference Empirical Methods Natural Language Processing, pp. 883890,
Vancouver, British Columbia.
Taskar, B., Lacoste-Julien, S., & Klein, D. (2005). discriminative matching approach
word alignment. Proceedings joint Human Language Technology Conference
Conference Empirical Methods Natural Language Processing, pp. 7380,
Vancouver, BC.
Tatu, M., & Moldovan, D. (2005). semantic approach recognizing textual entailment.
Proceedings joint Human Language Technology Conference Conference
Empirical Methods Natural Language Processing, pp. 371378, Vancouver, BC.
Taule, M., Mart, M., & Recasens, M. (2008). Ancora: Multilevel annotated corpora
Catalan Spanish. Proceedings 6th International Conference Language
Resources Evaluation, Marrakesh, Morocco.
Tiedemann, J. (2003). Combining clues word alignment. Proceedings 16th
Meeting European Chapter Association Computational Linguistics,
pp. 339346, Budapest, Hungary.
Tokarczyk, A., & Frank, A. (2009). Cross-lingual projection LFG f-structures: Resource
induction Polish. Proceedings Lexical Functional Grammar 2009, Cambridge,
UK.
van Leuven-Zwart, K. M. (1989). Translation original: Similarities dissimilarities.
Target, 1 (2), 151181.
Vogel, S., Ney, H., & Tillmann, C. (1996). HMM-based word alignment statistical translation. Proceedings 16th International Conference Computational Linguistics, pp. 836841, Copenhagen.
Weeds, J. (2003). Measures Applications Lexical Distributional Similarity. Ph.D.
thesis, University Sussex.
Widdows, D., Dorow, B., & Chan, C.-K. (2002). Using parallel corpora enrich multilingual
lexical resources. Proceedings 3rd International Conference Language
Resources Evaluation, pp. 240245, Las Palmas, Canary Islands.
Wu, D., & Fung, P. (2009a). semantic role labeling improve SMT?. Proceedings
13th Annual Conference European Association Machine Translation,
pp. 218225, Barcelona, Spain.
Wu, D., & Fung, P. (2009b). Semantic roles SMT: hybrid two-pass model. Proceedings joint Human Language Technology Conference Annual Meeting
North American Chapter Association Computational Linguistics, pp.
1316, Boulder, CO.
339

fiPado & Lapata

Xia, F., & McCord, M. (2004). Improving statistical MT system automatically
learned rewrite patterns. Proceedings 20th International Conference
Computational Linguistics, pp. 508514, Geneva, Switzerland.
Xue, N., & Palmer, M. (2004). Calibrating features semantic role labeling. Proceedings
9th Conference Empirical Methods Natural Language Processing, pp. 88
94, Barcelona, Spain.
Xue, N., & Palmer, M. (2009). Adding semantic roles Chinese treebank. Natural
Language Engineering, 15 (1), 143172.
Yamamoto, K., & Matsumoto, Y. (2000). Acquisition phrase-level bilingual correspondence using dependency structure. Proceedings 18th International Conference
Computational Linguistics, pp. 933939, Saarbrucken, Germany.
Yarowsky, D., & Ngai, G. (2001). Inducing multilingual POS taggers NP bracketers via
robust projection across aligned corpora. Proceedings 2nd Annual Meeting
North American Chapter Association Computational Linguistics, pp.
200207, Pittsburgh, PA.
Yarowsky, D., Ngai, G., & Wicentowski, R. (2001). Inducing multilingual text analysis
tools via robust projection across aligned corpora. Proceedings 1st Human
Language Technology Conference, pp. 161168, San Francisco, CA.
Yeh, A. (2000). accurate tests statistical significance result differences.
Proceedings 18th International Conference Computational Linguistics, pp.
947953, Saarbrucken, Germany.

340


