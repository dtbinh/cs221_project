Journal Artificial Intelligence Research 36 (2009) 129163

Submitted 04/09; published 10/09

Content Modeling Using Latent Permutations
Harr Chen
S.R.K. Branavan
Regina Barzilay
David R. Karger

harr@csail.mit.edu
branavan@csail.mit.edu
regina@csail.mit.edu
karger@csail.mit.edu

Computer Science Artificial Intelligence Laboratory
Massachusetts Institute Technology
32 Vassar Street, Cambridge, Massachusetts 02139 USA

Abstract
present novel Bayesian topic model learning discourse-level document structure. model leverages insights discourse theory constrain latent topic assignments way reflects underlying organization document topics. propose
global model topic selection ordering biased similar across
collection related documents. show space orderings effectively represented using distribution permutations called Generalized Mallows Model.
apply method three complementary discourse-level tasks: cross-document alignment,
document segmentation, information ordering. experiments show incorporating permutation-based model applications yields substantial improvements
performance previously proposed methods.

1. Introduction
central problem discourse analysis modeling content structure document.
structure encompasses topics addressed order topics
appear across documents single domain. Modeling content structure particularly
germane domains exhibit recurrent patterns content organization, news
encyclopedia articles. models aim induce, example, articles
cities typically contain information History, Economy, Transportation,
descriptions History usually precede Transportation.
Previous work (Barzilay & Lee, 2004; Elsner, Austerweil, & Charniak, 2007) demonstrated content models learned raw unannotated text, useful
variety text processing tasks summarization information ordering. However, expressive power approaches limited: taking Markovian view
content structure, model local constraints topic organization. shortcoming substantial since many discourse constraints described literature global
nature (Graesser, Gernsbacher, & Goldman, 2003; Schiffrin, Tannen, & Hamilton, 2001).
paper, introduce model content structure explicitly represents two
important global constraints topic selection.1 first constraint posits document follows progression coherent, nonrecurring topics (Halliday & Hasan, 1976).
Following example above, constraint captures notion single topic,
1. Throughout paper, use topic refer interchangeably discourse unit
language model views topic.

c
2009
AI Access Foundation. rights reserved.

fiChen, Branavan, Barzilay, & Karger

History, expressed contiguous block within document, rather spread
disconnected sections. second constraint states documents domain
tend present similar topics similar orders (Bartlett, 1932; Wray, 2002). constraint
guides toward selecting sequences similar topic ordering, placing History Transportation. constraints universal across genres human
discourse, applicable many important domains, ranging newspaper text
product reviews.2
present latent topic model related documents encodes discourse
constraints positing single distribution entirety documents content ordering. Specifically, represent content structure permutation topics.
naturally enforces first constraint since permutation allow topic repetition.
learn distribution permutations, employ Generalized Mallows Model
(GMM). model concentrates probability mass permutations close canonical
permutation. Permutations drawn distribution likely similar, conforming second constraint. major benefit GMM compact parameterization
using set real-valued dispersion values. dispersion parameters allow model
learn strongly bias documents topic ordering toward canonical permutation. Furthermore, number parameters grows linearly number topics,
thus sidestepping tractability problems typically associated large discrete space
permutations.
position GMM within larger hierarchical Bayesian model explains
set related documents generated. document, model posits topic
ordering drawn GMM, set topic frequencies drawn multinomial distribution. Together, draws specify documents entire topic structure,
form topic assignments textual unit. traditional topic models, words
drawn language models indexed topic. estimate model posterior,
perform Gibbs sampling topic structures GMM dispersion parameters
analytically integrating remaining hidden variables.
apply model three complex document-level tasks. First, alignment
task, aim discover paragraphs across different documents share topic.
experiments, permutation-based model outperforms Hidden Topic Markov
Model (Gruber, Rosen-Zvi, & Weiss, 2007) wide margin gap averaged 28% percentage points F-score. Second, consider segmentation task, goal
partition document sequence topically coherent segments. model yields
average Pk measure 0.231, 7.9% percentage point improvement competitive
Bayesian segmentation method take global constraints account (Eisenstein & Barzilay, 2008). Third, apply model ordering task, is, sequencing
held set textual units coherent document. previous two applications, difference model state-of-the-art baseline substantial:
model achieves average Kendalls 0.602, compared value 0.267
HMM-based content model (Barzilay & Lee, 2004).
success permutation-based model three complementary tasks demonstrates flexibility effectiveness, attests versatility general document
2. example domain first constraint violated dialogue. Texts domains follow
stack structure, allowing topics recur throughout conversation (Grosz & Sidner, 1986).

130

fiContent Modeling Using Latent Permutations

structure induced model. find encoding global ordering constraints
topic models makes suitable discourse-level analysis, contrast local
decision approaches taken previous work. Furthermore, evaluation scenarios, full model yields significantly better results simpler variants either
use fixed ordering order-agnostic.
remainder paper proceeds follows. Section 2, describe approach relates previous work topic modeling statistical discourse processing.
provide problem formulation Section 3.1 followed overview content
model Section 3.2. heart model distribution topic permutations,
provide background Section 3.3, employing formal description
models probabilistic generative story Section 3.4. Section 4 discusses estimation models posterior distribution given example documents using collapsed Gibbs
sampling procedure. Techniques applying model three tasks alignment,
segmentation, ordering explained Section 5. evaluate models performance tasks Section 6 concluding touching upon directions
future work Section 7. Code, data sets, annotations, raw outputs
experiments available http://groups.csail.mit.edu/rbg/code/mallows/.

2. Related Work
describe two areas previous work related approach. algorithmic
perspective work falls broad class topic models. earlier work topic
modeling took bag words view documents, many recent approaches expanded
topic models capture structural constraints. Section 2.1, describe extensions highlight differences model. linguistic side, work
relates research modeling text structure statistical discourse processing. summarize work Section 2.2, drawing comparisons functionality supported
model.
2.1 Topic Models
Probabilistic topic models, originally developed context language modeling,
today become popular range NLP applications, text classification document browsing. Topic models posit latent state variable controls generation
word. parameters estimated using approximate inference techniques
Gibbs sampling variational methods. traditional topic models Latent Dirichlet Allocation (LDA) (Blei, Ng, & Jordan, 2003; Griffiths & Steyvers, 2004), documents
treated bags words, word receives separate topic assignment words
assigned topic drawn shared language model.
bag words representation sufficient applications, many cases
structure-unaware view limited. Previous research considered extensions
LDA models two orthogonal directions, covering intrasentential extrasentential
constraints.

131

fiChen, Branavan, Barzilay, & Karger

2.1.1 Modeling Intrasentential Constraints
One promising direction improving topic models augment constraints
topic assignments adjoining words within sentences. example, Griffiths, Steyvers,
Blei, Tenenbaum (2005) propose model jointly incorporates syntactic
semantic information unified generative framework constrains syntactic classes
adjacent words. approach, generation word controlled two hidden
variables, one specifying semantic topic specifying syntactic class.
syntactic class hidden variables chained together Markov model, whereas semantic
topic assignments assumed independent every word.
another example intrasentential constraints, Wallach (2006) proposes way
incorporate word order information, form bigrams, LDA-style model.
approach, generation word conditioned previous word
topic current word, word topics generated perdocument topic distributions LDA. formulation models text structure
level word transitions, opposed work Griffiths et al. (2005) structure
modeled level hidden syntactic class transitions.
focus modeling high-level document structure terms semantic content.
such, work complementary methods impose structure intrasentential
units; possible combine model constraints adjoining words.
2.1.2 Modeling Extrasentential Constraints
Given intuitive connection notion topic LDA notion topic
discourse analysis, natural assume LDA-like models useful discourselevel tasks segmentation topic classification. hypothesis motivated research
models topic assignment guided structural considerations (Purver, Kording,
Griffiths, & Tenenbaum, 2006; Gruber et al., 2007; Titov & McDonald, 2008), particularly
relationships topics adjacent textual units. Depending application,
textual unit may sentence, paragraph, speaker utterance. common property
models bias topic assignments cohere within local segments text.
Models category vary terms mechanisms used encourage local topic
coherence. instance, model Purver et al. (2006) biases topic distributions
adjacent utterances (textual units) similar. model generates utterance
mixture topic language models. parameters topic mixture distribution assumed follow type Markovian transition process specifically, high
probability utterance u topic distribution previous utterance
u 1; otherwise, new topic distribution drawn u. Thus, textual units topic
distribution depends previous textual unit, controlled parameter indicating
whether new topic distribution drawn.
similar vein, Hidden Topic Markov Model (HTMM) (Gruber et al., 2007) posits
generative process sentence (textual unit) assigned single topic,
sentences words drawn single language model. model
Purver et al., topic transitions adjacent textual units modeled Markovian
fashion specifically, sentence topic sentence 1 high probability,
receives new topic assignment drawn shared topic multinomial distribution.

132

fiContent Modeling Using Latent Permutations

HTMM model, assumption single topic per textual unit allows
sections text related across documents topic. contrast, Purver et al.s model
tailored task segmentation, utterance drawn mixture topics.
Thus, model capture utterances topically aligned across related
documents. importantly, HTMM model Purver et al. able
make local decisions regarding topic transitions, thus difficulty respecting longrange discourse constraints topic contiguity. model instead takes global view
topic assignments textual units explicitly generating entire documents topic
ordering one joint distribution. show later paper, global view yields
significant performance gains.
recent Multi-Grain Latent Dirichlet Allocation (MGLDA) model (Titov & McDonald, 2008) studied topic assignments level sub-document textual units.
MGLDA, set local topic distributions induced sentence, dependent
window local context around sentence. Individual words drawn either
local topics document-level topics standard LDA. MGLDA represents
local context using sliding window, window frame comprises overlapping short
spans sentences. way, local topic distributions shared sentences
close proximity.
MGLDA represent complex topical dependencies models Purver
et al. Gruber et al., window incorporate much wider swath local
context two adjacent textual units. However, MGLDA unable encode longer
range constraints, contiguity ordering similarity, sentences close
proximity loosely connected series intervening window frames.
contrast, work specifically oriented toward long-range constraints, necessitating
whole-document notion topic assignment.
2.2 Modeling Ordering Constraints Statistical Discourse Analysis
global constraints encoded model closely related research discourse
information ordering applications text summarization generation (Barzilay,
Elhadad, & McKeown, 2002; Lapata, 2003; Karamanis, Poesio, Mellish, & Oberlander, 2004;
Elsner et al., 2007). emphasis body work learning ordering constraints
data, goal reordering new text domain. methods build
assumption recurring patterns topic ordering discovered analyzing
patterns word distribution. key distinction prior methods approach
existing ordering models largely driven local constraints limited ability
capture global structure. Below, describe two main classes probabilistic ordering
models studied discourse processing.
2.2.1 Discriminative Models
Discriminative approaches aim directly predict ordering given set sentences.
Modeling ordering sentences simultaneously leads complex structure prediction
problem. practice, however, computationally tractable two-step approach taken:
first, probabilistic models used estimate pairwise sentence ordering preferences; next,
local decisions combined produce consistent global ordering (Lapata, 2003;

133

fiChen, Branavan, Barzilay, & Karger

Althaus, Karamanis, & Koller, 2004). Training data pairwise models constructed
considering pairs sentences document, supervision labels based
actually ordered. Prior work demonstrated wide range features
useful classification decisions (Lapata, 2003; Karamanis et al., 2004; Ji &
Pulman, 2006; Bollegala, Okazaki, & Ishizuka, 2006). instance, Lapata (2003)
demonstrated lexical features, verb pairs input sentences, serve
proxy plausible sequences actions, thus effective predictors well-formed
orderings. second stage, local decisions integrated global order
maximizes number consistent pairwise classifications. Since finding
ordering NP-hard (Cohen, Schapire, & Singer, 1999), various approximations used
practice (Lapata, 2003; Althaus et al., 2004).
two-step discriminative approaches effectively leverage information
local transitions, provide means representing global constraints.
recent work, Barzilay Lapata (2008) demonstrated certain global properties captured discriminative framework using reranking mechanism.
set-up, system learns identify best global ordering given set n possible
candidate orderings. accuracy ranking approach greatly depends quality
selected candidates. Identifying candidates challenging task given large
search space possible alternatives.
approach presented work differs existing discriminative models two
ways. First, model represents distribution possible global orderings. Thus,
use sampling mechanisms consider whole space rather limited
subset candidates ranking models. second difference arises
generative nature model. Rather focusing ordering task, order-aware
model effectively captures layer hidden variables explain underlying structure
document content. Thus, effectively applied wider variety applications,
including sentence ordering already observed, appropriately adjusting
observed hidden components model.
2.2.2 Generative Models
work closer technique generative models treat topics hidden variables.
One instance work Hidden Markov Model (HMM)-based content model (Barzilay & Lee, 2004). model, states correspond topics state transitions represent
ordering preferences; hidden states emission distribution language model
words. Thus, similar approach, models implicitly represent patterns
level topical structure. HMM used ranking framework select
ordering highest probability.
recent work, Elsner et al. (2007) developed search procedure based simulated annealing finds high likelihood ordering. contrast ranking-based approaches, search procedure cover entire ordering space. hand,
show Section 5.3, define ordering objective maximized
efficiently possible orderings prediction model parameters
learned. Specifically, bag p paragraphs, O(pK) calculations paragraph
probabilities necessary, K number topics.

134

fiContent Modeling Using Latent Permutations

Another distinction proposed model prior work way global
ordering constraints encoded. Markovian model, possible induce global
constraints introducing additional local constraints. instance, topic contiguity
enforced selecting appropriate model topology (e.g., augmenting hidden states
record previously visited states). However, global constraints, similarity
overall ordering across documents, much challenging represent. explicitly
modeling topic permutation distribution, easily capture kind global
constraint, ultimately resulting accurate topic models orderings. show
later paper, model substantially outperforms approach Barzilay Lee
information ordering task applied HMM-based content model.

3. Model
section, describe problem formulation proposed model.
3.1 Problem Formulation
content modeling problem formalized follows. take input corpus
{d1 , . . . dD } related documents, specification number topics K.3
document comprised ordered sequence Nd paragraphs (pd,1 , . . . , pd,Nd ).
output, predict single topic assignment zd,p {1, . . . , K} paragraph p.4
z values reflect underlying content organization document
related content discussed within document, across separate documents,
receive z value.
formulation shares similarity standard LDA setup common
set topics assigned across collection documents. difference LDA
words topic assignment conditionally independent, following bag words view
documents, whereas constraints topics assigned let us connect word
distributional patterns document-level topic structure.
3.2 Model Overview
propose generative Bayesian model explains corpus documents
produced set hidden variables. high level, model first selects
frequently topic expressed document, topics ordered.
topics determine selection words paragraph. Notation used
subsequent sections summarized Figure 1.
document Nd paragraphs, separately generate bag topics td
topic ordering . unordered bag topics td , contains Nd elements, expresses
many paragraphs document assigned K topics. Equivalently,
td viewed vector occurrence counts topic, zero counts
topics appear all. Variable td constructed taking Nd samples
3. nonparametric extension model would learn K.
4. well structured documents, paragraphs tend internally topically consistent (Halliday & Hasan,
1976), predicting one topic per paragraph sufficient. However, note approach
applied modifications levels textual granularity sentences.

135

fiChen, Branavan, Barzilay, & Karger



parameters distribution
topic counts



parameters distribution
topic orderings



vector topic counts

v

vector inversion counts



topic ordering

z

paragraph topic assignment



language model parameters
topic

Dirichlet(0 )
j = 1 . . . K 1:
j GMM0 (0 , 0 )
k = 1 . . . K:
k Dirichlet(0 )

td
vd
=
zd =

w document words

document
Multinomial()
GMM()
Compute-(vd )
Compute-z(td , )

paragraph p
word w p
w Multinomial(zd,p )

K number topics
number documents corpus
Nd number paragraphs
document
Np number words paragraph p

Algorithm: Compute-
Input: Inversion count vector v

Algorithm: Compute-z
Input: Topic counts t, permutation

Create empty list
[1] K
j = K 1 1
= K 1 v[j]
[i + 1] [i]
[v[j]] j

Create empty list z
end 1
k = K 1
= 1 t[[k]]
z[end] [k]
end end + 1

Output: Permutation

Output: Paragraph topic vector z

Figure 1: plate diagram generative process model, along table
notation reference purposes. Shaded circles figure denote observed
variables, squares denote hyperparameters. dotted arrows indicate
constructed deterministically v according algorithm Compute-,
z constructed deterministically according Compute-z.
136

fiContent Modeling Using Latent Permutations

distribution topics , multinomial representing probability topic
expressed. Sharing documents captures notion certain topics
likely across documents corpus.
topic ordering variable permutation numbers 1 K
defines order topics appear document. draw Generalized
Mallows Model, distribution permutations explain Section 3.3.
see, particular distribution biases permutation selection close single
centroid, reflecting discourse constraint preferring similar topic structures across
documents.
Together, documents bag topics td ordering determine topic assignment
zd,p paragraphs. example, corpus K = 4, seven-paragraph
document td = {1, 1, 1, 1, 2, 4, 4} = (2, 4, 3, 1) would induce topic sequence
zd = (2, 4, 4, 1, 1, 1, 1). induced topic sequence zd never assign topic
two unconnected portions document, thus satisfying constraint topic contiguity.
assume topic k associated language model k . words
paragraph assigned topic k drawn topics language model k .
portion similar standard LDA topic relates language model.
However, unlike LDA, model enforces topic coherence entire paragraph rather
viewing paragraph mixture topics.
turning formal discussion generative process, first provide
background permutation model topic ordering.
3.3 Generalized Mallows Model Permutations
central challenge approach presented modeling distribution possible topic orderings. purpose use Generalized Mallows Model (GMM) (Fligner
& Verducci, 1986; Lebanon & Lafferty, 2002; Meila, Phadnis, Patterson, & Bilmes, 2007;
Klementiev, Roth, & Small, 2008), exhibits two appealing properties context
task. First, model concentrates probability mass canonical ordering
small perturbations (permutations) ordering. characteristic matches
constraint documents domain exhibit structural similarity. Second,
parameter set scales linearly number elements ordered, making
sufficiently constrained tractable inference.
first describe standard Mallows Model orderings (Mallows, 1957).
Mallows Model takes two parameters, canonical ordering dispersion parameter .
sets probability ordering proportional ed(,) ,
d(, ) represents distance metric orderings . Frequently, metric
Kendall distance, minimum number swaps adjacent elements needed
transform ordering canonical ordering . Thus, orderings close
canonical ordering high probability, many elements
moved less probability mass.
Generalized Mallows Model, first introduced Fligner Verducci (1986), refines
standard Mallows Model adding additional set dispersion parameters.
parameters break apart distance d(, ) orderings set independent
components. component separately vary sensitivity perturbation.

137

fiChen, Branavan, Barzilay, & Karger

tease apart distance function components, GMM distribution considers
inversions required transform canonical ordering observed ordering. first
discuss inversions parameterized GMM, turn distributions
definition characteristics.
3.3.1 Inversion Representation Permutations
Typically, permutations represented directly ordered sequence elements
example, (3, 1, 2) represents permuting initial order placing third element
first, followed first element, second. GMM utilizes alternative
permutation representation defined vector (v1 , . . . , vK1 ) inversion counts
respect identity permutation (1, . . . , K). Term vj counts number times
value greater j appears j permutation. Note jth inversion
count vj take integer values 0 K j inclusive. Thus inversion count
vector K 1 elements, vK always zero. instance, given standard
form permutation (3, 1, 5, 6, 2, 4), v2 = 3 3, 5, 6 appear 2, v3 = 0
numbers appear it; entire inversion count vector would (1, 3, 0, 2, 0).
Likewise, previous example permutation (2, 4, 3, 1) maps inversion counts (3, 0, 1).
sum components entire inversion count vector simply orderings
Kendall distance canonical ordering.
significant appeal inversion representation every valid, distinct vector
inversion counts corresponds distinct permutation vice versa. see this,
note permutation straightforwardly compute inversion counts.
Conversely, given sequence inversion counts, construct unique corresponding
permutation. insert items permutation, working backwards item K.
Assume already placed items j + 1 K proper order. insert
item j, note exactly vj items j + 1 K must precede it, meaning
must inserted position vj current order (see Compute- algorithm
Figure 1). Since one place j inserted fulfills inversion
counts, induction shows exactly one permutation constructed satisfy
given inversion counts.
model, take canonical topic ordering always identity ordering
(1, . . . , K). topic numbers task completely symmetric linked
extrinsic meaning, fixing global ordering specific arbitrary value
sacrifice representational power. general case GMM, canonical ordering
parameter distribution.
3.3.2 Probability Mass Function
GMM assigns probability mass particular order based order permuted canonical ordering. precisely, associates distance every
permutation, canonical ordering distance zero permutations many
inversions respect canonical ordering larger distance. distance assignment based K 1 real-valued dispersion parameters P
(1 , . . . , K1 ). distance
permutation inversion counts v defined j j vj . GMMs probability

138

fiContent Modeling Using Latent Permutations

mass function exponential distance:
P

e j j vj
GMM(v; ) =
()
=

K1

j=1

() =

Q

j

ej vj
,
j (j )

(1)

j (j ) normalization factor value:
j (j ) =

1 e(Kj+1)j
.
1 ej

(2)

Setting j equal single value recovers standard Mallows Model Kendall
distance function. factorization GMM independent probabilities per
inversion count makes distribution particularly easy apply; use GMMj refer
jth multiplicand probability mass function, marginal distribution
vj :
GMMj (vj ; j ) =

ej vj
.
j (j )

(3)

Due exponential form distribution, requiring j > 0 constrains GMM
assign highest probability mass vj zero, i.e., distributional mode
canonical identity permutation. higher value j assigns probability mass vj
close zero, biasing j fewer inversions.
3.3.3 Conjugate Prior
major benefit GMM membership exponential family distributions;
means particularly amenable Bayesian representation, admits
natural independent conjugate prior parameter j (Fligner & Verducci, 1990):
GMM0 (j | vj,0 , 0 ) e(j vj,0 log j (j ))0 .

(4)

prior distribution takes two parameters 0 vj,0 . Intuitively, prior states
0 previous trials, total number inversions observed 0 vj,0 . distribution
easily updated observed vj derive posterior distribution.
vj different range, inconvenient set prior hyperparameters
vj,0 directly. work, instead assign common prior value parameter j ,
denote 0 . set vj,0 maximum likelihood estimate
j 0 . differentiating likelihood GMM respect j , straightforward
verify works setting:
vj,0 =

e0

K j+1
1
(Kj+1)
.
0 1
1 e

139

(5)

fiChen, Branavan, Barzilay, & Karger

3.4 Formal Generative Process
fully specify details content model, whose plate diagram appears
Figure 1. observe corpus documents, document ordered
sequence Nd paragraphs paragraph represented bag words. number
topics K assumed pre-specified. model induces set hidden variables
probabilistically explain words corpus produced. final desired
output posterior distributions paragraphs hidden topic assignment variables.
following, variables subscripted 0 fixed prior hyperparameters.
1. topic k, draw language model k Dirichlet(0 ). LDA,
topic-specific word distributions.
2. Draw topic distribution Dirichlet(0 ), expresses likely topic
appear regardless position.
3. Draw topic ordering distribution parameters j GMM0 (0 , 0 ) j = 1
K 1. parameters control rapidly probability mass decays
inversions topic. separate j every topic allows us learn
topics likely reordered others.
4. document Nd paragraphs:
(a) Draw bag topics td sampling Nd times Multinomial().
(b) Draw topic ordering , sampling vector inversion counts vd GMM(),
applying algorithm Compute- Figure 1 vd .
(c) Compute vector topic assignments zd document ds paragraphs
sorting td according , algorithm Compute-z Figure 1.5
(d) paragraph p document d:
i. Sample word w p according language model p: w
Multinomial(zd,p ).
3.5 Properties Model
section describe rationale behind using GMM represent ordering
component content model.
Representational Power GMM concentrates probability mass around one centroid permutation, reflecting preferred bias toward document structures similar topic orderings. Furthermore, parameterization GMM using vector
dispersion parameters allows flexibility strongly model biases toward
single ordering one extreme ( = ) one ordering nonzero probability, ( = 0) orderings equally likely. comprised
5. Multiple permutations contribute probability single documents topic assignments zd ,
topics appear td . result, current formulation biased toward
assignments fewer topics per document. practice, find negatively impact model
performance.

140

fiContent Modeling Using Latent Permutations

independent dispersion parameters (1 , . . . , K1 ), distribution assign different penalties displacing different topics. example, may learn middle
sections (in case Cities, sections Economy Culture) likely
vary position across documents early sections (such Introduction
History).
Computational Benefits parameterization GMM using vector dispersion parameters compact tractable. Since number parameters grows
linearly number topics, model efficiently handle longer documents
greater diversity content.
Another computational advantage model seamless integration larger
Bayesian model. Due membership exponential family existence
conjugate prior, inference become significantly complex
GMM used hierarchical context. case, entire document generative
model accounts topic frequency words within topic.
One final beneficial effect GMM breaks symmetry topic assignments fixing distribution centroid. Specifically, topic assignments
invariant relabeling, probability underlying permutation would
change. contrast, many topic models assign probability relabeling
topic assignments. model thus sidesteps problem topic identifiability, issue model may multiple maxima likelihood due
underlying symmetry hidden variables. Non-identifiable models
standard LDA may cause sampling procedures jump maxima produce
draws difficult aggregate across runs.
Finally, show Section 6 benefits GMM extend theoretical empirical: representing permutations using GMM almost always leads
superior performance compared alternative approaches.

4. Inference
variables aim infer paragraph topic assignments z, determined bag topics ordering document. Thus, goal estimate
joint marginal distributions given document text integrating
remaining hidden parameters:
P (t, , | w).
(6)
accomplish inference task Gibbs sampling (Geman & Geman, 1984; Bishop,
2006). Gibbs sampler builds Markov chain hidden variable state space whose
stationary distribution actual posterior joint distribution. new sample
drawn distribution single variable conditioned previous samples
variables. collapse sampler integrating hidden
variables model, effect reducing state space Markov chain. Collapsed
sampling previously demonstrated effective LDA variants (Griffiths
& Steyvers, 2004; Porteous, Newman, Ihler, Asuncion, Smyth, & Welling, 2008; Titov &

141

fiChen, Branavan, Barzilay, & Karger

P (td,i = | . . .) P (td,i = | t(d,i) , 0 ) P (wd | td , , wd , zd , 0 )


N (t(d,i) , t) + 0

P (wd | z, wd , 0 ),
|t(d,i) | + K0

P (vd,j = v | . . .) P (vd,j = v | j ) P (wd | td , , wd , zd , 0 )
= GMMj (v; j ) P (wd | z, wd , 0 ),

P


vd,j + vj,0 0
P (j | . . .) = GMM0 j ;
, N + 0 ,
N + 0
Figure 2: collapsed Gibbs sampling inference procedure estimating models
posterior distribution. plate diagram, variable resampled
shown double circle Markov blanket highlighted black;
variables, impact variable resampled, grayed out.
Variables , shown dotted circles, never explicitly depended
re-estimated, marginalized sampler. diagram
accompanied conditional resampling distribution respective variable.

142

fiContent Modeling Using Latent Permutations

McDonald, 2008). typically preferred explicit Gibbs sampling hidden
variables smaller search space generally shorter mixing time.
sampler analytically integrates three sets hidden variables: bags
topics t, orderings , permutation inversion parameters . burn-in period,
treat last samples draw posterior. samples
marginalized variables necessary, estimated based topic
assignments show Section 5.3. Figure 2 summarizes Gibbs sampling steps
inference procedure.
Document Probability preliminary step, consider calculate probability
single documents words wd given documents paragraph topic assignments zd
remaining documents topic assignments. Note probability decomposable product probabilities individual paragraphs paragraphs
different topics conditionally independent word probabilities. Let wd zd indicate words topic assignments documents d, W vocabulary
size. probability words then:
K Z

P (wd | z, wd , 0 ) =
P (wd | zd , k ) P (k | z, wd , 0 ) dk

=

k=1 k
K


DCM({wd,i : zd,i = k} | {wd,i : zd,i = k}, 0 ),

(7)

k=1

DCM() refers Dirichlet compound multinomial distribution, result
integrating multinomial parameters Dirichlet prior (Bernardo & Smith, 2000).
Dirichlet prior parameters = (1 , . . . , W ), DCM assigns following
probability series observations x = {x1 , . . . , xn }:
P
W
( j j )
(N (x, i) + )
P
DCM(x; ) = Q
,
(8)
(|x| + j j )
j (j )
i=1

N (x, i) refers number times word appears x. Here, () Gamma
function, generalization factorial real numbers. algebra shows
DCMs posterior probability density function conditioned series observations =
{y1 , . . . , yn } computed updating counts often word appears
y:
DCM(x | y, ) = DCM(x; 1 + N (y, 1), . . . , W + N (y, W )).

(9)

Equations 7 9 used compute conditional distributions hidden
variables. turn individual random variable resampled.
Bag Topics First consider resample td,i , ith topic draw document
conditioned parameters fixed (note topic ith
paragraph, reorder topics using , generated separately):
P (td,i = | . . .) P (td,i = | t(d,i) , 0 ) P (wd | td , , wd , zd , 0 )


N (t(d,i) , t) + 0

P (wd | z, wd , 0 ),
|t(d,i) | + K0
143

(10)

fiChen, Branavan, Barzilay, & Karger

td updated reflect td,i = t, zd deterministically computed last step
using Compute-z Figure 1 inputs td . first step reflects application
Bayes rule factor term wd ; drop superfluous terms
conditioning. second step, former term arises DCM, updating
parameters 0 observations t(d,i) Equation 9 dropping constants.
latter document probability term computed using Equation 7. new td,i selected
sampling probability computed possible topic assignments.
Ordering parameterization permutation series inversion values vd,j
reveals natural way decompose search space Gibbs sampling. document
d, resample vd,j j = 1 K 1 independently successively according
conditional distribution:
P (vd,j = v | . . .) P (vd,j = v | j ) P (wd | td , , wd , zd , 0 )
= GMMj (v; j ) P (wd | z, wd , 0 ),

(11)

updated reflect vd,j = v, zd computed deterministically according
td . first term refers Equation 3; second computed using Equation 7.
probability computed every possible value v, ranges 0 K j,
term vd,j sampled according resulting probabilities.
GMM Parameters j = 1 K 1, resample j posterior distribution:
P


vd,j + vj,0 0
P (j | . . .) = GMM0 j ;
, N + 0 ,
(12)
N + 0
GMM0 evaluated according Equation 4. normalization constant
distribution unknown, meaning cannot directly compute invert cumulative distribution function sample distribution. However, distribution
univariate unimodal, expect MCMC technique slice
sampling (Neal, 2003) perform well. practice, Matlabs built-in slice sampler
provides robust draw distribution.6
Computational Issues inference, directly computing document probabilities
basis Equation 7 results many redundant calculations slow runtime
iteration considerably. improve computational performance proposed
inference procedure, apply memoization techniques sampling. Within
single iteration, document, Gibbs sampler requires computing documents
probability given topic assignments (Equation 7) many times, computation
frequently conditions slight variations topic assignments. nave approach
would compute probability every paragraph time document probability
desired, performing redundant calculations topic assignment sequences shared
subsequences repeatedly considered.
Instead, use lazy evaluation build three-dimensional cache, indexed tuple
(i, j, k), follows. time document probability requested, broken independent subspans paragraphs, subspan takes one contiguous topic assignment. possible due way Equation 7 factorizes independent per-topic
6. particular, use slicesample function Matlab Statistics Toolbox.

144

fiContent Modeling Using Latent Permutations

multiplicands. subspan starting paragraph i, ending paragraph j, assigned topic k, cache consulted using key (i, j, k). example, topic assignments
zd = (2, 4, 4, 1, 1, 1, 1) would result cache lookups (1, 1, 2), (2, 3, 4), (4, 7, 1).
cached value unavailable, correct probability computed using Equation 7
result stored cache location (i, j, k). Moreover, record values every
intermediate cache location (i, l, k) l = j 1, values computed
subproblems evaluating Equation 7 (i, j, k). cache reset proceeding
next document since conditioning changes documents. document,
caching guarantees O(Nd2 K) paragraph probability calculations.
practice, individual Gibbs steps small, bound loose
caching mechanism reduces computation time several orders magnitude.
maintain caches word-topic paragraph-topic assignment frequencies,
allowing us rapidly compute counts used equations 7 10. form caching
used Griffiths Steyvers (2004).

5. Applications
section, describe model applied three challenging discourselevel tasks: aligning paragraphs similar topical content documents, segmenting
document topically cohesive sections, ordering new unseen paragraphs
coherent document. particular, show posterior samples produced
inference procedure Section 4 used derive solution tasks.
5.1 Alignment
alignment task wish find paragraphs document topically
relate paragraphs documents. Essentially, cross-document clustering
task alignment assigns paragraph document one K topically related
groupings. instance, given set cell phone reviews, one group may represent text
fragments discuss Price, another group consists fragments Reception.
model readily employed task: view topic assignment
paragraph z cluster label. example, two documents d1 d2
topic assignments zd1 = (2, 4, 4, 1, 1, 1, 1) zd2 = (4, 4, 3, 3, 2, 2, 2), paragraph 1 d1
grouped together paragraphs 5 7 d2 , paragraphs 2 3 d1 1
2 d2 . remaining paragraphs assigned topics 1 3 form separate
per-document clusters.
Previously developed methods cross-document alignment primarily driven
similarity functions quantify lexical overlap textual units (Barzilay & Elhadad, 2003; Nelken & Shieber, 2006). methods explicitly model document
structure, specify global constraints guide search optimal
alignment. Pairs textual units considered isolation making alignment decisions. contrast, approach allows us take advantage global structure shared
language models across related textual units without requiring manual specification
matching constraints.

145

fiChen, Branavan, Barzilay, & Karger

5.2 Segmentation
Segmentation well-studied discourse task goal divide document
topically cohesive contiguous sections. Previous approaches typically relied lexical
cohesion is, similarity word choices within document subspan guide
choice segmentation boundaries (Hearst, 1994; van Mulbregt, Carp, Gillick, Lowe, &
Yamron, 1998; Blei & Moreno, 2001; Utiyama & Isahara, 2001; Galley, McKeown, FoslerLussier, & Jing, 2003; Purver et al., 2006; Malioutov & Barzilay, 2006; Eisenstein & Barzilay,
2008). model relies notion determining language models topics,
connecting topics across documents constraining topics appear allow
better learn words indicative topic cohesion.
output samples models inference procedure map straightforwardly
segmentations contiguous spans paragraphs assigned topic number taken one segment. example, seven-paragraph document topic
assignments zd = (2, 4, 4, 1, 1, 1, 1) would segmented three sections, comprised
paragraph 1, paragraphs 2 3, paragraphs 4 7. Note segmentation ignores specific values used topic assignments, heeds paragraph
boundaries topic assignments change.
5.3 Ordering
third application model problem creating structured documents
collections unordered text segments. text ordering task important step
broader NLP tasks text summarization generation. task, assume
provided well structured documents single domain training examples;
trained, model used induce ordering previously unseen collections
paragraphs domain.
training, model learns canonical ordering topics documents within
collection, via language models associated topic. GMM
concentrates probability mass around canonical (1, . . . , K) topic ordering, expect
highly probable words language models lower -numbered topics tend appear early
document, whereas highly probable words language models higher -numbered
topics tend appear late document. Thus, structure new documents according
intuition paragraphs words tied low topic numbers placed earlier
paragraphs words relating high topic numbers.
Formally, given unseen document comprised unordered set paragraphs
{p1 , . . . , pn }, order paragraphs according following procedure. First, find
probable topic assignment zi independently paragraph pi , according
parameters learned training phase:
zi = arg max P (zi = k | pi , , )
k

= arg max P (pi | zi = k, k )P (zi = k | ).

(13)

k

Second, sort paragraphs topic assignment zi ascending order since (1 . . . K)
GMMs canonical ordering, yields likely ordering conditioned single
estimated topic assignment paragraph. Due possible ties topic assignments,
146

fiContent Modeling Using Latent Permutations

resulting document may partial ordering; full ordering required, ties
broken arbitrarily.
key advantage proposed approach closed-form computationally
efficient. Though training phase requires running inference procedure Section 4,
model parameters learned, predicting ordering new set p paragraphs
requires computing pK probability scores. contrast, previous approaches
able rank small subset possible document reorderings (Barzilay &
Lapata, 2008), performed search procedure space orderings find
optimum (Elsner et al., 2007).7
objective function Equation 13 depends posterior estimates given
training documents. Since collapsed Gibbs sampler integrates two hidden
variables, need back values known posterior samples
z. easily done computing point estimate distribution based
word-topic topic-document assignment frequencies, respectively, done Griffiths
Steyvers (2004). probability mass kw word w language model topic k
given by:
N (k, w) + 0
kw =
,
(14)
N (k) + W 0
N (k, w) total number times word w assigned topic k, N (k)
total number words assigned topic k, according posterior sample z.
derive similar estimate k , prior likelihood topic k:
k =

N (k) + 0
,
N + K0

(15)

N (k) total number paragraphs assigned topic k according sample
z, N total number paragraphs entire corpus.

6. Experiments
section, evaluate performance model three tasks presented
Section 5: cross-document alignment, document segmentation, information ordering.
first describe preliminaries common three tasks, covering data sets,
reference comparison structures, model variants, inference algorithm settings shared
evaluation. provide detailed examination model performs
individual task.
6.1 General Evaluation Setup
Data Sets experiments use five data sets, briefly described (for additional
statistics, see Table 1):
7. approach describe finding probable paragraph ordering according
data likelihood, optimal ordering derived HMM-based content model.
proposed ordering technique essentially approximates objective using per-paragraph maximum
posteriori estimate topic assignments rather full posterior topic assignment distribution.
approximation makes much faster prediction algorithm performs well empirically.

147

fiChen, Branavan, Barzilay, & Karger

Articles
Corpus
CitiesEn
CitiesEn500
CitiesFr

large cities Wikipedia
Language Documents Sections
English
100
13.2
English
500
10.5
French
100
10.4

Paragraphs
66.7
45.9
40.7

Vocabulary
42,000
95,400
31,000

Tokens
4,920
3,150
2,630

Articles chemical elements Wikipedia
Corpus
Language Documents Sections
Elements
English
118
7.7

Paragraphs
28.1

Vocabulary
18,000

Tokens
1,920

Cell phone reviews PhoneArena.com
Corpus
Language Documents Sections
Phones
English
100
6.6

Paragraphs
24.0

Vocabulary
13,500

Tokens
2,750

Table 1: Statistics data sets used evaluations. values except vocabulary
size document count per-document averages.

CitiesEn: Articles English Wikipedia worlds 100 largest cities
population. Common topics include History, Culture, Demographics. articles typically substantial size share similar content organization patterns.
CitiesEn500 : Articles English Wikipedia worlds 500 largest cities
population. collection superset CitiesEn. Many lower-ranked
cities well known English Wikipedia editors thus, compared CitiesEn
articles shorter average exhibit greater variability content selection
ordering.
CitiesFr : Articles French Wikipedia 100 cities CitiesEn.
Elements: Articles English Wikipedia chemical elements periodic table,8 including topics Biological Role, Occurrence, Isotopes.
Phones: Reviews extracted PhoneArena.com, popular cell phone review website. Topics corpus include Design, Camera, Interface. reviews
written expert reviewers employed site, opposed lay users.9
heterogeneous collection data sets allows us examine behavior
model diverse test conditions. sets vary articles generated,
language articles written, subjects discuss. result,
patterns topic organization vary greatly across domains. instance, within Phones
corpus, articles formulaic, due centralized editorial control website,
establishes consistent standards followed expert reviewers. hand,
Wikipedia articles exhibit broader structural variability due collaborative nature
8. 118 elements http://en.wikipedia.org/wiki/Periodic table, including undiscovered element 117.
9. Phones set, 35 documents short express reviews without section headings; include
input model, evaluate them.

148

fiContent Modeling Using Latent Permutations

Wikipedia editing, allows articles evolve independently. Wikipedia articles
within category often exhibit similar section orderings, many idiosyncratic
inversions. instance, CitiesEn corpus, Geography History sections
typically occur toward beginning document, History appear either
Geography across different documents.
corpus consider manually divided sections authors,
including short textual heading section. Sections 6.2.1 6.3.1, discuss
author-created sections headings used generate reference annotations
alignment segmentation tasks. Note use headings evaluation;
none heading information provided methods consideration.
tasks alignment segmentation, evaluation performed datasets presented
Table 1. ordering task, however, data used training, evaluation
performed using separate held-out set documents. details held-out dataset
given Section 6.4.1.
Model Variants evaluation, besides comparing baselines literature,
consider two variants proposed model. particular, investigate
impact Mallows component model alternately relaxing tightening
way constrains topic orderings:
Constrained : variant, require documents follow exact canonical ordering topics. is, topic permutation inversions allowed, though
documents may skip topics before. case viewed special case
general model, Mallows inversion prior 0 approaches infinity.
implementation standpoint, simply fix inversion counts v zero
inference.10
Uniform: variant assumes uniform distribution topic permutations,
instead biasing toward small related set. Again, special case full
model, inversion prior 0 set zero, strength prior 0 approaching
infinity, thus forcing item always zero.
Note variants still enforce long-range constraint topic contiguity,
vary full model capture topic ordering similarity.
Evaluation Procedure Parameter Settings evaluation model
variants, run collapsed Gibbs sampler five random seed states, take
10,000th iteration chain sample. Results presented average
five samples.
Dirichlet prior hyperparameters bag topics 0 language models 0 set
0.1. GMM, set prior dispersion hyperparameter 0 1, effective
10. first glance, Constrained model variant appears equivalent HMM state
transition either + 1. However, case topics may appear zero times
document, resulting multiple possible transitions state. Furthermore, transition
probabilities would dependent position within document example, earlier absolute
positions within document, transitions high-index topics unlikely, would require
subsequent paragraphs high-index topic.

149

fiChen, Branavan, Barzilay, & Karger

sample size prior 0 0.1 times number documents. values minimally
tuned, similar results achieved alternative settings 0 0 . Parameters 0
0 control strength bias toward structural regularity, trading
Constrained Uniform model variants. values chosen middle ground
two extremes.
model takes parameter K controls upper bound number
latent topics. Note algorithm select fewer K topics document,
K determine number segments document. general, higher K
results finer-grained division document different topics, may result
precise topics, may split topics together. report results
evaluation using K = 10 20.
6.2 Alignment
first evaluate model task cross-document alignment, goal
group textual units different documents topically cohesive clusters. instance,
Cities-related domains, one cluster may include Transportation-related paragraphs. turning results first present details specific evaluation setup
targeted task.
6.2.1 Alignment Evaluation Setup
Reference Annotations generate sufficient amount reference data evaluating alignments use section headings provided authors. assume two
paragraphs aligned section headings identical. headings
constitute noisy annotations Wikipedia datasets: topical content may
labeled different section headings different articles (e.g., CitiesEn, Places
interest one article Landmarks another), call reference structure
noisy headings set.
clear priori effect noise section headings may evaluation accuracy. empirically estimate effect, use manually annotated
alignments experiments. Specifically, CitiesEn corpus, manually annotated articles paragraphs consistent set section headings, providing us
additional reference structure evaluate against. clean headings set, found
approximately 18 topics expressed one document.
Metrics quantify alignment output compute recall precision score
candidate alignment reference alignment. Recall measures, unique
section heading reference, maximum number paragraphs heading
assigned one particular topic. final score computed summing
section heading dividing total number paragraphs. High recall indicates
paragraphs section headings generally assigned topic.
Conversely, precision measures, topic number, maximum number paragraphs topic assignment share section heading. Precision summed
topic normalized total number paragraphs. High precision means
paragraphs assigned single topic usually correspond section heading.

150

fiContent Modeling Using Latent Permutations

Recall precision trade finely grained topics tend
improve precision cost recall. extremes, perfect recall occurs every
paragraph assigned topic, perfect precision paragraph
topic.
present one summary F-score results, harmonic mean
recall precision.
Statistical significance setup measured approximate randomization (Noreen,
1989), nonparametric test directly applied nonlinearly computed metrics
F-score. test used prior evaluations information extraction
machine translation (Chinchor, 1995; Riezler & Maxwell, 2005).
Baselines

task, compare two baselines:

Hidden Topic Markov Model (HTMM) (Gruber et al., 2007): explained Section 2, model represents topic change adjacent textual units Markovian fashion. HTMM capture local constraints, would allow topics
recur non-contiguously throughout document. use publicly available implementation,11 priors set according recommendations made original
work.
Clustering: use repeated bisection algorithm find clustering paragraphs maximizes sum pairwise cosine similarities items
cluster.12 clustering implemented using CLUTO toolkit.13 Note
approach completely structure-agnostic, treating documents bags paragraphs rather sequences paragraphs. types clustering techniques
shown deliver competitive performance cross-document alignment
tasks (Barzilay & Elhadad, 2003).
6.2.2 Alignment Results
Table 2 presents results alignment evaluation. datasets, best
performance achieved model variants, statistically significant usually
substantial margin.
comparative performance baseline methods consistent across domains
surprisingly, clustering performs better complex HTMM model. observation consistent previous work cross-document alignment multidocument
summarization, use clustering main component (Radev, Jing, & Budzikowska,
2000; Barzilay, McKeown, & Elhadad, 1999). Despite fact HTMM captures
dependencies adjacent paragraphs, sufficiently constrained. Manual examination actual topic assignments reveals HTMM often assigns topic
disconnected paragraphs within document, violating topic contiguity constraint.
one domain full GMM-based approach yields best performance compared variants. one exception Phone domain. Constrained
11. http://code.google.com/p/openhtmm/
12. particular clustering technique substantially outperforms agglomerative graph partitioningbased clustering approaches task.
13. http://glaros.dtc.umn.edu/gkhome/views/cluto/

151

fiK = 20

K = 10

K = 20

K = 10

Chen, Branavan, Barzilay, & Karger

Clustering
HTMM
Constrained
Uniform
model
Clustering
HTMM
Constrained
Uniform
model

CitiesEn
Clean headings
Recall
Prec
F-score
0.578
0.439 0.499
0.446
0.232 0.305
0.579
0.471 0.520
0.520
0.440 0.477
0.639 0.509
0.566
0.486
0.541 0.512
0.260
0.217 0.237
0.458
0.519 0.486
0.499
0.551 0.524
0.578 0.636
0.606

CitiesEn
Noisy headings
Recall
Prec
F-score
0.611
0.331 0.429
0.480
0.183 0.265
0.667
0.382 0.485
0.599
0.343 0.436
0.705 0.399
0.510
0.527
0.414 0.464
0.304
0.187 0.232
0.553
0.415 0.474
0.571
0.423 0.486
0.648 0.489
0.557

CitiesEn500
Noisy headings
Recall
Prec
F-score
0.609
0.329 0.427
0.461
0.269 0.340
0.643
0.385 0.481
0.582
0.344 0.432
0.722 0.426
0.536
0.489
0.391 0.435
0.351
0.234 0.280
0.515
0.394 0.446
0.557
0.422 0.480
0.620 0.473
0.537

Clustering
HTMM
Constrained
Uniform
model
Clustering
HTMM
Constrained
Uniform
model

CitiesFr
Noisy headings
Recall
Prec
F-score
0.588
0.283 0.382
0.338
0.190 0.244
0.652
0.356
0.460
0.587
0.310 0.406
0.657 0.360
0.464
0.453
0.317 0.373
0.253
0.195 0.221
0.584
0.379 0.459
0.571
0.373 0.451
0.633 0.431
0.513

Elements
Noisy headings
Recall
Prec
F-score
0.524
0.361 0.428
0.430
0.190 0.264
0.603
0.408 0.487
0.591
0.403 0.479
0.685 0.460
0.551
0.477
0.402 0.436
0.248
0.243 0.246
0.510
0.421 0.461
0.550
0.479 0.512
0.569 0.498
0.531

Phones
Noisy headings
Recall
Prec
F-score
0.599
0.456 0.518
0.379
0.240 0.294
0.745 0.506
0.602
0.656
0.422 0.513
0.738
0.493
0.591
0.486
0.507 0.496
0.274
0.229 0.249
0.652 0.576
0.611
0.608
0.471 0.538
0.683
0.546
0.607

Table 2: Comparison alignments produced model series baselines
model variations, 10 20 topics, evaluated clean noisy sets
section headings. Higher scores better. Within K, methods
model significantly outperforms indicated p < 0.001
p < 0.01.

152

fiContent Modeling Using Latent Permutations

baseline achieves best result K small margin. results
expected, given fact domain exhibits highly rigid topic structure across
documents. model permits permutations topic ordering, GMM,
flexible highly formulaic domains.
Finally, observe evaluations based manual noisy annotations exhibit
almost entirely consistent ranking methods consideration (see clean
noisy headings results CitiesEn Table 2). consistency indicates noisy
headings sufficient gaining insight comparative performance different
approaches.
6.3 Segmentation
Next consider task text segmentation. test whether model able
identify boundaries topically coherent text segments.
6.3.1 Segmentation Evaluation Setup
Reference Segmentations described Section 6.1, datasets used
evaluation manually divided sections authors. annotations
used create reference segmentations evaluating models output. Recall
Section 6.2.1 built clean reference structure CitiesEn set. structure encodes clean segmentation document adjusts granularity
section headings consistent across documents. Thus, compare
segmentation specified CitiesEn clean section headings.
Metrics Segmentation quality evaluated using standard penalty metrics Pk
WindowDiff (Beeferman, Berger, & Lafferty, 1999; Pevzner & Hearst, 2002). pass
sliding window documents compute probability words end
windows improperly segmented respect other. WindowDiff
stricter, requires number segmentation boundaries endpoints
window correct well.14
Baselines first compare BayesSeg (Eisenstein & Barzilay, 2008),15 Bayesian
segmentation approach current state-of-the-art task. Interestingly,
model reduces approach every document considered completely isolation,
topic sharing documents. Connecting topics across documents makes
much difficult inference problem one tackled Eisenstein Barzilay.
time, algorithm cannot capture structural relatedness across documents.
Since BayesSeg designed operated specification number segments,
provide baseline benefit knowing correct number segments
document, provided system. run baseline using
14. Statistical significance testing standardized usually reported segmentation task,
omit tests results.
15. evaluate corpora used work, since model relies content similarity across
documents corpus.

153

fiBayesSeg
U&I
U&I
Constrained
Uniform
model
Constrained
Uniform
model

CitiesEn
Clean headings
Pk
WD
# Segs
0.321
0.376
12.3
0.337
0.404
12.3
0.353
0.375
5.8
0.260 0.281
7.7
0.268
0.300
8.8
0.253
0.283
9.0
0.274
0.314
10.9
0.234
0.294
14.0
0.221 0.278
14.2

CitiesEn
Noisy headings
Pk
WD
# Segs
0.317
0.376
13.2
0.337
0.405
13.2
0.357
0.378
5.8
0.267
0.288
7.7
0.273
0.304
8.8
0.257 0.286
9.0
0.274
0.313
10.9
0.234
0.290
14.0
0.222 0.278
14.2

CitiesEn500
Noisy headings
Pk
WD
# Segs
0.282
0.335
10.5
0.292
0.350
10.5
0.321
0.346
5.4
0.221
0.244
6.8
0.227
0.257
7.8
0.196 0.225
8.1
0.226
0.261
9.1
0.203
0.256
12.3
0.196 0.247
12.1

BayesSeg
U&I
U&I
Constrained
Uniform
model
Constrained
Uniform
model

CitiesFr
Noisy headings
Pk
WD
# Segs
0.274
0.332
10.4
0.282
0.336
10.4
0.321
0.342
4.4
0.230
0.244
6.4
0.214 0.233
7.3
0.216 0.233
7.4
0.230
0.250
7.9
0.203
0.234
10.4
0.201 0.230
10.8

Elements
Noisy headings
Pk
WD
# Segs
0.279
0.316
7.7
0.248
0.286
7.7
0.294
0.312
4.8
0.227
0.244
5.4
0.226
0.250
6.6
0.201 0.226
6.7
0.231
0.257
6.6
0.209
0.248
8.7
0.203 0.243
8.6

Phones
Noisy headings
Pk
WD
# Segs
0.392
0.457
9.6
0.412
0.463
9.6
0.423
0.435
4.7
0.312 0.347
8.0
0.332
0.367
7.5
0.309
0.349
8.0
0.295 0.348
10.8
0.327
0.381
9.4
0.302
0.357
10.4

K = 20 K = 10

K = 20 K = 10

Chen, Branavan, Barzilay, & Karger

Table 3: Comparison segmentations produced model series baselines
model variations, 10 20 topics, evaluated clean noisy
sets section headings. Lower scores better. BayesSeg U&I given
true number segments, segments counts reflect reference structures
segmentations. contrast, U&I automatically predicts number segments.

154

fiContent Modeling Using Latent Permutations

authors publicly available implementation;16 priors set using built-in mechanism
automatically re-estimates hyperparameters.
compare method algorithm Utiyama Isahara (2001),
commonly used point reference evaluation segmentation algorithms.
algorithm computes optimal segmentation estimating changes predicted
language models segments different partitions. used publicly available
implementation system,17 require parameter tuning held-out
development set. contrast BayesSeg, algorithm mechanism predicting
number segments, take pre-specified number segments.
comparison, consider versions algorithm U&I denotes case
correct number segments provided model U&I denotes model
estimates optimal number segments.
6.3.2 Segmentation Results
Table 3 presents segmentation experiment results. every data set model outperforms BayesSeg U&I baselines substantial margin regardless K. result
provides strong evidence learning connected topic models related documents leads
improved segmentation performance.
best performance generally obtained full version model, three
exceptions. two cases (CitiesEn K = 10 using clean headings WindowDiff
metric, CitiesFr K = 10 Pk metric), variant performs better
full model minute margin. Furthermore, instances,
corresponding evaluation K = 20 using full model leads best overall
results respective domains.
case variant outperforms full model notable margin
Phones data set. result unexpected given formulaic nature dataset
discussed earlier.
6.4 Ordering
final task evaluate model finding coherent ordering
set textual units. Unlike previous tasks, prediction based hidden variable
distributions, ordering observed document. Moreover, GMM model uses
information inference process. Therefore, need divide data sets
training test portions.
past, ordering algorithms applied textual units various granularities, commonly sentences paragraphs. ordering experiments operate
level relatively larger unit sections. believe granularity suitable
nature model, captures patterns level topic distributions
rather local discourse constraints. ordering sentences paragraphs
studied past (Karamanis et al., 2004; Barzilay & Lapata, 2008) two types
models effectively combined induce full ordering (Elsner et al., 2007).
16. http://groups.csail.mit.edu/rbg/code/bayesseg/
17. http://www2.nict.go.jp/x/x161/members/mutiyama/software.html#textseg

155

fiChen, Branavan, Barzilay, & Karger

Corpus
CitiesEn
CitiesFr
Phones

Set
Training
Testing
Training
Testing
Training
Testing

Documents
100
65
100
68
100
64

Sections
13.2
11.2
10.4
7.7
6.6
9.6

Paragraphs
66.7
50.3
40.7
28.2
24.0
39.3

Vocabulary
42,000
42,000
31,000
31,000
13,500
13,500

Tokens
4,920
3,460
2,630
1,580
2,750
4,540

Table 4: Statistics training test sets used ordering experiments. values
except vocabulary average per document. training set statistics
reproduced Table 1 ease reference.

6.4.1 Ordering Evaluation Setup
Training Test Data Sets use CitiesEn, CitiesFr Phones data sets
training documents parameter estimation described Section 5. introduce
additional sets documents domains test sets. Table 4 provides statistics
training test set splits (note out-of-vocabulary terms test sets
discarded).18
Even though perform ordering section level, collections still pose
challenging ordering task: example, average number sections CitiesEn test
document 11.2, comparable 11.5 sentences (the unit reordering) per document
National Transportation Safety Board corpus used previous work (Barzilay & Lee,
2004; Elsner et al., 2007).
Metrics report Kendalls rank correlation coefficient ordering experiments. metric measures much ordering differs reference order
underlying assumption reasonable sentence orderings fairly similar
it. Specifically, permutation sections N -section document, ()
computed
d(, )
() = 1 2 N ,
(16)
2

d(, ) is, before, Kendall distance, number swaps adjacent textual
units necessary rearrange reference order. metric ranges -1 (inverse
orders) 1 (identical orders). Note random ordering yield zero score expectation. measure widely used evaluating information ordering (Lapata,
2003; Barzilay & Lee, 2004; Elsner et al., 2007) shown correlate human
assessments text quality (Lapata, 2006).
Baselines Model Variants ordering method compared original
HMM-based content modeling approach Barzilay Lee (2004). baseline delivers
18. Elements data set limited 118 articles, preventing us splitting reasonably sized
training test sets. Therefore consider ordering experiments. Citiesrelated sets, test documents shorter cities lesser population.
hand, Phones test set include short express reviews thus exhibits higher
average document length.

156

fiContent Modeling Using Latent Permutations

HMM-based Content Model
Constrained
K = 10
model
Constrained
K = 20
model

CitiesEn
0.245
0.587
0.571
0.583
0.575

CitiesFr
0.305
0.596
0.541
0.575
0.571

Phones
0.256
0.676
0.678
0.711
0.678

Table 5: Comparison orderings produced model series baselines
model variations, 10 20 topics, evaluated respective test sets.
Higher scores better.

state-of-the art performance number datasets similar spirit model
aims capture patterns level topic distribution (see Section 2). Again,
use publicly available implementation19 parameters adjusted according
values used previous work. content modeling implementation provides A*
search procedure use find optimal permutation.
include comparison local coherence models (Barzilay & Lapata, 2008;
Elsner et al., 2007). models designed sentence-level analysis particular,
use syntactic information thus cannot directly applied section-level ordering.
state above, models orthogonal topic-based analysis; combining two
approaches promising direction future work.
Note Uniform model variant applicable task, since
make claims preferred underlying topic ordering. fact, document likelihood perspective, proposed paragraph order reverse order would
probability Uniform model. Thus, model variant consider
Constrained.
6.4.2 Ordering Results
Table 5 summarizes ordering results GMM- HMM-based content models. Across
data sets, model outperforms content modeling large margin. instance,
CitiesEn dataset, gap two models reaches 35%. difference
expected. previous work, content models applied short formulaic texts.
contrast, documents collection exhibit higher variability original collections.
HMM provide explicit constraints generated global orderings. may
prevent effectively learning non-local patterns topic organization.
observe Constrained variant outperforms full model.
difference two small, fairly consistent across domains. Since
possible predict idiosyncratic variations test documents topic orderings,
constrained model better capture prevalent ordering patterns consistent
across domain.
19. http://people.csail.mit.edu/regina/code.html

157

fiChen, Branavan, Barzilay, & Karger

6.5 Discussion
experiments three separate tasks reveal common trends results.
First, observe single unified model document structure readily
successfully applied multiple discourse-level tasks, whereas previous work proposed
separate approaches task. versatility speaks power topic-driven
representation document structure. Second, within task model outperforms
state-of-the-art baselines substantial margins across wide variety evaluation scenarios. results strongly support hypothesis augmenting topic models
discourse-level constraints broadens applicability discourse-level analysis tasks.
Looking performance model across different tasks, make notes
importance individual topic constraints. Topic contiguity consistently
important constraint, allowing model variants outperform alternative baseline
approaches. cases, introducing bias toward similar topic ordering, without requiring identical orderings, provides benefits encoded model.
flexible models achieve superior performance segmentation alignment tasks.
case ordering, however, extra flexibility pay off, model distributes
probability mass away strong ordering patterns likely occur unseen data.
identify properties dataset strongly affect performance
model. Constrained model variant performs slightly better full model
rigidly formulaic domains, achieving highest performance Phones data set.
know priori domain formulaic structure, worthwhile choose
model variant suitably enforces formulaic topic orderings. Fortunately, adaptation
achieved proposed framework using prior Generalized Mallows
Model recall Constrained variant special case full model.
However, performance model invariant respect data set characteristics. Across two languages considered, model baselines exhibit
comparative performance task. Moreover, consistency holds
general-interest cities articles highly technical chemical elements articles. Finally, smaller CitiesEn larger CitiesEn500 data sets, observe
results consistent.

7. Conclusions Future Work
paper, shown unsupervised topic-based approach capture content
structure. resulting model constrains topic assignments way requires global
modeling entire topic sequences. showed Generalized Mallows Model
theoretically empirically appealing way capturing ordering component
topic sequence. results demonstrate importance augmenting statistical models
text analysis structural constraints motivated discourse theory. Furthermore,
success GMM suggests could applied modeling ordering
constraints NLP applications.
multiple avenues future extensions work. First, empirical results
demonstrated certain domains providing much flexibility model may
fact detrimental predictive accuracy. cases, tightly constrained
variant model yields superior performance. interesting extension current
158

fiContent Modeling Using Latent Permutations

model would allow additional flexibility prior GMM drawing
another level hyperpriors. technical perspective, form hyperparameter
re-estimation would involve defining appropriate hyperprior Generalized Mallows
Model adapting estimation present inference procedure.
Additionally, may cases assumption one canonical topic ordering
entire corpus limiting, e.g., data set consists topically related articles
multiple sources, editorial standards. model extended
allow multiple canonical orderings positing additional level hierarchy
probabilistic model, i.e., document structures generated mixture several
Generalized Mallows Models, distributional mode. case,
model would take additional burden learning topics permuted
multiple canonical orderings. change model would greatly complicate
inference re-estimating Generalized Mallows Model canonical ordering general NPhard. However, recent advances statistics produced efficient approximate algorithms
theoretically guaranteed correctness bounds (Ailon, Charikar, & Newman, 2008)
exact methods tractable typical cases (Meila et al., 2007).
generally, model presented paper assumes two specific global constraints
content structure. domains satisfy constraints plentiful,
domains modeling assumptions hold. example, dialogue well
known topics recur throughout conversation (Grosz & Sidner, 1986), thereby violating
first constraint. Nevertheless, texts domains still follow certain organizational
conventions, e.g. stack structure dialogue. results suggest explicitly incorporating domain-specific global structural constraints content model would likely
improve accuracy structure induction.
Another direction future work combine global topic structure model
local coherence constraints. previously noted, model agnostic toward
relationships sentences within single topic. contrast, models local coherence
take advantage wealth additional knowledge, syntax, make decisions
information flow across adjoining sentences. linguistically rich model would provide
powerful representation levels textual structure, could used even
greater variety applications considered here.

Bibliographic Note
Portions work previously presented conference publication (Chen, Branavan,
Barzilay, & Karger, 2009). article significantly extends previous work, notably
introducing new algorithm applying models output information ordering
task (Section 5) considering new data sets experiments vary genre,
language, size (Section 6).

Acknowledgments
authors acknowledge funding support NSF CAREER grant IIS-0448168
grant IIS-0712793, NSF Graduate Fellowship, Office Naval Research, Quanta,
Nokia, Microsoft Faculty Fellowship. thank many people offered

159

fiChen, Branavan, Barzilay, & Karger

suggestions comments work, including Michael Collins, Aria Haghighi, Yoong
Keok Lee, Marina Meila, Tahira Naseem, Christy Sauper, David Sontag, Benjamin Snyder,
Luke Zettlemoyer. especially grateful Marina Meila introducing us
Mallows model. paper greatly benefited thoughtful feedback
anonymous reviewers. opinions, findings, conclusions, recommendations expressed
paper authors, necessarily reflect views funding
organizations.

160

fiContent Modeling Using Latent Permutations

References
Ailon, N., Charikar, M., & Newman, A. (2008). Aggregating inconsistent information:
Ranking clustering. Journal ACM, 55 (5).
Althaus, E., Karamanis, N., & Koller, A. (2004). Computing locally coherent discourses.
Proceedings ACL.
Bartlett, F. C. (1932). Remembering: study experimental social psychology. Cambridge University Press.
Barzilay, R., & Elhadad, N. (2003). Sentence alignment monolingual comparable corpora.
Proceedings EMNLP.
Barzilay, R., Elhadad, N., & McKeown, K. (2002). Inferring strategies sentence ordering
multidocument news summarization. Journal Artificial Intelligence Research,
17, 3555.
Barzilay, R., & Lapata, M. (2008). Modeling local coherence: entity-based approach.
Computational Linguistics, 34 (1), 134.
Barzilay, R., & Lee, L. (2004). Catching drift: Probabilistic content models, applications generation summarization. Proceedings NAACL/HLT.
Barzilay, R., McKeown, K., & Elhadad, M. (1999). Information fusion context
multi-document summarization. Proceedings ACL.
Beeferman, D., Berger, A., & Lafferty, J. D. (1999). Statistical models text segmentation.
Machine Learning, 34, 177210.
Bernardo, J. M., & Smith, A. F. (2000). Bayesian Theory. Wiley Series Probability
Statistics.
Bishop, C. M. (2006). Pattern Recognition Machine Learning. Springer.
Blei, D. M., & Moreno, P. J. (2001). Topic segmentation aspect hidden markov
model. Proceedings SIGIR.
Blei, D. M., Ng, A., & Jordan, M. (2003). Latent dirichlet allocation. Journal Machine
Learning Research, 3, 9931022.
Bollegala, D., Okazaki, N., & Ishizuka, M. (2006). bottom-up approach sentence
ordering multi-document summarization. Proceedings ACL/COLING.
Chen, H., Branavan, S., Barzilay, R., & Karger, D. R. (2009). Global models document
structure using latent permutations. Proceedings NAACL/HLT.
Chinchor, N. (1995). Statistical significance MUC-6 results. Proceedings 6th
Conference Message Understanding.
Cohen, W. W., Schapire, R. E., & Singer, Y. (1999). Learning order things. Journal
Artificial Intelligence Research, 10, 243270.
Eisenstein, J., & Barzilay, R. (2008). Bayesian unsupervised topic segmentation. Proceedings EMNLP.
Elsner, M., Austerweil, J., & Charniak, E. (2007). unified local global model
discourse coherence. Proceedings NAACL/HLT.
161

fiChen, Branavan, Barzilay, & Karger

Fligner, M., & Verducci, J. (1986). Distance based ranking models. Journal Royal
Statistical Society, Series B, 48 (3), 359369.
Fligner, M. A., & Verducci, J. S. (1990). Posterior probabilities consensus ordering.
Psychometrika, 55 (1), 5363.
Galley, M., McKeown, K. R., Fosler-Lussier, E., & Jing, H. (2003). Discourse segmentation
multi-party conversation. Proceedings ACL.
Geman, S., & Geman, D. (1984). Stochastic relaxation, gibbs distributions bayesian
restoration images. IEEE Transactions Pattern Analysis Machine Intelligence, 12, 609628.
Graesser, A., Gernsbacher, M., & Goldman, S. (Eds.). (2003). Handbook Discourse
Processes. Erlbaum.
Griffiths, T. L., & Steyvers, M. (2004). Finding scientific topics. Proceedings National
Academy Sciences, 101, 52285235.
Griffiths, T. L., Steyvers, M., Blei, D. M., & Tenenbaum, J. B. (2005). Integrating topics
syntax. Advances NIPS.
Grosz, B. J., & Sidner, C. L. (1986). Attention, intentions, structure discourse.
Computational Linguistics, 12 (3), 175204.
Gruber, A., Rosen-Zvi, M., & Weiss, Y. (2007). Hidden topic markov models. Proceedings
AISTATS.
Halliday, M. A. K., & Hasan, R. (1976). Cohesion English. Longman.
Hearst, M. (1994). Multi-paragraph segmentation expository text. Proceedings
ACL.
Ji, P. D., & Pulman, S. (2006). Sentence ordering manifold-based classification
multi-document summarization. Proceedings EMNLP.
Karamanis, N., Poesio, M., Mellish, C., & Oberlander, J. (2004). Evaluating centeringbased metrics coherence text structuring using reliably annotated corpus.
Proceedings ACL.
Klementiev, A., Roth, D., & Small, K. (2008). Unsupervised rank aggregation distancebased models. Proceedings ICML, pp. 472479.
Lapata, M. (2003). Probabilistic text structuring: Experiments sentence ordering.
Proceedings ACL.
Lapata, M. (2006). Automatic evaluation information ordering: Kendalls tau. Computational Linguistics, 32 (4), 471484.
Lebanon, G., & Lafferty, J. (2002). Cranking: combining rankings using conditional probability models permutations. Proceedings ICML.
Malioutov, I., & Barzilay, R. (2006). Minimum cut model spoken lecture segmentation.
Proceedings ACL.
Mallows, C. L. (1957). Non-null ranking models. Biometrika, 44, 114130.

162

fiContent Modeling Using Latent Permutations

Meila, M., Phadnis, K., Patterson, A., & Bilmes, J. (2007). Consensus ranking
exponential model. Proceedings UAI.
Neal, R. M. (2003). Slice sampling. Annals Statistics, 31, 705767.
Nelken, R., & Shieber, S. M. (2006). Towards robust context-sensitive sentence alignment
monolingual corpora. Proceedings EACL.
Noreen, E. W. (1989). Computer Intensive Methods Testing Hypotheses. Introduction. Wiley.
Pevzner, L., & Hearst, M. A. (2002). critique improvement evaluation metric
text segmentation. Computational Linguistics, 28, 1936.
Porteous, I., Newman, D., Ihler, A., Asuncion, A., Smyth, P., & Welling, M. (2008). Fast
collapsed gibbs sampling latent dirichlet allocation. Proceedings SIGKDD.
Purver, M., Kording, K., Griffiths, T. L., & Tenenbaum, J. B. (2006). Unsupervised topic
modelling multi-party spoken discourse. Proceedings ACL/COLING.
Radev, D., Jing, H., & Budzikowska, M. (2000). Centroid-based summarization multiple documents: Sentence extraction, utility-based evaluation user studies.
Proceedings ANLP/NAACL Summarization Workshop.
Riezler, S., & Maxwell, J. T. (2005). pitfalls automatic evaluation significance testing MT. Proceedings ACL Workshop Intrinsic Extrinsic
Evaluation Measures Machine Translation and/or Summarization.
Schiffrin, D., Tannen, D., & Hamilton, H. E. (Eds.). (2001). Handbook Discourse
Analysis. Blackwell.
Titov, I., & McDonald, R. (2008). Modeling online reviews multi-grain topic models.
Proceedings WWW.
Utiyama, M., & Isahara, H. (2001). statistical model domain-independent text segmentation. Proceedings ACL.
van Mulbregt, P., Carp, I., Gillick, L., Lowe, S., & Yamron, J. (1998). Text segmentation
topic tracking broadcast news via hidden markov model approach. Proceedings
ICSLP.
Wallach, H. M. (2006). Topic modeling: beyond bag words. Proceedings ICML.
Wray, A. (2002). Formulaic Language Lexicon. Cambridge University Press, Cambridge.

163


