journal artificial intelligence

submitted published

content modeling latent permutations
harr chen
r k branavan
regina barzilay
david r karger

harr csail mit edu
branavan csail mit edu
regina csail mit edu
karger csail mit edu

computer science artificial intelligence laboratory
massachusetts institute technology
vassar street cambridge massachusetts usa

abstract
present novel bayesian topic model learning discourse level document structure model leverages insights discourse theory constrain latent topic assignments way reflects underlying organization document topics propose
global model topic selection ordering biased similar across
collection related documents space orderings effectively represented distribution permutations called generalized mallows model
apply method three complementary discourse level tasks cross document alignment
document segmentation information ordering experiments incorporating permutation model applications yields substantial improvements
performance previously proposed methods

introduction
central discourse analysis modeling content structure document
structure encompasses topics addressed order topics
appear across documents single domain modeling content structure particularly
germane domains exhibit recurrent patterns content organization news
encyclopedia articles aim induce example articles
cities typically contain information history economy transportation
descriptions history usually precede transportation
previous work barzilay lee elsner austerweil charniak demonstrated content learned raw unannotated text useful
variety text processing tasks summarization information ordering however expressive power approaches limited taking markovian view
content structure model local constraints topic organization shortcoming substantial since many discourse constraints described literature global
nature graesser gernsbacher goldman schiffrin tannen hamilton
introduce model content structure explicitly represents two
important global constraints topic selection first constraint posits document follows progression coherent nonrecurring topics halliday hasan
following example constraint captures notion single topic
throughout use topic refer interchangeably discourse unit
language model views topic

c

ai access foundation rights reserved

fichen branavan barzilay karger

history expressed contiguous block within document rather spread
disconnected sections second constraint states documents domain
tend present similar topics similar orders bartlett wray constraint
guides toward selecting sequences similar topic ordering placing history transportation constraints universal across genres human
discourse applicable many important domains ranging newspaper text
product reviews
present latent topic model related documents encodes discourse
constraints positing single distribution entirety documents content ordering specifically represent content structure permutation topics
naturally enforces first constraint since permutation allow topic repetition
learn distribution permutations employ generalized mallows model
gmm model concentrates probability mass permutations close canonical
permutation permutations drawn distribution likely similar conforming second constraint major benefit gmm compact parameterization
set real valued dispersion values dispersion parameters allow model
learn strongly bias documents topic ordering toward canonical permutation furthermore number parameters grows linearly number topics
thus sidestepping tractability typically associated large discrete space
permutations
position gmm within larger hierarchical bayesian model explains
set related documents generated document model posits topic
ordering drawn gmm set topic frequencies drawn multinomial distribution together draws specify documents entire topic structure
form topic assignments textual unit traditional topic words
drawn language indexed topic estimate model posterior
perform gibbs sampling topic structures gmm dispersion parameters
analytically integrating remaining hidden variables
apply model three complex document level tasks first alignment
task aim discover paragraphs across different documents share topic
experiments permutation model outperforms hidden topic markov
model gruber rosen zvi weiss wide margin gap averaged percentage points f score second consider segmentation task goal
partition document sequence topically coherent segments model yields
average pk measure percentage point improvement competitive
bayesian segmentation method take global constraints account eisenstein barzilay third apply model ordering task sequencing
held set textual units coherent document previous two applications difference model state art baseline substantial
model achieves average kendalls compared value
hmm content model barzilay lee
success permutation model three complementary tasks demonstrates flexibility effectiveness attests versatility general document
example domain first constraint violated dialogue texts domains follow
stack structure allowing topics recur throughout conversation grosz sidner



ficontent modeling latent permutations

structure induced model encoding global ordering constraints
topic makes suitable discourse level analysis contrast local
decision approaches taken previous work furthermore evaluation scenarios full model yields significantly better simpler variants
use fixed ordering order agnostic
remainder proceeds follows section describe relates previous work topic modeling statistical discourse processing
provide formulation section followed overview content
model section heart model distribution topic permutations
provide background section employing formal description
probabilistic generative story section section discusses estimation posterior distribution given example documents collapsed gibbs
sampling procedure techniques applying model three tasks alignment
segmentation ordering explained section evaluate performance tasks section concluding touching upon directions
future work section code data sets annotations raw outputs
experiments available http groups csail mit edu rbg code mallows

related work
describe two areas previous work related algorithmic
perspective work falls broad class topic earlier work topic
modeling took bag words view documents many recent approaches expanded
topic capture structural constraints section describe extensions highlight differences model linguistic side work
relates modeling text structure statistical discourse processing summarize work section drawing comparisons functionality supported
model
topic
probabilistic topic originally developed context language modeling
today become popular range nlp applications text classification document browsing topic posit latent state variable controls generation
word parameters estimated approximate inference techniques
gibbs sampling variational methods traditional topic latent dirichlet allocation lda blei ng jordan griffiths steyvers documents
treated bags words word receives separate topic assignment words
assigned topic drawn shared language model
bag words representation sufficient applications many cases
structure unaware view limited previous considered extensions
lda two orthogonal directions covering intrasentential extrasentential
constraints



fichen branavan barzilay karger

modeling intrasentential constraints
one promising direction improving topic augment constraints
topic assignments adjoining words within sentences example griffiths steyvers
blei tenenbaum propose model jointly incorporates syntactic
semantic information unified generative framework constrains syntactic classes
adjacent words generation word controlled two hidden
variables one specifying semantic topic specifying syntactic class
syntactic class hidden variables chained together markov model whereas semantic
topic assignments assumed independent every word
another example intrasentential constraints wallach proposes way
incorporate word order information form bigrams lda style model
generation word conditioned previous word
topic current word word topics generated perdocument topic distributions lda formulation text structure
level word transitions opposed work griffiths et al structure
modeled level hidden syntactic class transitions
focus modeling high level document structure terms semantic content
work complementary methods impose structure intrasentential
units possible combine model constraints adjoining words
modeling extrasentential constraints
given intuitive connection notion topic lda notion topic
discourse analysis natural assume lda useful discourselevel tasks segmentation topic classification hypothesis motivated
topic assignment guided structural considerations purver kording
griffiths tenenbaum gruber et al titov mcdonald particularly
relationships topics adjacent textual units depending application
textual unit may sentence paragraph speaker utterance common property
bias topic assignments cohere within local segments text
category vary terms mechanisms used encourage local topic
coherence instance model purver et al biases topic distributions
adjacent utterances textual units similar model generates utterance
mixture topic language parameters topic mixture distribution assumed follow type markovian transition process specifically high
probability utterance u topic distribution previous utterance
u otherwise topic distribution drawn u thus textual units topic
distribution depends previous textual unit controlled parameter indicating
whether topic distribution drawn
similar vein hidden topic markov model htmm gruber et al posits
generative process sentence textual unit assigned single topic
sentences words drawn single language model model
purver et al topic transitions adjacent textual units modeled markovian
fashion specifically sentence topic sentence high probability
receives topic assignment drawn shared topic multinomial distribution



ficontent modeling latent permutations

htmm model assumption single topic per textual unit allows
sections text related across documents topic contrast purver et al model
tailored task segmentation utterance drawn mixture topics
thus model capture utterances topically aligned across related
documents importantly htmm model purver et al able
make local decisions regarding topic transitions thus difficulty respecting longrange discourse constraints topic contiguity model instead takes global view
topic assignments textual units explicitly generating entire documents topic
ordering one joint distribution later global view yields
significant performance gains
recent multi grain latent dirichlet allocation mglda model titov mcdonald studied topic assignments level sub document textual units
mglda set local topic distributions induced sentence dependent
window local context around sentence individual words drawn
local topics document level topics standard lda mglda represents
local context sliding window window frame comprises overlapping short
spans sentences way local topic distributions shared sentences
close proximity
mglda represent complex topical dependencies purver
et al gruber et al window incorporate much wider swath local
context two adjacent textual units however mglda unable encode longer
range constraints contiguity ordering similarity sentences close
proximity loosely connected series intervening window frames
contrast work specifically oriented toward long range constraints necessitating
whole document notion topic assignment
modeling ordering constraints statistical discourse analysis
global constraints encoded model closely related discourse
information ordering applications text summarization generation barzilay
elhadad mckeown lapata karamanis poesio mellish oberlander
elsner et al emphasis body work learning ordering constraints
data goal reordering text domain methods build
assumption recurring patterns topic ordering discovered analyzing
patterns word distribution key distinction prior methods
existing ordering largely driven local constraints limited ability
capture global structure describe two main classes probabilistic ordering
studied discourse processing
discriminative
discriminative approaches aim directly predict ordering given set sentences
modeling ordering sentences simultaneously leads complex structure prediction
practice however computationally tractable two step taken
first probabilistic used estimate pairwise sentence ordering preferences next
local decisions combined produce consistent global ordering lapata



fichen branavan barzilay karger

althaus karamanis koller training data pairwise constructed
considering pairs sentences document supervision labels
actually ordered prior work demonstrated wide range features
useful classification decisions lapata karamanis et al ji
pulman bollegala okazaki ishizuka instance lapata
demonstrated lexical features verb pairs input sentences serve
proxy plausible sequences actions thus effective predictors well formed
orderings second stage local decisions integrated global order
maximizes number consistent pairwise classifications since finding
ordering np hard cohen schapire singer approximations used
practice lapata althaus et al
two step discriminative approaches effectively leverage information
local transitions provide means representing global constraints
recent work barzilay lapata demonstrated certain global properties captured discriminative framework reranking mechanism
set system learns identify best global ordering given set n possible
candidate orderings accuracy ranking greatly depends quality
selected candidates identifying candidates challenging task given large
search space possible alternatives
presented work differs existing discriminative two
ways first model represents distribution possible global orderings thus
use sampling mechanisms consider whole space rather limited
subset candidates ranking second difference arises
generative nature model rather focusing ordering task order aware
model effectively captures layer hidden variables explain underlying structure
document content thus effectively applied wider variety applications
including sentence ordering already observed appropriately adjusting
observed hidden components model
generative
work closer technique generative treat topics hidden variables
one instance work hidden markov model hmm content model barzilay lee model states correspond topics state transitions represent
ordering preferences hidden states emission distribution language model
words thus similar implicitly represent patterns
level topical structure hmm used ranking framework select
ordering highest probability
recent work elsner et al developed search procedure simulated annealing finds high likelihood ordering contrast ranking approaches search procedure cover entire ordering space hand
section define ordering objective maximized
efficiently possible orderings prediction model parameters
learned specifically bag p paragraphs pk calculations paragraph
probabilities necessary k number topics



ficontent modeling latent permutations

another distinction proposed model prior work way global
ordering constraints encoded markovian model possible induce global
constraints introducing additional local constraints instance topic contiguity
enforced selecting appropriate model topology e g augmenting hidden states
record previously visited states however global constraints similarity
overall ordering across documents much challenging represent explicitly
modeling topic permutation distribution easily capture kind global
constraint ultimately resulting accurate topic orderings
later model substantially outperforms barzilay lee
information ordering task applied hmm content model

model
section describe formulation proposed model
formulation
content modeling formalized follows take input corpus
dd related documents specification number topics k
document comprised ordered sequence nd paragraphs pd pd nd
output predict single topic assignment zd p k paragraph p
z values reflect underlying content organization document
related content discussed within document across separate documents
receive z value
formulation shares similarity standard lda setup common
set topics assigned across collection documents difference lda
words topic assignment conditionally independent following bag words view
documents whereas constraints topics assigned let us connect word
distributional patterns document level topic structure
model overview
propose generative bayesian model explains corpus documents
produced set hidden variables high level model first selects
frequently topic expressed document topics ordered
topics determine selection words paragraph notation used
subsequent sections summarized figure
document nd paragraphs separately generate bag topics td
topic ordering unordered bag topics td contains nd elements expresses
many paragraphs document assigned k topics equivalently
td viewed vector occurrence counts topic zero counts
topics appear variable td constructed taking nd samples
nonparametric extension model would learn k
well structured documents paragraphs tend internally topically consistent halliday hasan
predicting one topic per paragraph sufficient however note
applied modifications levels textual granularity sentences



fichen branavan barzilay karger



parameters distribution
topic counts



parameters distribution
topic orderings



vector topic counts

v

vector inversion counts



topic ordering

z

paragraph topic assignment



language model parameters
topic

dirichlet
j k
j gmm
k k
k dirichlet

td
vd

zd

w document words

document
multinomial
gmm
compute vd
compute z td

paragraph p
word w p
w multinomial zd p

k number topics
number documents corpus
nd number paragraphs
document
np number words paragraph p

compute
input inversion count vector v

compute z
input topic counts permutation

create empty list
k
j k
k v j

v j j

create empty list z
end
k k
k
z end k
end end

output permutation

output paragraph topic vector z

figure plate diagram generative process model along table
notation reference purposes shaded circles figure denote observed
variables squares denote hyperparameters dotted arrows indicate
constructed deterministically v according compute
z constructed deterministically according compute z


ficontent modeling latent permutations

distribution topics multinomial representing probability topic
expressed sharing documents captures notion certain topics
likely across documents corpus
topic ordering variable permutation numbers k
defines order topics appear document draw generalized
mallows model distribution permutations explain section
see particular distribution biases permutation selection close single
centroid reflecting discourse constraint preferring similar topic structures across
documents
together documents bag topics td ordering determine topic assignment
zd p paragraphs example corpus k seven paragraph
document td would induce topic sequence
zd induced topic sequence zd never assign topic
two unconnected portions document thus satisfying constraint topic contiguity
assume topic k associated language model k words
paragraph assigned topic k drawn topics language model k
portion similar standard lda topic relates language model
however unlike lda model enforces topic coherence entire paragraph rather
viewing paragraph mixture topics
turning formal discussion generative process first provide
background permutation model topic ordering
generalized mallows model permutations
central challenge presented modeling distribution possible topic orderings purpose use generalized mallows model gmm fligner
verducci lebanon lafferty meila phadnis patterson bilmes
klementiev roth small exhibits two appealing properties context
task first model concentrates probability mass canonical ordering
small perturbations permutations ordering characteristic matches
constraint documents domain exhibit structural similarity second
parameter set scales linearly number elements ordered making
sufficiently constrained tractable inference
first describe standard mallows model orderings mallows
mallows model takes two parameters canonical ordering dispersion parameter
sets probability ordering proportional ed
represents distance metric orderings frequently metric
kendall distance minimum number swaps adjacent elements needed
transform ordering canonical ordering thus orderings close
canonical ordering high probability many elements
moved less probability mass
generalized mallows model first introduced fligner verducci refines
standard mallows model adding additional set dispersion parameters
parameters break apart distance orderings set independent
components component separately vary sensitivity perturbation



fichen branavan barzilay karger

tease apart distance function components gmm distribution considers
inversions required transform canonical ordering observed ordering first
discuss inversions parameterized gmm turn distributions
definition characteristics
inversion representation permutations
typically permutations represented directly ordered sequence elements
example represents permuting initial order placing third element
first followed first element second gmm utilizes alternative
permutation representation defined vector v vk inversion counts
respect identity permutation k term vj counts number times
value greater j appears j permutation note jth inversion
count vj take integer values k j inclusive thus inversion count
vector k elements vk zero instance given standard
form permutation v appear v
numbers appear entire inversion count vector would
likewise previous example permutation maps inversion counts
sum components entire inversion count vector simply orderings
kendall distance canonical ordering
significant appeal inversion representation every valid distinct vector
inversion counts corresponds distinct permutation vice versa see
note permutation straightforwardly compute inversion counts
conversely given sequence inversion counts construct unique corresponding
permutation insert items permutation working backwards item k
assume already placed items j k proper order insert
item j note exactly vj items j k must precede meaning
must inserted position vj current order see compute
figure since one place j inserted fulfills inversion
counts induction shows exactly one permutation constructed satisfy
given inversion counts
model take canonical topic ordering identity ordering
k topic numbers task completely symmetric linked
extrinsic meaning fixing global ordering specific arbitrary value
sacrifice representational power general case gmm canonical ordering
parameter distribution
probability mass function
gmm assigns probability mass particular order order permuted canonical ordering precisely associates distance every
permutation canonical ordering distance zero permutations many
inversions respect canonical ordering larger distance distance assignment k real valued dispersion parameters p
k distance
permutation inversion counts v defined j j vj gmms probability



ficontent modeling latent permutations

mass function exponential distance
p

e j j vj
gmm v



k

j



q

j

ej vj

j j



j j normalization factor value
j j

e kj j

ej



setting j equal single value recovers standard mallows model kendall
distance function factorization gmm independent probabilities per
inversion count makes distribution particularly easy apply use gmmj refer
jth multiplicand probability mass function marginal distribution
vj
gmmj vj j

ej vj

j j



due exponential form distribution requiring j constrains gmm
assign highest probability mass vj zero e distributional mode
canonical identity permutation higher value j assigns probability mass vj
close zero biasing j fewer inversions
conjugate prior
major benefit gmm membership exponential family distributions
means particularly amenable bayesian representation admits
natural independent conjugate prior parameter j fligner verducci
gmm j vj e j vj log j j



prior distribution takes two parameters vj intuitively prior states
previous trials total number inversions observed vj distribution
easily updated observed vj derive posterior distribution
vj different range inconvenient set prior hyperparameters
vj directly work instead assign common prior value parameter j
denote set vj maximum likelihood estimate
j differentiating likelihood gmm respect j straightforward
verify works setting
vj

e

k j

kj


e





fichen branavan barzilay karger

formal generative process
fully specify details content model whose plate diagram appears
figure observe corpus documents document ordered
sequence nd paragraphs paragraph represented bag words number
topics k assumed pre specified model induces set hidden variables
probabilistically explain words corpus produced final desired
output posterior distributions paragraphs hidden topic assignment variables
following variables subscripted fixed prior hyperparameters
topic k draw language model k dirichlet lda
topic specific word distributions
draw topic distribution dirichlet expresses likely topic
appear regardless position
draw topic ordering distribution parameters j gmm j
k parameters control rapidly probability mass decays
inversions topic separate j every topic allows us learn
topics likely reordered others
document nd paragraphs
draw bag topics td sampling nd times multinomial
b draw topic ordering sampling vector inversion counts vd gmm
applying compute figure vd
c compute vector topic assignments zd document ds paragraphs
sorting td according compute z figure
paragraph p document
sample word w p according language model p w
multinomial zd p
properties model
section describe rationale behind gmm represent ordering
component content model
representational power gmm concentrates probability mass around one centroid permutation reflecting preferred bias toward document structures similar topic orderings furthermore parameterization gmm vector
dispersion parameters allows flexibility strongly model biases toward
single ordering one extreme one ordering nonzero probability orderings equally likely comprised
multiple permutations contribute probability single documents topic assignments zd
topics appear td current formulation biased toward
assignments fewer topics per document practice negatively impact model
performance



ficontent modeling latent permutations

independent dispersion parameters k distribution assign different penalties displacing different topics example may learn middle
sections case cities sections economy culture likely
vary position across documents early sections introduction
history
computational benefits parameterization gmm vector dispersion parameters compact tractable since number parameters grows
linearly number topics model efficiently handle longer documents
greater diversity content
another computational advantage model seamless integration larger
bayesian model due membership exponential family existence
conjugate prior inference become significantly complex
gmm used hierarchical context case entire document generative
model accounts topic frequency words within topic
one final beneficial effect gmm breaks symmetry topic assignments fixing distribution centroid specifically topic assignments
invariant relabeling probability underlying permutation would
change contrast many topic assign probability relabeling
topic assignments model thus sidesteps topic identifiability issue model may multiple maxima likelihood due
underlying symmetry hidden variables non identifiable
standard lda may cause sampling procedures jump maxima produce
draws difficult aggregate across runs
finally section benefits gmm extend theoretical empirical representing permutations gmm almost leads
superior performance compared alternative approaches

inference
variables aim infer paragraph topic assignments z determined bag topics ordering document thus goal estimate
joint marginal distributions given document text integrating
remaining hidden parameters
p w

accomplish inference task gibbs sampling geman geman bishop
gibbs sampler builds markov chain hidden variable state space whose
stationary distribution actual posterior joint distribution sample
drawn distribution single variable conditioned previous samples
variables collapse sampler integrating hidden
variables model effect reducing state space markov chain collapsed
sampling previously demonstrated effective lda variants griffiths
steyvers porteous newman ihler asuncion smyth welling titov



fichen branavan barzilay karger

p td p td p wd td wd zd


n

p wd z wd
k

p vd j v p vd j v j p wd td wd zd
gmmj v j p wd z wd

p


vd j vj
p j gmm j
n
n
figure collapsed gibbs sampling inference procedure estimating
posterior distribution plate diagram variable resampled
shown double circle markov blanket highlighted black
variables impact variable resampled grayed
variables shown dotted circles never explicitly depended
estimated marginalized sampler diagram
accompanied conditional resampling distribution respective variable



ficontent modeling latent permutations

mcdonald typically preferred explicit gibbs sampling hidden
variables smaller search space generally shorter mixing time
sampler analytically integrates three sets hidden variables bags
topics orderings permutation inversion parameters burn period
treat last samples draw posterior samples
marginalized variables necessary estimated topic
assignments section figure summarizes gibbs sampling steps
inference procedure
document probability preliminary step consider calculate probability
single documents words wd given documents paragraph topic assignments zd
remaining documents topic assignments note probability decomposable product probabilities individual paragraphs paragraphs
different topics conditionally independent word probabilities let wd zd indicate words topic assignments documents w vocabulary
size probability words
k z

p wd z wd
p wd zd k p k z wd dk



k k
k


dcm wd zd k wd zd k



k

dcm refers dirichlet compound multinomial distribution
integrating multinomial parameters dirichlet prior bernardo smith
dirichlet prior parameters w dcm assigns following
probability series observations x x xn
p
w
j j
n x
p
dcm x q


x j j
j j


n x refers number times word appears x gamma
function generalization factorial real numbers algebra shows
dcms posterior probability density function conditioned series observations
yn computed updating counts often word appears

dcm x dcm x n w n w



equations used compute conditional distributions hidden
variables turn individual random variable resampled
bag topics first consider resample td ith topic draw document
conditioned parameters fixed note topic ith
paragraph reorder topics generated separately
p td p td p wd td wd zd


n

p wd z wd
k




fichen branavan barzilay karger

td updated reflect td zd deterministically computed last step
compute z figure inputs td first step reflects application
bayes rule factor term wd drop superfluous terms
conditioning second step former term arises dcm updating
parameters observations equation dropping constants
latter document probability term computed equation td selected
sampling probability computed possible topic assignments
ordering parameterization permutation series inversion values vd j
reveals natural way decompose search space gibbs sampling document
resample vd j j k independently successively according
conditional distribution
p vd j v p vd j v j p wd td wd zd
gmmj v j p wd z wd



updated reflect vd j v zd computed deterministically according
td first term refers equation second computed equation
probability computed every possible value v ranges k j
term vd j sampled according resulting probabilities
gmm parameters j k resample j posterior distribution
p


vd j vj
p j gmm j
n

n
gmm evaluated according equation normalization constant
distribution unknown meaning cannot directly compute invert cumulative distribution function sample distribution however distribution
univariate unimodal expect mcmc technique slice
sampling neal perform well practice matlabs built slice sampler
provides robust draw distribution
computational issues inference directly computing document probabilities
basis equation many redundant calculations slow runtime
iteration considerably improve computational performance proposed
inference procedure apply memoization techniques sampling within
single iteration document gibbs sampler requires computing documents
probability given topic assignments equation many times computation
frequently conditions slight variations topic assignments nave
would compute probability every paragraph time document probability
desired performing redundant calculations topic assignment sequences shared
subsequences repeatedly considered
instead use lazy evaluation build three dimensional cache indexed tuple
j k follows time document probability requested broken independent subspans paragraphs subspan takes one contiguous topic assignment possible due way equation factorizes independent per topic
particular use slicesample function matlab statistics toolbox



ficontent modeling latent permutations

multiplicands subspan starting paragraph ending paragraph j assigned topic k cache consulted key j k example topic assignments
zd would cache lookups
cached value unavailable correct probability computed equation
stored cache location j k moreover record values every
intermediate cache location l k l j values computed
subproblems evaluating equation j k cache reset proceeding
next document since conditioning changes documents document
caching guarantees nd k paragraph probability calculations
practice individual gibbs steps small bound loose
caching mechanism reduces computation time several orders magnitude
maintain caches word topic paragraph topic assignment frequencies
allowing us rapidly compute counts used equations form caching
used griffiths steyvers

applications
section describe model applied three challenging discourselevel tasks aligning paragraphs similar topical content documents segmenting
document topically cohesive sections ordering unseen paragraphs
coherent document particular posterior samples produced
inference procedure section used derive solution tasks
alignment
alignment task wish paragraphs document topically
relate paragraphs documents essentially cross document clustering
task alignment assigns paragraph document one k topically related
groupings instance given set cell phone reviews one group may represent text
fragments discuss price another group consists fragments reception
model readily employed task view topic assignment
paragraph z cluster label example two documents
topic assignments zd zd paragraph
grouped together paragraphs paragraphs
remaining paragraphs assigned topics form separate
per document clusters
previously developed methods cross document alignment primarily driven
similarity functions quantify lexical overlap textual units barzilay elhadad nelken shieber methods explicitly model document
structure specify global constraints guide search optimal
alignment pairs textual units considered isolation making alignment decisions contrast allows us take advantage global structure shared
language across related textual units without requiring manual specification
matching constraints



fichen branavan barzilay karger

segmentation
segmentation well studied discourse task goal divide document
topically cohesive contiguous sections previous approaches typically relied lexical
cohesion similarity word choices within document subspan guide
choice segmentation boundaries hearst van mulbregt carp gillick lowe
yamron blei moreno utiyama isahara galley mckeown foslerlussier jing purver et al malioutov barzilay eisenstein barzilay
model relies notion determining language topics
connecting topics across documents constraining topics appear allow
better learn words indicative topic cohesion
output samples inference procedure map straightforwardly
segmentations contiguous spans paragraphs assigned topic number taken one segment example seven paragraph document topic
assignments zd would segmented three sections comprised
paragraph paragraphs paragraphs note segmentation ignores specific values used topic assignments heeds paragraph
boundaries topic assignments change
ordering
third application model creating structured documents
collections unordered text segments text ordering task important step
broader nlp tasks text summarization generation task assume
provided well structured documents single domain training examples
trained model used induce ordering previously unseen collections
paragraphs domain
training model learns canonical ordering topics documents within
collection via language associated topic gmm
concentrates probability mass around canonical k topic ordering expect
highly probable words language lower numbered topics tend appear early
document whereas highly probable words language higher numbered
topics tend appear late document thus structure documents according
intuition paragraphs words tied low topic numbers placed earlier
paragraphs words relating high topic numbers
formally given unseen document comprised unordered set paragraphs
p pn order paragraphs according following procedure first
probable topic assignment zi independently paragraph pi according
parameters learned training phase
zi arg max p zi k pi
k

arg max p pi zi k k p zi k



k

second sort paragraphs topic assignment zi ascending order since k
gmms canonical ordering yields likely ordering conditioned single
estimated topic assignment paragraph due possible ties topic assignments


ficontent modeling latent permutations

resulting document may partial ordering full ordering required ties
broken arbitrarily
key advantage proposed closed form computationally
efficient though training phase requires running inference procedure section
model parameters learned predicting ordering set p paragraphs
requires computing pk probability scores contrast previous approaches
able rank small subset possible document reorderings barzilay
lapata performed search procedure space orderings
optimum elsner et al
objective function equation depends posterior estimates given
training documents since collapsed gibbs sampler integrates two hidden
variables need back values known posterior samples
z easily done computing point estimate distribution
word topic topic document assignment frequencies respectively done griffiths
steyvers probability mass kw word w language model topic k
given
n k w
kw


n k w
n k w total number times word w assigned topic k n k
total number words assigned topic k according posterior sample z
derive similar estimate k prior likelihood topic k
k

n k

n k



n k total number paragraphs assigned topic k according sample
z n total number paragraphs entire corpus

experiments
section evaluate performance model three tasks presented
section cross document alignment document segmentation information ordering
first describe preliminaries common three tasks covering data sets
reference comparison structures model variants inference settings shared
evaluation provide detailed examination model performs
individual task
general evaluation setup
data sets experiments use five data sets briefly described additional
statistics see table
describe finding probable paragraph ordering according
data likelihood optimal ordering derived hmm content model
proposed ordering technique essentially approximates objective per paragraph maximum
posteriori estimate topic assignments rather full posterior topic assignment distribution
approximation makes much faster prediction performs well empirically



fichen branavan barzilay karger

articles
corpus
citiesen
citiesen
citiesfr

large cities wikipedia
language documents sections
english


english


french



paragraphs




vocabulary




tokens




articles chemical elements wikipedia
corpus
language documents sections
elements
english



paragraphs


vocabulary


tokens


cell phone reviews phonearena com
corpus
language documents sections
phones
english



paragraphs


vocabulary


tokens


table statistics data sets used evaluations values except vocabulary
size document count per document averages

citiesen articles english wikipedia worlds largest cities
population common topics include history culture demographics articles typically substantial size share similar content organization patterns
citiesen articles english wikipedia worlds largest cities
population collection superset citiesen many lower ranked
cities well known english wikipedia editors thus compared citiesen
articles shorter average exhibit greater variability content selection
ordering
citiesfr articles french wikipedia cities citiesen
elements articles english wikipedia chemical elements periodic table including topics biological role occurrence isotopes
phones reviews extracted phonearena com popular cell phone review website topics corpus include design camera interface reviews
written expert reviewers employed site opposed lay users
heterogeneous collection data sets allows us examine behavior
model diverse test conditions sets vary articles generated
language articles written subjects discuss
patterns topic organization vary greatly across domains instance within phones
corpus articles formulaic due centralized editorial control website
establishes consistent standards followed expert reviewers hand
wikipedia articles exhibit broader structural variability due collaborative nature
elements http en wikipedia org wiki periodic table including undiscovered element
phones set documents short express reviews without section headings include
input model evaluate



ficontent modeling latent permutations

wikipedia editing allows articles evolve independently wikipedia articles
within category often exhibit similar section orderings many idiosyncratic
inversions instance citiesen corpus geography history sections
typically occur toward beginning document history appear
geography across different documents
corpus consider manually divided sections authors
including short textual heading section sections discuss
author created sections headings used generate reference annotations
alignment segmentation tasks note use headings evaluation
none heading information provided methods consideration
tasks alignment segmentation evaluation performed datasets presented
table ordering task however data used training evaluation
performed separate held set documents details held dataset
given section
model variants evaluation besides comparing baselines literature
consider two variants proposed model particular investigate
impact mallows component model alternately relaxing tightening
way constrains topic orderings
constrained variant require documents follow exact canonical ordering topics topic permutation inversions allowed though
documents may skip topics case viewed special case
general model mallows inversion prior approaches infinity
implementation standpoint simply fix inversion counts v zero
inference
uniform variant assumes uniform distribution topic permutations
instead biasing toward small related set special case full
model inversion prior set zero strength prior approaching
infinity thus forcing item zero
note variants still enforce long range constraint topic contiguity
vary full model capture topic ordering similarity
evaluation procedure parameter settings evaluation model
variants run collapsed gibbs sampler five random seed states take
th iteration chain sample presented average
five samples
dirichlet prior hyperparameters bag topics language set
gmm set prior dispersion hyperparameter effective
first glance constrained model variant appears equivalent hmm state
transition however case topics may appear zero times
document resulting multiple possible transitions state furthermore transition
probabilities would dependent position within document example earlier absolute
positions within document transitions high index topics unlikely would require
subsequent paragraphs high index topic



fichen branavan barzilay karger

sample size prior times number documents values minimally
tuned similar achieved alternative settings parameters
control strength bias toward structural regularity trading
constrained uniform model variants values chosen middle ground
two extremes
model takes parameter k controls upper bound number
latent topics note select fewer k topics document
k determine number segments document general higher k
finer grained division document different topics may
precise topics may split topics together report
evaluation k
alignment
first evaluate model task cross document alignment goal
group textual units different documents topically cohesive clusters instance
cities related domains one cluster may include transportation related paragraphs turning first present details specific evaluation setup
targeted task
alignment evaluation setup
reference annotations generate sufficient amount reference data evaluating alignments use section headings provided authors assume two
paragraphs aligned section headings identical headings
constitute noisy annotations wikipedia datasets topical content may
labeled different section headings different articles e g citiesen places
interest one article landmarks another call reference structure
noisy headings set
clear priori effect noise section headings may evaluation accuracy empirically estimate effect use manually annotated
alignments experiments specifically citiesen corpus manually annotated articles paragraphs consistent set section headings providing us
additional reference structure evaluate clean headings set found
approximately topics expressed one document
metrics quantify alignment output compute recall precision score
candidate alignment reference alignment recall measures unique
section heading reference maximum number paragraphs heading
assigned one particular topic final score computed summing
section heading dividing total number paragraphs high recall indicates
paragraphs section headings generally assigned topic
conversely precision measures topic number maximum number paragraphs topic assignment share section heading precision summed
topic normalized total number paragraphs high precision means
paragraphs assigned single topic usually correspond section heading



ficontent modeling latent permutations

recall precision trade finely grained topics tend
improve precision cost recall extremes perfect recall occurs every
paragraph assigned topic perfect precision paragraph
topic
present one summary f score harmonic mean
recall precision
statistical significance setup measured approximate randomization noreen
nonparametric test directly applied nonlinearly computed metrics
f score test used prior evaluations information extraction
machine translation chinchor riezler maxwell
baselines

task compare two baselines

hidden topic markov model htmm gruber et al explained section model represents topic change adjacent textual units markovian fashion htmm capture local constraints would allow topics
recur non contiguously throughout document use publicly available implementation priors set according recommendations made original
work
clustering use repeated bisection clustering paragraphs maximizes sum pairwise cosine similarities items
cluster clustering implemented cluto toolkit note
completely structure agnostic treating documents bags paragraphs rather sequences paragraphs types clustering techniques
shown deliver competitive performance cross document alignment
tasks barzilay elhadad
alignment
table presents alignment evaluation datasets best
performance achieved model variants statistically significant usually
substantial margin
comparative performance baseline methods consistent across domains
surprisingly clustering performs better complex htmm model observation consistent previous work cross document alignment multidocument
summarization use clustering main component radev jing budzikowska
barzilay mckeown elhadad despite fact htmm captures
dependencies adjacent paragraphs sufficiently constrained manual examination actual topic assignments reveals htmm often assigns topic
disconnected paragraphs within document violating topic contiguity constraint
one domain full gmm yields best performance compared variants one exception phone domain constrained
http code google com p openhtmm
particular clustering technique substantially outperforms agglomerative graph partitioningbased clustering approaches task
http glaros dtc umn edu gkhome views cluto



fik

k

k

k

chen branavan barzilay karger

clustering
htmm
constrained
uniform
model
clustering
htmm
constrained
uniform
model

citiesen
clean headings
recall
prec
f score





















citiesen
noisy headings
recall
prec
f score





















citiesen
noisy headings
recall
prec
f score





















clustering
htmm
constrained
uniform
model
clustering
htmm
constrained
uniform
model

citiesfr
noisy headings
recall
prec
f score






















elements
noisy headings
recall
prec
f score





















phones
noisy headings
recall
prec
f score























table comparison alignments produced model series baselines
model variations topics evaluated clean noisy sets
section headings higher scores better within k methods
model significantly outperforms indicated p
p



ficontent modeling latent permutations

baseline achieves best k small margin
expected given fact domain exhibits highly rigid topic structure across
documents model permits permutations topic ordering gmm
flexible highly formulaic domains
finally observe evaluations manual noisy annotations exhibit
almost entirely consistent ranking methods consideration see clean
noisy headings citiesen table consistency indicates noisy
headings sufficient gaining insight comparative performance different
approaches
segmentation
next consider task text segmentation test whether model able
identify boundaries topically coherent text segments
segmentation evaluation setup
reference segmentations described section datasets used
evaluation manually divided sections authors annotations
used create reference segmentations evaluating output recall
section built clean reference structure citiesen set structure encodes clean segmentation document adjusts granularity
section headings consistent across documents thus compare
segmentation specified citiesen clean section headings
metrics segmentation quality evaluated standard penalty metrics pk
windowdiff beeferman berger lafferty pevzner hearst pass
sliding window documents compute probability words end
windows improperly segmented respect windowdiff
stricter requires number segmentation boundaries endpoints
window correct well
baselines first compare bayesseg eisenstein barzilay bayesian
segmentation current state art task interestingly
model reduces every document considered completely isolation
topic sharing documents connecting topics across documents makes
much difficult inference one tackled eisenstein barzilay
time cannot capture structural relatedness across documents
since bayesseg designed operated specification number segments
provide baseline benefit knowing correct number segments
document provided system run baseline
statistical significance testing standardized usually reported segmentation task
omit tests
evaluate corpora used work since model relies content similarity across
documents corpus



fibayesseg
u
u
constrained
uniform
model
constrained
uniform
model

citiesen
clean headings
pk
wd
segs


























citiesen
noisy headings
pk
wd
segs


























citiesen
noisy headings
pk
wd
segs


























bayesseg
u
u
constrained
uniform
model
constrained
uniform
model

citiesfr
noisy headings
pk
wd
segs

























elements
noisy headings
pk
wd
segs


























phones
noisy headings
pk
wd
segs


























k k

k k

chen branavan barzilay karger

table comparison segmentations produced model series baselines
model variations topics evaluated clean noisy
sets section headings lower scores better bayesseg u given
true number segments segments counts reflect reference structures
segmentations contrast u automatically predicts number segments



ficontent modeling latent permutations

authors publicly available implementation priors set built mechanism
automatically estimates hyperparameters
compare method utiyama isahara
commonly used point reference evaluation segmentation
computes optimal segmentation estimating changes predicted
language segments different partitions used publicly available
implementation system require parameter tuning held
development set contrast bayesseg mechanism predicting
number segments take pre specified number segments
comparison consider versions u denotes case
correct number segments provided model u denotes model
estimates optimal number segments
segmentation
table presents segmentation experiment every data set model outperforms bayesseg u baselines substantial margin regardless k
provides strong evidence learning connected topic related documents leads
improved segmentation performance
best performance generally obtained full version model three
exceptions two cases citiesen k clean headings windowdiff
metric citiesfr k pk metric variant performs better
full model minute margin furthermore instances
corresponding evaluation k full model leads best overall
respective domains
case variant outperforms full model notable margin
phones data set unexpected given formulaic nature dataset
discussed earlier
ordering
final task evaluate model finding coherent ordering
set textual units unlike previous tasks prediction hidden variable
distributions ordering observed document moreover gmm model uses
information inference process therefore need divide data sets
training test portions
past ordering applied textual units granularities commonly sentences paragraphs ordering experiments operate
level relatively larger unit sections believe granularity suitable
nature model captures patterns level topic distributions
rather local discourse constraints ordering sentences paragraphs
studied past karamanis et al barzilay lapata two types
effectively combined induce full ordering elsner et al
http groups csail mit edu rbg code bayesseg
http www nict go jp x x members mutiyama software html textseg



fichen branavan barzilay karger

corpus
citiesen
citiesfr
phones

set
training
testing
training
testing
training
testing

documents







sections







paragraphs







vocabulary







tokens







table statistics training test sets used ordering experiments values
except vocabulary average per document training set statistics
reproduced table ease reference

ordering evaluation setup
training test data sets use citiesen citiesfr phones data sets
training documents parameter estimation described section introduce
additional sets documents domains test sets table provides statistics
training test set splits note vocabulary terms test sets
discarded
even though perform ordering section level collections still pose
challenging ordering task example average number sections citiesen test
document comparable sentences unit reordering per document
national transportation safety board corpus used previous work barzilay lee
elsner et al
metrics report kendalls rank correlation coefficient ordering experiments metric measures much ordering differs reference order
underlying assumption reasonable sentence orderings fairly similar
specifically permutation sections n section document
computed

n



kendall distance number swaps adjacent textual
units necessary rearrange reference order metric ranges inverse
orders identical orders note random ordering yield zero score expectation measure widely used evaluating information ordering lapata
barzilay lee elsner et al shown correlate human
assessments text quality lapata
baselines model variants ordering method compared original
hmm content modeling barzilay lee baseline delivers
elements data set limited articles preventing us splitting reasonably sized
training test sets therefore consider ordering experiments citiesrelated sets test documents shorter cities lesser population
hand phones test set include short express reviews thus exhibits higher
average document length



ficontent modeling latent permutations

hmm content model
constrained
k
model
constrained
k
model

citiesen






citiesfr






phones






table comparison orderings produced model series baselines
model variations topics evaluated respective test sets
higher scores better

state art performance number datasets similar spirit model
aims capture patterns level topic distribution see section
use publicly available implementation parameters adjusted according
values used previous work content modeling implementation provides
search procedure use optimal permutation
include comparison local coherence barzilay lapata
elsner et al designed sentence level analysis particular
use syntactic information thus cannot directly applied section level ordering
state orthogonal topic analysis combining two
approaches promising direction future work
note uniform model variant applicable task since
make claims preferred underlying topic ordering fact document likelihood perspective proposed paragraph order reverse order would
probability uniform model thus model variant consider
constrained
ordering
table summarizes ordering gmm hmm content across
data sets model outperforms content modeling large margin instance
citiesen dataset gap two reaches difference
expected previous work content applied short formulaic texts
contrast documents collection exhibit higher variability original collections
hmm provide explicit constraints generated global orderings may
prevent effectively learning non local patterns topic organization
observe constrained variant outperforms full model
difference two small fairly consistent across domains since
possible predict idiosyncratic variations test documents topic orderings
constrained model better capture prevalent ordering patterns consistent
across domain
http people csail mit edu regina code html



fichen branavan barzilay karger

discussion
experiments three separate tasks reveal common trends
first observe single unified model document structure readily
successfully applied multiple discourse level tasks whereas previous work proposed
separate approaches task versatility speaks power topic driven
representation document structure second within task model outperforms
state art baselines substantial margins across wide variety evaluation scenarios strongly support hypothesis augmenting topic
discourse level constraints broadens applicability discourse level analysis tasks
looking performance model across different tasks make notes
importance individual topic constraints topic contiguity consistently
important constraint allowing model variants outperform alternative baseline
approaches cases introducing bias toward similar topic ordering without requiring identical orderings provides benefits encoded model
flexible achieve superior performance segmentation alignment tasks
case ordering however extra flexibility pay model distributes
probability mass away strong ordering patterns likely occur unseen data
identify properties dataset strongly affect performance
model constrained model variant performs slightly better full model
rigidly formulaic domains achieving highest performance phones data set
know priori domain formulaic structure worthwhile choose
model variant suitably enforces formulaic topic orderings fortunately adaptation
achieved proposed framework prior generalized mallows
model recall constrained variant special case full model
however performance model invariant respect data set characteristics across two languages considered model baselines exhibit
comparative performance task moreover consistency holds
general interest cities articles highly technical chemical elements articles finally smaller citiesen larger citiesen data sets observe
consistent

conclusions future work
shown unsupervised topic capture content
structure resulting model constrains topic assignments way requires global
modeling entire topic sequences showed generalized mallows model
theoretically empirically appealing way capturing ordering component
topic sequence demonstrate importance augmenting statistical
text analysis structural constraints motivated discourse theory furthermore
success gmm suggests could applied modeling ordering
constraints nlp applications
multiple avenues future extensions work first empirical
demonstrated certain domains providing much flexibility model may
fact detrimental predictive accuracy cases tightly constrained
variant model yields superior performance interesting extension current


ficontent modeling latent permutations

model would allow additional flexibility prior gmm drawing
another level hyperpriors technical perspective form hyperparameter
estimation would involve defining appropriate hyperprior generalized mallows
model adapting estimation present inference procedure
additionally may cases assumption one canonical topic ordering
entire corpus limiting e g data set consists topically related articles
multiple sources editorial standards model extended
allow multiple canonical orderings positing additional level hierarchy
probabilistic model e document structures generated mixture several
generalized mallows distributional mode case
model would take additional burden learning topics permuted
multiple canonical orderings change model would greatly complicate
inference estimating generalized mallows model canonical ordering general nphard however recent advances statistics produced efficient approximate
theoretically guaranteed correctness bounds ailon charikar newman
exact methods tractable typical cases meila et al
generally model presented assumes two specific global constraints
content structure domains satisfy constraints plentiful
domains modeling assumptions hold example dialogue well
known topics recur throughout conversation grosz sidner thereby violating
first constraint nevertheless texts domains still follow certain organizational
conventions e g stack structure dialogue suggest explicitly incorporating domain specific global structural constraints content model would likely
improve accuracy structure induction
another direction future work combine global topic structure model
local coherence constraints previously noted model agnostic toward
relationships sentences within single topic contrast local coherence
take advantage wealth additional knowledge syntax make decisions
information flow across adjoining sentences linguistically rich model would provide
powerful representation levels textual structure could used even
greater variety applications considered

bibliographic note
portions work previously presented conference publication chen branavan
barzilay karger article significantly extends previous work notably
introducing applying output information ordering
task section considering data sets experiments vary genre
language size section

acknowledgments
authors acknowledge funding support nsf career grant iis
grant iis nsf graduate fellowship office naval quanta
nokia microsoft faculty fellowship thank many people offered



fichen branavan barzilay karger

suggestions comments work including michael collins aria haghighi yoong
keok lee marina meila tahira naseem christy sauper david sontag benjamin snyder
luke zettlemoyer especially grateful marina meila introducing us
mallows model greatly benefited thoughtful feedback
anonymous reviewers opinions findings conclusions recommendations expressed
authors necessarily reflect views funding
organizations



ficontent modeling latent permutations

references
ailon n charikar newman aggregating inconsistent information
ranking clustering journal acm
althaus e karamanis n koller computing locally coherent discourses
proceedings acl
bartlett f c remembering study experimental social psychology cambridge university press
barzilay r elhadad n sentence alignment monolingual comparable corpora
proceedings emnlp
barzilay r elhadad n mckeown k inferring strategies sentence ordering
multidocument news summarization journal artificial intelligence

barzilay r lapata modeling local coherence entity
computational linguistics
barzilay r lee l catching drift probabilistic content applications generation summarization proceedings naacl hlt
barzilay r mckeown k elhadad information fusion context
multi document summarization proceedings acl
beeferman berger lafferty j statistical text segmentation
machine learning
bernardo j smith f bayesian theory wiley series probability
statistics
bishop c pattern recognition machine learning springer
blei moreno p j topic segmentation aspect hidden markov
model proceedings sigir
blei ng jordan latent dirichlet allocation journal machine
learning
bollegala okazaki n ishizuka bottom sentence
ordering multi document summarization proceedings acl coling
chen h branavan barzilay r karger r global document
structure latent permutations proceedings naacl hlt
chinchor n statistical significance muc proceedings th
conference message understanding
cohen w w schapire r e singer learning order things journal
artificial intelligence
eisenstein j barzilay r bayesian unsupervised topic segmentation proceedings emnlp
elsner austerweil j charniak e unified local global model
discourse coherence proceedings naacl hlt


fichen branavan barzilay karger

fligner verducci j distance ranking journal royal
statistical society series b
fligner verducci j posterior probabilities consensus ordering
psychometrika
galley mckeown k r fosler lussier e jing h discourse segmentation
multi party conversation proceedings acl
geman geman stochastic relaxation gibbs distributions bayesian
restoration images ieee transactions pattern analysis machine intelligence
graesser gernsbacher goldman eds handbook discourse
processes erlbaum
griffiths l steyvers finding scientific topics proceedings national
academy sciences
griffiths l steyvers blei tenenbaum j b integrating topics
syntax advances nips
grosz b j sidner c l attention intentions structure discourse
computational linguistics
gruber rosen zvi weiss hidden topic markov proceedings
aistats
halliday k hasan r cohesion english longman
hearst multi paragraph segmentation expository text proceedings
acl
ji p pulman sentence ordering manifold classification
multi document summarization proceedings emnlp
karamanis n poesio mellish c oberlander j evaluating centeringbased metrics coherence text structuring reliably annotated corpus
proceedings acl
klementiev roth small k unsupervised rank aggregation distancebased proceedings icml pp
lapata probabilistic text structuring experiments sentence ordering
proceedings acl
lapata automatic evaluation information ordering kendalls tau computational linguistics
lebanon g lafferty j cranking combining rankings conditional probability permutations proceedings icml
malioutov barzilay r minimum cut model spoken lecture segmentation
proceedings acl
mallows c l non null ranking biometrika



ficontent modeling latent permutations

meila phadnis k patterson bilmes j consensus ranking
exponential model proceedings uai
neal r slice sampling annals statistics
nelken r shieber towards robust context sensitive sentence alignment
monolingual corpora proceedings eacl
noreen e w computer intensive methods testing hypotheses introduction wiley
pevzner l hearst critique improvement evaluation metric
text segmentation computational linguistics
porteous newman ihler asuncion smyth p welling fast
collapsed gibbs sampling latent dirichlet allocation proceedings sigkdd
purver kording k griffiths l tenenbaum j b unsupervised topic
modelling multi party spoken discourse proceedings acl coling
radev jing h budzikowska centroid summarization multiple documents sentence extraction utility evaluation user studies
proceedings anlp naacl summarization workshop
riezler maxwell j pitfalls automatic evaluation significance testing mt proceedings acl workshop intrinsic extrinsic
evaluation measures machine translation summarization
schiffrin tannen hamilton h e eds handbook discourse
analysis blackwell
titov mcdonald r modeling online reviews multi grain topic
proceedings www
utiyama isahara h statistical model domain independent text segmentation proceedings acl
van mulbregt p carp gillick l lowe yamron j text segmentation
topic tracking broadcast news via hidden markov model proceedings
icslp
wallach h topic modeling beyond bag words proceedings icml
wray formulaic language lexicon cambridge university press cambridge




