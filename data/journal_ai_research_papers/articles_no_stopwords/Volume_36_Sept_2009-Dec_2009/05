Journal Artificial Intelligence Research 36 (2009) 267-306

Submitted 06/09; published 10/09

ParamILS: Automatic Algorithm Configuration Framework
Frank Hutter
Holger H. Hoos
Kevin Leyton-Brown

HUTTER @ CS . UBC . CA
HOOS @ CS . UBC . CA
KEVINLB @ CS . UBC . CA

University British Columbia, 2366 Main Mall
Vancouver, BC, V6T1Z4, Canada

Thomas Stutzle

STUETZLE @ ULB . AC .

Universite Libre de Bruxelles, CoDE, IRIDIA
Av. F. Roosevelt 50 B-1050 Brussels, Belgium

Abstract
identification performance-optimizing parameter settings important part development application algorithms. describe automatic framework algorithm
configuration problem. formally, provide methods optimizing target algorithms
performance given class problem instances varying set ordinal and/or categorical parameters. review family local-search-based algorithm configuration procedures
present novel techniques accelerating adaptively limiting time spent evaluating individual configurations. describe results comprehensive experimental evaluation
methods, based configuration prominent complete incomplete algorithms
SAT. present is, knowledge, first published work automatically configuring C PLEX mixed integer programming solver. algorithms considered default
parameter settings manually identified considerable effort. Nevertheless, using
automated algorithm configuration procedures, achieved substantial consistent performance
improvements.

1. Introduction
Many high-performance algorithms parameters whose settings control important aspects
behaviour. particularly case heuristic procedures used solving computationally hard problems.1 example, consider C PLEX, commercial solver mixed integer
programming problems.2 CPLEX version 10 80 parameters affect solvers search
mechanism configured user improve performance. many acknowledgements literature finding performance-optimizing parameter configurations heuristic algorithms often requires considerable effort (see, e.g., Gratch & Chien, 1996; Johnson, 2002;
Diao, Eskesen, Froehlich, Hellerstein, Spainhower & Surendra, 2003; Birattari, 2004; Adenso-Diaz
& Laguna, 2006). many cases, tedious task performed manually ad-hoc way. Automating task high practical relevance several contexts.
Development complex algorithms Setting parameters heuristic algorithm
highly labour-intensive task, indeed consume large fraction overall development
1. use term heuristic algorithm includes methods without provable performance guarantees well
methods guarantees, nevertheless make use heuristic mechanisms. latter case, use
heuristic mechanisms often results empirical performance far better bounds guaranteed rigorous
theoretical analysis.
2. http://www.ilog.com/products/cplex/
c
2009
AI Access Foundation. rights reserved.

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

time. use automated algorithm configuration methods lead significant time
savings potentially achieve better results manual, ad-hoc methods.
Empirical studies, evaluations, comparisons algorithms central question comparing heuristic algorithms whether one algorithm outperforms another fundamentally superior, developers successfully optimized parameters (Johnson, 2002). Automatic algorithm configuration methods mitigate problem unfair
comparisons thus facilitate meaningful comparative studies.
Practical use algorithms ability complex heuristic algorithms solve large
hard problem instances often depends critically use suitable parameter settings.
End users often little knowledge impact algorithms parameter
settings performance, thus simply use default settings. Even carefully
optimized standard benchmark set, default configuration may perform well
particular problem instances encountered user. Automatic algorithm configuration
methods used improve performance principled convenient way.
wide variety strategies automatic algorithm configuration explored literature. Briefly, include exhaustive enumeration, hill-climbing (Gratch & Dejong, 1992), beam
search (Minton, 1993), genetic algorithms (Terashima-Marn, Ross & Valenzuela-Rendon, 1999),
experimental design approaches (Coy, Golden, Runger & Wasil, 2001), sequential parameter optimization (Bartz-Beielstein, 2006), racing algorithms (Birattari, Stutzle, Paquete & Varrentrapp,
2002; Birattari, 2004; Balaprakash, Birattari & Stutzle, 2007), combinations fractional experimental design local search (Adenso-Diaz & Laguna, 2006). discuss
related work extensively Section 9. Here, note authors refer
optimization algorithms performance setting (typically numerical) parameters
parameter tuning, favour term algorithm configuration (or simply, configuration).
motivated fact interested methods deal potentially large number
parameters, numerical, ordinal (e.g., low, medium, high) categorical (e.g., choice heuristic). Categorical parameters used select combine discrete
building blocks algorithm (e.g., preprocessing variable ordering heuristics); consequently,
general view algorithm configuration includes automated construction heuristic algorithm building blocks. best knowledge, methods discussed article
yet general ones available configuration algorithms many categorical
parameters.
give overview follows highlight main contributions. formally stating algorithm configuration problem Section 2, Section 3 describe ParamILS
(first introduced Hutter, Hoos & Stutzle, 2007), versatile stochastic local search approach
automated algorithm configuration, two instantiations, BasicILS FocusedILS.
introduce adaptive capping algorithm runs, novel technique used
enhance search-based algorithm configuration procedures independently underlying search
strategy (Section 4). Adaptive capping based idea avoiding unnecessary runs
algorithm configured developing bounds performance measure optimized.
present trajectory-preserving variant heuristic extension technique. discussing experimental preliminaries Section 5, Section 6 present empirical evidence showing adaptive capping speeds BasicILS FocusedILS. show BasicILS
268

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

outperforms random search simple local search, well evidence FocusedILS
outperforms BasicILS.
present extensive evidence ParamILS find substantially improved parameter configurations complex highly optimized algorithms. particular, apply automatic
algorithm configuration procedures aforementioned commercial optimization tool C PLEX,
one powerful, widely used complex optimization algorithms aware of.
stated C PLEX user manual (version 10.0, page 247), great deal algorithmic development effort devoted establishing default ILOG C PLEX parameter settings achieve
good performance wide variety MIP models. demonstrate consistent improvements
default parameter configuration wide range practically relevant instance distributions. cases, able achieve average speedup order magnitude
previously-unseen test instances (Section 7). believe first results
published automatically configuring C PLEX piece software comparable complexity.
Section 8 review wide range (separately-published) ParamILS applications. Specifically, survey work considered optimization complete incomplete heuristic
search algorithms problems propositional satisfiability (SAT), probable explanation
(MPE), protein folding, university time-tabling, algorithm configuration itself. three
cases, ParamILS integral part algorithm design process allowed exploration
large design spaces. could done effectively manual way
existing automated method. Thus, automated algorithm configuration general ParamILS
particular enables new way (semi-)automatic design algorithms components.
Section 9 presents related work and, finally, Section 10 offers discussion conclusions.
distill common patterns helped ParamILS succeed various applications.
give advice practitioners would apply automated algorithm configuration general
ParamILS particular, identify promising avenues research future work.

2. Problem Statement Notation
algorithm configuration problem consider work informally stated follows:
given algorithm, set parameters algorithm set input data, find parameter
values algorithm achieves best possible performance input data.
avoid potential confusion algorithms whose performance optimized algorithms used carrying optimization task, refer former target algorithms
latter configuration procedures (or simply configurators). setup illustrated
Figure 1. Different algorithm configuration problems considered literature, including setting parameters per-instance basis adapting parameters algorithm
running. defer discussion approaches Section 9.
following, define algorithm configuration problem formally introduce
notation use throughout article. Let denote algorithm, let p1 , . . . , pk
parameters A. Denote domain possible values parameter pi . Throughout
work, assume parameter domains finite sets. assumption met
discretizing numerical parameters finite number values. Furthermore, parameters
269

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

Figure 1: configuration scenario includes algorithm configured collection problem instances. configuration procedure executes target algorithm specified parameter settings
instances, receives information performance runs, uses
information decide subsequent parameter configurations evaluate.

may ordered, exploit ordering relations. Thus, effectively assume
parameters finite categorical.3
problem formulation allows us express conditional parameter dependencies (for example,
one algorithm parameter might used select among search heuristics, heuristics
behaviour controlled parameters). case, values parameters
irrelevant heuristic selected. ParamILS exploits effectively searches
space equivalence classes parameter configuration space. addition, formulation supports
constraints feasible combinations parameter values. use 1 . . . k denote
space feasible parameter configurations, A() denoting instantiation algorithm
parameter configuration .
Let denote probability distribution space problem instances, denote element . may given implicitly, random instance generator distribution
generators. possible (and indeed common) consist finite sample
instances; case, define uniform distribution .
many ways measuring algorithms performance. example, might interested minimizing computational resources consumed given algorithm (such runtime,
memory communication bandwidth), maximizing quality solution found. Since
high-performance algorithms computationally-challenging problems often randomized,
behaviour vary significantly multiple runs. Thus, algorithm always achieve
performance, even run repeatedly fixed parameters single problem instance. overall goal must therefore choose parameter settings minimize cost
statistic algorithms performance across input data. denote statistic c().
example, might aim minimize mean runtime median solution cost.
intuition mind, define algorithm configuration problem formally.
Definition 1 (Algorithm Configuration Problem). instance algorithm configuration problem 6-tuple hA, , D, max , o, mi, where:
parameterized algorithm;
parameter configuration space A;
3. currently extending algorithm configuration procedures natively support parameter types.

270

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

distribution problem instances domain ;
max cutoff time (or captime), run terminated still running;
function measures observed cost running A() instance
captime R (examples runtime solving instance, cost solution found)
statistical population parameter (such expectation, median, variance).
parameter configuration candidate solution algorithm configuration
problem. configuration , denotes distribution costs induced function o,
applied instances drawn distribution multiple independent runs randomized
algorithms, using captime = max . cost candidate solution defined
c() := m(O ),

(1)

statistical population parameter cost distribution . optimal solution, , minimizes c():
arg min c().
(2)


algorithm configuration procedure procedure solving algorithm configuration
problem. Unfortunately, least algorithm configuration problems considered article, cannot optimize c closed form since access algebraic representation function. denote sequence runs executed configurator R =
((1 , 1 , s1 , 1 , o1 ), . . . , (n , n , sn , n , )). ith run described five values:
denotes parameter configuration evaluated;
denotes instance algorithm run;
si denotes random number seed used run (we keep track seeds able block
them, see Section 5.1.2);
denotes runs captime;
oi denotes observed cost run
Note , , s, , vary one element R next, regardless whether
elements held constant. denote ith run R R[i], subsequence
runs using parameter configuration (i.e., runs = ) R . configuration
procedures considered article compute empirical estimates c() based solely R ,
principle methods could used. compute cost estimates online,
runtime configurator, well offline, evaluation purposes.
Definition 2 (Cost Estimate). Given algorithm configuration problem hA, , D, max , o, mi,
define cost estimate cost c() based sequence runs R = ((1 , 1 , s1 , 1 , o1 ), . . . ,
(n , n , sn , n , )) c(, R) := m({oi | = }), sample statistic analogue
statistical population parameter m.
example, c() expected runtime distribution instances random number
seeds, c(, R) sample mean runtime runs R .
configuration procedures paper anytime algorithms, meaning times
keep track configuration currently believed lowest cost; refer configuration incumbent configuration, short incumbent, inc . evaluate configurators
performance time means incumbents training test performance, defined follows.
271

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

Definition 3 (Training performance). time configurator performed sequence runs R = ((1 , 1 , s1 , 1 , o1 ), . . . , (n , n , sn , n , )) solve algorithm configuration problem hA, , D, max , o, mi, thereby found incumbent configuration inc ,
training performance time defined cost estimate c(inc , R).
set instances {1 , . . . , n } discussed called training set. true cost
parameter configuration cannot computed exactly, estimated using training performance. However, training performance configurator biased estimator incumbents
true cost, instances used selecting incumbent evaluating it.
order achieve unbiased estimates offline evaluation, set aside fixed set instances
{10 , . . . , T0 } (called test set) random number seeds {s01 , . . . , s0T }, unknown
configurator, use evaluation.
Definition 4 (Test performance). time t, let configurators incumbent algorithm
configuration problem hA, , D, max , o, mi inc (this found means executing sequence runs training set). Furthermore, let R0 = ((inc , 10 , s01 , max , o1 ), . . . , (inc , T0 ,
s0T , max , oT )) sequence runs instances random number seeds test set
(which performed offline evaluation purposes), configurators test performance
time defined cost estimate c(inc , R0 ).
Throughout article, aim minimize expected runtime. (See Section 5.1.1 discussion
choice.) Thus, configurators training performance mean runtime runs
performed incumbent. test performance mean runtime incumbent
test set. Note that, configurator free use max , test performance always
computed using maximal captime, max .
obvious automatic algorithm configurator choose runs order best
minimize c() within given time budget. particular, make following choices:
1. parameter configurations 0 evaluated?
2. problem instances 0 used evaluating 0 0 ,
many runs performed instance?
3. cutoff time used run?
Hutter, Hoos Leyton-Brown (2009) considered design space detail, focusing
tradeoff (fixed) number problem instances used evaluation
parameter configuration (fixed) cutoff time used run, well interaction
choices number configurations considered. contrast, here, study
adaptive approaches selecting number problem instances (Section 3.3) cutoff
time evaluation parameter configuration (Section 4); study configurations
selected (Sections 3.1 6.2).

3. ParamILS: Iterated Local Search Parameter Configuration Space
section, address first important previously mentioned dimensions
automated algorithm configuration, search strategy, describing iterated local search
framework called ParamILS. start with, fix two dimensions, using unvarying
benchmark set instances fixed cutoff times evaluation parameter configuration. Thus, stochastic optimization problem algorithm configuration reduces simple
272

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

optimization problem, namely find parameter configuration yields lowest mean runtime given benchmark set. Then, Section 3.3, address second question many
runs performed configuration.
3.1 ParamILS framework
Consider following manual parameter optimization process:
1. begin initial parameter configuration;
2. experiment modifications single parameter values, accepting new configurations whenever result improved performance;
3. repeat step 2 single-parameter change yields improvement.
widely used procedure corresponds manually-executed local search parameter configuration space. Specifically, corresponds iterative first improvement procedure search
space consisting possible configurations, objective function quantifies performance
achieved target algorithm given configuration, neighbourhood relation based
modification one single parameter value time (i.e., one-exchange neighbourhood).
Viewing manual procedure local search algorithm advantageous suggests
automation procedure well improvement drawing ideas stochastic
local search community. example, note procedure stops soon reaches local optimum (a parameter configuration cannot improved modifying single parameter value).
sophisticated approach employ iterated local search (ILS; Lourenco, Martin & Stutzle,
2002) search performance-optimizing parameter configurations. ILS prominent stochastic
local search method builds chain local optima iterating main loop consisting
(1) solution perturbation escape local optima, (2) subsidiary local search procedure
(3) acceptance criterion decide whether keep reject newly obtained candidate solution.
ParamILS (given pseudocode Algorithm 1) ILS method searches parameter configuration space. uses combination default random settings initialization, employs
iterative first improvement subsidiary local search procedure, uses fixed number (s) random moves perturbation, always accepts better equally-good parameter configurations,
re-initializes search random probability prestart .4 Furthermore, based
one-exchange neighbourhood, is, always consider changing one parameter time.
ParamILS deals conditional parameters excluding configurations neighbourhood configuration differ conditional parameter relevant .
3.2 BasicILS Algorithm
order turn ParamILS specified Algorithm Framework 1 executable configuration
procedure, necessary instantiate function better determines two parameter settings preferred. ultimately propose several different ways this.
Here, describe simplest approach, call BasicILS. Specifically, use term
BasicILS(N ) refer ParamILS algorithm function better(1 , 2 ) implemented
shown Procedure 2: simply comparing estimates cN cost statistics c(1 ) c(2 )
based N runs each.
4. original parameter choices hr, s, prestart = h10, 3, 0.01i (from Hutter et al., 2007) somewhat arbitrary,
though expected performance quite robust respect settings. revisit issue Section 8.4.

273

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

Algorithm Framework 1: ParamILS(0 , r, prestart , s)
Outline iterated local search parameter configuration space; specific variants ParamILS
study, BasicILS(N) FocusedILS, derived framework instantiating procedure
better (which compares , 0 ). BasicILS(N) uses betterN (see Procedure 2), FocusedILS
uses betterF oc (see Procedure 3). neighbourhood Nbh() configuration set
configurations differ one parameter, excluding configurations differing conditional
parameter relevant .
Input : Initial configuration 0 , algorithm parameters r, prestart , s.
Output : Best parameter configuration found.
1 = 1, . . . , r
2
random ;
3
better(, 0 ) 0 ;
4
5
6

ils IterativeFirstImprovement (0 );
TerminationCriterion()
ils ;

7

// ===== Perturbation
= 1, . . . , random 0 Nbh();

8

// ===== Basic local search
IterativeFirstImprovement ();

9
10

// ===== AcceptanceCriterion
better(, ils ) ils ;
probability prestart ils random ;

11

return overall best inc found;

12
13
14
15
16

Procedure IterativeFirstImprovement ()
repeat
0 ;
foreach 00 N bh( 0 ) randomized order
better( 00 , 0 ) 00 ; break;

17
18

0 = ;
return ;

BasicILS(N ) simple intuitive approach since evaluates every parameter configuration
running N training benchmark instances using random number seeds.
many related approaches (see, e.g., Minton, 1996; Coy et al., 2001; Adenso-Diaz &
Laguna, 2006), deals stochastic part optimisation problem using estimate
based fixed training set N instances. benchmark instances heterogeneous
Procedure 2: betterN (1 , 2 )
Procedure used BasicILS(N ) RandomSearch(N ) compare two parameter configurations. Procedure objective(, N ) returns user-defined objective achieved A() first N instances
keeps track incumbent solution, inc ; detailed Procedure 4 page 279.
Input
: Parameter configuration 1 , parameter configuration 2
Output
: True 1 better equal 2 first N instances; false otherwise
Side Effect : Adds runs global caches performed algorithm runs R1 R2 ; potentially
updates incumbent inc
1 cN (2 ) objective(2 , N )
2 cN (1 ) objective(1 , N )
3 return cN (1 ) cN (2 )

274

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

user identify rather small representative subset instances, approach find
good parameter configurations low computational effort.
3.3 FocusedILS: Adaptively Selecting Number Training Instances
question choose number training instances, N , BasicILS(N ) straightforward answer: optimizing performance using small training set leads good training
performance, poor generalization previously unseen test benchmarks. hand,
clearly cannot evaluate every parameter configuration enormous training setif did,
search progress would unreasonably slow.
FocusedILS variant ParamILS deals problem adaptively varying
number training samples considered one parameter configuration another. denote
number runs available estimate cost statistic c() parameter configuration
N (). performed different numbers runs using different parameter configurations,
face question comparing two parameter configurations 0 N () N ( 0 ).
One option would simply compute empirical cost statistic based available number
runs configuration. However, lead systematic biases if, example, first
instances easier average instance. Instead, compare 0 based N () runs
instances seeds. amounts blocking strategy, straight-forward
adaptation known variance reduction technique; see 5.1 detailed discussion.
approach comparison leads us concept domination. say dominates 0
least many runs conducted 0 , performance A()
first N ( 0 ) runs least good A( 0 ) runs.
Definition 5 (Domination). 1 dominates 2 N (1 ) N (2 ) cN (2 ) (1 )
cN (2 ) (2 ).
ready discuss comparison strategy encoded procedure betterF oc (1 , 2 ),
used FocusedILS algorithm (see Procedure 3). procedure first acquires one
additional sample configuration smaller N (i ), one run configurations
number runs. Then, continues performing runs way one configuration dominates other. point returns true 1 dominates 2 , false otherwise.
keep track total number configurations evaluated since last improving step (i.e.,
since last time betterF oc returned true); denote number B. Whenever betterF oc (1 , 2 )
returns true, perform B bonus runs 1 reset B 0. mechanism ensures
perform many runs good configurations, error made every comparison two
configurations 1 2 decreases expectation.
difficult show limit, FocusedILS sample every parameter configuration
unbounded number times. proof relies fact that, instantiation ParamILS,
FocusedILS performs random restarts positive probability.
Lemma 6 (Unbounded number evaluations). Let N (J, ) denote number runs FocusedILS
performed parameter configuration end ILS iteration J estimate c(). Then,
constant K configuration (with finite ||), limJ P [N (J, ) K] = 1.
Proof. ILS iteration ParamILS, probability prestart > 0 new configuration
picked uniformly random, probability 1/||, configuration . probability
275

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

Procedure 3: betterF oc (1 , 2 )
Procedure used FocusedILS compare two parameter configurations. Procedure objective(, N )
returns user-defined objective achieved A() first N instances, keeps track incumbent solution, updates R (a global cache algorithm runs performed parameter configuration ); detailed Procedure 4 page 279. , N () = length(R ). B global
counter denoting number configurations evaluated since last improvement step.
Input
: Parameter configuration 1 , parameter configuration 2
Output
: True 1 dominates 2 , false otherwise
Side Effect: Adds runs global caches performed algorithm runs R1 R2 ; updates
global counter B bonus runs, potentially incumbent inc
1 B B+1
2 N (1 ) N (2 )
3
min 1 ; max 2
4
N (1 ) = N (2 ) B B + 1
else min 2 ; max 1
repeat
N (min ) + 1
ci (max ) objective(max , i) // N (min ) = N (max ), adds new run Rmax .
ci (min ) objective(min , i) // Adds new run Rmin .
10 dominates(1 , 2 ) dominates(2 , 1 )
11 dominates(1 , 2 )
5
6
7
8
9

// ===== Perform B bonus runs.
12
13
14

cN (1 )+B (1 ) objective(1 , N (1 ) + B) // Adds B new runs R1 .
B 0
return true

15

else return false

16
17
18

Procedure dominates(1 , 2 )
N (1 ) < N (2 ) return false
return objective(1 , N (2 )) objective(2 , N (2 ))

visiting ILS iteration thus p prestart
> 0. Hence, number runs performed
||
lower-bounded binomial random
variable B(k; J, p). Then, constant k < K obtain
limJ B(k; J, p) = limJ Jk pk (1 p)Jk = 0. Thus, limJ P [N (J, ) K] = 1.
Definition 7 (Consistent estimator). cN () consistent estimator c() iff
> 0 : lim P (|cN () c()| < ) = 1.
N

cN () consistent estimator c(), cost estimates become reliable
N approaches infinity, eventually eliminating overconfidence possibility mistakes
comparing two parameter configurations. fact captured following lemma.
Lemma 8 (No mistakes N ). Let 1 , 2 two parameter configurations
c(1 ) < c(2 ). Then, consistent estimators cN , limN P (cN (1 ) cN (2 )) = 0.
Proof. Write c1 shorthand c(1 ), c2 c(2 ), c1 cN (1 ), c2 cN (2 ). Define
= 21 (c2 + c1 ) midpoint c1 c2 , = c2 = c1 > 0
distance two points. Since cN consistent estimator c, estimate
c1 comes arbitrarily close real cost c1 . is, limN P (|c1 c1 | < ) = 1. Since
276

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

|m c1 | = , estimate c1 cannot greater equal m: limN P (c1 m) = 0.
Similarly, limN P (c2 < m) = 0. Since
P (c1 c2 ) = P (c1 c2 c1 m) + P (c1 c2 c1 < m)
= P (c1 c2 c1 m) + P (c1 c2 c1 < c2 < m)
P (c1 m) + P (c2 < m),
limN P (c1 c2 ) limN (P (c1 m) + P (c2 < m)) = 0 + 0 = 0.
Combining two lemmata show limit, FocusedILS guaranteed
converge true best parameter configuration.
Theorem 9 (Convergence FocusedILS). FocusedILS optimizes cost statistic c based
consistent estimator cN , probability finds true optimal parameter configuration
approaches one number ILS iterations goes infinity.
Proof. According Lemma 6, N () grows unboundedly . 1 , 2 ,
N (1 ) N (2 ) go infinity, Lemma 8 states pairwise comparison, truly better
configuration preferred. Thus eventually, FocusedILS visits finitely many parameter
configurations prefers best one others probability arbitrarily close one.
note many practical scenarios cost estimators may consistentthat is,
may fail closely approximate true performance given parameter configuration even
large number runs target algorithm. example, finite training set, , used
configuration rather distribution problem instances, D, even large N , cN
accurately reflect cost parameter configurations training set, . small
training sets, , cost estimate based may differ substantially true cost defined
performance across entire distribution, D. larger training set, , smaller
expected difference (it vanishes training set size goes infinity). Thus, important use
large training sets (which representative distribution interest) whenever possible.

4. Adaptive Capping Algorithm Runs
consider last dimensions automated algorithm configuration, cutoff time
run target algorithm. introduce effective simple capping technique
adaptively determines cutoff time run. motivation capping technique comes
problem encountered configuration procedures considered article: often
search performance-optimizing parameter setting spends lot time evaluating parameter configuration much worse other, previously-seen configurations.
Consider, example, case parameter configuration 1 takes total 10 seconds
solve N = 100 instances (i.e., mean runtime 0.1 seconds per instance), another parameter configuration 2 takes 100 seconds solve first instances. order compare
mean runtimes 1 2 based set instances, knowing runtimes 1 ,
necessary run 2 100 instances. Instead, already terminate first run 2
10 + seconds. results lower bound 2 mean runtime 0.1 + /100 since
remaining 99 instances could take less zero time. lower bound exceeds mean
runtime 1 , already certain comparison favour 1 . insight
provides basis adaptive capping technique.
277

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

4.1 Adaptive Capping BasicILS
section, introduce adaptive capping BasicILS. first introduce trajectory-preserving
version adaptive capping (TP capping) provably change BasicILSs search trajectory lead large computational savings. modify strategy heuristically
perform aggressive adaptive capping (Aggr capping), potentially yielding even better performance practice.
4.1.1 RAJECTORY- PRESERVING C APPING
Observe comparisons parameter configurations ParamILS pairwise.
BasicILS(N ), comparisons based Procedure betterN (1 , 2 ), 2 either
best configuration encountered ILS iteration best configuration last ILS iteration. Without adaptive capping, comparisons take long time, since poor parameter
configuration easily take order magnitude longer good configurations.
case optimizing mean non-negative cost functions (such runtime solution
cost), implement bounded evaluation parameter configuration based N runs
given performance bound Procedure objective (see Procedure 4). procedure sequentially
performs runs parameter configuration run computes lower bound cN ()
based N runs performed far. Specifically, objective mean runtime
sum runtimes runs, divide sum N ; since runtimes must
nonnegative, quantity lower bounds cN (). lower bound exceeds bound passed
argument, skip remaining runs . order pass appropriate bounds
Procedure objective, need slightly modify Procedure betterN (see Procedure 2 page 274)
adaptive capping. Procedure objective bound additional third argument,
set line 1 betterN , cN (2 ) line 2.
approach results computation exactly function betterN used
original version BasicILS, modified procedure follows exactly search trajectory
would followed without capping, typically requires much less runtime. Hence, within
amount overall running time, new version BasicILS tends able search
larger part parameter configuration space. Although work focus objective
minimizing mean runtime decision algorithms, note adaptive capping approach
applied easily configuration objectives.
4.1.2 AGGRESSIVE C APPING
demonstrate Section 6.4, use trajectory-preserving adaptive capping result
substantial speedups BasicILS. However, sometimes approach still less efficient
could be. upper bound cumulative runtime used capping computed
best configuration encountered current ILS iteration (where new ILS iteration begins
perturbation), opposed overall incumbent. perturbation resulted
new parameter configuration , new iterations best configuration initialized .
frequent case new performs poorly, capping criterion apply quickly
comparison performed overall incumbent.
counteract effect, introduce aggressive capping strategy terminate
evaluation poorly-performing configuration time. heuristic extension
adaptive capping technique, bound evaluation parameter configuration per278

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

Procedure 4: objective(, N, optional parameter bound)
Procedure computes cN (), either performing new runs exploiting previous cached runs.
optional third parameter specifies bound computation performed; parameter
specified, bound taken . , N () number runs performed ,
i.e., length global array R . computing runtimes, count unsuccessful runs 10
times cutoff time.
Input
: Parameter configuration , number runs, N , optional bound bound
Output
: cN () cN () bound, otherwise large constant (maxPossibleObjective) plus
number instances remain unsolved bound exceeded
Side Effect: Adds runs global cache performed algorithm runs, R ; updates global
incumbent, inc
// ===== Maintain invariant: N (inc ) N ()
1
2

=
6 inc N (inc ) < N
cN (inc ) objective(inc , N, ) // Adds N N (inc ) runs Rinc
// ===== aggressive capping, update bound.

3

Aggressive capping bound min(bound, bm cN (inc ))
// ===== Update run results tuple R .

= 1...N
sum runtime sum runtimes R [1], . . . , R [i 1] // Tuple indices starting 1.
0i max(max , N bound sum runtime)
N () (, , , oi ) R [i]
N () ((i 0i oi = unsuccessful) (i < 0i oi 6= unsuccessful))
o0i oi // Previous run longer yet unsuccessful shorter yet successful re-use result
9
else
10
o0i objective newly executed run A() instance seed si captime
4
5
6
7
8

11
12
13
14

R [i] (, , 0i , o0i )
1/N (sum runtime + o0i ) > bound return maxPossibleObjective + (N + 1)
N = N (inc ) (sum runtimes R ) < (sum runtimes Rinc ) inc
return 1/N (sum runtimes R )

formance incumbent parameter configuration multiplied factor call bound
multiplier, bm. comparison two parameter configurations 0 performed evaluations terminated preemptively, configuration solved
instances within allowed time taken better one. (This behaviour achieved
line 12 Procedure objective, keeps track number instances solved exceeding bound.) Ties broken favour moving new parameter configuration instead
staying current one.
Depending bound multiplier, use aggressive capping mechanism may change
search trajectory BasicILS. bm = heuristic method reduces trajectorypreserving method, aggressive setting bm = 1 means know parameter
configuration worse incumbent, stop evaluation. experiments set
bm = 2, meaning lower bound performance configuration exceeds twice
performance incumbent solution, evaluation terminated. (In Section 8.4, revisit
choice bm = 2, configuring parameters ParamILS itself.)
279

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

4.2 Adaptive Capping FocusedILS
main difference BasicILS FocusedILS latter adaptively varies number runs used evaluate parameter configuration. difference complicates,
prevent use adaptive capping. FocusedILS always compares pairs parameter configurations based number runs configuration, even though
number differ one comparison next.
Thus, extend adaptive capping FocusedILS using separate bounds every number
runs, N . Recall FocusedILS never moves one configuration, , neighbouring
configuration, 0 , without performing least many runs 0 performed .
Since keep track performance number runs N (), bound
evaluation 0 always available. Therefore, implement trajectory-preserving
aggressive capping BasicILS.
BasicILS, FocusedILS inner workings adaptive capping implemented
Procedure objective (see Procedure 4). need modify Procedure betterF oc (see Procedure
3 page 276) call objective right bounds. leads following changes
Procedure betterF oc . Subprocedure dominates line 16 takes bound additional
argument passes two calls objective line 18. two calls dominates
line 10 one call line 11 use bound cmax . three direct calls objective
lines 8, 9, 12 use bounds , cmax , , respectively.

5. Experimental Preliminaries
section give background information computational experiments presented
following sections. First, describe design experiments. Next, present
configuration scenarios (algorithm/benchmark data combinations) studied following section.
Finally, describe low-level details experimental setup.
5.1 Experimental Design
describe objective function methods used selecting instances seeds.
5.1.1 C ONFIGURATION BJECTIVE : P ENALIZED AVERAGE RUNTIME
Section 2, mentioned algorithm configuration problems arise context various
different cost statistics. Indeed, past work explored several them: maximizing solution
quality achieved given time, minimizing runtime required reach given solution quality,
minimizing runtime required solve single problem instance (Hutter et al., 2007).
work focus objective minimizing mean runtime instances
distribution D. optimization objective naturally occurs many practical applications.
implies strong correlation c() amount time required obtain good
empirical estimate c(). correlation helps make adaptive capping scheme effective.
One might wonder whether means right way aggregate runtimes. preliminary
experiments, found minimizing mean runtime led parameter configurations overall good runtime performance, including rather competitive median runtimes, minimizing
median runtime yielded less robust parameter configurations timed large (but < 50%)
fraction benchmark instances. However, encounter runs terminate within
280

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

given cutoff time mean ill-defined. order penalize timeouts, define penalized
average runtime (PAR) set runs cutoff time max mean runtime
runs, unsuccessful runs counted p max penalization constant p 1.
study, use p = 10.
5.1.2 ELECTING NSTANCES EEDS
mentioned previously, often finite set instances available upon evaluate
algorithm. case experiments report here. Throughout study, configuration experiments performed training set containing half given benchmark instances.
remaining instances solely used test set evaluate found parameter configurations.
evaluations within ParamILS based N runs, selected N instances
random number seeds used following common blocking technique (see, e.g., Birattari
et al., 2002; Ridge & Kudenko, 2006). ensured whenever two parameter configurations
compared, cost estimates based exactly instances seeds.
serves avoid noise effects due differences instances use different seeds.
example, prevents us making mistake considering configuration better
configuration 0 tested easier instances.
dealing randomized target algorithms, tradeoff number
problem instances used number independent runs performed instance.
extreme case, given sample size N , one could perform N runs single instance
single run N different instances. latter strategy known result minimal variance
estimator common optimization objectives minimization mean runtime (which
consider study) maximization mean solution quality (see, e.g., Birattari, 2004).
Consequently, performed multiple runs per instance wanted acquire
samples cost distribution instances training set.
Based considerations, configuration procedures study article
implemented take list hinstance, random number seedi pairs one inputs. Empirical
estimates cN () cost statistic c() optimized determined first N hinstance,
seedi pairs list. list hinstance, seedi pairs constructed follows. Given training
set consisting problem instances, N , drew sample N instances uniformly
random without replacement added list. wished evaluate algorithm
samples training instances, could happen case randomized
algorithms, repeatedly drew random samples size described before,
batch corresponded random permutation N training instances, added final sample
size N mod < , case N . sample drawn, paired
random number seed chosen uniformly random set possible seeds
added list hinstance, seedi pairs.
5.1.3 C OMPARISON C ONFIGURATION P ROCEDURES
Since choice instances (and degree seeds) important final outcome
optimization, experimental evaluations always performed number independent
runs configuration procedure (typically 25). created separate list instances seeds
run explained above, kth run configuration procedure uses
kth list instances seeds. (Note, however, disjoint test set used measure performance
parameter configurations identical runs.)
281

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

Configuration scenario
P -SWGCP
P E R -SWGCP
P -QCP
P E R -QCP
C P L E X -R E G N 100

Type benchmark instances & citation
Graph colouring (Gent, Hoos, Prosser & Walsh, 1999)
Graph colouring (Gent, Hoos, Prosser & Walsh, 1999)
Quasigroup completion (Gomes & Selman, 1997)
Quasigroup completion (Gomes & Selman, 1997)
Combinatorial Auctions (CATS) (Leyton-Brown, Pearson & Shoham, 2000)

Table 1: Overview five B R configuration scenarios.
Algorithm
APS
PEAR

C PLEX

Parameter type
Continuous
Categorical
Integer
Continuous
Categorical
Integer
Continuous

# parameters type
4
10
4
12
50
8
5

# values considered
7
220
58
36
27
57
35

Total # configurations, ||
2 401
8.34 1017

1.38 1037

Table 2: Parameter overview algorithms consider. information parameters algorithm given text.
detailed list parameters values considered found online appendix
http://www.cs.ubc.ca/labs/beta/Projects/ParamILS/algorithms.html.
performed paired statistical test compare final results obtained runs two
configuration procedures. paired test required since kth run procedures shared
kth list instances seeds. particular, performed two-sided paired Max-Wilcoxon
test null hypothesis difference performances, considering p-values
0.05 statistically significant. p-values reported tables derived using
test; p-values shown parentheses refer cases procedure expected perform better
actually performed worse.
5.2 Configuration Scenarios
Section 6, analyze configurators based five configuration scenarios, combining
high-performance algorithm widely-studied benchmark dataset. Table 1 gives overview
these, dub B R scenarios. algorithms benchmark instance sets used
scenarios described detail Sections 5.2.1 5.2.2, respectively. five
B R configuration scenarios, set fairly aggressive cutoff times five seconds per run
target algorithm allowed configuration procedure execute target algorithm
aggregate runtime five CPU hours. short cutoff times fairly short times algorithm
configuration deliberately chosen facilitate many configuration runs B R scenario. contrast, second set configuration scenarios (exclusively focusing C PLEX),
set much larger cutoff times allowed time configuration. defer description
scenarios Section 7.
5.2.1 TARGET LGORITHMS
three target algorithms listed Table 2 along configurable parameters.
282

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

APS first target algorithm used experiments APS, high-performance dynamic
local search algorithm SAT solving (Hutter, Tompkins & Hoos, 2002) implemented UBCSAT (Tompkins & Hoos, 2004). introduced 2002, APS state-of-the-art solver,
still performs competitively many instances. chose study algorithm
well known, relatively parameters, intimately familiar it. APSs four
continuous parameters control scaling smoothing clause weights, well probability random walk steps. original default parameters set manually based experiments
prominent benchmark instances; manual experimentation kept percentage random
steps fixed took one week development time. subsequently gained
experience APSs parameters general problem classes (Hutter, Hamadi, Hoos &
Leyton-Brown, 2006), chose promising intervals parameter, including, centered
at, original default. picked seven possible values parameter spread uniformly
across respective interval, resulting 2401 possible parameter configurations (these exactly
values used Hutter et al., 2007). starting configuration ParamILS, used
center point parameters domain.
PEAR second target algorithm considered PEAR, recent tree search algorithm
solving SAT problems. PEAR state-of-the-art SAT solver industrial instances,
appropriate parameter settings best available solver certain types hardware
software verification instances (Hutter, Babic, Hoos & Hu, 2007). Furthermore, configured
ParamILS, PEAR quantifier-free bit-vector arithmetic category 2007 Satisfiability
Modulo Theories Competition. PEAR 26 parameters, including ten categorical, four integer,
twelve continuous parameters, default values manually engineered developer. (Manual tuning required one week.) categorical parameters mainly control
heuristics variable value selection, clause sorting, resolution ordering, enable disable
optimizations, pure literal rule. continuous integer parameters mainly deal
activity, decay, elimination variables clauses, well interval randomized
restarts percentage random choices. discretized integer continuous parameters
choosing lower upper bounds reasonable values allowing three eight
discrete values spread relatively uniformly across resulting interval, including default,
served starting configuration ParamILS. number discrete values chosen according intuition importance parameter. discretization,
3.7 1018 possible parameter configurations. Exploiting fact nine parameters
conditional (i.e., relevant parameters take certain values) reduced 8.34 1017
configurations.
C PLEX third target algorithm used commercial optimization tool C PLEX 10.1.1,
massively parameterized algorithm solving mixed integer programming (MIP) problems.
159 user-specifiable parameters, identified 81 parameters affect C PLEXs search trajectory. careful omit parameters change problem formulation (e.g., changing
numerical accuracy solution). Many C PLEX parameters deal MIP strategy heuristics (such variable branching heuristics, probing, dive type subalgorithms)
amount type preprocessing performed. nine parameters governing
frequently different type cut used (those parameters four allowable
magnitude values value choose automatically; note last value prevents parameters ordinal). considerable number parameters deal simplex
283

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

barrier optimization, various algorithm components. categorical parameters
automatic option, considered categorical values well automatic one. contrast, continuous integer parameters automatic option, chose option instead
hypothesizing values might work well. identified numerical parameters
primarily deal numerical issues, fixed default values. numerical
parameters, chose five possible values seemed sensible, including default.
many categorical parameters automatic option, included automatic option choice
parameter, included manual options. Finally, ended 63 configurable parameters, leading 1.78 1038 possible configurations. Exploiting fact seven
C PLEX parameters relevant conditional parameters taking certain values,
reduced 1.38 1037 distinct configurations. starting configuration configuration
procedures, used default settings, obtained careful manual configuration
broad range MIP instances.
5.2.2 B ENCHMARK NSTANCES
applied target algorithms three sets benchmark instances: SAT-encoded quasi-group
completion problems, SAT-encoded graph-colouring problems based small world graphs,
MIP-encoded winner determination problems combinatorial auctions. set consisted
2000 instances, partitioned evenly training test sets.
QCP first benchmark set contained 23 000 instances quasi-group completion problem (QCP), widely studied AI researchers. generated QCP instances
around solubility phase transition, using parameters given Gomes Selman (1997).
Specifically, order n drawn uniformly interval [26, 43], number holes
H (open entries Latin square) drawn uniformly [1.75, 2.3] n1.55 . resulting
QCP instances converted SAT CNF format. use complete solver, PEAR,
sampled 2000 SAT instances uniformly random. average 1497 variables (standard deviation: 1094) 13 331 clauses (standard deviation: 12 473), 1182
satisfiable. use incomplete solver, APS, randomly sampled 2000 instances
subset satisfiable instances (determined using complete algorithm); number
variables clauses similar used PEAR.
SW-GCP second benchmark set contained 20 000 instances graph colouring problem
(GCP) based small world (SW) graphs Gent et al. (1999). these, sampled 2000
instances uniformly random use PEAR; average 1813 variables (standard
deviation: 703) 13 902 clauses (standard deviation: 5393), 1109 satisfiable.
use APS, randomly sampled 2000 satisfiable instances (again, determined using
complete SAT algorithm), whose number variables clauses similar used
PEAR.
Regions100 third benchmark set generated 2000 instances combinatorial auction
winner determination problem, encoded mixed-integer linear programs (MILPs). used
regions generator Combinatorial Auction Test Suite (Leyton-Brown et al., 2000),
goods parameter set 100 bids parameter set 500. resulting MILP instances
contained 501 variables 193 inequalities average, standard deviation 1.7 variables
2.5 inequalities.
284

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

Scenario
P -SWGCP
P E R -SWGCP
P -QCP
P E R -QCP
C P L E X -R E G N 100

Default
20.41
9.74
12.97
2.65
1.61

Test performance (penalized average runtime, CPU seconds)
mean stddev. 10 runs
Run best training performance
BasicILS
FocusedILS
BasicILS
FocusedILS
0.32 0.06 0.32 0.05
0.26
0.26
8.05 0.9
8.3 1.1
6.8
6.6
4.86 0.56 4.70 0.39
4.85
4.29
1.39 0.33
1.29 0.2
1.16
1.21
0.5 0.3
0.35 0.04
0.35
0.32

Fig.
2(a)
2(b)
2(c)
2(d)
2(e)

Table 3: Performance comparison default parameter configuration configurations found
BasicILS FocusedILS (both Aggr Capping bm = 2). configuration scenario, list test performance (penalized average runtime 1000 test instances, CPU seconds) algorithm default, mean stddev test performance across
25 runs BasicILS(100) & FocusedILS (run five CPU hours each), test performance run BasicILS FocusedILS best terms training performance. Boldface indicates better BasicILS FocusedILS. algorithm configurations found FocusedILSs run best training performance listed online appendix http://www.cs.ubc.ca/labs/beta/Projects/ParamILS/results.html. Column Fig. gives reference scatter plot comparing performance configurations
algorithm defaults.

5.3 Experimental Setup
carried experiments cluster 55 dual 3.2GHz Intel Xeon PCs 2MB
cache 2GB RAM, running OpenSuSE Linux 10.1. measured runtimes CPU time
reference machines. configuration procedures implemented Ruby scripts,
include runtime scripts configuration time. easy configuration
scenarios, algorithm runs finish milliseconds, overhead scripts
substantial. Indeed, longest configuration run observed took 24 hours execute five hours
worth target algorithm runtime. contrast, harder C PLEX scenarios described Section
7 observed virtually overhead.

6. Empirical Evaluation BasicILS, FocusedILS Adaptive Capping
section, use B R scenarios empirically study performance BasicILS(N )
FocusedILS, well effect adaptive capping. first demonstrate large speedups
ParamILS achieved default parameters study components responsible
success.
6.1 Empirical Comparison Default Optimized Parameter Configurations
section, five B R configuration scenarios, compare performance
respective algorithms default parameter configuration final configurations found
BasicILS(100) FocusedILS. Table 3 especially Figure 2 show configurators led
substantial speedups.
Table 3, report final performance achieved 25 independent runs configurator. independent configuration run, used different set training instances seeds
(constructed described Section 5.1.2). note often rather large variance
performances found different runs configurators, configuration found
285

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

4

4

10

Runtime [s], autotuned

Runtime [s], autotuned

10

3

10

2

10

1

10

0

10

1

10

2

10

2

10

1

10

0

10

1

10

2

10
2

1

10

0

10

1

10

10

2

10

3

4

10

2

10

Runtime [s], default

10

(a) P -SWGCP.
531s vs 0.15s; 499 vs timeouts
4

1

10

0

10

1

2

10

1

10

0

10

1

10

2

10

10

1

10

0

10

1

10

2

10

3

10

Runtime [s], default

(c) P -QCP.
72s vs 0.17s; 149 vs 1 timeouts

4

10

4

10

4

3

10

2

10

1

10

0

10

1

10

2

10
2

3

10

10

Runtime [s], autotuned

Runtime [s], autotuned

3

10

(b) P E R -SWGCP.
33s vs 17s; 3 vs 2 timeouts

10

10

2

10

Runtime [s], default

4

10

Runtime [s], autotuned

3

10

3

10

2

10

1

10

0

10

1

10

2

10
2

10

1

10

0

10

1

10

2

10

10

Runtime [s], default

(d) P E R -QCP.
9.6s vs 0.85s; 1 vs 0 timeouts

3

4

10

2

10

1

10

0

10

1

10

2

10

3

10

Runtime [s], default

4

10

(e) C P L E X -R E G N 100.
1.61s vs 0.32s; timeouts

Figure 2: Comparison default vs automatically-determined parameter configurations five B R
configuration scenarios. dot represents one test instance; timeouts (after one CPU hour)
denoted circles. dashed line five CPU seconds indicates cutoff time target
algorithm used configuration process. subfigure captions give mean runtimes
instances solved configurations (default vs optimized), well number
timeouts each.

run best training performance tended yield better test performance
others. reason, used configuration result algorithm configuration. (Note
choosing configuration found run best training set performance perfectly
legitimate procedure since require knowledge test set. course, improvements thus achieved come price increased overall running time, independent runs
configurator easily performed parallel.)
Figure 2, compare performance automatically-found parameter configuration
default configuration, runs allowed last hour. speedups
obvious figure Table 3, since penalized average runtime table counts
runtimes larger five seconds fifty seconds (ten times cutoff five seconds), whereas
data figure uses much larger cutoff time. larger speedups apparent scenarios
P -SWGCP, P -QCP, P E R -QCP: corresponding speedup factors mean runtime
3540, 416 11, respectively (see Figure 2).
286

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

Algorithm 5: RandomSearch(N, 0 )
Outline random search parameter configuration space; inc denotes incumbent parameter
configuration, betterN compares two configurations based first N instances training
set.
Input : Number runs use evaluating parameter configurations, N ; initial configuration
0 .
Output : Best parameter configuration inc found.
1 inc 0 ;
2 TerminationCriterion()
3
random ;
4
betterN (, inc )
5
inc ;
6

return inc

6.2 Empirical Comparison BasicILS Simple Baselines
section, evaluate effectiveness BasicILS(N ) two components:
simple random search, used BasicILS initialization (we dub RandomSearch(N )
provide pseudocode Algorithm 5);
simple local search, type iterative first improvement search used BasicILS(N )
(we dub SimpleLS(N )).
evaluate one component time, section Section 6.3 study algorithms
without adaptive capping. investigate effect adaptive capping methods Section
6.4.
sufficient structure search space, expect BasicILS outperform RandomSearch. local minima, expect BasicILS perform better simple local search.
experiments showed BasicILS indeed offer best performance.
Here, solely interested comparing effectively approaches search space
parameter configurations (and found parameter configurations generalize unseen
test instances). Thus, order reduce variance comparisons, compare configuration
methods terms performance training set.
Table 4, compare BasicILS RandomSearch B R configuration scenarios.
average, BasicILS always performed better, three five scenarios, difference
statistically significant judged paired Max-Wilcoxon test (see Section 5.1.3). Table 4
lists performance default parameter configuration scenarios. note
BasicILS RandomSearch consistently achieved substantial (and statistically significant)
improvements default configurations.
Next, compared BasicILS second component, SimpleLS. basic local search
identical BasicILS, stops first local minimum encountered. used order
study whether local minima pose problem simple first improvement search. Table 5 shows
three configuration scenarios BasicILS time perform multiple ILS iterations,
training set performance statistically significantly better SimpleLS. Thus,
conclude search space contains structure exploited local search algorithm
well local minima limit performance iterative improvement search.
287

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

Scenario
P -SWGCP
P E R -SWGCP
P -QCP
P E R -QCP
C P L E X -R E G N 100

Training performance (penalized average runtime, CPU seconds)
Default RandomSearch(100)
BasicILS(100)
19.93
0.46 0.34
0.38 0.19
10.61
7.02 1.11
6.78 1.73
12.71
3.96 1.185
3.19 1.19
2.77
0.58 0.59
0.36 0.39
1.61
1.45 0.35
0.72 0.45

p-value
0.94
0.18
1.4 105
0.007
1.2 105

Table 4: Comparison RandomSearch(100) BasicILS(100), without adaptive capping. table
shows training performance (penalized average runtime N = 100 training instances, CPU
seconds). Note approaches yielded substantially better results default configuration, BasicILS performed statistically significantly better RandomSearch three
five B R configuration scenarios judged paired Max-Wilcoxon test (see Section
5.1.3).
Scenario
P -SWGCP
P -QCP
P E R -QCP

SimpleLS(100)
Performance
0.5 0.39
3.60 1.39
0.4 0.39

BasicILS(100)
Performance
Avg. # ILS iterations
0.38 0.19
2.6
3.19 1.19
5.6
0.36 0.39
1.64

p-value
9.8 104
4.4 104
0.008

Table 5: Comparison SimpleLS(100) BasicILS(100), without adaptive capping. table shows
training performance (penalized average runtime N = 100 training instances, CPU seconds). configuration scenarios P E R -SWGCP C P L E X -R E G N 100, BasicILS complete first ILS iteration 25 runs; two approaches thus identical
listed here. configuration scenarios, BasicILS found significantly better configurations
SimpleLS.

6.3 Empirical Comparison FocusedILS BasicILS
section investigate FocusedILSs performance experimentally. contrast previous comparison RandomSearch, SimpleLS, BasicILS using training performance,
compare FocusedILS BasicILS using test performance. becausein contrast BasicILS SimpleLSFocusedILS grows number target algorithm runs used evaluate
parameter configuration time. Even different runs FocusedILS (using different training sets
random seeds) use number target algorithm runs evaluate parameter configurations. However, eventually aim optimize cost statistic, c, therefore
test set performance (an unbiased estimator c) provides fairer basis comparison training performance. compare FocusedILS BasicILS, since BasicILS already outperformed
RandomSearch SimpleLS Section 6.2.
Figure 3 compares test performance FocusedILS BasicILS(N ) N = 1, 10
100. Using single target algorithm run evaluate parameter configuration, BasicILS(1)
fast, generalize well test set all. example, configuration scenario
P -SWGCP, BasicILS(1) selected parameter configuration whose test performance turned
even worse default. hand, using large number target algorithm runs
evaluation resulted slow search, eventually led parameter configurations
good test performance. FocusedILS aims achieve fast search good generalization test
set. configuration scenarios Figure 3, FocusedILS started quickly led best
final performance.
288

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

2

2

Mean runtime [s], test

Mean runtime [s], test

10

1

10

0

10

1

10

BasicILS(1)
BasicILS(10)
BasicILS(100)
FocusedILS
0

10

2

1

0.5

0

4

10

1.5

10

BasicILS(1)
BasicILS(10)
BasicILS(100)
FocusedILS
0

10

CPU time used tuner [s]

2

4

10

10

CPU time used tuner [s]

(a) P -SWGCP

(b) C P L E X -R E G N 100

Figure 3: Comparison BasicILS(N ) N = 1, 10, 100 vs FocusedILS, without adaptive
capping. show median test performance (penalized average runtime across 1 000 test
instances) across 25 runs configurators two scenarios. Performance three
B R scenarios qualitatively similar: BasicILS(1) fastest move away
starting parameter configuration, performance robust all; BasicILS(10)
rather good compromise speed generalization performance, given enough time
outperformed BasicILS(100). FocusedILS started finding good configurations quickly
(except scenario P E R -QCP, took even longer BasicILS(100) improve
default) always amongst best approaches end configuration process.

Scenario
P -SWGCP
P E R -SWGCP
P -QCP
P E R -QCP
C P L E X -R E G N 100

Test performance (penalized average runtime, CPU seconds)
Default BasicILS(100)
FocusedILS
20.41
0.59 0.28
0.32 0.08
9.74
8.13 0.95
8.40 0.92
12.97
4.87 0.34
4.69 0.40
2.65
1.32 0.34
1.35 0.20
1.61
0.72 0.45
0.33 0.03

p-value
1.4 104
(0.21)
0.042
(0.66)
1.2 105

Table 6: Comparison BasicILS(100) FocusedILS, without adaptive capping. table shows
test performance (penalized average runtime 1 000 test instances, CPU seconds).
configuration scenario, report test performance default parameter configuration, mean
stddev test performance reached 25 runs BasicILS(100) FocusedILS, pvalue paired Max-Wilcoxon test (see Section 5.1.3) difference two configurators
performance.

compare performance FocusedILS BasicILS(100) configuration scenarios Table 6. three APS C PLEX scenarios, FocusedILS performed statistically significantly better BasicILS(100). results consistent past work FocusedILS achieved statistically significantly better performance BasicILS(100) (Hutter et al.,
2007). However, found configuration scenarios involving PEAR algorithm,
BasicILS(100) actually performed better average FocusedILS, albeit statistically significantly. attribute fact complete, industrial solver PEAR, two
benchmark distributions QCP SWGCP quite heterogeneous. expect FocusedILS
problems dealing highly heterogeneous distributions, due fact frequently tries
extrapolate performance based runs per parameter configuration.
289

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

2

2.5
BasicILS, capping
BasicILS, TP capping

Mean runtime [s], train

Mean runtime [s], train

10

1

10

0

10

1

10

2

3

10

10

BasicILS, capping
BasicILS, TP capping
2

1.5

1

0.5

0 2
10

4

10

CPU time used tuner [s]

3

10

4

10

CPU time used tuner [s]

(a) P -SWGCP, significant.

(b) C P L E X -R E G N 100, significant.

Figure 4: Speedup BasicILS adaptive capping two configuration scenarios. performed 25
runs BasicILS(100) without adaptive capping TP capping. time step,
computed training performance run configurator (penalized average runtime
N = 100 training instances) plot median 25 runs.
Scenario
P -SWGCP
P E R -SWGCP
P -QCP
P E R -QCP
C P L E X -R E G N 100

Training performance (penalized average runtime)
capping
TP capping
p-value
0.38 0.19
0.24 0.05
6.1 105
6.78 1.73
6.65 1.48
0.01
3.19 1.19
2.96 1.13
9.8 104
0.361 0.39 0.356 0.44
0.66
0.67 0.35
0.47 0.26
7.3 104

Avg. # ILS iterations
capping TP capping
3
12
1
1
6
10
2
3
1
1

Table 7: Effect adaptive capping BasicILS(100). show training performance (penalized average runtime N = 100 training instances, CPU seconds). configuration scenario,
report mean stddev final training performance reached 25 runs configurator without capping TP capping, p-value paired Max-Wilcoxon test difference
(see Section 5.1.3), well average number ILS iterations performed respective
configurator.

6.4 Empirical Evaluation Adaptive Capping BasicILS FocusedILS
present experimental evidence use adaptive capping strong impact
performance BasicILS FocusedILS.
Figure 4 illustrates extent TP capping sped BasicILS two configuration scenarios. cases, capping helped improve training performance substantially; P -SWGCP,
BasicILS found solutions order magnitude faster without capping.
Table 7 quantifies speedups five B R configuration scenarios. TP capping enabled
four times many ILS iterations (in P -SWGCP) improved average performance
scenarios. improvement statistically significant scenarios, except P E R -QCP.
Aggressive capping improved BasicILS performance one scenario. scenario
P -SWGCP, increased number ILS iterations completed within configuration time
12 219, leading significant improvement performance. first ILS iteration
BasicILS, capping techniques identical (the best configuration iteration always
incumbent). Thus, observe difference configuration scenarios P E R -SWGCP
C P L E X -R E G N 100, none 25 runs configurator finished first ILS iteration.
remaining two configuration scenarios, differences insignificant.
290

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

Scenario
P -SWGCP
P E R -SWGCP
P -QCP
P E R -QCP
C P L E X -R E G N 100

Number ILS iterations performed
capping
TP capping
p-value
121 12
166 15
1.2 105
37 12
43 15
0.0026
142 18
143 22
0.54
153 49
165 41
0.03
36 13
40 16
0.26

Aggr capping
244 19
47 18
156 28
213 62
54 15

p-value
1.2 105
9 105
0.016
1.2 105
1.8 105

Number runs performed incumbent parameter configuration
Scenario
capping
TP capping
p-value
Aggr capping
p-value
P -SWGCP
993 211
1258 262 4.7 104 1818 243 1.2 105
503 265
476 238
(0.58)
642 288
0.009
P E R -SWGCP
P -QCP
1575 385 1701 318
0.065
1732 340
0.084
P E R -QCP
836 509
1130 557
0.02
1215 501
0.003
761 215
795 184
0.40
866 232
0.07
C P L E X -R E G N 100

Table 8: Effect adaptive capping search progress FocusedILS, measured number ILS
iterations performed number runs performed incumbent parameter configuration.
configuration scenario, report mean stddev measures across 25
runs configurator without capping, TP capping, Aggr capping, well
p-values paired Max-Wilcoxon tests (see Section 5.1.3) differences capping
TP capping; capping Aggr capping.

evaluate usefulness capping FocusedILS. Training performance useful
quantity context comparing different versions FocusedILS, since number target
algorithm runs measure based varies widely runs configurator. Instead,
used two measures quantify search progress: number ILS iterations performed
number target algorithm runs performed incumbent parameter configuration. Table 8
shows two measures five B R configuration scenarios three capping schemes
(none, TP, Aggr). FocusedILS TP capping achieved higher values without capping
scenarios measures (although differences statistically significant).
Aggressive capping increased measures scenarios, differences
capping aggressive capping statistically significant. Figure 5 demonstrates
two configuration scenarios FocusedILS capping reached solution qualities
quickly without capping. However, finding respective configurations, FocusedILS
showed significant improvement.
Recall experiments Section 6.2 6.3 compared various configurators without adaptive capping. One might wonder comparisons change presence adaptive
capping. Indeed, adaptive capping worked box RandomSearch enabled
evaluate 3.4 33 times many configurations without capping. improvement
significantly improved simple algorithm RandomSearch point average performance came within 1% one BasicILS two domains (S P -SWGCP P E R -SWGCP;
compare much larger differences without capping reported Table 4). P E R -QCP,
still 25% difference average performance, result significant. Finally,
P -QCP C P L E X -R E G N 100 difference still substantial significant (22% 55%
difference average performance, p-values 5.2 105 0.0013, respectively).
Adaptive capping reduced gap BasicILS FocusedILS. particular,
P -SWGCP, where, even without adaptive capping, FocusedILS achieved best performance
encountered scenario, BasicILS caught using adaptive capping. Similarly,
291

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

2

2
FocusedILS, capping
FocusedILS, TP capping
FocusedILS, Aggr capping

Mean runtime [s], test

Mean runtime [s], test

10

1

10

0

10

1

10

2

10

3

10

4

10

FocusedILS, capping
FocusedILS, TPcapping
FocusedILS, Aggr capping
1.5

1

0.5

0 1
10

5

10

CPU time used tuner [s]

2

10

3

10

4

10

5

10

CPU time used tuner [s]

(a) P -SWGCP

(b) C P L E X -R E G N 100

Figure 5: Speedup FocusedILS adaptive capping two configuration scenarios. performed 25
runs FocusedILS without adaptive capping, TP capping Aggr capping.
time step, computed test performance run configurator (penalized average
runtime 1000 test instances) plot median 25 runs. differences
end trajectory statistically significant. However, capping time required
achieve quality lower two configuration scenarios. three scenarios,
gains due capping smaller.

C P L E X -R E G N 100, FocusedILS already performed well without adaptive capping
BasicILS not. Here, BasicILS improved based adaptive capping, still could rival
FocusedILS. scenarios, adaptive capping affect relative performance much;
compare Tables 6 (without capping) 3 (with capping) details.

7. Case Study: Configuring C PLEX Real-World Benchmarks
section, demonstrate ParamILS improve performance commercial optimization tool C PLEX variety interesting benchmark distributions. best knowledge,
first published study automatically configuring C PLEX.
use five C PLEX configuration scenarios. these, collected wide range MIP benchmarks public benchmark libraries researchers, split 50:50
disjoint training test sets; detail following.
Regions200 set almost identical Regions100 set (described Section 5.2.2
used throughout paper), instances much larger. generated 2 000 MILP
instances generator provided Combinatorial Auction Test Suite (LeytonBrown et al., 2000), based regions option goods parameter set 200
bids parameter set 1 000. instances contain average 1 002 variables 385
inequalities, respective standard deviations 1.7 3.4.
MJA set comprises 343 machine-job assignment instances encoded mixed integer
quadratically constrained programs (MIQCP). obtained Berkeley Computational Optimization Lab5 introduced Akturk, Atamturk S. Gurel (2007).
instances contain average 2 769 variables 2 255 constraints, respective
standard deviations 2 133 1 592.
5. http://www.ieor.berkeley.edu/atamturk/bcol/, set called conic.sch

292

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

CLS set comprises 100 capacitated lot-sizing instances encoded mixed integer linear
programs (MILP). obtained Berkeley Computational Optimization Lab
introduced Atamturk Munoz (2004). 100 instances contain 181 variables
180 constraints.
MIK set 120 MILP-encoded mixed-integer knapsack instances obtained
Berkeley Computational Optimization Lab originally introduced Atamturk
(2003). instances contain average 384 variables 151 constraints, respective standard deviations 309 127.
QP set quadratic programs originated RNA energy parameter optimization (Andronescu, Condon, Hoos, Mathews & Murphy, 2007). Mirela Andronescu generated 2 000 instances experiments. instances contain 9 3667 165 variables 9 1917 186
constraints. Since instances polynomial-time solvable quadratic programs, set
large number inconsequential C PLEX parameters concerning branch cut mechanism default values, ending 27 categorical, 2 integer 2 continuous parameters configured, discretized parameter configuration space size 3.27 1017 .
study ParamILSs behavior harder problems, set significantly longer cutoff times
C PLEX scenarios B R scenarios previous section. Specifically,
used cutoff time 300 CPU seconds run target algorithm training,
allotted two CPU days every run configurators. always, configuration
objective minimize penalized average runtime penalization constant 10.
Table 9, compare performance C PLEXs default parameter configuration
final parameter configurations found BasicILS(100) FocusedILS (both aggressive capping bm = 2). Note that, similar situation described Section 6.1, configuration
scenarios (e.g., C P L E X -CLS, C P L E X -MIK) substantial variance different runs
configurators, run best training performance yielded parameter configuration
good test set. BasicILS outperformed FocusedILS 3 5
scenarios terms mean test performance across ten runs, FocusedILS achieved better test
performance run best training performance one scenario (in performed almost well). scenarios C P L E X -R E G N 200 C P L E X -CLS, FocusedILS performed
substantially better BasicILS.
Note C PLEX configuration scenarios considered, BasicILS FocusedILS
found parameter configurations better algorithm defaults, sometimes
order magnitude. particularly noteworthy since ILOG expended substantial effort
determine strong default C PLEX parameters. Figure 6, provide scatter plots five scenarios. C P L E X -R E G N 200, C P L E X - C N C . C H , C P L E X -CLS, C P L E X -MIK, speedups quite
consistent across instances (with average speedup factors reaching 2 C P L E X - C N C . C H
23 C P L E X -MIK). Finally, C P L E X -QP see interesting failure mode ParamILS.
optimized parameter configuration achieved good performance cutoff time used
6. configuration scenario C P L E X -MIK, nine ten runs FocusedILS yielded parameter configurations
average runtimes smaller two seconds. One run, however, demonstrated interesting failure mode FocusedILS aggressive capping. Capping aggressively caused every C PLEX run unsuccessful,
FocusedILS selected configuration manage solve single instance test set. Counting unsuccessful runs ten times cutoff time, resulted average runtime 10 300 = 3000 seconds run.
(For full details, see Section 8.1 Hutter, 2009).

293

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

Scenario
C P L E X -R E G N 200
CP L E X-C N C.S C H
C P L E X -CLS
C P L E X -MIK
C P L E X -QP

Test performance (penalized average runtime, CPU seconds)
mean stddev. 10 runs
Run best training performance
Default
BasicILS
FocusedILS BasicILS
FocusedILS
72
45 24
11.4 0.9
15
10.5
5.37
2.27 0.11
2.4 0.29
2.14
2.35
712
443 294
327 860
80
23.4
64.8
20 27
301 948 6
1.72
1.19
969
755 214
827 306
528
525

Fig.
6(a)
6(b)
6(c)
6(d)
6(e)

Table 9: Experimental results C PLEX configuration scenarios.

configuration scenario, list test performance (penalized average runtime test instances) algorithm default, mean stddev test performance across ten runs BasicILS(100)
& FocusedILS (run two CPU days each), test performance run
BasicILS FocusedILS best terms training performance. Boldface indicates better BasicILS FocusedILS. algorithm configurations found
FocusedILSs run best training performance listed online appendix
http://www.cs.ubc.ca/labs/beta/Projects/ParamILS/results.html.
Column
Fig. gives reference scatter plot comparing performance configurations
algorithm defaults.

configuration process (300 CPU seconds, see Figure 6(f)), performance carry
higher cutoff time used tests (3600 CPU seconds, see Figure 6(e)). Thus, parameter configuration found FocusedILS generalize well previously unseen test data,
larger cutoff times.

8. Review ParamILS Applications
section, review number applications ParamILSsome dating back
earlier stages development, others recentthat demonstrate utility versatility.
8.1 Configuration SAPS, GLS+ SAT4J
Hutter et al. (2007), first publication ParamILS, reported experiments three target algorithms demonstrate effectiveness approach: SAT algorithm SAPS (which
4 numerical parameters), local search algorithm GLS+ solving probable explanation (MPE) problem Bayesian networks (which 5 numerical parameters; Hutter, Hoos &
Stutzle, 2005), tree search SAT solver SAT4J (which 4 categorical 7 numerical
parameters; http://www.sat4j.org). compared respective algorithms default performance,
performance CALIBRA system (Adenso-Diaz & Laguna, 2006), performance
BasicILS FocusedILS. four configuration scenarios studied, FocusedILS significantly outperformed CALIBRA two performed better average third. fourth
one (configuring SAT4J), CALIBRA applicable due categorical parameters,
FocusedILS significantly outperformed BasicILS.
Overall, automated parameter optimization using ParamILS achieved substantial improvements
previous default settings: GLS+ sped factor > 360 (tuned parameters found
solutions better quality 10 seconds default found one hour), SAPS factors 8
130 SAPS-QWH SAPS-SW, respectively, SAT4J factor 11.
294

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

4

4

2

10

1

10

0

10

1

10

2

10

Runtime [s], autotuned

3

10

10

3

10

2

10

1

10

0

10

1

10

2

10
2

10

1

10

0

10

1

10

2

10

3

10

Runtime [s], default

4

10

1

10

0

10

1

10

2

10

3

4

10

Runtime [s], default

1

10

0

10

1

10

2

10

10

1

10

0

10

1

10

2

10

10

Runtime [s], default

(d) C P L E X -MIK.
28s vs 1.2s; timeouts

3

4

10

2
1

10

0

10

1

10

2

10

3

10

Runtime [s], default

4

10

(c) C P L E X -CLS.
309s vs 21.5s; timeouts
4

10

3

10

2

10

1

10

0

10

1

10

2

3

10

2

10

1

10

0

10

1

10

2

10
2

1

10

2

Runtime [s], autotuned

Runtime [s], autotuned

2

10

0

10

10

4

3

1

10

10

10

10

2

10

4

(b) C P L E X - C N C . C H .
5.37s vs 2.39.5s; timeouts

10

3

10

10
2

10

(a) C P L E X -R E G N 200.
72s vs 10.5s; timeouts

Runtime [s], autotuned

4

10

Runtime [s], autotuned

Runtime [s], autotuned

10

10
2

10

1

10

0

10

1

10

2

10

3

10

Runtime [s], default

(e) C P L E X -QP.
296s vs 234s; 0 vs 21 timeouts

4

2

10

10

1

10

0

10

1

10

2

10

3

10

Runtime [s], default

4

10

(f) C P L E X -QP, test cutoff 300
seconds.
81s vs 44s; 305 vs 150 timeouts

Figure 6: Comparison default vs automatically-determined parameter configuration five C PLEX
configuration scenarios. dot represents one test instance; time-outs (after one CPU hour)
denoted red circles. blue dashed line 300 CPU seconds indicates cutoff time
target algorithm used configuration process. subfigure captions give mean
runtimes instances solved configurations (default vs optimized), well
number timeouts each.

8.2 Configuration Spear Industrial Verification Problems
Hutter et al. (2007) applied ParamILS specific real-world application domain: configuring
26 parameters tree-search DPLL solver PEAR minimize mean runtime set
practical verification instances. particular, considered two sets industrial problem
instances, bounded model-checking (BMC) instances Zarpas (2005) software verification
(SWV) instances generated C ALYSTO static checker (Babic & Hu, 2007).
instances problem distributions exhibited large spread hardness PEAR.
SWV instances, default configuration solved many instances milliseconds failed
solve others days. despite fact PEAR specifically developed type
instances, developer generated problem instances (and thus intimate
domain knowledge), week manual performance tuning expended order
optimize solvers performance.
PEAR first configured good general performance industrial SAT instances
previous SAT competitions. already led substantial improvements default perfor295

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

mance 2007 SAT competition.7 PEAR default solved 82 instances ranked 17th
first round competition, automatically configured version solved 93 instances
ranked 8th, optimized version solved 99 instances, ranking 5th (above MiniSAT).
speedup factors due general optimization 20 1.3 SWV BMC datasets,
respectively.
Optimizing specific instance sets yielded further, much larger improvements (a factor
500 SWV 4.5 BMC). encouragingly, best parameter configuration found
software verification instances take longer 20 seconds solve SWV
problem instances (compared multiple timeouts CPU day original default values).
Key good performance application perform multiple independent runs FocusedILS, select found configuration best training performance (as done
Sections 6.1 7 article).
8.3 Configuration SATenstein
KhudaBukhsh, Xu, Hoos Leyton-Brown (2009 ) used ParamILS perform automatic algorithm
design context stochastic local search algorithms SAT. Specifically, introduced
new framework local search SAT solvers, called SATenstein, used ParamILS choose
good instantiations framework given instance distributions. SATenstein spans three broad
categories SLS-based SAT solvers: WalkSAT-based algorithms, dynamic local search algorithms
G2 WSAT variants. combined highly parameterized framework solver
total 41 parameters 4.82 1012 unique instantiations.
FocusedILS used configure SATenstein six different problem distributions,
resulting solvers compared eleven state-of-the-art SLS-based SAT solvers. results
showed automatically configured versions SATenstein outperformed eleven
state-of-the-art solvers six categories, sometimes large margin.
SAT ENSTEIN work clearly demonstrated automated algorithm configuration methods
used construct new algorithms combining wide range components existing algorithms novel ways, thereby go beyond simple parameter tuning. Due low
level manual work required approach, believe automated design algorithms
components become mainstream technique development algorithms hard
combinatorial problems.
Key successful application FocusedILS configuring SAT ENSTEIN careful
selection homogeneous instance distributions, instances could solved within
comparably low cutoff time 10 seconds per run. Again, configuration best training
quality selected ten parallel independent runs FocusedILS per scenario.
8.4 Self-Configuration ParamILS
heuristic optimization procedure, ParamILS controlled number parameters:
number random configurations, r, sampled beginning search; perturbation
strength, s; probability random restarts, prestart . Furthermore, aggressive capping
mechanism makes use additional parameter: bound multiplier, bm. Throughout article,
used manually-determined default values hr, s, prestart , bmi = h10, 3, 0.01, 2i.
7. See http://www.cril.univ-artois.fr/SAT07. PEAR allowed participate second round
competition since source code publicly available.

296

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

recent work (see Section 8.2 Hutter, 2009), evaluated whether FocusedILSs performance could improved using ParamILS automatically find better parameter configuration.
self-configuration task, configuration scenarios play role instances, configurator optimized plays role target algorithm. avoid confusion, refer
configurator target configurator. Here, set fairly short configuration times one CPU
hour target configurator. However, still significantly longer cutoff times
used experiments, parallelization turned crucial finish experiment reasonable amount time. BasicILS easier parallelize
FocusedILS, chose BasicILS(100) meta-configurator.
Although notion algorithm configurator configure intriguing,
case, turned yield small improvements. Average performance improved four
five scenarios degraded remaining one. However, none differences
statistically significant.
8.5 Applications ParamILS
Thachuk, Shmygelska Hoos (2007 ) used BasicILS order determine performance-optimizing
parameter settings new replica-exchange Monte Carlo algorithm protein folding 2DHP 3D-HP models.8 Even though algorithm four parameters (two categorical
two continuous), BasicILS achieved substantial performance improvements. manuallyselected configurations biased favour either short long protein sequences, BasicILS
found configuration consistently yielded good mean runtimes types sequences.
average, speedup factor achieved approximately 1.5, certain classes protein
sequences 3. manually-selected configurations performed worse previous
state-of-the-art algorithm problem instances, robust parameter configurations
selected BasicILS yielded uniformly better performance.
recent work, Fawcett, Hoos Chiarandini (2009) used several variants ParamILS
(including version slightly extended beyond ones presented here) design
modular stochastic local search algorithm post-enrollment course timetabling problem.
followed design approach used automated algorithm configuration order explore
large design space modular highly parameterised stochastic local search algorithms.
quickly led solver placed third Track 2 2nd International Timetabling Competition
(ITC2007) subsequently produced improved solver shown achieve consistently
better performance top-ranked solver competition.

9. Related Work
Many researchers us dissatisfied manual algorithm configuration, various
fields developed approaches automatic parameter tuning. start section
closely-related workapproaches employ direct search find good parameter
configurationsand describe methods. Finally, discuss work related problems,
finding best parameter configuration algorithm per-instance basis, approaches
adapt parameters algorithms execution (see Hoos, 2008, related
work automated algorithm design).
8. BasicILS used, FocusedILS yet developed study conducted.

297

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

9.1 Direct Search Methods Algorithm Configuration
Approaches automated algorithm configuration go back early 1990s, number
systems developed adaptive problem solving. One systems Composer (Gratch
& Dejong, 1992), performs hill-climbing search configuration space, taking moves
enough evidence gathered render neighbouring configuration statistically significantly
better current configuration. Composer successfully applied improving five
parameters algorithm scheduling communication collection ground-based
antennas spacecrafts (Gratch & Chien, 1996).
Around time, MULTI-TAC system introduced Minton (1993, 1996). MULTITAC takes input generic heuristics, specific problem domain, distribution problem instances. adapts generic heuristics problem domain automatically generates
domain-specific LISP programs implementing them. beam search used choose best
LISP program program evaluated running fixed set problem instances
sampled given distribution.
Another search-based approach uses fixed training set introduced Coy et al. (2001).
approach works two stages. First, finds good parameter configuration instance Ii training set combination experimental design (full factorial fractional
factorial) gradient descent. Next, combines parameter configurations 1 , . . . , N thus determined setting parameter average values taken them. Note
averaging step restricts applicability method algorithms numerical parameters.
similar approach, based combination experimental design gradient descent,
using fixed training set evaluation, implemented CALIBRA system Adenso-Diaz
Laguna (2006). CALIBRA starts evaluating parameter configuration full factorial
design two values per parameter. iteratively homes good regions parameter
configuration space employing fractional experimental designs evaluate nine configurations
around best performing configuration found far. grid experimental design
refined iteration. local optimum found, search restarted (with coarser
grid). Experiments showed CALIBRAs ability find parameter settings six target algorithms
matched outperformed respective originally-proposed parameter configurations. main
drawback limitation tuning numerical ordinal parameters, maximum five
parameters. first introduced ParamILS, performed experiments comparing performance CALIBRA (Hutter et al., 2007). experiments reviewed Section 8.1.
Terashima-Marn et al. (1999) introduced genetic algorithm configuring constraint satisfaction algorithm large-scale university exam scheduling. constructed configured
algorithm works two stages seven configurable categorical parameters. optimized choices genetic algorithm 12 problem instances,
found configuration improved performance modified Brelaz algorithm. However, note
performed optimization separately instance. paper quantify
long optimizations took, stated Issues time delivering solutions
method still matter research.
Work automated parameter tuning found numerical optimization literature. particular, Audet Orban (2006) proposed mesh adaptive direct search algorithm.
Designed purely continuous parameter configuration spaces, algorithm guaranteed converge local optimum cost function. Parameter configurations evaluated fixed
298

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

set large unconstrained regular problems CUTEr collection, using optimization objectives runtime number function evaluations required solving given problem instance.
Performance improvements around 25% classical configuration four continuous parameters interior point methods reported.
Algorithm configuration stochastic optimization problem, exists large body
algorithms designed problems (see, e.g., Spall, 2003). However, many algorithms
stochastic optimization literature require explicit gradient information thus inapplicable
algorithm configuration. algorithms approximate gradient function evaluations
(e.g., finite differences), provably converge local minimum cost function
mild conditions, continuity. Still, methods primarily designed deal
numerical parameters find local minima. aware applications general
purpose algorithms stochastic optimization algorithm configuration.
9.2 Methods Algorithm Configuration
Sequential parameter optimization (SPO) (Bartz-Beielstein, 2006) model-based parameter optimization approach based Design Analysis Computer Experiments (DACE; see, e.g.,
Santner, Williams & Notz, 2003), prominent approach statistics blackbox function optimization. SPO starts running target algorithm parameter configurations Latin
hypercube design number training instances. builds response surface model based
Gaussian process regression uses models predictions predictive uncertainties determine next parameter configuration evaluate. metric underlying choice promising
parameter configurations expected improvement criterion used Jones, Schonlau Welch
(1998). algorithm run, response surface refitted, new parameter configuration determined based updated model. contrast previously-mentioned methods,
SPO use fixed training set. Instead, starts small training set doubles size
whenever parameter configuration determined incumbent already incumbent
previous iteration. recent improved mechanism resulted robust version, SPO+ (Hutter, Hoos, Leyton-Brown & Murphy, 2009). main drawbacks SPO variants,
fact entire DACE approach, limitation continuous parameters optimizing
performance single problem instances, well cubic runtime scaling number data
points.
Another approach based adaptations racing algorithms machine learning (Maron &
Moore, 1994) algorithm configuration problem. Birattari et al. (2002; 2004) developed procedure dubbed F-Race used configure various stochastic local search algorithms. F-Race
takes input algorithm A, finite set algorithm configurations , instance distribution D. iteratively runs target algorithm surviving parameter configurations
number instances sampled (in simplest case, iteration runs surviving configurations one instance). configuration eliminated race soon enough statistical
evidence gathered it. iteration, non-parametric Friedman test used check
whether significant differences among configurations. case, inferior
configurations eliminated using series pairwise tests. process iterated
one configuration survives given cutoff time reached. Various applications F-Race
demonstrated good performance (for overview, see Birattari, 2004). However, since
start procedure candidate configurations evaluated, approach limited situations
space candidate configurations practically enumerated. fact, published ex299

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

periments F-Race limited applications around 1200 configurations.
recent extension presented Balaprakash et al. (2007) iteratively performs F-Race subsets
parameter configurations. approach scales better large configuration spaces, version
described Balaprakash et al. (2007) handles algorithms numerical parameters.
9.3 Related Algorithm Configuration Problems
point, focused problem finding best algorithm configuration
entire set (or distribution) problem instances. Related approaches attempt find best
configuration algorithm per-instance basis, adapt algorithm parameters
execution algorithm. Approaches setting parameters per-instance basis
described Patterson Kautz (2001), Cavazos OBoyle (2006), Hutter et al. (2006).
Furthermore, approaches attempt select best algorithm per-instance basis
studied Leyton-Brown, Nudelman Shoham (2002), Carchrae Beck (2005), Gebruers,
Hnich, Bridge Freuder (2005), Gagliolo Schmidhuber (2006), Xu, Hutter, Hoos
Leyton-Brown (2008). related work, decisions restart algorithm made
online, run algorithm (Horvitz, Ruan, Gomes, Kautz, Selman & Chickering, 2001;
Kautz, Horvitz, Ruan, Gomes & Selman, 2002; Gagliolo & Schmidhuber, 2007). So-called reactive
search methods perform online parameter modifications (Battiti, Brunato & Mascia, 2008).
last strategy seen complementary work: even reactive search methods tend
parameters remain fixed search hence configured using offline approaches
ParamILS.
9.4 Relation Local Search Methods
Since ParamILS performs iterated local search one-exchange neighbourhood,
similar spirit local search methods problems, SAT (Selman, Levesque &
Mitchell, 1992; Hoos & Stutzle, 2005), CSP (Minton, Johnston, Philips & Laird, 1992), MPE
(Kask & Dechter, 1999; Hutter et al., 2005). Since ParamILS local search method, existing
theoretical frameworks (see, e.g., Hoos, 2002; Mengshoel, 2008), could principle used
analysis. main factor distinguishing problem ones faced standard local
search algorithms stochastic nature optimization problem (for discussion local
search stochastic optimization, see, e.g., Spall, 2003). Furthermore, exists compact
representation objective function could used guide search. illustrate this,
consider local search SAT, candidate variables flipped limited
occurring currently-unsatisfied clauses. general algorithm configuration, hand,
mechanism cannot used, information available target algorithm
performance runs executed far. While, obviously, (stochastic) local search
methods could used basis algorithm configuration procedures, chose iterated local
search, mainly conceptual simplicity flexibility.

10. Discussion, Conclusions Future work
work, studied problem automatically configuring parameters complex,
heuristic algorithms order optimize performance given set benchmark instances.
extended earlier algorithm configuration procedure, ParamILS, new capping mechanism
300

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

obtained excellent results applying resulting enhanced version ParamILS two
high-performance SAT algorithms well C PLEX wide range benchmark sets.
Compared carefully-chosen default configurations target algorithms, parameter configurations found ParamILS almost always performed much better evaluated
sets previously unseen test instances, configuration scenarios much two orders
magnitude. improvements C PLEXs default parameter configuration particularly
noteworthy, though claim found new parameter configuration C PLEX
uniformly better default. Rather, given somewhat homogeneous instance set, find
configuration specific set typically outperforms default, sometimes factor high
20. Note achieved results even though intimately familiar C PLEX
parameters; chose parameters optimize well values consider based
single person-day studying C PLEX user manual. success automated algorithm
configuration even extreme conditions demonstrates potential approach.
ParamILS source code executable freely available
http://www.cs.ubc.ca/labs/beta/Projects/ParamILS/,
along quickstart guide data configuration scenarios studied article.9
order apply ParamILS, automated algorithm configuration methods, practitioner must supply following ingredients.
parameterized algorithm must possible set configurable parameters externally, e.g., command line call. Often, search hard-coded parameters hidden
algorithms source code lead large number additional parameters exposed.
Domains parameters Algorithm configurators must provided allowable
values parameter. Depending configurator, may possible include additional knowledge dependencies parameters, conditional parameters
supported ParamILS. use ParamILS, numerical parameters must discretized
finite number choices. Depending type parameter, uniform spacing
values spacing, uniform log scale, typically reasonable.
set problem instances homogeneous problem set interest is, better
expect algorithm configuration procedure perform it. possible
configure algorithm good performance rather heterogeneous instance sets (e.g.,
industrial SAT instances, PEAR reported Section 8.2), results
homogeneous subsets interest improve configure instances subset. Whenever possible, set instances split disjoint training test sets
order safeguard over-tuning. configuring small and/or heterogeneous
benchmark set, ParamILS (or configuration procedure) might find configurations perform well independent test set.
objective function used median performance first study ParamILS
(Hutter et al., 2007), since found cases optimizing median performance led
parameter configurations good median poor overall performance. cases,
optimizing mean performance yielded robust parameter configurations. However,
optimizing mean performance one define cost unsuccessful runs.
article, penalized runs counting ten times cutoff time.
deal unsuccessful runs principled manner open research question.
9. ParamILS continues actively developed; currently maintained Chris Fawcett.

301

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

cutoff time unsuccessful runs smaller cutoff time run target
algorithm chosen, quickly configuration procedure able explore
configuration space. However, choosing small cutoff risks failure mode experienced C P L E X -QP scenario. Recall there, choosing 300 seconds timeout
yielded parameter configuration good judged cutoff time (see
Figure 6(f)), performed poorly longer cutoffs (see Figure 6(e)). experiments, parameter configurations performing well low cutoff times turned scale
well harder problem instances well. many configuration scenarios, fact, noticed
automatically-found parameter configurations showed much better scaling behaviour
default configuration. attribute use mean runtime configuration
objective. mean often dominated hardest instances distribution. However,
manual tuning, algorithm developers typically pay attention easier instances, simply
repeated profiling hard instances takes long. contrast, patient automatic
configurator achieve better results avoids bias.
Computational resources amount (computational) time required application
automated algorithm configuration clearly depends target application. target
algorithm takes seconds solve instances homogeneous benchmark set interest,
experience single five-hour configuration run suffice yield good results
domains achieved good results configuration times short half
hour. contrast, runs target algorithm slow performance large
cutoff time expected yield good results instances interest, time
requirements automated algorithm configuration grow. regularly perform multiple
parallel configuration runs pick one best training performance order deal
variance across configuration runs.
Overall, firmly believe automated algorithm configuration methods ParamILS
play increasingly prominent role development high-performance algorithms
applications. study methods rich fruitful research area many interesting questions remaining explored.
ongoing work, currently developing methods adaptively adjust domains
integer-valued continuous parameters configuration process. Similarly, plan
enhance ParamILS dedicated methods dealing continuous parameters require discretization user. Another direction development concerns strategic
selection problem instances used evaluation configurations instance-specific cutoff times used context. heuristically preventing configuration procedure spending inordinate amounts time trying evaluate poor parameter settings hard problem
instances, possible improve scalability.
believe significant room combining aspects methods studied
concepts related work similar algorithm configuration problems. particular,
believe would fruitful integrate statistical testing methodsas used, e.g., F-Race
ParamILS. Furthermore, see much potential use response surface models
active learning, believe combined approach. Finally, algorithm
configuration problem studied article significant practical importance, much
gained studying methods related problems, particular, instance-specific algorithm
configuration online adjustment parameters run algorithm.
302

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

Acknowledgments
thank Kevin Murphy many helpful discussions regarding work. thank Domagoj
Babic, author PEAR, Dave Tompkins, author UBCSAT APS implementation
used experiments. thank researchers provided instances instance
generators used work, particular Gent et al. (1999), Gomes Selman (1997), LeytonBrown et al. (2000), Babic Hu (2007), Zarpas (2005), Le Berre Simon (2004), Akturk
et al. (2007), Atamturk Munoz (2004), Atamturk (2003), Andronescu et al. (2007). Lin
Xu created specific sets QCP SWGCP instances used. Thanks Chris Fawcett
Ashique KhudaBukhsh comments draft article. Finally, thank
anonymous reviewers well Rina Dechter Adele Howe valuable feedback. Thomas
Stutzle acknowledges support F.R.S.-FNRS, Research Associate. Holger
Hoos acknowledges support NSERC Discovery Grant 238788.

References
Adenso-Diaz, B. & Laguna, M. (2006). Fine-tuning algorithms using fractional experimental design
local search. Operations Research, 54(1), 99114.
Akturk, S. M., Atamturk, A., & Gurel, S. (2007). strong conic quadratic reformulation machine-job
assignment controllable processing times. Research Report BCOL.07.01, University CaliforniaBerkeley.
Andronescu, M., Condon, A., Hoos, H. H., Mathews, D. H., & Murphy, K. P. (2007). Efficient parameter
estimation RNA secondary structure prediction. Bioinformatics, 23, i19i28.
Atamturk, A. (2003). facets mixedinteger knapsack polyhedron. Mathematical Programming,
98, 145175.
Atamturk, A. & Munoz, J. C. (2004). study lot-sizing polytope. Mathematical Programming, 99,
443465.
Audet, C. & Orban, D. (2006). Finding optimal algorithmic parameters using mesh adaptive direct search
algorithm. SIAM Journal Optimization, 17(3), 642664.
Babic, D. & Hu, A. J. (2007). Structural Abstraction Software Verification Conditions. W. Damm, H. H.
(Ed.), Computer Aided Verification: 19th International Conference, CAV 2007, volume 4590 Lecture
Notes Computer Science, (pp. 366378). Springer Verlag, Berlin, Germany.
Balaprakash, P., Birattari, M., & Stutzle, T. (2007). Improvement strategies F-Race algorithm: Sampling design iterative refinement. Bartz-Beielstein, T., Aguilera, M. J. B., Blum, C., Naujoks,
B., Roli, A., Rudolph, G., & Sampels, M. (Eds.), 4th International Workshop Hybrid Metaheuristics (MH07), (pp. 108122).
Bartz-Beielstein, T. (2006). Experimental Research Evolutionary Computation: New Experimentalism. Natural Computing Series. Springer Verlag, Berlin, Germany.
Battiti, R., Brunato, M., & Mascia, F. (2008). Reactive Search Intelligent Optimization, volume 45
Operations research/Computer Science Interfaces. Springer Verlag. Available online http://reactivesearch.org/thebook.
Birattari, M. (2004). Problem Tuning Metaheuristics Seen Machine Learning Perspective.
PhD thesis, Universite Libre de Bruxelles, Brussels, Belgium.
Birattari, M., Stutzle, T., Paquete, L., & Varrentrapp, K. (2002). racing algorithm configuring metaheuristics. Langdon, W. B., Cantu-Paz, E., Mathias, K., Roy, R., Davis, D., Poli, R., Balakrishnan, K.,
Honavar, V., Rudolph, G., Wegener, J., Bull, L., Potter, M. A., Schultz, A. C., Miller, J. F., Burke, E.,
& Jonoska, N. (Eds.), Proceedings Genetic Evolutionary Computation Conference (GECCO2002), (pp. 1118). Morgan Kaufmann Publishers, San Francisco, CA, USA.
303

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

Carchrae, T. & Beck, J. C. (2005). Applying machine learning low-knowledge control optimization
algorithms. Computational Intelligence, 21(4), 372387.
Cavazos, J. & OBoyle, M. F. P. (2006). Method-specific dynamic compilation using logistic regression.
Cook, W. R. (Ed.), Proceedings ACM SIGPLAN International Conference Object-Oriented Programming, Systems, Languages, Applications (OOPSLA-06), (pp. 229240)., New York, NY, USA.
ACM Press.
Coy, S. P., Golden, B. L., Runger, G. C., & Wasil, E. A. (2001). Using experimental design find effective
parameter settings heuristics. Journal Heuristics, 7(1), 7797.
Diao, Y., Eskesen, F., Froehlich, S., Hellerstein, J. L., Spainhower, L., & Surendra, M. (2003). Generic online
optimization multiple configuration parameters application database server. Brunner, M. &
Keller, A. (Eds.), 14th IFIP/IEEE International Workshop Distributed Systems: Operations Management (DSOM-03), volume 2867 Lecture Notes Computer Science, (pp. 315). Springer Verlag,
Berlin, Germany.
Fawcett, C., Hoos, H. H., & Chiarandini, M. (2009). automatically configured modular algorithm post
enrollment course timetabling. Technical Report TR-2009-15, University British Columbia, Department
Computer Science.
Gagliolo, M. & Schmidhuber, J. (2006). Dynamic algorithm portfolios. Amato, C., Bernstein, D., & Zilberstein, S. (Eds.), Ninth International Symposium Artificial Intelligence Mathematics (AI-MATH-06).
Gagliolo, M. & Schmidhuber, J. (2007). Learning restart strategies. Veloso, M. M. (Ed.), Proceedings
Twentieth International Joint Conference Artificial Intelligence (IJCAI07), volume 1, (pp. 792
797). Morgan Kaufmann Publishers, San Francisco, CA, USA.
Gebruers, C., Hnich, B., Bridge, D., & Freuder, E. (2005). Using CBR select solution strategies constraint programming. Munoz-Avila, H. & Ricci, F. (Eds.), Proceedings 6th International Conference Case Based Reasoning (ICCBR05), volume 3620 Lecture Notes Computer Science, (pp.
222236). Springer Verlag, Berlin, Germany.
Gent, I. P., Hoos, H. H., Prosser, P., & Walsh, T. (1999). Morphing: Combining structure randomness.
Hendler, J. & Subramanian, D. (Eds.), Proceedings Sixteenth National Conference Artificial
Intelligence (AAAI99), (pp. 654660)., Orlando, Florida. AAAI Press / MIT Press, Menlo Park, CA,
USA.
Gomes, C. P. & Selman, B. (1997). Problem structure presence perturbations. Kuipers, B. &
Webber, B. (Eds.), Proceedings Fourteenth National Conference Artificial Intelligence (AAAI97),
(pp. 221226). AAAI Press / MIT Press, Menlo Park, CA, USA.
Gratch, J. & Chien, S. A. (1996). Adaptive problem-solving large-scale scheduling problems: case
study. Journal Artificial Intelligence Research, 4, 365396.
Gratch, J. & Dejong, G. (1992). Composer: probabilistic solution utility problem speed-up
learning. Rosenbloom, P. & Szolovits, P. (Eds.), Proceedings Tenth National Conference
Artificial Intelligence (AAAI92), (pp. 235240). AAAI Press / MIT Press, Menlo Park, CA, USA.
Hoos, H. H. (2002). mixture-model behaviour SLS algorithms SAT. Proceedings
Eighteenth National Conference Artificial Intelligence (AAAI-02), (pp. 661667)., Edmonton, Alberta,
Canada.
Hoos, H. H. (2008). Computer-aided design high-performance algorithms. Technical Report TR-2008-16,
University British Columbia, Department Computer Science.
Hoos, H. H. & Stutzle, T. (2005). Stochastic Local Search Foundations & Applications. Morgan Kaufmann
Publishers, San Francisco, CA, USA.
Horvitz, E., Ruan, Y., Gomes, C. P., Kautz, H., Selman, B., & Chickering, D. M. (2001). Bayesian
approach tackling hard computational problems. Breese, J. S. & Koller, D. (Eds.), Proceedings
Seventeenth Conference Uncertainty Artificial Intelligence (UAI01), (pp. 235244). Morgan
Kaufmann Publishers, San Francisco, CA, USA.
304

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

Hutter, F. (2009). Automated Configuration Algorithms Solving Hard Computational Problems. PhD
thesis, University British Columbia, Department Computer Science, Vancouver, Canada.
Hutter, F., Babic, D., Hoos, H. H., & Hu, A. J. (2007). Boosting Verification Automatic Tuning Decision
Procedures. Proceedings Formal Methods Computer Aided Design (FMCAD07), (pp. 2734).,
Washington, DC, USA. IEEE Computer Society.
Hutter, F., Hamadi, Y., Hoos, H. H., & Leyton-Brown, K. (2006). Performance prediction automated
tuning randomized parametric algorithms. Benhamou, F. (Ed.), Principles Practice Constraint Programming CP 2006: Twelfth International Conference, volume 4204 Lecture Notes
Computer Science, (pp. 213228). Springer Verlag, Berlin, Germany.
Hutter, F., Hoos, H., & Leyton-Brown, K. (2009). Tradeoffs empirical evaluation competing algorithm designs. Technical Report TR-2009-21, University British Columbia, Department Computer
Science.
Hutter, F., Hoos, H. H., Leyton-Brown, K., & Murphy, K. P. (2009). experimental investigation
model-based parameter optimisation: SPO beyond. Proceedings Genetic Evolutionary
Computation Conference (GECCO-2009), (pp. 271278).
Hutter, F., Hoos, H. H., & Stutzle, T. (2005). Efficient stochastic local search MPE solving. Proceedings
Nineteenth International Joint Conference Artificial Intelligence (IJCAI05), (pp. 169174).
Hutter, F., Hoos, H. H., & Stutzle, T. (2007). Automatic algorithm configuration based local search.
Howe, A. & Holte, R. C. (Eds.), Proceedings Twenty-second National Conference Artificial
Intelligence (AAAI07), (pp. 11521157). AAAI Press / MIT Press, Menlo Park, CA, USA.
Hutter, F., Tompkins, D. A. D., & Hoos, H. H. (2002). Scaling probabilistic smoothing: Efficient dynamic
local search SAT. Hentenryck, P. V. (Ed.), Principles Practice Constraint Programming
CP 2002: Eighth International Conference, volume 2470 Lecture Notes Computer Science, (pp.
233248). Springer Verlag, Berlin, Germany.
Johnson, D. S. (2002). theoreticians guide experimental analysis algorithms. Goldwasser,
M. H., Johnson, D. S., & McGeoch, C. C. (Eds.), Data Structures, Near Neighbor Searches, Methodology: Fifth Sixth DIMACS Implementation Challenges, (pp. 215250). American Mathematical Society, Providence, RI, USA.
Jones, D. R., Schonlau, M., & Welch, W. J. (1998). Efficient global optimization expensive black box
functions. Journal Global Optimization, 13, 455492.
Kask, K. & Dechter, R. (1999). Stochastic local search Bayesian networks. Seventh International
Workshop Artificial Intelligence Statistics (AISTATS99).
Kautz, H., Horvitz, E., Ruan, Y., Gomes, C. P., & Selman, B. (2002). Dynamic restart policies. Dechter,
R., Kearns, M., & Sutton, R. (Eds.), Proceedings Eighteenth National Conference Artificial
Intelligence (AAAI02), (pp. 674681). AAAI Press / MIT Press, Menlo Park, CA, USA.
KhudaBukhsh, A., Xu, L., Hoos, H. H., & Leyton-Brown, K. (2009). SATenstein: Automatically building local search sat solvers components. Proceedings Twenty-first International Joint Conference
Artificial Intelligence (IJCAI09), (pp. 517524).
Le Berre, D. & Simon, L. (2004). Fifty-five solvers Vancouver: SAT 2004 competition. Hoos, H. H.
& Mitchell, D. G. (Eds.), Theory Applications Satisfiability Testing: Proceedings Seventh
International Conference (SAT04), volume 3542 Lecture Notes Computer Science, (pp. 321344).
Springer Verlag.
Leyton-Brown, K., Nudelman, E., & Shoham, Y. (2002). Learning empirical hardness optimization
problems: case combinatorial auctions. Hentenryck, P. V. (Ed.), Principles Practice
Constraint Programming CP 2002: Eighth International Conference, volume 2470 Lecture Notes
Computer Science, (pp. 556572). Springer Verlag, Berlin, Germany.
Leyton-Brown, K., Pearson, M., & Shoham, Y. (2000). Towards universal test suite combinatorial
auction algorithms. Jhingran, A., Mason, J. M., & Tygar, D. (Eds.), EC 00: Proceedings 2nd
305

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

ACM conference Electronic commerce, (pp. 6676)., New York, NY, USA. ACM.
Lourenco, H. R., Martin, O., & Stutzle, T. (2002). Iterated local search. F. Glover & G. Kochenberger
(Eds.), Handbook Metaheuristics (pp. 321353). Kluwer Academic Publishers, Norwell, MA, USA.
Maron, O. & Moore, A. (1994). Hoeffding races: Accelerating model selection search classification
function approximation. Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.), Advances Neural
Information Processing Systems 7 (NIPS-94), volume 6, (pp. 5966). Morgan Kaufmann Publishers, San
Francisco, CA, USA.
Mengshoel, O. J. (2008). Understanding role noise stochastic local search: Analysis experiments. Artificial Intelligence, 172(8-9), 955990.
Minton, S. (1993). analytic learning system specializing heuristics. Bajcsy, R. (Ed.), Proceedings
Thirteenth International Joint Conference Artificial Intelligence (IJCAI93), (pp. 922929). Morgan
Kaufmann Publishers, San Francisco, CA, USA.
Minton, S. (1996). Automatically configuring constraint satisfaction programs: case study. Constraints,
1(1), 140.
Minton, S., Johnston, M. D., Philips, A. B., & Laird, P. (1992). Minimizing conflicts: heuristic repair
method constraint-satisfaction scheduling problems. Artificial Intelligence, 58(1), 161205.
Patterson, D. J. & Kautz, H. (2001). Auto-WalkSAT: self-tuning implementation WalkSAT. Electronic
Notes Discrete Mathematics (ENDM), 9.
Ridge, E. & Kudenko, D. (2006). Sequential experiment designs screening tuning parameters
stochastic heuristics. Paquete, L., Chiarandini, M., & Basso, D. (Eds.), Workshop Empirical Methods
Analysis Algorithms Ninth International Conference Parallel Problem Solving
Nature (PPSN), (pp. 2734).
Santner, T. J., Williams, B. J., & Notz, W. I. (2003). Design Analysis Computer Experiments.
Springer Verlag, New York.
Selman, B., Levesque, H. J., & Mitchell, D. (1992). new method solving hard satisfiability problems.
Rosenbloom, P. & Szolovits, P. (Eds.), Proceedings Tenth National Conference Artificial
Intelligence (AAAI92), (pp. 440446). AAAI Press / MIT Press, Menlo Park, CA, USA.
Spall, J. C. (2003). Introduction Stochastic Search Optimization. New York, NY, USA: John Wiley &
Sons, Inc.
Terashima-Marn, H., Ross, P., & Valenzuela-Rendon, M. (1999). Evolution constraint satisfaction strategies examination timetabling. Proceedings Genetic Evolutionary Computation Conference
(GECCO-1999), (pp. 635642). Morgan Kaufmann.
Thachuk, C., Shmygelska, A., & Hoos, H. H. (2007). replica exchange monte carlo algorithm protein
folding hp model. BMC Bioinformatics, 8, 342342.
Tompkins, D. A. D. & Hoos, H. H. (2004). UBCSAT: implementation experimentation environment
SLS algorithms SAT & MAX-SAT. Theory Applications Satisfiability Testing: Proceedings Seventh International Conference (SAT04), volume 3542, (pp. 306320). Springer Verlag,
Berlin, Germany.
Xu, L., Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2008). SATzilla: portfolio-based algorithm selection
SAT. Journal Artificial Intelligence Research, 32, 565606.
Zarpas, E. (2005). Benchmarking SAT Solvers Bounded Model Checking. Bacchus, F. & Walsh, T.
(Eds.), Theory Applications Satisfiability Testing: Proceedings Eighth International Conference (SAT05), volume 3569 Lecture Notes Computer Science, (pp. 340354). Springer Verlag.

306


