journal artificial intelligence

submitted published

acquiring word meaning mappings
natural language interfaces
cynthia thompson

cindi cs utah edu

school computing university utah
salt lake city ut

raymond j mooney

mooney cs utexas edu

department computer sciences university texas
austin tx

abstract
focuses system wolfie word learning interpreted examples acquires semantic lexicon corpus sentences paired semantic
representations lexicon learned consists phrases paired meaning representations wolfie part integrated system learns transform sentences
representations logical database queries
experimental presented demonstrating wolfies ability learn useful
lexicons database interface four different natural languages usefulness
lexicons learned wolfie compared acquired similar system
favorable wolfie second set experiments demonstrates wolfies ability
scale larger difficult albeit artificially generated corpora
natural language acquisition difficult gather annotated data needed
supervised learning however unannotated data fairly plentiful active learning
methods attempt select annotation training informative examples
therefore potentially useful natural language applications however
date active learning considered standard classification tasks
reduce annotation effort maintaining accuracy apply active learning semantic
lexicons active learning significantly reduce number annotated
examples required achieve given level performance

introduction overview
long standing goal field artificial intelligence enable computer understanding human languages much progress made reaching goal much
remains done artificial intelligence systems meet goal first need
ability parse sentences transform representation easily
manipulated computers several knowledge sources required parsing
grammar lexicon parsing mechanism
natural language processing nlp researchers traditionally attempted build
knowledge sources hand often resulting brittle inefficient systems take
significant effort build goal overcome knowledge acquisition
bottleneck applying methods machine learning develop apply methods
empirical corpus nlp learn semantic lexicons active learning
reduce annotation effort required learn

c

ai access foundation morgan kaufmann publishers rights reserved

fithompson mooney

semantic lexicon one nlp component typically challenging time consuming construct update hand notion semantic lexicon formally defined
section list phrase meaning pairs meaning representation
determined language understanding task hand taking compositional view sentence meaning partee meulen wall describes
system wolfie word learning interpreted examples acquires semantic
lexicon phrase meaning pairs corpus sentences paired semantic representations goal automate lexicon construction integrated nlp system
acquires semantic lexicons parsers natural language interfaces single
training set annotated sentences
although many others sebillot bouillon fabre riloff jones siskind
hastings grefenstette brent presented systems learning
information lexical semantics present system learning lexicons phrasemeaning pairs work unique combination several features though
prior work included aspects first output used system
chill zelle mooney zelle learns parse sentences semantic
representations second uses fairly straightforward batch greedy heuristic learning
requires small number examples generalize well third
easily extendible representation formalisms fourth requires prior knowledge
although exploit initial lexicon provided finally simplifies learning
making several assumptions training data described
section
test wolfies ability acquire semantic lexicon natural language interface
geographical database corpus queries collected human subjects
annotated logical form test wolfie integrated chill
learns parsers requires semantic lexicon previously built manually
demonstrate final acquired parser performs nearly accurately answering novel
questions learned lexicon hand built lexicon wolfie
compared alternative lexicon acquisition system developed siskind
demonstrating superior performance task finally corpus translated
spanish japanese turkish experiments conducted demonstrating ability
learn successful lexicons parsers variety languages
second set experiments demonstrates wolfies ability scale larger
difficult albeit artificially generated corpora overall demonstrate robust
ability acquire accurate lexicons directly usable semantic parsing
integrated system task building semantic parser domain simplified
single representative corpus sentence representation pairs allows acquisition
semantic lexicon parser generalizes well novel sentences
building annotated corpus arguably less work building entire nlp
system still simple task redundancies errors may occur training data
goal minimize annotation effort yet still achieve reasonable level
generalization performance case natural language frequently large
amount unannotated text available would automatically intelligently
choose available sentences annotate



fiacquiring word meaning mappings

technique called active learning active learning
area machine learning features systems automatically select informative examples annotation training cohn atlas ladner primary goal
active learning reduce number examples system trained thereby
reducing example annotation cost maintaining accuracy acquired information demonstrate usefulness active learning techniques compared
accuracy parsers lexicons learned examples chosen active learning
lexicon acquisition learned randomly chosen examples finding active
learning saved significant annotation cost training randomly chosen examples
savings demonstrated geography query domain
summary provides statement lexicon acquisition
demonstrates machine learning technique solving next combining previous entire natural language interface
acquired one training corpus demonstrate application active
learning techniques minimize number sentences annotate training input
integrated learning system
remainder organized follows section gives background
information chill introduces siskinds lexicon acquisition system
compare wolfie section sections formally define learning
describe wolfie detail section present discuss experiments
evaluating wolfies performance learning lexicons database query domain
artificial corpus next section describes evaluates use active learning
techniques wolfie sections discuss related future directions
respectively finally section summarizes

background
section give overview chill system adds
describe jeff siskinds lexicon acquisition system
chill
output produced wolfie used assist larger language acquisition system
particular currently used part input parser acquisition system called
chill constructive heuristics induction language learning chill uses inductive
logic programming muggleton lavrac dzeroski learn deterministic
shift reduce parser tomita written prolog input chill corpus
sentences paired semantic representations input required wolfie
parser learned capable mapping sentences correct representations well
generalizing well novel sentences limit discussion chills
ability acquire parsers map natural language questions directly prolog queries
executed produce answer zelle mooney following two
sample queries database u geography paired corresponding prolog
query



fithompson mooney

sentence representation
training
examples

wolfie

chill

lexicon
phrase meaning

final
parser
prolog

figure integrated system
capital state biggest population
answer c capital c largest p state population p
state texarkana located
answer state eq c cityid texarkana loc c
chill treats parser induction learning rules control actions
shift reduce parser parsing current context maintained stack
buffer containing remaining input parsing complete stack contains
representation input sentence three types operators parser uses
construct logical queries one introduction onto stack predicate needed
sentence representation due phrases appearance front input buffer
operators require semantic lexicon background knowledge details
two parsing operators see zelle mooney wolfie
lexicon provided automatically figure illustrates complete system
jeff siskinds lexicon learning
closely related previous automated lexicon acquisition
siskind inspired work rayner hugosson hagert
comparing system section describe main features
section goal one cognitive modeling childrens acquisition
lexicon lexicon used comprehension generation goal
machine learning engineering one focuses lexicon comprehension
use parsing learning process claim cognitive plausibility
goal learning lexicon generalizes well small number training
examples
system takes incremental acquiring lexicon learning proceeds
two stages first stage learns symbols representation used


fiacquiring word meaning mappings

capital capital
biggest largest
highest point high point
traverse
loc

state state
loc
long len
capital capital

figure sample semantic lexicon
final conceptual expression represents meaning word versionspace second stage learns symbols put together form
final representation example learning meaning word raise
may learn set cause go first stage put together
form expression cause x go second stage
siskind shows effectiveness series artificial corpora
system handles noise lexical ambiguity referential uncertainty large corpora usefulness lexicons learned compared correct artificial
lexicon goal experiments presented evaluate correctness
completeness learned lexicons earlier work siskind evaluated versions
technique quite small corpus real english japanese sentences extend
evaluation demonstration systems usefulness performing real world natural
language processing tasks larger corpus real sentences

lexicon acquisition
although end goal acquire entire natural language interface currently
divide task two parts lexicon acquisition component parser acquisition
component section discuss acquiring semantic lexicons
assist parsing acquisition parsers training input consists natural language
sentences paired meaning representations pairs extract lexicon
consisting phrases paired meaning representations training pairs
given previous section sample lexicon shown figure
formal definition
present learning formally definitions needed
following use terms string substring extend straight forwardly
natural language sentences phrases respectively refer labeled trees making
assumption semantic meanings interest represented
common representations recast labeled trees forests formalism extends
easily latter
definition let v e finite alphabets vertex labels edge labels respectively
let v finite nonempty set vertices l total function l v v e set unordered
pairs distinct vertices called edges total function e e g v l e
labeled graph



fithompson mooney

string girl ate pasta cheese
vertex edge labels

tree




ingest
patient
agent
person food
age type accomp
sex








female

child pasta

food

type
cheese



interpretation f
f girl
f ate
f pasta
f cheese

figure labeled trees interpretations
definition labeled tree connected acyclic labeled graph
figure shows labeled tree vertices left associated vertex
edge labels right function l


ingest person food female child pasta
food cheese

tree semantic representation sentence girl ate pasta
cheese conceptual dependency schank representation prolog list form
meaning
ingest

agent person sex female age child
patient food type pasta accomp food type cheese

definition u v path graph g finite alternating sequence vertices edges
g vertex repeated begins vertex u ends vertex v
edge sequence connects vertex precedes sequence
vertex follows sequence
definition directed labeled tree v l e labeled tree whose edges consist
ordered pairs vertices distinguished vertex r called root property
every v v directed r v path underlying
undirected unlabeled graph induced v e connected acyclic graph
definition interpretation f finite string directed labeled tree
one one function mapping subset substrings two strings
overlap vertices root range f
omit enumeration function e could given similar manner example
agent element e



fiacquiring word meaning mappings

girl

person
sex
age
female

pasta food

type

child

pasta

cheese food
type
cheese

ate ingest

figure meanings
interpretation provides information parts meaning sentence
originate phrases figure interpretation f
note domain f since subset substrings thus
allowing words meaning disallow overlapping substrings
domain cheese cheese could map vertices
definition given interpretation f string tree element p domain
f meaning p relative f connected subgraph whose vertices
include f p descendents except vertices range f
descendents
meanings sense concern lowest level phrasal meanings occurring
terminal nodes semantic grammar namely entries semantic lexicon
grammar used construct meanings longer phrases entire sentences
motivation previously stated constraint root must included
range f want vertices sentence representation included
meaning phrase note meaning p directed tree f p
root figure shows meanings phrase domain interpretation function
f shown figure labels vertices edges readability
definition given finite set st f triples f sn tn fn
si finite string ti directed labeled tree interpretation function
si ti let language lst f p pk st f union substrings
occur domain pj lst f meaning set pj denoted
mst f pj set meanings pj relative si ti si ti st f
consider two meanings isomorphic trees taking labels
account
example given sentence man ate cheese labeled tree pictured
figure f defined f ate f man f cheese
consider two substrings string contain characters order
irrespective positions within larger string occur
omit subscript set st f obvious context



fithompson mooney

string man ate cheese
tree

vertex edge labels
ingest
patient
agent








person food
type
age
sex




male

adult

cheese

figure second tree
meaning set cheese respect st f f f food
type cheese one meaning though f f map cheese different vertices
two trees subgraphs denoting meaning cheese two
functions isomorphic
definition given finite set st f triples f sn tn fn
si finite string ti directed labeled tree interpretation
function si ti covering lexicon expressed st f
p p lst f p
covering lexicon l expressed st f f f


girl person sex female age child
man person sex male age adult
ate ingest
pasta food type pasta
cheese food type cheese

idea covering lexicon provides string sentence si meaning
phrases sentence meanings trees whose labeled
vertices together include labeled vertices tree ti representing meaning
si vertices duplicated containing vertices ti edge labels may
may included since idea due syntax
parser provide edges capturing lexical semantics lexicon note
include covering lexicon phrases substrings domains
words empty tree meaning included covering lexicon
note general use phrase mean substrings sentences whether
consist one word one finally strings covering lexicon may
contain overlapping words even though domain individual interpretation
function must since overlapping words could occurred different sentences
finally ready define learning hand



fiacquiring word meaning mappings

lexicon acquisition
given multiset strings sn multiset labeled trees tn
multiset interpretation functions f f fn cardinality
covering lexicon expressed st f f sn tn fn minimized
set found say found minimal set interpretations minimal
covering lexicon
less formally learner presented multiset sentences paired
meanings goal learning smallest lexicon consistent data
lexicon paired listing phrases occurring domain f
f multiset interpretation functions found elements
meaning sets motivation finding lexicon minimal size usual bias
towards simplicity representation generalization beyond training data
definition allows phrases length usually want limit length
phrases considered inclusion domain interpretation functions
efficiency purposes
determine set interpretation functions set strings trees
one unique covering lexicon expressed st f however might
set interpretation functions possible may lexicon smallest
cardinality example covering lexicon given previous example
minimal covering lexicon two sentences given could minimal though
rather degenerate lexicons


girl
man

ingest agent person sex female age child
patient food type pasta accomp food type cheese
ingest agent person sex male age adult
patient food type cheese

type lexicon becomes less likely size corpus grows
implications definition
definition lexicon acquisition differs given authors
including riloff jones siskind manning brent others
discussed section definition makes assumptions
training input first making f function instead relation definition
assumes meaning phrase sentence appears representation
sentence single use assumption second making f one one assumes
exclusivity vertex sentences representation due one phrase
sentence third assumes phrases meaning connected subgraph sentences
representation distributed representation connectedness assumption
first assumption may hold representation languages present
domains considered second third assumptions perhaps
less problematic respect general language use
definition assumes compositionality meaning sentence derived
meanings phrases contains addition perhaps connecting
information specific representation hand derived external sources


fithompson mooney

noise words vertices sentences representation included
within meaning word phrase sentence assumption similar
linking rules jackendoff used previous work grammar
language acquisition e g haas jayaraman siskind
debate linguistics community ability compositional techniques
handle phenomena fillmore goldberg making assumption simplifies
learning process works reasonably domains interest since
allow multi word phrases lexicon e g kick bucket die one objection
compositionality addressed
definition allows training input
words phrases multiple meanings homonymy might occur
lexicon
several phrases map meaning synonymy might occur
lexicon
words sentence map meanings leaving unused
assignment words meanings
phrases contiguous words map parts sentences meaning representation
particular note lexical ambiguity note could derived
ambiguous lexicon


girl person sex female age child
ate ingest
ate ingest agent person sex male age adult
pasta food type pasta
cheese food type cheese

sample corpus lexicon ate ambiguous word earlier example
minimizes ambiguity resulting alternative intuitively pleasing lexicon
definition first minimizes number entries lexicon learning
exploit preference minimizing ambiguity
note definition allows training input sentences
ambiguous paired one meaning since given sentence multiset
might appear multiple times appear one meaning fact training data
consider section ambiguous sentences
definition lexicon acquisition fit cleanly traditional
definition learning classification training example contains sentence
semantic parse trying extract semantic information phrases
sentence example potentially contains information multiple target
concepts phrases trying pick relevant features vertices
fact assumptions except single use made siskind see section
details
words may however serve cues parser assemble sentence meanings word
meanings



fiacquiring word meaning mappings

representation corresponding correct meaning phrase course assumptions single use exclusivity connectedness compositionality impose additional
constraints addition multiple examples one learning scenario
access negative examples derive implicit negatives
possibility ambiguous synonymous phrases
ways related clustering capable learning
multiple potentially non disjoint categories however clear clustering
system could made learn phrase meaning mappings needed parsing finally
current systems learn multiple concepts commonly use examples concepts
negative examples concept currently learned implicit assumption made
concepts disjoint unwarranted assumption presence
synonymy

wolfie example
section first discuss issues considered design
describe fully section
solving lexicon acquisition
first attempt solve lexicon acquisition might examine interpretation functions across corpus choose one minimal lexicon size
number possible interpretation functions given input pair dependent
size sentence representation sentence w words w
possible phrases particular challenge
however number possible interpretation functions grows extremely quickly
size input sentence p phrases associated tree n vertices
number possible interpretation functions
c n

c
x




n c



c min p n derivation formula follows must choose
phrases use domain f choose one phrase two
number min p n n p assign n phrases since f one one
p






p
p

number phrases chosen permute phrases
order assigned vertices different permutations must choose vertices include range interpretation
function choose root time choosing vertices
n choose vertices left choosing root
n






n

n


fithompson mooney

full number possible interpretation functions
min p n

x


p
n


p
n

simplifies equation n p largest term equation c
p grows least exponentially p general number interpretation
functions large allow enumeration therefore finding lexicon examining
interpretations across corpus choosing lexicon minimum size clearly
tractable
instead finding interpretations one could set candidate meanings
phrase final meaning phrase could chosen way
minimizes lexicon size one way candidate meanings fracture meanings
sentences phrase appears siskind defined fracturing calls
unlink operation terms includes subterms
expression plus representation formalism corresponds finding possible
connected subgraphs meaning adding empty graph interpretation
function technique discussed fracturing would lead exponential blowup
number candidate meanings phrase lower bound number connected
subgraphs full binary tree n vertices obtained noting subset
n leaves may deleted still maintain connectivity remaining tree
thus counting ways leaves deleted gives us lower bound n
fractures completely rule fracturing part technique lexicon
learning since trees tend get large indeed siskind uses many
systems constraints help control search however wish avoid
chance exponential blowup preserve generality tasks
another option force chill essentially induce lexicon
model would provide chill ambiguous lexicon phrase paired
every fracture every sentence appears chill would decide
set fractures leads correct parse training sentence would
include final learned parser lexicon combination thus search would
become exponential furthermore even small representations would likely lead
system poor generalization ability siskinds work e g siskind
took syntactic constraints account encounter difficulties
versions handle lexical ambiguity
could efficiently good candidates standard induction could
attempt use source training examples phrase however
attempt use list candidate meanings one phrase negative examples
another phrase would flawed learner could know advance phrases
possibly synonymous thus phrase lists use negative examples
phrase meanings many representation components would present lists
one phrase source conflicting evidence learner even without
presence synonymy since positive examples available one might think
specific conjunctive learning finding intersection representations
thanks net citizen dan hirshberg help analysis



fiacquiring word meaning mappings

phrase p two words
collect training examples p appears
calculate lics sampled pairs examples representations
l lics add p l set candidate lexicon entries
input representations covered candidate lexicon entries remain
add best phrase meaning pair candidate entries lexicon
update candidate meanings phrases sentences phrase learned
return lexicon learned phrase meaning pairs

figure wolfie overview
phrase proposed anderson however meanings ambiguous
phrase disjunctive intersection would empty similar difficulty would
expected positive compression muggleton
solution wolfie
analysis leads us believe lexicon acquisition computationally intractable therefore perform efficient search best lexicon
use standard induction therefore implemented wolfie
outlined figure finds approximate solution lexicon acquisition generate set candidate lexicon entries final
learned lexicon derived greedily choosing best lexicon item point
hopes finding final minimal covering lexicon actually learn interpretation
functions guarantee covering lexicon even
search interpretation functions greedy search would guarantee covering
input course guarantee minimal lexicon found however
later present experimental demonstrating greedy performs
well
wolfie first derives initial set candidate meanings phrase
generating candidates lics attempts maximally common meaning
phrase biases toward finding small lexicon covering many vertices tree
finding lexicon actually cover input second wolfie chooses
final lexicon entries candidate set one time updating candidate set
goes taking account assumptions single use connectedness exclusivity
basic scheme choosing entries candidate set maximize prediction
meanings given phrases general meanings adds tension
lics cover many vertices generality biases towards fewer vertices however generality lics helps lead small lexicon since general meaning
likely apply widely across corpus
code available upon request first author
though course interpretation functions way guarantee covering lexicon see
siskind alternative



fithompson mooney

answer











state

eq








c

cityid

loc


c




texarkana
figure tree variables
let us explain detail way example spanish
instead english illustrate difficulty somewhat clearly consider following
corpus
cual es el capital del estado con la poblacion mas grande
answer c capital c largest p state population p
cual es la punta mas alta del estado con la area mas grande
answer p high point p largest state area
en que estado se encuentra texarkana
answer state eq c cityid texarkana loc c
que capital es la mas grande
answer largest capital
que es la area de los estados unitos
answer area c eq c countryid usa
cual es la poblacion de un estado que bordean utah
answer p population p state next eq stateid utah
que es la punta mas alta del estado con la capital madison
answer c high point b c loc c b state b
capital b eq cityid madison

sentence representations slightly different tree representations given
definition main difference addition existentially quantified
variables shared leaves representation tree mentioned section
representations prolog queries database given query create
tree conforms formalism addition quantified variables
example shown figure representation third sentence vertex
predicate name arity prolog style e g state quantified variables
leaves outgoing edge n vertex n edge labeled
argument position filled subtree rooted edge labeled
given argument position argument free variable vertex labeled


fiacquiring word meaning mappings

variable occur leaves existentially quantified variable whose scope
entire tree query learned lexicon however need maintain
identity variables across distinct lexical entries
another representation difference strip answer predicate
input learner thus allowing forest directed trees input rather single
tree definition easily extends root tree
forest must domain interpretation function
evaluation system representation given section evaluation
representation without variables forests presented section previously
thompson presented demonstrating learning representations different
form case role representation fillmore augmented conceptual dependency schank information last representation conforms directly
definition
continuing example solving lexicon acquisition
corpus let us assume simplification although required sentences
stripped phrases know empty meanings e g que es con la
similarly assume known phrases refer directly given database
constants e g location names remove phrases meaning
training input
candidate generation phase
initial candidate meanings phrase produced computing maximally common
substructure sampled pairs representations sentences contain
derive common substructure computing largest isomorphic connected subgraphs
lics two labeled trees taking labels account isomorphism analogous
largest common subgraph garey johnson solvable polynomial
time assume inputs trees k number edges include
given thus start k set equal largest number edges two trees
compared test common subgraph iterate k stopping one
subgraphs found given k
prolog query representation complicated bit variables
therefore use lics addition similar computing least general generalization first order clauses plotkin lgg two sets literals least
general set literals subsumes sets literals add allowing
term argument literal conjunction tries orderings
matching terms conjunction overall finding lics
two trees prolog representation first finds common labeled edges
vertices usual lics treats variables equivalent computes
least general generalization conjunction taken account resulting trees
converted back literals example given two trees

predicate omitted chill initializes parse stack answer predicate thus
word mapped



fithompson mooney

phrase
capital

grande
estado

punta mas
encuentra

lics
largest
capital
state
largest state
largest
largest state
state
population state
capital
high point
state loc
high point
state
state loc

sentences















table sample candidate lexical entries derivation
answer c largest p state population p capital c
answer p high point p largest state area
common meaning answer largest state note lics two trees
may unique may multiple common subtrees contain
number edges case lics returns multiple answers
sets initial candidate meanings phrases sample corpus
shown table example lics pairs phrase
appears actual randomly sample subset efficiency reasons
golem muggleton feng phrases appearing one sentence
e g encuentra entire sentence representation excluding database constant
given background knowledge used initial candidate meaning candidates
typically generalized step correct portion
representation added lexicon see example
adding final lexicon
deriving initial candidates greedy search begins heuristic used evaluate
candidates attempts help assure small covering lexicon learned heuristic
first looks weighted sum two components p phrase candidate
meaning
p p p p p p p p p
generality
ties value broken preferring less ambiguous fewer current
meanings shorter phrases first component analogous cluster evaluation


fiacquiring word meaning mappings

heuristic used cobweb fisher measures utility clusters
attribute value pairs categories instead meanings phrases probabilities
estimated training data updated learning progresses account
phrases meanings already covered see updating works
continue example goal part heuristic
maximize probability predicting correct meaning randomly sampled
phrase equality holds bayes theorem looking right side p p
expected probability meaning correctly guessed given phrase p
assumes strategy probability matching meaning chosen p
probability p p correct probability term p p biases
component common phrase interpreting left side equation
first term biases towards lexicons low ambiguity second towards low synonymy
third towards frequent meanings
second component heuristic generality computed negation
number vertices meanings tree structure helps prefer smaller
general meanings example candidate set else equal
generality portion heuristic would prefer state generality value
largest state state loc generality value
meaning estado learning meaning fewer terms helps evenly distribute
vertices sentences representation among meanings phrases sentence
thus leads lexicon likely correct see note
pairs words tend frequently co occur grande estado example
joint representation meaning likely set candidate meanings
words preferring general meaning easily ignore incorrect joint
meanings
example experiments use weight first component
heuristic weight second first component smaller absolute
values therefore given higher weight modulo consideration
overly sensitive weights automatically setting cross validation
training set kohavi john little effect overall performance table
illustrate calculation heuristic measure fourteen pairs
value calculation shows sum multiplying first component
heuristic multiplying second component first component simplified
follows
p p
p
p p p p




p
p
p number times phrase p appears corpus initial number
candidate phrases p number times meaning paired
phrase p ignore since number phrases corpus
pair effect ranking highest scoring pair estado state
added lexicon
next candidate generalization step described algorithmically figure
one key ideas phrase meaning choice constrain
candidate meanings phrases yet learned given assumption portion
representation due one phrase sentence exclusivity part


fithompson mooney

candidate lexicon entry
capital largest
capital capital
capital state
grande largest state
grande largest
estado largest state
estado state
estado population state
estado capital
estado high point
estado state loc
punta mas high point
punta mas state
encuentra state loc

heuristic value















table heuristic value sample candidate lexical entries

given learned phrase meaning pair l g
sentence representation pairs containing l g mark covered
candidate phrase meaning pair p
p occurs training pairs l g
vertices intersect vertices g
occurrences covered
remove p set candidate pairs
else
adjust heuristic value p needed account
newly covered nodes training representations
generalize remove covered nodes obtaining
calculate heuristic value candidate pair p
candidate meanings remain uncovered phrase
derive lics uncovered representations
calculate heuristic values

figure candidate generalization phase



fiacquiring word meaning mappings

representation covered phrase sentence paired meaning
least sentence therefore step candidate meanings words
sentences word learned generalized exclude representation
learned use operation analogous set difference finding remaining
uncovered vertices representation generalizing meanings eliminate covered
vertices candidate pairs example meaning largest learned
phrase sentence meaning left behind would forest consisting
trees high point state area generalization
empty tree lics calculated example since state covered
sentences candidates several words sentences
generalized example meaning state loc encuentra
generalized loc heuristic value
single use assumption allows us remove candidate pairs containing estado
set candidate meanings since learned pair covers occurrences estado
set
note pairwise matchings generate candidate items together updating candidate set enable multiple meanings learned ambiguous phrases
makes less sensitive initial rate sampling lics example
note capital ambiguous data set though ambiguity artifact
way query language designed one ordinarily think
ambiguous word however meanings learned second pair added
final lexicon grande largest causes generalization empty
meaning first candidate entry table since lics sentence
generated entire remaining meaning added candidate meaning set
capital mas
subsequently greedy search continues resulting lexicon covers training
corpus candidate phrase meanings remain rare cases learning errors occur
leave portions representations uncovered example following lexicon
learned
estado state
grande largest
area area
punta high point
poblacion population
capital capital
encuentra loc
alta loc
bordean next
capital capital
next section discuss ability wolfie learn lexicons useful
parsers parser acquisition



fithompson mooney

evaluation wolfie
following two sections discuss experiments testing wolfies success learning lexicons
real artificial corpora comparing several cases previously developed
lexicon learning system
database query application
section describes experimental database query application first
corpus discussed contains questions u geography paired prolog
query extract answer question database domain originally
chosen due availability hand built natural language interface geobase
database containing facts geobase supplied turbo prolog
borland international designed specifically domain questions
corpus collected asking undergraduate students generate english questions
database though given cursory knowledge database without
given chance use broaden test sentences translated
spanish turkish japanese japanese translations word segmented roman
orthography translated questions paired appropriate logical queries
english corpus
evaluate learned lexicons measured utility background knowledge
chill performed choosing random set test examples
learning lexicons parsers increasingly larger subsets remaining examples
increasing examples time training test examples parsed
learned parser submit resulting queries database compare
answers generated submitting correct representation database
record percentage correct matching answers difficult gold standard
retrieving correct answer avoid measures partial accuracy believe
adequately measure final utility repeated process ten different random training
test sets evaluated performance differences two tailed paired test
significance level p
compared system incremental line lexicon learner developed siskind
make equitable comparison batch ran simulated batch mode repeatedly presenting corpus times analogous running
epochs train neural network actually add kinds data
learn allows perform inter sentential inference directions corpus instead one point compare accuracy
size training corpus metric optimized siskind worried
difference execution time lexicons learned running siskinds
system incremental mode presenting corpus single time resulted substantially
lower performance preliminary experiments data removed wolfies
ability learn phrases one word since current version siskinds system
cpu times two system directly comparable since one written prolog
lisp however learning time two systems approximately siskinds
system run incremental mode seconds training examples



fiacquiring word meaning mappings







accuracy









chill handbuilt
chill testlex
chill wolfie
chill siskind
geobase












training examples





figure accuracy english geography corpus
ability finally made comparisons parsers learned chill
hand coded lexicon background knowledge
similar applications many terms state city names
whose meanings automatically extracted database therefore tests
run names given learner initial lexicon helpful
required section gives different task initial lexicon
however unless otherwise noted tests within section strip
sentences phrases known empty meanings unlike example section
comparisons english
first experiment comparison original english corpus figure shows
learning curves chill lexicons learned wolfie chill wolfie
siskinds system chill siskind uppermost curve chill handbuilt shows
chills performance given hand built lexicon chill testlex shows performance words never appear training data e g test sentences
deleted hand built lexicon since learning chance learning
finally horizontal line shows performance geobase benchmark
lexicon learned wolfie led parsers almost
accurate generated hand built lexicon best accuracy achieved
parsers hand built lexicon followed hand built lexicon words
test set removed followed wolfie followed siskinds system systems
well better geobase time reach training examples
differences wolfie siskinds system statistically significant training


fithompson mooney

lexicon
hand built
wolfie
siskind

coverage




ambiguity




entries




table lexicon comparison
example sizes wolfie learn lexicons support learning
successful parsers better perspective learned
competing system comparing chill testlex curve see
drop accuracy hand built lexicon due words test set system
seen training fact none differences chill wolfie
chill testlex statistically significant
one implicit hypotheses definition coverage training
data implies good lexicon coverage training examples wolfie versus siskind addition lexicons learned siskinds
system ambiguous larger learned wolfie wolfies lexicons average meanings per word average size entries
training examples versus meanings per word entries siskinds lexicons comparison hand built lexicon meanings per word entries
differences summarized table undoubtedly contribute final performance
differences
performance natural languages
next examined performance two systems spanish version corpus
figure shows differences wolfie siskinds learned
lexicons chill statistically significant training set sizes
performance hand built lexicons without phrases present
testing set performance compared hand built lexicon test set phrases
removed still competitive difference significant examples
figure shows accuracy learned parsers wolfies learned lexicons
four languages performance differences among four languages quite small
demonstrating methods language dependent
larger corpus
next present larger diverse corpus geography domain
additional sentences collected computer science undergraduates
introductory ai course set questions smaller corpus collected
students german class special instructions complexity queries desired
ai students tended ask complex diverse queries task give five
interesting questions associated logical form homework assignment though
direct access database requested give least
one sentence whose representation included predicate containing embedded predicates



fiacquiring word meaning mappings






accuracy






span chill handbuilt
span chill testlex
span chill wolfie
span chill siskind










training examples









figure accuracy spanish






accuracy






english
spanish
japanese
turkish










training examples

figure accuracy four languages



fithompson mooney






accuracy






chill
wolfie
geobase














training examples









figure accuracy larger geography corpus
example largest state asked variety sentences
sentences total including original sentences
experiments split data training sentences test sentences random splits trained wolfie chill goal
see whether wolfie still effective difficult corpus since
approximately novel words sentences therefore tested performance chill extended hand built lexicon test stripped sentences
phrases known empty meanings example section
use phrases one word since seem make significant
difference domain compare wolfies lexicons chill
hand built lexicons without phrases appear test set
figure shows resulting learning curves differences chill
hand built learned lexicons statistically significant
examples four nine data points mixed indicate
difficulty domain variable vocabulary however improvement
machine learning methods geobase hand built interface much dramatic
corpus
lics versus fracturing
one component yet evaluated explicitly candidate generation
method mentioned section could use fractures representations sentences
phrase appears generate candidate meanings phrase instead
lics used compared previously described method
largest isomorphic connected subgraphs sampled pairs representations



fiacquiring word meaning mappings






accuracy






fractwolfie
wolfie










training examples





figure fracturing vs lics accuracy
candidate meanings attempt fair comparison sampled representations
fracturing number source representations number pairs
sampled lics
accuracy chill resulting learned lexicons background knowledge shown figure fracturing fractwolfie shows little advantage
none differences two systems statistically significant
addition number initial candidate lexicon entries choose
much larger fracturing lics method shown figure true even
though sampled number representations pairs lics
larger number fractures arbitrary representation number lics
arbitrary pair finally wolfies learning time fracturing greater
lics shown figure cpu time shown seconds
summary differences utility lics method generating
candidates thorough method better performance
longer learning times one could claim handicapping fracturing since
sampling representations fracturing may indeed help accuracy
learning time number candidates would likely suffer even domain
larger representations differences learning time would even dramatic
artificial data
previous section showed wolfie successfully learns lexicons natural corpus
realistic task however demonstrates success relatively small corpus
one representation formalism scales well
lexicon items learn ambiguity synonymy factors



fithompson mooney



number candidates







fractwolfie
wolfie












training examples





figure fracturing vs lics number candidates





learning time sec













fractwolfie
wolfie








training examples



figure fracturing vs lics learning time





fiacquiring word meaning mappings

difficult control real data input large corpora available
annotated semantic parses therefore present experimental
artificial corpus corpus sentences representations completely
artificial sentence representation variable free representation suggested
work jackendoff others
corpus discussed random lexicon mapping words simulated meanings
first constructed original lexicon used generate corpus random
utterances paired meaning representation corpus input
wolfie learned lexicon compared original lexicon weighted precision
weighted recall learned lexicon measured precision measures percentage
lexicon entries e word meaning pairs system learns correct
recall measures percentage lexicon entries hand built lexicon
correctly learned system
precision
recall

correct pairs
pairs learned

correct pairs

pairs hand built lexicon

get weighted precision recall measures weight pair
words frequency entire corpus training corpus
likely learned correct meaning arbitrarily chosen word
corpus
generated several lexicons associated corpora varying ambiguity rate number meanings per word synonymy rate number words per meaning siskind
meaning representations generated set conceptual symbols
combined form meaning word number conceptual symbols used
lexicon noted describe corpus lexicon
senses variable free simulate noun meanings contained
one three variables denote open argument positions simulate verb meanings
remainder words remaining empty meaning simulate function words addition functors meaning could depth two
arity two example noun meaning f f f verbmeaning f f b conceptual symbols example f f f f
f multi level meaning representations demonstrate learning
complex representations geography database domain none
hand built meanings phrases lexicon functors embedded arguments
used grammar generate utterances meanings original lexicon
terminal categories selected distribution zipfs law zipf
zipfs law occurrence frequency word inversely proportional ranking
occurrence
started baseline corpus generated lexicon words conceptual symbols ambiguity synonymy sentence meaning pairs generated
thanks jeff siskind initial corpus generation software enhanced tests
tests allowed wolfie learn phrases length two



fithompson mooney






accuracy






precision
recall














training examples









figure baseline artificial corpus
split five training sets sentences figure shows weighted
precision recall curves initial test demonstrates good scalability
slightly larger corpus lexicon u geography query domain
second corpus generated second lexicon words conceptual symbols increasing ambiguity meanings per word time pairs
generated corpus split five sets training examples weighted
precision examples drops previous level weighted
recall full learning curve shown figure quick comparison siskinds performance corpus confirmed system achieved comparable
performance showing current methods close best performance
able obtain difficult corpus one possible explanation smaller
performance difference two systems corpus versus geography domain domain correct meaning word necessarily
general terms number vertices candidate meanings therefore
generality portion heuristic may negatively influence performance wolfie
domain
finally change performance increasing ambiguity increasing
synonymy holding number words conceptual symbols constant figure shows
weighted precision recall training examples increasing levels ambiguity holding synonymy level constant figure shows increasing
levels synonymy holding ambiguity constant increasing level synonymy
effect much increasing level ambiguity expected
holding corpus size constant increasing number competing meanings
word increases number candidate meanings created wolfie decreasing
amount evidence available meaning e g first component heuristic


fiacquiring word meaning mappings





accuracy







precision
recall
















training examples









figure ambiguous artificial corpus






recall
precision

accuracy

















number meanings per word





figure increasing level ambiguity
measure makes learning task difficult hand increasing
level synonymy potential mislead learner
number training examples required reach certain level accuracy
informative table point standard precision first



fithompson mooney



accuracy





recall
precision









number words per meaning





figure increasing level synonymy
ambiguity level




number examples




table number examples reach precision
reached level ambiguity note however measured accuracy
set training examples numbers table approximate
performed second test scalability two corpora generated lexicons
order magnitude larger tests tests use lexicon
containing words conceptual symbols generated corpus
ambiguity one lexicon ambiguity synonymy similar found
wordnet database beckwith fellbaum gross miller ambiguity
approximately meanings per word synonymy words per meaning
corpora contained ambiguity examples respectively split
data five sets training examples easier large corpus maximum
average weighted precision recall training examples
harder corpus maximum average training examples

active learning
indicated previous sections built integrated system language
acquisition flexible useful however major difficulty remains construction
training corpora though annotating sentences still arguably less work building


fiacquiring word meaning mappings

apply learner n bootstrap examples creating classifier
examples remain annotator unwilling label examples
use recently learned classifier annotate unlabeled instance
k instances lowest annotation certainty
annotate instances
train learner bootstrap examples examples annotated far

figure selective sampling
entire system hand annotation task time consuming error prone
training pairs often contain redundant information would minimize
amount annotation required still maintaining good generalization accuracy
turned methods active learning active learning area
machine learning features systems automatically select informative
examples annotation training angluin seung opper sompolinsky
rather relying benevolent teacher random sampling primary goal
active learning reduce number examples system trained
maintaining accuracy acquired information active learning systems may construct examples request certain types examples determine set
unsupervised examples usefully labeled last selective sampling
cohn et al particularly attractive natural language learning since
abundance text would annotate informative sentences
many language learning tasks annotation particularly time consuming since requires
specifying complex output rather category label reducing number
training examples required greatly increase utility learning
section explore use active learning specifically selective sampling
lexicon acquisition demonstrate active learning fewer examples required
achieve accuracy obtained training randomly chosen examples
basic selective sampling relatively simple learning begins
small pool annotated examples large pool unannotated examples learner
attempts choose informative additional examples annotation existing work
area emphasized two approaches certainty methods lewis catlett
committee methods mccallum nigam freund seung shamir
tishby liere tadepalli dagan engelson cohn et al
focus former
certainty paradigm system trained small number annotated
examples learn initial classifier next system examines unannotated examples
attaches certainties predicted annotation examples k examples
lowest certainties presented user annotation retraining many
methods attaching certainties used typically attempt estimate
probability classifier consistent prior training data classify
example correctly



fithompson mooney

learn lexicon examples annotated far
phrase unannotated sentence
entries learned lexicon
certainty average heuristic values entries
else one word phrase
certainty zero
rank sentences use
total certainty phrases step
phrases counted step

figure active learning wolfie
figure presents abstract pseudocode certainty selective sampling
ideal situation batch size k would set one make intelligent decisions
future choices efficiency reasons retraining batch learning
frequently set higher number classification tasks demonstrated
general effective reducing need labeled examples see citations

applying certainty sample selection wolfie requires determining certainty
complete annotation potential training example despite fact individual
learned lexical entries parsing operators perform part overall annotation task
therefore general compute certainties pieces example
case phrases combine obtain overall certainty example since lexicon
entries contain explicit uncertainty parameters used wolfies heuristic measure
estimate uncertainty
choose sentences annotated round first bootstrapped initial
lexicon small corpus keeping track heuristic values learned items
unannotated sentence took average heuristic values
lexicon entries learned phrases sentence giving value zero unknown
words eliminating consideration words assume known advance
database constants thus longer sentences known phrases would
lower certainty shorter sentences number known phrases
desirable since longer sentences informative lexicon learning point
view sentences lowest values chosen annotation added
bootstrap corpus lexicon learned technique summarized figure
evaluate technique compared active learning learning randomly selected examples measuring effectiveness learned lexicons background knowledge chill used smaller u geography corpus original
wolfie tests lexicons background knowledge parser acquisition
examples parser acquisition
trial following experiments first randomly divide data
training test set n bootstrap examples randomly selected



fiacquiring word meaning mappings





accuracy




wolf active
wolfie
geobase









training examples





figure lexicon certainty active learning
training examples step active learning least certain k examples
remaining training examples selected added training set
learning set evaluated step accuracy resulting learned
parsers compared accuracy learned randomly chosen examples
learn lexicons parsers section words think k examples
round chosen randomly
figure shows accuracy unseen data parsers learned lexicons learned
wolfie examples chosen randomly actively annotation
savings around examples active learning maximum accuracy reached
examples versus random examples advantage active
learning clear beginning though differences two curves
statistically significant training examples since learning lexicons
parsers choosing examples wolfies certainty measures boost could
improved even chill say examples chosen see thompson califf
mooney description active learning chill

related work
section divide previous related topics areas lexicon
acquisition active learning
lexicon acquisition
work automated lexicon language acquisition dates back siklossy
demonstrated system learned transformation patterns logic back natural



fithompson mooney

language already noted closely related work jeff siskind
described briefly section whose system ran comparisons section
definition learning compared mapping siskind
formulation differs several respects first sentence representations terms instead trees however shown figure terms
represented trees conform formalism minor additions next
notion interpretation involve type tree carries entire representation
sentence root clear would handle quantified variables
representation sentences skolemization possible generalization across
sentences would require special handling make single use assumption
another difference bias towards minimal number lexicon entries
attempts monosemous lexicon later work siskind relaxes allow
ambiguity noise still biases towards minimizing ambiguity however formal
definition explicitly allow lexical ambiguity handles heuristic manner
though may lead robustness method face noise finally
definition allows phrasal lexicon entries
siskinds work topic explored many different variations along continuum
many constraints requiring time incorporate example siskind
versus constraints requiring training data siskind thus perhaps earlier systems would able learn lexicons section
quickly crucially systems allow lexical ambiguity thus may
learned accurate lexicon detailed comparisons versions
system outside scope goal wolfie learn possibly
ambiguous lexicon examples possible thus made comparisons along
dimension alone
siskinds takes account constraints word meanings
justified exclusivity compositionality assumptions
somewhat general handles noise referential uncertainty uncertainty
meaning sentence thus multiple possible candidates
specialized applications meaning meanings known experimental
section demonstrate advantage method application
demonstrated system capable learning reasonably accurate lexicons large
ambiguous noisy artificial corpora accuracy assured learning
converges occur smaller corpus experiments ran
already noted system operates incremental line fashion discarding
sentence processes batch addition search word meanings
proceeds two stages discussed section common substructures
combine two stages wolfie systems greedy aspects
choice next best lexical entry choice discard utterances noise create
homonymous lexical entry finally system compute statistical correlations
words possible meanings
besides siskinds work others cognitive
perspective example de marcken uses child language learning motivation approaches segmentation instead learning semantics
training input uses flat list tokens semantic representations


fiacquiring word meaning mappings

segment sentences words uses variant expectation maximization dempster
laird rubin together form parsing dictionary matching techniques
segment sentences associate segments likely meaning
childes corpus achieves high precision recall provided
others taking cognitive demonstrate language understanding ability
carry task parsing example nenov dyer describe
neural network model map visual verbal motor commands colunga
gasser use neural network modeling techniques learning spatial concepts
feldman colleagues berkeley feldman lakoff shastri actively
pursuing cognitive acquisition semantic concepts another berkeley effort
system regier given examples pictures paired natural language
descriptions apply picture learns judge whether sentence true
given picture
similar work suppes liang bottner uses robots demonstrate lexicon learning robot trained cognitive perceptual concepts associated
actions learns execute simple commands along similar lines tishby gorin
system learns associations words actions use
statistical framework learn associations handle structured representations similarly oates eyler walker cohen discuss acquisition lexical
hierarchies associated meaning defined sensory environment robot
automatic construction translation lexicons smadja mckeown
hatzivassiloglou melamed wu xia kumano hirakawa catizone russell warwick gale church brown et al definition
similar methods compute association scores
pairs case word word pairs use greedy choose best translation word take advantage constraints pairs one
exception melamed however allow phrases lexicon synonymy within one text segment yamazaki pazzani
merz learn translation rules semantic hierarchies parsed parallel
sentences japanese english course main difference body
work map words semantic structures words
mentioned introduction large body work learning lexical
semantics different formulations example collins
singer riloff jones roark charniak schneider
define semantic lexicons grouping words semantic categories latter
case add relational information typically applied semantic lexicon
information extraction entity tagging pedersen chen describe method
acquiring syntactic semantic features unknown word assuming access
initial concept hierarchy give experimental many systems fukumoto
tsujii haruno johnston boguraev pustejovsky webster marcus
focus acquisition verbs nouns rather types words
authors named experimentally evaluate systems
usefulness learned lexicons specific application
several authors rooth riezler prescher carroll beil collins ribas
manning resnik brent discuss acquisition subcategoriza

fithompson mooney

tion information verbs others describe work learning selectional restrictions
manning brent different information required
mapping semantic representation could useful source information
constrain search li expands subcategorization work
inducing clustering information finally several systems knight hastings
russell learn words context assuming large initial lexicon
parsing system already available
another related body work grammar acquisition especially areas tightly
integrate grammar lexicon categorial grammars retore bonato
dudau sofronie tellier tommasi watkinson manandhar
theory categorial grammar ties lexical semantics semantics
often used inference support high level tasks database
retrieval learning syntax semantics together arguably difficult task
aforementioned work evaluated large corpora presumably primarily
due difficulty annotation
active learning
respect additional active learning techniques cohn et al among
first discuss certainty active learning methods detail focus neural
network active learning version space concepts
researchers applying machine learning natural language processing utilized active
learning hwa schohn cohn tong koller thompson et al
argamon engelson dagan liere tadepalli lewis catlett
majority addressed classification tasks part speech tagging
text categorization example liere tadepalli apply active learning
committees text categorization improvements
active learning similar obtain use committee winnow
learners traditional classification task argamon engelson dagan
apply committee learning part speech tagging work committee
hidden markov used select examples annotation lewis catlett
use heterogeneous certainty methods simple classifier used select
examples annotated presented powerful classifier
however many language learning tasks require annotating natural language text
complex output parse tree semantic representation filled template
application active learning tasks requiring complex outputs well
studied exceptions hwa soderland thompson et al
latter two include work active learning applied information extraction thompson
et al includes work active learning semantic parsing hwa describes
interesting method evaluating statistical parsers uncertainty applied
syntactic parsing

future work
although wolfies current greedy search method performed quite well better search
heuristic alternative search strategy could improvements


fiacquiring word meaning mappings

thoroughly evaluate wolfies ability learn long phrases restricted ability
evaluations another issue robustness face noise current
guaranteed learn correct lexicon even noise free corpus addition noise
complicates analysis circumstances mistakes likely happen
theoretical empirical analysis issues warranted
referential uncertainty could handled increase complexity forming
lics pairs representations phrase appears
alternative representations sentence pair added lexicon
sentence containing word representations eliminated
contain learned meaning provided another representation contain thus allowing
lexical ambiguity plan flesh evaluate
different avenue exploration apply wolfie corpus sentences paired
common query language sql corpora easily constructible
recording queries submitted existing sql applications along english forms
translating existing lists sql queries english presumably easier direction
translate fact training data used learn semantic
lexicon parser helps limit overall burden constructing complete natural
language interface
respect active learning experiments additional corpora needed test
ability reduce annotation costs variety domains would
interesting explore active learning natural language processing
syntactic parsing word sense disambiguation machine translation
current involved certainty however proponents
committee approaches convincing arguments theoretical advantages
initial attempts adapting committee approaches systems
successful however additional topic indicated one critical
obtaining diverse committees properly sample version space cohn et al

conclusions
acquiring semantic lexicon corpus sentences labeled representations
meaning important widely studied present
formalism learning greedy approximate solution
wolfie demonstrates fairly simple greedy symbolic learning performs
well task obtains performance superior previous lexicon acquisition system
corpus geography queries demonstrate methods extend
variety natural languages besides english scale fairly well larger
difficult corpora
active learning area machine learning almost exclusively
applied classification tasks demonstrated successful application
complex natural language mappings phrases semantic meanings supporting
acquisition lexicons parsers wealth unannotated natural language data
along difficulty annotating data make selective sampling potentially
invaluable technique natural language learning realistic corpora indicate
example annotations savings high achieved employing active



fithompson mooney

sample selection simple certainty measures predictions unannotated data
improved sample selection methods applications important language
hold promise continued progress machine learning construct effective
natural language processing systems
experiments corpus natural language presented
subtask natural language whether learned subsystems
successfully integrated build complete nlp system experiments presented
demonstrated two learning systems wolfie chill successfully
integrated learn complete nlp system parsing database queries executable
logical form given single corpus annotated queries demonstrated
potential active learning reduce annotation effort learning nlp

acknowledgments
would thank jeff siskind providing us software help
adapting use corpus thanks agapito sustaita esra erdem
marshall mayberry translation efforts three anonymous reviewers
comments helped improve supported
national science foundation grants iri iri

references
anderson j r induction augmented transition networks cognitive science

angluin queries concept learning machine learning
argamon engelson dagan committee sample selection probabilistic classifiers journal artificial intelligence
beckwith r fellbaum c gross miller g wordnet lexical database
organized psycholinguistic principles zernik u ed lexical acquisition
exploiting line resources build lexicon pp lawrence erlbaum
hillsdale nj
borland international turbo prolog reference guide borland international
scotts valley ca
brent automatic acquisition subcategorization frames untagged text
proceedings th annual meeting association computational linguistics acl pp
brown p et al statistical machine translation computational
linguistics
catizone r russell g warwick deriving translation data bilingual
texts proceedings first international lexical acquisition workshop


fiacquiring word meaning mappings

cohn atlas l ladner r improving generalization active learning
machine learning
collins singer unsupervised named entity classification
proceedings conference empirical methods natural language processing
large corpora emnlp vlc university maryland
collins j three generative lexicalised statistical parsing proceedings th annual meeting association computational linguistics
acl pp
colunga e gasser linguistic relativity word acquisition computational proceedings twenty first annual conference
cognitive science society pp
dagan engelson p committee sampling training probabilistic classifiers proceedings twelfth international conference machine
learning icml pp san francisco ca morgan kaufman
de marcken c acquisition lexicon paired phoneme sequences
semantic representations lecture notes computer science vol pp
springer verlag
dempster laird n rubin maximum likelihood incomplete data
via em journal royal statistical society b
dudau sofronie tellier tommasi learning categorial grammars semantic
types proceedings th amsterdam colloquium pp
feldman j lakoff g shastri l neural theory language project
http www icsi berkeley edu ntl international computer science institute university
california berkeley ca
fillmore c case case bach e harms r eds universals
linguistic theory holt reinhart winston york
fillmore c mechanisms construction grammar axmaker jaisser
singmeister h eds proceedings fourteenth annual meeting
berkeley linguistics society pp berkeley ca
fisher h knowledge acquisition via incremental conceptual clustering machine
learning
freund seung h shamir e tishby n selective sampling
query committee machine learning
fukumoto f tsujii j representation acquisition verbal polysemy
papers aaai symposium representation acquisition
lexical knowledge polysemy ambiguity generativity pp stanford ca



fithompson mooney

gale w church k identifying word correspondences parallel texts
proceedings fourth darpa speech natural language workshop
garey johnson computers intractability guide theory
np completeness freeman york ny
goldberg constructions construction grammar argument
structure university chicago press
grefenstette g sextant extracting semantics raw text implementation
details integrated computer aided engineering
haas j jayaraman b context free definite clause grammars typetheoretic journal logic programming
haruno case frame learning method japanese polysemous verbs
papers aaai symposium representation acquisition
lexical knowledge polysemy ambiguity generativity pp stanford ca
hastings p implications automatic lexical acquisition mechanism
wermter riloff e scheler c eds connectionist statistical symbolic approaches learning natural language processing springer verlag berlin
hwa r minimizing training corpus parser acquisition proceedings
fifth computational natural language learning workshop
jackendoff r semantic structures mit press cambridge
johnston boguraev b pustejovsky j acquisition interpretation
complex nominals papers aaai symposium representation
acquisition lexical knowledge polysemy ambiguity generativity pp
stanford ca
knight k learning word meanings instruction proceedings thirteenth
national conference artificial intelligence aaai pp portland
kohavi r john g automatic parameter selection minimizing estimated
error proceedings twelfth international conference machine learning
icml pp tahoe city ca
kumano hirakawa h building mt dictionary parallel texts
linguistic statistical information proceedings fifteenth international
conference computational linguistics pp
lavrac n dzeroski inductive logic programming techniques applications ellis horwood
lewis catlett j heterogeneous uncertainty sampling supervised
learning proceedings eleventh international conference machine learning icml pp san francisco ca morgan kaufman


fiacquiring word meaning mappings

li h probabilistic lexical semantic knowledge acquisition structural disambiguation ph thesis university tokyo
liere r tadepalli p active learning committees text categorization
proceedings fourteenth national conference artificial intelligence aaai pp providence ri
manning c automatic acquisition large subcategorization dictionary
corpora proceedings st annual meeting association computational linguistics acl pp columbus oh
mccallum k nigam k employing em pool active learning
text classification proceedings fifteenth international conference
machine learning icml pp madison wi morgan kaufman
melamed automatic evaluation uniform filter cascades inducing n best
translation lexicons proceedings third workshop large corpora
melamed translational equivalence among words computational
linguistics
muggleton ed inductive logic programming academic press york
ny
muggleton inverse entailment progol generation computing journal

muggleton feng c efficient induction logic programs proceedings
first conference algorithmic learning theory tokyo japan ohmsha
nenov v dyer g perceptually grounded language learning part
dete neural procedural model connection science
oates eyler walker z cohen p syntax learn semantics
experiment language acquisition mobile robot tech rep university
massachusetts computer science department
partee b meulen wall r mathematical methods linguistics kluwer
academic publishers dordrecht netherlands
pedersen chen w lexical acquisition via constraint solving papers
aaai symposium representation acquisition lexical
knowledge polysemy ambiguity generativity pp stanford ca
plotkin g note inductive generalization meltzer b michie
eds machine intelligence vol elsevier north holland york
rayner hugosson hagert g logic grammar learn lexicon
tech rep r swedish institute computer science



fithompson mooney

regier human semantic potential spatial language constrained connectionism mit press
resnik p selection information class lexical relationships
ph thesis university pennsylvania cis department
retore c bonato r learning rigid lambek grammars minimalist grammars
structured sentences proceedings third learning language logic
workshop strasbourg france
ribas f experiment learning appropriate selectional restrictions
parsed corpus proceedings fifteenth international conference computational linguistics pp
riloff e jones r learning dictionaries information extraction multilevel bootstrapping proceedings sixteenth national conference artificial
intelligence aaai pp orlando fl
roark b charniak e noun phrase co occurrence statistics semi automatic
semantic lexicon construction proceedings th annual meeting
association computational linguistics coling acl coling pp

rooth riezler prescher carroll g beil f inducing semantically
annotated lexicon via em clustering proceedings th annual meeting
association computational linguistics pp
russell language acquisition unification grammar processing system real world knowledge base ph thesis university illinois urbana
il
schank r c conceptual information processing north holland oxford
schneider r lexically intensive domain specific knowledge acquisition proceedings joint conference methods language processing
computational natural language learning pp
schohn g cohn less active learning support vector machines proceedings seventeenth international conference machine learning icml pp stanford ca
sebillot p bouillon p fabre c inductive logic programming corpus
acquisition semantic lexicons proceedings nd learning language logic
lll workshop lisbon portugal
seung h opper sompolinsky h query committee proceedings
acm workshop computational learning theory pittsburgh pa
siklossy l natural language learning computer simon h siklossy
l eds representation meaning experiments information processsing
systems prentice hall englewood cliffs nj


fiacquiring word meaning mappings

siskind j learning word meaning mappings broeder p murre j
eds language acquisition inductive deductive approaches oxford
university press
siskind j naive physics event perception lexical semantics language
acquisition ph thesis department electrical engineering computer science massachusetts institute technology cambridge
siskind j computational study cross situational techniques learning
word meaning mappings cognition
siskind j lexical acquisition constraint satisfaction tech rep ircs
university pennsylvania
smadja f mckeown k r hatzivassiloglou v translating collocations
bilingual lexicons statistical computational linguistics
soderland learning information extraction rules semi structured free
text machine learning
suppes p liang l bottner complexity issues robotic machine learning
natural language lam l naroditsky v eds modeling complex phenomena proceedings rd woodward conference pp springer verlag
thompson c califf e mooney r j active learning natural language
parsing information extraction proceedings sixteenth international
conference machine learning icml pp bled slovenia
thompson c acquisition lexicon semantic representations sentences
proceedings rd annual meeting association computational linguistics acl pp cambridge
tishby n gorin algebraic learning statistical associations language
acquisition computer speech language
tomita efficient parsing natural language kluwer academic publishers
boston
tong koller support vector machine active learning applications
text classification proceedings seventeenth international conference
machine learning icml pp stanford ca
watkinson manandhar unsupervised lexical learning categorial
grammars lll corpus learning language logic lll workshop bled
slovenia
webster marcus automatic acquisition lexical semantics verbs
sentence frames proceedings th annual meeting association
computational linguistics acl pp



fithompson mooney

wu xia x large scale automatic extraction english chinese translation lexicon machine translation
yamazaki pazzani merz c learning hierarchies ambiguous natural
language data proceedings twelfth international conference machine
learning icml pp san francisco ca morgan kaufmann
zelle j inductive logic programming automate construction
natural language parsers ph thesis department computer sciences university texas austin tx appears artificial intelligence laboratory technical
report ai
zelle j mooney r j learning parse database queries inductive
logic programming proceedings thirteenth national conference artificial
intelligence aaai pp portland
zipf g human behavior principle least effort addison wesley
york ny




