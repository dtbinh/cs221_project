Journal Artificial Intelligence Research 18 (2003) 491-516

Submitted 11/02; published 6/03

Acquiring Correct Knowledge
Natural Language Generation
Ehud Reiter
Somayajulu G. Sripada

ereiter@csd.abdn.ac.uk
ssripada@csd.abdn.ac.uk

Department Computing Science,
University Aberdeen, Aberdeen AB24 3UE, UK

Roma Robertson

roma.robertson@ed.ac.uk

Division Community Health Sciences - General Practice Section
University Edinburgh
Edinburgh EH8 9DX, UK

Abstract
Natural language generation (nlg) systems computer software systems produce texts English human languages, often non-linguistic input data.
nlg systems, ai systems, need substantial amounts knowledge. However,
experience two nlg projects suggests difficult acquire correct knowledge
nlg systems; indeed, every knowledge acquisition (ka) technique tried significant problems. general terms, problems due complexity, novelty,
poorly understood nature tasks systems attempted, worsened
fact people write differently. meant particular corpus-based ka
approaches suffered impossible assemble sizable corpus high-quality
consistent manually written texts domains; structured expert-oriented ka techniques suffered experts disagreed could get enough information
special unusual cases build robust systems. believe problems
likely affect many nlg systems well. long term, hope new
ka techniques may emerge help nlg system builders. shorter term, believe
understanding individual ka techniques fail, using mixture different
ka techniques different strengths weaknesses, help developers acquire nlg
knowledge mostly correct.

1. Introduction
Natural language generation (nlg) systems use artificial intelligence (ai) natural language processing techniques automatically generate texts English human
languages, typically non-linguistic input data (Reiter & Dale, 2000).
ai systems, essential part building nlg system knowledge acquisition (ka),
acquiring relevant knowledge domain, users, language used
texts, forth.
ka nlg based structured expert-oriented techniques, think-aloud
protocols sorting, machine learning corpus analysis, currently
popular areas Natural Language Processing. used types techniques two nlg projects included significant ka efforts stop (Reiter, Robertson, &
Osman, 2003), generated tailored smoking cessation letters, SumTime-Mousam
c
2003
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiReiter, Sripada, & Robertson

(Sripada, Reiter, Hunter, Yu, & Davy, 2001), generated weather forecasts.
projects, techniques tried, main problem turned knowledge quality;
evaluation validation exercises identified flaws knowledge acquired using every
technique. flaws due variety factors, perhaps basic underlying
reason nature writing tasks attempting automate.
were:
complex (as many tasks involve interacting humans): hence lot
knowledge needed cover numerous special cases unusual circumstances;
sometimes novel (not done humans): hence sometimes experts
task whole, existing corpora texts analyse;
poorly understood: hence good theoretical models structure
knowledge acquired, fill gaps knowledge acquired experts
corpora;
ambiguous (allowed multiple solutions): hence different experts corpus authors
produced different texts (solutions) input data.
problems course occur degree ka expert system natural
language processing tasks, believe may especially severe nlg.
good solution problems, indeed believe ka one
biggest problems applied nlg. all, point using ai techniques
build text-generation system cannot acquire knowledge needed ai
techniques.
longer term, basic research ka nlg badly needed. shorter
term, however, believe developers likely acquire correct knowledge
building nlg system understand likely types errors knowledge
acquired different ka techniques. Also, degree different ka techniques
tried complementary strengths weaknesses; suggests using variety
different techniques, weaknesses one technique compensated
strengths techniques.
remainder paper give background information nlg, ka,
systems; describe various ka techniques used build systems problems
encountered; discuss generally ka nlg difficult
different ka techniques combined.

2. Background
section give background information natural language generation
knowledge acquistion validation. introduce briefly describe stop
SumTime-Mousam systems.

492

fiAcquiring Correct Knowledge NLG

2.1 Natural Language Generation
Natural Language Generation subfield artificial intelligence concerned
automatically generating written texts human languages, often non-linguistic input
data. nlg systems often three stages (Reiter & Dale, 2000):
Document Planning decides content structure generated text;
example smoking-cessation letter start section discusses
pros cons smoking.
Microplanning decides information structure expressed linguistically; example, phrase mid afternoon used weather
report refer time 1500.
Surface Realisation generates actual text according decisions made previous stages, ensuring text conforms grammar target language
(English systems).
nlg systems require many types knowledge order carry tasks. particular, Kittredge, Korelsky, Rambow (1991) point nlg systems need domain
knowledge (similar needed expert systems), communication knowledge (similar
needed Natural Language Processing systems), domain communication knowledge (DCK). DCK knowledge information domain usually
communicated, including standard document structures, sublanguage grammars, specialised lexicons. DCK plays role aspects language technology (for example,
speech recogniser work better given domain trained corpus texts
domain), may especially important nlg.
2.2 Knowledge Acquisition Validation
Knowledge acquisition subfield artificial intelligence concerned acquiring knowledge needed build ai systems. Broadly speaking two common
types ka techniques are:
Techniques based working experts structured fashion, structured interviews, think-aloud protocols, sorting, laddered grids (Scott, Clayton,
& Gibson, 1991; Buchanan & Wilkins, 1993);
Techniques based learning data sets correct solutions (such text corpora);
currently popular natural language processing used many
different types knowledge, ranging grammar rules discourse models (for
overview, see Jurafsky & Martin, 2000).
course possible ka techniques well, including directly asking experts
knowledge, conducting scientific experiments. research done
evaluating comparing ka techniques, research difficult interpret
methodological problems (Shadbolt, OHara, & Crow, 1999).
Research done verifying validating knowledge check
correct (Adelman & Riedel, 1997). Verification techniques focus detecting logical
493

fiReiter, Sripada, & Robertson

anomalies inconsistencies often reflect mistakes elicitation coding process;
discuss these, errors primary concern paper.
Validation techniques focus detecting whether knowledge acquired indeed correct
enable construction good system; relevant efforts
detect problems knowledge acquired nlg. Adelman Riedel (1997) describe two
general types validation techniques: (1) experts check acquired knowledge
built systems, (2) using library test cases known inputs outputs.
words, knowledge acquired experts data sets correct solutions,
knowledge validated experts data sets correct solutions. Knowledge
validated experimentally, determining system whole works
intended effect users. course care must taken validation process
uses different resources acquisition process. example, knowledge acquired
expert validated expert, knowledge learned data set
validated data set.
great deal previous research knowledge acquisition nlg;
Reiter, Robertson, Osman (2000) summarise previous efforts area. Generally
corpus analysis (analysis collections manually written texts) popular
ka technique nlg, areas Natural Language Processing, although sometimes
supplemented expert-oriented techniques (Goldberg, Driedger, & Kittredge, 1994;
McKeown, Kukich, & Shaw, 1994). Walker, Rambow, Rogati (2002) attempted
learn nlg rules user ratings generated texts, perhaps considered
type experiment-based ka.
2.3 STOP
stop (Reiter, Robertson, & Osman, 2003) nlg system generates tailored smokingcessation letters. Tailoring based 4-page multiple-choice questionnaire
smokers habits, health, concerns, forth. extract questionnaire shown
Figure 1, extract stop letter generated questionnaire shown
Figure 2 (we changed name smoker preserve confidentiality).
ka perspective, important knowledge needed stop content phrasing
appropriate individual smoker; example,
information given letter? example letter Figure 2,
instance, emphasises things smoker dislikes smoking, confidence building,
dealing stress weight gain; recommend specific techniques
stopping smoking.
letter adopt positive youll feel better stop tone (as done
letter Figure 2), adopt negative smoking killing tone?
stop never operationally deployed, tested real smokers clinical
trial, 857 smokers received stop letters (Lennox, Osman, Reiter, Robertson,
Friend, McCann, Skatun, & Donnan, 2001). evaluation, incidentally, showed stop
letters effective control non-tailored letters.

494

fiAcquiring Correct Knowledge NLG

SMOKING QUESTIONNAIRE

Please answer marking appropriate box question this: _

Q1 smoked cigarette last week, even puff?
YES _
Please complete following questions

Please read questions carefully.
Q2

Home situation:
Live
_
alone





Please return questionnaire unanswered
envelope provided. Thank you.
sure answer, give best answer can.

Live

husband/wife/partner



Live
adults

0 boys

0. girls

Q3

Number children 16 living home

Q4

anyone else household smoke? (If so, please mark boxes apply)
husband/wife/partner
family member
others

Q5



Live
children

long smoked for? 20 years
Tick smoked less year


Q6

many cigarettes smoke day? (Please mark amount below)

Less 5
Q7

5 10

11 15 _

16 20

21 - 30

31

soon wake smoke first cigarette? (Please mark time below)

Within 5 minutes

6 - 30 minutes _

31 - 60 minutes

Q8

find difficult smoke places
forbidden eg church, library, cinema?

Q9

cigarette would hate give up?

Q10

smoke frequently first hours
waking rest day?

Q11 smoke ill bed
day?
Q12
intending stop
smoking next 6
months?

YES





_

60 minutes
YES

_NO

first one morning _
others
YES



_

YES



_

Q13 yes, intending stop smoking
within next month?
YES
Q14 no, would stop smoking
easy?
YES Sure _





1

Figure 1: First page example smoker questionnaire

495

fiSmoking Information Heather Stewart
know make likely able stop.
people stop smoking good one attempt.

People stop smoking really want stop. encouraging
many good reasons stopping. scales show good
bad things smoking you. tipped favour.

Overcoming barriers stopping...

THINGS

it's relaxing
stops stress
enjoy
relieves boredom
stops weight gain
stops craving

THINGS DISLIKE
makes less fit
it's bad example kids
you're addicted
it's unpleasant others
people disapprove
it's smelly habit
it's bad
it's expensive
it's bad others' health

could it...
people really want stop eventually succeed. fact, 10
million people Britain stopped smoking - stayed stopped -
last 15 years. Many found much easier expected.
Although don't feel confident would able stop
try, several things favour.




stopped month.
good reasons stopping smoking.
expect support family, friends,
workmates.

said questionnaire might find difficult stop
smoking helps cope stress. Many people think
cigarettes help cope stress. However, taking cigarette
makes feel better short while. ex-smokers feel calmer
control smoking.
ideas coping stress back page leaflet.
said might find difficult stop would put
weight. people put weight. stop smoking,
appetite would improve would taste food much better.
would wise plan advance you're
reaching biscuit tin time. Remember putting weight
overeating problem, no-smoking one. tackle later
diet exercise.

finally...
hope letter help feel confident giving
cigarettes. go, real chance succeeding.
best wishes,
Health Centre.

Reiter, Sripada, & Robertson

496

Figure 2: Extract letter generated Figure 1 questionnaire

good reasons stop...

fiAcquiring Correct Knowledge NLG

day

hour

12-06-02
12-06-02
12-06-02
12-06-02
12-06-02
12-06-02
13-06-02

6
9
12
15
18
21
0

wind
direction
WSW
WSW
WSW
WSW
SW
SSW
SSW

wind speed
(10m altitude)
10
9
7
7
7
8
10

wind speed
(50m alt)
12
11
9
9
9
10
12

Figure 3: Wind data extract 12-Jun-2002 numerical weather prediction
Knowledge acquisition stop primarily based structured expert-oriented ka
techniques, including particular sorting think-aloud protocols. Knowledge acquired five health professionals; three doctors, nurse, health psychologist.
experts knowledgeable smoking patient information,
experts writing tailored smoking-cessation letters. fact experts
task, since one manually writes tailored smoking-cessation letters.
unusual nlg system attempt task currently performed
human experts; examples include descriptions software models (Lavoie, Rambow,
& Reiter, 1997), customised descriptions museum items (Oberlander, ODonnell, Knott,
& Mellish, 1998), written feedback adult literacy students (Williams, Reiter, &
Osman, 2003). Knowledge validation stop mostly based feedback users
(smokers), results clinical trial.
2.4 SumTime-Mousam
SumTime-Mousam (Sripada, Reiter, Hunter, & Yu, 2002) nlg system generates
marine weather forecasts offshore oil rigs, numerical weather simulation data.
extract SumTime-Mousams input data shown Figure 3, extract
forecast generated data shown Figure 4. ka perspective, main
knowledge needed SumTime-Mousam content expression best
users; example,
changes meteorological parameter significant enough reported
text? forecast Figure 4, example, mentions changes wind direction
changes wind speed.
words phrases used communicate time? example,
1800 described early evening (as Figure 4) late afternoon?
SumTime-Mousam currently used operationally meteorological company,
generate draft forecasts post-edited human forecasters.
Knowledge acquisition SumTime-Mousam based corpus analysis
manually-written forecasts structured ka expert meteorologists. Unlike experts worked stop, meteorologists worked SumTime-Mousam

497

fiReiter, Sripada, & Robertson

FORECAST 6 - 24 GMT, Wed 12-Jun 2002
WIND(KTS)
10M: WSW 8-13 gradually backing SW early evening SSW
midnight.
50M: WSW 10-15 gradually backing SW early evening SSW
midnight.
WAVES(M)
SIG HT: 0.5-1.0 mainly SW swell.
MAX HT: 1.0-1.5 mainly SW swell.
PER(SEC)
WAVE PERIOD: Wind wave 3-5 mainly 6 second SW swell.
WINDWAVE PERIOD: 3-5.
SWELL PERIOD: 5-7.
WEATHER:
Partly cloudy becoming overcast light rain around midnight.
VIS(NM):
Greater 10 reduced 5-8 precipitation.
AIR TEMP(C): 8-10 rising 9-11 around midnight.
CLOUD(OKTAS/FT): 2-4 CU/SC 1300-1800 lowering 7-8 ST/SC 700-900 around
midnight.
Figure 4: Extract forecast generated 12-Jun-2002
experienced writing target texts (weather forecasts). forecast corpus included numerical weather simulation data forecasters used writing
forecasts, well actual forecast texts (Sripada, Reiter, Hunter, & Yu, 2003).
Knowledge validation SumTime-Mousam mostly conducted checking
knowledge acquired corpus experts, checking knowledge acquired
experts corpus. words, tried make validation
technique different possible acquisition technique. currently evaluating
SumTime-Mousam system measuring number edits forecasters make
computer-generated draft forecasts.

3. Knowledge Acquisition Techniques Tried
section summarise main ka techniques used stop SumTimeMousam. technique give example knowledge acquired, discuss
learned tried validate knowledge. Table 1 gives high level
overview major advantages disadvantages different techniques tried,
different techniques perhaps useful, types knowledge
best suited acquiring (using classification Section 2.1). table shows,
one technique clearly best; different strengths weaknesses. Probably
best overall ka strategy use mix different techniques; discuss
Section 5.

498

fiAcquiring Correct Knowledge NLG

Techniques

Advantages

Disadvantages

directly ask
experts
structured ka
experts
corpus
analysis

get big picture

many gaps, may
match practice
limited coverage,
experts variable
hard create,
texts inconsistent,
poor models nlg
local optimisation,
major changes

expert
revision

get details,
get rationale
get lots
knowledge
quickly
fix problems
knowledge


Useful
initial
prototype
flesh
prototype
robustness,
unusual cases

Types
Knowledge
domain,
DCK
depends
expert
DCK,
communication

improve
system



Table 1: Summary Evaluation ka techniques nlg
3.1 Directly Asking Experts Knowledge
simplest perhaps obvious ka technique nlg simply ask experts
write texts question. stop SumTime-Mousam, experts initially
gave us spreadsheets flowcharts describing thought texts generated.
projects, turned experts description texts
generated fact match people actually wrote texts question.
common finding ka, partially due fact difficult experts
introspectively examine knowledge use practice (Anderson, 1995);
proponents expert-oriented ka prefer structured ka techniques.
example, beginning SumTime-Mousam, one meteorologists gave
us spreadsheet designed, essentially encoded thought
parts weather forecasts generated (the spreadsheet generate complete
weather forecast). analysed logic used spreadsheet, largely based first
version SumTime-Mousam logic.
One goal analysis create algorithm could decide change
parameter value significant enough mentioned weather
report. spreadsheet used context-dependent change thresholds make decision.
example, change wind speed would mentioned
change 10 knots more, final wind speed 15 knots less;
change 5 knots more, final wind speed 15 40
knots;
change 10 knots more, final wind speed 40 knots.
context-dependent thresholds reflect usage weather reports users (in
case, oil company staff making decisions related North Sea offshore oil rigs).
example, user deciding unload supply boat, moderate changes wind
speed dont matter low speeds (because light winds minimal impact supply
boat operations) high speeds (because boat wont even attempt unload
heavy winds), may affect decisions in-between speeds. context-dependent
499

fiReiter, Sripada, & Robertson

thresholds would expected vary according specific forecast recipient,
set consultation recipient.
perspective, two main pieces knowledge encoded algorithm:
1. absolute size change determines whether mentioned not,
2. threshold significance depends context ultimately user
use information.
3.1.1 Validation Direct Expert Knowledge
checked rules comparing observed corpus analysis
manually written forecasts (Section 3.3). suggested (2) probably
correct, (1) may incorrect. particular, linear segmentation model (Sripada et al.,
2002), basically looks changes slope rather changes absolute value
parameter, better matches corpus texts. expert designed spreadsheet model
agreed segmentation probably better approach. essentially commented
one reason use absolute size model something
easily comprehensible someone neither programmer expert numerical
data analysis techniques.
words, addition problems introspecting knowledge, perhaps
reasonable expect domain expert able write sophisticated data analysis
algorithm based expertise. issue knowledge needed purely
declarative, many ai applications; need procedural algorithmic
knowledge, must bear mind domain experts may sufficient computational
expertise express knowledge computer algorithm.
3.1.2 Role Directly Asking Experts Knowledge
Although experts spreadsheet SumTime-Mousam far ideal, extremely useful starting point. specified initial system could build fairly
easily, produced least vaguely plausible output. Much fact happened stop, one doctors gave us flowchart certainly many
weaknesses, useful initial specification relatively easy-to-build
somewhat plausible system. stop SumTime-Mousam, indeed nlg
projects involved in, initial prototype system working soon
possible useful developing ideas explaining domain experts
interested parties trying do.
terms types knowledge mentioned Section 2.1, stop flowchart
SumTime-Mousam spreadsheet specified domain knowledge (for example,
smokers categorised) domain communication knowledge (for example, use
ranges instead single numbers communicate wind speed). stop flowchart
specify generic communication knowledge English grammar morphology;
author probably believed knew things did. SumTimeMousam spreadsheet effect include English grammar rules,
get spreadsheet work, author much confidence them.
500

fiAcquiring Correct Knowledge NLG

summary, think directly asking experts knowledge excellent way
quickly build initial system, especially nlg developers supply communication
knowledge domain expert may possess. initial system place,
probably best use ka techniques, least poorly understood areas
nlg. However, applications solid theoretical basis, expert
simply say build system according theory X, experts direct knowledge may
perhaps needed.
3.2 Structured Expert-Oriented KA: Think-Aloud Protocols
numerous types structured expert-oriented ka techniques, including thinkaloud protocols, sorting, structured interviews (Scott et al., 1991). focus
think-aloud protocols, technique used most. tried
structured ka techniques well, sorting (Reiter et al., 2000);
describe here, broad conclusions structured ka techniques
similar conclusions think-aloud protocols.
think-aloud protocol, expert carries task question (in case, writing
text) thinking aloud audio (or video) recorder. used think-aloud
protocols stop SumTime-Mousam. especially important stop,
provided basis content phrasing rules.
simple example think-aloud process follows. One doctors wrote
letter smoker tried stop before, managed stop several weeks
starting again. doctor made following comments think-aloud transcript:
tried stop smoking before? Yes, longest managed
stop ticked one week right three months thats
encouraging managed stop least before,
always said people one two goes likely
succeed future.
included following paragraph letter wrote smoker:
see managed stop smoking one two occasions
gone back smoking, glad know common
people finally stop smoking one two attempts
past finally succeed. show capable
stopping even short period, means much likely
able stop permanently somebody never ever stopped smoking
all.
analysing session, proposed two rules:
(previous attempt stop) (message: likely succeed)
(previous attempt stop) (message: people stop
unsuccessful attempts first)

501

fiReiter, Sripada, & Robertson

final system incorporated rule (based several ka sessions,
one) stated smoker tried stop before, letter included
section confidence building, confidence-building section include short
message previous attempts stop. smoker managed quit
one week, mentioned message; otherwise message mention
recency smokers previous cessation attempt within past 6 months.
actual text generated rule example letter Figure 2
Although dont feel confident would able stop
try, several things favour.
stopped month.
Note text produced actual stop code considerably simpler
text originally written expert. fairly common, simplifications
logic used decide whether include message letter not. many cases
due expert much knowledge expertise computer system
(Reiter & Dale, 2000, pp 3036). general, process deriving implementable rules
nlg systems think-aloud protocols perhaps art science,
least different experts often write texts different ways.
3.2.1 Validation Structured KA Knowledge
attempted verify rules acquired stop think-aloud sessions performing series small experiments asked smokers comment letter,
compare two versions letter. Many rules supported experiments example, people general liked recap smoking likes dislikes (see
good reasons stop. . . section Figure 2). However, one general negative finding
experiments tailoring rules insufficiently sensitive unusual
atypical aspects individual smokers; smokers probably unusual atypical
way. example, stop letters go medical details smoking (as
none think-aloud expert-written letters contained information),
seemed right choice many smokers, smokers say would
liked see medical information smoking. Another example (again based
think-aloud sessions) adopted positive tone try scare smokers;
seemed right smokers, smokers said brutal
approach would effective them.
fact experts tailor letters ways may possibly reflect
fact tailoring would appropriate relatively small number
specific cases considered think-aloud sessions. 30 think-aloud sessions
experts, looked 24 different smoker questionnaires (6 questionnaires considered two experts). may sound lot, drop ocean
consider tremendously variable people are.
Comments made smokers stop clinical trial (Reiter, Robertson, & Osman, 2003) revealed problems think-aloud derived rules. example,
decided include practical how-to-stop information letters people currently intending stop smoking; smoker comments suggest mistake.
502

fiAcquiring Correct Knowledge NLG

fact, experts include information think-aloud letters people,
not. decision include information influenced Stages
Change theoretical model (Prochaska & diClemente, 1992) behaviour change,
states how-to-stop advice inappropriate people currently intending stop;
retrospect, decision probably mistake.
repeated two think-aloud exercises 15 months originally performed
them; is, went back one experts gave two questionnaires
analysed 15 months earlier, asked think aloud writing letters
based questionnaires. letters expert wrote second session
somewhat different ones originally written, preferred smokers
letters originally written (Reiter et al., 2000). suggests experts
static knowledge sources, learning task writing
tailored smoking-cessation letters course project. Perhaps
surprise given none experts ever attempted write letters
getting involved project.
3.2.2 Role Structured Expert-Oriented KA
Structured expert-oriented ka certainly useful way expand, refine, generally
improve initial prototypes constructed basis experts direct knowledge. focusing
actual cases structuring ka process, learned many things
experts mention directly. obtained types knowledge mentioned
Section 2.1, working experts relevant expertise. example stop
acquired domain knowledge (such medical effects smoking) doctors, domain
communication knowledge (such words use) psychologist expertise
writing patient information leaflets, communication knowledge graphic design
layout graphic designer.
However, structured expert-oriented ka problems, including particular coverage variability. mentioned above, 30 sessions examined 24 smoker
questionnaires could possibly give good coverage population smokers, given
complex variable people are. variation, fact different experts wrote
texts different ways made difficult extract rules think-aloud protocols.
undoubtably made mistakes regard, giving how-to-stop information people currently intending stop smoking. Perhaps focused
single expert order reduce variation. However, experiences suggested
different experts better different types information, experts changed
time (so might see substantial variation even texts single author);
observations raise doubts wisdom usefulness single-expert strategy.
short, complexity nlg tasks means large number structured ka
sessions may needed get good coverage; fact numerous ways
write texts fulfill communicative goal means different experts tend write
differently, makes analysis structured ka sessions difficult.

503

fiReiter, Sripada, & Robertson

3.3 Corpus Analysis
recent years great interest Natural Language Processing areas
ai using machine learning techniques acquire knowledge relevant data sets.
example, instead building medical diagnosis system trying understand expert
doctors diagnose diseases, instead analyse data sets observed symptoms actual
diseases, use statistical machine learning techniques determine symptoms
predict disease. Similarly, instead building English grammar working
expert linguists, instead analyse large collections grammatical English texts
order learn allowable structures (grammar) texts. collections texts
called corpora Natural Language Processing.
growing interest applying techniques learn knowledge
needed nlg. example, Barzilay McKeown (2001) used corpus-based machine
learning learn paraphrase possibilities; Duboue McKeown (2001) used corpus-based
machine learning learn NP constituents ordered; Hardt Rambow
(2001) used corpus-based machine learning learn rules VP ellipsis.
nlg researchers, McKeown et al. (1994), used term corpus
analysis refer manual analysis (without using machine learning techniques)
small set texts written explicitly nlg project domain experts (and
hence naturally occurring). certainly valid valuable ka technique,
regard form structured expert-oriented ka, ways similar thinkaloud protocols. paper, corpus analysis refers use machine learning
statistical techniques analyse collections naturally occurring texts.
Corpus analysis sense word possible stop
collection naturally occurring texts (since doctors currently write personalised
smoking-cessation letters). briefly considered analysing example letters produced
think-aloud sessions machine learning techniques, 30
texts, believed would successful learning, especially given
high variability experts. words, perhaps primary strength corpus
analysis ability extract information large data sets; large
data sets extract information from, corpus analysis loses much value.
SumTime-Mousam, able acquire analyse substantial corpus 1099
human-written weather forecasts, along data files forecasters looked
writing forecasts (Sripada et al., 2003). Details corpus analysis procedures
results presented elsewhere (Reiter & Sripada, 2002a; Sripada et al., 2003),
repeated here.
3.3.1 Validation Corpus Analysis Knowledge
many rules acquired corpus analysis valid, rules
problematical, primarily due two factors: individual variations writers,
writers making choices appropriate humans nlg systems.
simple example individual variation problems causes follows. One
first things attempted learn corpus express numbers wind
statements. initially searching common textual realisation
number. resulted rules said 5 expressed 5, 6
504

fiAcquiring Correct Knowledge NLG

form
5
05
6
06

F1
0
0
0
0

F2
7
0
44
0

F3
0
1
0
364

F4
0
46
0
154

F5
122
0
89
0

unknown
4
2
2
13

total
133
49
135
531

Table 2: Usage 5, 05, 6, 06 wind statements, forecaster
expressed 06. probably acceptable forecast always include leading
zeros single digits (that is, use 05 06), never include leading zeros (that is,
use 5 6). However, probably acceptable mix two (that is, use 5 06
forecast), rules would led to.
usage 5, 05, 6, 06 individual forecaster shown Table 2.
table suggests, individual forecaster consistent; forecasters F3 F4 always
include leading zeros, forecasters F2 F5 never include leading zeros. F1 fact
consistent always omits leading zeros; example uses 8 instead 08.
reason overall statistics favour 5 05 06 6 individuals differ
descriptions wind speed prefer use. example, F1 never explicitly
mentions low wind speeds 5 6 knots, instead always uses generic phrases
10 LESS; F2, F4, F5 use mix generic phrases explicit numbers low
wind speeds; F3 always uses explicit numbers never uses generic phrases.
forecasters (especially F3) strong preference even numbers. means
statistics 5 vs. 05 dominated F5 (the forecaster explicitly
mentions low wind speeds prefer even numbers); statistics 6 vs.
06 dominated F3 (who uses number lot avoids generic phrases
odd numbers). Hence somewhat odd result corpus overall favours 5
05 06 6.
example means unique. Reiter Sripada (2002b) explain
complex analysis using corpus, whose goal determine common time
phrase time, similarly led unacceptable rules, largely individual
differences forecasters.
obvious methods deal problems caused individual variation.
example, could restrict corpus texts one author; although
major drawback significantly reducing size corpus. could use
sophisticated model, learning one rule single digit numbers expressed,
separate rules number. could analyse behaviour individuals
identify choices (such presence leading zero) vary individuals
consistently made given individual; make choices parameters
user nlg system specify. last option probably best nlg
systems (Reiter, Sripada, & Williams, 2003), one used SumTime-Mousam
leading-zero choice.
main point simply would trouble accepted
initial corpus-derived rules (use 5 06) without question. corpus researchers
course aware, result corpus analysis depends learned (for
example, rule realise 5, rule realise single-digit numbers)
505

fiReiter, Sripada, & Robertson

features used learning (for example, number, number
author). complex analyses, analysis time-phrase choice rules
(Reiter & Sripada, 2002b), result depends algorithms used learning
alignment. dependence corpus analysis choices means results
particular analysis guaranteed correct need validated (checked)
results ka techniques. Also, often best approach
nlg perspective, namely identifying individual variations letting user choose
variation prefers, requires analysing differences individual writers.
best knowledge published nl corpus analyses done this, perhaps
part many popular corpora include author information.
recurring problem corpus-derived rules cases writers
produced sub-optimal texts particular shorter been,
probably texts quicker write. instance, noticed
parameter changed less steady fashion throughout forecast period,
forecasters often omitted time phrase. example, wind rose steadily speed
10 20 course forecast period covering calendar day, forecasters
might write 8-12 RISING 18-22, instead 8-12 RISING 18-22 MIDNIGHT.
statistical corpus analysis showed null time phrase common one
contexts, used 33% cases. next common time phrase, later,
used 14% cases. Accordingly, programmed system omit time phrase
circumstances. However, asked experts comment revise
generated forecasts (Section 3.4), told us behaviour incorrect,
forecasts useful end users included explicit time phrases rely
readers remembering forecast periods ended. words, example
forecasters wrong thing, course meant rule produced
corpus analysis incorrect.
dont know forecasters this, discussions forecast managers
mistakes (such forecast authors describing wind speed direction
changing time, even actually predicted change different
times) suggested one possible cause desire write forecasts quickly. particular,
numerical weather predictions constantly updated, customers want
forecasts based up-to-date prediction; limit amount time
available write forecasts.
fact perfectly rational human writers cut corners time
limitations. forecasters believe, example, quickly writing forecast
last minute let use up-to-date prediction data; benefits
up-to-date data outweighs costs abbreviated texts, making right
decision write shorter-than-optimal texts. nlg system, however, faces
different set tradeoffs (for example, omitting time phrase unlikely speed
nlg system), means blindly imitate choices made human
writers.
problem perhaps fundamental one individual variation problem,
solved appropriate choices learned,
features considered, forth. Corpus analysis, however performed, learns
choice rules used human authors. rules inappropriate nlg system,
506

fiAcquiring Correct Knowledge NLG

rules learned corpus analysis inappropriate ones well, regardless
corpus analysis carried out.
general terms, corpus analysis certainly many strengths, looking
people practice, collecting large data sets statistically
analysed. pure corpus analysis perhaps suffer drawback gives
information experts made choices made, means blindly imitating
corpus lead inappropriate behaviour human writers face different set
constraints tradeoffs nlg system.
3.3.2 Role Corpus Analysis
Corpus analysis machine learning wonderful ways acquire knowledge
1. large data set (corpus) covers unusual boundary cases well
normal cases;
2. members data set (corpus) correct would
software system produce;
3. members data set (corpus) consistent (modulo noise), example
given input generally leads output.
conditions probably satisfied learning rules medical diagnosis speech
recognition. However, satisfied projects. None conditions
satisfied stop, first satisfied SumTime-Mousam.
course, may ways alleviate problems. example, could
try acquire general communication knowledge domain dependent (such
English grammar) general corpora British National Corpus; could
argue certain aspects manually written texts (such lexical usage) unlikely
adversely affected time pressure hence probably correct; could analyse
behaviour individual authors order enhance consistency (in words, treat
author input feature par actual numerical semantic input data).
scope valuable research here, hope considered people interested
corpus-based techniques nlg.
primarily used corpus analysis SumTime-Mousam acquire domain communication knowledge, linguistically express numbers times weather
forecasts, elide information, sublanguage constraints grammar
weather forecasts. Corpus analysis course used acquire generic communication knowledge English grammar, mentioned probably best
done large general corpus British National Corpus. use corpus
analysis acquire domain knowledge meteorology. Meteorological researchers fact
use machine learning techniques learn meteorology, analyse numeric
data sets actual predicted weather, analyse textual corpora.
summary, machine learning corpus-based techniques extremely valuable
conditions satisfied, particular offer cost-effective solution
problem acquiring large amount knowledge needed complex nlg applications
(Section 3.2.2). Acquiring large amounts knowledge using expert-oriented ka techniques
507

fiReiter, Sripada, & Robertson

expensive time-consuming requires many sessions experts; contrast,
large corpus consistent correct texts created, large amounts
knowledge extracted low marginal cost. learning techniques,
corpus analysis vulnerable Garbage In, Garbage principle; corpus
small, incorrect, and/or inconsistent, results corpus analysis may
correct.
3.4 Expert Revision
stop SumTime-Mousam, made heavy use expert revision. is,
showed generated texts experts asked suggest changes would improve
them. sense, expert revision could considered type structured expertoriented ka, seems somewhat different strengths weaknesses
techniques mentioned Section 3.2, treat separately.
example expert revision, early version stop system used phrase
lots good reasons stopping. One experts commented revision
session phrasing changed emphasise reasons listed (in
particular section stop letter) ones smoker selected
questionnaire filled out. eventually led revised wording encouraging
many good reasons stopping, first paragraph example
letter Figure 2. example expert revision SumTime-Mousam mentioned
Section 3.3; showed experts generated texts omitted end-of-period time
phrases, told us incorrect, include time phrases.
stop, tried revision sessions recipients (smokers). less successful
hoped. Part problem smokers knew little stop (unlike
experts, familiar project), often made comments
useful improving system, stop 10 days til daughter threw
wobbly wanted cigarette bought some. Also, comments came
well-educated articulate smokers, university students. harder get
feedback less well-educated smokers, single mothers living council (public
housing) estates. Hence unsure revision comments obtained generally
applicable not.
3.4.1 Validation Expert Revision Knowledge
validate expert revision knowledge techniques. Indeed,
initially regarded expert revision validation technique, ka technique, although
retrospect probably makes sense think ka technique.
qualitative level, though, expert revision certainly resulted lot useful
knowledge ideas changing texts, particular proved useful way
improving handling unusual boundary cases. example, changed way
described uneventful days SumTime-Mousam (when weather changed little
day) based revision sessions.
comment made stop revision best suggesting specific localised changes generated text, less useful suggesting larger changes system.
One stop experts suggested, system built, might
508

fiAcquiring Correct Knowledge NLG

able suggest larger changes explained systems reasoning him, instead
giving letter revise. words, asked experts think-aloud
wrote letters, order understand reasoning, could useful revision
sessions experts understood computer system thinking well
actually produced. Davis Lenat (1982, page 260) similarly pointed
explanations help experts debug improve knowledge-based systems.
3.4.2 Role Expert Revision
certainly found expert revision extremely useful technique improving
nlg systems; furthermore useful improving types knowledge (domain,
domain communication, communication). time revision seem
largely local optimisation technique. nlg system already generating reasonable
texts, revision good way adjusting systems knowledge rules improve
quality generated text. local optimisation techniques, expert revision
may tend push systems towards local optimum, may less well suited finding
radically different solutions give better result.

4. Discussion: Problems Revisited
section 1 explained writing tasks difficult automate
complex, often novel, poorly understood, allow multiple solutions. section
discuss problems detail, based experiences stop
SumTime-Mousam.
4.1 Complexity
nlg systems communicate humans, need knowledge people, language, people communicate; since complex, means
general nlg systems need lot complex knowledge. one reasons knowledge acquisition nlg difficult. recall distinction Section 2.1
domain knowledge, domain communication knowledge, communication knowledge,
may communication knowledge (such grammar) generic hence acquired (perhaps corpus-based techniques) used many applications.
domain knowledge similar needed ai systems, problems acquiring
unique nlg. domain communication knowledge, optimal tone
smoking letter tone achieved, information weather
forecast elided, application dependent (and hence cannot acquired generically)
knowledge language communication (and hence complex). Hence
ka nlg may always require acquiring complex knowledge.
experience, best way acquire complex knowledge robustly get information large number individual cases handled. done corpus
analysis suitable corpus created. sometimes done expert revision, experts time look large number generated texts; regard
may useful tell comment major problems ignore minor
difficulties. however knowledge acquired, require substantial effort.

509

fiReiter, Sripada, & Robertson

4.2 Novelty
course, many ai systems need complex knowledge, comments hardly
unique nlg. one aspect nlg perhaps unusual many
tasks nlg systems expected perform novel tasks currently done
humans. ai expert systems attempt replicate performance human experts
areas medical diagnosis credit approval. Similarly, language technology
systems attempt replicate performance human language users tasks
speech recognition information retrieval. many nlg applications stop,
attempt task human performs. Even SumTime-Mousam, argument
could made task humans actually perform writing weather forecasts
time constraints, fact different task performed SumTime-Mousam.
Novelty fundamental problem, means knowledge acquired
expert-oriented ka may reliable (since experts fact experts
actual nlg task), corpus manually-written texts probably exist.
means none ka techniques described likely work. Indeed,
acquiring novel knowledge almost definition scientific research, perhaps
way acquire knowledge conduct scientific research domain. course,
knowledge need acquired way, even novel application
likely much knowledge needed (such grammar morphology) novel.
hand, novelty perhaps opportunity nlg. One drawbacks conventional expert systems performance often limited
human experts, case users may prefer consult actual experts instead computer
systems. experts task, nlg system may used even output
far ideal.
4.3 Poorly Understood Tasks
perhaps related problem good theoretical models many
choices nlg systems need make. example, ultimate goal stop
change peoples behaviour, number colleagues suggested base stop
argumentation theory, Grasso, Cawsey, Jones (2000) dietary advice
system. However, argumentation theory focuses persuading people change beliefs
desires, whereas goal stop encourage people act beliefs
desires already had. words, stops main goal encourage people
already wanted stop smoking make serious cessation attempt, convince people
desire quit change mind desirability
smoking. applicable theory could find Stages Change (Prochaska &
diClemente, 1992), indeed partially based stop theory. However, results
evaluation suggested choices rules based Stages
Change incorrect, mentioned Section 3.2.1.
Similarly, one problems SumTime-Mousam generating texts
interpreted correctly despite fact different readers different idiolects
particular probably interpret words different ways (Reiter & Sripada, 2002a; Roy, 2002).
Theoretical guidance would useful, able
find guidance.
510

fiAcquiring Correct Knowledge NLG

lack good theoretical models means nlg developers cannot use models
fill cracks knowledge acquired experts data sets,
done ai systems better understood areas scheduling configuring machinery.
turn means lot knowledge must acquired. applications
good theoretical basis, goal ka perhaps acquire limited amount high-level
information search strategies, taxonomies, best way represent knowledge, etc;
determined, details filled theoretical models.
applications details cannot filled theory need acquired, much
knowledge needed. Acquiring knowledge structured expert-oriented ka
could extremely expensive time consuming. Corpus-based techniques cheaper
large corpus available; however, lack good theoretical understanding perhaps
contributes problem know behaviour observe corpus
intended help reader (and hence copied nlg system)
behaviour intended help writer (and hence perhaps copied).
4.4 Expert Variation
Perhaps part lack good theories, stop SumTime-Mousam
observed considerable variation experts. words, different experts wrote
quite different texts input data. stop discovered experts
changed wrote time (Section 3.2.1).
Variability caused problems structured expert-oriented ka (because different
experts told us different things) corpus analysis (because variation among corpus
authors made harder extract consistent set rules good coverage). However,
variation seems less problem revision. suspect
experts vary less confident particular decision; revision
experts tended focus things confident about, case
ka techniques.
sense variability may especially dangerous corpus analysis,
information corpus degree confidence authors individual decisions, developers may even realise variability
authors, especially corpus include author information. contrast, structured expert-oriented techniques think-aloud sometimes give information
experts confidence, variations experts usually obvious.
experimented various techniques resolving differences experts/authors,
group discussions focusing decisions made one particular expert. None
really satisfactory. Given experiences revision, perhaps best way
reduce variation develop ka techniques clearly distinguish decisions experts confident decisions less confidence in.

5. Development Methodology: Using Multiple KA Techniques
methodological perspective, fact different ka techniques different
strengths weaknesses suggests makes sense use mixture several different
ka techniques. example, structured expert-oriented ka corpus analysis
used, explanatory information expert-oriented ka used
511

fiReiter, Sripada, & Robertson

help identify decisions intended help reader intended
help writer, thus helping overcome problem corpus analysis; broader
coverage corpus analysis show unusual boundary cases handled,
thus overcoming problem expert-oriented ka.
may make sense use different techniques different points development
process. example, directly asking experts knowledge could stressed initial
stages project, used build simple initial prototype; structured ka
experts corpus analysis could stressed middle phases project,
prototype fleshed converted something resembling real system;
revision could used later stages project, system refined
improved.
strategy, graphically shown Figure 5, basically one followed
stop SumTime-Mousam. Note suggests knowledge acquisition
something happens throughout development process. words, first
acquire knowledge build system; knowledge acquisition ongoing process
closely coupled general software development effort. course,
hardly novel observation, many development methodologies knowledgebased systems stress iterative development continual ka (Adelman & Riedel,
1997).
short term, believe using development methodology combines
different ka techniques manner, validating knowledge much possible,
best strategies acquiring nlg knowledge. believe whenever possible
knowledge acquired one way validated another way. words,
recommend validating corpus-acquired knowledge using corpus techniques (even
validation done held-out test set); validating expert-acquired knowledge
using expert-based validation (even validation done using different expert).
preferable (although always possible) validate corpus-acquired knowledge
experts, validate expert-acquired knowledge corpus.
Another issue related development methodology relationship knowledge acquisition system evaluation. Although usually considered separate activities, fact closely related. example, currently running
evaluation SumTime-Mousam based number edits forecasters
manually make computer-generated forecasts publishing them; similar
edit-cost evaluations machine translation systems (Jurafsky & Martin, 2000, page 823).
However, edits excellent source data improving system via expert
revision. take one recent example, forecaster edited computer-generated text SSE
23-28 GRADUALLY BACKING SE 20-25 dropping last speed range, giving SSE
23-28 GRADUALLY BACKING SE. considered evaluation data (2 token
edits needed make text acceptable), ka data (we need adjust rules eliding
similar identical speed ranges).
words, real-world feedback effectiveness quality generated texts
often used either improve evaluate nlg system. data
used depends goals project. scientific projects whose goal test
hypotheses, may appropriate point stop improving system use new
effectiveness data purely evaluation hypothesis testing; sense analogous
512

fiAcquiring Correct Knowledge NLG

Directly Ask Experts
Knowledge
Initial prototype

Structured KA
Experts

Corpus Analysis

Initial version full system

Expert Revision

Final System
Figure 5: Methodology
holding back part corpus testing purposes. applied projects whose goal
build maximally useful system, however, may appropriate use
effectiveness data improve quality generated texts.

6. Conclusion
Acquiring correct knowledge nlg difficult, knowledge needed
largely knowledge people, language, communication, knowledge complex poorly understood. Furthermore, perhaps writing art
science, different people write differently, complicates knowledge acquisition process; many nlg systems attempt novel tasks currently done manually,
makes hard find knowledgeable experts good quality corpora. Perhaps
problems, every single ka technique tried stop SumTimeMousam major problems limitations.
easy solution problems. short term, believe useful
use mixture different ka techniques (since techniques different strengths
weaknesses), validate knowledge whenever possible, preferably using different tech-

513

fiReiter, Sripada, & Robertson

nique one used acquire knowledge. helps developers understand
weaknesses different techniques, fact structured expert-oriented ka
may give good coverage complexities people language, fact
corpus-based ka distinguish behaviour intended help reader
behaviour intended help writer.
longer term, need research better ka techniques nlg. cannot
reliably acquire knowledge needed ai approaches text generation,
point using approaches, regardless clever algorithms models are.
first step towards developing better ka techniques acknowledge current ka
techniques working well, understand case; hope
paper constitutes useful step direction.

Acknowledgements
Numerous people given us valuable comments past five years struggled ka nlg, many acknowledge here. would thank Sandra
Williams reading several drafts paper considering light experiences, thank anonymous reviewers helpful comments. would
thank experts worked stop SumTime-Mousam, without
work would possible. work supported UK Engineering
Physical Sciences Research Council (EPSRC), grants GR/L48812 GR/M76881,
Scottish Office Department Health grant K/OPR/2/2/D318.

References
Adelman, L., & Riedel, S. (1997). Handbook Evaluating Knowledge-Based Systems.
Kluwer.
Anderson, J. (1995). Cognitive Psychology Implications (Fourth edition). Freeman.
Barzilay, R., & McKeown, K. (2001). Extracting paraphrases parallel corporus.
Proceedings 39th Meeting Association Computation Linguistics
(ACL-01), pp. 5057.
Buchanan, B., & Wilkins, D. (Eds.). (1993). Readings Knowledge Acquisition Learning. Morgan Kaufmann.
Davis, R., & Lenat, D. (1982). Knowledge-Based Systems Artificial Intelligence. McGraw
Hill.
Duboue, P., & McKeown, K. (2001). Empirically estimating order constraints content
planning generation. Proceedings 39th Meeting Association
Computation Linguistics (ACL-01), pp. 172179.
Goldberg, E., Driedger, N., & Kittredge, R. (1994). Using natural-language processing
produce weather forecasts. IEEE Expert, 9 (2), 4553.
Grasso, F., Cawsey, A., & Jones, R. (2000). Dialectical argumentation solve conflicts
advice giving: case study promotion healthy nutrition. International
Journal Human Computer Studies, 53, 10771115.
514

fiAcquiring Correct Knowledge NLG

Hardt, D., & Rambow, O. (2001). Generation VP-ellipsis: corpus-based approach.
Proceedings 39th Meeting Association Computation Linguistics
(ACL-01), pp. 282289.
Jurafsky, D., & Martin, J. (2000). Speech Language Processing. Prentice-Hall.
Kittredge, R., Korelsky, T., & Rambow, O. (1991). need domain communication
language. Computational Intelligence, 7 (4), 305314.
Lavoie, B., Rambow, O., & Reiter, E. (1997). Customizable descriptions object-oriented
models. Proceedings Fifth Conference Applied Natural-Language Processing (ANLP-1997), pp. 253256.
Lennox, S., Osman, L., Reiter, E., Robertson, R., Friend, J., McCann, I., Skatun, D., & Donnan, P. (2001). cost-effectiveness computer-tailored non-tailored smoking
cessation letters general practice: randomised controlled study. British Medical
Journal, 322, 13961400.
McKeown, K., Kukich, K., & Shaw, J. (1994). Practical issues automatic document
generation. Proceedings Fourth Conference Applied Natural-Language
Processing (ANLP-1994), pp. 714.
Oberlander, J., ODonnell, M., Knott, A., & Mellish, C. (1998). Conversation museum: experiments dynamic hypermedia intelligent labelling explorer. New
Review Hypermedia Multimedia, 4, 1132.
Prochaska, J., & diClemente, C. (1992). Stages Change Modification Problem
Behaviors. Sage.
Reiter, E., & Dale, R. (2000). Building Natural Language Generation Systems. Cambridge
University Press.
Reiter, E., Robertson, R., & Osman, L. (2000). Knowledge acquisition natural language
generation. Proceedings First International Conference Natural Language
Generation, pp. 217215.
Reiter, E., Robertson, R., & Osman, L. (2003). Lessons failure: Generating tailored
smoking cessation letters. Artificial Intelligence, 144, 4158.
Reiter, E., & Sripada, S. (2002a). Human variation lexical choice. Computational
Linguistics, 28, 545553.
Reiter, E., & Sripada, S. (2002b). corpora texts gold standards NLG?.
Proceedings Second International Conference Natural Language Generation,
pp. 97104.
Reiter, E., Sripada, S., & Williams, S. (2003). Acquiring using limited user models
NLG. Proceedings 2003 European Workshop Natural Language Generation, pp. 8794.
Roy, D. (2002). Learning visually grounded words syntax scene description task.
Computer Speech Language, 16, 353385.
Scott, A. C., Clayton, J., & Gibson, E. (1991). Practical Guide Knowledge Acquisition.
Addison-Wesley.
515

fiReiter, Sripada, & Robertson

Shadbolt, N., OHara, K., & Crow, L. (1999). experimental evaluation knowledge acquisition techniques methods: History, problems new directions. International
Journal Human Computer Studies, 51, 729755.
Sripada, S., Reiter, E., Hunter, J., & Yu, J. (2002). Segmenting time series weather
forecasting. Applications Innovations Intelligent Systems X, pp. 105118.
Springer-Verlag.
Sripada, S., Reiter, E., Hunter, J., & Yu, J. (2003). Summarising neonatal time-series data.
Proceedings Research Note Sessions EACL-2003, pp. 167170.
Sripada, S., Reiter, E., Hunter, J., Yu, J., & Davy, I. (2001). Modelling task summarising time series data using KA techniques. Applications Innovations
Intelligent Systems IX, pp. 183196. Springer-Verlag.
Walker, M., Rambow, O., & Rogati, M. (2002). Training sentence planner spoken
dialogue using boosting. Computer Speech Language, 16, 409433.
Williams, S., Reiter, E., & Osman, L. (2003). Experiments discourse-level choices
readability. Proceedings 2003 European Workshop Natural Language
Generation, pp. 127134.

516


