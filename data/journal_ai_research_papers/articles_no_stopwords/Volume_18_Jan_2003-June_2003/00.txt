Journal Artificial Intelligence Research 18 (2003) 1-44

Submitted 5/02; published 1/03

Acquiring Word-Meaning Mappings
Natural Language Interfaces
Cynthia A. Thompson

cindi@cs.utah.edu

School Computing, University Utah
Salt Lake City, UT 84112-3320

Raymond J. Mooney

mooney@cs.utexas.edu

Department Computer Sciences, University Texas
Austin, TX 78712-1188

Abstract
paper focuses system, Wolfie (WOrd Learning Interpreted Examples), acquires semantic lexicon corpus sentences paired semantic
representations. lexicon learned consists phrases paired meaning representations. Wolfie part integrated system learns transform sentences
representations logical database queries.
Experimental results presented demonstrating Wolfies ability learn useful
lexicons database interface four different natural languages. usefulness
lexicons learned Wolfie compared acquired similar system,
results favorable Wolfie. second set experiments demonstrates Wolfies ability
scale larger difficult, albeit artificially generated, corpora.
natural language acquisition, difficult gather annotated data needed
supervised learning; however, unannotated data fairly plentiful. Active learning
methods attempt select annotation training informative examples,
therefore potentially useful natural language applications. However,
results date active learning considered standard classification tasks.
reduce annotation effort maintaining accuracy, apply active learning semantic
lexicons. show active learning significantly reduce number annotated
examples required achieve given level performance.

1. Introduction Overview
long-standing goal field artificial intelligence enable computer understanding human languages. Much progress made reaching goal, much
remains done. artificial intelligence systems meet goal, first need
ability parse sentences, transform representation easily
manipulated computers. Several knowledge sources required parsing,
grammar, lexicon, parsing mechanism.
Natural language processing (NLP) researchers traditionally attempted build
knowledge sources hand, often resulting brittle, inefficient systems take
significant effort build. goal overcome knowledge acquisition
bottleneck applying methods machine learning. develop apply methods
empirical corpus-based NLP learn semantic lexicons, active learning
reduce annotation effort required learn them.

c
2003
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiThompson & Mooney

semantic lexicon one NLP component typically challenging time consuming construct update hand. notion semantic lexicon, formally defined
Section 3, list phrase-meaning pairs, meaning representation
determined language understanding task hand, taking compositional view sentence meaning (Partee, Meulen, & Wall, 1990). paper describes
system, Wolfie (WOrd Learning Interpreted Examples), acquires semantic
lexicon phrase-meaning pairs corpus sentences paired semantic representations. goal automate lexicon construction integrated NLP system
acquires semantic lexicons parsers natural language interfaces single
training set annotated sentences.
Although many others (Sebillot, Bouillon, & Fabre, 2000; Riloff & Jones, 1999; Siskind,
1996; Hastings, 1996; Grefenstette, 1994; Brent, 1991) presented systems learning
information lexical semantics, present system learning lexicons phrasemeaning pairs. Further, work unique combination several features, though
prior work included aspects. First, output used system,
Chill (Zelle & Mooney, 1996; Zelle, 1995), learns parse sentences semantic
representations. Second, uses fairly straightforward batch, greedy, heuristic learning
algorithm requires small number examples generalize well. Third,
easily extendible new representation formalisms. Fourth, requires prior knowledge
although exploit initial lexicon provided. Finally, simplifies learning
problem making several assumptions training data, described
Section 3.2.
test Wolfies ability acquire semantic lexicon natural language interface
geographical database using corpus queries collected human subjects
annotated logical form. test, Wolfie integrated Chill,
learns parsers requires semantic lexicon (previously built manually). results
demonstrate final acquired parser performs nearly accurately answering novel
questions using learned lexicon using hand-built lexicon. Wolfie
compared alternative lexicon acquisition system developed Siskind (1996),
demonstrating superior performance task. Finally, corpus translated
Spanish, Japanese, Turkish, experiments conducted demonstrating ability
learn successful lexicons parsers variety languages.
second set experiments demonstrates Wolfies ability scale larger
difficult, albeit artificially generated, corpora. Overall, results demonstrate robust
ability acquire accurate lexicons directly usable semantic parsing.
integrated system, task building semantic parser new domain simplified.
single representative corpus sentence-representation pairs allows acquisition
semantic lexicon parser generalizes well novel sentences.
building annotated corpus arguably less work building entire NLP
system, still simple task. Redundancies errors may occur training data.
goal minimize annotation effort, yet still achieve reasonable level
generalization performance. case natural language, frequently large
amount unannotated text available. would automatically, intelligently,
choose available sentences annotate.

2

fiAcquiring Word-Meaning Mappings

using technique called active learning. Active learning research
area machine learning features systems automatically select informative examples annotation training (Cohn, Atlas, & Ladner, 1994). primary goal
active learning reduce number examples system trained on, thereby
reducing example annotation cost, maintaining accuracy acquired information. demonstrate usefulness active learning techniques, compared
accuracy parsers lexicons learned using examples chosen active learning
lexicon acquisition, learned using randomly chosen examples, finding active
learning saved significant annotation cost training randomly chosen examples.
savings demonstrated geography query domain.
summary, paper provides new statement lexicon acquisition problem
demonstrates machine learning technique solving problem. Next, combining previous research, show entire natural language interface
acquired one training corpus. Further, demonstrate application active
learning techniques minimize number sentences annotate training input
integrated learning system.
remainder paper organized follows. Section 2 gives background
information Chill introduces Siskinds lexicon acquisition system,
compare Wolfie Section 5. Sections 3 4 formally define learning problem
describe Wolfie algorithm detail. Section 5 present discuss experiments
evaluating Wolfies performance learning lexicons database query domain
artificial corpus. Next, Section 6 describes evaluates use active learning
techniques Wolfie. Sections 7 8 discuss related research future directions,
respectively. Finally, Section 9 summarizes research results.

2. Background
section give overview Chill, system research adds to.
describe Jeff Siskinds lexicon acquisition system.
2.1 Chill
output produced Wolfie used assist larger language acquisition system;
particular, currently used part input parser acquisition system called
Chill (Constructive Heuristics Induction Language Learning). Chill uses inductive
logic programming (Muggleton, 1992; Lavrac & Dzeroski, 1994) learn deterministic
shift-reduce parser (Tomita, 1986) written Prolog. input Chill corpus
sentences paired semantic representations, input required Wolfie.
parser learned capable mapping sentences correct representations, well
generalizing well novel sentences. paper, limit discussion Chills
ability acquire parsers map natural language questions directly Prolog queries
executed produce answer (Zelle & Mooney, 1996). Following two
sample queries database U.S. geography, paired corresponding Prolog
query:

3

fiThompson & Mooney

<Sentence, Representation>
Training
Examples

WOLFIE

CHILL

Lexicon
<Phrase, Meaning>

Final
Parser
Prolog

Figure 1: Integrated System
capital state biggest population?
answer(C, (capital(S,C), largest(P, (state(S), population(S,P))))).
state Texarkana located in?
answer(S, (state(S), eq(C,cityid(texarkana, )), loc(C,S))).
Chill treats parser induction problem learning rules control actions
shift-reduce parser. parsing, current context maintained stack
buffer containing remaining input. parsing complete, stack contains
representation input sentence. three types operators parser uses
construct logical queries. One introduction onto stack predicate needed
sentence representation due phrases appearance front input buffer.
operators require semantic lexicon background knowledge. details
two parsing operators, see Zelle Mooney (1996). using Wolfie,
lexicon provided automatically. Figure 1 illustrates complete system.
2.2 Jeff Siskinds Lexicon Learning Research
closely related previous research automated lexicon acquisition
Siskind (1996), inspired work Rayner, Hugosson, Hagert (1988).
comparing system Section 5, describe main features
research section. goal one cognitive modeling childrens acquisition
lexicon, lexicon used comprehension generation. goal
machine learning engineering one, focuses lexicon comprehension
use parsing, using learning process claim cognitive plausibility,
goal learning lexicon generalizes well small number training
examples.
system takes incremental approach acquiring lexicon. Learning proceeds
two stages. first stage learns symbols representation used
4

fiAcquiring Word-Meaning Mappings

(capital, capital(_,_)),
(biggest, largest(_,_)),
(highest point, high_point(_,_)),
(through, traverse(_,_)),
(has, loc(_,_))

(state, state(_)),
(in, loc(_,_)),
(long, len(_,_)),
(capital, capital(_)),

Figure 2: Sample Semantic Lexicon
final conceptual expression represents meaning word, using versionspace approach. second stage learns symbols put together form
final representation. example, learning meaning word raise,
algorithm may learn set {CAUSE, GO, UP} first stage put together
form expression CAUSE(x, GO(y, UP)) second stage.
Siskind (1996) shows effectiveness approach series artificial corpora.
system handles noise, lexical ambiguity, referential uncertainty, large corpora, usefulness lexicons learned compared correct, artificial
lexicon. goal experiments presented evaluate correctness
completeness learned lexicons. Earlier work (Siskind, 1992) evaluated versions
technique quite small corpus real English Japanese sentences. extend
evaluation demonstration systems usefulness performing real world natural
language processing tasks, using larger corpus real sentences.

3. Lexicon Acquisition Problem
Although end goal acquire entire natural language interface, currently
divide task two parts, lexicon acquisition component parser acquisition
component. section, discuss problem acquiring semantic lexicons
assist parsing acquisition parsers. training input consists natural language
sentences paired meaning representations. pairs extract lexicon
consisting phrases paired meaning representations. training pairs
given previous section, sample lexicon shown Figure 2.
3.1 Formal Definition
present learning problem formally, definitions needed.
following use terms string substring, extend straight-forwardly
natural language sentences phrases, respectively. refer labeled trees, making
assumption semantic meanings interest represented such.
common representations recast labeled trees forests, formalism extends
easily latter.
Definition: Let V , E finite alphabets vertex labels edge labels, respectively.
Let V finite nonempty set vertices, l total function l : V V , E set unordered
pairs distinct vertices called edges, total function : E E . G = (V, l, E, a)
labeled graph.

5

fiThompson & Mooney

String 1: girl ate pasta cheese.
1 vertex edge labels:

Tree 1
1
2
4

ingest
patient
agent
person food
age type accomp
sex

3
5

6

7

female

child pasta

food

type
cheese

8

Interpretation f 1 1 t1 :
f 1 (girl) = 2
f 1 (ate") = 1
f 1 (pasta") = 3
f 1 (the cheese") = 7

Figure 3: Labeled Trees Interpretations
Definition: labeled tree connected, acyclic labeled graph.
Figure 3 shows labeled tree t1 (with vertices 1-8) left, associated vertex
edge labels right. function l is:1
{

(1, ingest), (2, person), (3, food), (4, female), (5, child), (6, pasta),
(7, food), (8, cheese) }.

tree t1 semantic representation sentence s1 : girl ate pasta
cheese. Using conceptual dependency (Schank, 1975) representation Prolog list form,
meaning is:
[ingest,

agent:[person, sex:female, age:child],
patient:[food, type:pasta, accomp:[food, type:cheese]]].

Definition: u-v path graph G finite alternating sequence vertices edges
G, vertex repeated, begins vertex u ends vertex v,
edge sequence connects vertex precedes sequence
vertex follows sequence.
Definition: directed, labeled tree = (V, l, E, a) labeled tree whose edges consist
ordered pairs vertices, distinguished vertex r, called root, property
every v V , directed r-v path , underlying
undirected unlabeled graph induced (V, E) connected, acyclic graph.
Definition: interpretation f finite string directed, labeled tree
one-to-one function mapping subset s0 substrings s, two strings
s0 overlap, vertices root range f .
1. omit enumeration function e could given similar manner, example ((1,2),
agent) element e.

6

fiAcquiring Word-Meaning Mappings

girl":

person
sex
age
female

pasta": food

type

child

pasta

cheese": food
type
cheese

ate": ingest

Figure 4: Meanings
interpretation provides information parts meaning sentence
originate phrases. Figure 3, show interpretation, f1 , s1 t1 .
Note domain f1 , since s0 subset substrings s, thus
allowing words meaning. disallow overlapping substrings
domain, cheese cheese could map vertices t1 .
Definition: Given interpretation f string tree t, element p domain
f , meaning p relative s, t, f connected subgraph whose vertices
include f (p) descendents except vertices range f
descendents.
Meanings sense concern lowest level phrasal meanings, occurring
terminal nodes semantic grammar, namely entries semantic lexicon.
grammar used construct meanings longer phrases entire sentences.
motivation previously stated constraint root must included
range f : want vertices sentence representation included
meaning phrase. Note meaning p directed tree f (p)
root. Figure 4 shows meanings phrase domain interpretation function
f1 shown Figure 3. show labels vertices edges readability.
Definition: Given finite set ST F triples < s1 , t1 , f1 >, . . . , < sn , tn , fn >,
si finite string, ti directed, labeled tree, interpretation function
si ti , let language LST F = {p1 , . . . , pk } ST F union substrings2
occur domain . pj LST F , meaning set pj , denoted
MST F (pj ),3 set meanings pj relative si , ti , < si , ti , > ST F .
consider two meanings isomorphic trees taking labels
account.
example, given sentence s2 : man ate cheese, labeled tree t2 pictured
Figure 5, f2 defined as: f2 (ate) = 1, f2 (man) = 2, f2 (the cheese) = 3;
2. consider two substrings string contain characters order,
irrespective positions within larger string occur.
3. omit subscript set ST F obvious context.

7

fiThompson & Mooney

String s2 : man ate cheese."
Tree t2 :

t2 vertex edge labels:
ingest
patient
agent

1
2

4

3

person food
type
age
sex
6

5

male

adult

cheese

Figure 5: Second Tree
meaning set cheese respect ST F = {< s1 , t1 , f1 >, < s2 , t2 , f2 >} {[food,
type:cheese]}, one meaning though f1 f2 map cheese different vertices
two trees, subgraphs denoting meaning cheese two
functions isomorphic.
Definition: Given finite set ST F triples < s1 , t1 , f1 >, . . . , < sn , tn , fn >,
si finite string, ti directed, labeled tree, interpretation
function si ti , covering lexicon expressed ST F
{(p, m) : p LST F , (p)}.
covering lexicon L expressed ST F = {< s1 , t1 , f1 >, < s2 , t2 , f2 >} is:
{

(girl, [person, sex:female, age:child]),
(man, [person, sex:male, age:adult]),
(ate, [ingest]),
(pasta, [food, type:pasta]),
(the cheese, [food, type:cheese]) }.

idea covering lexicon provides, string (sentence) si , meaning
phrases sentence. Further, meanings trees whose labeled
vertices together include labeled vertices tree ti representing meaning
si , vertices duplicated, containing vertices ti . Edge labels may
may included, since idea due syntax,
parser provide; edges capturing lexical semantics lexicon. Note
include covering lexicon phrases (substrings) domains
s, words empty tree meaning included covering lexicon.
Note general use phrase mean substrings sentences, whether
consist one word, one. Finally strings covering lexicon may
contain overlapping words even though domain individual interpretation
function must not, since overlapping words could occurred different sentences.
Finally, ready define learning problem hand.

8

fiAcquiring Word-Meaning Mappings

Lexicon Acquisition Problem:
Given: multiset strings = {s1 , . . . , sn } multiset labeled trees = {t1 , . . . , tn },
Find: multiset interpretation functions, F = {f1 , . . . , fn }, cardinality
covering lexicon expressed ST F = {< s1 , t1 , f1 >, . . . , < sn , tn , fn >} minimized.
set found, say found minimal set interpretations (or minimal
covering lexicon). 2
Less formally, learner presented multiset sentences (S) paired
meanings (T ); goal learning find smallest lexicon consistent data.
lexicon paired listing phrases occurring domain F
(where F multiset interpretation functions found) elements
meaning sets. motivation finding lexicon minimal size usual bias
towards simplicity representation generalization beyond training data.
definition allows phrases length, usually want limit length
phrases considered inclusion domain interpretation functions,
efficiency purposes.
determine set interpretation functions set strings trees,
one unique covering lexicon expressed ST F . However, might
set interpretation functions possible, may result lexicon smallest
cardinality. example, covering lexicon given previous example
minimal covering lexicon. two sentences given, could find minimal, though
rather degenerate, lexicons as:
{

(girl,
(man,

[ingest, agent:[person, sex:female, age:child],
patient:[food, type:pasta, accomp:[food, type:cheese]]]),
[ingest, agent:[person, sex:male, age:adult],
patient:[food, type:cheese]]) }

type lexicon becomes less likely size corpus grows.
3.2 Implications Definition
definition lexicon acquisition problem differs given authors,
including Riloff Jones (1999), Siskind (1996), Manning (1993), Brent (1991) others,
discussed Section 7. definition problem makes assumptions
training input. First, making f function instead relation, definition
assumes meaning phrase sentence appears representation
sentence, single-use assumption. Second, making f one-to-one, assumes
exclusivity, vertex sentences representation due one phrase
sentence. Third, assumes phrases meaning connected subgraph sentences
representation, distributed representation, connectedness assumption.
first assumption may hold representation languages, present
problem domains considered. second third assumptions perhaps
less problematic respect general language use.
definition assumes compositionality: meaning sentence derived
meanings phrases contains, addition, perhaps connecting
information specific representation hand, derived external sources
9

fiThompson & Mooney

noise. words, vertices sentences representation included
within meaning word phrase sentence. assumption similar
linking rules Jackendoff (1990), used previous work grammar
language acquisition (e.g., Haas Jayaraman, 1997; Siskind, 19964 )
debate linguistics community ability compositional techniques
handle phenomena (Fillmore, 1988; Goldberg, 1995), making assumption simplifies
learning process works reasonably domains interest here. Also, since
allow multi-word phrases lexicon (e.g., (kick bucket, die( ))), one objection
compositionality addressed.
definition allows training input which:
1. Words phrases multiple meanings. is, homonymy might occur
lexicon.
2. Several phrases map meaning. is, synonymy might occur
lexicon.
3. words sentence map meanings, leaving unused
assignment words meanings.5
4. Phrases contiguous words map parts sentences meaning representation.
particular note lexical ambiguity (1 above). Note could derived
ambiguous lexicon as:
{

(girl, [person, sex:female, age:child]),
(ate, [ingest]),
(ate, [ingest, agent:[person, sex:male, age:adult]]),
(pasta, [food, type:pasta]),
(the cheese, [food, type:cheese]) }.

sample corpus. lexicon, ate ambiguous word. earlier example
minimizes ambiguity resulting alternative, intuitively pleasing lexicon.
problem definition first minimizes number entries lexicon, learning
algorithm exploit preference minimizing ambiguity.
note definition allows training input sentences
ambiguous (paired one meaning), since given sentence (a multiset)
might appear multiple times appear one meaning. fact, training data
consider Section 5 ambiguous sentences.
definition lexicon acquisition problem fit cleanly traditional
definition learning classification. training example contains sentence
semantic parse, trying extract semantic information phrases
sentence. example potentially contains information multiple target
concepts (phrases), trying pick relevant features, vertices
4. fact, assumptions except single-use made Siskind (1996); see Section 7
details.
5. words may, however, serve cues parser assemble sentence meanings word
meanings.

10

fiAcquiring Word-Meaning Mappings

representation, corresponding correct meaning phrase. course, assumptions single-use, exclusivity, connectedness, compositionality impose additional
constraints. addition multiple examples one learning scenario,
access negative examples, derive implicit negatives,
possibility ambiguous synonymous phrases.
ways problem related clustering, capable learning
multiple, potentially non-disjoint categories. However, clear clustering
system could made learn phrase-meaning mappings needed parsing. Finally,
current systems learn multiple concepts commonly use examples concepts
negative examples concept currently learned. implicit assumption made
concepts disjoint, unwarranted assumption presence
synonymy.

4. Wolfie Algorithm Example
section, first discuss issues considered design algorithm,
describe fully Section 4.2.
4.1 Solving Lexicon Acquisition Problem
first attempt solve Lexicon Acquisition Problem might examine interpretation functions across corpus, choose one(s) minimal lexicon size.
number possible interpretation functions given input pair dependent
size sentence representation. sentence w words, (w2 )
possible phrases, particular challenge.
However, number possible interpretation functions grows extremely quickly
size input. sentence p phrases associated tree n vertices,
number possible interpretation functions is:
c!(n 1)!

c
X
i=1

1
.
(i 1)!(n i)!(c i)!

(1)

c min(p, n). derivation formula follows. must choose
phrases use domain f , choose one phrase, two,
number min(p, n) (if n < p assign n phrases since f one-to-one),
p


!

=

p!
i!(p i)!

number phrases chosen. permute phrases,
order assigned vertices different. i! permutations. must choose vertices include range interpretation
function. choose root time, choosing vertices,
n 1 choose 1 vertices left choosing root,
n1
i1

!

=

(n 1)!
.
(i 1)!(n i)!
11

fiThompson & Mooney

full number possible interpretation functions then:
min(p,n)

X
i=1

p!
(n 1)!
i!
,
i!(p i)!
(i 1)!(n i)!

simplifies Equation 1. n = p, largest term equation c! =
p!, grows least exponentially p, general number interpretation
functions large allow enumeration. Therefore, finding lexicon examining
interpretations across corpus, choosing lexicon(s) minimum size, clearly
tractable.
Instead finding interpretations, one could find set candidate meanings
phrase, final meaning(s) phrase could chosen way
minimizes lexicon size. One way find candidate meanings fracture meanings
sentences phrase appears. Siskind (1993) defined fracturing (he calls
Unlink* operation) terms result includes subterms
expression plus . representation formalism, corresponds finding possible
connected subgraphs meaning, adding empty graph. interpretation
function technique discussed, fracturing would lead exponential blowup
number candidate meanings phrase: lower bound number connected
subgraphs full binary tree n vertices obtained noting subset
(n + 1)/2 leaves may deleted still maintain connectivity remaining tree.
Thus, counting ways leaves deleted gives us lower bound 2(n+1)/2
fractures.6 completely rule fracturing part technique lexicon
learning since trees tend get large, indeed Siskind uses many
systems, constraints help control search. However, wish avoid
chance exponential blowup preserve generality approach tasks.
Another option force Chill essentially induce lexicon own.
model, would provide Chill ambiguous lexicon phrase paired
every fracture every sentence appears. Chill would decide
set fractures leads correct parse training sentence, would
include final learned parser-lexicon combination. Thus search would
become exponential. Furthermore, even small representations, would likely lead
system poor generalization ability. Siskinds work (e.g., Siskind,
1992) took syntactic constraints account encounter difficulties,
versions handle lexical ambiguity.
could efficiently find good candidates, standard induction algorithm could
attempt use source training examples phrase. However,
attempt use list candidate meanings one phrase negative examples
another phrase would flawed. learner could know advance phrases
possibly synonymous, thus phrase lists use negative examples
phrase meanings. Also, many representation components would present lists
one phrase. source conflicting evidence learner, even without
presence synonymy. Since positive examples available, one might think
using specific conjunctive learning, finding intersection representations
6. Thanks net-citizen Dan Hirshberg help analysis.

12

fiAcquiring Word-Meaning Mappings

phrase, p (of two words):
1.1) Collect training examples p appears
1.2) Calculate LICS (sampled) pairs examples representations
1.3) l LICS, add (p, l) set candidate lexicon entries
input representations covered, candidate lexicon entries remain do:
2.1) Add best (phrase, meaning) pair candidate entries lexicon
2.2) Update candidate meanings phrases sentences phrase learned
Return lexicon learned (phrase, meaning) pairs.

Figure 6: Wolfie Algorithm Overview
phrase, proposed Anderson (1977). However, meanings ambiguous
phrase disjunctive, intersection would empty. similar difficulty would
expected positive-only compression Muggleton (1995).
4.2 Solution: Wolfie
analysis leads us believe Lexicon Acquisition Problem computationally intractable. Therefore, perform efficient search best lexicon.
use standard induction algorithm. Therefore, implemented Wolfie7 ,
outlined Figure 6, finds approximate solution Lexicon Acquisition Problem. approach generate set candidate lexicon entries, final
learned lexicon derived greedily choosing best lexicon item point,
hopes finding final (minimal) covering lexicon. actually learn interpretation
functions, guarantee find covering lexicon.8 Even
search interpretation functions, using greedy search would guarantee covering
input, course guarantee minimal lexicon found. However,
later present experimental results demonstrating greedy approach performs
well.
Wolfie first derives initial set candidate meanings phrase. algorithm
generating candidates, LICS, attempts find maximally common meaning
phrase, biases toward finding small lexicon covering many vertices tree
once, finding lexicon actually cover input. Second, Wolfie chooses
final lexicon entries candidate set, one time, updating candidate set
goes, taking account assumptions single-use, connectedness, exclusivity.
basic scheme choosing entries candidate set maximize prediction
meanings given phrases, find general meanings. adds tension
LICS, cover many vertices, generality, biases towards fewer vertices. However, generality, LICS, helps lead small lexicon since general meaning
likely apply widely across corpus.
7. code available upon request first author.
8. Though, course, interpretation functions way guarantee covering lexicon see
Siskind (1993) alternative.

13

fiThompson & Mooney

answer/2

1

2

2



2

state/1

eq/2

1

1


2

C

cityid/2

loc/2
2
1
C



1
texarkana
Figure 7: Tree Variables
Let us explain algorithm detail way example, using Spanish
instead English illustrate difficulty somewhat clearly. Consider following
corpus:
1. Cual es el capital del estado con la poblacion mas grande?
answer(C, (capital(S,C), largest(P, (state(S), population(S,P))))).
2. Cual es la punta mas alta del estado con la area mas grande?
answer(P, (high point(S,P), largest(A, (state(S), area(S,A))))).
3. En que estado se encuentra Texarkana?
answer(S, (state(S), eq(C,cityid(texarkana, )), loc(C,S))).
4. Que capital es la mas grande?
answer(A, largest(A, capital(A))).
5. Que es la area de los estados unitos?
answer(A, (area(C,A), eq(C,countryid(usa)))).
6. Cual es la poblacion de un estado que bordean Utah?
answer(P, (population(S,P), state(S), next to(S,M), eq(M,stateid(utah)))).
7. Que es la punta mas alta del estado con la capital Madison?
answer(C, (high point(B,C), loc(C,B), state(B),
capital(B,A), eq(A,cityid(madison, )))).

sentence representations slightly different tree representations given
problem definition, main difference addition existentially quantified
variables shared leaves representation tree. mentioned Section 2.1,
representations Prolog queries database. Given query, create
tree conforms formalism, addition quantified variables.
example shown Figure 7 representation third sentence. vertex
predicate name arity, Prolog style, e.g., state/1, quantified variables
leaves. outgoing edge (n, m) vertex n, edge labeled
argument position filled subtree rooted m. edge labeled
given argument position, argument free variable. vertex labeled
14

fiAcquiring Word-Meaning Mappings

variable (which occur leaves) existentially quantified variable whose scope
entire tree (or query). learned lexicon, however, need maintain
identity variables across distinct lexical entries.
Another representation difference strip answer predicate
input learner,9 thus allowing forest directed trees input rather single
tree. definition problem easily extends root tree
forest must domain interpretation function.
Evaluation system using representation given Section 5.1; evaluation
using representation without variables forests presented Section 5.2. previously
(Thompson, 1995) presented results demonstrating learning representations different
form, case-role representation (Fillmore, 1968) augmented Conceptual Dependency (Schank, 1975) information. last representation conforms directly
problem definition.
Now, continuing example solving Lexicon Acquisition Problem
corpus, let us assume simplification, although required, sentences
stripped phrases know empty meanings (e.g., que, es, con, la).
similarly assume known phrases refer directly given database
constants (e.g., location names), remove phrases meaning
training input.
4.2.1 Candidate Generation Phase
Initial candidate meanings phrase produced computing maximally common
substructure(s) sampled pairs representations sentences contain it.
derive common substructure computing Largest Isomorphic Connected Subgraphs
(LICS) two labeled trees, taking labels account isomorphism. analogous
Largest Common Subgraph problem (Garey & Johnson, 1979) solvable polynomial
time if, assume, inputs trees K, number edges include,
given. Thus, start K set equal largest number edges two trees
compared, test common subgraph(s), iterate K = 1, stopping one
subgraphs found given K.
Prolog query representation, algorithm complicated bit variables.
Therefore, use LICS addition similar computing Least General Generalization first-order clauses (Plotkin, 1970). LGG two sets literals least
general set literals subsumes sets literals. add allowing
term argument literal conjunction, algorithm tries orderings
matching terms conjunction. Overall, algorithm finding LICS
two trees Prolog representation first finds common labeled edges
vertices usual LICS, treats variables equivalent. Then, computes
Least General Generalization, conjunction taken account, resulting trees
converted back literals. example, given two trees:

9. predicate omitted Chill initializes parse stack answer predicate, thus
word mapped it.

15

fiThompson & Mooney

Phrase
capital:

grande:
estado:

punta mas:
encuentra:

LICS
largest( , )
capital( , )
state( )
largest( ,state( ))
largest( , )
largest( ,state( ))
state( )
(population(S, ), state(S))
capital( , )
high point( , )
(state(S), loc( ,S))
high point( , )
state( )
(state(S), loc( ,S))

Sentences
1,4
1,7
1,7
1,2
1,4; 2,4
1,2
1,3; 1,7; 2,3; 2,6; 2,7; 3,6; 6,7
1,6
1,7
2,7
3,7
2,7
2,7
3

Table 1: Sample Candidate Lexical Entries Derivation
answer(C, (largest(P, (state(S), population(S,P))), capital(S,C))).
answer(P, (high point(S,P), largest(A, (state(S), area(S,A))))).,
common meaning answer( ,largest( ,state( )). Note LICS two trees
may unique: may multiple common subtrees contain
number edges; case LICS returns multiple answers.
sets initial candidate meanings phrases sample corpus
shown Table 1. example show LICS pairs phrase
appears in, actual algorithm randomly sample subset efficiency reasons,
Golem (Muggleton & Feng, 1990). phrases appearing one sentence
(e.g., encuentra), entire sentence representation (excluding database constant
given background knowledge) used initial candidate meaning. candidates
typically generalized step 2.2 algorithm correct portion
representation added lexicon; see example below.
4.2.2 Adding Final Lexicon
deriving initial candidates, greedy search begins. heuristic used evaluate
candidates attempts help assure small covering lexicon learned. heuristic
first looks weighted sum two components, p phrase candidate
meaning:
1. P (m | p) P (p | m) P (m) = P (p) P (m | p)2
2. generality
Then, ties value broken preferring less ambiguous (those fewer current
meanings) shorter phrases. first component analogous cluster evaluation
16

fiAcquiring Word-Meaning Mappings

heuristic used Cobweb (Fisher, 1987), measures utility clusters based
attribute-value pairs categories, instead meanings phrases. probabilities
estimated training data updated learning progresses account
phrases meanings already covered. see updating works
continue example algorithm. goal part heuristic
maximize probability predicting correct meaning randomly sampled
phrase. equality holds Bayes Theorem. Looking right side, P (m | p)2
expected probability meaning correctly guessed given phrase, p.
assumes strategy probability matching, meaning chosen p
probability P (m | p) correct probability. term, P (p), biases
component common phrase is. Interpreting left side equation,
first term biases towards lexicons low ambiguity, second towards low synonymy,
third towards frequent meanings.
second component heuristic, generality, computed negation
number vertices meanings tree structure, helps prefer smaller,
general meanings. example, candidate set above, else equal,
generality portion heuristic would prefer state( ), generality value -1,
largest( ,state( )) (state(S),loc( ,S)), generality value -2,
meaning estado. Learning meaning fewer terms helps evenly distribute
vertices sentences representation among meanings phrases sentence,
thus leads lexicon likely correct. see this, note
pairs words tend frequently co-occur (grande estado example),
joint representation (meaning) likely set candidate meanings
words. preferring general meaning, easily ignore incorrect joint
meanings.
example experiments, use weight 10 first component
heuristic, weight 1 second. first component smaller absolute
values therefore given higher weight. Modulo consideration, results
overly-sensitive weights automatically setting using cross-validation
training set (Kohavi & John, 1995) little effect overall performance. Table 2
illustrate calculation heuristic measure fourteen pairs,
value all. calculation shows sum multiplying 10 first component
heuristic multiplying 1 second component. first component simplified
follows:
| p | | p |2
| p |2
P (p) P (m | p)2 =


,

| p |2
|p|
| p | number times phrase p appears corpus, initial number
candidate phrases, | p | number times meaning paired
phrase p. ignore since number phrases corpus
pair, effect ranking. highest scoring pair (estado, state( )),
added lexicon.
Next candidate generalization step (2.2), described algorithmically Figure 8.
One key ideas algorithm phrase-meaning choice constrain
candidate meanings phrases yet learned. Given assumption portion
representation due one phrase sentence (exclusivity), part
17

fiThompson & Mooney

Candidate Lexicon Entry
(capital, largest( , )):
(capital, capital( , )):
(capital, state( , )):
(grande, largest( ,state( ))):
(grande, largest( , )):
(estado, largest( ,state( ))):
(estado, state( )):
(estado, (population(S, ), state(S)):
(estado, capital( , )):
(estado, high point( , )):
(estado, (state(S), loc( ,S))):
(punta mas, high point( , )):
(punta mas, state( )):
(encuentra, (state(S), loc( ,S))):

Heuristic Value
10(22 /3) + 1(1) = 12.33
12.33
12.33
10(22 /3) + 1(2) = 11.3
29
10(22 /5) + 1(2) = 6
10(52 /5) + 1(1) = 49
6
7
7
6
19
10(22 /2) + 1(1) = 19
10(12 /1) + 1(2) = 8

Table 2: Heuristic Value Sample Candidate Lexical Entries

Given: learned phrase-meaning pair (l, g)
sentence-representation pairs containing l g, mark covered.
candidate phrase-meaning pair (p, m):
p occurs training pairs (l, g)
vertices intersect vertices g
occurrences covered
Remove (p, m) set candidate pairs.
Else
Adjust heuristic value (p, m) needed account
newly covered nodes training representations.
Generalize remove covered nodes, obtaining m0 ,
Calculate heuristic value new candidate pair (p, m0 ).
candidate meanings remain uncovered phrase
Derive new LICS uncovered representations
calculate heuristic values.

Figure 8: Candidate Generalization Phase

18

fiAcquiring Word-Meaning Mappings

representation covered, phrase sentence paired meaning
(at least sentence). Therefore, step 2.2 candidate meanings words
sentences word learned generalized exclude representation
learned. use operation analogous set difference finding remaining
uncovered vertices representation generalizing meanings eliminate covered
vertices candidate pairs. example, meaning largest( , ) learned
phrase sentence 2, meaning left behind would forest consisting
trees high point(S, ) (state(S), area(S, )). Also, generalization results
empty tree, new LICS calculated. example, since state( ) covered
sentences 1, 2, 3, 6, 7, candidates several words sentences
generalized. example, meaning (state(S), loc( ,S)) encuentra,
generalized loc( , ), new heuristic value 10(12 /1) + 1(1) = 9. Also,
single-use assumption allows us remove candidate pairs containing estado
set candidate meanings, since learned pair covers occurrences estado
set.
Note pairwise matchings generate candidate items, together updating candidate set, enable multiple meanings learned ambiguous phrases,
makes algorithm less sensitive initial rate sampling LICS. example,
note capital ambiguous data set, though ambiguity artifact
way query language designed, one ordinarily think
ambiguous word. However, meanings learned: second pair added
final lexicon (grande, largest( , )), causes generalization empty
meaning first candidate entry Table 2, since new LICS sentence 4
generated, entire remaining meaning added candidate meaning set
capital mas.
Subsequently, greedy search continues resulting lexicon covers training
corpus, candidate phrase meanings remain. rare cases, learning errors occur
leave portions representations uncovered. example, following lexicon
learned:
(estado, state( )),
(grande, largest( )),
(area, area( )),
(punta, high point( , )),
(poblacion, population( , )),
(capital, capital( , )),
(encuentra, loc( , )),
(alta, loc( , )),
(bordean, next to( )),
(capital, capital( )).
next section, discuss ability Wolfie learn lexicons useful
parsers parser acquisition.

19

fiThompson & Mooney

5. Evaluation Wolfie
following two sections discuss experiments testing Wolfies success learning lexicons
real artificial corpora, comparing several cases previously developed
lexicon learning system.
5.1 Database Query Application
section describes experimental results database query application. first
corpus discussed contains 250 questions U.S. geography, paired Prolog
query extract answer question database. domain originally
chosen due availability hand-built natural language interface, Geobase,
database containing 800 facts. Geobase supplied Turbo Prolog 2.0
(Borland International, 1988), designed specifically domain. questions
corpus collected asking undergraduate students generate English questions
database, though given cursory knowledge database without
given chance use it. broaden test, 250 sentences translated
Spanish, Turkish, Japanese. Japanese translations word-segmented Roman
orthography. Translated questions paired appropriate logical queries
English corpus.
evaluate learned lexicons, measured utility background knowledge
Chill. performed choosing random set 25 test examples
learning lexicons parsers increasingly larger subsets remaining 225 examples
(increasing 50 examples time). training, test examples parsed using
learned parser. submit resulting queries database, compare
answers generated submitting correct representation database,
record percentage correct (matching) answers. using difficult gold standard
retrieving correct answer, avoid measures partial accuracy believe
adequately measure final utility. repeated process ten different random training
test sets evaluated performance differences using two-tailed, paired t-test
significance level p 0.05.
compared system incremental (on-line) lexicon learner developed Siskind
(1996). make equitable comparison batch algorithm, ran simulated batch mode, repeatedly presenting corpus 500 times, analogous running
500 epochs train neural network. actually add new kinds data
learn, allows algorithm perform inter-sentential inference directions corpus instead one. point compare accuracy
size training corpus, metric optimized Siskind. worried
difference execution time here,10 lexicons learned running Siskinds
system incremental mode (presenting corpus single time) resulted substantially
lower performance preliminary experiments data. removed Wolfies
ability learn phrases one word, since current version Siskinds system
10. CPU times two system directly comparable since one written Prolog
Lisp. However, learning time two systems approximately Siskinds
system run incremental mode, seconds 225 training examples.

20

fiAcquiring Word-Meaning Mappings

90

80

70

Accuracy

60

50

40

30

CHILL+handbuilt
CHILL-testlex
CHILL+Wolfie
CHILL+Siskind
Geobase

20

10

0
0

50

100
150
Training Examples

200

250

Figure 9: Accuracy English Geography Corpus
ability. Finally, made comparisons parsers learned Chill
using hand-coded lexicon background knowledge.
similar applications, many terms, state city names,
whose meanings automatically extracted database. Therefore, tests
run names given learner initial lexicon; helpful
required. Section 5.2 gives results different task initial lexicon.
However, unless otherwise noted, tests within Section (5.1) strip
sentences phrases known empty meanings, unlike example Section 4.
5.1.1 Comparisons using English
first experiment comparison original English corpus. Figure 9 shows
learning curves Chill using lexicons learned Wolfie (CHILL+Wolfie)
Siskinds system (CHILL+Siskind). uppermost curve (CHILL+handbuilt) shows
Chills performance given hand-built lexicon. CHILL-testlex shows performance words never appear training data (e.g., test sentences)
deleted hand-built lexicon (since learning algorithm chance learning
these). Finally, horizontal line shows performance Geobase benchmark.
results show lexicon learned Wolfie led parsers almost
accurate generated using hand-built lexicon. best accuracy achieved
parsers using hand-built lexicon, followed hand-built lexicon words
test set removed, followed Wolfie, followed Siskinds system. systems
well better Geobase time reach 125 training examples.
differences Wolfie Siskinds system statistically significant training
21

fiThompson & Mooney

Lexicon
hand-built
Wolfie
Siskind

Coverage
100%
100%
94.4%

Ambiguity
1.2
1.1
1.7

Entries
88
56.5
154.8

Table 3: Lexicon Comparison
example sizes. results show Wolfie learn lexicons support learning
successful parsers, better perspective learned
competing system. Also, comparing CHILL-testlex curve, see
drop accuracy hand-built lexicon due words test set system
seen training. fact, none differences CHILL+Wolfie
CHILL-testlex statistically significant.
One implicit hypotheses problem definition coverage training
data implies good lexicon. results show coverage 100% 225 training examples Wolfie versus 94.4% Siskind. addition, lexicons learned Siskinds
system ambiguous larger learned Wolfie. Wolfies lexicons average 1.1 meanings per word, average size 56.5 entries (after
225 training examples) versus 1.7 meanings per word 154.8 entries Siskinds lexicons. comparison, hand-built lexicon 1.2 meanings per word 88 entries.
differences, summarized Table 3, undoubtedly contribute final performance
differences.
5.1.2 Performance Natural Languages
Next, examined performance two systems Spanish version corpus.
Figure 10 shows results. differences using Wolfie Siskinds learned
lexicons Chill statistically significant training set sizes.
show performance hand-built lexicons, without phrases present
testing set. performance compared hand-built lexicon test-set phrases
removed still competitive, difference significant 225 examples.
Figure 11 shows accuracy learned parsers Wolfies learned lexicons
four languages. performance differences among four languages quite small,
demonstrating methods language dependent.
5.1.3 Larger Corpus
Next, present results larger, diverse corpus geography domain,
additional sentences collected computer science undergraduates
introductory AI course. set questions smaller corpus collected
students German class, special instructions complexity queries desired.
AI students tended ask complex diverse queries: task give five
interesting questions associated logical form homework assignment, though
direct access database. requested give least
one sentence whose representation included predicate containing embedded predicates,

22

fiAcquiring Word-Meaning Mappings

100
90
80
70

Accuracy

60
50
40
30

Span-CHILL+handbuilt
Span-CHILL-testlex
Span-CHILL+Wolfie
Span-CHILL+Siskind

20
10
0
0

50

100
150
Training Examples

200

250

200

250

Figure 10: Accuracy Spanish

100
90
80
70

Accuracy

60
50
40
30

English
Spanish
Japanese
Turkish

20
10
0
0

50

100
150
Training Examples

Figure 11: Accuracy Four Languages

23

fiThompson & Mooney

100
90
80
70

Accuracy

60
50
40
30

CHILL
WOLFIE
Geobase

20
10
0
0

50

100

150

200
250
Training Examples

300

350

400

450

Figure 12: Accuracy Larger Geography Corpus
example largest(S, state(S)), asked variety sentences.
221 new sentences, total 471 (including original 250 sentences).
experiments, split data 425 training sentences 46 test sentences, 10 random splits, trained Wolfie Chill before. goal
see whether Wolfie still effective difficult corpus, since
approximately 40 novel words new sentences. Therefore, tested performance Chill extended hand-built lexicon. test, stripped sentences
phrases known empty meanings, example Section 4.2. Again,
use phrases one word, since seem make significant
difference domain. results, compare Wolfies lexicons Chill using
hand-built lexicons without phrases appear test set.
Figure 12 shows resulting learning curves. differences Chill using
hand-built learned lexicons statistically significant 175, 225, 325, 425
examples (four nine data points). mixed results indicate
difficulty domain variable vocabulary. However, improvement
machine learning methods Geobase hand-built interface much dramatic
corpus.
5.1.4 LICS versus Fracturing
One component algorithm yet evaluated explicitly candidate generation
method. mentioned Section 4.1, could use fractures representations sentences
phrase appears generate candidate meanings phrase, instead
LICS. used approach compared previously described method
using largest isomorphic connected subgraphs sampled pairs representations

24

fiAcquiring Word-Meaning Mappings

100
90
80
70

Accuracy

60
50
40
30

fractWOLFIE
WOLFIE

20
10
0
0

50

100
150
Training Examples

200

250

Figure 13: Fracturing vs. LICS: Accuracy
candidate meanings. attempt fair comparison, sampled representations
fracturing, using number source representations number pairs
sampled LICS.
accuracy Chill using resulting learned lexicons background knowledge shown Figure 13. Using fracturing (fractWOLFIE) shows little advantage;
none differences two systems statistically significant.
addition, number initial candidate lexicon entries choose
much larger fracturing LICS method, shown Figure 14. true even
though sampled number representations pairs LICS,
larger number fractures arbitrary representation number LICS
arbitrary pair. Finally, Wolfies learning time using fracturing greater
using LICS, shown Figure 15, CPU time shown seconds.
summary, differences show utility LICS method generating
candidates: thorough method result better performance, results
longer learning times. One could claim handicapping fracturing since
sampling representations fracturing. may indeed help accuracy,
learning time number candidates would likely suffer even further. domain
larger representations, differences learning time would even dramatic.
5.2 Artificial Data
previous section showed Wolfie successfully learns lexicons natural corpus
realistic task. However, demonstrates success relatively small corpus
one representation formalism. show algorithm scales well
lexicon items learn, ambiguity, synonymy. factors

25

fiThompson & Mooney

600

Number Candidates

500

400

300

fractWOLFIE
WOLFIE

200

100

0
0

50

100
150
Training Examples

200

250

Figure 14: Fracturing vs. LICS: Number Candidates

4

3.5

Learning Time (sec)

3

2.5

2

1.5

1

0.5

fractWOLFIE
WOLFIE

0
0

50

100
150
Training Examples

200

Figure 15: Fracturing vs. LICS: Learning Time

26

250

fiAcquiring Word-Meaning Mappings

difficult control using real data input. Also, large corpora available
annotated semantic parses. therefore present experimental results
artificial corpus. corpus, sentences representations completely
artificial, sentence representation variable-free representation, suggested
work Jackendoff (1990) others.
corpus discussed below, random lexicon mapping words simulated meanings
first constructed.11 original lexicon used generate corpus random
utterances paired meaning representation. using corpus input
Wolfie12 , learned lexicon compared original lexicon, weighted precision
weighted recall learned lexicon measured. Precision measures percentage
lexicon entries (i.e., word-meaning pairs) system learns correct.
Recall measures percentage lexicon entries hand-built lexicon
correctly learned system:
precision =
recall =

# correct pairs
# pairs learned

# correct pairs
.
# pairs hand-built lexicon

get weighted precision recall measures, weight results pair
words frequency entire corpus (not training corpus). models
likely learned correct meaning arbitrarily chosen word
corpus.
generated several lexicons associated corpora, varying ambiguity rate (number meanings per word) synonymy rate (number words per meaning), Siskind
(1996). Meaning representations generated using set conceptual symbols
combined form meaning word. number conceptual symbols used
lexicon noted describe corpus below. lexicon, 47.5%
senses variable-free simulate noun-like meanings, 47.5% contained
one three variables denote open argument positions simulate verb-like meanings.
remainder words (the remaining 5%) empty meaning simulate function words. addition, functors meaning could depth two
arity two. example noun-like meaning f23(f2(f14)), verbmeaning f10(A,f15(B)); conceptual symbols example f23, f2, f14, f10,
f15. using multi-level meaning representations demonstrate learning
complex representations geography database domain: none
hand-built meanings phrases lexicon functors embedded arguments.
used grammar generate utterances meanings original lexicon,
terminal categories selected using distribution based Zipfs Law (Zipf, 1949).
Zipfs Law, occurrence frequency word inversely proportional ranking
occurrence.
started baseline corpus generated lexicon 100 words using 25 conceptual symbols ambiguity synonymy; 1949 sentence-meaning pairs generated.
11. Thanks Jeff Siskind initial corpus generation software, enhanced tests.
12. tests, allowed Wolfie learn phrases length two.

27

fiThompson & Mooney

100
90
80
70

Accuracy

60
50
40
30

Precision
Recall

20
10
0
0

200

400

600

800
1000
Training Examples

1200

1400

1600

1800

Figure 16: Baseline Artificial Corpus
split five training sets 1700 sentences each. Figure 16 shows weighted
precision recall curves initial test. demonstrates good scalability
slightly larger corpus lexicon U.S. geography query domain.
second corpus generated second lexicon, 100 words using 25 conceptual symbols, increasing ambiguity 1.25 meanings per word. time, 1937 pairs
generated corpus split five sets 1700 training examples each. Weighted
precision 1650 examples drops 65.4% previous level 99.3%, weighted
recall 58.9% 99.3%. full learning curve shown Figure 17. quick comparison Siskinds performance corpus confirmed system achieved comparable
performance, showing current methods, close best performance
able obtain difficult corpus. One possible explanation smaller
performance difference two systems corpus versus geography domain domain, correct meaning word necessarily
general, terms number vertices, candidate meanings. Therefore,
generality portion heuristic may negatively influence performance Wolfie
domain.
Finally, show change performance increasing ambiguity increasing
synonymy, holding number words conceptual symbols constant. Figure 18 shows
weighted precision recall 1050 training examples increasing levels ambiguity, holding synonymy level constant. Figure 19 shows results increasing
levels synonymy, holding ambiguity constant. Increasing level synonymy
effect results much increasing level ambiguity, expected.
Holding corpus size constant increasing number competing meanings
word increases number candidate meanings created Wolfie decreasing
amount evidence available meaning (e.g., first component heuristic
28

fiAcquiring Word-Meaning Mappings

70

60

Accuracy

50

40

30

Precision
Recall

20

10

0
0

200

400

600

800
1000
Training Examples

1200

1400

1600

1800

Figure 17: Ambiguous Artificial Corpus
100

95

90

Recall
Precision

Accuracy

85

80

75

70

65

60
1

1.25

1.5
Number Meanings per Word

1.75

2

Figure 18: Increasing Level Ambiguity
measure) makes learning task difficult. hand, increasing
level synonymy potential mislead learner.
number training examples required reach certain level accuracy
informative. Table 4, show point standard precision 75% first

29

fiThompson & Mooney

100

Accuracy

95

90

Recall
Precision

85

80
1

1.25

1.5
Number Words per Meaning

1.75

2

Figure 19: Increasing Level Synonymy
Ambiguity Level
1.0
1.25
2.0

Number Examples
150
450
1450

Table 4: Number Examples Reach 75% Precision
reached level ambiguity. Note, however, measured accuracy
set 100 training examples, numbers table approximate.
performed second test scalability two corpora generated lexicons
order magnitude larger tests. tests, use lexicon
containing 1000 words using 250 conceptual symbols. generated corpus
ambiguity, one lexicon ambiguity synonymy similar found
WordNet database (Beckwith, Fellbaum, Gross, & Miller, 1991); ambiguity
approximately 1.68 meanings per word synonymy 1.3 words per meaning.
corpora contained 9904 (no ambiguity) 9948 examples, respectively, split
data five sets 9000 training examples each. easier large corpus, maximum
average weighted precision recall 85.6%, 8100 training examples,
harder corpus, maximum average 63.1% 8600 training examples.

6. Active Learning
indicated previous sections, built integrated system language
acquisition flexible useful. However, major difficulty remains: construction
training corpora. Though annotating sentences still arguably less work building
30

fiAcquiring Word-Meaning Mappings

Apply learner n bootstrap examples, creating classifier.
examples remain annotator unwilling label examples, do:
Use recently learned classifier annotate unlabeled instance.
Find k instances lowest annotation certainty.
Annotate instances.
Train learner bootstrap examples examples annotated far.

Figure 20: Selective Sampling Algorithm
entire system hand, annotation task time-consuming error-prone.
Further, training pairs often contain redundant information. would minimize
amount annotation required still maintaining good generalization accuracy.
this, turned methods active learning. Active learning research area
machine learning features systems automatically select informative
examples annotation training (Angluin, 1988; Seung, Opper, & Sompolinsky, 1992),
rather relying benevolent teacher random sampling. primary goal
active learning reduce number examples system trained on,
maintaining accuracy acquired information. Active learning systems may construct examples, request certain types examples, determine set
unsupervised examples usefully labeled. last approach, selective sampling
(Cohn et al., 1994), particularly attractive natural language learning, since
abundance text, would annotate informative sentences.
many language learning tasks, annotation particularly time-consuming since requires
specifying complex output rather category label, reducing number
training examples required greatly increase utility learning.
section, explore use active learning, specifically selective sampling,
lexicon acquisition, demonstrate active learning, fewer examples required
achieve accuracy obtained training randomly chosen examples.
basic algorithm selective sampling relatively simple. Learning begins
small pool annotated examples large pool unannotated examples, learner
attempts choose informative additional examples annotation. Existing work
area emphasized two approaches, certainty-based methods (Lewis & Catlett,
1994), committee-based methods (McCallum & Nigam, 1998; Freund, Seung, Shamir,
& Tishby, 1997; Liere & Tadepalli, 1997; Dagan & Engelson, 1995; Cohn et al., 1994);
focus former.
certainty-based paradigm, system trained small number annotated
examples learn initial classifier. Next, system examines unannotated examples,
attaches certainties predicted annotation examples. k examples
lowest certainties presented user annotation retraining. Many
methods attaching certainties used, typically attempt estimate
probability classifier consistent prior training data classify new
example correctly.

31

fiThompson & Mooney

Learn lexicon examples annotated far
1) phrase unannotated sentence:
entries learned lexicon
certainty average heuristic values entries
Else, one-word phrase
certainty zero
2) rank sentences use:
Total certainty phrases step 1
# phrases counted step 1

Figure 21: Active Learning Wolfie
Figure 20 presents abstract pseudocode certainty-based selective sampling.
ideal situation, batch size, k, would set one make intelligent decisions
future choices, efficiency reasons retraining batch learning algorithms,
frequently set higher. Results number classification tasks demonstrated
general approach effective reducing need labeled examples (see citations
above).
Applying certainty-based sample selection Wolfie requires determining certainty
complete annotation potential new training example, despite fact individual
learned lexical entries parsing operators perform part overall annotation task.
Therefore, general approach compute certainties pieces example,
case, phrases, combine obtain overall certainty example. Since lexicon
entries contain explicit uncertainty parameters, used Wolfies heuristic measure
estimate uncertainty.
choose sentences annotated round, first bootstrapped initial
lexicon small corpus, keeping track heuristic values learned items.
Then, unannotated sentence, took average heuristic values
lexicon entries learned phrases sentence, giving value zero unknown
words eliminating consideration words assume known advance,
database constants. Thus, longer sentences known phrases would
lower certainty shorter sentences number known phrases;
desirable since longer sentences informative lexicon learning point
view. sentences lowest values chosen annotation, added
bootstrap corpus, new lexicon learned. technique summarized Figure 21.
evaluate technique, compared active learning learning randomly selected examples, measuring effectiveness learned lexicons background knowledge Chill. used (smaller) U.S. Geography corpus, original
Wolfie tests, using lexicons background knowledge parser acquisition (and
using examples parser acquisition).
trial following experiments, first randomly divide data
training test set. Then, n = 25 bootstrap examples randomly selected

32

fiAcquiring Word-Meaning Mappings

100

80

Accuracy

60

40
WOLF+active
WOLFIE
Geobase
20

0
0

50

100
150
Training Examples

200

250

Figure 22: Using Lexicon Certainty Active Learning
training examples step active learning, least certain k = 10 examples
remaining training examples selected added training set. result
learning set evaluated step. accuracy resulting learned
parsers compared accuracy learned using randomly chosen examples
learn lexicons parsers, Section 5; words, think k examples
round chosen randomly.
Figure 22 shows accuracy unseen data parsers learned using lexicons learned
Wolfie examples chosen randomly actively. annotation
savings around 50 examples using active learning: maximum accuracy reached
175 examples, versus 225 random examples. advantage using active
learning clear beginning, though differences two curves
statistically significant 175 training examples. Since learning lexicons
parsers, choosing examples based Wolfies certainty measures, boost could
improved even Chill say examples chosen. See Thompson, Califf,
Mooney (1999) description active learning Chill.

7. Related Work
section, divide previous research related topics areas lexicon
acquisition active learning.
7.1 Lexicon Acquisition
Work automated lexicon language acquisition dates back Siklossy (1972),
demonstrated system learned transformation patterns logic back natural

33

fiThompson & Mooney

language. already noted, closely related work Jeff Siskind,
described briefly Section 2 whose system ran comparisons Section 5.
definition learning problem compared mapping problem (Siskind,
1993). formulation differs several respects. First, sentence representations terms instead trees. However, shown Figure 7, terms
represented trees conform formalism minor additions. Next,
notion interpretation involve type tree, carries entire representation
sentence root. Also, clear would handle quantified variables
representation sentences. Skolemization possible, generalization across
sentences would require special handling. make single-use assumption
not. Another difference bias towards minimal number lexicon entries,
attempts find monosemous lexicon. later work (Siskind, 2000) relaxes allow
ambiguity noise, still biases towards minimizing ambiguity. However, formal
definition explicitly allow lexical ambiguity, handles heuristic manner.
This, though, may lead robustness method face noise. Finally,
definition allows phrasal lexicon entries.
Siskinds work topic explored many different variations along continuum
using many constraints requiring time incorporate new example (Siskind,
1993), versus constraints requiring training data (Siskind, 1996). Thus, perhaps earlier systems would able learn lexicons Section 5
quickly; crucially systems allow lexical ambiguity, thus may
learned accurate lexicon. detailed comparisons versions
system outside scope paper. goal Wolfie learn possibly
ambiguous lexicon examples possible, thus made comparisons along
dimension alone.
Siskinds approach, ours, takes account constraints word meanings
justified exclusivity compositionality assumptions. approach
somewhat general handles noise referential uncertainty (uncertainty
meaning sentence thus multiple possible candidates),
specialized applications meaning (or meanings) known. experimental
results Section 5 demonstrate advantage method application.
demonstrated system capable learning reasonably accurate lexicons large,
ambiguous, noisy artificial corpora, accuracy assured learning
algorithm converges, occur smaller corpus experiments ran.
Also, already noted, system operates incremental on-line fashion, discarding
sentence processes it, batch. addition, search word meanings
proceeds two stages, discussed Section 2.2. using common substructures,
combine two stages Wolfie. systems greedy aspects,
choice next best lexical entry, choice discard utterances noise create
homonymous lexical entry. Finally, system compute statistical correlations
words possible meanings, does.
Besides Siskinds work, others approach problem cognitive
perspective. example, De Marcken (1994) uses child language learning motivation, approaches segmentation problem instead learning semantics.
training input, uses flat list tokens semantic representations,
34

fiAcquiring Word-Meaning Mappings

segment sentences words. uses variant expectation-maximization (Dempster,
Laird, & Rubin, 1977), together form parsing dictionary matching techniques,
segment sentences associate segments likely meaning.
Childes corpus, algorithm achieves high precision, recall provided.
Others taking cognitive approach demonstrate language understanding ability
carry task parsing. example, Nenov Dyer (1994) describe
neural network model map visual verbal-motor commands, Colunga
Gasser (1998) use neural network modeling techniques learning spatial concepts.
Feldman colleagues Berkeley (Feldman, Lakoff, & Shastri, 1995) actively
pursuing cognitive models acquisition semantic concepts. Another Berkeley effort,
system Regier (1996) given examples pictures paired natural language
descriptions apply picture, learns judge whether new sentence true
given picture.
Similar work Suppes, Liang, Bottner (1991) uses robots demonstrate lexicon learning. robot trained cognitive perceptual concepts associated
actions, learns execute simple commands. Along similar lines, Tishby Gorin
(1994) system learns associations words actions, use
statistical framework learn associations, handle structured representations. Similarly, Oates, Eyler-Walker, Cohen (1999) discuss acquisition lexical
hierarchies associated meaning defined sensory environment robot.
problem automatic construction translation lexicons (Smadja, McKeown, &
Hatzivassiloglou, 1996; Melamed, 1995; Wu & Xia, 1995; Kumano & Hirakawa, 1994; Catizone, Russell, & Warwick, 1993; Gale & Church, 1991; Brown & et al., 1990) definition
similar own. methods compute association scores
pairs (in case, word-word pairs) use greedy algorithm choose best translation(s) word, take advantage constraints pairs. One
exception Melamed (2000); however, approach allow phrases lexicon synonymy within one text segment, does. Also, Yamazaki, Pazzani,
Merz (1995) learn translation rules semantic hierarchies parsed parallel
sentences Japanese English. course, main difference body
work paper map words semantic structures, words.
mentioned introduction, large body work learning lexical
semantics using different problem formulations own. example, Collins
Singer (1999), Riloff Jones (1999), Roark Charniak (1998), Schneider (1998)
define semantic lexicons grouping words semantic categories, latter
case, add relational information. result typically applied semantic lexicon
information extraction entity tagging. Pedersen Chen (1995) describe method
acquiring syntactic semantic features unknown word, assuming access
initial concept hierarchy, give experimental results. Many systems (Fukumoto &
Tsujii, 1995; Haruno, 1995; Johnston, Boguraev, & Pustejovsky, 1995; Webster & Marcus,
1995) focus acquisition verbs nouns, rather types words. Also,
authors named either experimentally evaluate systems, show
usefulness learned lexicons specific application.
Several authors (Rooth, Riezler, Prescher, Carroll, & Beil, 1999; Collins, 1997; Ribas,
1994; Manning, 1993; Resnik, 1993; Brent, 1991) discuss acquisition subcategoriza35

fiThompson & Mooney

tion information verbs, others describe work learning selectional restrictions
(Manning, 1993; Brent, 1991). different information required
mapping semantic representation, could useful source information
constrain search. Li (1998) expands subcategorization work
inducing clustering information. Finally, several systems (Knight, 1996; Hastings, 1996;
Russell, 1993) learn new words context, assuming large initial lexicon
parsing system already available.
Another related body work grammar acquisition, especially areas tightly
integrate grammar lexicon, Categorial Grammars (Retore & Bonato,
2001; Dudau-Sofronie, Tellier, & Tommasi, 2001; Watkinson & Manandhar, 1999).
theory Categorial Grammar ties lexical semantics, semantics
often used inference support high-level tasks database
retrieval. learning syntax semantics together arguably difficult task,
aforementioned work evaluated large corpora, presumably primarily
due difficulty annotation.
7.2 Active Learning
respect additional active learning techniques, Cohn et al. (1994) among
first discuss certainty-based active learning methods detail. focus neural
network approach active learning version-space concepts.
researchers applying machine learning natural language processing utilized active
learning (Hwa, 2001; Schohn & Cohn, 2000; Tong & Koller, 2000; Thompson et al., 1999;
Argamon-Engelson & Dagan, 1999; Liere & Tadepalli, 1997; Lewis & Catlett, 1994),
majority addressed classification tasks part speech tagging
text categorization. example, Liere Tadepalli (1997) apply active learning
committees problem text categorization. show improvements
active learning similar obtain, use committee Winnow-based
learners traditional classification task. Argamon-Engelson Dagan (1999)
apply committee-based learning part-of-speech tagging. work, committee
hidden Markov models used select examples annotation. Lewis Catlett (1994)
use heterogeneous certainty-based methods, simple classifier used select
examples annotated presented powerful classifier.
However, many language learning tasks require annotating natural language text
complex output, parse tree, semantic representation, filled template.
application active learning tasks requiring complex outputs well
studied, exceptions Hwa (2001), Soderland (1999), Thompson et al. (1999).
latter two include work active learning applied information extraction, Thompson
et al. (1999) includes work active learning semantic parsing. Hwa (2001) describes
interesting method evaluating statistical parsers uncertainty, applied
syntactic parsing.

8. Future Work
Although Wolfies current greedy search method performed quite well, better search
heuristic alternative search strategy could result improvements.
36

fiAcquiring Word-Meaning Mappings

thoroughly evaluate Wolfies ability learn long phrases, restricted ability
evaluations here. Another issue robustness face noise. current algorithm
guaranteed learn correct lexicon even noise-free corpus. addition noise
complicates analysis circumstances mistakes likely happen.
theoretical empirical analysis issues warranted.
Referential uncertainty could handled, increase complexity, forming
LICS pairs representations phrase appears,
alternative representations sentence. Then, pair added lexicon,
sentence containing word, representations eliminated
contain learned meaning, provided another representation contain (thus allowing
lexical ambiguity). plan flesh evaluate results.
different avenue exploration apply Wolfie corpus sentences paired
common query language, SQL. corpora easily constructible
recording queries submitted existing SQL applications along English forms,
translating existing lists SQL queries English (presumably easier direction
translate). fact training data used learn semantic
lexicon parser helps limit overall burden constructing complete natural
language interface.
respect active learning, experiments additional corpora needed test
ability approach reduce annotation costs variety domains. would
interesting explore active learning natural language processing problems
syntactic parsing, word-sense disambiguation, machine translation.
current results involved certainty-based approach; however, proponents
committee-based approaches convincing arguments theoretical advantages.
initial attempts adapting committee-based approaches systems
successful; however, additional research topic indicated. One critical problem
obtaining diverse committees properly sample version space (Cohn et al., 1994).

9. Conclusions
Acquiring semantic lexicon corpus sentences labeled representations
meaning important problem widely studied. present
formalism learning problem greedy algorithm find approximate solution
it. Wolfie demonstrates fairly simple, greedy, symbolic learning algorithm performs
well task obtains performance superior previous lexicon acquisition system
corpus geography queries. results demonstrate methods extend
variety natural languages besides English, scale fairly well larger,
difficult corpora.
Active learning new area machine learning almost exclusively
applied classification tasks. demonstrated successful application
complex natural language mappings phrases semantic meanings, supporting
acquisition lexicons parsers. wealth unannotated natural language data,
along difficulty annotating data, make selective sampling potentially
invaluable technique natural language learning. results realistic corpora indicate
example annotations savings high 22% achieved employing active

37

fiThompson & Mooney

sample selection using simple certainty measures predictions unannotated data.
Improved sample selection methods applications important language problems
hold promise continued progress using machine learning construct effective
natural language processing systems.
experiments corpus-based natural language presented results
subtask natural language, results whether learned subsystems
successfully integrated build complete NLP system. experiments presented
paper demonstrated two learning systems, Wolfie Chill, successfully
integrated learn complete NLP system parsing database queries executable
logical form given single corpus annotated queries, demonstrated
potential active learning reduce annotation effort learning NLP.

Acknowledgments
would thank Jeff Siskind providing us software, help
adapting use corpus. Thanks Agapito Sustaita, Esra Erdem,
Marshall Mayberry translation efforts, three anonymous reviewers
comments helped improve paper. research supported
National Science Foundation grants IRI-9310819 IRI-9704943.

References
Anderson, J. R. (1977). Induction augmented transition networks. Cognitive Science, 1,
125157.
Angluin, D. (1988). Queries concept learning. Machine Learning, 2, 319342.
Argamon-Engelson, S., & Dagan, I. (1999). Committee-based sample selection probabilistic classifiers. Journal Artificial Intelligence Research, 11, 335360.
Beckwith, R., Fellbaum, C., Gross, D., & Miller, G. (1991). WordNet: lexical database
organized psycholinguistic principles. Zernik, U. (Ed.), Lexical Acquisition:
Exploiting On-Line Resources Build Lexicon, pp. 211232. Lawrence Erlbaum,
Hillsdale, NJ.
Borland International (1988). Turbo Prolog 2.0 Reference Guide. Borland International,
Scotts Valley, CA.
Brent, M. (1991). Automatic acquisition subcategorization frames untagged text.
Proceedings 29th Annual Meeting Association Computational Linguistics (ACL-91), pp. 209214.
Brown, P., & et al. (1990). statistical approach machine translation. Computational
Linguistics, 16 (2), 7985.
Catizone, R., Russell, G., & Warwick, S. (1993). Deriving translation data bilingual
texts. Proceedings First International Lexical Acquisition Workshop.
38

fiAcquiring Word-Meaning Mappings

Cohn, D., Atlas, L., & Ladner, R. (1994). Improving generalization active learning.
Machine Learning, 15 (2), 201221.
Collins, M., & Singer, Y. (1999). Unsupervised models named entity classification.
Proceedings Conference Empirical Methods Natural Language Processing
Large Corpora (EMNLP/VLC-99) University Maryland.
Collins, M. J. (1997). Three generative, lexicalised models statistical parsing. Proceedings 35th Annual Meeting Association Computational Linguistics
(ACL-97), pp. 1623.
Colunga, E., & Gasser, M. (1998). Linguistic relativity word acquisition: computational approach. Proceedings Twenty First Annual Conference
Cognitive Science Society, pp. 244249.
Dagan, I., & Engelson, S. P. (1995). Committee-based sampling training probabilistic classifiers. Proceedings Twelfth International Conference Machine
Learning (ICML-95), pp. 150157 San Francisco, CA. Morgan Kaufman.
De Marcken, C. (1994). acquisition lexicon paired phoneme sequences
semantic representations. Lecture Notes Computer Science, Vol. 862, pp. 6677.
Springer-Verlag.
Dempster, A., Laird, N., & Rubin, D. (1977). Maximum likelihood incomplete data
via EM algorithm. Journal Royal Statistical Society B, 39, 138.
Dudau-Sofronie, Tellier, & Tommasi (2001). Learning categorial grammars semantic
types. Proceedings 13th Amsterdam Colloquium, pp. 7984.
Feldman, J., Lakoff, G., & Shastri, L. (1995). neural theory language project
http://www.icsi.berkeley.edu/ntl. International Computer Science Institute, University
California, Berkeley, CA.
Fillmore, C. (1968). case case. Bach, E., & Harms, R. T. (Eds.), Universals
Linguistic Theory. Holt, Reinhart Winston, New York.
Fillmore, C. (1988). mechanisms Construction Grammar. Axmaker, S., Jaisser,
A., & Singmeister, H. (Eds.), Proceedings Fourteenth Annual Meeting
Berkeley Linguistics Society, pp. 3555 Berkeley, CA.
Fisher, D. H. (1987). Knowledge acquisition via incremental conceptual clustering. Machine
Learning, 2, 139172.
Freund, Y., Seung, H. S., Shamir, E., & Tishby, N. (1997). Selective sampling using
query committee algorithm. Machine Learning, 28, 133168.
Fukumoto, F., & Tsujii, J. (1995). Representation acquisition verbal polysemy.
Papers 1995 AAAI Symposium Representation Acquisition
Lexical Knowledge: Polysemy, Ambiguity, Generativity, pp. 3944 Stanford, CA.

39

fiThompson & Mooney

Gale, W., & Church, K. (1991). Identifying word correspondences parallel texts.
Proceedings Fourth DARPA Speech Natural Language Workshop.
Garey, M., & Johnson, D. (1979). Computers Intractability: Guide Theory
NP-Completeness. Freeman, New York, NY.
Goldberg, A. (1995). Constructions: Construction Grammar Approach Argument
Structure. University Chicago Press.
Grefenstette, G. (1994). Sextant: Extracting semantics raw text, implementation
details. Integrated Computer-Aided Engineering, 6 (4).
Haas, J., & Jayaraman, B. (1997). context-free definite-clause grammars: typetheoretic approach. Journal Logic Programming, 30 (1), 123.
Haruno, M. (1995). case frame learning method Japanese polysemous verbs.
Papers 1995 AAAI Symposium Representation Acquisition
Lexical Knowledge: Polysemy, Ambiguity, Generativity, pp. 4550 Stanford, CA.
Hastings, P. (1996). Implications automatic lexical acquisition mechanism.
Wermter, S., Riloff, E., & Scheler, C. (Eds.), Connectionist, Statistical, Symbolic Approaches Learning natural language processing. Springer-Verlag, Berlin.
Hwa, R. (2001). minimizing training corpus parser acquisition. Proceedings
Fifth Computational Natural Language Learning Workshop.
Jackendoff, R. (1990). Semantic Structures. MIT Press, Cambridge, MA.
Johnston, M., Boguraev, B., & Pustejovsky, J. (1995). acquisition interpretation
complex nominals. Papers 1995 AAAI Symposium Representation
Acquisition Lexical Knowledge: Polysemy, Ambiguity, Generativity, pp.
6974 Stanford, CA.
Knight, K. (1996). Learning word meanings instruction. Proceedings Thirteenth
National Conference Artificial Intelligence (AAAI-96), pp. 447454 Portland, Or.
Kohavi, R., & John, G. (1995). Automatic parameter selection minimizing estimated
error. Proceedings Twelfth International Conference Machine Learning
(ICML-95), pp. 304312 Tahoe City, CA.
Kumano, A., & Hirakawa, H. (1994). Building MT dictionary parallel texts based
linguistic statistical information. Proceedings Fifteenth International
Conference Computational Linguistics, pp. 7681.
Lavrac, N., & Dzeroski, S. (1994). Inductive Logic Programming: Techniques Applications. Ellis Horwood.
Lewis, D. D., & Catlett, J. (1994). Heterogeneous uncertainty sampling supervised
learning. Proceedings Eleventh International Conference Machine Learning (ICML-94), pp. 148156 San Francisco, CA. Morgan Kaufman.
40

fiAcquiring Word-Meaning Mappings

Li, H. (1998). probabilistic approach lexical semantic knowledge acquisition structural disambiguation. Ph.D. thesis, University Tokyo.
Liere, R., & Tadepalli, P. (1997). Active learning committees text categorization.
Proceedings Fourteenth National Conference Artificial Intelligence (AAAI97), pp. 591596 Providence, RI.
Manning, C. D. (1993). Automatic acquisition large subcategorization dictionary
corpora. Proceedings 31st Annual Meeting Association Computational Linguistics (ACL-93), pp. 235242 Columbus, OH.
McCallum, A. K., & Nigam, K. (1998). Employing EM pool-based active learning
text classification. Proceedings Fifteenth International Conference
Machine Learning (ICML-98), pp. 350358 Madison, WI. Morgan Kaufman.
Melamed, I. D. (1995). Automatic evaluation uniform filter cascades inducing n-best
translation lexicons. Proceedings Third Workshop Large Corpora.
Melamed, I. D. (2000). Models translational equivalence among words. Computational
Linguistics, 26 (2), 221249.
Muggleton, S. (Ed.). (1992). Inductive Logic Programming. Academic Press, New York,
NY.
Muggleton, S. (1995). Inverse entailment Progol. New Generation Computing Journal,
13, 245286.
Muggleton, S., & Feng, C. (1990). Efficient induction logic programs. Proceedings
First Conference Algorithmic Learning Theory Tokyo, Japan. Ohmsha.
Nenov, V. I., & Dyer, M. G. (1994). Perceptually grounded language learning: Part 2
DETE: neural/procedural model. Connection Science, 6 (1), 341.
Oates, T., Eyler-Walker, Z., & Cohen, P. (1999). Using syntax learn semantics:
experiment language acquisition mobile robot. Tech. rep. 99-35, University
Massachusetts, Computer Science Department.
Partee, B., Meulen, A., & Wall, R. (1990). Mathematical Methods Linguistics. Kluwer
Academic Publishers, Dordrecht, Netherlands.
Pedersen, T., & Chen, W. (1995). Lexical acquisition via constraint solving. Papers
1995 AAAI Symposium Representation Acquisition Lexical
Knowledge: Polysemy, Ambiguity, Generativity, pp. 118122 Stanford, CA.
Plotkin, G. D. (1970). note inductive generalization. Meltzer, B., & Michie, D.
(Eds.), Machine Intelligence (Vol. 5). Elsevier North-Holland, New York.
Rayner, M., Hugosson, A., & Hagert, G. (1988). Using logic grammar learn lexicon.
Tech. rep. R88001, Swedish Institute Computer Science.

41

fiThompson & Mooney

Regier, T. (1996). human semantic potential: spatial language constrained connectionism. MIT Press.
Resnik, P. (1993). Selection information: class-based approach lexical relationships.
Ph.D. thesis, University Pennsylvania, CIS Department.
Retore, C., & Bonato, R. (2001). Learning rigid lambek grammars minimalist grammars
structured sentences. Proceedings Third Learning Language Logic
Workshop Strasbourg, France.
Ribas, F. (1994). experiment learning appropriate selectional restrictions
parsed corpus. Proceedings Fifteenth International Conference Computational Linguistics, pp. 769774.
Riloff, E., & Jones, R. (1999). Learning dictionaries information extraction multilevel bootstrapping. Proceedings Sixteenth National Conference Artificial
Intelligence (AAAI-99), pp. 10441049 Orlando, FL.
Roark, B., & Charniak, E. (1998). Noun-phrase co-occurrence statistics semi-automatic
semantic lexicon construction. Proceedings 36th Annual Meeting
Association Computational Linguistics COLING-98 (ACL/COLING-98), pp.
11101116.
Rooth, M., Riezler, S., Prescher, D., Carroll, G., & Beil, F. (1999). Inducing semantically
annotated lexicon via EM-based clustering. Proceedings 37th Annual Meeting
Association Computational Linguistics, pp. 104111.
Russell, D. (1993). Language Acquisition Unification-Based Grammar Processing System Using Real World Knowledge Base. Ph.D. thesis, University Illinois, Urbana,
IL.
Schank, R. C. (1975). Conceptual Information Processing. North-Holland, Oxford.
Schneider, R. (1998). lexically-intensive algorithm domain-specific knowledge acquisition. Proceedings Joint Conference New Methods Language Processing
Computational Natural Language Learning, pp. 1928.
Schohn, G., & Cohn, D. (2000). Less more: Active learning support vector machines. Proceedings Seventeenth International Conference Machine Learning (ICML-2000), pp. 839846 Stanford, CA.
Sebillot, P., Bouillon, P., & Fabre, C. (2000). Inductive logic programming corpus-based
acquisition semantic lexicons. Proceedings 2nd Learning Language Logic
(LLL) Workshop Lisbon, Portugal.
Seung, H. S., Opper, M., & Sompolinsky, H. (1992). Query committee. Proceedings
ACM Workshop Computational Learning Theory Pittsburgh, PA.
Siklossy, L. (1972). Natural language learning computer. Simon, H. A., & Siklossy,
L. (Eds.), Representation meaning: Experiments Information Processsing
Systems. Prentice Hall, Englewood Cliffs, NJ.
42

fiAcquiring Word-Meaning Mappings

Siskind, J. M. (2000). Learning word-to-meaning mappings. Broeder, P., & Murre, J.
(Eds.), Models Language Acquisition: Inductive Deductive Approaches. Oxford
University Press.
Siskind, J. M. (1992). Naive Physics, Event Perception, Lexical Semantics Language
Acquisition. Ph.D. thesis, Department Electrical Engineering Computer Science, Massachusetts Institute Technology, Cambridge, MA.
Siskind, J. M. (1996). computational study cross-situational techniques learning
word-to-meaning mappings. Cognition, 61 (1), 3991.
Siskind, J. M. (1993). Lexical acquisition constraint satisfaction. Tech. rep. IRCS-93-41,
University Pennsylvania.
Smadja, F., McKeown, K. R., & Hatzivassiloglou, V. (1996). Translating collocations
bilingual lexicons: statistical approach. Computational Linguistics, 22 (1), 138.
Soderland, S. (1999). Learning information extraction rules semi-structured free
text. Machine Learning, 34, 233272.
Suppes, P., Liang, L., & Bottner, M. (1991). Complexity issues robotic machine learning
natural language. Lam, L., & Naroditsky, V. (Eds.), Modeling Complex Phenomena, Proceedings 3rd Woodward Conference, pp. 102127. Springer-Verlag.
Thompson, C. A., Califf, M. E., & Mooney, R. J. (1999). Active learning natural language
parsing information extraction. Proceedings Sixteenth International
Conference Machine Learning (ICML-99), pp. 406414 Bled, Slovenia.
Thompson, C. A. (1995). Acquisition lexicon semantic representations sentences.
Proceedings 33rd Annual Meeting Association Computational Linguistics (ACL-95), pp. 335337 Cambridge, MA.
Tishby, N., & Gorin, A. (1994). Algebraic learning statistical associations language
acquisition. Computer Speech Language, 8, 5178.
Tomita, M. (1986). Efficient Parsing Natural Language. Kluwer Academic Publishers,
Boston.
Tong, S., & Koller, D. (2000). Support vector machine active learning applications
text classification. Proceedings Seventeenth International Conference
Machine Learning (ICML-2000), pp. 9991006 Stanford, CA.
Watkinson, S., & Manandhar, S. (1999). Unsupervised lexical learning categorial
grammars using lll corpus. Learning Language Logic (LLL) Workshop Bled,
Slovenia.
Webster, M., & Marcus, M. (1995). Automatic acquisition lexical semantics verbs
sentence frames. Proceedings 27th Annual Meeting Association
Computational Linguistics (ACL-89), pp. 177184.

43

fiThompson & Mooney

Wu, D., & Xia, X. (1995). Large-scale automatic extraction English-Chinese translation lexicon. Machine Translation, 9 (3-4), 285313.
Yamazaki, T., Pazzani, M., & Merz, C. (1995). Learning hierarchies ambiguous natural
language data. Proceedings Twelfth International Conference Machine
Learning (ICML-95), pp. 575583 San Francisco, CA. Morgan Kaufmann.
Zelle, J. M. (1995). Using Inductive Logic Programming Automate Construction
Natural Language Parsers. Ph.D. thesis, Department Computer Sciences, University Texas, Austin, TX. appears Artificial Intelligence Laboratory Technical
Report AI 96-249.
Zelle, J. M., & Mooney, R. J. (1996). Learning parse database queries using inductive
logic programming. Proceedings Thirteenth National Conference Artificial
Intelligence (AAAI-96), pp. 10501055 Portland, OR.
Zipf, G. (1949). Human behavior principle least effort. Addison-Wesley, New
York, NY.

44


