Journal Artificial Intelligence Research 18 (2003) 83-116

Submitted 07/02; published 1/03

Learning Order BDD Variables Verification
Orna Grumberg
Shlomi Livne
Shaul Markovitch

orna@cs.technion.ac.il
slivne@cs.technion.ac.il
shaulm@cs.technion.ac.il

Computer Science Department
Technion - Israel Institute Technology
Haifa 32000, Israel

Abstract
size complexity software hardware systems significantly increased
past years. result, harder guarantee correct behavior. One
successful methods automated verification finite-state systems model
checking. current model-checking systems use binary decision diagrams (BDDs)
representation tested model verification process properties.
Generally, BDDs allow canonical compact representation boolean function (given
order variables). compact BDD is, better performance one gets
verifier. However, finding optimal order BDD NP-complete problem.
Therefore, several heuristic methods based expert knowledge developed
variable ordering.
propose alternative approach variable ordering algorithm gains
ordering experience training models uses learned knowledge finding
good orders. methodology based offline learning pair precedence classifiers
training models, is, learning variable pair permutation likely
lead good order. training model, number training sequences evaluated.
Every training model variable pair permutation tagged based performance
evaluated orders. tagged permutations passed feature extractor
given examples classifier creation algorithm. Given model
order requested, ordering algorithm consults precedence classifier constructs
pair precedence table used create order.
algorithm integrated SMV, one widely used verification systems. Preliminary empirical evaluation methodology, using real benchmark
models, shows performance better random ordering competitive
existing algorithms use expert knowledge. believe sub-domains models
(alu, caches, etc.) system prove even valuable. features
ability learn sub-domain knowledge, something ordering algorithm does.

1. Introduction
size complexity software hardware systems significantly increased
past years. result, harder guarantee correct behavior. Thus, formal
methods, preferably computerized, needed task.
One successful methods automated verification finite-state systems
temporal logic model checking (Clarke, Emerson, & Sistla, 1986; Queille & Sifakis, 1981).
Temporal logics suitable formalisms describing behavior program time.
model checking procedure receives finite-state model system specification
c
2003
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiGrumberg, Livne, & Markovitch

written temporal logic formula. returns yes model satisfies formula
(meaning system behaves according specification). Otherwise, returns
no, along counter example demonstrates bad behavior.
Model checking successful finding subtle errors various systems.
currently recognized hardware industry important component
development phase new designs. However, model checking procedures often suffer
high space requirements, needed holding transition relation intermediate
results.
One promising solutions problem use binary decision diagrams (BDDs) (Akers, 1978; Bryant, 1986) basic data structure model checking.
BDDs canonical representations boolean functions often concise size.
conciseness yields efficiency computation time. Since straightforward
represent transition relation intermediate results boolean functions, BDDs
particularly suitable model checking. Today, existing industrial BDD-based verifiers, IBMs RuleBase (Beer, Ben-David, Eisner, & Landver, 1996) Motorolas
Verdict (Kaufmann & Pixley, 1997) used many companies development
infrastructure.
size BDD given function sensitive ordering variables
BDD. However, finding optimal ordering, yields smallest BDD given
function, NP-complete problem (Bollig & Wegener, 1996). Therefore, several heuristic
algorithms based expert knowledge developed variable ordering
hope reducing BDD size. Unfortunately, spite resources invested,
algorithms produce good enough variable orders. reason may
general rules used domain-specific knowledge exploited.
goal research develop learning techniques acquiring using
domain-specific knowledge variable ordering. assume availability one
training models. training models used off-line acquisition ordering experience
used ordering variables previously unseen model.
first present method converting ordering learning task concept
learning problem. concept set ordered variable pairs right
order. examples ordered pairs variables given training model. show
statistical method tagging examples based evaluated training orders present
set variable-pair features. result standard concept learning problem.
apply decision tree learning generate decision tree training model. used
unseen model, combine trees generate partial order used
generating required order. present extension algorithm learns
context-based precedence relations.
algorithm integrated SMV (McMillan, 1993), backbone
many verification systems. Empirical evaluation methodology, using real benchmark
models hardware designs, shows performance much better random ordering
competitive existing algorithms use expert knowledge.
Section 2 contains background model checking. Section 3 presents main algorithm
empirical evaluation. Section 4 shows context-based algorithm. conclusions
presented Section 5.
84

fiLearning Order BDD Variables Verification

2. Background
Model checking introduced Clarke Emerson (1986) Queille Sifakis
(1981) early 1980s. presented algorithms automatically reason
temporal properties finite state systems exploring state space. use binary
decision diagrams (BDDs) represent finite state systems perform symbolic state
traversal called symbolic model checking. use BDDs greatly extended
capacity model checkers. Models 2 100 states routinely verified.
BDDs introduced Akers (1978) compact representations boolean functions. Bryant (1986) proposed ordered binary decision diagrams (OBDDs) canonical
representations boolean functions. showed algorithms computing boolean
operations efficiently OBDDs.
following subsection gives overview finite state systems represented
symbolic model checking. BDDs described variable ordering problem
defined. Existing algorithms static variable ordering algorithms reviewed. Finally,
brief description machine learning algorithms used ordering given.
2.1 Finite State Machines Symbolic Model Checking
Finite state systems (FSM) described defining set possible states
system transition relation states. state typically describes values
components (e.g., latches digital circuits), component represented
state variable. Let V = {v0 , v1 , ...vn1 } set variables system. Let K vi
set possible values variable v . state system described
assigning values variables V . set possible states
SA = Kv0 Kv1 .... Kvn1 .
state written using function true state:
Vn1
i=0

(vi == cj ),

cj Kvi value vi state. set states described function
disjunction functions represent states.
Figure 1 shows 3-bit counter. state 3-bit counter described
tuple gives assignment 3 variables v 2 , v1 , v0 . example, tuple h1, 0, 0i
represents state v2 = 1, v1 = 0, v0 = 0. corresponding boolean expression
state (v2 == 1) (v1 == 0) (v0 == 0).
order describe system, need specify transition relation.
transition relation describes possible transitions system state. thus
described pairs states, hpresent state, next statei, next state system state
transition present state. variables V represent present state
variables, variable vi V define corresponding next state variable
vi0 V 0 . V 0 denote set next state variables.
example valid transition 3-bit counter h0, 0, 0i h0, 0, 1i.
boolean function represents transition (v 2 == 0) (v1 == 0) (v0 ==
0) (v20 == 0) (v10 == 0) (v00 == 1). transition relation represented
85

fiGrumberg, Livne, & Markovitch

V2

V1

V0

Figure 1: 3 bit counter
Present State
v2 v1
v0
0
0
0
0
0
1
0
1
0
0
1
1
1
0
0
1
0
1
1
1
0
1
1
1

Next State
v20 v10 v00
0
0
1
0
1
0
0
1
1
1
0
0
1
0
1
1
1
0
1
1
1
0
0
0

Table 1: 3-bit counter transition relation table
boolean function disjunction boolean functions transitions.
Table 1 shows transition relation 3-bit counter.
alternative method describing transition relation state variable
define valid next states. form known partitioned transition relation.
transition relation described set functions (instead one), one
variable. variable vi , boolean function Ti (V, vi0 ) defines next value vi , vi0 , given
current state system V .
synchronous systems, simultaneous transition system
components, transition relation

Vn1
i=0

Ti (V, vi0 ).

model checking common use partitioned transition relation form representation, since usually compact memory requirements thus allows handling
larger systems. 3-bit counter, next state boolean functions given below,
stands boolean operator Xor.

T0 (V, v00 ) : (v00 == v0 )
T1 (V, v10 ) : (v10 == (v0 v1 ))
T2 (V, v20 ) : (v20 == (v2 (v0 v1 ))).
86

fiLearning Order BDD Variables Verification

2.2 Binary Decision Diagrams
binary decision diagram (BDD) DAG (directed acyclic graph) representation
boolean function. BDD composed two sink nodes several non-sink nodes.
two sink nodes, labeled 0 1, represent corresponding boolean values. non-sink
node labeled boolean variable v two outgoing edges labeled 1 (or then)
0 (or else). non-sink node represents boolean function corresponding 1
edge v = 1, boolean function corresponding 0 edge v = 0.
ordered binary decision diagram (OBDD) BDD constraint
variables ordered, every root-to-sink path OBDD visits variables
ascending order.
reduced ordered binary decision diagram (ROBDD) OBDD node
represents distinct logic function. representation canonical BDD representation
compact representation possible given boolean function variable
ordering.
V2

V2

V2

V2

V1

V1

V1
V0
V0

0

0

0
0
0

0

0

1

0
1

0

0

0

0

1

0

0
1

0

0
1

0

0

0

0

1

1

0

0

V1
V0
V0

0
0

0

0

1

0
1

0

0

1

(a)

(b)

V2

V2

T2

V2

T2

V2

V1
V1
V0
V0

T1

V1

1

V1
V0
V0

T0
1
1
0

1

1

0

0

0

1

1

0

0
1

0

0
0

1

1
0

1

1

0

0

T1
T0
1
0
0

(c)

1

1

1

1

0

0

0

(d)

Figure 2: 3-bit counter transition relation (a),(b) partitioned transition relation (c),(d)

Figure 2 (a),(b) shows OBDD ROBDD (respectively) representations
transition relation function 3-bit counter. dashed lines 0 edges
solid lines 1 edges. ROBDDs two leaf nodes, one 1 one
0. drew several times enhance readability. ROBDDs use complement
edges, produces even compact representation. use complement
edges, reasons readability. Figure 2 (c),(d) shows OBDD ROBDD
representations partitioned transition relation 3-bit counter. variable
order v2 , v20 , v1 , v10 , v0 , v00 used representations. Variable ordering algorithms
model checking place next state variable v i0 adjacent present state variable v .
87

fiGrumberg, Livne, & Markovitch

rest document refer ROBDDs BDDs (unless explicitly state
otherwise).
Bollig Wegener (1996) proved finding optimal variable ordering NPcomplete problem. order optimal yields BDD smallest number
nodes. Bryant (1986) pointed variable ordering greatly influences size
BDD. showed boolean function, one variable ordering may yield BDD
exponential number variables, different ordering may yield BDD
polynomial size.
v1

v1

v3

v2

v2

v3

v4

v4

1

0

1

(a)

0
(b)

Figure 3: ROBDDs function F (v1, v2, v3, v4) = (v1 = v3) (v2 = v4)
Figure 3 gives example effect variable ordering BDD size function F (v1, v2, v3, v4) = (v1 = v3) (v2 = v4). (a) variable ordering v1, v3, v2, v4
(b) variable ordering v1, v2, v3, v4.
Various algorithms developed variable ordering. Exact algorithms (Ishiura,
Sawada, & Yajima, 1991; Drechsler, Drechsler, & Slobodova, 1998; Friedman & Supowit,
1987) algorithms find optimal order. algorithms use method similar
dynamic programming pruning find optimal order. Due complexity
problem, exact algorithms practical small cases, one usually
turn heuristic methods. heuristic methods roughly divided two
groups.
1. Static Ordering (Aziz, Tasiran, & Brayton, 1994; Butler, Ross, & Rohit Kapur, 1991;
Chung, Hajj, & Patel, 1993; Fujii, Ootomo, & Hori, 1993; Jain, Adams, & Fujita, 1998;
Fujita, Fujisawa, & Kawato, 1988; Malik, Wang, Brayton, & Sangiovanni-Vincentelli,
1988; Touati, Savoj, Lin, Brayton, & Sangiovanni-Vincetelli, 1990) try find
good ordering constructing BDD. algorithms based
topological structure verified system.
2. Dynamic Ordering (Rudell, 1993; Meinel & Slobodova, 1998; Bollig, Lobbing, & Wegener, 1995; Meinel & Slobodova, 1997; Meinel, Somenzi, & Theobald, 1997; Ishiura
et al., 1991; Bern, Meinel, & Slobodova, 1995; Fujita, Kukimoto, & Brayton, 1995;
Mercer, Kapur, & Ross, 1992; Zhuang, Benten, & Cheung, 1996; Drechsler, Becker,
88

fiLearning Order BDD Variables Verification

& Gockel, 1996; Panda & Somenzi, 1995; Panda, Somenzi, & Plessier, 1994),
given BDD variable order, reorder variables hope finding
smaller BDD.
model checking procedures, variable ordering central component. initial
phase model checking, system translated BDD representation, Static
Ordering used. order built stage greatly influences memory usage
whole computation. However, since model checking keeps producing eliminating
BDDs, variable order changed dynamically order effect size
current BDDs. Dynamic Ordering used order achieve goal. applied
model checking procedure whenever size BDDs reaches certain threshold.
Since work introduces static ordering algorithm based machine learning,
next subsection presents review existing static algorithms. algorithms
developed combinational circuits (i.e., models whose outputs depend
current inputs inputs previous cycles) described hardware
terminology. order simplify description, describe terminology
used far.
2.3 Static Ordering
Static ordering algorithms try find initial good order BDD. so,
extract topological data model use data determine order.
algorithms convert model, described set next state functions, directed
graph known model connectivity graph. Vertices graph variables
boolean operations (gates). variable vertex represents variable, gate vertex
represents function. edges ni nj graph ni , either
variable gate vertex, nj , gate vertex. edge ni nj placed
function represented ni operand (i.e., immediate subfunction) function
represented nj . divide static algorithms four groups differ
way use graph information.
2.3.1 Graph Search Algorithms
method suggested Malik et al. (1988) assigns vertex level metric orders
variables decreasing level value. level vertices edges set
zero level every vertex (v ) set level(vi ) = maxvj |vi vj (level(vj )+1).
method resembles BFS (breadth first search) originates nodes
edges, progresses backwards model. Fujita et al. (1988) proposed executing
DFS (depth first search) vertices edges, progressing backwards.
Variables algorithm added post order form.
algorithms Malik et al. Fujita et al. designed cases one
function represented BDD. hardly ever case model checking.
Butler et al. (1991) adapted algorithm Fujita et al. models multiple starting
points (that is, multiple vertices edges). heuristic guides algorithm
select first vertex vertex represents function depends
maximum number variables. heuristic guides search advance (backwards)
89

fiGrumberg, Livne, & Markovitch

inner vertex vertex leads maximum number different variables.
tie breaking heuristic (Fujita, Fujisawa, & Matsunaga, 1993) enhanced algorithm
advises selecting (in case tie) vertex maximum number edges.
DFS-based methods append variables variable order. Another DFS-based
algorithm relies interleaving variables order (Fujii et al., 1993). algorithm
adds variable variable precedes DFS order.
2.3.2 Graph Evaluation Algorithms
Graph evaluation algorithms use model graph evaluate model variables
perform guided search based evaluation values. Minato et al. (1990) propagate
values backward graph, starting vertices edges, whose value
set 1. vertices boolean operations, values edges summed
value obtained divided equally edges. done recursively
vertex variable reached. variable vertices propagated values accumulated
variable evaluation value. order constructed iteratively adding variable
highest value, removing graph, updating values.
Chung et al.(1993) proposed two algorithm frameworks. first framework composed two sweeps. first sweep vertex assigned value. values set
propagating algorithm starts variable vertices edges advances
forward (by edges) vertices graph. second sweep guided
DFS initiated vertices edges executed. search executed backward
graph guided maximal value. means order traversal
among vertex ancestors according assigned value. number heuristics
compute values vertices proposed:
1. Level-Based sets value variables input edges zero. value
vertices set maximal vertex value inputs plus one.
2. Fanout-Based propagates two values graph (one boolean value).
boolean operation vertex values summed passed along. Rather,
computed according boolean operation vertex. initial values
variables input edges. value set number edges
variable has.
second framework proposed Chung et al., shortest distance
pair variables computed. total distance variable computed sum
distances variables. variable lowest total distance selected
first variable. next variable selected closest variable last ordered
variable. Ties broken according distance previous ordered variables.
graph evaluation algorithms try order variables variable
influences models next state functions first. algorithms differ
methodology use order variables. algorithms order
variables substantially influence models next-state functions placed higher
variable order (toward beginning order). algorithms place
variables according proximity previously ordered variables.
90

fiLearning Order BDD Variables Verification

2.3.3 Decomposition Algorithms
Decomposition algorithms break model parts. algorithms solve
two different problems. first finding good order part, second
finding order parts. order constructed combining solutions
two problems.
algorithm Malik et al. extended adapted finite-state machines (FSM)
Toutai et al. (1990). algorithm, model decomposed next state functions,
considered separately. Variables next state function ordered
according Malik et al. next state functions ordered cost function.
ordered functions many overlapping variables adjacent.
variable order obtained adding variables next state functions according
order parts, removing variables already exist.
algorithm Aziz et al. (1994) decomposes model different way. model
hierarchical composition constructed joining number internal parts
pass information among themselves. Usually, less communication among parts
within them. Variables internal part tend depend highly one another.
algorithm uses process communication graph (PCG), models hierarchical
structure model communication parts. PCG vertex
internal part, edge j connects vertex vertex j part j depends
bit part i. PCG parallel edges j, one bit value j depends
upon. Alternatively, edges could weighted.
Given order parts, upper bound BDD size model computed. computation based size parts amount communication
them. Heuristics guided upper bound applied order determine
order parts. order variables part decided one previous
ordering algorithms.
2.3.4 Sample-Based Algorithms
Sample-based static algorithms (Jain et al., 1998) real static algorithms
sense create order based information extracted model
description. Sample algorithms perform tests parts model (building transition
relations reachable states). part, number orders evaluated. good
orders merged create complete order model. Sampling algorithms use
traditional algorithms order find candidate orders parts. candidate
orders checked sampling algorithm.
2.3.5 Summary
majority graph search algorithms graph evaluation algorithms developed
problems adapted symbolic model checking. algorithms
developed context combinational circuits, others developed
simple case one function. symbolic model checking models rarely combinational
(their outputs almost always depend inputs previous cycles),
one function display. Adapting existing algorithms conform needs
91

fiGrumberg, Livne, & Markovitch

symbolic model checking various degrees success. adapted algorithms
heuristic apply simple rule logical reasoning behind it.
decomposition algorithms either heuristic provide theoretical upper bound.
However, bounds use rarely realistic; models require much smaller
BDDs. algorithms based decomposing model parts solving
ordering part using graph search algorithms. Thus, inherit drawbacks
algorithms.
Despite efforts invested many algorithms
developed static ordering, results yet satisfactory. produced BDDs
large manipulate, dynamic ordering must applied. One problem
approaches generality: utilize domain-specific knowledge.
Domain-specific knowledge essential solving majority complex problems.
difficult retrieve. next subsection discuss machine learning methods
acquiring domain-specific knowledge ordering tasks.
2.4 Learning Order Elements
Learning order elements done first trying induce partial order,
used generating total order. context, partial order usually called
preference predicate. Preference predicate induction based set tagged pairs
elements binary tag identifies preferred element. Broos Branting (1994)
present method inducing preference predicate using nearest neighbor classification.
distance untagged pair tagged pair computed sum
distances corresponding elements. closest tagged pair selected.
preferred element untagged pair one matching preferred element
tagged pair.
Utgoff Saxena (1987) represent pair A, B concatenated feature vector
ha1 , . . . , b1 , . . . bn i. preference predicate decision tree induced examples.
Utgoff Clouse (1991) represent preference predicate polynomial. Let =
ha1 , . . . , B = hb1 , . . . bn pair elements represented feature vectors. Let
w1 , . . . , wn set weights. preference predicate P defined follows:
P (A, B) =

(

n
1
i=1 wi (ai bi ) 0
0 otherwise

P

example represents linear constraint weights found solving set
constraints.
Cohen, Schapire Singer (1999) extended mechanism allowing
preference function instead (ai bi ) expressions. present
two methods generating total order based induced preference predicate.
methods use preference predicate construct graph nodes elements
ordered directed edge placed two elements precedence
relation. Two algorithms inferring order graph given. first defines
node degree equals sum outgoing edges minus sum
incoming edges. order constructed selecting node greatest
92

fiLearning Order BDD Variables Verification

degree removing edges graph. second algorithm constructs order
two stages. first stage, strongly connected components graph
found, ordered according dependencies them. second
stage elements component ordered using first algorithm.

3. Learning Algorithm Static Variable Ordering
Producing good variable order requires extensive understanding BDDs relation
model represent. knowledge manually inserted human expert.
However, task complex large models. Therefore, rarely done. Existing
static ordering algorithms use relatively simple heuristic rules based expert
knowledge. rules look model structure compose ordering. Since
rules applied variables models, general thus limited
ability produce good orders. Alternatively, try build program
automatically acquires specific knowledge based ordering experience. section
present algorithm.
first step building learning algorithm deciding knowledge wish
acquire ordering experience. existing ordering algorithms demonstrate
precedence relation variables key consideration order creation.
graph search algorithms search-based graph evaluation algorithms try place
variable variables
influence next state value. Generally, variable order
n
n variables yields 2 precedence pairs. precedence pair v vj denotes variable
vi precede vj variable order. example, variable order a, b, c, yields
precedence pairs b, c, d, b c, b d, c d.
task learning precedence pairs transformed concept learning
task. concept learning task defined by:
universe X concept learned;
concept C subset items X want learn (usually marked
associated boolean characteristic function f c );
set examples pairs form hx, f c (x)i, x X;
set features functions X allow generalization.
many learning tasks difficult transform problem format listed
above. already clear discussion general concept wish
learn set variable pairs first precede second variable
ordering1 .
precisely, define universe concept learned set
pairs h(vi , vj ), i, (vi ,vj ) ordered variable pair comprised v vj ,
variables model . Since expect pairs preferred order,
define ternary instead binary concept. ternary concept following
classes:
1. practice, need small subset precedence pairs constructing total order.

93

fiGrumberg, Livne, & Markovitch

1. C+ , class h(vi , vj ), preferable place v prior vj
order get good initial order.
2. C , class h(vi , vj ), preferable place v vj order
get good initial order.
3. C? , class h(vi , vj ), placing vi vj likely lead
good variable order placing v vj .
following subsections describe algorithms learning using concept.
3.1 Algorithm Framework
start description general framework learning algorithm. goal
find variable orders yield BDDs small number nodes. Given training
model, algorithm first generates set orders variables. define utility
function u variable orders following. orders used initial order
building BDD representation model 2 . BDD (denoted M-BDD) includes
models partitioned transition relation set initial states. utility u
generated order defined reversely proportional number nodes
M-BDD constructed order.
subset consists variable pairs appear together next-state
function selected example extractor possible variable pairs. call
pairs interacting variable pairs. example, next(x) = z (y, z)
interacting variable pair. example tagger tags selected ordered pairs
one classes C+ , C , C? , based evaluated orders. tagged pairs
forwarded feature extractor which, based model, computes pair
feature vector. learner, ID3 (Quinlan, 1986) decision tree generator, uses
tagged feature vectors create pair precedence classifier.
Several training models used manner construct different pair precedence
classifiers. solving new unseen problem, pair precedence classifiers used
ordering algorithm create variable order.
learning framework creating pair precedence classifier training model
given Figure 4. complete data flow displayed Figure 5. following subsections
describe greater detail components framework.
3.2 Training Sequence Generator
goal training sequence generator produce orders high variance quality
exploited tagger (see Subsection 3.4). simplest strategy generating
sequences producing random orders. indeed strategy used
experiments described paper. One potential problem approach
domains good orders (or bad orders) rare. case, random generator
necessarily produce sequences desired diversity quality.
2. use SMV (McMillan, 1993) system purpose.

94

fiLearning Order BDD Variables Verification

Input : Training Model
Output : Precedence Classifier
1. Create sample orders.
2. Use SMV evaluate utility sample order M-BDD size.
3. Find interacting variable pairs training model.
4. Based evaluated sampled orders, tag ordered pair based
interacting variable pair.
5. Transform tagged pair tagged feature vector.
6. Create classifier based tagged feature vectors.
Figure 4: Training model precedence classifier construction

Training
Order
SMV

Training
Model

Evaluated
Orders

Tagged
Pairs

Example

Feature
Extractor

Tagger

M-BDD

Example

Interacting
Variable
Pairs

Learner

Extractor

Real
Model

Ordering
Algorithm

Classifier

Tagged
Feature
Vectors

Order

Figure 5: Data flow

alternative approach actively try producing orders good orders
bad, therefore creating large diversity quality. One way producing
good order taking orders result dynamic ordering process.
Another option using existing static ordering algorithm. One interesting idea
try bootstrap process using results adaptive ordering algorithm
training examples thus resulting progressively diverse input.
95

fiGrumberg, Livne, & Markovitch

3.3 Example Extractor
Given set n variables, extract n (n 1) example ordered pairs training.
actually use ordered pairs examples?
two main reasons selective examples use:
1. example carries computational costs associated tagging, feature extraction,
added computation induction procedure.
2. Noisy examples known harmful effect induction process.
process selecting subset examples, tagged, set untagged
examples called selective sampling. two common ways performing selective
sampling. One automatic methods use various general metrics selecting
informative examples (Lindenbaum, Markovitch, & Rusakov, 1999). way
using domain specific heuristics potential example informative.
work use second approach. Consider function f variables,
represented within BDD n variables (where n). number nodes used
represent f depends relative order variables. means changing
order n variables would influence BDD representation
function f .
BDD representation model checked consists initial states
model next-state functions variables. Since BDD representation
initial states typically small, take account. Therefore, looking
examples, consider next-state functions. Usually, function defined
subset model variables. Thus, order pair variables (v , vj ),
appear together next-state function less likely affect quality
generated order. therefore filter pairs.
3.4 Example Tagger
ordered variable pair (vi , vj ) tagged belonging C+ preferable
place vi vj . Let V = {v1 , . . . , vn } set variables given model. Let
set possible orderings V . Let vi vj set vi precedes
vj . ordered variable pair (vi , vj ) defined preferable (vj , vi )
Average{u(o)|o Ovj vi } Average{u(o)|o Ovi vj }.
Since feasible evaluate possible orders, sample space possible
orders, evaluate partition samples two sets above. averages
estimate real averages, replace term smaller definition
significantly smaller. determined unpaired t-test, tests
significance (with given confidence) difference averages two samples
two populations.
precisely, variable pair v , vj , set sampled orders partitioned two subsets Svi vj Ovi vj Svj vi Ovj vi . unpaired t-test
predetermined confidence level used check averages set utilities differ
significantly. do, ordered pair corresponding set smaller average
96

fiLearning Order BDD Variables Verification

tagged + ordered pair tagged - (meaning belong
C+ C , respectively). Otherwise, average difference significant,
ordered pairs tagged ? (meaning belong C ? ).
elaborative approach could use t-value weight important particular order is. weights could solve conflicts ordering process. scheme
would require, however, method incorporate weights induction algorithm. One
method trying induce continues function instead ternary function.
3.5 Feature Extractor
want generalize training models future unseen models, cannot represent
pairs variable names. Rather, use representation used
across models. induction algorithms require examples represented
feature vectors.
process constructing appropriate feature set crucial part applying
learning algorithm problem. common knowledge engineering process
domain expert comes set features might relevant. role
induction algorithm, then, find combination features relevant
specific problem.
come set features variable pairs. features extracted
model connectivity graph. attributes inspired traditional
static ordering algorithms. attributes categorized three groups:
Variable attributes defined single variable try capture characteristics
model. One example variable-dependence attribute, equals
number variables variable depends. attribute inspired
value used Butler et al. (1991) guide DFS search. higher value indicates
larger portion models variables needed determine variables
next-state value. Thus, higher value may indicate variable location
lower order. Another example variable-dependency, takes
complementary view variable-dependence. attribute equals number
variables depend given variable. higher value may indicate
variable placed higher variable order.
Symmetric pair attributes defined variable pair v , vj . attributes try
capture strength bond two variables, well
pair variables model. example, pair-minimal-distance
measures shortest path variables model connectivity graph.
shorter path indicate stronger bond variables. distance-based
ordering framework (Chung et al., 1993) uses similar feature order variables.
Another example pair-mutual-dependency, counts number variables
whose next-state function depends v vj .
Non-symmetric pair attributes try capture relationship two variables. example, pair-dependency-ratio ratio variabledependency values two variables. ratio relatively high low, may
indicate relative order two. pair-ns-distance evaluates influence one
97

fiGrumberg, Livne, & Markovitch

variable next state value other. measuring distance
variables subgraph represents next-state function.
complete list attributes found Appendix A.
3.6 Induction Algorithm
feature extraction phase, data represented set tagged feature vectors.
type representation used produce classifiers many induction algorithms,
including decision trees (Hunt, Marin, & Stone, 1966; Friedman, 1977; Quinlan, 1979;
Breiman, Frieman, Olshen, & Stone, 1984), neural networks (Widrow & Hoff, 1960; Parker,
1985; Rumelhart & McClelland, 1986) nearest neighbor (Cover & Hart, 1967; Duda &
Hart, 1973). decided use decision tree classifiers relatively fast
learning fast classification. Fast classification especially important since wish
competitive ordering algorithms number variable pairs need
classify large.
Decision trees researched thoroughly last decade, producing many
valuable extensions. One extension enables decision tree give
classification items associate classification confidence estimation. used variant allow conflict resolution. described
Section 3.7.3.
3.7 Ordering Algorithm
outcome learning process described last four subsections set decision
trees, one training model.
could generate one tree based union generated samples. One advantage
multiple-tree approach expect examples model
consistent, allowing generating compact trees. contrast, set examples coming
different models likely noisy, yielding large tree. addition, multiple-tree
version allows us using voting scheme ordering process, described below.
Given model M, algorithm first extracts interacting variable pairs.
classifiers applied feature vector representations pairs.
classifier, classifications pairs gathered form precedence table.
tables merged one table. order creation algorithm uses merged
precedence table construct models variable order. following subsections describe
components greater detail. Figure 6 shows data flow ordering algorithm.
3.7.1 Building Precedence Table
build precedence table based given classifier, algorithm asks two questions
interacting variable pair vi , vj :
1. vi vj ?
2. vj vi ?
98

fiLearning Order BDD Variables Verification

Pair
Precedence
Classifier
Pair
Precedence

Real

Pair

Feature

Problem

Extractor

Extractor

Classifier

Pair

Pair

Tree

Table

Pair

Pair

Tree

Table

Merger

Merged
Pair
Precedence
Classifier

Pair

Pair

Tree

Table

Table

Order
Creation
Algorithm

Variable
Order

Figure 6: Ordering algorithm data flow

1
2
3
4
5
6
7
8
9

vi v j ?



Yes
Yes
Yes
Unknown
Unknown
Unknown

vj vi ?

Yes
Unknown

Yes
Unknown

Yes
Unknown

vi , vj order
Unknown
v j vi
Unknown
v vj
Unknown
Unknown
Unknown
Unknown
Unknown

Table 2: Pair order table
two agree, pair order set agreed order. disagree, order
set unknown. Table 2 summarizes possible answers two questions
resulting pair order.
3.7.2 Merging Algorithm
constructing pair precedence tables training models classifiers, merge
tables using voting scheme. variable pair v , vj , count number tables
vote vi vj number tables vote vj vi . decide pair
order according majority (ignoring unknown votes).
Assuming majority vote chooses order v vj , confidence vote
conf (v v )conf (v v )
computed vote(vii vjj )+vote(vjjvii) , vote(vi vj ) number tables vote
99

fiGrumberg, Livne, & Markovitch

vi vj conf (vi vj ) sum confidence values votes. vote(v j vi )
conf (vj vi ) defined similarly. value turns lower 0.1, set
minimal value 0.1.
3.7.3 Cycle Resolution
order build total, strict order merged table, table must contain
cycle. However, algorithm guarantee this. therefore
apply cycle resolution algorithm makes table cycle-free.
precedence table seen directed graph nodes variables,
weighted edge vi vj vi vj . many possible ways
eliminate cycles directed graph. One reasonable bias removing least number
edges. problem known minimum feedback arc set proven NP-hard
(Karp, 1972). Approximation algorithms problem exist (Even, Naor, Schieber, &
Sudan, 1998), costly purposes.
use instead simple greedy algorithm solve problem. constraints
(edges) gathered list sorted decreasing order according weights
(i.e., confidence). graph initialized hold variable vertices. list
edges traversed edge added close cycle.
3.7.4 Pair Precedence Ordering
stage algorithm, hold acyclic merged precedence table. last step
ordering process convert partial order represented table total order.
done topological ordering. stage, algorithm finds minimal
variables, i.e., variables constrained follow unordered variable.
set, select variable vadd maximal fan-out add last ordered
variable. add variables larger v add appear
constraint unordered variable. desirable place
interacting variables near other. pair precedence ordering (PPO) algorithm
listed Figure 7. Figure 8 lists selection v add PPO .
One possible change ordering process delay cycle resolution last
stage. call version cycle resolution demand. modified algorithm
perform cycle resolution merged table. Instead, algorithm works
merged table may contain cycles. table contains cycle, algorithm must
reach stage variables ordered minimal variables.
case algorithm performs cycle resolution continues ordering
process.
3.8 Experiments
performed empirical evaluation PPO algorithm using models
ISCAS89 (Brglez, Bryan, & Kozminski, 1989) benchmark. ISCAS89 benchmark circuits
used empirically evaluate many algorithms deal various aspects
circuit design (Chamberlain, 1995; Wahba & Borrione, 1995; Nakamura, Takagi, Kimura,
& Watanabe, 1998; Long, Iyer, & Abramovici, 1995; Iyer & Abramovici, 1996; Konuk
& Larrabee, 1993). discovered circuits insensitive initial
100

fiLearning Order BDD Variables Verification

Input : merged pair precedence table.
Output : variable order.
Let V set variables.
Let before(v, V 0 ) = {v 0 V 0 |v v 0 }.
Let after(v, V 0 ) = {v 0 V 0 |v 0 v}.

1. VC = {vi |bef ore(vi , V ) 6= af ter(vi , V ) 6= }
VN C = V V C
2. VC 6=
(a) Vmin = {vi VC |af ter(vi , VC ) = }
(b) vadd = argmaxvi Vmin |bef ore(vi , VC )|
(c) order = order || vadd



b

(d) VC = VC {vadd }
(e) vi VC
af ter(vi , VC ) = bef ore(vi , VC ) =
order = order || vi , VC = VC {vi }
3. Add VN C end variable list.
a. one exists, select one.
b. Add variable order.

Figure 7: Pair precedence ordering
ordering. means entire sample initial orders yielded model BDDs similar
sizes. eliminated circuits set. remaining circuits selected
number variables SMV handle. ended following
five circuits: s1269 (55), s1423 (91), s1512(86), s4863 (153), s6669 (314). numbers
parentheses stand number variables model.
began offline learning session three smaller models (s1269, s1423,
s1512) used training models. models generated 200 random
orders extracted examples described previous section. algorithm
induced three precedence classifiers form decision trees.
number 200 selected since proved sufficiently large. real application
algorithm used anytime algorithm training sequences generated
long user willing wait offline learner. alternative approach would
keep aside validation set would used testing systems performance.
training could stopped learning curve flattens.
algorithm tested two larger models (s4863, s6669).
models, three learned decision trees used generate merged precedence table.
101

fiGrumberg, Livne, & Markovitch

Pair
Merged
Table

Minimal
Elements
Unordered
Variables

Find
Minimal

Filter
Maximal

Maximal
Minimal
Elements

Selected
Element
Select
One

Variable
Order

Figure 8: Pair precedence ordering v add selection

PPO algorithm (with cycle resolution demand) compared random
algorithm. addition, compared results two advanced graph search algorithms
static ordering: DFS append algorithm Fujita et al. (1988) interleave
algorithm Fujii et al. (1993). algorithms used adaptation multiple
starting points (Butler et al., 1991) expanded version, includes tie breaking
rule (Fujita et al., 1993). random results taken based 200 variable orders.
two algorithms run 10 times every model. performance
ordering algorithms measured number nodes model BDDs (partitioned
transition relation initial states).
Table 3 Figure 9 show results obtained. table shows model s6669,
PPO outperformed random order 300%. model s4863, PPO outperformed random order 5%.
PPO vs. Random

6

2.5

x 10

random
PPO

2

BDD nodes

1.5

1

0.5

0

s4863

Model

s6669

Figure 9: Comparative histogram PPO vs. Random

102

fiLearning Order BDD Variables Verification

Model
s4863
s6669

Random
Average
STD
849197
121376
2030880 1744493

PPO
Average
STD
807763 100754
713950
35446

Table 3: Comparative table PPO vs. Random
comparison algorithm two static algorithms given Figure 10.
results show learning algorithm, training, becomes competitive
existing ordering algorithms written experts.
5

10

x 10

Append
Interleave
PPO

9
8
7

BDD nodes

6
5
4
3
2
1
0

s4863

Model

s6669

Figure 10: Comparative histogram PPO vs. Static
evaluate utility learned knowledge would compare performance ordering process without learned knowledge. Ordering without
learned knowledge equivalent random ordering. comparison results
random-ordering algorithm reveals learner indeed induced meaningful knowledge learning process. method much stable random
ordering s6669 indicated comparing standard deviation. large variance
results random ordering indeed exploited tagging procedure explained Section 3.4. small variance results obtained random ordering
s4863 explain improvement obtained PPO algorithm much smaller
circuit. sophisticated training sequence generator, described
Section 3.2, might successful circuit.
comparison hand-crafted algorithms may look disappointing first look
since results learning system better existing algorithms. Recall,
however, comparing automated learning process human expertise.
works empirical machine learning make comparisons performance
various learning algorithms. common compare performance learning
103

fiGrumberg, Livne, & Markovitch

algorithm human expert expert system since cases clear handcrafted algorithms would outperform automated learning processes. Since hardly
learning systems built solve BDD variable ordering problem
could make common comparison learning systems.

4. Learning Context-Based Precedence Static Ordering
precedence relation one key considerations used traditional static ordering
algorithms. Another key consideration clustering variables subsequent
ordering. algorithms try place highly interacting variables near other.
effect variable clustering BDD seen simple example given
Figure 3. function, switching two variables v 2 v3 increases BDD size
3 nodes. function, orders variables two clusters,
v1 , v3 v2, , v4 , kept together yield minimal BDD representation. variable
orders yield less compact BDD. Thus, function, key consideration
compliance clustering (precedence taken account).
4.1 Variable Distance
discussion leads hypothesis distance variables important factor considering alternative orders. One way obtain distance information
learning distance function pairs variables. are, however, two
problems approach:
1. target distance function well-defined across models. example,
train small models, absolute distance function likely applicable
large models.
2. Information absolute distances variables sufficient construct
good ordering. absolute distance uniquely define
order variables. fact, defines two possible orders, one
reverse other.
example Figure 11 demonstrates order reverse yield BDDs
significantly different size. BDDs Figure 11 represents two
functions, f1 (a, b, c, d, e) = (a = b = c) (c = d) f2 (a, b, c, d, e) = (a = b =
c) (c = e). absolute distance variables orders clearly
same. However, upper BDD approximately double size lower one.
wanted check whether realistic examples reverse orders yield BDDs
significantly different size. tested models ISCAS89 benchmarks
created 5,000 variable orders model. order, compared
quality quality reversed order. found many cases one order
exceptionally good reversed one exceptionally bad. Thus, learning
absolute distance sufficient, information needed.
conclude problems inherent learning utilizing absolute
distances. Still, clustering key consideration pursued. suggest,
104

fiLearning Order BDD Variables Verification

f1



f2

b
c

e

1

1
0

1

1

1

1

0
0

1

1

0

(a)
f2

e
f1


c
b


1

1
0

0
0

1

1

0

(b)

Figure 11: ROBDDs functions f 1 (a, b, c, d, e) = (a = b = c) (c = d)
f2 (a, b, c, d, e) = (a = b = c) (c = e)

alternatively, learning relative distance determines, variables v , vj , vk ,
vj , vk closer vi , given vi precedes two.
remainder section describes method learning utilizing context-based
precedence infer relative distance variables.
4.2 Context-Based Precedence
context precedence relation triplet v vj vk : given vi precedes vj vk ,
variable vj come variable vk . Thus, context precedence relation adds
context pair ordering decisions.
pair precedence learning, define universe set pairs h(v , vj , vk ),
vi , vj , vk variables model . universe divided three classes,
C+ , C , C? , before. Examples classes drawn way. pair
precedence framework applied minor changes work context precedence
relations. minor changes described below.
4.3 Example Tagger
variable triplet (vi , vj , vk ) tagged C+ if, given vi precedes vj vk ,
preferable place vj vk (i.e., vi vj vk ). pair precedence learning, use
set evaluated variable orders tagging. set orders partitioned
three subsets, depending three variables first. Given partition defined
vi (for example), test order v j vk using t-test, described Section
105

fiGrumberg, Livne, & Markovitch

3.4. reduce number noisy examples, use partition yields
significant t-test results.
4.4 Feature Extractor
attributes triplet (vi , vj , vk ) computed based attributes two pairs
vi vj vi vk . attribute value division/subtraction two corresponding
attribute values two pair attributes.
precisely, assume pair v vj attributes f1 (vi , vj ), . . . , fn (vi , vj )
pair vi vk attributes f1 (vi , vk ), . . . , fn (vi , vk ). triple (vi , vj , vk )
attributes f1 (vi , vj )/f1 (vi , vk ), . . . , fn (vi , vj )/fn (vi , vk ). fl (vi , vk ) 0
corresponding attributes subtracted instead divided.
example consider attribute f l pair minimal distance (see Section 3.5).
fl (vi , vj )/fl (vi , vk ) greater 1 shortest path v vj larger
shortest path vi vk . attribute indicate v k appear
closer vi .
Similarly, fl pair mutual dependency fl (vi , vj )/fl (vi , vk ) > 1 indicates
number variables whose next-state function depends v vj greater
depending vi vk . may indicate preferable keep v
vj close together.
4.5 Ordering Algorithm
outcome learning phase set decision trees, one model.
case context-free pairs. subsection describe ways use
trees ordering.
4.5.1 Building Context Precedence Table
case pair precedence table size n 2 (where n number
variables), produce one table context variable. table
perform inconsistency elimination similar described Section 3.7.1. Here, however,
ask classifier two questions v j , vk vk , vj , add context variable
vi query.
4.5.2 Pair Precedence Ordering Context Precedence Filtering
ordering algorithm uses pair precedence table way PPO algorithm. However, often found case PPO algorithm several
minimal variables, even employing maximal fanout filter. use contextbased precedence table reduce size set minimal elements. use
variables already ordered sequence context variables look associated tables. set minimal elements contains pair variables constrained
vj vk one tables, eliminate vk set. Figure 12 lists code
added PPO algorithm, accepts variable set V add (from previously
selected randomly), returns one variable. call new algorithm PPO CPF .
Figure 13 lists selection vadd PPOCPF .
106

fiLearning Order BDD Variables Verification

Input : set candidate variables added, V add , merged context precedence
table.
Output : variable added.
Let after(v, Vi , Vj ) = {hvi , vj vi Vi , vj Vj |vi vj v}.
0
b.1 Vadd
= {vi Vadd |af ter(vi , VInOrder , Vadd ) = }
0
b.2 Vadd
6=
0
select randomly one variable V add
else select randomly one variable V add

Figure 12: Pair precedence ordering context precedence filtering
Context
Merged
Table

Pair
Merged
Table

Minimal
Elements
Unordered
Variables

Find
Minimal

Filter
Maximal

Maximal
Minimal
Elements

Filter
Context
Constrained

Unconstrained
Minimal
Elements

Selected
Element
Select
One

Variable
Order

Figure 13: Pair precedence ordering context precedence filtering v add selection

4.6 Experiments
evaluated performance PPO CPF , performing off-line learning
training models followed ordering test models. results shown Figure 14.
comparison show performance PPO algorithm two expert
algorithms.
P P CP F algorithm outperforms algorithms two tested models.
results show context-based precedence relations add valuable information.
tested effect resources invested learning phase performance algorithms. Since learning examples tagged based evaluated training
orders, since evaluation training orders resource-consuming operation, used number orders resource estimator. Figure 15 shows
learning curves algorithms, is, shows system performance changes
according offline resources consumed (the number training orders evaluated).
Without testing random order, system knowledge build
precedence classifiers, thus performance equivalent random ordering.
107

fiGrumberg, Livne, & Markovitch

5

10

x 10

Append
Interleave
PPO
CPF
PPO

9
8
7

BDD nodes

6
5
4
3
2
1
0

s4863

Model

s6669

Figure 14: Comparative histogram ordering algorithms
6

2.2

x 10

s4863
s6669

2

1.8

bdd nodes

1.6

1.4

1.2

1

0.8

0.6
0

20

40

60

80
100
120
number examples

140

160

180

200

Figure 15: Learning curves PPO CPF algorithm two testing models

tagging based 20 orders noisy. improves performance s6669,
degrades performance s4863. Forty orders sufficient generate stable tagging,
yields improved classifiers therefore improved ordering quality.

5. Discussion
work described paper presents general framework using machine learning
methods solve static variable ordering problem. method assumes availability
108

fiLearning Order BDD Variables Verification

training models. training model, learning algorithm generates set
random orders evaluates building associated BDDs. ordered pair
interacting variables tagged good example appears frequently
highly valued orders. ordered pairs converted feature-based representations
given, associated tags, induction algorithm. ordering
variables new unseen model, resulting classifiers (one model) used
determine ordering variable pairs. present extension method
learns context-based ordering.
algorithm empirically tested real models. performance significantly
better random ordering, meaning algorithm able acquire useful ordering
knowledge. results slightly better existing static ordering algorithms handcrafted experts. result significant compare applications learning
systems domains. would surely appreciate induction algorithm produces
classifier performance comparable expert system built medical
expert. chess learning program able learn evaluation function
equivalent power function produced expert similarly appreciated.
therefore claim ability learning algorithm achieve results
good manually designed algorithms indicates strong learning capabilities.
learning algorithms, expect get better performance testing
problems similar training problems. verification domain, expect get
good results testing training models come family similar models.
several occasions models similar enough considered family:
Models different versions design development; models reduced
versions design, respect different property; models designs
similar functionality ALUs, arbiters, pipelines bus controllers. Unfortunately,
due difficulty obtaining suitable real models experiments, ended
experimenting training testing models related. expect achieve
much better results related models.
Compared previous work machine learning, precedence relations resemble Utgoff Saxena (1987). ordering approach, construct
total order elements finding precedence relation them, essence
Cohen, Schapire Singer (1999). Specifically, second ordering algorithm Cohen, Schapire Singer uses topological ordering approach create
order. algorithm initially finds precedence graph connected components
and, ordering (using topological ordering), finds order connected
component. However, since quality final order determined sum constraints adhered to, topological orders theoretically quality. found
BDD variable ordering problem topological orders quality.
Thus, developed topological ordering takes consideration features
recognized true variable orders BDDs.
work differs previous research introduces notion contextbased precedence. Using concept able create ordering algorithm
produces best results.
several directions extending work described here. One problem
current empirical evaluation small number models. spite extensive
109

fiGrumberg, Livne, & Markovitch

search efforts able find large set suitable examples. majority
known examples simple (compared real industry problems), producing small
model BDD representations little variance. currently process
approaching companies use model checking. way hope obtain additional
real models, preferably families designs described above.
attributes variable pairs partially based substantive research
field static algorithms. could find information base contextbased variable attributes. Thus, based attributes variable
pairs. Nevertheless, believe human experts field may information
lead development better attributes. development attributes
help capture better way context-based precedence concept.
Given current results, immediate question whether concept precedence
pairs (context non-context) extended triplets, quadruples, etc. precedence relations take account larger part model thus may possess valuable
information. extension, however, could carry high cost learning and, even
worse, ordering.
framework solving static variable ordering problem shown valuable
model checking. Model checking one field verification BDDs used.
BDDs used verification simulation equivalence checking. algorithm
applied problems well. unaware special static variable ordering
algorithms fields, exist, variable attributes based algorithms
added.
interesting future direction generalization framework
ordering problems. Ordering set objects common sub-task problem solving.
common approach tackling problem evaluate object using
utility function order objects according utilities. approach
taken, example, heuristic search algorithms. many problems, however,
much easier determine relative order two objects give object global
utility value. works applied learning ordering techniques utility
based (Cohen et al., 1999). algorithms described Section 3 Section 4
applied ordering problem method evaluating training orders available,
set meaningful pair features defined.
believe research presented paper contributes field
machine learning field formal verification. machine learning, presents
new methodology learning order elements. methodology applied
various kinds ordering problems. formal verification, presents new learning-based
techniques variable ordering. Finding good variable ordering techniques one
key problems field.

Appendix A. Variable Pair Attributes
following definitions symbols used attribute description:
N S(vi ) next state function variable v
vi . vj indicate variable vi depends variable vj value (vj NS(vi ))
110

fiLearning Order BDD Variables Verification

vi ./ vj indicate variable vi interacts variable vj (vi . vj and/or vj . vi )
# variables number variables model
A.1 Variable Attributes
attributes computed vi
1. Variable-dependence: number variables upon v depends (|{vj |vi . vj }|)
2. Variable-dependency: number variables depend v (|{vj |vj . vi }|)
3. Variable-dependency-size: sum function sizes depend v (

P

vj .vi

|{vk N S(vj )}|)

4. Variable-dependency-average-size:
average function size dependent v
!
P
vj .vi

|{vk N S(vj )}|

|{vj |vj .vi }|

5. Variable-dependence-dependency-ratio: proportion number vari!
ables vi depends number variables depend

|{vj |vi .vj }|
|{vj |vj .vi }|

6. Variable-interaction: number variables interacting v (|{vj |vi ./ vj }|)
7. Variable-dependence-percentage:
percentage model variables v de!
pends

|{vj |vi .vj }|
#variables

8. Variable-dependency-percentage:
percentage model variables depend v
!
|{vj |vj .vi }|
#variables

9. Variable-interaction-percentage:
percentage model variables interacting v
!
|{vj |vi ./vj }|
#variables

A.2 Variable Pair Attributes
attributes computed hvi , vj
Symmetric attributes
1. Pair-minimal-distance: minimal distance v ,vj model graph
2. Pair-minimal-distance-eval: minimal distance v ,vj model
graph divided number times appears
3. Pair-minimal-dependency: number variables depend pair
minimal distance
4. Pair-minimal-dependency-eval: minimal distance v ,vj model
graph divided number variables depend minimal distance
111

fiGrumberg, Livne, & Markovitch

5. Pair-minimal-connection-class: minimal distance v ,vj connection class (the operators applied two variables divided
classes operator connected two variables minimal distance
class extracted)
6. Pair-minimal-maximal: maximal sized NS(v k ) connecting pair minimal distance
7. Pair-minimal-maximal-eval: minimal distance v ,vj model
graph divided maximal sized NS(v k ) connecting pair minimal distance
8. Pair-sum-distance: sum distances v ,vj model graph
9. Pair-dependency-ns-size: sum NS(v k ) sizes dependent vi
P
vj ( vk .vi & vk .vj |vl N S(vk )|)

10. Pair-sum-distance-dependency-ratio: sum distances v ,vj
model graph divided sum NS(vk ) sizes dependent vi vj
11. Pair-mutual-dependence: number variables v ,vj depend
(|{vk |vi . vk & vj . vk }|)
12. Pair-mutual-dependency: number variables depend v vj
(|{vk |vk . vi & vk . vj }|)
13. Pair-mutual-interaction: number variables interact v vj
(|{vk |vi ./ vk & vi ./ vk }|)

14. Pair-mutual-ns-dependency: v depends vj vj depends vi - (vi . vj & vj . vi )
Non-Symmetric attributes ( computed pair hv , vj relevance vi )
1. Pair-ns-distance: distance v ,vj NS(vi )
2. Pair-dependence-ratio: ratio number variables !
v depends
number variables v j depends

|{vl |vi .vl }|
|{vm |vj .vm }|

3. Pair-dependency-ratio: ratio number variables
! depend
vi number variable depend v j

|{vl |vl .vi }|
|{vm |vm .vj }|

4. Pair-interaction-ratio: ratio number variables !interact
|{vl |vi ./vl }|
|{vm |vj ./vm }|

vi number variables interact v j

5. Pair-dependence-flag: number variables v depends
! compared
number variables vj depends

|{vl |vi .vl }|
|{vm |vj .vm }|

>= 1.0

6. Pair-interaction-flag: number variables interact v compared

!
number variables vj interacts

112

|{vl |vi ./vl }|
|{vm |vj ./vm }|

>= 1.0

fiLearning Order BDD Variables Verification

References
Akers, S. (1978). Binary decision diagrams. IEEE Transactions Computers, C-27 (6),
509516.
Aziz, A., Tasiran, S., & Brayton, R. (1994). BDD variable ordering interacting finite
state machines. Proceedings 31st Design Automation Conference (DAC), pp.
283288, San Diego, California.
Beer, I., Ben-David, S., Eisner, C., & Landver, A. (1996). RuleBase: industry-oriented
formal verification tool. Proceedings 33rd Design Automation Conference
(DAC), pp. 655660, Las Vegas, Nevada. IEEE Computer Society Press.
Bern, J., Meinel, C., & Slobodova, A. (1995). Efficient OBDD-based boolean manipulation CAD beyond current limits. Proceedings 32nd Design Automation
Conference (DAC), pp. 408413, San Francisco, California.
Bollig, B., Lobbing, M., & Wegener, I. (1995). Simulated annealing improve variable orderings OBDDs. Proceedings International Workshop Logic Synthesis,
pp. 5b:5.15.10, Granlibakken, California.
Bollig, B., & Wegener, I. (1996). Improving variable ordering OBDDs NP-complete.
IEEE Transactions Computers, 45 (9), 9931002.
Breiman, L., Frieman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classification
Regression Trees. Wadsworth Publishing Company, Belmont, California, U.S.A.
Brglez, F., Bryan, D., & Kozminski, K. (1989). Combinational profiles sequential benchmark circuits. Proceedings International Symposium Circuits Systems,
pp. 19241934, Portland, Oregon.
Broos, P., & Branting, K. (1994). Compositional instance-based learning. Proceedings
12th National Conference Artificial Intelligence, pp. 651656, Menlo Park,
California. AAAI Press.
Bryant, R. (1986). Graph-based algorithms boolean function manipulation. IEEE Transactions Computers, C-35 (8), 677691.
Butler, K. M., Ross, D. E., & Rohit Kapur, a. M. R. M. (1991). Heuristics compute
variable orderings efficient manipulation ordered binary decision diagrams.
Proceedings 28th Design Automation Conference (DAC), pp. 417420, San Francisco, California.
Chamberlain, R. (1995). Parallel logic simulation VLSI systems. Proceedings
32nd Design Automation Conference (DAC), pp. 139143, San Francisco, California.
Chung, P., Hajj, I., & Patel, J. (1993). Efficient variable ordering heuristics shared
ROBDD. Proceedings International Symposium Circuits Systems,
pp. 16901693, Chicago, Illinois.
Clarke, E. M., Emerson, F. A., & Sistla, A. P. (1986). Automatic verification finite
state concurrent systems using temporal logic specifications. ACM Transactions
Programming Languages Systems, 8 (2), 244263.
113

fiGrumberg, Livne, & Markovitch

Cohen, W. W., Schapire, R. E., & Singer, Y. (1999). Learning order things. Journal
Artificial Intelligence Research, 10, 243270.
Cover, T. M., & Hart, P. E. (1967). Nearest neighbor pattern classification. IEEE Transactions Information Theory, 13, 2127.
Drechsler, R., Becker, B., & Gockel, N. (1996). Genetic algorithm variable ordering
OBDDs. IEEE Proceedings Computers Digital Techniques, 143 (6), 364368.
Drechsler, R., Drechsler, N., & Slobodova, A. (1998). Fast exact minimization BDDs.
Proceedings 35th Design Automation Conference (DAC), pp. 200205, San
Francisco, California.
Duda, R. O., & Hart, P. E. (1973). Pattern Classification Scene Analysis. John Wiley
Sons, New York.
Even, G., Naor, J., Schieber, B., & Sudan, M. (1998). Approximating minimum feedback
sets multi-cuts directed graphs. Algorithmica, 20, 151174.
Friedman, J. (1977). recursive partitioning decision rule nonparametric classification.
IEEE Transactions Computers, C-26 (4), 404408.
Friedman, S. J., & Supowit, K. J. (1987). Finding optimal variable ordering binary
decision diagrams. Proceedings 24th Design Automation Conference (DAC),
pp. 151174, Miami Beach, Florida.
Fujii, H., Ootomo, G., & Hori, C. (1993). Interleaving based variable ordering methods
ordered binary decision diagrams. Proceedings IEEE/ACM international
conference Computer-aided design, pp. 3841, Santa Clara, California.
Fujita, M., Fujisawa, H., & Kawato, N. (1988). Evaluation improvements boolean
comparison method based binary decision diagrams. Proceedings International Conference Computer-Aided Design, pp. 25, Santa Clara, California.
Fujita, M., Fujisawa, H., & Matsunaga, Y. (1993). Variable ordering algorithms ordered
binary decision diagrams evaluation. IEEE Transactions Computer-Aided
Design Integrated Circuits Systems, 12 (1), 612.
Fujita, M., Kukimoto, Y., & Brayton, R. (1995). BDD minimization truth table permutations. Proceedings International Workshop Logic Synthesis, pp. 596599,
Lake Tahoe, California.
Hunt, E., Marin, J., & Stone, P. (1966). Experiments Induction. Academic Press, New
York.
Ishiura, N., Sawada, H., & Yajima, S. (1991). Minimization binary decision diagrams
based exchanges variables. Proceedings International Conference
Computer-Aided Design, pp. 472475, Santa Clara, California.
Iyer, M., & Abramovici, M. (1996). FIRE: fault-independent combinational redundancy
identification algorithm. IEEE Transactions VLSI Systems, 4, 295301.
Jain, J., Adams, W., & Fujita, M. (1998). Sampling schemes computing variable orderings. Proceedings International Conference Computer-Aided Design, pp.
631638, San Jose, California.
114

fiLearning Order BDD Variables Verification

Karp, R. M. (1972). Reducibility among combinatorial problems. Miller, R., & Thatcher,
J. (Eds.), Complexity Computer Computations, pp. 85103, New York. Plenum
Press.
Kaufmann, M., & Pixley, C. (1997). Intertwined development formal verification
60x bus model. Proceedings International Conference Computer Design:
VLSI Computers Processors (ICCD 97), pp. 2530, Austin, Texas.
Konuk, H., & Larrabee, R. (1993). Explorations sequential atpg using boolean satisfiability. Proceedings 11th IEEE VLSI Test Symposium, pp. 8590.
Lindenbaum, M., Markovitch, S., & Rusakov, D. (1999). Selective sampling nearest
neighbor classifiers. Proceedings Sixteenth national confernce Artificial
Intelligence, pp. 366371, Orlando, Florida.
Long, D., Iyer, M., & Abramovici, M. (1995). Identifying sequentially untestable faults
using illegal states. Proceedings 13th IEEE VLSI Test Symposium, pp. 411,
Los Alamitos, California.
Malik, S., Wang, A., Brayton, R., & Sangiovanni-Vincentelli, A. (1988). Logic verification
using binary decision diagrams logic synthesis environment. Proceedings
International Conference Computer-Aided Design, pp. 69, Santa Clara, California.
McMillan, K. (1993). Symbolic Model Checking: Approach State Explosion Problem. Kluwer Academic Publisher.
Meinel, C., & Slobodova, A. (1997). Speeding variable ordering OBDDs. Proceedings International Conference Computer-Aided Design, pp. 338343, Austin,
Texas.
Meinel, C., & Slobodova, A. (1998). Sample method minimization OBDDs. Proceedings Conference Current Trends Theory Practice Informatics,
Vol. 1521 Lecture Notes Computer Science, pp. 419428. Springer-Verlag, New
York.
Meinel, C., Somenzi, F., & Theobald, T. (1997). Linear sifting decison diagrams.
Proceedings 34th Design Automation Conference (DAC), pp. 202207, Anaheim,
California.
Mercer, M. R., Kapur, R., & Ross, D. E. (1992). Functional approaches generating
orderings efficient symbolic representations. Proceedings 29th Design
Automation Conference (DAC).
Minato, S., Ishiura, N., & Yajima, S. (1990). Shared binary decision diagrams attributed edges efficient boolean function manipulation. Proceedings 27th
Design Automation Conference (DAC), pp. 5257, Orlando, Florida.
Nakamura, K., Takagi, K., Kimura, S., & Watanabe, K. (1998). Waiting false path analysis
sequential logic circuits performance optimization. Proceedings International Conference Computer-Aided Design, pp. 392395, San Jose, California.
Panda, S., & Somenzi, F. (1995). variables neighbourhood. Proceedings International Conference Computer-Aided Design, pp. 7477, San
Jose, California.
115

fiGrumberg, Livne, & Markovitch

Panda, S., Somenzi, F., & Plessier, B. F. (1994). Symmetry detection dynamic variable
ordering decision diagrams. Proceedings International Conference
Computer-Aided Design, pp. 628631, San Jose, California.
Parker, D. B. (1985). Learning logic. Tech. rep. TR-47, Center Computational Research
Economics Management Science, MIT, Cambridge, MA.
Queille, J., & Sifakis, J. (1981). Specification verification concurrent systems
cesar. Dezani-Ciancaglini, M., & Montanari, U. (Eds.), Proceedings 5th
International Symposium Programming, Vol. 137 Lecture Notes Computer
Science, pp. 337351. Springer-Verlag, New York.
Quinlan, J. R. (1979). Discovering rules induction large collections examples.
Expert Systems Micro Electronic Age, pp. 168201. Edinburgh University
Press.
Quinlan, J. R. (1986). Induction decision trees. Machine Learning, 1 (1), 81106.
Rudell, R. (1993). Dynamic variable ordering ordered binary decision diagrams.
Proceedings International Conference Computer-Aided Design, pp. 4247,
Santa Clara, California.
Rumelhart, D. E., & McClelland, J. L. (1986). Parallel distibuted processing: Exploration
microstructure cognition.. Vol. 1,2. MIT Press.
Touati, H., Savoj, H., Lin, B., Brayton, R., & Sangiovanni-Vincetelli, A. (1990). Implicit
state enumeration finite state machines using BDDs. Proceedings International Conference Computer-Aided Design, pp. 130133, Santa Clara, California.
Utgoff, P., & Clouse, J. (1991). Two kinds training information evaluation function
learning. Proceedings Ninth National Conference Artificial Intelligence,
pp. 596600, Anaheim, California.
Utgoff, P. E., & Saxena, S. (1987). Learning preference predicate. Proceedings
Fourth International Workshop Machine Learning, pp. 115121, Irvine, California.
Wahba, A., & Borrione, D. (1995). Design error diagnosis sequential circuits. Lecture
Notes Computer Science, 987, 171188.
Widrow, B., & Hoff, M. E. (1960). Adaptive switching circuits. 1960 IRE WESCON
Convention Record, pp. 96104, New York.
Zhuang, N., Benten, M., & Cheung, P. (1996). Improved variable ordering BDDs
novel genetic algorithm. Proceedings International Symposium Circuits
Systems., Vol. 3, pp. 414417, Atlanta, Georgia.

116


